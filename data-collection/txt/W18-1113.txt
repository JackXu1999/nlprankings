



















































Predicting Authorship and Author Traits from Keystroke Dynamics


Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 98–104
New Orleans, Louisiana, June 6, 2018. c©2018 Association for Computational Linguistics

Predicting Authorship and Author Traits from Keystroke Dynamics

Barbara Plank
IT University of Copenhagen

Department of Computer Science
Rued Langgaards Vej 7, 2300 Copenhagen S, Denmark

bapl@itu.dk

Abstract

Written text transmits a good deal of non-
verbal information related to the author’s iden-
tity and social factors, such as age, gender
and personality. However, it is less known to
what extent behavioral biometric traces trans-
mit such information. We use typist data to
study the predictiveness of authorship, and
present first experiments on predicting both
age and gender from keystroke dynamics. Our
results show that the model based on keystroke
features leads to significantly higher accura-
cies for authorship than the text-based system,
while being two orders of magnitude smaller.
For user attribute prediction, the best approach
is to combine the two, suggesting that extra-
linguistic factors are disclosed to a larger de-
gree in written text, while author identity is
better transmitted in typing behavior.

1 Introduction

Language is a social phenomenon (Nguyen et al.,
2015). Whenever we speak or write we transmit
a good deal of additional non-verbal information
that is related to identity and social factors of an
author. Early work in authorship analysis has typ-
ically been concerned with finding the author of
a text, i.e., authorship attribution (Mosteller and
Wallace, 1964; Stamatatos, 2009). In recent years,
there has been a surge of interest towards the so-
cial dimension of language. Studies are inter-
ested in linking social factors with linguistic fea-
tures, e.g., (Eisenstein et al., 2011; Bamman et al.,
2014), studying data biases (Hovy and Søgaard,
2015) or building actual attribute prediction mod-
els from linguistic features (i.e., author profiling).
Modeling author traits can further help to im-
prove prediction of related attributes (Liu et al.,
2016; Benton et al., 2017), help debiasing mod-
els (Hovy, 2015; Zhang et al., 2018) or can be used
for a wide range of applications like customer sup-

port, healthcare and personalized machine transla-
tion (Mirkin et al., 2015; Rabinovich et al., 2017).
Factors studied so far include gender, age, per-
sonality or income, to name but a few (Mairesse
and Walker, 2006; Luyckx and Daelemans, 2008;
Rao et al., 2010; Rosenthal and McKeown, 2011;
Nguyen et al., 2011; Volkova et al., 2013; Flekova
et al., 2016b; Verhoeven et al., 2016; van Dalen
et al., 2017; Ljubešić et al., 2017; Emmery et al.,
2017; van der Goot et al., 2018).

A key question in authorship analysis and pro-
filing is what sorts of evidence might bear on de-
termining authorship (Nerbonne, 2007) (or traits).
What all prior work has in common is that it al-
most exclusively focused on the written text itself.
As people read or write texts, they unconsciously
produce cognitive by-product, such as gaze pat-
terns or typist behavior. This evokes and motivates
our research question: to what extent is behavioral
data beyond the text predictive of authorship and
author traits? In this paper we focus on keystroke
dynamics. They concern a user’s typing pattern.
Keystroke logs have the distinct advantage over
other cognitive modalities like brain scans or gaze,
that keystroke logs are more readily available; they
do not rely on special equipment beyond a key-
board. While keystrokes are known to be infor-
mative for author verification (cf. Section 5), it is
less clear to what extent keystrokes are predictive
of authorship, and even more so, of author traits.

Contributions a) We study the effect of
keystrokes to identify authorship in two corpora
of varying size. b) We investigate the predictive
power of typist data for age and gender prediction.
c) We compare behavioral measures to traditional
stylometric features.

98



2 Keystroke dynamics

Keystroke logs are recordings of a user’s typing
dynamics. When a person types on a keyboard, the
latencies between successive keystrokes and their
duration reflect the typing behavior of a person.
For example, Figure 2 shows the keystroke hold
times (average over single letters) of two users
from our dataset. In its raw form, keystroke logs
contain information on which key was pressed for
how long (key, time press, time release). Research
on keystroke dynamics typically consider timing
measures derived from time press and time release
events between keystrokes, such as key hold times
or interkey durations (see Figure 1).

Figure 1: Keystroke logs illustrated: p are pauses be-
tween keystrokes. Figure adapted from (Goodkind and
Rosenberg, 2015).

Only very recently this source has been ex-
plored as information in natural language process-
ing, for example, to aid shallow syntactic pars-
ing (Plank, 2016) or deception detection (Baner-
jee et al., 2014) (Section 5). Keystroke logs have
been used in computer security for user verifi-
cation, however, combining keystroke biometrics
with traditional stylometry metrics has not been
proven successful (Stewart et al., 2011). The au-
thors focused on a single task and dataset only. In
contrast, in this paper we examine to what extent
keystroke dynamics are informative for authorship
attribution and author profiling.

3 Experiments

Given a dataset with keystroke logs, we run two
sets of experiments: a) authorship attribution, i.e.,
to determine who wrote a given piece of text; and
b) authorship profiling, i.e., to determine extra-
linguistic user traits, in particular age and gender.

Figure 2: Keystroke basic feature set: normalized mean
key hold time for two example users.

Datasets The two keystroke datasets differ in
the amount of users and available meta-data. The
first, STEWART, stems from students taking a test
on spreadsheet modeling (Stewart et al., 2011).
This dataset is not distributed with further meta-
data, hence it is used for authorship attribution
only. The second dataset, VILLANI (Tappert et al.,
2009), is larger (144 participants) and contains de-
mographic meta-data. Keystrokes were recorded
for two tasks: free text production and a copy task
(fixed text snippet). As we are interested in author
attribution/profiling, we consider only the former.

Pre-processing and Features First, we remove
users with fewer than 5 typing sessions, sessions
shorter than 5 words, users without demographics
and users that only participated in the copy task
(for VILLANI). We also removed two spammers
(random skribble). This resulted in a dataset with
34 and 121 users with an average of 99 and 125
tokens per session for STEWART and VILLANI, re-
spectively. The final gender/age distribution is not
balanced: 53 female/68 male users, and 56 users
above/65 user below thirty. For all keystrokes, the
type of key was derived: letters, numbers, punctu-
ation etc., ignoring control keys (FN etc).

Second, we derive 218 biometric features fol-
lowing (Stewart et al., 2011; Tappert et al., 2010).
These biometric features include duration features
(mean and standard deviation) and are grouped
into: i) basic keystroke features, i.e., key hold
time (key press and release time) features of the
26 letters from the English alphabet (cf. Figure 2
for an illustration); and ii) extended features: key
hold times over groups of keys (like digits, punc-
tuation etc) and transition (inter-key duration) fea-
tures between successive keystrokes, e.g., between
letters and non-letters, or individual letters and

99



AUTHORSHIP ATTRIBUTION

STEWART (34 users) #FEATURES F1-SCORE

Baseline (random) – 0.4
Text 54k 50.2
Keystrokes (basic) 52 81.0
Keystrokes (extended) 218 90.2
Keystrokes (ext.) + Text 55k 70.2
Keystrokes (ext.) + Embeds 413 91.4
VILLANI (121 users) #FEATURES F1-SCORE
Text 46k 67.1
Keystrokes (basic) 52 2.5
Keystrokes (ext.) 218 85.9
Keystrokes (ext.) + Text 46k 71.1
Keystrokes (ext.) + Embeds 413 88.1

Table 1: Results for authorship attribution (34 users),
comparison of features (text vs keystrokes), and com-
bined models. Best result in boldface.

groups of such. For these feature measurements,
outlier removal and feature standardization is ap-
plied (Stewart et al., 2011).

Finally, we extract the final text from
the keystroke logging data (employing revi-
sions/backspaces were appropriate). As features
we employ those used by the top performing
system of the latest PAN author profiling compe-
tition (Basile et al., 2017), i.e., word n-grams and
character n-grams. N-gram size is tuned on one
fold on STEWART, resulting in word unigrams and
character 2-3 grams. We also use word embedding
features using Polyglot embeddings of 64 dimen-
sions (Al-Rfou et al., 2013), representing text
snippets as average embeddings (CBOW) over
all tokens (Collobert et al., 2011), enriched with
max, sum, standard deviation and embeddings
coverage rate. These features worked best on dev.

Setup We use a Support Vector Machine
(SVM) (Pedregosa et al., 2011) with linear ker-
nel and `2 regularization, similar to the state-of-
the-art in author profiling (Flekova et al., 2016a;
Basile et al., 2017). We consider a single session
of a user as a data instance, and run experiments
using 5-fold cross-validation. For author profiling
we ensure that all instances of an author end up
in the same fold, to not confound profiling with
authorship. We report results using weighted F1-
score. To ease replicability, all code is released at:
https://github.com/bplank/aat

4 Results

The results of training a classifier to predict the
identity of an author are given in Table 1 and Fig-

Figure 3: Results for author attribution on VILLANI
for increasing number of authors. Keystroke features
clearly outperform textual features.

ure 3. The random baseline accuracy is low (0.4%
F1). Biometric behavioral features work incredi-
bly well, reaching a performance in the 80-90ies.
Already the basic feature set of 52 letter dura-
tion features clearly outperforms the stylistic fea-
tures, reaching 81% F1-score. In contrast, stylo-
metric features from the text alone reach an F1
of only 50%. Note that for the dataset with more
users (VILLANI, Figure 3), results for authorship
are actually higher, which may be explained by
the fact that the smaller dataset is more controlled
by topic (exam questions). Figure 3 shows that
also on the larger dataset keystroke features out-
perform the text-based features (word and charac-
ter n-grams) for authorship, even in setups with
few users. These are remarkable results. The
behavioral models employ a considerably smaller
feature space (cf. column 2 in Table 1). Adding
stylometric features improves performance over
keystrokes, but only for the embeddings setup,
which results in the best setup.

The results for author profiling are given in Ta-
ble 2. Baseline results (majority baseline) are
higher; this task is easier. The gap between stylo-
metric and behavioral features is smaller, but the
same trend holds: biometric behavioral features
are predictive of gender. To a certain extent this
also holds for age (albeit to a lesser extent). In-
terestingly, combining biometrics with traditional
token-based features consistently proves the most
effective for author profiling, albeit the best way
differs per trait.

Our results suggest that author identity is highly
captured in keystrokes alone, while the textual
signal provides complementary evidence that to-
gether proves the most effective for predicting age
and gender of an author.

100



AUTHOR PROFILING - 121 USERS

GENDER AGE #FEATS

Baseline 33.33 38.06 –
Text 61.34 72.59 80k
Keystrokes (basic) 44.63 37.61 52
Keystrokes (ext.) 63.29 60.58 218
KS + text 60.92 73.25 80k
KS + embeds 63.50 67.12 413

Table 2: Gender and age prediction results, F1-score
(age: above/below 30).

5 Related Work

Authorship attribution has a long tradition dating
back to early works in the 19th century. The most
influential work on authorship attribution goes
back to Mosteller and Wallace (1964). For a long
time approaches to authorship attribution focused
on distributions of function words, high-frequency
words that are presumably not consciously ma-
nipulated by the author (Nerbonne, 2007; Pen-
nebaker, 2011). Recent work also includes au-
thorship studies on microblog texts (Schwartz
et al., 2013). An recent survey is Stamatatos
(2009). We here study another source of informa-
tion that is presumably not consciously manipu-
lated, keystroke dynamics.

A major scientific interest in keystroke dynam-
ics arose in writing research, where it has de-
veloped into a promising non-intrusive method
for studying cognitive processes involved in writ-
ing (Sullivan et al., 2006; Nottbusch et al.,
2007; Wengelin, 2006; Van Waes et al., 2009;
Baaijen et al., 2012). In these studies time
measurements—pauses, bursts and revisions—are
considered traces of the recursive nature of the
writing process. Bursts are defined as consecutive
chunks of text produced and defined by a 2000ms
time of inactivity (Wengelin, 2006). In fact, most
prior work that uses keystroke logs focuses on
experimental research. For example, Hanoulle
et al. (2015) study whether a bilingual glossary
reduces the working time of professional transla-
tors. They consider pause durations before terms
extracted from keystroke logs and find that a bilin-
gual glossary reduces the translators’ workload.
An analysis of users’ typing behavior was studied
by Baba and Suzuki (2012) to measure the impact
of spelling mistakes. Goodkind and Rosenberg
(2015) investigate pre-word pauses and their re-

lation to multi-word expressions. They found that
within MWE pauses vary depending on the cogni-
tive task. Banerjee et al. (2014) were the first to
use keystroke patterns for deception detection.

Keystrokes were successfully used for author
verification in computer security research (Stew-
art et al., 2011; Monaco et al., 2013; Locklear
et al., 2014), as they are known to be idiosyn-
cratic (Leggett and Williams, 1988). Our results
show that keystroke biometrics are far superior
over stylometry-based features in authorship attri-
bution, and are predictive of author traits.

The study most related to ours (Stewart et al.,
2011) used features from both keystrokes and lin-
guistic stylometry for user verification in a k-
nearest neighbor setup. Their study differs from
ours in three aspects. First, they use a more elab-
orate set of stylometric features (like number of
words of a certain length, and readability mea-
sures). Second, they target user authentication,
thus their setup is a binary classification task (au-
thenticated vs not-authenticated), while we here
focus on a multi-class classification setup, which
is a considerably more difficult task. Third, they
use only a single dataset (STEWART), while we
here include results on a second and larger dataset
(n=121 authors). To the best of our knowledge,
prior work on predicting demographics from typ-
ing behavior is typically limited to a single vari-
able (Tsimperidis et al., 2015), except (Brizan
et al., 2015), whose data is not available. Our
study differs from theirs by studying age, and the
focus on complementing textual with behavioral
data.

Disclaimer While modeling user demographics
can be seen as one step towards addressing biases
in NLP it is important to be aware of potential
negative side effects, both from the modeling side
through potential exclusion or dual use (Hovy and
Spruit, 2016), as well as the data side, when deal-
ing with privacy sensitive data (cognitive behav-
ioral data) or labels (e.g., mental health).

6 Conclusions

We have shown that behavioral biometrics con-
tain highly predictive information for both author-
ship and author profiling. For authorship attri-
bution, behavioral keystroke metrics significantly
outperform traditional text-based features (words
and character unigrams), while using a feature set
which is orders of magnitude smaller (218 vs sev-

101



eral thousands of features). In addition, we show
that keystroke dynamics are also predictive for au-
thor traits (gender and age). Interestingly, for the
latter task, it is most beneficial to combine be-
havioral keystroke data with traditional text-based
features, suggesting that user traits are disclosed
to a larger degree in written text while identity is
better disclosed in typing behavior.

Acknowledgments

I would like to thank the anonymous reviewers
for their valuable feedback. This research is sup-
ported by NVIDIA corporation.

References
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.

2013. Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learn-
ing, chapter Polyglot: Distributed Word Represen-
tations for Multilingual NLP. Association for Com-
putational Linguistics.

Veerle M Baaijen, David Galbraith, and Kees de Glop-
per. 2012. Keystroke analysis: Reflections on pro-
cedures and measures. Written Communication.

Yukino Baba and Hisami Suzuki. 2012. How are
spelling errors generated and corrected? a study
of corrected and uncorrected spelling errors using
keystroke logs. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 373–377.
Association for Computational Linguistics.

David Bamman, Jacob Eisenstein, and Tyler Schnoe-
belen. 2014. Gender identity and lexical varia-
tion in social media. Journal of Sociolinguistics,
18(2):135–160.

Ritwik Banerjee, Song Feng, Jun Seok Kang, and Yejin
Choi. 2014. Keystroke patterns as prosody in digital
writings: A case study with deceptive reviews and
essays. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1469–1473, Doha, Qatar. Associa-
tion for Computational Linguistics.

Angelo Basile, Gareth Dwyer, Maria Medvedeva, Jo-
sine Rawee, Hessel Haagsma, and Malvina Nissim.
2017. N-gram: New groningen author-profiling
model. In Proceedings of the CLEF 2017 Evalua-
tion Labs and Workshop – Working Notes Papers,
11-14 September, Dublin, Ireland (Sept. 2017).

Adrian Benton, Margaret Mitchell, and Dirk Hovy.
2017. Multi-task learning for mental health us-
ing social media text. In Proceedings of European
Chapter of Association for Computational Linguis-
tics. Association for Computational Linguistics.

David Guy Brizan, Adam Goodkind, Patrick Koch, Ki-
ran Balagani, Vir V Phoha, and Andrew Rosenberg.
2015. Utilizing linguistically enhanced keystroke
dynamics to predict typist cognition and demograph-
ics. International Journal of Human-Computer
Studies, 82:57–68.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

Reinder Gerard van Dalen, Léon Redmar Melein, and
Barbara Plank. 2017. Profiling dutch authors on
twitter: Discovering political preference and income
level. Computational Linguistics in the Netherlands
Journal, 7:79–92.

Jacob Eisenstein, Noah A Smith, and Eric P Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1365–1374. Association for Computational
Linguistics.

Chris Emmery, Grzegorz Chrupała, and Walter Daele-
mans. 2017. Simple queries as distant labels for pre-
dicting gender on twitter. In Proceedings of the 3rd
Workshop on Noisy User-generated Text, pages 50–
55, Copenhagen, Denmark. Association for Compu-
tational Linguistics.

Lucie Flekova, Jordan Carpenter, Salvatore Giorgi,
Lyle Ungar, and Daniel Preoţiuc-Pietro. 2016a. An-
alyzing biases in human perception of user age and
gender from text. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 843–
854, Berlin, Germany. Association for Computa-
tional Linguistics.

Lucie Flekova, Daniel Preoţiuc-Pietro, and Lyle Ungar.
2016b. Exploring stylistic variation with age and in-
come on twitter. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 313–319.
Association for Computational Linguistics.

Adam Goodkind and Andrew Rosenberg. 2015. Mud-
dying the multiword expression waters: How cog-
nitive demand affects multiword expression produc-
tion. In Proceedings of the 11th Workshop on Multi-
word Expressions, pages 87–95, Denver, Colorado.
Association for Computational Linguistics.

Rob van der Goot, Nikola Ljubešić, Ian Matroos, Malv-
ina Nissim, and Barbara Plank. 2018. Bleaching
text: Abstract features for cross-lingual gender pre-
diction. In ACL, Melbourne, Australia. Association
for Computational Linguistics.

Sabien Hanoulle, Véronique Hoste, and Aline Remael.
2015. The translation of documentaries: Can do-

102



mainspecific, bilingual glossaries reduce the trans-
lators workload? an experiment involving profes-
sional translators. New Voices in Translation Stud-
ies, 23:13.

Dirk Hovy. 2015. Demographic factors improve clas-
sification performance. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 752–762. Association
for Computational Linguistics.

Dirk Hovy and Anders Søgaard. 2015. Tagging perfor-
mance correlates with author age. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 2: Short Papers), pages 483–488, Beijing,
China. Association for Computational Linguistics.

Dirk Hovy and L. Shannon Spruit. 2016. The social
impact of natural language processing. In Proceed-
ings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 591–598. Association for Computa-
tional Linguistics.

John Leggett and Glen Williams. 1988. Verifying iden-
tity via keystroke characterstics. International Jour-
nal of Man-Machine Studies, 28(1):67–76.

Fei Liu, Julien Perez, and Scott Nowson. 2016. A re-
current and compositional model for personality trait
recognition from short texts. In Proceedings of the
Workshop on Computational Modeling of People’s
Opinions, Personality, and Emotions in Social Me-
dia (PEOPLES), Osaka, Japan. The COLING 2016
Organizing Committee.

Nikola Ljubešić, Darja Fišer, and Tomaž Erjavec. 2017.
Language-independent gender prediction on twitter.
In Proceedings of the Second Workshop on NLP
and Computational Social Science, pages 1–6, Van-
couver, Canada. Association for Computational Lin-
guistics.

Hilbert Locklear, Sathya Govindarajan, Zdenka Sitova,
Adam Goodkind, David Guy Brizan, Andrew
Rosenberg, Vir V Phoha, Paolo Gasti, and Kiran S
Balagani. 2014. Continuous authentication with
cognition-centric text production and revision fea-
tures. In Biometrics (IJCB), 2014 IEEE Interna-
tional Joint Conference on.

Kim Luyckx and Walter Daelemans. 2008. Personae:
a corpus for author and personality prediction from
text. In LREC 2008.

François Mairesse and Marilyn Walker. 2006. Au-
tomatic recognition of personality in conversation.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers.

Shachar Mirkin, Scott Nowson, Caroline Brun, and
Julien Perez. 2015. Motivating personality-aware
machine translation. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1102–1108, Lisbon, Portu-
gal. Association for Computational Linguistics.

John V Monaco, John C Stewart, Sung-Hyuk Cha, and
Charles C Tappert. 2013. Behavioral biometric ver-
ification of student identity in online course assess-
ment and authentication of authors in literary works.
In Biometrics: Theory, Applications and Systems
(BTAS), 2013 IEEE Sixth International Conference
on.

Frederick Mosteller and David Wallace. 1964. In-
ference and disputed authorship: The Federalist.
Addison-Wesley.

John Nerbonne. 2007. The exact analysis of text. Fore-
word to the 3rd edition of Frederick Mosteller and
David Wallace Inference and Disputed Authorship:
The Federalist Papers CSLI: Stanford.

Dong Nguyen, A. Seza Dogruz, Carolyn Penstein Ros,
and Franciska de Jong. 2015. Computational soci-
olinguistics: A survey. Compututational Linguis-
tics, 42(3):537–59.

Dong Nguyen, A. Noah Smith, and P. Carolyn Rosè.
2011. Proceedings of the 5th ACL-HLT Workshop
on Language Technology for Cultural Heritage, So-
cial Sciences, and Humanities, chapter Author Age
Prediction from Text using Linear Regression. As-
sociation for Computational Linguistics.

Guido Nottbusch, Rdiger Weingarten, and Said Sahel.
2007. From written word to written sentence pro-
duction. Writing and cognition: Research and ap-
plications., pages 31–54.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

James W Pennebaker. 2011. Using computer analyses
to identify language style and aggressive intent: The
secret life of function words. Dynamics of Asymmet-
ric Conflict, 4(2):92–102.

Barbara Plank. 2016. Keystroke dynamics as signal for
shallow syntactic parsing. In Proceedings of COL-
ING 2016, the 26th International Conference on
Computational Linguistics: Technical Papers, pages
609–619, Osaka, Japan. The COLING 2016 Orga-
nizing Committee.

Barbara Plank and Dirk Hovy. 2015. Personality traits
on twitter–or–how to get 1,500 personality tests in
a week. In Proceedings of the 6th Workshop on

103



Computational Approaches to Subjectivity, Senti-
ment and Social Media Analysis, pages 92–98, Lis-
boa, Portugal. Association for Computational Lin-
guistics.

Ella Rabinovich, Raj Nath Patel, Shachar Mirkin, Lu-
cia Specia, and Shuly Wintner. 2017. Personal-
ized machine translation: Preserving original author
traits. In Proceedings of the 15th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: Volume 1, Long Papers, pages
1074–1084, Valencia, Spain. Association for Com-
putational Linguistics.

Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the 2nd in-
ternational workshop on Search and mining user-
generated contents, pages 37–44. ACM.

Sara Rosenthal and Kathleen McKeown. 2011. Age
prediction in blogs: A study of style, content, and
online behavior in pre- and post-social media gen-
erations. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 763–
772. Association for Computational Linguistics.

Roy Schwartz, Oren Tsur, Ari Rappoport, and Moshe
Koppel. 2013. Authorship attribution of micro-
messages. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1880–1891. Association for Compu-
tational Linguistics.

Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for information Science and Technology,
60(3):538–556.

John C Stewart, John V Monaco, Sung-Hyuk Cha,
and Charles C Tappert. 2011. An investigation of
keystroke and stylometry traits for authenticating
online test takers. In Biometrics (IJCB), 2011 In-
ternational Joint Conference on.

Kirk PH Sullivan, Eva Lindgren, et al. 2006. Computer
keystroke logging and writing: Methods and appli-
cations. Elsevier.

Charles C Tappert, Sung-Hyuk Cha, Mary Villani, and
Robert S Zack. 2010. A keystroke biometric system
for long-text input. International Journal of Infor-
mation Security and Privacy (IJISP), 4:32–60.

Charles C Tappert, Mary Villani, and Sung-Hyuk Cha.
2009. Keystroke biometric identification and au-
thentication on long-text input. Behavioral biomet-
rics for human identification: Intelligent applica-
tions, pages 342–367.

Ioannis Tsimperidis, Vasilios Katos, and Nathan
Clarke. 2015. Language-independent gender iden-
tification through keystroke analysis. Information &
Computer Security, 23(3):286–301.

Luuk Van Waes, Mariëlle Leijten, and Daphne
Van Weijen. 2009. Keystroke logging in writing re-
search: Observing writing processes with inputlog.
GFL-German as a foreign language, 2(3):41–64.

Ben Verhoeven, Walter Daelemans, and Barbara Plank.
2016. Twisty: A multilingual twitter stylometry cor-
pus for gender and personality profiling. In Pro-
ceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC 2016),
Paris, France. European Language Resources Asso-
ciation (ELRA).

Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic language
variations to improve multilingual sentiment anal-
ysis in social media. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1815–1827. Association
for Computational Linguistics.

Åsa Wengelin. 2006. Examining pauses in writing:
Theory, methods and empirical data. Computer key-
stroke logging and writing: methods and applica-
tions (Studies in Writing), 18:107–130.

Brian Hu Zhang, Blake Lemoine, and Margaret
Mitchell. 2018. Mitigating unwanted bi-
ases with adversarial learning. arXiv preprint
arXiv:1801.07593.

104


