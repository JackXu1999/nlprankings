















































Toward Finding Semantic Relations not Written in a Single Sentence: An Inference Method using Auto-Discovered Rules


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 902–910,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Toward Finding Semantic Relations not Written in a Single Sentence:
An Inference Method using Auto-Discovered Rules

Masaaki Tsuchida†§∗ Kentaro Torisawa‡ Stijn De Saeger ‡
Jong-Hoon Oh‡ Jun’ichi Kazama‡ Chikara Hashimoto‡ Hayato Ohwada§
† Information and Media Processing Laboratories, NEC Corporation, Nara, Japan

m-tsuchida@cq.jp.nec.com
‡ Information Analysis Laboratory, National Institute of

Information and Communications Technology, Kyoto, Japan
{torisawa,stijn,rovellia,kazama,ch}@nict.go.jp

§ Graduate school of Science and Technology, Tokyo University of Science, Chiba, Japan.
ohwada@ia.noda.tus.ac.jp

Abstract

Recent advances in automatic knowledge
acquisition methods have enabled us to
construct massive knowledge bases of se-
mantic relations. Most previous work has
focused on semantic relations explicitly
expressed in single sentences. Our goal
in this work is to obtain valid non-single
sentence relation instances, which are not
written in any single sentence and may not
be even written in a large corpus. We de-
velop a method to infer new semantic rela-
tion instances by applying auto-discovered
inference rules, and show that our method
inferred a considerable number of valid in-
stances that were not written in single sen-
tences even in 600 million Web pages.

1 Introduction

Recent advances in automatic relation acquisi-
tion methods (Agichtein and Luis, 2001; Et-
zioni et al., 2004; Pantel and Pennacchiotti,
2006; Paşca et al., 2006; Banko and Etzioni,
2008; De Saeger et al., 2009) have opened
the way to build large knowledge bases con-
taining a huge number of semantic relation in-
stances such as “CAUSE(allergen, allergy)” and
“PREVENTION(coffee, drowsiness)”. Such a mas-
sive knowledge base is valuable for applications
like innovation and risk management (Torisawa et
al., 2010), like finding potential and unexpected
causes of a disease, unknown side-effects of a
drug, and so on. In such tasks overlooking a small
piece of information can have grave consequences.

In this work, our goal is to acquire Non-single
Sentence relation instances (NS instances), which

∗This work was done when the author was at the National
Institute of Information and Communications Technology.

are not written in any single sentences, in order
to reduce the possibility of overlooking valuable
information. More precisely, if the two nouns
in an instance (e.g., “allergen” and “allergy” in
“CAUSE(allergen, allergy)”) do not appear in any
single sentences in a given corpus together with
the other clues indicating the relation type (e.g.,
the verb “cause”), the instance is an NS instance.
NS instances may be indirectly written in a se-
quence of several sentences using anaphora or may
not be written at all even in a given corpus. In
the following, we call the complement of NS in-
stances Single Sentence relation instances (SS in-
stances), which consist of two nouns that co-occur
in some single sentences in a given corpus together
with some evidence for a relation type.

Most existing relation acquisition methods ac-
quire relation instances using lexico-syntactic pat-
terns (Pantel and Pennacchiotti, 2006; Paşca et al.,
2006; De Saeger et al., 2009) such as “X causes Y”
or probabilistic sequence labeling models (Banko
and Etzioni, 2008). These methods basically rely
on the structure of single sentences and they are
for acquiring SS instances. Thus we consider that
NS instances are practically beyond their reach. A
few attempts to overcome this limitation include
inference-based methods. These methods take SS
instances provided by other methods as input and
infer relation instances including NS ones using
auto-discovered inference rules (Schoenmackers
et al., 2010; Carlson et al., 2010) or distributional
similarities (Tsuchida et al., 2010). We consider
such inference approaches to be promising for ac-
quiring NS instances.

This paper proposes an inference-based method
for acquiring NS instances. We start from a set
of seed relation instances of a target relation,
which are acquired by an existing semi-supervised

902



pattern-based method (De Saeger et al., 2009), and
induce a large number of recursive inference rules
to infer new relation instances. The instances in-
ferred by the rules are ranked based on the scores
assigned to the rules and the top instances are pro-
duced as final output.

Specifically, our method infers new instances of
particular target relations by applying the follow-
ing type of recursive rules to large corpora.

“X pattern Y” ∧ RSEED(Y,Z)→ RHYPO(X,Z)
Here, “X pattern Y” indicates that X and Y co-
occur in a certain lexico-syntactic pattern, RSEED
(Y,Z) is one of the seed relation instances, and an
inferred instance of the target relation is denoted
by RHYPO (X,Z). Though we distinguish RSEED (Y,Z)
and RHYPO (X,Z) by the subscripts “SEED” and
“HYPO”, they are supposed to be the same target
relation, and thus the rules are recursive. Note that
this type of recursive rules could not be employed
in an existing inference-based method (Schoen-
mackers et al., 2010) as described in Section 4.1.

For acquiring NS instances, rules combine pat-
terns and relation instances (“X pattern Y” and
RSEED (Y,Z)) scattered throughout many distinct
documents. The following is an example rule
learned by our method.

”X is rich in Y” ∧ PREVENTIONSEED(Y,Z)
→ PREVENTIONHYPO(X,Z)

This can be interpreted as: “If X is rich in Y
and Y prevents Z, then X prevents Z.”, and seems
valid in many cases. An interesting example
is that the rule generated the relation instance
“PREVENTION(X=blue-backed fish, Z=cerebral
thrombosis)“ where “Y=eicosapentaenoic acid”.
This is a rather well-known fact in Japan these
days, and you can find many Web pages stating
this fact in early 2011. However, the words “blue-
backed fish” and “cerebral thrombosis” do not co-
occur in any four sentence window in the 600M
Web page corpus used in our experiments, which
was crawled in 2008. Thus, “PREVENTION(blue-
backed fish, cerebral thrombosis)“ is an NS in-
stance and exemplifies the importance and neces-
sity of inferring NS instances.

This example also introduces the idea underly-
ing our evaluation scheme. Proper evaluation of
new relation hypotheses that are not mentioned to-
gether in the extraction corpus would require ver-
ification by domain experts, which is clearly be-
yond our resources. We therefore evaluate new re-
lation hypotheses using a larger and newer corpus

(i.e. a commercial search engine), under the naive
assumption that correct instances will be found
in this bigger corpus. We realize this evaluation
scheme cannot do justice to genuinely unknown
instances but only gives a lowerbound of the true
precision.

Through a series of experiments for acquir-
ing three semantic relations, causality, prevention,
and material, from a 600 million page Japanese
Web corpus, we show that our method can in-
fer valid NS instances. We also compare our
method to the markov logic inference algorithm
introduced in Schoenmackers et al. (2010) and
show that our procedure, while considerably sim-
pler, outperforms it.

2 Related Work

Previous work can be categorized into three
groups: 1) methods for extracting SS in-
stances (Etzioni et al., 2004; Pantel and Pennac-
chiotti, 2006; Paşca et al., 2006; Banko and Et-
zioni, 2008; De Saeger et al., 2009), 2) meth-
ods for inferring relation instances (Carlson et al.,
2010; Schoenmackers et al., 2010; Tsuchida et al.,
2010), and 3) work in bioinformatics aiming at
helping users to discover unseen relation instances
as novel knowledge (Swanson, 1986; Srinivasan,
2004; Hu et al., 2006; Hristovski et al., 2008).

The methods in the first category aim at ex-
tracting a large number of instances by employ-
ing automatically induced paraphrases of lexico-
syntactic patterns or probabilistic sequence la-
beling methods. In this category, we focus on
De Saeger et al. (2009), which is the seed in-
stance extractor employed in this work. This
takes seed patterns, such as “X causes Y”, as in-
put and learns a large amount of paraphrase pat-
terns to extract relation instances from a corpus.
Unlike other pattern-based methods, De Saeger
et al. (2009) learns class dependent paraphrase
patterns, which place semantic word class re-
strictions on the noun pairs they may extract,
like “X:chemical causes Y:disease”. These
class restrictions enable to distinguish between
multiple senses of frequent but highly ambigu-
ous patterns. For instance, given a class inde-
pendent pattern “X causes Y” as seed, if we re-
strict X and Y in “Y from X” to the classes of
chemicals and diseases (as in “cancer from cad-
mium”), the class dependent pattern “Y:disease
from X:chemical” becomes a valid paraphrase
of “X causes Y”. Note that, other class restric-

903



tions of the same pattern (e.g., “Y:products
from X:company”, as in “iPhone from Apple”)
may not yield a valid paraphrase of “X causes
Y”. To obtain word classes they use a large-scale
word clustering algorithm (Kazama and Torisawa,
2008), and rank each instance in the corpus ac-
cording to a score based on the semantic similarity
between the seed patterns and each class depen-
dent pattern the instance co-occurs with.

Although much work in this category success-
fully extracts the instances implicitly written in a
single sentence based on a wide range of non-
trivial evidence including paraphrases (e.g., “Y by
X” for causality), the applicability of these meth-
ods is restricted to SS instances.

In the second category, Schoenmackers et al.
(2010), which is the most relevant to our work,
takes relation instances provided by TextRun-
ner (Banko and Etzioni, 2008) as input and in-
duces Horn clauses as inference rules. The
weights of the rules and the probabilities of the
hypothesized instances are estimated by a markov
logic network (Richardson and Domingo, 2006;
Huynh and Mooney, 2008). They also proposed
a weighted counting method for discounting the
effects of uncertain (or infrequent) instances, and
a method for discounting weights of longer rules
by strong Gaussian prior.

The goal of Schoenmackers et al. (2010) was
to acquire implicit relation instances. Note that
their “implicit” instances cover what we call SS
and NS instances, while our target are only NS
instances. For instance, they regarded the in-
stances acquired by simple paraphrase rules such
as “CAN CAUSE(X,Y) → CAUSE(X,Y)” as im-
plicit instances. For “CAUSE(a,b)” to be inferred
by this rule, “CAN CAUSE(a,b)” must be written in
single sentences so that TextRunner can recognize
it. This means that such instances are SS instances.

Actually their algorithm was tuned to prefer SS
instances and around 70% of the acquired valid
instances were in this category. Certainly, their
method uses more complex rules that can infer
NS instances, but the precision of the instances in-
ferred by such non-paraphrase rules is quite low
(around 20%) 1. Also they have not empirically
examined how many NS instances are actually

1They acquired a total 2.6M instances with 50% preci-
sion for a variety of relation types. Also, about 1.25M SS
instances in those were inferred by simple paraphrase rules
with 80% precision. The precision of the instances inferred
by non-paraphrase rules can be estimated as the solution for
0.50 = 1.25×0.80+(2.6−1.25)×x

2.6
as x (= 0.22).

found in their output.
In contrast, we focus on more complex rules

that can infer NS instances. An important point
is that their method cannot deal with considerable
parts of our complex rules because their inference
algorithm for markov logic network (Huynh and
Mooney, 2008) poses some restrictions on recur-
sive rules as discussed in Section 4.1. We also em-
pirically show that their scoring mechanism does
not lead to higher precision at least in our setting,
although we have not checked whether this is due
to the restrictions on recursive rules or is due to
some other reasons.

Another work in this category, Carlson et
al. (2010) hypothesized instances using Horn
clauses discovered by Inductive Logic Pro-
gramming (Quinlan and Cameron-Jones, 1993).
Tsuchida et al. (2010) generates hypothesized re-
lation instances by substituting words in seed in-
stances with distributionally similar words.

In bioinformatics, there are attempts to develop
methods to help discoveries of relation instances
by linking clues from different literatures (Swan-
son, 1986; Srinivasan, 2004; Hu et al., 2006; Hris-
tovski et al., 2008). These methods cannot be
easily adapted to other domains, because they re-
quire heavily engineered resources like databases
of MEDLINE records and hand-annotation with
MeSH 2 metadata. These also require some input
and/or interactions to capture the interests of the
human expert. Our aim is to infer unseen NS in-
stances without such resources and human effort.

Recently, De Saeger et al. (2011) have shown
that it is possible to acquire SS instances from
highly complex and infrequent expressions, us-
ing word classes and lexico-syntactic pattern frag-
ments, which they call partial patterns. Such
approach may prove useful for acquiring NS in-
stances too, as the method can acquire relation in-
stances without considering any pattern connect-
ing the two words of the instance. Yet their work
focused only on SS instances.

3 Proposed Method

Our method takes a set of seed relation instances
and a large corpus as input. The seed instances
are obtained using De Saeger et al. (2009), after
some rudimentary cleaning (Section 4). Then, our
method induces possible inference rules, and as-
signs scores to the instances inferred by the rules.
The high-ranked instances are provided as output.

2 http://www.nlm.nih.gov/mesh/

904



In the following, we describe the details of each
step. Note that the algorithm presented here can
produce SS instances as well as NS instances. In
the evaluation, potential SS instances are excluded
from the output by checking the co-occurrence of
the word pairs in the instances in single sentences.

3.1 Inducing Inference Rules
We consider inference rules (a special case of
Horn-clauses) that have the following form:

“X pattern Y” ∧ RSEED(Y,Z)→ RHYPO(X,Z)

This rule hypothesizes relation instances by sub-
stituting the first argument Y of a seed instance
with X where the connection between X and Y
is provided by “X pattern Y”. 3 To handle multi-
word expressions such as “red wine”, we allow NP
chunks to fill the slots of the X, Y or Z variables.
Our motivation here is to have a more exhaustive
set of relation instances for a given relation type.
As a starting point we focus on the simple rules
as the one above, although a wide range of other
forms are possible. Sharing a variable (i.e. Y)
guarantees that at least some relationship between
X and Z holds and reduces the risk of generating
meaningless rules, while the arbitrariness of the
pattern allows us to consider a broad range of evi-
dence to find new instances.

For inducing inference rules, first we find
two seed instances that have the same sec-
ond argument. Suppose that the target relation
is causality, and we have two seed instances,
CAUSE(bronchitis, cough) and CAUSE(mold,
cough), with common second argument “cough”.
Then, we can search for “X pattern Y”, which in-
forms us of a certain relation between the first ar-
guments of the two seed instances, bronchitis and
mold in this example. We may be able to find “X
causes Y” from the expression “.. mold causes
bronchitis ..” and induce the following rule that in-
fers one seed instance from another seed instance.

”mold causes bronchitis” ∧ CAUSESEED(bronchitis,cough)
→ CAUSEHYPO(mold, cough)

By generalizing mold, bronchitis, and cough to
variables X , Y and Z, we can obtain the following
acceptable rule that can be interpreted as “if X is
a cause of a cause (Y) of Z, X is a cause of Z”.

“X causes Y” ∧ CAUSESEED(Y,Z) → CAUSEHYPO(X,Z)
3We also consider the rules where Z appears as the first

arguments, i.e., “X pattern Y” ∧ RSEED(Z,Y) → RHYPO(Z,X).

Here, by assuming that CAUSEHYPO (bronchitis,
cough) is inferred from CAUSESEED (mold, cough),
we also obtain the following unacceptable rule,
which contradicts with common sense.

“Y causes X” ∧ CAUSESEED(Y,Z) → CAUSEHYPO(X,Z)

We can say that the former is a swapped rule of
the latter and vice-versa. The above rule can be in-
terpreted as “If Y is a cause of X and Z, then X is
a cause of Z”, which is nonsense. In Section 3.2.1,
we introduce a heuristic to remove such unaccept-
able swapped rules.

To alleviate pattern ambiguity, all the patterns
in the rules are class dependent patterns (Section
2). The induced rules are augmented with class
restrictions on the variables in the pattern:

“X:virus causes Y:sickness” ∧ CAUSESEED(Y,Z)
→ CAUSEHYPO(X,Z)

As word classes for the class restrictions, we used
a word clustering result obtained by applying a
probabilistic word clustering algorithm (Kazama
and Torisawa, 2008) to dependency relations ex-
tracted from 100M Web pages. The results are
represented by the probability distribution P (c|w)
where w is a word (the number of words is 1M in
this paper) and c is a class identifier, which is a
hidden variable in a predefined set C.

To have a discrete boundary of class member-
ship, we assume w belongs to class c if and only if
P (c|n) ≥ 0.2 or c = arg max

c∈C
P (c|n). We set |C|

to 500, following De Saeger et al. (2009). Also,
the classes are just class identifiers (i.e., integers).
To simplify the explanation, we omit these class
restrictions in the rest of our paper.

Finally, to discard unproductive rules, all the
rules that cannot generate more than M seed re-
lation instances using the other seed instances are
discarded. In this work, we set M to 10.

3.2 Scoring Hypothesized Instances
After inducing the rules, we hypothesize relation
instances by applying the rules to an input cor-
pus, and assign scores to the instances. Instance
score for a hypothesized instance is defined as the
sum of the scores of the rules that generated the in-
stance. Rule score r score(r) is an approximated
precision of the instances inferred by the rule, as-
suming that the seed instances are correct relations
and the others are not.

r score(r) =
# of seeds in hypotheses by r

# of hypotheses by r

905



Here, r is an inference rule.
Instance score h score is defined as the sum of

the applicable rule scores.

h score(h) =
∑

r∈Irules(h,Seeds,Rules)
r score(r)

Here, h is a hypothesized instance, and Rules is
a set of inference rules. Seeds is a set of seed
relation instances, and Irules is a set of rules in
Rules that infer h from Seeds.

We use the following additional heuristics dur-
ing this ranking scheme.

3.2.1 Removing unacceptable swapped rules
The first heuristic is introduced for removing un-
acceptable swapped rules. Recall that our infer-
ence rule always has its swapped rule like the fol-
lowing two example rules.

A “X causes Y” ∧ CAUSESEED (Y, Z) → CAUSEHYPO (X, Z)
B “Y causes X” ∧ CAUSESEED (Y, Z) → CAUSEHYPO (X, Z)

Here, we have three observations: 1) if one rule
is acceptable, its swapped rule is not (e.g., rule
A is acceptable but rule B is not), 2) if a rule
often generates reflexive relation instances, i.e.,
RHYPO (x,x) for some word “x”, the rule is likely
to be unacceptable, and 3) the swapped rule of
such an unacceptable rule (like rule B) often rep-
resents a transitive relation. For example, above
rule A represents a transitive relation for causality
(X → Y → Z then X → Z). On the other hand,
the seed instance and the pattern in rule B rep-
resent that the same cause Y results in X and Z.
If the seed instance and the pattern represents the
same causal relation instance, CAUSEHYPO (X,Z)
becomes CAUSEHYPO (x,x).

We found that rules generating many RHYPO (x,x)
attain a high score in our rule scoring. To rem-
edy this we set r score(r) to 0 if r generates more
RHYPO (x,x) than its swapped version. If the two
rules do not generate RHYPO (x,x) or the number of
RHYPO (x,x) by the two rules are the same, this is
not applied. We refer to this as (X,X)-based rule
filtering hereafter.

3.2.2 Excluding highly vague words
Our inference rules are based on pivot words,
i.e., the shared variable Y in “X pattern Y ∧
RSEED(Y,Z) → RHYPO(X,Z)”. If the pivot is
so vague that it likely refers to different ob-
jects in the seed and the pattern, the inference
would be wrong (Schoenmackers et al., 2010).

For example, we can generate false hypothe-
sis CAUSEHYPO (cerebral stroke, pneumonia) from
“cerebral stroke causes disease” and CAUSESEED
(disease, pneumonia). Here “disease” is highly
vague, and likely refers to different types of dis-
eases in different texts.

To address this problem we prepared a stop
word list and discarded relation instances that con-
tain one of these stop words. We calculate the
document frequency of each word in the 600M
page Web corpus, and regard a word whose doc-
ument frequency is higher than threshold T as a
stop word. We set T to 400,000 and obtain about
15,000 stop words in our experiments.

4 Evaluation

Our evaluation focuses on three main questions: 1)
How accurate are the NS instances hypothesized
by our method? 2) How accurate are all the in-
stances hypothesized by our method? 3) Does our
method infer relation instances with higher preci-
sion than competing methods? After explaining
the experimental setting we answer each question
from our experimental results. We also analyze the
cause of errors at the end of this section.

4.1 Experimental Setting
We evaluated our results on the three relations:

Causality(X,Y): X can directly or indirectly
cause Y. e.g., CAUSE(allergen, allergy).

Prevention(X,Y): X can directly or indirectly
help to avoid the occurrence of Y.
e.g., PREVENTION(coffee, drowsiness).

Material(X,Y) X is a material or ingredient of Y.
e.g., MATERIAL(grape, wine).

We employed the evaluation scheme introduced
in Tsuchida et al. (2010). To evaluate the cor-
rectness of a given hypothesized instance, we pre-
sented three human judges with short texts as pos-
sible evidence. For each hypothesized instance,
we collected up to 20 short texts from the search
results provided by Yahoo! API 4 for a query
consisting of the two nouns and a priming term
for each target relation, i.e., the Japanese word
for “cause”, “material” or “prevention”. All texts
were collected from February to the middle of
March in 2011.

The expectation behind this scheme is that
many of correct NS instances in a smaller corpus,

4
http://developer.yahoo.co.jp/webapi/search/

906



i.e., the 600M page Web corpus, can be found with
explicit evidence in a larger corpus, i.e., pages ac-
cessible through a commercial search engine.

In our evaluation scheme, our three annotators
mark a hypothesized instance as correct if they
find “sufficient evidence” (see below) in at least
one of the presented text snippets. We say a text
snippet contains “sufficient evidence” if it either
explicitly asserts the target relation between the
word pair, or implicitly presupposes it. A hypothe-
sized instance is judged incorrect when 1) the pro-
vided texts do not present sufficient evidence, or 2)
a relation instance is not informative enough with-
out further context (e.g., CAUSE(insulin, change),
we don’t know what change can be caused by in-
sulin without further context). Correctness of a hy-
pothesized instance is determined by the judges’
majority vote. The inter-rater agreement (Fleiss’
kappa) was 0.57 for causality, 0.56 for prevention
and 0.57 for material, indicating the judgements
are reasonably stable.

For each relation, the seed instances were the
top 20,000 instances given by our implementa-
tion of De Saeger et al. (2009) with the class-
based cleaning that took about 15 minutes. More
precisely, we discarded inappropriate instances
by manually identifying and removing semantic
classes that are inappropriate for the target rela-
tion. The precision of the seed instances was 81%
for causality , 78% for material and 44% for pre-
vention. We used about 25 seed patterns for each
relation, which were created by one of the authors
with one hour tuning. We show some examples of
the seed patterns (translated from Japanese).

Causality(X,Y) ”X causes Y”, “Y caused by X”, “X that
causes Y”, ...

Prevention(X,Y) ”X prevents Y”, “Y prevented by X”, “X
that prevents Y”, ...

Material(X,Y) ”X is material of Y”, “Y made from X”, “X
that is material of Y”, ...

From the seed instances, our method induced
24,044 rules for causality, 17,868 for prevention
and 14,978 for material, and obtained 3.04M hy-
pothesized instances for causality, 2.44M for pre-
vention and 2.17M for material. These hypothe-
sized instances do not include the seed instances.
Table 1 shows some hypothesized instances.

In our experiments we compared three systems.

SUM: Proposed method.
MLN: Markov logic network based scoring method of

Schoenmackers et al. (2010).
MLN(X,X): MLN with the weight of the rules removed by

(X,X)-based rule filtering set to 0.

[Rule conversion]

[Instance conversion]

[Additional data generation]

“X:c1 pattern Y:c2”∧Rseed(Y,Z)→ Rhypo(X,Z)

For each c3 ∈ Class(Rseed(c2,*))

pattern(X:c1,Y:c2)∧R(Y:c2,Z:c3) → R(X:c1,Z:c3)

Rseed(x:c1,y:c2)         R(x:c1,y:c2)

“x:c1 pattern y:c2” in corpus        pattern(x:c1,y:c2)

Get PATTERNs that is a set of the patterns used by PAT for 

extracting seed instances in Seeds for the target relation R.

For each pattern in PATTERNs:

“x:c1 pattern y:c2”  in corpus        

sentence(x:c1,y:c2, sentence_id)

sentence(X:c1,Y:c2,S) → R(X:c1,Y:c2)  with weight 1.0.

For weighting rules and inferring instances

For inferring instances

Fact:

Rule:

Figure 1: Data generation for MLN. X, Y and Z
are variables, x, y and z are nouns. c1, c2 and c3
are noun classes, and Class(RSEED (c, ∗)) is a set of
classes, ci, for which some RSEED (y:c, z:ci) exists.
“x:c” indicates that x belongs to class c, and “X:c”
means that X restricts possible nouns by class c.

We do not compare our method to Carlson et al.
(2010), since Schoenmackers et al. (2010) already
showed that MLN outperforms that method.

In preparing MLN, we converted our rules to
the format used in Schoenmackers et al. (2010)
as shown in Figure 1. Unlike our method, all
rule variables in their work have class (or hyper-
nym) restrictions, and predicates with different ar-
gument classes are treated as different. Truly re-
cursive rules that have the same predicate with the
same class restrictions in both head and body were
removed due to limitations in their inference al-
gorithm.5 We removed 10,944 rules for causal-
ity (46% of all rules), 8,614 (48%) for prevention
and 7,074 (47%) for material, respectively. When
inferring instances, we also generated the addi-
tional rules and facts (lower half of Figure 1) that
reflect their assumption that an instance frequently
extracted from single sentences is likely to be true.
Also, we empirically set the standard deviations of
the Gaussian prior for learning rule weights and
the prior weight of all unknown facts for inference
to 2.0 and -3.0. We did not use the variable Gaus-
sian prior to discount the weights of longer rules,
because the lengths of all our rules are the same.

4.2 Evaluation Results
For SUM, MLN and MLN(X,X), we investigated
the precision of the top 10,000 NS instances —

5 The freely available inference system for markov logic
Alchemy (http://alchemy.cs.washington.edu/) allows re-
cursive rules, but turned out to be too slow for practical use.
In our experiments we gave up on it after waiting more than
a week for the program to finish.

907



Table 1: Examples of evaluated hypothesized instances and the rules that inferred the instances (trans-
lated from Japanese). The meaning of ”N4S” is described in Section 4.2. All the instances marked
with“NS”, “N4S” and “SS” were judged as correct. “*UR” marks incorrect instances generated by un-
acceptable rules. “*AR” denotes incorrect instances generated by acceptable rules. “*NE” denotes
instances having no evidence in the provided texts, though we expected that the instances may be correct
based on their original rules and seed instances. For simplicity, we omit the class restrictions of rules.

Label Inferred instance Samples of rules that generated the left hypothesized instance.

C
au

sa
lit

y

N4S CAUSEHYPO (Z=SO2 gas, X=allergy symptoms), CAUSEHYPO (Z,X)← X is caused by Y ∧ CAUSESEED (Z,Y)
Y= asthma CAUSEHYPO (Z,X)← X gets worse by Y ∧ CAUSESEED (Z,Y)

NS CAUSEHYPO (X=reactive oxygen species, Z=aneurysm), CAUSEHYPO (X,Z)← X’s increase causes Y ∧ CAUSESEED (Y,Z)
Y=high blood pressure CAUSEHYPO (X,Z)← X is a cause of Y ∧ CAUSESEED (Y,Z)

SS CAUSEHYPO (X=pesticide, Z=colorectal cancer), CAUSEHYPO (X,Z)← Y is contained in X ∧ CAUSESEED (Y,Z)
Y=harmful substance CAUSEHYPO (X,Z)← X is called Y ∧ CAUSESEED (Y,Z)

*UR CAUSEHYPO (X=bilirubin, Z=colorectal cancer) CAUSEHYPO (X,Z)← X is contained in Y ∧ CAUSESEED (Y,Z)
Y=bile CAUSEHYPO (X,Z)← X is excreted in Y ∧ CAUSESEED (Y,Z)

*AR CAUSEHYPO (Z=potato crisp, X=atherosclerosis) CAUSEHYPO (Z,X)← Y is a cause of X ∧ CAUSESEED (Z,Y)
Y=lifestyle diseases CAUSEHYPO (Z,X)← X caused by Y ∧ CAUSESEED (Z,Y)

*AR CAUSEHYPO (X=tobacco, Z=food poisoning) CAUSEHYPO (X,Z)← Y contained in X ∧ CAUSESEED (Y,Z)
Y=harmful component CAUSEHYPO (X,Z)← X contains Y ∧ CAUSESEED (Y,Z)

*NE CAUSEHYPO (Z=magnesium chloride, X=atherosclerosis) CAUSEHYPO (Z,X)← X gets worse by Y ∧ CAUSESEED (Z,Y)
Y=high blood pressure CAUSEHYPO (Z,X)← Y triggers X ∧ CAUSESEED (Z,Y)

Pr
ev

en
tio

n

N4S PREVENTIONHYPO (X=blue-backed fish, Z=cerebral thrombosis), PREVENTIONHYPO (X,Z)← Y is extracted from X ∧ PREVENTIONSEED (Y,Z)
Y=eicosapentaenoic acid PREVENTIONHYPO (X,Z)← X is rich in Y ∧ PREVENTIONSEED (Y,Z)

NS PREVENTIONHYPO (Z=sunflower oil,X=heart disease), PREVENTIONHYPO (Z,X)← X is caused by Y ∧ PREVENTIONSEED (Z,Y)
Y=high blood pressure, Y1=linoleic acid PREVENTIONHYPO (Z,X)← Y1 is contained in Z ∧ PREVENTIONSEED (Y1,X)

SS PREVENTIONHYPO (X=sesame lignan, Z=cerebral stroke), PREVENTIONHYPO (X,Z)← X having Y ∧ PREVENTIONSEED (Y,Z)
Y=antioxidant effects,Y1=high blood pressure PREVENTIONHYPO (X,Z)← Y1 gives rise to Z ∧ PREVENTIONSEED (X,Y1)

*UR PREVENTIONHYPO (Z=niacin, X=kidney disease), PREVENTIONHYPO (Z,X)← Y is accompanied by X ∧ PREVENTIONSEED (Z,Y)
Y=high blood pressure PREVENTIONHYPO (Z,X)← X is a cause of Y ∧ PREVENTIONSEED (Z,Y)

*NE PREVENTIONHYPO (Z=egg, X=dizziness), PREVENTIONHYPO (Z,X)← X is caused by Y ∧ PREVENTIONSEED (Z,Y)
Y=anemia, Y1=iron PREVENTIONHYPO (Z,X)← Z contains Y1 ∧ PREVENTIONSEED (Y1,X)

M
at

er
ia

l

N4S MATERIALHYPO (X=red grape, Z=cuvee), MATERIALHYPO (X,Z)← X such as Y ∧ MATERIALSEED (Y,Z)
Y=pinot noir MATERIALHYPO (X,Z)← X including Y ∧ MATERIALSEED (Y,Z)

NS MATERIALHYPO (Z=sugar beet, X=hydrogen), MATERIALHYPO (Z,X)← X is extracted from Y ∧ MATERIALSEED (Z,Y)
Y=ethanol MATERIALHYPO (Z,X)← Y is converted into X ∧ MATERIALSEED (Z,Y)

SS MATERIALHYPO (Z=corn, X=ethylene), MATERIALHYPO (Z,X)← X is made from Y ∧ MATERIALSEED (Z,Y)
Y=ethanol MATERIALHYPO (Z,X)← Y is material of X ∧ MATERIALSEED (Z,Y)

*UR MATERIALHYPO (Z=blueberry, X= plain yogurt), MATERIALHYPO (Z,X)← Y is mixed in X ∧ MATERIALSEED (Z,Y)
Y=blueberry jelly MATERIALHYPO (Z,X)← put Y in X ∧ MATERIALSEED (Z,Y)

*AR MATERIALHYPO (X=sugarcane, Z= zero-emisions vehicle), MATERIALHYPO (X,Z)← Y extracted from X ∧ MATERIALSEED (Y,Z)
Y=ethanol MATERIALHYPO (X,Z)← Y made from X ∧ MATERIALSEED (Y,Z)

Table 2: Results from the binomial one-tailed test
between SUM and the other methods. The signif-
icance level is 0.05. For each relation, cells show
the number of data points (8 max) in the precision
graph (like Figure 3) where “SUM wins / SUM
loses / no significant difference”.

MLN(X,X) MLN MAX No (X,X)
filtering

No rule score

Causality 0 / 0 / 8 6 / 0 / 2 8 / 0 / 0 6 / 0 / 2 3 / 0 / 5
Prevention 8 / 0 / 0 7 / 0 / 1 6 / 0 / 2 0 / 0 / 8 0 / 0 / 8
Material 7 / 0 / 1 7 / 0 / 1 1 / 0 / 7 8 / 0 / 0 2 / 0 / 6

word pairs that do not co-occur in any single sen-
tence — using 100 random samples. The preci-
sion curves in Figure 2 show that SUM outper-
forms both MLN and MLN(X,X). Although the
precision of the top 10,000 NS instances obtained
by SUM is relatively low (20% to 30%), we do
think this is a promising result given the difficulty
of the task. In tasks such as innovation support and
risk management, one must explore the border be-
tween the known and unknown. We expect even
hypothesized instances with 20 to 30% precision
can be useful in such contexts.

0.00

0.10

0.20

0.30

0.40

0.50

0.60

 2000  4000  6000  8000  10000

P
re

c
is

io
n

Samples Ranked by Score

SUM(Causation)

MLN(X,X)(Causation)

MLN(Causation)

SUM(Prevention)

MLN(X,X)(Prevention)

MLN(Prevention)

SUM(Material)

MLN(X,X)(Material)

MLN(Material)

Figure 2: Precision of the top 10,000 NS instances.

Next, we would like to address the question of
how many acquired NS instances are not written
in our corpus at all. Answering this question pre-
cisely is difficult because of anaphora and ellipsis.
Therefore we assume that if the two nouns of an
NS instance do not co-occur in any four sentence
window (“N4S”) in the corpus, the instance is un-
likely to be mentioned explicitly in any form in the
corpus. For causality, 44 of the 100 evaluated sam-
ples were N4S instances, 8 of which were correct

908



0.20

0.30

0.40

0.50

0.60

0.70

0.80

 2500  5000  7500  10000  12500  15000  17500  20000

P
re

c
is

io
n

Samples Ranked by Score

SUM

MLN

MLN(X,X)

No <X,X> filtering

No rule score

MAX

Figure 3: Causality: top 20,000 results’ precision

(18% precision). The evaluated prevention and
material samples respectively contained 22 and 35
N4S instances, with 2 and 8 correct ones (9% and
23% precision). We expect that at least some of
these instances are not mentioned in our corpus.

To confirm the superiority of the proposed
method, we investigated the precision of the top
20,000 instances of each method without remov-
ing SS instances from the evaluation data using
200 random samples. Our method showed 59%
for causality (Fig. 3), 46% for prevention and 43%
for material of the top 20,000 in precision, which
is considerably higher than the precision of the NS
samples only.

In addition to MLN and MLN(X,X) we also
evaluated the following three baseline methods to
confirm that all the design choices effectively con-
tribute to the performance of our method.

No (X,X) filtering: SUM without (X,X)-based filtering.
No rule score: SUM, where all rule scores are constant.
MAX: A variant of the instance scoring function,

max
r∈Irules(h,Seeds,Rules)

r score(r).

Figure 3 shows the precision curves for causal-
ity. In all relations SUM outperforms MLN,
MLN(X,X) and all baseline methods. Table 2
shows the results of the binomial one-tailed test
between SUM and each compared method, and
suggests that SUM had statistically significant im-
provements in many cases. SUM showed only mi-
nor improvements compared with “No rule score”.
This suggests that the instances inferred by many
rules tend to be correct, irrespective of the rule
scores. We think this is due to the overall qual-
ity of the rules, as this would not be the case
if many rules were invalid. Table 2 also shows
that for transitive relations (causality and mate-
rial), (X,X)-based rule filtering is effective.

4.3 Error Analysis
Table 1 shows some instances hypothesized by
the proposed method. We distinguish between in-
correct instances generated by unacceptable rules
(marked “*UR” in Table 1) and others. We
found that about 60% of incorrect instances are
generated by unacceptable rules. For example,
“CAUSEHYPO (X=bilirubin, Z=colorectal cancer)“
was generated by the rule “CAUSEHYPO (X,Z)← X
is contained in Y ∧ CAUSESEED (Y,Z)“.

Incorrect instances generated by seemingly cor-
rect rules are marked with “*AR” in Table 1.
These can further be categorized into three types
(examples of each are in Table 1).

Type A: Uninformative instances for our evaluation criteria
(See Section 4.1).
e.g., CAUSEHYPO (Z=potato crisps, X=atherosclerosis).
The validity of this instance depends on context, i.e.,
the amount of potato crisps that one eats.

Type B: Instances generated with vague pivot words (See
Section 3.2.2).
e.g., CAUSEHYPO (X=tobacco, Z=food poisoning),
Y=harmful component. harmful component is vague.

Type C: Instances generated from incorrect seed instances.
e.g., MATERIALHYPO (X=sugarcane, Z=zero-emissions
vehicle), derived from the incorrect seed instance
MATERIALSEED (Y=ethanol, Z=zero-emisions vehicle).

The ratio was roughly 10% for type A, 5% for
type B and 5% for type C for all the relation types.
For the remaining incorrect instances (about 20%),
the judges could not find sufficient evidence in
the presented text snippets, although some such
instances do appear to be valid. Such instances
are marked “*NE”. This suggests our evaluation
scheme may be underestimating the true precision.

5 Conclusion

This paper introduced an inference-based method
that takes a set of seed relation instances as in-
put, and outputs hypothesized instances using in-
ference rules induced from these seed instances.

We showed that our method can infer valid rela-
tion instances whose component nouns do not co-
occur in any single sentence or any four sentence
window even in a 600M page Web corpus. We
expect this result is promising for inferring new
knowledge, because such instances may contain
instances not mentioned in the context of that par-
ticular semantic relation even in 600M Web pages.

Acknowledgments

The authors thank Dr. Tuyen N. Huynh of SRI In-
ternational, who kindly provided the source code
of Huynh and Mooney (2008) for our experiments.

909



References
Eugene Agichtein and Gravano Luis. 2001. Snowball:

Extracting Relations from Large Plain-Text Collec-
tions. In Proc. of the 5th ICDL, pages 85–94.

Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proc. of the 46th ACL-08:HLT, pages 28–36.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for nev-
erending language learning. In Proc of the 24th
AAAI, pages 1306–1313.

Stijn De Saeger, Kentaro Torisawa, Jun’ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
Scale Relation Acquisition Using Class Dependent
Patterns. In Proc. of the 9th ICDM, pages 764–769.

Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,
Junfichi Kazama, Chikara Hashimoto, Ichiro Ya-
mada, Jong-Hoon Oh, Istvan Varga, and Yulan Yan.
2011. Relation Acquisition using Word Classes and
Partial Patterns. In Proc. of the EMNLP2011, pages
825–835.

Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-Scale Information Extraction in Know-
itall (Preliminary Results). In Proc. of the 13th
WWW, pages 100–110.

D. Hristovski, C. Friedman, T. C. Rindflesch, and
B. Peterlin. 2008. Literature-Based Knowl-
edge Discovery using Natural Language Process-
ing. Literature-based Discovery, Information Sci-
ence and Knowledge Management, 15:133–152.

Xiaouha Hu, Xiaodan Zhang, Illhoi Yoo, and Yanqing
Zhang. 2006. A semantic approach for mining hid-
den links from complementary and non-interactive
biomedical literature. In Proc. of the 6th SDM,
pages 200–209.

Tuyen N. Huynh and Raymond J. Mooney. 2008.
Discriminative structure and parameter learning for
markov logic networks. In Proc. of the 25th ICML,
pages 416–423.

Jun’ichi Kazama and Kentaro Torisawa. 2008. In-
ducing Gazetteers for Named Entity Recognition by
Large-scale Clustering of Dependency Relations. In
Proc. of the 46th ACL, pages 407–415.

Marius Paşca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, and Alpa Jain. 2006. Names and Similar-
ities on the Web: Fact Extraction in the Fast Lane.
In Proc. of the COLING-ACL06, pages 809–816.

Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations. In Proc. of the
COLING-ACL06, pages 113–120.

J. R. Quinlan and R. M. Cameron-Jones. 1993. FOIL:
A Midterm Report. In Proc. of the ECML, pages
3–20.

Matthew Richardson and Pedro Domingo. 2006.
Markov logic networks. Machine Learning,
26:107–136.

Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proc. of EMNLP2010,
pages 1088–1098.

Padmini Srinivasan. 2004. Text mining: generating
hypotheses from medline. Journal of the Ameri-
can Society for Information Science and Technology,
55(5):396–413.

Don R. Swanson. 1986. Undiscovered public knowl-
edge. Library Quarterly, 56(2):103–118.

Kentaro Torisawa, Stijn De Saeger, Jun’ichi Kazama,
Asuka Sumida, Daisuke Noguchi, Yasunari Kak-
izawa, Masaaki Murata, Kow Kuroda, and Ichiro Ya-
mada. 2010. Organizing the web’s information ex-
plosion to discover unknown unknowns. New Gen-
eration Computing, 28(3):217–236.

Masaaki Tsuchida, Stijn De Saeger, Kentaro Torisawa,
Masaki Murata, Jun’ichi Kazama, Kow Kuroda, and
Hayato Ohwada. 2010. Large scale similarity-based
relation exapnsion. In Proc of the 4th IUCS, pages
140–147.

910


