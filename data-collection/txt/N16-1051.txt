



















































Intra-Topic Variability Normalization based on Linear Projection for Topic Classification


Proceedings of NAACL-HLT 2016, pages 441–446,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Intra-Topic Variability Normalization based on Linear Projection for Topic
Classification

Quan Liu†, Wu Guo†, Zhen-Hua Ling†, Hui Jiang‡, Yu Hu†§
† National Engineering Laboratory for Speech and Language Information Processing

University of Science and Technology of China, Hefei, Anhui, China
‡ Department of Electrical Engineering and Computer Science

York University, 4700 Keele Street, Toronto, Ontario, M3J 1P3, Canada
§ iFLYTEK Research, Hefei, China

emails: quanliu@mail.ustc.edu.cn, guowu@ustc.edu.cn, zhling@ustc.edu.cn
hj@cse.yorku.ca, yuhu@iflytek.com

Abstract

This paper proposes a variability normaliza-
tion algorithm to reduce the variability be-
tween intra-topic documents for topic classi-
fication. Firstly, an optimization problem is
constructed based on linear variability remov-
able assumption. Secondly, a new feature s-
pace for document representation is found by
solving the optimization problem with kernel
principle component analysis (KPCA). Final-
ly, effective feature transformation is taken
through linear projection. As for experiments,
state-of-the-art SVM and KNN algorithm are
adopted for topic classification respectively.
Experimental results on a free-style conversa-
tional corpus show that the proposed variabili-
ty normalization algorithm for topic classifica-
tion achieves 3.8% absolute improvement for
micro-F1 measure.

1 Introduction

Topic classification is now faced with the problem of
enormous variability between documents due to the
exponential growth of free-style unstructured texts
in recent years. This paper treats variability as d-
ifferences between text documents and aims at re-
ducing the intra-topic document variability for bet-
ter topic classification. There are various factors
to cause the intra-topic variability problem, such as
the different language usages of different persons
(Chambers, 1995; Fillmore et al., 2014). In free-
style conversations experimented in this paper, d-
ifferent people would use very different words to
express their opinions. Therefore, documents in a

same topic could be quite different because of the
intra-topic variability problem.

In this work, we are interested in finding a ro-
bust document representation strategy to address the
intra-topic variability problem. Traditional method
represents document by a high-dimensional TF-IDF
vector based on the bag-of-word approach (Salton
and McGill, 1986; Salton and Buckley, 1988). How-
ever, the TF-IDF feature reveals little semantic sim-
ilarity information between terms, which would
increase the differences between intra-topic docu-
ments when different words are used. Beyond the
TF-IDF strategy, there are two class of techniques,
i.e., unsupervised technique and supervised tech-
nique for document representations. The unsuper-
vised technique includes some latent semantic anal-
ysis methods. The typical method is Latent Se-
mantic Indexing (LSI) while the features estimated
by LSI are linear combinations of the original fea-
tures (Deerwester et al., 1990; Wang et al., 2013).
Meanwhile, the popular Latent Dirichlet Allocation
(Blei et al., 2003; Morchid et al., 2014) algorithm
was proposed to represent document by a generative
probabilistic model (Blei et al., 2003). Moreover,
in recent years, many neural network based meth-
ods have been investigated for document represen-
tations (Hinton and Salakhutdinov, 2006; Srivasta-
va et al., 2013; Le and Mikolov, 2014). For exam-
ple, in (Le and Mikolov, 2014), a model called para-
graph vector was designed to represent each docu-
ment by a dense vector while the vector is trained
by predicting all words in the corresponding doc-
ument. On the other hand, supervised technique
for document representation includes some discrim-

441



inative approaches, e.g., Linear Discriminant Anal-
ysis (Berry et al., 1995; Chakrabarti et al., 2003;
Torkkola, 2004) and supervised latent semantic in-
dexing (Sun et al., 2004; Chakraborti et al., 2007;
Bai et al., 2009). Meanwhile, some improved lin-
ear analysis methods were proposed for encoding
documents with a reliable similarity information (Y-
ih et al., 2011; Chang et al., 2013). However, all
those works for document representation paid little
attention to the variability of intra-topic documents.
Therefore, they could hardly solve the intra-topic
variability problem in a direct way.

This paper makes a preliminary investigation to
deal with the intra-topic variability problem. The
main purpose of this work is to find a new feature
space with minimized intra-topic variability. An
objective criterion is constructed for optimization.
Mathematically, we make use of the topic label in-
formation of the training set to create a weighting
matrix, and then sum over all the differences of
intra-topic documents. Then a robust feature space
with minimized intra-topic variability is generated
by solving the optimization problem with effective
KPCA based algorithm. Finally, we accomplish the
variability normalization operations for the baseline
features. We also employ the linear discriminant
analysis as a supplementary algorithm. As for exper-
iments, state-of-the-art SVM and KNN algorithms
are employed for topic classification. System per-
formances are evaluated on a challenging free-style
conversational database.

The rest of this paper is organized as follows. In
section 2, we introduce the proposed variability nor-
malization algorithm for topic classification in de-
tail. After it, section 3 presents experimental set-
up and results. Finally, conclusions and future work
would be given in section 4.

2 Variability Normalization Algorithm

2.1 Motivation for variability normalization

This work aims to find a robust document repre-
sentation strategy for topic classification. The pro-
posed algorithm is motivated by the Nuisance At-
tribute Projection (NAP) algorithm in speaker veri-
fication field (Solomonoff et al., 2005; Solomonoff
et al., 2007). We firstly make a linear variability re-
movable assumption for document representation.

Mathematically, given a document, it could be de-
noted by a column vector x with dimensionality of
d as follows

x = xt + xv (1)

where xt denotes the useful signal information in
current document, xv stands for the remaining noise.
It is very difficult to model the noise signal in a
document since it could come from various sources.
Therefore, in this paper, we focus on the noise cre-
ated by the variability among intra-topic documents.
Our goal is to find a new document representation
through linear projection:

x̃ = Px (2)

where P is the projection matrix. Since the goal
of this paper is not dimensionality reduction, the
dimensionality of the new document representation
is the same as the source document representation.
Therefore, the size of P is d×d. This paper propos-
es to learn P by minimizing the following intra-topic
variability

Q =
∑
i,j

wij‖P(xi − xj)‖2 (3)

where wij is the i-th row and j-th column element
of a weighting matrix W created in this work. The
matrix is determined by the topic label information
of training set as follows

wij =

{
1 if xi and xj belong to a same topic
0 othervise

(4)

2.2 Variability normalization algorithm

For deriving the variability normalization algorithm,
we follow the work of (Solomonoff et al., 2007) and
re-write the projection matrix P by the variability
space (denoted as a unit vector v here) as follows

P = I− vvT (5)

where I is a (d × d) dimensional identity matrix.
Combining (3) and (5), we get

Q =
∑
i,j

wij(‖xi − xj‖2 − (vT(xi − xj))2). (6)

442



Since the first part of Q in (6) is independent on v,
we discard it and create the final criterion

Q = −
∑
i,j

wij(vT(xi − xj))2). (7)

Unfolding (7) by linear operation, we get

Q = 2vTX · (W − diag(W · 1)) ·XTv. (8)
where X denotes the training set matrix, each row
of X represents one document vector, 1 is a vec-
tor with all elements equal to 1. Minimizing (8) is
equivalent to solving the flowing eigenvalue decom-
position problem

X · (diag(W · 1)−W)XTv = λv. (9)
Here we apply the idea of KPCA (Solomonoff et al.,
2007; Schölkopf et al., 1997) to solve (9). Denoting
v by a new vector Xu, finding u turns to solving a
generalized eigenvalue problem in kernel space as

KZKu = λKu
K = XTX
Z = diag(W · 1)−W.

(10)

The variability space is then constructed by selecting
a set of eigenvectors corresponding to the d1 largest
eigenvalues.

U = [u1,u2, ...,ud1 ] (11)

Finally, a (d × d) projection matrix is obtained by
combining (5), (11) and v = Xu.

Based on this variability normalization algorithm,
the baseline document vectors could be transformed
to a new feature space with minimized intra-topic
variability. The main procedure to implement intra-
topic variation normalization could be divided into
the following steps:

• Generate sample matrix X using the whole n
documents of training set.

• Construct weighting matrix W according to (4)
with the use of topic label information.

• Estimate a projection matrix P by solving the
aforementioned eigenvalue problem.

• Transform all documents to new feature space
through linear projection according to (2).

It should be noticed that after making feature
transformation by the proposed variability normal-
ization algorithm, the dimensionality of document
representation has not been changed. This is dif-
ferent with all the existing dimensionality reduction
methods since our goal is to re-define the feature
representation space for topic document representa-
tion. To prove the effectiveness of the proposed al-
gorithm, this paper presents experimental results on
a challenging conversational dataset.

3 Experiments

In this section, we evaluate the proposed variability
normalization method in a typical topic classifica-
tion problem. We will firstly introduce the exper-
imental setup, including dataset, evaluation criteria
and system description. After it, all the experimen-
tal results would be reported in detail.

3.1 Experimental setup

3.1.1 Dataset
The data set used in this paper is the text tran-

scripts of free-style conversational speech database,
Fisher English corpus released by LDC, which con-
tains 11699 recorded conversations (Cieri et al.,
2004). This corpus is collected from 40 different
topics, and each document includes relatively a dis-
tinct topic (e.g. “Comedy”, “Smoking”, “Terroris-
m”, etc.) as well as topics covering similar sub-
ject areas (e.g. “Airport Security”, “Bioterrorism”,
“Issues in the Middle East”). This paper randomly
chooses 60 documents and 50 documents per topic
for the training set and testing set respectively. An-
other 50 documents for each topic are randomly se-
lected to for the development set.

3.1.2 Evaluation criteria
We use two types of criteria to make a compre-

hensive evaluations for this work. The first evalu-
ation creterion is F1 measure corresponding to the
recall and precision rates for a typical classification
system. In detail, we would report micro-average
F1 and macro-average F1 results. In consideration
of topic classification is similar to topic verification,
we choose equal error rate (EER) to be the second
criterion, which is the equal value of miss probabil-
ity and false probability.

443



3.1.3 System description

Module Methods
Text processing stop-word removal, stemming
Representation TF-IDF feature
Classification KNN, SVM algorithm
Table 1: Baseline system modules for topic classification.

This paper constructs several systems for compar-
ison. The configurations of our baseline system are
shown in Table 1. Porter algorithm (Porter, 1980)
is adopted for word stemming after stop-words re-
moval. Then a vocabulary with 19534 unique words
is determined according to the occurrence frequency
information of training set. Documents in the base-
line system are represented by using the popular TF-
IDF term weighting strategy (Salton and Buckley,
1988). Two popular algorithms SVM and KNN are
used for classification separately. The SVM classi-
fication is implemented using the LIBSVM toolkit
(Chang and Lin, 2011).

Based on the baseline system, descriptions of oth-
er systems are given as below.

(1) LSI: documents are represented in latent se-
mantic space estimated by the LSI algorithm (Deer-
wester et al., 1990) based on the baseline features.

(2) LDA: document features are transformed by
linear discriminant analysis. We select 50 eigenvec-
tors for the low dimensional feature space.

(3) VarNorm: document features are transformed
from the baseline TF-IDF vectors by the approach
proposed in this paper. We select 60 eigenvectors
for generating the project matrix.

(4) VarNorm-LDA: system combined VarNorm
with LDA, which employs feature transformation
operations twice on the original TF-IDF document
features. The number of eigenvectors for VarNorm
and LDA are set to 60 and 50 respectively.

All the parameters suggested in this paper are
tuned on the development set. However, the eigen-
vector number is not restricted to 50 or 60. It is rec-
ommended to set the eigenvector num from 45 to 75
since we have 40 topics for experiments.

3.2 Experimental Results
3.2.1 Variability normalization performance

According to (3), we compare the intra-topic vari-
ability for the baseline and the VarNorm system. The

difference for variability calculation is whether to
use the projection matrix P or not. Figure 1 shows
the intra-topic variability on 40 topics of the train-
ing set. The vertical axis represents the variabili-
ty for each topic, while the horizontal axis stands
for 40 topics in the conversation corpus. As we can
see clearly, the variability of baseline system is high.
After conducting variability normalization, it could
be reduced effectively.

0

5

10

15

20

25

30

35

EN
G

0
1

EN
G

0
2

EN
G

0
3

EN
G

0
4

EN
G

0
5

EN
G

0
6

EN
G

0
7

EN
G

0
8

EN
G

0
9

EN
G

1
0

EN
G

1
1

EN
G

1
2

EN
G

1
3

EN
G

1
4

EN
G

1
5

EN
G

1
6

EN
G

1
7

EN
G

1
8

EN
G

1
9

EN
G

2
0

EN
G

2
1

EN
G

2
2

EN
G

2
3

EN
G

2
4

EN
G

2
5

EN
G

2
6

EN
G

2
7

EN
G

2
8

EN
G

2
9

EN
G

3
0

EN
G

3
1

EN
G

3
2

EN
G

3
3

EN
G

3
4

EN
G

3
5

EN
G

3
6

EN
G

3
7

EN
G

3
8

EN
G

3
9

EN
G

4
0

Intra-topic variability statistic

Baseline system Variability removal

Figure 1: Variability normalization performance.

After making detailed analysis, we find for the
topic ENG06, the theme is “Hypothetical Situations:
Perjury – Do either of you think that you would com-
mit perjury for a close friend or family member?”,
the variability among documents from this topic is
largest in the whole corpus. However, for the top-
ic ENG13, “Movies: Do each of you enjoy going to
the movies in a theater, or would you rather rent a
movie and stay home? What was the last movie that
you saw? Was it good or bad and why?”, the vari-
ability is the lowest. This is the difference between
common topics and infrequent topics. Since people
would use various words to express their ideas, it
is reasonable to find the variability problem is more
serious for infrequent topics than common topics.

3.2.2 Classification Results using KNN
Experimental results using KNN classification al-

gorithm are given in Table 2. The results show
that, compared to the baseline system, the variabil-
ity normalization system VarNorm achieves 2% ab-
solute F1 improvement, and 29% relative improve-
ment for EER. When taking the variability remov-
ing as a preliminary process, and employing LDA as
the secondary transformation, the system VarNorm-
LDA achieves the best performance. The EER is im-

444



Table 2: Classification results using KNN algorithm
System EER macro-F1 micro-F1
Baseline 6.10 84.72 83.15
LSI 4.25 86.24 85.45
LDA 4.49 88.94 88.15
VarNorm 4.30 86.46 85.60
VarNorm-LDA 2.51 90.29 90.00

Table 3: Classification results using SVM algorithm
System EER macro-F1 micro-F1
Baseline 3.40 88.86 88.40
LSI 3.35 89.59 89.25
LDA 3.05 90.81 90.55
VarNorm 2.90 91.04 90.80
VarNorm-LDA 2.50 92.28 92.15

proved by 65% relatively, and the micro-F1 measure
is improved by 6.85% absolutely. The reason for this
performance is straightforward. Since the proposed
algorithm effectively reduce the differences among
intra-topic documents, the LDA algorithm would be
more easier and effective to maximize the ratio of
between-class-variance to within-class-variance.

3.2.3 Classification Results using SVM
Similarly, the experimental results using SVM

classification algorithm are shown in Table 3. The
baseline performance is better than system us-
ing KNN algorithm. The improvements achieved
by LSI in KNN sytem almost vanish here, while
the VarNorm system keeps its improvement. The
VarNorm system even works better than the LDA
system, with nearly 15% relative improvement on
EER, and 3.4% absolute improvement on micro-
F1 measure. The best results are obtained by the
VarNorm-LDA system. There are 36% relative im-
provement for EER, and 3.75% absolute improve-
ment for micro-F1 measure.

4 Conclusions and Future Work

In this paper, we investigated the intra-topic vari-
ability problem for topic classification. The major
contribution of this work is that we proposed a ef-
fective variability normalization approach for robust
document representation. An optimization problem
was constructed after making a linear variability re-
movable assumption. In order to take a deep insight

into the performance of the proposed variability nor-
malization algorithm, we conducted experiments on
a challenge free-style conversation corpus. Experi-
mental results based on the SVM and KNN classifi-
cation algorithm all confirmed the robustness of the
proposed approach. As a conclusion, the variability
normalization algorithm could be used as a front-end
feature transformation strategy, and we also suggest
to combine it with linear discriminant analysis algo-
rithm or some other algorithms to further improve
system performances.

Further study will investigate the adaptive meth-
ods for constructing robust feature spaces. We
would also combine this work with more document
representations methods as well. Moreover, it would
be very interesting to extend and combine our work
to some novel unsupervised machine learning tech-
niques, like the work of (Zhang and Jiang, 2015)
while they proposed a model for high-dimensional
data by combineing a linear orthogonal projection
and a finite mixture model under a unified genera-
tive modeling framework.

Acknowledgments

This work was supported in part by the Science and
Technology Development of Anhui Province, Chi-
na (Grants No. 2014z02006) and the Fundamental
Research Funds for the Central Universities (Grant
No. WK2350000001). At the same time, we want to
give special thanks to the anonymous reviewers for
their insightful comments as well as suggestions.

References

Bing Bai, Jason Weston, David Grangier, Ronan
Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier
Chapelle, and Kilian Weinberger. 2009. Supervised
semantic indexing. In Proceedings of the 18th ACM
conference on Information and knowledge manage-
ment, pages 187–196. ACM.

Michael W Berry, Susan T Dumais, and Gavin W
O’Brien. 1995. Using linear algebra for intelligent
information retrieval. SIAM review, 37(4):573–595.

D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
dirichlet allocation. the Journal of machine Learning
research, 3:993–1022.

Soumen Chakrabarti, Shourya Roy, and Mahesh V
Soundalgekar. 2003. Fast and accurate text classifica-

445



tion via multiple linear discriminant projections. The
VLDB Journal, 12(2):170–185.

Sutanu Chakraborti, Rahman Mukras, Robert Lothian,
Nirmalie Wiratunga, Stuart NK Watt, and David J
Harper. 2007. Supervised latent semantic indexing
using adaptive sprinkling. In IJCAI, pages 1582–1587.

Jack K Chambers. 1995. Sociolinguistic theory. Black-
well.

Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.

Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
EMNLP, pages 1602–1612.

C. Cieri, D. Miller, and K. Walker. 2004. The Fisher
corpus: a resource for the next generations of speech-
to-text. In LREC, pages 69–71.

S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. JASIS, 41(6):391–407.

Charles J Fillmore, Daniel Kempler, and William SY
Wang. 2014. Individual differences in language a-
bility and language behavior. Academic Press.

Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504–507.

Quoc V Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. arXiv preprint
arXiv:1405.4053.

Mohamed Morchid, Richard Dufour, and Georges
Linares. 2014. A lda-based topic classification ap-
proach from highly imperfect automatic transcriptions.
LREC14.

Martin F Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.

Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation processing & management, 24(5):513–523.

Gerard Salton and Michael J McGill. 1986. Introduction
to modern information retrieval.

B. Schölkopf, A. Smola, and KR. Müller. 1997. Ker-
nel principal component analysis. In Artificial Neural
Networks-ICANN’97, pages 583–588. Springer.

Alex Solomonoff, William M Campbell, and Ian Board-
man. 2005. Advances in channel compensation for
svm speaker recognition. In ICASSP, pages 629–632.

A. Solomonoff, W. M. Campbell, and C. Quillen. 2007.
Nuisance attribute projection. Speech Communica-
tion.

Nitish Srivastava, Ruslan R Salakhutdinov, and Geof-
frey E Hinton. 2013. Modeling documents with
deep boltzmann machines. arXiv preprint arX-
iv:1309.6865.

Jian-Tao Sun, Zheng Chen, Hua-Jun Zeng, Yu-Chang Lu,
Chun-Yi Shi, and Wei-Ying Ma. 2004. Supervised
latent semantic indexing for document categorization.
In Data Mining, 2004. ICDM’04. Fourth IEEE Inter-
national Conference on, pages 535–538. IEEE.

K. Torkkola. 2004. Discriminative features for text doc-
ument classification. Formal Pattern Analysis & Ap-
plications, 6(4):301–308.

Quan Wang, Jun Xu, Hang Li, and Nick Craswell. 2013.
Regularized latent semantic indexing: A new approach
to large-scale topic modeling. ACM Transactions on
Information Systems (TOIS), 31(1):5.

Wen-tau Yih, Kristina Toutanova, John C Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In CoNLL,
pages 247–256. Association for Computational Lin-
guistics.

Shiliang Zhang and Hui Jiang. 2015. Hybrid orthogonal
projection and estimation (hope): A new framework to
probe and learn neural networks. arXiv preprint arX-
iv:1502.00702.

446


