



















































A Vector Space for Distributional Semantics for Entailment


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2052–2062,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

A Vector Space for Distributional Semantics for Entailment ∗

James Henderson and Diana Nicoleta Popa
Xerox Research Centre Europe

james.henderson@xrce.xerox.com and diana.popa@xrce.xerox.com

Abstract

Distributional semantics creates vector-
space representations that capture many
forms of semantic similarity, but their re-
lation to semantic entailment has been less
clear. We propose a vector-space model
which provides a formal foundation for a
distributional semantics of entailment. Us-
ing a mean-field approximation, we de-
velop approximate inference procedures
and entailment operators over vectors of
probabilities of features being known (ver-
sus unknown). We use this framework
to reinterpret an existing distributional-
semantic model (Word2Vec) as approxi-
mating an entailment-based model of the
distributions of words in contexts, thereby
predicting lexical entailment relations. In
both unsupervised and semi-supervised
experiments on hyponymy detection, we
get substantial improvements over previ-
ous results.

1 Introduction

Modelling entailment is a fundamental issue in
computational semantics. It is also important for
many applications, for example to produce ab-
stract summaries or to answer questions from text,
where we need to ensure that the input text entails
the output text. There has been a lot of interest in
modelling entailment in a vector-space, but most
of this work takes an empirical, often ad-hoc, ap-
proach to this problem, and achieving good results
has been difficult (Levy et al., 2015). In this work,
we propose a new framework for modelling entail-
ment in a vector-space, and illustrate its effective-

∗This work was partially supported by French ANR grant
CIFRE N 1324/2014.

⇒ unk f g ¬f
unk 1 0 0 0
f 1 1 0 0
g 1 0 1 0
¬f 1 0 0 1

Table 1: Pattern of logical entailment between
nothing known (unk), two different features f and
g known, and the complement of f (¬f ) known.

ness with a distributional-semantic model of hy-
ponymy detection.

Unlike previous vector-space models of entail-
ment, the proposed framework explicitly models
what information is unknown. This is a crucial
property, because entailment reflects what infor-
mation is and is not known; a representation y en-
tails a representation x if and only if everything
that is known given x is also known given y. Thus,
we model entailment in a vector space where each
dimension represents something we might know.
As illustrated in Table 1, knowing that a feature f
is true always entails knowing that same feature,
but never entails knowing that a different feature g
is true. Also, knowing that a feature is true always
entails not knowing anything (unk), since strictly
less information is still entailment, but the reverse
is never true. Table 1 also illustrates that knowing
that a feature f is false (¬f ) patterns exactly the
same way as knowing that an unrelated feature g
is true. This illustrates that the relevant dichotomy
for entailment is known versus unknown, and not
true versus false.

Previous vector-space models have been very
successful at modelling semantic similarity, in par-
ticular using distributional semantic models (e.g.
(Deerwester et al., 1990; Schütze, 1993; Mikolov
et al., 2013a)). Distributional semantics uses the
distributions of words in contexts to induce vector-
space embeddings of words, which have been

2052



shown to be useful for a wide variety of tasks.
Two words are predicted to be similar if the dot
product between their vectors is high. But the
dot product is an anti-symmetric operator, which
makes it more natural to interpret these vectors
as representing whether features are true or false,
whereas the dichotomy known versus unknown is
asymmetric. We surmise that this is why distribu-
tional semantic models have had difficulty mod-
elling lexical entailment (Levy et al., 2015).

To develop a vector-space model of whether
features are known or unknown, we start with dis-
crete binary vectors, where 1 means known and 0
means unknown. Entailment between these dis-
crete binary vectors can be calculated by indepen-
dently checking each dimension. But as soon as
we try to do calculations with distributions over
these vectors, we need to deal with the case where
the features are not independent. For example, if
feature f has a 50% chance of being true and a
50% chance of being false, we can’t assume that
there is a 25% chance that both f and ¬f are
known. This simple case of mutual exclusion is
just one example of a wide range of constraints
between features which we need to handle in se-
mantic models. These constraints mean that the
different dimensions of our vector space are not
independent, and therefore exact models are not
factorised. Because the models are not factorised,
exact calculations of entailment and exact infer-
ence of vectors are intractable.

Mean-field approximations are a popular ap-
proach to efficient inference for intractable mod-
els. In a mean-field approximation, distributions
over binary vectors are represented using a sin-
gle probability for each dimension. These vectors
of real values are the basis of our proposed vector
space for entailment.

In this work, we propose a vector-space model
which provides a formal foundation for a distri-
butional semantics of entailment. This framework
is derived from a mean-field approximation to en-
tailment between binary vectors, and includes op-
erators for measuring entailment between vectors,
and procedures for inferring vectors in an entail-
ment graph. We validate this framework by us-
ing it to reinterpret existing Word2Vec (Mikolov
et al., 2013a) word embedding vectors as approxi-
mating an entailment-based model of the distribu-
tion of words in contexts. This reinterpretation al-
lows us to use existing word embeddings as an un-

supervised model of lexical entailment, success-
fully predicting hyponymy relations using the pro-
posed entailment operators in both unsupervised
and semi-supervised experiments.

2 Modelling Entailment in a Vector Space

To develop a model of entailment in a vector
space, we start with the logical definition of en-
tailment in terms of vectors of discrete known fea-
tures: y entails x if and only if all the known fea-
tures in x are also included in y. We formalise this
relation with binary vectors x, y where 1 means
known and 0 means unknown, so this discrete en-
tailment relation (y⇒x) can be defined with the
binary formula:

P ((y⇒x) | x, y) =
∏
k

(1− (1−yk)xk)

Given prior probability distributions P (x), P (y)
over these vectors, the exact joint and marginal
probabilities for an entailment relation are:

P (x, y, (y⇒x)) = P (x) P (y)
∏
k

(1−(1−yk)xk)

P ((y⇒x)) = EP (x)EP (y)
∏
k

(1−(1−yk)xk) (1)

We cannot assume that the priors P (x) and
P (y) are factorised, because there are many im-
portant correlations between features and there-
fore we cannot assume that the features are in-
dependent. As discussed in Section 1, even just
representing both a feature f and its negation ¬f
requires two different dimensions k and k′ in the
vector space, because 0 represents unknown and
not false. Given valid feature vectors, calculating
entailment can consider these two dimensions sep-
arately, but to reason with distributions over vec-
tors we need the prior P (x) to enforce the con-
straint that xk and xk′ are mutually exclusive. In
general, such correlations and anti-correlations ex-
ist between many semantic features, which makes
inference and calculating the probability of entail-
ment intractable.

To allow for efficient inference in such a model,
we propose a mean-field approximation. This in
effect assumes that the posterior distribution over
vectors is factorised, but in practice this is a much
weaker assumption than assuming the prior is fac-
torised. The posterior distribution has less un-
certainty and therefore is influenced less by non-
factorised prior constraints. By assuming a fac-
torised posterior, we can then represent distribu-
tions over feature vectors with simple vectors of

2053



probabilities of individual features (or as below,
with their log-odds). These real-valued vectors are
the basis of the proposed vector-space model of
entailment.

In the next two subsections, we derive a mean-
field approximation for inference of real-valued
vectors in entailment graphs. This derivation
leads to three proposed vector-space operators for
approximating the log-probability of entailment,
summarised in Table 2. These operators will be
used in the evaluation in Section 5. This inference
framework will also be used in Section 3 to model
how existing word embeddings can be mapped to
vectors to which the entailment operators can be
applied.

2.1 A Mean-Field Approximation

A mean-field approximation approximates the
posterior P using a factorised distribution Q. First
of all, this gives us a concise description of the
posterior P (x| . . .) as a vector of continuous val-
ues Q(x=1), where Q(x=1)k = Q(xk=1) ≈
EP (x|...)xk = P (xk=1| . . .) (i.e. the marginal
probabilities of each bit). Secondly, as is shown
below, this gives us efficient methods for doing ap-
proximate inference of vectors in a model.

First we consider the simple case where
we want to approximate the posterior distribu-
tion P (x, y|y⇒x). In a mean-field approxi-
mation, we want to find a factorised distribu-
tion Q(x, y) which minimises the KL-divergence
DKL(Q(x, y)||P (x, y|y⇒x)) with the true distri-
bution P (x, y|y⇒x).
L = DKL(Q(x, y)||P (x, y|(y⇒x)))
∝
∑
x

Q(x) log
Q(x, y)

P (x, y, (y⇒x))
=
∑
k

EQ(xk) logQ(xk) +
∑
k

EQ(yk) logQ(yk)

− EQ(x) logP (x)− EQ(y) logP (y)
−
∑
k

EQ(xk)EQ(yk) log(1−(1−yk)xk)

In the final equation, the first two terms are the
negative entropy of Q, −H(Q), which acts as a
maximum entropy regulariser, the final term en-
forces the entailment constraint, and the middle
two terms represent the prior for x and y. One ap-
proach (generalised further in the next subsection)
to the prior terms −EQ(x) logP (x) is to bound
them by assuming P (x) is a function in the ex-
ponential family, giving us:

EQ(x) logP (x) ≥ EQ(x) log
exp(

∑
k θ

x
kxk)

Zθ
=
∑
k

EQ(xk)θ
x
kxk − logZθ

where the logZθ is not relevant in any of our in-
ference problems and thus will be dropped below.

As typically in mean-field approximations, in-
ference ofQ(x) andQ(y) can’t be done efficiently
with this exact objective L, because of the non-
linear interdependence between xk and yk in the
last term. Thus, we introduce two approximations
to L, one for use in inferring Q(x) given Q(y)
(forward inference), and one for the reverse in-
ference problem (backward inference). In both
cases, the approximation is done with an appli-
cation of Jensen’s inequality to the log function,
which gives us an upper bound on L, as is stan-
dard practice in mean-field approximations. For
forward inference:

L ≤−H(Q)−Q(xk=1)θxk − EQ(yk)θykyk (2)
−Q(xk=1) logQ(yk=1) )

which we can optimise for Q(xk=1):

Q(xk=1) = σ( θxk + logQ(yk=1) ) (3)

where σ() is the sigmoid function. The sig-
moid function arises from the entropy regulariser,
making this a specific form of maximum entropy
model. And for backward inference:

L ≤−H(Q)− EQ(xk)θxkxk −Q(yk=1)θyk (4)
− (1−Q(yk=1)) log(1−Q(xk=1)) )

which we can optimise for Q(yk=1):

Q(yk=1) = σ( θ
y
k − log(1−Q(xk=1)) ) (5)

Note that in equations (2) and (4) the
final terms, Q(xk=1) logQ(yk=1) and
(1−Q(yk=1)) log(1−Q(xk=1)) respectively,
are approximations to the log-probability of the
entailment. We define two vector-space operators,
<© and >©, to be these same approximations.

logQ(y⇒x)
≈
∑
k

EQ(xk) log(EQ(yk)(1− (1−yk)xk))

= Q(x=1) · logQ(y=1) ≡ X<©Y
logQ(y⇒x)

≈
∑
k

EQ(yk) log(EQ(xk)(1− (1−yk)xk))

= (1−Q(y=1)) · log(1−Q(x=1)) ≡ Y >©X

2054



X<©Y ≡ σ(X) · log σ(Y )
Y >©X ≡ σ(−Y ) · log σ(−X)
Y ⇒̃X ≡

∑
k

log(1− σ(−Yk)σ(Xk))

Table 2: The proposed entailment operators, ap-
proximating logP (y⇒x).

We parametrise these operators with the vectors
X,Y of log-odds of Q(x), Q(y), namely X =
log Q(x=1)Q(x=0) = σ

-1(Q(x=1)). The resulting opera-
tor definitions are summarised in Table 2.

Also note that the probability of entailment
given in equation (1) becomes factorised when we
replace P with Q. We define a third vector-space
operator, ⇒̃, to be this factorised approximation,
also shown in Table 2.

2.2 Inference in Entailment Graphs
In general, doing inference for one entailment is
not enough; we want to do inference in a graph of
entailments between variables. In this section we
generalise the above mean-field approximation to
entailment graphs.

To represent information about variables that
comes from outside the entailment graph, we as-
sume we are given a prior P (x) over all variables
xi in the graph. As above, we do not assume that
this prior is factorised. Instead we assume that the
prior P (x) is itself a graphical model which can be
approximated with a mean-field approximation.

Given a set of variables xi each represent-
ing vectors of binary variables xik, a set of en-
tailment relations r = {(i, j)|(xi⇒xj)}, and
a set of negated entailment relations r̄ =
{(i, j)|(xi /⇒xj)}, we can write the joint posterior
probability as:

P (x, r, r̄) =
1
ZP (x)

∏
i

(
(
∏

j:r(i,j)

∏
k

P (xik⇒xjk|xik, xjk))

(
∏

j:r̄(i,j)

(1−
∏
k

P (xik⇒xjk|xik, xjk)))
)

We want to find a factorised distribution Q that
minimises L = DKL(Q(x)||P (x|r, r̄)). As
above, we bound this loss for each element
Xik=σ-1(Q(xik=1)) of each vector we want to
infer, using analogous Jensen’s inequalities for the
terms involving nodes i and j such that r(i, j) or
r(j, i). For completeness, we also propose similar

inequalities for nodes i and j such that r̄(i, j) or
r̄(j, i), and bound them using the constants

Cijk ≥
∏
k′ 6=k

(1−σ(−Xik′)σ(Xjk′)).

To represent the prior P (x), we use the terms

θik(Xīk) ≤ log
EQ(xīk)P (xīk, xik=1)

1− EQ(xīk)P (xīk, xik=1)
where xīk is the set of all xi′k′ such that either i

′ 6=i
or k′ 6=k. These terms can be thought of as the log-
odds terms that would be contributed to the loss
function by including the prior’s graphical model
in the mean-field approximation.

Now we can infer the optimal Xik as:

Xik = θik(Xīk) +
∑
j:r(i,j)

− log σ(−Xjk) (6)

+
∑
j:r(j,i)

log σ(Xjk) +
∑
j:r̄(j,i)

log
1−Cijkσ(Xjk)

1−Cijk

+
∑
j:r̄(i,j)

− log 1−Cijkσ(−Xjk)
1−Cijk

In summary, the proposed mean-field approx-
imation does inference in entailment graphs by
iteratively re-estimating each Xi as the sum of:
the prior log-odds, − log σ(−Xj) for each en-
tailed variable j, and log σ(Xj) for each entailing
variable j.1 This inference optimises Xi<©Xj for
each entailing j plus Xi >©Xj for each entailed j,
plus a maximum entropy regulariser on Xi. Neg-
ative entailment relations, if they exist, can also
be incorporated with some additional approxima-
tions. Complex priors can also be incorporated
through their log-odds, simulating the inclusion of
the prior within the mean-field approximation.

Given its dependence on mean-field approxima-
tions, it is an empirical question to what extent we
should view this model as computing real entail-
ment probabilities and to what extent we should
view it as a well-motivated non-linear mapping
for which we simply optimise the input-output be-
haviour (as for neural networks (Henderson and
Titov, 2010)). In Sections 3 and 5 we argue for the
former (stronger) view.

3 Interpreting Word2Vec Vectors

To evaluate how well the proposed framework pro-
vides a formal foundation for the distributional se-
mantics of entailment, we use it to re-interpret an

1It is interesting to note that − log σ(−Xj) is a non-
negative transform of Xj , similar to the ReLU nonlinear-
ity which is popular in deep neural networks (Glorot et al.,
2011). log σ(Xj) is the analogous non-positive transform.

2055



existing model of distributional semantics in terms
of semantic entailment. There has been a lot of
work on how to use the distribution of contexts in
which a word occurs to induce a vector represen-
tation of the semantics of words. In this paper,
we leverage this previous work on distributional
semantics by re-interpreting a previous distribu-
tional semantic model and using this understand-
ing to map its vector-space word embeddings to
vectors in the proposed framework. We then use
the proposed operators to predict entailment be-
tween words using these vectors. In Section 5 be-
low, we evaluate these predictions on the task of
hyponymy detection. In this section we motivate
three different ways to interpret the Word2Vec
(Mikolov et al., 2013a; Mikolov et al., 2013b) dis-
tributional semantic model as an approximation to
an entailment-based model of the semantic rela-
tionship between a word and its context.

Distributional semantics learns the semantics of
words by looking at the distribution of contexts
in which they occur. To model this relationship,
we assume that the semantic features of a word
are (statistically speaking) redundant with those of
its context words, and consistent with those of its
context words. We model these properties using
a hidden vector which is the consistent unification
of the features of the middle word and the context.
In other words, there must exist a hidden vector
which entails both of these vectors, and is consis-
tent with prior constraints on vectors. We split this
into two steps, inference of the hidden vector Y
from the middle vector Xm, context vectors Xc
and prior, and computing the log-probability (7)
that this hidden vector entails the middle and con-
text vectors:

max
Y

(logP (y, y⇒xm, y⇒xc)) (7)

We interpret Word2Vec’s Skip-Gram model as
learning its context and middle word vectors so
that the log-probability of this entailment is high
for the observed context words and low for other
(sampled) context words. The word embeddings
produced by Word2Vec are only related to the vec-
tors Xm assigned to the middle words; context
vectors are computed but not output. We model
the context vectors X ′c as combining (as in equa-
tion (5)) information about a context word itself
with information which can be inferred from this
word given the prior, X ′c = θc − log σ(−Xc).

The numbers in the vectors output by Word2Vec

are real numbers between negative infinity and in-
finity, so the simplest interpretation of them is as
the log-odds of a feature being known. In this case
we can treat these vectors directly as theXm in the
model. The inferred hidden vector Y can then be
calculated using the model of backward inference
from the previous section.

Y = θc − log σ(−Xc)− log σ(−Xm)
= X ′c − log σ(−Xm)

Since the unification Y of context and middle
word features is computed using backward infer-
ence, we use the backward-inference operator >© to
calculate how successful that unification was. This
gives us the final score:

logP (y, y⇒xm, y⇒xc)
≈ Y >©Xm + Y >©Xc +−σ(−Y )·θc
= Y >©Xm +−σ(−Y )·X ′c

This is a natural interpretation, but it ignores the
equivalence in Word2Vec between pairs of posi-
tive values and pairs of negative values, due to its
use of the dot product. As a more accurate in-
terpretation, we interpret each Word2Vec dimen-
sion as specifying whether its feature is known
to be true or known to be false. Translating this
Word2Vec vector into a vector in our entailment
vector space, we get one copy Y + of the vector
representing known-to-be-true features and a sec-
ond negated duplicate Y − of the vector represent-
ing known-to-be-false features, which we concate-
nate to get our representation Y .

Y + = X ′c − log σ(−Xm)
Y − = −X ′c − log σ(Xm)
logP (y, y⇒xm, y⇒xc)
≈ Y + >©Xm +−σ(−Y +)·X ′c

+ Y − >©(−Xm) +−σ(−Y −)·(−X ′c)
As a third alternative, we modify this latter in-

terpretation with some probability mass reserved
for unknown in the vicinity of zero. By subtract-
ing 1 from both the original and negated copies of
each dimension, we get a probability of unknown
of 1−σ(Xm−1)− σ(−Xm−1). This gives us:
Y + = X ′c − log σ(−(Xm−1))
Y − = −X ′c − log σ(−(−Xm−1))
logP (y, y⇒xm, y⇒xc)
≈ Y + >©(Xm−1) +−σ(−Y +)·X ′c

+ Y − >©(−Xm−1)) +−σ(−Y −)·(−X ′c)

2056



Figure 1: The learning gradients for Word2Vec,
the log-odds >©, and the unk dup >© interpretation
of its vectors.

To understand better the relative accuracy of
these three interpretations, we compared the train-
ing gradient which Word2Vec uses to train its
middle-word vectors to the training gradient for
each of these interpretations. We plotted these gra-
dients for the range of values typically found in
Word2Vec vectors for both the middle vector and
the context vector. Figure 1 shows three of these
plots. As expected, the second interpretation is
more accurate than the first because its plot is anti-
symmetric around the diagonal, like the Word2Vec
gradient. In the third alternative, the constant 1
was chosen to optimise this match, producing a
close match to the Word2Vec training gradient, as
shown in Figure 1 (Word2Vec versus Unk dup).

Thus, Word2Vec can be seen as a good ap-
proximation to the third model, and a progres-
sively worse approximation to the second and first
models. Therefore, if the entailment-based distri-
butional semantic model we propose is accurate,
then we would expect the best accuracy in hy-
ponymy detection using the third interpretation of
Word2Vec vectors, and progressively worse accu-
racy for the other two interpretations. As we will
see in Section 5, this prediction holds.

4 Related Work

There has been a significant amount of work on us-
ing distributional-semantic vectors for hyponymy
detection, using supervised, semi-supervised or
unsupervised methods (e.g. (Yu et al., 2015; Nec-
sulescu et al., 2015; Vylomova et al., 2015; Weeds
et al., 2014; Fu et al., 2015; Rei and Briscoe,
2014)). Because our main concern is modelling
entailment within a vector space, we do not do a
thorough comparison to models which use mea-

sures computed outside the vector space (e.g.
symmetric measures (LIN (Lin, 1998)), asym-
metric measures (WeedsPrec (Weeds and Weir,
2003; Weeds et al., 2004), balAPinc (Kotlerman
et al., 2010), invCL (Lenci and Benotto, 2012))
and entropy-based measures (SLQS (Santus et al.,
2014))), nor to models which encode hyponymy in
the parameters of a vector-space operator or clas-
sifier (Fu et al., 2015; Roller et al., 2014; Baroni
et al., 2012)). We also limit our evaluation of lex-
ical entailment to hyponymy, not including other
related lexical relations (cf. (Weeds et al., 2014;
Vylomova et al., 2015; Turney and Mohammad,
2014; Levy et al., 2014)), leaving more complex
cases to future work on compositional semantics.
We are also not concerned with models or evalua-
tions which require supervised learning about in-
dividual words, instead limiting ourselves to semi-
supervised learning where the words in the train-
ing and test sets are disjoint.

For these reasons, in our evaluations we repli-
cate the experimental setup of Weeds et al. (2014),
for both unsupervised and semi-supervised mod-
els. Within this setup, we compare to the results of
the models evaluated by Weeds et al. (2014) and to
previously proposed vector-space operators. This
includes one vector space operator for hyponymy
which doesn’t have trained parameters, proposed
by Rei and Briscoe (2014), called weighted cosine.
The dimensions of the dot product (normalised
to make it a cosine measure) are weighted to put
more weight on the larger values in the entailed
(hypernym) vector.

We base this evaluation on the Word2Vec
(Mikolov et al., 2013a; Mikolov et al., 2013b) dis-
tributional semantic model and its publicly avail-
able word embeddings. We choose it because it
is popular, simple, fast, and its embeddings have
been derived from a very large corpus. Levy and
Goldberg (2014) showed that it is closely related
to the previous PMI-based distributional semantic
models (e.g. (Turney and Pantel, 2010)).

The most similar previous work, in terms of mo-
tivation and aims, is that of Vilnis and McCallum
(2015). They also model entailment directly using
a vector space, without training a classifier. But
instead of representing words as a point in a vec-
tor space (as in this work), they represent words
as a Gaussian distribution over points in a vector
space. This allows them to represent the extent to
which a feature is known versus unknown as the

2057



amount of variance in the distribution for that fea-
ture’s dimension. While nicely motivated theoret-
ically, the model appears to be more computation-
ally expensive than the one proposed here, particu-
larly for inferring vectors. They do make unsuper-
vised predictions of hyponymy relations with their
learned vector distributions, using KL-divergence
between the distributions for the two words. They
evaluate their models on the hyponymy data from
(Baroni et al., 2012). As discussed further in sec-
tion 5.2, our best models achieve non-significantly
better average precision than their best models.

The semi-supervised model of Kruszewski et al.
(2015) also models entailment in a vector space,
but they use a discrete vector space. They train
a mapping from distributional semantic vectors
to Boolean vectors such that feature inclusion re-
spects a training set of entailment relations. They
then use feature inclusion to predict hyponymy,
and other lexical entailment relations. This ap-
proach is similar to the one used in our semi-
supervised experiments, except that their discrete
entailment prediction operator is very different
from our proposed entailment operators.

5 Evaluation

To evaluate whether the proposed framework is an
effective model of entailment in vector spaces, we
apply the interpretations from Section 3 to pub-
licly available word embeddings and use them to
predict the hyponymy relations in a benchmark
dataset. This framework predicts that the more ac-
curate interpretations of Word2Vec result in more
accurate unsupervised models of hyponymy. We
evaluate on detecting hyponymy relations between
words because hyponymy is the canonical type of
lexical entailment; most of the semantic features
of a hypernym (e.g. “animal”) must be included in
the semantic features of the hyponym (e.g. “dog”).
We evaluate in both a fully unsupervised setup and
a semi-supervised setup.

5.1 Hyponymy with Word2Vec Vectors

For our evaluation on hyponymy detection, we
replicate the experimental setup of Weeds et al.
(2014), using their selection of word pairs2 from
the BLESS dataset (Baroni and Lenci, 2011).3

2https://github.com/SussexCompSem/
learninghypernyms

3Of the 1667 word pairs in this data, 24 were removed
because we do not have an embedding for one of the words.

These noun-noun word pairs include positive hy-
ponymy pairs, plus negative pairs consisting of
some other hyponymy pairs reversed, some pairs
in other semantic relations, and some random
pairs. Their selection is balanced between positive
and negative examples, so that accuracy can be
used as the performance measure. For their semi-
supervised experiments, ten-fold cross validation
is used, where for each test set, items are removed
from the associated training set if they contain any
word from the test set. Thus, the vocabulary of
the training and testing sets are always disjoint,
thereby requiring that the models learn about the
vector space and not about the words themselves.
We had to perform our own 10-fold split, but apply
the same procedure to filter the training set.

We could not replicate the word embeddings
used in Weeds et al. (2014), so instead we use pub-
licly available word embeddings.4 These vectors
were trained with the Word2Vec software applied
to about 100 billion words of the Google-News
dataset, and have 300 dimensions.

The hyponymy detection results are given in Ta-
ble 3, including both unsupervised (upper box)
and semi-supervised (lower box) experiments. We
report two measures of performance, hyponymy
detection accuracy (50% Acc) and direction clas-
sification accuracy (Dir Acc). Since all the opera-
tors only determine a score, we need to choose a
threshold to get detection accuracies. Given that
the proportion of positive examples in the dataset
has been artificially set at 50%, we threshold each
model’s score at the point where the proportion of
positive examples output is 50%, which we call
“50% Acc”. Thus the threshold is set after seeing
the testing inputs but not their target labels.

Direction classification accuracy (Dir Acc) in-
dicates how well the method distinguishes the rel-
ative abstractness of two nouns. Given a pair of
nouns which are in a hyponymy relation, it classi-
fies which word is the hypernym and which is the
hyponym. This measure only considers positive
examples and chooses one of two directions, so it
is inherently a balanced binary classification task.
Classification is performed by simply comparing
the scores in both directions. If both directions
produce the same score, the expected random ac-
curacy (50%) is used.

As representative of previous work, we report

4https://code.google.com/archive/p/
word2vec/

2058



operator supervision 50% Acc Dir Acc
Weeds et.al. None 58% –
log-odds <© None 54.0% 55.9%

weighted cos None 55.5% 57.9%
dot None 56.3% 50%
dif None 56.9% 59.6%

log-odds ⇒̃ None 57.0% 59.4%
log-odds >© None 60.1%* 62.2%

dup >© None 61.7% 68.8%
unk dup ⇒̃ None 63.4%* 68.8%
unk dup >© None 64.5% 68.8%

Weeds et.al. SVM 75% –
mapped dif cross ent 64.3% 72.3%
mapped <© cross ent 74.5% 91.0%
mapped ⇒̃ cross ent 77.5% 92.3%
mapped >© cross ent 80.1% 90.0%

Table 3: Accuracies on the BLESS data from
Weeds et al. (2014), for hyponymy detection (50%
Acc) and hyponymy direction classification (Dir
Acc), in the unsupervised (upper box) and semi-
supervised (lower box) experiments. For unsuper-
vised accuracies, * marks a significant difference
with the previous row.

the best results from Weeds et al. (2014), who
try a number of unsupervised and semi-supervised
models, and use the same testing methodology
and hyponymy data. However, note that their
word embeddings are different. For the semi-
supervised models, Weeds et al. (2014) trains clas-
sifiers, which are potentially more powerful than
our linear vector mappings. We also compare the
proposed operators to the dot product (dot),5 vec-
tor differences (dif ), and the weighted cosine of
Rei and Briscoe (2014) (weighted cos), all com-
puted with the same word embeddings as for the
proposed operators.

In Section 3 we argued for three progressively
more accurate interpretations of Word2Vec vec-
tors in the proposed framework, the log-odds inter-
pretation (log-odds >©), the negated duplicate inter-
pretation (dup >©), and the negated duplicate inter-
pretation with unknown around zero (unk dup >©).
We also evaluate using the factorised calculation
of entailment (log-odds ⇒̃, unk dup ⇒̃), and the
backward-inference entailment operator (log-odds
<©), neither of which match the proposed interpre-

5We also tested the cosine measure, but results were very
slightly worse than dot.

tations. For the semi-supervised case, we train
a linear vector-space mapping into a new vector
space, in which we apply the operators (mapped
operators). All these results are discussed in the
next two subsections.

5.2 Unsupervised Hyponymy Detection

The first set of experiments evaluate the vector-
space operators in unsupervised models of hy-
ponymy detection. The proposed models are com-
pared to the dot product, because this is the stan-
dard vector-space operator and has been shown to
capture semantic similarity very well. However,
because the dot product is a symmetric operator,
it always performs at chance for direction clas-
sification. Another vector-space operator which
has received much attention recently is vector dif-
ferences. This is used (with vector sum) to per-
form semantic transforms, such as “king - male
+ female = queen”, and has previously been used
for modelling hyponymy (Vylomova et al., 2015;
Weeds et al., 2014). For our purposes, we sum the
pairwise differences to get a score which we use
for hyponymy detection.

For the unsupervised results in the upper box of
table 3, the best unsupervised model of Weeds et
al. (2014), and the operators dot, dif and weighted
cos all perform similarly on accuracy, as does the
log-odds factorised entailment calculation (log-
odds ⇒̃). The forward-inference entailment op-
erator (log-odds <©) performs above chance but not
well, as expected given the backward-inference-
based interpretation of Word2Vec vectors. By def-
inition, dot is at chance for direction classification,
but the other models all perform better, indicat-
ing that all these operators are able to measure
relative abstractness. As predicted, the >© opera-
tor performs significantly better than all these re-
sults on accuracy, as well as on direction classifi-
cation, even assuming the log-odds interpretation
of Word2Vec vectors.

When we move to the more accurate interpreta-
tion of Word2Vec vectors as specifying both orig-
inal and negated features (dup >©), we improve
(non-significantly) on the log-odds interpretation.
Finally, the third and most accurate interpretation,
where values around zero can be unknown (unk
dup >©), achieves the best results in unsupervised
hyponymy detection, as well as for direction clas-
sification. Changing to the factorised entailment
operator (unk dup ⇒̃) is worse but also signifi-

2059



cantly better than the other accuracies.
To allow a direct comparison to the model

of Vilnis and McCallum (2015), we also evalu-
ated the unsupervised models on the hyponymy
data from (Baroni et al., 2012). Our best model
achieved 81% average precision on this dataset,
non-significantly better than the 80% achieved by
the best model of Vilnis and McCallum (2015).

5.3 Semi-supervised Hyponymy Detection

Since the unsupervised learning of word embed-
dings may reflect many context-word correlations
which have nothing to do with hyponymy, we
also consider a semi-supervised setting. Adding
some supervision helps distinguish features that
capture semantic properties from other features
which are not relevant to hyponymy detection. But
even with supervision, we still want the resulting
model to be captured in a vector space, and not
in a parametrised scoring function. Thus, we train
mappings from the Word2Vec word vectors to new
word vectors, and then apply the entailment opera-
tors in this new vector space to predict hyponymy.
Because the words in the testing set are always dis-
joint from the words in the training set, this experi-
ment measures how well the original unsupervised
vector space captures features that generalise en-
tailment across words, and not how well the map-
ping can learn about individual words.

Our objective is to learn a mapping to a new
vector space in which an operator can be applied
to predict hyponymy. We train linear mappings
for the >© operator (mapped >©) and for vector dif-
ferences (mapped dif ), since these were the best
performing proposed operator and baseline opera-
tor, respectively, in the unsupervised experiments.
We do not use the duplicated interpretations be-
cause these transforms are subsumed by the ability
to learn a linear mapping.6 Previous work on using
vector differences for semi-supervised hyponymy
detection has used a linear SVM (Vylomova et al.,
2015; Weeds et al., 2014), which is mathemati-
cally equivalent to our vector-differences model,
except that we use cross entropy loss and they use
a large-margin loss and SVM training.

The semi-supervised results in the bottom box
of table 3 show a similar pattern to the unsuper-
vised results.7 The >© operator achieves the best

6Empirical results confirm that this is in practice the case,
so we do not include these results in the table.

7It is not clear how to measure significance for cross-
validation results, so we do not attempt to do so.

generalisation from training word vectors to test-
ing word vectors. The mapped >© model has the
best accuracy, followed by the factorised entail-
ment operator mapped ⇒̃ and Weeds et al. (2014).
Direction accuracies of all the proposed operators
(mapped >©, mapped ⇒̃, mapped <©) reach into the
90’s. The dif operator performs particularly poorly
in this mapped setting, perhaps because both the
mapping and the operator are linear. These semi-
supervised results again support our distributional-
semantic interpretations of Word2Vec vectors and
their associated entailment operator >©.

6 Conclusion

In this work, we propose a vector-space model
which provides a formal foundation for a distri-
butional semantics of entailment. We developed
a mean-field approximation to probabilistic entail-
ment between vectors which represent known ver-
sus unknown features. And we used this frame-
work to derive vector operators for entailment and
vector inference equations for entailment graphs.
This framework allows us to reinterpret Word2Vec
as approximating an entailment-based distribu-
tional semantic model of words in context, and
show that more accurate interpretations result in
more accurate unsupervised models of lexical en-
tailment, achieving better accuracies than previ-
ous models. Semi-supervised evaluations confirm
these results.

A crucial distinction between the semi-
supervised models here and much previous work
is that they learn a mapping into a vector space
which represents entailment, rather than learning
a parametrised entailment classifier. Within this
new vector space, the entailment operators and
inference equations apply, thereby generalising
naturally from these lexical representations to the
compositional semantics of multi-word expres-
sions and sentences. Further work is needed to
explore the full power of these abilities to extract
information about entailment from both unla-
belled text and labelled entailment data, encode it
all in a single vector space, and efficiently perform
complex inferences about vectors and entailments.
This future work on compositional distributional
semantics should further demonstrate the full
power of the proposed framework for modelling
entailment in a vector space.

2060



References
Marco Baroni and Alessandro Lenci. 2011. How

we blessed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
GEMS ’11, pages 1–10. Association for Computa-
tional Linguistics.

Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pages 23–32, Avignon, France. Association
for Computational Linguistics.

Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391–407.

Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng
Wang, and Ting Liu. 2015. Learning semantic hi-
erarchies: A continuous vector space approach. Au-
dio, Speech, and Language Processing, IEEE/ACM
Transactions on, 23(3):461–471.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectifier neural networks. In In-
ternational Conference on Artificial Intelligence and
Statistics, pages 315–323.

James Henderson and Ivan Titov. 2010. Incre-
mental sigmoid belief networks for grammar learn-
ing. Journal of Machine Learning Research,
11(Dec):3541–3570.

Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language
Engineering, 16(4):359–389.

Germn Kruszewski, Denis Paperno, and Marco Baroni.
2015. Deriving boolean structures from distribu-
tional vectors. Transactions of the Association for
Computational Linguistics, 3:375–388.

Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics, SemEval ’12, pages
75–79. Association for Computational Linguistics.

Omer Levy and Yoav Goldberg. 2014. Neural
word embedding as implicit matrix factorization.
In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, pages
2177–2185. Curran Associates, Inc.

Omer Levy, Ido Dagan, and Jacob Goldberger. 2014.
Focused entailment graphs for open ie propositions.
In Proceedings of the Eighteenth Conference on
Computational Natural Language Learning, pages

87–97, Ann Arbor, Michigan. Association for Com-
putational Linguistics.

Omer Levy, Steffen Remus, Chris Biemann, and Ido
Dagan. 2015. Do supervised distributional meth-
ods really learn lexical inference relations? In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
970–976, Denver, Colorado, May–June. Association
for Computational Linguistics.

Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th Inter-
national Conference on Computational Linguistics -
Volume 2, COLING ’98, pages 768–774. Associa-
tion for Computational Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed repre-
sentations of words and phrases and their compo-
sitionality. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

Silvia Necsulescu, Sara Mendes, David Jurgens, Núria
Bel, and Roberto Navigli. 2015. Reading between
the lines: Overcoming data sparsity for accurate
classification of lexical relationships. In Proceed-
ings of the Fourth Joint Conference on Lexical and
Computational Semantics, pages 182–192, Denver,
Colorado. Association for Computational Linguis-
tics.

Marek Rei and Ted Briscoe. 2014. Looking for hy-
ponyms in vector space. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning, pages 68–77, Ann Arbor, Michi-
gan. Association for Computational Linguistics.

Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet selective: Supervised distributional hy-
pernymy detection. In Proceedings of COLING
2014, the 25th International Conference on Compu-
tational Linguistics: Technical Papers, pages 1025–
1036, Dublin, Ireland. Dublin City University and
Association for Computational Linguistics.

Enrico Santus, Alessandro Lenci, Qin Lu, and
Sabine Schulte im Walde. 2014. Chasing hyper-
nyms in vector spaces with entropy. In Proceed-
ings of the 14th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL 2014, April 26-30, 2014, Gothenburg, Swe-
den, pages 38–42.

Hinrich Schütze. 1993. Word space. In Advances
in Neural Information Processing Systems 5, pages
895–902. Morgan Kaufmann.

2061



Peter D. Turney and Saif M. Mohammad. 2014. Ex-
periments with three approaches to recognizing lex-
ical entailment. CoRR, abs/1401.8269.

Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. J. Artif. Int. Res., 37(1):141–188.

Luke Vilnis and Andrew McCallum. 2015. Word rep-
resentations via Gaussian embedding. In Proceed-
ings of the International Conference on Learning
Representations 2015 (ICLR).

Ekaterina Vylomova, Laura Rimell, Trevor Cohn, and
Timothy Baldwin. 2015. Take and took, gaggle and
goose, book and read: Evaluating the utility of vec-
tor differences for lexical relation learning. In CoRR
2015.

Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the 2003 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’03, pages 81–
88.

Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th International
Conference on Computational Linguistics, COLING
’04, pages 1015–1021. Association for Computa-
tional Linguistics.

Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir,
and Bill Keller. 2014. Learning to distinguish hy-
pernyms and co-hyponyms. In Proceedings of COL-
ING 2014, the 25th International Conference on
Computational Linguistics: Technical Papers, pages
2249–2259, Dublin, Ireland. Dublin City University
and Association for Computational Linguistics.

Zheng Yu, Haixun Wang, Xuemin Lin, and Min Wang.
2015. Learning term embeddings for hypernymy
identification. In Proceedings of the Twenty-Fourth
International Joint Conference on Artificial Intelli-
gence, IJCAI 2015. AAAI Press / International Joint
Conferences on Artificial Intelligence.

2062


