



















































METEOR-WSD: Improved Sense Matching in MT Evaluation


Proceedings of SSST-9, Ninth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 49–51,
Denver, Colorado, June 4, 2015. c©2015 Association for Computational Linguistics

METEOR-WSD: Improved Sense Matching in MT Evaluation

Marianna Apidianaki
LIMSI-CNRS, Orsay, France
marianna@limsi.fr

Benjamin Marie
LIMSI-CNRS, Orsay, France

Lingua et Machina, Le Chesnay, France
benjamin.marie@limsi.fr

Abstract

We present an initial experiment in integrat-
ing a disambiguation step in MT evaluation.
We show that accounting for sense distinctions
helps METEOR establish better sense corre-
spondences and improves its correlation with
human judgments of translation quality.

1 Introduction

Synonym and paraphrase support are useful means
for capturing lexical variation in Machine Trans-
lation evaluation. In the METEOR metric (Baner-
jee and Lavie, 2005), some level of abstraction
from the surface forms of words is achieved through
the “stem” and “synonymy” modules which map
words with the same stem or belonging to the
same WordNet synset (Fellbaum, 1998). METEOR-
NEXT (Denkowski and Lavie, 2010) extends se-
mantic mapping to languages other than English
and to longer text segments, using the paraphrase
tables constructed by the pivot method (Bannard
and Callison-Burch, 2005). Although both met-
rics yield improvements regarding correlation with
human judgments of translation quality compared
to the standard METEOR configuration for English,
they integrate semantic information in a rather sim-
plistic way: matching is performed without disam-
biguation, which means that all the variants avail-
able for a particular text fragment are treated as se-
mantically equivalent. This is however not always
the case, as synonyms found in different WordNet
synsets correspond to different senses. Similarly,
paraphrase sets obtained by the pivot method of-

ten group phrases describing different senses (Apid-
ianaki et al., 2014). In these cases, a word sense dis-
ambiguation (WSD) step would help to identify the
correct synset or subset of paraphrases for a word
or phrase in context and avoid erroneous matchings
between text segments carrying different senses. We
present an initial experiment on the integration of
a disambiguation step in the METEOR metric and
show how it helps increase correlation with human
judgments of translation quality.

2 Disambiguation in METEOR

We apply the metric to translations of news texts
from the five languages involved in the WMT14
Metrics Shared Task (Machacek and Bojar, 2014)
(French, Hindi, German, Czech, Russian) into En-
glish. We disambiguate the English references – dif-
ferent for each language pair – using the Babelfy
tool (Moro et al., 2014), which performs graph-
based WSD by exploiting the structure of the mul-
tilingual network BabelNet (Navigli and Ponzetto,
2012). The assigned annotations are multilingual
synsets grouping word and phrase variants in differ-
ent languages coming from various sources (Word-
Net, Wikipedia, etc.) and carrying the same sense.
We use the WordNet literals found in the sense se-
lected by Babelfy to filter the WordNet synonym sets
used in METEOR and prevent METEOR from con-
sidering erroneous matchings as correct.1 As a re-
sult, only the synonyms found in the proposed Ba-
belNet synset are kept and considered as correct by
METEOR, while synonyms corresponding to other

1In future work, we intend to apply the same filtering to
paraphrases in different languages.

49



Figure 1: Good and erroneous matchings made by the synonymy module and WSD.

senses are discarded. METEOR is a tunable metric
able to assign a weight to each of its modules in or-
der to better correlate with human judgments. Since
METEOR needs to perform a costly grid-search on 8
parameters, we did not re-optimize the weights due
to time constraints. Considering this, the following
experiments are made in a suboptimal configuration
as we can expect a re-optimization to take the impact
of the disambiguation into account more efficiently.

3 Results

In Table 1, we present the results obtained for four
different configurations: METEOR with WordNet
synonym support vs METEOR with WSD, with and
without paraphrasing. The scores correspond to
segment-level Kentall τ correlations of the metric
with human judgments of translation quality. When
the paraphrase module is activated, WSD slightly im-
proves the correlation of the metric to human judg-
ments in all languages except for Czech. Never-
theless, it is worth noting that this would improve
METEOR’s ranking in the results of the WMT14
shared task for French-English, which would then
be ranked 4th, instead of 7th, among 18 participants.

When the WSD prediction is correct, it permits to
avoid erroneous matchings between synonyms cor-
responding to different WordNet senses. In the ex-
ample given in Figure 1, the synonymy module cre-
ates a wrong mapping between sound and voice. As
sound is not contained in the BabelNet synset se-
lected by the WSD component, this avoids establish-
ing an erroneous match. Given, however, that WSD
does not always succeed, the paraphrase module
manages to find correspondences in cases of wrong
disambiguation choices. This is the case illustrated
by the first annotation in Figure 1 where the synset
proposed by the WSD tool describes the “transport”
sense. This wrong WSD prediction establishes no

METEOR configuration fr-en de-en hi-en cs-en ru-en

w/ par. METEOR .406 .334 .420 .282 .329METEOR-WSD .410 .335 .422 .278 .331

w/o par. METEOR .400 .326 .401 .271 .313METEOR-WSD .403 .321 .396 .263 .312

Table 1: Segment-level Kendall’s τ correlations be-
tween METEOR and the official human judgments of the
WMT14 metrics shared task.

match but the paraphrase module that operates af-
ter WSD, manages to map carrying and conducting.
When the paraphrase module is deactivated, the cor-
relation of METEOR-WSD is lower than that of the
basic METEOR configuration. Although the disam-
biguation discards erroneous matchings made by the
synonymy module, there is no means to correct er-
roneous disambiguation choices without the para-
phrases.

4 Conclusion and Perspectives

Our results demonstrate the beneficial impact of dis-
ambiguation in MT evaluation. Accounting for sense
distinctions helps METEOR establish better qual-
ity correspondences between hypotheses and human
references. In future work, we intend to experiment
with other WSD methods such as the alignment-
based method recently proposed by Apidianaki and
Gong (2015). Moreover, we plan to integrate a WSD
step in evaluation for languages other than English.
We expect to observe substantial improvements in
languages where the synonymy module is unavail-
able and where the quality of pivot paraphrases is
lower than in English. We also plan to conduct ex-
periments using METEOR-WSD for tuning a Statis-
tical Machine Translation system and expect to ob-
serve improvements in translation quality compared
to the same system tuned with METEOR without
WSD.

50



References
Marianna Apidianaki and Li Gong. 2015. LIMSI: Trans-

lations as Source of Indirect Supervision for Mul-
tilingual All-Words Sense Disambiguation and En-
tity Linking. In Proceedings of the 9th International
Workshop on Semantic Evaluation (SemEval-2015),
Denver, Colorado, USA.

Marianna Apidianaki, Emilia Verzeni, and Diana Mc-
carthy. 2014. Semantic Clustering of Pivot Para-
phrases. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation
(LREC’14), Reykjavik, Iceland.

Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65–72, Ann Arbor,
Michigan, USA.

Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL’05), pages 597–604,
Ann Arbor, Michigan, USA.

Michael Denkowski and Alon Lavie. 2010. Extending
the METEOR Machine Translation Evaluation Met-
ric to the Phrase Level. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 250–253, Los Angeles, Cali-
fornia, USA.

Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.

Matous Machacek and Ondrej Bojar. 2014. Results of
the WMT14 Metrics Shared Task. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, pages 293–301, Baltimore, Maryland, USA.

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Dis-
ambiguation: a Unified Approach. Transactions of
the Association for Computational Linguistics (TACL),
2:231–244.

Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The Automatic Construction, Evaluation and
Application of a Wide-Coverage Multilingual Seman-
tic Network. Artificial Intelligence, 193:217–250.

51


