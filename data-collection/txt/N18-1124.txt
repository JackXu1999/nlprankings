



















































Self-Attentive Residual Decoder for Neural Machine Translation


Proceedings of NAACL-HLT 2018, pages 1366–1379
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Self-Attentive Residual Decoder for Neural Machine Translation

Lesly Miculicich Werlen*,†, Nikolaos Pappas*, Dhananjay Ram*,†,
Andrei Popescu-Belis‡

*Idiap Research Institute, Switzerland,
†École polytechnique fédérale de Lausanne (EPFL), Switzerland,

‡HEIG-VD / HES-SO, Switzerland
{lmiculicich, npappas, dram}@idiap.ch

andrei.popescu-belis@heig-vd.ch

Abstract

Neural sequence-to-sequence networks with
attention have achieved remarkable perfor-
mance for machine translation. One of the rea-
sons for their effectiveness is their ability to
capture relevant source-side contextual infor-
mation at each time-step prediction through an
attention mechanism. However, the target-side
context is solely based on the sequence model
which, in practice, is prone to a recency bias
and lacks the ability to capture effectively non-
sequential dependencies among words. To ad-
dress this limitation, we propose a target-side-
attentive residual recurrent network for decod-
ing, where attention over previous words con-
tributes directly to the prediction of the next
word. The residual learning facilitates the flow
of information from the distant past and is able
to emphasize any of the previously translated
words, hence it gains access to a wider context.
The proposed model outperforms a neural MT
baseline as well as a memory and self-attention
network on three language pairs. The analysis
of the attention learned by the decoder con-
firms that it emphasizes a wider context, and
that it captures syntactic-like structures.

1 Introduction

Neural machine translation (NMT) has recently
become the state-of-the-art approach to machine
translation (Bojar et al., 2016). Several architec-
tures have been proposed for this task (Kalchbren-
ner and Blunsom, 2013; Sutskever et al., 2014;
Cho et al., 2014; Gehring et al., 2017; Vaswani
et al., 2017), but the attention-based NMT model
designed by Bahdanau et al. (2015) is still con-
sidered the de-facto baseline. This architecture
is composed of two recurrent neural networks
(RNNs), an encoder and a decoder, and an at-
tention mechanism between them for modeling a

(a) Baseline NMT decoder (b) Self-attentive residual dec.

Figure 1: Comparison between the decoder of the base-
line NMT and the proposed decoder with self-attentive
residual connections.

soft word-alignment. First, the model encodes the
complete source sentence, and then decodes one
word at a time. The decoder has access to all the
context on the source side through the attention
mechanism. However, on the target side, the con-
textual information is represented only through a
fixed-length vector, namely the hidden state of the
decoder. As observed by Bahdanau et al. (2015),
this creates a bottleneck which hinders the ability
of the sequential model to learn longer-term infor-
mation effectively.

As pointed out by Cheng et al. (2016), sequen-
tial models present two main problems for natural
language processing. First, the memory of the en-
coder is shared across multiple words and is prone
to bias towards the recent past. Second, such mod-
els do not fully capture the structural composition
of language. To address these limitations, several
recent models have been proposed, namely mem-
ory networks (Cheng et al., 2016; Tran et al., 2016;
Wang et al., 2016) and self-attention networks
(Daniluk et al., 2016; Liu and Lapata, 2018). We
experimented with these methods, applying them
to NMT: memory RNN (Cheng et al., 2016) and
self-attentive RNN (Daniluk et al., 2016). How-

1366



ever, we observed no significant gains in perfor-
mance over the baseline architecture.

In this paper, we propose a self-attentive resid-
ual recurrent decoder, presented in Figure 1b,
which, if unfolded over time, represents a densely-
connected residual network. The self-attentive
residual connections focus selectively on previ-
ously translated words and propagate useful in-
formation to the output of the decoder, within
an attention-based NMT architecture. The at-
tention paid to the previously predicted words is
analogous to a read-only memory operation, and
enables the learning of syntactic-like structures
which are useful for the translation task.

Our evaluation on three language pairs shows
that the proposed model improves over several
baselines, with only a small increase in compu-
tational overhead. In contrast, other similar ap-
proaches have lower scores but a higher compu-
tational overhead. The contributions of this paper
can be summarized as follows:

• We propose and compare several options for
using self-attentive residual learning within a
standard decoder, which facilitates the flow of
contextual information on the target side.
• We demonstrate consistent improvements over

a standard baseline, and two advanced variants,
which make use of memory and self-attention
on three language pairs (English-to-Chinese,
Spanish-to-English, and English-to-German).
• We perform an ablation study and analyze the

learned attention function, providing additional
insights on its actual contributions.

2 Related Work

Several studies have been proposed to enhance
sequential models by capturing longer contexts.
Long short-term memory (LSTM) (Hochreiter and
Schmidhuber, 1997) is the most commonly used
recurrent neural network (RNN), because its in-
ternal memory allows to retain information from
a more distant past than a vanilla RNN. Several
studies attempt to increase the memory capacity
of LSTMs by using memory networks (Weston
et al., 2015; Sukhbaatar et al., 2015). For instance,
Cheng et al. (2016) incorporate different mem-
ory cells for each previous output representation,
which are later accessed by an attention mecha-
nism. Tran et al. (2016) include a memory block
to access recent input words in a selective manner.
Both methods show improvements on language

modeling. For NMT, Wang et al. (2016) presented
a decoder enhanced with an external shared mem-
ory. Memory networks extend the capacity of the
network and have the potential to read, write, and
forget information. Our method, which attends
over previously predicted words, can be seen as a
read-only memory, which is simpler but computa-
tionally more efficient because it does not require
additional memory space.

Other studies aim to improve the modeling
of source-side contextual information, for exam-
ple through a context-aware encoder using self-
attention (Zhang et al., 2017), or a recurrent atten-
tion NMT (Yang et al., 2017) that is aware of pre-
viously attended words on the source-side in or-
der to better predict which words will be attended
in future. Additionally, variational NMT (Zhang
et al., 2016a) introduces a latent variable to model
the underlying semantics of source sentences. In
contrast to these studies, we focus instead on the
contextual information on the target side.

The application of self-attention mechanisms to
RNNs have been previously studied, and in gen-
eral, they seem to capture syntactic dependen-
cies among distant words (Liu and Lapata, 2018;
Soltani and Jiang, 2016; Lee et al., 2017; Lin et al.,
2017). Daniluk et al. (2016) explore different ap-
proaches to self-attention for language modeling,
leading to improvements over a baseline LSTM
and over memory-augmented methods. However,
the methods do not fully utilize a longer context.
The main difference with our approach is that we
apply attention on the output embeddings rather
than the hidden states. Thus, the connections are
independent of the recurrent layer representations,
which is beneficial to NMT, as we show below.

Our model relies on residual connections,
which have been shown to improve the learning
process of deep neural networks by addressing
the vanishing gradient problem (He et al., 2016).
These connections create a direct path from pre-
vious layers, helping the transmission of informa-
tion. Recently, several architectures using resid-
ual connections with LSTMs have been proposed
for sequence prediction (Zhang et al., 2016b; Kim
et al., 2017; Zilly et al., 2017; Wang and Tian,
2016). To our knowledge, our study is the first one
to use self-attentive residual connections within
residual RNNs for NMT. In parallel to our study,
a similar method was recently proposed for senti-
ment analysis (Wang, 2017).

1367



3 Background: Neural Machine
Translation

Neural machine translation aims to compute the
conditional distribution of emitting a sentence in
a target language given a sentence in a source
language, denoted by pΘ(y|x), where Θ is the
set of parameters of the neural model, and y =
{y1, ..., yn} and x = {x1, ..., xm} are respectively
the representations of source and target sentences
as sequences of words. The parameters Θ are
learned by training a sequence-to-sequence neural
model on a corpus of parallel sentences. In par-
ticular, the learning objective is to maximize the
following conditional log-likelihood:

max
Θ

1

N

N∑

n=1

log(pΘ(y|x)) (1)

The models typically use gated recurrent units
(GRUs) (Cho et al., 2014) or LSTMs (Hochreiter
and Schmidhuber, 1997). Their architecture has
three main components: an encoder, a decoder,
and an attention mechanism.

The goal of the encoder is to build meaningful
representations of the source sentences. It consists
of a bidirectional RNN which includes contextual
information from past and future words into the
vector representation hi of a particular word vector
xi, formally defined as follows:

hi = [
−→
hi ,
←−
hi ] (2)

Here,
−→
hi = f(xi, hi−1) and

←−
hi = f(xi, hi+1)

are the hidden states of the forward and backward
passes of the bidirectional RNN respectively, and
f is a non-linear function.

The decoder (see Figure 1a) is in essence a re-
current language model. At each time step, it pre-
dicts a target word yt conditioned over the previ-
ous words and the information from the encoder
using the following posterior probability:

p(yt|y1, ..., yt−1, ct) ≈ g(st, yt−1, ct) (3)

where g is a non-linear multilayer function. The
hidden state of the decoder st is defined as:

st = f(st−1, yt−1, ct) (4)

and depends on a context vector ct that is com-
puted by the attention mechanism.

The attention mechanism allows the decoder to
select which parts of the source sentence are more

useful to predict the next output word. This goal is
achieved by considering a weighted sum over all
hidden states of the encoder as follows:

ct =
m∑

i=1

αtihi (5)

where αti is a weight calculated using a normalized
exponential function a, also known as alignment
function, which computes how good is the match
between the input at position i ∈ {1, ..., n} and
the output at position t:

αti = softmax(e
t
i) (6)

eti = a(st−1, hi) (7)

Different types of alignment functions have been
used for NMT, as investigated by Luong et al.
(2015). Here, we use the one originally defined
by Bahdanau et al. (2015).

4 Self-Attentive Residual Decoder

The decoder of the attention-based NMT model
uses a skip connection from the previously pre-
dicted word to the output classifier in order to en-
hance the performance of translation. As we can
see in Eq. (3), the probability of a particular word
is calculated by a function g which takes as input
the hidden state of the recurrent layer st, the rep-
resentation of the previously predicted word yt−1,
and the context vector ct. Within g, these quanti-
ties are typically summed up after going through
simple linear transformations, hence the addition
of yt−1 is indeed a skip connection as in residual
networks (He et al., 2016). In theory, st should be
sufficient for predicting the next word given that it
is dependent on the other two local-context com-
ponents according to Eq. (4). However, the yt−1
quantity makes the model emphasize the last pre-
dicted word for generating the next word. How
can we make the model consider a broader con-
text?

To answer this question, we propose to include
into the decoder’s formula skip connections not
only from the previous time step yt−1, but from all
previous time steps from y0 to yt−1. This defines
a residual recurrent network which, unfolded over
time, can be seen as a densely connected residual
network. These connections are applied to all pre-
viously predicted words, and reinforce the mem-
ory of the recurrent layer towards what has been
translated so far. At each time step, the model

1368



decides which of the previously predicted words
should be emphasized to predict the next one. In
order to deal with the dynamic length of this new
input, we use a target-side summary vector dt that
can be interpreted as the representation of the de-
coded sentence until the time t in the word embed-
ding space. We therefore modify Eq. (3) replacing
yt−1 with dt:

p(yt|y1, ..., yt−1, ct) ≈ g(st, dt, ct) (8)

The replacement of yt−1 with dt means that the
number of parameters added to the model is de-
pendent only on the calculation of dt. Figure 1b
illustrates the change made to the decoder. We de-
fine two methods for summarizing the context into
dt, which are described in the following sections.

4.1 Mean Residual Connections
One simple way to aggregate information from
multiple word embeddings is by averaging them.
This average can be seen as the sentence represen-
tation until time t. We hypothesize that this repre-
sentation is more informative than using only the
embedding of the previous word. Formally:

davgt =
1

t− 1
t−1∑

i=1

yi (9)

4.2 Self-Attentive Residual Connections
Averaging is a simple and cheap way to aggregate
information from multiple words, but may not be
sufficient for all kinds of dependencies. Instead,
we propose a dynamic way to aggregate informa-
tion in each sentence, such that different words
have different importance according to their re-
lation with the prediction of the next word. We
propose to use a shared self-attention mechanism
to obtain a summary representation of the transla-
tion, i.e. a weighted average representation of the
words translated from y0 to yt−1. This mechanism
aims to model, in part, important non-sequential
dependencies among words, and serves as a com-
plementary memory to the recurrent layer.

dcavgt =
t−1∑

i=1

αtiyi (10)

αti = softmax(e
t
i) (11)

The weights of the attention model are computed
by a scoring function eti that predicts how impor-
tant each previous word (y0, ..., or yt−1) is for the
current prediction yt.

We experiment with two different scoring func-
tions, as follows:

eti = v
ᵀtanh(Wyyi +Wsst) (content+scope) (12)

or eti = v
ᵀtanh(Wyyi) (content) (13)

where v ∈ Re, Wy ∈ Re×e, and Ws ∈ Re×d
are weight matrices, e and d are the dimensions
of the embeddings and hidden states respectively.
Firstly, we study the scoring function noted con-
tent+scope, as proposed by Bahdanau et al. (2015)
for NMT. Secondly, we explore a scoring func-
tion noted as content, which is calculated based
only on the previous hidden states of the decoder,
as proposed by Pappas and Popescu-Belis (2017).
In contrast to the first attention function, which
makes use of the hidden vector st, the second one
is based only on the previous word representa-
tions, therefore, it is independent of the current
prediction representation. However, the normal-
ization of this function still depends on t.

5 Other Self-Attentive Networks

To compare our approach with similar studies, we
adapted two representative self-attentive networks
for application to NMT.

5.1 Memory RNN
The Memory RNN decoder is based on the pro-
posal by Cheng et al. (2016) to modify an LSTM
layer to include a memory with different cells for
each previous output representation. Thus at each
time step, the hidden layer can select past infor-
mation dynamically from the memory. To adapt it
to our framework, we modify Eq. (4) as:

st = f(s̃t, yt−1, ct) (14)

where s̃t =
t−1∑

i=1

αtisi (15)

αti = softmax(e
t
i) (16)

eti = a(hi, yt−1, s̃t−1) (17)

5.2 Self-Attentive RNN
The Self-Attentive RNN is the simplest one pro-
posed by Daniluk et al. (2016), and incorporates a
summary vector from past predictions calculated
with an attention mechanism. Here, the attention
is applied over previous hidden states. This de-
coder is formulated as follows:

p(yt|y1, ..., yt−1, ct) ≈ g(st, yt−1, ct, s̃t) (18)

1369



where s̃t =
t−1∑

i=1

αtisi (19)

αti = softmax(e
t
i) (20)

eti = a(si, st) (21)

Additional details of the formulations in Sec-
tions 3, 4, and 5 are described in the Appendix A.

6 Experimental Settings

6.1 Datasets

To evaluate the proposed MT models in differ-
ent conditions, we select three language pairs
with increasing amounts of training data: English-
Chinese (0.5M sentence pairs), Spanish-English
(2.1M), and English-German (4.5M).

For English-to-Chinese, we use a subset of the
UN parallel corpus (Rafalovitch and Dale, 2009)1,
with 0.5M sentence pairs for training, 2K for
development, and 2K for testing. For training
Spanish-to-English MT, we use a subset of WMT
2013 (Bojar et al., 2013), corresponding to Eu-
roparl v7 and News Commentary v11 with ca.
2.1M sentence pairs. Newstest2012 and New-
stest2013 were used for development and test-
ing respectively. Finally, we use the complete
English-to-German set from WMT 2016 (Bojar
et al., 2016) with a total of ca. 4.5M sentence
pairs. The development set is Newstest2013, and
the testing set is Newstest2014. Additionally, we
include as testing sets Newstest2015 and New-
stest2016, for comparison with the state of the
art. We report translation quality using (a) BLEU
over tokenized and truecased texts, and (b) NIST
BLEU over detokenized and detruecased texts2.

6.2 Model Configuration

We use the implementation of the attention-based
NMT baseline provided in dl4mt-tutorial3

developed in Python using Theano (Theano De-
velopment Team, 2016). The system imple-
ments an attention-based NMT model, described
above, using one layer of GRUs (Cho et al.,
2014). The vocabulary size is 25K for English-
to-Chinese NMT, and 50K for Spanish-to-English
and English-German. We use the byte pair encod-
ing (BPE) strategy for out-of-vocabulary words

1http://www.uncorpora.org/
2Scrips from Moses toolkit (Koehn et al., 2007): BLEU multi-bleu,
NIST BLEU mteval-v13a.pl, tokenizer.perl, truecase.perl.

3https://github.com/nyu-dl/dl4mt-tutorial

|Θ||Θ||Θ| BLEU
Models En–Zh Es–En

SMT baseline – 21.6 25.2
NMT baseline 108.7M 22.6 25.4
+ Memory RNN 109.7M 22.5 25.5
+ Self-attentive RNN 110.2M 22.0 25.1
+ Mean residual connections 108.7M 23.6 25.7
+ Self-attentive residual connections 108.9M 24.0 26.3

Table 1: BLEU score (multi-bleu) on tokenized text.
The highest score per dataset is marked in bold. The
self-attentive residual connections make use of the con-
tent attention function. |Θ| indicates the number of pa-
rameters per model.

(Sennrich et al., 2016b). For all cases, the maxi-
mum sentence length of the training samples is 50,
the dimension of the word embeddings is 500, and
the dimension of the hidden layers is 1,024. We
use dropout with a probability of 0.5 after each
layer. The parameters of the models are initial-
ized randomly from a standard normal distribu-
tion scaled to a factor of 0.01. The loss function
is optimized using Adadelta (Zeiler, 2012) with
� = 10−6 and ρ = 0.95 as in the original paper.
The systems were trained in 7–12 days for each
model on a Tesla K40 GPU at the speed of about
1,000 words/sec.

7 Analysis of the Results

Table 1 shows the BLEU scores and the number
of parameters used by the different NMT models.
Along with the NMT baseline, we included a sta-
tistical machine translation (SMT) model based on
Moses (Koehn et al., 2007) with the same train-
ing/tuning/test data as the NMT. The performance
of memory RNN is similar to the baseline and, as
confirmed later, its focus of attention is mainly on
the prediction at t − 1. The self-attentive RNN
method is inferior to the baseline, which can be
attributed to the overhead on the hidden vectors
that have to learn the recurrent representations and
the attention simultaneously. The proposed mod-
els outperform the baseline, and the best scores
are obtained by the NMT model with self-attentive
residual connections. Despite their simplicity, the
mean residual connections already improve the
translation, without increasing the number of pa-
rameters.

Tables 2 and 3 show further experiments with
the proposed methods on various English-German
test sets, compared to several previous systems.

1370



BLEU
Models NT14 NT15

NMT (unk. word repl.) (Luong et al., 2015) 20.9 –
Context-aware NMT (Zhang et al., 2017) 22.57 –
Recurrent attention NMT (Yang et al., 2017) 22.1 25.0
Variational NMT (Zhang et al., 2016a) – 25.49
NMT baseline 22.3 24.8
+ Memory RNN 22.6 24.9
+ Self-attentive RNN 22.0 24.3
+ Mean residual connections 22.9 24.9
+ Self-attentive residual connections 23.2 25.5

Table 2: BLEU score (multi-bleu) on tokenized text for
English-to-German on Newstest (NT) 2014, and 2015.
The highest score per dataset is marked in bold. The
self-attentive residual connections makes use of the
content attention function.

BLEU (NIST)
Models NT14 NT15 NT16

Winning WMT 20.1 24.4 34.2
NMT (BPE) (Sennrich et al., 2016b) – 22.8 –
Syntax NMT (Nadejde et al., 2017) – – 29.0
NMT Baseline 21.0 24.4 28.8
+ Mean residual connections* 21.4 24.7 29.6
+ Self-attentive residual connections** 21.7 25.0 29.7

Table 3: NIST BLEU scores on detokenized and de-
truecased text for English-to-German on Newstest (NT)
2014, 2015, 2016. Significance test: * p < 0.05, **
p < 0.01. The Winning WMT systems are listed in the
text below.

Table 2 shows BLEU values calculated by multi-
bleu, and includes the NMT system proposed by
Luong et al. (2015) which replaces unknown pre-
dicted words with the most strongly aligned word
on the source sentence. Also, the table includes
other systems described in Section 2. Addition-
ally, Table 3 shows values calculated by the NIST
BLEU scorer, as well as results reported by the
“Winning WMT” systems for each test set re-
spectively: UEDIN-SYNTAX (Williams et al.,
2014), UEDIN-SYNTAX (Williams et al., 2015),
and UEDIN-NMT (Sennrich et al., 2016a). Also,
we include the results reported by Sennrich et al.
(2016b) for a baseline encoder-decoder NMT with
BPE for unknown words similar to our configu-
ration, and finally the system proposed by Nade-
jde et al. (2017), an explicit syntax-aware NMT
that introduces combinatory categorial grammar
(CCG) supertags on the target side by predicting
words and tags alternately. The comparison with
this work is relevant for the analysis described

BLEU
Attention function En-Zh Es-En
Content+Scope 23.1 25.6
Content 24.0 26.3

Table 4: BLEU scores for two scoring variants of the
attention function of the proposed decoder.

later in Section 8.2. The results confirm that the
self-attentive residual connections improve signif-
icantly the translations. To evaluate the signifi-
cance of the improvements against the NMT base-
line, we performed a one-tailed paired t-test.

7.1 Impact of the Attention Function

We now examine the two scoring functions that
can be used for the self-attentive residual con-
nections model presented in Eq. (12), considering
English-to-Chinese and Spanish-to-English. The
BLEU scores are presented in Table 4: the best
option is the content matching function, which de-
pends only on the word embeddings. The con-
tent+scope function, which depends additionally
on the hidden representation of the current pre-
diction is better than the baseline but scores lower
than content. The idea that the importance of the
context depends on the current prediction is ap-
pealing, because it can be interpreted as learning
internal dependencies among words. However, the
experimental results show that it does not neces-
sarily lead to the best translation. On the contrary,
the content attention function may be extracting
representations of the whole sentence which are
easier to learn and generalize.

7.2 Performance According to Human
Evaluation

Manual evaluation on samples of 50 sentences
for each language pair helped to corroborate the
conclusions obtained from the BLEU scores, and
to provide a qualitative understanding of the im-
provements brought by our model. For each lan-
guage, we employed one evaluator who was a na-
tive speaker of the target language and had good
knowledge of the source language. The evalua-
tors ranked three translations of the same source
sentence – one from each of our models: base-
line, mean residual connections, and self-attentive
residual connections – according to their transla-
tion quality. The three translations were presented
in a random order, so that the system that had gen-
erated them could not be identified. To integrate

1371



Ranking (%)
System En–Zh Es–En En–De

> = < > = < > = <

Mean vs. Baseline 26 56 18 20 64 16 28 58 24
Self-attentive vs. Baseline 28 60 12 28 56 16 32 54 14
Self-attentive vs. Mean 24 62 14 28 58 14 32 56 12

Table 5: Human evaluation of sentence-level transla-
tion quality on three language pairs. We compare the
models in pairs, indicating the percentages of sentences
that were ranked higher (>), equal to (=), or lower (<)
for the first system with respect to the second one. The
values correspond to percentages (%).

Systems d Perplexity
LSTM (Daniluk et al., 2016) 300 85.2
LSTM + Attention (Daniluk et al., 2016) 296 82.0
LSTM + 4-gram (Daniluk et al., 2016) 968 75.9
LSTM + Mean residual connections 296 80.2
LSTM + Self-attentive residual connections 296 80.4

Table 6: Evaluation of the proposed methods on lan-
guage modeling. The number of parameter for all mod-
els is 47M.

the judgments, we proceed in pairs, and count the
number of times each system was ranked higher,
equal to, or lower than another competing sys-
tem. The results shown in Table 5 indicate that
the self-attentive residual connections model out-
performs the one with mean residual connections,
and both outperform the baseline, for all three lan-
guage pairs. The rankings are thus identical to
those obtained using BLEU in Tables 1 and 3.

7.3 Performance on Language Modeling
To examine whether language modeling (LM) can
benefit from the proposed method, we incorporate
the residual connections into a neural LM. We use
the same setting as Daniluk et al. (2016) for a cor-
pus of Wikipedia articles (22.5M words), and we
compare with two methods proposed in the same
paper, namely attention LSTM and 4-gram LSTM.
As shown in Table 6, the proposed models out-
perform the LSTM baseline as well as the self-
attention model, but not the 4-gram LSTM. Ex-
periments using 4-gram LSTM for NMT showed
poor performance (13.9 BLEU points for English-
Chinese) which can be attributed to the difference
between the LM and NMT tasks. Both tasks pre-
dict one word at a time conditioned over previ-
ous words, however, in NMT the previous target-
word-inputs are not given, they have to be gener-
ated by the decoder. Thus, the output could be
conditioned over previous erroneous predictions

Figure 2: Percentage of words that received maximum
attention at a given relative position, ranging from −1
to −50 (maximum length).

affecting in higher proportion the 4-gram LSTM
model. This shows that even if a model improves
language modeling, it does not necessarily im-
prove machine translation.

8 Qualitative Analysis

8.1 Distribution of Attention

Figure 2 shows a comparison of the distribution of
attention of the different self-attentive models de-
scribed in this paper, on Spanish-to-English NMT
(the other two language pairs exhibit similar distri-
butions). The values correspond to the number of
words which received maximal attention for each
relative position (x-axis). We selected, at each pre-
diction, the preceding word with maximal weight,
and counted its relative position. We normalized
the count by the number of previous words at the
time of each prediction.

We observe that the memory RNN almost al-
ways selects the immediately previous word (t−1)
and ignores the rest of the context. On the con-
trary, the other two models distribute attention
more evenly among all previous words. In partic-
ular, the self-attentive RNN uses a longer context
than the self-attentive residual connections but, as
the performance on BLEU score shows, this fact
does not necessarily mean better translation.

Figure 3 shows the attention to previous words
generated by each model for one sentence trans-
lated from Spanish to English. The matrices
present the target-side attention weights, with the
vertical axis indicating the previous words, and the
color shades at each position (cell) representing
the attention weights. The weights of the mem-
ory RNN are concentrated on the diagonal, indi-
cating that the attention is generally located on

1372



(a) Memory RNN (b) Self-attentive RNN (c) Self-attentive residual connections

Figure 3: Matrix of distribution of the attention weights to previous words. The vertical axis represents the previous
words. A darker shade indicates a higher attention weight.

Algorithm 1 Binary Parse Tree
Require: A matrix of attention of size N ×N
Require: s sentence as list of words of size N

1: function SPLIT(tree,A, s)
2: n← length(s)
3: i← 0
4: while max(A[:][i]) = 0 or i < n do
5: i← i + 1
6: end while
7: tree.addChild(s[0 : i])
8: if i < n then
9: subtree← newTree()

10: SPLIT(subtree,A[i : n][i : n], s[i : n]))
11: tree.addChild(subtree)
12: end if
13: end function
14: tree← newTree(); SPLIT(tree,A, s)

the previous word, which makes the model al-
most equivalent to the baseline. The weights of
the self-attentive RNN show that attention is more
distributed towards the distant past, and they vary
for each word because the attention function de-
pends on the current prediction. This model tries
to find dependencies among words, although com-
plex relations seem difficult to learn. On the con-
trary, the proposed self-attentive residual connec-
tions model strongly focuses on particular words,
and we present a wider analysis of it in the follow-
ing section.

8.2 Structures Learned by the Model

When visualizing the matrix of attention weights
generated by our model (Figure 3c), we observed
the formation of sub-phrases which are grouped
depending on their attention to previous words.
To build the sub-phrases in a deterministic fash-
ion, we implemented Algorithm 1, which itera-
tively splits the sentence into two sub-phrases ev-
ery time the focus of attention changes to a new
word, from left-to-right. The results are binary
tree structures containing the sub-phrases, exem-

Figure 4: Examples of hypothesized syntactic struc-
tures obtained with Algorithm 1.

plified in Figure 4.
We formally evaluate the syntactic properties

of the binary tree structures by comparing them
with the results of an automatic constituent parser
(Manning et al., 2014), using the ParsEval ap-
proach (Black et al., 1991), i.e. by counting the
precision and recall of constituents, excluding sin-
gle words. Our models reaches a precision of 0.56,
which is better than the precision of 0.45 obtained
by a trivial right-branched tree model4. Note that
these structures were neither optimized for pars-
ing nor learned using part-of-speech tagging as
most parsers do. Our interpretation of the results
is that they are “syntactic-like” structures. How-
ever, given the simplicity of the model, they could

4A model constructed by dividing iteratively one word and the rest
of the sentence, from left-to-right.

1373



Better than baseline
S: Estudiantes y profesores se están tomando a la ligera la

fecha.
R: Students and teachers are taking the date lightly.
B: Students and teachers are being taken lightly to the

date.
O: Students and teachers are taking the date lightly.
S: No porque compartiera su ideologı́a, sino porque para

él los Derechos Humanos son indivisibles.
R: Not because he shared their world view, but because for

him, human rights are indivisible.
B: Not because I share his ideology, but because he is in-

divisible by human rights.
O: Not because he shared his ideology, but because for

him human rights are indivisible.
Worse than baseline

S: El gobierno intenta que no se construyan tantas casas
pequeñas.

R: The Government is trying not to build so many small
houses.

B: The government is trying not to build so many small
houses.

O: The government is trying to ensure that so many
small houses are not built.

S: Otras personas pueden tener niños .
R: Other people can have children.
B: Other people can have children.
O: Others may have children.

Table 7: Examples from Spanish to English.

also be viewed as more limited structures, similar
to sentence chunks.

8.3 Translation Examples
Table 7 shows examples of translations produced
with the baseline and the self-attentive residual
connections model. The first part shows examples
for which the proposed model reached a higher
BLEU score than the baseline. Here, the structure
of the sentences, or at least the word order, are im-
proved. The second part contains examples where
the baseline achieved better BLEU score than our
model. In the first example, the structure of the
sentence is different but the content and quality
are similar, while in the second one lexical choices
differ from the reference.

9 Conclusion

We presented a novel decoder which uses self-
attentive residual connections to previously trans-
lated words in order to enrich the target-side con-
textual information in NMT. To cope with the vari-
able lengths of previous predictions, we proposed
two methods for context summarization: mean
residual connections and self-attentive residual

connections. Additionally, we showed how sim-
ilar previous proposals, designed for language
modeling, can be adapted to NMT. We evaluated
the methods over three language pairs: Chinese-
to-English, Spanish-to-English, and English-to-
German. In each case, we improved the BLEU
score compared to the NMT baseline and two vari-
ants with memory-augmented decoders. A man-
ual evaluation over a small set of sentences for
each language pair confirmed the improvement.
Finally, a qualitative analysis showed that the pro-
posed model distributes weights throughout an
entire sentence, and learns structures resembling
syntactic ones.

As future work, we plan to enrich the present at-
tention mechanism with the key-value-prediction
technique (Daniluk et al., 2016; Miller et al., 2016)
which was shown to be useful for language mod-
eling. Moreover, we will incorporate relative po-
sitional information to the attention function. To
encourage further research in self-attentive resid-
ual connections for NMT an other similar tasks,
our code is made publicly available5.

Acknowledgments

We are grateful for support to the European Union
under the Horizon 2020 SUMMA project (grant n.
688139, see www.summa-project.eu). We
would also like to thank James Henderson for his
valuable feedback and suggestions.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the International Conference on Learning Represen-
tations. San Diego, USA.

Ezra W. Black, Steven Abney, Daniel P. Flickenger,
Claudia Gdaniec, Ralph Grishman, Philip Harri-
son, Donald Hindle, Robert J. P. Ingria, Freder-
ick Jelinek, Judith L. Klavans, Mark Y. Liberman,
Mitchell P. Marcus, Salim Roukos, Beatrice San-
torini, and Tomek Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage
of English grammars. In Speech and Natural Lan-
guage: Proceedings of a Workshop Held at Pacific
Grove. California, USA. http://www.aclweb.
org/anthology/H91-1060.

5This work is part of the project Towards Document-Level Neu-
ral Machine Translation (Miculicich Werlen, 2017). The code
is available at https://github.com/idiap/Attentive_
Residual_Connections_NMT

1374



Ondřej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation. Association for Computational
Linguistics, Sofia, Bulgaria, pages 1–44. http://
www.aclweb.org/anthology/W13-2201.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck,
Antonio Jimeno Yepes, Philipp Koehn, Varvara
Logacheva, Christof Monz, Matteo Negri, Aure-
lie Neveol, Mariana Neves, Martin Popel, Matt
Post, Raphael Rubino, Carolina Scarton, Lucia Spe-
cia, Marco Turchi, Karin Verspoor, and Marcos
Zampieri. 2016. Findings of the 2016 Conference
on Machine Translation. In Proceedings of the
First Conference on Machine Translation. Associ-
ation for Computational Linguistics, Berlin, Ger-
many, pages 131–198. http://www.aclweb.
org/anthology/W16-2301.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguis-
tics, Austin, Texas, pages 551–561. https://
aclweb.org/anthology/D16-1053.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learn-
ing phrase representations using RNN encoder–
decoder for statistical machine translation. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing. Associ-
ation for Computational Linguistics, Doha, Qatar,
pages 1724–1734. http://www.aclweb.org/
anthology/D14-1179.

Michał Daniluk, Tim Rocktäschel, Johannes Welbl,
and Sebastian Riedel. 2016. Frustratingly short at-
tention spans in neural language modeling. In Pro-
ceedings of the International Conference on Learn-
ing Representations. San Juan, Puerto Rico.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N. Dauphin. 2017. Convo-
lutional sequence to sequence learning. In Doina
Precup and Yee Whye Teh, editors, Proceedings
of the 34th International Conference on Machine
Learning. PMLR, International Convention Cen-
tre, Sydney, Australia, volume 70 of Proceed-
ings of Machine Learning Research, pages 1243–
1252. http://proceedings.mlr.press/
v70/gehring17a.html.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
Jian Sun. 2016. Deep residual learning for image
recognition. In 2016 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR). vol-
ume 00, pages 770–778. https://doi.org/
10.1109/CVPR.2016.90.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation
9(8):1735–1780.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing. Association for Com-
putational Linguistics, Seattle, Washington, USA,
pages 1700–1709. http://www.aclweb.org/
anthology/D13-1176.

Jaeyoung Kim, Mostafa El-Khamy, and Jungwon Lee.
2017. Residual LSTM: design of a deep re-
current architecture for distant speech recognition.
CoRR abs/1701.03360. http://arxiv.org/
abs/1701.03360.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions. Association for Computational Linguistics,
Prague, Czech Republic, pages 177–180. http://
www.aclweb.org/anthology/P07-2045.

Kenton Lee, Omer Levy, and Luke Zettlemoyer.
2017. Recurrent additive networks. CoRR
abs/1705.07393. http://arxiv.org/abs/
1705.07393.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. In Proceedings of the International
Conference on Learning Representations. Toulon,
France.

Yang Liu and Mirella Lapata. 2018. Learning struc-
tured text representations. Transactions of the
Association for Computational Linguistics 6:63–
75. https://transacl.org/ojs/index.
php/tacl/article/view/1185/280.

Thang Luong, Hieu Pham, and D. Christopher Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, pages 1412–1421. https:
//doi.org/10.18653/v1/D15-1166.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations. Asso-
ciation for Computational Linguistics, Baltimore,
Maryland, pages 55–60. http://www.aclweb.
org/anthology/P14-5010.

1375



Lesly Miculicich Werlen. 2017. Towards document-
level neural machine translation. Idiap-RR Idiap-
RR-25-2017, Idiap.

Alexander Miller, Adam Fisch, Jesse Dodge, Amir-
Hossein Karimi, Antoine Bordes, and Jason We-
ston. 2016. Key-value memory networks for di-
rectly reading documents. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Austin, Texas, pages 1400–
1409. https://aclweb.org/anthology/
D16-1147.

Maria Nadejde, Siva Reddy, Rico Sennrich, Tomasz
Dwojak, Marcin Junczys-Dowmunt, Philipp Koehn,
and Alexandra Birch. 2017. Predicting target lan-
guage CCG supertags improves neural machine
translation. In Proceedings of the Second Con-
ference on Machine Translation. Association for
Computational Linguistics, Copenhagen, Denmark,
pages 68–79. http://www.aclweb.org/
anthology/W17-4707.

Nikolaos Pappas and Andrei Popescu-Belis. 2017.
Explicit document modeling through weighted
multiple-instance learning. Journal of Artificial In-
telligence Research 58:591–626.

Alexandre Rafalovitch and Robert Dale. 2009. United
Nations General Assembly resolutions: A six-
language parallel corpus. In Proceedings of the MT
Summit. volume 12, pages 292–299.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Edinburgh Neural Machine Translation Sys-
tems for WMT 16. In Proceedings of the First
Conference on Machine Translation. Association
for Computational Linguistics, Berlin, Germany,
pages 371–376. http://www.aclweb.org/
anthology/W/W16/W16-2323.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Proceedings of the 54th
Annual Meeting of the ACL (Vol. 1: Long Pa-
pers). Association for Computational Linguistics,
Berlin, Germany, pages 1715–1725. http://
www.aclweb.org/anthology/P16-1162.

Rohollah Soltani and Hui Jiang. 2016. Higher order
recurrent neural networks. CoRR abs/1605.00064.
http://arxiv.org/abs/1605.00064.

Sainbayar Sukhbaatar, arthur szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory
networks. In C. Cortes, N. D. Lawrence, D. D.
Lee, M. Sugiyama, and R. Garnett, editors,
Advances in Neural Information Processing
Systems 28, Curran Associates, Inc., pages 2440–
2448. http://papers.nips.cc/paper/
5846-end-to-end-memory-networks.
pdf.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
27, Curran Associates, Inc., pages 3104–3112.

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints abs/1605.02688. http:
//arxiv.org/abs/1605.02688.

Ke Tran, Arianna Bisazza, and Christof Monz. 2016.
Recurrent memory networks for language modeling.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies. Association for Computational Linguistics,
San Diego, California, pages 321–331. http://
www.aclweb.org/anthology/N16-1036.

Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. 2017.
Attention is all you need. In I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Ad-
vances in Neural Information Processing Sys-
tems 30, Curran Associates, Inc., pages 5998–
6008. http://papers.nips.cc/paper/
7181-attention-is-all-you-need.
pdf.

Cheng Wang. 2017. RRA: recurrent residual atten-
tion for sequence learning. CoRR abs/1709.03714.
http://arxiv.org/abs/1709.03714.

Mingxuan Wang, Zhengdong Lu, Hang Li, and Qun
Liu. 2016. Memory-enhanced decoder for neu-
ral machine translation. In Proceedings of the
2016 Conference on Empirical Methods in Nat-
ural Language Processing. Association for Com-
putational Linguistics, Austin, Texas, pages 278–
286. https://aclweb.org/anthology/
D16-1027.

Yiren Wang and Fei Tian. 2016. Recurrent resid-
ual learning for sequence classification. In Pro-
ceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing. As-
sociation for Computational Linguistics, Austin,
Texas, pages 938–943. https://aclweb.org/
anthology/D16-1093.

Jason Weston, Sumit Chopra, and Antoine Bordes.
2015. Memory networks. In Proceedings of the
International Conference on Learning Representa-
tions. San Diego, USA.

Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Eva Hasler, and Philipp Koehn.
2014. Edinburgh’s Syntax-Based Systems at WMT
2014. In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation. Association for Com-
putational Linguistics, Baltimore, Maryland, USA,

1376



pages 207–214. http://www.aclweb.org/
anthology/W/W14/W14-3324.

Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, and Philipp Koehn. 2015. Ed-
inburgh’s Syntax-Based Systems at WMT 2015.
In Proceedings of the Tenth Workshop on Statisti-
cal Machine Translation. Association for Compu-
tational Linguistics, Lisbon, Portugal, pages 199–
209. http://aclweb.org/anthology/
W15-3024.pdf.

Zichao Yang, Zhiting Hu, Yuntian Deng, Chris Dyer,
and Alex Smola. 2017. Neural machine transla-
tion with recurrent attention modeling. In Pro-
ceedings of the 15th Conference of the Euro-
pean Chapter of the Association for Computa-
tional Linguistics: Volume 2, Short Papers. Associa-
tion for Computational Linguistics, Valencia, Spain,
pages 383–387. http://www.aclweb.org/
anthology/E17-2061.

Matthew D. Zeiler. 2012. ADADELTA: an adap-
tive learning rate method. CoRR abs/1212.5701.
http://arxiv.org/abs/1212.5701.

Biao Zhang, Deyi Xiong, Jinsong Su, and Hong
Duan. 2017. A context-aware recurrent encoder for
neural machine translation. IEEE/ACM Transac-
tions on Audio, Speech, and Language Processing
25(12):2424–2432.

Biao Zhang, Deyi Xiong, jinsong su, Hong Duan,
and Min Zhang. 2016a. Variational neural machine
translation. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Austin, Texas, pages 521–530. https://
aclweb.org/anthology/D16-1050.

Yu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yaco,
Sanjeev Khudanpur, and James Glass. 2016b.
Highway long short-term memory RNNs for
distant speech recognition. In 2016 IEEE Inter-
national Conference on Acoustics, Speech and
Signal Processing (ICASSP). pages 5755–5759.
https://doi.org/10.1109/ICASSP.
2016.7472780.

Julian Georg Zilly, Rupesh Kumar Srivastava, Jan
Koutnı́k, and Jürgen Schmidhuber. 2017. Re-
current highway networks. In Doina Precup
and Yee Whye Teh, editors, Proceedings of
the 34th International Conference on Machine
Learning. PMLR, International Convention Cen-
tre, Sydney, Australia, volume 70 of Proceed-
ings of Machine Learning Research, pages 4189–
4198. http://proceedings.mlr.press/
v70/zilly17a.html.

A Detailed Architecture

This appendix describes in detail the implemen-
tation of the self-attentive residual decoder for

NMT, which builds on the attention-based NMT
implementation of dl4mt-tutorial6.

The input of the model is a source sentence de-
noted as 1-of-k coded vector, where each element
of the sequence corresponds to a word:

x = (x1, x2, ..., xm), xi ∈ RV

and the output is a target sentence denoted as well
as 1-of-k coded vector:

y = (y1, y2, ..., yn), yi ∈ RV

where V is the size of the vocabulary of target and
source side, m and n are the lengths of the source
and target sentences respectively. We omit the bias
vectors for simplicity.

A.1 Encoder
Each word of the source sentence is embedded in
a e-dimensional vector space using the embedding
matrix Ē ∈ Re×V . The hidden states are 2d-
dimensional vectors modeled by a bi-directional
GRU. The forward states

−→
h = (

−→
h 1, ...,

−→
h m) are

computed as:

−→
h i =

−→z i �
−→
h i−1 + (1−−→z i)�

−→
h ′i

where

−→
h ′i = tanh(

−→
WĒxi +

−→
U [−→r i �

−→
h i−1])

−→z i = σ(
−→
W zĒxi +

−→
U z
−→
h i−1)

−→r i = σ(
−→
W rĒxi +

−→
U r
−→
h i−1)

Here,
−→
W,
−→
W z,
−→
W r ∈ Rd×e and

−→
U ,
−→
U z,
−→
U r ∈

Rd×d are weight matrices. The backward states←−
h = (

←−
h 1, ...,

←−
h m) are computed in similar man-

ner. The embedding matrix Ē is shared for both
passes, and the final hidden states are formed by
the concatenation of them:

hi =

[−→
h i←−
h i

]

A.2 Attention Mechanism
The context vector at time t is calculated by:

ct =
m∑

i=1

αtihi

6https://github.com/nyu-dl/dl4mt-tutorial

1377



where

αti =
exp(eti)∑
j exp(e

t
j)

eti = v
ᵀ
atanh(Wdst−1 +Wehi)

Here, va ∈ Rd, Wd ∈ Rd×d and We ∈ Rd×2d are
weight matrices.

A.3 Decoder
The input of the decoder are the previous word
yt−1 and the context vector ct, the objective is
to predict yt. The hidden states of the decoder
s = (s1, ..., sn) are initialized with the mean of
the context vectors:

s0 = tanh(Winit
1

m

m∑

i=1

ci)

where Winit ∈ Rd×2d is a weight matrix, m is the
size of the source sentence. The following hidden
states are calculated with a GRU conditioned over
the context vector at tine t as follows:

st = zt � s′t + (1− zt)� s′′t
where

s′′t = tanh(Eyt−1 + U [rt � st−1] + Cct)
zi = σ(WzEyt−1 + Uzst−1 + Czct)

ri = σ(WrEyt−1 + Urst−1 + Crct)

Here, E ∈ Re×V is the embedding matrix for the
target language. W,Wz,Wr ∈ Rd×e, U,Uz, Ur ∈
Rd×d, and C,Cz, Cr ∈ Rd×2d are weight matri-
ces. The intermediate vector s′t is calculated from
a simple GRU:

s′t = GRU(yt−1, st−1)

In the attention-based NMT model, the proba-
bility of a target word yt is given by:

p(yt|st, yt−1, ct) = softmax(Wotanh(
Wstst +Wytyt−1 +Wctct))

Here, Wo ∈ RV×e, Wst ∈ Re×d, Wyt ∈ Re×e,
Wct ∈ Re×2d are weight matrices.
A.3.1 Self-Attentive Residual Connections
In our model, the probability of a target word yt is
given by:

p(yt|st, dt, ct) = softmax(Wotanh(
Wstst +Wdtdt +Wctct))

Here, Wo ∈ RV×e, Wst ∈ Re×d, Wdt,Wyt ∈
Re×e, Wct ∈ Re×2d are weight matrices. The
summary vector dt can be calculated in different
manners based on previous words y1 to yt−1. First,
a simple average:

davgt =
1

t− 1
t−1∑

i=1

yi

The second, by using an attention mechanism:

dcavgt =
t−1∑

i=1

αtiyi

αti =
exp(eti)∑t−1
j=1 exp(e

t
j)

eti = v
ᵀtanh(Wyyi)

where v ∈ Re, Wy ∈ Re×e are weight matrices.

A.3.2 Memory RNN
This model modifies the recurrent layer of the de-
coder as follows:

st = zt � s′t + (1− zt)� s′′t

where

s′′t = tanh(Eyt−1 + U [rt � s̃t] + Cct)
zi = σ(WzEyt−1 + Uz s̃t + Czct)

ri = σ(WrEyt−1 + Urs̃t + Crct)

Here, E ∈ Re×V is the embedding matrix for the
target language. W,Wz,Wr ∈ Rd×e, U,Uz, Ur ∈
Rd×d, and C,Cz, Cr ∈ Rd×2d are weight matri-
ces. The intermediate vector s′t is calculated from
a simple GRU:

s′t = GRU(yt−1, s̃t)

The recurrent vector s̃t is calculated as following:

s̃t =

t−1∑

i=1

αtisi

where αti =
exp(eti)∑t−1
j=1 exp(e

t
j)

eti = v
ᵀtanh(Wmsi +Wsst)

where v ∈ Rd, Wm ∈ Rd×d, and Ws ∈ Rd×d are
weight matrices.

1378



A.3.3 Self-Attentive RNN
The formulation of this decoder is as following:

p(yt|y1, ..., yt−1, ct) ≈ softmax(Wotanh(
Wstst +Wytyt−1 +Wctct +Wmts̃t))

Here, Wo ∈ RV×e, Wst ∈ Re×d, Wyt ∈ Re×e,
Wct ∈ Re×2d, and Wmt ∈ Re×d are weight matri-
ces.

s̃t =
t−1∑

i=1

αtisi

αti =
exp(eti)∑t−1
j=1 exp(e

t
j)

eti = v
ᵀtanh(Wmsi +Wsst)

where v ∈ Rd, Wm ∈ Rd×d, and Ws ∈ Rd×d are
weight matrices.

1379


