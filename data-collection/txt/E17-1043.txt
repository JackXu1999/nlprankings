



















































May I take your order? A Neural Model for Extracting Structured Information from Conversations


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 450–459,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

May I take your order? A Neural Model for Extracting Structured
Information from Conversations

Baolin Peng1, Michael L. Seltzer2,
Yun-Cheng Ju2, Geoffrey Zweig2 and Kam-Fai Wong1

1The Chinese University of Hong Kong, Shatin, N.T. Hong Kong
2Microsoft Research, One Microsoft Way, Redmond, WA, USA

{blpeng, kfwong}@se.cuhk.edu.hk
{mseltzer, yuncj, gzweig}@microsoft.com

Abstract

In this paper we tackle a unique and im-
portant problem of extracting a structured
order from the conversation a customer
has with an order taker at a restaurant.
This is motivated by an actual system un-
der development to assist in the order tak-
ing process. We develop a sequence-to-
sequence model that is able to map from
unstructured conversational input to the
structured form that is conveyed to the
kitchen and appears on the customer re-
ceipt. This problem is critically differ-
ent from other tasks like machine trans-
lation where sequence-to-sequence mod-
els have been used: the input includes
two sides of a conversation; the out-
put is highly structured; and logical ma-
nipulations must be performed, for ex-
ample when the customer changes his
mind while ordering. We present a novel
sequence-to-sequence model that incorpo-
rates a special attention-memory gating
mechanism and conversational role mark-
ers. The proposed model improves per-
formance over both a phrase-based ma-
chine translation approach and a standard
sequence-to-sequence model.

1 Introduction

Extracting structured information from unstruc-
tured text is a critically important problem in nat-
ural language processing. In this paper, we attack
a deceptively simple form of the problem: under-
standing what a customer wants when ordering at a
restaurant. In this problem, we seek to convert the
conversation between the customer and the order
taker, i.e. the waiter or waitress, into the structured
form that is conveyed to the kitchen to prepare the
food, and which appears on the customer receipt.

Hi, how can I help you ?
We’d like a large cheese pizza.
Any toppings?
Yeah, how about pepperoni and two diet cokes.
What size?
Uh, medium and make that three cokes.
Anything else?
A small Caesar salad with the dressing on the side
Sure, is that it?
Yes, that’s all, thanks.

Waiter:
Customer:
Waiter:
Customer:
Waiter:
Customer:
Waiter:
Customer:
Waiter:
Customer:

Figure 1: A conversation example of an order-
taking interaction at a restaurant.

Item Size Qty Modifiers
Pizza large 1 add pepperoni
Caesar Salad small 1 side dressing
Diet Coke medium 3

Table 1: An example of the structured data record
corresponding to the conversation in Figure 1

We develop this system to analyze real-time in-
teractions with the aim of discovering errors in the
order-entry process. Note that the objective is to
analyze the interaction and suggest corrections to
the human order-taker. Thus, we take both sides
of the order-taking interaction as input, and are not
attempting to predict the order-taker’s side of the
conversation.

While we focus on the restaurant domain in
this work, this problem is relevant in any scenario
in which a conversation results in the creation of
structured information. Other examples include
a sales interaction which results in a purchase or-
der, a call to a help desk which results in a service
record, or a conversation with a travel agent that
results in an itinerary.

An example of the problem of interest is shown
in Figure 1. The structured data record that cor-
responds to this conversation is shown in Table 1.
There are several things to note about this exam-
ple:

450



• The output is a stylized and structured repre-
sentation of the input

• The items in the structured order may appear
in a different sequence than they are men-
tioned

• Inference occurs across turns, for example
that “medium” applies to the coke and not the
pizza whose size was earlier specified

• Logical manipulations must be done, for ex-
ample changing the number of cokes from
two to three

• In contrast to machine translation, we do not
wish to create a verbatim “translation” of the
input, but instead a logical distillation of it

To attack this problem, we implemented two
baselines and several sequence-to-sequence mod-
els. The first baseline is an information-retrieval
approach based on a TF-IDF match (Salton et al.,
1975) which finds the most similar conversation in
the training data, and returns the associated order.
The second uses phrase-based machine translation
(Koehn et al., 2003) to “translate” from the conver-
sational input to the tokens in the structured order.
We compare these to a sequence-to-sequence (s2s)
model with attention (Chan et al., 2016; Bahdanau
et al., 2014; Devlin et al., 2015; Yao and Zweig,
2015; Sutskever et al., 2014; Mei et al., 2016),
and then extend the s2s model with the addition
of a gating mechanism on the attention memory
and with an auxiliary input that indicates the con-
versational role of the speaker (customer or order-
taker). We show that it is in fact possible to extract
the orders from conversations recorded at a real
restaurant 1, and achieve an F measure of over 70
from raw text and 65 from ASR transcriptions.

2 Problem Formulation

The precise problem setting in this paper is as fol-
lows. The training data consists of input/output
pairs of examples (X1, Y1), . . . , (XN , YN ), where
Xk is a conversation consisting of several utter-
ances, similar to the example shown in Figure 1,
and Yk is the corresponding structured data record
such as the one in Table 1.

1The restaurant will remain anonymous for business rea-
sons, and we have changed the names of menu items in our
examples accordingly.

GRU

Attention	layer

M
em

.

GRU

Cu.		:	We’d	 like	a	large	pizza
Wa.	:	Any	topping
Cu.		:	pepoeroni

(Large	Pizza,	Qty=1,	
add	peperoni)

Figure 2: An input unstructured conversation and
the corresponding structured record.

Given a conversation Xk, the goal of our model
is to extract the structured data record Yk so that:

Yk = argmax
Y

logP (Y |Xk) (1)

We cast this task as a sequence modeling prob-
lem which aims to map the sequence of words
in a conversation Xk to the sequence of tokens
in the corresponding structured data record Yk.
The input sequence is formed by concatenating
the utterances in the conversation, while the out-
put sequence is formed by concatenating the rows
in the structured data record. For example, the
utterances in the conversation shown in Figure
1 are concatenated to predict the sequence y =
Pizza, size=large, qty=1, modifiers=(add pepper-
oni) | Diet Coke, size=medium, qty=3 | Caesar
Salad, size=small, qty=1, modifiers=(side dress-
ing) which is derived from Table 1. Under this se-
quential model, the conditional probability of the
structured data record Y given the observed con-
versation X can be written as

P (Y |X, θ) =
T∏

t=1

P (yt|y1:t−1, X, θ) (2)

where y1:t−1 denotes the first t − 1 terms in the
structured data record and θ represents the model
parameters.

3 Model

The proposed model is based on an encoder-
decoder architecture with attention (Bahdanau et
al., 2014), as shown in Figure 2. The encoder net-
work reads the input conversation X one word at
a time and updates its hidden state ht according to
current input wt and previous hidden state ht−1,

ht = fe(wt, ht−1), t ∈ {1, · · · ,M} (3)

where fe is a nonlinear function which is elabo-
rated in the following section. After reading all
the tokens, the encoder network yields a context

451



vector c as the representation of the entire conver-
sation.

The decoder then processes this representa-
tion and generates a hypothesized structured data
record Y as an output sequence, word by word
given the context vector c and all previous pre-
dicted tokens. The conditional probability can be
expressed as follows:

P (yt|y1, · · · , yt−1,X) = fd(yt−1, st, c) (4)
st = g(yt−1, st−1, c) t ∈ {1, · · · , N} (5)

where fd and g are nonlinear functions and st is
the hidden state of decoder at time t. Critically,
our decoder also utilizes an attention mechanism,
which stores the intermediate encoder representa-
tions of each input word for use by the decoder.

Two improvements to the conventional encoder-
decoder model architecture are proposed in this
work. First, we incorporate gates controlled by the
encoder into the neural attention memory to adap-
tively modulate the representations in the memory
based on their semantic importance. Second, we
propose a way to incorporate conversational role
information into the model to reflect the fact that
different participants in a multi-party interaction
have different roles and the meaning of certain ut-
terances may be dependent on the speaker’s role.

A detailed illustration of the proposed model is
shown in Figure 3. We elaborate on each compo-
nent of this model in the following sections.

3.1 Encoder Network

The encoder network is designed to generate a se-
mantically meaningful representation of unstruc-
tured conversations. Several neural network ar-
chitectures have been proposed for this purpose,
including CNNs (Kalchbrenner et al., 2014; Hu
et al., 2014), RNNs (Sutskever et al., 2014) and
LSTMs (Hochreiter and Schmidhuber, 1997). In
this work, we use an encoder constructed from a
recurrent neural network with gated RNN units
(GRU) (Cho et al., 2014). The GRU has been
shown to alleviate the gradient vanishing problem
of RNNs, enabling the model to learn long term
dependencies in the input sequence. GRUs have
been shown to perform comparably to LSTMs
(Chung et al., 2014).

At time t, the new state of a GRU is computed
as follows:

ℎ1

ℎ1

ℎ2

ℎ2

ℎ3

ℎ3

𝑊𝑔

𝑤1 𝑤2 𝑤3

𝑔1 𝑔2 𝑔3

𝑐1 𝑐2 𝑐3

…

…

…

+

𝑠𝑡−1

𝑠𝑡a𝑡

𝑦𝑡

α1 α2 α3 𝑦𝑡−1

Figure 3: Graphical structure of memory-gated
encoder-decoder model with attention mechanism.
w1 represents input;

−→
h1 and

←−
h1 are the hidden

states of forward and backward GRUs, respec-
tively. g1, α1 represent the context gates and atten-
tion weights, respectively. Small dot node means
element-wise product.

zt = σ(Wzxt + Uzht−1 + bz) (6)
rt = σ(Wrxt + Urht−1 + br) (7)
ĥt = tanh(Whxt + Uh(rt � ht−1)) (8)
ht = (1− zt)� ht−1 + zt � ĥt (9)

where � stands for element-wise multiplica-
tion. W , U are weight matrixes applied to input
and previous hidden state, respectively .ht is a lin-
ear combination of the previous state ht−1 and the
hypothesis state ĥt. ĥt is computed with new se-
quence information. The update gate, zt, controls
to what extent the past information is kept and how
much new information is added. The reset gate, rt,
controls to what extent the history state contributes
to the hypothesis state. If rt is zero, then GRU ig-
nores all the history information.

The conversation encoding is obtained by con-
catenating the GRU hidden state vectors from the
forward and backward directions. Thus the en-
coder operation can be summarized as follows

xt = Wewt, t ∈ [1, T ] (10)−→
ht =

−−−→
GRU(xt), t ∈ [1, T ] (11)←−

ht =
←−−−
GRU(xt), t ∈ [T, 1] (12)

h+t =
−→
ht ⊕←−ht (13)

452



wherewt is the one-hot input vector,We is the em-
bedding matrix, and xt is the word embedding for
wt. The functions

−−−→
GRU(xt) and

←−−−
GRU(xt) repre-

sent the GRU operating in the forward and back-
ward directions, respectively, with processing de-
fined by equations 6–9.

This produces a sequence of context vectors, h+t
which are subsequently consumed by an attention
mechanism in the decoder. We use the final atten-
tion vector h+T to initialize the hidden state of the
decoder.

3.2 Memory Gate

In most sequence-to-sequence tasks such as ma-
chine translation, every word in the input is im-
portant. However, in our scenario, where the input
to the system is conversational speech, not all the
words in the conversation contribute to the predic-
tion of structured data record. For example, it is
reasonable to ignore the chit-chat that is present in
many conversations. Further, in other tasks, gat-
ing mechanisms have been shown to be useful to
dynamically select important information (Yao et
al., 2015; Hochreiter and Schmidhuber, 1997; Tu
et al., 2016).

In light of this, we propose the use of an addi-
tional memory gate to select important informa-
tion from the memory vector. The memory gate
we use consists of a single-layer feed-forward neu-
ral network

gt = σ(Wgh+t + bg) (14)

where σ is a sigmoid activation function and Wg
and bg are weight matrix and bias, respectively,
and h+t is the context vector at time t defined in
equation 10. The gate is then applied to the con-
text vector h+t using an element-wise multiplica-
tion operation.

ct = gt � h+t (15)

After applying memory gate, the gated context
vector ct is then fed into attention memory of the
decoder network in place of the original context
vector h+t . Figure 4 illustrates an example of the
gating weights for a sample utterance. The darker
colors indicates values close to 1 while the lighter
colors indicate values close to 0. As the figure
shows, the network learns to suppress semanti-
cally unimportant words.

3.2.1 Role Information
In many sequence-to-sequence models, there is no
notion of different speakers with different roles.
Inspired by the work in dialog generation (Li et al.,
2016) and spoken language understanding (Hori
et al., 2016), we propose the addition of speaker
information into the encoder network to explicitly
model the interaction patterns of the customer and
order-taker.

Specifically we learn separate word and role
embeddings, and concatenate them to form the in-
put. The input to the encoder network becomes:

xwt = Wewt, t ∈ [1, T ] (16)
xrt = Wrrt, t ∈ [1, T ] (17)
xt = xwt ⊕ xrt , t ∈ [1, T ] (18)

3.3 Decoder Network

The decoder network is used to predict the next
word given all the previously predicted words and
the context vectors from the encoder network (Lu-
ong et al., 2015; Bahdanau et al., 2014).

We use an RNN with GRU units to predict each
word yt sequentially based on the previously pre-
dicted word yt−1 and the output of the attention
process at that computes a weighted combination
of the context vectors in memory.

If we define st as the hidden layer of the decoder
at time t, the decoder’s operation can be expressed
as

st =
−−−→
GRU(yt−1 ⊕ at) (19)

yt = softmax(Wost + bo) (20)

where yt−1 ⊕ at is the concatenation of the pre-
viously predicted output yt−1 and the output of
the attention process at, and

−−−→
GRU(·) is defined

by equations 6–9, as before.
The attention vector at is computed as a linear

combination of the gated context vectors gener-
ated by the encoder network. This can be written
as

at =
M∑

j=1

αijcj (21)

where the weights αij are computed as

αij =
exp(eij)∑N

k=1 exp(eik)
(22)

453



A single-layer feed-forward neural network is
used to compute eij as

eij = V Ta tanh(Wast−1 + Uacj) (23)

where Va, Wa, and Ua are weight matrices.

3.4 Model Training

The model is trained to maximize the log proba-
bility of the structured data records given the cor-
responding conversation,∑

(Yk,Xk)∈D
logP (Yk|Xk) (24)

where D is the set containing all the training pairs
and P (Yk|Xk) is computed with equation 2. The
standard adadelta algorithm (Zeiler, 2012) is used
for parameter updates. Gradients are clipped to 1
to avoid exponentially increasing values (Pascanu
et al., 2013).

4 Experiments

In this section, we evaluate our proposed model
on two data sets and compare performance with
several baseline systems.

4.1 Data sets

We conducted experiments on a corpus of con-
versations between a customer and an order taker
(waiter or waitress) captured in a real restaurant
environment. The conversations were manually
transcribed by professional annotators. There are
4823 examples in the training set, 543 in the de-
velopment (dev) set, and 843 in the test set. There
are approximately 260 unique items in the record
and 150 unique modifiers on these items, but not
all modifiers apply to all items. We experimented
with two version of the dev and test sets. The first
is manually transcribed in the same manner as the
training set, while the second is generated by a
speech recognition decoder that was trained on the
conversations in the training set. We denote the
second set as ASR-dev and ASR-test. Table 2 lists
the statistics of the data sets. Note that the audio
of a conversation was collected as a single file and
then automatically segmented into turns for ASR
decoding. This process was not perfect and likely
introduced some errors. Thus, the average length
and number of turns of differ between the ASR
transcriptions and the manual transcriptions.

Data Avg Avg Avg
Set Turns Length # Items

Training 8.8 53.5 3.7
Dev 9.7 57.07 3.8
Test 8.7 50.96 3.5

ASR-Dev 8.5 49.8 3.8
ASR-Test 7.8 44.7 3.5

Table 2: Statistics of the experimental corpus. The
table denotes the average number of utterances in
a conversation, average length of a conversation in
words, and average number of items in an order.

4.2 Experimental setup

All words are lower-cased and an unknown word
token is used for words which appear less than
four times in the training set. The word embed-
ding matrix is initialized by randomly sampling
from a normal distribution, and scaled by 0.01.
The recurrent connections of the GRU are initial-
ized with orthogonal matrices (Saxe et al., 2013)
and biases are initialized to zero. A single layer
GRU is used for both the encoder and decoder.
The network has 600 hidden units and uses 300-
dimensional word embeddings. The dropout rate
is set to 0.5. We did not tune hyper-parameters
except for the dimension of the role embedding
which is selected from {3, 5, 10} on the dev set.
During inference, we use beam search decod-
ing with a beam of 5 to generate the structured
records. In order to decode without a length bias,
the log probability of decoded results is normal-
ized by the number of tokens.

4.3 Evaluation

A typical metric to evaluate a generation system
is BLEU score (Papineni et al., 2002) which uses
ngram overlap to quantify the degree to which a
hypothesis matches the reference. However, our
scenario is more demanding: order items are ei-
ther correct or incorrect. Therefore, we adopt pre-
cision and recall at the item level as our evaluation
metric. Note that an item is defined as a row in the
structured data record and typically includes mul-
tiple fields. Using Table 1 as an example, there
are three items to be scored. Only when the model
produces an item that is exactly the same as the
reference item do we count it as correct. As an
additional measure, we report accuracy of the en-
tire order, in which every item in an order must be
correct for the order to be counted as correct.

454



Model Dev Test
Gate Role Recall Prec F1 Accy Recall Prec F1 Accy

IR - - 26.5 22.7 24.5 11.4 29.7 25.6 27.5 14.1
PBMT - - 64.4 19.3 35.2 29.3 62.6 20.8 36.0 28.4
NAM - - 64.9 70.9 67.9 45.7 68.4 71.3 69.8 48.1
NAM - X 65.6 71.6 68.6 45.3 68.8 72.9 70.8 49.1
NAM X - 67.6 72.7 70.1 46.2 68.3 74.1 71.1 48.5
NAM X X 66.9 71.6 69.2 48.8 70.2 72.3 71.2 51.8

Table 3: Results of different methods on dev and test set. Human transcriptions are used.

Model Dev Test
Gate Role Recall Prec F1 Accy Recall Prec F1 Accy

IR - - 21.9 18.9 20.3 6.9 25.7 19.3 23.8 10.2
PBMT - - 56.8 20.4 34.1 23.3 56.9 21.5 35.0 24.7
NAM - - 56.7 63.5 60.0 36.9 60.3 66.7 63.4 40.6
NAM - X 57.1 64.7 60.8 38.1 62.5 67.4 64.9 42.5
NAM X - 57.0 64.6 60.7 39.2 60.3 68.3 64.2 40.8
NAM X X 58.5 65.2 61.8 40.5 63.0 68.4 65.7 45.9

Table 4: Results of different methods on ASR-dev and ASR-test set.

4.4 Baseline systems

We compare the performance of our neural model
with baseline models that employ information re-
trieval (IR) and phrase-based machine translation
(PBMT) approaches.

IR: The IR method treated the training set of
transcriptions as a collection of documents, each
mapped to a corresponding order. The test conver-
sation was used as a query to find the most similar
training set conversation. The corresponding or-
der was returned as the estimated order. In our ex-
periment, we use TFIDF to compute the similarity
score.

PBMT: The goal of a phrase-based translation
model is to map a conversation into its structured
record with alignment and language models. In
our experiments, we use the Moses decoder, a
state-of-the-art phrase-based MT system available
for research purposes. We use GIZA++ (Och and
Ney, 2003) to learn word alignment and irstlm to
learn the language model. The models are trained
on the conversation/order pairs in the training set
and used to predict the structured data record given
a conversation.

4.5 Results

First we discuss the performance of our models
on manually transcribed data and then examine the
results on ASR recognized data. Table 3 lists the
experiment results on manually transcribed dev
and test sets. We refer to our model as the neu-
ral attention model (NAM). We see that the NAM
is superior to both the IR and PBMT methods
by a large margin. Both the proposed memory
gate and role modifications yield improvements
over the basic NAM. When combined, these pro-
duce the best performance in terms of accuracy
on the dev set, and both F1 and accuracy on the
test set. While there are only small differences in
the scores among some of the NAM methods, we
are unaware of a measure of statistical significance
suitable for this task.

Though not reported, we also found that a ba-
sic encoder-decoder s2s model without attention
performs poorly; it cannot summarize information
across multiple turns into a single vector. The at-
tention mechanism, acting on the entire encoding
sequence, is critical in our task.

Table 4 shows results on the ASR-dev and ASR-
test sets. These data sets are quite noisy since the
speech recognizer in this domain has a word error

455



h
i

h
o
w

ca
n

i h
e
lp

y
o
u

y
e
a
h

i\
'd

lik
e

a m
u
sh

ro
o
m

p
e
p
e
ro

n
i

p
iz

za
n
o

ch
e
e
se

a la
rg

e
ce

a
se

r
a la

rg
e

co
ke

o
ka

y
a
n
y
th

in
g

e
ls

e
fo

r
y
o
u

n
a
h

o
ka

y
<

/s
>

Figure 4: Example of memory gate weights at each time stamp.

(a) Attention weight of NAM (b) Attention weight of NAM with memory gate

Figure 5: Examples of attention weights of models (a) without memory gate and (b) with memory gate.
(b) shows sparse and more focused attention weights. (Better viewed in color.)

rate around 25%. With this noisy data, we find that
the memory gate and role additions consistently
improve performance. When combined, both F1
and accuracy improved.

4.6 Qualitative analysis

Figure 6 shows a sample input and the output from
each model. We see that the NAM augmented with
memory gates and role information successfully
captures the interaction and generates the correct
record.

To better understand the proposed model, we
visualize the attention weight at each time step
in Figure 5. The figure compares the attention
weights produced by a conventional context mem-

ory and the proposed gated context memory. We
see that both models are able to learn good soft
alignment between the input conversation and the
output structured data record. However, the atten-
tion weights in 5(b), with our proposed gated at-
tention mechanism, are sparser than those in 5(a)
and better able to ignore uninformative terms in
the input.

5 Related Work

There has been much work on information extrac-
tion from single utterances. Kate and Mooney
(2006) proposed the use of SVM classifiers based
on string kenels to parse natural language to
a formal meaning representation. Wong and

456



My	name’s	Alexis	how	can	I	help	you.
Yeah	can	I	get	a	cheese pizzacombo?
Something todrink?
Change	that	to	a	mushroompizza
combowith	a	large	sprite no	ice.
Okay.
Thank	you.
Thank	you.

Waiter:
Customer:
Waiter:
Customer:

Waiter:
Customer:
Waiter:

NAM:	
Cheese Pizza,Qty=1,Combo=True
Sprite,Size=Lrg,Qty=1,Modifier=(no ice );
Caesar Salad, Size=Med,Qty=1

NAM +Memory Gate:	
Cheese Pizza,Qty=1,Combo=True
Sprite,Size=Lrg,Qty=1,Modifier=(no ice );
Caesar Salad, Size=Med,Qty=1
Mushroom	Pizza,	Qty=1,	Combo=True
Sprite,Size=Lrg,Qty=1,Modifier=(no ice );
Caesar Salad, Size=Med,Qty=1

NAM +Memory Gate	+	Role:	
Mushroom	Pizza,	Qty=1,	Combo=True
Sprite,Size=Lrg,Qty=1,Modifier=(no ice );
Caesar Salad, Size=Med,Qty=1

Figure 6: Examples of outputs generated by each
model for the conversation in first row.

Mooney (2006) used syntax-based statistical ma-
chine translation method to do semantic parsing.
Translation of natural language to a formal mean-
ing representation is captured by a synchronous
context-free grammar in (Wu, 1997; Chiang et al.,
2006). Quirk et al. (2015) created models to map
natural language descriptions to executable code
using productions from the formal language. Belt-
agy and Quirk (2016) improved the performance
of semantic parsing on If-Then statements by us-
ing neural networks to model derivation trees and
leveraged several techniques like synthetic train-
ing data from paraphrases and grammar combina-
tions to improve generalization and reduce over-
fitting. In addition, there are some other research
works focusing on text generation from structured
data records. Angeli et al. (2010) proposed of a
domain independent probabilistic approach to per-
forming content selection and surface realization,
making text generation as a local decision process.
Konstas and Lapata (2013) created a global model
to generate text from structured records, which
jointly modeled content selection and surface real-
ization with a probabilistic context-free grammar.
In contrast, in this paper we focus on generating
structured data records from text descriptions.

Using spoken language understanding tech-
niques, (Mesnil et al., 2015) tag each word in a
sentence with a predefined slot. A dialog model-
ing approach (Young et al., 2013) is also relevant
to our task. However, this approach requires the
definition of semantic slot names and human la-
beling of dialog acts in each utterance.

There are a number of relevant applications of
neural attention models. Nallapati et al. (2016)
proposed using sequence to sequence model to
summarize source code into natural language; they
used a LSTM as encoder and another attentional
LSTM and decoder to jointly learn content selec-
tion and realization. Dong and Lapata (2016) pre-
sented a sequence to sequence model with a tree
structure decoder to map natural language to its
logical form. The tree structure decoder shows
superior performance on data that has nested out-
put structure. It has also been used in other do-
mains including machine translation (Sutskever et
al., 2014; Bahdanau et al., 2014), and image cap-
tion generation (Fang et al., 2015). From this
perspective, the most related work is (Mei et al.,
2016) in which they proposed using a sequence-to-
sequence model to map navigational instructions
in natural language to actions, which is conceptu-
ally similar to our work. However, we start from
conversations and our structured data records are
more complex.

6 Conclusion

In this paper we have presented an end to
end method for extracting structured information
from unstructured conversations using an encoder-
decoder neural network. The restaurant-ordering
domain we study is distinguished from past work
by its conversational nature, and the need to han-
dle user corrections and modifications. We incor-
porate a memory gate and role information into
the encoder network to selectively keep impor-
tant information and capture interaction patterns
between conversation participants. Experimental
results on both a human transcribed data set and
ASR-recognized data set demonstrate the feasibil-
ity and effectiveness of our approach.

Acknowledgements

This work was done while Baolin Peng was an in-
tern at Microsoft Research.

457



References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A

simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 502–512, Cambridge, MA, Oc-
tober. Association for Computational Linguistics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

I. Beltagy and Chris Quirk. 2016. Improved semantic
parsers for if-then statements. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
726–736, Berlin, Germany, August. Association for
Computational Linguistics.

William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol
Vinyals. 2016. Listen, attend and spell: A neural
network for large vocabulary conversational speech
recognition. In 2016 IEEE International Con-
ference on Acoustics, Speech and Signal Process-
ing, ICASSP 2016, Shanghai, China, March 20-25,
2016, pages 4960–4964. IEEE.

David Chiang, Mona T. Diab, Nizar Habash, Owen
Rambow, and Safiullah Shareef. 2006. Parsing ara-
bic dialects. In Diana McCarthy and Shuly Wintner,
editors, EACL 2006, 11st Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, Proceedings of the Conference, April 3-
7, 2006, Trento, Italy. The Association for Computer
Linguistics.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar, October. Association for Com-
putational Linguistics.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence mod-
eling. In Proceedings of NIPS Deep Learning and
Representation Learning Workshop.

Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta,
Li Deng, Xiaodong He, Geoffrey Zweig, and Mar-
garet Mitchell. 2015. Language models for image
captioning: The quirks and what works. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 2: Short Papers), pages 100–105,
Beijing, China, July. Association for Computational
Linguistics.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
33–43, Berlin, Germany, August. Association for
Computational Linguistics.

Hao Fang, Saurabh Gupta, Forrest N. Iandola, Ru-
pesh Kumar Srivastava, Li Deng, Piotr Dollár, Jian-
feng Gao, Xiaodong He, Margaret Mitchell, John C.
Platt, C. Lawrence Zitnick, and Geoffrey Zweig.
2015. From captions to visual concepts and back. In
IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2015, Boston, MA, USA, June
7-12, 2015, pages 1473–1482.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Chiori Hori, Takaaki Hori, Shinji Watanabe, and
John R. Hershey. 2016. Context-sensitive and
role-dependent spoken language understanding us-
ing bidirectional and attention lstms. In Interspeech
2016, pages 3236–3240.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences. In
Advances in Neural Information Processing Systems
27, pages 2042–2050.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
655–665, Baltimore, Maryland, June. Association
for Computational Linguistics.

Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 913–920, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT-NAACL 2003. The Associa-
tion for Computational Linguistics.

Ioannis Konstas and Mirella Lapata. 2013. A global
model for concept-to-text generation. J. Artif. Intell.
Res. (JAIR), 48:305–346.

Jiwei Li, Michel Galley, Chris Brockett, Georgios Sp-
ithourakis, Jianfeng Gao, and Bill Dolan. 2016. A
persona-based neural conversation model. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 994–1003, Berlin, Germany, August.
Association for Computational Linguistics.

458



Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1412–1421, Lisbon,
Portugal, September. Association for Computational
Linguistics.

Hongyuan Mei, Mohit Bansal, and Matthew R. Wal-
ter. 2016. Listen, attend, and walk: Neural
mapping of navigational instructions to action se-
quences. In Proceedings of the Thirtieth AAAI Con-
ference on Artificial Intelligence, February 12-17,
2016, Phoenix, Arizona, USA., pages 2772–2778.

Grégoire Mesnil, Yann Dauphin, Kaisheng Yao,
Yoshua Bengio, Li Deng, Dilek Z. Hakkani-Tür, Xi-
aodong He, Larry P. Heck, Gökhan Tür, Dong Yu,
and Geoffrey Zweig. 2015. Using recurrent neural
networks for slot filling in spoken language under-
standing. IEEE/ACM Trans. Audio, Speech & Lan-
guage Processing, 23(3):530–539.

Ramesh Nallapati, Bing Xiang, and Bowen Zhou.
2016. Sequence-to-sequence rnns for text summa-
rization. CoRR, abs/1602.06023.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. In Proceedings of the 30th International
Conference on Machine Learning, ICML 2013, At-
lanta, GA, USA, 16-21 June 2013, pages 1310–1318.

Chris Quirk, Raymond Mooney, and Michel Galley.
2015. Language to code: Learning semantic parsers
for if-this-then-that recipes. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 878–888, Beijing,
China, July. Association for Computational Linguis-
tics.

Gerard Salton, A. Wong, and C. S. Yang. 1975. A vec-
tor space model for automatic indexing. Commun.
ACM, 18(11):613–620.

Andrew M. Saxe, James L. McClelland, and Surya
Ganguli. 2013. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks.
CoRR, abs/1312.6120.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Zoubin Ghahramani, Max Welling,
Corinna Cortes, Neil D. Lawrence, and Kilian Q.
Weinberger, editors, Advances in Neural Informa-
tion Processing Systems 27: Annual Conference
on Neural Information Processing Systems 2014,
December 8-13 2014, Montreal, Quebec, Canada,
pages 3104–3112.

Zhaopeng Tu, Yang Liu, Zhengdong Lu, Xiaohua Liu,
and Hang Li. 2016. Context gates for neural ma-
chine translation. CoRR, abs/1608.06043.

Yuk Wah Wong and Raymond Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the Human Language
Technology Conference of the NAACL, Main Con-
ference, pages 439–446, New York City, USA, June.
Association for Computational Linguistics.

Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.

Kaisheng Yao and Geoffrey Zweig. 2015. Sequence-
to-sequence neural net models for grapheme-to-
phoneme conversion. In INTERSPEECH 2015,
16th Annual Conference of the International Speech
Communication Association, Dresden, Germany,
September 6-10, 2015, pages 3330–3334. ISCA.

Kaisheng Yao, Geoffrey Zweig, and Baolin Peng.
2015. Attention with intention for a neural network
conversation model. CoRR, abs/1510.08565.

Steve J. Young, Milica Gasic, Blaise Thomson, and Ja-
son D. Williams. 2013. Pomdp-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE, 101(5):1160–1179.

Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR, abs/1212.5701.

459


