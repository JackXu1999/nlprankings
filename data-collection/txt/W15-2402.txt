



















































Reading metrics for estimating task efficiency with MT output


Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning, pages 6–13,
Lisbon, Portugal, 18 September 2015. c©2015 Association for Computational Linguistics.

Reading metrics for estimating task efficiency with MT output

Sigrid Klerke† Sheila Castilho∗ Maria Barrett† Anders Søgaard†
†CST, University of Copenhagen, Denmark

{skl, barrett, soegaard}@hum.ku.dk
∗CNGL/SALIS, Dublin City University, Ireland

castils3@mail.dcu.ie

Abstract
We show that metrics derived from record-
ing gaze while reading, are better prox-
ies for machine translation quality than
automated metrics. With reliable eye-
tracking technologies becoming available
for home computers and mobile devices,
such metrics are readily available even in
the absence of representative held-out hu-
man translations. In other words, reading-
derived MT metrics offer a way of get-
ting cheap, online feedback for MT system
adaptation.

1 Introduction

What’s a good translation? One way of thinking
about this question is in terms of what the trans-
lations can be used for. In the words of Doyon et
al. (1999), “a poor translation may suffice to deter-
mine the general topic of a text, but may not permit
accurate identification of participants or the spe-
cific event.” Text-based tasks can thus be ordered
according to their tolerance of translation errors,
as determined by actual task outcomes, and task
outcome can in turn be used to measure the qual-
ity of translation (Doyon et al., 1999).

Machine translation (MT) evaluation metrics
must be both adequate and practical. Human
task performance, say participants’ ability to ex-
tract information from translations, is perhaps the
most adequate measure of translation quality. Par-
ticipants’ direct judgements of translation qual-
ity may be heavily biased by perceived grammati-
cality and subjective factors, whereas task perfor-
mance directly measures the usefulness of a trans-
lation. Of course different tasks rely on different
aspects of texts, but some texts are written with a
single purpose in mind.

In this paper, we focus on logic puzzles. The
obvious task in logic puzzles is whether readers
can solve the puzzles when given a more or less
erroneous translation of the puzzle. We assume
task performance on logic puzzles is an adequate
measure of translation quality for logic puzzles.

Task-performance is not always a practical mea-
sure, however. Human judgments, whether from
direct judgments or from answering text-related
questions, takes time and requires recruiting and
paying individuals. In this paper, we propose vari-
ous metrics derived from natural reading behavior
as proxies of task-performance. Reading has sev-
eral advantages over other human judgments: It is
fast, is relatively unbiased, and, most importantly,
something that most of us do effortlessly all the
time. Hence, with the development of robust eye
tracking methods for home computers and mobile
devices, this can potentially provide us with large-
scale, on-line evaluation of MT output.

This paper shows that reading-derived metrics
are better proxies of task-performance than the
standard automatic metric BLEU. Note also that
on-line evaluation with BLEU is biased by what
held-out human translations you have available,
whereas reading-derived metrics can be used for
tuning systems to new domains and new text types.

In our experiments, we include simplifications
of logic puzzles and machine translations thereof.
Our experiments show, as a side result, that a
promising approach to optimizing machine trans-
lation for task performance is using text simpli-
fication for pre-processing the source texts. The
intuition is that translation noise is more likely to
make processing harder in more complex texts.

6



1.1 Contributions
• We present an experimental eye-tracking

study of 20 participants reading simplifica-
tions and human/machine translations of 80
logic puzzles.1

• This is, to the best of our knowledge, the
first study to correlate reading-derived met-
rics, human judgments and BLEU with task
performance for evaluating MT. We show
that human judgments do not correlate with
task performance. We also show that reading-
derived metrics correlate significantly with
task performance (−.36 < r < −.35), while
BLEU does not.

• Finally, our results suggest that practical MT
can benefit much from incorporating sen-
tence compression or text simplification as a
pre-processing step.

2 Summary of the experiment

In our experiments, we presented participants with
80 different logic puzzles and asked them to solve
and judge the puzzles while their eye movements
were recorded. Each puzzle was edited into five
different versions: the original version in English
(L2), a human simplification thereof (S(·)), a hu-
man translation into Danish (L1) and a machine
translation of the original (M(·)), as well as a ma-
chine translation of the simplification (M(S(·))).
Consequently, we used 400 different stimuli in
our experiments. The participants were 20 native
speakers of Danish with proficiency in English.

We record fixation count, reading speed and
regression proportion (amount of fixations land-
ing on previously read text) from the gaze data.
Increased attention in the form of reading time
and re-reading of previously read text are well-
established indicators of increased cognitive pro-
cessing load, and they correlate with typical read-
ability indicators like word frequency, length and
some complex syntactic structures (Rayner et al.,
2013; Rayner, 1998; Holmqvist et al., 2011). We
study how these measures correlate with MT qual-
ity, as reflected by human judgments and partici-
pants’ task performance.

We thereby assume that the chance of quickly
solving a task decreases when more resources are

1The data will be made available from https://
github.com/coastalcph

Math
A DVD player with a list price of $100 is marked down 30%.
If John gets an employee discount of 20% off the sale price,
how much does John pay for the DVD player?
1: 86.00
2: 77.60
3: 56.00
4: 50.00

Conclude
Erin is twelve years old. For three years, she has been asking
her parents for a dog. Her parents have told her that they
believe a dog would not be happy in an apartment, but they
have given her permission to have a bird. Erin has not yet
decided what kind of bird she would like to have.
Choose the statement that logically follows
1: Erin’s parents like birds better than they like dogs.
2: Erin does not like birds.
3: Erin and her parents live in an apartment.
4: Erin and her parents would like to move.

Evaluate
Blueberries cost more than strawberries.
Blueberries cost less than raspberries.
Raspberries cost more than both strawberries and blueberries.
If the first two statements are true, the third statement is:
1: TRUE
2: FALSE
3: Impossible to determine

Infer
Of all the chores Michael had around the house, it was his
least favorite. Folding the laundry was fine, doing the dishes,
that was all right. But he could not stand hauling the large
bags over to the giant silver canisters. He hated the smell and
the possibility of rats. It was disgusting.
This paragraph best supports the statement that:
1: Michael hates folding the laundry.
2: Michael hates doing the dishes.
3: Michael hates taking out the garbage.
4: Michael hates cleaning his room.

Figure 1: Logic puzzles of four categories. The
stimuli contain 20 of each puzzle category.

required for understanding the task. By keeping
the task constant, we can assess the relative im-
pact of the linguistic quality of the task formula-
tion. We hypothesise that our five text versions
(L1, L2, M(·), S(·), M(S(·))), can be ranked in
terms of processing ease, with greater processing
ease allowing for more efficient task solving.

The experiments are designed to test the fol-
lowing hypothesized partial ordering of the text
versions (summarized in Table 1): text simplifica-
tion (S(·)) eases reading processing relative to sec-
ond language reading processing (L2) while pro-
fessional human translations into L1 eases pro-
cessing more (H1). In addition, machine trans-
lated text (M(·)) is expected to ease the processing
load, but less so than machine translation of sim-

7



H1: L1 ≺ S(·) ≺ L2
H2: L1 ≺ M(S(·)) ≺ M(·) ≺ L2

Table 1: Expected relative difficulty of process-
ing. L1 and L2 are human edited texts in the par-
ticipants’ native and non-native language, respec-
tively, S(·) are manually simplified texts, M(·) are
machine translated texts and M(S(·)) are machine
translations of manually simplified texts.

plified text (M(S(·))), although both of these ma-
chine translated versions are still expected to be
more demanding than the professionally translated
original text (L1). Table 1 provides an overview of
the hypotheses and the expected relative difficulty
of processing each text version.

2.1 Summary of the findings
Our experimental findings are summarized as fol-
lows: The data supports the base assumption that
L1 is easier than L2. We only find partial support
for H1; While S(·) tends to be easier to compre-
hend than L2, also leading to improved task per-
formance, S(·) is ranked as easier to process than
L1 as often as the opposite, hypothesised rank-
ing. This indicates that our proficient L2 read-
ers may be benefitting as much from simplifica-
tion as from translation in reasoning tasks. We
also only find partial support for H2: The rela-
tive ordering of the human translations, L1, and
the two machine translated versions, M(S(·)) and
M(·), is supported and we find that the simplifi-
cation improves MT a lot with respect to read-
ing processing. However, participants tended to
perform better with the original L2 logic puzzles
compared to the machine translated versions. In
other words, MT hurts while both manual sim-
plification and translation help even proficient L2
readers. In sum, simplification seems necessary if
L2-to-L1 MT is to ease comprehension, and not
make understanding harder for readers with a cer-
tain L2 command level.

Importantly, we proceed to study the correla-
tion of our eye-tracking measures, human judg-
ments and BLEU (Papineni et al., 2002) with task
performance. There has been considerable work
on how various automatic metrics correlate with
human judgments, as well as on inter-annotator
consistency among humans judging the quality of
translations (Callison-Burch et al., 2008). Vari-
ous metrics have been proposed over the years,

but BLEU (Papineni et al., 2002) remains the de
facto state-of-the-art evaluation metric. Our find-
ings, related to evaluation, are, as already men-
tioned, that (a) human judgments surprisingly do
not correlate with task performance, and that (b)
the reading-derived metrics TIME and FIXATIONS
correlate strongly with task performance, while
BLEU does not. This, in our view, questions the
validity of human judgments and the BLEU metric
and shows that reading-derived MT metrics may
provide a better feedback in system development
and adaptation.

3 Detailed description of the experiment

3.1 Stimuli
In this section, we describe the texts we have used
for stimuli, as well as the experimental design and
our participants.

We selected a set of 80 logic puzzles written in
English, all with multiple-choice answers.2 The
most important selection criterium was that par-
ticipants have to reason about the text and cannot
simply recognize a few entities directly to guess
the answer. The puzzles were of four different cat-
egories, all designed to train logic reasoning and
math skills in an educational context. We chose
20 of each of the four puzzle categories to ensure
a wide variety of reasoning requirements. Figure 1
shows an example question from each category.

The English (L2) questions and multiple choice
answer options were translated into Danish (L1)
by professional translators. The question text was
manually simplified by the lead author (S(·)). Both
of the English versions were machine-translated
into Danish (M(·), M(S(·))).3 This results in the
five versions of the question texts, which were
used for analysis. The multiple-choice answer op-
tions were not simplified or machine translated.
Thus the participants saw either the original En-
glish answers or the human-translated Danish an-
swers, matching the language of the question text.
The average number of words and long words in
each of the five versions are reported in Table 2.

Simplification is not a well-defined task and
is often biased intentionally to fit a target au-
dience or task. To allow for comparison with
parallel simplification corpora, we classified the
applied simplification operations into the follow-
ing set of seven abstract simplification operations

2From LearningExpress (2005).
3Google Translate, accessed on 29/09/2014 23.33 CET.

8



# Long words # Words
Variant mean std mean std

L2 9.56 6.67 38.33 19.29
S(·) 8.78 5.90 35.78 17.43
L1 10.22 6.97 38.87 21.28
M(S(·)) 9.70 6.75 35.19 19.07
M(·) 10.35 6.74 36.53 19.04

Table 2: Mean and standard deviation of number
of words and number of words with more than
seven letters per question for all five versions.

Simplification %

Lexical substitution 27.4
Paraphrase 24.2
Deletion 23.1
Information reordering 11.3
Anaphora substitution 7.5
Discourse marker insertion 4.3
Sentence splitting 2.2

Table 3: Simplification operations (SOps). The
total number of applied SOps was 186, the average
number of SOps applied per question was 2.0 (std
1.3).

and present their relative proportion in Table 3:
Sentence splitting. information deletion and in-
formation reordering, discourse marker insertion
(e.g., and, but), anaphora substitution (e.g., Zoe’s
garden vs. the garden), other lexical substitu-
tions (e.g., dogwoods vs. dogwood trees) and para-
phrasing (e.g., all dogwoods vs. all kinds of dog-
wood trees). On average 2.0 simplification opera-
tions was performed per question, while a total of
28.7% of the questions were left unchanged dur-
ing simplification. All simplified questions still
required the reader to understand and reason about
the text. The simplifications were performed with
the multiple answer texts in mind; leaving any in-
formation referenced in the answers intact in the
question, even when deleting it would have sim-
plified the question text.

3.2 Experimental design
The experiment followed a Latin-square design
where each participant completed 40 trials, judg-
ing and solving 40 different puzzles, eight of each
of the five versions.

A trial consisted of three tasks (see Figure 2):

a comprehension task, a solving task and a com-
parison task. Each trial was preceded by a 1.5
second display of a fixation cross. The remain-
der of the trial was self-paced. During the en-
tire trial - i.e., for the duration of the three tasks
- the question text was presented on the top part of
the screen. In the comprehension task, the partici-
pant was asked to rate the comprehensibility of the
question text on a 7-point Likert scale that was pre-
sented at the bottom part of the screen. This score
is called COMPREHENSION, henceforth. This is
our rough equivalent of human judgments of trans-
lation quality. For the solving task, the multiple-
choice answer options was presented in the middle
part of the screen below the question text and the
participant indicated an answer or “don’t know”
option in the bottom part of the screen. The mea-
sure EFFICIENCY, which was also introduced in
Doherty and O’Brien (2014), is the number of cor-
rect answers given for a version, Cv over the time
spent reading and solving the puzzles of that ver-
sion, Sv: E = CvSv . This score is our benchmarking
metric below.

In the last task, COMPARISON, a different ver-
sion of the same question text was presented be-
low the first question text, always in the same lan-
guage. Participants were asked to assess which
version provided a better basis for solving the task
using a 7-point Likert scale with a neutral mid-
point. The three leftmost options favored the text
at the top of the screen, while the three rightmost
choices favored the text at the lower half of the
screen.

Each participant completed three demo trials
with the experimenter present. Participants were
kept naı̈ve with regards to the machine translation
aspect of the study. They were instructed to solve
the puzzles as quickly and accurately as possible
and to judge COMPREHENSION and COMPARI-
SON quickly. Each session included a 5-10 minute
break with refreshments halfway through. At the
end of the experiment a brief questionnaire was
completed verbally. All participants completed the
entire session in 70–90 minutes.4

3.2.1 Apparatus
The stimuli were presented in black letters in the
typeface Verdana with a letter size of 20 pixels (ca.
.4◦ visual angle) on a light gray background with
100 pixels margins. The eye tracker was a Tobii

4Participants received a voucher for 10 cups of tea/coffee
upon completion.

9



One trial

Task 1: read for comprehension Task 2: solve puzzle Task 3: compare to other version

Figure 2: Illustration of one trial. Each trial consists of three individual tasks. The top third of the screen
displays the target text and is fixed for the duration of the entire trial.

X120, recording both eyes with 120hz sampling
rate. We used Tobii Studio standard settings for
fixation detection. The stimuli was presented on a
19” display with a resolution of 1920 x 1080 pix-
els and a viewing distance of ca 65 cm. Here we
focus on the initial reading task and report total
reading time per word (TIME), number of fixations
per word (FIXATIONS) and proportion of regres-
sions (REGRESSIONS). The calculations of the
eyetracking measures are detailed in Section 4.3.

3.2.2 Participants
We recruited participants until we obtained a to-
tal of 20 recordings of acceptable quality. In this
process we discarded two participants due to sam-
pling loss. Another two participants were dis-
missed due to unsuccessful calibration. All par-
ticipants completed a pre-test questionnaire iden-
tifying themselves as native Danish speakers with
at least a limited working proficiency of English.
None of the participants had been diagnosed with
dyslexia, and all had normal or corrected to nor-
mal vision. The 20 participants (4 males) were be-
tween 20 and 34 years old (mean 25.8) and mini-
mum education level was ongoing bachelor’s stud-
ies.

4 Results

The mean values for all metrics and the derived
rankings of the five versions are presented in
Table 4. Significance is computed using Stu-
dent’s paired t-test, comparing each version to
the version with the largest measured value. Ta-
ble 5 presents correlations with task performance

(EFFICIENCY) for each measure. We describe the
correlations, and their proposed interpretation, in
Section 4.4.

4.1 Subjective measures

We elicited subjective evaluations of text com-
prehension and pairwise comparisons of versions’
usefulness for solving the puzzles. Note that par-
ticipants evaluate MT output significantly lower
than human-edited versions.

We treated the pairwise COMPARISON scores as
votes, counting the preference of one version as
equally many positive and negative votes on the
preferred version and the dis-preferred version, re-
spectively. With this setup, we maintain zero as a
neutral evaluation. COMPARISON was only made
within the same language, so the scores should not
be interpreted across languages. Note, however,
how COMPARISON results show a clear ranking
of versions within each language.

4.2 Task performance measures

The task performance is reported as the EFFI-
CIENCY, i.e., correct answers per minute spent
reading and solving puzzles. We observe that the
absolute performance ranges from 48% to 52%
correct answers. This is well above chance level
(27%), and does not differ significantly between
the five versions, reflecting that the between-
puzzles difference in difficulty level, as expected,
is much larger than the between-versions differ-
ence.

EFFICIENCY, however, reveals a clearer rank-
ing. Participants were less efficient solving logic

10



VERSION µ RANKINGSL1 M(S(·)) M(·) S(·) L2
COMPREHENSION 5.58 **4.51 **4.50 5.61 5.46 S(·) ≺ L1 ≺ L2 ≺ M(S(·)) ≺ M(·)
COMPARISON 1.62 **−.54 **−1.07 .43 **−.43 L1 ≺ M(S(·)) ≺ M(·) | S(·) ≺ L2
EFFICIENCY .94 .90 **0.80 1.0 .87 S(·) ≺ L1 ≺ M(S(·)) ≺ L2 ≺ M(·)
TIME .54 .62 .65 .55 .54 L1 ≺ L2 ≺ S(·) ≺ M(S(·)) ≺ M(·)
REGRESSIONS 15.59 16.49 16.78 13.76 14.40 S(·) ≺ L2 ≺ L1 ≺ M(S(·)) ≺ M(·)
REGRESSIONS 17.77 18.46 19.15 15.55 16.55 S(·) ≺ L2 ≺ L1 ≺ M(S(·)) ≺ M(·)

Table 4: Mean values for the five text versions. COMPREHENSION and COMPARISON are Likert scale
scores respectively ranging from 0 to 7 and from−3 to 3, EFFICIENCY is correct answers relative to read-
ing speed, TIME is seconds per word, FIXATIONS is number of fixations per word and REGRESSIONS is
proportion of re-fixations (**: Student’s paired t-test relative to largest mean value p < 0.001)

puzzles when presented with machine translations
of the original puzzles. The machine transla-
tions of the simplified puzzles actually seemingly
eased task performance, compared to using the
English originals, but differences are not statisti-
cally significant. The simplified English puzzles
led to the best task performance.

4.3 Eye-tracking measures

The reading times in seconds per word (TIME)
are averages over reading times while fixating at
the question text located on the upper part of the
screen during the first sub-task of each trial (judg-
ing comprehension). This measure is comparable
to normalized total reading time in related work.
Participants spent most time on the machine trans-
lations, whether of the original texts or the simpli-
fied versions.

The measure FIXATIONS similarly was
recorded on the question part of the text during
the initial comprehension task, normalized by
text length, and averaged over participants and
versions. Again we observe a tendency towards
more fixations on machine translated text, and
fewest on the human translations into Danish.

Finally, we calculated REGRESSIONS during
initial reading as the proportion of fixations from
the furthest word read to a preceding point in the
text. Regressions may indicate confusion and on
average account for 10-15% of fixations during
reading (Rayner, 1998). Again we see more re-
gressions with machine translated text, and fewest
with simplified English puzzles.

4.4 Correlations between measures

We observe the following correlations between
our measures. All correlations with EFFICIENCY
are shown in Table 5. First of all, we found no

Data used r p ≤ .001

COMPREHENSION
all .25 -
M(S(·)) .36 -
M(·) -.27 -

COMPARISON
all .13 -
M(S(·)) .06 -
M(·) .26 -

TIME
all -.35 X
M(S(·)) -.19 -
M(·) -.54 -

FIXATIONS
all -.36 X
M(S(·)) -.26 -
M(·) -.57 -

REGRESSIONS
all -.17 -
M(S(·)) .01 -
M(·) -.33 -

BLEU M(S(·)) -.13 -M(·) -.17 -

Table 5: Correlations with EFFICIENCY (Pear-
son’s r). BLEU only available on translated text.
Correlation reported on these subsets for compa-
rability.

correlations between subjective measures and eye-
tracking measures nor between subjective mea-
sures and task performance. The two subjec-
tive measures, however, show a strong correlation
(Spearman’s r = .50 p < .001). EFFICIENCY
shows significant negative correlation with both of
the eye-tracking measures TIME (Pearson’s r =
−.35 p < .001 and FIXATIONS (Pearson’s r =
−.36 p < .001), but not REGRESSIONS . Within
the group of eye-tracking measures TIME and
FIXATION exhibit a high correlation (r = 0.94
p < .001). REGRESSIONS is significantly neg-
atively correlated with both of these (Pearson’s
r = −.38 p < .001 and Pearson’s r = −.43
p < .001, respectively).

We obtain BLEU scores (Papineni et al., 2002)

11



by using the human-translated Danish text (L1) as
reference for both of the MT outputs, M(·) and
M(S(·)). The overall BLEU score for M(·) version
is .691, which is generally considered very good,
and .670 for M(S(·)). The difference is not sur-
prising, since M(S(·)) inputs a different (simpler)
text to the MT system. On the other hand, given
that our participants tended to be more efficiently
comprehending and solving the logic puzzles us-
ing M(S(·)), this already indicates that BLEU is
not a good metric for talking about the usefulness
of translations of instructional texts such as logic
puzzles.

Our most important finding is that BLEU does
not correlate with EFFICIENCY, while two of our
reading-derived metrics do. In other words, the
normalised reading time and fixation counts are
better measures of task performance, and thereby
of translation quality, than the state-of-the-art met-
ric, BLEU in this context. This is an important
finding since reading-derived metrics are poten-
tially also more useful as they do not depend on
the availability of professional translators.

5 Discussion

Several of our hypotheses were in part falsified.
L2 is solved more efficiently by our participants
than M(·), not the other way around. Also, M(S(·))
is judged as harder to comprehend than S(·) and
consistently ranked so by all metrics. These ob-
servations suggest that MT is not assisting our par-
ticipants despite the fact that L2 ranks lower than
L1 in four out of five comparisons. Our partici-
pants are university students and did not report to
have skipped any questions due to the English text
suggesting generally very good L2 skills.

If we assume that EFFICIENCY – as a mea-
sure of task performance – is a good measure
of translation quality (or usefulness), we see that
the best indicator of translation quality that only
takes the initial reading into account are FIXA-
TIONS and TIME. This indicates that FIXATIONS
and TIME may be better MT benchmarking met-
rics than BLEU.

6 Related work

Eye tracking has been used for MT evaluation in
both post-editing and instruction tasks (Castilho et
al., 2014; Doherty and O’Brien, 2014).

Doherty et al. (2010) also used eye-tracking
measures for evaluating MT output and found

fixation count and gaze time to correlate nega-
tively with binary quality judgments for transla-
tion segments, whereas average fixation duration
and pupil dilation were not found to vary reliably
with the experimental conditions. A notable short-
coming of that study is that the translated segments
in each category were different, making it impos-
sible to rule out that the observed variation in both
text quality and cognitive load was caused in part
by an underlying variation in content complexity.

This shortcoming was alleviated in a recent re-
analysis of previous experiments (Doherty and
O’Brien, 2014; Doherty et al., 2012) which com-
pares the usability of raw machine translation out-
put in different languages and the original, well-
formed English input. In order to test usability,
a plausible task has to be set up. In this study
the authors used an instructional text on how to
complete a sequence of steps using a software ser-
vice, previously unknown to the participants. MT
output was obtained for four different languages
and three to four native speakers worked with each
output. Participants’ subjective assessment of the
usability of the instructions, their performance in
terms of efficiency and the cognitive load they en-
countered as measured from eye movements were
compared across languages. The results of this
study supports the previous finding that fixation
count and total task time depends on whether the
reader worked with the original or MT output, at
least when the quality of the MT output is low.
In addition, goal completion and efficiency (total
task time relative to goal completion) as well as
the number of shifts (between instructions and task
performance area) were shown to co-vary with the
text quality.

Castilho et al. (2014) employed a similar design
to compare the usability of lightly post-edited MT
output to raw MT output and found that also light
post-editing was accompanied by fewer fixations
and lower total fixation time (proportional to total
task time) as well as fewer attentional shifts and
increased efficiency.

In contrast, Stymne et al. (2012) found no sig-
nificant differences in total fixation counts and
overall gaze time (proportional to total task time),
when directly comparing output of different MT
systems with expected quality differences. How-
ever, they showed that both of these two eye-
tracking measures were increased for the parts
of the text containing errors in comparison with

12



error-free passages. In addition, they found gaze
time to vary with specific error types in machine
translated text.

From an application perspective, Specia (2011)
suggested the time-to-edit measure as an objective
and accessible measure of translation quality. In
their study it outperformed subjective quality as-
sessments as annotations for a model for transla-
tion candidate ranking. Their tool was aimed at
optimizing the productivity in post-editing tasks.

Eye tracking can be seen as a similarly objec-
tive metric for fluency estimation (Stymne et al.,
2012). The fact that eye tracking does not rely
on translators makes annotation even more acces-
sible.

Both Doherty and O’Brien (2014) and Castilho
et al. (2014) found subjective comprehensibility,
satisfaction and likelihood to recommend a prod-
uct to be especially sensitive to whether the in-
structional text for the product was raw MT out-
put. This suggests that the lower reliability of sub-
jective evaluations as annotations could be due to
a bias against MT-specific errors. Only Stymne
et al. (2012) report the correlations between eye
movement measures and subjective assessments
and found only moderate correlations.

This work is to the best of our knowledge the
first to study the correlation of reading-derived
MT metrics and task performance. Since we be-
lieve task performance to be a more adequate mea-
sure of translation quality – especially when the
texts are designed with a specific task in mind – we
therefore believe this to be a more adequate study
of the usefulness of reading-derived MT metrics
than previous work.

7 Conclusion

We presented an eye-tracking study of participants
reading original, simplified, and human/machine
translated logic puzzles. Our analysis shows that
the reading-derived metrics TIME and FIXATIONS
obtained from eye-tracking recordings can be used
to assess translation quality. In fact, such met-
rics seem to be much better proxies of task per-
formance, i.e., the practical usefulness of trans-
lations, than the state-of-the-art quality metric,
BLEU.

References
Chris Callison-Burch, Cameron Fordyce, Philipp

Koehn, Christof Monz, and Josh Schroeder. 2008.

Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation, pages 70–106. Association for
Computational Linguistics.

Sheila Castilho, Sharon O’Brien, Fabio Alves, and
Morgan O’Brien. 2014. Does post-editing increase
usability? a study with Brazilian Portuguese as tar-
get language. In EAMT.

Stephen Doherty and Sharon O’Brien. 2014. Assess-
ing the usability of raw machine translated output:
A user-centered study using eye tracking. Inter-
national Journal of Human-Computer Interaction,
30(1):40–51.

Stephen Doherty, Sharon O’Brien, and Michael Carl.
2010. Eye tracking as an mt evaluation technique.
Machine translation, 24(1):1–13.

Stephen Doherty, Dorothy Kenny, and Andrew Way.
2012. A user-based usability assessment of raw ma-
chine translated technical instructions. In AMTA.

Jennifer Doyon, Kathryn B Taylor, and John S White.
1999. Task-based evaluation for machine transla-
tion. In Proceedings of Machine Translation Summit
VII, volume 99.

Kenneth Holmqvist, Marcus Nyström, Richard An-
dersson, Richard Dewhurst, Halszka Jarodzka, and
Joost Van de Weijer. 2011. Eye tracking: A com-
prehensive guide to methods and measures. Oxford
University Press.

LearningExpress. 2005. 501 Challenging Logic and
Reasoning Problems. 501 Series. LearningExpress.

Kishore Papineni, Salim Roukus, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL.

Keith Rayner, Alexander Pollatsek, and D Reisberg.
2013. Basic processes in reading. The Oxford
Handbook of Cognitive Psychology, pages 442–461.

Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological bulletin, 124(3):372.

Lucia Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In Pro-
ceedings of the 15th Conference of the European As-
sociation for Machine Translation, pages 73–80.

Sara Stymne, Henrik Danielsson, Sofia Bremin,
Hongzhan Hu, Johanna Karlsson, Anna Prytz Lil-
lkull, and Martin Wester. 2012. Eye tracking as a
tool for machine translation error analysis. In LREC,
pages 1121–1126.

13


