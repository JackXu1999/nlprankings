



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2131–2141
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1195

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2131–2141
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1195

Semantic Parsing of Pre-university Math Problems

Takuya Matsuzaki1, Takumi Ito1, Hidenao Iwane2, Hirokazu Anai2, Noriko H. Arai3
1 Nagoya University, Japan

{matuzaki,takumi i}@nuee.nagoya-u.ac.jp
2 Fujitsu Laboratories Ltd., Japan

{iwane,anai}@jp.fujitsu.com
3 National Institute of Informatics, Japan

arai@nii.ac.jp

Abstract

We have been developing an end-to-end
math problem solving system that accepts
natural language input. The current paper
focuses on how we analyze the problem
sentences to produce logical forms. We
chose a hybrid approach combining a shal-
low syntactic analyzer and a manually-
developed lexicalized grammar. A feature
of the grammar is that it is extensively
typed on the basis of a formal ontology
for pre-university math. These types are
helpful in semantic disambiguation inside
and across sentences. Experimental results
show that the hybrid system produces a
well-formed logical form with 88% preci-
sion and 56% recall.

1 Introduction

Frege and Russell, the initiators of the mathemati-
cal logic, delved also into the exploration of a the-
ory of natural language semantics (Frege, 1892;
Russell, 1905). Since then, symbolic logic has
been a fundamental tool and a source of inspira-
tion in the study of language meaning. It suggests
that the formalization of the two realms, mathe-
matical reasoning and language meaning, is ac-
tually the two sides of the same coin – probably,
we could not even conceive the idea of formaliz-
ing language meaning without grounding it onto
mathematical reasoning. This point was first clari-
fied by Tarski (1936; 1944) mainly on formal lan-
guages and then extended to natural languages by
Davidson (1967). Montague (1970a; 1970b; 1973)
further embodied it by putting forward a terrify-
ingly arrogant and attractive idea of seeing a natu-
ral language as a formal language.

The automation of end-to-end math problem
solving thus has an outstanding status in the re-

Define the two straight lines L1 and L2 on the xy-plane
as L1: y = 0 (x-axis) and L2: y =

√
3x. Let P be

a point on the xy-plane. Let Q be the point symmetric
to P about the straight line L1, and let R be the point
symmetric to P about the straight line L2. Answer the
following questions:

(1) Let (a, b) be the coordinates of P , then represent the
coordinates of R using a and b.

(2) Assuming that the distance between the two points
Q and R is 2, find the locus C of P .

(3) When the point P moves on C, find the maximum
area of the triangle PQR and the coordinates of P
that gives the maximum area.

(Hokkaido Univ., 1999-Sci-3)

Figure 1: Example problem

search themes in natural language processing. The
conceptual basis has been laid down, which con-
nects text to the truth (= answer) through reason-
ing. However, we have not seen a fully automated
system that instantiates it end-to-end. We wish to
add a piece to the big picture by materializing it.

Past studies have mainly targeted at primary
school level arithmetic word problems (Bobrow,
1964; Charniak, 1969; Kushman et al., 2014; Hos-
seini et al., 2014; Shi et al., 2015; Roy and Roth,
2015; Zhou et al., 2015; Koncel-Kedziorski et al.,
2015; Mitra and Baral, 2016; Upadhyay et al.,
2016). In their nature, arithmetic questions are
quantifier-free. Moreover they tend to include
only ∧ (and) as the logical connective. The main
challenge in these works was to extract simple nu-
merical relations (most typically equations) from
a real-world scenario described in a text.

Seo et al. (2015) took SAT geometry ques-
tions as their benchmark. However, the nature of
SAT geometry questions restricts the resulting for-
mula’s complexity. In §3, we will show that none
of them includes ∀ (for all), ∨ (or) or→ (implies).
It suggests that this type of questions require little
need to analyze the logical structure of the prob-
lems beyond conjunctions of predicates.

2131

https://doi.org/10.18653/v1/P17-1195
https://doi.org/10.18653/v1/P17-1195


Problem shallow
parsing

coreference
resolution

math expr.
analysis

semantic
parsing

discourse
parsing

formula
rewriting

reasoning Solution

LexiconOntology Axioms CASATP

Type
constraint

Figure 2: Overview of the end-to-end math problem solving system

We take pre-university math problems falling
in the theory of real-closed fields (RCF) as our
benchmark because of their variety and complex-
ity. The subject areas include real and linear al-
gebra, complex numbers, calculus, and geometry.
Furthermore, many problems involve more than
one subject: e.g., algebraic curves and calculus
as in Fig. 1. Their logical forms include all the
logical connectives, quantifiers, and λ-abstraction.
Our goal is to recognize the complex logical struc-
tures precisely, including the scopes of the quanti-
fiers and other logical operators.

In the rest of the paper, we first present an
overview of an end-to-end problem solving sys-
tem (§2) and analyze the complexity of the pre-
university math benchmark in comparison with
others (§3). Among the modules in the end-to-end
system, we focus on the sentence-level semantic
parsing component and describe an extensively-
typed grammar (§4 and §5), an analyzer for the
math expressions in the text (§6), and two seman-
tic parsing techniques to fight against the scarcity
of the training data (§7) and the complexity of the
domain (§8). Experimental results show the effec-
tiveness of the presented techniques as well as the
complexity of the task through an in-depth analy-
sis of the end-to-end problem solving results (§9).

2 End-to-end Math Problem Solving

Fig. 2 presents an overview of our end-to-end
math problem solving system. A math problem
text is firstly analyzed with a dependency parser.
Anaphoric and coreferential expressions in the text
are then identified and their antecedents are de-
termined. We assume the math formulas in the
problems are encoded in MathML presentation
mark-up. A specialized parser processes each
one of them to determine its syntactic category
and semantic content. The semantic representa-
tion of each sentence is determined by a semantic
parser based on Combinatory Categorial Grammar
(CCG) (Steedman, 2001, 2012). The output from
the CCG parser is a ranked list of sentence-level
logical forms for each sentence.

Dataset
Succeeded Failed

Success% Avg. Timeout OtherTime
DEV 75.3% (131/174) 10.5s 16.7% 8.1%
TEST 78.2% (172/220) 16.2s 15.0% 6.8%

Table 1: Performance of the reasoning module on
manually formalized pre-university problems

After the sentence-level processing steps,
we determine the logical relations among the
sentence-level logical forms (discourse parsing)
by a simple rule-based system. It produces a tree
structure whose leaves are labeled with sentences
and internal nodes with logical connectives. Free
variables in the logical form are then bound by
some quantifiers (or kept free) and their scopes
are determined according to the logical structure
of the problem. A semantic representation of a
problem is obtained as a formula in a higher-order
logic through these language analysis steps.

The logical representation is then rewritten us-
ing a set of axioms that define the meanings of
the predicate and function symbols in the formula,
such as maximum defined as follows:

maximum(x, S)↔ x ∈ S ∧ ∀y(y ∈ S → y ≤ x),
as well as several logical rules such as β-
reduction. We hope to obtain a representation of
the initial problem expressed in a decidable math
theory such as RCF through these equivalence-
preserving rewriting. Once we find such a for-
mula, we invoke a computer algebra system (CAS)
or an automatic theorem prover (ATP) to derive the
answer.

The reasoning module (i.e., the formula rewrit-
ing and the deduction with CAS and ATP) of
the system has been extensively tested on a large
collection of manually formalized pre-university
math problems that includes more than 1,500
problems. It solves 70% of the them in the time
limit of 10 minutes per problem. Table 1 shows the
rate of successfully solved problems in the man-
ually formalized version of the benchmark prob-
lems used in the current paper.

2132



Prob- Avg. Avg. Uniq. Atoms ∃ ∀ λ ∧ ∨ ¬ → Uniquelems tokens sents. words sketches
JOBS 640 9.83 1.00 391 4.63 1.71 0.00 0.00 1.06 0.01 0.13 0.00 8
GEOQUERY 880 8.56 1.00 284 4.25 1.70 0.00 1.04 1.18 0.00 0.02 0.00 20
GEOMETRY 119 23.64 1.74 202 11.00 7.45 0.00 0.06 1.00 0.00 0.04 0.00 4
UNIV (DEV) 174 70.34 3.45 363 10.99 5.10 1.10 1.11 1.71 0.02 0.49 0.35 76
UNIV (TEST) 220 70.85 4.02 366 9.70 4.58 1.10 1.00 1.62 0.02 0.28 0.23 72

Table 2: Profile of pre-university math benchmark data and other semantic parsing benchmark data sets

JOBS GEOQUERY GEOMETRY UNIV (DEV)
∃P 81% ∃P 46% ∃P 94% ∃P 25%
P 6% ∃P (λ∃P ) 24% ∃(P∧¬P ) 3% ∃(P∧¬P ) 7%

∃(P∧¬∃P ) 5% P (λ∃P ) 8% ∃(P∧P (λP )) 2% P (λ∃P ) 5%
∃(P∧¬P ) 5% ∃(P∧P (λ∃P )) 7% P (λ∃P ) 1% ∃(P∧P (λf)) 4%

97% 85% 100% 41%

Table 3: Top four most frequest sketches and their coverage over the dataset

Sketch Freq.
∀(P → ∃(∀(P → P )∧P )) 2

∃(∃(¬P∧P )∧P∧P (λf))∧P (λ(P → P ))) 1
∃(P∧P (λ(¬P∧∃(∃P∧P )))) 1

∃(P∧P (λf))∧P (λ(¬P∧P ))∧P (λP )) 1

Table 4: Less frequent sketches in UNIV (DEV)

3 Profile of the Benchmark Data

Our benchmark problems, UNIV, were collected
from the past entrance exams of seven top-ranked
universities in Japan. In the exams held in odd
numbered years from 1999 to 2013, we exhaus-
tively selected the problems which are ultimately
expressible in RCF. They occupied 40% of all the
problems. We divided the problems into two sets:
DEV for development (those from year 1999 to
2005) and TEST for test (those from year 2007
to 2013). DEV was used for the lexicon devel-
opment and the tuning of the end-to-end system.
The problem texts (both in English and Japanese)
with MathML mark-up and manually translated
logical forms are publicly available at https:
//github.com/torobomath.

The manually translated logical forms were for-
mulated in a higher-order semantic language intro-
duced later in the paper. The translation was done
as faithfully as possible to the original wordings of
the problems. They thus keep the inherent logical
structures expressed in natural language.

Table 2 lists several statistics of the UNIV prob-
lems in the English version and their manual for-
malization. For comparison, the statistics of three
other benchmarks are also listed. JOBS and GEO-
QUERY are collections of natural language queries
against databases. They have been widely used as

benchmarks for semantic parsing (e.g., Tang and
Mooney, 2001; Zettlemoyer and Collins, 2005,
2007; Kwiatkowski et al., 2010, 2011; Liang et al.,
2011). The queries are annotated with logical
forms in Prolog. We converted them to equiva-
lent higher-order formulas to collect comparable
statistics. GEOMETRY is a collection of SAT ge-
ometry questions compiled by Seo et al. (2015).
We formalized the GEOMETRY questions1 in our
semantic language in the same way as UNIV.

In Table 2, the first column lists the number
of problems. The next three provide statistics of
the problem texts: average number of words and
sentences in a problem (‘Avg. tokens’ and ‘Avg.
sents’), and the number of unique words in the
whole dataset.2 They reveal that the sentences in
UNIV are significantly longer than the others and
more than three sentences have to be correctly pro-
cessed for a problem.

The remaining columns provide the statistics
about the logical complexities of the problems.
‘Atoms’ stands for the average number of the oc-
currences of predicates per problem. The next
three columns list the number of variables bound
by ∃, ∀, and λ. We count sequential occurrences of
the same binder as one. The columns for ∧, ∨, ¬,
and → list the average number of them per prob-
lem.3 We can see UNIV includes a wider variety
of quantifiers and connectives than the others.

The final column lists the numbers of unique
‘sketches’ of the logical forms in the dataset. What

1Including all conditions expressed in the diagrams.
2 All the math formulas in the texts were replaced with a

special token “MATH” before counting words.
3 ∧ and ∨ was counted as operators with arbitrary arity.

E.g., there is only one ∧ in A ∧B ∧ C.

2133



we call ‘sketch’ here is a signature that encodes the
overall structure of a logical form. Table 3 shows
the top four most frequent sketches observed in the
datasets. In a sketch, P stands for a (conjunction
of) predicate(s) and f stands for a term. ∃, ∀, and
λ stand for (immediately nested sequence of) the
binders.

To obtain the sketch of a formula φ, we first re-
place all the predicate symbols in φ to P and func-
tion symbols and constants to f . We then elimi-
nate all variables in φ and ‘flatten’ it by applying
the following rewriting rules to the sub-formulas
in φ in the bottom-up order:

f(..., f(α1, α2, ..., αn), ...)⇒ f(..., α1, α2, ..., αn, ...)
P (..., f(α1, α2, ..., αn), ...)⇒ P (..., α1, α2, ..., αn, ...)

α ∨ α ∨ β ⇒ α ∨ β, α ∧ α⇒ α
∃∃ψ ⇒ ∃ψ, ∀∀ψ ⇒ ∀ψ, λλψ ⇒ λψ

Finally, we sort the arguments of P s and fs and
remove the duplicates among them. For instance,
to obtain the sketch of the following formula:

∀k∀m
(

maximum(m, set(λe.(e < k)))
→ k − 1 ≤ m ∧m < k

)
,

we replace the predicate/function symbols as in:

∀k∀m
(

P (m, f(λe.P (e, k)))
→ P (f(k, f),m) ∧ P (m, k)

)
,

and then eliminate the variables to have:

∀∀(P (f(λP ))→ P (f(f)) ∧ P ),

and finally flatten it to:

∀(P (λP )→ P ).

Table 3 shows that a wide variety of structures
are found in UNIV while other data sets are dom-
inated by a small number of structures. Table 4
presents some of less frequent sketches found in
UNIV (DEV). In actuality, 67% of the unique
sketches found in UNIV (DEV) occur only once
in the dataset. These statistics suggest that the dis-
tribution of the logical structures found in UNIV,
and math text in general, is very long-tailed.

4 A Type System for Pre-university Math

Our semantic language is a higher-order logic
(lambda calculus) with parametric polymorphism.
Table 5 presents the types in the language. The
atomic types are defined so that they capture
the selectional restriction of verbs and other

truth values Bool
numbers Z (integers), Q (rationals),

R (reals), C (complex)
polynomials Poly
single variable functions R2R (R→R), C2C (C→C)
single variable equations EqnR (in R), EqnC (in C)
points in 2D/3D space 2d.Point, 3d.Point
geometric objects 2d.Shape, 3d.Shape
vectors and matrices 2d.Vector, 3d.Vector
matrices 2d.Matrix, 3d.Matrix
angles 2d.Angle, 3d.Angle
number sequences Seq
cardinals and ordinals Card, Ord
ratios among numbers Ratio
limit values of functions LimitVal
integer division QuoRem
polymorphic containers SetOf(α), ListOf(α)
polymorphic tuples Pair(α, β), Triple(α, β, γ)

Table 5: Types defined in the semantic language

argument-taking phrases as precisely as possible.
For instance, an equation in real domain, e.g.,
x2 − 1 = 0, can be regarded as a set of reals, i.e.,
{x | x2 − 1 = 0}. However, we never say ‘a so-
lution of a set.’ We thus discriminate an equation
from a set in the type system even though the con-
cept of equation is mathematically dispensable.

Entities of equation and set are built by con-
structor functions that take a higher-order term
as the argument as in eqn(λx.x2 − 1) and
set(λx.x2 − 1). Related concepts such as ‘solu-
tion’ and ‘element’ are defined by the axioms for
corresponding function and predicate symbols:

∀f∀x(solution(x, eqn(f))↔ fx)
∀s∀x(element(x, set(s))↔ sx).

Distinction of cardinal numbers (Card) and
ordinal numbers (Ord), and the introduction of
‘integer division’ type (QuoRem) are also lin-
guistically motivated. The former is neces-
sary to capture the difference between, e.g., ‘k-
th integer in n1, n2, . . . , nm’ and ‘k integers in
n1, n2, . . . , nm.’ An object of type QuoRem
is conceptually a pair of integers that represent
the quotient and the remainder of integer divi-
sion. It is linguistically distinct from the type of
Pair(Z,Z) because, e.g., in

Select a pair of integers (n,m) and divide n by
m. If the remainder (of φ) is zero, ...

the null (i.e., omitted) pronoun φ has ‘the result of
division n/m’ as its antecedent but not (n,m).

Polymorphism is a mandatory part of the lan-
guage. Especially, the semantics of plural noun

2134



>

>

When
S/(S\NP )/Sa

: λP.λQ.π2(P )→ Q(π1(P ))

any k in K is divided by m,
Sa

: (quorem(k,m), (∃k; k ∈ K))
S/(S\NP ) : λQ.(∃k; k ∈ K)→ Q(quorem(k,m)) >

the quotient
T\NP/(T\NP )

: λP.λx.P (quo of(x))

is 3.
S\NP

: λx.(x = 3)

S\NP : λx.quo of(x) = 3
S : (∃k; k ∈ K)→ quo of(quorem(k,m)) = 3

Figure 3: Sketch of the derivation tree for a sentence including an action verb and quantification

phrases is expressed by polymorphic lists and tu-
ples: e.g., ‘the radii of the circles C1, C2, and C3’
is of type ListOf(R) and ‘the function f and its
maximum value’ is of type Pair(R2R,R).

5 Lexicon and Grammar

5.1 Combinatory Categorial Grammar
An instance of CCG grammar consists of a lexi-
con and a small number of combinatory rules. A
lexicon is a set of lexical items, each of which as-
sociates a word surface form with a syntactic cat-
egory and a semantic function: e.g.,

sum :: NP/PP : λx.sum of(x)

intersects :: S\NP/PP : λy.λx.intersect(x, y)
A syntactic category is one of atomic categories,

such as NP, PP, and S, or a complex category in the
form of X/Y or X\Y, where X and Y are syntactic
categories.

The syntactic categories and the semantic func-
tions of constituents are combined by applying
combinatory rules. The most fundamental rules
are forward (>) and backward (<) application:

>
X/Y : f Y : x

X : fx
<

Y : x X\Y : f
X : fx

The atomic categories are further classified by
features such as num(ber) and case of noun
phrases. In the current paper, the features are writ-
ten as in NP[num=pl,case=acc].

5.2 A Japanese CCG Grammar and Lexicon
We developed a Japanese CCG following the anal-
ysis of basic constructions by Bekki (2010) but
significantly extending it by covering various phe-
nomena related to copula verbs, action verbs,
argument-taking nouns, appositions and so forth.
The semantic functions are defined in the format
of a higher-order version of dynamic predicate
logic (Eijck and Stokhof, 2006). The dynamic
property is necessary to analyze semantic phe-
nomena related to quantifications, such as donkey
anaphora. In the following examples, we use En-
glish instead of Japanese and the standard notation
of higher-order logic for the sake of readability.

We added two atomic categories, Sn and Sa, to
the commonly used S, NP, and N. Category Sn is
assigned to a proposition expressed as a math for-
mula, such as ‘x > 0’. Semantically it is of type
Bool but syntactically it behaves both like a noun
phrase and a sentence.

Category Sa is assigned to a sentence where the
main verb is an action verb such as add and ro-
tate. Such a sentence introduces the result of the
action as a discourse entity (i.e., what can be an
antecedent of coreferential expressions). The ac-
tion verbs can also mediate quantification as in:

When any k∈K is divided by m, the quotient is 3.
∀k(k ∈ K → quo of(quorem(k,m)) = 3)

where quorem(k,m) represents the result of the
division (i.e., the pair of the quotient and the re-
mainder) and quo of is a function that extracts
the quotient from it. To handle such phenomena,
we posit the semantic type of Sa as Pair(α,
Bool) where the two components respectively
bring the result of an action and the condition
on it (including quantification). Fig. 3 presents a
derivation tree for the above example.4

The atomic category NP, N, and Sa in our gram-
mar have type feature. Its value is one of the
types defined in the semantic language or a type
variable when the entity type is underspecified.
The lexical entry for ‘(an integer) divides (an in-
teger)’ and ‘(a set) includes (an element)’ would
thus have the following categories (other features
than type are not shown):

divides :: S\NP[type=Z]/NP[type=Z]
includes :: S\NP[type=SetOf(α)]/NP[type=α]

When defining a lexical item, we don’t have to
explicitly specify the type features in most cases.
They can be usually inferred from the definition of

4 In Fig. 3, the semantic part is in the dynamic logic
format as in our real grammar where the dynamic binding
(∃x;φ) → ψ is interpreted as ∀x(φ → ψ) in the standard
predicate logic. Following our analysis of an analogous con-
struction in Japanese, the null pronoun after ‘the quotient’ is
filled by analysing the second clause as including a gap rather
than filling it by zero-pronoun resolution.

2135



the semantic function. In the above example, di-
vides will have λy.λx.(x|y) and includes will have
λy.λx.(y ∈ x) as their semantic functions. For
both cases, the type feature of the NP arguments
can be determined from the type definitions of the
operators | and ∈ in the ontology.

The lexicon currently includes 54,902 lexical
items for 8,316 distinct surface forms, in which
5,094 lexical items for 1,287 surface forms are for
function words and functional multi-word expres-
sions. The number of unique categories in the lex-
icon is 10,635. When the type features are ig-
nored, there are still 4,026 distinct categories.

6 Math Expression Analysis

The meaning of a math expression is composed
with the semantic functions of surrounding words
to produce a logical form. We dynamically gen-
erate lexical items for each math expression in a
problem. Consider the following sentence includ-
ing two ‘equations’:

If a2−4=0, then x2+ax+1=0 has a real solution.

The latter, x2+ax+1 = 0, should receive a lexical
item of a noun phrase, NP : eqn(λx.x2 + a + 1),
but the former, a2−4 = 0, should receive category
S since it denotes a proposition. Such disambigua-
tion is not always possible without semantic anal-
ysis of the text. We thus generate more than one
lexical item for ambiguous expressions and let the
semantic parser make a choice.

To generate the lexical items, we first collect
appositions to the math expressions, such as ‘in-
teger n and m’ and ‘equation x2 + a = 0,’ and
use them as the type constraints on the variables
and the compound expressions. Compound ex-
pressions are then parsed with an operator prece-
dence parser (Aho et al., 2006). Overloaded op-
erators, such as + for numbers and vectors, are
resolved using the type constrains whenever pos-
sible. Finally, we generate all possible interpre-
tations of the expressions and select appropriate
syntactic categories.

We have seen only three categories of math ex-
pressions: NP, Sn, and T/(T\NP). The last one is
used for a NP with post-modification, as in:

>

n > 0

T/(T\NP )
: λP.(n > 0 ∧ P (n))

is an even number

S\NP
: λx.(even(x))

S : n > 0 ∧ even(n)

Naomi-NOM garden-LOC walk-PAST

Naomi ga niwa o arui ta

𝑁𝑎𝑜𝑚𝑖
𝑁𝑃

𝑔𝑎
𝑁𝑃 ∖ 𝑁𝑃

𝑁𝑃

𝑛𝑖𝑤𝑎
𝑁𝑃

𝑜
𝑁𝑃 ∖ 𝑁𝑃
𝑁𝑃

𝑎𝑟𝑢𝑖
𝑆 ∖ 𝑁𝑃 ∖ 𝑁𝑃

𝑆 ∖ 𝑁𝑃
𝑆

𝑡𝑎
𝑆 ∖ 𝑆

𝑆

(Naomi walked in the garden.)

Figure 4: Bunsetsu dependency structure (top) and
CCG derivation tree (bottom)

7 Two-step Semantic Parsing

Two central issues in parsing are the cost of the
search and the accuracy of disambiguation. Super-
vised learning is commonly used to solve both. It
is however very costly to create the training data
by manually annotating a large number of sen-
tences with CCG trees. Past studies have tried to
bypass it by so-called weak supervision, where a
parser is trained only with the logical form (e.g.,
Kwiatkowski et al. 2011) or even only with the
answers to the queries (e.g., Liang et al. 2011).

Although the adaptation of such methods to the
pre-university math data is an interesting future di-
rection, we developed yet another approach based
on a hybrid of shallow dependency parsing and the
detailed CCG grammar. The syntactic structure of
Japanese sentences has traditionally been analyzed
based on the relations among word chunks called
bunsetsus. A bunsetsu consists of one or more
content words followed by zero or more function
words. The dependencies among bunsetsus mostly
correspond to the predicate-argument and inter-
clausal dependencies (Fig. 4). The dependency
structure hence matches the overall structure of a
CCG tree only leaving the details unspecified.

We derive a full CCG-tree by using a bunsetsu
dependency tree as a constraint. We assume: (i)
the fringe of each sub-tree in the dependency tree
has a corresponding node in the CCG tree. We call
such a node in the CCG tree ‘a matching node.’
We further assume: (ii) a matching node is com-
bined with another CCG tree node whose span in-
cludes at least one word in the head bunsetsu of
the matching node. Fig. 5 presents an example of
a sentence consisting of four bunsetsus (rounded
squares), each of which contains two words. In
the figure, the i-th cell in the k-th row from the
bottom is the CKY cell for the span from i-th to

2136



w1 w2 w3 w4 w5 w6 w7 w8

Figure 5: Restricted CKY parsing based on a shal-
low dependency structure

(i+k-1)-th words. Under the two assumptions, we
only need to fill the hatched cells given the depen-
dency structure shown below the CKY chart. The
hatched cells with a white circle indicate the posi-
tions of the matching nodes.

Even under the constraint of a dependency tree,
it is impractical to do exhaustive search. We use
beam search based on a simple score function on
the chart items that combines several features such
as the number of atomic categories in the item. We
also use N -best dependency trees to circumvent
the dependency errors. The restricted CKY pars-
ing is repeated on the N -best dependency trees
until a CCG tree is obtained. Our hope is to re-
ject a dependency error as violation of the syntac-
tic and semantic constraints encoded in the CCG
lexicon. In the experiment, we used a Japanese
dependency parser developed by Kudo and Mat-
sumoto (2002). We modified it to produce N -best
outputs and used up to 20-best trees per sentence.

8 Global Type Coherency

The well-typedness of the logical form is usually
guaranteed by the combinatory rules. However,
they do not always guarantee the type coherency
among the interpretations of the math expressions.

For instance, consider the following derivation:

>

if x+ y ∈ U,
S/S : λP.(addR(x, y) ∈ U → P )

then x+ z ∈ V.
S : addV(x, y) ∈ V

S : addR(x, y) ∈ U → addV(x, z) ∈ V

The + symbol is interpreted as the addition of real
numbers (addR) in the first clause but that of vec-
tors (addV) in the second one. The logical form is
not typable because the two occurrences of xmust
have different types. The forward application rule
does not reject this derivation since the categories
of the two clauses perfectly match the rule schema.

We can reject such inconsistency by doing type
checking on the logical form at every step of the

Algorithm 1 Global type coherence check
procedure PARSEPROBLEM

Envs← ∅; AllDerivs← []
for each sentence s in the problem do

Chart← INITIALIZECKYCHART(s, Envs)
Derivs← TWOSTEPPARSING(s, Chart)
Envs← UPDATEENVIRONMENTS(Envs, Derivs)
AllDerivs← AllDerivs ⊕ [Derivs]

return AllDerivs

// s: a sentence; Envs: a set of environments
procedure INITIALIZECKYCHART(s, Envs)

Chart← empty CKY chart
for each token t in s do

for each lexical item C : f for t do
// C: category, f : semantic function
if t is a math expression then

for each environment Γ ∈ Envs do
if Γ is unifiable with FV(f ) then

add (C,Γ t FV(f)) to Chart
else // t is a normal word

add (C, ∅) to Chart
return Chart

FV(f ): the environment that maps the free variables in a
semantic function f to their principal types determined by
type inference on f .

// Envs: a set of environments; Derivs: derivations trees
procedure UPDATEENVIRONMENTS(Envs, Derivs)

NewEnvs← ∅ // environments for the next sentence
for each derivation d ∈ Derivs do

Γ← the environment at the root of d
if Γ 6= ∅ then // update the environments

NewEnvs← NewEnvs ∪{Γ}
else // no update: there was no math expression

NewEnvs← NewEnvs ∪ Envs
// eliminate those subsumed by other environments
return MOSTGENERALENVIRONMENTS(NewEnvs)

derivation. It is however quite time consuming be-
cause we cannot use dynamic programming any
more and need to do type checking on numerous
chart items. Furthermore, such type inconsistency
may happen across sentences. Instead, we con-
sider the type environment while parsing. A type
environment, written as {v1 : T1, v2 : T2, . . . },
is a finite function from variables to type expres-
sions. A pair v : T means that the variable v must
be of type T or its instance (e.g., SetOf(R) is an
instance of SetOf(α)). For example, the logical
form of the first clause of the above sentence is ty-
pable under {x :R, y :R, z :α,U :SetOf(R), V :β},
but that of the second clause isn’t. Please refer,
e.g., to (Pierce, 2002) for the formal definitions.
Two environments Γ1 and Γ2 are unifiable iff there
exists a substitution σ that maps the type variables
in Γ1 and Γ2 to some type expressions so that
Γ1σ = Γ2σ holds. We write Γ1 t Γ2 for the re-
sult of such substitution (i.e., unification) with the

2137



<

<

n

(NP [α], {n : α}) >

divides
(S\NP [Z]/NP [Z], ∅)

12

(NP [Z], ∅)
(S\NP [Z], ∅)

(S, {n : Z}) >

iff
(S\S/Sn, ∅)

n ∈ U
(Sn, {n : β, U : SetOf(β)})

(S\S, {n : β, U : SetOf(β)})
(S, {n : Z, U : SetOf(Z)})

Figure 6: CCG parsing with type environment

Dataset Correct Time- Wrong No Parseout RCF failure
DEV 27.6% 10.9% 12.1% 12.1% 37.4%
TEST 11.4% 1.8% 11.4% 6.8% 68.6%

(Correct: correct answer; Timeout: reasoning did not finish

in 10 min; Wrong: wrong answer; No RCF: no RCF formula

was obtained by rewriting the logical form; Parse failure: at

least one sentence in the problem did not receive a CCG tree)

Table 6: Result of end-to-end problem solving

Dataset Dep. Parsed Sentences (%)train N=1 N=5 N=10 N=20

DEV News 48.9 69.1 72.6 76.6News+Math 70.5 81.6 84.6 86.4

TEST News 46.6 58.7 61.9 64.7News+Math 59.3 65.3 66.9 68.3

Table 7: Fraction of sentences on which a CCG
tree was obtained in top N dependency trees

most general σ (most general unifier, mgu).

We associate a type environment with each
chart item and refine it through parsing. The type
constraints implied in a discourse are accumu-
lated in the environment and block the generation
of incoherent derivations (Algorithm 1). Fig. 6
presents an example of a parsing result, in which
the type constraints implied in the two clauses are
unified at the root and the type of U is determined.
When we apply a combinatory rule, we first check
if the environments of the child chart items are
unifiable. If so, we put the unified environment in
the parent item and apply the unifier to the type
features in the parent category. For instance, the
forward application rule is revised as follows:

(X/Y,Γ1) + (Y,Γ2)→ (Xσ,Γ1 t Γ2),
where σ is the mgu of Γ1 and Γ2 and Xσ means

the application of σ to the type features in X .5

5 To be precise, we also consider the type constraints in-
duced through the unification of the categories. It can be seen
in the derivation step for “n divides 12” in Fig. 6, where the
new constraint n :Z is induced by the unification of NP[α]
and NP[Z] and merged into the environment of the parent.

9 Experiments and Analysis

This section presents the overall performance of
the current end-to-end system and demonstrates
the effectiveness of the proposed parsing tech-
niques. We also present an analysis of the failures.

Table 6 presents the result of end-to-end prob-
lem solving on the UNIV data. It shows the failure
in the semantic parsing is a major bottleneck in the
current system. Since a problem in UNIV includes
more than three sentences on average, parsing a
whole problem is quite a high bar for a semantic
parser. It is however necessary to solve it by the
nature of the task. Once a problem-level logical
form was produced, the system yielded a correct
solution for 44% of such problems in DEV and
36% in TEST.

Table 7 lists the fraction of the sentences on
which the two-step parser produced a CCG tree
within top-N dependency trees. We compared
the results obtained with the dependency parser
trained only on a news corpus (News) (Kurohashi
and Nagao, 2003), which is annotated with bun-
setsu level dependencies, and that trained addi-
tionally with a math problem corpus consisting of
6,000 sentences6 (News+Math). The math prob-
lem corpus was developed according to the same
annotation guideline for the news corpus. The at-
tachment accuracy of the dependency parser was
84% on math problem text when trained only on
the news corpus but improved to 94% by the ad-
dition of the math problem corpus. The perfor-
mance gain by increasing N is more evident in
the results with the News parser than that with the
News+Math parser. It suggests the grammar prop-
erly rejected wrong dependency trees, which were
ranked higher by the News parser. The effect of
the additional training is very large at small Ns
and still significant at N = 20. It means that we
successfully boosted both the speed and the suc-
cess rate of CCG parsing only with the shallow
dependency annotation on in-domain data.

6 No overlap with DEV and TEST sections of UNIV.

2138



Dataset Parsing w/ Typing Correcttype env. failure (%) answer (%)

DEV no 9.8% 21.8%yes 0.6% 27.6%

TEST no 8.6% 8.6%yes 0.0% 11.4%

Table 8: Effect of parsing with type environment

Freq. Reason for the parse failures (on TEST-2007)
17 Unknown usage of known content words

9 Unknown content words
8 Errors in coreference resolution
4 Missing math expression interpretaions
3 Unknown usage of known function words
3 Unknown function words
2 No correct dependency tree in 20-best

Table 9: Reasons for the parse failures

Table 8 shows the effect of CCG parsing with
type environments. The column headed ‘Typing
failure’ is the fraction of the problems on which
no logical form was obtained due to typing fail-
ure. Parsing with type environment eliminated al-
most all such failures and significantly improved
the number of correct answers. The remaining
type failure was due to beam thresholding where
a necessary derivation fell out of the beam.

Table 9 lists the reasons for the parse failures on
1/4 of the TEST section (the problems taken from
exams on 2007). In the table, “unknown usage”
means a missing lexical item for a word already
in the lexicon. “Unknown word” means no lexi-
cal item was defined for the word. Collecting un-
known usages (especially that of a function word)
is much harder than just compiling a list of words.
Our experience in the lexicon development tells us
that once we find a usage example, in the large ma-
jority of the cases, it is not difficult to write down
its syntactic category and semantic function. Ta-
ble 9 suggests that we can efficiently detect and
collect unknown word usages through parsing fail-
ures on a large raw corpus of math problems.

Table 10 presents the accuracy of the sentence-
and problem-level logical forms produced on the
year 1999 subset of DEV and the year 2007 subset
of TEST. Although the recall on the unseen test
data is not as high as we hope, the high precision
of the sentence-level logical forms is encouraging.

Table 11 provides the counts of the error types
found in the wrong sentence-level logical forms
produced on DEV-1999 and TEST-2007. It re-
veals the majority of the errors are related to the
choice of quantifier (∃, ∀, or free) and logical op-

Dataset Precision Recall
sentence- DEV-1999 83% (64/77) 72% (64/ 89)

level TEST-2007 88% (64/73) 56% (64/114)
problem- DEV-1999 75% (18/24) 45% (18/40)

level TEST-2007 50% (8/16) 15% (8/53)

Table 10: Accuracy of logical forms

Error type DEV- TEST-
1999 2007

Bind a variable or leave it free 6 2
Wrong math expr. interpretaion 6 1
Quantifier choice 0 3
Quantifier scope 1 1
Logical connective choice 1 1
Logical connective scope 1 0
Others 1 2

Table 11: Types of errors in the logical forms

erators (e.g., → vs. ↔) as well as the determina-
tion of their scopes. Meanwhile, we did not find
an error related to the predicate-argument struc-
ture of a logical form. This fact and the results
in Table 6 suggest that the selectional restrictions,
encoded in the lexicon, properly rejected non-
sensical predicate-argument relations. Our next
step is to introduce a more sophisticated disam-
biguation model on top of the grammar, enjoying
the properly confined search space.

10 Conclusion

We have explained why the task of end-to-end
math problem solving matters for a practical the-
ory of natural language semantics and introduced
the semantic parsing of pre-university math prob-
lems as a novel benchmark. The statistics of the
benchmark data revealed that it includes far more
complex semantic structures than the other bench-
marks. We also presented an overview of an end-
to-end problem solving system and described two
parsing techniques motivated by the scarcity of the
annotated data and the need for the type coherency
of the analysis. Experimental results demonstrated
the effectiveness of the proposed techniques and
showed the accuracy of the sentence-level logical
form was 88% precision and 56% recall. Our fu-
ture work includes the expansion of the lexicon
with the aid of the semantic parser and the devel-
opment of a disambiguation model for the binding
and scoping structures.

2139



References
Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jef-

frey D. Ullman. 2006. Compilers: Principles, Tech-
niques, and Tools (2nd Edition). Addison-Wesley
Longman Publishing Co., Inc.

Daisuke Bekki. 2010. Nihongo-bunpou no keishiki-
riron (in Japanese). Kuroshio Shuppan.

Daniel Gureasko Bobrow. 1964. Natural language in-
put for a computer problem solving system. Ph.D.
thesis, Massachusetts Institute of Technology.

Eugene Charniak. 1969. Computer solution of cal-
culus word problems. In Proceedings of the 1st
International Joint Conference on Artificial Intel-
ligence. San Francisco, CA, USA, pages 303–316.
http://dl.acm.org/citation.cfm?id=1624562.1624593.

Donald Davidson. 1967. Truth and meaning. Synthese
17(1):304–323.

Jan Van Eijck and Martin Stokhof. 2006. The gamut of
dynamic logic. In Handbook of the History of Logic,
Volume 6 Logic and the Modalities in the Twentieth
Century, Elsevier, pages 499–600.

Gottlob Frege. 1892. Über Sinn und Bedeutung.
Zeitschrift für Philosophie und philosophische Kri-
tik 100:25–50.

Mohammad Javad Hosseini, Hannaneh Hajishirzi,
Oren Etzioni, and Nate Kushman. 2014. Learn-
ing to solve arithmetic word problems with
verb categorization. In Proceedings of the
2014 Conference on Empirical Methods in
Natural Language Processing. pages 523–533.
http://aclweb.org/anthology/D/D14/D14-1058.pdf.

Rik Koncel-Kedziorski, Hannaneh Hajishirzi,
Ashish Sabharwal, Oren Etzioni, and Siena
Ang. 2015. Parsing algebraic word problems
into equations. Transactions of the Associ-
ation for Computational Linguistics 3:585–597.
https://transacl.org/ojs/index.php/tacl/article/view/692.

Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In
CoNLL 2002: Proceedings of the 6th Confer-
ence on Natural Language Learning 2002 (COL-
ING 2002 Post-Conference Workshops). pages
63–69. http://aclweb.org/anthology/W/W02/W02-
2016.pdf.

Sadao Kurohashi and Makoto Nagao. 2003. Building
A Japanese Parsed Corpus, Springer Netherlands,
Dordrecht, pages 249–260.

Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automati-
cally solve algebra word problems. In Proceed-
ings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics. pages 271–281.
http://www.aclweb.org/anthology/P14-1026.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2010. Inducing
probabilistic ccg grammars from logical form
with higher-order unification. In Proceedings of
the 2010 Conference on Empirical Methods in
Natural Language Processing. pages 1223–1233.
http://dl.acm.org/citation.cfm?id=1870658.1870777.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon
Goldwater, and Mark Steedman. 2011. Lex-
ical generalization in ccg grammar induction
for semantic parsing. In Proceedings of the
Conference on Empirical Methods in Natu-
ral Language Processing. pages 1512–1523.
http://dl.acm.org/citation.cfm?id=2145432.2145593.

Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies. pages 590–
599. http://www.aclweb.org/anthology/P11-1060.

Arindam Mitra and Chitta Baral. 2016. Learning
to use formulas to solve simple arithmetic prob-
lems. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers). pages 2144–2153.
http://www.aclweb.org/anthology/P16-1202.

Richard Montague. 1970a. English as a formal lan-
guage. In Bruno Visentini, editor, Linguaggi nella
Societa e nella Tecnica, Edizioni di Communità,
pages 189–224.

Richard Montague. 1970b. Universal grammar. Theo-
ria 36(3):373–398. https://doi.org/10.1111/j.1755-
2567.1970.tb00434.x.

Richard Montague. 1973. The proper treatment of
quantification in ordinary english. In Patrick Sup-
pes, Julius Moravcsik, and Jaakko Hintikka, editors,
Approaches to Natural Language, Dordrecht, pages
221–242.

Benjamin C. Pierce. 2002. Types and Programming
Languages. MIT Press.

Subhro Roy and Dan Roth. 2015. Solving gen-
eral arithmetic word problems. In Proceedings
of the 2015 Conference on Empirical Methods in
Natural Language Processing. pages 1743–1752.
http://aclweb.org/anthology/D15-1202.

Bertrand Russell. 1905. On de-
noting. Mind 14(56):479–493.
http://www.jstor.org/stable/2248381.

Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi,
Oren Etzioni, and Clint Malcolm. 2015. Solv-
ing geometry problems: Combining text and di-
agram interpretation. In Proceedings of the
2015 Conference on Empirical Methods in Nat-
ural Language Processing. pages 1466–1476.
http://aclweb.org/anthology/D15-1171.

2140



Shuming Shi, Yuehui Wang, Chin-Yew Lin, Xi-
aojiang Liu, and Yong Rui. 2015. Automati-
cally solving number word problems by seman-
tic parsing and reasoning. In Proceedings of
the 2015 Conference on Empirical Methods in
Natural Language Processing. pages 1132–1142.
http://aclweb.org/anthology/D15-1135.

Mark Steedman. 2001. The Syntactic Process. Brad-
ford Books. MIT Press.

Mark Steedman. 2012. Taking Scope - The Nat-
ural Semantics of Quantifiers. MIT Press.
http://mitpress.mit.edu/books/taking-scope.

Lappoon R. Tang and Raymond J. Mooney.
2001. Using multiple clause constructors
in inductive logic programming for semantic
parsing. In Proceedings of the 12th Euro-
pean Conference on Machine Learning. pages
466–477. http://www.cs.utexas.edu/users/ai-
lab/?tang:ecml01.

Alfred Tarski. 1936. The concept of truth in formalized
languages. In A. Tarski, editor, Logic, Semantics,
Metamathematics, Oxford University Press, pages
152–278.

Alfred Tarski. 1944. The semantic conception of
truth: and the foundations of semantics. Philoso-
phy and Phenomenological Research 4(3):341–376.
http://www.jstor.org/stable/2102968.

Shyam Upadhyay, Ming-Wei Chang, Kai-Wei Chang,
and Wen-tau Yih. 2016. Learning from ex-
plicit and implicit supervision jointly for al-
gebra word problems. In Proceedings of
the 2016 Conference on Empirical Methods in
Natural Language Processing. pages 297–306.
https://aclweb.org/anthology/D16-1029.

Luke Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for pars-
ing to logical form. In Proceedings of the
2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computa-
tional Natural Language Learning. pages 678–687.
http://www.aclweb.org/anthology/D07-1071.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the 21st Conference
in Uncertainty in Artificial Intelligence. pages 658–
666.

Lipu Zhou, Shuaixiang Dai, and Liwei Chen.
2015. Learn to solve algebra word problems
using quadratic programming. In Proceedings
of the 2015 Conference on Empirical Methods
in Natural Language Processing. pages 817–822.
http://aclweb.org/anthology/D15-1096.

2141


	Semantic Parsing of Pre-university Math Problems

