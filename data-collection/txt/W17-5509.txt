



















































Reward-Balancing for Statistical Spoken Dialogue Systems using Multi-objective Reinforcement Learning


Proceedings of the SIGDIAL 2017 Conference, pages 65–70,
Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics

Reward-Balancing for Statistical Spoken Dialogue Systems using
Multi-objective Reinforcement Learning

Stefan Ultes, Paweł Budzianowski, Iñigo Casanueva, Nikola Mrkšić,
Lina Rojas-Barahona, Pei-Hao Su, Tsung-Hsien Wen, Milica Gašić and Steve Young

Engineering Department, University of Cambridge, Cambridge, United Kingdom
{su259,pfb30,ic340,nm480,lmr46,phs26,thw28,mg436,sjy11}@cam.ac.uk

Abstract

Reinforcement learning is widely used for
dialogue policy optimization where the re-
ward function often consists of more than
one component, e.g., the dialogue success
and the dialogue length. In this work, we
propose a structured method for finding a
good balance between these components
by searching for the optimal reward com-
ponent weighting. To render this search
feasible, we use multi-objective reinforce-
ment learning to significantly reduce the
number of training dialogues required. We
apply our proposed method to find opti-
mized component weights for six domains
and compare them to a default baseline.

1 Introduction

In a Spoken Dialogue System (SDS), one of the
main problems is to find appropriate system be-
haviour for any given situation. This problem is
often modelled using reinforcement learning (RL)
where the task is to find an optimal policy π(b) =
a which maps the current belief state b—an esti-
mate of the user goal— to the next system action
a. To do this, RL algorithms seek to optimize an
objective function, the reward r, using sample di-
alogues. In contrast to other RL tasks (like Al-
phaGo (Silver et al., 2016)), the reward used in
goal-oriented dialogue systems usually consists of
more than one objective (e.g., task success and di-
alogue length (Levin et al., 1998; Lemon et al.,
2006; Young et al., 2013)).

However, balancing these rewards is rarely con-
sidered and the goal of this paper is to propose a
structured method for finding the optimal weights
for a multiple objective reward function. Finding a
good balance between multiple objectives is usu-
ally domain-specific and not straight-forward. For

example, in the case of task success and dialogue
length, if the reward for success is too high, the
learning algorithm is insensitive to potentially ir-
ritating actions such as repeat provided that the
dialogue is ultimately successful. Conversely, if
the reward for success is too small, the resulting
policy may irritate users by offering inappropriate
solutions before fully illiciting the user’s require-
ments.

In this paper, we propose to find a suitable re-
ward balance by searching through the space of re-
ward component weights. Doing this with conven-
tional RL techniques is infeasible as a policy must
be trained for each candidate balance and this re-
quires an enormous number of training dialogues.
To alleviate this, we propose to use multi-objective
RL (MORL) which is specifically designed for this
task (among others (Roijers et al., 2013)). Then,
only one policy needs to be trained which may be
evaluated with several candidate balances. To the
best of our knowledge, this is the first time MORL
has been applied to dialogue policy optimization.

In contrast to previous work which explicitly
selects component weights to maximize user sat-
isfaction (Walker, 2000) explicitly, the proposed
method enables optimisation of an implicit goal
by allowing the interplay each reward component
to be explored at low computational cost.

Several different algorithms have previously
been used for MORL (Castelletti et al., 2013;
Van Moffaert et al., 2015; Pirotta et al., 2015;
Mossalam et al., 2016). In this work, we pro-
pose a novel MORL algorithm based on Gaussian
processes. This is described in Section 2 along
with a brief introduction to MORL. In Section 3,
the proposed method for finding a good reward
balance with MORL is presented. Section 4 de-
scribes the application and evaluation of the bal-
ancing method on six different domains. Finally
conclusions are drawn in Section 5.

65



2 Multi-objective Reinforcement
Learning with Gaussian Processes

In this Section we present our proposed exten-
sion of the GPSARSA algorithm for MORL af-
ter giving a brief introduction to single- and multi-
objective RL and the GPSARSA algorithm itself.

Reinforcement Learning Reinforcement learn-
ing (RL) is used in a sequential decision-making
process where a decision-model (the policy π) is
trained based on sample data and a potentially de-
layed objective signal (the reward r) (Sutton and
Barto, 1998). Implementing the Markov assump-
tion, the policy selects the next action a ∈ A based
on the current system belief state b to optimise the
accumulated future reward Rt at time t:

Rt =
∞∑
k=0

γkrt+k+1 . (1)

Here, k denotes the number of future steps, γ a
discount factor and rτ the reward at time τ .

The Q-function models the expected accumu-
lated future reward Rt when taking action a in be-
lief state b and then following policy π:

Qπ(b, a) = Eπ[Rt|bt = b, at = a] . (2)
GPSARSA For most real-world problems, find-
ing the exact optimal Q-values is not feasible. In-
stead, Engel et al. (2005) have proposed the GP-
SARSA algorithm which uses Gaussian processes
(GP) to approximate the Q-function. Gašić and
Young (2014) have shown that this works well
when applied to the problem of spoken dialogue
policy optimisation. GPSARSA is a Bayesian
on-line learning algorithm which models the Q-
function as a zero-mean GP which is fully defined
by a mean and a kernel function k:

Qπ(b, a) ∼ GP(0, k(b, a), (b, a))) , (3)
where the kernel models the correlation between
data points. Based on sample data, the GP is
trained to approximate Q such that the variance
derived from the kernel represents the uncertainty
of the approximation.

In dialogue management, the following kernel
has been successfully used:

k((b, a), (b′, a′)) = δ(a, a′) · klin(b, b′) . (4)
It consists of a linear kernel for the continuous be-
lief representation b and the δ-kernel for the dis-
crete system action a.

Multi-objective Reinforcement Learning In
multi-objective reinforcement learning (MORL),
the objective function does not consist of only one
but of many dimensions. Thus, the reward rt be-
comes a vector rt = (r1t , r

2
t , . . . , r

m
t ), where m is

the number of objectives.
To define the contribution of each objective, a

scalarization function f is introduced which uses
weights w for the different objectives to map the
vector representation to a scalar value. The solu-
tion to a MORL problem is a set of optimal poli-
cies containing an optimal policy for any given
weight configuration.

In MORL, the Q-function may either be mod-
elled as a vector of Q-functions or directly
as the expectation of the scalarized vector of
(R1t . . . R

m
t ):

Qπw(b) = E[f(Rt,w)|π, b, a] . (5)
In practice, the scalarization function is often
modelled as a linear function (the weighted sum):

f(rt,w) =
∑
m

wmr
m
t . (6)

Multi-objective GPSARSA The proposed
multi-objective (MO) GPSARSA is based on
Equation 5. By approximating the scalarized
Q-function directly using a GP, the GPSARSA
algorithm may be applied for MORL. The GP
(and thus the Q-function) is extended by one
parameter—the weight vector w: Q(b, a,w).

Approximating the Q-function with a GP relies
on the fact that the accumulated future reward Rt
(Eq. 1) may be decomposed as

Rt = rt+1 + γRt+1 . (7)

Accordingly, for using a GP to directly esti-
mate the scalarized reward in MO-GPSARSA, the
equation

f(Rt,w) = f(rt+1 + γRt+1,w)
!= f(rt+1,w) + γf(Rt+1,w) (8)

must hold. This is true in case of using a linear
scalarization function f (Eq. 6).

To alter the kernel accordingly, a linear kernel
for w is added to the state kernel1 resulting in

k((b, a,w), (b′, a′,w′))
= δ(a, a′) · (klin(b, b′) + klin(w,w′)) . (9)

1A similar type of kernel extension has been proposed
previously in a different context, e.g., (Casanueva et al.,
2015).

66



Algorithm 1: Training of the MO-GPSARSA.
Input: dialogue success reward rs, dialogue length

penalty rl
1 foreach training dialogue do
2 select ws, wl randomly
3 execute dialogue and record (bt, at,w) in D for

each turn t
// dialogue length penalty

4 r ← wl · |D| · rl
// dialogue success reward

5 if dialogue successful then
6 r ← r + wr · rs
7 update GP using D and r
8 reset D

Since a linear scalarization function is applied,
the correlations with other data points are also as-
sumed to be linear.

To train a policy using multi-objective GP-
SARSA, a new weight configuration is sampled
randomly for each training dialogue. An example
of the training process being applied to dialogue
policy optimization with the two objectives task
success and dialogue length is depicted in Algo-
rithm 1.

3 Reward Balancing using MORL

The main contribution of this paper is to provide a
structured method for finding a good balance be-
tween multiple rewards for learning dialogue poli-
cies. For the two-objective problem of having a
task success reward rs and a dialogue length re-
ward rl, r = (rs, rl), the scalarized reward is

r = f(r,w) = 1TS · wsrs + T · wsrl
= 1TS · rws + T · rwl , (10)

where T is the number of turns and 1TS = 1 iff
the dialogue is successful, zero otherwise.

To find a good reward balance, we adopt the fol-
lowing procedure:

1. Set initial reward values rws and r
w
l along

with the initial weight configuration.

2. Apply MORL to train a policy for a given
number of training dialogues and evaluate
with different weight configurations.

3. Select an appropriate balance based on
success-weight and length-weight curves to
optimise the individual implicit goal.

The method may be refined by applying it re-
cursively with different grid sizes. After selecting

a suitable weight configuration, a single-objective
policy may be trained.

4 Experiments and Results

The reward balancing method described in the pre-
vious section is applied to six domains: finding
TVs, laptops, restaurants or hotels (the latter two
in Cambridge and San Francisco). The following
table depicts the domain statistics with the number
of search constraints, the number of informational
items the user can request, and the number of data-
base entities:

Domain # constr. # requests # entities

CamRestaurants 3 9 110
CamHotels 5 11 33

SFRestaurants 6 11 271
SFHotels 6 10 182

TV 6 14 94
Laptops 11 21 126

For consistency with previous work (Gašić and
Young, 2014; Young et al., 2013; Su et al., 2016)
the rewards rws = 20 and r

w
l = −1 are used repre-

senting the weight configuration w = (0.5, 0.5).
This results in rs = 40 and rl = −2.

For the evaluation, simulated dialogues were
created using the statistical spoken dialogue
toolkit PyDial (Ultes et al., 2017). It contains
an agenda-based user simulator (Schatzmann and
Young, 2009) with an error model to simulate the
semantic error rate (SER) encountered in real sys-
tems due to the noisy speech channel.

A policy has been trained for each domain using
multi-objective GPSARSA with 3,000 dialogues
and an SER of 15%. Each policy was evaluated
with 300 dialogues for each weight configuration
in {(0.1, 0.9), (0.2, 0.8), . . . , (0.9, 0.1)}. The re-
sults in Figure 1 are the averages of five trained
policies with different random seeds. All curves
follow a similar pattern: at some point, the success
curve reaches a plateau where the performance
does not increase any further with higher ws.

The following weights were selected: Cam-
Restaurants ws = 0.4; CamHotels ws = 0.6;
SFRestaurants ws = 0.6; SFHotels ws = 0.7; TV
ws = 0.6; Laptops ws = 0.7. These weights were
selected by hand according to the success rate2 as
well as the average dialogue length.

The selected weights were scaled to keep the

2Taking into account the overall performance and the
proximity to the edge of the plateau. To compensate for pos-
sible inaccuracies of the MO-GPSARSA, the configuration
right at the edge has not been chosen.

67



0

2

4

6

8

10

0.0

0.2

0.4

0.6

0.8

1.0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

CamRestaurants

TSR	(m) TSR	(s)
T	(m) T	(s)

0

2

4

6

8

10

0.0

0.2

0.4

0.6

0.8

1.0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

CamHotels

TSR	(m) TSR	(s)
T	(m) T	(s)

0

2

4

6

8

10

0.0

0.2

0.4

0.6

0.8

1.0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

TV

TSR	(m) TSR	(s)
T	(m) T	(s)

0

2

4

6

8

10

0.0

0.2

0.4

0.6

0.8

1.0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

SFRestaurants

TSR	(m) TSR	(s)
T	(m) T	(s)

0

2

4

6

8

10

0.0

0.2

0.4

0.6

0.8

1.0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

SFHotels

TSR	(m) TSR	(s)
T	(m) T	(s)

0

2

4

6

8

10

0.0

0.2

0.4

0.6

0.8

1.0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

Laptops

TSR	(m) TSR	(s)
T	(m) T	(s)

Figure 1: The MORL success-weight and length-weight curves (m, task success rate (TSR) on left,
number of turns T on right vertical axes; success weights ws on horizontal axes) after 3,000 training
dialogues. Each data point is the average over five policies with different seeds where each policy/weight
configuration is evaluated with 300 dialogues. As a comparison, the same curves using single-objective
RL (s, separate policies trained for each balance) have been created after selecting the weights.

rws
TSR # Turns

base. opt. base. opt.

CamRestaurants 14 88.8% 86.2% 6.4 6.3
CamHotels 30 75.1% 79.8% 8.1 8.2
SFRestaurants 47 62.4% 65.7% 8.5 9.1
SFHotels 30 66.7% 69.4% 8.0 8.0
TV 30 75.7% 80.5% 7.4 7.4
Laptops 47 44.6% 54.6% 7.5 8.7

Table 1: Task success rates (TSRs) and number of
turns after 4,000 training dialogues using a success
reward of 20 (baseline) compared to the optimised
success reward rws . All TSR differences are statis-
tically significant (t-test, p < 0.05).

turn penalty wwl constant at −1. Using these re-
ward settings, each domain was evaluated with
4,000 dialogues in 10 batches. After each batch,
the policies were evaluated with 300 dialogues.
The final results shown in Table 1 (selection of
learning curves in Figure 2) are compared to the
baseline of w = (0.5, 0.5) (i.e. standard unopti-
mised reward component weight balance). Evi-
dently, optimising the balance has a significant im-
pact on the performance of the trained polices.

To analyse the performance of multi-objective
GPSARSA, policies were trained and evaluated
for each reward balance with single-objective (SO)
GPSARSA (see Figure 1) after the weights had
been selected. Each SO policy was trained with

1,000 dialogues and evaluated with 300 dialogues,
all averaged over five runs. The success-weight
curves for SORL clearly resemble the MORL
curves for almost all domains except for Cam-
Restaurants where it leads to an incorrect selection
of weights. This may be attributed to the kernel
used for multi-objective GPSARSA.

It is worth noting that for the presented full
MORL analysis, 3,000 training dialogues were
necessary for each domain to find a good balance.
This is significantly less than the 9,000 dialogues
needed for the SORL analysis and this difference
would increase further for a finer grain search grid.

5 Conclusion

In this work, we have addressed the problem of
finding a good balance between multiple rewards
for learning dialogue policies. We have shown
the relevance of the problem and demonstrated the
usefulness of multi-objective reinforcement learn-
ing to facilitate the search for a suitable balance.
Using the proposed procedure, only one policy
needs to be trained which can then be evaluated
for an arbitrary number of reward balances thus
drastically reducing the total amount of training
dialogues needed.

We have proposed and employed an extension
of the GPSARSA algorithm for multiple objec-
tives and applied it to six domains. The ex-

68



0

5

10

15

20

0.0

0.2

0.4

0.6

0.8

1.0

400 800 1200 1600 2000 2400 2800 3200 3600 4000

CamRestaurants

TSR	(base.) TSR	(opt.)
T	(base.) T	(opt.)

0

5

10

15

20

0.0

0.2

0.4

0.6

0.8

1.0

400 800 1200 1600 2000 2400 2800 3200 3600 4000

CamHotels

TSR	(base.) TSR	(opt.)
T	(base.) T	(opt.)

0

5

10

15

20

0.0

0.2

0.4

0.6

0.8

1.0

400 800 1200 1600 2000 2400 2800 3200 3600 4000

TV

TSR	(base.) TSR	(opt.)
T	(base.) T	(opt.)

0

5

10

15

20

0.0

0.2

0.4

0.6

0.8

1.0

400 800 1200 1600 2000 2400 2800 3200 3600 4000

SFRestaurants

TSR	(base.) TSR	(opt.)
T	(base.) T	(opt.)

0

5

10

15

20

0.0

0.2

0.4

0.6

0.8

1.0

400 800 1200 1600 2000 2400 2800 3200 3600 4000

SFHotels

TSR	(base.) TSR	(opt.)
T	(base.) T	(opt.)

0

5

10

15

20

0.0

0.1

0.2

0.3

0.4

0.5

0.6

400 800 1200 1600 2000 2400 2800 3200 3600 4000

Laptops

TSR	(base.) TSR	(opt.)
T	(base.) T	(opt.)

Figure 2: The task success rates (TSR, left axes) and dialogue length in number of turns (T, right axes)
for all six domains comparing the baseline (rws = 20, w = (0.5, 0.5)) with the optimised balance. The
horizontal axes show the number of training dialogues. Each data point is the average over five policies
with different seeds where each policy is evaluated with 300 dialogues.

periments show the successful application of our
method: the optimal balance improved task suc-
cess without unduly impacting on dialogue length
in all domains except CamRestaurants, where it is
clear that the weight selection criteria failed. In
practice, this could have been easily trapped by
applying a minimum weight to the success crite-
ria. Furthermore, the domain-dependence of the
reward balance has been confirmed.

For future work, the accuracy of the proposed
multi-objective GPSARSA will be further im-
proved with the ultimate goal of using the pro-
posed method to directly learn a multi-objective
policy through interaction with real users. To
achieve this, alternative weight kernels will be ex-
plored. The resulting multi-objective policy may
then directly be applied (without the need of re-
training a single-objective policy) and the weights
may even be adjusted according to a specific situ-
ation or user preferences.

Future work will also include an automatic
method to find the optimal balance as well as in-
vestigating the relationship between the optimal
success reward value and the domain characteris-
tics (similar to Papangelis et al. (2017)).

Acknowledgments

Tsung-Hsien Wen, Paweł Budzianowski and Ste-
fan Ultes are supported by Toshiba Research Eu-
rope Ltd, Cambridge Research Laboratory. This

research was partly funded by the EPSRC grant
EP/M018946/1 Open Domain Statistical Spoken
Dialogue Systems.

Data

All experiments were run in simulation. The cor-
responding source code is included in the PyDial
toolkit which can be found on www.pydial.org.

References
Inigo Casanueva, Thomas Hain, Heidi Christensen, Ri-

card Marxer, and Phil Green. 2015. Knowledge
transfer between speakers for personalised dialogue
management. In 16th Annual Meeting of the Special
Interest Group on Discourse and Dialogue. page 12.

A Castelletti, F Pianesi, and M Restelli. 2013. A mul-
tiobjective reinforcement learning approach to water
resources systems operation: Pareto frontier approx-
imation in a single run. Water Resources Research
49(6):3476–3486.

Yaakov Engel, Shie Mannor, and Ron Meir. 2005. Re-
inforcement learning with gaussian processes. In
Proceedings of the 22nd international conference on
Machine learning. ACM, pages 201–208.

Milica Gašić and Steve J. Young. 2014. Gaussian
processes for POMDP-based dialogue manager op-
timization. IEEE/ACM Transactions on Audio,
Speech, and Language Processing 22(1):28–40.

Oliver Lemon, Kallirroi Georgila, James Henderson,
and Matthew Stuttle. 2006. An isu dialogue system

69



exhibiting reinforcement learning of dialogue poli-
cies: generic slot-filling in the talk in-car system. In
Proceedings of the Eleventh Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Posters & Demonstrations. Association
for Computational Linguistics, pages 119–122.

Esther Levin, Roberto Pieraccini, and Wieland Eckert.
1998. Using markov decision process for learning
dialogue strategies. In Acoustics, Speech and Signal
Processing, 1998. Proceedings of the 1998 IEEE In-
ternational Conference on. IEEE, volume 1, pages
201–204.

Hossam Mossalam, Yannis M. Assael, Diederik M
Roijers, and Shimon Whiteson. 2016. Multi-
objective deep reinforcement learning. CoRR
abs/1610.02707. http://arxiv.org/abs/1610.02707.

Alexandros Papangelis, Stefan Ultes, and Yannis
Stylianou. 2017. Domain complexity and policy
learning in task-oriented dialogue systems. In Pro-
ceedings of the 8th International Workshop On Spo-
ken Dialogue Systems (IWSDS).

Matteo Pirotta, Simone Parisi, and Marcello Restelli.
2015. Multi-objective reinforcement learning with
continuous pareto frontier approximation. In Pro-
ceedings of the Twenty-Ninth AAAI Conference on
Artificial Intelligence. pages 2928–2934.

Diederik M Roijers, Peter Vamplew, Shimon White-
son, and Richard Dazeley. 2013. A survey of multi-
objective sequential decision-making. Journal of
Artificial Intelligence Research (JAIR) 48:67–113.

Jost Schatzmann and Steve J. Young. 2009. The hid-
den agenda user simulation model. Audio, Speech,
and Language Processing, IEEE Transactions on
17(4):733–747.

David Silver, Aja Huang, Chris J Maddison, Arthur
Guez, Laurent Sifre, George Van Den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, et al. 2016. Mastering
the game of go with deep neural networks and tree
search. Nature 529(7587):484–489.

Pei-Hao Su, M. Gašić, N. Mrkšić, L. Rojas-Barahona,
Stefan Ultes, D. Vandyke, T. H. Wen, and S. Young.
2016. On-line active reward learning for policy op-
timisation in spoken dialogue systems. In Proceed-
ings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics, pages 2431–2441.

Richard S. Sutton and Andrew G. Barto. 1998.
Reinforcement Learning: An Introduction.
MIT Press, Cambridge, MA, USA, 1st edition.
http://portal.acm.org/citation.cfm?id=551283.

Stefan Ultes, Lina M. Rojas-Barahona, Pei-Hao Su,
David Vandyke, Dongho Kim, Iñigo Casanueva,
Paweł Budzianowski, Nikola Mrkšić, Tsung-Hsien
Wen, Milica Gašić, and Steve J. Young. 2017. Py-
dial: A multi-domain statistical dialogue system

toolkit. In ACL Demo. Association of Computa-
tional Linguistics.

Kristof Van Moffaert, Tim Brys, and Ann Nowé. 2015.
Risk-sensitivity through multi-objective reinforce-
ment learning. In Evolutionary Computation (CEC),
2015 IEEE Congress on. IEEE, pages 1746–1753.

Marilyn Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in a spo-
ken dialogue system for email. Journal of Artificial
Intelligence Research 12:387–416.

Steve J. Young, Milica Gašić, Blaise Thomson, and Ja-
son D. Williams. 2013. POMDP-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE 101(5):1160–1179.

70


