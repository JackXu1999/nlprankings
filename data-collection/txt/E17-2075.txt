



















































A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 468–473,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

A Copy-Augmented Sequence-to-Sequence Architecture Gives
Good Performance on Task-Oriented Dialogue

Mihail Eric and Christopher D. Manning
Computer Science Department

Stanford University
meric@cs.stanford.edu, manning@stanford.edu

Abstract

Task-oriented dialogue focuses on con-
versational agents that participate in dia-
logues with user goals on domain-specific
topics. In contrast to chatbots, which sim-
ply seek to sustain open-ended meaning-
ful discourse, existing task-oriented agents
usually explicitly model user intent and
belief states. This paper examines bypass-
ing such an explicit representation by de-
pending on a latent neural embedding of
state and learning selective attention to di-
alogue history together with copying to in-
corporate relevant prior context. We com-
plement recent work by showing the effec-
tiveness of simple sequence-to-sequence
neural architectures with a copy mecha-
nism. Our model outperforms more com-
plex memory-augmented models by 7% in
per-response generation and is on par with
the current state-of-the-art on DSTC2, a
real-world task-oriented dialogue dataset.

1 Introduction

Effective task-oriented dialogue systems are be-
coming important as society progresses toward us-
ing voice for interacting with devices and perform-
ing everyday tasks such as scheduling. To that end,
research efforts have focused on using machine
learning methods to train agents using dialogue
corpora. One line of work has tackled the prob-
lem using partially observable Markov decision
processes and reinforcement learning with care-
fully designed action spaces (Young et al., 2013).
However, the large, hand-designed action and state
spaces make this class of models brittle and un-
scalable, and in practice most deployed dialogue
systems remain hand-written, rule-based systems.

Recently, neural network models have achieved

success on a variety of natural language process-
ing tasks (Bahdanau et al., 2015; Sutskever et al.,
2014; Vinyals et al., 2015b), due to their ability
to implicitly learn powerful distributed represen-
tations from data in an end-to-end trainable fash-
ion. This paper extends recent work examining the
utility of distributed state representations for task-
oriented dialogue agents, without providing rules
or manually tuning features.

One prominent line of recent neural dia-
logue work has continued to build systems with
modularly-connected representation, belief state,
and generation components (Wen et al., 2016b).
These models must learn to explicitly represent
user intent through intermediate supervision, and
hence suffer from not being truly end-to-end train-
able. Other work stores dialogue context in a
memory module and repeatedly queries and rea-
sons about this context to select an adequate sys-
tem response (Bordes and Weston, 2016). While
reasoning over memory is appealing, these mod-
els simply choose among a set of utterances rather
than generating text and also must have temporal
dialogue features explicitly encoded.

However, the present literature lacks results for
now standard sequence-to-sequence architectures,
and we aim to fill this gap by building increasingly
complex models of text generation, starting with
a vanilla sequence-to-sequence recurrent architec-
ture. The result is a simple, intuitive, and highly
competitive model, which outperforms the more
complex model of Bordes and Weston (2016) by
6.9%. Our contributions are as follows: 1) We per-
form a systematic, empirical analysis of increas-
ingly complex sequence-to-sequence models for
task-oriented dialogue, and 2) we develop a recur-
rent neural dialogue architecture augmented with
an attention-based copy mechanism that is able to
significantly outperform more complex models on
a variety of metrics on realistic data.

468



2 Architecture

We use neural encoder-decoder architectures to
frame dialogue as a sequence-to-sequence learn-
ing problem. Given a dialogue between a user (u)
and a system (s), we represent the dialogue utter-
ances as {(u1, s1), (u2, s2), . . . , (uk, sk)}where k
denotes the number of turns in the dialogue. At
the ith turn of the dialogue, we encode the aggre-
gated dialogue context composed of the tokens of
(u1, s1, . . . , si−1, ui). Letting x1, . . . , xm denote
these tokens, we first embed these tokens using a
trained embedding function φemb that maps each
token to a fixed-dimensional vector. These map-
pings are fed into the encoder to produce context-
sensitive hidden representations h1, . . . , hm.

The vanilla Seq2Seq decoder predicts the to-
kens of the ith system response si by first comput-
ing decoder hidden states via the recurrent unit.
We denote h̃1, . . . , h̃n as the hidden states of the
decoder and y1, . . . , yn as the output tokens. We
extend this decoder with an attention-based model
(Bahdanau et al., 2015; Luong et al., 2015a),
where, at every time step t of the decoding, an at-
tention score ati is computed for each hidden state
hi of the encoder, using the attention mechanism
of (Vinyals et al., 2015b). Formally this attention
can be described by the following equations:

uti = v
T tanh(W1hi +W2h̃t) (1)

ati = Softmax(u
t
i) (2)

h̃′t =
m∑

i=1

atihi (3)

ot = U [h̃t, h̃′t] (4)
yt = Softmax(ot) (5)

where W1, W2, U , and v are trainable parameters
of the model and ot represents the logits over the
tokens of the output vocabulary V . During train-
ing, the next token yt is predicted so as to max-
imize the log-likelihood of the correct output se-
quence given the input sequence.

An effective task-oriented dialogue system must
have powerful language modelling capabilities
and be able to pick up on relevant entities of an un-
derlying knowledge base. One source of relevant
entities is that they will commonly have been men-
tioned in the prior discourse context. Recent litera-
ture has shown that incorporating a copying mech-
anism into neural architectures improves perfor-
mance on various sequence-to-sequence tasks in-
cluding code generation, machine translation, and

text summarization (Gu et al., 2016; Ling et al.,
2016; Gulcehre et al., 2016). We therefore aug-
ment the attention encoder-decoder model with
an attention-based copy mechanism in the style
of (Jia and Liang, 2016). In this scheme, dur-
ing decoding we compute our new logits vector
as ot = U [h̃t, h̃′t, at[1:m]] where a

t
[1:m] is the con-

catenated attention scores of the encoder hidden
states, and we are now predicting over a vocab-
ulary of size |V | + m. The model, thus, either
predicts a token yt from V or copies a token xi
from the encoder input context, via the attention
score ati. Rather than copy over any token men-
tioned in the encoder dialogue context, our model
is trained to only copy over entities of the knowl-
edge base mentioned in the dialogue context, as
this provides a conceptually intuitive goal for the
model’s predictive learning: as training progresses
it will learn to either predict a token from the stan-
dard vocabulary of the language model thereby en-
suring well-formed natural language utterances, or
to copy over the relevant entities from the input
context, thereby learning to extract important dia-
logue context.

In our best performing model, we augment the
inputs to the encoder by adding entity type fea-
tures. Classes present in the knowledge base of
the dataset, namely the 8 distinct entity types re-
ferred to in Table 1, are encoded as one-hot vec-
tors. Whenever a token of a certain entity type
is seen during encoding, we append the appro-
priate one-hot vector to the token’s word embed-
ding before it is fed into the recurrent cell. These
type features improve generalization to novel enti-
ties by allowing the model to hone in on positions
with particularly relevant bits of dialogue context
during its soft attention and copying. Other cited
work using the DSTC2 dataset (Sukhbaatar et al.,
2015; Liu and Perez, 2016; Seo et al., 2016) im-
plement similar mechanisms whereby they expand
the feature representations of candidate system re-
sponses based on whether there is lexical entity
class matching with provided dialogue context. In
these works, such features are referred to as match
features.

All of our architectures use an LSTM cell as
the recurrent unit (Hochreiter and Schmidhuber,
1997) with a bias of 1 added to the forget gate in
the style of (Zaremba et al., 2015).

469



3 Experiments

3.1 Data
For our experiments, we used dialogues extracted
from the Dialogue State Tracking Challenge 2
(DSTC2) (Henderson et al., 2014), a restaurant
reservation system dataset. While the goal of the
original challenge was building a system for infer-
ring dialogue state, for our study, we use the ver-
sion of the data from Bordes and Weston (2016),
which ignores the dialogue state annotations, us-
ing only the raw text of the dialogues. The raw text
includes user and system utterances as well as the
API calls the system would make to the underlying
KB in response to the user’s queries. Our model
then aims to predict both these system utterances
and API calls, each of which is regarded as a turn
of the dialogue. We use the train/validation/test
splits from this modified version of the dataset.
The dataset is appealing for a number of reasons:
1) It is derived from a real-world system so it
presents the kind of linguistic diversity and con-
versational abilities we would hope for in an ef-
fective dialogue agent. 2) It is grounded via an un-
derlying knowledge base of restaurant entities and
their attributes. 3) Previous results have been re-
ported on it so we can directly compare our model
performance. We include statistics of the dataset
in Table 1.

3.2 Training
We trained using a cross-entropy loss and the
Adam optimizer (Kingma and Ba, 2015), apply-
ing dropout (Hinton et al., 2012) as a regularizer
to the input and output of the LSTM. We identified
hyperparameters by random search, evaluating on
a held-out validation subset of the data. Dropout
keep rates ranged from 0.75 to 0.95. We used word
embeddings with size 300, and hidden layer and
cell sizes were set to 353, identified through our
search. We applied gradient clipping with a clip-
value of 10 to avoid gradient explosions during
training. The attention, output parameters, word
embeddings, and LSTM weights were randomly
initialized from a uniform unit-scaled distribution
in the style of (Sussillo and Abbott, 2015).

3.3 Metrics
Evaluation of dialogue systems is known to be dif-
ficult (Liu et al., 2016). We employ several met-
rics for assessing specific aspects of our model,
drawn from previous work:

Avg. # of Utterances Per Dialogue 14
Vocabulary Size 1,229
Training Dialogues 1,618
Validation Dialogues 500
Test Dialogues 1,117
# of Distinct Entities 452
# of Entity (or Slot) Types 8

Table 1: Statistics of DSTC2

• Per-Response Accuracy: Bordes and We-
ston (2016) report a per-turn response accu-
racy, which tests their model’s ability to se-
lect the system response at a certain timestep.
Their system does a multiclass classification
over a predefined candidate set of responses,
which was created by aggregating all system
responses seen in the training, validation, and
test sets. Our model actually generates each
individual token of the response, and we con-
sider a prediction to be correct only if every
token of the model output matches the corre-
sponding token in the gold response. Evalu-
ating using this metric on our model is there-
fore significantly more stringent a test than
for the model of Bordes and Weston (2016).

• Per-Dialogue Accuracy: Bordes and Weston
(2016) also report a per-dialogue accuracy,
which assesses their model’s ability to pro-
duce every system response of the dialogue
correctly. We calculate a similar value of dia-
logue accuracy, though again our model gen-
erates every token of every response.

• BLEU: We use the BLEU metric, commonly
employed in evaluating machine translation
systems (Papineni et al., 2002), which has
also been used in past literature for evaluating
dialogue systems (Ritter et al., 2011; Li et
al., 2016). We calculate average BLEU score
over all responses generated by the system,
and primarily report these scores to gauge our
model’s ability to accurately generate the lan-
guage patterns seen in DSTC2.

• Entity F1: Each system response in the test
data defines a gold set of entities. To compute
an entity F1, we micro-average over the entire
set of system dialogue responses. This met-
ric evaluates the model’s ability to generate
relevant entities from the underlying knowl-
edge base and to capture the semantics of the
user-initiated dialogue flow.

470



Our experiments show that sometimes our
model generates a response to a given input that
is perfectly reasonable, but is penalized because
our evaluation metrics involve direct comparison
to the gold system output. For example, given a
user request for an australian restaurant, the gold
system output is you are looking for an australian
restaurant right? whereas our system outputs
what part of town do you have in mind?, which is a
more directed follow-up intended to narrow down
the search space of candidate restaurants the sys-
tem should propose. This issue, which recurs with
evaluation of dialogue or other generative systems,
could be alleviated through more forgiving evalu-
ation procedures based on beam search decoding.

3.4 Results

In Table 2, we present the results of our mod-
els compared to the reported performance of the
best performing model of (Bordes and Weston,
2016), which is a variant of an end-to-end mem-
ory network (Sukhbaatar et al., 2015). Their
model is referred to as MemNN. We also include
the model of (Liu and Perez, 2016), referred to
as GMemNN, and the model of (Seo et al., 2016),
referred to as QRN, which currently is the state-
of-the-art. In the table, Seq2Seq refers to our
vanilla encoder-decoder architecture with (1), (2),
and (3) LSTM layers respectively. +Attn refers to
a 1-layer Seq2Seq with attention-based decoding.
+Copy refers to +Attn with our copy-mechanism
added. +EntType refers to +Copy with entity class
features added to encoder inputs.

We see that a 1-layer vanilla encoder-decoder is
already able to significantly outperform MemNN
in both per-response and per-dialogue accuracies,
despite our more stringent setting. Adding layers
to Seq2Seq leads to a drop in performance, sug-
gesting an overly powerful model for the small
dataset size. Adding an attention-based decod-
ing to the vanilla model increases BLEU although
per-response and per-dialogue accuracies suffer
a bit. Adding our attention-based entity copy
mechanism achieves substantial increases in per-
response accuracies and entity F1. Adding en-
tity class features to +Copy achieves our best-
performing model, in terms of per-response accu-
racy and entity F1. This model achieves a 6.9%
increase in per-response accuracy on DSTC2 over
MemNN, including +1.5% per-dialogue accuracy,
and is on par with the performance of GMemNN,

Data Model Per- Per BLEU Ent.
Resp. Dial. F1

Test MemNN 41.1 0.0 – –
set GMemNN 48.7 1.4 – –

QRN 50.7 – – –
Seq2Seq (1) 46.4 1.5 55.0 69.7
Seq2Seq (2) 43.5 1.3 54.2 67.3
Seq2Seq (3) 44.2 1.7 55.4 65.9

+ Attn. 46.0 1.4 56.6 67.1
+ Copy 47.3 1.3 55.4 71.6
+ EntType 48.0 1.5 56.0 72.9

Dev Seq2Seq (1) 57.0 3.6 72.1 68.7
set Seq2Seq (2) 54.1 3.0 71.3 66.3

Seq2Seq (3) 54.0 3.2 71.5 64.3
+ Attn. 55.2 3.4 71.9 66.1
+ Copy 58.9 3.6 73.1 72.5
+ EntType 59.2 3.4 72.7 72.3

Table 2: Evaluation on DSTC2 test (top) and dev
(bottom) data. Bold values indicate our best per-
formance. A dash indicates unavailable values.

including beating its per-dialogue accuracy. It also
achieves the highest entity F1.

4 Discussion and Conclusion

We have iteratively built out a class of neural mod-
els for task-oriented dialogue that is able to outper-
form other more intricately designed neural archi-
tectures on a number of metrics. The model incor-
porates in a simple way abilities that we believe
are essential to building good task-oriented dia-
logue agents, namely maintaining dialogue state
and being able to extract and use relevant enti-
ties in its responses, without requiring intermedi-
ate supervision of dialogue state or belief tracker
modules. Other dialogue models tested on DSTC2
that are more performant in per-response accu-
racy are equipped with sufficiently more complex
mechanisms than our model. Taking inspiration
from (Sukhbaatar et al., 2015) and (Srivastava
et al., 2015), GMemNN uses an explicit mem-
ory module as well as an adaptive gating mech-
anism to learn to attend to relevant memories. The
QRN model employs a variant of a recurrent unit
that is intended to handle local and global interac-
tions in sequential data. We contrast with these
works by bootstrapping off of more empirically
accepted Seq2Seq architectures through intuitive
extensions, while still producing highly competi-
tive models.

We attribute the large gains in per-response ac-
curacy and entity F1 demonstrated by our +Ent-
Type to its ability to pick out the relevant KB en-
tities from the dialogue context fed into the en-
coder. In Figure 1, we see the attention-based copy

471



cheap restaurant in east part of town
api call r cuisine east cheap
<silence>
the missing sock is a nice place in the
east of town and the prices are cheap

address
sure, the missing sock is on the missing sock address
phone number
the phone number of the missing sock is
the missing sock phone
thank you good bye
you are welcome

Table 3: Sample dialogue generated. System re-
sponses are in italics. The dataset uses fake ad-
dresses and phone numbers.

Figure 1: Attention-copy weights for a generated
natural language response (top) and API call (bot-
tom). The decoder output is displayed vertically
and the encoder input is abbreviated for display.

weights of the model, indicating that the model is
able to learn the relevant entities it should focus
on in the input context. The powerful language
modelling abilities of the Seq2Seq backbone al-
low smooth integration of these extracted entities
into both system-generated API calls and natural
language responses as shown in the figure.

The appeal of our model comes from the sim-
plicity and effectiveness of framing system re-
sponse generation as a sequence-to-sequence map-
ping with a soft copy mechanism over relevant
context. Unlike the task-oriented dialogue agents
of Wen et. al (2016b), our architecture does not ex-
plicitly model belief states or KB slot-value track-
ers, and we preserve full end-to-end-trainability.
Further, in contrast to other referenced work on
DSTC2, our model offers more linguistic versatil-
ity due to its generative nature while still remain-
ing highly competitive and outperforming other
models. Of course, this is not to deny the im-

portance of dialogue agents which can more ef-
fectively use a knowledge base to answer user re-
quests, and this remains a good avenue for further
work. Nevertheless, we hope this simple and ef-
fective architecture can be a strong baseline for fu-
ture research efforts on task-oriented dialogue.

Acknowledgments

The authors wish to thank the reviewers, Lakshmi
Krishnan, Francois Charette, and He He for their
valuable feedback and insights. We gratefully ac-
knowledge the funding of the Ford Research and
Innovation Center, under Grant No. 124344. The
views expressed here are those of the authors and
do not necessarily represent or reflect the views of
the Ford Research and Innovation Center.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. ICLR.

A. Bordes and J. Weston. 2016. Learning
end-to-end goal-oriented dialog. arXiv preprint
arXiv:1605.07683.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1631–1640, Berlin, Germany, August. Asso-
ciation for Computational Linguistics.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,
Bowen Zhou, and Yoshua Bengio. 2016. Point-
ing the unknown words. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
140–149, Berlin, Germany, August. Association for
Computational Linguistics.

M. Henderson, B. Thomson, and J. Williams. 2014.
The second dialog state tracking challenge. 15th An-
nual Meeting of the Special Interest Group on Dis-
course and Dialogue, page 263.

G. E. Hinton, N. Srivastava, A. Krizhevsky,
I. Sutskever, and R. R. Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.

S. Hochreiter and J. Schmidhuber. 1997. Long short-
term memory. Neural Computation, pages 1735–
1780.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Proceedings of the

472



54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
12–22, Berlin, Germany, August. Association for
Computational Linguistics.

D. Kingma and J. Ba. 2015. Adam: a method for
stochastic optimization. In Proc. ICLR.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A diversity-promoting objec-
tive function for neural conversation models. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
110–119, San Diego, California, June. Association
for Computational Linguistics.

Wang Ling, Phil Blunsom, Edward Grefenstette,
Karl Moritz Hermann, Tomáš Kočiský, Fumin
Wang, and Andrew Senior. 2016. Latent predictor
networks for code generation. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
599–609, Berlin, Germany, August. Association for
Computational Linguistics.

F. Liu and J. Perez. 2016. Gated end-to-end memory
networks. arXiv preprint arXiv:1610.04211.

Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How not to evaluate your dialogue system: An em-
pirical study of unsupervised evaluation metrics for
dialogue response generation. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 2122–2132, Austin,
Texas, November. Association for Computational
Linguistics.

M. Luong, H. Pham, and C.D. Manning. 2015a. Effec-
tive approaches to attention-based neural machine
translation. Empirical Methods in Natural Lan-
guage Processing, pages 1412–1421.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.

A. Ritter, C. Cherry, and W. B. Dolan. 2011. Data-
driven response generation in social media. Empiri-
cal Methods in Natural Language Processing, pages
583–593.

M. Seo, S. Min, A. Farhadi, and H. Hajishirzi. 2016.
Query-reduction networks for question answering.
arXiv preprint arXiv:1606.04582.

R. Srivastava, K. Greff, and J. Schmidhuber. 2015.
Highway networks. In Proc. ICLR.

S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus.
2015. End-to-end memory networks. arXiv preprint
arXiv:1503.08895.

D. Sussillo and L.F. Abbott. 2015. Random walk ini-
tialization for training very deep feed forward net-
works. arXiv preprint arXiv:1412.6558.

I. Sutskever, O. Vinyals, and Q.V. Le. 2014. Sequence
to sequence learning with neural networks. In Ad-
vances in Neural Information Processing Systems,
pages 3104–3112.

O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever,
and G. Hinton. 2015b. Grammar as a foreign lan-
guage. In Advances in Neural Information Process-
ing Systems, pages 2755–2763.

T.H. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona,
P.H. Su, S. Ultes, D. Vandyke, and S. Young. 2016b.
A network-based end-to-end trainable task-oriented
dialogue system. arXiv preprint arXiv:1604.04562.

S. Young, M. Gasic, B. Thomson, and J.D. Williams.
2013. POMDP-based statistical spoken dialog
systems: a review. Proceedings of the IEEE,
28(1):114–133.

W. Zaremba, I. Sutskever, and O. Vinyals. 2015.
Recurrent neural network regularization. arXiv
preprint arXiv:1409.2329.

473


