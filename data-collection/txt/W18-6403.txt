



















































Findings of the WMT 2018 Biomedical Translation Shared Task: Evaluation on Medline test sets


Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 324–339
Belgium, Brussels, October 31 - Novermber 1, 2018. c©2018 Association for Computational Linguistics

https://doi.org/10.18653/v1/W18-64030

Findings of the WMT 2018 Biomedical Translation Shared Task:
Evaluation on Medline test sets

Mariana Neves
German Federal Institute for

Risk Assessment (BfR),
Germany

Antonio Jimeno Yepes
IBM Research,

Australia

Aurélie Névéol
LIMSI, CNRS,

Uni. Paris Saclay, France

Cristian Grozea
Fraunhofer Institute
FOKUS, Germany

Amy Siu
Beuth University of

Applied Sciences, Germany

Madeleine Kittner
Humboldt-Universität
zu Berlin, Germany

Karin Verspoor
University of Melbourne,

Australia

Abstract

Machine translation enables the automatic
translation of textual documents between lan-
guages and can facilitate access to information
only available in a given language for non-
speakers of this language, e.g. research results
presented in scientific publications. In this pa-
per, we provide an overview of the Biomed-
ical Translation shared task in the Workshop
on Machine Translation (WMT) 2018, which
specifically examined the performance of ma-
chine translation systems for biomedical texts.
This year, we provided test sets of scientific
publications from two sources (EDP and Med-
line) and for six language pairs (English with
each of Chinese, French, German, Portuguese,
Romanian and Spanish). We describe the de-
velopment of the various test sets, the sub-
missions that we received and the evaluations
that we carried out. We obtained a total of 39
runs from six teams and some of this year’s
BLEU scores were somewhat higher that last
year’s, especially for teams that made use of
biomedical resources or state-of-the-art MT
algorithms (e.g. Transformer). Finally, our
manual evaluation scored automatic transla-
tions higher than the reference translations for
German and Spanish.

1 Introduction

Automatic translation of documents from one lan-
guage to another facilitates broader information
access for resources only available in a partic-
ular language. Even in the scientific literature,
in which most important articles are published
only in English, an increasing number of re-
searchers support citing articles published in other

languages for the sake of not missing important
research or to avoid carrying out duplicate exper-
iments (Lazarev and Nazarovets, 2018). Recent
discussions on this topic in the journal Nature have
appealed for translation of the best Chinese papers
(Tao et al., 2018) and the development of auto-
matic tools for the automatic translation of pub-
lications (Prieto, 2018).

Therefore, biomedicine is a domain for which
suitable parallel corpora, official evaluation test
sets and machine translation (MT) systems are in
high demand. There is active development of par-
allel corpora in this domain (see the recent survey
in (Névéol et al., 2018)). In this year alone, three
new corpora have been published in a single con-
ference: a compilation of full texts from the Sci-
elo database for English, Portuguese, and Spanish
(Soares et al., 2018), medical documents and glos-
saries for Spanish/English (Villegas et al., 2018)
and a biomedical corpus for Romanian (Mitrofan
and Tufis, 2018). However, in spite of the grow-
ing number of parallel corpora and the many open
source tools for MT (e.g., Moses (Koehn et al.,
2007), OpenNMT (Klein et al., 2017) and Mar-
ian (Junczys-Dowmunt et al., 2018)), there is still
no ready-to-use tool for automatic translation of
biomedical publications for any language pair.

With the aim of fostering advances in this field,
we organized the third edition of the Biomedical
Translation Task in the Conference for Machine
Translation (WMT).1 It builds on the two previous
editions (Bojar et al., 2016; Jimeno Yepes et al.,
2017) by offering test sets from Medline for six

1http://www.statmt.org/wmt18/
biomedical-translation-task.html

324

https://doi.org/10.18653/v1/W18-64030


language pairs and from EDP for one language
pair, as detailed below:

• Chinese-English (zh/en); Eng.-Chinese (en/zh)

• French-English (fr/en); Eng.-French (en/fr)

• German-English (de/en); Eng.-German (en/de)

• Portuguese-English (pt/en); Eng.-Port. (en/pt)

• English-Romanian (en/ro)

• Spanish-English (es/en); Eng.-Span. (en/es)

Most test sets were derived from scientific ab-
stracts from Medline which were available in
both languages. Except for Romanian, we ad-
dressed translation in both directions for all lan-
guage pairs. This was not possible for Romanian
due to the low number (less than 50) of parallel
abstracts which are available in Medline. For the
first time, we have an Asian language, specifically
Chinese.

In this paper, we describe details of the chal-
lenge. Section 2 presents the construction and
quality analysis of the test sets, followed by the
details on the six participating teams in Section 3.
Section 4 presents the results for both automatic
and manual evaluation that we carried out, as well
as some additional evaluations which are new this
year. Finally, we provide a comprehensive discus-
sion of the results and quality of the translations in
Section 5.

2 Test sets

Test sets were obtained from Medline and EDP.
In these sources, text for both languages is readily
available from the authors of the publications.

EDP. This year’s test set was derived from last
year’s processing of publications. We kept one ex-
tra test set for this year’s challenge. It can be noted
that the sentence segmentation offered for the EDP
corpus this year was performed manually. More
details can be found in the description of the chal-
lenge in 2017 (Jimeno Yepes et al., 2017).

MEDLINE. We constructed the various Med-
line test sets following a similar strategy carried
out for the Scielo corpus (Neves et al., 2016). We
started by downloading MEDLINE 2018 and re-
trieving those entries whose abstract was available
for more than one language, usually English was

one of the languages. Such abstracts are iden-
tified by the XML tag OtherAbstract and its at-
tribute Language. We only considered the abstract
of the publications since the titles were frequently
only available in one language. We randomly se-
lected a subset of the abstracts for the six language
pairs under consideration and for which we have
native speakers of the foreign languages.

The text of the abstracts were extracted from the
XML files and 120 abstracts were randomly se-
lected, excepted for Romanian whose total of par-
allel documents in Medline was less than 50. The
number 120 accounts for possible errors in the pre-
processing of the abstract in order to have a final
test set of 100 abstracts to be split into the two
translation directions. The documents were auto-
matically split using the Stanford CoreNLP tool
and the respective available models for each lan-
guages, i.e., Chinese, French, German and Span-
ish (Manning et al., 2014).2 Since for Portuguese
and Romanian no models are available in the Stan-
ford CoreNLP tools, we used models for other
similar Roman languages (Spanish for Portuguese
and French for Romanian). The sentences were
then automatically aligned using the GMA tool for
which we provided a list of stopwords for each
language.3 After a short analysis of the alignment
of the Chinese/English abstracts, and given the bad
alignments that we obtained, we carried out a new
automatic alignment using the Champollion tool
(Ma, 2006).4 The resulting aligned sentences were
then manually checked for assessing their quality.

2.1 Manual evaluation of the automatic
alignment

After compiling the Medline test sets, we manu-
ally checked the totality of the abstracts to assess
the quality of the automatic alignment (cf. results
shown in Table 2). We utilized a modified version
of the Quality Checking task of our installation of
the Appraise tool (Federmann, 2010, 2018) and
one native speaker of each non-English language
carried out the validation (cf. Figure 1). The only
exception were the Chinese abstracts which were
manually checked without the use of the Appraise
tool. For each language pair, we checked the total-
ity of the abstracts for both translation directions,
e.g., en/de and de/en, which was later randomly

2https://stanfordnlp.github.io/
CoreNLP/

3https://nlp.cs.nyu.edu/GMA/
4http://champollion.sourceforge.net/

325



Test sets de/en fr/en pt/en es/en en/de en/fr en/pt en/es en/ro en/zh zh/en

EDP # documents 86 83# sentences 879/880 823/821

Medline # documents 48 49 50 50 48 49 50 50 40 50 49# sentences 342/337 318/328 283/286 286/300 352/378 279/281 332/318 299/263 301/293 311/307 279/311

Table 1: Overview of the test sets. We present the number of documents and sentences in each test set. The number
of sentences might be different for the two languages in a test set.

split into two test sets. The only exception was
the Romanian test set. Due to its small size, we
only built one test set for one translation direction
(en/ro).

The number of completely unaligned sentences
was rather uniform across the various language
pairs and usually less than 5%, with the excep-
tion of Spanish (more than 8%) and German (more
that 15%). All other partial alignments (Overlap,
Source>Target and Target>Source) had a contri-
bution of less than 10%. For three languages, at
least 80% of the sentences were correctly aligned,
while for Spanish and German only 70% and 65%
of the sentences were correctly aligned. The lower
quality of these two test sets could certainly affect
the calculation of the BLEU score and we will ad-
dress this problem later in Section 4.2.3.

During the manual validation, we detected
problems in the parallel abstracts. For instance,
four abstracts (PMIDs 24616752, 25767637,
26941877 and 24294348) in the German test set
had to be excluded because they were wrongly
tagged in Medline as being in German, while they
were written in Italian. The same occurred in
the French test sets, were two abstracts (PMIDs
23396711 and 24883131) were also in Italian.
This suggests that the use of automatic language
identification tools could be useful to validate the
language metadata retrieved from MEDLINE.

3 Participating teams and systems

We received submissions from six teams, as sum-
marized in Table 3. The teams came from research
and academic institutions of four countries (Brazil,
Germany, Spain and USA) and from three conti-
nents. An overview of the teams and their systems
is provided below.

FOKUS (Fraunhofer Institute FOKUS, Ger-
many). The FOKUS team participated with
a system based on neural machine translation
(NMT) based on the implementation of the Trans-
former architecture (Kaiser et al., 2017; Vaswani
et al., 2017) for MT (Grozea, 2018). The NMT

system made use of biomedical and news corpora
for either training or validation (tuning). In addi-
tion to this, and in order to automatically select the
highest fidelity translation, they developed heuris-
tics based on a dictionary and on stemming. Fur-
ther, they performed diacritics normalization in or-
der to account for recent ortographic changes in
the Romanian language.

Hunter MT (Hunter College, USA). The
Hunter team (Khan et al., 2018) used different
transfer learning methods and trained different in-
domain biomedical data sets one after another.
Their system was set up using parameters of pre-
vious training as the initialization of the following
training. A News based model was used as pre-
training.

LMU (Ludwig Maximilian University of Mu-
nich, Germany). The LMU team implemen-
tated various neural network models and trained
and tuned the models on parallel biomedical
data (Huck et al., 2018). They experimented
with implementations of the Transformer architec-
ture (Sockeye implementation) and the encoder-
decoder models (Nematus toolkit). The authors
highlight that the word segmentation used on the
German language for both translation directions
were responsible for the good performance of the
system in the human evaluation.

TFG TALP UPC (Technical University of
Catalunya, Spain). For their system that pro-
vides translations into English, the TGF TALP
UPC team participated with a Transformer archi-
tecture (Kaiser et al., 2017; Vaswani et al., 2017)
using both single-language and multi-source sys-
tems (Tubay and Costa-Jussà, 2018). The systems
were trained on the Scielo and Medline titles made
available by the shared task in the last years. The
multi-source systems utilized a concatenation of
training data from es/en, fr/en and pt/en.

UFRGS (Universidade Federal do Rio Grande
do Sul, Brazil). The UFRGS team participated
with two runs based either on Moses (Koehn et al.,

326



Figure 1: Screen-shot of Appraise during manual validation of the dataset for Portuguese.

Test sets No alignment OK Overlap Source > Target Target > Source Total
en/de, de/en 104 (15.38%) 437 (64.64%) 23 (3.40%) 60 (8.88%) 52 (7.69%) 676
en/es, es/en 46 (8.30%) 388 (70.04%) 38 (6.86%) 44 (7.94%) 38 (6.86%) 554
en/fr, fr/en 20 (3.36%) 528 (88.59%) 6 (1.01%) 20 (3.36%) 22 (3.69%) 596
en/pt, pt/en 11 (1.87%) 490 (83.33%) 9 (1.53%) 41 (6.97%) 37 (6.29%) 588

en/ro 7 (2.39%) 260 (88.74%) 3 (1.02%) 12 (4.10%) 11 (3.75%) 293
en/zh, zh/en 19 (3.26%) 528 (90.72%) 4 (0.69%) 18 (3.09%) 13 (2.23%) 582

Table 2: Manual validation of the automatic alignment sentences for the Medline test sets. Values are shown in
absolute and percentage numbers. The test include the abstracts for both languages directions, with the exception
of the Romanian language. The total column represents the totality of the aligned sentences

Team ID Institution
FOKUS Fraunhofer Institute FOKUS (Germany)

Hunter MT Hunter College (USA)
LMU Ludwig Maximilian University of Munich (Germany)

TFG TALP UPC Technical University of Catalunya (Spain)
UFRGS Universidade Federal do Rio Grande do Sul (Brazil)

UHH-DS University of Hamburg (Germany)

Table 3: List of the participating teams.

327



2007) or OpenNMT (Klein et al., 2017) systems
(Soares and Becker, 2018). Training data was pre-
pared by concatenating several in-domain and out-
of-domain resources. The in-domain corpora in-
cluded scientific articles (full texts) from Scielo,
the UFAL medical corpus, the EMEA corpus and
Brazilian theses and dissertations. Due to possi-
ble overlap with the test sets from Medline, the
team applied some procedures to automatically
exclude some publications from the Scielo train-
ing data. Terminological resources such as the
Unified Medical Language System (UMLS) (Bo-
denreider, 2004) were used as well.

UHH-DS (University of Hamburg, Germany).
The UHH-DS team utilized Moses (Koehn et al.,
2007) trained on a variety of in-domain and gen-
eral domain corpora (Duma and Menzel, 2018).
The main feature of their system was the develop-
ment of an unsupervised method to automatically
under-sample sentences from the general domain
collection that were better suited for the biomed-
ical domain. Their under-sampling algorithm can
be applied either on the source or target side of the
corpora, as well as on both sides.

4 Evaluation

In this section we describe the various submissions
that we obtained and present the results that these
achieved based on both automatic and manual val-
uation.

4.1 Submissions

In total, we received 39 submissions from the
six teams, as summarized in Table 4. Unfortu-
nately, we received no submissions for Chinese
(neither zh/en nor en/zh) and no submissions for
the French EDP test set (fr/en).

FOKUS. The FOKUS team submitted two runs
in which one (run1) was trained on a biomedical
corpus and validated on news corpora while the
second one (run2, primary run) is an ensemble of
various NMT systems and uses the heuristics they
defined for selecting the best translation.

Hunter. The Hunter’s team submitted two runs
for en/fr for each of the Medline and EDP test
sets. In these runs, they considered NMT based
ensembles and trained on various in-domain and
out-of-the-domain corpora. However, differences
between the runs are unclear.

LMU. The three en/de submissions from the
LMU team were the following: a right-to-left re-
ranked Transformer (run1, primary run), a Trans-
former ensemble without re-ranking (run2) and
the encoder-decoder built with Nematus (run3).
The only submission for de/en was a Transformer
without ensemble.

TFG TALP UPC. Each two submissions for
language pairs es/en, fr/en and pt/en utilized ei-
ther multi-source (run1, primary run) or the single-
source (run2) training.

UFRGS. The two submissions from the UFRGS
teams seem to have differed only on the MT tool
that they used, i.e., either OpenNMT (run1, pri-
mary run) or Moses (run2).

UHH-DS. The three submissions for each of the
language pairs (en/es, en/pt, en/ro, es/en and pt/en)
differed on whether the under-sampling algorithm
was applied only on the English side (run1), on
the non-English side (run2) or on both sides (run3,
primary run).

4.2 Automatic evaluation

Here we provide the results for the automatic eval-
uation and rank the systems regarding the resulting
scores. We computed BLEU scores at the sentence
level using the script mteval-v14.pl from the
Moses distribution.5 For all test sets and lan-
guage pairs, we compare the submissions (auto-
matic translations) to the respective reference one.

4.2.1 Automatic evaluation: EDP test sets
The BLEU scores for the EDP test set are pre-
sented in Table 5. Given that we received only two
submissions from a single team, we could not per-
form comparison between teams. We ranked the
two submissions as follows:

• en/fr: Hunter (run 1) < Hunter (run 2).

Run2 obtained a slightly higher score than run1,
however, reasons for this improvement are un-
known.

4.2.2 Automatic evaluation: Medline test sets
This year, we calculated BLEU scores based on
the totality of the sentences (including the ones
with incorrect alignments) as well as based only

5http://www.statmt.org/moses/?n=Moses.
SupportTools

328



Teams de/en en/de en/es en/fr en/pt en/ro es/en fr/en pt/en Total
FOKUS M2 2
Hunter E2M2 4
LMU M M3 4

TFG TALP UPC M2 M2 M2 6
UFRGS M2 M2 M2 M2 8

UHH-DS M3 M3 M3 M3 M3 15
Total 1 3 5 4 5 5 7 2 7 39

Table 4: Overview of submissions for each language pair and test set: [M]edline and [E]DP. The number next to
the letter indicates the number of runs that the team submitted for the corresponding test set (if larger than one).

Team Runs en/fr

Hunter run1 22.20run2 23.24*

Table 5: BLEU scores for the EDP en/fr dataset. * in-
dicates the primary run as informed by the participants.

on the sentences which were perfectly aligned (cf.
Section 2).

BLEU scores for the Medline test set are pre-
sented in Table 6. For some language pairs, i.e.,
de/en, en/de, en/fr and fr/en, we could not compare
results between various teams since we received
submissions only from one team. Moreover, we
only received one submission from one team for
de/en. Therefore, no further comparison was pos-
sible for this language pair. We ranked the var-
ious teams and submissions, for those languages
for which we received more than one submission,
as follows:

• en/de: LMU (run3) < LMU (runs 1,2);

• en/es: UHH-DS (runs 1,2,3) < UFRGS (runs
1,2);

• en/fr: Hunter (runs 1,2);

• en/pt: UHH-DS (runs 1,2,3) < UFRGS (runs
1,2);

• en/ro: UHH-DS (runs 1,2,3) < FOKUS (run
1) < FOKUS (run 2);

• es/en: UHH-DS (run 2) < UHH-DS (runs
1,3) < TGF TALP UPC (runs 1,2) < UFRGS
(runs 1,2);

• fr/en: TGF TALP UPC (run 2) < TGF TALP
UPC (run 1);

• pt/en: TGF TALP UPC (run 2) < TGF TALP
UPC (run 1) < UHH-DS (runs 1,2,3) <
UFRGS (runs 1,2).

In the following we provide a short summary of
the results with regard to the method or resources
that have been used.

de/en. The run based on the Transformer archi-
tecture from the LMU team obtained a reasonable
BLEU score. However, we could not compare this
to any other submission.

en/de. There was little difference in the BLEU
score between the two first submissions, both
based on the Transformer architecture, but both
did seem to be superior to the third run based on
the encoder-decoder model.

en/es. The best results for en/es were obtained
by the UFRGS team when using the Moses sys-
tem (run2) instead of neural MT (run1), as ex-
pected by the team. However, the difference be-
tween both submissions is not significant. We ob-
served no significant difference between the three
submissions from the UHH-DS team. However,
all of them yield much lower BLEU scores than
the submissions by the UFRGS team.

en/fr. The submissions from the Hunter team
obtained very similar scores for the Medline test
sets. Details on each run is unclear but these differ-
ences seem to have brought significant improve-
ment on the scores only on the EDP test set (cf.
Table 5).

en/pt. Both submissions from the UFRGS team
obtained the highest BLEU scores, which again
and similar to the results obtained for en/es, did
not confirm the superiority of neural MT. The
three runs from the UHH-DS team were closer
to the ones from the UFRGS team (in compar-
ison to the ones for en/es), but still rather infe-
rior. This time, run1 (under-sampling based on the
English side) did perform a little better than the
other two runs, specially regarding run2 (under-
sampling based on the non-English side).

329



Teams Runs de/en en/de en/es en/fr en/pt en/ro es/en fr/en pt/en

FOKUS run1 16.97run2 18.10*

Hunter MT run1 23.41run2 23.24*

LMU
run1 23.93* 18.81*
run2 18.75
run3 17.16

TFG TALP UPC run1 40.49* 25.78* 39.49*run2 39.06 19.42 38.54

UFRGS run1 39.62* 39.43* 43.31* 42.58*run2 39.77 39.43 43.41 42.58

UHH-DS
run1 31.32 34.92 14.60 36.16 41.84
run2 31.05 34.19 14.39 35.17 41.80
run3 31.33* 34.49* 14.07* 36.05* 41.79*

Table 6: Results for the Medline dataset. * indicates the primary run as informed by the participants.

en/ro. The run2 of the FOKUS team, which con-
sisted on an ensemble of various NMT systems
and used heuristics for selecting the best trans-
lation, obtained the highest BLEU score. The
two submissions from UHH-DS reached BLEU
scores which were slightly below results from the
FOKUS team. We observed no significant differ-
ence between the three runs. However, similar to
en/pt, under-sampling based on the English side
(run1) seems to perform slightly better than under-
sampling based on both sides (run3).

es/en. Once again the statistical MT system
(Moses) from the UFRGS team obtained a slightly
higher score than their neural MT system (Open-
NMT). Indeed, the BLEU scores obtained by
run2 of the team was the highest one among all
submissions to the shared task for all language
pairs. The following two best scores belonged
to the Transformer-based MT systems from the
TGF TALP UPC team. Even though a more re-
cent and currently state-of-the-art method (Trans-
former) was used by team TGF TALP UPC, the
better results obtained by UFRGS were probably
due to the larger training collection that they used.
The model trained on various sources obtained
a slightly better score. The three runs from the
UHH-DS team were rather inferior than the ones
from the two other teams. A significant difference
was only observed for run2 (under-sampling based
on the non-English side) which achieved lower
BLEU scores for this language pair.

fr/en. The only submissions for fr/en belonged
to the Transformer-based MT systems from the
TGF TALP UPC team. This time, the improve-
ment of the multi-source model over the single
model was very significant. However, the highest

score was rather low in comparison to the other
submissions of the team.

pt/en. There was no difference in the two sub-
missions from the UFRGS team, both obtained
the highest BLEU scores. The three runs from
the UHH-DS team obtained the second best results
but we observed no significant difference between
the three of them. Finally, the two lowest scores
were obtained by the Transformer-based MT sys-
tems from the TGF TALP UPC team. Similar to
results for es/en and fr/en, the system trained on
various sources obtained a little improvement over
the single models. Also similar to the es/en results,
the higher performance from UFRGS was proba-
bly due to the use of more resources for training
the MT systems.

4.2.3 Evaluation for sentences with good
alignment

This year, we also calculated additional BLEU
scores when considering only the sentences whose
alignments we manually classified as being cor-
rect. Correctly aligned means that sentences in
both languages contained exactly the same in-
formation and neither of the sentences contained
more information than the other (cf. Section 2).

Results for this subset of the test set are pre-
sented in Table 7. For most teams, improvements
were significant, ranging from two to four BLEU
points, but was up to one point for en2ro. The
overall order of the results mostly remained the
same.

4.2.4 Evaluation for Romanian after
diacritics normalization

In the particular case of the Romanian language,
there were fairly recent changes in the ortho-

330



Teams Runs de/en en/de en/es en/fr en/pt en/ro es/en fr/en pt/en

FOKUS run1 17.84run2 19.11*

Hunter run1 24.66run2 24.76*

LMU
run1 28.84* 24.30*
run2 23.88
run3 21.84

TFG TALP UPC run1 42.91* 27.10* 42.55*run2 41.26 20.20 41.56

UFRGS run1 44.50* 43.14* 46.92* 46.01*run2 44.50 43.14 46.92 46.01

UHH-DS run1 34.77 37.24 15.85 38.45 44.28run2 34.70 36.76 15.62 37.17 44.32
run3 35.08* 36.91* 15.28* 38.18* 44.27*

Table 7: Results for the Medline dataset using OK aligned sentences. * indicates the primary run as informed by
the participants.

graphic recommendation with respect to diacritics
notation6: comma-below s, and S, should be used
instead of cedilla-below ş and Ş; in the same way,
the comma-below t, and T, should be used instead
of the cedilla-below ţ and Ţ, according to a 2003
communicate from the “Iorgu Iordan” Institute of
Linguistics of the Romanian Academy.

While the two comma-below and cedilla-below
variants of those letters are hardly distinguishable
to a human reader, they have different unicode
codes and thus replacing one with another in a
word makes it a completely different word, for
an automated method. Having the “wrong” word
affects all n-grams containing that word for the
BLEU scoring.

In order to achieve more quality in the transla-
tion assessment, we normalized all diacritics both
in gold standard and in the submissions for Ro-
manian. Results for the Medline en/ro test set are
shown in Table 8, based on all sentences (en/ro)
and only based on correctly aligned sentences
(en/ro-OK). To this end, we wrote and used a sim-
ple sed-based script which brings the Romanian
diacritics to the latest standard7.

4.3 Manual evaluation

We performed manual evaluation of the primary
runs (as identified by the participants) for each
team and each language pair. The primary runs
are compared to the reference translation and to
each other, if more than one submission (from dis-
tinct teams) is available for the language pair. We

6For reference, the evolving standards for Romanian
are discussed here http://kitblog.com/2008/10/
romanian_diacritic_marks.html.

7The script is freely available at http://www.
brainsignals.de/fixrodia.sh

Teams Runs en/ro en/ro-OK

FOKUS run1 22.17 22.98run2 23.42* 24.22*

UHH-DS run1 15.40 15.95run2 15.09 15.69
run3 14.77 15.44

Table 8: Results for the Medline en2ro test set after
normalization of diacritics. * indicates the primary run
as informed by the participants.

computed pairwise combinations of translations
either between two automated systems, or one au-
tomated system and the reference translation. The
human validators were native speakers of the lan-
guages and were either members of the participat-
ing teams or colleagues from the research commu-
nity. These are primary runs from each team:

• FOKUS: Medline en/ro run2;
• Hunter: Medline en/fr run2, EDP en/fr run2;
• LMU: Medline de/en run1, Medline en/de

run1;

• TGF TALP UPC: Medline es/en run1, Med-
line fr/en run1, Medline pt/en run1;

• UFRGS: Medline en/es run1, Medline en/pt
run1, Medline es/en run1, Medline pt/en
run1;

• UHH-DS: Medline en/es run3, Medline en/pt
run3, Medline en/ro run3, Medline es/en
run3, Medline pt/en run3.

The validation task was carried out using the
3-way ranking task in our installation of the Ap-
praise tool (Federmann, 2010).8 For each pairwise

8https://github.com/cfedermann/
Appraise

331



comparison, we checked a total of 100 randomly-
chosen sentence pairs. The validation consisted of
reading the two sentences (A and B), i.e., transla-
tions from two systems or from the reference, and
choosing one of the options below:

• A<B: when the quality of translation B was
higher than A.

• A=B: when both translations had similar
quality.

• A>B: when the quality of translation A was
higher than B.

• Flag error: when the translation did not seem
to be derived from the same input sentence.
This is usually related to errors in corpus
alignment.

We present the results for the manual evaluation
of the Medline test sets in Table 9. Based on the
number of times that a translation was validated as
being better than another, we ranked the systems
for each language as listed below:

• de/en: LMU = reference

• en/de: reference < LMU

• en/es: reference, UHH-DS < UFRGS

• en/fr: HunterNMT < reference

• en/pt: UHH-DS < UFRGS < reference;

• en/ro: UHH-DS < FOKUS < reference

• es/en: UHH-DS < UFRGS < TGF TALP
UPC < reference

• fr/en: TGF TALP UPC < reference

• pt/en: UHH-DS < UFRGS < TGF TALP
UPC < reference

Even though the LMU runs obtained one of
the lowest BLEU scores (all of them less than 20
points), the primary run did score equally well or
even better than the reference translation in the
manual evaluation. The reason are misaligned sen-
tences in the German reference. Automatic Ger-
man translations on the other hand are most often
correct in translation and alignment of content. In-
deed, the quality of the German dataset was one of
the lowest (cf. Section 2.1).

We present the results for the manual evaluation
of the EDP test sets in Table 10. Based on the
number of times that the submission was validated
as being better than the reference translation, we
ranked the two translations as follow:

• en/fr: Hunter < reference.

5 Discussion

In this section we present insight from the auto-
matic and manual validations as well as on the
quality of the translations.

5.1 Differences between manual and
automatic evaluations

Similar to previous years, we did not notice any
difference while ranking the teams for most lan-
guage pairs regarding the automatic and manual
evaluation of the translations. This year, the only
significant difference we noticed was for the En-
glish translations, more specifically for es/en and
pt/en pairs.

For es/en, the ranking order changed between
the teams UFRGS and TGF TALP UPC. While
the runs from the UFRGS teams achieved a higher
BLEU score (43.31 vs. 40.49), our evaluators
found the translations from the TGF TALP UPC
team to be considerable better (79 vs. 7).

As for pt/en, the ranking of the teams changed
from TGF TAP UPC < UHH-DS < UFRGS (au-
tomatic evaluation: 42.55 < 44.27 < 46.01) to
UHH-DS < UFRGS < TGF TAP UPC (manual
evaluation: 55 vs. 21 to UFRGS, 58 vs. 24 to
UHH-DS). While no difference in ranking was ob-
served between teams UHH-DS and UFRGS, in
comparison to the automatic evaluation, team TGF
TAP UPC moved from being the last ranked in the
automatic evaluation to the best ranked one on the
manual evaluation.

We can only hypothesize that the better BLEU
scores that the UFRGS team obtained were prob-
ably due to better translation of particular con-
cepts or due to using the same terms as in the
reference translations. However, the TGF TAP
UPC team could obtain higher quality of the man-
ual translations using their Transformer architec-
ture. The better performance of the TGF TAP UPC
team could also have been due to the test set be-
ing included in the their training corpus, i.e. over-
laps between Medline and the Scielo databases.
While both teams trained on the Scielo corpus,

332



Languages Runs (A vs. B) Total A>B A=B A<B
de/en LMU vs. reference 75 29 14 32
en/de LMU vs. reference 76 29 32 15

en/es
UFRGS vs. reference 86 37 23 26
UFRGS vs. UHH-DS 88 29 37 22
reference vs. UHH-DS 92 30 33 29

en/fr Hunter vs. reference 92 14 13 65

en/pt
UFRGS vs. reference 86 6 43 42
UFRGS vs. UHH-DS 100 32 53 15
reference vs. UHH-DS 81 46 28 7

en/ro
FOKUS vs. reference 88/81 11/14 19/14 58/53
FOKUS vs. UHH-DS 100/97 57/55 31/27 12/15
reference vs. UHH-DS 88/85 80/78 6/6 2/1

es/en

TGF TALP UPC vs. reference 72 26 12 34
TGF TALP UPC vs. UFRGS 100 51 38 11

TGF TALP UPC vs. UHH-DS 98 79 12 7
reference vs. UFRGS 77 50 15 12

reference vs. UHH-DS 77 54 10 13
UFRGS vs. UHH-DS 100 45 24 31

fr/en TGF TALP UPC vs. reference 85 24 19 42

pt/en

TGF TALP UPC vs. reference 89 25 26 38
TGF TALP UPC vs. UFRGS 100 55 24 21

TGF TALP UPC vs. UHH-DS 100 58 24 18
reference vs. UFRGS 87 42 22 23

reference vs. UHH-DS 87 52 28 7
UFRGS vs. UHH-DS 100 48 27 25

Table 9: Results for the manual validation for the Medline test sets. Values are absolute numbers (not percentages).
They might not sum up to 100 due to the skipped sentences. Two evaluators (both participants) carried out the
validation of the Romanian dataset and results from both of them are shown (separated by a slash).

Languages Runs (A vs. B) Total A>B A=B A<B
en/fr Hunter vs. reference 91 11 26 54

Table 10: Results for the manual validation for the EDP test sets. Values are absolute numbers (not percentages).
They might not sum up to 100 due to the skipped sentences.

333



the UFRGS team reported that they tried to re-
move potential overlaps between Medline and Sci-
elo (Soares and Becker, 2018). Overlaps of Med-
line and Scielo do not explain the lower BLEU
scores obtained by the TGF TAP UPC team.

5.2 Differences across languages

Similar to previous years, comparison of results
across languages did not provide any unexpected
insight. The languages pairs which obtained
higher BLEU scores (above 30 points), i.e. en/es,
en/pt, es/en and pt/en, were also the ones for which
more training data specific for the biomedical do-
main is available. Indeed, teams that participated
with the same system for different language pairs
obtained lower scores for those languages with
fewer resources. This is the case of the scores of
the TGF TAP UPC team for fr/en (up to 27 points)
as opposed to the ones obtained for es/en and pt/en
(more than 40 points).

We hope that recently released corpora, e.g. the
BioRo corpus for Romanian (Mitrofan and Tu-
fis, 2018), can boost performance of MT systems
for these languages. However, more parallel cor-
pora are certainly necessary not only for those lan-
guages that scored worst in this challenge, but also
for the many other languages that we did not eval-
uate here. Unfortunately, open-access databases
such as Scielo are not available for most lan-
guages. Nevertheless, the number of parallel ab-
stracts in Medline are increasing and corpora de-
rived from these are starting to being published,
e.g. MeSpEn (Villegas et al., 2018).

5.3 Evolution of the performance in the last
years

Compared to previous years, we found an im-
proved BLEU score and improved manual evalua-
tion for languages already considered in previous
years, i.e. Portuguese, Spanish and French. This
year we have considered Medline abstracts instead
of Scielo ones.

However, the results are difficult to be directly
compared to previous years given that test sets
were from different sources for many of the lan-
guage pairs. For the EDP test set, which can
be considered very similar to last year’s one, the
Hunter team scored much better than their last par-
ticipation both in terms of BLEU scores (17.50 vs.
23.24 for en/fr) and in the manual validation (0 to
93 vs. 11 to 54 in the manual validation).

The Medline test sets for es/en, pt/en, fr/en,
en/es, en/pt and en/fr can be considered rather sim-
ilar to the Scielo ones released for these language
pairs in the two previous challenges (Bojar et al.,
2016; Jimeno Yepes et al., 2017). From values
below the 20 points in 2016, results from en/pt
jumped to almost 40 in 2017 and over the latter
(up to 43.14) this year. A similar increase was ob-
served for en/es that increased more slowly from
up to 33 points in 2016, up to 36 points in 2017 and
up to 44 this year. On the other hand, not much im-
provement can be noticed from en/fr in 2016 (up
to 22.75) to this year’s best score (23.24). These
values are also similar to the scores reported on
another MEDLINE dataset in 2013 (Jimeno-Yepes
and Névéol, 2013).

Regarding translations into English, for es/en,
BLEU scores experienced an improvement from
37 to 43 points in the last year. However, the same
could not be noticed for pt/en that remained rather
constant around 41-43 points.

Finally, during our manual validation, we ob-
served for the first time that the quality of some
automatic translations was either equal or better
than the reference translation. Two teams scored
as good as the reference translation, namely, LMU
for de/en and UHH-DS for en/es. Moreover, two
teams scored higher than the reference transla-
tions, namely, LMU for en/de and UFRGS for
en/es.

5.4 Quality of the automatic translations

Here we provide an overview of the quality of the
translations and the common errors that we identi-
fied during the manual validation.

English: The English translations appeared in
general to have improved qualitatively over prior
year submissions. While in prior years trans-
lations often contained remnants of untranslated
terms from the source language mixed into the
translation, this problem was noted less often in
this year’s evaluations. In addition, systems ap-
peared to make more effective use of capitalisa-
tion, avoiding translation of acronyms or attempt-
ing to translate an acronym semantically via its ex-
pansion.

In light of this overall improvement, a better
translation is often decided by subtle, more pre-
cise choices of English words this year. For in-
stance, an “increasing trend” is more precise as
well as the more customary usage than “accentu-

334



ating trend”; the “dissemination” of knowledge is
likewise a better word choice than “diffusion” of
knowledge. Similarly, a study objective “to as-
sess” the level of something would be preferred
to “to know” the level.

The automated systems maintained higher fi-
delity with the original texts than reference trans-
lations, with the latter often leaving out portions
of the original sentence or restructuring informa-
tion between contiguous sentences. Since the au-
tomated systems strive to translate the complete
content of a sentence, they were in many cases per-
ceived to be more accurate due to completeness,
even where minor usage errors occurred.

An error that was observed regularly for Span-
ish to English translations in particular was the
lack of a subject pronoun or insertion of a gen-
dered pronoun (“He”/“She”) at the start of a
sentence where a demonstrative pronoun (“It”,
“This”) would be more appropriate. As a pro-drop
language, the source Spanish texts often lacked an
overt subject; this subject needs to be introduced
for the English translation to be fully grammatical
but some systems appeared to struggle with this
requirement.

Another error observed across different lan-
guages was the partial translation of multi-word
biomedical terms. As an example, “upper diges-
tive endoscopy” was translated as “high diges-
tive endoscopy,” where presumably “digestive en-
doscopy” was referenced from a biomedical dic-
tionary but the word “high” was decoupled from
the multi-word term. Although this error was less
prevalent, its occurrences critically reduced the
quality and crippled the scientific meaning of the
translation.

French: The quality of translations for French
seemed quite equivalent to last year, and varied
from poor to good. A number of automatically
translated sentences carried out the meaning of the
original sentence properly, but were assessed as
inferior to the reference for stylistic reasons, be-
cause they provided a more literal translation that
mimicked the structure of the original sentence.
Arguably, those sentences could be considered as
useful to grasp the meaning of the original sen-
tence. However, translation omissions were noted
in long or complex sentences. For example, the
phrase ”potential drug-drug and food-drug inter-
actions” was translated by ”interactions poten-
tielles entre médicaments et médicaments”, which

does not account for food-drug interactions. A
couple of recurring errors also observed in previ-
ous years are the lack of translation for acronyms
and the erroneous choice of pronoun in translation.
For example, ”they” was systematically translated
as ”ils”, even when ”elles” was the correct trans-
lation based on context.

The use of manual segmentation on the EDP
corpus resulted in a number of single word ”sen-
tences” corresponding to the titles of the sections
in structured abstracts, such as ”Introduction:”
or ”Conclusion:”. Unsurprisingly, the systems
performed well on these isolated segments (ex-
cept for one occurrence of ”Materials and Meth-
ods” translated by ”Matériaux et Procédés” in-
stead of the usual ”Matériel et Méthodes”), which
may contribute to explain the number of instances
where the automatic translation and manual refer-
ence were considered equivalent. It can be noted
that dealing with section titles as isolated segments
successfully ensured there were no translation er-
rors linked to failure to identify the section words
as isolated titles.

German: Interestingly, for 80% of the sentences
automatic translation was evaluated equally good
or even better than the German reference. This
observed result has different reasons. Often the
German reference translation is correct but ei-
ther contains additional information or misses in-
formation present in the English source sentence
while the automatic translation does not have this
error. As previously mentioned this is strongly
related to the frequent alignment errors present
in the German dataset. In some cases valida-
tion was very difficult as both translations were
very good but we still tried to differentiate be-
tween them. For instance, “thromboembolic com-
plications” was translated to “thromboembolis-
che Ereignisse” (events) in the reference and
to “thromboembolische Komplikationen” (com-
plications) in the automatic translation. In this
case the evaluator scored LMU>Reference while
also LMU=Reference would be possible.

In general, we only observe minor mistakes in
automatic translation. Rarely, we find wrongly
translated technical terms such as cerebrum
wrongly translated to Gebärmutter (uterus). Often
mistakes originate from a slight misuse of terms
with the same overall meaning but different appli-
cation in the medical domain. For instance, soft
tissue repair was translated to Weichteilsanierung

335



instead of Weichteilrekonstruktion, while the lat-
ter is the correct medical term. Similarly, effi-
ciency of medication should be Wirksamkeit von
Medikamenten instead of Effizienz von Medika-
menten. Compared to submissions in 2017, we did
not observe problems in syntax or grammar which
could have caused misunderstanding the meaning
of the sentence. This year, only the LMU team
submitted reaults and already in 2017 their system
did not have syntax problems.

Portuguese: We observed both minor and ma-
jor mistakes in the automatic translations to Por-
tuguese. We classified as minor those errors that
did not compromise the overall understanding of
the sentence and that were limited to orthography
or minor grammatical errors. For instance, we
found many wrong spaces separating compound
words (e.g., “amarelo- palha”) and before com-
mas or the final period (e.g., “desafio médico ,
éticas”). Further, translations from one particu-
lar system seemed to consistently start sentences
with a lower case (e.g., “manifestações clı́nicas
de neurofibromatose tipo 1 são variáveis”). Fi-
nally, other frequent minor mistakes were miss-
ing definite articles, such as in “existem três vari-
antes do osteocondroma extraesquelético : con-
dromatose sinovial , condroma para-articular” in-
stead of “a condromatose sinovial, o condroma
para-articular”.

We classified as major those mistakes that con-
siderably compromised the understanding of the
sentence. These were cases of discordance with
number and gender for the adjectives, e.g., “é
um desafio médico , éticas e psicossociais” in-
stead of “é um desafio médico, ético e psicosso-
cial”. There were also verbal discordances, such
as in “houve um caso que foram tratados” in-
stead of “houve um caso que foi tratado”. Further,
we found many words that were not translated
into Portuguese and remained in English, such as
in the passages “sem tratamento tumor-directed”,
“Forty-seven casos”. Also some acronyms were
not correctly into Portuguese (e.g., PET/CT in-
stead of PET/TC), but translations from one of
the teams seem to have gotten most acronyms,
biomedical terms and numbers right. Finally,
given the differences in word ordering between
English and Portuguese, this error occurred in
passages such as “pacientes queixa instead of
“queixa dos pacientes”.

Some translation were exactly like the reference

translations, which makes us suspect that those ab-
stracts could be included in the Scielo corpus used
for training data by the systems. However, there
were just a couple of such cases and these should
not compromise overall evaluation. In spite of the
above, we also found very good translations even
for complex and long sentences and for biomedi-
cal terms with multiple modifiers, such as in “es-
clerose múltipla secundária progressiva”.

Spanish: Considering previous years of the
challenge, the translations seem to improve and
there are fewer issues compared to previous years.
On the other hand, the issues we identified are
similar to the ones identified in the Portuguese
sets. As with the translations from Spanish into
English, there were some cases of source words
not being translated into Spanish, as in “el es-
tado fsico motor 20 Meter Shuttle Run Test”
and “Substance-induced fueron”.

Both types of methods seem to suffer from
gender and number agreement for determiners as
in “La pulsos mejor en las” and sometimes for
verbs in terms of number as in “Legendre de-
scriben el primer modelo”, which might be mis-
leading. We also found that some systems dis-
played a preference for starting sentences with
lower case letters; however, different from the case
in Portuguese, for the manually evaluated cases
there no issues with acronyms or spaces between
hyphenated words.

Romanian: This year there fewer participants
than in the previous year. Especially regrettable
is the absence of the top performers from the Uni-
versity of Edinburgh. The only team which par-
ticipated last year as well is that of the University
of Hamburg, which improved this year by using
a training dataset subsampling heuristic in their
SMT translator, but trailed again the NMT system
in this task.

When manually comparing the translations, we
have prefered the ones having the better gram-
mar – for example “Diagnosticul precoce s, i trata-
mentul infect,iei sunt asociate (...)” was prefered
to “Diagnosticul precoce şi tratamentul infecţiei
este asociat(...)” for correct subject to verb agree-
ment. Disturbing in translations is the occurence
of words that have no correspondent in the origi-
nal, for example “upon patients’ arrival in the post
anaesthesia care unit” translated as “asupra sosirii
pacient,ilor ı̂n unitatea de ı̂ngrijire a tuberculozei”.

336



Totally incorrect translations were observed as
well, such as “In this observational study, clini-
cal data, vital signs and comfort parameters were
collected from surgical patients who arrived in
the PACU.” being translated into “În acest studiu
observat,ional, date contractuale, semne vitale s, i
parametri portuari au fost colectate de la pacient,i
morali care au sosit ı̂n PACU.”. In such a short
sentence there are already three incorrectly trans-
lated words. A dictionary-based method would
have done better, as there is also no ambiguity in-
volved.

An interesting aspect was observed where
the typical preprocessing leads to ambiguities.
“MAP” (mean arterial pressure) changes to “map”
(like in geographical map) and then is translated
as such: “MAP and HR” was translated as “Harta
şi hr”.

Another interesting and potentially dangerous
error is the mistranslation of the time units. In
one case “Haemofiltration was continued postop-
eratively in the ICU for another 48 h” was trans-
lated as “Haemofiltrarea a continuat postoperativ
ı̂n ICU pentru ı̂ncă 48 de ani” thus replacing hours
with years.

In some cases the translations were marked
equal because they were equally bad. In general,
the intelligibility and fidelity of the translation was
preferred to the form (grammar, smoothness, natu-
ralness), and only for equal content the better form
prevailed.

6 Conclusions

This was the third year we have organized the
WMT biomedical shared task and we found that
the performance has been increasing constantly.
Improvements in results seem to be due to a vari-
ety of reasons, including more in-domain training
data and the use of additional methods that con-
sider transfer approaches and ensemble combina-
tion of methods.

From an evaluation perspective, we find that the
results improve when we consider only sentences
that were perfectly aligned instead of consider-
ing all the automatically aligned sentences. This
shows some limitations on the quality of the auto-
matically generated test sets. On the other hand,
the comparative performance of the different par-
ticipating systems remains the same.

For some of the languages considered, there
were limitations in the quantity of available par-

allel abstracts. Recent publications include paral-
lel corpora from the database that were previously
used for obtaining our test sets. These new corpora
include Medline parallel abstracts (Villegas et al.,
2018) and full texts from Scielo (Soares et al.,
2018). Therefore, manual translation for building
the future test could be considered in the following
editions of the challenge.

Finally, future improvements should also ad-
dress problems reported by the participants regard-
ing the current format of the test sets. In the three
years of the challenge, we have used BioC as the
format for data exchange, which seemed to cause
some difficulties for sentence alignment. We will
evaluate available formats for data exchange with
the participants or inspired in other shared task in
WMT.

Acknowledgments

We would like to thank the participation of all
teams in the challenge and the support of selected
participants in the manual validation of the trans-
lations.

References
Olivier Bodenreider. 2004. The unified medical

language system (UMLS): integrating biomed-
ical terminology. Nucleic acids research,
32(suppl 1):D267–D270.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck,
Antonio Jimeno Yepes, Philipp Koehn, Varvara
Logacheva, Christof Monz, Matteo Negri, Aure-
lie Neveol, Mariana Neves, Martin Popel, Matt
Post, Raphael Rubino, Carolina Scarton, Lucia Spe-
cia, Marco Turchi, Karin Verspoor, and Marcos
Zampieri. 2016. Findings of the 2016 Conference
on Machine Translation. In Proceedings of the First
Conference on Machine Translation (WMT16) at
the Conference of the Association of Computational
Linguistics, pages 131–198.

Mirela-Stefania Duma and Wolfgang Menzel. 2018.
Translation of biomedical documents with focus on
Spanish-English. In Proceedings of the Third Con-
ference on Machine Translation, Brussels, Belgium.
Association for Computational Linguistics.

Christian Federmann. 2010. Appraise: An open-source
toolkit for manual phrase-based evaluation of trans-
lations. In In LREC.

Christian Federmann. 2018. Appraise evaluation
framework for machine translation. In Proceedings
of the 27th International Conference on Computa-
tional Linguistics: System Demonstrations, pages
86–88. Association for Computational Linguistics.

337



Cristian Grozea. 2018. Ensemble of translators with
automatic selection of the best translation - the sub-
mission of FOKUS to the wmt 18 biomedical trans-
lation task -. In Proceedings of the Third Conference
on Machine Translation, Brussels, Belgium. Associ-
ation for Computational Linguistics.

Matthias Huck, Dario Stojanovski, Viktor Hangya, and
Alexander Fraser. 2018. LMU Munichs neural ma-
chine translation systems at WMT 2018. In Pro-
ceedings of the Third Conference on Machine Trans-
lation, Brussels, Belgium. Association for Computa-
tional Linguistics.

Antonio Jimeno-Yepes and Aurélie Névéol. 2013. Ef-
fect of additional in-domain parallel corpora in
biomedical statistical machine translation. In Pro-
ceedings of the Fourth International Workshop on
Health Text Mining and Information Analysis.

Antonio Jimeno Yepes, Aurélie Névéol, Mariana
Neves, Karin Verspoor, Ondrej Bojar, Arthur Boyer,
Cristian Grozea, Barry Haddow, Madeleine Kittner,
Yvonne Lichtblau, Pavel Pecina, Roland Roller,
Rudolf Rosa, Amy Siu, Philippe Thomas, and
Saskia Trescher. 2017. Findings of the WMT 2017
Biomedical Translation Shared Task. In Proceed-
ings of the Second Conference on Machine Transla-
tion, pages 234–247, Copenhagen, Denmark. Asso-
ciation for Computational Linguistics.

Marcin Junczys-Dowmunt, Roman Grundkiewicz,
Tomasz Dwojak, Hieu Hoang, Kenneth Heafield,
Tom Neckermann, Frank Seide, Ulrich Germann,
Alham Fikri Aji, Nikolay Bogoychev, André F. T.
Martins, and Alexandra Birch. 2018. Marian: Fast
Neural Machine Translation in C++. In Proceedings
of ACL 2018, System Demonstrations, pages 116–
121. Association for Computational Linguistics.

Lukasz Kaiser, Aidan N. Gomez, Noam Shazeer,
Ashish Vaswani, Niki Parmar, Llion Jones, and
Jakob Uszkoreit. 2017. One model to learn them
all. CoRR, abs/1706.05137.

Abdul Rafae Khan, Subhadarshi Panda, Jia Xu, and
Lampros Flokas. 2018. Hunter NMT system for
WMT18 biomedical translation task: Transfer learn-
ing in neural machine translation. In Proceedings of
the Third Conference on Machine Translation, pages
1–2, Brussels, Belgium. Association for Computa-
tional Linguistics.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander Rush. 2017. OpenNMT:
Open-source toolkit for neural machine translation.
In Proceedings of ACL 2017, System Demonstra-
tions, pages 67–72. Association for Computational
Linguistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open

source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Vladimir S. Lazarev and Serhii A. Nazarovets. 2018.
Don’t dismiss citations to journals not published in
english. Nature Correspondence, 556:174.

Xiaoyi Ma. 2006. Champollion: A robust parallel text
sentence aligner. In In Proceedings of LREC-2006.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Maria Mitrofan and Dan Tufis. 2018. BioRo: The
Biomedical Corpus for the Romanian Language. In
Proceedings of the Eleventh International Confer-
ence on Language Resources and Evaluation (LREC
2018), Miyazaki, Japan. European Language Re-
sources Association (ELRA).

Aurélie Névéol, Antonio Jimeno Yepes, Mariana
Neves, and Karin Verspoor. 2018. Parallel cor-
pora for the biomedical domain. In Proceed-
ings of the Eleventh International Conference on
Language Resources and Evaluation (LREC 2018),
Paris, France. European Language Resources Asso-
ciation (ELRA).

Mariana Neves, Antonio Jimeno Yepes, and Aurélie
Névéol. 2016. The Scielo Corpus: a Parallel Corpus
of Scientific Publications for Biomedicine. In Pro-
ceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC 2016),
Paris, France. European Language Resources Asso-
ciation (ELRA).

Daniel Prieto. 2018. Make research-paper databases
multilingual. Nature Correspondence, 560:29.

Felipe Soares and Karin Becker. 2018. UFRGS par-
ticipation on the wmt biomedical translation shared
task. In Proceedings of the Third Conference on
Machine Translation, Brussels, Belgium. Associa-
tion for Computational Linguistics.

Felipe Soares, Viviane Moreira, and Karin Becker.
2018. A Large Parallel Corpus of Full-Text Sci-
entific Articles. In Proceedings of the Eleventh In-
ternational Conference on Language Resources and
Evaluation (LREC 2018), Miyazaki, Japan. Euro-
pean Language Resources Association (ELRA).

Juan Tao, Chengzhi Ding, and Yuh-Shan Ho. 2018.
Publish translations of the best chinese papers. Na-
ture Correspondence, 557:492.

Brian Tubay and Marta R. Costa-Jussà. 2018. Neural
machine translation with the Transformer and multi-
source romance languages for the biomedical wmt

338



2018 task. In Proceedings of the Third Conference
on Machine Translation, Brussels, Belgium. Associ-
ation for Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. CoRR, abs/1706.03762.

Marta Villegas, Ander Intxaurrondo, Aitor Gonzalez-
Agirre, Montserrat Marimn, and Martin Krallinger.
2018. The MeSpEN resource for English-Spanish
medical machine translation and terminologies:
Census of parallel corpora, glossaries and term
translations. In Proceedings of the Eleventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2018), Paris, France. European Lan-
guage Resources Association (ELRA).

339


