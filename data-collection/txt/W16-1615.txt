



















































Decomposing Bilexical Dependencies into Semantic and Syntactic Vectors


Proceedings of the 1st Workshop on Representation Learning for NLP, pages 127–136,
Berlin, Germany, August 11th, 2016. c©2016 Association for Computational Linguistics

Decomposing Bilexical Dependencies into Semantic and Syntactic Vectors

Jeff Mitchell
mitchelljeff@hotmail.com

Abstract

Bilexical dependencies have been com-
monly used to help identify the most likely
parses of a sentence. The probability of
a word occurring as the dependent of a
given head within a particular structure
provides a measure of semantic plausibil-
ity that complements the purely syntactic
part of the parsing model.

Here, we attempt to use the distribu-
tional information within these bilexical
dependencies to construct representations
that decompose into semantic and syntac-
tic components. In particular, we com-
pare two different approaches to compos-
ing vectors to explore how syntactic and
semantic representations should interact
within such a model.

Our results suggest a tensor product ap-
proach has advantages, which we believe
could be exploited in making more ef-
fective use of the information captured in
these bilexical dependencies.

1 Introduction

Using points within the geometry of a vector space
to represent the way words are distributed across
contexts has proven to be a fruitful tactic for many
language processing tasks. For example, Landauer
and Dumais (1997) projected raw tf-idf scores
of occurrence across a set of documents down
into lower dimensional vectors using a technique
called singular value decomposition. The result-
ing semantic representations were then applied to
semantic dismbiguation and to predict synonyms
in a TOEFL test. Working instead with the linear
structure of raw text, Collobert et al. (2011) trained
a neural language model to induce word vectors in

the hidden layer of their network. These versatile
representations were then applied to a wide range
of tasks including part-of-speech tagging, chunk-
ing, named entity recognition, and semantic role
labeling.

Two key elements within any such approach
to constructing representations are the contexts
across which the distribution of a word is tracked
and how vectors are constructed from these oc-
currences. Here, we investigate the construction
of distributional representations from bilexical de-
pendencies found in a parser and explore how such
vectors can be decomposed into semantic and syn-
tactic components.

Although, distributional approaches have com-
monly become most strongly associated with se-
mantic representations and tasks, they have also
seen applications to syntax. In fact, distributional
analysis was first applied by linguists to syntactic
categories rather than the representation of mean-
ing and Ross (1972) presented a continuous, or at
least graded, conception of syntax long before the
recent surge of interest in vectorial approaches to
semantics.

Practical applications of these distributional
techniques to syntactic problems have included
work on the induction (Brown et al., 1992) or
learning (Mintz, 2003) of categories and the com-
putational problems of tagging (Tsuboi, 2014) and
parsing (Socher et al., 2013). The latter problem
of parsing brings to the foreground the question
of how syntactic and semantic representations re-
late to and interact with each other, as the optimal
parse must maximise both syntactic and semantic
plausibility, in an integrated structure.

An unmodified PCFG, modelling just the de-
pendencies between syntactic categories, is gen-
erally inadequate to derive robust parses, and lexi-
calisation is commonly used to enhance such mod-
els. In particular, bilexical dependencies introduce

127



a measure of the plausibility of combining specific
heads and dependents within the possible syntactic
structures.

These dependencies contain much the same in-
formation used by Lin (1998) and Padó and Lapata
(2007) to construct semantic representations, and
we can easily see that the plausibility of cake as an
object of bake, eat and regret tells us something
about the semantic properties of cake. Nonethe-
less, these dependencies contain substantial quan-
tities of syntactic information, too. The depen-
dencies observed for cake and eat, for example,
are substantially different because the former is a
noun while the latter is a verb.

However, the sparsity of the resulting counts
can mean these dependencies may contribute lit-
tle to parser performance, particulaly on out of
domain data. One solution, proposed by Rei and
Briscoe (2013), is to smooth the bilexical depen-
dencies using a similarity measure. For example,
if counts for publication as an object of read are
lacking we might instead leverage the similarity
of publication to book to use the counts for book
as an object of read to make a reasonable infer-
ence about the unseen dependency. Alternatively,
we might try to use some form of dimensionality
reduction to smooth out the sparsity.

Levy and Goldberg (2014) use a modified ver-
sion of word2vec (Mikolov et al., 2013) to in-
duce 300 dimensional representations from word
distributions across 900,000 dependency contexts.
They find that these word vectors capture a form
of functional similarity, with the closest words in
the space typically being cohyponyms within the
same syntactic class. This syntactic specificity is
not particularly surprising, as we would expect the
strongest effects within these dependencies to re-
late to the syntactic class of a word - e.g. only a
noun can be the subject of a verb - with semantic
factors having a weaker influence merely on word
choice within the correct syntactic class.

In this paper, we will consider a couple of ap-
proaches that attempt to separate out semantic and
syntactic components of the dependencies, boost-
ing performance on both types of task. One popu-
lar method of boosting semantic performance has
been to ignore or average over syntactic structure.
By treating the context a target occurs in as a bag-
of-words (Landauer and Dumais, 1997; Blei et
al., 2003; Mikolov et al., 2013), syntactic infor-
mation is washed out and semantic information

is retained. Conversely, distributional approaches
to syntactic tasks typically make use of the se-
quential structure contained in bigrams (Brown et
al., 1992; Clark, 2003) or longer n-grams (Mintz,
2003; Redington et al., 1998).

Recent work, (Mitchell, 2013; Mitchell and
Steedman, 2015), has attempted to use both types
of information in a single model that decomposes
representations into syntactic and semantic com-
ponents. An open question, however, is the most
effective way of forming these combined represen-
tations. Mitchell and Steedman (2015) explicitly
employ a direct sum - i.e. concatenation - of se-
mantic and syntactic vectors. On the other hand,
the multiplicative combination used by Mitchell
(2013) is much closer to a tensor product formula-
tion.

Griffiths et al. (2005) also pursue the represen-
tation of semantics and syntax in a single distri-
butional model. They integrate a topic model and
HMM to produce a model of the sequential struc-
ture of raw text in which each word is either se-
mantic - chosen by the topic model to fit the long
range semantic context - or syntactic - chosen to fit
the short range dependencies of the HMM. This ei-
ther/or assumption is rejected by Boyd-graber and
Blei (2009) who moreover work with parsed sen-
tences, rather than raw text. In this model, each
word is chosen based on a product of a document
topic distribution and a set of syntactic transi-
tion probabilities, determined top-down within the
parse tree. Socher et al. (2010) are also concerned
with inducing distributional representations within
the stucture of parse trees. Their neural network
model composes vectors recursively from the bot-
tom up to represent possible phrases and from
those representations computes how likely each is
to be a valid constituent.

Although, parsing may seem, initially, to be the
ideal task in which to explore the relationship be-
tween semantic and syntactic representations, the
complexity of a working system - which Bikel
(2004a) describes as an intractable behemoth -
makes it difficult to isolate and investigate just
this question on its own. Parser performance de-
pends on a multitiude of interacting components,
and could only obliquely produce insights into the
merit of the approaches to representation we want
to consider here.

Instead, we follow the advice of Bikel (2004b)
to treat the model as data, and make direct eval-

128



uations of distributional representations induced
from the parameters of a wide coverage model.
We focus in on just the bilexical dependencies
within the BLLIP parser (Charniak and Johnson,
2005; McClosky et al., 2006) and explore models
of these parameters in which the representation for
each word decomposes into a semantic and a syn-
tactic vector. We evaluate both a direct sum and a
tensor product approach to this decomposition of
the representation space and find that the latter has
advantages.

In the next section, we describe the BLLIP
parser and the data we extract from the wide cov-
erage model of McClosky et al. (2006). Then in
Sections 3 and 4 we describe the models applied to
this data and their evaluation. Finally, we present
our results and conclusions in Sections 5 and 6.

2 BLLIP Parser

The BLLIP parser (Charniak and Johnson, 2005)
uses a two stage approach, based on discriminative
reranking applied to candidate parses produced by
a generative lexicalised PCFG. That first stage of
the model takes inspiration from loglinear models
to express the overall parse probability in terms of
a product of multiplicative factors.

Here we are specifically interested in the bilex-
ical dependencies, which are stored in the model
as a probability, p(d|h, t), of a dependent, d, given
a head, h within some tree structure, t, along with
a count for the occurence of that head-tree com-
bination. The tree structure, t, is only specified
in terms of the tags on the head and dependent
leaves, the node from which they branch, and the
category of the dependent branch below that point.
Thus, many distinct trees are collapsed into a sin-
gle class. For example, the model fails to distin-
guish between subjects and objects of a verb.

McClosky et al. (2006) expanded the domain of
the standard Penn Treebank (Marcus et al., 1993)
trained BLLIP model, applying self-training to
2.5M sentences from the NANC corpus (Graff,
1995). The resulting model has a large vocab-
ulary, with reliable estimates of probabilities for
many words, which provides a useful basis for our
investigations.

We extract the bilexical dependencies and head-
tree counts from the model file, replacing words
that occur less than 5 times with an 〈UNK〉 tag,
and also excluding any word that does not occur in
both the head and dependent positions. The head-

tree contexts are similarly filtered, with items that
occur less then 5 times replaced with a dummy
catch-all context.

3 Models

The models we discuss here derive a probability of
a dependent word, d, within the context of a tree,
t, with a head, h, in terms of latent variables, e.g.
i, j, k. So, the simplest model we will consider has
the form:

p(d|h, t) =
∑

i

p(d|i)p(i|h, t) (1)

It will be useful, notationally and conceptually,
to think of these models in terms of vectors. The
equation above already has a superficial similar-
ity to a dot product, being a sum over a series of
products.

We can rewrite this:

p(d|h, t) = p(d)
∑

i

p(i)
p(i|d)
p(i)

p(i|h, t)
p(i)

(2)

We will think of p(i|x)p(i) as being the components,
vxi , of a vector, v

x, representing x and define an in-
ner product in terms of a weighted 1 sum of com-
ponent products as follows:

u · v =
∑

i

λiuivi (3)

Taking λi = p(i), we can rewrite Eq. 2 as fol-
lows:

p(d|h, t) = p(d)vd · vht (4)
More generally, this model form will need to

include normalisation:

p(d|h, t) = p(d)v
d · vht

N(h, t)
(5)

One of the benefits of this model form is that
the normalising constant for each head-tree can be
calculated fairly efficiently in terms of a single in-
ner product.

N(h, t) = n · vht (6)
Here, n is a sum over all dependent probabilities

and vectors.
1The use of such a weighting implies we are working with

unnormalised basis vectors.

129



n =
∑

d

p(d)vd (7)

Given this model form, we must then specify
how the vectors v are constructed. In particular,
if our representations are based on semantic vec-
tors, a, and syntactic vectors, b, then we must de-
cide how these are to be combined. One obvious
choice is between a direct sum (Eq. 8) and a tensor
product (Eq. 9).

v̄ = ā⊕ b̄ (8)

ṽ = ã⊗ b̃ (9)
Although both these constructions consist of a

combination of vectors, a and b, the actual vec-
tors induced during EM training will inevitably
turn out to be substantially different for each ap-
proach. In fact, our purpose is precisely to in-
vestigate how this choice of combination affects
the representations induced in our trained models.
We therefore notationally distinguish the two ap-
proaches: using a bar, v̄, to indicate direct sum
vectors and a tilde, ṽ, for tensor product vectors.
However, we will also employ bare symbols with-
out bar or tilde when discussing general properties
across both types of structure.

So, if a and b are m and n dimensional vec-
tors respectively, then Eq. 8 corresponds to form-
ing the n+m dimensional concatenation of those
vectors, while Eq. 9 results in the n ×m dimen-
sional vector of all products of their components.
From a probabilistic perspective, a reasonable in-
terpretation would be that our models using the di-
rect sum representations in Eq. 8 assume that the
dependencies between head and dependent are ei-
ther syntactic or semantic, whereas tensor product
models, Eq. 9, assume that each word has both
semantic and syntactic characteristics.

Given some method for combining vectors a
and b, we also need to specify the form of their
components. In particular, we are interested here
in separating semantic and syntactic dependen-
cies.

Mitchell (2013) and Mitchell and Steedman
(2015) both exploit word order to decompose rep-
resentations into semantic and syntactic compo-
nents, with semantic dependencies being modelled
in terms of a similarity measure that is indepen-
dent of word order, while the syntactic part of the
model captures sequential information. However,

the bilexical dependencies we are working with
here do not explicitly relate to surface word or-
der. Nonetheless, the relationship is still directed,
distinguishing a head and a dependent, and we
can exploit this directedness to define a symmetric
semantic component and an asymmetric syntactic
component.

In each of the models below, any word has a sin-
gle semantic vector, a, whether it occurs in head or
dependent position, with components aj given by:

aj =
p(j|w)
p(j)

(10)

Ignoring the syntactic component of the model
for a moment, we can define a semantics only
model (leaving out the normalising constant for
brevity):

p(d|h, t) ∝ p(d)
∑

j

p(j)
p(j|wd)
p(j)

p(j|wh)
p(j)

= p(d)ad · ah (11)
This employs an inner product defined by

ad · ah = ∑j λjadjahj with λj = p(j). However,
this semantic only model ignores the tree t the
words occur in and gives words the same repre-
sentations whether they occur in head or depen-
dent position. It is therefore symmetric in relation
to these roles, and we can think of this model as
capturing what head and dependent have in com-
mon.

In contrast, there are two forms of syntactic vec-
tors, bd and bht, distinguishing between depen-
dents and the head-tree contexts they occur in,
with components given by:

bhtk =
p(k|h, t)
p(k)

(12)

bdk =
p(k|d)
p(k)

(13)

Again we can ignore the other part of the model
and consider this part on its own:

p(d|h, t) ∝ p(d)
∑

k

p(k)
p(k|d)
p(k)

p(k|h, t)
p(k)

= p(d)bd · bht (14)
This is exactly equivalent to the simple model

Eq. 4 above.

130



Taking the direct sum approach first (Eq. 8), we
concatenate the semantic vectors, ā, and syntac-
tic vectors, b̄, to form a combined vector, v̄, with
indices ranging over both j and k.

v̄i =
{
āj if i = j;
b̄k if i = k.

(15)

Inner products of such vectors will consist of a
sum over the j component products followed by
a sum over the k component products, with some
appropriate weighting. The simplest model having
this structure is an interpolation of the two models
above (Eq. 11 and Eq. 14) with proportions qa and
qb.

p(d|ht) ∝ qap(d)ād · āh + qbp(d)b̄d · d̄ht (16)
In terms of the model form of Eq. 5, this is

equivalent to defining an inner product on the di-
rect sum vectors, v̄d · v̄ht, with weightings λ of
qap(j) and qbp(k) for the two sets of components
respectively.

v̄d · v̄ht =

qa
∑

j

p(j)
p(j|wd)
p(j)

p(j|wh)
p(j)

+

qb
∑

k

p(k)
p(k|d)
p(k)

p(k|h, t)
p(k)

(17)

For the tensor product model (Eq. 9), the in-
dices of the combined vector, ṽ, range over all
combinations of j and k.

ṽjk = ãj b̃k (18)

The components of ṽht are then given by prod-
ucts of terms, which suggests conditional indepen-
dence of j and k on ht.

ṽjk =
p(k|h, t)
p(k)

p(j|h)
p(j)

=
p(j, k|h, t)
p(j)p(k)

(19)

Making a similar assumption of conditional in-
dependence in relation to ṽd is enough to derive a
model for p(d|h, t).

p(d|h, t) =
∑
jk

p(d|j, k)p(j, k|h, t)

∝ p(d)
∑
jk

p(j|d)p(k|d)
p(j, k)

p(j|h)p(k|h, t) (20)

This can be put into the form of Eq. 5 by defin-
ing the inner product ṽd · ṽht as follows:

ṽd · ṽht =
∑
j,k

λjkã
d
j b̃

d
k × ãhj b̃htk =

∑
j,k

p(j)2p(k)2

p(j, k)


p(j|wd)
p(j)

p(k|d)
p(k)

×
p(j|wh)
p(j)

p(k|h, t)
p(k)

 (21)

Here, the weighting λjk =
p(j)2p(k)2

p(j,k) is based on
the assumption that j and k are both conditionally
independent of d and ht.

As described above, the tensor product of a pair
of vectors (Eq. 9) of dimension m and n produces
a vector of dimension n×m. However, these vec-
tors only form a subset of the full n × m dimen-
sional space. Moreover, the form of the model in
both the direct sum and tensor product cases as-
sumes that the semantic relation of head and de-
pendent is independent of the syntactic relation.
That is, we employ the same semantic vectors to
represent head and dependent irrespective of the
tree they occur in. We could begin to address both
these issues by considering representations that lie
in the full n×m dimensional tensor product space.
This would essentially allow us to represent the
dependence of semantic content on syntactic con-
text. However, for now we restrict ourselves to the
models described above.

We calculate the cross-entropy between the
model and the BLLIP bilexical dependencies for
each head-tree context and our objective function
is then an average of these values, weighted by the
occurence of that context. Training maximises this
measure over 200 iterations of the EM algorithm.

4 Evaluation

We evaluate our models in a number of ways. We
assess the quality of the word representations in
terms of two similarity tasks on the semantic vec-
tors, a, and a POS induction task on the syntactic
vectors, b. In addition, both these tasks are ap-
plied to the raw data and to the vectors induced by
the undecomposed models, Eq. 2 and Eq. 11. We
also investigate the ability of our models to dif-
ferentiate semantically and syntactically implausi-
ble adjective-noun constructions. Finally, we list a
sample of nearest neighbours to allow a qualitative
insight into the best performing model.

131



Our semantic similarity tasks are based on the
ratings in two datasets, on both of which we eval-
uate our models using Spearman correlation. The
first is the WordSim353 dataset (Finkelstein et al.,
2002) containing ratings from 16 participants be-
tween pairs of nouns. The second dataset contains
similarity ratings for noun-verb pairs (Mitchell,
2013). The former measures the ability of the
model to capture semantic similarity within a POS
class, while the latter tells about its representa-
tion of similarity across classes. This cross-class
measure is useful in determining how effective the
model has been in separating semantic from syn-
tactic information. A model that bundles both into
a single representation may identify the similarity
in disappear-vanish but will typically fail to make
the same judgement about disappearance-vanish.
Making that judgement requires ignoring the syn-
tactic difference between nouns and verbs, which
we achieve in our models by representing that in-
formation separately.

Our syntactic task is POS induction. We clus-
ter the vocabulary into 45 classes using k-means,
and evaluate in terms of the many-to-one measure
using the PTB POS classes as a gold standard. Al-
though POS class information is already present in
the bilexical dependency data, we use this task as
a means of determining the quality of syntactic in-
formation contained in the vectors, rather than as
an example of a practical application.

We then examine how our models differentiate
semantic and syntactic plausibility. Our seman-
tic plausibility dataset is constructed by combin-
ing a set of food nouns (e.g. milk, meat, bread,
etc.) with either food appropriate adjectives (e.g.
hot, bitter, sweet, etc.) or implausible political
adjectives (e.g. bipartisan, legislative, constitu-
ional, etc.). To create an equivalent syntactic
plausibility dataset we combine common singu-
lar and plural nouns (e.g. year - years, player
- players, etc.) with the modifier several. In
each case, we calculate a semantic plausibility
(ad · ah = ∑ p(j)adjahj ) and a syntactic plausi-
bility (bd · bht = ∑ p(k)bdkbhtk ) for the resulting
adjective-noun phrase. Comparing the distribution
of these measures in the high and low plausibility
cases allows us to investigate further the extent to
which the model separates semantic and syntactic
dependencies.

Finally, we evaluate the best performing model
- based on a tensor product of vectors - qualita-

tively by examining the closest neighbours of set
of nouns, adjectives and verbs.

5 Results

Table 1 gives the correlations and many-to-one
measures for the raw data, the simple undecom-
posed model (Eq. 2), the symmetric undecom-
posed model (Eq. 11), the direct sum model (Eq.
8) and the tensor product model (Eq. 9). Look-
ing at the first two rows of the table, to compare
the raw data to the simple model, we can see that
the latter outpeforms the fomer on the POS clus-
tering task, but is worse on the semantic similar-
ity tasks. The improvement in performance on
the clustering task can probably be put down to
the excessive dimensionality (= number of head-
tree contexts) of the input space in the case of
the raw data. Reduction of this space using a la-
tent variable model appears to make the cluster-
ing more effective. On the other hand, achiev-
ing this dimensionality reduction requires preserv-
ing the strongest, typically syntactic, dependen-
cies and discarding weaker, frequently semantic,
dependencies with the result that performance on
semantic similarity tasks degrades. The predicted
noun-verb similarities for both approaches is only
weakly correlated with the human ratings, which
we ascribe to the fact that neither model has a
mechanism for finding the commonalities between
words found in distinct sets of syntactic contexts.

The undecomposed symmetric model, a, pro-
duces a better performance on the semantic tasks,
but is worse on the syntactic task. This is not sur-
prising, as the form of this model ignores the dif-
ference between heads and dependents, essentially
treating the dependencies as a bag-of-words.

Both the direct sum and tensor product models
also contain a similarity based component. How-
ever, only the tensor product model achieves an
improvement in performance over the undecom-
posed models. In fact, this model outperforms
the other models on all three measures, includ-
ing achieving a reasonable level of correlation on
the noun-verb dataset. This difference between the
two decomposed models can be related to the fact
that the form of the tensor product assumes that
a dependent should be semantically and syntacti-
cally appropriate to its head-tree context, while the
direct sum model uses an or condition between the
two parts of the model.

Figures 1 and 2 present the results of exper-

132



Model NV WS353 MTO
raw 0.15 0.37 0.39
v -0.06 0.22 0.73
a 0.24 0.42 0.42

ā⊕ b̄ 0.03 0.17 0.61
ã⊗ b̃ 0.38 0.49 0.74

Table 1: Correlations of model cosines with hu-
man similarity ratings on the noun-verb (NV) and
WordSim353 (WS353) datasets, alongside many-
to-one (MTO) measures of cluster quality on the
POS clustering task, for the raw data (raw), the
simple undecomposed model (v), the symmetric
undecomposed model (a), the direct sum model
(ā⊕ b̄) and the tensor product model (ã⊗ b̃).

iments on how these representations predict the
plausibility of various adjective-noun phrases. In
particular, these boxplots give an insight into the
ability of these models to differentiate semanti-
cally from syntactically implausible constructions.
Each plot contrasts the distribution of a logarithm
of a dot product for high and low plausibility
items. This dot product is either taken of seman-
tic vectors, a, or syntactic vectors, b, providing
a measure of, respectively, semantic and syntactic
plausibility as predicted by the model. The results
for each model are organised into a 2 × 2 array
of plots, with the left hand column relating to the
syntactic task and the right hand column to the se-
mantic task.

Examining the results for the tensor product
model in Figure 1 first, we find that of the syntactic
plots, 1a and 1c, the contrast between high and low
plausibility items is greatest for the ln(b̃ · b̃) mea-
sure. This indicates that these vectors have cap-
tured more of the syntactic information necessary
to identify phrases such as several year as implau-
sible. In contrast, the semantic plots, 1b and 1d,
show the reverse pattern. There, it is the ln(ã · ã)
measure which shows the largest contrast between
high and low plausibility items. Thus, the differ-
ence in plausibility between hot bread and bipar-
tisan bread is more effectively captured in the ã
vectors.

Turning to the results for the direct sum model
in Figure 2, this differentiation between semantic
and syntactic plausibility is no longer as clear, and
the largest contrast between high and low plausi-
bility items is always found in the ln(b̄ · b̄) mea-
sure. Specifically, in the plots for the semantic

Word ã⊗ b̃ ã b̃

black

khaki hispanic female

baggy latino decrepit

cashmere midterm handmade

plaid D.C. antique

lace Merle year-old

political

tribal cultural biomedical

tax-and-spend favoritism mechanical

populist sect sole

cultural dissidents mystical

staunch enlightenment forensic

found

discovered evidence unveiled

examined takers snared

pleaded fossils rejected

revealed researchers knocked

flagged conclusive backfired

help

blame strain permit

reprimand psychological resolve

inflict blisters nudge

relieve suffering laugh

prevent suffers jump-start

company

firm manufacturer think-tank

insurer maker nobody

unit pharmacuetical everybody

corporation conglomerate everyone

consortium subsidiary foreman

game

opener missed speech

finale preseason rotation

rout opener shootout

rematch games balloting

tournament NFC opener

Table 2: Nearest neighbours within the full tensor
product space (ã⊗ b̃) and its semantic (ã) and syn-
tactic (b̃) components for a sample of adjectives,
verbs and nouns.

task, 2b and 2d, the ln(ā · ā) measure does not pro-
duce a convincingly smaller prediction for the low
plausibility items. In other words, the ā vectors
do not contain the semantic information needed to
identify the implausibility of phrases such as bi-
partisan bread or constitutional milk.

Thus, the tensor product space appears to be
most effective in separating semantic and syntac-
tic information and its structure can be understood
more concretely in terms of the sample of nearest
neighbours shown in Table 2. Taking the adjective
black as an example, the first column, ã ⊗ b̃, lists
its nearest neighbours within the full tensor prod-

133



high low

ln
(b~

⋅b~
)

Syntactic

(a)

high low

ln
(b~

⋅b~
)

Semantic

(b)

high low

ln
(a~

⋅a~
)

Syntactic

(c)

high low

ln
(a~

⋅a~
)

Semantic

(d)

Figure 1: Boxplots of plausibility factors for ten-
sor product representations on syntactic and se-
mantic tasks.

uct space, which appear to be other descriptors of
material appearance or structure. In contrast, the
neighbours within the semantic space, ã, listed in
the second column, seem to be words with cultural
or political associations to black, while the syn-
tactic neighbours in the third column, b̃, are other
adjectives drawn from a much wider domain.

6 Conclusions

We have shown that the bilexical dependencies
within a parser capture useful semantic informa-
tion, and also that it is possible to, at least par-
tially, begin to separate out this semantic informa-
tion from the syntactic information. Our experi-
ments with vectors based on ratios of probabilities
suggest that a tensor product approach to decom-
posing the space of representations has advantages
over a direct sum approach. While the latter is
conceptually simpler, being just a concatenation of
the two vectors, the resulting model corresponds
to an assumption that semantic and syntactic de-
pendencies are disjoint, i.e. that the relationship
between head and dependent is either semantic or
syntactic. In contrast, the tensor product approach
leads to a model in which a dependent must be
syntactically and semantically appropriate to the
context of tree and head word, and this seems to
be more effective in practice.

These conclusions apply only to the ratio of

high low

ln
(b

⋅b
)

Syntactic

(a)

high low

ln
(b

⋅b
)

Semantic

(b)

high low

ln
(a

⋅a
)

Syntactic

(c)

high low

ln
(a

⋅a
)

Semantic

(d)

Figure 2: Boxplots of plausibility factors for di-
rect sum representations on syntactic and semantic
tasks.

probabilities type vectors that were investigated
here. Log-linear vectors, as produced by neural
network models, are likely to show substantially
different behaviours. In fact, Mitchell and Steed-
man (2015) have shown that a direct sum approach
can be effective for this type of model. Future
work should investigate tensor product models in
this setting.

Furthermore, there are theoretical reasons to
pursue the tensor product approach further. While
the models considered here are based on combin-
ing separate, independent semantic and syntactic
vectors, the tensor product approach also allows us
to consider the interaction of the two components.
The direct sum approach, on the other hand, is less
expressive.

In addition, implementation of parsers based on
these representations may also be a fertile direc-
tion for future work. Our results suggest the tech-
niques we investigated are effective in construct-
ing semantic representations. We would also like
to know whether capturing that semantic informa-
tion effectively has benefits in modelling the over-
all probability of the whole dependency. However,
our initial investigations suggest the syntactic part
of the dependency needs a more sophisticated ap-
proach.

134



References
Daniel Bikel. 2004a. A distributional analysis of a lex-

icalized statistical parsing mode. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 182–189.

Daniel M. Bikel. 2004b. On the parameter space
of generative lexicalized statistical parsing models.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA, USA.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022, March.

Jordan L. Boyd-graber and David M. Blei. 2009. Syn-
tactic topic models. In D. Koller, D. Schuurmans,
Y. Bengio, and L. Bottou, editors, Advances in Neu-
ral Information Processing Systems 21, pages 185–
192. Curran Associates, Inc.

Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Comput. Linguist., 18(4):467–479, Decem-
ber.

Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’05, pages 173–180, Ann Arbor, Michigan. As-
sociation for Computational Linguistics.

Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth Annual Meeting
of the European Association for Computational Lin-
guistics (EACL), pages 59–66.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Trans. Inf. Syst., 20(1):116–
131, January.

David Graff. 1995. North american news text corpus.
LDC95T21.

Thomas L. Griffiths, Mark Steyvers, David M. Blei,
and Joshua B. Tenenbaum. 2005. Integrating topics
and syntax. In In Advances in Neural Information
Processing Systems 17, pages 537–544. MIT Press.

Thomas K Landauer and Susan T. Dumais. 1997.
A solution to platos problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211–240.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, ACL, Volume 2: Short Papers,
pages 302–308, Baltimore, MD, USA.

Dekang Lin. 1998. An information-theoretic def-
inition of similarity. In Proceedings of the Fif-
teenth International Conference on Machine Learn-
ing, ICML ’98, pages 296–304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Com-
put. Linguist., 19(2):313–330, June.

David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Main Conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, HLT-NAACL ’06, pages 152–159, New
York, New York. Association for Computational
Linguistics.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.

Toben H. Mintz. 2003. Frequent frames as a cue
for grammatical categories in child directed speech.
Cognition., 90(1):91–117.

Jeff Mitchell and Mark Steedman. 2015. Orthogo-
nality of syntax and semantics within distributional
spaces. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1301–1310, Beijing, China, July. Association
for Computational Linguistics.

Jeff Mitchell. 2013. Learning semantic representa-
tions in a bigram language model. In Proceedings
of the 10th International Conference on Computa-
tional Semantics (IWCS 2013) – Short Papers, pages
362–368, Potsdam, Germany, March. Association
for Computational Linguistics.

Sebastian Padó and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Comput. Linguist., 33(2):161–199,
June.

Martin Redington, Nick Chater, and Steven Finch.
1998. Distributional information: A powerful cue
for acquiring syntactic categories. Cognitive Sci-
ence, 22(4):425–469.

135



Marek Rei and Ted Briscoe. 2013. Parser lexicali-
sation through self-learning. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 391–400, At-
lanta, Georgia, June. Association for Computational
Linguistics.

John R. Ross. 1972. The category squish: End-
station Hauptwort. In Papers from the Eighth Re-
gional Meeting, pages 316–328, Chicago. Chicago
Linguistic Society.

Richard Socher, Christopher D. Manning, and An-
drew Y. Ng. 2010. Learning Continuous Phrase
Representations and Syntactic Parsing with Recur-
sive Neural Networks. In Proceedings of the Deep
Learning and Unsupervised Feature Learning Work-
shop of NIPS 2010, pages 1–9.

Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
455–465, Sofia, Bulgaria, August. Association for
Computational Linguistics.

Yuta Tsuboi. 2014. Neural networks leverage corpus-
wide information for part-of-speech tagging. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 938–950, Doha, Qatar, October. Association
for Computational Linguistics.

136


