



















































Robust parfda Statistical Machine Translation Results


Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 345–354
Belgium, Brussels, October 31 - Novermber 1, 2018. c©2018 Association for Computational Linguistics

https://doi.org/10.18653/v1/W18-64032

Robust parfda Statistical Machine Translation Results

Ergun Biçici
ergun.bicici@boun.edu.tr

Department of Computer Engineering, Boğaziçi University
orcid.org/0000-0002-2293-2031

bicici.github.com

Abstract

We build parallel feature decay algorithms
(parfda) Moses statistical machine transla-
tion (SMT) models for language pairs in the
translation task. parfda obtains results close
to the top constrained phrase-based SMT with
an average of 2.252 BLEU points difference
on WMT 2017 datasets using significantly less
computation for building SMT systems than
that would be spent using all available cor-
pora. We obtain BLEU upper bounds based
on target coverage to identify which systems
used additional data. We use PRO for tuning
to decrease fluctuations in the results and post-
process translation outputs to decrease trans-
lation errors due to the casing of words. F1
scores on the key phrases of the English to
Turkish testsuite that we prepared reveal that
parfda achieves 2nd best results. Truecas-
ing translations before scoring obtained the
best results overall.

1 Introduction

Statistical machine translation is widely prone
to errors in text including encoding, tokeniza-
tion, morphological variations and the mass they
take, the size of the training and language model
datasets used, and model errors. parfda is
an instance selection tool based on feature de-
cay algorithms (Biçici and Yuret, 2015) we use
to select training and language model instances to
build Moses phrase-based SMT systems to trans-
late the test sets in the news translation task at
WMT18 (WMT, 2018). As we work towards tools
that can be used for multiple languages at the same
time, we aim to obtain robust results for compar-
ison and record the statistics of the data and the
resources used. Our contributions are:

• a test suite for machine translation that is out
of the domain of news task to take the chance
of taking a closer look at the current status of

SMT technology used by the task participants
when translating 10 sentences taken from lit-
erary context in Turkish, which shows that
parfda phrase-based SMT can obtain 2nd
best results on this test set,

• parfda results for language pairs in the
translation task and data statistics,

• comparison of processing alternatives for
translation outputs to obtain better results,

• upperbounds on the translation performance
using lowercased coverage to identify which
models used data in addition to the parallel
corpus,

• a set of rules that fix tokenization errors in
Turkish using Moses’ (Koehn et al., 2007) to-
kenization scripts.

We obtain parfda Moses phrase-based
SMT (Koehn et al., 2007) results for the language
pairs in both directions in the WMT18 news trans-
lation task, which include English-Czech (en-cs),
English-Estonian (en-et), English-German (en-
de), English-Finnish (en-fi), English-Russian
(en-ru), and English-Turkish (en-tr). Building a
language independent system that can perform
well in translation tasks is a challenging task
and SMT systems participating at WMT18 have
been largely built dependent on the translation
direction.

2 parfda

Parallel feature decay algorithms (parfda) (Bi-
cici, 2016) parallelize feature decay algorithms
(FDA), a class of instance selection algorithms
that use feature decay, for fast deployment of ac-
curate SMT systems. We use parfda to select
parallel training data and language model (LM)

345

https://doi.org/10.18653/v1/W18-64032


Figure 1: parfda Moses SMT workflow.

data for building SMT systems. parfda runs
separate FDA5 (Biçici and Yuret, 2015) models on
randomized subsets of the available data and com-
bines the selections afterwards. Figure 1 depicts
parfda Moses SMT workflow. The approach
also obtained improvements using NMT (Ponce-
las et al., 2018).

We obtain transductive learning results since
we use source sentences of the test set to select
data. However, decaying only on the source test
set features does not necessarily increase diversity
on the target side thus we also decay on the tar-
get features that we already select. With the new
parfda model, we select about 1.7 million in-
stances for training data and about 15 million sen-
tences for each LM data not including the selected
training set, which is added later. Table 1 shows
size differences with the constrained dataset (C).
We use 3-grams to select training data and 2-grams
for LM data. TCOV lists the target coverage in
terms of the 2-grams of the test set. We also use
CzEng17 (Bojar et al., 2016) for en-cs and SE-
TIMES2 (Tiedemann, 2009) for en-tr.

We set the maximum sentence length to 126 and
train 6-gram LM using kenlm (Heafield et al.,
2013). For increasing the robustness of the op-
timization results, we use PRO (Section 2.1) and
we use varying n-best list size. For word align-
ment, we use mgiza (Gao and Vogel, 2008) where
GIZA++ (Och and Ney, 2003) parameters set

max-fertility to 10, the number of iterations to
7,3,5,5,7 for IBM models 1,2,3,4, and the HMM
model, and learn 50 word classes in three itera-
tions with the mkcls tool during training. The de-
velopment set contains up to 4000 sentences ran-
domly sampled from previous years’ development
sets (2011-2017) and remaining come from the de-
velopment set for WMT18. Table 2 lists the cov-
erage of the test set.

2.1 Robust Optimization Results with PRO

Pairwise ranking optimization (PRO) (Hopkins
and May, 2011) is found to obtain scores that
monotonically increase, with results that are at
least as good as MERT (Och, 2003), and with a
standard deviation that is three times lower than
MERT. We use PRO for tuning to obtain ro-
bust results due to fluctuating scores with MERT.
PRO tuning performance graph is compared with
MERT performance plot in Figure 2. We used
monotonically increasing n-best list size at the
start to increase robustness by using multiples of
50 until the 8th iteration, 350 every 10th, and 150
in the remaining. We only need 4 iterations to find
parameters whose tuning score reach 1% close to
the best tuning parameter set score (Figure 3).

2.2 Testsuite for en-tr and tr-en

We prepared an SMT test suite that is out of the
domain of news translation task to take a closer

346



Figure 2: Comparison of MERT and PRO tuning on en-tr using results from 2017 and 2018 respectively.

10 2 10 1 100 101
 %

0

5

10

15

20

25

30

ite
ra

tio
n

11

33

1212

31

1

3
44

131313

1

44

7

21

2323

11

55

888

11

66666

1
22

3
44

6

PRO iterations required for convergence
en-cs
en-de
en-et
en-fi
en-ru
en-tr

10 2 10 1 100 101
 %

0

5

10

15

20

25

30

ite
ra

tio
n

11
3

4

111111

11
33

10
12

26

1
222222

1
222

23
24

28

22
4

9

323232

33
55

15

2525

PRO iterations required for convergence
cs-en
de-en
et-en
fi-en
ru-en
tr-en

Figure 3: The number of iterations PRO would need to reach ∆% close to the best tuning score.

look at the current status of SMT technology used
by the task participants to translate 10 sentences
taken from literary context in Turkish. The sen-
tences and their translations are provided in Ap-
pendix A.

Table 3 details the testsuite results on en-tr and
tr-en where the best translations of parfda are
selected based on their BLEU (Papineni et al.,
2002) and F1 (Biçici, 2011) scores:

en-tr lctc 1 align
en-tr ts lctc 1 align
tr-en tc 2 align
tr-en ts tc 1 align

where tc and lctc are defined in Section 2.3.
We count tokens of translation as non-

translation when they are found in the test source,
are not a number or punctuation, and are consid-
ered by the SMT model’s phrase table or the lex-
ical translation table as a token whose translation
differs from the source token. We have access to
the lexical tables of parfda SMT models and

among the tr-en lctc entries (Table 4), 2.7% con-
tain a translation the same as the source. Accord-
ing to the testsuite results using translations from
task participants, only RWTH and parfda con-
tained non-translations and RWTH had only a to-
ken non-translated. The scores for up to n-grams
in Table 12 show that alibaba.5744 achieves the
best results in en-tr and online-B achieves the best
results in tr-en in all scores. When we look at some
of the OOV tokens in en-tr, we observe that low-
ercasing and then truecasing might help.

We identified 5 key phrases for both en-tr and
tr-en that we would like to see translated cor-
rectly (Table 5). Some are trimmed to make them
closer to their root form so that suffixes can be
added without decreasing identification rates. Ap-
pendix A presents F1 scores based on the identi-
fication of them in the translations. We see that
even though parfda achieves the lowest scores
in BLEU, on the key phrases, it provides the 2rd

347



S → T Training Data LM DataData #word S (M) #word T (M) #sent (K) TCOV #word (M) TCOV
en-cs C 618.2 548.9 43274 0.749 1357.0 0.857
en-cs parfda 83.6 73.9 1777 0.663 416.2 0.809
cs-en C 548.9 618.2 43274 0.855 11100.4 0.948
cs-en parfda 78.1 88.1 1773 0.8 502.2 0.896
en-de C 580.2 548.0 24914 0.791 3078.9 0.892
en-de parfda 96.6 91.4 1776 0.74 467.3 0.839
de-en C 548.0 580.2 24914 0.859 11100.4 0.941
de-en parfda 90.4 94.9 1776 0.82 509.1 0.893
en-et C 20.4 27.0 1073 0.422 1197.1 0.772
en-et parfda 27.0 20.4 1072 0.422 375.8 0.654
et-en C 27.0 20.4 1073 0.673 11100.4 0.943
et-en parfda 20.4 27.0 1072 0.673 416.4 0.883
en-fi C 52.3 72.3 2846 0.45 1536.4 0.743
en-fi parfda 53.4 38.6 1598 0.438 468.0 0.676
fi-en C 72.3 52.3 2846 0.725 11100.4 0.943
fi-en parfda 37.2 50.8 1550 0.715 473.2 0.888
en-ru C 172.6 202.3 8766 0.739 9643.5 0.916
en-ru parfda 69.3 53.0 1712 0.684 561.3 0.83
ru-en C 202.3 172.6 8766 0.844 11100.4 0.95
ru-en parfda 61.7 71.6 1764 0.816 489.6 0.903
en-tr C 4.6 5.1 208 0.352 4026.5 0.824
en-tr parfda 5.1 4.6 207 0.352 474.5 0.72
tr-en C 5.1 4.6 208 0.569 11100.4 0.936
tr-en parfda 4.6 5.1 207 0.569 442.1 0.877

Table 1: Statistics for the training and LM corpora in the constrained (C) setting compared with the parfda
selected data. #words is in millions (M) and #sents in thousands (K). TCOV is target 2-gram coverage.

SCOV TCOV
1 2 3 4 5 1 2 3 4 5

en-cs 0.9635 0.8612 0.6212 0.3305 0.1334 0.9718 0.7587 0.4042 0.165 0.0553
en-de 0.9657 0.8646 0.627 0.3261 0.1253 0.9362 0.7939 0.5254 0.2534 0.0904
en-et 0.8912 0.6821 0.3717 0.1345 0.0376 0.8095 0.4308 0.1664 0.0528 0.0136
en-fi 0.9135 0.7339 0.4477 0.1889 0.0619 0.834 0.4595 0.1895 0.0612 0.0165
en-ru 0.9682 0.8683 0.6444 0.3581 0.1567 0.9703 0.78 0.4572 0.2076 0.0812
en-tr 0.8286 0.5817 0.2807 0.0905 0.0226 0.7944 0.3613 0.12 0.0282 0.006

Table 2: Test set SCOV and TCOV for n-grams.

best in en-tr among 9 models and 4th best among
6 in tr-en. Key phrase identification is important
since when scores are averaged, important phrases
that are missing only decrease the score by 1|p|N|p|
for BLEU calculation for a phrase of length |p|
over N|p| phrases with length |p|.

2.3 Comparing Text Processing Settings for
SMT

Experiment management system (EMS) (Koehn,
2010) of Moses prepares translations as follows:

truecase input
→ translate input

→ clean output (XML tags)
→ detruecase output

Truecasing updates the casing of words accord-
ing to the most common form observed in the
whole training corpus. EMS does not truecase the
translations of an SMT model when training data
are already truecased. However, each casing of

words are a different entry in the phrase table and
the casing we are interested in might be missing in
the translations. Therefore, truecasing (tc) before
detruecasing makes sense.

The casing of the text affects the number of to-
kens in the data sets. A casing of a token might
appear in the phrase table but not its lowercased
(lc) version. In EMS, truecasing is applied on
the input. We experiment with truecasing lower-
cased text (lctc) to decrease the number of out-
of-vocabulary words in the translations and to re-
duce the number of unique n-grams, dataset sizes,
and the binary LM size by about 2%.

We process tokenized Turkish text using a set
of rules since Moses’ (Koehn et al., 2007) tok-
enization scripts can encounter tokenization errors
in Turkish. A simpler approach was also tried
for fixing tokenization of Turkish by removing
space for unbalanced single quotes (Ding et al.,
2016). Additionally, we retain the casing of the

348



Figure 4: SMT text processing comparison of truecasing after lowercasing (lctc) and truecasing (tc).

testset setting % sent. OK non-trans. non-trans. % not in PT/lex oov % oov BLEU F1

en-tr tc 1 align 60.9 1941 18.81 22 1324 12.8 0.0746 0.1302
lctc 1 align 61.1 1933 18.74 0 1290 12.5 0.0883 0.1406

en-tr ts lctc 1 align 60.0 5 3.4 0 4 2.7 0.051 0.105

tr-en tc 2 align 38.9 3549 20.7 23 2643 15.4 0.1192 0.1708
lctc 2 align 44.8 3028 17.6 12 2132 12.4 0.1055 0.1567

tr-en ts tc 1 align 20.0 32 17.7 0 24 13.3 0.0 0.1011

Table 3: Best performing en-tr and tr-en translation results detailed with their types of errors.

setting N (lexical) % (wS == wT )

en-tr
tc 928K 2.91
lctc 890K 2.68

tr-en
tc 891K 3.02
lctc 860K 2.74

Table 4: Lexical translation table comparison.

phrase count

en
-t

r

Türk Dil Kurumu 6
Türkçe Sözlü 4
Yazım Kılavuzu Çalışma Grubu 3
Yazım Kılavuzu 7
yazım kural 4

tr-
en

Turkish Language Institution 6
Turkish Language 6
Turkish Dictionary 4
Working Group 3
Writing Manual 4

Table 5: Key phrases we look for in the translations.

test source sentences using the word alignment in-
formation (Ding et al., 2016). Using alignment in-
formation is more complicated since not all align-
ments are 1-to-1. We also experiment with finding
the casing of the input words in the development
and test sets according to the form found in the
translation tables to replace them before decoding.
Figure 4 compares tc and lctc approaches to
text processing for SMT. Both can use the align-
ment information for casing words.

Table 6 compares the results using transla-
tions that contain the alignment information and
the unknown words where tc 0 is the base-
line. The additional Moses decoder parameter is
--print-alignment-info. We obtain the
highest en-tr score using the alignments for cas-
ing but scores decrease for en-de and de-en. For
which translation directions it helps can be seen
in the lctc 0 row. The difference between the
base and the lowercased results are the gain we
can achieve if we fix casing accordingly. Using tc
translation as a start, the gain on average is about
1.1 BLEU points (0.011 BLEU). The best setting
overall is tc 2. The largest room for improvement
with lctc lc BLEU results are for cs-en and tr-
en.

349



BLEU cs-en de-en et-en fi-en ru-en tr-en en-cs en-de en-et en-fi en-ru en-tr sum

tc 0 base 0.2296 0.3332 0.1766 0.1536 0.247 0.1286 0.1567 0.2669 0.1182 0.1016 0.1934 0.0823 2.1877align 0.2134 0.1851 0.1684 0.1427 0.233 0.1269 0.1455 0.1637 0.1138 0.095 0.1821 0.083 1.8526

tc 1 0.2239 0.318 0.1718 0.1493 0.2464 0.1269 0.1506 0.2543 0.1117 0.0961 0.1918 0.0824 2.1232align 0.2127 0.1845 0.1682 0.1416 0.2329 0.1261 0.1455 0.1625 0.1137 0.0948 0.182 0.083 1.8475

tc 2 0.2295 0.3331 0.1765 0.1535 0.2526 0.1284 0.1566 0.2669 0.1182 0.1013 0.1989 0.0823 2.1978align 0.2136 0.1945 0.1681 0.1458 0.2347 0.1273 0.1469 0.1645 0.1151 0.0965 0.1827 0.0816 1.8713
lc 0.2394 0.3447 0.184 0.1614 0.2632 0.1373 0.1622 0.2727 0.1216 0.1053 0.2048 0.089 2.2856

lctc 0 0.1869 0.2736 0.1523 0.129 0.1901 0.1066 0.1339 0.1251 0.0975 0.0875 0.1671 0.0616 1.7112align 0.2113 0.2443 0.1661 0.1266 0.211 0.1043 0.1448 0.1603 0.1114 0.0941 0.1644 0.0882 1.8268

lctc 1 0.1817 0.2602 0.1484 0.1252 0.1901 0.1064 0.1294 0.1149 0.0934 0.0828 0.1656 0.0639 1.662align 0.2105 0.2437 0.1658 0.1247 0.211 0.1032 0.1447 0.1585 0.1113 0.094 0.1642 0.0887 1.8203

lctc 2 0.1896 0.275 0.1552 0.1303 0.1974 0.1083 0.1369 0.127 0.1005 0.0887 0.1712 0.0632 1.7433align 0.2121 0.2583 0.1663 0.1304 0.2125 0.1055 0.1468 0.1611 0.1125 0.0954 0.1648 0.0871 1.8528
lc 0.2445 0.3452 0.1871 0.1634 0.2506 0.1402 0.1651 0.2781 0.1212 0.1056 0.1803 0.0978 2.2791

Table 6: parfda tokenized and cased results with different text processing settings. Baseline is tc 0 (in italic).
bold lists the best for a translation direction.

BLEU cs-en de-en et-en fi-en ru-en tr-en en-cs en-de en-et en-fi en-ru en-tr
parfda 2018 0.2322 0.3343 0.1741 0.1547 0.2485 0.1267 0.1529 0.2674 0.1203 0.0968 0.1970 0.0821
parfda 2018 F1 0.2551 0.344 0.2123 0.1936 0.2685 0.1768 0.1921 0.2891 0.1613 0.1494 0.2214 0.1314
TopC NMT 2018 lc 0.348 0.499 0.315 0.258 0.358 0.291 0.266 0.489 0.258 0.192 0.348 0.207
TopC NMT 2018 0.339 0.484 0.307 0.249 0.349 0.28 0.26 0.483 0.252 0.182 0.348 0.20

- parfda 0.1068 0.1497 0.1329 0.0943 0.1005 0.1533 0.1071 0.2156 0.1317 0.0852 0.1510 0.1179
avg diff lc 0.1288

Table 7: parfda results compared with the top results in WMT18 and their difference.1

parfda results at WMT18 are in Table 7 us-
ing BLEU over tokenized text. We compare with
the top constrained submissions at WMT18 in
Table 7 and at WMT17 in Table 8. 2 Perfor-
mance compared with the top constrained (TopC)
phrase-based SMT improved to 2.252 in 2017
from 3 BLEU points difference on average com-
pared with WMT16 results, which is likely due to
the new parfda model and phrase-based SMT
being less common in 2017. parfdaMoses SMT
system can obtain 0.6 BLEU points close to the
top result in Finnish to English translation in 2017.
All top models use NMT in 2018 and most use
backtranslations, which means that their TCOV is
upper bounded by LM TCOV.

3 Translation Upper Bounds with TCOV

We obtain upper bounds on the translation perfor-
mance based on the target coverage (TCOV) of n-
grams of the test set found in the selected parfda
training data (Bicici, 2016) but using lowercased
text this time. For a given sentence T ′, the number
of OOV tokens are identified:

OOV r = round((1− TCOV) ∗ |T ′|) (1)
2Due to different tokenization rules used by

mteval-v14.pl in matrix.statmt.org, parfda
BLEU scores are higher than the scores in Table 6.

where |T ′| is the number of tokens in the sentence.
We obtain each bound using 500 such instances
and repeat for 10 times. TCOV BLEU bound
is optimistic since it does not consider reorder-
ings in the translation or differences in sentence
length. Each plot in Table 9 locates TCOV BLEU
bound obtained from each n-gram and from n-
gram TCOVS combined up to and including n
and � locates the parfda result and F locates
the top constrained result. In en-de and en-tr, the
top model achieves a higher score than the TCOV
BLEU bound, which indicates that data additional
to the constrained training data was used. In both,
backtranslations were used.

4 Conclusion

We use parfda for selecting instances for build-
ing SMT systems using less computation overall
and results at WMT18 provides new data about
using the current phrase-based SMT technology
towards rapid SMT system development. Our
data processing experiments show that lowercas-
ing and then truecasing data can improve SMT
models and translation results provided that we
can find the casing correctly and truecasing trans-
lations before scoring can improve the results. Our

2We use the results from matrix.statmt.org.

350



BLEU cs-en de-en fi-en lv-en ru-en tr-en en-cs en-de en-fi en-lv en-ru en-tr
parfda 2017 0.2276 0.2613 0.1987 0.1549 0.2812 0.1172 0.1381 0.1851 0.1282 0.1303 0.218 0.097
TopC NMT 2017 0.309 0.351 0.19 0.308 0.179 0.228 0.283 0.207 0.183 0.298 0.165

- parfda 0.0814 0.0897 0.0351 0.0268 0.0618 0.0899 0.0979 0.0788 0.0527 0.0800 0.0680
avg diff 0.0693
TopC phrase 2017 0.265 0.205 0.168 0.315 0.126 0.191 0.216 0.145 0.142 0.253 0.098

- parfda 0.0374 0.0063 0.0131 0.0338 0.0088 0.0529 0.0309 0.0168 0.0127 0.035 0.001
avg diff 0.02252

Table 8: parfda results compared with the top results in WMT17 and their difference.

Table 9: parfda results (�) and OOV r TCOV BLEU upper bounds for de and tr.

method of tuning with PRO provides robust re-
sults and the BLEU bounds we obtain show which
systems used additional training data. We are of-
ten interested to conserve the semantic content in
the translations and parfda Moses phrase-based
SMT achieves 2nd best results on the tr-en test-
suite in our evaluations with key phrases.

Acknowledgments

The research reported here received financial sup-
port in part from the Scientific and Technologi-
cal Research Council of Turkey (TÜBİTAK) with-
out contribution to the content nor responsibility
thereof. We also thank the reviewers’ comments.

References
2018. Proc. of the Third Conference on Machine

Translation. Association for Computational Lin-
guistics, Brussels, Belgium.

Ergun Biçici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koç University. Supervi-
sor: Deniz Yuret.

Ergun Biçici and Deniz Yuret. 2015. Optimizing in-
stance selection for statistical machine translation
with feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Processing
(TASLP), 23:339–350.

Ergun Bicici. 2016. ParFDA for instance selection
for statistical machine translation. In Proc. of the

351



First Conference on Statistical Machine Translation
(WMT16), pages 252–258, Berlin, Germany. Asso-
ciation for Computational Linguistics.

Ondřej Bojar, Ondřej Dušek, Tom Kocmi, Jindřich Li-
bovický, Michal Novák, Martin Popel, Roman Su-
darikov, and Dušan Variš. 2016. CzEng 1.6: En-
larged Czech-English Parallel Corpus with Process-
ing Tools Dockered. Springer International Publish-
ing, Cham.

Shuoyang Ding, Kevin Duh, Huda Khayrallah, Philipp
Koehn, and Matt Post. 2016. The jhu machine trans-
lation systems for wmt 2016. In Proceedings of
the First Conference on Machine Translation, pages
272–280, Berlin, Germany. Association for Compu-
tational Linguistics.

Qin Gao and Stephan Vogel. 2008. Software Engineer-
ing, Testing, and Quality Assurance for Natural Lan-
guage Processing, chapter Parallel Implementations
of Word Alignment Tool. Association for Computa-
tional Linguistics.

Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modified
Kneser-Ney language model estimation. In Proc. of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, pages 690–696, Sofia, Bul-
garia.

Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1352–1362. Association for Compu-
tational Linguistics.

Philipp Koehn. 2010. An experimental management
system. The Prague Bulletin of Mathematical Lin-
guistics, 94:87–96.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the Association
for Computational Linguistics Companion Volume
Proc. of the Demo and Poster Sessions, pages 177–
180. Association for Computational Linguistics.

Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. Association for
Computational Linguistics, 1:160–167.

Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of 40th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 311–318, Philadelphia, PA,
USA.

Alberto Poncelas, Gideon Maillette de Buy Wenniger,
and Andy Way. 2018. Feature decay algorithms for
neural machine translation. In Proc. of the 21th
Conference of the European Association for Ma-
chine Translation (EAMT), Spain.

Jorg Tiedemann. 2009. News from OPUS - A col-
lection of multilingual parallel corpora with tools
and interfaces. In N. Nicolov, K. Bontcheva,
G. Angelova, and R. Mitkov, editors, Recent
Advances in Natural Language Processing, vol-
ume V, pages 237–248. John Benjamins, Amster-
dam/Philadelphia, Borovets, Bulgaria.

352



A en-tr and tr-en Testsuite Sentences
E

ng
lis

h
1

T
he

Tu
rk

is
h

L
an

gu
ag

e
In

st
itu

tio
n’

s
Tu

rk
is

h
D

ic
tio

na
ry

an
d

W
ri

tin
g

M
an

ua
lW

or
ki

ng
G

ro
up

ha
s

re
ac

he
d

th
e

tw
en

ty
-s

ev
en

th
ed

iti
on

of
th

e
W

ri
tte

n
M

an
ua

l.
2

To
sa

y
up

fr
on

t;
to

m
ak

e
th

e
w

ri
tin

g
ru

le
s

la
st

in
g

an
d

ro
ot

ed
to

re
ac

h
un

ity
in

w
ri

tin
g,

no
ch

an
ge

s
ha

ve
be

en
m

ad
e

in
th

e
es

ta
bl

is
he

d
ru

le
s

in
th

is
ed

iti
on

as
w

el
l.

3
Tu

rk
is

h
L

an
gu

ag
e

In
st

itu
tio

n’
s

pr
in

ci
pl

es
of

ta
ki

ng
th

e
m

an
ua

lp
ub

lis
he

d
in

19
96

as
th

e
ba

si
s

fo
rt

ra
di

tio
na

lw
ri

tin
g

st
ru

ct
ur

es
an

d
th

e
pr

in
ci

pl
e

of
ta

ki
ng

th
e

m
id

dl
e

pa
th

to
en

d
di

sc
us

si
on

s
in

w
ri

tin
g

ar
e

al
so

fo
llo

w
ed

in
th

is
ed

iti
on

.
4

T
hi

s
ed

iti
on

of
th

e
W

ri
tin

g
M

an
ua

li
s

pr
ep

ar
ed

in
pa

ra
lle

lw
ith

th
e

la
st

ed
iti

on
of

th
e

Tu
rk

is
h

D
ic

tio
na

ry
an

d
ne

w
is

su
es

th
at

em
er

ge
w

ith
th

e
us

e
of

la
ng

ua
ge

ar
e

at
ta

ch
ed

to
ru

le
s

an
d

is
su

es
th

at
w

er
e

no
ta

dd
re

ss
ed

in
pr

ev
io

us
ed

iti
on

s
ar

e
co

nv
er

te
d

in
to

ru
le

s.
5

Tu
rk

is
h

D
ic

tio
na

ry
an

d
W

ri
tin

g
M

an
ua

lW
or

ki
ng

G
ro

up
re

vi
ew

s
hu

nd
re

ds
of

ex
am

pl
es

ab
ou

te
ve

ry
ru

le
an

d
co

ns
id

er
s

us
ag

e
st

at
is

tic
s

w
ith

th
e

pr
in

ci
pl

e
of

vi
ew

in
g

w
ri

tin
g

as
ha

bi
ta

nd
cu

st
om

.
6

A
ft

er
us

in
g

L
at

in
al

ph
ab

et
fo

rm
or

e
th

an
ei

gh
ty

ye
ar

s
w

e
ca

n
sa

y
th

at
Tu

rk
is

h
w

ri
tin

g
ha

s
be

en
tr

ad
iti

on
al

iz
ed

ag
ai

ns
ts

om
e

L
at

in
so

ur
ce

d
pr

ob
le

m
s.

7
W

ith
ou

ta
do

ub
t,

Tu
rk

is
h

L
an

gu
ag

e
In

st
itu

tio
n’

s
w

or
k

fo
r8

0
ye

ar
s

ha
s

pl
ay

ed
an

im
po

rt
an

tr
ol

e
in

th
is

.
8

T
he

ta
sk

of
pr

ep
ar

in
g,

w
ri

tin
g,

an
d

di
st

ri
bu

tin
g

w
ri

tin
g

m
an

ua
li

s
gi

ve
n

to
Tu

rk
is

h
L

an
gu

ag
e

In
st

itu
tio

n
ac

co
rd

in
g

to
th

e
ç

su
bi

te
m

of
th

e
10

th
ite

m
in

66
4

nu
m

be
re

d
de

cr
ee

la
w

ba
se

d
on

th
e

13
4t

h
ite

m
in

th
e

co
ns

tit
ut

io
n.

9
Si

nc
e

its
es

ta
bl

is
hm

en
t,

Tu
rk

is
h

L
an

gu
ag

e
In

st
itu

tio
n

ha
s

be
en

tr
yi

ng
to

fu
lfi

ll
its

du
ty

on
de

te
rm

in
in

g
w

ri
tin

g
ru

le
s

an
d

pu
bl

ic
at

io
n

of
w

ri
tin

g
m

an
ua

ls
.

10
Tu

rk
is

h
L

an
gu

ag
e

In
st

itu
tio

n
Tu

rk
is

h
D

ic
tio

na
ry

an
d

W
ri

tin
g

M
an

ua
lW

or
ki

ng
G

ro
up

ha
s

di
lig

en
tly

w
or

ke
d

on
ea

ch
w

ri
tin

g
ru

le
an

d
w

ri
tin

g
to

en
d

di
sc

us
si

on
s

in
w

ri
tin

g
an

d
to

sp
re

ad
w

ri
tin

g
ru

le
s

an
d

st
yl

es
th

at
ev

er
yb

od
y

w
ill

ac
ce

pt
an

d
us

e.

Tu
rk

is
h

1
T

ür
k

D
il

K
ur

um
u

G
ün

ce
lT

ür
kç

e
Sö

zl
ük

ve
Y

az
ım

K
ıla

vu
zu

Ç
al

ış
m

a
G

ru
bu

nu
n

ya
pt

ığ
ıç

al
ış

m
al

ar
la

Y
az

ım
K

ıla
vu

zu
yi

rm
iy

ed
in

ci
ba

sk
ıs

ın
a

ul
aş

m
ış

bu
lu

nu
yo

r.
2

H
em

en
be

lir
te

lim
;y

az
ım

ku
ra

lla
rı

nı
ka

lıc
ıh

âl
e

ge
tir

m
ek

,k
ök

le
şt

ir
m

ek
,b

öy
le

ce
ya

zı
m

da
bi

rl
iğ

is
ağ

la
m

ak
am

ac
ıy

la
bu

ba
sk

ıd
a

da
ye

rl
eş

m
iş

ku
ra

lla
rl

a
ilg

ili
ol

ar
ak

he
rh

an
gi

bi
rd

eğ
iş

ik
liğ

e
gi

di
lm

em
iş

tir
.

3
T

ür
k

D
il

K
ur

um
un

un
19

96
yı

lın
da

ya
yı

m
la

dı
ğı

kı
la

vu
zd

a
ya

zı
m

ıg
el

en
ek

le
şm

iş
bi

çi
m

le
ri

es
as

al
m

a,
ya

zı
m

da
ki

ta
rt

ış
m

al
ar

a
so

n
ve

rm
e

am
ac

ıy
la

ya
zı

m
da

or
ta

yo
lu

n
tu

ttu
ru

lm
as

ıi
lk

es
ib

u
ba

sk
ıd

a
da

gö
ze

til
m

iş
tir

.
4

Y
az

ım
K

ıla
vu

zu
’n

un
bu

ba
sk

ıs
ıT

ür
kç

e
Sö

zl
ük

’ü
n

so
n

ba
sk

ıs
ıy

la
eş

gü
dü

m
iç

er
is

in
de

ha
zı

rl
an

m
ış

,d
ild

e
ya

şa
na

n
ge

liş
m

el
er

so
nu

cu
nd

a
or

ta
ya

çı
ka

n
ya

zı
m

la
ilg

ili
ye

ni
so

ru
nl

ar
bi

rk
ur

al
a

ba
ğl

an
m

ış
,ö

nc
ek

ib
as

kı
la

rd
a

de
ği

ni
le

m
ey

en
ko

nu
la

ry
az

ım
ku

ra
lı

hâ
lin

e
ge

tir
ilm

iş
tir

.
5

G
ün

ce
lT

ür
kç

e
Sö

zl
ük

ve
Y

az
ım

K
ıla

vu
zu

Ç
al

ış
m

a
G

ru
bu

,h
er

ku
ra

lla
ilg

ili
yü

zl
er

ce
ör

ne
ği

gö
zd

en
ge

çi
ri

rk
en

ya
zı

m
ın

bi
ra

lış
ka

nl
ık

ve
ge

le
ne

k
ol

du
ğu

ilk
es

iy
le

ku
lla

nı
m

sı
kl

ık
la

rı
nı

gö
z

ön
ün

de
bu

lu
nd

ur
m

uş
tu

r.
6

Se
ks

en
yı

lı
aş

kı
n

bi
rs

ür
ed

ir
ku

lla
nm

ak
ta

ol
du

ğu
m

uz
L

at
in

ka
yn

ak
lı

T
ür

k
ya

zı
sı

yl
a

ki
m

is
or

un
la

ra
ka

rş
ın

ar
tık

ya
zı

m
ın

ge
le

ne
kl

eş
tiğ

in
i

sö
yl

ey
eb

ili
ri

z.
7

B
un

da
hi

ç
ku

şk
us

uz
,T

ür
k

D
il

K
ur

um
un

un
se

ks
en

yı
la

ul
aş

an
ça

lış
m

al
ar

ıö
ne

m
li

bi
rr

ol
oy

na
m

ak
ta

dı
r.

8
Ü

lk
em

iz
de

ya
zı

m
kı

la
vu

zu
ha

zı
rl

am
ak

,y
az

m
ak

ve
ya

yı
m

la
m

ak
gö

re
vi

,A
na

ya
sa

’n
ın

13
4.

m
ad

de
si

ne
da

ya
lı

ol
ar

ak
çı

ka
rı

la
n

66
4

sa
yı

lı
K

an
un

H
ük

m
ün

de
K

ar
ar

na
m

e’
ni

n
10

.m
ad

de
si

ni
n

ç
fık

ra
sı

yl
a

T
ür

k
D

il
K

ur
um

un
a

ve
ri

lm
iş

tir
.

9
T

ür
k

D
il

K
ur

um
u,

ku
ru

lu
şu

nd
an

bu
ya

na
ya

zı
m

ku
ra

lla
rı

nı
n

be
lir

le
nm

es
in

de
ve

ya
zı

m
kı

la
vu

zl
ar

ın
ın

ya
yı

m
la

nm
as

ın
da

ke
nd

is
in

e
dü

şe
n

gö
re

vi
ye

ri
ne

ge
tir

m
ey

e
ça

lış
m

ak
ta

dı
r.

10
T

ür
k

D
il

K
ur

um
u

G
ün

ce
lT

ür
kç

e
Sö

zl
ük

ve
Y

az
ım

K
ıla

vu
zu

Ç
al

ış
m

a
G

ru
bu

,y
az

ım
da

ya
şa

na
n

ta
rt

ış
m

al
ar

ıs
on

a
er

di
rm

ek
ve

he
rk

es
in

be
ni

m
se

ye
ce

ği
,k

ul
la

na
ca

ğı
ya

zı
m

ku
ra

lla
rı

nı
ve

ya
zı

m
bi

çi
m

le
ri

ni
ya

yg
ın

la
şt

ır
m

ak
ilk

es
iy

le
he

rk
ur

al
ın

,h
er

ya
zı

lış
ın

üz
er

in
de

tit
iz

lik
le

du
rm

uş
tu

r.

Table 10: Testsuite English and Turkish for en-tr / tr-en.

353



en-tr

alibaba.5732

Türk Dil Kurumu 1.0 6 6
Türkçe Sözlü 0.86 3 4
Yazım Kılavuzu Çalışma Grubu 1.0 3 3
Yazım Kılavuzu 0.6 3 7
yazım kural 1.0 4 4
F1 0.88 19 24

alibaba.5744

Türk Dil Kurumu 1.0 6 6
Türkçe Sözlü 0.86 3 4
Yazım Kılavuzu Çalışma Grubu 1.0 3 3
Yazım Kılavuzu 0.6 3 7
yazım kural 1.0 4 4
F1 0.88 19 24

parfda
Türk Dil Kurumu 1.0 6 6
Türkçe Sözlü 1.0 4 4
F1 0.59 10 24

uedin.5644
Türk Dil Kurumu 1.0 6 6
yazım kural 0.86 3 4
F1 0.54 9 24

online-B

Türk Dil Kurumu 0.91 5 6
Türkçe Sözlü 0.86 3 4
yazım kural 0.4 1 4
F1 0.54 9 24

NICT.5695
Türk Dil Kurumu 1.0 6 6
yazım kural 0.67 2 4
F1 0.5 8 24

RWTH.5632 Türk Dil Kurumu 1.0 6 6
F1 0.4 6 24

online-A Türk Dil Kurumu 0.8 4 6
F1 0.29 4 24

online-G Türkçe Sözlü 0.86 3 4
F1 0.22 3 24

tr-en

online-B

Turkish Language Institution 0.5 2 6
Turkish Language 0.86 8 6
Turkish Dictionary 0.67 2 4
Working Group 1.0 3 3
Writing Manual 0.4 1 4
F1 0.82 16 23

NICT.5708

Turkish Language Institution 0.67 3 6
Turkish Language 1.0 6 6
Working Group 1.0 3 3
Writing Manual 0.67 2 4
F1 0.76 14 23

uedin.5709

Turkish Language Institution 0.67 3 6
Turkish Language 1.0 6 6
Working Group 1.0 3 3
F1 0.69 12 23

parfda

Turkish Language Institution 0.29 1 6
Turkish Language 0.8 4 6
Working Group 1.0 3 3
F1 0.52 8 23

online-G Turkish Dictionary 0.86 3 4
F1 0.23 3 23

online-A Turkish Language 0.29 1 6
F1 0.08 1 23

Table 11: Testsuite F1 scores with key phrases.

B
L

E
U

lc
B

L
E

U
F
1

lc
F
1

m
od

el
1

2
3

4
1

2
3

4
1

2
3

4
1

2
3

4

en-tr

al
ib

ab
a.

57
32

0.
45

91
0.

44
82

0.
31

77
0.

31
12

0.
22

87
0.

22
55

0.
17

01
0.

16
84

0.
35

97
0.

35
37

0.
28

27
0.

27
89

0.
23

23
0.

22
94

0.
19

81
0.

19
58

al
ib

ab
a.

57
44

0.
47

78
0.

46
69

0.
32

97
0.

32
31

0.
23

94
0.

23
62

0.
17

85
0.

17
68

0.
37

17
0.

36
58

0.
29

36
0.

28
98

0.
24

16
0.

23
88

0.
20

55
0.

20
33

on
lin

e-
A

0.
39

77
0.

36
12

0.
24

86
0.

20
27

0.
14

69
0.

10
9

0.
07

14
0.

05
71

0.
29

2
0.

25
32

0.
21

46
0.

18
29

0.
16

49
0.

14
1

0.
13

5
0.

11
56

on
lin

e-
B

0.
44

12
0.

42
3

0.
30

05
0.

28
31

0.
20

45
0.

18
5

0.
12

96
0.

11
18

0.
34

19
0.

31
96

0.
26

31
0.

24
19

0.
20

83
0.

18
96

0.
17

11
0.

15
57

on
lin

e-
G

0.
41

05
0.

37
78

0.
24

93
0.

21
72

0.
15

44
0.

13
08

0.
09

34
0.

08
24

0.
30

99
0.

27
88

0.
23

13
0.

20
64

0.
18

17
0.

16
29

0.
15

0.
13

46
pa

rf
da

0.
33

67
0.

32
58

0.
18

18
0.

17
18

0.
10

55
0.

09
34

0.
05

58
0.

05
1

0.
24

17
0.

22
92

0.
17

75
0.

16
57

0.
13

75
0.

12
86

0.
11

23
0.

10
5

ue
di

n
0.

44
8

0.
43

34
0.

29
71

0.
28

08
0.

19
1

0.
17

08
0.

12
31

0.
10

07
0.

34
28

0.
32

55
0.

25
72

0.
24

15
0.

20
33

0.
18

9
0.

16
78

0.
15

51
N

IC
T

0.
43

76
0.

42
31

0.
28

51
0.

26
86

0.
19

47
0.

17
86

0.
13

2
0.

11
7

0.
34

04
0.

32
33

0.
26

2
0.

24
72

0.
21

01
0.

19
72

0.
17

41
0.

16
29

R
W

T
H

0.
37

52
0.

36
43

0.
20

65
0.

20
01

0.
11

9
0.

11
25

0.
06

76
0.

06
48

0.
25

07
0.

24
17

0.
18

03
0.

17
25

0.
13

95
0.

13
36

0.
11

42
0.

10
94

tr-en

pa
rf

da
0.

41
3

0.
40

06
0.

21
22

0.
19

63
0.

09
63

0.
08

61
0.

0
0.

0
0.

25
86

0.
24

04
0.

17
82

0.
16

56
0.

13
43

0.
12

49
0.

10
86

0.
10

11
on

lin
e-

A
0.

54
52

0.
48

77
0.

37
59

0.
30

56
0.

26
02

0.
19

88
0.

17
78

0.
13

69
0.

43
91

0.
35

73
0.

33
67

0.
26

61
0.

26
98

0.
21

3
0.

22
53

0.
17

82
on

lin
e-

B
0.

57
65

0.
56

56
0.

42
97

0.
41

8
0.

32
44

0.
31

52
0.

24
59

0.
23

9
0.

48
25

0.
47

32
0.

38
63

0.
37

86
0.

31
97

0.
31

35
0.

27
25

0.
26

79
on

lin
e-

G
0.

54
14

0.
49

72
0.

38
43

0.
33

62
0.

28
03

0.
23

64
0.

20
38

0.
16

8
0.

44
97

0.
39

29
0.

35
3

0.
30

24
0.

28
8

0.
24

41
0.

24
3

0.
20

45
ue

di
n

0.
53

33
0.

52
27

0.
36

06
0.

33
85

0.
25

07
0.

22
22

0.
17

38
0.

14
26

0.
42

4
0.

40
46

0.
32

53
0.

30
38

0.
26

03
0.

23
86

0.
21

6
0.

19
61

N
IC

T
0.

53
39

0.
52

6
0.

35
44

0.
34

78
0.

22
94

0.
22

44
0.

14
22

0.
13

99
0.

42
39

0.
41

99
0.

31
85

0.
31

6
0.

25
02

0.
24

88
0.

20
64

0.
20

55

Table 12: Testsuite BLEU and F1 results.

354


