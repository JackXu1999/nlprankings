











































Effective Adversarial Regularization for Neural Machine Translation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 204–210
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

204

Effective Adversarial Regularization for Neural Machine Translation

Motoki Sato
1
, Jun Suzuki

2,3
, Shun Kiyono

3,2

1Preferred Networks, Inc., 2Tohoku University,
3RIKEN Center for Advanced Intelligence Project

sato@preferred.jp, jun.suzuki@ecei.tohoku.ac.jp, shun.kiyono@riken.jp

Abstract

A regularization technique based on adversar-
ial perturbation, which was initially developed
in the field of image processing, has been suc-
cessfully applied to text classification tasks
and has yielded attractive improvements. We
aim to further leverage this promising method-
ology into more sophisticated and critical neu-
ral models in the natural language processing
field, i.e., neural machine translation (NMT)
models. However, it is not trivial to apply this
methodology to such models. Thus, this paper
investigates the effectiveness of several pos-
sible configurations of applying the adversar-
ial perturbation and reveals that the adversar-
ial regularization technique can significantly
and consistently improve the performance of
widely used NMT models, such as LSTM-
based and Transformer-based models.1

1 Introduction

The existence of (small) perturbations that in-
duce a critical prediction error in machine learn-
ing models was first discovered and discussed
in the field of image processing (Szegedy et al.,
2014). Such perturbed inputs are often referred
to as adversarial examples in the literature. Sub-
sequently, Goodfellow et al. (2015) proposed a
learning framework that simultaneously leverages
adversarial examples as additional training data
for reducing the prediction errors. This learning
framework is referred to as adversarial training.

In the field of natural language processing
(NLP), the input is a sequence of discrete symbols,
such as words or sentences. Since it is unreason-
able to add a small perturbation to the symbols, ap-
plying the idea of adversarial training to NLP tasks
has been recognized as a challenging problem. Re-
cently, Miyato et al. (2017) overcame this problem

1Our code for replicating the experiments in this paper is
available at the following URL: https://github.com/
pfnet-research/vat_nmt

Encoder Decoder

!"
#$"

!%
#$%

!&
#$&

'(
#$()

'"
#$")

'*
#$+)

," ,% ,+-"

Figure 1: An intuitive sketch that explains how we
add adversarial perturbations to a typical NMT model
structure for adversarial regularization. The definitions
of ei and fj can be found in Eq. 2. Moreover, those of
r̂i and r̂0j are in Eq. 8 and 13, respectively.

and reported excellent performance improvements
on multiple benchmark datasets of text classifica-
tion task. The key idea of their success is to apply
adversarial perturbations into the input embedding
layer instead of the inputs themselves as used in
image processing tasks. An important implication
of their study is that their method can be inter-
preted as a regularization method, and thus, they
do not focus on generating adversarial examples.
We refer to this regularization technique as adver-
sarial regularization.

We aim to further leverage this promising
methodology into more sophisticated and criti-
cal neural models, i.e., neural machine transla-
tion (NMT) models, since NMT models recently
play one of the central roles in the NLP research
community; NMT models have been widely uti-
lized for not only NMT but also many other NLP
tasks, such as text summarization (Rush et al.,
2015; Chopra et al., 2016), grammatical error cor-
rection (Ji et al., 2017), dialog generation (Shang
et al., 2015), and parsing (Vinyals et al., 2015;
Suzuki et al., 2018). Unfortunately, this appli-
cation is not fully trivial since we potentially
have several configurations for applying adversar-
ial perturbations into NMT models (see details in
Section 5). Figure 1 illustrates the model architec-
ture of NMT models with adversarial perturbation.

Therefore, the goal of this paper is to re-

https://github.com/pfnet-research/vat_nmt
https://github.com/pfnet-research/vat_nmt


205

veal the effectiveness of the adversarial regu-
larization in NMT models and encourage re-
searchers/developers to apply the adversarial reg-
ularization as a common technique for further
improving the performance of their NMT mod-
els. We investigate the effectiveness of several
possible configurations that can significantly and
consistently improve the performance of typical
baseline NMT models, such as LSTM-based and
Transformer-based models,

2 Related Work

Several studies have recently applied adversarial
training to NLP tasks, e.g., (Jia and Liang, 2017;
Belinkov and Bisk, 2018; Hosseini et al., 2017;
Samanta and Mehta, 2017; Miyato et al., 2017;
Sato et al., 2018). For example, Belinkov and Bisk
(2018); Hosseini et al. (2017) proposed methods
that generate input sentences with random char-
acter swaps. They utilized the generated (input)
sentences as additional training data. However,
the main focus of these methods is the incorpora-
tion of adversarial examples in the training phase,
which is orthogonal to our attention, adversarial
regularization, as described in Section 1.

Clark et al. (2018) used virtual adversarial train-
ing (VAT), which is a semi-supervised extension
of the adversarial regularization technique origi-
nally proposed in Miyato et al. (2016), in their
experiments to compare the results with those of
their proposed method. Therefore, the focus of
the neural models differs from this paper. Namely,
they focused on sequential labeling, whereas we
discuss NMT models.

In parallel to our work, Wang et al. (2019) also
investigated the effectiveness of the adversarial
regularization technique in neural language mod-
eling and NMT. They also demonstrated the im-
pacts of the adversarial regularization technique in
NMT models. We investigate the effectiveness of
the several practical configurations that have not
been examined in their paper, such as the combi-
nations with VAT and back-translation.

3 Neural Machine Translation Model

Model Definition In general, an NMT model
receives a sentence as input and returns a cor-
responding (translated) sentence as output. Let
Vs and Vt represent the vocabularies of the input
and output sentences, respectively. xi and yj de-
note the one-hot vectors of the i-th and j-th to-

kens in input and output sentences, respectively,
i.e. xi 2 {0, 1}|Vs| and yj 2 {0, 1}|Vt|. Here, we
introduce a short notation xi:j for representing a
sequence of vectors (xi, . . . ,xj). To explain the
NMT model concisely, we assume that its input
and output are both sequences of one-hot vectors
x1:I and y1:J that correspond to input and output
sentences whose lengths are I and J , respectively.
Thus, the NMT model approximates the following
conditional probability:

p(Y |X) =
YJ+1

j=1
p(yj |y0:j�1,X), (1)

where y0 and yJ+1 represent one-hot vectors of
special beginning-of-sentence (BOS) and end-of-
sentence (EOS) tokens, respectively, and X =
x1:I and Y = y1:J+1.

Let E 2 RD⇥|Vs| and F 2 RD⇥|Vt| be the
encoder and decoder embedding matrices, respec-
tively, where D is the dimension of the embedding
vectors. Thus, p(yj |y0:j�1,X) in Eq. 1 is calcu-
lated as follows:

p(yj |y0:j�1,X) = AttDec
�
fj ,h1:I

�
,

h1:I = Enc(e1:I),

fj = Fyj�1, ei = Exi, (2)

where Enc(·) and AttDec(·) represent functions
that abstract the entire encoder and decoder (with
an attention mechanism) procedures, respectively.

Training Phase Let D be the training data con-
sisting of a set of pairs of Xn and Yn, namely,
D = {(Xn,Yn)}Nn=1, where N represents the
amount of training data. For training, we generally
seek the optimal parameters ⇥̂ that can minimize
the following optimization problem:

⇥̂ = argmin
⇥

�
J (D,⇥)

 
, (3)

J (D,⇥) = � 1|D|
X

(X,Y )2D

`(X,Y ,⇥), (4)

`(X,Y ,⇥) = log
�
p(Y |X,⇥)

�
, (5)

where ⇥ represents a set of trainable parameters
in the NMT model.

Generation Phase We generally use a K-best
beam search to generate an output sentence with
the (approximated) K-highest probability given
input sentence X in the generation (test) phase.
We omit to explain this part in detail as our focus
is a regularization technique that is independent of
the generation phase.



206

4 Adversarial Regularization

This section briefly describes the adversarial reg-
ularization technique applied to the text classifica-
tion tasks proposed in Miyato et al. (2017). Let
r̂i 2 RD be an adversarial perturbation vector for
the i-th word in input X . The perturbed input em-
bedding e0i 2 RD is computed for each encoder
time-step i as follows:

e0i = Exi + r̂i. (6)

4.1 Adversarial Training (AdvT)

To obtain the worst case perturbations as an ad-
versarial perturbation in terms of minimizing the
log-likelihood of given X , we seek the optimal so-
lution r̂ by maximizing the following equation:

r̂ =argmax
r,||r||✏

n
`(X, r,Y ,⇥)

o
, (7)

where ✏ is a scalar hyper-parameter that con-
trols the norm of the perturbation, and r repre-
sents a concatenated vector of ri for all i. Here,
`(X, r,Y ,⇥) represents an extension of Eq. 5,
where the perturbation ri in r is applied to the po-
sition of r̂i as described in Eq. 6.

However, it is generally infeasible to exactly es-
timate r̂ in Eq. 7 for deep neural models. As a
solution, an approximation method was proposed
by Goodfellow et al. (2015), where `(X,Y , r,⇥)
is linearized around X . This approximation
method induces the following non-iterative solu-
tion for calculating r̂i for all encoder time-step i:

r̂i =✏
ai

||a||2
, ai = rei`(X,Y ,⇥). (8)

Thus, based on adversarial perturbation r̂, the loss
function can be defined as:

A(D,⇥) = � 1|D|
X

(X,Y )2D

`(X, r̂,Y ,⇥). (9)

Finally, we jointly minimize the objective func-
tions J (D,⇥) and A(D,⇥):

⇥̂ = argmin
⇥

n
J (D,⇥) + �A(D,⇥)

o
, (10)

where � is a scalar hyper-parameter that controls
the balance of the two loss functions.

4.2 Virtual Adversarial Training (VAT)

Miyato et al. (2016) proposed virtual adversar-
ial training, which is mainly used for the semi-
supervised extension of the adversarial regulariza-
tion technique. The difference appears in the loss
function ` in Eq. 7 and 9. Specifically, we can
use perturbations calculated based on the virtual
adversarial training by substituting ` with the fol-
lowing loss function:

`KL(X, r̂, ·,⇥) = KL
�
p(· |X,⇥)||p(· |X, r̂,⇥)

�
,

(11)

where KL(·||·) denotes the KL divergence.
It is worth noting here that, in our experiments,

we never applied the semi-supervised learning, but
used the above equation for calculating pertur-
bation as the replacement of standard adversarial
regularization. This means that the training data is
identical in both settings.

5 Adversarial Regularization in NMT

As strictly following the original definition of the
conventional adversarial training, the straightfor-
ward approach to applying the adversarial pertur-
bation is to add the perturbation into the encoder-
side embeddings ei as described in Eq. 6. How-
ever, NMT models generally have another embed-
ding layer in the decoder-side, as we explained in
Eq. 2. This fact immediately offers us also to con-
sider applying the adversarial perturbation into the
decoder-side embeddings fj .

For example, let r̂0j 2 RD be an adversarial per-
turbation vector for the j-th word in output Y . The
perturbed embedding f 0j 2 RD is computed for
each decoder time-step j as follows:

f 0j = Fyj�1 + r̂
0
j . (12)

Then similar to Eq. 8, we can calculate r̂0 as:

r̂0j =✏
bj

||b||2
, bj = rfj`(X,Y ,⇥), (13)

where b is a concatenated vector of bj for all j. In
addition, we need to slightly modify the definition
of r, which is originally the concatenation vector
of all ri for all i, to the concatenation vector of all
ri and r0j for all i and j.

Finally, we have three options for applying the
perturbation into typical NMT models, namely,
applying the perturbation into embeddings in the
(1) encoder-side only, (2) decoder-side only, and
(3) both encoder and decoder sides.



207

DE$EN FR$EN
training 189,318 208,323
test2012 (dev) 1,700 1,124
test2013 (test) 993 1,024
test2014 (test) 1,305 1,305

Table 1: Number of sentences in our datasets (Datasets
are cleaned from the original dataset).

Perturbation EN!DE
Model position test2013 test2014
LSTM (None) 27.73 23.98
+AdvT enc-emb 28.73 24.90

dec-emb 27.44 23.71
enc-dec-emb 28.47 24.78

+VAT enc-emb 29.03 24.75
dec-emb 27.49 23.20

enc-dec-emb 29.47 24.92
Transformer (None) 29.15 25.19
+AdvT enc-emb 29.04 25.16

dec-emb 28.95 25.75
enc-dec-emb 29.61 25.78

+VAT enc-emb 29.95 26.00
dec-emb 29.62 25.88

enc-dec-emb 30.13 26.06

Table 2: BLEU scores averaged over five models in var-
ious configurations of perturbation positions (enc-emb,
dec-emb, or enc-dec-emb) and adversarial regulariza-
tion techniques (AdvT or VAT).

6 Experiments

6.1 Datasets

We conducted experiments on the IWSLT evalua-
tion campaign dataset (Cettolo et al., 2012). We
used the IWSLT 2016 training set for training
models, 2012 test set (test2012) as the develop-
ment set, and 2013 and 2014 test sets (test2013
and test2014) as our test sets. Table 1 shows the
statistics of datasets used in our experiments.

For preprocessing of our experimental datasets,
we used the Moses tokenizer2 and the truecaser3.
We removed sentences over 50 words from the
training set. We also applied the byte-pair en-
coding (BPE) based subword splitting script4 with
16,000 merge operations (Sennrich et al., 2016b).

6.2 Model Configurations

We selected two widely used model archi-
tectures, namely, LSTM-based encoder-decoder

2
https://github.com/moses-smt/

mosesdecoder/blob/master/scripts/

tokenizer/tokenizer.perl

3
https://github.com/moses-smt/

mosesdecoder/blob/master/scripts/

recaser/truecase.perl

4
https://github.com/rsennrich/

subword-nmt

used in Luong et al. (2015) and self-attention-
based encoder-decoder, the so-called Trans-
former (Vaswani et al., 2017). We adapted the
hyper-parameters based on the several recent pre-
vious papers5.

Hereafter, we refer to the model trained with
the adversarial regularization (` in Eq. 7) as AdvT,
and similarly, with the virtual adversarial training
(`KL in Eq. 11) as VAT. We set � = 1 and ✏ = 1
for all AdvT and VAT experiments.

6.3 Results

Investigation of effective configuration Ta-
ble 2 shows the experimental results with config-
urations of perturbation positions (enc-emb, dec-
emb, or enc-dec-emb) and adversarial regulariza-
tion techniques (AdvT or VAT). As evaluation
metrics, we used BLEU scores (Papineni et al.,
2002)6. Note that all reported BLEU scores are
averaged over five models.

Firstly, in terms of the effective perturbation
position, enc-dec-emb configurations, which add
perturbations to both encoder and decoder embed-
dings, consistently outperformed other configura-
tions, which used either encoder or decoder only.
Moreover, we achieved better performance when
we added perturbation to the encoder-side (enc-
emb) rather than the decoder-side (dec-emb).

Furthermore, the results of VAT was consis-
tently better than those of AdvT. This tendency
was also observed in the results reported by Miy-
ato et al. (2016). As discussed in Kurakin et al.
(2017), AdvT generates the adversarial exam-
ples from correct examples, and thus, the models
trained by AdvT tend to overfit to training data
rather than those trained by VAT. They referred to
this phenomenon of AdvT as label leaking.

Results on four language pairs Table 3 shows
the BLEU scores of averaged over five models
on four different language pairs (directions),
namely German!English, French!English,
English!German, and English!French. Fur-
thermore, the row (b) shows the results obtained
when we incorporated pseudo-parallel corpora
generated using the back-translation method (Sen-
nrich et al., 2016a) as additional training data. For

5The detailed hyper-parameters are listed in Appendix A.
6We used the multi-bleu.perl script in the

Moses toolkit: https://github.com/moses-smt/
mosesdecoder/blob/master/scripts/

generic/multi-bleu.perl

https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/recaser/truecase.perl
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/recaser/truecase.perl
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/recaser/truecase.perl
https://github.com/rsennrich/subword-nmt
https://github.com/rsennrich/subword-nmt
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl


208

Perturbation
Model position

(a)

LSTM (None)
Transformer (None)

+VAT enc-dec-emb
+VAT+AdvT enc-dec-emb

(b) w/ BT Transformer enc-dec-emb+VAT enc-dec-emb
+VAT+AdvT enc-dec-emb

DE!EN
test2013 test2014

32.71 28.53
34.22 30.19
35.06 31.10
35.50 30.88
35.44 31.08
36.43 32.53
36.49 32.39

FR!EN
test2013 test2014

39.09 36.25
38.87 37.20
40.09 37.89
40.26 38.44

40.44 38.42
41.29 39.76
41.56 39.64

EN!DE
test2013 test2014

27.73 23.98
29.15 25.19
30.13 26.06
30.04 26.33
30.73 26.02
31.99 27.20

31.29 27.05

EN!FR
test2013 test2014

38.89 36.18
40.43 37.90
41.13 38.64
41.67 38.72

41.74 39.03
43.41 40.15

42.61 39.95

Table 3: BLEU scores averaged over five models in four different language pairs (directions). (b) Results with
using training data increased by back-translation method (BT).

Input meine gebildete Mutter aber wurde Lehrerin .
Reference but my educated mother became a teacher .
Baseline (Transformer) my educated mother , though , became a teacher

.

Proposed (Transformer+VAT w/ BT) but my educated mother became a teacher .
Input aber man kann sehen , wie die Menschen

miteinander kommunizieren , zu welchen Zeiten

sie einander anrufen , wann sie zu Bett gehen .

Reference but you can see how your people are
communicating with each other , what times they

call each other , when they go to bed .

Baseline (Transformer) but you can see how people talk to each other
about what time they call each other when they

go to bed .

Proposed (Transformer+VAT w/ BT) but you can see how people communicate with
each other , at which time they call each other

, when they go to bed .

Input wer im Saal hat ein Handy dabei ?
Reference who in the room has a mobile phone with you ?
Baseline (Transformer) who in the room has a cell phone in it ?
Proposed (Transformer+VAT w/ BT) who in the room has a cell phone with me ?

Table 4: Example translation from German!English (test2013).

generating the pseudo-parallel corpora, we used
the WMT14 news translation corpus.

We observe that Transformer+VAT consis-
tently outperformed the baseline Transformer
results in both standard (a) and back-translation
(b) settings. We report that VAT did not require
us to perform additional heavy hyper-parameter
search (excluding the hyper-parameter search in
base models). Therefore, we can expect that
VAT can improve the translation performance on
other datasets and settings with relatively high-
confidence.

In addition, the rows +VAT+AdvT show the
performance obtained by applying both AdvT and
VAT simultaneously. We can further improve the
performance in some cases, but the improvement
is not consistent among the datasets.

Actual Translation Examples Table 4 shows
actual translation examples generated by the mod-
els compared in our German!English translation
setting. We observe that Transformer+VAT
with using training data increased by the back-
translation method seems to generate higher qual-

ity translations compared with those of the base-
line Transformer.

7 Conclusion

This paper discussed the practical usage and ben-
efit of adversarial regularization based on adver-
sarial perturbation in the current NMT models.
Our experimental results demonstrated that ap-
plying VAT to both encoder and decoder embed-
dings consistently outperformed other configura-
tions. Additionally, we confirmed that adversarial
regularization techniques effectively worked even
if we performed them with the training data in-
creased by a back-translation method. We believe
that adversarial regularization can be one of the
common and fundamental technologies to further
improve the translation quality, such as model en-
semble, byte-pair encoding, and back-translation.

Acknowledgments

We thank three anonymous reviewers for their
helpful comments. We also thank Takeru Miyato,
who gave us valuable comments about AdvT/VAT.



209

References

Yonatan Belinkov and Yonatan Bisk. 2018. Syn-
thetic and Natural Noise Both Break Neural Ma-
chine Translation. In Proceedings of the 6th Inter-
national Conference on Learning Representations
(ICLR).

Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3: Web Inventory of Transcribed
and Translated Talks. In Proceedings of the 16th
Annual Conference of the European Association for
Machine Translation (EAMT), pages 261–268.

Sumit Chopra, Michael Auli, and Alexander M. Rush.
2016. Abstractive Sentence Summarization with At-
tentive Recurrent Neural Networks. In Proceed-
ings of the 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL), pages 93–98.

Kevin Clark, Minh-Thang Luong, Christopher D. Man-
ning, and Quoc Le. 2018. Semi-Supervised Se-
quence Modeling with Cross-View Training. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 1914–1925.

Ian J. Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2015. Explaining and Harnessing Adver-
sarial Examples. In Proceedings of the 3rd Inter-
national Conference on Learning Representations
(ICLR).

Hossein Hosseini, Sreeram Kannan, Baosen Zhang,
and Radha Poovendran. 2017. Deceiving Google’s
Perspective API Built for Detecting Toxic Com-
ments. arXiv preprint arXiv:1702.08138.

Jianshu Ji, Qinlong Wang, Kristina Toutanova, Yongen
Gong, Steven Truong, and Jianfeng Gao. 2017. A
Nested Attention Neural Hybrid Model for Gram-
matical Error Correction. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 753–762.

Robin Jia and Percy Liang. 2017. Adversarial Exam-
ples for Evaluating Reading Comprehension Sys-
tems. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Alexey Kurakin, Ian J Goodfellow, and Samy Bengio.
2017. Adversarial Machine Learning at Scale. In
Proceedings of the 5th International Conference on
Learning Representations (ICLR).

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective Approaches to Attention-
based Neural Machine Translation. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1412–1421.

Takeru Miyato, Andrew M Dai, and Ian Goodfellow.
2017. Adversarial Training Methods for Semi-
Supervised Text Classification. In Proceedings of
the 5th International Conference on Learning Rep-
resentations (ICLR).

Takeru Miyato, Shin ichi Maeda, Masanori Koyama,
Ken Nakae, and Shin Ishii. 2016. Distributional
Smoothing with Virtual Adversarial Training. In
Proceedings of the 4th International Conference on
Learning Representations (ICLR).

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311–318.

Alexander M. Rush, Sumit Chopra, and Jason We-
ston. 2015. A Neural Attention Model for Abstrac-
tive Sentence Summarization. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 379–389.

Suranjana Samanta and Sameep Mehta. 2017. Towards
Crafting Text Adversarial Samples. arXiv preprint
arXiv:1707.02812.

Motoki Sato, Jun Suzuki, Hiroyuki Shindo, and Yuji
Matsumoto. 2018. Interpretable Adversarial Pertur-
bation in Input Embedding Space for Text. In Pro-
ceedings of the Twenty-Seventh International Joint
Conference on Artificial Intelligence (IJCAI), pages
4323–4330.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving Neural Machine Translation
Models with Monolingual Data. In Proceedings of
54th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 86–96.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural Machine Translation of Rare Words
with Subword Units. In Proceedings of 54th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 1715–1725.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural Responding Machine for Short-Text Conver-
sation. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Nat-
ural Language Processing (ACL & IJCNLP), pages
1577–1586.

Jun Suzuki, Sho Takase, Hidetaka Kamigaito, Makoto
Morishita, and Masaaki Nagata. 2018. An Empir-
ical Study of Building a Strong Baseline for Con-
stituency Parsing. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 612–618.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and
Rob Fergus. 2014. Intriguing properties of neural
networks. In Proceedings of the 2nd International
Conference on Learning Representations (ICLR).



210

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Proceedings of the 31st Annual Con-
ference on Neural Information Processing Systems
(NIPS), pages 6000–6010.

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a Foreign Language. In Proceedings of the
29th Annual Conference on Neural Information Pro-
cessing Systems (NIPS), pages 2773–2781.

Dilin Wang, Chengyue Gong, and Qiang Liu. 2019.
Improving Neural Language Modeling via Adver-
sarial Training. In Proceedings of the 36th Inter-
national Conference on Machine Learning (ICML),
pages 6555–6565.


