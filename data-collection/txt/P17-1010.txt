



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 102–111
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1010

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 102–111
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1010

Generating and Exploiting Large-scale Pseudo Training Data for Zero
Pronoun Resolution

Ting Liu†, Yiming Cui‡, Qingyu Yin†, Weinan Zhang†, Shijin Wang‡ and Guoping Hu‡
†Research Center for Social Computing and Information Retrieval,

Harbin Institute of Technology, Harbin, China
‡iFLYTEK Research, Beijing, China

†{tliu,qyyin,wnzhang}@ir.hit.edu.cn
‡{ymcui,sjwang3,gphu}@iflytek.com

Abstract

Most existing approaches for zero pronoun
resolution are heavily relying on annotated
data, which is often released by shared
task organizers. Therefore, the lack of
annotated data becomes a major obstacle
in the progress of zero pronoun resolution
task. Also, it is expensive to spend man-
power on labeling the data for better per-
formance. To alleviate the problem above,
in this paper, we propose a simple but
novel approach to automatically generate
large-scale pseudo training data for zero
pronoun resolution. Furthermore, we suc-
cessfully transfer the cloze-style reading
comprehension neural network model into
zero pronoun resolution task and propose
a two-step training mechanism to over-
come the gap between the pseudo training
data and the real one. Experimental re-
sults show that the proposed approach sig-
nificantly outperforms the state-of-the-art
systems with an absolute improvements of
3.1% F-score on OntoNotes 5.0 data.

1 Introduction

Previous works on zero pronoun (ZP) resolution
mainly focused on the supervised learning ap-
proaches (Han, 2006; Zhao and Ng, 2007; Iida
et al., 2007; Kong and Zhou, 2010; Iida and Poe-
sio, 2011; Chen and Ng, 2013). However, a ma-
jor obstacle for training the supervised learning
models for ZP resolution is the lack of anno-
tated data. An important step is to organize the
shared task on anaphora and coreference resolu-
tion, such as the ACE evaluations, SemEval-2010
shared task on Coreference Resolution in Multiple
Languages (Marta Recasens, 2010) and CoNLL-
2012 shared task on Modeling Multilingual Unre-

stricted Coreference in OntoNotes (Sameer Prad-
han, 2012). Following these shared tasks, the an-
notated evaluation data can be released for the fol-
lowing researches. Despite the success and con-
tributions of these shared tasks, it still faces the
challenge of spending manpower on labeling the
extended data for better training performance and
domain adaptation.

To address the problem above, in this paper, we
propose a simple but novel approach to automati-
cally generate large-scale pseudo training data for
zero pronoun resolution. Inspired by data genera-
tion on cloze-style reading comprehension, we can
treat the zero pronoun resolution task as a special
case of reading comprehension problem. So we
can adopt similar data generation methods of read-
ing comprehension to the zero pronoun resolution
task. For the noun or pronoun in the document,
which has the frequency equal to or greater than 2,
we randomly choose one position where the noun
or pronoun is located on, and replace it with a spe-
cific symbol 〈blank〉. Let query Q and answer
A denote the sentence that contains a 〈blank〉,
and the noun or pronoun which is replaced by
the 〈blank〉, respectively. Thus, a pseudo training
sample can be represented as a triple:

〈D,Q,A〉 (1)

For the zero pronoun resolution task, a 〈blank〉
represents a zero pronoun (ZP) in query Q, and
A indicates the corresponding antecedent of the
ZP. In this way, tremendous pseudo training sam-
ples can be generated from the various documents,
such as news corpus.

Towards the shortcomings of the previous ap-
proaches that are based on feature engineering, we
propose a neural network architecture, which is
an attention-based neural network model, for zero
pronoun resolution. Also we propose a two-step

102

https://doi.org/10.18653/v1/P17-1010
https://doi.org/10.18653/v1/P17-1010


training method, which benefit from both large-
scale pseudo training data and task-specific data,
showing promising performance.

To sum up, the contributions of this paper are
listed as follows.

• To our knowledge, this is the first time that
utilizing reading comprehension neural net-
work model into zero pronoun resolution
task.

• We propose a two-step training approach,
namely pre-training-then-adaptation, which
benefits from both the large-scale automati-
cally generated pseudo training data and task-
specific data.

• Towards the shortcomings of the feature en-
gineering approaches, we first propose an
attention-based neural network model for
zero pronoun resolution.

2 The Proposed Approach

In this section, we will describe our approach in
detail. First, we will describe our method of gen-
erating large-scale pseudo training data for zero
pronoun resolution. Then we will introduce two-
step training approach to alleviate the gaps be-
tween pseudo and real training data. Finally, the
attention-based neural network model as well as
associated unknown words processing techniques
will be described.

2.1 Generating Pseudo Training Data

In order to get large quantities of training data for
neural network model, we propose an approach,
which is inspired by (Hermann et al., 2015), to
automatically generate large-scale pseudo training
data for zero pronoun resolution. However, our ap-
proach is much more simple and general than that
of (Hermann et al., 2015). We will introduce the
details of generating the pseudo training data for
zero pronoun resolution as follows.

First, we collect a large number of documents
that are relevant (or homogenous in some sense)
to the released OntoNote 5.0 data for zero pronoun
resolution task in terms of its domain. In our ex-
periments, we used large-scale news data for train-
ing.

Given a certain document D, which is com-
posed by a set of sentences D = {s1, s2, ..., sn},

we randomly choose an answer wordA in the doc-
ument. Note that, we restrictA to be either a noun
or pronoun, where the part-of-speech is identified
using LTP Toolkit (Che et al., 2010), as well as
the answer word should appear at least twice in
the document. Second, after the answer word A is
chosen, the sentence that contains A is defined as
a queryQ, in which the answer wordA is replaced
by a specific symbol 〈blank〉. In this way, given
the queryQ and documentD, the target of the pre-
diction is to recover the answer A. That is quite
similar to the zero pronoun resolution task. There-
fore, the automatically generated training samples
is called pseudo training data. Figure 1 shows an
example of a pseudo training sample.

In this way, we can generate tremendous triples
of 〈D,Q,A〉 for training neural network, without
making any assumptions on the nature of the orig-
inal corpus.

2.2 Two-step Training

It should be noted that, though we have generated
large-scale pseudo training data for neural network
training, there is still a gap between pseudo train-
ing data and the real zero pronoun resolution task
in terms of the query style. So we should do some
adaptations to our model to deal with the zero pro-
noun resolution problems ideally.

In this paper, we used an effective approach
to deal with the mismatch between pseudo train-
ing data and zero pronoun resolution task-specific
data. Generally speaking, in the first stage, we use
a large amount of the pseudo training data to train
a fundamental model, and choose the best model
according to the validation accuracy. Then we
continue to train from the previous best model us-
ing the zero pronoun resolution task-specific train-
ing data, which is exactly the same domain and
query type as the standard zero pronoun resolution
task data.

The using of the combination of proposed
pseudo training data and task-specific data, i.e.
zero pronoun resolution task data, is far more ef-
fective than using either of them alone. Though
there is a gap between these two data, they share
many similar characteristics to each other as illus-
trated in the previous part, so it is promising to
utilize these two types of data together, which will
compensate to each other.

The two-step training procedure can be con-
cluded as,

103



Document:
1 ||| welcome both of you to the studio to participate in our program ,

欢迎两位呢来演播室参与我们的 节目，
2 ||| it happened that i was going to have lunch with a friend at noon .

正好因为我也和朋友这个，这个中午一起吃饭。
3 ||| after that , i received an sms from 1860 .

然后我就收到 1860 的短信。
4 ||| uh-huh , it was by sms .

嗯，是通过短信 的方式 ，
5 ||| uh-huh , that means , er , you knew about the accident through the source of radio station .

嗯，就是说呃你是通过台里面的一个信息 的渠道知道这儿出了这样的事故。
6 ||| although we live in the west instead of the east part , and it did not affect us that much , 

虽然我们生活在西部不是在东部，对我们影响 不是很大，
7 ||| but i think it is very useful to inform people using sms .

但是呢，我觉得 有这样 一个短信告诉大家呢 是非常有用的啊。
Query:
8 ||| some car owners said that <blank> was very good。

有车主表示，说这 <blank> 非常的好。
Answer:
sms
短信

Figure 1: Example of pseudo training sample for zero pronoun resolution system. (The original data is
in Chinese, we translate this sample into English for clarity)

• Pre-training stage: by using large-scale train-
ing data to train the neural network model,
we can learn richer word embeddings, as well
as relatively reasonable weights in neural net-
works than just training with a small amount
of zero pronoun resolution task training data;

• Adaptation stage: after getting the best model
that is produced in the previous step, we con-
tinue to train the model with task-specific
data, which can force the previous model to
adapt to the new data, without losing much
knowledge that has learned in the previous
stage (such as word embeddings).

As we will see in the experiment section that
the proposed two-step training approach is effec-
tive and brings significant improvements.

2.3 Attention-based Neural Network Model
Our model is primarily an attention-based neu-
ral network model, which is similar to Atten-
tive Reader proposed by (Hermann et al., 2015).
Formally, when given a set of training triple
〈D,Q,A〉, we will construct our network in the
following way.

Firstly, we project one-hot representation of
document D and query Q into a continuous space
with the shared embedding matrix We. Then
we input these embeddings into different bi-
directional RNN to get their contextual represen-
tations respectively. In our model, we used the
bidirectional Gated Recurrent Unit (GRU) as RNN
implementation (Cho et al., 2014).

e(x) =We · x, where x ∈ D,Q (2)

−→
hs =

−−−→
GRU(e(x));

←−
hs =

←−−−
GRU(e(x)) (3)

hs = [
−→
hs;
←−
hs] (4)

For the query representation, instead of concate-
nating the final forward and backward states as its
representations, we directly get an averaged repre-
sentations on all bi-directional RNN slices, which
can be illustrated as

hquery =
1

n

n∑

t=1

hquery(t) (5)

For the document, we place a soft attention over
all words in document (Bahdanau et al., 2014),
which indicate the degree to which part of doc-
ument is attended when filling the blank in the
query sentence. Then we calculate a weighted sum
of all document tokens to get the attended repre-
sentation of document.

m(t) = tanh(W · hdoc(t) + U · hquery) (6)

α(t) =
exp(Ws ·m(t))

n∑
j=1

exp(Ws ·m(j))
(7)

hdoc att = hdoc · α (8)
where variable α(t) is the normalized attention
weight at tth word in document, hdoc is a matrix
that concatenate all hdoc(t) in sequence.

hdoc = concat[hdoc(1), hdoc(2), ..., hdoc(t)] (9)

Then we use attended document representation
and query representation to estimate the final an-
swer, which can be illustrated as follows, where V

104



Bi-GRU Encoder

Σ

d1 d2 d3 d4 q1 q2 q3

Query

Softmax Layer

Concat Layer

AttentionLayer

Answer

Document

Embedding Layer

Figure 2: Architecture of attention-based neural
network model for zero pronoun resolution task.

is the vocabulary,

r = concat[hdoc att, hquery] (10)

P (A|D,Q) ∝ softmax(Wr · r) , s.t. A ∈ V
(11)

Figure 2 shows the proposed neural network ar-
chitecture.

Note that, for zero pronoun resolution task,
antecedents of zero pronouns are always noun
phrases (NPs), while our model generates only one
word (a noun or a pronoun) as the result. To better
adapt our model to zero pronoun resolution task,
we further process the output result in the follow-
ing procedure. First, for a given zero pronoun, we
extract a set of NPs as its candidates utilizing the
same strategy as (Chen and Ng, 2015). Then, we
use our model to generate an answer (one word)
for the zero pronoun. After that, we go through
all the candidates from the nearest to the far-most.
For an NP candidate, if the produced answer is
its head word, we then regard this NP as the an-
tecedent of the given zero pronoun. By doing so,
for a given zero pronoun, we generate an NP as the
prediction of its antecedent.

2.4 Unknown Words Processing

Because of the restriction on both memory occu-
pation and training time, it is usually suggested
to use a shortlist of vocabulary in neural network
training. However, we often replace the out-of-
vocabularies to a unique special token, such as
〈unk〉. But this may place an obstacle in real

world test. When the model predicts the answer
as 〈unk〉, we do not know what is the exact word
it represents in the document, as there may have
many 〈unk〉s in the document.

In this paper, we propose to use a simple but
effective way to handle unknown words issue. The
idea is straightforward, which can be illustrated as
follows.

• Identify all unknown words inside of each
〈D,Q,A〉;

• Instead of replacing all these unknown words
into one unique token 〈unk〉, we make
a hash table to project these unique un-
known words to numbered tokens, such as
〈unk1〉, 〈unk2〉, ..., 〈unkN〉 in terms of its
occurrence order in the document. Note that,
the same words are projected to the same un-
known word tokens, and all these projections
are only valid inside of current sample. For
example, 〈unk1〉 indicate the first unknown
word, say “apple”, in the current sample, but
in another sample the 〈unk1〉 may indicate
the unknown word “orange”. That is, the
unknown word labels are indicating position
features rather than the exact word;

• Insert these unknown marks in the vocabu-
lary. These marks may only take up dozens of
slots, which is negligible to the size of short-
lists (usually 30K ∼ 100K).

(a) The weather today is not as pleasant as the weather of yesterday.

(b) The <unk> today is not as <unk> as the <unk> of yesterday.

(c) The <unk1> today is not as <unk2> as the <unk1> of yesterday.

Figure 3: Example of unknown words processing.
a) original sentence; b) original unknown words
processing method; c) our method

We take one sentence “The weather of today is
not as pleasant as the weather of yesterday.” as
an example to show our unknown word processing
method, which is shown in Figure 3.

If we do not discriminate the unknown words
and assign different unknown words with the same
token 〈unk〉, it would be impossible for us to
know what is the exact word that 〈unk〉 repre-
sents for in the real test. However, when using
our proposed unknown word processing method,
if the model predicts a 〈unkX〉 as the answer,

105



we can simply scan through the original document
and identify its position according to its unknown
word number X and replace the 〈unkX〉 with the
real word. For example, in Figure 3, if we adopt
original unknown words processing method, we
could not know whether the 〈unk〉 is the word
“weather” or “pleasant”. However, when using
our approach, if the model predicts an answer as
〈unk1〉, from the original text, we can know that
〈unk1〉 represents the word “weather”.

3 Experiments

3.1 Data

In our experiments, we choose a selection of
public news data to generate large-scale pseudo
training data for pre-training our neural network
model (pre-training step)1. In the adaptation step,
we used the official dataset OntoNotes Release
5.02 which is provided by CoNLL-2012 shared
task, to carry out our experiments. The CoNLL-
2012 shared task dataset consists of three parts:
a training set, a development set and a test set.
The datasets are made up of 6 different domains,
namely Broadcast News (BN), Newswires (NW),
Broadcast Conversations (BC), Telephone Con-
versations (TC), Web Blogs (WB), and Magazines
(MZ). We closely follow the experimental settings
as (Kong and Zhou, 2010; Chen and Ng, 2014,
2015, 2016), where we treat the training set for
training and the development set for testing, be-
cause only the training and development set are
annotated with ZPs. The statistics of training and
testing data is shown in Table 1 and 2 respectively.

Sentences # Query #

General Train 18.47M 1.81M
Domain Train 122.8K 9.4K
Validation 11,191 2,667

Table 1: Statistics of training data, including
pseudo training data and OntoNotes 5.0 training
data.

3.2 Neural Network Setups

Training details of our neural network models are
listed as follows.

1The news data is available at http://www.sogou.
com/labs/dl/cs.html

2http://catalog.ldc.upenn.edu/
LDC2013T19

Docs Sentences Words AZPs

Test 172 6,083 110K 1,713

Table 2: Statistics of test set (OntoNotes 5.0 de-
velopment data).

• Embedding: We use randomly initialized em-
bedding matrix with uniformed distribution
in the interval [-0.1,0.1], and set units num-
ber as 256. No pre-trained word embeddings
are used.

• Hidden Layer: We use GRU with 256 units,
and initialize the internal matrix by random
orthogonal matrices (Saxe et al., 2013). As
GRU still suffers from the gradient exploding
problem, we set gradient clipping threshold
to 10.

• Vocabulary: As the whole vocabulary is very
large (over 800K), we set a shortlist of 100K
according to the word frequency and un-
known words are mapped to 20 〈unkX〉 us-
ing the proposed method.

• Optimization: We used ADAM update rule
(Kingma and Ba, 2014) with an initial learn-
ing rate of 0.001, and used negative log-
likelihood as the training objective. The
batch size is set to 32.

All models are trained on Tesla K40 GPU. Our
model is implemented with Theano (Theano De-
velopment Team, 2016) and Keras (Chollet, 2015).

3.3 Experimental results
Same to the previous researches that are related
to zero pronoun resolution, we evaluate our sys-
tem performance in terms of F-score (F). We fo-
cus on AZP resolution process, where we assume
that gold AZPs and gold parse trees are given3.
The same experimental setting is utilized in (Chen
and Ng, 2014, 2015, 2016). The overall results are
shown in Table 3, where the performances of each
domain are listed in detail and overall performance
is also shown in the last column.

• Overall Performance
We employ four Chinese ZP resolution baseline
systems on OntoNotes 5.0 dataset. As we can

3All gold information are provided by the CoNLL-2012
shared task dataset

106



NW (84) MZ (162) WB (284) BN (390) BC (510) TC (283) Overall

Kong and Zhou (2010) 34.5 32.7 45.4 51.0 43.5 48.4 44.9
Chen and Ng (2014) 38.1 31.0 50.4 45.9 53.8 54.9 48.7
Chen and Ng (2015) 46.4 39.0 51.8 53.8 49.4 52.7 50.2
Chen and Ng (2016) 48.8 41.5 56.3 55.4 50.8 53.1 52.2

Our Approach† 59.2 51.3 60.5 53.9 55.5 52.9 55.3

Table 3: Experimental result (F-score) on the OntoNotes 5.0 test data. The best results are marked
with bold face. † indicates that our approach is statistical significant over the baselines (using t-test, with
p < 0.05). The number in the brackets indicate the number of AZPs.

see that our model significantly outperforms the
previous state-of-the-art system (Chen and Ng,
2016) by 3.1% in overall F-score, and substan-
tially outperform the other systems by a large mar-
gin. When observing the performances of differ-
ent domains, our approach also gives relatively
consistent improvements among various domains,
except for BN and TC with a slight drop. All these
results approve that our proposed approach is ef-
fective and achieves significant improvements in
AZP resolution.

In our quantitative analysis, we investigated the
reasons of the declines in the BN and TC domain.
A primary observation is that the word distribu-
tions in these domains are fairly different from
others. The average document length of BN and
TC are quite longer than other domains, which
suggest that there is a bigger chance to have un-
known words than other domains, and add dif-
ficulties to the model training. Also, we have
found that in the BN and TC domains, the texts
are often in oral form, which means that there are
many irregular expressions in the context. Such
expressions add noise to the model, and it is dif-
ficult for the model to extract useful information
in these contexts. These phenomena indicate that
further improvements can be obtained by filtering
stop words in contexts, or increasing the size of
task-specific data, while we leave this in the future
work.

• Effect of UNK processing
As we have mentioned in the previous section,
traditional unknown word replacing methods are
vulnerable to the real word test. To alleviate this
issue, we proposed the UNK processing mecha-
nism to recover the UNK tokens to the real words.
In Table 4, we compared the performance that
with and without the proposed UNK processing,

to show whether the proposed UNK processing
method is effective. As we can see that, by apply-
ing our UNK processing mechanism, the model
do learned the positional features in these low-
frequency words, and brings over 3% improve-
ments in F-score, which indicated the effective-
ness of our UNK processing approach.

F-score

Without UNK replacement 52.2
With UNK replacement 55.3

Table 4: Performance comparison on whether us-
ing the proposed unknown words processing.

• Effect of Domain Adaptation
We also tested out whether our domain adapta-
tion method is effective. In this experiments, we
used three different types of training data: only
pseudo training data, only task-specific data, and
our adaptation method, i.e. using pseudo train-
ing data in the pre-training step and task-specific
data for domain adaptation step. The results are
given in Table 5. As we can see that, using either
pseudo training data or task-specific data alone
can not bring inspiring result. By adopting our
domain adaptation method, the model could give
significant improvements over the other models,
which demonstrate the effectiveness of our pro-
posed two-step training approach. An intuition
behind this phenomenon is that though pseudo
training data is fairly big enough to train a reli-
able model parameters, there is still a gap to the
real zero pronoun resolution tasks. On the con-
trary, though task-specific training data is exactly
the same type as the real test, the quantity is not
enough to train a reasonable model (such as word
embedding). So it is better to make use of both to

107



take the full advantage.
However, as the original task-specific data is

fairly small compared to pseudo training data, we
also wondered if the large-scale pseudo training
data is only providing rich word embedding infor-
mation. So we use the large-scale pseudo training
data for embedding training using GloVe toolkit
(Pennington et al., 2014), and initialize the word
embeddings in the “only task-specific data” sys-
tem. From the result we can see that the pseudo
training data provide more information than word
embeddings, because though we used GloVe em-
beddings in “only task-specific data”, it still can
not outperform the system that uses domain adap-
tation which supports our claim.

F-score

Only Pseudo Training Data 41.1
Only Task-Specific Data 44.2
Only Task-Specific Data + GloVe 50.9
Domain Adaptation 55.3

Table 5: Performance comparison of using differ-
ent training data.

4 Error Analysis

To better evaluate our proposed approach, we per-
formed a qualitative analysis of errors, where two
major errors are revealed by our analysis, as dis-
cussed below.

4.1 Effect of Unknown Words
Our approach does not do well when there are lots
of 〈unk〉s in the context of ZPs, especially when
the 〈unk〉s appears near the ZP. An example is
given below, where words with # are regarded as
〈unk〉s in our model.

φ 登上# 太平山# 顶 , 将香港岛# 和维多
利亚港# 的美景尽收眼底。
φ Successfully climbed# the peak of [Taiping
Mountain]#, to have a panoramic view of the
beauty of [Hong Kong Island]# and [Victoria
Harbour]#.

In this case, the words “登上/climbed” and “太
平山/Taiping Mountain” that appears immediately
after the ZP “φ” are all regarded as 〈unk〉s in
our model. As we model the sequence of words
by RNN, the 〈unk〉s make the model more dif-
ficult to capture the semantic information of the
sentence, which in turn influence the overall per-
formance. Especially for the words that are near

the ZP, which play important roles when model-
ing context information for the ZP. By looking at
the word “顶/peak”, it is hard to comprehend the
context information, due to the several surround-
ing 〈unk〉s. Though our proposed unknown words
processing method is effective in empirical evalu-
ation, we think that more advanced method for un-
known words processing would be of a great help
in improving comprehension of the context.

4.2 Long Distance Antecedents
Also, our model makes incorrect decisions when
the correct antecedents of ZPs are in long distance.
As our model chooses answer from words in the
context, if there are lots of words between the ZP
and its antecedent, more noise information are in-
troduced, and adds more difficulty in choosing the
right answer. For example:

我帮不了那个人 ... ...那天结束后 φ 回到
家中。
I can’t help that guy ... ... After that day, φ return
home.

In this case, the correct antecedent of ZP “φ” is
the NP candidate “我/I”. By seeing the contexts,
we observe that there are over 30 words between
the ZP and its antecedent. Although our model
does not intend to fill the ZP gap only with the
words near the ZP, as most of the antecedents ap-
pear just a few words before the ZPs, our model
prefers the nearer words as correct antecedents.
Hence, once there are lots of words between ZP
and its nearest antecedent, our model can some-
times make wrong decisions. To correctly handle
such cases, our model should learn how to filter the
useless words and enhance the learning of long-
term dependency.

5 Related Work

5.1 Zero pronoun resolution
For Chinese zero pronoun (ZP) resolution, early
studies employed heuristic rules to Chinese ZP
resolution. Converse (2006) proposes a rule-based
method to resolve the zero pronouns, by utiliz-
ing Hobbs algorithm (Hobbs, 1978) in the CTB
documents. Then, supervised approaches to this
task have been vastly explored. Zhao and Ng
(2007) first present a supervised machine learn-
ing approach to the identification and resolution
of Chinese ZPs. Kong and Zhou (2010) develop
a tree-kernel based approach for Chinese ZP res-
olution. More recently, unsupervised approaches

108



have been proposed. Chen and Ng (2014) de-
velop an unsupervised language-independent ap-
proach, utilizing the integer linear programming
to using ten overt pronouns. Chen and Ng (2015)
propose an end-to-end unsupervised probabilistic
model for Chinese ZP resolution, using a salience
model to capture discourse information. Also,
there have been many works on ZP resolution for
other languages. These studies can be divided into
rule-based and supervised machine learning ap-
proaches. Ferrández and Peral (2000) proposed a
set of hand-crafted rules for Spanish ZP resolu-
tion. Recently, supervised approaches have been
exploited for ZP resolution in Korean (Han, 2006)
and Japanese (Isozaki and Hirao, 2003; Iida et al.,
2006, 2007; Sasano and Kurohashi, 2011). Iida
and Poesio (2011) developed a cross-lingual ap-
proach for Japanese and Italian ZPs where an ILP-
based model was employed to zero anaphora de-
tection and resolution.

In sum, most recent researches on ZP resolu-
tion are supervised approaches, which means that
their performance highly relies on large-scale an-
notated data. Even for the unsupervised approach
(Chen and Ng, 2014), they also utilize a super-
vised pronoun resolver to resolve ZPs. Therefore,
the advantage of our proposed approach is obvi-
ous. We are able to generate large-scale pseudo
training data for ZP resolution, and also we can
benefit from the task-specific data for fine-tuning
via the proposed two-step training approach.

5.2 Cloze-style Reading Comprehension

Our neural network model is mainly motivated by
the recent researches on cloze-style reading com-
prehension tasks, which aims to predict one-word
answer given the document and query. These
models can be seen as a general model of min-
ing the relations between the document and query,
so it is promising to combine these models to the
specific domain.

A representative work of cloze-style reading
comprehension is done by Hermann et al. (2015).
They proposed a methodology for obtaining large
quantities of 〈D,Q,A〉 triples. By using this
method, a large number of training data can be
obtained without much human intervention, and
make it possible to train a reliable neural network.
They used attention-based neural networks for
this task. Evaluation on CNN/DailyMail datasets
showed that their approach is much effective than

traditional baseline systems.
While our work is similar to Hermann et al.

(2015), there are several differences which can be
illustrated as follows. Firstly, though we both uti-
lize the large-scale corpus, they require that the
document should accompany with a brief sum-
mary of it, while this is not always available in
most of the document, and it may place an obstacle
in generating limitless training data. In our work,
we do not assume any prerequisite of the training
data, and directly extract queries from the docu-
ment, which makes it easy to generate large-scale
training data. Secondly, their work mainly focuses
on reading comprehension in the general domain.
We are able to exploit large-scale training data for
solving problems in the specific domain, and we
proposed two-step training method which can be
easily adapted to other domains as well.

6 Conclusion

In this study, we propose an effective way to gen-
erate and exploit large-scale pseudo training data
for zero pronoun resolution task. The main idea
behind our approach is to automatically generate
large-scale pseudo training data and then utilize an
attention-based neural network model to resolve
zero pronouns. For training purpose, two-step
training approach is employed, i.e. a pre-training
and adaptation step, and this can be also easily
applied to other tasks as well. The experimental
results on OntoNotes 5.0 corpus are encouraging,
showing that the proposed model and accompany-
ing approaches significantly outperforms the state-
of-the-art systems.

The future work will be carried out on two main
aspects: First, as experimental results show that
the unknown words processing is a critical part in
comprehending context, we will explore more ef-
fective way to handle the UNK issue. Second, we
will develop other neural network architecture to
make it more appropriate for zero pronoun resolu-
tion task.

Acknowledgements

We would like to thank the anonymous review-
ers for their thorough reviewing and propos-
ing thoughtful comments to improve our paper.
This work was supported by the National 863
Leading Technology Research Project via grant
2015AA015407, Key Projects of National Natural
Science Foundation of China via grant 61632011,

109



and National Natural Science Youth Foundation of
China via grant 61502120.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .

Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp:
A chinese language technology platform. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics: Demonstrations. Asso-
ciation for Computational Linguistics, pages 13–16.

Chen Chen and Vincent Ng. 2013. Chinese zero pro-
noun resolution: Some recent advances. In EMNLP.
pages 1360–1365.

Chen Chen and Vincent Ng. 2014. Chinese zero pro-
noun resolution: An unsupervised approach com-
bining ranking and integer linear programming. In
Twenty-Eighth AAAI Conference on Artificial Intel-
ligence.

Chen Chen and Vincent Ng. 2015. Chinese zero pro-
noun resolution: A joint unsupervised discourse-
aware model rivaling state-of-the-art resolvers. In
Proceedings of the 53rd Annual Meeting of the ACL
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers).
page 320.

Chen Chen and Vincent Ng. 2016. Chinese zero pro-
noun resolution with deep neural networks. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics,
pages 778–788. http://aclweb.org/anthology/P16-
1074.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

François Chollet. 2015. Keras. https://github.
com/fchollet/keras.

Susan P Converse. 2006. Pronominal anaphora resolu-
tion in chinese .

Antonio Ferrández and Jesús Peral. 2000. A compu-
tational approach to zero-pronouns in spanish. In
Proceedings of the 38th Annual Meeting on Associ-
ation for Computational Linguistics. Association for
Computational Linguistics, pages 166–172.

Na-Rae Han. 2006. Korean zero pronouns: analysis
and resolution. Ph.D. thesis, Citeseer.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems. pages 1684–
1692.

Jerry R Hobbs. 1978. Resolving pronoun references.
Lingua 44(4):311–338.

Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
resolution. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, pages 625–632.

Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007.
Zero-anaphora resolution by learning rich syntactic
pattern features. ACM Transactions on Asian Lan-
guage Information Processing (TALIP) 6(4):1.

Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ilp solution to zero anaphora resolution. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1. Association for Computa-
tional Linguistics, pages 804–813.

Hideki Isozaki and Tsutomu Hirao. 2003. Japanese
zero pronoun resolution based on ranking rules and
machine learning. In Proceedings of the 2003 con-
ference on Empirical methods in natural language
processing. Association for Computational Linguis-
tics, pages 184–191.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for chinese zero anaphora
resolution. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
pages 882–891.

Lluis Marquez Emili Sapena M Antonia Marti Mar-
iona Taule Veronique Hoste Massimo Poesio Yan-
nick Versley Marta Recasens. 2010. Semeval-2010
task 1: Coreference resolution in multiple languages
.

Jeffrey Pennington, Richard Socher, and Christo-
pher Manning. 2014. Glove: Global vectors
for word representation. In Proceedings of the
2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). Association
for Computational Linguistics, pages 1532–1543.
http://aclweb.org/anthology/D14-1162.

Alessandro Moschitti Nianwen Xue Olga Uryupina
Yuchen Zhang Sameer Pradhan. 2012. Conll-2012
shared task: Modeling multilingual unrestricted
coreference in ontonotes .

110



Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
IJCNLP. pages 758–766.

Andrew M Saxe, James L McClelland, and Surya Gan-
guli. 2013. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv
preprint arXiv:1312.6120 .

Theano Development Team. 2016. Theano: A
Python framework for fast computation of mathe-
matical expressions. arXiv e-prints abs/1605.02688.
http://arxiv.org/abs/1605.02688.

Shanheng Zhao and Hwee Tou Ng. 2007. Identifica-
tion and resolution of chinese zero pronouns: A ma-
chine learning approach. In EMNLP-CoNLL. vol-
ume 2007, pages 541–550.

111


	Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution

