



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1722–1731
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1158

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1722–1731
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1158

CANE: Context-Aware Network Embedding for Relation Modeling

Cunchao Tu1,2∗, Han Liu3∗, Zhiyuan Liu1,2†, Maosong Sun1,2
1Department of Computer Science and Technology, State Key Lab on Intelligent Technology and Systems,

National Lab for Information Science and Technology, Tsinghua University, China
2Jiangsu Collaborative Innovation Center for Language Ability, Jiangsu Normal University, China

3Northeastern University, China

Abstract

Network embedding (NE) is playing a
critical role in network analysis, due to
its ability to represent vertices with ef-
ficient low-dimensional embedding vec-
tors. However, existing NE models aim to
learn a fixed context-free embedding for
each vertex and neglect the diverse roles
when interacting with other vertices. In
this paper, we assume that one vertex usu-
ally shows different aspects when interact-
ing with different neighbor vertices, and
should own different embeddings respec-
tively. Therefore, we present Context-
Aware Network Embedding (CANE), a
novel NE model to address this issue.
CANE learns context-aware embeddings
for vertices with mutual attention mecha-
nism and is expected to model the seman-
tic relationships between vertices more
precisely. In experiments, we compare
our model with existing NE models on
three real-world datasets. Experimen-
tal results show that CANE achieves sig-
nificant improvement than state-of-the-art
methods on link prediction and compara-
ble performance on vertex classification.
The source code and datasets can be ob-
tained from https://github.com/
thunlp/CANE.

1 Introduction

Network embedding (NE), i.e., network represen-
tation learning (NRL), aims to map vertices of a
network into a low-dimensional space according
to their structural roles in the network. NE pro-
vides an efficient and effective way to represent

∗ Indicates equal contribution
†Corresponding Author: Z. Liu (liuzy@tsinghua.edu.cn)

and manage large-scale networks, alleviating the
computation and sparsity issues of conventional
symbol-based representations. Hence, NE is at-
tracting many research interests in recent years
(Perozzi et al., 2014; Tang et al., 2015; Grover and
Leskovec, 2016), and achieves promising perfor-
mance on many network analysis tasks including
link prediction, vertex classification, and commu-
nity detection.

I am studying NLP problems, 

including syntactic parsing, 

machine translation and so on.

My research focuses on typical 

NLP tasks, including word 

segmentation, tagging and 

syntactic parsing.

I am a NLP researcher in machine 

translation, especially using deep 

learning models to improve 

machine translation.

Figure 1: Example of a text-based information
network. (Red, blue and green fonts represent con-
cerns of the left user, right user and both respec-
tively.)

In real-world social networks, it is intuitive that
one vertex may demonstrate various aspects when
interacting with different neighbor vertices. For
example, a researcher usually collaborates with
various partners on diverse research topics (as il-
lustrated in Fig. 1), a social-media user contacts
with various friends sharing distinct interests, and
a web page links to multiple pages for different
purposes. However, most existing NE methods
only arrange one single embedding vector to each
vertex, and give rise to the following two invertible
issues: (1) These methods cannot flexibly cope
with the aspect transition of a vertex when inter-
acting with different neighbors. (2) In these mod-
els, a vertex tends to force the embeddings of its

1722

https://doi.org/10.18653/v1/P17-1158
https://doi.org/10.18653/v1/P17-1158


neighbors close to each other, which may be not
the case all the time. For example, the left user
and right user in Fig. 1, share less common inter-
ests, but are learned to be close to each other since
they both link to the middle person. This will ac-
cordingly make vertex embeddings indiscrimina-
tive.

To address these issues, we aim to propose
a Context-Aware Network Embedding (CANE)
framework for modeling relationships between
vertices precisely. More specifically, we present
CANE on information networks, where each ver-
tex also contains rich external information such as
text, labels or other meta-data, and the significance
of context is more critical for NE in this scenario.
Without loss of generality, we implement CANE
on text-based information networks in this paper,
which can easily extend to other types of informa-
tion networks.

In conventional NE models, each vertex is rep-
resented as a static embedding vector, denoted as
context-free embedding. On the contrary, CANE
assigns dynamic embeddings to a vertex according
to different neighbors it interacts with, named as
context-aware embedding. Take a vertex u and
its neighbor vertex v for example. The context-
free embedding of u remains unchanged when in-
teracting with different neighbors. On the con-
trary, the context-aware embedding of u is dy-
namic when confronting different neighbors.

When u interacting with v, their context em-
beddings concerning each other are derived from
their text information, Su and Sv respectively. For
each vertex, we can easily use neural models, such
as convolutional neural networks (Blunsom et al.,
2014; Johnson and Zhang, 2014; Kim, 2014) and
recurrent neural networks (Kiros et al., 2015; Tai
et al., 2015), to build context-free text-based em-
bedding. In order to realize context-aware text-
based embeddings, we introduce the selective at-
tention scheme and build mutual attention be-
tween u and v into these neural models. The mu-
tual attention is expected to guide neural models
to emphasize those words that are focused by its
neighbor vertices and eventually obtain context-
aware embeddings.

Both context-free embeddings and context-
aware embeddings of each vertex can be effi-
ciently learned together via concatenation using
existing NE methods such as DeepWalk (Per-
ozzi et al., 2014), LINE (Tang et al., 2015) and

node2vec (Grover and Leskovec, 2016).
We conduct experiments on three real-world

datasets of different areas. Experimental results
on link prediction reveal the effectiveness of our
framework as compared to other state-of-the-art
methods. The results suggest that context-aware
embeddings are critical for network analysis, in
particular for those tasks concerning about com-
plicated interactions between vertices such as link
prediction. We also explore the performance of
our framework via vertex classification and case
studies, which again confirms the flexibility and
superiority of our models.

2 Related Work

With the rapid growth of large-scale social net-
works, network embedding, i.e. network repre-
sentation learning has been proposed as a critical
technique for network analysis tasks.

In recent years, there have been a large num-
ber of NE models proposed to learn efficient ver-
tex embeddings (Tang and Liu, 2009; Cao et al.,
2015; Wang et al., 2016; Tu et al., 2016a). For ex-
ample, DeepWalk (Perozzi et al., 2014) performs
random walks over networks and introduces an ef-
ficient word representation learning model, Skip-
Gram (Mikolov et al., 2013a), to learn network
embeddings. LINE (Tang et al., 2015) optimizes
the joint and conditional probabilities of edges
in large-scale networks to learn vertex represen-
tations. Node2vec (Grover and Leskovec, 2016)
modifies the random walk strategy in DeepWalk
into biased random walks to explore the network
structure more efficiently. Nevertheless, most of
these NE models only encode the structural infor-
mation into vertex embeddings, without consider-
ing heterogeneous information accompanied with
vertices in real-world social networks.

To address this issue, researchers make great
efforts to incorporate heterogeneous information
into conventional NE models. For instance,
Yang et al. (2015) present text-associated Deep-
Walk (TADW) to improve matrix factorization
based DeepWalk with text information. Tu
et al. (2016b) propose max-margin DeepWalk
(MMDW) to learn discriminative network rep-
resentations by utilizing labeling information of
vertices. Chen et al. (2016) introduce group-
enhanced network embedding (GENE) to inte-
grate existing group information in NE. Sun et
al. (2016) regard text content as a special kind

1723



of vertices, and propose context-enhanced net-
work embedding (CENE) through leveraging both
structural and textural information to learn net-
work embeddings.

To the best of our knowledge, all existing NE
models focus on learning context-free embed-
dings, but ignore the diverse roles when a vertex
interacts with others. In contrast, we assume that
a vertex has different embeddings according to
which vertex it interacts with, and propose CANE
to learn context-aware vertex embeddings.

3 Problem Formulation

We first give basic notations and definitions in this
work. Suppose there is an information network
G = (V,E, T ), where V is the set of vertices,
E ⊆ V ×V are edges between vertices, and T de-
notes the text information of vertices. Each edge
eu,v ∈ E represents the relationship between two
vertices (u, v), with an associated weight wu,v.
Here, the text information of a specific vertex
v ∈ V is represented as a word sequence Sv =
(w1, w2, . . . , wnv), where nv = |Sv|. NRL aims
to learn a low-dimensional embedding v ∈ Rd for
each vertex v ∈ V according to its network struc-
ture and associated information, e.g. text and la-
bels. Note that, d � |V | is the dimension of rep-
resentation space.

Definition 1. Context-free Embeddings: Con-
ventional NRL models learn context-free embed-
ding for each vertex. It means the embedding of
a vertex is fixed and won’t change with respect to
its context information (i.e., another vertex it in-
teracts with).

Definition 2. Context-aware Embeddings:
Different from existing NRL models that learn
context-free embeddings, CANE learns various
embeddings for a vertex according to its differ-
ent contexts. Specifically, for an edge eu,v, CANE
learns context-aware embeddings v(u) and u(v).

4 The Method

4.1 Overall Framework
To take full use of both network structure and as-
sociated text information, we propose two types
of embeddings for a vertex v, i.e., structure-
based embedding vs and text-based embedding
vt. Structure-based embedding can capture the
information in the network structure, while text-
based embedding can capture the textual mean-
ings lying in the associated text information. With

these embeddings, we can simply concatenate
them and obtain the vertex embeddings as v =
vs ⊕ vt, where ⊕ indicates the concatenation op-
eration. Note that, the text-based embedding vt

can be either context-free or context-aware, which
will be introduced detailedly in section 4.4 and 4.5
respectively. When vt is context-aware, the over-
all vertex embeddings v will be context-aware as
well.

With above definitions, CANE aims to maxi-
mize the overall objective of edges as follows:

L =
∑

e∈E
L(e). (1)

Here, the objective of each edge L(e) consists of
two parts as follows:

L(e) = Ls(e) + Lt(e), (2)

where Ls(e) denotes the structure-based objective
and Lt(e) represents the text-based objective.

In the following part, we give the detailed intro-
duction to the two objectives respectively.

4.2 Structure-based Objective
Without loss of generality, we assume the network
is directed, as an undirected edge can be consid-
ered as two directed edges with opposite directions
and equal weights.

Thus, the structure-based objective aims to
measure the log-likelihood of a directed edge us-
ing the structure-based embeddings as

Ls(e) = wu,v log p(v
s|us). (3)

Following LINE (Tang et al., 2015), we define
the conditional probability of v generated by u in
Eq. (3) as

p(vs|us) = exp(u
s · vs)∑

z∈V exp(u
s · zs) . (4)

4.3 Text-based Objective
Vertices in real-world social networks usually ac-
company with associated text information. There-
fore, we propose the text-based objective to take
advantage of these text information, as well as
learn text-based embeddings for vertices.

The text-based objective Lt(e) can be defined
with various measurements. To be compatible
with Ls(e), we define Lt(e) as follows:

Lt(e) = α ·Ltt(e) + β ·Lts(e) + γ ·Lst(e), (5)

1724



where α, β and γ control the weights of various
parts, and

Ltt(e) = wu,v log p(v
t|ut),

Lts(e) = wu,v log p(v
t|us),

Lst(e) = wu,v log p(v
s|ut).

(6)

The conditional probabilities in Eq. (6) map the
two types of vertex embeddings into the same rep-
resentation space, but do not enforce them to be
identical for the consideration of their own char-
acteristics. Similarly, we employ softmax function
for calculating the probabilities, as in Eq. (4).

The structure-based embeddings are regarded as
parameters, the same as in conventional NE mod-
els. But for text-based embeddings, we intend to
obtain them from associated text information of
vertices. Besides, the text-based embeddings can
be obtained either in context-free ways or context-
aware ones. In the following sections, we will give
detailed introduction respectively.

4.4 Context-Free Text Embedding
There has been a variety of neural models to obtain
text embeddings from a word sequence, such as
convolutional neural networks (CNN) (Blunsom
et al., 2014; Johnson and Zhang, 2014; Kim, 2014)
and recurrent neural networks (RNN) (Kiros et al.,
2015; Tai et al., 2015).

In this work, we investigate different neural net-
works for text modeling, including CNN, Bidi-
rectional RNN (Schuster and Paliwal, 1997) and
GRU (Cho et al., 2014), and employ the best per-
formed CNN, which can capture the local seman-
tic dependency among words.

Taking the word sequence of a vertex as input,
CNN obtains the text-based embedding through
three layers, i.e. looking-up, convolution and
pooling.

Looking-up. Given a word sequence S =
(w1, w2, . . . , wn), the looking-up layer transforms
each word wi ∈ S into its corresponding word
embedding wi ∈ Rd′ and obtains embedding se-
quence as S = (w1,w2, . . . ,wn). Here, d′ indi-
cates the dimension of word embeddings.

Convolution. After looking-up, the convolu-
tion layer extracts local features of input embed-
ding sequence S. To be specific, it performs con-
volution operation over a sliding window of length
l using a convolution matrix C ∈ Rd×(l×d′) as fol-
lows:

xi = C · Si:i+l−1 + b, (7)

where Si:i+l−1 denotes the concatenation of word
embeddings within the i-th window and b is the
bias vector. Note that, we add zero padding vec-
tors (Hu et al., 2014) at the edge of the sentence.

Max-pooling. To obtain the text embedding vt,
we operate max-pooling and non-linear transfor-
mation over {xi0, . . . ,xin} as follows:

ri = tanh(max(x
i
0, . . . ,x

i
n)), (8)

At last, we encode the text information of a ver-
tex with CNN and obtain its text-based embedding
vt = [r1, . . . , rd]

T . As vt is irrelevant to the other
vertices it interacts with, we name it as context-
free text embedding.

4.5 Context-Aware Text Embedding

Text 

Description

Text 

Description

Convolutional 

Unit

Convolutional 

Unit

u v

P Q

A

tanh(PTAQ)

Row-pooling + 

softmax

Column-pooling + 

softmax

a
p a

q

u
t
(v)=P·a

p
v

t
(u)=Q·a

q

F

Edge

Text 

Embedding

Figure 2: An illustration of context-aware text em-
bedding.

As stated before, we assume that a specific ver-
tex plays different roles when interacting with oth-
ers vertices. In other words, each vertex should
have its own points of focus about a specific ver-
tex, which leads to its context-aware text embed-
dings.

To achieve this, we employ mutual attention to
obtain context-aware text embedding. It enables
the pooling layer in CNN to be aware of the vertex
pair in an edge, in a way that text information from
a vertex can directly affect the text embedding of
the other vertex, and vice versa.

1725



In Fig. 2, we give an illustration of the gen-
erating process of context-aware text embedding.
Given an edge eu,v with two corresponding text
sequences Su and Sv, we can get the matrices
P ∈ Rd×m and Q ∈ Rd×n through convolution
layer. Here, m and n represent the lengths of Su
and Sv respectively. By introducing an attentive
matrix A ∈ Rd×d, we compute the correlation ma-
trix F ∈ Rm×n as follows:

F = tanh(PTAQ). (9)

Note that, each element Fi,j in F represents the
pair-wise correlation score between two hidden
vectors, i.e., Pi and Qj .

After that, we conduct pooling operations along
rows and columns of F to generate the importance
vectors, named as row-pooling and column pool-
ing respectively. According to our experiments,
mean-pooling performs better than max-pooling.
Thus, we employ mean-pooling operation as fol-
lows:

gpi = mean(Fi,1, . . . ,Fi,n),

gqi = mean(F1,i, . . . ,Fm,i).
(10)

The importance vectors of P and Q are ob-
tained as gp = [gp1 , . . . , g

p
m]T and gq =

[gq1, . . . , g
q
n]T .

Next, we employ softmax function to transform
importance vectors gp and gq to attention vectors
ap and aq. For instance, the i-th element of ap is
formalized as follows:

api =
exp(gpi )∑

j∈[1,m] exp(g
p
j )
. (11)

At last, the context-aware text embeddings of u
and v are computed as

ut(v) = Pa
p,

vt(u) = Qa
q.

(12)

Now, given an edge (u, v), we can obtain the
context-aware embeddings of vertices with their
structure embeddings and context-aware text em-
beddings as u(v) = us⊕ut(v) and v(u) = vs⊕vt(u).

4.6 Optimization of CANE

According to Eq. (3) and Eq. (6), CANE aims
to maximize several conditional probabilities be-
tween u ∈ {us,ut(v)} and v ∈ {vs,vt(u)}. It

is intuitive that optimizing the conditional prob-
ability using softmax function is computation-
ally expensive. Thus, we employ negative sam-
pling (Mikolov et al., 2013b) and transform the
objective into the following form:

log σ(uT ·v)+
k∑

i=1

Ez∼P (v)[log σ(−uT ·z)], (13)

where k is the number of negative samples and σ
represents the sigmoid function. P (v) ∝ dv3/4
denotes the distribution of vertices, where dv is the
out-degree of v.

Afterward, we employ Adam (Kingma and Ba,
2015) to optimize the transformed objective. Note
that, CANE is exactly capable of zero-shot scenar-
ios, by generating text embeddings of new vertices
with well-trained CNN.

5 Experiments

To investigate the effectiveness of CANE on mod-
eling relationships between vertices, we conduct
experiments of link prediction on several real-
world datasets. Besides, we also employ vertex
classification to verify whether context-aware em-
beddings of a vertex can compose a high-quality
context-free embedding in return.

5.1 Datasets

Datasets Cora HepTh Zhihu

#Vertices 2, 277 1, 038 10, 000
#Edges 5, 214 1, 990 43, 894
#Labels 7 − −

Table 1: Statistics of Datasets.

We select three real-world network datasets as
follows:

Cora1 is a typical paper citation network con-
structed by (McCallum et al., 2000). After filter-
ing out papers without text information, there are
2, 277 machine learning papers in this network,
which are divided into 7 categories.

HepTh2 (High Energy Physics Theory) is
another citation network from arXiv3 released
by (Leskovec et al., 2005). We filter out papers
without abstract information and retain 1, 038 pa-
pers at last.

1https://people.cs.umass.edu/∼mccallum/data.html
2https://snap.stanford.edu/data/cit-HepTh.html
3https://arxiv.org/

1726



%Training edges 15% 25% 35% 45% 55% 65% 75% 85% 95%

MMB 54.7 57.1 59.5 61.9 64.9 67.8 71.1 72.6 75.9
DeepWalk 56.0 63.0 70.2 75.5 80.1 85.2 85.3 87.8 90.3

LINE 55.0 58.6 66.4 73.0 77.6 82.8 85.6 88.4 89.3
node2vec 55.9 62.4 66.1 75.0 78.7 81.6 85.9 87.3 88.2

Naive Combination 72.7 82.0 84.9 87.0 88.7 91.9 92.4 93.9 94.0
TADW 86.6 88.2 90.2 90.8 90.0 93.0 91.0 93.4 92.7
CENE 72.1 86.5 84.6 88.1 89.4 89.2 93.9 95.0 95.9

CANE (text only) 78.0 80.5 83.9 86.3 89.3 91.4 91.8 91.4 93.3
CANE (w/o attention) 85.8 90.5 91.6 93.2 93.9 94.6 95.4 95.1 95.5

CANE 86.8 91.5 92.2 93.9 94.6 94.9 95.6 96.6 97.7

Table 2: AUC values on Cora. (α = 1.0, β = 0.3, γ = 0.3)

Zhihu4 is the largest online Q&A website in
China. Users follow each other and answer ques-
tions on this site. We randomly crawl 10, 000 ac-
tive users from Zhihu, and take the descriptions of
their concerned topics as text information.

The detailed statistics are listed in Table 1.

5.2 Baselines

We employ the following methods as baselines:
Structure-only:
MMB (Airoldi et al., 2008) (Mixed Membership

Stochastic Blockmodel) is a conventional graphi-
cal model of relational data. It allows each vertex
to randomly select a different ”topic” when form-
ing an edge.

DeepWalk (Perozzi et al., 2014) performs ran-
dom walks over networks and employ Skip-Gram
model (Mikolov et al., 2013a) to learn vertex em-
beddings.

LINE (Tang et al., 2015) learns vertex embed-
dings in large-scale networks using first-order and
second-order proximities.

Node2vec (Grover and Leskovec, 2016) pro-
poses a biased random walk algorithm based on
DeepWalk to explore neighborhood architecture
more efficiently.

Structure and Text:
Naive Combination: We simply concatenate the

best-performed structure-based embeddings with
CNN based embeddings to represent the vertices.

TADW (Yang et al., 2015) employs matrix fac-
torization to incorporate text features of vertices
into network embeddings.

CENE (Sun et al., 2016) leverages both struc-
ture and textural information by regarding text
content as a special kind of vertices, and optimizes
the probabilities of heterogeneous links.

4https://www.zhihu.com/

5.3 Evaluation Metrics and Experiment
Settings

For link prediction, we adopt a standard evaluation
metric AUC (Hanley and McNeil, 1982), which
represents the probability that vertices in a random
unobserved link are more similar than those in a
random nonexistent link.

For vertex classification, we employ L2-
regularized logistic regression (L2R-LR) (Fan
et al., 2008) to train classifiers, and evaluate the
classification accuracies of various methods.

To be fair, we set the embedding dimension to
200 for all methods. In LINE, we set the number
of negative samples to 5; we learn the 100 dimen-
sional first-order and second-order embeddings re-
spectively, and concatenate them to form the 200
dimensional embeddings. In node2vec, we em-
ploy grid search and select the best-performed
hyper-parameters for training. We also apply grid
search to set the hyper-parameters α, β and γ in
CANE. Besides, we set the number of negative
samples k to 1 in CANE to speed up the train-
ing process. To demonstrate the effectiveness of
considering attention mechanism and two types of
objectives in Eqs. (3) and (6), we design three
versions of CANE for evaluation, i.e., CANE with
text only, CANE without attention and CANE.

5.4 Link Prediction

As shown in Table 2, Table 3 and Table 4, we eval-
uate the AUC values while removing different ra-
tios of edges on Cora, HepTh and Zhihu respec-
tively. Note that, when we only keep 5% edges
for training, most vertices are isolated, which re-
sults in the poor and meaningless performance of
all the methods. Thus, we omit the results under
this training ratio. From these tables, we have the
following observations:

1727



%Training edges 15% 25% 35% 45% 55% 65% 75% 85% 95%

MMB 54.6 57.9 57.3 61.6 66.2 68.4 73.6 76.0 80.3
DeepWalk 55.2 66.0 70.0 75.7 81.3 83.3 87.6 88.9 88.0

LINE 53.7 60.4 66.5 73.9 78.5 83.8 87.5 87.7 87.6
node2vec 57.1 63.6 69.9 76.2 84.3 87.3 88.4 89.2 89.2

Naive Combination 78.7 82.1 84.7 88.7 88.7 91.8 92.1 92.0 92.7
TADW 87.0 89.5 91.8 90.8 91.1 92.6 93.5 91.9 91.7
CENE 86.2 84.6 89.8 91.2 92.3 91.8 93.2 92.9 93.2

CANE (text only) 83.8 85.2 87.3 88.9 91.1 91.2 91.8 93.1 93.5
CANE (w/o attention) 84.5 89.3 89.2 91.6 91.1 91.8 92.3 92.5 93.6

CANE 90.0 91.2 92.0 93.0 94.2 94.6 95.4 95.7 96.3

Table 3: AUC values on HepTh. (α = 0.7, β = 0.2, γ = 0.2)

%Training edges 15% 25% 35% 45% 55% 65% 75% 85% 95%

MMB 51.0 51.5 53.7 58.6 61.6 66.1 68.8 68.9 72.4
DeepWalk 56.6 58.1 60.1 60.0 61.8 61.9 63.3 63.7 67.8

LINE 52.3 55.9 59.9 60.9 64.3 66.0 67.7 69.3 71.1
node2vec 54.2 57.1 57.3 58.3 58.7 62.5 66.2 67.6 68.5

Naive Combination 55.1 56.7 58.9 62.6 64.4 68.7 68.9 69.0 71.5
TADW 52.3 54.2 55.6 57.3 60.8 62.4 65.2 63.8 69.0
CENE 56.2 57.4 60.3 63.0 66.3 66.0 70.2 69.8 73.8

CANE (text only) 55.6 56.9 57.3 61.6 63.6 67.0 68.5 70.4 73.5
CANE (w/o attention) 56.7 59.1 60.9 64.0 66.1 68.9 69.8 71.0 74.3

CANE 56.8 59.3 62.9 64.5 68.9 70.4 71.4 73.6 75.4

Table 4: AUC values on Zhihu. (α = 1.0, β = 0.3, γ = 0.3)

(1) Our proposed CANE consistently achieves
significant improvement comparing to all the base-
lines on all different datasets and different train-
ing ratios. It indicates the effectiveness of CANE
when applied to link prediction task, and verifies
that CANE has the capability of modeling rela-
tionships between vertices precisely.

(2) What calls for special attention is that, both
CENE and TADW exhibit unstable performance
under various training ratios. Specifically, CENE
performs poorly under small training ratios, be-
cause it reserves much more parameters (e.g.,
convolution kernels and word embeddings) than
TADW, which need more data for training. Differ-
ent from CENE, TADW performs much better un-
der small training ratios, because DeepWalk based
methods can explore the sparse network struc-
ture well through random walks even with lim-
ited edges. However, it achieves poor performance
under large ones, as its simplicity and the limita-
tion of bag-of-words assumption. On the contrary,
CANE has a stable performance in various situa-
tions. It demonstrates the flexibility and robust-
ness of CANE.

(3) By introducing attention mechanism, the
learnt context-aware embeddings obtain consider-

able improvements than the ones without atten-
tion. It verifies our assumption that a specific ver-
tex should play different roles when interacting
with other vertices, and thus benefits the relevant
link prediction task.

To summarize, all the above observations
demonstrate that CANE can learn high-quality
context-aware embeddings, which are conducive
to estimating the relationship between vertices
precisely. Moreover, the experimental results on
link prediction task state the effectiveness and ro-
bustness of CANE.

5.5 Vertex Classification

In CANE, we obtain various embeddings of a ver-
tex according to the vertex it connects to. It’s intu-
itive that the obtained context-aware embeddings
are naturally applicable to link prediction task.
However, network analysis tasks, such as vertex
classification and clustering, require a global em-
bedding, rather than several context-aware embed-
dings for each vertex.

To demonstrate the capability of CANE to solve
these issues, we generate the global embedding
of a vertex u by simply averaging all the context-

1728



aware embeddings as follows:

u =
1

N

∑

(u,v)|(v,u)∈E
u(v),

where N indicates the number of context-aware
embeddings of u.

50

60

70

80

90

100

M
M

B

De
ep

W
alk

LI
NE

no
de

2v
ec NC

TA
DW

CE
NE

CA
NE

(te
xt

 on
ly)

CA
NE

(w
/0 

at
ten

tio
n)

CA
NE

A
cc

ur
ac

y 
(×

 1
00

)

Figure 3: Vertex classification results on Cora.

With the generated global embeddings, we con-
duct 2-fold cross-validation and report the aver-
age accuracy of vertex classification on Cora. As
shown in Fig. 3, we observe that:

(1) CANE achieves comparable performance
with state-of-the-art model CENE. It states that the
learnt context-aware embeddings can transform
into high-quality context-free embeddings through
simple average operation, which can be further
employed to other network analysis tasks.

(2) With the introduction of mutual attention
mechanism, CANE has an encouraging improve-
ment than the one without attention, which is in
accordance with the results of link prediction. It
denotes that CANE is flexible to various network
analysis tasks.

5.6 Case Study
To demonstrate the significance of mutual atten-
tion on selecting meaningful features from text in-
formation, we visualize the heat maps of two ver-
tex pairs in Fig. 4. Note that, every word in this
figure accompanies with various background col-
ors. The stronger the background color is, the
larger the weight of this word is. The weight of
each word is calculated according to the attention
weights as follows.

For each vertex pair, we can get the attention
weight of each convolution window according to
Eq. (11). To obtain the weights of words, we as-
sign the attention weight to each word in this win-
dow, and add the attention weights of a word to-
gether as its final weight.

Figure 4: Visualizations of mutual attention.

The proposed attention mechanism makes the
relations between vertices explicit and inter-
pretable. We select three connected vertices in
Cora for example, denoted as A, B and C. From
Fig. 4, we observe that, though there exists cita-
tion relations with identical paper A, paper B and
C concern about different parts of A. The atten-
tion weights over A in edge #1 are assigned to
“reinforcement learning”. On the contrary, the
weights in edge #2 are assigned to “machine learn-
ing’”, “supervised learning algorithms” and “com-
plex stochastic models”. Moreover, all these key
elements in A can find corresponding words in B
and C. It’s intuitive that these key elements give
an exact explanation of the citation relations. The
discovered significant correlations between vertex
pairs reflect the effectiveness of mutual attention
mechanism, as well as the capability of CANE for
modeling relations precisely.

6 Conclusion and Future Work

In this paper, we propose the concept of Context-
Aware Network Embedding (CANE) for the first

1729



time, which aims to learn various context-aware
embeddings for a vertex according to the neigh-
bors it interacts with. Specifically, we implement
CANE on text-based information networks with
proposed mutual attention mechanism, and con-
duct experiments on several real-world informa-
tion networks. Experimental results on link pre-
diction demonstrate that CANE is effective for
modeling the relationship between vertices. Be-
sides, the learnt context-aware embeddings can
compose high-quality context-free embeddings.

We will explore the following directions in fu-
ture:

(1) We have investigated the effectiveness of
CANE on text-based information networks. In fu-
ture, we will strive to implement CANE on a wider
variety of information networks with multi-modal
data, such as labels, images and so on.

(2) CANE encodes latent relations between ver-
tices into their context-aware embeddings. Fur-
thermore, there usually exist explicit relations in
social networks (e.g., families, friends and col-
leagues relations between social network users),
which are expected to be critical to NE. Thus, we
want to explore how to incorporate and predict
these explicit relations between vertices in NE.

Acknowledgements

This work is supported by the 973 Program (No.
2014CB340501), the National Natural Science
Foundation of China (NSFC No. 61572273,
61532010, 61661146007), and Tsinghua Uni-
versity Initiative Scientific Research Program
(20151080406).

References
Edoardo M Airoldi, David M Blei, Stephen E Fienberg,

and Eric P Xing. 2008. Mixed membership stochas-
tic blockmodels. JMLR 9(Sep):1981–2014.

Phil Blunsom, Edward Grefenstette, and Nal Kalch-
brenner. 2014. A convolutional neural network for
modelling sentences. In Proceedings of ACL.

Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015.
Grarep: Learning graph representations with global
structural information. In Proceedings of CIKM.
pages 891–900.

Jifan Chen, Qi Zhang, and Xuanjing Huang. 2016.
Incorporate group information to enhance network
embedding. In Proceedings of CIKM.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger

Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. In Proceedings
of EMNLP.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. JMLR 9:1871–
1874.

Aditya Grover and Jure Leskovec. 2016. Node2vec:
Scalable feature learning for networks. In Proceed-
ings of KDD.

James A Hanley and Barbara J McNeil. 1982. The
meaning and use of the area under a receiver operat-
ing characteristic (roc) curve. Radiology 143(1):29–
36.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network architec-
tures for matching natural language sentences. In
Proceedings of NIPS. pages 2042–2050.

Rie Johnson and Tong Zhang. 2014. Effective
use of word order for text categorization with
convolutional neural networks. arXiv preprint
arXiv:1412.1058 .

Yoon Kim. 2014. Convolutional neural networks for
sentence classification.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of ICLR.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Proceedings of NIPS. pages 3294–3302.

Jure Leskovec, Jon Kleinberg, and Christos Faloutsos.
2005. Graphs over time: densification laws, shrink-
ing diameters and possible explanations. In Pro-
ceedings of KDD. pages 177–187.

Andrew McCallum, Kamal Nigam, Jason Rennie, and
Kristie Seymore. 2000. Automating the construc-
tion of internet portals with machine learning. In-
formation Retrieval Journal 3:127–163.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of ICIR.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS. pages 3111–3119.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.
2014. Deepwalk: Online learning of social repre-
sentations. In Proceedings of KDD. pages 701–710.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing 45(11):2673–2681.

1730



Xiaofei Sun, Jiang Guo, Xiao Ding, and Ting Liu.
2016. A general framework for content-enhanced
network representation learning. arXiv preprint
arXiv:1610.02906 .

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of ACL.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun
Yan, and Qiaozhu Mei. 2015. Line: Large-scale in-
formation network embedding. In Proceedings of
WWW. pages 1067–1077.

Lei Tang and Huan Liu. 2009. Relational learning
via latent social dimensions. In Proceedings of
SIGKDD. pages 817–826.

Cunchao Tu, Hao Wang, Xiangkai Zeng, Zhiyuan Liu,
and Maosong Sun. 2016a. Community-enhanced
network representation learning for network analy-
sis. arXiv preprint arXiv:1611.06645 .

Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, and
Maosong Sun. 2016b. Max-margin deepwalk: Dis-
criminative learning of network representation. In
Proceedings of IJCAI.

Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Struc-
tural deep network embedding. In Proceedings of
KDD.

Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun,
and Edward Y Chang. 2015. Network representa-
tion learning with rich text information. In Proceed-
ings of IJCAI.

1731


	CANE: Context-Aware Network Embedding for Relation Modeling

