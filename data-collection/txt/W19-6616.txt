Proceedings of MT Summit XVII, volume 1

Dublin, Aug. 19-23, 2019 | p. 162

Selecting Informative Context Sentence by Forced Back-Translation

Ryuichiro Kimura†, Shohei Iida†, Hongyi Cui†, Po-Hsuan Hung†,

Takehito Utsuro† and Masaaki Nagata‡

†Graduate School of Systems and Information Engineering, University of Tsukuba, Japan

‡NTT Communication Science Laboratories, NTT Corporation, Japan

Abstract

the most

As one of the contributions of this paper,
this paper ﬁrst explores the upper bound
of context-based neural machine transla-
tion and attempt to utilize previously un-
used context information. We found that, if
we could appropriately select the most in-
formative context sentence for a given in-
put source sentence, we could boost trans-
lation accuracy as much as approximately
10 BLEU points. This paper next explores
a criterion to select
informa-
tive context sentences that give the high-
est BLEU score. Applying the proposed
criterion, context sentences that yield the
highest forced back-translation probability
when back-translating into the source sen-
tence are selected. Experimental results
with Japanese and English parallel sen-
tences from the OpenSubtitles2018 corpus
demonstrate that, when the context length
of ﬁve preceding and ﬁve subsequent sen-
tences are examined,
the proposed ap-
proach achieved signiﬁcant improvements
of 0.74 (Japanese to English) and 1.14 (En-
glish to Japanese) BLEU scores compared
to the baseline 2-to-2 model, where the or-
acle translation achieved upper bounds im-
provements of 5.88 (Japanese to English)
and 9.10 (English to Japanese) BLEU
scores.

1 Introduction
Recently, neural machine translation (NMT) mod-
els (Sutskever et al., 2014; Luong et al., 2015;
c(cid:2) 2019 The authors. This article is licensed under a Creative
Commons 4.0 licence, no derivative works, attribution, CC-
BY-ND.

Vaswani et al., 2017) have made remarkable
progress. Most NMT models are designed to
translate a single sentence and do not accept in-
put greater than one sentence, i.e., input sentences
that include additional context information. How-
ever, recently, several approaches that attempt to
translate inputs with more than one sentence have
been proposed (Tiedemann and Scherrer, 2017; Li-
bovick´y and Helcl, 2017; Maruf and Haffari, 2018;
Miculicich et al., 2018; Bawden et al., 2018; Voita
et al., 2018; Tu et al., 2018). These approaches to
context-based NMT models can be roughly cate-
gorized according to the width of the context con-
sidered in those models. A typical approach is
to consider the sentence immediately preceding
the source sentence to be translated as the con-
text (Tiedemann and Scherrer, 2017; Libovick´y
and Helcl, 2017; Bawden et al., 2018; Voita et al.,
2018). Context-based NMT models can be fur-
ther categorized according to whether the source
and context sentences are encoded using a sin-
gle (Tiedemann and Scherrer, 2017) or multiple
encoders (Libovick´y and Helcl, 2017; Bawden et
al., 2018; Voita et al., 2018). Another approach
considers a much wider context than the imme-
diately preceding sentence, e.g., three preceding
sentences (Miculicich et al., 2018), preceding sen-
tences within the document (Tu et al., 2018), and
all preceding and subsequent sentences within the
document (Maruf and Haffari, 2018).

Such approaches to context-based NMT models
possibly outperform existing models that only ac-
cept a single sentence to be translated. Note that
we refer to the model that only accepts a single
sentence as a “1-to-1” model. Among these exist-
ing models, the 2+2 or 2-to-2 model (Tiedemann
and Scherrer, 2017) uses the sentence immediately
preceding the source sentence to be translated as

Proceedings of MT Summit XVII, volume 1

Dublin, Aug. 19-23, 2019 | p. 163

1-to-1 (baseline)
2-to-2 (baseline)

selection from 20-best of 2-to-2 (baseline)

by 2-to-2 back-translation

1-to-1 + 2-to-2

(immediately preceding sent.)

1-to-1 + 2-to-2 (1st ∼ 5th preceding sents.)
1-to-1 + 2-to-2 (1st ∼ 5th subsequent sents.)
(1st ∼ 5th preceding + subsequent sents.)

1-to-1 + 2-to-2

BLEU

Ja-En
15.52
16.52

En-Ja
11.48
12.36

16.69 / —

12.61 / —

Oracle
BLEU

Ja-En En-Ja

—
—

—

—
—

—

17.04∗∗ / 16.51
17.25∗∗ / 16.67
17.04∗∗ / 16.52
17.26∗∗ / 16.68

13.24∗∗ / 12.47
13.50∗∗ / 13.14∗∗
13.46∗∗ / 13.13∗∗
13.02∗∗ / 12.81∗∗

18.15

15.61

21.09
20.84

19.55
19.51

22.40

21.46

Table 1: Evaluation results (maximizing forced back-translation probability / maximizing back-translation sentence-BLEU)
(∗∗ represents signiﬁcant difference (p < 0.01) against baseline 2-to-2 model)

an extended context. Here the context sentence
is concatenated to the source sentence using the
(cid:3)CONCAT(cid:4) token. The 2-to-2 model is easy to
implement into existing 1-to-1 models: however,
it only considers the immediately preceding sen-
tence as context. Thus, it is necessary to consider
much wider contexts such as the second through
ﬁfth preceding sentences and the ﬁrst through ﬁfth
subsequent sentences. We conducted an empirical
study that revealed that, in some cases, among the
ﬁrst through ﬁfth sentences preceding and subse-
quent to the source sentence, the most informative
sentence, i.e, the sentence that returns the high-
est BLEU score, may not be the sentence imme-
diately preceding the source sentence. We mea-
sured oracle BLEU scores by selecting context
sentences that give the maximum sentence-BLEU
scores among the ﬁve preceding and subsequent
sentences, as shown in Table 1 and Figure 1. Then,
we found that, if we could select the most infor-
mative context sentence for a given input source
sentence, we can improve translation accuracy by
as much as approximately 10 BLEU points, as in-
dicated by the oracle BLEU scores in Table 1 and
Figure 1. More speciﬁcally, compared to the base-
line 2-to-2 model, the oracle translation achieved
upper bound improvements of 5.88 (Japanese to
English) and 9.10 (English to Japanese) BLEU
scores.

Considering this result, within the framework
this
of the 2-to-2 context based NMT model,
study explored how to select the most informa-
tive context sentences that give the highest BLEU
score among the ﬁrst ﬁve preceding and subse-

quent sentences1. Here, we used the Transformer
model (Vaswani et al., 2017) as the base 1-to-1
model. To select the translation with the highest
BLEU score among the 11 translations (i.e., those
translated by the 1-to-1 and 10 2-to-2 models),
we propose an approach that selects the transla-
tion that yields the highest forced back-translation
probability when back-translating into the source
sentence. The evaluation results shown in Table 1
demonstrate that the proposed approach achieves
signiﬁcant BLEU score improvements over the
baseline 2-to-2 and 1-to-1 models. More specif-
ically, over the baseline 2-to-2 model, the pro-
posed approach achieved signiﬁcant improvements
of 0.74 (Japanese to English) and 1.14 (English to
Japanese) BLEU scores.

2 Selective Extended Context Decoding

Tiedemann and Scherrer (2017) proposed the 2-
to-2 model, which uses the sentence immediately
preceding the source sentence to be translated as
the extended context. We extend the 2-to-2 model
by considering the ﬁrst ﬁve preceding and ﬁrst
ﬁve subsequent sentences. In our extended 2-to-2
context-based NMT model, the immediately pre-
ceding sentence, the second through ﬁfth preced-
ing sentences, and the ﬁrst through ﬁfth subse-
1An obvious alternative to this approach is to simply em-
ploy 3-to-3 (or more) models using an approach similar to
the 2-to-2 model that concatenates context sentences using
the (cid:4)CONCAT(cid:5) token. However, due to the upper bound re-
striction of GPU memory, it is impractical to employ such
3-to-3 (or more) models. Furthermore, our preliminary evalu-
ation result also indicates that the 3-to-3 model underperforms
compared to the proposed approach.

Proceedings of MT Summit XVII, volume 1

Dublin, Aug. 19-23, 2019 | p. 164

24

22

20

18

16

14

12

10

 

+
g
n
i
d
e
c
e
r
p

 
t
n
e
u
q
e
s
b
u
s

Oracle BLEU

(cid:90)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:20)(cid:16)(cid:87)(cid:82)(cid:16)(cid:20)(cid:3)(cid:3)(cid:87)(cid:85)(cid:68)(cid:81)(cid:86)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:90)(cid:18)(cid:82)(cid:3)(cid:3)(cid:20)(cid:16)(cid:87)(cid:82)(cid:16)(cid:20)(cid:3)(cid:3)(cid:3)(cid:87)(cid:85)(cid:68)(cid:81)(cid:86)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

 

g
n
i
d
e
c
e
r
p

y
l
n
o

1-to-1

 
t
n
e
u
q
e
s
b
u
s

y
l
n
o

1-to-1 +
2-to-2
(1st 

preceding 
sentence)

2-to-2
(1st 
(1st 

preceding 
preceding 
sentence)

22

20

18

16

14

12

10

8

 

+
g
n
i
d
e
c
e
r
p

 
t
n
e
u
q
e
s
b
u
s

Oracle BLEU

(cid:90)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:20)(cid:16)(cid:87)(cid:82)(cid:16)(cid:20)(cid:3)(cid:3)(cid:87)(cid:85)(cid:68)(cid:81)(cid:86)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:90)(cid:18)(cid:82)(cid:3)(cid:3)(cid:20)(cid:16)(cid:87)(cid:82)(cid:16)(cid:20)(cid:3)(cid:3)(cid:3)(cid:87)(cid:85)(cid:68)(cid:81)(cid:86)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

 

g
n
i
d
e
c
e
r
p

y
l
n
o

1-to-1

 
t
n
e
u
q
e
s
b
u
s

y
l
n
o

1-to-1 +
2-to-2
(1st 

preceding 
sentence)

2-to-2
(1st 
(1st 

preceding 
preceding 
sentence)

1st through 5th
context sentences

1st through 5th
context sentences

(a) Ja-En

(b) En-Ja

Figure 1: Oracle BLEU and BLEU scores of baseline 2-to-2 (y22

0 (x−1, x0)) and 1-to-1 (y11

0 (x0)) models

quent sentences are considered candidates for con-
catenation to the source sentence. Then, among the
ﬁrst through ﬁfth preceding and ﬁrst through ﬁfth
subsequent sentences, we select the most informa-
tive context sentences in the 2-to-2 context-based
NMT model.

In this framework,

this paper employs the
notation described below.
xi and yi (i =
0,±1, . . . ,±5) denote the source and target sen-
tences, respectively. x0 denotes the source sen-
tence to be translated, while y0 denotes its trans-
lation in the target language. x−1 denotes the
context sentence in the source language immedi-
ately preceding x0, xi (i = −2, . . . ,−5) the
second through ﬁfth preceding sentences, and xi
(i = +1, . . . , +5) the ﬁrst through ﬁfth subse-
quent sentences.
In order to represent the direc-
tion of translation as the target language l (x or y
in this paper) of the translation, the model m (1-
to-1 or 2-to-2) used for the translation, the index
i (i = 0,±1, . . . ,±5) of the translated sentence,
and the source sentence s and the context sentence
c in the source language, this paper employs a gen-
eral notation to represent the translated sentence in
the target language as below:

Translation by 1-to-1 Model
For example, the target sentence translated from
x0 by the base 1-to-1 Transformer model is de-
noted as

y11
0 (x0).

In this case, the source sentence s is x0, and is
translated without a context sentence. Table 2
shows a typical Japanese subject zero pronoun
case improved by the proposed informative con-
text sentence selection approach by forced back-
translation, where the bottom line represents the
translation by the base 1-to-1 model. In Table 2,
0 (x0), i.e., the translation of x0 by the base 1-
y11
to-1 model is:

If we leave now , we’ll never get back .

Here, the base 1-to-1 model fails in the translation
of the Japanese zero pronoun subject in x0, i.e., it
is not translated as “you”, but translated as “we”.

Translation by Baseline 2-to-2 Model
The target sentence translated from x0 by the base-
line 2-to-2 Transformer model which uses the sen-
tence x−1 immediately preceding x0 as the ex-
tended context is denoted as

lm
i (c, s).

y22
0 (x−1, x0).

Proceedings of MT Summit XVII, volume 1

Dublin, Aug. 19-23, 2019 | p. 165

Source sentences

Target sentences

Forced

back-translation

probability
/ sentence-

BLEU

—

4th preceding
sentence x−4: 私に逆らうなら お前 は何

もなくなるぞ。(If you defy
me , you will have nothing
.)

Immediately
preceding
sentence x−1:

Source
sentence x0:

それが望みなのか ? (Is that
what you want ?)

出て行けば、戻れなくなる
ぞ。

Reference
translation:

Translation
y22
0 (x−4, x0)
by 2-to-2 model:

Walk out now and
you may never re-
turn .

If you leave , you’ll
never get back .

—

5.8×10−8
/ 14.99

Translation
y22
0 (x−1, x0)
by baseline
2-to-2 model:

Translation
y11
0 (x0)
by baseline
1-to-1 model:

If we leave now
, we’ll never get
back .

If we leave now
, we’ll never get
back .

2.8×10−10
/ 13.55

1.3×10−8
/ 10.57

Table 2: Example improvements over baseline 2-to-2 y22

0 (x−1, x0) (Ja-En)

(a) pronoun translation

In this case, x0 is concatenated with the im-
mediately preceding sentence x−1 as “ x−1
(cid:3)CONCAT(cid:4) x0”, and the concatenated sentences
are translated by the baseline 2-to-2 Transformer
model. We denote the translated (concatenated)
sentences as follows:

−1(x−1, x0) (cid:3)CONCAT(cid:4) y22
y22

0 (x−1, x0)

−1(x−1, x0) and y22

where y22
0 (x−1, x0) are the
translations of x−1 and x0, respectively.
In the
case of Table 2, the immediately preceding sen-
tence x−1 and the source sentence x0 are:
x−1: それが望みなのか ?
x0: 出て行けば、戻れなくなるぞ。

(Is that what you want ?)

(Walk out now and you may never return .)

Then, y22

0 (x−1, x0), i.e., the translation of x0 is:

If we leave now , we’ll never get back .

Translation by 2-to-2 Model with a Context
Sentence x−4
Similarly, the ﬁrst line of Table 2 also shows the
target sentence translated from x0 by the 2-to-2
Transformer model which uses the fourth sentence
x−4 preceding x0 as the extended context. In this
case, the translated sentence is denoted as

y22
0 (x−4, x0).

As shown in Table 2, the fourth preceding sentence
x−4 and the source sentence x0 are:
x−4: 私に逆らうならお前は何もなくなるぞ。
(If you defy me , you will have nothing .)
x0: 出て行けば、戻れなくなるぞ。

(Walk out now and you may never return .)
“x−4

concatenated

sentences

the

Then,
(cid:3)CONCAT(cid:4) x0” are translated into:
−4(x−4, x0) (cid:3)CONCAT(cid:4) y22
y22

0 (x−4, x0).
0 (x−4, x0), i.e., the translation of x0 is:

Here, y22

Again, the baseline 2-to-2 model fails in the trans-
lation of the Japanese zero pronoun subject in x0,
i.e., it is not translated as “you”, but translated as
“we”.

If you leave , you’ll never get back .

This time,
the fourth preceding source sen-
tence x−4 includes the Japanese pronoun “お前”

Proceedings of MT Summit XVII, volume 1

Dublin, Aug. 19-23, 2019 | p. 166

(mostly translated as “you” in English in the train-
ing corpus): thus, the translation y22
0 (x−4, x0) by
the 2-to-2 model successfully includes the transla-
tion of the Japanese zero pronoun subject in x0 as
“you”. This then contributes to having the highest
forced back-translation probability and sentence-
BLEU score with the reference translation com-
pared to y11
0 (x0) (translated by the base 1-to-1
model) and y22
0 (x−1, x0) (translated by the base-
line 2-to-2 model), in which the Japanese zero pro-
noun subject is translated as “we” in both cases.
This analysis clearly indicates that the baseline
2-to-2 model is insufﬁcient relative to correctly
translating Japanese zero pronouns into English.

0 (x−5, x0)

0 (x−1, x0), . . . , y22

Translation by 2-to-2 Model with a Context
Sentence xi (i = ±1, . . . ,±5)
More generally,
in addition to translation
0 (x0) obtained by the base 1-to-1 Trans-
y11
former model, we prepare 10 translated sen-
tences y22
and
0 (x+5, x0) as candidate
y22
0 (x+1, x0), . . . , y22
translations, each of which is generated using the
2-to-2 model based on the standard Transformer
model. Each y22
0 (xi, x0) (i = ±1, . . . ,±5)
of these 10 translated sentences is generated by
the 2-to-2 model, where one of the ﬁrst through
ﬁfth preceding and subsequent
sentences xi
(i = ±1, . . . ,±5) is used as the context sentence
of the 2-to-2 model2.
In the 2-to-2 model, only
one of the ﬁve preceding and subsequent sentences
xi (i = ±1, . . . ,±5) is concatenated to the source
sentence x0 using the (cid:3)CONCAT(cid:4) token as:

xi (cid:3)CONCAT(cid:4) x0.

Then, the concatenated sentences are translated
by the 2-to-2 Transformer model. We denote the
translated (concatenated) sentences as follows:

0 (xi, x0)

i (xi, x0) (cid:3)CONCAT(cid:4) y22
y22
i (xi, x0) and y22

where y22
lations of xi and x0, respectively.
3 Selecting Informative Context

0 (xi, x0) are the trans-

Sentences with Maximum Forced
Back-translation Probability

examined

In the proposed method of selecting a transla-
tion among the 11 candidate translations y11
0 (x0),
2We
translations
0 (x±1, x0), . . . , y22
y22
0 (x±5, x0) are exactly the same
as y11
0 (x0). The rates of cases where none of the 10
translations was exactly the same as y11
0 (x0). were 54% for
Japanese to English and 63% for English to Japanese.

how many

the

10

of

0 (x±5, x0), we select the
0 (x±1, x0), . . . , y22
y22
translation that yields the highest forced back-
translation probability when back-translating into
the source sentence. In this context, forced back-
translation is deﬁned as forced decoding from a
translated target sentence to its source sentence.

Here, assume the source sentence x0 of word
length n with a context sentence xi is given. For
the back-translation translation model, we used the
2-to-1 Transformer model with the setup described
in Section 5, rather than the 1-to-1 Transformer
model. This is simply because, in forced back-
translation into x0, the 2-to-1 model considers
0 (xi, x0), while the 1-
i (xi, x0) and y22
both y22
0 (xi, x0) (translation of
to-1 model considers y22
the source sentence x0) only, but not y22
i (xi, x0)
(translation of the context sentence xi). We
i (xi, x0) and
assume that considering both y22
0 (xi, x0) in forced back-translation will yield
y22
forced back-translation probabilities that are sig-
niﬁcantly informative3.

The forced back-translation probability score of
the source sentence word xj (1 ≤ j ≤ n) of x0 is
expressed as follows.
bj = − log p(cid:2) xj|x<j, y22

i (xi, x0),

y22

0 (xi, x0)(cid:3)

i (xi, x0) and y22

From y22
0 (xi, x0), the forced
back-translation probability score of the entire
source sentence x0 is obtained as the sum of each
bj.

B(cid:2) x0, y22

i (xi, x0), y22

0 (xi, x0)(cid:3) = (cid:4)j

bj

Similarly, the forced back-translation probability
score of the entire source sentence x0 for the base
1-to-1 model is obtained as below:

bj = − log p(cid:2) xj|x<j, y11
0 (x0)(cid:3)
0 (x0)(cid:3) = (cid:4)j

B(cid:2) x0, y11

bj

Finally, among the 11 candidate translations
0 (x±5, x0), we
y11
0 (x0), y22
the translation that yields the highest
select

0 (x±1, x0), . . . , y22

3In the evaluation discussed in Section 7.1, forced back-
translation using the 1-to-1 model achieved merely the same
BLEU scores as that of the 2-to-1 model.

Proceedings of MT Summit XVII, volume 1

Dublin, Aug. 19-23, 2019 | p. 167

forced back-translation probability B when back-
translating into the source sentence x0 as below:

i=0,±1,...,±5(cid:5) B(cid:2) x0, y11
B(cid:2) x0, y22

argmax

y22

i (xi, x0),

0 (x0)(cid:3)
0 (xi, x0)(cid:3)

(i = 0)
(i (cid:6)= 0)

Employing the forced back-translation prob-
ability differs from existing approaches (Rapp,
2009; Li and Jurafsky, 2016; Goto and Tanaka,
2017; Kimura et al., 2017) that incorporate back-
translation from the translated target sentence to
the source sentence. Rapp (2009) employed the
BLEU score between the source sentence and
source language sentence back-translated from the
target translated sentence in an automatic MT eval-
uation context. Li and Jurafsky (Li and Juraf-
sky, 2016) proposed to re-rank decoded transla-
tions based on mutual information between source
and target sentences x and y i.e., the probabilities
p(y | x) and p(x | y). Goto and Tanaka (2017)
and Kimura et al. (2017) also employed the ra-
tio of forced back-translation probabilities in the
context of detecting untranslated content in NMT.
These approaches differ from the proposed use of
the forced back-translation probability4.

4 Selecting Informative Context

Sentences with Maximum
Back-translation Sentence-BLEU

Rapp (2009) proposed an approach of using BLEU
score between the source sentence and source
language sentence back-translated from the tar-
get translated sentence in an automatic MT eval-
uation context.
Based on Rapp (2009), we
employ another approach to selecting informa-
tive context sentences, where back-translation
sentence-BLEU is maximized. As in the case
of selecting informative context sentences with
maximum forced back-translation probability pre-
sented in the previous section, candidate trans-
lations are the same as those 11 candidates
0 (x±5, x0). For
y11
0 (x0), y22
each of those 11 candidate translations, its back-

0 (x±1, x0), . . . , y22

translation back-tran(cid:6)i(cid:7)5 into the source language

4The proposed approach is included among those that con-
sider a much wider context than the immediately preced-
ing sentence, e.g., the approaches proposed by Miculicich et
al. (2018), Tu et al. (2018), and Maruf and Haffari (2018).
5For the back-translation translation model, we used the 1-
to-1 Transformer model (denoted as back-tran11) when back-

is given as below:

back-tran(cid:6)i(cid:7) =
(cid:5) back-tran11(cid:6) y11
back-tran21(cid:6) y22

y22

i (xi, x0),

0 (x0)(cid:7)(cid:7)
0 (xi, x0)(cid:7)

(i = 0)
(i (cid:6)= 0)

Then, we measure the sentence-BLEU score be-
tween the source sentence x0 and each back-
translation. We then select the one that gives the
highest sentence-BLEU score.

argmax

i=0,±1,...,±5

sent-BLEU(cid:6) x0, back-tran(cid:6)i(cid:7)(cid:7)

5 Dataset and Experimental Setup
The dataset used for the oracle translation statis-
tics and the BLEU evaluation comprised 2,083,576
English and Japanese parallel sentence pairs from
Opensubtitles 2018 (Lison et al., 2018). Note that
we followed Tiedemann and Scherrer (2017) to
create the extended context dataset. Here, 90%
of the dataset (1,876,624 sentence pairs) was used
for training, 5% (104,379 sentence pairs) for de-
velopment, and 5% (102,573 sentence pairs) for
oracle statistics and evaluation. Here, of these
102,573 sentence pairs, only 10,000 pairs were
actually used for oracle statistics and evaluation6
7. Throughout the paper, we approximate that all
the 2-to-2 models are trained with the immediately
preceding sentence as the context.

6 Oracle Translation of Context-based

NMT

When measuring the oracle sentence-BLEU score,
for each source sentence x0, we select the sentence
translating y11
0 (x0) (i = 0), while we used the 2-to-1
Transformer model (denoted as back-tran21) with the setup
described in section 5 when back-translating y22
0 (xi, x0)
(i (cid:6)= 0, i.e., translated from x0 with a context sentence by
the 2-to-2 model ).
6In training and development, the encoder rejects input sen-
tences (source sentence concatenated with the context sen-
tence for the 2-to-2 models) with greater than 50 tokens. Av-
erage token length of the 10,000 pairs for oracle statistics and
evaluation is 7.9 (English) and 6.9 (Japanese).
7Experimental setup is as follows: Tokenizers are Moses
tokenizer (Koehn et al., 2007) for English and MeCab (
http://taku910.github.io/mecab/ ) for Japanese
tokenization. OpenNMT-py (Klein et al., 2017) is used for
training and testing NMT models. 50,000 vocabulary sizes
are employed for both English and Japanese. Embedding
sizes are 512. Encoder and decoder are with six layers with
batch size as 4,096 and dropout rate as 0.3 and 100,000 steps
for training. Adam optimizer (Kingma and Ba, 2015) is used.
One NVIDIA Tesla P100 16GB GPU is used. MTEval Toolkit
( https://github.com/odashi/mteval ) is used to
measure BLEU, and Moses decoder’s sentence-bleu.cpp is
used to measure sentence-BLEU.

Proceedings of MT Summit XVII, volume 1

Dublin, Aug. 19-23, 2019 | p. 168

with the maximum sentence-BLEU score among
the candidate translations after obtaining 11 candi-
dates ( y11
0 (x0) translated by the 1-to-1 model and
0 (xi, x0) (i=±1, . . . ,±5) translated by the 2-
y22
to-2 models). Figure 1 shows the oracle BLEU
scores for the following seven cases:

(i) among y22

and without y11

(ii) among y22

and without y11

0 (x0)

0 (xi, x0) (i = ±1, . . . ,±5) with
0 (xi, x0) (i = −1, . . . ,−5) with
0 (xi, x0) (i = +1, . . . , +5) with

0 (x0)

(iii) among y22

and without y11

0 (x0)

(iv) between y11

0 (x0) and y22

0 (x−1, x0)

and the BLEU scores of the baseline 1-to-1 (
0 (x0) ) and 2-to-2 models with the immediately
y11
preceding sentence as the context ( y22
0 (x−1, x0)
). For all three 2-to-2 model cases with the can-
didate translation obtained by the 1-to-1 model,
the oracle BLEU increased by including y11
0 (x0).
the oracle BLEU score increases
Furthermore,
as more candidates are considered.
Table 1
shows that, by considering y22
0 (xi, x0) (i =
−5, . . . ,−2, +1, . . . , +5) in addition to y11
0 (x0)
0 (x−1, x0), the oracle BLEU score im-
and y22
proves by approximately four points for Japanese
to English and six points for English to Japanese.
These results indicate that longer contexts yield
obvious beneﬁt for the 2-to-2 context-based NMT
model, which is the primary motivation for select-
ing informative context sentences in that model.

7 Evaluation

7.1 Evaluation Results
For both English to Japanese and Japanese to
English directions, Table 1 shows the BLEU
scores obtained by selecting the translation can-
didate that maximizes the forced back-translation
and the back-translation sentence-BLEU score.
For the proposed method, we compare the fol-
lowing translation candidate cases:8 (i) between
0 (x0) and y22
y11
0 (x0)
0 (xi, x0) (i = −1, . . . ,−5), (iii) among
and y22
0 (x0) and y22
y11
0 (xi, x0) (i = +1, . . . , +5),
(iv) among y11
0 (xi, x0) (i =
±1, . . . ,±5). Compared to the BLEU scores of
8Throughout the evaluation results of this paper, when obtain-
ing the forced back-translation probability for y11, we used
the 1-to-1 Transformer model as the back-translation transla-
tion model.

0 (x−1, x0), (ii) among y11

0 (x0) and y22

0 (x−1, x0), all BLEU scores ob-
0 (x0) and y22
y11
tained by the proposed method demonstrate sig-
niﬁcant improvement (p < 0.01), except for the
Japanese to English translation obtained by maxi-
mizing the back-translation sentence-BLEU score.
By comparing the BLEU scores of y11
0 (x0),
the oracle among them, and
y22
0 (x−1, x0),
the selection between them by maximizing the
forced back-translation,
the selection between
0 (x−1, x0) by maximizing
0 (x0) and y22
y11
forced back-translation achieves BLEU scores that
are comparable to the oracle BLEU scores. Thus,
we conclude that the proposed method contributes
to selecting better
translation between those
candidates. However, the proposed method can-
not select informative context sentences among
y22
0 (xi, x0) (i = −5, . . . ,−2, +1, . . . , +5),
because
adding
y22
0 (xi, x0) (i = −5, . . . ,−2, +1, . . . , +5),
and
to
0 (x−1, x0) yields little or no gain in BLEU
y22
score. Note that this does not coincide with im-
proving the oracle BLEU score by approximately
four points for Japanese to English and six points
for English to Japanese with the overall 11 trans-
lation candidates. Thus, it can be concluded that
further study is required to appropriately select
the informative context sentences among the 11
candidates such that the BLEU score becomes
much closer to the oracle BLEU score.

y11
0 (x0)

candidates

translation

obtained

results

the

by

Another important comparison with a baseline
is also shown as “selection from 20-best of 2-
to-2 (baseline) by 2-to-2 back-translation” in Ta-
ble 1. With this baseline, it is intended to exam-
ine whether the ﬁve preceding and subsequent sen-
tences introduced in the proposed method are suf-
ﬁciently informative compared to other well stud-
ied translation candidates such as n-best transla-
tions. Speciﬁcally, the baseline 2-to-2 model with
the immediately preceding sentence as the context
is employed to generate 20-best translations, and
then, out of those generated 20-best translations,
the one with the maximum forced back-translation
into the source sentence is selected9. As shown
in Table 1, this baseline performed worse than the
proposed approach. From this result, it is obvious
that the proposed approach of introducing ﬁve pre-
ceding and subsequent sentences as the context is

9We compare the 2-to-2 and 2-to-1 models in the step of
forced back-translation here, where the 2-to-2 model outper-
formed the 2-to-1 model.
In Table 1, we show the results
obtained by the 2-to-2 model.

Proceedings of MT Summit XVII, volume 1

Dublin, Aug. 19-23, 2019 | p. 169

Ja-En

En-Ja

Ja-En

En-Ja

category of phe-
nomena
synonymous
expression
pronoun
untranslated by
baseline
article
other
manually judged

(comparable)
(baseline wins)
(baseline loses)

total

fail

succ-
eed

14

12

7
5

0
11

10
3
0
50

2
0

1
2

18
0
15
50

succ-
eed

17

2
10

0
7

13
1
0
50

fail

10

0
0

0
3

27
0
10
50

category of phe-
nomena
synonymous ex-
pression
pronoun
untranslated by
1-to-1
article
other
manually judged
(comparable)
(1-to-1 wins)
(2-to-2 wins)

total

2-to-2

wins

16

1-to-1

wins

20

2
8

1
7

15
1
0
50

2
0

0
8

15
0
5
50

2-to-2

wins

14

1
12

0
3

18
2
0
50

1-to-1

wins

22

2
2

0
4

16
0
4
50

Table 3: Distribution of oracle translation phenomena
(through manual analysis of 50 examples) (proposed method
succeeds / fails in identifying those oracle translations)

Table 4: Distribution of phenomena where baseline 2-to-2
y22
0 (x−1, x0) wins v.s. 1-to-1 y11
0 (x0) wins (through man-
ual analysis of 50 examples)

much more informative than 20-best translations
with just the preceding sentence as the context.

7.2 Analysis of Improvements and Errors
To analyze typical cases relative to the improve-
ments and errors of the proposed approach, we ran-
domly select 50 success cases and 50 failure cases
when identifying oracle translations using the pro-
posed method. Speciﬁcally, we ﬁrst collect cases
where oracle translation is selected from y11
0 (x0)
or y22
0 (xi, x0) (i = −5, . . . ,−2, +1, . . . , +5),
rather than from y22
0 (x−1, x0). Then, from these
cases, we randomly select 50 examples for each of
the following cases.

(a) Proposed approach (maximizing forced back-
translation) successfully identiﬁes collected
oracle translations.

(b) Proposed approach (maximizing forced back-
translation) fails to identify collected oracle
translations.

Then, we manually categorize the 50 examples (for
each case) according to the phenomena in Table 3.
For both Japanese to English and English to
Japanese, nearly 30∼40% are categorized as “syn-
onymous expression”, where the proposed ap-
proach of maximizing forced back-translation suc-
cessfully selects the oracle translation that includes
the synonymous expression rather than exactly the
same expression (as in the reference translation).
Due to this synonymous expression, the sentence

has the highest sentence-BLEU score and is se-
lected as the oracle translation. Although this phe-
nomenon is top ranked among others, it is also top
ranked among the failure cases. Thus, it is nec-
essary to incorporate other criteria to reduce the
failure cases.

For comparison, we also categorize the phenom-
ena of randomly selected 50 cases when the base-
line 2-to-2 model outperforms the 1-to-1 model,
i.e., translation y22
0 (x−1, x0) by the baseline 2-
to-2 model achieves a sentence-BLEU score that
is greater than that of translation y11
0 (x0) by the
1-to-1 model. We also categorize the phenomena
of randomly selected 50 cases of its opposite, i.e.,
when the 1-to-1 model outperforms the baseline 2-
to-2 model. These results are shown in Table 4. As
can be seen, even in the comparison of the base-
line 2-to-2 and 1-to-1 models, the “synonymous
expression” category is top ranked.

It is interesting to compare the second and third
ranked categories, i.e., “pronoun translation” and
“untranslated by baseline,” among Japanese to En-
glish and English to Japanese in Tables 3 and 4.
The “pronoun translation” category is ranked high
only in the Japanese to English case with the pro-
posed approach (Table 3). Table 2 shows a typi-
cal Japanese subject zero pronoun case and its de-
tail is described in section 2. With the “untrans-
lated by baseline / 1-to-1” categories, it is obvi-
ous from Table 3 and Table 4 that the proposed ap-
proach outperforms the baseline 2-to-2 model for

Proceedings of MT Summit XVII, volume 1

Dublin, Aug. 19-23, 2019 | p. 170

Source sentences

Target sentences

Forced

back-translation

probability
/ sentence-

BLEU

—

Immediately
preceding
sentence x−1:

Source
sentence x0:

2nd
subsequent
sentence x+2:

お前の一挙一動がここに
お前を導いた。(Every step
you took led you to here .)

お前の受けた全ての苦しみ
はお前の 罪 に対する罰だ
った。

お 前 の 命 を 奪 う た め に
悪魔 が送ったものを見よ!
(See what the devil has sent
to claim you .)

Reference
translation:

Translation
y22
0 (x−1, x0)
by baseline
2-to-2 model:

Translation
y11
0 (x0)
by baseline
1-to-1 model:

Translation
y22
0 (x+2, x0)
by 2-to-2
model:

Every pain you suf-
fered was punish-
ment for your sins .

—

Every
suffering
you suffered was
your punishment .

the suffering
All
you’ve
had was
your punishment .

All your suffering
was punishment for
your sins .

3.4×10−19
/ 27.64

5.3×10−19
/ 16.62

5.9×10−14
/ 57.18

Table 5: Example improvements over baseline 2-to-2 y22

0 (x−1, x0) (Ja-En)

(b) untranslated by baseline

both Japanese to English and English to Japanese
directions. In addition, the baseline 2-to-2 model
outperforms the 1-to-1 model. Thus, it can be con-
cluded that the matter of untranslated content in
context-based NMT can be handled consistently
by appropriately extending the range of the context
considered within a certain framework of context-
based NMT models such as the 2-to-2 model. For
example, as shown in Table 5, the baseline 2-to-2
model fails to produce the word “sins” in its trans-
lation. In contrast, the translation y22
0 (x+2, x0)
obtained by the 2-to-2 model with the second sub-
sequent source sentence x+2 as the context sen-
tence successfully includes the word “sins,” proba-
bly because the second subsequent source sentence
x+2 includes “悪魔” (“devil”).

By examining the cases of improvements over
the baseline 2-to-2 model, we observe that the es-
sential advantage of the proposed approach is that
the measure of forced back-translation probability
can distinguish translation errors from relatively
acceptable translations, with which the sentence-
BLEU score with the reference translation is typi-
cally higher than that of the baseline 2-to-2 model.
As a result, we conclude that it is unnecessary to
consider a context with a much greater number of
sentences, such as 3-to-3 (or higher) models.

8 Conclusion

Within the framework of the 2-to-2 context-based
NMT model, this paper has explored how to se-
lect the most informative context sentences that
provide the highest BLEU score among the ﬁve
preceding and ﬁve subsequent sentences.
In fu-
ture, we plan to compare the proposed method to
an existing approach (Li and Jurafsky, 2016) that
incorporates back-translation into the MT frame-
work. In addition, we plan to incorporate mono-
lingual techniques such as BERT (Devlin et al.,
2018) and neural coreference resolution (Lee et
al., 2017), to evaluate whether context sentences
(i.e., the second through ﬁfth sentences preceding
the source sentence and the ﬁrst through ﬁfth sen-
tences subsequent to the source sentence) are in
fact informative. Also, in the context of translation
quality estimation techniques (Specia et al., 2015),
the proposed approach of estimating the quality of
translation by maximizing forced back-translation
is novel and has never been studied so far in the
task of translation quality estimation.

Proceedings of MT Summit XVII, volume 1

Dublin, Aug. 19-23, 2019 | p. 171

Rapp, R. 2009. The back-translation score: Automatic
mt evaluation at the sentence level without reference
translations.
In Proc. 47th ACL and 4th IJCNLP,
pages 133–136.

Specia, L., G. Paetzold, and C. Scarton. 2015. Multi-
level translation quality prediction with QuEst++. In
Proc. 53rd ACL and 7th IJCNLP System Demonstra-
tions, pages 115–120.

Sutskever, I., O. Vinyals, and Q. V. Le. 2014. Sequence
to sequence learning with neural machine transla-
tion. In Proc. 27th NIPS, pages 3104–3112.

Tiedemann, J. and Y. Scherrer. 2017. Neural machine
translation with extended context. In Proc. 3rd Dis-
coMT, pages 82–92.

Tu, Z., Y. Liu, S. Shi, and T. Zhang. 2018. Learn-
ing to remember translation history with a continu-
ous cache. Transactions of ACL, 6:407–420.

Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. Gomez, L. Kaiser, and I. Polosukhin.
2017. Attention is all you need. In Proc. 30th NIPS,
pages 5998–6008.

Voita, E., P. Serdyukov, R. Sennrich, and I. Titov.
2018. Context-aware neural machine translation
learns anaphora resolution. In Proc. 56th ACL, pages
1264–1274.

References
Bawden, R., R. Sennrich, A. Birch, and B. Haddow.
2018. Evaluating discourse phenomena in neural
machine translation.
In Proc. NAACL-HLT, pages
1304–1313.

Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova.
2018. BERT: Pre-training of deep bidirectional
transformers for language understanding. In CoRR,
volume abs/1810.04805.

Goto, I. and H. Tanaka. 2017. Detecting untranslated
content for neural machine translation. In Proc. 1st
NMT, pages 47–55.

Kimura, R., Z. Long, T. Utsuro, T. Mitsuhashi, and
M. Yamamoto. 2017. Effect on reducing untrans-
lated content by neural machine translation with a
large vocabulary of technical terms.
In Proc. 7th
PSLT, pages 13–24.

Kingma, D. P. and J. Ba. 2015. Adam: A method for

stochastic optimization. Proc. ICLR.

Klein, G., Y. Kim, Y. Deng, J. Senellart, and A. M.
Rush. 2017. OpenNMT: Open-source toolkit for
neural machine translation.
In Proc. 55th ACL,
pages 67–72.

Koehn, P., H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation.
In Proc. 45th ACL,
pages 177–180.

Lee, K., L. He, M. Lewis, and L. Zettlemoyer. 2017.
End-to-end neural coreference resolution. In Proc.
EMNLP, pages 188–197.

Li, J. and D. Jurafsky. 2016. Mutual information and
diverse decoding improve neural machine transla-
tion. In CoRR, volume abs/1601.00372.

Libovick´y, J. and J. Helcl. 2017. Attention strategies
for multi-source sequence-to-sequence learning. In
Proc. 55th ACL, pages 196–202.

Lison, P., J. Tiedemann, and M. Kouylekov. 2018.
Opensubtitles2018: Statistical rescoring of sentence
alignments in large, noisy parallel corpora. In Proc.
11th LREC, pages 1742–1748, May 7-12, 2018.

Luong, T., H. Pham, and C. D. Manning. 2015. Ef-
fective approaches to attention-based neural machine
translation.
In Proc. 2015 EMNLP, pages 1412–
1421.

Maruf, S. and G. Haffari. 2018. Document context
neural machine translation with memory networks.
In Proc. 56th ACL, pages 1275–1284.

Miculicich, L., D. Ram, N. Pappas, and J. Hender-
son. 2018. Document-level neural machine trans-
lation with hierarchical attention networks. In Proc.
EMNLP, pages 2947–2954.

