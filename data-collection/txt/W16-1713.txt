



















































Focus Annotation of Task-based Data: Establishing the Quality of Crowd Annotation


Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 110–119,
Berlin, Germany, August 11, 2016. c©2016 Association for Computational Linguistics

Focus Annotation of Task-based Data:
Establishing the Quality of Crowd Annotation

Kordula De Kuthy Ramon Ziai Detmar Meurers
Collaborative Research Center 833

University of Tübingen
{kdk,rziai,dm}@sfs.uni-tuebingen.de

Abstract

We explore the annotation of informa-
tion structure in German and compare the
quality of expert annotation with crowd-
sourced annotation taking into account the
cost of reaching crowd consensus.

Concretely, we discuss a crowd-sourcing
effort annotating focus in a task-based
corpus of German containing reading
comprehension questions and answers.
Against the backdrop of a gold stan-
dard reference resulting from adjudicated
expert annotation, we evaluate a crowd
sourcing experiment using majority voting
to determine a baseline performance. To
refine the crowd-sourcing setup, we intro-
duce the Consensus Cost as a measure of
agreement within the crowd. We investi-
gate the usefulness of Consensus Cost as a
measure of crowd annotation quality both
intrinsically, in relation to the expert gold
standard, and extrinsically, by integrating
focus annotation information into a system
performing Short Answer Assessment tak-
ing into account the Consensus Cost.

We find that low Consensus Cost in crowd
sourcing indicates high quality, though
high cost does not necessarily indicate low
accuracy but increased variability. Over-
all, taking Consensus Cost into account
improves both intrinsic and extrinsic eval-
uation measures.

1 Introduction

This paper addresses the question of how to ex-
plore and evaluate the annotation of information
structural concepts to support the analysis of au-
thentic data. While the formal pragmatic concepts

in information structure, such as the focus of an
utterance, are precisely defined in theoretical lin-
guistics and potentially very useful in conceptual
and practical terms, it has turned out to be dif-
ficult to reliably annotate such notions in corpus
data (Ritz et al., 2008; Calhoun et al., 2010).

Theoretical linguists have discussed the notion
of focus for decades (cf., e.g., Jackendoff 1972;
Stechow 1981; Rooth 1992; Schwarzschild 1999;
Büring 2007). Following the work of Rooth
(1992), one of the widely used definitions of focus
is that “Focus indicates the presence of alternatives
that are relevant for the linguistic expressions” (cf.
Krifka 2007). Which part of an utterance is in the
focus thus depends on the context of the utterance,
as illustrated by the question-answers pairs in ex-
amples (1) and (2).

(1) A: What did John show Mary?
B: John showed Mary [[the PICTures]]F .

(2) A: Who did John show the pictures?
B: John showed [[MARy]]F the pictures.

Since focus is signalled by prosodic prominence
in an intonation language like English, the answers
also show different prominence patterns, as indi-
cated by the pitch accents on picture in (1) and
Mary in (2).

The linguistic discussions of focus phenom-
ena generally are based on few example sen-
tences, without an apparent exploration of sub-
stantial amounts of authentic data. Only few at-
tempts at systematically identifying focus in au-
thentic data have been made (Ritz et al., 2008; Cal-
houn et al., 2010). They generally ran into signifi-
cant problems trying to reach good inter-annotator
agreement, as they tried to identify focus in news-
paper text or other data types where no explicit
questions are available, making the task of deter-
mining the question under discussion, and thus re-
liably annotating focus, particularly difficult.

110



More recently, Ziai and Meurers (2014) showed
that reliable focus annotation is feasible, even for
somewhat ill-formed learner language, if one has
access to explicit questions and takes them into ac-
count in an incremental annotation scheme. They
demonstrate the effectiveness of the approach by
reporting both substantial inter-annotator agree-
ment and a substantial extrinsic improvement re-
sulting from integration of focus information into
a Short Answer Assessment system.

However, manual focus annotation by experts
is time consuming, both for annotator training
and the annotation itself. Additionally, in com-
putational linguistics it has been argued (Riezler,
2014) that annotation of theoretical linguistic no-
tions by experts should be complemented by ex-
ternal grounding, either in the form of extrinsic
evaluation, as reported above, or by using crowd-
sourcing: by formulating the annotation task in
such a way that non-experts can understand it and
carry it out, one ensures that the task does not de-
pend on implicit knowledge shared only by a team
of experts.

In this paper, we explore the use of crowd-
sourcing – which has been shown to work well for
a number of linguistic tasks (see, e.g., Finin et al.
2010; Tetreault et al. 2010; Zaidan and Callison-
Burch 2011) – for focus annotation. We investi-
gate how systematically the untrained crowd can
identify a meaning-based linguistic notion like fo-
cus in authentic data and which characteristics of
the data and context lead to consistent annotation
results.

Having established the general feasibility of
non-expert focus annotation, we refine the crowd-
sourcing approach by taking into account the vari-
ability within the set of crowd judgements. The
approach is based on the idea that sentences with
little variation in the annotation provided by the
crowd are more reliably annotated, i.e., are of a
higher quality. We spell out a measure of crowd di-
versity, Consensus Cost, and investigate its useful-
ness both intrinsically, by relating it to the expert-
based gold-standard, and extrinsically, by integrat-
ing cost-based focus annotation data in a Short
Answer Assessment system.

2 Data

We base our work on the CREG corpus (Ott et al.,
2012), a freely available task-based corpus con-
sisting of answers to reading comprehension ques-

tions written by American learners of German at
the university level. The overall corpus includes
164 reading texts, 1,517 reading comprehension
questions, 2,057 target answers provided by the
teachers, and 36,335 learner answers. Each an-
swer was rated by two annotators with respect to
whether it is a correct (appropriate) answer or not.
The CREG-5K subset used for the present annota-
tion study is an extended version of CREG-1032
(Meurers et al., 2011), selected using the same
criteria after the overall, four year corpus collec-
tion effort was completed. The criteria include
balancedness (equal number of correct and incor-
rect answers), a minimum answer length of four
tokens, and a language course level at the interme-
diate level or above.

(3) provides an example of a question-answer
pair from the CREG corpus.

(3) Q: Welches
which

Thema
topic

wurde
was

am
on the

4.
4th

November
November

nicht
not

diskutiert?
discussed

‘Which topic was not discussed on Nov. 4th?’

A: Die
the

deutsche
German

Einheit
unity

stand
stood

nicht
not

auf
on

der
the

Agenda.
agenda

‘The German unification was not on the agenda.’

2.1 Gold Standard Annotation

As a reference point for the evaluation of the focus
annotation by crowd workers, we first obtained a
gold-standard annotation using experts. We based
this effort on the focus annotation scheme and
annotation of the CREG-1032 data set provided
in Ziai and Meurers (2014). We extended this
by manually focus-annotating both target answers
and student answers in the larger CREG-5K data
set. The annotation was performed by two gradu-
ate research assistants in linguistics using the brat1

rapid annotation tool directly at token level. An
important characteristic of the annotation scheme
is that it is applied incrementally: annotators first
look at the surface question form, then determine
the set of alternatives (Krifka, 2007, sec. 3), and
finally mark instances of the alternative set in an-
swers. The following three types of categories are
distinguished:

• Question Form encodes the surface form of
a question (e.g., WhPhrase, Yes/No or
Alternative).

1http://brat.nlplab.org

111



• Focus marks the focused words or phrases in
an answer.

• Answer Type expresses the semantic category
of the focus in relation to the question form.
Examples include Time/Date, Location,
Entity, Action, and Reason.

Figure 1 shows a brat screen shot with an ex-
ample including a WhPhrase Question Form and
two answers, a target answer (TA) and a student
answer (SA), containing a word selected as focus
with Answer Type Action.

Q: ‘Which sport does Isabel do?’

TA: ‘She likes to go [[jogging]]F .’

SA: ‘[[Jogging]]F is fun for her.’

Figure 1: Brat annotation example

In the following we will only evaluate the agree-
ment results for the category Focus of our anno-
tation scheme. Ziai and Meurers (2014) anno-
tated 1,255 answers (1,032 student answers and
223 target answers of CREG-1032) and reported
88.1% percentage agreement for focus in all an-
swers, with κ = 0.75, calculated over all answer
tokens. We applied the approach to another 2,922
answers (2,155 student answers and 767 target an-
swers) of CREG-5K using two annotators and ob-
tained a percentage agreement for focus annota-
tion calculated over all answer tokens of 86.3%,
with κ = .70, demonstrating the robustness of
the annotation approach when applied to new data.
Altogether, 4,177 answers (3,187 student answers
and 990 target answers) of the CREG-5K corpus
are manually annotated with focus. The overall
percentage agreement for focus is 86.6% with a κ
of 0.71.

To obtain the gold standard focus annotation of
the combined corpus, the two annotation versions
were merged into one focus annotation by a third
expert, who determined the annotation in case the
two annotators disagreed.

3 Crowd Annotation

3.1 Setup of the crowd-sourcing experiment

To study non-expert focus annotation, we imple-
mented a crowd-sourcing task using the crowd-
sourcing platform CrowdFlower2 to collect focus
annotations from crowd workers. CrowdFlower
makes it possible to require workers to come
from German speaking countries, a feature that
other platforms like Amazon Mechanical Turk do
not provide as transparently, and it has a built-in
quality control mechanism ensuring that workers
throughout the entire job maintain a certain level
of accuracy on interspersed test items.

As data for our crowd-sourcing experiment,
we used 5,597 question-answer pairs from the
CREG-5K corpus and 100 manually constructed
test question-answer pairs. The task of the crowd
workers was to mark those words in an answer
sentence that “contain the information asked for in
the question”. Workers were shown five question-
answer pairs at a time. One of those five was from
our set of hand-crafted test question-answer pairs.
The workers were paid two cents per annotated
sentence.

Since CREG-5K consists of reading compre-
hension questions and answers provided by learn-
ers of German, there are cases where a stu-
dent response does not answer a given question
at all, for example, when the learner misunder-
stood the question. In the gold standard annota-
tion described in section 2.1, the annotators had
the option to mark such cases as “question ig-
nored”. Since we also wanted to provide the crowd
workers with this option, we included a check-
box “Frage nicht beantwortet” (“question not an-
swered”). When this option is selected, no word
in the answer sentence can be marked as focus.

Figure 2 shows an example CrowdFlower task
with the marked words in yellow. These marked
words are the ones that we counted as focus. The
English translation shown below was not part of
the CrowdFlower task.

We collected 11 focus annotations per answer
sentence and crowd workers had to maintain an ac-
curacy of 60% on the test question-answer pairs.
Altogether we collected 62,247 annotated sen-
tences.

2http://www.crowdflower.com/

112



Q: ‘Which topic was not discussed on November 4th?’
A: ‘[[The German unification]]F was not on the agenda.’

Figure 2: Example CrowdFlower annotation task

3.2 Evaluation
To evaluate the quality of our crowd focus annota-
tion, we wanted to find out how the annotations
produced by the crowd workers compare to the
gold standard expert annotation described in sec-
tion 2.1. We therefore chose to calculate all possi-
bilities of combining one through eleven workers
into one “virtual” annotator using majority voting
on individual word judgments. Ties in voting are
resolved by random assignment. The procedure is
similar to the approach described by Snow et al.
(2008). We did not employ any bias correction
or other types of weighting schemes, as discussed,
e.g., by Qing et al. (2014), but plan to do so in
future research.

In measuring agreement between crowd work-
ers and the expert gold-standard on the word level,
for the following reasons we opted for percent-
age agreement instead of Kappa or other mea-
sures that include a notion of expected agreement:
i) Kappa assumes the annotators to be the same
across all instances and this is systematically vio-
lated by the crowd-sourcing setup, and ii) calculat-
ing Kappa on a per-answer basis is not sensible in
cases where only one class occurs, as in all-focus
and no-focus answers.

3.2.1 Overall agreement of crowd with gold
We performed the evaluation on the CREG-5K
data subset for which we obtained both expert and
crowd annotations. Figure 3 shows the observed
per-token percentage agreement reached by the
crowd workers compared to the gold standard an-
notation.

As reference, the dotted lines show the percent-
age agreement between the two expert annotators.
We see that the quality improves from 74.9% for
one worker to 79.8% for eleven workers3. Given

3Note that agreement does not improve when increasing
from odd to even worker numbers, which is due to the fact
that the probability of drawing a majority does not increase
in these cases.

● ●

● ●

● ●
● ●

● ● ●

0.6

0.7

0.8

0.9

1.0

1 2 3 4 5 6 7 8 9 10 11
numWorkers

pe
rc

en
ta

ge
A

gr
ee

m
en

tW
ith

G
ol

dS
ta

nd
ar

d
Figure 3: Agreement of crowd with gold standard

that this is below the agreement of 88.8% reached
by the expert annotators for this data set, we next
investigated which cases the crowd can handle,
and which ones turn out to be difficult for the non-
experts.

3.2.2 Evaluation for different question forms

To identify patterns that show which types of data
can be annotated with focus most consistently by
crowd workers compared to the experts, we par-
ticularly want to look at properties of our data that
take characteristics of the context into account –
which in our case is the question context in which
an answer annotated with focus occurs. We there-
fore investigated the impact of different types of
questions on annotation agreement.

We carried out the comparison for the spe-
cific question form subtypes distinguishing sur-
face forms of wh-questions as annotated in CREG
(Meurers et al., 2011). Figure 4 shows how the
different question form subtypes impact the agree-
ment between the crowd and the gold-standard fo-
cus annotation.

As reference, the dotted lines again show the
percentage agreements between the two expert an-
notators for the different question forms. The
question forms make the answers fall into three
broad categories in terms of worker-gold agree-
ment: the most concrete ones (who, when and
where) in terms of surface realization in answers
come out on top with percentage agreements at
91% (where), 87% (who), and 86% (when).

113



●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●
●

●

●

●

●

●

●
●

●

●

●

●

●

0.6

0.7

0.8

0.9

1.0

1 2 3 4 5 6 7 8 9 10 11
numWorkers

pe
rc

en
ta

ge
A

gr
ee

m
en

tW
ith

G
ol

dS
ta

nd
ar

d

qType

●

●

●

●

●

●

●

who

where

when

which

what

how

why

Figure 4: Agreement by question form

The second group (which, what and how) are
at 80–82% percentage agreement, which is likely
due to their more ambiguous answer realization
possibilities, e.g., a what-question can ask for an
activity (‘What did Peter do?’) or an object (‘What
does Peter wear?’).

The third group consists only of why-questions
at an agreement level of 71%. For such ques-
tions asking for reasons, the range of possible an-
swer realizations arguably is the greatest given that
reasons are typically expressed by whole clauses.
However, for the gold expert-annotation, the more
explicit guidelines seem to have paid off in this
case, as why-questions come out at a much higher
agreement level of 86%.

To test whether more explicit guidelines could
also help the crowd annotators to be more sys-
tematic in their focus annotation, we conducted a
small additional crowdsourcing annotation study
with a smaller data set only containing answers to
why and what-questions. While the general set up
was the same as described in section 3.1, we pro-
vided the crowd workers with more examples il-
lustrating focus in different kind of answers. The
result was only a small improvement in agreement
between crowd and gold standard annotation, with
answers to what-questions 1% higher than before,
and 2% higher for why-questions. Even more ex-
plicit guidelines thus do not seem to help the non-
experts to handle answers occurring with why-
questions when annotating focus.

Summing up the results so far, the crowd anno-
tation study shows that i. the percentage agreement
improves the more crowd workers are taken into
account, and ii. majority voting on crowd worker
judgments compared to the expert gold annotation
can reach the expert level for specific cases (e.g.,
where-questions).

3.2.3 Qualitative discussion

To gain a better understanding of why the anno-
tation agreement differs so widely with respect to
question types for the crowd annotators, we take a
closer look at the variation in the linguistic mate-
rial that apparently impacts focus annotation. We
discuss a typical example for a who-question (4)
and a why-question (5) together with a sample of
given answers from the CREG-5K data set as the
two most extreme cases with respect to the ob-
served annotation agreement.

In the case of the different answers to the who-
question shown in (4), we can see that the variation
both in meaning and form is very limited:

(4) Q: Wer
who

war
was

an
at

der
the

Tür?
door

A1: [[Drei
three

Soldaten]]F
soldiers

waren
were

an
at

der
the

Tür.
door

A2: [[Drei
three

Männer
men

in
in

alten
old

Uniformen]]F
uniforms

waren
were

an
at

der
the

Tür.
door

A3: [[Die
the

drei
three

Männer]]F
men

waren
were

an
at

der
the

Tür.
door

A4: [[Drei
three

alte
old

Uniformen]]F
uniforms

waren
were

an
at

der
the

Tür.
door

Syntactically, the focused part of the answers
shown in [[. . . ]]F is expressed as a nominal phrase.
Contentwise, the same type of entity (a person) is
expressed by semantically related words. The rest
of the sentence shows no variation at all. The only
inconsistency in annotation by the crowd occurred
with NPs such as Die drei Männer in answer A3
in (4), where some of the crowd annotated the en-
tire NP as the focus, while the rest of the crowd
annotators only marked drei Männer as the focus,
leaving out the definite article.

In the case of the various answers to the why-
question shown in (5), multiple ways of answering
the same questions can be observed, both syntac-
tically and semantically.

114



(5) Q: Warum
why

ist
is

das
the

Haus
house

der
of the

Kameliendame
lady of the camellias

so
so

interessant?
interesting

A1: [[Ein
a

Klimacomputer
air computer

regelt
regulates

Temperatur,
temperature

Belüftung,
ventilation

Luftfeuchte
humidity

und
and

Beschattung.]]F
shading

A2: Das
the

Haus
house

der
of the

Kamelie
camellia

ist
is

so
so

interessant,
interesting

[[weil
because

es
it

230
230

Jahre
years

alt
old

und
and

8,90
8.90

m
m

hohe
high

ist.]]F
is

A3: [[In
in

der
the

warmen
warm

Jahreszeit
season

wird
is

das
the

Haus
house

neben
next to

die
the

Kamelie
camellia

gerollt.]]F
rolled

A4: Das
the

Haus
house

der
of the

Kamelie
camellia

ist
is

so
so

interessant,
interesting

[[weil
because

es
it

ist
is

ein
a

fahrbares
mobile

Haus.]]F
house

A5: Der
the

Kamelie
camellia

ist
is

interessant
interesting

[[wegen
because of

des
the

Computers.]]F
computer

Syntactically, the focused part of the answer is ei-
ther expressed as the entire sentence as in A1 and
A3 in (5), the subordinate clause starting with weil
(because) as in A2 and A4 in (5), or as a PP intro-
duced by wegen (because of) as in A5. Semanti-
cally, all four answers present a different proposi-
tional content. The relation between the question
and potential answers thus is not particularly ob-
vious or direct. Establishing the relation between
question and answer – as needed to identify the
focus of the answer – thus requires more effort by
the annotator. This leads to less consistent results
in the annotation for the crowd. For example, parts
of the crowd annotators did not interpret the sen-
tence A3 in (5) as an answer to the why-question
in (5) at all and consequently did not mark any
words in that sentence as focus, while the rest of
the crowd annotators marked the entire clause as
the focus.

For the expert annotators, the more explicit
guidelines including a conceptual discussion of
the key notions and explicit tests with minimal
pairs, results in less pronounced differences in an-
notation quality for the different question types.

4 Predicting when the crowd is reliable

Apart from taking the question type into account,
is it possible to predict when crowd focus annota-
tion is particularly reliable based on characteristics
of the crowd judgements?

Previous research on this issue has looked pri-
marily at individual crowd worker characteristics,

such as worker trustfulness (cf., e.g., Hantke et al.
(2016). Hsueh et al. (2009) calculate sentiment
ambiguity by considering the strength and the po-
larity of the sentiment’s ratings. We here go into a
similar direction for focus annotation, investigat-
ing the idea to take into account the diversity of
the crowd performance, i.e., how diverse the focus
annotations obtained from crowd workers for in-
dividual sentences are. Our hypothesis here is that
sentences where the crowd agrees more on the an-
notation are annotated more reliably.

4.1 Calculating the cost of crowd consensus
We propose to measure the diversity of the focus
annotation provided by the crowd workers in terms
of the Consensus Cost in annotating a sentence of
length n. The Consensus Cost (CC) is defined to
be the sum of the minority annotation (i.e., focus
or background) for all tokens in a sentence divided
by the total number of tokens and the largest pos-
sible minority annotation for a token (in our case
5, since 6 would be a majority with 11 workers).

CC =

n∑
w=0

changeNeededForConsensus(w)

largestPossibleMinority × n
The formula measures how many annotation

changes would be needed to reach total consen-
sus in annotating a given token. Sentences where
the crowd workers mostly agreed on an annota-
tion have a low consensus cost, because for every
token only few annotation changes are needed to
reach total agreement. Sentences where a larger
number of workers diverge from the majority an-
notation have a higher consensus cost, since more
changes would be needed in order to reach com-
plete consensus on that annotation.

Figure 5 exemplifies the calculation of the
Consensus Cost for the actual eleven crowd
annotations from the crowdsourcing experiment
for the short example answer Die/the drei/three
Männer/men war/was an/at der/the Tür/door from
our CREG data.

For the first word die, only two of the 11 crowd
workers marked the word as Focus, so the cost to
reach total agreement (in this case that the token
is (b)ackground, i.e., not focus) is 2. The next
two words (drei/three) and (Männer/men) were
marked as focus by 10 of the 11 of workers and
thus each have a cost of one. The rest of the words
in the sentence were unanimously not marked as
focus by the crowd workers and thus have a cost

115



Die drei Männer war an der Tür
1 F F F b b b b
2 F F F b b b b
3 b F F b b b b
4 b F F b b b b
5 b F F b b b b
6 b F F b b b b
7 b F F b b b b
8 b F F b b b b
9 b F F b b b b

10 b F F b b b b
11 b b b b b b b

Cost 2 1 1 0 0 0 0

ConsensusCost = 4
5×7 = 0.11

Figure 5: Calculating the Consensus Cost

of 0. The resulting Consensus Cost for the focus
annotation for this sentence according to our for-
mula is 0.11.

Since not all crowd workers perform equally
well, it would in principle make sense to incorpo-
rate their individual reliability. As a first step to-
wards this idea, we are excluding all workers from
annotation who fail to reach a particular accuracy
threshold (0.66) on the test questions.

We can now investigate whether the Consen-
sus Cost, i.e., the amount of agreement within the
crowd, can serve as an indicator of the quality of
the annotations provided by the crowd.

4.2 Consensus Cost and Annotation Quality

In order to determine whether Consensus Cost can
function as a proxy for annotation quality, let us
compare it to the agreement of the crowd workers
with the gold standard expert annotation we dis-
cussed in section 3.2.

To explore the relation between Consensus Cost
and quality of the annotation of an answer, we di-
vided the possible values (0.0 to 1.0) of Consen-
sus Cost into four ranges, using 0.25, 0.5 and 0.75
as boundaries. Figure 6 shows the boxplots for
each of the four groups of answers by Consensus
Cost, with the percentage agreement with the gold
standard shown on the y-axis. The width of the
box plots indicates the number of instances repre-
sented, whereas the height represents the distribu-
tion of agreement values.

For answers annotated with low Consensus Cost
(< 0.5), the quality of annotation is generally
high, with agreement with the gold standard be-
tween 0.7 and 1.0. The majority of data points fall
into this interval. Interestingly, answers annotated

●

●●

●

●

●

●●

●

●●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●
●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●●

●

●●●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●●●●

●

●●●●

●

0.00

0.25

0.50

0.75

1.00

[0,0.25] (0.25,0.5] (0.5,0.75] (0.75,1]
consensusCost

pe
rc

en
ta

ge
A

gr
ee

m
en

tW
ith

G
ol

d

Figure 6: Consensus Cost and Annotation Quality

with higher Consensus Cost values, in the inter-
vals (0.5,0.75] and (0.75,1], show a more hetero-
geneous picture. While their median agreement is
much lower, they also show a more varied distri-
bution, including some high quality annotations.

In sum, we can conclude that there is a clear
association between Consensus Cost and annota-
tion quality. A low Consensus Cost can serve as a
proxy for high annotation quality. The relationship
is not a simple linear one, though, so that some an-
notations with high Consensus Cost may also be of
high quality.

4.3 Consensus Costs by Question Type

When we evaluated the quality of the crowd focus
annotation in relation to the gold-standard expert
annotation in section 3.2, we found that the crowd
annotations fall into three groups with respect to
question types: Answers to the who, when and
where questions showed a high percentage agree-
ment with the expert annotation, answers to which,
what and how questions had a much lower per-
centage agreement and answers to why questions
were the most difficult ones for the crowd and had
the lowest agreement numbers. The data by ques-
tion type thus makes an interesting test case for
Consensus Cost as a proxy for annotation quality.
If sentences with a low consensus cost provide an-
notation of higher quality, we should be able to
find a similar division of the annotation in terms
of question types as as in comparison with the ex-
pert annotation.

116



Figure 7 shows the consensus cost of our crowd
annotation plotted according to question types.

● ● ●

●●

● ● ●●

why

what

which

how

when

where

who

0.00 0.25 0.50 0.75 1.00
consensusCost

qu
es

tio
nF

or
m

questionForm

who

where

when

how

which

what

why

Figure 7: Consensus Cost per Question Type

The figure shows clear differences by question
type: The annotations of answers to who, when,
and where questions have the lowest consensus
costs, while answers to why questions have high-
est cost. And in addition, focus annotations of an-
swers to why and how are most varied.

Consensus cost by question type thus patterns
parallel to the quality of the crowd annotation
compared to the expert annotation. The analysis
by question type thus confirms the overall anal-
ysis in the previous subsection establishing a low
Consensus Cost in crowd annotation as a proxy for
high quality annotation.

4.4 Extrinsic evaluation

To externally establish the relevance and quality
of the crowd focus annotation, we extrinsically
evaluated the expert gold standard annotation in
an independent task, Short Answer Assessment,
specifically the automatic assessment of answers
to reading comprehension questions. For this pur-
pose, we employed the CoMiC system (Meurers
et al., 2011), which assesses student answers by
analyzing the quantity and quality of alignment
links it finds between the student and the target
answer.

Our goal here is twofold: on the one hand, we
want to find out whether the previously introduced

Consensus Cost measure is helpful in determin-
ing the quality of focus annotation as measured by
its impact on Short Answer Assessment. On the
other hand, it is interesting to determine whether
the state of the art in automatic answer assessment
can be advanced by integrating non-expert anno-
tation of focus (as a step towards automatic focus
annotation developed using the crowd-annotated
data).

To cleanly separate the data used for testing the
Answer Assessment system CoMiC from the data
used for training CoMiC, we randomly sampled
approximately 20% of the CREG-5K data set and
set it aside as the final test set. The remaining 80%
was used as training set.

In exploring the impact of different Consensus
Costs, we used the same four cutoffs as before:
0.25, 0.5, 0.75 and the maximum value 1.0. For
each cutoff, we picked the answers with crowd fo-
cus annotations satisfying the cutoff constraint in
training and test set, and ran CoMiC on the re-
sulting data excerpt, aligning only words in stu-
dent and target answer that are focused. For the
rest of the data, which did not meet the Consen-
sus Cost criterion or for which no focus annota-
tion was available, we used the standard version
of CoMiC that only aligns words not previously
mentioned in the question. We then calculated
a weighted average (by number of test instances)
of both system accuracies in order to arrive at an
overall system result for the respective Consensus
Cost value. The results are displayed in Table 1.

Cost Focus Given Avg
≤ train/test % train/test % %
base – 4136/1001 81.5 81.5
0.25 1009/252 88.1 3127/749 80.4 82.3
0.5 2019/489 84.5 2117/512 80.7 82.5
0.75 3087/747 84.5 1049/254 79.5 83.2
1.0 3638/882 82.7 498/119 76.5 81.9

Table 1: Results on the “unseen answers” test set

The ‘train/test’ column shows the number of
training and test instances each system was run on,
and the ‘%’ column shows the classification accu-
racy achieved. The ‘base’ row gives the baseline
resulting from using CoMiC as-is, without any fo-
cus information.

Looking at the results for the focus partition of
the data, one can see that accuracy drops when
taking into account focus annotation with higher
Consensus Cost, even though thereby in principle

117



more training data is becoming available.
For the ‘Given’ column, when data with higher

Consensus Cost is used for the ’Focus’ version of
the system and thereby less data is available for
training the ’Given’ system, accuracy of the latter
decreases.

Overall, a Consensus Cost cutoff of 0.75 gives
the optimal trade-off between both system vari-
ants, yielding 83.2% classification accuracy.

Test with answers to unseen questions In a
second experiment, we also compiled a question-
based train/test split, meaning that for approx-
imately 20% of randomly picked questions in
CREG-5K, all answers were held out as the test
set. This is a much harder benchmark since the
system in the test has to classify answers to previ-
ously unseen questions, providing some indication
of the system’s ability to learn something general
rather than about specific question-answer pairs.
The remainder of the testing procedure was the
same as described above, yielding the results de-
tailed in Table 2.

Cost Focus Given Avg
≤ train/test % train/test % %
base – 4016/1121 78.8 78.8
0.25 970/291 81.4 3046/830 78.2 79.0
0.5 1938/570 80.4 2078/551 78.2 79.3
0.75 2973/861 81.6 1043/260 76.9 80.6
1.0 3515/1005 79.6 501/116 78.4 79.5

Table 2: Results on the “unseen questions” test set

The accuracies are generally lower due to the
harder test scenario. Moreover, the clear trends
observed above with regard to training and test
size do not seem to apply as clearly here, likely
again owing to the ‘unseen questions’ scenario.
Given the many different types of potential ques-
tions and the relatively small number of differ-
ent questions the system sees during training, it
is more important for which questions the system
has seen answers, than how many. However, de-
spite the differences to the previous experiment,
the optimal result is again achieved with a Con-
sensus Cost of 0.75, supporting the conclusion that
Consensus Cost supports a systematic characteri-
zation of annotation quality.

5 Conclusion

We described a crowd-sourcing experiment for the
annotation of focus, establishing its success both

intrinsically by comparing it to a gold-standard ex-
pert annotation, and extrinsically by using the re-
sulting annotations successfully in an independent
CL task, Short Answer Assessment.

In order to distinguish between high and low
quality crowd annotations, we define the measure
of Consensus Cost, which essentially is the num-
ber of minority votes for each markable. We show
that low values of Consensus Cost indicate high
annotation quality and that training data selection
based on Consensus Cost is beneficial in the Short
Answer Assessment task.

In the future, we plan to extend our assessment
of annotation quality beyond simple Consensus
Cost cut-offs to a supervised machine-learning ap-
proach that can also take other characteristics of
the authentic data (e.g., the question type) into ac-
count. The relationship between Consensus Cost
and annotation quality is not simply linear and
the additional information could help determine
which of the more variable-quality data with high
Consensus Cost is of high quality.

References

Daniel Büring. 2007. Intonation, semantics and
information structure. In Gillian Ramchand and
Charles Reiss, editors, The Oxford Handbook of
Linguistic Interfaces, Oxford University Press.

Sasha Calhoun, Jean Carletta, Jason Brenier, Neil
Mayo, Dan Jurafsky, Mark Steedman, and
David Beaver. 2010. The NXT-format switch-
board corpus: A rich resource for investigating
the syntax, semantics, pragmatics and prosody
of dialogue. Language Resources and Evalua-
tion 44:387–419.

Tim Finin, Will Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark
Dredze. 2010. Annotating named entities in
twitter data with crowdsourcing. In Proceedings
of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon’s Me-
chanical Turk. Association for Computational
Linguistics, Stroudsburg, PA, USA, CSLDAMT
’10, pages 80–88.

Simone Hantke, Erik Marchi, and Björn Schuller.
2016. Introducing the weighted trustability
evaluator for crowdsourcing exemplified by
speaker likability classification. In Nicoletta
Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Sara Goggi, Marko Grobel-
nik, Bente Maegaard, Joseph Mariani, Helene

118



Mazo, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Tenth In-
ternational Conference on Language Resources
and Evaluation (LREC 2016). European Lan-
guage Resources Association (ELRA), Paris,
France.

Pei-Yun Hsueh, Prem Melville, and Vikas Sind-
hwani. 2009. Data quality from crowdsourcing:
A study of annotation selection criteria. In Pro-
ceedings of the NAACL HLT 2009 Workshop on
Active Learning for Natural Language Process-
ing. Association for Computational Linguistics,
Stroudsburg, PA, USA, HLT ’09, pages 27–35.

Ray Jackendoff. 1972. Semantic Interpretation in
Generative Grammar. MIT Press, Cambridge,
MA.

Manfred Krifka. 2007. Basic notions of in-
formation structure. In Caroline Fery, Gis-
bert Fanselow, and Manfred Krifka, editors,
The notions of information structure, Univer-
sitätsverlag Potsdam, Potsdam, volume 6 of In-
terdisciplinary Studies on Information Structure
(ISIS), pages 13–55.

Detmar Meurers, Ramon Ziai, Niels Ott, and Jan-
ina Kopp. 2011. Evaluating answers to read-
ing comprehension questions in context: Re-
sults for German and the role of information
structure. In Proceedings of the TextInfer 2011
Workshop on Textual Entailment. ACL, Edin-
burgh, pages 1–9.

Niels Ott, Ramon Ziai, and Detmar Meurers.
2012. Creation and analysis of a reading com-
prehension exercise corpus: Towards evaluating
meaning in context. In Thomas Schmidt and
Kai Wörner, editors, Multilingual Corpora and
Multilingual Corpus Analysis, Benjamins, Am-
sterdam, Hamburg Studies in Multilingualism
(HSM), pages 47–69.

Ciyang Qing, Ulle Endriss, Raquel Fernandez, and
Justin Kruger. 2014. Empirical analysis of ag-
gregation methods for collective annotation. In
Proceedings of COLING 2014, the 25th Inter-
national Conference on Computational Linguis-
tics: Technical Papers. Dublin City University
and Association for Computational Linguistics,
Dublin, Ireland, pages 1533–1542.

Stefan Riezler. 2014. On the problem of theoreti-
cal terms in empirical computational linguistics.
Computational Linguistics 40(1):235–245.

Julia Ritz, Stefanie Dipper, and Michael Götze.
2008. Annotation of information structure: An
evaluation across different types of texts. In
Proceedings of the 6th International Confer-
ence on Language Resources and Evaluation.
Marrakech, Morocco, pages 2137–2142.

Mats Rooth. 1992. A theory of focus interpreta-
tion. Natural Language Semantics 1(1):75–116.

Roger Schwarzschild. 1999. GIVENness, AvoidF
and other constraints on the placement of ac-
cent. Natural Language Semantics 7(2):141–
177.

Rion Snow, Brendan O’Connor, Daniel Jurafsky,
and Andrew Y. Ng. 2008. Cheap and fast—but
is it good?: Evaluating non-expert annotations
for natural language tasks. In Proceedings of
the Conference on Empirical Methods in Natu-
ral Language Processing. Association for Com-
putational Linguistics, Stroudsburg, PA, USA,
EMNLP ’08, pages 254–263.

Arnim von Stechow. 1981. Topic, focus, and local
relevance. In Wolfgang Klein and W. Levelt,
editors, Crossing the Boundaries in Linguistics,
Reidel, Dordrecht, pages 95–130.

Joel Tetreault, Elena Filatova, and Martin
Chodorow. 2010. Rethinking grammatical er-
ror annotation and evaluation with the amazon
mechanical turk. In NAACL-HLT: 2010 Pro-
ceedings of the 5th Workshop on Building Edu-
cational Applications (BEA-5). Association for
Computational Linguistics.

Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional qual-
ity from non-professionals. In Proceedings of
the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies - Volume 1. Association for Com-
putational Linguistics, Stroudsburg, PA, USA,
HLT ’11, pages 1220–1229.

Ramon Ziai and Detmar Meurers. 2014. Focus an-
notation in reading comprehension data. In Pro-
ceedings of the 8th Linguistic Annotation Work-
shop (LAW VIII, 2014). COLING, Association
for Computational Linguistics, Dublin, Ireland,
pages 159–168.

119


