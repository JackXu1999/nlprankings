



















































K-best Iterative Viterbi Parsing


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 305–310,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

K-best Iterative Viterbi Parsing

Katsuhiko Hayashi and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan

{hayashi.katsuhiko, nagata.masaaki}@lab.ntt.co.jp

Abstract

This paper presents an efficient and op-
timal parsing algorithm for probabilis-
tic context-free grammars (PCFGs). To
achieve faster parsing, our proposal em-
ploys a pruning technique to reduce un-
necessary edges in the search space. The
key is to repetitively conduct Viterbi in-
side and outside parsing, while gradually
expanding the search space to efficiently
compute heuristic bounds used for prun-
ing. This paper also shows how to extend
this algorithm to extract K-best Viterbi
trees. Our experimental results show that
the proposed algorithm is faster than the
standard CKY parsing algorithm. More-
over, its K-best version is much faster
than the Lazy K-best algorithm when K is
small.

1 Introduction

The CKY or Viterbi inside algorithm is a well-
known algorithm for PCFG parsing (Jurafsky and
Martin, 2000), which is a dynamic programming
parser using a chart table to calculate the Viterbi
tree. This algorithm is commonly used in natural
language parsing, but when the size of the gram-
mar is extremely large, exhaustive parsing be-
comes impractical. One way to reduce the compu-
tational cost of PCFG parsing is to prune the edges
produced during parsing. In fact, modern parsers
have often employed pruning techniques such as
beam search (Ratnaparkhi, 1999) and coarse-to-
fine search (Charniak et al., 2006).

Despite their practical success, both pruning
methods are approximate, so the solution of the
parser is not always optimal, i.e., the parser does
not always output the Viterbi tree. Recently, an-
other line of work has explored A* search algo-

rithms, in which simpler problems are used to
estimate heuristic scores for prioritizing edges to
be processed during parsing (Klein and Manning,
2003). If the heuristic is consistent, A* parsing
always outputs the Viterbi tree. As Tsuruoka and
Tsujii (2004) mentioned, however, A* parsing has
a serious difficulty from an implementation point
of view: “One of the most efficient way to im-
plement an agenda, which keeps edges to be pro-
cessed in A* parsing, is to use a priority queue,
which requires a computational cost of O(log(n))
at each action, where n is the number of edges in
the agenda. The cost of O(log(n)) makes it diffi-
cult to build a fast parser by the A* algorithm.”

This paper presents an alternative way of prun-
ing unnecessary edges while keeping the optimal-
ity of the parser. We call this algorithm itera-
tive Viterbi parsing (IVP) for the reason that the
iterative process plays a central role in our pro-
posal. The IVP algorithm conducts repetitively
Viterbi inside and outside parsing, while gradually
expanding the search space to efficiently compute
lower and upper bounds used for pruning. IVP is
easy to implement and is much faster in practice
than the standard CKY parsing algorithm.

In addition, we also show how to extend the
IVP algorithm to extract K-best Viterbi parse trees.
The idea is to integrate Huang and Chiang (2005)’s
K-best algorithm 3, which is called as Lazy, with
the iterative parsing process. Lazy performs a
Viterbi inside pass and then extracts K-best lists in
a top-down manner. Although especially the first
Viterbi inside pass is a bottleneck of the Lazy algo-
rithm, the K-best IVP algorithm avoids its amount
of work as well as in the 1-best case.

2 Iterative Viterbi Parsing

Following Pauls and Klein (2009), we define some
notations. The IVP algorithm takes as input a

305



(a)
A
B
C
D

A
B
C
D

A
B
C
D

A
B
C
D

A
B
C
D

A
B
C
D

A
B
C
D

A
B
C
D

A
B
C
D

A
B
C
D

(b)
A
B

X2
X1
X2

A
B

X2 X1
C
D

A
B

X2

A
B

X2
X1
X2

A
B

X2

A
B

X2

A
B

X2

Figure 1: (a) An original chart table consisting of
non-terminal symbols only. (b) A coarse chart ta-
ble consisting of both non-terminal symbols and
shrinkage symbols. There exists a correspond-
ing derivation A(X2(B(X1 B) X2) X2) in (b) to
a derivation A(C(B(A B) C) D) in (a), both consist
of black-shaded symbols.

PCFG G and a sentence x consisting of terminal
symbols t0 . . . tn−1. Without loss of generality, we
assume Chomsky normal form: each non-terminal
rule r in G has the form r = A → B C with log
probability weight log q(r), where A, B and C are
elements inN , which is a set of non-terminal sym-
bols. Chart edges are labeled spans e = (A, i, j).
Inside derivations of an edge e = (A, i, j) are
trees rooted at A and spanning ti . . . tj−1. The
score of a derivation d is denoted by s(d)1. The
score of the best (maximum) inside derivation for
an edge e is called the Viterbi inside score β(e).
The goal of 1-best PCFG parsing is to compute the
Viterbi inside score of the goal edge (TOP, 0, n)
where TOP is a special root symbol. For the goal
edge, we call its derivation goal derivation. The
score of the best derivation of TOP→ t0 . . . ti−1 A
tj . . . tn−1 is called the Viterbi outside score α(e).

We assume N = {A,B,C,D}. By grouping
several symbols in the same cell of the chart ta-
ble, we can make a smaller table than the original
one. While the original chart table in Figure 1 (a)
contains non-terminal symbols only, the chart ta-
ble in Figure 1 (b) contains not only non-terminal

1The score of a derivation is the sum of rule weights for
all rules used in the derivation.

Level 0 1 2

Op



ADJ

 JJJJRJJS
ADV


RB
RBR
RBS
WRB

NOUN


NN
NNP
NNPS
NNS

VERB



MD
VB
VBD
VBG
VBN
VBP
VBZ

Cl



IN
CC
CD

DET


DT
EX
PDT
WDT

PRON


PRP
PRP$
WP
WP$

PRT

 POSRPTO

Ot



X


FW
LS
SYM
UH

PUNC



#
$
”
“
-LRB-
-RRB-
,
:
.

Figure 2: The levels of non-terminal symbols.

symbols but also new symbols X1 and X2. The
new symbols, which are made by grouping several
non-terminal symbols, are refered to as shrinkage
symbols. For example, the shrinkage symbols X1
and X2 consist of non-terminal symbols {A,B}
and {C,D}, respectively.

In this paper, to make shrinkage symbols, we
use hierarchical clustering of non-terminal sym-
bols defined in (Charniak et al., 2006). Figure 2
shows a part of the hierarchical symbol definition.
Formally, we hierarchically cluster N into m + 1
sets N0 . . . Nm where N = Nm. For some i ∈
[0 . . .m − 1], we call an element in Ni i-th layer
shrinkage symbol. For some 0 ≤ i ≤ j ≤ m,

306



Algorithm 1 Iterative Viterbi Parsing
1: lb← det(x, G) or lb← −∞
2: chart← init-chart(x, G)
3: for all i ∈ [1 . . . ] do
4: d̂← Viterbi-inside(chart)
5: if d̂ consists of non-terminals only then
6: return d̂
7: if lb < best(chart) then
8: lb← best(chart)
9: expand-chart(chart, d̂, G)

10: Viterbi-outside(chart)
11: prune-chart(chart, lb)

we define a mapping πi→j : Ni 7→ P(Nj) where
P(·) is the power set of ·. Taking a symbol HP
in Figure 2 as an example, π0→1(HP) = {S ,N }.
When i = j, for some i-th layer shrinkage sym-
bol A ∈ Ni, πi→j(A) returns a singleton {A}. For
all 0 ≤ i, j, k ≤ m, the rule parameter associated
with symbols Xi ∈ Ni, Xj ∈ Nj , Xk ∈ Nk is
defined as the following:

log q(Xi → Xj Xk) = max
A∈πi→m(Xi)
B∈πj→m(Xj)
C∈πk→m(Xk)

log q(A→ B C).

By this construction, each derivation in a coarse
chart gives an upper bound on its correspond-
ing derivation in the original chart (Klein and
Manning, 2003) and we can obtain the following
lemma:

Lemma 1. If the best goal derivation d̂ in the
coarse chart does not include any shrinkage sym-
bol, it is equivalent to the best goal derivation in
the original chart.

Proof . Let Y be the set of all goal derivations in
the original chart, Y ′ ⊂ Y be the subset of Y not
appearing in the coarse chart, and Y ′′ be the set of
all goal derivations in the coarse chart. For each
derivation d ∈ Y ′, there exists its unique corre-
sponding derivation d′ in Y ′′ (see Figure 1). Then,
we have

∀d ∈ Y ,∃d′ ∈ Y ′′, s(d) ≤ s(d′) < s(d̂)
and this means that d̂ is the best derivation in the
original chart. 2

Algorithm 1 shows the pseudo code for IVP.
The IVP algorithm starts by initializing coarse
chart, which consists of only 0-th layer shrinkage
symbols. It conducts Viterbi inside parsing to find
the best goal derivation. If the derivation does not
contain any shrinkage symbols, the algorithm re-
turns it and terminates. Otherwise, the chart table

is expanded, and the above procedure is repeated
until the termination condition is satisfied.

For efficient parsing, we integrate a pruning
technique with IVP. For an edge e = (A, i, j), we
denote by αβ(e) = α(e) + β(e) the score of the
best goal derivation which passes through e, where
β(e) and α(e) are Viterbi inside and outside scores
for e. Then, if we obtain a lower bound lb such
that lb ≤ maxd∈Y s(d) where Y is the set of all
goal derivations in the original chart, an edge e
with αβ(e) < lb is no longer necessary to be pro-
cessed. Though it is expensive to compute αβ(e)
in the original chart, we can efficiently compute by
Viterbi inside-outside parsing its upper bound in a
coarse chart table:

αβ(e) ≤ α̂(e) + β̂(e) = α̂β(e)

where α̂(e) and β̂(e) are the Viterbi inside and
outside scores of e in the coarse chart table. If
α̂β(e) < lb, we can safely prune the edge e away
from the coarse chart. Note that this pruning sim-
ply reduces the search space at each IVP iteration
and does not affect the number of iterations taken
until convergence at all.

We initialize the lower bound lb with the score
of a goal derivation obtained by deterministic pars-
ing det() in the original chart. The deterministic
parsing keeps only one non-terminal symbol with
the highest score per chart cell and removes the
other non-terminal symbols. The det() function is
very fast but causes many search errors. For effi-
cient pruning, a tighter lower bound is important,
thus we update the current lower bound with the
score of the best derivation, having non-terminals
only, obtained by the best() function in the current
coarse chart, if the former is less than the latter.

At line 9, IVP expands the current chart ta-
ble by replacing all shrinkage symbols in d̂ with
their next layer symbols using mapping π. While
this expansion cannot derive a reasonable worst
time complexity since it takes many iterations un-
til convergence, we show from our experimental
results that it is highly effective in practice.

3 K-best Extension

Algorihtm 2 shows the K-best IVP algorithm
which applies the iterative process to the Lazy
K-best algorithm of (Huang and Chiang, 2005).
If the best derivation is found, which consists of
non-terminal symbols only, this algorithm calls the

307



Algorithm 2 K-best IVP
1: lb← beam(x, G, k) or lb← −∞
2: chart← init-chart(x, G)
3: for all i ∈ [1 . . . ] do
4: d̂1 ← Viterbi-inside(chart)
5: if d̂1 consists of non-terminals only then
6: [d̂2, . . . , d̂k]← Lazy K-best(chart)
7: if All of [d̂2, . . . , d̂k] consist of non-terminals only

then
8: return [d̂1, d̂2, . . . , d̂k]
9: else

10: d̂1 = getShrinkageDeriv([d̂2, . . . , d̂k])
11: if lb < k-best(chart, k) then
12: lb← k-best(chart, k)
13: expand-chart(chart, d̂1, G)
14: Viterbi-outside(chart)
15: prune-chart(chart, lb)

Lazy K-best algorithm. If all of the K-best deriva-
tions do not contain any shrinkage symbol, it re-
turns them and terminates.

The K-best IVP algorithm also prunes unnec-
essary edges and initializes the lower bound lb
with the score of the k-th best derivation ob-
tained by beam search parsing in the original chart.
For efficient pruning, we update lb with the k-th
best derivation, which consists of non-terminals
only, obtained by the k-best() function in the
current coarse chart. The getShrinkageDeriv()
function seeks the best derivation, which contains
shrinkage symbols, from [d̂2, . . . , d̂k]. The K-best
IVP algorithm inherits the other components from
standard IVP.

4 Experiments

We used the Wall Street Journal (WSJ) part of the
English Penn Treebank: Sections 02–21 were used
for training, sentences of length 1–35 in Section
22 for testing. We estimated a Chomsky normal
form PCFG by maximum likelihood from right-
branching binarized trees without function labels
and trace-fillers. Note that while this grammar is a
proof-of-concept, CKY on a larger grammar does
not work well even for short sentences.

Table 1 shows that the number of edges pro-
duced by the IVP algorithm is significantly smaller
than standard CKY. Moreover, many of the edges
are pruned during the iterative process. While IVP
takes many iterations util convergence, it is about
8 times faster than CKY. The fact means that the
computational cost of the Viterbi inside and out-
side algorithms on a small chart is negligible.

Next, we examine the K-best IVP algorithm.
Figure 3 shows parsing speed of Lazy and K-best

CKY IVP
len. edges time edges pruned iters time

20 10590 1.25 2864 2089 68 0.13
23 13938 1.76 2219 1462 41 0.06
22 12771 1.52 2204 1425 46 0.05
17 7701 0.72 1526 1119 32 0.03
28 20538 3.14 7306 5338 144 1.18
34 30141 5.44 6390 4634 98 0.49
...

...
...

21 12801 1.77 3502 2456 70 0.21

Table 1: The number of the edges produced in 1-
best parsing on testing set. Many of the edges are
pruned during the IVP parsing iterations. The last
row denotes the mean values.

 0

 1

 2

 3

 4

 5

 20  40  60  80  100  120

ti
m

e
 (

c
p

u
 s

e
c
)

k-best size

Lazy K-best
K-best IVP

Figure 3: K-best Parsing time for various k.

IVP algorithms for various k (2 ∼ 128). When
k is small (2 ∼ 64), K-best IVP is much faster
than Lazy. However, K-best IVP did not work well
when setting k to more than 128. We show the
reason in Figure 4 where we plot the number of
edges in chart table at each K-best IVP iterations
for some test sentence with length 28. It is clear
that the smaller k is, the earlier it is convergent.
Moreover, when setting k too large, it is difficult to
compute a tight lower bound, i.e., K-best IVP does
not prune unnecessary edges efficiently. However,
in practice, this is not likely to be a serious prob-
lem since many NLP tasks use only very small k-
best parse trees (Choe and Charniak, 2016).

5 Related Work

Huang and Chiang (2005) presented an efficient
K-best parsing algorithm, which extracts K-best
lists after a Viterbi inside pass. Huang (2005) also
described a K-best extension of the Knuth pars-
ing algorithm (Knuth, 1977; Klein and Manning,
2004). Pauls and Klein (2009) successfully inte-
grated A* search with the K-best Knuth algorithm.

Tsuruoka and Tsujii (2004) proposed an itera-

308



 0

 2000

 4000

 6000

 8000

 10000

 12000

 14000

 0  50  100  150  200  250

th
e

 n
u

m
b

e
r 

o
f 
e

d
g

e
s

iterations

k=8 w/ Pruning
k=32 w/ Pruning

k=128 w/ Pruning
k=256 w/ Pruning

k=256 w/o Pruning

Figure 4: The plot of the number of edges in chart
table at each K-best IVP parsing iteration.

tive CKY algorithm, which is similar to our IVP
algorithm in that it conducts repeatedly CKY pars-
ing with a threshold until the best parse is found.
The main difference is that IVP employs a coarse-
to-fine chart expansion to compute better lower
and upper bounds efficiently. Moreover, Tsuruoka
and Tsujii (2004) did not mention how to extend
their algorithm to K-best parsing.

The coarse-to-fine parsing (Charniak et al.,
2006) is used in many practical parsers such
as Petrov and Klein (2007). However, the coarse-
to-fine search is approximate, so the solution of
the parser is not always optimal.

For sequential decoding, Kaji et al. (2010) also
proposed the iterative Viterbi algorithm. Huang et
al. (2012) extended it to extract K-best strings by
integrating the backward K-best A* search (Soong
and Huang, 1991) with the iterative process. Our
proposed algorithm can be regarded as a general-
ization of their methods to the parsing problem.

6 Conclusion and Future Work

This paper presents an efficient K-best parsing al-
gorithm for PCFGs. This is based on standard
Viterbi inside-outside algorithms and is easy to
implement. Now, we plan to conduct experi-
ments using latent-variable PCFGs (Matsuzaki et
al., 2005; Cohen et al., 2012) to prove that our
method is useful for a variety of grammars.

Acknowledgments

The authors would like to thank the anonymous
reviewers for their valuable comments and sug-
gestions to improve the quality of the paper. This
work was supported in part by JSPS KAKENHI
Grant Number 26730126.

References
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph

Austerweil, David Ellis, Isaac Haxton, Catherine
Hill, R. Shrivaths, Jeremy Moore, Michael Pozar,
and Theresa Vu. 2006. Multilevel coarse-to-fine
pcfg parsing. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference, pages 168–175, New York City, USA,
June. Association for Computational Linguistics.

Do Kook Choe and Eugene Charniak. 2016. Parsing
as language modeling. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2331–2336, Austin, Texas,
November. Association for Computational Linguis-
tics.

Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable pcfgs. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 223–
231, Jeju Island, Korea, July. Association for Com-
putational Linguistics.

Liang Huang and David Chiang. 2005. Better k-
best parsing. In Proceedings of the Ninth Interna-
tional Workshop on Parsing Technology, pages 53–
64, Vancouver, British Columbia, October. Associa-
tion for Computational Linguistics.

Zhiheng Huang, Yi Chang, Bo Long, Jean-Francois
Crespo, Anlei Dong, Sathiya Keerthi, and Su-Lin
Wu. 2012. Iterative viterbi a* algorithm for k-best
sequential decoding. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 611–
619, Jeju Island, Korea, July. Association for Com-
putational Linguistics.

Liang Huang. 2005. K-best knuth algorithm. http:
//cis.upenn.edu/˜lhuang3/knuth.pdf.

Daniel Jurafsky and James H Martin. 2000. Speech
and Language Processing. Prentice Hall.

Nobuhiro Kaji, Yasuhiro Fujiwara, Naoki Yoshinaga,
and Masaru Kitsuregawa. 2010. Efficient stag-
gered decoding for sequence labeling. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 485–494, Up-
psala, Sweden, July. Association for Computational
Linguistics.

Dan Klein and Christopher D Manning. 2003. A*
parsing: Fast exact viterbi parse selection. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 40–47, Edmonton, USA, May-June.
Association for Computational Linguistics.

Dan Klein and Christopher D Manning. 2004. Parsing
and hypergraphs. In New developments in parsing
technology, pages 351–372. Springer.

309



Donald E Knuth. 1977. A generalization of dijkstra’s
algorithm. Information Processing Letters, 6(1):1–
5.

Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL’05),
pages 75–82, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.

Adam Pauls and Dan Klein. 2009. K-best a* parsing.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 958–966, Suntec, Singapore,
August. Association for Computational Linguistics.

Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404–411, Rochester, New York, April.
Association for Computational Linguistics.

Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151–175.

Frank K Soong and E-F Huang. 1991. A tree-
trellis based fast search for finding the n-best sen-
tence hypotheses in continuous speech recogni-
tion. In Acoustics, Speech, and Signal Processing,
1991. ICASSP-91., 1991 International Conference
on, pages 705–708, Toronto, Ontario, Canada, May.
IEEE.

Yoshimasa Tsuruoka and Junichi Tsujii. 2004. Itera-
tive cky parsing for probabilistic context-free gram-
mars. In International Conference on Natural Lan-
guage Processing, pages 52–60, Hyderabad, India,
December. Springer.

310


