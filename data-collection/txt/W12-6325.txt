










































Chinese Name Disambiguation Based on Adaptive Clustering with the Attribute Features


Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 132–137,
Tianjin, China, 20-21 DEC. 2012

Chinese Name Disambiguation Based on Adaptive Clustering with the
Attribute Features

Wei Tian, Xiao Pan, Zhengtao Yu, Yantuan Xian, Xiuzhen Yang
School of Information Engineering and Automation,

Kunming University of Science and Technology, Kunming, China.

Abstract

To aim at the evaluation task of CLP2012
named entity recognition and disambigua-
tion in Chinese, a Chinese name disam-
biguation method based on adaptive clus-
tering with the attribute features is pro-
posed. Firstly, 12-dimensional character
attribute features is defined, and tagged at-
tribute feature corpus are used to train to
obtain the recognition model of attribute
features by Conditional Random Fields al-
gorithm, in order to do the attribute recog-
nition of given texts and knowledge bases.
Secondly, the training samples are tagged
by utilizing the correspondences of the
text attribute and answer, and attribute fea-
ture weight model is trained based on the
maximum entropy model and the weight-
s are acquired. Finally, the fuzzy cluster-
ing matrix is achieved by the correlation
of Knowledge Base(KB) ID attributes and
text attributes for each KB ID, the cluster-
ing threshold is selected adaptively based
on the F statistic, and clustering texts cor-
responding to ID are obtained, thus the
texts corresponding to each ID are gained
followed. For the texts not belong to KB,
Out and Other types are obtained by fuzzy
clustering to realize name disambiguation.
The evaluation result is: P = 0.7424, R =
0.7428, F = 0.7426.

1 Introduction

Person search is an information retrieval way for
a specific person, due to the phenomenon of name
repetition, therefore, name disambiguation prob-
lem becomes more and more important. In recent
years, various types of evaluation tasks related to
name disambiguation have been launched succes-
sively at home and abroad. One task is WPS (Web

People Search). WPS is aimed at English names
and does not provide any knowledge base, instead
it require names referring to the same entity to
be clustered together. Another related is the KBP
(Knowledge Base Population) task in TAC (Tex-
t Analysis Conference) has a named entity dis-
ambiguation task, which they use the term entity
linking. KBP provides a knowledge base (KB) of
named entities. The KB provides a mapping from
names to entities. One name can be mapped to
many entities. The goal of KBP is to link names
occurring in the document to the corresponding
entities in KB and to cluster names referring to
the same entity, if this entity is not included in
the KB. The 2nd task of the CIPS-SIGHAN2012
(CLP2012) [1]—-Named Entity Recognition and
Disambiguation in Chinese, can be seen as com-
bination of related tasks in WPS and KBP: First
the test names in the document should be judged
to be common words or named entities; if a name
is predicted as a named entity, participants should
further determine which named entity in the KB
it refer to; finally, if some names are predicted as
named entities that do not occur in the KB, par-
ticipants should instead cluster these names by the
named entities they refer to.

For the name disambiguation, most of the work
is concentrated on unsupervised-based or semi-
supervised clustering disambiguation method,
such as Wang proposed to use the vector space
model of web content to do expert evidence-pages
clustering disambiguation to solve the multi-
document coreference resolution problem to some
extent [2]. Bollegala put forward the experts clus-
tering disambiguation solution on key phrases ex-
traction automatically in the context and com-
puting similarity, particularly keyword extraction
method depended mainly on the individual in-
formation, and the entire extraction process was
prone to error cascade phenomenon [3]. Zhou
presented a two-stage method for name disam-

132



biguation based on exclusive and non-exclusive
character attributes, which can improve the dis-
ambiguation effect to some extent, but it did not
give a clear explanation for threshold selection
on the improvement of the hierarchical clustering
[4]. Zhang used hierarchical clustering algorithm
to solve the multi-text ambiguity issue of Chinese
names, though it can better distinguish the names’
features, considering verb information as features
led to a larger noise introduction without making
noise reduction processing [5]. Through the anal-
ysis of a large number of name texts, the names’
attributes in the text have an important impact on
name disambiguation. Therefore, this paper uses
the training corpus of the CLP2012 name disam-
biguation to establish the model of attribute recog-
nition and weight distribution, and applies adap-
tive clustering method, which can automatically s-
elect clustering threshold, to achieve the Chinese
name disambiguation.

2 Chinese Name Disambiguation Based
on Adaptive Clustering with the
Attribute Features

2.1 Corpus Preparation

We must first define the ambiguous name be-
fore the name disambiguation. As same as the
definition of ambiguous names in CLP2010, the
CLP2012 is based on the assumption that “one text
corresponds to one person name”, that is, suppos-
ing a text corresponds to only one person’s name,
there is no one-to-many problem between the text
and the name.

According to text analysis of the CLP2012
training corpus, we found that not all text con-
tent is to play a significant role in name disam-
biguation, but the momentous attributes related
to the person appearing in the text is very im-
portant to distinguish the persons, for example,
there are some sentences about the sports figure
inserting into the text of the artistic literature top-
ic, and a few words in above sentences written to
the attribute information, such as character’s ca-
reer, just plays the important role in the name dis-
ambiguation. Therefore, we need to select the at-
tributes related to the character as the name disam-
biguation features, which can be named as char-
acter attribute features, including 12-dimension,
respectively, the person’s name (rm), place (dm),
organization(jg), career(zy), position(zw), award-
s (ry), gender (xb), nation(mz), education back-

ground(xl), graduate school(byyx), birthday(csrq),
works(zp). KB files corresponding to each name
must be analyzed to extract ID number and cor-
responding text messages. And mark the relevant
features and do attribute recognition.

2.2 Attribute Recognition
Attribute recognition based on Conditional Ran-
dom Fields (CRFs) achieves very good recogni-
tion effect [6]. Therefore, the CRFs Tools package
is used to train on the marked attribute features
to obtain the recognition model of 12-dimension
attribute feature. The texts and KB files of the
CLP2012 test corpus are respectively done the at-
tribute feature recognition by using the recognition
model.

Due to the feature of an attribute may be re-
peated many times, the duplicate must be re-
moved after text recognition completed. Corre-
sponding to each text, there is a feature set N =
{ai|i = 1, 2, ..., 12}, whichN represents the tex-
t number, aiis the ith dimensional feature. Ac-
cording to the feature dimension defined the fea-
ture set will be organized into the form of the fea-
ture vector. Similarly, each ID which each xml file
of knowledge base contained is corresponding to
a set of attribute features. As the 001th text and to
the xml ID = 01 of “xÈ(Xue Bai)” for examples,
the attribute features of specific text and Knowl-
edge Base as shown in Table 1.

2.3 Attribute Feature Weighting
After obtaining the attribute feature vector of tex-
t file, use the answer corresponding to the tex-
t to mark the answer category which the text be-
longs to, and then consider the category number
as one new feature to add to the attribute fea-
ture vector to form a new feature vector, which
is regarded as weight training corpus. Then em-
ploy the weight training command of the maxi-
mum entropy model for training the weights of
feature functions on the corpus, namely the at-
tribute weights Woi (i = 1, 2, ..., 12) for the cor-
responding dimension.

After getting the weight of each attribute fea-
ture, the next is matching calculation of the at-
tribute features§that is similarity calculating be-
tween the attribute feature set of texts and the
KB feature collection on the test corpus. The
attribute feature matching problem is considered
as the words’ matching. The existing matching
methods mainly for Chinese words are “HowNet”,

133



Table 1: The representation examples of the at-
tribute feature set and vector of /xÈ£Xue
Bai¤0.

Document
Type

Text KB

attribute
feature

set repre-
sentation

001={xÈ
£Xue
Bai¤§ú
ô
£Zhejiang¤§
úôLì
£delegation
of Zhe-
jiang¤§y
Ã
£singer¤§
Ã
£null¤§
Ã
£null¤§
Ã
£null¤§
Ã
£null¤§
Ã
£null¤§
Ã
£null¤§
Ã
£null¤§
Ã£null¤}

01={xÈ£Xue
Bai¤§úô§²
£Wenzhou

Zhejiang¤§úô
�«©óì

£Military district
enter-tainment
regi-ment in

Zhejiang¤§yÃ
£singer¤§Ã
£null¤§Ã
£null¤§Ã
£null¤§Ã
£null¤§Ã
£null¤§úô§
²{�zs�
ìì£Xiaobaihua
Yueju regiment in

Qingxian,
Wenzhou

Zhejiang¤§1975
c2�28F

£birthday¤§Ã
£null¤}

feature
vector

represen-
tation

£001xÈ
£Xue
Bai¤úô
£Zhejiang¤
úôLì
£delegation
of
Zhejiang¤
yÃ
£singer¤
Ã£null¤
Ã£null¤
Ã£null¤
Ã£null¤
Ã£null¤
Ã£null¤
ÃÃ
£null¤¤

£01xÈ£Xue
Bai¤úô§²
£Wenzhou

Zhejiang¤úô�
«©óì

£Military district
entertainment
regiment in

Zhejiang¤yÃ
£singer¤Ã
£null¤Ã
£null¤Ã
£null¤Ã
£null¤Ã

£null¤úô§²
{�zs�ì
ì£Xiaobaihua

Yueju regiment in
Qingxian,
Wenzhou

Zhejiang¤1975
c2�28FÃ
£null¤¤

“Tongyici Cilin” and “Chinese Concept Dictio-
nary” [7]. Word similarity calculated by “Tongyi-
ci Cilin” is the closest to the similarity of people’s
thinking, so Cilin is selected to calculate the simi-
larity. On the basis of analyzing the classification
mode and the word-coding table of Cilin [7] and
related theory of meanings, the meaning similari-
ty of the two words is calculated according to their
meanings coding and the maximum is taken as the
similarity finally, the calculating method is shown
as follows:

Assume Sim(x, y) is the similarity of the t-
wo meanings, if the first letter of the two words’
meaning code is the same, then in the same treeT ,
where T ∈ {A,B,C,D,E, F,G,H, I, J,K,L}.
The formula of Sim(x, y) is shown as follows:

Sim (x, y) =



f, (x ∈ T1, y ∈ T2)
δ × cos

(
n× π180

) (
n−k+1
n

)
, (x, y ∈ T1, Cx 6= Cy)

e,

(
Cx = Cy,
Cxend = Cyend =

′ #′

)

1,

(
Cx = Cy,
Cxend = Cyend =

′=′

)
(1)

Whereδ ∈ {a, b, c, d}, and if x and y branches
at the second layer, then the coefficientδ = a =
0.65, similarly, the thirdδ = b = 0.8, the fourth
δ = c = 0.9, the fifth δ = d = 0.96. In order to
control the similarity between 0 and 1, a parame-
ter cos

(
n× π180

)
is introduced, where n is the to-

tal number of nodes of branch layers, the control
parameter

(
n−k+1
n

)
and kis the distance between

two branches. Define Cx, Cy as the meaning code
of x, y, and Cxend, Cyend is respectively the end
symbol of x, y. Take f = 0.1, e = 0.5 as a matter
of experience.

A large number of statistical results show that
two words with the similarity above 0.7 are gener-
ally considered to have a similar meaning in peo-
ple’s thinking, so defined that if Sim (x, y) ≥ 0.8,
then the attribute features in the same dimension is
perceived as matching successful. The weight of
the vector matching successful is regardedWoi∗10
as matching weightWmi (i = 1, 2, ..., 12), and the
matching weight becomes 0 if the matching is not
successful.

2.4 Fuzzy Clustering Matrix Construction

After matching the attribute features between each
text and any one ID (short text) in the knowledge

134



base, the matching weight vector of each text cor-
responding to the above ID gotten by matching, is
the row vector of initial matrix. Since the initial
matrix is not square, which is the product of at-
tribute feature matching and not the similarity of
the texts in the true sense. All above makes that
the adaptive clustering can not work. Therefore
transform and adjust on the initial matrix by the
cosine of the angle is to make it become the fuzzy
clustering matrix of fuzzy clustering.

Assume text setU = {x1, x2, ..., xn}, where
xi = {xi1, xi2, ..., xim} (m = 12) is the attribute
feature vector, build the fuzzy clustering matrix.
The similarity between xi and xj is:

A = rij =

m∑
k=1

xik · xjk√
m∑
k=1

x2ik

√
m∑
k=1

x2jk

(2)

Where xik, xjk represents the feature vector in the
same dimension between texts, and calculate to
obtain the similarity matrix, that is fuzzy cluster-
ing matrixA.

2.5 The Adaptive Clustering Based on the
Attribute Features

The Adaptive Algorithm Thought and the Pro-
cess Description. The fuzzy clustering is a com-
mon clustering method in pattern recognition, and
has achieved very good effect on pre-classification
of characters in Chinese character recognition [8]
and classification and matching of speech recog-
nition [9]. Aiming at the task characteristics, dif-
ferent text may has different attribute feature rela-
tionships with the knowledge base. If we use the
same clustering threshold, it may cause one ID-
type clustering better, while another is not good
consequences. Thus this paper selects fuzzy clus-
tering method to do name disambiguation pro-
cessing, according to the difference between the
fuzzy clustering matrix generated each time and
the content of knowledge base ID, adjust the clus-
tering threshold dynamically and adaptively, and
then cluster for each ID of the Knowledge Base
through adaptive clustering way. The main idea is
to make the classical partition definition fuzzifica-
tion and dynamically adjust the threshold, which
can be solved effectively that 0,1 binary member-
ship can not fully reflect the actual relationship be-
tween the data points and the cluster center.

Different thresholdsλ ∈ [0, 1]can lead to differ-
ent classifications in Fuzzy clustering analysis, in

order to form a dynamic clustering diagram, which
makes the classification of the sample image and
intuitive. We need to find the optimal λ to ef-
fectively cluster some texts with their correspond-
ing ID of KB, and then the clustering result corre-
sponding to λ now is the best result. In this paper,
theF statistic is used to determine the optimal λ.

Set the text set U = {x1, x2, . . . xn} is the text
sample space, and each textxihas mfeatures:xi =
(xi1, xi2, ..., xim) (i = 1, 2, ..., n).Thereby
the initial matrix is obtained, where

xk =
1
n

n∑
i=1

xik (k = 1, 2, . . . ,m) , and x

represents the center vector of the overall sample,
that is any one ID of KB. Set the number of
categories is r corresponding to λ , the number of
texts is nj in the jth cluster, x

(j)
1 , x

(j)
2 , ..., x

(j)
nj is

denoted. The cluster center, that is the jth ID of K-
B, of thejth cluster is x(j) =

(
x

(j)
1 , x

(j)
2 , ..., x

(j)
m

)
,

wherex(j)k is the average of thekth features, namely

x
(j)
k =

1
nj

nj∑
i=1

x
(j)
ik (k = 1, 2, ...,m).

The F statistic is shown as follow:

F =

r∑
j=1

nj
∥∥∥x(j) − x∥∥∥2/(r − 1)

r∑
j=1

nj∑
i=1

∥∥∥x(j)i − x(j)∥∥∥2
/

(n− r)
(3)

Figure 1: The flowchart of adaptive clustering for
name disambiguation

135



Where
∥∥∥x(j) − x∥∥∥ = √ m∑

k=1

(
x

(j)
k − xk

)2
is the

distance between x(j)and x,
∥∥∥x(j)i − x(j)∥∥∥ shows

the distance of x(j)i and the center x
(j) in the jth

sample. The formula (1) is named F statistic,
which follows a Fdistribution with the degree of
freedomr − 1, n − r. For the F statistic, the dis-
tance between clusters is represented by the nu-
merator while the distance in one cluster, the de-
nominator. So the largerF statistic, the larger dis-
tance between clusters, that is the larger distance
is inferred between the texts not related to the ID
and the texts corresponding to, which shows out a
better clustering result.

It can be known that the difference between
clusters is significant and illustrates a more rea-
sonable classification result according to the theo-
ry of mathematical statistics and analysis of vari-
ance, if F > Fα (r − 1, n− r) (α = 0.05). If
there are more than one F statistics meeting the re-
quirements, (F − Fα)/Fαmust be examined fur-
ther, and we can get the maximum of F , which the
λcorresponding to is the optimal threshold.

The Realization of the Adaptive Clustering.
According to the evaluation task of CLP2012 Chi-
nese name disambiguation, the final answer to the
clustering usually consists of three types, name-
ly one is the ID type marked in the “KB”, an-
other is the “out” type, which contains not only
text attribute features but also not appeared in the
Knowledge Base. Besides, there is an “other” type
not containing entities and considered as ordinary
word. So each type is processed respectively in
this article.

For “KB” type, firstly the attribute feature cor-
relation of KB ID and text is used to obtain
fuzzy clustering matrix for each KB ID. Second-
ly adaptive clustering threshold is adaptively se-
lected based on the F statistic, and the clustering
result corresponding to the threshold is acquired,
that is, the texts corresponding to the above thresh-
old. Finally these texts clustered should exclude,
and then the rest of the texts and the next ID is
used for clustering. Repeat the above process until
the rest of the text can not be clustered into a group
with the KB ID. For the “other” type, if the texts
not related to the KB are extracted no attribute fea-
tures, and then these texts are regarded as “oth-
er” type. For the “out” type, clustering, , a text in
the texts excluded the “KB” type and the “other”
is randomly selected as a basis for the matching

of attribute features, and fuzzy clustering matrix
is obtained, then clustering threshold is adaptively
chosen to get the clustering result according to the
F statistic.

3 Experiments

3.1 Experimental Data

Table 2: Experimental data statement.

Experimental
Data

The number of
text set

The number of
text in each

text
training data 16 50-200
test data 32 50-500

There are two types of data given in the evalu-
ation. One is knowledge base, NameKB. A XML
file for each test name is provided. This file con-
tains several entries describing the name. The file
is named as Name.xml, where Name is the test
name. For example, the file for X(Yu Lei) is
X(Yu Lei).xml. Another is text collection, T
for each test name. All texts containing the name
N are placed under the folder N. For example, all
text containingX(Yu Lei) are under the folder
X(Yu Lei). Every file in the folder is a plain
text file, named as XXX.txt, where XXX is three
numbers.

The evaluation tool used in the experimen-
t is provided by the evaluation project group of
CLP2012. The overall evaluation indexes are pre-
cision, recall and F-value for all test names.

3.2 Experiment Results and Analysis

Do the experiment on the test data by using our
approach. The evaluation results are given as fol-
lows:

Table 3: The evaluation index comparisons of
training data and test data.

DataSets Precision Recall F-
value

training data 0.9256 0.9032 0.9143
test data 0.7424 0.7428 0.7426

As can be seen from the results, the attribute
recognition model for name disambiguation has

136



taken good effect. The identification effect is bet-
ter on training data than the test data. Analyzing
the reasons, the recognition errors may be caused
by a variety of reasons. For example, the error that
the original text is related with name but identified
to common word that accounted for 1/2 of the er-
ror portion. According to the statistics, the error
distribution is shown in the following table.

Table 4: The distribution of the attribute feature
recognition errors.

Error Types Error
Proportion

names are recognized
to common words

0.5162

only recognized a part
of names

0.1956

common words are
recognized to names

0.0659

the most important
attribute is not
identified

0.2223

4 Conclusion

For the characteristics of the evaluation task
CLP2012 named entity recognition and disam-
biguation in Chinese, a Chinese name disambigua-
tion method based on adaptive clustering with the
attribute features is proposed, which will resolve
this complex disambiguation task into KB type,
out and other three types for processing. Do the
attribute recognition of given texts and knowledge
bases§using the recognition model of attribute
features trained by Conditional Random Fields al-
gorithm. Then the attribute feature weight model
is trained by utilizing the corresponding attribute
feature with answer tag based on the maximum
entropy model. After that, the initial matrix is ob-
tained by matching and weighting on the attribute
features, on which the fuzzy clustering matrix is
generated by transforming, and then clustering by
the adaptive method. The algorithm is character-
ized in automatically finding the optimal cluster-
ing threshold to realize name disambiguation ac-
cording to the different contents of the text and
knowledge base. Further research will focus on
non-attribute feature selection and the clustering
method optimization.

Acknowledgments

This paper is supported by National Nature Sci-
ence Foundation (No.61175068), and the Open
Fund of Software Engineering Key Laboratory of
Yunnan Province (No.2011SE14), and the Min-
istry of Education of Returned Overseas Students
to Start Research and Fund Projects. We appreci-
ate the help and assistance of Yu Qin and Wenxu
Long.

The corresponding author is Zhengtao
Yu(ztyu@hotamil.com).

References
CIPS, SIGHAN. 2012. http://www.cipsc.org.cn

/clp2012/task2.html. Tianjin, China.

Houfeng Wang, Zheng Mei. 2005. Chinese multi-
document person name disambiguation. High Tech-
nology Letters, 11(3):280–283.

Bollegala D, Matsuo Y, Ishizuka M. 2006. Dis-
ambiguation person names on the Web using auto-
matically extracted key phrases. Proceedings of the
17th European Conference on Artificial Intelligence,
553–557. Riva del Garda, Italy.

Xiao Zhou, Chao Li, Minghan Hu, Huizhen Wang.
2006. Chinese Name Disambiguation Based on Ex-
clusive Character Attributes. The 6th CCIR Confer-
ence Proceedings, 2010:333–340. Harbin, China.

Shunrui Zhang, Lianghong You. 2010. Chinese Peo-
ple Name Disambiguation by Hierarchical Cluster-
ing. New Techology of Library and Information Ser-
vice, 2010(11):64–68.

John L., Andrew M., Fernando P.. 2001. The ICM-
L2001 Proceedings, 2001:282–289.

Jiule Tian, Wei Zhao. 2010. Word Similarity Al-
gorithm Based on Tongyici Cilin in Semantic Web
Adaptive Learning System. Journal of Jilin Univer-
sity (Information Science Edition), 2010,28(6):602–
608.

Da Lu, Mingpei Xie, Wei Pu. 2000. A Character
Preclassification Method Based on Fuzzy Structure
Analysis of Typographical Characters. Journal of
Software, 2000,11(10):1397–1404.

Xiangdong Yu, Xiuyun Suo, Jianren Zhai. 2002.
Speech Recognition Based on Fuzzy Clustering.
Fuzzy Systems and Mathematics, 2002,16(1):75–79.

137


