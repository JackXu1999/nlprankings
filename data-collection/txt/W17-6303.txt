



















































Lexicalized vs. Delexicalized Parsing in Low-Resource Scenarios


Proceedings of the 15th International Conference on Parsing Technologies, pages 18–24,
Pisa, Italy; September 20–22, 2017. c©2017 Association for Computational Linguistics

Lexicalized vs. Delexicalized Parsing in Low-Resource Scenarios

Agnieszka Falenska and Özlem Çetinoğlu
Institute for Natural Language Processing

University of Stuttgart
{falenska,ozlem}@ims.uni-stuttgart.de

Abstract

We present a systematic analysis of lexi-
calized vs. delexicalized parsing in low-
resource scenarios, and propose a method-
ology to choose one method over another
under certain conditions. We create a
set of simulation experiments on 41 lan-
guages and apply our findings to 9 low-
resource languages. Experimental results
show that our methodology chooses the
best approach in 8 out of 9 cases.

1 Introduction

The recent CoNLL Shared Task on Parsing Uni-
versal Dependencies (CoNLL-ST) (Zeman et al.,
2017) gave researchers the opportunity to study
dependency parsing on a wide selection of tree-
banks. While the ultimate goal remained the same,
i.e., achieving the best accuracy in predicting the
head and dependency label of a token, the start-
ing point changed from one group of languages to
another, depending on the available resources.

In the surprise languages scenario, participants
were given a very small training treebank, no de-
velopment set, relatively accurate POS tags for
the test set, and little or no parallel data.1 When
parallel data is not available, many of the stan-
dard cross-lingual parsing techniques (e.g. an-
notation projection (Hwa et al., 2005; McDon-
ald et al., 2011); treebank translation (Tiedemann
and Agić, 2016); utilizing cross-lingual word clus-
ters (Täckström et al., 2012) or word embeddings

1This is due to the CoNLL-ST rules that restricted the use
of parallel resources to OPUS (Tiedemann, 2012). But ac-
tually no or little parallel data is not an unrealistic assump-
tion. Among four surprise languages, three of them have
only Linux distro translations in OPUS, none are part of the
135-language Watchtower Corpus (Agić et al., 2016) (but
two have a few documents on the Watchtower website), and
none are part of the 100-language Edinburgh Bible Corpus
(Christodouloupoulos and Steedman, 2015).

(Duong et al., 2015; Guo et al., 2015)) become im-
possible to apply.

Delexicalized parsing (Zeman and Resnik,
2008) provides a suitable alternative, while it does
not require parallel data. The central idea is to
train a source-side parser without any lexical fea-
tures, i.e., typically using only POS tags, and
then use this trained parser to parse a target, low-
resource language that shares the same POS tag
set. No gold trees are required on the target side,
and only POS tags have to be predicted prior to
parsing. Given the simplicity of this method, sev-
eral CoNLL-ST participants have chosen delex-
icalized approaches, not only for surprise lan-
guages but also for the other CoNLL-ST scenario
– small languages2. In this scenario, as opposed
to the surprise languages, the small training tree-
banks were the only source of gold POS tags.

When the data for training POS taggers is small
– as for the small languages scenario as well as
for many upcoming UD treebanks3 – the delexi-
calized methods might be affected by poor POS
accuracy. On the other hand, there are some gold
trees for those scenarios and it is possible to train
lexicalized parsers on them. Could there be cases
in which such a low-resource lexicalized parser is
preferred over a delexicalized one?

The central question we examine is whether we
can find cases where a low-resource lexicalized
parser achieves better accuracy than a delexical-
ized one. As a related problem, we investigate the
following case: when there is a new language to
parse, with no treebank but with the chance to pre-
dict POS tags, should one pursue a delexicalized
parsing or invest in some tree annotation?

Our goal is to investigate those questions sys-

2The set of small treebanks with no development sets.
3For instance, currently there are 17 upcom-

ing treebank projects within Universal Dependencies
http://universaldependencies.org/#upcoming-ud-treebanks.

18



tematically in low-resource settings and to find the
conditions under which one strategy leads to better
results than the other. While our scenarios origi-
nate from the CoNLL-ST, our approach should be
applicable to other settings. For example, our con-
clusions might prove helpful in developing early
parsing models of a new treebank or in deciding
how to proceed when there is a large gold POS
tagged corpus but no trees (e.g. Echelmeyer et al.
(2017) present a Middle High German corpus with
20,000 tokens of gold POS, no trees, and no appar-
ent parallel data). They can also help plan resource
creation. While POS annotation can be relatively
fast (Garrette and Baldridge, 2013), creating tree-
banks is costly (Zeman and Resnik, 2008; Souček
et al., 2013). The decision of building a large POS
annotated corpus vs. a small treebank in a lim-
ited time, could depend on whether delexicalized
models would work well for a target language.

2 Methodology

We compare low-resource lexicalized vs. delex-
icalized parsing in two settings used for surprise
and small languages scenarios of the CoNLL-ST.
In the first scenario, we assume the existence of
a small treebank, and an external POS-annotated
corpus – larger than the treebank – to train a POS
tagger (EXTPOS). In the second scenario, only
the treebank exists, thus the gold POS tags neces-
sary to train a tagger must be extracted from the
treebank (TBPOS). In both cases, we change the
treebank sizes to observe the difference in accu-
racy between lexicalized and delexicalized pars-
ing. While the POS accuracy is not affected from
the treebank size in EXTPOS, it changes with the
size of the treebank in TBPOS.

We employ multi-source delexicalized parsing
(McDonald et al., 2011) in both scenarios. We fol-
low Rosa and Žabokrtský (2015) and Agić (2017)
in combining sources by blending (also known
as reparsing) (Sagae and Lavie, 2006). Unlike
in their original study, we apply blending on la-
beled arcs. We also employ weighted blending
by assigning weights to sources based on lan-
guage/treebank similarity measures.

2.1 Similarity Measures

We employ two measures of similarity between
languages used in previous work:

KLpos: Rosa and Žabokrtský’s (2015) KL di-
vergence metric (Kullback and Leibler, 1951) be-

tween POS trigrams. Instead of their smoothing,
we use Laplace smoothing with α = 0.01.

WALS: Agić’s (2017) Hamming distance be-
tween each language’s feature vectors from The
World Atlas of Language Structures (WALS)
(Dryer and Haspelmath, 2013). For languages
with no WALS entry, or for languages with less
than 10 features in common, WALS is not defined.

2.2 Weights

We use three methods employing aforementioned
similarity measures for weighting source lan-
guages while blending:

R&Z15: Rosa and Žabokrtský’s (2015) KLpos
calculation between the gold POS tags of the
source and target training data, and their KL−4

weighting.
A17: Agić’s (2017) combined weighting that

calculates KLpos between the gold POS tags of the
source training data and the predicted POS tags of
the target development data, and then combines it
with WALS.

LAStgt: We utilize gold trees as a source of
weights for the blender. We parse the target train-
ing trees with the source delexicalized parsers and
use their LAS as weights. Moreover, we rank the
sources and take the n-best giving the best blended
accuracy. n is tuned for every treebank and every
training size separately on the training data.

3 Experimental Setup

Data We use the Universal Dependencies v2.0
treebanks (Nivre et al., 2016) released for the
CoNLL-ST (Nivre et al., 2017). We use all the
treebanks except domain-specific4 ones as sources
(46 languages). As targets we take two groups of
languages from the CoNLL-ST that correspond to
the two settings we experiment with:

Surprise languages: Kurmanji (kmr), Upper
Sorbian (hsb), North Sami (sme), Buryat (bxr).
Each language contains a small sample of gold
training data (see Table 1) and its test set is pro-
vided with POS tags predicted by a system trained
on a data set much larger than the training data.
Those languages represent the EXTPOS setting.

Small languages: Latin (la), Irish (ga),
Ukrainian (uk), Kazakh (kk), Uyghur (ug). They
have small treebanks (especially Kazakh and

4Some languages have multiple UD treebanks, often from
different domains. In such cases we chose the canonical tree-
bank for a language.

19



0 200 400 600 800 1000 1200 1400 1600
Training data size in #tokens

35

40

45

50

55

60

65

av
er

ag
e

L
A

S

LEX
R&Z15
A17
LAStgt

(a) Average parsing accuracy. The average cutting
point is marked by the red dot.

0 1000 2000 3000 4000 5000
Training data size in #tokens

20

30

40

50

60

70

80

L
A

S

ar

bg ca

cs (8k)da

de

el

en

es

et

eu
fa

fi

fr

he

hi hr

hu

id

it (5k)

ja

ko

lv

nl

pl

pt (13k)

roru

sk (18k)
sl (5k)

sv

tr

ur

vi

zh

cu

gl

got

grc

0.00 0.04 0.08 0.12 0.16 0.20 0.24 0.28 0.32 0.36 0.40

Best source distance by WALS

(b) Cutting points for all targets. Points which fall far right are moved left
and their corresponding training sizes are given in brackets. cross (X): no
WALS entry. circle size ∼ number of most similar sources (WALS < 0.2).

Figure 1: Parsing accuracies for EXTPOS scenario.

Uyghur), with no additional POS data. They cor-
respond to the TBPOS setting.

The 9 target treebanks do not contain develop-
ment sets, so as to not compromise our test sets,
we set those languages aside as test cases. We
perform analysis on simulated low-resource lan-
guages instead. For this purpose we use the sub-
set of source treebanks that has a development set
(41 languages). For each of them we sample small
training treebanks starting with only 100 tokens.

Tools To train POS taggers, we employ Mar-
MoT (Müller et al., 2013). In EXTPOS, POS tag-
gers are trained on the whole treebanks, where in
TBPOS, training is done only on small samples.
We use Universal Part of Speech tags (UPOS) in
all experiments.

For parsing, we use a beam-search transition-
based parser (Björkelund and Nivre, 2015).5

Delexicalized parsers (DELEX), and lexicalized
parsers (LEX) for TBPOS are trained on gold
POS tags. For EXTPOS lexicalized parsers are
trained on 5-fold jackknifed POS tags for better
performance (we sample the small treebanks after
performing jackknifing). We blend delexicalized
models’ outputs via methods described in Section
2.2. In presenting the experimental results, we re-
fer to these DELEX models with their weighting
scheme, namely R&Z15, A17, and LAStgt.

Evaluation We use labeled attachment score
(LAS) as the evaluation metric and evaluate using
the script provided by the CoNLL-ST organizers.

5We also experimented with the graph-based parser Mate
(Bohnet, 2010) to test our hypotheses on a different parsing
architecture. We achieved results parallel to the transition-
based parser, thus we only present one set of results.

4 Results and Analysis

We apply LEX and DELEX to our artificially cre-
ated low-resource languages and analyze the re-
sults for EXTPOS and TBPOS scenarios.

4.1 The EXTPOS Scenario

Figure 1(a) shows average results for EXTPOS.
We observe that DELEX methods using POS tags
as weight source (A17 and R&Z15) achieve on av-
erage 55% LAS. Both of them are surpassed by
LEX when gold trees are available for only 750 to-
kens (which corresponds to 40 sentences for the
used treebanks). Creating weights from gold trees
(LAStgt) instead of POS tags improves the per-
formance but the improvement is modest and LEX
trained on only 1,100 tokens (avg. 55 sentences)
outperforms it. We find this small number surpris-
ing, and investigate it further.

Instead of looking at averages we analyze target
languages separately. We call the point in which
LEX surpasses LAStgt (the red dot in Figure 1(a))
a cutting point and plot the cutting points for all
the target languages separately (Figure 1(b)). We
note that most of the languages do not fall around
the average cutting point of 1,100 tokens. Instead
they can be grouped into three clusters: 10 lan-
guages on the far left for which LEX trained on
even less than 500 tokens (avg. 25 sentences) is
better than DELEX; 9 languages on the far right
for which even 3,400 tokens (avg. 176 sentences)
is not enough to surpass DELEX; and the middle
part. In order to explain this distribution we take a
look at the target languages characteristics.

We use WALS to measure the distance between
languages (we normalize WALS by the number of

20



0 200 400 600 800 1000 1200 1400 1600
Training data size in #tokens

15

20

25

30

35

40

45

LEX
R&Z15
A17
LAStgt

(a) Average parsing accuracy. The average cutting
point is marked by the red dot.

0 500 1000 1500 2000 2500 3000
Training data size in #tokens

0

10

20

30

40

50

60

L
A

S

ar

bg

ca

cs

cu

da

de
el

en

es

et

eu
fa

fi

fr

gl
got

grc

he
hi

hr

id

it

ja

ko

lv

nl
pl

pt

ro ru

sk
sl sv

tr

ur

vi

zh
45 50 55 60 65 70 75 80 85

POS accuracy

(b) Cutting points for all target languages.

Figure 2: Parsing accuracies for TBPOS scenario.

common features). For every target we select the
best source according to WALS and represent its
distance by color (the darker the circle the more
similar the best source is). The number of good
sources (WALS < 0.2) is represented by size (the
bigger the circle the more good sources exist). For
example, Korean’s (ko) best source according to
WALS is Urdu with a distance of 0.27 and its cir-
cle is light and small. In contrast, Slovenian (sl)
for which five good sources exist (among which
Ukrainian has the smallest distance of 0.03) is rep-
resented by a dark and big circle.

In Figure 1(b) we can observe a pattern among
the two border groups. The left group tends to
have small and light circles which means that there
is no good source for them. When we look at the
languages which fall into this group (like Arabic
(ar) and Vietnamese (vi)) we see that they come
from language families less represented among the
source languages. The far right circles in compar-
ison are bigger and darker which means that they
fit well into the set of existing source languages.
Indeed many Slavic languages fall here.

4.2 The TBPOS Scenario

Figure 2(a) shows average results for TBPOS. As
expected the accuracy of parsers drops due to
lower POS accuracy. Other than the LAS scores,
there are two major differences between those two
plots. In this setting, POS taggers are trained on
tags coming from the treebanks and POS accuracy
changes with the data size. That is why the A17
and R&Z15 lines are not flat any more. The dif-
ference between them is once again very slight and
LAStgt outperforms both of them for all data sizes.

As the second major difference to EXTPOS,

LEX quickly outperforms DELEX methods at
around 250 tokens. We show the breakdown of
this cutting point by target language in Figure 2(b).
The shades of the circles denote the POS accuracy
(the darker the more accurate). In this case, the
circles are placed in a smaller area with an upper
limit of 3,100 tokens. Note that the source lan-
guage similarity is not shown in this plot, yet it
still affects the underlying distribution of targets.
Comparing to Figure 1(b), the relative placement
of the languages is almost the same. The dense
distribution also causes a more homogeneous scat-
ter making the groupings less visible.

4.3 Overall Picture

Delexicalized models use only POS tags as fea-
tures and therefore are more influenced by low
POS tagging accuracy than lexicalized parsers.
That is why the choice which method to apply de-
pends strongly on the existing resources.

In EXTPOS in order for DELEX to work better
than LEX good sources must exist. If the target
belongs to an underrepresented language family,
even a very small sample of gold trees is enough
for LEX to achieve better results. In contrast, if the
target is similar to many sources DELEX can help
even if the target gold data exists. In that case,
the gold trees can be exploited for example as a
source for weights for blending. For all our targets
regardless of existing sources, having a treebank
bigger than 18,000 tokens is enough for LEX to
give higher accuracies than any DELEX model.

In TBPOS on average LEX outperforms DELEX
even when trained on only few gold sentences.
The only situation when using other sources might
help is when there are many similar sources and

21



gold trees for more than 1,000 tokens (avg. 50
sentences). In that case, the POS tagging accu-
racy is able to reach a reasonable 70% and DELEX
achieves comparable performance to LEX. But al-
ready having a treebank bigger than only 3,100 to-
kens (avg. 160 sentences) is enough for LEX to
work better for all our targets.

5 Application to Test Languages

We use findings from Section 4 to decide which
test languages should employ DELEX methods.

In EXTPOS all the treebanks have less than
500 tokens and according to Figure 1(a) DELEX
methods might be a good choice. We compare the
test languages with the breakdown in Figure 1(b).
Buryat is a Mongolic language and does not have
any close relatives among the source languages.
For Kurmanji there is only one similar language
– Persian. Therefore for both of those languages
the cutting point would likely occur quickly and
DELEX would give no or very little improvement
over LEX. On the contrary, North Sami is Finno-
Ugric and Finnish and Estonian should be good
sources for it. Upper Sorbian is Slavic and its fam-
ily is well represented (e.g. by Czech or Polish).
Therefore DELEX should work very well for them.

In TBPOS, most of the languages are much big-
ger than 3,100 tokens and for none of them DELEX
should help. Kazakh is smaller than the threshold
of 1,000 tokens. Most probably POS tagging accu-
racy for it would be poor and LEX should be a bet-
ter choice to overcome that. The only language for
which DELEX methods might help is Uyghur. But
it is a language for which not many good sources
exist, the closest being Turkish. Most probably
LEX is also a better choice in this case.

To test our hypotheses, we apply all the meth-
ods to the test languages and present results in
Table 1. For languages not present in WALS
(Kazakh, Uyghur) A17 uses only KLpos. We do
not apply R&Z15 to the surprise languages since
their POS tagging training data is not available6.
We see that our intuition was right in 8 out of 9
cases. For all the small languages LEX performs
better. For the surprise languages as expected Up-
per Sorbian and North Sami gain from DELEX the
most – 9.22 and 6 points LAS respectively when
compared to LAStgt. For Kurmanji LEX trained on
only 242 tokens (20 sentences) gives 2.12 points

6Their test sets were annotated via jackknifing by the
CoNLL-ST organizers to mimic EXTPOS scenario.

size POS LEX R&Z15 A17 LAStgt

Sm
al

l

la 18184 84.41 41.02 33.62 35.06 35.06
ga 13826 89.99 64.66 41.75 42.17 44.54
uk 12846 88.58 64.81 62.20 58.46 63.67
ug 1662 74.96 34.81 21.04 20.72 30.11
kk 529 58.48 26.55 21.49 24.49 26.17

Su
rp

ri
se

hsb 460 90.30 49.59 – 57.09 58.81
kmr 242 90.04 40.94 – 38.82 38.17
bxr 153 84.12 28.06 – 29.07 32.01
sme 147 86.81 29.8 – 33.44 35.80

Table 1: Results on the test languages.

LAS more than the best DELEX. Surprisingly, for
Buryat both DELEX methods outperform LEX.

6 Conclusion

In this paper we looked into lexicalized vs. delex-
icalized parsing in cases where there are few trees
for targets, POS tagging accuracies for the test set
vary, and no reasonable amount of parallel data be-
tween source and target languages is available. To
systematically compare these two approaches and
to observe under what circumstances one is more
favorable than the other, we created a simulation
scenario of 41 low-resource languages and applied
our findings to a set of 9 real low-resource targets.

We found out that lexicalized parsing can sur-
pass delexicalized methods even when trained on
very few sentences, so one should not be deceived
by the small size of a target treebank. By ana-
lyzing the typological relations between the source
and target languages, and the accuracy of POS tag-
ging it is possible to develop intuition about which
of two methods to apply to a new language. For 8
out of 9 test languages our findings hold true.

In our experiments we assumed specific con-
straints on existing resources, e.g., no parallel data
between source and target languages. If some par-
allel data is available, other transfer parsing ap-
proaches should be taken into comparison. For
instance, Agić et al. (2016) analyze a related sce-
nario where parallel data is available but no gold
trees. They show that projecting POS and depen-
dency annotations from multiple source languages
outperforms single-best delexicalized parsing as
well as blending. Whether such a method would
be a preferable choice when a small amount of
gold trees is available is a venue to explore.

Acknowledgments

This project is funded by the Deutsche
Forschungsgemeinschaft (DFG) via the SFB
732, projects D2 and D8 (PI: Jonas Kuhn).

22



References
Željko Agić. 2017. Cross-lingual parser selection

for low-resource languages. In Proceedings of the
NoDaLiDa 2017 Workshop on Universal Dependen-
cies. Gothenburg Sweden, pages 1–10.

Željko Agić, Anders Johannsen, Barbara Plank, Hc-
tor Martnez Alonso, Natalie Schluter, and Anders
Søgaard. 2016. Multilingual projection for parsing
truly low-resource languages. Transactions of the
Association for Computational Linguistics 4:301–
312.

Anders Björkelund and Joakim Nivre. 2015. Non-
deterministic oracles for unrestricted non-projective
transition-based dependency parsing. In Proceed-
ings of the 14th International Conference on Parsing
Technologies. Association for Computational Lin-
guistics, Bilbao, Spain, pages 76–86.

Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proc. of
COLING.

Christos Christodouloupoulos and Mark Steedman.
2015. A massively parallel corpus: the bible in
100 languages. Language resources and evaluation
49(2):375.

Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig. http://wals.info/.

Long Duong, Trevor Cohn, Steven Bird, and Paul
Cook. 2015. A neural network model for low-
resource universal dependency parsing. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing. Association for
Computational Linguistics, Lisbon, Portugal, pages
339–348.

Nora Echelmeyer, Nils Reiter, and Sarah Schulz. 2017.
Ein PoS-Tagger fur das Mittelhochdeutsche . In

Book of Abstracts of DHd 2017 . Bern, Switzerland.
https://doi.org/10.18419/opus-9023.

Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Association for Computational Linguistics, Atlanta,
Georgia, pages 138–147.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2015. Cross-lingual depen-
dency parsing based on distributed representations.
In Proceedings of ACL-IJCNLP. Beijing, China,
pages 1234–1244.

Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural language engineering 11(3):311–325.

Solomon Kullback and Richard A Leibler. 1951. On
information and sufficiency. The Annals of Mathe-
matical Statistics pages 79–86.

Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the conference on empir-
ical methods in natural language processing. Asso-
ciation for Computational Linguistics, pages 62–72.

Thomas Müller, Helmut Schmid, and Hinrich Schütze.
2013. Efficient higher-order CRFs for morpholog-
ical tagging. In Proceedings of EMNLP. Seattle,
Washington, USA, pages 322–332.

Joakim Nivre, Željko Agić, Lars Ahrenberg, Maria Je-
sus Aranzabe, Masayuki Asahara, Aitziber Atutxa,
Miguel Ballesteros, John Bauer, Kepa Ben-
goetxea, Riyaz Ahmad Bhat, Eckhard Bick,
Cristina Bosco, Gosse Bouma, Sam Bowman,
Marie Candito, Gülşen Cebirolu Eryiit, Giuseppe
G. A. Celano, Fabricio Chalub, Jinho Choi, Çar
Çöltekin, Miriam Connor, Elizabeth Davidson,
Marie-Catherine de Marneffe, Valeria de Paiva,
Arantza Diaz de Ilarraza, Kaja Dobrovoljc, Tim-
othy Dozat, Kira Droganova, Puneet Dwivedi,
Marhaba Eli, Tomaž Erjavec, Richárd Farkas, Jen-
nifer Foster, Cláudia Freitas, Katarı́na Gajdošová,
Daniel Galbraith, Marcos Garcia, Filip Ginter, Iakes
Goenaga, Koldo Gojenola, Memduh Gökrmak,
Yoav Goldberg, Xavier Gómez Guinovart, Berta
Gonzáles Saavedra, Matias Grioni, Normunds
Grūzītis, Bruno Guillaume, Nizar Habash, Jan
Hajič, Linh Hà M, Dag Haug, Barbora Hladká,
Petter Hohle, Radu Ion, Elena Irimia, Anders Jo-
hannsen, Fredrik Jørgensen, Hüner Kaşkara, Hiroshi
Kanayama, Jenna Kanerva, Natalia Kotsyba, Simon
Krek, Veronika Laippala, Phng Lê Hng, Alessan-
dro Lenci, Nikola Ljubešić, Olga Lyashevskaya,
Teresa Lynn, Aibek Makazhanov, Christopher Man-
ning, Cătălina Mărănduc, David Mareček, Héctor
Martı́nez Alonso, André Martins, Jan Mašek,
Yuji Matsumoto, Ryan McDonald, Anna Mis-
silä, Verginica Mititelu, Yusuke Miyao, Simon-
etta Montemagni, Amir More, Shunsuke Mori, Bo-
hdan Moskalevskyi, Kadri Muischnek, Nina Musta-
fina, Kaili Müürisep, Lng Nguyn Th, Huyn Nguyn
Th Minh, Vitaly Nikolaev, Hanna Nurmi, Stina
Ojala, Petya Osenova, Lilja Øvrelid, Elena Pascual,
Marco Passarotti, Cenel-Augusto Perez, Guy Per-
rier, Slav Petrov, Jussi Piitulainen, Barbara Plank,
Martin Popel, Lauma Pretkalnia, Prokopis Proko-
pidis, Tiina Puolakainen, Sampo Pyysalo, Alexan-
dre Rademaker, Loganathan Ramasamy, Livy Real,
Laura Rituma, Rudolf Rosa, Shadi Saleh, Manuela
Sanguinetti, Baiba Saulīte, Sebastian Schuster,
Djamé Seddah, Wolfgang Seeker, Mojgan Seraji,
Lena Shakurova, Mo Shen, Dmitry Sichinava, Na-
talia Silveira, Maria Simi, Radu Simionescu, Katalin
Simkó, Mária Šimková, Kiril Simov, Aaron Smith,
Alane Suhr, Umut Sulubacak, Zsolt Szántó, Dima
Taji, Takaaki Tanaka, Reut Tsarfaty, Francis Tyers,
Sumire Uematsu, Larraitz Uria, Gertjan van No-
ord, Viktor Varga, Veronika Vincze, Jonathan North

23



Washington, Zdeněk Žabokrtský, Amir Zeldes,
Daniel Zeman, and Hanzhi Zhu. 2017. Universal
dependencies 2.0. LINDAT/CLARIN digital library
at the Institute of Formal and Applied Linguistics,
Charles University. http://hdl.handle.net/11234/1-
1983.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D. Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal dependencies v1: A multilingual
treebank collection. In Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC 2016).

Rudolf Rosa and Zdeněk Žabokrtský. 2015. Klcpos3
- a language similarity measure for delexicalized
parser transfer. In Proceedings of ACL-IJCNLP.

Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of NAACL. pages 129–
132.

Milan Souček, Timo Järvinen, and Adam LaMontagne.
2013. Managing a multilingual treebank project. In
Proceedings of the Second International Conference
on Dependency Linguistics (DepLing 2013). Charles
University in Prague, Matfyzpress, Prague, Czech
Republic, Prague, Czech Republic, pages 292–297.
http://www.aclweb.org/anthology/W13-3732.

Oscar Täckström, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies. Montreal, Canada,
NAACL HLT ’12, pages 477–487.

Jörg Tiedemann. 2012. Parallel data, tools and in-
terfaces in opus. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Ugur Dogan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC’12). Is-
tanbul, Turkey.

Jörg Tiedemann and Željko Agić. 2016. Synthetic tree-
banking for cross-lingual dependency parsing. Jour-
nal of AI Research 55(1):209–248.

Daniel Zeman, Martin Popel, Milan Straka, Jan Ha-
jic, Joakim Nivre, Filip Ginter, Juhani Luotolahti,
Sampo Pyysalo, Slav Petrov, Martin Potthast, Fran-
cis Tyers, Elena Badmaeva, Memduh Gokirmak,
Anna Nedoluzhko, Silvie Cinkova, Jan Hajic jr.,
Jaroslava Hlavacova, Václava Kettnerová, Zdenka
Uresova, Jenna Kanerva, Stina Ojala, Anna Mis-
silä, Christopher D. Manning, Sebastian Schuster,
Siva Reddy, Dima Taji, Nizar Habash, Herman Le-
ung, Marie-Catherine de Marneffe, Manuela San-
guinetti, Maria Simi, Hiroshi Kanayama, Valeria de-
Paiva, Kira Droganova, Héctor Martı́nez Alonso,

Çağr Çöltekin, Umut Sulubacak, Hans Uszkor-
eit, Vivien Macketanz, Aljoscha Burchardt, Kim
Harris, Katrin Marheinecke, Georg Rehm, Tolga
Kayadelen, Mohammed Attia, Ali Elkahky, Zhuoran
Yu, Emily Pitler, Saran Lertpradit, Michael Mandl,
Jesse Kirchner, Hector Fernandez Alcalde, Jana Str-
nadová, Esha Banerjee, Ruli Manurung, Antonio
Stella, Atsuko Shimada, Sookyoung Kwak, Gustavo
Mendonca, Tatiana Lando, Rattima Nitisaroj, and
Josie Li. 2017. Conll 2017 shared task: Multilingual
parsing from raw text to universal dependencies. In
Proceedings of the CoNLL 2017 Shared Task: Mul-
tilingual Parsing from Raw Text to Universal Depen-
dencies. Association for Computational Linguistics,
Vancouver, Canada, pages 1–19.

Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of the IJCNLP-08 Workshop
on NLP for Less Privileged Languages. Hyderabad,
India.

24


