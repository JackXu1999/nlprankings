



















































Generating Market Comments Referring to External Resources


Proceedings of The 11th International Natural Language Generation Conference, pages 135–139,
Tilburg, The Netherlands, November 5-8, 2018. c©2018 Association for Computational Linguistics

135

Generating Market Comments Referring to External Resources

Tatsuya Aoki| Akira Miyazawa}~| Tatsuya Ishigaki| Keiichi Goshima]|

Kasumi Aoki[| Kobayashi Ichiro[| Hiroya Takamura| Yusuke Miyao\|

Tokyo Institute of Technology }The Graduate University for Advanced Studies
~National Institute of Informatics ]Bank of Japan* [Ochanomizu University

|National Institute of Advanced Industrial Science and Technology \The University of Tokyo

{aoki, ishigaki}@lr.pi.titech.ac.jp miyazawa-a@nii.ac.jp
keiichi.goshima@boj.or.jp {g1120501, koba}@is.ocha.ac.jp

takamura@pi.titech.ac.jp yusuke@is.s.u-tokyo.ac.jp

Abstract

Comments on a stock market often include
the reason or cause of changes in stock
prices, such as “Nikkei turns lower as yen’s
rise hits exporters.” Generating such infor-
mative sentences requires capturing the re-
lationship between different resources, in-
cluding a target stock price. In this paper,
we propose a model for automatically gen-
erating such informative market comments
that refer to external resources. We evalu-
ated our model through an automatic met-
ric in terms of BLEU and human evalua-
tion done by an expert in finance. The re-
sults show that our model outperforms the
existing model both in BLEU scores and
human judgment.

1 Introduction

Nikkei Stock Average opens at a high
price after Dow Jones Industrial Aver-
age closes at a high price.

This is an example of a comment onmarkets that
describes the stock prices shown in Figure 1. The
closing price of the Dow Jones Industrial Average
at 5 am JST is represented as the right-most point
in the figure on the top, while the opening price of
the Nikkei StockAverage is represented as the left-
most point in the figure at the bottom. While the
comment describes the behavior of Nikkei Stock
Average (henceforth, Nikkei 225), the main indi-
cator of the Japanese stock market, it also refers to
an external indicator, the Dow Jones IndustrialAv-
erage, which represents the US stock market. Such

*Views expressed in this paper are those of the authors
and do not necessarily reflect the official views of the Bank of
Japan.

2016-08-03 2016-08-04

18250

18300

18350

18400

23:00 0:00 1:00 2:00 3:00 4:00 5:00
Time [JST]

D
ow

 J
on

es
 In

du
st

ria
l A

ve
ra

ge
 [U

S
D

]

2016-08-04 Morning 2016-08-04 Afternoon

15900

16000

16100

16200

16300

9:00 10:00 11:00 12:00 13:00 14:00 15:00
Time [JST]

N
ik

ke
i 2

25
 [J

P
Y

]

Figure 1: Relationship between Dow Jones Indus-
trial Average and Nikkei 225 (Nikkei Stock Aver-
age).

mentions of external resources as a cause of the be-
havior of a target index are very common in market
comments. Comments on the Japanese stock mar-
ket can also refer to, for example, other stock mar-
ket indices, foreign exchange rates, and oil prices,
and comments that also describe causes will facil-
itate readers in understanding financial situations.

In this paper, we address the task of generat-
ing market comments that refer to external re-
sources. Specifically, we extend the encoder in the
encoder-decoder model proposed by Murakami
et al. (2017) so that the model can take into ac-
count external resources related to the financial do-



136

main. We encode each of the external resources in
addition to the target index, i.e., Nikkei 225, and
feed them to the decoder. The experimental results
show that our proposed model outperforms the ex-
isting single-source model in terms of the fluency
and informativeness of human evaluation, in addi-
tion to the BLEU score.

2 RelatedWork

There has been a lot of work on generating text
from numerical time series or structured data in-
cluding weather data (Belz, 2008), healthcare data
(Portet et al., 2009), sports data (Liang et al., 2009),
and market data (Kukich, 1983). Approaches to
such tasks are traditionally dependent on hand-
crafted rules (Goldberg et al., 1994; Dale et al.,
2003) or are template-based.

Neural encoder-decoders (Sutskever et al.,
2014; Bahdanau et al., 2015; Luong et al., 2015)
have also been successfully applied to various
data-to-text generation tasks. While many gener-
ate text from table data, such as reviews from prod-
uct attributes (Dong et al., 2017) and biographies
from the infoboxes of Wikipedia (Lebret et al.,
2016), there is an attempt to generate text from
numerical data (Murakami et al., 2017), in which
market comments are generated from a time-series
of stock prices. However, the model of Mu-
rakami et al. (2017), which is based on an encoder-
decoder, takes only a target time series and ignores
the fact that there are many mentions of external
resources.

3 Generating Market Comments

We describe our model for generating comments.
We extend the encoder part of the model proposed
by Murakami et al. (2017), which had a limitation
in generating informative market comments due to
the lack of a capability to consider multiple data
sources as input. We first explain the encoder used
in the existingmodel and then show howwe extend
it.

3.1 Base Model (base)

The existing model by Murakami et al. (2017)
takes only a single source of data, a sequence of
prices of Nikkei 225, as input. Specifically, the
prices are recorded every five minutes in the data.
The model first converts the input data into two
vectors: a short-term vector xshort and a long-term
vector xlong. The vector xshort is N -dimensional

Short-Term

Preprocess

MLP

Long-Term

Preprocess

MLP

Short-Term

Preprocess

MLP

Long-Term

Preprocess

MLP

Linear

LSTM

LSTM

LSTM

N
ik
ke
i

ga
in
s

Data 1 Data L

...

...

...

Concat

<
p
r
i
c
e
/
>

T
T

T
<
s
>

Figure 2: Neural-network architecture of our
model. The symbol T denotes embedding of pub-
lishing time (see Murakami et al. (2017) for de-
tails).

and consists of the N previous stock prices, while
xlong is M -dimensional and consists of the clos-
ing prices of the M preceding trading days. Thus,
xshort contains short-term changes in the stock
price, while xlong contains long-term changes.

In the encoding step, the vectors are passed to
multilayer perceptrons (MLPs) with three layers
and concatenated as

vsingle D
�
MLPshort .xshort/ ; MLPlong

�
xlong

� �
;

(1)

where the semicolon represents the concatenation.
The vector vsingle is then transformed to a vector s0
by an affine transformation s0 D Wsvsingle C bs ,
where Ws is a weight matrix and bs is a bias term.

In the decoding step, the hidden state of the de-
coder is initialized by s0, and LSTMcells (Hochre-
iter and Schmidhuber, 1997) are used following the
model by Murakami et al. (2017). Please refer to
the original paper for more details on the decoder.

They also replaced numerical values in the train-
ing data with placeholders representing arithmetic
operations, e.g., rounding down the difference be-
tween the latest price and the closing price of the
previous day.

3.2 Multiple Source-Aware Model (multi)

The architecture of our model is shown in Figure 2.
We extend the encoder part of the base model so
that themodel can takeL different sources as input,



137

including the Dow Jones Industrial Average, and
US dollar/Japanese yen exchange rates in addition
to Nikkei 225. We convert each input source to a
continuous representation vi .1 � i � L/ as

vi D
h
MLP ishort

�
xishort

�
; MLP ilong

�
xilong

� i
:

(2)

Note that the model has 2L MLPs; L MLPs are
for short-term data and the others are for long-term
data. Each xishort is an N -dimensional short-term
vector for the i -th data source generated with the
same approach as Murakami et al. (2017). Each
xilong is anM -dimensional long-term vector for the
i -th data source.

The representations v1; � � � ; vL are then con-
catenated to a representation vmulti as:

vmulti D
�
v1 ; � � � ; vL

�
: (3)

It is then passed to an affine transform function as
is done in the base model s0 D Wmvmulti C bm,
and s0 is used for the initial state of the decoder.

Our model is clearly a straight extension of the
model by Murakami et al. (2017). All the multiple
input resources are treated equally with this archi-
tecture. The target resource to be described will
be determined by the training data. For example,
if the comments in the training data describe the
behavior of Nikkei 225, the other resources are re-
garded as causes influencing Nikkei 225.

4 Experiments

4.1 Data

Training the model requires pairs consisting of a
time series and a market comment aligned with
it. As market comments, we used 20,093 head-
lines of Nikkei Quick News (NQN) that describe
the behavior of Nikkei 225. They are provided by
Nikkei, Inc. and written in Japanese. We divided
them into three parts on the basis of the period of
publication: 16,276 for training (Dec. 2010–Oct.
2015), 1,866 for validation (Oct. 2015–April 2016)
and 1,951 for testing (April 2016–Oct. 2016). In
addition, we retrieved the five-minute charts of 10
indices fromThomson Reuters DataScope Select1.
They consist of seven stock market indices (Nikkei
225, TOPIX Price Index, S&P 500 Index, FTSE
100 Index, Hang Seng Index, Shanghai SE Com-
posite Index, and Dow Jones Industrial Index),

1https://hosted.datascope.reuters.com/
DataScope/

a forward transaction index (Nikkei 225 Future),
and two currency exchange rates, USD/JPY and
EUR/JPY.

4.2 Preprocessing and Parameters

As a preprocessing procedure, we created short-
and long-term sequences of each index from the
five-minute charts in the same way as Murakami
et al. (2017). The sizeN of a short-term vector was
set to 62, and the size M of a long-term one was
set to 7. We used Adam (Kingma and Ba, 2015)
for optimization with a learning rate of 0.0001 and
a mini-batch size of 100. The dimensions of the
three hidden layers in MLPs were all set to 32.

4.3 Evaluation Settings

We compared our model with the model by Mu-
rakami et al. (2017). The latter was not provided
with external resources as input, but could still re-
fer to them groundlessly simply because mentions
of external resources are found in the training data.

We conducted both an automatic evaluation in
terms of BLEU scores and a manual evaluation
done by a financial expert. The outputs from the
proposed model were compared with reference
market comments extracted from NQN and com-
ments generated by the base model. In the au-
tomatic evaluation by BLEU score, we used the
market comments collected from NQN as refer-
ences. We calculated the BLEU scores for both
the base model and our model. In the human eval-
uation, a human judge (an expert in finance) man-
ually judged the outputs in terms of two criteria:
fluency and informativeness. Specifically, we pre-
sented three market comments generated by a hu-
man (human), the base model (base), and our
model (multi). For fluency, the human judgeman-
ually selected a label from two labels (fluent
and not_fluent) for each comment. For in-
formativeness, the judge was asked to evaluate
whether a comment included a correct mention of
an external resource. The human judge was asked
to select one out of four labels: no, correct,
wrong, and subtle. The label no means that
a comment did not contain a mention of an exter-
nal resource. The label correct means that the
comment contained correct mentions of external
resources, whereas the labelwrongmeans that the
comment contained a wrong mention of external
resources. The label subtle corresponded to the
other cases. For example, when a comment con-
tained a mention of an external resource that was

https://hosted.datascope.reuters.com/DataScope/
https://hosted.datascope.reuters.com/DataScope/


138

Method BLEU (%)

base 21.88 ˙ 0.31
multi 23.66 ˙ 0.35

Table 1: Result of evaluation in terms of BLEU.
Scores were macro-averaged over 5 runs. Each
value after ˙ is standard deviation.

fluent not_fluent

human 98 2
base 95 5
multi 96 4

(a) Fluency.

Mentions of Ext. Resources
no yes (cr / wr / sb )

human 54 46 ( 5 / 0 / 41 )
base 51 49 ( 13 / 9 / 27 )
multi 46 54 ( 11 / 2 / 41 )

(b) Informativeness.

Table 2: Result of human evaluation. In (a), values
are number of times that comments were judged
fluent or not_fluent. In (b), no indicates
number of comments that do not contain any men-
tion of external resources. yes indicates num-
ber of comments that contain mention of external
resources. yes is divided into correct (cr),
wrong (wr), and subtle (sb), which respec-
tively mean numbers of comments with correct,
wrong, and subtle mentions.

not any of the L inputs, subtle was assigned.
When evaluating the informativeness, the human
judge does not simply measure the similarity be-
tween the generated comments and the reference
comments; he referred to the input data to check
the correctness of the generated comments.

5 Results

Table 1 shows the BLEU scores for each model.
The scores were calculated by averaging the scores
of five trials. By incorporating multiple resources
as input, our model outperformed the base model
with an improvement of 1.78 points in BLEU. This
suggests that integrating multiple resources into
the encoder helps to improve the ability to generate
comments similar to human generated ones.

Table 2 shows the results of the human evalu-
ation for each model. In terms of fluency, most
of the comments generated by all of the meth-
ods were judged fluent. base and multi were
slightly worse than human in fluency because they
failed to output the correct placeholders represent-
ing arithmetic operations.

In terms of informativeness, our model referred
to external resourcesmore often than base. Specif-
ically, our model outputs 54 comments with men-
tions of external resources, while 46 were with-
out the mentions. The method base outputs only
49 comments with such a mention. In addition,
the proportion of wrong was notably reduced by
our model. The results suggest that our proposed
model improved the ability to generate more in-
formative sentences including correct mentions of
external resources.

We show examples of the generated comments
in Table 3. The method base erroneously men-
tioned external information, “US stock rise,” due to
the lack of input information. Our method, multi,
tended to avoid generating clearly erroneous men-
tions such as “US stock rise.” We also found that
human often referred to important events as in the
output example “easing Brexit concerns.” Gener-
ating such comments requires yet other external re-
sources such as news streams, which we leave for
future work.

6 Conclusion

We proposed an encoder-decoder model for gen-
erating market comments that refer to external re-
sources. Our automatic and manual evaluation
showed that integrating multiple resources into the
encoder improves the ability to include such infor-
mation in the outputs and to generate more infor-
mative comments.

Our code is available at https://github.
com/aistairc/market-reporter.

Acknowledgements

This paper is based on results obtained from a
project commissioned by the New Energy and
Industrial Technology Development Organization
(NEDO). This work is partially supported by JST
PRESTO (Grant Number JPMJPR1655).

https://github.com/aistairc/market-reporter
https://github.com/aistairc/market-reporter


139

Method Output

human
Toushou yoritsuki zokushin, agehaba 300 en koeru, ei EU ridatsu kenen-ga koutai
TSE opening continual_rise, gain 300 yen jump_over, UK EU leaving concern-nom retreat
“Tokyo stocks open 300 yen higher with a continual rise, due to easing Brexit concerns.”

base
Toushou yoritsuki zokushin, agehaba 300 en chou, bei-kabu-daka ya en-yasu-o koukan
TSE opening continual_rise, gain 300 yen over US-stock-high and yen-cheap-acc good_feeling
“Tokyo stocks open 300 yen higher with a continual rise, helped by a cheaper yen and US stocks rise.”

multi
Toushou yoritsuki zokushin, agehaba 300 en chou, en-yasu-de yushutsu-kabu-ni kai
TSE opening continual_rise, gain 300 yen over yen-cheap-ins exporting-stock-dat purchase
“Tokyo stocks open … a continual rise, thanks to demand for export-related shares boosted by a cheaper yen.”

Table 3: Examples of generated comments. Each example is accompanied by original Japanese com-
ment transliterated into English alphabet, its word-for-word translation, and the corresponding English
sentence. TSE stands for Tokyo Stock Exchange. Abbreviations used in word-for-word translation are as
follows. nom: nominative, acc: accusative, ins: instrumental, and dat: dative.

References

Dzmitry Bahdanau, Kyunghyun Cho, andYoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of In-
ternational Conference on Learning Representations
(ICLR).

Anja Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Natural Language Engi-
neering, 14(4):431–455.

Robert Dale, Sabine Geldof, and Jean-Philippe Prost.
2003. Coral: Using natural language generation for
navigational assistance. In Proceedings of the 26th
Australasian computer science conference-Volume
16, pages 35–44. Australian Computer Society, Inc.

Li Dong, Shaohan Huang, Furu Wei, Mirella Lapata,
Ming Zhou, and Ke Xu. 2017. Learning to generate
product reviews from attributes. In Proceedings of
the 15th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
volume 1, pages 623–632.

Eli Goldberg, Norbert Driedger, and Richard I Kit-
tredge. 1994. Using natural-language processing to
produce weather forecasts. IEEE Intelligent Sys-
tems, (2):45–53.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of International Conference on Learning Represen-
tations (ICLR).

Karen Kukich. 1983. Design of a knowledge-based re-
port generator. In Proceedings of Association for
Computational Linguistics (ACL), pages 145–150.

Rémi Lebret, David Grangier, and Michael Auli. 2016.
Neural text generation from structured data with ap-
plication to the biography domain. In Proceedings of

Empirical Methods in Natural Language Processing
(EMNLP), pages 1203–1213.

Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL (ACL2009), pages
91–99. Association for Computational Linguistics.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1412–1421.

Soichiro Murakami, Akihiko Watanabe, Akira
Miyazawa, Keiichi Goshima, Toshihiko Yanase, Hi-
roya Takamura, and Yusuke Miyao. 2017. Learning
to generate market comments from stock prices.
In Proceedings of Association for Computational
Linguistics (ACL), pages 1374–1384.

François Portet, Ehud Reiter, Albert Gatt, Jim Hunter,
Somayajulu Sripada, Yvonne Freer, and Cindy
Sykes. 2009. Automatic generation of textual sum-
maries from neonatal intensive care data. Artificial
Intelligence, 173(7-8):789–816.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in neural information processing sys-
tems (NIPS), pages 3104–3112.

http://aclweb.org/anthology/E17-1059
http://aclweb.org/anthology/E17-1059
https://arxiv.org/abs/1412.6980
https://arxiv.org/abs/1412.6980
https://doi.org/10.18653/v1/D16-1128
https://doi.org/10.18653/v1/D16-1128
http://www.aclweb.org/anthology/P09-1011
http://www.aclweb.org/anthology/P09-1011
https://doi.org/10.18653/v1/D15-1166
https://doi.org/10.18653/v1/D15-1166
https://doi.org/10.18653/v1/P17-1126
https://doi.org/10.18653/v1/P17-1126

