



















































Synthetic Word Parsing Improves Chinese Word Segmentation


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 262–267,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Synthetic Word Parsing Improves Chinese Word Segmentation

Fei Cheng Kevin Duh Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology

8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
{fei-c,kevinduh,matsu}@is.naist.jp

Abstract

We present a novel solution to improve
the performance of Chinese word seg-
mentation (CWS) using a synthetic word
parser. The parser analyses the inter-
nal structure of words, and attempts to
convert out-of-vocabulary words (OOVs)
into in-vocabulary fine-grained sub-words.
We propose a pipeline CWS system that
first predicts this fine-grained segmenta-
tion, then chunks the output to recon-
struct the original word segmentation stan-
dard. We achieve competitive results on
the PKU and MSR datasets, with substan-
tial improvements in OOV recall.

1 Introduction

Since Chinese has no spaces between words to in-
dicate word boundaries, Chinese word segmenta-
tion is a task to determine word boundaries be-
tween characters. In recent years, research in Chi-
nese word segmentation has progressed signifi-
cantly, with state-of-the-art performing at around
96% in precision and recall (Xue, 2003; Zhang
and Clark, 2007; Li and Sun, 2009).

However, frequent OOVs are still a crucial issue
that causes low accuracy in word segmentation.
Li and Zhou (2012) defined those words that are
OOVs but consisting of frequent internal parts as
pseudo-OOV words and estimated that over 60%
of OOVs are pseudo-OOVs in five common Chi-
nese corpora. For instance, PKU corpus does not
contain the word陈列室 (exhibition room), even
though the word陈列 (exhibit) and室 (room) ap-
pear hundreds of times. Goh et al. (2006) also
claimed that most OOVs are proper nouns taking
the form of Chinese synthetic words.

These previous works suggest that by analysing
the internal structure of the synthetic words, we
can transform pseudo-OOVs into in-vocabulary

words (IVs). By running a synthetic word parser
on each of the words in a CWS training set, we can
generate a fine-grained segmentation standard that
contains more IVs. Since the current conditional
random field (CRF) word segmenters (Tseng et al.,
2005; Sun and Xu, 2011) perform well on IVs, this
transforming process can conceivably improve the
handling of pseudo-OOV words, as long as we can
recover the original word segmentation standard
from the fine-grained sub-word segmentation.

In recent years, some related works about im-
proving OOV problem in CWS have been ongo-
ing. Sun et al. (2012) presented a joint model for
Chinese word segmentation and OOVs detection.
Their models achieved fast training speed, high ac-
curacies and increase on OOV recall. Sun (2011)
proposed a similar sub-word structure which is
generated by merging the segmentations provided
by different segmenters (a word-based segmenter,
a character-based segmenter and a local character
classifier). However, her models does not predict
the sub-words of all the synthetic words, but those
words with different segmented results of the three
segmenters. Her work maximizes the agreement
of different models to improve CWS performance.
Different from her work, we aim to provide an uni-
fied way to incorporate morphological information
of the synthetic words into the CWS task.

In this paper, we propose a pipeline word seg-
mentation system to address the pseudo-OOV
problem. Our word segmentation system first con-
verts the original training data into a fine-grained
standard by parsing all words with a synthetic
word parser (Section 2.1), then trains a CRF-
based sub-word segmenter (Section 2.2). A sec-
ond CRF chunker is trained to recover the origi-
nal word segmentation given the fine-grained re-
sults of the first CRF. The intuition is that fine-
grained sub-word segmentations resolve pseudo-
OOVs into IVs, which are easier to predict cor-
rectly by the first CRF. Secondly, by training an-

262



other CRF that predicts the original word segmen-
tation given the fine-grained segmentation as in-
put, we can recover the fine-grained output into
original word segmentation standard (Section 2.3).
The flow chart of our word segmentation system is
shown in Figure 1.

Synthetic Word 
Parser

CRF-Based Word 
Segmenter

CRF-Based 
Chunking Model

Testing Data

NoTraining

Training Data

Training Data  
(fine-grained)

Training

Fine-grained 
Output

Original 
Standard Output

Figure 1: The Flow Chart of the Chinese Word
Segmentation System.

2 System Components

2.1 Synthetic Word Parser

Intuitively, Chinese synthetic words contain inter-
nal morphological information that is helpful to
recognize OOVs. Cheng et al. (2014) proposed
a character-based parser to parse the internal tree
structure of words. For instance, the tree and flat
segmented result of the word 市政府 (munici-
pal government) are shown in Figure 2. In this
work, we train a graph-based parser (McDonald,
2006) on the data released by Cheng et al. (2014)
and include the dictionary (NAIST Chinese Dic-
tionary1) features and Brown clustering features
extracted from a large unlabeled corpus (Chinese
Gigaword Second Edition2) as described in Cheng
et al. (2014).

For native Chinese speakers, single character
and two character words are usually treated as the

1http://cl.naist.jp/index.php?%B8%F8%B3%AB%A5%E
A%A5%BD%A1%BC%A5%B9%2FNCD

2https://catalog.ldc.upenn.edu/LDC2005T14

smallest units. In this work, we parse all the words
in the PKU and MSR training data with character
length greater than two. By replacing the words
with the flat segmented results, we convert the
training data into a fine-grained word segmenta-
tion standard as shown in Figure 3.

Figure 2: The Tree Structure of a Sample Word
and the Flat Segmented Result.

Original 市政府 /办公厅 /等 /单位
CWS tags B I E / B I E / S / B E

Fine-grained 市 /政府 /办公 /厅 /等 /单位
CWS tags S / B E / B E / S / S / B E

Figure 3: A Sample Sentence of Labeling Chinese
word segmentation tags on the Original and Fine-
grained Standard. In this work, we adopt 4-tag set
for word segmentation. ”B” denotes the beginning
character of a word. ”I” denotes the middle char-
acter of a word. ”E” denotes the end character of
a word. ”S” denotes a single character word.

2.2 CRF-based Word Segmenter

Xue et al. (2003) proposed a method which treated
Chinese word segmentation as a character-based
sequential labeling problem and exploited sev-
eral discriminative learning algorithms. Tseng
et al. (2005) adopted the CRFs as the learning
method and obtained the best results in the second
international Chinese word segmentation bakeoff-
2005. Moreover, Sun and Xu (2011) attempted to
extract information from large unlabeled data to
enhance the Chinese word segmentation results.

In this work, we train a traditional CRF-based
supervised model on the fine-grained training data,
include the dictionary (NAIST Chinese Dictio-
nary) features and access variety features extracted
from a large unlabeled corpus (Chinese Giga-
word Second Edition) as described in Sun and
Xu (2011).

263



2.3 CRF-based Chunking Model

In order to obtain the word segmentation result
with original word segmentation standard, we
train a CRF-based chunking model on the original
and fine-grained training data. We show a sam-
ple sentence of labeling chunking tags in Figure 4.
Comparing two sentences, we label all common
units with the tag ”S”. The words市 and政府 are
tagged as ”B” and ”E”, since 市 is the beginning
part of the synthetic word市政府 and政府 is the
ending part. In the chunking process, the frequent
prefix市 is coordinated with neighbouring units to
compose the synthetic word市政府.

For each labeling, we include previous, current
and next word as the features for the chunking
model.

Original 市政府 /办公厅 /等 /单位
Fine-grained 市 /政府 /办公 /厅 /等 /单位

Chunking tags B / E / B / E / S / S

Figure 4: A Sample Sentence of Labeling Chunk-
ing Tags. In this work, we adopt 4-tag set for
chunking. ”B” denotes the beginning part of a syn-
thetic word. ”I” denotes the middle part. ”E” de-
notes the end part. ”S” denotes a single word.

3 Experiments

3.1 Settings

Cheng et al. (2014) released a dictionary of
31,849 synthetic words with internal structure an-
notated. Since transliteration words (e.g. 贝克
汉姆 Becham) exist in Chinese, our synthetic
word parser should perform well not only on syn-
thetic words but also on transliteration words.
We extracted 6,574 transliteration words from the
NAIST Chinese Dictionary and automatically as-
signed flat structures for these words. As a result,
we obtained 38,423 words as the training data for
our parser.

The second international Chinese word seg-
mentation bakeoff-2005 provided two annotated
simplified Chinese corpora: PKU and MSR. We
conducted all word segmentation experiments on
these two corpora.

We used CRF++3 (version 0.58) as the imple-
mentation of CRFs in our experiments with the de-
fault regularization algorithm L2.

3The CRF++ package can be found in the following web-
site: http://taku910.github.io/crfpp/

3.2 Word Segmentation Results

Table 1 summarizes the word segmentation re-
sults on PKU and MSR corpora. For compari-
son, we give a baseline result by training a CRF
word segmenter on the original PKU and MSR
data sets with the same features. Our proposed
system is expected to improve the word segmen-
tation performance on pseudo-OOVs. Compared
to the baseline, there are significant increases on
OOV recall from 0.792 to 0.822 on PKU and 0.682
to 0.717 on MSR. We also evaluated the pseudo-
OOV recall and observed 4% increases from the
baseline to the proposed system. Our proposed
system achieves higher F-score with 0.961 on
PKU and 0.971 on MSR. Comparing to other sys-
tems, our proposed method obtains the state-of-
the-art F-score as the results of Zhang et al. (2013)
who extracted dynamic statistical features from
both in-domain and out-domain corpus and our
OOV recall significantly outperforms theirs with
a 9% lead. In MSR, we obtain very close OOV
recall and slightly lower F-score than the state-of-
the-art system (Sun et al., 2009), which adopted a
latent variable CRF model. However, our system
significantly outperforms their system in PKU. In
both corpora, our proposed system outperforms
the best ”Bakeoff-2005” results.

We also test the statistical significance of the
results by using the criterion (Sproat and Emer-
son, 2003; Emerson, 2005). The 95% confidence
interval is given as ±2√p(1− p)/n, where n is
the number of words in the test data. They treat
two systems as significantly different (at the 95%
confidence level), if at least one of their precision-
based confidences ”Cp” or recall-based ”Cr” are
different. As the results shown in Table 2, the
baseline and proposed method are significantly
different on precision and recall in both PKU and
MSR corpus. In conclusion, our proposed method
significantly outperforms the baseline.

3.3 Additional Experiments

We conducted additional experiments to evaluate
the performance of the synthetic word parser and
CRF-based chunking model.

First, we are interested in how much parsing ac-
curacy is needed for good results. Figure 5 dis-
plays the OOV recall results of our word segmen-
tation system when the synthetic word parser is
trained with amounts of labeled synthetic words
data. As the data size increases, our word segmen-

264



System
PKU MSR

P R F Roov Rpseudo P R F Roov Rpseudo
Baseline 0.957 0.960 0.959 0.792 0.797 0.971 0.968 0.970 0.682 0.689
Proposed method 0.960 0.962 0.961 0.822 0.838 0.972 0.970 0.971 0.717 0.73
Zhang et al. (2013) 0.965 0.958 0.961 0.731 - - - - - -
Sun et al. (2009) 0.956 0.948 0.952 0.778 - 0.973 0.973 0.973 0.722 -
Bakeoff-2005 0.953 0.946 0.950 0.636 - 0.962 0.966 0.964 0.717 -

Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSR
Corpora. Here, ”Rpseudo” denotes the recall of pseudo-OOV words. ”Bakeoff-2005” denotes the best
results of the second international Chinese word segmentation bakeoff-2005 on two corpora. Since
we use extra resources and our proposed method replies on the synthetic word parser trained on an
dictionary with internal structure annotated, the results cannot be directly compared with the state-of-
the-art systems.

System
PKU MSR

Words P Cp R Cr Words P Cp R Cr
Baseline 104372 0.957 ±0.00126 0.960 ±0.00121 106873 0.971 ±0.00103 0.968 ±0.00108
Proposed 104372 0.960 ±0.00121 0.962 ±0.00118 106873 0.972 ±0.00101 0.970 ±0.00104

Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.

tation system obtains consistent gains on OOV re-
call on both corpora. On the whole 38K words
training data, our system reaches the highest OOV
recall. An interesting observation is that the OOV
recall on MSR is more sensitive on data size
changing. The main reason is the different anno-
tation standard of the two corpus. PKU is a cor-
respondingly fine-grained annotated corpus with
shorter average word length than MSR. Our syn-
thetic word parser reaches high parsing accuracy
on short length words (three-character and four-
character words) even with a small training data
size. With the increase of word length, the parser
needs more training data. These factors cause that
our system reaches high OOV recall on PKU start-
ing from a small training data size and obtains
more OOV recall gains on MSR when increasing
the training data size.

Our pipeline system adopts a chunking model
to recover the original standard from the fine-
grained standard. One question is how difficult
is this task. Unfortunately, we do not have the
gold fine-grained input to evaluate the perfor-
mance of our chunking model directly; i.e. it is
not clear whether a segmentation error is due to
mis-predictions in the first or second CRF. There-
fore, we use the synthetic word parser to parse all
the words in the gold testing data and generate an
artificial gold fine-grained input for the chunking
model. This data keeps the original word bound-

5 10 15 20 25 30 35 40
80

80.5

81

81.5

82

82.5

83

Training data size (thousands of words)

O
O

V
re

ca
ll

(p
er

ce
nt

ag
e)

PKU

(a) PKU Corpus

5 10 15 20 25 30 35 40
69.5

70

70.5

71

71.5

72

72.5

Training data size (thousands of words)

O
O

V
re

ca
ll

(p
er

ce
nt

ag
e)

MSR

(b) MSR Corpus

5 10 15 20 25 30 35 40
92

93

94

95

96

97

Training data size (thousands of words)

L
ab

el
ed

A
cc

ur
ac

y
(p

er
ce

nt
ag

e) Synthetic Word Parser

(c) Parsing Performance

Figure 5: The OOV Recall Evaluation and
the Character Labeled Accuracy (5-fold cross-
validation) of the Synthetic Word Parser on Train-
ing Data Size.

aries and can be used to observe the chunking per-
formance. Table 3 shows that the chunking model
on the artificial data obtains a 0.822 to 0.847 im-
provement in OOV recall. We can interpret this
to mean that 0.025 improvement is possible if the
first CRF was perfect; on the other hand, the gap
between 0.847 and 1.0 shows that potentially the
second CRF is a harder task. However, the real

265



gap is less for the lose of the parsing step and the
existence of non-pseudo OOVs.

System
PKU MSR

F Roov F Roov
Proposed 0.961 0.822 0.971 0.717
Artifical gold 0.965 0.847 0.973 0.743

Table 3: The Word Segmentation evaluation of
the Chunking Model. ”Artificial gold” denotes
the word segmentation result when the chunking
model runs on the artificial gold input.

3.4 Analysis
As we expected, the proposed method obtains sig-
nificant improvement on OOV recall. In both cor-
pora, we observed a number of OOVs are seg-
mented correctly. For instance,管理法 (manage-
ment law) is an OOV word in PKU corpus. In this
word,管理 (management) appears frequently and
法 (law) is a common suffix in Chinese synthetic
words, such as行政法 (administrative law) or国
际法 (international law). This type of pseudo-
OOVs share a major contribution to upgrade the
system performance. We also observed that some
polysemous words bring ambiguities to the chunk-
ing step. The character 会 carries the meanings
”will” as an auxiliary verb or ”meeting” in a syn-
thetic word运动会 (sports meeting).

4 Conclusion

In this paper, we presented a series processes to
reduce OOV rate and extract morphological infor-
mation inside Chinese synthetic words on a fine-
grained word segmentation standard. As a result,
we can improve the Chinese word segmentation
performance (especially on pseudo-OOVs) with-
out introducing any new feature types. Our pro-
posed method achieved the state-of-the-art F-score
and OOV recall on two common corpus PKU and
MSR. However, note that we only exploited the
flat segmented results of internal word structure
here. As future work, we plan to exploit the full
tree structure of synthetic words to improve not
only CWS but also additional downstream tasks
such as sentence parsing.

References
Fei Cheng, Kevin Duh, and Yuji Matsumoto. 2014.

Parsing chinese synthetic words with a character-

based dependency model. In Proceedings of the
Ninth International Conference on Language Re-
sources and Evaluation (LREC’14), Reykjavik, Ice-
land, may. European Language Resources Associa-
tion (ELRA).

Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the fourth SIGHAN workshop on Chinese language
Processing, volume 133.

Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-
sumoto. 2006. Machine learning-based methods to
chinese unknown word detection and pos tag guess-
ing. Journal of Chinese Language and Computing,
16(4):185–206.

Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for chinese word segmentation.
Computational Linguistics, 35(4):505–512.

Zhongguo Li and Guodong Zhou. 2012. Unified de-
pendency parsing of chinese morphological and syn-
tactic structures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1445–1454. Association for
Computational Linguistics.

Ryan McDonald. 2006. Discriminative learning and
spanning tree algorithms for dependency parsing.
Ph.D. thesis, PhD Thesis. University of Pennsylva-
nia.

Richard Sproat and Thomas Emerson. 2003. The
first international chinese word segmentation bake-
off. In Proceedings of the second SIGHAN work-
shop on Chinese language processing-Volume 17,
pages 133–143. Association for Computational Lin-
guistics.

Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 970–979. As-
sociation for Computational Linguistics.

Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-
masa Tsuruoka, and Jun’ichi Tsujii. 2009. A dis-
criminative latent variable chinese segmenter with
hybrid word/character information. In Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 56–64. Association for Computational Lin-
guistics.

Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast on-
line training with frequency-adaptive learning rates
for chinese word segmentation and new word de-
tection. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 253–262. Asso-
ciation for Computational Linguistics.

266



Weiwei Sun. 2011. A stacked sub-word model
for joint chinese word segmentation and part-of-
speech tagging. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1385–1394. Association for Computational
Linguistics.

Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the fourth SIGHAN
workshop on Chinese language Processing, volume
171.

Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29–48.

Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In ANNUAL MEETING-ASSOCIATION FOR COM-
PUTATIONAL LINGUISTICS, volume 45, page 840.

Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for Chinese word seg-
mentation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 311–321, Seattle, Washington, USA,
October. Association for Computational Linguistics.

267


