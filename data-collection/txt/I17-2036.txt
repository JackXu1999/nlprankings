Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 210–216,

Taipei, Taiwan, November 27 – December 1, 2017 c(cid:13)2017 AFNLP

210

Hyperspherical Query Likelihood Models with Word Embeddings

Ryo Masumura†

Taichi Asami†

Hirokazu Masataki†

Kugatsu Sadamitsu‡ Kyosuke Nishida† Ryuichiro Higashinaka†

NTT Media Intelligence Laboratories, NTT Corporation,

1-1, Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan

† {masumura.ryo, asami.taichi, masataki.hirokazu

nishida.kyosuke, higashinaka.ryuichiro}@lab.ntt.co.jp

‡ k.sadamitsu.ic@future.co.jp

Abstract

This paper presents an initial study on
hyperspherical query likelihood models
(QLMs) for information retrieval (IR).
Our motivation is to naturally utilize pre-
trained word embeddings for probabilis-
tic IR. To this end, key idea is to di-
rectly leverage the word embeddings as
random variables for directional proba-
bilistic models based on von Mises-Fisher
distributions that are familiar to cosine dis-
tances. The proposed method enables us
to theoretically take semantic similarities
between document and target queries into
consideration without introducing heuris-
tic expansion techniques.
In addition,
this paper reveals relationships between
hyperspherical QLMs and conventional
QLMs. Experiments show document re-
trieval evaluation results in which a hy-
perspherical QLM is compared to conven-
tional QLMs and document distance met-
rics using word or document embeddings.

1 Introduction

In information retrieval (IR), language modeling
is known to be one of the most successful tech-
niques (Ponte and Croft, 1998). A typical usage
is query likelihood models (QLMs), in which lan-
guage models are constructed from each retrieved
document.
In QLM-based probabilistic IR, the
documents are ranked by probabilities for which a
query can be generated by the document language
model.

In this ﬁeld, categorical QLMs which model
generative probability of words using categorical
distributions are fundamental models (Ponte and
Croft, 1998; Zhai and Lafferty, 2001). It is known
that the categorical QLMs do not perform well for

vocabulary mismatches because categorical distri-
bution cannot consider semantic relationships be-
tween words. Therefore, several expansion tech-
niques such as query expansion (Bai et al., 2005),
translation QLMs (Berger and Lafferty, 1999), and
latent variable models (Wei and Croft, 2006) have
been proposed in order to take semantic relation-
ships between document and target query into ac-
count.

Recently, word embeddings, which are contin-
uous vector representations embedding word se-
mantic information, have been utilized for en-
hancing the previous expansion techniques (Zhang
et al., 2016; Mitra and Craswell, 2017). The word
embeddings can be easily acquired in an unsuper-
vised manner from large scale text sets based on
embedding modeling, i.e., skip-gram, continuous
bag-of-words (CBOW) (Mikolov et al., 2013) or
GloVe (Pennington et al., 2014). Zuccon et al.
(2015); Ganguly et al. (2015); Zamani and Croft
(2016a) used the word embeddings in order to as-
sist translation QLMs. Zamani and Croft (2016b);
Kuzi et al. (2016) used the word embeddings in or-
der to perform query expansion. However, previ-
ous word embedding-based probabilistic IR meth-
ods have no theoretical validity since the word em-
beddings were heuristically introduced.

In order to perform more natural word em-
bedding based probabilistic IR, our key idea is
to directly leverage word embeddings rather than
words as random variables for language models.
In fact, the word embeddings can capture semantic
similarity of words using directional information
based on cosine distance (Mnih and Kavukcuglu,
2013). This motivates us to introduce directional
probabilistic models based on von Mises-Fisher
distributions which are familiar to the cosine dis-
tance (Banerjee et al., 2005; Sra, 2016).

This paper proposes a hyperspherical QLMs in
which random variables are modeled by a mix-

211

ture of von Mises-Fisher distributions. The hy-
perspherical QLMs can theoretically utilize word
embeddings for probabilistic IR without introduc-
ing heuristic formulations. Main contributions are
summarized as follows.

• This paper formulates hyperspherical QLMs
based on both a maximum likelihood estima-
tion and a maximum a posteriori estimation.
• This paper reveals that hyperspherical QLMs
can be represented as an extended form of
categorical QLMs and a theoretical form of
translation QLMs.
• This paper shows document retrieval evalua-
tion results in which a hyperspherical QLM is
compared with conventional QLMs and doc-
ument distance metrics with word or docu-
ment embeddings.

2 Related Work

This paper is closely related to document dis-
tance metrics using word or document embed-
dings. One major distance metric is the cosine
distance between two document vectors that are
composed by averaging word embeddings (Vulic
and Moens, 2015; Brokos et al., 2016) or docu-
ment embeddings called paragraph vectors (PVs)
(Le and Mikolov, 2014). Another highly efﬁcient
distance metric is word mover’s distance (WMD),
which leverages word embeddings (Kusner et al.,
2015)．In this work, we also examined these dis-
tance metrics in a document retrieval evaluation.

Generative models of word embeddings have
recently been proposed in topic modeling in or-
der to capture the semantic structure of words and
documents (Das et al., 2015; Batmanghelich et al.,
2016). To the best of our knowledge, this paper is
the ﬁrst work on language modeling that handles
word embeddings as random variables.

3 IR based on QLMs

IR based on probabilistic modeling uses the prob-
ability of a document D given a query Q. One
of the most famous approaches is QLM-based IR
in which documents are ranked by the probabili-
ties that a query can be generated by the document
language model (Ponte and Croft, 1998). Given
a query Q = {w1,··· , wT}, IR based on QLMs
ranks documents as

P (D|Q) rank∝ T∏

t=1

P (wt|ΘD),

(1)

where ΘD denotes a parameter of QLM for D.
3.1 Categorical QLMs
Categorical QLMs model P (w|ΘD) using a cat-
egorical distribution.
In a maximum likelihood
(ML) estimation for the categorical QLM, a gen-
erative probability of a word w is deﬁned as

P (w|ΘML

D ) = c(w, D)
|D|

,

(2)

where c(w, D) is the word count of w in D, and
|D| is the number of all words in D.

In a maximum a posteriori (MAP) estimation, a
document collection C in which all of the retrieved
documents are included is used for a prior (Zhai
and Lafferty, 2001). MAP estimated generative
probability of a word w is deﬁned as

P (w|ΘMAP

D ) =

c(w, D) + τ c(w,C)

|C|

|D| + τ

,

(3)

where c(w, D) is the word count of w in C, and
|C| is the number of all words in C. τ is a hyper
parameter for adjusting smoothing.

∑

3.2 Translation QLMs
Translation QLMs were introduced for expand-
ing categorical QLMs (Berger and Lafferty, 1999).
The translation QLMs are usually used together
with the categorical QLMs, and enable us to take
into account relationships between a word in the
query and semantically related words in the doc-
ument. A generative probability of a word w is
deﬁned as

v∈V

D ) =

P (w|ΘTR

P (v|ΘD)P (w|v),

(4)
where V is the vocabulary. P (v|ΘD) is the gener-
ative probability of a word v, which is also cal-
(2) or (3). P (w|v) represents
culated by Eq.
the probability of translating word v into word w.
P (w|v) is heuristically calculated as
∑
sim(w, v)
w∈V sim(w, v) ,

P (w|v) =

(5)

where sim(w, v) is the word similarity between
In order to calculate P (w|v), cosine
w and v.
distances between pre-trained word embeddings
were recently utilized (Zuccon et al., 2015; Gan-
guly et al., 2015). Thus, the word similarity is cal-
culated as

sim(w, v) = w⊤v,

(6)
where w is the word embedding normalized to a
unit length for w.

212

4 IR based on Hyperspherical QLMs
This paper proposes a novel probabilistic IR
method based on hyperspherical QLM that lever-
ages pre-trained word embeddings as random vari-
ables for directional probabilistic models. The
pre-trained word embeddings can capture seman-
tic similarity of words on a unit hypersphere, so
we deal with normalized word embeddings to unit
length. Given a query Q = {w1,··· , wT}, nor-
malized word embeddings {w1,··· , wT} can be
acquired using embedding models.
IR based on
hyperspherical QLMs ranks documents as

P (D|Q) rank∝ T∏

p(wt|ΛD),

(7)

M∑

t=1

where p means a probability density and ΛD de-
notes a parameter of a hyperspherical QLM for D.

4.1 Formulation
The hyperspherical QLMs are formulated by a
mixture of von Mises-Fisher distributions which
are familiar to cosine distances (Banerjee et al.,
2005; Sra, 2016). The von Mises-Fisher distribu-
tion deﬁnes a probability density over points on
a unit hypersphere. A probability density based
on the mixture of von Mises-Fisher distributions
is formulated as

p(w|Λ) =

αmf(w|µm),

(8)

m=1

f(w|µm) = Cd(κ) exp(κw⊤µm),

(9)
where M is the number of mixtures. The parame-
ter Λ corresponds to {αm, µm}. αm means a mix-
ture weight, and µm means a directional mean of
the m-th von Mises-Fisher distribution. κ is a con-
centration parameter that is treated as a smoothing
parameter. Cd(κ) is a normalized parameter that
depends on κ and the number of dimensions of
word embeddings d. Note that w⊤µm is the co-
sine distance between w and µm.
4.2 ML and MAP Estimation for Mixture of

von Mises-Fisher Distributions

ML estimation for a mixture of von Mises-Fisher
distributions given normalized word embeddings
D = {w1,··· , w|D|} determines model parame-
ters ΛML

D as

ΛML

D = argmax

Λ

P (D|Λ).

(10)

The ML estimation is based on the expectation
maximization algorithm. ML estimated parame-
D = {ˆαm},{ ˆµm} are recursively calcu-
ters ΛML
lated as

|D|∑

ˆαm =

ˆrm =

1
|D|

|D|∑

t=1

q(m|wt, Λ),

t=1

wtq(m|wt, Λ),

(11)

(12)

ˆrm||ˆrm|| ,

ˆµm =

(13)
where q(m|wt, Λ) is a load factor of the m-th dis-
tribution for the t-th word embedding.

MAP estimation for a mixture of von Mises-
Fisher distributions given normalized word em-
beddings D = {w1,··· , w|D|} determines model
parameters ΛML

D as

ΛMAP

D = argmax

Λ

P (D|Λ)P (Λ).

(14)

Given pre-trained parameters ¯Λ = {¯αm, ¯rm},
D = {ˆαm, ˆµm}
MAP-estimated parameters ΛMAP
|D|∑
are calculated as
|D|∑

|D|
|D| + τ

|D| + τ

1
|D|

ˆαm =

¯αm+

t=1

τ

ˆrm =

τ

|D| + τ

¯rm +

q(m|wt, ¯Λ),
(15)
wtq(m|wt, ¯Λ),
(16)
(17)

|D|
|D| + τ
ˆrm||ˆrm|| ,

t=1

ˆµm =

where τ is a hyper parameter for adjusting smooth-
ing.

For computation of load factors, both the soft-
assignment rule and the hard-assignment rule can
be used. Both computations are deﬁned as

∑M
qs(m|wt, Λ) = αmf(wt|µm)
1 m = argmax
l=1 αlf(wt|µl)

l
otherwise,

0

qh(m|wt, Λ) =

,

(18)

αlf(wt|µl),

(19)
where qs is the load factor using the soft-
assignment rule and qh is the load factor using the
hard-assignment rule.

213

4.3 Training of Hyperspherical QLMs
In order to introduce a mixture of von Mises-
Fisher distributions to hyperspherical QLMs, uni-
ﬁed assumptions are essential for each document
modeling because the hyperspherical QLMs are
utilized for IR. Therefore, we introduce the fol-
lowing assumptions.

• The number of mixtures corresponds to vo-

cabulary size.

M = |V|.

(20)
• The mean direction of each von Mises-Fisher
distribution is ﬁxed to normalized word em-
beddings of each word in the vocabulary.

ˆµm = vm,

(21)

where vm ∈ V.

• For computation of load factors in document

modeling, a hard-assignment rule is used.

To summarize the above,

the hyperspherical
QLMs can be theoretically estimated as simple
forms using ML or MAP estimation. In fact, mix-
ture weights are estimated as ML or MAP esti-
mated values in categorical QLMs.
In ML esti-
mation, ˆαm estimated from document D is deter-
mined as

.

(22)

|D|

ˆαm = c(vm, D)
∑

Thus, ML estimated generative probability of w in
the hyperspherical QLMs is formulated as

p(w|ΛML

D ) =

c(v, D)
|D| f(w|v),

v∈V

(23)

where v is a normalized word embedding of v.

In MAP estimation, ˆαm estimated from docu-

ment D is determined as

ˆαm =

c(vm, D) + τ c(vm,C)

|C|

|D| + τ

,

(24)

where C is document collection in which all re-
trieved documents are included. Thus, MAP esti-
mated generative probability of w in the hypersh-
erical QLMs is formulated as

p(w|ΛMAP

D ) =

c(v, D) + τ c(v,C)
|C|

|D| + τ

f(w|v).

(25)

∑

v∈V

4.4 Relationships
The hyperspherical QLMs can be interpreted as an
extended form of categorical QLMs. Eq. (23) in-
cludes the ML estimated term presented in Eq. (2),
and Eq.
(25) includes the MAP estimated term
presented in Eq. (3). In fact, hyperspherical QLMs
can be converted into categorical QLMs by

κ→∞ p(w|ΘD) = P (w|ΛD).
lim

(26)

In addition, hyperspherical QLMs can be re-
garded as a theoretical form of translation QLMs.
Eqs.
In
fact, hyperspherical QLMs are almost the same as
translation QLMs by deﬁning a word similarity as

(23) and (25) are similar to Eq.

(4).

sim(w, v) = exp(κw⊤v).

(27)

While Eq. (6) is heuristically formulated, Eq. (27)
is theoretically formulated as a log-linear form
based on the directional probabilistic modeling.

5 Experiments
5.1 Setups
We performed an experiment on a document re-
trieval task,
in which we used 20 news group
datasets1 for evaluation. The datasets were for-
mally split into 11,314 training and 7,531 test ar-
ticles. The training articles were used for collect-
ing documents and the test articles were used for
queries. Label information about news groups was
only utilized for deciding whether a retrieved doc-
ument is relevant to the query in evaluation. These
setups are equivalent to the evaluation in Salakhut-
dinov and Hinton (2009); Larochelle and Lauly
(2012). We removed common stop words, and the
5,000 most frequent words in the training articles
were used for the vocabulary.

In order to utilize word embeddings, data sets in
a one billion word language modeling benchmark2
were prepared. We constructed CBOW and PV
with distributed BOW (PV-DBOW). These pre-
trained embedding models were utilized for IR-
methods. The dimension of the word and docu-
ment embeddings was set to 200.

For evaluation, the following IR methods were
used. TFIDF used cosine distance between two
document vectors composed by word TF-IDF val-
ues. CWV used cosine distance between two

1

2

http://qwone.com/˜jason/20Newsgroups/

20news-bydate.tar.gz

http://github.com/ciprian-chelba/

1-billion-word-language-modeling-benchmark

214

Table 1: mAP and P@10 results.

IR methods Embeddings
TFIDF
CWV
WCWV
PV
WMD
CQLM
TQLM
HQLM

√
√
√
√
√
√

mAP P@10
0.478
0.123
0.321
0.109
0.165
0.438
0.284
0.105
0.287
0.103
0.586
0.182
0.126
0.343
0.594
0.198

of text sets can efﬁciently capture semantic in-
formation in continuous space, the document dis-
tance metrics using the embeddings were insuf-
ﬁcient for document retrieval.
In addition, we
attained superior performance only introducing
HQLM compared to CQLM while single use of
TQLM did not perform well.
In fact, previous
work attained performance improvements by com-
bining TQLM with CQLM. These results conﬁrm
that HQLM can effectively utilize word embed-
dings for document retrieval. Furthermore, we
analyzed that relevant documents ranked low in
CQLM were moved up by HQLM. This indicates
HQLM can robustly calculate generative proba-
bilities of words that were not included in a tar-
get query. We also veriﬁed that HQLM showed
similar results to CQLM when κ was set to a
large value (κ = 500). This conﬁrms that Eq.
(12), which insists HQLM can be represented as
CQLM, is a proper theory.

6 Conclusions

In this paper, we proposed a word embedding-
based probabilistic IR method based on hyper-
spherical QLMs that are modeled by a mixture
of von Mises-Fisher distributions. We found
the hyperspherical QLMs could theoretically uti-
lize word embeddings for IR without introducing
heuristic formulations. We found that the hyper-
spherical QLMs can be represented as an extended
form of categorical QLMs and a theoretical form
of translation QLMs. Our experiments on a doc-
ument retrieval task showed hyperspherical QLM
outperformed previous QLMs and document dis-
tance metrics with word or document embeddings.
In the future, we will examine large scale docu-
ment retrieval evaluation.

Figure 1: Precision-recall curves results.

document vectors composed by averaging word
embeddings (Vulic and Moens, 2015). WCWV
used cosine distance between two document vec-
tors composed by adding word embeddings with
IDF weights (Brokos et al., 2016). PV used co-
sine distance of two document vectors composed
by PV-DBOW (Le and Mikolov, 2014). WMD
used WMD using word embeddings (Kusner et al.,
2015). CQLM used discrete QLMs estimated
by Eq.
(4) where τ was set to 2,000 (Zhai and
Lafferty, 2001). TQLM used translation QLMs
where word similarity was calculated by Eq. (6)
using word embeddings, and Eq. (3) was used for
calculating generative probability (Ganguly et al.,
2015). HQLM used hyperspherical QLMs esti-
mated by Eq.
(11) using word embeddings, in
which τ and κ were respectively set to 2,000 and
20.

5.2 Results
We assessed the performance of each IR-method
with precision-recall curves, mean average preci-
sion (mAP), and precision at 10 (P@10). Figure 1
shows the precision-recall curve results and Table
1 shows the mAP and P@10 results. The results
are averaged over all possible queries.

The results show that CQLM and HQLM
clearly outperformed document distance metric-
based IR methods with word or document em-
beddings. This conﬁrms that QLM-based IR is a
helpful approach for document retrieval. Although
word or document embeddings trained from a lot

0.7

0.6

0.5

0.4

0.3

0.2

0.1

n
o
i
s
i
c
e
r
P

TFIDF

CWV

WCWV

PV

WMD

CQLM

TQLM

HQLM

0

0.01

0.1

Recall

1

215

References
Jing Bai, Dawei Song, Peter Bruza, Jian-Yun Nie, and
Guihong Cao. 2005. Query expansion using term
relationships in language models for information re-
trieval.
In Proc. Conference on Information and
Knowledge Management (CIKM) pages 688–695.

Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh,
and Suvrit Sra. 2005. Clustering on the unit hyper-
sphere using von mises-ﬁsher distribution. Journal
of Machine Learning Research 6:1345–1382.

Nematollah Kayhan Batmanghelich, Ardavan Saeedi,
Karthik R. Narasimhan, and Samuel J. Gershman.
2016. Nonparametric spherical topic modeling with
word embeddings. In Proc. Annual Meeting of the
Association for Computational Linguistics (ACL)
pages 537–542.

Adam Berger and John Lafferty. 1999. Information re-
trieval as statistical translation. In Proc. Annual In-
ternational ACM Conference on Research and De-
velopment in Information Retrieval (SIGIR) pages
222–229.

Georgios-Ioannis Brokos, Prodromos Malakasiotis,
and Ion Androutsopoulos. 2016. Using centroids
of word embeddings and word mover’s distance for
biomedical document retrieval in question answer-
ing. In Proc. Workshop on Biomedical Natural Lan-
guage Processing (BioNLP) pages 114–118.

Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.
Gaussian LDA for topic models with word embed-
dings. In Proc. Annual Meeting of the Association
for Computational Linguistics (ACL) pages 795–
804.

Debasis Ganguly, Dwaipayan Roy, Mandar Mitra,
and Gareth J.F. Jones. 2015. A word embedding
based generalized language model for information
retrieval. In Proc. Annual International ACM Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR) pages 795–798.

Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kil-
ian Q. Weinberger. 2015. From word embeddings to
document distances. In Proc. International Confer-
ence on Machine Learning (ICML) .

Saar Kuzi, Anna Shtok, and Oren Kurland. 2016.
Query expansion using word embeddings. In Proc.
Conference on Information and Knowledge Man-
agement (CIKM) pages 1929–1932.

Hugo Larochelle and Stanislas Lauly. 2012. A neural
autoregressive topic model.
In Proc. Advances in
Neural Infomation Processing Systems (NIPS) pages
2708–2716.

Quoc Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents.
In
Proc. International Conference on Machine Learn-
ing (ICML) pages 1188–1196.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Ggeg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tation of words and phrases and their composition-
ality. In Proc. Advances in Neural Information Pro-
cessing Systems (NIPS) pages 3111–3119.

Bhaskar Mitra and Nick Craswell. 2017. Neural text
embeddings for information retrieval. In Proc. ACM
International Conference on Web Search and Data
Mining (WSDM) pages 813–814.

Andriy Mnih and Koray Kavukcuglu. 2013. Learning
word embeddings efﬁciently with noise-contrastive
estimation.
In Proc. Advances in Neural Informa-
tion Processing Systems (NIPS) pages 2265–2273.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In Proc. Empirical Methods in Nat-
ural Language Processing (EMNLP) pages 1532–
1543.

Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Proc.
Annual International ACM Conference on Research
and Development in Information Retrieval (SIGIR)
pages 275–281.

Ruslan Salakhutdinov and Geoffrey Hinton. 2009.
Replicated softmax: an undirected topic model. In
Proc. Advances in Neural Information Processing
Systems (NIPS) pages 1607–1614.

Suvrit Sra. 2016.
chine learning:
arXiv:1605.00316 .

Directional statistics in ma-
arXiv prerint

a brief review.

Ivan Vulic and Marie-Francine Moens. 2015. Mono-
lingual and cross-lingual information retrieval mod-
els based on (bilingual) word embeddings. In Proc.
Annual International ACM Conference on Research
and Development in Information Retrieval (SIGIR)
pages 363–372.

Xing Wei and W. Bruce Croft. 2006. LDA-based doc-
ument models for ad-hoc retrieval. In Proc. Annual
International ACM Conference on Research and De-
velopment in Information Retrieval (SIGIR) pages
178–185.

Hamed Zamani

and W. Bruce Croft. 2016a.
Embedding-based query language models.
In
Proc. ACM International Conference on the Theory
of Information Retrieval (ICTIR) pages 147–156.

Hamed Zamani and W. Bruce Croft. 2016b. Estimating
embedding vectors for queries. In Proc. ACM Inter-
national Conference on the Theory of Information
Retrieval (ICTIR) pages 123–132.

Chengxiang Zhai and John Lafferty. 2001. A study of
smoothing methods for language models applied to
ad hoc information retrieval. In Proc. Annual Inter-
national ACM Conference on Research and Devel-
opment in Information Retrieval (SIGIR) pages 334–
342.

216

Ye Zhang, Md Mustaﬁzur Rahman, Alex Braylan,
Brandon Dang, Heng-Lu Chang, Henna Kim, Quin-
ten McNamara, Aaron Angert, Edward Banner,
Vivek Khetan, Tyler McDonnell, An Thanh Nguyen,
Dan Xu, Byron C. Wallace, and Matthew Lease.
2016. Neural information retrieval: A literature re-
view. arXiv prerint arXiv:1611.06792 .

Guido Zuccon, Bevan Koopman, Peter Bruza, and Leif
Integrating and evaluating neu-
Azzopardi. 2015.
ral word embeddings in information retrieval.
In
Proc. Australasian Document Computing Sympo-
sium (ADCS) 12:1–8.

