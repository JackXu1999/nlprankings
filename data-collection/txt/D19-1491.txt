



















































Recursive Context-Aware Lexical Simplification


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4853–4863,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4853

Recursive Context-Aware Lexical Simplification

Sian Gooding
Dept of Computer Science and Technology

University of Cambridge
shg36@cam.ac.uk

Ekaterina Kochmar
ALTA Institute

Unversity of Cambridge
ek358@cam.ac.uk

Abstract
This paper presents a novel architecture for
recursive context-aware lexical simplification,
REC-LS, that is capable of (1) making use of
the wider context when detecting the words
in need of simplification and suggesting alter-
natives, and (2) taking previous simplification
steps into account. We show that our system
outputs lexical simplifications that are gram-
matically correct and semantically appropri-
ate, and outperforms the current state-of-the-
art systems in lexical simplification.

1 Introduction

Text simplification (TS) is aimed at reducing the
reading and grammatical complexity of text while
retaining the meaning and grammaticality (Chan-
drasekar and Bangalore, 1997). This is usually
achieved by a series of transformations at the lex-
ical and syntactic level. A number of systems in
the recent years have approached this task in an
integral manner (Zhu et al., 2010; Kauchak, 2013;
Zhang and Lapata, 2017). Such comprehensive
systems can perform a number of simplification
operations at once, but the results are sometimes
ungrammatical and meaning can be changed, ar-
guably making the original text less clear and more
complex (Siddharthan, 2014).

In this paper, we assume the lexical and syn-
tactic components of a TS system to be indepen-
dent and complementary to each other, and focus
on the lexical simplification (LS) component for a
number of reasons. First, it has been shown that
lexical simplification techniques positively impact
the readability of text and improve reader under-
standing and information retention (Leroy et al.,
2012). Secondly, it has been argued that a large
number of people with reading difficulties, in-
cluding those with disabilities, low-literacy, non-
native backgrounds or non-expert knowledge ben-
efit from LS (Xu et al., 2015). For instance, James

(1998) shows that vocabulary of the non-native
language plays central role in second language ac-
quisition. As we aim to make information more
accessible to such readers, the quality of the sim-
plified text is of a paramount importance.

We stress the importance of three key points of
quality assessment in LS: it is vital for the simpli-
fied text to be of a lower complexity, while being
semantically equivalent to the original, and gram-
matically correct. Context plays a central role in
fulfilling these requirements. For instance, con-
sider the different uses of situation in the complex
word dataset collected by Yimam et al. (2017):

(1) the gravity of the economic situation

(2) the situation has remained unchanged

This example demonstrates two types of contex-
tual effects: first of all, in (1) both economic and
situation are marked as complex in mutual con-
text, but when situation occurs in a different con-
text it is not annotated as complex. Secondly, this
example illustrates the impact of the context on the
choice of the appropriate substitution: substituting
climate for situation will work for (1), but will re-
sult in a semantically different expression in (2).

In addition, we argue that as word complexity
depends on context, the order and choice of ap-
plied simplifications matters. For instance, con-
sider the following simplifications for the sentence
‘This is a problem in the contemporary world’:

(3) This is a problem in the modern earth

(4) This is a problem in the modern world

Example (3) shows an output of a system that tries
to simplify all words, which results in a nonsen-
sical sentence, while (4) exemplifies the result of
recursive word replacement in context.

Context effects in LS have not been thoroughly
investigated before. In this paper, we introduce a



4854

novel approach to LS that addresses these issues.
In particular, we make the following contributions:

1. As each simplification step changes the com-
plexity of the output sentence, our LS algo-
rithm applies simplification recursively tak-
ing word complexity in context into account;

2. To ensure grammaticality and meaning
equivalence to the input in the output, our
algorithm takes context both at the complex
word identification and substitution selection
steps into account. We use a novel sequence
labelling component for the former step, and
assess semantic equivalence at the latter us-
ing deep contextualized word representations
provided by ELMo (Peters et al., 2018).

3. To facilitate reproducibility, we release
the code and the output of our system
at github.com/siangooding/
lexical_simplification.

2 Previous work

2.1 Approaches
Early approaches to TS have mostly relied on rule-
based systems (Carroll et al., 1998; Canning et al.,
2000; Siddharthan, 2006), with many of the ear-
lier systems prioritising syntactic operations, such
as sentence splitting, deletion or reordering. Some
work combined lexical simplification with syn-
tactic operations (Zhu et al., 2010; Coster and
Kauchak, 2011a; Kauchak, 2013). The availabil-
ity of parallel corpora of “normal” and simpli-
fied text has inspired a number of approaches that
treated TS as a monolingual machine translation
problem (Zhu et al., 2010; Coster and Kauchak,
2011a,b) or allowed the researchers to apply lan-
guage modelling (Kauchak, 2013).

Building on this line of work, Zhang and Lap-
ata (2017) combine a novel sequence-to-sequence
encoder-decoder model with a deep reinforcement
learning framework that rewards the system for
providing simple, fluent output, similar in mean-
ing to the input. However, one of the main chal-
lenges for such comprehensive end-to-end sys-
tems is the ability to address specific types of
errors independently. For instance, the DRESS-
LS model (Zhang and Lapata, 2017) sometimes
changes the meaning of the input to the opposite as
in “Inspections, she said, rarely cost more than $
1,400”→ “Inspections, she said, often cost more
than $ 1,400”, or produces nonsensical output as

in “Archaeologists digging on the grounds”→ Ar-
chaeologists digging on the zebras”.

A number of approaches focused on generation
and assessment of lexical simplification (Yatskar
et al., 2010; Biran et al., 2011; Horn et al., 2014;
Glavaš and Štajner, 2015). Paetzold and Spe-
cia (2016a) note the lack of consistent evaluation
for text simplification, and in particular lexical
simplification, and introduce an evaluation dataset
BENCHLS, on which they perform benchmarking
of several LS systems. They argue that lexical sim-
plification consists of a number of steps, includ-
ing substitution generation, substitution selection
and ranking. They show that the unsupervised sys-
tem of Paetzold and Specia (2016b) outperforms a
range of other systems in all steps, with the only
exception of the feature-based system by Horn
et al. (2014), which performs better in terms of
precision in a round-trip system evaluation.

Finally, Shardlow (2013) introduced complex
word identification (CWI) as the first step in an
LS pipeline to detect words within text that require
simplification. He showed that systems’ perfor-
mance on this stage is crucial for the overall per-
formance, as low recall of this component might
result in an overly difficult text with many missed
complex words, while low precision might result
in meaning distortions. The recent shared task on
CWI shows that most systems rely on classifica-
tion approaches using features that pertain to in-
dividual words, not taking wider context into ac-
count (Yimam et al., 2018).

2.2 Datasets

Until recently, parallel Wikipedia and Simple
Wikipedia datasets have been the most widely
used data for training and evaluating TS sys-
tems (Zhu et al., 2010; Yatskar et al., 2010;
Coster and Kauchak, 2011a,b; Biran et al., 2011;
Kauchak, 2013; Horn et al., 2014). Wikipedia
allows free access to large quantities of data,
and Simple Wikipedia represents a simplified ver-
sion of original articles that uses simpler vocabu-
lary and syntactic structures (Coster and Kauchak,
2011a). Researchers in the past applied mono-
lingual alignment techniques to construct parallel
versions of the two Wikipedias and learn the trans-
formations from these parallel versions using such
tools as GIZA++ (Och and Ney, 2000).

Despite the popularity of the Wikipedia-based

github.com/siangooding/lexical_simplification
github.com/siangooding/lexical_simplification


4855

datasets for TS research, Xu et al. (2015) argue
that focusing on Wikipedia limits simplification
research and propose using a dataset based on
news articles. They use a dataset from Newsela
as an example, where the texts are simplified by
professional editors at 4 levels of simplificity in
accordance with the grade levels defined by the
Common Core Standards (Porter et al., 2011).

In their benchmarking study, Paetzold and
Specia (2016a) introduce an evaluation dataset
BENCHLS that combines two previously released
datasets for TS – LexMTurk (Horn et al., 2014)
and LSeval (De Belder and Moens, 2012). This
dataset contains 929 instances with an original
sentence, a target complex word, and several can-
didate substitutions ranked by English speakers
from the U.S. according to their simplicity. Paet-
zold and Specia (2016a) additionally filter out mis-
spelled candidates and inflect all candidates to the
grammatical form of the target word. The dataset
contains an average of 7.37 candidate substitutions
per complex word.

Finally, the CEFR-LS dataset (Uchida et al.,
2018) contains simplifications, which not only
represent a semantically good fit and are gram-
matically correct, but are also at different lev-
els of simplicity annotated with respect to non-
native speakers. Simpler candidates at lower lev-
els of language proficiency (A1-B1) according
to the Common European Framework of Refer-
ence for Languages (CEFR) (Council of Europe,
2011) are provided for the original words that are
at higher levels of language proficiency (B2-C2).
The dataset contains 406 target words and 4912
possible substitutions. Unlike BENCHLS, substi-
tute candidates within this dataset may not be cor-
rect for the given context, and if a substitute can-
didate is not appropriate in context it is labelled
as such. The dataset contains an average of 2.35
candidate substitutions per complex word.

3 Data

In this study we implement an LS system that
includes complex word identification, substitute
generation, filtering and ranking as steps in a sim-
plification pipeline. To train our CWI system, we
use the CWI 2018 shared task dataset (Yimam
et al., 2017), which contains texts on three gen-
res – professionally written NEWS, amateurishly
written WIKINEWS, and WIKIPEDIA articles. The
words in the dataset are annotated as complex or

not by 10 native and 10 non-native speakers of En-
glish.

We evaluate each step of the LS pipeline on two
datasets – BENCHLS and CEFR-LS. We select
the BENCHLS dataset because it contains multi-
ple simplification alternatives ranked with respect
to their simplicity by a number of human annota-
tors. The CEFR-LS dataset is a useful resource
for evaluation because, in addition to contextually
suitable, grammatically correct, simpler alterna-
tives, it also contains substitution candidates that
do not fit the context. Furthermore, this dataset is
aimed at non-native speakers of English which we
view as the future target group for our LS system.
For further details on datasets collection and anno-
tation, we refer the readers to the original papers.

Finally, we evaluate our LS system in an end-
to-end manner and compare its performance to
that of the current state-of-the-art systems, includ-
ing those reported in Paetzold and Specia (2016a)
and the DRESS-LS system of Zhang and Lapata
(2017). In contrast to Zhang and Lapata (2017),
we perform lexical simplification only. For fair
comparison of the two systems, we extract only
lexical simplifications from the parallel “normal”
to simplified versions of the data used in Zhang
and Lapata (2017), as well as from the original
“normal” text and the DRESS-LS system output.

To extract the lexical transformations from the
data, we use GIZA++ (Och and Ney, 2000) simi-
larly to previous research (Coster and Kauchak,
2011b; Xu et al., 2015). Following Horn et al.
(2014), we extract the examples that constitute
one-to-one word correspondences between the
two sides identified by the automatically induced
word alignment, where the part-of-speech tag of
the two words is the same while the lemmas are
different. In addition, we filter out the instances
involving modification of stopwords on the orig-
inal, “normal” side as well as rewrites involving
proper nouns. All preprocessing steps for our
algorithm are performed using the RASP parser
(Briscoe et al., 2006).

4 Lexical Simplification Pipeline

Lexical simplification can be viewed as a multi-
stage process involving complex word identifica-
tion, substitute generation, filtering and substitute
ranking. In this section we outline each step of the
simplification process as presented in Figure 1.



4856

Figure 1: Recursive Simplification (REC-LS) pipeline: x is a complex word and yi are the generated substitutes

4.1 Complex Word Identification (CWI)

The first stage of the algorithm is complex word
identification (CWI), which aims to identify which
words should be simplified within the text and,
thus, allows for a personalisable and targeted ap-
proach to LS. Furthermore, it helps to reduce
the number of unnecessary and potentially “harm-
ful” simplifications performed by a ‘simplify all
words’ approach. To train and test our CWI com-
ponent we use the dataset by Yimam et al. (2017).

Word complexity depends on the surrounding
context: for instance, between 3% and 10% of
lexical items (depending on genre) in the Yimam
et al. (2017) dataset receive different annotations
in different contexts. However, most CWI systems
to date have approached this task on an individual
word basis (Yimam et al., 2018). For instance, the
2018 CWI shared task winning system CAMB uses
a classification-based approach with 27 word-level
features (Gooding and Kochmar, 2018). As con-
text impacts the perceived complexity of text, we
argue that CWI should be framed as a sequence la-
belling task and propose a novel architecture SEQ
based on this idea (Gooding and Kochmar, 2019).
We extend the implementation of a sequence la-
beller by Rei (2017),1 which achieves state-of-the-
art results on a number of NLP tasks. The de-
sign of this architecture is highly suitable for CWI
as: (1) it uses bi-directional long short-term mem-
ory units (BiLSTM) (Hochreiter and Schmidhu-
ber, 1997), which allow the system to learn about
both the left and right context of a target word;
(2) the context is combined with both word and
character-level representations (Rei et al., 2016)
which helps capture complexity due to rare char-
acter sequences as well as morphological struc-
ture; (3) this architecture uses a language mod-
elling objective, which enables the model to take
one of the highly informative complexity factors
of word frequency into account.

1https://github.com/marekrei/
sequence-labeler

Test Set Macro F-Score
CAMB SEQ

NEWS 0.8633 0.8763
WIKINEWS 0.8317 0.8540
WIKIPEDIA 0.7780 0.8140

Table 1: SEQ vs. CAMB system results on CWI

DATASET Precision Recall F-Score
CEFR-LS 0.9327 0.7936 0.8575
BENCHLS 0.5000 0.4025 0.4460

Table 2: SEQ results for CWI on the two datasets

SEQ uses 300-dimensional GloVe embeddings
as word representations (Pennington et al., 2014).
The model is trained to predict the binary com-
plexity of words as annotated in the dataset of Yi-
mam et al. (2017). Training is performed over 20
iterations on randomly shuffled sentences from all
genres included within the dataset. To test this
novel architecture on CWI, we apply it to the CWI
2018 shared task test data (Yimam et al., 2018)
and compare the results to the current state-of-the-
art (SOTA) CAMB system. Table 1 shows that the
SEQ model outperforms the current SOTA system
on all three text genres for binary CWI (statisti-
cally significant using McNemar’s test, p=0.0016,
χ2=9.95).

The proposed SEQ model (Gooding and
Kochmar, 2019) has a number of additional advan-
tages: it takes context into account, helps avoid
the necessity of extensive feature engineering re-
lying on word embeddings as the only input in-
formation at run time, and generalises well across
all three datasets. To further assess generalisabil-
ity of the model, we test it on CEFR-LS, as well
as BENCHLS for consistency (see Table 2). How-
ever, as the words in BENCHLS dataset were se-
lected at random rather than according to their
complexity (Paetzold and Specia, 2016a), the re-
sults as expected are lower.

https://github.com/marekrei/sequence-labeler
https://github.com/marekrei/sequence-labeler


4857

4.1.1 Lexical Complexity Threshold

The SEQ system labels each word with a lexical
complexity score. This score represents the like-
lihood (p) of each word belonging to the complex
class. Whether a word is considered as complex
is set according to a predefined threshold for p,
which allows the ‘aggressiveness’ of the algorithm
to be tailored according to the application. For in-
stance, consider the following example:

(5) I believe that ignoring public opinion
discredits0.89 the authorities0.79 and
destabilises0.84 the situation.

The SEQ labeller assigns complexity probabil-
ity p>0.80 to the words discredits and desta-
bilises, whereas for authorities p=0.79. If the
complexity threshold in the simplification pipeline
is set to 0.80, the word authorities would not be a
candidate for lexical simplification. Our simplifi-
cation pipeline will attempt to simplify words one
at a time, starting with the highest p values above
this predefined threshold.

4.2 Substitute Generation

Substitute generation refers to the process of gen-
erating candidates that can be used as simpler al-
ternatives to the target word. The goal of the sys-
tem at this stage is to generate a diverse set of po-
tential substitutions that can be filtered and ranked
according to different criteria at later steps. We
note that the use of multiple resources for substi-
tute generation at this step is crucial as each in-
dividual resource has only a limited coverage. To
this end, we first use traditional approaches, which
do not take the context of the target word or the
simplicity of substitutes into account. In summary,
we rely on the following resources:

• we extract synonyms from WordNet (Fell-
baum, 2012), following Devlin (1998);

• we use Big Huge Thesaurus2 to find can-
didates by querying for the word form and
lemma;

• following Glavaš and Štajner (2015), we
use word embeddings to create substitutions.
Synonyms are identified by finding 10 words
with the highest cosine similarity to the tar-
get word in the vector space. These are
obtained for both GloVe (Pennington et al.,
2014) and Word2Vec embeddings (Mikolov

2https://words.bighugelabs.com

O Water engulfed0.89 Beringia.
S1 Water overwhelmed0.78 Beringia.
S2 Water flooded0.39 Beringia.

Table 3: Contextual simplicity scores for 2 possible al-
ternatives (S1-S2) for the original O

et al., 2013), using the Gensim library.3

The candidate lemmas retrieved using these re-
sources are added to the candidate substitutions
set. To match the grammatical form of the original
word, we convert all candidates to the appropriate
word forms using NodeBox English Linguistics li-
brary.4

4.3 Substitution Filtering and Ranking

Following substitute generation, the next step in
the LS pipeline is to choose one of the generated
candidates to replace the original target word. This
is done by filtering and ranking the candidates us-
ing a set of criteria, and choosing the top candi-
date. Previous work has framed this task as rank-
ing words according to simplicity (Paetzold and
Specia, 2016a). However, a system that is only
aimed at selecting the simplest candidate may re-
turn a substitution that does not fit the surround-
ing context, is ungrammatical or not semantically
equivalent to the original. Therefore, we consider
all three aspects of high-quality substitution selec-
tion: contextual simplicity, semantic equivalence
and grammaticality, which are outlined below.

Contextual Simplicity: The complexity of each
candidate is calculated using our sequential CWI
model from Section 4.1. We calculate the com-
plexity for each substitution within context and re-
fer to this as the simplicity score S. Table 3 shows
2 possible substitutions for engulfed, with their re-
spective simplicity scores.

Contextual Semantic Equivalence: To assess
whether a substitution is semantically equivalent
to the original we use ELMo embeddings (Peters
et al., 2018), which to the best of our knowledge
have not been used for LS before. ELMo provides
deep contextualized word representations, which
are learned from the internal states of a deep bi-
directional language model. As a result, these em-
beddings are able to model complex syntactic and

3https://github.com/RaRe-Technologies/
gensim

4https://www.nodebox.net/code/index.
php/Linguistics

https://words.bighugelabs.com
https://github.com/RaRe-Technologies/gensim
https://github.com/RaRe-Technologies/gensim
https://www.nodebox.net/code/index.php/Linguistics
https://www.nodebox.net/code/index.php/Linguistics


4858

O This illustrates how sociologists [...]
S1 This demonstrates0.10 how sociologists [...]
S2 This shows0.16 how sociologists [...]
S3 This draws0.44 how sociologists [...]

Table 4: Contextual semantic equivalence scores for 3
candidate substitutions (S1-S3) for the original O

O Oak is strong and also gives shade
g.c. Oak gives shade

Table 5: Original sentence O and the grammatical con-
text (g.c.) used to calculate CG for the word gives

semantic characteristics of word use, as well as
how these word uses vary across linguistic con-
texts. In addition, contextualised embeddings help
filter out antonyms that might be included in the
set of potential substitutes by the previous step.

The similarity of words in a given sentential
context is calculated using the ELMo embeddings
associated with the target word and each of the
substitutes. The contextual similarity score (CS)
is calculated by taking the cosine distance between
the ELMo word vectors associated with the origi-
nal word vo and the substitute vs. This allows us to
reason about the likelihood that the substitution is
semantically equivalent to the original in the given
context: the smaller the distance the more likely
this word fits the context. Table 4 provides some
examples for this score.

This technique works particularly well when the
target word appears in the immediate context of
the words grammatically related to it, for example
when a verb is surrounded by its subject and ob-
ject. To counteract the effect of long range depen-
dencies in sentences, we constrain the context to
the grammatical dependents and calculate a sec-
ond contextual score CG using cosine distance.
Table 5 provides an example of a full sentence O
and the grammatical context for the word gives.

Grammaticality: To assess whether a substitution
results in a grammatically correct sentence, we
calculate bigram frequencies of the candidate sub-
stitute and one word to the left and to the right of it
using the 520 million word COCA corpus (Davies,
2014). If either of these frequencies equals 0, it is
assumed that the substitute is not a valid grammat-
ical fit or is extremely rare, making it a poor can-
didate for simplification. Table 6 shows one such
example.

left right
He was capable of [...]
bigram was capable capable of
freq 778 12341
He was able of [...]
bigram was able able of
freq 10281 0

Table 6: Bigram frequencies for left and right contexts
of a candidate substitute

Here, we use the bigram frequencies as a proxy
for grammaticality: although “able” and “capable”
are semantically similar, it is the grammatical con-
straints, captured by the bigram frequencies, that
rule one of the alternatives out.

4.3.1 Threshold-based filtering
Threshold-based filtering is performed by remov-
ing all substitutes that are unlikely to be grammat-
ical or do not fit the target context. First, sub-
stitutes are removed from consideration if their
right or left bigram frequency equals 0. Then,
we remove substitutes if either of their contextual
ELMo scores is below a given threshold t (CS
∨ CG < t), as this implies the substitute is not
equivalent in this context. We test our filtering
approach on the CEFR-LS dataset as it contains
annotations for contextually suitable (value 1) as
well as unsuitable (value 0) substitutions. We em-
pirically find the optimal threshold value on the
CEFR-LS dataset to be 0.175. With this opti-
mal value, we can identify contextually suitable
and grammatically correct substitutes on CEFR-
LS with the precision of 0.7968, recall of 0.8081
and F1=0.8014. As we show in Section 5, this fil-
tering technique generalises well to other datasets.

4.3.2 Ranking Algorithm
Once filtering has been performed we then rank
substitutes. In order to rank substitutes, the sim-
plicity and contextual semantic equivalence scores
are combined to produce an overall suitability
score. We evaluate our ranking techniques on the
BENCHLS and CEFR-LS datasets.

We perform ranking using the sum of the
contextual simplicity score (S) and the aver-
age contextual semantic equivalence score C̄ =
avg(CS , CG), and evaluate the results using the
TRank-at-n measure introduced in Paetzold and
Specia (2016a), which estimates the proportion
of times a candidate with a gold-standard rank



4859

f
u
ll
(9

2
9
)

BENCHLS n=1 n=2 n=3 MRR
S 0.4974 0.7381 0.8899 0.6648

C̄ 0.3509 0.5885 0.7877 0.5998

S+C̄ 0.5602 0.8064 0.9428 0.7219

te
st

(4
6
4
) S 0.5839 0.7546 0.9302 0.7083

C̄ 0.4086 0.7142 0.8950 0.6563
S+C̄ 0.6774 0.7857 0.9308 0.8218
P&S 0.4841 0.5596 0.7004 0.6615

Table 7: Ranking results on BENCHLS dataset

CEFR-LS n=1 n=2 n=3 MRR
S 0.2166 0.3357 0.5271 0.4182
C̄ 0.3754 0.6137 0.7689 0.5857
S+C̄ 0.4624 0.5906 0.7314 0.6582
P&S 0.2129 0.4339 0.5760 0.4736

Table 8: Ranking results on CEFR-LS dataset

r≤n is ranked first by the system. For instance,
our best performing ranking technique on the full
BENCHLS dataset for n=1 is based on the com-
bination of S+C̄ scores and achieves TRank-at-
n=0.5602. This means that for approximately
56% of the test instances the top ranked substi-
tute in the gold standard is correctly ranked first.
In addition, we report the mean reciprocal rank
(MRR) (Voorhees, 1999), which takes into account
the rank of the substitutes proposed by each rank-
ing technique.

We report the results on the full BENCHLS
dataset in the upper half of Table 7. In the lower
half, we compare our results on the test set of
464 instances to those running the Paetzold and
Specia (2016a) system (P&S) on the same test
splits. Since the P&S system was trained on half
of BENCHLS we cannot run it on the full dataset.
Table 7 shows that ranking with S+C̄ works best
according to all measures and across both sets.

Table 8 reports the ranking results on the
CEFR-LS data. We observe a decrease in perfor-
mance, however this is expected: as all substitutes
within the BENCHLS dataset are valid, ranking
by simplicity is more informative; in contrast, the
CEFR-LS dataset contains irrelevant substitutes,
so contextual fit has more pronounced effects.

4.4 Recursive Step

Following CWI, substitute generation, filtering
and substitute ranking steps, the system is able
to perform a simplification. As outlined in Sec-
tion 4.1.1, our system attempts to simplify one
word at a time, starting with the word considered
most complex. Since word complexity depends
on the context, each individual lexical simplifica-

tion made to a sentence has a subsequent impact
on the perceived complexity of the surrounding
words. Such sequential effects cannot be modelled
by systems that apply several simplification steps
at once. For instance, when considering examples
(6) and (7) we see that the word situation is given a
high likelihood of being complex in the context of
another complex word hazardous. However, once
dangerous is substituted for hazardous, the subse-
quent complexity of situation is reduced as well.
(6) It was a hazardous0.90 situation0.87
(7) It was a dangerous0.30 situation0.35

Our lexical simplification algorithm is applied
to a sentence recursively: it starts by identifying
and simplifying the most complex words in the
sentence. Once the simplification is applied in step
n, the algorithm reassesses word complexity in
step n+1, which, in light of the simplifications ap-
plied in previous steps, might have changed. This
prevents unnecessarily simplifying situation in ex-
ample (7), which is no longer necessary after sim-
plifying hazardous→ dangerous. The simplifica-
tion algorithm stops when there are no words with
the complexity score above the predefined thresh-
old left within the sentence, set to 0.5.

Table 9 exemplifies the benefit of using the re-
cursive simplification approach REC-LS. We see
that both the ‘Simplify CW’, aimed at individu-
ally identified complex words, and ‘Simplify all’
approaches result in unnecessary simplifications,
whilst REC-LS stops when it recognises that the
surrounding words are no longer complex.

Original prepare
0.32

for a hazardous
0.81

journey
0.52

REC-LS prepare for a dangerous
0.35

journey
0.45

Simplify CW prepare for a dangerous trip
Simplify all arrange for a dangerous trip

Table 9: Recursive simplification compared with sim-
plifying only complex words and simplifying all words

We note that the only consequence of perform-
ing simplification recursively is that fewer words,
or potentially different words, are simplified, and
highlight that it does not lead to any error propa-
gation. Table 10 presents the entire REC-LS algo-
rithm including the recursive simplification step.

5 End-to-end System Performance

Finally, we test the full LS pipeline, visualised
in Figure 1, on the BENCHLS and CEFR-LS



4860

Result: Lexically simplified sentence
1 S ← Input Sentence
2 c← Complexity threshold
3 REC-LS(S, c, ignore list)
4 while complexity(S) > c do
5 complex words← CWI(S) - ignore list
6 SORT(complex words)
7 for word in complex words do
8 subs← SUB GENERATION(word);
9 subs← MORPH(substitutes);

10 subs← SUB RANK(substitutes);
11 top substitute← head(substitutes);
12 if top substitute == word or [] then
13 ignore list.add(word)

REC-LS(S, c, ignore list)
14 else
15 REPLACE(S,word, top substitute)

REC-LS(S, c, ignore list)
16 end
17 end

datasets. The CWI step of our recursive LS algo-
rithm identifies 77% of the target complex words
in the BENCHLS and 86% of the target complex
words in the CEFR-LS dataset. Next, the sub-
stitution generation and ranking produce lists of
simplification candidates. We evaluate these steps
using precision and measuring the percentage of
times that our system ranks one of the gold stan-
dard candidates as its top choice for substitution.

The best precision P=0.7945 on BENCHLS is
achieved with threshold-based filtering of the un-
suitable candidates and ranking of the candidates
according to the combination of contextual sim-
plicity (S) and contextual fit (C̄) scores. We note
that this result outperforms the previous SOTA
system by Horn et al. (2014), which is reported
to have P=0.5460 (Paetzold and Specia, 2016a),
by a large margin.

With a similar approach, we achieve P=0.4628
on the CEFR-LS, and to the best of our knowl-
edge, this is the first time an LS system is bench-
marked on this dataset. We note that the preci-
sion on this dataset is lower, which can be at-
tributed to the smaller set of gold standard sub-
stitutes per word (an average of 2.35 in CEFR-
LS vs 7.37 in BENCHLS). We also note that in a
number of cases our system generates valid substi-
tutes, which are not included in the gold standard.
For example, wealthy→ rich in “could participate

Table 10: REC-LS Algorithm Steps

(4)
Check if complexity of any word in S
is above the threshold c

(5)
Check sentence for complex words
minus those which are in ignore list

(6)
Sort complex words according to
confidence score

(8)
Generate substitutes for the top complex
word

(9)
Convert substitutions to correct
word form (e.g., tense for verbs)

(10)
Rank substitutes according to
grammaticality, contextual semantic
equivalence and simplicity

(11) Take the top substitute

(13)
If no appropriate substitutes found,
or best fit is original word, then retain
original word and add word to ignore list

(15)
Otherwise replace word and call function
with new sentence

in government just as wealthy men could”. We
present more examples of such cases in the Ap-
pendix.

We also run our entire recursive simplifica-
tion system REC-LS on the three simplification
datasets: WikiSmall, WikiLarge and Newsela,
used in Zhang and Lapata (2017). We then com-
pare the results to the SOTA simplification sys-
tems DRESS-LS by Zhang and Lapata (2017),
which contains a specialised LS model, and the
P&S simplification system (Paetzold and Specia,
2016a). As DRESS-LS is trained using the above
datasets, we test using the 649 sentences that are
reserved for testing only. Simplifications are as-
sessed using the gold standard for lexical substitu-
tions. For each dataset, the number of lexical sim-
plifications performed by the systems is recorded.
We then compare the simplifications performed by
the system with the gold standard simplifications
and calculate the recall, precision and correct pro-
portion as follows:

• For recall we estimate the number of simplifi-
cations present in the gold standard |G|, and
the total number produced by the system S,
which are in the gold standard |S∩G|. Re-
call is then calculated as |S∩G|/|G|.

• For precision, we identify the proportion of
simplifications out of the total |S|, which
are also in the gold standard: |S∩G|/|S|.



4861

Newsela WikiSmall WikiLarge
DRESS-LS P&S REC-LS DRESS-LS P&S REC-LS DRESS-LS P&S REC-LS

Precision 0.1673 0.0836 0.3000 0.0000 0.0602 0.1275 0.2258 0.0697 0.2778
Recall 0.2389 0.9721 0.2393 0.0000 0.9743 0.4815 0.0004 0.9763 0.1856
Correct 0.3256 0.0402 0.5238 0.0000 0.0526 0.4615 0.4286 0.0787 0.4001

Table 11: Lexical substitution results for DRESS-LS, P&S and our system (REC-LS) on three genres

• Finally, correct stands for the proportion of
instances where the top lexical substitution
returned by the system is exactly the same as
the gold standard one.

The results show that the recall for REC-LS is
higher than that of DRESS-LS across all datasets,
while the P&S system performs the best in terms
of recall as it applies a ‘simplify all’ approach.
The difference is especially pronounced in the
WikiSmall dataset, where the DRESS-LS system
does not perform any required lexical substitu-
tions. REC-LS outperforms both DRESS-LS and
P&S systems across all datasets in terms of pre-
cision, which indicates that the CWI stage of the
algorithm is able to narrow down simplifications
to relevant words. Finally, the three systems show
different results in terms of correct proportion:
REC-LS outperforms other systems on Newsela
and WikiSmall, and DRESS-LS has a higher cor-
rect proportion on WikiLarge.

Finally, we note that there are many instances
where the REC-LS system performs substitu-
tions with valid alternatives that are not contained
within the gold standard. For instance, consider
the word “separated” in the following context:

(8) The island chain forms part of the He-
brides, separated from the Scottish main-
land.

The gold standard substitution suggested by
the human annotators in this context is “demar-
cated”, whereas REC-LS substitutes this word
with “split”. We argue that, in this context, “split”
would be both grammatically correct and seman-
tically appropriate, while also being simpler than
the original word and, arguably, the substitute sug-
gested by the human annotators. However, as it
is not an exact match with the gold standard sub-
stitution in the dataset, cases like this negatively
impact precision of our system. We provide more
examples of the system’s output and compare the
simplifications from the three systems in the Ap-
pendix.

6 Conclusions

In this paper, we have presented a novel recur-
sive context-aware lexical simplification architec-
ture REC-LS.5 We summarise the main contribu-
tions of this work as follows:

1. our model takes the surrounding context into
account at the complex word identification
step, using a novel sequence labelling ap-
proach;

2. it filters and ranks the substitution alterna-
tives taking contextual fit and grammaticality
of the output into account;

3. the novel architecture performs simplification
recursively and can be adapted to different
levels of language complexity in the future;

4. as a result, REC-LS outperforms the current
state-of-the-art systems in lexical simplifica-
tion on various datasets.

We note that REC-LS can be used as a stand-alone
lexical simplification system as well as an LS com-
ponent as part of an integral TS system.

In this paper, we focus on intrinsic evaluation
of our system, and we show that it outperforms
SOTA in terms of the widely accepted set of mea-
sures, which we believe to be important for bench-
marking purposes. In the future, we plan to ex-
tend this work with a syntactic simplification com-
ponent and test the architecture extrinsically with
non-native readers.

Acknowledgments

We thank Cambridge English for supporting this
research via the ALTA Institute. We are grateful to
the anonymous reviewers for their valuable feed-
back. We also gratefully acknowledge the support
of NVIDIA Corporation with the donation of the
Titan V GPU used for this research.

5The code and output is available on github.com/
siangooding/lexical_simplification

github.com/siangooding/lexical_simplification
github.com/siangooding/lexical_simplification


4862

References
Or Biran, Samuel Brody, and Noemie Elhadad. 2011.

Putting it Simply: a Context-Aware Approach to
Lexical Simplification. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 496–501.

Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the COLING/ACL on Interactive presen-
tation sessions, pages 77–80. Association for Com-
putational Linguistics.

Yvonne Canning, John Tait, Jackie Archibald, and Ros
Crawley. 2000. Cohesive generation of syntactically
simplified newspaper text. In Proceedings of the
Third International Workshop on Text, Speech and
Dialogue (TDS ’00), pages 145–150.

John Carroll, Gido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of English newspaper text to assist aphasic readers.
In Proceedings of AAAI Workshop on Integrating AI
and Assistive Technology, pages 7–10.

Raman Chandrasekar and Srinivas Bangalore. 1997.
Automatic induction of rules for text simplification.
Knowledge Based Systems, 10:183–190.

William Coster and David Kauchak. 2011a. Learn-
ing to Simplify Sentences Using Wikipedia. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation, pages 1–9.

William Coster and David Kauchak. 2011b. Simple
English Wikipedia: A New Text Simplification Task.
In Proceedings of Annual Meeting of the Association
for Computational Linguistics (ACL), pages 665–
669.

Mark Davies. 2014. N-grams data from the Corpus of
Contemporary American English (COCA).

Jan De Belder and Marie-Francine Moens. 2012. A
dataset for the evaluation of lexical simplification.
In International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, pages 426–
437. Springer.

Siobhan Devlin. 1998. The use of a psycholinguis-
tic database in the simplification of text for aphasic
readers. Linguistic databases.

Council of Europe Council of Europe. 2011. Common
European Framework of Reference for Languages:
Learning, Teaching, Assessment.

Christiane Fellbaum. 2012. WordNet. The Encyclope-
dia of Applied Linguistics.

Goran Glavaš and Sanja Štajner. 2015. Simplifying
lexical simplification: do we need simplified cor-
pora? In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics

and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers),
volume 2, pages 63–68.

Sian Gooding and Ekaterina Kochmar. 2018. CAMB at
CWI Shared Task 2018: Complex Word Identifica-
tion with Ensemble-Based Voting. In Proceedings
of the Thirteenth Workshop on Innovative Use of
NLP for Building Educational Applications, pages
184–194.

Sian Gooding and Ekaterina Kochmar. 2019. Complex
Word Identification as a Sequence Labelling Task.
In Proceedings of the 57th Conference of the Asso-
ciation for Computational Linguistics, pages 1148–
1153.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Colby Horn, Cathryn Manduca, and David Kauchak.
2014. Learning a Lexical Simplifier Using
Wikipedia. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Short Papers), pages 458–463.

Carl James. 1998. Errors in Language Learning and
Use: Exploring Error Analysis. London: Longman.

David Kauchak. 2013. Improving Text Simplification
Language Modeling Using Unsimplified Text Data.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1537–1546.

Gondy Leroy, James E. Endicott, Obay Mouradi, David
Kauchak, and Melissa L. Just. 2012. Improving
perceived and actual text difficulty for health infor-
mation consumers using semi-automated methods.
In Proceedings of the American Medical Informat-
ics Association (AMIA) Fall Symposium, pages 522–
531.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics (ACL ’00), pages 440–447.

Gustavo Paetzold and Lucia Specia. 2016a. Bench-
marking lexical simplification systems. In Pro-
ceedings of the Tenth International Conference on
Language Resources and Evaluation (LREC 2016),
pages 3074–3080, Paris, France. European Lan-
guage Resources Association (ELRA).

Gustavo Paetzold and Lucia Specia. 2016b. Unsuper-
vised lexical simplification for non-native speakers.
In Proceedings of The 30th AAAI, pages 3761–3767.

http://aclweb.org/anthology/P11-2087
http://aclweb.org/anthology/P11-2087
http://users.sussex.ac.uk/~johnca/papers/aaai98.pdf
http://users.sussex.ac.uk/~johnca/papers/aaai98.pdf
http://www.aclweb.org/anthology/W11-1601
http://www.aclweb.org/anthology/W11-1601
http://www.aclweb.org/anthology/P11-2117
http://www.aclweb.org/anthology/P11-2117
http://www.aclweb.org/anthology/P14-2075
http://www.aclweb.org/anthology/P14-2075
http://aclweb.org/anthology/P13-1151
http://aclweb.org/anthology/P13-1151
http://www.aclweb.org/anthology/P00-1056
http://www.aclweb.org/anthology/P00-1056


4863

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of NAACL-HLT 2018,
pages 2227–2237.

Andrew Porter, Jennifer McMaken, Jun Hwang, and
Rui Yang. 2011. Common Core Standards: The
New U.S. Intended Curriculum. Educational Re-
searcher, 40:103–116.

Marek Rei. 2017. Semi-supervised multitask learn-
ing for sequence labeling. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
2121–2130.

Marek Rei, Gamal K.O. Crichton, and Sampo Pyysalo.
2016. Attending to characters in neural sequence
labeling models. In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 309–318.

Matthew Shardlow. 2013. A Comparison of Tech-
niques to Automatically Identify Complex Words.
In Proceedings of the Student Research Workshop at
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 103–109.

Advaith Siddharthan. 2006. Syntactic Simplification
and Text Cohesion. Research on Language and
Computation, 4:77–109.

Advaith Siddharthan. 2014. A survey of research on
text simplification. Special issue of International
Journal of Applied Linguistics, 165:259–298.

Satoru Uchida, Shohei Takada, and Yuki Arase. 2018.
CEFR-based Lexical Simplification Dataset. In Pro-
ceedings of the Eleventh International Conference
on Language Resources and Evaluation (LREC-
2018), pages 3254–3258.

Ellen M. Voorhees. 1999. TREC-8 Question Answer-
ing Track Report. In Proceedings of the 8th Text
Retrieval Conference, pages 77–82.

Wei Xu, Chris Callison-Burch, and Courtney Napoles.
2015. Problems in Current Text Simplification Re-
search: New Data Can Help. Transactions of the
Association for Computational Linguistics, 3:283–
297.

Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from Wikipedia. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 365–368.

Seid Muhie Yimam, Chris Biemann, Shervin Mal-
masi, Gustavo Paetzold, Lucia Specia, Sanja Štajner,
Anaı̈s Tack, and Marcos Zampieri. 2018. A Report
on the Complex Word Identification Shared Task
2018. In Proceedings of the 13th Workshop on In-
novative Use of NLP for Building Educational Ap-
plications, pages 66–78.

Seid Muhie Yimam, Sanja Štajner, Martin Riedl, and
Chris Biemann. 2017. CWIG3G2 - Complex Word
Identification Task across Three Text Genres and
Two User Groups. In Proceedings of the Eighth In-
ternational Joint Conference on Natural Language
Processing (Volume 2: Short Papers), pages 401–
407.

Xingxing Zhang and Mirella Lapata. 2017. Sentence
Simplification with Deep Reinforcement Learning.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
584–594.

Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A Monolingual Tree-based Translation Model
for Sentence Simplification. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1353–1361.

http://www.aclweb.org/anthology/N18-1202
http://www.aclweb.org/anthology/N18-1202
http://aclweb.org/anthology/L18-1514
http://aclweb.org/anthology/N10-1056
http://aclweb.org/anthology/N10-1056
http://aclweb.org/anthology/N10-1056
http://www.aclweb.org/anthology/I17-2068
http://www.aclweb.org/anthology/I17-2068
http://www.aclweb.org/anthology/I17-2068
https://www.aclweb.org/anthology/D17-1062
https://www.aclweb.org/anthology/D17-1062
http://aclweb.org/anthology/C10-1152
http://aclweb.org/anthology/C10-1152

