



















































Assessing Objective Recommendation Quality through Political Forecasting


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2348–2357
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Assessing Objective Recommendation Quality
through Political Forecasting

H. Andrew Schwartz† Masoud Rouhizadeh†,‡ Michael Bishop‡
Philip Tetlock‡ Barbara Mellers‡ Lyle H. Ungar�

†Computer Science, Stony Brook University
‡Psychology, University of Pennsylvania

�Computer & Information Science, University of Pennsylvania
has@cs.stonybrook.edu, ungar@cis.upenn.edu

Abstract

Recommendations are often rated for their
subjective quality, but few researchers
have studied quality in terms of objec-
tive utility. We explore quality assess-
ment with respect to both subjective (i.e.
users’ ratings) and objective (i.e., did it in-
fluence? did it improve decisions?) met-
rics in a massive online geopolitical fore-
casting system, ultimately comparing lin-
guistic characteristics of each quality met-
ric. Using a variety of features, we predict
all types of quality with better accuracy
than the simple yet strong baseline of rec-
ommendation length. For example, more
complex sentence constructions, as evi-
denced by subordinate conjunctions, are
characteristic of recommendations leading
to objective improvements in forecasting.
Our analyses also reveal rater biases; for
example, forecasters are subjectively bi-
ased in favor of recommendations men-
tioning business deals and material things,
even though such recommendations do not
indeed prove any more useful objectively.

1 Introduction

Finding good recommendations is an integral part
of a modern information-seeking life – from pur-
chasing products based on reviews to finding an-
swers to baffling questions. Following the tradi-
tion of sentiment analysis, many have proposed
methods to automatically assess the quality of rec-
ommendations or comments based on subjective
ratings of their usefulness (Liu et al., 2007; Siers-
dorfer et al., 2010; Becker et al., 2012; Momeni
et al., 2013) or of persuasiveness (Wei et al., 2016).
However, information thought to be useful does
not always prove so, and subjective ratings may be

driven by biases. Reviews which convince you to
watch a movie or buy a product do not guarantee
that you will enjoy the product, and the most con-
vincing or highest rated answers to questions on
sites like Stack Overflow or Yahoo Answers are
not always the most accurate.

We explore recommendations in a unique
dataset, an online forecasting competition, which
offers a rare glimpse into both subjective and ob-
jective quality. In this competition, the users
(forecasters) had a measurable goal — to fore-
cast the outcomes of geopolitical events — and a
need to effectively gather information in order to
reach this goal. They viewed recommendations (or
“tips”) from other forecasters, rated them, and po-
tentially updated their own forecast. This data not
only allows us to access what information the fore-
casters thought useful based on their ratings, but
also what was objectively useful based on (a) the
rate at which forecasters change their prediction
after viewing tips and, (b) the average improve-
ment (or decrease) in their prediction accuracy af-
ter this change.

We seek to tease out subjective biases by dis-
tinguishing the linguistic characteristics of rec-
ommendations with high subjective ratings from
those of objective utility. Since objectively good
recommendations tend to get higher subjective
assessments, detecting such differences is non-
trivial. Past literature has suggested that sub-
jective quality is well predicted by comment
length (Agichtein et al., 2008; Beygelzimer et al.,
2015); We seek differences beyond this. We build
quality predictive models from linguistic features
of recommendations — 1 to 3 word sequences,
parts-of-speech, and mentions of concepts from a
taxonomy — comparing to surface-level features
(length and readability). We then explore the lan-
guage which distinguishes high quality comments
from low quality, controlling for comment length,

2348



and ultimately what distinguishes high subjective
quality from objective quality in order to reveal
subjective biases.

Contributions. We see the key contribution of
this paper, perhaps non-conventional for NLP, as
presenting an evidence-based suggestion for the
field to consider metrics of objective quality be-
yond that of subjective ratings. To the best of
our knowledge this represents the first study of
objective comment quality using randomized ex-
perimental data. Specific novel contributions in-
clude (a) the development of automated assess-
ments fit to objective outcomes, (b) the identifi-
cation of linguistic features distinguishing high-
from low-quality comments, (c) the use of a new,
important, real-world domain for NLP – geopo-
litical information, and (d), most consequentially,
the identification of subjective biases manifested
in the comments’ text itself.

2 Data Set

Data were collected from the massive online
geopolitical forecasting system (MOOF) de-
scribed by Mellers et al. (Mellers et al., 2015;
Atanasov et al., 2016). Forecasters completed
tasks where they indicated the probability of dis-
crete outcomes for specific future geopolitical
events around the world. For example, forecasters
might be asked to forecast the likelihood of a coup
in Venezuela within the next 12 months. Respon-
dents indicated a probability for the event and, af-
ter the event resolved, they received a score (Brier,
1950) based on how close their probability re-
flected the outcome of the event.1

Recommendations generated from a another
parallel forecasting system were presented as
“tips”. In the parallel system, the recommendation
writers were asked “Why did you answer the way
you did?” when making forecasts, and were given
the option to mark their response as potentially
being useful to others. Comments marked useful
from the parallel system were then presented as
tips within the main MOOF system, where they
were evaluated for their subjective and objective
usefulness. Below is an example of one such com-
ment:

1Data, in the form of quality ratings, POS fea-
tures, and aggregate concept features is released at
http://http://www3.cs.stonybrook.edu/
˜has/objective_quality/. Due to IRB privacy
considerations we are unable to release the full data set.

Predicting the foreign ministers will
meet and state something along the lines
of ”hoping for a summit at the appropri-
ate time.” ... Predict this because of the
cautious language here and elsewhere:
http://www.china.org.cn/wap/2015-
03/13/... If wrong, will have time to
redress the damage done before any
possible summit.

The above forecaster identifies a reason for their
decision, a source of information (a Chinese news
website), and even a contingency plan if their rea-
soning doesn’t seem to be planning out. Fig-
ure 1 shows a representative screen shot from the
MOOF system.

We focus on one subjective and two objective
quality metrics for these recommendations:
rating: subjective ratings on a 5-point scale by the
forecasters in the parallel system.
influence: the rate at which MOOF forecasters up-
date their predictions after reading the comment.
benefit: the mean change in MOOF forecaster ac-
curacy resulting from updating their predictions
after rating the comment. Because there were nu-
merous confounding factors regarding the magni-
tude of change (i.e. forecaster quality, time un-
til task resolution), we simply encoded the change
as a binary indicator of whether it was positive or
negative.

For the purposes of this study, we consider both
influence and benefit as assessments of objective
utility, though they each capture different aspects
of utility (behavioral influence of others in the case
of influence; and specifically positive influence in
the case of benefit). The MOOF forecasters did
not know who wrote the tip or any other infor-
mation about it; their actions were based on the
comment’s content and not reputation of the com-
ment author. MOOF forecasters were also linked
directly to the form to update their forecast from
the comment to minimize outside influences be-
tween the reading of the comment and prediction
update. However, as the forecasters were not in a
laboratory, other Web browsing behavior in other
tabs could not be controlled.

In order to balance quality score reliability with
quantity of recommendation data, we restricted
the dataset to recommendations with at least 10
words that received at least 3 ratings. This re-
sulted in 8, 498 comments with ratings and influ-
ence scores. Out of those, 4, 317 comments had at

2349



Figure 1: Screenshot of the forecasting system prompting users to justify their forecasts with recommen-
dations. These recommendations are then passed on to the parallel MOOF system as “tips” which are
rated and sometimes lead forecasters to change their predictions.

rating v rating v influence v
influence benefit benefit

.45* .01 .06*

Table 1: Correlation (Pearson product-moment
coefficient) between all subjective and objective
quality metrics: rating: forecaster subjective rat-
ing, influence: forecaster update rate, benefit:
change in forecast accuracy due to update. *sig-
nificant at p < .01.

least one associated change in forecast, and thus
had a score for the final metric, benefit.

Table 1 shows correlations (Pearson product-
moment coefficients) between all subjective and
objective quality metrics. Rating and influence are
fairly predictive of each other (r = 0.45, p <
0.01), and neither correlates particularly strongly
with benefit. (r = 0.01, p > .01 for rating v bene-
fit, and r = 0.06, p < .01 for influence v benefit).
This implies that while comments rated for subjec-
tive utility are also more likely to influence users,
they are not necessary influencing them in a pos-
itive way. Further, it is possible (and indeed the
case) that the characteristics of comments deemed
subjectively useful may differ from those that ob-
jectively provide benefit. In Section 4 we consider
the content that leads to these differences in the
metrics.

3 Regression

Our goal is to predict the quality score of a com-
ment from its content over each of the three qual-
ity metrics: rating, influence, and benefit. These
3 types of features were chosen in order to ac-
count for qualitatively different types of linguistic
attributes. To capture linguistic variance at vary-
ing resolutions, we use a combination of open-
vocabulary and taxonomic linguistic features:
ngrams: 1 to 3 word sequences. These ngrams
were recorded as binary variables indicating
whether each ngram appeared in each comment.
We limited ngrams to those mentioned in at least
0.1% of all comments.
parts-of-speech: POS frequences The Stanford
Part-of-Speech tagger was used to identify parts of
speech in each comment. The relative frequency
of each tag (i.e., the probability of the tag, given
the comment) was recorded.
concepts: Nominal concepts within a hierarchy.
The WordNet noun ontology (Fellbaum, 1998)
was used. For each comment, we tracked all of
the hypernyms of each noun within the comment.
As features per comment, we use the presence of
each hypernym concept, limited to those concepts
that appeared in at least 0.1% of all comments.

To control for task-specific language, features
(n-grams) were restricted to those mentioned in at
least 50% of forecasting tasks. The n-grams, in-
tended to capture fine-grained lexical information,

2350



were also restricted to those mentioned in at least
1% of comments, while the parts-of-speech and
concepts, intended to capture more coarse-grained
linguistic characteristics, were restricted to those
appearing in 5% of comments. These thresholds
were chosen such that total number of features was
within the same order of magnitude as the number
of recommendations with objective scores In the
end, there were 1, 202 unique n-grams, 32 unique
parts-of-speech, and 155 unique concepts.

3.1 Predictive Modeling
We built predictive models of all three quality met-
rics based on the linguistic features of the mes-
sages. To handle covariance and avoid over-fitting
with so many features, we used a series of feature
selection and dimensionality reduction techniques
fed into a ridge regression to create the models.
Specifically, we filtered the features (which were
already restricted to those present in at least 50%
of the forecasting tasks) to those with a small lin-
ear correlation with the outcome, defined as hav-
ing a family-wise error rate < 60 (Toothaker,
1993). This feature selection was run indepen-
dently on each type of feature (n-grams, parts-of-
speech, and concepts). Similar to correcting for
multiple hypothesis tests, family-wise error pe-
nalizes groups containing more features (i.e. n-
grams) more stringently.

We then used randomized singular value de-
composition (SVD) (Halko et al., 2011) to reduce
the feature set to 1/5 the number of original fea-
tures. Randomized SVD uses stochastic sampling
to more efficiently calculate the principal com-
ponents (Martinsson et al., 2011).In this context,
SVD functions as a type of regularization to re-
duce variance by removing lesser principal com-
ponents (and thus helping to avoid overfit). Fi-
nally, the resulting dimensions were fit to the given
quality metric using L2 penalized linear regres-
sion. All feature selection, dimensionality reduc-
tion, and L2 parameter tuning were done over a
held-out portion of the training set.

3.2 Evaluation
To evaluate our models out-of-sample, we use 10-
fold cross-validation over the subjective ratings
(rating) and update rates (influence). In this pro-
cess, a random selection of 1/10th of the com-
ments are held-out as a test set, while the other
9/10ths are used to train (estimate) the model. This
model is then used to predict the quality of the

rating influence benefit

baseline .59 .24 .02

our model .76* .37 .21*

Table 2: Predictive accuracy (out-of-sample Pear-
son correlation coefficient) of our content-based
models across the subjective and objective mea-
sures. baseline: square-root number of words;
our model: based on ngrams, parts-of-speech, and
concepts. *significant reduction in error over the
baseline at p < .001.

comments in the 1/10th sample and compared to
the true quality for those comments (using Pear-
son correlation in this case). However, many of
our scores for change in forecaster accuracy (ben-
efit) are based simply on one change and thus quite
unreliable. While it is best to include such noisy
data when training, it does not provide a very ac-
curate assessment. Therefore, we use dedicated
training and test sets, where the test set is a ran-
dom sample of 500 comments with more than 3
updates and thus a more reliable mean change in
forecaster accuracy.

As a baseline, we use the square-root of the
number of words in the comment. This may
seem like weak measure of quality, but the his-
tory of automatic quality assessment is saturated
with findings that length is the best predictor of
quality. This holds true for both answers to ques-
tions (Agichtein et al., 2008; Surdeanu et al., 2011;
Beygelzimer et al., 2015); as well as e-commerce
reviews (Cao et al., 2011; Racherla and Friske,
2012). Of course, length is not as shallow as it
may seem at first; given no strong incentive for
authors to leave long comments, length is likely
a proxy for thoroughness of the comment. Still,
because we would like to understand the content
distinguishing various metrics of quality, we view
length as a baseline to move beyond.

Table 2 compares the accuracy of models built
on content (ngrams, parts-of-speech, and con-
cepts) to the baseline of length. In all cases, our
models, based on content, perform significantly
better than those based only on length. Further in
the case of benefit (change in forecaster accuracy),
length has virtually no predictive power.

2351



measure readability length & readability

rating .23 .59

influence .08 .24

Table 3: Predictive accuracy (out-of-sample
Pearson correlation coefficient) of the baseline of
length and the baseline of readability and their
combination across the subjective and objective
measures. length: square-root number of words;
readability: Flesch-Kincaid Scale.

3.3 Relation to Readability

Some previous work used on readability measures
to evaluate comment quality (e.g. (Agichtein
et al., 2008; Hsu et al., 2009)). Readability of-
ten refers to the difficulty or complexity scale of
a comment, determining the minimum age group
able to perceive it. Various readability measures
have been suggested in the past such as Flesch-
Kincaid Formula (Kincaid et al., 1975), Gunning-
Fog Index (Gunning, 1952), and SMOG Grading
(Mc Laughlin, 1969). All methods are based on
the combination of the count of syllables or words
in the comment (as a representation of syntactic
complexity), and the number of sentences in the
comments (representing the semantic complexity).
Such measures of readability are often considered
naive and questionable, however, they are com-
monly used and present a coarse evaluation of the
comment’s complexity.

We used Flesch-Kincaid scale to measure read-
ability. This scale measures readability by the
average number of syllables per word as well as
the average number of words per sentence (Doak
et al., 1996). We combined the baseline of length
and the baseline of readability in order to measure
the quality of comments. Table 3 shows results for
readability and length plus readability. We find no
significant improvement in the baseline and that
our model based on content still adds significantly
more predictive accuracy.2

4 Differential Analysis

The prediction results show promise for automated
quality assessment, and that linguistic content can
predict quality above-and-beyond length (a proxy
for comprehensiveness). Next, we explore what

2Adding length and readability together to the full model
had no benefit.

content exactly it is that is predictive of each qual-
ity metric, and what content suggests biases dis-
tinguishing subjective versus objective quality.

4.1 Method
To identify distinguishing features, we use a series
of multivariate linear regressions to find the rela-
tionship between each individual linguistic feature
and the given quality metric, controlling for com-
ment length – a process known as differential lan-
guage analysis (Schwartz et al., 2013). Specifi-
cally, the individual linguistic feature along with
the comment length (logged) are standardized and
used as independent variables, and then fit to the
standardized form of the given quality metric. The
coefficient given is then taken as the standardized
effect size of the relationship between that fea-
ture and the quality metric, holding length con-
stant (Fox, 1997). In other words, it tells us how
much the feature can explain the quality score, be-
yond what is explainable simply from length.

Using regression to relate variables, although
rarely done in Computer Science domains, is stan-
dard practice in social and political science (Fox,
1997), though typically not over thousands of
potential independent variables as we do here.
Therefore, we also correct for multiple hypothe-
ses by using the Benjamini-Hochberg proce-
dure (Benjamini and Hochberg, 1995) over our
significance tests.

Differential language analysis allows us to ob-
serve and test the unique relationship between
each feature and each metric, holding length con-
stant. In addition, we use the difference between
standardized metric scores to find the features that
distinguish high quality comments in one metric
versus another. All methods were implemented
within the package, dlatk (Schwartz et al., 2017).

4.2 Quality Comment Features
Figure 2 shows the n-grams most highly correlated
with each of our quality metrics. Size indicates
correlation strength while color represents over-
all frequency. Across both subjective ratings and
objective update rates, we see discussion of news
plays an important role (e.g. “news”, “article”,
and “www.reuters.com”). We do not see the same
from comments resulting in positive changes of
forecaster accuracy (benefit), which seemed to be
distinguished by negation (e.g. “no”, “unlikely”).
For influence, we see other features indicating
probabilistic reasoning (e.g. “%”); the individual

2352



Figure 2: Top: ngrams (words and phrases) most distinguishing high quality comments based on (a)
subjective ratings, (b) objective forecaster update rates, and (c) objective changes in forecaster accuracy.
Bottom: ngrams (words and phrases) most distinguishing (d) subjective ratings from objective update
rates and (e) objective update rates from objective changes in forecaster accuracy. All correlates are
significant at p < .05 after a Benjamini-Hochberg false-discovery rate correction.

numbers in influence actually represent numbered
lists of signal. These features are more indicative
of a comment that convinces one to update their
prediction rather than one that highly rated.

We can directly observe the differences in what
the metrics capture by looking at the final two vi-
sualizations in Figure 2, rating v influence and rat-
ing v benefit. Rating v influence tells us which
ngrams were predictive of high quality comments
that were less likely to result in a forecast update.
Discussion of energy topics (e.g. “oil”, “prices”,
“cut”, “production”) are predictive of comments
subjectively rated higher than their update rates
would suggest. Further, discussion of dates (e.g.
“january”, “may”, “days”, “2015”, “2013”) seems
to predict comments that lead forecasters to update
but which do not actually result in better predic-
tions (influence v benefit). Discussion of news and
articles (“article”, “news”, “html”, “question” ap-
pears to predict subjectively top-rated comments
from those comments that actually result in better

predictions (influence v benefit).

Table 4 shows differential language of quality
based on part-of-speech. We observe that some
patterns from the ngram results are generalized.
Examples of these patterns include more num-
bers, parentheses, and quotes in highly rated and
influential comments. Other results are some-
what novel, such as the use of more adverbs and
subordinate conjunctions (e.g. “though”, “since”,
“whereas”) in comments leading to better forecast
accuracy, and that both ratings and influence fa-
vored quotes.

We notice that including explanation or af-
terthought (using more parentheses) can predict
subjectively high rated comments from the com-
ments leading to updated ratings. The use of
quotes and numbers along with mentioning proper
nouns can predict influential comments that do
not help in better predictions. Including explana-
tion, quotes, and numbers as well as reporting past
events seems to predict comment that convinces

2353



Parts-of-speech

rating parentheses number line-break quote verb, past-tense

influence line-break number parentheses verb, past-tense quote

benefit adverb sub-conjunction - - -

rating v influence parentheses - - - -

influence v benefit quotes proper noun number - -

rating v benefit parentheses quotes number verb, past-tense -

Table 4: Top: Most distinguishing parts-of-speech correlating with the three metrics of quality: sub-
jective rating, forecaster updates (influence), and forecaster accuracy (benefit). Bottom: parts-of-speech
most predictive of differences in quality metric scores (rating v influence and rating v benefit). All
correlates are significant at p < .05 after a Benjamini-Hochberg false-discovery rate correction.

one to update their prediction as opposed to help-
ing better forecasting accuracy.

Distinguishing concepts, in Table 5 offer a dif-
ferent perspective. Discussions of documents and
written material characterize highly rated and in-
fluential comments, while changes in accuracy
(benefit) were characterized by more discussion of
abstract attributes or states of being. Highly rated
comments were more likely to discuss concepts re-
lated to transactions and materials than influential
comments, but they are more likely to discuss con-
cepts about creation compared to comments re-
sulting in better predictions.

5 Related Work

While no prior work has focused on recommenda-
tion quality in terms of how readers change or im-
prove decisions (i.e. objective metrics), there is an
extensive body of literature on the automated anal-
ysis of subjective comment quality from which
we build. Such work typically uses subjective
assessments specific to their application domain
(e.g. thumbs up/ thumbs down over YouTube com-
ments, answer ratings in Yahoo Answers, or help-
fulness ratings of Amazon product reviews). Be-
low we discuss such work organized into three
main categories of subjective comment quality:
comment usefulness, the quality of answers in QA
platform, and comment helpfulness.

Comment usefulness concerns the acceptance
(vs. non-acceptance) of comments by a commu-
nity (Siersdorfer et al., 2010). Some previous
work has focused on the usefulness of YouTube
comments. For example, Siersdorfer et. al. (Siers-

dorfer et al., 2010) have used support vector ma-
chines to identify the acceptance of comments by
the community in 6 million comments on 67,000
YouTube videos. They showed that community
feedback and term weight features can be good
predictors of comment acceptance. Other work
such as (Momeni et al., 2013) predicted com-
ment usefulness on YouTube and Flickr, and found
that comments rated as useful usually include
named entities and “insight”-related terms (think,
know, consider, etc.), whereas non-useful com-
ments contain emotional and affective expression,
and “certainty”-related (always, never, etc.).

Others working on comment quality assessment
focus on user-generated answers in social media
and QA platforms. (Bian et al., 2008) present a
general ranking approach for finding the answers
from 1,250 TREC factoid questions containing at
least one similar question from Yahoo! Answers.
They found that various features including tex-
tual (e.g. word overlap, length ratio), and com-
munity (e.g. total points, total answers) are im-
portant in retrieving factual answers, whereas sta-
tistical features (e.g. length, lifetime, popularity)
are not very effective. Exploring if additional fea-
tures could outperform answer length in predicting
the best answer, Beygelzimer et al. (2015) con-
sidered a wide variety of features including func-
tional, linguistic, questioner and answerer person-
alization, and “superlative” features, but were un-
able to overcome the length baseline.

Helpfulness is mainly defined in the context of
online reviews and represents the number of users
indicating a particular review was helpful. Us-
ing structural features like sentence tokens, length,

2354



Concepts

rating
document, written document, papers – writ-
ing that provides information (especially in-
formation of an official nature).

gathering, assemblage – a group of persons
together in one place.

influence

writing, written material, piece of writing –
the work of a writer; anything expressed in
letters of the alphabet (especially when con-
sidered from the point of view of style and
effect).

auditory communication – communication
that relies on hearing.

benefit attribute – an abstraction belonging to orcharacteristic of an entity.
state – the way something is with respect to
its main attributes.

rating v influence
transaction, dealing, dealings – the act of
transacting within or between groups (as car-
rying on commercial activities).

material, stuff – the tangible substance that
goes into the makeup of a physical object.

rating v benefit creation – an artifact that has been broughtinto existence by someone.

Table 5: Top: Select concepts correlating with the three metrics of quality: subjective rating, forecaster
updates (influence), and forecaster accuracy (benefit). Bottom: concepts most predictive of differences
in qualtric metric scores (rating v influence and rating v benefit). All correlates are significant at p < .05
after a Benjamini-Hochberg false-discovery rate correction.

proportion of question sentences along with lex-
ical and syntactic features, Kim et. al. (2006)
could achieve rank correlations of up to 0.66 with
helpfulness votes of Amazon reviews. Ghose
and Ipeirotis (2011) analyzed length, readability,
and subjective and objective information on Ama-
zon.com reviews finding that reviews with ob-
jective, and highly subjective sentences are rated
more helpful. Similar findings were reported by
Mudambi and Schuff (2010), finding that review
extremity, review depth, and product type affect
the perceived helpfulness of the review.

Most studies listed thus far found length of
comment to be the dominant predictor, with other
features providing minimal benefit. However, a
few studies (including our own) have found this
baseline can be overcome. For example, Racherla
and Friske (Racherla and Friske, 2012) investi-
gated perceived usefulness of consumer reviews
on Yelp and found that reputation and expertise
were more important than total number of words
on perceived usefulness.

All of these prior works focus on assessing sub-
jective aspects of comments (usefulness, quality,
and helpfulness); Perhaps the study coming clos-
est in spirit to our own was Ghose et al. Ghose
et al. (2007) who quantified quality of reviews
by the economic change they produced. How-
ever, they still were not dealing with a randomized
experiment and so conclusions were correlational

and the objective was better sales of the product
rather than benefit to the reader (i.e. leading to a
better decision).

6 Conclusion

Our results suggest three key findings. First, what
one writes in a comment is more important than
simply how much one writes; this is true across
both subjective and objective outcomes, though
length had virtually no predictive ability for im-
proving forecaster accuracy. Second, we found
many linguistic features characteristic of quality,
many of which seemingly align with attributes
of strong forecasters (Mellers et al., 2015). For
example, high quality comments contained sig-
nals of probabilistic reasoning (e.g. “%”, “un-
likely”, numerical parts-of-speech), inductive rea-
soning (e.g. justifications with “news” and doc-
uments), and cognitive flexibility (e.g. subordi-
nate conjunctions which signal more complex sen-
tence constructions used to relate two independent
clauses or ideas).

Most importantly, our results suggest a sub-
jective bias: that what people believe to be use-
ful does not always turn out to be truly useful.
Subjective ratings were favorably biased towards
comments containing energy-related content (e.g.
“oil”, “production”, “prices”), news articles, and
nouns of creation and materials (as opposed to ab-
stractions or attributes).

2355



The implications of identifying subjective bi-
ases in comment quality extend to many domains
involving comment ratings. Consumers of com-
ments, typically, desire information that ultimately
leads to real utility benefits, and this domain is
not the only one where objective quality can be
obtained: For example, one could: (1) ask con-
sumers of restaurant reviews to indicate if one con-
vinces them to go to the restaurant and then follow
up on their experience, (2) consider evaluating re-
search paper quality – reviewer ratings versus ci-
tation count (influence), (3) determine whether the
answer to the question about how to drive to con-
serve fuel lead to the reader actually using less
gas? A review being convincing or being ”liked”
may correlate with better outcomes, but it is not
equivalent.

7 Acknowledgments

Funding This research was supported by the
Intelligence Advanced Research Projects Activ-
ity (IARPA) via the Department of Interior Na-
tional Business Center (DoI/NBC) Contract No.
D11PC20061. The U.S. government is authorized
to reproduce and distribute reprints for govern-
ment purposes notwithstanding any copyright an-
notation thereon. The views and conclusions ex-
pressed herein are those of the authors and should
not be interpreted as necessarily representing the
official policies or endorsements, either expressed
or implied, of IARPA, DoI/ NBC, or the U.S. gov-
ernment. This work also supported, in part, from
the Templeton Religion Trust, grant TRT-0048.

References
Eugene Agichtein, Carlos Castillo, Debora Donato,

Aristides Gionis, and Gilad Mishne. 2008. Finding
high-quality content in social media. In Proceedings
of the 2008 International Conference on Web Search
and Data Mining. ACM, pages 183–194.

P. Atanasov, W. Chang, S. Patil, B. Mellers, and P Tet-
lock. 2016. Accountability and adaptive perfor-
mance: The long-term view. Under Review .

Hila Becker, Dan Iter, Mor Naaman, and Luis Gra-
vano. 2012. Identifying content for planned events
across social media sites. In Proceedings of the fifth
ACM international conference on Web search and
data mining. ACM, pages 533–542.

Yoav Benjamini and Yosef Hochberg. 1995. Control-
ling the false discovery rate: a practical and pow-
erful approach to multiple testing. Journal of the

Royal Statistical Society. Series B (Methodological)
pages 289–300.

Alina Beygelzimer, Ruggiero Cavallo, and Joel
Tetreault. 2015. On yahoo answers, long answers
are best. In Proceedings of CrowdML: The ICML 15
Workshop on Crowdsourcing and Machine Learn-
ing.

Jiang Bian, Yandong Liu, Eugene Agichtein, and
Hongyuan Zha. 2008. Finding the right facts in the
crowd: factoid question answering over social me-
dia. In Proceedings of the 17th international confer-
ence on World Wide Web. ACM, pages 467–476.

Glenn W Brier. 1950. Verification of forecasts ex-
pressed in terms of probability. Monthly weather
review 78(1):1–3.

Qing Cao, Wenjing Duan, and Qiwei Gan. 2011. Ex-
ploring determinants of voting for the “helpfulness”
of online user reviews: A text mining approach. De-
cision Support Systems 50(2):511–521.

Cecilia Conrath Doak, Leonard G Doak, and Jane H
Root. 1996. Teaching patients with low literacy
skills. AJN The American Journal of Nursing
96(12):16M.

Christiane Fellbaum. 1998. WordNet. Wiley Online
Library.

John Fox. 1997. Applied regression analysis, linear
models, and related methods.. Sage Publications,
Inc.

Anindya Ghose and Panagiotis G Ipeirotis. 2011. Es-
timating the helpfulness and economic impact of
product reviews: Mining text and reviewer charac-
teristics. Knowledge and Data Engineering, IEEE
Transactions on 23(10):1498–1512.

Anindya Ghose, Panagiotis G Ipeirotis, and Arun Sun-
dararajan. 2007. Opinion mining using economet-
rics: A case study on reputation systems. In an-
nual meeting-association for computational linguis-
tics. volume 45, page 416.

Robert Gunning. 1952. {The Technique of Clear
Writing} .

Nathan Halko, Per-Gunnar Martinsson, and Joel A
Tropp. 2011. Finding structure with random-
ness: Probabilistic algorithms for constructing ap-
proximate matrix decompositions. SIAM review
53(2):217–288.

Chiao-Fang Hsu, Elham Khabiri, and James Caver-
lee. 2009. Ranking comments on the social web.
In Computational Science and Engineering, 2009.
CSE’09. International Conference on. IEEE, vol-
ume 4, pages 90–97.

Soo-Min Kim, Patrick Pantel, Tim Chklovski, and
Marco Pennacchiotti. 2006. Automatically assess-
ing review helpfulness. In Proceedings of the 2006

2356



Conference on empirical methods in natural lan-
guage processing. Association for Computational
Linguistics, pages 423–430.

J Peter Kincaid, Robert P Fishburne Jr, Richard L
Rogers, and Brad S Chissom. 1975. Derivation of
new readability formulas (automated readability in-
dex, fog count and flesch reading ease formula) for
navy enlisted personnel. Technical report, DTIC
Document.

Jingjing Liu, Yunbo Cao, Chin-Yew Lin, Yalou Huang,
and Ming Zhou. 2007. Low-quality product review
detection in opinion summarization. In EMNLP-
CoNLL. pages 334–342.

Per-Gunnar Martinsson, Vladimir Rokhlin, and Mark
Tygert. 2011. A randomized algorithm for the de-
composition of matrices. Applied and Computa-
tional Harmonic Analysis 30(1):47–68.

G Harry Mc Laughlin. 1969. Smog grading-a new
readability formula. Journal of reading 12(8):639–
646.

Barbara Mellers, Eric Stone, Pavel Atanasov, Nick
Rohrbaugh, S Emlen Metz, Lyle Ungar, Michael M
Bishop, Michael Horowitz, Ed Merkle, and Philip
Tetlock. 2015. The psychology of intelligence anal-
ysis: Drivers of prediction accuracy in world poli-
tics. Journal of experimental psychology: applied
21(1):1.

Elaheh Momeni, Claire Cardie, and Myle Ott. 2013.
Properties, prediction, and prevalence of useful user-
generated comments for descriptive annotation of
social media objects. In ICWSM-2013: Proceed-
ings of the International AAAI Conference on We-
blogs and Social Media.

Susan M Mudambi and David Schuff. 2010. What
makes a helpful review? a study of customer reviews
on amazon. com. MIS quarterly 34(1):185–200.

Pradeep Racherla and Wesley Friske. 2012. Perceived
’usefulness’ of online consumer reviews: An ex-
ploratory investigation across three services cate-
gories. Electronic Commerce Research and Appli-
cations 11(6):548–559.

H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Stephanie M
Ramones, Megha Agrawal, Achal Shah, Michal
Kosinski, David Stillwell, Martin EP Seligman, and
Lyle H. Ungar. 2013. Personality, gender, and age in
the language of social media: The open-vocabulary
approach. PloS one 8(9).

H Andrew Schwartz, Salvatore Giorgi, Maarten Sap,
Patrick Crutchley, Johannes C Eichstaedt, and Lyle
Ungar. 2017. DLATK: Differential Language Anal-
ysis ToolKit. In Proceedings of the 2017 Confer-
ence on Empirical Methods in Natural Language
Processing.

Stefan Siersdorfer, Sergiu Chelaru, Wolfgang Nejdl,
and Jose San Pedro. 2010. How useful are your
comments?: analyzing and predicting youtube com-
ments and comment ratings. In Proceedings of the
19th international conference on World wide web.
ACM, pages 891–900.

Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics 37(2):351–383.

Larry E Toothaker. 1993. Multiple comparison proce-
dures. 89. Sage.

Zhongyu Wei, Yang Liu, and Yi Li. 2016. Is this post
persuasive? ranking argumentative comments in the
online forum. In The 54th Annual Meeting of the As-
sociation for Computational Linguistics. page 195.

2357


