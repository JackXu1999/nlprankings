

















































Adversarial Deep Averaging Networks
for Cross-Lingual Sentiment Classification

Xilun Chen†
xlchen@cs.cornell.edu

Yu Sun†
ys646@cornell.edu

Ben Athiwaratkun‡
pa338@cornell.edu

Claire Cardie†
cardie@cs.cornell.edu

Kilian Weinberger†
kqw4@cornell.edu

†Department of Computer Science, Cornell University
‡Department of Statistical Science, Cornell University

Abstract

In recent years great success has been
achieved in sentiment classification for En-
glish, thanks in part to the availability of
copious annotated resources. Unfortunately,
most languages do not enjoy such an abun-
dance of labeled data. To tackle the senti-
ment classification problem in low-resource
languages without adequate annotated data,
we propose an Adversarial Deep Averaging
Network (ADAN1) to transfer the knowledge
learned from labeled data on a resource-rich
source language to low-resource languages
where only unlabeled data exist. ADAN has
two discriminative branches: a sentiment
classifier and an adversarial language dis-
criminator. Both branches take input from a
shared feature extractor to learn hidden rep-
resentations that are simultaneously indica-
tive for the classification task and invariant
across languages. Experiments on Chinese
and Arabic sentiment classification demon-
strate that ADAN significantly outperforms
state-of-the-art systems.

1 Introduction

Many state-of-the-art models for sentiment classi-
fication (Socher et al., 2013; Iyyer et al., 2015; Tai
et al., 2015) are supervised learning approaches
that rely on the availability of an adequate amount
of labeled training data. For a few resource-rich
languages, including English, such labeled data
are indeed available. For the vast majority of lan-
guages, however, it is the norm that only a lim-
ited amount of annotated text exists. Worse still,
many low-resource languages have no labeled data
at all.

1The source code of ADAN is available at https://
github.com/ccsasuke/adan.

To aid the creation of sentiment classification
systems in such low-resource languages, an active
research direction is cross-lingual sentiment clas-
sification (CLSC), in which the abundant re-
sources of a source language (likely English,
denoted as SOURCE) are leveraged to produce sen-
timent classifiers for a target language (TARGET).
In general, CLSC methods make use of general-
purpose bilingual resources—such as hand-crafted
bilingual lexica or parallel corpora—to alleviate
or eliminate the need for task-specific TARGET an-
notations. In particular, the bilingual resource of
choice for the majority of previous CLSC models
is a full-fledged Machine Translation (MT) sys-
tem (Wan, 2008, 2009; Lu et al., 2011; Zhou et al.,
2016), a component that is expensive to obtain.
In this work, we propose a language-adversarial
training approach that does not need a highly
engineered MT system, and requires orders of
magnitude less in terms of the size of a parallel
corpus. Specifically, we propose an Adversarial
Deep Averaging Network (ADAN) that leverages
a set of bilingual word embeddings (BWEs; Zou
et al., 2013) trained on bitexts, in order to elimi-
nate the need for labeled TARGET training data.2

We introduce the ADAN model in §2, and in §3
evaluate ADAN using English as the SOURCE with
two TARGET choices: Chinese and Arabic. ADAN
is first compared to two baseline systems: (i) one
trained only on labeled SOURCE data, relying on
BWEs for cross-lingual generalization; and (ii) a
domain adaptation method (Chen et al., 2012) that
views the two languages simply as two distinct
domains. We then validate ADAN against two
state-of-the-art CLSC methods: (iii) an approach

2When not using any TARGET annotations, the setting
is sometimes referred to as unsupervised (in the target lan-
guage) in the literature. Similarly, when some labeled data is
used, it is called the semi-supervised setting.

557

Transactions of the Association for Computational Linguistics, vol. 6, pp. 557–570, 2018. Action Editor: Trevor Cohn.
Submission batch: 2/2018; Revision batch: 5/2018; Published 12/2018.

c© 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.

https://github.com/ccsasuke/adan
https://github.com/ccsasuke/adan


that uses a powerful MT system, and (iv) the cross-
lingual “distillation” approach of Xu and Yang
(2017) that makes direct use of a parallel corpus
(see §3.2). In all cases, we find that ADAN achieves
statistically significantly better results.

We further investigate the semi-supervised set-
ting, where a small amount of annotated TARGET
data exists, and show that ADAN continues to out-
perform the alternatives given the same amount of
TARGET supervision (§3.3.1). We provide an anal-
ysis and visualization of ADAN (§3.3.2), shedding
light on how our approach manages to achieve
its strong cross-lingual performance. Addition-
ally, we study the bilingual resource that ADAN
depends on, the BWEs, and demonstrate that
ADAN’s performance is robust with respect to the
choice of BWEs. Furthermore, even without the
pre-trained BWEs (i.e., using random initialized
embeddings), ADAN outperforms all but the state-
of-the-art MT-based and distillation systems
(§3.3.3). This makes ADAN the first CLSC model
that outperforms BWE-based baseline systems
without relying on any bilingual resources.

A final methodological contribution distin-
guishes ADAN from previous adversarial networks
for text classification (Ganin et al., 2016): ADAN
minimizes the Wasserstein distance (Arjovsky
et al., 2017) between the feature distributions of
SOURCE and TARGET (§2.2), which yields bet-
ter performance and smoother training than the
standard training method with a Gradient Rever-
sal Layer (Ganin et al., 2016; see §3.3.5).

2 The ADAN Model

The central hypothesis of ADAN is that an ideal
model for CLSC should learn features that both
perform well on sentiment classification for the
SOURCE, and are invariant with respect to the
shift in language. Therefore, as shown in Figure 1,
ADAN has a joint feature extractor F that aims to
learn features that aid prediction of the sentiment
classifier P , and hamper the language discrim-
inator Q, whose goal is to identify whether an
input text is from SOURCE or TARGET. The intu-
ition is that if a well-trained Q cannot tell the lan-
guage of a given input using the features extracted
by F , those features are effectively language-
invariant. Q is hence adversarial because it does
its best to identify language from learned fea-
tures, yet good performance from Q indicates
that ADAN is not successful in learning language-

Joint 

Feature Extractor 

F

The movie was great.

(Bilingual) Word Embeddings

En Texts Ch Texts

Sentiment 

Classifier 

P

Language 

Discriminator 

Q

Sentiment Label

Jp

Language Score

EnCh
+∞-∞

Jq−Jq

Figure 1: ADAN with Chinese as the target language.
The lines illustrate the training flows and the arrows
indicate forward and/or backward passes. Blue lines
show the flow for English samples and yellow ones are
for Chinese. Jp and Jq are the training objectives of P
and Q, respectively (§2.2). The parameters of F , P ,
and the embeddings are updated together (solid lines).
The parameters ofQ are updated using a separate opti-
mizer (dotted lines) because of its adversarial objective.

invariant features. Upon successful ADAN training,
F should have learned features discriminative for
sentiment classification, at the same time provid-
ing no information for the adversarial Q to guess
the language of a given input.

As seen in Figure 1, ADAN is exposed to both
SOURCE and TARGET texts during training. Un-
labeled SOURCE (blue lines) and TARGET (yellow
lines) data go through the language discrimina-
tor, whereas only the labeled SOURCE data pass
through the sentiment classifier.3 The feature ex-
tractor and the sentiment classifier are then used
for TARGET texts at test time. In this manner, we
can train ADAN with labeled SOURCE data and
only unlabeled TARGET text. When some labeled
TARGET data exist, ADAN could naturally be ex-
tended to take advantage of that for improved per-
formance (§3.3.1).

3“Unlabeled” and “labeled” refer to sentiment labels; all
texts are assumed to have the correct language label.

558



2.1 Network Architecture

As illustrated in Figure 1, ADAN is a feed-forward
network with two branches. There are three main
components in the network: a joint feature extrac-
tor F that maps an input sequence x to the shared
feature space, a sentiment classifier P that pre-
dicts the label for x given the feature representa-
tion F(x), and a language discriminator Q that
also takes F(x) but predicts a scalar score indicat-
ing whether x is from SOURCE or TARGET.

An input document is modeled as a sequence
of words x = w1, . . . , wn, where each w is
represented by its word embedding vw (Turian
et al., 2010). For improved performance, pre-
trained BWEs (Zou et al., 2013; Gouws et al.,
2015) can be used to induce bilingual distributed
word representations so that similar words are
closer in the embedded space regardless of lan-
guage.

A parallel corpus is often required to train high-
quality BWEs, making ADAN implicitly depen-
dent on the bilingual corpus. However, compared
with the MT systems used in other CLSC meth-
ods, training BWEs only requires one to two
orders of magnitude less parallel data, and some
methods only take minutes to train on a consumer
CPU (Gouws et al., 2015), whereas state-of-the-
art MT systems need days to weeks for training
on multiple GPUs. Moreover, even with randomly
initialized embeddings, ADAN can still outperform
some baseline methods that use pre-trained BWEs
(§3.3.3). Another possibility is to take advantage
of the recent work that trains BWEs with no bilin-
gual supervision (Lample et al., 2018).

We adopt the Deep Averaging Network (DAN)
by Iyyer et al. (2015) for the feature extractor
F . We choose DAN for its simplicity to illus-
trate the effectiveness of our language-adversarial
training framework, but other architectures can
also be used for the feature extractor (§3.3.4). For
each document, DAN takes the arithmetic mean of
the word vectors as input, and passes it through
several fully connected layers until a softmax for
classification. In ADAN, F first calculates the av-
erage of the word vectors in the input sequence,
then passes the average through a feed-forward
network with rectified linear unit (ReLU) nonlin-
earities. The activations of the last layer in F are
considered the extracted features for the input and
are then passed on toP andQ. The sentiment clas-
sifierP and the language discriminatorQ are stan-

dard feed-forward networks.P has a softmax layer
on top for sentiment classification andQ ends with
a linear layer of output width 1 to assign a lan-
guage identification score.4

2.2 Adversarial Training
Before describing the adversarial training ap-
proach used in ADAN, we introduce a pre-
existing formulation of its adversarial component
in which training is done using a Gradient Rever-
sal Layer (Ganin et al., 2016). We refer to this
version of ADAN as ADAN-GRL.

In ADAN-GRL, Q is a binary classifier with a
sigmoid layer on top so that the language identi-
fication score is always between 0 and 1 and is
interpreted as the probability of whether an input
text x is from SOURCE or TARGET given its hid-
den features F(x). For training,Q is connected to
F via a GRL (Ganin and Lempitsky, 2015), which
preserves the input during the a forward pass but
multiplies the gradients by −λ during a backward
pass. λ is a hyperparameter that balances the
effects that P and Q have on F respectively.
This way, the entire network can be trained in its
entirety using standard backpropagation.

Unfortunately, researchers have found that the
training of F and Q in ADAN-GRL might not be
fully in sync (Ganin and Lempitsky, 2015), and
efforts need to be made to coordinate the adver-
sarial training. This is achieved by setting λ to
a non-zero value only once out of k batches, as
in practice we observe that F trains faster than Q.
Here, k is another hyperparameter that coordinates
the training of F and Q. When λ = 0, the gradi-
ents fromQwill not be backpropagated toF . This
allows Q more iterations to adapt to F before F
makes another adversarial update.

To illustrate the limitations of ADAN-GRL and
motivate the formal introduction of our ADAN
model, consider the distribution of the joint hid-
den features F for both SOURCE and TARGET
instances:

P srcF , P (F(x)|x ∈ SOURCE)
P tgtF , P (F(x)|x ∈ TARGET)

In order to learn language-invariant features,
ADAN trains F to make these two distribu-
tions as close as possible for better cross-lingual

4Q simply tries to maximize scores for SOURCE texts and
minimize for TARGET, and the scores are not bounded.

559



Algorithm 1 ADAN Training
Require: labeled SOURCE corpus Xsrc; unlabeled

TARGET corpus Xtgt; Hyperpamameter λ > 0,
k ∈ N, c > 0.

1: repeat
2: . Q iterations
3: for qiter = 1 to k do
4: Sample unlabeled batch xsrc ∼ Xsrc
5: Sample unlabeled batch xtgt ∼ Xtgt
6: fsrc = F(xsrc)
7: ftgt = F(xtgt) . feature vectors
8: lossq = −Q(fsrc) +Q(ftgt) . Eqn (2)
9: Update Q parameters to minimize lossq

10: ClipWeights(Q,−c, c)
11: . Main iteration
12: Sample labeled batch (xsrc,ysrc) ∼ Xsrc
13: Sample unlabeled batch xtgt ∼ Xtgt
14: fsrc = F(xsrc)
15: ftgt = F(xtgt)
16: loss = Lp(P(fsrc);ysrc) + λ(Q(fsrc) −
Q(ftgt)) . Eqn (4)

17: Update F , P parameters to minimize loss
18: until convergence

generalization. In particular, as argued by Arjovsky
et al. (2017), previous approaches to training
adversarial networks such as ADAN-GRL are
equivalent to minimizing the Jensen-Shannon di-
vergence between two distributions, in our case
P srcF and P

tgt
F . And because the Jensen-Shannon

divergence suffers from discontinuities, providing
less useful gradients for trainingF , Arjovsky et al.
(2017) propose instead to minimize the Wasserstein
distance and demonstrate its improved stability for
hyperparameter selection.

As a result, the ADAN training algorithm (see
Algorithm 1) departs from the earlier ADAN-
GRL training method. In ADAN, we instead mini-
mize the Wasserstein distance W between P srcF
and P tgtF according to the Kantorovich-Rubinstein
duality (Villani, 2008):

W (P srcF , P
tgt
F ) = (1)

sup
‖g‖L≤1

E
f(x)∼P srcF

[g(f(x))]− E
f(x′)∼P tgtF

[
g(f(x′))

]
where the supremum (maximum) is taken over the
set of all 1-Lipschitz5 functions g. In order to
(approximately) calculate W (P srcF , P

tgt
F ), we use

the language discriminator Q as the function g in
(1), whose objective is then to seek the supremum

5A function g is 1-Lipschitz iff |g(x) − g(y)| ≤ |x − y|
for all x and y.

in Equation 1. To make Q a Lipschitz function
(up to a constant), the parameters of Q are always
clipped to a fixed range [−c, c]. Let Q be parame-
terized by θq, then the objective Jq ofQ becomes:

Jq(θf ) ≡ max
θq

E
F(x)∼P srcF

[Q(F(x))]

− E
F(x′)∼P tgtF

[
Q(F(x′))

]
(2)

Intuitively, Q tries to output higher scores for
SOURCE instances and lower scores for TARGET
(as shown in line 8 of Algorithm 1). More for-
mally, Jq is an approximation of the Wasserstein
distance between P srcF and P

tgt
F in Equation 1.

For the sentiment classifier P parameterized by
θp, we use the traditional cross-entropy loss, de-
noted as Lp(ŷ, y), where ŷ and y are the predicted
label distribution and the true label, respectively.
Lp is the negative log-likelihood that P predicts
the correct label. We therefore seek the minimum
of the following loss function for P:

Jp(θf ) ≡ min
θp

E
(x,y)

[Lp(P(F(x)), y)] (3)

Finally, the joint feature extractor F parameter-
ized by θf strives to minimize both the sentiment
classifier loss Jp and W (P srcF , P

tgt
F ) = Jq:

Jf ≡ min
θf

Jp(θf ) + λJq(θf ) (4)

where λ is a hyperparameter that balances the two
branches P and Q. (See line 16 in Algorithm 1.)

As proved by Arjovsky et al. (2017) and ob-
served in our experiments (§3.3.5), minimizing the
Wasserstein distance is much more stable with re-
spect to hyperparameter selection compared with
ADAN-GRL, saving the hassle of carefully varying
λ during training (Ganin and Lempitsky, 2015).
In addition, ADAN-GRL needs to laboriously coor-
dinate the alternating training of the two compet-
ing components by setting the hyperparameter k,
which indicates the number of iterations one com-
ponent is trained before training the other. The
performance can degrade substantially if k is not
properly set. In our case, however, delicate tuning
of k is no longer necessary becauseW (P srcF , P

tgt
F )

is approximated by maximizing Equation 2; thus,
training Q to optimum using a large k can pro-
vide better performance (but is slower to train). In
our experiments, we fix λ = 0.1 and k = 5 for
all experiments (train five Q iterations per F and
P iteration), and the performance is stable over a
large set of hyperparameters (§3.3.5).

560



Methodology Approach Accuracy

Chinese Arabic

Train-on-SOURCE-only Logistic Regression 30.58% 45.83%
DAN 29.11% 48.00%

Domain Adaptation mSDA (Chen et al., 2012) 31.44% 48.33%

Machine Translation Logistic Regression + MT 34.01% 51.67%
DAN + MT 39.66% 52.50%

CLD-based CLTC CLD-KCNN (Xu and Yang, 2017) 40.96% 52.67%
†

CLDFA-KCNN (Xu and Yang, 2017) 41.82% 53.83%†

Ours ADAN 42.49%±0.19% 54.54%±0.34%
† As Xu and Yang (2017) did not report results for Arabic, these numbers are obtained based on our

reproduction using their code.

Table 1: ADAN performance for Chinese (5-cls) and Arabic (3-cls) sentiment classification without using labeled
TARGET data. All systems but the CLD ones use BWE to map SOURCE and TARGET words into the same space.
CLD-based CLTC represents cross-lingual text classification methods based on cross-lingual distillation (Xu and
Yang, 2017) and is explained in §3.2. For ADAN, average accuracy and standard errors over five runs are shown.
Bold numbers indicate statistical significance over all baseline systems with p < 0.05 under a one-sample t-test.
As a comparison, the supervised English accuracy of our ADAN model is 58.7% (5-class) and 75.6% (3-class).

3 Experiments and Discussions

To demonstrate the effectiveness of our model, we
experiment on Chinese and Arabic sentiment clas-
sification, using English as SOURCE for both. For
all data used in experiments, tokenization is done
using Stanford CoreNLP (Manning et al., 2014).

3.1 Data
Labeled English Data We use a balanced data
set of 700K Yelp reviews from Zhang et al. (2015)
with their ratings as labels (scale 1−5). We also
adopt their train−validation split: 650K reviews
for training and 50K form a validation set.

Labeled Chinese Data Because ADAN does not
require labeled Chinese data for training, these
annotated data are solely used to validate the per-
formance of our model. We use 10K balanced
Chinese hotel reviews from Lin et al. (2015) as the
validation set for model selection and parameter
tuning. The results are reported on a separate test
set of another 10K hotel reviews. For Chinese, the
data are annotated with five labels (1−5).
Unlabeled Chinese Data For the unlabeled
TARGET data used in training ADAN, we use an-
other 150K unlabeled Chinese hotel reviews.

English−Chinese Bilingual Word Embeddings
For Chinese, we used the pre-trained BWEs
by Zou et al. (2013). Their work provides 50-
dimensional embeddings for 100K English words

and another set of 100K Chinese words. See
§3.3.3 for more experiments and discussions.

Labeled Arabic Data We use the BBN Arabic
Sentiment Analysis data set (Mohammad et al.,
2016) for Arabic sentiment classification. The data
set contains 1,200 sentences (600 validation +
600 test) from social media posts annotated with
3 labels (−, 0, +). The data set also provides
machine translated text to English. Since the la-
bel set does not match with the English data set,
we map all the rating 4 and 5 English instances to
+ and the rating 1 and 2 instances to −, and the
rating 3 sentences are converted to 0.

Unlabeled Arabic Data For Arabic, no addi-
tional unlabeled data are used. We only use the
text from the validation set (without labels) during
training.

English−Arabic Bilingual Word Embed-
dings For Arabic, we train a 300d BilBOWA
BWE (Gouws et al., 2015) on the United Nations
corpus (Ziemski et al., 2016).

3.2 Cross-Lingual Sentiment Classification

Our main results are shown in Table 1, which
shows very similar trends for Chinese and Ara-
bic. Before delving into discussions on the perfor-
mance of ADAN compared with various baseline
systems in the following paragraphs, we begin by

561



clarifying the bilingual resources used in all the
methods. Note first that in all of our experiments,
traditional features like bag of words cannot be
directly used because SOURCE and TARGET have
completely different vocabularies. Therefore, un-
less otherwise specified, BWEs are used as the in-
put representation for all systems to map words
from both SOURCE and TARGET into the same
feature space. (The only exceptions are the CLD-
based CLTC systems of Xu and Yang [2017] ex-
plained later in this section, which directly make
use of a parallel corpus instead of relying on
BWEs.) The same BWEs are adopted in all sys-
tems that utilize BWEs.

Train-on-SOURCE-only Baselines We start by
considering two baselines that train only on the
SOURCE language, English, and rely solely on the
BWEs to classify the TARGET. The first varia-
tion uses a standard supervised learning algorithm,
Logistic Regression (LR), shown in row 1 in
Table 1. In addition, we evaluate a non-adversarial
variation of ADAN, just the DAN portion of our
model (row 2), which is one of the modern neu-
ral models for sentiment classification. We can
see from Table 1 that, in comparison with ADAN
(bottom line), the train-on-SOURCE-only base-
lines perform poorly. This indicates that BWEs by
themselves do not suffice to transfer knowledge of
English sentiment classification to TARGET.

Domain Adaptation Baselines We next com-
pare ADAN with domain adaptation baselines,
because domain adaptation can be viewed as a
generalization of the cross-lingual task. Nonethe-
less, the divergence between languages is much
more significant than the divergence between two
domains, which are typically two product cat-
egories in practice. Among domain adaptation
methods, the widely used TCA (Pan et al., 2011)
did not work because it required quadratic space in
terms of the number of samples (650K). We thus
compare to mSDA (Chen et al., 2012), a very ef-
fective method for cross-domain sentiment classi-
fication on Amazon reviews. However, as shown
in Table 1 (row 3), mSDA did not perform com-
petitively. We speculate that this is because many
domain adaptation models including mSDA were
designed for the use of bag-of-words features,
which are ill-suited in our task where the two lan-
guages have completely different vocabularies. In
summary, this suggests that even strong domain

adaptation algorithms cannot be used out of the
box with BWEs for the CLSC task.

Machine Translation Baselines We then evalu-
ate ADAN against MT baselines (rows 4−5) that
(1) translate the TARGET text into English and
then (2) use the better of the train-on-SOURCE-
only models for sentiment classification. Previ-
ous studies (Banea et al., 2008; Salameh et al.,
2015) on sentiment classification for Arabic and
European languages claim this MT approach to
be very competitive and find that it can some-
times match the state-of-the-art system trained on
that language. For Chinese, where translated text
was not provided, we use the commercial Google
Translate engine,6 which is highly engineered,
trained on enormous resources, and arguably one
of the best MT systems currently available. As
shown in Table 1, our ADAN model substantially
outperforms the MT baseline on both languages,
indicating that our adversarial model can suc-
cessfully perform cross-lingual sentiment classi-
fication without any annotated data in the target
language.

Cross-Lingual Text Classification Baselines
Finally, we conclude ADAN’s effectiveness by
comparing against a state-of-the-art cross-lingual
text classification (CLTC) method (Xu and Yang,
2017), as sentiment classification is one type of
text classification. They propose a cross-lingual
distillation (CLD) method that makes use of soft
SOURCE predictions on a parallel corpus to train a
TARGET model (CLD-KCNN). They further pro-
pose an improved variant (CLDFA-KCNN) that
utilizes adversarial training to bridge the domain
gap between the labeled and unlabeled texts within
the source and the target language, similar to
the adversarial domain adaptation by Ganin et al.
(2016). In other words, CLDFA-KCNN consists
of three conceptual adaptation steps: (i) Domain
adaptation from source-language labeled texts to
source-language unlabeled texts using adversarial
training; (ii) cross-lingual adaptation using distil-
lation; and (iii) domain adaptation in the target
language from unlabeled texts to the test set. Note,
however, that Xu and Yang (2017) use adversarial
training for domain adaptation within a single lan-
guage vs. our work that uses adversarial training
directly for cross-lingual generalization.

6https://translate.google.com.

562

https://translate.google.com


102 103 104
Number of Labeled Chinese Reviews

36

38

40

42

44

46

48

50

52
Ac
cu
ra
cy

Unsupervised
DAN
DAN + MT
ADAN
Chn Only

Figure 2: ADAN performance and standard deviation
for Chinese in the semi-supervised setting when using
various amounts of labeled Chinese data.

As shown in Table 1, ADAN significantly
outperforms both variants of CLD-KCNN and
achieves a new state of the art performance, in-
dicating that our direct use of adversarial neural
nets for cross-lingual adaptation can be more ef-
fective than chaining three adaptation steps as in
CLDFA-KCNN. This is the case in spite of the
fact that ADAN does not explicitly separate lan-
guage variation from domain variation. In fact,
the monolingual data we use for the source and
target languages is indeed from different domains.
ADAN’s performance suggests that it could poten-
tially bridge the divergence introduced by both
sources of variation in one shot.

Supervised SOURCE Accuracy By way of
comparison, it is also instructive to compare
ADAN’s “transferred” accuracy on the TARGET
with its (supervised) performance on the SOURCE.
As noted in the caption of Table 1, ADAN
achieves 58.7% accuracy on English for the
5-class English-Chinese setting, and 75.6% for
the 3-class English-Arabic setting. The SOURCE
accuracy for the DAN baselines (rows 2 and 5) is
similar to the SOURCE accuracy of ADAN.

3.3 Analysis and Discussion
Because the Arabic data set is small, we choose
Chinese as an example for our further analysis.

3.3.1 Semi-Supervised Learning
In practice, it is usually not very difficult to obtain
at least a small amount of annotated data. ADAN
can be readily adapted to exploit such extra la-
beled data in the target language, by letting those
labeled instances pass through the sentiment clas-

sifier P as the English samples do during train-
ing. We simulate this semi-supervised scenario by
adding labeled Chinese reviews for training. We
start by adding 100 labeled reviews and keep dou-
bling the number until 12,800. As shown in Fig-
ure 2, when adding the same number of labeled
reviews, ADAN can better utilize the extra supervi-
sion and outperform the DAN baseline trained with
combined data, as well as the supervised DAN us-
ing only labeled Chinese reviews. The margin is
naturally decreasing as more supervision is incor-
porated, but ADAN is still superior when adding
12,800 labeled reviews. On the other hand, the
DAN with translation baseline seems unable to ef-
fectively utilize the added supervision in Chinese,
and the performance only starts to show a slightly
increasing trend when adding 6,400 or more la-
beled reviews. One possible reason is that when
adding to the training data a small number of En-
glish reviews translated from the labeled Chinese
data, the training signals they produce might be
lost in the vast number of English training sam-
ples, and thus not effective in improving perfor-
mance. Another potentially interesting find is that
it seems a very small amount of supervision (e.g.,
100 labels) could significantly help DAN. How-
ever, with the same number of labeled reviews,
ADAN still outperforms the DAN baseline.

3.3.2 Qualitative Analysis and Visualizations
To qualitatively demonstrate how ADAN bridges
the distributional discrepancies between English
and Chinese instances, t-SNE (Van der Maaten
and Hinton, 2008) visualizations of the activations
at various layers are shown in Figure 3. We ran-
domly select 1,000 reviews from the Chinese and
English validation sets, respectively, and plot the
t-SNE of the hidden node activations at three lo-
cations in our model: the averaging layer, the end
of the joint feature extractor, and the last hidden
layer in the sentiment classifier just prior to soft-
max. The train-on-English model is the DAN base-
line in Table 1. Note that there is actually only
one “branch” in this baseline model, but in order
to compare with ADAN, we conceptually treat the
first three layers as the feature extractor.

Figure 3a shows that BWEs alone do not suf-
fice to bridge the gap between the distributions
of the two languages. To shed more light on the
surprisingly clear separation given that individ-
ual words have a mixed distribution in both lan-
guages (not shown in figure), we first try to isolate

563



(a) Averaging Layer Outputs (b) Joint Hidden Features (c) Sentiment Branch Outputs
Tr

ai
n 

on
 E

ng
lis

h
2.

 A
D

AN

I have been here twice and both times have been great. They really have a nice service staff & very Attentive!
Food is pretty good as well! They seem to be always busy but super glad you are there with them. Well done!

, , , ,

Avg Hausdorff Dist = 0.24 Avg Hausdorff Dist = 0.98 Avg Hausdorff Dist = 0.25
Avg Hausdorff Dist = 0.24 Avg Hausdorff Dist = 0.22 Avg Hausdorff Dist = 0.08

Figure 3: t-SNE visualizations of activations at various layers for the train-on-SOURCE-only baseline model (top)
and ADAN (bottom). The distributions of the two languages are brought much closer in ADAN as they are repre-
sented deeper in the network (left to right) measured by the Averaged Hausdorff Distance (see text). The green
circles are two 5-star example reviews (shown below the figure) that illustrate how the distribution evolves (zoom
in for details).

the content divergence from the language diver-
gence. In particular, the English and Chinese
reviews are not translations of each other, and
in fact may even come from different domains.
Therefore, the separation could potentially come
from two sources: the content divergence between
the English and Chinese reviews, and the language
divergence of how words are used in the two lan-
guages. To control for content divergence, we tried
plotting (not shown in figure) the average word
embeddings of 1,000 random Chinese reviews
and their machine translations into English using
t-SNE, and surprisingly the clear separation was
still present. There are a few relatively short re-
views that reside close to their translations, but
the majority still form two language islands. (The
same trend persists when we switch to a differ-
ent set of pre-trained BWEs, and when we plot a
similar graph for English−Arabic.) When we re-
move stop words (the most frequent word types
in both languages), the two islands finally start

to become slightly closer with less-clean bound-
aries, but the separation remains clear. We think
this phenomenon is interesting, and a thorough in-
vestigation is out of the scope of this work. We hy-
pothesize that at least in certain distant language
pairs such as English−Chinese,7 the divergence
between languages may not only be determined by
word semantics, but also largely depends on how
words are used.

Furthermore, we can see in Figure 3b that
the distributional discrepancies between Chinese
and English are significantly reduced after pass-
ing through the joint feature extractor (F). The
learned features in ADAN bring the distributions
in the two languages dramatically closer compared
to the monolingually trained baseline. This is
shown via the Averaged Hausdorff Distance

7In a personal correspondence with Ahmed Elgohary, he
noted that he did not observe the same phenomenon between
English and French.

564



Model Random BilBOWA Zou et al.

DAN 21.66% 28.75% 29.11%
DAN+MT 37.78% 38.17% 39.66%
ADAN 34.44% 40.51% 42.95%

Table 2: Model performance on Chinese with various
BWE initializations.

(AHD; Shapiro and Blaschko, 2004), which mea-
sures the distance between two sets of points. The
AHD between the English and Chinese reviews is
provided for all subplots in Figure 3.

Finally, when looking at the last hidden layer
activations in the sentiment classifier of the base-
line model (Figure 3c), there are several notable
clusters of red dots (English data) that roughly cor-
respond to the class labels. These English clusters
are the areas where the classifier is the most con-
fident in making decisions. However, most Chi-
nese samples are not close to one of those clusters
because of the distributional divergence and may
thus cause degraded classification performance in
Chinese. On the other hand, the Chinese samples
are more in line with the English ones in ADAN,
which results in the accuracy boost over the base-
line model. In Figure 3, a pair of similar English
and Chinese 5-star reviews is highlighted to visu-
alize how the distribution evolves at various points
of the network. We can see in Figure 3c that the
highlighted Chinese review gets close to the “posi-
tive English cluster” in ADAN, whereas in the base-
line, it stays away from dense English clusters
where the sentiment classifier trained on English
data is not confident to make predictions.

3.3.3 Impact of Bilingual Word Embeddings

In this section we discuss the effect of the bilin-
gual word embeddings. We start by initializing the
systems with random word embeddings (WEs),
shown in Table 2. ADANwith random WEs outper-
forms the DAN and mSDA baselines using BWEs
and matches the performance of the LR+MT base-
line (Table 1), suggesting that ADAN successfully
extracts features that could be used for cross-
lingual classification tasks without any bitext.
This impressive result vindicates the power of ad-
versarial training to reduce the distance between
two complex distributions without any direct su-
pervision, which is also observed in other re-
cent works for different tasks (Zhang et al., 2017;
Lample et al., 2018).

Model Accuracy Run time

DAN 42.95% 0.127 (s/iter)
CNN 46.24% 0.554 (s/iter)
BiLSTM 44.55% 1.292 (s/iter)
BiLSTM + dot attn 46.41% 1.898 (s/iter)

Table 3: Performance and speed for various feature
extractor architectures on Chinese.

With the introduction of BWEs (columns 2 and
3), the performance of ADAN is further boosted.
Therefore, it seems the quality of the BWEs plays
an important role in CLSC. To investigate the
impact of the specific choice of BWEs, we also
trained 100d BilBOWA BWEs (Gouws et al.,
2015) using the UN parallel corpus for Chinese.
All systems achieve slightly lower performance
compared with the pre-trained BWEs from Zou
et al. (2013), yet ADAN still outperforms other
baseline methods (Table 2), demonstrating that
ADAN’s effectiveness is relatively robust with re-
spect to the choice of BWEs. We conjecture that
all systems show inferior results with BilBOWA,
because it does not require word alignments dur-
ing training as Zou et al. (2013) do. By only
training on a sentence-aligned corpus, BilBOWA
requires less resources and is much faster to train,
potentially at the expense of quality.

3.3.4 Feature Extractor Architectures

As mentioned in §2.1, the architecture of ADAN’s
feature extractor is not limited to a Deep Averag-
ing Network (DAN), and one can choose different
feature extractors to suit a particular task or data
set. While an extensive study of alternative archi-
tectures is beyond the scope of this work, in this
section we present a brief experiment illustrating
that our adversarial framework works well with
other F architectures. In particular, we consider
two popular choices: (i) a CNN (Kim, 2014) that
has a 1d convolutional layer followed by a single
fully-connected layer to extract a fixed-length vec-
tor; and (ii) a Bi-LSTM with two variants: one that
takes the average of the hidden outputs of each
token as the feature vector, and one with the dot
attention mechanism (Luong et al., 2015) that
learns a weighted linear combination of all hidden
outputs.

As shown in Table 3, ADAN’s performance can
be improved by adopting more sophisticated fea-
ture extractors, at the expense of slower running

565



k

lambda lambda

ADAN without Wasserstein Distance ADAN

Figure 4: A grid search on k and lambda for ADAN
(right) and the ADAN-GRL variant (left). Numbers in-
dicate the accuracy on the Chinese development set.

time. This demonstrates that ADAN’s language-
adversarial training framework can be successfully
used with other F choices.
3.3.5 ADAN Hyperparameter Stability
In this section, we show that the training of ADAN
is stable over a large set of hyperparameters, and
provides improved performance compared with
the standard ADAN-GRL.

To verify the superiority of ADAN, we conduct a
grid search over k and λ, which are the two hyper-
parameters shared by ADAN and ADAN-GRL. We
experiment with k ∈ {1, 2, 4, 8, 16}, and λ ∈
{0.00625, 0.0125, 0.025, 0.05, 0.1, 0.2, 0.4, 0.8}.
Figure 4 reports the accuracy on the Chinese dev
set for both ADAN variants, and shows higher
accuracy and greater stability over the Ganin
and Lempitsky (2015) variant. This suggests that
ADAN overcomes the well-known problem that
adversarial training is sensitive to hyperparameter
tuning.

3.4 Implementation Details
For all our experiments on both languages, the fea-
ture extractor F has three fully connected layers
with ReLU non-linearities, whereas both P and
Q have two. All hidden layers contain 900 hid-
den units. Batch normalization (Ioffe and Szegedy,
2015) is used in each hidden layer in P and Q.
F does not use batch normalization. F and P
are optimized jointly using Adam (Kingma and
Ba, 2015) with a learning rate of 0.0005. Q is
trained with another Adam optimizer with the
same learning rate. The weights of Q are clipped
to [−0.01, 0.01]. We train ADAN for 30 epochs
and use early stopping to select the best model
on the validation set. ADAN is implemented in
PyTorch (Paszke et al., 2017).

4 Related Work

Cross-lingual sentiment classification is moti-
vated by the lack of high-quality labeled data in
many non-English languages (Bel et al., 2003;
Mihalcea et al., 2007; Banea et al., 2008, 2010;
Soyer et al., 2015). For Chinese and Arabic in
particular, there are several representative works
(Wan, 2008, 2009; He et al., 2010; Lu et al., 2011;
Mohammad et al., 2016). Our work is compa-
rable to these in objective but very different in
method. The work by Wan uses MT to directly
convert English training data to Chinese; this is
one of our baselines. Lu et al. (2011) instead uses
labeled data from both languages to improve the
performance on both. Other papers make direct
use of a parallel corpus either to learn a bilingual
document representation (Zhou et al., 2016) or to
conduct cross-lingual distillation (Xu and Yang,
2017). Zhou et al. (2016) require the translation
of the entire English training set, which is pro-
hibitive for our setting, and ADAN outperforms Xu
and Yang (2017)’s approach in our experiments.

Domain adaptation tries to learn effective clas-
sifiers for which the training and test samples
are from different underlying distributions (Blitzer
et al., 2007; Pan et al., 2011; Glorot et al., 2011;
Chen et al., 2012; Liu et al., 2015). This can
be thought of as a generalization of cross-lingual
text classification. However, one main difference
is that, when applied to text classification tasks,
most of these domain adaptation work assumes
a common feature space such as a bag-of-words
representation, which is not available in the cross-
lingual setting. See Section 3.2 for experiments
on this. In addition, most works in domain adap-
tation evaluate on adapting product reviews across
domains (e.g., books to electronics), where the di-
vergence in distribution is less significant than that
between two languages.

Adversarial networks have enjoyed much suc-
cess in computer vision (Goodfellow et al., 2014;
Ganin et al., 2016). A series of work in image
generation has used architectures similar to ours,
by pitting a neural image generator against a dis-
criminator that learns to classify real vs. gen-
erated images (Goodfellow et al., 2014). More
relevant to this work, adversarial architectures
have produced the state-of-the-art in unsupervised
domain adaptation for image object recognition:
Ganin et al. (2016) train with many labeled source

566



images and unlabeled target images, similar
to our set-up. In addition, other recent work
(Arjovsky et al., 2017; Gulrajani et al., 2017) pro-
poses improved methods for training generative
adversarial nets. In a preliminary version of the
current work (Chen et al., 2016), we proposed
language-adversarial training, the first adversar-
ial neural net for cross-lingual NLP. As of the writ-
ing of this paper, there are several other recent
works that adopt adversarial training for cross-
lingual NLP tasks, such as cross-lingual text clas-
sification (Xu and Yang, 2017), cross-lingual word
embedding induction (Zhang et al., 2017; Lample
et al., 2018) and cross-lingual question similarity
reranking (Joty et al., 2017).

5 Conclusion and Future Work

In this paper, we presented ADAN, an adversar-
ial deep averaging network for cross-lingual senti-
ment classification. ADAN leverages the abundant
labeled resources from English to help sentiment
classification on other languages where little or no
annotated data exist. We validate ADAN’s effec-
tiveness by experiments on Chinese and Arabic
sentiment classification, where we have labeled
English data and only unlabeled data in the target
language. Experiments show that ADAN outper-
forms several baselines including domain adap-
tation models, a competitive MT baseline, and
state-of-the-art cross-lingual text classification
methods. We further show that even without any
bilingual resources, ADAN trained with randomly
initialized embeddings can still achieve encourag-
ing performance. In addition, we show that in the
presence of labeled data in the target language,
ADAN can naturally incorporate this additional
supervision and yields even more competitive
results.

For future work, we plan to apply our language-
adversarial training framework to other NLP adap-
tation tasks where explicit maximum likelihood
estimation training is not feasible because of
the lack of direct supervision. Our framework is
not limited to sentiment classification or even to
generic text classification: It can be applied, for
example, to phrase-level tagging tasks (İrsoy and
Cardie, 2014) where labeled data might not exist
for certain languages. In another direction, we can
look beyond a single SOURCE and TARGET lan-
guage and utilize our adversarial training frame-
work for multilingual text classification.

Acknowledgments

We thank Trevor Cohn, Lillian Lee, Cindy
Robinson, and the anonymous reviewers for their
invaluable help and feedback. We also thank mem-
bers of Cornell NLP and ML groups for helpful
comments. This work was funded in part by a
grant from the DARPA Deft Program.

References

Martin Arjovsky, Soumith Chintala, and Léon
Bottou. 2017. Wasserstein generative adversar-
ial networks. In Proceedings of the 34th Inter-
national Conference on Machine Learning,
volume 70, pages 214–223, Sydney, Australia.

Carmen Banea, Rada Mihalcea, and Janyce
Wiebe. 2010. Multilingual subjectivity: Are
more languages better? In Proceedings of the
23rd International Conference on Computa-
tional Linguistics (COLING 2010), pages 28–36,
Beijing, China.

Carmen Banea, Rada Mihalcea, Janyce Wiebe,
and Samer Hassan. 2008. Multilingual sub-
jectivity analysis using machine translation. In
Proceedings of the 2008 Conference on Empir-
ical Methods in Natural Language Processing,
pages 127–135, Honolulu, Hawaii.

Nuria Bel, Cornelis H. A. Koster, and Marta Villegas.
2003. Cross-lingual text categorization. In Re-
search and Advanced Technology for Digital
Libraries, pages 126–139, Berlin, Heidelberg.

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes
and blenders: Domain adaptation for sentiment
classification. In Proceedings of the 45th An-
nual Meeting of the Association of Compu-
tational Linguistics, pages 440–447, Prague,
Czech Republic.

Minmin Chen, Zhixiang Xu, Kilian Weinberger,
and Fei Sha. 2012. Marginalized denoising auto-
encoders for domain adaptation. In Proceed-
ings of the 29th International Conference on
Machine Learning (ICML-12), pages 767–774,
Edinburgh, Scotland, GB.

Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire
Cardie, and Kilian Weinberger. 2016. Ad-
versarial deep averaging networks for

567

http://proceedings.mlr.press/v70/arjovsky17a.html
http://proceedings.mlr.press/v70/arjovsky17a.html
http://aclweb.org/anthology/C10-1004
http://aclweb.org/anthology/C10-1004
http://aclweb.org/anthology/D08-1014
http://aclweb.org/anthology/D08-1014
https://doi.org/10.1007/978-3-540-45175-4_13
http://aclweb.org/anthology/P07-1056
http://aclweb.org/anthology/P07-1056
http://aclweb.org/anthology/P07-1056
http://icml.cc/2012/papers/416.pdf
http://icml.cc/2012/papers/416.pdf
https://arxiv.org/abs/1606.01614
https://arxiv.org/abs/1606.01614


cross-lingual sentiment classification. ArXiv
e-prints 1606.01614v4.

Yaroslav Ganin and Victor Lempitsky. 2015. Un-
supervised domain adaptation by backpropaga-
tion. In Proceedings of the 32nd International
Conference on Machine Learning, volume 37,
pages 1180–1189, Lille, France.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François
Laviolette, Mario Marchand, and Victor
Lempitsky. 2016. Domain-adversarial train-
ing of neural networks. Journal of Machine
Learning Research, 17(59):1–35.

Xavier Glorot, Antoine Bordes, and Yoshua
Bengio. 2011. Domain adaptation for large-
scale sentiment classification: A deep learn-
ing approach. In Proceedings of the 28th
International Conference on Machine Learn-
ing (ICML-11), pages 513–520, Bellevue,
Washington, USA.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi
Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio.
2014. Generative adversarial nets. In Advances
in Neural Information Processing Systems 27,
pages 2672–2680, Montreal, Canada.

Stephan Gouws, Yoshua Bengio, and Greg
Corrado. 2015. BilBOWA: Fast bilingual
distributed representations without word align-
ments. In Proceedings of the 32nd International
Conference on Machine Learning, volume 37,
pages 748–756, Lille, France.

Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky,
Vincent Dumoulin, and Aaron Courville. 2017.
Improved training of Wasserstein GANs. In Ad-
vances in Neural Information Processing Sys-
tems 30, pages 5767–5777, Long Beach, USA.

Yulan He, Harith Alani, and Deyu Zhou. 2010.
Exploring English lexicon knowledge for Chi-
nese sentiment analysis. In CIPS-SIGHAN
Joint Conference on Chinese Language Pro-
cessing, pages 121–128, Beijing, China.

Sergey Ioffe and Christian Szegedy. 2015. Batch
normalization: Accelerating deep network
training by reducing internal covariate shift.
In Proceedings of the 32nd International

Conference on Machine Learning, volume 37,
pages 448–456, Lille, France.

Ozan İrsoy and Claire Cardie. 2014. Opinion
mining with deep recurrent neural networks.
In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing,
pages 720–728, Doha, Qatar.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-
Graber, and Hal Daumé III. 2015. Deep un-
ordered composition rivals syntactic methods
for text classification. In Proceedings of the 53rd
Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing
(Volume 1: Long Papers), pages 1681–1691,
Beijing, China.

Shafiq Joty, Preslav Nakov, Lluís Màrquez, and
Israa Jaradat. 2017. Cross-language learning
with adversarial neural networks: Application
to community question answering. In Pro-
ceedings of the 21st Conference on Compu-
tational Natural Language Learning (CoNLL
2017), pages 226–237, Vancouver, Canada.

Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of
the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP),
pages 1746–1751, Doha, Qatar.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Interna-
tional Conference on Learning Representations,
San Diego, California.

Guillaume Lample, Alexis Conneau,
Marc’Aurelio Ranzato, Ludovic Denoyer,
and Hervé Jégou. 2018. Word translation
without parallel data. In International Confer-
ence on Learning Representations, Vancouver,
Canada.

Yiou Lin, Hang Lei, Jia Wu, and Xiaoyu Li. 2015.
An empirical study on sentiment classification
of chinese review using word embedding. In
Proceedings of the 29th Pacific Asia Confer-
ence on Language, Information and Computa-
tion: Posters, pages 258–266, Shanghai, China.

568

https://arxiv.org/abs/1606.01614
http://proceedings.mlr.press/v37/ganin15.html
http://proceedings.mlr.press/v37/ganin15.html
http://proceedings.mlr.press/v37/ganin15.html
http://jmlr.org/papers/v17/15-239.html
http://jmlr.org/papers/v17/15-239.html
http://www.icml-2011.org/papers/342_icmlpaper.pdf
http://www.icml-2011.org/papers/342_icmlpaper.pdf
http://www.icml-2011.org/papers/342_icmlpaper.pdf
http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf
http://proceedings.mlr.press/v37/gouws15.html
http://proceedings.mlr.press/v37/gouws15.html
http://proceedings.mlr.press/v37/gouws15.html
http://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans.pdf
http://aclweb.org/anthology/W10-4116
http://aclweb.org/anthology/W10-4116
http://proceedings.mlr.press/v37/ioffe15.html
http://proceedings.mlr.press/v37/ioffe15.html
http://proceedings.mlr.press/v37/ioffe15.html
http://aclweb.org/anthology/D14-1080
http://aclweb.org/anthology/D14-1080
https://doi.org/10.3115/v1/P15-1162
https://doi.org/10.3115/v1/P15-1162
https://doi.org/10.3115/v1/P15-1162
http://aclweb.org/anthology/K17-1024
http://aclweb.org/anthology/K17-1024
http://aclweb.org/anthology/K17-1024
https://doi.org/10.3115/v1/D14-1181
https://doi.org/10.3115/v1/D14-1181
https://arxiv.org/abs/1412.6980
https://arxiv.org/abs/1412.6980
https://openreview.net/forum?id=H196sainb
https://openreview.net/forum?id=H196sainb
http://aclweb.org/anthology/Y15-2030
http://aclweb.org/anthology/Y15-2030


Biao Liu, Minlie Huang, Jiashen Sun, and Xuan
Zhu. 2015. Incorporating domain and sen-
timent supervision in representation learning
for domain adaptation. In Proceedings of the
24th International Conference on Artificial In-
telligence, pages 1277–1283, Buenos Aires,
Argentina.

Bin Lu, Chenhao Tan, Claire Cardie, and
Benjamin K. Tsou. 2011. Joint bilingual
sentiment classification with unlabeled paral-
lel corpora. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 320–330, Portland, Oregon, USA.

Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to
attention-based neural machine translation. In
Proceedings of the 2015 Conference on Empir-
ical Methods in Natural Language Processing,
pages 1412–1421, Lisbon, Portugal.

Laurens Van der Maaten and Geoffrey Hinton.
2008. Visualizing data using t-SNE. Journal
of Machine Learning Research, 9:2579–2605.

Christopher D. Manning, Mihai Surdeanu, John
Bauer, Jenny Finkel, Steven J. Bethard, and
David McClosky. 2014. The Stanford CoreNLP
natural language processing toolkit. In Associ-
ation for Computational Linguistics (ACL) Sys-
tem Demonstrations, pages 55–60.

Rada Mihalcea, Carmen Banea, and Janyce
Wiebe. 2007. Learning multilingual subjective
language via cross-lingual projections. In
Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics,
pages 976–983, Prague, Czech Republic.

Saif M. Mohammad, Mohammad Salameh, and
Svetlana Kiritchenko. 2016. How translation
alters sentiment. Journal of Artificial Intelli-
gence Research, 55(1):95–130.

Sinno J. Pan, Ivor W. Tsang, James T. Kwok,
and Qiang Yang. 2011. Domain adaptation
via transfer component analysis. IEEE Transac-
tions on Neural Networks, 22(2):199–210.

Adam Paszke, Sam Gross, Soumith Chintala,
Gregory Chanan, Edward Yang, Zachary
DeVito, Zeming Lin, Alban Desmaison, Luca

Antiga, and Adam Lerer. 2017. Automatic dif-
ferentiation in PyTorch. In NIPS 2017 Autodiff
Workshop, Long Beach, USA.

Mohammad Salameh, Saif Mohammad, and
Svetlana Kiritchenko. 2015. Sentiment after
translation: A case-study on arabic social me-
dia posts. In Proceedings of the 2015 Con-
ference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies, pages 767–777,
Denver, Colorado.

Michael D Shapiro and Matthew B Blaschko.
2004. On hausdorff distance measures. Tech-
nical Report UM-CS-2004-071, University of
Massachusetts Amherst.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, D. Christopher Manning, Andrew Ng,
and Christopher Potts. 2013. Recursive deep
models for semantic compositionality over a
sentiment treebank. In Proceedings of the 2013
Conference on Empirical Methods in Natu-
ral Language Processing, pages 1631–1642,
Seattle, Washington, USA.

Hubert Soyer, Pontus Stenetorp, and Akiko
Aizawa. 2015. Leveraging monolingual data
for crosslingual compositional word representa-
tions. In International Conference on Learning
Representations, San Diego, California.

Sheng Kai Tavi, Richard Socher, and D. Christopher
Manning. 2015. Improved semantic represen-
tations from tree-structured long short-term
memory networks. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing
(Volume 1: Long Papers), pages 1556–1566,
Beijing, China.

Joseph Turian, Lev-Arie Ratinov, and Yoshua
Bengio. 2010. Word representations: A simple
and general method for semi-supervised learn-
ing. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguis-
tics, pages 384–394, Uppsala, Sweden.

Cédric Villani. 2008. Optimal transport: old and
new, volume 338. Springer Science & Business
Media.

569

http://dl.acm.org/citation.cfm?id=2832415.2832427
http://dl.acm.org/citation.cfm?id=2832415.2832427
http://dl.acm.org/citation.cfm?id=2832415.2832427
http://aclweb.org/anthology/P11-1033
http://aclweb.org/anthology/P11-1033
http://aclweb.org/anthology/P11-1033
https://doi.org/10.18653/v1/D15-1166
https://doi.org/10.18653/v1/D15-1166
http://www.jmlr.org/papers/volume9/vandermaaten08a/\vandermaaten08a.pdf
http://www.aclweb.org/anthology/P/P14/P14-5010
http://www.aclweb.org/anthology/P/P14/P14-5010
http://aclweb.org/anthology/P07-1123
http://aclweb.org/anthology/P07-1123
http://dl.acm.org/citation.cfm?id=3013558.3013562
http://dl.acm.org/citation.cfm?id=3013558.3013562
https://doi.org/10.1109/TNN.2010.2091281
https://doi.org/10.1109/TNN.2010.2091281
https://openreview.net/forum?id=BJJsrmfCZ
https://openreview.net/forum?id=BJJsrmfCZ
https://doi.org/10.3115/v1/N15-1078
https://doi.org/10.3115/v1/N15-1078
https://doi.org/10.3115/v1/N15-1078
https://web.cs.umass.edu/publication/docs/2004/UM-CS-2004-071.pdf
http://aclweb.org/anthology/D13-1170
http://aclweb.org/anthology/D13-1170
http://aclweb.org/anthology/D13-1170
https://arxiv.org/abs/1412.6334
https://arxiv.org/abs/1412.6334
https://arxiv.org/abs/1412.6334
https://doi.org/10.3115/v1/P15-1150
https://doi.org/10.3115/v1/P15-1150
https://doi.org/10.3115/v1/P15-1150
http://aclweb.org/anthology/P10-1040
http://aclweb.org/anthology/P10-1040
http://aclweb.org/anthology/P10-1040
https://www.springer.com/us/book/9783540710493
https://www.springer.com/us/book/9783540710493


Xiaojun Wan. 2008. Using bilingual knowl-
edge and ensemble techniques for unsupervised
chinese sentiment analysis. In Proceedings of
the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 553–561,
Honolulu, Hawaii.

Xiaojun Wan. 2009. Co-training for cross-lingual
sentiment classification. In Proceedings of the
Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Con-
ference on Natural Language Processing of the
AFNLP: Volume 1 - Volume 1, pages 235–243,
Suntec, Singapore.

Ruochen Xu and Yiming Yang. 2017. Cross-
lingual distillation for text classification. In
Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1415–1425,
Vancouver, Canada.

Meng Zhang, Yang Liu, Huanbo Luan, and
Maosong Sun. 2017. Adversarial training for
unsupervised bilingual lexicon induction. In
Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1959–1970,
Vancouver, Canada.

Xiang Zhang, Junbo Zhao, and Yann LeCun.
2015. Character-level convolutional networks for
text classification. In Advances in Neural Infor-
mation Processing Systems 28, pages 649–657,
Montreal, Canada.

Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao.
2016. Cross-lingual sentiment classification with
bilingual document representation learning. In
Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1403–1412, Berlin,
Germany.

Michał Ziemski, Marcin Junczys-Dowmunt, and
Bruno Pouliquen. 2016. The united nations par-
allel corpus. In Proceedings of the Tenth Inter-
national Conference on Language Resources and
Evaluation (LREC 2016), pages 3530–3534,
Portoroz̆, Slovenia.

Will Y. Zou, Richard Socher, Daniel Cer, and
Christopher D. Manning. 2013. Bilingual word
embeddings for phrase-based machine transla-
tion. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Pro-
cessing, pages 1393–1398, Seattle, Washington,
USA.

570

http://aclweb.org/anthology/D08-1058
http://aclweb.org/anthology/D08-1058
http://aclweb.org/anthology/D08-1058
http://dl.acm.org/citation.cfm?id=1687878.1687913
http://dl.acm.org/citation.cfm?id=1687878.1687913
https://doi.org/10.18653/v1/P17-1130
https://doi.org/10.18653/v1/P17-1130
http://aclweb.org/anthology/P17-1179
http://aclweb.org/anthology/P17-1179
http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf
http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf
https://doi.org/10.18653/v1/P16-1133
https://doi.org/10.18653/v1/P16-1133
https://conferences.unite.un.org/UNCorpus/Content/Doc/un.pdf
https://conferences.unite.un.org/UNCorpus/Content/Doc/un.pdf
http://www.aclweb.org/anthology/D13-1141
http://www.aclweb.org/anthology/D13-1141
http://www.aclweb.org/anthology/D13-1141

