



















































Label-Agnostic Sequence Labeling by Copying Nearest Neighbors


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5363–5369
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

5363

Label-Agnostic Sequence Labeling by Copying Nearest Neighbors

Sam Wiseman Karl Stratos
Toyota Technological Institute at Chicago

Chicago, IL, USA
{swiseman,stratos}@ttic.edu

Abstract

Retrieve-and-edit based approaches to struc-
tured prediction, where structures associated
with retrieved neighbors are edited to form
new structures, have recently attracted in-
creased interest. However, much recent work
merely conditions on retrieved structures (e.g.,
in a sequence-to-sequence framework), rather
than explicitly manipulating them. We show
we can perform accurate sequence labeling
by explicitly (and only) copying labels from
retrieved neighbors. Moreover, because this
copying is label-agnostic, we can achieve im-
pressive performance in zero-shot sequence-
labeling tasks. We additionally consider a dy-
namic programming approach to sequence la-
beling in the presence of retrieved neighbors,
which allows for controlling the number of
distinct (copied) segments used to form a pre-
diction, and leads to both more interpretable
and accurate predictions.

1 Introduction

Retrieve-and-edit style structured prediction,
where a model retrieves a set of labeled nearest
neighbors from the training data and conditions
on them to generate the target structure, is a
promising approach that has recently received
renewed interest (Hashimoto et al., 2018; Guu
et al., 2018; Gu et al., 2018; Weston et al., 2018).
This approach captures the intuition that while
generating a highly complex structure from
scratch may be difficult, editing a sufficiently
similar structure or set of structures may be easier.

Recent work in this area primarily uses the near-
est neighbors and their labels simply as an ad-
ditional context for a sequence-to-sequence style
model to condition on. While effective, these
models may not explicitly capture the discrete op-
erations (like copying) that allow for the neighbors
to be edited into the target structure, making inter-

preting the behavior of the model difficult. More-
over, since many retrieve-and-edit style models
condition on dataset-specific labels directly, they
may not easily allow for transfer learning and in
particular to porting a trained model to a new task
with different labels.

We address these limitations in the context of
sequence labeling by developing a simple label-
agnostic model that explicitly models copying
token-level labels from retrieved neighbors. Since
the model is not a function of the labels themselves
but only of a learned notion of similarity between
an input and retrieved neighbor inputs, it can be
effortlessly ported (zero shot) to a task with differ-
ent labels, without any retraining. Such a model
can also take advantage of recent advances in rep-
resentation learning, such as BERT (Devlin et al.,
2018), in defining this similarity.

We evaluate the proposed approach on stan-
dard sequence labeling tasks, and show it is com-
petitive with label-dependent approaches when
trained on the same data, but substantially outper-
forms strong baselines when it comes to zero-shot
transfer applications, such as when training with
coarse labels and testing with fine-grained labels.

Finally, we propose a dynamic programming
based approach to sequence labeling in the pres-
ence of retrieved neighbors, which allows for trad-
ing off token-level prediction confidence with try-
ing to minimize the number of distinct segments
in the overall prediction that are taken from neigh-
bors. We find that such an approach allows us to
both increase the interpretability of our predictions
as well as their accuracy.

2 Related Work

Nearest neighbor based structured prediction (also
referred to as instance- or memory-based learning)
has a long history in machine learning and NLP,



5364

  

Ms. Haag plays Elianti

The index fell 109.9 Monday

DT NN VBD CD NNP

SEC Mr. Lane

NNP NNP NNP

...

...

Mr. Ridley ‘s decision fires

NNP NNP POS NNP VBZ

NNP NNP VBZ NNP

...

The

DT

‘s

POS

...

Figure 1: A visualization of POS tagging an input
sentence x by copying token-labels from the label se-
quences y′(m) of M =3 retrieved sentences x′(m).

with early successes dating back at least to the
taggers of Daelemans (Daelemans, 1993; Daele-
mans et al., 1996) and the syntactic disambigua-
tion system of Cardie (1994). Similarly motivated
approaches remain popular for computer vision
tasks, especially when it is impractical to learn
a parametric labeling function (Shakhnarovich
et al., 2006; Schroff et al., 2015).

More recently, there has been renewed interest
in explicitly conditioning structured predictions on
retrieved neighbors, especially in the context of
language generation (Hashimoto et al., 2018; Guu
et al., 2018; Gu et al., 2018; Weston et al., 2018),
although much of this work uses neighbors as ex-
tra conditioning information within a sequence-
to-sequence framework (Sutskever et al., 2014),
rather than making discrete edits to neighbors in
forming new predictions.

Retrieval-based approaches to structured pre-
diction appear particularly compelling now with
the recent successes in contextualized word em-
bedding (McCann et al., 2017; Peters et al., 2018;
Radford et al.; Devlin et al., 2018), which should
allow for expressive representations of sentences
and phrases, which in turn allow for better retrieval
of neighbors for structured prediction.

Finally, we note that there is a long history of
transfer-learning based approaches to sequence la-
beling (Ando and Zhang, 2005; Daume III, 2007;
Schnabel and Schütze, 2014; Zirikly and Hagi-
wara, 2015; Peng and Dredze, 2016; Yang et al.,
2017; Rodriguez et al., 2018, inter alia), though
it is generally not zero-shot. There has, how-
ever, been recent work in zero-shot transfer for
sequence labeling problems with binary token-
labels (Rei and Søgaard, 2018).

3 Nearest Neighbor Based Labeling

While nearest-neighbor style approaches are com-
pelling for many structured prediction problems,
we will limit ourselves here to sequence-labeling
problems, such as part-of-speech (POS) tagging
or named-entity recognition (NER), where we are
given a T -length sequence x = x1:T (which we
will assume to be a sentence), and we must pre-
dict a corresponding T -length sequence of labels
ŷ = ŷ1:T for x. We will assume that for any given
task there are Z distinct labels, and denote x’s true
but unknown labeling as y= y1:T ∈{1, . . . , Z}T .

Sequence-labeling is particularly convenient for
nearest-neighbor based approaches, since a pre-
diction ŷ can be formed by simply concate-
nating labels extracted from the label-sequences
associated with neighbors. In particular, we
will assume we have access to a database
D = {x′(m), y′(m)}Mm=1 of M retrieved sentences
x′(m) and their corresponding true label-sequences
y′(m). We will predict a labeling ŷ for x by con-
sidering each token xt, selecting a labeled token
x
′(m)
k from D, and then setting ŷt = y

′(m)
k .

1

3.1 A Token-Level Model
We consider a very simple token-level model for
this label-agnostic copying, where the probability
that x’s t’th label yt is equal to y

′(m)
k — the k’th

label token of sequence x′(m) — simply depends
on the similarity between xt and x

′(m)
k , and is in-

dependent of the surrounding labels, conditioned
on x and D.2 In particular, we define

p(yt= y
′(m)
k |x,D) ∝ exp(x

T
t x
′(m)
k ), (1)

where the above probability is normalized over all
label tokens of all label-sequences in D. Above,
xt and x

′(m)
k (both in R

D) represent the contextual
word embeddings of the t’th token in x and the
k’th token in x′(m), respectively, as obtained by
running a deep sequence-model over x and over
x′(m). In all experiments we use BERT (Devlin
et al., 2018), a model based on the Transformer

1More precisely, we will set ŷt to be an instance of the
label type of which y′(m)k is a label token; this distinction
between label types and tokens can make the exposition un-
necessarily obscure, and so we avoid it when possible.

2While recent sequence labeling models (Ma and Hovy,
2016; Lample et al., 2016), often model inter-label depen-
dence with a first-order CRF (Lafferty et al., 2001), Devlin
et al. (2018) have recently shown that excellent performance
can be obtained by modeling labels as conditionally indepen-
dent given a sufficiently expressive representation of x.



5365

architecture (Vaswani et al., 2017), to obtain con-
textual word embeddings.

We fine-tune these contextual word embeddings
by maximizing a latent-variable style probabilistic
objective

T∑
t=1

ln

M∑
m=1

∑
k: y

′(m)
k = yt

p(yt = y
′(m)
k |x,D), (2)

where we sum over all individual label tokens in
D that match yt.

At test time, we predict ŷt to be
the label type with maximal marginal
probability. That is, we set ŷt to be
argmaxz

∑M
m=1

∑
k: y

′(m)
k =z

p(yt= y
′(m)
k |x,D),

where z ranges over the label types (e.g., POS or
named entity tags) present in D. As noted in the
introduction, predicting labels in this way allows
for the prediction of any label type present in the
database D used at test time, and so we can easily
predict label types unseen at training time without
any additional retraining.

4 Data and Methods

Our main experiments seek to determine both
whether the label-agnostic copy-based approach
introduced above results in competitive sequence-
labeling performance on standard metrics, as well
as whether this approach gives rise to better zero-
shot transfer. Accordingly, our first set of experi-
ments consider several standard sequence-labeling
tasks and datasets, namely, POS tagging the Penn
Treebank (Marcus et al., 1993) with both the stan-
dard Penn Treebank POS tags and Universal POS
tags (Petrov et al., 2012; Nivre et al., 2016), and
the CoNLL 2003 NER task (Sang and Buchholz,
2000; Sang and De Meulder, 2003). We com-
pare with the sequence-labeling performance of
BERT (Devlin et al., 2018), which we take to be
near state of the art. We use the standard dataset-
splits and evaluations for all tasks, and BIO encod-
ing for all segment-level tagging tasks.

We evaluate zero-shot transfer performance by
training on one dataset and evaluating on another,
without any retraining. In particular, we con-
sider three zero-shot transfer scenarios: training
with Universal POS Tags on the Penn Treebank
and then predicting the standard, fine-grained POS
tags, training on the CoNLL 2003 NER data and
predicting on the fine-grained OntoNotes NER
data (Hovy et al., 2006) using the setup of Strubell

et al. (2017), and finally training on the CoNLL
2003 chunking data and predicting on the CoNLL
2003 NER data. We again compare with a BERT
baseline, where labels from the original task are
deterministically mapped to the most frequent la-
bel on the new task with which they coincide.3

Our nearest-neighbor based models were fine-
tuned by retrieving the 50 nearest neighbors of
each sentence in a mini-batch of either size 16 or
20, and maximizing the objective (2) above. For
training, nearest neighbors were determined based
on cosine-similarity between the averaged top-
level (non-fine-tuned) BERT token embeddings of
each sentence. In order to make training more effi-
cient, gradients were calculated only with respect
to the input sentence embeddings (i.e., the xt in
(1)) and not the embeddings x′(m)k of the tokens in
D. At test time, 100 nearest neighbors were re-
trieved for each sentence to be labeled using the
fine-tuned embeddings.

The baseline BERT models were fine-tuned us-
ing the publicly available huggingface BERT
implementation,4 and the “base” weights made
available by the BERT authors (Devlin et al.,
2018). We made word-level predictions based on
the embedding of the first tokenized word-piece
associated with a word (as Devlin et al. (2018)
do), and ADAM (Kingma and Ba, 2014) was
used to fine-tune all models. Hyperparameters
were chosen using a random search over learning
rate, batch size, and number of epochs. Code for
duplicating all models and experiments is avail-
able at https://github.com/swiseman/
neighbor-tagging.

5 Main Results

The results of our experiments on standard se-
quence labeling tasks are in Table 1. We first note
that all results are quite good, and are competi-
tive with the state of the art. The label-agnostic
model tends to underperform the standard fine-
tuned BERT model only very slightly, though con-
sistently, and is typically within several tenths of a
point in performance.

The results of our zero-shot transfer experi-
ments are in Table 2. We see that in all cases the
label-agnostic model outperforms standard fine-

3For the Chunk → NER task, this results in mapping all
tags to ‘O’, so we instead use the more favorable mapping of
NPs to PERSON tags.

4https://github.com/huggingface/
pytorch-pretrained-BERT

https://github.com/swiseman/neighbor-tagging
https://github.com/swiseman/neighbor-tagging
https://github.com/huggingface/pytorch-pretrained-BERT
https://github.com/huggingface/pytorch-pretrained-BERT


5366

NER Dev. F1 Test F1

BERT 95.14 90.76
NN 94.48 89.94

POS Dev. Acc. Test Acc.

BERT 97.56 97.91
NN 97.33 97.64

U-POS Dev. Acc. Test Acc.

BERT 98.34 98.62
NN 98.08 98.36

Table 1: Performance of fine-tuned BERT and nearest-
neighbor based labeling (NN) on NER, POS tagging,
and universal POS tagging; see text. BERT numbers
are from fine-tuning the huggingface implementa-
tion, and differ slightly from Devlin et al. (2018).

tuned BERT, often significantly. In particular, we
note that when going from universal POS tags to
standard POS tags, the fine-tuned label-agnostic
model manages to outperform the standard most-
frequent-tag-per-word baseline, which itself ob-
tains slightly less than 92% accuracy. The most
dramatic increase in performance, of course, oc-
curs on the Chunking to NER task, where the
label-agnostic model is successfully able to use
chunking-based training information in copying
labels, whereas the parametric fine-tuned BERT
model can at best attempt to map NP-chunks to
PERSON labels (the most frequent named entity
in the dataset).

In order to check that the increase in perfor-
mance is not due only to the BERT representations
themselves, Table 2 also shows the results of near-
est neighbor based prediction without fine-tuning
(“NN (no FT)” in the table) on any task. In all
cases, this leads to a decrease in performance.

6 Encouraging Contiguous Copies

Although we model token-level label copying, at
test time each ŷt is predicted by selecting the la-
bel type with highest marginal probability, with-
out any attempt to ensure that the resulting se-
quence ŷ resembles one or a few of the labeled
neighbors y′(m). In this section we therefore con-
sider a decoding approach that allows for control-
ling the trade-off between prediction confidence
and minimizing the number of distinct segments
in ŷ that represent direct (segment-level) copies
from some neighbor, in the hope that having fewer

CoNLL→ Onto NER Dev. F1 Test F1
BERT 58.41 58.05
NN 62.17 62.33
NN (no FT) 54.29 55.35

U-POS→ POS Dev. Acc. Test Acc.

BERT 61.78 59.86
NN 96.70 96.98
NN (no FT) 87.44 87.13

Chunk→ NER Dev. F1 Test F1
BERT 9.55 8.03
NN 78.05 71.74
NN (no FT) 75.21 67.19

Table 2: Zero-shot performance of models trained on
CoNLL NER and applied to fine-grained OntoNotes
NER, with universal POS tags and applied to standard
POS tagging, and on CoNLL chunking and applied to
CoNLL NER. “NN (no FT)” indicates BERT was not
fine tuned even on the original task.

distinct copied segments in our predictions might
make them more interpretable or accurate. We em-
phasize that the following decoding approach is in
fact applicable even to standard sequence labeling
models (i.e., non-nearest-neighbor based models),
as long as neighbors can be retrieved at test time.

To begin with a simple case, suppose we already
know the true labels y for a sequence x, and are
simply interested in being able to reconstruct y by
concatenating as few segments y′i:j that appear in
some y′(m) ∈D as possible. More precisely, de-
fine the set ZD to contain all the unique label type
sequences appearing as a subsequence of some se-
quence y′(m) ∈D. Then, if we’re willing to toler-
ate some errors in reconstructing y, we can use a
dynamic program to minimize the number of mis-
labelings in our now “prediction” ŷ, plus the num-
ber of distinct segments used in forming ŷ multi-
plied by a constant c, as follows:

J(t) = min
1≤k≤t

z∈ZD:|z|=k

J(t−k) + c+
k∑

j=1

1[yt−k+j 6= zj ],

where J(0)= 0 is the base case and |z| is the
length of sequence z. Note that greedily selecting
sequences that minimize mislabelings may result
in using more segments, and thus a higher J .

In the case where we do not already know y, but
wish to predict it, we might consider a modifica-
tion of the above, which tries to minimize c times



5367

  

NATO’s top military men – General George Joulwan ...

1986  –  Bishop  Desmond  Tutu  was  enthroned  as  Archbishop  of  Cape  Town  ,  South  Africa  . 
  O     O        O            B-PER       I-PER    O             O            O             O             O   B-LOC   I-LOC  O  B-LOC    I-LOC  O    

Phan’s accidental journey started last week in Prince Rupert , British Columbia , ...

  

… tampering and off-spinner Muttiah Muralitharan was called for throwing ...

1986  –  Bishop  Desmond  Tutu  was  enthroned  as  Archbishop  of  Cape  Town  ,  South  Africa  . 
  O     O        O            B-PER       I-PER    O             O            O             O             O   B-LOC   I-LOC  O  B-LOC    I-LOC  O    

… set up under a 1993 interim peace deal to control  parts of the Gaza Strip and West Bank , ...

Figure 2: A CoNLL NER development example, which can be labeled with only two distinct segments. We show
those used by a model trained on the NER data (top), and on chunking data and transferred zero-shot (bottom).

the number of distinct segments used in forming ŷ
plus the expected number of mislabelings:

J(t) = min
1≤k≤t

z∈ZD:|z|=k

[
J(t−k) + c

+

k∑
j=1

1− p(yt−k+j = zj |x,D)
]
,

where we have used the linearity of expecta-
tion. Note that to use such a dynamic pro-
gram to predict ŷ we only need an estimate of
p(yt−k+j = zj |x,D), which we can obtain as in
Section 3 (or from a more conventional model).

In Figure 3 we plot both the F1 score and the
average number of distinct segments used in pre-
dicting each ŷ against the c parameter from the dy-
namic program above, for the CoNLL 2003 NER
development data in both the standard and zero-
shot settings. First we note that we are able to
obtain excellent performance with only about 1.5
distinct segments per prediction, on average; see
Figure 2 for examples. Interestingly, we also find
that using a higher c (leading to fewer distinct seg-
ments) can in fact improve performance. Indeed,
taking the best values of c from Figure 3 (0.4 in
the standard setting and 0.5 in the zero-shot set-
ting), we are able to improve our performance on
the test set from 89.94 to 90.20 in the standard set-
ting and from 71.74 to 73.61 in the zero shot set-
ting, respectively; see Tables 1 and 2.

7 Conclusion

We have proposed a simple label-agnostic
sequence-labeling model, which performs nearly
as well as a standard sequence labeler, but im-
proves on zero-shot transfer tasks. We have also

94.60

94.65

94.70

94.75

94.80

F1

NER Dev. Performance

0.1 0.2 0.3 0.4 0.5 0.6

c

1.500

1.505

1.510

1.515

1.520

se
gm

en
ts

/s
en

te
nc

e

78.4

78.6

78.8

79.0

79.2

79.4

F1

Chunk NER Dev. Performance

0.1 0.2 0.3 0.4 0.5 0.6

c

1.40

1.45

1.50

1.55

se
gm

en
ts

/s
en

te
nc

e

Figure 3: F1 performance and the average number of
distinct segments per predicted labeling on the CoNLL
NER development data as c is varied, when training ei-
ther (top) on the standard training set or (bottom) on the
CoNLL chunking data (i.e., zero-shot performance).

proposed an approach to sequence label predic-
tion in the presence of retrieved neighbors, which
allows for discouraging the use of many distinct
segments in a labeling. Future work will consider
problems where more challenging forms of neigh-
bor manipulation are necessary for prediction.



5368

References
Rie Kubota Ando and Tong Zhang. 2005. A framework

for learning predictive structures from multiple tasks
and unlabeled data. Journal of Machine Learning
Research, 6(Nov):1817–1853.

Claire Cardie. 1994. Domain-specific knowledge ac-
quisition for conceptual sentence analysis. Com-
puter Science Department Faculty Publication Se-
ries, page 60.

Walter Daelemans. 1993. Memory-based lexical ac-
quisition and processing. In Workshop on Machine
Translation and Lexicon, pages 85–98. Springer.

Walter Daelemans, Jakob Zavrel, Peter Berck, and
Steven Gillis. 1996. Mbt: A memory-based part
of speech tagger-generator. In Fourth Workshop on
Very Large Corpora.

Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256–263.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Jiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-
tor OK Li. 2018. Search engine guided neural ma-
chine translation. In Thirty-Second AAAI Confer-
ence on Artificial Intelligence.

Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren,
and Percy Liang. 2018. Generating sentences by
editing prototypes. Transactions of the Association
of Computational Linguistics, 6:437–450.

Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren,
and Percy S Liang. 2018. A retrieve-and-edit frame-
work for predicting structured outputs. In Advances
in Neural Information Processing Systems, pages
10073–10083.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90\% solution. In Proceedings of the hu-
man language technology conference of the NAACL,
Companion Volume: Short Papers.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning (ICML
2001), pages 282–289.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL-HLT, pages 260–270.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1064–1074.

Mitchell P Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2).

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In Advances in Neural In-
formation Processing Systems, pages 6294–6305.

Joakim Nivre, Marie-Catherine De Marneffe, Filip
Ginter, Yoav Goldberg, Jan Hajic, Christopher D
Manning, Ryan T McDonald, Slav Petrov, Sampo
Pyysalo, Natalia Silveira, et al. 2016. Universal de-
pendencies v1: A multilingual treebank collection.
In LREC.

Nanyun Peng and Mark Dredze. 2016. Improving
named entity recognition for chinese social media
with word segmentation representation learning. In
The 54th Annual Meeting of the Association for
Computational Linguistics, page 149.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), volume 1,
pages 2227–2237.

Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings
of the Eighth International Conference on Language
Resources and Evaluation (LREC-2012).

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. Improving language understanding
by generative pre-training.

Marek Rei and Anders Søgaard. 2018. Zero-shot se-
quence labeling: Transferring knowledge from sen-
tences to tokens. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
293–302.

Juan Diego Rodriguez, Adam Caldwell, and Alexander
Liu. 2018. Transfer learning for entity recognition
of novel classes. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 1974–1985.

Erik F Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task chunk-
ing. In Fourth Conference on Computational Nat-
ural Language Learning and the Second Learning
Language in Logic Workshop.



5369

Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the Seventh Conference on Natural
Language Learning at HLT-NAACL 2003.

Tobias Schnabel and Hinrich Schütze. 2014. Flors:
Fast and simple domain adaptation for part-of-
speech tagging. Transactions of the Association for
Computational Linguistics, 2:15–26.

Florian Schroff, Dmitry Kalenichenko, and James
Philbin. 2015. Facenet: A unified embedding for
face recognition and clustering. In Proceedings of
the IEEE conference on computer vision and pattern
recognition, pages 815–823.

Gregory Shakhnarovich, Trevor Darrell, and Piotr In-
dyk. 2006. Nearest-neighbor methods in learning
and vision: theory and practice (neural information
processing). The MIT press.

Emma Strubell, Patrick Verga, David Belanger, and
Andrew McCallum. 2017. Fast and accurate en-
tity recognition with iterated dilated convolutions.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2670–2680.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Jason Weston, Emily Dinan, and Alexander H Miller.
2018. Retrieve and refine: Improved sequence
generation models for dialogue. arXiv preprint
arXiv:1808.04776.

Zhilin Yang, Ruslan Salakhutdinov, and William W
Cohen. 2017. Transfer learning for sequence tag-
ging with hierarchical recurrent networks. In ICLR.

Ayah Zirikly and Masato Hagiwara. 2015. Cross-
lingual transfer of named entity recognizers without
parallel corpora. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 2: Short
Papers), volume 2, pages 390–396.


