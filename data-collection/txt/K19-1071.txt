



















































Slot Tagging for Task Oriented Spoken Language Understanding in Human-to-Human Conversation Scenarios


Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 757–767
Hong Kong, China, November 3-4, 2019. c©2019 Association for Computational Linguistics

757

Slot Tagging for Task Oriented Spoken Language Understanding in
Human-to-human Conversation Scenarios

Kunho Kim†, Rahul Jha†, Kyle Williams†, Alex Marin†, Imed Zitouni‡∗
†Microsoft Corporation, Redmond, WA, USA

‡Google, Mountain View, CA, USA
{kuki, rajh, kywillia, alemari}@microsoft.com, izitouni@google.com

Abstract

Task oriented language understanding (LU) in
human-to-machine (H2M) conversations has
been extensively studied for personal digital
assistants. In this work, we extend the task ori-
ented LU problem to human-to-human (H2H)
conversations, focusing on the slot tagging
task. Recent advances on LU in H2M con-
versations have shown accuracy improvements
by adding encoded knowledge from different
sources. Inspired by this, we explore sev-
eral variants of a bidirectional LSTM architec-
ture that relies on different knowledge sources,
such as Web data, search engine click logs, ex-
pert feedback from H2M models, as well as
previous utterances in the conversation. We
also propose ensemble techniques that aggre-
gate these different knowledge sources into a
single model. Experimental evaluation on a
four-turn Twitter dataset in the restaurant and
music domains shows improvements in the
slot tagging F1-score of up to 6.09% compared
to existing approaches.

1 Introduction

Spoken Language Understanding (SLU) is the first
component in digital assistants geared towards
task completion, such as Amazon Alexa or Mi-
crosoft Cortana. The input to an SLU compo-
nent is a natural language utterance from the user
and its output is a structured representation that
can be used by the downstream dialog components
to select the next action. The structured repre-
sentation used by most standard dialog agents is
a semantic frame consisting of domains, intents
and slots (Tur and De Mori, 2011). For example,
the structured representation of “Find me a cheap
Italian restaurant” is the domain Restaurant,
the intent find place, and slots [cheap]price range,

∗Work done while the author was at Microsoft Corpora-
tion

Figure 1: Example of language understanding for task
completion on a H2H conversation. In this work, our
goal is to identify useful slots (marked with red rectan-
gles).

[Italian]cuisine, [restaurant]place type. Different
sub-tasks within SLU have been extensively stud-
ied for human-to-machine (H2M) task completion
scenarios (Sarikaya et al., 2016).

We extend the task oriented SLU problem to
human-to-human (H2H) conversations. A digi-
tal assistant can listen to the conversation between
two or more humans and provide relevant informa-
tion or suggest actions based on the structured rep-
resentation captured with SLU. Figure 1 shows an
example of capturing intents and slots expressed
implicitly during a conversation between two hu-
mans. The digital assistant can show general in-
formation about the restaurant Mua, and provide
the opening hours based on the captured structured
representation. These types of H2H task comple-
tion scenarios may allow digital assistants to sug-
gest useful information to users in advance with-
out them needing to explicitly ask questions.

In this paper, we investigate SLU oriented to-



758

wards task completion for H2H scenarios with a
specific focus on solving the slot tagging task.
Some early conceptual ideas on this problem were
presented in DARPA projects on developing cog-
nitive assistants, such as CALO1 and RADAR2.
This work can be seen as an effort to formalize the
problem and propose a practical framework.

SLU for task completion in H2H conversations
is a challenging problem. Firstly, since the prob-
lem has not been studied before, there are no ex-
isting datasets to use. Therefore, we built a multi-
turn dataset for two H2H domains that we found to
be prevalent in Twitter conversations: Music and
Restaurants. The dataset is described in more de-
tail in Section 4. Secondly, the task is harder than
H2M conversations in several aspects. It is hard
to identify the semantics of noisy H2H conversa-
tion text with slang and abbreviations, and such
conversations have no explicit commands toward
the digital assistants requiring the assistant to in-
directly infer users intent.

In this work, we introduce a modular architec-
ture with a core bi-directional LSTM network, and
additional network components that utilize knowl-
edge from multiple sources including: sentence
embeddings to encode semantics and intents of
noisy texts with web-data and click logs, H2M
based expert feedback, and contextual models re-
lying on previous turns in the conversation. The
idea of adding components is inspired from some
recent advances in H2M SLU that use additional
encoded information (Chen et al., 2016; Su et al.,
2018; Kim et al., 2017; Jha et al., 2018). How-
ever, these work only considered adding a compo-
nent from a single knowledge resource. Further-
more, since these additional components bring in
information from different perspectives, we also
experimented with deep learning based ensemble
methods. Our best ensemble method outperforms
existing methods by 6.09% for the Music domain
and 2.62% for the Restaurant domain.

In summary, this paper makes the following
contributions:

• A practical framework on slot tagging for
task oriented SLU on H2H conversations us-
ing bidirectional LSTM architecture.

• Extension of the LSTM architecture utilizing
knowledge from external sources (e.g. Web

1https://en.wikipedia.org/wiki/CALO
2https://www.cmu.edu/cmnews/extra/

030718_darpa.html

data, click logs, H2M expert feedback, and
pervious sentences) with deep learning based
ensemble methods

• Newly developed dataset for evaluating task
oriented LU on H2H conversations

We begin by describing our methods for H2H
slot tagging in Section 3. We then describe the
data used in our experiments in Section 4 and dis-
cuss results in Section 5. This is followed by a
review of the related work and conclusion.

2 Related Work

LU involves domain classification, intent classifi-
cation, and slot tagging (Tur and De Mori, 2011;
Sarikaya et al., 2016). Recently various deep neu-
ral network (DNN) models have been studied to
solve each of these task, such as deep belief net-
work (Sarikaya et al., 2011), deep convex network
(Deng et al., 2012), RNN and LSTM (Ravuri and
Stolcke, 2015; Mesnil et al., 2015).

Recent advances in LU use additional encoded
information to improve DNN based models. There
have been some attempts to use data or models
from existing domains. One direction is to do
transfer learning. Kim et al. (2017) and Jha et al.
(2018) utilized previously trained models relevant
to the target domain as expert models. They use
the output of expert models as additional input to
add relevant knowledge while training for the tar-
get domain. Goyal et al. (2018) reused low-level
features from previously trained models and only
retrained high level layers to adapt to a new do-
main.

There have also been some attempts to use con-
textual information. Xu and Sarikaya (2014) used
past predictions of domains and intents in the pre-
vious turn for predicting current utterance. Chen
et al. (2016) expanded upon this work by using a
set of past utterances utilizing a memory network
(Sukhbaatar et al., 2015) with an attention model.
Subsequent works attempted to use the order and
time information. Bapna et al. (2017) addition-
ally used the chronological order of previous sen-
tences, and Su et al. (2018) used time decaying
functions to add temporal information.

Our work trains a sentence embedding that en-
codes the semantics and intents. DSSM and its
variants (Huang et al., 2013; Shen et al., 2014;
Palangi et al., 2016) are used for training sentence
embedding, which were originally used for finding

https://en.wikipedia.org/wiki/CALO
https://www.cmu.edu/cmnews/extra/030718_darpa.html
https://www.cmu.edu/cmnews/extra/030718_darpa.html


759

Feed 
Foward

Word-level
Bidirectional LSTM

w1 w2 wn

e11
c

e12
c e13

c
e21

c e22
c e2m

c en1
c en2

c enm
c

l1

Feed 
Foward

l2

Feed 
Foward

ln

Character-level 
Bidirectional LSTM

Character-level 
Bidirectional LSTM

Character-level 
Bidirectional LSTM

Ds

Dc

Sentence
Embedding

Contextual
Encoding

H2M
Expert

vs

vc

ve

u
Global Context Vector

Soft-
max

Dot

Dot

Dot
De

v's

v'c

v'e

h1 h2 hn

Additional Models 

Baseline Model

e1
w f1

c+b1
c e2

w f2
c+b2

c fn
c+bn

cen
w

k1 k2 kn

Predicted
Slots

Char 
embedding

Word
 embedding

Utterance

Figure 2: Overview of our slot tagging architecture. Our architecture consists with the core network (Section 3.1)
and additional network components utilizing knowledge from multiple sources (Each discussed in Section 3.2.1,
3.2.2, 3.2.3). A network ensembling approach is applied on additional components (Section 3.3), figure shows
with the attention mechanism.

relevance between the query and retrieved docu-
ments in a search engine. Also there have been at-
tempts to use sentence embeddings similar to our
data (Twitter). Dhingra et al. (2016) trained an em-
bedding for predicting hash tags of a tweet using
RNNs, Vosoughi et al. (2016) used an encoder-
decoder model for sentiment classification.

All of the previous methods have studied LU
components for task completion in H2M con-
versations. On the other hand, prior work on
LU on H2H conversations has focused on dia-
log state detection and tracking for spoken dia-
log systems. Shi et al. (2017) used CNN model,
and later extended multiple channel model for a
cross-language scenario (Shi et al., 2016). Jang
et al. (2018) used attention mechanism to focus
on words with meaningful context, and Su et al.
(2018) used a time decay model to incorporate
temporal information.

3 Methods

Figure 2 shows the overview of our slot tag-
ging architecture. Our modular architecture is a
core LSTM-based network and additional network
components that encode knowledge from multiple
sources. Slot prediction is done with the final feed
forward layer, whose input is the composition of
the output of the core network and the additional
components. We first describe our core network
and then the additional network components, fol-
lowed by our network ensembling approach.

3.1 Core Network
Our core network is a bidirectional model simi-
lar to Lample et al. (2016). The first character-
level bidirectional LSTM layer extracts the encod-
ing from a sequence of characters from each word.
Each character c is represented with a character
embedding ec ∈ R25, and the sequence of the em-
bedding is used as the input. The layer outputs

f c = LSTMforward(e
c) (1)

bc = LSTMbackward(e
c) (2)

for each character, where f c, bc ∈ R25.
The second word-level bidirectional LSTM

layer extracts the encoding from a sequence of
words for each sentence. For each word wi, the
input of the layer is gi = f ci ⊕ bci ⊕ ewi where f ci
and bci is the output of previous layer, e

w
i ∈ R100

is the word embedding vector, and ⊕ is a con-
catenation operator of vectors. We use pre-trained
GloVe with 2B tweets 3 (Pennington et al., 2014)
for the word embedding. The forward and back-
ward word-level LSTM’s produce

fwi = LSTMforward(gi) (3)

bwi = LSTMbackward(gi) (4)

where fwi , b
w
i ∈ R100. Finally, slot li is predicted

with the last feed forward layer with the input hi =
fwi ⊕ bwi .

3Downloaded from https://nlp.stanford.edu/
projects/glove/

https://nlp.stanford.edu/projects/glove/
https://nlp.stanford.edu/projects/glove/


760

Our model is trained using stochastic gradient
descent with Adam optimizer (Kingma and Ba,
2015), with the mini batch size 64 and the learning
rate 0.7×10−3. We also apply dropout (Srivastava
et al., 2014) on embeddings and other layers to
avoid overfitting. The learning rate and dropout ra-
tio were optimized using random search (Bergstra
and Bengio, 2012). The core network can be used
alone for slot tagging, however we discuss our ad-
ditional network components in the following sec-
tions for improving our architecture.

3.2 Additional Network Components

In this section, we discuss additional network
components that encode knowledge from different
sources. Encoded vectors are used as additional
input to the feed forward layer as shown in Fig-
ure 2.

3.2.1 Sentence Embedding for H2H
Conversations

Texts from H2H conversations are noisy and con-
tain slang and abbreviations, which can make
identifying their semantics challengins. In addi-
tion, it can be challenging to infer their intents
since there are no explicit commands toward the
digital assistants. The upper part of Figure 3
shows part of a conversation from Twitter. The
sentence lacks the semantics needed to fully un-
derstand ”club and country”. However, if we fol-
low the URL in the original text, we can get addi-
tional information to assist with the understand-
ing. For instance, the figure shows texts found
from two sources, 1) web page title of the URL
in the tweet and 2) web search engine queries
that lead to the URL in the tweet. We use web
search queries and click logs from a major com-
mercial Web search engines to find queries that
lead to clicks on the URL. Using this informa-
tion, we can infer from the Web page title that
the ”club and country” referred to in the tweet are
Atletico Madrid and Nigeria. Furthermore, the
search queries from the search engine logs indi-
cates possible user intents.

In our approach, we encode knowledge found
from these two sources based on the URL. In
our dataset, we were able to gather 2.35M pairs
of tweet text with URL and web search engine
queries that lead to the same URL, and 420K pairs
of tweet text and web page titles of the URL. We
then use this information to train a sentence em-
bedding model that can be used to encode the

semantics and implicit intents of each H2H con-
versation sentence. Our approach is to train a
model that projects texts from H2H conversation
and texts from each knowledge sources into a
same embedding space, keeping the correspond-
ing text pairs close to each other with other non-
relevant texts being apart, as shown in Figure 3.
The learned embedding model F then can be used
to represent any texts from H2H sentences with a
vector with semantically similar texts (or similar
intents) being projected close to each other in the
embedding space. Embeddings are used as addi-
tional component of our modular architecture, so
that the semantic and intent information can be uti-
lized in our slot tagging model.

We use the deep structured semantic model
(DSSM) architecture (Huang et al., 2013) to train
the sentence embedding encoder. DSSM uses
letter-trigram word hashing, so it is capable of
partially matching noisy spoken words so that we
can get more robust sentence embeddings for H2H
conversations. Let S be the set of sentences from
the H2H conversations that have the URL. For
each sentence s ∈ S, we find corresponding texts
(web page title of the URL, web search engine
queries to the URL) T+s and randomly choose
non-related texts T−s from corresponding texts of
other sentences (in other words, from different
URLs). Like the original DSSM model, each sen-
tence s, t+s ∈ T+s , and t−s ∈ T−s are initially en-
coded with letter-trigram word hashing vector x,
and used as the input of two consecutive dense lay-
ers,

x′ =W1x+ b1 (5)

y =W2x
′ + b2 (6)

where x′ ∈ R1000 and y ∈ R300. We train the
model to favor choosing t+s ∈ T+s over t−s ∈ T−s
for each s. So the loss function is defined as mini-
mizing the likelihood,

loss = −log
∏
s,T+s

P (T+s |s) (7)

P (T+s |s) =
∏

s,t+s ∈T+s

exp(γsim(s, t+s ))∑
t∈T exp(γsim(s, t))

(8)

sim(s, t+s ) = cos(ys, yt+s ) (9)

where cos is cosine similarity of two encoded vec-
tors. Please refer to the original paper (Huang
et al., 2013) for further details. The dropout ratio,



761

Figure 3: Example of H2H conversation text with URL link and corresponding texts found by following the URL.
We use those two sources of corresponding texts to train sentence embedding models. Each model projects the
original text and its corresponding texts to a close position in the sentence embedding space, while non-relevant
texts are being apart.

learning rate, and γ are selected based on a ran-
dom search (Bergstra and Bengio, 2012), which
are 0.0275, 0.4035 × 10−2, and 15 respectively.
The output of the second dense layer y of trained
model is used as the sentence embedding: for
each sentence we extract the sentence embedding
vs ∈ R300.

3.2.2 Contextual Information

Contextual information extracted from previous
sentences is known to be useful to improve un-
derstanding of human spoken language on other
scenarios (Xu and Sarikaya, 2014; Chen et al.,
2016; Su et al., 2018). To obtain knowledge from
a previous sentence in the conversation, we ex-
tract a contextual encoded vector using the mem-
ory network (Chen et al., 2016), which uses the
weighted sum of the output of word-level bidirec-
tional LSTM h in the core network (Section 3.1)
from previous sentences. We did not consider a
time decaying model (Su et al., 2018) since our
data has a small number of turns.

We tested the model with some variations on
1) number of previous sentences to use and 2)
weighting scheme (uniform or with attention).
using the implementation from the original pa-

per4. From our experiments, the best result was
achieved using the previous two sentences with a
uniform weight. We use this model to extract the
contextual encoded vector vc ∈ R100.

3.2.3 Human-to-Machine Expert Feedback
Kim et al. (2017) and Jha et al. (2018) introduced a
transfer learning method, which reuses the knowl-
edge from existing trained models on relevant do-
mains (i.e. expert models) to take advantage of
previous knowledge to train on a new domain.
They extract the output of the expert model and
use it as an additional input of feed forward layer
for the model on a new domain.

We adopt this idea to take advantage of mas-
sive amount of labeled data for H2M conversa-
tions. Instead of transferring knowledge from do-
main to domain, we transfer the knowledge of dif-
ferent tasks within a similar domain. For exam-
ple, we use Places (H2M) domain for the Restau-
rant (H2H) domain, and Entertainment (H2M)
domain for the Music (H2H) domain. We use pre-
viously trained slot tagging models on H2M con-
versations on similar domains as our expert model,
which has the same architecture as our core net-

4https://github.com/yvchen/
ContextualSLU

https://github.com/yvchen/ContextualSLU
https://github.com/yvchen/ContextualSLU


762

work (Section 3.1). These H2M models were orig-
inally used for the SLU component of a commer-
cial digital assistant. The output of word-level
bidirectional LSTM h is then extracted as the en-
coded vector from H2M expert model ve ∈ R200.

3.3 Network Ensemble Approaches

Since additional network components (sentence
embedding vs, contextual information from pre-
vious turns of the conversation vc, and H2M based
expert feedback ve) bring information from differ-
ent perspectives, we discuss how to compose them
into a single vector k with various ensemble ap-
proaches.

• Concatenation: Here, we simply concate-
nate all encodings into a single vector,

k = vs ⊕ vc ⊕ ve (10)

• Mean: We first apply a separate dense layer
to each encoded vector to match dimensions
and transform into the same latent space, and
then take the arithmetic mean of transformed
vectors.

v′{s,c,e} =W{s,c,e} + b{s,c,e} (11)

k = mean(v′s, v
′
c, v
′
e) (12)

In the Figure 2, we denote the dense
layer applied to each encoded vector v{s,c,e}
as D{s,c,e} for simplicity of representation.
Each transformed vector v′{s,c,e} ∈ R

100, so
k ∈ R100.

• Attention: We apply an attention mechanism
to apply different weights on the encoded
vectors for each sentence. For our problem,
it is not straightforward to define a context
vector for each sentence to calculate the im-
portance of each encoded vector; therefore,
we adopted the idea of using a global con-
text vector (Yang et al., 2016). The global
context vector u ∈ R100 can be thought as
a fixed query of “finding the informative en-
coded vector for slot tagging” used for each
sentence. The weight of each encoded vec-
tor is calculated with the standard equation of
calculating the attention weight, which is the
softmax of the dot product of encoding and

context vector,

w{s,c,e} =
exp(tanh(v′{s,c,e})

ᵀu)∑
v′∈{v′s,v′c,v′e} exp(tanh(v

′)ᵀu))

(13)

k = wsv
′
s + wcv

′
c + wev

′
e (14)

where v′{s,c,e} are same as Equation 11.

The combined single vector k is then aggre-
gated with the output of core network h, k ⊕ h
is used as the input of the final feed forward layer
as shown in Figure 2. The same hyperparameters
(mini batch size, learning rate, dropout ratio) and
optimizer is used as stated in the baseline model
(Section 3.1).

4 Data

Although some datasets with H2H conversa-
tions are available (Forsythand and Martell, 2007;
Danescu-Niculescu-Mizil and Lee, 2011; Nio
et al., 2014; Sordoni et al., 2015; Lowe et al., 2015;
Li et al., 2017), they were not feasible to use for
experimenting on our task. All datasets excluding
the Ubuntu Dialogue (Lowe et al., 2015) were col-
lected without any restrictions on the domain and,
as a result, there were insufficient training sam-
ples to train a slot tagging model for a specific do-
main. In addition, the Ubuntu Dialogue dataset
(Lowe et al., 2015) focuses on questions related to
Ubuntu OS, which is not an attractive domain an
intelligent focus that focuses on task completion
rather than question answering.

Since there were no existing datasets that were
sufficient for our task in H2H conversation, we
built our own dataset for the experiments. It
was difficult to acquire actual H2H conversations
from instant messages due to privacy concerns.
Therefore, we chose to use public conversations
on Twitter and extracted sequences in which two
users engage in a multi-turn conversation. Using
this approach, we were able to collect 3.8 million
sequences of four-turn conversations using Twitter
Firehose.

We focused on two domains for our experi-
ments: Restaurants and Music. To acquire the
dataset for each domain, we first defined a set
of key phrases and found the candidate conversa-
tions with at least one of those key phrases. Key
phrases consisted of the top 100 most frequently
used unigrams and bigrams on each relevant do-
main from the H2M conversation dataset. We used



763

Restaurant
A: [lunch]meal type?
B: [lunch]meal type sounds good. Our routine
usually involves sitting at [Nano’s]place name
with our packed/purchased [lunches]meal type
care to join?
A: great. I’ll get something from
[physiol]place name and meet you there at...?
B: I’ll be there in [5-10 mins]time

Music
A: [quavo]media person got another one
B: I was bout to listen earlier but it said
[feat]media role [Lil Uzi Vert]media person lol
A: he [rap]media genre for about two minutes you
don’t even gotta listen to [lil uzi]media person
B: [quavo]media person already a legend man

Table 1: Example conversation in each domain of our
dataset

the H2M Places domain to find the top n-grams for
the Restaurant domain and the H2M Entertain-
ment domain to find top n-grams for the Music
domain. Places includes other type of places be-
sides restaurants (e.g. tour sights), and also Enter-
tainment includes other genre (e.g. movies). So
we manually replaced unigrams and bigrams that
were not music or entertainment related, and also
some terms that are too general (e.g. time, call,
find). We were able to gather 16K and 22K candi-
date conversations for the Restaurant and Music
domains, respectively, using the keyphrases.

We randomly sampled 10K conversations for
each domain for annotating slots and domain. An-
notation was done by managed judges, who had
been trained over time for annotating SLU com-
ponents such as intents, slots and domains. A
guideline document was provided with the pre-
cise definition and annotated examples of each of
the slots and intents. Agreement between judges
and manual inspection of samples for quality as-
surance was done by a linguist trained for manag-
ing annotation tasks. We also ensured that judges
did not attempt to guess at the underlying intents
and slots, and annotate objectively within the con-
text from the text. We only keep the conversations
that are labeled relevant to each domain by annota-
tors. Table 1 shows an example conversation from
the dataset in each domain, and Table 2 shows the
dataset statistics.

Domain #Conv. #Words/Conv. #Slots
Restaurant 6,514 37.64 155

Music 5,582 44.72 206

Table 2: Statistics of our dataset. Each column shows
the number of items in the dataset. ”Conv.” stands for
conversations.

5 Experiments

5.1 Experimental Setup
All experiments were done with 10-fold cross val-
idation for the slot tagging task, and we generated
training, development, test datasets using 80%,
10%, and 10% of the data. The development
dataset is used for hyperparameter tuning with ran-
dom search (Bergstra and Bengio, 2012) and early
stopping. The baseline is set with core network
only (Section 3.1). We evaluated the performance
of each of the models with precision, recall, and
F1. We checked for statistical significance over the
baseline at the p-value < 0.05 using the Wilcoxon
signed-rank test.

5.2 Evaluation on Adding Sentence
Embeddings for H2H Conversations

In this section, we evaluate adding the sentence
embeddings into our slot tagging architecture in-
troduced in Section 3.2.1. Table 3 shows the re-
sults of adding sentence embeddings, compared
with the baseline and existing sentence embed-
ding methods. We extracted two months of recent
tweets that had non-twitter domain URLs in the
text for our method. Below is the brief description
of each method:

• DSSM (Deep Structured Semantic Model)
(Huang et al., 2013): Pre-trained DSSM
model from the authors , trained with pairs
of (Major commercial web search engine
queries, clicked page titles).

• Tweet2Vec (Dhingra et al., 2016): The model
was originally used to predict hashtags of a

5Slots in Restaurant domain include absolute location,
amenities, atmosphere, cuisine, date, distance, meal type,
open status, place name, place type, price range, product,
rating, service provided, time.

6Slots in Music domain include app name, me-
dia award, media category, media content rating,
media genre, media keyword, media language, me-
dia lyrics, media nationality, media person, me-
dia price, media release date, media role, media source,
media technical type, media title, media type, me-
dia user rating, radio call sign, radio frequency



764

Model
Restaurant Music

P R F1 P R F1
Core Network (Baseline) 71.23 62.68 66.63 64.33 44.14 51.61
+ DSSM (Huang et al., 2013) 70.82 61.60 65.88∗ 62.83 43.76 51.57
+ Tweet2Vec (Dhingra et al., 2016) 70.58 59.99 64.75∗ 62.67 41.38 49.79∗

+ Ours (Tweets, Web Search Engine Queries) 71.73 62.38 66.70 63.13 44.09 51.86
+ Ours (Tweets, Web Paage Titles) 71.19 63.51 67.11∗ 63.70 44.44 52.32

Table 3: Comparison of adding sentence embedding component to our architecture. P, R, F1 stands for precision,
recall, F1-score (%) respectively. * denotes the F1-score is statistically significant compared to the baseline.

tweet. We use the pre-trained model from the
authors, which used 2M tweets for training.

• Ours (Tweets, Web Search Engine Queries):
Trained our model with 2.35M pairs of
(Tweet text with shared URL, Web serach en-
gine queries that lead to the shared URL). We
extracted most frequent queries (up to eight)
found from the major commercial web search
engine query logs.

• Ours (Tweets, Web Page Titles): Trained our
model with 420K pairs of (Tweet text with
shared URL, web page title of URL).

The result shows that adding our proposed sen-
tence embedding network improves the slot tag-
ging result compared to the baseline, while other
previous methods have a negative effect. This
implies that 1) a sentence embedding specifically
trained for H2H conversation texts are needed
(compared with original DSSM), 2) our idea of
embedding semantics and intentions from web
data and search engine query logs can help to
improve the slot tagging task (compared to the
Tweet2Vec). Since our sentence embedding net-
work trained with web page titles gives the most
significant improvement, we used this for further
evaluation.

5.3 Evaluation on Utilizing Knowledge
Sources

We also tested adding contextual information and
H2M expert feedback network components to our
slot tagging architecture. Contextual informa-
tion is extracted from previous two sentences with
uniform weighting. For the H2M expert model,
we used the pretrained model of Entertainment
domain for the target Music domain, and the
Places domain for the target Restaurant domain
for H2M LU.

The upper part (rows 2-4) in Table 4 shows the
result of adding each. Results show that 1) adding

network component from each knowledge source
leads to an improvement on at least one of the do-
main, 2) improvement on each method varies with
the domain. Adding sentence embeddings and
contextual information led to significant improve-
ments for the Restaurant domain while contex-
tual information and H2M expert feedback led to
significant improvements for the Music domain.

5.4 Evaulation on Network Ensemble
Approaches

We also conducted an experiment to include all
network components to see if we can improve fur-
ther by considering multiple knowledge sources
together. The result is shown in the lower part (row
5-7) of Table 4 with different ensembling meth-
ods introduced in Section 3.3. It shows that any of
the ensemble approaches to add all of the network
components leads to better results than adding ei-
ther of them individually.

The result implies that each of the proposed
method improves the slot tagging method from
different perspectives so all of them can be con-
sidered. Also, we see that attention has the best
results among ensemble approaches, with 2.62%
higher F1 score for the Restaurant domain, and
6.09% for the Music domain compared to the
baseline. This implies the attention model can
help to find the best way to ensemble additional
components by predicting the importance of each
component for each sentence. Especially, we
could see a statistically significant improvement
on the Music domain compared with other meth-
ods. We believe this is because the improvement
of each network component on the Music domain
is more obvious compared to the Restaurant do-
main. We would like to test in other domains for
the future work.



765

Model
Restaurant Music

P R F1 P R F1
Core Network (Baseline) 71.23 62.68 66.63 64.33 44.14 51.61
+ Sentence Embedding 71.19 63.51 67.11∗ 63.70 44.44 52.32
+ Contextual 72.74 64.75 68.47∗ 64.42 49.16 55.72∗

+ H2M Expert 71.91 61.21 66.63 64.14 44.67 52.64∗

+ Ensemble (Concatenation) 74.09 64.89 69.21∗ 66.41 50.10 57.07∗
+ Ensemble (Mean) 73.20 65.43 69.07∗ 65.18 51.01 57.11∗
+ Ensemble (Attention) 74.03 65.11 69.25∗ 66.19 51.29 57.70∗∗

Table 4: Comparison on adding additional network components from each knowledge source and network ensem-
ble approaches that adds all components. P, R, F1 stands for precision, recall, F1-score (%) respectively. * denotes
the F1 score is statistically significant compared to the baseline. ** denotes the F1 score of ensemble model is also
statistically significant compared to the concatenation ensemble model.

6 Conclusion

We studied slot tagging in H2H online text con-
versations. Starting from a core network with
bidirectional LSTM, we proposed to use addi-
tional network components and ensemble them to
augment useful knowledge from multiple sources
(web data, search engine click logs, H2M expert
feedback, and previous utterances). Experiments
with our four-turn Twitter dataset on Restaurant
and Music domains showed that our method im-
proves up to 6.09%-points higher F1 on slot tag-
ging compared to existing approaches. For fu-
ture work, we plan to study our model on domain
and intent classification, and also on additional do-
mains.

Acknowledgement

We would like to thank Eric Mei and Soumya Ba-
tra for their help on data collection and labeling.

References
Ankur Bapna, Gokhan Tur, Dilek Hakkani-Tur, and

Larry Heck. 2017. Sequential dialogue context
modeling for spoken language understanding. In
Proceedings of the 18th Annual SIGdial Meeting on
Discourse and Dialogue, pages 103–114.

James Bergstra and Yoshua Bengio. 2012. Random
search for hyper-parameter optimization. Journal of
Machine Learning Research, 13(Feb):281–305.

Yun-Nung Chen, Dilek Hakkani-Tür, Gökhan Tür,
Jianfeng Gao, and Li Deng. 2016. End-to-end mem-
ory networks with knowledge carryover for multi-
turn spoken language understanding. In INTER-
SPEECH, pages 3245–3249.

Cristian Danescu-Niculescu-Mizil and Lillian Lee.
2011. Chameleons in imagined conversations: A

new approach to understanding coordination of lin-
guistic style in dialogs. In Proceedings of the
2nd Workshop on Cognitive Modeling and Compu-
tational Linguistics, pages 76–87. Association for
Computational Linguistics.

Li Deng, Gokhan Tur, Xiaodong He, and Dilek
Hakkani-Tr. 2012. Use of kernel deep convex net-
works and end-to-end learning for spoken language
understanding. In IEEE Workshop on Spoken Lan-
guage Technologies (SLT).

Bhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick,
Michael Muehl, and William W Cohen. 2016.
Tweet2vec: Character-based distributed representa-
tions for social media. In The 54th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), page 269.

Eric N Forsythand and Craig H Martell. 2007. Lex-
ical and discourse analysis of online chat dialog.
In International Conference on Semantic Computing
(ICSC 2007), pages 19–26. IEEE.

Anuj Kumar Goyal, Angeliki Metallinou, and Spyros
Matsoukas. 2018. Fast and scalable expansion of
natural language understanding functionality for in-
telligent agents. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT), volume 3, pages
145–152.

Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM
international conference on Conference on infor-
mation and knowledge management (CIKM), pages
2333–2338.

Youngsoo Jang, Jiyeon Han, Byung-Jun Lee, and Kee-
Eung Kim. 2018. Cross-language neural dialog state
tracker for large ontologies using hierarchical atten-
tion. IEEE/ACM Transactions on Audio, Speech,
and Language Processing, 26(11):2072–2082.



766

Rahul Jha, Alex Marin, Suvamsh Shivaprasad, and
Imed Zitouni. 2018. Bag of experts architectures
for model reuse in conversational language under-
standing. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL-HLT), volume 3, pages 153–161.

Young-Bum Kim, Karl Stratos, and Dongchan Kim.
2017. Domain attention with an ensemble of ex-
perts. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), volume 1, pages 643–653.

Diederik P Kingma and Jimmy Lei Ba. 2015. Adam:
Amethod for stochastic optimization. In Proceed-
ings of the 3rd International Conference of Learning
Representations (ICLR).

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
arXiv preprint arXiv:1603.01360.

Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang
Cao, and Shuzi Niu. 2017. Dailydialog: A manually
labelled multi-turn dialogue dataset. In Proceed-
ings of the The 8th International Joint Conference
on Natural Language Processing (IJCNLP), pages
986–995.

Ryan Lowe, Nissan Pow, Iulian V Serban, and Joelle
Pineau. 2015. The ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn di-
alogue systems. In 16th Annual Meeting of the
Special Interest Group on Discourse and Dialogue
(SIGDIAL), page 285.

Grégoire Mesnil, Yann Dauphin, Kaisheng Yao,
Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi-
aodong He, Larry Heck, Gokhan Tur, Dong Yu, et al.
2015. Using recurrent neural networks for slot fill-
ing in spoken language understanding. IEEE/ACM
Transactions on Audio, Speech, and Language Pro-
cessing, 23(3):530–539.

Lasguido Nio, Sakriani Sakti, Graham Neubig, Tomoki
Toda, and Satoshi Nakamura. 2014. Conversation
dialog corpora from television and movie scripts.
In 2014 17th Oriental Chapter of the International
Committee for the Co-ordination and Standardiza-
tion of Speech Databases and Assessment Tech-
niques (COCOSDA), pages 1–4. IEEE.

Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao,
Xiaodong He, Jianshu Chen, Xinying Song, and
Rabab Ward. 2016. Deep sentence embedding using
long short-term memory networks: Analysis and ap-
plication to information retrieval. IEEE/ACM Trans-
actions on Audio, Speech and Language Processing
(TASLP), 24(4):694–707.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for

word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Suman Ravuri and Andreas Stolcke. 2015. Recurrent
neural network and lstm models for lexical utterance
classification. In Sixteenth Annual Conference of the
International Speech Communication Association.

Ruhi Sarikaya, Paul A Crook, Alex Marin, Minwoo
Jeong, Jean-Philippe Robichaud, Asli Celikyilmaz,
Young-Bum Kim, Alexandre Rochette, Omar Zia
Khan, Xiaohu Liu, et al. 2016. An overview of end-
to-end language understanding and dialog manage-
ment for personal digital assistants. In IEEE Spoken
Language Technology Workshop (SLT), pages 391–
397.

Ruhi Sarikaya, Geoffrey E Hinton, and Bhuvana Ram-
abhadran. 2011. Deep belief nets for natural lan-
guage call-routing. In IEEE International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP), pages 5680–5683.

Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Grégoire Mesnil. 2014. A latent semantic model
with convolutional-pooling structure for information
retrieval. In Proceedings of the 23rd ACM Inter-
national Conference on Conference on Information
and Knowledge Management (CIKM), pages 101–
110.

Hongjie Shi, Takashi Ushio, Mitsuru Endo, Katsuyoshi
Yamagami, and Noriaki Horii. 2016. A multichan-
nel convolutional neural network for cross-language
dialog state tracking. In IEEE Spoken Language
Technology Workshop (SLT), pages 559–564.

Hongjie Shi, Takashi Ushio, Mitsuru Endo, Katsuyoshi
Yamagami, and Noriaki Horii. 2017. Convolutional
neural networks for multi-topic dialog state track-
ing. In Dialogues with Social Robots, pages 451–
463. Springer.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceed-
ings of the 2015 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-
HLT).

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Shang-Yu Su, Pei-Chieh Yuan, and Yun-Nung Chen.
2018. How time matters: Learning time-decay at-
tention for contextual spoken language understand-
ing in dialogues. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Asso-



767

ciation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT), volume 1, pages
2133–2142.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in neural information processing systems (NIPS),
pages 2440–2448.

Gokhan Tur and Renato De Mori. 2011. Spoken lan-
guage understanding: Systems for extracting seman-
tic information from speech. John Wiley & Sons.

Soroush Vosoughi, Prashanth Vijayaraghavan, and Deb
Roy. 2016. Tweet2vec: Learning tweet embeddings
using character-level cnn-lstm encoder-decoder. In
Proceedings of the 39th International ACM Interna-
tional conference on Research and Development in
Information Retrieval (SIGIR), pages 1041–1044.

Puyang Xu and Ruhi Sarikaya. 2014. Contextual do-
main classification in spoken language understand-
ing systems using recurrent neural network. In IEEE
Internation Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 136–140.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), pages 1480–1489.


