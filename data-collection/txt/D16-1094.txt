



















































Richer Interpolative Smoothing Based on Modified Kneser-Ney Language Modeling


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 944–949,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Richer Interpolative Smoothing Based on Modified Kneser-Ney
Language Modeling

Ehsan Shareghi,♣ Trevor Cohn♠ and Gholamreza Haffari♣
♣ Faculty of Information Technology, Monash University

♠ Computing and Information Systems, The University of Melbourne
first.last@{monash.edu,unimelb.edu.au}

Abstract

In this work we present a generalisation of the
Modified Kneser-Ney interpolative smoothing
for richer smoothing via additional discount
parameters. We provide mathematical under-
pinning for the estimator of the new discount
parameters, and showcase the utility of our
rich MKN language models on several Euro-
pean languages. We further explore the in-
terdependency among the training data size,
language model order, and number of dis-
count parameters. Our empirical results illus-
trate that larger number of discount parame-
ters, i) allows for better allocation of mass in
the smoothing process, particularly on small
data regime where statistical sparsity is se-
vere, and ii) leads to significant reduction in
perplexity, particularly for out-of-domain test
sets which introduce higher ratio of out-of-
vocabulary words.1

1 Introduction

Probabilistic language models (LMs) are the core
of many natural language processing tasks, such as
machine translation and automatic speech recogni-
tion. m-gram models, the corner stone of language
modeling, decompose the probability of an utter-
ance into conditional probabilities of words given a
fixed-length context. Due to sparsity of the events
in natural language, smoothing techniques are criti-
cal for generalisation beyond the training text when
estimating the parameters of m-gram LMs. This
is particularly important when the training text is

1For the implementation see: https://github.com/
eehsan/cstlm

small, e.g. building language models for translation
or speech recognition in low-resource languages.

A widely used and successful smoothing method
is interpolated Modified Kneser-Ney (MKN) (Chen
and Goodman, 1999). This method uses a linear in-
terpolation of higher and lower order m-gram prob-
abilities by preserving probability mass via absolute
discounting. In this paper, we extend MKN by in-
troducing additional discount parameters, leading to
a richer smoothing scheme. This is particularly im-
portant when statistical sparsity is more severe, i.e.,
in building high-order LMs on small data, or when
out-of-domain test sets are used.

Previous research in MKN language modeling,
and more generally m-gram models, has mainly
dedicated efforts to make them faster and more com-
pact (Stolcke et al., 2011; Heafield, 2011; Shareghi
et al., 2015) using advanced data structures such as
succinct suffix trees. An exception is Hierarchical
Pitman-Yor Process LMs (Teh, 2006a; Teh, 2006b)
providing a rich Bayesian smoothing scheme, for
which Kneser-Ney smoothing corresponds to an ap-
proximate inference method. Inspired by this work,
we directly enrich MKN smoothing realising some
of the reductions while remaining more efficient in
learning and inference.

We provide estimators for our additional discount
parameters by extending the discount bounds in
MKN. We empirically analyze our enriched MKN
LMs on several European languages in in- and out-
of-domain settings. The results show that our dis-
counting mechanism significantly improves the per-
plexity compared to MKN and offers a more elegant

944



way of dealing with out-of-vocabulary (OOV) words
and domain mismatch.

2 Enriched Modified Kneser-Ney

Interpolative Modified Kneser-Ney (MKN) (Chen
and Goodman, 1999) smoothing is widely accepted
as a state-of-the-art technique and is implemented in
leading LM toolkits, e.g., SRILM (Stolcke, 2002)
and KenLM (Heafield, 2011).

MKN uses lower order k-gram probabilities to
smooth higher order probabilities. P (w|u) is de-
fined as,

c(uw)− Dm(c(uw))
c(u)

+
γ(u)

c(u)
× P̄ (w|π(u))

where c(u) is the frequency of the pattern u, γ(.) is
a constant ensuring the distribution sums to one, and
P̄ (w|π(u)) is the smoothed probability computed
recursively based on a similar formula2 conditioned
on the suffix of the pattern u denoted by π(u). Of
particular interest are the discount parameters Dm(.)
which remove some probability mass from the max-
imum likelihood estimate for each event which is
redistributed over the smoothing distribution. The
discounts are estimated as

Dm(i) =





0, if i = 0
1− 2n2[m]

n1[m]
n1[m]

n1[m]+2n2[m]
, if i = 1

2− 3n3[m]
n2[m]

n1[m]
n1[m]+2n2[m]

, if i = 2

3− 4n4[m]
n3[m]

. n1[m]
n1[m]+2n2[m]

, if i ≥ 3

where ni(m) is the number of unique m-grams3 of
frequency i. This effectively leads to three discount
parameters {Dm(1),Dm(2),Dm(3+)} for the distri-
butions on a particular context length, m.

2.1 Generalised MKN
Ney et al. (1994) characterized the data sparsity us-
ing the following empirical inequalities,

3n3[m] < 2n2[m] < n1[m] for m ≤ 3

It can be shown (see Appendix A) that these em-
pirical inequalities can be extended to higher fre-

2Note that in all but the top layer of the hierarchy, con-
tinuation counts, which count the number of unique contexts,
are used in place of the frequency counts (Chen and Goodman,
1999).

3Continuation counts are used for the lower layers.

quencies and larger contexts m > 3,

(N −m)nN−m[m] < ... < 2n2[m]
< n1[m] <

∑

i>0

ni[m]� n0[m] < σm

where σm is the possible number of m-grams over
a vocabulary of size σ, n0[m] is the number of m-
grams that never occurred, and

∑
i>0 ni[m] is the

number of m-grams observed in the training data.
We use these inequalities to extend the discount

depth of MKN, resulting in new discount parame-
ters. The additional discount parameters increase the
flexibility of the model in altering a wider range of
raw counts, resulting in a more elegant way of as-
signing the mass in the smoothing process. In our
experiments, we set the number of discounts to 10
for all the levels of the hierarchy, (compare this to
these in MKN).4 This results in the following esti-
mators for the discounts,

Dm(i) =





0, if i = 0
i− (i+ 1)ni+1[m]

ni[m]
n1[m]

n1[m]+2n2[m]
, if i < 10

10− 11n11[m]
n10[m]

. n1[m]
n1[m]+2n2[m]

, if i ≥ 10

It can be shown that the above estimators for our dis-
count parameters are derived by maximizing a lower
bound on the leave-one-out likelihood of the training
set, following (Ney et al., 1994; Chen and Goodman,
1999) (see Appendix B for the proof sketch).

3 Experiments

We compare the effect of using different numbers of
discount parameters on perplexity using the Finnish
(FI), Spanish (ES), German (DE), English (EN) por-
tions of the Europarl v7 (Koehn, 2005) corpus. For
each language we excluded the first 10K sentences
and used it as the in-domain test set (denoted as EU),
skipped the second 10K sentences, and used the rest
as the training set. The data was tokenized, sentence
split, and the XML markup discarded. We tested
the effect of domain mismatch, under two settings
for out-of-domain test sets: i) mild using the Span-
ish section of news-test 2013, the German, English
sections of news-test 2014, and the Finnish section

4We have selected the value of 10 arbitrarily; however our
approach can be used with larger number of discount parame-
ters, with the caveat that we would need to handle sparse counts
in the higher orders.

945



Perplexity
size (M) size (K) MKN (D1...3) MKN (D[1...4]) MKN (D[1...10])

Training tokens sents Test tokens sents OOV% m = 2 m = 5 m = 10 m = 2 m = 5 m = 10 m = 2 m = 5 m = 10
NT 19.8 3 9.2 6536.6 5900.3 5897.3 6451.3 5827.6 5824.6 6154.4 5575.0 5572.5

FI 46.5 2.2 EU 197.3 10 6.1 390.7 287.4 286.8 390.7 287.3 286.6 390.4 287.3 286.8
TW 10.9 1.3 52.1 57 825.1 51 744.1 51 740.1 55 550.2 49 884.2 49 881.3 47 696.2 43 277.3 43 275.5
NT 70.7 3 9.1 565.6 431.5 429.4 560.0 425.5 423.5 541.5 409.0 407.3

ES 68.0 2.2 EU 281.5 10 2.4 92.7 51.5 51.1 92.8 51.5 51.1 92.8 51.4 51.0
TW 3141.3 293 78.5 17 804.2 14 062.7 14 027.1 17 121.4 13 487.4 13 454.1 14 915.7 11 832.1 11 807.2
NT 64.5 3 18.7 2190.7 1784.6 1781.8 2158.9 1755.8 1753.2 2065.3 1680.6 1678.3

DE 61.2 2.3 EU 244.0 10 4.6 156.9 91.7 91.2 156.9 91.6 91.2 156.4 91.7 91.2
MED 317.7 10 59.8 5135.7 4232.4 4226.7 5007.5 4123.0 4117.5 4636.0 3831.2 3826.6
NT 69.5 3 5.5 1089.2 875.0 872.2 1071.1 857.2 854.4 1011.5 806.7 804.4

EN 67.5 2.2 EU 274.9 10 1.7 90.1 48.4 48.1 90.1 48.3 48.0 90.5 48.3 48.0
MED 405.9 10 44.1 2319.7 1947.9 1942.5 2261.6 1893.3 1888.2 2071.9 1734.9 1730.8

Table 1: Perplexity for various m-gram orders m ∈ 2, 3, 10 and training languages from Europarl, using different
numbers of discount parameters for MKN. MKN (D[1...3]), MKN (D[1...4]), MKN (D[1...10]) represent vanilla MKN,
MKN with 1 more discounts, and MKN with 7 more discount parameters, respectively. Test sets sources EU, NT,
TW, MED are Europarl, news-test, Twitter, and medical patent descriptions, respectively. OOV is reported as the ratio
|{OOV ∈test-set}|
|{w∈test-set}| .

0
1
2

4

8

14

CZ FI ES DE EN FR CZ FI ES DE EN FR

[Corpus]

[%
P

er
pl

ex
ity

 R
ed

uc
tio

n]

pplD[1...10]
pplD[1...4]

Figure 1: Percentage of perplexity reduction for
pplxD[1...4] and pplxD[1...10] compared with pplxD[1..3] on
different training corpora (Europarl CZ, FI, ES, DE, EN,
FR) and on news-test sets (NT) for m = 2 (left), and
m = 10 (right).

of news-test 2015 (all denoted as NT)5, and ii) ex-
treme using a 24 hour period of streamed Finnish,
and Spanish tweets6 (denoted as TW), and the Ger-
man and English sections of the patent description
of medical translation task7 (denoted as MED). See
Table 1 for statistics of the training and test sets.

3.1 Perplexity

Table 1 shows substantial reduction in perplexity on
all languages for out-of-domain test sets when ex-
panding the number of discount parameters from 3
in vanilla MKN to 4 and 10. Consider the English

5http://www.statmt.org/{wmt13,14,15}/test.tgz
6Streamed via Twitter API on 17/05/2016.
7http://www.statmt.org/wmt14/medical-task/

news-test (NT), in which even for a 2-gram language
model a single extra discount parameter (m = 2,
D[1...4]) improves the perplexity by 18 points and
this improvement quadruples to 77 points when us-
ing 10 discounts (m = 2, D[1...10]). This effect
is consistent across the Europarl corpora, and for
all LM orders. We observe a substantial improve-
ments even for m = 10-gram models (see Figure 1).
On the medical test set which has 9 times higher
OOV ratio, the perplexity reduction shows a simi-
lar trend. However, these reductions vanish when an
in-domain test set is used. Note that we use the same
treatment of OOV words for computing the perplex-
ities which is used in KenLM (Heafield, 2013).

3.2 Analysis

Out-of-domain and Out-of-vocabulary We se-
lected the Finnish language for which the number
and ratio of OOVs are close on its out-of-domain
and in-domain test sets (NT and EU), while show-
ing substantial reduction in perplexity on out-of-
domain test set, see FI bars on Figure 1. Figure 2
(left), shows the full perplexity results for Finnish
for vanilla MKN, and our extensions when tested on
in-domain (EU) and out-of-domain (NT) test sets.
The discount plot, Figure 2 (middle) illustrates the
behaviour of the various discount parameters. We
also measured the average hit length for queries by
varyingm on in-domain and out-of-domain test sets.
As illustrated in Figure 2 (right) the in-domain test
set allows for longer matches to the training data as

946



2 3 4 5 6 7 8 9 10 ∞

285

390

5500

5800
5900

6550

m

P
er

pl
ex

ity

●

●
● ● ● ● ● ● ● ●

●

●
● ● ● ● ● ● ● ●

●

D[1...3]
D[1...4]
D[1...10]

NT
EU

1 2 3 4 5 6 7 8 9 10

1

1.5

2.5

3.5

4.5

i

D
is

co
un

t

●

●

●

●

●

●

●

● ●

●

●

●
●

●
●

● ●

● ●

●

●

● ● ●
●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

● ● ● ●

●

●

●

●

●●

●

●

●

●

1−gram
2−gram
3−gram
4−gram
5−gram
6−gram
7−gram
8−gram
9−gram
10−gram

2 3 4 5 6 7 8 9 10

1

1.5

2

2.5

3

m

A
ve

ra
ge

 h
it 

le
ng

th

NT
EU

Figure 2: Statistics for the Finnish section of Europarl. The left plot illustrates the perplexity when tested on an out-
of-domain (NT) and in-domain (EU) test sets varying LM order, m. The middle plot shows the discount parameters
Di∈[1...10] for different m-gram orders. The right plot correspond to average hit length on EU and NT test sets.

m Di
sc

ou
nt

P
er

pl
ex

ity

2 3 4 5 6 7 8

9

10

3

4

101678

2190

m Di
sc

ou
nt

P
er

pl
ex

ity

2 3 4 5 6 7 8

9

10

3

4

10180

426

Figure 3: Perplexity (z-axis) vs. m ∈ [2...10] (x-axis) vs.
number of discounts Di∈3,4,10 (y-axis) for German lan-
guage trained on Europarl (left), and CommonCrawl2014
(right) and tested on news-test. Arrows show the direc-
tion of the increase.

m grows. This indicates that having more discount
parameters is not only useful for test sets with ex-
tremely high number of OOV, but also allows for a
more elegant way of assigning mass in the smooth-
ing process when there is a domain mismatch.

Interdependency of m, data size, and discounts
To explore the correlation between these factors we
selected the German and investigated this correla-
tion on two different training data sizes: Europarl
(61M words), and CommonCrawl 2014 (984M
words). Figure 3 illustrates the correlation between
these factors using the same test set but with small
and large training sets. Considering the slopes of the
surfaces indicates that the small training data regime
(left) which has higher sparsity, and more OOV in
the test time benefits substantially from the more ac-
curate discounting compared to the large training set
(right) in which the gain from discounting is slight.8

8Nonetheless, the improvement in perplexity consistently
grows with introducing more discount parameters even under

4 Conclusions

In this work we proposed a generalisation of Modi-
fied Kneser-Ney interpolative language modeling by
introducing new discount parameters. We provide
the mathematical proof for the discount bounds used
in Modified Kneser-Ney and extend it further and il-
lustrate the impact of our extension empirically on
different Europarl languages using in-domain and
out-of-domain test sets.

The empirical results on various training and test
sets show that our proposed approach allows for a
more elegant way of treating OOVs and mass assign-
ments in interpolative smoothing. In future work,
we will integrate our language model into the Moses
machine translation pipeline to intrinsically measure
its impact on translation qualities, which is of partic-
ular use for out-of-domain scenario.

Acknowledgements

This research was supported by the National ICT
Australia (NICTA) and Australian Research Council
Future Fellowship (project number FT130101105).
This work was done when Ehsan Shareghi was an
intern at IBM Research Australia.

A. Inequalities

We prove that these inequalities hold in expectation
by making the reasonable assumption that events in

the large training data regime, which suggests that more dis-
count parameters, e.g., up to D30, may be required for larger
training corpus to reflect the fact that even an event with fre-
quency of 30 might be considered rare in a corpus of nearly 1
billion words.

947



the natural language follow the power law (Clauset
et al., 2009), p

(
C(u) = f

)
∝ f−1−

1
sm , where sm

is the parameter of the distribution, and C(u) is the
random variable denoting the frequency of the m-
grams pattern u. We now compute the expected
number of unique patterns having a specific fre-
quency E[ni[m]]. Corresponding to each m-grams
pattern u, let us define a random variable Xu which
is 1 if the frequency of u is i and zero otherwise. It
is not hard to see that ni[m] =

∑
uXu, and

E
[
ni[m]

]
= E

[∑

u

Xu
]

=
∑

u

E[Xu] = σ
mE[Xu]

= σm
(
p
(
C(u) = i

)
× 1 + p

(
C(u) 6= i

)
× 0
)

∝ σmi−1− 1sm .

We can verify that

(i+ 1)E
[
ni+1[m]

]
< iE

[
ni[m]

]
⇔

(i+ 1)σm(i+ 1)−1−
1

sm < iσmi−1−
1

sm ⇔
i

1
sm < (i+ 1)

1
sm .

which completes the proof of the inequalities.

B. Discount bounds proof sketch

The leave-one-out (leaving those m-grams which
occurred only once) log-likelihood function of the
interpolative smoothing is lower bounded by back-
off model’s (Ney et al., 1994), hence the estimated
discounts for later can be considered as an approx-
imation for the discounts of the former. Consider a
backoff model with absolute discounting parameter
D, were P (wi|wi−1i−m+1) is defined as:




c(wii−m+1)−D
c(wi−1i−m+1)

if c(wii−m+1) > 0

Dn1+(w
i−1
i−m+1 ·)

c(wi−1i−m+1)
P̄ (wi|wi−1i−m+2) if c(wii−m+1) = 0

where n1+(wi−1i−m+1 ·) is the number of unique
right contexts for the wi−1i−m+1 pattern. Assume that
for any choice of 0 < D < 1 we can define P̄ such
that P (wi|wi−1im+1) sums to 1. For readability we
use the λ(wi−1i−m+1) =

n1+(w
i−1
i−m+1 ·)

c(wi−1i−m+1)−1
replacement.

Following (Chen and Goodman, 1999), rewriting
the leave-one-out log-likelihood for KN (Ney et al.,
1994) to include more discounts (in this proof up to

D4), results in:

∑

wii−m+1
c(wii−m+1)>4

c(wii−m+1) log
c(wii−m+1)− 1−D4
c(wi−1i−m+1)− 1

+

4∑

j=2

(∑

wii−m+1
c(wii−m+1)=j

c(wii−m+1) log
c(wii−m+1)− 1−Dj−1

c(wi−1i−m+1)− 1
)

+

∑

wii−m+1
c(wii−m+1)=1

(
c(wii−m+1) log(

4∑

j=1

nj [m]Dj)λ(w
i−1
i−m+1)P̄

)

which can be simplified to,
∑

wii−m+1
c(wii−m+1)>4

c(wii−m+1) log(c(w
i
i−m+1)− 1−D4)+

4∑

j=2

(
jnj [m] log(j − 1−Dj−1)

)
+

n1[m] log(

4∑

j=1

nj [m]Dj) + const

To find the optimal D1, D2, D3, D4 we set the par-
tial derivatives to zero. For D3,

∂

∂D3
= n1[m]

n3[m]∑4
j=1 nj [m]Dj

− 4n4[m]
3−D3

= 0⇒

n1[m]n3[m](3−D3) = 4n4[m]
4∑

j=1

nj [m]Dj ⇒

3n1[m]n3[m]−D3n1[m]n3[m]− 4n4[m]n1[m]D1 > 0

⇒ 3− 4n4[m]
n3[m]

D1 > D3 �

And after taking c(wii−m+1) = 5 out of the summa-
tion, for D4:

∂

∂D4
=

∑

c(wii−m+1)>5

−c(wii−m+1)
c(wii−m+1)− 1−D

− 5n5[m]
4−D4

+ n1[m]
n4[m]∑4

j=1 nj [m]Dj
= 0⇒ −5n5[m]

4−D4

+ n1[m]
n4[m]∑4

j=1 nj [m]Dj
> 0⇒ n1[m]n4[m](4−D4)

> 5n5[m]

4∑

j=1

nj [m]Dj ⇒ 4− 5
n5[m]

n4[m]
D1 > D4 �

948



References
Stanley F. Chen and Joshua Goodman. 1999. An empir-

ical study of smoothing techniques for language mod-
eling. Computer Speech & Language, 13(4):359–393.

Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJ
Newman. 2009. Power-law distributions in empirical
data. SIAM review, 51(4):661–703.

Kenneth Heafield. 2011. KenLM: Faster and smaller lan-
guage model queries. In Proceedings of the Workshop
on Statistical Machine Translation.

Kenneth Heafield. 2013. Efficient Language Modeling
Algorithms with Applications to Statistical Machine
Translation. Ph.D. thesis, Carnegie Mellon University.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
Machine Translation summit.

Hermann Ney, Ute Essen, and Reinhard Kneser. 1994.
On structuring probabilistic dependences in stochastic
language modelling. Computer Speech & Language,
8(1):1–38.

Ehsan Shareghi, Matthias Petri, Gholamreza Haffari, and
Trevor Cohn. 2015. Compact, efficient and unlimited
capacity: Language modeling with compressed suffix
trees. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.

Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at sixteen: Update and
outlook. In Proceedings of IEEE Automatic Speech
Recognition and Understanding Workshop.

Andreas Stolcke. 2002. SRILM–an extensible language
modeling toolkit. In Proceedings of the International
Conference of Spoken Language Processing.

Yee Whye Teh. 2006a. A Bayesian interpretation of in-
terpolated Kneser-Ney. Technical report, NUS School
of Computing.

Yee Whye Teh. 2006b. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceedings
of the Annual Meeting of the Association for Compu-
tational Linguistics.

949


