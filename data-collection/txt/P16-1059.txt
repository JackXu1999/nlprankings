



















































Collective Entity Resolution with Multi-Focal Attention


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 621–631,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Collective Entity Resolution with Multi-Focal Attention

Amir Globerson∗ and Nevena Lazic and Soumen Chakrabarti† and
Amarnag Subramanya and Michael Ringgaard and Fernando Pereira

Google, Mountain View CA, USA
gamir@post.tau.ac.il, nevena@google.com, soumen@cse.iitb.ac.in,

{asubram, ringgaard, pereira}@google.com

Abstract

Entity resolution is the task of linking each
mention of an entity in text to the cor-
responding record in a knowledge base
(KB). Coherence models for entity resolu-
tion encourage all referring expressions in
a document to resolve to entities that are
related in the KB. We explore attention-
like mechanisms for coherence, where the
evidence for each candidate is based on a
small set of strong relations, rather than
relations to all other entities in the doc-
ument. The rationale is that document-
wide support may simply not exist for
non-salient entities, or entities not densely
connected in the KB. Our proposed sys-
tem outperforms state-of-the-art systems
on the CoNLL 2003, TAC KBP 2010,
2011 and 2012 tasks.

1 Introduction

Entity resolution (ER) is the task of mapping men-
tions of entities in text to corresponding records in
a knowledge base (KB) (Bunescu and Pasca, 2006;
Cucerzan, 2007; Kulkarni et al., 2009; Dredze et
al., 2010; Hoffart et al., 2011; Hachey et al., 2013).
ER is a challenging problem because mentions are
often ambiguous on their own, and can only be
resolved given appropriate context. For example,
the mention Beirut may refer to the capital of
Lebanon, the band from New Mexico, or a drink-
ing game (Figure 1). Names may also refer to en-
tities that are not in the KB, a problem known as
NIL detection.

Most ER systems consist of a mention model,
a context model, and a coherence model (Milne
and Witten, 2008; Cucerzan, 2007; Ratinov et al.,

∗Currently at Tel Aviv University
†Currently at IIT Bombay

2011; Hoffart et al., 2011; Hachey et al., 2013).
The mention model associates each entity with
its possible textual representations (also known as
aliases or surface forms). The context model helps
resolve an ambiguous mention using textual fea-
tures extracted from the surrounding context. The
coherence model, the focus of this work, encour-
ages all mentions to resolve to entities that are re-
lated to each other. Relations may be established
via the KB, Web links, embeddings, or other re-
sources.

Coherence models often define an objective
function that includes local and pairwise candi-
date scores, where the pairwise scores correspond
to some notion of coherence or relation strength.1

Support for a candidate is typically aggregated
over relations to all other entities in the document.
One problem with this approach is that it may di-
lute evidence for entities that are not salient in the
document, or not well-connected in the KB. Our
work aims to address this issue.

We introduce a novel coherence model with an
attention mechanism, where the score for each
candidate only depends on a small subset of men-
tions. Attention has recently been used with con-
siderable empirical success in tasks such as trans-
lation (Bahdanau et al., 2014) and image caption
generation (Xu et al., 2015). We argue that atten-
tion is also desirable for collective ER due to the
discussed imbalance in the number of relations for
different entities.

Attention models typically have a single focus,
implemented using the softmax function. Our
model allows each candidate to focus on multi-
ple mentions, and, to implement it, we introduce a
novel smooth version of the multi-focus attention

1An exception to this framework are topic models in
which a topic may generate both entities and words, e.g.,
(Kataria et al., 2011; Han and Sun, 2012; Houlsby and Cia-
ramita, 2014).

621



Beirut
(city in Leb.)

Beirut
(band)

Beirut
(game)

y1

Santa Fe
(city in NM)

Santa Fe
(film)

Santa Fe
(city in Cuba)

y2

New Mexico
(state)

New Mexico
(university)

New Mexico
(ship)

y3

Figure 1: Illustration of the ER problem for three mentions “Beirut”, “New Mexico” and “Santa Fe”.
each mention has three possible disambiguations. Edges link disambiguations that have Wikipedia links
between their respective pages.

function, which generalizes soft-max.
Our system uses mention and context models

similar to those of Lazic et al. (2015), along with
our novel multi-focal attention model to enforce
coherence, leading to significant performance im-
provements on CoNLL 2003 (Hoffart et al., 2011)
and TAC KBP 2010–2012 tasks (Ji et al., 2010;
Ji et al., 2011; Mayfield et al., 2012). In partic-
ular, we achieve a 20% relative reduction in er-
ror from Chisholm and Hachey (2015) on CoNLL,
and a 22% error reduction from Cucerzan (2012)
on TAC 2012. Our contributions thus consist of
defining a novel multi-focal attention model and
applying it successfully to an entity resolution sys-
tem.

2 Definitions and notation

We are given a document with n mentions, where
each mention i has a set of ni candidate entities
Ci = {ci,1, ..., ci,ni}. The goal is to assign a label
yi ∈ Ci to each mention.

Similarly to previous work, our approach to dis-
ambiguation relies on local and pairwise candidate
scores, which we denote by si(yi) and sij(yi, yj)
respectively. The local score is based only on lo-
cal evidence, such as the mention phrase and tex-
tual features, while the pairwise score is based
on the relatedness of the two candidates. In Sec-
tions 3.2 and 3.3 we discuss how these scores
may be parameterized and learned. Many systems
(Cucerzan, 2007; Milne and Witten, 2008; Kulka-
rni et al., 2009) simply hardwire pairwise scores.

Coherence models typically attempt to maxi-
mize a global objective function that assigns a
score to each complete labeling y = (y1, . . . , yn).
An example of such a function is the sum of all

singleton and pairwise scores for each label:2

g(y) =
∑
i

si(yi) +
∑
i

∑
j:j 6=i

sij(yi, yj). (1)

One disadvantage of this approach is that max-
imizing g corresponds to finding the MAP as-
signment of a general pairwise Markov random
field, and is hence NP hard for the general case
(Wainwright and Jordan, 2008). Another limi-
tation is that non-salient entities may be related
to very few other entities mentioned in the doc-
ument, and summing over all mentions may dilute
the evidence for such entities. In this paper we
explore alternative objectives, relying on attention
and tractable inference.

3 Attention model

We now describe our multi-focal attention model.
We first introduce the inference approach and op-
timization objective, and then provide details on
how scores are calculated and learned.

3.1 Inference

As noted earlier, the global score function in
Eq. (1) is hard to maximize. Here we simplify in-
ference by decomposing the task over mentions,
which makes it easy to integrate attention in terms
of both inference and learning.

3.1.1 Star model
We start by considering a simple attention-free
model in which inference is tractable, which we
call a star model. For a particular mention i, the
star model is a graphical model that contains yi,

2The scores usually depend not only on the labels, but also
on the input text. We omit this dependence for brevity.

622



y1 y2 y3 y4(a)

y1 y2 y3 y4(b)

y1 y2 y3 y4(c)

Figure 2: (a) The complete graph corresponding to
Eq. (1). (b) A star shaped subgraph corresponding
to y2. This will be used to obtaining the label y2.
(c) The star graph for y3.

all interactions between yi and other labels, and
no other interactions, as illustrated in Fig. 2.

While the star graph centered at i contains up to
n variables, we will only use it to infer the label of
mention i. Let qij(yi) be the support for label yi
from mention j, defined as follows:

qij(yi) = max
yj

sij(yi, yj) + sj(yj), (2)

and we also define qii(yi) = −∞ to simplify nota-
tion for later. We define the following score func-
tion for mention i:

fi(yi) = si(yi) +
∑
j:j 6=i

qij(yi) (3)

and predict the label yi = arg maxy fi(y).
Due to the structure of the star graph, infer-

ence is easy and can be done in O(nC2), where
C is the maximum number of candidates. A simi-
lar decomposition has previously been used in the
context of approximate learning for structured pre-
diction (Sontag et al., 2011). Note that we do
not view this approach as an approximation to the
global problem, but rather as our inference proce-
dure.

3.1.2 Adding attention
The score function in Eq. (3) aggregates pairwise
scores for each label yi over all mentions. In
this section, we restrict this to only consider K
mentions with the strongest relations to yi.3 Let
amxK(z) be the sum of the largest K values in the
vector z = (z1, . . . , zn). For each label yi, we
redefine the score function to be

fi(yi) = si(yi) + amxK(qi(yi)), (4)
3It is possible to relax this to allow up to K relations, but

we focus on exactly K for simplicity.

where qi(yi) = (qi1(yi), . . . , qin(yi)) and qij(yi)
is as defined in Eq. (2). The inference rule is again
yi = arg maxy fi(y), and the computational cost
is O(nC2 + n log n) since sorting is required.4

3.1.3 Soft attention
Previous work on attention has shown that it is ad-
vantageous to use a soft form of attention, where
the level of attention is not zero or one, but can
rather take intermediate values. Existing attention
models focus on a single object, such as a single
word (Bahdanau et al., 2014) or a single image
window (Xu et al., 2015). In such models, it is
natural to change the max function in the attention
operator to a soft-max. In our case, the attention
beam contains K elements, and we require a dif-
ferent notion of a soft-max, which we develop be-
low.

To obtain a soft version of the function
amxK(z), we first use an alternative definition.
Denote by S the set u = (u1, . . . , un) such that
0 ≤ ui ≤ 1 and

∑
i ui = K. Then amxK(z) is

equivalent to the optimization problem:

max
·u∈S

z · u (5)
The optimization problem above is a linear pro-
gram, whose solution is the sum of topK elements
of z as required. This follows since the optimal ui
can easily be shown to attain only integral values.

Given this optimization view of amxK(z) it is
natural to smooth it (Nesterov, 2005) by adding a
non-linearity to the optimization. Since the vari-
ables are non-negative, one possible choice is an
entropy-like regularizer. We shall see that this
choice results in a closed form solution, and also
recovers the standard soft-max case for K = 1.
Consider the optimization problem:

smxK(z) = max
u∈S

∑
i

ziui − β−1
∑
i

ui log ui,

(6)
where β is a tuned hyperparameter.5 The follow-
ing proposition provides a closed form solution for
smxK , as well as its gradient.
Proposition 3.1. Assume w.l.o.g. that z is sorted
such that z1 ≥ . . . ≥ zn. Denote by R the maxi-
mum index r ∈ {1, . . . ,K − 1} such that:

zr ≥ β−1 log
∑n

j=r+1 exp (βzj)
K − r (7)

4Note that if K < logn, we spend only nK instead of
n logn time.

5Note that −∑i ui log ui is different from the entropy
function since variables ui sum to K and not to 1.

623



If this doesn’t hold for any r, set R = 0. Then:

smxK(z) =
R∑
j=1

zj+
K −R
β

log

∑n
j=R+1 exp (βzj)

K −R
(8)

The function smxK(z) is differentiable with a gra-
dient v given by:

vi =

{
1 1 ≤ i ≤ R
(K −R) exp(βzi)∑n

j=R+1 exp(βzj)
R < i ≤ n

}
(9)

Proof is provided in the appendix.
As noted, K = 1 recovers the standard soft-

max function.6 As β → ∞, smxK will approach
the sum of the top K elements as expected. For
finite β we have a soft version of amxK .

Our soft attention based model will therefore
consider the soft-variant of Eq. (4):

fi(yi) = si(yi) + smxK(qi(yi)) , (10)

and maximize f(yi) to obtain the label.

3.2 Score parameterization
Thus far we assumed the singleton and pairwise
scores were given. We next discuss how to param-
eterize and learn these scores. As in other struc-
tured prediction work, we will assume that the
scores are functions of the features of the input x
and labels. Specifically, denote a set of singleton
features for mention i and label yi by φsi (x, yi) ∈
Rns and a set of pairwise features for mentions
i and j and their labels by φpij(x, yi, yj) ∈ Rnp .
Then the model has two sets of weights ws and
wp and the scores are obtained as a linear combi-
nation of the features. Namely:7

si(yi;ws) = ws · φsi (x, yi)
sij(yi, yj ;wp) = wp · φpij(x, yi, yj) ,

where we have explicitly denoted the dependence
of the scores on the weight vectors. See Sec. 6.2.2
for details on how the features are chosen. It is of
course possible to consider non-linear alternatives
for the score function, as in recent deep learning

6When we refer to the soft-max function, we mean the
function β−1 log

∑
exp (βai), which is an often used differ-

entiable convex upper bound of the max function (e.g., see
(Gimpel and Smith, 2010)). Soft-max sometimes also refers
to the activation function exp(ai)∑

j exp(aj)
. The latter is in fact the

gradient of the former (for β = 1).
7We again omit the dependence of the scores on the input

x for brevity.

parsing models (Chen and Manning, 2014; Weiss
et al., 2015), but we focus on the linear case for
simplicity.

3.3 Parameter learning

The parameters ws,wp are learned from labeled
data, as explained next. Since inference decom-
poses over mentions, we use a simple hinge loss
for each mention. Denote by y∗i the ground
truth label for mention i, and let si(yi) ≡
(si1(yi), . . . , sin(yi)). Then the hinge loss for
mention i is:

Li = max
yi

[si(yi) + smxK(si(yi))

−si(y∗i )− smxK(si(y∗i )) + ∆(yi, y∗i )]

where ∆(yi, y∗i ) is zero if yi = y
∗
i and one other-

wise. If there are unlabeled mentions in the train-
ing data, we add those to the star graph, and max-
imize over the unknown labels in the positive and
negative part of the hinge loss. The overall loss
is simply the sum of losses for all the mentions,
plus `2 regularization over ws,wp. We minimize
the loss using AdaGrad (Duchi et al., 2011) with
learning rate η = 0.1.

4 Single-link model

To motivate our modeling choices of using multi-
focal attention and decomposed inference, we ad-
ditionally consider a simple baseline model with
single-focus attention and global inference. In this
approach, which we name single-link, each men-
tion i attends to exactly one other mention that
maximizes the pairwise relation score. The cor-
responding objective can be written as

gSL(y) =
∑
i

(
si(yi) + max

j
sij(yi, yj)

)
(11)

where sij(yi, yj) = −∞ if there is no relation be-
tween yi and yj , and we set sii(yi, yi) = 0.

While exact inference in this model remains in-
tractable, we can find approximate solutions us-
ing max-sum belief propagation (Kschischang et
al., 2001). As a reminder, max-sum is an itera-
tive algorithm for MAP inference which can be
described in terms of messages sent from model
factors ga(ya) to each of their variables y ∈ ya.
At convergence, each variable is assigned to the
value that maximizes belief b(y), defined as the
sum of incoming messages. The message updates

624



have the following form:

µga→Y (y) = max
ya\y

[
ga(ya) +

∑
j 6=i

q
\a
j (yj)

]
(12)

where q\aj (yj) is the sum of all messages to yj
except the one from factor ga. While the single-
link model contains high-order factors over n vari-
ables, computing the messages from these factors
is tractable and requires sorting.

5 Related work

Ji (2016) and Ling et al. (2015) provide summaries
of recent ER research. Here we review work re-
lated to the three main facets of our approach.

5.1 Coherence scores
Several systems (Milne and Witten, 2008; Kulka-
rni et al., 2009; Hoffart et al., 2011) use the “Milne
and Witten” measure for relatedness between a
pair of entities, which is based on the number of
Wikipedia articles citing each entity page, and the
number of articles citing both; Cucerzan (2007)
has also relied on the Wikipedia category struc-
ture. Internal links from one entity page to an-
other in Wikipedia also provide direct evidence
of relatedness between them. Another (possibly
more noisy) source of information are Web pages
containing links (Singh et al., 2012) to Wikipedia
pages of both entities. Such links have been
used in several recent systems (Cheng and Roth,
2013; Chisholm and Hachey, 2015). Yamada et
al. (2016) train embedding vectors for entities, and
use them to define similarities.

5.2 Collective inference for ER
Optimizing most global coherence objectives is in-
tractable. Milne and Witten (2008) and Ferragina
and Scaiella (2010) decompose the problem over
mentions and select the candidate that maximizes
their relatedness score, which includes relations to
all other mentions. Hoffart et al. (2011) use an it-
erative heuristic to remove unpromising mention-
entity edges. Cucerzan (2007) creates a relation
vector for each candidate, and disambiguates each
entity to the candidate whose vector is most sim-
ilar to the aggregate (which includes both correct
and incorrect labels). Cheng and Roth (2013) use
an integer linear program solver and Kulkarni et
al. (2009) use a convex relaxation. Ratinov et al.
(2011) use relation scores as features in a rank-
ing SVM. Belief propagation without attention has

been used by Ganea et al. (2015). Personalized
PageRank (PPR) (Jeh and Widom, 2003) is an-
other tractable alternative, adopted by several re-
cent systems (Han and Sun, 2011; He et al., 2013;
Alhelbawy and Gaizauskas, 2014; Pershina et al.,
2015). Laplacian smoothing (Huang et al., 2014)
is closely related.

5.3 Attention models

Attention models have shown great promise in
several applications, including machine transla-
tion (Bahdanau et al., 2014) and image caption
generation (Xu et al., 2015). We address a new ap-
plication of attention, and introduce a significantly
different attention mechanism, which allows each
variable to focus on multiple objects. We develop
a novel smooth version of the multi-focus atten-
tion function, which generalizes the single focus
softmax-function. While some existing entity res-
olution systems (Jin et al., 2014; Lazic et al., 2015)
may be viewed as having attention mechanisms,
these are intended for single textual features and
not readily extensible to structured inference.

6 Experiments

6.1 Evaluation data

CoNLL: The CoNLL dataset (Hoffart et al.,
2011) contains 1393 articles with about 34K men-
tions, and the standard performance metric is
mention-averaged accuracy. The documents are
partitioned into train, test-a and test-b. Like most
authors, we report performance on the 231 test-b
documents with 4483 linkable mentions.

TAC KBP: The TAC KBP 2010, 2011, and
2012 evaluation datasets (Ji et al., 2010; Ji et al.,
2011; Mayfield et al., 2012) include 2250, 2250,
and 2226 mentions respectively, of which roughly
half are linkable to the reference KB. The compe-
tition evaluation includes NIL entities; participants
are required to cluster NIL mentions across docu-
ments so that all mentions of each unknown entity
are assigned a unique identifier. For these datasets,
we report in-KB accuracy, overall accuracy (with
all NILs in one cluster), and the competition metric
B3+F1 which evaluates NIL clustering.

6.2 Experimental setup

6.2.1 KB and entity aliases
Our KB is derived from the Wikipedia subset of
Freebase (Bollacker et al., 2008), with about 4M

625



entities. To obtain our mention prior (the proba-
bility of candidate entities given a mention), we
collect alias counts from Wikipedia page titles (in-
cluding redirects and disambiguation pages), Free-
base aliases, and Wikipedia anchor text. 99.31%
of CoNLL test-b mentions are covered by the KB,
and 96.19% include the gold entity in the candi-
dates.

We optionally use the mapping from aliases
to candidate entities released by Hoffart et al.
(2011), obtained by extending the “means” tables
of YAGO (Hoffart et al., 2013). When released,
it had 100% mention and gold recall on CoNLL,
i.e. every annotated mention could be mapped to
at least one entity, and the set of entities included
the gold entity. However, changes in canonical
Wikipedia URLs, accented characters and unicode
usually result in mention losses over time, as not
all URLs can be mapped to the KB (Hasibi et al.,
2016, Sec. 4).

For CoNLL only, we experiment with a third
alias-entity mapping derived from Hoffart et al.
(2011) by Pershina et al. (2015); we call it “HP”.
It is not known how candidates were pruned, but
it has high recall and very low ambiguity: only
12.6 on CoNLL test-b, compared to 22.34 in our
KB and 65.9 in YAGO. Unsurprisingly, using only
this source of aliases results in high accuracy on
CoNLL (Pershina et al., 2015; Yamada et al.,
2016).

Table 1 lists the statistics of the three alias-entity
mappings and some of their combinations on the
CoNLL test-b dataset. Table 2 provides the same
statistics on the TAC KBP datasets (restricted to
non-NIL mentions) for the of the YAGO+KB alias-
entity mapping.

6.2.2 Local and pairwise scores
Our baseline system is similar in design and accu-
racy to Plato (Lazic et al., 2015). Given the ref-
erent phrase mi and textual context features bi,
it computes the probability of a candidate entity
as pi(c) ∝ p(c|mi)p(bi|c). The system resolves
mentions independently and does not have an ex-
plicit coherence model; however, it does capture
some coherence information indirectly as referent
phrases are included as string context features. We
experiment with several versions of the mention
prior p(c|mi) as described in the previous section.
Scores for single-link model: In the single-link
model, we simply set the local score for mention i

Alias Mention Gold Uniq. Avg.
map recall recall % ambig.
KB 99.31 96.19 17.93 22.3
YAGO 97.17 96.30 15.50 65.9
+KB 99.84 99.51 16.28 73.6

HP 99.87 99.84 17.98 12.6
+KB 99.87 99.87 16.40 28.7

All 99.87 99.87 15.37 78.7

Table 1: Alias-entity map statistics on CoNLL
test-b, 4483 gold mentions. Mention recall is the
percentage of mentions with at least one known
entity; gold recall is the percentage of mentions
where the gold entity was included in the candi-
dates. Unique aliases map to exactly one entity.
The last column shows the number of candidates
averaged over test-b mentions.

Dataset Mention Gold Uniq. Avg.
recall recall % ambig.

TAC 2010 98.14 93.04 22.45 45.34
TAC 2011 98.40 89.23 27.82 49.13
TAC 2012 97.36 87.83 20.00 68.93

Table 2: YAGO+KB alias-entity map statistics on
the TAC KBP datasets, restricted to non-NIL men-
tions.

and candidate c to si(c) = ln
pi(c)

1−pi(c) , so that likely
candidates get positive scores. We set the pair-
wise score between two candidates heuristically to
sij(yi, yj) = ln o(yi, yj) + 2.3, where o(yi, yj) is
the number of outlinks from the Wikipedia page
of yi to the page of yj . We consider up to three
candidates for each mention for CONLL, and ten
for TAC; if the baseline probability of the top can-
didate exceeds 0.9, we only consider the top can-
didate. Including more candidates did not make
a difference in performance, as additional candi-
dates had low baseline scores and were almost
never chosen in practice.

Scores for attention model: Local features
φsi (x, yi) for the attention model are derived from
pi(c). As the attention models have no probabilis-
tic interpretation, we inject as features log pi(c)
and log(1 − pi(c)). We set log 0 = 0 by conven-
tion, and handle the case where log is undefined
by introducing two additional binary indicator fea-
tures for pi(c) = 0 and pi(c) = 1.

Edge features φpij are set based on three sources
of information: (1) number of Freebase relations

626



System Alias map In-KB acc. %
Lazic (2015) N/A 86.4
Our baseline KB 87.9
Single link KB 88.2
Attention KB 89.5
Chisholm (2015) YAGO 88.7
Ganea (2015) YAGO 87.6
Our baseline KB+YAGO 85.2
Single link KB+YAGO 86.6
Attention KB+YAGO 91.0
Our baseline KB+HP 89.9
Single link KB+HP 89.9
Attention KB+HP 91.7
Our baseline KB+HP* 91.9
Single link KB+HP* 92.1
Attention KB+HP* 92.7
Pershina (2015) HP 91.8
Yamada (2016) HP 93.1

Table 3: CoNLL test-b evaluation for recent com-
petitive systems and our models, using different
alias-entity maps. “KB+HP*” means we train and
score entities using KB+HP, but output entities
only in HP.

between yi and yj , (2) number of hyperlinks be-
tween Wikipedia pages of yi and yj (in either di-
rection), and (3) number of mentions of yi on the
Wikipedia page of yj and vice versa, after annotat-
ing Wikipedia with our baseline resolver. We cap
each count to five and encode it using five binary
indicator features, where the jth feature is set to 1
if the count is j and 0 otherwise. Additionally, for
each count c we add a feature log (1 + c). We also
added a binary feature which is one if yi = yj .

We train the scores for the attention model on
the 946 CoNLL train documents for CoNLL, and
on the TAC 2009 evaluation and TAC 2010 train-
ing documents for TAC.

6.3 Results

CoNLL: Table 3 compares our models to recent
competitive systems on CoNLL test-b in terms of
mention-averaged (micro) accuracy. We also note
the alias-entity map used in each system, as the
corresponding gold recall is an upper bound on
accuracy, and alias ambiguity determines the dif-
ficulty of the task. Therefore performance is not
strictly comparable between maps.

Our baseline is slightly better than Lazic et al.
(2015), but degrades after adding YAGO aliases

which increase ambiguity. The attention model
provides a substantial gain over the baseline,
and outperforms Chisholm and Hachey (2015) by
2.3% in absolute accuracy.

The extremely low ambiguity (Tab. 1) of the HP
alias mapping, coupled with guaranteed gold re-
call, makes the task too easy to be considered a
realistic benchmark. Although we match Pershina
et al. (2015) using KB+HP, for completeness, we
provide the performance of our system with candi-
date entities restricted to those in HP (KB+HP*),
but this is not equivalent to using only HP during
training and inference. With KB+HP*, we outper-
form Pershina et al. (2015), and are competitive
with recent unpublished work by Yamada et al.
(2016), which uses entity and word embeddings.
Including embeddings as features in our system
may lead to further gains.

TAC KBP: Table 4 shows our results for the
TAC KBP 2010, 2011, and 2012 evaluation
datasets, where we used the KB+YAGO entity-
alias map for all our experiments. To compute NIL
clusters required for B3 + F1, we simply rely on
the fact that our KB is larger than the TAC ref-
erence KB, similarly to previous work. We as-
sign a unique NIL label to all mentions of an en-
tity that is in our KB but not in TAC. For men-
tions that cannot be linked to our KB, we simply
use the mention string as the NIL identifier. Once
again, our attention models improve the perfor-
mance over the baseline system in nearly all exper-
iments, with multi-focus attention outperforming
single-link. Compared to prior work, we achieve
competitive performance on TAC 2010 and the
best results to date on TAC 2011 and TAC 2012.

Table 5 shows two examples from the TAC 2011
dataset in which our multi-focus attention model
improves over the baseline, along with the focus
mentions in the document.

6.4 Effect of K and β on attention

We set the size of the multi-focus attention beam
K based on accuracy on CoNLL test-a (for
CoNLL) and training accuracy (for TAC). Fig.
3 shows the effect of K on the performance on
CoNLL test-a dataset. Performance peaks for
K = 6, with a sharp decrease after K = 10. This
validates our central premise: all-pairs label cou-
pling may hurt accuracy.

In Sec. 3.1.3 we proposed an extension of soft-
max smoothing to the K attention case. In our

627



System In-KB Overall B3+F1
acc.(%) acc.(%)

Chisholm (2015) 80.7 - -
Ling (2015) - 88.8 -
Yamada (2016) 85.2 - -
Our baseline 84.5 87.6 83.0
Single link 84.3 87.5 82.8
Attention 87.2 88.7 84.4
Cucerzan (2011) - 86.8 84.1
Lazic (2015) 79.3 86.5 84.0
Ling (2015) - - 81.6
Our baseline 81.5 86.8 84.3
Single link 82.8 87.3 84.9
Attention 84.3 88.0 85.6
Cucerzan (2012) R1 72.0 76.2 72.1
Cucerzan (2012) R3 71.2 76.6 73.0
Lazic (2015) 74.2 76.6 71.2
Ling (2015) - - 66.7
Our baseline 78.8 80.3 76.9
Single link 79.7 80.7 77.3
Attention 82.4 81.9 78.9

Table 4: Results on the TAC 2010 (top), TAC
2011 (middle), and TAC 2012 bottom evaluation
datasets.

1 2 3 4 5 6 7 8 10 15
88

89

90

91

K

A
cc

ur
ac

y
(%

)

Figure 3: Effect of parameter K on entity linking
accuracy. Trained on CoNLL train and tested on
CoNLL test-a.

experiments we cross-validated over a wide range
of β values, including β = ∞ which corresponds
to taking the exact sum of K largest values. We
found that the optimal value in most cases was
large: β = 10, 100, or even∞. This suggests that
a hard attention model, where exactlyK mentions
are picked is adequate in the current settings.

7 Conclusion

We have described an attention-based approach to
collective entity resolution, motivated by the ob-
servation that a non-salient entity in a long doc-
ument may only have relations to a small subset

of other entities. We explored two approaches
to attention: a multi-focus attention model with
tractable inference decomposed over mentions,
and a single-focus model with global inference im-
plemented using belief propagation. Our empir-
ical results show that the methods results in sig-
nificant performance gains across several bench-
marks.

Experiments in varying the size of the atten-
tion beam K in the star-shaped model suggest that
multi-focus attention is beneficial. It is of course
possible to extend the global single-link model to
the multi-focus case, by modifying the model fac-
tors and resulting messages. However, the sim-
plicity of the star-shaped model, its empirical ef-
fectiveness, and ease of learning parameters make
it an attractive approach for easily incorporating
attention into existing resolution models. The
model can also readily be applied to other struc-
tured prediction problems in language processing,
such as selecting antecedents in coreference reso-
lution.

Deep learning has recently been used in mutli-
ple NLP applications, including parsing (Chen and
Manning, 2014) and translation (Bahdanau et al.,
2014). Learning the local and pairwise scores in
our model using a deep architecture rather than
a linear model would likely lead to performance
improvements. The star-shaped model is partic-
ularly amenable to this architecture, as it can be
implemented via a feed-forward sequence of op-
erations (including sorting, which can be imple-
mented with soft-max gates).

Finally, one may consider a more elaborate
model in which attention depends on the current
state of the system; for example, the state can sum-
marize the mention context. The dynamics of the
underlying state can be modeled by recurrent neu-
ral networks or LSTMs (Bahdanau et al., 2014).

In conclusion, we have shown that attention is
an effective mechanism for improving entity reso-
lution models, and that it can be implemented via
a simple inference mechanism, where model pa-
rameters can be easily learned.

8 Proof of Proposition 3.1

Begin with the optimization problem in Eq. (6).
Introduce the following Lagrange multipliers: λ
for the

∑
i ui = K constraint, and αi ≥ 0 for

the ui ≤ 1 constraint. We can ignore the ui ≥ 0
constraint, as it will turn out to be satisfied. Denote

628



Sentence with mention Entity Attn. focus mentions
Caroline has dropped her name base: Caroline (given name) Democratic Party
from consideration for the seat attn: Caroline Kennedy New York
that Hillary has left vacant. Robert Kennedy
Chris Johnson had just 13 tackles last base: Chris Johnson (running back) Oakland Raiders
season, and the Raiders currently have attn: Chris Johnson (cornerback) Oakland Raiders
have 11 defensive backs on their roster. Oakland Raiders

Table 5: Examples of gains by our algorithm, showing the resolved mention, the entities it resolves to in the baseline and the
attention models, and the mentions in the document that are attended to (here K = 3). In the first example, the baseline labels

the mention “Caroline” as the given name, whereas the attention model attends to mentions that identify it as the diplomat

Caroline Kennedy. In the second example, both models resolve “Chris Johnson” to football players, but the attention model

finds the correct one by attending to three mentions of his former team, the Oakland Raiders.

the corresponding Lagrangian by L(u, λ, α). We
will show the result by using the dual g(λ, α) =
maxu L(u, λ, α) and the fact that the solution of
Eq. (6) is minλ,α g(λ, α).

Maximizing L with respect to ui yields:

ui = eβzi−1+βλ−βαi (13)

From this we can obtain the convex dual g(λ, α),
and after minimizing over λ we arrive at:

g(α) = Kβ−1 log
∑

i e
βzi−βαi

K
+
∑
i

αi (14)

Next, we maximize the above with respect to α ≥
0. Introduce Lagrange multipliers γi for the con-
straint αi ≥ 0 and the corresponding Lagrangian
L̄(α, γ). We propose a solution for α, γ and show
that it satisfies the KKT conditions. Minimizing L̄
wrt α we can characterize the optimal γ as:

γi = −K e
βzi−βαi∑
i e
βzi−βαi + 1 (15)

Set αi as follows:

αi =

{
zi − 1β log

∑n
i=R+1 e

βzi

K−R 1 ≤ i ≤ R
0 R < i ≤ n

(16)
It can now be confirmed that the α, γ from Equa-
tions 16 and 15 satisfy the KKT conditions. Plug-
ging the α value into g(α) yields the solution
in the proposition. Differentiability follows from
Nesterov (2005) and the gradient is ui in Eq. (13).

References
[Alhelbawy and Gaizauskas2014] Ayman Alhelbawy

and Robert Gaizauskas. 2014. Graph ranking

for collective named entity disambiguation. In
Proc. 52nd Annual Meeting of the Association for
Computational Linguistics, ACL 14, pages 75–80.

[Bahdanau et al.2014] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2014. Neural machine
translation by jointly learning to align and translate.
arXiv preprint arXiv:1409.0473.

[Bollacker et al.2008] Kurt D. Bollacker, Colin Evans,
Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: a collaboratively created graph
database for structuring human knowledge. In Proc.
of the 2008 ACM SIGMOD International Confer-
ence on Management of Data, pages 1247–1250.
ACM.

[Bunescu and Pasca2006] Razvan C. Bunescu and Mar-
ius Pasca. 2006. Using encyclopedic knowledge for
named entity disambiguation. In Proc. 11th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, EACL 06.

[Chen and Manning2014] Danqi Chen and Christo-
pher D Manning. 2014. A fast and accurate de-
pendency parser using neural networks. In EMNLP,
pages 740–750.

[Cheng and Roth2013] Xiao Cheng and Dan Roth.
2013. Relational inference for wikification. In
EMNLP Conference, pages 1787–1796.

[Chisholm and Hachey2015] Andrew Chisholm and
Ben Hachey. 2015. Entity disambiguation with
web links. Transactions of the Association for
Computational Linguistics, 3:145–156.

[Cucerzan2007] Silviu Cucerzan. 2007. Large-scale
named entity disambiguation based on Wikipedia
data. In Proc. of EMNLP-CoNLL 2007, pages 708–
716.

[Cucerzan2012] Silviu Cucerzan. 2012. The MSR sys-
tem for entity linking at TAC 2012. In In Proc. of
the Text Analysis Conference, TAC 12.

[Dredze et al.2010] Mark Dredze, Paul McNamee,
Delip Rao, Adam Gerber, and Tim Finin. 2010.

629



Entity disambiguation for knowledge base popula-
tion. In Proc. of the 23rd International Conference
on Computational Linguistics, COLING 10, pages
277–285.

[Duchi et al.2011] John Duchi, Elad Hazan, and Yoram
Singer. 2011. Adaptive subgradient methods for on-
line learning and stochastic optimization. The Jour-
nal of Machine Learning Research, 12:2121–2159.

[Ferragina and Scaiella2010] Paolo Ferragina and Ugo
Scaiella. 2010. TAGME: on-the-fly annotation of
short text fragments (by Wikipedia entities). In
Proc. of the 19th ACM International Conference on
Information Knowledge and Management, CIKM
10, pages 1625–1628. ACM.

[Ganea et al.2015] Octavian-Eugen Ganea, Marina
Horlescu, Aurelien Lucchi, Carsten Eickhoff, and
Thomas Hofmann. 2015. Probabilistic bag-of-
hyperlinks model for entity linking. arXiv preprint
arXiv:1509.02301.

[Gimpel and Smith2010] Kevin Gimpel and Noah A
Smith. 2010. Softmax-margin crfs: Training log-
linear models with cost functions. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 733–736. As-
sociation for Computational Linguistics.

[Hachey et al.2013] Ben Hachey, Will Radford, Joel
Nothman, Matthew Honnibal, and James R. Curran.
2013. Evaluating entity linking with Wikipedia. Ar-
tificial Intelligence, 194(0):130 – 150.

[Han and Sun2011] Xianpei Han and Le Sun. 2011. A
generative entity-mention model for linking entities
with knowledge base. In Proc. of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, volume 1
of ACLHLT 11. ACL.

[Han and Sun2012] Xianpei Han and Le Sun. 2012.
An entity-topic model for entity linking. In
EMNLP-CoNLL, pages 105–115.

[Hasibi et al.2016] Faegheh Hasibi, Krisztian Balog,
and Svein Erik Bratsberg. 2016. On the repro-
ducibility of the TagMe entity linking system. In
Advances in Information Retrieval, pages 436–449.
Springer.

[He et al.2013] Zhengyan He, Shujie Liu, Mu Li, Ming
Zhou, Longkai Zhang, and Houfeng Wang. 2013.
Learning entity representation for entity disam-
biguation. In Proc. of the 51st Annual Meeting of
the Association for Computational Linguistics, ACL
13, pages 30–34.

[Hoffart et al.2011] Johannes Hoffart, Mohamed Amir
Yosef, Ilaria Bordino, Hagen Fürstenau, Man-
fred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan
Thater, and Gerhard Weikum. 2011. Robust disam-
biguation of named entities in text. In Proc. of the
2011 Conference on Empirical Methods in Natural
Language Processing, EMNLP11. ACL.

[Hoffart et al.2013] Johannes Hoffart, Fabian M
Suchanek, Klaus Berberich, and Gerhard Weikum.
2013. Yago2: A spatially and temporally en-
hanced knowledge base from wikipedia. Artificial
Intelligence, 194:28–61.

[Houlsby and Ciaramita2014] Neil Houlsby and Massi-
miliano Ciaramita. 2014. A scalable Gibbs sampler
for probabilistic entity linking. In Advances in In-
formation Retrieval, pages 335–346. Springer.

[Huang et al.2014] Hongzhao Huang, Yunbo Cao, Xi-
aojiang Huang, Heng Ji, and Chin-Yew Lin.
2014. Collective tweet wikification based on semi-
supervised graph regularization. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), pages 380–390, Baltimore, Maryland, June.
Association for Computational Linguistics.

[Jeh and Widom2003] Glen Jeh and Jennifer Widom.
2003. Scaling personalized web search. In Proceed-
ings of the 12th international conference on World
Wide Web, pages 271–279. ACM.

[Ji et al.2010] Heng Ji, Ralph Grishman, Hoa Trang
Dang, Kira Griffitt, and Joe Ellis. 2010. Overview
of the TAC 2010 knowledge base population track.
In Proc. of the 3rd Text Analysis Conference, TAC
10.

[Ji et al.2011] Heng Ji, Ralph Grishman, and Hoa Trang
Dang. 2011. Overview of the TAC 2011 knowledge
base population track. In Proc. of the 4th Text Anal-
ysis Conference, TAC 11.

[Ji2016] Heng Ji. 2016. Entity discovery and
linking and Wikification reading list. On-
line. http://nlp.cs.rpi.edu/kbp/2014/
elreading.html.

[Jin et al.2014] Yuzhe Jin, Emre Kiciman, Kuansan
Wang, and Ricky Loynd. 2014. Entity linking at
the tail: sparse signals, unknown entities, and phrase
models. In Proc. of the 7th ACM International Con-
ference on Web Search and Data Mining, WSDM
’14, pages 453–462, New York, NY, USA. ACM.

[Kataria et al.2011] Saurabh S. Kataria, Krishnan S.
Kumar, Rajeev R. Rastogi, Prithviraj Sen, and Srini-
vasan H. Sengamedu. 2011. Entity disambiguation
with hierarchical topic models. In Proc. of the 17th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 1037–1045. ACM.

[Kschischang et al.2001] Frank R Kschischang, Bren-
dan J Frey, and Hans-Andrea Loeliger. 2001. Factor
graphs and the sum-product algorithm. IEEE Trans-
actions on Information Theory, 47(2):498–519.

[Kulkarni et al.2009] Sayali Kulkarni, Amit Singh,
Ganesh Ramakrishnan, and Soumen Chakrabarti.
2009. Collective annotation of Wikipedia entities in
web text. In Proc. of the 15th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining,
pages 457–466. ACM.

630



[Lazic et al.2015] Nevena Lazic, Amarnag Subra-
manya, Michael Ringgaard, and Fernando Pereira.
2015. Plato: A selective context model for entity
resolution. Transactions of the Association for
Computational Linguistics, 3:503–515.

[Ling et al.2015] Xiao Ling, Sameer Singh, and
Daniel S Weld. 2015. Design challenges for
entity linking. Transactions of the Association for
Computational Linguistics, 3:315–328.

[Mayfield et al.2012] James Mayfield, Javier Artiles,
and Hoa Trang Dang. 2012. Overview of the TAC
2012 knowledge base population track. In Proc. of
the 5th Text Analysis Conference, TAC 12.

[Milne and Witten2008] David N. Milne and Ian H.
Witten. 2008. Learning to link with Wikipedia.
In Proc. of the 17th ACM Conference on Informa-
tion and Knowledge Management, CIKM 07, pages
509–518.

[Nesterov2005] Yu Nesterov. 2005. Smooth minimiza-
tion of non-smooth functions. Mathematical pro-
gramming, 103(1):127–152.

[Pershina et al.2015] Maria Pershina, Yifan He, and
Ralph Grishman. 2015. Personalized Page Rank
for named entity disambiguation. In Proc. 2015 An-
nual Conference of the North American Chapter of
the ACL, NAACL HLT 14, pages 238–243.

[Ratinov et al.2011] Lev-Arie Ratinov, Dan Roth, Doug
Downey, and Mike Anderson. 2011. Local and
global algorithms for disambiguation to Wikipedia.
In Proc. of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies, ACLHLT 11, pages 1375–
1384. ACL.

[Singh et al.2012] Sameer Singh, Amarnag Subra-
manya, Fernando Pereira, and Andrew McCallum.
2012. Wikilinks: A large-scale cross-document
coreference corpus labeled via links to Wikipedia.
Technical Report UM-CS-2012-015, University of
Massachusetts, Amherst.

[Sontag et al.2011] D. Sontag, O. Meshi, T. Jaakkola,
and A. Globerson. 2011. More data means less
inference: A pseudo-max approach to structured
learning. In R. Zemel and J. Shawe-Taylor, editors,
Advances in Neural Information Processing Systems
23, pages 2181–2189. MIT Press, Cambridge, MA.

[Wainwright and Jordan2008] Martin J Wainwright and
Michael I Jordan. 2008. Graphical models, expo-
nential families, and variational inference. Founda-
tions and Trends R© in Machine Learning, 1(1-2):1–
305.

[Weiss et al.2015] David Weiss, Chris Alberti, Michael
Collins, and Slav Petrov. 2015. Structured training
for neural network transition-based parsing. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language

Processing (Volume 1: Long Papers), pages 323–
333, Beijing, China, July. Association for Computa-
tional Linguistics.

[Xu et al.2015] Kelvin Xu, Jimmy Ba, Ryan Kiros,
Aaron Courville, Ruslan Salakhutdinov, Richard
Zemel, and Yoshua Bengio. 2015. Show, attend and
tell: Neural image caption generation with visual at-
tention. arXiv preprint arXiv:1502.03044.

[Yamada et al.2016] Ikuya Yamada, Hiroyuki Shindo,
Hideaki Takeda, and Yoshiyasu Takefuji. 2016.
Joint learning of the embedding of words and en-
tities for named entity disambiguation. CoRR,
abs/1601.01343.

631


