



















































Natural Answer Generation with Heterogeneous Memory


Proceedings of NAACL-HLT 2018, pages 185–195
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Natural Answer Generation with Heterogeneous Memory

Yao Fu and Yansong Feng
Institute of Computer Science and Technology, Peking University

The MOE Key Laboratory of Computational Linguistics, Peking University
{francis_yao,fengyansong}@pku.edu.cn

Abstract

Memory augmented encoder-decoder frame-
work has achieved promising progress for nat-
ural language generation tasks. Such frame-
works enable a decoder to retrieve from a
memory during generation. However, less re-
search has been done to take care of the mem-
ory contents from different sources, which are
often of heterogeneous formats. In this work,
we propose a novel attention mechanism to en-
courage the decoder to actively interact with
the memory by taking its heterogeneity into
account. Our solution attends across the gen-
erated history and memory to explicitly avoid
repetition, and introduce related knowledge to
enrich our generated sentences. Experiments
on the answer sentence generation task show
that our method can effectively explore het-
erogeneous memory to produce readable and
meaningful answer sentences while maintain-
ing high coverage for given answer informa-
tion.

1 Introduction

Most previous question answering systems fo-
cus on finding candidate words, phrases or sen-
tence snippets from many resources, and ranking
them for their users (Chu-Carroll et al., 2004; Xu
et al., 2016). Typically, candidate answers are col-
lected from different resources, such as knowledge
base (KB) or textual documents, which are often
with heterogeneous formats, e.g., KB triples or
semi-structured results from Information Extrac-
tion (IE). For factoid questions, a single answer
word or phrase is chosen as the response for users,
as shown in Table 1 (A1).

However, in many real-world scenarios, users
may prefer more natural responses rather than a
single word. For example, as A2 in Table 1, James
Cameron directed the Titanic. is more favorable
than the single name James Cameron. A straight-
forward solution to compose an answer sentence is
to build a template based model, where the answer

Q Who is the director of the Titanic?
A1 James Cameron
A2 James Cameron directed the Titanic.
A3 James Cameron directed it.
A4 James Cameron directed it in 1999.

Table 1: Answer sentences generated by different QA
systems

word James Cameron and topic word in the ques-
tion the Titanic are filled into a pre-defined tem-
plate (Chu-Carroll et al., 2004). But such systems
intrinsically lack variety, hence hard to generalize
to new domains.

To produce more natural answer sentences,
Yin et al. (2015) proposed GenQA, an encoder-
decoder based model to select candidate answers
from a KB styled memory during decoding to gen-
erate an answer sentence. CoreQA (He et al.,
2017b) further extended GenQA with a copy
mechanism to learn to copy words from the ques-
tion. The application of attention mechanism en-
ables those attempts to successfully learn sentence
varieties from the memory and training data, such
as usage of pronouns (A3 in Table 1). However,
since they are within the encoder-decoder frame-
work, they also encounter the well noticed rep-
etition issue: due to loss of temporary decoder
state, an RNN based decoder may repeat what
has already been said during generation (Tu et al.,
2016a,b).

Both GenQA and CoreQA are designed to work
with a structured KB as the memory, while in
most real-world scenarios, we require knowledge
from different resources, hence of different for-
mats. This knowledge may come from structured
KBs, documents, or even tables. It is admittedly
challenging to leverage a heterogeneous memory
in a neural generation framework, and it is not well
studied in previous works (Miller et al., 2016).
Here in our case, the memory should contain two
main formats: KB triples and semi-structured en-

185



tities from IE, forming a heterogeneous memory
(HM). The former is usually organized in is a
subject-predicate-object form, while, the latter is
usually extracted from textual documents, in the
form of keywords, sometimes associated with cer-
tain categories or tags oriented to specific tasks
(Bordes and Weston, 2016).

Miller et al. (2016) discuss different knowledge
representations for a simple factoid QA task and
show that classic structured KBs organized in a
Key-Value Memory style work the best. However,
dealing with heterogeneous memory is not trivial.
Figure 1 shows an example of generating answer
sentences from HM in a Key-Value style, which is
indeed more challenging than only using a classic
KB memory. Keys and values play different roles
during decoding. A director key indicates this slot
contains the answer. Same James Cameron val-
ues with different keys indicate duplication. The
decoder needs this information to proactively per-
form memory addressing. Because keys from doc-
uments are not canonicalized, e.g., doc directed
and doc director, they may lead to redundancy
with the structured KB, e.g., kb directed_by and
doc director. A decoder could repetitively output
a director twice simply because there are two dif-
ferent memory slots hit by the query, both indi-
cating the same director. This will make the the
repetition issue even worse.

Although many neural generation systems can
produce coherent answer sentences, they often fo-
cus on how to guarantee the chosen answer words
to appear in the output, while ignoring many re-
lated or meaningful background information in the
memory that can further improve user experiences.
In real-world applications like chatbots or personal
assistants, users may want to know not only the
exact answer word, but also information related
to the answers or the questions. This informa-
tion is potentially helpful to attract users’ atten-
tion, and make the output sentences more natural.
For example in Table 1 (A4), the extra 1999 not
only enriches the answer with the movie’s release
year, but also can act as a clue to help distinguish
ambiguous candidate answers, e.g., Titanic (1999)
and Titanic (HD, 2016).

In this paper, we propose a sequence to se-
quence model tailing for heterogeneous memory.
In order to bridge the gap between decoder states
and memory heterogeneity, we split decoder states
into separate vectors, which can be used to address

Figure 1: An example qa-pair with heterogeneous
memory

different memory components explicitly. To avoid
redundancy, we propose the Cumulative Atten-
tion mechanism, which uses the context of the de-
coder history to address the memory, thus reduces
repetition at memory addressing time. We conduct
experiments on two WikiMovies datasets, and
experimental results show that our model is able
to generate natural answer sentences composed of
extra related facts about the question.

2 Related Work

Natural Answer Generation with Sequence to
Sequence Learning: Sequence to sequence mod-
els (with attention) have achieved successful re-
sults in many NLP tasks (Cho et al., 2014; Bah-
danau et al., 2014; Vinyals et al., 2015; See et al.,
2017). Memory is an effective way to equip
seq2seq systems with external information (We-
ston et al., 2014; Sukhbaatar et al., 2015; Miller
et al., 2016; Kumar et al., 2015). GenQA (Yin
et al., 2015) applies a seq2seq model to gener-
ate natural answer sentences from a knowledge
base, and CoreQA (He et al., 2017b) extends it
with copying mechanism (Gu et al., 2016). But
they do not consider the heterogeneity of the mem-
ory, only tackle questions with one single answer
word, and do not study information enrichment.
Memory and Attention: There are also increas-
ing works focusing on different memory repre-
sentations and the interaction between the de-
coder and memory, i.e., attention. Miller et al.
(2016) propose the Key-Value style memory to ex-
plore textual knowledge (both structured and un-
structured) from different sources, but they still
utilize them separately, without a uniform ad-
dressing and attention mechanism. Daniluk et al.
(2017) split the decoder states into key and value
representation, and increase language modeling

186



performance. Multiple variants of attention mech-
anism have also been studied. Sukhbaatar et al.
(2015) introduce multi-hop attention, and extend
it to convolutional sequence to sequence learn-
ing (Gehring et al., 2017). Kumar et al. (2015)
further extend it by using a Gated Recurrent Unit
(Chung et al., 2014) between hops. These models
show that multiple hops may increase the model’s
ability to reason. These multi-hop attention is
performed within a single homogeneous memory.
Our Cumulative Attention is inspired by them, but
we utilize it cross different memory, hence can ex-
plicitly reason over different memory components.

Conditional Sentence Generation: Controllable
sentence generation with external information is
wildly studied from different views. From the task
perspective, Fan et al. (2017) utilize label informa-
tion for generation, and tackle information cover-
age in a summarization task. He et al. (2017a) use
recursive Network to represent knowledge base,
and Bordes and Weston (2016) track generation
states and provide information enrichment, both
are in a dialog setting. In terms of network ar-
chitecture, Wen et al. (2015) equip LSTM with a
semantic control cell to improve informativeness
of generated sentence. Kiddon et al. (2016) pro-
pose the neural checklist model to explicitly track
what has been mentioned and what left to say by
splitting these two into different lists. Our model is
related to these models with respect to information
representation and challenges from coverage and
redundancy. The most closely related one is the
checklist model. But it does not explicitly study
information redundancy. Also, the information we
track is heterogeneous, and we track it in a differ-
ent way, i.e. using Cumulative attention.

Due to loss of states across time steps, the de-
coder may generate duplicate outputs. Attempts
have been made to address this problem. Some ar-
chitectures try to utilize History attention records.
See et al. (2017) introduce a coverage mecha-
nism, and Paulus et al. (2017) use history atten-
tion weights to normalize new attention. Others
are featured in network modules. Suzuki and Na-
gata (2017) estimate the frequency of target words
and record the occurrence. Our model shows that
simply attending to history decoder states can re-
duce redundancy. Then we use the context vector
of attention to history decoder states to perform
attention to the memory. Doing this enables the
decoder to correctly decide what to say at mem-

ory addressing time, rather than decoding time,
thus increasing answer coverage and information
enrichment.

3 Task Definition

Given a question q and a memory M storing re-
lated information, our task is to retrieve all the an-
swer words from the memory, generate an answer
sentence x, and use the rest information as enrich-
ment.

Answer Coverage is the primary objective of
our task. Since many answers contain multiple
words, the system needs to cover all the target
words.

Information Redundancy is one challenge for
this task. It is well noticed that the decoder lan-
guage model may lose track of its state, thus re-
peating itself. Also, the decoder needs to rea-
son over the semantic gap between heterogeneous
memory slots, figuring out different keys may re-
fer to the same value. These two kinds of redun-
dancy should both be addressed.

Information Enrichment is another challenge.
It requires the decoder to interact with the mem-
ory effectively and use the right word to enrich the
answer.

The tradeoff between redundancy and cov-
erage/enrichment is one of our main considera-
tions. This is because when the decoder generates
a word, it either generates a new word or a men-
tioned word. The more answer words and infor-
mation enrichment are considered, the more likely
the model repeats what it has already generated.

4 Our Model

Our model consists of the question encoder, the
heterogeneous memory, and the decoder. The en-
coder embeds the question into a vector represen-
tation. The decoder reads questions, retrieves the
memory, and generates answer sentences.

We use a Long Short Term Memory (LSTM)
(Hochreiter and Schmidhuber, 1997) for question
encoding and encode the question into an embed-
ding. It takes every word embedding (q1, q2...qn)
of question words as inputs, and generates hidden
states st = LSTMenc(qt, st−1). These s are later
used for decoder’s attention. The last hidden state
sn is used as the vector representation of the ques-
tion, and is later put into the initial hidden state of
the decoder.

187



Figure 2: The Decoder with Heterogeneous States

We use a key-value memory M to represent
the information heterogeneity. In our experiments,
we study information from KB, topic words, and
words extracted from documents. The memory
is formatted as ((m⟨K⟩0 ,m

⟨V ⟩
0 ), (m

⟨K⟩
1 ,m

⟨V ⟩
1 ) ...

(m
⟨K⟩
n ,m

⟨V ⟩
n )), where m

⟨K⟩
i and m

⟨V ⟩
i are respec-

tively the key embedding and word embedding for
the i-th memory slot. The vocabulary for keys
V key consists of all predicates in the KB, and all
tags we use to classify the value words (e.g: di-
rector, actor, or release_year). The vocabulary for
values V val consists all related words from web
documents, subjects and objects from the KB. This
memory is later used in two ways: 1. the decoder
uses its previous hidden state to perform atten-
tion and generate context vectors. 2. the decoder
uses the updated hidden states as pointers (Vinyals
et al., 2015) to retrieve the memory and copy the
memory contents into the decoder’s output.

4.1 Decoder with Heterogeneous States

As in the standard encoder-decoder architecture
with attention, the word embedding of the de-
coder’s previous time step xt and context vector
ct is fed as the input of the next time step, and the
hidden state ht is updated then. The initial hidden
state is the question embedding concatenated with
average memory key and value:

ht = LSTMdec(xt, ct, ht−1)

h0 = [sn, avg(m
⟨K⟩), avg(m⟨V ⟩)]

where [·, ·] denotes concatenation.
As shown in figure 2, to match the key-value

memory representation, we use three linear trans-
formations to convert the decoder’s current ht into

h
⟨N⟩
t , h

⟨K⟩
t , and h

⟨V ⟩
t :

h
⟨N⟩
t = Wnht

h
⟨K⟩
t = Wkht

h
⟨V ⟩
t = Wvht

where the W s are initialized as identity matrix
I = diag(1, 1...1). h⟨N⟩t will be projected to nor-
mal word vocabulary V norm to form a distribu-
tion p⟨N⟩t . h

⟨K⟩
t and h

⟨V ⟩
t will be used as point-

ers to perform attention to memory keys m⟨K⟩ and
values m⟨V ⟩, respectively, and forms two distribu-
tions: p⟨MK⟩t and p

⟨MV ⟩
t . We use the average of

the two as distribution over the memory: p⟨M⟩t =
(p

⟨MK⟩
t + p

⟨MV ⟩
t )/2. By doing this, we bridge the

decoder’s semantic space with the memory’s se-
mantic space, and explicitly maintains heterogene-
ity.

The decoder then uses a gating mechanism g =
sigmoid(Wght+bg) to decide whether the output
xt comes from the normal vocabulary or the mem-
ory. By mixing p⟨N⟩t and p

⟨M⟩
t with g, we get the

distribution for the next decoder output:

P (xt|q,M, x0, x1, ...xt−1) = (1)
g × P (Xt = wk|q,M, x0, x1...xt−1) +
(1− g)× P (Xt = mk|q,M, x0, x1...xt−1)

where

P (Xt = wk|q,M, x0, x1...xt−1) = p⟨N⟩t
P (Xt = mk|q,M, x0, x1...xt−1) = p⟨M⟩t

The three hs are then recorded as history states
for later decoding time steps to perform the self-
attention. We will explain this in the next section.

4.2 Cumulative Attention
As shown in Figure 3, our Cumulative Attention
mechanism is exploited similarly to a multi-hop
attention (Sukhbaatar et al., 2015). The difference
is that the multi-hop attention uses context vector
over one single memory at different hops, while
our Cumulative Attention utilizes the context vec-
tor to query different memories. As shown in the
left part of Figure 3, the decoder first performs
self-attention to its history h⟨N⟩t , h

⟨K⟩
t , and h

⟨V ⟩
t ,

and generates corresponding context vectors c as:

c
⟨HN⟩
t = attn(ht−1, hist(h

⟨N⟩
t ))

c
⟨HK⟩
t = attn(ht−1, hist(h

⟨K⟩
t )

c
⟨HV ⟩
t = attn(ht−1, hist(h

⟨V ⟩
t ))

188



Figure 3: The Cumulative Attention Mechanism

where c = attn(query,memory) denotes the at-
tention function (Bahdanau et al., 2014), and the
decoder’s history states are defined as:

hist(h
⟨N⟩
t ) = (h

⟨N⟩
0 , h

⟨N⟩
1 , ...h

⟨N⟩
t−1)

hist(h
⟨K⟩
t ) = (h

⟨K⟩
0 , h

⟨K⟩
1 , ...h

⟨K⟩
t−1)

hist(h
⟨V ⟩
t ) = (h

⟨V ⟩
0 , h

⟨V ⟩
1 , ...h

⟨V ⟩
t−1)

The overall context vector is obtained through
concatenation : c⟨H⟩t = [c

⟨HN⟩
t , c

⟨HK⟩
t , c

⟨HV ⟩
t ],

which is then used together with h⟨K⟩ and h⟨V ⟩ to
perform attention to m⟨K⟩ and m⟨V ⟩, respectively:

c
⟨MK⟩
t = attn([h

⟨K⟩
t−1 , c

⟨H⟩
t ],m

⟨K⟩)

c
⟨MV ⟩
t = attn([h

⟨V ⟩
t−1, c

⟨H⟩
t ],m

⟨V ⟩)

where m⟨K⟩ = (m⟨K⟩0 ,m
⟨K⟩
1 ...m

⟨K⟩
n ) and m⟨V ⟩ =

(m
⟨V ⟩
0 ,m

⟨V ⟩
1 ...m

⟨V ⟩
n ), as shown in the right part of

Figure 3.
The decoder also performs attention to the ques-

tion to get context vector c⟨Q⟩t , as in the standard
seq2seq attention model.

At time step t, all context vectors are concate-
nated: ct = [c

⟨Q⟩
t , c

⟨H⟩
t , c

⟨MK⟩
t , c

⟨MV ⟩
t ] to form the

current input to the decoder. The decoder takes the
context vector, the previous output, and the previ-
ous state to update its state, then generates a distri-
bution for the next token, as shown in Section 4.1.
We use the greedy decoding approach and choose
the word with the highest probability as the current
output.

For optimization, we jointly optimize the nega-
tive log-probability of the output sentence and the
cross entropy H for gate g. Since g is the proba-
bility about whether the current output comes from
the memory or the vocabulary, we can extract the
label for g by matching sentence words with the

memory. The overall loss function L can be writ-
ten as:

L = −
N∑

t=1

log(P (xt|q,M, x0...xt−1)) +H(g, ĝ)

We optimize L with gradient descent based opti-
mizers.

5 Experiments

Our experiments are designed to answer the fol-
lowing questions: (1) whether our model can prop-
erly utilize heterogeneous memories to generate
readable answer sentences, (2) whether our model
can cover all target answers during generation, (3)
whether our model can introduce related knowl-
edge in the output while avoiding repetition.

5.1 Datasets
Our task requires a question, and a memory stor-
ing all the answer words and related knowledge as
input, and produces a natural, readable sentence
as the output. Unfortunately, there is no existing
dataset that naturally fits to our task. We thus tailor
the WikiMovies1 dataset according to our re-
quirements. This WikiMovies dataset was orig-
inally constructed for answering simple factoid
questions, using memory networks with differ-
ent knowledge representations, i.e., structured KB
(KB entries in Table 2), raw textual documents
(Doc), or processed documents obtained through
information extraction (IE), respectively. The first
is in the classic subject-predicate-object format.
The second contains sentences from Wikipedia
and also sentences automatically generated from
predefined templates. The third is in the subject-
verb-object format, collected by applying off-the-
shell information extractor to all sentences.

1http://fb.ai/babi

189



The original data format
Question Who directed the film Blade Runner?
KB
entries

Blade Runner directed_by Ridley Scott

Blade Runner release_year 1982
Blade Runner written_by Philip K. Dick

IE year 1982
starred Harrison Ford

Doc Blade Runner is a 1982 American film di-
rected by Ridley Scott and starring Harri-
son Ford.
It is directed by Ridley Scott and written by
Philip K. Dick.
It comes out in 1982.

Answer Ridley Scott
Our modified data format

Question Who directed the film Blade Runner?
Memory Key Value

directed_by Ridley Scott
release_year 1982
written_by Philip K. Dick
movie Blade Runner
year 1982
starred Harrison Ford

Answer Blade Runner is a 1982 American film di-
rected by Ridley Scott and starring Har-
rison Ford.

Table 2: The data format of WikiMovies used in our
experiment.

As shown in Table 2, we treat each ques-
tion in WikiMovies with its original answer
(usually one or more words) as a QA pair, and
one of the question’s supportive sentences (ei-
ther from Wikipedia or templates) as its gold-
standard answer sentence. For each question, the
memory will contain all knowledge triples about
the question’s topic movie from the KB entries,
and also include entities and keywords extracted
from its IE portion. For each entry in KB en-
tries, we use the predicate as the key and the
object as value to construct a new entry in our
memory. For those from IE, we keep the ex-
tracted tags as the key and entities or other ex-
pressions as the value. Given a question, if an en-
tity/expression in the memory is not the answer,
it will be treated as information enrichment. Ac-
cording to whether the supportive sentences are
generated by predefined templates or not, we split
the dataset into WikiMovies-Synthetic and
WikiMovies-Wikipedia.

The resulting WikiMovies-Synthetic in-
cludes 115 question patterns and 194 answer pat-
terns, covering 10 topics, e.g., director, genre, ac-
tor, release year, etc. We follow its original data
split, i.e., 47,226 QA-pairs for training, 8,895 for
validation and 8,910 for testing.

In WikiMovies-Wikipedia, answer sen-
tences are extracted from Wikipedia, admittedly
noisy in nature. Note that there are more than
10K Wikipedia sentences that cannot be paired
with any questions. We thus left their questions
as blank and treat it as a pure generation task from
a given memory, which can be viewed as a form
of data augmentation to improve sentence variety.
We split WikiMovies-Wikipedia the dataset
randomly into 47,309 cases for training, 4,093 for
testing and 3,954 for validation. We treat normal
words occurring less than 10 times as UNK, and,
eventually, have 24,850 normal words and 37,898
entity words. We cut the maximum length of an-
swer sentences to 20, and the maximum memory
size to 10, which covers most cases in both syn-
thetic and Wikipedia datasets.

5.2 Metrics
We evaluate our answer sentences in terms of an-
swer coverage, information enrichment, and re-
dundancy. For cases with only one answer word,
we design Csingle to indicate the percentage of
cases being correctly answered. Cases with more
than one answer word are evaluated by Cpart, i.e.,
the percentage of answer words covered correctly,
and Cperfect is the percentage of cases whose an-
swers are perfectly covered. Here, the definition of
coverage is similar in spirit with the conventional
recall as both measure how many gold words are
included in the output. Specifically, Cpart is es-
sentially the same as recall with respect to its own
cases. Note that perfect coverage is the most diffi-
cult, while single coverage is the easiest one. For
Enrich, we measure the number of none-answer
memory items included in the output. Regarding
Redundancy, we calculate the times of repetition
for memory values in the answer sentence. We
also compute BLEU scores (Papineni et al., 2002)
on the WikiMovies-Wikipedia, as an indi-
cator of naturalness, to some extent.

5.3 Comparison Models
We compare our full model (HS-CumuAttn) with
state-of-the-art answer generation models and
constrained sentence generation models. Our first
baseline is GenQA (Yin et al., 2015), a standard
encoder-decoder model with attention mechanism.
We equip it with our Key-Value style heteroge-
neous memory. We also compare with its two
variants. HS-GenQA: we split its decoder state
into heterogeneous representations. The other one,

190



Model Redundancy Csingle Cpart Cperfect Enrich
GenQA 0.1109 91.25% 69.19% 38.92% 0.1535
HS-GenQA 0.1218 94.10% 76.47% 50.10% 0.1951
GenQA-AttnHist 0.1280 95.99% 73.44% 44.94% 0.1903
CheckList 0.1176 93.80% 76.32% 50.04% 0.1963
HS-AttnHist 0.1295 97.17% 77.90% 51.55% 0.1996
HS-CumuAttn 0.0983 98.15% 77.28% 50.79% 0.1665

Table 3: Results on the WikiMovies-Synthetic dataset

Model BLEU Redundancy Cpart Cperfect Enrich
GenQA 42.50 0.2603 62.80% 18.24% 0.5903
CheckList 43.69 0.2744 63.42% 18.23% 0.6094
HS-CumuAttn 44.97 0.2385 64.06% 19.09% 0.6218

Table 4: Results on the WikiMovies-Wikipedia dataset

GenQA-AttnHist, is enhanced with a history at-
tention during decoding.

CheckList (Kiddon et al., 2016) is the state-of-
the-art model for generating long sentences with
large agenda to mention. It keeps words that have
been mentioned and words to mention using two
separate records, and updates the records dynam-
ically during decoding. To adapt to our task, we
modify CheckList with a question encoder and a
KV memory.

We also compare with one variant of our own
model, HS-AttnHist, which does not benefit from
the Cumulative Attention.

5.4 Implementation
Our model is implemented with the Tensorflow
framework2, version 1.2. We use the Adam opti-
mizer (Kingma and Ba, 2014) with its default set-
ting. The embedding dimension is set to be 256,
as is the LSTM state size. We set the batch size to
128 and train the model up to 80 epochs.

As mentioned, there is a tradeoff between Cov-
erage/Enrichment and Redundancy. To set up a
more fair comparison for different models, we
ask the control group to reach a comparable level
of Redundancy, i.e., approximately 0.11-0.12 on
WikiMovies-Synthetic and 0.26-0.27 on
WikiMovies-Wikipedia. Keeping the Re-
dundancy in around the same bucket, we compare
their Coverage and Enrichment.

5.5 Results and Discussion
Let us first look at the performance on the
Synthetic set in Table 3. GenQA is origi-
nally proposed to read only one single fact dur-
ing decoding, so it is not surprising that it has
the lowest answer coverage (38.92% Cperfect)

2www.tensorflow.org

Question the movie Torn Curtain starred who?
Memory 0 actor Julie Andrews

1 starred_actors Julie Andrews
2 starred_actors Paul Newman
3 movie Torn Curtain
4 year 1966
5 director Alfred Hitchcock
6 actor Paul Newman

GenQA It stared Julie Andrews0 and
Julie Andrews0 and and.

CheckList Torn Curtain3 is a 19664 Ameri-
can film starring Paul Newman2 and
Julie Andrews0 and Julie Andrews1.

HS-
CumuAttn

Torn Curtain3 is a 19664 Amer-
ican political thriller film directed
by Alfred Hitchcock5, starring
Paul Newman2 and Julie Andrews0.

Table 5: Example sentences generated by different
models, where an underlined bold phrase is the value
of a memory slot selected from the memory by its cor-
responding generation model, and its subscript number
is the index of this slot in the memory.

and information enrichment (0.1535). After split-
ting the decoder state, HS-GenQA obtains sig-
nificant improvement in both coverage (50.10%
Cperfect) and enrichment (0.1952). When con-
sidering history for attention, GenQA-AttnHist
achieves even better coverage ( +3.% in Cpart and
+5% in Cperfect). By combining these two mecha-
nisms, HS-AttnHist achieves the best perfect cov-
erage, 51.55%. Although CheckList is not origi-
nally designed for our task, it still gives a strong
performance (50.04% Cperfect and 0.1963 enrich-
ment), at a slightly lower redundancy (0.1176). Fi-
nally, our full model, HS-CumuAttn, achieves the
best single coverage 98.15%, and comparable par-
tial/perfect coverage, with the lowest redundancy
(0.0983). Due to the lower level of redundancy,
HS-CumuAttn does not include as much enrich-
ment as other strong models, but still outperforms
GenQA.

191



Question 1 who starred in Cemetery Man ?
Memory 0 ans_actor Rupert Everett 1 ans_actor Anna Falchi

2 starred_actors Rupert Everett 3 starred_actors Anna Falchi
4 movie Cemetery Man

Answer The film stars Rupert Everett0 , _UNK , and Anna Falchi1 .
Question 2 who was Dying Breed written by ?
Memory 0 ans_release_year 2008 1 ans_writer Jody Dwyer

2 ans_actor Nathan Phillips 3 ans_writer Leigh Whannell
4 written_by Jody Dwyer 5 movie Dying Breed

Answer Dying Breed5 is a 20080 Australian horror film that was directed by Jody Dwyer1 and stars
Leigh Whannell3 and Nathan Phillips2.

Question 3 who is the director that directed Livid ?
Memory 0 ans_director Julien Maury 1 directed_by Alexandre Bustillo

2 ans_release_year 2011 3 ans_director Alexandre Bustillo
4 movie Livid 5 directed_by Julien Maury
6 ans_language French

Answer Livid4 ( ) is a 20112 French6 supernatural horror film directed and written by Julien Maury0 and
Alexandre Bustillo3.

Question 4 Drag Me to Hell , when was it released?
Memory 0 ans_director Sam Raimi 1 ans_wiki Scream

2 release_year 2009 3 ans_genre Horror
4 ans_release_year 2009 5 movie Drag Me to Hell

Answer Scream1 is a 20094 film
Question 5 the movie Lights in the Dusk starred who ?
Memory 0 starred_actors Janne Hyytiäinen 1 ans_language Finnish

2 starred_actors Maria Järvenhelmi 3 ans_actor Janne Hyytiäinen
4 starred_actors Ilkka Koivula 5 movie Lights in the Dusk
6 ans_actor Ilkka Koivula 7 ans_release_year 2006
8 ans_actor Maria Järvenhelmi

Answer Lights in the Dusk5 ( , ) is a 20067 Finnish1 drama film starring Janne Hyytiäinen3 , Ilkka Koivula6 and
Maria Järvenhelmi8 .

Table 6: Example answers generated by our model. In an answer sentence, an underlined phrase is the value of
a memory slot selected from the memory by our model, and the subscript number is the index of this slot in the
memory.

Figure 4: Two methods of using context of history to
address the memory

We further break down the contributions from
different mechanisms. Compared to vanilla
GenQA, HS-GenQA splits the decoder states, thus
improves the decoder’s memory addressing pro-
cess by performing attention separately, leading to
improvements in both coverage and enrichment.
Improvements of GenQA-AttnHist are of a differ-
ent rationale. Looking at the history enables the
decoder to avoid what are already said. Compared
with HS-GenQA, GenQA-AttnHist improves En-
richment by avoiding repetition when introducing
related information, while, HS-GenQA improves
Enrichment by better memory addressing to select
proper slots. Combining the two mechanisms to-

gether gives HS-AttnHist the best performance in
Enrichment. However, HS-AttnHist still suffers
from the repetition issue, to certain extent. Be-
cause when choosing memory content, there is no
explicit mechanism to help the decoder to avoid
repetitions according to the history (left of Figure
4). Therefore, a generated word may still be cho-
sen again at the memory addressing step, leaving
all the burden of avoiding repetition to the genera-
tion step. Our Cumulative Attention mechanism is
designed to utilize the context vector of the history
to address the memory, thus helps avoid choosing
those already mentioned slots at memory address-
ing time (right of Figure 4), leading to almost the
best coverage with the lowest redundancy.

Now we compare the three main models,
GenQA, CheckList and our HS-CumuAttn
on WikiMovies-Wikipedia (Table 4),
which is admittedly more challenging than
WikiMovies-Synthetic. We skip the
Csingle metrics here since most questions in
WikiMovies-Wikipedia contain more
than one answer word. It is not surprising that

192



CheckList, with a lower redundancy, still out-
performs GenQA in almost all metrics, except
Cperfect, since CheckList is originally designed
to perform well with larger agenda/memory and
longer sentences. On the other hand, our model,
HS-CumuAttn, achieves the best performance
in all metrics. Although the BLEU score is not
designed to fully reflect the naturalness, it still
indicates that our model can output sentences
that share more n-gram snippets with reference
sentences and are more similar to those composed
by humans.

Case Study and Error Analysis Table 5 pro-
vides the system outputs from different models for
an example question. We can see that GenQA may
lose track of the decoder history, and repeat itself
(and and), because there is no explicit mechanism
to help avoid repetition. Also, it lacks informative-
ness and may not utilize other information stored
in the memory. CheckList keeps records of what
have been said and what are left to mention, thus
reaches a good answer coverage. But its decoder is
unable to explicitly address separate components
within one memory slot, so it may not realize that
the two Julie Andrewss are essentially the same
person. HS-CumuAttn is able to find all the an-
swer words correctly and also include the director
into the sentence. After generating Paul Newman,
the Cumulative Attention mechanism enables the
model to realize that Paul Newman in slot 2 has
been said, and Paul Newman in slot 6 is the same
as slot 2, so it should not choose the 6th slot again.
Rather it should move to Julie Andrews. Although
the decoder may figure out the two Paul Newman
are the same during decoding, the Cumulative At-
tention can explicitly help make the clarification
during memory addressing. Intuitively, the atten-
tion across memory and history induces a stronger
signal for the decoder to gather the right informa-
tion.

Table 6 lists more typical imperfect output from
our model. In question 1, there is considerable re-
dundancy in the memory, but our decoder is still
able to avoid repeatedly choosing the same enti-
ties from difference sources, though it produces a
"_UNK" showing a slight incoherence. We think it
comes from the gate g as it fails to decide that the
current word should come from the memory. In
question 2, the model correctly chooses the mem-
ory slot, but outputs the word "directed" while
the correct word should be "written". This also

shows an word choice inconsistency between the
language model and the memory retrieval. Ques-
tion 3 makes the same mistake, where it indeed
chooses the right answer, but adds an incorrect
word "written". We also observe a pair of addi-
tional parentheses, which are often used to acco-
modate movie tags, but we do not see any tags in
this memory, so it has to be left blank. Question
4 shows an incorrect memory retrieval, where the
decoder should have chosen slot 5 as the movie
name. Question 5 is generally good enough, ex-
cept the same parenthesis error as in question 4.

It is also interesting to see additional de-
scriptions like "Australian", "supernatural" and
"drama" in question 2, 3, and 5, introduced by
the language model, rather than the memory. Al-
though our model prevents repetition and obtains
general naturalness, it cannot guarantee that the
decoder can precisely use the right language to de-
scribe the memory information. We see the gen-
eral readability of these sentences, yet they are
still not as good as human composed ones. It is
fairly subtle for the decoder to collaborate with the
memory in different levels of semantics. The se-
mantic coherency and word choice consistency is
still a challenge in natural language generation.

6 Conclusion and Future Work

In this paper, we propose a novel mechanism
within an encoder-decoder framework to enable
the decoder to actively interact with a memory by
taking its heterogeneity into account. Our solu-
tion can read multiple memory slots from different
sources, attend across the generated history and
the memory to explicitly avoid repetition, and en-
rich the answer sentences with related information
from the memory. In the future, we plan to extend
our work through 1) investigating more sophisti-
cated structures in the memory such as knowledge
graph, 2) solving more complex questions, such
as those involving deep reasoning over multiple
facts.

Acknowledgments

This work is supported by National High
Technology R&D Program of China (Grant
No.2015AA015403), Natural Science Foundation
of China (Grant No. 61672057, 61672058). For
any correspondence, please contact Yansong Feng.

193



References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR
abs/1409.0473.

Antoine Bordes and Jason Weston. 2016. Learn-
ing end-to-end goal-oriented dialog. CoRR
abs/1605.07683. http://arxiv.org/abs/
1605.07683.

Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. CoRR abs/1406.1078. http:
//arxiv.org/abs/1406.1078.

Jennifer Chu-Carroll, Krzysztof Czuba, John M.
Prager, Abraham Ittycheriah, and Sasha Blair-
Goldensohn. 2004. Ibm’s piquant ii in trec 2004.
In TREC.

Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence mod-
eling. CoRR abs/1412.3555. http://arxiv.
org/abs/1412.3555.

Michal Daniluk, Tim Rocktäschel, Johannes Welbl,
and Sebastian Riedel. 2017. Frustratingly short at-
tention spans in neural language modeling. CoRR
abs/1702.04521. http://arxiv.org/abs/
1702.04521.

Angela Fan, David Grangier, and Michael Auli. 2017.
Controllable abstractive summarization. CoRR
abs/1711.05217. http://arxiv.org/abs/
1711.05217.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N. Dauphin. 2017. Con-
volutional sequence to sequence learning. CoRR
abs/1705.03122. http://arxiv.org/abs/
1705.03122.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor
O. K. Li. 2016. Incorporating copying mech-
anism in sequence-to-sequence learning. CoRR
abs/1603.06393. http://arxiv.org/abs/
1603.06393.

He He, Anusha Balakrishnan, Mihail Eric, and Percy
Liang. 2017a. Learning symmetric collaborative
dialogue agents with dynamic knowledge graph
embeddings. CoRR abs/1704.07130. http://
arxiv.org/abs/1704.07130.

Shizhu He, Cao Liu, Kang Liu, and Jun Zhao.
2017b. Generating natural answers by incorporating
copying and retrieving mechanisms in sequence-to-
sequence learning. pages 199–208.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput. 9(8):1735–
1780. https://doi.org/10.1162/neco.
1997.9.8.1735.

Chloé Kiddon, Luke S. Zettlemoyer, and Yejin Choi.
2016. Globally coherent text generation with neural
checklist models. In EMNLP.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR
abs/1412.6980. http://arxiv.org/abs/
1412.6980.

Ankit Kumar, Ozan Irsoy, Jonathan Su, James Brad-
bury, Robert English, Brian Pierce, Peter Ondruska,
Ishaan Gulrajani, and Richard Socher. 2015. Ask
me anything: Dynamic memory networks for nat-
ural language processing. CoRR abs/1506.07285.
http://arxiv.org/abs/1506.07285.

Alexander H. Miller, Adam Fisch, Jesse Dodge, Amir-
Hossein Karimi, Antoine Bordes, and Jason We-
ston. 2016. Key-value memory networks for di-
rectly reading documents. CoRR abs/1606.03126.
http://arxiv.org/abs/1606.03126.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics. Association for
Computational Linguistics, Stroudsburg, PA, USA,
ACL ’02, pages 311–318. https://doi.org/
10.3115/1073083.1073135.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive
summarization. CoRR abs/1705.04304. http:
//arxiv.org/abs/1705.04304.

Abigail See, Peter Liu, and Christopher Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Association for Computa-
tional Linguistics. https://arxiv.org/abs/
1704.04368.

Sainbayar Sukhbaatar, Arthur Szlam, Jason We-
ston, and Rob Fergus. 2015. End-to-end mem-
ory networks. In Proceedings of the 28th
International Conference on Neural Information
Processing Systems - Volume 2. MIT Press,
Cambridge, MA, USA, NIPS’15, pages 2440–
2448. http://dl.acm.org/citation.
cfm?id=2969442.2969512.

Jun Suzuki and Masaaki Nagata. 2017. Cutting-off re-
dundant repeating generations for neural abstractive
summarization. In EACL.

Zhaopeng Tu, Yang Liu, Zhengdong Lu, Xiaohua Liu,
and Hang Li. 2016a. Context gates for neural ma-
chine translation .

194



Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016b. Modeling coverage for neu-
ral machine translation. In Proceedings of the
54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers).
Association for Computational Linguistics, pages
76–85. https://doi.org/10.18653/v1/
P16-1008.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Proceedings of the
28th International Conference on Neural Informa-
tion Processing Systems - Volume 2. MIT Press,
Cambridge, MA, USA, NIPS’15, pages 2692–
2700. http://dl.acm.org/citation.
cfm?id=2969442.2969540.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrkšić, Pei-
Hao Su, David Vandyke, and Steve Young. 2015.
Semantically conditioned lstm-based natural lan-
guage generation for spoken dialogue systems.
In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing.
Association for Computational Linguistics, pages
1711–1721. https://doi.org/10.18653/
v1/D15-1199.

Jason Weston, Sumit Chopra, and Antoine Bordes.
2014. Memory networks. CoRR abs/1410.3916.
http://arxiv.org/abs/1410.3916.

Kun Xu, Yansong Feng, Songfang Huang, and
Dongyan Zhao. 2016. Hybrid question answering
over knowledge base and free text. In COLING.

Jun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang,
Hang Li, and Xiaoming Li. 2015. Neural gener-
ative question answering. CoRR abs/1512.01337.
http://arxiv.org/abs/1512.01337.

195


