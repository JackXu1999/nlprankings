



















































Generating Bilingual Pragmatic Color References


Proceedings of NAACL-HLT 2018, pages 2155–2165
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Generating Bilingual Pragmatic Color References

Will Monroe
Computer Science Department

Stanford University
wmonroe4@cs.stanford.edu

Jennifer Hu
Department of Mathematics

Harvard University
jenniferhu@college.harvard.edu

Andrew Jong
Department of Computer Science

San Jose State University
andrewjong.cs@gmail.com

Christopher Potts
Department of Linguistics

Stanford University
cgpotts@stanford.edu

Abstract

Contextual influences on language often ex-
hibit substantial cross-lingual regularities; for
example, we are more verbose in situations
that require finer distinctions. However, these
regularities are sometimes obscured by seman-
tic and syntactic differences. Using a newly-
collected dataset of color reference games in
Mandarin Chinese (which we release to the
public), we confirm that a variety of construc-
tions display the same sensitivity to contextual
difficulty in Chinese and English. We then
show that a neural speaker agent trained on
bilingual data with a simple multitask learning
approach displays more human-like patterns
of context dependence and is more pragmat-
ically informative than its monolingual Chi-
nese counterpart. Moreover, this is not at the
expense of language-specific semantic under-
standing: the resulting speaker model learns
the different basic color term systems of En-
glish and Chinese (with noteworthy cross-
lingual influences), and it can identify syn-
onyms between the two languages using vector
analogy operations on its output layer, despite
having no exposure to parallel data.

1 Introduction

In grounded communication tasks, speakers face
pressures in choosing referential expressions that
distinguish their targets from others in the con-
text, leading to many kinds of pragmatic mean-
ing enrichment. For example, the harder a target
is to identify, the more the speaker will feel the
need to refer implicitly and explicitly to alterna-
tives to draw subtle contrasts (Zipf, 1949; Horn,
1984; Levinson, 2000). However, the ways in
which these contrasts are expressed depend heav-
ily on language-specific syntax and semantics.

x x x 鲜绿 xiān l`̈u‘bright green’

x x x 不亮的橙色 bu-liàng de chéngsè‘not-bright orange’

x x x 紫红色 zı̌ hóngsè‘purple-red’

Figure 1: Reference game contexts and utterances
from our Chinese corpus. The boxed color is the
target. Some color terms show differences be-
tween Chinese and English, such as绿 l`̈u ‘green’
in the first example for a color that might be re-
ferred to with ‘blue’ or ‘aqua’ in English.

In this paper, we seek to develop a model
of contextual language production that captures
language-specific syntax and semantics while also
exhibiting responsiveness to contextual differ-
ences. We focus on a color reference game
(Rosenberg and Cohen, 1964; Dale and Reiter,
1995; Krahmer and van Deemter, 2012) played in
both English and Mandarin Chinese. A reference
game (Figure 1) involves two agents, one desig-
nated the “speaker” and the other the “listener”.
The speaker and listener are shown the same set
of k colors C = {c1, . . . , ck} (in our experiments,
k = 3), and one of these colors ct is indicated
secretly to the speaker as the “target”. Both play-
ers share the same goal: that the listener correctly
guesses the target color. The speaker may com-
municate with the listener in free-form natural-
language dialogue to achieve this goal. Thus, a
model of the speaker must process representations
of the colors in the context and produce an utter-
ance to distinguish the target color from the oth-
ers. We evaluate a sequence-to-sequence speaker

2155



agent based on that of Monroe et al. (2017), who
also collected the English data we use; our Chi-
nese data are new and were collected according to
the same protocols.

While English and Chinese both use fairly sim-
ilar syntax for color descriptions, our reference
game is designed to elicit constructions that make
reference to the context, and these constructions—
particularly comparatives and negation—differ
morpho-syntactically and pragmatically between
the two languages. Additionally, Chinese is con-
sidered to have a smaller number of basic color
terms (Berlin and Kay, 1969), which predicts
markedness of more specific descriptions.

Our primary goal is to examine the effects of
bilingual training: building one speaker trained on
both English and Chinese data with a shared vo-
cabulary, so that it can produce utterances in either
language. The reference game setting offers an
objective measure of success on the grounded lan-
guage task, namely, the speaker’s ability to guide
the listener to the target. We use this to address
the tricky problem of speaker evaluation. Specifi-
cally, we use the speaker model and an application
of Bayes’ rule to infer the most likely target color
given a human utterance, and we report the accu-
racy of that process at identifying the target color.
We refer to this metric as pragmatic informative-
ness because it requires not only accuracy but also
effectiveness at meeting the players’ shared goal
(Grice, 1975). A more formal definition and a dis-
cussion of alternatives are given in Section 4.1.

We show that a bilingually-trained model pro-
duces distributions over Chinese utterances that
have higher pragmatic informativeness than a
monolingual model. An analysis of the learned
word embeddings reveals that the bilingual model
learns color synonyms between the two languages
without being directly exposed to labeled pairs.
However, using a context-independent color term
elicitation task from Berlin and Kay (1969) on our
models, we show that the learned lexical meanings
are largely faithful to each language’s basic color
system, with only minor cross-lingual influences.
This suggests that the improvements due to adding
English data are not primarily due to better repre-
sentations of the input colors or lexical semantics
alone. The bilingual model does better resemble
human patterns of utterance length as a function
of contextual difficulty, suggesting the pragmatic
level as one possible area of cross-lingual general-

ization.

2 Data collection

We adapted the open-source reference game
framework of Hawkins (2015) to Chinese and fol-
lowed the data collection protocols of Monroe
et al. (2017) as closely as possible, in the hope that
this can be the first step in a broader multilingual
color reference project. We recruit pairs of play-
ers on Amazon Mechanical Turk in real time, ran-
domly assigning one the role of the speaker and
the other the listener. Players are self-reported
Chinese speakers, but they must pass a series
of Chinese comprehension questions in order to
proceed, with instructions in a format preventing
copy-and-paste translation. The speaker and lis-
tener are placed in a game environment in which
they both see the three colors of the context and
a chatbox. The speaker sends messages through
the chatbox to describe the target to the listener,
who then attempts to click on the target. This ends
the round, and three new colors are generated for
the next. Both players can send messages through
the chatbox at any time. After filtering out ex-
tremely long messages (number of tokens greater
than 4σ above the mean), spam games,1 and play-
ers who self-reported confusion about the game,
we have a new corpus of 5,774 Chinese messages
in color reference games, which we will release
publicly. Data management information is given
in Appendix B.

As in Monroe et al. (2017), the contexts are di-
vided into three groups of roughly equal size: in
the far condition (1,421 contexts), all the colors
are at least a threshold distance θ from each other;
in the split condition (1,412 contexts), the target
and one distractor are less than θ from each other,
with the other distractor at least θ away from both;
and in the close condition (1,425 contexts), all col-
ors are within θ from each other. We set θ = 20 by
the CIEDE2000 color-difference formula (Sharma
et al., 2005), with all colors different by at least 5.

3 Human data analysis

As we mentioned earlier, our main goal with this
work is to investigate the effects of bilingual train-
ing on pragmatic language use. We first examine
the similarities and differences in pragmatic be-

1Some players found they could advance through rounds
by sending duplicate messages. Games were considered
spam if the game contained 25 or more duplicates.

2156



far split close
Condition

1.50

1.75

2.00

2.25

2.50

2.75

3.00

3.25

M
ea

n 
nu

m
be

r o
f t

ok
en

s p
er

 m
es

sa
ge

Chinese
English

Figure 2: Comparison of mean length of messages
in English and Chinese. The split and close condi-
tions have more similar context colors (Section 2).

haviors between the English and Chinese corpora
we use. The picture that emerges accords well
with our expectations about pragmatics: the broad
patterns are aligned across the two languages, with
the observed differences mostly tracing to the de-
tails of their lexicons and constructions.

3.1 Message length

We expect message length to correlate with the
difficulty of the context: as the target becomes
harder to distinguish from the distractors, the
speaker will produce more complex messages, and
length is a rough indicator of such complexity. To
test this hypothesis, we used the Natural Language
Toolkit (NLTK; Bird et al. 2009) and Jieba (Junyi,
2015) to tokenize English and Chinese messages,
respectively, and counted the number of tokens in
both languages as a measure of message length.
The results (Figure 2) confirm that in both lan-
guages, players become more verbose in more dif-
ficult conditions.2

3.2 Specificity

In the split and far conditions, the speaker must
make fine-grained distinctions. A broad color term
like red will not suffice if there are two reds, but
more specific terms like maroon might identify the
target. Thus, we expect specificity to increase as
the difficulty of the context does. To assess this,
we use WordNet (Fellbaum, 1998) to transform
adjectives into derivationally-related noun forms,
filter for nouns with color in their hypernym paths,
and mark a message as “specific” if it contains at

2We do not believe that the overall drop in message length
from English to Chinese reflects a fundamental difference be-
tween the languages; this has a few possible explanations,
from Chinese messages taking the form of “sentence seg-
ments” (Wang and Qin, 2010) to differences in tokenization.

far split close
Condition

0.02

0.03

0.04

0.05

0.06

0.07

0.08

M
ea

n 
va

lu
e 

of
 sp

ec
ifi

ci
ty

 in
di

ca
to

r

Chinese
English

Figure 3: Comparison of WordNet specificity in
Chinese and English.

least one word with a hypernym depth greater than
7.

For Chinese, we translate to English via Google
Translate, then measure the translated word using
WordNet. It should be noted that this method has
the drawback of obscuring differences between the
two languages’ color systems, as well as the po-
tential for introducing noise due to errors in auto-
matic translation. Though Mandarin variations of
WordNet exist, we chose this translation method
to standardize hypernym paths for both languages.
Differences in ontology decisions between lexi-
cal resources prevent straightforward cross-lingual
comparisons of hypernym depths, while automatic
translation to a common language ensures the re-
sulting hypernym paths are directly comparable.

Figure 3 summarizes the results of this mea-
surement. In general, the usage of high-specificity
color words increases in more difficult conditions,
as expected. However, we see that Chinese speak-
ers use them significantly less than English speak-
ers. Instead, Chinese speakers use nominal mod-
ifiers, such as 草 cǎo ‘grass’ and 海 hǎi ‘ocean’,
which do not contain “color” in their hypernym
paths and are thus not marked as high-specificity.
To quantify this observation, we annotated ran-
dom samples of 200 messages from each language
for whether they contained nominal color descrip-
tions, and found that 3.5% of the English messages
contain such nominals versus 13.5% of the Chi-
nese messages.

The use of nominal modifiers as opposed to
adjectives (‘dark orange’, ‘dull brown’) is ar-
guably expected given the claims of Berlin and
Kay (1969) and others that Chinese has fewer ba-
sic color terms than English, thus requiring more
visually evocative modifiers to clarify distinctions
between similar hues. (This isn’t a complete ex-
planation, since Chinese is rich in narrow but rare

2157



far split close
Condition

0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16

Pr
op

or
tio

n 
of

 m
sg

s w
/ c

om
pa

ra
tiv

e

Chinese
English

(a) Usage of comparative adjectives in Chinese and English.

far split close
Condition

0.02

0.04

0.06

0.08

0.10

0.12

0.14

0.16

Pr
op

or
tio

n 
of

 m
sg

s w
/ s

up
er

la
tiv

e

Chinese
English

(b) Usage of superlative adjectives in Chinese and English.

far split close
Condition

0.04

0.06

0.08

0.10

0.12

Pr
op

or
tio

n 
of

 m
sg

s w
/ n

eg
at

io
n Chinese

English

(c) Usage of negation in Chinese and English.

Figure 4: Comparison of usage of comparatives,
superlatives, and negation in English and Chinese.

non-basic color terms. For the cases where Chi-
nese has an appropriate narrow color term, it is
possible that speakers make a pragmatic decision
to avoid obscure vocabulary in favor of more fa-
miliar nouns.)

3.3 Comparatives, superlatives, and negation

To detect comparative and superlative adjectives in
English, we use NLTK POS-tagging, which out-
puts JJR and RBR for comparatives, and JJS and
RBS for superlatives. In Chinese, we look for the
tokens 更 gèng ‘more’ and 比 bı̌ ‘comparatively’
to detect comparatives and 最 zuı̀ ‘most’ to de-
tect superlatives. We detect negation by tokenizing
messages with NLTK and Jieba and then looking
for the tokens not and n’t in English and corre-
sponding不 bù and没 méi in Chinese.

These statistics are shown in Figure 4. Both lan-

guages exhibit similar trends for superlative adjec-
tives. In English, comparatives are used most fre-
quently in the split condition and second most fre-
quently in the close condition, while in Chinese,
they occur at around the same rate in the split and
close conditions. The literature is not conclusive
about the source of these differences. Xia (2014)
argues that complex attributives are rarely used
and sound “syntactically deviant or Europeanized”
(Zhu, 1982; Xie, 2001) in Chinese, citing the left-
branching nature of the language as restricting at-
tributives in length and complexity. There are also
conflicting theories on the markedness of gradable
adjectives in Chinese (Grano, 2012; Ito, 2008);
such markedness may contribute to the frequency
at which comparative forms are used.

We also see that both languages follow the same
general trend of using negation more frequently as
the condition becomes more difficult.

4 Models

We build and evaluate three artificial agents on this
reference game task, two trained on monolingual
descriptions (one for each language) and one on
bilingual descriptions. We base these models on
the basic speaker architecture from Monroe et al.
(2017). The monolingual speakers represent the
context by passing all the context colors as input to
a long short-term memory (LSTM) sequence en-
coder, then concatenating this representation with
a word vector for each previous output token as the
input to an LSTM decoder that produces a color
description token-by-token. This defines a distri-
bution over descriptions u conditioned on the tar-
get and context, S(u | ct, C).

To accommodate bilingual training with this ar-
chitecture, we expand the vocabulary to include
English and Chinese words, and we add a flag `
to the input specifying whether the model’s output
should be in English (` = 0) or Chinese (` = 1):

S(u | `, ct, C) =
|u|∏

i=1

s(ui | u1..i−1, `, ct, C)

The flag ` is embedded as a single additional di-
mension that is concatenated alongside the context
and input (previous token) vectors for the encoder.
See Appendix A for additional training details.

4.1 Pragmatic informativeness
As mentioned in Section 1, we evaluate the two
models on a measure of pragmatic informative-

2158



ness: how well does the model represent a human
speaker, such that a generative model of a listener
can be built from it to interpret utterances? For-
mally, for a speaker S(u | `, ct, C) and an exam-
ple consisting of an utterance, language identifier,
and color context (u, `, C), we identify the t∗ that
maximizes the probability of u according to S:

t∗ = argmax
t

S(u | ct, C)

That is, L uses a noisy-channel model with a uni-
form prior over target colors and S as a generation
model to infer the most likely target color given
the input utterance. The pragmatic informative-
ness of a speaker is the proportion of target colors
in a test set correctly identified by t∗.

One drawback of this metric is it does not eval-
uate how faithful the model is to the overall distri-
bution of human utterances, only the relative con-
ditional likelihoods of human utterances for differ-
ent target colors. In practice, since the agents are
trained to minimize log likelihood, we do not ob-
serve our agents frequently producing wildly un-
humanlike utterances; however, this is a caveat to
keep in mind for evaluating agents that do not nat-
urally approximate a language model.

The understanding model implied in this metric
is equivalent to a version of the Rational Speech
Acts model of pragmatic language understanding
(Frank and Goodman, 2012; Goodman and Frank,
2016), or the pragmatic posterior of the Rational
Observer model (McMahan and Stone, 2015). An
important difference between our speaker model
and those in the work cited above is that our
speaker model is a neural network that makes a
combined judgment of applicability (semantic ap-
propriateness) and availability (utterance prior),
instead of modeling the two components sepa-
rately. However, we stop short of directly predict-
ing the referent of an expression discriminatively,
as is done by e.g. Kennington and Schlangen
(2015), so as to require a model that is usable as a
speaker.

A related metric is communicative success as
defined by Golland et al. (2010), which judges the
speaker by the accuracy of a human listener when
given model-produced utterances. Our pragmatic
informativeness metric instead gives a model-
derived listener human utterances and assesses its
accuracy at identifying colors. Pragmatic infor-
mativeness has the advantage of not requiring ad-
ditional expensive human labeling in response to

model outputs; it can be assessed on an existing
collection of human utterances, and can therefore
be considered an automatic metric.

4.2 A note on perplexity

Perplexity is a common intrinsic evaluation met-
ric for generation models.3 However, for compar-
ing monolingual and bilingual models, we found
perplexity to be unhelpful, owing largely to its
vocabulary-dependent definition. Specifically, if
we fix the vocabulary in advance to include tokens
from both languages, then the monolingual model
performs unreasonably poorly, and bilingual train-
ing helps immensely. However, this is an unfair
comparison: the monolingual model’s high per-
plexity is dominated by low probabilities assigned
to rare tokens in the opposite-language data that it
did not see. Thus, perplexity ceases to be a mea-
sure of language modeling ability and assumes the
role of a proxy for the out-of-vocabulary rate.

On the other hand, if we define the output vo-
cabulary to be the set of tokens seen at least n
times in training (n = 1 and 2 are common), then
monolingual training yields better perplexity than
bilingual training, but mainly because including
opposite-language training data forces the bilin-
gual model to predict more rare words that would
otherwise be replaced with 〈unk〉.4 This produces
the counterintuitive result that perplexity initially
goes up (gets worse) when increasing the amount
of training data. (As a pathological case, with no
training data, a model can get a perfect perplexity
of 1 by predicting 〈unk〉 for every token.)

5 Experimental results and analysis

Pragmatic informativeness of the models on En-
glish and Chinese data is shown in Table 1. The
main result is that training a bilingual model helps
compared to a Chinese monolingual one; however,
the benefit is asymmetrical, as training on mono-
lingual English data is superior for English data to
training on a mix of Chinese and English. All dif-
ferences in Table 1 are significant at p < 0.001

3Two other intrinsic metrics, word error rate (WER) and
BLEU (Papineni et al., 2002), were at or worse than chance
despite qualitatively adequate speaker outputs, due to high
diversity in valid outputs for similar contexts. This problem
is common in dialogue tasks, for which BLEU is known to be
an ineffective speaker evaluation metric (Liu et al., 2016).

4The rare words that make this difference are primarily
the small number of English words that were used by the
Chinese-language participants; no Chinese words were ob-
served in the English data from Monroe et al. (2017)

2159



test train dev acc test acc

en en 80.51 83.06
en+zh 79.73 81.43

zh zh 67.16 67.75
en+zh 71.81 72.89

Table 1: Pragmatic informativeness scores (%) for
monolingual and bilingual speakers.

(approximate permutation test, 10,000 samples;
Padó, 2006), except for the decrease on the En-
glish dev set, which is significant at p < 0.05.

An important difference between our corpora
is that the English dataset is an order of magni-
tude larger than the Chinese. Intuitively, we ex-
pect adding more training data on the same task
will improve the model, regardless of language.
However, we find that the effect of dataset size
is not so straightforward. In fact, the differences
in training set size convey a non-linear benefit.
Figure 5 shows the pragmatic informativeness of
the monolingual and bilingual speakers on the de-
velopment set as a function of dataset size (num-
ber of English and Chinese utterances). The blue
curves (circles) in the plots on the left, Figure 5a
and Figure 5c, are standard learning curves for
the monolingual models, and their parallel red
curves (triangles) show the pragmatic informative-
ness of the bilingual model with the same amount
of in-language data plus all available data in the
opposite language. The plots on the right, Fig-
ure 5b and Figure 5d, show the effect of gradu-
ally adding opposite-language data to the bilingual
model starting with all of the in-language data.

Overall, we see that adding all English data con-
sistently helps the Chinese monolingual model,
whereas adding all Chinese data consistently hurts
the English monolingual model (though with di-
minishing effects as the amount of English data in-
creases). Adding small amounts of English data—
especially amounts comparable to the size of the
Chinese dataset—decreases accuracy of the Chi-
nese model dramatically. This suggests an inter-
action between the total amount of data and the
effect of bilingual training: a model trained on
a moderately small number of in-language exam-
ples can benefit from a much larger training set
in another language, but combining data in two
languages is detrimental when both datasets are
very small and has very little effect when the in-

0 3000 6000 9000 12000 15000
# en examples

0.4

0.5

0.6

0.7

0.8

0.9

1.0

en
 p

ra
g.

 in
f.

without zh
with all zh

(a) English without any /
with all Chinese

0 300 600 900 1200 1500
# zh examples

0.4

0.5

0.6

0.7

0.8

0.9

1.0

en
 p

ra
g.

 in
f.

best without zh
with zh

(b) All English, varying
amount of Chinese data

0 300 600 900 1200 1500
# zh examples

0.4

0.5

0.6

0.7

0.8

0.9

1.0

zh
 p

ra
g.

 in
f.

without en
with all en

(c) Chinese without any /
with all English

0 3000 6000 9000 12000 15000
# en examples

0.4

0.5

0.6

0.7

0.8

0.9

1.0

zh
 p

ra
g.

 in
f.

best without en
with en

(d) All Chinese, varying
amount of English data

Figure 5: Pragmatic informativeness (dev set) for
different amounts and languages of training data.

language training set is large. This implies a bene-
fit primarily in low-resource settings, which agrees
with the findings of Johnson et al. (2016) using a
similar architecture for machine translation.

5.1 Bilingual lexicon induction

To get a better understanding of the influence of
the bilingual training on the model’s lexical repre-
sentations in the two languages, we extracted the
weights of the final softmax layer of the bilingual
speaker model and used them to induce a bilin-
gual lexicon with a word vector analogy task. For
two pairs of lexical translations, 蓝色 lánsè →
“blue” and “red” → 红 hóng, we took the dif-
ference between the source language word vector
and the target language word vector. To “trans-
late” a word, we added this “translation vector” to
the word vector for the source word, and found the
word in the opposite language with the largest in-
ner product to the resulting vector. The results are
presented in Table 2. We identified the 10 most
frequent color-related words in each language to
translate. (In other words, we did not use this
process to find translations of function words like
“the” or the Chinese nominalization/genitive par-
ticle的 de, but we show proposed translations that
were not color-related, such as灰 huı̄ being trans-
lated as the English comparative ending “-er”.)

2160



zh en en zh

绿色 ‘green’ green green 绿绿绿 ‘green’
紫色 ‘purple’ purple blue 蓝蓝蓝 ‘blue’
蓝色 ‘blue’ purple purple 蓝 ‘blue’
灰色 ‘grey’ grey bright 鲜鲜鲜艳艳艳 ‘bright’
亮 ‘bright’ bright pink 粉粉粉色色色 ‘pink’
灰 ‘grey’ -er grey 灰灰灰 ‘grey’
蓝 ‘blue’ teal dark 暗暗暗 ‘dark’
绿 ‘green’ green gray 灰灰灰 ‘grey’
紫 ‘purple’ purple yellow 黄黄黄色色色 ‘yellow’
草 ‘grass’ green light 最 ‘most’

Table 2: Bilingual lexicon induction from Chinese
to English (first two columns) and vice versa (last
two). Correct translations in bold, semantically
close words in italic.

The majority of common color words are trans-
lated correctly by this simple method, showing
that the vectors in the softmax layer do express a
linear correspondence between the representation
of synonyms in the two languages.

5.2 Color term semantics

The above experiment suggests that the bilingual
model has learned word semantics in ways that
discover translation pairs. However, we wish to
know whether bilingual training has resulted in
changes to the model’s output distribution reflect-
ing differences in the two languages’ color sys-
tems. To evaluate this, we performed an experi-
ment similar to the basic color term elicitations in
the World Color Survey (WCS; Berlin and Kay,
1969) on our models. For each of the 330 colors
in the original WCS, we presented that color to our
monolingual and bilingual models and recorded
the most likely color description according to the
conditional language model. Our models require
a three-color context to produce a description; as
an approximation to eliciting context-insensitive
color terms, we gave the model ten contexts with
randomly generated (uniform in H, S, and V) dis-
tractor colors and averaged the language model
probabilities. We also identified, for each color
term produced as the most likely description of
one or more colors, the color that resulted in the
highest probability of producing that term.

The results are in Figure 6. The charts use the
layout of the WCS stimulus, in which the two axes
represent dimensions of color variation similar to
hue and lightness. Each region represents a set of
colors that the model labeled with the same color
term, and a star marks the color that resulted in the

huı̄

hóng

zōng

huáng

l`̈u lán
zı̌

hóng

zōng

Hue

V
al

ue

(a) Monolingual Chinese

huı̄

zōng

hóng
chéng

zōng

huáng

l`̈u lán zı̌

hóng

Hue

V
al

ue

(b) Chinese with English data

grey red
orange

brown

yellow
green

teal blue
purple pink

Hue

V
al

ue

(c) Monolingual English

Figure 6: Color term lexica: colors in the
World Color Survey palette grouped by highest-
probability description, averaged over 10
randomly-generated pairs of distractor colors.
The color that results in the highest probability of
each description is marked with a star. English
influences on the bilingual model include the ap-
pearance of橙色 chéngsè ‘orange’ and narrowing
of黄色 huángsè ‘yellow’ and绿色 l`̈usè ‘green’.

highest probability of producing that term. The
Chinese terms, except for红 hóng, are abbreviated
by deleting the final morpheme色 sè ‘color’.

The charts agree with Berlin and Kay (1969)
on most of the differences between the two lan-
guages: orange and pink have clear regions of
dominance in English, whereas in the Mandarin
monolingual model pink is subsumed by红 hóng
‘red’, and orange is subsumed by 黄色 huángsè
‘yellow’. Our models produce three colors not in
the six-color system5 identified by Berlin and Kay
for Mandarin: 灰色 huı̄sè ‘grey’, 紫色 zı̌sè ‘pur-
ple’, and棕色 zōngsè ‘brown’. We do not specifi-
cally claim these should be considered basic color
terms, since Berlin and Kay give a theoretical def-
inition of “basic color term” that is not rigorously
captured by our model. In particular, they explic-
itly exclude灰色 huı̄sè from the set of basic color
terms, despite its frequency, because it has a mean-

5Notably absent are ‘black’ and ‘white’. The collection
methodology of Monroe et al. (2017) restricted colors to a
single lightness, so black and white are not in the data. For
these charts, we replaced the World Color Survey swatches
with the closest color used in our data collection.

2161



ing that refers to an object (‘ashes’). The other two
may have been excluded for the same reason, or
they may represent a change in the language or the
influence of English on the participants’ usage.6

A few differences between the monolingual and
bilingual models can be characterized as an influ-
ence of one language’s color system on the other.
First, teal appears as a common description of a
few color swatches from the English monolingual
model, but the bilingual model, like the Chinese
model, does not feature a common word for teal.
Second, the Chinese monolingual model does not
include a common word for orange, but the bilin-
gual model identifies 橙色 chéngsè ‘orange’. Fi-
nally, the English green is semantically narrower
than the Chinese绿色 l`̈usè, and the Chinese bilin-
gual model exhibits a corresponding narrowing of
the range of绿色 l`̈usè.

Overall, however, the monolingual models cap-
ture largely accurate maps of each language’s ba-
sic color system, and the bilingual model retains
the major contrasts between them, rather than “av-
eraging” between the two. This suggests that the
bilingual model learns a representation of the in-
put colors that encodes their categorization in both
languages, and that for the most part these lexi-
cal semantic representations do not influence each
other.

5.3 Comparing model and human utterances

One observation indicates that the improvements
in the bilingually-trained model are primarily at
the pragmatic (context-dependent) level of lan-
guage production. Figure 7 reveals that the
bilingually-trained model better captures the main
pragmatic pattern we observe in the human data,
that of increasing message length in harder condi-
tions. In both languages, the monolingual model
uses longer utterances in the easy far condition
than human speakers do, whereas the bilingual
model is significantly closer on that condition to
the human statistics. We see similar results in the
use of negations and comparatives; the use of su-
perlatives is not substantially different between the
monolingual and bilingual models.

We note that this result does not rule out several
competing hypotheses. In particular, we do not
exclude improvements in compositional semantics
or syntax, nor do we distinguish improvements in

6MTurk’s restriction to US workers makes English influ-
ence more likely than would otherwise be expected.

(a) Human and model utterance lengths in English.

(b) Human and model utterance lengths in Chinese.

Figure 7: Comparison of mean length of messages
between human and model utterances.

specific linguistic areas from broader regulariza-
tion effects of having additional data in general.
Preliminary experiments involving augmentation
of the data by duplicating and deleting constituents
show no gains, suggesting that the improvement
depends on certain kinds of regularities in the En-
glish data that are not provided by artificial manip-
ulations. However, more investigation is needed to
thoroughly assess the role of general-purpose reg-
ularization in our observations.

6 Related work

The method we use to build a bilingual model in-
volves adding a single dimension to the previous-
token vectors in the encoder representing the lan-
guage (Section 4). In essence, the two languages
have separate vocabulary representation at the in-
put and output but shared hidden representations.
Adding a hard constraint on the output vocabulary
would make this equivalent to a simple form of
multitask learning (Caruana, 1997; Collobert and
Weston, 2008). However, allowing the model to
use tokens from either language at any time is
simpler and results in better modeling of mixed-
language data, which is more common in non-
English environments. In fact, our model occa-
sionally ignores the flag and “code-switches” be-

2162



tween the two languages within a single output,
which is not possible in typical multitask architec-
tures.

Using shared parameters for cross-lingual rep-
resentation transfer has a large literature. Kle-
mentiev et al. (2012) and Hermann and Blun-
som (2014) use multitask learning with multilin-
gual document classification to build cross-lingual
word vectors, and observe accurate lexical transla-
tions from linear vector analogy operations. They
include predicting translations for words in par-
allel data as one of their tasks. Our translations
from vector relationships (Section 5.1) derive their
cross-lingual relationships from the non-linguistic
input of our grounded task, without parallel data.

Huang et al. (2013) note gains in speech recog-
nition from cross-lingual learning with shared pa-
rameters. In machine translation, Johnson et al.
(2016) add the approach of setting the output lan-
guage using a symbol in the input. Kaiser et al.
(2017) extend this to image captioning, speech
recognition, and parsing in one multitask system.
Our work complements these efforts with an in-
depth analysis of bilingual training on a grounded
generation task and an exploration of the relation-
ship between cross-lingual semantic differences
and pragmatics. In general, we see grounding in
non-linguistic input, including images and sensory
input from real and simulated worlds, as an in-
triguing substitute for direct linguistic supervision
in low-resource settings. We encourage evaluation
of multitask and multilingual models on tasks that
require reference to the context for effective lan-
guage production and understanding.

7 Conclusion

In this paper, we studied the effects of training on
bilingual data in a grounded language task. We
show evidence that bilingual training can be help-
ful, but with a non-obvious effect of dataset size:
accuracy as a function of opposite-language data
follows a U-shaped curve. The resulting model
is more human-like in measures of sensitivity to
contextual difficulty (pragmatics), while exhibit-
ing language-specific lexical learning in the form
of vector relationships between lexical pairs and
differences between the two languages in common
color-term extensions (semantics).

It should be noted that color descriptions in En-
glish and Chinese are similar both in their syn-
tax and in the way they divide up the semantic

space. We might expect that for languages like
Arabic and Spanish (with their different place-
ment of modifiers), or Waorani and Pirahã (with
their much smaller color term inventories), the in-
troduction of English data could have detrimental
effects that outweigh the language-general gains.
An investigation across a broader range of lan-
guages is desirable.

Our contribution includes a new dataset of hu-
man utterances in a color reference game in Man-
darin Chinese, which we release to the public7

with our code and trained model parameters.8

Acknowledgments

We thank Jiwei Li for extensive editing of our
Chinese translations of the Mechanical Turk task
instructions, Robert X.D. Hawkins for assistance
setting up the data collection platform, and mem-
bers of the Stanford NLP group—particularly
Reid Pryzant, Sebastian Schuster, and Reuben
Cohn-Gordon—for valuable feedback on earlier
drafts. This material is based in part upon work
supported by the Stanford Data Science Initiative
and by the NSF under Grant Nos. BCS-1456077
and SMA-1659585.

References
Brent Berlin and Paul Kay. 1969. Basic Color Terms:

their Universality and Evolution. University of Cal-
ifornia Press, Berkeley and Los Angeles.

Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media, Sebastopol, CA.

Rich Caruana. 1997. Multitask learning. Machine
Learning, 28:41–75.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning (ICML), pages 160–167. ACM.

Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233–263.

Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
7https://cocolab.stanford.edu/

datasets/colors.html
8https://github.com/futurulus/

colors-in-context

2163



Michael C. Frank and Noah D. Goodman. 2012. Pre-
dicting pragmatic reasoning in language games. Sci-
ence, 336(6084):998.

Dave Golland, Percy Liang, and Dan Klein. 2010. A
game-theoretic approach to generating spatial de-
scriptions. In Proceedings of the 2010 Conference
on Empirical Methods on Natural Language Pro-
cessing (EMNLP).

Noah D. Goodman and Michael C. Frank. 2016. Prag-
matic language interpretation as probabilistic infer-
ence. Trends in Cognitive Sciences, 20(11):818–
829.

Thomas Grano. 2012. Mandarin ‘hen’ and universal
markedness in gradable adjectives. Natural Lan-
guage & Linguistic Theory, 30(2):513–565.

H. Paul Grice. 1975. Logic and conversation. In Pe-
ter Cole and Jerry Morgan, editors, Syntax and Se-
mantics, volume 3: Speech Acts, pages 43–58. Aca-
demic Press, New York.

Robert X. D. Hawkins. 2015. Conducting real-time
multiplayer experiments on the web. Behavior Re-
search Methods, 47(4):966–976.

Karl Moritz Hermann and Phil Blunsom. 2014. Mul-
tilingual models for compositional distributed se-
mantics. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 58–68. Association for Computational
Linguistics.

Laurence R. Horn. 1984. Toward a new taxonomy for
pragmatic inference: Q-based and R-based implica-
ture. In Deborah Schiffrin, editor, Meaning, Form,
and Use in Context: Linguistic Applications, pages
11–42. Georgetown University Press, Washington,
D.C.

Jui-Ting Huang, Jinyu Li, Dong Yu, Li Deng, and Yi-
fan Gong. 2013. Cross-language knowledge transfer
using multilingual deep neural network with shared
hidden layers. In Proceedings of the 2013 IEEE
International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 7304–7308.
IEEE.

Satomi Ito. 2008. Typology of comparatives. In Pro-
ceedings of the 22nd Pacific Asia Conference on
Language, Information and Computation (PACLIC),
pages 197–206.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Viégas, Martin Wattenberg, Greg Corrado,
et al. 2016. Google’s multilingual neural machine
translation system: enabling zero-shot translation.
arXiv preprint arXiv:1611.04558.

Sun Junyi. 2015. Jieba Python Library.

Łukasz Kaiser, Aidan N. Gomez, Noam Shazeer,
Ashish Vaswani, Niki Parmar, Llion Jones, and
Jakob Uszkoreit. 2017. One model to learn them
all. arXiv preprint arXiv:1706.05137.

Casey Kennington and David Schlangen. 2015. Simple
learning and compositional application of perceptu-
ally grounded word meanings for incremental refer-
ence resolution. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (ACL-IJCNLP),
pages 292–301. Association for Computational Lin-
guistics.

Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of COLING
2012, pages 1459–1474. International Committee on
Computational Linguistics.

Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173–218.

Stephen C. Levinson. 2000. Presumptive Meanings:
The Theory of Generalized Conversational Implica-
ture. MIT Press, Cambridge, MA.

Chia-Wei Liu, Ryan Lowe, Iulian V. Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How NOT to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
2122–2132. Association for Computational Linguis-
tics.

Brian McMahan and Matthew Stone. 2015. A
Bayesian model of grounded color semantics.
Transactions of the Association for Computational
Linguistics, 3:103–115.

Will Monroe, Robert X.D. Hawkins, Noah D. Good-
man, and Christopher Potts. 2017. Colors in con-
text: A pragmatic neural model for grounded lan-
guage understanding. Transactions of the Associa-
tion for Computational Linguistics, 5:325–338.

Sebastian Padó. 2006. User’s guide to sigf: Signifi-
cance testing by approximate randomisation.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL). Association for Com-
putational Linguistics.

Seymour Rosenberg and Bertram D. Cohen. 1964.
Speakers’ and listeners’ processes in a word com-
munication task. Science, 145(3637):1201–1203.

2164



Gaurav Sharma, Wencheng Wu, and Edul N. Dalal.
2005. The CIEDE2000 color-difference formula:
Implementation notes, supplementary test data, and
mathematical observations. Color Research & Ap-
plication, 30(1):21–30.

Kefei Wang and Hongwu Qin. 2010. A parallel
corpus-based study of translational chinese. In
Richard Xiao, editor, Using Corpora in Contrastive
and Translation Studies, pages 164–181. Cambridge
Scholars Publishing.

Yun Xia. 2014. Normalization in Translation: Corpus-
based Diachronic Research into Twentieth-century
English Chinese Fictional Translation. Cambridge
Scholars Publishing.

Yaoji Xie. 2001. 汉语语法欧化综述 / Hànyǔ yǔfǎ
Ōuhuà zōngshù (A review of Europeanized Chinese
grammar). 语文研究 Yǔwén Yánjiū (Chinese Lan-
guage Research), 1:17–22.

Dexi Zhu. 1982. 语法讲义 / Yǔfǎ Jiǎngyı̀ (Lectures on
Grammar). The Commercial Press, Beijing.

George Kingsley Zipf. 1949. Human Behavior and the
Principle of Least Effort. Addison-Wesley, Cam-
bridge.

A Model details

Hyperparameters for the two main models are
given in Table 3. Both Chinese monolingual and
bilingual model were tuned for perplexity on a
held-out subset of the training set, by random
search followed by a local search from the best
candidate until no single parameter change pro-
duced a better result. However, the tuned settings
for the Chinese monolingual model did not outper-
form the settings from Monroe et al. (2017) for the
English model on the development set, so in our fi-
nal experiments the monolingual models used the
same parameters.

The vocabulary for each model consisted of
all tokens that were seen at least twice in train-
ing; the bilingual model’s vocabulary is larger
than the union of the words in each monolingual
model because some tokens occurred once in each
language (largely meta-commentary—e.g., dunno,
HIT, xD—and some English color word typos).

B Data management

This work was done under Stanford IRB Protocol
17827, which has the title “Pragmatic enrichment
and contextual inference”. Data was only col-
lected from workers who indicated their informed
consent. Workers on Amazon Mechanical Turk

hyperparameter mono. biling.

optimizer ADAM RMSProp
learning rate 0.004 0.004
dropout 0.1 0.1
gradient clip norm – 1
LSTM cell size 100 50
embedding size 100 100
initial forget bias 0 5
nonlinearity tanh sigmoid

vocabulary size 895 (en) 1,326
260 (zh)

Table 3: Values of hyperparameters optimized in
tuning for the monolingual and bilingual models,
plus vocabulary sizes.

were paid $2.00 to complete each game consist-
ing of 50 dialogue contexts, plus a bonus of $0.01
for each target the listener correctly identified. All
worker identifiers have been removed from data
that is released; the only other information col-
lected about the workers was their Chinese lan-
guage proficiency.

2165


