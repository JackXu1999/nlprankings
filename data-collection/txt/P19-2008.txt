



















































Question Answering in the Biomedical Domain


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 54–63
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

54

Question Answering in the Biomedical Domain

Vincent Nguyen
Research School of Computer Science, Australian National University

Data61, CSIRO
vincent.nguyen@anu.edu.au

Abstract

Question answering techniques have mainly
been investigated in open domains. How-
ever, there are particular challenges in extend-
ing these open-domain techniques to extend
into the biomedical domain. Question answer-
ing focusing on patients is less studied. We
find that there are some challenges in patient
question answering such as limited annotated
data, lexical gap and quality of answer spans.
We aim to address some of these gaps by ex-
tending and developing upon the literature to
design a question answering system that can
decide on the most appropriate answers for
patients attempting to self-diagnose while in-
cluding the ability to abstain from answering
when confidence is low.

1 Introduction

Question Answering (QA) is the downstream task
of information seeking wherein a user presents
a question in natural language, Q, and a system
finds an answer or a set of answers from a col-
lection of natural language documents or knowl-
edge bases (Lende and Raghuwanshi, 2016), A,
that satisfies the user’s question (Molla and Gon-
zlez, 2007).

Questions fall into one of two categories: fac-
toid and non-factoid. Factoid QA provides brief
facts to the users’ questions; for example, Ques-
tion: What day is it? Answer: Monday. Non-
factoid question answering is a more complex
task. It involves answering questions that require
specific knowledge, common sense or a procedure
due to ambiguity or the scope of the question. An
example from the Yahoo non-factoid question an-
swer dataset1 illustrates this: Question: Why is it
considered unlucky to open an umbrella indoors?.
The answer is not apparent and requires specific
knowledge about cultural superstitions.

1https://ciir.cs.umass.edu/downloads/nfL6/

Question answering is fundamental in high-
level tools such as chatbots (Qiu et al., 2017;
Yan et al., 2016; Amato et al., 2017; Ram et al.,
2018), search engines (Kadam et al., 2015), and
virtual assistants (Yaghoubzadeh and Kopp, 2012;
Austerjost et al., 2018; Bradley et al., 2018). How-
ever, being a downstream task, question answering
suffers from pipeline error, as it often relies on the
quality of several upstream tasks such as coref-
erence resolution (Vicedo and Ferrández, 2000),
anaphora resolution (Ram et al., 2018), named en-
tity recognition (Aliod et al., 2006), information
retrieval (Mao et al., 2014), and tokenisation (De-
vlin et al., 2019).

Thus, there has been a growing demand for
these QA systems to deliver precise question-
specific answers (Pudaruth et al., 2016) and con-
sequently has sparked much research into improv-
ing upon relevant natural language processing ap-
proaches (Malik et al., 2013), datasets (Rajpurkar
et al., 2016; Kociský et al., 2017) and informa-
tion retrieval techniques (Weienborn et al., 2013;
Mao et al., 2014). These improvements have al-
lowed the domain to evolve from shallow keyword
matching to contextual and semantic retrieval sys-
tems (Kadam et al., 2015). However, most of
these techniques have been focused on the open-
domain (Soares and Parreiras, 2018) and the chal-
lenges harbouring the biomedical domain have not
been well addressed and remain unsolved. Here,
we define biomedical QA as either factoid or non-
factoid QA on biomedical literature.

One such challenge is due to the creation
of complex medical queries which require ex-
pert knowledge and up to four hours per
query (Russell-Rose and Chamberlain, 2017) to
adequately answer. This requirement of expert
knowledge leads to a lack of high-quality, publicly
available biomedical QA datasets. Furthermore,
medical datasets tend to be locked behind ethical,
obligatory agreements and are usually small due to



55

cost constraints and lack of domain experts for an-
notation (Pampari et al., 2018; Shen et al., 2018).
Therefore, open-domain techniques which assume
data-rich conditions are not suitable for direct ap-
plication to the biomedical domain.

Another challenge is clinical term ambiguity,
which is due to the temporally and spatially vary-
ing nature of clinical terminology, and the fre-
quent use of abbreviation and esoteric medical ter-
minology (Lee et al., 2019) (see Table 1 for ex-
amples). It is difficult for systems to adequately
disambiguate clinical words to be used in down-
stream QA systems due to the complexity of the
ambiguity of medical terminology, such as abbre-
viations, due to their varying contexts. Though
there are existing tools such as MetaMap (Aron-
son and Lang, 2010) to disambiguate these terms
by mapping them to the UMLS (Unified Medi-
cal Language System) metathesaurus, coverage of
these systems is low and mappings are often inac-
curate (Wu et al., 2012).

Furthermore, systems in the open-domain typ-
ically retrieve a long answer before extracting a
short continuous span of text to present to the
user (Soares and Parreiras, 2018; Rajpurkar et al.,
2016). However, for biomedical responses, it is
not always sufficient to retrieve short answer con-
tinuous spans, and Answer Evidence spans that are
discontinuous that cross the sentence boundary are
often required (Pampari et al., 2018; Hunter and
Cohen, 2006; Nentidis et al., 2018).

These problems are not yet solved in the
biomedical domain and are reflected in the
BioASQ challenge (Nentidis et al., 2018), an an-
nual challenge with a biomedical question answer-
ing track. Currently, the state-of-the-art systems
do not perform much better than random guess
with an accuracy of 66.67% for binary question
answering (Chandu et al., 2017), 24.24% for fac-
toid (ranked list of named entities as answers) and
an F1-score of 0.3312 for list-type (unranked list
of named entities) (Peng et al., 2015) suggesting
that there is much room for improvement in terms
of algorithms and research.

Furthermore, we found that there is a lack of
a biomedical question answering system directed
for patients. Biomedical question answering for
patients is important as studies from the Pew Re-
search Centre have shown that 35% of U.S. adults
have diagnosed themselves using the information

they found online2. Of these adults, 35% said
that they did not get a professional opinion on
their self-diagnosis, illustrating that patients may
blindly trust the results of search engines without
consulting a medical professional. This is cause
for concern, as search engines tend to display the
most severe ailments first which could lead to a
potential waste of hospital resources or deteriora-
tion in patient health (Korfage et al., 2006).

Furthermore, although there are negatives to
searching symptoms via search engine, for the par-
ticipants who visited doctors after self-diagnosis,
research has revealed that doctor-patient relation-
ships and patient compliance with treatment im-
prove as the patients have a clearer understanding
of their symptoms and potential disease after self-
diagnosis (Cocco et al., 2018). These studies mo-
tivate the need for a strong biomedical question
answering question for patients as it will benefit
patients who self-diagnose and patients who seek
medical advice after looking up their symptoms
online.

Finally, we highlight that there is a lexical and
semantic gap between clinical and patient lan-
guage. For example, the expression “hole in lung“
taken literally is about a punctured lung. However,
this colloquialism refers to the condition known
as Pleurisy (Ben Abacha and Demner-Fushman,
2019; Abacha and Demner-Fushman, 2016), illus-
trating that patients do not have the level of literacy
to formulate complex medical queries nor under-
stand them (Graham and Brookey, 2008).

We aim to address the challenges in applying
question answering to biomedical question an-
swering for patients. We highlight that the cur-
rent gaps of biomedical QA research stem from
lack of clinical disambiguation tools, lack of high-
quality data, the quality of answer spans, weak
algorithms and clinical-patient lexical gaps. Our
goal is to present a patient biomedical QA system
that can address the gaps in biomedical research
and allows a patient to query their symptoms, dis-
eases or available treatment options accurately, but
will also abstain from providing answers in cases
where there is low confidence in the best answer,
question malformation or insufficiency of data to
answer the question.

2https://www.pewinternet.org/2013/01/15/health-online-
2013/



56

Type Example Explanation
Temporally varying Flu The Flu evolves every year and the cause is predicated on the

year it is contracted
Spatially varying Cancer Cancer is a disease that varies with severity based on location

(Late stage brain cancer is much worse than early stage skin
cancer)

Abbreviation HR A common clinical abbreviation that typically means heart
rate, but may mean hazard ratio depending on the context

Esoteric terminology c.248T>C A gene mutation that does not appear in any open-domain cor-
pus such as Wikipedia and has no layman definition

Table 1: Examples of ambiguity in biomedical text.

2 Literature Review

Here, we detail a review of question answering in
the open and biomedical domains.

2.1 Information Retrieval Approaches

Biomedical QA systems up until 2015 relied heav-
ily on Information Retrieval (IR) techniques such
as tf-idf ranking (Lee et al., 2006) and entity
extraction tools such as MetaMap (Aronson and
Lang, 2010) in order to obtain candidate answers
(by querying biomedical databases) and feature
extraction before using machine learning mod-
els such as logistic regression (Weienborn et al.,
2013). While other techniques included using
cosine similarity between one-hot encoded vec-
tors of answer and question for candidate re-
ranking (Mao et al., 2014). However, these tech-
niques were inherently bag-of-word approaches
that ignored the context of words. Furthermore,
these techniques relied on complete matches of
question terms and answer paragraphs, which is
not realistic in practice. Patients use different ter-
minology to that of medical experts and biomedi-
cal literature (Graham and Brookey, 2008).

In more recent years, more neural approaches to
IR have been used in the biomedical space (Nen-
tidis et al., 2017, 2018) such as Position-
Aware Convolutional Recurrent Relevance Match-
ing (Hui et al., 2017), Deep Relevance Match-
ing Model (Guo et al., 2017) and Attention Based
Convolutional Neural Network (Yin et al., 2015).
However, though these approaches do not rely on
complete matching of words and capture seman-
tics, they either ignore local or global contexts
which are useful for disambiguation of clinical ter-
minology and comprehension (McDonald et al.,
2018).

2.2 Semantic-level Approach

QA requires the retrieval of long answers be-
fore summarisation or retrieval of answer spans.
Punyakanok et al. (2004) introduced the use of
a question’s dependency trees and candidate an-
swers’ dependency trees and aligning with the
Tree Edit Distance metric to augment statistical
classifiers such as Logistic Regression and Con-
ditional Random Fields. However, these meth-
ods failed to capture complex semantic informa-
tion due to a reliance on effective part-of-speech
tagging and were not attractive end-to-end solu-
tions. Otherwise, WordNet was utilised to extract
semantic relationships and estimate semantic dis-
tances between answers and questions (Terol et al.,
2007). However, WordNet suffered from being
open-domain focused and also was not able to cap-
ture complex semantic information such as poly-
semy (Molla and Gonzlez, 2007).

2.3 Neural Approaches

In recent years, approaches that use neural net-
works have become popular. Word embedding
techniques such as Word2vec and GloVe can
model the latent semantic distribution of language
through unsupervised learning (Chiu et al., 2016).
Furthermore, they are quickly adopted into neu-
ral networks as these models take fixed-sized
vector inputs, where embeddings could be used
as encoded inputs into neural networks such as
LSTM (Hochreiter and Schmidhuber, 1997) and
CNN (LeCun et al., 1999) in the biomedical do-
main (Nentidis et al., 2017, 2018).

Though these embedding techniques were use-
ful in capturing latent semantics, they did not dis-
tinguish between multiple meanings of clinical
text (Molla and Gonzlez, 2007; Vine et al., 2015).

There have been several solutions to this prob-



57

lem (Peters et al., 2018; Howard and Ruder,
2018; Devlin et al., 2019) proposed but they are
not relevant specifically to the biomedical do-
main. Instead, we highlight BioBERT (Lee et al.,
2019), a biomedical version of BERT (Devlin
et al., 2019) which is a deeply bidirectional trans-
former (Vaswani et al., 2017) that is able to incor-
porate rich context into the encoding or embed-
ding process that has pre-trained on the Wikipedia
and PubMed corpora. However, this model fails to
account for the spatial and temporal aspects of dis-
eases in biomedical literature as temporality is not
encoded into its input. Furthermore, Biobert uses
a WordPiece tokeniser (Wu et al., 2016) which
keeps a fixed-size vocabulary dictionary for learn-
ing new words. However, the vocabulary within
the model is derived from Wikipedia, a general
domain corpus, and thus Biobert is unable to learn
distinct morphological semantics of medical terms
like -phobia, where ’-‘ denotes suffixation, mean-
ing fear as it only has the internal representation
for -bia.

3 Research Plan

We list the research questions to address some
of the research gaps in biomedical QA and the
system we aim to design, alongside baseline ap-
proaches and methodology as starting points. We
will also mention future directions to address these
research questions.

RQ1: What are the limitations of current
biomedical QA? The limitations in current
biomedical QA include the lack of: sufficient
ambiguity resolution tools (Wu et al., 2012),
robust techniques to using semantic neural ap-
proaches (Lee et al., 2019; Nentidis et al., 2018).
The lack of strong comprehension from systems
to produce sufficient answer spans that cross the
sentence boundary as reflected by poor results
in ideal answer production in BioASQ (Nentidis
et al., 2018, 2017) and addressing issues using
real-world patient queries rather than artificially
curated queries (Pampari et al., 2018; Guo et al.,
2006) which contain colloquial ambiguous non-
medical terminology such as hole in lung.

In our research, we aim to address each of these
gaps by researching into: higher coverage clini-
cal ambiguity tools that use contexts in the spatial
and temporal domains, summarisation techniques
that can translate from biomedical terminology to
patient language (Mishra et al., 2014; Shi et al.,

2018) and tuning biomedical models to solve com-
plex answer span tasks that cross sentence bound-
aries (Kociský et al., 2017) or require common
sense (Talmor et al., 2018).

RQ2: Data-driven approaches require high-
quality datasets. How can we construct or
leverage existing datasets to mimic real-world
biomedical question answering? By leverag-
ing existing techniques such as variational auto-
encoder (Shen et al., 2018) and Snorkel (Bach
et al., 2018), we will be able to generate, label and
process additional data that can meet stringent data
requirements of neural approaches.

However, synthetic datasets generally perform
weaker than handcrafted datasets (Bach et al.,
2018). In order to bridge this gap in the re-
search, we propose augmenting these data gener-
ation methods via crowd-sourcing methods with
textual entailment (Abacha and Demner-Fushman,
2016) and natural language inference (Johnson
et al., 2016) to improve the quality of the gener-
ated labels and data. For instance, we can use fo-
rums like Quora or medical specific forums such
as Health243 and utilise techniques such as ques-
tion entailment to find questions that are related
to ones seen in the dataset in order to generate
higher-quality annotated labels.

We will then develop techniques that can com-
bine synthetic and higher-quality labelled datasets
that can be utilized downstream in a QA system.
We will compare this against baselines such as
majority voting and Snorkel to evaluate our ap-
proaches.

Allowing the model to abstain from a deci-
sion, through comprehension, has been the focus
of many datasets as of late (Rajpurkar et al., 2016;
Kociský et al., 2017). We can use these datasets as
a starting problem to solve before applying these
techniques to the biomedical domain. However,
we will also develop and research further tech-
niques in order to allow for improved confidence
and low uncertainty from the model.

RQ3: How do we indicate the confidence of the
answer that the model has provided? Often re-
searchers interpret softmax or confidence scores
from the classifier models as direct correlations to
probability but often forget about uncertainties in
this measurement (Kendall and Gal, 2017). Due
to the real-world application and sensitivity of pre-

3https://www.health24.com/Experts



58

dictions in a health-based QA system, there needs
to be guarantees that predictions are of both high
accuracy and low uncertainty.

In order to account for uncertainty, techniques
such as Inductive Conformal Prediction (Pa-
padopoulos, 2008) and Deep Bayesian Learn-
ing (Siddhant and Lipton, 2018) can be used to
model epistemic uncertainty, which is not inher-
ently captured by the model during training, in or-
der to make the loss function more robust to noise
and uncertainty and thereby strengthen the predic-
tions of the model. This would then allow soft-
max scores to be used as confidence scores within
a reasonable level of uncertainty.

RQ4: How do we include temporality or lo-
cality of diseases into answers? Diseases are
non-static, they evolve such as the flu or are sea-
sonal such as the summer cold. Current models
utilise only static vector inputs, such as word em-
beddings, that do not account for this temporal as-
pect of the input. Furthermore, though diseases
are non-static, they may be more likely in different
countries as there is a spatiotemporal relationship
where countries will experience different seasons
and thus different diseases. In order to accommo-
date for these relationships, we can draw on prior
research as starting points such as space-time lo-
cal embeddings (Sun et al., 2015), dynamic word
embeddings (Bamler and Mandt, 2017) or time-
embeddings (Barbieri et al., 2018) as baselines and
extend them into the biomedical setting.

RQ5: How do we bridge the semantic gap be-
tween clinical text and terminology that a pa-
tient can understand? Most patients lack the
expertise in utilising resources such as biomed-
ical literature in order to self-diagnose. There-
fore, knowledge or answers should be presented
in a form that they can understand (Graham and
Brookey, 2008). Biomedical language and pa-
tient language can be construed as two sepa-
rate languages as biomedical language changes
and evolves over time (Yan and Zhu, 2018) and
also pose the same problems (Hunter and Cohen,
2006). Therefore, we can model this problem as
a language translation problem and thus can use
techniques in neural machine translation (Qi et al.,
2018; Chousa et al., 2018) based on word embed-
dings.

However, as biomedical language and patient
English are primarily borne of the same language,
this poses unique problems. For instance, a token

in plain English may translate to several tokens in
the biomedical space or vice versa. This is known
as the alignment problem (Qi et al., 2018). We can
potentially remedy this by borrowing ideas from
n-gram embedding (Zhao et al., 2017) as a starting
point or using Biobert (Lee et al., 2019) projected
to a dual-language embedding space and use atten-
tion to produce the alignment. Furthermore, there
are biomedical abbreviations that need to be dis-
ambiguated before translation (Festag and Spreck-
elsen, 2017), for which we would use direct, rule-
based approaches using thesauri or tools such as
Metamap (Aronson and Lang, 2010) as our base-
line approaches and extend upon using data-driven
approaches (Wu et al., 2017).

4 Experimental Framework

4.1 Datasets

High-quality data is required to address the chal-
lenges we outlined. We therefore consider the
following datasets: (1) MEDNLI (Johnson et al.,
2016; Goldberger et al., 2000) for medical lan-
guage inference; (2) i2b2 in the form of em-
rQA (Pampari et al., 2018) for synthetic question-
answer pairs; (3) SQuAD (Rajpurkar et al.,
2016) for open-domain transfer learning; (4) the
question-answering datasets provided on MediQA
20194; (5) the question entailment dataset and
MedQuAD (Ben Abacha and Demner-Fushman,
2019); (5) CLEF eHealth (Suominen et al., 2018)
to utilize and evaluate IR methods; and (6) we will
supplement our datasets by generating labels for
unlabelled data by leveraging the signals from the
labelled datasets through the use of tools such as
Snorkel (Bach et al., 2018) and CVAE (Shen et al.,
2018).

4.2 Evaluation Metrics

In our experiments, we will evaluate our
summarisation strategies with metrics such as
ROGUE (Lin, 2004), in particular, rogue-
2 (Owczarzak and Dang, 2009) and BLEU (Pap-
ineni et al., 2002). For question-answering, we use
standard ranking metrics such as Mean Average
Precision and Mean Reciprocal Rank for evaluat-
ing candidate ranking and standard metrics such
as f1-score, Precision, Accuracy and more medi-
cal targeted metrics such as sensitivity and speci-
ficity (Parikh et al., 2008).

4https://sites.google.com/view/mediqa2019



59

4.3 Proposed Framework

From the research questions mentioned, we pro-
pose a framework to unify their solutions.

Embeddings To begin, we need to construct our
date/seasonal embeddings (Barbieri et al., 2018),
to do this, we will need datasets that have mentions
of the seasonality and locality of disease entities.
Also, we will require embeddings that are repre-
sentative of the text, we will consider state-of-the-
art word-level context sensitive embeddings (Lee
et al., 2019; Peters et al., 2018) and word-level
context insensitive embeddings (Chiu et al., 2016)
and ensure they properly represent the biomedi-
cal datasets. For instance, BERT will need to pre-
trained with a biomedical vocabulary rather than
a general purpose open-domain one, and, in doing
so, we will be able to resolve ambiguity in poly-
semy or abbreviations.

Furthermore, we will also be researching
methodologies to handle out-of-vocabulary words
as the current WordPiece tokenization (Devlin
et al., 2019) or character-level embeddings (Barbi-
eri et al., 2018) would not be sufficient to address
esoteric terminology (Lee et al., 2019). The time
embeddings and the word-level embeddings will
be concatenated and used as input to the model.

Model Architecture Given the success of multi-
task learning (Zhao et al., 2018; Liu et al., 2019),
and having been proposed as the blocking task in
NLP (McCann et al., 2018) that needs to be solved.
We therefore apply multi-task learning to this
problem. From the state of the art multi-task learn-
ing models, we borrow the fundamental building
blocks such as multi-headed self-attention (Liu
et al., 2019) and multi-pointer generation (Mc-
Cann et al., 2018) to be used as decisions in a
Neural Architecture Search (NAS) (Zoph and Le,
2016). NAS will use reinforcement learning tech-
niques to find a suitable architecture for multi-task
learning. We elect to find the architecture to rep-
resent our problem this way due to one main rea-
son. The reason is that the field of deep learning
in NLP is quickly changing, and thus the state-
of-the-art techniques will always change. There-
fore, by having a tool that builds architectures
from the building blocks of state-of-the-art mod-
els is vital. However, crucially, we must add Het-
eroscedastic Aleatoric Uncertainty and Epistemic
Uncertainty minimisation to the model by adjust-
ing the loss function and weight distribution which

will allow the model to be more certain about deci-
sions (Kendall and Gal, 2017). One such decision
must be the ability to abstain from answering.

Concretely, we use NAS to discover models for
NMT from clinical text to the patient language
by conditioning to an encoder-decoder structure.
From here, using this model a starting point, NAS
will add task-specific layers that will minimise the
joint loss over the biomedical tasks such as ques-
tion answering (Nentidis et al., 2018), question
entailment (Abacha and Demner-Fushman, 2016)
and natural language inference (Johnson et al.,
2016). In doing so, multi-task learning will allow
for stronger generalisability and end-to-end train-
ing (McCann et al., 2018; Liu et al., 2019).

5 Summary

We highlight gaps within the literature in ques-
tion answering in the biomedical domain. We
outline challenges associated with implementing
these systems due to the limitations of current
work: lack of annotated data, ambiguity in clin-
ical text and lack of comprehension of ques-
tion/answer text by models.

We motivate this research in the area of patient
QA due to the high volume of medical queries in
search engines that are trusted by patients. Our re-
search aims to build upon the strengths of the cur-
rent state-of-the-art and research new strategies in
solving technical challenges to support a patient
in retrieving the answers they require with low un-
certainty and high confidence.

Acknowledgements

I thank for my supervisors, Dr Sarvnaz Karimi and
Dr Zhenchang Xing for providing invaluable in-
sight into the writing of this proposal. This re-
search is supported by the Australian Research
Training Program and the CSIRO Postgraduate
Scholarship.

References
Ben Abacha and Demner-Fushman. 2016. Recogniz-

ing Question Entailment for Medical Question An-
swering. American Medical Informatics Association
Annual Symposium Proceedings, 2016:310–318.

Diego Aliod, Menno Zaanen, and Daniel Smith. 2006.
Named entity recognition for question answering. In
The Australasian Language Technology Association,
Sydney, Australia.

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333286/
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333286/
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333286/
https://www.aclweb.org/anthology/U06-1009


60

Flora Amato, Stefano Marrone, Vincenzo Moscato,
Gabriele Piantadosi, Antonio Picariello, and Carlo
Sansone. 2017. Chatbots meet ehealth: Automa-
tizing healthcare. In Proceedings of the Workshop
on Artificial Intelligence with Application in Health
co-located with the 16th International Conference
of the Italian Association for Artificial Intelligence,
Bari, Italy.

Alan Aronson and François-Michel Lang. 2010. An
overview of Metamap: Historical perspective and
recent advances. Journal of the American Medical
Informatics Association, 17(3):229–236.

Jonas Austerjost, Marc Porr, Noah Riedel, Dominik
Geier, Thomas Becker, Thomas Scheper, Daniel
Marquard, Patrick Lindner, and Sascha Beutel.
2018. Introducing a virtual assistant to the lab: A
voice user interface for the intuitive control of lab-
oratory instruments. SLAS Technology: Translating
Life Sciences Innovation, 23:476–482.

Stephen Bach, Daniel Rodriguez, Yintao Liu, Chong
Luo, Haidong Shao, Cassandra Xia, Souvik Sen,
Alexander Ratner, Braden Hancock, Houman Al-
borzi, Rahul Kuchhal, Christopher Ré, and Rob
Malkin. 2018. Snorkel drybell: A case study in de-
ploying weak supervision at industrial scale. Com-
puting Research Repository, abs/1812.00417.

Robert Bamler and Stephan Mandt. 2017. Dy-
namic Word Embeddings. arXiv e-prints, page
arXiv:1702.08359.

Francesco Barbieri, Luı́s Marujo, Pradeep Karuturi,
William Brendel, and Horacio Saggion. 2018. Ex-
ploring emoji usage and prediction through a tempo-
ral variation lens. Computing Research Repository,
abs/1805.00731.

Asma Ben Abacha and Dina Demner-Fushman.
2019. A question-entailment approach to ques-
tion answering. Computing Research Repository,
abs/1901.08079.

Nick Bradley, Thomas Fritz, and Reid Holmes. 2018.
Context-aware conversational developer assistants.
In Proceedings of the 40th International Conference
on Software Engineering, pages 993–1003, New
York, NY, US.

Khyathi Chandu, Aakanksha Naik, Aditya Chan-
drasekar, Zi Yang, Niloy Gupta, and Eric Nyberg.
2017. Tackling biomedical text summarization:
OAQA at BioASQ 5B. In BioNLP 2017, pages 58–
66, Vancouver, Canada,.

Billy Chiu, Gamal Crichton, Anna Korhonen, and
Sampo Pyysalo. 2016. How to train good word em-
beddings for biomedical NLP. In Proceedings of
the 15th Workshop on Biomedical Natural Language
Processing, pages 166–174, Berlin, Germany.

Katsuki Chousa, Katsuhito Sudoh, and Satoshi Naka-
mura. 2018. Training neural machine translation
using word embedding-based loss. Computing Re-
search Repository, abs/1807.11219.

Anthony Cocco, Rachel Zordan, David Taylor, Tracey
Weiland, Stuart Dilley, Joyce Kant, Mahesha Dom-
bagolla, Andreas Hendarto, Fiona Lai, and Jennie
Hutton. 2018. Dr Google in the ED: searching for
online health information by adult emergency de-
partment patients. The Medical Journal of Aus-
tralia, 209:342–347.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language un-
derstanding. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Minneapolis, MN.

Sven Festag and Cord Spreckelsen. 2017. Word
Sense Disambiguation of Medical Terms via Recur-
rent Convolutional Neural Networks, volume 236.
Health Informatics Meets eHealth.

Ary Goldberger, Luis Amaral, Leon Glass, Jeffrey
Hausdorff, Plamen Ivanov, Roger Mark, Joseph Mi-
etus, George Moody, Chung-Kang Peng, and Eu-
gene Stanley. 2000. PhysioBank, PhysioToolkit,
and PhysioNet: components of a new research re-
source for complex physiologic signals. Circula-
tion, 101(23):E215–220.

Suzanne Graham and John Brookey. 2008. Do patients
understand? The Permanente journal, 12(3):67–69.

Jiafeng Guo, Yixing Fan, Qingyao Ai, and Bruce
Croft. 2017. A deep relevance matching model for
ad-hoc retrieval. Computing Research Repository,
abs/1711.08611.

Yikun Guo, Robert Gaizauskas, Ian Roberts, and
George Demetriou. 2006. Identifying personal
health information using support vector machines.
In i2b2 Workshop on Challenges in Natural Lan-
guage Processing for Clinical Data, Washington,
DC, US.

Sepp Hochreiter and Jrgen Schmidhuber. 1997. Long
short-term memory. Neural Computing, 9(8):1735–
1780.

Jeremy Howard and Sebastian Ruder. 2018. Univer-
sal language model fine-tuning for text classifica-
tion. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 328–339, Melbourne,
Australia.

Kai Hui, Andrew Yates, Klaus Berberich, and Gerard
de Melo. 2017. A position-aware deep model for
relevance matching in information retrieval. Com-
puting Research Repository, abs/1704.03940.

Lawrence Hunter and Bretonnel Cohen. 2006.
Biomedical language processing: what’s beyond
pubmed? Molecular Cell, 21(5):589–594.

http://ceur-ws.org/Vol-1982/paper6.pdf
http://ceur-ws.org/Vol-1982/paper6.pdf
https://doi.org/10.1136/jAmerican Medical Informatics Association.2009.002733
https://doi.org/10.1136/jAmerican Medical Informatics Association.2009.002733
https://doi.org/10.1136/jAmerican Medical Informatics Association.2009.002733
https://doi.org/10.1177/2472630318788040
https://doi.org/10.1177/2472630318788040
https://doi.org/10.1177/2472630318788040
http://arxiv.org/abs/1812.00417
http://arxiv.org/abs/1812.00417
http://arxiv.org/abs/1702.08359
http://arxiv.org/abs/1702.08359
http://arxiv.org/abs/1805.00731
http://arxiv.org/abs/1805.00731
http://arxiv.org/abs/1805.00731
http://arxiv.org/abs/1901.08079
http://arxiv.org/abs/1901.08079
https://doi.org/10.1145/3180155.3180238
https://doi.org/10.18653/v1/W17-2307
https://doi.org/10.18653/v1/W17-2307
https://doi.org/10.18653/v1/W16-2922
https://doi.org/10.18653/v1/W16-2922
http://arxiv.org/abs/1807.11219
http://arxiv.org/abs/1807.11219
https://doi.org/10.5694/mja17.00889
https://doi.org/10.5694/mja17.00889
https://doi.org/10.5694/mja17.00889
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
https://doi.org/10.3233/978-1-61499-759-7-8
https://doi.org/10.3233/978-1-61499-759-7-8
https://doi.org/10.3233/978-1-61499-759-7-8
https://www.ncbi.nlm.nih.gov/pubmed/10851218
https://www.ncbi.nlm.nih.gov/pubmed/10851218
https://www.ncbi.nlm.nih.gov/pubmed/10851218
https://www.ncbi.nlm.nih.gov/pubmed/21331214
https://www.ncbi.nlm.nih.gov/pubmed/21331214
http://arxiv.org/abs/1711.08611
http://arxiv.org/abs/1711.08611
https://www.researchgate.net/publication/267239246_Identifying_Personal_Health_Information_Using_Support_Vector_Machines
https://www.researchgate.net/publication/267239246_Identifying_Personal_Health_Information_Using_Support_Vector_Machines
https://doi.org/10.1162/neco.1997.9.8.1735
https://doi.org/10.1162/neco.1997.9.8.1735
https://www.aclweb.org/anthology/P18-1031
https://www.aclweb.org/anthology/P18-1031
https://www.aclweb.org/anthology/P18-1031
http://arxiv.org/abs/1704.03940
http://arxiv.org/abs/1704.03940
https://doi.org/10.1016/j.molcel.2006.02.012
https://doi.org/10.1016/j.molcel.2006.02.012


61

Alistair Johnson, Tom Pollard, Lu Shen, Li-wei
Lehman, Mengling Feng, Mohammad Ghassemi,
Benjamin Moody, Peter Szolovits, Anthony Celi,
and Roger Mark. 2016. MIMIC-III, a freely accessi-
ble critical care database. Scientific Data, 3:160035.

Aniket Kadam, Shashank Joshi, Sachin Shinde, and
Sampat Medhane. 2015. Notice of retraction ques-
tion answering search engine short review and road-
map to future qa search engine. In Interna-
tional Conference on Electrical, Electronics, Sig-
nals, Communication and Optimization, pages 1–8,
Visakhapatnam, India.

Alex Kendall and Yarin Gal. 2017. What uncertain-
ties do we need in bayesian deep learning for com-
puter vision? Computing Research Repository,
abs/1703.04977.

Tomás Kociský, Jonathan Schwarz, Phil Blunsom,
Chris Dyer, Karl Hermann, Gábor Melis, and Ed-
ward Grefenstette. 2017. The narrativeqa read-
ing comprehension challenge. Computing Research
Repository, abs/1712.07040.

Ida Korfage, Harry Koning, Monique Roobol, Fritz
Schrder, and Marie-Louise Essink-Bot. 2006.
Prostate cancer diagnosis: The impact on pa-
tients mental health. European Journal of Cancer,
42(2):165 – 170.

Yann LeCun, Patrick Haffner, Léon Bottou, and
Yoshua Bengio. 1999. Object recognition with
gradient-based learning. In Shape, Contour and
Grouping in Computer Vision, page 319, London,
UK.

Jinhyuk Lee, Wonjin Yoon, Sungdong Kim,
Donghyeon Kim, Sunkyu Kim, Chan Ho So,
and Jaewoo Kang. 2019. BioBERT: A pre-trained
biomedical language representation model for
biomedical text mining. arXiv e-prints, page
arXiv:1901.08746.

Minsuk Lee, James Cimino, Hai Zhu, Carl Sable, Vi-
jay Shanker, John Ely, and Hong Yu. 2006. Beyond
information retrieval–medical question answering.
American Medical Informatics Association Annual
Symposium Proceedings, 2006:469–473.

Sweta Lende and Mukesh Raghuwanshi. 2016. Ques-
tion answering system on education acts using nlp
techniques. In World Conference on Futuristic
Trends in Research and Innovation for Social Wel-
fare (Startup Conclave), pages 1–6, Coimbatore, In-
dia.

Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out: Proceedings of the Association
for Computational Linguistics Workshop, pages 74–
81, Barcelona, Spain.

Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-
feng Gao. 2019. Improving multi-task deep neural

networks via knowledge distillation for natural lan-
guage understanding. Computing Research Reposi-
tory, arXiv:1904.0948.

Nidhi Malik, Aditi Sharan, and Payal Biswas. 2013.
Domain knowledge enriched framework for re-
stricted domain question answering system. In IEEE
International Conference on Computational Intelli-
gence and Computing Research, pages 1–7, Madu-
rai, Tamilnadu, India.

Yuqing Mao, Chih-Hsuan Wei, and Zhiyong Lu. 2014.
NCBI at the 2014 BioASQ challenge task: Large-
scale biomedical semantic indexing and question an-
swering. In Conference and Labs of the Evaluation
Forum, Sheffield, UK.

Bryan McCann, Nitish Shirish Keskar, Caiming Xiong,
and Richard Socher. 2018. The natural language de-
cathlon: Multitask learning as question answering.
Computing Research Repository, abs/1806.08730.

Ryan McDonald, Georgios-Ioannis Brokos, and Ion
Androutsopoulos. 2018. Deep relevance ranking us-
ing enhanced document-query interactions. Com-
puting Research Repository, abs/1809.01682.

Rashmi Mishra, Jiantao Bian, Marcelo Fiszman, Char-
lene Weir, Siddhartha Jonnalagadda, Javed Mostafa,
and Guilherme Del Fiol. 2014. Text summarization
in the biomedical domain: a systematic review of
recent research. Journal of Biomedical Informatics,
52:457–467.

Diego Molla and Jos Gonzlez. 2007. Question answer-
ing in restricted domains: An overview. Computa-
tional Linguistics, 33:41–61.

Anastasios Nentidis, Konstantinos Bougiatiotis, Anas-
tasia Krithara, Georgios Paliouras, and Ioannis
Kakadiaris. 2017. Results of the fifth edition of the
bioasq challenge. In Biomedical Natural Language
Processing, pages 48–57, Vancouver, Canada.

Anastasios Nentidis, Anastasia Krithara, Konstanti-
nos Bougiatiotis, Georgios Paliouras, and Ioannis
Kakadiaris. 2018. Results of the sixth edition
of the BioASQ challenge. In Proceedings of the
6th BioASQ Workshop A challenge on large-scale
biomedical semantic indexing and question answer-
ing, pages 1–10, Brussels, Belgium.

Karolina Owczarzak and Hoa Dang. 2009. Evaluation
of automatic summaries: Metrics under varying data
conditions. In Proceedings of the Workshop on Lan-
guage Generation and Summarisation, pages 23–30,
Stroudsburg, PA, US.

Anusri Pampari, Preethi Raghavan, Jennifer Liang, and
Jian Peng. 2018. emrqa: A large corpus for question
answering on electronic medical records. Comput-
ing Research Repository, abs/1809.00732.

Harris Papadopoulos. 2008. Inductive Conformal Pre-
diction: Theory and Application to Neural Net-
works.

https://doi.org/10.1038/sdata.2016.35
https://doi.org/10.1038/sdata.2016.35
https://doi.org/10.1109/EESCO.2015.7253949
https://doi.org/10.1109/EESCO.2015.7253949
https://doi.org/10.1109/EESCO.2015.7253949
http://arxiv.org/abs/1703.04977
http://arxiv.org/abs/1703.04977
http://arxiv.org/abs/1703.04977
http://arxiv.org/abs/1712.07040
http://arxiv.org/abs/1712.07040
https://doi.org/https://doi.org/10.1016/j.ejca.2005.10.011
https://doi.org/https://doi.org/10.1016/j.ejca.2005.10.011
http://dl.acm.org/citation.cfm?id=646469.691875
http://dl.acm.org/citation.cfm?id=646469.691875
http://arxiv.org/abs/1901.08746
http://arxiv.org/abs/1901.08746
http://arxiv.org/abs/1901.08746
https://www.ncbi.nlm.nih.gov/pubmed/17238385
https://www.ncbi.nlm.nih.gov/pubmed/17238385
https://doi.org/10.1109/STARTUP.2016.7583963
https://doi.org/10.1109/STARTUP.2016.7583963
https://doi.org/10.1109/STARTUP.2016.7583963
https://www.aclweb.org/anthology/W04-1013
https://www.aclweb.org/anthology/W04-1013
http://arxiv.org/abs/1904.0948
http://arxiv.org/abs/1904.0948
http://arxiv.org/abs/1904.0948
https://doi.org/10.1109/ICCIC.2013.6724163
https://doi.org/10.1109/ICCIC.2013.6724163
https://www.semanticscholar.org/paper/NCBI-at-the-2014-BioASQ-Challenge-Task%3A-Large-scale-Mao-Wei/ade85f13fe19572641743f0090617983f50d0f2e
https://www.semanticscholar.org/paper/NCBI-at-the-2014-BioASQ-Challenge-Task%3A-Large-scale-Mao-Wei/ade85f13fe19572641743f0090617983f50d0f2e
https://www.semanticscholar.org/paper/NCBI-at-the-2014-BioASQ-Challenge-Task%3A-Large-scale-Mao-Wei/ade85f13fe19572641743f0090617983f50d0f2e
http://arxiv.org/abs/1806.08730
http://arxiv.org/abs/1806.08730
http://arxiv.org/abs/1809.01682
http://arxiv.org/abs/1809.01682
https://doi.org/10.1016/j.jbi.2014.06.009
https://doi.org/10.1016/j.jbi.2014.06.009
https://doi.org/10.1016/j.jbi.2014.06.009
https://doi.org/10.1162/coli.2007.33.1.41
https://doi.org/10.1162/coli.2007.33.1.41
https://doi.org/10.18653/v1/W17-2306
https://doi.org/10.18653/v1/W17-2306
https://www.aclweb.org/anthology/W18-5301
https://www.aclweb.org/anthology/W18-5301
http://dl.acm.org/citation.cfm?id=1708155.1708161
http://dl.acm.org/citation.cfm?id=1708155.1708161
http://dl.acm.org/citation.cfm?id=1708155.1708161
http://arxiv.org/abs/1809.00732
http://arxiv.org/abs/1809.00732
https://doi.org/10.5772/6078
https://doi.org/10.5772/6078
https://doi.org/10.5772/6078


62

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, US.

Rajul Parikh, Annie Mathai, Shefali Parikh, Chandra
Sekhar, and Ravi Thomas. 2008. Understanding and
using sensitivity, specificity and predictive values.
Indian Journal of Ophthalmology, 56(1):45–50.

Shengwen Peng, Ronghui You, Zhikai Xie, Beichen
Wang, Yanchun Zhang, and Shanfeng Zhu. 2015.
The fudan participation in the 2015 bioasq chal-
lenge: Large-scale biomedical semantic indexing
and question answering. In Conference and Labs of
the Evaluation Forum 2015: Conference and Labs of
the Evaluation Forum Experimental IR meets Mul-
tilinguality, Multimodality and Interaction, volume
1391, Toulouse, France.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word
representations. Computing Research Repository,
abs/1802.05365.

Sameerchand Pudaruth, Kajal Boodhoo, and Lushika
Goolbudun. 2016. An intelligent question answer-
ing system for ICT. In 2016 International Con-
ference on Electrical, Electronics, and Optimiza-
tion Techniques, pages 2895–2899, Paralakhemundi,
Odisha, India.

Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2004.
Mapping dependencies trees: An application to
question answering. In In Proceedings of the 8th In-
ternational Symposium on Artificial Intelligence and
Mathematics, Fort, Fort Lauderdale, Flordia.

Ye Qi, Devendra Sachan, Matthieu Felix, Sarguna Pad-
manabhan, and Graham Neubig. 2018. When and
why are pre-trained word embeddings useful for
neural machine translation? Computing Research
Repository, abs/1804.06323.

Minghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan
Chen, Weipeng Zhao, Haiqing Chen, Jun Huang,
and Wei deep-multi-task-learning Chu. 2017. Al-
iMe chat: A sequence to sequence and rerank based
chatbot engine. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 498–503,
Vancouver, Canada.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100, 000+ questions
for machine comprehension of text. Computing Re-
search Repository, abs/1606.05250.

Ashwin Ram, Rohit Prasad, Chandra Khatri, Anu
Venkatesh, Raefer Gabriel, Qing Liu, Jeff Nunn,
Behnam Hedayatnia, Ming Cheng, Ashish Nagar,
Eric King, Kate Bland, Amanda Wartick, Yi Pan,

Han Song, Sk Jayadevan, Gene Hwang, and Art Pet-
tigrue. 2018. Conversational AI: the science behind
the alexa prize. Computing Research Repository,
abs/1801.03604.

Tony Russell-Rose and Jon Chamberlain. 2017. Expert
search strategies: The information retrieval practices
of healthcare information professionals. JMIR Med-
ical Informatics, 5(4):e33.

Sheng Shen, Yaliang Li, Nan Du, Xian Wu, Yusheng
Xie, Shen Ge, Tao Yang, Kai Wang, Xingzheng
Liang, and Wei Fan. 2018. On the generation
of medical question-answer pairs. Computing Re-
search Repository, abs/1811.00681.

Tian Shi, Yaser Keneshloo, Naren Ramakrishnan, and
Chandan Reddy. 2018. Neural abstractive text
summarization with sequence-to-sequence models.
Computing Research Repository, abs/1812.02303.

Aditya Siddhant and Zachary Lipton. 2018. Deep
bayesian active learning for natural language pro-
cessing: Results of a large-scale empirical study.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2904–2909, Brussels, Belgium.

Marco Soares and Fernando Parreiras. 2018. A lit-
erature review on question answering techniques,
paradigms and systems. Journal of King Saud Uni-
versity - Computer and Information Sciences.

Ke Sun, Jun Wang, Alexandros Kalousis, and Stephane
Marchand-Maillet. 2015. Space-time local embed-
dings. In Advances in Neural Information Process-
ing Systems 28, pages 100–108.

Hanna Suominen, Liadh Kelly, Lorraine Goeuriot,
Aurélie Névéol, Lionel Ramadier, Aude Robert,
Evangelos Kanoulas, Rene Spijker, Leif Azzopardi,
Dan Li, Jimmy, João Palotti, and Guido Zuccon.
2018. Overview of the conference and labs of the
evaluation forum ehealth evaluation lab 2018. In Ex-
perimental IR Meets Multilinguality, Multimodality,
and Interaction, pages 286–301, Avignon, France.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2018. Commonsenseqa: A
question answering challenge targeting common-
sense knowledge. Computing Research Repository,
abs/1811.00937.

Rafael Terol, Patricio Martinez-Barco, and Manuel
Palomar. 2007. A knowledge based method for the
medical question answering problem. Computers in
Biology and Medicine, 37(10):1511–1521.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is
all you need. Computing Research Repository,
abs/1706.03762.

https://doi.org/10.3115/1073083.1073135
https://doi.org/10.3115/1073083.1073135
https://www.ncbi.nlm.nih.gov/pubmed/18158403
https://www.ncbi.nlm.nih.gov/pubmed/18158403
http://vuir.vu.edu.au/30662/
http://vuir.vu.edu.au/30662/
http://vuir.vu.edu.au/30662/
http://arxiv.org/abs/1802.05365
http://arxiv.org/abs/1802.05365
https://doi.org/10.1109/ICEEOT.2016.7755228
https://doi.org/10.1109/ICEEOT.2016.7755228
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.1.4864
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.1.4864
http://arxiv.org/abs/1804.06323
http://arxiv.org/abs/1804.06323
http://arxiv.org/abs/1804.06323
https://doi.org/10.18653/v1/P17-2079
https://doi.org/10.18653/v1/P17-2079
https://doi.org/10.18653/v1/P17-2079
http://arxiv.org/abs/1606.05250
http://arxiv.org/abs/1606.05250
http://arxiv.org/abs/1801.03604
http://arxiv.org/abs/1801.03604
https://doi.org/10.2196/medinform.7680
https://doi.org/10.2196/medinform.7680
https://doi.org/10.2196/medinform.7680
http://arxiv.org/abs/1811.00681
http://arxiv.org/abs/1811.00681
http://arxiv.org/abs/1812.02303
http://arxiv.org/abs/1812.02303
https://www.aclweb.org/anthology/D18-1318
https://www.aclweb.org/anthology/D18-1318
https://www.aclweb.org/anthology/D18-1318
https://doi.org/https://doi.org/10.1016/j.jksuci.2018.08.005
https://doi.org/https://doi.org/10.1016/j.jksuci.2018.08.005
https://doi.org/https://doi.org/10.1016/j.jksuci.2018.08.005
http://papers.nips.cc/paper/5971-space-time-local-embeddings.pdf
http://papers.nips.cc/paper/5971-space-time-local-embeddings.pdf
https://link.springer.com/chapter/10.1007/978-3-319-98932-7_26
https://link.springer.com/chapter/10.1007/978-3-319-98932-7_26
http://arxiv.org/abs/1811.00937
http://arxiv.org/abs/1811.00937
http://arxiv.org/abs/1811.00937
https://www.ncbi.nlm.nih.gov/pubmed/17374369
https://www.ncbi.nlm.nih.gov/pubmed/17374369
http://arxiv.org/abs/1706.03762
http://arxiv.org/abs/1706.03762


63

José Vicedo and Antonio Ferrández. 2000. Importance
of pronominal anaphora resolution in question an-
swering systems. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics, pages 555–562, Hong Kong.

Lance Vine, Mahnoosh Kholghi, Guido Zuccon, Lau-
rianne Sitbon, and Anthony Nguyen. 2015. Anal-
ysis of word embeddings and sequence features for
clinical information extraction. In Proceedings of
the Australasian Language Technology Association
Workshop 2015, pages 21–30, Parramatta, Australia.

Dirk Weienborn, George Tsatsaronis, and Michael
Schroeder. 2013. Answering factoid questions in the
biomedical domain. In CEUR Workshop Proceed-
ings, volume 1094, Valencia, Spain.

Yonghui Wu, Joshua Denny, Trent Rosenbloom, Ran-
dolph Miller, Dario Giuse, and Hua Xu. 2012. A
comparative study of current clinical natural lan-
guage processing systems on handling abbreviations
in discharge summaries. American Medical Infor-
matics Association Annual Symposium Proceedings,
2012:997–1003.

Yonghui Wu, Joshua Denny, Rosenbloom Trent, Ran-
dolph Miller, Dario Giuse, Lulu Wang, Carmelo
Blanquicett, Ergin Soysal, Jun Xu, and Hua Xu.
2017. A long journey to short abbreviations: de-
veloping an open-source framework for clinical ab-
breviation recognition and disambiguation (CARD).
Journal of the American Medical Informatics Asso-
ciation, 24(e1):e79–e86.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc Le,
Mohammad Norouzi, Wolfgang Macherey, Maxim
Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff
Klingner, Apurva Shah, Melvin Johnson, Xiaob-
ing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo
Kato, Taku Kudo, Hideto Kazawa, Keith Stevens,
George Kurian, Nishant Patil, Wei Wang, Cliff
Young, Jason Smith, Jason Riesa, Alex Rudnick,
Oriol Vinyals, Greg Corrado, Macduff Hughes, and
Jeffrey Dean. 2016. Google’s neural machine trans-
lation system: Bridging the gap between human and
machine translation. Computing Research Reposi-
tory, abs/1609.08144.

Ramin Yaghoubzadeh and Stefan Kopp. 2012. Toward
a virtual assistant for vulnerable users: Designing
careful interaction. In Proceedings of the 1st Work-
shop on Speech and Multimodal Interaction in As-
sistive Environments, pages 13–17, Jeju, Republic of
Korea. Association for Computational Linguistics.

Erjia Yan and Yongjun Zhu. 2018. Tracking word
semantic change in biomedical literature. Interna-
tional Journal of Medical Informatics, 109:76 – 86.

Zhao Yan, Nan Duan, Junwei Bao, Peng Chen, Ming
Zhou, Zhoujun Li, and Jianshe Zhou. 2016. Doc-
Chat: An information retrieval approach for chatbot
engines using unstructured documents. In Proceed-
ings of the 54th Annual Meeting of the Association

for Computational Linguistics (Volume 1: Long Pa-
pers), pages 516–525, Berlin, Germany. Association
for Computational Linguistics.

Wenpeng Yin, Hinrich Schütze, Bing Xiang, and
Bowen Zhou. 2015. ABCNN: attention-based
convolutional neural network for modeling sen-
tence pairs. Computing Research Repository,
abs/1512.05193.

Sendong Zhao, Ting Liu, Sicheng Zhao, and Fei Wang.
2018. A neural multi-task learning framework to
jointly model medical named entity recognition and
normalization. Computing Research Repository,
abs/1812.06081.

Zhe Zhao, Tao Liu, Shen Li, Bofang Li, and Xiaoyong
Du. 2017. Ngram2vec: Learning improved word
representations from ngram co-occurrence statistics.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
244–253, Copenhagen, Denmark.

Barret Zoph and Quoc Le. 2016. Neural architecture
search with reinforcement learning. Computing Re-
search Repository, abs/1611.01578.

https://doi.org/10.3115/1075218.1075288
https://doi.org/10.3115/1075218.1075288
https://doi.org/10.3115/1075218.1075288
https://www.aclweb.org/anthology/U15-1003
https://www.aclweb.org/anthology/U15-1003
https://www.aclweb.org/anthology/U15-1003
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.403.3868
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.403.3868
https://www.ncbi.nlm.nih.gov/pubmed/23304375
https://www.ncbi.nlm.nih.gov/pubmed/23304375
https://www.ncbi.nlm.nih.gov/pubmed/23304375
https://www.ncbi.nlm.nih.gov/pubmed/23304375
https://www.ncbi.nlm.nih.gov/pubmed/27539197
https://www.ncbi.nlm.nih.gov/pubmed/27539197
https://www.ncbi.nlm.nih.gov/pubmed/27539197
http://arxiv.org/abs/1609.08144
http://arxiv.org/abs/1609.08144
http://arxiv.org/abs/1609.08144
https://www.aclweb.org/anthology/W12-3503
https://www.aclweb.org/anthology/W12-3503
https://www.aclweb.org/anthology/W12-3503
https://doi.org/https://doi.org/10.1016/j.ijmedinf.2017.11.006
https://doi.org/https://doi.org/10.1016/j.ijmedinf.2017.11.006
https://doi.org/10.18653/v1/P16-1049
https://doi.org/10.18653/v1/P16-1049
https://doi.org/10.18653/v1/P16-1049
http://arxiv.org/abs/1512.05193
http://arxiv.org/abs/1512.05193
http://arxiv.org/abs/1512.05193
http://arxiv.org/abs/1812.06081
http://arxiv.org/abs/1812.06081
http://arxiv.org/abs/1812.06081
https://doi.org/10.18653/v1/D17-1023
https://doi.org/10.18653/v1/D17-1023
http://arxiv.org/abs/1611.01578
http://arxiv.org/abs/1611.01578

