



















































Translation reranking using source phrase dependency features


Proceedings of SSST-9, Ninth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 57–60,
Denver, Colorado, June 4, 2015. c©2015 Association for Computational Linguistics

Translation reranking using source phrase dependency features

Antonio Valerio Miceli-Barone
Dipartimento di Informatica

Largo B. Pontecorvo, 3
56127 Pisa, Italy

miceli@di.unipi.it

Abstract

We describe a N-best reranking model
based on features that combine source-
side dependency syntactical information
and segmentation and alignment in-
formation. Specifically, we consider
segmentation-aware ”phrase depen-
dency” features.

1 Introduction

Dependency features have been used in the
past for both direct translation and reranking
(Gimpel and Smith, 2013), usually in a string-
to-tree or a tree-to-tree configuration. These
approaches generally require the decoder to
be specifically designed to produce suitable
dependency structures on its output, or to
use a specialized target-side parser capable of
parsing potentially ungrammatical and unid-
iomatic sentences.

Instead, we investigated a tree-to-string N-
best reranking model suitable for use with a
standard phrase-based decoder and a standard
source-side dependency parser.

2 Source phrase dependency model

Dependency relations in a conventional depen-
dency tree are syntactical relations between in-
dividual words. A phrase-based decoder, in-
stead, operates in terms of phrase-pairs.

Each N-best candidate translation ei of a
source sentence f is defined by its derivation,
which describes how f has been segmented

into source phrases, how these source phrases
have been reorederd and for each source phrase
which corresponding target phrase has been
chosen.

In our model, we focus on the quality of
phrase segmentation and reordering.

Segmentation features The source phrases
produced by the segmentation performed by
the decoder do not necessarily correspond to
subtrees in the dependency parse tree (or for-
est) g f of the sentence. And if the dependency
parse is not projective, subtrees do not neces-
sarily correspond to contiguous phrases in any
possible segmentation.

We propose a set of multiple features which
operate at source phrase level, inspired by the
concept of phrase dependency relations of Gimpel
and Smith (2013):
Given a source phrase f̄ j in a derivation,
we define the set of its parent phrases
PARENTS( f̄ j) as the set of other phrases in
the same derivation which contain at least
one word that is a parent of some word in
f̄ j. We also define the sets of left parents
PARENTSL( f̄ j), right parents PARENTSR( f̄ j),
left children CHILDRENL( f̄ j) and right chil-
dren CHILDRENR( f̄ j). Note that only word
dependency relations that cross the phrase
boundaries are relevant to the definition of
these phrase dependency relations.

We propose the following segmentation
phrase feature functions:
No parents PARENTS( f̄ j) = ∅, no left par-

57



ents PARENTSL( f̄ j) = ∅, no right par-
ents PARENTSR( f̄ j) = ∅, one-sided par-
ents PARENTSL( f̄ j) = ∅ ∨ PARENTSR( f̄ j) =
∅. Unambiguous (no more than one) par-
ents |PARENTS( f̄ j)| ≤ 1, Unambiguous
left parents |PARENTSL( f̄ j)| ≤ 1, Unam-
biguous right parents |PARENTSL( f̄ j)| ≤ 1.
Unique parent |PARENTS( f̄ j)| = 1. No
children CHILDREN( f̄ j) = ∅, no left chil-
dren CHILDRENL( f̄ j) = ∅, no right children
CHILDRENR( f̄ j) = ∅, one-sided children
CHILDRENL( f̄ j) = ∅ ∨ CHILDRENR( f̄ j) =
∅.

When phrase segmentation breaks the syn-
tactic structures these features should be able
to detect it, and the model will penalize (or per-
haps reward) different types of breakages us-
ing parameters automatically learned by tun-
ing, similarly to Cherry (2008) or Marton and
Resnik (2008).

Distortion features We consider pairs of
source phrases which are aligned to target
phrases that are contiguous in target order.

Let f̃ j ≡ ( f̄a(j−1), f̄a(j)) be one of such pairs.
We define the following, mutually exclusive,
feature functions:
Unique parent-child PARENTS( f̄a(j)) =
{ f̄a(j−1)}. Unique child-parent
PARENTS( f̄a(j−1)) = { f̄a(j)}. Siblings with
unique parent ∃j′ : PARENTS( f̄a(j)) =
PARENTS( f̄a(j−1)) = f̄ j′ . None of the above.

We also define the inversion feature function
a(j− 1) > a(j) which is included both as an in-
dividual feature and in logical conjunction with
each of the feature functions defined above, re-
sulting in a total of nine boolean distortion fea-
ture functions.

These features detect reordering operations
which swap syntactic structures related by a
dependency relation between themselves or
with a shared parent structure, similarly to the
reordering operations in the synchronous depen-
dency insertion grammar of Ding and Palmer
(2005) or the syntactic coupling features of
Nikoulina and Dymetman (2008).

Scoring model The feature functions defined
in the two previous paragraphs are combined
into a vector which is concatenated to the fea-
ture vector produced by the decoder and multi-
plied by a parameter vector θ to obtain the final
reranking score for each candidate translation.
θ is trained using a standard machine trans-
lation tuning technique, namely K-best batch
MIRA (Cherry and Foster, 2012).

3 Experiments

Setup We tested our model in a Italian-to-
English 1000-best translation reranking task.

We trained the baseline phrase-based sys-
tem using a parallel corpus assembled from
Europarl v7 (Koehn, 2005), JRC-ACQUIS v2.2
(Steinberger et al., 2006) and additional bilin-
gual articles crawled from online newspaper
websites1, totaling 3,081,700 sentence pairs,
which were split into a 3,075,777 sp. phrase-
table training corpus, a 3,923 sp. tuning corpus,
and a 2,000 sp. test corpus.

We trained and tuned phrase-based Moses
(Koehn et al., 2007) using a ”sparse fea-
tures” configuration (the ”word translation”
and ”phrase translation” feature sets described
by Chiang et al. (2009)). We performed model
parameter tuning using k-best batch MIRA.
Non-projective dependency parse trees (actu-
ally, forests) for the Italian source sentences
have been computed using the transition-based
DeSR parser in tree revision configuration (At-
tardi and Ciaramita, 2007).

Significance was estimated using paired boot-
strap resampling (Koehn, 2004).

Results The results of these experiments are
shown in fig. 1.

We obtain a small but significant BLUE score
improvement.

We also performed other experiments with
slightly different feature function configura-
tions but we obtained lower scores, although
never lower than the baseline score of the de-
coder.

From a computational time point of view,
the reranker adds a negligible overhead the the

1Corriere.it and Asianews.it

58



Configuration BLEU-c BLEU
Moses + sparse feats. 29.02 29.82
Moses + sparse feats. + dep. feats. 29.17 (+ 0.15) 29.97 (+ 0.15)

Figure 1: Experimental results. BLEU and case-insensitive BLEU scores over a 2,000 sp. it-en test
corpus. Improvements are significant at the p ¡ 0.05 significance level.

runtime of the decoder, even in our unopti-
mized Python implementation.

Conclusions and future work We identified a
set of syntactic dependency features which can
provide small but significant translation qual-
ity improvements when used in N-best rerank-
ing, at least on the Italian-to-English language
pair. We need to perform experiments on other
language pairs to determine whether this result
generalizes.

Spurious effects due to optimizer instability
that can’t be detected by our significance tests
might be present. More advanced statistical
tests such as Clark et al. (2011) should be per-
formed to increase the confidence in the valid-
ity of our result.

In addition to reranking, our feature func-
tions could also be used for decoding in a stan-
dard phrase-based or hierarchical translation
system without a significant increase of de-
coding complexity, since they decompose addi-
tively over phrases or pair of phrase adjacent in
target-order. Performing such experiment will
be a natural extension of our work.

References

Giuseppe Attardi and Massimiliano Ciaramita.
2007. Tree revision learning for dependency
parsing. In Candace L. Sidner, Tanja Schultz,
Matthew Stone, and ChengXiang Zhai, editors,
HLT-NAACL, pages 388–395. The Association for
Computational Linguistics.

Colin Cherry and George Foster. 2012. Batch
tuning strategies for statistical machine transla-
tion. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 427–436. Association for Computa-
tional Linguistics.

Colin Cherry. 2008. Cohesive phrase-based decod-

ing for statistical machine translation. In In Pro-
ceedings of ACL-08: HLT, pages 72–80.

David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 218–226. Association for
Computational Linguistics.

Jonathan H Clark, Chris Dyer, Alon Lavie, and
Noah A Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Control-
ling for optimizer instability. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies:
short papers-Volume 2, pages 176–181. Association
for Computational Linguistics.

Yuan Ding and Martha Palmer. 2005. Machine
translation using probabilistic synchronous de-
pendency insertion grammars. In Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, ACL ’05, pages 541–548,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Kevin Gimpel and Noah A Smith. 2013. Phrase
dependency machine translation with quasi-
synchronous tree-to-tree features.

Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondřej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the ACL on Interactive Poster and Demonstra-
tion Sessions, ACL ’07, pages 177–180, Strouds-
burg, PA, USA. Association for Computational
Linguistics.

Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In EMNLP,
pages 388–395.

Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79–86, Phuket, Thailand. AAMT, AAMT.

59



Yuval Marton and Philip Resnik. 2008. Soft syn-
tactic constraints for hierarchical phrased-based
translation. In ACL, pages 1003–1011.

Vassilina Nikoulina and Marc Dymetman. 2008.
Using syntactic coupling features for discrimi-
nating phrase-based translations (wmt-08 shared
translation task). In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 159–
162. Association for Computational Linguistics.

Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Dniel Varga. 2006. The jrc-acquis: A multilin-
gual aligned parallel corpus with 20+ languages.
In Proceedings of the 5th International Conference
on Language Resources and Evaluation (LREC’2006),
Genoa, Italy.

60


