










































Chinese Event Coreference Resolution: Understanding the State of the Art


International Joint Conference on Natural Language Processing, pages 822–828,
Nagoya, Japan, 14-18 October 2013.

Chinese Event Coreference Resolution:
Understanding the State of the Art

Chen Chen and Vincent Ng
Human Language Technology Research Institute

University of Texas at Dallas
Richardson, TX 75083-0688

{yzcchen,vince}@hlt.utdallas.edu

Abstract

Given the relatively small amount of work
on event coreference resolution, our under-
standing of the task is arguably fairly lim-
ited. This makes it difficult to determine
how to improve an event coreference re-
solver. We seek to gain a better under-
standing of the state of the art in event
coreference resolution by performing the
first publicly available analysis of a Chi-
nese event coreference resolver.

1 Introduction

Event coreference resolution is the task of deter-
mining which event mentions in a text refer to the
same real-world event. Compared to entity coref-
erence (the task of determining which entity men-
tions in a text refer to the same real-world entity),
there is much less work on event coreference.
Given the lesser amount of work on event coref-

erence, our understanding of the task is arguably
fairly limited. Specifically, while it is not sur-
prising that the performance of an event corefer-
ence resolver depends heavily on the quality of the
output of its preprocessing components, it is not
clear to what extent the noise inherent in the out-
put of each preprocessing component is limiting
the performance of a resolver. Note that this is-
sue is more serious for event coreference than for
entity coreference: since an event coreference re-
solver lies towards the end of the information ex-
traction pipeline, it has to rely on the noisy output
produced by more upstream components than its
entity counterpart. The lack of understanding of
this issue makes it difficult to identify the compo-
nents that need most attention. Moreover, when
analyzing the errors made by a resolver, it is not
clear which types of errors can be fixed by im-
proving the resolution algorithm and which ones
can be fixed by improving the preprocessing com-

ponents. This makes it difficult to precisely de-
termine how to improve the resolution algorithm.
Taken together, these two issues make it difficult
to understand how an event coreference resolver
should be improved.
Our goal in this paper is to better understand the

state of the art in event coreference and provide di-
rections for further work on this task by addressing
the aforementioned issues, which are summarized
by the following two questions. First, to what ex-
tent is the noise inherent in the output of each of
its upstream components limiting its performance?
Second, what are the major types of errors that are
attributable to (and therefore can be fixed by im-
proving) the resolution algorithm?
We address these two questions by presenting

a systematic analysis of a state-of-the-art Chinese
event coreference resolver. Our decision to focus
on Chinese can be attributed in part to the lack of
publicly available results on Chinese event coref-
erence resolution. In particular, to our knowledge,
almost all recent work on event coreference has re-
ported results for English (e.g., Humphreys et al.
(1997), Chen et al. (2009), Bejan and Harabagiu
(2010), Chen et al. (2011), Lee et al. (2012)).1
Hence, the results in the paper can serve as a base-
line against which future work on Chinese event
coreference can be compared.
It is worth mentioning that similar questions

were faced by entity coreference researchers, who
have reached a point where it is crucial to answer
these questions in order to make further progress.
For that reason, a number of recent research pa-
pers have focused on these questions via analyzing
the inner workings of entity coreference resolvers
(e.g., Stoyanov et al. (2009), Recasens and Hovy
(2010), Chen and Ng (2012a)). On the other hand,
no attempts have been made to address these ques-
tions in the context of event coreference.

1A notable exception is Zinmeister et al.'s (2012) work,
which reports results for both German and English.

822



2 ACE Event Coreference

As mentioned in the introduction, event corefer-
ence is the task of determining which event men-
tions in a text refer to the same real-world event.
The ACE 2005 event coreference task, which is
the version of the event coreference task we focus
on, requires that an event coreference resolver per-
forms coreference only on event mentions belong-
ing to one of the ACE event types.
More specifically, an ACE event mention has

a type and a subtype. In ACE 2005, eight types
are defined, which are further subcategorized into
33 subtypes. Not surprisingly, two event mentions
that have different subtypes cannot be coreferent.
To better understand the ACE 2005 coreference

task, consider the sentence in Figure 1, which is
taken from the ACE 2005 corpus. This example
contains three event mentions: 砍 (stabbed), 伤
(injured) and 行凶 (criminal). 砍 and 行凶 have
type Conflict and subtype Attack, whereas伤
has type Life and subtype Injure. In this exam-
ple,砍 and行凶 are coreferent because they refer
to the same real-world event.

(张家荣)(昨天傍晚)在 (路上)骑自行车运动时遭到了 (两
名歹徒)持 (刀)[砍][伤]。(歹徒)的 [行凶]动机可能和 (张
家荣)的问证有关联。
(Zhang Jiarong)was cycling on (the road) (yesterday evening)
and was [injured] when (two men) [stabbed] (him) with (a
knife). (The thugs)' [criminal] motivation may have some-
thing to do with (Zhang Jiarong)'s testimony in a criminal
case.

Figure 1: An example. Event mentions are bracketed
and entity mentions are parenthesized.

3 Six Upstream Components

Our event coreference resolver adopts a fairly stan-
dard ACE event coreference system architecture,
relying on six components. Aswewill see, the first
four components have a direct influence on event
coreference, meaning that their output will be used
to create features for use by the event coreference
model. On the other hand, the last two components
only have an indirect influence on event corefer-
ence through other components.
Component 1: Event mention boundary iden-
tification and subtyping. This component (1)
provides the event mentions for event coreference
resolution, and (2) labels each event mention with
its event subtype. Since two event mentions with
different subtypes cannot be coreferent, subtypes

can be used to create useful features for event
coreference. To implement this component, we
use our Chinese event extraction system (Chen and
Ng, 2012c), which jointly learns these tasks.
Component 2: Event mention attribute value
computation. This component takes as input a
set of event mentions (provided by Component 1)
and computes for each mention its attributes, in-
cluding its Polarity, Modality, Genericity
and Tense. Since two event mentions that differ
with respect to any of these attributes cannot be
coreferent, they can be used to create useful fea-
tures for event coreference. Following Chen et al.
(2009), we train a classifier to compute the value of
each attribute of each event mention (see Chen et
al. for details on the implementation of these clas-
sifiers).
Component 3: Event argument and role classi-
fication. This component takes as input a set of
event mentions (provided by Component 1) and a
set of candidate arguments (provided by Compo-
nent 5). For each event mention em, it (1) iden-
tifies those candidate arguments that are the true
arguments of em (e.g., the participants, time, and
place of em), and then (2) assigns a role (e.g., Vic-
tim, Place, Time-Within) to each of its true
arguments. Since two events involving different
times, places, or participants cannot be coreferent,
the arguments and their roles can be used to create
useful features for event coreference. To imple-
ment this component, we use our Chinese event
extraction system (Chen and Ng, 2012c), which
jointly learns these two tasks.
Component 4: Entity coreference resolution.
This component takes as input a set of entity men-
tions (provided by Component 5) and creates a
coreference partition in which each cluster con-
tains all and only those entity mentions that re-
fer to the same real-world entity. Since two event
mentions having coreferent arguments are likely
to be coreferent, the output of this component can
be used to create useful features for event corefer-
ence. To create a coreference partition from a set
of entity mentions, we employ our Chinese entity
coreference resolver (Chen and Ng, 2012b).
Component 5: Entity mention boundary identi-
fication. This component provides the candidate
arguments and the entity mentions needed by the
aforementioned components, so it only has an in-
direct influence on event coreference. Since can-
didate arguments can be entity mentions, time ex-

823



pressions, and value expressions2, we train one
CRF (using CRF++3) to extract each of these three
types of candidate arguments.
Component 6: Entity typing and subtyping.
This component takes a set of entitymentions (pro-
vided by Component 5) and determines the se-
mantic type and subtype of each entity mention.
Knowing the semantic type and subtype of an ar-
gument is helpful for classifying the role of event
arguments. For example, we can assign the role
Victim only to those arguments with entity type
Person. To determine semantic type and sub-
type, we train two SVM multiclass classifiers us-
ing SVMmulticlass (Tsochantaridis et al., 2004).

4 Chinese Event Coreference Resolver

Underlying our learning-based Chinese event
coreference resolver is a mention-pair model
(Soon et al., 2001) trained using the SVMlight
package (Joachims, 1999). Training instances are
created as follows. For each anaphoric event men-
tion em, we create one positive instance between
em and its closest antecedent. To create negative
instances, we pair em with each of its preceding
event mentions that is not coreferent with it.
Each instance is represented using 32 features,

which are modeled after a state-of-the-art English
event coreference resolver (Chen and Ji, 2009;
Chen et al., 2009) (see the Appendix for a detailed
description of these features). After training, the
resulting mention-pair model is used in combina-
tion with a closest-first single-link clustering al-
gorithm to impose a coreference partition on the
event mentions in a test text (Soon et al., 2001).

5 Empirical Analysis

Next, we address our first question: to what extent
is the noise inherent in the output of each of the
upstream components limiting a resolver's perfor-
mance? To answer this question, we start with a
resolver where all of its upstream components are
assumed to be oracle components, and then replace
each of them with its real (i.e., imperfect) version
one after the other, as described below.

5.1 Experimental Setup
For evaluation, we report five-fold cross-
validation results over the 633 Chinese documents

2See the ACE 2005 task definition
(http://www.itl.nist.gov/iad/mig/tests/ace/2005/doc/ace05-
evalplan.v2a.pdf).

3http://crfpp.googlecode.com/svn/trunk/doc/index.html

in the ACE 2005 training corpus. Results, ex-
pressed in terms of recall (R), precision (P), and
F-measure (F), are obtained by applying three
commonly-used coreference scoring programs,
MUC (Vilain et al., 1995), B3 (Bagga and Bald-
win, 1998), and CEAFe (a.k.a. ϕ4-CEAF) (Luo,
2005), to the coreference partitions produced by
our resolver after singleton event mentions are
removed. Stanford's Chinese NLP and Speech
Processing tool4 is used for word segmentation,
syntactic parsing, and dependency parsing.

5.2 Results and Analysis

As mentioned above, we start with an event coref-
erence resolver that assumes that all the six up-
stream components are error-free (see row 1 of Ta-
ble 1 for its performance), and then replace each
oracle component with its system (i.e., machine-
learned) counterpart one after the other (rows 2-
-7 of Table 1). Therefore, the results in the last
row of Table 1 correspond to the performance of
an end-to-end event coreference resolver that relies
solely on system components. Below, we discuss
the impact of each component on coreference per-
formance. Note that these components can be con-
sidered in a different order than what we show in
this section. Here, we show one ordering in which
the parent(s) of a component are considered after
the component itself.
Component 2 (Event mention attribute value

computation). First, we replace the oracle event
mention attribute predictors with their system
counterparts. Since there are four event men-
tion attributes, namely, Polarity, Modality,
Genericity and Tense, we trained four classi-
fiers to predict the attribute values of an event men-
tion. Our results suggest that each of these four
classifiers is only marginally better than a sim-
ple majority baseline.5 We then used the values
predicted by these classifiers to compute features
for the test instances for the event coreference re-
solver. Note that the features for the training in-
stances for the resolver are computed based on
gold rather than system event attribute values.

4http://nlp.stanford.edu/projects/chinese-nlp.shtml
5Chen et al. (2009) trained classifiers to predict the at-

tribute values of English event mentions. While their Polar-
ity, Modality, and Genericity classifiers perform only
slightly better than a majority baseline, their Tense classifier
has reasonably good performance. This is perhaps not sur-
prising: tense classification for English verbs is easier than
for Chinese verbs since Chinese verb forms do not change
according to tense.

824



MUC B3 CEAFe Avg
R P F R P F R P F F

1. All oracle 80.4 70.0 74.8 88.4 79.7 83.8 57.3 66.8 61.7 73.4
2. + System event mention attribute values
2a. system event mention attributes on test only 59.9 60.8 60.3 76.8 78.5 77.6 53.3 52.5 52.9 63.6
2b. no event mention attribute features 72.8 63.4 67.8 84.2 76.6 80.2 52.1 60.0 55.8 67.9
2c. system event mention attributes on train & test 72.5 64.5 68.3 83.8 77.4 80.5 53.1 59.9 56.3 68.3
3. + System event arguments and roles 71.2 61.2 65.8 83.9 74.9 79.1 49.9 58.0 53.6 66.2
4. + System entity coreference chains 61.6 58.5 60.0 79.0 75.7 77.3 49.1 51.5 50.3 62.5
5. + System entity types & subtypes 62.2 57.9 60.0 79.4 75.2 77.2 49.0 52.3 50.6 62.6
6. + System entity mention boundaries 63.3 57.4 60.2 80.2 74.4 77.2 48.2 52.8 50.4 62.6
7. + System entity mention boundaries & subtypes 37.4 36.7 37.1 72.8 71.1 71.9 40.6 41.1 40.8 49.9

Table 1: Results when oracle components are replaced with system components one after the other.

Given the poor performance of these attribute
predictors, we hypothesize that coreference per-
formance will drop considerably when the oracle
attribute predictors are replaced with their system
counterparts. Results of this experiment, shown
in row 2a, are consistent with this hypothesis: in
comparison to row 1, the Avg F-score (unweighted
average of the MUC, B3, and CEAFe F-scores)6
drops significantly by nearly 10%.7 A natural
question is: do these results represent an unnec-
essary amplification of the impact of event men-
tion attributes on event coreference performance?
To answer this question, we consider two alterna-
tive ways of employing the system event mention
attribute values for event coreference resolution.
One way is to simply discard all features created
from these attributes when training and testing the
event coreference resolver. Results of this exper-
iment are shown in row 2b. Somewhat interest-
ingly, we can see by comparing rows 2a and 2b that
discarding these features does yield a less consid-
erable drop in performance than employing them.
Another way is to employ system attribute values
to generate features for both the training and test
instances for the event coreference resolver. Re-
sults of this experiment are shown in row 2c. In
comparison to row 2b, we see that there is a slight,
but insignificant, improvement in coreference per-
formance. Consequently, we assume in the rest
of our experiments that we employ system event
mention attribute values on both the training set
and the test set (i.e., the configuration in row 2c),
since it seems to more accurately reflect the impact
of event mention attributes on event coreference
performance.

Conclusion 1: Improving the four event attribute classi-

6For ease of exposition, we follow the CoNLL 2011 and
2012 shared tasks and use Avg F when discussing results.

7All statistical significance test results reported in this pa-
per are conducted using the paired t-test (p < 0.05).

fiers could significantly improve event coreference.
Component 3 (Event argument and role clas-

sification). Next, we replace the oracle event ar-
gument and role classification component with its
system counterpart. Results on the test set indi-
cate that when gold event mentions and subtype,
gold entity type and subtype, and gold entity men-
tion boundaries are used, the F-scores of system
argument classification and role classification are
76.9% and 68.2% respectively. Replacing the or-
acle component with this system counterpart, we
see that the Avg coreference performance drops
slightly, though still significantly, by 2.1%.

Conclusion 2: Event argument classification and role
classification have a small, but significant, impact on event
coreference performance.
Component 4 (Entity coreference). Next, we

replace oracle entity coreference with system en-
tity coreference. As noted before, event coref-
erence directly depends on entity coreference.
Our system entity coreference resolver achieves
a MUC F-score of 78.0% when gold entity men-
tions are used. Comparing rows 3 and 4, we see
that replacing oracle entity coreference with sys-
tem entity coreference incurs nearly a 4% drop
in coreference performance according to Avg F-
score. These results suggest that employing a bet-
ter entity coreference resolver can improve event
coreference. Joint learning of event and entity
coreference may help to improve both tasks.

Conclusion 3: Improving entity coreference could signif-
icantly improve event coreference.
Component 6 (Entity typing and subtyping).

Next, we replace oracle entity typing and subtyp-
ing with its system counterpart. Recall that this
component has only an indirect impact on event
coreference but a direct impact on event argument
and role classification, since the entity type and
subtype of a mention are used to create features for
training the event argument and role classifier. Our

825



system entity type and subtype classifiers achieve
F-scores of 90.1% and 81.6%, respectively, when
gold entity mentions are used. Using system rather
than gold entity types and subtypes, the F-scores
of the event argument classifier and the event role
classifier drop by 2.8% and 4.3% respectively, but
comparing rows 4 and 5, coreference performance
does not drop.

Conclusion 4: Improving entity type and subtype classi-
fication is unlikely to improve event coreference.

Component 5 (Entity mention boundary de-
tection). Next, we replace gold entity mention
boundary detection with its system counterpart.
The performance of our system mention bound-
ary detection component is reasonably good: it
achieves an F-score of 84.7%. Comparing rows 5
and 6, we see that replacing gold mention bound-
ary detection with its system counterpart does not
alter event coreference performance.

Conclusion 5: Improving entity mention boundary detec-
tion may not improve event coreference.

Component 1 (Eventmention boundary iden-
tification and subtyping). Finally, we replace
the oracle event mention boundary identification
and subtyping component with its system counter-
part. Our learned event mention boundary identi-
fier achieves an F-score of 65.1%, while our event
subtype classifier achieves an F-score of 61.30%.
Comparing rows 6 and 7, we see that replacing or-
acle with system event mention boundary identi-
fication and subtyping causes Avg F-score to drop
by 12.7%, even though the event extraction system
we employ for event mention boundary identifica-
tion and subtype classification is a state-of-the-art
system. Furthermore, MUC recall decreases more
abruptly than MUC precision, which suggests that
the low recall of event mention boundary identifi-
cation severely harms system performance.

Conclusion 6: Event mention boundary identification and
subtyping is the upstream component that has the largest im-
pact on event coreference. There is a lot of room for improv-
ing this component, especially its recall.

We conclude this section with two noteworthy
points. First, the cumulative study we conducted
in this section is just one possible way to examine
the extent to which the noise inherent in the out-
put of each of the upstream components limits a
resolver's performance. Another way is to conduct
an ablation study: we start with a resolver where
all of its upstream components are assumed to be
oracle components, and then replace exactly one

oracle upstream component with its real (i.e., im-
perfect) version in each ablation experiment. Since
the conclusions that can be drawn from the ablation
study and the cumulative study are similar, we will
omit the description of the ablation experiments
and their results for the sake of brevity.
Second, although we discuss the results in this

section in terms of Avg F, it turns out that in these
experiments Avg F exhibits the same performance
trends as those ofMUC, B3, and CEAFe. In partic-
ular, the significance test results that we obtained
using Avg F remain unchanged when Avg F is re-
placed with any of the three scoring metrics.

6 Error Analysis

Next, we address our second question: what are the
major types of errors that are attributable to (and
therefore can be fixed by improving) the resolution
algorithm? To answer this question, we perform a
qualitative error analysis on the output produced
by the resolver where all six upstream components
are gold. This ensures that all the errors are at-
tributable to the resolution algorithm.

6.1 Three Major Types of Precision Errors

Lack of event timestamping. Only those
events that occur at the same time can be coref-
erent. We use the Tense event attribute as an
feature to enforce Tense consistency, essentially
employing Tense as a very rough approximation
of the timestamp of an event. Perhaps not sur-
prisingly, many events having the same tense are
not coreferent. For example, consider the fol-
lowing pair of sentences, where the triggers (i.e.,
the words/phrases corresponding to the event men-
tions) are enclosed in brackets.
去年三月间杨光南在上海首度 [被捕]
In last March, Yang Guangnan was [arrested] in Shanghai

for the first time
杨光南在上海再度 [被捕]
Yang Guangnan was [arrested] again in Shanghai

It is fairly easy to see that the first arrest occurred
before the second one and therefore the two event
mentions should not be coreferent. However,
our resolver incorrectly posits them as coreferent,
since they have the same Person and Place ar-
guments (i.e., 杨光南 (Yang Guangnan) and 上
海 (Shanghai)) and the same tense (i.e., past). If
we could assign a timestamp to each event, our re-
solver might fix this type of precision error.

826



Incompatible triggers. As coreference be-
tween arguments is a strong indicator that the
corresponding event mentions are coreferent, our
resolver tends to link two events mentions with
coreferent arguments together even when the trig-
gers are not semantically compatible. The follow-
ing pair of sentences illustrates this type of error.
萨姆·努乔马２８日乘专机抵达平壤开始对朝鲜进行

正式友好 [访问]
On the 28th, Sam Nujoma arrived in Pyongyang by plane

for an official goodwill [visit] to the DPRK
纳米比亚总统萨姆·努乔马２８日乘专机 [抵达]平壤
Namibian President Sam Nujoma [arrived] in Pyongyang

by plane on the 28th

As we can see, the triggers are 访问 (visit) and
抵达 (arrived). Since both their Artifact argu-
ments (i.e., 萨姆·努乔马 (Sam Nujoma)) and
their Vehicle arguments (i.e., 专机 (plane)) are
coreferent, our resolver wrongly posits the two
event mentions as coreferent, although the corre-
sponding triggers,访问 and抵达, are semantically
incompatible. If we had access to a semantic dic-
tionary from which we can derive the fine-grained
semantic type of these triggers, our resolver might
fix this type of error.
Incompatible important arguments. Our re-

solver has the tendency to posit two largely com-
patible event mentions as coreferent, i.e., they
have only a small number of incompatible argu-
ments but many features that suggest that they are
coreferent (e.g, same trigger word, some corefer-
ent arguments). Consider the example below.
代表团 [访问]了瑞典
The delegation [visited] Sweden
[访问]丹麦期间，中国基督教代表团举行记者招待会
During their [visit] in Denmark, the Chinese Christian del-

egation held a press conference

Note that the two event mentions have identical
triggers 访问 (visited) and Artifact arguments
代表团 (The delegation). Given these positive
evidences, our resolver posits the event mentions
as coreferent despite the fact that their Destina-
tion arguments are incompatible: one is 瑞典
(Sweden) and the other is丹麦 (Denmark). To fix
this kind of error, we may employ human knowl-
edge to mark each argument role of each event
subtype as important or unimportant, and enforce
the constraint that two event mentions cannot be
coreferent if their arguments in an important ar-
gument role are incompatible. In our example,
we would mark both Artifact and Destina-

tion as important argument roles for the Meet-
ing event subtype (i.e., the subtype for visited),
meaning that we will disallow the two event men-
tions in the example to be coreferent unless both
the Artifact and Destination arguments are
compatible.

6.2 Two Major Types of Recall Errors
Coreferent mentions with synonymous trig-

gers. Our resolver fails to link some event men-
tions that have synonymous but lexically different
trigger words. Consider the following example.
犹太人针对阿拉伯人的 [暴力]
Jewish [violence] against the Arabs
[冲突]双方
Two parties of [conflict]

While the event mentions corresponding to the two
synonymous triggers, 暴力 (violence) and 冲突
(conflict), are coreferent, our resolver fails to iden-
tify them as coreferent because the triggers are
lexically different. We could fix this type of er-
ror if we had access to a semantic dictionary that
can suggest that violence and conflict have similar
meaning.
Coreferent mentions with compatible argu-

ments. Some arguments are not coreferent but
compatible. Consider the following sentences.
南斯拉夫国家元首第一次对波黑进行这样的 [访问]
Yugoslavia's head of state [visited] Bosnia-Herzegovina

for the first time
科什图尼察 [访问]波黑首都萨拉热窝
Kostunica [visited] Sarajevo, the capital of Bosnia-

Herzegovina

Here, the triggers are访问 (visited). Our resolver
does not posit these two event mentions as corefer-
ent since their arguments are not coreferent. How-
ever, if we had the world knowledge to recognize
波黑 (Bosnia and Herzegovina) and 萨拉热窝
(Sarajevo) are compatible arguments, then our re-
solver might be able to discover the coreference
link between the two event mentions.

7 Conclusion

We conducted the first empirical analysis of an
ACE-style Chinese event coreference system, fo-
cusing on the questions of (1) the extent to which
event coreference performance is affected by er-
rors made by upstream components in the infor-
mation extraction pipeline, and (2) the types of er-
rors made by the resolution algorithm. We hope
our analysis will help direct future research.

827



Acknowledgments

We thank the four reviewers for their insightful
comments. This work was supported in part by
NSF Grants IIS-1147644 and IIS-1219142.

Appendix: Event Coreference Features

The 32 features used by our event coreference re-
solver can be divided into five groups. Group i
(1 ≤ i ≤ 4) contains the features computed based
on Component i's output, and Group 5 contains the
remaining features. For convenience, we use em2
to refer to an event mention to be resolved and em1
to refer to one of its candidate antecedents.
Group 1 (4): Whether em1 and em2 agree w.r.t.

event type; whether they agree w.r.t. event sub-
type; the concatenation of their event types; the
concatenation of their event subtypes.
Group 2 (8): The four event mention attributes

of em2; whether em1 and em2 are compatible
w.r.t. each of the event mention attributes.
Group 3 (4): The roles and number of the argu-

ments that only appear in em1; the roles and num-
ber of the arguments that only appear in em2.
Group 4 (6): The roles and number of argu-

ments between em1 and em2 that have the same
role and are also in the same entity coreference
chain; the roles and number of arguments between
em1 and em2 that have same role but are in dif-
ferent coreference chains; the roles and number of
arguments between em1 and em2 that have differ-
ent roles but are in the same coreference chain.
Group 5 (10): The sentence distance between

em1 and em2; the number of event mentions inter-
vening them; the number of words between them;
whether they have the same trigger word; whether
they are in a coordinating structure; whether they
have same basic verb; whether they agree in num-
ber if they are nouns; whether they have incompat-
ible modifiers if they are nouns; the concatenation
of the part-of-speech tags of their heads; the con-
catenation of their trigger words.

References
Amit Bagga and Breck Baldwin. 1998. Algorithms for

scoring coreference chains. In Proc. of the LREC
Workshop on Linguistic Coreference, page 563--566.

Cosmin Bejan and Sanda Harabagiu. 2010. Unsuper-
vised event coreference resolution with rich linguis-
tic features. In Proc. of the ACL, pages 1412--1422.

Zheng Chen and Heng Ji. 2009. Graph-based event
coreference resolution. In Proc. of TextGraphs-4,
pages 54--57.

Chen Chen and Vincent Ng. 2012a. Chinese noun
phrase coreference resolution: Insights into the state
of the art. In Proc. of COLING 2012: Posters Vol-
ume, pages 185--194.

Chen Chen and Vincent Ng. 2012b. Combining the
best of two worlds: A hybrid approach to multilin-
gual coreference resolution. In Proc. of EMNLP-
CoNLL: Shared Task, pages 56--63.

Chen Chen and Vincent Ng. 2012c. Joint modeling
for Chinese event extraction with rich linguistic fea-
tures. In Proc. of COLING, pages 529--544.

Zheng Chen, Heng Ji, and Robert Haralick. 2009.
A pairwise event coreference model, feature impact
and evaluation for event coreference resolution. In
Proc. of the RANLP Workshop on Events in Emerg-
ing Text Types, pages 17--22.

Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim
Tan. 2011. A unified event coreference resolution
by integrating multiple resolvers. In Proc. of IJC-
NLP, pages 102--110.

Kevin Humphreys, Robert Gaizauskas, and Saliha Az-
zam. 1997. Event coreference for information ex-
traction. In Proc. of the ACL Workshop on Opera-
tional Factors in Practical, Robust Anaphora Reso-
lution for Unrestricted Texts, pages 75--81.

Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods -
Support Vector Learning, pages 44--56. MIT Press.

Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proc. of EMNLP-CoNLL, pages 489--500.

Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proc. of HLT/EMNLP, pages
25--32.

Marta Recasens and Eduard Hovy. 2010. Corefer-
ence resolution across corpora: Languages, coding
schemes, and preprocessing information. In Proc. of
the ACL, pages 1423--1432.

Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning
approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521--544.

Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-of-
the-art. In Proc. of ACL-IJCNLP, pages 656--664.

Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proc. of ICML, pages 104--112.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proc. of
MUC-6, pages 45--52.

Heike Zinmeister, Stefanie Dipper, and Melanie Seiss.
2012. Abstract pronominal anaphors and label
nouns in German and English: Selected case studies
and quantitative investigations. Translation: Cor-
pora, Computation, Cognition, 2(1):47--80.

828


