



















































Exploiting Sentential Context for Neural Machine Translation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6197–6203
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

6197

Exploiting Sentential Context for Neural Machine Translation

Xing Wang
Tencent AI Lab

brightxwang@tencent.com

Zhaopeng Tu
Tencent AI Lab

zptu@tencent.com

Longyue Wang
Tencent AI Lab

vinnylywang@tencent.com

Shuming Shi
Tencent AI Lab

shumingshi@tencent.com

Abstract

In this work, we present novel approaches to
exploit sentential context for neural machine
translation (NMT). Specifically, we first show
that a shallow sentential context extracted
from the top encoder layer only, can improve
translation performance via contextualizing
the encoding representations of individual
words. Next, we introduce a deep sentential
context, which aggregates the sentential
context representations from all the internal
layers of the encoder to form a more compre-
hensive context representation. Experimental
results on the WMT14 English⇒German
and English⇒French benchmarks show
that our model consistently improves per-
formance over the strong TRANSFORMER
model (Vaswani et al., 2017), demonstrating
the necessity and effectiveness of exploiting
sentential context for NMT.

1 Introduction

Sentential context, which involves deep syntactic
and semantic structure of the source and target lan-
guages (Nida, 1969), is crucial for machine trans-
lation. In statistical machine translation (SMT),
the sentential context has proven beneficial for
predicting local translations (Meng et al., 2015;
Zhang et al., 2015). The exploitation of sentential
context in neural machine translation (NMT, Bah-
danau et al., 2015), however, is not well studied.
Recently, Lin et al. (2018) showed that the trans-
lation at each time step should be conditioned on
the whole target-side context. They introduced a
deconvolution-based decoder to provide the global
information from the target-side context for guid-
ance of decoding.

In this work, we propose simple yet effec-
tive approaches to exploiting source-side global
sentence-level context for NMT models. We use
encoder representations to represent the source-

side context, which are summarized into a senten-
tial context vector. The source-side context vector
is fed to the decoder, so that translation at each
step is conditioned on the whole source-side con-
text. Specifically, we propose two types of senten-
tial context: 1) the shallow one that only exploits
the top encoder layer, and 2) the deep one that ag-
gregates the sentence representations of all the en-
coder layers. The deep sentential context can be
viewed as a more comprehensive global sentence
representation, since different types of syntax and
semantic information are encoded in different en-
coder layers (Shi et al., 2016; Peters et al., 2018;
Raganato and Tiedemann, 2018).

We validate our approaches on top of the state-
of-the-art TRANSFORMER model (Vaswani et al.,
2017). Experimental results on the benchmarks
WMT14 English⇒German and English⇒French
translation tasks show that exploiting sentential
context consistently improves translation perfor-
mance across language pairs. Among the model
variations, the deep strategies consistently outper-
form their shallow counterparts, which confirms
our claim. Linguistic analyses (Conneau et al.,
2018) on the learned representations reveal that the
proposed approach indeed provides richer linguis-
tic information.

The contributions of this paper are:

• Our study demonstrates the necessity and ef-
fectiveness of exploiting source-side senten-
tial context for NMT, which benefits from
fusing useful contextual information across
encoder layers.

• We propose several strategies to better cap-
ture useful sentential context for neural ma-
chine translation. Experimental results em-
pirically show that the proposed approaches
achieve improvement over the strong base-
line model TRANSFORMER.



6198

layer 2

layer 1

layer 3

layer 2

layer 1

layer 3

layer 2

layer 1

layer 3

(a) Vanilla

layer 2

layer 1

layer 3

layer 2

layer 1

layer 3

layer 2

layer 1

layer 3

(b) Shallow Sentential Context

layer 2

layer 1

layer 3

layer 2

layer 1

layer 3

layer 2

layer 1

layer 3

(c) Deep Sentential Context

Figure 1: Illustration of the proposed approache. As on a 3-layer encoder: (a) vanilla model without sentential
context, (b) shallow sentential context representation (i.e. blue square) by exploiting the top encoder layer only;
and (c) deep sentential context representation (i.e. brown square) by exploiting all encoder layers. The circles
denote hidden states of individual tokens in the input sentence, and the squares denote the sentential context
representations. The red up arrows denote that the representations are fed to the subsequent decoder. This figure is
best viewed in color.

2 Approach

Like a human translator, the encoding process is
analogous to reading a sentence in the source lan-
guage and summarizing its meaning (i.e. senten-
tial context) for generating the equivalents in the
target language. When humans translate a source
sentence, they generally scan the sentence to cre-
ate a whole understanding, with which in mind
they incrementally generate the target sentence by
selecting parts of the source sentence to trans-
late at each decoding step. In current NMT mod-
els, the attention model plays the role of selecting
parts of the source sentence, but lacking a mech-
anism to guarantee that the decoder is aware of
the whole meaning of the sentence. In response to
this problem, we propose to augment NMT mod-
els with sentential context, which represents the
whole meaning of the source sentence.

2.1 Framework

Figure 1 illustrates the framework of the proposed
approach. Let g = g(X) be the sentential context
vector, and g(·) denotes the function to summa-
rize the source sentence X, which we will discuss
in the next sections. There are many possible ways
to integrate the sentential context into the decoder.
The target of this paper is not to explore this whole
space but simply to show that one fairly straight-
forward implementation works well and that sen-
tential context helps. In this work, we incorporate
the sentential context into decoder as

dli = f(LAYERdec(D̂
l−1), cli), (1)

D̂l−1 = FFNl(Dl−1,g), (2)

where dli is the l-th layer decoder state at decod-
ing step i, cli is a dynamic vector that selects cer-
tain parts of the encoder output, FFNl(·) is a dis-
tinct feed-forward network associated with the l-th
layer of the decoder, which reads the l−1-th layer
output Dl−1 and the sentential context g. In this
way, at each decoding step i, the decoder is aware
of the sentential context g embedded in D̂l−1.

In the following sections, we discuss the choice
of g(·), namely shallow sentential context (Fig-
ure 1b) and deep sentential context (Figure 1c),
which differ at the encoder layers to be exploited.
It should be pointed out that the new parameters
introduced in the proposed approach are jointly
updated with NMT model parameters in an end-
to-end manner.

2.2 Shallow Sentential Context

Shallow sentential context is a function of the top
encoder layer output HL:

g = g(HL) = GLOBAL(HL), (3)

where GLOBAL(·) is the composition function.

Choices of GLOBAL(·) Two intuitive choices
are mean pooling (Iyyer et al., 2015) and max
pooling (Kalchbrenner et al., 2014):

GLOBALMEAN = MEAN(HL), (4)

GLOBALMAX = MAX(HL). (5)

Recently, Lin et al. (2017) proposed a self-
attention mechanism to form sentence representa-
tion, which is appealing for its flexibility on ex-
tracting implicit global features. Inspired by this,



6199

g3

g2

g1

r2

r1

r3

r0

g3

g2

g1

gi

βi,1

βi,2

βi,3

di-1

g3

g2

g1

gi
(a) RNN

g3

g2

g1

r2

r1

r3

r0

g3

g2

g1

gi

βi,1

βi,2

βi,3

di-1

g3

g2

g1

gi
(b) TAM

Figure 2: Illustration of the deep functions. “TAM”
model dynamically aggregates sentence representa-
tions at each decoding step with state di−1.

we propose an attentive mechanism to learn sen-
tence representation:

GLOBALATT = ATT(g0,HL), (6)

g0 = MAX(H0), (7)

where H0 is the word embedding layer, and its
max pooling vector g0 serves as the query to ex-
tract features to form the final sentential context
representation.

2.3 Deep Sentential Context
Deep sentential context is a function of all encoder
layers outputs {H1, . . . ,HL}:

g = g(H1, . . . ,HL) = DEEP(g1, . . . ,gL), (8)

where gl is the sentence representation of the l-th
layer Hl, which is calculated by Equation 3. The
motivation for this mechanism is that recent stud-
ies reveal that different encoder layers capture lin-
guistic properties of the input sentence at different
levels (Peters et al., 2018), and aggregating layers
to better fuse semantic information has proven to
be of profound value (Shen et al., 2018; Dou et al.,
2018; Wang et al., 2018; Dou et al., 2019). In this
work, we propose to fuse the global information
across layers.

Choices of DEEP(·) In this work, we investigate
two representative functions to aggregate informa-
tion across layers, which differ at whether the de-
coding information is taken into account.
RNN Intuitively, we can treat G = {g1, . . . ,gL}
as a sequence of representations, and recurring all
the representations with an RNN:

DEEPRNN = RNN(G). (9)

We use the last RNN state as the sentence rep-
resentation: g = rL. As seen, the RNN-based

aggregation repeatedly revises the sentence rep-
resentations of the sequence with each recurrent
step. As a side effect coming together with the
proposed approach, the added recurrent induc-
tive bias of RNNs has proven beneficial for many
sequence-to-sequence learning tasks such as ma-
chine translation (Dehghani et al., 2018).
TAM Recently, Bapna et al. (2018) proposed a
novel transparent attention model (TAM) to train
very deep NMT models. In this work, we apply
TAM to aggregate sentence representations:

DEEPTAM =
L∑
l=1

βi,lg
l, (10)

βi = ATTg(dli−1,G), (11)

where ATTg(·) is an attention model with its own
parameters, that specifics which context repre-
sentations is relevant for each decoding output.
Again, dli−1 is the decoder state in the l-th layer.

Comparing with its RNN counterpart, the TAM
mechanism has three appealing strengths. First,
TAM dynamically generates the weights βi based
on the decoding information at every decoding
step dli−1, while RNN is unaware of the decoder
states and the associated parameters are fixed after
training. Second, TAM allows the model to adjust
the gradient flow to different layers in the encoder
depending on its training phase.

3 Experiment

We conducted experiments on WMT14 En⇒De
and En⇒Fr benchmarks, which contain 4.5M
and 35.5M sentence pairs respectively. We re-
ported experimental results with case-sensitive 4-
gram BLEU score. We used byte-pair encoding
(BPE) (Sennrich et al., 2016) with 32K merge op-
erations to alleviate the out-of-vocabulary prob-
lem. We implemented the proposed approaches
on top of TRANSFORMER model (Vaswani et al.,
2017). We followed Vaswani et al. (2017) to set
the model configurations, and reproduced their re-
ported results. We tested both Base and Big mod-
els, which differ at the layer size (512 vs. 1024)
and the number of attention heads (8 vs. 16).

3.1 Ablation Study
We first investigated the effect of components in
the proposed approaches, as listed in Table 1.

Shallow Sentential Context (Rows 3-5) All the
shallow strategies achieve improvement over the



6200

# Model GLOBAL(·) DEEP(·) # Para. Train Decode BLEU
1 BASE n/a n/a 88.0M 1.39 3.85 27.31
2 MEDIUM n/a n/a +25.2M 1.08 3.09 27.81
3

SHALLOW
Mean Pooling

n/a
+18.9M 1.35 3.45 27.58

4 Max Pooling +18.9M 1.34 3.43 27.81↑

5 Attention +19.9M 1.22 3.23 28.04⇑

6
DEEP Attention

RNN +26.8M 1.03 3.14 28.38⇑

7 TAM +26.4M 1.07 3.03 28.33⇑

Table 1: Impact of components on WMT14 En⇒De translation task. BLEU scores in the table are case sensitive.
“Train” denotes the training speed (steps/second), and “Decode” denotes the decoding speed (sentences/second)
on a Tesla P40. “TAM” denotes the transparent attention model to implement the function DEEP(·). “↑ / ⇑”:
significant over TRANSFORMER counterpart (p < 0.05/0.01), tested by bootstrap resampling (Koehn, 2004).

baseline Base model, validating the importance of
sentential context in NMT. Among them, atten-
tive mechanism (Row 5) obtains the best perfor-
mance in terms of BLEU score, while maintains
the training and decoding speeds. Therefore, we
used the attentive mechanism to implement the
function GLOBAL(·) as the default setting in the
following experiments.

Deep Sentential Context (Rows 6-7) As seen,
both RNN and TAM consistently outperform their
shallow counterparts, proving the effectiveness of
deep sentential context. Introducing deep context
significantly improves translation performance by
over 1.0 BLEU point, while only marginally de-
creases the training and decoding speeds.

Compared to Strong Base Model (Row 2) As
our model has more parameters than the Base
model, we build a new baseline model (MEDIUM
in Table 1) which has a similar model size as
the proposed deep sentential context model. We
change the filter size from 1024 to 3072 in the de-
coder’s feed-forward network (Eq.2). As seen, the
proposed deep sentential context models also out-
perform the MEDIUM model over 0.5 BLEU point.

3.2 Main Result
Experimental results on both WMT14 En⇒De
and En⇒Fr translation tasks are listed in Table
2. As seen, exploiting deep sentential context rep-
resentation consistently improves translation per-
formance across language pairs and model archi-
tectures, demonstrating the necessity and effec-
tiveness of modeling sentential context for NMT.
Among them, TRANSFORMER-BASE with deep
sentential context achieves comparable perfor-
mance with the vanilla TRANSFORMER-BIG, with
only less than half of the parameters (114.4M

Model En⇒De En⇒Fr
TRANSFORMER-BASE 27.31 39.32

+ DEEP (RNN) 28.38⇑ 40.15⇑
+ DEEP (TAM) 28.33⇑ 40.27⇑

TRANSFORMER-BIG 28.58 41.41
+ DEEP (RNN) 29.04↑ 41.87
+ DEEP (TAM) 29.19⇑ 42.04⇑

Table 2: Case-sensitive BLEU scores on WMT14
En⇒De and En⇒Fr test sets. “↑ / ⇑”: significant over
TRANSFORMER counterpart (p < 0.05/0.01), tested
by bootstrap resampling.

vs. 264.1M, not shown in the table). Fur-
thermore, DEEP (TAM) consistently outperforms
DEEP (RNN) in the TRANSFORMER-BIG config-
uration. One possible reason is that the big models
benefit more from the improved gradient flow with
the transparent attention (Bapna et al., 2018).

3.3 Linguistic Analysis

To gain linguistic insights into the global and
deep sentence representation, we conducted prob-
ing tasks1 (Conneau et al., 2018) to evaluate lin-
guistics knowledge embedded in the encoder out-
put and the sentence representation in the varia-
tions of the Base model that are trained on En⇒De
translation task. The probing tasks are classifica-
tion problems that focus on simple linguistic prop-
erties of sentences. The 10 probing tasks are cate-
gories into three groups: (1) Surface information.
(2) Syntactic information. (3) Semantic informa-
tion. For each task, we trained the classifier on the
train set, and validated the classifier on the vali-
dation set. We followed Hao et al. (2019) and Li

1https://github.com/facebookresearch/
SentEval/tree/master/data/probing

https://github.com/facebookresearch/SentEval/tree/master/data/probing
https://github.com/facebookresearch/SentEval/tree/master/data/probing


6201

Model
Surface Syntactic Semantic

SeLen WC Avg TrDep ToCo BShif Avg Tense SubN ObjN SoMo CoIn Avg

L4 IN BASE 94.18 66.24 80.21 43.91 77.36 69.25 63.51 88.03 83.77 83.68 52.22 60.57 73.65
L5 IN BASE 93.40 63.95 78.68 44.36 78.26 71.36 64.66 88.84 84.05 84.56 52.58 61.56 74.32

L6 IN BASE 92.20 63.00 77.60 44.74 79.02 71.24 65.00 89.24 84.69 84.53 52.13 62.47 74.61
+ SSR 92.09 62.54 77.32 44.94 78.39 71.31 64.88 89.17 85.79 85.21 53.14 63.32 75.33
+ DSR 91.86 65.61 78.74 45.52 78.77 71.62 65.30 89.08 85.89 84.91 53.40 63.33 75.32

Table 3: Performance on the linguistic probing tasks of evaluating linguistics embedded in the encoder outputs.
“BASE” denotes the representations from TRANFORMER-BASED encoder. “SSR” denotes shallow sentence repre-
sentation. “DSR” denotes deep sentence representation. “AVG” denotes the average accuracy of each category.

et al. (2019) to set the model configurations. We
also listed the results of lower layer representa-
tions (L = 4, 5) in TRANSFORMER-BASE to con-
duct better comparison.

The accuracy results on the different test sets are
shown in Table 3. From the tale, we can see that

• For different encoder layers in the baseline
model (see “L4 in BASE”, “L5 in BASE” and
“L6 in BASE”), lower layers embed more
about surface information while higher lay-
ers encode more semantics, which are consis-
tent with previous findings in (Raganato and
Tiedemann, 2018).

• Integrating the shallow sentence representa-
tion (“+ SSR”) obtains improvement over the
baseline on semantic tasks (75.33 vs. 74.61),
while fails to improve on the surface (77.32
vs. 77.60) and syntactic tasks (64.88 vs.
65.00). This may indicate that the shallow
representations that exploits only the top en-
coder layer (“L6 in BASE”) encodes more se-
mantic information.

• Introducing deep sentence representation (“+
DSR”) brings more improvements. The rea-
son is that our deep sentence representation is
induced from the sentence representations of
all the encoder layers, and lower layers that
contain abound surface and syntactic infor-
mation are exploited.

Along with the above translation experiments,
we believe that the sentential context is necessary
for NMT by enriching the source sentence repre-
sentation. The deep sentential context which is in-
duced from all encoder layers can improve trans-
lation performance by offering different types of
syntax and semantic information.

4 Related Work

Sentential context has been successfully applied
in SMT (Meng et al., 2015; Zhang et al., 2015).
In these works, sentential context representation
which is generated by the CNNs is exploited to
guided the target sentence generation. In broad
terms, sentential context can be viewed as a sen-
tence abstraction from a specific aspect. From
this point of view, domain information (Foster
and Kuhn, 2007; Hasler et al., 2014; Wang et al.,
2017b) and topic information (Xiao et al., 2012;
Xiong et al., 2015; Zhang et al., 2016) can also be
treated as the sentential context, the exploitation
of which we leave for future work.

In the context of NMT, several researchers
leverage document-level context for NMT (Wang
et al., 2017a; Choi et al., 2017; Tu et al., 2018),
while we opt for sentential context. In addition,
contextual information are used to improve the en-
coder representations (Yang et al., 2018, 2019; Lin
et al., 2018). Our approach is complementary to
theirs by better exploiting the encoder represen-
tations for the subsequent decoder. Concerning
guiding the NMT generation with source-side con-
text, Zheng et al. (2018) split the source content
into translated and untranslated parts, while we fo-
cus on exploiting global sentence-level context.

5 Conclusion

In this work, we propose to exploit sentential
context for neural machine translation. Specifi-
cally, the shallow and the deep strategies exploit
the top encoder layer and all the encoder lay-
ers, respectively. Experimental results on WMT14
benchmarks show that exploiting sentential con-
text improves performances over the state-of-the-
art TRANSFORMER model. Linguistic analyses re-
veal that the proposed approach indeed captures
more linguistic information as expected.



6202

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural Machine Translation by Jointly
Learning to Align and Translate. In ICLR.

Ankur Bapna, Mia Chen, Orhan Firat, Yuan Cao, and
Yonghui Wu. 2018. Training deeper neural machine
translation models with transparent attention. In
EMNLP.

Heeyoul Choi, Kyunghyun Cho, and Yoshua Bengio.
2017. Context-dependent word representation for
neural machine translation. Computer Speech &
Language, 45:149–160.

Alexis Conneau, Germán Kruszewski, Guillaume
Lample, Loı̈c Barrault, and Marco Baroni. 2018.
What you can cram into a single $&!#* vector:
Probing sentence embeddings for linguistic proper-
ties. In ACL.

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals,
Jakob Uszkoreit, and Łukasz Kaiser. 2018. Univer-
sal transformers. arXiv preprint arXiv:1807.03819.

Zi-Yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi,
and Tong Zhang. 2018. Exploiting deep representa-
tions for neural machine translation. In EMNLP.

Zi-Yi Dou, Zhaopeng Tu, Xing Wang, Longyue Wang,
Shuming Shi, and Tong Zhang. 2019. Dynamic
layer aggregation for neural machine translation
with routing-by-agreement. In AAAI.

George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for smt. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
128–135. Association for Computational Linguis-
tics.

Jie Hao, Xing Wang, Baosong Yang, Longyue Wang,
Jinfeng Zhang, and Zhaopeng Tu. 2019. Modeling
recurrence for transformer. In NAACL.

Eva Hasler, Barry Haddow, and Philipp Koehn. 2014.
Combining domain and topic adaptation for smt. In
Proceedings of AMTA, volume 1, pages 139–151.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daumé III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classification.
In ACL.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In ACL.

Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In EMNLP.

Jian Li, Baosong Yang, Zi-Yi Dou, Xing Wang,
Michael R. Lyu, and Zhaopeng Tu. 2019. Infor-
mation aggregation for multi-head attention with
routing-by-agreement. In NAACL.

Junyang Lin, Xu Sun, Xuancheng Ren, Shuming Ma,
Jinsong Su, and Qi Su. 2018. Deconvolution-based
global decoding for neural machine translation. In
COLING.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. In ICLR.

Fandong Meng, Zhengdong Lu, Mingxuan Wang,
Hang Li, Wenbin Jiang, and Qun Liu. 2015. Encod-
ing source language with convolutional neural net-
work for machine translation. In ACL.

Eugene A Nida. 1969. Science of translation. Lan-
guage, pages 483–498.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep Contextualized Word Rep-
resentations. In NAACL.

Alessandro Raganato and Jörg Tiedemann. 2018. An
analysis of encoder representations in transformer-
based machine translation. In EMNLP 2018 work-
shop BlackboxNLP.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural Machine Translation of Rare Words
with Subword Units. In ACL.

Yanyao Shen, Xu Tan, Di He, Tao Qin, and Tie-Yan
Liu. 2018. Dense information flow for neural ma-
chine translation. In NAACL.

Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does
string-based neural mt learn source syntax? In
EMNLP.

Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong
Zhang. 2018. Learning to remember translation his-
tory with a continuous cache. TACL.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is All
You Need. In NIPS.

Longyue Wang, Zhaopeng Tu, Andy Way, and Qun
Liu. 2017a. Exploiting cross-sentence context for
neural machine translation. In EMNLP.

Qiang Wang, Fuxue Li, Tong Xiao, Yanyang Li, Yin-
qiao Li, and Jingbo Zhu. 2018. Multi-layer repre-
sentation fusion for neural machine translation. In
COLING.

Rui Wang, Andrew Finch, Masao Utiyama, and Ei-
ichiro Sumita. 2017b. Sentence embedding for neu-
ral machine translation domain adaptation. In ACL.

Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A topic similarity model for
hierarchical phrase-based translation.



6203

Deyi Xiong, Min Zhang, and Xing Wang. 2015.
Topic-based coherence modeling for statistical ma-
chine translation. IEEE/ACM Transactions on Au-
dio, Speech and Language Processing (TASLP),
23(3):483–493.

Baosong Yang, Jian Li, Derek F. Wong, Lidia S. Chao,
Xing Wang, and Zhaopeng Tu. 2019. Context-aware
self-attention networks. In AAAI.

Baosong Yang, Zhaopeng Tu, Derek F. Wong, Fan-
dong Meng, Lidia S. Chao, and Tong Zhang. 2018.
Modeling localness for self-attention networks. In
EMNLP.

Jiajun Zhang, Dakun Zhang, and Jie Hao. 2015. Local
translation prediction with global sentence represen-
tation. In IJCAI.

Jian Zhang, Liangyou Li, Andy Way, and Qun Liu.
2016. Topic-informed neural machine translation.
In COLING.

Zaixiang Zheng, Hao Zhou, Shujian Huang, Lili Mou,
Xinyu Dai, Jiajun Chen, and Zhaopeng Tu. 2018.
Modeling past and future for neural machine trans-
lation. TACL.


