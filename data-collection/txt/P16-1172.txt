



















































Optimizing an Approximation of ROUGE - a Problem-Reduction Approach to Extractive Multi-Document Summarization


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1825–1836,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Optimizing an Approximation of ROUGE – a Problem-Reduction
Approach to Extractive Multi-Document Summarization

Maxime Peyrard and Judith Eckle-Kohler
Research Training Group AIPHES and UKP Lab

Computer Science Department, Technische Universität Darmstadt
www.aiphes.tu-darmstadt.de, www.ukp.tu-darmstadt.de

Abstract

This paper presents a problem-reduction
approach to extractive multi-document
summarization: we propose a reduction
to the problem of scoring individual sen-
tences with their ROUGE scores based on
supervised learning. For the summariza-
tion, we solve an optimization problem
where the ROUGE score of the selected
summary sentences is maximized. To this
end, we derive an approximation of the
ROUGE-N score of a set of sentences, and
define a principled discrete optimization
problem for sentence selection. Mathe-
matical and empirical evidence suggests
that the sentence selection step is solved
almost exactly, thus reducing the problem
to the sentence scoring task. We perform
a detailed experimental evaluation on two
DUC datasets to demonstrate the validity
of our approach.

1 Introduction

Multi-document summarization (MDS) is the task
of constructing a summary from a topically re-
lated document collection. This paper focuses on
the variant of extractive and generic MDS, which
has been studied in detail for the news domain us-
ing available benchmark datasets from the Docu-
ment Understanding Conference (DUC) (Over et
al., 2007).

Extractive MDS can be cast as a budgeted sub-
set selection problem (McDonald, 2007; Lin and
Bilmes, 2011) where the document collection is
considered as a set of sentences and the task is
to select a subset of the sentences under a length
constraint. State-of-the-art and recent works in
extractive MDS solve this discrete optimization
problem using integer linear programming (ILP)

or submodular function maximization (Gillick and
Favre, 2009; Mogren et al., 2015; Li et al., 2013b;
Kulesza and Taskar, 2012; Hong and Nenkova,
2014). The objective function that is maximized
in the optimization step varies considerably in pre-
vious work. For instance, Yih et al. (2007) maxi-
mize the number of informative words, Gillick and
Favre (2009) the coverage of particular concepts,
and others maximize a notion of “summary wor-
thiness”, while minimizing summary redundancy
(Lin and Bilmes, 2011; Kågebäck et al., 2014).

There are also multiple approaches which max-
imize the evaluation metric for system sum-
maries itself based on supervised Machine Learn-
ing (ML). System summaries are commonly eval-
uated using ROUGE (Lin, 2004), a recall oriented
metric that measures the n-gram overlap between
a system summary and a set of human-written ref-
erence summaries.

The benchmark datasets for MDS can be em-
ployed in two different ways for supervised learn-
ing of ROUGE scores: either by training a model
that assigns ROUGE scores to individual tex-
tual units (e.g., sentences), or by performing
structured output learning and directly maximiz-
ing the ROUGE scores of the created summaries
(Nishikawa et al., 2014; Takamura and Okumura,
2010; Sipos et al., 2012). The latter approach suf-
fers both from the limited amount of training data
and from the higher complexity of the machine
learning models.

In contrast, supervised learning of ROUGE
scores for individual sentences can be performed
with simple regression models using hundreds of
sentences as training instances, taken from a sin-
gle pair of documents and reference summaries.
Extractive MDS can leverage the ROUGE scores
of individual sentences in various ways, in partic-
ular, as part of an optimization step. In our work,
we follow the previously successful approaches to

1825



extractive MDS using discrete optimization, and
make the following contributions:

We provide a theoretical justification and em-
pirical validation for using ROUGE scores of
individual sentences as an optimization objec-
tive. Assuming that ROUGE scores of individ-
ual sentences have been estimated by a super-
vised learner, we derive an approximation of the
ROUGE-N score for a set of sentences from the
ROUGE-N scores of the individual sentences in
the general case of N >= 1.

We use our approximation to define a math-
ematically principled discrete optimization prob-
lem for sentence selection. We empirically evalu-
ate our framework on two DUC datasets, demon-
strating the validity of our approximation, as
well as its ability to achieve competitive ROUGE
scores in comparison to several strong baselines.

Most importantly, the resulting framework re-
duces the MDS task to the problem of scoring in-
dividual sentences with their ROUGE scores. The
overall summarization task is converted to two se-
quential tasks: (i) scoring single sentences, and (ii)
selecting summary sentences by solving an opti-
mization problem where the ROUGE score of the
selected sentences is maximized.

The optimization objective we propose almost
exactly solves (ii), which we justify by provid-
ing both mathematical and empirical evidence.
Hence, solving the whole problem of MDS is re-
duced to solving (i).

The rest of this paper is structured as follows:
in Section 2, we discuss related work. Section 3
presents our subset selection framework consist-
ing of an approximation of the ROUGE score of a
set of sentences, and a mathematically principled
discrete optimization problem for sentence selec-
tion. We evaluate our framework in Section 4 and
discuss the results in Section 5. Section 6 con-
cludes.

2 Related Work

Related to our approach is previous work in ex-
tractive MDS that (i) casts the summarization
problem as budgeted subset selection, and (ii)
employs supervised learning on MDS datasets to
learn a scoring function for textual units.

Budgeted Subset Selection Extractive MDS
can be formulated as the problem of selecting a
subset of textual units from a document collection
such that the overall score of the created summary

is maximal and a given length constraint is ob-
served. The selection of textual units for the sum-
mary relies on their individual scores, assigned
by a scoring function which represents aspects of
their relevance for a summary. Often, sentences
are considered as textual units.

Simultaneously maximizing the relevance
scores of the selected units and minimizing their
pairwise redundancy given a length constraint
is a global inference problem which can be
solved using ILP (McDonald, 2007). Several
state-of-the-art results in MDS have been obtained
by using ILP to maximize the number of relevant
concepts in the created summary while minimiz-
ing the pairwise similarity between the selected
sentences (Gillick and Favre, 2009; Boudin et al.,
2015; Woodsend and Lapata, 2012).

Another way to formulate the problem of find-
ing the best subset of textual units is to maxi-
mize a submodular function. Maximizing sub-
modular functions is a general technique that uses
a greedy optimization algorithm with a mathe-
matical guarantee on optimality (Nemhauser and
Wolsey, 1978). Performing summarization in the
framework of submodularity is natural because
summaries try to maximize the coverage of rele-
vant units while minimizing redundancy (Lin and
Bilmes, 2011). However, several different cover-
age and redundancy functions have been proposed
(Lin and Bilmes, 2011; Kågebäck et al., 2014; Yin
and Pei, 2015) recently, and there is not yet a clear
consensus on which coverage function to maxi-
mize.

Supervised Learning Supervised learning us-
ing datasets with reference summaries has already
been employed in early work on summarization to
classify sentences as summary-worthy or not (Ku-
piec et al., 1995; Aone et al., 1995).

Learning a scoring function for various kinds
of textual units has become especially popular in
the context of global optimization: scores of tex-
tual units, learned from data, are fed into an ILP
problem solver to find the subset of sentences
with maximal overall score. For example, Yih
et al. (2007) score each word in the document
cluster based on frequency and position, Li et al.
(2013b) learn bigram frequency in the reference
summaries, and Hong and Nenkova (2014) learn
word importance from a rich set of features.

Closely related to our work are summarization
approaches that include a supervised component

1826



which assigns ROUGE scores to individual sen-
tences. For example, Ng et al. (2012), Li et
al. (2013a) and Li et al. (2015) all use a regres-
sion model to learn ROUGE-2 scores for indi-
vidual sentences, but use it in different ways for
the summarization. While Ng et al. (2012) use
the ROUGE scores of sentences in combination
with the Maximal Marginal Relevance algorithm
as a baseline approach, Li et al. (2013a) use the
scores to select the top-ranked sentences for sen-
tence compression and subsequent summarization.
Li et al. (2015), in contrast, use the ROUGE scores
to re-rank a set of sentences that are output by an
optimization step.

While learning ROUGE scores of textual units
is widely used in summarization systems, the the-
oretical background on why this is useful has not
been well studied yet. In our work, we present the
mathematical and empirical justification for this
common practice. In the next section, we start
with the mathematical justification.

3 Content Selection Framework

3.1 Approximation of ROUGE-N

Notation: Let S = {si|i ≤ m} be a set of m
sentences which constitute a system summary. We
use ρN (S) or simply ρ(S) to denote the ROUGE-
N score of S. ROUGE-N evaluates the n-gram
overlap between S and a set of reference sum-
maries (Lin, 2004). Let S∗ denote the reference
summary and RN the number of n-gram tokens
in S∗. RN is a function of the summary length
in words, in particular, R1 is the target size of
the summary in words. Finally, let FS(g) denote
the number of times the n-gram type g occurs in
S. For a single reference summary, ROUGE-N is
computed as follows:

ρ(S) =
1
RN

∑
g∈S∗

min(FS(g), FS∗(g)) (1)

For compactness, we use the following notation
for any set of sentences X:

CX,S∗(g) = min(FX(g), FS∗(g)) (2)

CX,S∗(g) can be understood as the contribution of
the n-gram g.

ROUGE-N for a Pair of Sentences: Using this
notation, the ROUGE-N score of a set of two sen-

tences a and b can be written as:

ρ(a ∪ b) = 1
RN

∑
g∈S∗

min(Ca∪b,S∗(g), FS∗(g))

(3)
We observe that ρ(a ∪ b) can be expressed as a

function of the individual scores ρ(a) and ρ(b):

ρ(a ∪ b) = ρ(a) + ρ(b)− �(a ∩ b) (4)
where �(a∩ b) is an error correction term that dis-
cards overcounted n-grams from the sum of ρ(a)
and ρ(b):

�(a ∩ b) =
1
RN

∑
g∈S∗

max(Ca,S∗(g)+Cb,S∗(g)−FS∗(g), 0)

(5)

A proof that this error correction is correct is given
in appendix A.1.

General Formulation of ROUGE-N: We can
extend the previous formulation of ρ to sets of ar-
bitrary cardinality using recursion. If ρ(S) is given
for a set of sentences S, and a is a sentence then:

ρ(S ∪ a) = ρ(S) + ρ(a)− �(S ∩ a) (6)
We prove in appendix A.1 that this formula is the
ROUGE-N score of S ∪ a.

Another way to obtain ρ for an arbitrary set S is
to adapt the principle of inclusion-exclusion:

ρ(S) =
m∑

i=1

ρ(si)+

m∑
k=2

(−1)k+1(
∑

1≤i1≤···≤ik≤m
�(k)(si1 ∩· · ·∩sik))

(7)

This formula can be understood as adding up
scores of individual sentences, but n-grams ap-
pearing in the intersection of two sentences might
be overcounted. �(2) is used to account for these
n-grams. But now, n-grams in the intersection of
three sentences might be undercounted and �(3) is
used to correct this. Each �(k) contributes to im-
proving the accuracy by refining the errors made
by �(k−1) for the n-grams appearing in the inter-
section of k sentences. When k = |S|, ρ(S) is
exactly the ROUGE-N of S. A rigorous proof and
details about �(k) are provided in appendix A.2.

1827



Approximation of ROUGE-N for a Pair of Sen-
tences: To find a valid approximation of ρ as de-
fined in (7), we first consider the ρ(a ∪ b) from
equation (3) and then extend it to the general case.
When maximizing ρ, scores for sentences are as-
sumed to be given (e.g., estimated by a ML com-
ponent). We still need to estimate �(a ∩ b), which
means, according to (5), to estimate:∑

g∈S∗
max(Ca,S∗(g)+Cb,S∗(g)−FS∗(g), 0) (8)

At inference time, neither S∗ (the reference sum-
mary) nor FS∗ (number of occurrences of n-grams
in the reference summary) is known.

At this point, we can observe that, similar as
for sentence scoring, � can be estimated via a su-
pervised ML component. Such an ML model can
easily be trained on the intersections of all sen-
tence pairs in a given training dataset. Hence, we
can assume that both the scores for individual sen-
tences and the � are learned empirically from data
using ML. As a result, we have pushed all estima-
tion steps into supervised ML components, which
leaves the subset selection step fully principled.

However, we found in our experiments that even
a simple heuristic yields a decent approximation of
�. The heuristic uses the frequency freq(g) of an
n-gram g observed in the source documents:∑

g∈S∗
max(Ca,S∗(g) + Cb,S∗(g)− FS∗(g), 0)

≈
∑

g∈a∩b
1[freq(g) ≥ α] (9)

The threshold α tells us which n-grams are likely
to appear in the reference summary, and it is de-
termined by grid-search on the training set. This
is penalizing n-grams which appear twice and are
likely to occur in the summary. It can be under-
stood as a way of limiting redundancy. In prac-
tice, we used α = 0.3. However, we experimented
with various values of the hyper-parameter α and
found that its value has no significant impact as
long as it is fairly small (< 0.5). Higher values
will ignore too many redundant n-grams and the
summary will have a high redundancy.
RN is known since it is simply the number of

n-gram tokens in the summaries. We end up with
the following approximation for the pairwise case:

ρ̃(a ∪ b) = ρ(a) + ρ(b)− �̃(a ∪ b), where
�̃(a ∪ b) = 1

RN

∑
g∈a∩b

1[freq(g) ≥ α] (10)

General Approximation of ROUGE-N: Now,
we can approximate ρ(S) for the general case de-
fined by equation (7). We recall that ρ(S) con-
tains the sum of ρ(si), the pairwise error terms
�(2)(si∩sj), the error terms of three sentences �(3)
and so on.

We can restrict ourselves to the individual sen-
tences and the pairwise error corrections. Indeed,
the intersection between more than two sentences
is often empty, and accounting for it does not im-
prove the accuracy significantly, but greatly in-
creases the computational cost.

A formulation of � in the case of two sentences
has already been defined in (10). Thus, we have an
approximation of the ROUGE-N function for any
set of sentences that can be computed at inference
time:

ρ̃(S) =
n∑

i=1

ρ(si)−
∑

a,b∈S,a6=b
�̃(a ∩ b) (11)

We empirically checked the validity of this ap-
proximation. For this, we sampled 1000 sets of
sentences from source documents of DUC-2003
(sets of 2 to 5 sentences) and compared their ρ̃
score to the real ROUGE-N. We observe a pear-
son’s r correlation ≥ 0.97, which validates ρ̃.
3.2 Discrete Optimization
ρ̃ from equation (11) defines a set function that
scores a set of sentences. The task of summariza-
tion is now to select the set S∗ with maximal ρ̃(S∗)
under a length constraint.

Submodularity: A submodular function is a set
function obeying the diminishing returns property:
∀S ⊆ T and a sentence a: F (S ∪ a) − F (S) ≥
F (T ∪a)−F (T ). Submodular functions are con-
venient because maximization under constraints
can be done greedily with a guarantee of the op-
timality of the solution (Nemhauser et al., 1978).

It has been shown that ROUGE-N is submodu-
lar (Lin and Bilmes, 2011) and it is easy to verify
that ρ̃ is submodular as well (the proof is given in
the supplemental material).

We can therefore apply the greedy maximiza-
tion algorithm to find a good set of sentences. This
has the advantage of being straightforward and
fast, however it does not necessarily find the op-
timal solution.

ILP: A common way to solve a discrete opti-
mization problem is to formulate it as an ILP. It

1828



maximizes (or minimizes) a linear objective func-
tion with some linear constraints where the vari-
ables are integers. ILP has been well studied and
existing tools can efficiently retrieve the exact so-
lution of an ILP problem.

We observe that it is possible to formulate the
maximization of ρ̃(S) as an ILP. Let x be the
binary vector whose i-th entry indicates whether
sentence i is in the summary or not, ρ̃(si) the
scores of sentences, and K the length constraint.
We pre-compute the symmetric matrix P̃ where
P̃i,j = �̃(si ∩ sj) and solve the following ILP:

max(
n∑

i=1
xi ∗ ρ̃(si)− d 1R

∑
i≥j

αi,j ∗ P̃ i, j)∑n
i=1 xi ∗ len(si) ≤ K
∀(i, j), αi,j − xi ≤ 0
∀(i, j), αi,j − xj ≤ 0

∀(i, j), xi + xj − αi,j ≤ 1
d is a damping factor that allows to account for

approximation errors. When d = 0, the problem
becomes the maximization of “summary worthi-
ness” under a length constraint, with “summary
worthiness” being defined by ρ(si).

In practice, we used a value d = 0.9 because
we observed that the learner tends to slightly over-
estimate the ROUGE-N scores of sentences. The
mathematical derivation implies d = 1, however
we can easily adjust for shifts in average scores
of sentences from the estimation step by adjust-
ing d. Another option would be to post-process
the scores after the estimation step to fix the av-
erage and let d = 1 in the optimization step. In-
deed, if dmoves away from 1, we move away from
the mathematical framework of ROUGE-N maxi-
mization.

If d 6= 0, it seems intuitive to interpret the sec-
ond term as minimizing the summary redundancy,
which is in accordance to previous works.

However, in our framework, this term has a pre-
cise interpretation:

it maximizes ROUGE-N scores up to the second
order of precision, and the ROUGE-N formula it-
self already induces a notion of “summary worthi-
ness” and redundancy, which we can empirically
infer from data via supervised ML for sentence
scoring, and a simple heuristic for sentence inter-
sections.

4 Evaluation

We perform three kinds of experiments in order
to empirically evaluate our framework: first, we

show that our proposed approximation is valid,
then we analyze a basic supervised sentence scor-
ing component, and finally we perform an extrin-
sic evaluation on end-to-end extractive MDS.

In our experiments, we use the DUC datasets
from 2002 and 2003 (DUC-02 and DUC-03).
We use the variants of ROUGE identified by
Owczarzak et al. (2012) as strongly correlating
with human evaluation methods: ROUGE-2 re-
call with stemming and stopwords not removed
(giving the best agreement with human evalua-
tion), and ROUGE-1 recall (as the measure with
the highest ability to identify the better summary
in a pair of system summaries). For DUC-03,
summaries are truncated to 100 words, and to 200
words for DUC-02. 1 The truncation is done auto-
matically by ROUGE. 2

4.1 Framework Validity
Given that sentences receive scores close to their
individual ROUGE-N, we presented a function
that approximates the ROUGE-N of sets of these
sentences and proposed an optimization to find the
best scoring set under a length constraint.

To validate our framework empirically, we con-
sider its upper-bound, which is obtained when
our ILP/submodular optimizations use the real
ROUGE-N scores of the individual sentences, cal-
culated based on the reference summaries. We
compare this upper bound to a greedy approach,
which simply adds the best scoring sentences one
by one to the subset until the length limit is
reached, and to the real upper bound for extractive
summarization which is determined by solving a
maximum coverage problem for n-grams from the
reference summary (as it was done by Takamura
and Okumura (2010)).

Table 1 shows the results. We observe that ILP-
R produces scores close to the reference, thus re-
ducing the problem of extractive summarization to
the task of sentence scoring, because the perfect
scores induced near perfect extracted summaries
in this framework. SBL-R seems less promising
than ILP-R because it greedily maximizes a func-
tion which ILP-R exactly maximizes. Therefore,
we continue our experiments in the following sec-

1In the official DUC-03 competitions, summaries of
length 665 bytes were expected. Systems could produce dif-
ferent numbers of words. The variation in length has a no-
ticeable impact on ROUGE recall scores.

2ROUGE-1.5.5 with the parameters: -n 2 -m -a -l 100 -x
-c 95 -r 1000 -f A -p 0.5 -t 0. The length parameter becomes
-l 200 for DUC-02.

1829



tions with ILP-R only. However, SBL-R offers a
nice trade-off between performance and computa-
tion cost. The greedy optimization of SBL-R is
noticeably faster than ILP-R.

DUC-02 DUC-03
R1 R2 R1 R2

Greedy 0.597 0.414 0.391 0.148

SBL-R 0.630 0.484 0.424 0.160
ILP-R 0.644 0.495 0.447 0.178

Upper Bound 0.648 0.497 0.452 0.181

Table 1: Upper bound of our framework compared
to extractive upper bound.

In practice, the learner will not produce per-
fect scores. We experimentally validated that with
learned scores converging to true scores, the ex-
tracted summary converges to the best extrac-
tive summary (w.r.t to ROUGE-N). To this end,
we simulated approximate learners by artificially
randomizing the true scores to end up with lists
having various correlations with the true scores.
We fed these scores to ILP-R and computed the
ROUGE-1 of the generated summaries for an ex-
ample topic from DUC-2003. Figure 1 displays
the expected ROUGE-1 versus the performance of
the artificial learner (correlation with true scores
of sentences). We observe that, as the learner im-
proves, the generated summaries approach the best
ROUGE scoring summary.

Figure 1: ROUGE-1 of summary against sentence
scores correlation with true ROUGE-1 scores of
sentences (d30003t from DUC-2003).

4.2 Sentence Scoring
Now we look at the supervised learning compo-
nent which learns ROUGE-N scores for individ-
ual sentences. We know that we can achieve an
overall summary ROUGE-N score close to the up-
per bound, if a learner would be able to learn the
scores perfectly. For better understanding the dif-
ficulty of the task of sentence scoring, we look at

the correlation of the scores produced by a basic
learner and the true scores given in a reference
dataset.

Model and Features From an existing summa-
rization dataset (e.g. a DUC dataset), a training
set can straightforwardly be extracted by annotat-
ing each sentence in the source documents with
its ROUGE-N score. For each topic in the dataset,
this yields a list of sentences and their target score.

To support the claim that learning ROUGE
scores for individual sentences is easier than solv-
ing the whole summarization task, it is suffi-
cient to choose a basic learner with simple fea-
tures and little in-domain training data (models
are trained on one DUC dataset and evaluated
on another). Specifically, we employ a support
vector regression (SVR).3 We use only classical
surface-level features to represent sentences (po-
sition, length, overlap with title) and combine
them with frequency features. The latter include
TF*IDF weighting of the terms (similar to Luhn
(1958)), the sum of the frequency of the bi-grams
in the sentence, as well as the sum of the document
frequency (number of source documents in which
the n-grams appear) of the terms and bi-grams in
a sentence.

We trained two models, R1 and R2 on DUC-
02 and DUC-03. For R1, the target score is the
ROUGE-1 recall, while R2 learns ROUGE-2 re-
call.

Correlation Analysis We evaluated our sen-
tence scoring models R1 and R2 by calculating the
correlation of the scores produced by R1 and R2
and the true scores given in the DUC-03 data. We
compare both models to the true ROUGE-1 and
ROUGE-2 scores. In addition, we calculated the
correlation of the TF*IDF and LexRank scores,
in order to understand how well they would fit
into our framework (TF*IDF and LexRank are de-
scribed in section 4.3).

The results are displayed in Table 2. Even
with a basic learner it is possible to learn scores
that correlate well with the true ROUGE-N scores,
which supports the claim that it is easier to learn
scores for individual sentences than to solve the
whole problem of summarization. This finding
strongly supports our proposed reduction of the
extractive MDS problem to the task of learning

3We use the implementation in scikit-learn (Pedregosa et
al., 2011).

1830



with ROUGE-1 with ROUGE-2
Pearson’s r Kendall’s tau nDCG@15 Pearson’s r Kendall’s tau nDCG@15

TF*IDF 0.923 0.788 0.916 0.607 0.512 0.580
LexRank 0.210 0.120 0.534 0.286 0.178 0.379

model R1 0.940 0.813 0.951 0.653 0.545 0.693
model R2 0.729 0.496 0.891 0.743 0.576 0.752

Table 2: Correlation of different kinds of sentence scores and their true ROUGE-1 and ROUGE-2 scores.

scores for individual sentences, which correlate
well with their true ROUGE-N scores.

We observe that TF*IDF correlates surprisingly
well with the ROUGE-1 score, which indicates
that we can expect a significant performance gain
when feeding TF*IDF scores to our optimization
framework. LexRank, on the other hand, orders
sentences according to their centrality and does
not look at individual sentences. Accordingly, we
observe a low correlation with the true ROUGE-
N scores, and thus LexRank may not benefit from
the optimization (which we confirmed in our ex-
periments).

Finally, we observe that there is significant
room for improvement regarding ROUGE-2, as
well as for Kendall’s tau in ROUGE-1 where a
more sophisticated learner could produce scores
that correlate better with the true scores. The
higher the correlation of the sentence scores as-
signed by a learner and the true scores, the better
the summary produced by the subsequent subset
selection.

4.3 End-to-End Evaluation
In our end-to-end evaluation on extractive MDS,
we use the following baselines for comparison:

• TF*IDF weighting: This simple heuristic
was introduced by Luhn (1958). Each sen-
tence receives a score from the TF*IDF of its
terms. We trained IDFs (Inverse Document
Frequencies) on a background corpus 4 to im-
prove the original algorithm.

• LexRank: Among other graph-based ap-
proaches to summarization (Mani and Bloe-
dorn, 1997; Radev et al., 2000; Mihalcea,
2004), LexRank (Erkan and Radev, 2004) has
become the most popular one. A similar-
ity graph G(V,E) is constructed where V is
the set of sentences and an edge eij is drawn
between sentences vi and vj if and only if

4We used DBpedia long abstract:
http://wiki.dbpedia.org/Downloads2015-04.

the cosine similarity between them is above
a given threshold. Sentences are scored ac-
cording to their PageRank score in G. For
our experiments, we use the implementation
available in the sumy package.5

• ICSI: ICSI is a recent system that has been
identified as one of the state-of-the-art sys-
tems by Hong et al. (2014). It is a global
linear optimization framework that extracts
a summary by solving a maximum coverage
problem considering the most important con-
cepts in the source documents. Concepts are
identified as bi-grams and their importance is
estimated via their frequency in the source
documents. Boudin et al. (2015) released a
Python implementation (ICSI sume) that we
use in our experiments.

• SFOUR: SFOUR is a structured prediction
approach that trains an end-to-end system
with a large-margin method to optimize a
convex relaxation of ROUGE (Sipos et al.,
2012). We use the publicly available imple-
mentation. 6

As described in the previous section, two models
are trained: R1 and R2. We evaluate both of them
in the end-to-end setup with and without our op-
timization. In the greedy version, sentences are
added as long as the summary length is valid.

We apply the optimization for sentence scor-
ing models trained on ROUGE-1 and ROUGE-2
as well. The scoring models are trained on one
dataset and evaluated on the other. For the ILP op-
timization, the damping factor can vary and leads
to different performance. We report the best re-
sults among few variations. In order to speed-up
the ILP step, we propose to limit the search space
by only looking at the top K sentences7 (hence

5https://github.com/miso-belica/sumy
6http://www.cs.cornell.edu/˜rs/sfour/
7We used K=50 and observed that a range from K=25 to

K=70 yields a good trade-off between computation cost and
performance.

1831



the importance of learning a correct ordering as
well, like Kendall’s tau). This results in a mas-
sive speed-up and can even lead to better results as
it prunes parts of the noise. Finally, we perform
significance testing with the t-test to compare dif-
ferences between two means.8

DUC-02 DUC-03
R1 R2 R1 R2

TFIDF 0.403 0.120 0.322 0.066
LexRank 0.446 0.158 0.354 0.077
ICSI 0.445 0.155 0.375 0.094
SFOUR 0.442 0.181 0.365 0.087

Greedy-R1 0.480 0.115 0.353 0.084
Greedy-R2 0.499 0.132 0.369 0.093

TFIDF+ILP 0.415 0.135 0.335 0.075
R1+ILP 0.509 0.187 0.378 0.101
R2+ILP 0.516* 0.192* 0.379 0.102

Table 3: Impact of the optimization step on sen-
tence subset selection.

Results Table 3 shows the results. The proposed
optimization significantly and systematically im-
proves TF*IDF performance as we expected from
our analysis in the previous section. This re-
sult suggests that using only a frequency signal
in source documents is enough to get high scor-
ing summaries, which supports the common belief
that frequency is one of the most useful features
for generic news summarization. It also aligns
well with the strong performance of ICSI, which
combines an ILP step with frequency information
as well.

The optimization also significantly and system-
atically improves upon the greedy approach com-
bined with our scoring models. Combining a SVR
learner (SVR-1 and SVR-2) and our ILP-R pro-
duces results on par with ICSI and sometimes sig-
nificantly better. SFOUR maximizes ROUGE in
an end-to-end fashion, but is outperformed by our
framework when using the same training data. The
framework is able to reach a competitive perfor-
mance even with a basic learner. These results
again suggest that investigating better learners for
sentence scoring might be promising in order to
improve the quality of the summaries.

We observe that the model trained on ROUGE-
2 is performing better than the model trained
on ROUGE-1, although learning the ROUGE-2
scores seems to be harder than learning ROUGE-1

8The symbol * indicates that the difference compared to
the previous best baseline is significant with p ≤ 0.05.

scores (as shown in table 2). However, errors and
approximations propagate less easily in ROUGE-
2, because the number of bi-grams in the intersec-
tion of two given sentences is far less. Hence we
conclude that learning ROUGE-2 scores should be
put into the focus of future work on improving
sentence scoring.

5 Discussion

This section discusses our contributions in a
broader context.

ROUGE Our subset selection framework per-
forms the task of content selection, selecting an
unordered set of textual units (sentences for now)
for a system summary. The re-ordering of the
sentences is left to a subsequent processing step,
which accounts for aspects of discourse coherence
and readability.

While we justified our choice of ROUGE-1 re-
call and ROUGE-2 recall as optimization objec-
tives by their strong correlation with human evalu-
ation methods, ROUGE-N has also various draw-
backs. In particular, it does not take into account
the overall discourse coherence of a system sum-
mary (see the supplemental material for examples
of summaries generated by our framework).

From a broader perspective, systems that have
high ROUGE scores can only be as good as
ROUGE is, as a proxy for summary quality.
However, as long as systems are evaluated with
ROUGE, a natural approach is to develop systems
that maximize it.

Should novel automatic evaluation metrics be
developed, our approach can still be applied, pro-
vided that the new metrics can be expressed as a
function of the scores of individual sentences.

Structured Learning Compared to MDS ap-
proaches using structured learning, our problem-
reduction has the important advantage that it con-
siderably scales-up the available training data
by working on sentences instead of docu-
ments/summaries pairs. Moreover, the task of sen-
tence scoring is not dependent on arbitrary param-
eters such as the summary length which are inher-
ently abstracted from the “summary worthiness”
of individual textual units.

Error Propagation The first step of the frame-
work is left to a ML component which can only
produce approximate scores. Empirical results (in
Figure 1 and Table 2) suggest that even with an

1832



imperfect first step, the subsequent optimization
is able to produce high scoring summaries. How-
ever, it might be insightful to study rigorously and
in greater detail the propagation of errors induced
by the first step.

Other Metrics This work focused on maxi-
mizing ROUGE-N recall because it is a widely
acknowledged automatic evaluation metric.
ROUGE-N relies on reference summaries which
forces us to perform an estimation step. In our
framework, we use ML to estimate the individual
scores of sentences without using reference
summaries.

However, Louis and Nenkova (2013) proposed
several alternative evaluation metrics for system
summaries which do not need reference sum-
maries. They are based on the properties of the
system summary and the source documents alone,
and correlate well with human evaluation. Some
of them can even reach a correlation with human
evaluation similar to the ROUGE-2 recall.

An example of such a metric is the Jensen-
Shannon Divergence (JSD) which is a symmet-
ric smoothed version of the Kullback-Leibler di-
vergence. Maximizing JSD can not be solved ex-
actly with an ILP because it can not be factorized
into individual sentences. However, applying an
efficient greedy algorithm or maximizing a fac-
torizable relaxation might produce strong results
as well (for example, a simple greedy maximiza-
tion of Kullback-Leibler divergence already yields
good results (Haghighi and Vanderwende, 2009)).

Future Work In this work, we developed a prin-
cipled subset selection framework and empirically
justified it. We focused on solving the second
step of the framework while keeping the machine
learning component as simple as possible. Essen-
tially, our framework performs a modularization
of the task of MDS, where all characteristics of the
data and feature representations are pushed into a
separate machine learning module – they should
not affect the subsequent optimization step which
remains fixed.

The promising results we obtained for sum-
marization with a basic learner (see Section 4.3)
encourage future work on plugging in more so-
phisticated supervised learners in our framework.
For example, we plan to incorporate lexical-
semantic information in the feature representa-
tion and leverage large-scale unsupervised pre-

training. This direction is particularly promising
because we have shown that we can expect sig-
nificant performance gains for end-to-end MDS as
the sentence scoring component improves.

6 Conclusion

We proposed a problem-reduction approach to ex-
tractive MDS, which performs a reduction to the
problem of scoring individual sentences with their
ROUGE scores based on supervised learning. We
defined a principled discrete optimization prob-
lem for sentence selection which relies on an ap-
proximation of ROUGE. We empirically checked
the validity of the approach on standard datasets
and observed that even with a basic learner the
framework produces promising results. The code
for our optimizers is available at github.com/
UKPLab/acl2016-optimizing-rouge.

Acknowledgments

This work has been supported by the German Re-
search Foundation as part of the Research Training
Group “Adaptive Preparation of Information from
Heterogeneous Sources” (AIPHES) under grant
No. GRK 1994/1.

References

Chinatsu Aone, Mary Ellen Okurowski, James Gor-
linsky, and Bjornar Larsen. 1995. A Trainable
Summarizer with Knowledge Acquired from Robust
NLP Techniques. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in Automatic Text Sum-
marization, pages 68–73. MIT Press, Cambridge,
MA, USA.

Florian Boudin, Hugo Mougard, and Benot Favre.
2015. Concept-based Summarization using Inte-
ger Linear Programming: From Concept Pruning to
Multiple Optimal Solutions. In Llus Mrquez, Chris
Callison-Burch, Jian Su, Daniele Pighin, and Yuval
Marton, editors, Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1914–1918, Lisbon, Portugal.

Günes Erkan and Dragomir R. Radev. 2004. LexRank:
Graph-based Lexical Centrality As Salience in Text
Summarization. Journal of Artificial Intelligence
Research, pages 457–479.

Dan Gillick and Benoit Favre. 2009. A Scalable
Global Model for Summarization. In Proceedings
of the Workshop on Integer Linear Programming for
Natural Langauge Processing, ILP ’09, pages 10–
18, Boulder, Colorado.

1833



Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing Content Models for Multi-document Summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 362–370.

Kai Hong and Ani Nenkova. 2014. Improving the
Estimation of Word Importance for News Multi-
Document Summarization. In Proceedings of the
14th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 712–721, Gothenburg, Sweden.

Kai Hong, John Conroy, benoit Favre, Alex Kulesza,
Hui Lin, and Ani Nenkova. 2014. A Reposi-
tory of State of the Art and Competitive Baseline
Summaries for Generic News Summarization. In
Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC’14),
pages 1608–1616, Reykjavik, Iceland.

Mikael Kågebäck, Olof Mogren, Nina Tahmasebi, and
Devdatt Dubhashi. 2014. Extractive Summariza-
tion using Continuous Vector Space Models. In Pro-
ceedings of the 2nd Workshop on Continuous Vector
Space Models and their Compositionality (CVSC),
pages 31–39, Gothenburg, Sweden.

Alex Kulesza and Ben Taskar. 2012. Determinantal
Point Processes for Machine Learning. Foundations
and Trends in Machine Learning, 5:123–286.

Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A Trainable Document Summarizer. In Proceed-
ings of the 18th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 68–73, Seattle, Washington,
USA. Association for Computing Machinery.

Chen Li, Fei Liu, Fuliang Weng, and Yang Liu.
2013a. Document Summarization via Guided Sen-
tence Compression. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 490–500, Seat-
tle, Washington, USA.

Chen Li, Xian Qian, and Yang Liu. 2013b. Using Su-
pervised Bigram-based ILP for Extractive Summa-
rization. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 1004–1013, Sofia, Bulgaria.

Chen Li, Yang Liu, and Lin Zhao. 2015. Improving
Update Summarization via Supervised ILP and Sen-
tence Reranking. In Proceedings of the 2015 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1317–1322, Denver, Col-
orado.

Hui Lin and Jeff A. Bilmes. 2011. A Class of Submod-
ular Functions for Document Summarization. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
510–520, Portland, Oregon.

Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out
at ACL, pages 74–81, Barcelona, Spain.

Annie Louis and Ani Nenkova. 2013. Automati-
cally Assessing Machine Summary Content With-
out a Gold Standard. Computational Linguistic,
39(2):267–300, June.

Hans Peter Luhn. 1958. The Automatic Creation of
Literature Abstracts. IBM Journal of Research De-
velopment, 2:159–165.

Inderjeet Mani and Eric Bloedorn. 1997. Multi-
document Summarization by Graph Search and
Matching. In Proceedings of the Fourteenth Na-
tional Conference on Artificial Intelligence and
Ninth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 622–628, Providence,
Rhode Island. AAAI Press.

Ryan McDonald. 2007. A Study of Global Inference
Algorithms in Multi-document Summarization. In
Proceedings of the 29th European Conference on
IR Research, pages 557–564, Rome, Italy. Springer-
Verlag.

Rada Mihalcea. 2004. Graph-based Ranking Al-
gorithms for Sentence Extraction, Applied to Text
Summarization. In Proceedings of the ACL 2004
on Interactive Poster and Demonstration Sessions,
ACLdemo ’04, page 20, Barcelona, Spain.

Olof Mogren, Mikael Kågebäck, and Devdatt Dub-
hashi. 2015. Extractive Summarization by Aggre-
gating Multiple Similarities. In Recent Advances
in Natural Language Processing, pages 451–457,
Hissar, Bulgaria.

George L. Nemhauser and Laurence A. Wolsey. 1978.
Best Algorithms for Approximating the Maximum
of a Submodular Set Function. Mathematics of Op-
erations Research, 3(3):177–188.

George L. Nemhauser, Laurence A. Wolsey, and
Marschall L. Fisher. 1978. An Analysis of Ap-
proximations for Maximizing Submodular Set Func-
tionsI. Mathematical Programming, 14:265–294.

Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen
Kan, and Chew-Lim Tan. 2012. Exploiting
Category-Specific Information for Multi-Document
Summarization. In Proceedings of the 24th Inter-
national Conference on Computational Linguistics
(COLING 2012), pages 2093–2108, Mumbai, India.

Hitoshi Nishikawa, Kazuho Arita, Katsumi Tanaka,
Tsutomu Hirao, Toshiro Makino, and Yoshihiro
Matsuo. 2014. Learning to Generate Coherent
Summary with Discriminative Hidden Semi-Markov
Model. In Proceedings of COLING 2014, the 25th
International Conference on Computational Lin-
guistics: Technical Papers, pages 1648–1659.

1834



Paul Over, Hoa Dang, and Donna Harman. 2007. DUC
in Context. Information Processing and Manage-
ment, 43(6):1506–1520.

Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An Assessment of
the Accuracy of Automatic Evaluation in Summa-
rization. In Proceedings of Workshop on Evaluation
Metrics and System Comparison for Automatic Sum-
marization, pages 1–9, Montreal, Canada.

Fabian Pedregosa, Gael Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine Learning in Python. Journal
of Machine Learning Research, 12:2825–2830.

Dragomir R. Radev, Hongyan Jing, and Malgorzata
Budzikowska. 2000. Centroid-based Summariza-
tion of Multiple Documents: Sentence Extraction,
Utility-based Evaluation, and User Studies. In Pro-
ceedings of the NAACL-ANLP Workshop on Auto-
matic Summarization, volume 4, pages 21–30, Seat-
tle, Washington.

Ruben Sipos, Pannaga Shivaswamy, and Thorsten
Joachims. 2012. Large-margin Learning of Sub-
modular Summarization Models. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, pages
224–233, Avignon, France.

Hiroya Takamura and Manabu Okumura. 2010. Learn-
ing to Generate Summary as Structured Output. In
Proceedings of the 19th ACM international Confer-
ence on Information and Knowledge Management,
pages 1437–1440. Association for Computing Ma-
chinery.

Kristian Woodsend and Mirella Lapata. 2012. Multi-
ple Aspect Summarization Using Integer Linear Pro-
gramming. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, (EMNLP-CoNLL), pages 233–243, Jeju
Island, Korea.

Wen-tau Yih, Joshua Goodman, Lucy Vanderwende,
and Hisami Suzuki. 2007. Multi-document Summa-
rization by Maximizing Informative Content-words.
In Proceedings of the 20th International Joint Con-
ference on Artifical Intelligence, pages 1776–1782,
Hyderabad, India. Morgan Kaufmann Publishers
Inc.

Wenpeng Yin and Yulong Pei. 2015. Optimizing Sen-
tence Modeling and Selection for Document Sum-
marization. In Proceedings of the 24th International
Conference on Artificial Intelligence, pages 1383–
1389, Buenos Aires, Argentina. AAAI Press.

A Supplemental Material

A.1 Recursive Expression of ROUGE-N
Let S = {si|i ≤ m} and T = {ti|i ≤ l} be
two sets of sentences, S∗ the reference summary,
and ρ(X) denote the ROUGE-N score of the set
of sentences X . Assuming that ρ(S) and ρ(T ) are
given, we prove the following recursive formula:

ρ(S ∪ T ) = ρ(S) + ρ(T )− �(S ∩ T ) (12)
For compactness, we use the following notation as
well:

CX,S∗(g) = min(FX(g), FS∗(g)) (13)

Proof: We have the following definitions:

ρ(S) =
1
RN

∑
g∈S∗

F̃S,S∗(g) (14)

ρ(T ) =
1
RN

∑
g∈S∗

F̃T,S∗(g) (15)

�(S ∩ T ) =
1
RN

∑
g∈S∗

max(CS,S∗(g)+CT,S∗(g)−FS∗(g), 0)

(16)

And by definition of ROUGE, the formula of S ∪
T :

ρ(S ∪ T ) = 1
RN

∑
g∈S∗

min(FS∪T (g), FS∗(g))

(17)
In order to prove equation (12), we have to show

that the following equation holds:∑
g∈S∗

CS,S∗(g) +
∑
g∈S∗

CT,S∗(g)

−
∑
g∈S∗

max(CS,S∗(g) + CT,S∗(g)− FS∗(g), 0)

=
∑
g∈S∗

min(FS∪T (g), FS∗(g)) (18)

It is sufficient to show:

∀g ∈ S∗, CS,S∗(g) + CT,S∗(g)−
max(CS,S∗(g) + CT,S∗(g)− FS∗(g), 0)

= min(FS∪T (g), FS∗(g)) (19)

Let g ∈ S∗ be a n-gram. There are two possi-
bilities:

1835



• FS(g) + FT (g) ≤ FS∗(g): g appears less
times in S ∪ T than in the reference sum-
mary. It implies: min(FS∪T (g), FS∗(g)) =
FS∪T (g) = FS(g) + FT (g). Moreover,
all FX(g) are positive numbers by defini-
tion, and FS(g) ≤ FS∗(g) is equivalent
to: CS,S∗(g) = min(FS(g), FS∗(g)) =
FS(g). Similarly, we have: CT,S∗(g) =
min(FT (g), FS∗(g)) = FT (g). Since
max(CS,S∗(g)+CT,S∗(g)−FS∗(g), 0) = 0,
the equation (19) holds in this case.

• FS(g) + FT (g) ≥ FS∗(g): g appears more
frequently in S∪T than in the reference sum-
mary. It implies: min(FS∪T (g), FS∗(g)) =
FS∗(g). Here we have: max(CS,S∗(g) +
CT,S∗(g) − FS∗(g), 0) = CS,S∗(g) +
CT,S∗(g) − FS∗(g), and it directly follows
that equation (19) holds in this case as well.

Equation (19) has been proved, which proves (12)
as well.

A.2 Expanded Expression of ROUGE-N

Let S = {si|i ≤ m} be a set of sentences and
ρ(S) its ROUGE-N score. We prove the following
formula:

ρ(S) =
m∑

i=1

ρ(si)+

m∑
k=2

(−1)k+1(
∑

1≤i1≤···≤ik≤m
�(k)(si1 ∩· · ·∩sik))

(20)

Proof: Let g ∈ S∗ be a n-gram in the ref-
erence summary, and k ∈ [1,m] the number
of sentences in which it appears. Specifically,
∃{si1 , · · · , sik},∀sij ∈ {si1 , . . . , sik}, g ∈ sij . In
order to prove the formula (20), we have to find an
expression for the �(k) that gives to g the correct
contribution to the formula:

1
RN

min(FS(g), FS∗(g)) (21)

First, we observe that g does not appear in the
terms that contain the intersection of more than k
sentences. Specifically, �(t) is not affected by g if
t ≥ k. However, g is affected by all the �(t) for
which t ≤ k.

Given that g appears in the sentences
{si1 , . . . , sij}, we can determine the score

attributed to g by the previous �(t) (t ≤ k):

S(k−1)(g) =
∑

s∈{si1 ,...,sik}
ρ(s)+

k∑
l=2

(−1)(l+1)
∑

1≤i1≤···≤il≤k
�(l)(si1 ∩ · · · ∩ sil))

(22)

Now, g receives the correct contribution to the
overall scores if �(k) is defined as follows:

�(k)(si1 ∩ · · · ∩ sij ) =
1
R

∑
g∈si1∩···∩sij

min(C{si1 ,...,sik}(g), FS∗(g))

− S(k−1)(g) (23)
Indeed, with this expression for �(k), the score of
g is:

S(k−1)(g) +
1
RN

min(C{si1 ,...,sik}(g), FS∗(g))

− S(k−1)(g) (24)
Which can be simplified to:

1
RN

min(C{si1 ,...,sik}(g), FS∗(g)) (25)

Since g appears only in the sentences
{si1 , . . . , sik}, F̃{si1 ,...,sik}(g) = FS(g) and
it follows that:

1
RN

min(C{si1 ,...,sik}(g), FS∗(g)) =

1
RN

min(FS(g), FS∗(g)) (26)

This proves equation (20) because we observe
that g will not be affected by any other terms. Ev-
ery �(t) for t ≤ k including g is counted by S(k−1),
and no other terms from �(k) will affect g because
all the other terms �(k) should contain at least one
sentence that is not in {si1 , . . . , sik} and g would
not belong to this intersection by definition.

Finally, it has been proved in the appendix A.1
that for k = 2, �(2) has a reduced form:

�(2)(sa ∩ sb) =
1
RN

∑
g∈S∗

max(Csa,S∗(g)+Csb,S∗(g)−FS∗(g), 0)

(27)

In the paper, we ignore the terms for k ≥ 2, there-
fore we do not search for a reduced form for these
terms.

1836


