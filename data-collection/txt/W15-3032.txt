



















































Results of the WMT15 Tuning Shared Task


Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 274–281,
Lisboa, Portugal, 17-18 September 2015. c©2015 Association for Computational Linguistics.

Results of the WMT15 Tuning Shared Task

Miloš Stanojević and Amir Kamran
University of Amsterdam

ILLC
{m.stanojevic,a.kamran}@uva.nl

Ondřej Bojar
Charles University in Prague

MFF ÚFAL
bojar@ufal.mff.cuni.cz

Abstract

This paper presents the results of the
WMT15 Tuning Shared Task. We pro-
vided the participants of this task with a
complete machine translation system and
asked them to tune its internal parameters
(feature weights). The tuned systems were
used to translate the test set and the out-
puts were manually ranked for translation
quality. We received 4 submissions in the
English-Czech and 6 in the Czech-English
translation direction. In addition, we ran
3 baseline setups, tuning the parameters
with standard optimizers for BLEU score.

1 Introduction

Almost all modern statistical machine translation
(SMT) systems internally consider translation can-
didates from several aspects. Some of these as-
pects can be very simple and one parameter is suf-
ficient to capture them, such as the word penalty
incurred for every word produced or the phrase
penalty controlling whether the sentence should be
translated in fewer or more independent phrases,
leading to more or less word-for-word translation.
Other aspects try to assess e.g. the fidelity of the
translation, the fluency of the output or the amount
of reordering. These are far more complex and for-
mally captured in a model such as the translation
model or language model.

Both the simple penalties as well as the scores
from the more complex models are called features
and need to be combined to a single score to allow
for ranking of translation candidates. This is usu-
ally done using a linear combination of the scores:

score(e) =
M∑

m=1

λmhm(e, f) (1)

where e and f are the candidate translation and
the source, respectively, and hm(·, ·) is one of the

M penalties or models. The tuned parameters are
λm ∈ R, called feature weights.

Feature weights have a tremendous effect on
the final translation quality. For instance the sys-
tem can produce extremely long outputs, fabulat-
ing words just in order to satisfy a negatively-
weighted word penalty, i.e. a bonus for each word
produced. An inherent part of the preparation
of MT systems is thus some optimization of the
weight settings.

If we had to set the weights manually, we would
have to try a few configurations and pick one that
leads to reasonable outputs. The common prac-
tice is to use an optimization algorithm that ex-
amines many settings, evaluating the produced
translations automatically against reference trans-
lations using some evaluation measure (tradition-
ally called “metric” in the MT field). In short,
the optimizer tunes model weights so that the final
combined model score correlates with the metric
score.

The metric score, in turn, is designed to cor-
relate well with human judgements of translation
quality, see Stanojević et al. (2015) and the pre-
vious papers summarizing WMT metrics tasks.
However, a metric that correlates well with hu-
mans on final output quality may not be usable
in weight optimization for various technical rea-
sons. BLEU (Papineni et al., 2002) was shown to
be very hard to surpass (Cer et al., 2010) and this is
also confirmed by the results of the invitation-only
WMT11 Tunable Metrics Task (Callison-Burch et
al., 2010)1. Note however, that some metrics have
been successfully used for system tuning (Liu et
al., 2011; Beloucif et al., 2014).

The aim of the WMT15 Tuning Task2 is to at-
tract attention to the exploration of all the three

1http://www.statmt.org/wmt11/
tunable-metrics-task.html

2http://www.statmt.org/wmt15/
tuning-task/

274



Sentences Tokens Types
Source cs en cs en cs en

LM corpora News Commentary v8 162309 247966 3.6M 6.2M 162K 81K
TM corpora Europarl v7, CCrawl and News Comm. v9 911952 17.7M 20.8M 652K 361K
Dev set newstest2014 3003 51K 60K 19K 13K
Test set newstest2015 2656 39K 47K 16K 11K

Table 1: Data used in the WMT15 tuning task.

Dev Test
Direction Token Type Token Type

en-cs 2570 2032 2003 1655
cs-en 3891 3415 3381 3011

Table 2: Out of vocabulary word counts

aspects of model optimization: (1) the set of fea-
tures in the model, (2) optimization algorithm, and
(3) MT quality metric used in optimization.

For (1), we provide a fixed set of “dense” fea-
tures and also allow participants to add additional
“sparse” features. For (2), the optimization al-
gorithm, task participants are free to use one of
the available algorithms for direct loss optimiza-
tion (Och, 2003; Zhao and Chen, 2009), which are
usually capable of optimizing only a dozen of fea-
tures, or one of the optimizers handling also very
large sets of features (Cherry and Foster, 2012;
Hopkins and May, 2011), or a custom algorithm.
And finally for (3), participants can use any estab-
lished evaluation metric or a custom one.

1.1 Tuning Task Assignment

Tuning task participants were given a complete
model for the hierarchical variant of the machine
translation system Moses (Hoang et al., 2009)
and the development set (newstest2014), i.e. the
source and reference translations. No “dev test”
set was provided, since we expected that partic-
ipants will internally evaluate various variants of
their method by manually judging MT outputs. In
fact, we offered to evaluate a certain number of
translations into Czech for free to ease the partici-
pation for teams without any access to speakers of
Czech; only one team used this service once.

A complete model consists of a rule table ex-
tracted from the parallel corpus, the default glue
grammar and the language model extracted from
the monolingual data. As such, this defines a fixed
set of dense features. The participants were al-
lowed to add any sparse features implemented in
Moses Release 3.0 (corresponds to Github com-
mit 5244a7b607) and/or to use any optimization
algorithm and evaluation metric. Fully manual

optimization was also not excluded but nobody
seemed to take this approach.

Each submission in the tuning task consisted of
the configuration of the MT system, i.e. the addi-
tional sparse features (if any) and the values of all
the feature weights, λm.

2 Details of Systems Tuned

The systems that were distributed for tuning are
based on Moses (Hoang et al., 2009) implementa-
tion of hierarchical phrase-based model (Chiang,
2005). The language models were 5-gram mod-
els with Kneser-Ney smoothing (Kneser and Ney,
1995) built using KenLM (Heafield et al., 2013).
For word alignments, we used Mgiza++ (Gao and
Vogel, 2008).

The parallel data used for training translation
models consisted of the Europarl v7, News Com-
mentary data (parallel-nc-v9) and Com-
monCrawl, as released for WMT14.3 We excluded
CzEng because we wanted to keep the task small
and accessible to more groups.

Since the test set (newstest2015) and the de-
velopment set (newstest2014) are in the news do-
main, we opted to exclude Europarl from the lan-
guage model data. We did not add any monolin-
gual news on top of News Commentary, which are
quite close to the news domain. In retrospect, we
should have added also some of the monolingual
news data as released by WMT, esp. since we used
a 5-gram LM.

Before any further processing, the data was to-
kenized (using Moses tokenizer) and lowercased.
We also removed sentences longer than 60 words
or shorter than 4 words. Table 1 summarizes the
final dataset sizes and Table 2 provides details on
out-of-vocabulary items.

Aside from the dev set provided, the partici-
pants were free to use any other data for tuning
(making their submission “unconstrained”), but no
participant decided to do that. All tuning task sub-
missions are therefore also constraint in terms of

3http://www.statmt.org/wmt14/
translation-task.html

275



System Participant
BLEU-* baselines
AFRL United States Air Force Research Laboratory (Erdmann and Gwinnup, 2015)
DCU Dublin City University (Li et al., 2015)

HKUST Hong Kong University of Science and Technology (Lo et al., 2015)
ILLC-UVA ILLC – University of Amsterdam (Stanojević and Sima’an, 2015)

METEOR-CMU Carnegie Mellon University (Denkowski and Lavie, 2011)
USAAR-TUNA Saarland University (Liling Tan and Mihaela Vela; no corresponding paper)

Table 3: Participants of WMT15 Tuning Shared Task

the WMT15 Translation Task (Bojar et al., 2015).
We leave all decoder settings (n-best list size,

pruning limits etc.) at their default values. While
the participants may have used different limits dur-
ing tuning, the final test run was performed at our
site with the default values. It is indeed only the
feature weights that differ.

3 Tuning Task Participants

The list of participants and the names of the sub-
mitted systems are shown in Table 3, along with
references to the details of each method.

USAAR-TUNA by Liling Tan and Mihaela
Vela has no accompanying paper, so we sketch
it here. The method sets each weight as the har-
monic mean ( 2xyx+y ) of the weight proposed by
batch MIRA and MERT. Batch MIRA and MERT
are run side by side and the harmonic mean is
taken and used in moses.ini at every iteration.
The optimization stops when the averaged weights
change only very little, which happened around it-
eration 17 or 18 in this case (Liling Tan, pc).

ILLC-UVA (Stanojević and Sima’an, 2015)
was tuned using KBMIRA with modified version of
BEER evaluation metric. The authors claim that
standard trained evaluation metrics learn to give
too much importance to recall and thus lead to
overly long translations in tuning. For that reason
they modify the training of BEER to value recall
and precision equally. This modified version of
BEER is used to train the MT system.

DCU (Li et al., 2015) is tuned with RED, an
evaluation metric based on maching of depen-
dency n-grams. Authors have tried tuning with
both MERT and KBMIRA and found that KBMIRA
gives better results so the submitted system uses
KBMIRA.

HKUST (Lo et al., 2015) is with an improved
version of MEANT. MEANT is an evaluation met-
ric that pays more attention to semantic aspect
of translation. Better correlation on the sentence
level was achieved by integrating distributional se-

mantics into MEANT and handling failures of the
underlying semantic parser. The submission of
HKUST contained a bug that was discovered af-
ter human evaluation period so the corrected sub-
mission HKUST-LATE is evaluated only with
BLEU.

METEOR-CMU (Denkowski and Lavie,
2011) is a system tuned for an adapted version of
Meteor. Meteor’s parameters are set to give an
equal importance to precision and recall.

AFRL (Erdmann and Gwinnup, 2015) is the
only submission trained with a new tuning al-
gorithm “Drem” instead of the standard MERT
or KBMIRA. Drem uses scaled derivative-free
trust-region optimization instead of line search or
(sub)gradient approximations. For weight settings
that were not tested in the decoder yet, it interpo-
lates the decoder output using the information of
which settings produced which translations. The
optimized metric is a weighted combination of
NIST, Meteor and Kendall’s τ .

In addition to the systems submitted, we pro-
vided three baselines:

• BLEU-MERT-DENSE – MERT tuning with
BLEU without additional features

• BLEU-MIRA-DENSE – KBMIRA tuning with
BLEU without additional features

• BLEU-MIRA-SPARSE – KBMIRA tuning
with BLEU with additional sparse features

Since all the submissions including the base-
lines were subject to manual evaluation, we did
not run the MERT or MIRA optimizations more
than once (as is the common practice for estimat-
ing variance due to optimizer instability). We sim-
ply used the default settings and stopping criteria
and picked the weights that performed best on the
dev set according to BLEU.

Of all the submissions, only the submission
METEOR-CMU used sparse features. For a
more interesting comparison, we set our baseline

276



(BLEU-MIRA-SPARSE) to use the very same set
of sparse features. These features are automati-
cally constructed using Moses’ feature templates
named PhraseLengthFeature0, SourceWordDele-
tionFeature0, TargetWordInsertionFeature0 and
WordTranslationFeature0. They were made for
the 50 most frequent words in the training data.
For both language pairs these feature templates
produce around 1000 features.

4 Results

We used the submitted moses.ini and (option-
ally) sparse weights files to translate the test set.
The test set was not available to the participants at
the time of their submission (not even the source
side). We used the Moses recaser trained on the
target side of the parallel corpus to recase the out-
puts of all the models.

Finally, the recased outputs were manually eval-
uated, jointly with regular translation task submis-
sions of WMT (Bojar et al., 2015). This was not
enough to reliably separate tuning systems in the
Czech-to-English direction, so we asked task par-
ticipants to provide some further rankings.

The resulting human rankings were used to
compute the overall manual score using the
TrueSkill method, same as for the main translation
task (Bojar et al., 2015). We report two variants
of the score: one is based on manual judgements
related to tuning systems only and one is based
on all judgements. Note that the actual ranking
tasks shown to the annotators were identical, mix-
ing tuning systems with regular submissions.

Tables 4 and 5 contain the results of the submit-
ted systems sorted by their manual scores.

The horizonal lines represent separation be-
tween clusters of systems that perform similarly.
Cluster boundaries are established by the same
method as for the main translation task. Inter-
estingly, cluster boundaries for Czech-to-English
vary as we change the set of judgements.

Some systems do not have the TrueSkill
score because they were either submitted af-
ter the deadline (HKUST-LATE) or served as
additional baselines and performed similarly to
our baselines (USAAR-BASELINE-MIRA and
USAAR-BASELINE-MERT).

5 Discussion

There are a few interesting observations that can
be made about the baseline results. Various details

System Name TrueSkill Score BLEU
Tuning-Only All

BLEU-MIRA-DENSE 0.153 -0.182 12.28
ILLC-UVA 0.108 -0.189 12.05

BLEU-MERT-DENSE 0.087 -0.196 12.11
AFRL 0.070 -0.210 12.20

USAAR-TUNA 0.011 -0.220 12.16
DCU -0.027 -0.263 11.44

METEOR-CMU -0.101 -0.297 10.88
BLEU-MIRA-SPARSE -0.150 -0.320 10.84

HKUST -0.150 -0.320 10.99
HKUST-LATE — — 12.20

Table 4: Results on Czech-English tuning

System Name TrueSkill Score BLEU
Tuning-Only All

DCU 0.320 -0.342 4.96
BLEU-MIRA-DENSE 0.303 -0.346 5.31

AFRL 0.303 -0.342 5.34
USAAR-TUNA 0.214 -0.373 5.26

BLEU-MERT-DENSE 0.123 -0.406 5.24
METEOR-CMU -0.271 -0.563 4.37

BLEU-MIRA-SPARSE -0.992 -0.808 3.79
USAAR-BASELINE-MIRA — — 5.31
USAAR-BASELINE-MERT — — 5.25

Table 5: Results on English-Czech tuning

of the submissions including the exact weight set-
tings are in Table 6.

5.1 Dense vs. Sparse Features
It is surprising how well the baseline based on KB-
MIRA and BLEU tuning (BLEU-MIRA-DENSE)
performs on both language pairs. On Czech-
English, it is better than all the other submitted
systems while on English-Czech, only one system
outperforms it (staying in the same performance
cluster anyway).

Using BLEU-MIRA-DENSE for tuning dense
features is becoming more common in the MT
community, compared to the previous standard of
using MERT. Our results confirm this practice.
Preferring KBMIRA to MERT is often motivated by
possibility to include sparse features, but we see
that even for dense features only KBMIRA is better
than MERT.

The sparse models, BLEU-MIRA-SPARSE and
METEOR-CMU, however, perform rather poorly
even though they were trained with KBMIRA. Both
of the sparse submissions use the same set of fea-
tures and the same tuning algorithm, although the
optimization was run at different sites. The only
difference is the metric they optimize. Tuning
for Meteor (Denkowski and Lavie, 2011) gives
better results than tuning for BLEU (Papineni et
al., 2002). Unfortunately, we had no system with

277



-1.2

-1

-0.8

-0.6

-0.4

-0.2

 0

 0.2

 0.4

-0.3 -0.25 -0.2 -0.15 -0.1 -0.05

M
an

ua
l S

co
re

Word Penalty (after L2 normalization)

bleu_MERT
DCU

bleu_MIRA_dense
USAAR-Tuna

AFRL
METEOR_CMU

bleu_MIRA_sparse

Figure 1: Relation between the word penalty and
the final performance of systems translating from
English to Czech.

●

−1.5 −1.0 −0.5 0.0 0.5 1.0

−1
.0

−0
.5

0.
0

0.
5

1.
0

1.
5

2.
0

PC1

P
C

2

● DCU
bleu_MIRA_dense
AFRL
USAAR−Tuna
bleu_MERT
bleu_MIRA_sparse
METEOR_CMU

Figure 2: PCA for English-Czech. The darker the
point, the higher the manual score.

dense features tuned for Meteor so we could not
see if Meteor outperforms BLEU in the dense-only
setting as well.

It is not clear why the sparse methods perform
badly. One explanation could be the relatively
small development set or some pruning settings.
In any case, we find it unfortunate that sparse fea-
tures in the hierarchical model harm performance
in the default configuration4.

5.2 Some Observations on Weight Settings
We tried to find some patterns in the weight set-
tings and the performance of the system, but ad-
mittedly, it is difficult to make much sense of the
few points in the 8-dimensional space.

For English-to-Czech, we can see a gist of a
bell-like shape when normalizing the weights with
L2 norm and plotting the word penalty and the

4MERT and two MIRA runs reached BLEU of not more
than +0.02 points higher when the size of n-best list was in-
creased from 100 to 200. So n-best list size does not seem to
be the problem.

C
ze

ch
-t

o-
E

ng
lis

h
Ty

pe
M

an
ua

lS
co

re
Te

st
B

L
E

U
D

ev
B

L
E

U
L

M
0

Ph
rP

en
T

M
0

T
M

1
T

M
2

T
M

3
G

lu
e

W
rd

Pe
n

A
FR

L
de

ns
e

0.
07

00
12

.2
0

14
.8

3
0.

15
88

-0
.3

33
0

0.
05

45
0.

08
59

0.
19

58
0.

17
16

0.
63

09
-0

.6
22

7
bl

eu
M

E
R

T
de

ns
e

0.
08

70
12

.1
1

14
.6

4
0.

09
92

-0
.0

50
7

0.
06

88
0.

03
50

0.
12

96
0.

09
19

0.
18

20
-0

.3
42

8
bl

eu
M

IR
A

de
ns

e
de

ns
e

0.
15

30
12

.2
8

14
.8

5
0.

06
71

-0
.1

68
9

0.
03

63
0.

04
13

0.
07

47
0.

06
80

0.
29

82
-0

.2
45

4
bl

eu
M

IR
A

sp
ar

se
sp

ar
se

-0
.1

50
0

10
.8

4
13

.1
6

0.
09

06
-0

.0
56

8
0.

04
31

0.
05

56
0.

09
28

0.
09

33
0.

35
84

-0
.2

09
3

D
C

U
de

ns
e

-0
.0

27
0

11
.4

4
13

.5
8

0.
05

58
-0

.1
40

7
0.

03
60

0.
05

17
0.

08
56

0.
06

71
0.

24
81

-0
.3

15
0

H
K

U
ST

M
E

A
N

T
de

ns
e

-0
.1

50
0

10
.9

9
13

.2
3

0.
13

33
0.

08
68

0.
13

18
0.

01
15

0.
05

34
0.

12
21

0.
05

00
-0

.4
11

0
H

K
U

ST
M

E
A

N
T

L
A

T
E

de
ns

e
—

12
.2

0
14

.4
2

0.
06

38
-0

.1
69

6
0.

06
55

0.
02

17
0.

07
13

0.
06

77
0.

30
74

-0
.2

33
0

IL
L

C
U

vA
de

ns
e

0.
10

80
12

.0
5

14
.5

7
0.

09
18

-0
.1

21
5

0.
04

52
0.

06
24

0.
11

03
0.

06
97

0.
22

95
-0

.2
69

6
M

E
T

E
O

R
C

M
U

sp
ar

se
-0

.1
01

0
10

.8
8

13
.3

5
0.

09
36

-0
.0

10
3

0.
06

02
0.

05
09

0.
11

62
0.

11
87

0.
29

46
-0

.2
55

6
U

SA
A

R
-T

un
a

de
ns

e
0.

01
10

12
.1

6
14

.5
7

0.
07

89
-0

.0
71

5
0.

03
83

0.
05

75
0.

10
39

0.
07

44
0.

18
39

-0
.2

95
2

E
ng

lis
h-

to
-C

ze
ch

Ty
pe

M
an

ua
lS

co
re

Te
st

B
L

E
U

D
ev

B
L

E
U

L
M

0
Ph

rP
en

T
M

0
T

M
1

T
M

2
T

M
3

G
lu

e
W

rd
Pe

n
A

FR
L

de
ns

e
0.

30
30

5.
34

6.
96

0.
05

43
-0

.4
32

6
-0

.0
02

5
0.

03
82

0.
26

96
0.

07
88

0.
83

32
-0

.1
87

8
bl

eu
M

E
R

T
de

ns
e

0.
12

30
5.

24
7.

11
0.

05
10

-0
.1

35
3

0.
00

48
0.

01
69

0.
17

72
0.

04
08

0.
35

08
-0

.2
23

1
bl

eu
M

IR
A

de
ns

e
de

ns
e

0.
30

30
5.

31
7.

20
0.

03
80

-0
.2

04
6

-0
.0

00
4

0.
02

86
0.

13
38

0.
03

20
0.

39
36

-0
.1

68
9

bl
eu

M
IR

A
sp

ar
se

sp
ar

se
-0

.9
92

0
3.

79
5.

19
0.

03
64

-0
.1

23
2

-0
.0

05
3

0.
03

50
0.

09
05

0.
04

80
0.

55
24

-0
.1

09
3

D
C

U
de

ns
e

0.
32

00
4.

96
6.

87
0.

02
47

-0
.1

94
9

-0
.0

02
2

0.
03

67
0.

13
70

0.
03

45
0.

37
67

-0
.1

93
2

M
E

T
E

O
R

C
M

U
sp

ar
se

-0
.2

71
0

4.
37

5.
86

0.
03

94
-0

.0
93

5
-0

.0
08

7
0.

03
31

0.
16

11
0.

06
73

0.
45

48
-0

.1
42

1
Sa

ar
la

nd
ba

se
lin

e
m

er
t

de
ns

e
—

5.
25

7.
16

0.
03

94
-0

.1
61

9
-0

.0
01

1
0.

02
18

0.
19

47
0.

02
11

0.
39

73
-0

.1
62

8
Sa

ar
la

nd
ba

se
lin

e
m

ir
a

de
ns

e
—

5.
31

7.
11

0.
03

77
-0

.2
02

3
-0

.0
00

7
0.

02
93

0.
13

04
0.

03
44

0.
39

36
-0

.1
71

4
U

SA
A

R
-T

un
a

de
ns

e
0.

21
40

5.
26

7.
15

0.
03

86
-0

.1
79

9
-0

.0
00

8
0.

02
50

0.
15

62
0.

02
62

0.
39

54
-0

.1
67

0

Table 6: Detailed scores and weights of Czech-
to-English (left) and English-to-Czech (right) sys-
tems.

278



●

−2 −1 0 1 2

−1
.0

−0
.5

0.
0

0.
5

1.
0

1.
5

PC1

P
C

2
● bleu_MIRA_dense

ILLC_UvA
bleu_MERT
AFRL
USAAR−Tuna
HKUST_MEANT
bleu_MIRA_sparse
METEOR_CMU
DCU

Figure 3: PCA for Czech-English. The darker the
point, the higher the manual score.

manual score, see Figure 1. The middle values
seemed to be a good setting. For the other transla-
tion direction or other weights, no such clear rela-
tion is apparent.

We tried to interpret the weight settings also
using principal component analysis (PCA), de-
spite the low number of observations. (Ideally,
we would like to have at least 40–80 systems, we
have 7 or 9). Before running PCA, we normalized
the weights with L2 norm. After running Cattell
Scree test, the results showed that two components
would be appropriate to summarize the dataset. To
make components more interpretable, we applied
varimax rotation.

Figure 2 plots the two principal components of
the set of systems for English-to-Czech. We see
that the first component (PC1) explains the perfor-
mance almost completely with middle values be-
ing the best. Looking at loadings (correlations of
components with the original feature function di-
mensions) in Table 7, we learn, that PC1 primar-
ily accounts for the first two weights of transla-
tion model (TM 0 and TM 1, which correspond
to phrase and lexically-weighted inverse probabil-
ities, resp.) and the word penalty (WrdPen) and
language model weight (LM0). Knowing that in
almost all systems the weight of word penalty is
several times bigger than weights of TM 0, TM 1,
and LM0, we conclude that tuning of word penalty
(in balance with LM weight) was the most appar-
ent decisive factor of English-Czech tuning task.
The second component (PC2) primarily covers the
weights of the remaining features, that is the direct
translation probabilites and phrase penalty. Unfor-
tunately, PC2 is not very informative about the fi-
nal quality of the translation.

The Czech-to-English results in Figure 3 do not

PC1 PC2
LM0 -0.69 0.44
PhrasePenalty0 0.15 -0.63
TranslationModel0 0 -0.91 -0.13
TranslationModel0 1 0.91 -0.03
TranslationModel0 2 -0.55 0.72
TranslationModel0 3 0.36 0.75
TranslationModel1 0.42 0.84
WordPenalty0 0.84 0.27

Table 7: Loadings (correlations) of each compo-
nent with each feature function for English-Czech

seem to lend themselves to any simple conclusion.
Based on closeness of systems in the PCA

plots, we can say that for English-Czech, two out
of three best systems (BLEU-MIRA-DENSE and
DCU) found similar settings while AFRL stands
out. Czech-English results show that systems of
very similar weight settings give translations of
very different quality. Again, AFRL stands out
while leading to very good outputs.

6 Conclusion

This paper presented the WMT shared task in opti-
mizing parameters of a given hierarchical phrase-
based system (WMT Tuning Task) when translat-
ing from English to Czech and vice versa. The
underlying system was intentionally restricted to
small data setting and somewhat unusually, the
data for the language model were smaller than for
the translation model.

Overall, six teams took part in one or both direc-
tions, sticking to the constrained setting, with only
METEOR-CMU and our baseline BLEU-MIRA-
SPARSE using sparse features.

The submitted configurations were manually
evaluated jointly with the systems of the main
WMT translation task. Given the small data set-
ting, we did not expect the tuning task systems to
perform competitively to other submissions in the
WMT translation task.

The results confirm that KBMIRA with the stan-
dard (dense) features optimized towards BLEU
should be preferred over MERT. Two other sys-
tems (DCU and AFRL) performed equally well in
English-to-Czech translation. The two systems us-
ing sparse features (METEOR-CMU and BLEU-
MIRA-SPARSE) performed poorly, but the sam-
ple is too small to draw any conclusions from
this. Overall, the variance in translation quality
obtained using various weight settings is apparent
and justifies the efforts put into optimization tech-

279



niques.
Since the task attracted a good number of sub-

missions and was generally considered interesting
and useful by our colleagues, we plan to run the
task again for WMT in 2016. The next year’s un-
derlying systems will use all data available in the
WMT constraint setting, to test the tuning methods
in the range where state-of-the-art systems oper-
ate.

Acknowledgments

We are grateful to Christian Federmann and Matt
Post for all the processing of human evaluation
and to the annotators who quickly helped us in
getting additional judgements. Thanks also go to
Matthias Huck for a thorough check of the paper,
all outstanding errors are our own. This project
has received funding from the European Union’s
Horizon 2020 research and innovation programme
under grant agreements no 645452 (QT21) and
no 644402 (HimL). The work on this project was
also supported by the Dutch organisation for sci-
entific research STW grant nr. 12271.

References
Meriem Beloucif, Chi-kiu Lo, and Dekai Wu. 2014.

Improving MEANT Based Semantically Tuned
SMT. In Proc. of 11th International Workshop on
Spoken Language Translation (IWSLT 2014), pages
34–41, Lake Tahoe, California.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Barry Haddow, Matthias Huck, Chris Hokamp,
Philipp Koehn, Varvara Logacheva, Christof Monz,
Matteo Negri, Matt Post, Carolina Scarton, Lucia
Specia, and Marco Turchi. 2015. Findings of the
2015 Workshop on Statistical Machine Translation.
In Proceedings of the Tenth Workshop on Statistical
Machine Translation, Lisboa, Portugal, September.
Association for Computational Linguistics.

Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 Joint Workshop on Sta-
tistical Machine Translation and Metrics for Ma-
chine Translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 17–53, Uppsala, Sweden, July.
Association for Computational Linguistics. Revised
August 2010.

Daniel Cer, Christopher D. Manning, and Daniel Juraf-
sky. 2010. The best lexical metric for phrase-based
statistical mt system optimization. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association

for Computational Linguistics, HLT ’10, pages 555–
563, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ’12, pages 427–436, Stroudsburg, PA,
USA. Association for Computational Linguistics.

David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’05), pages
263–270, Ann Arbor, Michigan, June.

Michael Denkowski and Alon Lavie. 2011. Me-
teor 1.3: Automatic Metric for Reliable Optimiza-
tion and Evaluation of Machine Translation Sys-
tems. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 85–91, Ed-
inburgh, Scotland, July. Association for Computa-
tional Linguistics.

Grant Erdmann and Jeremy Gwinnup. 2015. Drem:
The AFRL Submission to the WMT15 Tuning Task.
In Proceedings of the Tenth Workshop on Statistical
Machine Translation, Lisboa, Portugal, September.
Association for Computational Linguistics.

Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In In Proc. of the
ACL 2008 Software Engineering, Testing, and Qual-
ity Assurance Workshop.

Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modified
Kneser-Ney Language Model Estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690–696,
Sofia, Bulgaria, August.

Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchi-
cal, and Syntax-Based Statistical Machine Trans-
lation. In Proceedings of IWSLT, pages 152–159,
Tokyo, Japan, December.

Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 1352–1362, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language model-
ing. In IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP),, volume 1,
pages 181–184. IEEE.

Liangyou Li, Hui Yu, and Qun Liu. 2015. MT Tuning
on RED: A Dependency-Based Evaluation Metric.
In Proceedings of the Tenth Workshop on Statistical
Machine Translation, Lisboa, Portugal, September.
Association for Computational Linguistics.

280



Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2011. Better Evaluation Metrics Lead to Better Ma-
chine Translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’11, pages 375–384, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Chi-kiu Lo, Philipp Dowling, and Dekai Wu. 2015.
Improving evaluation and optimization of MT sys-
tems against MEANT. In Proceedings of the Tenth
Workshop on Statistical Machine Translation, Lis-
boa, Portugal, September. Association for Computa-
tional Linguistics.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 160–
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318.

Miloš Stanojević and Khalil Sima’an. 2015. BEER
1.1: ILLC UvA submission to metrics and tuning
task. In Proceedings of the Tenth Workshop on
Statistical Machine Translation, Lisboa, Portugal,
September. Association for Computational Linguis-
tics.

Miloš Stanojević, Amir Kamran, Philipp Koehn, and
Ondřej Bojar. 2015. Results of the WMT15 Metrics
Shared Task. In Proceedings of the Tenth Workshop
on Statistical Machine Translation, Lisboa, Portu-
gal, September. Association for Computational Lin-
guistics.

Bing Zhao and Shengyuan Chen. 2009. A simplex
armijo downhill algorithm for optimizing statistical
machine translation decoding parameters. In HLT-
NAACL (Short Papers), pages 21–24. The Associa-
tion for Computational Linguistics.

281


