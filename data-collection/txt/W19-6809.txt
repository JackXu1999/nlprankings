




































Multilingual Multimodal Machine Translation for Dravidian Languages
utilizing Phonetic Transcription

Bharathi Raja Chakravarthi,
Bernardo Stearns, Mihael Arcan,

Manel Zarrouk, and John P. McCrae
Insight Centre for Data Analytics

National University of Ireland Galway
Galway, Ireland

name.surname@insight-centre.org

Ruba Priyadharshini
Saraswathi Narayanan College,

Madurai, India
Arun Jayapal

Smart Insights from Conversations,
Hyderabad, India

S. Sridevy
Tamil Nadu Agricultural University,

Coimbatore, India

Abstract

Multimodal machine translation is the task
of translating from a source text into the
target language using information from
other modalities. Existing multimodal
datasets have been restricted to only highly
resourced languages. In addition to that,
these datasets were collected by manual
translation of English descriptions from
the Flickr30K dataset. In this work, we
introduce MMDravi, a Multilingual Multi-
modal dataset for under-resourced Dravid-
ian languages. It comprises of 30,000 sen-
tences which were created utilizing several
machine translation outputs. Using data
from MMDravi and a phonetic transcrip-
tion of the corpus, we build an Multilingual
Multimodal Neural Machine Translation
system (MMNMT) for closely related Dra-
vidian languages to take advantage of mul-
tilingual corpus and other modalities. We
evaluate our translations generated by the
proposed approach with human-annotated
evaluation dataset in terms of BLEU, ME-
TEOR, and TER metrics. Relying on
multilingual corpora, phonetic transcrip-
tion, and image features, our approach im-
proves the translation quality for the under-
resourced languages.

1 Introduction

The development of a Multilingual Multimodal
Neural Machine Translation (MMNMT) system
requires multilingual parallel corpora and images

Â© 2019 The authors. This article is licensed under a Creative
Commons 4.0 licence, no derivative works, attribution, CC-
BY-ND.

which are aligned with the parallel sentences for
training. The largest existing dataset contain-
ing captions, images, and translations for English,
German, French and Czech is the WMT shared
task Multi30K dataset which is derived from the
Flickr30k dataset (Plummer et al., 2015; Plummer
et al., 2017). Typically this data is manually cre-
ated with the help of bilingual annotators (Elliott et
al., 2016), however, for many languages, such re-
sources are not available. In those cases, machine
translation can be a useful tool for the quick ex-
pansion to new languages by producing candidate
translation (Dutta Chowdhury et al., 2018). In or-
der to reduce the amount of time, we pose transla-
tion as a post-editing task. We automatically trans-
lated the English sentences from the WMT cor-
pus using a pre-trained general domain Statistical
Machine Translation (SMT) and Neural Machine
Translation (NMT).

Multilingual NMT models (Firat et al., 2016)
have been shown to increase the translation qual-
ity for under-resourced languages. Closely related
Dravidian languages such as Tamil (ISO-639-1:
ta), Kannada (ISO-639-1: kn), and Malayalam
(ISO-639-1: ml) exhibit a large overlap in their
vocabulary and strong syntactic and lexical simi-
larities. Dravidian languages are a family of lan-
guages spoken primarily in the southern part of
India and spread over South Asia and are con-
sidered as under-resourced languages. However,
the scripts used to write these languages are dif-
ferent and they differ in their morphology. Re-
cently Chakravarthi et al. (2019) have shown that
phonetic transcription of a corpus into Latin script
improves the multilingual NMT performance for
under-resourced Dravidian languages.

In this paper, we propose applying Multilingual
Multimodal NMT for translating between closely

LoResMT 2019 Dublin, Aug. 19-23, 2019 | p. 56



related Dravidian languages and English. We cre-
ated multimodal data using SMT and NMT meth-
ods, trained on a general domain corpus for under-
resourced languages. Combining multilingual and
multimodal data along with a phonetic transcrip-
tion of the corpus improves translation perfor-
mance for closely related Dravidian languages is
shown in the results.

2 Related Work

To capture rich information from multimodal con-
tent available on the Web, especially images with
descriptions in English was explored in the con-
tent of NMT (Specia et al., 2016a) in the WMT
shared task. The WMT shared task also pro-
vided resources for other popular languages Ger-
man, Czech and French (Elliott et al., 2017).
Most of those data were expensive, for example,
English-German corpus was created by Elliott et
al. (2016), and cost e23,000 for data collection
(e0.06 per word). Such resources are not available
for under-resourced languages. Recent work by
(Dutta Chowdhury et al., 2018), carried out experi-
ments by utilizing synthetic data for Hindi-English
language pair. In contrast, we created MMDravi as
a translation post-editing task by utilizing transla-
tions of the English sentences associated with im-
ages using SMT and NMT trained on general do-
main data.

The shared task on Multimodal NMT (MNMT)
was introduced by Specia et al. (2016b) to generate
image descriptions for a target language, given an
image and/or a description in the source language.
In previous works on MNMT, the researchers uti-
lized visual context by involving both NMT and
Image Description Generation (IDG) features that
explicitly uses an encoder-decoder (Cho et al.,
2014). However, the encoder-decoder architecture
encodes the source sentence into a fixed-length
vector. To overcome this drawback (Bahdanau et
al., 2015) introduced attention mechanism to fo-
cus on parts of the source sentence. The work by
Calixto and Liu (2017), carried out different ex-
periments to incorporate visual features into NMT
by projecting an image feature vector as words
into the source sentence, using the image to ini-
tialize the encoder hidden state, and using im-
age features to initialize the decoder hidden state.
In Calixto et al. (2017), the author incorporated
features through a separate encoder and doubly-
attentive attention of the decoder to depend on the

image feature. This allowed them to predict the
next word and showed that the image feature im-
proved the translation quality. Although all these
approaches have demonstrated the possibility of
MNMT, they rely on manually collected corpora
but under-resourced languages do not have such
resources. Our work follows the doubly-attentive
model (Calixto et al., 2017) with MMDravi data
for the multilingual model by phonetic transcrip-
tion.

In Ha et al. (2016) and Johnson et al. (2017), the
authors have demonstrated that multilingual NMT
improves translation quality. For this, they created
multilingual NMT without changing the architec-
ture by introducing special tokens at the beginning
of the source sentence indicating the source lan-
guage and target language as shown in Figure 1.
We follow this by introducing special tokens in
the source sentence to indicate the target language.
Phonetic transcription to Latin script and the In-
ternational Phonetic Alphabet (IPA) was studied
by (Chakravarthi et al., 2019) and showed that
Latin script outperforms IPA for the Multilingual
NMT of Dravidian languages. We propose to com-
bine multilingual, phonetic transcription and mul-
timodal content to improve the translation qual-
ity of under-resourced Dravidian languages. Our
contribution is to use the closely related languages
from the Dravidian language family to exploit the
similar syntax and semantic structures by phonetic
transcription of the corpora into Latin script along
with image feature to improve the translation qual-
ity.

3 Background

3.1 Dravidian Languages

Dravidian languages have individual writing
scripts and have been assigned a unique block in
the Unicode computing industry standard. The
similarity of these languages is that they are all
written from left to right, consist of sequences
of simple or complex characters and follow an
alpha-syllabic writing system in which the individ-
ual symbols are syllables (Bhanuprasad and Sven-
son, 2008). The languages also have different sets
of vowels and consonants. Vowels and conso-
nants are atomic but when they are combined with
each other they form consonant ligatures. Dravid-
ian languages such as Tamil do not represent dif-
ferences between aspirated and unaspirated stops,
while other Dravidian languages such as Kannada

LoResMT 2019 Dublin, Aug. 19-23, 2019 | p. 57



and Malayalam have a large number of loan words
from Indo-Aryan languages and support a large
number of compound characters resulting from the
combination of two consonants symbols (Kumar et
al., 2015).

3.2 Phonetic Transcription

Phonetic transcription is the use of phonetic sym-
bols such as IPA or non-native script. As the Dra-
vidian languages under study are written in dif-
ferent scripts, they must be converted to some
common representation before training the MM-
NMT to take advantage of closely related language
resources. Phonetic transcription to Latin script
and the International Phonetic Alphabet (IPA) was
studied by (Chakravarthi et al., 2019) and showed
that Latin script outperforms IPA for the Multi-
lingual NMT Dravidian languages. The improve-
ments in translation performance were shown in
terms of the BLEU (Papineni et al., 2002) met-
ric. We used the Indic-trans library by Bhat et al.
(2015) for phonetic transcription of corpora into
the Latin script, which brings all the languages
into a single representation by a phoneme match-
ing algorithm. The same library was used to back-
transliterate from Latin script to the corresponding
Dravidian language to evaluate the translation per-
formance.

3.3 Neural Machine Translation

Neural Machine Translation is a sequence-to-
sequence approach (Sutskever et al., 2014) us-
ing an encoder-decoder architecture with an atten-
tion mechanism (Bahdanau et al., 2015). Given
a source sentence X=x1, x2, x3,...xn and target
sentence Y =y1, y2, y3,...yn the bidirectional en-
coder transforms the source sentence into annota-
tion vectors C=h1, h2, h3,...hn. At each time step
t, the source context vector ct is computed based
on the annotation vector and the decoderâs previ-
ous hidden state stâ1. The decoder generates one
target word at a time by computing the probabil-
ity of P (yt = k|y<t, ct) given a hidden state st as
follows

P (yt = k|y<t, ct)
â exp(L0tanh(Lsst+

LwEy[ytâ1] + Lcct)) (1)

The L0,Ls,LW and Lc are transformation matri-
ces.

The attention model calculates ct as the
weighted sum of the source side context vectors:

ct =
Nâ

i=1

Î±srct,i hi (2)

Î±srct,i =
exp (esrct,i )âN
j=1 exp (e

src
t,j )

(3)

Î±srct,i is the normalized alignment matrix be-
tween each source annotation vector hi and word
yt to be emitted at a time step t. Expected align-
ment esrct,i between each source annotation vector
hi and the target word yt is computed using the
following formula:

esrct,i = (V
src
a )

T tanh(U srca s
â²
t +W

src
a hi) (4)

V srca , U
src
a and W

src
a are model parameters.

3.4 Multimodal Neural Machine Translation

The Multimodal NMT (MNMT) (Calixto et al.,
2017) model is an extension of the encoder-
decoder framework, by incorporating visual in-
formation. To incorporate the visual features ex-
tracted from the pre-trained model the authors have
integrated another attention mechanism to the de-
coder. The doubly-attentive decoder Recurrent
Neural Network is conditioned on the previous
hidden state, previously emitted word, source sen-
tence and the image via attention mechanism (Cal-
ixto et al., 2017). In the original attention-based
NMT model described in Section 3.3, a single en-
coder for the source sentence, a single decoder for
the target sentence and the attention mechanism
are conditioned on the source sentence. MNMT
integrates two separate attention mechanism over
the source language and visual features associated
with the source and target sentence. The decoder
generates a target word by computing a new prob-
ability P (yt = k|y<t, C,A) given a hidden state
st, the previously emitted word y<t, and the two
context vectors ct from encoder of source sentence
and it from image features.

P (yt = k|y<t, C,A)
â exp(L0tanh(Lsst+
LwEy[ytâ1] + Lcsct + Lciit)) (5)

L0, Ls, Lw, Ey, Lcs,and Lci are projection matri-
ces. The mechanism in MNMT is similar to NMT

LoResMT 2019 Dublin, Aug. 19-23, 2019 | p. 58



Figure 1: Example of sentences with special tokens to indicate the source and target languages.

with an attention model, except for the source sen-
tence and previous hidden state, it also takes the
context vector a from the image features using a
double attention layer to calculate the current hid-
den state. The doubly-attentive model calculates
the time-dependent vector it as follows:

it = Î²t

Lâ

l=1

Î±imgt,l al (6)

Where,
Î²t = Ï(WÎ²stâ1 + bÎ²) (7)

The expected alignment vector of image is given
by

Î±imgt,l =
exp (eimgt,l )âL
j=1 exp (e

img
t,j )

(8)

eimgt,l = (V
img
a )

T tanh(U imga s
â²
t +W

img
a al) (9)

V imga , U
img
a and W

img
a are model parameters.

Corpus Statistics
Lang pair sent s-tokens t-tokens

En-Ta 0.8M 6.4M 13.3M
En-Kn 0.5M 2.6M 4.5M
En-Ml 1.4M 16.7M 23.5M

Table 1: Statistics of the parallel corpora used to train the
general domain translation systems. sent: Number of sen-
tences, s-tokens: Number of source tokens, and t-tokens:
Number of target tokens.

4 Experimental Setup

4.1 Data

The images required for our work were collected
from Flicker by Plummer et al. (2015). The

BLEU Score
Lang pair SMT NMT

En-Ta 30.29 35.52
En-Kn 28.81 26.86
En-Ml 36.73 38.56

Table 2: Results of general domain SMT and NMT transla-
tion systems on general domain evaluation set

Multi30K dataset contains parallel corpora for En-
glish and German. There were two types of multi-
lingual annotations released by Multi30K dataset
(Elliott et al., 2016). The first one is an En-
glish description for each image and its German
translation. The second is a corpus of five in-
dependently collected English and German de-
scription pairs for each image. Synthetic data or
back-transliterated data have been widely used to
improve the performance of NMT and MNMT.
To produce a target side description of an im-
age, we create a general domain SMT and NMT
for English-Tamil, English-Kannada, and English-
Malayalam pairs. We collected the general do-
main parallel corpora for the Dravidian languages
from the OPUS website (Tiedemann and Nygaard,
2004) and (Chakravarthi et al., 2018). The corpus
statistics are shown in Table 1. The corpus is tok-
enized and standardized to lowercase. The general
domain SMT was created with Moses (Koehn et
al., 2007) while the NMT system was trained with
OpenNMT (Klein et al., 2017). After tokenization,
we fed the parallel corpora to Moses and Open-
NMT. Preprocessed files are then used to train the
models. We used the default OpenNMT parame-
ters for training, i.e. 2 layers LSTM with 500 hid-
den units for both, the encoder and decoder.

The SMT and NMT system results on general

LoResMT 2019 Dublin, Aug. 19-23, 2019 | p. 59



Figure 2: Example of sentence and image with candidate
translation to choose.

domain evaluation set are shown in Table 2. The
development and test set of the multimodal corpus
was collected with the help of volunteer annota-
tors. To reduce the annotation time, we posed the
translation task of the development and test set as a
post-editing task. We provided the candidate trans-
lation of the English sentence from SMT, NMT,
and an option to choose the best translation or pro-
vide an original translation. Eighteen annotators
participated in this annotation process, with dif-
ferent backgrounds, they all are native speakers of
the language that they annotated. The data for the
Malayalam language was collected from three dif-
ferent native speakers. Ten Tamil native speakers
participated in creating data for the Tamil language
and five Kannada native speakers annotated for
the Kannada language. Since voluntary annotators
are scarce and annotate little data, each sentence
was annotated by only one annotator. We then se-
lected the system that performed better based on
the choice of annotators. We designed an annota-
tion tool to meet the objective of method. We de-
cided to use Google Forms to collect the data from
the voluntary annotatorâs. An example is shown
in Figure 2. We chose NMT and used the gen-
eral domain NMT to post-edit the translation for
the training set of MMDravi.

For our tasks, all descriptions in English were
converted to lowercase and tokenized, while we

Table 3: Results are expressed in BLEU score: Baseline is
Multimodal NMT, MMNMT is trained on native script, and
MMNMT-T is trained utilizing phonetic transcription.

BLEUscore
Lang pair Baseline MMNMT MMNMT-T

En-Ta 50.2 51.0 52.3
En-Ml 35.6 36.0 36.5
En-Kn 44.5 45.1 45.9
Ta-En 45.2 47.4 48.9
Ml-En 34.3 36.2 37.6
Kn-En 50.0 50.2 50.8

did not have to bother about the case correction for
Dravidian languages (as they do not have cases).
We tokenized the Dravidian language using the
OpenNMT tokenizer with segment alphabet op-
tions for Tamil, Kannada, and Malayalam. For
the sub-word level representation, we chose the
10,000 most frequent units to train the BPE (Sen-
nrich et al., 2016) model. We used this model
for the sub-word level segmentation for the train-
ing, development, and evaluation set. We trained
the MMNMT model to translate from English into
Dravidian languages as well as from Dravidian
languages into English. Visual features were ex-
tracted from publicly available pre-trained CNNâs.
Specifically, we extract spatial image features us-
ing the VGG-19 network (Simonyan and Zisser-
man, 2014). In our experiment, we pass all the im-
ages in our dataset through the pre-trained VGG-
19 layered network to extract global information
and use them in a separate visual attention mecha-
nism as described in Calixto et al. (2017).

4.2 Multilingual Multimodal Neural Machine
Translation

Since we translate between closely related lan-
guages and English, we set up the translation set-
ting in two scenarios, 1) One-to-Many and 2)
Many-to-One.

4.2.1 One-to-Many Approach

In this setting, we create a model to translate
from English into Tamil, Malayalam, and Kan-
nada. The source language sentence was replicated
three times for the three languages with a token in-
dicating target language. Figure 1 shows the ex-
ample of sentences.

LoResMT 2019 Dublin, Aug. 19-23, 2019 | p. 60



src a black dog runs on green grass with a toy 

in his mouth .

ref à®à®°à¯ à®à®°à¯à®ªà¯à®ªà¯ à®¨à®¾à®¯à¯ à®µà®¾à®¯à®¿à®²à¯ à®à®°à¯

à®ª à®¾à®®à¯à®®à¯à®¯à¯à®à®©à¯ à®à¯à®®à¯ à®ªà¯à®²à¯à®²à®¿à®²à¯

à®à®à¯à®à®¿à®±à®¤à¯.

MMNMT à®à®°à¯ à®à®°à¯à®ªà¯à®ªà¯ à®¨à®¾à®¯à¯ à®µà®¾à®¯à®¿à®²à¯ à®à®°à¯

à®ª à®¾à®®à¯à®®à¯à®¯à¯à®à®©à¯ à®à¯à®®à¯ à®ªà¯à®²à¯ à®®à¯à®¤à¯à®à®¯

à®à¯à®à¯à®®à¯ .

MMNMT

-T

à®à®°à¯ à®à®°à¯à®ªà¯à®ªà¯ à®¨à®¾à®¯à¯ à®µà®¾à®¯à®¿à®²à¯ à®à®°à¯

à®ª à®¾à®®à¯à®®à¯à®¯à¯à®à®©à¯ à®à¯à®®à¯ à®ªà¯à®²à¯ à®®à¯à®¤à¯ à®à®à¯à®à®¿à®±à®¤.

Figure 3: Example showing improvement of translation quality and readability of the translation over baseline model. Errors
are shown in red color.

src a woman and two men , that are dressed 

professionally, are having a discussion.

ref à®à®°à¯ à®ªà¯à®£à¯ à®®à®±à¯à®±à¯à®®à¯à®à®°à®£à¯à®à¯à®à®£à¯à®à®³à¯, 

à®ª à¯à®´à®¿à®²à¯à®®à¯à®±à¯ à®à®±à¯à®¯à®£à®¿à®¨à¯à®¤à¯, à®à®°à¯

à®µà®¿à®µà¯ à®¤à¯à®¤à®¿à®²à¯ à®à¯à®³à¯à®°à¯à®à®³à¯.

MMNMT à®à®°à¯ à®ªà¯à®£à¯ , à®®à®±à¯à®±à¯à®®à¯à®à®°à¯à®à®£à¯à®à®³à¯

professionally à®à®¯à®¤à¯ à®®à¯à¯à®£à¯à®£à®¿ , à®à®°à¯

à®µà®¿à®µà¯ à®¤à¯à®¤à®¿à®±à¯ à®µà®µà®£à¯à®à®¿à®µ à¯à®®à¯ .

MMNMT

-T

à®à®°à¯ à®ªà¯à®£à¯ à®®à®±à¯à®±à¯à®®à¯à®à®°à®£à¯à®à¯à®à®£à¯à®à®³à¯, 

professional à®à®±à¯à®¯à®£à®¿à®¨à¯à®¤à¯, à®à®°à¯

à®µà®¿à®µà¯ à®¤à¯à®¤à®¿à®²à¯ à®à¯à®³à¯à®°.

Figure 4: Example showing translation with accurate transfer of important information. Errors are shown in red color.

4.2.2 Many-to-One Approach

In the many-to-one MMNMT system, we cre-
ate a model to translate from Tamil, Malayalam,
and Kannada (Dravidian languages) to English.
We replicated the English sentence three times for
three languages on the target side of the corpus.
We then train the MNMT system with a visual fea-
ture for individual language level with the MM-
Dravi data. We compared the results with the MM-
NMT for one-to-many and many-to-one models.

4.3 Results

We applied the baseline bilingual Multimodal
NMT systems with respect to the MMDravi data
created from the Multi30k dataset. Then we
trained our MMNMT and MMNMT-T (phonetic
transcription of corpus) for English into Dravidian
languages and vice versa. Results are presented in
BLEU (Papineni et al., 2002) (BiLingual Evalua-
tion Understudy), which measures the n-gram pre-
cision with respect to the evaluation set.

Table 3 provides the BLUE scores for the MM-
NMT model. We observed that the translation
performance of MMNMT is higher compared to

the Bilingual Multimodal NMT model in BLEU.
Translation from Dravidian to English has the
highest improvement in terms of BLEU Score.
Our experiments show that the MMNMT system
compared with the bilingual system has an im-
provement in several language directions, which
are likely gained from phonetic transcription, im-
age features, and transfer of parameters from dif-
ferent languages.

The results show that for MMNMT with phonet-
ically transcribed corpora, helps more in Dravidian
to English than English to Dravidian. An explana-
tion for this is that in the dataset, each source sen-
tence has three targets, which encourages the lan-
guage model to improve the translation results. In
Table 3, we compare the BLEU scores with a base-
line approach and our method. In order to evaluate
the effectiveness of our proposed model, we have
explored MMNMT trained on original scripts and
MMNMT trained on a single script. Our empirical
results show that the best result is achieved when
we phonetically transcribed the corpus and brought
it to a single script for both English to Dravidian
and Dravidian to English translation tasks.

LoResMT 2019 Dublin, Aug. 19-23, 2019 | p. 61



Figure 3 shows the examples of where the MM-
NMT model improves the translation quality and
readability of the translation over the baseline
model. The results given by the human evalu-
ation confirm the results observed in evaluation
BLEU metric. The second example for English-
Tamil translation of MMNMT system outperform-
ing the baseline is shown in Figure 4. The first
example showns an almost perfect translation ob-
tained with the MMNMT system for English to
Tamil. In the second example, translation obtained
with the MMNMT system is acceptable with the
accurate transfer of important information (Cough-
lin, 2003). This suggests the synthetic data with
our MMNMT model can be used in an under-
resourced language setting to improve the transla-
tion quality.

5 Conclusion

We introduced a new dataset, named MMDravi
and proposed a MMNMT method for closely re-
lated Dravidian languages to overcome the re-
source issues. Compared to the baseline ap-
proach, the results show that our approach can im-
prove translation quality, especially for Dravidian
languages. Our evaluation, using phonetic tran-
scription, multilingual and multimodal NMT, has
shown that the proposed MMNMT-T outperforms
the existing approach of multimodal, multilingual
in low-resource neural machine translation across
all the language pairs considered. We plan to re-
lease multilingual translations as an addition to
Flickr30k set, and explore the effect of the qual-
ity of this synthetic data in our future work.

Acknowledgments

This work is supported by a research grant from
Science Foundation Ireland, co-funded by the Eu-
ropean Regional Development Fund, for the In-
sight Centre under Grant Number SFI/12/RC/2289
and the European Unionâs Horizon 2020 research
and innovation programme under grant agreement
No 731015, ELEXIS - European Lexical Infras-
tructure and grant agreement No 825182, PreÌt-aÌ-
LLOD.

References
Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua

Bengio. 2015. Neural machine translation by
jointly learning to align and translate. In 3rd Inter-
national Conference on Learning Representations,

ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Bhanuprasad, Kamadev and Mats Svenson. 2008. Er-
rgrams â a way to improving ASR for highly in-
flected Dravidian languages. In Proceedings of
the Third International Joint Conference on Natural
Language Processing: Volume-II.

Bhat, Irshad Ahmad, Vandan Mujadia, Aniruddha Tam-
mewar, Riyaz Ahmad Bhat, and Manish Shrivastava.
2015. IIIT-H System Submission for FIRE2014
Shared Task on Transliterated Search. In Proceed-
ings of the Forum for Information Retrieval Evalua-
tion, FIRE â14, pages 48â53, New York, NY, USA.
ACM.

Calixto, Iacer and Qun Liu. 2017. Incorporating global
visual features into attention-based neural machine
translation. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Process-
ing, pages 992â1003. Association for Computational
Linguistics.

Calixto, Iacer, Qun Liu, and Nick Campbell. 2017.
Doubly-attentive decoder for multi-modal neural
machine translation. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1913â
1924. Association for Computational Linguistics.

Chakravarthi, Bharathi Raja, Mihael Arcan, and John P.
McCrae. 2018. Improving Wordnets for Under-
Resourced Languages Using Machine Translation.
In Proceedings of the 9th Global WordNet Confer-
ence. The Global WordNet Conference 2018 Com-
mittee.

Chakravarthi, Bharathi Raja, Mihael Arcan, and John P.
McCrae. 2019. Comparison of Different Orthogra-
phies for Machine Translation of Under-resourced
Dravidian Languages. In Proceedings of the 2nd
Conference on Language, Data and Knowledge.

Cho, Kyunghyun, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoderâdecoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724â
1734, Doha, Qatar, October. Association for Com-
putational Linguistics.

Coughlin, Deborah. 2003. Correlating automated and
human assessments of machine translation quality.
In In Proceedings of MT Summit IX, pages 63â70.

Dutta Chowdhury, Koel, Mohammed Hasanuzzaman,
and Qun Liu. 2018. Multimodal neural machine
translation for low-resource language pairs using
synthetic data. In Proceedings of the Workshop on
Deep Learning Approaches for Low-Resource NLP,
pages 33â42, Melbourne, July. Association for Com-
putational Linguistics.

LoResMT 2019 Dublin, Aug. 19-23, 2019 | p. 62



Elliott, Desmond, Stella Frank, Khalil Simaâan, and Lu-
cia Specia. 2016. Multi30K: Multilingual English-
German Image Descriptions. In Proceedings of the
5th Workshop on Vision and Language, pages 70â74.
Association for Computational Linguistics.

Elliott, Desmond, Stella Frank, LoÄ±Ìc Barrault, Fethi
Bougares, and Lucia Specia. 2017. Findings of
the Second Shared Task on Multimodal Machine
Translation and Multilingual Image Description. In
Proceedings of the Second Conference on Machine
Translation, pages 215â233. Association for Com-
putational Linguistics.

Firat, Orhan, Kyunghyun Cho, and Yoshua Bengio.
2016. Multi-way, multilingual neural machine trans-
lation with a shared attention mechanism. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
866â875. Association for Computational Linguis-
tics.

Ha, Thanh-Le, Jan Niehues, and Alexander H. Waibel.
2016. Toward multilingual neural machine trans-
lation with universal encoder and decoder. CoRR,
abs/1611.04798.

Johnson, Melvin, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda VieÌgas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Googleâs
multilingual neural machine translation system: En-
abling zero-shot translation. Transactions of the As-
sociation for Computational Linguistics, 5:339â351,
December.

Klein, Guillaume, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander Rush. 2017. OpenNMT:
Open-Source Toolkit for Neural Machine Transla-
tion. In Proceedings of ACL 2017, System Demon-
strations, pages 67â72. Association for Computa-
tional Linguistics.

Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177â180. Association for Computa-
tional Linguistics.

Kumar, Arun, LluÄ±Ìs PadroÌ, and Antoni Oliver. 2015.
Joint Bayesian morphology learning for Dravidian
languages. In Proceedings of the Joint Workshop
on Language Technology for Closely Related Lan-
guages, Varieties and Dialects, pages 17â23, Hissar,
Bulgaria, September. Association for Computational
Linguistics.

Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic

Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.

Plummer, Bryan A., Liwei Wang, Chris M. Cervantes,
Juan C. Caicedo, Julia Hockenmaier, and Svetlana
Lazebnik. 2015. Flickr30k entities: Collecting
region-to-phrase correspondences for richer image-
to-sentence models. In Proceedings of the 2015
IEEE International Conference on Computer Vision
(ICCV), ICCV â15, pages 2641â2649, Washington,
DC, USA. IEEE Computer Society.

Plummer, Bryan A., Liwei Wang, Chris M. Cervantes,
Juan C. Caicedo, Julia Hockenmaier, and Svet-
lana Lazebnik. 2017. Flickr30k entities: Col-
lecting region-to-phrase correspondences for richer
image-to-sentence models. Int. J. Comput. Vision,
123(1):74â93, May.

Sennrich, Rico, Barry Haddow, and Alexandra Birch.
2016. Neural Machine Translation of Rare Words
with Subword Units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715â
1725. Association for Computational Linguistics.

Simonyan, Karen and Andrew Zisserman. 2014. Very
deep convolutional networks for large-scale image
recognition. CoRR, abs/1409.1556.

Specia, Lucia, Stella Frank, Khalil Simaâan, and
Desmond Elliott. 2016a. A Shared Task on Multi-
modal Machine Translation and Crosslingual Image
Description. In Proceedings of the First Conference
on Machine Translation: Volume 2, Shared Task Pa-
pers, pages 543â553. Association for Computational
Linguistics.

Specia, Lucia, Stella Frank, Khalil Simaâan, and
Desmond Elliott. 2016b. A shared task on mul-
timodal machine translation and crosslingual im-
age description. In Proceedings of the First Con-
ference on Machine Translation, pages 543â553,
Berlin, Germany, August. Association for Compu-
tational Linguistics.

Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
In Proceedings of the 27th International Conference
on Neural Information Processing Systems - Vol-
ume 2, NIPSâ14, pages 3104â3112, Cambridge, MA,
USA. MIT Press.

Tiedemann, JoÌrg and Lars Nygaard. 2004. The OPUS
Corpus - Parallel and Free: http://logos.uio.no/opus.
In Proceedings of the Fourth International Con-
ference on Language Resources and Evaluation
(LRECâ04). European Language Resources Associ-
ation (ELRA).

LoResMT 2019 Dublin, Aug. 19-23, 2019 | p. 63


