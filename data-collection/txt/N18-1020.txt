



















































Zero-Shot Question Generation from Knowledge Graphs for Unseen Predicates and Entity Types


Proceedings of NAACL-HLT 2018, pages 218–228
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Zero-Shot Question Generation from Knowledge Graphs for Unseen
Predicates and Entity Types

Hady Elsahar, Christophe Gravier, Frederique Laforest
Université de Lyon

Laboratoire Hubert Curien
Saint-Étienne, France

{firstname.lastname}@univ-st-etienne.fr

Abstract

We present a neural model for question gener-
ation from knowledge base triples in a “Zero-
Shot” setup, that is generating questions for
triples containing predicates, subject types or
object types that were not seen at training
time. Our model leverages triples occurrences
in the natural language corpus in an encoder-
decoder architecture, paired with an original
part-of-speech copy action mechanism to gen-
erate questions. Benchmark and human evalu-
ation show that our model sets a new state-of-
the-art for zero-shot QG.

1 Introduction

Questions Generation (QG) from Knowledge
Graphs is the task consisting in generating natural
language questions given an input knowledge
base (KB) triple (Serban et al., 2016). QG from
knowledge graphs has shown to improve the
performance of existing factoid question answer-
ing (QA) systems either by dual training or by
augmenting existing training datasets (Dong et al.,
2017; Khapra et al., 2017). Those methods rely
on large-scale annotated datasets such as Simple-
Questions (Bordes et al., 2015). Building such
datasets is a tedious task in practice, especially
to obtain an unbiased dataset – i.e. a dataset that
covers equally a large amount of triples in the KB.
In practice many of the predicates and entity types
in KB are not covered by those annotated datasets.
For example 75.6% of Freebase predicates are
not covered by the SimpleQuestions dataset 1.
Among those we can find important missing
predicates such as: fb:food/beer/country,
fb:location/country/national anthem,
fb:astronomy/star system/stars.

One challenge for QG from knowledge graphs
is to adapt to predicates and entity types that

1replicate the observation http://bit.ly/2GvVHae

were not seen at training time (Zero-Shot Ques-
tion Generation). Since state-of-the-art systems in
factoid QA rely on the tremendous efforts made
to create SimpleQuestions, these systems can only
process questions on the subset of 24.4% of free-
base predicates defined in SimpleQuestions. Pre-
vious works for factoid QG (Serban et al., 2016)
claims to solve the issue of small size QA datasets.
However encountering an unseen predicate / entity
type will generate questions made out of random
text generation for those out-of-vocabulary predi-
cates a QG system had never seen. We go beyond
this state-of-the-art by providing an original and
non-trivial solution for creating a much broader
set of questions for unseen predicates and entity
types. Ultimately, generating questions to predi-
cates and entity types unseen at training time will
allow QA systems to cover predicates and entity
types that would not have been used for QA other-
wise.

Intuitively, a human who is given the task to
write a question on a fact offered by a KB, would
read natural language sentences where the entity
or the predicate of the fact occur, and build up
questions that are aligned with what he reads from
both a lexical and grammatical standpoint. In this
paper, we propose a model for Zero-Shot Question
Generation that follows this intuitive process. In
addition to the input KB triple, we feed our model
with a set of textual contexts paired with the input
KB triple through distant supervision. Our model
derives an encoder-decoder architecture, in which
the encoder encodes the input KB triple, along
with a set of textual contexts into hidden represen-
tations. Those hidden representations are fed to a
decoder equipped with an attention mechanism to
generate an output question.
In the Zero-Shot setup, the emergence of new
predicates and new class types during test time re-
quires new lexicalizations to express these pred-

218



icates and classes in the output question. These
lexicalizations might not be encountered by the
model during training time and hence do not ex-
ist in the model vocabulary, or have been seen
only few times not enough to learn a good rep-
resentation for them by the model. Recent works
on Text Generation tackle the rare words/unknown
words problem using copy actions (Luong et al.,
2015; Gülçehre et al., 2016): words with a spe-
cific position are copied from the source text to
the output text – although this process is blind to
the role and nature of the word in the source text.
Inspired by research in open information extrac-
tion (Fader et al., 2011) and structure-content neu-
ral language models (Kiros et al., 2014), in which
part-of-speech tags represent a distinctive feature
when representing relations in text, we extend
these positional copy actions. Instead of copying
a word in a specific position in the source text, our
model copies a word with a specific part-of-speech
tag from the input text – we refer to those as part-
of-speech copy actions. Experiments show that
our model using contexts through distant supervi-
sion significantly outperforms the strongest base-
line among six (+2.04 BLEU-4 score). Adding
our copy action mechanism further increases this
improvement (+2.39). Additionally, a human
evaluation complements the comprehension of our
model for edge cases; it supports the claim that the
improvement brought by our copy action mecha-
nism is even more significant than what the BLEU
score suggests.

2 Related Work

QG became an essential component in many ap-
plications such as education (Heilman and Smith,
2010), tutoring (Graesser et al., 2004; Evens
and Michael, 2006) and dialogue systems (Shang
et al., 2015). In our paper we focus on the prob-
lem of QG from structured KB and how we can
generalize it to unseen predicates and entity types.
(Seyler et al., 2015) generate quiz questions from
KB triples. Verbalization of entities and predi-
cates relies on their existing labels in the KB and a
dictionary. (Serban et al., 2016) use an encoder-
decoder architecture with attention mechanism
trained on the SimpleQuestions dataset (Bordes
et al., 2015). (Dong et al., 2017) generate para-
phrases of given questions to increases the per-
formance of QA systems; paraphrases are gener-
ated relying on paraphrase datasets, neural ma-

chine translation and rule mining. (Khapra et al.,
2017) generate a set of QA pairs given a KB en-
tity. They model the problem of QG as a sequence
to sequence problem by converting all the KB en-
tities to a set of keywords. None of the previous
work in QG from KB address the question of gen-
eralizing to unseen predicates and entity types.
Textual information has been used before in the
Zero-Shot learning. (Socher et al., 2013) use infor-
mation in pretrained word vectors for Zero-Shot
visual object recognition. (Levy et al., 2017) in-
corporates a natural language question to the rela-
tion query to tackle Zero-Shot relation extraction
problem.

Previous work in machine translation dealt with
rare or unseen word problem problem for trans-
lating names and numbers in text. (Luong et al.,
2015) propose a model that generates positional
placeholders pointing to some words in source
sentence and copy it to target sentence (copy ac-
tions). (Gülçehre et al., 2016; Gu et al., 2016)
introduce separate trainable modules for copy ac-
tions to adapt to highly variable input sequences,
for text summarization. For text generation from
tables, (Lebret et al., 2016) extend positional copy
actions to copy values from fields in the given ta-
ble. For QG, (Serban et al., 2016) use a place-
holder for the subject entity in the question to gen-
eralize to unseen entities. Their work is limited to
unseen entities and does not study how they can
generalize to unseen predicates and entity types.

3 Model

Let F = {s, p, o} be the input fact provided to
our model consisting of a subject s, a predicate
p and an object o, and C be the set of textual
contexts associated to this fact. Our goal is to
learn a model that generates a sequence of T to-
kens Y = y1, y2, . . . , yT representing a question
about the subject s, where the object o is the cor-
rect answer. Our model approximates the condi-
tional probability of the output question given an
input fact p(Y |F ), to be the probability of the out-
put question, given an input fact and the additional
textual context C, modelled as follows:

p(Y |F ) =
T∏

t=1

p(yt|y<t, F, C) (1)

where y<t represents all previously generated to-
kens until time step t. Additional textual contexts
are natural language representation of the triples

219



Figure 1: The proposed model for Question Generation. The model consists of a single fact encoder and n textual
context encoders, each consists of a separate GRU. At each time step t, two attention vectors generated from the
two attention modules are fed to the decoder to generate the next word in the output question.

that can be drawn from a corpus – our model is
generic to any textual contexts that can be ad-
ditionally provided, though we describe in Sec-
tion 4.1 how to create such texts from Wikipedia.

Our model derives the encoder-decoder archi-
tecture of (Sutskever et al., 2014; Bahdanau et al.,
2014) with two encoding modules: a feed forward
architecture encodes the input triple (sec. 3.1) and
a set of recurrent neural network (RNN) to en-
code each textual context (sec. 3.2). Our model
has two attention modules (Bahdanau et al., 2014):
one acts over the input triple and another acts over
the input textual contexts (sec. 3.4). The decoder
(sec. 3.3) is another RNN that generates the output
question. At each time step, the decoder chooses
to output either a word from the vocabulary or a
special token indicating a copy action (sec. 3.5)
from any of the textual contexts.

3.1 Fact Encoder

Given an input fact F = {s, p, o}, let each of es,
ep and eo be a 1-hot vectors of size K. The fact
encoder encodes each 1-hot vector into a fixed size
vector hs = Ef es, hp = Ef ep and ho = Ef eo,
where Ef ∈ RHk×K is the KB embedding matrix,
Hk is the size of the KB embedding and K is the
size of the KB vocabulary. The encoded fact hf ∈
R3Hk represents the concatenation of those three
vectors and we use it to initialize the decoder.

hf = [hs; hp; ho] (2)

Following (Serban et al., 2016), we learn Ef using
TransE (Bordes et al., 2015). We fix its weights
and do not allow their update during training time.

3.2 Textual Context Encoder
Given a set of n textual contexts C =
{c1, c2, . . . , cn : cj = (xj1, xj2, . . . , xj|cj |)}, where
xji represents the 1-hot vector of the i

th token in
the jth textual context cj , and |cj | is the length of
the jth context. We use a set of n Gated Recur-
rent Neural Networks (GRU) (Cho et al., 2014) to
encode each of the textual concepts separately:

h
cj
i = GRUj

(
Ec x

j
i , h

cj
i−1
)

(3)

where hcji ∈ RHc is the hidden state of the GRU
that is equivalent to xji and of size Hc . Ec is the
input word embedding matrix. The encoded con-
text represents the encoding of all the textual con-
texts; it is calculated as the concatenation of all the
final states of all the encoded contexts:

hc = [h
c1
|c1|;h

c2
|c2|; . . . ;h

cn
|cn|]. (4)

3.3 Decoder
For the decoder we use another GRU with an
attention mechanism (Bahdanau et al., 2014), in
which the decoder hidden state st ∈ RHd at each
time step t is calculated as:

st = zt ◦ st−1 + (1− zt) ◦ s̃t , (5)

Where:

s̃t = tanh
(
WEwyt−1 + U [rt ◦ st−1] +A [aft ; act ]

)
(6)

zt = σ
(
Wz Ew yt−1 + Uz st−1 +Az [a

f
t ; a

c
t ]
)

(7)

rt = σ
(
Wr Ew yt−1 + Ur st−1 +Ar [a

f
t ; a

c
t ]
)

(8)

W,Wz,Wr ∈ Rm×Hd , U,Uz, Ur, A,Az, Ar ∈
RHd×Hd are learnable parameters of the GRU.

220



Ew ∈ Rm×V is the word embedding matrix, m is
the word embedding size and Hd is the size of the
decoder hidden state. aft , a

c
t are the outputs of the

fact attention and the context attention modules
respectively, detailed in the following subsection.
In order to enforce the model to pair output words
with words from the textual inputs, we couple the
word embedding matrices of both the decoder Ew
and the textual context encoder Ec (eq.(3)). We
initialize them with GloVe embeddings (Penning-
ton et al., 2014) and allow the network to tune
them.
The first hidden state of the decoder s0 = [hf ; hc]
is initialized using a concatenation of the encoded
fact (eq.(2)) and the encoded context (eq.(4)) .
At each time step t, after calculating the hidden
state of the decoder, the conditional probability
distribution over each token yt of the generated
question is computed as the softmax(Wo st)
over all the entries in the output vocabulary,
Wo ∈ RHd×V is the weight matrix of the output
layer of the decoder.

3.4 Attention

Our model has two attention modules:
Triple attention over the input triple to determine
at each time step t an attention-based encoding of
the input fact aft ∈ RHk :

aft = αs,t hs + αp,t hp + αs,t ho , (9)

αs,t, αp,t, αo,t are scalar values calculated by the
attention mechanism to determine at each time
step which of the encoded subject, predicate, or
object the decoder should attend to.
Textual contexts attention over all the hidden
states of all the textual contexts act ∈ RHc :

act =

|C|∑

i=1

|ci|∑

j=1

αcit,j h
ci
j , (10)

αcit,j is a scalar value determining the weight of the
jth word in the ith context ci at time step t.

Given a set of encoded input vectors I =
{h1, h2, ...hk} and the decoder previous hidden
state st−1, the attention mechanism calculates
αt = αi,t, . . . , αk,t as a vector of scalar weights,
each αi,t determines the weight of its correspond-

What caused the [C1 NOUN] of the [C3 NOUN] [S] ?

C1 [S] death by [O]

[S] [C1 NOUN] [C1 ADP] [O]

C2 Disease

[C2 NOUN]

C3 Musical artist

[C3 ADJ] [C3 NOUN]

Table 1: An annotated example of part-of-speech copy
actions from several input textual contexts (C1, C2,
C3), the words or placeholders in bold are copied in
the generated question

ing encoded input vector hi.

ei,t = va
> tanh(Wa st−1 +Ua hi) (11)

αi,t =
exp (ei,t)∑k
j=1 exp (ej,t)

, (12)

where va,Wa,Ua are trainable weight matrices
of the attention modules. It is important to no-
tice here that we encode each textual context sep-
arately using a different GRU, but we calculate an
overall attention over all tokens in all textual con-
texts: at each time step the decoder should ideally
attend to only one word from all the input contexts.

3.5 Part-Of-Speech Copy Actions
We use the method of (Luong et al., 2015) by
modeling all the copy actions on the data level
through an annotation scheme. This method treats
the model as a black box, which makes it adapt-
able to any text generation model. Instead of using
positional copy actions, we use the part-of-speech
information to decide the alignment process be-
tween the input and output texts to the model.
Each word in every input textual context is re-
placed by a special token containing a combina-
tion of its context id (e.g. C1) and its POS tag
(e.g. NOUN). Then, if a word in the output question
matches a word in a textual context, it is replaced
with its corresponding tag as shown in Table 1.
Unlike (Serban et al., 2016; Lebret et al., 2016)
we model the copy actions in the input and the
output levels. Our model does not have the draw-
back of losing the semantic information when re-
placing words with generic placeholders, since we
provide the model with the input triple through the
fact encoder. During inference the model chooses
to either output words from the vocabulary or spe-
cial tokens to copy from the textual contexts. In

221



a post-processing step those special tokens are re-
placed with their original words from the textual
contexts.

4 Textual contexts dataset

As a source of question paired with KB triples
we use the SimpleQuestions dataset (Bordes et al.,
2015). It consists of 100K questions with their
corresponding triples from Freebase, and was cre-
ated manually through crowdsourcing. When
asked to form a question from an input triple,
human annotators usually tend to mainly fo-
cus on expressing the predicate of the input
triple. For example, given a triple with the pred-
icate fb:spacecraft/manufacturer the
user may ask ”What is the manufacturer of [S]
?”. Annotators may specify the entity type of the
subject or the object of the triple: ”What is the
manufacturer of the spacecraft [S]?” or ”Which
company manufactures [S]?”. Motivated by this
example we chose to associate each input triple
with three textual contexts of three different types.
The first is a phrase containing lexicalization of the
predicate of the triple. The second and the third are
two phrases containing the entity type of the sub-
ject and the object of the triple. In what follows we
show the process of collection and preprocessing
of those textual contexts.

4.1 Collection of Textual Contexts

We extend the set of triples given in the Sim-
pleQuestions dataset by using the FB5M (Bordes
et al., 2015) subset of Freebase. As a source of
text documents, we rely on Wikipedia articles.

Predicate textual contexts: In order to collect
textual contexts associated with the SimpleQues-
tions triples, we follow the distant supervision
setup for relation extraction (Mintz et al., 2009).
The distant supervision assumption has been ef-
fective in creating training data for relation extrac-
tion and shown to be 87% correct (Riedel et al.,
2010) on Wikipedia text.
First, we align each triple in the FB5M KB to sen-
tences in Wikipedia if the subject and the object
of this triple co-occur in the same sentence. We
use a simple string matching heuristic to find en-
tity mentions in text2. Afterwards we reduce the

2 We map Freebase entities to Wikidata through the Wiki-
data property P646, then we extract their labels and aliases.
We use the Wikidata truthy dump: https://dumps.
wikimedia.org/wikidatawiki/entities/

Freebase Relation Predicate Textual Context

person/place of birth [O] is birthplace of [S]
currency/former countries [S] was currency of [O]
dish/cuisine [O] dish [S]
airliner accident/flight origin[S] was flight from [O]
film featured song/performer[S] is release by [O]
airline accident/operator [S] was accident for [O]
genre/artists [S] became a genre of [O]
risk factor/diseases [S] increases likelihood of [O]
book/illustrations by [S] illustrated by [O]
religious text/religion [S] contains principles of [O]
spacecraft/manufacturer [S] spacecraft developed by [O]

Table 2: Table showing an example of textual contexts
extracted for freebase predicates

sentence to the set of words that appear on the de-
pendency path between the subject and the object
mentions in the sentence. We replace the posi-
tions of the subject and the object mentions with
[S] and [O] to the keep track of the information
about the direction of the relation. The top occur-
ring pattern for each predicate is associated to this
predicate as its textual context. Table 2 shows ex-
amples of predicates and their corresponding tex-
tual context.

Sub-Type and Obj-Type textual contexts: We
use the labels of the entity types as the sub-type
and obj-type textual contexts. We collect the list of
entity types of each entity in the FB5M through the
predicate fb:type/instance. If an entity has
multiple entity types we pick the entity type that
is mentioned the most in the first sentence of each
Wikipedia article. Thus the textual contexts will
opt for entity types that is more natural to appear
in free text and therefore questions.

4.2 Generation of Special tokens

To generate the special tokens for copy ac-
tions (sec. 3.5) we run POS tagging on each of the
input textual contexts3. We replace every word in
each textual context with a combination of its con-
text id (e.g. C1) and its POS tag (e.g. NOUN). If
the same POS tag appears multiple times in the
textual context, it is given an additional id (e.g.
C1 NOUN 2). If a word in the output question
overlaps with a word in the input textual context,
this word is replaced by its corresponding tag.
For sentence and word tokenization we use the
Regex tokenizer from the NLTK toolkit (Bird,
2006), and for POS tagging and dependency pars-

3For the predicate textual contexts we run pos tagging on
the original text not the lexicalized dependency path

222



Train Valid Test
pr

ed

# pred 169.4 24.2 48.4
# samples 55566.7 7938.1 15876.2
% samples 70.0 ± 2.77 10.0 ± 1.236 20.0 ± 2.12

su
b-

ty
pe

s

# types 112.7 16.1 32.2
# samples 60002.6 8571.8 17143.6
% samples 70.0 ± 7.9 10.0 ± 3.6 20.0 ± 6.2

ob
j-

ty
pe

s

# types 521.6 189.9 282.2
# samples 57878.1 8268.3 16536.6
% samples 70.0 ± 4.7 10.0 ± 2.5 20.0 ± 3.8

Table 3: Dataset statistics across 10 folds for each ex-
periment

ing we use the Spacy4 implementation.

5 Experiments

5.1 Zero-Shot Setups

We develop three setups that follow the same pro-
cedure as (Levy et al., 2017) for Zero-Shot relation
extraction to evaluate how our model generalizes
to: 1) unseen predicates, 2) unseen sub-types and
3) unseen obj-types.
For the unseen predicates setup we group all
the samples in SimpleQuestions by the predicate
of the input triple, and keep groups that con-
tain at least 50 samples. Afterwards we ran-
domly split those groups to 70% train, 10%
valid and 20% test mutual exclusive sets re-
spectively. This guarantees that if the predi-
cate fb:person/place of birth for exam-
ple shows during test time, the training and vali-
dation set will not contain any input triples hav-
ing this predicate. We repeat this process to create
10 cross validation folds, in our evaluation we re-
port the mean and standard deviation results across
those 10 folds. While doing this we make sure
that the number of samples in each fold – not only
unique predicates – follow the same 70%, 30%,
10% distribution. We repeat the same process for
the subject entity types and object entity types (an-
swer types) individually. Similarly, for example in
the unseen object-type setup, the question ”Which
artist was born in Berlin?” appearing in the test
set means that, there is no question in the train-
ing set having an entity of type artist. Table 3
shows the mean number of samples, predicates,
sub-types and obj-types across the 10 folds for
each experiment setup.

4https://spacy.io/

5.2 Baselines

SELECT is a baseline built from (Serban et al.,
2016) and adapted for the zero shot setup. During
test time given a fact F , this baseline picks a fact
Fc from the training set and outputs the question
that corresponds to it. For evaluating unseen pred-
icates, Fc has the same answer type (obj-type) as
F . And while evaluating unseen sub-types or obj-
types, Fc and F have the same predicate.

R-TRANSE is an extension that we propose
for SELECT. The input triple is encoded us-
ing the concatenation of the TransE embeddings
of the subject, predicate and object. At test
time, R-TRANSE picks a fact from the train-
ing set that is the closest to the input fact us-
ing cosine similarity and outputs the question that
corresponds to it. We provide two versions of
this baseline: R-TRANSE which indexes and re-
trieves raw questions with only a single place-
holder for the subject label, such as in (Serban
et al., 2016). And R-TRANSEcopy which in-
dexes and retrieves questions using our copy ac-
tions mechanism (sec. 3.5).

IR is an information retrieval baseline. Infor-
mation retrieval has been used before as base-
line for QG from text input (Rush et al., 2015;
Du et al., 2017). We rely on the textual con-
text of each input triple as the search keyword
for retrieval. First, the IR baseline encodes each
question in the training set as a vector of TF-
IDF weights (Joachims, 1997) and then does di-
mensionality reduction through LSA (Halko et al.,
2011). At test time the textual context of the input
triple is converted into a dense vector using the
same process and then the question with the clos-
est cosine distance to the input is retrieved. We
provide two versions of this baseline: IR on raw
text and IRcopy on text with our placeholders for
copy actions.

Encoder-Decoder. Finally, we compare
our model to the Encoder-Decoder model with
a single placeholder, the best performing model
from (Serban et al., 2016). We initialize the en-
coder with TransE embeddings and the decoder
with GloVe word embeddings. Although this
model was not originally built to generalize to un-
seen predicates and entity types, it has some gener-
alization abilities represented in the encoded infor-

223



mation in the pre-trained embeddings. Pretrained
KB terms and word embeddings encode relations
between entities or between words as translations
in the vector space. Thus the model might be able
to map new classes or predicates in the input fact
to new words in the output question.

5.3 Training & Implementation Details
To train the neural network models we optimize
the negative log-likelihood of the training data
with respect to all the model parameters. For that
we use the RMSProp optimization algorithm with
a decreasing learning rate of 0.001, mini-batch
size = 200, and clipping gradients with norms
larger than 0.1. We use the same vocabulary for
both the textual context encoders and the decoder
outputs. We limit our vocabulary to the top 30, 000
words including the special tokens. For the word
embeddings we chose GloVe (Pennington et al.,
2014) pretrained embeddings of size 100. We
train TransE embeddings of size Hk = 200, on
the FB5M dataset (Bordes et al., 2015) using the
TransE model implementation from (Lin et al.,
2015). We set GRU hidden size of the decoder
to Hd = 500, and textual encoder to Hc = 200.
The networks hyperparameters are set with respect
to the final BLEU-4 score over the validation set.
All neural networks are implemented using Ten-
sorflow (Abadi et al., 2015). All experiments and
models source code are publicly available5 for the
sake of reproducibility.

5.4 Automatic Evaluation Metrics
To evaluate the quality of the generated question,
we compare the original labeled questions by
human annotators to the ones generated by
each variation of our model and the baselines.
We rely on a set of well established evaluation
metrics for text generation: BLEU-1, BLEU-
2, BLEU-3, BLEU-4 (Papineni et al., 2002),
METEOR (Denkowski and Lavie, 2014) and
ROUGEL (Lin, 2004).

5.5 Human Evaluation
Automatic Metrics for evaluating text generation
such as BLEU and METEOR give an measure
of how close the generated questions are to the
target correct labels. However, they still suffer
from many limitations (Novikova et al., 2017).

5https://github.com/hadyelsahar/
Zeroshot-QuestionGeneration

Automatic metrics might not be able to evaluate
directly whether a specific predicate was explicitly
mentioned in the generated text or not.
As an example, taking a target question and two
corresponding generated questions A and B:

What kind of film is kill bill vol. 2? BLEU

A) What is the name of the film kill bill vol. 2? 71

B) Which genre is kill bill vol. 2 in? 55

We can find that the sentence A having a
better BLEU score than B although it is not
able to express the correct target predicate (film
genre). For that reason we decide to run two
further human evaluations to directly measure the
following:
Predicate identification: annotators were asked to
indicate whether the generated question contains
the given predicate in the fact or not, either
directly or implicitly.
Naturalness: following (Ngomo et al., 2013), we
measure the comprehensibility and readability
of the generated questions. Each annotator was
asked to rate each generated question using a
scale from 1 to 5, where: (5) perfectly clear and
natural, (3) artificial but understandable, and
(1) completely not understandable. We run our
studies on 100 randomly sampled input facts
alongside with their corresponding generated
questions by each of the systems using the help of
4 annotators.

6 Results & Discussion

Automatic Evaluation Table 4 shows results of
our model compared to all other baselines across
all evaluation metrics. Our that encodes the KB
fact and textual contexts achieves a significant en-
hancement over all the baselines in all evalua-
tion metrics, with +2.04 BLEU-4 score than the
Encoder-Decoder baseline. Incorporating the part-
of-speech copy actions further improves this en-
hancement to reach +2.39 BLEU-4 points.
Among all baselines, the Encoder-Decoder base-
line and the R-TRANSE baseline performed the
best. This shows that TransE embeddings encode
intra-predicates information and intra-class-types
information to a great extent, and can generalize to
some extent to unseen predicates and class types.

Similar patterns can be seen in the evaluation
on unseen sub-types and obj-types (Table 5). Our
model with copy actions was able to outperform

224



Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGEL METEOR
U

ns
ee

n
Pr

ed
ic

at
es

SELECT 46.81 ± 2.12 38.62 ± 1.78 31.26 ± 1.9 23.66 ± 2.22 52.04 ± 1.43 27.11 ± 0.74
IR 48.43 ± 1.64 39.13 ± 1.34 31.4 ± 1.66 23.59 ± 2.36 52.88 ± 1.24 27.34 ± 0.55
IRCOPY 48.22 ± 1.84 38.82 ± 1.5 31.01 ± 1.72 23.12 ± 2.24 52.72 ± 1.26 27.24 ± 0.57
R-TRANSE 49.09 ± 1.69 40.75 ± 1.42 33.4 ± 1.7 25.97 ± 2.22 54.07 ± 1.31 28.13 ± 0.54
R-TRANSECOPY 49.0 ± 1.76 40.63 ± 1.48 33.28 ± 1.74 25.87 ± 2.23 54.09 ± 1.35 28.12 ± 0.57
Encoder-Decoder 58.92 ± 2.05 47.7 ± 1.62 38.18 ± 1.86 28.71 ± 2.35 59.12 ± 1.16 34.28 ± 0.54
Our-Model 60.8 ± 1.52 49.8 ± 1.37 40.32 ± 1.92 30.76 ± 2.7 60.07 ± 0.9 35.34 ± 0.43
Our-Modelcopy 62.44 ± 1.85 50.62 ± 1.46 40.82 ± 1.77 31.1 ± 2.46 61.23 ± 1.2 36.24 ± 0.65

Table 4: Evaluation results of our model and all other baselines for the unseen predicate evaluation setup

Model BLEU-4 ROUGEL

Su
b-

Ty
pe

s R-TRANSE 32.41 ± 1.74 59.27 ± 0.92
Encoder-Decoder 42.14 ± 2.05 68.95 ± 0.86
Our-Model 42.13 ± 1.88 69.35 ± 0.9
Our-Modelcopy 42.2 ± 2.0 69.37 ± 1.0

O
bj

-T
yp

es

R-TRANSE 30.59 ± 1.3 57.37 ± 1.17
Encoder-Decoder 37.79 ± 2.65 65.69 ± 2.25
Our-Model 37.78 ± 2.02 65.51 ± 1.56
Our-Modelcopy 38.02 ± 1.9 66.24 ± 1.38

Table 5: Automatic evaluation of our model against se-
lected baselines for unseen sub-types and obj-types

Model % Pred. Identified Natural.

Encoder-Decoder 6 3.14
Our-Model (No Copy) 6 2.72
Our-Modelcopy (Types context) 37 3.21
Our-Modelcopy (All contexts) 46 2.61

Table 6: results of Human evaluation on % of predi-
cates identified and naturalness 0-5

all the other systems. Majority of systems have
reported a significantly higher BLEU-4 scores in
these two tasks than when generalizing to unseen
predicates (+12 and +8 BLEU-4 points respec-
tively). This indicates that these tasks are rela-
tively easier and hence our models achieve rela-
tively smaller enhancements over the baselines.

Human Evaluation Table 6 shows how dif-
ferent variations of our system can express the
unseen predicate in the target question with
comparison to the Encoder-Decoder baseline.
Our proposed copy actions have scored a sig-
nificant enhancement in the identification of
unseen predicates with up to +40% more than
best performing baseline and our model version
without the copy actions.

By examining some of the generated ques-
tions (Table 7) we see that models without
copy actions can generalize to unseen pred-
icates that only have a very similar free-
base predicate in the training set. For ex-
ample fb:tv program/language and
fb:film/language, if one of those predi-
cates exists in the training set the model can use
the same questions for the other during test time.
Copy actions from the sub-type and the obj-type
textual contexts can generalize to a great extent
to unseen predicates because of the overlap
between the predicate and the object type in many
questions (Example 2 Table 7). Adding the pred-
icate context to our model has enhanced model
performance for expressing unseen predicates by
+9% (Table 6). However we can see that it has
affected the naturalness of the question. The post
processing step does not take into consideration
that some verbs and prepositions do not fit in
the sentence structure, or that some words are
already existing in the question words (Example
4 Table 7). This does not happen as much when
having copy actions from the sub-type and the
obj-type contexts because they are mainly formed
of nouns which are more interchangeable than
verbs or prepositions. A post-processing step to
reform the question instead of direct copying from
the input source is considered in our future work.

7 Conclusion

In this paper we presented a new neural model
for question generation from knowledge bases,
with a main focus on predicates, subject types
or object types that were not seen at the train-
ing phase (Zero-Shot Question Generation). Our
model is based on an encoder-decoder architecture
that leverages textual contexts of triples, two at-
tention layers for triples and textual contexts and

225



1 Reference what language is spoken in the
tv show three sheets?

Enc-Dec. in what language is three sheets
in?

Our-Model what the the player is the three
sheets?

Our-ModelCopy what is the language of three
sheets?

2 Reference how is roosevelt in Africa clas-
sified?

Enc-Dec. what is the name of a roosevelt in
Africa?

Our-Model what is the name of the movie
roosevelt in Africa?

Our-ModelCopy what is a genre of roosevelt in
Africa?

3 Reference where can 5260 philvron be
found?

Enc-Dec. what is a release some that 5260
philvron wrote?

Our-Model what is the name of an artist 5260
philvron?

Our-ModelCopy which star system contains the
star system body 5260 philvron?

4 Reference which university did ezra cor-
nell create?

Enc-Dec. which films are part of ezra cor-
nell?

Our-Model what is a position of ezra cornell?
Our-ModelCopy what founded the name of a uni-

versity that ezra cornell founded?

5 Reference who founded snocap , inc .?
Enc-Dec. which asian snocap is most as?
Our model what is the name of a person of

snocap?
Our-ModelCopy who is the person behind sno-

cap?

Table 7: Examples of generated questions from differ-
ent systems in comparison

finally a part-of-speech copy action mechanism.
Our method exhibits significantly better results
for Zero-Shot QG than a set of strong baselines
including the state-of-the-art question generation
from KB. Additionally, a complimentary human
evaluation, helps in showing that the improvement
brought by our part-of-speech copy action mech-
anism is even more significant than what the au-
tomatic evaluation suggests. The source code and
the collected textual contexts are provided for the
community 6

6https://github.com/hadyelsahar/
Zeroshot-QuestionGeneration

Acknowledgements

This research is partially supported by the An-
swering Questions using Web Data (WDAqua)
project, a Marie Skłodowska-Curie Innovative
Training Network under grant agreement No
642795, part of the Horizon 2020 programme.

References

Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Va-
sudevan, Fernanda Viégas, Oriol Vinyals, Pete War-
den, Martin Wattenberg, Martin Wicke, Yuan Yu,
and Xiaoqiang Zheng. 2015. TensorFlow: Large-
scale machine learning on heterogeneous systems.
Software available from tensorflow.org. https:
//www.tensorflow.org/.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR
abs/1409.0473. http://arxiv.org/abs/
1409.0473.

Steven Bird. 2006. NLTK: the natural language toolkit.
In ACL 2006, 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, Proceedings of the Conference, Sydney, Aus-
tralia, 17-21 July 2006. http://aclweb.org/
anthology/P06-4018.

Antoine Bordes, Nicolas Usunier, Sumit Chopra, and
Jason Weston. 2015. Large-scale simple ques-
tion answering with memory networks. CoRR
abs/1506.02075. http://arxiv.org/abs/
1506.02075.

Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. CoRR abs/1406.1078.

Michael J. Denkowski and Alon Lavie. 2014. Me-
teor universal: Language specific translation eval-
uation for any target language. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, WMT@ACL 2014, June 26-
27, 2014, Baltimore, Maryland, USA. pages 376–
380. http://aclweb.org/anthology/W/
W14/W14-3348.pdf.

226



Li Dong, Jonathan Mallinson, Siva Reddy, and
Mirella Lapata. 2017. Learning to paraphrase
for question answering. In Proceedings of the
2017 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP 2017, Copen-
hagen, Denmark, September 9-11, 2017. pages
875–886. https://aclanthology.info/
papers/D17-1091/d17-1091.

Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn-
ing to ask: Neural question generation for read-
ing comprehension. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2017, Vancouver, Canada,
July 30 - August 4, Volume 1: Long Papers. pages
1342–1352. https://doi.org/10.18653/
v1/P17-1123.

Martha Evens and Joel Michael. 2006. One-on-one tu-
toring by humans and machines. Computer Science
Department, Illinois Institute of Technology .

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2011, 27-31 July 2011, John McIn-
tyre Conference Centre, Edinburgh, UK, A meeting
of SIGDAT, a Special Interest Group of the ACL.
pages 1535–1545. http://www.aclweb.org/
anthology/D11-1142.

Arthur C Graesser, Shulan Lu, George Tanner Jack-
son, Heather Hite Mitchell, Mathew Ventura, An-
drew Olney, and Max M Louwerse. 2004. Autotu-
tor: A tutor with dialogue in natural language. Be-
havior Research Methods 36(2):180–192.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for
Computational Linguistics, ACL 2016, August 7-
12, 2016, Berlin, Germany, Volume 1: Long Pa-
pers. http://aclweb.org/anthology/P/
P16/P16-1154.pdf.

Çaglar Gülçehre, Sungjin Ahn, Ramesh Nallapati,
Bowen Zhou, and Yoshua Bengio. 2016. Point-
ing the unknown words. In Proceedings of the
54th Annual Meeting of the Association for Com-
putational Linguistics, ACL 2016, August 7-12,
2016, Berlin, Germany, Volume 1: Long Pa-
pers. http://aclweb.org/anthology/P/
P16/P16-1014.pdf.

Nathan Halko, Per-Gunnar Martinsson, and Joel A.
Tropp. 2011. Finding structure with random-
ness: Probabilistic algorithms for constructing ap-
proximate matrix decompositions. SIAM Re-
view 53(2):217–288. https://doi.org/10.
1137/090771806.

Michael Heilman and Noah A. Smith. 2010. Good
question! statistical ranking for question genera-

tion. In Human Language Technologies: Confer-
ence of the North American Chapter of the As-
sociation of Computational Linguistics, Proceed-
ings, June 2-4, 2010, Los Angeles, California, USA.
pages 609–617. http://www.aclweb.org/
anthology/N10-1086.

Thorsten Joachims. 1997. A probabilistic analysis of
the rocchio algorithm with TFIDF for text catego-
rization. In Proceedings of the Fourteenth Inter-
national Conference on Machine Learning (ICML
1997), Nashville, Tennessee, USA, July 8-12, 1997.
pages 143–151.

Mitesh M. Khapra, Dinesh Raghu, Sachindra Joshi,
and Sathish Reddy. 2017. Generating natural lan-
guage question-answer pairs from a knowledge
graph using a RNN based question generation
model. In Proceedings of the 15th Conference of
the European Chapter of the Association for Com-
putational Linguistics, EACL 2017, Valencia, Spain,
April 3-7, 2017, Volume 1: Long Papers. pages 376–
385. https://aclanthology.info/pdf/
E/E17/E17-1036.pdf.

Ryan Kiros, Ruslan Salakhutdinov, and Richard S.
Zemel. 2014. Unifying visual-semantic embed-
dings with multimodal neural language models.
CoRR abs/1411.2539. http://arxiv.org/
abs/1411.2539.

Rémi Lebret, David Grangier, and Michael Auli.
2016. Neural text generation from structured
data with application to the biography domain.
In Proceedings of the 2016 Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP 2016, Austin, Texas, USA, November 1-
4, 2016. pages 1203–1213. http://aclweb.
org/anthology/D/D16/D16-1128.pdf.

Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
Zettlemoyer. 2017. Zero-shot relation extraction via
reading comprehension. In Proceedings of the 21st
Conference on Computational Natural Language
Learning (CoNLL 2017), Vancouver, Canada, Au-
gust 3-4, 2017. pages 333–342. https://doi.
org/10.18653/v1/K17-1034.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Text summariza-
tion branches out: Proceedings of the ACL-04 work-
shop. Barcelona, Spain, volume 8.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu,
and Xuan Zhu. 2015. Learning entity and re-
lation embeddings for knowledge graph comple-
tion. In Proceedings of the Twenty-Ninth AAAI
Conference on Artificial Intelligence, January 25-
30, 2015, Austin, Texas, USA.. pages 2181–
2187. http://www.aaai.org/ocs/index.
php/AAAI/AAAI15/paper/view/9571.

Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol
Vinyals, and Wojciech Zaremba. 2015. Address-
ing the rare word problem in neural machine trans-

227



lation. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguis-
tics and the 7th International Joint Conference on
Natural Language Processing of the Asian Feder-
ation of Natural Language Processing, ACL 2015,
July 26-31, 2015, Beijing, China, Volume 1: Long
Papers. pages 11–19. http://aclweb.org/
anthology/P/P15/P15-1002.pdf.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation extrac-
tion without labeled data. In ACL 2009, Proceed-
ings of the 47th Annual Meeting of the Association
for Computational Linguistics and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, 2-7 August 2009, Singapore.
pages 1003–1011. http://www.aclweb.org/
anthology/P09-1113.

Axel-Cyrille Ngonga Ngomo, Lorenz Bühmann,
Christina Unger, Jens Lehmann, and Daniel Ger-
ber. 2013. Sorry, i don’t speak SPARQL: translating
SPARQL queries into natural language. In 22nd In-
ternational World Wide Web Conference, WWW ’13,
Rio de Janeiro, Brazil, May 13-17, 2013. pages 977–
988. http://dl.acm.org/citation.cfm?
id=2488473.

Jekaterina Novikova, Ondrej Dusek, Amanda Cercas
Curry, and Verena Rieser. 2017. Why we need
new evaluation metrics for NLG. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2017, Copen-
hagen, Denmark, September 9-11, 2017. pages
2241–2252. https://aclanthology.info/
papers/D17-1238/d17-1238.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics, July 6-12, 2002, Philadel-
phia, PA, USA.. pages 311–318. http://www.
aclweb.org/anthology/P02-1040.pdf.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors
for word representation. In Proceedings of the
2014 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP 2014, October
25-29, 2014, Doha, Qatar, A meeting of SIGDAT,
a Special Interest Group of the ACL. pages 1532–
1543. http://aclweb.org/anthology/D/
D14/D14-1162.pdf.

Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their men-
tions without labeled text. In Machine Learn-
ing and Knowledge Discovery in Databases, Euro-
pean Conference, ECML PKDD 2010, Barcelona,
Spain, September 20-24, 2010, Proceedings, Part
III. pages 148–163. https://doi.org/10.
1007/978-3-642-15939-8_10.

Alexander M. Rush, Sumit Chopra, and Jason We-
ston. 2015. A neural attention model for ab-
stractive sentence summarization. In Proceedings
of the 2015 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2015, Lis-
bon, Portugal, September 17-21, 2015. pages 379–
389. http://aclweb.org/anthology/D/
D15/D15-1044.pdf.

Iulian Vlad Serban, Alberto Garcı́a-Durán, Çaglar
Gülçehre, Sungjin Ahn, Sarath Chandar, Aaron C.
Courville, and Yoshua Bengio. 2016. Generating
factoid questions with recurrent neural networks:
The 30m factoid question-answer corpus. In Pro-
ceedings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics, ACL 2016, Au-
gust 7-12, 2016, Berlin, Germany, Volume 1: Long
Papers. http://aclweb.org/anthology/
P/P16/P16-1056.pdf.

Dominic Seyler, Mohamed Yahya, and Klaus
Berberich. 2015. Generating quiz questions from
knowledge graphs. In Proceedings of the 24th Inter-
national Conference on World Wide Web Compan-
ion, WWW 2015, Florence, Italy, May 18-22, 2015
- Companion Volume. pages 113–114. https:
//doi.org/10.1145/2740908.2742722.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conver-
sation. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Nat-
ural Language Processing of the Asian Federation
of Natural Language Processing, ACL 2015, July
26-31, 2015, Beijing, China, Volume 1: Long Pa-
pers. pages 1577–1586. http://aclweb.org/
anthology/P/P15/P15-1152.pdf.

Richard Socher, Milind Ganjoo, Christopher D. Man-
ning, and Andrew Y. Ng. 2013. Zero-shot learning
through cross-modal transfer. In Advances in Neu-
ral Information Processing Systems 26: 27th Annual
Conference on Neural Information Processing Sys-
tems 2013. Proceedings of a meeting held Decem-
ber 5-8, 2013, Lake Tahoe, Nevada, United States..
pages 935–943.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural In-
formation Processing Systems 2014, December 8-
13 2014, Montreal, Quebec, Canada. pages 3104–
3112.

228


