



















































CKY-based Convolutional Attention for Neural Machine Translation


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1–6,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

CKY-based Convolutional Attention for Neural Machine Translation

Taiki Watanabe and Akihiro Tamura and Takashi Ninomiya
Ehime University

3 Bunkyo-cho, Matsuyama, Ehime, JAPAN
{t_watanabe@ai.cs, tamura@cs, ninomiya@cs}.ehime-u.ac.jp

Abstract

This paper proposes a new attention
mechanism for neural machine transla-
tion (NMT) based on convolutional neu-
ral networks (CNNs), which is inspired
by the CKY algorithm. The proposed at-
tention represents every possible combi-
nation of source words (e.g., phrases and
structures) through CNNs, which imitates
the CKY table in the algorithm. NMT,
incorporating the proposed attention, de-
codes a target sentence on the basis of
the attention scores of the hidden states
of CNNs. The proposed attention en-
ables NMT to capture alignments from un-
derlying structures of a source sentence
without sentence parsing. The evalua-
tions on the Asian Scientific Paper Excerpt
Corpus (ASPEC) English-Japanese trans-
lation task show that the proposed atten-
tion gains 0.66 points in BLEU.

1 Introduction

Recently, neural machine translation (NMT) based
on neural networks (NNs) is known to provide
both high-precision and human-like translation
through its simple architecture. In NMT, the
encoder-decoder model, which is intensively stud-
ied, converts a source-language sentence into a
fixed-length vector and then generates a target-
language sentence from the vector by using re-
current NNs (RNNs) with gated recurrent units
(GRUs) (Cho et al., 2014a) or long short-term
memory (LSTM) (Hochreiter and Schmidhuber,
1997; Gers et al., 2000; Sutskever et al., 2014).
An attention-based NMT (ANMT) is one of the
state-of-the-art technologies for MT, which is an
extension of the encoder-decoder model and pro-
vides highly accurate translation (Luong et al.,

2015; Dzmitry et al., 2015). ANMT is a method of
translation in which the decoder generates a target-
language sentence, referring to the history of the
encoder’s hidden layer state.

The encoder-decoder model has also been ex-
tended to syntax-based NMT, which utilizes struc-
tures of source sentences, target sentences, or
both. In particular, Eriguchi et al. (2016b) have
shown that a source-side structure (i.e., constituent
trees of source sentences) are useful for NMT
on the English-Japanese translation. However,
syntax-based NMT requires sentence parsing in
advance.

This paper proposes a new attention mechanism
for NMT based on convolutional neural networks
(CNNs) to leverage the structures of source sen-
tences in NMT without parsing. In the parsing
field, the CKY algorithm (Kasami, 1965; Younger,
1967) parses a sentence in a bottom-up manner
through the CKY table, which efficiently consid-
ers all possible combinations of words and rep-
resents the structure of the sentence through dy-
namic programming. Inspired by the algorithm,
we incorporate CNNs that imitate the CKY table
into the attention mechanism of ANMT. In par-
ticular, the proposed attention constructs CNNs in
the same order as the calculation procedures in the
CKY table, and then ANMT decodes a target sen-
tence by referring to each state of the hidden lay-
ers of CNNs, which corresponds to each cell in
the CKY table. The proposed attention enables the
ANMT model to capture underlying structures of
a source sentence that are useful for a prediction
of each target word, without sentence parsing in
advance.

The evaluations on the ASPEC English-
Japanese translation task (Nakazawa et al., 2016)
show that the proposed attention gains 0.66 points
in BLEU. Furthermore, they show that our atten-
tion can capture structural alignments (e.g., align-

1



ment to a case structure), which is not a word-level
alignment.

There are several previous studies on NMT
using CNNs (Kalchbrenner and Blunsom,
2013; Cho et al., 2014b; Lamb and Xie, 2016;
Kalchbrenner et al., 2016). Their models consist
of serially connected multi-layer CNNs for en-
coders or decoders, similar to image recognition
CNNs for 1D image processing. Therefore,
their models do not have any direct mecha-
nisms for dealing with the connections between
phrases/words in long distance. Our model adopts
CKY-based connections between multi-layer
CNNs, which enables the NMT to calculate direct
connections between phrases/words in encoders,
and the attention mechanism enables the NMT
to capture structural alignment between decoders
and encoders1.

2 Attention-based NMT (ANMT)

ANMT (Luong et al., 2015; Dzmitry et al., 2015)
is an extension of the encoder-decoder model
(Sutskever et al., 2014; Cho et al., 2014a). The
model uses its RNN encoder to convert a source-
language sentence into a fixed-length vector and
then uses its RNN decoder to generate a target-
language sentence from the vector.

We used a bi-directional two-layer LSTM net-
work as the encoder. Given a source-language sen-
tence x = x1, x2, · · · , xT , the encoder represents
the i-th word, xi, as a d-dimensional vector, vi, by
a word embedding layer. The encoder then com-
putes the hidden state of vi, hi, as follows:

−−→
h

(1)
i = LSTM

(1)(vi), (1)←−−
h

(1)
i = LSTM

(1)(vi), (2)−−→
h

(2)
i = LSTM

(2)(
−−→
h

(1)
i ) +

−−→
h

(1)
i , (3)←−−

h
(2)
i = LSTM

(2)(
←−−
h

(1)
i ) +

←−−
h

(1)
i , (4)

hi =
−−→
h

(2)
i +

←−−
h

(2)
i , (5)

where → and ← indicate the forward direction
(i.e., from the beginning to the end of a sentence)
and the reverse direction, respectively. LSTM (1)

and LSTM (2) represent the first- and second-
layer LSTM encoders, respectively. The dimen-

sions of
←−−
h

(1)
i ,
−−→
h

(1)
i ,
←−−
h

(2)
i ,
−−→
h

(2)
i , and hi are d.

1In a preliminary experiment, we directly applied a CNN
to the encoder of the encoder-decoder model. However, the
method (BLEU: 25.91) does not outperform our proposed
method (BLEU: 26.75).

In ANMT, the decoder generates a target-
language sentence, referring to the hidden layer’s
states of the LSTM encoder, hi. The attention
mechanism explained below is called global atten-
tion (dot) (Luong et al., 2015). We used a two-
layer LSTM network as the decoder. The ini-
tial states of the first- and second-layer LSTM de-
coders are initialized as the states of the first- and
second-layer LSTM encoders in the reverse direc-
tion, respectively.

Each state of the hidden layers of LSTM de-
coders, s(1)j and s

(2)
j , is calculated by

s
(1)
j = LSTM

(1)([wj−1; ŝj−1]), (6)

s
(2)
j = LSTM

(2)(s(1)j ), (7)

where wj−1 indicates word embedding of the out-
put word yj−1, ‘;’ represents a concatenation of
matrices, and ŝj−1 is an attentional vector used
for generating the output word yj−1, which is ex-
plained below2.

The dimensions of wj−1 and ŝj−1 are d. The
attention score αj(i) is calculated as follows:

αj(i) =
exp(hi · s(2)j )∑T

k=1 exp(hk · s(2)j )
. (8)

The context vector cj for generating a target-
language sentence is calculated by

cj =
T∑

i=1

αj(i)hi. (9)

The attentional vector ŝj is calculated by using the
context vector as follows:

ŝj = tanh(Wc[s
(2)
j ; cj ]), (10)

and then using the state of this hidden layer, the
probability of the output word yj is given by

p(yj |y<j ,x) = softmax(Wsŝj), (11)
where Wc and Ws represent weight matrices3.

3 NMT with CKY-based Convolutional
Attention

Figure 1 shows the overall structure of the pro-
posed attention. In the proposed attention, a gen-

2Providing an attentional vector as inputs to the LSTM in
the next time step is called input feeding (Luong et al., 2015).

3In our experiments, target sentences are generated by the
greedy algorithm on the basis of output probabilities.

2



��� ���

���

���

Figure 1: Overall View of CKY-based Attention

erative rule in the CKY algorithm is imitated by
the network structure shown in Figure 2. We
call the network as the Deduction Unit (DU). In
a DU, four types of CNNs are connected by a
residual connection4. In Figure 2, the size of fil-
ters and the number of output channels for each
CNN are shown in a parenthesis. In particular, the
filter sizes of CNN1, CNN2, CNN3, and CNN4,
are 1 × 1, 1 × 2, 1 × 1, and 1 × 2, and their
channel numbers are d2 ,

d
2 , d, and d, respectively.

Each DU receives d-dimensional vectors (states)
of two cells in a CKY table and computes a d-
dimensional vector for an upper-level cell, which
corresponds to a generation rule in the CKY al-
gorithm. By using DUs, the state of each cell in
a CKY table is induced by folding the states of
lower-level cells in the same order as the calcu-
lation procedures in the CKY algorithm. We call
the network for this overall procedure as the CKY-
CNN. We hereafter denote the state of the j-th cell
in the i-th CKY-CNN layer as h(cky)i,j . Note that
the states of the first-layer of the CKY-CNN (i.e.,
h(cky)1 = (h

(cky)
1,1 , ..., h

(cky)
1,T )) are set to the states

of the LSTM encoder (i.e., h = (h1, ..., hT )). In
the CKY-CNN, the state of a cell is induced from
multiple candidates of outputs from DUs, similar
to the CKY algorithm. Specifically, the state of a
cell is set to the output vector with the highest sum
of values of all dimensions as follows:

h
(cky)
i,j = Max1≤k≤i−1DU(h

(cky)
k,j , h

(cky)
i−k,j+k)

(12)
4Through a preliminary experiment, we confirmed that

a simple DU composed of one type of CNN did not work
well. Therefore, we have improved the DU in reference to
(He et al., 2016).

����� �

�����	�
��������
�	�

���

�����	�
��������
�	�

�����	�
��������
�	�

����� �

����� �

���

���

����� �

�

���

���������	�
�

���������	� �

����������� �
�����������
�

�����������
�

��������	� �

���������� �

��������	�
�

� �

Figure 2: Deduction Unit in CKY-based Attention

��

��

��

���

���	�

 ������

���	�

�������

���	�

�������

���	�

�������

���	�

�������

Figure 3: An Example of Max-pooling with CKY-
CNN

Figure 3 shows an example of convolutions in the
CKY-CNN, highlighting the process of generating
the state of the yellow cell. In this process, three
DUs generate vectors based on the states of the
two blue cells, those of the two red cells, and those
of the two green cells, respectively. The vector
with the highest sum of vector elements is then set
to the state of the yellow cell. Through the CKY-
CNN, the states of the cells in a CKY table (h(cky))
are obtained.

NMT with the CKY-based convolutional atten-
tion decodes a target sentence by referring to the
states of the hidden layers of the CKY-CNN in ad-
dition to the states of the hidden layer of the LSTM
encoder. The alignment scores are calculated as
follows:

α
′
(i, j)

=
exp(hi·s(2)j )∑T

k=1
exp(hk·s(2)j )+

∑T
k=1

∑T−k+1
l=1

exp(h
(cky)
k,l

·s(2)j )
,

(13)

3



α
′′
(i1, i2, j)

=
exp(h

(cky)
i1,i2

·s(2)j )∑T
k=1

exp(hk·s(2)j )+
∑T

k=1

∑T−k+1
l=1

exp(h
(cky)
k,l

·s(2)j )
.

(14)

Note that s(2)j is the hidden layer’s state of the
second-layer LSTM encoder (see Section 2). The
context vector c

′
j for CKY-CNN is calculated by

c
′
j =

T∑
k=1

α
′
(k, j)hk+

T∑
k=1

T−k+1∑
l=1

α
′′
(k, l, j)h(cky)k,l .

(15)
ŝj is calculated on the basis of the context vector
of the LSTM encoder (cj), which is defined in Sec-
tion 2, and that of the CKY-CNN (c

′
j) as follows:

ŝj = tanh(Ŵ [s
(2)
j ; cj ; c

′
j ]), (16)

where Ŵ ∈ Rd×3d is a weight matrix. By apply-
ing the softmax function to the ŝj in the same way
as in the conventional ANMT (see Section 2), the
encoder predicts the j-th target word.

4 Experiments

4.1 Settings

We used Asian Scientific Paper Excerpt Corpus
(ASPEC)’s English-Japanese corpus5 in this ex-
periment. We used the Moses decoder for word
segmentation of the English corpus and Kytea
(Neubig et al., 2011) for the Japanese corpus. For
each corpus, all characters are lowercased. We
used the first 100,000 sentences (< 50 words)
for training, 1,790 sentences for parameter tuning,
and 1,812 sentences for testing. The words that
appeared less than twice in the training data were
replaced with the special symbol UNK.

The number of dimensions of word vectors and
hidden layers was 256. Adam (Kingsma and Ba,
2014) was used for learning each parameter, and
the initial values of the parameters were set to
α = 0.01, β1 = 0.9, and β2 = 0.99. The learning
rate was halved after 9 and 12 epochs. A gradient
clipping technique was used with a clipping value
of 3.0, following (Eriguchi et al., 2016a). We used
dropout (Srivastava et al., 2014) and weight de-
cay to prevent over-fitting. The dropout ratio for
LSTMs was 0.2, that for the CNN was 0.3, and
the weight decay coefficient was 10−6.

Table 1: Evaluation Results

BLEU (%)
Baseline Model 26.09
Proposed Model 26.75

4.2 Results

We compared the NMT with the CKY-based con-
volutional attention (see Section 3) with the NMT
with the conventional attention (see Section 2) to
confirm the effectiveness of the proposed CKY-
based attention. The only difference between the
baseline and the proposed model is their attention
mechanisms. Table 1 shows the translation perfor-
mance by BLEU (Papineni et al., 2002). For ref-
erence, we obtained a 18.69% BLEU score using
the Moses phrase-based statistical machine trans-
lation system (Koehn et al., 2007) with the default
settings.

Table 1 shows that the proposed model outper-
forms the baseline model, which indicates that the
proposed attention is useful for NMT.

Figure 4 shows the attention scores of an in-
stance in the test data. The deeper color of a
cell represents a higher attention score. The ver-
tical axis represents a source sentence. In Fig-
ure 4, the test sentence is "finally, this paper de-
scribes the recent trend and problems in this field
.". The horizontal axis indicates the depth of the
CKY-CNN. Note that an attention score of the first
layer of the CKY-CNN corresponds to an atten-
tion score of the hidden layer of the LSTM. Figure
4 shows that for the words whose alignments are
clearly defined such as content words (e.g., "最後
(finally)", "分野 (field)", "述べ (describe)"), high
alignment scores are located in the first layer. On
the other hand, for the words whose alignments are
not clearly defined such as function words (e.g., "
に", "け", "る"), high alignment scores are located
at a deeper layer. The Japanese word "に" shows
a case structure, and "け" and "る" are parts of the
Japanese preposition "おける (in)". This indicates
that while the conventional attention finds word-
level alignments, the proposed attention captures
structural alignments.

5http://orchid.kuee.kyoto-u.ac.jp
/WAT/WAT2015/index.html

4



����������� �����	�
� ���
	����	�

�������������
� �������������
� 	������������
�

Figure 4: Examples of Attention Scores

5 Conclusions

This paper proposed an attention mechanism for
NMT based on CNNs, which imitates the CKY al-
gorithm. The evaluations on the ASPEC English-
Japanese translation task showed that the proposed
attention gained 0.66 points in BLEU and cap-
tured structural alignments, which could not be
captured by a conventional attention mechanism.
The proposed model consumes excessive amounts
of memory because the proposed model keeps hid-
den states of all cells in a CKY table. In future,
we would like to improve the proposed attention
in terms of memory consumption, and then ver-
ify the effectiveness of the proposed attention for
larger datasets.

Acknowledgements

This work was supported by JSPS Grants-in-Aid
for Scientific Research Grant Number 25280084.
We are grateful to Shinsuke Mori, Kazuma
Hashimoto, and Akiko Eriguchi for their technical
advice to this work.

References
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-

cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014a. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1724–1734.

Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014b. On the proper-
ties of neural machine translation: Encoder-decoder
approaches. arXiv preprint arXiv:1409.1259.

Bahdanau Dzmitry, Cho KyungHyun, and Bengio
Yoshua. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the 3rd International Conference on Learning Rep-
resentations.

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tsuruoka. 2016a. Character-based decoding in tree-
to-sequence attention-based neural machine transla-
tion. In Proceedings of the 3rd workshop on Asian
Translation, pages 175–183.

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tsuruoka. 2016b. Tree-to-sequence attentional neu-
ral machine translation. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 823–833.

Felix A Gers, Jürgen Schmidhuber, and Fred Cummins.
2000. Learning to forget: Continual prediction with
lstm. Neural computation, 12(10):2451–2471.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on

5



computer vision and pattern recognition, pages 770–
778.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, page 413.

Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan,
Aaron van den Oord, Alex Graves, and Koray
Kavukcuoglu. 2016. Neural machine translation in
linear time. arXiv preprint arXiv:1610.10099.

Tadao Kasami. 1965. An efficient recognition and syn-
tax algorithm for context-free languages. Technical
Report AFCRL-65-758.

Diederik Kingsma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. In 5th Interna-
tional Conference on Learning Representations.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the Associ-
ation for Computational Linguistics on interactive
poster and demonstration sessions, pages 177–180.

Andrew Lamb and Michael Xie. 2016. Convolutional
encoders for neural machine translation. arXiv
preprint arXiv:1611.02344.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412–1421.

Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-
moto, Masao Utiyama, Eiichiro Sumita, Sadao
Kurohashi, and Hitoshi Isahara. 2016. Aspec: Asian
scientific paper excerpt corpus. In Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation, pages 2204–2208.

Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics, pages 529–533.

Kishore Papineni, Salam Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 311–318.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks

from overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 2(10):189–208.

6


