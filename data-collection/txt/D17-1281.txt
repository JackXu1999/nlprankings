



















































Speeding up Reinforcement Learning-based Information Extraction Training using Asynchronous Methods


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2658–2663
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Speeding up Reinforcement Learning-based Information Extraction
Training using Asynchronous Methods

Aditya Sharma
Indian Institute of Science

Bangalore, India
adisharma075@gmail.com

Zarana Parekh∗
DA-IICT

Gandhinagar, India
zaranaparekh17@gmail.com

Partha Talukdar
Indian Institute of Science

Bangalore, India
ppt@iisc.ac.in

Abstract

RLIE-DQN is a recently proposed Re-
inforcement Learning-based Information
Extraction (IE) technique which is able to
incorporate external evidence during the
extraction process. RLIE-DQN trains a
single agent sequentially, training on one
instance at a time. This results in sig-
nificant training slowdown which is unde-
sirable. We leverage recent advances in
parallel RL training using asynchronous
methods and propose RLIE-A3C. RLIE-
A3C trains multiple agents in parallel and
is able to achieve upto 6x training speedup
over RLIE-DQN, while suffering no loss
in average accuracy.

1 Introduction

Extracting information about an event (or entity)
involves multiple decisions, as one first needs to
identify documents relevant to the event, extract
relevant information from those documents, and
finally reconcile various values obtained for the
same relation of the event from different sources
(Ahn, 2006). Search based methods for Infor-
mation Extraction have been increasingly investi-
gated (West et al., 2014); (Hegde and Talukdar,
2015); (Zhang et al., 2016); (Bing et al., 2017).

(Kanani and McCallum, 2012) combine search
and Information Extraction (IE) using Reinforce-
ment Learning (RL), with the goal of selecting
good actions while staying within resource con-
straints, but don’t optimze for extraction accuracy.
More recently, (Narasimhan et al., 2016) proposed
a RL-based approach to model the IE process out-
lined above. We shall refer to this approach as
RLIE-DQN in this paper. RLIE-DQN trains an RL

∗Research carried out during an internship at the Indian
Institute of Science, Bangalore.

agent using Deep Q-Network (DQN) (Mnih et al.,
2015) to select optimal actions to query for docu-
ments and also reconcile extracted values.

DQN trains a single agent sequentially, up-
dating parameters based on one instance at a
time. Each such instance is sampled from the en-
tire training data, also called the experience re-
play. Such sequential experience replay-based
training results in slow learning, while requiring
high memory and computation resources. In order
to overcome this challenge, A3C (Asynchronous
Advantage Actor-Critic), an asynchronous deep
RL training algorithm, has been proposed recently
(Mnih et al., 2016). A3C trains multiple RL agents
in parallel, each of which estimates gradients lo-
cally, and asynchronously updates globally shared
parameters. Recent work has explored applica-
tions for A3C in varied domains (Fernando et al.,
2017), (Mirowski et al., 2016).

In this paper, we propose RLIE-A3C which uses
A3C-based parallel asynchronous agents for train-
ing. This is in contrast to the sequential DQN
training in RLIE-DQN. Differences between the
training regimes of the two methods are shown in
Figure 1. Through experiments on multiple real-
world datasets, we find that RLIE-A3C achieves
upto 6x training speedup compared to RLIE-DQN,
while suffering no loss in accuracy. To the best
of our knowledge, this is the first application of
asynchronous deep RL methods in IE (and also
in NLP), and we hope this paper will foster fur-
ther adoption and research into such methods in
the NLP community. RLIE-A3C code is available
at https://github.com/adi-sharma/RLIE A3C

2658



Figure 1: Left: DQN-based sequential learning framework used in RLIE-DQN (Narasimhan et al.,
2016), as discussed in Section 2. At each time step, the agent looks at a specific instance from the
training data. Right: A3C-based parallel learning framework in RLIE-A3C (proposed approach). The
parallel agents look at different parts of the training data, estimate parameter update statistics locally, and
then perform asynchronous updates on the globally shared parameters (θt) at time step t. See Section 3
for details. Due to the asynchronous parallel updates, RLIE-A3C achieves significant training speedup
without loss in accuracy, as we shall see in Section 4.

Figure 2: Sample state transition in the MDP of
RLIE-DQN (and also RLIE-A3C). In each transi-
tion, two actions are carried out: (1) reconcile new
values with current values,; and (2) issue query to
retrieve other relevant documents and extract val-
ues from those documents. Please see Section 2
for details.

2 RLIE-DQN: Information Extraction
using Reinforcement Learning

We first present a brief overview of RLIE-DQN
(Narasimhan et al., 2016). Given a document to
extract information about an event, RLIE-DQN is-
sues a search query to retrieve other documents
related to the event, extracts event information
from those documents, and finally reconciles val-
ues extracted from the documents. If confidence
in the extracted values are low, then RLIE-DQN
repeats this process with additional queries. This
way, RLIE-DQN incorporates evidences from ex-
ternal sources to improve information extraction

(IE) from a given source document.

RLIE-DQN models the task as a Markov De-
cision Process (MDP) in order to reconcile newly
extracted information selectively and dynamically.
The MDP describes the environment in which the
RL agent learns to make decisions. The MDP is
represented using a tuple 〈S,A,R, T 〉, where S is
the set of states, A = {ad, aq} is the set of actions,
R(s, a) is the reward for taking action a ∈ A from
state s ∈ S, and T (s′ |s, (ad, aq)) is the state tran-
sition function. Here, ad is the reconciliation ac-
tion, while aq is the query action. Based on ad,
the agent may accept extracted values for one or
all relations, reject all newly extracted values or
stop (episode ends).

A sample state transition in RLIE-DQN’s MDP
is shown in Figure 2. State representation con-
sists of many details such as confidence scores of
current and newly extracted relation values, con-
text statistics from which the extractions are per-
formed, etc. But for better readability, only the
set of current and new values are shown for the
states in this figure. Each transition consists of
two actions: reconcile decision and query. The RL
agent uses the reconcile action (ad) to update value
of the ShooterName relation from Paul Kiska to
Kevin Wardzala. The agent uses the query ac-
tion (aq) to issue a new query (”Cleveland shoot-
ing” + ”injured”) to retrieve other relevant docu-
ments and extract new values 3 and 1 for relations
NumKilled and NumWounded, respectively. The
transitions stop whenever ad is a stop decision.

2659



The reward function at each state, is the cum-
mulative difference of current and previous ex-
tracted accuracies, summed over all relations.
Also, a negative reward per step is added to the
reward in order to penalize the agent for longer
episodes, since issuing queries to a search engine
is expensive.
Deep Q-Learning (DQN): Let Q(s, a) be the
measure of long-term cumulative reward obtained
by taking action a from state s. The RL agent
makes use ofQ(s, a) to select the next action from
a from state s. Q-learning (Watkins and Dayan,
1992) is a popular technique to estimate this func-
tion, in which the function for optimal Q-value
is estimated using the Bellman equation (Sutton
and Barto, 1998) Qi+1(s, a) = E(s,a)[R(s, a) +
γmax

a′
Qi(s′, a′)|s, a]. Here, R(s, a) is the imme-

diate reward and γ is the discount factor for the
value of future rewards.

For high dimensional state spaces, Deep Q-
Network (DQN) (Mnih et al., 2013) approximates
Q(s, a) as Q(s, a; θ) using parameters θ of a deep
network. RLIE-DQN used such a DQN-based
agent to learn optimal policy, as shown in Figure 1.

3 Proposed Approach: RLIE-A3C

The DQN-based agent used in RLIE-DQN is se-
quential, as it moves from one instance to another
to update parameters. This can result in signif-
icant training time slowdown, especially in large
data settings. Instead of using experience replay
of the DQN algorithm for stabilizing updates, we
consider a framework with multiple asynchronous
agents, each of which explore different areas of the
environment in parallel.

Asynchronous Advantage Actor-critic (A3C)
(Mnih et al., 2016) is a recently proposed deep RL
algorithm which makes use of parallel agents for
parameter estimation. We replace DQN with A3C
in RLIE-DQN and call the resulting method RLIE-
A3C – our proposed approach (See Figure 1).

At time instant t, RLIE-A3C poses decision
policy πd(ad|st), and query policy πq(aq|st) as
probability distributions over candidate actions ad
and aq, respectively. RLIE-A3C also calculates
the state value function V (st), as an estimate of
the cumulative long-term reward obtained starting
from state st. Owing to the large continuous state
space of the problem, RLIE-A3C approximates
each of these three functions using three sepa-
rate deep neural networks, which are parametrized

by θd, θq, and θv, as πd(ad|st) ≈ πd(ad|st; θd),
πq(aq|st) ≈ πq(aq|st; θd) and V (st) ≈ V (st; θv).
These parameters are updated by agents working
in parallel. Based on the policies, each agent se-
lects the query and decision actions to be per-
formed and updates it’s state accordingly. This is
repeated up to tmax steps or until a terminal state
is reached.
Local Gradient Calculation: The agents estimate
parameter gradients using a local copy of the net-
work parameters, and then perform asynchronous
updates on the globally shared parameters θd, θq,
and θv. Hence, the policy and value functions
are jointly estimated. The policy gradient update
equations calculated over the local copy of net-
work parameters of each parallel RLIE-A3C agent
p are as follows:

dθpx ← dθpx +∇θpx log πd(axi , s;θpx)A(si, axt ; θpv)
+ βx∇θpxH(πx(st; θpx)) (1)

where, x ∈ {d, q} for decision and query. The ad-
vantage function A(st, at; θv) =

∑k−1
i=0 γ

iRt+i +
γkV (st+k; θv) − V (st; θv) above significantly re-
duces the variance of the policy gradient, where
γ is the discount factor and k can vary from state
to state and is upper-bounded by tmax. Since the
gradient updates are accumulated, training stabil-
ity increases. Further, exploration is encouraged
by introducing the entropy regularization term
βx∇θpxH(πx(st; θpx)) in the equation above. Here,
H is the entropy function and βx controls domi-
nance of the entropy term.

The gradient update for the parameters of value
function V is calculated locally by every parallel
agent p as follows:

dθpv ← dθpv + ∂(Gt − V (st; θpv))2/∂θpv
where Gt = E(s)[

∑∞
k=0 γ

kRt+k+1] is the return.
Global Parameter Update: The parameters θd,
θq and θv are learnt using stochastic gradient
descent with RMSprop (Tieleman and Hinton,
2012). The standard non-centered RMSProp up-
date is used by the parallel agents p to update the
shared parameters asynchronously using the gra-
dients obtained from Equation (1), as follows:

g = αg + (1− α)(dθpx)2 ; θx ← θx − η
dθpx√
g + �

where x ∈ {d, q, v}, α is the decay factor, η is
the learning rate, � is the smoothing constant and

2660



Figure 3: Left, Middle Panels: Extraction accuracy of the baseline (RLIE-DQN) and our system (RLIE-
A3C) on the Shooting Incidents and Food Adulteration datasets. Right Panel: Training time comparison
between RLIE-DQN and RLIE-A3C. Overall, we observe that RLIE-A3C results in upto 6x speedup
over RLIE-DQN, without any loss in average extraction accuracy. This is our main result. Please see
Section 4.1 for details.

g is the moving average of element-wise squared
gradients. The pseudo-code for RLIE-A3C can be
found in the Appendix.

4 Experiments and Results

Setup: We compare RLIE-A3C against RLIE-
DQN using the same protocol and hyperparame-
ters as reported in (Narasimhan et al., 2016). Also,
we experiment with the same two datasets used
in that paper: the Gun Violence Archive1 and the
Foodshield EMA database2. The train, dev and
test datasets contain 372, 146 and 146 source ar-
ticles respectively for the Shooting incident cases
and 292, 42 and 148 source articles respectively
for the food adulteration cases. We used the im-
plementation of RLIE-DQN provided by the au-
thors of that system. For more details on the
dataset and other parameters, we refer the reader
to (Narasimhan et al., 2016).

RLIE-A3C: This is our proposed method which
is described in Section 3. For the sake of fair com-
parison, the network, base classifier, and evalua-
tion metrics are same as that of RLIE-DQN. Each
of the three deep networks in RLIE-A3C, one each
for π(ad|s), π(aq|s) and V (s), is built using two
linear layers with 20 hidden units, followed by Re-
LUs. MaxEnt classifier is used as the base extrac-
tor, and the model is evaluated on the entire test
set for 1.6 million steps. The dev set is used to
tune all hyperparameters, which can be found in
the Appendix. For RLIE-A3C, the evaluation is
carried out 50 times after training and the average
accuracy values are taken over the top 5 evalua-
tions, as done in (Mnih et al., 2016).

Figure 4: Evolution of accuracy of RLIE-A3C for
the four relations on test set of the Shooting Inci-
dents dataset. Please see Section 4.1 for details.

4.1 Results and Discussion

Extraction Accuracy: Experimental results com-
paring extraction accuracies of RLIE-DQN and
RLIE-A3C are presented in Figure 33 (left and
middle panels). From this figure, we observe that
there is no loss in average accuracy in transition-
ing from RLIE-DQN to RLIE-A3C (in fact there
is a slight gain in case of the Adulteration dataset).
Please note that, improvement in accuracy is not
our primary goal in the paper – it is the training
time speedup, as discussed next.
Speedup: Training times of RLIE-DQN and
RLIE-A3C over both datasets are compared in the
right panel of Figure 34. From Figure 3, we find
that RLIE-A3C is able to achieve upto 6x training

1
http://www.shootingtracker.com/Main_Page

2
http://www.foodshield.org/member/login/

3The tolerance for all the relation accuracy values for
RLIE-A3C is within ± 1%

4During test, policies learned by RLIE-DQN and RLIE-
A3C are executed. Since this is an identical process for both,
we don’t compare test runtimes in this paper.

2661



speedup over RLIE-DQN. In other words, RLIE-
A3C is able to achieve significant speedup in train-
ing time over RLIE-DQN, without any loss in av-
erage accuracy. This is our main result.

While RLIE-DQN was implemented in Torch,
RLIE-A3C was implemented in Python using Ten-
sorFlow framework. We note that Torch is known
to be faster than TensorFlow (Bahrampour et al.,
2015). This makes the speedup gains above even
more impressive, and outlines the possibility that
further gains may be possible with a Torch-based
implementation of RLIE-A3C.

Figure 4 shows evolution of test accuracy of
RLIE-A3C for the four relations of the Shooting
Incidents dataset. For this dataset, state value
function of RLIE-A3C converged at 48 minutes.
However, from Figure 4, we observe that accura-
cies converge to the final values much before that.
Why do Asynchronous Methods work? An
asynchronous approach fits more naturally with
the training data, since different parallel agents
look at different events at the same time (see Fig-
ure 1), and the model is able to exploit the regu-
larities between events in the dataset. The gradi-
ent updates to the global network are less biased
and the model is not easily distracted by noise in
the data. The asynchronous parallel model is able
to converge much faster as compared to replay
memory based methods like DQN, as also seen in
(Mnih et al., 2016).

5 Conclusion

In this paper, we proposed RLIE-A3C, an asyn-
chronous deep Reinforcement Learning (RL) al-
gorithm for Information Extraction (IE). In con-
trast to sequential training in previously proposed
RLIE-DQN (Narasimhan et al., 2016), RLIE-A3C
employs asynchronous parallel training. This re-
sults in upto 6x training speedup, without suffer-
ing any loss in average accuracy. We hope that
this first application of asynchronous deep RL al-
gorithms will open up more adoption of such tech-
niques in the NLP community.

6 Acknowledgements

We thank members of the MALL Lab, IISc who
read drafts of the paper and gave valuable feed-
back. We also thank the anonymous reviewers
for their insightful comments. Special thanks to
Karthik Narasimhan for all the helpful discus-
sions. We also gratefully acknowledge support

from a gift from Microsoft Research India.

References
David Ahn. 2006. The stages of event extraction. In

Proceedings of the Workshop on Annotating and
Reasoning about Time and Events, pages 1–8. As-
sociation for Computational Linguistics.

Soheil Bahrampour, Naveen Ramakrishnan, Lukas
Schott, and Mohak Shah. 2015. Comparative
study of deep learning software frameworks. arXiv
preprint arXiv:1511.06435.

Lidong Bing, Zhiming Zhang, Wai Lam, and
William W Cohen. 2017. Towards a language-
independent solution: Knowledge base completion
by searching the web and deriving language pattern.
Knowledge-Based Systems, 115:80–86.

Chrisantha Fernando, Dylan Banarse, Charles Blun-
dell, Yori Zwols, David Ha, Andrei A Rusu, Alexan-
der Pritzel, and Daan Wierstra. 2017. Pathnet: Evo-
lution channels gradient descent in super neural net-
works. arXiv preprint arXiv:1701.08734.

Manjunath Hegde and Partha P Talukdar. 2015. An
entity-centric approach for overcoming knowledge
graph sparsity. In EMNLP, pages 530–535.

Pallika H Kanani and Andrew K McCallum. 2012.
Selecting actions for resource-bounded information
extraction using reinforcement learning. In Pro-
ceedings of the fifth ACM international conference
on Web search and data mining, pages 253–262.
ACM.

Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert
Soyer, Andy Ballard, Andrea Banino, Misha Denil,
Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu,
et al. 2016. Learning to navigate in complex envi-
ronments. arXiv preprint arXiv:1611.03673.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi
Mirza, Alex Graves, Timothy P Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu.
2016. Asynchronous methods for deep reinforce-
ment learning. In International Conference on Ma-
chine Learning.

Volodymyr Mnih, Koray Kavukcuoglu, David Sil-
ver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. 2013. Playing atari
with deep reinforcement learning. arXiv preprint
arXiv:1312.5602.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level
control through deep reinforcement learning. Na-
ture, 518(7540):529–533.

2662



Karthik Narasimhan, Adam Yala, and Regina Barzilay.
2016. Improving information extraction by acquir-
ing external evidence with reinforcement learning.
arXiv preprint arXiv:1603.07954.

Richard S Sutton and Andrew G Barto. 1998. Re-
inforcement learning: An introduction, volume 1.
MIT press Cambridge.

Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture
6.5-rmsprop: Divide the gradient by a running av-
erage of its recent magnitude. COURSERA: Neural
networks for machine learning, 4(2).

Christopher JCH Watkins and Peter Dayan. 1992. Q-
learning. Machine learning, 8(3-4):279–292.

Robert West, Evgeniy Gabrilovich, Kevin Murphy,
Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014.
Knowledge base completion via search-based ques-
tion answering. In Proceedings of the 23rd inter-
national conference on World wide web, pages 515–
526. ACM.

Zhenzhong Zhang, Le Sun, and Xianpei Han. 2016. A
joint model for entity set expansion and attribute ex-
traction from web search queries. In Proceedings of
the Thirtieth AAAI Conference on Artificial Intelli-
gence, pages 3101–3107. AAAI Press.

2663


