



















































Negative Focus Detection via Contextual Attention Mechanism


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2251–2261,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2251

Negative Focus Detection via Contextual Attention Mechanism

Longxiang Shen1, Bowei Zou12∗, Yu Hong1,
Qiaoming Zhu1, Guodong Zhou1, Ai Ti Aw2

1School of Computer Scienceand Technology, Soochow University, China
2Aural & Language Intelligence Department, Institute for Infocomm Research, Singapore

lxshen.scu@gmail.com, zoubowei@suda.edu.cn,
{yhong, qmzhu, gdzhou}@suda.edu.cn, aaiti@i2r.a-star.edu.sg

Abstract
Negation is a universal but complicated lin-
guistic phenomenon, which has received con-
siderable attention from the NLP community
over the last decade, since a negated state-
ment often carries both an explicit negative fo-
cus and implicit positive meanings. For the
sake of understanding a negated statement, it
is critical to precisely detect the negative fo-
cus in context. However, how to capture con-
textual information for negative focus detec-
tion is still an open challenge. To well ad-
dress this, we come up with an attention-based
neural network to model contextual informa-
tion. In particular, we introduce a framework
which consists of a Bidirectional Long Short-
Term Memory (BiLSTM) neural network and
a Conditional Random Fields (CRF) layer to
effectively encode the order information and
the long-range context dependency in a sen-
tence. Moreover, we design two types of
attention mechanisms, word-level contextual
attention and topic-level contextual attention,
to take advantage of contextual information
across sentences from both the word perspec-
tive and the topic perspective, respectively.
Experimental results on the SEM’12 shared
task corpus show that our approach achieves
the best performance on negative focus de-
tection, yielding an absolute improvement of
2.11% over the state-of-the-art. This demon-
strates the great effectiveness of the two types
of contextual attention mechanisms.

1 Introduction

As a linguistic phenomenon that reverses the po-
larity of a statement or its property into opposite,
negation is ubiquitous in human languages. Con-
sider following sentence S1. A negative affix -n’t
negates the statement that mutual fund trades take
effect until the market close. Negation process-
ing has been shown critical for a number of NLP

∗ corresponding auther

applications, such as biomedical information ex-
traction (Morante, 2010; Mehrabi et al., 2015) and
sentiment analysis (Wiegand et al., 2010; Jimenez-
Zafra et al., 2017).

(S1) Mutual fund trades don’t take effect {until the
market close}.1

In general, negation is marked by a word or
an affix and interact with other parts in sen-
tence. Blanco and Moldovan (2011) demonstrated
that over 65% of negated statements in Prop-
Bank (Palmer et al., 2005) convey implicit positive
meanings. For instance, underneath sentence S1,
the statement that mutual fund trades take effect is
true, when removing the propositional clause until
the market close from the original sentence. As the
definition of negation focus in SEM’12 (Morante
and Blanco, 2012), a negative focus is the most
prominently negated part of sentence (Huddleston
and Pullum, 2002). In sentence S1, the negative
focus is the propositional clause until the mar-
ket close, yielding the interpretation that mutual
fund trades take effect, but not until the market
close. As discussed above, correctly understand-
ing a negated statement requires precise detection
of the negative focus. In this paper, we focus on
detecting the negative focus in a sentence with
a negative keyword and its corresponding domi-
nated verb.

Not as simple as that in speech, negative focus
detection poses considerable challenges on text
understanding without knowing stress or intona-
tion information. Previous studies (Blanco and
Moldovan, 2011; Zou et al., 2014) pointed out that
the contextual information plays a critical role in
negative focus detection, since the real negative in-
tention often relates to what the author repeatedly

1#13 of the test set in *SEM 2012 shared task corpus.
Throughout this paper, the negated verbs are marked in bold,
and the negative foci are in curly braces.



2252

state in adjacent sentences. The same negative
statement might be interpreted discriminately de-
pending on different contexts. For example, in fol-
lowing three scenarios, consider what is the more
likely negative focus of sentence S1 with different
contexts:

I. ..., but exchange fund trades do.
negative focus: mutual fund trades

II. ..., unless it reboots.
negative focus: take effect

III. ..., for the shareholders stayed put today.
negative focus: until the market close

In scenario I, since exchange fund trades is em-
phasized in context, the negative focus should be
the phrase mutual fund trades, yielding the inter-
pretation that the trades which don’t take effect are
mutual fund trades, but not exchange fund trades.
Unlike scenario I, the interpretation of negative
statement from scenario II is that Mutual fund
trades don’t take effect without reboot, with the
context unless it reboots, and the phrase take effect
is the negative focus. Similarly, scenario III points
out the reason of not take effect is that the share-
holders stayed put today, which corresponds to the
negative focus until the market close. In addition,
it is worth noting that a negative focus is often not
syntactically determined by sentence structure, in-
stead pragmatically judged by authors’ intentions
conveyed in context.

This is consistent with the attention mecha-
nism in neural networks which has been proven
effective to improve the performances for many
NLP tasks, such as sentiment analysis (Wang
et al., 2016) and relation classification (Zhou et al.,
2016). In this paper, we come up with two types
of attention mechanisms to enforce the model to
take full advantage of the contextual information
for negative focus detection.

In particular, we introduce a framework which
consists of a bidirectional long short-term memory
neural network and a conditional random fields
layer (BiLSTM-CRF) to effectively encode the or-
der information and the long-range context depen-
dency in sentence. On this basis, we propose two
attention mechanisms to capture contextual infor-
mation across sentences. One is to additionally
append word-level vectors of adjacent sentences
into the input vectors. Such attention can auto-
matically focus on the words in context related to
the negative focus. The other is to concatenate
the topic-level representations into the input vec-

tors to better capture the contextual information.
Experimentation on the SEM’12 shared task cor-
pus (Morante and Blanco, 2012) shows that our
approach achieves the best performance on nega-
tive focus detection task, yielding an absolute im-
provement of 2.11% over the state-of-the-art.

The rest of this paper is organized as follows. In
Section 2, we give a brief review of related work.
In Section 3, we introduce the details of the pro-
posed approach. We show our experimental re-
sults and discussions in Section 4. Finally, Sec-
tion 5 concludes with possible directions for future
work.

2 Related Work

Earlier studies of negation processing in compu-
tational linguistics mainly lie in biomedical infor-
mation extraction (Chapman et al., 2001; Goldin
and Chapman, 2003). With the release of the Bio-
Scope corpus (Vincze et al., 2008), the negation
processing techniques received a substantial boost
(Morante et al., 2008; Apostolova et al., 2011; Zou
et al., 2013). In addition, negation also was stud-
ied as a critical factor in polarity shifting in senti-
ment and opinion analysis with various lexical and
syntactic features (Socher et al., 2013; Cruz et al.,
2016).

In general, existing studies of negation process-
ing mainly concentrate on three aspects: 1) cue
detection, which finds negative triggers or expres-
sions in text; 2) scope resolution, which deter-
mines the grammatical scope in a sentence af-
fected by a negative cue; and 3) focus detection,
which identifies the most prominently/explicitly
negated part in a negative statement. A negative
focus could be considered as the semantic part in
scope that is intended to be interpreted as false
to make other parts to be true. Among above
these subtasks, negative focus detection is most
extremely challenging in negation processing.

In the literature, research on negative focus de-
tection was pioneered by Blanco and Moldovan
(2011), who proposed a supervised learning model
with a set of highly hand-crafted features as a
benchmark for this task. On the basis, Blanco and
Moldvan (2013) incorporates negation into other
existing semantic representations. The dataset an-
notated by Blanco and Moldovan is used as a stan-
dard evaluation corpus for the SEM’12 shared task
(Morante and Blanco, 2012). In this shared task,
Rosenberg and Bergler (2012) employed three



2253

Sp

l1

c1 c2 c3 c4 c5

O O O I I

l2 l3 l4 l5

r1 r2 r3 r4 r5

Forward 

LSTM

Backword

LSTM

CRF

layer

Embedding

layer

Output

Input

Contextual 

embeddings

Word-level

contextual 

attention

Mixed 

embeddings

w1 w2 w3 w4 w5 S Sn

Xp XnXw

softmax softmax

X*p X*nXw

X*w

Figure 1: Architecture of BiLSTM-CRF network with word-level contextual attention mechanism for negative
focus detection.

kinds of heuristic rules for negative focus detec-
tion. Moreover, Zou et al. (2014; 2015) proposed
graph-based models to enrich intra-sentence fea-
tures with inter-sentence features from both lexical
and topic perspectives, respectively. Almost all of
the above studies have demonstrated that the in-
formation contained in context plays a critical role
in negative focus detection. In this paper, we ex-
plore the effectiveness of different representations
and processing manners for modeling context in
this task.

In addition to the SEM’12 shared task corpus,
there are several annotated datasets for negative
focus detection. Anand and Martell (2012) made
a point about how to describe the contribution of
negative focus to a sentence via the theory of ques-
tion under discussion, and reannotated a part of
the SEM’12 shared task corpus. Matsuyoshi et al.
(2014) annotated 1,327 instances of negative foci
in Japanese text, and proposed a heuristic rule-
based approach to identify them. Banjade and Rus
(2016) annotated the DT-Neg corpus on tutorial di-
alogue genre, which contains 1,088 of negative fo-
cus instances. Considering the scale and accessi-
bility of the dataset, we utilize the SEM’12 shared
task corpus for experimentation.

3 Contextual Attention-based Negative
Focus Detection

In this section, we first introduce the BiLSTM-
CRF framework. On the basis, we then come up
with two kinds of attention mechanisms to model
the contextual information, i.e. word-level atten-
tion and topic-level attention.

3.1 BiLSTM-CRF Framework

In this paper, we recast negative focus detection
problem as an I/O tagging task, and apply the IO
label scheme, where I denotes the token is Inside
the scope of negative focus in a sentence, and O
indicates the token is Outside of this scope. For
example, the sentence S1 are tagged as below.

mutual/O fund/O trades/O don’t/O take/O effect/O
until/I the/I market/I close/I

The left part of Figure 1 illustrates the BiLSTM-
CRF framework for negative focus detection.
First, the sequence of embeddings (xi) is given
as input to BiLSTM networks, which generates
a representation of the left context (li) and the
right context (ri) for each token in a sentence.
Then, these representations are concatenated (ci)
and linearly projected onto a CRF layer to take
into account neighboring tags, yielding the final
prediction for every token (yi).

Embedding Layer We build an embedding
layer to encode words and semantic role labels.
Given an input sentence S = (w1, w2, ..., wn), we
first transform each word into a real-valued vector
xw ∈ Rdw . In addition, according to the annotated
guideline (Blanco and Moldovan, 2011), semantic
roles in sentence often have a close relationship
with negative focus and negated verb. To capture
such informative features, we then map the seman-
tic role tag of each word to a real-valued vector
xsr of dimension dsr using a embedding matrix
SR ∈ Rdsr×|Vsr|, where Vsr is the set of seman-
tic role label vocabulary. Finally, we represent an
input sentence as a vector sequence x = [xw;xsr]



2254

with the embedding dimension d = (dw + dsr).
BiLSTM Layer We employ LSTM networks

(Hochreiter and Schmidhuber, 1997) which incor-
porate memory-cells to capture long contextual
dependencies in sequence. Formally, each cell in
LSTM can be computed as follows:

Xt = [ht−1 xt]
T

it = σ(W iXt + bi)

f t = σ(W fXt + bf )

ot = σ(W oXt + bo)

ct = f t � ct−1 + it � tanh(W cXt + bc)
ht = ot � tanh(ct)

(1)

where σ is the sigmoid function, and � is the
element-wise multiplication. W i, W f , and W o
are the weighted matrices, and bi, bf , and bo are
the biases, which parametrize the transformations
of input, forget, and output gates, respectively. ht
is the vector of hidden state.

Moreover, to benefit from both previous and fu-
ture information, a forward LSTM and a backward
LSTM are employed to generate a representation−→
ht of the left context and

←−
ht of the right, respec-

tively. The word representation ht is obtained by
concatenating the left and right outputs

[−→
ht;
←−
ht

]
.

CRF Layer To learn the dependencies across
output labels of the BiLSTM layer, we model them
jointly using a conditional random field (CRF)
layer (Lafferty et al., 2001). For an input sentence
x, we denote C as the matrix of output by BiL-
STM. C is of size n × k, where k is the number
of distinct tags, and ci,j corresponds to the score
of the jth tag of the ith token in a sentence. For
a sequence of predictions y, we define its score to
be

s(x,y) =

n∑
i=0

Ayi,yi+1 +

n∑
i=1

ci,yi , (2)

where Ai,j denotes the score of a transition from
the tag i to the tag j. y0 and yn+1 are the additional
tags of START and END, respectively. A softmax
layer over all possible tag sequences yields a prob-
ability for the sequence y:

p(y|x) = 1
Z(x)

exp(s(x,y)), (3)

where Z(x) =
∑

Y exp(s(x, Y )), and Y denotes
all possible tag sequences.

During training, we maximize the log-
probability of the correct tag sequence:

Lc = max log(p(y|x)). (4)

While during decoding, we predict the output se-
quence that obtains the maximum score given by

y∗ = argmax
Y

s(x, Y ). (5)

3.2 Word-level Contextual Attention

As discussed in Section 1, contextual information
is vital for negative focus detection. However, the
BiLSTM-CRF model might fail to identify the im-
portant information related to a negative focus in
contexts. To address this issue, we propose two
kinds of attention mechanisms, i.e. the word-level
contextual attention and the topic-level contextual
attention.

The right part of Figure 1 illustrates the ar-
chitecture of the word-level contextual attention
mechanism. Given previous sentence Sp and next
sentence Sn, we respectively encode each word
wpi and wni into real-valued vectors xpi and
xni ∈ Rdw by the word embeddings described in
Subsection 3.1. Suppose that Np, N , and Nn de-
note the lengths of previous sentence, current sen-
tence, and next sentence, Xp ∈ Rdw×Np , Xw ∈
Rdw×N , and Xn ∈ Rdw×Nn are made up of all
word embeddings in Sp, S, and Sn, respectively.

This means, the word-level contextual attention
mechanism produces attention weight vectors αp
andαn, and weighted hidden representationsX∗p
andX∗n as follows:

αp = softmax(max
col

(
XTp ·Xw√

dw
)) (6)

αn = softmax(max
col

(
XTn ·Xw√

dw
)) (7)

X∗p =Xw � (αp ⊗ edw) (8)
X∗n =Xw � (αn ⊗ edw) (9)

where the contextual attention weights vectorsαp,
αn ∈ RN , and the contextual representation
matrices X∗p, X∗n ∈ Rdw×N . The operator
maxcol (·) in Eq.(6) and Eq.(7) means extracting
the maximum value for each column in matrix to
generate a vector. Besides, we apply the scaling
factor of 1/

√
dw to counteract the effect that the

dot products grow large in magnitude (Vaswani
et al., 2017). The operation α⊗ edw in Eq.(8) and



2255

Topic 

distribution

Topic-level

contextual 

attention

Mixed 

embeddings

BiLSTM-CRF

Framework

Sp

p(t|sp)

Sn

p(t|sn)

S

p(t|wi)

XT

Fully-connected hidden layer 

X*t Xw

X*w

Figure 2: Architecture of topic-level contextual attention mechanism for negative focus detection.

Eq.(9) means that it repeatedly concatenates α for
dw times, where edw is a row vector with dw of 1s.

Finally, the final sentence representation is ob-
tained by

X∗w = [X
∗
p;Xw;X

∗
n] , (10)

and we replace Xw with X∗w in the embedding
layer of the BiLSTM-CRF framework.

3.3 Topic-level Contextual Attention
To acquire the latent topical distribution of words,
we apply the GenSim topic modeling package2

(Rehurek and Sojka, 2010) to generate an LSI se-
mantic model (Bradford, 2008) and obtain two
probabilities with the topic distribution derived
from the Wikipedia corpus3: 1) p(t|w), the topic
distribution given word w, and 2) p(t|s), the topic
distribution given sentence s. The correlation be-
tween the word w and the sentence s can be calcu-
lated by the similarity between their correspond-
ing topic distribution:

Simws = p(t|w)� p(t|s). (11)

Figure 2 illustrates the architecture of topic-
level contextual attention mechanism. In this pa-
per, we obtain the topic distribution of each word
in current sentence p(t|wi) and its adjacent sen-
tences p(t|sp) and p(t|sn), respectively. The
topic-level contextual attention mechanism pro-
duces the topical hidden representations

xti = [Simwisp ;Simwisn ], (12)
2https://radimrehurek.com/gensim/index.html
3https://radimrehurek.com/gensim/wiki.html

where xti ∈ R2dT , and dT is the dimension of
topic distribution. Suppose that N denotes the
length of current sentence, XT ∈ R2dT×N is
made up of all words’ topical hidden representa-
tions. To better learn the latent features and ex-
pand the dimension of XT from 2dT to dt, we
utilize a fully-connected hidden layer:

X∗t = tanh(WXT + b), (13)

where W ∈ Rdt×2dT is the transpose of parame-
ter matrix and b ∈ Rdt . Note that dt is a hyper-
parameter are fine-tuned on the development set.
As a result, we can obtain the sentence representa-
tion as follows

X∗w = [Xw;X
∗
t] , (14)

and replaceXw withX∗w in the embedding layer
of the BiLSTM-CRF framework.

4 Experimentation

In this section, we first introduce the details of
dataset, hyper-parameters settings, and baselines.
Then we compare our model with existing systems
and baselines to demonstrate the effectiveness of
the proposed contextual attention mechanisms. Fi-
nally, we perform in-depth analysis of the impact
factors of our approach and conduct careful error
analysis to better understand the limitation of our
model.



2256

Hyper-parameter Value
word embed dim dw 1,024
semantic role embed dim dsr 200
topic embed dim dt 300
topic number T 80
LSTM hidden layer size h 250
dropout probability p 0.3
learning rate η 0.015

Table 1: Hyper-parameter settings.

4.1 Dataset and Settings

All of experiments conduct on the SEM’12 shared
task corpus4 (Morante and Blanco, 2012) which is
annotated on top of PropBank. According to the
guideline (Blanco and Moldovan, 2011), negative
focus is restricted to verbal negation (marked with
MNEG labels), thus all of the words belonging to
a semantic role most likely to correspond to the
verbal negation are selected as negative focus. In
total, this corpus contains 3,544 instances of neg-
ative focus. For fair comparison, we adopt the
same partition as SEM’12 shared task in all exper-
iments, i.e., with 2,304 for training, 531 for devel-
opment, and 712 for testing. For each instance, the
corpus provides the previous and next sentences as
contexts. We evaluate our results in terms of accu-
racy (acc for short), i.e., for each negation, the pre-
dicted focus is considered correct if it is a perfect
match with its gold annotation.

Table 1 lists the hyper-parameters in our exper-
iments. We utilize the pre-trained word embed-
dings by ELMo5 (Peters et al., 2018). The seman-
tic role embeddings are initialized randomly by a
continuous uniform distribution. All the models
are optimized using the stochastic gradient descent
(SGD). We pick the parameters showing the best
performance on the development set when no fur-
ther improvement occurs within 50 epochs, and re-
port the performances on the test set.

To verify the effectiveness of our approach, we
compare with several baselines on negative focus
detection, which are briefly introduced as follows.

LSTM: The LSTM model which contains only
a forward LSTM layer with a fully-connected
layer followed by a softmax classifier.

BiLSTM: The BiLSTM model employs the
same architecture as LSTM, except a additional

4http://www.clips.ua.ac.be/sem2012-st-neg
5https://allennlp.org/elmo

System Acc

Existing
Methods

CLaC (Rosenberg (2012)) 60.00
FOC-DET (Blanco (2013)) 65.50
WTGM (Zou (2015)) 68.40

Our
Methods

LSTM 58.71
BiLSTM 60.81
BiLSTM-CRF 67.28
W-Att BiLSTM-CRF 70.22
T-Att BiLSTM-CRF 70.51
WT-Att BiLSTM-CRF 69.80

Table 2: Performances of negative focus detection sys-
tems on the SEM’12 corpus.

backward LSTM layer.
BiLSTM-CRF: The BiLSTM-CRF model de-

scribed in Subsection 3.1 without any contextual
attention mechanism.

W-Att BiLSTM-CRF: The word-level con-
textual attention-based BiLSTM-CRF model de-
scribed in Subsection 3.2.

T-Att BiLSTM-CRF: The topic-level con-
textual attention-based BiLSTM-CRF model de-
scribed in Subsection 3.3.

WT-Att BiLSTM-CRF: The BiLSTM-
CRF model with an input representation
[X∗p;Xw;X

∗
n;X

∗
t]. This system com-

bines the above two types of contextual attention
mechanisms.

CLaC: A heuristic-based system with three lin-
guistic rules related to adverb constituent, passive,
and negative trigger of focus, respectively (Rosen-
berg and Bergler, 2012).

FOC-DET: A decision tree based system with
22 kinds of manually designed features (Blanco
and Moldovan, 2013).

WTGM: A graph model consists of a word
layer and a topic layer to capture lexical contex-
tual information (Zou et al., 2015).

4.2 Results

Table 2 shows the performance comparison of var-
ious negative focus detection models. We can see
that all of contextual attention based models (row
7-9 in Table 2) achieve better perfomances than
existing methods (row 1-3 in Table 2) and mod-
els without contextual attention (row 4-6 in Ta-
ble 2). In addition, both word-level and topic-
level contextual attention based models outper-
form the state-of-the-art system (WTGM in Ta-
ble 2) with about 2% accuracy gain at least. The



2257

Method Context Acc
No Attention N/A 67.28

Word-level
Attention

only previous sentence 69.24
only next sentence 69.94
previous+next sentences 70.22

Topic-level
Attention

only previous sentence 69.38
only next sentence 70.08
previous+next sentences 70.51

Table 3: Performance comparison by using different
contextual information into the BiLSTM-CRF frame-
work.

results demonstrate the effectiveness of these two
types of contextual attention mechanisms.

Moreover, to better quantify the contribution
of the different attention mechanisms of our ap-
proach, we also conduct several attention vari-
ants. Comparing the three types of attention
mechanisms (row 7-9 in Table 2), the topic-level
attention based model achieves the best perfor-
mance. We also observe that the word-level atten-
tion based model also achieves comparative per-
formance with the topic-level one. However, when
combining the two types of attention mechanisms,
the performance declines. It indicates that both
the word-level attention mechanism and the topic-
level one can capture the contextual information
effectively, but applying such information repeat-
edly might lead to feature redundancy.

In addition to the methods that take advantage
of the contextual features in adjacent sentences,
we also compare the performances of different
frameworks for negative focus detection (row 4-
6 in Table 2), which only apply the features in
current sentence. The results indicate that the
BiLSTM-CRF framework is a better fit for encod-
ing order information and long-range context de-
pendency for such sequence labeling task.

4.3 Analysis and Discussion

Impact of Contextual Information. We examine
the impact of the context position on the perfor-
mance for our approach. Table 3 shows the per-
formance comparison when using different con-
textual information. We can see that, both the
word-level and topic-level attention based models
achieve the best performances by utilizing the con-
textual features from the previous and next sen-
tences, which verifies the effectiveness of the two
types of contexts. Besides, the results also indicate

Method Embeddings Acc

BiLSTM-CRF
only word 67.28
word + SR 68.82

W-Att
BiLSTM-CRF

only word 68.26
word + SR 70.22

T-Att
BiLSTM-CRF

only word 68.40
word + SR 70.51

Table 4: Performance comparison for systems only us-
ing word embeddings and those using word and SR em-
beddings (SR: semantic role).

Semantic Role Train Dev Test
A1 980 222 309
AM-NEG 592 138 172
AM-TMP 161 35 46
AM-MNR 127 27 38
A2 112 28 36
A0 94 23 31
AM-ADV 78 23 26
Others 165 30 61
NONE 88 19 35
Total 2,304 531 712

Table 5: Statistics of the top seven semantic role labels
of negative focus. NONE class: the negative focus has
at least one word that does not belong to any role.

that contextual information is key for negative fo-
cus detection, especially the next sentence.

Impact of Semantic Role Information. Since
a completed and exclusive semantic role that is
most likely negated is annotated as a negative fo-
cus in sentence, according to the annotation guide-
line (Blanco and Moldovan, 2011), semantic role
information plays a critical role in predicting the
focus. Therefore, we compare the performances
between the models adding semantic role infor-
mation in embedding layer and those only using
the word embeddings. As shown in Table 4, we
can see that the performances of all three models
are improved obviously, which demonstrates the
effectiveness of semantic role information for this
task.

In addition, Table 5 lists the top seven seman-
tic role labels of negative focus in the SEM’12
shared task corpus. We can observe that the dis-
tribution of semantic role labels is relatively con-
centrated into A1, AM-NEG, AM-TMP, and AM-
MNR. Unlike other negation processing task, such
as scope resolution, which heavily relies on the



2258

Method Composition Manner Acc
W-Att

BiLSTM-CRF
within input layer 70.22
within hidden layer 69.38

T-Att
BiLSTM-CRF

within input layer 70.51
within hidden layer 69.80

Table 6: Performance comparison for different compo-
sition manners of attention matrices.

Word Embedding Dimension Acc
Senna 50 70.08
Glove 100 69.66

Word2vec 300 69.10
BERT 768 70.22
ELMo 1,024 70.51

Table 7: Performance comparison for different pre-
trained word embeddings on the T-Att BiLSTM-CRF
model.

lexical or syntactic features (Zou et al., 2013; Qian
et al., 2016), the negative focus detection task
tends to consider more semantic relation between
the focus and other words and phrases.

Impact of Attention Manner. In this pa-
per, our contextual attention matrices are concate-
nated into the input embeddings. In addition, we
also explore another manner of adding attention
mechanisms, which directly concatenates the at-
tention matrix into the hidden layer of BiLSTM-
CRF framework. As shown in Table 6, it indi-
cates that the composition manner within the input
layer is more effective than that within the hidden
layer for both word-level and topic-level attention
mechanisms.

Impact of Pre-trained Word Embedding. To
compare the impacts of different pre-trained word
embeddings for negative focus detection task, we
attempt to employ other pre-trained word em-
beddings. Table 7 show the performances of
the T-Att BiLSTM-CRF model with different
pre-trained word embeddings, including Senna6,
Glove7, Word2vec8, and BERT9. We can see that
ELMo achieves the best performance, but the per-
formance gaps between different pre-trained word
embeddings and different dimensions are not sig-
nificant.

6http://ronan.collobert.com/senna
7http://nlp.stanford.edu/projects/glove
8https://code.google.com/archive/p/word2vec
9https://github.com/google-research/bert

Embeddings Acc
word 67.28
word+chunking label 67.84
word+dependency label 68.12
word+PoS tag 68.26
word+semantic role 68.82
all features 68.54

Table 8: Performance comparison for different features
with the BiLSTM-CRF model.

Impact of Different Features. Table 8 shows
the comparison of the BiLSTM-CRF models with
different feature embeddings. The reason we em-
ploy the basic model is to avoid that the impacts
of different features are covered by contextual at-
tention mechanisms. The results indicate that the
semantic role feature is the most effective for neg-
ative focus detection, which confirms our analysis
above. In addition, the performance of the sys-
tem with all features is lower than that only using
the semantic role feature. It might be that other
features cannot capture more effective information
than the semantic role features.

Error Analysis. We manually analyzed a to-
tal of 210 of the incorrect instances labeled by the
T-Att BiLSTM-CRF system. The main error pat-
terns include:

1) Focus is the negative trigger (50/210, 23.8%).
According to the annotation guideline of nega-
tive focus (Blanco and Moldovan, 2011), when
a negated statement does not carry any positive
meaning, or if it cannot be inferred that a part of
sentence is negated, the gold standard annotation
regards the negative trigger (e.g., not) itself as the
negative focus. In this case, neither the local infor-
mation nor the contextual information could iden-
tify the negative focus. It is difficult to accurately
identify such negative focus by our approach.

2) Annotation issue (84/210, 40%). As men-
tioned in the above, a negative focus always cor-
responds a single semantic role. However, we find
that some of labeled instances do not meet this
constraint. Such non-conformity contains three
aspects: a) the focus has at least one word that
does not belong to any role (35/84), b) the focus
is labeled as a fragment of semantic role (36/84),
and c) the focus is labeled across more than one
roles (13/84). These situations may heavily limit
the effects of the semantic features in embedding
layer and the CRF layer which learns the depen-



2259

dencies across output labels. To verify the impact
of this issue, we conduct the T-Att BiLSTM-CRF
system on the processed test set that remove the 84
of instances. The performance achieves 79.94 in
accuracy, yielding an improvement of 9.43% (Ta-
ble 2). Therefore, the future work should fix the
issue of corpus, or reconsider whether the guide-
lines could generalize the linguistic phenomenon
of negative focus.

3) Subjectivity of the annotation process
(11/210, 5.2%). There are several golden anno-
tated instances that we disagree with. For exam-
ple,

(S2) Previous sentence: ..., one equipment failure
forces a complete plant shutdown.

(S3) {On some days}, the Nucor plant doesnt pro-
duce anything.10

Consider the previous sentence S2, the author em-
phasize that the Nucor plant will shut down, thus
the most likely negative focus should be any-
thing in S3, whereas the prepositional phrase on
some days only provides a temporal statement in
our view. However, different annotators might
have different understandings and interpretations
for this case. Note that the inter-annotator agree-
ment of the corpus is only 0.72 after all, which
indicates the inherent challenge in negative focus
detection task.

5 Conclusion

In this work, we discussed the unique necessity
of modeling contextual information for negative
focus detection. We designed an attention based
neural architecture which can better capture con-
textual features by utilizing the word-level and
topic-level attention mechanisms. Experiments
on the SEM’12 shared task corpus demonstrate
the effectiveness of our approach. The datasets
and source code of this paper are publicly avail-
able at http://github.com/longxshen/
NegFocus.

In the future, we intend to review the annotated
guidelines for negative focus with the problem-
atic annotation cases mentioned in this paper, and
explore the ways to combine additional patterns
and constraints from negative focus definition with
neural network techniques to further improve neg-
ative focus detection.

10#287 of test set in SEM’12 shared task corpus.

Acknowledgements

This research is supported by the National Natural
Science Foundation of China (No. 61703293, No.
61751206, and No. 61672368). This work was
also supported by the joint research project of Al-
ibaba and Soochow University. Finally, we would
like to thank the anonymous reviewers for their in-
sightful comments and suggestions.

References
Pranav Anand and Craig Martell. 2012. Annotating the

focus of negation in terms of questions under discus-
sion. In Proceedings of the ACL-2012 Workshop on
Extra-Propositional Aspects of Meaning in Compu-
tational Linguistics (ExProM), pages 65–69. Asso-
ciation for Computational Linguistics.

Emilia Apostolova, Noriko Tomuro, and Dina Demn-
erfushman. 2011. Automatic extraction of lexico-
syntactic patterns for detection of negation and spec-
ulation scopes. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 283–287. Association for
Computational Linguistics.

Rajendra Banjade and Vasile Rus. 2016. Dt-neg: Tu-
torial dialogues annotated for negation scope and
focus in context. In Proceedings of Language Re-
sources and Evaluation Conference (LREC), pages
3768–3771.

Eduardo Blanco and Dan Moldovan. 2011. Semantic
representation of negation using focus detection. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
581–589. Association for Computational Linguis-
tics.

Eduardo Blanco and Dan Moldovan. 2013. Retrieving
implicit positive meaning from negated statements.
Natural Language Engineering (NLE), 20(4):501–
535.

Roger Bradford. 2008. An empirical study of required
dimensionality for large-scale latent semantic index-
ing applications. In Proceedings of the 17th Confer-
ence on Information and Knowledge Management
(CIKM), pages 153–162.

Wendy W Chapman, Will Bridewell, Paul Hanbury,
Gregory F Cooper, and Bruce G Buchanan. 2001.
A simple algorithm for identifying negated findings
and diseases in discharge summaries. Journal of
Biomedical Informatics, 34(5):301–310.

Noa P Cruz, Maite Taboada, and Ruslan Mitkov. 2016.
A machine-learning approach to negation and spec-
ulation detection for sentiment analysis. Journal of
the Association for Information Science and Tech-
nology, 67(9):2118–2136.

http://github.com/longxshen/NegFocus
http://github.com/longxshen/NegFocus
http://www.aclweb.org/anthology/W12-3808
http://www.aclweb.org/anthology/W12-3808
http://www.aclweb.org/anthology/W12-3808
http://www.aclweb.org/anthology/P/P11/P11-2049.pdf
http://www.aclweb.org/anthology/P/P11/P11-2049.pdf
http://www.aclweb.org/anthology/P/P11/P11-2049.pdf
http://www.lrec-conf.org/proceedings/lrec2016/pdf/119_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2016/pdf/119_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2016/pdf/119_Paper.pdf
http://www.aclweb.org/anthology/P/P11/P11-1059.pdf
http://www.aclweb.org/anthology/P/P11/P11-1059.pdf
https://doi.org/10.1017/S1351324913000041
https://doi.org/10.1017/S1351324913000041
http://cll.stanford.edu/~willb/publications/chapman_jbmi01.pdf
http://cll.stanford.edu/~willb/publications/chapman_jbmi01.pdf
https://doi.org/10.1002/asi.23533
https://doi.org/10.1002/asi.23533


2260

Ilya M. Goldin and Wendy W. Chapman. 2003. Learn-
ing to detect negation with not in medical texts. In
Proceedings of the Workshop on Text Analysis and
Search for Bioinformatics of SIGIR.

Sepp Hochreiter and Jurgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press.

Salud Maria Jimenez-Zafra, M Teresa Martin Valdivia,
Eugenio Martinez Camara, and Luis Alfonso Ure-
nalopez. 2017. Studying the scope of negation for
spanish sentiment analysis on twitter. IEEE Trans-
actions on Affective Computing.

John D. Lafferty, Andrew Mccallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Eighteenth International Confer-
ence on Machine Learning, pages 282–289.

Suguru Matsuyoshi, Ryo Otsuki, and Fumiyo Fuku-
moto. 2014. Annotating the focus of negation in
japanese text. In Proceedings of Language Re-
sources and Evaluation Conference (LREC), pages
1743–1750.

Saeed Mehrabi, Anand Krishnan, Sunghwan Sohn,
Alexandra M Roch, Heidi Schmidt, Joe Kesterson,
Chris Beesley, Paul Dexter, C Max Schmidt, Hong-
fang Liu, et al. 2015. Deepen: A negation detection
system for clinical text incorporating dependency re-
lation into negex. Journal of Biomedical Informat-
ics, 54:213–219.

Roser Morante. 2010. Descriptive analysis of nega-
tion cues in biomedical texts. In Proceedings of the
Seventh conference on International Language Re-
sources and Evaluation (LREC), pages 1429–1436.

Roser Morante and Eduardo Blanco. 2012. *sem 2012
shared task: Resolving the scope and focus of nega-
tion. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics (*SEM),
pages 256–274. Association for Computational Lin-
guistics.

Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the scope of negation in
biomedical texts. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 715–724. Association
for Computational Linguistics.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. pages 2227–2237.

Zhong Qian, Peifeng Li, Qiaoming Zhu, Guodong
Zhou, Zhunchen Luo, and WeiLuo. 2016. Specula-
tion and negation scope detection via convolutional
neural networks. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 815–825.

Radim Rehurek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Valletta,
Malta. ELRA.

Sabine Rosenberg and Sabine Bergler. 2012. Uconcor-
dia: Clac negation focus detection at *sem 2012. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics (*SEM), pages 294–
300. Association for Computational Linguistics.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 conference on
empirical methods in natural language processing
(EMNLP), pages 1631–1642. Association for Com-
putational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of the 31st Conference
on Neural Information Processing Systems (NIPS),
pages 5998–6008.

Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The bioscope
corpus: biomedical texts annotated for uncertainty,
negation and their scopes. BMC Bioinformatics,
9(11):1–9.

Yequan Wang, Minlie Huang, Xiaoyan Zhu, and
Li Zhao. 2016. Attention-based lstm for aspect-level
sentiment classification. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 606–615. Asso-
ciation for Computational Linguistics.

Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andres Montoyo. 2010. A
survey on the role of negation in sentiment analy-
sis. In Proceedings of the Workshop on Negation
and Speculation in Natural Language Processing,
pages 60–68.

Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
based bidirectional long short-term memory net-
works for relation classification. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 207–212. Asso-
ciation for Computational Linguistics.

Bowei Zou, Guodong Zhou, and Qiaoming Zhu. 2013.
Tree kernel-based negation and speculation scope

http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.2573&rep=rep1&type=pdf
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.2573&rep=rep1&type=pdf
https://doi.org/10.1162/neco.1997.9.8.1735
https://doi.org/10.1109/TAFFC.2017.2693968
https://doi.org/10.1109/TAFFC.2017.2693968
http://cs.nju.edu.cn/daixinyu/CRF.pdf
http://cs.nju.edu.cn/daixinyu/CRF.pdf
http://cs.nju.edu.cn/daixinyu/CRF.pdf
http://www.lrec-conf.org/proceedings/lrec2014/pdf/777_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2014/pdf/777_Paper.pdf
https://doi.org/10.1016/j.jbi.2015.02.010
https://doi.org/10.1016/j.jbi.2015.02.010
https://doi.org/10.1016/j.jbi.2015.02.010
http://www.lrec-conf.org/proceedings/lrec2010/pdf/335_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2010/pdf/335_Paper.pdf
http://www.aclweb.org/anthology/S12-1035
http://www.aclweb.org/anthology/S12-1035
http://www.aclweb.org/anthology/S12-1035
http://www.aclweb.org/anthology/D/D08/D08-1075.pdf
http://www.aclweb.org/anthology/D/D08/D08-1075.pdf
http://www.aclweb.org/anthology/J05-1004
http://www.aclweb.org/anthology/J05-1004
http://aclweb.org/anthology/N18-1202
http://aclweb.org/anthology/N18-1202
http://aclweb.org/anthology/D/D16/D16-1078.pdf
http://aclweb.org/anthology/D/D16/D16-1078.pdf
http://aclweb.org/anthology/D/D16/D16-1078.pdf
https://radimrehurek.com/gensim/lrec2010_final.pdf
https://radimrehurek.com/gensim/lrec2010_final.pdf
http://www.aclweb.org/anthology/S/S12/S12-1039.pdf
http://www.aclweb.org/anthology/S/S12/S12-1039.pdf
http://www.aclweb.org/anthology/D13-1170
http://www.aclweb.org/anthology/D13-1170
http://www.aclweb.org/anthology/D13-1170
https://arxiv.org/pdf/1706.03762.pdf
https://arxiv.org/pdf/1706.03762.pdf
https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-9-S11-S9?site=bmcbioinformatics.biomedcentral.com
https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-9-S11-S9?site=bmcbioinformatics.biomedcentral.com
https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-9-S11-S9?site=bmcbioinformatics.biomedcentral.com
http://aclweb.org/anthology/D/D16/D16-1058.pdf
http://aclweb.org/anthology/D/D16/D16-1058.pdf
http://www.aclweb.org/anthology/W10-3111
http://www.aclweb.org/anthology/W10-3111
http://www.aclweb.org/anthology/W10-3111
http://aclweb.org/anthology/P/P16/P16-2034.pdf
http://aclweb.org/anthology/P/P16/P16-2034.pdf
http://aclweb.org/anthology/P/P16/P16-2034.pdf
http://www.aclweb.org/anthology/D13-1099


2261

detection with structured syntactic parse features. In
Proceedings of the 2013 conference on empirical
methods in natural language processing (EMNLP),
pages 968–976. Association for Computational Lin-
guistics.

Bowei Zou, Guodong Zhou, and Qiaoming Zhu. 2014.
Negation focus identification with contextual dis-
course information. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 522–530. Association for
Computational Linguistics.

Bowei Zou, Qiaoming Zhu, and Guodong Zhou.
2015. Unsupervised negation focus identification
with word-topic graph model. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1632–1636.
Association for Computational Linguistics.

http://www.aclweb.org/anthology/D13-1099
https://doi.org/10.3115/v1/P14-1049
https://doi.org/10.3115/v1/P14-1049
http://www.aclweb.org/anthology/D15-1187
http://www.aclweb.org/anthology/D15-1187

