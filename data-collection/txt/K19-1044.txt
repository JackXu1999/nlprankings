



















































Active Learning via Membership Query Synthesis for Semi-Supervised Sentence Classification


Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 472–481
Hong Kong, China, November 3-4, 2019. c©2019 Association for Computational Linguistics

472

Active Learning via Membership Query Synthesis
for Semi-supervised Sentence Classification

Raphael Schumann
Institute for Computational Linguistics

Heidelberg University, Germany
rschuman@cl.uni-heidelberg.de

Ines Rehbein
Leibniz ScienceCampus
Heidelberg/Mannheim

rehbein@ids-mannheim.de

Abstract
Active learning (AL) is a technique for reduc-
ing manual annotation effort during the an-
notation of training data for machine learn-
ing classifiers. For NLP tasks, pool-based and
stream-based sampling techniques have been
used to select new instances for AL while gen-
erating new, artificial instances via Member-
ship Query Synthesis was, up to know, con-
sidered to be infeasible for NLP problems.
We present the first successful attempt to use
Membership Query Synthesis for generating
AL queries for natural language processing,
using Variational Autoencoders for query gen-
eration. We evaluate our approach in a text
classification task and demonstrate that query
synthesis shows competitive performance to
pool-based AL strategies while substantially
reducing annotation time.

1 Introduction

Active learning (AL) has the potential to sub-
stantially reduce the amount of labeled instances
needed to reach a certain classifier performance
in supervised machine learning. It works by se-
lecting new instances that are highly informative
for the classifier, so that comparable classifica-
tion accuracies can be obtained on a much smaller
training set. AL strategies can be categorized into
pool-based sampling, stream-based sampling and
Membership Query Synthesis (MQS). The first
two strategies sample new instances either from
a data pool or from a stream of data. The third,
MQS, generates artificial AL instances from the
region of uncertainty of the classifier. While it is
known that MQS can reduce the predictive error
rate more quickly than pool-based sampling (Ling
and Du, 2008), so far it has not been used for NLP
tasks because artificially created textual instances
are uninterpretable for human annotators.

We provide proof of concept that generating
highly informative artificial training instances for

text classification is feasible. We use Variational
Autoencoders (VAE) (Kingma and Welling, 2013)
to learn representations from unlabeled text in an
unsupervised fashion by encoding individual sen-
tences as low-dimensional vectors in latent space.
In addition to mapping input sequences into la-
tent space, the VAE can also learn to generate new
instances from this space. We utilize these abili-
ties to generate new examples for active learning
from a region in latent space where the classifier is
most uncertain, and hand them over to the annota-
tor who then provides labels for the newly created
instances.

We test our approach in a text classification
setup with a real human annotator in the loop. Our
experiments show that query synthesis for NLP
is not only feasible but can outperform other AL
strategies in a sentiment classification task with re-
spect to annotation time.

The paper is structured as follows. We first re-
view related work (§2) and introduce a formal de-
scription of the problem (§3). Then we describe
our approach (§4), present the experiments (§5)
and analyze the results (§6). We discuss limita-
tions and possible further experiments (§7) and fi-
nally conclude our findings (§8).

2 Related work

Membership query synthesis was introduced by
Angluin (1988) and describes a setting where the
model generates new queries instead of selecting
existing ones. Early experiments in image pro-
cessing (Lang and Baum, 1992), however, showed
that the generated queries are hard to interpret
by human annotators. This holds true even for
recent approaches using Generative Adversarial
Networks (GANs) (Goodfellow et al., 2014) to
create uncertain instances (Zhu and Bento, 2017;
Huijser and van Gemert, 2017). In contrast to im-



473

age processing, discrete domains like natural lan-
guage do not exhibit a direct mapping from feature
to instance space. Strategies that circumvent this
problem include the search for nearest (observed)
neighbors in feature space (Wang et al., 2015) or
crafting queries by switching words (Awasthi and
Kanade, 2012).

Sentence representation learning (Kiros et al.,
2015; Conneau et al., 2017; Subramanian et al.,
2018; Wang et al., 2019) in combination with new
methods for semi-supervised learning (Kingma
et al., 2014; Hu et al., 2017; Xu et al., 2017;
Odena, 2016; Radford et al., 2017) have shown
to improve classification tasks by leveraging un-
labeled text. Methods based on deep generative
models like GANs or VAEs are able to generate
sentences from any point in representation space.
Mehrjou et al. (2018) use VAEs to learn structural
information from unlabeled data and use it as an
additional criterion in conventional active learning
to make it more robust against outliers and noise.

We use VAEs to generate AL queries from spe-
cific regions in latent space. To ensure that the
generated instances are not only informative for
the ML classifier but also meaningful for the hu-
man annotator, we adapt the approach of Wang
et al. (2015) (see §3.1). In contrast to their work,
however, we do not sample existing instances from
the pool that are similar to the synthetic ones but
directly generate the new queries. To our best
knowledge, our work is the first to present posi-
tive results for Membership Query Synthesis for
text classification.

3 Background

3.1 Query Synthesis and Nearest Neighbors

Arbitrary points in feature space are hard to in-
terpret for humans. To evade this problem, Wang
et al. (2015) use the nearest neighbor in a pool of
unlabeled data as a representative which is then
presented to the human annotator. To identify un-
certain points along the separating hyperplane of
an SVM the following approach is proposed. First
the location of the decision boundary is approxi-
mated by a binary-search like procedure. An ini-
tial Opposite Pair (z+, z−) is formed by centroid
c+ and centroid c− of positive and negative la-
beled instances respectively. The mid point zs
is queried and, depending on the annotated label
l, replaces the corresponding zl. This step is re-
peated b times, reducing the distance between the

Figure 1: a) finds Opposite Pair close to the deci-
sion boundary. b) identify points close to the decision
boundary.

initial centroids by a factor of 2b. Figure 1a depicts
this process. Then the mid-perpendicular vector of
the Opposite Pair is calculated by using the Gram-
Schmidt process to orthogonalize a random vec-
tor zr and normalize its magnitude to λ. The new
point zs = zr + (z+ + z−)/2 is close to the deci-
sion boundary and queried for its class. Depend-
ing on the receive label the point zs replaces z+ or
z− in the Opposite Pair. This process (Figure 1b)
is repeated until n− b points along the separating
hyperplane are queried.

3.2 VAE for Sentence Generation

The Variational Autoencoder is a generative model
first introduced by Kingma and Welling (2013).
Like other autoencoders, VAEs learn a mapping
qθ(z|x) from high dimensional input x to a low di-
mensional latent variable z. Instead of doing this
in a deterministic way, the encoder learns the pa-
rameters of e.g. a normal distribution. The de-
sired effect is that each area in the latent space has
a semantic meaning and thus samples from p(z)
can be decoded in a meaningful way. The decoder
pθ(x|z), also referred to as dec(z), is trained to re-
construct the input x based on the latent variable
z. In order to approximate θ via gradient descent
the reparametrization trick (Kingma and Welling,
2013) was introduced. This trick allows the gradi-
ent to flow through non-deterministic z by separat-
ing the discrete sampling operation. Let µ and σ
be deterministic outputs of the encoder qθ(µ, σ|x):

z = µ+ σ � � where � ∼ N (0, I) (1)

and � is the element-wise product. To prevent the
model from pushing σ close to 0 and thus falling
back to a deterministic autoencoder, the objective
is extended by the Kullback-Leibler (KL) diver-



474

gence between prior p(z) and q(z|x):
L(θ;x) = −KL(qθ(z|x)||p(z))

+Eqθ(z|x)[logpθ(x|z)].
(2)

Bowman et al. (2016) apply this idea for sen-
tence generation using an RNN as encoder and de-
coder. They observe that a strong auto-regressive
language modeling ability in the decoder reduces
the information stored in the latent variable, right
up to a complete collapse of the KL term. They ex-
plore different techniques to weaken the decoder,
like word dropout or KL term weight annealing, as
possible solutions. This guarantees a semantically
rich latent variable and good sentence generation
ability. Below, we describe how to combine both
techniques in order to generate meaningful queries
for Membership Query Synthesis.

4 Active Learning Schedule

We train a Variational Autoencoder on an unla-
beled corpus of sentences. The text classification
task is performed on a binary sentiment dataset
split into training, development and test set. As de-
picted in Figure 2, the sentences in the classifica-
tion dataset are vectorized using the VAE encoder
which generates the latent variable z for each sen-
tence x. This is done deterministically by drop-
ping the σ term in Equation 1, further referred to
as z = enc(x).

Next, a Learner is trained to fit a linear hyper-
plane to separate the positive from the negative in-
stances. We use the procedure described in §3.1
to select new query points for AL. But instead of
searching for the nearest neighbor in the pool, we
decode the point x = dec(z) into a human read-
able sentence which is then handed over to the hu-
man annotator. The annotator assigns a binary la-
bel to the instance and the next query point is cal-
culated.

One important parameter for active learning de-
termines how many new instances are to be se-
lected in each AL iteration. Wang et al. (2015)
use a predefined number of instances to be se-
lected along the hyperplane. Because we know
that a Gaussian prior is imposed on the fea-
ture space, we instead stop the selection process
when the magnitude of zs exceeds the expectation.
The expected distance of a point sampled from
the k-dimensional Gaussian prior to the origin is√
E[χ2k] =

√
k. Then the schedule restarts, learn-

ing a new decision boundary, and ultimately ter-

Figure 2: a) Instances in corpus are encoded to latent
space. b) Learner fits a hyperplane to separate points.
c) Query points selected by method described in Fig.1.
Decoder translates point in latent space to human read-
able sequence. Annotator chooses label for instance.

minates when the annotation budget is exhausted.
We refer to this method as gen wang. When near-
est neighbor search is used instead, we refer to the
selection method as nn wang.

In addition, we explore a method, gen uniform,
where step b) in Figure 1 is reduced to generating
only one midperpendicular vector with a magni-
tude drawn from a uniform distribution. In each
iteration this vector will point to a random direc-
tion with a different magnitude, selecting diverse
points close to the hyperplane. The maximum
magnitude is set in a way that the resulting point
is not further away than

√
k from the origin. Simi-

lar to above we refer to this method as nn uniform
when using nearest neighbor search. The number
of possible directions along the hyperplane grows
with the size of the latent variable. With this mod-
ification we expect to explore more diverse points
than following the same direction for several steps.

5 Experiments

In this section we want to explore how the abil-
ity to generate human readable sentences from ar-
bitrary points in the feature space affects active
learning performance. We compare our approach
to a number of baselines (§5.3), where in each ex-
periment we select/generate 500 instances, present
them to a human annotator to get a label and eval-
uate the performance of each setting in a senti-
ment classification task. We start the active learn-
ing process with two utterances in the seed set,
namely ’good movie’ and ’bad movie’. The clas-
sifier is trained to separate instances with positive
sentiment from negative ones. The human anno-



475

Parameter Value
vocabulary size 20.000
RNN cell size 512
embedding size 512
latent variable size 50
dropout 0.3
dropword 0.5
learning rate 0.005
epochs 20

Table 1: Training parameters for the Variational Au-
toencoder.

tator can skip neutral or uninterpretable instances.
These skip actions also count towards the annota-
tion budget.

5.1 Data

The data used in our experiments comes from two
sources, (i) the SST2 (Socher et al., 2013) and (ii)
SAR14 (Nguyen et al., 2014). We limit sentence
length to a maximum of 15 words. This is moti-
vated by lower training times and the tendency of
vanilla VAEs not to perform well on longer sen-
tences (Shen et al., 2019).

Sentiment task SST2 (Socher et al., 2013) is
a binary sentiment classification dataset compiled
from rottentomatoes.com. As we only consider
sentences with up to 15 words, the sizes of the
training, development and test sets are 3103, 380
and 814 instances, respectively.

Sentence pool The active learning pool consists
of 1.2M unique sentences from the SAR14 dataset
(Nguyen et al., 2014). SAR14 contains 234k
movie reviews from IMDB. The data is annotated
on review level, which prevents us from remov-
ing single neutral sentences. Although the datasets
stem from different sources, there is a small over-
lap. These sentences are removed from the pool.

5.2 Training

Variational Autoencoder Table 1 lists the pa-
rameter used for the VAE. For training we limit
the vocabulary of the VAE to the top 20k words.
Encoder and decoder RNN have layer normal-
ized (Ba et al., 2016) LSTM cells (Hochreiter and
Schmidhuber, 1997) with size 512. As additional
regularization we set weight dropout to 0.3 (Sri-
vastava et al., 2014). Input embeddings are also
of size 512, which allows us to share the embed-

ding weights with the softmax weights of the out-
put layer (Press and Wolf, 2016). To prevent pos-
terior collapse we use logistic annealing of the KL
term weight and weaken the decoder by apply-
ing word dropout with probability 0.5 (Bowman
et al., 2016). The model is trained using the Adam
optimizer (Kingma and Ba, 2014) with an initial
learning rate of 0.005. Once the KL term weight
is close to 1, the learning weight is linearly de-
creased to 0. The training stops after 20 epochs
and the latent variable z has k = 50 dimensions.
The trained VAE achieves a reconstruction loss of
45.3 and KL divergence of 13.2 on the SST2 train-
ing set.

Learner The Learner is an SVM1 with linear
kernel. Each instance is represented as the latent
variable z learned by the autoencoder. The latent
variable is a vector with 50 dimensions and the
SVM is trained on this representation. We cal-
culate classification performance on the reduced
SST2 test set and report F1-scores.

Generator The generator is the decoder of the
VAE described above. Once a point z in feature
space is selected, it is used as the input of the
decoder x = dec(z) which generates the human
readable sentence x in an autoregressive way.

5.3 Baselines
We compare our approach to Membership Query
Synthesis for text classification to four baselines.
The first baseline selects instances from the pool
by random choice. The least confidence base-
line computes the distance of the instances in the
pool to the separating hyperplane and chooses
the one closest to the hyperplane. The third and
fourth baseline follow the procedure described in
§4 but search for the nearest neighbor (nn uniform,
nn wang) instead of synthesising the exact query
point. Nearest neighbor is defined by the minimal
euclidean distance between the query point and the
latent representation of the pool instance.

5.4 Annotation
The instances selected or generated by any model
or baseline are annotated manually by one human
coder.2 Although the pool data has labels on the
review level, we do not use these labels in our ex-
periments. Positive reviews can include negative

1https://scikit-learn.org/stable/
modules/generated/sklearn.svm.SVC.html

2The first author of this paper.

https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html
https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html


476

Figure 3: F1-Score as a function of annotation steps
(including skipped queries). Averaged over 3 runs.

sentences and vice versa. This means that using
document-level labels would introduce noise and
might impair the baselines. During each of the
three experimental runs, all models and baselines
are annotated simultaneously by the same person.
The annotator is presented with one instance at a
time and has no information which of the models
has produced each particular instance. Once a la-
bel is selected, it is transmitted to the correspond-
ing model and triggers the selection/generation of
the next instance. Thus, at any given time there is
one unlabeled instance for each model or baseline.
From this set of unlabeled instances, one instance
is chosen randomly and presented to the annota-
tor. This procedure is repeated until 500 instances
are labeled for each model or baseline. Hiding the
instance source from the annotator is intended to
prevent any bias during the annotation process.

6 Results and Analysis

6.1 Classification Performance
F-scores as a function of annotated instances
Figure 3 shows learning curves for the different
AL strategies and baselines as a function of the
number of annotation instances added to the train-
ing data. The random and least conf baselines
perform reasonably well. Least conf struggles
in the beginning, likely attributed to the minimal
seed set. Once enough instances are labeled it
catches up. Gen uniform has a strong start but,
after around 200 instances, is outperformed by the
nearest neighbor approaches which yield the high-
est F1-scores. Among the nearest neighbor ap-
proaches, the uniform schedule ranks better than
wang. The same behaviour is observed for the
generation methods, although gen wang produces

Figure 4: F1-scores as a function of annotation time.
Results averaged over 3 runs.

the worst results overall. Overall, gen uniform is
competitive with respect to F1-scores and shows
that sentences generated from points in the feature
space are informative and useful for training a text
classifier.

F-scores as a function of annotation time AL
simulations have often been criticized for report-
ing unrealistic results, based merely on the number
of annotated instances (see, e.g., Settles (2009),
pp. 37 ff.). It is well known, however, that the
number of annotated instances is often not a good
predictor for the real annotation costs. AL strate-
gies tend to select the hard nuts for human anno-
tators and it is not unreasonable to assume that the
annotation of N instances in an AL setup might
take longer and thus might be more expensive
than annotating the same number of randomly se-
lected instances. Therefore, we also show learning
curves as a function of annotation time (Figure 4).

The results show a clear advantage for the gen-
eration models. The reduction in annotation time
is due to shorter query length and less neutral or
noisy instances, as shown in Table 2. This speeds
up the annotation by a significant margin while
providing the Learner with informative instances,
despite their short length.

Figure 5 shows that the length of generated in-
stances increase over time and further exploration
also hints that the generated length is correlated
with the length of the sentences in the seed set.

As listed in Table 2, the random baseline reveals
that 36.8 percent of sentences in the pool are neu-
tral/artifacts and positive sentences outweigh neg-
ative ones by a factor of 2.6. This means that
random sampling results in unbalanced datasets
with far more positive examples. Our generation



477

Figure 5: Development of average length of se-
lected/generated instances as more instances are anno-
tated.

method does not show this disadvantage. In con-
trast, the generated instances maintain a more bal-
anced distribution of class labels and are less likely
to be skipped. These are indicators that the se-
lected points are close to the hyperplane and the
VAE is able to generate coherent and highly infor-
mative sentences from them.

6.2 Computational Complexity

To assure a seamless annotation procedure, the
supply of new instances has to be reasonably fast.
The generation and selection of the next instance
is dependant on the label of the previous instance.
Because of this, there is no way to pre-fetch the
next instance in the background and the annotator
has to wait for the selection/generation process to
finish before the next instance is presented for an-
notation. However, the runtime for pool-based AL
methods is increasing with the pool’s size. In con-
trast, the generation method presented in this work
does not have this limitation.

The least confidence baseline has a complexity
of O(n) where n is the number of instances in the
pool. The complexity of nearest neighbor search
without any approximation techniques like pre-
clustering is also O(n). Query generation from
an exact point with the decoder has a complexity
ofO(m) wherem is the length of the sentence and
n >> m. Because sentences have a natural length
limit and in this work are capped to 15 words, one
could argue that the complexity is O(1).

6.3 Generated Instances

Table 3 shows examples of generated instances us-
ing the gen uniform method. Example 1-6 show

% skips M sec M len p/n
gen uniform 28.1 1.4 4 1.7
gen wang 20.9 1.9 5 1.2
nn uniform 34.2 4.1 9 1.9
nn wang 35.8 4.1 10 2.4
least conf 39.0 4.2 10 2.1
random 36.8 4.1 9 2.6

Table 2: Percentage of skips (neutral or noisy sen-
tences); Median annotation time in seconds; Median
number of words in query; Ratio of positive to negative
labels.

prototypical positive and negative instances. Ex-
ample 7 is ambiguous, caused by the decoder gen-
erating an unknown (UNK) token at the position
where one would normally expect an evaluative
adjective. We see this as an indicator that the
point is positioned close to the hyperplane and
thus the sentiment of the latent variable is ambigu-
ous. We also observe instances with UNK token
which still express a sentiment, as seen in Exam-
ple 8 an 9. This can be interpreted as a placeholder
for a named entity or, in other cases, a specifier
like movie genre and does not impact the annota-
tion process.

To explore the ability of the model to generate
unseen instances we calculate the percentage of in-
stances not seen in the pool. We only look at in-
stances with an annotated sentiment label, because
skipped examples often include noise and thus are
unlikely to be present in the pool. 41 and 51 per-
cent of labeled instances are newly generated by
gen uniform and gen wang respectively. This pro-
vides more evidence that the model is capable of
generating new and informative instances.

No. Instance Label
1. the acting is excellent 1
2. powerful and moving 1
3. this movie is very enjoyable 1
4. a complete mess 0
5. nothing spectacular 0
6. absolutely terrible ! 0
7. the plot is UNK skip
8. well done by UNK 1
9. the UNK is a disappointment 0

Table 3: Example instances generated by gen uniform.
Label 1 for positive and 0 for negative class.



478

Figure 6: Plot of the 2 most important dimensions
of selected/generated instances in latent space. Gray
points indicate negative, black points positive labels.
The blue square denotes ’bad movie’ and the red cross
’good movie’.

6.4 Latent Space

To further analyze the behavior of the different AL
strategies, we apply dimensionality reduction and
visualize the instances in latent space (Figure 6).

The two largest absolute coefficients of the
trained SVM’s linear kernel identify the most im-
portant dimensions. Figure 6 plots the points, rep-
resented by theses two dimensions, selected by
different active learning schedules. The generated
instances lie densely around the seed points, while
pool instances are more distributed. In gen wang
one can see how the instances are loosely follow-
ing one direction similar to Figure1.

As indicated in Figure 2 a pool instance is rep-
resented as z = enc(x). The same is true for
the instances in the development, test and seed
set. For the generated instances there are two op-
tions. If z is a point selected in feature space and
x = dec(z) is the decoded query sequence, the an-
notated instance can either be represented as z or
as ẑ = enc(x). In a perfect VAE z and ẑ should
be nearly identical. In practice however ẑ ends
up at a different location in feature space. Fig-
ure 7 depicts the distribution of distances between
z and ẑ generated with the gen uniform method.
We observe that models trained on ẑ perform bet-
ter than those trained on z, presumably because
the test instances are represented the same way.
To evaluate if ẑ is still an informative point and
not just positioned randomly in feature space, we
train a model on actual randomly sampled points.
The sampled point z ∼ N (0, I) is decoded to
query sequence x, labeled and subsequently re-

Figure 7: Distribution of euclidean distances between
z before and ẑ after re-encoding during gen uniform.

encoded to ẑ = enc(x). With the same amount of
instances, this model performs much worse than
gen uniform, indicating that point ẑ still preserves
some of the informativeness of z. We thus assume
that the closer ẑ is to selected point z, the better
the generation based active learning schedules will
work.

7 Discussion

Related work in the context of semi-supervised
learning has focused on developing methods to
generate synthetic training instances for different
tasks (Sennrich et al., 2016; Hayashi et al., 2018;
Alberti et al., 2019; Winata et al., 2019), in order
to accelerate the learning process. Sennrich et al.
(2016) create artificial training instances for ma-
chine translation, using monolingual data paired
with automatic back-translations. Their work ob-
tains substantial improvements for several lan-
guages and has triggered many follow-up studies
that apply the idea of back-translation to different
tasks.

For example, Hayashi et al. (2018) augment the
training data for attention-based end-to-end auto-
matic speech recognition with synthetic instances,
and Winata et al. (2019) generate artificial train-
ing examples to improve automatic speech recog-
nition on code-switching material. Alberti et al.
(2019) use a large number of synthetic instances to
pre-train a Question Answering (QA) model that
is then fine-tuned on the target QA dataset. Their
approach results in significant improvements over
models that are trained without the synthetic data-
points.

While these studies show that huge amounts of
synthetic training data can crucially improve the



479

learning process, our approach uses a different
paradigm. Instead of generating millions of syn-
thetic data points, our method is data-lean and only
needs a few hundred instances to improve the clas-
sifier. Another difference is that we do not rely on
automatically generated labels but use human an-
notations instead. Due to the practical constraints
of the active learning process, we need to keep the
training time short enough so that the human an-
notator does not have to wait for the next set of
instances to annotate. This rules out the use of
computation-intensive models and large training
sets. Given that we use an SVM for classification,
we do not expect a strong effect for adding large
numbers of additional training instances, given
that the majority of those data points will not be
positioned close to the decision boundary.

One of the main drawbacks of our work is its
limitation to binary sentence classification. How-
ever, multi-class classification in an one-vs-rest
schema is compatible with our method and worth
further exploration. Another interesting direction
for future work is the synthesis of data for more
complex tasks like Natural Language Inference
(NLI) or QA. This, however, requires modifica-
tions to the structure of the autoencoder and ex-
ceeds the scope of this work.

Membership Query Synthesis might also be an
interesting approach for tasks where the automatic
extraction of large amounts of unlabelled data is
not straight-forward. One example that comes
to mind is the detection of offensive language or
’hate speech’, where we have to deal with highly
unbalanced training sets with only a small number
of positive instances, and attempts to increase this
number have been shown to result in systemati-
cally biased datasets (Davidson et al., 2019; Wie-
gand et al., 2019). Table 2 suggests that the gen-
erator produces instances with a more balanced
class ratio (1.7 and 1.2) than the pool data (2.6)
it was trained on. It might be worthwhile to ex-
plore whether the generation of synthetic training
instances can help to mitigate the problem to se-
lect instances from both classes in an highly im-
balanced data pool.

8 Conclusion

This work is the first to show that Membership
Query Synthesis in an NLP setting is feasible. Our
approach uses a Variational Autoencoder as a rep-
resentation learner and generates informative ac-

tive learning queries from latent space. The classi-
fication performance for the generated instances is
competitive with pool-based active learning strate-
gies and outperforms other AL strategies with re-
gard to annotation cost (time) and computational
complexity.

The main advantage of Membership Query Syn-
thesis for active learning is that it allows us to
target specific points along the separating hyper-
plane and thus to provide the classifier with in-
formation on specific areas of uncertainty in the
data space. While pool-based active learning has
the same objective, Membership Query Synthesis
gives us a more precise tool to explore the data
space and to generate exactly those instances that
we need, making MQS a promising approach for
future work in active learning.

Acknowledgments

Part of this research has been conducted within
the Leibniz Science Campus “Empirical Linguis-
tics and Computational Modeling”, funded by the
Leibniz Association under grant no. SAS-2015-
IDS-LWC and by the Ministry of Science, Re-
search, and Art (MWK) of the state of Baden-
Württemberg.

References
Chris Alberti, Daniel Andor, Emily Pitler, Jacob De-

vlin, and Michael Collins. 2019. Synthetic QA cor-
pora generation with roundtrip consistency. In The
57th Conference of the Association for Computa-
tional Linguistics, ACL 2019, pages 6168–6173.

Dana Angluin. 1988. Queries and concept learning.
Machine Learning, 2(4):319–342.

Pranjal Awasthi and Varun Kanade. 2012. Learning us-
ing local membership queries under smooth distribu-
tions. CoRR, abs/1211.0996.

Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton.
2016. Layer normalization. CoRR, abs/1607.06450.

Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-
drew Dai, Rafal Jozefowicz, and Samy Bengio.
2016. Generating sentences from a continuous
space. In Proceedings of The 20th SIGNLL Con-
ference on Computational Natural Language Learn-
ing, pages 10–21, Berlin, Germany. Association for
Computational Linguistics.

Alexis Conneau, Douwe Kiela, Holger Schwenk,
Loı̈c Barrault, and Antoine Bordes. 2017. Su-
pervised learning of universal sentence representa-
tions from natural language inference data. CoRR,
abs/1705.02364.

https://www.aclweb.org/anthology/P19-1620/
https://www.aclweb.org/anthology/P19-1620/
https://doi.org/10.1023/A:1022821128753
http://arxiv.org/abs/1211.0996
http://arxiv.org/abs/1211.0996
http://arxiv.org/abs/1211.0996
http://arxiv.org/abs/1607.06450
http://www.aclweb.org/anthology/K16-1002
http://www.aclweb.org/anthology/K16-1002
http://arxiv.org/abs/1705.02364
http://arxiv.org/abs/1705.02364
http://arxiv.org/abs/1705.02364


480

Thomas Davidson, Debasmita Bhattacharya, and Ing-
mar Weber. 2019. Racial bias in hate speech and
abusive language detection datasets. In The Third
Workshop on Abusive Language Online, pages 25–
35.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative
adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger,
editors, Advances in Neural Information Processing
Systems 27, pages 2672–2680. Curran Associates,
Inc.

Tomoki Hayashi, Shinji Watanabe, Yu Zhang, Tomoki
Toda, Takaaki Hori, Ramón Fernández Astudillo,
and Kazuya Takeda. 2018. Back-translation-style
data augmentation for end-to-end ASR. In 2018
IEEE Spoken Language Technology Workshop, SLT
2018, pages 426–433.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780.

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P. Xing. 2017. Controllable
text generation. CoRR, abs/1703.00955.

Miriam W. Huijser and Jan C. van Gemert. 2017. Ac-
tive decision boundary annotation with deep gener-
ative models. CoRR, abs/1703.06971.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Diederik P. Kingma, Danilo Jimenez Rezende, Shakir
Mohamed, and Max Welling. 2014. Semi-
supervised learning with deep generative models.
CoRR, abs/1406.5298.

Diederik P. Kingma and Max Welling. 2013. Auto-
encoding variational bayes. CoRR, abs/1312.6114.

Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,
Richard S. Zemel, Antonio Torralba, Raquel Urta-
sun, and Sanja Fidler. 2015. Skip-thought vectors.
CoRR, abs/1506.06726.

Kevin Lang and Eric Baum. 1992. Query learning can
work poorly when a human oracle is used. IEEE
Intl. JointConference on Neural Networks.

Charles X. Ling and Jun Du. 2008. Active learning
with direct query construction. In The 14th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’08, pages 480–
487.

Arash Mehrjou, Mehran Khodabandeh, and Greg Mori.
2018. Distributionaware active learning. arXiv
preprint, abs/1805.08916.

Dai Quoc Nguyen, Dat Quoc Nguyen, Thanh Vu, and
Son Bao Pham. 2014. Sentiment classification on
polarity reviews: An empirical study using rating-
based features. In Proceedings of the 5th Workshop
on Computational Approaches to Subjectivity, Senti-
ment and Social Media Analysis, pages 128–135.

Augustus Odena. 2016. Semi-supervised learn-
ing with generative adversarial networks. CoRR,
abs/1606.01583.

Ofir Press and Lior Wolf. 2016. Using the output
embedding to improve language models. CoRR,
abs/1608.05859.

Alec Radford, Rafal Józefowicz, and Ilya Sutskever.
2017. Learning to generate reviews and discovering
sentiment. CoRR, abs/1704.01444.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation models
with monolingual data. In The 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2016.

Burr Settles. 2009. Active learning literature survey.
Technical report, Computer Sciences Technical Re-
port 1648, University of Wisconsin-Madison.

Dinghan Shen, Asli Celikyilmaz, Yizhe Zhang, Liqun
Chen, Xin Wang, and Lawrence Carin. 2019.
Hierarchically-structured variational autoencoders
for long text generation.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642. Association for Computa-
tional Linguistics.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. J. Mach. Learn. Res., 15(1):1929–
1958.

Sandeep Subramanian, Adam Trischler, Yoshua Ben-
gio, and Christopher J. Pal. 2018. Learning
general purpose distributed sentence representa-
tions via large scale multi-task learning. CoRR,
abs/1804.00079.

Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In Inter-
national Conference on Learning Representations.

Liantao Wang, Xuelei Hu, bo Yuan, and Jianfeng
Lu. 2015. Active learning via query synthesis
and nearest neighbour search. Neurocomputing,
147:426434.

http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf
http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf
https://doi.org/10.1109/SLT.2018.8639619
https://doi.org/10.1109/SLT.2018.8639619
https://doi.org/10.1162/neco.1997.9.8.1735
https://doi.org/10.1162/neco.1997.9.8.1735
http://arxiv.org/abs/1703.00955
http://arxiv.org/abs/1703.00955
http://arxiv.org/abs/1703.06971
http://arxiv.org/abs/1703.06971
http://arxiv.org/abs/1703.06971
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1406.5298
http://arxiv.org/abs/1406.5298
http://arxiv.org/abs/1312.6114
http://arxiv.org/abs/1312.6114
http://arxiv.org/abs/1506.06726
http://arxiv.org/abs/1805.08916
http://arxiv.org/abs/1608.05859
http://arxiv.org/abs/1608.05859
http://arxiv.org/abs/1704.01444
http://arxiv.org/abs/1704.01444
https://www.aclweb.org/anthology/P16-1009/
https://www.aclweb.org/anthology/P16-1009/
https://openreview.net/forum?id=Hk41X2AqtQ
https://openreview.net/forum?id=Hk41X2AqtQ
http://aclweb.org/anthology/D13-1170
http://aclweb.org/anthology/D13-1170
http://aclweb.org/anthology/D13-1170
http://dl.acm.org/citation.cfm?id=2627435.2670313
http://dl.acm.org/citation.cfm?id=2627435.2670313
http://arxiv.org/abs/1804.00079
http://arxiv.org/abs/1804.00079
http://arxiv.org/abs/1804.00079
https://openreview.net/forum?id=rJ4km2R5t7
https://openreview.net/forum?id=rJ4km2R5t7
https://doi.org/10.1016/j.neucom.2014.06.042
https://doi.org/10.1016/j.neucom.2014.06.042


481

Michael Wiegand, Josef Ruppenhofer, and Thomas
Kleinbauer. 2019. Detection of abusive language:
the problem of biased datasets. In The 2019 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2019, pages
602–608.

Genta Indra Winata, Andrea Madotto, Chien-Sheng
Wu, and Pascale Fung. 2019. Code-switched lan-
guage models using neural based synthetic data
from parallel sentences. In The SIGNLL Confer-
ence on Computational Natural Language Learning,
CoNLL 2019.

Weidi Xu, Haoze Sun, Chao Deng, and Ying Tan.
2017. Variational autoencoder for semi-supervised
text classification. In Thirty-First AAAI Conference
on Artificial Intelligence.

Jia-Jie Zhu and José Bento. 2017. Generative adversar-
ial active learning. CoRR, abs/1702.07956.

https://www.aclweb.org/anthology/N19-1060/
https://www.aclweb.org/anthology/N19-1060/
http://arxiv.org/abs/1702.07956
http://arxiv.org/abs/1702.07956

