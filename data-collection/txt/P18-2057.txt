




















































Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358–363
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

358

Hearst Patterns Revisited:
Automatic Hypernym Detection from Large Text Corpora

Stephen Roller, Douwe Kiela, and Maximilian Nickel

Facebook AI Research

{roller,dkiela,maxn}@fb.com

Abstract

Methods for unsupervised hypernym de-

tection may broadly be categorized accord-

ing to two paradigms: pattern-based and

distributional methods. In this paper, we

study the performance of both approaches

on several hypernymy tasks and find that

simple pattern-based methods consistently

outperform distributional methods on com-

mon benchmark datasets. Our results show

that pattern-based models provide impor-

tant contextual constraints which are not

yet captured in distributional methods.

1 Introduction

Hierarchical relationships play a central role in

knowledge representation and reasoning. Hyper-

nym detection, i.e., the modeling of word-level hier-

archies, has long been an important task in natural

language processing. Starting with Hearst (1992),

pattern-based methods have been one of the most

influential approaches to this problem. Their key

idea is to exploit certain lexico-syntactic patterns to

detect is-a relations in text. For instance, patterns

like “NPy such as NPx”, or “NPx and other NPy”

often indicate hypernymy relations of the form x
is-a y. Such patterns may be predefined, or they
may be learned automatically (Snow et al., 2004;

Shwartz et al., 2016). However, a well-known prob-

lem of Hearst-like patterns is their extreme sparsity:

words must co-occur in exactly the right configura-

tion, or else no relation can be detected.

To alleviate the sparsity issue, the focus in hy-

pernymy detection has recently shifted to distri-

butional representations, wherein words are repre-

sented as vectors based on their distribution across

large corpora. Such methods offer rich represen-

tations of lexical meaning, alleviating the sparsity

problem, but require specialized similarity mea-

sures to distinguish different lexical relationships.

The most successful measures to date are generally

inspired by the Distributional Inclusion Hypothe-

sis (DIH) (Zhitomirsky-Geffet and Dagan, 2005),

which states roughly that contexts in which a nar-

row term x may appear (“cat”) should be a subset
of the contexts in which a broader term y (“ani-
mal”) may appear. Intuitively, the DIH states that

we should be able to replace any occurrence of

“cat” with “animal” and still have a valid utterance.

An important insight from work on distributional

methods is that the definition of context is often

critical to the success of a system (Shwartz et al.,

2017). Some distributional representations, like

positional or dependency-based contexts, may even

capture crude Hearst pattern-like features (Levy

et al., 2015; Roller and Erk, 2016).

While both approaches for hypernym detec-

tion rely on co-occurrences within certain con-

texts, they differ in their context selection strategy:

pattern-based methods use predefined manually-

curated patterns to generate high-precision extrac-

tions while DIH methods rely on unconstrained

word co-occurrences in large corpora.

Here, we revisit the idea of using pattern-based

methods for hypernym detection. We evaluate sev-

eral pattern-based models on modern, large corpora

and compare them to methods based on the DIH.

We find that simple pattern-based methods con-

sistently outperform specialized DIH methods on

several difficult hypernymy tasks, including detec-

tion, direction prediction, and graded entailment

ranking. Moreover, we find that taking low-rank

embeddings of pattern-based models substantially

improves performance by remedying the sparsity

issue. Overall, our results show that Hearst pat-

terns provide high-quality and robust predictions

on large corpora by capturing important contextual

constraints, which are not yet modeled in distribu-

tional methods.



359

2 Models

In the following, we discuss pattern-based and

distributional methods to detect hypernymy rela-

tions. We explicitly consider only relatively simple

pattern-based approaches that allow us to directly

compare their performance to DIH-based methods.

2.1 Pattern-based Hypernym Detection

First, let P = {(x, y)}ni=1 denote the set of hyper-
nymy relations that have been extracted via Hearst

patterns from a text corpus T . Furthermore let
w(x, y) denote the count of how often (x, y) has
been extracted and let W =

∑

(x,y)∈P w(x, y) de-
note the total number extractions. In the first, most

direct application of Hearst patterns, we then sim-

ply use the counts w(x, y) or, equivalently, the ex-
traction probability

p(x, y) =
w(x, y)

W
(1)

to predict hypernymy relations from T .
However, simple extraction probabilities as in

Equation (1) are skewed by the occurrence proba-

bilities of their constituent words. For instance, it is

more likely that we extract (France, country) over

(France, republic), just because the word coun-

try is more likely to occur than republic. This

skew in word distributions is well-known for nat-

ural language and also translates to Hearst pat-

terns (see also Figure 1). For this reason, we also

consider predicting hypernymy relations based on

the Pointwise Mutual Information of Hearst pat-

terns: First, let p−(x) =
∑

(x,y)∈P w(x, y)/W

and p+(x) =
∑

(y,x)∈P w(y, x)/W denote the
probability that x occurs as a hyponym and hy-
pernym, respectively. We then define the Positive

Pointwise Mutual Information for (x, y) as

ppmi(x, y) = max

(

0, log
p(x, y)

p−(x)p+(y)

)

. (2)

While Equation (2) can correct for different word

occurrence probabilities, it cannot handle missing

data. However, sparsity is one of the main issues

when using Hearst patterns, as a necessarily incom-

plete set of extraction rules will lead inevitably to

missing extractions. For this purpose, we also study

low-rank embeddings of the PPMI matrix, which al-

low us to make predictions for unseen pairs. In par-

ticular, let m = |{x : (x, y) ∈ P ∨ (y, x) ∈ P}|
denote the number of unique terms in P . Further-
more, let X ∈ Rm×m be the PPMI matrix with

●

●
●

● ●
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●

●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●

●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●
●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●

●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●

●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●

1

2

3

4

5

0 1 2 3 4 5

Rank (log scale)

F
re

q
u
e
n
c
y
 (

lo
g
 s

c
a
le

)

Figure 1: Frequency distribution of words appear-

ing in Hearst patterns.

entries Mxy = ppmi(x, y) and let M = UΣV
⊤

be its Singular Value Decomposition (SVD). We

can then predict hypernymy relations based on the

truncated SVD of M via

spmi(x, y) = u⊤xΣrvy (3)

where ux, vy denote the x-th and y-th row of U
and V , respectively, and where Σr is the diagonal
matrix of truncated singular values (in which all

but the r largest singular values are set to zero).

Equation (3) can be interpreted as a smoothed

version of the observed PPMI matrix. Due to the

truncation of singular values, Equation (3) com-

putes a low-rank embedding of M where similar
words (in terms of their Hearst patterns) have simi-

lar representations. Since Equation (3) is defined

for all pairs (x, y), it allows us to make hyper-
nymy predictions based on the similarity of words.

We also consider factorizing a matrix that is con-

structed from occurrence probabilities as in Equa-

tion (1), denoted by sp(x, y). This approach is then
closely related to the method of Cederberg and

Widdows (2003), which has been proposed to im-

prove precision and recall for hypernymy detection

from Hearst patterns.

2.2 Distributional Hypernym Detection

Most unsupervised distributional approaches for

hypernymy detection are based on variants of the

Distributional Inclusion Hypothesis (Weeds et al.,

2004; Kotlerman et al., 2010; Santus et al., 2014;

Lenci and Benotto, 2012; Shwartz et al., 2017).

Here, we compare to two methods with strong em-

pirical results. As with most DIH measures, they

are only defined for large, sparse, positively-valued

distributional spaces. First, we consider WeedsPrec

(Weeds et al., 2004) which captures the features of



360

x which are included in the set of a broader term’s

features, y:

WeedsPrec(x,y) =

∑n
i=1 xi ∗ ✶yi>0
∑n

i=1 xi

Second, we consider invCL (Lenci and Benotto,

2012) which introduces a notion of distributional

exclusion by also measuring the degree to which

the broader term contains contexts not used by the

narrower term. In particular, let

CL(x,y) =

∑n
i=1 min(xi, yi)
∑n

i=1 xi

denote the degree of inclusion of x in y as proposed
by Clarke (2009). To measure both the inclusion

of x in y and the non-inclusion of y in x, invCL is
then defined as

invCL(x,y) =
√

CL(x,y) ∗ (1− CL(y,x))

Although most unsupervised distributional ap-

proaches are based on the DIH, we also con-

sider the distributional SLQS model based on on

an alternative informativeness hypothesis (Santus

et al., 2014; Shwartz et al., 2017). Intuitively, the

SLQS model presupposes that general words ap-

pear mostly in uninformative contexts, as measured

by entropy. Specifically, SLQS depends on the me-

dian entropy of a term’s top N contexts, defined
as

Ex = median
N
i=1 [H(ci)] ,

where H(ci) is the Shannon entropy of context ci
across all terms, and N is chosen in hyperparameter
selection. Finally, SLQS is defined using the ratio

between the two terms:

SLQS(x, y) = 1−
Ex
Ey

.

Since the SLQS model only compares the rela-

tive generality of two terms, but does not make

judgment about the terms’ relatedness, we report

SLQS-cos, which multiplies the SLQS measure by

cosine similarity of x and y (Santus et al., 2014).

For completeness, we also include cosine simi-

larity as a baseline in our evaluation.

3 Evaluation

To evaluate the relative performance of pattern-

based and distributional models, we apply them to

several challenging hypernymy tasks.

Pattern

X which is a (example|class|kind|. . . ) of Y
X (and|or) (any|some) other Y
X which is called Y

X is JJS (most)? Y

X a special case of Y

X is an Y that

X is a !(member|part|given) Y
!(features|properties) Y such as X1, X2, . . .
(Unlike|like) (most|all|any|other) Y, X
Y including X1, X2, . . .

Table 1: Hearst patterns used in this study. Patterns

are lemmatized, but listed as inflected for clarity.

3.1 Tasks

Detection: In hypernymy detection, the task is

to classify whether pairs of words are in a hyper-

nymy relation. For this task, we evaluate all mod-

els on five benchmark datasets: First, we employ

the noun-noun subset of BLESS, which contains

hypernymy annotations for 200 concrete, mostly

unambiguous nouns. Negative pairs contain a mix-

ture of co-hyponymy, meronymy, and random pairs.

This version contains 14,542 total pairs with 1,337

positive examples. Second, we evaluate on LEDS

(Baroni et al., 2012), which consists of 2,770 noun

pairs balanced between positive hypernymy exam-

ples, and randomly shuffled negative pairs. We

also consider EVAL (Santus et al., 2015), contain-

ing 7,378 pairs in a mixture of hypernymy, syn-

onymy, antonymy, meronymy, and adjectival rela-

tions. EVAL is notable for its absence of random

pairs. The largest dataset is SHWARTZ (Shwartz

et al., 2016), which was collected from a mixture

of WordNet, DBPedia, and other resources. We

limit ourselves to a 52,578 pair subset excluding

multiword expressions. Finally, we evaluate on

WBLESS (Weeds et al., 2014), a 1,668 pair subset

of BLESS, with negative pairs being selected from

co-hyponymy, random, and hyponymy relations.

Previous work has used different metrics for evalu-

ating on BLESS (Lenci and Benotto, 2012; Levy

et al., 2015; Roller and Erk, 2016). We chose to

evaluate the global ranking using Average Preci-

sion. This allowed us to use the same metric on

all detection benchmarks, and is consistent with

evaluations in Shwartz et al. (2017).

Direction: In direction prediction, the task is

to identify which term is broader in a given pair



361

of words. For this task, we evaluate all models

on three datasets described by Kiela et al. (2015):

On BLESS, the task is to predict the direction

for all 1337 positive pairs in the dataset. Pairs

are only counted correct if the hypernymy direc-

tion scores higher than the reverse direction, i.e.

score(x, y) > score(y, x). We reserve 10% of the
data for validation, and test on the remaining 90%.

On WBLESS, we follow prior work (Nguyen et al.,

2017; Vulić and Mrkšić, 2017) and perform 1000

random iterations in which 2% of the data is used

as a validation set to learn a classification threshold,

and test on the remainder of the data. We report

average accuracy across all iterations. Finally, we

evaluate on BIBLESS (Kiela et al., 2015), a variant

of WBLESS with hypernymy and hyponymy pairs

explicitly annotated for their direction. Since this

task requires three-way classification (hypernymy,

hyponymy, and other), we perform two-stage clas-

sification. First, a threshold is tuned using 2% of

the data, identifying whether a pair exhibits hy-

pernymy in either direction. Second, the relative

comparison of scores determines which direction is

predicted. As with WBLESS, we report the average

accuracy over 1000 iterations.

Graded Entailment: In graded entailment, the

task is to quantify the degree to which a hyper-

nymy relation holds. For this task, we follow

prior work (Nickel and Kiela, 2017; Vulić and

Mrkšić, 2017) and use the noun part of HYPER-

LEX (Vulić et al., 2017), consisting of 2,163 noun

pairs which are annotated to what degree x is-a y
holds on a scale of [0, 6]. For all models, we report
Spearman’s rank correlation ρ. We handle out-of-
vocabulary (OOV) words by assigning the median

of the scores (computed across the training set) to

pairs with OOV words.

3.2 Experimental Setup

Pattern-based models: We extract Hearst patterns

from the concatenation of Gigaword and Wikipedia,

and prepare our corpus by tokenizing, lemmatiz-

ing, and POS tagging using CoreNLP 3.8.0. The

full set of Hearst patterns is provided in Table 1.

Our selected patterns match prototypical Hearst pat-

terns, like “animals such as cats,” but also include

broader patterns like “New Year is the most impor-

tant holiday.” Leading and following noun phrases

are allowed to match limited modifiers (compound

nouns, adjectives, etc.), in which case we also gen-

erate a hit for the head of the noun phrase. Dur-

ing postprocessing, we remove pairs which were

not extracted by at least two distinct patterns. We

also remove any pair (y, x) if p(y, x) < p(x, y).
The final corpus contains roughly 4.5M matched

pairs, 431K unique pairs, and 243K unique terms.

For SVD-based models, we select the rank from

r ∈ {5, 10, 15, 20, 25, 50, 100, 150, 200, 250,
300, 500, 1000} on the validation set. The other
pattern-based models do not have any hyperparam-

eters.

Distributional models: For the distributional

baselines, we employ the large, sparse distribu-

tional space of Shwartz et al. (2017), which is com-

puted from UkWaC and Wikipedia, and is known to

have strong performance on several of the detection

tasks. The corpus was POS tagged and dependency

parsed. Distributional contexts were constructed

from adjacent words in dependency parses (Padó

and Lapata, 2007; Levy and Goldberg, 2014). Tar-

gets and contexts which appeared fewer than 100

times in the corpus were filtered, and the result-

ing co-occurrence matrix was PPMI transformed.1

The resulting space contains representations for

218K words over 732K context dimensions. For

the SLQS model, we selected the number of con-

texts N from the same set of options as the SVD
rank in pattern-based models.

3.3 Results

Table 2 shows the results from all three experimen-

tal settings. In nearly all cases, we find that pattern-

based approaches substantially outperform all three

distributional models. Particularly strong improve-

ments can be observed on BLESS (0.76 average

precision vs 0.19) and WBLESS (0.96 vs. 0.69) for

the detection tasks and on all directionality tasks.

For directionality prediction on BLESS, the SVD

models surpass even the state-of-the-art supervised

model of Vulić and Mrkšić (2017). Moreover, both

SVD models perform generally better than their

sparse counterparts on all tasks and datasets except

on HYPERLEX. We performed a posthoc analy-

sis of the validation sets comparing the ppmi and

spmi models, and found that the truncated SVD im-

proved recall via its matrix completion properties.

We also found that the spmi model downweighted

1In addition, we also experimented with further distri-
butional spaces and weighting schemes from Shwartz et al.
(2017). We also experimented with distributional spaces using
the same corpora and preprocessing as the Hearst patterns (i.e.,
Wikipedia and Gigaword). We found that the reported setting
generally performed best, and omit others for brevity.



362

Detection (AP) Direction (Acc.) Graded (ρs)

BLESS EVAL LEDS SHWARTZ WBLESS BLESS WBLESS BIBLESS HYPERLEX

Cosine .12 .29 .71 .31 .53 .00 .54 .52 .14

WeedsPrec .19 .39 .87 .43 .68 .63 .59 .45 .43

invCL .18 .37 .89 .38 .66 .64 .60 .47 .43

SLQS .15 .35 .60 .38 .69 .75 .67 .51 .16

p(x, y) .49 .38 .71 .29 .74 .46 .69 .62 .62

ppmi(x, y) .45 .36 .70 .28 .72 .46 .68 .61 .60

sp(x, y) .66 .45 .81 .41 .91 .96 .84 .80 .51

spmi(x, y) .76 .48 .84 .44 .96 .96 .87 .85 .53

Table 2: Experimental results comparing distributional and pattern-based methods in all settings.

many high-scoring outlier pairs composed of rare

terms.

When comparing the p(x, y) and ppmi models
to distributional models, we observe mixed results.

The SHWARTZ dataset is difficult for sparse models

due to its very long tail of low frequency words

that are hard to cover using Hearst patterns. On

EVAL, Hearst-pattern based methods get penalized

by OOV words, due to the large number of verbs

and adjectives in the dataset, which are not captured

by our patterns. However, in 7 of the 9 datasets, at

least one of the sparse models outperforms all dis-

tributional measures, showing that Hearst patterns

can provide strong performance on large corpora.

4 Conclusion

We studied the relative performance of Hearst

pattern-based methods and DIH-based methods

for hypernym detection. Our results show that

the pattern-based methods substantially outper-

form DIH-based methods on several challenging

benchmarks. We find that embedding methods

alleviate sparsity concerns of pattern-based ap-

proaches and substantially improve coverage. We

conclude that Hearst patterns provide important

contexts for the detection of hypernymy relations

that are not yet captured in DIH models. Our

code is available at https://github.com/

facebookresearch/hypernymysuite.

Acknowledgments

We would like to thank the anonymous reviewers

for their helpful suggestions. We also thank Vered

Shwartz, Enrico Santus, and Dominik Schlechtweg

for providing us with their distributional spaces and

baseline implementations.

References

Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proceed-
ings of the 2012 Conference of the European Chap-
ter of the Association for Computational Linguists,
pages 23–32, Avignon, France.

Scott Cederberg and Dominic Widdows. 2003. Using
lsa and noun coordination information to improve
the recall and precision of automatic hyponymy ex-
traction. In Proceedings of the Seventh Confer-
ence on Natural Language Learning at HLT-NAACL
2003.

Daoud Clarke. 2009. Context-theoretic semantics for
natural language: An overview. In Proceedings of
the 2011 Workshop on GEometrical Models of Nat-
ural Language Semantics, pages 112–119, Athens,
Greece. Association for Computational Linguistics.

Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 1992 Conference on Computational Linguistics,
pages 539–545, Nantes, France.

Douwe Kiela, Laura Rimell, Ivan Vulić, and Stephen
Clark. 2015. Exploiting image generality for lexical
entailment detection. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 2:
Short Papers), pages 119–124, Beijing, China. As-
sociation for Computational Linguistics.

Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16:359–389.

Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the First Joint Conference on Lex-
ical and Computational Semantics, pages 75–79,
Montréal, Canada. Association for Computational
Linguistics.

https://github.com/facebookresearch/hypernymysuite
https://github.com/facebookresearch/hypernymysuite


363

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics, pages 302–308, Baltimore,
Maryland. Association for Computational Linguis-
tics.

Omer Levy, Steffen Remus, Chris Biemann, and Ido
Dagan. 2015. Do supervised distributional meth-
ods really learn lexical inference relations? In Pro-
ceedings of the 2015 North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 970–976, Den-
ver, Colorado.

Kim Anh Nguyen, Maximilian Keper, Sabine Schulte
im Walde, and Ngoc Thang Vu. 2017. Hierarchi-
cal Embeddings for Hypernymy Detection and Di-
rectionality. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 233–243, Copenhagen, Denmark.

Maximillian Nickel and Douwe Kiela. 2017. Poincaré
embeddings for learning hierarchical representa-
tions. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 30, pages 6338–6347. Curran Asso-
ciates, Inc.

Sebastian Padó and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Com-
putational Linguistics, 33(2):161–199.

Stephen Roller and Katrin Erk. 2016. Relations such
as hypernymy: Identifying and exploiting hearst pat-
terns in distributional vectors for lexical entailment.
In Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, Austin,
Texas, USA. Association for Computational Linguis-
tics.

Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine
Schulte im Walde. 2014. Chasing hypernyms in vec-
tor spaces with entropy. In Proceedings of the 14th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, volume 2: Short
Papers, pages 38–42, Gothenburg, Sweden. Associ-
ation for Computational Linguistics.

Enrico Santus, Frances Yung, Alessandro Lenci, and
Chu-Ren Huang. 2015. EVALution 1.0: An evolv-
ing semantic dataset for training and evaluation of
distributional semantic models. In Proceedings of
the Fourth Workshop on Linked Data in Linguistics:
Resources and Applications, pages 64–69, Beijing,
China. Association for Computational Linguistics.

Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016.
Improving hypernymy detection with an integrated
path-based and distributional method. In Proceed-
ings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 2389–
2398, Berlin, Germany. Association for Computa-
tional Linguistics.

Vered Shwartz, Enrico Santus, and Dominik
Schlechtweg. 2017. Hypernyms under siege:
Linguistically-motivated artillery for hypernymy
detection. In Proceedings of the 15th Conference
of the European Chapter of the Association for
Computational Linguistics: Volume 1, Long Papers,
pages 65–75, Valencia, Spain. Association for
Computational Linguistics.

Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. In L. K. Saul, Y. Weiss, and L. Bottou,
editors, Advances in Neural Information Processing
Systems 17, pages 1297–1304. MIT Press.

Ivan Vulić, Daniela Gerz, Douwe Kiela, Felix Hill, and
Anna Korhonen. 2017. Hyperlex: A large-scale eval-
uation of graded lexical entailment. Computational
Linguistics, 43(4):781–835.

Ivan Vulić and Nikola Mrkšić. 2017. Specialis-
ing word vectors for lexical entailment. CoRR,
abs/1710.06371.

Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir,
and Bill Keller. 2014. Learning to distinguish hyper-
nyms and co-hyponyms. In Proceedings of the 2014
International Conference on Computational Linguis-
tics, pages 2249–2259, Dublin, Ireland.

Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 2004 International
Conference on Computational Linguistics, pages
1015–1021, Geneva, Switzerland.

Maayan Zhitomirsky-Geffet and Ido Dagan. 2005. The
distributional inclusion hypotheses and lexical en-
tailment. In Proceedings of the 2005 Annual Meet-
ing of the Association for Computational Linguistics,
pages 107–114, Ann Arbor, Michigan.


