



















































Compositional Distributional Semantics Models in Chunk-based Smoothed Tree Kernels


Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 93–98,
Dublin, Ireland, August 23-24 2014.

Compositional Distributional Semantics Models
in Chunk-based Smoothed Tree Kernels

Nghia The Pham
University of Trento

thenghia.pham@unitn.it

Lorenzo Ferrone
University of Rome “Tor Vergata”

lorenzo.ferrone@gmail.com

Fabio Massimo Zanzotto
University of Rome “Tor Vergata”

fabio.massimo.zanzotto@uniroma2.it

Abstract

The field of compositional distributional
semantics has proposed very interesting
and reliable models for accounting the
distributional meaning of simple phrases.
These models however tend to disregard
the syntactic structures when they are ap-
plied to larger sentences. In this paper we
propose the chunk-based smoothed tree
kernels (CSTKs) as a way to exploit the
syntactic structures as well as the reliabil-
ity of these compositional models for sim-
ple phrases. We experiment with the rec-
ognizing textual entailment datasets. Our
experiments show that our CSTKs per-
form better than basic compositional dis-
tributional semantic models (CDSMs) re-
cursively applied at the sentence level, and
also better than syntactic tree kernels.

1 Introduction

A clear interaction between syntactic and semantic
interpretations for sentences is important for many
high-level NLP tasks, such as question-answering,
textual entailment recognition, and semantic tex-
tual similarity. Systems and models for these tasks
often use classifiers or regressors that exploit con-
volution kernels (Haussler, 1999) to model both
interpretations.

Convolution kernels are naturally defined on
spaces where there exists a similarity function be-
tween terminal nodes. This feature has been used
to integrate distributional semantics within tree
kernels. This class of kernels is often referred to as
smoothed tree kernels (Mehdad et al., 2010; Croce
et al., 2011), yet, these models only use distribu-
tional vectors for words.

Compositional distributional semantics models
(CDSMs) on the other hand are functions map-
ping text fragments to vectors (or higher-order ten-
sors) which then provide a distributional meaning

for simple phrases or sentences. Many CDSMs
have been proposed for simple phrases like non-
recursive noun phrases or verbal phrases (Mitchell
and Lapata, 2008; Baroni and Zamparelli, 2010;
Clark et al., 2008; Grefenstette and Sadrzadeh,
2011; Zanzotto et al., 2010). Non-recursive
phrases are often referred to as chunks (Abney,
1996), and thus, CDSMs are good and reliable
models for chunks.

In this paper, we present the chunk-based
smoothed tree kernels (CSTK) as a way to merge
the two approaches: the smoothed tree kernels
and the models for compositional distributional se-
mantics. Our approach overcomes the limitation
of the smoothed tree kernels which only use vec-
tors for words by exploiting reliable CDSMs over
chunks. CSTKs are defined over a chunk-based
syntactic subtrees where terminal nodes are words
or word sequences. We experimented with CSTKs
on data from the recognizing textual entailment
challenge (Dagan et al., 2006) and we compared
our CSTKs with other standard tree kernels and
standard recursive CDSMs. Experiments show
that our CSTKs perform better than basic compo-
sitional distributional semantic models (CDSMs)
recursively applied at the sentence level and better
than syntactic tree kernels.

The rest of the paper is organized as follows.
Section 2 describes the CSTKs. Section 3 re-
ports on the experimental setting and on the re-
sults. Finally, Section 4 draws the conclusions and
sketches the future work.

2 Chunk-based Smoothed Tree Kernels

This section describes the new class of kernels.
We first introduce the notion of the chunk-based
syntactic subtree. Then, we describe the recursive
formulation of the class of kernels. Finally, we in-
troduce the basic CDSMs we use and we introduce
two instances of the class of kernels.

93



2.1 Notation and preliminaries

Shhhhh
(((((

NPXXXX����
DT

the:d

NN

rock:n

NN

band:n

VPXXXX����
VBZ

holds:v

NPXXXX
����

PRP

its:p

JJ

final:j

NN

concert:n

Figure 1: Sample Syntactic Tree

A Chunk-based Syntactic Sub-Tree is a subtree
of a syntactic tree where each non-terminal node
dominating a contiguous word sequence is col-
lapsed into a chunk and, as usual in chunks (Ab-
ney, 1996), the internal structure is disregarded.
For example, Figure 2 reports some chunk-based
syntactic subtrees of the tree in Figure 1. Chunks
are represented with a pre-terminal node dominat-
ing a triangle that covers a word sequence. The
first subtree represents the chunk covering the sec-
ond NP and the node dominates the word sequence
its:d final:n concert:n. The second subtree repre-
sents the structure of the whole sentence and one
chunk, that is the first NP dominating the word
sequence the:d rock:n band:n. The third subtree
again represents the structure of the whole sen-
tence split into two chunks without the verb.

NP̀
``̀    

its:p final:j concert:n

SXXXX����
NPXXXX
����

the:d rock:n band:n

VP
ZZ��

VBZ NP
S̀
```

    
NPXXXX
����

the:d rock:j band:n

VP
PPP

���
VBZ NP̀

``̀    
its:p final:j concert:n

Figure 2: Some Chunk-based Syntactic Sub-Trees
of the tree in Figure 1

In the following sections, generic trees are de-
noted with the letter t and N(t) denotes the set of
non-terminal nodes of tree t. Each non-terminal
node n ∈ N(t) has a label sn representing its syn-
tactic tag. As usual for constituency-based parse
trees, pre-terminal nodes are nodes that have a sin-
gle terminal node as child. Terminal nodes of trees
are words denoted with w:pos where w is the ac-
tual token and pos is its postag. The structure of
these trees is represented as follows. Given a tree

t, ci(n) denotes i-th child of a node n in the set of
nodes N(t). The production rule headed in node
n is prod(n), that is, given the node nwithm chil-
dren, prod(n) is:

prod(n) = sn → sc1(n) . . . scm(n)
Finally, for a node n in N(t), the function d(n)
generates the word sequence dominated by the
non-terminal node n in the tree t. For example,
d(VP) in Figure 1 is holds:v its:p final:j concert:n.

Chunk-based Syntactic Sub-Trees (CSSTs) are
instead denoted with the letter τ . Differently
from trees t, CSSTs have terminal nodes that
can represent subsequences of words of the
original sentence. The explicit syntactic structure
of a CSST is the structure not falling in chunks
and it is represented as s(τ). For example, s(τ3) is:

S
HH��

NP VP
ZZ��

VBZ NP

where τ3 is the third subtree of Figure 2.
Given a tree t, the set S(t) is defined as the set

containing all the relevant CSSTs of the tree t.
As for the tree kernels (Collins and Duffy, 2002),
the set S(t) contains all CSSTs derived from the
subtrees of t such that if a node n belongs to a
subtree ts, all the siblings of n in t belongs to ts.
In other words, productions of the initial subtrees
are complete. A CSST is obtained by collapsing
in a single terminal nodes a contiguous sequence
of words dominated by a single non-terminal
node. For example:

NP
PPP���

DT NN
HH��

NN

rock:n

NN

band:n

is collapsed into:

NP
aaa!!!

DT NN:X
PPP���

rock:n band:n

Finally,
→
wn ∈ Rm represent the distributional

vectors for words wn and f(w1 . . . wk) represents
a compositional distributional semantics model
applied to the word sequence w1 . . . wk.

94



2.2 Smoothed Tree Kernels on Chunk-based
Syntactic Trees

As usual, a tree kernel, although written in a re-
cursive way, computes the following general equa-
tion:

K(t1, t2) =
∑

τi ∈ S(t1)
τj ∈ S(t2)

λ|N(τi)|+|N(τj)|KF (τi, τj)

(1)
In our case, the basic similarity KF (ti, tj) is de-
fined to take into account the syntactic structure
and the distributional semantic part. Thus, we de-
fine it as follows in line with what done with sev-
eral other smoothed tree kernels:

KF (τi, τj) = δ(s(τi), s(τj))
∏

a ∈ PT (τi)
b ∈ PT (τj)

〈f(a), f(b)〉

where δ(s(τi), s(τj)) is the Kroneker’s delta
function between the the structural part of two
chunk-based syntactic subtrees, PT (τ) are the
nodes in τ directly covering a chunk or a word,
and 〈→x,→y 〉 is the cosine similarity between the
two vectors

→
x and

→
y . For example, given the

chunk-based subtree τ3 in Figure 2 and

τ4 =

SXXXX
����

NPXXXX����
the:d orchestra:n

VP
aa!!

VBZ NP
PPP���

its:p show:n

the similarity KF (τ3, τ4) is:
〈f(the:d orchestra:n), f(the:d rock:n band:n)〉 ·
〈f(its:p show:n), f(its:p final:j concert:n)〉.

The recursive formulation of the Chunk-based
Smoothed Tree Kernel (CSTK) is a bit more com-
plex but very similar to the recursive formulation
of the syntactic tree kernels:

K(t1, t2) =
∑

n1 ∈ N(t1)
n2 ∈ N(t2)

C(n1, n2) (2)

where C(n1, n2) =

〈f(d(n1)), f(d(n2))〉 if label(n1) = label(n2)
and prod(n1) 6= prod(n2)

〈f(d(n1)), f(d(n2))〉
+

∏nc(n1)
j=1 (1 + C(cj(n1), cj(n2)))

−∏nc(n1)j=1 〈f(d(cj(n1))), f(d(cj(n2)))〉
if n1, n2 are not pre-terminals and
prod(n1) = prod(n2)

0 otherwise

where nc(n1) is the lenght of the production
prod(n1).

2.3 Compositional Distributional Semantic
Models and two Specific CSTKs

To define specific CSTKs, we need to introduce
the basic compositional distributional semantic
models (CDSMs). We use two CDSMs: the Ba-
sic Additive model (BA) and teh Full Additive
model (FA). We thus define two specific CSTKs:
the CSTK+BA that is based on the basic additive
model and the CSTK+FA that is based on the full
additive model. We describe the two CDSMs in
the following.

The Basic Additive model (BA) (introduced in
(Mitchell and Lapata, 2008)) computes the disti-
butional semantics vector of a pair of words a =
a1a2 as:

ADD(a1, a2) = α
→
a1 + β

→
a2

where α and β weight the first and the second
word of the pair. The basic additive model for
word sequences s = w1 . . . wk is recursively de-
fined as follows:

fBA(s) =

{→
w1 if k = 1
α
→
w1 + βfBA(w2 . . . wk) if k > 1

The Full Additive model (FA) (used in (Gue-
vara, 2010) for adjective-noun pairs and (Zanzotto
et al., 2010) for three different syntactic relations)
computes the compositional vector

→
a of a pair us-

ing two linear tranformations AR and BR respec-
tively applied to the vectors of the first and the
second word. These matrices generally only de-
pends on the syntactic relation R that links those
two words. The operation follows:

fFA(a1, a2, R) = AR
→
a1 +BR

→
a2

95



RR RRTWS
RTE1 RTE2 RTE3 RTE5 Average RTE1 RTE2 RTE3 RTE5 Average

Add 0.541 0.496 0.507 0.520 0.516 0.560 0.538 0.643 0.578 0.579

FullAdd 0.512 0.516 0.507 0.569 0.526 0.571 0.608 0.643 0.643 0.616

TK 0.561 0.552 0.531 0.54 0.546 0.608 0.627 0.648 0.630 0.628

CSTK+BA 0.553 0.545 0.562 0.568 0.557† 0.626 0.616 0.648 0.628 0.629†

CSTK+FA 0.543 0.550 0.574 0.576 0.560† 0.628 0.616 0.652 0.630 0.631†

Table 1: Task-based analysis: Accuracy on Recognizing Textual Entailment († is different from both ADD and
FullADD with a stat.sig. of p > 0.1.)

The full additive model for word sequences s =
w1 . . . wk, whose node has a production rule s →
sc1 . . . scm is also defined recursively:

fFA(s) =



→
w1 if k = 1

Avn
→
V +BvnfFA(NP )
if s→ V NP

Aan
→
A +BanfFA(N)
if s→ A N∑
fFA(sci) otherwise

where Avn, Bvn are matrices used for verb and
noun phrase interaction, andAan, Ban are used for
adjective, noun interaction.

3 Experimental Investigation

3.1 Experimental set-up
We experimented with the Recognizing Textual
Entailment datasets (RTE) (Dagan et al., 2006).
RTE is the task of deciding whether a long text
T entails a shorter text, typically a single sen-
tence, called hypothesis H . It has been often seen
as a classification task (see (Dagan et al., 2013)).
We used four datasets: RTE1, RTE2, RTE3, and
RTE5, with the standard split between training and
testing. The dev/test distribution for RTE1-3, and
RTE5 is respectively 567/800, 800/800, 800/800,
and 600/600 T-H pairs.

Distributional vectors are derived with
DISSECT (Dinu et al., 2013) from a cor-
pus obtained by the concatenation of ukWaC
(wacky.sslmit.unibo.it), a mid-2009 dump of the
English Wikipedia (en.wikipedia.org) and the
British National Corpus (www.natcorp.ox.ac.uk),
for a total of about 2.8 billion words. We collected
a 35K-by-35K matrix by counting co-occurrence
of the 30K most frequent content lemmas in
the corpus (nouns, adjectives and verbs) and all
the content lemmas occurring in the datasets

within a 3 word window. The raw count vectors
were transformed into positive Pointwise Mutual
Information scores and reduced to 300 dimensions
by Singular Value Decomposition. This setup was
picked without tuning, as we found it effective in
previous, unrelated experiments.

We built the matrices for the full additive mod-
els using the procedure described in (Guevara,
2010). We considered only two relations: the
Adjective-Noun and Verb-Noun. The full addi-
tive model falls back to the basic additional model
when syntactic relations are different from these
two.

To build the final kernel to learn the clas-
sifier, we followed standard approaches (Dagan
et al., 2013), that is, we exploited two models:
a model with only a rewrite rule feature space
(RR) and a model with the previous space along
with a token-level similarity feature (RRTWS).
The two models use our CSTKs and the stan-
dard TKs in the following way as kernel func-
tions: (1) RR(p1, p2) = κ(ta1, t

a
2) + κ(t

b
1, t

b
2);

(2) RRTWS(p1, p2) = κ(ta1, t
a
2) + κ(t

b
1, t

b
2) +

(TWS(a1, b1) · TWS(a2, b2) + 1)2 where TWS
is a weighted token similarity (as in (Corley and
Mihalcea, 2005)).

3.2 Results

Table 1 shows the results of the experiments, the
table is organised as follows: columns 2-6 re-
port the accuracy of the RTE systems based on
rewrite rules (RR) and columns 7-11 report the ac-
curacies of RR systems along with token similar-
ity (RRTS). We compare five differente models:
ADD is the Basic Additive model with parameters
α = β = 1 (as defined in 2.3) applied to the words
of the sentence (without considering its tree struc-
ture), the same is done for the Full Additive (Ful-
lADD), defined as in 2.3. The Tree Kernel (TK) as
defined in (Collins and Duffy, 2002) are applied to

96



the constituency-based tree representation of the
tree, without the intervening collapsing step de-
scribed in 2.2. These three models are the base-
line against which we compare the CSTK models
where the collapsing procedure is done via Basic
Additive (CSTK + BA, again with α = β = 1) and
FullAdditive (CSTK + FA), as described in sec-
tion 2.2, again, with the aforementioned restric-
tion on the relation considered. For RR models we
have that CSTK+BA and CSTK+FA both achieve
higher accuracy than ADD and FullAdd, with a
statistical significante greater than 93.7%, as com-
puted with the sign test. Specifically we have that
CSTK+BA has an average accuracy 7.94% higher
than ADD and 5.89% higher than FullADD, while
CSTK+FA improves on ADD and FullADD by
8.52% and 6.46%, respectively. The same trend is
visible for the RRTS model, again both models are
statistically better than ADD and FullADD, in this
case we have that CSTK+BA is 8.63% more ac-
curate then ADD and 2.11% more than FullADD,
CSTK+FA is respectively 8.98% and 2.43% more
accurate than ADD and FullADD. As for the TK
models we have that both CSTK models achieve
again an higher average accuracy: for RR models
CSTK+BA and CSTK+FA are respectively 2.01%
and 0.15% better than TK, while for RRTS models
the number are 2.54% and 0.47%. These results
though are not statistically significant, as is the
difference between the two CSTK models them-
selves.

4 Conclusions and Future Work

In this paper, we introduced a novel sub-class
of the convolution kernels in order exploit reli-
able compositional distributional semantic mod-
els along with the syntactic structure of sen-
tences. Experiments show that this novel sub-
class, namely, the Chunk-based Smoothed Tree
Kernels (CSTKs), are a promising solution, per-
forming significantly better than a naive recursive
application of the compositional distributional se-
mantic models. We experimented with CSTKS
equipped with the basic additive and the full addi-
tive CDSMs but these kernels are definitely open
to all the CDSMs.

Acknowledgments

We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES).

References
Steven Abney. 1996. Part-of-speech tagging and par-

tial parsing. In G.Bloothooft K.Church, S.Young,
editor, Corpus-based methods in language and
speech. Kluwer academic publishers, Dordrecht.

Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183–1193, Cambridge, MA, October. Association
for Computational Linguistics.

Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distributional
model of meaning. Proceedings of the Second
Symposium on Quantum Interaction (QI-2008),
pages 133–140.

Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of ACL02.

Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proc. of the
ACL Workshop on Empirical Modeling of Seman-
tic Equivalence and Entailment, pages 13–18. As-
sociation for Computational Linguistics, Ann Arbor,
Michigan, June.

Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ’11, pages
1034–1046, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Quionero-Candela et al., editor,
LNAI 3944: MLCW 2005, pages 177–190. Springer-
Verlag, Milan, Italy.

Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing Textual Entail-
ment: Models and Applications. Synthesis Lectures
on Human Language Technologies. Morgan & Clay-
pool Publishers.

Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. DISSECT: DIStributional SEmantics Com-
position Toolkit. In Proceedings of ACL (System
Demonstrations), pages 31–36, Sofia, Bulgaria.

Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ’11, pages
1394–1404, Stroudsburg, PA, USA. Association for
Computational Linguistics.

97



Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, pages 33–37, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.

David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of Califor-
nia at Santa Cruz.

Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic struc-
tures for textual entailment recognition. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT ’10, pages
1020–1028, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236–244, Columbus, Ohio,
June. Association for Computational Linguistics.

Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING), August,.

98


