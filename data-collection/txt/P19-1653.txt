



















































Distilling Translations with Visual Awareness


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6525–6538
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

6525

Distilling Translations with Visual Awareness

Julia Ive1, Pranava Madhyastha2 and Lucia Specia2
1DCS, University of Sheffield, UK

2Department of Computing, Imperial College London, UK
j.ive@sheffield.ac.uk

{pranava,l.specia}@imperial.ac.uk

Abstract

Previous work on multimodal machine transla-
tion has shown that visual information is only
needed in very specific cases, for example in
the presence of ambiguous words where the
textual context is not sufficient. As a conse-
quence, models tend to learn to ignore this in-
formation. We propose a translate-and-refine
approach to this problem where images are
only used by a second stage decoder. This ap-
proach is trained jointly to generate a good first
draft translation and to improve over this draft
by (i) making better use of the target language
textual context (both left and right-side con-
texts) and (ii) making use of visual context.
This approach leads to the state of the art re-
sults. Additionally, we show that it has the
ability to recover from erroneous or missing
words in the source language.

1 Introduction

Multimodal machine translation (MMT) is an area
of research that addresses the task of translat-
ing texts using context from an additional modal-
ity, generally static images. The assumption is
that the visual context can help ground the mean-
ing of the text and, as a consequence, generate
more adequate translations. Current work has fo-
cused on datasets of images paired with their de-
scriptions, which are crowdsourced in English and
then translated into different languages, namely
the Multi30K dataset (Elliott et al., 2016).

Results from the most recent evaluation cam-
paigns in the area (Elliott et al., 2017; Barrault
et al., 2018) have shown that visual information
can be helpful, as humans generally prefer trans-
lations generated by multimodal models than by
their text-only counterparts. However, previous
work has also shown that images are only needed
in very specific cases (Lala et al., 2018). This
is also the case for humans. Frank et al. (2018)

(see Figure 1) concluded that visual information
is needed by humans in the presence of the fol-
lowing: incorrect or ambiguous source words
and gender-neutral words that need to be marked
for gender in the target language. In an experi-
ment where human translators were asked to first
translate descriptions based on their textual con-
text only and then revise their translation based on
a corresponding image, they report that these three
cases accounted for 62-77% of the revisions in the
translations in two subsets of Multi30K.

Ambiguities are very frequent in Multi30K, as
in most language corpora. Barrault et al. (2018)
shows that in its latest test set, 358 (German) and
438 (French) instances (out of 1,000) contain at
least one word that has more than one translation
in the training set. However, these do not always
represent a challenge for translation models: of-
ten the text context can easily disambiguate words
(see baseline translation in Figure 4(a)); addition-
ally, the models are naturally biased to generate
the most frequent translation of the word, which
by definition is the correct one in most cases.

The need to gender-mark words in a target
language when translating from English can be
thought of as a disambiguation problem, except
that the text context is often less telling and the
frequency bias plays ends up playing a bigger role
(see baseline translation in Figure 4(c)). This has
been shown to be a common problem in neural
machine translation (Vanmassenhove et al., 2018;
Font and Costa-Jussà, 2019), as well as in areas
such as image captioning (Hendricks et al., 2018)
and co-reference resolution (Zhao et al., 2018).

Incorrect source words are common in
Multi30K, as in many other crowdsourced or user-
generated dataset. In this case the context may not
be enough (see DE translation in Figure 1(c)). We
posit that models should be robust to such a type
of noise and note that similar treatment would be



6526

EN: Three children in football uniforms are playing football.

DE: Drei Kinder in Fußballtrikots spielen Fußball.

PE: Drei Kinder in Footballtrikots spielen Football.

(a) Ambiguous word football translated as soccer (Fußball)

EN: A baseball player in a black shirt just tagged a player in a white shirt.

DE: Ein Baseballspieler in einem schwarzen Shirt fängt einen Spieler in einem weißen Shirt.

PE: Eine Baseballspielerin in einem schwarzen Shirt fängt eine Spielerin in einem weißen
Shirt.

(b) Gender-neutral word player translated as male player (Spieler)

EN: A woman wearing a white shirt works out on an elliptical machine.

DE: Eine Frau in einem weißen Shirt trainiert auf einem Crosstrainer.

PE: Eine Frau in einem weißen Pullover trainiert auf einem Crosstrainer.

(c) Inaccurate English word shirt instead of sweater or pullover

Figure 1: Examples of lexical and gender ambiguity, and inaccurate English description where post-edits (PE)
required the image to correct human translation from English (EN) to German (DE).

required for out of vocabulary (OOV) words, i.e.
correct words that are unknown to the model.

We propose an approach that takes into account
the strengths of a text-only baseline model and
only refines its translations when needed. Our ap-
proach is based on deliberation networks (Xia
et al., 2017) to jointly learn to generate draft trans-
lations and refine them based on left and right side
target context as well as structured visual informa-
tion. This approach outperforms previous work.

In order to further probe how well our models
can address the three problems mentioned above,
we perform a controlled experiment where we
minimise the interference of the frequency bias by
masking ambiguous and gender-related words, as
well as randomly selected words (to simulate noise
and OOV). This experiment shows that our multi-
modal refinement approach outperforms the text-
only one in more complex linguistic setups.

Our main contributions are: (i) a novel ap-
proach to MMT based on deliberation networks
and structured visual information which gives
state of the art results (Sections 3.2 and 5.1); (ii) a
frequency bias-free investigation on the need for
visual context in MMT (Sections 4.2 and 5.2);
and (iii) a thorough investigation on different vi-

sual representations for transformer-based archi-
tectures (Section 3.3).

2 Related work

MMT: Approaches to MMT vary with regards
to how they represent images and how they in-
corporate this information in the models. Initial
approaches use RNN-based sequence to sequence
models (Bahdanau et al., 2015) enhanced with a
single, global image vector, extracted as one of
the layers of a CNN trained for object classifica-
tion (He et al., 2016), often the penultimate or fi-
nal layer. The image representation is integrated
into the MT models by initialising the encoder or
decoder (Elliott et al., 2015; Caglayan et al., 2017;
Madhyastha et al., 2017); element-wise multipli-
cation with the source word annotations (Caglayan
et al., 2017); or projecting the image representa-
tion and encoder context to a common space to ini-
tialise the decoder (Calixto and Liu, 2017). Elliott
and Kádár (2017) and Helcl et al. (2018) instead
model the source sentence and reconstruct the im-
age representation jointly via multi-task learning.

An alternative way of exploring image rep-
resentations is to have an attention mechanism



6527

(Bahdanau et al., 2015) on the output of the last
convolutional layer of a CNN (Xu et al., 2015).
The layer represents the activation of K dif-
ferent convolutional filters on evenly quantised
N × N spatial regions of the image. Caglayan
et al. (2017) learn the attention weights for both
source text and visual encoders, while Calixto
et al. (2017); Delbrouck and Dupont (2017) com-
bine both attentions independently via a gating
scalar, and Libovický and Helcl (2017); Helcl
et al. (2018) apply a hierarchical attention distribu-
tion over two projected vectors where the attention
for each is learnt independently.

Helcl et al. (2018) is the closest to our work: we
also use a doubly-attentive transformer architec-
ture and explore spatial visual information. How-
ever, we differ in two main aspects (Section 3):
(i) our approach explores additional textual con-
text through a second pass decoding process and
uses visual information only at this stage, and (ii)
in addition to convolutional filters we use object-
level visual information. The latter has only been
explored to generate a single global representation
(Grönroos et al., 2018) and used for example to
initialise the encoder (Huang et al., 2016). We
note that translation refinement is different trans-
lation re-ranking from a text-only model based on
image representation (Shah et al., 2016; Hitschler
et al., 2016; Lala et al., 2018), since the latter as-
sumes that the correct translation can already be
produced by a text-only model.

Caglayan et al. (2019) investigate the impor-
tance and the contribution of multimodality for
MMT. They perform careful experiments by us-
ing input degradation and observe that, specially
under limited textual context, multimodal models
exploit the visual input to generate better transla-
tions. Caglayan et al. (2019) also show that MMT
systems exploit visual cues and obtain correct
translations even with typographical errors in the
source sentences. In this paper, we build upon this
idea and investigate the potential of visual cues for
refining translation.

Translation refinement: The idea of treating
machine translation as a two step approach dates
back to statistical models, e.g. in order to im-
prove a draft sentence-level translation by explor-
ing document-wide context through hill-climbing
for local refinements (Hardmeier et al., 2012). It-
erative refinement approaches have also been pro-
posed that start with a draft translation and then

predict discrete substitutions based on an attention
mechanism (Novak et al., 2016), or using non-
autoregressive methods with a focus on speeding
up decoding (Lee et al., 2018). Translation re-
finement can also be done through learning a sep-
arate model for automatic post-editing (Niehues
et al., 2016; Junczys-Dowmunt and Grundkiewicz,
2017; Chatterjee et al., 2018), but this requires ad-
ditional training data with draft translations and
their correct version.

An interesting approach is that of deliberation
networks, which jointly train an encoder and first
and second stage decoders (Xia et al., 2017). The
second stage decoder has access to both left and
right side context and this has been shown to im-
prove translation (Xia et al., 2017; Hassan et al.,
2018). We follow this approach as it offers a very
flexible framework to incorporate additional infor-
mation in the second stage decoder.

3 Model

We base our model on the transformer archi-
tecture (Vaswani et al., 2017) for neural ma-
chine translation. Our implementation is a multi-
layer encoder-decoder architecture that uses the
tensor2tensor1 (Vaswani et al., 2018) library.
The encoder and decoder blocks are as follows:

Encoder Block (E): The encoder block com-
prises of 6 layers, with each containing two sub-
layers of multi-head self-attention mechanism fol-
lowed by a fully connected feed forward neural
network. We follow the standard implementation
and employ residual connections between each
layer, as well as layer normalisation. The output
of the encoder forms the encoder memory which
consists of contextualised representations for each
of the source tokens (ME ).

Decoder Block (D): The decoder block also
comprises of 6 layers. It contains an additional
sublayer which performs multi-head attention over
the outputs of the encoder block. Specifically,
decoding layer dli is the result of a) multi-head
attention over the outputs of the encoder which
in turn is a function of the encoder memory and
the outputs from the previous layer: AD→E =
f(ME , dli−1) where, the keys and values are the
encoder outputs and the queries correspond to the
decoder input, and b) the multi-head self attention

1https://github.com/tensorflow/
tensor2tensor

https://github.com/tensorflow/tensor2tensor
https://github.com/tensorflow/tensor2tensor


6528

which is a function of the generated outputs from
the previous layer: AD = f(dli−1).

3.1 Deliberation networks

Deliberation networks (Hassan et al., 2018; Xia
et al., 2017) build on the standard sequence to se-
quence architecture to add an additional decoder
block (in our case, with 3 layers – see Figure 2).
The additional decoder (also referred to as second-
pass decoder) is conditioned on the source and
sampled outputs from the standard transformer de-
coder (the first-pass decoder). More concretely,
the second-pass decoder (D′) at layer d′l consists
of AD′ , AD′→E , AD′→D, where, AD′ and AD′→E
is similar to the standard deliberation architec-
ture multi-head attention over the encoder memory
and self attention respectively while, AD′→D is
the multi-head attention over outputs Od from the
first-pass decoder (D) (AD→E = f(Od, d

′
li−1

)).2

In our experiments, we obtain samples as a set
of translations from the first-pass decoder using
beam-search. Given a translation candidate, Od
consists of the first-pass decoder’s hidden layer be-
fore softmax concatenated with the embeddings of
the resultant words.

Figure 2: Our deliberation architecture: The second-
pass decoder is conditioned on the source and samples
output from the first-pass decoder. The second-pass de-
coder has access to (a) the object based features repre-
sented by embeddings, or (b) spacial image features.

3.2 Multimodal transformer & deliberation

Our multimodal transformer models follow one of
the two formulations below for conditioning trans-
lations on image information:

2In the implementation we used, the deliberation network
trains 345M parameters, as compared to the Transformer with
210M parameters.

Additive image conditioning (AIC): A pro-
jected image vector is added to each of the out-
puts of the encoder. The projections matrices are
parameters that are jointly learned with the model.

Attention over image features (AIF): The
model attends over image features, as in Helcl
et al. (2018), where the decoder block now
contains an additional cross-attention sub-layer
AD′→V which attends to the visual information
(V). The keys and values correspond to the visual
information.

Within the deliberation network framework,
based on the previously discussed observation
(Section 1) that images are only needed in a small
number of cases, we propose to add visual cross-
attention only to the second-pass decoder block
(see Figure 2).

3.3 Image features

Motivated by previous work that indicates the
importance of structured information from im-
ages (Caglayan et al., 2017; Wang et al., 2018;
Madhyastha et al., 2018), we focus on structural
forms of image representations, including the spa-
tially aware feature maps from CNNs and infor-
mation extracted from automatic object detectors.

Spatial image features: We use spatial feature
maps from the last convolutional layer of a pre-
trained ResNet-50 (He et al., 2016) CNN-based
image classifier for every image.3 These feature
maps contain output activations for various filters
while preserving spatial information. They have
been used in various vision to language tasks in-
cluding image captioning (Xu et al., 2015) and
multimodal machine translation (Section 2). Our
formulation for the integration of these features
into the deliberation network is shown in Figure 2,
setup (b). We use the the AIF setup and refer to
models that use the representation as att.

Object-based image features: We use a
bag-of-objects representation where the objects
are obtained using an off-shelf object detec-
tor (Kuznetsova et al., 2018) based on the Open
Images dataset. This representations is a sparse
545-dimensional vector with the frequency of each
(545) given object in an image. This is inspired
by previous research that investigates the potential
of object-based information for vision to language
tasks (Mitchell et al., 2012; Wang et al., 2018). We

3Provided at http://statmt.org/wmt18/
multimodal-task.html.

http://statmt.org/wmt18/multimodal-task.html.
http://statmt.org/wmt18/multimodal-task.html.


6529

use the the AIC setup and refer to models that use
the representation as sum.

Object-based embedding features: The bag-
of-objects representations makes it hard to ex-
ploit object-to-object similarity, since visual rep-
resentations of different objects can be very dif-
ferent. To mitigate this, we propose a simple ex-
tension using bag-of-object embeddings. We rep-
resent each object using the pre-trained GLoVe-
based (Pennington et al., 2014) 50-dimensional
word vectors for their categories (e.g. woman).
We use the the AIF based setup and refer to mod-
els that use the representation as obj (Figure 2
setup (a)).

4 Experimental settings

4.1 Data

We build and test our MMT models on the
Multi30K dataset (Elliott et al., 2016). Each
image in Multi30K contains one English (EN)
description taken from Flickr30K (Young et al.,
2014) and human translations into German (DE),
French (FR) and Czech (Specia et al., 2016; El-
liott et al., 2017; Barrault et al., 2018). The dataset
contains 29,000 instances for training, 1,014 for
development, and 1,000 for test. We only exper-
iment with German and French, which are lan-
guages for which we have in-house expertise for
the type of analysis we present. In addition to the
official Multi30K test set (test 2016), we also use
the test set from the latest WMT evaluation com-
petition, test 2018 (Barrault et al., 2018).4

4.2 Degradation of source

In addition to using the Multi30K dataset as is
(standard setup), we probe the ability of our
models to address the three linguistic phenom-
ena where additional context has been proved im-
portant (Section 1): ambiguities, gender-neutral
words and noisy input. In a controlled experi-
ment where we aim to remove the influence of fre-
quency biases, we degrade the source sentences by
masking words through three strategies to replace
words by a placeholder: random source words,
ambiguous source words and gender unmarked
source words. The procedure is applied to the
train, validation and test sets. For the resulting
dataset generated for each setting, we compare
models having access to text-only context versus

4The pre-processed datasets provided by the organisers
were used without additional pre-processing.

additional text and multimodal contexts. We seek
to get insights into the contribution of each type of
context to address each type of degradation.

Random content words In this setting (RND)
we simulate erroneous source words by randomly
dropping source content words. We first tag
the entire source sentences using the spacy
toolkit (Honnibal and Montani, 2017) and then
drop nouns, verbs, adjectives and adverbs and re-
place these with a default BLANK token. By fo-
cusing on content words, we differ from previ-
ous work that suggests that neural machine trans-
lation is robust to non-content word noise in the
source (Klubička et al., 2017).

Ambiguous words In this setting (AMB), we
rely on the MLT dataset (Lala et al., 2018) which
provides a list of source words with multiple trans-
lations in the Multi30k training set. We replace
ambiguous words with the BLANK token in the
source language, which results in two language-
specific datasets.

Person words In this setting (PERS), we use
the Flickr Entities dataset (Plummer et al., 2017)
to identify all the words that were annotated by
humans as corresponding to the category per-
son.5 We then replace such source words with the
BLANK token.

The statistics of the resulting datasets for the
three degradation strategies are shown in Table 1.
We note that RND and PERS are the same for lan-
guage pairs as the degradation only depends on the
source side, while for AMB the words replaced de-
pend on the target language.

setup % sent. avg. blanks per sent.

RND 100 1.5
AMB DE 83 2
AMB FR 77 1.8
PERS 92 1.6

Table 1: Statistics of datasets after applying source
degradation strategies

4.3 Models

Based on the models described in Section 3
we experiment with eight variants: (a) baseline
transformer model (base); (b) base with AIC

5We pre-processed the initial dataset to remove noise. We
also add the gender-marked pronouns he, she, her and his to
the person word list.



6530

(base+sum); (c) base with AIF using spacial
(base+att) or object based (base+obj) image
features; (d) standard deliberation model (del);
(e) deliberation models enriched with image infor-
mation: del+sum, del+att and del+obj.

4.4 Training
In all cases, we optimise our models with cross
entropy loss. For deliberation network models,
we first train the standard transformer model un-
til convergence, and use it to initialise the encoder
and first-pass decoder. For each of the training
samples, we follow (Xia et al., 2017) and obtain a
set of 10-best samples from the first pass decoder,
with a beam search of size 10. We use these as the
first-pass decoder samples. We use Adam as opti-
miser (Kingma and Ba, 2014) and train the model
until convergence.6

test 2016 test 2018
model M B M B

D
E

MMT (Helcl et al., 2018) 53.1 38.4 - -
base 54.5 36.4 45.0 26.5

base+sum 54.2 35.9 45.0 26.4
base+att 54.5 36.9 45.3 27.2
base+obj 54.5 36.4 45.0 26.7

del 55.5* 37.7 46.3* 27.7
del+sum 55.2* 37.3 46.3* 27.7
del+att 55.1* 37.2 46.1* 27.4
del+obj 55.6* 38.0 46.5* 27.6

FR

MMT (Helcl et al., 2018) 75.0 60.6 - -
base 73.7 59.0 56.4 37.0

base+sum 73.9 59.2 56.6 37.1
base+att 73.5 58.7 56.1 36.2
base+obj 72.9 57.3 55.8 36.3

del 74.6* 60.1 57.2* 37.8
del+sum 74.3* 59.6 56.9* 37.2
del+att 73.7† 59.2 56.3† 36.9
del+obj 74.4* 59.8 57.0* 37.4

Table 2: Results for the test sets 2016 and 2018. M
denotes METEOR, B – BLEU; * marks statistically sig-
nificant changes for METEOR (p-value≤ 0.05) as com-
pared to base, † – as compared to del. Bold high-
lights statistically significant improvements. We report
previous state of the art results for multimodal models
from (Helcl et al., 2018).

5 Results

In this section we present results of our exper-
iments, first in the original dataset without any

6We built on the tensor2tensor implementation of
deliberation nets in https://github.com/ustctf/
delibnet using the transformer big parameters with
a learning rate of 0.05 with 8K warmup steps for both the first
and the second-pass decoders, and early stopping with the pa-
tience of 10 epochs based on the validation BLEU score.

source degradation (Section 5.1) and then in the
setup with various source degradation strategies
(Section 5.2).

5.1 Standard setup

Table 2 shows the results of our main experiments
on the 2016 and 2018 test sets for French and
German. We use Meteor (Denkowski and Lavie,
2014) as the main metric, as in the WMT tasks
(Barrault et al., 2018). We compare our trans-
former baseline to transformer models enriched
with image information, as well as to the delibera-
tion models, with or without image information.

We first note that our multimodal models
achieve the state of the art performance for trans-
former networks (constrained models) on the
English-German dataset, as compared to (Helcl
et al., 2018). Second, our deliberation models
lead to significant improvements over this base-
line across test sets (average ∆METEOR = 1,
∆BLEU = 1).

Transformer-based models enriched with im-
age information (base+sum, base+att and
base+obj), on the other hand, show no ma-
jor improvements with respect to the base per-
formance. This is also the case for delibera-
tion models with image information (del+sum,
del+att, del+obj), which do not show sig-
nificant improvement over the vanilla deliberation
performance (del).

However, as it has been shown in the WMT
shared tasks on MMT (Specia et al., 2016; El-
liott et al., 2017; Barrault et al., 2018), automatic
metrics often fail to capture nuances in transla-
tion quality, such as, the ones we expect the vi-
sual modality to help with, which – according to
human perception – lead to better translations. To
test this assumption in our settings, we performed
human evaluation involving professional transla-
tors and native speakers of both French and Ger-
man (three annotators).

The annotators were asked to rank randomly
selected test samples according to how well they
convey the meaning of the source, given the image
(50 samples per language pair per annotator). For
each source segment, the annotator was shown the
outputs of three systems: base+att, the current
MMT state-of-the-art (Helcl et al., 2018), del and
del+obj. A rank could be assigned from 1 to
3, allowing ties (Bojar et al., 2017). Annotators
could assign zero rank to all translations if they

https://github.com/ustctf/delibnet
https://github.com/ustctf/delibnet


6531

EN: Two men work under the hood of a white race car.

base+att: Zwei Männer arbeiten unter der Motorhaube eines weißen Rennens.

del: Zwei Männer arbeiten unter der Motorhaube eines weißen Autos.

del+obj: Zwei Männer arbeiten unter der Motorhaube eines weißen Rennwagen.

DE: Zwei Männer arbeiten unter der Haube eines weißen Rennautos.

(a) base+att translates race car with Rennen (race), del with Auto (car) and del+obj with Rennwagen (race car).
Objects: land, vehicle, car, wheel

EN: A young child holding an oar paddling a blue kayak in a body of water.

base+att: Un jeune enfant tenant une rame dans un kayak bleu.

del: Un jeune enfant tenant une rame dans un kayak bleu sur un plan d’eau.

del+obj: Un jeune enfant tenant une rame dans un kayak bleu pagayant
sur un plan d’eau.

FR: Un jeune enfant avec une rame pagayant dans un kayak bleu
sur un plan d’eau.

(b) del and del+obj translate in a body of water with sur un plan d’eau (on a body of water), missing in base+att.
del+obj translates the word paddling with pagayant (paddling). Objects: paddle, canoe

Figure 3: Examples of improvements of del and del+obj over base+att for test set 2016 for French and
German. Underlined words represent some of the improvements.

were judged incomprehensible.
Following the common practice in WMT (Bo-

jar et al., 2017), each system was then assigned a
score which reflects the proportion of times it was
judged to be better or equal other systems.

Table 3 shows the human evaluation results.
They are consistent with the automatic evaluation
results when it comes to the preference of hu-
mans towards the deliberation-based setups, but
show a more positive outlook regarding the addi-
tion of visual information (del+obj over del)
for French.

lang base+att del del+obj

DE 0.35 0.62 0.59
FR 0.41 0.6 0.67

Table 3: Human ranking results: normalised rank
(micro-averaged). Bold highlights best results.

Manual inspection of translations suggests that
deliberation setups tend to improve both the gram-
maticality and adequacy of the first pass outputs.
For German, the most common modifications per-
formed by the second-pass decoder are substitu-
tions of adjectives and verbs (for test 2016, 15%
and 12% respectively, of all the edit distance oper-
ations). Changes to adjectives are mainly gram-
matical, changes to verbs are contextual (e.g.,
changing laufen to rennen, both verbs mean run,
but the second refers to running very fast). For

French, 15% of all the changes are substitutions
of nouns (for test 2016). These are again very
contextual. For example, the French word tra-
vailleur (worker) is replaced by ouvrier (manual
worker) in the contexts where tools, machinery or
buildings are mentioned. For our analysis we used
again spacy.

The information on detected objects is partic-
ularly helpful for specific adequacy issues. Fig-
ure 3 demonstrates some such cases. In the first
case, the base+att model misses the transla-
tion of race car: the German word Rennen trans-
lates only the word race. del introduces the word
car (Auto) into the translation. Finally, del+obj
correctly translates the expression race car (Ren-
nwagen) by exploiting the object information. For
French, del translates the source part in a body
of water, missing from the base+att transla-
tion. del+obj additionally translated the word
paddling according to the detected object Paddle.

5.2 Source degradation setup

Results of our source degradation experiments are
shown in Table 4. A first observation is that – as
with the standard setup – the performance of our
deliberation models is overall better than that of
the base models. The results of the multimodal
models differ for German and French. For Ger-
man, del+obj is the most successful configu-
ration and shows statistically significant improve-



6532

RND AMB PERS
test 2016 test 2018 test 2016 test 2018 test 2016 test 2018

model M B M B M B M B M B M B
D

E
base 45.6 27.1 37.7 20.0 48.4 30.1 38.9 21.0 47.0 28.6 40.3 22.2
del 44.6* 25.1 36.8* 18.1 47.7 29.0 38.0* 19.0 47.5 29.0 40.9 22.0

del+sum 45.7† 27.2 38.1† 19.9 46.9*† 27.9 37.2*† 18.7 48.1* 29.8 41.1* 22.4
del+obj 46.5*† 28.1 39.0*† 20.7 49.8*† 31.3 40.0*† 21.3 48.1* 29.4 41.6*† 23.4

FR

base 59.3 43.4 46.3 28.1 66.4 51.2 49.2 30.4 63.9 48.6 50.3 31.7
del 61.0* 45.3 47.1* 28.4 67.3* 52.2 50.2* 31.3 64.5* 49.3 51.2* 32.4

del+sum 60.4* 44.4 47.5* 29.3 67.7* 52.8 50.4* 31.5 65.0* 49.7 51.1* 32.1
del+obj 61.3* 45.4 47.9*† 29.4 67.7* 52.6 50.5* 31.7 65.0* 49.5 50.9* 32.2

Table 4: Results for the test sets 2016 and 2018 for the three degradation configurations: RND, AMB and PERS. M
denotes METEOR, B – BLEU; * marks statistically significant changes as computed for METEOR (p-value ≤ 0.05)
as compared to base, † – as compared to del. Bold highlights statistically significant improvements over base.

ments over base for all setups. Moreover, for
RND and AMB, it shows statistically significant im-
provements over del. However, especially for
RND and AMB, del and del+sum are either the
same or slightly worse than base.

For French, all the deliberation models show
statistically significant improvements over base
(average ∆METEOR = 1, ∆BLEU = 1.1), but
the image information added to del only improve
scores significantly for test 2018 RND.

This difference in performances for French and
German is potentially related to the need of more
significant restructurings while translating from
English into German.7 This is where a more com-
plex del+obj architecture is more helpful. This
is especially true for RND and AMB setups where
blanked words could also be verbs, the part-of-
speech most influenced by word order differences
between English and German (see the decreasing
complexity of translations for del and del+obj
for the example (c) in Figure 4).

To get an insight into the contribution of differ-
ent contexts to the resolution of blanks, we per-
formed manual analysis of examples coming from
the English-German base, del and del+obj
setups (50 random examples per setup), where we
count correctly translated blanks per system.

The results are shown in Table 5. As expected,
they show that the RND and AMB blanks are more
difficult to resolve (at most 40% resolved as com-
pared to 61% for PERS). Translations of the ma-
jority of those blanks tend to be guessed by the

7English and French are both languages with the subject–
verb–object (SVO) sentence structure. German, on the other
hand, can have subject–object–verb (SOV) constructions. For
example, a German sentence Gestern bin ich in London gewe-
sen (Yesterday have I to London been) would need to be re-
structured to Yesterday I have been to London in English.

setup base del del+obj gold

RND 22 23 24 79
AMB 29 25 33 88
PERS 43 46 51 84

Table 5: Results of human annotation of blanked trans-
lations (English-German). We report counts of blanks
resolved by each system, as well as total source blank
count for each selection (50 sentences selected ran-
domly).

textual context alone (especially for verbs). Im-
age information is more helpful for PERS: we ob-
serve an increase of 10% in resolved blanks for
del+obj as compared to del. However, for
PERS the textual context is still enough in the ma-
jority of the cases: models tend to associate men
with sports or women with cooking and are usu-
ally right (see Figure 4 example (c)).

The cases where image helps seem to be those
with rather generic contexts: see Figure 4 (b)
where enjoying a summer day is not associated
with any particular gender and make other mod-
els choose homme (man) or femme (woman), and
only base+obj chooses enfant (child) (the op-
tion closest to the reference).

In some cases detected objects are inaccurate or
not precise enough to be helpful (e.g., when an ob-
ject Person is detected) and can even harm correct
translations.

6 Conclusions

We have proposed a novel approach to multi-
modal machine translation which makes better
use of context, both textual and visual. Our
results show that further exploring textual con-
text through deliberation networks already leads



6533

EN: Three farmers harvest rice out in a rice field.

base: Drei Bauern ernten sich mit einem Reisfeld.

del: Drei Bauern ernten Reis mit einem Reisfeld.

del+obj: Drei Bauern ernten sich mit einem Reishut auf.

DE: Drei Farmer ernten Reis auf einem Feld.

(a) Example of a blank resolved by the textual context for AMB: field translated as Reisfeld (rice field) by base. del+obj
incorrectly translated the blank into Reishut (rice hat) due to detected objects. Objects: person, clothing, mammal

EN: The boy is outside enjoying a summer day.

base: L’homme profite d’une journée d’été.

del: La femme profite d’une journée d’été.

del+obj: L’enfant profite d’une journée d’été.

FR: Le garçon est dehors, profitant d’une journée d’été.

(b) Example of a blank resolved by the multimodal context for PERS. The textual context is too generic and del+obj uses
the detected objects to correctly translate boy into l’enfant (child). Objects: clothing, face, tree, boy, jeans

EN: Dirt biker makes a sloping turn in a forest during the fall.

base: Geländemotorradfahrer macht in einem Wald eine Kurve.

del: Geländemotorradfahrer macht in einem Herbst während Zuschauer eine
Kurve.

del+obj: Geländemotorradfahrer macht in einem Herbst eine Kurve.

DE: Ein offroad-biker fährt im Herbst durch eine steile Kurve.

(c) Example of a blank resolved by the textual context for PERS. biker correctly translated into the Masc. form
Geländemotorradfahrer (dirt biker) by base. Objects: person, tree, bike, helmet

Figure 4: Examples of resolved blanks for test set 2016. Underlined text denotes blanked words and their transla-
tions. Object field indicates the detected objects.

to better results than the previous state of the
art. Adding visual information, and in particu-
lar structural representations of this information,
proved beneficial when input text contains noise
and the language pair requires substantial restruc-
turing from source to target. Our findings sug-
gest that the combination of a deliberation ap-
proach and information from additional modali-
ties is a promising direction for machine transla-
tion that is robust to noisy input. Our code and
pre-processing scripts are available at https://
github.com/ImperialNLP/MMT-Delib.

Acknowledgments

The authors thank the anonymous reviewers for
their useful feedback. This work was supported
by the MultiMT (H2020 ERC Starting Grant No.
678017) and MMVC (Newton Fund Institutional
Links Grant, ID 352343575) projects. We also
thank the annotators for their valuable help.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
International Conference on Learning Representa-
tions (ICLR).

Loı̈c Barrault, Fethi Bougares, Lucia Specia, Chi-
raag Lala, Desmond Elliott, and Stella Frank. 2018.
Findings of the third shared task on multimodal ma-
chine translation. In Proceedings of the Third Con-
ference on Machine Translation: Shared Task Pa-
pers, pages 304–323. Association for Computational
Linguistics.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Shujian Huang,
Matthias Huck, Philipp Koehn, Qun Liu, Varvara
Logacheva, Christof Monz, Matteo Negri, Matt
Post, Raphael Rubino, Lucia Specia, and Marco
Turchi. 2017. Findings of the 2017 conference
on machine translation (WMT17). In Proceedings
of the Second Conference on Machine Translation,
pages 169–214. Association for Computational Lin-
guistics.

Ozan Caglayan, Walid Aransa, Adrien Bardet, Mer-
cedes Garcı́a-Martı́nez, Fethi Bougares, Loı̈c Bar-
rault, Marc Masana, Luis Herranz, and Joost van de

https://github.com/ImperialNLP/MMT-Delib
https://github.com/ImperialNLP/MMT-Delib
https://arxiv.org/abs/1409.0473
https://arxiv.org/abs/1409.0473
http://aclweb.org/anthology/W18-6402
http://aclweb.org/anthology/W18-6402
https://doi.org/10.18653/v1/W17-4717
https://doi.org/10.18653/v1/W17-4717


6534

Weijer. 2017. LIUM-CVC submissions for WMT17
multimodal translation task. In Proceedings of the
Second Conference on Machine Translation, pages
432–439. Association for Computational Linguis-
tics.

Ozan Caglayan, Pranava Madhyastha, Lucia Specia,
and Loı̈c Barrault. 2019. Probing the need for visual
context in multimodal machine translation. In Pro-
ceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Vol-
ume 1 (Long and Short Papers), pages 4159–4170,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.

Iacer Calixto and Qun Liu. 2017. Incorporating global
visual features into attention-based neural machine
translation. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 992–1003. Association for Compu-
tational Linguistics.

Iacer Calixto, Qun Liu, and Nick Campbell. 2017.
Doubly-attentive decoder for multi-modal neural
machine translation. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1913–
1924. Association for Computational Linguistics.

Rajen Chatterjee, Matteo Negri, Raphael Rubino, and
Marco Turchi. 2018. Findings of the WMT 2018
shared task on automatic post-editing. In Proceed-
ings of the Third Conference on Machine Transla-
tion, Volume 2: Shared Task Papers, pages 723–738,
Belgium, Brussels. Association for Computational
Linguistics.

Jean-Benoit Delbrouck and Stéphane Dupont. 2017.
An empirical study on the effectiveness of images in
multimodal neural machine translation. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 910–919.
Association for Computational Linguistics.

Michael Denkowski and Alon Lavie. 2014. Meteor
universal: Language specific translation evaluation
for any target language. In Proceedings of the EACL
2014 Workshop on Statistical Machine Translation.

Desmond Elliott, Stella Frank, Loı̈c Barrault, Fethi
Bougares, and Lucia Specia. 2017. Findings of the
second shared task on multimodal machine transla-
tion and multilingual image description. In Proceed-
ings of the Second Conference on Machine Trans-
lation, Volume 2: Shared Task Papers, pages 215–
233, Copenhagen, Denmark. Association for Com-
putational Linguistics.

Desmond Elliott, Stella Frank, and Eva Hasler. 2015.
Multi-language image description with neural se-
quence models. CoRR, abs/1510.04709.

Desmond Elliott, Stella Frank, Khalil Sima’an, and Lu-
cia Specia. 2016. Multi30k: Multilingual english-
german image descriptions. In 5th Workshop on Vi-
sion and Language, pages 70–74, Berlin, Germany.

Desmond Elliott and Àkos Kádár. 2017. Imagination
improves multimodal translation. In Proceedings of
the Eighth International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 130–141. Asian Federation of Natural Lan-
guage Processing.

Joel Escudé Font and Marta R. Costa-Jussà. 2019.
Equalizing gender biases in neural machine trans-
lation with word embeddings techniques. CoRR,
abs/1901.03116.

Stella Frank, Desmond Elliott, and Lucia Specia. 2018.
Assessing multilingual multimodal image descrip-
tion: Studies of native speaker preferences and
translator choices. Natural Language Engineering,
24(3):393413.

Stig-Arne Grönroos, Benoit Huet, Mikko Kurimo,
Jorma Laaksonen, Bernard Merialdo, Phu Pham,
Mats Sjöberg, Umut Sulubacak, Jörg Tiedemann,
Raphael Troncy, and Raúl Vázquez. 2018. The
MeMAD submission to the WMT18 multimodal
translation task. In Proceedings of the Third Confer-
ence on Machine Translation: Shared Task Papers,
pages 603–611. Association for Computational Lin-
guistics.

Christian Hardmeier, Joakim Nivre, and Jörg Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179–1190. Association for Computational Linguis-
tics.

Hany Hassan, Anthony Aue, Chang Chen, Vishal
Chowdhary, Jonathan Clark, Christian Feder-
mann, Xuedong Huang, Marcin Junczys-Dowmunt,
William Lewis, Mu Li, et al. 2018. Achieving hu-
man parity on automatic Chinese to English news
translation. CoRR, abs/1803.05567.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.

Jinďrich Helcl, Jinďrich Libovický, and Dusan Varis.
2018. CUNI system for the WMT18 multimodal
translation task. In Proceedings of the Third Confer-
ence on Machine Translation: Shared Task Papers,
pages 616–623. Association for Computational Lin-
guistics.

Lisa Anne Hendricks, Kaylee Burns, Kate Saenko,
Trevor Darrell, and Anna Rohrbach. 2018. Women
also snowboard: Overcoming bias in captioning
models. In Computer Vision – ECCV 2018, pages
793–811. Springer International Publishing.

Julian Hitschler, Shigehiko Schamoni, and Stefan Rie-
zler. 2016. Multimodal pivots for image caption

https://doi.org/10.18653/v1/W17-4746
https://doi.org/10.18653/v1/W17-4746
https://www.aclweb.org/anthology/N19-1422
https://www.aclweb.org/anthology/N19-1422
https://doi.org/10.18653/v1/D17-1105
https://doi.org/10.18653/v1/D17-1105
https://doi.org/10.18653/v1/D17-1105
https://doi.org/10.18653/v1/P17-1175
https://doi.org/10.18653/v1/P17-1175
http://www.aclweb.org/anthology/W18-6453
http://www.aclweb.org/anthology/W18-6453
https://doi.org/10.18653/v1/D17-1095
https://doi.org/10.18653/v1/D17-1095
http://www.cs.cmu.edu/~alavie/METEOR/pdf/meteor-1.5.pdf
http://www.cs.cmu.edu/~alavie/METEOR/pdf/meteor-1.5.pdf
http://www.cs.cmu.edu/~alavie/METEOR/pdf/meteor-1.5.pdf
http://www.aclweb.org/anthology/W17-4718
http://www.aclweb.org/anthology/W17-4718
http://www.aclweb.org/anthology/W17-4718
http://arxiv.org/abs/1510.04709
http://arxiv.org/abs/1510.04709
http://aclweb.org/anthology/W16-3210
http://aclweb.org/anthology/W16-3210
http://aclweb.org/anthology/I17-1014
http://aclweb.org/anthology/I17-1014
http://arxiv.org/abs/1901.03116
http://arxiv.org/abs/1901.03116
https://doi.org/10.1017/S1351324918000074
https://doi.org/10.1017/S1351324918000074
https://doi.org/10.1017/S1351324918000074
http://aclweb.org/anthology/W18-6439
http://aclweb.org/anthology/W18-6439
http://aclweb.org/anthology/W18-6439
http://aclweb.org/anthology/D12-1108
http://aclweb.org/anthology/D12-1108
http://arxiv.org/abs/1803.05567
http://arxiv.org/abs/1803.05567
http://arxiv.org/abs/1803.05567
https://ieeexplore.ieee.org/document/7780459
https://ieeexplore.ieee.org/document/7780459
http://aclweb.org/anthology/W18-6441
http://aclweb.org/anthology/W18-6441
http://openaccess.thecvf.com/content_ECCV_2018/papers/Lisa_Anne_Hendricks_Women_also_Snowboard_ECCV_2018_paper.pdf
http://openaccess.thecvf.com/content_ECCV_2018/papers/Lisa_Anne_Hendricks_Women_also_Snowboard_ECCV_2018_paper.pdf
http://openaccess.thecvf.com/content_ECCV_2018/papers/Lisa_Anne_Hendricks_Women_also_Snowboard_ECCV_2018_paper.pdf
https://doi.org/10.18653/v1/P16-1227


6535

translation. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 2399–2409, Berlin,
Germany. Association for Computational Linguis-
tics.

Matthew Honnibal and Ines Montani. 2017. spacy 2:
Natural language understanding with bloom embed-
dings, convolutional neural networks and incremen-
tal parsing. To appear.

Po-Yao Huang, Frederick Liu, Sz-Rung Shiang, Jean
Oh, and Chris Dyer. 2016. Attention-based multi-
modal neural machine translation. In Proceedings of
the First Conference on Machine Translation, pages
639–645. Association for Computational Linguis-
tics.

Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2017. An exploration of neural sequence-to-
sequence architectures for automatic post-editing.
In Proceedings of the Eighth International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 120–129. Asian Feder-
ation of Natural Language Processing.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. International
Conference on Learning Representations.

Filip Klubička, Antonio Toral, and Vı́ctor M Sánchez-
Cartagena. 2017. Fine-grained human evaluation
of neural versus phrase-based machine translation.
The Prague Bulletin of Mathematical Linguistics,
108(1):121–132.

Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper
Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Ka-
mali, Stefan Popov, Matteo Malloci, Tom Duerig,
et al. 2018. The open images dataset v4: Uni-
fied image classification, object detection, and vi-
sual relationship detection at scale. arXiv preprint
arXiv:1811.00982.

Chiraag Lala, Pranava Swaroop Madhyastha, Carolina
Scarton, and Lucia Specia. 2018. Sheffield sub-
missions for WMT18 multimodal translation shared
task. In Proceedings of the Third Conference on Ma-
chine Translation, Volume 2: Shared Task Papers,
pages 630–637, Belgium, Brussels. Association for
Computational Linguistics.

Jason Lee, Elman Mansimov, and Kyunghyun Cho.
2018. Deterministic non-autoregressive neural se-
quence modeling by iterative refinement. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1173–
1182. Association for Computational Linguistics.

Jinďrich Libovický and Jinďrich Helcl. 2017. Attention
strategies for multi-source sequence-to-sequence
learning. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 196–202, Van-
couver, Canada. Association for Computational Lin-
guistics.

Pranava Madhyastha, Josiah Wang, and Lucia Specia.
2017. Sheffield MultiMT: Using object posterior
predictions for multimodal machine translation. In
Proceedings of the Second Conference on Machine
Translation, pages 470–476. Association for Com-
putational Linguistics.

Pranava Swaroop Madhyastha, Josiah Wang, and Lu-
cia Specia. 2018. End-to-end image captioning ex-
ploits multimodal distributional similarity. CoRR,
abs/1809.04144.

Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Men-
sch, Alex Berg, Tamara Berg, and Hal Daume III.
2012. Midge: Generating image descriptions from
computer vision detections. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 747–
756, Avignon, France. Association for Computa-
tional Linguistics.

Jan Niehues, Eunah Cho, Thanh-Le Ha, and Alex
Waibel. 2016. Pre-translation for neural machine
translation. In Proceedings of COLING 2016, the
26th International Conference on Computational
Linguistics: Technical Papers, pages 1828–1836.
The COLING 2016 Organizing Committee.

Roman Novak, Michael Auli, and David Grangier.
2016. Iterative refinement for machine translation.
CoRR, abs/1610.06602.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar. Association for Computational Linguistics.

Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Juan C Caicedo, Julia Hockenmaier, and Svetlana
Lazebnik. 2017. Flickr30k entities: Collecting
region-to-phrase correspondences for richer image-
to-sentence models. volume 123, pages 74–93,
Hingham, MA, USA. Kluwer Academic Publishers.

Kashif Shah, Josiah Wang, and Lucia Specia. 2016.
SHEF-Multimodal: Grounding machine translation
on images. In First Conference on Machine Trans-
lation, Volume 2: Shared Task Papers, WMT, pages
657–662. Association for Computational Linguis-
tics.

Lucia Specia, Stella Frank, Khalil Sima’an, and
Desmond Elliott. 2016. A shared task on multi-
modal machine translation and crosslingual image
description. In Proceedings of the First Conference
on Machine Translation, pages 543–553. Associa-
tion for Computational Linguistics.

Eva Vanmassenhove, Christian Hardmeier, and Andy
Way. 2018. Getting gender right in neural machine
translation. In Proceedings of the 2018 Conference

https://doi.org/10.18653/v1/P16-1227
http://www.aclweb.org/anthology/W/W16/W16-2360
http://www.aclweb.org/anthology/W/W16/W16-2360
http://aclweb.org/anthology/I17-1013
http://aclweb.org/anthology/I17-1013
https://arxiv.org/pdf/1412.6980.pdf
https://arxiv.org/pdf/1412.6980.pdf
https://arxiv.org/pdf/1706.04389.pdf
https://arxiv.org/pdf/1706.04389.pdf
https://arxiv.org/pdf/1811.00982.pdf
https://arxiv.org/pdf/1811.00982.pdf
https://arxiv.org/pdf/1811.00982.pdf
http://www.aclweb.org/anthology/W18-6442
http://www.aclweb.org/anthology/W18-6442
http://www.aclweb.org/anthology/W18-6442
http://aclweb.org/anthology/D18-1149
http://aclweb.org/anthology/D18-1149
https://doi.org/10.18653/v1/P17-2031
https://doi.org/10.18653/v1/P17-2031
https://doi.org/10.18653/v1/P17-2031
https://doi.org/10.18653/v1/W17-4752
https://doi.org/10.18653/v1/W17-4752
http://arxiv.org/abs/1809.04144
http://arxiv.org/abs/1809.04144
https://www.aclweb.org/anthology/E12-1076
https://www.aclweb.org/anthology/E12-1076
http://aclweb.org/anthology/C16-1172
http://aclweb.org/anthology/C16-1172
http://arxiv.org/abs/1610.06602
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.1007/s11263-016-0965-7
https://doi.org/10.1007/s11263-016-0965-7
https://doi.org/10.1007/s11263-016-0965-7
http://www.aclweb.org/anthology/W/W16/W16-2363
http://www.aclweb.org/anthology/W/W16/W16-2363
https://doi.org/10.18653/v1/W16-2346
https://doi.org/10.18653/v1/W16-2346
https://doi.org/10.18653/v1/W16-2346
http://aclweb.org/anthology/D18-1334
http://aclweb.org/anthology/D18-1334


6536

on Empirical Methods in Natural Language Pro-
cessing, pages 3003–3008. Association for Compu-
tational Linguistics.

Ashish Vaswani, Samy Bengio, Eugene Brevdo, Fran-
cois Chollet, Aidan Gomez, Stephan Gouws, Llion
Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Par-
mar, Ryan Sepassi, Noam Shazeer, and Jakob
Uszkoreit. 2018. Tensor2Tensor for neural machine
translation. pages 193–199.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30, pages 5998–6008. Curran Asso-
ciates, Inc.

Josiah Wang, Pranava Swaroop Madhyastha, and Lu-
cia Specia. 2018. Object counts! Bringing explicit
detections back into image captioning. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), New Orleans, Louisiana. Associa-
tion for Computational Linguistics.

Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,
Nenghai Yu, and Tie-Yan Liu. 2017. Deliberation
networks: Sequence generation beyond one-pass de-
coding. In Advances in Neural Information Pro-
cessing Systems 30, pages 1784–1794. Curran As-
sociates, Inc.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual atten-
tion. In Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of
Proceedings of Machine Learning Research, pages
2048–2057. PMLR.

Peter Young, Alice Lai, Micah Hodosh, and Julia
Hockenmaier. 2014. From image descriptions to vi-
sual denotations: New similarity metrics for seman-
tic inference over event descriptions. 2:67–78.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2018. Gender bias in
coreference resolution: Evaluation and debiasing
methods. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 2 (Short Papers), pages 15–20. As-
sociation for Computational Linguistics.

A Appendices

https://www.aclweb.org/anthology/W18-1819
https://www.aclweb.org/anthology/W18-1819
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
https://www.aclweb.org/anthology/N18-1198
https://www.aclweb.org/anthology/N18-1198
http://papers.nips.cc/paper/6775-deliberation-networks-sequence-generation-beyond-one-pass-decoding.pdf
http://papers.nips.cc/paper/6775-deliberation-networks-sequence-generation-beyond-one-pass-decoding.pdf
http://papers.nips.cc/paper/6775-deliberation-networks-sequence-generation-beyond-one-pass-decoding.pdf
http://proceedings.mlr.press/v37/xuc15.html
http://proceedings.mlr.press/v37/xuc15.html
http://proceedings.mlr.press/v37/xuc15.html
https://transacl.org/ojs/index.php/tacl/article/view/229
https://transacl.org/ojs/index.php/tacl/article/view/229
https://transacl.org/ojs/index.php/tacl/article/view/229
https://doi.org/10.18653/v1/N18-2003
https://doi.org/10.18653/v1/N18-2003
https://doi.org/10.18653/v1/N18-2003


6537

EN: A bride and groom kiss under the bride’s veil.

base: Ein Mann und eine Frau küssen sich unter den Blicken der Frau.

del: Ein Mann und eine Frau küssen sich unter dem Brautschleier.

del+obj: Ein Mann und eine Frau küssen sich unter den hin.

DE: Eine Braut und Bräutigam küssen sich unter dem Brautschleier .

(a) PERS example: bride and groom translated are correctly translated by base into Frau (wife) and Mann (husband). Objects:
face, woman, dress

EN: A brown dog runs down the sandy beach.

base: Ein brauner Hund läuft an einem sandigen Strand.

del: Ein brauner Hund rennt den Sandstrand hinunter.

del+obj: Ein brauner Hund läuft an einem sandigen Strand hinunter.

FR: Ein brauner Hund läuft über den Sandstrand.

(b) AMB example: runs is correctly translated by base into läuft. Objects: dog

Figure 5: Examples of blanks for test set 2016 that were correctly resolved by the textual context. The underlined
words denote blanked words and their translations.

EN: A woman and a dog sit on a white bench near a beach.

base: Eine Frau und ein Hund sitzen an einem weißen Strand nahe einem
Strand.

del: Eine Frau und ein Hund sitzen auf einem weißen Sofa in der nähe eines
Strands.

del+obj: Eine Frau und ein Hund sitzen auf einer weißen Bank nahe einem
Strand.

DE: Eine Frau und eine Hund sitzen auf einer weißen Bank in der nähe eines
Strandes.

(a) RND example: the blank bench is correctly translated by del+obj into Bank due to the detected object Bench. Objects:
person, dog, bench

EN: Two men dressed in green are preparing food in a restaurant.

base: Deux femmes vêtues de vert préparent des aliments dans un restaurant.

del: Deux femmes vêtues de vert préparent de la nourriture dans un
restaurant.

del+obj: Deux asiatiques en vert préparent de la nourriture dans un restaurant.

FR: Deux hommes habillés en vert préparent de la nourriture dans un
restaurant.

(b) PERS example. men correctly translated into asiatiques (asians) by del+obj. Objects: person, clothing, man, food, cake

Figure 6: Examples of blanks for test set 2016 that were correctly resolved by the multimodal context. The
underlined words denote blanked words and their translations.



6538

EN: A guy give a kiss to a guy also.

base: Ein Mann, der sich vor, um eine Frau zu knüssen .

del: Ein Mann, der sich vor, um eine Frau zu küssen.

del+obj: Ein Mann, der einem kuss küsst, um eine Frau zu küssen.

DE: Ein Typ küsst einen anderen Typ .

(a) PERS example: the second mention of guy is consistently translated into Frau (woman). Objects: clothing, man, face

EN: A group of students sit and listen to the speaker.

base: Eine Gruppe von Studenten sitzt und schaut nach rechts .

del: Eine Gruppe Schüler sitzt und schaut nach rechts.

del+obj: Eine Gruppe Schüler sitzt und schaut zu rechts auf das Wasser.

DE: Eine Gruppe von Studenten sitzt und hört der Sprecherin zu.

(b) AMB example. The blanks listen and speaker are consistently translated into schaut (look) and rechts (right) or Wasser
(water). Objects: person, clothing, man, food, cake

Figure 7: Examples of unresolved blanks. The underlined words denote blanked words and their translations.


