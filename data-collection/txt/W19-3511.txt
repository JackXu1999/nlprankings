



















































A System to Monitor Cyberbullying based on Message Classification and Social Network Analysis


Proceedings of the Third Workshop on Abusive Language Online, pages 105–110
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

105

A System to Monitor Cyberbullying based on
Message Classification and Social Network Analysis

Stefano Menini‡, Giovanni Moretti‡, Michele Corazza†
Elena Cabrio†, Sara Tonelli‡, Serena Villata†

‡Fondazione Bruno Kessler, Trento, Italy
†Université Côte d’Azur, CNRS, Inria, I3S, France
{menini,moretti,satonelli}@fbk.eu

{michele.corazza}@inria.fr
{elena.cabrio,serena.villata}@unice.fr

Abstract

Social media platforms like Twitter and Insta-
gram face a surge in cyberbullying phenom-
ena against young users and need to develop
scalable computational methods to limit the
negative consequences of this kind of abuse.
Despite the number of approaches recently
proposed in the Natural Language Process-
ing (NLP) research area for detecting different
forms of abusive language, the issue of identi-
fying cyberbullying phenomena at scale is still
an unsolved problem. This is because of the
need to couple abusive language detection on
textual message with network analysis, so that
repeated attacks against the same person can
be identified. In this paper, we present a sys-
tem to monitor cyberbullying phenomena by
combining message classification and social
network analysis. We evaluate the classifica-
tion module on a data set built on Instagram
messages, and we describe the cyberbullying
monitoring user interface.

1 Introduction

The presence on social networks like Twitter,
Facebook and Instagram is of main importance for
teenagers, but this may also lead to undesirable
and harmful situations. We refer to these forms
of harassment as cyberbullying, i.e., ‘an aggres-
sive, intentional act carried out by a group or an
individual, using electronic forms of contact, re-
peatedly and over time against a victim who can-
not easily defend him or herself’ (Smith et al.,
2008). In online social media, each episode of on-
line activity aimed at offending, menacing, harass-
ing or stalking another person can be classified as
a cyberbullying phenomenon. This is connected
even with concrete public health issues, since re-
cent studies show that victims are more likely to
suffer from psycho-social difficulties and affective
disorders (Tokunaga, 2010).

Given its societal impact, the implementation of
cyberbullying detection systems, combining abu-
sive language detection and social network anal-
ysis, has attracted a lot of attention in the last
years (Tomkins et al., 2018; Hosseinmardi et al.,
2015a; Ptaszynski et al., 2015; Dinakar et al.,
2012). However, the adoption of such systems in
real life is not straightforward and their use in a
black box scenario is not desirable, given the nega-
tive effects misleading analyses could have on po-
tential abusers and victims. A more transparent
approach should be adopted, in which cyberbul-
lying identification should be mediated by human
judgment.

In this paper, we present a system for the mon-
itoring of cyberbullying phenomena on social me-
dia. The system aims at supporting supervising
persons (e.g., educators) at identifying potential
cases of cyberbullying through an intuitive, easy-
to-use interface. This displays both the outcome
of a hate speech detection system and the network
in which the messages are exchanged. Supervis-
ing persons can therefore monitor the escalation of
hateful online exchanges and decide whether to in-
tervene or not, similar to the workflow introduced
in Michal et al. (2010). We evaluate the NLP clas-
sifier on a set of manually annotated data from In-
stagram, and detail the network extraction algo-
rithm starting from 10 Manchester high schools.
However, this is only one possible use case of
the system, which can be employed over different
kinds of data.

2 Network Extraction

Since cyberbullying is by definition a repeated at-
tack towards a specific victim by one or more bul-
lies, we include in the monitoring system an algo-
rithm to identify local communities in social net-
works and isolate the messages exchanged only



106

within such communities. In this demo, we focus
on high-schools, but the approach can be extended
to other communities of interest. Our case study
concerns the network of Manchester high-school
students, and we choose to focus on Instagram,
since it is widely used by teenagers of that age.

Reconstructing local communities on Instagram
is a challenging task. Indeed, differently from how
other social networks operate (e.g., Facebook), In-
stagram does not provide a page for institutions
such as High Schools, that therefore need to be
inferred. To overcome this issue, and to identify
local communities of students, we proceed in two
steps that can be summarised as follow:

• Expansion stage. We start from few users
that are very likely to be part of the local high
school community, and we use them to iden-
tify an increasing number of other possible
members expanding our network coverage.

• Pruning stage. We identify, within the large
network, smaller communities of users and
we isolate the ones composed by students.
For these, we retrieve the exchanged mes-
sages in a given period of time (in our case,
the ongoing school year), which will be used
to identify abusive messages.

2.1 Expansion Stage
In this stage, we aim to build an inclusive net-
work of people related to local high schools. Since
schools do not have an Instagram account, we de-
cide to exploit the geo-tagging of pictures. We
manually define a list of 10 high schools from
Manchester, and we search for all the photos as-
sociated with one of these locations by matching
the geo-tagged addresses.

Given that anyone can tag a photo with the ad-
dress of a school, this stage involves not only
actual students, but also their teachers, parents,
friends, alumni and so on. The reason to adopt this
inclusive approach is that not every student is di-
rectly associated with his/her school on Instagram
(i.e., by sharing pictures in or of the school), there-
fore we need to exploit also their contacts with
other people directly related to the schools. We re-
strict our analysis to pictures taken from Septem-
ber 2018 on to focus on the current school year and
obtain a network including actual students rather
than alumni.

With this approach, we identify a first layer of
756 users, corresponding to the authors of the pho-

Figure 1: Network obtained starting from 10 Manch-
ester schools and expanding +2 layers

tos tagged in one of the 10 schools. Starting from
these users, we expand our network with a broader
second layer of users related to the first ones. We
assume that users writing messages to each other
are likely to be somehow related, therefore we in-
clude in the network all users exchanging com-
ments with the first layer of users in the most re-
cent posts. In this step, we do not consider the
connections given by likes, since they are prone
to introduce noise in the network. With this step
we obtain a second layer of 17,810 users that we
consider related to the previous ones as they in-
teract with each other in the comments. Using
the same strategy, we further expand the network
with a third layer of users commenting the con-
tents posted by users in the second layer. It is in-
teresting to notice that in the first layer of users, i.e.
the ones directly related to the schools, the groups
of users associated with each school are well sep-
arated. As soon as we increase the size of the net-
work with additional layers, user groups start to
connect to each other through common “friends”.

We stop the expansion at a depth of three lay-
ers since additional layers would exponentially in-
crease the number of users. At the end of the ex-
pansion stage, we gather a list of 544,371 unique
users obtained from an exchange of 1,539,292
messages. The resulting network (Figure 1) is gen-
erated by representing each user as a node, while
the exchanged messages correspond to edges.
Each edge between two users is weighted accord-
ing to the number of messages between the two.



107

2.2 Pruning Stage

After generating a large network of users starting
from the list of schools, the following step consists
in pruning the network from unnecessary nodes by
identifying within the network smaller communi-
ties of high school students and teenagers. These
communities define the scenario in which we want
to monitor the possible presence of cyberbullying.
To identify local communities, we proceed incre-
mentally dividing the network into smaller por-
tions. For this task, we apply the modularity func-
tion in Gephi (Blondel et al., 2008), a hierarchi-
cal decomposition algorithm that iteratively opti-
mizes modularity for small communities, aggre-
gating then nodes of the same community to build
a new network.

Then, we remove the groups of people falling
out of the scope of our investigation by automati-
cally looking for geographical or professional cues
in the user biographies. For example, we remove
nodes that contain the term blogger or photogra-
pher in the bio, and all the nodes that are only con-
nected to them in the network. This step is done
automatically, but we manually check the nodes
that have the highest centrality in the network be-
fore removing them, so as to ensure that we do not
prune nodes of interest for our use case.

We then run again the modularity function to
identify communities among the remaining nodes.
Finally, we apply another pruning step by looking
for other specific cues in the user bios that may
identify our young demographic of interest. In this
case, we define regular expressions to match the
age, year of birth or school attended, reducing the
network to a core of 892 nodes (users) and 2,435
edges, with a total of 14,565 messages (Figure 2).

3 Classification of abusive language

To classify the messages exchanged in the net-
work extracted in the previous step as containing
or not abusive language, we use a modular neu-
ral architecture for binary classification in Keras
(Chollet et al., 2015), which uses a single feed-
forward hidden layer of 100 neurons, with a ReLu
activation and a single output with a sigmoid acti-
vation. The loss used to train the model is binary
cross-entropy. We choose this particular architec-
ture because it proved to be rather effective and ro-
bust: we used it to participate in two shared tasks
for hate speech detection, one for Italian (Corazza
et al., 2018a) and one for German (Corazza et al.,

Figure 2: Manchester network after pruning

2018b), obtaining competitive results w.r.t. state-
of-the-art systems.

The architecture is built upon a recurrent
layer, namely a Long Short-Term Memory
(LSTM) whose goal is to learn an encoding de-
rived from word embeddings, obtained as the out-
put of the recurrent layer at the last timestep. We
use English Fasttext embeddings1 trained on Com-
mon Crawl with a size of 300. Concerning hy-
perparameters, our model uses no dropout and no
batch normalization on the outputs of the hidden
layer. Instead, a dropout on the recurrent units of
the recurrent layers is used (Gal and Ghahramani,
2016) with value 0.2. We select a batch size of 32
for training and a size of 200 for the output (and
hidden states) of the recurrent layers. Such hyper-
parameters and features have been selected from a
system configuration that performed consistently
well on the above mentioned shared tasks for hate
speech detection, both on Facebook and on Twitter
data.

4 Experimental setting and evaluation

Although our use case focuses on Instagram mes-
sages, we could not find available datasets from
this social network with annotated comments. The
widely used dataset used by (Hosseinmardi et al.,
2015b) has indeed annotations at thread level.

We therefore train our classification algorithm
using the dataset described in (Waseem and Hovy,
2016), containing 16k English tweets manually

1https://fasttext.cc/docs/en/
english-vectors.html

https://fasttext.cc/docs/en/english-vectors.html
https://fasttext.cc/docs/en/english-vectors.html


108

Figure 3: Interface view for network exploration Figure 4: Interface view for hate speech monitoring

annotated for hate speech. More precisely, 1,924
are annotated as containing racism, 3,082 as con-
taining sexism, while 10,884 tweets are annotated
as not containing offensive language. We merge
the sexist and racist tweets in a single class, so that
5,006 tweets are considered as positive instances
of hate speech. As a test set, we manually anno-
tate 900 Instagram comments, randomly extracted
from the Manchester network, labeling them as
hate speech or not. Overall, the test set contains
787 non-offensive and 113 offensive messages.

We preprocess both data sets, given that hash-
tags, user mentions, links to external media and
emojis are common in social media interactions.
To normalize the text as much as possible while re-
taining all relevant semantic information, we first
replace URLs with the word “url” and “@” user
mentions with “username” by using regular ex-
pressions. We also use the Ekphrasis tool (Bazi-
otis et al., 2017) to split hashtags into sequences
of words, when possible.

The system obtained on the test set a micro-
averaged F1 of 0.823. We then run the classifier
on all messages extracted for the Manchester net-
work, and make the output available through the
platform interface.

5 Interface

The system2 relies on a relational database and
a tomcat application server. The interface is
based on existing javascript libraries such as C3.js
(https://c3js.org) and Sigma.js (http:

2A video of the demo is available at https:
//dh.fbk.eu/sites/dh.fbk.eu/files/
creepdemo_1.m4v

//sigmajs.org).
The platform can be used with two settings:

in the first one (Figure 3), the Manchester net-
work is displayed, with colors denoting different
sub-communities characterised by dense connec-
tions. By clicking on a node, the platform displays
the cloud of key-concepts automatically extracted
from the conversations between the given user
and her connections using the KD tool (Moretti
et al., 2015). This view is useful to understand
the size and the density of the network and to
browse through the topics present in the threads.
In the second setting (Figure 4), which can be ac-
tivated by clicking on “Show offensive messages”,
the communities are all colored in grey, while the
system highlights in red the messages classified as
offensive by the system described in Section 3.
By clicking on red edges it is possible to view
the content of the messages classified as offensive,
enabling also to check the quality of the classi-
fier. This second view is meant to support educa-
tors and stakeholders in monitoring cyberbullying
without focusing on single users, but rather keep-
ing an eye on the whole network and zooming in
only when hateful exchanges, flagged in red, are
escalating.

6 Discussion

The current system has been designed to support
the work of educators in schools, although it is
not meant to be open to everyone but only to spe-
cific personnel. For example, in Italy there must
be one responsible teacher to counter cyberbully-
ing in every school, and access to the system could
be given only to that specific person. For the same

https://c3js.org
http://sigmajs.org
http://sigmajs.org
https://dh.fbk.eu/sites/dh.fbk.eu/files/creepdemo_1.m4v
http://sigmajs.org
https://dh.fbk.eu/sites/dh.fbk.eu/files/creepdemo_1.m4v
http://sigmajs.org
https://dh.fbk.eu/sites/dh.fbk.eu/files/creepdemo_1.m4v
http://sigmajs.org


109

reason, the system does not show the actual user-
names but only placeholders, and the possibility
to de-anonymise the network of users could be ac-
tivated only after cyberbullying phenomena have
been identified, and only for the users involved in
such cases. Indeed, we want to avoid the use of
this kind of platforms for the continuous surveil-
lance of students, and prevent a malicious use of
the monitoring platform.

The system relies on public user profiles, and
does not have access to content that users want
to keep private. This limits the number of cyber-
bullying cases and hate messages in our use case,
where detected abusive language concerns less
than 1% of the messages, while a previous study
on students’ simulated WhatsApp chats around
controversial topics reports that 41% of the col-
lected tokens were offensive or abusive (Sprugnoli
et al., 2018). This limitation is particularly rele-
vant when dealing with Instagram, but the work-
flow presented in this paper can be potentially ap-
plied to other social networks and chat applica-
tions. Another limitation of working with Insta-
gram is the fact that the monitoring cannot hap-
pen in real time. In fact, the steps to extract and
prune the network require some processing time
and cannot be performed on the fly, especially
in case of large user networks. We estimate that
the time needed to download the data, extract the
network, retrieve and classify the messages and
upload them in the visualisation tool would be
around one week.

7 Conclusion

In this paper, we presented a platform to monitor
cyberbullying phenomena that relies on two com-
ponents: an algorithm to automatically detect on-
line communities starting from geo-referenced on-
line pictures, and a hate speech classifier. Both
components have been combined in a single plat-
form that, through two different views, allows ed-
ucators to visualise the network of interest and
to detect in which sub-communities hate speech
is escalating. Although the evaluation has been
carried out only on English, the system supports
also Italian, and will be showcased in both lan-
guages. In the future, we plan to improve the clas-
sifier performance by extending the Twitter train-
ing set with more annotated data from Instagram.
We will also experiment with cross-lingual strate-
gies to train the classifier on English datasets and

use it on other languages.

Acknowledgments

Part of this work was funded by the CREEP
project (http://creep-project.eu/), a
Digital Wellbeing Activity supported by EIT Dig-
ital in 2018 and 2019. This research was also sup-
ported by the HATEMETER project (http://
hatemeter.eu/) within the EU Rights, Equal-
ity and Citizenship Programme 2014-2020.

References
Christos Baziotis, Nikos Pelekis, and Christos Doulk-

eridis. 2017. DataStories at SemEval-2017 Task 4:
Deep LSTM with Attention for Message-level and
Topic-based Sentiment Analysis. In Proceedings of
the 11th International Workshop on Semantic Eval-
uation (SemEval-2017), pages 747–754, Vancouver,
Canada. Association for Computational Linguistics.

Vincent D Blondel, Jean-Loup Guillaume, Renaud
Lambiotte, and Etienne Lefebvre. 2008. Fast un-
folding of communities in large networks. Jour-
nal of statistical mechanics: theory and experiment,
2008(10):P10008.

François Chollet et al. 2015. Keras. https://
github.com/fchollet/keras.

Michele Corazza, Stefano Menini, Pinar Arslan,
Rachele Sprugnoli, Elena Cabrio, Sara Tonelli, and
Serena Villata. 2018a. Comparing Different Super-
vised Approaches to Hate Speech Detection. In
Proceedings of the Sixth Evaluation Campaign of
Natural Language Processing and Speech Tools for
Italian. Final Workshop (EVALITA 2018) co-located
with the Fifth Italian Conference on Computational
Linguistics (CLiC-it 2018), Turin, Italy, December
12-13, 2018., volume 2263 of CEUR Workshop Pro-
ceedings.

Michele Corazza, Stefano Menini, Pinar Arslan,
Rachele Sprugnoli, Elena Cabrio, Sara Tonelli, and
Serena Villata. 2018b. Inriafbk at germeval 2018:
Identifying offensive tweets using recurrent neural
networks. In GermEval 2018 Workshop.

Karthik Dinakar, Birago Jones, Catherine Havasi,
Henry Lieberman, and Rosalind W. Picard. 2012.
Common Sense Reasoning for Detection, Pre-
vention, and Mitigation of Cyberbullying. TiiS,
2(3):18:1–18:30.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in Neural Informa-
tion Processing Systems 29: Annual Conference on
Neural Information Processing Systems 2016, De-
cember 5-10, 2016, Barcelona, Spain, pages 1019–
1027.

http://creep-project.eu/
http://hatemeter.eu/
http://hatemeter.eu/
https://github.com/fchollet/keras
https://github.com/fchollet/keras
http://ceur-ws.org/Vol-2263/paper039.pdf
http://ceur-ws.org/Vol-2263/paper039.pdf
https://doi.org/10.1145/2362394.2362400
https://doi.org/10.1145/2362394.2362400
http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks
http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks
http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks


110

Homa Hosseinmardi, Sabrina Arredondo Mattson, Ra-
hat Ibn Rafiq, Richard Han, Qin Lv, and Shivakant
Mishra. 2015a. Analyzing labeled cyberbullying in-
cidents on the instagram social network. In Social
Informatics - 7th International Conference, SocInfo
2015, Beijing, China, December 9-12, 2015, Pro-
ceedings, pages 49–66.

Homa Hosseinmardi, Sabrina Arredondo Mattson, Ra-
hat Ibn Rafiq, Richard O. Han, Qin Lv, and Shiv-
akant Mishra. 2015b. Prediction of Cyberbullying
Incidents on the Instagram Social Network. CoRR,
abs/1503.03909.

Ptaszynski Michal, Dybala Pawel, Matsuba Tatsuaki,
Masui Fumito, Rzepka Rafal, Araki Kenji, and Mo-
mouchi Yoshio. 2010. In the service of online order:
Tackling cyber-bullying with machine learning and
affect analysis. International Journal of Computa-
tional Linguistics Research, 1(3):135–154.

Giovanni Moretti, Rachele Sprugnoli, and Sara Tonelli.
2015. Digging in the Dirt: Extracting Keyphrases
from Texts with KD. In Proceedings of the Second
Italian Conference on Computational Linguistics.

Michal Ptaszynski, Fumito Masui, Yasutomo Kimura,
Rafal Rzepka, and Kenji Araki. 2015. Automatic
extraction of harmful sentence patterns with appli-
cation in cyberbullying detection. In Human Lan-
guage Technology. Challenges for Computer Sci-
ence and Linguistics - 7th Language and Technology
Conference, LTC 2015, Poznań, Poland, November
27-29, 2015, Revised Selected Papers, pages 349–
362.

Peter K. Smith, Jess Mahdavi, Manuel Carvalho, Sonja
Fisher, Shanette Russell, and Neil Tippett. 2008.
Cyberbullying: its nature and impact in secondary
school pupils. Journal of Child Psychology and Psy-
chiatry, 49(4):376–385.

Rachele Sprugnoli, Stefano Menini, Sara Tonelli, Fil-
ippo Oncini, and Enrico Piras. 2018. Creating a
whatsapp dataset to study pre-teen cyberbullying. In
Proceedings of the 2nd Workshop on Abusive Lan-
guage Online (ALW2), pages 51–59. Association for
Computational Linguistics.

Robert S. Tokunaga. 2010. Following you home from
school: A critical review and synthesis of research
on cyberbullying victimization. Computers in Hu-
man Behavior, 26(3):277 – 287.

Sabina Tomkins, Lise Getoor, Yunfei Chen, and
Yi Zhang. 2018. A socio-linguistic model for cy-
berbullying detection. In IEEE/ACM 2018 Interna-
tional Conference on Advances in Social Networks
Analysis and Mining, ASONAM 2018, Barcelona,
Spain, August 28-31, 2018, pages 53–60.

Zeerak Waseem and Dirk Hovy. 2016. Hateful Sym-
bols or Hateful People? Predictive Features for
Hate Speech Detection on Twitter. In SRW@HLT-
NAACL.

https://doi.org/10.1007/978-3-319-27433-1_4
https://doi.org/10.1007/978-3-319-27433-1_4
https://doi.org/10.1007/978-3-319-93782-3_25
https://doi.org/10.1007/978-3-319-93782-3_25
https://doi.org/10.1007/978-3-319-93782-3_25
https://doi.org/10.1111/j.1469-7610.2007.01846.x
https://doi.org/10.1111/j.1469-7610.2007.01846.x
http://aclweb.org/anthology/W18-5107
http://aclweb.org/anthology/W18-5107
https://doi.org/https://doi.org/10.1016/j.chb.2009.11.014
https://doi.org/https://doi.org/10.1016/j.chb.2009.11.014
https://doi.org/https://doi.org/10.1016/j.chb.2009.11.014
https://doi.org/10.1109/ASONAM.2018.8508294
https://doi.org/10.1109/ASONAM.2018.8508294

