



















































A Working Memory Model for Task-oriented Dialog Response Generation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2687–2693
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2687

A Working Memory Model for Task-oriented Dialog Response Generation

Xiuyi Chen1,2,3, Jiaming Xu1,2∗and Bo Xu1,2,3,4
1Institute of Automation, Chinese Academy of Sciences (CASIA). Beijing, China

2Research Center for Brain-inspired Intelligence, CASIA
3University of Chinese Academy of Sciences

4Center for Excellence in Brain Science and Intelligence Technology, CAS. China
{chenxiuyi2017,jiaming.xu,xubo}@ia.ac.cn

Abstract

Recently, to incorporate external Knowledge
Base (KB) information, one form of world
knowledge, several end-to-end task-oriented
dialog systems have been proposed. These
models, however, tend to confound the dialog
history with KB tuples and simply store them
into one memory. Inspired by the psycholog-
ical studies on working memory, we propose
a working memory model (WMM2Seq) for
dialog response generation. Our WMM2Seq
adopts a working memory to interact with two
separated long-term memories, which are the
episodic memory for memorizing dialog his-
tory and the semantic memory for storing KB
tuples. The working memory consists of a cen-
tral executive to attend to the aforementioned
memories, and a short-term storage system to
store the “activated” contents from the long-
term memories. Furthermore, we introduce a
context-sensitive perceptual process for the to-
ken representations of the dialog history, and
then feed them into the episodic memory. Ex-
tensive experiments on two task-oriented dia-
log datasets demonstrate that our WMM2Seq
significantly outperforms the state-of-the-art
results in several evaluation metrics.

1 Introduction

Task-oriented dialog systems, such as hotel book-
ing or technical support service, help users to
achieve specific goals with natural language. Com-
pared with traditional pipeline solutions (Williams
and Young, 2007; Young et al., 2013; Wen et al.,
2017), end-to-end approaches recently gain much
attention (Zhao et al., 2017; Eric and Manning,
2017a; Lei et al., 2018), because they directly map
dialog history to the output responses and conse-
quently reduce human effort for modular designs
and hand-crafted state labels. To effectively incor-
porate KB information and perform knowledge-

∗ Corresponding Author

based reasoning, memory augmented models have
been proposed (Bordes et al., 2017; Seo et al., 2017;
Eric and Manning, 2017b; Madotto et al., 2018;
Raghu et al., 2018; Reddy et al., 2019; Wu et al.,
2019). Bordes et al. (2017) and Seo et al. (2017)
attended to retrieval models, lacking the ability of
generation, while others incorporated the memory
(i.e. end-to-end memory networks, abbreviated
as MemNNs, Sukhbaatar et al. (2015)) and copy
mechanism (Gu et al., 2016) into a sequential gen-
erative architecture. However, most models tended
to confound the dialog history with KB tuples and
simply stored them into one memory. A shared
memory forces the memory reader to reason over
the two different types of data, which makes the
task harder, especially when the memory is large.
To explore this problem, Reddy et al. (2019) very
recently proposed to separate memories for model-
ing dialog context and KB results. In this paper, we
adopt working memory to interact with two long-
term memories. Furthermore, compared to Reddy
et al. (2019), we leverage the reasoning ability of
MemNNs to instantiate the external memories.

Our intuition comes from two aspects. First,
psychologists tend to break down the long-term
memory1 into episodic memory for events (e.g. vi-
sual and textual perceptual inputs) and semantic
memory for facts (world knowledge, such as KB
information) as not all memory of experiences is
the same (Gazzaniga and Ivry, 2013). Second, a
successful task-oriented dialog system needs more
intelligence, and recent works suggest that a crit-
ical component of intelligence may be working
memory (Sternberg and Sternberg, 2016). Hence,
leveraging the knowledge from psychological stud-
ies (Baddeley and Hitch, 1974; Baddeley, 2000;
Dosher, 2003), we explore working memory for
the dialog response generation. Our contributions

1Here, the long-term memory is referred to declarative
memory that we have conscious access to.



2688

are summarized as follows:
Firstly, inspired by the psychological studies on

working memory, we propose the WMM2Seq for
dialog generation which separates the storage of
dialog history and KB information by using the
episodic and semantic memories and then leverages
the working memory to interact with them.

Secondly, we leverage two kinds of transforma-
tions (CNN and biGRU) to incorporate the context
information for better token representations. This
procedure can be seen as a part of perceptual pro-
cesses before the episodic memory storage, and can
alleviate the Out-Of-Vocabulary (OOV) problem.

Finally, our WMM2Seq outperforms the exist-
ing methods on several evaluation metrics in two
task-oriented dialog datasets and shows a better
reasoning ability in the OOV situation.

2 Model Description

Figure 1 illustrates the flow of our WMM2Seq for
dialog response generation. WMM2Seq can be
seen as an encoder-decoder model, where decoder
is the Working Memory (WM) which could in-
teract with two long-term memories (the episodic
memory memorizing dialog history and semantic
memory storing KB information). As MemNN is
well-known for its multiple hop reasoning ability,
we instantiate the encoder and the two memories
with three different MemNNs (MemNN Encoder,
E-MemNN and S-MemNN). Furthermore, we aug-
ment E-MemNN and S-MemNN with copy mecha-
nism from where we need to copy tokens or entities.
The encoder encodes the dialog history to obtain
the high-level signal, a distributed intent vector.
The WM consists of a Short-Term Storage system
(STS) and a Central-EXE including an Attention
Controller (Attn-Ctrl) and a rule-based word selec-
tion strategy. The Attn-Ctrl dynamically generates
the attention control vector to query and reason
over the two long memories and then stores three
“activated” distributions into STS. Finally a gener-
ated token is selected from the STS under the word
selection strategy at each decoder step.

The symbols are defined in Table 1, and more
details can be found in the supplementary material.
We omit the subscript E or S2, following Madotto
et al. (2018) to define each pointer index set:

ptri =

{
max(z) if ∃z s.t. yi = xbz
nxb + 1 otherwise

, (1)

2Note, all variables belonging to the episodic memory are
with subscript E, and semantic memory are with subscript S.

Symbol Definition
xi or yi a token in the dialog history or system response

$ a special token used as a sentinel (Madotto et al., 2018)
X X = {x1, . . . , xn, $}, the dialog history
Y Y = {y1, · · · , ym}, the expected response
bi one KB tuple, actually the corresponding entity
B B = {b1, · · · , bl, $}, the KB tuples

PTRE = {ptrE,1, · · · , ptrE,m}, dialog pointer index set.
PTRE supervised information for copying words in dialog history

PTRS = {ptrS,1, · · · , ptrS,m}, KB pointer index set.
PTRS supervised information for copying entities in KB tuples

Table 1: Notation Table.

where xbz ∈ X or B is the dialog history or KB tu-
ples according to the subscript (E or S) and nxb+1
is the sentinel position index as nxb is equal to the
dialog history length n or the number of KB triples
l. The idea behind Eq. 1 is that we can obtain the
positions of where to copy by matching the target
text with the dialog history or KB information. Fur-
thermore, we hope this provides the model with
an accurate guidance of how to activate the two
long-term memories.

2.1 MemNN Encoder
Here, on the context of our task, we give a brief de-
scription of K-hop MemNN with adjacent weight
tying and more details can be found in (Sukhbaatar
et al., 2015). The memory of MemNN is repre-
sented by a set of trainable embedding matrices
C = {C1, . . . , CK+1}. Given input tokens in the
dialog history X , MemNN first writes them into
memories by Eq. 2 and then uses a query to iter-
atively read from them with multi hops to reason
about the required response by Eq. 3 and Eq. 4. For
each hop k, we update the query by Eq. 5 and the
initial query is a learnable vector as like Yang et al.
(2016). The MemNN encoder finally outputs a user
intent vector oK .

Aki = C
k(xi) (2)

pki = Softmax((q
k)TAki ) (3)

ok =
∑
i

pkiA
k+1
i (4)

qk+1 = qk + ok (5)

To incorporate the context information, we ex-
plore two context-aware transformation TRANS(·)
by replacing Eq. 2 with Aki = TRANS(C

k(xi)),
which is defined as follows:

hi = TRANS(φ
e(xi))

= CNN([φe(xi−2), . . . , φ
e(xi+2)])

, (6)



2689

Resto#1 phone resto#1_phone
Resto#1 R_cuisine french
Resto#1 R_address resto#1_address
Resto#1 R_location paris
Resto#1 R_number six
Resto#1 R_price moderate
Resto#1 R_rating 6
… …   
Resto#N R_rating 3

Ko

q
Encoder

KB Tuples

STS

Working Memory

ˆ
ty

U: hi
S: hello what can I help you with today
U: may I have a table in paris 
S: i’m on it
S: any preference on a type of cuisine
U: i love  indian food
… … 

S: api_call italian paris six moderate
U: instead could it be with french food  
… … 

S: ok let me look into some options for you
U: <SILENCE>        
S: api_call french paris six moderate
U: <SILENCE>

Dialog History

S-MemNN

tq
GRU

1
ˆ
ty 

1tq 
Attn-Ctrl

tq

S ptrP 

Central-EXE

E-MemNN

High-level
Signal

Gate

Ko

Predicted Response……1ŷ 1ˆty  ˆ ty

……what do you thinkEncoder Decoder

tq

,E tq

vocabP

E ptrP 

Figure 1: The Working Memory (WM) interacts with two long-term memories to generate the response.

or

hi = TRANS(φ
e(xi))

=

[
⇀

hi
↼

hi

]
=

[ −−−→
GRU(φe(xi),

⇀

hi−1)
←−−−
GRU(φe(xi),

↼

hi+1)

]
, (7)

where hi is the context-aware representation, and
φe is a trainable embedding function. We combine
MemNNs with TRANS(·) to alleviate the OOV
problem when reasoning about memory contents.

2.2 Working Memory Decoder
Inspired by the studies on the working memory, we
design our decoder as an attentional control system
for dialog generation which consists of the working
memory and two long-term memories. As shown in
Figure 1, we adopt the E-MemNN to memorize the
dialog history X as described in Section 2.1, and
then store KB tuples into the S-MemNN without
TRANS(·). We also incorporate additional tempo-
ral information and speaker information into dialog
utterances as (Madotto et al., 2018) and adopt a
(subject, relation, object) representation of KB in-
formation as (Eric and Manning, 2017b). More
details can be found in the supplementary material.

Having written dialog history and KB tuples into
E-MemNN and S-MemNN, we then use the WM to
interact with them (to query and reason over them)
to generate the response. At each decoder step,
the Attn-Ctrl, instantiated as a GRU, dynamically
generates the query vector qt as follows:

qt = GRU(C
1
E(ŷt−1), qt−1). (8)

Here, query qt is used to access E-MemNN activat-
ing the final query qE = oKE , vocabulary distribu-
tion Pvocab by Eq. 9 and copy distribution for dialog
history PE·ptr. When querying S-MemNN, we con-
sider the dialog history by using query q′t = qE+qt
and then obtain the copy distribution for KB enti-
ties PS·ptr. The two copy distributions are obtained
by augmenting MemNNs with copy mechanism
that is PE·ptr = pKE,t and PS·ptr = p

K
S,t.

Pvocab(ŷt) = Softmax(W1[qt; o
1
E]). (9)

Now, three distributions, Pvocab, PE·ptr and
PS·ptr, are activated and moved into the STS, and
then a proper word is generated from the acti-
vated distributions. We here use a rule-based word
selection strategy by extending the sentinel idea
in (Madotto et al., 2018), which is shown in Fig-
ure 1. If the expected word is not appearing either
in the episodic memory or the semantic memory,
the two copy pointers are trained to produce the
sentinel token and our WMM2Seq generates the
token from Pvocab; otherwise, the token is gener-
ated by copying from either the dialog history or
KB tuples and this is done by comparing the two
copy distributions. We always select the other dis-
tribution if one of the two distributions points to the
sentinel or select to copy the token corresponding
to the biggest probability of the two distributions.
Hence, during the training stage, all the parameters
are jointly learned by minimizing the sum of three
standard cross-entropy losses with the correspond-
ing targets (Y , PTRE and PTRS).



2690

Task Ptr-Unk Mem2Seq HyP-MN GLMP WMM2Seq+CNN WMM2Seq+biGRU WMM2Seq WMM2Seq+biGRU (H1)
T1 100 (100) 100 (100) 100 (100) 100 (100) 100 (100) 100 (100) 100 (100) 100 (100)
T2 100 (100) 100 (100) 99.9 (99.8) 100 (100) 100 (100) 100 (100) 100 (100) 100 (100)
T3 85.1 (19.0) 94.7 (62.1) 94.9 (63.2) 96.3 (75.6) 95.03 (63.6) 95.32 (68.2) 94.94 (63.9) 95.01 (64.6)
T4 100 (100) 100 (100) 100 (100) 100 (100) 100 (100) 100 (100) 100 (100) 100 (100)
T5 99.4 (91.5) 97.9 (69.6) 97.7 (67) 99.2 (88.5) 98.49 (76.6) 99.34 (90.3) 97.95 (71.2) 99.26 (88.8)

T1-OOV 92.5 (54.7) 94.0 (62.2) 100 (100) 100 (100) 100 (100) 100 (100) 91.28 (57.2) 100 (100)
T2-OOV 83.2 (0) 86.5 (12.4) 100 (100) 100 (100) 100 (100) 100 (100) 83.28 (0) 100 (100)
T3-OOV 82.9 (13.4) 90.3 (38.7) 95.6 (63.9) 95.5 (65.7) 94.87 (66.2) 94.64 (61.6) 94.54 (60.5) 94.80 (62.2)
T4-OOV 100 (100) 100 (100) 100 (100) 100 (100) 100 (100) 100 (100) 100 (100) 100 (100)
T5-OOV 73.6 (0) 84.5 (2.3) 89.3 (9.7) 92.0 (21.7) 92.32 (24.3) 92.56 (24.3) 84.45 (3.6) 91.86 (22.0)

Table 2: Per-response and per-dialog (in the parentheses) accuracy on bAbI dialogs.

3 Experiments

We conduct experiments on the simulated bAbI Di-
alogue dataset (Bordes et al., 2017) and the Dialog
State Tracking Challenge 2 (DSTC2) (Henderson
et al., 2014). We actually adopt the refined ver-
sion of DSTC2 from Bordes et al. (2017) and their
statistics are given in the supplementary material.

Our model is trained end-to-end using Adam op-
timizer (Kingma and Ba, 2014), and the responses
are generated using greedy search without any re-
scoring techniques. The shared size of embedding
and hidden units is selected from [64, 512] and
the default hop K = 3 is used for all MemNNs.
The learning rate is simply fixed to 0.001 and the
dropout ratio is sampled from [0.1, 0.4]. Further-
more, we randomly mask some memory cells with
the same dropout ratio to simulate the OOV situa-
tion for both episodic and semantic memories. The
hyper-parameters for best models are given in the
supplementary material.

3.1 Results and Analysis

We use Per-response/dialog Accuracy (Bordes
et al., 2017), BLEU (Papineni et al., 2002) and
Entity F1 (Madotto et al., 2018) to compare the
performance of different models. And the baseline
models are Seq2Seq+Attn (Luong et al., 2015),
Pointer to Unknown (Ptr-Unk, Gulcehre et al.
(2016)), Mem2Seq (Madotto et al., 2018), Hier-
archical Pointer Generator Memory Network (HyP-
MN, Raghu et al. (2018)) and Global-to-Local
Memory Pointer (GLMP, Wu et al. (2019)).
Automatic Evaluation: The results on the bAbI
dialog dataset are given in Table 2. We can see
that our model does much better on the OOV sit-
uation and is on par with the best results on T5.
Moreover, our model can perfectly issue API calls
(task 1), update API calls (task 2) and provide extra
information (task 4). As task 5 is a combination
of tasks 1-4, our best performance on T5-OOV ex-
hibits the powerful reasoning ability to the unseen

Ent. F1 BLEU Per-Resp.(Dial.)
Seq2Seq 69.7 55.0 46.4 (1.5)

Seq2Seq+Attn 67.1 56.6 46.0 (1.4)
Seq2Seq+Copy 71.6 55.4 47.3 (1.3)

Mem2Seq 75.3 55.3 45.0 (0.5)
HyP-MN 73.9 55.4 46.4 (1.7)

WMM2Seq+CNN 80.73 57.33 48.80 (1.61)
WMM2Seq+biGRU 80.23 58.39 49.02 (1.25)

WMM2Seq 75.45 56.81 45.25 (1.25)
WMM2Seq+biGRU (H1) 78.87 58.57 48.81 (1.61)

Table 3: Automatic Evaluation on DSTC2.

dialog history and KB tuples. And this reasoning
ability is also proved by the performance improve-
ments on the DSTC2 dataset according to several
metrics in Table 3. Especially, a significant im-
provement on entity F1 scores indicates that our
model can choose the right entities and incorporate
them into responses more naturally (with highest
BLEU scores). Furthermore, there is no significant
difference between the two kinds of the transfor-
mation TRANS(·).
Ablation Study: To better understand the compo-
nents used in our model, we report our ablation
studies from three aspects. First, we remove the
context-sensitive transformation TRANS(·) and
then find significant performance degradation. This
suggests that perceptual processes are a necessary
step before storing perceptual information (the di-
alog history) into the episodic memory and it is
important for the performance of working mem-
ory. Second, we find that WMM2Seq outper-
forms Mem2Seq, which uses a unified memory
to store dialog history and KB information. We
can safely conclude that the separation of context
memory and KB memory benefits the performance,
as WMM2Seq performs well with less parameters
than Mem2Seq on task 5. Finally, we additionally
analysis how the multi-hop attention mechanism
helps by showing the performance differences be-
tween the hop K = 1 and the default hop K = 3.
Though multi-hop attention strengthens the rea-
soning ability and improves the results, we find
that the performance difference between the hops
K = 1 and K = 3 is not so obvious as shown in



2691

Mem2Seq WMM2Seq Gold
Appropriate 4.31 4.47 4.61
Humanlike 4.37 4.48 4.80

Table 4: Human Evaluation.

(Madotto et al., 2018; Wu et al., 2019). Further-
more, our model performs well even with one hop,
which we mainly attribute to the reasoning ability
of working memory. The separation of memories
and stacking S-MemNN on E-MemNN also help a
lot, because the whole external memory, consisting
of the episodic and semantic memories, can be seen
as a multi-hop (two-level) structure (the first level
is the episode memory and the second level is the
semantic memory).
Attention Visualization: As an intuitive way to
show the model’s dynamics, attention weight visu-
alization is also used to understand how the Central-
EXE controls the access to the two long-term mem-
ories (E-MemNN and S-MemNN). Figure 2 shows
the episodic and semantic memory attention vec-
tors at the last hop for each generated token. Firstly,
our model generates a different but still correct re-
sponse as the customer wants a moderately priced
restaurant in the west and does not care about the
type of food. Secondly, the generated response
has tokens from the vocabulary (e.g. “is” and “a”),
dialog history (e.g. “west” and “food”) and KB
information (e.g. “saint johns chop house” and
“british”), indicating that our model learns to inter-
act well with the two long-term memories by two
sentinels.
Human Evaluation: Following the methods in
(Eric and Manning, 2017b; Wu et al., 2019), we
report human evaluation of the generated responses
in Table 4. We adopt Mem2Seq as the baseline
for human evaluation considering its good perfor-
mance and code release 3. First we randomly select
100 samples from the DSTC2 test set, then generate
the corresponding responses using WMM2Seq and
Mem2Seq, and finally ask two human subjects to
judge the quality of the generated responses accord-
ing to the appropriateness and humanlikeness on a
scale from 1 to 5. As shown in Table 4, WMM2Seq
outperforms Mem2Seq in both measures, which is
coherent to the automatic evaluation. More details
about human evaluation are reported in the supple-
mentary material.

3We thank the authors for releasing their code at
https://github.com/HLTCHKUST/Mem2Seq.

Figure 2: Last hop semantic and episodic memory at-
tention visualization from the DSTC2 dataset.

4 Conclusion

We leverage the knowledge from the psychologi-
cal studies and propose our WMM2Seq for dialog
response generation. First, the storage separation
of the dialog history and KB information is very
important and we explore two context-sensitive per-
ceptual processes for the word-level representations
of the dialog history. Second, working memory is
adopted to interact with the long-term memories
and then generate the responses. Finally, the im-
proved performance on two task-oriented datasets
demonstrates the contributions from the separated
storage and the reasoning ability of working mem-
ory. Our future work will focus on how to transfer
the long-term memory across different tasks.

Acknowledgments

We would like to thank the anonymous review-
ers for their insightful comments. This work was
supported by the National Natural Science Foun-
dation of China (61602479), the Strategic Prior-
ity Research Program of the Chinese Academy of
Sciences (XDB32070000) and the Beijing Brain
Science Project (Z181100001518006).



2692

References
Alan Baddeley. 2000. The episodic buffer: a new com-

ponent of working memory? Trends in Cognitive
Sciences, 4(11):417 – 423.

Alan D Baddeley and Graham Hitch. 1974. Working
memory. In Psychology of learning and motivation,
volume 8, pages 47–89. Elsevier.

Antoine Bordes, Y-Lan Boureau, and Jason Weston.
2017. Learning end-to-end goal-oriented dialog. In
International Conference on Learning Representa-
tions.

Dosher. 2003. Working memory. In L. Nadel (Ed.),
Encyclopedia of cognitive science, volume 4, pages
569–577. London: Nature Publishing Group.

Mihail Eric and Christopher Manning. 2017a. A copy-
augmented sequence-to-sequence architecture gives
good performance on task-oriented dialogue. In Pro-
ceedings of the 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics: Volume 2, Short Papers, volume 2, pages
468–473.

Mihail Eric and Christopher D Manning. 2017b. Key-
value retrieval networks for task-oriented dialogue.
Proceedings of the 18th Annual SIGdial Meeting on
Discourse and Dialogue, pages 37–49.

Michael Gazzaniga and Richard B Ivry. 2013. Cogni-
tive Neuroscience: The Biology of the Mind: Fourth
International Student Edition. WW Norton.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1631–1640, Berlin, Germany. Association for
Computational Linguistics.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,
Bowen Zhou, and Yoshua Bengio. 2016. Pointing
the unknown words. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 140–149.

Matthew Henderson, Blaise Thomson, and Jason D
Williams. 2014. The second dialog state tracking
challenge. In Proceedings of the 15th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue (SIGDIAL), pages 263–272.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Wenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun
Ren, Xiangnan He, and Dawei Yin. 2018. Sequicity:
Simplifying task-oriented dialogue systems with sin-
gle sequence-to-sequence architectures. In Proceed-
ings of the 56th Annual Meeting of the Association

for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1437–1447. Association for Computa-
tional Linguistics.

Thang Luong, Hieu Pham, and Christopher D Manning.
2015. Effective approaches to attention-based neu-
ral machine translation. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1412–1421.

Andrea Madotto, Chien-Sheng Wu, and Pascale Fung.
2018. Mem2seq: Effectively incorporating knowl-
edge bases into end-to-end task-oriented dialog sys-
tems. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1468–1478. Associa-
tion for Computational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Dinesh Raghu, Nikhil Gupta, and Mausam. 2018. Hi-
erarchical pointer memory network for task oriented
dialogue. arXiv preprint arXiv:1805.01216.

Revanth Reddy, Danish Contractor, Dinesh Raghu, and
Sachindra Joshi. 2019. Multi-level memory for task
oriented dialogs. Proceedings of the 2019 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Pa-
pers).

Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh
Hajishirzi. 2017. Query-reduction networks for
question answering. In International Conference on
Learning Representations.

Robert J Sternberg and Karin Sternberg. 2016. Cogni-
tive psychology. Nelson Education.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Advances
in neural information processing systems, pages
2440–2448.

Tsung-Hsien Wen, David Vandyke, Nikola Mrkšić,
Milica Gasic, Lina M Rojas Barahona, Pei-Hao Su,
Stefan Ultes, and Steve Young. 2017. A network-
based end-to-end trainable task-oriented dialogue
system. In Proceedings of the 15th Conference of
the European Chapter of the Association for Com-
putational Linguistics: Volume 1, Long Papers, vol-
ume 1, pages 438–449.

Jason D Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech & Language,
21(2):393–422.

https://doi.org/https://doi.org/10.1016/S1364-6613(00)01538-2
https://doi.org/https://doi.org/10.1016/S1364-6613(00)01538-2
https://openreview.net/pdf?id=S1Bb3D5gg
http://aclweb.org/anthology/E17-2075
http://aclweb.org/anthology/E17-2075
http://aclweb.org/anthology/E17-2075
http://aclweb.org/anthology/W17-5506
http://aclweb.org/anthology/W17-5506
http://www.aclweb.org/anthology/P16-1154
http://www.aclweb.org/anthology/P16-1154
http://aclweb.org/anthology/P16-1014
http://aclweb.org/anthology/P16-1014
http://www.aclweb.org/anthology/W14-4337
http://www.aclweb.org/anthology/W14-4337
https://arxiv.org/pdf/1412.6980.pdf
https://arxiv.org/pdf/1412.6980.pdf
http://aclweb.org/anthology/P18-1133
http://aclweb.org/anthology/P18-1133
http://aclweb.org/anthology/P18-1133
https://doi.org/10.18653/v1/D15-1166
https://doi.org/10.18653/v1/D15-1166
http://aclweb.org/anthology/P18-1136
http://aclweb.org/anthology/P18-1136
http://aclweb.org/anthology/P18-1136
http://aclweb.org/anthology/P02-1040
http://aclweb.org/anthology/P02-1040
http://arxiv.org/abs/1805.01216
http://arxiv.org/abs/1805.01216
http://arxiv.org/abs/1805.01216
https://www.aclweb.org/anthology/N19-1375
https://www.aclweb.org/anthology/N19-1375
https://arxiv.org/pdf/1606.04582.pdf
https://arxiv.org/pdf/1606.04582.pdf
https://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf
http://aclweb.org/anthology/E17-1042
http://aclweb.org/anthology/E17-1042
http://aclweb.org/anthology/E17-1042


2693

Chien-Sheng Wu, Richard Socher, and Caiming Xiong.
2019. Global-to-local memory pointer networks for
task-oriented dialogue. International Conference on
Learning Representations.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchical
attention networks for document classification. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.

Steve Young, Milica Gašić, Blaise Thomson, and Ja-
son D Williams. 2013. Pomdp-based statistical spo-
ken dialog systems: A review. Proceedings of the
IEEE, 101(5):1160–1179.

Tiancheng Zhao, Allen Lu, Kyusong Lee, and Max-
ine Eskenazi. 2017. Generative encoder-decoder
models for task-oriented spoken dialog systems with
chatting capability. In Proceedings of the 18th An-
nual SIGdial Meeting on Discourse and Dialogue.

http://arxiv.org/abs/1901.04713
http://arxiv.org/abs/1901.04713
https://doi.org/10.18653/v1/N16-1174
https://doi.org/10.18653/v1/N16-1174
http://mi.eng.cam.ac.uk/~sjy/papers/ygtw13.pdf
http://mi.eng.cam.ac.uk/~sjy/papers/ygtw13.pdf
http://aclweb.org/anthology/W17-5505
http://aclweb.org/anthology/W17-5505
http://aclweb.org/anthology/W17-5505

