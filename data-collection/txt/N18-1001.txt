



















































Label-Aware Double Transfer Learning for Cross-Specialty Medical Named Entity Recognition


Proceedings of NAACL-HLT 2018, pages 1–15
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Label-aware Double Transfer Learning for Cross-Specialty
Medical Named Entity Recognition

Zhenghui Wang†, Yanru Qu†, Liheng Chen†, Jian Shen†, Weinan Zhang†∗
Shaodian Zhang†‡, Yimei Gao‡, Gen Gu‡, Ken Chen‡, Yong Yu†

†APEX Data and Knowledge Management Lab, Shanghai Jiao Tong University
‡Synyi LLC.

{felixwzh,wnzhang,shaodian}@apex.sjtu.edu.cn
chen.ken@synyi.com

Abstract

We study the problem of named entity recog-
nition (NER) from electronic medical records,
which is one of the most fundamental and
critical problems for medical text mining.
Medical records which are written by clini-
cians from different specialties usually con-
tain quite different terminologies and writ-
ing styles. The difference of specialties and
the cost of human annotation makes it par-
ticularly difficult to train a universal medical
NER system. In this paper, we propose a label-
aware double transfer learning framework (La-
DTL) for cross-specialty NER, so that a med-
ical NER system designed for one specialty
could be conveniently applied to another one
with minimal annotation efforts. The trans-
ferability is guaranteed by two components:
(i) we propose label-aware MMD for feature
representation transfer, and (ii) we perform
parameter transfer with a theoretical upper
bound which is also label aware. We conduct
extensive experiments on 12 cross-specialty
NER tasks. The experimental results demon-
strate that La-DTL provides consistent accu-
racy improvement over strong baselines. Be-
sides, the promising experimental results on
non-medical NER scenarios indicate that La-
DTL is potential to be seamlessly adapted to a
wide range of NER tasks.

1 Introduction

The development of hospital information system
and medical informatics drives the leverage of var-
ious medical data for a more efficient and intel-
ligent medical care service. Among many kinds
of medical data, electronic health records (EHRs)
are one of the most valuable and informative data
as they contain detailed information about the pa-
tients and the clinical practices. EHRs are essen-
tial to many intelligent clinical applications, such

∗Weinan Zhang is the corresponding author.

as hospital quality control and clinical decision
support systems (Wu et al., 2015). Most of EHRs
are recorded in an unstructured form, i.e., natural
language. Hence, extracting structured informa-
tion from EHRs using natural language processing
(NLP), e.g., named entity recognition (NER) and
entity linking, plays a fundamental role in medical
informatics (Zhang and Elhadad, 2013). In this pa-
per, we focus on medical NER from EHRs, which
is a fundamental task and is widely studied in the
research community (Nadeau and Sekine, 2007;
Uzuner et al., 2011).

In practice, the difficulty of building a univer-
sally robust and high-performance medical NER
system lies in the variety of medical terminologies
and expressions among different departments of
specialties and hospitals. However, building sepa-
rate NER systems for so many specialties comes
with a prohibitively high cost. The data privacy
issue further discourages the sharing of the data
across departments or hospitals, making it more
difficult to train a canonical NER system to be ap-
plied everywhere. This raises a natural question:
if we have sufficient annotated EHRs data in one
source specialty, can we distill the knowledge and
transfer it to help training models in a related tar-
get specialty with few annotations? By transfer-
ring the knowledge we can achieve higher perfor-
mance in target specialties with lower annotation
cost and bypass the data sharing concerns. This
is commonly referred to as transfer learning (Pan
and Yang, 2010).

Current state-of-the-art transfer learning meth-
ods for NER are mainly based on deep neural net-
works, which perform an end-to-end training to
distill sequential dependency patterns in the nat-
ural language (Ma and Hovy, 2016; Lample et al.,
2016). These transfer learning methods include (i)
feature representation transfer (Peng and Dredze,
2017; Kulkarni et al., 2016), which normally lever-

1



ages deep neural networks to learn a close feature
mapping between the source and target domains,
and (ii) parameter transfer (Murthy et al., 2016;
Yang et al., 2017), which performs parameter shar-
ing or joint training to get the target-domain model
parameters close to those of the source-domain
model. To the best of our knowledge, there is no
previous literature working on transfer learning for
NER in the medical domain, or even in a larger
scope, i.e., medical natural language processing.

In this paper, we propose a novel NER trans-
fer learning framework, namely label-aware dou-
ble transfer learning (La-DTL): (i) We leverage
bidirectional long-short term memory (Bi-LSTM)
network (Graves and Schmidhuber, 2005) to au-
tomatically learn the text representations, based
on which we perform a label-aware feature rep-
resentation transfer. We propose a variant of max-
imum mean discrepancy (MMD) (Gretton et al.,
2012), namely label-aware MMD (La-MMD), to
explicitly reduce the domain discrepancy of fea-
ture representations of tokens with the same label
between two domains. (ii) Based on the learned
feature representations from Bi-LSTM, two con-
ditional random field (CRF) models are performed
for sequence labeling for source and target do-
main separately, where parameter transfer learning
is performed. Specifically, an upper bound of KL
divergence between the source and target domain’s
CRF label distributions is added over the emis-
sion and transition matrices across the source and
target CRF models to explore the shareable parts
of the parameters. Both (i) and (ii) have a label-
aware characteristic, which will be discussed later.
We further argue that label-aware characteristic is
crucial for transfer learning in sequence labeling
problems, e.g., NER, because only when the cor-
responding labels are matched, can the “similar”
contexts (i.e. feature representation) and model pa-
rameters be efficiently borrowed to improve the la-
bel prediction.

Extensive experiments are conducted on 12
cross-specialty medical NER tasks with real-world
EHRs. The experimental results demonstrate that
La-DTL provides consistent accuracy improve-
ment over strong baselines, with overall 2.62%
to 6.70% absolute F1-score improvement over the
state-of-the-art methods. Besides, the promising
experimental results on other two non-medical
NER scenarios indicate that La-DTL has the po-
tential to be seamlessly adapted to a wide range of

NER tasks.

2 Related Works

Named Entity Recognition (NER) is fundamen-
tal in information extraction area which aims at
automatic detection of named entities (e.g., per-
son, organization, location and geo-political) in
free text (Marrero et al., 2013). Many high-level
applications such as entity linking (Moro et al.,
2014) and knowledge graph construction (Hachey
et al., 2011) could be built on top of an NER sys-
tem. Traditional high-performance approaches in-
clude conditional random fields models (CRFs)
(Lafferty et al., 2001), maximum entropy Markov
models (MEMMs) (McCallum et al., 2000) and
hidden Markov models (HMMs). Recently, many
neural network-based models have been proposed
(Collobert et al., 2011; Chiu and Nichols, 2016;
Ma and Hovy, 2016; Lample et al., 2016), in
which few feature engineering works are needed
to train a high-performance NER system. The ar-
chitecture of those neural network-based mod-
els are similar, where different neural networks
(LSTMs, CNNs) at different levels (char- and
word-level) are applied to learn feature representa-
tions, and on top of neural networks, a CRF model
is employed to make label predictions.
Transfer Learning distills knowledge from a
source domain to help create a high-performance
learner for a target domain. Transfer learning al-
gorithms are mainly categorized into three types,
namely instance transfer, feature representation
transfer and parameter transfer (Pan and Yang,
2010). Instance transfer normally samples or re-
weights source-domain samples to match the dis-
tribution of the target domain (Chen et al., 2011;
Chu et al., 2013). Feature representation transfer
typically learns a feature mapping which projects
source and target domain data simultaneously onto
a common feature space following similar distri-
butions (Zhuang et al., 2015; Long et al., 2015;
Shen et al., 2017). Parameter transfer normally in-
volves a joint or constrained training for the mod-
els on source and target domains, usually intro-
duce connections between source target param-
eters via sharing (Srivastava and Salakhutdinov,
2013), initialization (Perlich et al., 2014), or inter-
model parameter penalty schemes (Zhang et al.,
2016).
Transfer Learning for NER Training a high-
performance NER system requires expensive and

2



time-consuming manually annotated data. But suf-
ficient labeled data is critical for the generalization
of an NER system, especially for neural network-
based models. Thus, transfer learning for NER is
a practically important problem. The first group
of methods focuses on sharing model parameters
but they differ in the training schemes. He and
Sun (2017) proposed to train the parameter-shared
model with source and target data jointly, while
the learning rates for sentences from source do-
main are re-weighted by the similarity with target
domain corpus. Yang et al. (2017) proposed a fam-
ily of frameworks which share model parameters
in hierarchical recurrent networks to handle cross-
application, cross-lingual, and cross-domain trans-
fer in sequence labeling tasks. Differently, Lee
et al. (2017) first trained the model with source do-
main data and then fine-tuned the model with little
annotated target domain data.

Domain adaptation method has been well stud-
ied in NER scenarios such as using distributed
word representations (Kulkarni et al., 2016) and
leveraging rule-based annotators (Chiticariu et al.,
2010). Multi-task learning has also been stud-
ied to improve performance in multiple NER
tasks by transferring meaningful knowledge from
other tasks (Collobert et al., 2011; Peng and
Dredze, 2016). To take the advantages of both
domain adaptation and multi-task learning, Peng
and Dredze (2017) proposed a multi-task domain
adaptation model.

3 Preliminaries

This section briefly introduces bidirectional
LSTM, conditional random field and maximum
mean discrepancy, which are the building blocks
of our transfer learning framework.
Bidirectional LSTM Recurrent neural networks
(RNNs) are widely used in NLP tasks for their
great capability to capture contextual information
in sequence data. A widely used variant of RNNs
is long short-term memory (LSTM) (Hochreiter
and Schmidhuber, 1997), which incorporates in-
put and forget gates to capture both long and short
term dependencies. Furthermore, it will be ben-
eficial if we process the sequence in not only a
forward but also a backward way. Thus, bidirec-
tional LSTM (Bi-LSTM) was employed in many
previous works (Chiu and Nichols, 2016; Ma and
Hovy, 2016; Lample et al., 2016) to capture bidi-
rectional information in a sequence. More specifi-

cally, for token xt (embedding vector) at timestep
t in sequence X = (x1,x2, ...,xn), the θb-
parameterized Bi-LSTM recurrently updates hid-
den vectors h→t = G

f
θb
(X,h→t−1) and h

←
t =

Gbθb(X,h
←
t+1) produced by a forward LSTM and a

backward one, respectively. Then we concatenate
h→t and h

←
t to ht as the final hidden vector pro-

duced by Bi-LSTM:

ht = h
→
t ⊕ h←t .

The representations learned from Bi-LSTM for se-
quence X is thus denoted as H = (h1,h2, ...,hn).
Conditional Random Field The goal of NER is
to detect named entities in a sequence X by pre-
dicting a sequence of labels y = (y1, y2, ..., yn).
Conditional random field (CRF) is widely used to
make joint labeling of the tokens in a sequence
(Lafferty et al., 2001).

Recently, Lample et al. (2016) proposed to build
a CRF layer on top of a Bi-LSTM so that the au-
tomatically learned feature representation H =
(h1,h2, ...,hn) of the sequence can be directly
fed into the CRF for sequence labeling. For a se-
quence of labels y, given the hidden vector se-
quence H, we define its θc-parametrized score
function sθc(H,y) as:

sθc(H,y) =

n∑

i=1

Ei,yi +

n−1∑

i=1

Ayi,yi+1 ,

where E is the emission score matrix of size n×m
(m is the number of unique labels), and is com-
puted by E = HW where W is the label emission
parameter matrix; A is the label transition parame-
ter matrix; thus θc = {W,A}. We then define the
conditional probability of label sequence y given
H by a softmax over all possible label sequences
in set Y(H) as:

pθc(y|H) = exp{sθc(H,y)}/Z(H) (1)
=exp{sθc(H,y)}

/ ∑

y′∈Y(H)
exp{sθc(H,y′)},

where θc is omitted for simplification in the
following part. The training objective in the
CRF layer is to maximize the log-likelihood
maxθc log p(y|H). In the label prediction
phase, we give the output label sequence
y∗ with the highest conditional probability
y∗ = argmaxy′∈Y(H) p(y′|H) by dynamic
programming (Sutton et al., 2012).
Maximum Mean Discrepancy Maximum Mean
Discrepancy (Gretton et al., 2012) is a non-

3



La-MMD

Source domain Target domain

Shared

Bi-LSTM

Word embedding

CRF CRF

Hidden 
vector  

Hidden 
vector  

L2

Input data 

Figure 1: La-DTL framework overview: embedding
and Bi-LSTM layers are shared across domains, predic-
tors in red (upper) boxes are task-specific CRFs, with
label-aware MMD and L2 constraints to perform fea-
ture representation transfer and parameter transfer.

parametric test statistic to measure the distribu-
tion discrepancy in terms of the distance between
the kernel mean embeddings of two distributions p
and q. The MMD is defined in particular function
spaces that witness the difference in distributions

MMD(F , p, q) = sup
f∈F

(Ex∼p[f(x)]− Ey∼q[f(y)]).

By defining the function class F as the unit
ball in a universal Reproducing Kernel Hilbert
Space (RKHS), denoted by H, it holds that
MMD[F , p, q] = 0 if and only if p = q. And
then given two sets of samples X = {x1, ..., xm}
and Y = {y1, ..., yn} independently and identi-
cally distributed (i.i.d.) from p and q on the data
space X , the empirical estimate of MMD can be
written as the distance between the empirical mean
embeddings after mapping to RKHS

MMD(X,Y ) =
∥∥∥ 1
m

m∑

i=1

φ(xi)− 1
n

n∑

j=1

φ(yj)
∥∥∥
H
, (2)

where φ(·) : X → H is the nonlinear feature map-
ping that inducesH.

4 Methodology

In this section, we present a label-aware double
transfer learning (La-DTL) framework and discuss
its rationale.

4.1 Framework Overview
Figure 1 gives an overview of La-DTL for NER.
From bottom up, each input sentence is converted

into a sequence of embedding vectors, which are
then fed into a Bi-LSTM to sequentially encode
contextual information into fixed-length hidden
vectors. The embedding and Bi-LSTM layers are
shared among source/target domains. With label-
aware maximum mean discrepancy (La-MMD) to
reduce the feature representation discrepancy be-
tween two domains, the hidden vectors are directly
fed into source/target domain specific CRF layers
to predict the label sequence. We use domain con-
strained CRF layers to enhance the target domain
performance.

More formally, let Ds = {(Xsi ,ysi )}N
s

i=1 be
the training set of N s samples from the source
domain and Dt = {(Xti,yti)}

Nt

i=1 be the train-
ing set of N t samples from the target domain,
with N t � N s. Bi-LSTM encodes a sentence
X = (x1,x2, ...,xn) to hidden vectors H =
(h1,h2, ...,hn). We occasionally use H(X) to de-
note the corresponding hidden vectors when feed-
ing X into the Bi-LSTM. CRF decodes hidden
vectors H to a label sequence ŷ = (ŷ1, ŷ2, ..., ŷn).
Our goal is to improve label prediction accuracy
on the target domain Dt by utilizing the knowl-
edge from the source domain Ds:

p(y|X) =p(y|H(X)),

log p(y|H) =
n∑

i=1

Ei,yi +

n−1∑

i=1

Ayi,yi+1 − logZ(H). (3)

Thus training a transferable model p(y|X) re-
quires both H(X) and p(y|H) to be transferable.

We use share word embedding and Bi-LSTM
by approaching the feature representation distribu-
tions p(h|Ds) and p(h|Dt), i.e., the distributions
of Bi-LSTM hidden vectors at each timestep of
the sentences from the source and target domains
respectively. The rationale behind it lies on the
insufficiency of labeled target data. Even though
LSTM has high capacity, its generalization abil-
ity highly relies on viewing “sufficient” data. Oth-
erwise, LSTM is very likely to overfit the data.
Training on both source and target data, the Bi-
LSTM is expected to learn feature representations
with high quality. Yosinski et al. (2014) provided
a justification of this solution that sharing bottom
layers is promising for transfer learning in prac-
tice.

With the sentences projected onto the same hid-
den space, the conditional distribution p(hs|Ds)
and p(ht|Dt), however, may be distant because

4



LSTM hidden vectors contain contextual informa-
tion which is different across domains. In order to
reduce source/target discrepancy, we refine MMD
(Gretton et al., 2012) with label constraints, i.e.,
label-aware MMD (La-MMD). Using La-MMD,
the source/target hidden states are pushed to simi-
lar distributions to make the feature representation
H(X) transfer feasible.

Based on the hidden vectors from Bi-LSTM,
we adopt independent CRF layers for each do-
main. The rationale lies in the hypotheses that (i)
the target domain predictor can better capture tar-
get data distribution which could be very unique;
(ii) a good predictor trained on the source do-
main directly could be leveraged to assist the tar-
get domain predictor without directly borrowing
the source domain training data to bypass the data
privacy issue. With respect to the emission and
transition score matrices

∑
Ei,yi and

∑
Ayi,yi+1 ,

we adopt an upper bound between source/target
domains, which helps the target domain predictor
to be guided by the source domain predictor. Thus
p(y|H) is also transferable.

There are also other transfer methods, including
fine-tuning, sharing parameter directly (without
constraints) (He and Sun, 2017; Lee et al., 2017;
Yang et al., 2017), etc. However, simply sharing
models may dismiss target specific instances.

4.2 Learning Objective

The learning objective is to minimize the fol-
lowing loss L with respect to parameters Θ =
{θb, θc}:

L = Lc + α LLa-MMD + β Lp + γ Lr,

where Lc is the CRF loss, LLa-MMD is the La-
MMD loss, Lp is the parameter similarity loss on
CRF layers, andLr is the regularization term, with
α, β, γ as hyperparameters to balance loss terms.

The CRF loss is our ultimate objective predict-
ing the label sequence given the input sentence,
i.e., we minimize the negative log-likelihood of
training samples from both source/target domains:

Lc = − ε
Ns

Ns∑

i=1

log p(ysi |Hsi )−
1− ε
N t

Nt∑

i=1

log p(yti |Hti),

where H are hidden vectors obtained from Bi-
LSTM, ε is the balance coefficient. The La-MMD
loss LLa-MMD and parameter similarity loss Lp are
discussed in Section 4.3 and 4.4, respectively. The

Figure 2: Illustration for La-MMD. MMD-y is com-
puted between two domains’ hidden representations
with the same ground truth label y. A linear combi-
nation is then applied to each label-wise MMD to form
La-MMD and the coefficient is set as µy = 1.

regularization term is to generally control overfit-
ting:

Lr = ‖θb‖22 + ‖θc‖22.

We will provide the model convergence and hy-
perparameter study in Section 5.1.

4.3 Bi-LSTM Feature Representation
Transfer

To learn transferable feature representations, the
maximum mean discrepancy (MMD) which mea-
sures the distance between two distributions, has
been widely used in domain adaptation scenar-
ios (Long et al., 2015; Rozantsev et al., 2016).
Almost all these works focus on reducing the
marginal distribution distance between different
domain features in an unsupervised manner to
make them indistinguishable. However, consider-
ing a word is not evenly distributed conditioning
on different labels, it may result in that the dis-
criminative property of features from different do-
mains may not be similar, which means that close
source and target samples may not have the same
label. Different from previous works, we propose
label-aware MMD (La-MMD) in Eq. (5) to explic-
itly reduce the discrepancy between hidden repre-
sentations with the same label, i.e., the linear com-
bination of the MMD for each label. For each label
class y ∈ Yv, where Yv is the set of matched labels
in two domains, we compute the squared popula-
tion MMD between the hidden representations of
source/target samples with the same label y:

5



MMD2(Rsy,Rty) =
1

(Nsy )2

Nsy∑

i,j=1

k(hsi ,h
s
j) +

1

(N ty)2

Nty∑

i,j=1

k(hti,h
t
j)

− 2
NsyN ty

Nsy ,N
t
y∑

i,j=1

k(hsi ,h
t
j), (4)

where Rsy and Rty are sets of hidden represen-
tation hs and ht with corresponding number
N sy and N

t
y. Eq. (4) can be easily derived by

casting Eq. (2) into inner product form and
applying 〈φ(x), φ(y)〉H = k(x, y) where k is the
reproducing kernel function (Gretton et al., 2012).
For each label class, we compute the MMD loss
in a normal manner. After that, we define the
La-MMD loss as:

LLa-MMD =
∑

y∈Yv
µy ·MMD2(Rsy,Rty), (5)

where µy is the corresponding coefficient. The il-
lustration of La-MMD is shown in Figure 2.

Once we have applied this La-MMD to our rep-
resentations learned from Bi-LSTM, the represen-
tation distribution of instances with the same la-
bel from different domains should be close. Then
the standard CRF layer which has a simple linear
structure takes these similar representations as in-
put and is likely to give a more transferable label
decision for instances with the same label.

4.4 CRF Parameter Transfer

Simply sharing the CRF layer is non-promising
when source/target data are diversely distributed.
According to probability decomposition in Eq. (3),
in order to transfer on source/target CRF layers,
more specifically, p(y|H), we reduce the KL di-
vergence from pt(y|H) to ps(y|H). But directly
reducing DKL(ps(y|H)||pt(y|H)) is intractable,
we tend to reduce its upper bound:

DKL(p
s(y|H)||pt(y|H))

=
∑

y∈Y(H)
ps(y|H) log(p

s(y|H)
pt(y|H) )

=−H(ps(y|H))−
∑

y∈Y(H)
ps(y|H) log pt(y|H)

≤c(‖Ws −Wt‖22 + ‖As −At‖22)
1
2 , (6)

where H(·) is the entropy of distribution (·) and c
is a constant. The detailed proof is provided in Ap-
pendix A.1. Since c(‖Ws−Wt‖22+‖As−At‖22)
is the upper bound of DKL(ps(y|H)‖pt(y|H)),

L2

L2

L2 L2 L2 L2

L2 L2

Figure 3: Illustration for CRF parameter transfer.

we conduct CRF parameter transfer by minimiz-
ing

Lp = ‖Ws −Wt‖22 + ‖A
s −At‖22.

It turns out that a similar regularization term is
applied in our CRF parameter transfer method
and the regularization framework (RF) for do-
main adaptation (Lu et al., 2016). However, RF
is proposed to generalize the feature augmenta-
tion method in (Daume III, 2007), and these two
methods are only discussed from a perspective
of the parameter. There is no guarantee that two
models having similar parameters yields similar
output distributions. In this work, we discuss the
model behavior in CRF conditions, and we suc-
cessfully prove that two CRF models having sim-
ilar parameters (in Euclidean space) yields similar
output distributions. In another word, our method
guarantees transferability in the model behavior
level, while previous works are limited in parame-
ter level.

The CRF parameter transfer is illustrated in Fig-
ure 3, which is also label-aware since the L2 con-
straint is added over parameters corresponding to
the same label in two domains, e.g., WsO and W

t
O.

4.5 Training

We train La-DTL in an end-to-end manner with
mini-batch AdaGrad (Duchi et al., 2011). One
mini-batch contains training samples from both
domains, otherwise the computation of LLa-MMD
can not be performed. During training, word (and
character) embeddings are fine-tuned to adjust real
data distribution. During both training and decod-
ing (testing) of CRF layers, we use dynamic pro-
gramming to compute the normalizer in Eq. (1)
and infer the label sequence.

6



Department # Train # Dev # Test

Cardiology 3,004 601 601
Respiratory 3,025 605 606
Neurology 932 187 187
Gastroenterology 1,517 303 304

Sum 8,478 1,696 1,698

Table 1: Sentence numbers for CM-NER corpus.

5 Experiments

In this section, we evaluate La-DTL1 and other
baseline methods on 12 cross-specialty NER prob-
lems based on real-world datasets. The experimen-
tal results show that La-DTL steadily outperforms
other baseline models in all tasks significantly. We
also conduct further ablation study and robustness
study. We evaluate La-DTL on two more non-
medical NER transfer tasks to validate its general
efficacy over a wide range of applications.

5.1 Cross-Specialty NER

Datasets We collected a Chinese medical NER
(CM-NER) corpus for our experiments. This cor-
pus contains 1600 de-identified EHRs of our affili-
ated hospital from four different specialties in four
departments: Cardiology (500), Respiratory (500),
Neurology (300) and Gastroenterology (300), and
the research had been reviewed and approved by
the ethics committee. Named entities are anno-
tated in the BIOES format (Begin, Inside, Outside,
End and Single), with 30 types in total. The statis-
tics of CM-NER is shown in Table 1.
Baselines The following methods are compared.
For a fair comparison, we implement La-DTL and
baselines with the same base model introduced in
(Lample et al., 2016) but with different transfer
techniques.

• Non-transfer uses the target domain labeled
data only.

• Domain mask and Linear projection be-
long to the same framework proposed by
Peng and Dredze (2017) but have differ-
ent implementations at the projection layer,
which aims to produce shared feature repre-
sentations among different domains through
a linear transformation.

• Re-training is proposed by Lee et al. (2017),
where an artificial neural networks (ANNs)

1https://github.com/felixwzh/La-DTL

is first trained on the source domain and then
re-trained on the target domain.

• Joint-training is a transfer learning method
proposed by Yang et al. (2017) where differ-
ent tasks are trained jointly.

• CD-learning is a cross-domain learning
method proposed by He and Sun (2017),
where each source domain training example’s
learning rate is re-weighted.

Experimental Settings We use 23,217 unla-
beled clinical records to train the word embed-
dings (word2vec) at 128 dimensions using skip-
gram model (Mikolov et al., 2013). The hidden
state size is set to be 200 for word-level Bi-LSTM.
We evaluate La-DTL for cross-specialty NER with
CM-NER in 12 transfer tasks, results shown in Ta-
ble 2. For each task, we take the whole source
domain training set Ds and 10% sentences of the
target domain training set Dt as training data. We
use the development set in target domain to search
hyper-parameters including training epochs. We
then take the models to make the prediction in tar-
get domain test set and use F1-score as the evalua-
tion metric. Statistical significance has been deter-
mined using a randomization version of the paired
sample t-test (Cohen, 1995).
Results and Discussion From the results of
12 cross-specialty NER tasks shown in Table 2,
we find that La-DTL outperforms all the strong
baselines in all the 12 cross-specialty transfer
learning tasks, with 2.62% to 6.70% F1-score
lift over state-of-the-art baseline methods. Mean-
while, Linear projection and Domain mask (Peng
and Dredze, 2017) do not perform as good as
other three baselines, which may be because
such linear transformation methods are likely to
weaken the representations. While other three
baseline methods all share the whole model be-
tween source/target domains but differ in the train-
ing schemes and performance.

To better understand the transferability of La-
DTL, we also evaluate three variants of La-
DTL: La-MMD, CRF-L2, and MMD-CRF-L2.
La-MMD and CRF-L2 have the same networks
and loss function as La-DTL but with different
building blocks: La-MMD has β = 0, while CRF-
L2 has α = 0. In MMD-CRF-L2, we replace
La-MMD loss LLa-MMD in La-DTL with a vanilla
MMD loss:

LMMD = MMD2(Rs,Rt),

7



Method C→R C→N C→G R→C R→N R→G N→C N→R N→G G→C G→R G→N AVG

Non-transfer 67.20 54.51 49.01 65.63 54.51 49.01 65.63 67.20 49.01 65.63 67.20 54.51 59.09
Linear projection (Peng and Dredze, 2017) 69.01 67.02 57.40 69.79 65.87 57.71 67.70 68.77 51.33 68.00 69.65 61.12 64.45
Domain mask (Peng and Dredze, 2017) 70.76 63.97 58.62 70.18 64.27 58.16 67.93 69.89 56.18 68.87 69.89 63.49 65.18
CD-learning (He and Sun, 2017) 71.38 64.01 56.72 72.17 64.91 58.14 68.99 71.13 56.27 70.17 71.76 62.06 65.64
Re-training (Lee et al., 2017) 72.45 70.55 59.58 72.56 68.59 60.94 69.60 70.08 56.58 70.14 71.90 66.01 67.42
Joint-training (Yang et al., 2017) 69.82 70.49 63.52 71.45 67.03 67.71 70.96 71.43 60.54 69.68 71.55 68.15 68.53

La-MMD 73.08 69.48 59.86 72.53 70.28 60.16 71.31 73.04 57.94 69.80 73.99 67.19 68.22
CRF-L2 73.34 71.52 60.17 72.43 69.72 67.61 69.76 71.54 59.96 69.75 71.82 67.30 68.74
MMD-CRF-L2 73.05 72.35 60.80 72.65 69.87 66.82 70.25 71.75 58.98 70.48 73.98 67.43 69.03
La-DTL 73.59† 72.91† 64.60† 73.88† 73.01† 70.17† 73.08† 73.11† 62.14† 71.61† 74.21† 71.49† 71.15

Table 2: Results (F1-score %) of 12 cross-specialty medical NER tasks. C, R, N, G are short for the department
of Cardiology, Respiratory, Neurology, and Gastroenterology, respectively. † indicates La-DTL outperforms the 6
baselines significantly (p < 0.05).

10% 25% 50% 100%
Sampling rate 

(a)

0.68

0.74

0.80

0.86

0.92

F1
-s

co
re

La-DTL
Joint-training
Non-transfer

0 30 60 90 120 150 180 210
Epochs 

(b)

0.63

0.65

0.67

0.69

0.71

La-DTL
Joint-training

Figure 4: (a) F1-score of La-DTL, Joint-training and
Non-transfer method in C→R task with different sam-
pling rate. (b) The learning curve of La-DTL and Joint-
training in C→R task.

where Rs and Rt are sets of hidden representa-
tion from source and target domain. Results in Ta-
ble 2 show that: (i) Using La-MMD alone does
achieve satisfactory performance since it outper-
forms the best baseline Joint-training (Yang et al.,
2017) in 7 of 12 tasks. And it has a significant
improvement over Domain mask and Linear pro-
jection methods (Peng and Dredze, 2017), which
indicates that using La-MMD to reduce the do-
main discrepancy of feature representations in se-
quence tagging tasks is promising. (ii) CRF-L2
is also a promising method when transferring be-
tween NER tasks, and it improves the La-MMD
method significantly when these two methods are
combined to form La-DTL. (iii) Label-aware char-
acteristic is important in sequence labeling prob-
lems because there is an obvious performance
drop when La-MMD is replaced with a vanilla
MMD in La-DTL. But MMD-CRF-L2 still has
very competitive performance compared to all the
baseline methods. This shows positive empirical
evidence that transferring knowledge at both Bi-
LSTM feature representation level and CRF pa-
rameter level for NER tasks is better than transfer-
ring knowledge at only one of these two levels, as
discussed in Section 4.1.

0.0
05 0.0

1
0.0

2
0.0

4
0.0

6

0.710

0.713

0.716

0.719

0.722

F1
-s

co
re

0.0
1

0.0
3

0.0
8

0.1
2

0.1
6 0.2 0.2

5 0.3 0.3
5 0.4

Figure 5: Hyperparameter study for α, β, and ε.

Robustness to Target Domain Data Sparsity
We further study the sparsity problem (target do-
main) of La-DTL in C→R task comparing to
Joint-training (Yang et al., 2017) and Non-transfer
method. We evaluate La-DTL with different data
volume (sampling rate: 10%, 25%, 50%, 100%) on
the target domain training set. Results are shown
in Figure 4(a). We observe that La-DTL outper-
forms Joint-training and Non-transfer results un-
der all circumstances, and the improvement of La-
DTL is more significant when the sampling rate is
lower.

To show La-DTL’s convergence and significant
improvement over Joint-training, we repeat the
10% sampling rate experiment for 10 times with
10 random seeds. The F1-score on the target do-
main development set for two methods with a 95%
confidence interval is shown in Figure 4(b) where
La-DTL outperforms Joint-training method signif-
icantly.
Hyperparameter Study We study the influence
of three key hyperparameters in La-DTL: α, β,
and ε in C→R task with 10% target domain sam-
pling rate. We first apply a rough grid search for
the three hyperparameters, and the result is (α =
0.02, β = 0.03, ε = 0.3). We then fix two hyper-
parameters and test the third one in a finer gran-
ularity. The results in Figure 5 indicate that set-
ting α ∈ [0.01, 0.04] could better leverage La-
MMD and further setting β ∈ [0.03, 0.12] and
ε ∈ [0.3, 0.4] yields the best empirical perfor-

8



Corpus # Train # Dev # Test

SighanNER 23,182 - 4,636
WeiboNER 1,350 270 270
CoNLL 2003 14,987 3,466 3,684
TwitterNER 1,900 240 254

Table 3: Sentence numbers for non-medical corpora.

Method F1-score

Non-transfer 54.78
Linear projection (Peng and Dredze, 2017)∗ 56.40
Linear projection (Peng and Dredze, 2017) 56.99
Domain mask (Peng and Dredze, 2017)∗ 56.80
Domain mask (Peng and Dredze, 2017) 56.32
CD-learning (He and Sun, 2017)∗ 52.05
CD-learning (He and Sun, 2017) 56.46
Re-training (Lee et al., 2017) 55.36
Joint-training (Yang et al., 2017) 56.80

La-DTL 57.74

Table 4: Results (F1-score %) of WeiboNER transfer.
∗ indicates the result reported in the corresponding ref-
erence.

mance. This shows that we need to balance the
learning objective of the source and target domains
for better transferability.

5.2 NER Transfer Experiment on
Non-medical Corpus

To show La-DTL could be applied in a wide range
of NER transfer learning scenarios, we make ex-
periments on two non-medical NER tasks. Cor-
pora’s details are shown in Table 3.
WeiboNER Transfer Following He and Sun
(2017); Peng and Dredze (2017), we transfer
knowledge from SighanNER (MSR corpus of the
sixth SIGHAN Workshop on Chinese language
processing) to WeiboNER (a social media NER
corpus) (Peng and Dredze, 2015). Results in Table
4 show that La-DTL outperforms all the baseline
methods in Chinese social media domain.
TwitterNER Transfer Following Yang et al.
(2017) we transfer knowledge from CoNLL 2003
English NER (Tjong Kim Sang and De Meulder,
2003) to TwitterNER (Ritter et al., 2011). Since the
entity types in these two corpora cannot be exactly
matched, La-DTL and Joint-training (Yang et al.,
2017) can be applied directly in this case while
other baselines can not. Because the CRF parame-
ter transfer of La-DTL is label-aware, and Joint-
training simply leverages two independent CRF
layers. The results are shown in Table 5, where La-
DTL again outperforms Joint-training, indicating
that La-DTL could be applied seamlessly to trans-

Method F1-score

Non-transfer 34.65
Joint-training (Yang et al., 2017)∗ 43.24

La-DTL 45.71

Table 5: Results (F1-score %) of TwitterNER transfer.
∗ indicates the result reported in the corresponding ref-
erence.

fer learning scenarios with mismatched label sets
and languages like English.

6 Conclusions

In this paper, we propose La-DTL, a label-aware
double transfer learning framework, to conduct
both Bi-LSTM feature representation transfer and
CRF parameter transfer with label-aware con-
straints for cross-specialty medical NER tasks. To
our best knowledge, this is the first work on trans-
fer learning for medical NER in cross-specialty
scenario. Experiments on 12 cross-specialty NER
tasks show that La-DTL provides consistent per-
formance improvement over strong baselines. We
further perform a set of experiments on differ-
ent target domain data size, hyperparameter study
and other non-medical NER tasks, where La-DTL
shows great robustness and wide efficacy. For fu-
ture work, we plan to jointly perform NER and en-
tity linking for better cross-specialty media struc-
tural information extraction.

Acknowledgments

The work done by SJTU is sponsored by
Synyi-SJTU Innovation Program, National Nat-
ural Science Foundation of China (61632017,
61702327, 61772333) and Shanghai Sailing Pro-
gram (17YF1428200).

References
Minmin Chen, Kilian Q Weinberger, and John Blitzer.

2011. Co-training for domain adaptation. In Ad-
vances in Neural Information Processing Systems
24, pages 2456–2464. Curran Associates, Inc.

Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010. Domain adaptation of rule-based annotators
for named-entity recognition tasks. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1002–
1012, Cambridge, MA. Association for Computa-
tional Linguistics.

9



Jason Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional lstm-cnns. Transac-
tions of the Association for Computational Linguis-
tics, 4:357–370.

Wen-Sheng Chu, Fernando De la Torre, and Jeffery F
Cohn. 2013. Selective transfer machine for person-
alized facial action unit detection. In Proceedings of
the IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 3515–3522.

Paul R Cohen. 1995. Empirical methods for artificial
intelligence, volume 139. MIT press Cambridge,
MA.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537.

Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 256–263. Association for Computational Lin-
guistics.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121–2159.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works, 18(5):602–610.

Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch,
Bernhard Schölkopf, and Alexander Smola. 2012.
A kernel two-sample test. J. Mach. Learn. Res.,
13:723–773.

Ben Hachey, Will Radford, and James R. Curran.
2011. Graph-based named entity linking with
wikipedia. In Proceedings of the 12th International
Conference on Web Information System Engineer-
ing, WISE’11, pages 213–226, Berlin, Heidelberg.
Springer-Verlag.

Hangfeng He and Xu Sun. 2017. A unified model
for cross-domain and semi-supervised named entity
recognition in chinese social media. In AAAI, pages
3216–3222.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Vivek Kulkarni, Yashar Mehdad, and Troy Chevalier.
2016. Domain adaptation for named entity recogni-
tion in online media with word embeddings. arXiv
preprint arXiv:1612.00148.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML

’01, pages 282–289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 260–270, San Diego, California. Association
for Computational Linguistics.

Ji Young Lee, Franck Dernoncourt, and Peter Szolovits.
2017. Transfer learning for named-entity recog-
nition with neural networks. arXiv preprint
arXiv:1705.06273.

Mingsheng Long, Yue Cao, Jianmin Wang, and
Michael Jordan. 2015. Learning transferable fea-
tures with deep adaptation networks. In Proceedings
of the 32nd International Conference on Machine
Learning, volume 37 of Proceedings of Machine
Learning Research, pages 97–105, Lille, France.
PMLR.

Wei Lu, Hai Leong Chieu, and Jonathan Löfgren.
2016. A general regularization framework for do-
main adaptation. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, pages 950–954, Austin, Texas. Associa-
tion for Computational Linguistics.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1064–1074, Berlin, Germany.
Association for Computational Linguistics.

Mnica Marrero, Julin Urbano, Sonia Snchez-Cuadrado,
Jorge Morato, and Juan Miguel Gmez-Berbs. 2013.
Named entity recognition: Fallacies, challenges and
opportunities. Computer Standards & Interfaces,
35(5):482 – 489.

Andrew McCallum, Dayne Freitag, and Fernando C. N.
Pereira. 2000. Maximum entropy markov models
for information extraction and segmentation. In
Proceedings of the Seventeenth International Con-
ference on Machine Learning, ICML ’00, pages
591–598, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119. Curran Associates,
Inc.

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity linking meets word sense disam-
biguation: a unified approach. Transactions of the
Association for Computational Linguistics, 2:231–
244.

10



V Murthy, Mitesh Khapra, Pushpak Bhattacharyya,
et al. 2016. Sharing network parameters for
crosslingual named entity recognition. arXiv
preprint arXiv:1607.00198.

David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3–26.

Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Trans. on Knowl. and Data
Eng., 22(10):1345–1359.

Nanyun Peng and Mark Dredze. 2015. Named en-
tity recognition for chinese social media with jointly
trained embeddings. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 548–554, Lisbon, Portugal.
Association for Computational Linguistics.

Nanyun Peng and Mark Dredze. 2016. Improving
named entity recognition for chinese social media
with word segmentation representation learning. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 149–155, Berlin, Germany. As-
sociation for Computational Linguistics.

Nanyun Peng and Mark Dredze. 2017. Multi-task do-
main adaptation for sequence tagging. In Proceed-
ings of the 2nd Workshop on Representation Learn-
ing for NLP, pages 91–100, Vancouver, Canada. As-
sociation for Computational Linguistics.

Claudia Perlich, Brian Dalessandro, Troy Raeder, Ori
Stitelman, and Foster Provost. 2014. Machine learn-
ing for targeted display advertising: Transfer learn-
ing in action. Mach. Learn., 95(1):103–127.

Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1524–1534, Edinburgh, Scotland,
UK. Association for Computational Linguistics.

Artem Rozantsev, Mathieu Salzmann, and Pascal Fua.
2016. Beyond sharing weights for deep domain
adaptation. arXiv preprint arXiv:1603.06432.

Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu.
2017. Wasserstein distance guided representation
learning for domain adaptation. arXiv preprint
arXiv:1707.01217.

Nitish Srivastava and Ruslan R Salakhutdinov. 2013.
Discriminative transfer learning with tree-based pri-
ors. In Advances in Neural Information Processing
Systems 26, pages 2094–2102. Curran Associates,
Inc.

Charles Sutton, Andrew McCallum, et al. 2012. An
introduction to conditional random fields. Founda-
tions and Trends R© in Machine Learning, 4(4):267–
373.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the Seventh Conference on Natu-
ral Language Learning at HLT-NAACL 2003, pages
142–147.

Özlem Uzuner, Brett R South, Shuying Shen, and
Scott L DuVall. 2011. 2010 i2b2/va challenge on
concepts, assertions, and relations in clinical text.
Journal of the American Medical Informatics Asso-
ciation, 18(5):552–556.

Yonghui Wu, Min Jiang, Jianbo Lei, and Hua Xu. 2015.
Named entity recognition in chinese clinical text us-
ing deep neural network. Studies in health technol-
ogy and informatics, 216:624.

Zhilin Yang, Ruslan Salakhutdinov, and William W
Cohen. 2017. Transfer learning for sequence tag-
ging with hierarchical recurrent networks. In ICLR.

Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod
Lipson. 2014. How transferable are features in deep
neural networks? In Proceedings of the 27th In-
ternational Conference on Neural Information Pro-
cessing Systems - Volume 2, NIPS’14, pages 3320–
3328, Cambridge, MA, USA. MIT Press.

Shaodian Zhang and Noémie Elhadad. 2013. Unsuper-
vised biomedical named entity recognition: Experi-
ments with clinical and biological texts. Journal of
biomedical informatics, 46(6):1088–1098.

Weinan Zhang, Ulrich Paquet, and Katja Hofmann.
2016. Collective noise contrastive estimation for
policy transfer learning. In AAAI, pages 1408–1414.

Fuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno Jialin
Pan, and Qing He. 2015. Supervised representation
learning: Transfer learning with deep autoencoders.
In Proceedings of the 24th International Conference
on Artificial Intelligence, IJCAI’15, pages 4119–
4125. AAAI Press.

11



A Appendix

A.1 Detailed Proof
Recall the bound as in Eq. (6):

Lemma A.1. c1(‖Ws −Wt‖22 + ‖As −At‖22) is the upper bound of (ss(H,y)− st(H,y))2.

Proof of Lemma A.1. ⊗ refers to convolutional product, HW ,HA are mask matrices corresponding to
the given hidden vectors H, and c1 is a constant. We have:

(ss(H,y)− st(H,y))2

=(

n∑

i=1

Esi,yi +
n−1∑

i=1

Asyi,yi+1 −
n∑

i=1

Eti,yi −
n−1∑

i=1

Atyi,yi+1)
2

=(Ws ⊗HW + As ⊗HA −Wt ⊗HW −At ⊗HA)2

=((Ws −Wt)⊗HW + (As −At)⊗HA)2

≤2((Ws −Wt)⊗HW )2 + 2((As −At)⊗HA)2

=2(
∑

i,j

(Ws −Wt)i,j ·HWi,j)2 + 2(
∑

p,q

(As −At)p,q ·HAp,q)2

≤2(
∑

i,j

(Ws −Wt)2i,j ·
∑

i,j

(HWi,j)
2) + 2(

∑

p,q

(As −At)2p,q ·
∑

p,q

(HAp,q)
2)

=2(‖Ws −Wt‖22 · ‖HW ‖22) + 2(‖As −At‖22 · ‖HA‖22)
≤c1(‖Ws −Wt‖22 + ‖As −At‖22).

Lemma A.2. c(‖Ws −Wt‖22 + ‖As −At‖22)
1
2 is the upper bound of DKL(ps(y|H)||pt(y|H)).

Proof of Lemma A.2. With Lemma. (A.1), we set ε = (c1(‖Ws −Wt‖22 + ‖As − At‖22))
1
2 ≥ 0 and

c = 2c
1
2
1 , and we have:

ss(H,y)− ε ≤ st(H,y) ≤ ss(H,y) + ε, (7)

log{
∑

y′∈Y(H)
exp[ss(H,y′)]} − ε ≤ log{

∑

y′∈Y(H)
exp[st(H,y′)]} ≤ log{

∑

y′∈Y(H)
exp[ss(H,y′)]}+ ε.

(8)

12



With Eq. (7) and Eq. (8), we can derive

−
∑

y∈Y(H)
ps(y|H) log pt(y|H)

=−
∑

y∈Y(H)
ps(y|H) log exp[s

t(H,y)]∑
y′∈Y(H) exp[s

t(H,y′)]

=−
∑

y∈Y(H)
ps(y|H)

{
st(H,y)− log{

∑

y′∈Y(H)
exp[st(H,y′)]}

}

≤−
∑

y∈Y(H)
ps(y|H)

{
ss(H,y)− ε− log{

∑

y′∈Y(H)
exp[ss(H,y′)]} − ε

}

=−
∑

y∈Y(H)
ps(y|H)

{
log

exp[ss(H,y)]∑
y′∈Y(H) exp[s

s(H,y′)]
−2ε

}

=−
∑

y∈Y(H)
ps(y|H)

{
log ps(y|H)−2ε

}

=H(ps(y|H)) + 2ε.

Finally, we have

DKL(p
s(y|H)||pt(y|H))

=
∑

y∈Y(H)
ps(y|H) log(p

s(y|H)
pt(y|H) )

=−H(ps(y|H))−
∑

y∈Y(H)
ps(y|H) log pt(y|H)

≤−H(ps(y|H)) +H(ps(y|H)) + 2ε
=c(‖Ws −Wt‖22 + ‖As −At‖22)

1
2 .

A.2 Case Analysis

In clinical practice, patients with specific diseases
would be assigned to different departments, and
specialist doctors in their department may pay
more attention to the specific disease. When writ-
ing a medical chart, these specific diseases and
related clinical findings would have a more de-
tailed description. Therefore, some medical terms
would have enriched meanings in different de-
partments accordingly. For example, patients with
rheumatic heart disease are often treated in the de-
partment of Cardiology. The term, “rheumatic”, a
modifier, describes and limits the type of “heart
disease”. In English, “rheumatic” is an adjective
modifying “heart disease”. However, in Chinese,
“rheumatic heart disease” can be regarded as two
diseases, “rheumatism” and “heart disease”. In the
department of Cardiology, “rheumatic heart dis-

ease” is usually mentioned as a single term. While
in other departments, “rheumatism” and “heart
disease” are mostly two independent named enti-
ties in annotated datasets. As such, it is difficult to
train an NER model to capture the relationship be-
tween “rheumatism” and “heart disease”, and band
them as a whole. In the training set of our study,
the diagnostic term “rheumatic heart disease” (in-
cluding synonym) is mentioned for 17 times in
Dept. Cardiology, 16 times in Dept. Respiratory,
none in Dept. Neurology and 3 times in Dept.
Gastroenterology. We use the data from the first
3 departments as source domain training set re-
spectively, and the data from Dept. Gastroenterol-
ogy as the target domain training set. We test our
models on the test set from Dept. Gastroenterol-
ogy, where “rheumatic heart disease” is mentioned
3 times, and compare the results across models

13



Disease TransferTask

# disease term in
source domain

training set

# disease term in
target domain

training set

# disease term in
target domain

test set

# accurate
labeling

without transfer

# accurate
labeling

with transfer

rheumatic
heart disease

C→G 17
0 3 0

3
N→G 0 0
R→G 16 3

pulmonary
heart disease

C→G 4
0 2 0

2
N→G 0 0
R→G 24 2

coronary
atherosclerotic
heart disease

G→N 5
0 15 10

3
C→N 136 15
R→N 23 11

Table 6: Case analysis for cross-specialty medical NER tasks. C, R, N, G are short for department of Cardiology,
Respiratory, Neurology, and Gastroenterology, respectively.

with/without transfer learning. As expected, mod-
els with source training data from Dept. Cardio-
vascular and Respiration correctly predict all these
entities, but the model using source data from
Dept. Neurology fails and so does a model with-
out transfer learning.

Patients with pulmonary heart disease were of-
ten referred to Dept. Respiratory and Dept. Car-
diology. In our training set, “pulmonary heart dis-
ease” (including synonym) is labeled for 24 times
in Dept. Respiratory and 4 times in Dept. Cardi-
ology. In English, “pulmonary” modified “heart
disease”. In Chinese, “pulmonary heart disease”
contains body structure “lung” and disease name
“heart disease”. The model trained with the source
set from both from department of respiratory and
cardiology could correctly recognize the relation
between lung and heart disease and predict the en-
tity in the test set from Dept. Gastroenterology.

Similarly, “coronary atherosclerotic heart dis-
ease” contains two disease names, “coronary
atherosclerosis” and “heart disease”. Training
model using source set from a department where
the terms are enriched could improve the perfor-
mance of recognizing the whole entity.

A.3 Medical Experiments Details

The 30 entity types for medical domain are:
Symptom, Disease, Examination, Treatment, Lab-
oratory index, Products, Body structure, Fre-
quency, Negative word, Value, Trend, Modifica-
tion, Temporal word, Noun of locality, Degree
modifier, Probability, Object, Organism, Location,
Person, Pronoun, Privacy information, Accident,
Action, Header, Instrument and material, Non-
physiological structure, Dosage, Scale, and Prepo-
sition.

La-MMD

Source domain Target domain

Shared

Word Bi-LSTM

Word embedding

CRF CRF

Hidden 
vector

Hidden 
vector

L2

Char  Bi-LSTM

Char  embedding

Input data 

Figure 6: La-DTL framework for language like En-
glish.

A.4 Non-medical Experiments Details

WeiboNER Transfer
Both SighanNER and WeiboNER are annotated in
the BIO format (Begin, Inside and Outside), but
there is one more entity type (geo-political) in Wei-
boNER. For a fair comparison, we follow Peng
and Dredze (2017); He and Sun (2017) to merge
geo-political entities and locations in WeiboNER,
to match different labeling schemes between Wei-
boNER and SighanNER. We use the inconsisten-
cies fixed second version of WeiboNER data and
word embeddings provided by WeiboNER’s devel-
opers (Peng and Dredze, 2015)2 in this experi-
ment.

TwitterNER Transfer
To show that La-DTL could be applied in trans-
fer learning for NER scenario with mismatched

2
https://github.com/hltcoe/golden-horse

14



named entity types and languages like English,
we conduct this experiment transfer from CoNLL
2003 English NER to TwitterNER. The four en-
tity types in CoNLL 2003 English NER are LOC,
PER, ORG, and MISC. The ten entity types in
TwitterNER are company, facility, geo-loc, movie,
musicartist, other, person, product, sportsteam,
and tvshow.

The Joint-training method (Yang et al., 2017)
separates the CRF layers for each domain to
bypass the label mismatch problem. Since our
La-DTL is label-aware, we match four pairs of
named entities between two CoNLL 2003 English
NER and TwitterNER: LOC with geo-loc, PER
with person, ORG with company and MISC with
other to compute LLa-MMD and Lp, and leave six
named entities unmatched. Following Yang et al.
(2017), We leverage char-level Bi-LSTM to gener-
ate better word representations, concatenate it with
pre-trained word embeddings and feed concate-
nated embeddings to the word-level Bi-LSTM.
The framework used for language like English is
illustrated in Figure 6.

We also convert all characters to lowercase and
use the same word embeddings provided by Yang
et al. (2017)3. Also, we concatenate the training
set and the development set for both domains and
sample the same 10% from TwitterNER as (Yang
et al., 2017) to be target domain training data.
Since Yang et al. (2017) merge training and de-
velopment set into training data, both Yang et al.
(2017) and we report the best performance in the
target domain test set.

3
https://github.com/kimiyoung/transfer

15


