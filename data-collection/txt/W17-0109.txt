



















































Instant annotations in ELAN corpora of spoken and written Komi, an endangered language of the Barents Sea region


Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 57–66,
Honolulu, Hawai‘i, USA, March 6–7, 2017. c©2017 Association for Computational Linguistics

Instant annotations in ELAN corpora of spoken and written Komi, an
endangered language of the Barents Sea region

Ciprian Gerstenberger∗
UiT The Arctic University of Norway

Giellatekno – Saami Language Technology
ciprian.gerstenberger@uit.no

Niko Partanen
University of Hamburg

Department of Uralic Studies
niko.partanen@uni-hamburg.de

Michael Rießler
University of Freiburg

Freiburg Institute for Advanced Studies
michael.riessler@frias.uni-freiburg.de

Abstract

The paper describes work-in-progress by
the Izhva Komi language documentation
project, which records new spoken lan-
guage data, digitizes available recordings
and annotate these multimedia data in
order to provide a comprehensive lan-
guage corpus as a databases for future re-
search on and for this endangered – and
under-described – Uralic speech commu-
nity. While working with a spoken vari-
ety and in the framework of documentary
linguistics, we apply language technol-
ogy methods and tools, which have been
applied so far only to normalized writ-
ten languages. Specifically, we describe
a script providing interactivity between
ELAN, a Graphical User Interface tool for
annotating and presenting multimodal cor-
pora, and different morphosyntactic ana-
lysis modules implemented as Finite State
Transducers and Constraint Grammar for
rule-based morphosyntactic tagging and
disambiguation. Our aim is to challenge
current manual approaches in the annota-
tion of language documentation corpora.

1 Introduction

Endangered language documentation (aka docu-
mentary linguistics) has made huge technological
progress in regard to collaborative tools and user
interfaces for transcribing, searching, and archiv-
ing multimedia recordings. However, paradoxi-
cally, the field has only rarely considered applying

∗The order of the authors’ names is alphabetical.

NLP methods to more efficiently annotate qualita-
tively and quantitatively language data for endan-
gered languages, and this, despite the fact that the
relevant computationalmethods and tools arewell-
known from corpus-driven linguistic research on
larger written languages. With respect to the data
types involved, endangered language documenta-
tion generally seems similar to corpus linguistics
(i.e. “corpus building”) of non-endangered lan-
guages. Both provide primary data for secondary
(synchronic or diachronic) data derivations and
analyses (for data types in language documenta-
tion, cf. Himmelmann 2012; for a comparison be-
tween corpus linguistics and language documenta-
tion, cf. Cox 2011).
The main difference is that traditional cor-

pus (and computational) linguistics deals predom-
inantly with larger non-endangered languages, for
which huge amounts of mainly written corpus data
are available. The documentation of endangered
languages, on the other hand, typically results in
rather small corpora of spoken genres.
Although relatively small endangered languages

are also increasingly gaining attention by the com-
putational linguistic research (as an example for
Northern Saami, see Trosterud 2006a; and for
Plains Cree, see Snoek et al. 2014), these projects
work predominantly with written language vari-
eties. Current computational linguistic projects
on endangered languages seem to have simply
copied their approach from already established re-
search on the major languages, including the fo-
cus on written language. The resulting corpora are
impressively large and include higher-level mor-
phosyntactic annotations. However, they repre-
sent a rather limited range of text genres and in-
clude predominantly translations from the relevant

57



majority languages.
In turn, researchers working within the frame-

work of endangered language documentation, i.e.
fieldwork-based documentation, preservation, and
description of endangered languages, often col-
lect and annotate natural texts from a broad va-
riety of genres. Commonly, the resulting spoken
language corpora have phonemic transcriptions as
well as several morphosyntactic annotation layers
produced either manually or semi-manually with
the help of software like Field Linguist’s Toolbox
(or Toolbox, for short),1 FieldWorks Language Ex-
plorer (or FLEx, for short),2 or similar tools. Com-
mon morphosyntactic annotations include glossed
text with morpheme-by-morpheme interlineariza-
tion. Whereas these annotations are qualitatively
rich, including the time alignment of annotation
layers to the original audio or video recordings,
the resulting corpora are relatively small and rarely
reach 150,000 word tokens. Typically, they are
considerably smaller. The main reason for the lim-
ited size of such annotated language documenta-
tion corpora is thatmanual glossing is an extremely
time consuming task and even semi-manual gloss-
ing using FLEx or similar tools is a bottleneck
because disambiguation of homonyms has to be
solved manually.
Another problem we identified especially in the

documentation of endangered languages in North-
ern Eurasia is that sometimes the existence of or-
thographies is ignored, and instead, phonemic or
even detailed phonetic transcription is employed.
It is a matter of fact that as a result of institutional-
ized and/or community-driven language planning
and revitalization efforts many endangered lan-
guages in Russia have established written stan-
dards and do regularly use their languages in writ-
ing. Russia seems to provide a special case com-
pared to many other small endangered languages
in the world, but for some of these languages, not
only Komi-Zyrian, but also for instance North-
ern Khanty, Northern Selkup, or Tundra Nenets
(which are all object to documentation at present),
a significant amount of printed texts can be found
in books and newspapers printed during different
periods, and several of these languages are also
used digitally on the Internet today.3 Compared
to common practice in corpus building for non-

1http://www-01.sil.org/computing/toolbox
2http://fieldworks.sil.org/flex
3See, for instance, The Finno-Ugric Languages and The

Internet Project ( Jauhiainen et al. 2015).

endangered languages or dialects (for Russian di-
alects, cf.Waldenfels et al. 2014), the use of phone-
mic transcriptions in the mentioned language doc-
umentation projects seems highly anachronistic
if orthographic systems are available and in use.
Note also, that many contemporary and historical
printed texts written in these languages are also al-
ready digitized,4 which wouldmake it easily possi-
ble in principle to combine spoken and written data
in one and the same corpus later. The use of an
orthography-based transcription system not only
ease, and hence, speeds up the transcription pro-
cess, but it enables a straightforward integration of
all available (digitized) printed texts into our cor-
pus in a uniform way, too. Note also, that com-
bining all available spoken and written data into
one corpus would not only make the corpora larger
token-wise, but such corpora will also be ideal for
future corpus-based variational sociolinguistic in-
vestigations. Falling back to orthographic tran-
scriptions of our spoken language documentation
corpora seems therefore most sensible. Last but
not least, if research in language documentation is
intended to be useful for the communities as well,
the representation of transcribed texts in orthogra-
phy is the most logical choice.
In our own language documentation projects,

we question especially the special value given
to time consuming phonemic transcriptions and
(semi-)manual morpheme-by-morpheme interlin-
earizations when basic phonological and morpho-
logical descriptions are already available (which
is arguably true for the majority of languages in
Northern Eurasia), as such descriptions serve as
a resource to accessing phonological and morpho-
logical structures. Instead, we propose a step-by-
step approach to reach higher-level morphosyntac-
tic annotations by using and incrementally improv-
ing genuine computational methods. This proce-
dure encompasses a systematic integration of all
available resources into the language documenta-
tion endeavor: textual, lexicographic, and gram-
matical.
The language documentation projects we work

with (cf. Blokland, Gerstenberger, et al. 2015;
Gerstenberger et al. 2017b; Gerstenberger et al.
2017a) are concerned with the building of mul-

4For printed sources from the Soviet Union and earlier,
the Fenno-Ugrica Collection is especially relevant: http:
//fennougrica.kansalliskirjasto.fi; contemporary
printed sources are also systematically digitized, e.g. for both
Komi languages, cf. http://komikyv.ru.

58



timodal language corpora, including at least spo-
ken and written (i.e. transcribed spoken) data and
applying innovative methodology at the interface
between endangered language documentation and
endangered language technology. We understand
language technology as the functional application
of computational linguistics as it is aimed at an-
alyzing and generating natural language in vari-
ous ways and for a variety of purposes. Language
technology can create tools which analyze spoken
language corpora in a much more effective way,
and thus allow one to create better linguistic an-
notations for and descriptions of the endangered
languages in question. A morphosyntactic tagger
applied in corpus building is but one example of
such a practical application. Using automated tag-
ging, rather than the non-automated methods de-
scribed in the previous section, allows for direct-
ing more resources towards transcription and for
working with larger data sets because slow man-
ual annotation no longer necessarily forms a bot-
tleneck in a project’s data management workflow.
The examples in our paper are taken specifically

from Komi-Zyrian, although we work with several
endangeredUralic languages spoken in the Barents
Sea Area. We are developing a common frame-
work for these different language projects in order
to systematically applymethods fromNatural Lan-
guage Processing (NLP) to enrich the respective
corpora with linguistic annotations. In order to do
so, we rely on the following two main principles,
which we have begun implementing consistently
in our own documentation projects:

1. the use an orthography-based transcription
system, and

2. the application of computer-based methods as
much as possible in creating higher-level an-
notations of the compiled corpus data.

2 Available Data and Tools for Dialectal
and Standard Komi

Izhva Komi is a dialect of Komi-Zyrian, hence-
forth Komi, spoken in the Komi Republic as well
as outside the republic in several language islands
in Northeastern Europe and Western Siberia. Be-
side Izhva dialect data we have also a smaller
amount of data from the Udora dialect, spoken in
the West of the Komi Republic. Komi belongs to
the Permic branch of the Uralic language family

and is spoken by approximately 160,000 people
who live predominantly in the Komi Republic of
the Russian Federation. Although formally recog-
nized as the second official language of the Re-
public, the language is endangered as the result of
rapid language shift to Russian.
Komi is a morphologically rich and agglutinat-

ing language with primarily suffixing morphol-
ogy and predominantly head-final constituent or-
der, differential object marking, and accusative-
nominative alignment. The linguistic structure of
Komi and its dialects is relatively well described
(for an English language overview, cf. Hausenberg
1998). However, the existing descriptions focus
on phonology and morphology and are not written
in modern descriptive frameworks.
The most important official actor for (corpus

and status) language planning for standard written
Komi is the Centre for Innovative Language Tech-
nology at the Komi Republican Academy of State
Service and Administration in Syktyvkar, which
is currently creating the Komi National Corpus, a
corpus that already now contains over 30M words.
The spoken data we work with come from our own
“Iźva-Komi Documentation Project” (2014–2016)
and include fully transcribed and translated dialect
recordings on audio and (partly) video. These
recordings were mostly collected during fieldwork
or origin from legacy data. The Udora data are
similar in structure and origin from the project
“Down River Vashka” (2013). An overview of the
Komi data we have at our disposal is shown in Ta-
ble 1.
Our language documentations are archived at

The Language Archive (TLA, for short) at the
Max Planck Institute for Psycholinguistics in Ni-
jmegen/Netherlands (cf. Partanen et al. 2013;
Blokland, Fedina, et al. 2009–2017). For the writ-
ten Komi data, see Fedina et al. (2016–2017) and
the online portal to the above mentioned Centre’s
data and tools, which is called “The Finno-Ugric
Laboratory for Support of the Electronic Represen-
tation of Regional Languages”.5 The Centre has
also made free electronic dictionaries and a Hun-
spell checker (including morpheme lists) avail-
able, but research towards a higher-level grammar
parser has not been carried out in Syktyvkar so far.
Another important open-source language tech-

nology infrastructure for Komi is under devel-
opment by Jack Rueter (Helsinki) at Giella-

5http://fu-lab.ru

59



Table 1: Overview on the amount of Komi spoken and written data in our projects at present; the cat-
egory Tokens refers to the number of transcribed tokens in both audio/video recordings and digitized
transcribed spoken texts lacking a recording; note that these numbers are only very rough estimates; note
also that typically, our data include translations into at least one majority language too.

Language Modality Recorded spea- Time span Tokens in
kers/writers of texts corpus

Komi-Zyrian (Standard) written ~2,500 1920–2017 30,000,000
Komi-Zyrian (Izhva dialect) spoken ~150 1844–2016 200,000
Komi-Zyrian (Udora dialect) spoken ~50 1902–2013 40,000

tekno/Divvun – Saami Language Technology at
UiT The Arctic University of Norway.6 The
Giellatekno group works with computational lin-
guistic research into the analysis of Saamic and
other languages of the circumpolar area. Giella-
tekno has the know-how and the infrastructure nec-
essary to deal with all aspects of corpus and com-
putational linguistics and has fully implemented
this for Northern Saami (cf. Moshagen et al. 2013;
Johnson et al. 2013; Trosterud 2006b).
The project described in this paper makes use

of the infrastructure and tools already available in
Syktyvkar and Tromsø, but works specificallywith
corpus construction and corpus annotation of spo-
ken data, which have not been in focus of compu-
tational linguistic research so far.

3 Data Annotation Process

Nowadays, there is a multitude of approaches for
Natural Language Processing (NLP) with ‘pure’
statistic-based on one end of a scale, ‘pure’ rule-
based on the other end, and a wide range of hy-
bridization in between (cf. also a recent “hybrid”
approach using a manually interlinearized/glossed
language documentation corpus from Ingush as
training data for a tagger, Tiedemann et al. 2016).
For major languages such as English, Spanish,

or Russian, the dominating paradigm within com-
putational linguistics is based on statistical me-
thods: computer programs are trained to under-
stand the behavior of natural language by means of
presenting themwith vast amounts of either unana-
lyzed or manually analyzed data. However, for the
majority of the world’s languages, and especially
for low-resourced endangered languages, this ap-
proach is not a viable option because the amounts
of texts that would be required – analyzed or not –
are often not available. In many cases the language

6http://giellatekno.uit.no, http://divvun.no

documentation work is the first source of any texts
ever, although the increasing written use of many
minority languages cannot be underestimated ei-
ther. There have been successful projects which
have built online corpora for a large of variety of
these languages,7 and it remains to be seen how
they can be integrated to language documentation
materials on these and related languages.
The older paradigm of language data analysis

is the rule-based or grammar-based approach: the
linguist writes a grammar rules in a specific for-
mat that is machine-readable, the formal grammar
is then compiled into a program capable of ana-
lyzing (and eventually also generating) text input.
There are several schools within the rule-based
paradigm; the approach chosen by our projects is
a combination of Finite-State Transducer (FST)
technology for the morphological analysis, and
Constraint Grammar (CG) for the syntactic analy-
sis.
This approach has been tested with several writ-

ten languages, for which it routinely provides
highly robust analyses for unconstrained text in-
put. We adapt the open preprocessing and ana-
lysis toolkit provided by Giellatekno (cf. Mosha-
gen et al. 2013) for both written and spoken, tran-
scribed language data. Since the chosen infras-
tructure is built for standard written languages, we
have developed a set of conventions to convert our
spoken language data into a “written-like” format,
which is thus more easily portable into the Giella-
tekno infrastructure. First, we represent our spo-
ken recordings in standardized orthography (with
adaptations for dialectal and other sub-standard
forms if needed). Second, we mark clause bound-
aries and use other punctuation marks as in writ-
ten language, although surface text structuring in
spoken texts is prosodic rather than syntactic and
the alignment of our texts to the original record-

7http://web-corpora.net/wsgi3/minorlangs/

60



ing is utterance-based, rather than sentence-based.
For specific spoken language phenomena, such as
false starts, hesitations or self-corrections as well
as for marking incomprehensible sections in our
transcription, we use a simple (and orthography-
compatible) markup adapted from annotation con-
ventions commonly used in spoken language cor-
pora.
Our transcribed spoken text data (using standard

orthography) as well as any written text data are
stored in the XML format provided by the multi-
media language annotation programEUDICOLin-
guistic Annotator (ELAN, for short)8 which allows
audio and video recordings to be time-aligned with
detailed, hierarchically organized tiers for tran-
scriptions, translations and further annotations.
The annotation process contains the following

steps:

1. preprocessing: a Perl script configured with
a list of language-specific abbreviations that
takes care of tokenization;

2. morphosyntactic analysis: an FST that
models free and bound morpheme by means
of linear (lexc) and non-linear rules (twolc)
needed for word formation and inflection;

3. disambiguation: a formal grammar written
in the CG framework.

The process of annotation enrichment in ELAN
follows the usual analysis pipeline of the Giella-
tekno infrastructure. The string of each utterance is
extracted from the orthography-tier, tokenized,
then sent to the morphosyntactic analyzer, and fi-
nally to the disambiguation module. The analysis
output is then parsed and the bits of information
are structured and put back into the ELAN file.
Yet, as simple as it looks, the implementation

required a careful analysis of item indexing in
ELAN. On the one hand, all new annotation items
have to land in the correct place in the struc-
ture, which involves keeping track of the respec-
tive indices for speaker and utterance. On the
other hand, new XML element indices have to
be generated in such a way that they should not
conflict with the extant indices assigned when an
ELAN file is created. Since ELAN data can in-
clude the transcribed overlapping speech of se-
veral recorded speakers, it is not only four new
tiers for word, lemma, part-of-speech, and

8https://tla.mpi.nl/tools/tla-tools/elan.

morphosyntactic description that need to be
generated and added to the initial structure, but
4xN, with N being the total number of speak-
ers recorded in the ELAN file. If the new tiers
were not generated and placed in the correct place,
the ELAN XML structure would be spoiled, thus
blocking the enriched ELAN file from showing up
in the ELAN GUI as desired.
Since the ELAN files are in XML format, they

can be both read and edited by humans with any
text editor and accessed automatically by virtually
any programming language. For the implementa-
tion of the script that links the ELAN data and the
FST/CG, we decided to use Python because:

1. it is a flexible, interpreted programming lan-
guage with support for multiple systems and
platforms;

2. it is easy to read and to learn for even a novice
programmer, which is perhaps the reasonwhy
it is often used for linguistic applications;

3. and finally, it offers XML processing support
by means of XML packages such as Element-
Tree and lxml.

The input file for the whole process is an ELAN
file lacking word, lemma, part-of-speech, and
morphological description tiers. Thus, all
tiers dependent on the word-tier are inserted
dynamically (cf. Figure 1). For each speaker
recorded in the ELAN file, the values of each ut-
terance string from each individual orth-tier are
extracted by the Python script and sent to the ap-
propriate morphosyntactic analyzer.
After the FST has analyzed the word forms, has

output the analyses, and the analyses are sent to
disambiguation, the Python script parses the final
output of the pipeline and restructures it whenmul-
tiple analyses in a cohort are possible, that means
when the disambiguation module could not disam-
biguate the output totally. A cohort is a word form
along with all its possible analyses from the FST.
Each individual lemma depends on the word form
sent to the FST, each part-of-speech depends on a
specific lemma, and finally each morphosyntactic
description depends on a specific part-of-speech.
With these constraints, new ELAN tiers for the
analysis are built by factoring the different item
types accordingly.
Ambiguities in language analyses are quite com-

mon, but with FSTs in development for minor-
ity languages, they are even more frequent. Our

61



Figure 1: The result of the first two processing steps – tokenization and morphosyntactic analysis for the
Komi sentenceМиян нёль баба чомъянум пыр оліс. ‘In our chum (tent) four women lived permanently.’

Figure 2: The result of the last processing step – (partial) disambiguation of morphosyntactic ambiguous
analyses for the same Komi sentence as in Figure 1: for instance, for the word form чомъянум the
contextually appropriate noun reading with lemma чом is chosen while the adjective reading with
lemma чомъя is discarded.

62



plan is to further develop the extant disambigua-
tion module in a similar way as for Northern Saami
(cf. Antonsen, Huhmarniemi, et al. 2009).
Note that the lack of a higher-level analysis of-

ten leads to cases of ambiguity concerning themor-
phological analysis, i.e., multiple analyses for one
and the same word form. As already mentioned,
for the disambiguation of these homonyms, we
use CG, which takes the morphologically analyzed
text as its input, and ideally only returns the ap-
propriate reading, based on the immediate context
of a specific word form–analysis pair. CG is a
language-independent formalism for morphologi-
cal disambiguation and syntactic analysis of text
corpora developed by Karlsson (1990) and Karls-
son et al. (1995). Since we use the standard ana-
lysis pipeline offered by the Giellatekno’s infras-
tructure, we use the CG implementation vislcg3,9
which is freely available.
The CG analysis can be enriched with syntac-

tic functions and dependency relations if all under-
lying grammatical rules are described sufficiently.
Since the output of a CG analysis is a dependency
structure for a particular sentence, the output may
also be converted into phrase structure representa-
tions.
Our work with the CG description of Komi is

only at the beginning stage. To be completed,
it would likely need to include several thousand
rules. However, the experience of other Giella-
tekno projects working with CG shows that some
months of concentrated work can result in a CG
description that can already be implemented in a
preliminary tagger useful for lexicographic work
as well as for several other purposes. For instance,
the rather shallow grammar parser for South-
ern Saami described by Antonsen and Trosterud
(2011) includes only somewhat more than 100
CG rules, but already results in reasonably good
lemmatization accuracy for open class parts-of-
speech. This means that the approach is readily ad-
justable to language documentation projects with
limited resources. Furthermore, CG rules can po-
tentially be ported from one language to another,
e.g. the rule for disambiguating the connegative
verb in Komi would also work in several other
Uralic languages.
Komi is an agglutinative language, so with mor-

phologically more complex forms the homonymy
is rare. However, there are specific forms which

9https://visl.sdu.dk/cg.html

tend to collide with one another, especially with
some verb and noun stems which happen to be
identical. The monosyllabic stems have canonical
shape CV(C), and this simple structure is one rea-
son that makes morphologically unmarked forms
to be homonymous: nominative singular nouns,
and some of the imperative and connegative forms
in the verbal paradigm.

4 Post-Processing of Corpus Data in
ELAN

Note that so far, our work with the script has re-
sulted in a rather insular solution which is directly
applicable to our own projects only. In order to
make our workflow usable for other projects we
have to find a more generic solution. Potentially,
our script could be integrated into the ELAN pro-
gram using a web service. On the other hand, mod-
ifying the Python script to work with somewhat
different ELAN input files would not be very dif-
ficult.
However, since our approach aims at separating

the annotation tool (i.e. the FST/CG-based tagger)
from the non-annotated corpus data (i.e. the tran-
scription), we are relying on the ELAN GUI only
until the transcription (and translation) of the origi-
nal recordings is done in this program. The ELAN-
FST/CG interaction does not depend on the ELAN
tool, but only on the data in ELANXML. Once the
corpus is extended by new transcripts (or existing
transcripts are corrected) or the FST/CG rules are
refined, the whole corpus will be analyzed anew.
This will simply overwrite the previous data in the
relevant ELAN tiers.
In this way, we will release new annotated cor-

pus in regular intervals and make them available
at TLA in Nijmegen, where we archive and make
available our project data. One significant advan-
tage of working with ELAN XML is that it can be
accessed through the online tools Annotation Ex-
plorer (ANNEX, for short)10 and TROVA11, which
are basically online GUIs of the same search en-
gines built into the ELAN tool. ANNEX is an in-
terface that links annotations and media files from
the archive online (just as ELAN does on a local
computer). The TROVA tool can be used to per-
form complex searches on multiple layers of the
corpus and across multiple files in basically the

10https://tla.mpi.nl/tools/tla-tools/annex
11Search engine for annotation content archived at TLA,

https://tla.mpi.nl/tools/tla-tools/trova

63



same was as within ELAN itself. As a practical
benefit, the integration of TROVA and the whole
infrastructure at TLA makes it easy to control
corpus access, something that regularly demands
significant consideration when sensitive language
documentation materials are concerned. At the
same time this infrastructure also allows examples
to be disseminated more widely because it is easy
to provide links for specific utterances in the data.
Ultimately of course, the access rights of a specific
file in the archive determine whether this works or
not.

5 Summary

In this paper, we describe a project at the interface
between language technology and language docu-
mentation. Our aim is to overcome manual anno-
tation of our language documentation corpora. We
achieve this by implementing Finite State Trans-
ducers and Constraint Grammar for rule-based
morphosyntactic tagging and disambiguation. The
differentmodules in our annotation tool interact di-
rectly with our corpus data, which is consistently
structured in ELAN XML.
Since Constraint Grammar can be simply

chained, we plan not only to extend and improve
the current disambiguation module but also to im-
plement Constraint Grammar for further, syntac-
tic analysis, to achieve fully disambiguated depen-
dency treebanks. The relevant work will be carried
out as part of the project “Language Documenta-
tion meets Language Technology: The Next Step
in the Description of Komi” which already started
in 2017.
While the rule-basedmorphosyntactic modeling

is not state of the art in contemporary NLP, it does
have significant advantages specifically in endan-
gered language documentation:

1. the results of the automatic tagging are ex-
ceptionally precise and cannot normally be
reached with statistical methods applied on
the relatively small corpora endangered lan-
guage documentation normally creates;

2. while incrementally formulating rules and
testing them on the corpus data, we are not
only creating a tool but producing a full-
fledged grammatical description based on
broad empirical evidence at the same time;

3. and last but not least, our workwill eventually
also help develop new (I)CALL technology,

i.e. (intelligent) computer-assisted language
learning systems for the languages we work
on.

One relatively simple example for the latter is
the direct implementation of our formalized mor-
phosyntactic description in Voikko spell-checkers,
which is a easily donewith the Giellatekno/Divvun
infrastructure setup and which is an highly impor-
tant tool to support futurewriters of the endangered
language.
Last but not least, since the official support

and language planning activities are significant at
present for Komi and some of the other languages
we are working on, these languages are increas-
ingly used in spoken and written form. Better
adaptation of computational technology by lan-
guage documenters will eventually be necessary in
order to annotate and make efficient use of the ever
increasing amount of available data.
Ultimately, our work will therefore also con-

tribute to future language planning, language de-
velopment, and language vitalization.

Acknowledgments

This paper describes on-going work by the Izhva
Komi Documentation Project, which is funded
by the Kone Foundation between 2014–2016 and
2017–2020 and embedded in a larger research
group12 working with the documentation and de-
scription of several endangered languages of the
Barents Sea Area in Northeastern Europe. The
present article builds on and continues our pre-
liminary research published in Blokland, Gersten-
berger, et al. 2015; Gerstenberger et al. 2017a;
Gerstenberger et al. 2017b. We would like to
thank the other co-authors of these papers, Rogier
Blokland, Marina Fedina, and Joshua Wilbur for
their ongoing collaboration. We also want to ex-
press our gratitude to Trond Trosterud and Jack
Rueter for continuous valuable feedback. Last but
not least, important ideas for our research were
developed while writing the application for the
long-term project INEL13 by Beáta Wagner-Nagy
together with Michael Rießler and The Hamburg
Center for Language Corpora (University of Ham-
burg).

12http://saami.uni-freiburg.de
13https://inel.corpora.uni-hamburg.de

64



References

Antonsen, Lene, S. Huhmarniemi, and Trond
Trosterud (2009). “Constraint Grammar in dia-
logue systems”. In: NEALT Proceedings Series
2009. Vol. 8. Tartu: Tartu ülikool, pp. 13–21.

Antonsen, Lene and Trond Trosterud (2011).
“Next to nothing. A cheap South Saami dis-
ambiguator”. In: Proceedings of the NODAL-
IDA 2011 Workshop Constraint Grammar Ap-
plications, May 11, 2011 Riga, Latvia. Ed. by
Eckhard Bick, Kristin Hagen, Kaili Müürisep,
and Trond Trosterud. NEALT Proceedings Se-
ries 14. Tartu: Tartu University Library, pp. 1–
7. url: http://hdl.handle.net/10062/
19296.

Blokland, Rogier, Marina Fedina, Niko Partanen,
andMichael Rießler (2009–2017). “IzhvaKyy”.
In: The Language Archive (TLA). Donated Cor-
pora. In collab. with Vasilij Čuprov, Marija Fe-
dina, Dorit Jackermeier, Elena Karvovskaya,
Dmitrij Levčenko, and Kateryna Olyzko. Ni-
jmegen:Max Planck Institute for Psycholinguis-
tics. url: https : / / corpus1 . mpi . nl / ds /
asv/?5&openhandle=hdl:1839/00-0000-
0000-000C-1CF6-F.

Blokland, Rogier, Ciprian Gerstenberger, Marina
Fedina, Niko Partanen, Michael Rießler, and
Joshua Wilbur (2015). “Language documenta-
tionmeets language technology”. In:First Inter-
national Workshop on Computational Linguis-
tics for Uralic Languages, 16th January, 2015,
Tromsø, Norway. Proceedings of the workshop.
Ed. by Tommi A. Pirinen, Francis M. Tyers,
and Trond Trosterud. Septentrio Conference Se-
ries 2015:2. Tromsø: The University Library of
Tromsø, pp. 8–18. doi: 10.7557/scs.2015.2.

Cox, Christopher (2011). “Corpus linguistics and
language documentation. Challenges for col-
laboration”. In: Corpus-based Studies in Lan-
guage Use, Language Learning, and Language
Documentation. Ed. by John Newman, Harald
Baayen, and Sally Rice. Amsterdam: Rodopi,
pp. 239–264.

Fedina, Marina, Enye Lav, Dmitri Levchenko,
Ekaterina Koval, and Inna Nekhorosheva
(2016–2017). Nacional’nyj korpus komi
jazyka. Syktyvkar: FU-Lab. url: http :
//komicorpora.ru.

Gerstenberger, Ciprian, Niko Partanen, Michael
Rießler, and JoshuaWilbur (2017a). “Instant an-
notations. Applying NLP methods to the anno-

tation of spoken language documentation cor-
pora”. In: Proceedings of the 3rd International
Workshop on Computational Linguistics for
Uralic languages. Proceedings of the workshop.
Ed. by Tommi A. Pirinen, Michael Rießler,
Trond Trosterud, and FrancisM. Tyers. ACL an-
thology. Baltimore, Maryland, USA: Associa-
tion for Computational Linguistics (ACL). url:
http://aclweb.org/anthology/. In press.

– (2017b). “Utilizing language technology in
the documentation of endangered Uralic lan-
guages”. In:Northern European Journal of Lan-
guage Technology: Special Issue on Uralic Lan-
guage Technology. Ed. by Tommi A. Pirinen,
Trond Trosterud, and Francis M. Tyers. url:
http://www.nejlt.ep.liu.se/. In press.

Hausenberg, Anu-Reet (1998). “Komi”. In: ed. by
Daniel Abondolo. Routledge Language Family
Descriptions. London: Routledge, pp. 305–326.

Himmelmann, Nikolaus (2012). “Linguistic data
types and the interface between language docu-
mentation and description”. In: Language Doc-
umentation & Conservation 6, pp. 187–207. url:
http://hdl.handle.net/10125/4503.

Jauhiainen, Heidi, Tommi Jauhiainen, and Kris-
ter Lindén (2015). “The Finno-Ugric Languages
and The Internet Project”. In: First Interna-
tional Workshop on Computational Linguis-
tics for Uralic Languages, 16th January, 2015,
Tromsø, Norway. Proceedings of the workshop.
Ed. by Tommi A. Pirinen, Francis M. Tyers,
and Trond Trosterud. Septentrio Conference Se-
ries 2015:2. Tromsø: The University Library of
Tromsø, pp. 87–98. doi: 10.7557/5.3471.

Johnson, Ryan, Lene Antonsen, and Trond
Trosterud (2013). “Using finite state transduc-
ers for making efficient reading comprehension
dictionaries”. In: Proceedings of the 19th
Nordic Conference of Computational Linguis-
tics (NODALIDA 2013), May 22–24, 2013,
Oslo. Ed. by Stephan Oepen and Janne Bondi
Johannessen. Linköping Electronic Conference
Proceedings 85. Linköping: Linköping Univer-
sity, pp. 59–71. url: http://emmtee.net/oe/
nodalida13/conference/45.pdf.

Karlsson, Fred (1990). “Constraint Grammar as
a framework for parsing unrestricted text”. In:
Proceedings of the 13th International Confer-
ence of Computational Linguistics. Ed. by Hans
Karlgren. Vol. 3. Helsinki, pp. 168–173.

65



Karlsson, Fred, Atro Voutilainen, Juha Heikkilä,
and Arto Anttila, eds. (1995). Constraint Gram-
mar. A language-independent system for pars-
ing unrestricted text. Natural Language Process-
ing 4. Berlin: Mouton de Gruyter.

Moshagen, Sjur, Tommi A. Pirinen, and Trond
Trosterud (2013). “Building an open-source
development infrastructure for language tech-
nology projects”. In: Proceedings of the 19th
Nordic Conference of Computational Linguis-
tics (NODALIDA 2013), May 22–24, 2013,
Oslo. Ed. by Stephan Oepen and Janne Bondi
Johannessen. Linköping Electronic Conference
Proceedings 85. Linköping: Linköping Univer-
sity, pp. 343–352. url: http://emmtee.net/
oe/nodalida13/conference/43.pdf.

Partanen, Niko, Alexandra Kellner, Timo
Rantakaulio, Galina Misharina, and Hamel
Tristan (2013). “Down River Vashka. Corpus
of the Udora dialect of Komi-Zyrian”. In: The
Language Archive (TLA). Donated Corpora.
Nijmegen: Max Planck Institute for Psycholin-
guistics. url: https : / / hdl . handle . net /
1839/00-0000-0000-001C-D649-8.

Snoek, Conor, Dorothy Thunder, Kaidi Lõo, Antti
Arppe, Jordan Lachler, Sjur Moshagen, and
Trond Trosterud (2014). “Modeling the noun
morphology of Plains Cree”. In: Proceedings
of the 2014 Workshop on the Use of Computa-
tional Methods in the Study of Endangered Lan-
guages. Baltimore, Maryland, USA: Associa-
tion for Computational Linguistics, pp. 34–42.
url: http://www.aclweb.org/anthology/
W/W14/W14-2205.

Tiedemann, Jörg, Johanna Nichols, and Ronald
Sprouse (2016). “Tagging Ingush. Language
technology for low-resource languages using re-
sources from linguistic field work”. In: Pro-
ceedings of the Workshop on Language Tech-
nology Resources and Tools for Digital Human-
ities (LT4DH). Osaka, Japan, December 11–
17 2016, pp. 148–155. url: https : / / www .
clarin-d.de/joomla/images/lt4dh/pdf/
LT4DH20.pdf.

Trosterud, Trond (2006a). “Grammar-based lan-
guage technology for the Sámi Languages”. In:
Lesser used Languages & Computer Linguis-
tics. Bozen: Europäische Akademie, pp. 133–
148.

– (2006b). “Grammatically based language tech-
nology for minority languages. Status and poli-

cies, casestudies and applications of information
technology”. In: Lesser-known languages of
South Asia. Ed. by Anju Saxena and Lars Borin.
Trends in Linguistics. Studies and Monographs
175. Berlin: Mouton de Gruyter, pp. 293–316.

Waldenfels, Ruprecht von, Michael Daniel, and
Nina Dobrushina (2014). “Why standard or-
thography? Building the Ustya River Basin Cor-
pus, an online corpus of a Russian dialect”. In:
Computational linguistics and intellectual tech-
nologies. Proceedings of the Annual Interna-
tional Conference «Dialogue» (2014). Moskva:
Izdatel’stvo RGGU, [720–729]. url: http://
www.dialog-21.ru/digests/dialog2014/
materials/pdf/WaldenfelsR.pdf.

66


