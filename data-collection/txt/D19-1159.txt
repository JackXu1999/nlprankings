




































Robust Navigation with Language Pretraining and Stochastic Sampling


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1494–1499,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1494

Robust Navigation with Language Pretraining and Stochastic Sampling

Xiujun Li♠♦ Chunyuan Li♦ Qiaolin Xia♣ Yonatan Bisk♠♦♥
Asli Celikyilmaz♦ Jianfeng Gao♦ Noah A. Smith♠♥ Yejin Choi♠♥

♠Paul G. Allen School of Computer Science & Engineering, University of Washington
♣Peking University ♦Microsoft Research AI ♥Allen Institute for Artificial Intelligence

{xiujun,ybisk,nasmith,yejin}@cs.washington.edu
xql@pku.edu.cn {xiul,chunyl,jfgao}@microsoft.com

Abstract

Core to the vision-and-language navigation
(VLN) challenge is building robust instruction
representations and action decoding schemes,
which can generalize well to previously un-
seen instructions and environments. In this pa-
per, we report two simple but highly effective
methods to address these challenges and lead
to a new state-of-the-art performance. First,
we adapt large-scale pretrained language mod-
els to learn text representations that generalize
better to previously unseen instructions. Sec-
ond, we propose a stochastic sampling scheme
to reduce the considerable gap between the ex-
pert actions in training and sampled actions
in test, so that the agent can learn to correct
its own mistakes during long sequential action
decoding. Combining the two techniques, we
achieve a new state of the art on the Room-to-
Room benchmark with 6% absolute gain over
the previous best result (47% → 53%) on the
Success Rate weighted by Path Length metric.

1 Introduction

The vision-and-language navigation (VLN) task,
learning to navigate in visual environments based
on natural language instructions, has attracted
interest throughout the artificial intelligence re-
search community (Hemachandra et al., 2015; An-
derson et al., 2018; Chen et al., 2019; Savva et al.,
2019). It fosters research on multimodal represen-
tations and reinforcement learning, and serves as a
test bed for many real-world applications such as
in-home robots.

In the recent Room-to-Room (R2R) VLN chal-
lenge (Anderson et al., 2018), most state-of-the-
art methods are developed based on an encoder-
decoder framework (Cho et al., 2014; Sutskever
et al., 2014), where a natural language instruc-
tion is represented as a sequence of words, and
a navigation trajectory as a sequence of actions,

Exposure Bias

Generalization

Training

Unseen Env.Seen Env.

Evaluation

Teacher.Forcing Student.ForcingChallenge 2

Challenge 1

Figure 1: Two challenges in VLN.

enhanced with attention (Anderson et al., 2018;
Wang et al., 2019; Fried et al., 2018; Ma et al.,
2019a). Two important components are shared by
all VLN agents: (i) an Instruction Encoder that
employs a language model (LM) for instruction
understanding; and (ii) an Action Decoder, where
an appropriate sequence-level training scheme is
required for sequential decision-making. Each
component faces its own challenges (see Figure 1).

The first challenge is generalizing grounded
natural language instruction understanding from
seen to unseen environments. Specifically, in the
R2R task, only 69% of bigrams are shared be-
tween training and evaluation.1 Existing work
leverages pretrained GloVe embeddings (Penning-
ton et al., 2014) to help generalize. In computer
vision, it has been shown that large-scale models
pretrained on ImageNet can transfer the knowl-
edge to downstream applications (Yosinski et al.,
2014), thus improving generalization. Compara-
ble language-based transfer learning has not been
shown for instruction understanding in VLN.

The second challenge is exposure bias (Ranzato
et al., 2016) for the action decoder, due to the
discrepancy between training and inference. This
problem is common to many tasks where decod-
ing is needed, including text generation, abstrac-
tive summarization, and machine translation (Ben-

1Table 1 shows n-gram overlap statistics between training
seen and validation seen/unseen environments.



1495

n-gram(s) Validation Seen Validation Unseen
Tr

ai
ni

ng 1 87.2% 80.7%
2 77.4% 68.9%
3 65.6% 57.3%
4 50.8% 44.4%

Table 1: N-grams instruction overlap statistics between
validation seen and unseen environments.

gio et al., 2015). Two widely used training strate-
gies are student-forcing and teacher-forcing (de-
scribed in detail in Section 2.2). It is well-known
that the sequence length determines which train-
ing strategy is more effective. In the VLN lit-
erature, student-forcing has been widely used, as
early work (Anderson et al., 2018) used long tra-
jectories (up to 20 steps) with a simple discrete ac-
tion space. Most recent work, however, has relied
on a panoramic action space (Fried et al., 2018)
in which most trajectories are only up to seven
steps long. In such cases, teacher-forcing is prefer-
able (Tan et al., 2019). Neither strategy is perfect:
teacher-forcing has exposure bias, while student-
forcing’s random actions can cause an agent to de-
viate far from the correct path, rendering the orig-
inal instruction invalid.2

To tackle these challenges, we have devel-
oped two techniques to enable the agent to nav-
igate more efficiently. For the first challenge,
we leverage the recent large-scale pretrained lan-
guage models, BERT (Devlin et al., 2019) and
GPT (Radford et al., 2018), to improve the agent’s
robustness in unseen environments. We show that
large-scale language-only pretraining improves
generalization in grounded environments. For
the second challenge, we propose a stochastic
sampling scheme to balance teacher-forcing and
student-forcing during training, so that the agent
can recover from its own mistakes at inference
time. As a result of combining both techniques, on
the R2R benchmark test set, our agent (PRESS)3

achieves 53% on SPL, an absolute 6% gain over
the current state of the art.

2 Method

In the VLN task, instructions are represented as a
set X = {xi}Mi=1 of M instructions per trajectory.

2To compensate, beam search is often used to improve
success rates. Recent work, e.g., using search strategies (Ke
et al., 2019) or progress monitors (Ma et al., 2019b), has fo-
cused on mitigating the cost of computing top-k rollouts.

3PRETRAINED LMS AND STOCHASTIC SAMPLING

st
<latexit sha1_base64="yZ01YkzdYab6K7wka8OreoshB5Y=">AAAB8HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzEOSEGYns8mQmdllplcIS77CiwdFvPo53vwbJ8keNLGgoajqprsrTKSw6PvfXmFtfWNzq7hd2tnd2z8oHx41bZwaxhsslrFph9RyKTRvoEDJ24nhVIWSt8Lx7cxvPXFjRawfcJLwnqJDLSLBKDrpMeuGEbHTPvbLFb/qz0FWSZCTCuSo98tf3UHMUsU1Mkmt7QR+gr2MGhRM8mmpm1qeUDamQ95xVFPFbS+bHzwlZ04ZkCg2rjSSufp7IqPK2okKXaeiOLLL3kz8z+ukGF33MqGTFLlmi0VRKgnGZPY9GQjDGcqJI5QZ4W4lbEQNZegyKrkQguWXV0nzohr41eD+slK7yeMowgmcwjkEcAU1uIM6NICBgmd4hTfPeC/eu/exaC14+cwx/IH3+QO265BW</latexit><latexit sha1_base64="yZ01YkzdYab6K7wka8OreoshB5Y=">AAAB8HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzEOSEGYns8mQmdllplcIS77CiwdFvPo53vwbJ8keNLGgoajqprsrTKSw6PvfXmFtfWNzq7hd2tnd2z8oHx41bZwaxhsslrFph9RyKTRvoEDJ24nhVIWSt8Lx7cxvPXFjRawfcJLwnqJDLSLBKDrpMeuGEbHTPvbLFb/qz0FWSZCTCuSo98tf3UHMUsU1Mkmt7QR+gr2MGhRM8mmpm1qeUDamQ95xVFPFbS+bHzwlZ04ZkCg2rjSSufp7IqPK2okKXaeiOLLL3kz8z+ukGF33MqGTFLlmi0VRKgnGZPY9GQjDGcqJI5QZ4W4lbEQNZegyKrkQguWXV0nzohr41eD+slK7yeMowgmcwjkEcAU1uIM6NICBgmd4hTfPeC/eu/exaC14+cwx/IH3+QO265BW</latexit><latexit sha1_base64="yZ01YkzdYab6K7wka8OreoshB5Y=">AAAB8HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzEOSEGYns8mQmdllplcIS77CiwdFvPo53vwbJ8keNLGgoajqprsrTKSw6PvfXmFtfWNzq7hd2tnd2z8oHx41bZwaxhsslrFph9RyKTRvoEDJ24nhVIWSt8Lx7cxvPXFjRawfcJLwnqJDLSLBKDrpMeuGEbHTPvbLFb/qz0FWSZCTCuSo98tf3UHMUsU1Mkmt7QR+gr2MGhRM8mmpm1qeUDamQ95xVFPFbS+bHzwlZ04ZkCg2rjSSufp7IqPK2okKXaeiOLLL3kz8z+ukGF33MqGTFLlmi0VRKgnGZPY9GQjDGcqJI5QZ4W4lbEQNZegyKrkQguWXV0nzohr41eD+slK7yeMowgmcwjkEcAU1uIM6NICBgmd4hTfPeC/eu/exaC14+cwx/IH3+QO265BW</latexit><latexit sha1_base64="yZ01YkzdYab6K7wka8OreoshB5Y=">AAAB8HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzEOSEGYns8mQmdllplcIS77CiwdFvPo53vwbJ8keNLGgoajqprsrTKSw6PvfXmFtfWNzq7hd2tnd2z8oHx41bZwaxhsslrFph9RyKTRvoEDJ24nhVIWSt8Lx7cxvPXFjRawfcJLwnqJDLSLBKDrpMeuGEbHTPvbLFb/qz0FWSZCTCuSo98tf3UHMUsU1Mkmt7QR+gr2MGhRM8mmpm1qeUDamQ95xVFPFbS+bHzwlZ04ZkCg2rjSSufp7IqPK2okKXaeiOLLL3kz8z+ukGF33MqGTFLlmi0VRKgnGZPY9GQjDGcqJI5QZ4W4lbEQNZegyKrkQguWXV0nzohr41eD+slK7yeMowgmcwjkEcAU1uIM6NICBgmd4hTfPeC/eu/exaC14+cwx/IH3+QO265BW</latexit>

zt
<latexit sha1_base64="lS9+rv3/YddsZ2cvTXESPXlhaUU=">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBJEoSwt5mL1myu3fszgnxyK+wsVDE1p9j579xk1yhiQ8GHu/NMDMvTKSw6PvfXmFldW19o7hZ2tre2d0r7x80bZwaxhsslrG5D6nlUmjeQIGS3yeGUxVK3gpH11O/9ciNFbG+w3HCu4oOtIgEo+ikh6wTRuRp0sNeueJX/RnIMglyUoEc9V75q9OPWaq4Riapte3AT7CbUYOCST4pdVLLE8pGdMDbjmqquO1ms4Mn5MQpfRLFxpVGMlN/T2RUWTtWoetUFId20ZuK/3ntFKPLbiZ0kiLXbL4oSiXBmEy/J31hOEM5doQyI9ythA2poQxdRiUXQrD48jJpnlUDvxrcnldqV3kcRTiCYziFAC6gBjdQhwYwUPAMr/DmGe/Fe/c+5q0FL585hD/wPn8AwZyQXQ==</latexit><latexit sha1_base64="lS9+rv3/YddsZ2cvTXESPXlhaUU=">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBJEoSwt5mL1myu3fszgnxyK+wsVDE1p9j579xk1yhiQ8GHu/NMDMvTKSw6PvfXmFldW19o7hZ2tre2d0r7x80bZwaxhsslrG5D6nlUmjeQIGS3yeGUxVK3gpH11O/9ciNFbG+w3HCu4oOtIgEo+ikh6wTRuRp0sNeueJX/RnIMglyUoEc9V75q9OPWaq4Riapte3AT7CbUYOCST4pdVLLE8pGdMDbjmqquO1ms4Mn5MQpfRLFxpVGMlN/T2RUWTtWoetUFId20ZuK/3ntFKPLbiZ0kiLXbL4oSiXBmEy/J31hOEM5doQyI9ythA2poQxdRiUXQrD48jJpnlUDvxrcnldqV3kcRTiCYziFAC6gBjdQhwYwUPAMr/DmGe/Fe/c+5q0FL585hD/wPn8AwZyQXQ==</latexit><latexit sha1_base64="lS9+rv3/YddsZ2cvTXESPXlhaUU=">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBJEoSwt5mL1myu3fszgnxyK+wsVDE1p9j579xk1yhiQ8GHu/NMDMvTKSw6PvfXmFldW19o7hZ2tre2d0r7x80bZwaxhsslrG5D6nlUmjeQIGS3yeGUxVK3gpH11O/9ciNFbG+w3HCu4oOtIgEo+ikh6wTRuRp0sNeueJX/RnIMglyUoEc9V75q9OPWaq4Riapte3AT7CbUYOCST4pdVLLE8pGdMDbjmqquO1ms4Mn5MQpfRLFxpVGMlN/T2RUWTtWoetUFId20ZuK/3ntFKPLbiZ0kiLXbL4oSiXBmEy/J31hOEM5doQyI9ythA2poQxdRiUXQrD48jJpnlUDvxrcnldqV3kcRTiCYziFAC6gBjdQhwYwUPAMr/DmGe/Fe/c+5q0FL585hD/wPn8AwZyQXQ==</latexit><latexit sha1_base64="lS9+rv3/YddsZ2cvTXESPXlhaUU=">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBJEoSwt5mL1myu3fszgnxyK+wsVDE1p9j579xk1yhiQ8GHu/NMDMvTKSw6PvfXmFldW19o7hZ2tre2d0r7x80bZwaxhsslrG5D6nlUmjeQIGS3yeGUxVK3gpH11O/9ciNFbG+w3HCu4oOtIgEo+ikh6wTRuRp0sNeueJX/RnIMglyUoEc9V75q9OPWaq4Riapte3AT7CbUYOCST4pdVLLE8pGdMDbjmqquO1ms4Mn5MQpfRLFxpVGMlN/T2RUWTtWoetUFId20ZuK/3ntFKPLbiZ0kiLXbL4oSiXBmEy/J31hOEM5doQyI9ythA2poQxdRiUXQrD48jJpnlUDvxrcnldqV3kcRTiCYziFAC6gBjdQhwYwUPAMr/DmGe/Fe/c+5q0FL585hD/wPn8AwZyQXQ==</latexit>

ci,t
<latexit sha1_base64="G2VWR8giqP77iY8hyFXpkXQctng=">AAAB9HicbVDLSgNBEOyNrxhfUY9eBoPgQcKuCHoMevEYwTwgWcLsZDYZMju7zvQGwrLf4cWDIl79GG/+jZPHQRMLGoqqbrq7gkQKg6777RTW1jc2t4rbpZ3dvf2D8uFR08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupv6rTHXRsTqEScJ9yM6UCIUjKKV/KwbhITlvUxcYN4rV9yqOwNZJd6CVGCBeq/81e3HLI24QiapMR3PTdDPqEbBJM9L3dTwhLIRHfCOpYpG3PjZ7OicnFmlT8JY21JIZurviYxGxkyiwHZGFIdm2ZuK/3mdFMMbPxMqSZErNl8UppJgTKYJkL7QnKGcWEKZFvZWwoZUU4Y2p5INwVt+eZU0L6ueW/Ueriq120UcRTiBUzgHD66hBvdQhwYweIJneIU3Z+y8OO/Ox7y14CxmjuEPnM8fnCOR+w==</latexit><latexit sha1_base64="G2VWR8giqP77iY8hyFXpkXQctng=">AAAB9HicbVDLSgNBEOyNrxhfUY9eBoPgQcKuCHoMevEYwTwgWcLsZDYZMju7zvQGwrLf4cWDIl79GG/+jZPHQRMLGoqqbrq7gkQKg6777RTW1jc2t4rbpZ3dvf2D8uFR08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupv6rTHXRsTqEScJ9yM6UCIUjKKV/KwbhITlvUxcYN4rV9yqOwNZJd6CVGCBeq/81e3HLI24QiapMR3PTdDPqEbBJM9L3dTwhLIRHfCOpYpG3PjZ7OicnFmlT8JY21JIZurviYxGxkyiwHZGFIdm2ZuK/3mdFMMbPxMqSZErNl8UppJgTKYJkL7QnKGcWEKZFvZWwoZUU4Y2p5INwVt+eZU0L6ueW/Ueriq120UcRTiBUzgHD66hBvdQhwYweIJneIU3Z+y8OO/Ox7y14CxmjuEPnM8fnCOR+w==</latexit><latexit sha1_base64="G2VWR8giqP77iY8hyFXpkXQctng=">AAAB9HicbVDLSgNBEOyNrxhfUY9eBoPgQcKuCHoMevEYwTwgWcLsZDYZMju7zvQGwrLf4cWDIl79GG/+jZPHQRMLGoqqbrq7gkQKg6777RTW1jc2t4rbpZ3dvf2D8uFR08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupv6rTHXRsTqEScJ9yM6UCIUjKKV/KwbhITlvUxcYN4rV9yqOwNZJd6CVGCBeq/81e3HLI24QiapMR3PTdDPqEbBJM9L3dTwhLIRHfCOpYpG3PjZ7OicnFmlT8JY21JIZurviYxGxkyiwHZGFIdm2ZuK/3mdFMMbPxMqSZErNl8UppJgTKYJkL7QnKGcWEKZFvZWwoZUU4Y2p5INwVt+eZU0L6ueW/Ueriq120UcRTiBUzgHD66hBvdQhwYweIJneIU3Z+y8OO/Ox7y14CxmjuEPnM8fnCOR+w==</latexit><latexit sha1_base64="G2VWR8giqP77iY8hyFXpkXQctng=">AAAB9HicbVDLSgNBEOyNrxhfUY9eBoPgQcKuCHoMevEYwTwgWcLsZDYZMju7zvQGwrLf4cWDIl79GG/+jZPHQRMLGoqqbrq7gkQKg6777RTW1jc2t4rbpZ3dvf2D8uFR08SpZrzBYhnrdkANl0LxBgqUvJ1oTqNA8lYwupv6rTHXRsTqEScJ9yM6UCIUjKKV/KwbhITlvUxcYN4rV9yqOwNZJd6CVGCBeq/81e3HLI24QiapMR3PTdDPqEbBJM9L3dTwhLIRHfCOpYpG3PjZ7OicnFmlT8JY21JIZurviYxGxkyiwHZGFIdm2ZuK/3mdFMMbPxMqSZErNl8UppJgTKYJkL7QnKGcWEKZFvZWwoZUU4Y2p5INwVt+eZU0L6ueW/Ueriq120UcRTiBUzgHD66hBvdQhwYweIJneIU3Z+y8OO/Ox7y14CxmjuEPnM8fnCOR+w==</latexit>

xi
<latexit sha1_base64="R3kVmaupHwFqDyHV/vQYXaWxTLE=">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBJEoSwt5mL1myu3fszonhyK+wsVDE1p9j579xk1yhiQ8GHu/NMDMvTKSw6PvfXmFldW19o7hZ2tre2d0r7x80bZwaxhsslrG5D6nlUmjeQIGS3yeGUxVK3gpH11O/9ciNFbG+w3HCu4oOtIgEo+ikh6wTRuRp0hO9csWv+jOQZRLkpAI56r3yV6cfs1RxjUxSa9uBn2A3owYFk3xS6qSWJ5SN6IC3HdVUcdvNZgdPyIlT+iSKjSuNZKb+nsiosnasQtepKA7tojcV//PaKUaX3UzoJEWu2XxRlEqCMZl+T/rCcIZy7AhlRrhbCRtSQxm6jEouhGDx5WXSPKsGfjW4Pa/UrvI4inAEx3AKAVxADW6gDg1goOAZXuHNM96L9+59zFsLXj5zCH/gff4AreKQUA==</latexit><latexit sha1_base64="R3kVmaupHwFqDyHV/vQYXaWxTLE=">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBJEoSwt5mL1myu3fszonhyK+wsVDE1p9j579xk1yhiQ8GHu/NMDMvTKSw6PvfXmFldW19o7hZ2tre2d0r7x80bZwaxhsslrG5D6nlUmjeQIGS3yeGUxVK3gpH11O/9ciNFbG+w3HCu4oOtIgEo+ikh6wTRuRp0hO9csWv+jOQZRLkpAI56r3yV6cfs1RxjUxSa9uBn2A3owYFk3xS6qSWJ5SN6IC3HdVUcdvNZgdPyIlT+iSKjSuNZKb+nsiosnasQtepKA7tojcV//PaKUaX3UzoJEWu2XxRlEqCMZl+T/rCcIZy7AhlRrhbCRtSQxm6jEouhGDx5WXSPKsGfjW4Pa/UrvI4inAEx3AKAVxADW6gDg1goOAZXuHNM96L9+59zFsLXj5zCH/gff4AreKQUA==</latexit><latexit sha1_base64="R3kVmaupHwFqDyHV/vQYXaWxTLE=">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBJEoSwt5mL1myu3fszonhyK+wsVDE1p9j579xk1yhiQ8GHu/NMDMvTKSw6PvfXmFldW19o7hZ2tre2d0r7x80bZwaxhsslrG5D6nlUmjeQIGS3yeGUxVK3gpH11O/9ciNFbG+w3HCu4oOtIgEo+ikh6wTRuRp0hO9csWv+jOQZRLkpAI56r3yV6cfs1RxjUxSa9uBn2A3owYFk3xS6qSWJ5SN6IC3HdVUcdvNZgdPyIlT+iSKjSuNZKb+nsiosnasQtepKA7tojcV//PaKUaX3UzoJEWu2XxRlEqCMZl+T/rCcIZy7AhlRrhbCRtSQxm6jEouhGDx5WXSPKsGfjW4Pa/UrvI4inAEx3AKAVxADW6gDg1goOAZXuHNM96L9+59zFsLXj5zCH/gff4AreKQUA==</latexit><latexit sha1_base64="R3kVmaupHwFqDyHV/vQYXaWxTLE=">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0DJoYxnBJEoSwt5mL1myu3fszonhyK+wsVDE1p9j579xk1yhiQ8GHu/NMDMvTKSw6PvfXmFldW19o7hZ2tre2d0r7x80bZwaxhsslrG5D6nlUmjeQIGS3yeGUxVK3gpH11O/9ciNFbG+w3HCu4oOtIgEo+ikh6wTRuRp0hO9csWv+jOQZRLkpAI56r3yV6cfs1RxjUxSa9uBn2A3owYFk3xS6qSWJ5SN6IC3HdVUcdvNZgdPyIlT+iSKjSuNZKb+nsiosnasQtepKA7tojcV//PaKUaX3UzoJEWu2XxRlEqCMZl+T/rCcIZy7AhlRrhbCRtSQxm6jEouhGDx5WXSPKsGfjW4Pa/UrvI4inAEx3AKAVxADW6gDg1goOAZXuHNM96L9+59zFsLXj5zCH/gff4AreKQUA==</latexit>

at
<latexit sha1_base64="Ohy8Py/VNTU4MVsPLCKX4O681IU=">AAAB8HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzEOSEGYns8mQmdllplcIS77CiwdFvPo53vwbJ8keNLGgoajqprsrTKSw6PvfXmFtfWNzq7hd2tnd2z8oHx41bZwaxhsslrFph9RyKTRvoEDJ24nhVIWSt8Lx7cxvPXFjRawfcJLwnqJDLSLBKDrpMeuGEaHTPvbLFb/qz0FWSZCTCuSo98tf3UHMUsU1Mkmt7QR+gr2MGhRM8mmpm1qeUDamQ95xVFPFbS+bHzwlZ04ZkCg2rjSSufp7IqPK2okKXaeiOLLL3kz8z+ukGF33MqGTFLlmi0VRKgnGZPY9GQjDGcqJI5QZ4W4lbEQNZegyKrkQguWXV0nzohr41eD+slK7yeMowgmcwjkEcAU1uIM6NICBgmd4hTfPeC/eu/exaC14+cwx/IH3+QObbZBE</latexit><latexit sha1_base64="Ohy8Py/VNTU4MVsPLCKX4O681IU=">AAAB8HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzEOSEGYns8mQmdllplcIS77CiwdFvPo53vwbJ8keNLGgoajqprsrTKSw6PvfXmFtfWNzq7hd2tnd2z8oHx41bZwaxhsslrFph9RyKTRvoEDJ24nhVIWSt8Lx7cxvPXFjRawfcJLwnqJDLSLBKDrpMeuGEaHTPvbLFb/qz0FWSZCTCuSo98tf3UHMUsU1Mkmt7QR+gr2MGhRM8mmpm1qeUDamQ95xVFPFbS+bHzwlZ04ZkCg2rjSSufp7IqPK2okKXaeiOLLL3kz8z+ukGF33MqGTFLlmi0VRKgnGZPY9GQjDGcqJI5QZ4W4lbEQNZegyKrkQguWXV0nzohr41eD+slK7yeMowgmcwjkEcAU1uIM6NICBgmd4hTfPeC/eu/exaC14+cwx/IH3+QObbZBE</latexit><latexit sha1_base64="Ohy8Py/VNTU4MVsPLCKX4O681IU=">AAAB8HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzEOSEGYns8mQmdllplcIS77CiwdFvPo53vwbJ8keNLGgoajqprsrTKSw6PvfXmFtfWNzq7hd2tnd2z8oHx41bZwaxhsslrFph9RyKTRvoEDJ24nhVIWSt8Lx7cxvPXFjRawfcJLwnqJDLSLBKDrpMeuGEaHTPvbLFb/qz0FWSZCTCuSo98tf3UHMUsU1Mkmt7QR+gr2MGhRM8mmpm1qeUDamQ95xVFPFbS+bHzwlZ04ZkCg2rjSSufp7IqPK2okKXaeiOLLL3kz8z+ukGF33MqGTFLlmi0VRKgnGZPY9GQjDGcqJI5QZ4W4lbEQNZegyKrkQguWXV0nzohr41eD+slK7yeMowgmcwjkEcAU1uIM6NICBgmd4hTfPeC/eu/exaC14+cwx/IH3+QObbZBE</latexit><latexit sha1_base64="Ohy8Py/VNTU4MVsPLCKX4O681IU=">AAAB8HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMegF48RzEOSEGYns8mQmdllplcIS77CiwdFvPo53vwbJ8keNLGgoajqprsrTKSw6PvfXmFtfWNzq7hd2tnd2z8oHx41bZwaxhsslrFph9RyKTRvoEDJ24nhVIWSt8Lx7cxvPXFjRawfcJLwnqJDLSLBKDrpMeuGEaHTPvbLFb/qz0FWSZCTCuSo98tf3UHMUsU1Mkmt7QR+gr2MGhRM8mmpm1qeUDamQ95xVFPFbS+bHzwlZ04ZkCg2rjSSufp7IqPK2okKXaeiOLLL3kz8z+ukGF33MqGTFLlmi0VRKgnGZPY9GQjDGcqJI5QZ4W4lbEQNZegyKrkQguWXV0nzohr41eD+slK7yeMowgmcwjkEcAU1uIM6NICBgmd4hTfPeC/eu/exaC14+cwx/IH3+QObbZBE</latexit>

ActionVisual Feature

Language
Instructions

Student Action

g(·)
<latexit sha1_base64="LV4YcFOcojap3BT6BSXOasQ8eR8=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWsB/QhrLZbNqlm03cnQil9E948aCIV/+ON/+N2zYHbX0w8Hhvhpl5QSqFQdf9dgpr6xubW8Xt0s7u3v5B+fCoZZJMM95kiUx0J6CGS6F4EwVK3kk1p3EgeTsY3c789hPXRiTqAccp92M6UCISjKKVOoNqj4UJnvfLFbfmzkFWiZeTCuRo9MtfvTBhWcwVMkmN6Xpuiv6EahRM8mmplxmeUjaiA961VNGYG38yv3dKzqwSkijRthSSufp7YkJjY8ZxYDtjikOz7M3E/7xuhtG1PxEqzZArtlgUZZJgQmbPk1BozlCOLaFMC3srYUOqKUMbUcmG4C2/vEpaFzXPrXn3l5X6TR5HEU7gFKrgwRXU4Q4a0AQGEp7hFd6cR+fFeXc+Fq0FJ585hj9wPn8AZJmPiA==</latexit><latexit sha1_base64="LV4YcFOcojap3BT6BSXOasQ8eR8=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWsB/QhrLZbNqlm03cnQil9E948aCIV/+ON/+N2zYHbX0w8Hhvhpl5QSqFQdf9dgpr6xubW8Xt0s7u3v5B+fCoZZJMM95kiUx0J6CGS6F4EwVK3kk1p3EgeTsY3c789hPXRiTqAccp92M6UCISjKKVOoNqj4UJnvfLFbfmzkFWiZeTCuRo9MtfvTBhWcwVMkmN6Xpuiv6EahRM8mmplxmeUjaiA961VNGYG38yv3dKzqwSkijRthSSufp7YkJjY8ZxYDtjikOz7M3E/7xuhtG1PxEqzZArtlgUZZJgQmbPk1BozlCOLaFMC3srYUOqKUMbUcmG4C2/vEpaFzXPrXn3l5X6TR5HEU7gFKrgwRXU4Q4a0AQGEp7hFd6cR+fFeXc+Fq0FJ585hj9wPn8AZJmPiA==</latexit><latexit sha1_base64="LV4YcFOcojap3BT6BSXOasQ8eR8=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWsB/QhrLZbNqlm03cnQil9E948aCIV/+ON/+N2zYHbX0w8Hhvhpl5QSqFQdf9dgpr6xubW8Xt0s7u3v5B+fCoZZJMM95kiUx0J6CGS6F4EwVK3kk1p3EgeTsY3c789hPXRiTqAccp92M6UCISjKKVOoNqj4UJnvfLFbfmzkFWiZeTCuRo9MtfvTBhWcwVMkmN6Xpuiv6EahRM8mmplxmeUjaiA961VNGYG38yv3dKzqwSkijRthSSufp7YkJjY8ZxYDtjikOz7M3E/7xuhtG1PxEqzZArtlgUZZJgQmbPk1BozlCOLaFMC3srYUOqKUMbUcmG4C2/vEpaFzXPrXn3l5X6TR5HEU7gFKrgwRXU4Q4a0AQGEp7hFd6cR+fFeXc+Fq0FJ585hj9wPn8AZJmPiA==</latexit><latexit sha1_base64="LV4YcFOcojap3BT6BSXOasQ8eR8=">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjxWsB/QhrLZbNqlm03cnQil9E948aCIV/+ON/+N2zYHbX0w8Hhvhpl5QSqFQdf9dgpr6xubW8Xt0s7u3v5B+fCoZZJMM95kiUx0J6CGS6F4EwVK3kk1p3EgeTsY3c789hPXRiTqAccp92M6UCISjKKVOoNqj4UJnvfLFbfmzkFWiZeTCuRo9MtfvTBhWcwVMkmN6Xpuiv6EahRM8mmplxmeUjaiA961VNGYG38yv3dKzqwSkijRthSSufp7YkJjY8ZxYDtjikOz7M3E/7xuhtG1PxEqzZArtlgUZZJgQmbPk1BozlCOLaFMC3srYUOqKUMbUcmG4C2/vEpaFzXPrXn3l5X6TR5HEU7gFKrgwRXU4Q4a0AQGEp7hFd6cR+fFeXc+Fq0FJ585hj9wPn8AZJmPiA==</latexit>

Stochastic Action Sampling

Pre5trained Language Models

Teacher Action aT
<latexit sha1_base64="YNuN9YRP1nVP2fYvNfiNQkhUtL4=">AAAB/HicbVDLSsNAFJ34rPUV7dLNYBFclUQEXRbduKzQF7SxTKaTduhkEmZuhBDir7hxoYhbP8Sdf+OkzUJbDwwczrmXe+b4seAaHOfbWlvf2NzaruxUd/f2Dw7to+OujhJFWYdGIlJ9n2gmuGQd4CBYP1aMhL5gPX92W/i9R6Y0j2Qb0ph5IZlIHnBKwEgjuzb0g4zkD9kwJDAFyNp5PrLrTsOZA68StyR1VKI1sr+G44gmIZNABdF64DoxeBlRwKlgeXWYaBYTOiMTNjBUkpBpL5uHz/GZUcY4iJR5EvBc/b2RkVDrNPTNZBFRL3uF+J83SCC49jIu4wSYpItDQSIwRLhoAo+5YhREagihipusmE6JIhRMX1VTgrv85VXSvWi4TsO9v6w3b8o6KugEnaJz5KIr1ER3qIU6iKIUPaNX9GY9WS/Wu/WxGF2zyp0a+gPr8we8+5V2</latexit><latexit sha1_base64="YNuN9YRP1nVP2fYvNfiNQkhUtL4=">AAAB/HicbVDLSsNAFJ34rPUV7dLNYBFclUQEXRbduKzQF7SxTKaTduhkEmZuhBDir7hxoYhbP8Sdf+OkzUJbDwwczrmXe+b4seAaHOfbWlvf2NzaruxUd/f2Dw7to+OujhJFWYdGIlJ9n2gmuGQd4CBYP1aMhL5gPX92W/i9R6Y0j2Qb0ph5IZlIHnBKwEgjuzb0g4zkD9kwJDAFyNp5PrLrTsOZA68StyR1VKI1sr+G44gmIZNABdF64DoxeBlRwKlgeXWYaBYTOiMTNjBUkpBpL5uHz/GZUcY4iJR5EvBc/b2RkVDrNPTNZBFRL3uF+J83SCC49jIu4wSYpItDQSIwRLhoAo+5YhREagihipusmE6JIhRMX1VTgrv85VXSvWi4TsO9v6w3b8o6KugEnaJz5KIr1ER3qIU6iKIUPaNX9GY9WS/Wu/WxGF2zyp0a+gPr8we8+5V2</latexit><latexit sha1_base64="YNuN9YRP1nVP2fYvNfiNQkhUtL4=">AAAB/HicbVDLSsNAFJ34rPUV7dLNYBFclUQEXRbduKzQF7SxTKaTduhkEmZuhBDir7hxoYhbP8Sdf+OkzUJbDwwczrmXe+b4seAaHOfbWlvf2NzaruxUd/f2Dw7to+OujhJFWYdGIlJ9n2gmuGQd4CBYP1aMhL5gPX92W/i9R6Y0j2Qb0ph5IZlIHnBKwEgjuzb0g4zkD9kwJDAFyNp5PrLrTsOZA68StyR1VKI1sr+G44gmIZNABdF64DoxeBlRwKlgeXWYaBYTOiMTNjBUkpBpL5uHz/GZUcY4iJR5EvBc/b2RkVDrNPTNZBFRL3uF+J83SCC49jIu4wSYpItDQSIwRLhoAo+5YhREagihipusmE6JIhRMX1VTgrv85VXSvWi4TsO9v6w3b8o6KugEnaJz5KIr1ER3qIU6iKIUPaNX9GY9WS/Wu/WxGF2zyp0a+gPr8we8+5V2</latexit><latexit sha1_base64="YNuN9YRP1nVP2fYvNfiNQkhUtL4=">AAAB/HicbVDLSsNAFJ34rPUV7dLNYBFclUQEXRbduKzQF7SxTKaTduhkEmZuhBDir7hxoYhbP8Sdf+OkzUJbDwwczrmXe+b4seAaHOfbWlvf2NzaruxUd/f2Dw7to+OujhJFWYdGIlJ9n2gmuGQd4CBYP1aMhL5gPX92W/i9R6Y0j2Qb0ph5IZlIHnBKwEgjuzb0g4zkD9kwJDAFyNp5PrLrTsOZA68StyR1VKI1sr+G44gmIZNABdF64DoxeBlRwKlgeXWYaBYTOiMTNjBUkpBpL5uHz/GZUcY4iJR5EvBc/b2RkVDrNPTNZBFRL3uF+J83SCC49jIu4wSYpItDQSIwRLhoAo+5YhREagihipusmE6JIhRMX1VTgrv85VXSvWi4TsO9v6w3b8o6KugEnaJz5KIr1ER3qIU6iKIUPaNX9GY9WS/Wu/WxGF2zyp0a+gPr8we8+5V2</latexit>

aS
<latexit sha1_base64="5NKqRoYCuZBdM4ZCa1QaypRL+rI=">AAAB/HicbVDLSsNAFJ34rPUV7dLNYBFclUQEXRbduKxoH9DGMplO2qGTSZi5EUKIv+LGhSJu/RB3/o2TNgttPTBwOOde7pnjx4JrcJxva2V1bX1js7JV3d7Z3du3Dw47OkoUZW0aiUj1fKKZ4JK1gYNgvVgxEvqCdf3pdeF3H5nSPJL3kMbMC8lY8oBTAkYa2rWBH2Qkf8gGIYEJQHaX50O77jScGfAycUtSRyVaQ/trMIpoEjIJVBCt+64Tg5cRBZwKllcHiWYxoVMyZn1DJQmZ9rJZ+ByfGGWEg0iZJwHP1N8bGQm1TkPfTBYR9aJXiP95/QSCSy/jMk6ASTo/FCQCQ4SLJvCIK0ZBpIYQqrjJiumEKELB9FU1JbiLX14mnbOG6zTc2/N686qso4KO0DE6RS66QE10g1qojShK0TN6RW/Wk/VivVsf89EVq9ypoT+wPn8Au3WVdQ==</latexit><latexit sha1_base64="5NKqRoYCuZBdM4ZCa1QaypRL+rI=">AAAB/HicbVDLSsNAFJ34rPUV7dLNYBFclUQEXRbduKxoH9DGMplO2qGTSZi5EUKIv+LGhSJu/RB3/o2TNgttPTBwOOde7pnjx4JrcJxva2V1bX1js7JV3d7Z3du3Dw47OkoUZW0aiUj1fKKZ4JK1gYNgvVgxEvqCdf3pdeF3H5nSPJL3kMbMC8lY8oBTAkYa2rWBH2Qkf8gGIYEJQHaX50O77jScGfAycUtSRyVaQ/trMIpoEjIJVBCt+64Tg5cRBZwKllcHiWYxoVMyZn1DJQmZ9rJZ+ByfGGWEg0iZJwHP1N8bGQm1TkPfTBYR9aJXiP95/QSCSy/jMk6ASTo/FCQCQ4SLJvCIK0ZBpIYQqrjJiumEKELB9FU1JbiLX14mnbOG6zTc2/N686qso4KO0DE6RS66QE10g1qojShK0TN6RW/Wk/VivVsf89EVq9ypoT+wPn8Au3WVdQ==</latexit><latexit sha1_base64="5NKqRoYCuZBdM4ZCa1QaypRL+rI=">AAAB/HicbVDLSsNAFJ34rPUV7dLNYBFclUQEXRbduKxoH9DGMplO2qGTSZi5EUKIv+LGhSJu/RB3/o2TNgttPTBwOOde7pnjx4JrcJxva2V1bX1js7JV3d7Z3du3Dw47OkoUZW0aiUj1fKKZ4JK1gYNgvVgxEvqCdf3pdeF3H5nSPJL3kMbMC8lY8oBTAkYa2rWBH2Qkf8gGIYEJQHaX50O77jScGfAycUtSRyVaQ/trMIpoEjIJVBCt+64Tg5cRBZwKllcHiWYxoVMyZn1DJQmZ9rJZ+ByfGGWEg0iZJwHP1N8bGQm1TkPfTBYR9aJXiP95/QSCSy/jMk6ASTo/FCQCQ4SLJvCIK0ZBpIYQqrjJiumEKELB9FU1JbiLX14mnbOG6zTc2/N686qso4KO0DE6RS66QE10g1qojShK0TN6RW/Wk/VivVsf89EVq9ypoT+wPn8Au3WVdQ==</latexit><latexit sha1_base64="5NKqRoYCuZBdM4ZCa1QaypRL+rI=">AAAB/HicbVDLSsNAFJ34rPUV7dLNYBFclUQEXRbduKxoH9DGMplO2qGTSZi5EUKIv+LGhSJu/RB3/o2TNgttPTBwOOde7pnjx4JrcJxva2V1bX1js7JV3d7Z3du3Dw47OkoUZW0aiUj1fKKZ4JK1gYNgvVgxEvqCdf3pdeF3H5nSPJL3kMbMC8lY8oBTAkYa2rWBH2Qkf8gGIYEJQHaX50O77jScGfAycUtSRyVaQ/trMIpoEjIJVBCt+64Tg5cRBZwKllcHiWYxoVMyZn1DJQmZ9rJZ+ByfGGWEg0iZJwHP1N8bGQm1TkPfTBYR9aJXiP95/QSCSy/jMk6ASTo/FCQCQ4SLJvCIK0ZBpIYQqrjJiumEKELB9FU1JbiLX14mnbOG6zTc2/N686qso4KO0DE6RS66QE10g1qojShK0TN6RW/Wk/VivVsf89EVq9ypoT+wPn8Au3WVdQ==</latexit>

Figure 2: Illustration of proposed methods.

Each instruction xi is a sequence of Li words,
xi = [xi,1, xi,2, ..., xi,Li ]. Given X , the goal is
to train an agent to navigate from a starting posi-
tion s0 to a target position, via completing a T -
step trajectory τ = [s0,a0, s1,a1, · · · , sT ,aT ],
where st and at are the visual state and naviga-
tion action, respectively, at step t. The training
datasetDE = {τ ,X} consists of example pairs of
instruction set X and a corresponding expert tra-
jectory τ . Our goal is to learn a policy πθ(τ |X )
that maximizes the log-likelihood of the target tra-
jectory τ given instructions X :

log πθ(τ |X ) =
T∑
t=1

log πθ(at|st,X ), (1)

where θ are trainable parameters. The policy
is usually parameterized as an attention-based
seq2seq model, with a language encoder zt =
fθE (x), and an action decoder at = fθD(zt, st).
Successful navigation depends on (i) precisely
grounding the instructions X in τ in various envi-
ronments, and (ii) correctly making the current de-
cision at based on previous actions/observations
τ<t = [s0,a0, · · · , st−1]. To address these con-
cerns, we propose PRESS, illustrated in Figure 2.

2.1 Instruction Understanding with
Pretrained Language Models

At each step t, the agent decides where to navi-
gate by updating a dynamic understanding of the
instructions zt, according to its current visual state
st. Given instruction x, the language encoder pro-
ceeds in two steps, end-to-end, by considering a
function decomposition fθE = fθx→e ◦ fθe→z :

• fθx→e : x → e, where x = [x1, · · · , xL] is
represented as its (contextualized) word em-
bedding form e = [e1, · · · , eL], with ei as the
representation for word xi;

• fθe→z : e→ zt: For each embedded instruc-
tion e, we ground its representations as ci,t
for state st via neural attention. To handle



1496

language variability, one may aggregate fea-
tures of multiple instructions Ct = {ci,t}Mi=1
into a single joint feature zt = 1M

∑M
i=1 ci,t.

4

Previous methods in VLN learn e either from
pretrained word embeddings (Pennington et al.,
2014) which do not take into account word con-
text, or from scratch. As a result, their repre-
sentations do not capture contextual information
within each instruction. More importantly, they
tend to overfit the training instructions associated
with seen environments, limiting their utility in
unseen environments. To remedy these issues, we
propose to represent e with contextualized word
embeddings produced using large-scale pretrained
language models, such as BERT and GPT.

Instruction Encoder. The agent’s memory vec-
tor ht−1 captures the perception and action his-
tory and is used to attend to the instruction x.
A pretrained LM fθx→e encodes the instruction
e = [e1, · · · , eL]; ei where the representation for
word xi, is built with fθx→e ∈ { GPT, BERT },
and θx→e are fine-tuned parameters. The embed-
ded words e = [e1, · · · , eL] are passed through an
LSTM fθe→z to produce a sequence of textual fea-
tures [he1, · · · ,heL]. At each time step t, the tex-
tual context for the instruction x is computed as
weighted sum of textual features in the sequence:

ci,t =

L∑
l=1

αlh
e
l (2)

where αl = Softmax(h>t h
e
l ), αl places more

weight on the word representations that are most
relevant to the agent’s current status.

Decoder. At each step, the agent takes an action
at, and the environment returns new visual obser-
vations; the agent first performs one-hop visual
attention f(·) to all the visual image features st,
based on its previous memory vector ht−1. Then,
the agent updates its visual state st as the weighted
sum of the panoramic features, st =

∑
j γt,jst,j .

The attention weight γt,j for the j-th visual feature
st,j represents its importance with respect to the
previous history context ht−1, computed as γt,j =
Softmax((Whht−1)>Wsst,j) (Fried et al., 2018)
where Softmax(rj) = exp(rj)/

∑
j′ exp(rj′),

4This recovers zt = ct when only a single instruction is
available.

Wh and Ws are trainable projection matrices.

ht = fθD([st,at−1],ht−1) (3)

where at−1 is the action taken at previous step,
and θD are the LSTM decoder parameters.

Two-stage learning. The parameters of our
agent are θ = {θx→e,θe→z, θD}. In practice,
we find that the agent overfits quickly, when the
full model is naively fine-tuned, with θx→e ini-
tialized by pretrained LMs (e.g., BERT). In this
paper, we consider a two-stage learning scheme
to facilitate the use of pretrained LMs for VLN.
(i) Embedding-based stage: We fix θx→e, and use
BERT or GPT to provide instruction embeddings.
Only {θe→z,θD} are updated (while tuning on
validation). (ii) Fine-tuning stage: We train all
model parameters θ with a smaller learning rate,
so that θx→e can adapt to our VLN task.

2.2 Stochastic Action Sampling
A core question is how to learn useful state repre-
sentations st in Eq. (1) during the trajectory roll-
out. In other words, which action should we use
to interact with the environment to elicit the next
state? As noted, most existing work uses one of
two schemes: (i) Teacher-forcing (TF), where the
agent takes ground-truth actions aT only. Though
TF enables efficient training, it results in “ex-
posure bias” because agents must follow learned
rather than gold trajectories at test time. In con-
trast, (ii) Student-forcing (SF), where an action aS

is drawn from the current learned policy, allows
the agent to learn from its own actions (aligning
training and evaluation), however, it is inefficient,
as the agent explores randomly when confused or
in the early stages of training.

In this work, we consider a stochastic scheme
(SS) to alternate between choosing actions from aT

and aS for state transition s← g(aT,aS), inspired
by scheduled sampling (Bengio et al., 2015). As
illustrated in Figure 2, at each step, the agent “flips
a coin” with some probability � to decide whether
to take the teacher’s action aT or a sampled one aS:

a = δaT + (1− δ)aS, (4)

where δ ∼ Bernoulli(�). This allows the agent to
leverage the advantages of both TF and SF, yield-
ing a faster and less biased learner. We fix � as a
constant during learning, which is different from
the decaying schedule in (Bengio et al., 2015).



1497

3 Experiments

3.1 Dataset

We use the Room-to-Room dataset for the VLN
task, built upon the Matterport3D dataset (Chang
et al., 2017), which consists of 10,800 panoramic
views and 7,189 trajectories. Each trajectory is
paired with three natural language instructions.
The R2R dataset consists of four splits: train seen,
validation seen, validation unseen, and test un-
seen. There is no overlap between seen and unseen
environments. At the beginning of each episode,
the agent starts at a specific location, and is given
natural instructions, the goal of the agent is to nav-
igate to the target location as quickly as possible.

3.2 Baseline Systems

We compare our approach with eight recently pub-
lished systems:
• RANDOM: an agent that randomly selects a

direction and moves five step in that direction
(Anderson et al., 2018).
• SEQ2SEQ: sequence-to-sequence model pro-

posed by Anderson et al. as a baseline for the
R2R benchmark (Anderson et al., 2018) and
analyzed in (Thomason et al., 2019).
• RPA (Wang et al., 2018): is an agent which

combines model-free and model-based rein-
forcement learning, using a look-ahead mod-
ule for planning.
• SPEAKER-FOLLOWER (Fried et al., 2018):

an agent trained with data augmentation from
a speaker model with panoramic actions.
• SMNA (Ma et al., 2019a): an agent trained

with a visual-textual co-grounding module
and progress monitor on panoramic actions.
• RCM+SIL(TRAIN) (Wang et al., 2019): an

agent trained with cross-modal grounding lo-
cally and globally via reinforcement learning.
• REGRETFUL (Ma et al., 2019b): an agent

with a trained progress monitor heuristic for
search that enables backtracking.
• FAST (Ke et al., 2019): an agent which com-

bines global and local knowledge to compare
partial trajectories of different lengths, en-
abling efficient backtrack after a mistake.
• ENVDROP (Tan et al., 2019): proposed an

environment dropout method, which can gen-
erate more environments based on the limited
seen environments.

Validation Seen Validation Unseen
Setting Agent SR ↑ SPL ↑ SR ↑ SPL ↑

S seq2seq 51 46 32 25PRESS 47 (-4) 43 (-3) 43 (+11) 38 (+13)

M seq2seq 49 44 33 26PRESS 56 (+7) 53 (+9) 56 (+23) 50 (+24)

Table 2: Comparison of PRESS and seq2seq.

3.3 Evaluation Metrics

We benchmark our agent on the following metrics:

TL Trajectory Length measures the average
length of the navigation trajectory.

NE Navigation Error is the mean of the shortest
path distance in meters between the agent’s
final location and the target location.

SR Success Rate with which the agent’s final lo-
cation is less than 3 meters from the target.

SPL Success weighted by Path Length trades-off
SR against TL.

SPL is the recommended primary metric, other
metrics are considered as auxiliary measures.

3.4 Implementation

We use a LSTM/GPT/BERT for the language en-
coder, and a second single-layer LSTM for the ac-
tion decoder (h=1024). We use Adamax and batch
sizes of 24/16 for pretraining/finetuning. The
learning rates for MLE are 1e−4, during finetuning
BERT the learning rate is 5e−5. Following (Fried
et al., 2018), we use a panoramic action space and
the ResNet image features provided by (Anderson
et al., 2018). The code is publicly available here:
https://github.com/xjli/r2r_vln.

3.5 Results

Robust Generalization. First, we compare
PRESS to a baseline seq2seq model5 in two eval-
uation settings on the validation splits: (1) S: A
single instruction is provided to the agent at a
time. Thus, three separate navigation trajectories
are generated corresponding to three alternative
instructions in this setting. We report the averaged
performance over three separate runs. (2) M: All
three instructions are provided to the agent at once.
The seq2seq baseline does not have an aggrega-
tion strategy so we report its performance for the
single trajectory with maximum likelihood. For
PRESS, we aggregate the instructions via context

5The baseline seq2seq agent is the FOLLOWER of
SPEAKER-FOLLOWER (Fried et al., 2018).



1498

Validation Seen Validation Unseen Test Unseen
Model TL ↓ NE ↓ SR ↑ SPL ↑ TL ↓ NE ↓ SR ↑ SPL ↑ TL ↓ NE ↓ SR ↑ SPL ↑

RANDOM 9.58 9.45 16 - 9.77 9.23 16 - 9.93 9.77 13 12
SEQ2SEQ 11.33 6.01 39 - 8.39 7.81 22 - 8.13 7.85 20 18
RPA - 5.56 43 - - 7.65 25 - 9.15 7.53 25 23

G
re

ed
y SPEAKER-FOLLOWER - 3.36 66 - - 6.62 35 - 14.82 6.62 35 28

SMNA - - - - - - - - 18.04 5.67 48 35
RCM+SIL(TRAIN) 10.65 3.53 67 - 11.46 6.09 43 - 11.97 6.12 43 38
REGRETFUL - 3.23 69 63 - 5.32 50 41 13.69 5.69 48 40
FAST - - - - 21.17 4.97 56 43 22.08 5.14 54 41
ENVDROP 11.00 3.99 62 59 10.70 5.22 52 48 11.66 5.23 51 47
PRESS 10.35 3.09 71 67 10.06 4.31 59 55 10.52 4.53 57 53

Human - - - - - - - - 11.85 1.61 86 76

Table 3: Comparison with the state-of-the-art methods. Blue indicates best value overall.

mean-pooling and generate a single trajectory. No
data augmentation is applied to either model.

The results are summarized in Table 2. (i)
PRESS drastically outperforms the seq2seq mod-
els on unseen environments in both settings,
and (ii) Interestingly, our method shows a much
smaller gap between seen and unseen environ-
ments than seq2seq. It demonstrates the impor-
tance of pretrained LMs and stochastic sampling
for strong generalization in unseen environments.

Comparison with SoTA. In Table 3, we com-
pare the performance of our agent against all the
published methods, our PRESS agent outperforms
the existing models on nearly all the metrics.

Ablation Analysis. Key to this work is leverag-
ing large-scale pretrained LMs and effective train-
ing strategies for action sequence decoding. Ta-
ble 4 shows an ablation of these choices. (1)
BERT and GPT are better than LSTM on both
seen and unseen environments, and BERT gener-
alizes better than GPT on unseen environments.
(2) Teacher-forcing performs better than student-
forcing on validation unseen environments, while
an opposite conclusion is drawn on validation seen
environments. SS performs the best on unseen en-
vironments.

Qualitative Examples. We provide two naviga-
tion examples of PRESS on the validation unseen
environments with the step-by-step views and top-
down views in Appendix.

(1) Figure 3 shows how the agent with LSTM
instruction encoder performs compared with our
PRESS agent. There are two rare words “man-
nequins” and “manikins” which are not in the
training dataset and confuse the LSTM agent,
while, PRESS successfully maps these two “man-
nequins” and “manikins” to the correct objects.

Validation Seen Validation Unseen
LM TL NE SR SPL TL NE SR SPL

L
ST

M TF 10.50 5.74 44 42 9.86 6.23 42 39
SF 11.87 3.97 59 53 13.23 6.17 40 31
SS 10.99 3.46 64 59 10.73 4.89 53 48

G
PT

TF 10.03 4.05 60 58 9.43 3.36 49 46
SF 11.46 2.53 73 67 13.13 5.13 49 41
SS 10.60 2.99 71 68 10.79 3.05 56 51

B
E

R
T TF 10.57 4.06 59 56 9.61 5.13 51 47

SF 12.39 2.71 73 64 13.12 5.06 51 42
SS 10.35 3.09 71 67 10.06 4.31 59 55

Table 4: Ablation results of different language pretrain-
ings and training strategies: Teacher Forcing (TF), Stu-
dent Forcing (SF) and Stochastic Sampling (SS).

(2) The second set in Figure 4 shows how the
agents trained with different training strategies
performs in an unseen environment. The agents
trained with teacher-forcing and student-forcing
both fail, while PRESS succeeds.

4 Conclusion

We present PRESS, a navigation agent based on
two previously underexplored techniques in VLN:
pretrained language models and stochastic action
sampling. Our PRESS demonstrates robust gener-
alization in the unseen environments, leading to a
new state-of-the-art performance over many of the
much more complex approaches previously pro-
posed. As both the components of PRESS can
be easily integrated, future models can consider
building upon them as a strong baseline system.

Acknowledgments

We thank the anonymous reviewers for their in-
sightful comments, NSF IIS-1703166, DARPA’s
CwC program through ARO W911NF-15-1-0543,
and the Allen Institute for Artificial Intelligence.



1499

References
Peter Anderson, Qi Wu, Damien Teney, Jake Bruce,

Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen
Gould, and Anton van den Hengel. 2018. Vision-
and-language navigation: Interpreting visually-
grounded navigation instructions in real environ-
ments. In IEEE Conference on Computer Vision and
Pattern Recognition.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks.
In Neural Information Processing Systems.

Angel Chang, Angela Dai, Thomas Funkhouser, Ma-
ciej Halber, Matthias Nießner, Manolis Savva, Shu-
ran Song, Andy Zeng, and Yinda Zhang. 2017. Mat-
terport3D: Learning from RGB-D data in indoor en-
vironments. In International Conference on 3D Vi-
sion.

Howard Chen, Alane Shur, Dipendra Misra, Noah
Snavely, and Yoav Artzi. 2019. Touchdown: Natural
language navigation and spatial reasoning in visual
street environments. In IEEE Conference on Com-
puter Vision and Pattern Recognition.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. In Conference on
Empirical Methods in Natural Language Process-
ing.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies.

Daniel Fried, Ronghang Hu, Volkan Cirik, Anna
Rohrbach, Jacob Andreas, Louis-Philippe Morency,
Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,
and Trevor Darrell. 2018. Speaker-follower models
for vision-and-language navigation. In Neural In-
formation Processing Systems.

Sachithra Hemachandra, Felix Duvallet, Thomas M
Howard, Nicholas Roy, Anthony Stentz, and
Matthew R Walter. 2015. Learning models for fol-
lowing natural language directions in unknown en-
vironments. In IEEE International Conference on
Robotics and Automation.

Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtz-
man, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin
Choi, and Siddhartha Srinivasa. 2019. Tactical
rewind: Self-correction via backtracking in vision-
and-language navigation. In IEEE Conference on
Computer Vision and Pattern Recognition.

Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Al-
Regib, Zsolt Kira, Richard Socher, and Caiming

Xiong. 2019a. Self-monitoring navigation agent via
auxiliary progress estimation. In International Con-
ference on Learning Representations.

Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming
Xiong, and Zsolt Kira. 2019b. The regretful agent:
Heuristic-aided navigation through progress estima-
tion. In IEEE Conference on Computer Vision and
Pattern Recognition.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Conference on empirical methods
in natural language processing.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
ing with recurrent neural networks. In International
Conference on Learning Representations.

Manolis Savva, Abhishek Kadian, Oleksandr
Maksymets, Yili Zhao, Erik Wijmans, Bha-
vana Jain, Julian Straub, Jia Liu, Vladlen Koltun,
Jitendra Malik, et al. 2019. Habitat: A platform for
embodied ai research. In International Conference
on Computer Vision.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Neural Information Processing Systems.

Hao Tan, Licheng Yu, and Mohit Bansal. 2019. Learn-
ing to navigate unseen environments: Back transla-
tion with environmental dropout. In the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies.

Jesse Thomason, Daniel Gordon, and Yonatan Bisk.
2019. Shifting the Baseline: Single Modality Per-
formance on Visual Navigation & QA. In the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.

Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jian-
feng Gao, Dinghan Shen, Yuan-Fang Wang,
William Yang Wang, and Lei Zhang. 2019. Re-
inforced cross-modal matching and self-supervised
imitation learning for vision-language navigation.
In IEEE Conference on Computer Vision and Pat-
tern Recognition.

Xin Wang, Wenhan Xiong, Hongmin Wang, and
William Yang Wang. 2018. Look before you
leap: Bridging model-free and model-based rein-
forcement learning for planned-ahead vision-and-
language navigation. In IEEE European Conference
on Computer Vision.

Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod
Lipson. 2014. How transferable are features in deep
neural networks? In Neural Information Processing
Systems.


