



















































NTHU at the CoNLL-2014 Shared Task


Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 91–95,
Baltimore, Maryland, 26-27 July 2014. c©2014 Association for Computational Linguistics

NTHU at the CoNLL-2014 Shared Task

Jian-Cheng Wu*, Tzu-Hsi Yen*, Jim Chang*, Guan-Cheng Huang*,
Jimmy Chang*, Hsiang-Ling Hsu+, Yu-Wei Chang+, Jason S. Chang*

* Department of Computer Science
+ Institute of Information Systems and Applications

National Tsing Hua University
HsinChu, Taiwan, R.O.C. 30013

{wujc86, joseph.yen, cwebb.tw, a2961353,
rocmewtwo, ilovecat6717, teer1990, jason.jschang}@gmail.com

Abstract

In this paper, we describe a system for cor-
recting grammatical errors in texts written
by non-native learners. In our approach, a
given sentence with syntactic features are
sent to a number of modules, each focuses
on a specific error type. A main program
integrates corrections from these modules
and outputs the corrected sentence. We
evaluated our system on the official test
data of the CoNLL-2014 shared task and
obtained 0.30 in F-measure.

1 Introduction

Millions of non-native learners are using English
as their second language (ESL) or foreign lan-
guage (EFL). These learners often make different
kinds of grammatical errors and are not aware of
it. With a grammatical error corrector applies rules
or statistical learning methods, learners can use the
system to improve the quality of writing, and be-
come more aware of the common errors. It may
also help learners improve their writing skills.

The CoNLL-2014 shared task is aimed at pro-
moting research on correcting grammatical errors.
Types of errors handled in the shared task are ex-
tended from the five types in the previous shared
task to include all common errors present in an es-
say.

In this paper, we focus on the following errors
made by ESL writers:

• Spelling and comma

• Article and determiner
• Preposition
• Preposition + verb (interactive)
• Noun number
• Word form
• Subject-verb-agreement
For each error type, we developed and tuned a

module based on the official development data. A
main program combines the correction hypotheses
from these modules and produces the final correc-
tion. If multiple modules propose different cor-
rections to the same word/phrase, the correction
proposed by the module with the highest precision
will be chosen.

2 Method

2.1 Spelling and Comma module
In this section, we correct comma errors and
spelling errors, including missing/extraneous hy-
phens. For simplicity, we adopt Aspell1 and
GingerIt2 to detect spelling errors and generate
possible replacements, considered as confusable
words, which might contain the word with cor-
rect spelling. Then, we replace the word being
checked with confusable words to generate sen-
tences. Language models trained on well-formed
texts are used to measure the probability of these

1http://aspell.net/
2https://pypi.python.org/pypi/gingerit

We use GingerIt only for correcting missing/extraneous
hyphens

91



candidates. Candidate with the highest probability
is chosen as correction.

Omitted commas form a large proportion of
punctuation errors. We apply the CRF model pro-
posed by Israel, et. al. (2012) with some mod-
ification. We replace distance features with syn-
tactic features. More specifically, we do not use
features such as distances to the start of sentence
or last comma. And we add two features, one in-
dicates whether a word is at the end of a clause,
and the other indicates whether the current clause
starts with a prepositional phrase.

2.2 Subject-verb-agreement module
This module corrects subject-verb-agreement er-
rors in a given sentence. Consider the sentence
”The boy in blue pants are my brother”. The cor-
rect sentence should be ”The boy in blue pants is
my brother”. Since a verb could be far from it’s
subject, using ngram counts may fail to detect and
correct such an error.

We use a rule-based method in this module.
In the first stage, we identify the subject of each
clause by using information from the parser. Both
the dependency relation and syntactic structure are
used in this stage. Dependency relations such as
nsubj and rcmod indicate subjects of subject-
verb relation. If there is a verb that has not been
assigned a subject via dependency relations, head
of noun phrase in the same clause will be used in-
stead. And in the second stage, we check whether
subject and verbs agree, for each clause in the sen-
tence.

Here we explain our correction process in more
detail. For each clause, the singular and plural
forms of verbs in the clause must be consistent
with the subject of the clause unless the subject
is a quantifier. Consider the following sentences:

The number of cats is ten.
A number of cats are playing.

Since our judgment only depends on the subject
number, it’s hard to tell whether should we use
a plural verb or not in this case. The quantifiers
we do not handle are listed as follow: number, lot,
quantity, variety, type, amount, neither.

2.3 Number module and Forms module
We correct noun number error in two stages. In
the first stage, we generate a confusion set for each

word. While constructing confusion set for noun
number, both of the singular form and plural form
are included in the set. While constructing con-
fusion set for word forms, we use the word fam-
ilies in Academic Word List (AWL)3 and British
National Corpus (BNC4000) 4. Given a content
word, all the words in the same family except
antonyms are entered into the confusion set. How-
ever, comparative form and superlative form of an
adjective are eliminated from the confusion set,
since replacing an adjective with these forms is a
semantic rather than syntactic decision. The fol-
lowing examples illustrate what kinds of alterna-
tives are eliminated:

antonyms: misleading for the word lead
semantics: higher for the word high

Additionally, in the forms module, a correc-
tion is ignored, if it is actually correcting a verb
tense, subject-verb-agreement, or noun number er-
ror. We use part-of-speech (POS) tag to check this.
More specifically, any corrections that changes a
word with a VBZ tag to a word with a VBP or
VBD tag is ignored, and vice versa. And any cor-
rections that switches a noun between it’s singular
form and plural form is also ignored.

With the confusion sets, we proceed to the
second stage. In this stage, we use words in
the confusion set to attempt to replace potential
errors. Language models trained on well-formed
text are used to validate the replacement decisions.
Given a word w, If there is an alternative w’ that
fits in the context better, w is flagged as an error
and w’ is returned as a correction.
Here is our formula for correcting errors

P (O) =
Pngram(O) + Prnn(O)

2

P (R) =
Pngram(R) + Prnn(R)

2

Promotion =
P (R)− P (O)

|O|
While checking a content word w, we replace

w in the original sentence O with alternatives and
generate candidates C. We then generate the can-
didate R with the highest probability among all

3http://www.victoria.ac.nz/lals/
resources/academicwordlist/sublists

4http://simple.wiktionary.org/wiki/
Wiktionary:BNC_spoken_freq

92



candidates. We use an interpolated probability5 of
ngram language model Pngram and recurrent neu-
ral network language model Prnn. Promotion in-
dicates the increase in probability per word after
we replace sentence O with the candidate R. We
use word number to normalize Promotion follow-
ing Dahlmeier, et al. (2012). Corrections are made
only if Promotion is higher than a empirically de-
termined threshold.6

2.4 Article and Determiner module

In this subsection, we describe how we correct er-
rors of omitting a determiner or adding a spuri-
ous determiner. The language models mentioned
in the last subsection are also used in this module.
We tune our thresholds for making corrections on
development data7, and found that deleting a de-
terminer should have a lower threshold while in-
serting one should have a higher one, so we set
different thresholds accordingly. 8

To cope with the situation where a deter-
miner/article is far ahead of the head of the noun
phrase, we apply another constraint while making
correction decision.

First, we calculate statistics on the head of noun
phrases. We extract the most frequent 100,000
terms in Web-1T 5-gram corpus. These terms are
used to search their definitions in Wikipedia (usu-
ally at the first paragraph). The characteristic of a
definition is that it has no prior context and most
of the noun phrases with a determiner are unique
or known to the general public. Heads of these
nouns phrases are likely to always appear with a
determiner.

Heads that tend to appear with a determiner
the help us to decide whether a determiner should
be added to a noun phrase. We add a determiner
using two constraints. We only insert a determiner
or an article, if the statistics indicate that head of
a noun phrase tends to have a determiner, or the
promotion of log probability is much higher than
the threshold. A similar constraint is also applied,
for deleting a determiner or an article.

5the probabilities present in the formula are log probabil-
ities

6the threshold for noun number module is 0.035 and 0.050
for word form module. These threshold were set empirically
after testing on development data

7test data of the CoNLL-2013 shared task
8the threshold for insertion is 0.035 and 0.040 for deletion

2.5 Preposition module
For preposition errors, we focus on handling two
types of errors: REPLACE and DELETE. A
preposition, which should be deleted from the
given sentence, is regarded as a DELETE error,
whereas for a preposition, which should be re-
placed with a more appropriate alternative, is re-
garded as a REPLACE error. In this work, we
correct the two types of errors based on the as-
sumption that the usage of preposition often de-
pends on the collocation relations with a verb or
noun. Therefore, we use the dependency relations
such as dobj and pobj, and prep to identify
the related words, and then we validate the usage
of prepositions, and correct the preposition errors.

A dependency-based model is proposed in this
paper to handle the preposition errors. The model
consists of two stages: detecting the possible
preposition errors and correcting the errors.

In the first stage, we use the Stanford depen-
dency parser (Klein and Manning, 2003) to extract
the dependency relations for each preposition. The
relation tuples, which contain the preposition, verb
or noun, and prepositional object. For example,
the tuple of verb-prep-object (listen, to, music),
or the tuple of noun-prep-object (point, of, view)
are extracted for validation. We identify a preposi-
tion containing as an error, if the tuple containing
the preposition does not occur in a reference list
built using a reference corpus. In order to resolve
the data sparseness and false alarm problems, we
need a sufficiently large list of validated tuples.
In this study, the reference tuple lists are gener-
ated from the Google Books Syntactic N-grams
(Goldberg and Orwant, 2013)9. For example, we
can find (come, to, end, 236864) and (lead, to, re-
sult, 57632) in the verb-preposition-object refer-
ence list. We have generated 21,773,752 different
dependency tuples for our purpose.

In the second stage, we attempt to correct all
potential preposition errors. At first, a list of can-
didate tuples is generated by substituting the orig-
inal preposition in the error tuple with alterna-
tive prepositions. For example, the generated can-
didate tuples for the error tuple (join, at, party)
will include (join, in, party), (join, on, party), etc.
On the other hand, the tuple, (join, party), which

9Data sets available from http://
commondatastorage.googleapis.com/books/
syntactic-ngrams/index.html

93



deletes the preposition, is also taken into consid-
eration. All candidates are ranked according to
the frequency provided by the reference lists. The
preposition in the tuple with the highest frequency
is returned as the correction suggestion.

Figure 1: Sample annotated trigrams

Figure 2: Sample trigram group

Figure 3: Sample phrase translation for a trigram
group

2.6 Interactive errors module
This module uses a new method for correcting
serial grammatical errors in a given sentence in
learners writing. A statistical translation model is
generated to attempt to translate the input with se-
rial and interactive errors into a grammatical sen-
tence. The method involves automatically learn-
ing translation models based on Web-scale n-
gram. The model corrects trigrams containing se-

rial preposition-verb errors via translation. Eval-
uation on a set of sentences in a learner corpus
shows that the method corrects serial errors rea-
sonably well.

Consider an error sentence ”I have difficulty to
understand English.” The correct sentence for this
should be ”I have difficulty in understanding En-
glish.” It is hard to correct these two errors one by
one, since the errors are dependent on each other.
Intuitively, by identifying difficulty to understand
as containing serial errors and correct it to diffi-
culty in understanding, we can handle this kind of
problem more effectively.

First, we generate translation phrase table as
follows. We begin by selecting trigrams related to
serial errors and correction from Web 1T 5-gram.
Figure 1 shows some sample annotation trigrams.
Then, we group the selected trigrams by the first
and last word in the trigrams. See Figure 2 for a
sample VPV group of trigrams. Finally, we gener-
ate translation phrase table for each group. Figure
3 shows a sample translation phrase table.

At run time, we tag the input sentence with part
of speech information in order to find trigrams
that fit the type of serial errors. Then, we search
phrase table and generate translations for the
input phrases in a machine translation decoder to
produce a corrected sentence.

3 Experiment

Two types of trigram language models, ngram
model and recurrent neural network (RNN) model,
are used in correcting spelling, noun number, word
form, and determiner errors. We trained the ngram
language model on English Gigaword and BNC
corpus, using the SRILM tool (Stolcke, 2002).
We train the RNN model with RNNLM toolkit
(Mikolov et al., 2011). Complexity of training the
RNN language model is much higher, so we train
it on a smaller corpus, the British National Corpus
(BNC).

We used the Stanford Parser (Klein and Christo-
pher D. Manning, 2003) to obtain dependency re-
lations in the preposition module, and to obtain
POS tags for the word form module. The subject-
verb-agreement module also uses dependency re-
lations contained in test data. Dependency rela-
tions in Google Books Syntactic N-grams (Gold-

94



berg and Orwant, 2013) were also used to develop
our dendepency-based model in the preposition
module.

To assess the effectiveness of the proposed
method, we used the official training, develop-
ment, and test data of the CoNLL-2014 shared
task. On the test data, our system obtained the pre-
cision, recall and F0.5 score of 0.351, 0.189, and
0.299. The following table shows the performance
breakdown by module.

Figure 4: The performance breakdown by module.
(Displayed in %)

In the spelling and hyphen module, candidates
from Aspell include words that only differ from
the original word in one character, s. Language
models are then used to choose the candidate with
highest probability as our correction. The module
therefore gives some corrections about noun num-
bers or subject-verb-agreement. As a result, some
corrections made by this module overlap with cor-
rections made by the noun numbers module and
the subject-verb-agreement module, which makes
the recall of correcting spelling and hyphen errors,
4.11%, overestimated.

4 Conclusion

In this work, we have built several modules for er-
ror detection and correction. For different types
of errors, we developed modules independently
using different features and thresholds. If mul-
tiple modules propose different corrections to a
word/phrase, the one proposed by the module with
higher precision will be chosen. Many avenues
for future work present themselves. We plan to
integrate modules in a more flexible way. When
faced with different corrections made by different
modules, the decision would better be based on
the confidence of each correction with a uniform
standard, but not on the confidence of modules.

Acknowledgments

We would like to acknowledge the funding sup-
ports from Delta Electronic Corp and National
Science Council (contract no: NSC 100-2511-S-
007-005-MY3), Taiwan. We are also thankful to
anonymous reviewers and the organizers of the
shared task.

References
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng Ng

2012. NUS at the HOO 2012 Shared Task. In Pro-
ceedings of the Seventh Workshop on Building Ed-
ucational Applications Using NLP, Association for
Computational Linguistics, June 7.

Daniel Dahlmeier and Hwee Tou Ng, 2012. Better
Evaluation for Grammatical Error Correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL 2012).,568-672

Daniel Dahlmeier, Hwee Tou Ng, Siew Mei Wu. 2013.
Building a Large Annotated Corpus of Learner En-
glish: The NUS Corpus of Learner English. In
Proceedings of the 8th Workshop on Innovative Use
of NLP for Building Educational Applications(BEA
2013)

Yoav Goldberg and Jon Orwant 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In Proceedings of the Second
Joint Conference on Lexical and Computational Se-
mantics, Atlanta, GA, 2013.

Ross Israel, Joel Tetreault, and Martin Chodorow
2012. Correcting Comma Errors in Learner Essays,
and Restoring Commas in Newswire Text. In Pro-
ceeding of the 2012 Conference of the North Amer-
ica Chapter of the Association for Computational
Linguistics: Human Language Technologies,284-
294, Montreal Canada, June. Association for Com-
putational Linguistics

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, 423-430.

Tomas Mikolov, Anoop Deoras, Dan Povey, Lukar
Burget, and Jan Honza Cernocky 2011. Strategies
for Training Large Scale Neural Network Language
Models Proceedings of ASRU 2011

Andreas Stolcke 2002. SRILM-An Extensible Lan-
guage Modeling Toolkit In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, vol 2, 901-904

95


