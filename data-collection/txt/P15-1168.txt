



















































Gated Recursive Neural Network for Chinese Word Segmentation


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1744–1753,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Gated Recursive Neural Network for Chinese Word Segmentation

Xinchi Chen, Xipeng Qiu∗, Chenxi Zhu, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University

School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China

{xinchichen13,xpqiu,czhu13,xjhuang}@fudan.edu.cn

Abstract

Recently, neural network models for natu-
ral language processing tasks have been in-
creasingly focused on for their ability of al-
leviating the burden of manual feature en-
gineering. However, the previous neural
models cannot extract the complicated fea-
ture compositions as the traditional meth-
ods with discrete features. In this paper,
we propose a gated recursive neural net-
work (GRNN) for Chinese word segmen-
tation, which contains reset and update
gates to incorporate the complicated com-
binations of the context characters. Since
GRNN is relative deep, we also use a
supervised layer-wise training method to
avoid the problem of gradient diffusion.
Experiments on the benchmark datasets
show that our model outperforms the pre-
vious neural network models as well as the
state-of-the-art methods.

1 Introduction

Unlike English and other western languages, Chi-
nese do not delimit words by white-space. There-
fore, word segmentation is a preliminary and im-
portant pre-process for Chinese language process-
ing. Most previous systems address this problem
by treating this task as a sequence labeling prob-
lem and have achieved great success. Due to the
nature of supervised learning, the performance of
these models is greatly affected by the design of
features. These features are explicitly represented
by the different combinations of context charac-
ters, which are based on linguistic intuition and sta-
tistical information. However, the number of fea-
tures could be so large that the result models are
too large to use in practice and prone to overfit on
training corpus.

∗Corresponding author.

Rainy

下 雨
Day

天
Ground

地 面
Accumulated water

积 水

M E SB

Figure 1: Illustration of our model for Chinese
word segmentation. The solid nodes indicate the
active neurons, while the hollow ones indicate the
suppressed neurons. Specifically, the links denote
the information flow, where the solid edges de-
note the acceptation of the combinations while the
dashed edges means rejection of that. As shown in
the right figure, we receive a score vector for tag-
ging target character “地” by incorporating all the
combination information.

Recently, neural network models have been in-
creasingly focused on for their ability to minimize
the effort in feature engineering. Collobert et al.
(2011) developed a general neural network archi-
tecture for sequence labeling tasks. Following this
work, many methods (Zheng et al., 2013; Pei et
al., 2014; Qi et al., 2014) applied the neural net-
work to Chinese word segmentation and achieved
a performance that approaches the state-of-the-art
methods.
However, these neural models just concatenate

the embeddings of the context characters, and feed
them into neural network. Since the concatena-
tion operation is relatively simple, it is difficult to
model the complicated features as the traditional
discrete feature based models. Although the com-
plicated interactions of inputs can be modeled by
the deep neural network, the previous neural model
shows that the deep model cannot outperform the
one with a single non-linear model. Therefore, the

1744



neural model only captures the interactions by the
simple transition matrix and the single non-linear
transformation . These dense features extracted via
these simple interactions are not nearly as good as
the substantial discrete features in the traditional
methods.
In this paper, we propose a gated recursive neu-

ral network (GRNN) to model the complicated
combinations of characters, and apply it to Chi-
nese word segmentation task. Inspired by the suc-
cess of gated recurrent neural network (Chung et
al., 2014), we introduce two kinds of gates to con-
trol the combinations in recursive structure. We
also use the layer-wise training method to avoid
the problem of gradient diffusion, and the dropout
strategy to avoid the overfitting problem.
Figure 1 gives an illustration of how our ap-

proach models the complicated combinations of
the context characters. Given a sentence “雨
(Rainy)天 (Day)地面 (Ground)积水 (Accumu-
lated water)”, the target character is “地”. This
sentence is very complicated because each consec-
utive two characters can be combined as a word.
To predict the label of the target character “地” un-
der the given context, GRNN detects the combina-
tions recursively from the bottom layer to the top.
Then, we receive a score vector of tags by incorpo-
rating all the combination information in network.
The contributions of this paper can be summa-

rized as follows:

• We propose a novel GRNN architecture to
model the complicated combinations of the
context characters. GRNN can select and pre-
serve the useful combinations via reset and
update gates. These combinations play a sim-
ilar role in the feature engineering of the tra-
ditional methods with discrete features.

• We evaluate the performance of Chinese
word segmentation on PKU, MSRA and
CTB6 benchmark datasets which are com-
monly used for evaluation of Chinese word
segmentation. Experiment results show that
our model outperforms other neural network
models, and achieves state-of-the-art perfor-
mance.

2 Neural Model for Chinese Word
Segmentation

Chinese word segmentation task is usually re-
garded as a character-based sequence labeling

Input Window

Characters
Ci-2 Ci-1 Ci+1 Ci+2Ci

Lookup Table

·
·
·

·
·
·

·
·
·

·
·
·

·
·
·

3
4
5

2

6

1

·
·
·
d-1
d

Features

Linear

W1  ×□+b1

· · ·

Number of Hidden Units

Sigmoid
g(□) · · ·

Number of Hidden Units

Linear

W2  ×□+b2
Number of tags

· · ·

Tag Inference

f(t|1) f(t|2) f(t|i) f(t|n-1) f(t|n)

Aij

Concatenate

B

E

M

S

Figure 2: General architecture of neural model for
Chinese word segmentation.

problem. Each character is labeled as one of {B,
M, E, S} to indicate the segmentation. {B, M, E}
represent Begin, Middle, End of a multi-character
segmentation respectively, and S represents a Sin-
gle character segmentation.
The general neural network architecture for Chi-

nese word segmentation task is usually character-
ized by three specialized layers: (1) a character
embedding layer; (2) a series of classical neural
network layers and (3) tag inference layer. A il-
lustration is shown in Figure 2.
The most common tagging approach is based on

a local window. The window approach assumes
that the tag of a character largely depends on its
neighboring characters.
Firstly, we have a character set C of size |C|.

Then each character c ∈ C is mapped into an d-
dimensional embedding space as c ∈ Rd by a
lookup tableM ∈ Rd×|C|.
For each character ci in a given sentence c1:n,

the context characters ci−w1:i+w2 are mapped
to their corresponding character embeddings as
ci−w1:i+w2 , where w1 and w2 are left and right
context lengths respectively. Specifically, the un-
known characters and characters exceeding the

1745



sentence boundaries are mapped to special sym-
bols, “unknown”, “start” and “end” respectively.
In addition, w1 and w2 satisfy the constraint w1 +
w2 + 1 = w, where w is the window size of the
model. As an illustration in Figure 2, w1, w2 and
w are set to 2, 2 and 5 respectively.
The embeddings of all the context characters are

then concatenated into a single vector ai ∈ RH1 as
input of the neural network, where H1 = w × d is
the size of Layer 1. And ai is then fed into a con-
ventional neural network layer which performs a
linear transformation followed by an element-wise
activation function g, such as tanh.

hi = g(W1ai + b1), (1)

whereW1 ∈ RH2×H1 , b1 ∈ RH2 , hi ∈ RH2 . H2
is the number of hidden units in Layer 2. Here, w,
H1 and H2 are hyper-parameters chosen on devel-
opment set.
Then, a similar linear transformation is per-

formed without non-linear function followed:

f(t|ci−w1:i+w2) = W2hi + b2, (2)

where W2 ∈ R|T |×H2 , b2 ∈ R|T | and T is the
set of 4 possible tags. Each dimension of vector
f(t|ci−w1:i+w2) ∈ R|T | is the score of the corre-
sponding tag.
To model the tag dependency, a transition score

Aij is introduced to measure the probability of
jumping from tag i ∈ T to tag j ∈ T (Collobert et
al., 2011).

3 Gated Recursive Neural Network for
Chinese Word Segmentation

To model the complicated feature combinations,
we propose a novel gated recursive neural network
(GRNN) architecture for Chinese word segmenta-
tion task (see Figure 3).

3.1 Recursive Neural Network
A recursive neural network (RNN) is a kind of
deep neural network created by applying the same
set of weights recursively over a given struc-
ture(such as parsing tree) in topological order (Pol-
lack, 1990; Socher et al., 2013a).
In the simplest case, children nodes are com-

bined into their parent node using a weight matrix
W that is shared across the whole network, fol-
lowed by a non-linear function g(·). Specifically,
if hL and hR are d-dimensional vector representa-
tions of left and right children nodes respectively,

E M B S

…
…

…
…

…
…

…
…

…
…

…
…

…
…

…
…

…
…

…
…

…
…

…
…

…
…

…
…

…
…

……

…
…

ci-2 ci-1 ci ci+1 ci+2

…
…

…
…

…
…

…
…

Linear
xiyi = Ws  ×  xi + bs

Concatenate

yi

Figure 3: Architecture of Gated Recursive Neural
Network for Chinese word segmentation.

their parent node hP will be a d-dimensional vec-
tor as well, calculated as:

hP = g
(
W

[
hL
hR

])
, (3)

where W ∈ Rd×2d and g is a non-linear function
as mentioned above.

3.2 Gated Recursive Neural Network
The RNN need a topological structure to model a
sequence, such as a syntactic tree. In this paper, we
use a directed acyclic graph (DAG), as showing in
Figure 3, to model the combinations of the input
characters, in which two consecutive nodes in the
lower layer are combined into a single node in the
upper layer via the operation as Eq. (3).
In fact, the DAG structure can model the com-

binations of characters by continuously mixing the
information from the bottom layer to the top layer.
Each neuron can be regarded as a complicated fea-
ture composition of its governed characters, simi-
lar to the discrete feature basedmodels. The differ-
ence between them is that the neural one automat-
ically learns the complicated combinations while
the conventional one need manually design them.

1746



When the children nodes combine into their parent
node, the combination information of two children
nodes is also merged and preserved by their parent
node.
Although the mechanism above seem to work

well, it can not sufficiently model the complicated
combination features for its simplicity in practice.
Inspired by the success of the gated recurrent

neural network (Cho et al., 2014b; Chung et al.,
2014), we propose a gated recursive neural net-
work (GRNN) by introducing two kinds of gates,
namely “reset gate” and “update gate”. Specifi-
cally, there are two reset gates, rL and rR, par-
tially reading the information from left child and
right child respectively. And the update gates zN ,
zL and zR decide what to preserve when combin-
ing the children’s information. Intuitively, these
gates seems to decide how to update and exploit
the combination information.
In the case of word segmentation, for each char-

acter ci of a given sentence c1:n, we first repre-
sent each context character cj into its correspond-
ing embedding cj , where i − w1 ≤ j ≤ i + w2
and the definitions of w1 and w2 are as same as
mentioned above.
Then, the embeddings are sent to the first layer

of GRNN as inputs, whose outputs are recursively
applied to upper layers until it outputs a single
fixed-length vector.
The outputs of the different neurons can be re-

garded as the different feature compositions. After
concatenating the outputs of all neurons in the net-
work, we get a new big vector xi. Next, we receive
the tag score vector yi for character cj by a linear
transformation of xi:

yi = Ws × xi + bs, (4)
where bs ∈ R|T |,Ws ∈ R|T |×Q. Q = q × d is di-
mensionality of the concatenated vector xi, where
q is the number of nodes in the network.

3.3 Gated Recursive Unit
GRNNconsists of theminimal structures, gated re-
cursive units, as showing in Figure 4.
By assuming that the window size is w, we will

have recursion layer l ∈ [1, w]. At each recursion
layer l, the activation of the j-th hidden node h(l)j ∈
Rd is computed as

h(l)j =

{
zN ⊙ ĥlj + zL ⊙ hl−1j−1 + zR ⊙ hl−1j , l > 1,
cj , l = 1,

(5)

Gate z

Gate rL Gate rR

hj-1
(l-1) hj

(l-1)

hj
^(l)

hj
(l)

Figure 4: Our proposed gated recursive unit.

where zN , zL and zR ∈ Rd are update gates
for new activation ĥlj , left child node hl−1j−1 and
right child node hl−1j respectively, and⊙ indicates
element-wise multiplication.
The update gates can be formalized as:

z =

 zNzL
zR

 =
 1/Z1/Z

1/Z

⊙ exp(U
 ĥ

l
j

hl−1j−1
hl−1j

),
(6)

where U ∈ R3d×3d is the coefficient of update
gates, and Z ∈ Rd is the vector of the normal-
ization coefficients,

Zk =
3∑

i=1

[exp(U

 ĥ
l
j

hl−1j−1
hl−1j

)]d×(i−1)+k, (7)
where 1 ≤ k ≤ d.
Intuitively, three update gates are constrained

by:


[zN ]k + [zL]k + [zR]k = 1, 1 ≤ k ≤ d;
[zN ]k ≥ 0, 1 ≤ k ≤ d;
[zL]k ≥ 0, 1 ≤ k ≤ d;
[zR]k ≥ 0, 1 ≤ k ≤ d.

(8)

The new activation ĥlj is computed as:

ĥlj = tanh(Wĥ

[
rL ⊙ hl−1j−1
rR ⊙ hl−1j

]
), (9)

where Wĥ ∈ Rd×2d, rL ∈ Rd, rR ∈ Rd. rL and
rR are the reset gates for left child node hl−1j−1 and
right child node hl−1j respectively, which can be

1747



formalized as:[
rL
rR

]
= σ(G

[
hl−1j−1
hl−1j

]
), (10)

(11)

where G ∈ R2d×2d is the coefficient of two reset
gates and σ indicates the sigmoid function.
Intuiativly, the reset gates control how to select

the output information of the left and right chil-
dren, which results to the current new activation
ĥ.
By the update gates, the activation of a parent

neuron can be regarded as a choice among the the
current new activation ĥ, the left child, and the
right child. This choice allows the overall structure
to change adaptively with respect to the inputs.
This gating mechanism is effective to model the

combinations of the characters.

3.4 Inference
In Chinese word segmentation task, it is usually to
employ the Viterbi algorithm to inference the tag
sequence t1:n for a given input sentence c1:n.
In order to model the tag dependencies, the

previous neural network models (Collobert et al.,
2011; Zheng et al., 2013; Pei et al., 2014) intro-
duce a transition matrix A, and each entry Aij is
the score of the transformation from tag i ∈ T to
tag j ∈ T .
Thus, the sentence-level score can be formu-

lated as follows:

s(c1:n, t1:n, θ) =
n∑

i=1

(
Ati−1ti + fθ(ti|ci−w1:i+w2)

)
,

(12)

where fθ(ti|ci−w1:i+w2) is the score for choosing
tag ti for the i-th character by our proposed GRNN
(Eq. (4)). The parameter set of our model is θ =
(M,Ws,bs,Wĥ,U,G,A).

4 Training

4.1 Layer-wise Training
Deep neural network with multiple hidden layers
is very difficult to train for its problem of gradient
diffusion and risk of overfitting.
Following (Hinton and Salakhutdinov, 2006),

we employ the layer-wise training strategy to avoid
problems of overfitting and gradient vanishing.
The main idea of layer-wise training is to train the

network with adding the layers one by one. Specif-
ically, we first train the neural network with the
first hidden layer only. Then, we train at the net-
work with two hidden layers after training at first
layer is done and so on until we reach the top hid-
den layer. When getting convergency of the net-
work with layers 1 to l , we preserve the current
parameters as initial values of that in training the
network with layers 1 to l + 1.

4.2 Max-Margin Criterion
We use the Max-Margin criterion (Taskar et al.,
2005) to train our model. Intuitively, the Max-
Margin criterion provides an alternative to prob-
abilistic, likelihood based estimation methods by
concentrating directly on the robustness of the de-
cision boundary of a model. We use Y (xi) to de-
note the set of all possible tag sequences for a given
sentence xi and the correct tag sequence for xi is
yi. The parameter set of our model is θ. We first
define a structured margin loss ∆(yi, ŷ) for pre-
dicting a tag sequence ŷ for a given correct tag se-
quence yi:

∆(yi, ŷ) =
n∑
j

η1{yi,j ̸= ŷj}, (13)

where n is the length of sentence xi and η is a dis-
count parameter. The loss is proportional to the
number of characters with an incorrect tag in the
predicted tag sequence. For a given training in-
stance (xi, yi), we search for the tag sequence with
the highest score:

y∗ = argmax
ŷ∈Y (x)

s(xi, ŷ, θ), (14)

where the tag sequence is found and scored by
the proposed model via the function s(·) in Eq.
(12). The object of Max-Margin training is that
the tag sequence with highest score is the correct
one: y∗ = yi and its score will be larger up to a
margin to other possible tag sequences ŷ ∈ Y (xi):

s(x, yi, θ) ≥ s(x, ŷ, θ) + ∆(yi, ŷ). (15)
This leads to the regularized objective function for
m training examples:

J(θ) =
1
m

m∑
i=1

li(θ) +
λ

2
∥θ∥22, (16)

li(θ) = max
ŷ∈Y (xi)

(s(xi, ŷ, θ)+∆(yi, ŷ))−s(xi, yi, θ).
(17)

1748



By minimizing this object, the score of the correct
tag sequence yi is increased and score of the high-
est scoring incorrect tag sequence ŷ is decreased.
The objective function is not differentiable due to
the hinge loss. We use a generalization of gradient
descent called subgradient method (Ratliff et al.,
2007) which computes a gradient-like direction.
Following (Socher et al., 2013a), we minimize

the objective by the diagonal variant of AdaGrad
(Duchi et al., 2011) with minibatchs. The parame-
ter update for the i-th parameter θt,i at time step t
is as follows:

θt,i = θt−1,i − α√∑t
τ=1 g

2
τ,i

gt,i, (18)

where α is the initial learning rate and gτ ∈ R|θi|
is the subgradient at time step τ for parameter θi.

5 Experiments

We evaluate our model on two different kinds of
texts: newswire texts and micro-blog texts. For
evaluation, we use the standard Bakeoff scoring
program to calculate precision, recall, F1-score.

5.1 Word Segmentation on Newswire Texts
5.1.1 Datasets
We use three popular datasets, PKU, MSRA and
CTB6, to evaluate our model on newswire texts.
The PKU and MSRA data are provided by the
second International Chinese Word Segmentation
Bakeoff (Emerson, 2005), and CTB6 is from
Chinese TreeBank 6.0 (LDC2007T36) (Xue et
al., 2005), which is a segmented, part-of-speech
tagged, and fully bracketed corpus in the con-
stituency formalism. These datasets are commonly
used by previous state-of-the-art models and neu-
ral network models. In addition, we use the first
90% sentences of the training data as training set
and the rest 10% sentences as development set for
PKU and MSRA datasets, and we divide the train-
ing, development and test sets according to (Yang
and Xue, 2012) for the CTB6 dataset.
All datasets are preprocessed by replacing the

Chinese idioms and the continuous English char-
acters and digits with a unique flag.

5.1.2 Hyper-parameters
We set the hyper-parameters of the model as list
in Table 1 via experiments on development set.
In addition, we set the batch size to 20. And we

Window size k = 5
Character embedding size d = 50

Initial learning rate α = 0.3
Margin loss discount η = 0.2

Regularization λ = 10−4
Dropout rate on input layer p = 20%

Table 1: Hyper-parameter settings.

0 10 20 30 40
88

90

92

94

96

epoches

F-
va
lu
e(
%
) 1 layer

2 layers
3 layers
4 layers
5 layers
layer-wise

Figure 5: Performance of different models with or
without layer-wise training strategy on PKUdevel-
opment set.

find that it is a good balance between model per-
formance and efficiency to set character embed-
ding size d = 50. In fact, the larger embedding
size leads to higher cost of computational resource,
while lower dimensionality of the character em-
bedding seems to underfit according to the experi-
ment results.

Deep neural networks contain multiple non-
linear hidden layers are always hard to train for it
is easy to overfit. Several methods have been used
in neural models to avoid overfitting, such as early
stop and weight regularization. Dropout (Srivas-
tava et al., 2014) is also one of the popular strate-
gies to avoid overfitting when training the deep
neural networks. Hence, we utilize the dropout
strategy in this work. Specifically, dropout is to
temporarily remove the neuron away with a fixed
probability p independently, along with the incom-
ing and outgoing connections of it. As a result,
we find dropout on the input layer with probability
p = 20% is a good tradeoff between model effi-
ciency and performance.

1749



models without layer-wise with layer-wiseP R F P R F
GRNN (1 layer) 90.7 89.6 90.2 - - -
GRNN (2 layers) 96.0 95.6 95.8 96.0 95.6 95.8
GRNN (3 layers) 95.9 95.4 95.7 96.0 95.7 95.9
GRNN (4 layers) 95.6 95.2 95.4 96.1 95.7 95.9
GRNN (5 layers) 95.3 94.7 95.0 96.1 95.7 95.9

Table 2: Performance of different models with or without layer-wise training strategy on PKU test set.

5.1.3 Layer-wise Training
We first investigate the effects of the layer-wise
training strategy. Since we set the size of context
window to five, there are five recursive layers in
our architecture. And we train the networks with
the different numbers of recursion layers. Due to
the limit of space, we just give the results on PKU
dataset.
Figure 5 gives the convergence speeds of the

five models with different numbers of layers and
the model with layer-wise training strategy on de-
velopment set of PKU dataset. The model with
one layer just use the neurons of the lowest layer
in final linear score function. Since there are no
non-linear layer, its seems to underfit and perform
poorly. The model with two layers just use the
neurons in the lowest two layers, and so on. The
model with five layers use all the neurons in the
network. As we can see, the layer-wise training
strategy lead to the fastest convergence and the
best performance.
Table 2 shows the performances on PKU test

set. The performance of the model with layer-wise
training strategy is always better than that with-
out layer-wise training strategy. With the increase
of the number of layers, the performance also in-
creases and reaches the stable high performance
until getting to the top layer.

5.1.4 Results
We first compare our model with the previous neu-
ral approaches on PKU,MSRA and CTB6 datasets
as showing in Table 3. The character embed-
dings of the models are random initialized. The
performance of word segmentation is significantly
boosted by exploiting the gated recursive archi-
tecture, which can better model the combinations
of the context characters than the previous neural
models.
Previous works have proven it will greatly im-

prove the performance to exploit the pre-trained

character embeddings instead of that with random
initialization. Thus, we pre-train the embeddings
on a huge unlabeled data, the Chinese Wikipedia
corpus, with word2vec toolkit (Mikolov et al.,
2013). By using these obtained character embed-
dings, our model receives better performance and
still outperforms the previous neural models with
pre-trained character embeddings. The detailed re-
sults are shown in Table 4 (1st to 3rd rows).

Inspired by (Pei et al., 2014), we utilize the bi-
gram feature embeddings in our model as well.
The concept of feature embedding is quite similar
to that of character embedding mentioned above.
Specifically, each context feature is represented as
a single vector called feature embedding. In this
paper, we only use the simply bigram feature em-
beddings initialized by the average of two embed-
dings of consecutive characters element-wisely.

Although the model of Pei et al. (2014) greatly
benefits from the bigram feature embeddings, our
model just obtains a small improvement with them.
This difference indicates that our model has well
modeled the combinations of the characters and do
not needmuch help of the feature engineering. The
detailed results are shown in Table 4 (4-th and 6-th
rows).

Table 5 shows the comparisons of our model
with the state-of-the-art systems on F-value. The
model proposed by Zhang and Clark (2007) is
a word-based segmentation method, which ex-
ploit features of complete words, while remains
of the list are all character-based word segmenters,
whose features are mostly extracted from the con-
text characters. Moreover, some systems (such as
Sun and Xu (2011) and Zhang et al. (2013)) also
exploit kinds of extra information such as the un-
labeled data or other knowledge. Although our
model only uses simple bigram features, it outper-
forms the previous state-of-the-art methods which
use more complex features.

1750



models PKU MSRA CTB6P R F P R F P R F
(Zheng et al., 2013) 92.8 92.0 92.4 92.9 93.6 93.3 94.0* 93.1* 93.6*
(Pei et al., 2014) 93.7 93.4 93.5 94.6 94.2 94.4 94.4* 93.4* 93.9*

GRNN 96.0 95.7 95.9 96.3 96.1 96.2 95.4 95.2 95.3

Table 3: Performances on PKU,MSRA and CTB6 test sets with random initialized character embeddings.

models PKU MSRA CTB6P R F P R F P R F
+Pre-train

(Zheng et al., 2013) 93.5 92.2 92.8 94.2 93.7 93.9 93.9* 93.4* 93.7*
(Pei et al., 2014) 94.4 93.6 94.0 95.2 94.6 94.9 94.2* 93.7* 94.0*

GRNN 96.3 95.9 96.1 96.2 96.3 96.2 95.8 95.4 95.6
+bigram
GRNN 96.6 96.2 96.4 97.5 97.3 97.4 95.9 95.7 95.8

+Pre-train+bigram
(Pei et al., 2014) - 95.2 - - 97.2 - - - -

GRNN 96.5 96.3 96.4 97.4 97.8 97.6 95.8 95.7 95.8

Table 4: Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em-
beddings.

models PKU MSRA CTB6
(Tseng et al., 2005) 95.0 96.4 -

(Zhang and Clark, 2007) 95.1 97.2 -
(Sun and Xu, 2011) - - 95.7
(Zhang et al., 2013) 96.1 97.4 -

This work 96.4 97.6 95.8

Table 5: Comparison of GRNN with the state-of-
the-art methods on PKU, MSRA and CTB6 test
sets.

5.2 Word Segmentation on Micro-blog Texts

5.2.1 Dataset
Weuse the NLPCC 2015 dataset1 (Qiu et al., 2015)
to evaluate our model on micro-blog texts. The
NLPCC 2015 data are provided by the shared task
in the 4th CCF Conference on Natural Language
Processing & Chinese Computing (NLPCC 2015):
Chinese Word Segmentation and POS Tagging for
micro-blog Text. Different with the popular used
newswire dataset, the NLPCC 2015 dataset is col-
lected from Sina Weibo2, which consists of the
relatively informal texts from micro-blog with the
various topics, such as finance, sports, entertain-
ment, and so on. The information of the dataset is

1http://nlp.fudan.edu.cn/nlpcc2015/
2http://www.weibo.com/

shown in Table 6.
To train our model, we also use the first 90%

sentences of the training data as training set and
the rest 10% sentences as development set.
Here, we use the default setting of CRF++

toolkit with the feature templates as shown in Ta-
ble 7. The same feature templates are also used for
FNLP.

5.2.2 Results
Since the NLPCC 2015 dataset is a new released
dataset, we compare our model with the two popu-
lar open source toolkits for sequence labeling task:
FNLP3 (Qiu et al., 2013) and CRF++4. Our model
uses pre-trained and bigram character embeddings.
Table 8 shows the comparisons of our model

with the other systems on NLPCC 2015 dataset.

6 Related Work

Chinese word segmentation has been studied with
considerable efforts in the NLP community. The
most popular word segmentation method is based
on sequence labeling (Xue, 2003). Recently, re-
searchers have tended to explore neural network

3https://github.com/xpqiu/fnlp/
4http://taku910.github.io/crfpp/
*The result is from our own implementation of the corre-

sponding method.

1751



Dataset Sents Words Chars Word Types Char Types OOV Rate
Training 10,000 215,027 347,984 28,208 39,71 -
Test 5,000 106,327 171,652 18,696 3,538 7.25%
Total 15,000 322,410 520,555 35,277 4,243 -

Table 6: Statistical information of NLPCC 2015 dataset.

unigram feature c−2, c−1, c0, c+1, c+2
bigram feature c−1 ◦ c0, c0 ◦ c+1
trigram feature c−2◦c−1◦c0, c−1◦c0◦c+1,

c0 ◦ c+1 ◦ c+2
Table 7: Templates of CRF++ and FNLP.

models P R F
CRF++ 93.3 93.2 93.3
FNLP 94.1 93.9 94.0
This work 94.7 94.8 94.8

Table 8: Performances on NLPCC 2015 dataset.

based approaches (Collobert et al., 2011) to re-
duce efforts of the feature engineering (Zheng et
al., 2013; Qi et al., 2014). However, the features
of all these methods are the concatenation of the
embeddings of the context characters.
Pei et al. (2014) also used neural tensor model

(Socher et al., 2013b) to capture the complicated
interactions between tags and context characters.
But the interactions depend on the number of the
tensor slices, which cannot be too large due to the
model complexity. The experiments also show
that the model of (Pei et al., 2014) greatly bene-
fits from the further bigram feature embeddings,
which shows that their model cannot even handle
the interactions of the consecutive characters. Dif-
ferent with them, our model just has a small im-
provement with the bigram feature embeddings,
which indicates that our approach has well mod-
eled the complicated combinations of the context
characters, and does not need much help of further
feature engineering.
More recently, Cho et al. (2014a) also proposed

a gated recursive convolutional neural network in
machine translation task to solve the problem of
varying lengths of sentences. However, their ap-
proach only models the update gate, which can not
tell whether the information is from the current
state or from sub notes in update stage without re-
set gate. Instead, our approach models two kinds
of gates, reset gate and update gate, by incorporat-

ing which we can better model the combinations
of context characters via selection function of re-
set gate and collection function of update gate.

7 Conclusion

In this paper, we propose a gated recursive neu-
ral network (GRNN) to explicitly model the com-
binations of the characters for Chinese word seg-
mentation task. Each neuron in GRNN can be re-
garded as a different combination of the input char-
acters. Thus, the whole GRNN has an ability to
simulate the design of the sophisticated features in
traditional methods. Experiments show that our
proposed model outperforms the state-of-the-art
methods on three popular benchmark datasets.
Despite Chineseword segmentation being a spe-

cific case, our model can be easily generalized and
applied to other sequence labeling tasks. In future
work, we would like to investigate our proposed
GRNN on other sequence labeling tasks.

Acknowledgments

We would like to thank the anonymous review-
ers for their valuable comments. This work
was partially funded by the National Natural Sci-
ence Foundation of China (61472088, 61473092),
the National High Technology Research and De-
velopment Program of China (2015AA015408),
Shanghai Science and Technology Development
Funds (14ZR1403200), Shanghai Leading Aca-
demic Discipline Project (B114).

References
Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-

danau, and Yoshua Bengio. 2014a. On the proper-
ties of neural machine translation: Encoder–decoder
approaches. In Proceedings of Workshop on Syntax,
Semantics and Structure in Statistical Translation.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014b. Learning phrase representations
using rnn encoder-decoder for statistical machine
translation. In Proceedings of EMNLP.

1752



Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Research,
12:2493–2537.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal ofMachine
Learning Research, 12:2121–2159.

T. Emerson. 2005. The second international Chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHANWorkshop on Chinese Language
Processing, pages 123–133. Jeju Island, Korea.

Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504–507.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of ACL.

Jordan B Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46(1):77–105.

Yanjun Qi, Sujatha G Das, Ronan Collobert, and Jason
Weston. 2014. Deep learning for character-based
information extraction. In Advances in Information
Retrieval, pages 668–674. Springer.

Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013.
FudanNLP: A toolkit for Chinese natural language
processing. In Proceedings of Annual Meeting of the
Association for Computational Linguistics.

Xipeng Qiu, Peng Qian, Liusong Yin, and Xuan-
jing Huang. 2015. Overview of the NLPCC
2015 shared task: Chinese word segmentation and
POS tagging for micro-blog texts. arXiv preprint
arXiv:1505.07599.

Nathan D Ratliff, J Andrew Bagnell, and Martin A
Zinkevich. 2007. (online) subgradient methods
for structured prediction. In Eleventh International
Conference on Artificial Intelligence and Statistics
(AIStats).

Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with compo-
sitional vector grammars. In In Proceedings of the
ACL conference. Citeseer.

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013b. Reasoning with neural ten-
sor networks for knowledge base completion. In Ad-
vances in Neural Information Processing Systems.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 970–979. As-
sociation for Computational Linguistics.

Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured pre-
diction models: A large margin approach. In Pro-
ceedings of the international conference on Machine
learning.

Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the fourth SIGHAN
workshop on Chinese language Processing, volume
171.

Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural lan-
guage engineering, 11(2):207–238.

N. Xue. 2003. Chinese word segmentation as charac-
ter tagging. Computational Linguistics and Chinese
Language Processing, 8(1):29–48.

Yaqin Yang and Nianwen Xue. 2012. Chinese comma
disambiguation for discourse analysis. In Proceed-
ings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics: Long Papers-
Volume 1, pages 786–794. Association for Compu-
tational Linguistics.

Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. In
ACL.

Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for Chinese word seg-
mentation. In Proceedings of the 2013 Conference
on EmpiricalMethods in Natural Language Process-
ing.

Xiaoqing Zheng, Hanyang Chen, and TianyuXu. 2013.
Deep learning for chinese word segmentation and
pos tagging. In EMNLP, pages 647–657.

1753


