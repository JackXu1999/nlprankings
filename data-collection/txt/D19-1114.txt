



















































Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1204–1209,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1204

Incorporating Contextual and Syntactic Structures
Improves Semantic Similarity Modeling

Linqing Liu,1 Wei Yang,1 Jinfeng Rao,2 Raphael Tang,1 and Jimmy Lin1
1 David R. Cheriton School of Computer Science, University of Waterloo
2 Department of Computer Science, University of Maryland, College Park

{linqing.liu,w85yang,r33tang,jimmylin}@uwaterloo.ca, raojinfeng@gmail.com

Abstract
Semantic similarity modeling is central to
many NLP problems such as natural language
inference and question answering. Syntactic
structures interact closely with semantics in
learning compositional representations and al-
leviating long-range dependency issues. How-
ever, such structure priors have not been well
exploited in previous work for semantic mod-
eling. To examine their effectiveness, we start
with the Pairwise Word Interaction Model,
one of the best models according to a recent
reproducibility study, then introduce compo-
nents for modeling context and structure us-
ing multi-layer BiLSTMs and TreeLSTMs. In
addition, we introduce residual connections to
the deep convolutional neural network com-
ponent of the model. Extensive evaluations
on eight benchmark datasets show that in-
corporating structural information contributes
to consistent improvements over strong base-
lines.

1 Introduction

Modeling the semantic similarity between a pair
of sentences is a fundamental task in natural lan-
guage processing. It is the core problem of
many tasks such as question answering (He et al.,
2015; Rao et al., 2017; Wang et al., 2018) and
query ranking (Mitra and Craswell, 2019). Re-
cently, various neural networks have been pro-
posed for textual similarity modeling. These mod-
els share three main components: (1) sequential
sentence encoders, which incorporate word con-
text and sentence order for better sentence rep-
resentations, e.g., by using recurrent neural net-
works (RNNs; Mikolov et al., 2010; Seo et al.,
2016), (2) interaction and attention mechanisms,
which use the encoding outputs of sentences to
calculate or reweight salient word pair interac-
tions (He and Lin, 2016; Chen et al., 2017), and
(3) incorporating syntactic parsing information as

an intuitive structure prior for sentence modeling
(Chen and Manning, 2014; Zhao et al., 2016; Chen
et al., 2017).

Our work is inspired by the recent reproducibil-
ity study by Lan and Xu (2018), which examines
many neural network architectures for semantic
similarity modeling through extensive evaluations
on multiple benchmark datasets. Their results sug-
gest that syntactic structure information captured
by a TreeLSTM encoder either provides few ben-
efits or even hurts performance. Structure infor-
mation has often been overlooked in recent seman-
tic modeling methods, such as InferSent (Conneau
et al., 2017), DecAtt (Parikh et al., 2016), and
BiMPM (Wang et al., 2017). It is not yet clear
whether the syntactic structures implicitly cap-
tured by sequential modeling of texts from large
annotated data or existing structure modeling tech-
niques (Tai et al., 2015; Kipf and Welling, 2017)
are effective in learning tree representations.

To further explore the effects of tree structures
in sentence modeling, we start with the Pairwise
Word Interaction Model (PWIM) of He and Lin
(2016) as our base architecture, which has shown
strong performance on various datasets from Lan
and Xu (2018). In summary, PWIM uses a Bi-
LSTM to learn word-level context vectors from
both input sentences and builds a novel similar-
ity focus layer with pairwise metrics to identify
important word pairs. It then converts the simi-
larity measurement problem to a pattern recogni-
tion problem for the final classification. We argue
that PWIM approaches semantic modeling from a
word-level matching perspective, and hence fails
to capture syntactic and contextual semantics. To
this end, we add multi-layer BiLSTMs with short-
cut connections to capture long-range context, as
well as TreeLSTM encoders to capture the syntac-
tic structure of sentences.

We conduct thorough evaluations across eight



1205

datasets in four NLP tasks: paraphrase identifica-
tion, semantic textual similarity, natural language
inference, and answer sentence selection. Exten-
sive experiments show that our proposed com-
ponents lead to consistent improvements against
PWIM and other strong baselines, suggesting that
incorporating contextual and syntactic structures
can help semantic modeling. Our improved model
achieves competitive numbers on eight datasets,
and we open-source our code to improve repro-
ducibility and to facilitate future research.1

2 Methods

2.1 The Pairwise Word Interaction Model
The Pairwise Word Interaction model (He and Lin,
2016) captures fine-grained word-level informa-
tion to measure textual similarity. They use a Bi-
LSTM for context modeling, where the word at
time step t is encoded as a forward hidden state
hfort and a backward hidden state h

back
t . Pairwise

word interactions are modeled through a multi-
metric comparison unit coU , which computes co-
sine distance, L2 distance, and dot-product dis-
tance over two hidden states. This comparison
unit is applied to not only the forward and back-
ward hidden states hfort and h

back
t , but also their

concatenation
←→
ht = [h

for
t , h

back
t ] and summation

h+t = h
for
t + h

back
t .

The output is a similarity tensor of size
R13×|sent1|×|sent2| with one extra dimension for
the padding indicator. Instead of using attention
weight vectors or weighted representations, He
and Lin apply a focus layer on the similarity ten-
sor to decrease the weights of unimportant word
interactions by a factor of ten. They then consider
the tensor as an “image” with 13 channels and use
a 19-layer-deep convolutional neural network to
predict the final classification.

2.2 Residual Connections
Since He and Lin (2016) phrase the similarity
measurement problem as a pattern recognition
(image processing) problem and apply deep con-
volutional neural networks, we explore the addi-
tion of residual connections (He et al., 2016) to
deal with the potential vanishing gradient prob-
lems in deep networks. A building block is defined
as y = f(x,Wi) + x, where x, y are the input and
output of the layer considered, and f(x,Wi) is the
learned residual mapping.

1https://github.com/likicode/spwim

2.3 Multi-layer BiLSTM Sentence Encoders
We use multiple stacked, bi-directional LSTM lay-
ers with shortcut connections, similar to Nie and
Bansal (2017). In this architecture, the input se-
quences of the ith BiLSTM layer are the concate-
nated outputs of all the previous layers and the
initial word embedding sequences. Let W =
{w1, w2, ..., wn} represent the word embeddings
associated with each word in the source sentence.
Define the output of the ith BiLSTM layer at time
t as hit = BiLSTM

i(xit). Then, the input of the i
th

BiLSTM layer at time t is:

x1t = wt, x
i
t = [wt, h

1
t , ..., h

i−2
t , h

i−1
t ](i > 1)

Differing from the original paper, we directly use
the output of the last BiLSTM layer to encode the
sentence v = (hm1 , h

m
2 , ...h

m
n ), where n denotes

the length of the sentence and m the number of
BiLSTM layers. In our experiments we setm = 3.

2.4 Hybrid Inference Model with Parse Trees
While stacked BiLSTMs capture long-term de-
pendency and contextual information over each
sentence, we are also interested in investigat-
ing explicit hierarchical relationships among lin-
guistic phrases and clauses. To incorporate this
domain-specific information, we use the Depen-
dency TreeLSTM (Tai et al., 2015), whose nodes
condition their components on the sum of the hid-
den states of their children. Suppose hL and hR
are the sentence representations in the pair over the
parse tree of each sentence: to model similarity, we
compute the element-wise product hL � hR and
absolute difference |hL − hR|. Then, we feed the
two similarity vectors to a fully-connected layer
with softmax whose output is the probability dis-
tribution over labels. To compute the final label for
the sentence pair, we interpolate between the out-
put probabilities of this model and those of PWIM.
Chen et al. (2017) also incorporate tree structures
produced by a constituency parser into the ESIM
model, then average the predicted probabilities.

3 Experimental Setup

We conducted experiments on eight separate
datasets—one natural language inference dataset,
two paraphrase identification datasets, three
SemEval competition datasets, and two QA
datasets—which are as follows:

• SNLI (Bowman et al., 2015) is a collection
of 570k manually-labeled sentence pairs for

https://github.com/likicode/spwim


1206

Dataset
SNLI Quora Twitter PIT-2015 STS-2014 WikiQA TrecQA SICK

Acc Acc F1 F1 Pearson’s r MAP/MRR MAP/MRR Pearson’s r/ρ

InferSent 0.846 0.866 0.746 0.451 0.715 0.287/0.287 0.521/0.559 -
SSE 0.855 0.878 0.650 0.422 0.378 0.624/0.638 0.628/0.670 -
DecAtt 0.856 0.845 0.652 0.430 0.317 0.603/0.619 0.660/0.712 -
ESIMtree 0.864 0.755 0.740 0.447 0.493 0.618/0.633 0.698/0.734 -
ESIMseq 0.870 0.850 0.748 0.520 0.602 0.652/0.664 0.771/0.795 -
ESIMseq+tree 0.871 0.854 0.759 0.538 0.589 0.647/0.658 0.749/0.768 -

PWIMour 0.822 0.853 0.745 0.602 0.695 0.709/0.723 0.759/0.822 0.871/0.809
mPWIMseq 0.851 0.862 0.757 0.612 0.714 0.717/0.728 0.774/0.835 0.878/0.821
mPWIMseq+tree 0.855 0.870 0.743 0.623 0.718 0.735/0.751 0.781/0.821 0.887/0.834

Abs increase (%) 3.3 1.7 - 2.1 2.3 2.6/2.8 2.2/- 1.6/2.5

Table 1: Test results on different datasets.

the task of natural language inference. The
relationship between two sentences includes
entailment, contradiction, and neutral.

• Quora (Iyer et al., 2017) consists of 400k
question pairs collected from the Quora web-
site, with binary labels indicating if they are
duplicates of each other.

• Twitter-URL (Lan et al., 2017) is a para-
phrase corpus with 50k sentence pairs.

• PIT-2015 (Xu et al., 2015) is a para-
phrase dataset that comes from SemEval-
2015 Task 1.

• STS-2014 (Agirre et al., 2014) comes from
SemEval-2014 Task 10 and each pair of sen-
tences has a similarity score ∈ [0, 5].

• WikiQA (Yang et al., 2015) is an open-
domain question-answering dataset. Af-
ter applying the same pre-processing meth-
ods in He and Lin (2016), it contains 12k
question-answer pairs with binary labels.

• TrecQA (Wang et al., 2007) is from the Text
Retrieval Conferences and consists of 56k
question-answer pairs.

• SICK (Marelli et al., 2014) comes from
SemEval-2014 Task 1 with 10k annotated
sentence pairs. Each pair has a similarity
score ∈ [1, 5].

The first seven datasets are the same as the
ones examined in Lan and Xu (2018), except for
MNLI (Williams et al., 2018), since SNLI is much
larger than MNLI for the task of natural language

inference. We also add the SICK dataset (Marelli
et al., 2014), which is unexplored in Lan and
Xu (2018). Across multiple tasks and domains,
we systematically compare our proposed mod-
els with state-of-the-art neural models: InferSent
(Conneau et al., 2017), Shortcut-stacked Sentence
Encoder (SSE; Nie and Bansal, 2017), Decom-
posable Attention Model (DecAtt; Parikh et al.,
2016), and Enhanced Sequential Inference Model
(ESIM; Chen et al., 2017).

For our experiments on SNLI, Quora, Twitter-
URL, PIT-2015, WikiQA and TrecQA, the train-
ing objective is to minimize the NLL loss. For
STS-2014 and SICK datasets, we use the KL di-
vergence loss. Following He and Lin (2016), for
all cases, we use the RMSProp optimizer (Tiele-
man and Hinton, 2012). Our word representations
use 300-dimensional GloVe word vectors (Pen-
nington et al., 2014), which we make static in
all experiments. We produce dependency parse
trees for each sentence using the Stanford Neural
Network Dependency Parser (Chen and Manning,
2014). The TreeLSTM then models sentence rep-
resentations over each sentence’s parse tree.

4 Results and Analysis

Table 1 shows the results of our models on dif-
ferent datasets. The first block of the table con-
tains figures copied directly from Lan and Xu
(2018); note that they do not use SICK. PWIMour
refers to our own implementation. Note that there
are at least three independent open-source imple-
mentations of the PWIM base model that we are
aware of, which confirms the robustness and re-
producibility of the model. Most results from
these implementations are consistent; however, for



1207

(a) mPWIMseq (b) mPWIMseq+tree (c) mPWIMseq (d) mPWIMseq+tree

Figure 1: Visualization of cosine values in the focusCube of two sentence pairs in the SICK test set.

PIT-2015 and STS-2014, we observe some differ-
ences, which we were unable to reconcile even af-
ter contacting the previous authors. Thus, for com-
parison purposes, we report results from our base
PWIMour implementation.

4.1 Effects of the Multi-Layer BiLSTM

The entry mPWIMseq denotes PWIM using multi-
layer BiLSTMs for modeling the context of the in-
put sentences and also incorporating residual con-
nections in the final classification. On all datasets
listed in the table, adding multi-layer BiLSTMs
leads to a higher performance than that of the orig-
inal model PWIMour.

SSE (Nie and Bansal, 2017) is a stacked Bi-
LSTM model with shortcut connections and fine-
tuning of word embeddings. Unlike our setting,
where each word is represented by its own hidden
state in the final output layer, SSE applies max-
pooling over time to the output of the last BiLSTM
layer to extract the final sentence feature vector.
Based on Table 1, mPWIMseq clearly outperforms
SSE on Twitter, PIT-2015, STS-2014, WikiQA,
and TrecQA. However, for the SNLI and Quora
datasets, SSE slightly exceeds mPWIM by 0.4%
and 1.6%, respectively. SNLI and Quora have the
largest training data among all the datasets with
550k and 393k training sentence pairs, respec-
tively, which suggests that SSE performs better on
larger data beyond a certain threshold. We surmise
that as the dataset increases in size, the simplicity
of SSE will have more performance advantages.

4.2 Effects of TreeLSTM

The mPWIMseq+tree further enhances mPWIMseq
by incorporating syntactic TreeLSTMs based on
syntactic parse trees of each sentence. It aver-
ages the prediction probabilities of the PWIM us-
ing multi-layer BiLSTMs and TreeLSTMs sepa-
rately to arrive at the final label. ESIMseq+tree also
computes its final predictions by averaging predic-

tion probabilities of two ESIM variants that use
BiLSTMs and TreeLSTMs as sentence encoders,
respectively.

From the table, we observe that adding Tree-
LSTMs to the ESIM model only marginally helps
or has no effect for most datasets. On the
other hand, TreeLSTM complements PWIM well:
for WikiQA, it increases mean average precision
(MAP) by 1.8% and mean reciprocal rank (MRR)
by 2.3%. TreeLSTM also contributes to an 1.1%
increase in the F1-measure for PIT-2015, 0.9%
Pearson’s r for SICK, and 0.7% MAP for TrecQA.

We hypothesize that these observed differences
can be attributed to the model architectures. The
inference model of ESIM is based on chain
LSTMs, which might encode overlapping infor-
mation with TreeLSTMs. For PWIM, the sen-
tence context information is transformed into pair-
wise word interaction similarity units, and then
a 19-layer-deep CNN exploits the spatially local-
ized patterns. During this process, its focus is
word-level similarities in sentences. The syntactic
parsing structure introduced by TreeLSTM com-
pensates for some of the information deficiencies.
Notably, TreeLSTM does not help PWIM on the
Twitter dataset; this makes sense, as Kong et al.
(2014) note that many elements in tweets have no
syntactic function, including hashtags and URLs.
Furthermore, tweets often contain multiple frag-
ments, each with its own syntactic span. Both of
these issues may degrade the quality of the syntac-
tic modeling of tweets.

4.3 Sample Visualization and Analysis

To better understand why our models achieve im-
proved effectiveness, we visualize the cosine val-
ues of the focusCube (the final output of the sim-
ilarity layer) for pairwise word interactions in
mPWIMseq and mPWIMseq+tree, using the same
method as Chen et al. (2017), where darker colors
indicate stronger pairwise word interactions.



1208

In Figure 1, we show visualizations from two
pairs of sentences from SICK: 1a and 1b form a
contrastive pair, as do 1c and 1d. We see that,
in both cases, the TreeLSTM helps the model
find syntactically important pairwise word inter-
actions. For example, in Figure 1a, for the
mPWIMseq model, the cluster of dark patches
near the top shows obviously irrelevant correspon-
dences, e.g., “on” with “comfortably”, “dead”
with “tree”, and several of the articles are mis-
aligned with respect to their positions in phrase
structure. With the incorporation of syntactic in-
formation in Figure 1b, the correspondences are
much more accurate.

We see that this is similarly the case when com-
paring Figures 1c and 1d, where the TreeLSTM
yields more accurate correspondences. With the
TreeLSTM, the model has learned the correct
correspondence between “is being jumped” and
“is jumping over”, whereas without the syntactic
structure, the correspondences are quite muddled.
In both cases, we observe that mPWIMseq+tree is
able to capture the passive construction for para-
phrase detection.

5 Conclusion

We examine the hypothesis of whether incorpo-
rating contextual and syntactic structures can im-
prove semantic similarity modeling. We extend
the strong PWIM model and add additional com-
ponents comprised of TreeLSTMs and multi-layer
BiLSTMs to capture syntax and context infor-
mation. Thorough experiments on eight datasets
show that our improved models achieve consistent
gains in effectiveness.

Acknowledgments

This research was supported by the Natu-
ral Sciences and Engineering Research Council
(NSERC) of Canada, and enabled by computa-
tional resources provided by Compute Ontario and
Compute Canada.

References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel

Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 task 10: multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014), pages 81–91.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017. Enhanced LSTM for
natural language inference. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1657–1668.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680.

Hua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi-
perspective sentence similarity modeling with con-
volutional neural networks. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1576–1586, Lisbon,
Portugal.

Hua He and Jimmy Lin. 2016. Pairwise word interac-
tion modeling with deep neural networks for seman-
tic similarity measurement. In Proceedings of the
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 937–948.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages
770–778.

Shankar Iyer, Nikhil Dandekar, and Kornél Csernai.
2017. First Quora Dataset Release: Question Pairs.

Thomas N. Kipf and Max Welling. 2017. Semi-
supervised classification with graph convolutional
networks. In Proceedings of the 5th International
Conference on Learning Representations.

Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
tweets. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1001–1012.

Wuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017.
A continuously growing dataset of sentential para-
phrases. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1224–1234.

https://doi.org/10.3115/v1/S14-2010
https://doi.org/10.3115/v1/S14-2010
https://doi.org/10.18653/v1/D15-1075
https://doi.org/10.18653/v1/D15-1075
https://doi.org/10.3115/v1/D14-1082
https://doi.org/10.3115/v1/D14-1082
https://doi.org/10.3115/v1/D14-1082
https://doi.org/10.18653/v1/P17-1152
https://doi.org/10.18653/v1/P17-1152
https://doi.org/10.18653/v1/D17-1070
https://doi.org/10.18653/v1/D17-1070
https://doi.org/10.18653/v1/D17-1070
https://doi.org/10.18653/v1/D15-1181
https://doi.org/10.18653/v1/D15-1181
https://doi.org/10.18653/v1/D15-1181
https://doi.org/10.18653/v1/N16-1108
https://doi.org/10.18653/v1/N16-1108
https://doi.org/10.18653/v1/N16-1108
https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs
https://doi.org/10.3115/v1/D14-1108
https://doi.org/10.3115/v1/D14-1108
https://doi.org/10.18653/v1/D17-1126
https://doi.org/10.18653/v1/D17-1126


1209

Wuwei Lan and Wei Xu. 2018. Neural network mod-
els for paraphrase identification, semantic textual
similarity, natural language inference, and ques-
tion answering. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 3890–3902.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. SemEval-2014 task 1: evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014),
pages 1–8.

Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan
Cernocký, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Proceed-
ings of the 11th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH), pages 1045–1048.

Bhaskar Mitra and Nick Craswell. 2019. An intro-
duction to neural information retrieval. Foundations
and Trends in Information Retrieval, 13(1):1–126.

Yixin Nie and Mohit Bansal. 2017. Shortcut-stacked
sentence encoders for multi-domain inference. In
Proceedings of the 2nd Workshop on Evaluating
Vector Space Representations for NLP, pages 41–45.

Ankur Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 2249–2255.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: global vectors for word rep-
resentation. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 1532–1543.

Jinfeng Rao, Hua He, and Jimmy Lin. 2017. Experi-
ments with convolutional neural network models for
answer selection. In Proceedings of the 40th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR
2017), pages 1217–1220, Tokyo, Japan.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi,
and Hannaneh Hajishirzi. 2016. Bidirectional
attention flow for machine comprehension.
arXiv:1611.01603.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1556–1566.

Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture
6.5-RMSProp: divide the gradient by a running aver-
age of its recent magnitude. Coursera: Neural Net-
works for Machine Learning, 4(2):26–31.

Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? A
quasi-synchronous grammar for QA. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 22–32.

Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,
Tim Klinger, Wei Zhang, Shiyu Chang, Gerry
Tesauro, Bowen Zhou, and Jing Jiang. 2018. R3:
Reinforced ranker-reader for open-domain ques-
tion answering. In Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence
(AAAI-18), pages 5981–5988.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences. In Proceedings of the 26th Inter-
national Joint Conference on Artificial Intelligence,
pages 4144–4150.

Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), pages 1112–1122.

Wei Xu, Chris Callison-Burch, and Bill Dolan. 2015.
SemEval-2015 task 1: paraphrase and semantic sim-
ilarity in Twitter (PIT). In Proceedings of the 9th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2015), pages 1–11.

Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
WikiQA: a challenge dataset for open-domain ques-
tion answering. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2013–2018.

Kai Zhao, Liang Huang, and Mingbo Ma. 2016. Tex-
tual entailment with structured attentions and com-
position. In Proceedings of COLING 2016, the
26th International Conference on Computational
Linguistics: Technical Papers, pages 2248–2258.

https://www.aclweb.org/anthology/C18-1328
https://www.aclweb.org/anthology/C18-1328
https://www.aclweb.org/anthology/C18-1328
https://www.aclweb.org/anthology/C18-1328
https://doi.org/10.3115/v1/S14-2001
https://doi.org/10.3115/v1/S14-2001
https://doi.org/10.3115/v1/S14-2001
https://doi.org/10.3115/v1/S14-2001
http://www.isca-speech.org/archive/interspeech_2010/i10_1045.html
http://www.isca-speech.org/archive/interspeech_2010/i10_1045.html
https://doi.org/10.18653/v1/W17-5308
https://doi.org/10.18653/v1/W17-5308
https://doi.org/10.18653/v1/D16-1244
https://doi.org/10.18653/v1/D16-1244
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/P15-1150
https://doi.org/10.3115/v1/P15-1150
https://doi.org/10.3115/v1/P15-1150
https://www.aclweb.org/anthology/D07-1003
https://www.aclweb.org/anthology/D07-1003
https://doi.org/10.18653/v1/N18-1101
https://doi.org/10.18653/v1/N18-1101
https://doi.org/10.18653/v1/S15-2001
https://doi.org/10.18653/v1/S15-2001
https://doi.org/10.18653/v1/D15-1237
https://doi.org/10.18653/v1/D15-1237
https://www.aclweb.org/anthology/C16-1212
https://www.aclweb.org/anthology/C16-1212
https://www.aclweb.org/anthology/C16-1212

