



















































Learning Kernels for Semantic Clustering: A Deep Approach


Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 79–87,
Denver, Colorado, June 1, 2015. c©2015 Association for Computational Linguistics

Learning Kernels for Semantic Clustering: A Deep Approach

Ignacio Arroyo-Fernández
Universidad Nacional Autónoma de México (UNAM)

iarroyof@iingen.unam.mx

Abstract

In this thesis proposal we present a novel
semantic embedding method, which aims at
consistently performing semantic clustering at
sentence level. Taking into account special
aspects of Vector Space Models (VSMs), we
propose to learn reproducing kernels in clas-
sification tasks. By this way, capturing spec-
tral features from data is possible. These fea-
tures make it theoretically plausible to model
semantic similarity criteria in Hilbert spaces,
i.e. the embedding spaces. We could im-
prove the semantic assessment over embed-
dings, which are criterion-derived representa-
tions from traditional semantic vectors. The
learned kernel could be easily transferred to
clustering methods, where the Multi-Class Im-
balance Problem is considered (e.g. semantic
clustering of definitions of terms).

1 Introduction

Overall in Machine Learning algorithms (Duda et
al., 2012), knowledge is statistically embedded via
the Vector Space Model (VSM), which is also
named the semantic space (Landauer et al., 1998;
Padó and Lapata, 2007; Baroni and Lenci, 2010).
Contrarily to it is usually conceived in text data
analysis (Manning et al., 2009; Aggarwal and Zhai,
2012), not any data set is suitable to embed into `p
metric spaces, including euclidean spaces (p = 2)
(Riesz and Nagy, 1955). This implies that, in par-
ticular, clustering algorithms are being adapted to
some `p-derived metric, but not to semantic vector
sets (clusters) (Qin et al., 2014).

The above implication also means that seman-
tic similarity measures are commonly not consis-
tent, e.g. the cosine similarity or transformation-
based distances (Sidorov et al., 2014). These are
mainly based on the concept of triangle. Thus
if the triangle inequality does not hold (which in-
duces norms for Hilbert spaces exclusively), then
the case of the cosine similarity becomes mathe-
matically inconsistent1. Despite VSMs are some-
times not mathematically analyzed, traditional algo-
rithms work well enough for global semantic anal-
ysis (hereinafter global analysis, i.e. at document
level where Zipf’s law holds). Nevertheless, for lo-
cal analysis (hereinafter local analysis, i.e., at sen-
tence, phrase or word level) the issue remains still
open (Mikolov et al., 2013).

In this thesis proposal, we will address the main
difficulties raised from traditional VSMs for local
analysis of text data. We consider the latter as an ill-
posed problem (which implies unstable algorithms)
in the sense of some explicit semantic similarity cri-
terion (hereinafter criterion), e.g. topic, concept,
etc. (Vapnik, 1998; Fernandez et al., 2007). The fol-
lowing feasible reformulation is proposed. By learn-
ing a kernel in classification tasks, we want to induce
an embedding space (Lanckriet et al., 2004; Cortes
et al., 2009). In this space, we will consider rele-
vance (weighting) of spectral features of data, which
are in turn related to the shape of semantic vector
sets (Xiong et al., 2014). These vectors would be
derived from different Statistical Language Models
(SLMs); i.e. countable things, e.g. n-grams, bag-of-
words (BoW), etc.; which in turn encode language

1Riesz (1955) gives details about Hilbert spaces.

79



aspects (e.g. semantics, syntax, morphology, etc.).
Learned kernels are susceptible to be transferred to
clustering methods (Yosinski et al., 2014; Bengio et
al., 2014), where spectral features would be properly
filtered from text (Gu et al., 2011).

When both learning and clustering processes are
performed, the kernel approach is tolerant enough
for data scarcity. Thus, eventually, we could have
any criterion-derived amount of semantic clusters
regardless of the Multi-Class Imbalance Problem
(MCIP) (Sugiyama and Kawanabe, 2012). It is a
rarely studied problem in Natural Language Pro-
cessing (NLP), however, contributions can be help-
ful in a number of tasks such as IE, topic modeling,
QA systems, opinion mining, Natural Language Un-
derstanding, etc.

This paper is organized as follows: In Section
2 we show our case study. In Section 3 we show
the embedding framework. In Section 4 we present
our learning problem. Sections 5 and 6 respectively
show research directions and related work. In Sec-
tion 7, conclusions and future work are presented.

2 A case study and background
A case study. Semantic clustering of definitions of
terms is our case study. See the next extracted2 ex-
amples for the terms window and mouse. For each
of them, the main acception is showed first, and af-
terwards three secondary acceptions:

1. A window is a frame including a sheet of glass or other
material capable of admitting light...

(a) The window is the time elapsed since a passenger
calls to schedule...

(b) A window is a sequence region of 20-codon length
on an alignment of homologous genes...

(c) A window is any GUI element and is usually iden-
tified by a Windows handle...

2. A mouse is a mammal classified in the order Rodentia,
suborder Sciurognathi....

(a) A mouse is a small object you can roll along a
hard, flat surface...

(b) A mouse is a handheld pointing device used to
position a cursor on a computer...

(c) The Mouse is a fictional character in Alice’s Ad-
ventures in Wonderland by Lewis Carroll...

In the example 1, it is possible to assign the four
acceptions to four different semantic groups (the
window (1), transport services (1a), genetics (1b)

2www.describe.com.mx

and computing (1c)) by using lexical features (bold
terms). This example also indicates how abstract
concepts are always latent in the definitions. The
example 2 is a bit more complex. Unlike to example
1, there would be three clusters because there are
two semantically similar acceptions (2a and 2b are
related to computing). However, they are lexically
very distant. See that in both examples the amount
of semantic clusters can’t be defined a priory (un-
like to Wikipedia). Additionally, it is impossible to
know what topic the users of an IE system could be
interested in. These issues, point out the need for an-
alyzing the way we are currently treating semantic
spaces in the sense of stability of algorithms (Vap-
nik, 1998), i.e. the existence of semantic similarity
consistence, although Zipf’s law scarcely holds (e.g.
in local analysis).

Semantic spaces and embeddings. Erk (2012)
and Brychcı́n (2014) showed insightful empiricism
about well known semantic spaces for different
cases in global analysis. In this work we have spe-
cial interest in local analysis, where semantic vec-
tors are representations (embeddings) derived from
learned feature maps for specific semantic assess-
ments (Mitchell and Lapata, 2010). These feature
maps are commonly encoded in Artificial Neural
Networks (ANNs) (Kalchbrenner et al., 2014).

ANNs have recently attracted worldwide atten-
tion. Given their surprising adaptability to unknown
distributions, they are used in NLP for embedding
and feature learning in local analysis, i.e. Deep
Learning (DL) (Socher et al., 2011; Socher et al.,
2013). However, we require knowledge transfer to-
wards clustering tasks. It is still not feasible by using
ANNs (Yosinski et al., 2014). Thus, theoretical ac-
cess becomes ever more necessary, so it is worth ex-
tending Kernel Learning (KL) studies as alternative
feature learning method in NLP (Lanckriet et al.,
2004). Measuring subtle semantic displacements,
according to a criterion, is theoretically attainable in
a well defined (learned) reproducing kernel Hilbert
space (RKHS), e.g. some subset of L2 (Aronszajn,
1950). In these spaces, features are latent abstrac-
tion levels3 of data spectrum, which improves kernel
scaling (Dai et al., 2014; Anandkumar et al., 2014).

3Mainly in DL, it is known there are different hierarchies of
generality of features learned by a learning machine.

80



Figure 1: General schema of the transformation frame-
work from some traditional VSM (left) to a well defined
embedding space (right).

3 RKHS and semantic embeddings

We propose mapping sets of semantic vectors (e.g.
BoW) into well defined function spaces (RKHSs),
prior to directly endowing such sets (not elliptical or
at least convex (Qin et al., 2014)) with the euclidean
norm, ‖.‖2 (see Figure 1). For the aforesaid purpose,
we want to take advantage of the RKHSs.

Any semantic vector xo ∈ X could be consis-
tently embedded (transformed) into a well defined
Hilbert space by using the reproducing property of a
kernel k(·, ·) (Shawe-Taylor and Cristianini, 2004):

fxo(x) = 〈f(·), k(·, x)〉H; ∀x ∈ X (1)

where: H ⊂ L2 is a RKHS, fxo(·) ∈ H is the em-
bedding derived from xo, which can be seen as fixed
parameter of k(·, xo) = f(·) ∈ H. This embedding
function is defined over the vector domain {x} ⊂ X
and 〈·, ·〉H : X → H is the inner product inH.

Always that (1) holds, k(·, ·) is a positive definite
(PD) kernel function, so X does not need even to
be a vector space and even then, convergence of any
sequence {fn(x) : fn ∈ H;n ∈ N} can be ensured.
The above is a highly valuable characteristic of the
resulting function space (Smola et al., 2007):

lim
n→∞ fn = f ⇐⇒ limn→∞ kn(·, x) = k(·, x). (2)

The result (2) implies that convergence of summa-
tion of initial guessing kernel functions kn(·, ·) ∈ H
always occurs, hence talking about the existence of

a suitable kernel function k(·, ·) ∈ H in (1) is ab-
solutely possible. It means that L2 operations can
be consistently applied, e.g. the usual norm ‖ · ‖2,
trigonometric functions (e.g. cos θ) and distance
d2 = ‖fn − fm‖2 : m 6= n. Thus, from right side
of (2), in order that (1) holds convergence of the
Fourier series decomposition of k(·, ·) towards the
spectrum of desired features from data is necessary;
i.e., by learning parameters and hyperparameters4 of
the series (Ong et al., 2005; Băzăvan et al., 2012).

3.1 Learnable kernels for language features

Assume (1) and (2) hold. For some SLM a encoded
in a traditional semantic space, it is possible to de-
fine a learnable kernel matrix Ka as follows (Lanck-
riet et al., 2004; Cortes et al., 2009):

Ka :=
p∑
i=1

βiKi, (3)

where {Ki}pi=1 ⊂ K is the set of p initial guess-
ing kernel matrices (belonging to the family K, e.g.
Gaussian) with fixed hyperparameters and βi’s are
parameters weighting Ki’s. Please note that, for
simplicity, we are using matrices associated to ker-
nel functions ki(·, ·), ka(·, ·) ∈ H, respectively.

In the Fourier domain and bandwidth. In fact
(3) is a Fourier series, where βi’s are decomposi-
tion coefficients of Ka (Băzăvan et al., 2012). This
kernel would be fitting the spectrum of some SLM
that encodes some latent language aspect from text
(Landauer et al., 1998). On one hand, in Fourier
domain operations (e.g. the error vector norm) are
closed inL2, i.e., according to (2) convergence is en-
sured as a Hilbert space is well defined. Moreover,
the L2-regularizer is convex in terms of the Fourier
series coefficients (Cortes et al., 2009). The afore-
mentioned facts imply benefits in terms of compu-
tational complexity (scaling) and precision (Dai et
al., 2014). On the other hand, hyperparameters of
initial guessing kernels are learnable for detecting
the bandwitdh of data (Ong et al., 2005; Băzăvan et
al., 2012; Xiong et al., 2014). Eventually, the lat-
ter fact would lead us to know (learning) bounds for

4So called in order to make distinction between weights
(kernel parameters or coefficients) and the basis function pa-
rameters (hyperparameters), e.g. mean and variance.

81



the necessary amount of data to properly train our
model (the Nyquist theorem).

Cluster shape. A common shape among clusters
is considered even for unseen clusters with differ-
ent, independent and imbalanced prior probability
densities (Vapnik, 1998; Sugiyama and Kawanabe,
2012). For example, if data is Guassian-distributed
in the input space, then shape of different clusters
tend to be elliptical (the utopian `2 case), although
their densities are not regular or even very imbal-
anced. Higher abstraction levels of the data spec-
trum possess mentioned traits (Ranzato et al., 2007;
Baktashmotlagh et al., 2013). We will suggest below
a more general version of (3), thereby considering
higher abstraction levels of text data.

4 Learning our kernel in a RKHS

A transducer is a setting for learning parameters and
hyperparameters of a multikernel linear combination
like the Fourier series (3) (Băzăvan et al., 2012).

Overall, the above setting consists on defining
a multi-class learning problem over a RKHS: let
Yθ = {y`}y`∈N be a sequence of targets induc-
ing a semantic criterion θ, likewise a training set
X = {x`}x`∈Rn and a set of initial guessing kernels
{Kσi}pi=1 ⊂ K with the associated hyperparameter
vector σa = {σi}pi=1. Then for some SLM a ∈ A,
we would learn the associated kernel matrix Ka by
optimizing the SLM empirical risk functional:

JA(σa, βa) = LA(Ka,X ,Yθ) + ψ(σa) + ξ(βa),
(4)

where in JA(·, ·) we have:

Ka =
∑

1≤i≤p
βiKσi . (5)

The learning is divided in two interrelated stages:
at the first stage, the free parameter vector βa =
{βi}pi=1 in (5) (a particular version of (3)), is opti-
mized for learning a partial kernel K̂a, given a fixed
(sufficiently small) σa and by using the regularizer
ξ(βa) over the SLM prediction loss LA(·, ·) in (4).
Conversely at the second stage σa is free, thus by
using the regularizer ψ(σa) over the prediction loss
LA(·, ·), given that the optimal β∗a was found at the
first stage, we could have the optimal σ∗a and there-
fore K∗a is selected.

At higher abstraction levels, given the association
{X ,Yθ}, the transducer setting would learn a ker-
nel function that fits a multi-class partition of X via
summation of Ka’s. Thus, we can use learned ker-
nels K∗a as new initial guesses in order to learn a
compound kernel matrix Kθ for a higher abstraction
level:

J (γθ) = L(Kθ,X ,Yθ) + ζ(γθ), (6)
where in the general risk functional J (·) we have:

Kθ =
∑
a∈A

γaK
∗
a . (7)

In (6) the vector γθ = {γa}a∈A weights seman-
tic representations K∗a associated to each SLM and
ζ(γθ) is a proper regularizer over the general loss
L(·, ·). The described learning processes can even
be jointly performed (Băzăvan et al., 2012). The
aforementioned losses and regularizers can be con-
veniently defined (Cortes et al., 2009).

4.1 The learned kernel function
In order to make relevant features to emerge from
text, we would use our learned kernel K∗θ . Thus if
{γ∗θ , {β∗a, σ∗a}a∈A} is the solution set of the learn-
ing problems (4) and (6), then combining (5) and (7)
gives the embedding kernel function, for |A| differ-
ent SLMs as required (see Figure 2):

Definition 1. Given a semantic criterion θ, then the
learned parameters {γ∗θ , {β∗a, σ∗a}a∈A} are eigen-
values of kernels {K∗a}a∈A ≺ K∗θ , respectively5.
Thus according to (1), we have for any semantic vec-
tor xo ∈ X its representation fxo(x) ∈ H:

fxo(x) :=
∑
a∈A

p∑
i=1

γ∗aβ
∗
i ki(x, xo)

= kθ(x, xo) ≈ K∗θxo.
(8)

In (8), ki(·, ·), kθ(·, ·) ∈ H ⊂ L2 are reproducing
kernel functions associated to matrices Kσi and Kθ,
respectively. The associated {σ∗a}a∈A would be op-
timally fitting the bandwidth of data. X ⊃ X is a
compounding semantic space from different SLMs

5(i) The symbol ‘≺’ denotes subordination (from right to
left) between operators, i.e. hierarchy of abstraction levels. (ii)
See (Shawe-Taylor and Cristianini, 2004; Anandkumar et al.,
2014) for details about eigendecompositions.

82



Figure 2: Sketch (bold plot) of the abstraction levels of
some learned kernel function kθ(·.·) ∈ H ⊂ L2.

a ∈ A (Băzăvan et al., 2012). According to θ, se-
mantic clustering could be consistently performed in
H by computing any L2 similarity measure between
embeddings {fxn , fxm}, which are derived from any
semantic vectors xn, xm ∈ X , e.g. (i) the kernel
correlation coefficient ρθ = µkθ(xn, xm) ∈ [0, 1];
with µ = 1‖fxn‖‖fxm‖ , and (ii) the distance by sim-
ply computing d2 = ‖fxn − fxm‖2.

Please note that we could extend Definition 1 to
deeper levels (layers) associated to abstraction lev-
els of SLMs. These levels could explicitly encode
morphology, syntax, semantics or compositional se-
mantics, i.e. {Ka}a∈A = KSLMs ≺ Kaspects.

5 Research directions

Our main research direction is to address in detail
linguistic interpretations associated to second mem-
ber of (8), which is still not clear. There are poten-
tial ways of interpreting pooling operations over the
expansion of either eigenvalues or eigenfunctions of
fxo(·). This fact could lead us to an alternative way
of analyzing written language, i.e. in terms of the
spectral decomposition of X given θ.

As another direction we consider data scarcity
(low annotated resources). It is a well handled issue
by spectral approaches like the proposed one, so it
is worth investigating hyperparameter learning tech-
niques. We consider hyperparameters as the lowest
abstraction level of the learned kernel and they are
aimed at data bandwidth estimation (i.e. by tuning
the σi associated to each ki(·, ·) in (8)). This esti-
mation could help us to try to answer the question
of how much training data is enough. This ques-
tion is also related to the quality bounds of a learned
kernel. These bounds could be used to investigate
the possible relation among the number of annotated

clusters, the training set size and the generalization
ability. The latter would be provided (transferred)
by the learned kernel to a common clustering algo-
rithm for discovering imbalanced unseen semantic
clusters. We are planning to perform the above por-
trayed experiments at least for a couple of semantic
criteria6, including term acception discovering (Sec-
tion 2). Nevertheless, much remains to be done.

6 Related work

Clustering of definitional contexts. Molina (2009)
processed snippets containing definitions of terms
(Sierra, 2009). The obtained PD matrix is not more
than a homogeneous quadratic kernel that induces a
Hilbert space: The Textual Energy of data (Fernan-
dez et al., 2007; Torres-Moreno et al., 2010). Hi-
erarchical clustering is performed over the resulting
space, but some semantic criterion was not consid-
ered. Thus, such as Cigarran (2008), they ranked re-
trieved documents by simply relying on lexical fea-
tures (global analysis). ML analysis was not per-
formed, so their approach suffers from high sensibil-
ity to lexical changes (instability) in local analysis.

Paraphrase extraction from definitional sen-
tences. Hashimoto, et.al. (2011) and Yan, et.al.
(2013) engineered vectors from contextual, syntac-
tical and lexical features of definitional sentence
paraphrases (similarly to Lapata (2007) and Ferrone
(2014)). As training data they used a POS anno-
tated corpus of sentences that contain noun phrases.
It was trained a binary SVM aimed at both para-
phrase detection and multi-word term equivalence
assertion (Choi and Myaeng, 2012; Abend et al.,
2014). More complex constructions were not con-
sidered, but their feature mixure performs very well.

Socher et al., (2011) used ANNs for paraphrase
detection. According to labeling, the network unsu-
pervisedly capture as many language features as la-
tent in data (Kalchbrenner et al., 2014). The network
supervisedly learns to represent desired contents in-
side phrases (Mikolov et al., 2013); thus paraphrase
detection is highly generalized. Nevertheless, it is
notable the necessity of a tree parser. Unlike to
(Socher et al., 2013), the network must to learn syn-
tactic features separately.

6For example: SemEval-2014; Semantic Evaluation Exer-
cises.

83



Definitional answer ranking. Fegueroa (2012)
and (2014) proposed to represent definitional an-
swers by a Context Language Model (CLM), i.e. a
Markovian process as probabilistic language model.
A knowledge base (WordNET) is used as an an-
notated corpus of specific domains (limited to
Wikipedia). Unlike to our approach, queries must
be previously disambiguated; for instance: “what is
a computer virus?”, where “computer virus” disam-
biguates “virus”. Answers are classified according
to relevant terms (Mikolov et al., 2013), similarly to
the way topic modeling approaches work (Fernan-
dez et al., 2007; Lau et al., 2014).

Learning kernels for clustering. Overall for
knowledge transfer from classification (source)
tasks to clustering (target) tasks, the state of the art is
not bast. This setting is generally explored by using
toy Gaussian-distributed data and predefined kernels
(Jenssen et al., 2006; Jain et al., 2010). Particularly
for text data, Gu et.al. (2011) addressed the setting
by using multi-task kernels for global analysis. In
their work, it was not necessary neither to discover
clusters nor to model some semantic criterion. Both
them are assumed as a presetting of their analysis,
which differs from our proposal.

Feasibility of KL over DL. We want to perform
clustering over an embedding space. At the best of
our knowledge there exist two dominant approaches
for feature learning: KL and DL. However, knowl-
edge transfer is equally important for us, so both
procedures should be more intuitive by adopting the
KL approach instead of DL. We show the main rea-
sons: (i) Interpretability. The form (8) has been de-
ducted from punctual items (e.g. SLMs encoding
language aspects), which leads us to think that a la-
tent statistical interpretation of language is worthy
of further investigation. (ii) Modularity. Any ker-
nel can be transparently transferred into kernelized
and non-kernelized clustering methods (Schölkopf
et al., 1997; Aguilar-Martin and De Mántaras, 1982;
Ben-Hur et al., 2002). (iii) Mathematical sup-
port. Theoretical access provided by kernel meth-
ods would allow for future work on semantic assess-
ments via increasingly abstract representations. (iv)
Data scarcity. It is one of our principal challenges,
so kernel methods are feasible because of their gen-
eralization predictability (Cortes and Vapnik, 1995).

Regardless of its advantages, our theoretical

framework exhibit latent drawbacks. The main of
them is that feature learning is not fully unsuper-
vised, which suggests the underlying possibility of
preventing learning from some decisive knowledge
related to, mainly, the tractability of the MCIP. Thus,
many empirical studies are pending.

7 Conclusions and future work

At the moment, our theoretical framework analyzes
semantic embedding in the sense of a criterion for
semantic clustering. However, correspondences be-
tween linguistic intuitions and the showed theoret-
ical framework (interpretability) are actually incip-
ient, although we consider these challenging corre-
spondences are described in a generalized way in the
seminal work of Harris (1968). It is encouraging
(not determinant) that our approach can be associ-
ated to his operator hypothesis on composition and
separability of both linguistic entities and language
aspects. That is why we consider it is worth inves-
tigating spectral decomposition methods for NLP as
possible rapprochement to elucidate improvements
in semantic assessments (e.g. semantic clustering).
Thus, by performing this research we also expect to
advance the state of the art in statistical features of
written language.

As immediate future work we are planning to
learn compositional distributional operators (ker-
nels), which can be seen as stable solutions of op-
erator equations (Harris, 1968; Vapnik, 1998). We
would like to investigate this approach for morphol-
ogy, syntax and semantics (Mitchell and Lapata,
2010; Lazaridou et al., 2013). Another future pro-
posal could be derived from the abovementioned ap-
proach (operator learning), i.e. multi-sentence com-
pression for automatic sumarization.

A further extension could be ontology learning. It
would be proposed as a multi-structure KL frame-
work (Ferrone and Zanzotto, 2014). In this case, IE
and knowledge organization would be our main aims
(Anandkumar et al., 2014).

Aknowledgements. This work is funded
by CONACyT Mexico (grant: 350326/178248).
Thanks to the UNAM graduate program in CS.
Thanks to Carlos Méndez-Cruz, to Yang Liu and to
anonymous reviewers for their valuable comments.

84



References
Omri Abend, B. Shay Cohen, and Mark Steedman. 2014.

Lexical inference over multi-word predicates: A distri-
butional approach. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 644–654. ACL.

Charu C. Aggarwal and Cheng Xiang Zhai. 2012. An
introduction to text mining. In Charu C Aggarwal and
ChengXiang Zhai, editors, Mining Text Data, pages 1–
10. Springer US.

J Aguilar-Martin and R De Mántaras. 1982. The process
of classification and learning the meaning of linguis-
tic descriptors of concepts. Approximate Reasoning in
Decision Analysis, 1982:165–175.

Animashree Anandkumar, Rong Ge, Daniel Hsu,
Sham M Kakade, and Matus Telgarsky. 2014. Ten-
sor decompositions for learning latent variable mod-
els. The Journal of Machine Learning Research,
15(1):2773–2832.

Nachman Aronszajn. 1950. Theory of reproducing ker-
nels. Transactions of the American mathematical so-
ciety, pages 337–404.

Mahsa Baktashmotlagh, Mehrtash T Harandi, Brian C
Lovell, and Mathieu Salzmann. 2013. Unsuper-
vised domain adaptation by domain invariant projec-
tion. In Computer Vision (ICCV), 2013 IEEE Interna-
tional Conference on, pages 769–776. IEEE.

Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673–
721.

Eduard Gabriel Băzăvan, Fuxin Li, and Cristian Smin-
chisescu. 2012. Fourier kernel learning. In Computer
Vision–ECCV 2012, pages 459–473. Springer.

Asa Ben-Hur, David Horn, Hava T Siegelmann, and
Vladimir Vapnik. 2002. Support vector clustering.
The Journal of Machine Learning Research, 2:125–
137.

Yoshua Bengio, Ian J. Goodfellow, and Aaron Courville.
2014. Deep learning. Book in preparation for MIT
Press.

Tomás̆ Brychcı́n and Miroslav Konopı́k. 2014. Semantic
spaces for improving language modelling. Computer
Speech and Language, 28:192–209.

Sung-Pil Choi and Sung-Hyon Myaeng. 2012. Termino-
logical paraphrase extraction from scientific literature
based on predicate argument tuples. Journal of Infor-
mation Science, pages 1–19.

Juan Manuel Cigarrán Recuero. 2008. Organización de
resultados de búsqueda mediante análisis formal de
conceptos. Ph.D. thesis, Universidad Nacional de Ed-
ucación a Distancia; Escuela Técnica Superior de In-
genierı́a Informática.

Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273–297.

Corinna Cortes, Mehryar Mohri, and Afshin Ros-
tamizadeh. 2009. L 2 regularization for learning ker-
nels. In Proceedings of the Twenty-Fifth Conference
on Uncertainty in Artificial Intelligence, pages 109–
116. AUAI Press.

Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj,
Maria-Florina F Balcan, and Le Song. 2014. Scalable
kernel methods via doubly stochastic gradients. In
Advances in Neural Information Processing Systems,
pages 3041–3049.

Richard O Duda, Peter E Hart, and David G Stork. 2012.
Pattern classification. John Wiley & Sons.

Katrin Erk. 2012. Vector space models of word meaning
and phrase meaning: A survey. Language and Lin-
guistics Compass, 6(10):635–653.

Silvia Fernandez, Eric San Juan, and Juan-Manuel
Torres-Moreno. 2007. Textual energy of associa-
tive memories: Performant applications of enertex al-
gorithm in text summarization and topic segmenta-
tion. MICAI 2007: Advances in Artificial Intelligence,
pages 861–871.

Lorenzo Ferrone and Fabio Massimo Zanzotto. 2014.
Towards syntax-aware compositional distributional se-
mantic models. In Proceedings of COLING 2014:
Technical Papers, pages 721–730. Dublin City Uni-
versity and Association for Computational Linguistics
(ACL).

Alejandro Figueroa and John Atkinson. 2012. Contex-
tual language models for ranking answers to natural
language definition questions. Computational Intelli-
gence, pages 528–548.

Alejandro Figueroa and Günter Neumann. 2014.
Category-specific models for ranking effective para-
phrases in community question answering. Expert
Systems with Applications, 41(10):4730–4742.

Quanquan Gu, Zhenhui Li, and Jiawei Han. 2011.
Learning a kernel for multi-task clustering. In Pro-
ceedings of the 25th AAAI conference on artificial in-
telligence. Association for the Advancement of Artifi-
cial Intelligence (AAAI).

Zellig S. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York, NY, USA.

Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,
Jun’ichi Kazama, and Sadao Kurohashi. 2011. Ex-
tracting paraphrases from definition sentences on the
web. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 1087–
1097.

Prateek Jain, Brian Kulis, and Inderjit S Dhillon. 2010.
Inductive regularized learning of kernel functions. In

85



Advances in Neural Information Processing Systems,
pages 946–954.

Robert Jenssen, Torbjørn Eltoft, Mark Girolami, and
Deniz Erdogmus. 2006. Kernel maximum entropy
data transformation and an enhanced spectral cluster-
ing algorithm. In Advances in Neural Information
Processing Systems, pages 633–640.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for mod-
elling sentences. arXiv preprint arXiv:1404.2188.

Gert R. G. Lanckriet, Nello Cristianini, Peter Bartlett,
Laurent El Ghaoui, and Michael I. Jordan. 2004.
Learning the kernel matrix with semidefinite program-
ming. J. Mach. Learn. Res., 5:27–72, December.

Thomas K Landauer, Peter W. Foltz, and Darrell Laham.
1998. An introduction to latent semantic analysis.
Discourse Processes, 25(2-3):259–284.

Jey Han Lau, Paul Cook, Diana McCarthy, Spandana
Gella, and Timothy Baldwin. 2014. Learning word
sense distributions, detecting unattested senses and
identifying novel senses using topic models. In Pro-
ceedings of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics, ACL 2014, vol-
ume 1, pages 259–270.

Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli,
and Marco Baroni. 2013. Compositional-ly derived
representations of morphologically complex words in
distributional semantics. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1517–
1526.

Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schütze. 2009. An Introduction to Information
Retrieval. Cambridge UP.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representations
of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems,
pages 3111–3119.

Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(34):1388–1429. Cognitive Science Society, ISSN:
1551-6709.

A Molina. 2009. Agrupamiento semántico de contex-
tos definitorios. Mémoire de Master, Universidad Na-
cional Autónoma de México–Posgrado en Ciencia e
Ingenierı́a de la Computación, México, 108.

Cheng S Ong, Robert C Williamson, and Alex J Smola.
2005. Learning the kernel with hyperkernels. In Jour-
nal of Machine Learning Research, pages 1043–1071.

Sebastian Padó and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2):161–199.

Danfeng Qin, Xuanli Chen, Matthieu Guillaumin, and
Luc V Gool. 2014. Quantized kernel learning for
feature matching. In Advances in Neural Information
Processing Systems, pages 172–180.

M Ranzato, Fu Jie Huang, Y-L Boureau, and Yann Le-
Cun. 2007. Unsupervised learning of invariant feature
hierarchies with applications to object recognition.
In Computer Vision and Pattern Recognition, 2007.
CVPR’07. IEEE Conference on, pages 1–8. IEEE.

F. Riesz and Sz Nagy. 1955. Functional analysis.
Dover Publications, Inc., New York. First published in,
3(6):35.

Bernhard Schölkopf, Alexander Smola, and Klaus-
Robert Müller. 1997. Kernel principal component
analysis. In Artificial Neural Networks–ICANN’97,
pages 583–588. Springer.

Jhon Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge UP. ISBN:
978-0-521-81397-6.

Grigori Sidorov, Alexander Gelbukh, Helena Gómez-
Adorno, and David Pinto. 2014. Soft similarity and
soft cosine measure: Similarity of features in vector
space model. Computación y Sistemas, 18(3).

Gerardo Sierra. 2009. Extracción de contex-
tos definitorios en textos de especialidad a partir
del reconocimiento de patrones lingüı́sticos. Lin-
guaMÁTICA, 2:13–38, Dezembro.

Alex Smola, Arthur Gretton, Le Song, and Bernhard
Schölkopf. 2007. A hilbert space embedding
for distributions. In Algorithmic Learning Theory:
18th International Conference, pages 13–31. Springer-
Verlag.

Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011. Dynamic
pooling and unfolding recursive autoencoders for para-
phrase detection. In Advances in Neural Information
Processing Systems, pages 801–809.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
1631–1642. Citeseer.

M. Sugiyama and M. Kawanabe. 2012. Machine Learn-
ing in Non-stationary Environments: Introduction to
Covariate Shift Adaptation. Adaptive computation
and machine learning. MIT Press.

Juan-Manuel Torres-Moreno, Alejandro Molina, and
Gerardo Sierra. 2010. La energı́a textual como me-
dida de distancia en agrupamiento de definiciones. In
Statistical Analysis of Textual Data, pages 215–226.

Vladimir Naumovich Vapnik. 1998. Statistical learning
theory. Wiley New York.

86



Yuanjun Xiong, Wei Liu, Deli Zhao, and Xiaoou Tang.
2014. Zeta hull pursuits: Learning nonconvex data
hulls. In Advances in Neural Information Processing
Systems, pages 46–54.

Yulan Yan, Chikara Hashimoto, Kentaro Torisawa, Takao
Kawai, Jun’ichi Kazama, and Stijn De Saeger. 2013.
Minimally supervised method for multilingual para-
phrase extraction from definition sentences on the web.
In HLT-NAACL, pages 63–73.

Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lip-
son. 2014. How transferable are features in deep neu-
ral networks? In Advances in Neural Information Pro-
cessing Systems, pages 3320–3328.

87


