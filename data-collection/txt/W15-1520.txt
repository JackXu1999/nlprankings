



















































Neural word embeddings with multiplicative feature interactions for tensor-based compositions


Proceedings of NAACL-HLT 2015, pages 143–150,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Neural word embeddings with multiplicative feature interactions for
tensor-based compositions

Joo-Kyung Kim†, Marie-Catherine de Marneffe‡, Eric Fosler-Lussier†
†Department of Computer Science and Engineering,

‡Department of Linguistics,
The Ohio State University, Columbus, OH 43210, USA

kimjook@cse.ohio-state.edu, mcdm@ling.ohio-state.edu,
fosler@cse.ohio-state.edu

Abstract

Categorical compositional distributional mod-
els unify compositional formal semantic mod-
els and distributional models by composing
phrases with tensor-based methods from vec-
tor representations. For the tensor-based com-
positions, Milajevs et al. (2014) showed that
word vectors obtained from the continuous
bag-of-words (CBOW) model are competitive
with those from co-occurrence based mod-
els. However, because word vectors from the
CBOW model are trained assuming additive
interactions between context words, the word
composition used for the training mismatches
to the tensor-based methods used for evaluat-
ing the actual compositions including point-
wise multiplication and tensor product of con-
text vectors. In this work, we show whether
the word embeddings from extended CBOW
models using multiplication or tensor product
between context words, reflecting the actual
composition methods, can show better perfor-
mance than those from the baseline CBOW
model in actual tasks of compositions with
multiplication or tensor-based methods.

1 Introduction

In recent years, there has been a surge of inter-
est in using word vectors for modeling semantics.
Mikolov et al. (2013a,b) introduced word2vec
that includes the continuous bag-of-words (CBOW)
model and the skip-gram model.1 These models
have been most widely used for generating word
vectors to be used for word related tasks because of

1https://code.google.com/p/word2vec

their efficient but still effective architectures. The
CBOW model takes the mean vector of projections
of the context words and use it to predict the target
word as the following objective function:2

1
T

T∑
t=1

ln p

wt
∣∣∣∣∣∣ 12c

∑
−c≤j≤c,j ̸=0

pt+j

 , (1)
where T is the total number of words in a corpus, wt
is the tth word, pt is the tth word vector, and c is the
half window size.

Milajevs et al. (2014) showed that the word vec-
tors generated from the CBOW model are compet-
itive with those from co-occurrence based models
for both simple arithmetic compositions and tensor-
based compositions for categorical compositional
distributional models (Coecke et al., 2010).3

Categorical compositional distributional models
represent compositional semantics with algebra of
Pregroup by representing each grammatical reduc-
tion as a linear map in vector spaces (Coecke et al.,
2010; Kartsaklis et al., 2012). For example, cats like
milk consists of a subject noun, a transitive verb re-
quiring a subject and an object, and an object noun,
respectively. In Pregroup grammar, the types of the
three words in this example are n,

(
nrsnl

)
, and n,

respectively, where n is a noun, nr can be combined
with a n in the left, nl can be combined with a n
in the right, and s is a declarative statement. Then,

2Although sum is used in Mikolov et al. (2013a), the current
version of word2vec implementation uses mean.

3Although Milajevs et al. (2014) described that the skip-
gram model was used to generate the word vectors, the CBOW
model was actually used in their work.

143



they can be reduced to represent the entire phrase
with single entity as follows:

n
(
nrsnl

)
n → 1snln → 1s1 → s (2)

In the reduction, nr is composed with the left n re-
sulting in an identity element, 1. Then, nl is com-
posed with the right n resulting in another 1. Be-
cause 1 is an identity element, 1s1 is reduced to s.

Since there is no specification of actual imple-
mentation of the composition in categorical compo-
sitional distributional models, different composition
methods have been introduced; they are reviewed
in Section 2. However, there are few studies about
the vector representation of single words regarding
those compositions.

One issue of using the word vectors from the
CBOW model as the constituent vectors for tensor-
based composition is that their assumptions of the
composition are different. Word embeddings of the
CBOW model are trained with an additive context
composition, which is the mean of the context pro-
jection. However, most tensor-based compositions
use point-wise multiplication or tensor product as
composition operators. This means that there is a
mismatch between the composition method used for
the training of the underlying word vectors and the
actual composition methods we evaluate.

To alleviate the mismatch, we introduce exten-
sions of the CBOW model with multiplicative in-
teractions between word projections to obtain word
embeddings more suitable for the tensor-based com-
positions. For four datasets, evaluating different
types of compositions, we show that those exten-
sions of the CBOW model improve the performance
of the actual composition tasks with multiplication
or tensor product operations.

2 Tensor-based compositions

Prior to discussing the modification to the CBOW
algorithm, we review different composition methods
used in the literature (Table 1).

Addition and Multiplication are compositions by
point-wise addition and multiplication, respectively
(Mitchell and Lapata, 2008). They can be done sim-
ply without any other information, but they cannot
reflect word orders and grammatical structures.

Mitchell and Lapata (2008, 2009) showed that
composition by multiplication can be more effec-
tive than composition by addition because additive
models compose by considering the content alto-
gether whereas multiplicative models focus on the
content relevant to the composition by scaling each
element of one with the strength of the correspond-
ing element of the other. Using multiplication as the
composition method could be unstable in the previ-
ous work because multiplication with zero or nega-
tive values changes the value abruptly (Mitchell and
Lapata, 2009). In our models, however, these in-
stability issues could be alleviated since the train-
ing model adapt the constituent word vectors to
be proper for the composition by multiplication.
Mitchell and Lapata (2010) also showed that the
tensor product is effective to represent composition
because it allows the interactions between different
features in different vectors whereas point-wise mul-
tiplication can interact with only the same feature in
different vectors. Therefore, we also examine an ex-
tension of the CBOW model using tensor product
for modeling local context.

There are neural network models using multi-
plicative interactions in the architectures. Sum-
Product Networks use layer-wise multiplicative in-
teractions (Poon and Domingos, 2011; Cheng et al.,
2014) and multiplicative recurrent neural networks
use multiplication of hidden state outputs from pre-
vious time step with the current word projections
(Sutskever et al., 2011; Irsoy and Cardie, 2014).
These approaches capture multiplicative interactions
with hidden layer outputs. Our approach instead uti-
lizes multiplicative interactions in the training of the
CBOW model, making the embedded vector spaces
more in tune with the compositions of end tasks.

The third to the last composition methods of Ta-
ble 1 shows tensor-based composition methods for
representing phrases consist of subjects, transitive
verbs, and objects in categorical compositional dis-
tributional models. verb =

∑
i
−−→
Sbji ⊗ −−→Obji repre-

sents a verb with the subjects and the objects of the
verb across the corpus. The subject and the object
of each transitive verb required for calculating verb
are identified from the dependency tree of PukWaC
1.0 dataset, which consists of web documents in .uk
domain crawled with the medium-frequency words
from the British National Corpus (BNC) (Burnard,

144



Method Phrase Composition formula Reference

Addition
w1w2...wn

−→w1 +−→w2 + ... +−→wn Mitchell and Lapata (2008)Multiplication −→w1 ⊙−→w2 ⊙ ...⊙−→wn
Relational Sbj Verb Obj verb⊙ (

−−→
Sbj ⊗−−→Obj) Grefenstette and Sadrzadeh (2011a)

Kronecker ṽerb⊙ (−−→Sbj ⊗−−→Obj) Grefenstette and Sadrzadeh (2011b)
Copy sbj. Sbj Verb Obj

−−→
Sbj ⊙ (V erb×−−→Obj) Kartsaklis et al. (2012)

Copy obj. −−→Obj ⊙ (V erb⊤ ×−−→Sbj)
Frob. add.

Sbj Verb Obj
(−−→Sbj ⊙ (V erb×−−→Obj)) + (−−→Obj ⊙ (V erb⊤ ×−−→Sbj))

Kartsaklis and Sadrzadeh (2014)Frob. mult. (−−→Sbj ⊙ (V erb×−−→Obj))⊙ (−−→Obj ⊙ (V erb⊤ ×−−→Sbj))
Frob. outer (−−→Sbj ⊙ (V erb×−−→Obj))⊗ (−−→Obj ⊙ (V erb⊤ ×−−→Sbj))

Table 1: Tensor-based composition methods (Milajevs et al., 2014).

2007) as the seeds (Baroni et al., 2009; Johansson,
2007). 4 ṽerb = −−→verb ⊗ −−→verb represents a verb as
the tensor product of the corresponding verb vector.
Those methods consider the relations between tran-
sitive verbs and their subjects and objects. There-
fore, we can represent their compositions more ef-
fectively. Recursive neural tensor networks also use
tensor product information in the recursive compo-
sition (Socher et al., 2013), but they require training
labels and only support binary compositions.

Relational and Kronecker represent each phrase
by the multiplication of the verb matrix to the ten-
sor product of the subject and the object (Grefen-
stette and Sadrzadeh, 2011a,b). Although they can
represent interactions between subjects and objects
as well as the verbs, it is difficult to compose them
with other phrases in a uniform way since the result
dimensionality is the square of the original vectors.
In addition, dealing with large dimensional tensors
is not very scalable.

The fifth to the last composition methods use
Frobenius operators for the compositions (Kartsak-
lis et al., 2012), which can resolve the dimensional-
ity issues by maintaining the original dimensionality
through matrix-vector multiplication. In Copy sub-
ject, the verb matrix verb is multiplied with the ob-
ject vector and then composed with the subject vec-
tor by point-wise multiplication. Copy object is op-
posite in terms of the positions of the subject and the
object. These two methods are different ways of di-
agonal placement of a plane into a cube (Kartsaklis
et al., 2012). The last three methods, Frobenius ad-

4Available at http://wacky.sslmit.unibo.it/
doku.php?id=corpora.

dition, multiplication, and outer product, represent
different combinations of Copy subject and Copy
object (Kartsaklis and Sadrzadeh, 2014).

3 Extending the CBOW model with
multiplicative interactions between word
projections

As briefly discussed in the introduction, the CBOW
model is an additive model in terms of the composi-
tion since the mean of the context word projections
is used to predict the target word. As many com-
position methods in Table 1 use multiplication or
tensor product as the composition operators, if these
operators are used to compose the contexts in the
CBOW model, then the training process can opti-
mize the model to consider their word embeddings
to be composed with those multiplicative operations.
Therefore, we can train word embeddings that are
more suitable for the composition methods that we
are evaluating.

In the CBOW model, the point-wise mean of the
word projections is used to predict the target word
as shown in Equation 1. In addition to the baseline,
we experimented with adding different multiplica-
tive terms as shown in Table 2. The added terms
are selected to reflect the operations of composition
methods in Table 1 and their combinations. In the
expressions, pi is the projection of the ith input con-
text word and c is the size of the context window,
which is the number of neighboring words used as
the input for each direction.

The second model, mult, uses only the multiplica-
tion of projections, which best fits to the composi-
tion by point-wise multiplication. The third and the

145



Type Expression

1 mean (baseline, Milajevs et al. (2014))
∑
−c≤i≤c,i̸=0 pi/2c

2 pointwise multiplication
∏
−c≤i≤c,c≠0 pi

3 mean + pointwise multiplication mean +
∏
−c≤i≤c,c̸=0 pi

4 concat{mean, pointwise multiplication} concat{mean, ∏−c≤i≤c,c ̸=0 pi}
5 mean + projection of pi−1 and pi+1 mean +Wpconcat{pi−1, pi+1}
6 projection of tensor product of pi−1 and pi+1 Wtp(pi−1 ⊗ pi+1)
7 mean + projection of tensor product of pi−1 and pi+1 mean +Wtp(pi−1 ⊗ pi+1)

Table 2: Different outputs of the projection layer. pi is the projection of the ith input context word, c is the size of the
context window, and Wp and Wtp are projection matrices.

fourth models evaluate the performance when both
the additive and multiplicative interactions are used
together since their combination has been shown to
be effective (Mitchell and Lapata, 2008). The third
model adds the additive terms and multiplicative
terms whereas the fourth model concatenates these
terms so that they influence the output separately.

In the fifth to the last models, we try to further use
the information from pi−1 and pi+1, which are the
projections of the nearest neighbor words of the ith
target word in the training corpus. The fifth model
concatenates pi−1 and pi+1 and project to the orig-
inal dimension with a projection matrix Wp. This
result is added to the baseline model so that infor-
mation from the nearest words considering the order
can be used to estimate the target. Wp is also up-
dated during the training.

In the sixth model, since the tensor-based com-
positions are used as Table 1 and they can rep-
resent multiplicative interactions between different
features, we use the tensor product of the projections
of (pi−1 and pi+1). The tensor product output is also
projected to the original dimensionality by multiply-
ing a projection matrix Wtp, which is also updated
during the training. Although this model can use
more powerful interactions of neighbor words, it can
only use the information from the nearest neighbor
words and it cannot use two word sentences in the
training corpus for the training. To deal with these
issues, in the last model, we combine the mean with
the projection of the tensor product.

4 Experiment results

To evaluate the five different CBOW-based mod-
els proposed in Section 3, we use the following
datasets: similarity of transitive verbs with multi-

ple senses from Grefenstette and Sadrzadeh (2011a),
three-word sentence similarity from Kartsaklis and
Sadrzadeh (2014), paraphrase detection from Dolan
et al. (2013), and dialog act tagging for the Switch-
board corpus (Godfrey et al., 1992) from Stolcke
et al. (2000). These are all the datasets evaluated in
Milajevs et al. (2014)’s work as well. Each phrase
in the first two datasets is fixed as a subject, a tran-
sitive verb, and an object whereas the length of each
phrase in the last two datasets is arbitrary.

There are several differences between our word
vectors and the ones used in Milajevs et al. (2014).
First, we use BNC as the training set while Mi-
lajevs et al. (2014) use pretrained word vectors
from word2vec that are trained using GoogleNews
dataset. To reduce the size of projection matri-
ces, all the words are lower-cased and words occur-
ring 20 times or less are converted to the words’
POS tags. Second, instead of negative sampling,
our models use hierarchical softmax as the objective
function, where each word is represented as a leaf
node of Huffman tree since hierarchical softmax is
better for training with infrequent words (Mikolov
et al., 2013b). Third, we use gradient clipping for
more stable training since gradient can be fluctuating
when the projections are multiplied. All the other
parameters for the training are the same as those
used for Milajevs et al. (2014)’s experiments.

Using the mean as the network combination func-
tion can be considered a reimplementation of Mi-
lajevs et al. (2014)’s system subject to the changes
mentioned above. We trained the CBOW-based
models and obtained 300 dimensional word vec-
tors, which are with the same dimensionality used
in Mikolov et al. (2013a,b); Milajevs et al. (2014).

146



Task Method (Milajevs et al., 2014) mean mult mean + concat mean + nbr outer prj mean +mult {mean,mult} nbr prj nbr outer prj

Similarity
of tran-
sitive
verbs

Verb only 0.107 0.130 0.014 0.136 0.204 0.187 0.072 0.250
Addition 0.149 0.066 0.012 0.046 -0.030 0.100 0.111 0.145

Multiplication 0.095 0.160 0.249 0.058 0.219 0.113 0.050 0.204
Kronecker 0.117 0.160 0.160 0.121 0.229 0.168 0.047 0.245
Relational 0.362 0.330 0.276 0.319 0.280 0.344 0.316 0.365
Copy sbj. 0.131 0.249 0.064 0.262 0.209 0.262 0.168 0.290
Copy obj. 0.456 0.302 0.361 0.329 0.382 0.300 0.371 0.322
Frob. add. 0.359 0.337 0.293 0.345 0.288 0.349 0.250 0.355
Frob. mult. 0.239 0.270 0.252 0.255 0.189 0.293 0.196 0.309
Frob. outer. 0.375 0.330 0.275 0.339 0.351 0.329 0.293 0.387

Sentence
similarity

Verb only 0.561 0.528 0.360 0.520 0.531 0.527 0.260 0.536
Addition 0.689 0.728 0.572 0.738 0.770 0.722 0.401 0.706

Multiplication 0.341 0.062 0.625 0.178 0.440 0.110 0.269 0.220
Kronecker 0.561 0.206 0.623 0.277 0.501 0.203 0.003 0.457
Relational 0.618 0.505 0.665 0.540 0.527 0.525 0.157 0.574
Copy sbj. 0.405 0.390 0.453 0.353 0.436 0.396 0.139 0.454
Copy obj. 0.655 0.481 0.607 0.487 0.500 0.488 0.190 0.510
Frob. add. 0.585 0.489 0.610 0.407 0.528 0.439 0.210 0.501
Frob. mult. 0.387 0.211 0.608 0.323 0.419 0.335 0.065 0.349
Frob. outer. 0.622 0.504 0.664 0.510 0.544 0.524 0.165 0.569

Table 3: Spearman’s ρ on the similarity of transitive verbs with multiple senses (top) and three-word sentence similarity
(bottom). The mean column can be considered an implementation of the Milajevs et al. (2014)’s model on the BNC
corpus.

4.1 Fixed phrases (three-word)

Table 3 shows the experiment results for the three-
word phrases. The first column represents the two
evaluation tasks, the second column is the composi-
tion methods described in Table 1, and the third col-
umn shows the results of neural word embeddings
(NWE) from previous work (Milajevs et al., 2014).5

Bold entries in the table indicate the highest scores
among our models.

In the datasets, human annotators rated each
phrase pair for semantic similarity (from 1 “no sim-
ilarity” to 7 “high similarity”). As each unique
phrase pair is judged by multiple people, following
Milajevs et al. (2014), we took the mean of the rat-
ings to set the rating of each unique pair. Scores in
the table entries are Spearman’s ρs. A high value
of Spearman’s ρ in the table means that the similar-
ity of the composed phrases in the vector space is
highly correlated with the semantic similarity of the
phrases judged by humans. Therefore, if a model
shows high scores, it reflects that the model is good
at representing the semantics for those short phrases.

5The word vectors are available at
https://drive.google.com/file/d/
0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit.

4.1.1 Similarity of transitive verbs

The top of Table 3 shows the results for the sim-
ilarity of 199 three-word phrase (subject, transitive
verb, and object) pairs introduced in Grefenstette
and Sadrzadeh (2011a).6 In each phrase pair, the
transitive verbs are the same but the subjects and the
objects are different for each other. We try to iden-
tify the senses of a transitive verb with the different
contexts. For example, “meet” is a verb with multi-
ple senses. If the given subject is “system” and the
object is “specification”, “meet” would be seman-
tically closer to “satisfy” than “visit”. Then, given
“system meets specification” and “system satisfies
specification” as a pair, the judge would give a high
rating for the similarity of the verbs.

Our results were not consistently better than Mi-
lajevs et al. (2014)’s results. However, considering
that the model used for the previous work and our
baseline (mean) are similar CBOW models, the per-
formance difference would mainly due to the differ-
ent training sets (GoogleNews and BNC). Among
our models, adding tensor product result to the mean
(mean+nb outer prj) showed the best performance
in most types of compositions. Interestingly, the

6Available at http://www.cs.ox.ac.uk/
activities/compdistmeaning/GS2011data.txt.

147



power seems to come from the combination of mean
and the tensor product, as both individually perform
worse than the combination. For this dataset, as the
verbs are the same for both phrases in each pair, the
subjects and the objects play important roles for the
verb disambiguation. When a transitive verb is an
ith word in a sentence denoted as wi, in many cases,
the subject and the object are wi−1 and wi+1, re-
spectively. Since tensor product of the i− 1th word
projection and the i + 1th word projection can rep-
resent multiplicative interactions between different
features of the two words, considering the tensor
product of those projections in the model could be
helpful to identify the transitive verbs.

4.1.2 Similarity of three-word phrases
The bottom of Table 3 shows the evaluation re-

sults on the similarity of 109 three-word phrase pairs
with human judged ratings from (Kartsaklis and
Sadrzadeh, 2014).7 For example, the similarities of
two sentences like “programme offer support” and
“service provide help” are evaluated.

In this evaluation, considering the interleaved
words with tensor product (mean+nb outer prj) still
showed better performance than the baseline (mean)
for the most composition methods except addi-
tion. However, the multiplication only model, mult,
showed the best performance in most cases except
when verb only or addition were used as the compo-
sition methods.

4.2 Arbitrary length phrases

The three-word phrases in the previous section are
useful for the evaluation of the tensor-based compo-
sitions since we do not need to care about the struc-
tural variations of the phrases. However, we would
be more interested in phrases where the lengths are
not fixed. As each phrase can have different length,
we cannot use the tensor-based compositions used
for the fixed-length phrases. Therefore, we evalu-
ated the composition of each phrase by only using
point-wise addition and multiplication.

Table 4 shows the accuracies of classification
tasks given arbitrary length phrases as the inputs.
The results evaluate whether composition of arbi-

7Available at http://www.cs.ox.ac.uk/
activities/compdistmeaning/emnlp2013_turk.
txt.

trary length phrases can be well represented with the
word vectors from the proposed models.

4.2.1 Paraphrase detection
The top of Table 4 shows the binary classification

accuracies on the Microsoft Research Paraphrase
Corpus (Dolan et al., 2013), which consists of arbi-
trary length phrase pairs. In this dataset, each phrase
pair comes with a binary label: 1 if the phrases were
judged to be paraphrases, 0 otherwise. The mini-
mum, mean, and maximum lengths of the phrases in
the training set are 6, 19.8, and 35, respectively.

With this dataset, we can evaluate if our models
work well for representing general phrases. Follow-
ing the setting of Milajevs et al. (2014)’s work, we
trained a linear binary classifier on 2000 phrase pairs
and tested on 1726 phrase pairs. The classifier is
trained to find the threshold of cosine similarity de-
ciding if two phrases are paraphrases or not.

Comparing to the baseline CBOW model, there
were no significant gain in the proposed models for
the composition by addition. However, using mul-
tiplication of the projections (mult) showed signifi-
cantly better performance when composed by mul-
tiplication, and started to show statistical insignifi-
cance to additive composition methods when tested
by McNemar’s test with p-value 0.05.

4.2.2 Dialog act tagging
The bottom of Table 4 shows the classification ac-

curacies of dialog act tagging (Stolcke et al., 2000)
on the Switchboard corpus (Godfrey et al., 1992).
Switchboard is a collection of about 2400 telephone
dialogs among 543 speakers in the United States.
Each utterance is assigned one of 42 dialog-act tags,
which summarize syntactic, semantic and pragmatic
information about the turns (e.g., yes/no question,
yes answer, agree).8 The minimum, mean, and max-
imum lengths of the phrases in the training set are
0, 34.1, and 549, respectively. Zero length phrases
exist because of the preprocessing, and they are ig-
nored.

The task in this section is identifying the dialog
act tags from given utterances. Following Milajevs
and Purver (2014); Milajevs et al. (2014), we used
the first 1115 utterances as the training set and the

8The tags are described in http://web.stanford.
edu/˜jurafsky/ws97/manual.august1.html.

148



Task Method (Milajevs et al., 2014) mean mult mean + concat mean + nbr outer prj mean +mult {mean,mult} nbr prj nbr outer prj
Paraphrase
detection

Addition 0.73 0.686 0.665 0.690 0.688 0.689 0.684 0.688
Multiplication 0.42 0.393 0.652 0.388 0.587 0.387 0.412 0.371

Dialog act
tagging

Addition 0.63 0.638 0.636 0.633 0.636 0.636 0.565 0.626
Multiplication 0.58 0.522 0.606 0.593 0.515 0.581 0.573 0.598

Table 4: Accuracies on the paraphrase detection (top) and the dialog act tagging (bottom). The mean column can be
considered an implementation of the Milajevs et al. (2014)s model on our training set.

following 19 utterances as the test set. We also con-
catenated utterances separated by an interruption by
the other person (Webb et al., 2005), and we re-
moved disfluency markers and punctuation signs.
Once we have the vectors composed by either ad-
dition or multiplication for all of the utterances in
the training set, the vector dimensionality is reduced
to 50 by Singular Value Decomposition (SVD) and
a k-nearest-neighbor classifier (k=5) is used to iden-
tify the dialog act tags.9 The baseline (mean) model
showed the best performance for the composition by
addition and the mult model was the best for the
composition by multiplication, but the differences
were insignificant in this case.

The results on both evaluation for arbitrary length
phrases support that matching the composition of
contexts for the training of constituent word vectors
with the actual composition methods shows better or
competitive performance.

5 Discussion

We showed the experiment results on seven types
of word vectors trained using different composition
methods. Overall, we can see that multiplicative in-
teractions in the CBOW models can help represent-
ing compositions that are multiplicative in nature.

Using only the multiplication of projections
showed significant improvement for all the evalu-
ated datasets when the phrases are composed with
multiplications. Because the composition used for
the training of word vectors is matching to the ac-
tual evaluated compositions, we can think that the
word vectors are trained to represent their multipli-
cations properly. One evidence is that the mean of
word vectors of the mult model is around 0.12 while
the means of the other models are around 0. Since

9We used scikit-learn (Pedregosa et al., 2011) to run SVD
and k-NN classifiers.

there are fewer negative elements in the word vec-
tors of the mult model, the composition by multi-
plication produces relatively more positive values.
This possibly gives more stable results when used
in multiplication-based compositions since fluctu-
ations of the composition by multiplication with
negative values is reduced. In the task of transi-
tive verb disambiguation, since the interactions be-
tween non-adjacent subjects and objects are impor-
tant, having their tensor product as a term in the
model (mean+nbr outer prj) was noticeably help-
ful. In the task of three-word phrase similarity, us-
ing the tensor product as a term still showed bet-
ter performance than using the models of mean and
mean+mult in most cases except when the phrases
are composed with addition. Interestingly, however,
the model with only multiplication showed the best
performance for most of the compositions by multi-
plication and tensor product.

In summary, for better representation of phrase
compositions, we showed that it can be helpful to
train the word embedding models by composing the
input contexts of the model to be similar to the ac-
tual composition methods to be used because the
word vectors are adjusted to more properly repre-
sent the composition by the composition method
used. Specifically, using point-wise multiplication
in the training model consistently showed better per-
formance when the actual composition is also mul-
tiplication. The mean+nbr outer prj model, which
is with the combination of mean and tensor prod-
uct also showed better or similar performance for
tensor-based composed phrases compared to the
mean model and the mean+mult model.

One issue is that we used the word vectors of
targets’s neighbors to obtain tensor product terms.
Since only the compositions of subjects, verbs, and
objects are evaluated, we can expect better perfor-
mance if only tensor products of subject-object pairs

149



are used as the tensor product terms. As future work,
An in-depth analysis of the strengths and weak-
nesses of each approach would be helpful to gain
more insights about the patterns we see in the re-
sults.

References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros

Zanchetta. The WaCky Wide Web: A collection of very large
linguistically processed web-crawled corpora. Language Re-
sources and Evaluation, 43(3):209–226, 2009.

Lou Burnard, editor. Reference Guide for the British National
Corpus. Research Technologies Service at Oxford Univer-
sity Computing Services, 2007.

Wei-Chen Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong
Chieu, and Kian Ming A. Chai. Language modeling with
sum-product networks. In Proceedings of Interspeech, 2014.

Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. Math-
ematical foundations for a compositional distributional
model of meaning. Linguistic Analysis, 36:345–384, 2010.

Bill Dolan, Chris Brockett, and Chris Quirk. Microsoft research
paraphrase corpus, 2013.

John J Godfrey, Edward C Holliman, and Jane McDaniel.
Switchboard: Telephone speech corpus for research and de-
velopment. In IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), pages 517–520,
1992.

Edward Grefenstette and Mehrnoosh Sadrzadeh. Experimental
support for a categorical compositional distributional model
of meaning. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing, pages
1394–1404, 2011a.

Edward Grefenstette and Mehrnoosh Sadrzadeh. Experiment-
ing with transitive verbs in a discocat. In Proceedings of the
GEMS 2011 Workshop on Geometrical Models of Natural
Language Semantics, EMNLP 2011, pages 62–66, 2011b.

Ozan Irsoy and Claire Cardie. Modeling compositional-
ity with multiplicative recurrent neural networks. arXiv,
abs/1412.6577, 2014.

Richard Johansson. Dependency Syntax in the CoNLL Shared
Task 2008, 2007.

Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. A study of en-
tanglement in a categorical framework of natural language.
In Proceedings of the 11th Workshop on Quantum Physics
and Logic (QPL), 2014.

Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen Pul-
man. A unified sentence space for categorical distributional-
compositional semantics: Theory and experiments. In Pro-
ceedings of 24th International Conference on Computational
Linguistics (COLING), pages 549–558, 2012.

Tomas Mikolov, Greg Corrado, Kai Chen, and Jeffrey Dean. Ef-
ficient Estimation of Word Representations in Vector Space.
In International Conference on Learning Representations
(ICLR) workshop, 2013a.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
and Jeffrey Dean. Distributed representations of words and
phrases and their compositionality. In Advances in Neural
Information Processing Systems (NIPS), pages 3111–3119,
2013b.

Dmitrijs Milajevs and Matthew Purver. Investigating the con-
tribution of distributional semantic information for dialogue
act classification. In Proceedings of the 2nd Workshop on
Continuous Vector Space Models and their Compositional-
ity (CVSC), pages 40–47, 2014.

Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh,
and Matthew Purver. Evaluating neural word representations
in tensor-based compositional settings. In Proceedings of
the Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 708–719, 2014.

Jeff Mitchell and Mirella Lapata. Vector-based models of se-
mantic composition. In Proceedings of ACL-08: HLT, pages
236–244, 2008.

Jeff Mitchell and Mirella Lapata. Language models based on
semantic compositon. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Processing
(EMNLP), pages 430–439, 2009.

Jeff Mitchell and Mirella Lapata. Composition in distribu-
tional models of semantics. Cognitive science, 34:1388–
1429, 2010.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Ma-
chine learning in Python. Journal of Machine Learning Re-
search, 12:2825–2830, 2011.

Hoifung Poon and Pedro Domingos. Sum-product networks:
A new deep architecture. In Proceedings of Uncertainty in
Artificial Intelligence (UAI), 2011.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris
Manning, Andrew Ng, and Chris Potts. Recursive deep mod-
els for semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages 1631–
1642, 2013.

Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor,
Carol Van Ess-Dykema, Rachel Martin, and Marie Meteer.
Dialogue act modeling for automatic tagging and recogni-
tion of conversational speech. Computational Linguistics,
26(3):339–373, 2000.

Ilya Sutskever, James Martens, and Geoffrey E Hinton. Gen-
erating text with recurrent neural networks. In Proceedings
of the 28th International Conference on Machine Learning
(ICML), pages 1017–1024, 2011.

Nick Webb, Mark Hepple, and Yorick Wilks. Dialogue act clas-
sification based on intra-utterance features. In Proceedings
of the AAAI Workshop on Spoken Language Understanding,
2005.

150


