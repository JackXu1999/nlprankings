



















































Short Text Understanding by Leveraging Knowledge into Topic Model


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1232–1237,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Short Text Understanding by Leveraging Knowledge into Topic Model

Shansong Yang, Weiming Lu∗, Dezhi Yang, Liang Yao, Baogang Wei
Zhejiang University

Hangzhou, Zhejiang 310000, China
{yangshansong,luwm,deathyyoung,yaoliang,wbg}@zju.edu.cn

Abstract

In this paper, we investigate the challenging
task of understanding short text (STU task) by
jointly considering topic modeling and knowl-
edge incorporation. Knowledge incorporation
can solve the content sparsity problem effec-
tively for topic modeling. Specifically, the
phrase topic model is proposed to leverage
the auto-mined knowledge, i.e., the phrases,
to guide the generative process of short tex-
t. Experimental results illustrate the effective-
ness of the mechanism that utilizes knowledge
to improve topic modeling’s performance.

1 Introduction

The explosion of online text content, such as twitter
messages, text advertisements, QA community mes-
sages and product reviews has given rise to the ne-
cessity of understanding these prevalent short texts.

Conventional topic modeling, like PLSA
(Hofmann, 1999) and LDA (Blei et al., 2003) are
widely used for uncovering the hidden topics from
text corpus. However, the sparsity of content in
short texts brings new challenges to topic modeling.

In fact, short texts usually do not contain suf-
ficient statistical signals to support many state-of-
the-art approaches for text processing such as top-
ic modeling (Hua et al., 2015). Knowledge is indis-
pensable to STU task, where knowledge-based topic
model (Andrzejewski et al., 2009; Hu et al., 2011;
Jagarlamudi et al., 2012; Mukherjee and Liu, 2012;
Chen et al., 2013; Yan et al., 2013) has attracted
more attention recently.

∗Corresponding author

We consider, in the STU task, the available
knowledge can be divided into two classes: self-
contained knowledge and external knowledge.
Self-contained knowledge, which is focused in
this paper, is extracted from the short text itself,
such as key-phrase. External knowledge is con-
structed without special purpose, such as WordNet
(Miller, 1995), KnowItAll (Etzioni et al., 2005),
Wikipedia (Gabrilovich and Markovitch, 2007),
Yago (Suchanek et al., 2007), NELL
(Carlson et al., 2010) and Probase (Wu et al., 2012).

PLSA and LDA are the typical unsupervised topic
models, that is non-knowledgeable model. In con-
trast, Biterm topic model (BTM) (Yan et al., 2013)
leverages self-contained knowledge into semantic
analysis. BTM learns topics over short texts by mod-
eling the generation of biterms in the whole corpus.
A biterm is an unordered word-pair co-occurring in
short contexts. BTM posits that the two words in a
biterm share the same topic drawn from a mixture of
topics over the whole corpus. The major advantage
of BTM is that BTM explicitly model the word co-
occurrences in the local context, which well captures
the short-range dependencies between words.

External knowledge-based models incorporate
expert domain knowledge to help guide the mod-
els. DF-LDA (Andrzejewski et al., 2009) model in-
corporates domain knowledge in the form of must-
link and cannot-link. Must-link states that two word-
s should belong to the same topic, while cannot-
link states that two words should not be in the same
topic. GK-LDA (Chen et al., 2013) leverages lexi-
cal semantic relations of words such as synonyms,
antonyms and adjective attributes in topic models. A

1232



],1[ Mm  

],1[
m

Nn  

],1[ ,nmNl 

!
m
"
 

nm
z ,

lnm
w ,,

sm,#

sm,$

],1[
m
Ss],1[ Kk  

%

k
&
 

Figure 1: The phrase topic model proposed in this paper.

vast amount of lexical knowledge about words and
their relationships, denoted as LR-sets, available in
online dictionaries or other resources can be exploit-
ed by this model to generate more coherent topics.

However, for external knowledge-based model-
s, the incorporated knowledge is too general to be
consistent with the short text in the semantic space.
On the other hand, BTM, as a typical self-contained
knowledge-based model, makes rough assumption
on the generated biterms. The generated biterms are
inundated with noise, for not any two terms in short
text share same topic. Based on the above anal-
ysis, we first identify key-phrases from short text,
which can be deemed as self-contained knowledge,
then propose phrase topic model (PTM), which con-
strains same topic for terms in key-phrase and sam-
ple topics for non-phrase terms from mixture of key-
phrase’s topic.

2 Phrase Topic Model

2.1 Model

A phrase is defined as a consecutive sequence
of terms, or unigrams. In this paper, we fo-
cus on self-contained knowledge in short text,
i.e., the key-phrases. Key-phrase extraction is a
fundamental component in our work. We use
CRF++1 to identify key-phrases in a short text.
The training data is built manually, and the fea-
tures contain the word itself, the part of speech
tagged by Stanford Log-linear Part-Of-Speech Tag-

1http://crfpp.googlecode.com/svn/trunk/doc/index.html

ger (Toutanova et al., 2003). Sample identified key-
phrases are shown in Table 2.

In this paper, our phrase topic model is proposed
based on three assumptions:

• Key-phrases are the key points of interest in the
short text, which should be the focus.

• Terms consisting of the same key-phrase will
share common topic.

• Non-phrase term’s topic assignment should de-
pend on that of key-phrases in the same text.

Our assumptions is indeed similar to other mod-
els (Gruber et al., 2007), for example each sentence
is assumed to be assigned to one topic, however this
assumption is too general, in many cases, differen-
t words should be assigned different topics even in
short text. Our model is more refined to distinguish
key-phrase and non-phrase. In addition, if two or
more key-phrases exist in the same short text, they
are probably assigned different topics.

The graphical representation of PTM is illustrat-
ed in Figure 1. α and β are hyper-parameters, which
are experienced tuned. φ is corpus-level parameter,
while θ is document-level parameter. The hidden
variables consist of zm,n and δm,s. The generative
process of phrase topic model is presented as fol-
lows.

• For each topic k ∈ [1,K]
– draw a topic-specific word distribution

φk ∼ Dir(β)
• For each document m ∈ [1,M ]

– draw a topic distribution θm ∼ Dir(α)
– For each key-phrase n ∈ [1, Nm]

∗ draw topic assignment zm,n ∼
Multi(θm)

∗ For each word l ∈ [1, Nm,n]
· draw wm,n,l ∼ Multi(φzm,n)

– For each non-phrase word s ∈ [1, Sm]
∗ draw a topic assignment δm,s ∼

Uniform(zm,1, . . . , zm,Nm)
∗ draw word om,s ∼ Multi(φδm,s)

From this process, we can see the generation of key-
phrases and non-phrases are distinguished and non-
phrase’s generation is based on the topic assignment
of key-phrases in the same document.

1233



2.2 Inference By Gibbs Sampling

Similarly with LDA, collapsed Gibbs sampling
(Griffiths and Steyvers, 2004) can be utilized to per-
form approximate inference. In our model, the hid-
den variables are key-phrase’s topic assignment z
and non-phrase word’s topic assignment δ. To per-
form Gibbs sampling, we first randomly initialize
the hidden variables. Then we sample the top-
ic assignment based on the conditional distribu-
tion p(zm,n = k|z¬(m,n),w,o, δ) and p(δm,s =
k|z,w,o, δ¬(m,s)).

We can derive the conditional probability for
zm,n following Equation 1, where nkm,¬(m,n) de-
notes the number of key-phrases whose topic assign-
ment are k in document m without consideration of
key-phrase {m,n}, which is similar to nk′m,¬(m,n).
n

wm,n,l
k,¬(m,n) denotes the number of times key-phrase

term wm,n,l assigned to topic k without consid-
eration of key-phrase {m,n}, which is similar to
nwk,¬(m,n). n

om,s
k,¬m denotes the number of times non-

phrase term om,s assigned to topic k without consid-
eration of document m, which is similar to nwk,¬m.

Similarly, we can derive the conditional probabil-
ity for δm,s following Equation 2, where n

om,s
k,¬(m,s)

denotes the number of times non-phrase term om,s
assigned to topic k without consideration of non-
phrase term {m, s}, which is similar to nwk,¬(m,s).
Lm denotes the number of topics assigned to key-
phrases in document m.

Finally, we can easily estimate the topic distribu-
tion θm,k and topic-word distribution φk,w following
Equation 3 and 4.

θm,k =
nkm + α∑K

k
′
=1

nk
′

m + Kα
(3)

φk,w =
nwk + β∑V

w
′
=1

nw
′

k + V β
(4)

3 Experiments and Results

Online reviews dataset (Chen et al., 2013), which
consists of four domains, is utilized to evaluate our
model, where each domain collection contains 500
reviews. Each review’s average length is 20.42. The
statistics of each domain are presented in Table 1.
It’s worth noting that the Phrase is auto-identified
by the key-phrase extraction method. And the Word

represents the whole distinct words for those identi-
fied key-phrases.

In our paper, we assumed each domain has a sin-
gle topic model. For different domain, we think the
semantic space is quite different. So we performed
the proposed topic model with respect to different
domain. The number of topics is usually determined
by experience, in our experiment, each domain col-
lection contains 500 reviews, we think the number
of topics ranging from 2 to 20 is appropriate, and
these reviews are sufficient to train a topic model.

Table 1: Statistic information of the dataset.
Dataset Phrase Word Vocabulary

Computer 1439 1423 5109
Cellphone 1110 1109 4184

Camera 2962 2620 8366
Food 1235 1350 4488

Recent research (Chang et al., 2009;
Newman et al., 2010) shows that the models
which achieve better predictive perplexity often
have less interpretable latent spaces. So the Topic
Coherence Metric (Mimno et al., 2011) is utilized
to assess topic quality, which is consistent with
human labeling.

We compare our model with four baseline mod-
els: non-knowledgeable model LDA, self-contained
knowledgeable model BTM, external knowledge-
based model GK-LDA (Chen et al., 2013) and DF-
LDA (Andrzejewski et al., 2009). Those identified
key-phrases are used as must-links in DF-LDA and
LR-sets in GK-LDA. This can ensure the incorporat-
ed knowledge upon different models are equal.

Table 2 illustrates the auto-identified phrases from
cellphone dataset. From this result, we can see
key-phrase extraction method can efficiently identi-
fy mostly phrases. More than one phrase, for exam-
ple warranty service and android phone, may appear
in a single sentence, and their topic assignments are
probably different. Our proposed phrase topic mod-
el(PTM) can well handle this case, which is more
well-defined than the assumption of all words within
a sentence share one topic. Our phrase topic model
assumes non-phrase term’s topic assignment should
depend on that of key-phrases in the same text. This
assumption can be clearly confirmed by Table 2, for
example, Nokia N97 mini is semantic dependent US-

1234



p(zm,n = k|z¬(m,n),w,o, δ) =
nkm,¬(m,n) + α∑K

k
′
=1

nk
′

m,¬(m,n) + Kα
·

∏Nm,n
l=1 (n

wm,n,l
k,¬(m,n) + β)∏Nm,n

l=1 (
∑V

w=1 n
w
k,¬(m,n) + V β)

·
∏Sm

s=1(n
om,s
k,¬m + β)∏Sm

s=1(
∑V

w=1 n
w
k,¬m + V β)

(1)

p(δm,s = k|z,w,o, δ¬(m,s)) =
n

om,s
k,¬(m,s) + β∑V

w=1 n
w
k,¬(m,s) + V β

· 1
Lm

(2)

B charge cable, the same as company and warranty
service.

For all models, posterior inference was drawn af-
ter 1000 Gibbs iterations with an initial burn-in of
800 iterations. For all models, we set the hyperpa-
rameters α = 2 and β = 0.5.

The evaluation results over Topic Coherence Met-
ric are presented in Figure 2 and Figure 3. This
figure indicates our model and BTM can get high-
er topic coherence score than GK-LDA and DF-
LDA, which means the self-defined knowledge and
the mechanism of knowledge incorporation are ef-
fective to topic model. LDA’s performance is ac-
ceptable but not stable. Our model performs better
than BTM, which is probably because the rough as-
sumption of BTM on generated biterms. From the
above analysis, we can see our proposed model can
get the best performance.

T-test results show that the performance improve-
ment of our model over baselines is statistically sig-
nificant on Topic Coherence Metric. All p-values for
t-test are less than 0.00001.

Figure 4 presents the fluctuation of topic coher-
ence when tuning the hyper-parameter α and β. We
can see that the performance fluctuates within a lim-
ited range as we vary α and β. The topic coherence
fluctuates between −550 and −950 other than food
dataset, which gets less fluctuation range.

Table 3 shows example topics for each domain,
where inconsistent words are highlighted in red.
From this results, we can see the number of errors in
phrase topic model(PTM) is significantly less than
LDA, which indicates our proposed topic model is
more suitable than LDA for short text.

4 Conclusions and Future Work

In this paper, we present a topic model to achieve
STU task starting from key-phrases. The terms in
key-phrases identified from the short texts are sup-
posed to share a common topic respectively. And
those key-phrases are assumed to be the central fo-
cus in the generative process of documents. In
the future work, the self-contained knowledge, such
as those identified key-phrases, and the external
knowledge-base should be integrated to guide top-
ic modeling.

Acknowledgements

This work is supported by the National Natu-
ral Science Foundation of China No.61103099,
the Fundamental Research Funds for the Cen-
tral Universities(2014QNA5008), Chinese Knowl-
edge Center of Engineering Science and Tech-
nology(CKCEST) and Specialized Research Fund
for the Doctoral Program of Higher Educa-
tion(SRFDP)(No.20130101110136).

References
[Andrzejewski et al.2009] David Andrzejewski, Xiaojin

Zhu, and Mark Craven. 2009. Incorporating domain
knowledge into topic modeling via dirichlet forest pri-
ors. In Proceedings of the 26th Annual International
Conference on Machine Learning, ICML 2009, pages
25–32.

[Blei et al.2003] David M Blei, Andrew Y Ng, and
Michael I Jordan. 2003. Latent dirichlet allocation.
the Journal of machine Learning research, 3:993–
1022.

[Carlson et al.2010] Andrew Carlson, Justin Betteridge,
Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr.,

1235



Table 2: Identified Key-phrase in Cellphone dataset
[1] Both sites list compatible devices including the Samsung Galaxy Tab.
[2] My Dell Streak needed more power than any normal USB car adapter could give me.
[3] This actually comes with a micro USB charge cable which fits and works perfectly

for my Nokia N97 mini.
[4] I contacted the company for warranty service. On my android phone I paired . . .
[5] Everything from pulling it out of the box to syncing it with both my IPhone and

Ipod touch 4g were effortless.

 

!1400.000 

!1200.000 

!1000.000 

!800.000 

!600.000 

!400.000 

!200.000 

2 4 6 8 10 12 14 16 18 20

T
o
p
ic

 C
o
h
e
re
n
c
e

Number of Topics

Topic Coherence vs. Number of Topics

OurModel

LDA

BTM

GK!LDA

DF!LDA

Figure 2: Average Topic Coherence score of each model.

 
!2500

!2000

!1500

!1000

!500

0

Computer Cellphone Camera Food

OurModel

LDA

BTM

GK!LDA

DF!LDA

Figure 3: Detailed Topic Coherence score of T = 15.

 

 

 1150

 1050

 950

 850

0.5!1.5!2.5! 3.5! 4.5!

 

0.1!

0.6!

!

 9

8

 

 9

 

 

950  

850

1050 

950

1150 

1050  

 

 

 

 5

 950

 850

750

650

550

0.5!1.5!22.5! 3.5! 4.5!

 

0.1!

0.6!

!

 650 

 550

 750 

 650

 850 

 750

 950 

 850

 

 

 

 6

 950

 850

750

650

0.5! 2.0!

0

3.5! 5.0!

 

0.1!

0.6!

!

 650

 600

 750

 650

 850

 750

 950

 850

0 

0

0 

0

0 

0

0 

0

 

 

 950

 850

 750

 650

 550

0.5!1.5! 2.5! 3.5! 4.5!

 

Compu

0.1!

0.6!

!

ter

 650 

 550

 750 

 650

 850 

 750

 950 

 850

Figure 4: Parameter influences with fixed topic number T = 15.

Table 3: Example topics. First row: domain, Second row: inferred topic tag. Errors are highlighted in red.
Cellphone Computer Food Camera

music game dinner buy
LDA PTM LDA PTM LDA PTM LDA PTM
phone phone buy fps coffee soup camera camera
music music make disruption product good bought bought
iphone car games dips found bread wanted pictures
calls radio time playable love mix year video
play device fast wars amazon popcorn time sony

bluetooth sound play age bread taste happy back
hear quality people pretty popcorn great purchase canon
free iphone thing update eating bag day battery
cell volume card star ordered flavor month time

listen bluetooth full empires bought make love price
hands easy product laggy good coffee ago lens
charge good read unplayable taste eat week quality

1236



and Tom M. Mitchell. 2010. Toward an architecture
for never-ending language learning. In Proceedings of
the Twenty-Fourth AAAI Conference on Artificial In-
telligence, AAAI 2010.

[Chang et al.2009] Jonathan Chang, Jordan L. Boyd-
Graber, Sean Gerrish, Chong Wang, and David M.
Blei. 2009. Reading tea leaves: How humans interpret
topic models. In 23rd Annual Conference on Neural
Information Processing Systems 2009, pages 288–296.

[Chen et al.2013] Zhiyuan Chen, Arjun Mukherjee, Bing
Liu, Meichun Hsu, Malu Castellanos, and Riddhiman
Ghosh. 2013. Discovering coherent topics using gen-
eral knowledge. In Proceedings of the 22nd ACM in-
ternational conference on Conference on information
& knowledge management, pages 209–218. ACM.

[Etzioni et al.2005] Oren Etzioni, Michael J. Cafarel-
la, Doug Downey, Ana-Maria Popescu, Tal Shaked,
Stephen Soderland, Daniel S. Weld, and Alexander
Yates. 2005. Unsupervised named-entity extraction
from the web: An experimental study. Artif. Intell.,
165(1):91–134.

[Gabrilovich and Markovitch2007] Evgeniy Gabrilovich
and Shaul Markovitch. 2007. Computing seman-
tic relatedness using wikipedia-based explicit seman-
tic analysis. In Proceedings of the 20th Internation-
al Joint Conference on Artificial Intelligence, pages
1606–1611.

[Griffiths and Steyvers2004] Thomas L Griffiths and
Mark Steyvers. 2004. Finding scientific topics. Pro-
ceedings of the National academy of Sciences of the
United States of America, 101(Suppl 1):5228–5235.

[Gruber et al.2007] Amit Gruber, Yair Weiss, and Michal
Rosen-Zvi. 2007. Hidden topic markov models. In
Proceedings of the Eleventh International Conference
on Artificial Intelligence and Statistics, AISTATS 2007,
San Juan, Puerto Rico, March 21-24, 2007, pages
163–170.

[Hofmann1999] Thomas Hofmann. 1999. Probabilis-
tic latent semantic indexing. In Proceedings of the
22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 50–57.

[Hu et al.2011] Yuening Hu, Jordan L. Boyd-Graber, and
Brianna Satinoff. 2011. Interactive topic modeling. In
The 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 248–257.

[Hua et al.2015] Wen Hua, Zhongyuan Wang, Haixun
Wang, Kai Zheng, and Xiaofang Zhou. 2015. Short
text understanding through lexical-semantic analy-
sis. In International Conference on Data Engineering
(ICDE).

[Jagarlamudi et al.2012] Jagadeesh Jagarlamudi,
Hal Daum III, and Raghavendra Udupa. 2012.

Incorporating lexical priors into topic models. In
13th Conference of the European Chapter of the
Association for Computational Linguistics, pages
204–213.

[Miller1995] George A. Miller. 1995. Wordnet: A lexical
database for english. Commun. ACM, 38(11):39–41.

[Mimno et al.2011] David M. Mimno, Hanna M. Wal-
lach, Edmund M. Talley, Miriam Leenders, and An-
drew McCallum. 2011. Optimizing semantic coher-
ence in topic models. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2011, pages 262–272.

[Mukherjee and Liu2012] Arjun Mukherjee and Bing Li-
u. 2012. Aspect extraction through semi-supervised
modeling. In The 50th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 339–348.

[Newman et al.2010] David Newman, Youn Noh, Ed-
mund M. Talley, Sarvnaz Karimi, and Timothy Bald-
win. 2010. Evaluating topic models for digital li-
braries. In Proceedings of the 2010 Joint International
Conference on Digital Libraries, pages 215–224.

[Suchanek et al.2007] Fabian M. Suchanek, Gjergji Kas-
neci, and Gerhard Weikum. 2007. Yago: a core of
semantic knowledge. In Proceedings of the 16th Inter-
national Conference on World Wide Web, WWW 2007,
Banff, Alberta, Canada, May 8-12, 2007, pages 697–
706.

[Toutanova et al.2003] Kristina Toutanova, Dan Klein,
Christopher D. Manning, and Yoram Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In HLT-NAACL 2003, Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.

[Wu et al.2012] Wentao Wu, Hongsong Li, Haixun Wang,
and Kenny Qili Zhu. 2012. Probase: a probabilistic
taxonomy for text understanding. In Proceedings of
the ACM SIGMOD International Conference on Man-
agement of Data, SIGMOD 2012, Scottsdale, AZ, US-
A, May 20-24, 2012, pages 481–492.

[Yan et al.2013] Xiaohui Yan, Jiafeng Guo, Yanyan Lan,
and Xueqi Cheng. 2013. A biterm topic model for
short texts. In 22nd International World Wide Web
Conference, pages 1445–1456.

1237


