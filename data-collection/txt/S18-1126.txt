



















































UC3M-NII Team at SemEval-2018 Task 7: Semantic Relation Classification in Scientific Papers via Convolutional Neural Network


Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 793–797
New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics

UC3M-NII Team at SemEval-2018 Task 7: Semantic Relation
Classification in Scientific Papers via Convolutional Neural Network

Vı́ctor Suárez-Paniagua, Isabel Segura-Bedmar
Computer Science Department

Universidad Carlos III de Madrid
Leganés 28911, Madrid, Spain

vspaniag,isegura@inf.uc3m.es

Akiko Aizawa
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku

Tokyo, Japan
aizawa@nii.ac.jp

Abstract

This paper reports our participation for
SemEval-2018 Task 7 on extraction and clas-
sification of relationships between entities in
scientific papers. Our approach is based on the
use of a Convolutional Neural Network (CNN)
trained on 350 abstract with manually anno-
tated entities and relations. Our hypothesis is
that this deep learning model can be applied
to extract and classify relations between en-
tities for scientific papers at the same time.
We use the Part-of-Speech and the distances
to the target entities as part of the embedding
for each word and we blind all the entities by
marker names. In addition, we use sampling
techniques to overcome the imbalance issues
of this dataset. Our architecture obtained an
F1-score of 35.4% for the relation extraction
task and 18.5% for the relation classification
task with a basic configuration of the one step
CNN.

1 Introduction

Nowadays, there is a high increase in the publica-
tion of scientific articles every year, which demon-
strates that we are living in an emerging knowl-
edge era. Experts cannot deal with this explo-
sion of information and it is very hard to be up
to date about the state-of-the-art techniques in a
given field. This arduous task could be reduced
if we automatically identify concepts from scien-
tific articles and recognize the semantic relations
between them with Natural Language Processing
(NLP) techniques.

The Semantic Relation Extraction and Classifi-
cation in Scientific Papers task at SemEval-2018
task 7 (Gábor et al., 2018) provides a framework
for measuring the automatic annotation perfor-
mance by models which are trained on scientific
publications abstracts. The task defines six cate-
gories of relations between concepts and two tasks

are proposed: (1) the classification of the relations
between two entities in the predefined categories,
which is divided in two scenarios according to the
data used: clean or noisy; and (2) the extraction of
the relations given the entities from the clean data,
which also could involve their subsequent classifi-
cation.

In this paper, we describe our participation for
SemEval-2018 Task 7 on the extraction of rela-
tionships between entities in scientific papers and
also the subsequent classification in the predefined
classes of this relations with one step classifier.
The model is based on the Convolutional Neural
Network (CNN) proposed in (Kim, 2014), which
was the first work to exploit this architecture for
the task of sentence classification. CNN is a ro-
bust deep-learning architecture which has exhib-
ited good performance in others NLP tasks such
as semantic clustering (Wang et al., 2016), senti-
ment analysis (Dos Santos and Gatti, 2014) and
event detection (Nguyen and Grishman, 2015).
The model uses as the input of each instance the
transformation into real value vectors of the words
of the sentence, the distances to the target entities
of each word and the Part-of-Speech types. Fur-
thermore, we carry out a sampling technique to al-
leviate the imbalance issues of the dataset equaliz-
ing the number of the instances for all the classes.

2 Dataset

An annotated corpus for training and testing
the participating systems was provided in the
SemEval-2018 Task 7. The dataset contains 350
and 150 abstract from scientific articles for train-
ing and testing set, respectively.

The relation instances are divided into the fol-
lowing classes: USAGE, RESULT, MODEL, PART
WHOLE, TOPIC and COMPARISON. All of them
are asymmetrical except COMPARISON, where

793



both entities are involved in the same bidirectional
relation. A detailed description and analysis of
the corpus and its methodology used to collect
and process the scientific abstracts can be found
in (Gábor et al., 2018).

2.1 Pre-processing phase

The relations between scientific concepts are an-
notated pair by pair in the abstracts. All annotated
relations span within one sentence, thus, we split
the paragraphs of the abstracts into sentences with
NLTK tool1 to generate all the possible instances
in the corpus.

After that, each instance was tokenized, all
words were converted to lower-case and special
character were removed in order to clean the sen-
tences as the approach described in (Kim, 2014).
In addition, we used entity blinding for each rela-
tion to generalize the model, in which the two tar-
get entities of the relations were replaced by en-
tity markers as ”entity1” and ”entity2”, and ”en-
tity0” for the remaining entities. Since relations
can be asymmetrical, we considered both direc-
tions. In other words, for each pair of candidates
entities, we generated two different instances. For
the COMPARISON class, which is a bidirectional
relationship, we annotated both instances with the
same class label. For example, the sentence: ’We
suggest a method that mimics the behaviour of
the oracle using a neural network or a decision
tree.’ should be transformed to the relation in-
stances showed in Table 1.

Instances after entity blinding (entity1, entity2)
(oracle, neural network)
’We suggest a method that mimics the behaviour of
the entity1 using a entity2 or a entity0.’
(neural network, oracle)
’We suggest a method that mimics the behaviour of
the entity2 using a entity1 or a entity0.’
(oracle, decision tree)
’We suggest a method that mimics the behaviour of
the entity1 using a entity0 or a entity2.’
(decision tree, oracle)
’We suggest a method that mimics the behaviour of
the entity2 using a entity0 or a entity1.’
(neural network, decision tree)
’We suggest a method that mimics the behaviour of
the entity0 using a entity1 or a entity2.’
(decision tree, neural network)
’We suggest a method that mimics the behaviour of
the entity0 using a entity2 or a entity1.’

Table 1: Instances of a sentence in the corpus after ap-
plying the pre-processing phase with entity blinding.

1http://www.nltk.org

Table 2 shows the number of the instances ex-
tracted in the training set per each class. The None
class represents the number of pairs of entities that
are not related (negative instances). The number of
positive instances is very low compared to the neg-
ative ones, 1323 over 19210 (around 7%), mainly
because most classes are unidirectional and we an-
notated the reverse instance as None.

We followed a similar sampling technique de-
scribed in (Wang et al., 2017) to adjust the same
numbers of instances per each class. Therefore,
we randomly discard 60% of the negative in-
stances and we duplicate the instances in each
class until having the same number as the more
representative class, 483 corresponding to US-
AGE. Thus, we try to solve possible issues asso-
ciated with the imbalanced dataset.

Classes Instances
COMPARE 190
MODEL-FEATURE 326
PART WHOLE 234
RESULT 72
TOPIC 18
USAGE 483
None 17887
Total 19210

Table 2: Number of instances in the dataset.

3 Method

In this section, we present a CNN model to detect
and classify relationships between scientific con-
cepts. Figure 1 shows the whole process from its
input, which is a sentence with blinded entities,
until the output, which is the classification of the
instance into one of the relation types defined by
the task.

3.1 Word table layer

Firstly, we determined n as the maximum sentence
length in the training dataset. Those sentences
with lengths shorter than n are padded with an
auxiliary token ”0”. After that, we assigned a ran-
domly initialized vector for each different word,
creating thus a word embedding matrix: We ∈
R|V |×me where V is the vocabulary size and me
is the word embedding dimension. Finally, we ob-
tained a matrix x = [x1;x2; ...;xn] for each in-
stance where the words are represented by their
corresponding word embedding vectors.

In addition, we used the word position em-
bedding described in (Zeng et al., 2014), which

794



Position 
embeddings

POS
embeddings

We

on

mPOS

Wd2Wd1

Position 
embeddingsWord embeddings

Convolutional layer Pooling
layer

Softmax layer
with dropout

Look-up table layer

The <e1>classification accuracy</e1> of the method is evaluated on <e2>spoken language system domains</e2>

Preprocessing 

me

md md

entity2

…

the
entity1

of
the

method
is

evaluated

0

0
0

w

n-w+1 m

|V| 2n-1

n

X
S

z

Ws

k

o

WPOS

|P|

0

the entity1 of the method is evaluated on entity2
DT NN DT NNIN NNINVBZ VBNPOS:

Sentence:

distance1:
distance2:

-1 0 2 31 764 5
-8 -7 -5 -4-6 0-1-3 -2

Figure 1: CNN model for the semantic relation classification in scientific papers of SemEval-2018 Task 7.

maps the distances of each word with respect to
the two candidate entities into a real value vec-
tor using two position embedding matrices Wd1 ∈
R(2n−1)×md and Wd2 ∈ R(2n−1)×md where md is
the position embedding dimension. Moreover, we
extracted the Part-of-Speech (POS) feature of each
word (entities are marked as common nouns) and
create a POS embedding matrix as (Zhao et al.,
2016) WPOS ∈ R|P |×mPOS where P is the POS
types vocabulary size and mPOS is the POS em-
bedding dimension.

Finally, we created an input matrix X ∈
Rn×(me+mPOS+2md) which is represented by the
concatenation of the word embedding, the POS
embedding and the two position embeddings for
each word in the instance.

3.2 Convolutional layer
Once we obtained the input matrix, we applied the
convolutional operation with a context window of
size w to create higher level features. For each fil-
ter in f = [f1; f2; ...; fw], we created a score matrix
for the whole sentence as

si = g(

w∑

j=1

fjx
T
i+j−1 + b)

where b is a bias term and g is a non-linear func-
tion (such as tangent or sigmoid) of m number of
filters.

3.3 Pooling layer
We extracted the most relevant features of each
filter using the max function, which produces a
single value in each filter as zf = max{s} =
max{s1; s2; ...; sn−w+1}. Thus, we created a vec-
tor z = [z1, z2, ..., zm], whose dimension is the
total number of filters m representing the relation
instance. In the end, we concatenated the output
values of the different filters in this layer.

3.4 Softmax layer
In this layer, we performed a dropout to pre-
vent over-fitting obtaining a reduced vector zd ran-
domly dropping elements in z. After that, we fed
this vector into a fully connected softmax layer
with weights Ws ∈ Rm×k to compute the output
prediction values for the classification as

o = zdWs + d

where d is a bias term. At test time, the vector z of
a new instance is directly classified by the softmax
layer without a dropout.

3.5 Learning
We defined the CNN parameter set to be learned in
the training phase as θ = (We, WPOS , Wd1, Wd2,
Ws, Fm), where Fm are all of the m filters f. For
this purpose, we used the conditional probability

795



of a relation r obtained by the softmax operation
as

p(r|x, θ) = exp(or)∑k
l=1 exp(ol)

to minimize the cross-entropy function for all in-
stances (xi,yi) in the training set T as follows

J(θ) =
T∑

i=1

log p(yi|xi, θ)

In addition, we minimized the objective function
by using stochastic gradient descent over shuffled
mini-batches and the Adam update rule (Kingma
and Ba, 2014) to learn the parameters.

4 Results and Discussion

We define the CNN parameters for the experi-
ments using the values described in Table 3. The
number of epochs was fine-tuned in the validation
set using the stopping criteria.

Parameter Value
Maximal length in the dataset , n 152
Word embeddings dimension, Me 300
POS embeddings dimension, MPOS 10
Position embeddings dimension, Md 5
Filters for each window size, m 200
Filter sizes, w (3, 4, 5)
Dropout rate, p 50%
Mini-batch size 50
Non-linear function, g ReLU

Table 3: The CNN model parameters and their values
used for the results.

Our CNN system obtained an F1-score of
35.4% for the relation extraction task in which
only the detection of relation is taken into consid-
eration. The official results obtained for the rela-
tion classification task are showed in Table 4. Our
model reaches an F1-score in Macro-average of
18.5% with one step classifier, which means that
the extraction and classification are considered at
the same time. This performance was expected be-
cause we reached the similar results with a valida-
tion set created from the training set. Furthermore,
we correctly predicted 147 instances with correct
directionality over 367 (i.e. 40.05% in coverage).

The main problem is the high number of FP in
the majority of classes, which are the None in-
stances classified as a class. In some classes such
as PART WHOLE and USAGE we have also a high
number of FN compared to the total number of in-
stances. We consider that the main reason is that

the representation of the two directions of each re-
lation is very similar, only the position distances
and the target entity names are inverted, and the
CNN cannot distinguish between them.

Classes TP FP FN P R F1
COMPARE 8 116 11 6.45% 42.11% 11.19%
MODEL-FEATURE 36 185 37 16.29% 49.32% 24.49%
PART WHOLE 22 66 60 25% 26.83% 25.88%
RESULT 2 21 14 8.7% 12.5% 10.26%
TOPIC 0 0 3 0% 0% 0%
USAGE 41 96 133 29.93% 23.56% 26.37%
Micro-averaged - - - 18.38% 29.7% 22.71%
Macro-averaged - - - 14.39% 25.72% 18.46%

Table 4: Results over the dataset using a CNN model
measured by True Positives, False Positives, False Neg-
atives, Precision, Recall and F1-measure, respectively.

5 Conclusions and Future work

A CNN model is used for the Relation Classifica-
tion task of SemEval 2018 by UC3M-NII Team.
Moreover, we balanced the dataset using sampling
techniques, blinded the entities in the sentence and
aggregated position embedding and POS embed-
ding to the word embedding of each word to have
more representation of each instance. This archi-
tecture obtained an F1-score of 35.4% and 18.5%
for the relation extraction and classification task,
respectively.

As future work, we proposed to use a two steps
model to overcome the extraction of the rela-
tionships between two concepts and subsequently
classify them in the different semantic classes. In
addition, we also plan to rule out the reverse in-
stances of each class as None in order to avoid
having very similar representation with different
labels. We plan to tackle the directionality prob-
lem with post-processing rules after the classifica-
tion. Furthermore, we will train a CNN with dif-
ferent pre-trained word embedding models instead
of using a random initialization.

Funding

This work was supported by the Research Program
of the Ministry of Economy and Competitive-
ness - Government of Spain, (DeepEMR project
TIN2017-87548-C2-1-R) and the TEAM project
(Erasmus Mundus Action 2-Strand 2 Programme)
funded by the European Commission.

Acknowledgments

We would like to thank the members of the Aizawa
Laboratory and the HULAT research group for
their fruitful discussions which were held.

796



References
C.N. Dos Santos and M. Gatti. 2014. Deep convo-

lutional neural networks for sentiment analysis of
short texts. In Proceedings of the 25th International
Conference on Computational Linguistics, (COL-
ING 2014), Technical Papers, pages 69–78.

Kata Gábor, Davide Buscaldi, Anne-Kathrin Schu-
mann, Behrang QasemiZadeh, Haı̈fa Zargayouna,
and Thierry Charnois. 2018. Semeval-2018 Task
7: Semantic relation extraction and classification in
scientific papers. In Proceedings of International
Workshop on Semantic Evaluation (SemEval-2018),
New Orleans, LA, USA.

Y. Kim. 2014. Convolutional neural networks for sen-
tence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1746–1751.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Thien Huu Nguyen and Ralph Grishman. 2015. Event
detection and domain adaptation with convolutional
neural networks. In ACL-IJCNLP 2015 - 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing of the
Asian Federation of Natural Language Processing,
Proceedings of the Conference, volume 2, pages
365–371. Association for Computational Linguistics
(ACL).

Peng Wang, Bo Xu, Jiaming Xu, Guanhua Tian,
Cheng-Lin Liu, and Hongwei Hao. 2016. Se-
mantic expansion using word embedding cluster-
ing and convolutional neural network for improving
short text classification. Neurocomputing, 174, Part
B:806 – 814.

Wei Wang, Xi Yang, Canqun Yang, Xiaowei Guo, Xi-
ang Zhang, and Chengkun Wu. 2017. Dependency-
based long short term memory network for drug-
drug interaction extraction. BMC Bioinformatics,
18(16):578.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings
of the 25th International Conference on Computa-
tional Linguistics (COLING 2014), Technical Pa-
pers, pages 2335–2344, Dublin, Ireland. Dublin City
University and Association for Computational Lin-
guistics.

Zhehuan Zhao, Zhihao Yang, Ling Luo, Hongfei Lin,
and Jian Wang. 2016. Drug drug interaction extrac-
tion from biomedical literature using syntax convo-
lutional neural network. Bioinformatics.

797


