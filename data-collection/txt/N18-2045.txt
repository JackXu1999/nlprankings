



















































Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-Based Sentiment Analysis


Proceedings of NAACL-HLT 2018, pages 278–283
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Recurrent Entity Networks with Delayed Memory Update for
Targeted Aspect-based Sentiment Analysis

Fei Liu Trevor Cohn Timothy Baldwin
School of Computing and Information Systems

The University of Melbourne
Victoria, Australia

fliu3@student.unimelb.edu.au
t.cohn@unimelb.edu.au tb@ldwin.net

Abstract

While neural networks have been shown to
achieve impressive results for sentence-level
sentiment analysis, targeted aspect-based sen-
timent analysis (TABSA) — extraction of fine-
grained opinion polarity w.r.t. a pre-defined
set of aspects — remains a difficult task.
Motivated by recent advances in memory-
augmented models for machine reading, we
propose a novel architecture, utilising exter-
nal “memory chains” with a delayed mem-
ory update mechanism to track entities. On
a TABSA task, the proposed model demon-
strates substantial improvements over state-of-
the-art approaches, including those using ex-
ternal knowledge bases.1

1 Introduction

Targeted aspect-based sentiment analysis
(TABSA) is the task of identifying fine-grained
opinion polarity towards a specific aspect as-
sociated with a given target. The task requires
classification of opinions on different entities
across a range of different attributes, with the
expectation that there will be no overt opinion
expressed on a given entity for many attributes.
This can be seen in Example (1), e.g., where
opinions on the aspects SAFETY and PRICE are
expressed for entity LOC1 but not entity LOC2:2

(1) LOC1 is your best bet for secure although
expensive and LOC2 is too far.

Target Aspect Sentiment

LOC1 SAFETY positive
LOC1 PRICE negative
LOC2 TRANSIT-LOCATION negative

1Code available at https://github.com/
liufly/delayed-memory-update-entnet.

2Note that in our dataset, all entity mentions have been
pre-nomalised to LOCn, where n is an index.

The earliest work on (T)ABSA relied heavily
on feature engineering (Wagner et al., 2014; Kir-
itchenko et al., 2014), but more recent work based
on deep learning has used models such as LSTMs
to automatically learn aspect-specific word and
sentence representations (Tang et al., 2016a).

Despite these successes, keeping track of mul-
tiple entity–aspect pairs remains a difficult task,
even for an LSTM. As reported in Saeidi et al.
(2016), a target-dependent biLSTM is ineffective,
both in terms of aspect detection and sentiment
classification, compared to a simple logistic re-
gression model with n-gram features. Intuitively,
we would expect that a model which better cap-
tures linguistic structure via the original word se-
quencing should perform better, which provides
the motivation for this research.

More recently, successful works in (T)ABSA
have explored the idea of leveraging external
memory (Tang et al., 2016b; Chen et al., 2017).
Their models are largely based on memory net-
works (Weston et al., 2015), originally developed
for reasoning-focused machine reading compre-
hension tasks. In contrast to memory networks,
where each input sentence/word occupies a mem-
ory slot and is then accessed via attention indepen-
dently, recent advances in machine reading sug-
gest that processing inputs sequentially is bene-
ficial to overall performance (Seo et al., 2017;
Henaff et al., 2017).

However, successful machine reading models
may not be directly applicable to TABSA due to
the key difference in the granularity of inputs be-
tween the two tasks: on the Children’s Book Test
corpus (CBT), for example, competitive models
take as input a window of text, centred around can-
didate entities, with crucial information contained
within that window (Hill et al., 2015; Henaff et al.,
2017). In TABSA, given the fine-grained nature
of the task, it is common practice for models to

278



operate at the word- rather than chunk/sentence-
level. It is not uncommon to see examples like
Example (1), where the sentence starts with LOC1,
but the negative PRICE sentiment towards the en-
tity is not expressed until much later. Moreover,
phrases such as best bet and although play the role
of triggers, indicating that succeeding tokens bear
aspect/sentiment signal. This key difference ne-
cessitates the ability to model the delayed activa-
tion of memory updates.

In this work, we propose a novel model ar-
chitecture for TABSA, augmented with multiple
“memory chains”, and equipped with a delayed
memory update mechanism, to keep track of nu-
merous entities independently. We evaluate the
effectiveness of the proposed model over the task
of TABSA, and achieve substantial improvements
over a number of baselines, including one incor-
porating external knowledge bases, setting a new
state of the art in both sentiment classification and
aspect detection.

2 Methodology

Task description. In TABSA, a sentence s
typically consists of a sequence of words:
{w1, . . . , wi, . . . , wm} where wi denotes words
interleaved with one or more targets (t), which
we assume to be pre-identified as with LOC1
and LOC2 in Example (1). Following Saeidi
et al. (2016), we frame the task as a 3-class
classification problem: given a sentence s, a
pre-identified set of target entities T and fixed
set of aspects A, predict the sentiment polarity
y ∈ {positive, negative, none} over the full set
of target–aspect pairs {(t, a) : t ∈ T, a ∈
A}. For example, (LOC1,SAFETY) has gold-
standard polarity positive, while (LOC1,TRANSIT-
LOCATION) has polarity none.

Proposed model. To this end, we design a neu-
ral network architecture, capable of tracking and
updating the states of entities at the right time with
external memory, making it a natural fit for the
task. Specifically, our model maintains a number
of “memory chains” hj , one for each entity with
the key kj and dynamically updates the states (hj)
of them as it progresses through the sentence with
the help of the delay recurrence dj , taking previ-
ous activations into account. An illustration of our
model is provided in Figure 1.

wi

φ

GRU
key kj

delay dji−1

h̃ji

dji

σL

C

�
update
gate
gji

+
memory hji−1

hji

Figure 1: Illustration of our model with a single mem-
ory chain at time i. σ, φ and GRU represent Equa-
tions (2), (3) and (4), while circled nodes L, C, � and
+ depict the location, content terms, Hadamard prod-
uct, and addition, resp.

Delayed memory update. Update of each
memory chain is controlled by a gating mecha-
nism, consisting of three components: the “con-
tent” term wi · hji−1 , the “location” term wi · kj
and the “delay” term v ·dji where d

j
i carries knowl-

edge regarding previous activation of the gate and
v is a trainable parameter vector. All three terms
may lead to the activation of gji , but differ in how
they turn the gate on. While the “location” term
causes the gate to open for memory chains whose
keys (kj) match the input, the “content” term trig-
gers the activation when the content of the entities
(hji−1) matches the input. The delay term models
how and when the gate was turned on in the past
with a GRU (Chung et al., 2014) and how past ac-
tivations should influence the current one.

More formally, with arrows denoting processing
direction, the update gate is defined as:

−→g ji = σ(wi ·
−→
h ji−1 + wi · kj +−→v ·

−→
d ji ) (2)

where −→g ji is the update gate value for the j-th
memory at time i,3 kj is the embedding for the j-
th entity (key),

−→
h ji−1 is the hidden memory repre-

sentation responsible for keeping track of the state
of the j-th entity (content), and σ is the sigmoid
activation function. The delay recurrence

−→
d ji is

defined as:
−→̃
h ji = φ(

−→
U
−→
h ji−1 +

−→
V kj +

−→
Wwi) (3)

−→
d ji =

−−→
GRU(

−→̃
h ji ,
−→
d ji−1) (4)

3While −→g ji could instead be a vector for finer-grained
control, following Henaff et al. (2017), we use a scalar for
simplicity.

279



where
−→̃
h ji is the new candidate memory vector to

be incorporated into the existing memory
−→
h ji−1

to form the new memory
−→
h ji , φ is the paramet-

ric ReLU activation function (He et al., 2015), and−→
U ,
−→
V and

−→
W are trainable weight matrices.

Once the update gate value has been computed,
the j-th memory is then updated according to the
intensity of −→g ji :

−→̊
h ji =

−→
h ji−1 +

−→g ji �
−→̃
h ji (5)

where � is the Hadamard product, and
−→̊
h ji is the

unnormalised memory representation for the j-th
entity.

Essentially, gate −→g ji determines how much the
j-th memory should be updated, factoring in three
elements: (1) how similar the current input wi is to
the entity being tracked (kj); (2) how related the
current input wi is to the state of the j-th entity
(
−→
h ji−1); and (3) how past activation should influ-

ence the current one. Update of the memory of an
entity is only triggered when the gate is activated.

Normalisation. Following the update, the
model performs a normalisation step, allowing

the memory to forget:
−→
h ji =

−→̊
h ji/‖

−→̊
h ji‖ where

‖
−→̊
h ji‖ denotes the Euclidean norm of

−→̊
h ji . As all

information stored in
−→
h ji is constrained to be of

unit length, when new information
−→̃
h ji is added

to the existing memory
−→
h ji−1, the cosine distance

between the original and updated memory de-
creases, allowing the model to forget information
deemed out-of-date.

Bi-directionality. We apply the above steps both
forward and backward over the sentence, enabling
the model to capture sentiment terms appearing
before and after its associated entity. The memory
representation incorporating contexts from both
directions is obtained by hji =

−→
h ji +

←−
h ji , with←−

h ji computed analogously to
−→
h ji .

Final classifier. Our model predicts the senti-
ment polarity ŷ to the given target t and aspect
a embeddings by incorporating the states of all
tracked entities in the form of a weighted sum u:

pj = softmax
(

(kj)>Watt

[
t
a

])
(6)

u =
∑

j

pjhjm (7)

where [ ] denotes concatenation, m is sentence
length, and Watt is a trainable weight matrix.
Here, the values of both t and a take the em-
bedding values of their corresponding words (i.e.
t and a are drawn from the same embedding
matrix as are the input words wi). In the case
of multi-word aspect expressions (e.g. TRANSIT-
LOCATION), we take the mean of the embeddings
of the constituent words. We then transform u to
get:

ŷ = softmax(Rφ(Hu + a)) (8)

Training is carried out based on cross entropy loss.

L = CrossEntropy(y, ŷ) (9)

Comparision with EntNet. While our model
is largely inspired by Recurrent Entity Networks
(EntNets: Henaff et al. (2017)), it differs in
three main respects. First, we explicitly model
the delay of activation of the update gates gj with
the GRU in Equations (2) and (4) as opposed to
making hji implicitly assume the same responsi-
bility in EntNets. Admittedly, for EntNets
on bAbI and CBT, given the coarse-grained na-
ture and the difference in the granularity of inputs
(sentences vs. words), the demand for modelling
delayed memory update is less obvious. With this
delayed gate activation mechanism, we essentially
decouple the duty of capturing transitions of acti-
vations between steps from the task of entity state
tracking. That is, hjt is now dedicated to keep-
ing track of the state of the j-th entity only and
released from the burden of monitoring the acti-
vation of the update gate. Second, tailoring to the
task of TABSA, we incorporate not only the target
t but also the aspect a when trying to determine
the attention in the softmax function. Third, the
proposed model is bi-directional.

3 Experiments

3.1 Experimental Setup
Dataset. To test the effectiveness of our model,
we use Sentihood, a dataset constructed by
Saeidi et al. (2016) for the purpose of detecting
aspects and identifying sentiments for each target–
aspect pair, consisting of 5, 215 sentences, 3, 862
of which contain a single target, and the remainder
multiple targets. Each sentence is annotated with
a list of tuples {(t, a, y)} with each identifying the
sentiment polarity y towards a specific aspect a of

280



Model
Aspect Sentiment

Acc. F1 AUC Acc. AUC

LR (Saeidi et al., 2016) — 39.3 92.4 87.5 90.5
LSTM-Final (Saeidi et al., 2016) — 68.9 89.8 82.0 85.4
LSTM-Loc (Saeidi et al., 2016) — 69.3 89.7 81.9 83.9
LSTM+TA+SA (Ma et al., 2018) 66.4 76.7 — 86.8 —
SenticLSTM (Ma et al., 2018) 67.4 78.2 — 89.3 —
EntNet† 66.3 69.8 89.5 87.6 89.7
Our model† 73.5 78.5 94.4 91.0 94.8

Table 1: Performance on Sentihood. We take the results reported in Saeidi et al. (2016) and Ma et al. (2018),
resp; Bold = best performance; “—” = not reported; † = average performance over 5 runs.

a given target t in s. Ultimately, given a sentence
s, we are interested in both detecting the mention
of an aspect a for target t (a label other than none),
and also identifying the specific sentiment y w.r.t.
the target–aspect pair. A detailed description of
the task is presented in Section 2.

Model configuration. We initialise our model
with GloVe (300-D, trained on 42B tokens, 1.9M
vocab, not updated during training: Pennington
et al. (2014)) 4 and pre-process the corpus with to-
kenisation using NLTK (Bird et al., 2009) and case
folding. Training is carried out over 800 epochs
with the FTRL optimiser (McMahan et al., 2013)
and a batch size of 128 and learning rate of 0.05.
We use the following hyper-parameters for weight
matrices in both directions: R ∈ R300×3, H, U,
V, W are all matrices of size R300×300, v ∈ R300,
and hidden size of the GRU in Equation (4) is 300.
Dropout is applied to the output of φ in the final
classifier (Equation (8)) with a rate of 0.2. More-
over, we employ the technique introduced by Gal
and Ghahramani (2016) where the same dropout
mask is applied to the input wi at every step with
a rate of 0.2. Lastly, to curb overfitting, we reg-
ularise the last layer (Equation (8)) with an L2
penalty on its weights: λ‖R‖ where λ = 0.001.

We empirically set the number of memory
chains to 6, with the keys of two of them set to
the same embeddings as the target words LOC1
and LOC2, resp., and the other 4 chains with free
key embeddings which are updated during train-
ing, and therefore free to capture any entities.5

4http://nlp.stanford.edu/data/glove.
42B.300d.zip

5In line with the findings of Henaff et al. (2017) that ty-
ing key vectors damages model performance, we observed
similar performance deterioration when using tied keys only.
While we also experimented with various configurations (all

Consistent with Saeidi et al. (2016), we tackle
the data unbalanced problem (none� positive +
negative) by sampling the same number of training
instances within a batch randomly from each class.

Evaluation. We benchmark against baseline
systems presented in the works of Saeidi et al.
(2016) and Ma et al. (2018): (1) LR: a logistic re-
gression classifier with n-gram and POS tag fea-
tures; (2) LSTM-Final: a biLSTM taking the
final states as representations; (3) LSTM-Loc: a
biLSTM taking the states at the location where
target t is mentioned as representations; (4)
LSTM+TA+SA: a biLSTM equipped with complex
target and sentence-level attention mechanisms;
(5) SenticLSTM: an improved version of (4) in-
corporating the SenticNet external knowledge
base (Cambria et al., 2016). We additionally im-
plement a bi-directional EntNet with the same
hyper-parameter settings and GloVe embeddings
as our model (Henaff et al., 2017).

In terms of evaluation, we adopt the stan-
dard 70/10/20 train/validation/test split, and re-
port the test performance corresponding to the
model with the best validation score. Following
Saeidi et al. (2016), we consider the top 4 aspects
only (GENERAL, PRICE, TRANSIT-LOCATION,
and SAFETY) and employ the following evalua-
tion metrics: macro-average F1 and AUC for as-
pect detection ignoring the none class, and accu-
racy and macro-average AUC for sentiment clas-
sification. Following Ma et al. (2018), we also
report strict accuracy for aspect detection, as the
fraction of sentences where all aspects are detected
correctly.

tied vs. all free), this hybrid setup results in the best perfor-
mance on the validation set.

281



L
O

C
1

is a
lo

ve
ly

to
w

n
w

ith
pl

en
ty

of
re

st
au

ra
nt

s
,

yo
u

w
on

’t
go

hu
ng

ry
th

at
’s

fo
r

su
re

an
d if yo
u

w
an

te
d

to po
p

up in
to

lo
nd

on
fo

r
an

ev
en

in
g

th
er

e’
s

tr
ai

ns
ev

er
y

15
m

in
ut

es

E
n
t
N
e
t

O
ur

m
od

el

0 0.5 1

Figure 2: Example of the gate value gt averaged across
memory chains, forward and backward, in EntNet vs.
our model.

3.2 Results

The experimental results are presented in Table 1.

State-of-the-art results. Our model achieves
state-of-the-art results for both aspect detection
and sentiment classification. It is impressive that
the proposed model, equipped only with domain-
independent general-purpose GloVe embeddings,
outperforms SenticLSTM, an approach heavily
reliant on external knowledge bases and domain-
specific embeddings.

EntNet vs. our model. We see consistent per-
formance gains for our model in both aspect de-
tection and sentiment classification, compared to
EntNet, esp. for aspect detection, underlining
the benefit of delayed update gate activation.

3.3 Discussion

To better understand what the model has learned,
we visualise the average gate value gt in Figure 2,
where colour intensity indicates how much mem-
ory is updated. Observe that, while updated less by
the mention of LOC1, our model carries out mem-
ory updates upon seeing lovely town and plenty
of restaurants, key phrases associated with as-
pects such as GENERAL and DINNING. Perhaps
even more importantly, despite the distance be-
tween LOC1 and the final portion of the sentence,
our model recognises the relevance to TRANSIT-
LOCATION and keeps the update gates open to
track this particular aspect, as opposed to EntNet
where the last phase is overlooked. The ultimate
prediction for the TRANSIT-LOCATION aspect of
LOC1 is correct with our model (positive), but not
detected by EntNet (none), resulting in a false
negative. More interestingly, with EntNet, once
distant from a target, it can be frequently observed

2 3 4 5 6 7 8 9 10
76

77

78

79

# of memory chains

A
sp

ec
td

et
ec

tio
n
F
1

Figure 3: Sensitivity study of model performance to
# of memory chains n. Note that we report average
performance over 5 runs with standard deviation.

that the activation rate of gt tends to drop, a ten-
dency not so apparent with our model.

In Figure 3, we further study the sensitivity
of model performance to the number of mem-
ory chains n (2 of which are constrained to
track LOC1 and LOC2, the rest are unconstrained
chains). Observe that, when n < 5, the model suf-
fers from insufficient capacity (not enough mem-
ory chains) to capture the various aspects required
by the task, with aspect detection F1 remaining be-
low 78. In particular, when n = 2 (no uncon-
strained chains), model performance drops sub-
stantially to a F1 of 76.6 ± 0.4. Once n ≥ 5,
aspect detection F1 increases to around 78, and is
quite stable even with as many as n = 10 chains.

4 Conclusion

In this paper, we have proposed a model which
is capable of dynamically tracking entities with a
delayed memory update mechanism, and demon-
strated the effectiveness of the method over the
task of targeted aspect-based sentiment analysis.

Acknowledgments

We thank the anonymous reviewers for their
valuable feedback, and gratefully acknowledge
the support of Australian Government Research
Training Program Scholarship and National Com-
putational Infrastructure (NCI Australia). This
work was also supported in part by the Australian
Research Council.

References
Steven Bird, Ewan Klein, and Edward Loper.

2009. Natural Language Processing with Python.
O’Reilly Media.

282



Erik Cambria, Soujanya Poria, Rajiv Bajpai, and Bjo-
ern Schuller. 2016. Senticnet 4: A semantic re-
source for sentiment analysis based on conceptual
primitives. In Proceedings of the 26th International
Conference on Computational Linguistics (COLING
2016). Osaka, Japan, pages 2666–2677.

Peng Chen, Zhongqian Sun, Lidong Bing, and Wei
Yang. 2017. Recurrent attention network on mem-
ory for aspect sentiment analysis. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2017). Copen-
hagen, Denmark, pages 452–461.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence mod-
eling. In Proceedings of the NIPS 2014 Deep
Learning and Representation Learning Workshop.
Montréal, Canada.

Yarin Gal and Zoubin Ghahramani. 2016. A theo-
retically grounded application of dropout in recur-
rent neural networks. In Proceedings of Advances
in Neural Information Processing Systems (NIPS
2016). Barcelona, Spain, pages 1027–1035.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectifiers: Surpass-
ing human-level performance on imagenet classifi-
cation. In Proceedings of the 2015 IEEE Interna-
tional Conference on Computer Vision (ICCV 2015).
Washington, DC, USA, pages 1026–1034.

Mikael Henaff, Jason Weston, Arthur Szlam, Antoine
Bordes, and Yann LeCun. 2017. Tracking the world
state with recurrent entity networks. In Proceed-
ings of the 5th International Conference on Learn-
ing Representations (ICLR 2017). Toulon, France.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2015. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. In Proceedings of the 4th International Con-
ference on Learning Representations (ICLR 2016).
San Juan, Puerto Rico.

Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and
Saif Mohammad. 2014. NRC-Canada-2014: De-
tecting Aspects and Sentiment in Customer Re-
views. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014).
Dublin, Ireland, pages 437–442.

Yukun Ma, Haiyun Peng, and Erik Cambria. 2018.
Targeted aspect-based sentiment analysis via em-
bedding commonsense knowledge into an attentive
lstm. In Proceedings of the 32rd AAAI Conference
on Artificial Intelligence (AAAI 2018). New Orleans,
USA.

H. Brendan McMahan, Gary Holt, David Sculley,
Michael Young, Dietmar Ebner, Julian Grady,
Lan Nie, Todd Phillips, Eugene Davydov, Daniel
Golovin, et al. 2013. Ad click prediction: A view

from the trenches. In Proceedings of the 19th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD 2013). Chicago,
USA, pages 1222–1230.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2014). Doha, Qatar, pages
1532–1543.

Marzieh Saeidi, Guillaume Bouchard, Maria Liakata,
and Sebastian Riedel. 2016. Sentihood: Targeted
aspect based sentiment analysis dataset for urban
neighbourhoods. In Proceedings of the 26th Inter-
national Conference on Computational Linguistics
(COLING 2016). Osaka, Japan, pages 1546–1556.

Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh
Hajishirzi. 2017. Query-reduction networks for
question answering. In Proceedings of the 5th Inter-
national Conference on Learning Representations
(ICLR 2017). Toulon, France.

Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu.
2016a. Effective lstms for target-dependent senti-
ment classification. In Proceedings of the 26th Inter-
national Conference on Computational Linguistics
(COLING 2016). Osaka, Japan, pages 3298–3307.

Duyu Tang, Bing Qin, and Ting Liu. 2016b. Aspect
level sentiment classification with deep memory net-
work. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2016). Austin, USA, pages 214–224.

Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. DCU: Aspect-based polarity
classification for SemEval task 4. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014). Dublin, Ireland, pages 223–
229.

Jason Weston, Sumit Chopra, and Antoine Bordes.
2015. Memory networks. In Proceedings of the 3rd
International Conference on Learning Representa-
tions (ICLR 2015). San Diego, USA.

283


