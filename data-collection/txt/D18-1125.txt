



















































Genre Separation Network with Adversarial Training for Cross-genre Relation Extraction


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1018–1023
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1018

Genre Separation Network with Adversarial Training for Cross-genre
Relation Extraction

Ge Shi1, Chong Feng1∗, Lifu Huang2, Boliang Zhang2,
Heng Ji2, Lejian Liao1, Heyan Huang1

1 Beijing Engineering Research Center of High Volume Language Information Processing
and Cloud Computing Applications, School of Computer,
Beijing Institute of Technology, Beijing, 100081, China

{shige, fengchong, liaolj, hhy63}@bit.edu
2 Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180, USA

{huangl7, zhangb8, jih}@rpi.edu

Abstract
Relation Extraction suffers from dramatical
performance decrease when training a model
on one genre and directly applying it to a
new genre, due to the distinct feature distribu-
tions. Previous studies address this problem by
discovering a shared space across genres us-
ing manually crafted features, which requires
great human effort. To effectively automate
this process, we design a genre-separation net-
work, which applies two encoders, one genre-
independent and one genre-shared, to explic-
itly extract genre-specific and genre-agnostic
features. Then we train a relation classifier us-
ing the genre-agnostic features on the source
genre and directly apply to the target genre.
Experiment results on three distinct genres
of the ACE dataset show that our approach
achieves up to 6.1% absolute F1-score gain
compared to previous methods. By incorpo-
rating a set of external linguistic features, our
approach outperforms the state-of-the-art by
1.7% absolute F1 gain. We make all programs
of our model publicly available for research
purpose 1 .

1 Introduction

Relation extraction aims to identify and categorize
the semantic relation between two entity mentions
based on the contexts within the sentence. Super-
vised learning approaches have shown to be ef-
fective on this task. However, as relation extrac-
tion highly depends on information about entities
and their contexts, a supervised model trained in
one genre suffers from dramatical performance de-
crease when applied to a new genre, due to the dis-
tinct contexts among various genres.

Previous studies (Plank and Moschitti, 2013;
Nguyen and Grishman, 2014, 2015; Yu et al.,

∗*Corresponding author
1We make all cleaned codes and resources publicly

available at https://github.com/Garym713/Genre-Separation-
Network-for-Relation-Extraction

shared feature

Feature
Output

Feature
Input

Source Genre Target Genre

Feature
Projection

Feature
Engineering

Genre Separation
Network

source feature target feature

Figure 1: Comparison of Genre Separation Methods.

2015; Gormley et al., 2015) tackle this problem
by manually crafting genre-agnostic features such
as word clusters and word embeddings, to train
a genre-shared relation extractor. These methods
suffer from information loss due to the limited hu-
man knowledge to capture all genre-agnostic fea-
tures. As depicted in Figure 1, where red rectan-
gles are features shared by two genres, and blue
and green triangles are source and target genre fea-
tures respectively, Feature Engineering only cap-
tures a portion of the genre-agnostic features. Fu
et al. (2017), depicted as Feature Projection, ap-
plies a domain adversarial neural network to auto-
matically project the source and target genre fea-
tures into one unified feature space. However,
it unnecessarily introduces genre-specific features
which undermine the overall performance.

To address these problems, we propose a genre-
separation network, which consists of two sep-
arate Convolutional Neural Networks (CNNs) to
automatically separates genre-specific and genre-
agnostic features for each genre, which is depicted
as Genre Separation Network in Figure 1. To
avoid information loss during feature encoding,
we reconstruct the original input from the two
separate feature spaces via a novel reconstruction
loss. Then we use an adversarial similarity loss
to limit the genre-agnostic features into one fea-



1019

South Korean newspaper 
reported today …

Hi, this is Sara, from 
California.

… Source private 
CNN encoder

Shared  
CNN encoder

Target private 
CNN encoder

…

Ldiff

Ldiff

Input

Source

Target

Word Embedding Genre Separation Network

Deconvolution

Deconvolution

Gradient 
Reversed Layer

Fully  
Connected Layer

Sum

Sum

South Korean newspaper 
reported today …

Reconstructed Source

Hi, this is Sara, from 
California.

Reconstruction

Reconstructed Target

Relation  
Classifier

Genre  
Classifier

Relation  
Label

Genre  
Label (0/1)

Lrelation

Ladv
Adversarial 

Training

Source Specific Features Genre Shared Features Target Specific Features

Figure 2: Overall genre separation framework for cross-genre relation extraction

ture space. The genre-agnostic features are finally
used to predict entity relations in the source and
target genres.

2 Approach

2.1 Overview

An overview of our framework is presented
in Figure 2. We formulate the task as fol-
lows: given a labeled source genre corpus
S = {(s1, e11, e12, r1), ..., (sn, en1, en2, rn)},
where si = [wi1, ..., wim] denotes a sentence.
ei1 and ei2 denote two entity mentions, and ri
denotes the relation betwern ei1 and ei2, we
build a relation extraction model on S and ap-
ply it to a different target genre corpus T =
{(ŝ1, ê11, ê12), ..., (ŝn, ˆen1, ˆen2)}.

2.2 Genre Separation Network (GSN)

As shown in Figure 1, our goal is to distinguish the
genre-agnostic features (red rectangles) and genre-
specific features (blue triangles and green crosses).
Using source genre as an example, we apply a
source private CNN encoder on the source sen-
tence to generate the source-specific feature repre-
sentation fps , and a shared CNN encoder to gener-
ate genre-agnostic feature f cs . Similarly, we get f

p
t

and f ct from the target private CNN encoder and
the shared CNN encoder respectively. To separate
fps from f cs and separate f

p
t from f

c
t , we introduce

a difference loss following previous studies (Bous-
malis et al., 2016; Liu et al., 2017). More details
will be elaborated below.

Formally, given a source sentence (s, e1, e2, r)
where s = [w1, ..., wm], for each word wik,
we generate a multi-type embedding: ṽi =

[vi, pi, p̃i, ti, t̃i, ci, ηi] where vi denotes a pre-
trained word embedding. pi and p̃i are position
embeddings (Al-Badrashiny et al., 2017) indicat-
ing the distance from wi to e1 and e2 respec-
tively. ti and t̃i are entity type embedding (Ren
et al., 2016; Huang et al., 2016) of e1 and e2.
ci is the chunking embedding, and ηi is a bi-
nary digit indicating whether the word is within
the shortest dependency path between e1 and
e2 (Bunescu and Mooney, 2005; Liu et al., 2015;
Huang et al., 2017). All these embeddings except
pre-trained word embedding are randomly initial-
ized and optimized during training. Thus the in-
put layer is a sequence of word representations
V = {ṽ1, ṽ2, ..., ṽn}. We then apply the convo-
lution weightsW to each sliding n-gram phrase gj
with a biased vector b, i.e., g

′
j = tanh(W ·V )+ b.

All n-gram representations g
′
j are further used to

get an overall vector representation f by max-
pooling.

Once we obtain fps , f cs , f
p
s and f cs , we compute

the difference loss:

Ldiff = ||fp>s · f cs + f
p>
t · f ct ||F2

where ||.||F2 represents the squared Frobenius
norm.

To limit the genre-agnostic features from vari-
ous genres into a shared feature space, we further
design a genre adversarial training component. We
take the genre-agnostic features from both source
genre and target genre as input to a Gradient Re-
versed Layer(GRL) (Ganin et al., 2016), which
acts as a general hidden layer in forward process
and reverses the gradient in loss backward phase to
confuse the genre classifier, so that it cannot dis-
tinguish the input features from the source genre



1020

to the target genre:

Ladv =

Ns+Nt∑
i=0

dil log(d
ˆ
i) + (1− di) log(1− dˆi)

where di ∈ {0, 1} indicates the samples from the
source genre or the target genre, and Ns, Nt refer
to the number of examples in the source genre and
the target genre respectively. The term d̂i repre-
sents the probability of the sample from the source
genre, which is acquired by a linear function of the
genre classifier.

2.3 Genre Reconstruction
Till now, we can separate the features of each
genre into two separated feature spaces by opti-
mizingLdiff andLadv. However, there is no guar-
antee that the separated feature spaces are actually
meaningful. From equation Ldiff , we can see that
the fps , f

p
t would be easily optimized to zero if we

did not place a constraint, in which case the model
would fail to train. Therefore, we further recon-
struct the input sentence from both genre-specific
features and genre-agnostic features.

For each genre, e.g., the source genre, we
first sum the genre-specific feature vector fps
and genre-agnostic feature vector f cs , i.e., fs =
fps + f cs . We take fs as input to an unpooling
layer (Zeiler and Fergus, 2014) followed by a de-
convolutional neural netowrk (DcNN) (Xu et al.,
2014). The output of DcNN will include the same
number of decoded vectors V ∗ = {ṽ∗1, ṽ∗2, ..., ṽ∗n}
as input V = {ṽ1, ṽ2, ..., ṽn}. We optimize the
DcNN with the following reconstruction loss:

Lrec = 1−
n∑

i=0

|cos(ṽi, ṽ∗i )|

where n indicates the total number of words in the
input sentence, ṽi represents the input word rep-
resentation described in Section 2.2, and ṽ∗i is the
corresponding reconstructed vector from DcNN.

2.4 Cross Genre Relation Extraction
We next utilize the genre-agnostic features from
the source genre f cs to train a relation classifier.
We first feed f cs into a fully connected layer and
obtain a dense vector, then we use a linear projec-
tion function with a softmax as the relation classi-
fier to determine the relation type

Lrelation =

Ns∑
i=0

K∑
k=0

−xk log(xk)

where K is the total number of relation types. xk
represents the probability of entities being classi-
fied to category k.

We finally linearly combine all the loss func-
tions and jointly optimize the model using
SGD (Bottou, 2010).

L = Lrelation + αLdiff + βLrec + γLadv

where α, β, γ denote the weights of various
losses.2

3 Experiments

3.1 Data and Parameters
We evaluate our approach on the English portion
of ACE2005 dataset (Walker et al., 2006; Ji et al.,
2010; Hong et al., 2015; Yu et al., 2016). It covers
6 genres: Newswire (nw), Broadcast Conversa-
tion (bc), Broadcast News (bn), Telephone Speech
(cts), Usenet Newsgroups (un), and Weblogs (wl)
and 11 relation types. Following previous work
(Yu et al., 2015; Nguyen and Grishman, 2015;
Gormley et al., 2015), we use newswire and broad-
cast news (nw&bn) as training data, half of bc
as development set, and test the model on the re-
maining half of bc, cts, wl. We conduct the same
preprocessing steps as previous work and yield
43,497 entity pairs in total for training.

Table 3.1 shows the hyper-parameters that we
use to train our model.

Hyper-parameters Value
# of Filters in Shared/Private CNN encoder 800
Filter Width 3
Hidden Size of Fully Connected Layer 300
Position Embedding Size 25
Entity Type/Chunking Embedding Size 25
Optimizer SGD
Learning rate 0.001
Pre-trained Word Embedding Glove-1003

Table 1: Hyper-parameters

3.2 Baseline Models
We compare our approach with the following
methods:

FCM (Gormley et al., 2015) is a feature combi-
nation model which composes word embeddings
with traditional linguistic features.

Hybrid FCM (Gormley et al., 2015) incorpo-
rates many more selected linguistic features com-
pared to FCM.

2We set α = 0.075, β = 0.01, γ = 0.25 when the model
performs the best on the development set.



1021

LRFCM (Yu et al., 2015) is a feature compo-
sitional model which scales to more features and
more labels.

Log-linear & DNN (Nguyen and Grishman,
2015) explores CNN, Bi-GRU, Forward GRU,
Backward GRU, and log-linear model for relation
extraction. We compare against the performances
of individual models instead of assembled models.

CNN+DANN (Fu et al., 2017) utilizes do-
main adversarial training to automatically extract
genre-agnostic features for source and target genre
within one feature space.

3.3 Comparison and Analysis

Table 2 shows the cross-genre relation extrac-
tion performance among various methods. Our
approach significantly outperforms all previous
baselines by 1.2%-1.7% (F1). Table 3 presents the
results without using extra linguistic features (only
embedding based features), our approach achieves
2.9%-6.1% absolute gain over baselines. The ab-
lation test by removing each component at a time
justifies the contribution of each method. The
difference and reconstruction components ensure
the features to be separated into shared and pri-
vate spaces, and they can remove redundant genre-
specific features to some extent. That’s why we
got a significant F-score improvement when only
utilizing these two components. The adversarial
training component can further encourage the fea-
tures of each genre from the shared encoder to be
close to each other, thus the performance is further
improved. We also conduct ablation experiments
on each feature components. Among the linguistic
features we used, the entity type and position fea-
tures contribute the most to the performance. For
example, the relation extraction performance de-
creases by about 8% if removing the entity type
feature. We analyze the reasons and find that the
entity type feature is vital to ensure the types of
two entity mentions to be consistent with the hard
entity type constraint of each relation type defined
in ACE schema.

For the remaining errors, we notice that our
model easily fails to predict relations between
nested entity mentions. For example, in “Our
president has put homeland security in the hands
of failed Republican hacks.”, our model mistak-
enly predicts the relationship between Republican
and failed Republican hacks as None instead of
organization-affiliation, due to the lack of context

System bc cts wl
FCM 61.9 52.93 50.36

Hybrid FCM 63.48 56.12 55.17
LRFCM 59.4 - -

Log-linear 57.83 53.14 53.06
CNN 63.26 55.63 53.91

Bi-GRU 63.07 56.47 53.65
Forward GRU 61.44 54.93 55.10

Backward GRU 60.82 56.03 51.78
CNN+DANN 65.16 - -
w/o Difference 59.87 54.10 52.73
w/o Adversarial 64.42 57.32 55.63

w/o Reconstruction 59.12 53.48 53.17
Our Approach 66.38 57.92 56.84

Table 2: Cross Genre Relation Extraction Perfor-
mances (Macro F-score %) on Various Genres. w/o
Difference means to ablate the Ldiff loss. w/o Adver-
sarial means to ablate the adversarial training compo-
nent. w/o Reconstruction means to ablate the genre re-
construction component.

System bc cts wl
CNN 46.3 40.8 35.8
GRU 45.2 40.2 35.1

Bi-GRU 46.7 41.2 36.5
Our Approach 52.8 45.3 39.4

Table 3: Cross Genre Relation Extraction Perfor-
mances (Macro F-score %) on Various Genres (without
linguistic features)

information. Besides, we also observe some failed
cases where the two entities are separated with a
extreme wide context, which suggests us to in-
corporate dependency path based deep neural net-
works into the framework.

4 Related Work

Previous studies on cross-genre relation extraction
either manually or automatically extract genre-
agnostic features (Plank and Moschitti, 2013;
Nguyen and Grishman, 2014; Yu et al., 2015;
Gormley et al., 2015; Nguyen and Grishman,
2015), suffering from human labor and limited
coverage of effective features, or automatically
project source and target genres into one unified
feature space and learn genre shared features (Fu
et al., 2017), which inevitably introduces noise
from genre specific features. Compared with these
methods, our approach separates genre-specific
features from genre-agnostic features first, and
then automatically extracts meaningful features
for cross-genre relation extraction.

Our work is also related to studies on domain
separation networks (Bousmalis et al., 2016; Liu
et al., 2017; Chen et al., 2017), which explic-
itly extracts features from two separate subspaces:
domain-specific and domain-agnostic. We adopt



1022

a similar framework for cross-genre relation ex-
traction and introduce a novel reconstruction com-
ponent which is proved to be suitable to relation
extraction.

5 Conclusions

We propose a genre separation framework for
cross-genre relation extraction. Without requiring
human crafted features, this framework can effec-
tively separate genre-specific features from genre-
agnostic ones, and automatically extract mean-
ingful features for the task. To ensure the sepa-
ration of features within each genre and enforce
the genre agnostic features from source genre and
target genre to be in the same feature space, we
design a difference loss and an adversarial train-
ing component. Experiments on various genres
demonstrate the effectiveness of our framework.
In the future, we will extend our framework to
cross-lingual and cross-domain information ex-
traction tasks.

6 Acknowledgements

We thank the anonymous reviewers for their valu-
able comments and suggestions. This work is sup-
ported by the National Key Research and Devel-
opment Program of China No. 2017YFB1002101,
the National Natural Science Foundation of China
No. U1636203, China Scholarship Council CSC,
No. 201706030131. RPI co-authors were sup-
ported by the U.S. DARPA AIDA Program No.
FA8750-18-2-0014 and U.S. ARL NS-CTA No.
W911NF-09-2-0053. The views and conclusions
contained in this document are those of the authors
and should not be interpreted as representing the
official policies, either expressed or implied, of the
U.S. Government. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernment purposes notwithstanding any copyright
notation here on.

References
Mohamed Al-Badrashiny, Jason Bolton, Arun Tejasvi

Chaganty, Kevin Clark, Craig Harman, Lifu Huang,
Matthew Lamm, Jinhao Lei, Di Lu, Xiaoman Pan,
et al. 2017. Tinkerbell: Cross-lingual cold-start
knowledge base construction. In TAC.

Léon Bottou. 2010. Large-scale machine learning
with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pages 177–186. Springer.

Konstantinos Bousmalis, George Trigeorgis, Nathan
Silberman, Dilip Krishnan, and Dumitru Erhan.
2016. Domain separation networks. In Advances in
Neural Information Processing Systems, pages 343–
351.

Razvan C Bunescu and Raymond J Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the conference on human
language technology and empirical methods in nat-
ural language processing, pages 724–731. Associa-
tion for Computational Linguistics.

Xinchi Chen, Zhan Shi, Xipeng Qiu, and Xuanjing
Huang. 2017. Adversarial multi-criteria learning
for chinese word segmentation. arXiv preprint
arXiv:1704.07556.

Lisheng Fu, Thien Huu Nguyen, Bonan Min, and
Ralph Grishman. 2017. Domain adaptation for re-
lation extraction with domain adversarial neural net-
work. In Proceedings of the Eighth International
Joint Conference on Natural Language Processing
(Volume 2: Short Papers), volume 2, pages 425–429.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor Lempitsky.
2016. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research,
17(1):2096–2030.

Matthew R Gormley, Mo Yu, and Mark Dredze.
2015. Improved relation extraction with feature-rich
compositional embedding models. arXiv preprint
arXiv:1505.02419.

Yu Hong, Di Lu, Dian Yu, Xiaoman Pan, Xiaobin
Wang, Yadong Chen, Lifu Huang, and Heng Ji.
2015. Rpi blender tac-kbp2015 system description.
In Proc. Text Analysis Conference (TAC2015).

Lifu Huang, Jonathan May, Xiaoman Pan, and Heng Ji.
2016. Building a fine-grained entity typing system
overnight for a new x (x= language, domain, genre).
arXiv preprint arXiv:1603.03112.

Lifu Huang, Avirup Sil, Heng Ji, and Radu Florian.
2017. Improving slot filling performance with at-
tentive neural networks on dependency structures.
arXiv preprint arXiv:1707.01075.

Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the tac 2010
knowledge base population track. In Third Text
Analysis Conference (TAC 2010), volume 3, pages
3–3.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classifica-
tion. arXiv preprint arXiv:1704.05742.

Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou,
and Houfeng Wang. 2015. A dependency-based
neural network for relation classification. arXiv
preprint arXiv:1507.04646.



1023

Thien Huu Nguyen and Ralph Grishman. 2014. Em-
ploying word representations and regularization for
domain adaptation of relation extraction. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), volume 2, pages 68–74.

Thien Huu Nguyen and Ralph Grishman. 2015.
Combining neural networks and log-linear mod-
els to improve relation extraction. arXiv preprint
arXiv:1511.05926.

Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 1498–1507.

Xiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng Ji,
and Jiawei Han. 2016. Afet: Automatic fine-grained
entity typing by hierarchical partial-label embed-
ding. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1369–1378.

Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. Ace 2005 multilin-
gual training corpus. Linguistic Data Consortium,
Philadelphia, 57.

Li Xu, Jimmy SJ Ren, Ce Liu, and Jiaya Jia. 2014.
Deep convolutional neural network for image decon-
volution. In Advances in Neural Information Pro-
cessing Systems, pages 1790–1798.

Dian Yu, Xiaoman Pan, Boliang Zhang, Lifu Huang,
Di Lu, Spencer Whitehead, and Heng Ji. 2016.
Rpi blender tac-kbp2016 system description. In
Proceedings of the 2016 Text Analysis Conference
(TAC2016).

Mo Yu, Matthew R Gormley, and Mark Dredze. 2015.
Combining word embeddings and feature embed-
dings for fine-grained relation extraction. In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
1374–1379.

Matthew D Zeiler and Rob Fergus. 2014. Visualizing
and understanding convolutional networks. In Eu-
ropean conference on computer vision, pages 818–
833. Springer.


