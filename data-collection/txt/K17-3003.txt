



















































CoNLL-2017 Shared Task


Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 31–39,
Vancouver, Canada, August 3-4, 2017. c© 2017 Association for Computational Linguistics

Combining Global Models for Parsing Universal Dependencies

Tianze Shi Felix G. Wu Xilun Chen Yao Cheng
Cornell University

{tianze,felixgwu,xlchen,yc2258}@cs.cornell.edu

Abstract

We describe our entry, C2L2, to the
CoNLL 2017 shared task on parsing
Universal Dependencies from raw text.
Our system features an ensemble of
three global parsing paradigms, one
graph-based and two transition-based.
Each model leverages character-level bi-
directional LSTMs as lexical feature ex-
tractors to encode morphological informa-
tion. Though relying on baseline tokeniz-
ers and focusing only on parsing, our sys-
tem ranked second in the official end-to-
end evaluation with a macro-average of
75.00 LAS F1 score over 81 test treebanks.
In addition, we had the top average perfor-
mance on the four surprise languages and
on the small treebank subset.

1 Introduction

General Parsing Approach Our submitted sys-
tem to the CoNLL 2017 shared task (Zeman
et al., 2017) focuses only on the task of depen-
dency parsing, assuming that tokenization, sen-
tence boundary detection, part-of-speech (POS)
tagging and morphological features are already
handled by a baseline model. In this paper, we
highlight our neural-network-based feature extrac-
tors and ensemble of global parsing models, in-
cluding two novel global transition-based models.

Bi-directional long-short term memory net-
works (Graves and Schmidhuber, 2005, bi-
LSTMs) have recently achieved state-of-the-art
performance on syntactic parsing (Kiperwasser
and Goldberg, 2016; Cross and Huang, 2016;
Dozat and Manning, 2017). Our system leverages
the representational power of bi-LSTMs to gen-
erate compact features for both graph-based and
transition-based parsing frameworks. The latter

further enables the application of dynamic pro-
gramming techniques (Huang and Sagae, 2010;
Kuhlmann et al., 2011) for global training and ex-
act decoding. With just two bi-LSTM vectors as
features, all three global parsing paradigms in our
system have efficient Opn3q implementations. The
full system consists of 3-5 each of these unlabeled
parsing models (9-15 in total, depending on the
treebank), and another ensemble of arc labelers.

Adaptation of General Approach to the Shared
Task The CoNLL 2017 shared task presents
two unique challenges: 1. A large fraction of
the datasets are morphologically-rich languages.
Some languages have an exceedingly-high out-of-
vocabulary ratio of over 30%. 2. For many lan-
guages, very little training data is provided. Fur-
thermore, there are four surprise language, for
which we only have tens of sample sentences.

We address the first challenge with character-
level bi-LSTMs, which have previously been
shown to be effective in multi-lingual POS tag-
ging (Plank et al., 2016) and dependency pars-
ing (Ballesteros et al., 2015; Alberti et al., 2017).
Character-level representation gives better cover-
age, and it directly learns sub-word information
through end-to-end training.

The second challenge is approached by transfer-
ring delexicalized information. For each of those
languages with little training data, we select the
most similar language according to linguistic ty-
pology. We then train delexicalized models taking
only part-of-speech and morphology tags as input
features, which are made available through base-
line prediction during test time.

Our full system scored a macro-average LAS
F1 score of 75.00, which ranked second among
all participating systems. Additionally, in the cat-
egories of small treebanks and surprise languages,
we obtained the best average performance.

31



2 System Overview

Character bi-LSTM / delexicalized features

Word-level bi-LSTM

AEDP

Ensemble

IV. Arc Labeling

AHDPMST

II. Feature Extraction

III. Unlabeled Parsing

I. UDPipe pre-processing

Figure 1: Overview of our system.

Figure 1 illustrates our pipelined system. It
processes raw texts in four stages starting from
baseline UDPipe (Straka et al., 2016) tokenization
and sentence delimitation. For this stage we use
predictions provided by the organizers instead of
training our own UDPipe models.

For each sentence, Stage II (§3) extracts a dense
feature vector for each word in the sentence. For
most languages, we employ character-level bi-
LSTMs to capture morphological information. On
top of the character-level representations, there is
another layer of bi-LSTMs processing at the word
level, the output of which gives context-sensitive
features associated with every word in the sen-
tence. For the four surprise languages and a se-
lected set of languages with small training tree-
banks, we substitute the character-level encodings
of each word in Stage II with concatenation of
part-of-speech (POS) tag embeddings and mor-
phological feature embeddings, but keep the word-
level bi-LSTMs. We call these delexicalized fea-
tures as opposed to the lexicalized features in the
general case. All later stages are kept the same.
The POS tags and morphological features are pro-
vided by baseline UDPipe predictions.

Stage III (§4.1) focuses on unlabeled parsing
with an ensemble of three global models, one first-
order graph-based maximal spanning tree algo-
rithm (MST), and two transition-based, namely

arc-hybrid and arc-eager dynamic programming
(AHDP and AEDP). They share the same un-
derlying feature extractors. We combine outputs
from the unlabeled parsing models with a uniform
weight reparsing model (Sagae and Lavie, 2006).

The final stage (§4.2) of our system is arc label-
ing. Based on the extracted LSTM features and
predicted unlabeled parse trees, this stage assigns
the highest scoring label to each arc. Similar to
Stage III, we train multiple models with different
random initializations, and the ensemble predic-
tion is obtained via majority vote.

Our system was implemented with DyNet
(Neubig et al., 2017). Each single model is of
small size and runs efficiently. The submitted full
system completed the test phase in 4.64 hours with
2 threads. We provide implementation details for
all the modules and training process in §6. The
code is available at https://github.com/
CoNLL-UD-2017/C2L2.

3 Feature Extractors

In Stage II of our system, we first extract features
for each word in isolation, then consider one sen-
tence at a time for context-sensitive representa-
tions. These two feature extractors both leverage
the representational power of bi-LSTMs.

3.1 Character LSTMs

Among the most straightforward ways for repre-
senting a word are through binary features or word
embeddings. Though popular in many existing
parsers, they are not ideal for languages with high
out-of-vocabulary (OOV) ratios. In Universal De-
pendencies, the 56 development sets have an av-
erage OOV ratio of 14.4%, with four languages
(et, hu, ko and sk) higher than 30%, posing a
severe challenge for lexical representation. On the
other hand, the average out-of-charset (OOC) ra-
tio is 0.03%, with the highest (zh) not exceeding
0.1%, suggesting the promise of character-level
representations in terms of coverage.

Our system adopts character-level bi-LSTMs
similar to Plank et al. (2016) and Ballesteros et al.
(2015). They show that the obtained sub-word in-
formation is especially useful for rare and OOV
words in morphologically-rich languages.

Formally, for a word w with its character se-
quence rBOW, c1, ..., cm, EOWs, with two spe-
cial begin-of-word (BOW, or c0) and end-of-word
(EOW, or cm`1) symbols, we run a forward and a

32



backward LSTM at layer l:

r
Ñ
cli s “ LSTMforwardpr

ÑÐ

cl´1i sq

r
Ð
cli s “ LSTMbackwardpr

ÑÐ

cl´1i sq
ÑÐ

cli “
Ñ
cli ˝

Ð
cli

each cli denotes the vector representation at layer
l for ci, ˝ denotes concatenation of vectors, and
r¨s is a shorthand for a list of vectors. The inputs
to the first layer c0i are character embeddings that
are jointly trained with the model. We take the
concatenation of

Ñ
cm`1 and

Ð
c0 at the final layer of

the LSTMs as the output vectors. We use two-
layer bi-LSTMs in our system.

Efficiency Improvement Considering the Zip-
fian distribution for word frequencies, most of the
time is spent on getting char bi-LSTM representa-
tions for frequent words. On the other hand, for
those words, it is considerably easier to train de-
cent representations even without char bi-LSTMs.
We thus directly learn the dense word vectors for
frequent words, as a proxy for character-level bi-
LSTMs and they can be considered as fast look-up
tables without actually running the LSTMs1 .

3.2 Delexicalized Features
For languages with small treebanks, the provided
data is not adequate to learn character bi-LSTMs.
We choose to use the available delexicalized in-
formation predicted by UDPipe. Namely, we use
information from two fields: universal POS tags
(UPOS) and morphological tags.

To get dense vectors for each word w in the
same form as the output of char bi-LSTMs, we
use the concatenation of UPOS embeddings

Ñ
pw

and the bag-of-morphology (BOM) embeddings
poolpt

Ñ
mwuq. The BOM embeddings require a

pooling function poolp¨q because each word may
receive multiple morphological tags. In our sys-
tem, we use element-wise max operator as the
pooling function.

3.3 Word-level LSTMs
The character bi-LSTM vector for each word is
computed in isolation from other words in the
sentence. In this module, we again leverage bi-
LSTMs for integration of contextual information.

1In retrospect, we could have used pre-trained word vec-
tors as extra features.

Similar to §3.1, we pad a sentence with two
special begin-of-sentence (BOS, or w0), and
end-of-sentence (EOS, or wn`1) symbols into
rBOS, w1, ..., wn, EOSs. Inputs to the first layer
are character bi-LSTM encodings, or concatena-
tion of POS-tag and BOM embeddings in the
case of delexicalized models. We take the bi-
directional vectors

ÑÐ
wi at the final layer as the

context-sensitive representation associated with
wi. All parsing components to be described in the
following section will build from these vectors.

4 Parsing Components

Our system parses a sentence in two steps, first
predicting the unlabeled parse tree, and next pre-
dicting the label for each arc in the unlabeled tree.

4.1 Global Models for Unlabeled Parsing

Our system includes one graph-based and two
transition-based, a total of three different global
parsing paradigms. All of these models only han-
dle projective cases. For this reason, before train-
ing, we projectivize all gold-standard trees in the
training sets.

First-order Graph-based Parsing Our graph-
based model is based on the popular edge-factored
Eisner’s algorithm (Eisner, 1996; Eisner and Satta,
1999). Each potential arc ph, mq in the graph
(Opn2q in total with sentence length n) is first
scored with a function scoreMSTph, mq. Then Eis-
ner’s algorithm is used to find the maximum span-
ning tree among all possible projective trees:

argmax
valid parses y

ÿ

ph,mqPy
scoreMSTph, mq

Following Dozat and Manning (2017), we use a
deep bi-affine scoring function:

scoreMSTph, mq “ vᵀhUvm`bh ¨vh`bm ¨vm`b

where

vh “ MLPMST-headp
ÑÐ
hq

vm “ MLPMST-modp
ÑÐ
mq

are representations transformed by two multi-layer
perceptrons (MLPs) from their bi-LSTM vectors.
We train separate MLPs for head and modifier
transformation. The weight matrix U , bias vectors
bh, bm and term b are parameters of the function.

33



Global Transition-based Parsing We include
global training and exact decoders for two tran-
sition systems, arc-hybrid and arc-eager. They
are based on dynamic programming approaches
(Huang and Sagae, 2010; Kuhlmann et al., 2011),
thus we call the two models AHDP and AEDP.

The dynamic programming shares computation
for parser configurations with the same extracted
features. In our system, we only use two bi-LSTM
vectors, one from the top of the stack (

ÑÐ
s0), and one

from the top of the buffer (
ÑÐ
b0). This compact set

of features enables dynamic programming to com-
press the exponentially-large search space down to
Opn3q for the two transition systems.

Below we illustrate the AHDP decoder, with
AEDP being similar. The bare deduction system,
adapted from Kuhlmann et al. (2011) is:

sh
ri, js

rj, j ` 1s
reñ

rk, is ri, js
rk, js

kñi

reð
rk, is ri, js
rk, js

iðj

each deduction item ri, js corresponds to a push
computation detailed in Kuhlmann et al. (2011).
For the purpose of our decoder, the deduction item
can also be understood as a parser configuration
with wi being s0 and wj being b0. The deduction
system has an axiom r0, 1s and goal r0, n`1s cor-
responding to initial and terminal configurations.

Next, we incorporate scoring functions:

ri, js : v
rj, j ` 1s : 0

pshq
rk, is : v1 ri, js : v2
rk, js : v1 ` v2 `∆

preñq

where ∆ “ scoreshp
ÑÐ
wk,

ÑÐ
wiq ` scorereñp

ÑÐ
wi,

ÑÐ
wjq.

The scoring functions are bi-affine and take the
same form as scoreMSTp¨q. The highest-scoring
proof for the goal item r0, n ` 1s constitutes the
predicted transition sequence.

Training We employ discriminative training
strategies for all three global parsing models.
Cost-augmented decoding (Taskar et al., 2005;
Smith, 2011) is applied during training. A correct
parse tree is instructed to get higher scores than
an incorrect parse tree by a margin set to be the
number of incorrectly-attached nodes (Hamming
distance). This technique has previously been ap-
plied in training a neural MST parser (Kiperwasser
and Goldberg, 2016).

UAS F1 LAS F1
Official
Ranking

Big Treebanks 85.16 79.85 2
Small Treebanks 70.59 61.49 1
PUD Treebanks 80.17 71.49 2

Surprise Languages 58.40 47.54 1

Overall 80.35 75.00 2

Table 1: Official UAS and LAS scores on the test
sets. Rankings are based on the macro-average
LAS F1 scores over all treebanks in the set.

Target Source UAS F1 LAS F1
Official
Ranking

bxr hi 50.79 31.98 2
hsb cs 69.45 61.70 1
kmr fa 54.51 47.53 1
sme fi 58.85 48.96 1

Average 58.40 47.54 1

Table 2: Evaluation results of our system on the
surprise languages. We show the source treebanks
from which we trained the delexicalized parsers.

4.2 Arc Labeling
We separate out the stage of arc labeling and adopt
a simple labeler proposed by Kiperwasser and
Goldberg (2016). For a predicted arc with h as
the head and m being the modifier, their associ-
ated vectors are concatenated to be the input to a
MLP. Each dimension of the output from the MLP
corresponds to the score for a potential label, And
we select the label with the highest score:

labelph, mq “ argmax
l

MLPlabell p
ÑÐ
h ˝

ÑÐ
mq

The arc-labeling models are trained with gold-
standard ph, mq tuples. And we use a discrimina-
tive hinge loss, with margin of 1.

5 Results

The main official evaluation results are given in
Table 1. Our system achieved second place in
overall ranking. When considering average perfor-
mance on small treebanks (8 treebanks) and sur-
prise languages (4 treebanks, detailed in Table 2),
we scored the first among all teams.

We show per-treebank LAS F1 results in Fig-
ure 2. Our system lacks customized modules

34



20

30

40

50

60

70

80

90

100

hu ko lv uk et eu got tr ug el da cu ga pl sk fa hr de ro bg id kk hi ar ur zh ca he vi ja

Best Ours Avg. Top 10 We use UDPipe tokenization

(larger) Gap between ours and avg. top 10 (smaller)
Languages w/ single treebank

Languages w/ multiple treebanks

50

55

60

65

70

75

80

85

90

95
Best Ours Avg. Top 10

Figure 2: LAS F1 score per treebank. The top/bottom row results are on languages with single/multiple
treebank(s). For comparison, we include the best official result and the average of the top ten results on
each treebank. Each row is sorted by the gap between our system and the average of the top ten.

for tokenization and sentence boundary detection,
which is reflected by the gap between our system
and the best-performing systems on ja, vi, he
and zh. The other large source of gaps comes
from languages with large non-projective ratios,
such as grc, la and nl. The global transition-
based AHDP and AEDP models are not compat-
ible with non-projective parsing, and we did not
implement or test with non-projective graph-based
parsers due to time and resource constraints.

Our system performs relatively well on lan-
guages with high OOV ratios, such as hu, ko, lv
and et, with the help of character bi-LSTMs. In
addition, the strategies of concatenating multiple
training treebanks for the same language (see §6)
brought success on small treebanks.

Table 3 gives the performance of our system
on the 14 additional parallel treebanks. The re-
sults are largely consistent with in-domain eval-
uation results, and we ranked within top third
for most treebanks except ja pud, en pud and
ru pud. We did not implement our own tokenizer
for Japanese, explaining the gap. For the other two

languages, our selected models were not domain-
robust. We perform a post-evaluation analysis and
parse the PUD treebanks (Nivre et al., 2017a) with
models trained on the canonical treebanks. The
two languages observe an improvement on LAS
scores of 7.53 and 14.73 respectively.

Ablation Analaysis To examine the effect of in-
dividual components in our ensemble system, we
evaluate several variations, where we use single or
an incomplete set of models for unlabeled pars-
ing and arc-labeling. Results are shown in Ta-
ble 4. AEDP gives higher unlabeled parsing per-
formance, and an ensemble of three instances of
AEDPs achieves comparable performance to our
full system. The arc-labeling ensemble gives an-
other gain in LAS result of 0.31.

6 Implementation Details

Our system was trained on the UD 2.0 dataset
(Nivre et al., 2016, 2017b), with the provided
training and development splits when available.
For languages without development sets, we split

35



Target
Treebank

Selected
Model

LAS F1 Rank

pt pud pt 78.48 1
de pud de 73.92 2
sv pud sv 77.97 2
fr pud fr 78.25 2
es pud es 80.50 2
fi pud fi 85.42 2
it pud it 86.74 2
tr pud tr 37.65 3
ar pud ar 49.03 3
hi pud hi 54.12 3
cs pud cs cac 82.23 3
cs pud cs˚ 83.38˚ 2˚

ja pud ja 78.22 6
ru pud ru syntagrus 61.82 22
ru pud ru˚ 76.55˚ 1˚

en pud en lines 76.56 23
en pud en˚ 84.09˚ 2˚

Average 71.49 2

Table 3: Evaluation results of our system on PUD
treebanks. We give post-evaluation (non-official)
results˚ where we tested with models trained on
treebanks with canonical language codes. The ta-
ble is sorted by our rankings.

Unlabeled
Parser

Arc
Labeler

LAS F1

Full Full

73 74 75

75.00
74.79
74.69

74.32
74.00

73.75

3ˆAEDP Full
Full Single

1ˆAEDP Full
1ˆAHDP Full
1ˆMST Full

Table 4: Ablation of our ensemble system.

the training sets into train/dev sets with ratio
0.9{0.1. We did not use any additional data.
All neural network computation was implemented
with DyNet (Neubig et al., 2017).

Stage I of our system is the baseline system UD-
Pipe 1.1, and we directly used the outputs pro-
vided by the organizers. We implemented modules
for all later stages. They were trained with gold-
standard features and tokenizations. For all lan-
guages and all treebanks, we trained models with

2-layer-deep and 192-unit-wide (96 units for each
direction) word-level bi-LSTMs as feature extrac-
tors. Lexicalized character bi-LSTMs are 2 lay-
ers deep and 128 units wide, with 64-dimensional
input character embeddings. For languages with-
out lexicalized feature extractors, we used con-
catenation of 64-dimensional UPOS embeddings,
and max pooling of 64-dimensional morphologi-
cal embeddings as input to word-level bi-LSTMs.

The word-level bi-LSTM feature vectors were
passed through MLPs with 1 hidden layer and 192
hidden units, before the bi-affine scoring functions
for MST, AHDP and AEDP unlabeled parsing. In
arc-labelers, we concatenated the word-level fea-
ture vectors and passed it through a 1-layer MLP
with 192 hidden units to get scores for the arc la-
bels. Output layer size depends on the number of
labels appearing in the training set for the con-
cerned treebank. We projected language-specific
arc tags into universal ones before training.

All the aforementioned hidden layers used tanh
as activation functions. And the parameters were
uniformly initialized (Glorot and Bengio, 2010),
except for the weight matrices in the bi-affine scor-
ing functions, which were initialized to be orthog-
onal (Saxe et al., 2013). We did not use any pre-
trained word embeddings.

We applied dropout at every stage. MLPs had
dropout rates of 0.3 (Srivastava et al., 2014). Bi-
LSTMs, both character-level and word-level, also
had dropout rates of 0.3 for input and recurrent
connections (Gal and Ghahramani, 2016). Further,
we zeroed out input vectors to word-level LSTMs
for 15% of the time, to encourage the models gain
more information from context.

When we trained each model, we randomly
shuffled the training set before starting each epoch,
and grouped sentences into mini-batches of ap-
proximately 100 words. The discriminative loss
functions were optimized via Adam optimizer
(Kingma and Ba, 2015), with default hyperparam-
eters except initial learning rate set to be 0.002.
We evaluated the models with development data
after every 500 mini-batches. We halved the learn-
ing rate if the performance plateaued in 5 consecu-
tive evaluations, The process was repeated 3 times
before we terminated the training process.

We employed the technique of stack-
propagation (Zhang and Weiss, 2016), where
the auxiliary task of UPOS prediction was used as
a regularizer. It received 0.1 the weight of other

36



components in computing the loss.
For the languages with multiple treebanks,

we first concatenated the training treebanks and
trained a general model. We then fine-tuned the
models on the respective individual treebanks.

To speed up training,we simultaneously trained
MST, AHDP, AEDP and arc labeling models with
shared LSTM feature extractors. Their losses were
linearly combined with weights 0.6, 0.3, 0.3, 1.5
respectively. After a joint model had been trained,
we fine-tuned each of the four tasks separately.

Our final system included ensembles both for
unlabeled parsing and arc labeling. They were ob-
tained with different random initializations of the
neural network, but trained in the same fashion.
For languages with multiple treebanks, we trained
3 sets of models (3 for each parsing paradigm, 9
unlabeled parsing models in total, plus 3 for arc
labeling). For languages with single treebanks, we
trained 5 sets of models.

For surprise languages, we first trained delexi-
calized models using the training data in a most
similar language according to the WALS features
(Dryer and Haspelmath, 2013). We selected fi,
fa, hi, cs for sme, kmr, bxr, hsb respectively.
We then fine-tuned the models on the sample data
for these languages. We treated kk and ug simi-
larly as they have quite small training sets. Both
of them used tr as the source language.

The entire training process of all models in the
ensemble for all treebanks was done using 8 CPU
cores (2 ˆ Intel i7-4790 @ 3.60GHz) in approx-
imately one week. Each model required at most
2GB RAM plus the amount needed for holding the
training sets. On the online evaluation platform
TIRA (Potthast et al., 2014), the test phase for our
full model finished in 4.64 hours with 2 threads.
Each model required at most 500MB RAM plus
the amount needed for holding the test sets.

Acknowledgments

The first author was supported by a Google fo-
cused research grant. The second author was sup-
ported by Kilian Q. Weinberger with IIS-1550179,
IIS-1525919, IIS-1618134 grants from National
Science Foundation. The fourth author was sup-
ported by DARPA DEFT Grant FA8750-13-2-
0015. We thank Lillian Lee for her helpful input
and support throughout the shared task. And we
thank the two anonymous reviewers for their valu-
able comments.

References
Chris Alberti, Daniel Andor, Ivan Bogatyy, Michael

Collins, Dan Gillick, Lingpeng Kong, Terry Koo,
Ji Ma, Mark Omernick, Slav Petrov, Chayut
Thanapirom, Zora Tung, and David Weiss. 2017.
SyntaxNet models for the CoNLL 2017 shared task.
arXiv:1703.04929. http://arxiv.org/abs/1703.04929.

Miguel Ballesteros, Chris Dyer, and Noah A. Smith.
2015. Improved transition-based parsing by mod-
eling characters instead of words with LSTMs. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, pages 349–359.
https://doi.org/10.18653/v1/D15-1041.

James Cross and Liang Huang. 2016. Incre-
mental parsing with minimal features using bi-
directional LSTM. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers). Asso-
ciation for Computational Linguistics, pages 32–37.
https://doi.org/10.18653/v1/P16-2006.

Timothy Dozat and Christopher D. Manning. 2017.
Deep biaffine attention for neural dependency pars-
ing. In Proceedings of the 5th International Confer-
ence on Learning Representations.

Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig. http://wals.info/.

Jason Eisner. 1996. Three new probabilistic
models for dependency parsing: An explo-
ration. In Proceedings of the 16th Interna-
tional Conference on Computational Linguistics.
http://aclweb.org/anthology/C96-1058.

Jason Eisner and Giorgio Satta. 1999. Effi-
cient parsing for bilexical context-free grammars
and head automaton grammars. In Proceed-
ings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics. pages 457–464.
http://aclweb.org/anthology/P99-1059.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in Neural Information
Processing Systems. pages 1019–1027.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Proceedings of the 13th International
Conference on Artificial Intelligence and Statistics.
volume 9, pages 249–256.

Alex Graves and Jürgen Schmidhuber. 2005.
Framewise phoneme classification with bidi-
rectional LSTM and other neural network ar-
chitectures. Neural Networks 18(56):602–610.
https://doi.org/10.1016/j.neunet.2005.06.042.

Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In

37



Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association
for Computational Linguistics, pages 1077–1086.
http://aclweb.org/anthology/P10-1110.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceed-
ings of the 4th International Conference on Learn-
ing Representations.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional LSTM feature representations. Transactions
of the Association for Computational Linguistics
4:313–327. http://aclweb.org/anthology/Q16-1023.

Marco Kuhlmann, Carlos Gómez-Rodrı̀guez, and Gior-
gio Satta. 2011. Dynamic programming algo-
rithms for transition-based dependency parsers.
In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguis-
tics: Human Language Technologies. Associa-
tion for Computational Linguistics, pages 673–682.
http://aclweb.org/anthology/P11-1068.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng
Ji, Lingpeng Kong, Adhiguna Kuncoro, Gau-
rav Kumar, Chaitanya Malaviya, Paul Michel,
Yusuke Oda, Matthew Richardson, Naomi Saphra,
Swabha Swayamdipta, and Pengcheng Yin. 2017.
DyNet: The dynamic neural network toolkit.
arXiv:1701.03980. http://arxiv.org/abs/1701.03980.

Joakim Nivre, Željko Agić, Lars Ahrenberg, et al.
2017a. Universal dependencies 2.0 CoNLL 2017
shared task development and test data. LIN-
DAT/CLARIN digital library at the Institute of For-
mal and Applied Linguistics, Charles University.
http://hdl.handle.net/11234/1-2184.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajič, Christopher Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal Dependencies v1: A multilingual
treebank collection. In Proceedings of the 10th In-
ternational Conference on Language Resources and
Evaluation. European Language Resources Associ-
ation, Portoro, Slovenia, pages 1659–1666.

Joakim Nivre et al. 2017b. Universal Dependencies
2.0. LINDAT/CLARIN digital library at the Insti-
tute of Formal and Applied Linguistics, Charles Uni-
versity, Prague. http://hdl.handle.net/11234/1-1983.

Barbara Plank, Anders Søgaard, and Yoav Goldberg.
2016. Multilingual part-of-speech tagging with
bidirectional long short-term memory models and
auxiliary loss. In Proceedings of the 54th An-
nual Meeting of the Association for Computational

Linguistics (Volume 2: Short Papers). Associa-
tion for Computational Linguistics, pages 412–418.
https://doi.org/10.18653/v1/P16-2067.

Martin Potthast, Tim Gollub, Francisco Rangel, Paolo
Rosso, Efstathios Stamatatos, and Benno Stein.
2014. Improving the reproducibility of PAN’s
shared tasks: Plagiarism detection, author identifica-
tion, and author profiling. In Evangelos Kanoulas,
Mihai Lupu, Paul Clough, Mark Sanderson, Mark
Hall, Allan Hanbury, and Elaine Toms, editors, In-
formation Access Evaluation meets Multilingual-
ity, Multimodality, and Visualization. 5th Interna-
tional Conference of the CLEF Initiative. Springer,
Berlin Heidelberg New York, pages 268–299.
https://doi.org/10.1007/978-3-319-11382-1 22.

Kenji Sagae and Alon Lavie. 2006. Parser com-
bination by reparsing. In Proceedings of the
Human Language Technology Conference of the
North American Chapter of the ACL, Com-
panion Volume: Short Papers. pages 129–132.
http://aclweb.org/anthology/N06-2033.

Andrew M. Saxe, James L. McClelland, and Surya
Ganguli. 2013. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks.
In Proceedings of the International Conference on
Learning Representations.

Noah A. Smith. 2011. Linguistic Structure Prediction,
volume 4 of Synthesis Lectures on Human Language
Technologies. Morgan & Claypool Publishers.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: A simple way to prevent neural
networks from overfitting. Journal of Machine
Learning Research 15(1):1929–1958.

Milan Straka, Jan Hajič, and Jana Straková. 2016. UD-
Pipe: trainable pipeline for processing CoNLL-U
files performing tokenization, morphological anal-
ysis, POS tagging and parsing. In Proceedings of
the 10th International Conference on Language Re-
sources and Evaluation. European Language Re-
sources Association, Portoro, Slovenia.

Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured predic-
tion models: A large margin approach. In Proceed-
ings of the 22nd International Conference on Ma-
chine Learning. pages 896–903.

Daniel Zeman, Martin Popel, Milan Straka, Jan
Hajič, Joakim Nivre, Filip Ginter, Juhani Luotolahti,
Sampo Pyysalo, Slav Petrov, Martin Potthast, Fran-
cis Tyers, Elena Badmaeva, Memduh Gökırmak,
Anna Nedoluzhko, Silvie Cinková, Jan Hajič jr.,
Jaroslava Hlaváčová, Václava Kettnerová, Zdeňka
Urešová, Jenna Kanerva, Stina Ojala, Anna Mis-
silä, Christopher Manning, Sebastian Schuster, Siva
Reddy, Dima Taji, Nizar Habash, Herman Leung,
Marie-Catherine de Marneffe, Manuela Sanguinetti,
Maria Simi, Hiroshi Kanayama, Valeria de Paiva,

38



Kira Droganova, Hěctor Martı́nez Alonso, Hans
Uszkoreit, Vivien Macketanz, Aljoscha Burchardt,
Kim Harris, Katrin Marheinecke, Georg Rehm,
Tolga Kayadelen, Mohammed Attia, Ali Elkahky,
Zhuoran Yu, Emily Pitler, Saran Lertpradit, Michael
Mandl, Jesse Kirchner, Hector Fernandez Alcalde,
Jana Strnadova, Esha Banerjee, Ruli Manurung, An-
tonio Stella, Atsuko Shimada, Sookyoung Kwak,
Gustavo Mendonça, Tatiana Lando, Rattima Nitis-
aroj, and Josie Li. 2017. CoNLL 2017 Shared Task:
Multilingual Parsing from Raw Text to Universal
Dependencies. In Proceedings of the CoNLL 2017
Shared Task: Multilingual Parsing from Raw Text to
Universal Dependencies. Association for Computa-
tional Linguistics.

Yuan Zhang and David Weiss. 2016. Stack-
propagation: Improved representation learning for
syntax. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Association
for Computational Linguistics, pages 1557–1566.
https://doi.org/10.18653/v1/P16-1147.

39


