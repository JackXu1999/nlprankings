



















































Evaluation of Crowdsourced User Input Data for Spoken Dialog Systems


Proceedings of the SIGDIAL 2015 Conference, pages 427–431,
Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics

Evaluation of Crowdsourced User Input Data for Spoken Dialog Systems

Maria Schmidt, Markus Müller,
Martin Wagner, Sebastian Stüker

and Alex Waibel
Karlsruhe Institute of Technology

Karlsruhe, Germany
maria.schmidt@kit.edu

Hansjörg Hofmann and Steffen Werner
Daimler AG

Sindelfingen, Germany

hansjoerg.hofmann@daimler.com
steffen.s.werner@daimler.com

Abstract

Using the Internet for the collection of
data is quite common these days. This
process is called crowdsourcing and en-
ables the collection of large amounts of
data at reasonable costs. While being
an inexpensive method, this data typically
is of lower quality. Filtering data sets
is therefore required. The occurring er-
rors can be classified into different groups.
There are technical issues and human er-
rors. For speech recording, technical is-
sues could be a noisy background. Human
errors arise when the task is misunder-
stood. We employ several techniques for
recognizing errors and eliminating faulty
data sets in user input data for a Spo-
ken Dialog System (SDS). Furthermore,
we compare three different kinds of ques-
tionnaires (QNRs) for a given set of seven
tasks. We analyze the characteristics of the
resulting data sets and give a recommenda-
tion which type of QNR might be the most
suitable one for a given purpose.

1 Introduction

Similar to research in other areas, Automatic
Speech Recognition (ASR) systems and SDSs are
facing the challenge how to get new training data,
e. g., if there is the urge to cover new domains. Un-
til several years ago, a common procedure was to
record the required audio samples in an anechoic
chamber and let experts (e. g., linguistics students)
create the transcriptions. Although the data col-
lected via this method is of high quality and can
be used as a gold standard, researchers found that
this approach is very time-consuming and results
in quite little data related to the effort.

A few years ago, companies like Amazon Me-
chanical Turk started to offer so-called crowd-

sourcing approaches, which means that Human In-
telligence Tasks (HITs) are performed by a group
of non-experts. Furthermore, these tasks are open
calls and are assigned to the different crowdsource
workers. Especially in industrial contexts, crowd-
sourcing seems to be the means to choose because
development cycles are short and much data for
ASR or SDS development can be generated right
as it is needed, although the data collected needs
to be checked for quality (Snow et al., 2008).

Our work analyzes crowdsourced data collected
by the company Clickworker (Eskenazi et al.,
2013, ch. 9.3.4). The data consists of user input
to an in-car SDS, where the crowdworkers had to
input one German utterance for each of the seven
tasks, after which they had to transcribe the utter-
ance themselves. This procedure was conducted
for three different types of QNRs: pictures, se-
mantics, and text. We show the differences among
these QNRs as well as an overall quality evalua-
tion of the collected data. For this, we make use of
Natural Language Processing (NLP) tools.

2 Related Work

2.1 Collection of Speech Data via
Crowdsourcing

Crowdsourcing is a common part for collecting
speech data nowadays. Eskénazi defines it as “a
crowd as a group of non-experts who have an-
swered an open call to perform a given task” (Es-
kenazi et al., 2013). Such a call will be adver-
tised using special platforms on the Internet. Even
though the participants are called “non-experts”,
they are skilled enough to perform these tasks.
For collecting speech data, recording audio from
a variety of different speakers helps to build better
systems. Different speakers have different back-
grounds. This is reflected in their speaking style
and choice of words (Hofmann et al., 2012). These
aspects are key for training a speaker-independent

427



system. The choice of participants should reflect
the target audience of the system. Using untrained
workers is also cheaper than to hire experts.

2.2 Using ASR to Improve the Quality

Using an ASR system is an integral part of the col-
lection of annotated speech data. Such systems
are being used to optimize the collection meth-
ods. (Williams et al., 2011) have shown how to
process HITs for difficult speech data efficiently.
One approach is to first create a transcription and
let crowdworkers correct it. Since humans are op-
timistic about correcting errors (Audhkhasi et al.,
2012), a two step approach was proposed in (Par-
ent and Eskenazi, 2010): let the workers first rate
the quality/correctness of transcriptions and per-
form the corrections in a separate step.

Another approach (van Dalen et al., 2015) deals
with the combination of automatic and manual
transcriptions. The errors produced are orthogo-
nal: while humans tend to introduce spelling er-
rors or skip words, automatic transcriptions fea-
ture wrong words, additional or even missing
words. The usual approach for combining mul-
tiple transcriptions is ROVER (Fiscus, 1997) re-
quiring an odd (typically three) amount of differ-
ent transcriptions to be merged to break the tie.
By the use of an ASR system, van Dalen et. al
have shown that two manual transcriptions are suf-
ficient to produce high quality.

3 Analysis of Crowdsourced User Input
Data for Spoken Dialog Systems

In this section, we describe our approach to an-
alyze the given corpus containing crowdsourced
user input data for a goal-oriented in-car SDS.

3.1 The Corpus

The underlying German utterances for our analy-
sis were collected by the German company Click-
worker (http://www.clickworker.com/en). The
participants were asked to invoke seven specific
actions of an imaginary SDS deployed in a car.
First, they got a task description, then they should
record an audio of their input via a browser-based
application on their own PC incl. microphone at
home. After that the subjects were asked to tran-
scribe their own utterance without hearing or see-
ing it again. In the following we describe the tasks
1, 4 and 5 exemplarily: In task 1, the imaginary
user tells the system that he/she wants to listen to

a certain radio station. Task 4 comprises the navi-
gation to the address “Stieglitzweg 23, Berlin”. In
task 5, the user should call Barack Obama on his
cell phone. There were three different QNRs, each
one asking for all seven tasks named above. The
QNRs differed in the way how the tasks were pre-
sented to the subject: by means of pictures, text, or
semantics (see Figure 1). In the pictures QNR, the
participants were shown one or more pictures de-
picting the task they should perform. Without any
written text, this type of task description does not
imply the use of specific terms. For type text, the
participants were presented a few lines describ-
ing the situation they are in and the actions they
should perform. This form of textual representa-
tion of the objects is more influencing towards the
use of specific terms. In the semantics QNR, the
participants are influenced the most, as they get
presented a few keywords. This does not favor the
use of different words. Each participant answered
all seven tasks, but was presented only one type of
task description across them. Each type of QNR
was assigned to approximately 1,080 users result-
ing in 22,680 utterances (34.7 hours) in total, i. e.,
roughly 7,560 per QNR. Most subjects were be-
tween 18 and 35 years old, a smaller number of
subjects was up to 55 years old. 90% of the sub-
jects were between 18 and 35 years old, 8% be-
tween 36 and 55. The smallest group was aged
over 55 which resulted in 2% of the data. Our par-
ticipants were 60% men and 40% women.

3.2 Evaluation of Self-Entered Transcripts

To be able to tell the overall quality of the under-
lying corpus, we had to analyze the self-entered
transcripts, too. For this purpose, we developed
an NLP analysis chain which contains a large
part of preprocessing (i. e. mainly cleaning the
text) apart from the actual analysis. Concern-
ing preprocessing, we first applied a basic tok-
enizer to split the punctuation marks from the
rest of the text. Second, we went over the
transcripts with a spell checker called Language-
Tool (https://www.languagetool.org/). For all mis-
spelled words, we checked whether it equals one
of the predefined, special keywords which should
be entered for the current task (e. g., “Michael
Jackson”, “Stieglitzweg”). If such a keyword was
found, we processed the next word; if not, we
checked which of the correct alternatives proposed
by LanguageTool is most similar to one of the

428



Figure 1: Instructions for task 4 in form of pictures, text and semantic entities

words on our “synonymously used words” list by
using the Levenshtein distance. Third, after decid-
ing which spelling is the most appropriate one for
each word, we store the corrected utterances and
use them for further analysis. The latter included
Part-of-Speech (POS) Tagging with the TreeTag-
ger (Schmid, 1994) to investigate, which and how
many different POS patterns, i.e. types of sen-
tence patterns, occur in the corpus and how the
QNRs differ from each other on this level. Fur-
ther, we investigated the most frequent words used
in each task, and how many words in total are
used in a specific task and in a specific QNR. With
our analysis, we provide answers to the following
questions: (a) How large is the linguistic varia-
tion in the data set (on sentence and word level)?
(b) Which pros and cons do the presented QNRs
have? (c) Which QNR is the right one for a certain
purpose? We present the results in Section 4.2.

3.3 Evaluation of Self-Recorded Audio Data

To determine the usability of the recordings, we
compared the length of the recordings and ana-
lyzed them using an ASR system. Generally, we
assume that most recordings are done appropri-
ately and that their quality resembles a normal dis-
tribution. We conducted our analysis using the
Janus Recognition Toolkit (JRTk) (Woszczyna et
al., 1994) which features the IBIS decoder (Soltau
et al., 2001). For each task, a certain answer length
is expected. This length may vary, but a signif-
icantly shorter or longer audio file indicates an
error. Whether due to a technically false record-
ing setup or a misunderstanding of the task de-
scription, in both cases the recording needs to be
discarded. Even if the length is within a suit-
able range, the transcription of the audio might be
wrong. To see if the transcription matches the spo-
ken words, we use JRTk to perform a forced align-
ment. We use a GMM/HMM-based recognizer for
German with 6,000 context-dependent quintphone

states for aligning a phoneme sequence to the au-
dio using forced Viterbi alignment. If there is a
mismatch between audio and transcriptions, there
will be phonemes covering unusual long or short
parts of the audio.

4 Results & Discussion

4.1 Results of the Audio Data Analysis
We divided the recordings into 21 different sets as
there are 3 different QNRs and 7 tasks each. Ta-
ble 1 shows a detailed overview of the recording
lengths for different tasks. While task 4 produces
the longest recordings, the semantics QNR pro-
duces the shortest recordings.

Task Pictures Semantics Text
1 5.21s 5.04s 5.04s
2 5.75s 5.65s 6.01s
3 4.97s 4.56s 5.01s
4 6.80s 6.44s 6.79s
5 5.45s 5.26s 5.39s
6 5.46s 5.51s 5.78s
7 5.37s 4.73s 5.21s

Table 1: Average length of recordings.

We also performed a forced Viterbi alignment:
Figure 2 shows a histogram of the length of the
longest phoneme per utterance used to indicate
whether recording and transcription fit together.
Since we do not have multiple transcriptions per
utterance, we could not determine an optimal pa-
rameter set for identifying mismatched cases. But
our preliminary results indicate that the longer the
longest phoneme, the more likely a mismatch.

4.2 Results of the Transcript Analysis
Aiming at answering the questions posed in Sec-
tion 3.2, we show the results of the transcript anal-
ysis in the following together with a short discus-
sion. Tables 2 and 3 show the total number of ut-
terances in the respective QNR data sets. The sec-

429



 0
 20
 40
 60
 80

 100
 120
 140
 160

 0  0.5  1  1.5  2  2.5  3  3.5  4  4.5  5  5.5

co
un

t

Length in seconds of the longest phoneme

Figure 2: Longest phoneme length per utterance.

ond line of each table displays how many oblig-
atory semantic entities were named, i.e. whether
the two main content words (nouns in many tasks)
were named, like ”Sender, SWR3”. The third line
displays the number of insufficient utterances ac-
cording to this criterion. Similarly, line four and
five tell how many entities, which were actually
asked for, i.e. all three (or four) items, were named
and how many utterances were dismissed accord-
ingly. As shown, the pictures QNR has to dismiss
the most entities, while the semantics QNR dis-
misses the least. The values of the text QNR are
in between the latter two QNRs. In total over all
QNRs, we have dismissal rates of 17% and 37%.

Table 4 displays the variance of words used for
all three QNRs and across tasks 1-7. It is valid
for all tasks that the semantics QNR has the low-
est number of different words. This is probably
caused by displaying three exact semantic items,
inevitably being the corresponding words. For
tasks 1-3 and 7, the text QNR has the highest num-
ber of different words, while the pictures QNR
leads the number of different words in tasks 4-6.

The analysis of the most frequent POS se-
quences per QNR showed that in the semantics
QNR, most people used a polite modal construc-
tion ”Ich möchte den Sender SWR hören” (PPER
VMFIN ART NN NN VVINF). In the other QNRs
”Radio SWR3” (NN NN) is the most common one
among finite and infinite constructions.

Table 5 displays the most common sentence for
each task. As you can see, there is a wide variety
of linguistic patterns in each task.

5 Conclusion

We have presented various methods for evaluat-
ing the collected data set and that different types
of QNRs lead to different styles in performing the
tasks. With respect to the actual application sce-

total number of utterances 7, 546
number of obligatory entities 5, 420 72%
number of insufficient utterances 2, 126 28%
number of asked for entities 3, 033 40%
number of insufficient utterances 4, 513 60%

Table 2: Picture QNR with its dismissal rate.

total number of utterances 7, 581
number of obligatory entities 6, 947 92%
number of insufficient utterances 634 8%
number of asked for entities 6, 126 81%
number of insufficient utterances 1, 455 19%

Table 3: Semantics QNR with its dismissal rate.

# words used pictures semantics text avg.
Task 1 199 176 237 204
Task 2 216 206 256 226
Task 3 279 225 326 277
Task 4 327 260 309 299
Task 5 266 179 253 233
Task 6 297 188 264 250
Task 7 340 229 377 315
Average 275 209 289 258

Table 4: Variance of used words across all QNRs.

Task Most frequent sentences
1 Ich möchte den Sender SWR3 hören
4 Navigiere [mich] zu[m] Stieglitzweg 23 in Berlin
5 Barack Obama [auf [dem]] Handy anrufen

Table 5: Most common sentences for tasks 1, 4, 5.

nario, the way in presenting the task to the partici-
pants has to be chosen in the correct manner.

The semantics QNR is precise by using three
semantic items and is the best choice for generat-
ing exact phrases; it generates very few utterance
dismissals. But at the same time it displays the
words themselves. To avoid the mere usage of
these, one approach for future studies would be
to display the semantic items in English. Simulta-
neously, the QNR would be easily reusable for the
generation of data from other languages.

The pictures QNR is optimal to generate a very
high linguistic variance in the data. The downside
of this approach is the high dismissal rate, if one
aims at generating specific utterances.

The text QNR is a good compromise between
the latter two QNRs. Looking at the data analyzed
in this work, the text QNR has a lower priming
effect on formulations than the semantics QNR.

430



References
Kartik Audhkhasi, Panayiotis G Georgiou, and

Shrikanth S Narayanan. 2012. Analyzing quality of
crowd-sourced speech transcriptions of noisy audio
for acoustic model adaptation. In Acoustics, Speech
and Signal Processing (ICASSP), 2012 IEEE Inter-
national Conference on, pages 4137–4140. IEEE.

Maxine Eskenazi, Gina-Anne Levow, Helen Meng,
Gabriel Parent, and David Suendermann. 2013.
Crowdsourcing for Speech Processing: Applications
to Data Collection, Transcription and Assessment.
John Wiley & Sons.

Jonathan G Fiscus. 1997. A post-processing system
to yield reduced word error rates: Recognizer out-
put voting error reduction (rover). In Automatic
Speech Recognition and Understanding, 1997. Pro-
ceedings., 1997 IEEE Workshop on, pages 347–354.
IEEE.

Hansjörg Hofmann, Ute Ehrlich, André Berton, and
Wolfgang Minker. 2012. Speech interaction with
the internet–a user study. In Intelligent Environ-
ments (IE), 2012 8th International Conference on,
pages 323–326. IEEE.

Gabriel Parent and Maxine Eskenazi. 2010. Toward
better crowdsourced transcription: Transcription of
a year of the let’s go bus information system data.
In Spoken Language Technology Workshop (SLT),
2010 IEEE, pages 312–317. IEEE.

Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
international conference on new methods in lan-
guage processing, volume 12, pages 44–49. Cite-
seer.

Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast—but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the conference
on empirical methods in natural language process-
ing, pages 254–263. Association for Computational
Linguistics.

Hagen Soltau, Florian Metze, Christian Fugen, and
Alex Waibel. 2001. A one-pass decoder based
on polymorphic linguistic context assignment. In
Automatic Speech Recognition and Understanding,
2001. ASRU’01. IEEE Workshop on, pages 214–217.
IEEE.

R van Dalen, K Knill, P Tsiakoulis, and M Gales.
2015. Improving multiple-crowd-sourced transcrip-
tions using a speech recogniser. In Proceedings of
the International Conference on Acoustics, Speech
and Signal Processing (ICASSP). Institute of Elec-
trical and Electronics Engineers.

Jason D Williams, I Dan Melamed, Tirso Alonso, Bar-
bara Hollister, and Jay Wilpon. 2011. Crowd-
sourcing for difficult transcription of speech. In
Automatic Speech Recognition and Understanding

(ASRU), 2011 IEEE Workshop on, pages 535–540.
IEEE.

Monika Woszczyna, N. Aoki-Waibel, Finn Dag Buø,
Noah Coccaro, Keiko Horiguchi, Thomas Kemp,
Alon Lavie, Arthur McNair, Thomas Polzin, Ivica
Rogina, Carolyn Rose, Tanja Schultz, Bernhard
Suhm, M. Tomita, and Alex Waibel. 1994. Janus
93: Towards spontaneous speech translation. In In-
ternational Conference on Acoustics, Speech, and
Signal Processing 1994, Adelaide, Australia.

431


