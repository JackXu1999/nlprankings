




































Improving Neural Abstractive Document Summarization with Structural Regularization


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4078–4087
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4078

Improving Neural Abstractive Document Summarization
with Structural Regularization∗

Wei Li1,2,3 Xinyan Xiao2 Yajuan Lyu2 Yuanzhuo Wang1
1Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China

2Baidu Inc., Beijing, China
3University of Chinese Academy of Sciences, Beijing, China

weili.ucas.ict@gmail.com, {xiaoxinyan,lvyajan}@baidu.com,
wangyuanzhuo@ict.ac.cn

Abstract

Recent neural sequence-to-sequence models
have shown significant progress on short text
summarization. However, for document sum-
marization, they fail to capture the long-
term structure of both documents and multi-
sentence summaries, resulting in information
loss and repetitions. In this paper, we pro-
pose to leverage the structural information
of both documents and multi-sentence sum-
maries to improve the document summariza-
tion performance. Specifically, we import
both structural-compression and structural-
coverage regularization into the summariza-
tion process in order to capture the infor-
mation compression and information cover-
age properties, which are the two most im-
portant structural properties of document sum-
marization. Experimental results demonstrate
that the structural regularization improves the
document summarization performance signif-
icantly, which enables our model to generate
more informative and concise summaries, and
thus significantly outperforms state-of-the-art
neural abstractive methods.

1 Introduction

Document summarization is the task of generat-
ing a fluent and condensed summary for a docu-
ment while retaining the gist information. Recent
success of neural sequence-to-sequence (seq2seq)
architecture on text generation tasks like ma-
chine translation (Bahdanau et al., 2014) and im-
age caption (Vinyals et al., 2015), has attracted
growing attention to abstractive summarization
research. Huge success has been witnessed in
abstractive sentence summarization (Rush et al.,
2015; Takase et al., 2016; Chopra et al., 2016; Cao
et al., 2017; Zhou et al., 2017), which builds one-
sentence summaries from one or two-sentence in-

∗This work was done while the first author was doing in-
ternship at Baidu Inc.

(a) Gold Reference

I1 I2 I3 I4 I5 I6 I7 I8 I9 I10 I11 I12 I13 I14 I15 I16 I17 I18 I19 I20
O1
O2
O3
O4
O5

(b) Seq2Seq-baseline

O1
O2
O3

(d) Hierarchical-baseline

O1
O2
O3
O4

(c) Point-gen-cov

O1
O2

(e) Our Method

O1
O2
O3

I1 I2 I3 I4 I5 I6 I7 I8 I9 I10 I11 I12 I13 I14 I15 I16 I17 I18 I19 I20

I1 I2 I3 I4 I5 I6 I7 I8 I9 I10 I11 I12 I13 I14 I15 I16 I17 I18 I19 I20

I1 I2 I3 I4 I5 I6 I7 I8 I9 I10 I11 I12 I13 I14 I15 I16 I17 I18 I19 I20

I1 I2 I3 I4 I5 I6 I7 I8 I9 I10 I11 I12 I13 I14 I15 I16 I17 I18 I19 I20

Figure 1: Comparison of sentence-level attention distribu-
tions for the summaries in Table 1 on a news article. (a) is
the heatmap for the gold reference summary, (b) is for the
Seq2seq-baseline system, (c) is for the Point-gen-cov (See
et al., 2017) system, (d) is for the Hierarchical-baseline sys-
tem and (e) is for our system. Ii and Oi indicate the i-th
sentence of the input and output, respectively. Obviously, the
seq2seq models, including the Seq2seq-baseline model and
the Point-gen-cov model, lose much salient information of
the input document and focus on the same set of sentences
repeatedly. The Hierarchical-baseline model fails to detect
several specific sentences that are salient and relevant for each
summary sentence and focuses on the same set of sentences
repeatedly. On the contrary, our method with structural regu-
larizations focuses on different sets of source sentences when
generating different summary sentences and discovers more
salient information from the document.

put. However, the extension of sentence abstrac-
tive methods to document summarization task is
not straightforward.

As long-distance dependencies are difficult to
be captured in the recurrent framework (Bengio
et al., 1994), the seq2seq models are not yet able
to achieve convincing performance in encoding
and decoding for a long sequence of multiple sen-
tences (Chen et al., 2017; Koehn and Knowles,



4079

Original Text (truncated): the family of conjoined twin sisters who died 19 days after they were born have been left mortified(2) after they arrived at their gravesite to find cemetery
staff had cleared the baby section of all mementos and tossed them in the rubbish(3) . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year
with one body and two faces due to an extremely rare condition known as disrosopus (1) . they died in hospital less than a month after they were born and their parents , simon howie
and renee young , laid them to rest at pinegrove memorial park in sydney ’s west(2) . scroll down for video . faith and hope howie were dubbed the miracle twins when they were born
on may 8 last year with one body and two faces due to an extremely rare condition known as disrosopus(1) . family members have visited the grave every week to leave mementos
and flowers for faith and hope , but when mr howie and ms young arrived on thursday they found the site completely bare(3) . ’ we took renee’s aunts to see the girls for the first time
and we found everything had been stripped away , ’ mr howie told daily mail australia . ’ we were devastated and mortified . we ’ve had a little shrine set up and we ’ve been adding
to it since the funeral . ’ it ’s heartbreaking to know we ’ve set this up and it has been treated like rubbish . ’ faith and hope were buried in a pink coffin and their family and friends
released doves and pink and white balloons at their funeral . their family and friends had built up a small memorial with pink and white statues , flowers , pebbles and toys over the past
11 months . when they arrived on thursday , everything had been removed apart from a bunch of flowers . the twins were buried at pinegrove memorial park in western sydney after they
died after just 19 days(2) . their family and friends had built a small shrine at their gravesite , which they have added to since the funeral . family members have visited the grave every
week to leave mementos and flowers for faith and hope , but when parents simon howie and renee young arrived on thursday they found the site completely bare(3) .
Gold Reference: faith and hope howie were born with one body and two faces on may 8. they tragically died in hospital just 19 days after they were born . parents simon howie and
renee young visit their grave at pinegrove in western sydney fortnightly . they arrived on thursday to find the grave bare of all the girls ’ mementos . staff had cleared entire baby section
and thrown belongings in rubbish .
Seq2seq-baseline: faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known
as disrosopus . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as
disrosopus . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition known as
disrosopus .
Point-cov (See et al., 2017): faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare
condition known as disrosopus . they died in hospital less than a month after they were born and their parents , simon howie and renee young , laid them to rest at pinegrove memorial
park in sydney ’s west.
Hierarchical-baseline: faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition
. they died in hospital less than a month after they died in hospital less than a month after they were born and laid them to rest at pinegrove memorial park in sydney ’s west . family
members have visited the grave every week to leave mementos and flowers for faith and hope , but when they were born on thursday they found the site completely bare . family
members have visited the grave every week to leave mementos and flowers for faith and hope , but when they found the site completely bare .
Our Method: faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one body and two faces due to an extremely rare condition(1) . they
died in hospital less than a month after they were born and their parents laid them to rest at pinegrove memorial park in sydney ’s west(2) . family members have visited the grave every
week to leave mementos and flowers for faith and hope , but when mr howie and ms young arrived on thursday they found the site completely bare(3) .

Table 1: Comparison of the generated summaries of four abstractive summarization models and the gold reference summary
on a news article. The summaries generated by the seq2seq models, both the Seq2seq-baseline model and the Point-cov model,
lose some salient information. The Seq2seq-baseline model even contains serious information repetitions. The Hierarchical-
baseline model not only contains serious repetitions, but also makes non-grammatical or non-coherent sentences. On the
contrary, the summary generated by our model contains more salient information and is more concise. Our model also shows
the ability to generate a summary sentence by compressing several source sentences, such as shortening a long sentence.

2017). In document summarization, it is also diffi-
cult for the seq2seq models to discover important
information from too much input content of a doc-
ument (Tan et al., 2017a,b). The summary gener-
ated by the seq2seq models usually loses salient
information of the original document or even con-
tains repetitions (see Table 1).

In fact, both document and summary naturally
have document-sentence hierarchical structure, in-
stead of being a flat sequence of words. It is
widely aware that the hierarchical structure is nec-
essary and useful for neural document modeling.
Hierarchical neural models have already been suc-
cessfully used in document-level language mod-
eling (Lin et al., 2015) and document classifi-
cation (Yang et al., 2016). However, few work
makes use of the hierarchical structure of docu-
ment and multi-sentence summary in document
summarization. The basic hierarchical encoder-
decoder model (Li et al., 2015) is also not yet able
to capture the structural properties of both docu-
ment and summary (see Figure 11), resulting in

1To simulate the sentence-level attention mechanism on
the gold reference summary, we compute the words-matching
similarities (based on TF-IDF cosine similarity) between a
reference-summary sentence and the corresponding source
document sentences and normalize them into attention dis-
tributions. The sentence-level attention distributions of the
Seq2seq-baseline model and the Point-gen-cov model are
computed by summing the attention weights of all words in
each sentence and then normalized across sentences.

more serious repetitions and even nonsensical sen-
tences (see Table 1).

In document summarization, information com-
pression and information coverage are the two
most important structural properties. Based on
the hierarchical structure of document and sum-
mary, they can be realized at the sentence-
level as: (1) Structural-compression: each sum-
mary sentence is generated by compressing sev-
eral specific source sentences; (2) Structural-
coverage: different summary sentences usually
focus on different sets of source sentences to
cover more salient information of the original doc-
ument. Figure 1(a) intuitively shows the two
properties in human-written gold reference sum-
maries. We import both structural-compression
and structural-coverage regularizations into the
document summarization process based on a hi-
erarchical encoder-decoder with hybrid sentence-
word attention model. Typically, we design
an effective learning and inference algorithm to
explicitly model the structural-compression and
structural-coverage properties of document sum-
marization process, so as to generate more infor-
mative and concise summaries (see Table 1).

We conduct our experiments on benchmark
datasets and the results demonstrate that prop-
erly modeling the structural-compression and
structural-coverage properties based on the hier-



4080

sentence-level encoder

w
ord-level encoder

hierarchical decoder

Sentence-level 
Attention

Word-level 
Attention

ct
s

GRU

ct−1
s xt−1

'

dt
s

ct ,k
w

GRU

ct ,k−1
w et ,k−1

dt ,k
w

γ t ,k

Structural 
Compression

Structural 
Coverage

α t

h1 h2 h3

h1,1

h1,2

h1,3

h2,1

h2,2

h2,3

h3,1

h3,2

h3,3

ht−1
'

ht ,k−1
'

α1...t

Figure 2: Our hierarchical encoder-decoder model with
structural regularization for abstractive document summa-
rization.

archical structure of document and summary, im-
proves document summarization performance sig-
nificantly. Our model is able to generate more
informative and concise summaries by enhancing
sentences compression and coverage, and signifi-
cantly outperforms state-of-the-art seq2seq-based
abstractive methods, especially on summarizing
long documents with long summaries.

2 Hierarchical Encoder-Decoder Model

In this section, we introduce our baseline hierar-
chical encoder-decoder model which consists of
two parts: a hierarchical encoder and a hierarchi-
cal decoder, as shown in Figure 2. Similar to (Li
et al., 2015), both the encoder and decoder consists
of two levels: a sentence level and a word level.
The main distinction is that we design a hybrid
sentence-word attention mechanism on the hierar-
chical decoder to help organize summary content
and realize summary sentences.

2.1 Hierarchical Encoder

The goal of the encoder is to map the input doc-
ument to a hidden vector representation. We con-
sider a source document X as a sequence of sen-
tences: X = {si}, and each sentence si as a
sequence of words: si = {wij}. The word-
level encoder encodes the words of a sentence
into a sentence representation, and the sentence-
level encoder encodes the sentences of a docu-
ment into the document representation. In this
work, both the word-level encoder and sentence-
level encoder use the bidirectional Gated Recur-
rent Unit (BiGRU) (Chung et al., 2014). The
word-level encoder sequentially updates its hid-
den state upon each received word, as hi,j =
BiGRU(hi,j−1, ei,j) where hi,j and ei,j denote
the hidden state and the embedding of word wi,j ,
respectively. The concatenation of the forward and

backward final hidden states in the word-level en-
coder is indicated as the vector representation ri of
sentence si, which is used as input to the sentence-
level encoder. The sentence-level encoder updates
its hidden state after receiving each sentence rep-
resentation, as hi = BiGRU(hi−1, ri) where hi
denotes the hidden state of sentence si. The con-
catenation of the forward and backward final states
in the sentence-level encoder is used as the vector
representation of document d.

In the hierarchical encoder architecture, long
dependency problem will be largely reduced at
both the sentence level and the word level, so it
can better capture the structural information of the
input document.

2.2 Hierarchical Decoder with Hybrid
Sentence-Word Attention

The goal of the decoder is to generate output sum-
mary according to the representation of the in-
put document. Let Y = {s′i} indicates a candi-
date summary of document X , and each sentence
s′i consists of a sequence of words s

′
i = {w′ij}.

The hierarchical decoder organizes summary Y
sentence by sentence, and realizes each sentence
word by word. In this work, both the sentence-
level decoder and word-level decoder use a single
layer of unidirectional GRU. The sentence-level
decoder receives the document representation d
as initial state h′0 and predicts sentence represen-
tations sequentially by h′t = GRU(h

′
t−1, r

′
t−1),

where h′t denotes the hidden state of the tth sum-
mary sentence s′t and r

′
t−1 denotes the encoded

representation of the previously generated sen-
tence s′t−1. The word-level decoder receives a sen-
tence representation h′t as initial state h

′
t,0 and pre-

dicts word representations sequentially by h′t,k =
GRU(h′t,k−1, et,k−1) where h

′
t,k denotes the hid-

den state of word w′t,k in sentence s
′
t and et,k−1 de-

notes the embedding of previously generated word
w′t,k−1 in sentence s

′
t.

In this work, we design a hybrid sentence-
word attention mechanism based on the hierarchi-
cal encoder-decoder architecture, which contains
both sentence-level attention and word-level at-
tention, to better exploit both the sentence-level
information and word-level information from the
input document and the output summary.

2.2.1 Sentence-level Attention
The sentence-level attention mechanism is de-
signed on the sentence-level encoder and decoder,



4081

which is used to help our model to detect impor-
tant and relevant source sentences in each sen-
tence generation step. αit indicates how much
the t-th summary sentence attends to the i-th
source sentence, which is computed by αit =
ef(hi,h

′
t)/

∑
l e

f(hl,h
′
t) where f is the function

modeling the relation between hi and h′t. We use
the function f(a,b) = vT tanh(Waa + Wbb),
where v, Wa, Wb are all learnable parameters.
Then the sentence level context vector cst when
generating the tth sentence s′t can be computed
as: cst =

∑
i α

i
thi, which is incorporated into the

sentence-level decoding process.

2.2.2 Word-level Attention with
Sentence-level Normalization

The word-level attention is designed on the word-
level encoder and decoder, which is used to help
our model to realize the summary sentence by lo-
cating relevant words in the selected source sen-
tences in each word generation step. Let βi,jt,k
denotes how much the j-th word in source sen-
tence si contributes to generating the k-th word
in summary sentence s′t, which is computed by
βi,jt,k = e

f(hi,j ,h
′
t,k)/

∑
l e

f(hi,l,h
′
t,k).

Since the word-level attention above is within
each source sentence, we normalize it by sentence-
level attentions to get word attention over all
source words, as γit,k = β

i
t,kα

i
t. Then the word-

level context vector when generating word w′t,k
can be computed as: cwt,k =

∑
i

∑
j γ

i,j
t,khi,j ,

which is also incorporated into the word-level de-
coding process.

At each word generation step, the vocabulary
distribution is calculated from the context vector
cwt,k and the decoder state h

′
t,k by:

Pvocab(w
′
t,k) = softmax(Wv(Wc[h

′
t,k, c

w
t,k] + bc) + bv)

(1)

where Wv, Wc, bc and bv are learned parame-
ters. We also incorporate the copy mechanism
(See et al., 2017) based on the normalized word-
level attention to help generate out-of-vocabulary
(OOV) words during the sentence realization pro-
cess.

3 Structural Regularization

Although the above hierarchical encoder-decoder
model is designed based on the document-
sentence hierarchical structure, it can’t capture
the basic structural properties of document sum-
marization (see Figure 1(d) and Table 1). How-

ever, the hierarchical architecture makes it possi-
ble for importing structural regularization to cap-
ture the sentence-level characteristics of docu-
ment summarization process. In this work, we
propose to model the structural-compression and
structural-coverage properties based on the hier-
archical encoder-decoder model by adding struc-
tural regularization during both the model learning
phase and inference phase.

3.1 Structural Compression
Compression is a basic property of document sum-
marization, which has been widely explored in tra-
ditional document summarization research, such
as sentence compression-based methods which
shorten sentences by removing non-salient parts
(Li et al., 2013; Durrett et al., 2016) and sentence
fusion-based methods which merge information
from several different source sentences (Barzilay
and McKeown, 2005; Cheung and Penn, 2014).
As shown in Figure 1, each summary sentence in
the human-written reference summary is also cre-
ated by compressing several specific source sen-
tences.

In this paper, we propose to model the
structural-compression property of document
summarization based on sentence-level attention
distributions by:

strCom(αt) = 1−
1

logN

N∑
i=1

αitlogα
i
t (2)

where αt denotes the sentence-level attention dis-
tribution when generating the tth summary sen-
tence and N denotes the length of distribution
αt. The right part in the above formula is actu-
ally the entropy of the distribution αt. As the at-
tention distribution becomes sparser, the entropy
of the distribution becomes lower, so the value
of strCom(αt) defined above will become larger.
Sparse sentence-level attentions help the model
compress and generalize several specific source
sentences which are salient and relevant in the
sentence generation process. Note that, 0 ≤
strCom(αt) ≤ 1.

3.2 Structural Coverage
A good summary should have the ability to cover
most of the important information of an input
document. As shown in Figure 1, the human-
written reference summary covers the information
of many source sentences. Coverage has been



4082

used as a measure in many traditional document
summarization research, such as the submodular-
based methods which optimize the information
coverage of the summary with similarity-based
coverage metrics (Lin and Bilmes, 2011; Chali
et al., 2017).

In this work, we simply model the structural-
coverage property of summary based on the hi-
erarchical architecture by encouraging different
summary sentences to focus on different sets of
source sentences so that the summary can cover
more salient sentences of the input document.
We measure the structural-coverage of summary
based on the sentence-level attention distributions:

strCov(αt) = 1−
∑
i

min(αit,

t−1∑
t′=0

αit′) (3)

which is used to encourage different summary sen-
tences to focus on different sets of source sen-
tences during the summary generation process. As
the sentence-level attention distributions of dif-
ferent summary sentences become more diversi-
fied, the summary will cover more source sen-
tences, which is effective to improve the informa-
tiveness and conciseness of summaries. Note that,
0≤strCov(αt) ≤ 1.

3.3 Model Learning
Experimental results reveal that the properties of
structural-compression and structural-coverage
are hard to be captured by both the seq2seq mod-
els and the hierarchical encoder-decoder baseline
model, which largely restricts their performance
(Section 4). In this work, we model them ex-
plicitly by regulating the sentence-level attention
distributions based on the hierarchical encoder-
decoder framework. The loss function L of
the model is the mix of negative log-likelihood
of generating summaries over training set T ,
the structural-compression loss and the structural-
coverage loss:

L =
∑

(X,Y )∈T

{−logP (Y |X; θ) + λ1
∑
t

strCom(αt)︸ ︷︷ ︸
structural−compression loss

+ λ2
∑
t

strCov(αt)︸ ︷︷ ︸
structural−coverage loss

}

(4)

where λ1 and λ2 are hyper-parameters tuned on
the validation set. We use Adagrad (Duchi et al.,

2011) with learning rate 0.1 and an initial accumu-
lator value 0.1 to optimize the model parameters θ.

3.4 Hierarchical Decoding Algorithm
The traditional beam search algorithm that widely
used for text generation can only help generate
fluent sentence, and is not easy to extend to the
sentence level. The reason is that the K-best sen-
tences generated by a word decoder will mostly
be similar to each other (Li et al., 2016; Tan
et al., 2017a). We propose a hierarchical beam
search algorithm with structural-compression and
structural-coverage regularization.

The hierarchical decoding algorithm has two
levels: K-best word-level beam search and N -best
sentence-level beam search. At the word-level,
the vanilla beam search algorithm is used to max-
imize the accumulated score P̂ (s′t) of generating
current summary sentence s′t. At the sentence-
level, N -best beam search is realized by maxi-
mizing the accumulated score scoret of all the
sentences generated, including the sentences gen-
eration score, structural-compression score and
structural-coverage score, which are defined as:

scoret =

t∑
t′=0

{P̂ (s′t′)+ζ1strCom(αt′)+ζ2strCov(αt′)}

(5)

where ζ1 and ζ2 are factors introduced to con-
trol the influence of structural regularization dur-
ing the decoding process.

4 Experiments

4.1 Experimental Settings
We conduct our experiments on the CNN/Daily
Mail dataset (Hermann et al., 2015), which has
been widely used for exploration on summarizing
documents with multi-sentence summaries (Nal-
lapati et al., 2016; See et al., 2017; Tan et al.,
2017a; Paulus et al., 2017). The CNN/DailyMail
dataset contains input sequences of about 800 to-
kens in average and multi-sentence summaries of
up to 200 tokens. The average number of sen-
tences in documents and summaries are 42.1 and
3.8, respectively. We use the same version of
non-anonymized data (the original text without
pre-processing) as See et al. (2017), which has
287,226 training pairs, 13,368 validation pairs and
11,490 test pairs.

For all experiments, the word-level encoder and
decoder both use 256-dimensional hidden states,
and the sentence-level encoder and decoder both



4083

Method Rouge-1 Rouge-2 Rouge-L
SummaRuNNer-abs 37.5 14.5 33.4
SummaRuNNer 39.6 16.2 35.3
Seq2seq-baseline 36.64 15.66 33.42
ABS-temp-attn 35.46 13.30 32.65
Graph-attention 38.1 13.9 34.0
Point-cov 39.53 17.28 36.38
Hierachical-baseline 34.95 14.79 32.68
Our Model 40.30 18.02 37.36

Table 2: Rouge F1 scores on the test set. All our ROUGE
scores have a 95% confidence interval of at most ±0.25
as reported by the official ROUGE script.

use 512-dimensional hidden states. The dimen-
sion of word embeddings is 128, which is learned
from scratch during training. We use a vocabulary
of 50k words for both the encoder and decoder.

We trained our model on a single Tesla K40m
GPU with a batch size of 16 and an epoch is set
containing 10,000 randomly sampled documents.
Convergence is reached within 300 epochs. After
tuning on the validation set, parameters λ1, λ2, ζ1
and ζ2, are set as -0.5, -1.0, 1.2 and 1.4, respec-
tively. At the test time, we use the hierarchical
decoding algorithm with sentence-level beam size
4 and word-level beam size 8.

4.2 Evaluation

ROUGE Evaluation. We evaluate our models
with the widely used ROUGE (Lin, 2004) toolkit.
We compare our system’s results with the re-
sults of state-of-the-art neural summarization ap-
proaches reported in recent papers, including both
abstractive models and extractive models. The ex-
tractive models include SummaRuNNer (Nallap-
ati et al., 2017) and SummaRuNNer-abs which is
similar to SummaRuNNer but is trained directly
on the abstractive summaries. The abstractive
models include:

1) Seq2seq-baseline, which uses the basic
seq2seq encoder-decoder architecture with
attention mechanism, and incorporates with
copy mechanism (See et al., 2017) to allevi-
ate the OOV problem.

2) ABS-temp-attn (Nallapati et al., 2016),
which uses Temporal Attention on the
seq2seq architecture to overcome the repeti-
tion problem.

3) Point-cov (See et al., 2017), which is an ex-
tension of the Seq2seq-baseline model by im-
porting word-coverage mechanism to reduce
repetitions in summary.

4) Graph-attention (Tan et al., 2017a), which

length Method Rouge-1 Rouge-2 R.-L
< 100 Our M. 39.66 17.28 36.69
(94.47%) Point-cov 39.44 17.20 36.30
[100, 125) Our M. 43.07 19.96 39.47
(4.00%) Point-cov 41.78 19.00 38.41
[125, 150) Our M. 43.25 19.21 40.08
(1.07%) Point-cov 41.31 18.02 37.75
> 150 Our M. 40.64 18.30 38.00
(0.46%) Point-cov 35.64 17.76 33.12

Table 3: Comparison results w.r.t different length of refer-
ence summary. < 100 indicates the reference summary has
less than 100 words (occupy 94.47% of test set).

uses a graph-ranking based attention mecha-
nism based on a hierarchical architecture to
identify important sentences.

5) Hierachical-baseline, which just uses the
basic hierarchical encoder-decoder with hy-
brid attention model proposed in this paper.

Results in Table 2 show that our model sig-
nificantly outperforms all the neural abstractive
baselines and extractive baselines. An inter-
esting observation is that the performance of
the Hierarchical-baseline model are lower than
the Seq2seq-baseline model, which demonstrates
the difficulty for a traditional model to iden-
tify the structural properties of document sum-
marization process. Our model outperforms the
Hierarchical-baseline model by more than 4
ROUGE points, which demonstrates that the struc-
tural regularization improves the document sum-
marization performance significantly.

To verify the superiority of our model on gen-
erating long summaries, we also compare our
method with the best seq2seq model Point-cov
(See et al., 2017) by evaluating them on a test set
w.r.t. different length of reference summaries. The
results are shown in Table 3, which demonstrate
that our model is better at generating long sum-
mary than the seq2seq model. As the summary
becomes longer, our system will obtain larger ad-
vantages over the baseline (from +0.22 Rouge-1,
+0.08 Rouge-2 and +0.39 Rouge-L for summary
less than 100 words, rising to +5.00 Rouge-1,
+0.54 Rouge-2 and +4.88 Rouge-L for summaries
more than 150 words).

Human Evaluation. In addition to the ROUGE
evaluation, we also conducted human evaluation
on 50 random samples from CNN/DailyMail test
set and compared the summaries generated by our
method with the outputs of Seq2seq-baseline and
Point-cov (See et al., 2017). Three data annotators
were asked to compare the generated summaries



4084

Method Informat. Concise Coherent Fluent
Seq2seq-b. 2.79∗ 2.52∗ 2.68∗ 3.57
Point-cov 3.17∗ 2.92∗ 3.00∗ 3.54
Our Model 3.67 3.39 3.51 3.70

Table 4: Human evaluation results. ∗ indicates the difference
between Our Model and other models are statistic significant
(p < 0.05) by two-tailed t-test.

Method R-1 R-2 R-L strCom strCov
Hierarchical-b. 34.95 14.79 32.68 0.22 0.31
+strCom 37.03 16.21 34.44 0.64 0.71
+strCov 39.52 17.12 36.44 0.65 0.87
+hierD 40.30 18.02 37.36 0.68 0.93

Table 5: Results of adding different components of our
method in terms of ROUGE-1, ROUGE-2, ROUGE-L, str-
Com (Equation 1) and strCov (Equation 2) scores.

with the human summaries, and assess each sum-
mary from four independent perspectives: (1) In-
formative: How informative the summary is? (2)
Concise: How concise the summary is? (3) Co-
herent: How coherent (between sentences) the
summary is? (4) Fluent: How fluent, grammatical
the sentences of a summary are? Each property
is assessed with a score from 1(worst) to 5(best).
The average results are presented in Table 4.

The results show that our model consistently
outperforms the Seq2seq-baseline model and the
previous state-of-the-art method Point-cov. As
shown in Table 1, the summary generated by
Seq2Seq-Baseline usually contains repetition of
sentences or phrases, which seriously affects its
informativeness, conciseness as well as coherence.
The Point-cov model effectively alleviates the in-
formation repetition problem, however, it usually
loses some salient information and mainly copies
original sentences directly from the input docu-
ment. The summaries generated by our method
obviously contains more salient information and
are more concise through sentences compression,
which shows the effectiveness of the structural
regularization in our model. The results also show
that the sentence-level modeling of document and
summary in our model makes the generated sum-
maries achieve better inter-sentence coherence.

5 Discussion

5.1 Model Validation
To verify the effectiveness of each component
in our model, we conduct several ablation ex-
periments. Based on the Hierarchical-baseline
model, several different structural regulariza-
tions are added one by one: +strCom indi-

sparsity

diversity

sparsity

diversity

(a) gold reference summary (b) Seq2seq-baseline

sparsity

diversity

(c) Hierarchical-baseline

sparsity

diversity

(d) Our model with structural regularizations

Figure 3: Comparisons of structural-compression and
structural-coverage analysis results on random samples from
CNN/Daily Mail datasets, which demonstrate that both the
Seq2seq-baseline model and the Hierarchical-baseline model
are not yet able to capture them properly, but our model with
structural regularizations achieves similar behavior with the
gold reference summary.

cates adding structural-compression regulariza-
tion during model learning, +strCov indicates
adding structural-coverage regularization during
model learning, +hierD indicates using the hier-
archical decoding algorithm with both structural-
compression and structural-coverage regulariza-
tions during inference.

Results on the test set are shown in Table 5.
Our method much outperforms all the compared
systems, which verifies the effectiveness of each
component of our model. Note that, both the
structural-compression and structural-coverage
regularization significantly affect the summa-
rization performance. The higher structural-
compression and structural-coverage scores will
lead to higher ROUGE scores. Therefore, we
can conclude that the structural-compression and
structural-coverage regularization based on our hi-
erarchical model have significant contributions to
the increase of ROUGE scores.

5.2 Structural Properties Analysis

We further compare the ability of different
models in capturing the structural-compression
and structural-coverage properties of document
summarization. Figure 3 shows the compar-
ison results of 4000 document-summary pairs
with 14771 reference-summary sentences sampled
from CNN/Daily Mail dataset. Figure 3(a) shows
that most samples (over 95%) fall into the right-
top area in human-made summaries, which indi-
cates high structural-compression and structural-



4085

Figure 4: The structural regularization reduces undesirable
repetitions while summaries from the Seq2seq-baseline and
the Hierarchical-baseline contains many n-gram repetitions.

coverage scores. However, Figure 3 (b) and (c)
show that in both the Seq2seq-baseline model
and the Hierarchical-baseline model, most sam-
ples fall into the left-bottom area (low structural-
compression and structural-coverage), and only
about 13% and 7% samples fall into the right-
top area, respectively. Figure 3 (d) shows that
our system with structural regularization achieves
similar behaviors to human-made summaries (over
80% samples fall into the right-top area). The re-
sults demonstrate that the structural-compression
and structural-coverage properties are common in
document summarization, but both the seq2seq
models and the basic hierarchical encoder-decoder
models are not yet able to capture them properly.

5.3 Effects of Structural Regularization

The structural regularization based on our hi-
erarchical encoder-decoder with hybrid attention
model improves the quality of summaries from
two aspects: (1) The summary covers more salient
information and contains very few repetitions,
which can be seen both qualitatively (Table 1 and
Figure 1) and quantitatively (Table 5 and Figure
4). (2) The model has the ability to shorten a
long sentence to generate a more concise one or
compress several different sentences to generate
a more informative one by merging the informa-
tion from them. Table 6 shows several examples of
abstractive summaries produced by sentence com-
pression in our model.

6 Related Work

Recently some work explored the seq2seq mod-
els on document summarization, which exhibit
some undesirable behaviors, such as inaccurately
reproducing factual details, OOVs and repetitions.
To alleviate these issues, copying mechanism (Gu
et al., 2016; Gulcehre et al., 2016; Nallapati et al.,
2016) has been incorporated into the encoder-
decoder architecture to help generate informa-
tion correctly. Distraction-based attention model

Original Text: luke lazarus , a 23-year-old former private school boy , was jailed
for at least three years on march 27 for raping an 18-year-old virgin in an alleyway
outside his father ’s soho nightclub in kings cross , inner sydney in may 2013 .(...)
Summary: luke lazarus was jailed for at least three years on march 27 for raping an
18-year-old virgin in an alleyway outside his father ’s soho nightclub in may 2013 .
Original Text: (...) amy wilkinson , 28 , claimed housing benefit and council tax
benefit even though she was living in a home owned by her mother and her partner ,
who was also working .wilkinson , who was a british airways cabin crew attendant
, was ordered to pay back a total of 17,604 that she claimed over two years when
she appeared at south and east cheshire magistrates court last week . (...)
Summary: amy wilkinson , 28 , claimed housing benefit and council tax benefit
even though she was living in a home owned by her mother and her partner . she
was ordered to pay back a total of 17,604 that she claimed over two years when she
appeared at south and east cheshire magistrates court last week .
Original Text: (...) a grand jury charged durst with possession of a firearm by a
felon , and possession of both a firearm and an illegal drug : 5 ounces of marijuana
, said assistant district attorney chris bowman , spokesman for the district attorney .
millionaire real estate heir robert durst was indicted wednesday on the two weapons
charges that have kept him in new orleans even though his lawyers say he wants
to go to los angeles as soon as possible to face a murder charge there . his arrest
related to those charges has kept durst from being extradited to los angeles , where
he ’s charged in the december 2000 death of longtime friend susan berman .(...)
Summary: durst entered his plea during an arraignment in a new orleans court on
weapons charges that accused him of possessing both a firearm and an illegal drug
, marijuana . the weapons arrest has kept durst in new orleans even though he is
charged in the december 2000 death of a longtime friend .

Table 6: Examples of sentences compression or fusion by
our model. The link-through denotes deleting the non-salient
part of the original text. The italic denotes novel words or
sentences generated by sentences fusion or compression.

(Chen et al., 2016) and word-level coverage mech-
anism (See et al., 2017) have also been investi-
gated to alleviate the repetition problem. Rein-
forcement learning has also been studied to im-
prove the document summarization performance
from global sequence level (Paulus et al., 2017).

Hierarchical Encoder-Decoder architecture is
first proposed by Li et al. (2015) to train an
auto-encoder to reconstruct multi-sentence para-
graphs. In summarization field, hierarchical en-
coder has first been used to alleviate the long de-
pendency problem for long inputs (Cheng and La-
pata, 2016; Nallapati et al., 2016). Tan et al.
(2017b) also propose to use a hierarchical encoder
to encode multiple summaries produced by several
extractive summarization methods, and then de-
code them into a headline. However, these models
don’t model the decoding process hierarchically.

Tan et al. (2017a) first use the hierarchical
encoder-decoder architecture on generating multi-
sentences summaries. They mainly focus on incor-
porating sentence ranking into abstractive docu-
ment summarization to help detect important sen-
tences. Different from that, our work mainly tends
to verify the necessity of leveraging document
structure in document summarization and studies
how to properly capture the structural properties
of document summarization based on the hierar-
chical architecture to improve the performance of
document summarization.



4086

7 Conclusions

In this paper we analyze and verify the neces-
sity of leveraging document structure in docu-
ment summarization, and explore the effective-
ness of capturing structural properties of docu-
ment summarization by importing both structural-
compression and structural-coverage regulariza-
tion based on the proposed hierarchical encoder-
decoder with hybrid attention model. Experimen-
tal results demonstrate that the structural regular-
ization enables our model to generate more in-
formative and concise summaries by enhancing
sentences compression and coverage. Our model
achieves considerable improvement over state-of-
the-art seq2seq-based abstractive methods, espe-
cially on long document with long summary.

Acknowledgments

This work was supported by National Key Re-
search and Development Program of China under
grants 2016YFB1000902 and 2017YFC0820404,
and National Natural Science Foundation of China
under grants 61572469, 91646120, 61772501 and
61572473. We thank the anonymous reviewers for
their helpful comments about this work.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Regina Barzilay and Kathleen R McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297–
328.

Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE transactions on neural
networks, 5(2):157–166.

Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li.
2017. Faithful to the original: Fact aware neu-
ral abstractive summarization. arXiv preprint
arXiv:1711.04434.

Yllias Chali, Moin Tanvee, and Mir Tafseer Nayeem.
2017. Towards abstractive multi-document sum-
marization using submodular function-based frame-
work, sentence compression and merging. In Pro-
ceedings of the Eighth International Joint Confer-
ence on Natural Language Processing (Volume 2:
Short Papers), volume 2, pages 418–424.

Huadong Chen, Shujian Huang, David Chiang, and Ji-
ajun Chen. 2017. Improved neural machine transla-
tion with a syntax-aware encoder and decoder. arXiv
preprint arXiv:1707.05436.

Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei,
and Hui Jiang. 2016. Distraction-based neural net-
works for document summarization. arXiv preprint
arXiv:1610.08462.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.
arXiv preprint arXiv:1603.07252.

Jackie Chi Kit Cheung and Gerald Penn. 2014. Unsu-
pervised sentence enhancement for automatic sum-
marization. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 775–786.

Sumit Chopra, Michael Auli, and Alexander M Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 93–98.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12(Jul):2121–2159.

Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein.
2016. Learning-based single-document summariza-
tion with compression and anaphoricity constraints.
arXiv preprint arXiv:1603.08887.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. arXiv preprint
arXiv:1603.06393.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallap-
ati, Bowen Zhou, and Yoshua Bengio. 2016.
Pointing the unknown words. arXiv preprint
arXiv:1603.08148.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.

Philipp Koehn and Rebecca Knowles. 2017. Six
challenges for neural machine translation. arXiv
preprint arXiv:1706.03872.

Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013.
Document summarization via guided sentence com-
pression. In Proceedings of the 2013 Conference on



4087

Empirical Methods in Natural Language Process-
ing, pages 490–500.

Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015.
A hierarchical neural autoencoder for paragraphs
and documents. arXiv preprint arXiv:1506.01057.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. A sim-
ple, fast diverse decoding algorithm for neural gen-
eration. arXiv preprint arXiv:1611.08562.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Text summariza-
tion branches out: Proceedings of the ACL-04 work-
shop, volume 8. Barcelona, Spain.

Hui Lin and Jeff Bilmes. 2011. A class of submodu-
lar functions for document summarization. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 510–520. As-
sociation for Computational Linguistics.

Rui Lin, Shujie Liu, Muyun Yang, Mu Li, Ming Zhou,
and Sheng Li. 2015. Hierarchical recurrent neural
network for document modeling.

Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.
Summarunner: A recurrent neural network based se-
quence model for extractive summarization of docu-
ments. AAAI, 1:1.

Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre,
Bing Xiang, et al. 2016. Abstractive text summa-
rization using sequence-to-sequence rnns and be-
yond. arXiv preprint arXiv:1602.06023.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304.

Alexander M Rush, Sumit Chopra, and Jason We-
ston. 2015. A neural attention model for ab-
stractive sentence summarization. arXiv preprint
arXiv:1509.00685.

Abigail See, Peter J Liu, and Christopher D Man-
ning. 2017. Get to the point: Summarization
with pointer-generator networks. arXiv preprint
arXiv:1704.04368.

Sho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu Hi-
rao, and Masaaki Nagata. 2016. Neural headline
generation on abstract meaning representation. In
EMNLP, pages 1054–1059.

Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017a.
Abstractive document summarization with a graph-
based attentional neural model. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
volume 1, pages 1171–1181.

Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017b.
From neural sentence summarization to headline
generation: A coarse-to-fine approach. IJCAI.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Proceedings of the IEEE
conference on computer vision and pattern recogni-
tion, pages 3156–3164.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alexander J Smola, and Eduard H Hovy. 2016. Hi-
erarchical attention networks for document classifi-
cation. In HLT-NAACL, pages 1480–1489.

Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou.
2017. Selective encoding for abstractive sentence
summarization. arXiv preprint arXiv:1704.07073.


