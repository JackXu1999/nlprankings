



















































A Domain Adaptation Regularization for Denoising Autoencoders


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 26–31,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

A Domain Adaptation Regularization for Denoising Autoencoders

Stéphane Clinchant, Gabriela Csurka and Boris Chidlovskii
Xerox Research Centre Europe

6 chemin Maupertuis, Meylan, France
Firstname.Lastname@xrce.xerox.com

Abstract

Finding domain invariant features is crit-
ical for successful domain adaptation and
transfer learning. However, in the case of
unsupervised adaptation, there is a signif-
icant risk of overfitting on source training
data. Recently, a regularization for domain
adaptation was proposed for deep models
by (Ganin and Lempitsky, 2015). We build
on their work by suggesting a more appro-
priate regularization for denoising autoen-
coders. Our model remains unsupervised
and can be computed in a closed form.
On standard text classification adaptation
tasks, our approach yields the state of the
art results, with an important reduction of
the learning cost.

1 Introduction

Domain Adaptation problem arises each time
when we need to leverage labeled data in one or
more related source domains, to learn a classifier
for unseen data in a target domain. It has been
studied for more than a decade, with applications
in statistical machine translation, opinion mining,
part of speech tagging, named entity recognition
and document ranking (Daumé and Marcu, 2006;
Pan and Yang, 2010; Zhou and Chang, 2014).

The idea of finding domain invariant features
underpins numerous works in domain adapta-
tion. A shared representation eases prediction
tasks, and theoretical analyses uphold such hy-
potheses (Ben-David et al., 2007). For instance,
(Daumé and Marcu, 2006; Daumé, 2009) have
shown that replicating features in three main sub-
spaces (source, common and target) yields im-
proved accuracy as the classifier can subsequently
pick the most relevant common features. With
the pivoting technique (Blitzer et al., 2006; Pan

et al., 2010), the bag of words features are pro-
jected on a subspace that captures the relations
between some central pivot features and the re-
maining words. Similarly, there are several ex-
tensions of topic models and matrix factorization
techniques where the latent factors are shared by
source and target collections (Chen and Liu, 2014;
Chen et al., 2013).

More recently, deep learning has been pro-
posed as a generic solution to domain adaptation
and transfer learning problems by demonstrating
their ability to learn invariant features. On one
hand, unsupervised models such as denoising au-
toencoders (Glorot et al., 2011) or models built
on word embeddings (Bollegala et al., 2015) are
shown to be effective for domain adaptation. On
the other hand, supervised deep models (Long et
al., 2015) can be designed to select an appropri-
ate feature space for classification. Adaptation to
a new domain can also be performed by fine tun-
ing the neural network on the target task (Chopra
et al., 2013). While such solutions perform rel-
atively well, the refinement may require a signif-
icant amount of new labeled data. Recent work
by (Ganin and Lempitsky, 2015) has proposed a
better strategy; they proposed to regularize inter-
mediate layers with a domain prediction task, i.e.
deciding whether an object comes from the source
or target domain.

This paper proposes to combine the domain pre-
diction regularization idea of (Ganin and Lempit-
sky, 2015) with the denoising autoencoders. More
precisely, we build on stacked Marginalized De-
noising Autoencoders (sMDA) (Chen et al., 2012),
which can be learned efficiently with a closed form
solution. We show that such domain adaptation
regularization keeps the benefits of the sMDA and
yields results competitive to the state of the art re-
sults of (Ganin and Lempitsky, 2015).

26



2 Target Regularized MDA

Stacked Denoising Autoencoders (sDA) (Vincent
et al., 2008) are multi-layer neural networks
trained to reconstruct input data from partial ran-
dom corruption. The random corruption, called
blank-out noise or dropout, consists in randomly
setting to zero some input nodes with probability
p; it has been shown to act as a regularizer (Wa-
ger et al., 2013). The sDA is composed of a
set of stacked one-layer linear denoising autoen-
coder components, which consider a set of N in-
put documents (represented by d-dimensional fea-
tures xn) to be corrupted M times by random fea-
ture dropout and then reconstructed with a linear
mapping W ∈ Rd×d by minimizing the squared
reconstruction loss:

L(W) =
N∑

n=1

M∑
m=1

||xn − x̃nmW||2. (1)

As explicit corruption comes at a high com-
putational cost, (Chen et al., 2012) propose to
marginalize the loss (1) by considering the limit-
ing case when M →∞ and reducing de facto the
learning cost. The main advantage of this method
is a closed form solution for W, which depends
only on the uncorrupted inputs (xn) and the drop-
out probability. Several Marginalized Denoising
Autoencoders (MDA) can be then stacked together
to create a deep architecture where the representa-
tions of the (l − 1)th layer serves as inputs to the
lth layer1.

In the case of domain adaptation, the idea is to
apply MDA (or sMDA) to the union of unlabeled
source Xs and target Xt examples. Then, a stan-
dard learning algorithm such as SVM or Logistic
Regression is trained on the labeled source data us-
ing the new feature representations (xsnW) which
captures better the correlation between the source
and target data.

In Figure 1, we illustrate the effect of the MDA;
it shows the relation between the word log docu-
ment frequency (x-axes) and the expansion mass
defined as the total mass of words transformed
into word i by MDA and represented by

∑
j Wji.

We can see that the mapping W learned by MDA
is heavily influenced by frequent words. In fact,
MDA behaves similarly to document expansion
on text documents: it adds new words with a

1Between layers, in general, a non linear function such as
tanh or ReLU is applied.

Figure 1: Relation between log document fre-
quency and expansion mass. One dot represents
one word.

very small frequency and sometimes words with a
small negative weight. As the figure shows, MDA
promotes common words (despite the use of tf-idf
weighting scheme) that are frequent both in source
and target domains and hence aims to be domain
invariant.

This is in line with the work of (Ganin et al.,
2015). To strengthen the invariance effect, they
suggested a deep neural architecture which em-
beds a domain prediction task in intermediate lay-
ers, in order to capture domain invariant features.
In this paper we go a step further and refine this
argument by claiming that we want to be domain
invariant but also to be as close as possible to the
target domain distribution. We want to match the
target feature distribution because it is where the
classification takes place.

We therefore propose a regularization for the
denoising autoencoders, in particular for MDA,
with the aim to make source data resemble the tar-
get data and hence to ease the adaptation.

We describe here the case of two domains, but
it can be easily generalized to multiple domains.
Let D be the vector of size N indicating for each
document its domain, e.g. taking values of ’−1’
for source and ’+1’ for target examples. Let c be
a linear classifier represented as a d dimensional
vector trained to distinguish between source and
target, e.g. a ridge classifier that minimizes the loss
R(c, α) = ||D−Xc>||2 + α||c||2.

We guide the mapping W in such a way that
the denoised data points xW go towards the target
side, i.e. xWc> = 1 for both source and target

27



samples. Hence, we can extend each term of the
loss (1) as follows:

||xn − x̃nmW||2 + λ||1− x̃nmWc>||2. (2)
The first term here represents the reconstruction

loss of the original input, like in MDA. In the sec-
ond term, x̃mnWc> is the domain classifier pre-
diction for the denoised objects forced to be close
to 1, the target domain indicator, and λ > 0.

Let X̄ be the concatenation ofM replicated ver-
sion of the original data X, and X̃ be the matrix
representation of the M corrupted versions. Tak-
ing into account the domain prediction term, the
loss can be written as:

LR(W) = ||X̄− X̃W||2 + λ||R̄− X̃Wc>||2,
(3)

where R is a vector of sizeN , indicating a desired
regularization objective, and R̄ its M -replicated
version. Loss (3) represents a generic form to cap-
ture three different ideas:

• If R = 1, the model incites the reconstructed
features moving towards target specific fea-
tures.

• If R = −D, the model aims to promote do-
main invariant features as in (Ganin et al.,
2015).

• If R = [0; 1], where 0 values are used for
source data, the model penalizes the source
specific features.

Learning the mapping W. (Chen et al., 2012)
observed that the random corruption from equa-
tion (1) could be marginalized out from the re-
construction loss, yielding a unique and optimal
solution. Furthermore, the mapping W can be ex-
pressed in closed form as W = PQ−1, with:

Qij =
[

Sijqiqj , if i 6= j,
Sijqi, if i = j,

Pij = Sijqj , (4)

where2 q = [1 − p, . . . , 1 − p] ∈ Rd , p is the
dropout probability, and S = XXT is the covari-
ance matrix of the uncorrupted data X.

The domain regularization term in (3) is
quadratic in W, the random corruption can still be

2In contrast to (Chen et al., 2012), we do not add a bias
feature so that the domain and MDA have the same dimen-
sionality. Experiments shown no impact on the performance.

marginalized out and the expectations obtained in
closed form. Indeed, the mapping W which mini-
mizes the expectation of 1MLR(W) is the solution
of the following linear system3:

(P + λ(1− p)X>Rc>)(I + λcc>)−1 = QW.
(5)

In (5), parameter λ controls the effect of the
proposed target regularization in the MDA and
the regularization on c is controlled by parame-
ter α. This approach preserves the good properties
of MDA, i.e. the model is unsupervised and can
be computed in closed form. In addition, we can
easily stack several layers together and add non-
linearities between layers.

3 Experiments

We conduct unsupervised domain adaptation ex-
periments on two standard collections: the Ama-
zon reviews (Blitzer et al., 2011) and the 20News-
group (Pan and Yang, 2010) datasets.

From the Amazon dataset we consider the four
most used domains: dvd (D), books (B), electron-
ics (E) and kitchen (K), and adopt the settings of
(Ganin et al., 2015) with the 5000 most frequent
common features selected for each adaptation task
and a tf-idf weighting. We then use the Logistic
Regression (LR) to classify the reviews.

Our previous experiments with MDA revealed
that the MDA noise probability p needs to be set
to high values (e.g. 0.9). A possible explanation is
that document representations are already sparse
and adding low noise has no effect on the features
already equal to zero. Figure 2 shows the average
accuracy for the twelve Amazon tasks, when we
vary the noise probability p.

In addition, we observed that a single layer
with a tanh activation function is sufficient to
achieve top performance; stacking several layers
and/or concatenating the outputs with the original
features yields no improvement but increases the
computational time.

The dropout probability p is fixed to 0.9 in all
experiments, for both the MDA baseline and our
model; we test the performance with a single layer
and a tanh activation function. Stacking several
layers is left for future experiments. Parameters
α and λ are tuned on a grid of values4 by cross
validation on the source data. In other words, we

3The derivation is not included due to space limitation.
4α ∈ [.1, 1, 50, 100, 150, 200, 300], λ ∈ [.01, .1, 1, 10].

28



0.2 0.4 0.6 0.8

noise p

0.775

0.780

0.785

0.790

0.795

0.800

0.805

0.810

0.815

A
cc

u
ra

cy
MDA
MDA+TR

Figure 2: Impact of the noise parameter p on the
average accuracy for the 12 Amazon adaptation
tasks. Both MDA and its extension with the reg-
ularization (MDA+TR) perform better with a high
dropout-out noise. Here MDA+TR is run with
fixed parameters α = 100 and λ = 1.

select the LR parameters and the parameters α, λ
by cross validating the classification results using
only the ”reconstructed” source data; for estimat-
ing W we used the source with an unlabeled tar-
get set (excluded at test time). This corresponds
to the setting used in (Ganin et al., 2015), with the
difference that they use SVM and reverse cross-
validation5.

Table 3 shows the results for twelve adapta-
tion tasks on the Amazon review dataset for the
four following methods. Columns 1 and 2 show
the LR classification results on the target set for
the single layer MDA and the proposed target
regularized MDA (MDA+TR). Column 3 reports
the SVM result on the target from (Ganin et al.,
2015). They used a 5 layers sMDA where the
5 outputs are concatenated with input to generate
30,000 features, on which the SVM is then trained
and tested (G-sMDA). Finally, column 4 shows
the current state of the art results obtained with
Domain-Adversarial Training of Neural Networks
(DA NN) instead of SVM (Ganin et al., 2015).

Despite a single layer and LR trained on the
source only, the MDA baseline (80.15% on aver-
age) is very close to the G-sMDA results obtained
with 5 layer sMDA and 6 times larger feature set
(80.18%). Furthermore, adding the target regular-
ization allows to significantly outperform in many

5It consists in using self training on the target validation
set and calibrating parameters on a validation set from the
source labeled data.

S T MDA MDA+TR G-sMDA DA NN
D B 81.1 81.4 82.6 82.5
D K 84.1 85.3 84.2 84.9
D E 76.0 81.1 73.9 80.9
B D 82.7 81.7 83.0 82.9
B K 79.8 81.8 82.1 84.3
B E 75.9 79.3 76.6 80.4
K D 78.5 79.0 78.8 78.9
K B 77.0 77.0 76.9 71.8
K E 87.2 87.4 86.1 85.6
E D 78.5 78.3 77.0 78.1
E B 73.3 75.1 76.2 77.4
E K 87.7 88.2 84.7 88.1

Avg 80.15 81.27 80.18 81.32

Table 1: Accuracies of MDA, MDA+TR, G-
sMDA and DA NN on the Amazon review dataset.
Underline indicates improvement over the base-
line MDA, bold indicates the highest value.

cases the baseline and the state of the art DA NN.
We note that our method has a much lower cost,
as it uses the closed form solution for the recon-
struction and a simple LR on the reconstructed
source data, instead of domain-adversarial train-
ing of deep neural networks.

We also look at the difference between the pre-
viously introduced expansion mass for the MDA
and MDA+TR. In the adaptation task from dvd (D)
to electronics (E), the words for which the mass
changed the most are the following6: worked,
to use, speakers, i have, work, mouse, bought, ca-
ble, works, quality, unit, ipod, price, number ,
sound, card, phone, use, product, my. These words
are mostly target specific and the results confirm
that they get promoted by the new model.

Our model favors features which are more likely
to appear in target examples, while DA NN seeks
domain invariant features. Despite this difference,
the two approaches achieve similar results. It is
surprising, and we argue that eventually both ap-
proaches penalize source specific features. To test
this hypothesis, we use MDA with R = [0; 1]
(case 3) that penalizes source specific features and
we obtain again similar performances.

Finally, we test our approach on the 20News-
group adaptation tasks described in (Pan and
Yang, 2010). We first filter out rare words and
keep at most 10,000 features. Then, we apply both
MDA and MDA+TR as above. Table 3 shows re-
sults for ten adaptation tasks. As we can see, in all
cases the target regularization (MDA+TR) helps
improve the classification accuracy.

6In ascending order of the differences.

29



Task MDA MDA+TR
comp vs sci 73.69 73.38
sci vs comp 69.39 69.92
rec vs talk 72.54 85.10
talk vs rec 72.30 76.22
rec vs sci 77.25 82.70
sci vs rec 79.95 80.00
sci vs talk 78.94 79.26
talk vs sci 77.17 77.91
comp vs rec 89.84 89.66
rec vs comp 89.92 90.29
Avg 78.1 80.40

Table 2: Accuracies of MDA and MDA+TR on
20Newsgroup adaptation tasks.

4 Conclusion

This paper proposes a domain adaptation regu-
larization for denoising autoencoders, in particu-
lar for marginalized ones. One limitation of our
model is the linearity assumption for the domain
classifier, but for textual data, linear classifiers are
the state of the art technique. As new words and
expressions become more frequent in a new do-
main, the idea of using the dropout regularization
that forces the reconstruction of initial objects to
resemble target domain objects is rewarding. The
main advantage of the new model is in the closed
form solution. It is also unsupervised, as it does
not require labeled target examples and yields per-
formance results comparable with the current state
of the art.

References

Shai Ben-David, John Blitzer, Koby Crammer, and
Fernando Pereira. 2007. Analysis of representa-
tions for domain adaptation. In Advances in Neural
Information Processing Systems, NIPS Conference
Proceedings, Vancouver, British Columbia, Canada,
December 4-7, 2006., volume 19.

John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP, 22-23 July 2006, Sydney, Australia.

John Blitzer, Sham Kakade, and Dean P. Foster. 2011.
Domain adaptation with coupled subspaces. In Pro-
ceedings of the Fourteenth International Conference
on Artificial Intelligence and Statistics, AISTATS,
Fort Lauderdale, USA, April 11-13, 2011.

Danushka Bollegala, Takanori Maehara, and Ken-ichi
Kawarabayashi. 2015. Unsupervised cross-domain
word representation learning. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics, ACL, July 26-31, 2015, Beijing,
China, volume 1.

Zhiyuan Chen and Bing Liu. 2014. Topic modeling
using topics from many domains, lifelong learning
and big data. In Proceedings of the 31st Interna-
tional Conference on Machine Learning, ICML Be-
jing, 21-16 June 2014.

M. Chen, Z. Xu, K. Q. Weinberger, and F. Sha. 2012.
Marginalized denoising autoencoders for domain
adaptation. ICML, arXiv:1206.4683.

Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh.
2013. Leveraging multi-domain prior knowledge in
topic models. In Proceedings of the Twenty-Third
International Joint Conference on Artificial Intelli-
gence, IJCAI ’13, pages 2071–2077. AAAI Press.

S. Chopra, S. Balakrishnan, and R. Gopalan. 2013.
DLID: Deep learning for domain adaptation by in-
terpolating between domains. In Proceedings of the
30th International Conference on Machine Learn-
ing, ICML, Atlanta, USA, 16-21 June 2013.

H. Daumé and D. Marcu. 2006. Domain adaptation
for statistical classifiers. JAIR, 26:101–126.

H. Daumé. 2009. Frustratingly easy domain adapta-
tion. CoRR, arXiv:0907.1815.

Yaroslav Ganin and Victor S. Lempitsky. 2015. Un-
supervised domain adaptation by backpropagation.
In Proceedings of the 32nd International Confer-
ence on Machine Learning, ICML, Lille, France, 6-
11 July 2015, pages 1180–1189.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor S. Lempitsky.
2015. Domain-adversarial training of neural net-
works. CoRR, abs/1505.07818.

X. Glorot, A. Bordes, and Y. Bengio. 2011. Domain
adaptation for large-scale sentiment classification:
A deep learning approach. In Proceedings of the
28th International Conference on Machine Learn-
ing, ICML, Bellevue, Washington, USA, June 28-
July 2, 2011.

M. Long, Y. Cao, , J. Wang, and M. Jordan. 2015.
Learning transferable features with deep adapta-
tion networks. In Proceedings of the 32nd Inter-
national Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015.

S. J. Pan and Q. Yang. 2010. A survey on transfer
learning. Knowledge and Data Engineering, IEEE
Transactions on, 22(10):1345–1359.

30



Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain sen-
timent classification via spectral feature alignment.
In Proceedings of the 19th International Conference
on World Wide Web, WWW, New York, NY, USA.
ACM.

P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Man-
zagol. 2008. Extracting and composing robust
features with denoising autoencoders. In Proceed-
ings of the 25nd International Conference on Ma-
chine Learning, ICML, Helsinki, Finland on July 5-
9, 2008.

Stefan Wager, Sida I. Wang, and Percy Liang. 2013.
Dropout training as adaptive regularization. In 26,
editor, Advances in Neural Information Process-
ing Systems, NIPS Conference Proceedings, Lake
Tahoe, Nevada, United States, December 5-8, 2013.

Mianwei Zhou and Kevin C. Chang. 2014. Unify-
ing learning to rank and domain adaptation: En-
abling cross-task document scoring. In Proceedings
of the 20th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD
’14, pages 781–790, New York, NY, USA. ACM.

31


