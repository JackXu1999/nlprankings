



















































Multi-grained Attention Network for Aspect-Level Sentiment Classification


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3433–3442
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

3433

Multi-grained Attention Network for Aspect-Level
Sentiment Classification

Feifan Fan1 Yansong Feng 12∗ Dongyan Zhao13
1Institute of Computer Science and Technology, Peking University, China

2MOE Key Laboratory of Computational Linguistics, Peking University, China
3Beijing Institute of Big Data Research, China

{fanff, fengyansong, zhaody}@pku.edu.cn

Abstract

We propose a novel multi-grained attention
network (MGAN) model for aspect level sen-
timent classification. Existing approaches
mostly adopt coarse-grained attention mecha-
nism, which may bring information loss if the
aspect has multiple words or larger context.
We propose a fine-grained attention mecha-
nism, which can capture the word-level in-
teraction between aspect and context. And
then we leverage the fine-grained and coarse-
grained attention mechanisms to compose the
MGAN framework. Moreover, unlike previ-
ous works which train each aspect with its
context separately, we design an aspect align-
ment loss to depict the aspect-level interac-
tions among the aspects that have the same
context. We evaluate the proposed approach
on three datasets: laptop and restaurant are
from SemEval 2014, and the last one is a twit-
ter dataset. Experimental results show that the
multi-grained attention network consistently
outperforms the state-of-the-art methods on all
three datasets. We also conduct experiments to
evaluate the effectiveness of aspect alignment
loss, which indicates the aspect-level interac-
tions can bring extra useful information and
further improve the performance.

1 Introduction

Aspect level sentiment classification is a funda-
mental task in sentiment analysis (Pang et al.,
2008; Liu, 2012), which aims to infer the senti-
ment polarity (e.g. positive, neutral, negative) of
sentence with respect to the aspects. For exam-
ple, in sentence “I like coming back to Mac OS
but this laptop is lacking in speaker quality com-
pared to my $400 old HP laptop”, the polarity of
the sentence towards the aspect “Mac OS” is pos-
itive while the polarity is negative in terms of as-
pect “speaker quality”.

∗corresponding author.

Many statistical methods, such as support vec-
tor machine (SVM) (Wagner et al., 2014; Kir-
itchenko et al., 2014), are employed with well-
designed handcrafted features. In recent years,
neural network models (Socher et al., 2011; Dong
et al., 2014; Nguyen and Shirai, 2015) are stud-
ied to automatically learn low-dimensional repre-
sentations for aspects and their context. Attention
mechanism (Wang et al., 2016; Li et al., 2017;
Ma et al., 2017) is also be studied to character-
ize the effect of aspect on enforcing the model
to pay more attention on the important words of
the context. Previous works (Tang et al., 2016b;
Chen et al., 2017) mainly employed the simple av-
eraged aspect vector to learn the attention weights
on the context words. Ma et al. [2017] further
proposed the bidirectional attention mechanism,
which interactively learns the attention weights on
context/aspect words, with respect to the averaged
vector of aspect/context, respectively.

These above attention methods are all at the
coarse-grained level, which simply averages the
aspect/context vector to guide learning the atten-
tion weights on the context/aspect words. The
simple average pooling mechanism might cause
information loss, especially for the aspect with
multiple words or larger context. For example, in
sentence “I like coming back to Mac OS but this
laptop is lacking in speaker quality compared to
my $400 old HP laptop”, the simple averaged vec-
tor of long context might lose information when
steering the attention weights on aspect words.
Similarly, the simple averaged vector of aspect
(i.e. “speaker quality”) may deviate from the intu-
itive core meaning (i.e. “quality”) when enforcing
the model to pay varying attentions on the context
words. From another perspective, previous works
all regard the aspect and its context words as one
instance, and train each instance separately. How-
ever, they do not consider the relationship among



3434

the instances that have the same context words.
The aspect-level interactions among the instances
with same context might bring extra useful in-
formation. Considering the above example, in-
tuitively, the aspect “speaker quality” should pay
more attention on “lacking” and less attention on
“like”, compared with aspect “Mac OS”, since
they have different sentiment polarities.

In this paper, we propose a multi-grained at-
tention network to address the above two is-
sues in aspect level sentiment classification.
Specifically, we propose a fine-grained atten-
tion mechanism (i.e. F-Aspect2Context and F-
Context2Aspect), which is employed to character-
ize the word-level interactions between aspect and
context words, and relieve the information loss
occurred in coarse-grained attention mechanism.
In addition, we utilize the bidirectional coarse-
grained attention (i.e. C-Aspect2Context and C-
Context2Aspect) and combine them with fine-
grained attention vectors to compose the multi-
grained attention network for the final sentiment
polarity prediction, which can leverage the advan-
tages of them. More importantly, in order to make
use of the valuable aspect-level interaction infor-
mation, we design an aspect alignment loss in the
objective function to enhance the difference of the
attention weights towards the aspects which have
the same context and different sentiment polari-
ties. As far as we know, we are the first to explore
the interactions among the aspects with the same
context.

To evaluate the proposed approach, we conduct
experiments on three datasets: laptop and restau-
rant are from the SemEval 2014 Task 4 and the
third one is a tweet collection. Experimental re-
sults show that our method achieves the best per-
formance on all three datasets.

2 Related Work

Aspect-level sentiment analysis is a branch of sen-
timent classification, which requires considering
both the sentence and aspect information.

Traditional approaches (Jiang et al., 2011; Kir-
itchenko et al., 2014; Vo and Zhang, 2015) regard
this task as the text classification problem and de-
sign effective features, which are utilized in sta-
tistical learning algorithms for training a classi-
fier. Kiritchenko et al. [2014] proposed to use
SVM based on n-gram features, parse features and
lexicon features, which achieved the best perfor-

mance in SemEval 2014. Vo and Zhang [2015]
designed sentiment-specific word embedding and
sentiment lexicons as rich features for prediction.
These methods highly depend on the effectiveness
of the laborious feature engineering work and eas-
ily reach the performance bottleneck.

In recent works, there are growing studies on
neural network based methods due to their ca-
pability of encoding original features as continu-
ous and low-dimensional vectors without feature
engineering. Recursive Neural Network (Socher
et al., 2011; Dong et al., 2014; Nguyen and Shirai,
2015) are studied to conduct semantic composi-
tions on tree structures, and generate representa-
tions for prediction. Methods on LSTM (Hochre-
iter and Schmidhuber, 1997) were proposed to
model the context information and use an ag-
gregated vector for prediction. TD-LSTM (Tang
et al., 2016a) adopted LSTM to model the left
context and right context of the aspect, and con-
catenate them as the representation for prediction.
However, these works only focused on model-
ing the contexts without considering the aspects,
which performed an important role in estimate the
sentiment polarity.

Attention mechanisms (Wang et al., 2016; Lei
et al., 2016; Li et al., 2017) are studied to enhance
the influence of aspects on the final representa-
tion for prediction. Many approaches (Tang et al.,
2016b; Chen et al., 2017) adopted the averaged as-
pect vector to learn the attention weights on the
hidden vectors of context words. Ma et al. [2017]
further proposed bidirectional attention mecha-
nism, which also learns the attention weights on
aspect words towards the averaged vector of con-
text words. These attention methods only con-
sider the coarse-grained level attention, through
using the simple averaged aspect/context vector
to steer the attention weights learning on the con-
text/aspect words, which might cause some infor-
mation loss on the long aspect or context case.

In contrast, motivated by the bidirectional atten-
tion flow approaches (Seo et al., 2017; Pan et al.,
2017) in machine comprehension, we propose
a fine-grained attention mechanism which is re-
sponsible for linking and fusing information from
the aspect and the context words. Furthermore,
we leverage both the coarse-grained and fine-
grained attentions to compose the multi-grained
attention network (MGAN). In addition, existing
works train each instance separately. However, we



3435

observe that the interactions among the aspects,
which have the same context words, could bring
extra useful information. Thus we design the as-
pect alignment loss in the objective function to de-
pict such kind of relationship, which is the first
work to explore the aspect-level interactions.

3 Our Approach

3.1 Task Definition

Given a sentence s = {w1, w2, · · · , wN} con-
sisting of N words, and an aspect list A =
{a1, · · · , ak}, where the aspect list size is k and
each aspect ai = {wi1 , · · · , wiM } is a subse-
quence of sentence s, which contains M ∈ [1, N)
words. Aspect-level sentiment classification eval-
uates sentiment polarity of the sentence s with re-
spect to each aspect ai.

We present the overall architecture of the pro-
posed Multi-grained Attention Network (MGAN)
model in Figure 1. It consists of the Input Em-
bedding layer, the Contextual Layer, the Multi-
grained Attention Layer and the Output Layer.

3.2 Input Embedding Layer

Input Embedding Layer maps each word to a high
dimensional vector space. We employ the pre-
trained word vector, GloVe (Pennington et al.,
2014), to obtain the fixed word embedding of
each word. Specifically, we denote the embedding
lookup matrix as L ∈ Rdv×|V |, where dv is the
word vector dimension and |V | is the vocabulary
size.

3.3 Contextual Layer

We employ a bidirectional Long Short-Term
Memory Network (BiLSTM) on top of the em-
bedding layer to capture the temporal interactions
among words. Specifically, at time step t, given
the input word embedding x, the update process
of forward LSTM network can be formalized as
follows:

it = σ(
−→
W i · [

−→
h t−1,−→x t] +

−→
b i) (1)

ft = σ(
−→
W f · [

−→
h t−1,−→x t] +

−→
b f ) (2)

ot = σ(
−→
W o · [

−→
h t−1,−→x t] +

−→
b o) (3)

gt = tanh(
−→
W g · [

−→
h t−1,−→x t] +

−→
b g) (4)

−→c t = ft ∗ −→c t−1 + it ∗ gt (5)
−→
h t = ot ∗ tanh(−→c t) (6)

Where σ is the sigmoid activation function, it, ft,
ot are the input gate, forget gate and output gate,
respectively. −→W i,

−→
W f ,

−→
W o,
−→
W g ∈ Rd∗(d+dv),−→

b i,
−→
b f ,
−→
b o,
−→
b g ∈ Rd, and d is the hidden di-

mension size. The backward LSTM does the simi-
lar process and we can get the concatenated output
ht = [

−→
h t,
←−
h t] ∈ R2d. Given the word embed-

dings of a context sentence s and a corresponding
aspect aj , we will employ the BiLSTM separately
and get the sentence contextual outputH ∈ R2d∗N
and aspect contextual output Q ∈ R2d∗M .

In addition, considering that the context words
with closer distance to an aspect may have higher
influence to the aspect, we utilize the position
encoding mechanism to simulate the observation.
Formally, the weight for a context word wj , which
has l word-level distance from the aspect (here we
treat the aspect phrase as a single unit), is defined
as follows:

wt = 1−
l

N −M + 1
(7)

Specifically, we treat the weights of words within
the aspect as 0 in order to focus on the context
words in the sentence. Then we can obtain the
final contextual outputs of context words H =
[H1 ∗ w1, · · · , HN ∗ wN ].

3.4 Multi-grained Attention Layer
Attention mechanism is a common way to capture
the interactions between the aspect and context
words. Previous methods (Tang et al., 2016b; Ma
et al., 2017; Chen et al., 2017) only adopt coarse-
grained attentions, which simply use the aver-
aged aspect/context vector as the guide to learn
the attention weights on context/aspect. How-
ever, the simple average pooling in generating the
guide vector might bring some information loss,
especially for the aspect with multiple words or
larger context. We propose the fine-grained at-
tention mechanism, which is responsible for link-
ing and fusing information from the aspect and
context words. This mechanism is designed to
capture the word-level interactions which esti-
mate how each aspect/context word affect each
context/aspect word. In addition, we concate-
nate both the fine-grained and coarse-grained at-
tention vectors to obtain the final representation.
From other perspective, we observe the relation-
ship among aspects can introduce extra valuable
information. Hence, we propose an aspect align-
ment loss, which is designed to strengthen the at-



3436

hNh1 h2

…

wi1 wi2 wiM

…

wNw1 w2

hMh1 h2

Location Encoding

M

N

Alignment Matrix

Pooling
Pooling

Aspect Context

Aspect Sentiment

Softmax

Input Embedding

Contextual Layer

Multi-grained Attention

Output Layer

C-Aspect2Context

F-Aspect2Context

C-Context2Aspect

F-Context2Aspect

Aspect Alignment Loss

Figure 1: The architecture of the proposed multi-grained attention network.

tention difference among aspects with same con-
text and different sentiment polarities.

Coarse-grained Attention
Coarse-grained attention is a widely used mech-
anism to capture the interactions between aspect
and context, which utilizes an averaged aspect
vector to steer the attention weights on the con-
text words. Follow the work in (Ma et al., 2017),
we employ the bidirectional attention mechanism,
namely C-Aspect2Context and C-Context2Aspect.

(1) C-Aspect2Context learns to assign atten-
tion scores to the context words with respect to the
averaged aspect vector. Here we employ an aver-
age pooling layer above aspect contextual output
Q to generate the averaged aspect vector Qavg ∈
R2d. For each word vector Hi in context, we can
compute the attention score acai as follows:

sca(Qavg, Hi) = Qavg ∗Wca ∗Hi (8)

acai =
exp(sca(Qavg, Hi))∑N

k=1 exp(sca(Qavg, Hk))
(9)

Where the score function sca computes the weight
which indicates the importance of a context word
towards aspect sentiment. Wca ∈ R2d∗2d is the
attention weight matrix. Then the weighted com-
bination of the context output mca ∈ R2d is calcu-

lated as follows:

mca =

N∑
i=1

acai ·Hi (10)

(2) C-Context2Aspect learns to assign atten-
tion weights on aspects words, which follows the
similar learning process with C-Aspect2Context.
We utilize the average pooling mechanism to ob-
tain the averaged context vector Havg, and com-
pute the weights for each word wi in the aspect
phrase. We compute the final weighted combina-
tion of aspect vector mcc ∈ R2d as follows:

scc(Havg, Qi) = Havg ∗Wcc ∗Qi (11)

acci =
exp(scc(Havg, Qi))∑M

k=1 exp(scc(Havg, Qk))
(12)

mcc =

M∑
i=1

acci ·Qi (13)

where W cc ∈ R2d∗2d is the attention weight ma-
trix.

Fine-grained Attention
As introduced above, we propose a fine-grained at-
tention mechanism to characterize the word-level
interactions and evaluate how each aspect/context



3437

word affect each context/aspect word. Consider-
ing the previous example “I like coming back to
Mac OS but this laptop is lacking in speaker qual-
ity compared to my $400 old HP laptop”, the word
“quality” in aspect “speaker quality” should have
more effect on the context words compared with
word “speaker”. Accordingly, the context words
should pay more attention on “quality” instead of
“speaker”.

Formally, we define an alignment matrix U ∈
RN∗M , between the contextual output of and the
context H and the aspect Q, where Uij indicates
the similarity between i-th context word and j-th
aspect word. The similarity matrix U is computed
by

Uij = Wu([Hi;Qj ;Hi ∗Qj ]) (14)
Where Wu ∈ R1∗6d is the weight matrix, [; ] is the
vector concatenation across row, ∗ is the elemen-
twise multiplication. Then we use U to calculate
the attention vectors in both directions.

(1) F-Aspect2Context estimates which context
word has the closest similarity to one of the aspect
word and are hence critical for determining the
sentiment. We can compute the attention weights
afa on context words by

sfai = max(Ui,:) (15)

afai =
exp(sfai )∑N
k=1 exp(s

fa
k )

(16)

where sfai obtains the maximum similarity across
column. And then we can get the attended vector
mfa ∈ R2d as follows:

mfa =
N∑
i=1

afai ·Hi (17)

(2) F-Context2Aspect measures which aspect
words are most relevant to each context word. Let
afci ∈ RM be the attention weights on aspect
contextual output Q with respect to the i-th con-
text word vector Hi. The attended aspect vector
qfc ∈ R2d∗N is defined as follows:

afcij =
exp(Uij)∑M
k=1 exp(Uik)

(18)

qfci =
M∑
j=1

afcij ·Qj (19)

Then we use an average pooling layer on qfc to get
the attended vector mfc ∈ R2d:

mfc = Pooling([qfc1 , · · · , q
fc
N ]) (20)

3.5 Output Layer
At last, we concatenate both the coarse-grained
and fine-grained attention vectors as the final rep-
resentation m ∈ R8d, which will be fed to a soft-
max layer for determining the aspect sentiment po-
larity.

m = [mca;mcc;mfa;mfc] (21)

p = softmax(Wp ∗m+ bp) (22)

where p ∈ RC is the probability distribution for
the polarity of aspect sentiment, Wp ∈ RC∗8d and
bp ∈ RC are the weight matrix and bias, respec-
tively. Here we set C = 3, which is the number of
aspect sentiment classes.

3.6 Model Training
Aspect Alignment Loss
Existing approaches train each aspect with its con-
text separately, without considering the relation-
ship among the aspects. However, we observe the
aspect-level interactions can bring extra valuable
information. In order to enhance the attention dif-
ferences of aspects, which have the same context
and different sentiment polarities, we design the
aspect alignment loss on the C-Aspect2Context at-
tention weights. C-Aspect2Context is employed to
find the important context words in terms of a spe-
cific aspect. With the constraint of aspect align-
ment loss, each aspect will pay more attention on
the important words through the comparisons with
other related aspects. In terms of the previous ex-
ample, the aspect “speaker quality” should pay
more attention on “lacking” and less attention on
“like”, compared with aspect “Mac OS” due to
their different sentiment polarities.

Specifically, for each aspect pair ai and aj in
aspect list A, we compute the square loss on the
coarse-grained attention vector acai and a

ca
j , and

also estimate the distance dij ∈ [0, 1] between ai
and aj as the loss weight.

dij = σ(Wd([Qi;Qj ;Qi ∗Qj ]) (23)

Lalign = −
M−1∑
i=1

M∑
j=i+1,yi 6=yj

N∑
k=1

dij · (acaik − acajk)2

(24)
Where σ is the sigmoid function, Wd ∈ R1∗6d is
weight matrix for computing the distance, yi and
yj are the true labels of the aspect ai and aj , acaik
and acajk are the attention weights on k-th context
word towards aspect ai and aj , respectively.



3438

For training the multi-grained attention
network (MGAN), we should optimize all
the parameters Θ from the LSTM networks:
[Wi,Wo,Wf ,Wg, bi, bo, bf , bg], the attention and
alignment loss parameters: [Wca,Wcc,Wu,Wd]
and softmax parameters: [Wp, bp]. The final loss
function is consisting of the cross-entropy loss,
aspect alignment loss and regularization item as
follows:

L = −
C∑
i=1

yilog(pi) + βLalign + λ ‖Θ‖2 (25)

Where β ≥ 0 and λ ≥ 0 controls the influence
of the aspect alignment loss and the L2 regulariza-
tion item, respectively. We employ the stochastic
gradient descent (SGD) optimizer to compute and
update the training parameters. In addition, we uti-
lize dropout strategy to avoid overfitting.

4 Experiments

In this section, we conduct experiments to eval-
uate our two hypotheses: (1) whether the word-
level interaction between aspect and context can
help relieve the information loss and improve the
performance. (2) whether the relationship among
the aspects, which have the same context and dif-
ferent sentiment polarities, can bring extra useful
information.

4.1 Experiment Setting

We conduct experiments on three datasets, as
shown in Table 1. The first two are from the Se-
mEval 2014 Task 41 (Pontiki et al., 2014), which
contains the reviews in laptop and restaurants, re-
spectively. The third one is a tweet collection,
which are gathered by (Dong et al., 2014). Each
aspect with the context is labeled by three senti-
ment polarities, namely Positive, Neutral and Neg-
ative. In addition, we adopt Accuracy and Macro-
F1 as the metrics to evaluate the performance
of aspect-level sentiment classification, which is
widely used in previous works (Tang et al., 2016b;
Ma et al., 2017; Chen et al., 2017; Wang et al.,
2016).

In our experiments, word embeddings for
both context and aspect words are initialized by
Glove (Pennington et al., 2014). The dimension
of word embedding dv and hidden state d are

1The detailed task introduction can be found in
http://alt.qcri.org/semeval2014/task4/.

Dataset Positive Neutral NegativeTrain Test Train Test Train Test
Laptop 994 341 870 128 464 169

Restaurant 2164 728 807 196 637 196
Twitter 1561 173 3127 346 1560 173

Table 1: The statistics of the datasets.

set to 300. The weight matrix and bias are ini-
tialized by sampling from a uniform distribution
U(0.01, 0.01). The coefficient λ of L2 regulariza-
tion item is 10−5, the parameter β of aspect align-
ment loss and drop out rate are set to 0.5.

4.2 Compared Methods

To evaluate the performance of proposed ap-
proach, we compared with the following methods:
Majority is the basic baseline, which chooses the
largest sentiment polarity in the training set to each
instance in the test set.
Feature+SVM (Kiritchenko et al., 2014) uses n-
gram features, parse features and lexicon features
based on SVM, which achieves the state-of-the-art
performance in SemEval 2014.
LSTM (Wang et al., 2016) utilizes one LSTM net-
work to learn the hidden states and obtain the av-
eraged vector to predict the sentiment polarity.
ATAE-LSTM (Wang et al., 2016) learns attention
embeddings and combine them with the LSTM
hidden states to predict the polarity.
TD-LSTM (Tang et al., 2016a) employs two di-
rectional LSTM networks, which estimate the left
context and right context of the target aspect, re-
spectively. Finally it takes the last hidden states of
LSTM networks for prediction.
MemNet (Tang et al., 2016b) applys multi-hop at-
tentions on the word embeddings, learns the atten-
tion weights on context word vectors with respect
to the averaged query vector.
IAN (Ma et al., 2017) interactively learns the
coarse-grained attentions between the context and
aspect, and concatenate the vectors for prediction.
BILSTM-ATT-G (Liu and Zhang, 2017) mod-
els left and right context with two attention-based
LSTMs and utilizes gates to control the impor-
tance of left context, right context and the entire
sentence for prediction.
RAM(Chen et al., 2017) learns multi-hop atten-
tions on the hidden states of bidirectional LSTM
networks for context words, and proposes to use
GRU network to get the aggregated vector from
the attentions. Similar with MemNet, the atten-



3439

tion weights on context words are steered by the
simple averaged aspect vector.

We also list the variants of MGAN model,
which are used to analyze the effects of coarse-
grained attention, fine-grained attention and aspect
alignment loss, respectively.
MGAN-C only employs the coarse-grained atten-
tions for prediction, which is similar with IAN.
MGAN-F only utilizes the proposed fine-grained
attentions for prediction.
MGAN-CF adopts both the coarse-grained and
fine-grained attentions, while without applying the
aspect alignment loss.
MGAN is the complete multi-grained attention
network model.

4.3 Overall Performance Comparison

Table 2 shows the performance comparison results
of MGAN with other baseline methods. We can
have the following observations.

(1) Majority performs worst since it only uti-
lizes the data distribution information. Fea-
ture+SVM can achieve much better performance
on all the datasets, with the well-designed feature
engineering. Our method MGAN outperforms
Majority and Feature+SVM since MGAN could
learn the high quality representation for predic-
tion.

(2) ATAE-LSTM is better than LSTM since it
employs attention mechanism on the hidden states
and combines with attention embedding to gener-
ate the final representation. TD-LSTM performs
slightly better than ATAE-LSTM, and it employs
two LSTM networks to capture the left and right
context of the aspect. TD-LSTM performs worse
than our method MGAN since it could not prop-
erly pay more attentions on the important parts of
the context.

(3) IAN achieves slightly better results with the
previous LSTM-based methods, which interac-
tively learns the attended aspect and context vector
as final representation. Our method consistently
performs better than IAN since we utilize the fine-
grained attention vectors to relieve the informa-
tion loss in IAN. MemNet continuously learns
the attended vector on the context word embed-
ding memory, and updates the query vector at each
hop. BILSTM-ATT-G models left context and
right context using attention-based LSTMs, which
achieves better performance than MemNet. RAM
performs better than other baselines. It employs

bidirectional LSTM network to generate contex-
tual memory, and learns the multiple attended vec-
tor on the memory. Similar with MemNet, it uti-
lizes the averaged aspect vector to learn the atten-
tion weights on context words.

Our proposed MGAN consistently performs
better than MemNet, BILSTM-ATT-G and RAM
on all three datasets. On one hand, they only
consider to learn the attention weights on context
towards the aspect, and do not consider to learn
the weights on aspect words towards the context.
On the other hand, they just use the averaged as-
pect vector to guide the attention, which will lose
some information, especially on the aspects with
multiple words. From another perspective, our
method employs the aspect alignment loss, which
can bring extra useful information from the aspect-
level interactions.

4.4 Analysis of MGAN model

Table 3 shows the performance comparison among
the variants of MGAN model. We can have the
following observations.

(1) the proposed fine-grained attention mech-
anism MGAN-F, which is responsible for link-
ing and fusing the information between the con-
text and aspect word, achieves competitive per-
formance compared with MGAN-C, especially on
laptop dataset. To investigate this case, we col-
lect the percentage of aspects with different word
lengths in Table 4. We can find that laptop dataset
has the highest percentage on the aspects with
more than two words, and the second-highest per-
centage on two words. It demonstrates MGAN-
F has better performance on aspects with more
words, and make use of the word-level interactions
to relieve the information loss occurred in coarse-
grained attention mechanism.

(2) MGAN-CF is better than both MGAN-C
and MGAN-F, which demonstrates the coarse-
grained attentions and fine-grained attentions
could improve the performance from different per-
spectives. Compared with MGAN-CF, the com-
plete MGAN model gains further improvement
by bringing the aspect alignment loss, which is
designed to capture the aspect level interactions.
Specifically, we collect the statistics of sentence-
level with different aspect amounts, which is
shown in Table 5. We can observe that both laptop
and restaurant datasets have relatively high per-
centage on the sentences with multiple aspects.



3440

Method Laptop Restaurant TwitterAcc Macro-F1 Acc Macro-F1 Acc Macro-F1
Majority 0.5350 0.3333 0.6500 0.3333 0.5000 0.3333

Feature-SVM 0.7049 - 0.8016 - 0.6340 0.6330
ATAE-LSTM 0.6870 - 0.7720 - - -

TD-LSTM 0.7183 0.6843 0.7800 0.6673 0.6662 0.6401
IAN 0.7210 - 0.7860 - - -

MemNet 0.7237 - 0.8032 - 0.6850 0.6691
BILSTM-ATT-G 0.7312 0.6980 0.7973 0.6925 0.7038 0.6837

RAM 0.7449 0.7135 0.8023 0.7080 0.6936 0.6730
MGAN 0.7539 0.7247 0.8125 0.7194 0.7254 0.7081

Table 2: The performance comparisons of different methods on the three datasets, where the results of baseline
methods are retrieved from published papers. The best performances are marked in bold.

Method Laptop Restaurant TwitterAcc Macro-F1 Acc Macro-F1 Acc Macro-F1
MGAN-C 0.7273 0.6933 0.8054 0.7099 0.7153 0.6952
MGAN-F 0.7398 0.7082 0.8000 0.7092 0.7110 0.6918

MGAN-CF 0.7445 0.7121 0.8089 0.7135 0.7254∗ 0.7081∗

MGAN 0.7539 0.7247 0.8125 0.7194 0.7254 0.7081

Table 3: The performance comparisons of MGAN variants. ∗ means MGAN-CF and MGAN can be regarded as
the same method on twitter dataset.

Dataset #words=1 #words=2 #words>2
Laptop 61.60% 29.16% 9.24%

Restaurant 74.47% 17.32% 8.21%
Twitter 29.99% 69.91% 0.10%

Table 4: The percentage of aspects with different word
length on three datasets. Here we give the overall statis-
tic of each dataset.

The improved performance on the two datasets
shows the importance of capturing the aspect-level
interactions. In terms of twitter dataset, almost all
of the sentences only has one aspect. In this case,
the method MGAN can be regarded as MGAN-
CF.

Dataset #aspects=1 #aspects=2 #aspects>2
Laptop 63.94% 23.32% 12.74%

Restaurant 50.89% 28.60% 20.51%
Twitter 99.91% 0.09% 0.00%

Table 5: The percentage of sentences with different as-
pect numbers on three datasets. Aspects with the same
context are regarded as the same sentence.

4.5 Case Study

In order to demonstrate the effect of aspect align-
ment loss, we visualize the attention weights
of the C-Aspect2Context mechanism. Figure 2
shows the attention weights of two aspects “res-
olution” and “fonts”, whose sentiment polarities
are positive and negative, respectively. From

the above two bars, we can observe that the C-
Aspect2Context can enforce the model to pay
more attentions on the important words with re-
spect to the aspect. For example, in terms of
the aspect“resolution”, the words “has”, “higher”
and “but” have higher attention weights compared
with other words. In contrast, aspect “fonts”
pays more attentions on words “but”, “fonts” and
“small”. In addition, we evaluate the effect of as-
pect alignment loss, which enhances the attention
difference between the aspect “resolution” and
“fonts”. For the two bars at bottom, we can find
that aspect “fonts” has more attention on “small”
and less attention on “higher”, compared with the
aspect “resolution”. This phenomenon shows that
with the constraint of aspect alignment loss, C-
Aspect2Context can not only learn the important
context words for each aspect, but also can make
the attention gaps on the important words be as
large as possible for aspects with different polari-
ties.

5 Conclusion

In this paper, we propose a multi-grained atten-
tion network (MGAN) for aspect-level sentiment
classification. Specifically, we propose a fine-
grained attention mechanism, which is responsible
for linking and fusing the words from the aspect
and context, to capture the word-level interaction.
And we combine it with the coarse-grained atten-



3441

air has higher resolution but the fonts are small

Aspect resolution

Aspect fonts

Aspect resolution

Aspect fonts

C-Aspect2Context

C-Aspect2Context with 
Aspect Alignment Loss

Figure 2: The attention visualizations on aspect “resolution” and “fonts”. The above two bars are from the C-
Aspect2Context attention mechanism, and the two bars at bottom are from the C-Aspect2Context attention mech-
anism with the constraint of aspect alignment loss.

tion mechanism to compose the MGAN model.
In addition, we design an aspect alignment loss
to characterize the aspect-level interactions among
aspects, which have the same context and dif-
ferent sentiment polarities, to explore extra valu-
able information. Experimental results demon-
strate the effectiveness of our approach on three
public datasets.

Acknowledgments

This work is supported by the National Nat-
ural Science Foundation of China (61672057,
61672058); KLSTSPI Key Lab. of Intelligent
Press Media Technology. For any correspondence,
please contact Yansong Feng.

References
Peng Chen, Zhongqian Sun, Lidong Bing, and Wei

Yang. 2017. Recurrent attention network on mem-
ory for aspect sentiment analysis. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 452–461.

Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), volume 2, pages 49–54.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter senti-
ment classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 151–160. Association for Computational
Linguistics.

Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and
Saif Mohammad. 2014. Nrc-canada-2014: Detect-
ing aspects and sentiment in customer reviews. In
Proceedings of the 8th International Workshop on

Semantic Evaluation (SemEval 2014), pages 437–
442.

Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.
Rationalizing neural predictions. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 107–117.

Cheng Li, Xiaoxiao Guo, and Qiaozhu Mei. 2017.
Deep memory networks for attitude identification.
In Proceedings of the Tenth ACM International Con-
ference on Web Search and Data Mining, pages 671–
680. ACM.

Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis lectures on human language tech-
nologies, 5(1):1–167.

Jiangming Liu and Yue Zhang. 2017. Attention mod-
eling for targeted sentiment. In Proceedings of the
15th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Volume 2,
Short Papers, volume 2, pages 572–577.

Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng
Wang. 2017. Interactive attention networks for
aspect-level sentiment classification. In IJCAI.

Thien Hai Nguyen and Kiyoaki Shirai. 2015.
Phrasernn: Phrase recursive neural network for
aspect-based sentiment analysis. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2509–2514.

Boyuan Pan, Hao Li, Zhou Zhao, Bin Cao, Deng Cai,
and Xiaofei He. 2017. Memen: Multi-layer embed-
ding with memory networks for machine compre-
hension. arXiv preprint arXiv:1707.09098.

Bo Pang, Lillian Lee, et al. 2008. Opinion mining and
sentiment analysis. Foundations and Trends R© in In-
formation Retrieval, 2(1–2):1–135.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Maria Pontiki, Dimitris Galanis, John Pavlopoulos,
Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4: As-
pect based sentiment analysis. Proceedings of the



3442

8th International Workshop on Semantic Evaluation
(SemEval 2014), pages 27–35.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In Proceedings of
ICLR.

Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
conference on empirical methods in natural lan-
guage processing, pages 151–161. Association for
Computational Linguistics.

Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu.
2016a. Effective lstms for target-dependent senti-
ment classification. pages 3298–3307.

Duyu Tang, Bing Qin, and Ting Liu. 2016b. Aspect
level sentiment classification with deep memory net-
work. pages 214–224.

Duy-Tin Vo and Yue Zhang. 2015. Target-dependent
twitter sentiment classification with rich automatic
features. In IJCAI, pages 1347–1353.

Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. Dcu: Aspect-based polarity
classification for semeval task 4. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014), pages 223–229.

Yequan Wang, Minlie Huang, Li Zhao, et al. 2016.
Attention-based lstm for aspect-level sentiment clas-
sification. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, pages 606–615.


