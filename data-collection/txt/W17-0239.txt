



















































Redefining Context Windows for Word Embedding Models: An Experimental Study


Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 284–288,
Gothenburg, Sweden, 23-24 May 2017. c©2017 Linköping University Electronic Press

Redefining Context Windows for Word Embedding Models:
An Experimental Study

Pierre Lison
Norwegian Computing Center

Oslo, Norway
plison@nr.no

Andrei Kutuzov
Language Technology Group

University of Oslo
andreku@ifi.uio.no

Abstract

Distributional semantic models learn vec-
tor representations of words through the
contexts they occur in. Although the
choice of context (which often takes the
form of a sliding window) has a direct in-
fluence on the resulting embeddings, the
exact role of this model component is still
not fully understood. This paper presents
a systematic analysis of context windows
based on a set of four distinct hyper-
parameters. We train continuous Skip-
Gram models on two English-language
corpora for various combinations of these
hyper-parameters, and evaluate them on
both lexical similarity and analogy tasks.
Notable experimental results are the pos-
itive impact of cross-sentential contexts
and the surprisingly good performance of
right-context windows.

1 Introduction

Distributional semantic models represent words
through real-valued vectors of fixed dimen-
sions, based on the distributional properties of
these words observed in large corpora. Re-
cent approaches such as prediction-based models
(Mikolov et al., 2013b) and GloVe (Pennington et
al., 2014) have shown that it is possible to esti-
mate dense, low-dimensional vectors (often called
embeddings) able to capture various functional or
topical relations between words. These embed-
dings are used in a wide range of NLP tasks,
including part-of-speech tagging, syntactic pars-
ing, named entity recognition and semantic role
labelling; see (Collobert et al., 2011; Lin et al.,
2015; Zhou and Xu, 2015; Lample et al., 2016),
among others.

As recently shown by (Levy et al., 2015), the
empirical variations between embedding models

are largely due to differences in hyper-parameters
(many of which are tied to the underlying defini-
tion of context) rather than differences in the em-
bedding algorithms themselves. In this paper, we
further develop their findings with a comprehen-
sive analysis of the role played by context window
parameters when learning word embeddings. Four
specific aspects are investigated:

1. The maximum size of the context window;

2. The weighting scheme of context words ac-
cording to their distance to the focus word;

3. The relative position of the context window
(symmetric, left or right side);

4. The treatment of linguistic boundaries such
as end-of-sentence markers.

The next section 2 provides a brief overview on
word embeddings and context windows. Section 3
describes the experimental setup used to evaluate
the influence of these four aspects. Finally, Sec-
tion 4 presents and discusses the results.

2 Background

The works of (Bengio et al., 2003) and (Mikolov
et al., 2013b) introduced a paradigm-shift for dis-
tributional semantic models with new prediction-
based algorithms outperforming the existing
count-based approaches (Baroni et al., 2014). The
word2vec models from (Mikolov et al., 2013b),
comprising the Continuous Skip-gram and the
Continuous Bag-of-Words algorithms, are now a
standard part of many NLP pipelines.

Despite their differences, all types of distribu-
tional semantic models require the definition of
a context for each word observed in a given cor-
pus. Given a set of (word, context) pairs extracted
from the corpus, vector representations of words
can be derived through various estimation meth-
ods, such as predicting words given their contexts

284



(CBOW), predicting the contexts from the words
(Skip-Gram), or factorizing the log of their co-
occurrence matrix (GloVe). In all of these ap-
proaches, the choice of context is a crucial factor
that directly affects the resulting vector representa-
tions. The most common method for defining this
context is to rely on a window centered around the
word to estimate (often called the focus word)1.
The context window thus determines which con-
textual neighbours are taken into account when es-
timating the vector representations.

The most prominent hyper-parameter associ-
ated to the context window is the maximum win-
dow size (i.e. the maximum distance between the
focus word and its contextual neighbours). This
parameter is the easiest one to adjust using exist-
ing software, which is why it is comparatively well
studied. Larger windows are known to induce em-
beddings that are more ‘topical’ or ‘associative’,
improving their performance on analogy test sets,
while smaller windows induce more ‘functional’
and ‘synonymic’ models, leading to better perfor-
mance on similarity test sets (Goldberg, 2016).

However, the context window is also affected
by other, less obvious hyper-parameters. Inside
a given window, words that are closer to the fo-
cus word should be given more weights than more
distant ones. To this end, CBOW and Continu-
ous Skip-gram rely on a dynamic window mech-
anism where the actual size of the context win-
dow is sampled uniformly from 1 to L, where L
is the maximum window size. This mechanism is
equivalent to sampling each context word w j with
a probability that decreases linearly with the dis-
tance | j− i| to the focus word wi:

P(w j|wi) =
L

∑
window=1

P(w j|wi,window)P(window)

=
1
L
(L−| j− i|+1)

where window is the actual window size (from 1
to L) sampled by the algorithm. Similarly, the co-
occurrence statistics used by GloVe rely on har-
monic series where words at distance d from the
focus word are assigned a weight 1d . For example,
with the window size 3, the context word at the
position 2 will be sampled with the probability of
2/3 in word2vec and the probability of 1/2 in GloVe.

1Other types of context have been proposed, such as
dependency-based contexts (Levy and Goldberg, 2014) or
multilingual contexts (Upadhyay et al., 2016), but these are
outside the scope of the present paper.

Another implicit hyper-parameter is the sym-
metric nature of the context window. The
word2vec and GloVe models pay equivalent atten-
tion to the words to the left and to the right of the
focus word. However, the relative importance of
left or right contexts may in principle depend on
the linguistic properties of the corpus language, in
particular its word ordering constraints.

Finally, although distributional semantic mod-
els do not themselves enforce any theoretical limit
on the boundaries of context windows, word em-
beddings are in practice often estimated on a sen-
tence by sentence basis, thus constraining the con-
text windows to stop at sentence boundaries. How-
ever, to the best of our knowledge, there is no sys-
tematic evaluation of how this sentence-boundary
constraint affects the resulting embeddings.

3 Experimental setup

To evaluate how context windows affect the em-
beddings, we trained Continuous Skip-gram with
Negative Sampling (SGNS) embeddings for var-
ious configurations of hyper-parameters, whose
values are detailed in Table 1. In particu-
lar, the “weighting scheme” encodes how the
context words should be weighted according to
their distance with the focus word. This hyper-
parameter is given two possible values: a lin-
ear weighting scheme corresponding to the default
word2vec weights, or an alternative scheme using
the squared root of the distance.

Hyper-parameter Possible values

Max. window size {1,2,5,10}
Weighting scheme {L−d+1L , L−

√
d+1

L }
Window position {left, right, symmetric}
Cross-sentential {yes, no}
Stop words removal {yes, no}

Table 1: Range of possible hyper-parameter values
evaluated in the experiment.

The embeddings were trained on two English-
language corpora: Gigaword v5 (Parker et al.,
2011), a large newswire corpus of approx. 4 billion
word tokens, and the English version of OpenSub-
titles (Lison and Tiedemann, 2016), a large repos-
itory of movie and TV subtitles, of approx. 700
million word tokens. The two corpora correspond
to distinct linguistic genres, Gigaword being a cor-
pus of news documents (average sentence length

285



21.7 tokens) while OpenSubtitles is a conversa-
tional corpus (average sentence length 7.3 tokens).
OpenSubtitles notably contains a large number
of non-sentential utterances, which are utterances
lacking an overt predicate and depend on the sur-
rounding dialogue context for their interpretation
(Fernández, 2006). The corpora were lemma-
tized and POS-tagged with the Stanford CoreNLP
(Manning et al., 2014) and each token was re-
placed with its lemma and POS tag. Two versions
of the corpora were used for the evaluation: one
raw version with all tokens, and one filtered ver-
sion after removal of stop words and punctuation.

The word embeddings were trained with 300-
dimensional vectors, 10 negative samples per
word and 5 iterations. Very rare words (less
than 100 occurrences in Gigaword, less than 10
in OpenSubtitles) were filtered out. The models
were then evaluated using two standard test work-
flows: Spearman correlation against SimLex-999
semantic similarity dataset (Hill et al., 2015) and
accuracy on the semantic sections of the Google
Analogies Dataset (Mikolov et al., 2013a).

4 Results

All in all, we trained 96 models on Gigaword
(GW) and 96 models on OpenSubtitles (OS)2. Fig-
ure 1 illustrates the results for the SGNS embed-
dings on lexical similarity and analogy tasks using
various types of context windows. The main find-
ings from the experiments are as follows.

Window size
As expected for a lexical similarity task (Schütze
and Pedersen, 1993), narrow context windows per-
form best with the SimLex999 dataset, which con-
tains pairs of semantically similar words (not just
related). For the analogy task, larger context win-
dows are usually beneficial, but not always: the
word embeddings trained on OpenSubtitles per-
form best with the window size of 10, while the
best results on the analogy task for Gigaword are
obtained with the window size of 2.

Window position
Table 2 shows how the position of the context win-
dow influences the average model performance.
Note that symmetric windows of, for instance, 10
are in fact 2 times larger than the ‘left’ or ‘right’

2Encompassing different values of window size, weight-
ing scheme, window position, cross-sentential boundaries
and stop-words removal (4×2×3×2×2 = 96).

Window position SimLex999 Analogies

OS left 0.40 0.35
OS right 0.43 0.35
OS symmetric 0.43 0.45

GW left 0.43 0.64
GW right 0.44 0.65
GW symmetric 0.45 0.68

Table 2: Average performance across all models
depending on the window position.

windows of the same size, as they consider 10
words both to the left and to the right of the focus
word. This is most likely why symmetric windows
consistently outperform ‘single-sided’ ones on the
analogy task, as they are able to include twice as
much contextual input.

However, the average performance on the se-
mantic similarity task (as indicated by the Spear-
man correlation with the SimLex999 test set) does
not exhibit the same trend. ‘Left’ windows are in-
deed worse than symmetric ones, but ‘right’ win-
dows are on par with the symmetric windows for
OpenSubtitles and only one percent point behind
them for Gigaword. It means that in many cases
(at least with English texts) taking into account
only n context words to the right of the focus word
is sufficient to achieve the same performance with
SimLex999 as by using a model which addition-
ally considers n words to the left, and thus requires
significantly more training time.

Cross-sentential contexts
The utility of cross-sentential contexts depends on
several covariates, most importantly the type of
corpus and the nature of the evaluation task. For
similarity tasks, cross-sentential contexts do not
seem useful, and can even be detrimental for large
window sizes. However, for analogy tasks, cross-
sentential contexts lead to improved results thanks
to the increased window it provides. This is espe-
cially pronounced for corpora with short sentences
such as OpenSubtitles (see Table 3).

Weighting scheme
Our experimental results show that none of the two
evaluated weighting schemes (with weights that
decrease respectively linearly or with the square-
root of the distance) gives a consistent advantage
averaged across all models. However, the squared
weighting scheme is substantially slower (as it

286



1 2 5 10
0.30

0.35

0.40

0.45

0.50

S
im

Le
x

(S
p
e
a
rm

a
n
 c

o
rr

.) corpus = Gigaword, unfiltered

1 2 5 10

corpus = Gigaword, filtered

1 2 5 10

corpus = OpenSubtitles, unfiltered

1 2 5 10

corpus = OpenSubtitles, filtered

window position
left

right

symmetric

1 2 5 10
0.30

0.35

0.40

0.45

0.50

S
im

Le
x

(S
p
e
a
rm

a
n
 c

o
rr

.)

1 2 5 10 1 2 5 10 1 2 5 10

distance measure
sqrt

linear

1 2 5 10

window size

0.25
0.30
0.35
0.40
0.45
0.50

S
im

Le
x

(S
p
e
a
rm

a
n
 c

o
rr

.)

1 2 5 10

window size
1 2 5 10

window size
1 2 5 10

window size

cross-sentential
False

True

1 2 5 10
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8

G
o
o
g
le

 A
n
a
lo

g
y corpus = Gigaword, unfiltered

1 2 5 10

corpus = Gigaword, filtered

1 2 5 10

corpus = OpenSubtitles, unfiltered

1 2 5 10

corpus = OpenSubtitles, filtered

window position
left

right

symmetric

1 2 5 10
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8

G
o
o
g
le

 A
n
a
lo

g
y

1 2 5 10 1 2 5 10 1 2 5 10

distance measure
sqrt

linear

1 2 5 10

window size

0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8

G
o
o
g
le

 A
n
a
lo

g
y

1 2 5 10

window size
1 2 5 10

window size
1 2 5 10

window size

cross-sentential
False

True

Figure 1: Results for the SGNS word embeddings trained with various types of context windows.

Cross-sentential SimLex999 Analogies

OS False 0.44 0.34
OS True 0.40 0.43

GW False 0.44 0.66
GW True 0.44 0.65

Table 3: Average performance across all models
with and without cross-sentential contexts.

increases the number of context words to con-
sider for each focus word), decreasing the train-
ing speed about 25% with window size 5. Thus,
the original linear weighting scheme proposed in
(Mikolov et al., 2013b) should be preferred.

Stop words removal
As shown in Table 4, the removal of stop words
does not really influence the average model perfor-
mance for the semantic similarity task. The anal-
ogy task, however, benefits substantially from this
filtering, for both corpora. Although not shown in
the table, filtering stop words also significantly de-
creases the size of the corpus, thereby reducing the
total time needed to train the word embeddings.

Stop words removal SimLex999 Analogies

OS no removal 0.41 0.34
OS with removal 0.42 0.43

GW no removal 0.44 0.64
GW with removal 0.44 0.68

Table 4: Average performance across all models
depending on the removal of stop words.

5 Conclusion

Our experiments demonstrate the importance of
choosing the right type of context window when
learning word embedding models. The two most
prominent findings are (1) the positive role of
cross-sentential contexts and (2) the fact that, at
least for English corpora, right-side contexts seem
to be more important than left-side contexts for
similarity tasks, and achieve a performance com-
parable to that of symmetric windows.

In the future, we wish to extend this study to
the CBOW algorithm, to other weighting schemes
(such as the harmonic series employed by GloVe),
and to non-English corpora.

287



References
Marco Baroni, Georgiana Dinu, and Germán

Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers), pages 238–247. Association for Computa-
tional Linguistics.

Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137–1155.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

Raquel Fernández. 2006. Non-Sentential Utterances
in Dialogue: Classification, Resolution and Use.
Ph.D. thesis, King’s College London.

Yoav Goldberg. 2016. A primer on neural network
models for natural language processing. Journal of
Artificial Intelligence Research, 57:345–420.

Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics, 41(4):665–695.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer.
2016. Neural architectures for named entity recog-
nition. In Proceedings of the 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 260–270, San Diego, California,
June. Association for Computational Linguistics.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2014, pages 302–308.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics, 3:211–225.

Chu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori
Levin. 2015. Unsupervised pos induction with word
embeddings. In Proceedings of the 2015 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 1311–1316, Denver, Colorado,
May–June. Association for Computational Linguis-
tics.

P. Lison and J. Tiedemann. 2016. Opensubtitles2016:
Extracting large parallel corpora from movie and TV
subtitles. In Proceedings of the 10th International
Conference on Language Resources and Evaluation
(LREC 2016), pages 923–929.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed represen-
tations of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English gigaword fifth edi-
tion, June. LDC2011T07.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Hinrich Schütze and Jan Pedersen. 1993. A vector
model for syntagmatic and paradigmatic relatedness.
In Proceedings of the 9th Annual Conference of the
UW Centre for the New OED and Text Research,
pages 104–113.

Shyam Upadhyay, Manaal Faruqui, Chris Dyer, and
Dan Roth. 2016. Cross-lingual models of word
embeddings: An empirical comparison. In Proc. of
ACL.

Jie Zhou and Wei Xu. 2015. End-to-end learning of
semantic role labeling using recurrent neural net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1127–1137, Beijing, China, July. Association
for Computational Linguistics.

288


