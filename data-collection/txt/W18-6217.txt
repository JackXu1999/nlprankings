



















































Aspect Based Sentiment Analysis into the Wild


Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 116–122
Brussels, Belgium, October 31, 2018. c©2018 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17

116

Aspect Based Sentiment Analysis into the Wild

Caroline Brun, Vassilina Nikoulina∗
firstname.lastname@naverlabs.com

Naver Labs Europe, 6 chemin de Maupertuis, 38240 Meylan

Abstract

In this paper, we test state-of-the-art Aspect
Based Sentiment Analysis (ABSA) systems
trained on a widely used dataset on actual data.
We created a new manually annotated dataset
of user generated data from the same domain
as the training dataset, but from other sources
and analyse the differences between the new
and the standard ABSA dataset. We then anal-
yse the results in performance of different ver-
sions of the same system on both datasets. We
also propose light adaptation methods to in-
crease system robustness.

1 Introduction

The aim of Aspect Based Sentiment Analysis
(ABSA) is to detect fine-grained opinions ex-
pressed about different aspects of a given entity,
on user-generated comments.

Aspects are attributes of an entity, e.g. the
screen of a cell phone, the service for a restau-
rant, or the picture quality of a camera, and can be
described by an ontology associated to the entity.
ABSA includes therefore to identify aspects of an
entity, and the sentiment expressed by the writer
of the comment about different aspects. For exam-
ple, from a sentence extracted from a review about
a museum, an ABSA system could extract the fol-
lowing information: This museum hosts remark-
able collections, however, prices are quite high
and the attendants are not always friendly.
”collections”: aspect=museum#collection, polarity=positive;

”prices”: aspect=museum#price, polarity=negative;

”attendants”: aspect=museum#service, polarity=negative;

ABSA receives now a specific interest from the
scientific community, especially with the SemEval
dedicated challenges, (Pontiki et al., 2014), (Pon-
tiki et al., 2015), (Pontiki et al., 2016), that pro-
vided a framework to design and evaluate ABSA

∗Both authors contributed equally.

systems, for different domains, initially on English
but for 8 languages in the 2016 (last) edition. Be-
sides SemEval, other challenges focussing on the
task have been also launched recently, for exam-
ple TASS, dedicated to Spanish, (Villena-Román
et al., 2015b), (Villena-Román et al., 2015a),
(Cumbreras et al., 2016), or GermanEval, dedi-
cated to German ABSA, (Wojatzki et al., 2017).

Following this particular interest, the technol-
ogy performing ABSA becomes more and more
mature, however, experiments and evaluation are
restricted to a small number of academic datasets,
in relatively favorable settings. The goal of this
paper is to test a state-of-the-art ABSA system on
actual data, to evaluate the performance loss in
real-world application conditions, and to experi-
ment potential solutions to it. To achieve this goal,
we’ve created a new ABSA annotated dataset, de-
veloped on Foursquare data. We also performed
evaluation of the full ABSA processing chain (as
opposed to sub-tasks evaluation which is tradition-
ally performed). We also propose a weakly super-
vised method for aspect-based lexical acquisition
designed to improve the robustness of our initial
system.

2 Related Work

Most of the systems dedicated to ABSA use ma-
chine learning algorithms such as SVMs (Wag-
ner et al., 2014; Kiritchenko et al., 2014), or
CRFs (Toh and Wang, 2014; Hamdan et al., 2015),
which are often combined with semantic lexi-
cal information, n-gram models, and sometimes
more fine-grained syntactic or semantic informa-
tion. For example, (Kumar et al., 2016) proposed
a very efficient system on different languages of
SemEval2016. The system use information ex-
tracted from dependency graphs and distributional
thesaurus learned on the different domains and



117

languages of the challenge. Deep Learning meth-
ods are also emerging: for example, (Ruder et al.,
2016) proposed a method using multiple filters
CNNs and obtained competitive results on both
polarity and aspect detection tasks. However,
ABSA datasets are very costly to annotate by hu-
mans, and they are usually small, which is a prob-
lem for Deep Learning supervised methods.

3 Datasets

Usually, ABSA systems are tested on the same
dataset as they are developped on. One of the
widely used ABSA datasets was released in Se-
meval2016 challenge (Pontiki et al., 2016), in par-
ticular the dataset for restaurant domain. It is
based on the dataset of (Ganu et al., 2009) who ex-
tracted restaurant reviews from City Search New
York over year 2006. Since then, the notion of the
user review has evolved. Many factors may im-
pact the linguistic structure of a review, e.g. the
support it was written on (computer vs. smart-
phone), the age of the user, the location (US vs.
UK English), the user mother tongue (native vs.
non-native speakers), etc. How would a system
trained on Semeval2016 dataset perform on a new
data coming from different sources?

In order to assess ABSA real-world perfor-
mances, we manually annotated a completely new
dataset from Foursquare1 comments. We have ac-
cess to about 215K user reviews of restaurants all
over the world in English2. The reviews were writ-
ten during the period between 2009 to 2018. From
these reviews, we randomly selected 585 sam-
ples, which contain 1006 sentences and annotate
these sentences with the SemEval2016 annotation
guidelines for the restaurant domain. The annota-
tions have been performed by a single annotator,
expert linguist with a very good knowledge of the
SemEval2016 annotation guidelines, using BRAT,
(Stenetorp et al., 2012).

Each sentence contains annotations about:
1. Opinion Target Expression (OTE), i.e. the lin-
guistic expression (term) used in the text to re-
fer to the reviewed entity, annotated as “NULL”
if the aspect is implicit; 2. Aspect Categories,
i.e. the semantic categories of the opinionated as-
pects, which are part of a predefined ontology (12
semantic classes for the restaurant domain from

1https://foursquare.com/
2Countries with most of English comments include US,

UK, Australia, Canada, Indonesia, Malaysia, Philippines, In-
dia, Thailand

Dataset #Rev #S #W/S #A/S
Semeval 92 676 12.8 1.27

Foursquare 585 1006 8.0 1.15

Table 1: Dataset statistics: Semeval 2016 test set and
Foursquare dataset. #Rev: number of reviews, #S:
number of sentences, #W/S : number of words per
sentence, #A/S: number of <OTE, Aspect Category,
Polarity> tuples per sentence

(Pontiki et al., 2016)); 3. Sentiment Polarities: po-
larities (positive, negative or neutral) associated to
the tuple <OTE, Aspect Category>. An illustra-
tion of such annotation is given on figure 1.

<text>Their sake list was extensive,
but we were looking for Purple Haze,
which wasn’t listed but made for us
upon request!</text>
<Opinions>
<Opinion target="sake list"

category="DRINKS#STYLE_OPTIONS"
polarity="positive"/>

<Opinion target="NULL"
category="SERVICE#GENERAL"
polarity="positive"/>

</Opinions>

Figure 1: ABSA: an annotated sentence from the
Semeval-2016 training corpus

Table 1 gives some statistics about the
Foursquare and Semeval2016 datasets. One may
notice, that in average, Foursquare reviews are
shorter and therefore contain less aspects per sen-
tence. We believe this is due to the generalisation
of smart-phones (and other mobile devices) usage
over the world in the last decade, which influenced
the way users write. We release the Foursquare
dataset to the community in order to better assess
robustness of ABSA systems3.

4 Evaluation Procedure

We consider different evaluation measures. First,
we re-use the SemEval2016 ABSA evaluation
paradigm and scripts, where the evaluation was
run in two phases, phase A and phase B. In phase
A, raw reviews have to be annotated with aspects
(slot 1 of the challenge) and OTE (slot 2 of the
challenge). In phase B, gold annotations for phase
A, i.e. tuples <OTE, aspect>, have to be an-
notated with polarities (slot 3 of the challenge).

3http://www.europe.naverlabs.com/Research/Natural-
Language-Processing/Aspect-Based-Sentiment-Analysis-
Dataset



118

Thus, we evaluate separately the OTE detection,
aspect detection and finally, we evaluate the po-
larity of opinion detection on the ground truth of
phase A. The advantage of this evaluation proce-
dure is of course to assess the quality of the sys-
tems on each of the different subtasks involved
in the full ABSA system. However, these mea-
sures do not reflect the overall results such sys-
tems would obtain on the full chain of annotations
starting from raw data, in end-to-end application
settings. Therefore, we also propose to evalu-
ate the results obtained with the complete anno-
tation chain, i.e. computing F1-measure on the
triplets <OTE, Aspect, Polarity>. In addition,
we compute the F1-measure on the pairs <Aspect,
Polarity> at sentence level. This last measure can
be useful to assess ABSA general Aspect-Polarity
performance since many ABSA applications may
not require the OTE step. In what follows, we refer
to these measures as slot1,3 and slot1,2,3 to make
connection with the challenge tasks.

5 Baseline ABSA Systems

In our experiments, we use several baseline sys-
tems. Each of the systems consists in the follow-
ing pipeline of different components: 1. Opinion-
ated domain term extraction (OTE); 2. aspect cat-
egorization, for opinionated term (OT), and whole
sentence level; final aspect is predicted as a com-
bination of both; 3. polarity classification of each
aspect identified in the previous step. The differ-
ence between baselines lies in the implementation
of each component of the pipeline, and the level of
external resources involved.

5.1 Baseline-1

The first system is resource-rich system relying
on available syntactic and semantic parser, and
domain-specific semantic lexicons. It is based
on composite models combining sophisticated lin-
guistic features with machine learning algorithms.
The linguistic features are extracted via a NLP
pipeline (based on in-house parser) comprising
lexical semantic information, POS tagging, syn-
tactic parsing and a partial semantic parsing that
outputs semantic relations between polarity pred-
icates and their opinionated targets (OTE). These
linguistic features are then used by classifiers to
perform each step of the pipeline.

The OTE detection is performed with Condi-

tional Random Fields (implemented with CRF++4

toolkit), trained with some standard features (POS,
lemma, presence of upper-case letters, features
combining syntactic/semantic dependencies with
semantic lexicons, embedding-based features).

Aspect and polarity classification components
rely on the same features as for OTE, exclud-
ing embedding-based features, but extended with
bi-grams features. In addition, polarity classi-
fier feature representation is extended with entity
and attribute of aspect category (e.g. RESTAU-
RANT#PRICES results in two additional features:
(restaurant, prices)). Classification is performed
with CoreNLP (Manning et al., 2014) implemen-
tation of Maximum Entropy.

5.2 Baseline-2

The second baseline system (baseline-2) replaces
each component of the previous pipeline with neu-
ral network classifiers. Aspect classification and
polarity classification components are based on
multiple filters CNNs as in (Ruder et al., 2016).
OTE component is based on Bidirectional GRU
architecture (similar to (Jebbara and Cimiano,
2016)). All the components are implemented with
the keras (Chollet et al., 2015) library.

Since the size of the training data is relatively
small, we attempt to enrich an input with prior
knowledge to help the system to generalize bet-
ter. In order to do so, we enrich word representa-
tion with semantic lexicon features5, which are en-
coded as one-hot vector of dimension 100 and con-
catenated with word embedding. These new word
representations are fed to the same pipeline as
baseline-2. We’ll refer to this system as baseline-
2’.

Both baseline-2 and baseline-2’ are initialised
with pre-trained word embeddings.

5.3 Baseline Results

Common ressources between all baselines are pre-
trained word embeddings and semantic lexicon.
We use word2vec (Mikolov et al., 2013) 300-
dimensional Google News word embeddings, on
which some“noise” filtering has been performed.
Semantic lexicon was created semi-automatically
using existing polarity lexicons and capitalizing on
the annotated vocabulary present in the SemEval

4https://taku910.github.io/crfpp/
5This is close to the idea of sentic features (Jebbara and

Cimiano, 2016), integrating aspect categories and polarities,
rather than sentics.



119

Model Foursquare
s2 s1 s3 s1,3 s1,2,3

baseline-1 68.9 63.8 88.7 56.9 33.6
baseline-2 47.9 62.9 86.0 52.5 9.1
baseline-2’ 47.7 62.7 86.1 52.6 8.8

Semeval
baseline-1 75.3 70.4 87.3 63.0 37.1
baseline-2 61.1 69.9 80.2 54.9 12.0
baseline-2’ 61.0 68.8 78.7 53.8 11.8

Table 2: Performance of various baseline systems.
s1: Aspect Category detection (F1), s2: Opinion
Target Expression (F1), s3: Sentiment Polarity (Ac-
curacy). s1,3: Aspect,Polarity (F1), s1,2,3: As-
pect,OTE,Polarity (F1).

ABSA datasets. It contains ∼1000 words with as-
pect categories and/or polarities associated to each
word.

Results for all the baselines are summarized in
the table 2. Note, that for baseline-2,2’, we report
an average performance after executing the whole
pipeline 10 times.

First, we observe an important performance
drop in aspect prediction (tasks s2, s1) for the new
Foursquare dataset for both baselines. This is of
course related to the fact that this dataset is differ-
ent from the one the training has been performed
on. Thus, the aspects may not be expressed in
the same way, style of the reviews are different6.
However, for polarity prediction we observe bet-
ter results on Foursquare dataset than on Semeval
dataset. It can be explained by shorter length
of Foursquare comments, resulting in less aspect
mentions per sentence (rarely more that one opin-
ionated term per sentence), and thus less ambigu-
ity in polarity prediction.

The second observation is a pretty low overall
pipeline performance (s1,3 and s1,2,3). Although
our baseline-1 has pretty good performances on
each individual task (best, or close to best official
SemEval2016 results) when putting all together, it
results in 63.0 F1-score on aspect-polarity tuples.
The performance on <OTE, Aspect, Polarity> tu-
ples drops down to F1 of 37.1. This evaluation
procedure allows us to get an idea on what would
be “real-world” system performance, and also in-
dicates the capacities and limitations of the sys-
tem.

6a lot of emojis are used in Foursquare dataset, but not in
Semeval dataset

Finally, we note that baseline-1 (“ressource
rich” baseline) has the best performances from all
the baselines we explored (as expected). The per-
formances of baseline-2 and baseline-2’ are pretty
close on the Semeval dataset, but baseline-2 seems
to perform slightly better.

6 Exploring Additional Ressources for
Adaptation

One of the natural resources to explore for system
adaptation is a set non-annotated reviews. In our
case, we exploit all Foursquare reviews in English
we have access to.

6.1 Domain Specific Embeddings

First, we learn domain dependent words embed-
dings (300-dimensional) on the Foursquare restau-
rant data using Gensim (Řehůřek and Sojka, 2010)
implementation of word2vec. We filtered out the
words occurring less than 5 times, and used a con-
text window of 10 words, which resulted in 60K
word embeddings.

6.2 Weakly Supervised Lexical Acquisition

Among other components, our system relies on
semantic lexical ressources encoding domain as-
pect and polarity vocabulary, that were developed
semi-automatically, based on SemEval2016 train-
ing datasets. In order to enrich these lexicons,
we have adapted a semantic clustering method de-
scribed in (Pelevina et al., 2016)7. The core idea
of this approach is to induce a sense inventory
from existing word embeddings via clustering of
ego-networks of related words. An ego network
consists of a single node (ego) together with the
nodes they are connected to and the edges between
the connected nodes. Words referring to the same
sense tend to have a large number of connections,
and to be clustered together. The clustering is done
with the Chinese Whispers algorithm (Biemann,
2006).

In the case of the present experiments, we ini-
tialize the algorithm with a set of seed words to-
gether with their semantic aspect (e.g cider:drink,
tikka:food), in order to obtain clusters of aspect
words. We used 60 seed words randomly selected
from our existing semantic lexicon and learned
clusters from Foursquare embeddings. Table 4

7This method was initially experimented for word sense
disambiguation, but we directly adapted it for domain aspect
lexicon creation



120

Model Foursquare Semeval
s2 s1 s3 s1,3 s1,2,3 s2 s1 s3 s1,3 s1,2,3

baseline-1
baseline-1 68.9 63.8 88.7 56.9 33.6 75.3 70.4 87.3 63.0 37.1
f lex 69.2 64.1 88.8 57.1 33.8 76.4 70.4 86.6 63.5 38.1
f emb 66.7 63.8 88.7 57.3 34.1 75.3 70.5 87.1 63.4 37.4
f lex + f emb 67.1 64.3 88.8 57.3 33.9 75.8 70.7 86.6 63.5 37.7

baseline-2
baseline-2 47.9 62.9 86.0 52.5 9.1 61.1 69.9 80.2 54.9 12.0
f emb 54.5 66.4 87.1 56.7 9.1 61.7 69.7 80.6 54.7 11.3

baseline-2’
baseline-2’ 47.7 62.7 86.1 52.5 8.8 61.1 68.8 78.7 53.8 11.8
f lex 47.7 62.4 86.1 52.6 8.7 61.0 68.9 78.7 53.9 11.4
f emb 53.8 65.9 86.7 56.2 9.2 62.4 70.0 80.5 55.8 11.4
f lex + f emb 53.8 65.8 86.7 56.2 9.2 62.4 69.9 80.5 55.8 11.4

Table 3: Experimental results with foursquare embeddings and automatically acquired lexicon

Seed:Aspect Aspect Cluster
kimchi:food kimchee, bulgogi, galbi,

bibimbap, jigae, chigae, ...
waiter:service waitress, server, hostess,

nikki, melissa, kyle, kelly, ...
expensive:price over-priced, pricey, costly

pricy, cheap, spendy, ...

Table 4: Clusters learnt on Foursquare embeddings

gives some cluster examples. It’s interesting to ob-
serve that we obtain a cluster of first names, often
used to mention a waiter in Foursquare data, with
semantic class service.

We use these clusters of aspect words by con-
catenating them to the existing lexicon of the sys-
tem.

6.3 Experimental Results

We’ve performed following series of experiments
(summarized in table 3): 1. f lex: foursquare
lexicon extending existing lexicon (for systems
using lexicons); 2. f emb: all baselines with
foursquare embeddings replacing generic embed-
dings (GoogleNews-based) 3. f lex +f emb: com-
bination of the previous two. We observe light
improvements for baseline-1 which are especially
due to lexicon enrichment experiments. We
think that Foursquare embeddings didn’t bring
expected improvements for baseline-1 (embed-
dings are used only for OTE/s2 task, which in
it’s turn impacts s1 task), mostly because these

embeddings are much smaller and we lose some
non domain-specific knowledge when they replace
GoogleNews embeddings.

The impact of embedding is opposite for
baseline-2 experiments. Foursquare pretrained
embeddings bring important gains on Foursquare
dataset thus moving baseline-2 system above
baseline-1 for s1 evaluation. It also improves
(although less) system performance on Semeval
dataset. Automatically acquired lexicon on
baseline-2 systems seems to be very low. We plan
to explore other ways to integrate this knowledge
into deep learning framework.

7 Conclusion

In this work, we release a new ABSA dataset,
in order to better assess state-of-the-art systems
robustness; we also evaluate a full ABSA chain
of various systems, to reflect end-to-end perfor-
mances. We show that even for the systems with
good performances on individual ABSA subtasks,
an overall aspect/polarity F1 score drops down
to 63.0. Evaluation of various baselines on the
new dataset have shown that standard ABSA sys-
tems may suffer a significant decrease in perfor-
mance, especially for aspect detection. We’ve ex-
perimented with light adaptation methods integrat-
ing in-domain embeddings and automatically ac-
quired lexicons, and showed their impact on dif-
ferent systems. Both the new Foursquare ABSA
dataset and the evaluation script of the full pipeline
are distributed with the paper.



121

References
Chris Biemann. 2006. Chinese whispers: An efficient

graph clustering algorithm and its application to nat-
ural language processing problems. In Proceed-
ings of the First Workshop on Graph Based Meth-
ods for Natural Language Processing, TextGraphs-
1, pages 73–80, Stroudsburg, PA, USA. Association
for Computational Linguistics.

François Chollet et al. 2015. Keras. https://
keras.io.

Miguel Ángel Garcı́a Cumbreras, Julio Villena-Román,
Eugenio Martı́nez Cámara, Manuel Carlos Dı́az-
Galiano, Maria Teresa Martı́n-Valdivia, and Luis Al-
fonso Ureña López. 2016. Overview of TASS 2016.
In Proceedings of TASS 2016: Workshop on Senti-
ment Analysis at SEPLN co-located with 32nd SE-
PLN Conference (SEPLN 2016), Salamanca, Spain,
September 13th, 2016., pages 13–21.

G. Ganu, N. Elhadad, and A. Marian. 2009. Beyond
the stars: Improving rating predictions using review
text content. In Proceedings of the 12th Interna-
tional Workshop on the Web and Databases, Prov-
idence, Rhode Island.

Hussam Hamdan, Patrice Bellot, and Frederic Bechet.
2015. Lsislif: Crf and logistic regression for opinion
target extraction and sentiment polarity analysis. In
Proceedings of the 9th International Workshop on
Semantic Evaluation (SemEval 2015), pages 753–
758, Denver, Colorado. Association for Computa-
tional Linguistics.

Soufian Jebbara and Philipp Cimiano. 2016. Aspect-
Based Sentiment Analysis Using a Two-Step Neural
Network Architecture. In Semantic Web Challenges.
Third SemWebEval Challenge at ESWC 2016. Re-
vised Selected Papers, volume 641, pages 153–170.
Springer.

Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and
Saif Mohammad. 2014. Nrc-canada-2014: Detect-
ing aspects and sentiment in customer reviews. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014), pages 437–
442, Dublin, Ireland. Association for Computational
Linguistics and Dublin City University.

Ayush Kumar, Sarah Kohail, Amit Kumar, Asif Ek-
bal, and Chris Biemann. 2016. Iit-tuda at semeval-
2016 task 5: Beyond sentiment lexicon: Combin-
ing domain dependency and distributional seman-
tics features for aspect based sentiment analysis. In
Proceedings of the 10th International Workshop on
Semantic Evaluation (SemEval-2016), pages 1129–
1135, San Diego, California. Association for Com-
putational Linguistics.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of the 26th International Con-
ference on Neural Information Processing Systems -
Volume 2, NIPS’13, pages 3111–3119.

Maria Pelevina, Nikolay Arefyev, Chris Biemann, and
Alexander Panchenko. 2016. Making sense of word
embeddings. In Proceedings of the 1st Workshop on
Representation Learning for NLP, pages 174–183.

Maria Pontiki, Dimitrios Galanis, Haris Papageor-
giou, Ion Androutsopoulos, Suresh Manandhar, Mo-
hammad AL-Smadi, Mahmoud Al-Ayyoub, Yanyan
Zhao, Bing Qin, Orphée De Clercq, Véronique
Hoste, Marianna Apidianaki, Xavier Tannier, Na-
talia Loukachevitch, Evgeny Kotelnikov, Nuria Bel,
Salud Marı́a Jiménez-Zafra, and Gülşen Eryiǧit.
2016. SemEval-2016 task 5: Aspect based senti-
ment analysis. In Proceedings of the 10th Interna-
tional Workshop on Semantic Evaluation, SemEval
’16, San Diego, California. Association for Compu-
tational Linguistics.

Maria Pontiki, Dimitris Galanis, Haris Papageorgiou,
Suresh Manandhar, and Ion Androutsopoulos. 2015.
Semeval-2015 task 12: Aspect based sentiment anal-
ysis. In Proceedings of the 9th International Work-
shop on Semantic Evaluation (SemEval 2015), pages
486–495, Denver, Colorado. Association for Com-
putational Linguistics.

Maria Pontiki, Dimitris Galanis, John Pavlopoulos,
Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4: As-
pect based sentiment analysis. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014), pages 27–35, Dublin, Ireland. As-
sociation for Computational Linguistics and Dublin
City University.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45–50, Val-
letta, Malta. ELRA. http://is.muni.cz/
publication/884893/en.

S. Ruder, P. Ghaffari, and J. G. Breslin. 2016.
INSIGHT-1 at SemEval-2016 Task 5: Deep Learn-
ing for Multilingual Aspect-based Sentiment Analy-
sis. ArXiv e-prints.

Pontus Stenetorp, Sampo Pyysalo, Goran Topi,
Tomoko Ohta, and Sophia Ananiadou. 2012. Brat: a
web-based tool for nlp-assisted text annotation. In
Proceedings of the Association for Computational
Linguistics (ACL), pages 102–107.

Zhiqiang Toh and Wenting Wang. 2014. Dlirec: As-
pect term extraction and term polarity classifica-
tion system. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval
2014), pages 235–240, Dublin, Ireland. Association

https://keras.io
https://keras.io
http://is.muni.cz/publication/884893/en
http://is.muni.cz/publication/884893/en


122

for Computational Linguistics and Dublin City Uni-
versity.

Julio Villena-Román, Janine Garcı́a-Morera, Miguel
Ángel Garcı́a Cumbreras, Eugenio Martı́nez-
Cámara, Maria Teresa Martı́n-Valdivia, and Luis
Alfonso Ureña López. 2015a. Overview of TASS
2015. In Proceedings of TASS 2015: Workshop on
Sentiment Analysis at SEPLN co-located with 31st
SEPLN Conference (SEPLN 2015), Alicante, Spain,
September 15, 2015., pages 13–21.

Julio Villena-Román, Eugenio Martı́nez-Cámara, Ja-
nine Garcı́a-Morera, and Salud M. Jiménez Zafra.
2015b. TASS 2014 - the challenge of aspect-based
sentiment analysis. Procesamiento del Lenguaje
Natural, 54:61–68.

Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. Dcu: Aspect-based polarity
classification for semeval task 4. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014), pages 392–397, Dublin, Ire-
land. Association for Computational Linguistics and
Dublin City University.

Michael Wojatzki, Eugen Ruppert, Sarah Holschnei-
der, Torsten Zesch, and Chris Biemann. 2017. Ger-
mEval 2017: Shared Task on Aspect-based Senti-
ment in Social Media Customer Feedback. In Pro-
ceedings of the GermEval 2017 Shared Task on
Aspect-based Sentiment in Social Media Customer
Feedback, pages 1–12, Berlin, Germany.


