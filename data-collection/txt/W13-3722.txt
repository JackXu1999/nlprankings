



















































Why So Many Nodes?


Proceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages 197–206,
Prague, August 27–30, 2013. c© 2013 Charles University in Prague, Matfyzpress, Prague, Czech Republic

 
 

Why so many nodes? 
Dan Maxwell 

US Chess Center 
Washington DC, USA 
dan.n.maxwell@gmail.com

         
Abstract 

This paper provides an analysis of the 
representation of various grammatical phenomena 
in both constituency structure and dependency 
structure (hereafter c-structure and d-structure), 
including agreement, case marking, and word order 
in transitive sentences, as well as three theoretical 
constructs, and the interface between the form of a 
sentence and its meaning. There is a crucial 
structural relationship for all of these phenomena in 
both constituency grammar and dependency 
grammar, but only the version used in dependency 
grammar is fully reliable. This insight evokes the 
following question: Why do linguists working with 
constituency grammars think that so many nodes 
are necessary?  Upon examination, the extra nodes 
succeed only in confusing our understanding of 
syntactic phenomena. 

1    Introduction 
The obvious difference between constituency 
grammar and dependency grammar (for which 
the seminal work is Tesnière 1959) is that the 
former has more nodes due to the distinction it 
makes between lexical and phrasal categories. 
As first discussed in Hudson 1984:94-95, this 
distinction has an important consequence: 
nodes which are related to each other are 
directly connected to each other in dependency 
grammar, but only indirectly in constituency 
grammar. That is, they are parent and child in 
dependency grammar but at best siblings (the 
gender-neutral version of the more commonly 
used ‘sisters’) in a one-bar constituency 
grammar. A further problem is that the  
constituency grammars often use a system of 
two bars or more rather than a one-bar system. 
This contribution examines this fundamental 
difference and its consequences for the 
treatment of several kinds of linguistic 
phenomena and theoretical constructs to be 
treated in more detail below.  
 
2  Agreement 
 
In many languages, the form of one word 
varies according to the form of some other 
word in the same construction. These 

relationships include subject and verb, (more 
rarely) object and verb, noun and adjective, 
and noun and determiner. Highly inflected 
languages like Russian have all of these kinds 
of agreement. English has only two of them 
(and then only to a limited extent). So-called 
isolating languages like Vietnamese do not 
show agreement at all.  The agreement can be 
in person, number, gender, and (for case-
inflecting languages only) case.  

English has agreement between the subject 
and the verb in the third person singular of the 
present tense, as shown by the contrast 
between (1a) on the one hand and (1b) and (1c) 
on the other: 

(1) a. John walks slowly.  
b. John and Sarah walk quickly. 
c. I walk more quickly than they do. 

The c-structures of these sentences assume that 
the subject NP (which is a parent of the noun 
which determines the form of the verb) is a 
sibling of the parent VP of the verb, so the 
structural relationship between this noun and 
this verb is something more complicated and 
indirect than that shown in dependency 
grammar and apparently varies according to 
the precise details of the construction or 
perhaps the system of X-bar theory 
(Jackendoff 1977) chosen by the grammarian, 
e.g. 

(2)     S 

  NP       VP 

    N     V       Adv 

  John  walks      slowly 

Neither the subject NP nor its child, the N 
John is a sibling of V, although this is the word 
with which the subject agrees.  

In dependency grammar in contrast, there is 
of course neither an NP node nor a VP node. 
The verb is simply the parent of the noun, 
whatever kind it is. The nodes for these two 
words contain both the phonological 
information associated with them and, in the 

197



 
 

semantics, the associated words that make up 
the phrase of the same category. 

(3)     V: 1walks 

  Name: John    Adv: slowly 

          

The V walks is the parent of the name John.  
Agreement in number also occurs in English 

between common nouns and demonstrative 
determiners, as shown in the contrast between 
(4a) and (4b):  

(4) a. this/that student 

b. these/those students 

In the most widely accepted analysis of the 
internal structure of the NP within 
constituency grammar frameworks, the 
determiner is a sibling of the N′, but not of the 
N that it agrees with. This is shown in (5): 

(5)     NP 

    Det       N′ 

         N     PP 

  those stories about him 

those agrees with stories, but the category Det 
of the former is a sibling of N′ rather than the 
category N of the latter. 

In dependency grammar, either the N is the 
parent of the Det, or vice-versa, depending on 
which analysis is applied to the construction 
(NP vs. DP). The corresponding two 
competing treatments are also found in 
constituency grammar analyses. The more 
traditional N-as-head analysis is assumed here: 

 

(6)    

                     N: stories PL 
 
  Det: those PL          Prep: about (him) 

This shows that for the relationship between 
nouns and determiners, as between verbs and 

                                                            
1 Colons are used in this paper to separate a syntactic 
category and its associated phonology at a specific node 
in a dependency tree. On the other hand, the ‘AdvP’ in 
the same tree is used to label a parent node for a 
constituent consisting of more than one word and headed 
by an Adverb.  This corresponds to the use of phrasal 
nodes to abbreviate a series of words in constituent 
grammars, when the internal details of these structures 
are not relevant to the point being made. 

nouns, the source of the features and their 
target are directly connected to each other in 
DG as parent and child. In other words, they 
form a very restricted kind of catena (Osborne 
and Groβ 2012). A consequence of this is that 
the features can be expressed for both 
categories for whatever rule unites them.  

In constituency grammar, on the other hand, 
the trees in (3) and (4) show that the N that is 
the source of the features is not always a 
sibling of the constituent that gets them from 
it. To allow such non-siblings to nevertheless 
get the features from the source noun,  
additional principles have been used. These 
include the ‘head feature convention’ in 
Generalized Phrase Structure Grammar 
(GPSG, Gazdar, Klein, Pullum, and Sag, 
1984), the ‘head feature principle’  in Head-
Driven Phrase Structure Grammar (HPSG, 
Pollard and Sag, 1994)  or  the ‘projection 
principle’ in Principles and Parameters (P&P, 
Chomsky and Lasnik, 1991). Lexical 
Functional Grammar (Bresnan, 2001) has a 
system of up-arrows and down-arrows to pass 
the features from the sibling node of the source 
of the features to the node that needs them. 
Such devices are not necessary in dependency 
grammar. 
 
3     Case 
 
Some languages that inflect for features like 
person, number, and gender, also inflect for 
case. If they inflect for case, they may also 
have agreement in case, as noted in the 
previous section. But how is case assigned in 
the first place?  Unlike for features like person, 
number and gender, case cannot be inherent in 
nouns; it is, rather, clearly determined by the 
syntactic structure of the clause in which the 
nouns occur. More specifically, the case of a 
noun is determined by the verb or preposition 
(or occasionally a combination of both) which 
governs it. Just as with agreement, government 
is determined in dependency grammar by the 
parent-child relationship, but in constituency 
grammar, government is determined by a 
sibling relationship which requires some other 
grammatical device to pass the case feature 
between the word which ultimately gets case 
and the node which is the maximal extension 
of this word, e.g.  

 

198



 
 

(7)          VP                                                                                                                                                                                       

V                      NP         

         PRO[acc] 
    saw     him 

The accusative case of him is assigned by the 
verb saw. The sibling of the verb is the NP, not 
the PRO, so the feature must be passed to the 
PRO by some additional principle. Compare 
(7) with the direct connection between case 
assigner and case recipient in DG, as shown in 
(8): 

(8)    V: saw 
                            
     Pro: I      Pro: him 

The nominative case of I and the case of him 
are both assigned by the parent verb saw. 

4   Non-branching Phrasal Nodes2 
Within a c-structure analysis, a PS rule may 
generate a phrasal node to allow for the 
possibility of a modifier or optional argument. 
But if this option is not taken, then the node 
appears to be superfluous. An example of such 
a situation is shown in (9). 

(9)                    NP 

               Det                 N’ 

                            AP               N 

                             A 

               The        best      students 

The AP(adjective phrase) does not branch.  
APs never branch unless they are modified, for 
example by a degree adverb such as  very .  If 
there were no adjective, then the N’ node 
would likewise not branch.   

Ross (1969) examines several other cases of 
non-branching nodes and suggests that such 
nodes should be removed by a rule of pruning. 
In its most general form, this rule results in all 
non-branching nodes being deleted.  This is in 
fact Ross’ initial proposal, but he goes on to 
find several problems with this due to the kind 
of transformational analysis which was in 
general use at the time of writing.   

No such rule is necessary in d-structure 
grammars, since any node may or may not 

                                                            
2 The essential point of this section is made in Hudson 
2007:118 

branch. Non-branching nodes are clearly not 
superfluous, since each one has phonological 
and/or semantic information (usually both) 
associated with a specific word.  The d-
structure which corresponds to (9) is shown in 
(10): 

(10)                          N: students3   
        
        Det:the   Adj:best 

 
5      Word Oder: General 
 
P&P is the most wide-spread c-structure 
framework and deals with variation of word 
order for a given sentence with transformations 
or some other derivational device. I will not go 
into this here, since such a solution appears to 
imply a distinction between ‘deep structure’ 
and ‘surface structure’. P&P, the related 
Minimalist Program, Meaning-Text-Mapping 
(Mel’čuk 1988) and work within the Prague 
school tradition (Firbas 1992) still make this 
distinction, but both GPSG and HPSG, which 
are also c-structure frameworks, as well as 
Construction Grammar(Goldberg 2006), reject 
such a distinction. The first two of these, at 
least, have a different way of dealing with such 
word order variation. They distinguish 
between hierarchical structure and linear order 
and have separate sets of rules for each of 
these. In cases of discourse driven word order, 
as in the case of Russian (discussed in section 
6) and similar languages¸ this distinction 
would allow them, when they deal with such 
phenomena4, to create linearization rules 
which are sensitive to discourse information 
such as focus and topic. 

But this is only a partial solution. Suppose 
we write linearization rules as follows: N1< 
N2, to mean N1 precedes N2, where N1 and 
N2 are any two nodes of the tree. As it stands, 
this needs to be interpreted. How do different 
rules of this sort interact with each other? 
What happens to the nodes dominated by N1 
and N2?   

 

                                                            
3 The idea of using arrows as branches when the 
dependent is an adjunct is introduced in Osborne and 
Groβ 2009.   
4 Some work along these lines has been done in HPSG, 
notably Murphy (1995). 

199



 
 

(11)                                                                             

        S      

                 
   NP         VP 
            
      V   NP 

     I  like  tea. 

In this tree, the subject NP is the sibling of the 
VP, but not of the V, so no linearization rule 
can be written for the sequence of the Subject 
NP and V. Rather the subject NP must be 
linearized with respect to the VP. The object 
NP, on the other hand, is the sibling of the V, 
so writing a linearization rule is no problem for 
it:  V < NP (V precedes the sibling NP).   

To make the constituency grammar 
approach coherent, it is necessary to interpret 
these nodes as constituents and thereby include 
all the nodes they dominate in the linearization 
process. In the works cited above, their power 
is restricted to applying only to nodes in a 
sibling relationship, for example, between any 
preposition and the sibling NP that it governs, 
or between any verb and its direct object. One 
problem with this restriction is that it does not 
allow us to write a rule which determines the 
linear relationship between the verb and its 
subject, because they are not siblings in the 
tree.  The subject NP is a sibling of the VP, not 
of the verb itself.   

The so-called true tree principle (e.g. 
Schubert 1987:87-90) makes it impossible for 
different parts of a constituent C1 to be split 
apart by another constituent C2 which is not a 
part of C1. The constituent consisting of verb 
and object (the VP), for example, cannot be 
split apart by the subject, since the latter is not 
part of the VP. This has the consequence that 
in a sentence with VSO order, there can be no 
VP, at least not in surface structure, and 
accordingly not at all in any monostratal 
framework. It is therefore necessary to attach 
the verb and its arguments to the S node. This 
makes linearization easier.   

The corresponding dependency grammar 
approach to the problem, shown in tree (12), 
would be to have linearization rules order the 
parent node with each of its child nodes.   

(12)      V: like 

   N: I     N: tea 

The verb is the parent of both the subject N 
and the object N, so the linearization rules can 
say that the verb follows one N and precedes 
the other: Nsu < V < Nob. 

It turns out, however, that for languages like 
English, at least, linearization rules are not 
necessary at all, as has been shown by Groß 
and Osborne 2009 and numerous other publi-
cations. The linear relationship between the 
parent and its child in the D-structure tree can 
be determined by their linear relationship in 
the rule, assuming the rule corresponds closely 
to the resulting tree. The direction of the 
branch which unites parent and child provides 
this linear relationship. This is seen in the 
above tree. 
  Of course, it would be possible to eliminate 
the VP node of c-grammar and attach the verb 
and all its arguments directly to the S node at 
the top of the tree. This is the approach taken 
for all languages discussed in Starosta 1988, 
but has generally found little support within 
generative grammar. This approach solves the 
problem and makes the grammar more like a 
dependency grammar.  

6  Free Word Order 
 
All the data in this section is taken from 
Kallestinova 2007. From now on, this name 
followed by one or more numbers refers to a 
page or pages of this work.  

Russian is one of quite a few languages 
which are often said to allow relatively “free” 
word order.  According to a number of studies 
cited by Kallestinova, this freedom does not 
encode grammatical information, and 
accordingly does not affect truth conditions5. 
What this does mean is best illustrated with an 
example, which is taken from Kallestinova 1-
2: 

(13)  a. Boris navestil Ivana.   SVO 
   Boris-Nom. visited Ivan-Acc.6 

‘Boris visited Ivan.’ 

b. Boris Ivana navestil.   SOV 

c. Ivana navestil Boris.   OVS 

                                                            
5 In case of different word orders affecting the scope of 
adjuncts, truth conditions may be affected, as in the case 
of languages like English, but this is not the type of 
phenomenon under discussion here.  
6 I use the standard abbreviations for case-endings: acc. 
= accusative, instr. = instrumental, nom. =nominative 

200



 
 

d. Ivana Boris navestil.   OSV 

e. Navestil Boris Ivana.   VSO 

f. Navestil Ivana Boris.   VOS 

These sentences show that the same idea can 
be expressed by more than one arrangement of 
a fixed set of words, in this case, there are 
three words in the set, and they can be 
arranged in any one of the logically possible 
six sequences of the Subject (S), Verb (V), and 
Object (O).   

Furthermore, the different positions of the 
sentence are used to distinguish new 
information or Foc(us) from old information, 
or the Top(ic). Old information goes at the 
beginning of the sentence, and new 
information at the end. For example, a subject-
focus answer using a non-emotive strategy7 
requires an order with the subject at the end of 
the sentence, that is, either OVS or VOS. In a 
sentence using the non-emotive strategy, the 
only word order with a constituent other than 
the focused one in sentence final position to be 
found in significant numbers is SVO. The 
results of the perception experiment show, 
however, that one of the two orders found in 
the non-emotive strategy is distinctly preferred 
over the other.  So in response to the question 
in (14a), (14b) is strongly preferred to (14c).   

(14) a.  Kto gryzjot kapustu?  

Who bite     cabbage-acc. 
‘Who is biting the cabbage?’                         

b.  Kapustu    gryzjot   zajac.  
cabbage-acc  bite    rabbit-nom 

  ‘The rabbit is biting the cabbage.’ 

c.  ?Gryzjot kapustu zajac. 

This shows that it is possible to place any of 
the major constituents of the verb at the end of 
the sentence, since any of these constituents 
can be the answer to such a question. 

 Representing some of the various possible 
word orders is no problem for a c-structure 
framework. As long as the verb and its 
dependents and modifiers are all siblings in the 
tree, linearization rules can determine their 
respective order. But in most c-structure 
frameworks, the subject is not considered to be 

                                                            
7 The emotive strategy, signaling the focus by prosodic 
means, allows speakers to use the basic SVO order. This 
is not treated here, however. 

a sibling of the verb. So if the subject 
intervenes between any of the constituents in 
the VP, this is a sequence which cannot be 
linearized using the assumptions mentioned so 
far.  So of the six possible linear orders of the 
constituents (subject (S), verb (V), object (O)) 
of transitive sentences, two of them, namely 
VSO and OSV show these unlinearizable 
sequences. Do these orders really exist in 
Russian speech? The results of Kallestinova’s 
perception experiment show that they are 
acceptable, although SVO is strongly preferred 
over VSO as a way of focusing on the O, and 
OVS is strongly preferred to VOS as a way of 
focusing on the S. Nevertheless, the two orders 
VSO and VOS occur often enough in 
Kallestinova’s elicitation experiment that they 
cannot be attributed to chance.  

There seems to be a further problem within 
constituency grammars of unifying the feature 
Foc with the final position of the sentence, in 
order to create responses that are appropriate in 
a given context. Once again, the problem is 
that transitive sentences in constituency 
grammars are created by a combination of two 
different phrase structure rules, one as an 
expansion of the S node and one as an 
expansion of the VP node. Either of these rules 
can be written to include this feature in its final 
constituent, and either of these rules can be 
linearized to place its two child constituents in 
either of the two logically possible orders.  
Then there needs to be a rule showing that 
whatever lexical constituent is in final position 
gets the feature Foc, and this rule needs to 
unify with the final position of the tree created 
by the above described phrase structure rules.  

One problem with this is that few if any of 
the major c-structure frameworks mentioned 
earlier makes use of pragmatic features like 
[Foc].8 This of course will change if and when 
they start incorporating more work on 
pragmatics into their grammars.  

Another problem is the need to combine 
such a feature with a phonological feature such 
as ‘#’(clause final, corresponding to a break in 
the prosody). Because the phonology is 
generally taken for granted in work on syntax, 
there are not many examples of formal 

                                                            
8 Kallestinova (chapters 3 and 4) cites some analyses 
within the  Minimalist Program which use phrasal nodes 
with the names FocP or TopP, thus apparently creating a 
pragmatic category rather than a syntactic one. 

201



 
 

treatments involving interactions between it 
and syntax, semantics, or pragmatics. But 
cases of such interactions are discussed 
informally quite regularly, for example in 
discussions of sentences which are 
grammatical or have a specific meaning only 
in the presence of emphatic stress or a specific 
intonation. Goldberg 2006 postulates that 
representations of these different aspects of 
linguistic signs must always be paired together. 
There have been some attempts to incorporate 
such ideas into formal analyses, but no general 
agreement about how this should be done, 
even within one formal framework.  

So far we have discussed word order when 
the subject, the object or the verb is focused.  It 
is also possible to focus certain non-
constituents: verb plus object or verb plus 
subject. In the following sentences 
(Kallestinova 60), (15b) shows an example of 
the former, which typically occurs in response 
to a question like (15a):  

(15) a. Chto  delaet  devochka? 
      What  does  girl 

‘What is the girl doing?’ 

b. Devochka  [podmetaet     pol]-Foc.  
Girl-Nom.    sweeps-3sg. floor-Acc. 
‘The girl is sweeping the floor.’ 

The following data (Kallestinova 17) shows an 
example of verb-subject focus. 

(16) a. What happened to the paintings? 

b. Neskol’ko kartin   [priobrel mestnyj 
   muzej]-Foc.  

a few      paintings acquired  local          
museum 

     ‘The local museum acquired a few of  
    the paintings.’ 

c. Who acquired the paintings? 

d. Neskol’ko kartin priobrel 
[mestnyj muzej]-Foc.  

  A few paintings acquired 
local museum 
‘The local museum acquired a few  

 of the paintings.’ 

(16a) is the kind of question that requires an 
answer like the one in (16b), in which both the 
verb and subject are focused. (16c), on the 
other hand, requires an answer like the one in 
(16d), in which only the subject is in focus, 
since only the subject provides new 
information. The answers to the questions in 
(17a) and (17c) differ only in focus. 

(17)  a. Chto   s    oknom? 
   What      with  window-instr. 

‘What happened to the window?’ 

b. Okno     [razbila  Olja] 
Window-Acc. broke   Olja-Nom. 
‘Olja broke the window.’ 

c. ??Okno     [Olja    razbila]. 
   Window-Acc. Olja-Nom. broke 

‘Olja broke the window.’ 

Sentence (17b) is the preferred answer to the 
question in (17a). Sentence (17c) is a less 
satisfactory answer, but is nevertheless 
possible. Both answers show the subject and 
verb in focus position at the end of the 
sentence.  What is the structural relationship 
between a subject NP and its verb in a 
constituency grammar that has a VP? This 
varies according to the kind of X-bar system 
being used, but in the simplest kind, namely a 
one-bar system, the verb would be the 
niece/nephew of the subject NP. There is no 
one node which includes both of them and no 
other part of the sentence. This would appear 
to make any analysis linking [foc] in one node 
to the other quite awkward and ad hoc. 

Recognizing such extended focus units as 
subject-verb and verb-object is also a problem 
for DG, but it is comforting that whatever 
solution is used for one of these can also be 
used for the other.  
 
 
7  Heads 
 
The head of a syntactic unit such as a phrase or 
sentence is the word which determines the 
category of the larger unit. Within a word, the 
head is the morph that determines the category 
of the word as a whole. The following 
examples show how this is represented in c-
structure and d-structure.   In the former, I will 
assume that superfluous nodes have been 
pruned away, as discussed in section 4. 
 
 
 
 
(18)   good idea 
 
a.              NP 
 
         A               N 
       good          idea 

202



 
 

 
b.              N: idea 
 
         A: good 
 
 On the basis of these examples, there seems to 
be nothing new to say about the differences 
between c-structure and d-structure. In c-
structure, the head is a sibling of its 
complement and has the same category as its 
parent. In d-structure the head is the parent of 
its complement. 
 
However, a problem comes up for c-structure, 
if the complement has the same category as the 
head, as in (19): 
 
(19) She wants to help.       
                               S 
 
                    NP                    VP 
               
                                        V             VP 
 
                   she            wants       to help 
 
Both wants and to help have the category verb. 
They differ merely in bar level. How do we 
determine which one is the head of the higher 
VP?  What formal criterion can be used to 
automate this selection?  One way would be to 
say that in cases like this the lexical node is 
always the head.  Another response to this 
question would be that taken in HPSG: the 
head is designated as H in the PS rule and 
inherits the category of the parent via the Head 
Feature Principle mentioned in section 3. 
 

In any case, there is no such problem for d-
structure. The head is always the parent of its 
complement rather than its sibling.  So the 
category of the head is also the category of the 
entire phrase.  
 
 On the other hand, not every parent needs to 
be considered a head. I believe that the 
categories of words from small closed classes 
such as complementizers (to, that) do not 
determine the category of the entire phrase that 
they are part of and introduce and are therefore 
not heads.  In any case, this question can and 
should be debated separately from the 
questions dealt with in this paper.  
 

8 Catenae: a necessary part of Ellipsis 
and Idioms 
 
Osborne and Groß 2012 provide evidence that 
ellipsis and idioms always meet a specific 
structural condition which is easily formulated 
in dependency grammars, but not in 
constituency grammars. This is that the 
missing words of any construction involving 
ellipsis as well as the idiomatic part of a 
sentence always form a catena (plural: 
catenae). They define this as part of a 
dependency tree which is “continuous with 
respect to dominance.” In other words, the 
nodes of a catena are all connected to each 
other. Osborne and Groß claim that the 
idiomatic part of any sentence must form a 
catena.9 

Tree (20) shows a c-structure representation 
of the WXDY construction (Kay and Fillmore 
1999), which as far as I can determine from 
their discussion would be the one provided by 
the authors, if they had been using tree 
diagrams of this sort10.  

(20)     S 

   NP           S 

    V    NP          VP 

            V     PP 

 What is that fly doing in my soup? 

Tree (21) shows the same sentence in a d-
structure tree (I have omitted the lexical 
categories). 

(21)      is 

 

 What    fly doing 

     that      in 

               soup  
             my 
 What is that fly doing in my soup? 

                                                            
9 They note that there are a few idioms, such as spill the 
beans, which in their passive form (the beans were 
spilled) do not form a catena due to the intervening 
auxiliary verb. To get around this, one might suggest that 
the passive and active forms of the idiom are generated 
by the same lexical entry, which necessarily is an idiom. 
10 They in fact use matrices, specifically attribute-value 
matrices. 

203



 
 

Kay and Fillmore consider the following 
words to be the idiomatic part of this sentence: 
What, is, doing. In other versions of this idiom, 
is can be replaced by any finite form of the 
verb be, depending on what is required for 
agreement with the subject.  This fly and in my 
soup are the non-idiomatic parts of the 
construction. The idiomatic words are all 
connected to each other as a catena in the d-
structure.  It is clear from a glance at (20) that 
no two of the three nodes corresponding to 
what, is, and doing in the c-structure are in a 
sibling relationship. This apparently removes 
the justification for connecting these nodes 
directly.  The alternative is to connect them by 
going through the three phrasal nodes. It is 
necessary to go through both S nodes and the 
VP in order to connect What to the other two 
words of the idiom. But once we allow catenae 
to go through any number of phrasal nodes as 
well as the required lexical nodes, it appears 
that any combination of words can be 
considered a catena.  This would make the 
concept meaningless. This is not the case in d-
structure trees.  For example, the combination 
of what in (21) with any word in the sentence 
except is does not form a catena. 

 
Note that the extended focus units (verb-

subject and verb-object) discussed in the 
previous section do form catenae, thus perhaps 
providing the basis for a solution to the 
problem discussed there.  This is not the place 
to go into details of this, however. 

 
 The significance of catenae for ellipsis is 

that the elided material, no matter how many 
words it consists of, always forms a catena. 
This is dealt with in detail in Osborne and 
Groβ 2012.  I summarize this point with the 
following examples, taken from Osborne and 
Groβ 2012: 

 
(22)   
    a. Fred will attempt to help you, and 
    b. Tom [will attempt to help] me                                 

c. Tom [will attempt] to help me 
 
(23) 

a. she may take a picture of me, and 
b. he [may take a picture] of me 
c. he [may take] a picture of me 

 
 
 

(24)  
a. Mom intends to require me to mow the 
front lawn this week, and 
b. Dad [intends to require me to mow the 
front lawn] next week 
c. Dad [intends to require me to mow] the 
back lawn next week 

 
The a clauses show the first clause of a 
coordinate structure. The b and  c clauses show 
various forms of the second clause. The 
bracketed material can be omitted in each case. 
And in these cases, the words of the omitted 
material form a catena. 
 
9   Interface with Semantics 
 
Words in a sentence combine with each other 
to create meanings of larger units such as 
propositions. Within a constituency grammar, 
this is thought to happen (Partee 1975) by 
combining meanings of the lexical nodes at the 
bottom of the tree to form the meanings of the 
constituent(s) represented by the immediately 
dominating phrasal node(s). The meanings at 
such nodes then combine with each other to 
create the meanings of the phrasal nodes that 
dominate them, and so on, until the meaning at 
the top of the tree is created. Given this 
situation, it appears that the way nodes 
combine depends on the nature of these nodes. 
So for every different phrase structure rule, 
there would be a specific rule combining the 
meanings of each constituent. The view that 
meanings combine this way is known as the 
rule-to-rule hypothesis. 

The rule-to-rule hypothesis works 
differently in dependency grammar, since the 
parent node has its own contribution to the 
meaning of the sentence and this meaning 
needs to combine with the meanings of the 
child or children. If the child nodes are 
complements of their head, they combine with 
the parent as its arguments.  So the parent node 
in DG would generally serve a double function 
as both the head child of the construction and 
place where the meanings of this head and the 
children are shown. 

These two scenarios are shown in the 
following two figures: 

 

 

204



 
 

 

(25)    

  a.               VP drink (milk) 
 
         V     NP 

   drink      milk 

In c-structure, the italicized words show how 
the SEM (semantics) of the verb combines 
with the SEM of the NP to create a predicate-
argument structure.  

 

 b.   V: drink 
    drink (milk)  

       N: milk 

In d-structure, the same node is used both to 
store the predicate drink and the combination 
of the SEM of this word with that of its 
complement noun. The d-structure in (b) 
shows one general pattern found in many 
specific d-structures: the head node is the 
predicate, and the complements its arguments.  
The other general pattern is the one used for 
adjuncts or adjunct phrases: they take their 
head (and its complements, if it has any) as 
their argument. The c-structure version of this  
is shown in (26a) without a VP and (26b) with 
a VP. 
 
 
(26) 
          S 
a.  soon (leaves (John)) 
 
   NP  V   Adv 
 
    John  leaves  soon. 
 
       S     
b.  soon (leaves (John)) 
 
   NP       VP 

                                                                  
V        Adv 

     John   leaves soon. 

In the predicate-argument structures which 
must result from both of the above trees, the 
adjunct soon takes the meaning of the rest of 
the sentence as its argument. 

The problem for c-structure is shown in 
(26b). What meaning is stored at the VP?  It 

should be just the combination of the two 
SEMs which are in its child nodes, that is soon 
(leaves); leaves still has not combined with its 
argument  John, which becomes available only 
in the next step up the tree. But John is also a 
complement of leaves. So it seems to be 
necessary to leave a placeholder slot in the 
argument position of the V to signal that just 
the subject does not immediately fill the V’s 
argument slots, allowing it to remain empty 
until the next step up the tree is made. This 
empty slot is then filled by the subject in that 
next step.   This can be done, but it seems to be 
an unnecessary complication. In the d-
structure, or in the c-structure without the VP, 
all of the argument slots are immediately 
available. 

10      Conclusion  
I have argued that c-structure faces problems 
that d-structure does not face in the areas of 
agreement, case marking, word order 
(especially free word order), eliminating or 
otherwise dealing with superfluous nodes, 
defining catena, and combining meaningful 
parts to create predicate-argument structures. It 
is true that these problems could be reduced by 
eliminating an intermediate level of the phrasal 
projection (the VP node and the N’ node), but 
the fact these intermediate levels have become 
firmly entrenched in frameworks based on c-
structure militates against this happening. In 
fact, in the Minimalist Program the trend has 
been in the opposite direction – to more nodes 
and more structure, making their 
representations of sentence structure still less 
like those provided by dependency grammars. 
These additional nodes guarantee further 
complications in creating meaning 
representations in ways which have been 
shown in this paper.  
 
Acknowledgements 
 
I wish to thank Tim Osborne for comments on 
the first draft of this paper and three 
anonymous referees for comments on the 
submitted version.  This final version has been 
influenced by all of these comments. 
 
References 
 
Joan Bresnan. 2001. Lexical Functional Syntax. 

Oxford: Blackwell.  
 

205



 
 

Noam Chomsky and Howard Lasnik. 1993. "The 
theory of Principles and Parameters". In 
Jacobs, J.; von Stechow, A.; Sternefeld, W. et 
al. Syntax: An international handbook of 
contemporary research. Berlin: de Gruyter.  

Jan Firbas. 1992. Functional Sentence Perspective 
in written and spoken communication.   
Cambridge University Press. republished in 
2006, edited by John Alges and Bas Aarts. 

Gerald Gazdar, Evan Klein et al. 1985. Generalized 
Phrase Structure Grammar. Oxford: Basil 
Blackwell.  

Adele Goldberg. 2006. Constructions at Work: The 
Nature of Generalization in Language. Oxford 
University Press. 

Thomas Groß and Timothy Osborne. 2009. Toward    
a practical dependency grammar of theory 
 discontinuities. SKY Journal of Linguistics 22, 
43-90. 

Richard Hudson. 1984. Word Grammar. Oxford 
University Press.  

Richard Hudson. 2007. Language Networks: the 
New Word Grammar. Oxford University Press.  

Ray Jackendoff. 1977. X-bar Theory. MIT Press. 

Elena Kallestinova. 2007.  Aspects of Russian Word 
Order.  University of Iowa Dissertation. 
http://ir.uiowa.edu/etd/165. 

Paul Kay and Charles Fillmore. 1999. Grammatical 
Constructions and Linguistic Generalizations. 
Language: 75:1-34. 

Igor Mel’čuk. 1988. Dependency Syntax: Theory 
and Practice. Albany: State University of New 
York Press. 

Patrick Murphy. 1995. Word Order, Themes and 
Discourse in Head-Driven Phrase Structure   
Grammar. www.unc/~murphy/research 

 
Timothy Osborne and Thomas Groß. 2012. 

Construction Grammar meets Dependency 
Grammar of syntactic analysis. Cognitive 
Linguistics 23.1: 364-396. 

Barbara Partee. 1975.  Montague grammar and 
transformational Grammar.  Linguistic 
Inquiry 6 (1975): 203-300.   

Carl Pollard and Ivan Sag. 1994. Head-driven 
Phrase Structure Grammar. Chicago 
University Press. 

 
John Robert Ross. 1969. A Proposed Rule of tree 

Pruning. 288-299. In Modern Studies in 
English. Readings in Transformational 

Grammar. Edited by David A. Reibel and 
Sanford Schane. Prentice-Hall:Englewood 
cliffs, New Jersey. 

Klaus Schubert. 1987. Metataxis: Contrastive 
dependency syntax for machine translation. 
Dordrecht: Foris Publications. 

 
Stanley Starosta. 1988.  The case for lexicase: an 

outline of lexicase grammatical theory. Open 
Linguistics Series, ed. by Robin Fawcett.  
London: Pinter Publishers Limited; Cassell. 

Lucien Tesnière. 1959. Éléments de syntaxe 
structural. Paris: Klincksieck, Paris 1959.   

206


