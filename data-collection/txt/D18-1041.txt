












































Untitled


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 447–457
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

447

Multi-Domain Neural Machine Translation with
Word-Level Domain Context Discrimination

Jiali Zeng1 Jinsong Su1∗ Huating Wen1 Yang Liu2 Jun Xie3

Yongjing Yin1 Jianqiang Zhao4

1Xiamen University, Xiamen, China 2Tsinghua University, Beijing, China
3Mobile Internet Group, Tencent Technology Co., Ltd, Beijing, China

4Meiya Pico information Co.,Ltd, Xiamen, China

lemon@stu.xmu.edu.cn, {jssu,htwen,yinyongjing}@xmu.edu.cn
liuyang2011@tsinghua.edu.cn, stiffxie@tencent.com, zhaojq@300188.cn

Abstract

With great practical value, the study of Multi-

domain Neural Machine Translation (NMT)

mainly focuses on using mixed-domain paral-

lel sentences to construct a unified model that

allows translation to switch between different

domains. Intuitively, words in a sentence are

related to its domain to varying degrees, so

that they will exert disparate impacts on the

multi-domain NMT modeling. Based on this

intuition, in this paper, we devote to distin-

guishing and exploiting word-level domain

contexts for multi-domain NMT. To this end,

we jointly model NMT with monolingual

attention-based domain classification tasks

and improve NMT as follows: 1) Based on

the sentence representations produced by a

domain classifier and an adversarial domain

classifier, we generate two gating vectors

and use them to construct domain-specific

and domain-shared annotations, for later

translation predictions via different attention

models; 2) We utilize the attention weights

derived from target-side domain classifier

to adjust the weights of target words in the

training objective, enabling domain-related

words to have greater impacts during model

training. Experimental results on Chinese-

English and English-French multi-domain

translation tasks demonstrate the effec-

tiveness of the proposed model. Source

codes of this paper are available on Github

https://github.com/DeepLearnXMU/WDCNMT.

1 Introduction

In recent years, neural machine translation (NMT)

has achieved great advancement (Nal and Phil,

2013; Sutskever et al., 2014; Bahdanau et al.,

2015). However, two difficulties are encoun-

tered in the practical applications of NMT. On

the one hand, training a NMT model for a spe-

∗Corresponding author

✎�
dàhuì

✚✁

✂✄☎✆✝

✱
de

❆✞
yìàn

◆

✟✝

✠✡

☛☞✌✟✍✏✑

✒✓
lièrù

❆✔
yìchéng

CH

Give priority to congress bills for inclusion in the agenda

EN

Figure 1: Word-level correlation heat map to Laws

domain for a Chinese(CH)-English(EN) parallel

sentence.

cific domain requires a large quantity of paral-

lel sentences in such domain, which is often not

readily available. Hence, the much more com-

mon practice is to construct NMT models using

mixed-domain parallel sentences. In this way,

the domain-shared translation knowledge can be

fully exploited. On the other hand, the translated

sentences often belong to multiple domains, thus

requiring a NMT model general to different do-

mains. Since the textual styles, sentence structures

and terminologies in different domains are of-

ten remarkably distinctive, whether such domain-

specific translation knowledge is effectively pre-

served could have a direct effect on the perfor-

mance of the NMT model. Therefore, how to

simultaneously exploit the exclusive and shared

translation knowledge of mixed-domain parallel

sentences for multi-domain NMT remains a chal-

lenging task.

To tackle this problem, recently, researchers

have carried out many constructive and in-depth

studies (Kobus et al., 2016; Zhang et al., 2016;

Pryzant et al., 2017; Farajian et al., 2017). How-

ever, most of these studies mainly focus on the uti-

lization of domain contexts as a whole in NMT,

while ignoring the discrimination of domain con-

texts at finer-grained level. In each sentence,

some words are closely associated with its do-

main, while others are domain-independent. In-

tuitively, these two kinds of words play differ-



448

ent roles in multi-domain NMT, nevertheless, they

are not being distinguished by the current models.

Take the sentence shown in Figure 1 for exam-

ple. The Chinese words “‘¬”(congress), “Æ

Y”(bills), “�\”(inclusion), and “Æ§”(agenda)

are frequently used in Laws domain and imply the

Laws style of the sentence, while other words in

this sentence are common in all domains and they

mainly indicate the semantic meaning of the sen-

tence. Thus, it is reasonable to distinguish and

encode these two types of words separately to

capture domain-specific and domain-shared con-

texts, allowing the exclusive and shared knowl-

edge to be exploited without any interference from

the other. Meanwhile, the English words “prior-

ity”,“government”, “bill” and “agenda” are also

closely related to Laws domain. To preserve the

domain-related text style and idioms in generated

translations, it is also reasonable for our model to

pay more attention to these domain-related words

than the others during model training. On this ac-

count, we believe that it is significant to distin-

guish and explore word-level domain contexts for

multi-domain NMT.

In this paper, we propose a multi-domain NMT

model with word-level domain context discrimi-

nation. Specifically, we first jointly model NMT

with monolingual attention-based domain classi-

fication tasks. In source-side domain classifica-

tion and adversarial domain classification tasks,

we perform two individual attention operations on

source-side annotations to generate the domain-

specific and domain-shared vector representations

of source sentence, respectively. Meanwhile, an

attention operation is also placed on target-side

hidden states to implement target-side domain

classification. Then, we improve NMT with the

following two approaches:

(1) According to the sentence representations

produced by source-side domain classifier and ad-

verisal domain classifier, we generate two gating

vectors for each source annotation. With these two

gating vectors, the encoded information of source

annotation is selected automatically to construct

domain-specific and domain-shared annotations,

both of which are used to guide translation pre-

dictions via two attention mechanisms;

(2) Based on the attention weights of the target

words from target-side domain classifier, we em-

ploy word-level cost weighting strategy to refine

our model training. In this way, domain-specific

target words will be assigned greater weights than

others in the objective function of our model.

Our work demonstrates the benefits of sepa-

rate modeling of the domain-specific and domain-

shared contexts, which echoes with the success-

ful applications of the multi-task learning based on

shared-private architecture in many tasks, such as

discourse relation recognition (Liu et al., 2017b),

word segmentation (Chen et al., 2017b), text clas-

sification (Liu et al., 2017a), and image classifica-

tion (Liu et al., 2016). Overall, the main contribu-

tions of our work are summarized as follows:

• We propose to construct domain-specific and
domain-shared source annotations from ini-

tial annotations, of which effects are respec-

tively captured for translation predictions.

• We propose to adjust the weights of target
words in the training objective of NMT ac-

cording to their relevance to different do-

mains.

• We conduct experiments on large-scale
multi-domain Chinese-English and English-

French datasets. Experimental results

demonstrate the effectiveness of our model.

2 Model

Figure 2 illustrates the architecture of our model,

which includes a neural encoder equipped with a

domain classifier and an adversarial domain clas-

sifier, and a neural decoder with two attention

models and a target-side domain classifier.

2.1 Neural Encoder

As shown in the lower part of Figure 2, our

encoder leverages the sentence representations

produced by these two classifiers to construct

domain-specific and domain-shared annotations

from initial ones, preventing the exclusive and

shared translation knowledge from interfering

with each other. In our encoder, the input sen-

tence x=x1, x2, ..., xN are first mapped to word

vectors and then fed into a bidirectional GRU

(Cho et al., 2014) to obtain
−→
h =
−→
h 1,
−→
h 2, ...,

−→
h N

and
←−
h =
←−
h 1,
←−
h 2, ...,

←−
h N in the left-to-right and

right-to-left directions, respectively. These two

sequences are then concatenated as hi =

{
−→
h ⊤i ,
←−
h ⊤i }

⊤ to form the word-level semantic rep-

resentation of the input sentence.

Domain Classifier and Adversarial Domain

Classifier. With annotations {hi}
N
i=1, we employ



449

✒✞☛
�
✁

Decoder

✂

✄☎ ✄✆ ✄✝

Domain Classifier

✟✠ ✟☎ ✟✡
☞

☞

Encoder
✂

Domain-Specific

Annotations ✒✌☛
�
✁

✂
Domain-Shared

Annotations ✒✌☛
✍
✁

Domain Classifier

✎✏☛
✑✓

Er( )

✒✌☛✁

✟✠ ✟☎ ✟✡
☞

Es( )

Er(y)

✟✠ ✟☎ ✟✡
☞

Adversarial Domain Classifier

✂ ✂

✔☎ ✔✆ ✔✕

Figure 2: The architecture illustration of our model. Note that our two source-side domain classifiers

are used to produce domain-specific and domain-shared annotations, respectively, and our target-side

domain classifier is only used during model training.

two attention-like aggregators to generate the se-

mantic representations of sentence x, denoted by

the vectors Er(x) and Es(x), respectively. Based
on these two vectors, we employ the same neu-

ral network to model two classifiers with different

context modeling objectives:

One is a domain classifier that aims to dis-

tinguish different domains in order to generate

domain-specific source-side contexts. It is trained

using the objective function J sdc(x;θ
s

dc
) = log

p(d|x;θs
dc
), where d is the domain tag of x and

θs
dc

is its parameter set. The other is an adversarial

domain classifier capturing source-side domain-

shared contexts. To this end, we train it using the

following adversarial loss functions:

J s1adc(x;θ
s1
adc) = log p(d|x;θ

s1
adc,θ

s2
adc), (1)

J s2adc(x;θ
s2
adc) = H(p(d|x;θ

s1
adc,θ

s2
adc)), (2)

where H(p(·))=−
∑K

k=1 pk(·) log pk(·) is an en-
tropy of distribution p(·) with K domain labels,
θs1
adc

and θs2
adc

denote the parameters of softmax

layer and the generation layer of Es(x) in this
classifier, respectively. By this means, Er(x) and
Es(x) are expected to encode the domain-specific
and domain-shared semantic representations of x,

respectively. It should be noted that our utiliza-

tion of domain classifiers is similar to adversarial

training used in (Pryzant et al., 2017) which injects

domain-shared contexts into annotations. How-

ever, by contrast, we introduce domain classifier

and adversarial domain classifier simultaneously

to distinguish different kinds of contexts for NMT

more explicitly.

Here we describe only the modeling procedure

of the domain classifier, while it is also applicable

to the adversarial domain classifier. Specifically,

Er(x) is defined as follows:

Er(x) =
N∑

i=1

αihi, (3)

where αi =
exp(ei)∑N
i′ exp(ei′)

,

ei = (va)
⊤ tanh(Wahi),

and va and Wa are the relevant attention pa-

rameters. Then, we feed Er(x) into a fully
connected layer with ReLU function (Ballesteros

et al., 2015), and then pass its output through a

softmax layer to implement domain classification

p(·|x;θsdc)

=softmax(W s⊤dc ReLU(Er(x)) + b
s
dc), (4)

where W sdc and b
s
dc are softmax parameters.

Domain-Specific and Domain-Shared Anno-

tations. Since domain-specific and domain-shared

contexts have different effects on NMT, and thus



450

should be distinguished and separately captured

by NMT model. Specifically, we first leverage the

sentence representations Er(x) and Es(x) to gen-
erate two gating vectors, gri and g

s
i , for annotation

hi in the following way:

gri = sigmoid(W
(1)
gr Er(x) +W

(2)
gr hi + bgr),

(5)

gsi = sigmoid(W
(1)
gs Es(x) +W

(2)
gs hi + bgs),

(6)

where W ∗gr, W
∗
gs, bgr and bgs denote the rele-

vant matrices and bias, respectively. With these

two vectors, we construct domain-specific and

domain-shared annotations hri and h
s
i from hi:

hri = g
r
i ⊙ hi, (7)

hsi = g
s
i ⊙ hi. (8)

2.2 Neural Decoder

The upper half of Figure 2 illustrates the architec-

ture of our decoder. In particular, with the atten-

tion weights of target words from the domain clas-

sifier, we employ word-level cost weighting strat-

egy to refine model training.

Formally, our decoder applies a nonlinear func-

tion g(∗) to define the conditional probability of
translation y=y1, y2, ..., yM :

p(y|x) =
M∏

j=1

p(yj |x, y<j) =
M∏

j=1

g(yj−1, sj , c
r
j , c

s
j),

(9)

where the vector sj denotes the GRU hidden state.

It is updated as

sj = GRU(sj−1, yj−1, c
r
j , c

s
j). (10)

Here the vectors crj and c
s
j represent the domain-

specific and domain-shared contexts, respectively.

Domain-Specific and Domain-Shared Con-

text Vectors. When generating yj , we define c
r
j

as a weighted sum of the domain-specific annota-

tions {hri }:

crj =
N∑

i=1

exp(erj,i)∑N
i′=1 exp(e

r
j,i′)
· hri , (11)

where erj,i = a(sj−1, h
r
i ),

and a(*) is a feedforward neural network. Mean-

while, we produce csj from the domain-shared an-

notations {hsi} as in Eq. 11. By introducing c
r
j

and csj into sj , our decoder is able to distinguish

and simultaneously exploit two types of contexts

for translation predictions.

Domain Classifier. We equip our decoder

with a domain classifier with parameters θtdc,

which maximizes the training objective i.e.,

J tdc(y;θ
t

dc
) = log p(d|y;θt

dc
). To do this,

we also apply attention operation to produce the

domain-aware semantic representation Er(y) of
y,

Er(y) =
M∑

j=1

βjsj , (12)

where βj =
exp(ej)∑M
j′ exp(ej′)

,

ej = (vb)
⊤ tanh(Wbsj),

and vb and Wb are the related parameters. Like-

wise, we stack a domain classifier on top of Er(y).
Note that this classifier is only used in model

training to infer attention weights of target words.

These weights measure their semantic relevance

to different domains and can be utilized to adjust

their cost weights in NMT training objective.

NMT Training Objective with Word-Level

Cost Weighting. Formally, we define the objec-

tive function of NMT as follows:

Jnmt(x,y;θnmt)

=
M∑

j=1

(1 + βj) log p(yj |x, y<j ;θnmt), (13)

where βj is the attention weight of yj obtained

by Eq. (12), and θnmt denotes the parameter

set of NMT. By this scaling strategy, domain-

specific words are emphasized with a bonus, while

domain-shared words are updated as usual.

Please note that scaling costs with a multiplica-

tive scalar essentially changes the magnitude of

parameter update but without changing its direc-

tion (Chen et al., 2017a). Besides, although our

scaling strategy is similar to the cost weighting

proposed by Chen et al. (2017a), our approach dif-

fers from it in two aspects: First, we employ word-

level cost weighting rather than sentence-level one

to refine NMT training; Second, our approach is

less time-consuming for multi-domain NMT.

2.3 Overall Training Objective

Given a mixed-domain training corpus D =
{(x,y, d)}, we train the proposed model accord-



451

ing to the following objective function:

J (D;θ) =
∑

(x,y,d)∈D

{Jnmt(x,y;θnmt)

+ J sdc(x;θ
s

dc) + J
t
dc(y;θ

t

dc) (14)

+ J s1adc(x;θ
s1
adc) + λ · J

s2
adc(x;θ

s2
adc)}

where Jnmt(∗), J
s
dc(∗), J

t
dc(∗) and J

s∗
adc(∗) are

the objective functions of NMT, source-side do-

main classifier, target-side domain classifier, and

source-side adversarial domain classifier, respec-

tively, θ={θnmt, θ
s

dc
, θt

dc
, θs1

adc
, θs2

adc
}, and λ is

the hyper-parameter for adversarial learning.

Particularly, to ensure encoding accuracy of

domain-shared contexts, we follow Chen et al.

(2017b) to adopt an alternative two-phase strat-

egy in training, where we alternatively optimize

J (D;θ) with θs1
adc

and {θ-θs1
adc
} respectively

fixed at a time.

3 Experiment

To investigate the effectiveness of our model, we

conducted multi-domain translation experiments

on Chinese-English and English-French datasets.

3.1 Setup

Datasets. For Chinese-English translation, our

data comes from UM-Corpus (Tian et al., 2014)

and LDC1. To ensure data quality, we chose only

the parallel sentences with domain label Laws,

Spoken, and Thesis from UM-Corpus, and the

LDC bilingual sentences related to News domain

as our dataset. We used randomly selected sen-

tences from UM-Corpus and LDC as development

set, and combined the test set of UM-Corpus and

randomly selected sentences from LDC to con-

struct our test set. For English-French transla-

tion, we conducted experiments on the datasets

of OPUS corpus2, containing sentence pairs from

Medical, News, and Parliamentary domains. We

also divided these datasets into training, develop-

ment and test sets. Table 1 provides the statistics

of the corpora used in our experiments.

We performed word segmentation on Chi-

nese sentences using Stanford Segmenter3, and

tokenized English and French sentences using

MOSES script4. Then, we employed Byte Pair

1https://www.ldc.upenn.edu/.
2http://opus.nlpl.eu/
3https://nlp.stanford.edu/
4http://www.statmt.org/moses/

Task Domain Train Dev Test

CH-EN

Laws 219K 600 456

Spoken 219K 600 455

Thesis 299K 800 625

News 300K 800 650

EN-FR

Medical 1.09M 800 2000

News 180K 800 2000

Parliamentary 2.04M 800 2000

Table 1: Sentence numbers of data sets in our ex-

periments.

Encoding (Sennrich et al., 2016) to convert all

words into subwords. The translation quality was

evaluated by case-sensitive BLEU (Papineni et al.,

2002).

Contrast Models. Since our model is essen-

tially a standard attentional NMT model enhanced

with word-level domain contexts, we refer to it as

+WDC. We compared it with the following mod-

els, namely:

• OpenNMT5. A famous open-source NMT
system used widely in the NMT community

trained on mix-domain training set.

• DL4NMT-single (Bahdanau et al., 2015). A
reimplemented attentional NMT trained on a

single domain dataset.

• DL4NMT-mix (Bahdanau et al., 2015). A
reimplemented attentional NMT trained on

mix-domain training set.

• DL4NMT-finetune (Luong and Manning,
2015). A reimplemented attentional NMT

which is first trained using out-of-domain

training corpus and then fine-tuned using in-

domain dataset.

• +Domain Control (+DC) (Kobus et al.,
2016). It directly introduces embeddings of

source domain tag to enrich annotations of

encoder.

• +Multitask Learning (+ML1) (Dong et al.,
2015). It adopts a multi-task learning frame-

work that shares encoder representation and

separates the decoder modeling of different

domains.

• +Multitask Learning (+ML2) (Pryzant
et al., 2017). This model jointly trains

5http://opennmt.net/.



452

NMT with domain classification via multi-

task learning.

• +Adversarial Discriminative Mixing
(+ADM) (Pryzant et al., 2017). It employs

adversarial training to achieve the domain

adaptation in NMT.

• +Target Token Mixing (+TTM) (Pryzant
et al., 2017). This model is similar to

+DC, with the only difference that it enriches

source annotations by adding target-side do-

main tag rather than source-side one.

Note that our model uses two annotation se-

quences, thus we also compared it with the afore-

mentioned models with two times of hidden state

size (2×hd). To further examine the effectiveness
of the proposed components in our model, we also

provided the performance of the following vari-

ants of our model:

• +WDC(S). It only exploits the source-side
word-level domain contexts for multi-domain

NMT.

• +WDC(T). It only employ word-level cost
weighting on the target side to refine the

model training.

Implementation Details. Following the com-

mon practice, we only used the training sentences

within 50 words to efficiently train NMT models.

Thus, 85.40% and 88.96% of the Chinese-English

and English-French parallel sentences were cov-

ered in our experiments. In addition, we set the

vocabulary size for Chinese-English and English-

French as 32,000 and 32,000, respectively. In do-

ing so, our vocabularies covered 99.97% Chinese

words and 99.99% English words of the Chinese-

English corpus, and almost 100% English words

and 99.99% French words of the English-French

corpus, respectively.

We applied Adam (Kingma and Ba, 2015) to

train models and determined the best model pa-

rameters based on the model performance on de-

velopment set. The used hyper-parameter were set

as follows: β1 and β2 of Adam as 0.9 and 0.999,

word embedding dimension as 500, hidden layer

size as 1000, learning rate as 5×10−4, batch size
as 80, gradient norm as 1.0, dropout rate as 0.1,

and beamsize as 10. Other settings were set fol-

lowing (Bahdanau et al., 2015).

Model Laws Spoken Thesis News

Contrast Models (1×hd)

OpenNMT 45.82 9.15 13.93 19.73

DL4NMT-single 43.66 5.49 14.54 18.74

DL4NMT-mix 46.82 8.95 15.93 20.33

DL4NMT-finetune 54.19 8.77 16.71 21.55

+DC 49.83 9.18 16.71 20.58

+ML1 46.82 6.66 15.10 20.17

+ML2 48.95 9.45 15.85 20.48

+ADM 48.30 9.41 16.34 20.06

+TTM 49.05 9.36 16.42 20.44

Contrast Models (2×hd)

DL4NMT-single 44.48 6.29 14.66 19.87

DL4NMT-mix 48.74 9.01 16.12 20.14

DL4NMT-finetune 54.69 9.07 17.11 21.85

+DC 50.43 9.38 16.45 20.44

+ML1 49.49 7.67 15.50 20.34

+ML2 50.05 9.35 16.03 20.64

+ADM 48.33 9.06 16.59 19.69

+TTM 49.92 9.01 16.38 21.04

Our Models

+WDC(S) 54.55 10.12 17.22 22.16

+WDC(T) 51.94 9.76 17.72 21.02

+WDC 55.03 10.20 18.04 22.29

Table 2: Overall Evaluation of the Chinese-

English translation task. 2×hd = two times of hid-
den state size.

3.2 Results on Chinese-English Translation

We first determined the optimal hyper-parameter

λ (see Eq. (14)) on the development set. To do

this, we gradually varied λ from 0.1 to 1.0 with

an increment of 0.1 in each step. Since our model

achieved the best performance when λ=0.1, hence,

we set λ=0.1 for all experiments thereafter.

Table 2 shows the overall experimental results.

Using almost the same hyper-parameters, our re-

implemented DL4NMT outperforms OpenNMT

in all domains, demonstrating that our baseline

is competitive in performance. Moreover, on all

test sets of different domains, our model signifi-

cantly outperforms other contrast models no mat-

ter which hyper-parameters they use. Further-

more, we arrive at the following conclusions:

First, our model surpasses DL4NMT-single,

DL4NMT-mix and DL4NMT-finetune, all of

which are commonly used in domain adaptation

for NMT. Please note that DL4NMT-finetune re-

quires multiple adapted NMT models to be con-

structed, while ours is a unified one that works

well in all domains.

Second, compared with +DC, +ML2 and

+ADM which all exploit source-side domain con-

texts for multi-domain NMT, our +WDC(S) still



453

(b) An Example Sentence in Thesis Domain

✒�
✁✂✄☎✆✝

✣✞
✟✠✡✄☎☛☞✌

✱
de

✘✍
✄☞✎✏✑✠✓✄

✔✕
yìnglì

✖
✗✎

✙✚
shíyàn

❆✛
jìsuàn

(a) An Example Sentence in Laws Domain

✪✜
àomén

✭✢
tèbié

❃✤✥
✦✧★✩✫✬✮★✩✯✰

✶✲✳
✴✵✷✸✬✹✵

✺
de

✻✼
✽✬✸★✾✬✿★✩

❀✲
❁❂★✷✸

Figure 3: The correlation heat map of the gating

vectors(blue/green) to domain-specific/domain-

shared annotations in two example sen-

tences. Note that domain-specific words “e

”(Macao), “á{¬”(Legislative Council), “�

)”(Formation), “{”(Method), “µ4”(Seal),

“O”(Calculation), “¢�” (Experiment) are

strengthened by gri , while most of the domain-

shared words “�”(of) and “”(and) are focused

by gsi .

exhibits better performance. This is because that

these models focus on one aspect of domain con-

texts, while our model considers both domain-

specific and domain-shared contexts on the source

side.

Third, +WDC(T) also outperforms DL4NMT,

revealing that it is reasonable and effective to em-

phasize domain-specific words in model training..

Last, +WDC achieves the best performance

when compared with both +WDC(S) and

+WDC(T). Therefore, we believe that word-level

domain contexts on the both sides are com-

plementary to each other, and utilizing them

simultaneously is beneficial to multi-domain

NMT.

3.3 Experimental Analysis

Furthermore, we conducted several visualization

experiments to empirically analyze the individual

effectiveness of the added model components.

3.3.1 Visualizations of Gating Vectors

We first visualized the gating vectors gri and g
s
i

to quantify their effects on extracting domain-

specific and domain-shared contexts from initial

source-side annotations. Since both gri and g
s
i are

high dimension vectors, which are difficult to be

visualized directly, we followed Li et al. (2016)

and Zhou et al. (2017) to visualize their individ-

ual contributions to the final output, which can be

(a) Sentence Representation Er(x)

(c) Average of Sentence Annotations

{❄❅
❇(x)}

(b) Sentence Representation Es(x)

(d) Average of Sentence Annotations  

{❄❅
❈(x)}

Figure 4: The visualization of the sentence

representations and their corresponding average

annotations, where the triangle-shaped(purple),

circle-shaped(red), square-shaped(green) and

pentagonal-shaped(blue) points denote News,

Laws, Spoken and Thesis sentences, respectively.

approximated by their first derivatives.

Figure 3 shows the first derivative heat maps

for two example sentences in Laws and Thesis

domain, respectively. We can observe that with-

out any loss of semantic meanings from source

sentences, most of the domain-specific words are

strengthened by gri , while most of the domain-

shared words, especially function words, are fo-

cused by gsi . This result is consistent with our ex-

pectation for the function of two gating vectors.

3.3.2 Visualizations of Sentence

Representations and Annotations

Furthermore, we applied the hypertools (Heusser

et al., 2018) to visualize the sentence representa-

tions Er(x) and Es(x), and the domain-specific
and domain-shared annotation sequences {hri }

N
i=1

and {hsi}
N
i=1. Here we represent each annotation

sequence with its average vector in the figure.

As shown in Figure 4 (a) and (b), the sentence

representation vectors and the average annotation

vectors of different domains are clearly distributed

in different regions. By contrast, their distribu-

tions are much more concentrated in Figure 4 (c)

and (d). Thus, we conclude that our model is able

to distinctively learn domain-specific and domain-

shared contexts. Moreover, from Figure 4 (b), we

observe that the sentence representation vectors of

Laws domain does not completely coincide with



454

Domain Top10 Target Words

Laws
Article, Chapter, Principles, regulations, Pro-
visions, Political, Servants, specify, China,
Municipal

Spoken
meanly, Rusty, 1910s, scours, mountaintops,
paralyze, Puff, perpetrators, hitter, weightlift-
ing

Thesis
aggregation, Activities, Computation, Alzhei-
mer, nn, Contemporarily, EVALUATION,
ethoxycarbonyl, sCRC, Announced

News
months, agency, outweighed, unconstitution-
ally, Congolese, session, Asia, news, hurts,
francs

Table 3: Examples of Domain-Specific Target

Words.

those of the other domains, this may be caused by

the more formal and consistent sentence styles in

Laws domain.

3.3.3 Illustrations of Domain-Specific Target

Words

Lastly, for each domain, we presented the top ten

target words with the highest weights learned by

our target-side domain classifier. To do this, we

calculated the average attention weight of each

word in the training corpus as its corresponding

domain weight.

As is clearly shown in Table 3 that most listed

target words are closely related to their domains.

This result validates the aforementioned hypothe-

sis that some words are domain-dependent while

others are domain-independent, and our target-

side domain classifier is capable of distinguishing

them with different attention weights.

3.4 Results on English-French Translation

Likewise, we determined the optimal λ=0.1 on

the development set. Table 4 gives the results of

English-French multi-domain translation. Similar

to the previous experimental result in Section 3.2,

our model continues to achieve the best perfor-

mance compared to all contrast models using two

different hidden state size settings, which demon-

strates again that our model is effective and gen-

eral to different language pairs in multi-domain

NMT.

4 Related Work

In this work, we study on multi-domain machine

translation in the field of domain adaptation for

machine translation, which has attracted great at-

tention since SMT (Clark et al., 2012; Huck et al.,

Model Medical Parliamentary News

Contrast Models (1×hd)

OpenNMT 78.78 32.96 30.22

DL4NMT-single 77.34 33.28 29.56

DL4NMT-mix 78.48 33.16 31.62

DL4NMT-finetune 78.61 33.72 34.04

+DC 79.34 33.38 33.94

+ML1 77.29 33.39 31.92

+ML2 78.65 33.55 33.48

+ADM 76.74 33.06 33.43

+TTM 78.27 33.29 33.37

Contrast Models (2×hd)

DL4NMT-single 78.50 33.38 30.23

DL4NMT-mix 78.84 33.19 33.28

DL4NMT-finetune 79.17 33.88 34.20

+DC 79.96 33.44 33.52

+ML1 78.38 33.20 31.90

+ML2 79.41 33.55 33.62

+ADM 79.31 33.50 33.34

+TTM 79.36 33.13 33.68

Our Models

+WDC(S) 82.76 34.13 34.31

+WDC(T) 81.51 33.76 33.78

+WDC 83.35 34.17 34.87

Table 4: Overall Evaluation on the English-French

translation task.

2015; Sennrich et al., 2013). As for NMT, the

dominant strategies for domain adaptation gener-

ally fall into two categories:

The first category is to transfer out-of-domain

knowledge to in-domain translation. The con-

ventional method is fine-tuning, which first trains

the model on out-of-domain dataset and then fine-

tunes it on in-domain dataset (Luong and Man-

ning, 2015; Zoph et al., 2016; Servan et al., 2016).

Freitag and Al-Onaizan (2016) proceeded further

by ensembling the fine-tuned model with the origi-

nal one. Chu et al. (2017) fine-tuned the model us-

ing the mix of in-domain and out-of-domain train-

ing corpora. From the perspective of data selec-

tion, Chen et al. (2017a) scaled the top-level costs

of NMT system according to each training sen-

tence’s similarity to the development set. Wang

et al. (2017a) explored the data selection strategy

based on sentence embeddings for NMT domain

adaptation. Moreover, Wang et al. (2017b) further

proposed several sentence and domain weighting

methods with a dynamic weight learning strategy.

However, these approaches usually only perform

well on target domain while being highly time

consuming in transferring translation knowledge

to all the constitute domains.

The second category is to directly use a mixed-



455

domain training corpus to construct NMT model

for the translated sentences derived from different

domains. In this aspect, Kobus et al. (2016) in-

corporated domain information into NMT by ap-

pending a domain indicator token to each source

sequence. Similarly, Johnson et al. (2016) added

an artificial token to the input sequence to indicate

the required target language. Contrastingly, Fara-

jian et al. (2017) utilized the similarity between

each test sentence and the training instances to dy-

namically set the hyper-parameters of the learn-

ing algorithm and update the generic model on

the fly. Pryzant et al. (2017) proposed three novel

models: discriminative mixing that jointly models

NMT with domain classification, adversarial dis-

criminative mixing, and target token mixing which

appends a domain token to the target sequence.

Sajjad et al. (2017) explored data concatenation,

model stacking, data selection and multi-model

ensemble to train multi-domain NMT. By exploit-

ing domain as a tag or a feature, Tars and Fishel

(2018) treated text domains as distinct languages

in order to use multi-lingual approaches when im-

plementing multi-domain NMT. Inspired by topic-

based SMT, some researchers resorted to incor-

porating topical contexts into NMT. Chen et al.

(2016) used the topic information of input sen-

tence as an additional input to decoder. Zhang

et al. (2016) enhanced the word representation by

adding its topic embedding. However, these meth-

ods require to have explicit document boundaries

between training data, which unfortunately do not

exist in most datasets.

Overall, our work is related to the second type

of approach with (Pryzant et al., 2017) and (Chen

et al., 2017a) most related to ours. Unlike (Pryzant

et al., 2017) applying adversarial training to only

capture domain-shared translation knowledge, we

further exploit domain-specific translation knowl-

edge for multi-domain NMT. Also, in sharp con-

trast to (Chen et al., 2017a), our model not only

exploits the source-side word-level domain con-

texts differently, but also employs a word-level

cost weighting strategy for multi-domain NMT.

5 Conclusion and Future Work

In this work, we have explored how to uti-

lize word-level domain contexts for multi-domain

NMT. By jointly modeling NMT and domain clas-

sification tasks, we utilize the sentence represen-

tations of source-side domain classifier and ad-

versarial domain classifier to construct domain-

specific and domain-shared source annotations,

which are then exploited by decoder. Moreover,

using the attentional weights of target-side domain

classifier, we adjust the weights of target words in

the training objective to refine model training. Ex-

perimental results and in-depth analyses demon-

strate the effectiveness of the proposed model.

In the future, we would like to extend the pro-

posed word-level cost weighting strategy to source

words. Besides, our method is also general to

other NMT models. Therefore, we plan to ap-

ply our method to the NMT with complex ar-

chitectures, for example, lattice-to-sequence NMT

(Su et al., 2017), hierarchy-to-sequence NMT (Su

et al., 2018), NMT with context-aware encoder

(Zhang et al., 2017) and Transformer (Vaswani

et al., 2017) and so on.

Acknowledgments

The authors were supported by National Natural

Science Foundation of China (No. 61672440), the

Fundamental Research Funds for the Central Uni-

versities (Grant No. ZK1024), and Scientific Re-

search Project of National Language Committee

of China (Grant No. YB135-49). We also thank

the reviewers for their insightful comments.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. of ICLR
2015.

Miguel Ballesteros, Chris Dyer, and Noah A. Smith.
2015. Improved transition-based parsing by model-
ing characters instead of words with lstms. In Proc.
of EMNLP 2015.

Boxing Chen, Colin Cherry, George Foster, and
Samuel Larkin. 2017a. Cost weighting for neural
machine translation domain adaptation. In Proc. of
the First Workshop on Neural Machine Translation.

Wenhu Chen, Evgeny Matusov, Shahram Khadivi,
and Jan-Thorsten Peter. 2016. Guided alignment
training for topic-aware neural machine translation.
CoRR abs/1607.01628.

Xinchi Chen, Zhan Shi, Xipeng Qiu, and Xuanjing
Huang. 2017b. Adversarial multi-criteria learning
for chinese word segmentation. In Proc. of ACL
2017.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger



456

Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proc. of
EMNLP 2014.

Chenhui Chu, Raj Dabre, and Sadao Kurohashi. 2017.
An empirical comparison of domain adaptation
methods for neural machine translation. In Proc. of
ACL 2017.

Jonathan H. Clark, Alon Lavie, and Chris Dyer. 2012.
One system, many domains: Open-domain statisti-
cal machine translation via feature augmentation. In
Proc. of AMTA 2012.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
tiple language translation. In Proc. of ACL 2015.

M. Amin Farajian, Marco Turchi, Matteo Negri, and
Marcello Federico. 2017. Multi-domain neural ma-
chine translation through unsupervised adaptation.
In Proc. of WMT 2017.

Markus Freitag and Yaser Al-Onaizan. 2016. Fast
domain adaptation for neural machine translation.
CoRR abs/1612.06897.

Andrew C. Heusser, Kirsten Ziman, Lucy L. W. Owen,
and Jeremy R. Manning. 2018. Hypertools: a
python toolbox for gaining geometric insights into
high-dimensional data. Journal of Machine Learn-
ing Research.

M Huck, A Birch, and B Haddow. 2015. Mixed-
domain vs. multi-domain statistical machine trans-
lation. In Proc. of MT Summit 2015.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-
rat, Fernanda B. Viégas, Martin Wattenberg, Greg
Corrado, Macduff Hughes, , and Jeffrey Dean.
2016. Google’s multilingual neural machine trans-
lation system: Enabling zero-shot translation. CoRR
abs/1611.04558.

Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam:
A method for stochastic optimization. In Proc. of
ICLR 2015.

Catherine Kobus, Josep Crego, and Jean Senellart.
2016. Domain control for neural machine transla-
tion. CoRR abs/1612.06140.

Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.
2016. Visualizing and understanding neural models
in nlp. In Proc. of NAACL 2016.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016.
Domain separation networks. In Proc. of NIPS
2016.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017a.
Adversarial multi-task learning for text classifica-
tion. In Proc. of ACL 2017.

Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui.
2017b. Implicit discourse relation classification via
multi-task neural networks. In Proc. of ACL 2017.

Minh-Thang Luong and Christopher D Manning. 2015.
Stanford neural machine translation systems for spo-
ken language domains. In Proc. of IWSLT 2015.

Kalchbrenner Nal and Blunsom Phil. 2013. Recurrent
continuous translation models. In Proc. of EMNLP
2013.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002.

Reid Pryzant, Denny Britz, and Q Le. 2017. Effective
domain mixing for neural machine translation. In
Proc. of WMT 2017.

Hassan Sajjad, Nadir Durrani, Fahim Dalvi, Yonatan
Belinkov, and Stephan Vogel. 2017. Neural ma-
chine translation training in a multi-domain sce-
nario. CoRR abs/1708.08712.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proc. of ACL 2016.

Rico Sennrich, Holger Schwenk, and Walid Aransa.
2013. A multi-domain translation model framework
for statistical machine translation. In Proc. of ACL
2013.

Christophe Servan, Josep Crego, and Jean Senel-
lart. 2016. Domain specialization: a post-training
domain adaptation for neural machine translation.
CoRR abs/1612.06141.

Jinsong Su, Zhixing Tan, Deyi Xiong, Rongrong Ji, Xi-
aodong Shi, and Yang Liu. 2017. Lattice-based re-
current neural network encoders for neural machine
translation. In Proc. of AAAI 2017, pages 3302–
3308.

Jinsong Su, Jiali Zeng, Deyi Xiong, Yang Liu, Mingx-
uan Wang, and Jun Xie. 2018. A hierarchy-
to-sequence attentional neural machine translation
model. IEEE/ACM Trans. Audio, Speech & Lan-
guage Processing, 26(3):623–632.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proc. of NIPS 2014.

Sander Tars and Mark Fishel. 2018. Multi-domain neu-
ral machine translation. CoRR abs/1805.02282.

Liang Tian, Derek F. Wong, Lidia S. Chao, Paulo
Quaresma, Francisco Oliveira, Shuo Li, Yiming
Wang, and Yi Lu. 2014. Um-corpus: A large
english-chinese parallel corpus for statistical ma-
chine translation. In Proc. of LREC 2014.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proc. of NIPS 2017.



457

Rui Wang, Andrew Finch, Masao Utiyama, and Ei-
ichiro Sumita. 2017a. Sentence embedding for neu-
ral machine translation domain adaptation. In Proc.
of ACL 2017.

Rui Wang, Masao Utiyama, Lemao Liu, Kehai Chen,
and Eiichiro Sumita. 2017b. Instance weighting for
neural machine translation domain adaptation. In
Proc. of EMNLP 2017.

Biao Zhang, Deyi Xiong, Jinsong Su, and Hong Duan.
2017. A context-aware recurrent encoder for neu-
ral machine translation. IEEE/ACM Trans. Audio,
Speech & Language Processing, 25(12):2424–2432.

Jian Zhang, Liangyou Li, Andy Way, and Qun Liu.
2016. Topic-informed neural machine translation.
In Proc. of COLING 2016.

Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou.
2017. Selective encoding for abstractive sentence
summarization. In Proc. of ACL 2017.

Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
neural machine translation. Proc. of EMNLP 2016.


