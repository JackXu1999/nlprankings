



















































Learning Bilingual Projections of Embeddings for Vocabulary Expansion in Machine Translation


Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 139–145,
Vancouver, Canada, August 3, 2017. c©2017 Association for Computational Linguistics

Learning Bilingual Projections of Embeddings
for Vocabulary Expansion in Machine Translation

Pranava Swaroop Madhyastha∗
Department of Computer Science

University of Sheffield
Sheffield, S1 4DP, UK

p.madhyastha@sheffield.ac.uk

Cristina España-Bonet∗
University of Saarland

DFKI, German Research Center
for Artificial Intelligence
Saarbrücken, Germany
cristinae@dfki.de

Abstract

We propose a simple log-bilinear softmax-
based model to deal with vocabulary ex-
pansion in machine translation. Our model
uses word embeddings trained on signif-
icantly large unlabelled monolingual cor-
pora and learns over a fairly small, word-
to-word bilingual dictionary. Given an
out-of-vocabulary source word, the model
generates a probabilistic list of possible
translations in the target language using
the trained bilingual embeddings. We
integrate these translation options into
a standard phrase-based statistical ma-
chine translation system and obtain con-
sistent improvements in translation qual-
ity on the English–Spanish language pair.
When tested over an out-of-domain test-
set, we get a significant improvement of
3.9 BLEU points.

1 Introduction

Data-driven machine translation systems are able
to translate words that have been seen in the train-
ing parallel corpora, however translating unseen
words is still a major challenge for even the best
performing systems. The amount of parallel data
is finite (and sometimes scarce) and, therefore,
word types like named entities, domain specific
content words, or infrequent terms are rare. This
lack of information can potentially result in in-
complete or erroneous translations.

This problem has been actively studied in
the field of machine translation (MT) (Habash,
2008; Daumé III and Jagarlamudi, 2011; Mar-
ton et al., 2009; Rapp, 1999; Dou and Knight,

∗ This work was done while the authors were in
TALP Research Center, Universitat Politècnica de Catalunya,
Barcelona.

2012; Irvine and Callison-Burch, 2013). Lexicon-
based resources have been used for resolving un-
seen content words by exploiting a combination
of monolingual and bilingual resources (Rapp,
1999; Callison-Burch et al., 2006; Zhao et al.,
2015). In this context, distributed word repre-
sentations, or word embeddings (WE), have been
recently applied to resolve unseen word related
problems (Mikolov et al., 2013b; Zou et al., 2013).
In general, word representations capture rich lin-
guistic relationships and several works (Gouws
et al., 2015; Wu et al., 2014) try to use them to im-
prove MT systems. However, very few approaches
use them directly to resolve the out-of-vocabulary
(OOV) problem in MT systems.

Previous research in MT systems suggests that
a significant number of named entities (NE) can
be handled by using simple pre or post-processing
methods, e.g., transliteration techniques (Herm-
jakob et al., 2008; Al-Onaizan and Knight, 2002).
However, a change in domain results in a signif-
icant increase in the number of unseen content
words for which simple pre or post-processing
methods are sub-optimal (Zhang et al., 2012).

Our work is inspired by the recent ad-
vances (Zou et al., 2013; Zhang et al., 2014) in
applications of word embeddings to the task of vo-
cabulary expansion in the context of statistical ma-
chine translation (SMT). Our focus in this paper
is to resolve unseen content words by using con-
tinuous word embeddings on both the languages
and learn a model over a small seed lexicon to
map the embedding spaces. To this extent, our
work is similar to Ishiwatari et al. (2016) where
the authors map distributional representations us-
ing a linear regression method similar to Mikolov
et al. (2013b) and insert a new feature based on
cosine similarity metric into the MT system. On
the other hand, there is a rich body of recent lit-
erature that focuses on obtaining bilingual word

139



embeddings using either sentence aligned or doc-
ument aligned corpora (Bhattarai, 2012; Gouws
et al., 2015; Kočiský et al., 2014). Our approach
is significantly different as we obtain embeddings
separately on monolingual corpora and then use
supervision in the form of a small sparse bilin-
gual dictionary, in some terms similar to Faruqui
and Dyer (2014). We use a simple yet principled
method to obtain a probabilistic conditional dis-
tribution of words directly and these probabilities
allow us to expand the translation model for new
words.

The rest of the paper is organised as follows.
Section 2 presents the log-bilinear softmax model,
and its integration into an SMT system. The ex-
perimental work is described in Section 3. Finally,
we conclude and sketch some avenues for future
work.

2 Mapping Continuous Word Represen-
tations using a Bilinear Model

Definitions. Let E and F be the vocabularies of
the two languages, source and target, and let e ∈ E
and f ∈ F be words in these languages respec-
tively. Let us assume, we have a source word to
target word e → f dictionary. We also assume
that we have access to some kind of distributed
word embeddings in both languages, φs for the
source and φt for the target, where φ(.)→ Rn de-
notes the n-dimensional distributed representation
of the words. The task we are interested in is to
learn a model for the conditional probability distri-
bution Pr(f |e). That is, given a word in a source
language, say English (e), we want to get a condi-
tional probability distribution of all the words in a
foreign language (f).

Log-Bilinear Softmax Model. We formulate
the problem as a bilinear prediction task as pro-
posed by Madhyastha et al. (2014a) and extend
it for the bilingual setting. The proposed model
makes use of word embeddings on both languages
with no additional features. The basic function
is formulated as log-bilinear softmax model and
takes the following form:

Pr(f |e;W ) = exp{φs(e)
>Wφt(f)}∑

f ′∈F exp{φs(e)>Wφt(f ′)}
(1)

Essentially, our problem reduces to: a) first ob-
taining the corresponding word embeddings of the
vocabularies from both the languages using a sig-

nificantly large monolingual corpus and b) esti-
matingW given a relatively small dictionary. That
is, to learn W we use the source word to target
word dictionary as training supervision. The dic-
tionary can be a true bilingual dictionary or the
word alignments generated by the SMT system,
therefore, no additional resources to the training
parallel corpus are needed.

We learn W by minimizing the negative log-
likelihood of the dictionary using a regularized
(relaxed low-rank regularization based) objective
as: L(W ) = −∑e,f log(Pr(f |e;W )) + λ‖W‖p.
λ is the constant that controls the capacity of W .
To find the optimum, we follow previous (Mad-
hyastha et al., 2014b) work and use an optimiza-
tion scheme based on Forward-Backward Splitting
(FOBOS) (Singer and Duchi, 2009).

We experiment with two regularization
schemes, p = 2 or the `2 regularizer and p = ∗
or the `∗ (nuclear norm) regularizer. We find
that both norms have approximately similar
performance, however the trace norm regularized
W has lower capacity and hence, smaller number
of parameters. This is also observed by (Bach,
2008; Madhyastha et al., 2014a,b). In general,
we can apply the ideas used by Mikolov et al.
(2013b) to speed up the training as this model is
equivalent to a softmax model. We can obtain
models with similar properties if we change the
loss from bilinear log softmax to a bilinear margin
based loss. We leave this exploration for future
work.

A by-product of regularizing with `∗ norm is
a lower-dimensional, language aligned, and com-
pressed embeddings for both languages. This is
possible because of the induced low-dimensional
properties of W . That is, assume W has rank k,
where k < n, such that W ≈ UkV >k , then the
product:

φs(e)>UkV >k φt(f) (2)

gives us φs(e)>Uk and V >k φt(f) compressed em-
beddings with shared properties. These are similar
to the CCA based projections obtained in Faruqui
and Dyer (2014).

Integrating the Probabilistic List into the SMT
System. We integrate the probabilistic list of
translation options into the phrase-based decoder
using the standard log-linear approach (Och and
Ney, 2002). Consider a word pair (e, f), where the
decoder searches for a foreign word f̂ that maxi-

140



Table 1: Top-10 accuracy (in percentage) for bilingual dictionary induction for English–German and
English–French.

Strong supervision Soft supervision

l1 l2 BiSkip BiCVM BiCCA BiVCD Ours-300 Ours-100

en de 79.7 74.5 72.4 62.5 73.8 71.1
en fr 78.9 72.9 70.1 68.8 72.1 69.7

mizes a linear combination of feature functions:

f̂ = argmaxf{
∑
λi log (hi(f, e)) + λoov log (Pr(f, e))}

here, λi is the weight associated with feature
hi(f, e) and λoov is the weight associated with the
unseen word.

3 Empirical Analysis

Quality of the Learned Embeddings. To un-
derstand the performance of the embedding pro-
jections in our model, we perform experiments to
compute the top-10 accuracy of our models in the
same setting provided in Upadhyay et al. (2016)
for cross-lingual dictionary induction1. The eval-
uation task judges how good cross-lingual embed-
dings are at detecting word pairs that are semanti-
cally similar across languages. Similarly to Upad-
hyay et al. (2016), we compare against BiSkip em-
beddings (Luong et al., 2015a), BiCVM (Hermann
and Blunsom, 2014), BiCCA (Faruqui and Dyer,
2014) and BiVCD (Vulic and Moens, 2015). We
experiment with English–German and English–
French language pairs, so that we can induce the
dictionaries for the five systems. As seen in Ta-
ble 1, our full 300-dimensional embeddings per-
form better than the BiCCA-based model, whereas
100-dimensional compressed embedding perform
slightly worse, but still are competitive. Since our
model and BiCCA use similar supervision, we ob-
tain similar results and differ in a similar way to
those that use stronger supervision like BiCVM
and BiSkip based embeddings.

MT Data and System Settings. For estimating
the monolingual WE, we use the CBOW algo-
rithm as implemented in the Word2Vec pack-
age (Mikolov et al., 2013a) using a 5-token win-
dow. We obtain 300 dimension vectors for English
and Spanish from a Wikipedia dump of 2015 and
the Quest data2. The final corpus contains 2.27 bil-

1We also used the script provided here: https://
github.com/shyamupa/biling-survey

2http://statmt.org/˜buck/wmt13qe/
wmt13qe_t13_t2_MT_corpus.tgz

lion tokens for English and 0.84 for Spanish. We
remove any occurrence of sentences from the test
set that are contained in our corpus. The coverage
in our test sets is of 97% of the words.

To train the log-bilinear softmax based model,
we use the dictionary from the Apertium
project3 (Forcada et al., 2011). The dictionary
contains 37651 words, 70% of them are used for
training and 30% as a development set for model
selection. The average precision @1 is 86% for
the best model over the development set.

A state-of-the-art phrase-based SMT system is
trained on the Europarl corpus (Koehn, 2005)
for the English-to-Spanish language pair. We
use a 5-gram language model that is estimated
on the target side of the corpus using interpo-
lated Kneser-Ney discounting with SRILM (Stol-
cke, 2002). Additional monolingual data available
within Quest corpora is used to build a larger lan-
guage model with the same characteristics. Word
alignment is done with GIZA++ (Och and Ney,
2003) and both phrase extraction and decoding
are done with the Moses package (Koehn et al.,
2007). At decoding time, Moses allows to in-
clude additional translation pairs with their associ-
ated probabilities to selected words via xml mark-
up. We take advantage of this feature to add our
probabilistic estimations to each OOV. Since, by
definition, OOV words do no appear in the par-
allel training corpus, they are not present in the
translation model either and the new translation
options only interact with the language model.
The optimization of the weights of the model with
the additional translation options is trained with
MERT (Och, 2003) against the BLEU (Papineni
et al., 2002) evaluation metric on the NewsCom-
mentaries 20124 (NewsDev) set. We test our sys-
tems on the NewsCommentaries 2013 set (New-
sTest) for an in-domain evaluation and on a test set

3The bilingual dictionary can be downloaded here:
http://goo.gl/TjH31q.

4http://www.statmt.org/wmt13/
translation-task.html

141



Table 2: OOVs on the dev and test sets.

Sent. Tokens OOVall OOVCW

NewsDev 3003 72988 1920 (2.6%) 378 (0.5%)
NewsTest 3000 64810 1590 (2.5%) 296 (0.5%)
WikiTest 500 11069 798 (7.2%) 201 (1.8%)

extracted from Wikipedia by Smith et. al. (2010)
for an out-of-domain evaluation (WikiTest).

The domainess of the test set is established with
respect to the number of OOVs. Table 2 shows the
figures of these sets paying special attention to the
OOVs in the basic SMT system. Less than a 3%
of the tokens are OOVs for News data (OOVall),
whereas it is more than a 7% for Wikipedia’s. In
our experiments, we distinguish between OOVs
that are named entities and the rest of content
words (OOVCW). Only about 0.5% (NewsTest)
and 1.8% (WikiTest) of the tokens fall into this
category, but we show that they are relevant for
the final performance.

MT Experiments. We consider two baseline
systems, the first one does not output any trans-
lation for OOVs (noOOV), it just ignores the to-
ken; the second one outputs a verbatim copy of
the OOV as a translation (verbatimOOV). Table 3
shows the performance of these systems under
three widely used evaluation metrics TER (Snover
et al., 2006), BLEU and METEOR (MTR) (Baner-
jee and Lavie, 2005). Including the verbatim copy
improves all the lexical evaluation metrics. Spe-
cially for NEs and acronyms (the 80% of OOVs
in our sets), this is a hard baseline to be compared
to as in most cases the same word is the correct
translation.

We then enrich the systems with information
gathered from the large monolingual corpora in
two ways, using a bigger language model (BLM)
and using our newly proposed log-bilinear model
that uses word embeddings (BWE). BLMs are im-
portant to improve the fluency of the translations,
however they may not be helpful for resolving
OOVs as they can only promote translations avail-
able in the translation model. On the other hand,
BWEs are important to make available to the de-
coder new vocabulary on the topic of the otherwise
OOVs. Given the large percentage of NEs in the
test sets (Table 2), our models add the source word
as an additional option to the list of target words
to mimic the verbatimOOV system.

Table 3 includes seven systems with the addi-

Table 3: Automatic evaluation of the translation
systems defined in Section 3. The best system is
bold-faced (see text for statistical significance).

NewsTest WikiTest
TER BLEU MTR TER BLEU MTR

noOOV 58.21 21.94 45.79 61.26 16.24 38.76
verbatimOOV 57.90 22.89 47.06 58.55 21.90 45.77

BWE 58.33 22.23 45.76 58.38 21.96 44.84
BWECW50 57.66 23.09 47.14 56.19 24.16 48.49
BWECW10 57.85 23.06 47.11 55.64 24.71 49.05
BLM 55.37 25.83 49.19 52.60 30.63 51.04
BLM+BWE 55.89 24.92 47.84 51.02 32.20 52.09
BLM+BWE50 55.55 25.61 49.01 49.50 33.94 54.93
BLM+BWE10 55.31 25.86 49.04 49.12 34.58 55.52

tional monolingual information. Three of them
add, at decoding time, the top-n translation op-
tions given by the BWE for a OOV. BWE sys-
tem uses the top-50 for all the OOVs, BWECW50
also uses the top-50 but only for content words
other than named entities5, and BWECW10 limits
the list to 10 elements. BLM is the same as the
baseline system verbatimOOV but with the large
language model. BLM+BWE, BLM+BWE50 and
BLM+BWE10 combine the three BWE systems
with the large language model.

In the NewsTest, most of unseen words are
named entities and using BWEs to translate them
barely improves the translation. The reason is
that embeddings of related NEs are usually equiv-
alent. This affects the overall integration of the
scores into the decoder and induces ambiguity in
the system. However, we observe that the decoder
benefits from the information on content words,
specially for the out-of-domain WikiTest set. In
this case, given the constrained list of alternative
translations (BWECW10) one achieves 2.75 BLEU
points of improvement.

The addition of the large language model im-
proves the results significantly. When combined
with the BWEs we observe that the BWEs clearly
help in the translation of WikiTest but do not seem
as relevant in the in-domain set. We achieve a sta-
tistically significant improvement of 3.9 points of
BLEU with the BLM and BWE combo system –
BLM+BWE10 with respect to BLM– in WikiTest
(p<0.001); the improvement in the NewsTest is
not statistically significant (p-value=0.29). The
number of translation options in the list is also

5We consider a named entity any word that begins with
a capital letter and is not after a punctuation mark, and any
fully capitalized word.

142



Table 4: Top-n list of translations obtained with
the bilingual embeddings.

GALAXY NYMPHS STUART FOLKSONG

galaxia ninfas William música
planeta ninfa Henry folclore
universo crı́as John literatura
planetas diosa Charles himno
galaxias dioses Thomas folklore
... ... Estuardo (#48) canción (#7)

relevant, and for BLM+BWECW50 we have a sig-
nificant but smaller improvement of 3.3 points on
BLEU in WikiTest. All these results are consistent
among different evaluation metrics.

In order to estimate the relevance of the bilin-
gual embeddings into the final translation, we have
manually evaluated the translation of WikiTest us-
ing the BWECW50 model. For the translation of
the OOVs, we obtain an accuracy of a 68%, that
is, the BWE gives the correct translation option at
least 68% of the times. We note that, even if the
correct translation option is in the translation list
obtained by the BWE, the decoder may choose not
to consider it.

In general, we observe that when our model
fails, in most of the cases, the words in the trans-
lated language happened to be either a multiword
expression or a named entity. In Table 4 we
present some of the these examples. The first two
examples galaxy and nymphs are nouns where we
obtain the first option as the correct translation.
The problem is harder for named entities as we
observe in the table, the name Stuart in English
has William as most probable translation in Span-
ish, the correct translation Estuardo however ap-
pears as the 48th choice. Our model is also un-
able to generate multiword expressions, as shown
in the table for the english word folksong, the cor-
rect translation being canción folk. This would
need two words in Spanish in order to be translated
correctly, however, our model does obtain words:
canción and folclore as the most probable transla-
tion options.

4 Conclusions

We have presented a method for resolving OOVs
in SMT that performs vocabulary expansion by
using a simple log-bilinear softmax based model.
The model estimates bilingual word embeddings
and, as a by-product, generates low-dimensional
compressed embeddings for both languages.The

addition of the new translation options to a mere
1.8% of the words has allowed the system to ob-
tain a relative improvement of a 13% in BLEU
(3.9 points) for out-of-domain data. For in-domain
data, where the number of content words is small,
improvements are more moderate.

The analysis of the results shows how the per-
formance is damaged by not considering multi-
word expressions. The automatic detection of
these elements in the monolingual corpus together
with the addition of one-to-many dictionary en-
tries for learning the W matrix can alleviate this
problem and will be considered in future work.

We also note that this approach can be ex-
tended directly within neural machine translation
systems, where its effects could be even larger
due to the limited vocabulary. While one of the
popular approaches to deal with OOVs is to use
subword units (Sennrich et al., 2016) in order to
resolve of unknown words, dictionary-based ap-
proaches, where an unknown word is translated by
its corresponding translation in a dictionary or a
(SMT) translation table, have also been used (Lu-
ong et al., 2015b). Our method can go further in
the latter direction by learning correspondences of
source and target vocabularies using large mono-
lingual corpora and either a small dictionary or the
word alignments.

References

Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Pro-
ceedings of the ACL-02 workshop on Computational
approaches to semitic languages. Association for
Computational Linguistics, pages 1–13.

Francis R Bach. 2008. Consistency of the group lasso
and multiple kernel learning. The Journal of Ma-
chine Learning Research 9:1179–1225.

Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization. Association for Com-
putational Linguistics, Ann Arbor, Michigan, pages
65–72.

Alexandre Klementiev Ivan Titov Binod Bhattarai.
2012. Inducing Crosslingual Distributed Represen-
tations of Words. In Proceedings of COLING 2012.
The COLING 2012 Organizing Committee, Mum-
bai, India, pages 1459–1474.

143



Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved Statistical Machine Transla-
tion Using Paraphrases. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics. HLT-NAACL ’06, pages 17–24.

Hal Daumé III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining
unseen words. In Association for Computational
Linguistics. Portland, OR, pages 407–412.

Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning. Asso-
ciation for Computational Linguistics, pages 266–
275.

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, EACL 2014, April 26-
30, 2014, Gothenburg, Sweden. pages 462–471.

Mikel L Forcada, Mireia Ginestı́-Rosell, Jacob Nord-
falk, Jim ORegan, Sergio Ortiz-Rojas, Juan An-
tonio Pérez-Ortiz, Felipe Sánchez-Martı́nez, Gema
Ramı́rez-Sánchez, and Francis M Tyers. 2011.
Apertium: a free/open-source platform for rule-
based machine translation. Machine translation
25(2):127–144.

Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. BilBOWA: Fast Bilingual Distributed Rep-
resentations without Word Alignments. In Proceed-
ings of the 32nd International Conference on Ma-
chine Learning, ICML 2015, Lille, France, 6-11 July
2015. pages 748–756.

Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In ACL
2008, Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics. pages
57–60.

Karl Moritz Hermann and Phil Blunsom. 2014. Multi-
lingual models for compositional distributed seman-
tics. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics. As-
sociation for Computational Linguistics, Baltimore,
Maryland, pages 58–68.

Ulf Hermjakob, Kevin Knight, and Hal Daumé III.
2008. Name translation in statistical machine trans-
lation: Learning when to transliterate. In Proceed-
ings of ACL-08: HLT . pages 389–397.

Ann Irvine and Chris Callison-Burch. 2013. Combin-
ing bilingual and comparable corpora for low re-
source machine translation. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion. pages 262–270.

Shonosuke Ishiwatari, Naoki Yoshinaga, Masashi Toy-
oda, and Masaru Kitsuregawa. 2016. Instant transla-
tion model adaptation by translating unseen words in
continuous vector space. In Proceedings of the 17th
International Conference on Intelligent Text Pro-
cessing and Computational Linguistics (CICLing
2016). Konya, Turkey.

Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Conference Pro-
ceedings: the Tenth Machine Translation Summit.
AAMT, AAMT, Phuket, Thailand, pages 79–86.

Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Annual Meeting of the Association
for Computation Linguistics (ACL), Demonstration
Session. pages 177–180.

Tomáš Kočiský, Karl Moritz Hermann, and Phil Blun-
som. 2014. Learning bilingual word representations
by marginalizing alignments. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, Baltimore, Maryland, pages 224–229.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015a. Bilingual word representations with
monolingual quality in mind. In Proceedings of the
1st Workshop on Vector Space Modeling for Natural
Language Processing. pages 151–159.

Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,
and Wojciech Zaremba. 2015b. Addressing the
rare word problem in neural machine translation.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers). Associ-
ation for Computational Linguistics, Beijing, China,
pages 11–19.

Pranava Swaroop Madhyastha, Xavier Carreras, and
Ariadna Quattoni. 2014a. Learning task-specific
bilexical embeddings. In Proceedings of COLING
2014, the 25th International Conference on Compu-
tational Linguistics: Technical Papers. Dublin City
University and Association for Computational Lin-
guistics, pages 161–171.

Pranava Swaroop Madhyastha, Xavier Carreras, and
Ariadna Quattoni. 2014b. Tailoring word embed-
dings for bilexical predictions: An experimental
comparison. International Conference on Learning
Representations 2015, Workshop Track .

Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, pages 381–390.

144



Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of Work-
shop at ICLR.

Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013b. Exploiting similarities among languages
for machine translation. CoRR abs/1309.4168.
http://arxiv.org/abs/1309.4168.

Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the Association for Computational Linguis-
tics. Sapporo, Japan, pages 160–167.

Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics. Association for Computational
Linguistics, Stroudsburg, PA, USA, ACL 2002,
pages 295–302.

Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics 29(1):19–51.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the Association of Computational Linguis-
tics. pages 311–318.

Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics. Association for Compu-
tational Linguistics, pages 519–526.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers. pages 1715–1725.

Yoram Singer and John C Duchi. 2009. Efficient learn-
ing using forward-backward splitting. In Advances
in Neural Information Processing Systems. pages
495–503.

Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting Parallel Sentences from Compara-
ble Corpora Using Document Level Alignment. In
In Proceedings of Human Language Technologies:
The 11th Annual Conference of the North American
Cha pter of the Association for Computational Lin-
guistics (NAACL-HLT 2010). pages 403–411.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of the Seventh Conference
of the Association for Machine Translation in the
Americas (AMTA 2006). Cambridge, Massachusetts,
USA, pages 223–231.

Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the Sev-
enth International Conference of Spoken Language
Processing (ICSLP2002). Denver, Colorado, USA,
pages 901–904.

Shyam Upadhyay, Manaal Faruqui, Chris Dyer, and
Dan Roth. 2016. Cross-lingual models of word em-
beddings: An empirical comparison. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics, ACL 2016, August 7-12,
2016, Berlin, Germany. pages 1661–1670.

Ivan Vulic and Marie-Francine Moens. 2015. Bilingual
Word Embeddings from Non-Parallel Document-
Aligned Data Applied to Bilingual Lexicon Induc-
tion. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Nat-
ural Language Processing of the Asian Federation
of Natural Language Processing, ACL 2015. pages
719–725.

Haiyang Wu, Daxiang Dong, Xiaoguang Hu, Dian-
hai Yu, Wei He, Hua Wu, Haifeng Wang, and Ting
Liu. 2014. Improve Statistical Machine Translation
with Context-Sensitive Bilingual Semantic Embed-
ding Model. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP). Association for Computational
Linguistics, Doha, Qatar, pages 142–146.

Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics. Association
for Computational Linguistics, Baltimore, Mary-
land, pages 111–121.

Jiajun Zhang, Feifei Zhai, and Chengqing Zong.
2012. Handling unknown words in statistical ma-
chine translation from a new perspective. In Natu-
ral Language Processing and Chinese Computing,
Springer, pages 176–187.

Kai Zhao, Hany Hassan, and Michael Auli. 2015.
Learning Translation Models from Monolingual
Continuous Representations. In The 2015 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, NAACL HLT 2015. pages
1527–1536.

Will Y. Zou, Richard Socher, Daniel M. Cer, and
Christopher D. Manning. 2013. Bilingual Word Em-
beddings for Phrase-Based Machine Translation. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2013. pages 1393–1398.

145


