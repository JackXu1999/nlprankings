



















































Language-Guided Adaptive Perception for Efficient Grounded Communication with Robotic Manipulators in Cluttered Environments


Proceedings of the SIGDIAL 2018 Conference, pages 151–160,
Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics

151

Language-Guided Adaptive Perception for Efficient Grounded
Communication with Robotic Manipulators in Cluttered Environments

Siddharth Patki
University of Rochester

Rochester, NY, 14627, USA
spatki@ur.rochester.edu

Thomas M. Howard
University of Rochester

Rochester, NY, 14627, USA
thoward@ece.rochester.edu

Abstract

The utility of collaborative manipulators
for shared tasks is highly dependent on
the speed and accuracy of communication
between the human and the robot. The
run-time of recently developed probabilis-
tic inference models for situated symbol
grounding of natural language instructions
depends on the complexity of the repre-
sentation of the environment in which they
reason. As we move towards more com-
plex bi-directional interactions, tasks, and
environments, we need intelligent percep-
tion models that can selectively infer pre-
cise pose, semantics, and affordances of
the objects when inferring exhaustively
detailed world models is inefficient and
prohibits real-time interaction with these
robots. In this paper we propose a model
of language and perception for the prob-
lem of adapting the configuration of the
robot perception pipeline for tasks where
constructing exhaustively detailed models
of the environment is inefficient and in-
consequential for symbol grounding. We
present experimental results from a syn-
thetic corpus of natural language instruc-
tions for robot manipulation in example
environments. The results demonstrate
that by adapting perception we get signifi-
cant gains in terms of run-time for percep-
tion and situated symbol grounding of the
language instructions without a loss in the
accuracy of the latter.

1 INTRODUCTION

Perception is a critical component of an intelli-
gence architecture that converts raw sensor obser-
vations to a suitable representation for the task

that the robot is to perform. Models of environ-
ments vary significantly depending on the applica-
tion. For example, a robotic manipulator may need
to model the objects in its environment with their
six degree-of-freedom pose for grasping and dex-
terous manipulation tasks, whereas a self-driving
car may need to model the dynamics of the envi-
ronment in addition to domain-specific semantics
such as stop signs, sidewalks and pedestrians etc.
to safely navigate through the environment.

The ability of robots to perform complex tasks
is linked to the richness of the robot’s world
model. As inferring exhaustively detailed world
representations is impractical, it is common to in-
fer representations which are highly specific to the
task that the robot is to perform. However, in
collaborative domains as we move towards more
complex bi-directional interactions, manipulation
tasks, and the environments, it becomes unclear
how to best represent the environment in order to
facilitate planning and reasoning for a wide distri-
bution of tasks. As shown in the Figure 1, mod-
eling the affordance between the chips can and its
lid would be unnecessary for the task of picking up
the mustard sauce bottle and vice versa. Inferring
exhaustively detailed models of all of the objects
in the environment is computationally expensive
and inconsequential for the individual tasks, and
inhibits real-time interaction with these collabora-
tive robots.

The utility of collaborative manipulators is also
highly dependent on the speed and accuracy of
communication between the human operator and
the robot. Natural language interfaces provide in-
tuitive and muti-resolution means to interact with
the robots in shared realms. In this work, we pro-
pose learning a model of language and percep-
tion that can adapt the configurations of the per-
ception pipeline according to the task in order to
infer representations that are necessary and suffi-



152

Figure 1: On the left is an image showing the Baxter Research Robot in a cluttered tabletop environment
in the context of collaborative human-robot tasks. A perception system that does not use the context of
the instruction when interpreting the observations would inefficiently construct detailed world model
that is only partially utilized by the symbol grounding algorithm. On the right are the adaptively inferred
representations using our proposed language perception model for the instructions, “pick up the leftmost
blue gear” and “pick up the largest red object” respectively.

cient to facilitate planning and grounding for the
intended task. e.g. the top-right image in the Fig-
ure 1 shows the adaptively inferred world model
pertaining to the instruction “pick up the leftmost
blue gear” which is different than the one inferred
for the instruction “pick up the largest red object”.

2 BACKGROUND

The algorithms and models presented in this paper
span the topics that include robot perception and
natural language understanding for human-robot
interaction. Perception is a central problem in
the the field of situated robotics. Consequently, a
plenty of research has focused on developing rep-
resentations that can faciliate planning and reason-
ing for highly specific situated tasks. These repre-
sentations vary significantly depending on the ap-
plication, from two-dimensional costmaps (Elfes,
1987), volumetric 3D voxel representations (Hor-
nung et al., 2013, 2010), primitive shape based
object approximations (Miller et al., 2003; Hueb-
ner and Kragic, 2008) to more rich representations
that model high level semantic properties (Galindo
et al., 2005; Pronobis and Jensfelt, 2012), 6 DOF
pose of the objects of interest (Hudson et al., 2012)
or affordances between objects (Daniele et al.,
2017). Since inferring exhaustively detailed world
models is impractical, one solution is to design
perception pipelines that infer task relevant world
models (Eppner et al., 2016; Fallon et al., 2014).
Inferring efficient models that can support reason-

ing and planning for a wide distribution of tasks
remains an open research question.

Natural language interfaces provides intutive
and multi-resolution means to interact with the
collaborative robots. Contemporary models
(Tellex et al., 2011; Howard et al., 2014; Boular-
ias et al., 2015; Matuszek et al., 2013) frame the
problem of language understanding as a symbol
grounding problem (Harnad, 1990). Specifically,
of inferring correspondences between the linguis-
tic constituents of the instruction and the symbols
that represent perceived entities in the robot’s en-
vironment such as objects and regions or desired
actions that the robot can take. (Howard et al.,
2014) frames this problem as one of inference in a
probabilistic graphical model called a Distributed
Correspondence Graph (DCG). This model lever-
ages the hierarchical structure of the syntactically
parsed instruction and conditional independence
assumptions across constituents of a discrete sym-
bol space to improve the run-time of probabilis-
tic inference. Other variations include the Hier-
archical DCG (Propp et al., 2015) and Adaptive
DCG (Paul et al., 2016) to further improve the
run-time performance in cluttered environments
with known environment models. Recently, these
models have been used to augment perception and
representations. (Daniele et al., 2017) uses DCG
for supplementing perception with linguistic in-
formation for efficiently inferring kinematic mod-
els of articulated objects. (Duvallet et al., 2014;



153

Hemachandra et al., 2015) use DCG to augment
the representations by exploiting information in
language instruction to build priors over the un-
known parts of the world. A limitation of cur-
rent applications of probabilistic graphical models
for natural language symbol grounding is that they
do not consider how to efficiently convert obser-
vations or measurements into sufficiently detailed
representation suitable for inference. We propose
to use DCG for the problem of adapting the per-
ception pipelines for inferring task optimal repre-
sentations.

Our work is most closely related to that of
(Matuszek et al., 2013). Their work presents an
approach for jointly learning the language and
perception models for grounded attribute learn-
ing. Their model infers the subset of objects
based on color and shape which satisfy the at-
tributes described in the natural language descrip-
tion. Similarly, (Hu et al., 2016) proposes deep
learning based approach to directly segment ob-
jects in RGB images that are described by the in-
struction. We differentiate our approach by ex-
panding the diversity and complexity of percep-
tual classifiers, enabling verbs to modify object
representations, and presenting an end-to-end ap-
proach to representation adaptation and symbol
grounding using computationally efficient proba-
bilistic graphical models. In the following sections
we introduce our approach to adapting perception
pipelines, define our experiments, and present re-
sults against a suitable baseline.

3 TECHNICAL APPROACH

We describe the problem of understanding natural
language instructions as one of probabilistic in-
ference where we infer a distribution of symbols
that express the intent of the utterance. The mean-
ing of the instruction is taken in the context of a
symbolic representation (Γ), observations (zt) and
a representation of the language used to describe
the instruction (Λ). A probabilistic inference us-
ing a symbolic representation that is described by
the space of trajectories X (t) that the robot may
take takes the form of equation:

x(t)∗ = arg max
x(t)∈X(t)

p(x(t)|Λ, zt) (1)

Solving this inference problem is computation-
ally intractable when the space of possible trajec-
tories is large. Contemporary approaches (Tellex

et al., 2011; Howard et al., 2014) frame this prob-
lem as a symbol grounding problem, i.e. inferring
the most likely set of groundings (Γs∗) given a
syntactically parsed instruction Λ = {λ1, ..., λm}
and the world model Υ.

Γs∗ = arg max
γ1...γn∈Γs

p(γ1...γn|Λ,Υ) (2)

Here, the world model Υ is a function of the
constructs of the robot’s perception pipeline (P ),
and the raw observations zt.

Υ ≈ f(P, zt) (3)

The groundings Γs are symbols that represent
objects, their semantic properties, regions derived
from the world model, and robot actions and goals
such as grasping the object of interest or navigat-
ing to a specific region in the environment. The set
of all groundings Γs = {γ1, γ2, ..., γn} is called as
the symbol space. Thus the symbol space forms
a finite space of interpretations in which the in-
struction will be grounded. The DCG is a prob-
abilistic graphical model of the form described in
equation 2. The model relates the linguistic com-
ponents λi ∈ Λ to the groundings γj ∈ Γs through
the binary correspondence variables φij ∈ Φ.
DCG facilitates inferring the groundings at a par-
ent phrase in the context of the groundings at its
child phrases Φci. Formally, DCG searches for
the most likely correspondence variables Φ∗ in the
context of the groundings γij , phrases λi, child
correspondences Φci and the world model Υ by
maximizing the product of individual factors.

Φ∗ = arg max
φij∈Φ

|Λ|∏
i=1

|Γs|∏
j=1

p(φij |γij , λi,Φci,Υ) (4)

Inferred correspondence variables Φ∗ represent
the expression of the most likely groundings Γs∗.
The factors in the equation 4 are approximated by
log-linear models Ψ:

Φ∗ = arg max
φij∈Φ

|Λ|∏
i=1

|Γs|∏
j=1

Ψ(φij , γij , λi,Φci,Υ)

(5)
Model training involves learning the log-linear

factors from the labeled data relating phrases
with true groundings. Inference process involves
searching for the set of correspondence variables
that satisfy the above equation. The run-time per-
formance of probabilistic inference with the DCG



154

is positively correlated with the complexity of
the world model Υ. This is because the size of
the symbolic representation Γs increases with the
number of objects in the environment representa-
tion. Recognizing that some objects (and the sym-
bols based on those objects) are inconsequential to
the meaning of the instruction, we consider the op-
timal representation of the environment Υ∗ as one
which is necessary and sufficient to solve equa-
tion 5. Thus we hypothesize that the time to solve
equation 6 will be less than that for the equation 5.

Φ∗ = arg max
φij∈Φ

|Λ|∏
i=1

|Γs|∏
j=1

Ψ(φij , γij , λi,Φci,Υ
∗)

(6)
Typically the environment model Υ is com-

puted by a perception module P from a set of
observations z1:t = {z1 . . . zt}. In cluttered en-
vironments we assume that inferring an exhaus-
tively detailed representation of the world that sat-
isfies all possible instructions is impractical for
real-time human-robot interactions. We propose
using language as mean to guide the generation
of these necessary and sufficient environment rep-
resentations Υ∗ in turn making it a task adaptive
process. Thus we define Υ∗ inferred from a single
observation as:

Υ∗ ≈ f(P, zt,Λ) (7)

where P denotes the perception pipeline of the
robotic intelligence architecture. We adapt DCG
to model the above function by creating a novel
class of symbols called as perceptual symbols ΓP .
Perceptual symbols are tied to their corresponding
elements in the perception pipeline. i.e. to the vi-
sion algorithms. Since this grounding space is in-
dependent of the world model Υ, the random vari-
able used to represent the environment is removed
from equation 5. We add a subscript p to denote
that we are reasoning in the perceptual grounding
space.

Φ∗ = arg max
φij∈Φ

|Λ|∏
i=1

|ΓP |∏
j=1

Ψ(φij , γij , λi,Φci) (8)

Equation 8 represents the proposed model
which we refer to as the language-perception
model (LPM). It infers the symbols that inform
the perception pipeline configurations given a nat-
ural language instruction describing the task. The

space of symbols ΓP describe all possible configu-
rations of the perception pipeline. For example, as
shown in the Figure 1, for the instruction “pick up
the leftmost blue gear”, we may need elements in
our pipeline that can detect blue objects and gears.
Detecting green objects, spherical shapes, or six-
dimensional pose of the chips can object would not
be necessary to generate the symbols necessary for
the robot to perform the instruction.

We assume that the perception pipeline (P ) is
populated with a set of elements E = {E1, ..., En}
such that each subset Ei ∈ E represents a set
of algorithms that are responsible for inferring a
specific property of an object. e.g. a red color-
detection algorithm would be a member of the
color detector family responsible for inferring the
semantic property “color” of the object. While a
six degree-of-freedom (DOF) pose detection al-
gorithm would be a member of the pose detec-
tor family. More generally, E can be defined as:
E = {e1, e2, ..., em}. With these assumptions, we
define our independent perceptual symbols as:

ΓIDP = {γei |ei ∈ E} (9)

We can imagine that these symbols would be
useful to ground simple phrases such as ”the red
object” or ”the ball” etc. where the phrases re-
fer to a single property of the object. In the more
complicated phrases such as ”the red ball” or ”the
blue box” we have a joint expression of proper-
ties. i.e. we are looking for objects which maxi-
mize the joint likelihood p(red, sphere|o). Since
these properties are independent we can infer them
separately for every object ok ∈ O. However, we
can represent the above joint likelihood expression
as p(red, sphere) = p(red)p(sphere|red). In
this case, it allows conditioning the evaluation of
sphere detection on only a subset of objects which
were classified as being red by the red detector. To
add this degree of freedom in the construction of
the perception pipeline, we define additional set of
symbols which we refer to as conditionally depen-
dent perceptual symbols:

ΓCDP = {γei,ej |ei, ej ∈ E ; i 6= j} (10)

The expression of the symbol γei,ej refers
to running the element ei from the perception
pipeline on the subset of objects which were clas-
sified positive by the element ej . Finally the com-
plete perceptual symbol space is:

ΓP = {ΓIDP ∪ ΓCDP } (11)



155

4 EXPERIMENTAL DESIGN

Herein with our experiments we demonstrate the
utility of our language perception model for the
task of grounded language understanding of the
manipulation instructions. As shown in Figure 3
the process involves two distinct inferences: Infer-
ring the perceptual groundings given a language
instruction ( eq. 8 ), and inferring high level mo-
tion planning constraints given the language and
the generated world model ( eq. 5 and eq. 6 ). In
this section we describe our assumptions, and de-
fine the distinct symbolic representations used in
our experiments for each of the above tasks. We
then discuss our instruction corpus and the details
of the individual experiments.

Robot and the Environment
For our experiments a Rethink Robotics Baxter
Research Robot is placed behind a table. The
robot is assumed to perceive the environment us-
ing a head-mounted RGB-D sensor. Robot’s work
space is populated using objects from the stan-
dard YCB dataset (Berk Calli, 2017), custom 3D
printed ABS plastic objects, and multicolored rub-
ber blocks. We define the world complexity in
terms of the number of objects present on the table
in the robot’s field of view. The world complexity
ranges from 15 to 20 in our experiments.

Symbolic Representation
The symbolic representation defines the space of
symbols or meanings in which the natural lan-
guage instruction will be grounded or understood.
As mentioned before we define two distinct sets
of symbols in our experiments. ΓP defines the set
of perceptual symbols which are used by the lan-
guage perception model, and ΓS defines the set of
symbols which are used by the symbol grounding
model.

ΓP is a function of the elements E of the per-
ception pipeline. The elements ei ∈ E in our
perception pipeline are selected such that they can
model the robot’s environment with a spectrum of
semantic and metric properties which will be nec-
essary towards performing symbol grounding and
planning for all of the instructions in our corpus.
In our experiment we define E as:

E = {C ∪ G ∪ L ∪ B ∪ R ∪ P} (12)

Here, C is a set of color detectors, G is a set of
geometry detectors, L is a set of object label de-
tectors, B is a set of bounding box detectors, R

is a set of region detectors, and P is a set of pose
detectors.

C = { cdi | i ∈ color}
G = { gdi | i ∈ geometry}
L = { ldi | i ∈ label}
B = { bdi | i ∈ bbox}
R = { rdi | i ∈ region}
P = { pdi | i ∈ pose}

(13)

where color = {red, green, blue, white, yellow, or-
ange}, geometry = {sphere, cylinder, cuboid}, la-
bel = {crackers box, chips can, pudding box, mas-
ter chef can, bleach cleanser, soccer ball, mustard
sauce bottle, sugar packet}, bbox = {non-oriented,
oriented }, region = {left, right, center}, pose = {
3 DOF, 6 DOF }. Given the perception elements
defined in the equation 13, we define the indepen-
dent perceptual groundings ( ΓIDP ) previously de-
fined in equation 9 as follows:

ΓC = {γcdi | cdi ∈ C}
ΓG = {γgdi | gdi ∈ B}
ΓL = {γldi | ldi ∈ L}
ΓB = {γbdi | bdi ∈ B}
ΓR = {γrdi | rdi ∈ R}
ΓP = {γpdi | pdi ∈ P}

(14)

ΓIDP = { ΓC ∪ ΓG ∪ ΓL ∪ ΓB ∪ ΓR ∪ ΓP} (15)

We define the conditionally dependent percep-
tual groundings ( ΓCDP ) previously defined in equa-
tion 10 as following:

ΓGC = {γ(gdi,cdj) | gdi ∈ G, cdj ∈ C}
ΓLC = {γ(ldi,cdj) | ldi ∈ L, cdj ∈ C}
ΓPC = {γ(pdi,cdj) | pdi ∈ P, cdj ∈ C}
ΓPG = {γ(pdi,gdj) | pdi ∈ P, gdj ∈ G}
ΓPL = {γ(pdi,ldj) | pdi ∈ P, ldj ∈ L}

(16)

ΓCDP = { ΓGC ∪ ΓLC ∪ ΓPC ∪ ΓPG ∪ ΓPL} (17)

These symbols provide us the ability to selec-
tively infer desired properties in the world. Above
presented independent and conditionally depen-
dent symbols together cover the complete space
of perceptual symbols used by the LPM:

ΓP = {ΓIDP ∪ ΓCDP } (18)



156

Algorithmic details of the percepion elements
are as follows : A single RGB point cloud is fed
in as a raw sensor observation to the pipeline. A
RANSAC (Fischler and Bolles, 1981) based 3D
plane detection technique is used for segmenting
the table-top and the objects. HSV colorspace is
used for detecting colors. RANSAC based model
fitting algorithms form the core of the geometry
detectors. A 4 layer ( 256 - 128 - 64 - 32 ) feed for-
ward neural network is trained to infer the seman-
tic labels of the objects. It takes in a 32 x 32 RGB
image and infers a distribution over 8 unique YCB
object classes. A PCA based oriented bounding
box estimation algorithm is used to approximate
the 6 DOF pose for the individual objects. Algo-
rithms are implemented using OpenCV and PCL
library (Rusu and Cousins, 2011).

The space of symbols for the symbol ground-
ing model is similar to the representation defined
in (Paul et al., 2016). This space uses symbols
to represent objects in the world model (ΓO), se-
mantic object labels (ΓL), object color(ΓC), object
geometry(ΓG) regions in the world(ΓR), spatial re-
lationships (ΓSR) and finally high level planning
constraints that define the end goal (ΓPC). The
inferred constraints forms an input to a planning
algorithm that can then generate trajectories to ac-
complish the desired task. Thus the complete sym-
bol space for the symbol grounding model is:

ΓS = { ΓO∪ΓL∪,ΓC∪ΓG∪ΓR∪ΓSR∪ΓPC} (19)

Corpus
For training and testing the performance of the
system we generate an instruction corpus using
the linguistic patterns similar to that described in
(Paul et al., 2016). The corpus used in our experi-
ments consists of 100 unique natural language in-
structions. Details of the grammar extracted from
this corpus is described in the appendix. Each
instruction describes a manipulation command to
the robot while referring to the objects of inter-
est using their semantic or metric properties. e.g.
“pick up the green cup” or “pick up the biggest
blue object”. If multiple instances of the same
objects are present in the robot’s work space then
the reference resolution is achieved by using spa-
tial relationships to describe the object of interest.
e.g.“the leftmost blue cube” or “rightmost red ob-
ject” etc.

As shown in Figure 2, the instructions in the
corpus are in the form of syntactically parsed trees.

Figure 2: Syntactically parsed tree for the instruc-
tion ”pick up the leftmost red block”.

Each instruction is generated in the context of a
specific table-top object arrangement. Thus each
instruction is associated with a pair of RGB-D im-
age. A total of 10 unique table-top arrangements
are used to generate the set of 100 instructions.

One copy of the corpora is annotated for train-
ing LPM using (ΓP ) while another for training
the symbol grounding model using (ΓS). The an-
notations for LPM corpus are selected such that
that the perception pipelines configured using the
annotated groundings would generate the optimal
world representations that are necessary and suf-
ficient to support grounding and planning for the
given tasks.

We have instructions with varying complexity
in our corpus. The instruction complexity from
the perception point of view is quantified in terms
of the total number of perceptual groundings ex-
pressed at the root level. e.g. “pick up the ball”
is relatively a simple instruction with only sin-
gle grounding expressed at the root level, while
“pick up the blue cube and put the blue cube near
the crackers box” is a more complicated instruc-
tion having seven groundings expressed at the root
level. This number was found to vary in the range
of one to seven in our corpus.

Experiments and Metrics

We structure our experiments to validate two
claims. The first claim is that adaptively infer-
ring the task optimal representations reduce the
perception run-time by avoiding exhaustively de-
tailed uniform modeling of the world. The sec-
ond claim is that reasoning in the context of these
optimal representations also reduces the inference
run-time of the symbol grounding model. An out-
line of our experiments is illustrated in Figure 3.
In the first experiment, we study the root-level in-
ference accuracy of LPM ( groundings expressed
at the root level of the phrase ) as a function of the
gradual increase in the training fraction. For each



157

Figure 3: Comparative Experiments: Boxes in the bottom half denote the baseline framework whereas
the boxes in the top half represent the proposed framework. Filled boxes enclose the variables that are
compared in the experiments.

value of training fraction in the range [ 0.2 , 0.9 ]
increasing with a step of 0.1, we perform 15 vali-
dation experiments. The training data is sampled
randomly for every individual experiment. Addi-
tionally, we perform a leave-one-out cross valida-
tion experiment. We use the inferences generated
by the leave-one-out cross validation experiments
as inputs to drive the adaptive perception for each
instruction.

In the second experiment, we compare the cu-
mulative run-time of LPM inference ( eq. 8 ) and
adaptive perception ( T1+T2 ) against the run-time
for complete perception ( T4 ) - our baseline, for
increasingly complex worlds.

In the third experiment, we compare the infer-
ence time of the symbol grounding model reason-
ing in the context of the adaptively generated op-
timal world models ( T3, eq. 6 ) against the infer-
ence time of the same model but when reasoning
in the context of the complete world models ( T5,
eq. 5 ). We also check whether the planning con-
straints inferred in both cases match the ground
truth or not. Experiments are performed on a sys-
tem running a 2.2 GHz Intel Core i7 CPU with 16
GB RAM.

5 RESULTS

This section presents the results obtained for the
above mentioned three experiments. Specifically,
the learning characteristics of LPM, the impact
of LPM on the perception run-time, and the im-
pact the adaptive representations on the symbol
grounding run-time.

Leftmost graph in the Figure 4 shows the results
of the first experiment. We can see that the infer-
ence accuracy grows as a function of a gradual in-
crease in the training data. A growing trend is an
indicator of the language diversity in the corpus.

Mean inference accuracy starts at 39.25%±5 for k
= 0.2 and it reaches 84% for leave-one-out cross
validation experiment ( k = 0.99 ).

Middle graph in the Figure 4 shows the result
of the second experiment. We can clearly see that
the run-time for complete perception grows with
the world complexity while the run-time of adap-
tive perception stays nearly flat and is significantly
lower in all cases. Since the adaptive perception
run-time varies according to the task, we see big-
ger error bars. The drop in the complete percep-
tion run-time for world complexity of 20 is jus-
tifiable as the run-time of our geometry detection
algorithm was proportional to the size of the indi-
vidual objects, and all of the objects for that exam-
ple world were smaller than other examples.

World T4 ( sec ) T1 + T2 ( sec )
Complexity baseline proposed
15 4.40 ± 0.05 0.96 ± 0.07
16 4.99 ± 0.02 1.33 ± 0.34
17 5.40 ± 0.06 1.11 ± 0.11
18 5.82 ± 0.18 1.51 ± 0.26
20 4.17 ± 0.05 1.11 ± 0.25
Mean 5.03 ± 0.07 1.20 ± 0.21

Table 1: Adaptive perception run-time compared
against complete perception run-time. Deviation
measures are 95% confidence interval values.

Rightmost graph in the Figure 4 shows the result
of the third experiment. It shows that the symbol
grounding run-time when reasoning in the context
of detailed world models( Υ ) grows as a func-
tion of the world complexity. However, it is sig-
nificantly lower when reasoning in the context of
adaptively generated world models ( Υ∗ ) and is
independent of the world complexity.

The achieved run-time gains are meaningful



158

Figure 4: Graph on the left shows the LPM inference accuracy as a function of gradual increase in the
training fraction. In the middle is the bar chart comparing the run-time for complete perception ( T4 )
against the cumulative run-time of LPM inference and adaptive perception ( T1 + T2 ). Finally, on the
right is a bar chart comparing the run-time of symbol grounding when reasoning in the context of the
adaptively generated optimal representations ( T3 ) against when reasoning in the context of exhaustively
detailed world models ( T5 ). The error bars indicate 95% confidence intervals.

World T5 ( ms ) T3 ( ms )
complexity baseline proposed
15 167 ± 12 21 ± 2
16 191 ± 16 23 ± 5
17 222 ± 12 22 ± 3
18 245 ± 12 27 ± 4
20 325 ± 10 20 ± 5
Mean 214 ± 12 23 ± 4

Table 2: Per phrase symbol grounding run-time in
ms ( rounded to the nearest integer ) using adaptive
representations compared against the same when
using complete representations. Deviation mea-
sures are 95% confidence interval values.

only if we do not incur a loss in the symbol
grounding accuracy. Table 3 shows the impact of
LPM on SG accuracy and summarizes the gains.

Perception Avg. Avg. SG
Type TP ( sec ) TSG ( ms ) Acc.
Complete 5.03 ± 0.07 214 ± 12 63%
Adaptive 1.20 ± 0.21 23 ± 4 66%
Ratio 4.19 9.30

Table 3: Impact of LPM on average percep-
tion run-time per instruction (TP ), average symbol
grounding run-time per instruction (TSG), and the
symbol grounding accuracy.

6 CONCLUSIONS

Real-time human-robot interaction is critical for
the utility of the collaborative robotic manipula-

tors in shared tasks. In scenarios where inferring
exhaustively detailed models of all the objects is
prohibitive, perception represents a bottleneck that
inhibits real-time interactions with collaborative
robots. Language provides an intuitive and multi-
resolution interface to interact with these robots.
While recent probabilistic frameworks have ad-
vanced our ability to interpret the meaning of com-
plex instructions in cluttered environments, the
problem of how language can channel the interpre-
tation of the raw observations to construct world
models which are necessary and sufficient for the
symbol grounding task is not extensively stud-
ied. Our proposed DCG based Language Percep-
tion Model, demonstrates that we can guide per-
ception using language to construct world mod-
els which are suitable for efficiently interpreting
the instruction. This provides run-time gains in
terms of both perception and symbol grounding,
thereby improving the speed with which collabo-
rative robots can understand and act upon human
instructions. In ongoing and future work we are
exploring how language can aid efficient construc-
tion of global maps for robot navigation and ma-
nipulation by intelligently sampling relevant ob-
servations from a set of observations.

7 ACKNOWLEDGMENTS

This work was supported in part by the National
Science Foundation under grant IIS-1637813 and
the New York State Center of Excellence in Data
Science at the University of Rochester.



159

References

James Bruce Aaron Walsman Kurt Konolige Sid-
dhartha Srinivasa Pieter Abbeel Aaron M Dollar
Berk Calli, Arjun Singh. 2017. Yale-cmu-berkeley
dataset for robotic manipulation research. vol-
ume 36, page 261 268.

Abdeslam Boularias, Felix Duvallet, Jean Hyaejin Oh,
and Anthony (Tony) Stentz. 2015. Grounding spa-
tial relations for outdoor robot navigation. In 2015
IEEE International Conference on Robotics and Au-
tomation (ICRA).

Andrea F. Daniele, Thomas M. Howard, and
Matthew R. Walter. 2017. A multiview approach
to learning articulated motion models. In Proceed-
ings of the International Symposium of Robotics Re-
search (ISRR).

F. Duvallet, M.R. Walter, T.M. Howard, S. Hemachan-
dra, J. Oh, S. Teller, N. Roy, and A. Stentz. 2014. A
probabilistic framework for inferring maps and be-
haviors from natural language. In Proceedings of
the 14th International Symposium on Experimental
Robotics.

A Elfes. 1987. Sonar-based real-world mapping and
navigation. IEEE Journal of Robotics and Automa-
tion, 3(3).

Clemens Eppner, Sebastian Höfer, Rico Jonschkowski,
Roberto Martı́n-Martı́n, Arne Sieverling, Vincent
Wall, and Oliver Brock. 2016. Lessons from the
amazon picking challenge: Four aspects of building
robotic systems. In Proceedings of Robotics: Sci-
ence and Systems, AnnArbor, Michigan.

Maurice Fallon, Scott Kuindersma, Sisir Karumanchi,
Matthew Antone, Toby Schneider, Hongkai Dai,
Claudia Prez D’Arpino, Robin Deits, Matt DiCicco,
Dehann Fourie, Twan Koolen, Pat Marion, Michael
Posa, Andrs Valenzuela, KuanTing Yu, Julie Shah,
Karl Iagnemma, Russ Tedrake, and Seth Teller.
2014. An architecture for online affordancebased
perception and wholebody planning. Journal of
Field Robotics, 32(2):229–254.

Martin A. Fischler and Robert C. Bolles. 1981. Ran-
dom sample consensus: A paradigm for model fit-
ting with applications to image analysis and auto-
mated cartography. Commun. ACM, 24(6):381–395.

C. Galindo, A. Saffiotti, S. Coradeschi, P. Buschka,
J. A. Fernandez-Madrigal, and J. Gonzalez. 2005.
Multi-hierarchical semantic maps for mobile
robotics. In 2005 IEEE/RSJ International Con-
ference on Intelligent Robots and Systems, pages
2278–2283.

Stevan Harnad. 1990. The symbol grounding problem.
In Physica D: Nonlinear Phenomena, volume 42,
pages 335–346.

S. Hemachandra, F. Duvallet, T.M. Howard, N. Roy,
A. Stentz, and M.R. Walter. 2015. Learning mod-
els for following natural language directions in un-
known environments. In Proceedings of the IEEE
International Conference on Robotics and Automa-
tion. IEEE.

A. Hornung, K. M. Wurm, and M. Bennewitz. 2010.
Humanoid robot localization in complex indoor en-
vironments. In 2010 IEEE/RSJ International Con-
ference on Intelligent Robots and Systems, pages
1690–1695.

Armin Hornung, Kai M. Wurm, Maren Bennewitz,
Cyrill Stachniss, and Wolfram Burgard. 2013. Oc-
tomap: an efficient probabilistic 3d mapping frame-
work based on octrees. Autonomous Robots,
34(3):189–206.

T.M Howard, S. Tellex, and N. Roy. 2014. A natu-
ral language planner interface for mobile manipula-
tors. In Proceedings of the IEEE International Con-
ference on Robotics and Automation, pages 6652–
6659. IEEE.

Ronghang Hu, Marcus Rohrbach, and Trevor Darrell.
2016. Segmentation from natural language expres-
sions. In European Conference on Computer Vision,
pages 108–124. Springer.

N. Hudson, T.M. Howard, J. Ma, A. Jain, M. Ba-
jracharya, S. Myint, L. Matthies, P. Backes,
P Hebert, T. Fuchs, and J. Burdick. 2012. End-to-
end dexterous manipulation with deliberative inter-
active estimation. In Proceedings of the 2012 IEEE
International Conference on Robotics and Automa-
tion.

K. Huebner and D. Kragic. 2008. Selection of robot
pre-grasps using box-based shape approximation. In
2008 IEEE/RSJ International Conference on Intelli-
gent Robots and Systems, pages 1765–1770.

Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer,
and Dieter Fox. 2013. Learning to Parse Natural
Language Commands to a Robot Control System.
Springer International Publishing, Heidelberg.

Andrew T. Miller, Steffen Knoop, Henrik I. Chris-
tensen, and Peter K. Allen. 2003. Automatic grasp
planning using shape primitives. In ICRA.

R. Paul, J. Arkin, N. Roy, and T.M. Howard. 2016. Ef-
ficient grounding of abstract spatial concepts for nat-
ural language interaction with robot manipulators.
In Proceedings of the 2016 Robotics: Science and
Systems Conference.

A. Pronobis and P. Jensfelt. 2012. Large-scale se-
mantic mapping and reasoning with heterogeneous
modalities. In 2012 IEEE International Conference
on Robotics and Automation, pages 3515–3522.

O. Propp, I. Chung, M.R. Walter, and T.M. Howard.
2015. On the performance of hierarchical dis-
tributed correspondence graphs for efficiency sym-
bol grounding of robot instructions. In Proceedings



160

of the 2015 IEEE/RSJ International Conference on
Intelligent Robots and Systems.

Radu Bogdan Rusu and Steve Cousins. 2011. 3D is
here: Point Cloud Library (PCL). In IEEE Inter-
national Conference on Robotics and Automation
(ICRA), Shanghai, China.

Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew Walter, Ashis Banerjee, Seth Teller, and
Nicholas Roy. 2011. Understanding natural lan-
guage commands for robotic navigation and mobile
manipulation.

A Grammar and Lexicon of the Corpus

We list the grammar rules and the lexicon for our
corpus to demonstrate the diversity of the instruc-
tions. Following table lists the words scraped from
the instructions in our corpus. We have a total of
56 unique words.

VB → {pick|put}
RP → {up}
DT → {the|all}
CC → {and}

VBZ → {is}
WDT → {that}

VB → {near|in|on}
PRP → {your}

NN →



cup|pudding|box|cube|
object|ball|master|
chef|can|soccer|
gear|mustard|sauce|bottle|
sugar|packet|block|
cleanser|middle|left|
right|crackers|cheezit|
cleanser|packet|block


NNS →

{
cups|chips|cubes|
objects|balls

}

JJ →

{
blue|green|yellow|
red|white

}

JJS →


nearest|rightmost|leftmost|
farthest|biggest|smallest|
largest|closest


Table 4: The words scraped from the corpus of
annotated examples

Following table lists the grammar rules scraped
from the instructions in our corpus. We have a
total of 23 unique grammar rules.

SBAR → WHNP S
S → VP

VP → VB PRT NP
VP → CC VP VP
VP → VB NP PP
VP → VBZ PP

WHNP → WDT
PRT → RP

PP → IN NP
NP → DT JJ NN
NP → DT NN NN
NP → DT JJS JJ NN
NP → NP PP
NP → DT
NP → DT JJ NNS
NP → DT NN
NP → DT NN NN NN
NP → DT JJ NN NN
NP → DT JJS NN
NP → DT NNS NN
NP → PRP NN
NP → NP SBAR
NP → DT NNS

Table 5: The grammar rules scraped from the cor-
pus of annotated examples


