















































Deep Reinforcement Learning for Chinese Zero Pronoun Resolution


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 569–578
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

569

Deep Reinforcement Learning for Chinese Zero pronoun Resolution

Qingyu Yin
]
, Yu Zhang

]
, Weinan Zhang

]
, Ting Liu

]⇤
, William Yang Wang

[

]Harbin Institute of Technology, China
[University of California, Santa Barbara, USA

{qyyin, yzhang, wnzhang, tliu}@ir.hit.edu.cn
william@cs.ucsb.edu

Abstract

Deep neural network models for Chinese
zero pronoun resolution learn semantic in-
formation for zero pronoun and candidate
antecedents, but tend to be short-sighted—
they often make local decisions. They typ-
ically predict coreference chains between
the zero pronoun and one single candi-
date antecedent one link at a time, while
overlooking their long-term influence on
future decisions. Ideally, modeling use-
ful information of preceding potential an-
tecedents is critical when later predicting
zero pronoun-candidate antecedent pairs.
In this study, we show how to integrate lo-
cal and global decision-making by exploit-
ing deep reinforcement learning models.
With the help of the reinforcement learn-
ing agent, our model learns the policy of
selecting antecedents in a sequential man-
ner, where useful information provided by
earlier predicted antecedents could be uti-
lized for making later coreference deci-
sions. Experimental results on OntoNotes
5.0 dataset show that our technique sur-
passes the state-of-the-art models.

1 Introduction

Zero pronoun, as a special linguistic phenomenon
in pro-dropped languages, is pervasive in Chinese
documents (Zhao and Ng, 2007). A zero pronoun
is a gap in the sentence, which refers to the com-
ponent that is omitted because of the coherence of
language. Following shows an example of zero
pronoun in Chinese document, where zero pro-
nouns are represented as “�”.

[Sã∫ Nö�] dÜ h: �1 #6 •◊ F
�2 _��˝∂Å ∫�#⇥

⇤Corresponding author.

([Litigant Li Yading] not only shows �1 willing
of acception, but also �2 hopes that there should
be someone in charge of it.)

A zero pronoun can be an anaphoric zero pronoun
if it coreferes to one or more mentions in the as-
sociated text, or unanaphoric, if there are no such
mentions. In this example, the second zero pro-
noun “�2” is anaphoric and corefers to the men-
tion “Sã∫Nö�/Litigant Li Yading” while
the zero pronoun “�1” is unanaphoric. These men-
tions that contain the important information for
interpreting the zero pronoun are called the an-
tecedents.

In recent years, deep learning models for
Chinese zero pronoun resolution have been
widely investigated (Chen and Ng, 2016; Yin
et al., 2017a,b). These solutions concentrate on
anaphoric zero pronoun resolution, applying nu-
merous neural network models to zero pronoun-
candidate antecedent prediction. Neural network
models have demonstrated their capabilities to
learn vector-space semantics of zero pronouns and
their antecedents (Yin et al., 2017a,b), and sub-
stantially surpass classic models (Zhao and Ng,
2007; Chen and Ng, 2013, 2015), obtaining state-
of-the-art results on the benchmark dataset.

However, these models are heavily making local
coreference decisions. They simply consider the
coreference chain between the zero pronoun and
one single candidate antecedent one link at a time
while overlooking their impacts on future deci-
sions. Intuitively, antecedents provide key linguis-
tic cues for explaining the zero pronoun, it is there-
fore reasonable to leverage useful information pro-
vided by previously predicted antecedents as cues
for predicting the later zero pronoun-candidate an-
tecedent pairs. For instance, given a sentence “I
have confidence that � can do it.” with its can-
didate mentions “he”, “the boy” and “I”, it is
challenging to infer whether mention “I” is pos-



570

sible to be the antecedent if it is considered sepa-
rately. In that case, the resolver may incorrectly
predict “I” to be the antecedent since “I” is the
nearest mention. Nevertheless, if we know that
“he” and “the boy” have already been predicted
to be the antecedents, it is uncomplicated to infer
the �-“I” pair as “non-coreference” because “I”
corefers to the disparate entity that is refered by
“he”. Hence, a desirable resolver should be able to
1) take advantage of cues of previously predicted
antecedents, which could be incorporated to help
classify later candidate antecedents and 2) model
the long-term influence of the single coreference
decision in a sequential manner.

To achieve these goals, we propose a deep rein-
forcement learning model for anaphoric zero pro-
noun resolution. On top of the neural network
models (Yin et al., 2017a,b), two main innova-
tions are introduced that are capable of effica-
ciously leveraging effective information provided
by potential antecedents, and making long-term
decisions from a global perspective. First, when
dealing with a specific zero pronoun-candidate an-
tecedent pair, our system encodes all its preced-
ing candidate antecedents that are predicted to
be the antecedents in the vector space. Conse-
quently, this representative vector is regarded as
the antecedent information, which can be utilized
to measure the coreference probability of the zero
pronoun-candidate antecedent pair. In addition,
the policy-based deep reinforcement learning al-
gorithm is applied to learn the policy of making
coreference decisions for zero pronoun-candidate
antecedent pairs. The innovative idea behind our
reinforcement learning model is to model the an-
tecedent determination as a sequential decision
process, where our model learns to link the zero
pronoun to its potential antecedents incrementally.
By encoding the antecedents predicted in previous
states, our model is capable of exploring the long-
term influence of independent decisions, produc-
ing more accurate results than models that sim-
ply consider the limited information in one single
state.

Our strategy is favorable in the following as-
pects. First, the proposed model learns to make de-
cisions by linguistic cues of previously predicted
antecedents. Instead of simply making local de-
cisions, our technique allows the model to learn
which action (predict to be an antecedent) avail-
able from the current state can eventually lead to

a high-scoring overall performance. Second, in-
stead of requiring supervised signals at each time
step, deep reinforcement learning model optimizes
its policy based on an overall reward signal. In
other words, it learns to directly optimize the over-
all evaluation metrics, which is more effective than
models that learn with loss functions that heuris-
tically define the goodness of a particular single
decision. Our experiments are conducted on the
OntoNotes dataset. Comparing to baseline sys-
tems, our model obtains significant improvements,
achieving the state-of-the-art performance for zero
pronoun resolution. The major contributions of
this paper are three-fold.

• We are the first to consider reinforcement
learning models for zero pronoun resolution
in Chinese documents;

• The proposed deep reinforcement learning
model leverages linguistic cues provided by
the antecedents predicted in earlier states
when classifying later candidate antecedents;

• We evaluate our reinforcement learning
model on a benchmark dataset, where a con-
siderable improvement is gained over the
state-of-the-art systems.

The rest of this paper is organized as follows.
The next section describes our deep reinforcement
learning model for anaphoric zero pronoun resolu-
tion. Section 3 presents our experiments, includ-
ing the dataset description, evaluation metrics, ex-
periment results, and analysis. We outline related
work in Section 4. The Section 5 is about the con-
clusion and future work.

2 modelology

In this section, we introduce the technical details
of the proposed reinforcement learning frame-
work. The specific task of anaphoric zero pronoun
resolution is to select antecedents from candidate
antecedents for the zero pronoun. Here we formu-
late it as a sequential decision process in a rein-
forcement learning setting. We first describe the
environment of the Markov decision making pro-
cess and our reinforcement learning agent. Then,
we introduce the modules. The last subsection is
about the supervised pre-training technique of our
model.



571

... ... ...

Agent Agent Agent

...

...

...ZP

NPk

Antecedent

Result

...

...

ZPZPZP

1NP 2NP nNP

1S 2S nS

1NP

.

.

.

1NP

rNP

kNP

Figure 1: Illustration of our reinforcement learning framework. Given a zero pronoun with n candi-
date antecedents (presented as “NP”), for each time, the agent scores pairs of zero pronoun-candidate
antecedent for their likelihood of coreference by 1) zero pronoun; 2) candidate antecedent and 3) an-
tecedent information. Antecedent information at time t is generated by all the antecedents predicted in
previous states.

2.1 Reinforcement Learning for Zero

Pronoun Resolution

Given an anaphoric zero pronoun zp, a set of
candidate antecedents are required to be selected
from its associated text. In particular, we adopt
the heuristic model utilized in recent Chinese
anaphoric zero pronoun resolution work (Chen
and Ng, 2016; Yin et al., 2017a,b) for this pur-
pose. For those noun phrases that are two sen-
tences away at most from the zero pronoun, we se-
lect those who are maximal noun phrases or mod-
ifier ones to compose the candidate set. These
noun phrases ({np1, np2, ..., npn}) and the zero
pronoun (zp) are then encoded into representation
vectors: {vnp1 , vnp2 , ..., vnpn} and vzp.

Previous neural network models (Chen and
Ng, 2016; Yin et al., 2017a,b) generally con-
sider some pairwise models to select an-
tecedents. In these work, candidate antecedents
and the zero pronoun are first merged into pairs
{(zp, np1), (zp, np2), ..., (zp, npn)}, and then dif-
ferent neural networks are applied to deal with
each pair independently. We argue that these
models only make local decisions while overlook-
ing their impacts on future decisions. In con-
trast, we formulate the antecedent determination
process in as Markov decision process problem.
An innovative reinforcement learning algorithm

is designed that learns to classify candidate an-
tecedents incrementally. When predicting one sin-
gle zero pronoun-candidate antecedent pair, our
model leverages antecedent information gener-
ated by previously predicted antecedents, making
coreference decisions based on global signals.

The architecture of our reinforcement learning
framework is shown in Figure 1. For each time
step, our reinforcement learning agent predicts
the zero pronoun-candidate antecedent pair by us-
ing 1) the zero pronoun; 2) information of cur-
rent candidate antecedent and 3) antecedent in-
formation generated by antecedents predicted in
previous states. In particular, our reinforcement
learning agent is designed as a policy network
⇡✓(s, a) = p(a|s; ✓), where s represents the state;
a indicates the action and ✓ represents the param-
eters of the model. The parameters ✓ are trained
using stochastic gradient descent. Compared with
Deep Q-Network (Mnih et al., 2013) that com-
monly learns a greedy policy, policy network is
able to learn a stochastic policy that prevents the
agent from getting stuck at an intermediate state
(Xiong et al., 2017). Additionally, the learned
policy is more explainable, comparing to learned
value functions in Deep Q-Network. We here in-
troduce the definitions of components of our re-
inforcement learning model, namely, state, action



572

and reward.

2.1.1 State

Given a zero pronoun zp with its representation
vzp and all of its candidate antecedents represen-
tations {vnp1 , vnp2 , ..., vnpn}, our model generate
coreference decisions for zero pronoun-candidate
antecedent pairs in sequence. More specifically,
for each time, the state is generated by using both
the vectors of the current zero pronoun-candidate
antecedent pair and candidates that have been pre-
dicted to be the antecedents in the previous states.
For time t, the state vector st is generated as fol-
lows:

st = (vzp, vnpt , vante(t), vfeaturet) (1)

where vzp and vnpt are the vectors of zp and npt at
time t. As shown in Chen and Ng (2016), human-
designed handcrafted features are essential for the
resolver since they reveal the syntactical, posi-
tional and other relations between a zero pronoun
and its counterpart antecedents. Hence, to eval-
uate the coreference possibility of each candidate
antecedent in a comprehensive manner, we inte-
grate a group of features that are utilized in pre-
vious work (Zhao and Ng, 2007; Chen and Ng,
2013, 2016) into our model. For these multi-
value features, we decompose them into a corre-
sponding set of binary-value ones. vfeaturet repre-
sents the feature vector. vante(t) represents the an-
tecedent information generated by candidates that
have been predicted to be antecedents in previous
states. After that, these vectors are concatenated
to be the representation of state and fed into the
deep reinforcement learning agent to generate the
action.

2.1.2 Action

The action for each state is defined to be: core-
fer that indicates the zero pronoun and candidate
antecedent are coreference; or otherwise, non-
corefer. If an action corefer is made, we retain
the vector of the counterpart antecedent together
with those of the antecedents predicted in previ-
ous states to generate the vector vante, which is
utilized to produce the antecedent information in
the next state.

2.1.3 Reward

Normally, once the agent executes a series of ac-
tions, it observes a reward R(a1:T ) that could be

any function. To encourage the agent to find ac-
curate antecedents, we regard the F-score for the
selected antecedents as the reward for each action
in a path.

2.2 Reinforcement Learning Agent

Basically, our reinforcement learning agent is
comprised of three parts, namely, the zero pro-
noun encoder that learns to encode a zero pronoun
into vectors by using its context words; the candi-
date mention encoder that represents the candidate
antecedents by content words; and the agent that
maps the state vector s to a probability distribu-
tion over all possible actions.

In this work, the ZP-centered neural network
model proposed by Yin et al. (2017a) is employed
to be the zero pronoun encoder. The encoder
learns to encode the zero pronoun by its associ-
ated text into its vector-space semantics. In par-
ticular, two standard recurrent neural networks
are employed to encode the preceding text and
the following text of a zero pronoun, separately.
Such a model learns to encode the associated text
around the zero pronoun, exploiting sentence-level
information for the zero pronoun. For the can-
didate mentions encoder, we adopt the recurrent
neural network-based model that encodes these
phrases by using its content words. More specif-
ically, we utilize a standard recurrent neural net-
work to model the content of a phrase from left
to right. This model learns to produce the vec-
tor of a phrase by considering its content, pro-
viding our model an ability to reveal its vector-
space semantics. In this way, we generate the vec-
tor for zp, the vzp, and representation vectors of
all its candidate antecedents, which are denoted as
{vnp1 , vnp2 , ..., vnpn}.

Moreover, we employ pooling operations to
encode antecedent information by using the an-
tecedents that are predicted in previous states.
In particular, we generate two vectors by apply-
ing the max-pooling and average-pooling, respec-
tively. These two vectors are then concatenated
together. Let the representative vector of the tth
candidate antecedent to be vnpt 2 Rd, and the pre-
dicted antecedents at time t be writen as S(t) =
[vnpi , vnpj , ..., vnpr ], the vector at time t, vante(t)k
is generated by:

vante(t)k =

(
max{S(t)k,·} for 0  k < d
ave{S(t)k�d,·} for d  k < 2d



573

Softmax

tanh

tanh

Candidate
AntecedentsZP Features Antecedents

State

action

Figure 2: Illustration of the feedforward neural
network model employed as the agent. Its in-
put vector includes these parts: (1) Zero pronoun;
(2) Candidate Antecedents; (3) Pair Features and
(4) Antecedents. By going through all the full-
connected hidden layers and one softmax layer,
the agent maps the state vector into the proba-
bility distribution over actions that indicates the
coreference likelihood of the input zero pronoun-
candidate antecedent pair.

The concatenation of these vectors is regarded
as input and is fed into our reinforcement learn-
ing agent. More specifically, a feed-forward neu-
ral network is utilized to constitute the agent that
maps the state vector to a probability distribution
over all possible actions. Figure 2 shows the ar-
chitecture of the agent. Two hidden layers are em-
ployed in our model, each of which utilizes the
tanh as the activation function. For each layer,
we generate the output by:

hi(st) = tanh(Wihi�1(st) + bi) (2)

where Wi and bi are the parameters of the ith hid-
den layer; si represents the state vector. After
going through all the layers, we can get the rep-
resentative vector for the zero pronoun-candidate
antecedent pair (zp, npt). We then feed it into a
scoring-layer to get their coreference score. The
scoring-layer is a fully-connected layer of dimen-
sion 2:

score(zp, npt) = Wsh2(st) + bs (3)

where h2 represents the output of the second hid-
den layer; Ws 2 R2⇥r is the parameter of the layer
and r is the dimension of h2. Consequently, we
generate the probability distribution over actions
using the output generated by the scoring-layer of
the neural network, where a softmax layer is em-

ployed to gain the probability of each action:

p✓(a) / escore(zp,npt) (4)

In this work, the policy-based reinforcement learn-
ing model is employed to train the parameter of the
agent. More specifically, we explore using the RE-
INFORCE policy gradient algorithm (Williams,
1992), which learns to maximize the expected re-
ward:

J(✓) = Ea1:T⇠p(a|zp,npt;✓)R(a1:T )

=
X

t

X

a

p(a|zp, npt; ✓)R(at) (5)

where p(a|zp, npt; ✓) indicates the probability of
selecting action a.

Intuitively, the estimation of the gradient might
have very high variance. One commonly used
remedy to reduce the variance is to subtract a base-
line value b from the reward. Hence, we utilize the
gradient estimate as follows:

r✓J(✓) = r✓
X

t

log p(a|zp, npt; ✓)(R(at)� bt)

(6)
Following Clark and Manning (2016), we intor-
duce the baseline b and get the value of bt at time
t by Eat0⇠pR(a1, ..., at0 , ..., aT ).

2.3 Pretraining

Pretraining is crucial in reinforcement learning
techniques (Clark and Manning, 2016; Xiong
et al., 2017). In this work, we pretrain the model
by using the loss function from Yin et al. (2017a):

loss = �
NX

i=1

X

np2A(zpi)

�(zpi, np)log(P (np|zpi))

(7)
where P (np|zpi) is the coreference score gen-
erated by the agent (the probability of choosing
corefer action); A(zpi) represents the candidate
antecedents of zpi; �(zp, np) is 1 or 0, represent-
ing zp and np are coreference or not.

3 Experiments

3.1 Dataset and Settings

3.1.1 Dataset

Same to recent work on Chinese zero pronoun
(Chen and Ng, 2016; Yin et al., 2017a,b), the



574

proposed model is evaluated on the Chinese por-
tion of the OntoNotes 5.0 dataset1 that was used
in the Conll-2012 Shared Task. Documents in
this dataset are from six different sources, namely,
Broadcast News (BN ), Newswires (NW ), Broad-
cast Conversations (BC), Telephone Conversa-
tions (TC), Web Blogs (WB) and Magazines
(MZ). Since zero pronoun coreference annota-
tions exist in only the training and development
set (Chen and Ng, 2016), we utilize the training
dataset for training purposes and test our model on
the development set. The statistics of our dataset
are reported in Table 1. To make equal compari-
son, we adopt the strategy as utilized in the exist-
ing work (Chen and Ng, 2016; Yin et al., 2017a),
where 20% of the training dataset are randomly
selected and reserved as a development dataset for
tuning the model.

#Documents #Sentences #AZPs
Training 1,391 36,487 12,111
Test 172 6,083 1,713

Table 1: Statistics on the training and test dataset.

3.1.2 Evaluation Measures

Following previous work on zero pronoun resolu-
tion (Zhao and Ng, 2007; Chen and Ng, 2016; Yin
et al., 2017a,b), metrics employed to evaluate our
model are: recall, precision, and F-score (F). We
report the performance for each source in addition
to the overall result.

3.1.3 Baselines and Experiment Settings

Five recent zero pronoun resolution systems are
employed as our baselines, namely, Zhao and Ng
(2007), Chen and Ng (2015), Chen and Ng (2016),
Yin et al. (2017a) and Yin et al. (2017b). The
first of them is machine learning-based, the sec-
ond is the unsupervised and the other ones are all
deep learning models. Since we concentrate on the
anaphoric zero pronoun resolution process, we run
experiments by employing the experiment setting
with ground truth parse results and ground truth
anaphoric zero pronoun, all of which are from the
original dataset. Moreover, to illustrate the ef-
fectiveness of our reinforcement learning model,
we run a set of ablation experiments by using dif-
ferent pretraining iterations and report the perfor-

1http://catalog.ldc.upenn.edu/
LDC2013T19

mance of our model with different iterations. Be-
sides, to explore the randomness of the reinforce-
ment learning technique, we report the perfor-
mance variation of our model with different ran-
dom seeds.

3.1.4 Implementation Details

We randomly initialize the parameters and mini-
mize the objective function using Adagrad (Duchi
et al., 2011). The embedding dimension is 100,
and hidden layers are 256 and 512 dimensions, re-
spectively. Moreover, the dropout (Hinton et al.,
2012) regularization is added to the output of each
layer. Table 2 shows the hyperparameters we uti-
lized for both the pre-training and reinforcement
learning process. Hyperparameters here are se-

Pre RL
hidden dimentions 256 & 512 256 & 512
training epochs 70 50
batch 256 256
dropout rate 0.5 0.7
learning rate 0.003 0.00009

Table 2: Hyperparameters for the pre-training
(Pre) and reinforcement learning (RL).

lected based on preliminary experiments and there
remains considerable space for improvement, for
instance, applying the annealing.

3.2 Experiment Results

In Table 3, we compare the results of our model
with baselines in the test dataset. Our reinforce-
ment learning model surpasses all previous base-
lines. More specifically, for the “Overall” results,
our model obtains a considerable improvement by
2.3% in F-score over the best baseline (Yin et al.,
2017a). Moreover, we run experiments in differ-
ent sources of documents and report the results
for each source. The number following a source’s
name indicates the amount of anaphoric zero pro-
noun in that source. Our model beats the best
baseline in four of six sources, demonstrating the
efficiency of our reinforcement learning model.
The improvement gained over the best baseline in
source “BC” is 4.3% in F-score, which is encour-
aging since it contains the most anaphoric zero
pronoun. In all words, all these suggest that our
model surpasses existed baselines, which demon-
strates the efficiency of the proposed technique.

Ideally, our model learns useful information



575

NW (84) MZ (162) WB (284) BN (390) BC (510) TC (283) Overall
Zhao and Ng (2007) 40.5 28.4 40.1 43.1 44.7 42.8 41.5
Chen and Ng (2015) 46.4 39.0 51.8 53.8 49.4 52.7 50.2
Chen and Ng (2016) 48.8 41.5 56.3 55.4 50.8 53.1 52.2
Yin et al. (2017b) 50.0 45.0 55.9 53.3 55.3 54.4 53.6
Yin et al. (2017a) 48.8 46.3 59.8 58.4 53.2 54.8 54.9
Our model 63.1 50.2 63.1 56.7 57.5 54.0 57.2

Table 3: Experiment results on the test data. The first six columns show the results on different source
of documents and the last column is the overall results.

gathered from candidates that have been predicted
to be the antecedents in previous states, which
brings a global-view instead of simply making
partial decisions. By applying the reinforcement
learning, our model learns to directly optimize
the overall performance in expectation, guiding
benefit in making decisions in a sequential man-
ner. Consequently, they bring benefit to predict
accurate antecedents, leading to the better perfor-
mance.

Moreover, on purpose of better illustrating the
effectiveness of the proposed reinforcement learn-
ing model, we run a set of experiments with dif-
ferent settings. In particular, we compare the
model with and without the proposed reinforce-
ment learning process using different pre-training
iterations. For each time, we report the perfor-
mance of our model on both the test and devel-
opment set. For all these experiments, we retain
the rest of the model unchanged.

Figure 3: Experiment results of different models,
where “RL” represents the reinforcement learning
algorithm and “Pre” presents the model without
reinforcement learning. “dev” shows the perfor-
mance of our reinforcement learning model on the
development dataset.

Figure 3 shows the performance of our model
with and without reinforcement learning. We can
see from the table that our model with reinforce-
ment learning achieves better performance than
the model without this all across the board. With
the help of reinforcement learning, our model
learns to choose effective actions in sequential de-
cisions. It empowers the model to directly opti-
mize the overall evaluation metrics, which brings a
more effective and natural way of dealing with the
task. Moreover, by seeing that the performance
on development dataset stops increasing with it-
erations bigger than 70, we therefore set the pre-
training iterations to 70.

Following Reimers and Gurevych (2017), to il-
lustrate the impact of randomness in our reinforce-
ment learning model, we run our model with dif-
ferent random seed values. Table 4 shows the
performance of our model with different random
seeds on the test dataset. We report the mini-
mum, the maximum, the median F-scores results
and the standard deviation � of F-scores. We run

Min F Median F Max F �
56.5 57.1 57.5 0.00253

Table 4: Performance of our model with different
random seeds.

the model with 38 different random seeds. The
maximum F-score is 57.5% and the minimum one
is 56.5%. Based on this observation, we can draw
the conclusion that our proposed reinforcement
learning model generally beats the baselines and
achieves the state-of-the-art performance.

3.3 Case Study

Lastly, we show a case to illustrate the effective-
ness of our proposed model, as is shown in Fig-
ure 4. In this case, we can see that our model
correctly predict mentions “£✏W/The Xiaohui”



576

那 小穗 她 本来 就是 好  觉得 φ  聘 一次 的 话 心里 就 不是 很 有 把握 。

, , ,. φ , ,
 

I
我

, ,

Figure 4: Example of case study. Noun phrases
with pink background color are the ones selected
to be the antecedents by our model.

and “y/She” as the antecedents of the zero pro-
noun “�”. This case demonstrates the efficiency
of our model. Instead of making only local de-
cisions, our model learns to predict potential an-
tecedents incrementally, selecting global-optimal
antecedents in a sequential manner. In the end, our
model successfully predicts “y/She” as the result.

4 Related Work

4.1 Zero Pronoun Resolution

A wide variety of techniques for machine learning
models for Chinese zero pronoun resolution have
been proposed. Zhao and Ng (2007) utilized the
decision tree to learn the anaphoric zero pronoun
resolver by using syntactical and positional fea-
tures. It is the first time that machine learning tech-
niques are applied for this task. To better explore
syntactics, Kong and Zhou (2010) employed the
tree kernel technique in their model. Chen and Ng
(2013) extended Zhao and Ng (2007)’s model fur-
ther by integrating innovative features and coref-
erence chains between zero pronoun as bridges to
find antecedents. In contrast, unsupervised tech-
niques have been proposed and shown their effi-
ciency. Chen and Ng (2014) proposed an unsu-
pervised model, where a model trained on manu-
ally resolved pronoun was employed for the reso-
lution of zero pronoun. Chen and Ng (2015) pro-
posed an unsupervised anaphoric zero pronoun re-
solver, using the salience model to deal with the
issue. Besides, there has been extensive work on
zero anaphora for other languages. Efforts for
zero pronoun resolution fall into two major cat-
egories, namely, (1) heuristic techniques (Han,
2006); and (2) learning-based models (Iida and
Poesio, 2011; Isozaki and Hirao, 2003; Iida et al.,
2006, 2007; Sasano and Kurohashi, 2011; Iida and
Poesio, 2011; Iida et al., 2015, 2016).

In recent years, deep learning techniques have

been extensively studied for zero pronoun resolu-
tion. Chen and Ng (2016) introduced a deep neural
network resolver for this task. In their work, zero
pronoun and candidates are encoded by a feed-
forward neural network. Liu et al. (2017) explored
to produce pseudo dataset for anaphoric zero pro-
noun resolution. They trained their deep learn-
ing model by adopting a two-step learning method
that overcomes the discrepancy between the gen-
erated pseudo dataset and the real one. To better
utilize vector-space semantics, Yin et al. (2017b)
employed recurrent neural network to encode zero
pronoun and antecedents. In particular, a two-
layer antecedent encoder was employed to gener-
ate the hierarchical representation of antecedents.
Yin et al. (2017a) developed an innovative deep
memory network resolver, where zero pronouns
are encoded by its potential antecedent mentions
and associated text.

The major difference between our model and
existed techniques lies in the applying of deep re-
inforcement learning. In this work, we formu-
late the anaphoric zero pronoun resolution as a se-
quential decision process in a reinforcement learn-
ing setting. With the help of reinforcement learn-
ing, our resolver learns to classify mentions in
a sequential manner, making global-optimal de-
cisions. Consequently, our model learns to take
advantage of earlier predicted antecedents when
making later coreference decisions.

4.2 Deep Reinforcement Learning

Recent advances in deep reinforcement learning
have shown promise results in a variety of natural
language processing tasks (Branavan et al., 2012;
Narasimhan et al., 2015; Li et al., 2016). In recent
time, Clark and Manning (2016) proposed a deep
reinforcement learning model for coreference res-
olution, where an agent is utilized for linking men-
tions to their potential antecedents. They utilized
the policy gradient algorithm to train the model
and achieves better results compared with the
counterpart neural network model. Narasimhan
et al. (2016) introduced a deep Q-learning based
slot-filling technique, where the agent’s action is
to retrieve or reconcile content from a new doc-
ument. Xiong et al. (2017) proposed an innova-
tive reinforcement learning framework for learn-
ing multi-hop relational paths. Deep reinforce-
ment learning is a natural choice for tasks that re-
quire making incremental decisions. By combin-



577

ing non-linear function approximations with rein-
forcement learning, the deep reinforcement learn-
ing paradigm can integrate vector-space semantic
into a robust joint learning and reasoning process.
Moreover, by optimizing the policy-based on the
reward signal, deep reinforcement learning model
relies less on heuristic loss functions that require
careful tuning.

5 Conclusion

We introduce a deep reinforcement learning
framework for Chinese zero pronoun resolution.
Our model learns the policy on selecting an-
tecedents in a sequential manner, leveraging ef-
fective information provided by the earlier pre-
dicted antecedents. This strategy contributes to the
predicting for later antecedents, bringing a natu-
ral view for the task. Experiments on the bench-
mark dataset show that our reinforcement learning
model achieves an F-score of 67.2% on the test
dataset, surpassing all the existed models by a con-
siderable margin.

In the future, we plan to explore neural network
models for efficaciously resolving anaphoric zero
pronoun documents and research on some spe-
cific components which might influence the per-
formance of the model, such as the embedding.
Meanwhile, we plan to research on the possibil-
ity of applying adversarial learning (Goodfellow
et al., 2014) to generate better rewards than the
human-defined reward functions. Besides, to deal
with the problematic scenario when ground truth
parse tree and anaphoric zero pronoun are un-
available, we are interested in exploring the neural
network model that integrates the anaphoric zero
pronoun determination and anaphoric zero pro-
noun resolution jointly in a hierarchical architec-
ture without using parser or anaphoric zero pro-
noun detector.

Our code is available at https://github.
com/qyyin/Reinforce4ZP.git.

Acknowledgments

Thank the anonymous reviewers for their valuable
comments. This work was supported by the Major
State Basic Research Development 973 Program
of China (No.2014CB340503), National Natural
Science Foundation of China (No.61472105 and
No.61502120). According to the meaning by
Harbin Institute of Technology, the contact author
of this paper is Ting Liu.

References

SRK Branavan, David Silver, and Regina Barzilay.
2012. Learning to win by reading manuals in a
monte-carlo framework. Journal of Artificial Intel-
ligence Research, 43:661–704.

Chen Chen and Vincent Ng. 2013. Chinese zero pro-
noun resolution: Some recent advances. In EMNLP,
pages 1360–1365.

Chen Chen and Vincent Ng. 2014. Chinese zero pro-
noun resolution: An unsupervised approach com-
bining ranking and integer linear programming. In
Twenty-Eighth AAAI Conference on Artificial Intel-

ligence.

Chen Chen and Vincent Ng. 2015. Chinese zero pro-
noun resolution: A joint unsupervised discourse-
aware model rivaling state-of-the-art resolvers. In
Proceedings of the 53rd Annual Meeting of the ACL

and the 7th International Joint Conference on Natu-

ral Language Processing (Volume 2: Short Papers),
page 320.

Chen Chen and Vincent Ng. 2016. Chinese zero pro-
noun resolution with deep neural networks. In Pro-
ceedings of the 54rd Annual Meeting of the ACL.

Kevin Clark and Christopher D Manning. 2016. Deep
reinforcement learning for mention-ranking corefer-
ence models. Proceedings of EMNLP’16.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12(Jul):2121–2159.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Advances in neural information
processing systems, pages 2672–2680.

Na-Rae Han. 2006. Korean zero pronouns: analysis
and resolution. Ph.D. thesis, Citeseer.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.

Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
resolution. In Proceedings of the 21st International
Conference on Computational Linguistics and the

44th annual meeting of the Association for Compu-

tational Linguistics, pages 625–632. Association for
Computational Linguistics.

Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007.
Zero-anaphora resolution by learning rich syntactic
pattern features. ACM Transactions on Asian Lan-
guage Information Processing (TALIP), 6(4):1.



578

Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ilp solution to zero anaphora resolution. In Proceed-
ings of the 49th Annual Meeting of the Association

for Computational Linguistics: Human Language

Technologies-Volume 1, pages 804–813. Association
for Computational Linguistics.

Ryu Iida, Kentaro Torisawa, Chikara Hashimoto, Jong-
Hoon Oh, and Julien Kloetzer. 2015. Intra-
sentential zero anaphora resolution using subject
sharing recognition. Proceedings of EMNLP’15,
pages 2179–2189.

Ryu Iida, Kentaro Torisawa, Jong-Hoon Oh, Cana-
sai Kruengkrai, and Julien Kloetzer. 2016. Intra-
sentential subject zero anaphora resolution using
multi-column convolutional neural network. In Pro-
ceedings of EMNLP.

Hideki Isozaki and Tsutomu Hirao. 2003. Japanese
zero pronoun resolution based on ranking rules and
machine learning. In Proceedings of the 2003 con-
ference on Empirical methods in natural language

processing, pages 184–191. Association for Compu-
tational Linguistics.

Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for chinese zero anaphora
resolution. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-

cessing, pages 882–891. Association for Computa-
tional Linguistics.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-

ods in Natural Language Processing, pages 1192–
1202.

Ting Liu, Yiming Cui, Qingyu Yin, Shijin Wang,
Weinan Zhang, and Guoping Hu. 2017. Generat-
ing and exploiting large-scale pseudo training data
for zero pronoun resolution. In ACL.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin Riedmiller. 2013. Playing atari with
deep reinforcement learning. NIPS.

Karthik Narasimhan, Tejas Kulkarni, and Regina
Barzilay. 2015. Language understanding for text-
based games using deep reinforcement learning.
EMNLP’15.

Karthik Narasimhan, Adam Yala, and Regina Barzilay.
2016. Improving information extraction by acquir-
ing external evidence with reinforcement learning.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2355–2365.

Nils Reimers and Iryna Gurevych. 2017. Reporting
score distributions makes a difference: Performance
study of lstm-networks for sequence tagging. In
Proceedings of the 2017 Conference on Empirical

Methods in Natural Language Processing, pages
338–348.

Ryohei Sasano and Sadao Kurohashi. 2011. A dis-
criminative approach to japanese zero anaphora res-
olution with large-scale lexicalized case frames. In
IJCNLP, pages 758–766.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Wenhan Xiong, Thien Hoang, and William Yang
Wang. 2017. Deeppath: A reinforcement learning
method for knowledge graph reasoning. Proceed-
ings of the 2016 Conference on Empirical Methods

in Natural Language Processing.

Qingyu Yin, Yu Zhang, Weinan Zhang, and Ting Liu.
2017a. Chinese zero pronoun resolution with deep
memory network. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language

Processing, pages 1309–1318.

Qingyu Yin, Yu Zhang, Weinan Zhang, and Ting Liu.
2017b. A deep neural network for chinese zero pro-
noun resolution. In IJCAI.

Shanheng Zhao and Hwee Tou Ng. 2007. Identifica-
tion and resolution of chinese zero pronouns: A ma-
chine learning approach. In EMNLP-CoNLL, vol-
ume 2007, pages 541–550.


