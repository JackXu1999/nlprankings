



















































A Language Model based Evaluator for Sentence Compression


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 170–175
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

170

A Language Model based Evaluator for Sentence Compression

Yang Zhao Zhiyuan Luo
The University of Tokyo

7-3-1 Hongo, Bunkyo-ku, Tokyo
{zhao,zyluo24}@is.s.u-tokyo.ac.jp

Akiko Aizawa
National Institute of Informatics

2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo
aizawa@nii.ac.jp

Abstract

We herein present a language-model-
based evaluator for deletion-based sen-
tence compression, and viewed this task
as a series of deletion-and-evaluation op-
erations using the evaluator. More specif-
ically, the evaluator is a syntactic neural
language model that is first built by learn-
ing the syntactic and structural collocation
among words. Subsequently, a series of
trial-and-error deletion operations are con-
ducted on the source sentences via a re-
inforcement learning framework to obtain
the best target compression. An empirical
study shows that the proposed model can
effectively generate more readable com-
pression, comparable or superior to sev-
eral strong baselines. Furthermore, we in-
troduce a 200-sentence test set for a large-
scale dataset, setting a new baseline for the
future research.

1 Introduction

Deletion-based sentence compression aims to
delete unnecessary words from source sentence
to form a short sentence (compression) while re-
taining grammatical and faithful to the under-
lying meaning of the source sentence. Previ-
ous works used either machine-learning-based ap-
proach or syntactic-tree-based approaches to yield
most readable and informative compression (Jing,
2000; Knight and Marcu, 2000; Clarke and La-
pata, 2006; McDonald, 2006; Clarke and La-
pata, 2008; Filippova and Strube, 2008; Berg-
Kirkpatrick et al., 2011; Filippova et al., 2015;
Bingel and Søgaard, 2016; Andor et al., 2016;
Zhao et al., 2017; Wang et al., 2017). For example,
(Clarke and Lapata, 2008) proposed a syntactic-
tree-based method that considers the sentence

compression task as an optimization problem by
using integer linear programming, whereas (Filip-
pova et al., 2015) viewed the sentence compres-
sion task as a sequence labeling problem using
the recurrent neural network (RNN), using max-
imum likelihood as the objective function for op-
timization. The latter sets a relatively strong base-
line by training the model on a large-scale parallel
corpus. Although an RNN (e.g., Long short-term
memory networks) can implicitly model syntactic
information, it still produces ungrammatical sen-
tences. We argue that this is because (i) the la-
bels (or compressions) are automatically yielded
by employing the syntactic-tree-pruning method.
It thus contains some errors caused by syntactic
tree parsing error, (ii) more importantly, the op-
timization objective of an RNN is the likelihood
function that is based on individual words instead
of readability (or informativeness) of the whole
compressed sentence. A gap exists between opti-
mization objective and evaluation. As such, we are
of great interest that: (i) can we take the readabil-
ity of the whole compressed sentence as a learning
objective and (ii) can grammar errors be recovered
through a language-model-based evaluator to yield
compression with better quality?

To answer the above questions, a syntax-based
neural language model is trained on large-scale
datasets as a readability evaluator. The neural
language model is supposed to learn the correct
word collocations in terms of both syntax and se-
mantics. Subsequently, we formulate the deletion-
based sentence compression as a series of trial-
and-error deletion operations through a reinforce-
ment learning framework. The policy network
performs either RETAIN or REMOVE action to
form a compression, and receives a reward (e.g.,
readability score) to update the network.

The empirical study shows that the proposed
method can produce more readable sentences that



171

preserve the source sentences, comparable or su-
perior to several strong baselines. In short, our
contributions are two-fold: (i) an effective syntax-
based evaluator is built as a post-hoc checker,
yielding compression with better quality based
upon the evaluation metrics; (ii) a large scale news
dataset with 1.02 million sentence compression
pairs are compiled for this task in addition to 200
manually created sentences. We made it publicly
available.

2 Methodology

2.1 Task and Framework

Formally, deletion-based sentence compression
translates word tokens, (w1, w2, ..., wn) into a se-
ries of ones and zeros, (l1, l2, ..., ln), where n
refers to the length of the original sentence and
li ∈ {0, 1}. Here, ”1” refers to RETAIN and
”0” refers to REMOVE. We first converted the
word sequence into a dense vector representa-
tion through the parameter matrix E. Except
for word embedding, (e(w1), e(w2), ..., e(wn)),
we also considered the part-of-speech tag and
the dependency relation between wi and its
head word as extra features. Each part-of-
speech tag was mapped into a vector represen-
tation, (p(w1), p(w2), ..., p(wn)) through the pa-
rameter matrix P , while each dependency re-
lation was mapped into a vector representation,
(d(w1), d(w2), ..., d(wn)) through the parameter
matrix D. Three vector representations are con-
catenated, [e(wi); p(wi); d(wi)] as the input to the
next part, policy network.

Figure 1 shows the graphical illustration of our
model. The policy network is a bi-directional
RNN that uses the input [e(wi); p(wi); d(wi)] and
yields the hidden states in the forward direction,
(hf1 , h

f
2 , ..., h

f
n), and hidden states in the backward

direction, (hb1, h
b
2, ..., h

b
n). Then, concatenation of

hidden states in both directions, [hfi ;h
b
i ] are fol-

lowed by a nonlinear layer to turn the output into a
binary probability distribution, yi = σ(W [h

f
i ;h

b
i ])

where σ is a nonlinear function sigmoid, and W
is a parameter matrix.

The policy network continues to sample actions
from the binary probability distribution above un-
til the whole action sequence is yielded. In
this task, binary actions space is {RETAIN, RE-
MOVE}. We turn the action sequence into the pre-
dicted compression, (w1, w2, ..., wm), by deleting
the words whose current action is REMOVE. Then

[e(w1);p(w1);d(w1)] [e(wn);p(wn);d(wn)] 

w2  …… wn

Evaluator

Po
lic

y 
N

et

Update

Figure 1: Graphical illustration of the framework.

the (w1, w2, ..., wm) is fed into a pre-trained eval-
uator which will be described in the next section.

2.2 Syntax-based Evaluator

The syntax-based evaluator should assess the de-
gree to which the compressed sentence is gram-
matical, through being used as a reward function
during the reinforcement learning phase. It needs
to satisfy three conditions: (i) grammatical com-
pressions should obtain a higher score than un-
grammatical compressions, (ii) for two ungram-
matical compressions, it should be able to discrim-
inate them through the score despite the ungram-
maticality, (iii) lack of important parts (such as the
primary subject or verb) in the original sentence
should receive a greater penalty.

We therefore considered an ad-hoc evaluator,
i.e., the syntax-based language model (evaluator-
SLM) for these requirements. It integrates the
part-of-speech tags and the dependency relations
in the input, while the output to be predicted is
the next word token. We observed that the pre-
diction of the next word could not only be based
on the previous word but also the syntactic com-
ponents, e.g., for the part-of-speech tag, the noun
is often followed by a verb instead of an adjec-
tive or adverb and the integration of the part-of-
speech tag allows the model to learn such cor-
rect word collocations. Figure 2 shows the graphi-
cal illustration of the evaluator-SLM where the in-
put is xi = [e(wi); p(wi); d(wi)], followed by a
bi-directional RNN whose last layer is the Soft-
max layer used to represent word probability dis-
tribution. Similar to (Mousa and Schuller, 2017),
we added two special tokens, <S> and </S> in
the input so as to stagger the hidden vectors, thus
avoiding self-prediction. Finally, we have the fol-
lowing formula as one part of the reward functions
in the learning framework.



172

<S>                    X1 Xn-2                         Xn-1

w1 w2 w3 w4 w5

X2                             X3                             Xn </S> 

W1                              W2                          Wn-1                    Wn

Figure 2: Graphical illustration of bi-directional
recurrent neural network language model.

RSLM (Ŷ ) = e
( 1
|Ŷ |

∑|Ŷ |
t=1

logPLM (yt|y0:t−1))
(1)

where RSLM ∈ [0, 1] and Ŷ is the predicted
compression by the policy network.

Further, it is noteworthy that the performance
comparison should be based on a similar com-
pression rate1 (CR) (Napoles et al., 2011), and a
smooth reward function RCR =

(a+b)(a+b)

aabb
xa(1−

x)b (both a, b are positive integers; e.g. a = 2,
b = 2 could lead the compression rate to 0.5) is
also used to attain a compressed sentence of simi-
lar length.

The total reward is R = RSLM +RCR. By us-
ing policy gradient methods (Sutton et al., 2000),
the policy network is updated with the following
gradient:

∇L(θ) =
|Ŷ |∑
t=1

R(Ŷ )∇logπθ(at|St) (2)

Where at ∈ {RETAIN, REMOVE}, is the ac-
tion token by the policy network, and St refers to
hidden state of the network, [hfi ;h

b
i ] (section 2.1).

3 Experiments

3.1 Data

As neural network-based methods require a large
amount of training data, we for the first time
considered using Gigaword2, a news domain cor-
pus. More specifically, the first sentence and the
headline of each article are extracted. After data
cleansing, we finally compiled 1.02 million sen-
tence and headline pairs (see details here3). It is
noteworthy that the headline is not the extractive

1compression rate is the length of compression divided by
the length of the sentence.

2https://catalog.ldc.upenn.edu/ldc2011t07
3https://github.com/code4conference/Data

compression. Further, we asked two near native
English speakers to create 200 extractive compres-
sions for the first 200 sentences of this dataset; us-
ing it as the testing set, the first 1,000 sentences
(excluding the testing set) is the development set,
and the remainder is the training set. To assess the
inter-assessor agreements, we computed Cohen ’s
unweighted κ. The computed unweighted κ was
0.423, reaching a moderate agreement level4

The second dataset we used was the Google
dataset that contains 200,000 sentence compres-
sion pairs (Filippova et al., 2015). For the purpose
of comparison, we used the very first 1,000 sen-
tences as the testing set, the next 1,000 sentences
as the development set, and the remainder as the
training set.

3.2 Comparison Methods

We choose several strong baselines; the first one is
the dependency-tree-based method that considers
the sentence compression task as an optimization
problem by using integer linear programming5.
Inspired by (Filippova and Strube, 2008), (Clarke
and Lapata, 2008), and (Wang et al., 2017), we de-
fined some constrains: (1) if a word is retained in
the compression, its parent should be also retained.
(2) whether a word wi is retained should partly
depend on the word importance score that is the
product of the TF-IDF score and headline score
h(wi), tf -idf(wi) · h(wi) where h(wi) represents
that whether a word (limited to nouns and verbs) is
also in the headline. h(wi)=5 if wi is in the head-
line; h(wi)=1 otherwise. (3) the dependency rela-
tions, ROOT, dobj, nsubj, pobj, should be retained
as they are the skeletons of a sentence. (4) the sen-
tence length should be over than α but less than
β. (5) the depth of the node (word), λdep(wi),
in the dependency tree. (6) the word with the de-
pendency relation amod is to be removed. It is
noteworthy that the method is unsupervised.

The second method is the long short-term
memory networks (LSTMs) which showed strong
promise in sentence compression by (Filippova
et al., 2015). The labels were obtained using the
dependency tree pruning method (Filippova and
Altun, 2013) and the LSTMs were applied in a su-
pervised manner. Following their works, we also

4(Landis and Koch, 1977) characterize κ values <0 as no
agreement, 0 ∼ 0.20 as slight, 0.21 ∼ 0.40 as fair, 0.41 ∼
0.60 as moderate, 0.61 ∼ 0.80 as substantial, and 0.81 ∼ 1
as almost perfect agreement.)

5we use http://pypi.python.org/pypi/PuLP



173

Gigaword Dataset Annotator 1 Annotator 2
F1 RASP-F1 F1 RASP-F1 CR

#1 Seq2seq with attention 54.9 60.3 58.6 64.6 0.53
#2 Dependency tree+ILP 58.0 65.1 61.0 70.9 0.55
#3 LSTMs+pseudo label 60.3 64.1 64.1 69.2 0.51
#4 Evaluator-LM 64.5 67.3 66.9 72.2 0.50
#5 Evaluator-SLM 65.0 69.6 68.2 73.9 0.51

Table 1: F1 and RASP-F1 results for Gigaword dataset.

consider the labels yielded by our dependency-
tree-based method as pseudo labels and employ
LSTMs as a baseline.

Furthermore, for a comprehensive comparison,
we applied the sequence-to-sequence with atten-
tion method widely used in abstractive text sum-
marization for sentence compression. Previous
works such as (Rush et al., 2015; Chopra et al.,
2016) have shown promising results with this
framework, although the focus was generation-
based summarization rather than extractive sum-
marization. More specifically, the source sequence
of this framework is the original sentence, while
the target sequence is a series of zeros and ones
(zeros represents REMOVE and ones represents
RETAIN). Further, we incorporated dependency
labels and part-of-speech tag features in the source
side of the sequence-to-sequence method.

3.3 Training

The embedding size for word, part-of-speech tag,
and the dependency relation is 128. We employed
the vanilla RNN with a hidden size of 512 for both
the policy network and neural language model.
The mini-batch size was chosen from [5, 50, 100].
Vocabulary size was 50,000. The learning rate for
neural language model is 2.5e-4, and 1e-05 for
the policy network. For policy learning, we used
the REINFORCE algorithm (Williams, 1992) to
update the parameters of the policy network and
find an policy that maximizes the reward. Because
starting from a random policy is impractical ow-
ing to the high variance, we pre-trained the policy
network using pseudo labels in a supervised man-
ner. For the comparison methods, the hyperparam-
eters and were set to 0.4 and 0.7, respectively, and
was set to 0.5. For reproduction, we released the
source code here6.

6https://github.com/code4conference/code4sc

4 Result and Discussion

This section demonstrates the experimental results
on both datasets. As the Gigaword dataset has no
ground truth, we evaluated the baseline and our
method on the 200-sentence test sets created by
two human annotators. For the automatic evalua-
tion, we employed F1 and RASP-F1 (Briscoe and
Carroll, 2002) to measure the performances. The
latter compares grammatical relations (such as nc-
subj and dobj ) found in the system compressions
with those found in the gold standard, providing
a means to measure the semantic aspects of the
compression quality. For the human evaluation,
we asked two near native English speakers to as-
sess the quality of 50 compressed sentences out
of the 200-sentence test set in terms of readability
and informativeness. Here are our observations:

Gigaword Readability Informativeness
$1 LSTMs 3.56 3.10
$2 SLM 4.16† 3.16

Table 2: Human Evaluation for Gigaword dataset.
†stands for significant difference with 0.95 confi-
dence in the column.

Google Dataset F1 RASP-F1 CR
&1 Seq2seq with attention 71.7 63.8 0.34
&2 LSTM (Filippova, 2015) 82.0 - 0.38
&3 LSTMs (our implement) 84.8 81.9 0.40
&4 Evaluator-LM 85.0 82.0 0.41
&5 Evaluator-SLM 85.1 82.3 0.39

Table 3: F1 and RASP-F1 results for Google
dataset.

(1) As shown in Table 1, our Evaluator-SLM-
based method yields a large improvement over the
baselines, demonstrating that the language-model-
based evaluator is effective as a post-hoc gram-
mar checker for the compressed sentences. This
is also validated by the significant improvement
in the readability score in Table 2 ($1 vs $2). To
investigate the evaluator in detail, a case study is
shown in section 4.1.



174

Case study
SENTENCE The Dalian shipyard has built two new huge ships

POS tags DET ADJ NOUN VERB VERB NUM ADJ ADJ NOUN
DEP. rels det compound nsubj aux root nummod amod amod dobj

#1 The Dalian shipyard has built two new huge ships
#2 The Dalian shipyard has built two new huge 
#3 The Dalian shipyard has two new huge ships
#4 The Dalian has built two new huge ships
#5 The Dalian has built two ships
#6 The has built two ships
#7 The Dalian shipyard has built two huge ships
#8 The Dalian shipyard has built two ships
#9 The shipyard has built two ships

e-logR

59.8
140.5
582.9
1313.5
1244.8
1331.2
46.9
18.2
66.5

Figure 3: Case study for evaluator.

(2) by comparing annotator 1 with annotator 2
in Table 1, we observed different performances for
two annotated test sets, showing that compress-
ing a text while preserving the original sentence
is subjective across the annotators.

(3) As for Google news dataset, LSTMs
(LSTM+pos+dep) (&3) is a relatively strong base-
line, suggesting that incorporating dependency re-
lations and part-of-speech tags may help model
learn the syntactic relations and thus make a bet-
ter prediction. When further applying Evaluator-
SLM, only a tiny improvement is observed (&3
vs &4), not comparable to the improvement be-
tween #3 and #5. This may be due to the differ-
ence in perplexity of the our Evaluator-SLM. For
Gigaword dataset with 1.02 million instances, the
perplexity of the language model is 20.3, while
for the Google news dataset with 0.2 million in-
stances, the perplexity is 76.5.

(4) To further explore the degree to which syn-
tactic knowledge (dependency relations and part-
of-speech tags) is helpful to evaluator (language
model), we implemented a naive language model,
i.e., Evaluator-LM, which did not include depen-
dency relations and part-of-speech tags as input
features. The results shows that small improve-
ments are observed on two datasets (#4 vs #5;
&4 vs &5), suggesting that incorporating syntactic
knowledge may help evaluator to encourage more
unseen but reasonable word collocations.

4.1 Evaluator Analysis

To further analyze the Evaluator-SLM perfor-
mance, we used an example sentence, “The Dalian
shipyard has built two new huge ships” to observe
how a language model scores different word dele-
tion operations. We converted the reward function
RSLM to e−logRSLM for a better observation (sim-

ilar to ”sentence perplexity”, the higher the score
is, the worse is the sentence). As shown in Figure
3, deleting the object(#2), verb(#3), or subject(#4)
results in a significant increase in ”sentence per-
plexity”, implying that the syntax-based language
model is highly sensitive to the lack of such syn-
tactic components. Interestingly, when deleting
words such as new or/and huge, the score be-
comes lower, suggesting that the model may pre-
fer short sentences, with unnecessary parts such
as amod being removed. This property makes it
quite suitable for the sentence compression task
aiming to shorten sentences by removing unnec-
essary words.

5 Conclusion

We presented a syntax-based language model
for the sentence compression task. We em-
ployed unsupervised methods to yield labels to
train a policy network in a supervised man-
ner. The experimental results demonstrates that
the compression could be further improved by a
post-hoc language-model-based evaluator, and our
evaluator-enhanced model performs better or com-
parable upon the evaluation metrics on two large-
scale datasets.

Acknowledgments

This work was supported by JSPS KAKENHI
Grant Numbers JP15H01721, JP18H03297. We
are thankful for the reviewers’ helpful comments
and suggestions. We would also like to thank Fil-
ippova for sharing their data with us and Clarke for
the Annotator Sentence Compression Instructions
in his PhD dissertation.



175

References
Daniel Andor, Chris Alberti, David Weiss, Aliaksei

Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers). volume 1, pages 2442–2452.

Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1. Association for Com-
putational Linguistics, pages 481–490.

Joachim Bingel and Anders Søgaard. 2016. Text sim-
plification as tree labeling. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers). vol-
ume 2, pages 337–343.

Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the Third International Conference on Language
Resources and Evaluation (LREC’02).

Sumit Chopra, Michael Auli, and Alexander M Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies. pages 93–98.

James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression an integer program-
ming approach. In Proceedings of the COL-
ING/ACL on Main conference poster sessions. As-
sociation for Computational Linguistics, pages 144–
151.

James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research 31:399–429.

Katja Filippova, Enrique Alfonseca, Carlos A Col-
menares, Lukasz Kaiser, and Oriol Vinyals. 2015.
Sentence compression by deletion with lstms. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. pages
360–368.

Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing. pages
1481–1491.

Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. Fifth Inter-
national Natural Language Generation Conference
on - INLG ’08 page 25.

Hongyan Jing. 2000. Sentence reduction for automatic
text summarization. In Proceedings of the sixth con-
ference on Applied natural language processing. As-
sociation for Computational Linguistics, pages 310–
315.

Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. In Proceedings of the Seventeenth National
Conference on Artificial Intelligence and Twelfth
Conference on Innovative Applications of Artificial
Intelligence. AAAI Press, pages 703–710.

J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics pages 159–174.

Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In 11th Con-
ference of the European Chapter of the Association
for Computational Linguistics.

Amr Mousa and Björn Schuller. 2017. Contextual bidi-
rectional long short-term memory recurrent neural
network language models: A generative approach to
sentiment analysis. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Volume 1, Long Pa-
pers. volume 1, pages 1023–1032.

Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation. pages 91–97.

Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing. pages 379–389.

Richard S Sutton, David A McAllester, Satinder P
Singh, and Yishay Mansour. 2000. Policy gradi-
ent methods for reinforcement learning with func-
tion approximation. In Advances in neural informa-
tion processing systems. pages 1057–1063.

Liangguo Wang, Jing Jiang, Hai Leong Chieu,
Chen Hui Ong, Dandan Song, and Lejian Liao.
2017. Can syntax help? improving an lstm-based
sentence compression model for new domains. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). volume 1, pages 1385–1393.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. In Reinforcement Learning,
Springer, pages 5–32.

Yang Zhao, Hajime Senuma, Xiaoyu Shen, and Akiko
Aizawa. 2017. Gated neural network for sentence
compression using linguistic knowledge. In Interna-
tional Conference on Applications of Natural Lan-
guage to Information Systems. Springer, pages 480–
491.


