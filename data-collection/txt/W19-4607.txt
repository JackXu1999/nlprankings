



















































Arabic Named Entity Recognition: What Works and What’s Next


Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 60–67
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

60

Arabic Named Entity Recognition: What Works and What’s Next

Liyuan Liu
University of Illinois
Urbana-Champaign

ll2@illinois.edu

Jingbo Shang
University of Illinois
Urbana-Champaign

shang7@illinois.edu

Jiawei Han
University of Illinois
Urbana-Champaign

hanj@illinois.edu

Abstract

This paper presents the winning solution to the
Arabic Named Entity Recognition challenge
run by Topcoder.com. The proposed model
integrates various tailored techniques together,
including representation learning, feature en-
gineering, sequence labeling, and ensemble
learning. The final model achieves a test F1
score of 75.82% on the AQMAR dataset and
outperforms baselines by a large margin. De-
tailed analyses are conducted to reveal both its
strengths and limitations. Specifically, we ob-
serve that (1) representation learning modules
can significantly boost the performance but
requires a proper pre-processing and (2) the
resulting embedding can be further enhanced
with feature engineering due to the limited size
of the training data. All implementations and
pre-trained models are made public1.

1 Introduction

Aiming to identify entities in natural language,
named entity recognition (NER) serves as one
of the fundamental steps in various applications.
In many languages, the performance of NER
has been significantly improved because of re-
cent advances in representation learning (Peters
et al., 2018; Akbik et al., 2018). To promote
the development of Arabic NER, a challenge was
hosted on Topcoder.com2 based on the public Ara-
bic NER benchmark dataset (i.e., the AQMAR
dataset) (Mohit et al., 2012). Challenge submis-
sions were required to only use annotations from
the training set, and manual reviews on the sub-
mitted solutions were further conducted to prevent
cheating.

1https://github.com/LiyuanLucasLiu/
ArabicNER

2https://www.topcoder.com/challenges/
30087004

Among 137 registrants competing in the chal-
lenge3, we placed the first by tailoring various
techniques and incorporating them all together. In-
tuitively, it is hard to only rely on feature engineer-
ing to capture textual signals, especially for mor-
phologically rich languages like Arabic (Habash,
2010). At the same time, neural networks have
demonstrated their great potentials to automate
high-quality representation construction in an end-
to-end manner. Therefore, we leverage embedding
modules to represent words with pre-trained vec-
tors for a better quality. Besides, we observe that
handcrafted features can bring a considerable im-
provement. Consuming all these features, we train
multiple LSTM-CRF models to construct the map-
ping from representations to predictions, and fur-
ther aggregate their outputs with ensemble learn-
ing. Moreover, we incorporate a dictionary-based
string matching model and observe that it can im-
prove the recall at some cost of precision, which
results in a marginal F1-score improvement.

Our final ensemble model achieves a test F1
score of 75.82%, outperforming all other partici-
pants as well as the previous state-of-the-arts by
significant margins. We further conduct analyses
on our solution to get deeper insights on the task:
(1) the effectiveness of representation learning and
(2) the role of feature engineering.

The rest of paper is organized as follow. The
next section discusses related work. Section 3
introduces the problem setting and presents the
data analysis. The proposed framework is pre-
sented in Section 4, including model ensemble and
dictionary-based model. Tailored representations
modules are introduced in Section 5. Finally, we
discuss the experimental results in Section 6.

3220 submissions from 30 participates are made in total.

https://github.com/LiyuanLucasLiu/ArabicNER
https://github.com/LiyuanLucasLiu/ArabicNER
https://www.topcoder.com/challenges/30087004
https://www.topcoder.com/challenges/30087004


61

2 Related Work

Typically, named entity recognition is conducted
as a sequence labeling task. Before deep learning
demonstrated its effectiveness, traditional meth-
ods rely on handcrafted features (e.g., features
based on POS tags) and language-specific re-
sources (e.g., gazetteers) to capture textual signals.
Machine learning models like conditional random
field (CRF) and hidden Markov model (HMM) are
employed to capture the label dependency (Laf-
ferty et al., 2001; Florian et al., 2003; Chieu and
Ng, 2002). Many attempts have been made to re-
duce the reliance on feature engineering or other
human endeavors, which makes the NER task be
solved in an end-to-end manner (Lample et al.,
2016; Ma and Hovy, 2016; Shang et al., 2018).
Recent studies have revealed that language model
is an effective representation module for NER (Pe-
ters et al., 2017, 2018; Liu et al., 2018b; Akbik
et al., 2018; Liu et al., 2018a).

At the same time, many approaches have been
proposed specifically to solve the NER task in
Arabic. Traditional Arabic NER models are
mostly rule-basedmodels (Shaalan, 2014). Re-
cently, people have started to attach this task
with machine learning methods (Helwe and El-
bassuoni, 2017; Gridach, 2016). To further im-
prove the performance, attempts have been made
to combine both rule-based and learning-based ap-
proaches into a unified framework (Pasha et al.,
2014; Abdelali et al., 2016). Besides, incorpo-
rating additional supervision from other domains
or languages has been explored as well (Darwish,
2013).

3 Problem Setting

In this section, we first introduce the problem set-
ting of sequence labeling. Then, we discuss the
aforementioned Arabic NER challenge.

3.1 Sequence Labeling

In the sequence labeling framework, NER prob-
lems are usually annotated following the label-
ing schemes like BIO and IOBES. These labeling
schemes help us encode the information about en-
tities (Ratinov and Roth, 2009). For example, in
the BIO scheme, when a token sequence is identi-
fied as a named entity, its starting token and mid-
dle/end tokens are labeled as B- and I- followed
by the type; and all other words are labeled as

O. The IOBES scheme is similar to BIO but fur-
ther use S- for singleton entity and E- for end-of-
entity, respectively.

Using such labels, we define the input se-
quence as X = {x1, x2, . . . , xT }, where xi
is i-th token and its label is yi. Moreover,
we define the character-level input for X as
C = {c1,1, c1,2, · · · , c1, , c2,1, · · · , cT, }, where
{ci,1, · · · , ci, } are the characters contained in the
word xi and ci, is the space character right after
xi. Then, the goal of NER becomes to predict the
label yi for each token xi in the input sequence X .

3.2 Arabic NER Challenge

The Arabic NER challenge uses the public Ara-
bic NER benchmark dataset (i.e., the AQMAR
dataset) (Mohit et al., 2012). Its annotated en-
tities are classified into four types (i.e., “Per-
son”, “Location”, “Organization” and “Miscella-
neous”). This dataset contains 28 hand-annotated
Arabic Wikipedia articles, 14 articles are used as
the training set, 7 articles are used as the develop-
ment set, and 7 articles are used as the test set.

Data cleaning is further conducted on this
dataset. Specifically, we observed that the label
sequence is encoded in a noisy manner. For ex-
ample, some entities are labelled as {B-, O, I-},
while the legit label sequence should be {B-, I-,
I-}; Some entities are labelled as {B-T0, I-T1}
(here, T0 and T1 are two different entity types),
while the legit label sequence should be {B-T0,
B-T1}. In the pursuit of more powerful models
and more meaningful comparisons, we conduct
a label cleaning to regularize the label sequence.
The resulting dataset is released for future study4,
and its statistics are summarized in Table ??. In
the following sections, all comparisons are con-
ducted on this cleaned dataset.

4 Model Framework

As visualized in Figure 1, we design a hetero-
geneous framework, which incorporates various
techniques: (1) It employs representation learning
and sequence labeling as the basic sequence label-
ing model; (2) It leverages ensemble learning to
combine outputs from different NER models; and
(3) It further incorporates a dictionary-based string
matching model.

4https://github.com/LiyuanLucasLiu/
ArabicNER

https://github.com/LiyuanLucasLiu/ArabicNER
https://github.com/LiyuanLucasLiu/ArabicNER


62

.      ةسفان@ا      ةيادبل      =قباسلا      نيرهشلا      ىتح      اقيرف      ةيبوروأ      ةلود      لسرت  مل      ،      لعفلابو

Word
EmbeddingFeature

Engineering

Contextualized
 Representation

O B—LOCO
I— 

LOC O OO O

CRF For Model Ensembling

OOOOO

Building Dictionary-based
Named Entity Recognition

Model

Integrating Dictionary-based Model Outputs

Translation: Indeed, no European country has sent a team until the two months before the start of the competition.

Sequence Labeling

Dictionary-based

Integrated Results

B-LOC O O

B-LOC I-LOC B-LOC

B-LOC O B-LOC

Figure 1: Our proposed framework for Arabic NER.

Train Dev Test

# Sent 1,329 711 606
# Token 36,050 19,519 18,284

# Char 156,941 83,267 80,565
# PER 752 292 424
# LOC 971 146 326
# ORG 234 114 102

# MISC 1,092 660 722

Table 1: Dataset Statistics of the AQMAR dataset.

4.1 Sequence Labeling Model

As to the basic sequence labeling model, we as-
sume there are n different representation modules,
namely Mi (1 ≤ i ≤ n). Given the j-th to-
ken in the input sequence, the representation vec-
tor produced by module Mi is denoted as fi,j . In
this paper, we concatenate the output from differ-
ent modules as the representation (input of LSTM-
CRF), i.e., fj = [f1,j ; f2,j ; · · · ; fn,j ]. Given the in-
put sequence X , we define its token representa-
tions as F = {f1, f2, · · · , fT }. Building upon rep-
resentation modules, we use LSTM-CRF (Huang
et al., 2015) to conduct entity extraction: we first
feed F into Bi-LSTMs, whose outputs are marked
as Z = {z1, z2, · · · , zT }. A linear-chain CRF

is further leveraged to model the whole label se-
quence simultaneously. Specifically, for the input
sequence Z, CRF defines the conditional probabil-
ity of Y = {y1, · · · , yT } as

p(Y|Z) =
∏T

t=1 φ(yt−1, yt, zt)∑
Ŷ∈Y(Z)

∏T
t=1 φ(ŷt−1, ŷt, zt)

(1)

where Ŷ = {ŷ1, · · · , ŷT } is a possible label se-
quence, Y(Z) refers to the set of all possible label
sequences for Z, and φ(yt−1, yt, zt) is the poten-
tial function of the CRF. In this paper, we define
the potential function as:

φ(yt−1, yt, zt) = exp(Wytzt + byt−1,yt)

where Wyt and byt−1,yt are the weight and bias.
During the model training, we use the negative

log-likelihood of Equation 1 as the loss function.
In the inference stage, the predicted label sequence
for input X is the one maximizing the probability
in Equation 1. Although the denominator in Equa-
tion 1 contains an exponential number of terms 5,
due to the definition of the potential function, both
training and inference can be efficiently conducted
using dynamic programming.

5The number of terms is exponential to the sequence
length T .



63

The dictionary-based NER model and represen-
tation learning modules would be introduced in the
following sections.

4.2 Sequence Labeling Model Ensemble

To get better performance, we applied the ensem-
ble learning on sequence labeling results. Specif-
ically, as in Figure 1, multiple NER models are
separately trained with the shared representation
modules, and their results are combined as the fi-
nal output.

Specifically, we refer the output ofN models as
{Ŷ1, Ŷ2, · · · , ŶN}, where Ŷi = {ŷi,1, · · · , ŷi,T }.
Following the previous work (Nguyen and Guo,
2007), we first construct a list of transition
matrices {R1, · · · , RT−1}, where Ri(j, k) =
|{n|ŷn,i = j, ŷn,i+1 = k}| is the number of times
that i-th and i+1-th tokens are labelled as j and k
in {Ŷn}. Also, we calculate Bi(j) = |{n|ŷn,i =
j}|, which is the times of i-th token being labelled
as j. Then the integrated label sequence is calcu-
lated with dynamic programming:

Ŷ = argmax
T−1∑
t=1

Rt(ŷt, ŷt+1) +
T∑
t=1

Bt(ŷt)

where Ŷ = {ŷ1, · · · , ŷT } is the integrated label
sequence.

4.3 Dictionary-based NER Model

Besides the sequence labeling ensemble model,
we also incorporate a dictionary-based NER
model. Specifically, we first build a dictionary to
map surface names to their types from the training
set, then apply this dictionary via string matching.
We will add the dictionary-extracted entities into
the final prediction, if and only if they do not con-
flict with the sequence labeling results. For ex-
ample, in Figure 1, since the two-word entity (i.e.,
B-LOC I-LOC) detected by the dictionary-based
model overlaps with the sequence labeling results,
this entity is dropped; At the same time, because
the one-word entity (i.e., the second B-LOC) de-
tected by the dictionary-based model is not over-
lapped with any entities detected by the sequence
labeling model, it is therefore integrated to the fi-
nal results. In our experiments, we found this en-
richment by the dictionary-based model improves
the recall at a relatively smaller cost of the preci-
sion, thus improving the F1 score.

5 Representation Learning Modules

In this section, we introduce the three representa-
tion learning modules: (1) word embedding, (2)
contextualized representation, and (3) handcrafted
features.

5.1 Word Embedding

Based on the distributional hypothesis (i.e.,
“a word is characterized by the company it
keeps” (Harris, 1954)), word embedding meth-
ods aim to learn the distributed representations
by analyzing their contexts (Mikolov et al.,
2013). Recent work shows that word embedding
could uncover textual information of various lev-
els (Artetxe et al., 2018). Hence, we leverage word
embedding as a part of the word representation.
Due to the limited size of the training set, we fix
the pre-trained word embedding during the train-
ing of NER models. When the pre-trained embed-
ding has a high dimension, we will add a linear
projection to further project them to a relatively
low dimension.

5.2 Contextualized Representation

Contextualized representations have been widely
adopted in the state-of-the-art sequence labeling
models. Typically, they rely on bidirectional neu-
ral language models to capture the local contex-
tual information before and after a certain word.
Such representations provide rich, supplementary
information to the context-agnostic information
contained in a word embedding. Specifically,
character-level language models are first used to
provide additional supervision (Liu et al., 2018b),
and further exploration observes its effectiveness
as the pre-training task to construct contextualized
word representation (Akbik et al., 2018).

We present the details of character-level lan-
guage modeling and integration as below.

Character-Level Language Modeling. A bi-
directional character-level language model con-
tains two character-level language models to cap-
ture information from two directions. Character-
level language modeling aims to model the proba-
bility distribution of the character sequence. Typi-
cally, the probability of the sequence {c1, · · · , cT }
is defined in a “forward” manner: p(c1, · · · , cT ) =∏T

t=1 p(ct|c1, · · · , ct−1).
To calculate this conditional probability, we first

map the input sequence C to a list of charac-
ter embedding vectors and pass them into a re-



64

current neural network, whose output is referred
to ht. Then, the probability p(ct|c1, · · · , ct−1)
is calculated using the softmax function. The
backward language model is the same as the
forward language model, except that it decom-
poses the probability of the sequence {c1, · · · , cT }
from the end to the front as p(c1, · · · , cT ) =∏T

t=1 p(ct|ct+1, · · · , cT ). Its output for character
ct is denoted as hrt . Both language models use
negative log-likelihood as the training objective.

Language Model Integration. Using the bidi-
rectional character-level language models, we
construct contextualized representations for each
word. Specifically, we feed the input character
sequence C to language models, and then con-
catenate the hidden state of the forward language
model at ci, and the hidden state of the back-
ward language model at ci−1, as the representa-
tions for xi. We refer these two hidden states as hi
and hri . Due to the complex nature of natural lan-
guage, large dimensions of hi and hri are usually
required in language models, which might lead to
overfitting in the NER task. To avoid such cases,
we add a linear transformation layer to project hi
and hri to a lower dimension. In details, we use
ri = Wcr · [hi,hri ] + bcr, where Wcr and bcr
are parameters to learn during the training of NER
models. The output ri is the contextualized repre-
sentation for xi.

5.3 Handcrafted Features

Due to the limited amount of available annota-
tions, we further handcraft word shape features
to help the model better capture the textual fea-
tures. Specifically, all words are classified into
three classes: (1) We mark all numbers as “num”;
(2) For remaining words, if it contains English
characters, it would be marked as “en”; (3) Oth-
erwise, it would be marked as “ar”. These three
categories would be further mapped to three dif-
ferent vectors as the token representation.

Although these handcrafted features are quite
simple, similar to existing work (Dozat, 2016), it
results in a remarkable performance improvement
in our experiments. More discussion on this fea-
ture engineering design is included in Section 6.

6 Experiments

In this section, we present the experimental results
on the AQMAR dataset.

6.1 Implementation Detail

As to pre-trained language models, we conduct
training on the Arabic Wikipedia texts with a
vocabulary of 256 characters (out-of-vocabulary
characters are mapped to a special <UNK> char-
acter). Since the resulting language model would
be used to construct contextualized represen-
tations for the downstream task, whose input
would be space separated, we conduct further pre-
processing. Specifically, we first tokenize the text,
then concatenate the token sequence by space. To
demonstrate the importance of pre-processing, we
trained two kinds of language models, one with
pre-processing, and the other without.

For pre-trained word embedding, we adopt two
sets of pre-trained embedding. One is trained
with the word2vec model (Mikolov et al., 2013).
It has 100 dimensions and is public available 6.
The other is trained with the Fasttext model (Bo-
janowski et al., 2017), which is released together
with 156 other languages 7. It has 300 dimensions
and would be projected to 100 dimensions before
concatenating with other vectors.

6.2 Hyper-parameter

For language model training, we use
Nadam (Dozat, 2016) as the optimizer, set
the learning rate as 0.002, clip the gradient at
1, set the batch size as 128 and limit the back
propagation length to 256. As to the RNN, we use
one-layer LSTMs with 2048 hidden states. We
set its character embedding to be 128 dimensional
and project its outputs to 50 dimension before
concatenating with other vectors.

As to the sequence labeling task, we use LSTMs
with 250 hidden states in the LSTM-CRF layer,
and apply dropout with a ratio of 0.5, and use
additional word dropout to each representation
module with a ratio of 0.1. Following the pre-
vious work (Reimers and Gurevych, 2017), we
use Nadam (Dozat, 2016) as the optimizer, set the
learning rate as 0.002, clip the learning rate at 1
and set the batch size as 32.

6.3 Performance Comparison

As summarized in Table 2, our final model
achieves a F1 score of 75.82%. Further ablation
study is conducted to analyze the effectiveness of
each module.

6https://github.com/bakrianoo/aravec
7https://fasttext.cc/docs/en/crawl-vectors.html



65

Methods Pre Rec F1

Final Model 81.06 71.22 75.82
– Dict-based 81.27 70.84 75.70
– Ensemble 79.33 68.99 73.80
– Word shape 76.43 67.13 71.47
– Pre-process 71.60 61.33 66.07
– Language model 66.92 45.96 54.50

Table 2: Model Performance and Ablation Study for
the AQMAR dataset.

Ablation Study Setting. In the ablation study,
we first detach the dictionary-based NER from
the resulting system and refer ensemble sequence
labeling model as “– Dict-based”. Then, we
refer the basic sequence labeling model as “–
Ensemble”. After that, we detach hand-crafted
features and mark the resulting model as “–
Word shape”. Pre-processing is further removed
from language model training, which is marked
as “– Pre-process”. In the end, we remove lan-
guage model which leads to a typical LSTM-CRF
model (Huang et al., 2015) with pre-trained word
embedding, we refer this model as “– Language
model”. Their results are summarized in Table 2.

Discussion. We find that the dictionary-based
NER model8 improves the recall at the cost of the
precision and improves the F1 score by a small
margin. Also, we observe that the results demon-
strate the effectiveness of ensemble learning. At
the same time, we find the major F1 improve-
ments come from a better capturing of task-related
signals. For example, by properly adding lan-
guage models or designing handcrafted features,
the F1 boosts significantly. It verifies the effec-
tiveness of contextualized representation. Also, it
reveals the weakness of these techniques. Specifi-
cally, although the constructed character-level lan-
guage model has the potential to capture the word
shape signals, adding handcrafted features (i.e.,
word shape) can improve the F1 from 71.47% to
73.80%. We conjugate this is caused by the lim-
ited size of training data with English entities,
which limits the model from properly construct-
ing task-related representations. Further compar-
ison between these two models finds their major
differences are the predictions for entities contain-
ing both Arabic and English and validates our in-
tuition. Besides, we find the pre-processing used

8The dictionary-based NER model achieves Pre: 64.35%,
Rec: 8.83%, F1: 15.53%.

in language model training is crucial for the per-
formance, which has a big impact on the model
performance (from 66.07% to 71.47%). The main
reason is that although pre-trained language mod-
els are powerful, they are agnostic to the target
task corpus and suffer from their differences.

7 Conclusion

In this paper, we introduce the winning solution to
the Arabic Named Entity Recognition challenge.
First of all, we give a detailed introduction on sys-
tem design and the integrated technologies. We
further conduct ablation study to reveal the effec-
tiveness of each module and figure out all modules
bring performance improvements. We observe
that properly capturing the task-related features
is crucial to the performance. We also noticed
the current contextualized representation learning
techniques, although effective, could be further
enhanced by incorporating handcrafted features to
better handle some corner cases.

Acknowledge

Research was sponsored in part by U.S. Army
Research Lab. under Cooperative Agreement
No. W911NF-09-2-0053 (NSCTA), DARPA
under Agreement No. W911NF-17-C-0099,
National Science Foundation IIS 16-18481,
IIS 17-04532, and IIS-17-41317, DTRA HD-
TRA11810026, and grant 1U54GM114838
awarded by NIGMS through funds provided by
the trans-NIH Big Data to Knowledge (BD2K)
initiative (www.bd2k.nih.gov). Any opinions,
findings, and conclusions or recommendations
expressed in this document are those of the au-
thor(s) and should not be interpreted as the views
of any U.S. Government. The U.S. Government
is authorized to reproduce and distribute reprints
for Government purposes notwithstanding any
copyright notation hereon. This research was
supported by grant 1U54GM114838 awarded by
NIGMS through funds provided by the trans-
NIH Big Data to Knowledge (BD2K) initiative
(www.bd2k.nih.gov). The views and conclusions
contained in this paper are those of the authors
and should not be interpreted as representing any
funding agencies.



66

References
Ahmed Abdelali, Kareem Darwish, Nadir Durrani, and

Hamdy Mubarak. 2016. Farasa: A fast and furious
segmenter for arabic. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Demon-
strations, pages 11–16.

Alan Akbik, Duncan Blythe, and Roland Vollgraf.
2018. Contextual string embeddings for sequence
labeling. In Proceedings of the 27th International
Conference on Computational Linguistics, pages
1638–1649.

Mikel Artetxe, Gorka Labaka, Inigo Lopez-Gazpio,
and Eneko Agirre. 2018. Uncovering divergent
linguistic information in word embeddings with
lessons for intrinsic and extrinsic evaluation. In Pro-
ceedings of the 22nd Conference on Computational
Natural Language Learning, pages 282–291.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Hai Leong Chieu and Hwee Tou Ng. 2002. Named en-
tity recognition: a maximum entropy approach using
global information. In COLING.

Kareem Darwish. 2013. Named entity recognition us-
ing cross-lingual resources: Arabic as an example.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), volume 1, pages 1558–1567.

Timothy Dozat. 2016. Incorporating nesterov momen-
tum into adam.

Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In NAACL.

Mourad Gridach. 2016. Character-aware neural net-
works for arabic named entity recognition for social
media. In Proceedings of the 6th workshop on South
and Southeast Asian natural language processing
(WSSANLP2016), pages 23–32.

Nizar Y Habash. 2010. Introduction to arabic natural
language processing. Synthesis Lectures on Human
Language Technologies.

Zellig S Harris. 1954. Distributional structure. Word,
10(2-3):146–162.

Chadi Helwe and Shady Elbassuoni. 2017. Arabic
named entity recognition via deep co-learning. Arti-
ficial Intelligence Review, pages 1–19.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. ICML.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL-HLT, pages 260–270.

Liyuan Liu, Xiang Ren, Jingbo Shang, Xiaotao Gu,
Jian Peng, and Jiawei Han. 2018a. Efficient con-
textualized representation: Language model pruning
for sequence labeling. In EMNLP.

Liyuan Liu, Jingbo Shang, Xiang Ren,
Frank Fangzheng Xu, Huan Gui, Jian Peng,
and Jiawei Han. 2018b. Empower sequence la-
beling with task-aware neural language model.
In Thirty-Second AAAI Conference on Artificial
Intelligence.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end
sequence labeling via bi-directional lstm-cnns-crf.
arXiv preprint arXiv:1603.01354.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Behrang Mohit, Nathan Schneider, Rishav Bhowmick,
Kemal Oflazer, and Noah A Smith. 2012. Recall-
oriented learning of named entities in arabic
wikipedia. In EACL.

Nam Nguyen and Yunsong Guo. 2007. Comparisons
of sequence labeling algorithms and extensions. In
ICML, pages 681–688. ACM.

Arfath Pasha, Mohamed Al-Badrashiny, Mona T Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan Roth.
2014. Madamira: A fast, comprehensive tool for
morphological analysis and disambiguation of ara-
bic. In LREC, volume 14, pages 1094–1101.

Matthew Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language models.
In Proceedings of the ACL, volume 1, pages 1756–
1765.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of NAACL-HLT, vol-
ume 1, pages 2227–2237.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the ACL.

Nils Reimers and Iryna Gurevych. 2017. Optimal hy-
perparameters for deep lstm-networks for sequence
labeling tasks. arXiv preprint arXiv:1707.06799.



67

Khaled Shaalan. 2014. A survey of arabic named en-
tity recognition and classification. Computational
Linguistics.

Jingbo Shang, Liyuan Liu, Xiaotao Gu, Xiang Ren,
Teng Ren, and Jiawei Han. 2018. Learning named
entity tagger using domain-specific dictionary. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
2054–2064.


