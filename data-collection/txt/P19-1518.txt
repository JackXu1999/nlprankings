
























































A Deep Reinforced Sequence-to-Set Model for Multi-Label Classification


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5252–5258
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

5252

A Deep Reinforced Sequence-to-Set Model for Multi-Label Classification

Pengcheng Yang1,2, Fuli Luo2, Shuming Ma2, Junyang Lin2, Xu Sun1,2
1Deep Learning Lab, Beijing Institute of Big Data Research, Peking University

2MOE Key Lab of Computational Linguistics, School of EECS, Peking University
{yang pc, luofuli, shumingma, linjunyang, xusun}@pku.edu.cn

Abstract

Multi-label classification (MLC) aims to pre-
dict a set of labels for a given instance. Based
on a pre-defined label order, the sequence-to-
sequence (Seq2Seq) model trained via maxi-
mum likelihood estimation method has been
successfully applied to the MLC task and
shows powerful ability to capture high-order
correlations between labels. However, the
output labels are essentially an unordered set
rather than an ordered sequence. This in-
consistency tends to result in some intractable
problems, e.g., sensitivity to the label order. To
remedy this, we propose a simple but effective
sequence-to-set model. The proposed model is
trained via reinforcement learning, where re-
ward feedback is designed to be independent
of the label order. In this way, we can reduce
the dependence of the model on the label or-
der, as well as capture high-order correlations
between labels. Extensive experiments show
that our approach can substantially outperform
competitive baselines, as well as effectively re-
duce the sensitivity to the label order. 1

1 Introduction

Multi-label classification (MLC) aims to assign
multiple labels to each sample. It can be applied
in many real-world scenarios, such as text catego-
rization (Schapire and Singer, 2000) and informa-
tion retrieval (Gopal and Yang, 2010). Due to the
complex dependency between labels, a key chal-
lenge for the MLC task is how to effectively cap-
ture high-order correlations between labels (Zhang
and Zhou, 2014).

When involving in capturing high-order correla-
tions between labels, one line of research focuses
on exploring the hierarchical structure of the la-
bel space (Prabhu and Varma, 2014; Jernite et al.,
2017; Peng et al., 2018; Singh et al., 2018), while

1The code is available at https://github.com/
lancopku/Seq2Set

another line strives to extend specific learning al-
gorithms (Zhang and Zhou, 2006; Baker and Ko-
rhonen, 2017; Liu et al., 2017). However, most of
these work tends to result in intractable computa-
tional costs (Chen et al., 2017).

Recently, based on a pre-defined label order,
Nam et al. (2017); Yang et al. (2018) succeeded
in applying the sequence-to-sequence (Seq2Seq)
model to the MLC task, which shows its power-
ful ability to capture high-order label correlations
and achieves excellent performance. However, the
Seq2Seq model suffers from some thorny flaws on
the MLC task. The output labels are essentially an
unordered set with swapping-invariance2, rather
than an ordered sequence. This inconsistency usu-
ally leads to some intractable problems, e.g., sen-
sitivity to the label order. Previous work (Vinyals
et al., 2016) has shown that the order has a great
impact on the performance of the Seq2Seq model.
Therefore, the performance of classifier is sensi-
tive to the pre-defined label order. Besides, even
if the model accurately predicts all true labels, it
still may result in an unreasonable training loss
due to the inconsistent order with the pre-defined
label sequence3.

Therefore, in this work, we propose a simple but
effective sequence-to-set model, which aims at al-
leviating the dependence of the model on the label
order. Instead of maximizing the log-likelihood of
pre-defined label sequences, we apply reinforce-
ment learning (RL) (Sutton et al., 1999) to guild
the model training. The designed reward not only
comprehensively evaluates the quality of the out-
put labels, but also satisfies swapping-invariance
of the set, which leads to a reduction in the depen-
dence of the model on the label order.

2Swapping-invariance means that swapping any two ele-
ments in the set will make no difference.

3For example, for the pre-defined label sequence [A, B,
C], the training loss will be large if model generates [C, A, B].

https://github.com/lancopku/Seq2Set
https://github.com/lancopku/Seq2Set


5253

The main contributions of this paper are sum-
marized as follows:

• We propose a simple but effective sequence-
to-set (Seq2Set) model based on reinforce-
ment learning, which not only captures the
correlations between labels, but also allevi-
ates the dependence on the label order.

• Experimental results show that our Seq2Set
model can outperform baselines by a large
margin. Further analysis demonstrates that
our approach can effectively reduce the sen-
sitivity of the model to the label order.

2 Methodology

2.1 Overview
Here we define some necessary notations and de-
scribe the MLC task. Given a text sequence x con-
taining m words, the MLC task aims to assign a
subset y containing n labels in the total label set
Y to x. From the perspective of sequence learn-
ing, once the order of output labels is pre-defined,
the MLC task can be regarded as the generation of
target label sequence y conditioned on the source
text sequence x.

2.2 Neural Sequence-to-Set Model
Our proposed Seq2Set model consists of an en-
coder E and a set decoderD, which are introduced
in detail as follows.

Encoder E: We implement the encoder E as
a bidirectional LSTM. Given the input text x =
(x1, · · · , xm), the encoder computes the hidden
states of each word as follows:

−→
h i =

−−−−→
LSTM

(−→
h i−1, e(xi)

)
(1)

←−
h i =

←−−−−
LSTM

(←−
h i+1, e(xi)

)
(2)

where e(xi) is the embedding of xi. The final
representation of the i-th word is hi = [

−→
h i;
←−
h i],

where semicolon denotes vector concatenation.
Set decoder D: Due to its powerful ability of

LSTM to model sequence dependency, we also
implement D as a LSTM model to capture high-
order correlations between labels. In particular,
the hidden state st of the set decoder D at time-
step t is computed as:

st = LSTM
(
st−1, [e(yt−1); ct]

)
(3)

where [e(yt−1); ct] denotes the concatenation of
vectors e(yt−1) and ct, e(yt−1) is the embedding

of the label yt−1 generated at the last time-step,
and ct is the context vector obtained by the atten-
tion mechanism. Readers can refer to Bahdanau
et al. (2015) for more details. Finally, the set de-
coder D samples a label yt from the output proba-
bility distribution, which is computed as follows:

ot = W2f(W1st +Uct) (4)

yt ∼ softmax(ot + It) (5)

where W1, W2, and U are trainable parameters,
f is a nonlinear activation function, and It ∈ R|Y|
is the mask vector that preventsD from generating
repeated labels,

(It)i =

{
−∞ if the i-th label has been predicted.
0 otherwise.

2.3 Model Training
MLC as a RL Problem
In order to alleviate the dependence of the model
on the label order, here we model the MLC task as
a RL problem. Our set decoder D can be viewed
as an agent, whose state at time-step t is the cur-
rent generated labels (y1, · · · , yt−1). A stochastic
policy defined by the parameter of D decides the
action, which is the prediction of the next label.
Once a complete label sequence y is generated,
the agent D will observe a reward r. The training
objective is to minimize negative expected reward,
which is as follows:

L(θ) = −Ey∼pθ [r(y)] (6)

where θ refers to the model parameter. In our
model, we use the self-critical policy gradient al-
gorithm (Rennie et al., 2017). For each training
sample in the minibatch, the gradient of Eq.(6) can
be approximated as:

∇θL(θ) ≈ − [r(ys)− r(yg)]∇θlog
(
pθ(y

s)
)
(7)

where ys is the label sequence sampled from prob-
ability distribution pθ and yg is the label sequence
generated with the greedy search algorithm. r(yg)
in Eq.(7) is the baseline, which aims to reduce the
variance of gradient estimate and enhance the con-
sistency of the model training and testing to alle-
viate exposure bias (Ranzato et al., 2016).

Reward Design
The ideal reward is supposed to be a good measure
of the quality of the generated labels. Besides,



5254

Models HL (–) 0/1 Loss (–) F1 (+) Precision (+) Recall (+)

BR-LR (Boutell et al., 2004) 0.0083 0.393 0.858 0.919 0.804
PCC-LR (Read et al., 2011) 0.0079 0.325 0.864 0.901 0.827
FastXML (Prabhu and Varma, 2014) 0.0078 0.358 0.863 0.956 0.786
XML-CNN (Liu et al., 2017) 0.0086 0.390 0.853 0.914 0.799
CNN-RNN (Chen et al., 2017) 0.0085 0.378 0.856 0.889 0.825
Seq2Seq (Yang et al., 2018) 0.0076 0.332 0.871 0.906 0.838

Seq2Set (Ours) 0.0073 0.314 0.879 0.900 0.858

Table 1: Performance of different systems. “HL”, “0/1 Loss”, “F1”, “Precision”, and “Recall” denote hamming
loss, subset zero-one loss, micro-F1, micro-precision, and micro-recall, respectively. “+” indicates higher is better
and “–” is opposite. The best performance is highlighted in bold.

in order to free the model from the strict restric-
tion of label order, it should also satisfy swapping-
invariance of the output label set. Motivated by
this, we design the reward r as the F1 score cal-
culated by comparing the generated labels y with
ground-truth labels y∗.4

r(y) = F1(y,y
∗) (8)

We also tried other reward designs, such as ham-
ming accuracy. Results show that reward based on
F1 score gives the best overall performance.

3 Experiments

3.1 Datasets
We conduct experiments on the RCV1-V2 cor-
pus (Lewis et al., 2004), which consists of a large
number of manually categorized newswire stories.
The total number of labels is 103. We adopt the
same data-splitting in Yang et al. (2018).

3.2 Settings
We tune hyper-parameters on the validation set
based on the micro-F1 score. The vocabulary size
is 50,000 and the batch size is 64. we set the em-
bedding size to 512. Both encoder and set decoder
is a 2-layer LSTM with the hidden size 512, but
the former is set to bidirectional. We pre-train the
model for 20 epochs via MLE method. The opti-
mizer is Adam (Kingma and Ba, 2015) with 10−3

learning rate for pre-training and 10−5 for RL. Be-
sides, we use dropout (Srivastava et al., 2014) to
avoid overfitting and clip the gradients (Pascanu
et al., 2013) to the maximum norm of 8.

3.3 Baselines
We compare our approach with the following com-
petitive baselines:

4When calculating F1 score, we convert y and y∗ into
|Y|-dimensional sparse vectors.

• BR-LR (Boutell et al., 2004) amounts to in-
dependently training one binary classifier (lo-
gistic regression) for each label.

• PCC-LR (Read et al., 2011) transforms the
MLC task into a chain of binary classification
(logistic regression) problems.

• FastXML (Prabhu and Varma, 2014) learns a
hierarchy of training instances and optimizes
the objective at each node of the hierarchy.

• XML-CNN (Liu et al., 2017) uses a dynamic
max pooling scheme and a hidden bottleneck
layer for better representations of documents.

• CNN-RNN (Chen et al., 2017) presents an
ensemble approach of CNN and RNN to cap-
ture both global and local textual semantics.

• Seq2Seq (Nam et al., 2017; Yang et al., 2018)
adapts the Seq2Seq model to perform multi-
label classification.

3.4 Evaluation Metrics

The evaluation metrics include: subset zero-one
loss calculating the fraction of misclassifications,
hamming loss denoting the fraction of wrongly
predicted labels to total labels, and micro-F1 that
is the weighted average of F1 score of each class.
Micro-precision and micro-recall are also reported
for reference.

4 Results and Discussion

Here we conduct an in-depth analysis on the
model and experimental results. For simplicity, we
use BR to represent the baseline BR-LR.

4.1 Experimental Results

The comparison between our approach and all
baselines is presented in Table 1, showing that



5255

Models HL (–) 0/1 Loss (–) F1 (+)

BR 0.0083 (↓0.0%) 0.393 (↓0.0%) 0.858 (↓0.0%)
Seq2Seq 0.0083 (↓9.2%) 0.363 (↓9.3%) 0.859 (↓1.4%)

Seq2Set 0.0075 (↓2.7%) 0.318 (↓1.2%) 0.876 (↓0.3%)

Table 2: Comparison on the label-shuffled RCV1-V2
dataset. “↓” indicates that the model is degraded.

the proposed Seq2Set model can outperform all
baselines by a large margin in all evaluation met-
rics. Compared to BR which completely ignores
the label correlations, our Seq2Set model achieves
a reduction of 12.05% hamming-loss. It shows
that modeling high-order label correlations can
largely improve results. Compared to Seq2Seq
that makes strict requirements on the label order,
our Seq2Set model achieves a reduction of 3.95%
hamming-loss on the RCV1-V2 dataset. This in-
dicates that our approach can achieve substantial
improvements by reducing the dependence of the
model on the label order.

4.2 Reducing Sensitivity to Label Order

To verify that our approach can reduce the sensi-
tivity to the label order, we randomly shuffle the
order of the label sequences. Table 2 presents
the performance of various models on the label-
shuffled RCV1-V2 dataset. Results show that for
the shuffled label order, BR is not affected, but the
performance of Seq2Seq declines drastically. The
reason is that the decoder of Seq2Seq is essentially
a conditional language model. It relies heavily on
a reasonable label order to model the intrinsic as-
sociation between labels, while labels in this case
present an unordered state. However, our model’s
performance on subset zero-one loss drops by only
1.2%5, while Seq2Seq drops by 9.3%. This shows
that our Seq2Set model is more robust, which can
resist disturbances in the label order. Our model
is trained via reinforcement learning and reward
feedback is independent of the label order, which
reduces sensitivity to the label order.

4.3 Improving Model Universality

The labels in the RCV1-V2 dataset exhibits a long-
tail distribution. However, in real-scenarios, there
are other common label distributions, e.g., uni-
form distribution (Lin et al., 2018a). Therefore,
here we analyze the universality of the Seq2Set

5This weak decline can be attributed to the influence of
the label order on the pre-training.

Figure 1: Left: Performance of different models.
Right: The gap of performance of different models.

model, which means that it can achieve stable im-
provements in performance under different label
distributions. In detail, we remove the most fre-
quent k labels in turn on the RCV1-V2 dataset
and perform the evaluation on the remaining la-
bels. The larger the k, the more uniform the label
distribution. Figure 1 shows changes in the perfor-
mance of different systems.

First, as the number of removed high-frequency
labels increases, the performance of all methods
deteriorates. This is reasonable because predicting
low-frequency labels is relatively difficult. How-
ever, compared to other methods, the performance
of the Seq2Seq model is greatly degraded. We
suspect this is because it’s difficult to define a
reasonable order for uniformly distributed labels
while Seq2Seq imposes strict requirements on the
label order. This conflict may damage perfor-
mance. However, as shown in Figure 1, as more
labels are removed, the advantage of Seq2Set
over Seq2Seq continues to grow. This illustrates
that our Seq2Set model has excellent universality,
which works for different label distributions. Our
approach not only has the ability of Seq2Seq to
capture label correlations, but also alleviates the
strict requirements of Seq2Seq for label order via
reinforcement learning. This avoids the problem
of difficulty in predefining a reasonable label or-
der on the uniform distribution, leading to excel-
lent universality.

4.4 Error Analysis

We find that all methods perform poorly when
predicting low-frequency (LF) labels compared to
high-frequency (HF) labels. This is reasonable
because samples assigned LF labels are sparse,
making it hard for the model to learn an effec-
tive pattern to make predictions. Figure 2 shows
the results of different methods on HF labels and



5256

Figure 2: Performance of different systems on the HF
labels and LF labels. “Impv-BR” and “Impv-Seq2Seq”
denote the improvement of our model compared to BR-
LR and Seq2Seq, respectively.

LF labels6. However, compared to other systems,
our proposed Seq2Set model achieves better per-
formance on both LF labels and HF labels. Be-
sides, the relative improvements achieved by our
approach are greater on LF labels than HF labels.
In fact, the distribution of LF labels is relatively
more uniform. As analyzed in Section 4.3, the la-
bel order problem is more serious in the uniform
distribution. Our Seq2Set model can reduce the
dependence on the label order via reinforcement
learning, leading to larger improvements in per-
formance on the LF labels.

5 Related Work

Multi-label classification (MLC) aims to assign
multiple labels to each sample in the dataset. Early
work on exploring the MLC task focuses on ma-
chine learning algorithms, mainly including prob-
lem transformation methods and algorithm adap-
tation methods. Problem transformation methods,
such as BR (Boutell et al., 2004), LP (Tsoumakas
and Katakis, 2006) and CC (Read et al., 2011),
aim at mapping the MLC task into multiple single-
label learning tasks. Algorithm adaptation meth-
ods strive to extend specific learning algorithms to
handle multi-label data directly. The correspond-
ing representative work includes ML-DT (Clare
and King, 2001), Rank-SVM (Elisseeff and We-
ston, 2001), ML-KNN (Zhang and Zhou, 2007),
and so on. In addition, some other methods,
including ensemble method (Tsoumakas et al.,
2011) and joint training (Li et al., 2015), can also
be used for the MLC task. However, they can only
be used to capture the first or second order label
correlations (Chen et al., 2017), or are computa-
tionally intractable when high-order label correla-
tions are considered.

6By frequency, the top 10% of labels are regarded as HF
labels, and the last 10% of labels are regarded as LF labels.

Recent years, some neural network models have
also been successfully used for the MLC task. For
instance, the BP-MLL proposed by Zhang and
Zhou (2006) applies a fully-connected network
and the pairwise ranking loss to perform classifi-
cation. Nam et al. (2013) further replace the pair-
wise ranking loss with cross-entropy loss func-
tion. Kurata et al. (2016) present an initialization
method to model label correlations by leveraging
neurons. Chen et al. (2017) present an ensemble
approach of CNN and RNN so as to capture both
global and local semantic information. Liu et al.
(2017) use a dynamic max pooling scheme and a
hidden bottleneck layer for better representations
of documents. Graph convolution operations are
employed by Peng et al. (2018) to capture non-
consecutive and long-distance semantics. The two
milestones are Nam et al. (2017) and Yang et al.
(2018), both of which utilize the Seq2Seq model
to capture the label correlations. Going a step fur-
ther, Lin et al. (2018b) propose a semantic-unit-
based dilated convolution model and Zhao et al.
(2018) present a label-graph based neural network
equipped with a soft training mechanism to cap-
ture label correlations. Most recently, Qin et al.
(2019) present new training objectives propose
based on set probability to effectively model the
mathematical characteristics of the set.

6 Conclusion

In this work, we present a simple but effec-
tive sequence-to-set model based on reinforce-
ment learning, which aims to reduce the stringent
requirements of the sequence-to-sequence model
for label order. The proposed model not only cap-
tures high-order correlations between labels, but
also reduces the dependence on the order of out-
put labels. Experimental results show that our
Seq2Set model can outperform competitive base-
lines by a large margin. Further analysis demon-
strates that our approach can effectively reduce the
sensitivity to the label order.

Acknowledgement

We thank the anonymous reviewers for their
thoughtful comments. We also would like to thank
Lei Li, Yi Zhang, and Xuancheng Ren for their in-
sightful suggestions. Xu Sun is the contact author
of this paper.



5257

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In 3rd International
Conference on Learning Representations, Confer-
ence Track Proceedings.

Simon Baker and Anna Korhonen. 2017. Initializ-
ing neural networks for hierarchical multi-label text
classification. pages 307–315.

Matthew R. Boutell, Jiebo Luo, Xipeng Shen, and
Christopher M. Brown. 2004. Learning multi-
label scene classification. Pattern Recognition,
37(9):1757–1771.

Guibin Chen, Deheng Ye, Zhenchang Xing, Jieshan
Chen, and Erik Cambria. 2017. Ensemble applica-
tion of convolutional and recurrent neural networks
for multi-label text categorization. In 2017 Interna-
tional Joint Conference on Neural Networks, pages
2377–2383.

Amanda Clare and Ross D King. 2001. Knowledge
discovery in multi-label phenotype data. In Euro-
pean Conference on Principles of Data Mining and
Knowledge Discovery, pages 42–53. Springer.

André Elisseeff and Jason Weston. 2001. A kernel
method for multi-labelled classification. In Ad-
vances in Neural Information Processing Systems 14
[Neural Information Processing Systems: Natural
and Synthetic], pages 681–687.

Siddharth Gopal and Yiming Yang. 2010. Multilabel
classification with meta-level features. In Proceed-
ing of the 33rd International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 315–322.

Yacine Jernite, Anna Choromanska, and David Sontag.
2017. Simultaneous learning of trees and represen-
tations for extreme classification and density estima-
tion. In Proceedings of the 34th International Con-
ference on Machine Learning, pages 1665–1674.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
Conference Track Proceedings.

Gakuto Kurata, Bing Xiang, and Bowen Zhou. 2016.
Improved neural network-based multi-label classifi-
cation with better initialization leveraging label co-
occurrence. In NAACL HLT 2016, The 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 521–526.

David D. Lewis, Yiming Yang, Tony G. Rose, and Fan
Li. 2004. RCV1: A new benchmark collection for
text categorization research. Journal of Machine
Learning Research, 5:361–397.

Li Li, Houfeng Wang, Xu Sun, Baobao Chang, Shi
Zhao, and Lei Sha. 2015. Multi-label text catego-
rization with joint learning predictions-as-features
method. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 835–839.

Junyang Lin, Qi Su, Pengcheng Yang, Shuming Ma,
and Xu Sun. 2018a. Semantic-unit-based dilated
convolution for multi-label text classification. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing,, pages
4554–4564.

Junyang Lin, Qi Su, Pengcheng Yang, Shuming Ma,
and Xu Sun. 2018b. Semantic-unit-based dilated
convolution for multi-label text classification. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
4554–4564.

Jingzhou Liu, Wei-Cheng Chang, Yuexin Wu, and
Yiming Yang. 2017. Deep learning for extreme
multi-label text classification. In Proceedings of the
40th International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,,
pages 115–124.

Jinseok Nam, Jungi Kim, Iryna Gurevych, and Jo-
hannes Fürnkranz. 2013. Large-scale multi-label
text classification - revisiting neural networks. arXiv
preprint arXiv:1312.5419.

Jinseok Nam, Eneldo Loza Mencı́a, Hyunwoo J Kim,
and Johannes Fürnkranz. 2017. Maximizing subset
accuracy with recurrent neural networks in multi-
label classification. In Advances in Neural Informa-
tion Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, pages
5419–5429.

Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-
gio. 2013. On the difficulty of training recurrent
neural networks. In Proceedings of the 30th Inter-
national Conference on Machine Learning,, pages
1310–1318.

Hao Peng, Jianxin Li, Yu He, Yaopeng Liu, Mengjiao
Bao, Lihong Wang, Yangqiu Song, and Qiang Yang.
2018. Large-scale hierarchical text classification
with recursively regularized deep graph-cnn. In Pro-
ceedings of the 2018 World Wide Web Conference on
World Wide Web,, pages 1063–1072.

Yashoteja Prabhu and Manik Varma. 2014. Fastxml: A
fast, accurate and stable tree-classifier for extreme
multi-label learning. In The 20th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining,, pages 263–272.

Kechen Qin, Cheng Li, Virgil Pavlu, and Javed A
Aslam. 2019. Adapting rnn sequence prediction
model to multi-label set prediction. arXiv preprint
arXiv:1904.05829.



5258

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
ing with recurrent neural networks. In 4th Inter-
national Conference on Learning Representations,
Conference Track Proceedings.

Jesse Read, Bernhard Pfahringer, Geoff Holmes, and
Eibe Frank. 2011. Classifier chains for multi-label
classification. Machine learning, 85(3):333.

Steven J. Rennie, Etienne Marcheret, Youssef Mroueh,
Jarret Ross, and Vaibhava Goel. 2017. Self-critical
sequence training for image captioning. In 2017
IEEE Conference on Computer Vision and Pattern
Recognition,, pages 1179–1195.

Robert E Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine learning, 39(2-3):135–168.

Gaurav Singh, James Thomas, Iain James Marshall,
John Shawe-Taylor, and Byron C. Wallace. 2018.
Structured multi-label biomedical text tagging via
attentive neural tree decoding. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, pages 2837–2842.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: a simple way to prevent neural
networks from overfitting. Journal of Machine
Learning Research, 15(1):1929–1958.

Richard S Sutton, David A McAllester, Satinder P
Singh, and Yishay Mansour. 1999. Policy gradi-
ent methods for reinforcement learning with func-
tion approximation. In Advances in Neural Infor-
mation Processing Systems 12, [NIPS Conference],
pages 1057–1063.

Grigorios Tsoumakas and Ioannis Katakis. 2006.
Multi-label classification: An overview. Interna-
tional Journal of Data Warehousing and Mining,
3(3).

Grigorios Tsoumakas, Ioannis Katakis, and Ioannis
Vlahavas. 2011. Random k-labelsets for multilabel
classification. IEEE Transactions on Knowledge
and Data Engineering, 23(7):1079–1089.

Oriol Vinyals, Samy Bengio, and Manjunath Kudlur.
2016. Order matters: Sequence to sequence for sets.
In 4th International Conference on Learning Repre-
sentations, Conference Track Proceedings.

Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma, Wei
Wu, and Houfeng Wang. 2018. SGM: Sequence
generation model for multi-label classification. In
Proceedings of the 27th International Conference on
Computational Linguistics, pages 3915–3926.

Min-Ling Zhang and Zhi-Hua Zhou. 2006. Multilabel
neural networks with applications to functional ge-
nomics and text categorization. IEEE Transactions
on Knowledge and Data Engineering, 18(10):1338–
1351.

Min-Ling Zhang and Zhi-Hua Zhou. 2007. ML-KNN:
A lazy learning approach to multi-label learning.
Pattern recognition, 40(7):2038–2048.

Min-Ling Zhang and Zhi-Hua Zhou. 2014. A re-
view on multi-label learning algorithms. IEEE
transactions on knowledge and data engineering,
26(8):1819–1837.

Guangxiang Zhao, Jingjing Xu, Qi Zeng, and Xu-
ancheng Ren. 2018. Review-driven multi-label mu-
sic style classification by exploiting style correla-
tions. arXiv preprint arXiv:1808.07604.


