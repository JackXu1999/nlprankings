










































Prefix Probabilities for Linear Context-Free Rewriting Systems


Proceedings of the 12th International Conference on Parsing Technologies, pages 151–162,
October 5-7, 2011, Dublin City University. c© 2011 Association for Computational Linguistics

Prefix Probabilities for
Linear Context-Free Rewriting Systems

Mark-Jan Nederhof
School of Computer Science

University of St Andrews
North Haugh, St Andrews, Fife

KY16 9SX
United Kingdom

Giorgio Satta
Dept. of Information Engineering

University of Padua
via Gradenigo, 6/A

I-35131 Padova
Italy

Abstract

We present a novel method for the com-
putation of prefix probabilities for linear
context-free rewriting systems. Our ap-
proach streamlines previous procedures to
compute prefix probabilities for context-free
grammars, synchronous context-free gram-
mars and tree adjoining grammars. In addi-
tion, the methodology is general enough to
be used for a wider range of problems in-
volving, for example, several prefixes.

1 Introduction

There are a number of problems related to proba-
bilistic grammatical formalisms that involve sum-
ming an infinite number of values. For example, if
P is a probability distribution over strings defined
by a probabilistic grammar, and w is a string, then
the prefix probability of w is defined to be:∑

v

P (wv)

In words, all possible suffixes v that may follow
prefix w are considered, and the probabilities of
the concatenations of v and w are summed.

Prefix probabilities can be exploited to predict
the next word or part of speech, for incremental
processing of text or speech from left to right (Je-
linek and Lafferty, 1991). They can also be used
in speech processing to score partial hypotheses in
beam search (Corazza et al., 1991).

At first sight, it is not clear that prefix proba-
bilities can be effectively computed, as the num-
ber of possible strings v is infinite. It was shown
however by Jelinek and Lafferty (1991) that in the
case of probabilistic context-free grammars, the
infinite sums can be isolated from any particular
w, and these sums can be computed off-line by
solving linear systems of equations. For any par-
ticular w, the prefix probability can then be com-
puted in cubic time in the length of w, on the

basis of the values computed off-line. Whereas
Jelinek and Lafferty (1991) consider parsing in
the style of the Cocke-Kasami-Younger algorithm
(Younger, 1967; Harrison, 1978), prefix probabil-
ities for probabilistic context-free grammars can
also be computed in the style of the algorithm by
Earley (1970), as shown by Stolcke (1995).

This approach is not restricted to context-free
grammars. It was shown by Nederhof et al. (1998)
that prefix probabilities can also be effectively
computed for probabilistic tree adjoining gram-
mars. That effective computation is also possible
for probabilistic synchronous context-free gram-
mars was shown by Nederhof and Satta (2011b),
which departed from earlier papers on the subject
in that the solution was divided into a number of
steps, namely a new type of transformation of the
grammar, followed by elimination of epsilon and
unit rules, and the computation of the inside prob-
ability of a string.

In this paper we focus on a much more general
formalism than those mentioned above, namely
that of probabilistic linear context-free rewriting
systems (PLCFRS). This formalism is equivalent
to the probabilistic simple RCGs discussed by
Maier and Søgaard (2008) and by Kallmeyer and
Maier (2010), and probabilistic extensions of mul-
tiple context-free grammars, such as those con-
sidered by Kato et al. (2006). Nonterminals in a
PLCFRS can generate discontinuous constituents.
For this reason, (P)LCFRSs have recently been
used to model discontinuous phrase structure tree-
banks as well as non-projective dependency tree-
banks; see (Maier and Lichte, 2009; Kuhlmann
and Satta, 2009; Kallmeyer and Maier, 2010).

The main contribution of this paper is a method
for computing prefix probabilities for PLCFRSs.
We are not aware of any existing algorithm in the
literature for this task. Our method implies ex-
istence of algorithms for the computation of pre-
fix probabilities for probabilistic versions of for-

151



malisms that are special cases of LCFRSs, such as
the generalized multitext grammars of Melamed
et al. (2004), which are used to model transla-
tion, and the already mentioned formalism pro-
posed by Kuhlmann and Satta (2009) to model
non-projective dependency structures.

We follow essentially the same approach as
Nederhof and Satta (2011b), and reduce the prob-
lem of computing the prefix probabilities for
PLCFRSs to the well-known problem of comput-
ing inside probabilities for PLCFRSs. The reduc-
tion is obtained by the composition of a PLCFRS
with a special finite-state transducer. Most impor-
tantly, this composition is independent of the spe-
cific input string for which we need to solve the
prefix probability problem, and can therefore be
computed off-line. We also show how off-line ap-
plication of a generic form of epsilon and unit rule
elimination for PLCFRS can be exploited, which
simplifies the computation of the inside probabil-
ity.

The rest of this paper is organized as follows.
In Section 2 we introduce PLCFRS and finite-state
transducers. In Section 3 we discuss a general al-
gorithm for composing a PLCFRS with a finite-
state transducer. This construction will be used in
several places in later sections. Section 4 shows an
effective way of computing the inside probabilities
for PLCFRSs, and Section 5 presents a method
for the elimination of epsilon and unit rules in a
PLCFRS. Section 6 combines all of the previous
techniques, resulting in an algorithm for the com-
putation of prefix probabilities via a reduction to
the computation of inside probabilities. We then
conclude in Section 7 with some discussion.

2 Definitions

This section summarizes the terminology and no-
tation of linear context-free rewriting systems, and
their probabilistic extension. For more detailed
definitions on linear context-free writing systems,
see Vijay-Shanker et al. (1987).

For an integer n ≥ 1, we write [n] to de-
note the set {1, . . . , n} and [0] = ∅. We write
[n]0 to denote [n] ∪ {0}. A linear context-free
rewriting system (LCFRS for short) is a tuple
G = (N,Σ,P, S), where N and Σ are finite, dis-
joint sets of nonterminal and terminal symbols, re-
spectively. EachA ∈ N is associated with an inte-
ger value φ(A) ≥ 1, called its fan-out. The non-
terminal S is the start symbol, with φ(S) = 1.

Finally, P is a set of rules, each of the form:

π : A→ g(A1, A2, . . . , Ar)

where A,A1, . . . , Ar ∈ N , and:

g : (Σ∗)φ(A1) × · · · × (Σ∗)φ(Ar) → (Σ∗)φ(A)

is a linear, non-erasing function. In other words, g
takes r tuples of strings as input, the j-th tuple be-
ing of size φ(Aj), and provides as output a tuple of
strings of size φ(A). Each of the output strings is
the concatenation of a sequence of elements, each
element being an input string or a terminal sym-
bol. Each input string is used precisely once in
such a sequence. The number r is called the rank
of the rule, and is denoted by ρ(π) or ρ(g).

The symbol π is the label of the rule, and each
rule is uniquely identified by its label. For tech-
nical reasons, we allow the existence of multi-
ple rules that are identical apart from their labels.
Again for technical reasons, we assume that each
function g is uniquely identified with one rule π.

The rank of LCFRS G, written ρ(G), is the
maximum rank among all rules of G. The fan-
out of LCFRS G, written φ(G), is the maximum
fan-out among all nonterminals of G.

Let a rule π be:

π : A→ g(A1, A2, . . . , Ar), where
g( 〈x1,1, . . . , x1,φ(A1)〉,

. . . ,
〈xr,1, . . . , xr,φ(Ar)〉 ) =

〈 y1,1 · · · y1,m1 ,
. . . ,
yφ(A),1 · · · yφ(A),mφ(A) 〉

where for each k ∈ [φ(A)] and l ∈ [mk], yk,l is
from the set Σ ∪ {xi,j | i ∈ [r], j ∈ [φ(Ai)]}.
We say π is monotone if for each i ∈ [r], for each
j1, j2 ∈ [φ(Ai)] such that j1 < j2, and for each
k1, k2 ∈ [φ(A)], l1 ∈ [mk1 ] and l2 ∈ [mk2 ] such
that yk1,l1 = xi,j1 and yk2,l2 = xi,j2 , we have that
either k1 < k2, or k1 = k2 and l1 < l2. In other
words, the order of variables associated with each
of the right-hand side nonterminals is preserved in
the output of function g. In this paper we will
assume all rules in a LCFRS are monotone. Re-
striction to monotone rules preserves the genera-
tive power of LCFRS (Michaelis, 2001; Kracht,
2003; Kallmeyer, 2010). Furthermore, known
techniques to extract (probabilistic) LCFRSs from
treebanks all provide grammars with monotone

152



rules (Maier and Lichte, 2009; Kuhlmann and
Satta, 2009).

For each nonterminal A in a LCFRS G, there
is a set of derivations for A, denoted DG(A), or
D(A) when G is understood. The simultaneous
definition ofDG(A) for allA is inductively as fol-
lows. Let:

π : A→ g(A1, A2, . . . , Ar)

be a rule of G, and let ζ1, . . . , ζr be derivations for
A1, . . . , Ar. Then the expression g(ζ1, . . . , ζr) is
a derivation for A, with an uninterpreted function
symbol g. When r = 0, we write the derivation as
g().

The yield of a derivation ζ for nonterminal A
in grammar G is the φ(A)-tuple of strings result-
ing from evaluating all occurrences in ζ of func-
tion symbols. It will be denoted as yieldG(ζ),
with once more G omitted when the grammar is
understood. The language generated by G, de-
noted L(G), is the set of all strings w such that
〈w〉 is the yield of a derivation for S; formally,
L(G) = {w | 〈w〉 ∈ yieldG(ζ), ζ ∈ DG(S)}.

Example 1 Consider the LCFRS G defined by
the rules:

π1 : S → g1(A), where
g1(〈x1,1, x1,2〉) = 〈x1,1x1,2〉

π2 : A→ g2(A), where
g2(〈x1,1, x1,2〉) = 〈ax1,1b, cx1,2d〉

π3 : A→ g3(), where
g3() = 〈ε, ε〉

We have φ(S) = 1, φ(A) = φ(G) = 2, ρ(π3) = 0
and ρ(π1) = ρ(π2) = ρ(G) = 1. G generates the
language {anbncndn | n ∈ N}. For instance,
the string a3b3c3d3 is obtained through the yield
〈a3b3c3d3〉 of the derivation g1(g2(g2(g2(g3()))))
for S. �

Let π : A → g(A1, A2, . . . , Ar) be a rule in
G and let dπ be the number of occurrences of ter-
minal symbols in the definition of the associated
function g. We define the size of π as:

|π| = dπ + φ(A) +
∑
i∈[r]

φ(Ai)

It is not difficult to see that we can encode rule π
and its associated function g using space linear in
|π|. We define the size of G as |G| =

∑
π∈P |π|,

where P is the set of all rules in G.

A LCFRS is said to be reduced if the function
g from each rule occurs in some derivation from
D(S). Because each function uniquely identifies
a rule, this means that also all rules are useful for
obtaining some derivation for S. A procedure for
transforming a LCFRS to make it reduced will be
discussed in Section 3.

A probabilistic LCFRS (PLCFRS for short) is
a pair G = (G, p) where G = (N,Σ,P, S) is a
LCFRS and p is a function from P to real numbers
in [0, 1]. We say that G is proper if for each A:∑

π:A→g(A1,A2,...,Aρ(g))

p(π) = 1

The probability of a derivation ζ in G, denoted
LG(ζ), is obtained by multiplying the probability
of the rule corresponding to each occurrence of a
function symbol in ζ. The probability of a string
w, denoted LG(w), is the sum of the probabilities
of all derivations in D(S) of which the yield is
〈w〉. A PLCFRS G is consistent if

∑
w LG(w) =

1. It is not difficult to see that if a PLCFRS G is
reduced, proper and consistent, then for each A,∑

ζ∈D(A) LG(ζ) = 1.

Example 2 Consider the extension of the LCFRS
from the previous example with function p defined
by p(π1) = 1, p(π2) = 0.3 and p(π3) = 0.7.
The resulting PLCFRS G is proper, as the sum of
probabilities of all rules with left-hand side S is
1, and so is the sum of probabilities of all rules
with left-hand side A. The probability of deriva-
tion g1(g2(g2(g2(g3())))) is 1 · (0.3)3 ·0.7. This is
also the probability of the string a3b3c3d3. It can
be easily shown that G is consistent, as:

∞∑
j=0

1 · (0.3)j · 0.7 = 1

�

In the following sections, we need to consider
PLCFRSs G that are not necessarily proper or con-
sistent, and our discussion will involve computa-
tion of values LG(A) for each nonterminal A, de-
fined by:

LG(A) =
∑

ζ∈D(A)

LG(ζ)

In the literature, LG is also called the partition
function of the grammar. We can relate these val-
ues by means of the following equations, one for

153



each nonterminal A:

LG(A) =
∑

π:A→g(A1,...,Aρ(g))

p(π) ·
∏

j∈[ρ(g)]

LG(Aj)

The above relations specify a system of poly-
nomial, nonlinear equations. The smallest non-
negative solution to this system gives us the values
of LG(A) for all A.

The sought solutions for the nonlinear sys-
tem described above can be irrational and non-
expressible by radicals, even if we assume that
all the rules of our LCFRS have probabilities that
are rational numbers, as observed by Etessami and
Yannakakis (2009). Hence values LG(A) can only
be approximated.

Approximate solutions can be obtained using
the fixed-point iteration method, as described for
example in (Abney et al., 1999). This method is
easy to implement but shows very slow conver-
gence in the worst case, resulting in exponential
time behaviour (Etessami and Yannakakis, 2009).

Alternatively, the more powerful Newton’s
method can be exploited; see again (Etessami and
Yannakakis, 2009). It has been shown by Kiefer et
al. (2007) that, after a certain number of initial it-
erations, each new iteration of Newton’s method
adds a fixed number of bits to the precision of
the approximate solution, resulting in polynomial
time convergence in the size of the grammar and
the number of bits in the desired approximation.
However, Kiefer et al. (2007) also show that, in
some degenerate cases, the number of iterations
needed to compute the first bit of the solution can
be at least exponential in the size of the system.
For further discussion of practical issues in the
computation of values LG(A), we refer the reader
to Wojtczak and Etessami (2007) and Nederhof
and Satta (2008).

Despite the computational limitations discussed
above, there are cases in which values LG(A) can
be computed exactly, and simpler methods can be
exploited. This happens when there are no cyclic
dependencies in the probabilistic grammar. This is
the situation we will deal with in Section 4.

In this paper, we will consider a type of finite-
state transducer (FST) that has a single final state
and that consumes exactly one input symbol in
each transition. Although such restricted FSTs
cannot describe all rational transductions (Berstel,
1979), they are sufficient for our purposes, and the
restrictions greatly simplify the definitions in the
following sections.

Hence, our type of FST can be defined by a tu-
ple M = (Σ1, Σ2, Q, qs, qf , T ), where Σ1 and
Σ2 are finite sets of input and output symbols, re-
spectively, Q is a finite set of states, of which qs
is the start state and qf is the final state, and T is
a finite set of transitions.

Each transition has the form s
a,u7→ s′, where

s, s′ ∈ Q, a ∈ Σ1 and u ∈ Σ2 ∪ {�}. This means
that we can jump from s to s′ by reading a from
the input, and generating u as output.

The transduction generated by FST M , denoted
L(M), is the set of all pairs 〈w, v〉 such that there
is a sequence of n transitions s0

a1,u17→ s1, . . . ,
sn−1

an,un7→ sn, such that s0 = qs, sn = qf ,
a1 · · · an = w, and u1 · · ·un = v. The defini-
tion allows n = 0, in which case qf must be qs
and the transduction includes 〈ε, ε〉. We say a FST
M is unambiguous if each pair 〈w, v〉 is obtained
by at most one sequence of transitions as above.

3 Composition

Seki et al. (1991, Thm 3.9(3), pg. 203) have shown
that the class of languages generated by LCFRSs
are closed under intersection with regular lan-
guages.1 The proof is a generalization of the proof
that context-free languages are closed under inter-
section with regular languages (Bar-Hillel et al.,
1964). We slightly extend this result by show-
ing that if we point-wise map the language gen-
erated by a LCFRS G by means of a FST M ,
then the resulting language is generated by an-
other LCFRS G′, or formally, L(G′) = {v | w ∈
L(G)∧ 〈w, v〉 ∈ L(M)}. As we will show below,
LCFRS G′ can be effectively constructed from G
andM . The construction is denoted by operator ◦,
hence G′ = G ◦M . The construction can be ex-
tended to take as input a PLCFRS G and a FST M
and the output is then another PLCFRS G′, and we
denote G′ = G ◦M . If the FST is unambiguous,
then the probabilities are preserved, or formally:

LG′(v) =
∑

〈w,v〉∈L(M)

LG(w)

This follows directly from the fact that rule proba-
bilities from G are copied unchanged to G′.

Without loss of generality, we will assume that
G = (N,Σ,P, S) and M = (Σ1, Σ2, Q, qs,
qf , T ), with Σ = Σ1. The construction produces
G′ = (N ′, Σ2, P

′, S′).
1The result by Seki et al. (1991) is obtained for a syntactic

variant of LCFRSs called multiple context-free grammars.

154



The nonterminals in N ′ are of the form
A(〈s1, s′1〉, . . . , 〈sφ(A), s′φ(A)〉), whereA ∈ N and
s1, s

′
1, . . . , sφ(A), s

′
φ(A) ∈ Q. The intuition behind

this definition is discussed in what follows. From
the derivations for A in G, we take the subset of
those that have yield 〈w1, ..., wφ(A)〉 such that for
each j ∈ [φ(A)] it must be possible to take the au-
tomaton from state sj to state s′j while consuming
input wj . In addition, input symbols are replaced
by the corresponding output strings, according to
the relevant transitions of the automaton. The start
symbol S′ is naturally S(〈qs, qf 〉), as the composi-
tion ultimately needs to match strings that are gen-
erated byG against those input strings that take the
automaton from the start state to the final state.

The rules of G′ are constructed by exhaustively
applying the following procedure. Choose a rule
from G of the form:

π : A→ g(A1, A2, . . . , Ar), where
g( 〈x1,1, . . . , x1,φ(A1)〉,

. . . ,
〈xr,1, . . . , xr,φ(Ar)〉 ) =

〈 y1,1 · · · y1,m1 ,
. . . ,
yφ(A),1 · · · yφ(A),mφ(A) 〉

where for each k ∈ [φ(A)] and l ∈ [mk], yk,l is
from the set Σ ∪ {xi,j | i ∈ [r], j ∈ [φ(Ai)]}.
Further choose two states si,j , s′i,j from M for
each i ∈ [r] and each j ∈ [φ(Ai)], and choose two
states qk,l, q′k,l from M for each k ∈ [φ(A)] and
each l ∈ [mk], under the following constraints,
which if satisfied define zk,l for k ∈ [φ(A)] and
l ∈ [mk]:

• if yk,l = xi,j , then qk,l must be si,j and q′k,l
must be s′i,j , and we let zk,l = xi,j ,

• if yk,l = a ∈ Σ then we must be able to
choose a transition qk,l

a,u7→ q′k,l, and we let
zk,l = u,

• for each k ∈ [φ(A)] and each l ∈ [mk − 1],
q′k,l = qk,l+1

In words, all variables coming from right-hand
side nonterminals are associated with two states,
whose intended meaning was explained before. In
addition, each terminal occurrence in the output of
the function g must correspond to a transition be-
tween two states. Lastly, an output component of
the form yk,1 · · · yk,mk must match a contiguous

path through the automaton, with each yk,l repre-
senting a segment of that path from state qk,l to
state q′k,l.

This determines a rule in G′, which is of the
form:

π′ : A′ → g′(A′1, A′2, . . . , A′r), where
g′( 〈x1,1, . . . , x1,φ(A1)〉,

. . . ,
〈xr,1, . . . , xr,φ(Ar)〉 ) =
〈 z1,1 · · · z1,m1 ,
. . . ,
zφ(A),1 · · · zφ(A),mφ(A) 〉

Here A′ is:

A(〈q1,1, q′1,m1〉, . . . , 〈qφ(A),1, q
′
φ(A),mφ(A)

〉)

and for each i ∈ [r], A′i is:

Ai(〈si,1, s′i,1〉, . . . , 〈si,φ(Ai), s
′
i,φ(Ai)

〉)

A new label π′ and new function name g′ are pro-
duced for each rule in G′ that is constructed as
above.

If we are dealing with probabilities, then the
rules in G′ = G ◦M are constructed in the same
manner, and in addition, the probability of each
rule π is copied to each rule π′ constructed from
it.

Example 3 Assume the following is a rule in G:

π : A→ g(A1, A2), where
g(〈x1,1, x1,2〉, 〈x2,1, x2,2, x2,3〉) =

〈x1,1ax2,1, x2,2x1,2, x2,3〉

Further assume the existence of states s1, . . . , s9
in M and the existence of a transition s2

a,b7→ s3 for
M . Then we add to G′ the rule:

π′ : A(〈s1, s4〉, 〈s5, s7〉, 〈s8, s9〉)→ g′(
A1(〈s1, s2〉, 〈s6, s7〉),
A2(〈s3, s4〉, 〈s5, s6〉, 〈s8, s9〉)), where

g′(〈x1,1, x1,2〉, 〈x2,1, x2,2, x2,3〉) =
〈x1,1bx2,1, x2,2x1,2, x2,3〉

�
A LCFRS G′ = G ◦ M (or PLCFRS G′ =
G◦M ) is generally not reduced. We can make it re-
duced by a process almost identical to the process
of reduction for context-free grammars (Sippu and

155



Soisalon-Soininen, 1988). This consists of three
phases. First, there is a bottom-up phase to iden-
tify the ‘generating’ nonterminals, that is, those
that have derivations. Second, restricting atten-
tion to the generating nonterminals, a top-down
phase identifies the possibly smaller set of nonter-
minals that are also reachable from the start sym-
bol. Lastly, all rules are removed except those that
are generating and reachable.

Computation Let us first relate |G′| to |G|,
where G′ = G ◦ M . To simplify the discus-
sion, consider a rule of G of the form π : A →
g(A1, A2, . . . , Ar) without terminal symbols, or
in other words dπ = 0. Let Q be the set of states
of M . Then the composition construction defines
a new rule inG′ for each possible choice of a num-
ber of states in Q equal to φ(A) +

∑
i∈[r] φ(Ai).

This results in |Q||π| new rules in G′ that are de-
rived from π, where each new rule has sizeO(|π|).
Thus the target grammar has size exponential in
|G|.

Our algorithm for composition can be easily im-
plemented to run in time O(|G′|), that is, in linear
time in the size of the output grammar. Because of
the above discussion, the algorithm runs in expo-
nential time in the size of the input. Exponential
time for the composition construction is not un-
expected: the problem at hand is a generalization
of the parsing problem for LCFRS, and the latter
problem is known to be NP-hard when the gram-
mar is part of the input (Satta, 1992).

The critical term in the above analysis is |π|.
If we can cast our LCFRS in a form in which
each rule has length bounded by some constant,
then composition can be carried out in polynomial
time. The process of reducing the length of rules
in a LCFRS is called factorization. It is known
that not all LCFRSs can be factorized in such a
way that each rule has length bounded by some
constant (Rambow and Satta, 1999). However,
in the context of natural language parsing, it has
been observed that the vast majority of rules in real
world applications can be factorized to some small
length, and that excluding the worst-case rules
which cannot be handled in this way does not sig-
nificantly affect accuracy; see for instance (Huang
et al., 2009) and (Kuhlmann and Satta, 2009) for
discussion. Efficient algorithms for factorization
of LCFRSs have been presented by Kuhlmann and
Satta (2009), Gómez-Rodrı́guez and Satta (2009)
and Sagot and Satta (2010).

Finally, the procedure for reducing a LCFRS
that we outlined in this section can be easily im-
plemented to run in time linear in the size of the
source grammar.

4 Effective LCFRS Parsing

String parsing with LCFRS G and input w =
a1 · · · an ∈ Σ∗ can be described in terms of com-
position as follows. We construct a FST Mw with
states s0, . . . , sn, of which s0 is the start state and
sn is the final state, and transitions si−1

ai,ai7→ si for
each i ∈ [n]. In words, the FST describes a map-
ping from only one string w to itself. The compo-
sition G′ = G ◦Mw restricts the derivations from
G to only those that have 〈w〉 as yield.

This approach is consistent with the observation
by Lang (1994) that parsing is a form of intersec-
tion. In the case of very ambiguous grammars, the
approach has the advantage that the set of all parse
trees is represented in a compact manner by the in-
tersection grammar, or G′ = G ◦Mw in our case.

Because Mw is unambiguous, the composition
also has favourable properties for PLCFRS G. In
particular, the probability LG(w) of a string w can
be determined by summing the probabilities of all
derivations of PLCFRS G′ = G ◦Mw, or formally:

LG(w) =
∑

v LG′(v) = LG′(S(〈s0, sn〉))

We have thus reduced the problem of computing
the inside probability of w under G to the problem
of computing the values of the partition function
for G′. Because LG(w) can be less than 1, it is
clear that G′ need not be consistent, even if we as-
sume that G is.

As we have discussed in Section 2, if G′ is any
PLCFRS that may not be proper or consistent, then
the valuesLG′(A) for the different nonterminals of
G′ can be expressed in terms of a system of equa-
tions. Solving such equations can be computation-
ally expensive.

A more efficient way to compute the values
LG′(A) is possible however if there are no cyclic
dependencies in G′, that is, in the system of equa-
tions specified in Section 2, no value is defined in
terms of itself. This can be ensured if the functions
g of the rules of the grammar are such that the mul-
tiset of non-empty strings in the output are never
the same as the multiset of non-empty strings in
the input arguments. This in turn is ensured if
the grammar contains no epsilon rules and no unit
rules.

156



Analogously to the theory of context-free gram-
mars, we define an epsilon rule to be of the form:

π : A→ g(), where g() = 〈ε, . . . , ε〉

We define a unit rule to be of the form:

π : A→ g(A1), where
g(〈x1, . . . , xφ(A1)〉) = 〈x1, . . . , xφ(A1)〉

Note that we assume that φ(A1) = φ(A). The
reason why we do not need to consider a reorder-
ing of variables is because we have assumed that
all LCFRSs are monotone, as mentioned in Sec-
tion 2. In Section 5 we will show that a LCFRS
can be transformed to eliminate all epsilon rules
and unit rules, while preserving the language as
well as the assignment of probabilities to strings.
In the remainder of the present section, we discuss
the computation of the values LG′(A) under the
assumption that we do not have to deal with cyclic
dependencies.

We define a binary relation ≺ over sequences
of strings by letting 〈w1, . . . , wk〉 ≺ 〈v1, . . . , vm〉
if and only if |w1 · · ·wk| < |v1 · · · vm| or
|w1 · · ·wk| = |v1 · · · vm| and m < k. In
words, a sequence is smaller than another if it con-
tains fewer occurrences of symbols, and when the
two sequences contain the same number of oc-
currences of symbols, then the first is smaller if
the symbol occurrences are distributed over more
strings.

Let w be any string. If LCFRS G does not
contain epsilon rules or unit rules, then LCFRS
G′ = G ◦ Mw has the following property. If
ζ = g′(ζ1, . . . , ζr) is a derivation for some non-
terminal A′ in G′, then yield(ζi) ≺ yield(ζ)
for every i ∈ [r]. Note that yield(ζ) =
g′(yield(ζ1), . . . , yield(ζr)).

If we extend this to the probabilistic case, with
G′ = G ◦ Mw, G = (G, p) and G′ = (G′, p′),
then LG′(ζ) = p′(π′) ·

∏
i LG′(ζi), where π

′ is the
label of the rule in which g′ occurs. If we use the
fact that multiplication distributes over addition,
we derive:

LG′(A
′) =

∑
π′:A′→g′(A′1,...,A′ρ(g′))

p′(π′) ·
∏

i∈[ρ(g′)]

LG′(A
′
i)

Recall that Mw has the set of states {s0, . . . , sn},
where n = |w|. Using the notation in Section 3,
A′ is therefore of the form:

A(〈sb1,1 , sb′1,m1 〉, . . . , 〈sbφ(A),1 , sb′φ(A),mφ(A)
〉),

where for each k ∈ [φ(A)] and l ∈ [mk], bk,l and
b′k,l are integers in [n]0. Furthermore, for each i ∈
[ρ(g′)], A′i is of the form:

Ai(〈sci,1 , sc′i,1〉, . . . , 〈sci,φ(Ai) , sc′i,φ(Ai)〉),

where for each j ∈ [φ(Ai)], ci,j and c′i,j are inte-
gers in [n]0.

By associating each pair 〈sbk,l , sb′k,l〉 (or
〈sci,j , sc′i,j 〉) with a corresponding substring
abk,l+1 · · · ab′k,l (or aci,j+1 · · · ac′i,j , respectively)
ofw, we obtain sequences of strings forA′i that are
smaller than those for A′, by relation ≺. Because
≺ is acyclic, this means we can compute LG′(A′i)
strictly before computing LG′(A′). All values of
LG′ for different nonterminals can be obtained by
enumerating all sequences of substrings from w in
an order that is consistent with ≺. We refer to this
procedure as the inside algorithm for PLCFRSs.

Computation It is not difficult to implement the
inside algorithm in such a way that all values
LG′(A) can be computed in time linear in the size
of G′ = G ◦ Mw. This is essentially a gener-
alization of the well-known inside algorithm for
probabilistic context-free grammars (Manning and
Schütze, 1999) to a special kind of non-recursive
PLCFRS.

5 Grammar Transformations

In this section we introduce grammar transfor-
mations that remove cyclic dependencies from
LCFRS, and discuss their application to a special
class of PLCFRSs.

We say a nonterminal A is nullable in gram-
mar G if there is a derivation ζ ∈ DG(A) with
yieldG(ζ) = 〈ε, . . . , ε〉. The set EG of nullable
nonterminals in G can be found as a straight-
forward generalization of the algorithm to find
nullable nonterminals in a context-free grammar
(Sippu and Soisalon-Soininen, 1988). Similarly,
we can construct the set of all nonterminals A for
which at least one derivation ζ ∈ DG(A) has a
yield containing at least one non-empty string. We
denote this set by EG.

A LCFRS G can now be transformed into a
LCFRS G′ without epsilon rules, by applying the

157



following exhaustively.2 Take a rule from G:

π : A→ g(A1, . . . , Ar), where
g( 〈x1,1, . . . , x1,φ(A1)〉,

. . . ,
〈xr,1, . . . , xr,φ(Ar)〉 ) =

〈α1, . . . , αφ(A)〉

and determine:

Uπ = {i | i ∈ [r], Ai ∈ EG}
Uπ = {i | i ∈ [r], Ai ∈ EG}

and then choose any set U with (Uπ \ Uπ) ⊆
U ⊆ Uπ. That is, the set must contain all indices
of right-hand side nonterminals that only generate
yields with empty strings, and it may additionally
contain the indices of other nullable nonterminal
occurrences. Now construct the rule:

πU : A→ gU (B1, . . . , Br′), where
gU ( 〈x′1,1, . . . , x′1,φ(B1)〉,

. . . ,
〈x′r′,1, . . . , x′r′,φ(Br′ )〉 ) =

〈α′1, . . . , α′φ(A)〉

where B1, . . . , Br′ is obtained from A1, . . . , Ar
by omittingAi if i ∈ U , where r′ = r−|U|. Simi-
larly, gU has only r′ arguments, by omitting its i-th
argument if i ∈ U . Lastly, for each k ∈ [φ(A)],
α′k is obtained from αk by omitting any variables
of the form xi,j where i ∈ U . The constructed rule
πU now becomes a rule in the transformed gram-
mar G′ if it is not an epsilon rule.

Example 4 Assume a rule in G of the form:

π : A→ g(A1, A2, A3), where
g(〈x1〉, 〈x2〉, 〈x3〉) = 〈x3ax1x2〉

Further assume that EG includes A1 and A2 but
not A3, and EG includes A1 and A3 but not A2.
Then G′ will contain:

π{2} : A→ g{2}(A1, A3), where
g{2}(〈x1〉, 〈x3〉) = 〈x3ax1〉

π{1,2} : A→ g{1,2}(A3), where
g{1,2}(〈x3〉) = 〈x3a〉

2Removal of epsilon rules is also investigated by Seki et
al. (1991, Lemma 2.2(N2), pg. 197) for multiple context-free
grammars, a syntactic variant of LCFRSs. Here we extend
the result to the probabilistic version of LCFRSs.

�
In the case of a PLCFRS G = (G, p) being

transformed to G′ = (G′, p′), we have:

p′(πU ) = p(π) ·
∏
i∈U

∑
ζ ∈ DG(Ai) :

yield(ζ) = 〈ε, . . . , ε〉

LG(ζ)

The summation of all derivations with yields con-
sisting only of empty strings is difficult in general,
and involves the solution of a system of polyno-
mial, nonlinear equations, a topic which we ad-
dressed in Section 2.

However, there is a special case that can be eas-
ily dealt with, namely that EG ∩ EG = ∅ and for
each A ∈ EG: ∑

ζ ∈ DG(A) :
yield(ζ) = 〈ε, . . . , ε〉

LG(ζ) = 1

This means that there is precisely one possible set
U for each rule π, and p′(πU ) will always be iden-
tical to p(π). In this case the overall construction
of elimination of nullable rules from PLCFRS G
can be implemented in time linear in |G|. It is this
special case that we will encounter in Section 6.

Now we turn to elimination of unit rules from
a PLCFRS G. We investigate sequences of appli-
cations of unit rules, multiplying the probabilities
of all rule occurrences, and adding the probabil-
ities of all such sequences between each pair of
nonterminals. Formally, for each pair A and B of
nonterminals, we have a value ∆G(A,B), and we
write:

∆G(A,B) = δ(A = B) +

+
∑

π : A→ g(A1), where
g(〈x1, . . . , xφ(A1)〉) =
〈x1, . . . , xφ(A1)〉

p(π) ·∆G(A1, B)

where δ(A = B) is defined to be 1 ifA = B and 0
otherwise. This forms a system of linear equations
in the unknown variables ∆G(·, ·). Such a system
can be solved in polynomial time in the number of
variables, for example using Gaussian elimination.

In order to construct the transformed grammar
G′, we exhaustively choose a rule from G:

π : A→ g(A1, A2, . . . , Ar)

158



and choose B such that ∆G(B,A) > 0, and let G′
contain the rule:

πB : B → gB(A1, A2, . . . , Ar)

where gB is defined to be identical to g. The prob-
ability of πB is p′(πB) = p(π) ·∆G(B,A).

The size of G′ is quadratic in G. The time com-
plexity is dominated by the computation of the so-
lution of the linear system of equations. This com-
putation takes cubic time in the number of vari-
ables. The number of variables in this case is
O(|G|2), which makes the running time O(|G|6).

6 Prefix Probabilities

In this section we gather all the constructions that
have been presented in the previous sections, and
provide the main result of this paper.

Let G be a reduced, proper and consistent
PLCFRS. We assume G does not contain any ep-
silon rules, and therefore the empty string is not in
the generated language.

The first step towards the computation of prefix
probabilities is the construction of a FST Mpref
that maps any string w over an assumed input al-
phabet to any non-empty prefix of w. In other
words, its transduction is the set of pairs 〈wv,w〉,
for any non-empty string w and any string v, both
over the alphabet Σ of G. The reason why we
do not consider empty prefixes w is because this
simplifies the definition of Mpref below, assuming
there can be only one final state. The restriction
is without loss of generality, as the computation of
the prefix probability of the empty string is easy:
it is always 1.

The transducer Mpref has two states, qs and qf ,
which are also the start and final states, respec-
tively. The transitions have the following forms,
for each a ∈ Σ:

qs
a,a7→ qs

qs
a,a7→ qf

qf
a,ε7→ qf

In words, symbols are copied unchanged from in-
put to output as long as the automaton is in the start
state. After it jumps to the final state, no more out-
put can be produced.

Note that if we consider any pair of states s and
s′, such that the automaton can reach s′ from s,
then precisely one of the following two cases is
possible:

• On any path through the automaton, the
empty string is produced in the output. This
applies when s = s′ = qf .

• On any path through the automaton, a non-
empty string is produced in the output. This
applies when s = s′ = qs, or s = qs and
s′ = qf .

Note that in the first case, for s = s′ = qf , any
string can be consumed in the input, in exactly
one way. This has an important implication for
the composition G′ = G ◦Mpref , namely that all
nullable nonterminals in G′ must be of the form
A′ = A(〈qf , qf 〉, . . . , 〈qf , qf 〉). By the construc-
tion of G′, and by the assumption that G is reduced,
proper and consistent we have:∑

ζ ∈ DG′ (A′) :
yield(ζ) = 〈ε, . . . , ε〉

LG′(ζ) =
∑

ζ∈DG(A)

LG(ζ) = 1

Therefore, we can apply a simplified procedure to
eliminate epsilon rules, maintaining the probabil-
ity of a rule where we eliminate nullable nontermi-
nals from its right-hand side, as explained in Sec-
tion 5.

After also unit rules have been eliminated, by
the procedure in Section 5, we obtain a gram-
mar G′′ without epsilon rules and without unit
rules. For a given prefix w we can now construct
G′′◦Mw, and apply the inside algorithm, by which
we obtain LG′′(w), which is the required prefix
probability of w by G.

Computation Consider the PLCFRS G′ = G ◦
Mpref ; assume G′ is subsequently reduced. Due
to the special topology of Mpref and to the as-
sumption that G is monotonic, it is not difficult
to see that all nonterminals of G′ have the form
A(〈s1, s2〉, . . . , 〈s2φ(A)−1, s2φ(A)〉) such that, for
some k ∈ [2φ(A)]0, we have si = qs for each
i ∈ [k] and si = qf for each i ∈ [2φ(A)] \ [k].
Related to this, for each rule π ofG, there are only
O(|π|) many rules in G′ (as opposed to a num-
ber exponential in |π|, as in the general case dis-
cussed in Section 3). Since each new rule in G′

constructed from π has size proportional to |π|, we
may conclude that |G′| = O(|G|2). Furthermore,
G′ can be computed in quadratic time.

The simplified procedure for eliminating ep-
silon rules and the procedure for eliminating unit
rules both take polynomial time, as discussed in

159



Section 5. This results in a PLCFRS G′′ such that
|G′′| is polynomially related to |G|, where G is
the source LCFRS. Note that G′′ can be computed
off-line, that is, independently of the specific in-
put string for which we need to compute the prefix
probability.

Finally, computation of G′′ ◦ Mw, for a given
prefix w, can take exponential time in |w|. How-
ever, the exponential time behaviour of our algo-
rithm seems to be unavoidable, since the problem
at hand is more general than the problem of pars-
ing of w under a LCFRS model, which is known
to be NP-hard (Satta, 1992). As already discussed
in Section 3, the problem can be solved in polyno-
mial time in case we can cast the source LCFRS
G in a normal form where each rule has length
bounded by some constant.

7 Discussion

Our findings subsume computation of the prefix
probability:

• for probabilistic context-free grammars in
time O(n3) (Jelinek and Lafferty, 1991),

• for probabilistic tree-adjoining grammars in
time O(n6) (Nederhof et al., 1998), and

• for probabilistic synchronous context-free
grammars (Nederhof and Satta, 2011b).

The latter becomes clear once we see that a syn-
chronous context-free grammar can be expressed
in terms of a restricted type of LCFRS. All non-
terminals of this LCFRS, except the start symbol,
have fan-out 2, and all generated strings are of the
form w$v, where w functions as input string, v
functions are output string, and $ is a separator be-
tween the two, which does not occur in the input
and output alphabets of the synchronous context-
free grammar. The rules of the LCFRS are of two
forms. The first form is a single rule with the start
symbol S† in the left-hand side:

π† : S† → g†(S), where
g†(〈x1,1, x1,2〉) = 〈x1,1$x1,2〉

The other rules all have a form that ensures that
variables associated with the input string are never
combined with variables associated with the out-
put string:

π : A→ g(A1, . . . , Ar), where
g(〈x1,1, x1,2〉, . . . 〈xr,1, xr,2〉) =

〈y1,1 · · · y1,m1 , y2,1 · · · y2,m2〉

and each y1,l, l ∈ [m1], is either a terminal (from
the input alphabet) or a variable of the form x1,j ,
and each y2,l, l ∈ [m2], is either a terminal (from
the output alphabet) or a variable of the form x2,j .

Let G be a PLCFRS mimicking a probabilistic
synchronous CFG as outlined above. We can de-
fine an unambiguous FST M of which the trans-
duction is the set of pairs 〈w1v1$w2v2, w1$w2〉 for
any strings w1, v1 over the input alphabet and any
strings w2, v2 over the output alphabet of the syn-
chronous CFG, where w1 and w2 are non-empty.
The FST has states s0, s1, s2, s3, of which s0 and
s3 are the start and the final state, respectively. The
transitions are of the form:

s0
a,a7→ s0 s0

a,a7→ s1
s1

a,ε7→ s1 s1
$,$7→ s2

s2
a,a7→ s2 s2

a,a7→ s3
s3

a,ε7→ s3

We can now proceed by constructing G ◦ M ,
and eliminating its epsilon and unit rules, to give
G′, much as in Section 6. Let w1 and w2 be two
strings over the input and output alphabets, respec-
tively. The prefix probability can now be effec-
tively computed by constructing G′ ◦Mw1$w2 and
computing the inside algorithm.

We can use a similar idea to compute prefix
probabilities for probabilistic extensions of the
generalized multitext grammars of Melamed et al.
(2004), a formalism used to model translation for
which no prefix probability algorithm was previ-
ously known.

A seemingly small variation of the problem
considered in this paper is to compute infix proba-
bilities (Corazza et al., 1991; Nederhof and Satta,
2008). It is straightforward to define a FST
M of which the transduction is the set of pairs
〈v1wv2, w〉. However, it does not seem possible
to construct an unambiguous FST that achieves
this. Therefore G ◦M does not have the required
probabilistic properties that would allow a cor-
rect computation by means of the inside algorithm.
The problem of infix probabilities for probabilis-
tic context-free grammars was also considered by
Nederhof and Satta (2011a).

References

S. Abney, D. McAllester, and F. Pereira. 1999.
Relating probabilistic grammars and automata.

160



In 37th Annual Meeting of the Association for
Computational Linguistics, Proceedings of the
Conference, pages 542–549, Maryland, USA,
June.

Y. Bar-Hillel, M. Perles, and E. Shamir. 1964.
On formal properties of simple phrase structure
grammars. In Y. Bar-Hillel, editor, Language
and Information: Selected Essays on their The-
ory and Application, chapter 9, pages 116–150.
Addison-Wesley, Reading, Massachusetts.

J. Berstel. 1979. Transductions and Context-Free
Languages. B.G. Teubner, Stuttgart.

A. Corazza, R. De Mori, R. Gretter, and G. Satta.
1991. Computation of probabilities for an
island-driven parser. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
13(9):936–950.

J. Earley. 1970. An efficient context-free pars-
ing algorithm. Communications of the ACM,
13(2):94–102, February.

K. Etessami and M. Yannakakis. 2009. Re-
cursive Markov chains, stochastic grammars,
and monotone systems of nonlinear equations.
Journal of the ACM, 56(1):1–66.

C. Gómez-Rodrı́guez and G. Satta. 2009. An
optimal-time binarization algorithm for linear
context-free rewriting systems with fan-out two.
In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 985–
993, Suntec, Singapore, August.

M.A. Harrison. 1978. Introduction to Formal
Language Theory. Addison-Wesley.

Liang Huang, Hao Zhang, Daniel Gildea, and
Kevin Knight. 2009. Binarization of syn-
chronous context-free grammars. Computa-
tional Linguistics, 35:559–595, December.

F. Jelinek and J.D. Lafferty. 1991. Computation
of the probability of initial substring generation
by stochastic context-free grammars. Computa-
tional Linguistics, 17(3):315–323.

L. Kallmeyer and W. Maier. 2010. Data-
driven parsing with probabilistic linear context-
free rewriting systems. In The 23rd Interna-

tional Conference on Computational Linguis-
tics, pages 537–545, Beijing, China, August.

Laura Kallmeyer. 2010. Parsing Beyond Context-
Free Grammars. Springer-Verlag.

Y. Kato, H. Seki, and T. Kasami. 2006.
RNA pseudoknotted structure prediction using
stochastic multiple context-free grammar. IPSJ
Digital Courier, 2:655–664.

S. Kiefer, M. Luttenberger, and J. Esparza. 2007.
On the convergence of Newton’s method for
monotone systems of polynomial equations. In
Proceedings of the 39th ACM Symposium on
Theory of Computing, pages 217–266.

M. Kracht. 2003. The Mathematics of Language.
Mouton de Gruyter, Berlin.

M. Kuhlmann and G. Satta. 2009. Treebank
grammar techniques for non-projective depen-
dency parsing. In Proceedings of the 12th Con-
ference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 478–
486, Athens, Greece.

B. Lang. 1994. Recognition can be harder
than parsing. Computational Intelligence,
10(4):486–494.

W. Maier and T. Lichte. 2009. Characteriz-
ing discontinuity in constituent treebanks. In
P. de Groote, M. Egg, and L. Kallmeyer, editors,
Proceedings of the 14th Conference on Formal
Grammar, volume 5591 of Lecture Notes in Ar-
tificial Intelligence, Bordeaux, France.

W. Maier and A. Søgaard. 2008. Treebanks and
mild context-sensitivity. In P. de Groote, editor,
Proceedings of the 13th Conference on Formal
Grammar, pages 61–76, Hamburg, Germany.
CSLI Publications.

C.D. Manning and H. Schütze. 1999. Founda-
tions of Statistical Natural Language Process-
ing. MIT Press.

I. Dan Melamed, Giorgio Satta, and Ben Welling-
ton. 2004. Generalized multitext grammars. In
Proceedings of ACL-04.

J. Michaelis. 2001. On Formal Properties of Min-
imalist Grammars. Ph.D. thesis, Potsdam Uni-
versity.

161



M.-J. Nederhof and G. Satta. 2008. Computing
partition functions of PCFGs. Research on Lan-
guage and Computation, 6(2):139–162.

M.-J. Nederhof and G. Satta. 2011a. Computation
of infix probabilities for probabilistic context-
free grammars. In Conference on Empirical
Methods in Natural Language Processing, Pro-
ceedings of the Conference, pages 1213–1221,
Edinburgh, Scotland, July.

M.-J. Nederhof and G. Satta. 2011b. Prefix proba-
bility for probabilistic synchronous context-free
grammars. In 49th Annual Meeting of the Asso-
ciation for Computational Linguistics, Proceed-
ings of the Conference, pages 460–469, Port-
land, Oregon, June.

M.-J. Nederhof, A. Sarkar, and G. Satta. 1998.
Prefix probabilities from stochastic tree adjoin-
ing grammars. In 36th Annual Meeting of
the Association for Computational Linguistics
and 17th International Conference on Compu-
tational Linguistics, volume 2, pages 953–959,
Montreal, Quebec, Canada, August.

O. Rambow and G. Satta. 1999. Independent par-
allelism in finite copying parallel rewriting sys-
tems. Theoretical Computer Science, 223:87–
120.

B. Sagot and G. Satta. 2010. Optimal rank reduc-
tion for linear context-free rewriting systems
with fan-out two. In Proceedings of the 48th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 525–533, Uppsala,
Sweden, July.

G. Satta. 1992. Recognition of linear context-
free rewriting systems. In 30th Annual Meeting
of the Association for Computational Linguis-
tics, Proceedings of the Conference, pages 89–
95, Newark, Delaware, USA, June–July.

H. Seki, T. Matsumura, M. Fujii, and T. Kasami.
1991. On multiple context-free grammars.
Theoretical Computer Science, 88:191–229.

S. Sippu and E. Soisalon-Soininen. 1988. Pars-
ing Theory, Vol. I: Languages and Parsing, vol-
ume 15 of EATCS Monographs on Theoretical
Computer Science. Springer-Verlag.

A. Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes

prefix probabilities. Computational Linguistics,
21(2):167–201.

K. Vijay-Shanker, D.J. Weir, and A.K. Joshi.
1987. Characterizing structural descriptions
produced by various grammatical formalisms.
In 25th Annual Meeting of the Association for
Computational Linguistics, Proceedings of the
Conference, pages 104–111, Stanford, Califor-
nia, USA, July.

D. Wojtczak and K. Etessami. 2007. PReMo: an
analyzer for Probabilistic Recursive Models. In
Tools and Algorithms for the Construction and
Analysis of Systems, 13th International Confer-
ence, volume 4424 of Lecture Notes in Com-
puter Science, pages 66–71, Braga, Portugal.
Springer-Verlag.

D.H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information
and Control, 10:189–208.

162


