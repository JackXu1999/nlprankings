



















































Automated Essay Scoring in the Presence of Biased Ratings


Proceedings of NAACL-HLT 2018, pages 229–237
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Automated Essay Scoring in the Presence of Biased Ratings

Evelin Amorim
Computer Science Department

UFMG
Brazil

evelin@dcc.ufmg.br

Marcia Cançado
Linguistics Department

UFMG
Brazil

mcancado@ufmg.br

Adriano Veloso
Computer Science Department

UFMG
Brazil

adrianov@dcc.ufmg.br

Abstract

Studies in Social Sciences have revealed that
when people evaluate someone else, their eval-
uations often reflect their biases. As a re-
sult, rater bias may introduce highly subjec-
tive factors that make their evaluations inaccu-
rate. This may affect automated essay scoring
models in many ways, as these models are typ-
ically designed to model (potentially biased)
essay raters. While there is sizeable literature
on rater effects in general settings, it remains
unknown how rater bias affects automated es-
say scoring. To this end, we present a new
annotated corpus containing essays and their
respective scores. Different from existing cor-
pora, our corpus also contains comments pro-
vided by the raters in order to ground their
scores. We present features to quantify rater
bias based on their comments, and we found
that rater bias plays an important role in auto-
mated essay scoring. We investigated the ex-
tent to which rater bias affects models based
on hand-crafted features. Finally, we propose
to rectify the training set by removing essays
associated with potentially biased scores while
learning the scoring model.

1 Introduction

Automated Essay Scoring (AES) aims at develop-
ing models that can grade essays automatically or
with reduced involvement of human raters (Page,
1967). AES systems may rely not only on gram-
mars, but also on more complex features such as
semantics, discourse and pragmatics (Davis and
Veloso, 2016; Song et al., 2014; Farra et al., 2015;
Somasundaran et al., 2014). Thus, a prominent
approach to AES is to learn scoring models from
previously graded samples, by modeling the scor-
ing process of human raters. When given the same
set of essays to evaluate and enough graded sam-
ples, AES systems tend to achieve high agreement
levels with trained human raters (Taghipour and

Ng, 2016).
While research in AES has focused on design-

ing scoring models that maximize the agreement
with human raters(Chen and He, 2013; Alikan-
iotis et al., 2016), there is a lack of discussion
on how biased are human ratings. Despite mak-
ing judgments on a common dimension, raters
may be influenced by their attitudes, their cul-
tural background, and their political and economic
views (Guerra et al., 2011). Since AES models
are designed to learn by analyzing human-graded
essays, AES models could inherit rating biases
present in the scores from human raters, and this
may result in systematic errors. Thus, our ob-
jective in this paper is to examine the extent to
which rater bias affects the effectiveness of state-
of-the-art AES models. A deeper understanding
of such factors may help mitigating the effects of
rater bias, enabling AES models to achieve greater
objectivity.

In order to study the effects of rater bias in essay
scoring, we created an annotated corpus contain-
ing essays written by high school students as part
of a standardized Brazilian national exam. Our
corpus contains a number of essays, written in Por-
tuguese, along with their respective scores. Fur-
ther, raters must also provide a comment for each
essay in order to ground their scores. As in (Re-
casens et al., 2013) we built subjectivity and sen-
timent lexicons that serve as features to represent
the comments, that is, rater comments are repre-
sented according to the subjectivity distribution as
given by specific subjectivity cues in our lexicons.
We present empirical evidence suggesting that the
subjectivity distribution within rater comment is
a proxy for the score that is given to the essay.
More specifically, very low (or very high) scores
are associated with essays for which rater com-
ments showed a very particular subjectivity distri-
bution. We also investigated the relationship be-

229



tween subjectivity distribution and the misalign-
ment between human raters and AES models. In-
terestingly, the subjectivity distribution becomes
very characteristic as the misalignment increases.

Our main contributions are three-fold:

• We built subjectivity lexicons for the Por-
tuguese language. These lexicons include
words and phrases associated with different
subjectivity dimensions − sentiments, factive
verbs, entailments, intensifiers and hedges.
We identify biased language within rater
comments by calculating the word mover’s
distance (Kusner et al., 2015) between com-
ments and the lexicons. This approach ben-
efits from large unsupervised corpora, that
can be used to learn effective word embed-
dings (Mikolov et al., 2013). By identify-
ing biased language, we observed that biases
can work to inflate essay scores or to deflate
them.

• We employ a set of linguistic features in or-
der to learn different AES models, and we
evaluate the effects of biased ratings in the
efficacy of these models. In summary, biased
ratings affect AES models in different ways,
but in general the misalignment between hu-
man rater and the AES model is more acute
when the rater shows biased language in their
comments.

• We propose simple ways of preventing and
reducing the negative effects of biased ratings
while learning AES models. Results in a con-
trolled experimental setting revealed that de-
tecting and removing biased ratings from the
training set lead to significant improvements
in automated essay scoring.

In the remainder of this paper, Section 2 dis-
cusses related work on automated essay scoring.
Section 3 describes the features used for learning
AES models, as well as the features used for iden-
tifying biased language in rater comments. Fur-
ther, our debiasing approach is also discussed in
Section 3. Section 4 describes the data, the setup
and the results of our empirical evaluation. Fi-
nally, Section 5 provides our conclusions.

2 Related Work

Research in cognitive science, psychology and
other social studies offer a great amount of work

on (conscious and unconscious) biases and their
effects on a variety of human activities (Kaheman
and Tversky, 1972; Tversky and Kaheman, 1974).
Biases can create situations that lead us to make
decisions that project our experiences and values
onto others (Baron, 2007; Ariely, 2008). While
there is sizeable literature on rater effects in gen-
eral settings (Myford and Wolfe, 2003), it remains
unknown how biased ratings affect automated es-
say scoring models. Rather, works on automated
essay scoring are mainly focused on designing
AES models by maximizing the agreement with
human raters, despite the assertiveness of the rat-
ings.

Typically, AES systems are built on the basis of
predefined linguistic features that are then given
to a machine learning algorithm (Amorim and
Veloso, 2017). Works that fall into this approach
include (Srihari et al., 2008, 2007; Cummins et al.,
2016; McNamaraa et al., 2015). Further, authors
in (Dong and Zhang, 2016) presented an empiri-
cal analysis of features typically used for learning
AES models. Authors in (Crossley et al., 2015)
studied a broader category of features that can also
be used to build AES models. There are also more
recent approaches for learning AES models that do
not assume a set of predefined features. These ap-
proaches are based on deep architectures, and in-
clude (Alikaniotis et al., 2016; Taghipour and Ng,
2016; Riordan et al., 2017; Dong et al., 2017). Fi-
nally, there also models based on domain adapta-
tion (Phandi et al., 2015) and unsupervised learn-
ing (Chen et al., 2010).

Few works have investigated the subjective na-
ture of essay scoring. An interesting excep-
tion is (Allen et al., 2015), in which the au-
thors investigated the misalignment between stu-
dents’ and teachers’ ratings of essay. Results re-
vealed that students who were less accurate in their
self-assessments produced essays that were more
causal, contained less meaningful words, and had
less argument overlap between sentences.

The work in this paper builds upon prior work
on building subjectivity lexicons (Klebanov et al.,
2012) and subjectivity detection (Recasens et al.,
2013), but in our case applied to score agree-
ment. In this respect, our work is more comparable
to (Klebanov and Beigman, 2009; Beigman and
Klebanov, 2009), where authors discussed and in-
vestigated the problem of learning in the presence
of biased annotators. Other works that are also

230



close to ours include (Farra et al., 2015; Somasun-
daran et al., 2016; Song et al., 2014), in which the
authors studied the problem of scoring persuasive
and argumentative essays.

3 Method

Our aim in this work is to learn AES models that
are less prone to the effects of biased ratings, that
is, models that are able to perform highly objec-
tive and impartial judgements. Thus, we start this
section by proposing features that are useful for
building AES models. Then, we propose another
set of features that are useful for identifying bi-
ased ratings based on subjectivity cues. Finally,
we propose an approach to remove biased ratings
from the training set, thus learning more objective
AES models.

3.1 Features for Essay Scoring

As most existing AES systems, our models are
built on the basis of predefined features (e.g. num-
ber of words, average word length, and number of
spelling errors) that are given to a machine learn-
ing algorithm. The features used to build our AES
models are discussed and evaluated in (Amorim
and Veloso, 2017). They may fall into two broad
categories:

Domain features: These are simple linguistic
features, including the number of first-person
pronouns, demonstrative pronouns and verbs.
Features also include the number of pronouns
and verbs normalized by the number of to-
kens in the corresponding sentence.

General features: Most of the general features
are based on (Attali and Burstein, 2006).
However, due to lack of tools for processing
the Portuguese language, we implemented
the following features, which are sub-divided
as follows:

Grammar and style: Features include the
number of grammar errors and mis-
spellings. These numbers are also nor-
malized by the number of tokens in
the corresponding sentence. In or-
der to evaluate style, we designed fea-
tures based on the style rules suggested
in (Martins, 2000). Features include the
number of style errors and the number
of style of errors per sentence.

Organization and development: Features
include the number of discourse mark-
ers from the Portuguese grammar, and
the number of discourse markers per
sentence. Discourse markers are lin-
guistic units that establish connections
between sentences to build coherent and
knit discourse.

Lexical complexity: Features include
the Portuguese version for the Flesh
score (Martins et al., 1996), the average
word length (i.e., the number of sylla-
bles), the number of tokens in an essay,
and the number of different words in an
essay.

Prompt-specific vocabulary usage:
Features include different distances
between prompt and essay (i.e., cosine
distance). In this case, both the prompt
and the essay are treated as frequency
vectors of words.

3.2 Features for Identifying Biased Ratings
We assume a scenario in which essay raters must
ground the provided scores with specific com-
ments. We also assume that we can identify bi-
ased ratings by detecting comments with biased
language. In order to detect biased language, we
developed subjectivity lexicons for the Portuguese
language. Specifically, a linguist built a list of
Portuguese lexicons based on the analysis of ex-
pressions that seem to express some subjectivity
of the human evaluator. Our subjectivity lexicons
are categorized into the following groups:

Argumentation: This lexicon includes markers
of argumentative discourse. Argumentative
markers include lexical expressions and con-
nectives, such as: “even” (até), “by the
way” (aliás), “as a consequence” (como con-
sequência), “or else” (ou então), “as if”
(como se), “rather than” (em vez de), “some-
how” (de certa forma), “despite” (apesar de),
among others.

Presupposition: This lexicon includes markers
that suggest the rater assumes something is
true. Some examples of such markers in-
clude: “nowadays” (hoje em dia), “to keep
on doing” (continuar a), and factive verbs.

Modalization: This lexicon indicates that the
writer exhibits a stance towards its own state-

231



ment. Some examples of such markers are
adverbs, auxiliary verbs, modality clauses,
and some type of verbs.

Sentiment: This lexicon also includes markers
that indicate a state of mind or a sentiment
of the rater while evaluating the essay. Some
examples of such markers include: “with re-
gret” (infelizmente), “with pleasure” (feliz-
mente), and “it is preferable” (preferencial-
mente).

Valuation: This lexicon assigns a value to facts.
Usually, adjectives are employed as valua-
tion, but as adjectives are context dependent
we use only in this class the markers related
to intensification, such as: “absolutely” (ab-
solutamente), “highly” (altamente), and “ap-
proximately” (aproximadamente).

3.3 Debiasing the Training Set
Bias is generally defined as a deviation from a
norm. If the norm is unknown to us, then bias is
hard to identify. Thus, our approach for debias-
ing the training set starts by finding the norm (in
terms of the subjectivity within rater comments)
for each score value. Intuitively, the amount of
subjectivity within a comment should be similar
to the amount of subjectivity within another com-
ment, given that the scores associated with the cor-
responding essays are close to each other. So, we
should not expect to find essays having discrepant
scores, but for which the corresponding comments
show a similar amount of subjectivity. Our debi-
asing approach is divided into three steps:

1. Rater comments are represented according to
the amount of subjectivity cues. In order to
represent a comment, we calculate the dis-
tance between it and each of the five subjec-
tivity lexicons. More specifically, we learn
word embeddings (Mikolov et al., 2013) for
the Portuguese language, and then we em-
ployed the Word Mover’s Distance func-
tion (Kusner et al., 2015) between a comment
and the five subjectivity lexicons. As a re-
sult, each comment is finally represented by
a five-dimensional subjectivity vector, where
each dimension corresponds to the amount of
a specific type of subjectivity. This results
in a subjectivity space, where comments are
placed according to their amount of subjec-
tivity.

2. We group subjectivity vectors according to
the score misalignment associated with the
corresponding essay. Then, we calculate cen-
troids for each group in order to find the pro-
totypical subjectivity vector for each group
(or misalignment level).

3. The distance to the prototypical subjectivity
vector is used as a measure of deviation from
the norm. Specifically, we sort essays accord-
ing to the distance between the subjectivity
vector and the corresponding centroid. Then,
we define a number of essays to be removed
from the training set. The relative number of
essays to be removed from the training set is
controlled by hyper-parameter α.

4 Experiments

In this section, we present the data we used to
learn and evaluate different AES models. Then,
we discuss our evaluation procedure and report the
results obtained with our debiasing approach. In
particular, our experiments aim to answer the fol-
lowing research questions:

RQ1: How scores are distributed across the es-
says? How aligned with human raters are dif-
ferent AES models?

RQ2: Does subjectivity in rater comments vary
depending on the given score?

RQ3: Does subjectivity in rater comments vary
depending on the misalignment between the
AES model and the human rater?

RQ4: Can we mitigate the effects of biased rat-
ings?

4.1 Corpus
Our corpus is composed of essays (n = 1, 840)
that were written by high-school students as part
of a standardized Brazilian national exam. Each
essay is evaluated according to the following five
objective aspects:

Formal language: Mastering of the formal Por-
tuguese language.

Relevance to the prompt: Understanding of es-
say prompt and application of concepts from
different knowledge fields, to develop the
theme in an argumentative dissertation for-
mat.

232



 0

 100

 200

 300

 400

 500

 600

0 1 2 3 4 5 6 7 8 9 10

#
 e

s
s
a

y
s

Score

SVR
RF
LR
GB

MLP
Human

Figure 1: Distribution of the scores given by human
raters. Also, distribution of the scores given by differ-
ent AES models.

Organization of information: Selecting, con-
necting, organizing, and interpreting
information.

Argumentation: Demonstration of knowledge of
linguistic mechanisms required to construct
arguments.

Solution proposal: Formulation of a proposal to
the problem presented.

The final score is given as the sum of the scores
associated with each aspect. Raters are supposed
to perform impartial and objective evaluations,
and they must enter specific comments in order to
ground their scores. Also, each essay was assessed
by one rater.

Bias-free ratings: We also separate a number of
essays (n = 50) which received similar scores by
three expert raters who were directly instructed to
perform impartial, objective, and unbiased evalu-
ations. These raters are PhD-level in Linguistics
with unlimited time to provide their ratings, and
they do not participate on the creation of the train-
ing set. We assume the ratings given to these es-
says were not contaminated by biased judgements,
and we will use these essays for evaluating the ef-
ficacy of AES models learned after the training set
is debiased.

4.2 Setup

We implemented the different AES models us-
ing scikit-learn (Pedregosa et al., 2011). Specif-
ically, we learn AES models using Support Vec-
tor Regression (SVR), Random Forests (RF), Lo-
gistic Regression (LR), Gradient Boosting (GB),
and Multi-Layer Perceptron (MLP). All models
are based on the same set of features, previously

 0

 50

 100

 150

 200

 250

 300

 350

-6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6

#
 e

s
s
a

y
s

Misalignment

SVR
RF
LR
GB

MLP

Figure 2: Distribution of misalignment for the different
AES models.

described in Section 3.1, and all models are trained
in regression mode. The measure used to evalu-
ate the effectiveness of the different models is the
quadratic weighted kappa (κ) which measures the
inter-agreement between human raters and AES
models (Cohen, 1960). We conducted five-fold
cross validation, where the dataset is arranged into
five folds with approximately the same number of
examples. At each run, four folds are used as train-
ing set, and the remaining fold is used as test set.
We also kept a separate validation set. The train-
ing set is used to learn the models, the validation
set is used to tune hyper-parameters and the test
set is used to estimate κ numbers for the different
the models. Unless otherwise stated, the results re-
ported are the average of the five runs, and are used
to assess the overall effectiveness of each model.
To ensure the relevance of the results, we assess
the statistical significance of our measurements by
comparing each pair of models using a Welch’s t-
test with p−value ≤ 0.01.

4.3 Results and Discussion

Next we report results obtained from the execution
of the experiments, and discuss these results in the
light of our research questions.

Score distribution: The first experiment is con-
cerned with RQ1. Figure 1 shows how scores
are distributed over the essays in our corpus.
Although the distribution differs for each AES
model, scores are centered around 4, and few
essays received extreme scores. The LR model
seems to have a preference for lower scores. The
scores provided by the GB and MLP models are
better distributed.

Figure 2 shows how aligned with human raters
are the different AES models. For most of the es-
says, AES models are well aligned with human

233



 0.35

 0.4

 0.45

 0.5

 0.55

 0.6

 0.65

 0  1  2  3  4  5  6  7  8  9  10

D
is

tr
ib

u
ti
o

n

Score

Argumentation
Presupposition 

Modalization

Valuation
Sentiment

Figure 3: Subjectivity distribution for human raters.

raters, showing misalignments that vary from −2
to +2. For some essays, the LR model tends to
give scores that are much smaller than the score
given by the human rater. The GB and MLP mod-
els perform very similary, but the MLP model
shows a slightly better alignment.

Subjectivity vectors and biased ratings: The
second experiment is concerned with RQ2. Fig-
ure 3 shows the average subjectivity vector
grouped according to the score given to the cor-
responding essay (i.e., the centroid or prototypi-
cal vector of a score). More specifically, we first
grouped subjectivity vectors according to the score
associated with the corresponding essay, and then
we calculated the average subjectivity vector for
each group. As shown in Figure 3, the argumen-
tation dimension increases with the score, while
modalization tends to decrease. Presupposition,
valuation and sentiment dimensions show a very
similar trend with varying score values.

Figure 4 shows t-SNE representations (van ter
Maaten and Hinton, 2008) for the average subjec-
tivity vectors (centroids for each group of score).
Three larger clusters emerged: subjectivity vectors
associated with score 0, subjectivity vectors asso-
ciated with scores between 1 and 6, and subjectiv-
ity vectors associated with scores between 6 and
10.

Subjectivity vectors and misalignment: The
third experiment is concerned with RQ3. Fig-
ure 5 shows the average subjectivity vector con-
sidering different levels of misalignment. More
specifically, we grouped essays according to the
misalignment between the score provided by the
AES model and the human rater. Then, we cal-
culated the average subjectivity vector for each
group. As we can see, subjectivity affects AES

0

1

2

3

4
5

6

7

8

9
10

Figure 4: t-SNE representation for subjectivity vectors.
Numbers correspond to the scores assigned to corre-
sponding essays.

models in different ways. In general, however,
subjectivity vectors within groups of essays asso-
ciated with extreme misalignments are very differ-
ent from subjectivity vectors associated with mild
misalignments.

Figure 6 shows t-SNE representations for sub-
jectivity vectors grouped by misalignment levels.
Each cluster contains ≈ 80% of the vectors associ-
ated with one of the misalignment levels inside the
cluster. That is, 20% of the essays will be removed
from the training set (i.e., α = 0.2).

Debiasing the training set: The last experiment
is concerned with RQ4. As described in Section
3.3, our debiasing approach works by removing
from the training set a number of essays (con-
trolled by α) that are more likely to be associated
with biased ratings. Table 1 shows κ numbers for
different α values. Clearly, the inter-agreement
decreases as we remove essays with potentially bi-
ased ratings from the training set. This happens
because the test set remains with essays that are
potentially associated with biased ratings. In this
case, removing biased ratings from the training set
is always detrimental to the efficacy of AES mod-
els.

In order to properly evaluate our debiasing ap-
proach, we employ the 50 separate essays with
bias-free ratings as our test set. In this case, biased
ratings are removed from the training set, and the
test set is composed by unbiased ratings. Table 2
shows κ numbers for different α values. As ex-
pected, the inter-agreement increases significantly
with α, until a point in which keeping removing
essays from the training set becomes detrimental.
This happens either because we start to remove
unbiased ratings, or the training set becomes too
small. In all cases, the MLP model showed to be

234



 0.35

 0.4

 0.45

 0.5

 0.55

 0.6

 0.65

-6 -5 -4 -3 -2 -1  0  1  2  3  4  5  6

D
is

tr
ib

u
ti
o

n

Misalignment

Argumentation
Presupposition 

Modalization

Valuation
Sentiment

 0.35

 0.4

 0.45

 0.5

 0.55

 0.6

 0.65

-6 -5 -4 -3 -2 -1  0  1  2  3  4  5  6

D
is

tr
ib

u
ti
o

n

Misalignment

Argumentation
Presupposition 

Modalization

Valuation
Sentiment

 0.35

 0.4

 0.45

 0.5

 0.55

 0.6

 0.65

-6 -5 -4 -3 -2 -1  0  1  2  3  4  5  6

D
is

tr
ib

u
ti
o

n

Misalignment

Argumentation
Presupposition 

Modalization

Valuation
Sentiment

 0.35

 0.4

 0.45

 0.5

 0.55

 0.6

 0.65

-6 -5 -4 -3 -2 -1  0  1  2  3  4  5  6

D
is

tr
ib

u
ti
o

n

Misalignment

Argumentation
Presupposition 

Modalization

Valuation
Sentiment

 0.35

 0.4

 0.45

 0.5

 0.55

 0.6

 0.65

-6 -5 -4 -3 -2 -1  0  1  2  3  4  5  6

D
is

tr
ib

u
ti
o

n

Misalignment

Argumentation
Presupposition 

Modalization

Valuation
Sentiment

Figure 5: Subjectivity distribution. (Top to bottom)
SVR, RF, LR, GB, and MLP.

-9

-8

-7
-6

-5

-4

-3

-2 -1
0

1

2

3

4

5

6

-10

-9-8
-7

-6

-5

-4-3
-2

-1

0

1
2

3

4

5

6
7

Figure 6: t-SNE representation for subjectivity vectors
grouped by misalignment levels. The corresponding
regions comprise essays associated with specific mis-
alignment levels. (Top) GB model. (Bottom) MLP
model.

κ
α SVR RF LR GB MLP
− .404 .410 .408 .432 .446
0.1 .390 .339 .364 .378 .393
0.2 .365 .331 .344 .370 .393
0.3 .345 .326 .338 .365 .386
0.4 .340 .324 .333 .361 .384
0.5 .307 .317 .328 .358 .382

Table 1: κ numbers for different models with varying α
values. There are potentially biased ratings in the test
set.

κ
α SVR RF LR GB MLP
− .451 .472 .466 .491 .521
0.1 .467 .491 .481 .505 .544
0.2 .481 .511 .490 .521 .562
0.3 .488 .526 .497 .542 .571
0.4 .491 .523 .499 .547 .569
0.5 .481 .518 .494 .545 .560

Table 2: κ numbers for different models with varying
α values. Ratings in the the test set are likely to be
unbiased.

statistically superior than the other models.

235



5 Conclusions

In this paper, we investigated the problem of au-
tomated essay scoring in the presence of biased
ratings. Most of the existing work on automated
essay scoring is devoted to maximize the agree-
ment with the human rater. This is fairly danger-
ous, since human ratings may be biased. Overall,
discussion about the quality of the ratings in au-
tomated essay scoring is lacking, and this was a
central interest in this paper. Specifically, we cre-
ate a subjectivity space from which potentially bi-
ased scores/ratings can be identified. We showed
that removing biased scores from the training set
results in improved AES models. Finally, the es-
say data as well as the subjectivity lexicons that
we will release as part of this research could prove
useful in other bias related tasks.

Acknowledgments

This work was partially funded by projects InWeb
(grant MCT/CNPq 573871/2008-6) and MASWeb
(grant FAPEMIG/PRONEX APQ-01400-14), and
by the authors individual grants from CNPq and
FAPEMIG. AV thanks the support received from
Kunumi.

References
Dimitrios Alikaniotis, Helen Yannakoudakis, and

Marek Rei. 2016. Automatic text scoring using neu-
ral networks. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics. pages 715–725.

Laura Allen, Scott Crossley, and Danielle McNamara.
2015. Predicting misalignment between teachers’
and students’ essay scores using natural language
processing tools. In Proceedings of the 17th Inter-
national Conference on Artificial Intelligence in Ed-
ucation. pages 529–532.

Evelin Amorim and Adriano Veloso. 2017. A multi-
aspect analysis of automatic essay scoring for brazil-
ian portuguese. In Proceedings of the 15th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (Student Research Work-
shop). pages 94–102.

Dan Ariely. 2008. Predictably Irrational: The Hidden
Forces That Shape Our Decisions. HarperCollins.

Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater R© v. 2. The Journal of Technol-
ogy, Learning and Assessment 4(3).

Jonathan Baron. 2007. Thinking and Deciding, vol-
ume 4. Cambridge University Press.

Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with annotation noise. In Proceedings of
the 47th Annual Meeting of the Association for Com-
putational Linguistics. pages 280–287.

Hongbo Chen and Ben He. 2013. Automated essay
scoring by maximizing human-machine agreement.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing. pages
1741–1752.

Yen-Yu Chen, Chien-Liang Liu, Chia-Hoang Lee, and
Tao-Hsing Chang. 2010. An unsupervised auto-
mated essay scoring system. IEEE Intelligent Sys-
tems 25(5):61–67.

Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. Educational and Psychological
Measurement 20(1):37–46.

Scott Crossley, Laura Allen, Erica Snow, and Danielle
McNamara. 2015. Pssst... textual features... there is
more to automatic essay scoring than just you! In
Proceedings of the Fifth International Conference
on Learning Analytics and Knowledge. pages 203–
207.

Ronan Cummins, Meng Zhang, and Ted Briscoe. 2016.
Constrained multi-task learning for automated essay
scoring. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics. pages 789–799.

Alexandre Davis and Adriano Veloso. 2016. Subject-
related message filtering in social media through
context-enriched language models. Trans. Compu-
tational Collective Intelligence 21:97–138.

Fei Dong and Yue Zhang. 2016. Automatic features
for essay scoring - an empirical study. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing. pages 1072–1077.

Fei Dong, Yue Zhang, and Jie Yang. 2017. Attention-
based recurrent convolutional neural network for au-
tomatic essay scoring. In Proceedings of the 21st
Conference on Computational Natural Language
Learning. pages 153–162.

Noura Farra, Swapna Somasundaran, and Jill Burstein.
2015. Scoring persuasive essays using opinions and
their targets. In Proceedings of the 10th Workshop
on Innovative Use of NLP for Building Educational
Applications.

Pedro Henrique Calais Guerra, Adriano Veloso, Wag-
ner Meira Jr., and Virgı́lio A. F. Almeida. 2011.
From bias to opinion: a transfer-learning approach
to real-time sentiment analysis. In Proceedings of
the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. pages 150–
158.

Daniel Kaheman and Amos Tversky. 1972. Subjective
probability: A judgment of representativeness. Cog-
nitive Psychology 3(3):430–454.

236



Beata Klebanov, Jill Burstein, Nitin Madnani, Adam
Faulkner, and Joel Tetreault. 2012. Building sub-
jectivity lexicon(s) from scratch for essay data. In
Proceedings of the 13th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing. pages 591–602.

Beata Beigman Klebanov and Eyal Beigman. 2009.
From annotator agreement to noise models. Com-
putational Linguistics 35(4):495–503.

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian
Weinberger. 2015. From word embeddings to docu-
ment distances. In Proceedings of the 32nd Interna-
tional Conference on Machine Learning. pages 957–
966.

E. Martins. 2000. Manual de redação e estilo. O Es-
tado de São Paulo. https://books.google.
com.br/books?id=CAkLnwEACAAJ.

Teresa BF Martins, Claudete M Ghiraldelo, Maria
das Graças Volpe Nunes, and Osvaldo Novais
de Oliveira Junior. 1996. Readability formulas ap-
plied to textbooks in brazilian portuguese. Icmsc-
Usp.

Danielle McNamaraa, Scott Crossley, Rod Roscoe,
Laura Allen, and Jianmin Dai. 2015. A hierarchical
classification approach to automated essay scoring.
Assessing Writing 23:35–59.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Proceedings of the 27th Annual Con-
ference on Neural Information Processing Systems.
pages 3111–3119.

C. Myford and E. Wolfe. 2003. Detecting and mea-
suring rater effects using many-facet rasch measure-
ment. Journal Appl Meas. 4(4):386–422.

Ellis Page. 1967. Grading essays by computer:
progress report. In Proceedings of the Invitational
Conference on Testing Problems. pages 87–100.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in python. Journal
of Machine Learning Research 12:2825–2830.

Peter Phandi, Kian Ming Adam Chai, and Hwee Tou
Ng. 2015. Flexible domain adaptation for auto-
mated essay scoring using correlated linear regres-
sion. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing. pages 431–439.

Marta Recasens, Cristian Danescu-Niculescu-Mizil,
and Dan Jurafsky. 2013. Linguistic models for an-
alyzing and detecting biased language. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics. pages 1650–1659.

Brian Riordan, Andrea Horbach, Aoife Cahill, Torsten
Zesch, and Chong Min Lee. 2017. Investigating
neural architectures for short answer scoring. In
Proceedings of the 12th Workshop on Innovative
Use of NLP for Building Educational Applications.
pages 159–168.

Swapna Somasundaran, Jill Burstein, and Martin
Chodorow. 2014. Lexical chaining for measuring
discourse coherence quality in test-taker essays. In
Proceedings of the 25th International Conference on
Computational Linguistics. pages 950–961.

Swapna Somasundaran, Brian Riordan, Binod
Gyawali, and Su-Youn Yoon. 2016. Evaluating
argumentative and narrative essays using graphs. In
Proceedings of the 26th International Conference
on Computational Linguistics. pages 1568–1578.

Yi Song, Michael Heilman, Beata Klebanov, and Paul
Deane. 2014. Applying argumentation schemes for
essay scoring. In Proceedings of the 1st Workshop
on Argument Mining. pages 69–78.

Sargur Srihari, Jim Collins, Rohini Srihari, Har-
ish Srinivasan, Shravya Shetty, and Janina Brutt-
Griffler. 2008. Automatic scoring of short handwrit-
ten essays in reading comprehension tests. Artif. In-
tell. 172(2-3):300–324.

Sargur Srihari, Rohini Srihari, Pavithra Babu, and Har-
ish Srinivasan. 2007. On the automatic scoring
of handwritten essays. In Proceedings of the 20th
International Joint Conference on Artificial Intelli-
gence. pages 2880–2884.

Kaveh Taghipour and Hwee Tou Ng. 2016. A neural
approach to automated essay scoring. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing. pages 1882–1891.

Amos Tversky and Daniel Kaheman. 1974. Judgement
under uncertainty: Heuristics and biases. Science
185:1124–1131.

Laurens van ter Maaten and Geoffrey Hinton. 2008.
Visualizing high-dimensional data using t-sne.
Journal of Machine Learning Research 9:2579–
2605.

237


