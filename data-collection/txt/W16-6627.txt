



















































The aNALoGuE Challenge: Non Aligned Language GEneration


Proceedings of The 9th International Natural Language Generation conference, pages 168–170,
Edinburgh, UK, September 5-8 2016. c©2016 Association for Computational Linguistics

The aNALoGuE Challenge: Non Aligned Language GEneration

Jekaterina Novikova and Verena Rieser
The Interaction Lab, Heriot-Watt University, Edinburgh, UK

j.novikova, v.t.rieser@hw.ac.uk

Abstract

We propose a shared task based on recent
advances in learning to generate natural lan-
guage from meaning representations using se-
mantically unaligned data. The aNALoGuE
challenge aims to evaluate and compare recent
corpus-based methods with respect to their
scalability to data size and target complexity,
as well as to assess predictive quality of auto-
matic evaluation metrics.

1 Relevance

Natural language generation plays a critical role for
Conversational Agents (CAs) as it has a significant
impact on a users impression of the system. Most
CAs utilise domain-dependent methods including
hand-written grammars or domain-specific language
templates for surface realisation, both of which are
costly to develop and maintain. Recent corpus-based
methods hold the promise of being easily portable
across domains, e.g. (Angeli et al., 2010; Kon-
stas and Lapata, 2012; Mairesse and Young, 2014),
but require high quality training data consisting of
meaning representations (MR) paired with natural
language (NL) utterances, augmented by alignments
between elements of meaning representation and
natural language words. Creating aligned data is a
non-trivial task in its own right, see e.g. (Liang et
al., 2009). This shared task aims to strengthen re-
cent research on corpus-based NLG from unaligned
data, e.g. (Dušek and Jurcicek, 2015; Wen et al.,
2015; Mei et al., 2015; Sharma et al., 2016). These
approaches do not require costly semantic align-
ment, but are based on parallel data sets, which can

be collected in sufficient quality and quantity using
effective crowd-sourcing techniques (Novikova and
Rieser, 2016), and as such open the door for rapid
development of NLG components for CAs in new
domains.

In addition, we hope to attract interest from re-
lated disciplines, such as semantic parsing or statis-
tical machine translation, which face similar chal-
lenges when learning from parallel non-aligned data
sets.

Flat MR NL reference

name[The Eagle],
eatType[coffee shop],
food[French],
priceRange[moderate],
customerRating[3/5],
area[riverside],
kidsFriendly[yes],
near[Burger King]

1. There is a riverside coffee
shop called The Eagle that has
French food at an average price
range. It is child friendly,
located near Burger King, and
has a 3 star customer rating.

2. The three star coffee shop,
The Eagle, gives families a
mid-priced dining experience
featuring a variety of wines and
cheeses. Find The Eagle near
Burger King.

3. The Eagle coffee shop is based
in the riverside area near Burger
King. It serves food at mid range
prices. It has a three star rating
and is family friendly.

Table 1: An example of a data instance.

2 Data Description

The data provided for this shared challenge was
collected by using the CrowdFlower platform and
quality controlled as described in (Novikova and

168



Rieser, 2016). The dataset provides information
about restaurants and consists of more than 50k
combinations of a dialogue act-based meaning rep-
resentation and up to 5 references in natural lan-
guage, as shown in Table 1. Each MR consists of
3 - 8 attributes (labels), such as name, food or area.
The detailed ontology of all attributes and values is
provided in Table 2. The dataset will be split into
training, validation and testing sets (70/15/15). The
training and validation sets will be provided to the
participants, while the testing set is used for the final
evaluation of the systems. The sets are constructed
to ensure a similar distribution of single-sentenced
and multi-sentenced references in each set, as well
as a similar distribution of MRs of different length.

Attribute Data Type Example value
name verbatim string The Eagle, ...
eatType dictionary restaurant, pub, ...
familyFriendly boolean Yes / No
priceRange dictionary cheap, expensive, ...
food dictionary French, Italian, ...
near verbatim string market square, ...
area dictionary riverside, city center, ...
customerRating enumerable 1 of 5 (low), 4 of 5 (high), ...

Table 2: Domain ontology.

3 Evaluation

We will provide two types of baseline systems,
which are frequently used by previous corpus-based
methods, e.g. (Wen et al., 2015; Mairesse and
Young, 2014): a challenging hand-crafted genera-
tor and n-gram Language Models, following early
work by (Oh and Rudnicky, 2002). To evaluate
the results, both objective and subjective metrics
will be used. We will explore automatic mea-
sures, such as BLEU-4 (Papineni et al., 2002) and
NIST (Doddington, 2002) scores, which are widely
used in a machine translation and NLG research,
and will allow comparing the results of this chal-
lenge with previous work. Since automatic met-
rics may not consistently agree with human per-
ception, human evaluation will be used to assess
subjective quality of generated utterances. Hu-
man judges will be recruited using CrowdFlower.
Judges will be asked to compare utterance gener-
ated by different systems and score them in terms
of informativeness (“Does the utterance contains

all the information specified in the MR?”), natural-
ness (“Could the utterance have been produced by
a native speaker?”) and phrasing (“Do you like the
way the utterance has been expressed?”). Here, we
will explore different experimental setups for evalu-
ation following previous shared tasks, e.g. (Belz and
Kow, 2011). The challenge will also benefit from
a national research grant on Domain Independent
NLG (EP/M005429/1) which will provide funds for
crowd-based evaluation.

4 Research Questions

The task is set up to answer the following research
questions with respect to corpus-driven methods:
• “How much data is enough?” So far, corpus-based
methods have been trained on limited data sets, such
as BAGEL (404 target utterances), Cambridge SF
(5193) or RoboCup (1919). We release a data set
which is almost 10-times times bigger in size than
previous corpora. This allows us to test the upper
quality boundary of corpus-driven NLG, as well as
to determine the optimal/minimal data size per algo-
rithm.
• “Can they model more complex targets?”So far,
corpus-driven methods are restricted to single sen-
tences. Our corpus contains 37% examples with
multiple (2-6) sentences. We predict that longer tar-
get outputs are challenging for, e.g. neural networks
due to the vanishing gradient problem. Furthermore,
our crowd-sourced utterances were elicited using
pictures, which makes them more varied in sentence
structure and vocabulary than previously used cor-
pora (Novikova and Rieser, 2016).
• “How good is BLEU?” Previous research has
shown that automatic metrics like BLEU do not con-
sistently agree with human perception (Stent et al.,
2004; Belz and Gatt, 2008). We will therefore ex-
plore how well they correlate with human judge-
ment. We will also explore how well these met-
rics are able to capture desired variation given a set
of possible reference sentences, following similar
shared tasks in machine translation, e.g. (Stanojević
et al., 2015).

Acknowledgments
This research received funding from the EPSRC projects GUI

(EP/L026775/1), DILiGENt (EP/M005429/1) and MaDrIgAL

(EP/N017536/1).

169



References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A

simple domain-independent probabilistic approach to
generation. In Conference on Empirical Methods in
Natural Language Processing (EMNLP).

Anja Belz and Albert Gatt. 2008. Intrinsic vs. extrin-
sic evaluation measures for referring expression gen-
eration. In Proceedings of ACL-08: HLT, Short Pa-
pers, pages 197–200, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.

Anja Belz and Eric Kow. 2011. Discrete vs. continu-
ous rating scales for language evaluation in nlp. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: Short Papers - Volume 2, HLT
’11, pages 230–235, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 138–145. Morgan Kaufmann Publish-
ers Inc.

Ondřej Dušek and Filip Jurcicek. 2015. Training a nat-
ural language generator from unaligned data. In Pro-
ceedings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics and the 7th Inter-
national Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 451–461, Bei-
jing, China, July. Association for Computational Lin-
guistics.

Ioannis Konstas and Mirella Lapata. 2012. Unsupervised
concept-to-text generation with hypergraphs. In Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL).

Percy Liang, Michael I. Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less supervi-
sion. In Proc. of ACL-IJCNLP.

François Mairesse and Steve Young. 2014. Stochastic
language generation in dialogue using factored lan-
guage models. Comput. Linguist., 40(4):763–799, De-
cember.

Hongyuan Mei, Mohit Bansal, and Matthew R. Walter.
2015. What to talk about and how? selective genera-
tion using lstms with coarse-to-fine alignment. CoRR,
abs/1509.00838.

Jekaterina Novikova and Verena Rieser. 2016. Crowd-
sourcing NLG data: Pictures elicit better data. In Proc.
of the 9th International Natural Language Generation
conference (INLG).

Alice H. Oh and Alexander I. Rudnicky. 2002. Stochas-
tic natural language generation for spoken dialog sys-
tems. Computer Speech and Language, 16:387–407.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual meeting on association for computational
linguistics, pages 311–318. Association for Computa-
tional Linguistics.

Shikhar Sharma, Jing He, Kaheer Suleman, Hannes
Schulz, and Philip Bachman. 2016. Natural language
generation in dialogue using lexicalized and delexical-
ized data. CoRR, abs/1606.03632.

Miloš Stanojević, Amir Kamran, Philipp Koehn, and
Ondřej Bojar. 2015. Results of the wmt15 metrics
shared task. In Proceedings of the Tenth Workshop on
Statistical Machine Translation, pages 256–273, Lis-
bon, Portugal, September. Association for Computa-
tional Linguistics.

Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex infor-
mation presentation in spoken dialog systems. In Pro-
ceedings of the 42nd annual meeting on association
for computational linguistics, page 79. Association for
Computational Linguistics.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrkšić, Pei-Hao
Su, David Vandyke, and Steve Young. 2015. Seman-
tically Conditioned LSTM-based Natural Language
Generation for Spoken Dialogue Systems. In Proceed-
ings of the 2015 Conference on Empirical Methods in
Natural Language Processing, pages 1711–1721, Lis-
bon, Portugal, September. Association for Computa-
tional Linguistics.

170


