



















































To Annotate or Not? Predicting Performance Drop under Domain Shift


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2163–2173,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2163

To Annotate or Not?
Predicting Performance Drop under Domain Shift

Hady Elsahar and Matthias Gallé
NAVER LABS Europe

{hady.elsahar,matthias.galle}@naverlabs.com

Abstract

Performance drop due to domain-shift is an
endemic problem for NLP models in produc-
tion. This problem creates an urge to con-
tinuously annotate evaluation datasets to mea-
sure the expected drop in the model perfor-
mance which can be prohibitively expensive
and slow. In this paper, we study the problem
of predicting the performance drop of mod-
ern NLP models under domain-shift, in the ab-
sence of any target domain labels. We investi-
gate three families of methods (H-divergence,
reverse classification accuracy and confidence
measures), show how they can be used to pre-
dict the performance drop and study their ro-
bustness to adversarial domain-shifts. Our re-
sults on sentiment classification and sequence
labelling show that our method is able to pre-
dict performance drops with an error rate as
low as 2.15% and 0.89% for sentiment analy-
sis and POS tagging respectively.

1 Introduction

Building Natural Language Processing models
that perform well in the wild is still an open and
challenging problem. It is well known that mod-
ern machine-learning models can be brittle, mean-
ing that – even when achieving impressive perfor-
mance on the evaluation set – their performance
can degrade significantly when exposed to new ex-
amples with differences in vocabulary and writ-
ing style (Blitzer and Pereira, 2007; Jia and Liang,
2017; Brun and Nikoulina, 2018). This drop in
performance when changing from domain Ds to
domain Dt can be due to a variety of causes.
It could be because of Co-variate Shift (Shi-
modaira, 2000; Storkey, 2009), where the input
distribution changes, but the conditional distribu-
tion does not, i.e. PDs(y|x) = PDt(y|x) but
PDs(x) 6= PDt(x); or due to Concept Shift, when
PDs(y|x) 6= PDt(y|x) and PDs(x) = PDt(x),

Value	of	the	Domain-shift
Detection	Metric	on	Dt

Expected	Accuracy
Drop	on	Dt	

Figure 1: In this paper we introduce several domain-
shift detection metrics (x-axis) and employ them to es-
timate the performance drop on a new target domain
Dt by regressing on those metrics and their associated
real performance drop (green dots).

or Label Shift, when PDs(y) 6= PDt(y) and
PDs(x|y) = PDt(x|y) (Zhang et al., 2013; Lip-
ton et al., 2018), or a mix between them (Moreno-
Torres et al., 2012; Quionero-Candela et al., 2009).

Such changes are often the norm in many real-
world applications, creating an urge in the industry
to continuously check that models in production
are performing well in the wild or when there is a
new client or a new type of data. However, con-
tinuously sampling and annotating data-points can
be prohibitively slow and costly, particularly when
a large annotated sample is needed to correctly
represent the target joint distribution or when the
change can be gradual (Kifer et al., 2004).
In this work, we investigate how we can estimate

the performance drop of a model when evaluated
on a new target domain, without the need of any la-
beled examples from this target domain. Perform-
ing this estimation accurately has an important im-
pact on the decision process of real-time debug-
ging and maintaining machine learning models in
production. For instance, such insights can drive



2164

the decision to annotate more data for retraining
or even adjusting the model accordingly (e.g. per-
forming unsupervised domain adaptation).

We propose a method that takes advantage of
several domain-shift detection metrics and employ
them to estimate through regression the perfor-
mance drop on a target domain on which no an-
notated data is available, The overall approach is
schematically depicted in Fig. 1. The relation-
ship between the domain shift metrics and the
real performance drop of different domains is pre-
computed over different existing domains (green
dots in Fig. 1), and for a new target domains
the performance drop can be estimated simply by
evaluating the function learned through simple re-
gression. This process directly yields the value of
performance drop the model will suffer when ex-
posed to this target domain examples. We believe
is more interpretable and useful to ML practition-
ers than other intermediate signals such as of out-
of-distribution detection methods.

This paper introduces the following contribu-
tions:

• We introduce a new task and methodology
for directly predicting performance drop of a
model under domain-shift, without the need
of labeled examples from the target domain.

• We survey, formalize and evaluate domain-
shift detection metrics from 3 different fami-
lies (§2) and propose new adaptations.

• We benchmark each proposes metric on two
tasks of different natures: document classi-
fication and sequence labeling (§3 and §4),
and show their robustness under adversarial
domain-shift scenarios.

2 Measuring Performance Drop

We are interested in the problem of measuring the
performance drop of classifier C, trained on sam-
ples from the source domain Ds when applied to
the target domain Dt samples. In the presence of
labeled samples fromDt, this could empirically be
measured by the difference in test errors between
the source and target domain.

∆R = Pr
(x,y)∈Ds

(C(x) 6= y)− Pr
(x,y)∈Dt

(C(x) 6= y) (1)

To calculate this value empirically, one would
need annotated examples in the form of a labeled
test set for each new target domain, which is costly

and/or time-consuming. In this section we intro-
duce several metrics of different natures that cor-
relate with the drop in a model performance with-
out the need of any annotated examples from the
target domain. In particular, we will consider three
families of measures:

• H-divergence based metrics: based on the
capacity of another classification model to
distinguish between samples from Ds and
Dt.

• Confidence-based, using the certainty of the
model over its predictions.

• Reverse classification accuracy, where pre-
dicted values are used as pseudo-labels over
Dt.

2.1 H-divergence based metrics
Building upon previous work from Kifer et al.
(2004) for detecting domain change in data
streams, Ben-David et al. (2010) define the tar-
get error of a model under domain-shift in terms
of its source error and the divergence between the
distributions of the two domains. This domain di-
vergence is referred to as the H-divergence and
formalized as follows: given a hypothesis class H
that consists of a set of binary classifiers h : X →
{0, 1} theH-divergence can be represented as:

2 sup
h∈H
|Prx∼Ds [h(x) = 1]− Prx∼Dt [h(x) = 1]| (2)

This translates to calculating the capacity of the
hypothesis classH to distinguish between the dis-
tributions of both domains Ds and Dt.1 Ben-
David et al. (2010) prove that for a symmetric hy-
pothesis class the H-divergence can be calculated
through a finite sample sampled fromDs andDt.2

Calculating the value forH-divergence exactly re-
quires finding the hypothesis h ∈ H that has
minimum error on the binary classification prob-
lem between samples from Ds and Dt, which is
intractable in practice. Thus, Ben-David et al.
(2006) approximates this through learning a model
that discriminates between the source and target
examples. This approximated value is often re-
ferred to as the Proxy A-distance (PAD).
Given a domain classifier Gd : x → [0, 1] param-
eterized by θd, we calculate PAD as the following:

1For simplicity we abuse the notation here by using Ds
and Dt to refer both to the distribution and the set of samples
of the source and target domain respectively.

2See (Ben-David et al., 2010, Appendix) for the proof.



2165

PAD = 1− 2E(Gd)
s.t.;

E(G) = 1− 1
|D|

∑
xi∈Ds,Dt

|G(xi)− I(xi ∈ Ds)|

(3)

where I is an indicator function.
The PADmeasure is task-agnostic and measures

therefore only the co-variate shift. It has been
used before to measure domain discrepancy be-
tween datasets (Blitzer et al., 2007; Rai et al.,
2010) for NLP applications. However, different
from the time when PAD was introduced, mod-
ern NLP models not only compute a mapping be-
tween input and labels, but also infer an intermedi-
ate representation. For a given task, this represen-
tation is supposed to provide a view of the input
that highlights the relevant part that could be help-
ful for a correct classification in this task. In par-
ticular, those intermediate representations should
not be sensitive to task-irrelevant features that pro-
vide nevertheless strong signals to distinguish be-
tween the source and the target domains (yielding
high PAD values). As an example, consider the
task of named entity extraction on top of the 20
newsgroup dataset: the To: field is a highly dis-
criminating feature for domain classification but
arguably irrelevant to the task of extracting named
entities.

Following this intuition, we propose a modifi-
cation to the PAD measure. It is the classification
accuracy of discriminating between the interme-
diate representation coming from – respectively –
source and target domain (we use the last layer of
the neural network). We assume that the task clas-
sifier C consists of two functions Gf and Gy. The
first projects the input to a hidden representation of
size m: Gf : X → Rm; while the second is a lin-
ear layer that uses this representation to predict the
class labelsGy : Rm → [0, 1]|Y |. Differently from
PAD, the domain classifierG∗d : Rm → [0, 1] takes
the hidden representations as an input instead of
the original input. The learnable parameters of
Gf ,Gy andG∗d are θf , θy and θ

∗
d respectively. Our

proposed metric PAD* is then:

PAD* = 1− 2E (G∗d(Gf (x))

θf , θy are learned by minimizing the loss func-
tion of the task. Afterwards θf is frozen and θ∗d is

learned by minimizing the negative log-likelihood
loss3 for the domain discrimination task between
Ds and Dt.

2.2 Confidence Based Metrics
While the final decision of classifiers is discrete,
the weight given to that decision can be inter-
preted as the confidence the model has in that de-
cision. This has been the basis of overcoming
domain-shift using many self-training techniques
which select the most confident examples as new
training examples, together with the predicted
class as pseudo-label (McClosky et al., 2006).
However, modern neural networks are known to
give wrongly calibrated confidence scores (Guo
et al., 2017) meaning that the associated proba-
bility scores to the predicted class label does not
reflect its correctness likelihood.

A few calibration techniques have been pro-
posed to overcome this problem. For its simplicity
and effectiveness we follow the temperature scal-
ing method (Guo et al., 2017). It is a post training
method that rescales the logits of any neural net-
work model to soften the softmax by raising the
output entropy of the probabilities scores.
Given a model trained on the source domain
dataset Ds, let z be the logits vector produced
by the very last layer for a given input, yield-
ing the non-calibrated confidence score q =
max

k
(softmax(z))k for the predicted class label.

The calibrated confidence score q̂ is calculated
then as follows:

q̂ = max
k

(softmax(z/T ))k (4)

Where T is a learnable scalar “temperature” pa-
rameter. T can be learned by minimizing the neg-
ative log likelihood loss over the validation set
Dvals ;

4

T ∗ = argmin
T

(
−

N∑
i=1

1[k=yi] log (softmax (zi/T ))

)
(xi, yi) ∈ Dvals , T > 0 (5)

Where 1[k=yi] is a one hot vector containing
one in front of the true class label yi and zero
elsewhere.
Accordingly, we introduce two confidence based
metrics to measure the domain-shift between the

3Minimizing the Huber loss as in (Ben-David et al., 2006)
provides very similar results.

4Following (Guo et al., 2017) we use for this validation
set the same set as for hyper-parameter tuning.



2166

source and the target datasets Ds and Dt.

1) CONF the drop in average probability scores
of the predicted class:

CONF =
1

|Ds|
∑

i:xi∈Ds

qi −
1

|Dt|
∑

j:xj∈Dt

qj (6)

2) CONF CALIB the drop in average calibrated
probability scores for the predicted class:

CONF CALIB =
1

|Ds|
∑

i:xi∈Ds

q̂i −
1

|Dt|
∑

j:xj∈Dt

q̂j

(7)

2.3 Reverse Classification Accuracy
The idea of reverse classification accuracy is to
use a classifier trained on the source domain to
pseudo-label the target domain. That new dataset
is then used to train a new classifier whose accu-
racy is measured on held-out data from the source
domain. This was used in the past to select among
existing models or datasets the best-performing
one for a given target domain (Fan and Davidson,
2006; Zhong et al., 2010). In order to use this to
create a proxy for domain-shift, we proceed as fol-
lows: a task classifier C is trained on the annotated
source domain datasetDs which is then run to cre-
ate pseudo-labels for the unlabeled target data Dt.
Those pseudo-labels are then used as training data
for a reverse classifier Ĉ – using the same architec-
ture and training algorithm used to obtain C. The
performance of both classifiers are compared on a
heldout subset D

′
s ∈ Ds from the source domain

datasets and used to define the RCA measure

RCA =
1

|D ′s |

m
′∑

xi ,yi∈D
′
s

I [yi = C(xi)]− I
[
yi = Ĉ(xi)

]
(8)

The RCA measure could be low because of two
reasons. It could be due to the domain-shift we try
to capture, as a very different distribution would
have a major impact in the training data generated
on top of Dt. Or it could be due to the accumula-
tion of error created by the “back-and-forth” train-
ing. If Dt follows the same distribution than Ds,
than the measure would only capture the impact of
that accumulation of errors. To remove this source
of errors, we also propose another measure RCA*
which is the performance difference of Ĉ and a
classifier C′ trained in the same way but using as
target domain held-out data from the source do-
main. C is used again to pseudo-label a dataset,

(a) RCA

(b) RCA*

Figure 2: The classifier C is trained on source domain
(blue) with true labels (green) and is applied on target
domain data (red) to create pseudo labels (grey). In the
case of RCA* C is also applied on a held-out source
domain data (blue). Pseudo-labels (grey) are then used
to train new classifiers (Ĉ and for RCA* also C′). They
are later applied on a test set of the source domain to
calculate RCA and RCA*.

which is this time taken from the same distribu-
tion than Ds, and this new dataset is then used as
training data for C′. RCA* is then calculated as
follows:

RCA* =
1

|D ′s |

m
′∑

xi ,yi∈D
′
s

I
[
yi = C′(xi)

]
− I

[
yi = Ĉ(xi)

]
(9)

A schematic view of these two measures is
depicted in Fig. 2.

3 Experiments

3.1 Regression of Performance Drop
We present a regression based method that can di-
rectly estimate the performance drop of a model
trained on Ds and tested on Dt. This method does
not require any labeling in Dt however it assumes
the availability of a small fixed number of labeled
evaluation datasets Do ∈ D \ {Ds, Dt}.
For each one of these fixed evaluation datasets,
using simple linear regression we fit a regression
line between the drop in the model accuracy and a
domain-shift detection metrics of choice (§2). Af-
terwards, by calculating the value of this domain-
shift detection metric on dt we can then use this re-
gression line to predict the performance drop when
evaluating the model dt.
In our experiments we report the Mean Absolute



2167

0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
0.0

0.1

0.2

0.3
PAD

0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
0.0

0.1

0.2

0.3
RCA

0.00 0.02 0.04 0.06 0.08 0.10 0.12
0.0

0.1

0.2

0.3
CONF

0.5 0.6 0.7 0.8 0.9
0.0

0.1

0.2

0.3
PAD*

0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35
0.0

0.1

0.2

0.3
RCA*

0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14
0.0

0.1

0.2

0.3
CONF_CALIB

Metric

Cl
as
sif
ica

tio
n 
Dr

op

Figure 3: Scatter plots showing correlations between the values of each proposed domain similarity metric (x-axis)
and the actual classification drop (y-axis) for the sentiment classification task. Each point corresponds to a single
domain-shift scenario. Different colors represent different source domains and hence a single trained model on the
source domain dataset.

Error (MAE) and Max error between our predicted
values of performance drop using our method and
the actual performance drop on dt.
To put in perspective the predictive power of this
regression method on each proposed metric, we
implement a baseline (Mean) that instead of learn-
ing a regression it always gives the average classi-
fication drop over all evaluation datasets in Do.
We also experiment with doing linear regres-
sion using all metrics proposed in §2 at once
(Ensemble).

3.2 Datasets

We evaluate our metrics across two tasks of differ-
ent nature namely sentiment analysis and part-of-
speech (POS) tagging.

3.2.1 Sentiment Analysis
For sentiment analysis we follow (Blitzer et al.,
2007; Ruder and Plank, 2018) by using the Ama-
zon multidomain reviews dataset in English.5 Al-
though this dataset contains several domains they
still come from the same platform which can re-
strain some diversity. To alleviate this we com-
bine it with two other datasets namely Yelp6 and
IMDB movie reviews dataset7. Although the pre-
processed dataset in (Blitzer et al., 2007) is widely
used, it only consists of 4 domains and the input
documents are reduced to their TF-IDF weights
which is not a suitable input for modern NLP ar-
chitectures. Thus we perform a different prepro-

5http://jmcauley.ucsd.edu/data/amazon/index.html
6https://www.yelp.com/dataset
7https://ai.stanford.edu/ amaas/data/sentiment/

cessing across a wider range of domains as fol-
lows: After removing redundant reviews, we pre-
process the dataset to obtain binary labels such
that reviews with 1 to 3 stars are labeled as neg-
ative while reviews with 4 or 5 stars are labeled
as positive. Finally we randomly sample 21K re-
views (10K train, 10K valid and 1K test) from 21
domains. Yelp and IMDB datasets follow the same
preprocessing steps and are added as 2 extra do-
mains. This yield a total new dataset with 23 do-
mains for sentiment analysis yielding 506 domain-
shift scenarios.8

3.2.2 Part of Speech Tagging
For part of speech tagging we select 4 publicly
available 9 Universal Dependencies datasets for
English (Nivre et al., 2016). We split the EWT
dataset (UD for English web treebank) according
to each sub-category, while keeping the rest of the
smaller datasets as is. This yields in total 8 do-
mains with roughly comparable sizes (∼ 4K sen-
tences each) yielding in total 56 domain-shift sce-
narios.

3.3 Experiment Setup and Training Details

For each domain-shift scenario, the task model is
trained on the source domain training split. Test-
ing is performed on both source and target do-
mains test sets. Simultaneously, we calculate each
of our proposed metrics in §2. Note here that
some of those metrics such as PAD, PAD*, RCA

8This makes 10x more examples and 40x more domain-
shift scenarios than the widely used version of (Blitzer et al.,
2007).

9ParTUT, GUM, EWT, LinES



2168

and RCA* require the text of the target domain
test set. None of the proposed metrics require
any labels from the target domain, in line with the
unsupervised scenario we are considering. The
initial word embeddings are a hyperparameter,
and we consider random initialization, pretrained
GloVe (Pennington et al., 2014) with several di-
mensions and contextualized word embeddings
using ELMo (Peters et al., 2018). As model ar-
chitectures we use Multi-Layer Bi-LSTM (Graves
and Schmidhuber, 2005) followed by a multi-layer
feed-foward NN and a softmax. In sentiment anal-
ysis the feed forward network is applied on the last
output of the Bi-LSTM to produce one label pre-
diction for the whole sentence, while for POS tag-
ging it is applied to each output to produce a label
prediction for each corresponding token.
For training the domain classifiers used to calcu-
late the PAD and PAD* metrics, we use similar
model architecture as in sentiment analysis, ini-
tialized from scratch in case of PAD or initialized
with the weights of the best task model in case of
PAD*. Afterwards, they are trained to discrimi-
nate between inputs of the source and target do-
main datasets.
To calculate CONF CALIB, the best performing
model is selected and its confidence weights are
calibrated using temperature scaling on the source
domain validation set.
Each model is trained using Adam optimza-
tion (Kingma and Ba, 2015) and early stopping
with patience 5 over the source domain validation
set.
All models and training code have been imple-
mented using AllenNLP (Gardner et al., 2018)
and made publicly available in addition to the
Datasets.10 The detailed hyper-parameters and
the test results for each source domain dataset are
shown in the appendix.

4 Evaluation Results

Fig. 3 contains a plot for each of the proposed met-
rics, showing the value of that measure vs the ac-
tual drop in performance for the sentiment analysis
task. Each point corresponds to a single domain-
shift scenario. While there is an overall trend be-
tween each of our proposed metrics and the actual
drop in classification accuracy, the actual trend
differs depending on which source domain was

10https://github.com/hadyelsahar/
domain-shift-prediction

used.11 This is unsurprising as all measures but
PAD are model-specific. Instead of computing
overall correlation trends, we decide to evaluate
the capacity of each measure to serve as a predic-
tor of the classification drop, as detailed in the next
section.

4.1 Error in Performance Drop Prediction
Table 1 shows the mean absolute and maximum
error values of the regression process that predicts
the performance drop of a model trained over ds
and evaluated on dt. The baseline that predicts
always the mean performance drop achieves on
a mean absolute error of 5.2% and max error of
12.77% for sentiment analysis, while this num-
ber drops for POS tagging to a mean of 1.06%
and max of 1.67% in the worse case. All our
proposed metrics improve significantly over that,
with PAD* clearly improving over all other in both
datasets. Overall, the best performing method is
PAD* with 2.15% and 0.88% mean absolute error
in prediction of performance drop for sentiment
analysis and POS tagging respectively. Learning
an ensemble between all metrics does not guaran-
tee to provide the best predictions, which could be
due to the small size of the points used for regres-
sion.

4.2 Impact of Number of Domains
Our proposed method for detecting performance
drop assumes the existence of several evaluation
datasets from different domains. This is a non-
negligible cost, as having so many evaluation
dataset from different domains might not be real-
istic in many scenarios. In this section we evaluate
the impact of having a lower number of source do-
mains from which to learn the classification drop.
We sample randomly a smaller number of datasets,
and repeat the experiment. In Fig. 4 and Fig. 5
we report the results of that with 5 different runs
of sampling. As expected, the error decreases
by increasing the number of out of domain test
sets. However, prediction error enters an accept-
able score with only 3 annotated source domains.

4.3 Adversarial Shift
PAD* and PAD are calculated solely from learn-

ing to classify between the source and target do-
mains and could therefore be particularly sensi-
tive to task irrelevant domain-shifts i.e. input sig-

11Each source domain corresponds to a single trained
model, denoted by the same color in Fig. 3.

https://github.com/hadyelsahar/domain-shift-prediction
https://github.com/hadyelsahar/domain-shift-prediction


2169

Sentiment POS tagging
MAE Max MAE Max

Mean 5.2± 2.02 12.77 1.06± 0.36 1.67

RCA 2.88± 1.31 7.17 1.08± 0.31 1.58
RCA* 2.92± 1.39 7.42 1.05± 0.27 1.42

CONF 2.85± 1.69 8.86 0.89± 0.39 1.75
CONF_CALIB 2.67± 1.49 8.13 1.12± 0.45 1.83

PAD 2.51± 1.54 8.16 1.24± 0.37 1.7
PAD* 2.15 ± 0.88 4.64 0.89 ± 0.1 0.99

Ensemble 2.22± 0.9 5.02 1.03± 0.42 1.78

Table 1: Mean absolute error (MAE) and max error
(Max) of the performance drop prediction for the task
of sentiment analysis (left) and POS tagging (right).
Best results (the lower the better) are shown in bold
while best results within each family of measure are
underlined.

2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22
Number of Domains used for regression

2×100

3×100
4×100

6×100

Er
ro
r i
n 
es
tim

at
io
n 
of
 c
lf 
dr
op PAD

PAD*
RCA
RCA*
CONF
CONF_CALIB
Mean

Figure 4: Fig. showing effect of number of domains on
the prediction of accuracy drop for sentiment analysis.

nals that can help to differentiate domains apart
have has no impact on the task itself. To evalu-
ate the robustness of each of our proposed metrics
in this specific scenario, we perform an experi-
ment by applying an adversarial domain-shift. For
each domain-shift scenario in the sentiment analy-
sis task we add a different unique tag <SOURCE>
and <TARGET> in the beginning of each exam-
ple in the source and the target domains respec-
tively: this has no impact on the final task classifi-
cation, but makes it trivial to discriminate between
the two domains. The results of re-running the
same experiment on this modified dataset are in
Table 2. As expected, PAD is greatly affected, not
performing better than the baseline which just pre-
dicts the mean classification drop. The other two
families are less or not at all affected. Surprisingly,
PAD* also degrades significantly, despite using a
task representation which should learn to discard
the useless (for the task prediction) newly intro-
duced token. To understand this better, we an-
alyze the behaviour of the models with different
depths. In Fig. 7 the best performing model for a

2 3 4 5 6 7
Number of Domains used for regression

100

1.5 × 100

3 × 100
4 × 100
6 × 100

Er
ro

r i
n 

es
tim

at
io

n 
of

 c
lf 

dr
op PAD

PAD*
RCA
RCA*
CONF
CONF_CALIB
Mean

Figure 5: Fig. showing effect of number of domains on
the prediction of accuracy drop for POS tagging.

MAE max

Mean 4.35± 2.31 10.79

RCA 3.63± 1.95 9.56
RCA* 3.67± 1.93 8.44

CONF 2.88± 1.16 5.5
CONF_CALIB 2.81 ± 1.09 5.48

PAD 4.35± 2.31 10.79
PAD* 4.6± 2.14 10.77

Ensemble 3.02± 0.98 5.88

Table 2: Mean absolute error and Max error of per-
formance drop prediction for sentiment analysis under
adversarial shift evaluation (The lower the better).

given depth is used: the results indicate that the
capacity for predicting the classification drop us-
ing PAD* is greatly influenced only if the model is
very shallow. This becomes clearer in Fig. 6 where
we repeat the plots from Fig. 3 for this adversar-
ial dataset restricting the hyper-paramater search
of the task model to models of a fixed depths. At
any model depth the PAD measure is always maxi-
mal (1.00) as it learns to differentiate perfectly the
two domains. While this is also the case for PAD*
in the case of model of depth 2, deeper model are
less sensitive to that. This might indicate that the
higher layer (which are used as input representa-
tion for the domain classification models used to
calculate PAD*) are learning to ignore the domain
token as it is irrelevant for the task at hand. The
rest of the metrics are more robust with respect
to adversarial examples and model depth, which
might make them good candidates for cases of se-
vere co-variate shift.

5 Related Work

A large body of work has tackled the problem
of defining, measuring and adapting to domain-
shift in machine learning and NLP (Quionero-
Candela et al., 2009; Blitzer and Pereira, 2007;



2170

Model depth = 2

0.995 1.000 1.005

0.0

0.1

0.2

0.3
PAD

0.0 0.2 0.4

0.0

0.1

0.2

0.3
RCA

0.000 0.025 0.050

0.0

0.1

0.2

0.3
CONF

0.96 0.98 1.00

0.0

0.1

0.2

0.3
PAD*

0.0 0.2

0.0

0.1

0.2

0.3
RCA*

0.00 0.05

0.0

0.1

0.2

0.3
CONF_CALIB

Model depth = 5

0.995 1.000 1.005

0.0

0.1

0.2

PAD

0.0 0.2 0.4

0.0

0.1

0.2

RCA

0.000 0.025 0.050 0.075

0.0

0.1

0.2

CONF

0.6 0.8

0.0

0.1

0.2

PAD*

0.0 0.2

0.0

0.1

0.2

RCA*

0.00 0.05

0.0

0.1

0.2

CONF_CALIB

Figure 6: Measure vs drop in performance in the adversarial case, restricting the hyper-parameter search to models
of depth 2 (left) and 5 (right)

2 3 4 5 6 7 8
Model Depth

2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0

er
ro
r i
n 
es
tim

at
io
n 
of
 c
lf 
dr
op

Baseline
PAD
PAD*

Figure 7: The effect of the task model depth on the
capacity of the metric to estimate the drop in classifica-
tion under adversarial evaluation.

Ben-David et al., 2010). Kifer et al. (2004); Ben-
David et al. (2006) formalize the concept of H-
divergence to measure domain discrepancy. Later
on, Blitzer et al. (2008) introduce the Proxy A-
distance as a proxy to H-divergence which was
used by Blitzer et al. (2007); Rai et al. (2010)
to gain insights about adaptability of representa-
tions for domain adaptation and active learning.
More recently, Ruder et al. (2017) showed that
ProxyA-distance is effective as a similarity metric
for dataset selection. H-divergence has inspired
a large body of work (Ganin et al., 2016; Tzeng
et al., 2017) for domain adaptation by minimizing
the H-divergence between domains using a do-
main adversarial loss.

There exists a large line of work on develop-
ing domain similarity metrics for data selection
under transfer learning (Moore and Lewis, 2010;
Plank and van Noord, 2011; Axelrod et al., 2011;
Wu and Huang, 2016), however there is no con-
sensus on which similarity measure is suitable for
which NLP task. Ruder and Plank (2017) show
that a combination of several of those similar-
ity features re-weighted using Bayesian Optimiza-

tion performs the best across several tasks. This
method however is not fully unsupervised as it re-
quires an annotated validation set from each target
domain.

The closest work to ours are Ravi et al. (2008);
Van Asch and Daelemans (2010), as they also
tackle the problem of predicting performance of
models at test time. They introduce a set of do-
main similarity metric that correlates with the per-
formance of a model on a test set for both part-
of-speech tagging and parsing. However, many of
those measure are built on top of heavily hand-
crafted features and evaluated through shallow
models which are different to the way modern
NLP models are built now.

Confidence scores can be a good estimation
of domain similarity, however it is well known
for modern neural networks that their confidence
scores are usually mis-calibrated. Because of that,
many self-training techniques rely on an ensem-
ble of models to calculate for bootstrapping in-
domain examples training examples, such as co-
training and tri-training (Zhou and Li, 2005), tri-
training with dis-agreement (Søgaard, 2010) and
multi-task tri-training (Ruder and Plank, 2018). In
our paper we aim to measure the potential classi-
fication drop of a specific model due to domain-
shift. Training an ensemble of models of the
same architecture in hand might be expensive and
inefficient for just measuring domain-shift. We
try to overcome this problem by calibrating con-
fidence scores (Zadrozny and Elkan, 2002; Guo
et al., 2017; DeVries and Taylor, 2018). Us-
ing confidence scores of calibrated models has
shown a large success in out of distribution detec-
tion (Hendrycks and Gimpel, 2017; Liang et al.,
2018, 2017; Lee et al., 2018).

The idea of reverse classification accuracy (Fan



2171

and Davidson, 2006; Zhong et al., 2010) was first
introduced as a part of “Transfer Cross Validation”
to select both models and data in a cross valida-
tion framework, optimized for transfer learning.
More recently, this was adapted as a confidence
estimator to predicting segmentation performance
in the clinical domain (Valindria et al., 2017). The
same idea has been used recently used for evaluat-
ing GANs for image generation (Shmelkov et al.,
2018).

6 Conclusion

In this paper we studied the problem of predic-
tion of performance drop due to domain-shift for
modern NLP models, having no labeled target do-
main data but at least few fixed evaluation datasets
from other domains. We investigated three fam-
ily of metrics for measuring domain similarity. In
each of them we introduced a novel adaptation on
existing metrics to adapt to the different nature of
modern NLP models and possibly obtain higher
prediction scores of the performance drop. Our
evaluation over two NLP tasks show that this drop
can be estimated very accurately even when only
few other source-domains evaluation datasets are
available. In general, the family of H-divergence
based measures perform the best. However, they
are prone to fail when there is a severe change
in the marginal distribution that is task irrelevant.
In particular, the well established PAD measure is
rendered useless in a setting where we artificially
exaggerate that phenomena. Using a task-specific
representation is slightly more robust to that prob-
lem, although only if a deeper model is used.

A strong family of measures that does not have
that drawback are confidence-based measures, but
they require access to the confidence weights and
not only the predicted labels by the model. Un-
fortunately, our adaptation of the reverse classi-
fication accuracy measure RCA* does not obtain
higher performance than the simple RCA. This
could be due to the fact that datasets sampled from
the same domain can still be subjected to sample
selection bias. In the future work, we plan to in-
vestigate ways to solve that. These conclusions are
summarized in Table 3, which we recommend as
guideline when deciding what measure to use.

Acknowledgments

This work was partially funded through the Eu-
ropean Unions Horizon 2020 research and in-

Measure Robust to Black box Good with small
co-variate shift Target Data

PAD X X X
PAD* (X) X X

RCA X X X
RCA* X X X

CONF X X X
CONF CALIB X X X

Table 3: Summary of domain similarity metrics dis-
cussed in this work and their different characteristics.

novation programme under grant agreement No.
786741 (SMOOTH Platform).

References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.

2011. Domain adaptation via pseudo in-domain data
selection. ACM.

Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79(1-2):151–175.

Shai Ben-David, John Blitzer, Koby Crammer, and
Fernando Pereira. 2006. Analysis of representa-
tions for domain adaptation. In Advances in Neu-
ral Information Processing Systems 19, Proceedings
of the Twentieth Annual Conference on Neural In-
formation Processing Systems, Vancouver, British
Columbia, Canada, December 4-7, 2006, pages
137–144.

John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jennifer Wortman. 2008. Learning
bounds for domain adaptation. In Advances in neu-
ral information processing systems, pages 129–136.

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL 2007, Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics, June 23-30, 2007, Prague, Czech Re-
public.

John Blitzer and Fernando Pereira. 2007. Domain
adaptation of natural language processing systems.
University of Pennsylvania, pages 1–106.

Caroline Brun and Vassilina Nikoulina. 2018. As-
pect based sentiment analysis into the wild. In Pro-
ceedings of the 9th Workshop on Computational Ap-
proaches to Subjectivity, Sentiment and Social Me-
dia Analysis, WASSA@EMNLP 2018, Brussels, Bel-
gium, October 31, 2018, pages 116–122.

Terrance DeVries and Graham W. Taylor. 2018. Learn-
ing confidence for out-of-distribution detection in
neural networks. CoRR, abs/1802.04865.

https://www.microsoft.com/en-us/research/publication/domain-adaptation-via-pseudo-in-domain-data-selection/
https://www.microsoft.com/en-us/research/publication/domain-adaptation-via-pseudo-in-domain-data-selection/
https://doi.org/10.1007/s10994-009-5152-4
https://doi.org/10.1007/s10994-009-5152-4
http://papers.nips.cc/paper/2983-analysis-of-representations-for-domain-adaptation
http://papers.nips.cc/paper/2983-analysis-of-representations-for-domain-adaptation
http://aclweb.org/anthology/P07-1056
http://aclweb.org/anthology/P07-1056
http://aclweb.org/anthology/P07-1056
https://aclanthology.info/papers/W18-6217/w18-6217
https://aclanthology.info/papers/W18-6217/w18-6217
http://arxiv.org/abs/1802.04865
http://arxiv.org/abs/1802.04865
http://arxiv.org/abs/1802.04865


2172

Wei Fan and Ian Davidson. 2006. Reverse testing: an
efficient framework to select amongst classifiers un-
der sample selection bias. In Proceedings of the 12th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 147–156.
ACM.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor S. Lempitsky.
2016. Domain-adversarial training of neural net-
works. Journal of Machine Learning Research,
17:59:1–59:35.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Pe-
ters, Michael Schmitz, and Luke Zettlemoyer. 2018.
Allennlp: A deep semantic natural language pro-
cessing platform. arXiv preprint arXiv:1803.07640.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works, 18(5-6):602–610.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-
berger. 2017. On calibration of modern neural net-
works. In Proceedings of the 34th International
Conference on Machine Learning, ICML 2017, Syd-
ney, NSW, Australia, 6-11 August 2017, pages 1321–
1330.

Dan Hendrycks and Kevin Gimpel. 2017. A baseline
for detecting misclassified and out-of-distribution
examples in neural networks. In 5th International
Conference on Learning Representations, ICLR
2017, Toulon, France, April 24-26, 2017, Confer-
ence Track Proceedings. OpenReview.net.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
In Empirical Methods in Natural Language Process-
ing (EMNLP).

Daniel Kifer, Shai Ben-David, and Johannes Gehrke.
2004. Detecting change in data streams. In
(e)Proceedings of the Thirtieth International Con-
ference on Very Large Data Bases, Toronto, Canada,
August 31 - September 3 2004, pages 180–191.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin.
2018. Training confidence-calibrated classifiers for
detecting out-of-distribution samples. In 6th Inter-
national Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May
3, 2018, Conference Track Proceedings. OpenRe-
view.net.

Shiyu Liang, Yixuan Li, and R. Srikant. 2017. Prin-
cipled detection of out-of-distribution examples in
neural networks. CoRR, abs/1706.02690.

Shiyu Liang, Yixuan Li, and R. Srikant. 2018. Enhanc-
ing the reliability of out-of-distribution image detec-
tion in neural networks. In 6th International Confer-
ence on Learning Representations, ICLR 2018, Van-
couver, BC, Canada, April 30 - May 3, 2018, Con-
ference Track Proceedings. OpenReview.net.

Zachary C. Lipton, Yu-Xiang Wang, and Alexander J.
Smola. 2018. Detecting and correcting for label
shift with black box predictors. In Proceedings
of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsmässan, Stock-
holm, Sweden, July 10-15, 2018, volume 80 of
Proceedings of Machine Learning Research, pages
3128–3136. PMLR.

David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, Proceedings, June 4-9, 2006,
New York, New York, USA.

Robert C. Moore and William D. Lewis. 2010. Intel-
ligent selection of language model training data. In
ACL 2010, Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, July 11-16, 2010, Uppsala, Sweden, Short Pa-
pers, pages 220–224. The Association for Computer
Linguistics.

Jose G Moreno-Torres, Troy Raeder, Rocı́O Alaiz-
Rodrı́Guez, Nitesh V Chawla, and Francisco Her-
rera. 2012. A unifying view on dataset shift in clas-
sification. Pattern Recognition, 45(1):521–530.

Joakim Nivre, Marie-Catherine de Marneffe, Filip
Ginter, Yoav Goldberg, Jan Hajic, Christopher D.
Manning, Ryan T. McDonald, Slav Petrov, Sampo
Pyysalo, Natalia Silveira, Reut Tsarfaty, and Daniel
Zeman. 2016. Universal dependencies v1: A mul-
tilingual treebank collection. In Proceedings of
the Tenth International Conference on Language
Resources and Evaluation LREC 2016, Portorož,
Slovenia, May 23-28, 2016. European Language Re-
sources Association (ELRA).

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1532–1543. ACL.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT 2018, New Or-
leans, Louisiana, USA, June 1-6, 2018, Volume 1
(Long Papers), pages 2227–2237. Association for
Computational Linguistics.

http://jmlr.org/papers/v17/15-239.html
http://jmlr.org/papers/v17/15-239.html
http://proceedings.mlr.press/v70/guo17a.html
http://proceedings.mlr.press/v70/guo17a.html
https://openreview.net/forum?id=Hkg4TI9xl
https://openreview.net/forum?id=Hkg4TI9xl
https://openreview.net/forum?id=Hkg4TI9xl
http://www.vldb.org/conf/2004/RS5P1.PDF
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
https://openreview.net/forum?id=ryiAv2xAZ
https://openreview.net/forum?id=ryiAv2xAZ
http://arxiv.org/abs/1706.02690
http://arxiv.org/abs/1706.02690
http://arxiv.org/abs/1706.02690
https://openreview.net/forum?id=H1VGkIxRZ
https://openreview.net/forum?id=H1VGkIxRZ
https://openreview.net/forum?id=H1VGkIxRZ
http://proceedings.mlr.press/v80/lipton18a.html
http://proceedings.mlr.press/v80/lipton18a.html
http://aclweb.org/anthology/N/N06/N06-1020.pdf
http://www.aclweb.org/anthology/P10-2041
http://www.aclweb.org/anthology/P10-2041
http://www.lrec-conf.org/proceedings/lrec2016/summaries/348.html
http://www.lrec-conf.org/proceedings/lrec2016/summaries/348.html
http://aclweb.org/anthology/D/D14/D14-1162.pdf
http://aclweb.org/anthology/D/D14/D14-1162.pdf
https://aclanthology.info/papers/N18-1202/n18-1202
https://aclanthology.info/papers/N18-1202/n18-1202


2173

Barbara Plank and Gertjan van Noord. 2011. Effective
measures of domain similarity for parsing. In The
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, Proceedings of the Conference, 19-24 June,
2011, Portland, Oregon, USA, pages 1566–1576.
The Association for Computer Linguistics.

Joaquin Quionero-Candela, Masashi Sugiyama, Anton
Schwaighofer, and Neil D Lawrence. 2009. Dataset
shift in machine learning. The MIT Press.

Piyush Rai, Avishek Saha, Hal Daumé III, and Suresh
Venkatasubramanian. 2010. Domain adaptation
meets active learning. In Proceedings of the NAACL
HLT 2010 Workshop on Active Learning for Natural
Language Processing, pages 27–32. Association for
Computational Linguistics.

Sujith Ravi, Kevin Knight, and Radu Soricut. 2008.
Automatic prediction of parser accuracy. In 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2008, Proceedings of
the Conference, 25-27 October 2008, Honolulu,
Hawaii, USA, A meeting of SIGDAT, a Special In-
terest Group of the ACL, pages 887–896. ACL.

Sebastian Ruder, Parsa Ghaffari, and John G. Breslin.
2017. Data selection strategies for multi-domain
sentiment analysis. CoRR, abs/1702.02426.

Sebastian Ruder and Barbara Plank. 2017. Learning
to select data for transfer learning with bayesian
optimization. In Proceedings of the 2017 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP 2017, Copenhagen, Denmark,
September 9-11, 2017, pages 372–382. Association
for Computational Linguistics.

Sebastian Ruder and Barbara Plank. 2018. Strong
baselines for neural semi-supervised learning under
domain shift. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2018, Melbourne, Australia, July 15-
20, 2018, Volume 1: Long Papers, pages 1044–1054.

Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning
and inference, 90(2):227–244.

Konstantin Shmelkov, Cordelia Schmid, and Karteek
Alahari. 2018. How good is my gan? In Com-
puter Vision - ECCV 2018 - 15th European Con-
ference, Munich, Germany, September 8-14, 2018,
Proceedings, Part II, volume 11206 of Lecture Notes
in Computer Science, pages 218–234. Springer.

Anders Søgaard. 2010. Simple semi-supervised train-
ing of part-of-speech taggers. In ACL 2010, Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, July 11-16,
2010, Uppsala, Sweden, Short Papers, pages 205–
208.

Amos Storkey. 2009. When training and test sets are
different: characterizing learning transfer. Dataset
shift in machine learning, pages 3–28.

Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor
Darrell. 2017. Adversarial discriminative domain
adaptation. In 2017 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2017, Hon-
olulu, HI, USA, July 21-26, 2017, pages 2962–2971.
IEEE Computer Society.

Vanya V Valindria, Ioannis Lavdas, Wenjia Bai, Kon-
stantinos Kamnitsas, Eric O Aboagye, Andrea G
Rockall, Daniel Rueckert, and Ben Glocker. 2017.
Reverse classification accuracy: predicting segmen-
tation performance in the absence of ground truth.
IEEE transactions on medical imaging, 36(8):1597–
1606.

Vincent Van Asch and Walter Daelemans. 2010. Us-
ing domain similarity for performance estimation.
In Proceedings of the 2010 Workshop on Domain
Adaptation for Natural Language Processing, pages
31–36. Association for Computational Linguistics.

Fangzhao Wu and Yongfeng Huang. 2016. Sentiment
domain adaptation with multiple sources. In Pro-
ceedings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics, ACL 2016, Au-
gust 7-12, 2016, Berlin, Germany, Volume 1: Long
Papers. The Association for Computer Linguistics.

Bianca Zadrozny and Charles Elkan. 2002. Transform-
ing classifier scores into accurate multiclass proba-
bility estimates. In Proceedings of the Eighth ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, July 23-26, 2002, Ed-
monton, Alberta, Canada, pages 694–699.

Kun Zhang, Bernhard Schölkopf, Krikamol Muandet,
and Zhikun Wang. 2013. Domain adaptation under
target and conditional shift. In Proceedings of the
30th International Conference on Machine Learn-
ing, ICML 2013, Atlanta, GA, USA, 16-21 June
2013, volume 28 of JMLR Workshop and Confer-
ence Proceedings, pages 819–827. JMLR.org.

Erheng Zhong, Wei Fan, Qiang Yang, Olivier Ver-
scheure, and Jiangtao Ren. 2010. Cross validation
framework to choose amongst models and datasets
for transfer learning. In Joint European Conference
on Machine Learning and Knowledge Discovery in
Databases, pages 547–562. Springer.

Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Ex-
ploiting unlabeled data using three classifiers. IEEE
Trans. Knowl. Data Eng., 17(11):1529–1541.

http://www.aclweb.org/anthology/P11-1157
http://www.aclweb.org/anthology/P11-1157
http://www.aclweb.org/anthology/D08-1093
http://arxiv.org/abs/1702.02426
http://arxiv.org/abs/1702.02426
https://aclanthology.info/papers/D17-1038/d17-1038
https://aclanthology.info/papers/D17-1038/d17-1038
https://aclanthology.info/papers/D17-1038/d17-1038
https://aclanthology.info/papers/P18-1096/p18-1096
https://aclanthology.info/papers/P18-1096/p18-1096
https://aclanthology.info/papers/P18-1096/p18-1096
https://doi.org/10.1007/978-3-030-01216-8_14
http://www.aclweb.org/anthology/P10-2038
http://www.aclweb.org/anthology/P10-2038
https://doi.org/10.1109/CVPR.2017.316
https://doi.org/10.1109/CVPR.2017.316
http://aclweb.org/anthology/P/P16/P16-1029.pdf
http://aclweb.org/anthology/P/P16/P16-1029.pdf
https://doi.org/10.1145/775047.775151
https://doi.org/10.1145/775047.775151
https://doi.org/10.1145/775047.775151
http://proceedings.mlr.press/v28/zhang13d.html
http://proceedings.mlr.press/v28/zhang13d.html
https://doi.org/10.1109/TKDE.2005.186
https://doi.org/10.1109/TKDE.2005.186

