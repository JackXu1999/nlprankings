



















































Encoding Source Language with Convolutional Neural Network for Machine Translation


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 20–30,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Encoding Source Language with Convolutional Neural Network for
Machine Translation

Fandong Meng1 Zhengdong Lu2 Mingxuan Wang1 Hang Li2 Wenbin Jiang1 Qun Liu3,1
1Key Laboratory of Intelligent Information Processing,

Institute of Computing Technology, Chinese Academy of Sciences
{mengfandong,wangmingxuan,jiangwenbin,liuqun}@ict.ac.cn

2Noah’s Ark Lab, Huawei Technologies
{Lu.Zhengdong,HangLi.HL}@huawei.com

3ADAPT Centre, School of Computing, Dublin City University

Abstract
The recently proposed neural network
joint model (NNJM) (Devlin et al.,
2014) augments the n-gram target lan-
guage model with a heuristically cho-
sen source context window, achieving
state-of-the-art performance in SMT.
In this paper, we give a more sys-
tematic treatment by summarizing the
relevant source information through a
convolutional architecture guided by
the target information. With dif-
ferent guiding signals during decod-
ing, our specifically designed convolu-
tion+gating architectures can pinpoint
the parts of a source sentence that are
relevant to predicting a target word,
and fuse them with the context of en-
tire source sentence to form a unified
representation. This representation, to-
gether with target language words, are
fed to a deep neural network (DNN)
to form a stronger NNJM. Experiments
on two NIST Chinese-English trans-
lation tasks show that the proposed
model can achieve significant improve-
ments over the previous NNJM by up
to +1.08 BLEU points on average.

1 Introduction

Learning of continuous space representation
for source language has attracted much at-
tention in both traditional statistical machine
translation (SMT) and neural machine trans-
lation (NMT). Various models, mostly neural
network-based, have been proposed for repre-
senting the source sentence, mainly as the en-
coder part in an encoder-decoder framework
(Bengio et al., 2003; Auli et al., 2013; Kalch-
brenner and Blunsom, 2013; Cho et al., 2014;

Sutskever et al., 2014). There has been some
quite recent work on encoding only “relevant”
part of source sentence during the decoding
process, most notably neural network joint
model (NNJM) in (Devlin et al., 2014), which
extends the n-grams target language model by
additionally taking a fixed-length window of
source sentence, achieving state-of-the-art per-
formance in statistical machine translation.

In this paper, we propose novel convolu-
tional architectures to dynamically encode the
relevant information in the source language.
Our model covers the entire source sentence,
but can effectively find and properly summa-
rize the relevant parts, guided by the informa-
tion from the target language. With the guiding
signals during decoding, our specifically de-
signed convolution architectures can pinpoint
the parts of a source sentence that are relevant
to predicting a target word, and fuse them with
the context of entire source sentence to form a
unified representation. This representation, to-
gether with target words, are fed to a deep neu-
ral network (DNN) to form a stronger NNJM.
Since our proposed joint model is purely lexi-
calized, it can be integrated into any SMT de-
coder as a feature.

Two variants of the joint model are also
proposed, with coined name tagCNN and
inCNN, with different guiding signals used
from the decoding process. We integrate the
proposed joint models into a state-of-the-art
dependency-to-string translation system (Xie
et al., 2011) to evaluate their effectiveness.
Experiments on NIST Chinese-English trans-
lation tasks show that our model is able
to achieve significant improvements of +2.0
BLEU points on average over the baseline. Our
model also outperforms Devlin et al. (2014)’s
NNJM by up to +1.08 BLEU points.

20



(a) tagCNN (b) inCNN

Figure 1: Illustration for joint LM based on CNN encoder.

RoadMap: In the remainder of this paper,
we start with a brief overview of joint language
model in Section 2, while the convolutional en-
coders, as the key component of which, will be
described in detail in Section 3. Then in Sec-
tion 4 we discuss the decoding algorithm with
the proposed models. The experiment results
are reported in Section 5, followed by Section 6
and 7 for related work and conclusion.

2 Joint Language Model

Our joint model with CNN encoders can be il-
lustrated in Figure 1 (a) & (b), which consists
1) a CNN encoder, namely tagCNN or inCNN,
to represent the information in the source sen-
tences, and 2) an NN-based model for predict-
ing the next words, with representations from
CNN encoders and the history words in target
sentence as inputs.

In the joint language model, the probabil-
ity of the target word en, given previous k
target words {en−k, · · ·, en−1} and the repre-
sentations from CNN-encoders for source sen-
tence S are

tagCNN: p(en|φ1(S, {a(en)}), {e}n−1n−k)
inCNN: p(en|φ2(S, h({e}n−1n−k)), {e}n−1n−k),

where φ1(S, {a(en)}) stands for the represen-
tation given by tagCNN with the set of indexes
{a(en)} of source words aligned to the target
word en, and φ2(S, h({e}n−1n−k)) stands for the
representation from inCNN with the attention

signal h({e}n−1n−k).

Let us use the example in Figure 1, where
the task is to translate the Chinese sentence

into English. In evaluating a target lan-
guage sequence “holds parliament
and presidential”, with “holds
parliament and” as the proceeding
words (assume 4-gram LM), and the affiliated
source word1 of “presidential” being
“Zǒngtǒng” (determined by word align-
ment), tagCNN generates φ1(S, {4}) (the in-
dex of “Zǒngtǒng” is 4), and inCNN gener-
ates φ2(S, h(holds parliament and)).
The DNN component then takes
"holds parliament and" and
(φ1 or φ2) as input to give the con-
ditional probability for next word, e.g.,
p("presidential"|φ1|2, {holds,
parliament, and}).
3 Convolutional Models

We start with the generic architecture for
convolutional encoder, and then proceed to
tagCNN and inCNN as two extensions.

1For an aligned target word, we take its aligned source
words as its affiliated source words. And for an unaligned
word, we inherit its affiliation from the closest aligned
word, with preference given to the right (Devlin et al.,
2014). Since the word alignment is of many-to-many,
one target word may has multi affiliated source words.

21



Figure 2: Illustration for the CNN encoders.

3.1 Generic CNN Encoder
The basic architecture is of a generic CNN en-
coder is illustrated in Figure 2 (a), which has a
fixed architecture consisting of six layers:

Layer-0: the input layer, which takes words
in the form of embedding vectors. In our
work, we set the maximum length of sen-
tences to 40 words. For sentences shorter
than that, we put zero padding at the be-
ginning of sentences.

Layer-1: a convolution layer after Layer-0,
with window size = 3. As will be dis-
cussed in Section 3.2 and 3.3, the guid-
ing signal are injected into this layer for
“guided version”.

Layer-2: a local gating layer after Layer-
1, which simply takes a weighted sum
over feature-maps in non-adjacent win-
dow with size = 2.

Layer-3: a convolution layer after Layer-2, we
perform another convolution with window
size = 3.

Layer-4: we perform a global gating over
feature-maps on Layer-3.

Layer-5: fully connected weights that maps
the output of Layer-4 to this layer as the
final representation.

3.1.1 Convolution
As shown in Figure 2 (a), the convolution in
Layer-1 operates on sliding windows of words
(width k1), and the similar definition of win-
dows carries over to higher layers. Formally,

for source sentence input x = {x1, · · · ,xN},
the convolution unit for feature map of type-f
(among F` of them) on Layer-` is

z
(`,f)
i (x) = σ(w

(`,f)ẑ(`−1)i + b
(`,f)),

` = 1, 3, f = 1, 2, · · · , F` (1)
where

• z(`,f)i (x) gives the output of feature map
of type-f for location i in Layer-`;

• w(`,f) is the parameters for f on Layer-`;
• σ(·) is the Sigmoid activation function;

• ẑ(`−1)i denotes the segment of Layer-`−1
for the convolution at location i , while

ẑ(0)i
def= [x>i , x

>
i+1, x

>
i+2]

>

concatenates the vectors for 3 words from
sentence input x.

3.1.2 Gating
Previous CNNs, including those for NLP
tasks (Hu et al., 2014; Kalchbrenner et al.,
2014), take a straightforward convolution-
pooling strategy, in which the “fusion” deci-
sions (e.g., selecting the largest one in max-
pooling) are based on the values of feature-
maps. This is essentially a soft template match-
ing, which works for tasks like classification,
but harmful for keeping the composition func-
tionality of convolution, which is critical for
modeling sentences. In this paper, we propose
to use separate gating unit to release the score
function duty from the convolution, and let it
focus on composition.

22



We take two types of gating: 1) for Layer-
2, we take a local gating with non-overlapping
windows (size = 2) on the feature-maps of con-
volutional Layer-1 for representation of seg-
ments, and 2) for Layer-4, we take a global
gating to fuse all the segments for a global rep-
resentation. We found that this gating strategy
can considerably improve the performance of
both tagCNN and inCNN over pooling.

• Local Gating: On Layer-1, for every gat-
ing window, we first find its original in-
put (before convolution) on Layer-0, and
merge them for the input of the gating net-
work. For example, for the two windows:
word (3,4,5) and word (4,5,6) on Layer-0,
we use concatenated vector consisting of
embedding for word (3,4,5,6) as the input
of the local gating network (a logistic re-
gression model) to determine the weight
for the convolution result of the two win-
dows (on Layer-1), and the weighted sum
are the output of Layer-2.

• Global Gating: On Layer-3, for feature-
maps at each location i, denoted z(3)i , the
global gating network (essentially soft-
max, parameterized wg), assigns a nor-
malized weight

ω(z(3)i ) = e
w>g z

(3)
i /

∑
j

ew
>
g z

(3)
j ,

and the gated representation on Layer-
4 is given by the weighted sum∑

i ω(z
(3)
i )z

(3)
i .

3.1.3 Training of CNN encoders
The CNN encoders, including tagCNN and
inCNN that will be discussed right below, are
trained in a joint language model described in
Section 2, along with the following parameters

• the embedding of the words on source and
the proceeding words on target;

• the parameters for the DNN of joint lan-
guage model, include the parameters of
soft-max for word probability.

The training procedure is identical to that of
neural network language model, except that the

parallel corpus is used instead of a monolin-
gual corpus. We seek to maximize the log-
likelihood of training samples, with one sam-
ple for every target word in the parallel corpus.
Optimization is performed with the conven-
tional back-propagation, implemented as sto-
chastic gradient descent (LeCun et al., 1998)
with mini-batches.

3.2 tagCNN

tagCNN inherits the convolution and gating
from generic CNN (as described in Section
3.1), with the only modification in the input
layer. As shown in Figure 2 (b), in tagCNN,
we append an extra tagging bit (0 or 1) to the
embedding of words in the input layer to indi-
cate whether it is one of affiliated words

x(AFF)i = [x
>
i 1]

>, x(NON-AFF)j = [x
>
j 0]

>.

Those extended word embedding will then be
treated as regular word-embedding in the con-
volutional neural network. This particular en-
coding strategy can be extended to embed more
complicated dependency relation in source lan-
guage, as will be described in Section 5.4.

This particular “tag” will be activated in a
parameterized way during the training for pre-
dicting the target words. In other words, the
supervised signal from the words to predict
will find, through layers of back-propagation,
the importance of the tag bit in the “affiliated
words” in the source language, and learn to put
proper weight on it to make tagged words stand
out and adjust other parameters in tagCNN
accordingly for the optimal predictive perfor-
mance. In doing so, the joint model can pin-
point the parts of a source sentence that are rel-
evant to predicting a target word through the
already learned word alignment.

3.3 inCNN

Unlike tagCNN, which directly tells the loca-
tion of affiliated words to the CNN encoder,
inCNN sends the information about the pro-
ceeding words in target side to the convolu-
tional encoder to help retrieve the information
relevant for predicting the next word. This is
essentially a particular case of attention model,
analogous to the automatic alignment mecha-
nism in (Bahdanau et al., 2014), where the at-

23



举行/VV

智利/NN 选举/NN

总统/NN

与/CC国会/NN

Chinese:  智利   举行        国会      与       总统            选举
English:  Chile   holds  parliament  and  presidential   elections

举行

智利 X1:NN

(a)

(b)

Chile    holds    X1

举行

(c)

holds

Figure 3: Illustration for a dependency tree (a) with three head-dependents relations in shadow,
an example of head-dependents relation rule (b) for the top level of (a), and an example of head
rule (c). “X1:NN” indicates a substitution site that can be replaced by a subtree whose root has
part-of-speech “NN”. The underline denotes a leaf node.

tention signal is from the state of a generative
recurrent neural network (RNN) as decoder.

Basically, the information from proceeding
words, denoted as h({e}n−1n−k), is injected into
every convolution window in the source lan-
guage sentence, as illustrated in Figure 2 (c).
More specifically, for the window indexed by
t, the input to convolution is given by the con-
catenated vector

ẑt = [h({e}n−1n−k), x>t , x>t+1, x>t+2]>.
In this work, we use a DNN to transform
the vector concatenated from word-embedding
for words {en−k · · · , en−k} into h({e}n−1n−k),
with sigmoid activation function. Through lay-
ers of convolution and gating, inCNN can 1)
retrieve the relevant segments of source sen-
tences, and 2) compose and transform the
retrieved segments into representation recog-
nizable by the DNN in predicting the words
in target language. Different from that of
tagCNN, inCNN uses information from pro-
ceeding words, hence provides complementary
information in the augmented joint language
model of tagCNN. This has been empirically
verified when using feature based on tagCNN
and that based on inCNN in decoding with
greater improvement.

4 Decoding with the Joint Model

Our joint model is purely lexicalized, and
therefore can be integrated into any SMT de-

coders as a feature. For a hierarchical SMT
decoder, we adopt the integrating method pro-
posed by Devlin et al. (2014). As inherited
from the n-gram language model for perform-
ing hierarchical decoding, the leftmost and
rightmost n− 1 words from each constituent
should be stored in the state space. We ex-
tend the state space to also include the in-
dexes of the affiliated source words for each
of these edge words. For an aligned target
word, we take its aligned source words as its
affiliated source words. And for an unaligned
word, we use the affiliation heuristic adopted
by Devlin et al. (2014). In this paper, we in-
tegrate the joint model into the state-of-the-art
dependency-to-string machine translation de-
coder as a case study to test the efficacy of our
proposed approaches. We will briefly describe
the dependency-to-string translation model and
then the description of MT system.

4.1 Dependency-to-String Translation

In this paper, we use a state-of-the-art
dependency-to-string (Xie et al., 2011) decoder
(Dep2Str), which is also a hierarchical de-
coder. This dependency-to-string model em-
ploys rules that represent the source side as
head-dependents relations and the target side
as strings. A head-dependents relation (HDR)
is composed of a head and all its dependents
in dependency trees. Figure 3 shows a depen-
dency tree (a) with three HDRs (in shadow),

24



an example of HDR rule (b) for the top level
of (a), and an example of head rule (c). HDR
rules are constructed from head-dependents re-
lations. HDR rules can act as both translation
rules and reordering rules. And head rules are
used for translating source words.

We adopt the decoder proposed by Meng
et al. (2013) as a variant of Dep2Str trans-
lation that is easier to implement with com-
parable performance. Basically they extract
the HDR rules with GHKM (Galley et al.,
2004) algorithm. For the decoding procedure,
given a source dependency tree T , the de-
coder transverses T in post-order. The bottom-
up chart-based decoding algorithm with cube
pruning (Chiang, 2007; Huang and Chiang,
2007) is used to find the k-best items for each
node.

4.2 MT Decoder

Following Och and Ney (2002), we use a gen-
eral loglinear framework. Let d be a derivation
that convert a source dependency tree into a tar-
get string e. The probability of d is defined as:

P (d) ∝
∏
i

φi(d)λi (2)

where φi are features defined on derivations
and λi are the corresponding weights. Our de-
coder contains the following features:
Baseline Features:

• translation probabilities P (t|s) and
P (s|t) of HDR rules;
• lexical translation probabilities PLEX(t|s)

and PLEX(s|t) of HDR rules;
• rule penalty exp(−1);
• pseudo translation rule penalty exp(−1);
• target word penalty exp(|e|);
• n-gram language model PLM(e);

Proposed Features:

• n-gram tagCNN joint language model
PTLM(e);

• n-gram inCNN joint language model
PILM(e).

Our baseline decoder contains the first eight
features. The pseudo translation rule (con-
structed according to the word order of a HDR)
is to ensure the complete translation when no
matched rules is found during decoding. The
weights of all these features are tuned via
minimum error rate training (MERT) (Och,
2003). For the dependency-to-string decoder,
we set rule-threshold and stack-threshold to
10−3, rule-limit to 100, stack-limit to 200.

5 Experiments

The experiments in this Section are designed to
answer the following questions:

1. Are our tagCNN and inCNN joint lan-
guage models able to improve translation
quality, and are they complementary to
each other?

2. Do inCNN and tagCNN benefit from
their guiding signal, compared to a
generic CNN?

3. For tagCNN, is it helpful to embed more
dependency structure, e.g., dependency
head of each affiliated word, as additional
information?

4. Can our gating strategy improve the per-
formance over max-pooling?

5.1 Setup
Data: Our training data are extracted from
LDC data2. We only keep the sentence pairs
that the length of source part no longer than
40 words, which covers over 90% of the sen-
tence. The bilingual training data consist of
221K sentence pairs, containing 5.0 million
Chinese words and 6.8 million English words.
The development set is NIST MT03 (795 sen-
tences) and test sets are MT04 (1499 sen-
tences) and MT05 (917 sentences) after filter-
ing with length limit.

Preprocessing: The word alignments are ob-
tained with GIZA++ (Och and Ney, 2003) on
the corpora in both directions, using the “grow-
diag-final-and” balance strategy (Koehn et al.,
2003). We adopt SRI Language Modeling

2The corpora include LDC2002E18, LDC2003E07,
LDC2003E14, LDC2004T07, LDC2005T06.

25



Systems MT04 MT05 Average
Moses 34.33 31.75 33.04
Dep2Str 34.89 32.24 33.57

+ BBN-JM (Devlin et al., 2014) 36.11 32.86 34.49
+ CNN (generic) 36.12* 33.07* 34.60
+ tagCNN 36.33* 33.37* 34.85
+ inCNN 36.92* 33.72* 35.32
+ tagCNN + inCNN 36.94* 34.20* 35.57

Table 1: BLEU-4 scores (%) on NIST MT04-test and MT05-test, of Moses (default settings),
dependency-to-string baseline system (Dep2Str), and different features on top of Dep2Str: neural
network joint model (BBN-JM), generic CNN, tagCNN, inCNN and the combination of tagCNN
and inCNN. The boldface numbers and superscript ∗ indicate that the results are significantly
better (p<0.01) than those of the BBN-JM and the Dep2Str baseline respectively. “+” stands for
adding the corresponding feature to Dep2Str.

Toolkit (Stolcke and others, 2002) to train a
4-gram language model with modified Kneser-
Ney smoothing on the Xinhua portion of the
English Gigaword corpus (306 million words).
We parse the Chinese sentences with Stanford
Parser into projective dependency trees.

Optimization of NN: In training the neural
network, we limit the source and target vocab-
ulary to the most frequent 20K words for both
Chinese and English, covering approximately
97% and 99% of two corpus respectively. All
the out-of-vocabulary words are mapped to a
special token UNK. We used stochastic gradient
descent to train the joint model, setting the size
of minibatch to 500. All joint models used a 3-
word target history (i.e., 4-gram LM). The di-
mension of word embedding and the attention
signal h({e}n−1n−k) for inCNN are 100. For the
convolution layers (Layer 1 and Layer 3), we
apply 100 filters. And the final representation
of CNN encoders is a vector with dimension
100. The final DNN layer of our joint model is
the standard multi-layer perceptron with soft-
max at the top layer.

Metric: We use the case-insensitive 4-
gram NIST BLEU3 as our evaluation met-
ric, with statistical significance test with sign-
test (Collins et al., 2005) between the proposed
models and two baselines.

3ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v11b.pl

5.2 Setting for Model Comparisons
We use the tagCNN and inCNN joint lan-
guage models as additional decoding fea-
tures to a dependency-to-string baseline sys-
tem (Dep2Str), and compare them to the neu-
ral network joint model with 11 source con-
text words (Devlin et al., 2014). We use
the implementation of an open source toolkit4

with default configuration except the global
settings described in Section 5.1. Since our
tagCNN and inCNN models are source-to-
target and left-to-right (on target side), we only
take the source-to-target and left-to-right type
NNJM in (Devlin et al., 2014) in compari-
son. We call this type NNJM as BBN-JM here-
after. Although the BBN-JM in (Devlin et al.,
2014) is originally tested in the hierarchical
phrase-based (Chiang, 2007) SMT and string-
to-dependency (Shen et al., 2008) SMT, it is
fairly versatile and can be readily integrated
into Dep2Str.

5.3 The Main Results
The main results of different models are given
in Table 1. Before proceeding to more detailed
comparison, we first observe that

• the baseline Dep2Str system gives BLEU
0.5+ higher than the open-source phrase-
based system Moses (Koehn et al., 2007);

• BBN-JM can give about +0.92 BLEU
score over Dep2Str, a result similar as re-
ported in (Devlin et al., 2014).

4http://nlg.isi.edu/software/nplm/

26



Systems MT04 MT05 Average
Dep2str 34.89 32.24 33.57
+tagCNN 36.33 33.37 34.85
+tagCNN dep 36.54 33.61 35.08

Table 2: BLEU-4 scores (%) of tagCNN
model with dependency head words as addi-
tional tags (tagCNN dep).

Clearly from Table 1, tagCNN and inCNN
improve upon the Dep2Str baseline by +1.28
and +1.75 BLEU, outperforming BBN-JM in
the same setting by respectively +0.36 and
+0.83 BLEU, averaged on NIST MT04 and
MT05. These indicate that tagCNN and
inCNN can individually provide discrimina-
tive information in decoding. It is worth not-
ing that inCNN appears to be more informative
than the affiliated words suggested by the word
alignment (GIZA++). We conjecture that this
is due to the following two facts

• inCNN avoids the propagation of mis-
takes and artifacts in the already learned
word alignment;

• the guiding signal in inCNN provides
complementary information to evaluate
the translation.

Moreover, when tagCNN and inCNN are both
used in decoding, it can further increase its
winning margin over BBN-JM to +1.08 BLEU
points (in the last row of Table 1), indicating
that the two models with different guiding sig-
nals are complementary to each other.

The Role of Guiding Signal It is slight sur-
prising that the generic CNN can also achieve
the gain on BLEU similar to that of BBN-
JM, since intuitively generic CNN encodes the
entire sentence and the representations should
in general far from optimal representation for
joint language model. The reason, as we con-
jecture, is CNN yields fairly informative sum-
marization of the sentence (thanks to its so-
phisticated convolution and gating architec-
ture), which makes up some of its loss on
resolution and relevant parts of the source
senescence. That said, the guiding signal in
both tagCNN and inCNN are crucial to the

Systems MT04 MT05 Average
Dep2Str 34.89 32.24 33.57
+inCNN 36.92 33.72 35.32
+inCNN-2-pooling 36.33 32.88 34.61
+inCNN-4-pooling 36.46 33.01 34.74
+inCNN-8-pooling 36.57 33.39 34.98

Table 3: BLEU-4 scores (%) of inCNN mod-
els implemented with gating strategy and k
max-pooling, where k is of {2, 4, 8}.

power of CNN-based encoder, as can be eas-
ily seen from the difference between the BLEU
scores achieved by generic CNN, tagCNN, and
inCNN. Indeed, with the signal from the al-
ready learned word alignment, tagCNN can
gain +0.25 BLEU over its generic counterpart,
while for inCNN with the guiding signal from
the proceeding words in target, the gain is more
saliently +0.72 BLEU.

5.4 Dependency Head in tagCNN

In this section, we study whether tagCNN can
further benefit from encoding richer depen-
dency structure in source language in the input.
More specifically, the dependency head words
can be used to further improve tagCNN model.
As described in Section 3.2, in tagCNN, we
append a tagging bit (0 or 1) to the embedding
of words in the input layer as tags on whether
they are affiliated source words. To incorpo-
rate dependency head information, we extend
the tagging rule in Section 3.2 to add another
tagging bit (0 or 1) to the word-embedding for
original tagCNN to indicate whether it is part
of dependency heads of the affiliated words.
For example, if xi is the embedding of an af-
filiated source word and xj the dependency
head of word xi, the extended input of tagCNN
would contain

x(AFF, NON-HEAD)i = [x
>
i 1 0]

>

x(NON-AFF, HEAD)j = [x
>
j 0 1]

>

If the affiliated source word is the root of a
sentence, we only append 0 as the second tag-
ging bit since the root has no dependency head.
From Table 2, with the help of dependency
head information, we can improve tagCNN by
+0.23 BLEU points averagely on two test sets.

27



5.5 Gating Vs. Max-pooling

In this section, we investigate to what extent
that our gating strategy can improve the trans-
lation performance over max pooling, with the
comparisons on inCNN model as a case study.
For implementation of inCNN with max-
pooling, we replace the local-gating (Layer-2)
with max-pooling with size 2 (2-pooling for
short), and global gating (Layer-4) with k max-
pooling (“k-pooling”), where k is of {2, 4, 8}.
Then, we use the mean of the outputs of k-
pooling as the final input of Layer-5. In do-
ing so, we can guarantee the input dimension
of Layer-5 is the same as the architecture with
gating. From Table 3, we can clearly see
that our gating strategy can improve translation
performance over max-pooling by 0.34∼0.71
BLEU points. Moreover, we find 8-pooling
yields performance better than 2-pooling. We
conjecture that this is because the useful rel-
evant parts for translation are mainly concen-
trated on a few words of the source sentence,
which can be better extracted with a larger pool
size.

6 Related Work

The seminal work of neural network language
model (NNLM) can be traced to Bengio et al.
(2003) on monolingual text. It is recently ex-
tended by Devlin et al. (2014) to include ad-
ditional source context (11 source words) in
modeling the target sentence, which is clearly
most related to our work, with however two im-
portant differences: 1) instead of the ad hoc
way of selecting a context window in (Devlin
et al., 2014), our model covers the entire source
sentence and automatically distill the context
relevant for target modeling; 2) our convo-
lutional architecture can effectively leverage
guiding signals of vastly different forms and
nature from the target.

Prior to our model there is also work on
representing source sentences with neural net-
works, including RNN (Cho et al., 2014;
Sutskever et al., 2014) and CNN (Kalchbren-
ner and Blunsom, 2013). These work typi-
cally aim to map the entire sentence to a vec-
tor, which will be used later by RNN/LSTM-
based decoder to generate the target sentence.
As demonstrated in Section 5, the representa-

tion learnt this way cannot pinpoint the rele-
vant parts of the source sentences (e.g., words
or phrases level) and therefore is inferior to
be directly integrated into traditional SMT de-
coders.

Our model, especially inCNN, is inspired
by is the automatic alignment model proposed
in (Bahdanau et al., 2014). As the first effort
to apply attention model to machine transla-
tion, it sends the state of a decoding RNN as
attentional signal to the source end to obtain a
weighted sum of embedding of source words
as the summary of relevant context. In con-
trast, inCNN uses 1) a different attention sig-
nal extracted from proceeding words in partial
translations, and 2) more importantly, a con-
volutional architecture and therefore a highly
nonlinear way to retrieve and summarize the
relevant information in source.

7 Conclusion and Future Work

We proposed convolutional architectures for
obtaining a guided representation of the entire
source sentence, which can be used to augment
the n-gram target language model. With differ-
ent guiding signals from target side, we devise
tagCNN and inCNN, both of which are tested
in enhancing a dependency-to-string SMT with
+2.0 BLEU points over baseline and +1.08
BLEU points over the state-of-the-art in (De-
vlin et al., 2014). For future work, we will con-
sider encoding more complex linguistic struc-
tures to further enhance the joint model.

Acknowledgments

Meng, Wang, Jiang and Liu are supported
by National Natural Science Foundation of
China (Contract 61202216). Liu is partially
supported by the Science Foundation Ireland
(Grant 12/CE/I2267 and 13/RC/2106) as part
of the ADAPT Centre at Dublin City Univer-
sity. We sincerely thank the anonymous re-
viewers for their thorough reviewing and valu-
able suggestions.

References
[Auli et al.2013] Michael Auli, Michel Galley,

Chris Quirk, and Geoffrey Zweig. 2013. Joint
language and translation modeling with recur-
rent neural networks. In Proceedings of the

28



2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1044–1054,
Seattle, Washington, USA, October.

[Bahdanau et al.2014] Dzmitry Bahdanau,
Kyunghyun Cho, and Yoshua Bengio. 2014.
Neural machine translation by jointly learn-
ing to align and translate. arXiv preprint
arXiv:1409.0473.

[Bengio et al.2003] Yoshua Bengio, Rjean
Ducharme, Pascal Vincent, and Christian
Jauvin. 2003. A neural probabilistic lan-
guage model. Journal OF Machine Learning
Research, 3:1137–1155.

[Chiang2007] David Chiang. 2007. Hierarchical
phrase-based translation. Computational Lin-
guistics, 33(2):201–228.

[Cho et al.2014] Kyunghyun Cho, Bart van Mer-
rienboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representa-
tions using rnn encoder–decoder for statistical
machine translation. In Proceedings of the 2014
Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar, October.

[Collins et al.2005] Michael Collins, Philipp
Koehn, and Ivona Kučerová. 2005. Clause
restructuring for statistical machine translation.
In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics,
pages 531–540.

[Devlin et al.2014] Jacob Devlin, Rabih Zbib,
Zhongqiang Huang, Thomas Lamar, Richard
Schwartz, and John Makhoul. 2014. Fast and
robust neural network joint models for statistical
machine translation. In Proceedings of the 52nd
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1370–1380, Baltimore, Maryland, June.

[Galley et al.2004] Michel Galley, Mark Hopkins,
Kevin Knight, and Daniel Marcu. 2004.
What’s in a translation rule. In Proceedings of
HLT/NAACL, volume 4, pages 273–280. Boston.

[Hu et al.2014] Baotian Hu, Zhengdong Lu, Hang
Li, and Qingcai Chen. 2014. Convolutional
neural network architectures for matching natu-
ral language sentences. In NIPS.

[Huang and Chiang2007] Liang Huang and David
Chiang. 2007. Forest rescoring: Faster de-
coding with integrated language models. In
Annual Meeting-Association For Computational
Linguistics, volume 45, pages 144–151.

[Kalchbrenner and Blunsom2013] Nal Kalchbren-
ner and Phil Blunsom. 2013. Recurrent contin-
uous translation models. In Proceedings of the

2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700–1709,
Seattle, Washington, USA, October.

[Kalchbrenner et al.2014] Nal Kalchbrenner, Ed-
ward Grefenstette, and Phil Blunsom. 2014. A
convolutional neural network for modelling sen-
tences. ACL.

[Klein and Manning2002] Dan Klein and Christo-
pher D Manning. 2002. Fast exact inference
with a factored model for natural language pars-
ing. In Advances in neural information process-
ing systems, volume 15, pages 3–10.

[Koehn et al.2003] Philipp Koehn, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-
based translation. In Proceedings of the 2003
Conference of the North American Chapter
of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1,
pages 48–54.

[Koehn et al.2007] Philipp Koehn, Hieu Hoang,
Alexandra Birch, Chris Callison-Burch, Mar-
cello Federico, Nicola Bertoldi, Brooke Cowan,
Wade Shen, Christine Moran, Richard Zens,
Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics Compan-
ion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180, Prague, Czech Repub-
lic, June.

[LeCun et al.1998] Y. LeCun, L. Bottou, G. Orr, and
K. Muller. 1998. Efficient backprop. In Neural
Networks: Tricks of the trade. Springer.

[Meng et al.2013] Fandong Meng, Jun Xie, Linfeng
Song, Yajuan Lü, and Qun Liu. 2013. Trans-
lation with source constituency and dependency
trees. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1066–1076, Seattle, Washington,
USA, October.

[Och and Ney2002] Franz Josef Och and Hermann
Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine
translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Lin-
guistics, pages 295–302.

[Och and Ney2003] Franz Josef Och and Hermann
Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational
linguistics, 29(1):19–51.

[Och2003] Franz Josef Och. 2003. Minimum error
rate training in statistical machine translation. In
Proceedings of the 41st Annual Meeting on As-
sociation for Computational Linguistics-Volume
1, pages 160–167.

29



[Shen et al.2008] Libin Shen, Jinxi Xu, and Ralph
Weischedel. 2008. A new string-to-dependency
machine translation algorithm with a target de-
pendency language model. In Proceedings of
ACL-08: HLT, pages 577–585.

[Stolcke and others2002] Andreas Stolcke et al.
2002. Srilm-an extensible language modeling
toolkit. In Proceedings of the international
conference on spoken language processing, vol-
ume 2, pages 901–904.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
and Quoc V. Le. 2014. Sequence to se-
quence learning with neural networks. CoRR,
abs/1409.3215.

[Xie et al.2011] Jun Xie, Haitao Mi, and Qun Liu.
2011. A novel dependency-to-string model for
statistical machine translation. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 216–226.

30


