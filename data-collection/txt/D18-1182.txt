















































Spot the Odd Man Out: Exploring the Associative Power of Lexical Resources


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1533–1542
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1533

Spot the Odd Man Out:
Exploring the Associative Power of Lexical Resources

Gabriel Stanovsky1,2 and Mark Hopkins3

1Allen Institute for Artificial Intelligence, Seattle, WA
2Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA

3Department of Mathematics, Reed College, Portland, OR
gabis@allenai.org, hopkinsm@reed.edu

Abstract

We propose Odd-Man-Out, a novel task which
aims to test different properties of word repre-
sentations. An Odd-Man-Out puzzle is com-
posed of 5 (or more) words, and requires the
system to choose the one which does not be-
long with the others. We show that this simple
setup is capable of teasing out various proper-
ties of different popular lexical resources (like
WordNet and pre-trained word embeddings),
while being intuitive enough to annotate on a
large scale. In addition, we propose a novel
technique for training multi-prototype word
representations, based on unsupervised clus-
tering of ELMo embeddings, and show that it
surpasses all other representations on all Odd-
Man-Out collections.

1 Introduction

Correctly disambiguating the sense of a polyse-
mous word (e.g., “spring is a beautiful season”
versus “John was ready to spring into action”) is
a crucial part of various NLP tasks, such as trans-
lation, question answering, or textual entailment.
The state-of-the-art, and the de-facto common
practice for essentially all of these tasks, involves
neural networks (see (Goldberg, 2015) for a recent
survey), which are commonly initialized with pre-
trained word vectors, such as Word2Vec (Mikolov
et al., 2013a) or GloVe (Pennington et al., 2014).

These widely-used representations often signif-
icantly improve performance in downstream tasks,
as they are able to leverage large amounts of un-
structured data. However, most of the popular
collections of word embeddings assign only one
vector to each word, thus shifting the burden of
word disambiguation to deeper, task-specific lay-
ers, which commonly rely on data of much smaller
scales.

While there has been a significant body of work
around sense embeddings (i.e., embedding senses,

instead of lexical units), evaluating such repre-
sentations remains the subject of debate (Faruqui
et al., 2016; Gladkova and Drozd, 2016).

In this work, we propose a new evaluation task
called Odd-Man-Out. The goal of an Odd-Man-
Out puzzle is simple. Given a set of words1 like
cherry, orange, apple, grass, grape, the objec-
tive is to identify the word that does not belong
(here, the answer is grass, because it is not a
fruit). While there are often multiple relationships
among the words, we will show that non-experts
typically agree on the odd-man-out, and are able
to generate hard puzzles on a large scale, using a
novel crowdsourcing protocol.

Following the creation of this large test set,
we conduct a thorough analysis of the ability of
various lexical resources to correctly solve the
task. In doing so, we embrace the suggestion of
Gladkova and Drozd (2016), which argue for “a
shift from absolute ratings of word embeddings
towards more exploratory evaluations that would
aim not for generic scores, but for identification
of strengths and weaknesses of embeddings”, and
conduct rigorous analysis of each representation.
Overall, we find that all lexical resources are prone
to miss associations which are intuitive for hu-
mans, leaving ample room for future improve-
ment. Moreover, we show empirical evidence that
lexical resources that do not account for polysemy
are handicapped by this weakness.

Finally, we propose a new sense embedding
technique, which leverages the recent introduction
of ELMo embeddings (Peters et al., 2018) by per-
forming unsupervised clustering over a large un-
structured corpus. We show that this new resource
surpasses all other baselines on various Odd-Man-
Out datasets.

1In this paper, we use the term “word” loosely to also
include the multi-word expressions like “fire engine” and
“magnifying glass.”



1534

We make all our code and models publicly
available.2

2 Existing Evaluation Methods

In this section, we briefly survey some existing
evaluation methods for lexical resources, and dis-
cuss their pros and cons.

2.1 Word Similarity

The word similarity task (Rubenstein and Goode-
nough, 1965) has been a dominant approach to as-
sess word vector quality. In this task, systems are
required to score the similarity of two words on
a numeric scale, sometimes without context and
sometimes in a sentential context (Huang et al.,
2012). For instance, the pair (tiger, mammal) has
a similarity score of 6.85 (out of 10) on the Word-
Sim dataset (Finkelstein et al., 2001).

A common criticism (Faruqui et al., 2016; Glad-
kova and Drozd, 2016) of the word similarity task
is that “similarity” is subjective, and conflates
several different potential relationships between
words. For instance, (Faruqui et al., 2016) ques-
tions why the pair (cup, coffee) should be consid-
ered more similar than (car, train), as it is accord-
ing to the WordSim dataset.

Odd-man-out puzzles naturally disambiguate
the nebulous concept of “similarity”, because each
puzzle implicitly defines a specific relationship.
The words “car” and “train” may be similar, but
this similarity is irrelevant is the context of a puz-
zle like car, train, checkered flag, racetrack, pit
stop.

2.2 Analogies

Analogies like “king is to queen as man is to...”
(Mikolov et al., 2013b; Jurgens et al., 2012) are
related to the odd-man-out task. Analogies, how-
ever, are best suited to particular relationships,
such as hypernym→ hyponym (which are the sub-
ject of extensive research, e.g., (Shwartz et al.,
2016)), while odd-man-out puzzles can capture
a broader range of associations (see for instance,
the auto-racing puzzle from the previous section).
Analogies are also more subject to ambiguity,
since the premise can involve only two words. For
instance, the puzzle “cherry is to strawberry as
grass is to...” could refer to the fact that cherry and
strawberry are both fruits, both red, or both red

2https://github.com/gabrielStanovsky/
odd-man-out

fruits. Odd-man-out puzzles provide a simple way
of reducing ambiguity: adding more choices to the
puzzle.

2.3 Word Sense Disambiguation and
Induction

Word sense disambiguation (Navigli, 2009) is a
popular way to evaluate polysemous word rep-
resentations. The common criticism is that sys-
tems are rewarded based on their ability to classify
words according to a fixed inventory of senses,
whose granularity is regarded by some as too
coarse and others as too fine. An alternative is
word sense induction (Manandhar et al., 2010),
which allows systems to cluster word senses with-
out an agreed-upon sense inventory. However,
there is not an obvious evaluation metric. The two
metrics used in SemEval 2010 Task 14 (Manand-
har et al., 2010) yielded highly divergent system
rankings.

2.4 Word Context Relevance

A recent evaluation method is Word Context Rel-
evance (Arora et al., 2016; Sun et al., 2017a).
The task is to identify whether a particular bag of
words is “relevant” to a target word. For instance,
“tie” is considered relevant to the bag “winner,
score, tied, completion, identical, results, sports,”
but irrelevant to the bag “domestic, hog, pig, culi-
nary, eaten, cooked, fat”. This task has the attrac-
tiveness of being a simple binary evaluation, but
demands only that a model can identify a broad
sense of relatedness, not the ability to pinpoint
specific relationships.

2.5 Lexical Substitution

Lexical substitution tasks (McCarthy and Navigli,
2007; Biemann, 2013; Kremer et al., 2014), in
which the task is to determine whether one word
can replace another word in a particular context,
are effective, but are restricted in the kind of rela-
tionships they can test (mainly synonymy).

3 Odd Man Out Datasets

In this section, we describe the creation of several
Odd-Man-Out datasets. We begin by describing
a small-scale, curated annotation, then show how
to scale the annotation using crowdsourcing tech-
niques. In the subsequent sections, we use these
datasets to explore the properties of a wide array
of lexical resources.



1535

3.1 Expert annotation

To create our first odd-man-out datasets, we used
categories from the card game Anomia.3 Anomia
is a slapjack-style game in which players attempt
to name instances of a particular category as
quickly as possible. Categories include: “percus-
sion instrument”, “Mexican food”, and “Michael
Jackson song”. A team of 4 people, trained in-
house, created one odd-man-out puzzle for each
category, yielding a total of 404 puzzles. For in-
stance, the puzzle corresponding to “percussion
instrument” is clarinet, drum, xylophone, tam-
bourine, cymbals, where clarinet is the odd-man-
out. The puzzle-writing team attempted to make
the odd-man-out similar to the category, e.g. clar-
inet is a musical instrument but not a percussion
instrument.

We divided the puzzles into 2 sets, one for
puzzles comprised of common words (like the
“percussion instruments” example above), and
one for puzzles comprised of proper nouns
(like the puzzle derived from the category
“Michael Jackson songs”). Each set contained
exactly 202 puzzles. We further divided each
of these sets into a development set (used
for error analysis) of size 100 and a test set
of size 102. Henceforth, we will refer to
these four datasets as ANOMIACOMMONDEV,
ANOMIACOMMONTEST,ANOMIAPROPERDEV,
andANOMIAPROPERTEST.

There are two potential pitfalls in creating these
puzzles. First, the underlying category may not
be detectable by humans. Second, we may inad-
vertently create an ambiguous puzzle with multi-
ple odd-men-out. For instance, given the category
“vegetables”, we might create the puzzle grass,
celery, cucumber, carrot, lettuce, for which the
intended odd-man-out is grass, but the answer
carrot is also a reasonable odd-man-out for the
category “things that are green.”

To ensure the answerability of our puzzles, we
administered the ANOMIACOMMONDEV to five
college-educated individuals, three of whom were
native English speakers. Both groups did well.
The native speakers averaged 95.3% accuracy,
while the non-native speakers averaged 91.5%.

3https://boardgamegeek.com/boardgame/142271/anomia-
party-edition

3.2 Crowdsourcing

Following the success of the curated annotation, as
described above, we devised a crowdsourcing pro-
tocol to achieve annotation at a much larger scale.
In this section, we describe this protocol and show
that the Odd-Man-Out task is intuitive enough to
be collected and annotated with high agreement by
non-trained annotators, using a small expert seed
annotation. This enables us to efficiently obtain
a large set of 500K hard “training” samples on a
small budget of $400. Finally, we also validate
a set of 1000 puzzles, which we use in following
sections for testing purposes.

Crowdsourcing protocol Our semi-automatic
crowdsourcing protocol starts from a seed set of
about 250 polysemous words,4 and is composed
of the following three consecutive stages:

1. Seed categorization (expert): given a poly-
semous word w, we ask expert annotators to
come up with at least two categories c1, c2,
which describe w. For example, given w =
“bat”, the corresponding categories can be
c1 = “nocturnal mammal” and c2 = “baseball
instrument”. This categorization was per-
formed by the authors of this paper, and was
done in about 3 person hours.

2. Category expansion (crowdsourced):
Given a seed word and a corresponding
category (w, c) turkers are asked to provide
five more examples of the category c , which
are similar to, but different than, w. For
example, given (“bat”, “nocturnal mammal”)
turkers are expected to provide examples
such as “beaver”, “badger”, or “hedgehog”,
while for (“bat”, “baseball instrument”),
proper answers include “ball” or “cap”. We
used the Amazon Mechanical Turk (AMT)5

platform, paying 15¢ per elicitation.

3. Puzzle creation (automatic): For each poly-
semous seed word w, belonging to categories
c1, c2, we automatically create Odd-Man-Out
puzzles by concatenating to w each possi-
ble combination of three words from c1 (c2),
while the intended odd-man-out is any word
from c2 (c1). For example, using the previous

4https://en.wikipedia.org/wiki/List_
of_true_homonyms, following (Sun et al., 2017b).

5https://www.mturk.com



1536

seed word and categories, we get the follow-
ing puzzle: (“cap”, “bat”, “beaver”, “bad-
ger”, “hedgehog”). Creating puzzles by com-
bining words from c1 and c2 in this manner
ensures that task is challenging, as it requires
to disambiguate the polysemous word (e.g.,
“bat”).

Performing the process described above with the
249 polysemous seed words yielded a large cor-
pus of 497,365 puzzles, thanks to the combina-
tion process, with a vocabulary of 1,312 differ-
ent words. To create a gold high quality test
corpus, and to assess the validity of this semi-
automatic process, we sampled 1000 puzzles and
administered each puzzle to three annotators on
AMT, paying 6¢ per response. We found that
for 84.3% of the instances there was a major-
ity vote agreement on the intended odd-man-out
word. We refer to this set of 843 puzzles as
CROWDSOURCED843. Table 1 shows examples
of the crowdsourced puzzles.

From an examination of turkers disagreement,
we can attribute the vast majority to noise in one of
the annotation stages, e.g., turkers which provide
examples of the wrong sense of the seed word, re-
sulting in invalid puzzles (where all of the words
belong to the same category), or turkers marking
the wrong odd-man-out, thus invalidating an oth-
erwise correct puzzle.

Overall, the cost of the annotation and valida-
tion was below $500, for a large high-quality an-
notated resource and a smaller gold standard test
corpus. Both corpora are made available.6

4 Taxonomy-Based Solvers

In this section, we show how to create odd-man-
out solvers for taxonomies like WordNet (Miller,
1992).

Define a taxonomy as a triple (V,E,L), where
(V,E) is a directed acyclic graph, and L maps
each vertex V to a string. A simple example is
shown in Figure 1, where the each vertex v is la-
beled with L(v).

We create an odd-man-out solver from a taxon-
omy as follows:

• The specificity of a vertex v is defined as the
reciprocal of the number of its descendants.

6https://github.com/gabrielStanovsky/
odd-man-out

Category Puzzle
construction crane, pelican, excavator,

hoist, upraise
guitar part fret, crying, inlays,

truss rod, neck
alcoholic drinks gin, poker, vodka,

tequila, wine
card games gin, vodka, bridge

canasta, uno

Table 1: Selection of examples from the crowd-
sourced Odd-Man-Out dataset. The polysemous
seed word appears in bold, while the correct an-
swer is in italics. The last two examples demon-
strate how a polysemous word (gin) can partici-
pate in two different puzzles.

Figure 1: Example taxonomy.

For instance, the specificity of any leaf in Fig-
ure 1 is 1, while the specificity of the vertex
labeled “element” is 16 .

• Given an odd-man-out puzzle w1, ...,wn, the
explanation of word wk is the vertex v of
highest specificity such that: (i) for each
word wj such that j /= k, there exists some
descendent v′ of v where L(v′) = wj , (ii)
there does not exist a descendent v′ of v such
that L(v′) = wk. For instance, the explana-
tion of helium with respect to the puzzle he-
lium, mercury, lead, silver, gold is the node
labeled “metallic element.”

• If some word does not have an explanation,
or if there is no word whose explanation is
uniquely most specific, then the solver ab-
stains from answering. Otherwise, the solver
returns the word with the most specific expla-
nation.

Using WordNet 3.0 (Miller, 1992; Fellbaum,
1998) as the taxonomy, the solver correctly solves



1537

Puzzle Explanation
chicken, screwdriver, margarita, mixed drink

mimosa, daiquiri
silver, steel, brass, alloy

bronze, pewter
canoe, school, flock, animal group

herd, pack
nightgown, afternoon, morning, abstraction

evening, midnight
king, president, queen, leader

prince, princess
dinghy, crab, boat, travel (verb)

canoe, raft

Table 2: Some of the WordNet solver’s answers
and explanations on ANOMIACOMMONDEV. The
solver’s answer is in bold, while the correct answer
is in italics.

40.6% of the ANOMIACOMMON puzzles, answer-
ing incorrectly for 13.4% and abstaining for 46.0%
of the puzzles. Unsurprisingly (since WordNet
focuses on common words), the WordNet solver
gets only 1 out of 202 ANOMIAPROPER puz-
zles, abstaining from all the rest. On CROWD-
SOURCED843, the solver correctly solves 22.0%
of the puzzles, answering 15.1% incorrectly and
abstaining from the rest.

4.1 Error Analysis

A nice property of the taxonomy-based solver is
that it provides an explanation for its answer (i.e.
the “explanation” vertex defined above). In Ta-
ble 2, we show some of the WordNet solver’s an-
swers and explanations (specifically we show the
name of the WordNet synset corresponding to the
explanation vertex). For 85.4% of its correct an-
swers, the solver also returns the correct expla-
nation (like the “mixed drink,” “alloy,” and “ani-
mal group” explanations). In the handful of cases
when it does not, it finds either an incorrect or
an overly vague explanation (like the “abstraction”
explanation) that still results in the correct answer.

Typically, the incorrect answers result from in-
completeness in the WordNet taxonomy. For
instance, the word “king” is not a hyponym
of “leader,” even though “president,” “queen,”
“prince,” and “princess” all are. In the bottom-
most puzzle of Table 2, the error results from
“raft” not being considered a watercraft (or a con-

veyance of any kind) by WordNet. Because of
this, the solver finds a more tenuous connection
between “crab” and three of the other words, lever-
aging a rare sense of “crab” as a verb meaning “to
move like a crab.”

5 Embedding-Based Solvers

In this section, we create odd-man-out solvers
from collections of word embeddings. Specifi-
cally, we experiment with two types of embed-
dings: (1) traditional word embeddings, which
map words to a single vector representation (Sub-
section 5.2), and (2) sense embeddings, which
map words to a set of vectors, each pertaining to a
different sense of the word (Subsection 5.3).

5.1 Embedding Evaluation Framework
For the sake of comparison, we use a common
framework to create odd-man-out solvers based
on traditional word embeddings and sense embed-
dings. In this framework, given n puzzle options,
we find the subset of n−1 words of maximal simi-
larity (given some similarity score), and return the
excluded word as the odd-man-out.

Specifically, define an embedding as a function
that maps every word to a set (possibly empty)
of real vectors. Note that this definition is gen-
eral enough to allow for sense embeddings as well
as traditional word embeddings, for which the re-
turned sets are either singletons or the empty set
(in case the word is not in the embedding’s vocab-
ulary).

Next, given a similarity score σ that maps any
vector pair to a real number and an embedding E,
define the cohesion κσ,E of a set of words W as:

κσ,E(W ) = max
v1∈E(w1),...,vn∈E(wn)

∑
1≤i<j≤n

σ(vi, vj)

(1)
Cohesion is undefined if any word w ∈W maps to
the empty set. Throughout this paper we employ
the widely used cosine similarity as our similiarity
score.

Finally, given an embedding E and puzzle W =
⟨w1, ...,wn⟩, we create an odd-man-out solver as
follows:

1. If E(wi) is the empty set for any puzzle
choice wi ∈W , then the solver abstains from
answering.

2. Otherwise, the solver returns the word ŵ ∈
W whose omission from the puzzle word set



1538

Embedding Map Training AnomiaCommon AnomiaProper Crowdsourced
Tokens C W A C W A C W A

ELMo clusters (K = 5) 1B + 2B∗ 76.7 13.9 9.4 42.6 17.8 39.6 55.5 18.8 25.6
w2v.googlenews 100B 61.9 25.2 12.9 40.1 14.9 45.0 46.3 28.8 24.9

glove.commoncrawl2 840B 60.9 23.8 15.4 32.2 14.4 53.5 47.1 28.4 24.6
glove.commoncrawl1 42B 57.4 29.2 13.4 30.7 17.8 51.5 40.1 36.3 23.7

glove.wikipedia 6B 54.5 24.3 21.3 29.2 10.9 59.9 42.7 29.0 28.4
Neelakantan 1B 35.2 25.7 39.1 18.3 14.4 67.3 32.6 27.3 40.2
w2v.freebase 100B 22.3 28.7 49.0 34.2 14.9 51.0 9.9 11.3 78.3

WordNet - 40.6 13.4 46.0 0.5 0.0 99.5 22.0 15.1 63.0

Table 3: Performance (%Correct, %Wrong, %Abstained) of the different odd-man-out solvers on the
ANOMIATEST and crowdsourcing datasets, using different vector directories. ∗ We used ELMo clusters
pretrained on 1B tokens and clustered on a 2B token Wikipedia dump.

yields maximal cohesion:

ŵ = argmax
wi∈W

κσ(W ∖ {wi}) (2)

5.2 Word Embeddings Solvers
Table 3 shows the results of embedding-based
solvers on the Anomia and crowdsourced datasets,
using several different pre-trained embedding
maps. We find the best performance on
ANOMIACOMMON and ANOMIAPROPER using
the word2vec vectors trained on 100 billion tokens
from the Google News corpus.7

An analysis of the best embedding-based
solver (w2v.googlenews) on ANOMIACOMMON-
DEV shows that a high proportion of incorrect an-
swers were polysemous (20 out of the 27 incor-
rect answers). A selection of these are shown
in Table 4. We observe that for the “cocktails”
puzzle, the word2vec solver zeroes in on “screw-
driver” (whose dominant sense is the tool, not the
cocktail), and for the “types of lettuce” puzzle, the
solver selects “iceberg” (whose dominant sense is
“large floating block of ice,” not the lettuce).

To provide additional evidence of this bias, we
came up with 5 cocktails whose dominant sense is
the cocktail itself (mint julep, mai tai, mojito, mar-
tini, and bloody mary) and 5 polysemous cocktails
(old fashioned, hurricane, cosmopolitan, zombie,
and Manhattan). We then replaced “screwdriver”
in the “cocktails” puzzle with each of these op-
tions and solved the resulting puzzle with the
w2v.googlenews solver. In all 5 monosemous in-
stances, the correct answer of “chicken” was se-
lected. In 4 of the 5 polysemous instances (the

7https://code.google.com/archive/p/word2vec

exception being “Manhattan”), the polysemous re-
placement was selected. We repeated this exper-
iment with the “groups of animals” puzzle, and
again all 5 monosemous replacements yielded the
correct answer, while 3 of 5 polysemous replace-
ments were incorrectly chosen by the solver.

In a more rigorous experiment, we randomly
generated 500 puzzles in the following way. Given
a category (hyponym), we came up with 10 in-
stances (hypernyms) of that category. For ex-
ample, the category “type of transport” yielded:
train, car, bus, airplane, helicopter, boat, ferry,
taxi, tram, and monorail. We did this for 10 cate-
gories, yielding a total of 100 words. From these
10 lists, we randomly generated 500 puzzles by
sampling 4 words from one list and 1 word from
another. We call this puzzle set HYPERNYMS500.

The w2v.googlenews solver performs impres-
sively on HYPERNYMS500, getting 90.8% cor-
rect, with only 46 incorrect answers (and no ab-
stentions). Tellingly however, only 3 words are
responsible for 67% of the incorrect answers: saw
(as a kind of tool), lead (as a kind of metal), and
rose (as a kind of flower). All three of these
words have at least one dominant alternate sense.
Given that there are 100 possible incorrect answers
(which all appear with roughly equal frequency in
the puzzles), the fact that only 3 of them com-
prise 67% of the incorrect answers suggests that
the inability of vector directories to model multi-
ple senses is an Achilles heel.

5.3 ELMo Sense Vectors

The previous analysis suggests that embeddings
that explicitly model multiple senses may be im-
portant for the odd-man-out task. In this section,



1539

Category Puzzle
cocktails screwdriver, chicken, margarita

mimosa, daiquiri
mammals bear, cobra, cat

dog, leopard
nocturnal bat, robin, owl
animals raccoon, coyote
military private, judo, general

ranks corporal, colonel
whales blue, grizzly, humpback

orca, beluga
groups of school, canoe, flock
animals herd, pack

shades of navy, amber, cobalt
blue azure, sky

aquatic seal, horse, whale
animals manatee, dolphin

bird tweet, snore, chirp
sounds cluck, quack
types of iceberg, serrano, romaine
lettuce butterhead, bibb

political green, garden, democratic
parties republican, libertarian

Table 4: Selection of errors made by the
w2v.googlenews solver on ANOMIACOMMON-
DEV, indicating a tendency to choose polysemous
words as the odd-man-out. The incorrect selection
is in bold, while the correct answer is in italics.

we investigate this further.

Background: sense vectors There is a signif-
icant literature on how to learn a one-to-many
mapping from words to vector representations.
An early paradigm (Reisinger and Mooney, 2010;
Huang et al., 2012; Liu et al., 2015; Wu and
Giles, 2015) took a 2-pass approach. First, they
clustered contexts of a target word like “bank.”
For instance, “the bank had an ATM” and “I got
money from the bank” might fall into one cluster,
while “I fished at the river bank” might fall into
a second cluster. Second, they annotated each in-
stance of the target word with its sense cluster and
then learned a standard one-to-one vector map-
ping from the annotated corpus. For instance, they
would learn vector representations using the sen-
tences “the bank-1 had an ATM,” “I got money
from the bank-1,” and “I fished at the river bank-
2.” Other researchers (Neelakantan et al., 2014;
Tian et al., 2014; Chen et al., 2014; Li and Juraf-

sky, 2015; Bartunov et al., 2016) focused on mod-
ifying the vector learning model itself, typically
the skip-gram model (Mikolov et al., 2013b), to
directly learn multiple embeddings for each word.
Additional work focused on using the technique
of retrofitting (Faruqui et al., 2014) to adapt pre-
trained word vectors into sense vectors using aux-
iliary resources like WordNet (Jauhar et al., 2015)
or parallel corpora (Ettinger et al., 2016). Other
work (Guo et al., 2014; Suster et al., 2016; Upad-
hyay et al., 2017) used parallel corpora as the main
signal for learning sense vectors.

Background: ELMo Recently, Peters et al.
(2018) introduced the concept of Embeddings
from Language Models (ELMo). ELMo dynam-
ically represents each word based on the con-
text with which it appears, achieved by repre-
senting a word in a sentence using its represen-
tation from a pretrained bidirectional Language
Model (biLM), encoded using a bi-directional
RNN (Schuster and Paliwal, 1997). Subsequently,
the same word may get different representations
in different contexts. For example, the represen-
tation of “bank” may differ between “She fished
by the river bank” and “She deposited her check
at the nearest bank”. While ELMo embeddings
were recently proven extremely beneficial in vari-
ous sentence-level tasks (e.g., semantic role label-
ing, question answering, and textual entailment),
many lexical tasks require the interpretation of
words out of context, to which ELMo cannot be
readily applied. We suggest to port ELMo’s effec-
tiveness back to the context-free setting, and pro-
pose a first approach for doing so.

Unsupervised ELMo clustering We compute
pre-trained ELMo embedding (using the Al-
lenNLP framework (Gardner et al., 2018)) in con-
text of sentences in a large corpusC, while record-
ing the observed representation rw,s ∈ Rd of each
word w, along with the corresponding context in
which it appeared, i.e., the sentence s ∈ C. The
result of this process is an embedding space per
word, rw = {rw,s ∣ s ∈ C,w ∈ s}. Ideally, simi-
lar senses of w would appear in similar contexts,
and would therefore be closer in rw, while differ-
ent senses would be further apart. Following this
intuition, we cluster each word to K clusters, us-
ing the k-means algorithm (Lloyd, 1982), whereby
every vector in a cluster is interpreted as pertain-
ing to the same sense ofw. We collect the centroid



1540

Figure 2: An example of a 2d projection of an
ELMo embedding space for the word “bat” with
4 senses (denoted by different colors). Two exam-
ple sentence excerpts appear next to their respec-
tive cluster.

vector wi (where 1 ≤ i ≤ K) of each cluster as a
“sense vector” of w. Using K = 1, we get a sin-
gle representation per word, averaging the differ-
ent senses of w, comparable to “traditional“ word
embeddings. Conversely, we can get senses of ar-
bitrarily fine granularity by increasing the number
of clusters. These sense representations can then
be readily plugged in to the embedding solver, as
described in Subsection 5.1. We computed the
sense vectors using a 2 billion token (97 Million
sentences) Wikipedia dump from January 2018,
which was extracted using WikiExtractor,8 and
tokenized with the SpaCy 2.0 toolkit (Honnibal
and Montani, 2017).9 These pre-computed vec-
tors, which are readily applicable for other tasks,
are made publicly available. Figure 2 depicts the
resulting embedding space and clustering for the
word “bat”.

Evaluation We start by estimating an ideal
value for K (the number of clusters). In Fig-
ure 3 we evaluate the performance of the ELMo
clustering method on the expert development set,
using different values for K, compared to the
second best performing vectors on that dataset
(Word2Vec). Several observations can be made
based on this analysis. First, using K = 1,
Word2Vec outperforms ELMo. This may be ex-
plained due to Word2Vec’s larger training set
(100B tokens versus only 1B). However, as K in-
creases, ELMo clusters outperform the baseline,

8https://github.com/attardi/
wikiextractor

9https://spacy.io

1 2 3 4 5 6 7 8 9 10

10

30

50

70

90

# Clusters

%
co

rr
ec

t

ELMo Clusters
Word2Vec

Figure 3: Performance of the ELMo cluster model
(blue line) by the varying number of cluster
(K), compared to the best performing baseline
(word2vec), in the red horizontal line.

reaching its maximum performance at K = 5, in-
dicating that a finer level of sense granularity is
beneficial for focusing on the intended word sense.
Inversely, having too many clusters hurts perfor-
mance. This may be due to over-specification,
harming the sense generalization obtained by a
smaller number of clusters. Based on this tun-
ing, we fixed the value of K to 5, and repeated
the experiments from Section 5 (see the first row
in Table 3). ELMo sense vectors clearly outper-
form all previous baselines on all Odd-Man-Out
datasets. This improved performance can be at-
tributed both to ELMo’s better ability to capture
context, as well as to the finer sense representa-
tion, as opposed to the single representation per
word in most of the other baselines. We also evalu-
ated another publicly available collection of sense
embeddings (Neelakantan et al., 2014), but it did
not perform on par with solvers based on conven-
tional single-sense embeddings.

6 Discussion: Hypernyms vs. Other
Associations

The attentive reader may have wondered why the
w2v.googlenews solver performed so much bet-
ter on the HYPERNYMS dataset (over 90% cor-
rect) than on the ANOMIACOMMON dataset (ap-
proximately 62% correct). Hypothesizing that
embedding-based solvers can identify hypernym-
hyponym relationships more easily than other
associations, we created another odd-man-out
dataset using the same methodology as HYPER-
NYMS.

This time, given a color, we came up with 8 to
13 objects that are typically associated with that
color. For example, the category “yellow” yielded



1541

(among others): taxi, canary, corn kernel, daffodil,
lemon, sun, and school bus. We did this for 10 col-
ors. From these 10 lists, we again randomly gen-
erated 500 puzzles by sampling 4 words from one
list and 1 word from another. We call this puzzle
set COLORS500.

The w2v.googlenews solver performed consid-
erably worse on COLORS500 than on HYPER-
NYMS500. Out of the puzzles it attempted, it
guessed only 30% correctly (compared to the ran-
dom chance baseline of 20%). There are several
possible reasons why this dataset may be more
difficult for an embedding-based solver. Possibly
colors are not easily identifiable using embeddings
built from language cues (maybe people do not of-
ten explicitly talk about how lemons are yellows).
But perhaps it is also true that using the cosine
similarity of the embeddings is not a good way to
spot non-fundamental properties (like color) that
link a group of words. We leave open how best to
identify more oblique relationships.

7 Conclusion

We presented a new task for the evaluation of lexi-
cal resources, the Odd-Man-Out task. We showed
that the task can be annotated reliably on a large
scale. Following the creation of several Odd-Man-
Out datasets, we conducted analyses showing that
current word representations are suboptimal, espe-
cially in the presence of polysemous words. We
concluded with a novel ELMo clustering sense-
embedding technique which surpasses all base-
lines on the Odd-Man-Out task.

Acknowledgements

We would like to thank Santosh Divvala,
Fereshteh Sadeghi, Marco Valenzuela, and Isaac
Cowhey, who initiated this project during an AI2
hackathon.

References
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,

and Andrej Risteski. 2016. Linear algebraic struc-
ture of word senses, with applications to polysemy.
CoRR, abs/1601.03764.

Sergey Bartunov, Dmitry Kondrashkin, Anton Osokin,
and Dmitry P. Vetrov. 2016. Breaking sticks and am-
biguities with adaptive skip-gram. In AISTATS.

Christian Biemann. 2013. Creating a system for lexi-
cal substitutions from scratch using crowdsourcing.
Language Resources and Evaluation, 47:97–122.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A unified model for word sense representation and
disambiguation. In EMNLP.

Allyson Ettinger, Philip Resnik, and Marine Carpuat.
2016. Retrofitting sense-specific word vectors using
parallel text. In HLT-NAACL.

Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris
Dyer, Eduard Hovy, and Noah A Smith. 2014.
Retrofitting word vectors to semantic lexicons.
arXiv preprint arXiv:1411.4166.

Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi,
and Chris Dyer. 2016. Problems with evaluation of
word embeddings using word similarity tasks. In
RepEval@ACL.

Christiane D. Fellbaum. 1998. Book reviews: Word-
net: An electronic lexical database.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: the con-
cept revisited. ACM Trans. Inf. Syst., 20:116–131.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew E.
Peters, Michael Schmitz, and Luke S. Zettlemoyer.
2018. Allennlp: A deep semantic natural language
processing platform. CoRR, abs/1803.07640.

Anna Gladkova and Aleksandr Drozd. 2016. Intrinsic
evaluations of word embeddings: What can we do
better? In RepEval@ACL.

Yoav Goldberg. 2015. A primer on neural network
models for natural language processing. arXiv
preprint arXiv:1510.00726.

Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Learning sense-specific word embed-
dings by exploiting bilingual resources. In COL-
ING.

Matthew Honnibal and Ines Montani. 2017. spacy 2:
Natural language understanding with bloom embed-
dings, convolutional neural networks and incremen-
tal parsing. To appear.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In ACL.

Sujay Kumar Jauhar, Chris Dyer, and Eduard H. Hovy.
2015. Ontologically grounded multi-sense represen-
tation learning for semantic vector space models. In
HLT-NAACL.

David Jurgens, Saif Mohammad, Peter D. Turney,
and Keith J. Holyoak. 2012. Semeval-2012 task
2: Measuring degrees of relational similarity. In
SemEval@NAACL-HLT.



1542

Gerhard Kremer, Katrin Erk, Sebastian Padó, and Ste-
fan Thater. 2014. What substitutes tell us - analy-
sis of an ”all-words” lexical substitution corpus. In
EACL.

Jiwei Li and Daniel Jurafsky. 2015. Do multi-sense
embeddings improve natural language understand-
ing? In EMNLP.

Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong
Sun. 2015. Topical word embeddings. In AAAI.

Stuart P. Lloyd. 1982. Least squares quantization in
pcm. IEEE Trans. Information Theory, 28:129–136.

Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer Pradhan. 2010. Semeval-2010
task 14: Word sense induction and disambiguation.
In SemEval@ACL.

Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
SemEval@ACL.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS.

George A. Miller. 1992. Wordnet: A lexical database
for english. Commun. ACM, 38:39–41.

Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Comput. Surv., 41:10:1–10:69.

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew D McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In EMNLP.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, volume 14, pages 1532–
1543.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. CoRR, abs/1802.05365.

Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In HLT-NAACL.

Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8:627–633.

Mike Schuster and Kuldip K. Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Trans. Sig-
nal Processing, 45:2673–2681.

Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016.
Improving hypernymy detection with an integrated
path-based and distributional method. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2389–2398, Berlin, Germany. Associa-
tion for Computational Linguistics.

Yifan Sun, Nikhil Rao, and Weicong Ding. 2017a. A
simple approach to learn polysemous word embed-
dings. CoRR, abs/1707.01793.

Yifan Sun, Nikhil Rao, and Weicong Ding. 2017b. A
simple approach to learn polysemous word embed-
dings. CoRR, abs/1707.01793.

Simon Suster, Ivan Titov, and Gertjan van Noord. 2016.
Bilingual learning of multi-sense embeddings with
discrete autoencoders. In HLT-NAACL.

Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In COLING.

Shyam Upadhyay, Kai-Wei Chang, Matt Taddy,
Adam Tauman Kalai, and James Y. Zou. 2017. Be-
yond bilingual: Multi-sense word embeddings using
multilingual context. In Rep4NLP@ACL.

Zhaohui Wu and C. Lee Giles. 2015. Sense-aware se-
mantic analysis: A multi-prototype word represen-
tation model using wikipedia.


