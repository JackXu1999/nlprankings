



















































Incorporating Copying Mechanism in Sequence-to-Sequence Learning


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1631‚Äì1640,
Berlin, Germany, August 7-12, 2016. c¬©2016 Association for Computational Linguistics

Incorporating Copying Mechanism in Sequence-to-Sequence Learning

Jiatao Gu‚Ä† Zhengdong Lu‚Ä° Hang Li‚Ä° Victor O.K. Li‚Ä†
‚Ä†Department of Electrical and Electronic Engineering, The University of Hong Kong

{jiataogu, vli}@eee.hku.hk
‚Ä°Huawei Noah‚Äôs Ark Lab, Hong Kong

{lu.zhengdong, hangli.hl}@huawei.com

Abstract

We address an important problem in
sequence-to-sequence (Seq2Seq) learning
referred to as copying, in which cer-
tain segments in the input sequence are
selectively replicated in the output se-
quence. A similar phenomenon is ob-
servable in human language communica-
tion. For example, humans tend to re-
peat entity names or even long phrases
in conversation. The challenge with re-
gard to copying in Seq2Seq is that new
machinery is needed to decide when to
perform the operation. In this paper, we
incorporate copying into neural network-
based Seq2Seq learning and propose a new
model called COPYNET with encoder-
decoder structure. COPYNET can nicely
integrate the regular way of word gener-
ation in the decoder with the new copy-
ing mechanism which can choose sub-
sequences in the input sequence and put
them at proper places in the output se-
quence. Our empirical study on both syn-
thetic data sets and real world data sets
demonstrates the efficacy of COPYNET.
For example, COPYNET can outperform
regular RNN-based model with remark-
able margins on text summarization tasks.

1 Introduction

Recently, neural network-based sequence-to-
sequence learning (Seq2Seq) has achieved re-
markable success in various natural language pro-
cessing (NLP) tasks, including but not limited to
Machine Translation (Cho et al., 2014; Bahdanau
et al., 2014), Syntactic Parsing (Vinyals et al.,
2015b), Text Summarization (Rush et al., 2015)
and Dialogue Systems (Vinyals and Le, 2015).

Seq2Seq is essentially an encoder-decoder model,
in which the encoder first transform the input se-
quence to a certain representation which can then
transform the representation into the output se-
quence. Adding the attention mechanism (Bah-
danau et al., 2014) to Seq2Seq, first proposed
for automatic alignment in machine translation,
has led to significant improvement on the perfor-
mance of various tasks (Shang et al., 2015; Rush et
al., 2015). Different from the canonical encoder-
decoder architecture, the attention-based Seq2Seq
model revisits the input sequence in its raw form
(array of word representations) and dynamically
fetches the relevant piece of information based
mostly on the feedback from the generation of the
output sequence.

In this paper, we explore another mechanism
important to the human language communication,
called the ‚Äúcopying mechanism‚Äù. Basically, it
refers to the mechanism that locates a certain seg-
ment of the input sentence and puts the segment
into the output sequence. For example, in the
following two dialogue turns we observe differ-
ent patterns in which some subsequences (colored
blue) in the response (R) are copied from the input
utterance (I):

I: Hello Jack, my name is Chandralekha.
R: Nice to meet you, Chandralekha.

I: This new guy doesn‚Äôt perform exactly
as we expected.

R: What do you mean by "doesn‚Äôt perform
exactly as we expected"?

Both the canonical encoder-decoder and its
variants with attention mechanism rely heavily
on the representation of ‚Äúmeaning‚Äù, which might
not be sufficiently inaccurate in cases in which
the system needs to refer to sub-sequences of in-
put like entity names or dates. In contrast, the

1631



copying mechanism is closer to the rote memo-
rization in language processing of human being,
deserving a different modeling strategy in neural
network-based models. We argue that it will ben-
efit many Seq2Seq tasks to have an elegant unified
model that can accommodate both understanding
and rote memorization. Towards this goal, we pro-
pose COPYNET, which is not only capable of the
regular generation of words but also the operation
of copying appropriate segments of the input se-
quence. Despite the seemingly ‚Äúhard‚Äù operation
of copying, COPYNET can be trained in an end-to-
end fashion. Our empirical study on both synthetic
datasets and real world datasets demonstrates the
efficacy of COPYNET.

2 Background: Neural Models for
Sequence-to-sequence Learning

Seq2Seq Learning can be expressed in a prob-
abilistic view as maximizing the likelihood (or
some other evaluation metrics (Shen et al., 2015))
of observing the output (target) sequence given an
input (source) sequence.

2.1 RNN Encoder-Decoder
RNN-based Encoder-Decoder is successfully ap-
plied to real world Seq2Seq tasks, first by Cho et
al. (2014) and Sutskever et al. (2014), and then
by (Vinyals and Le, 2015; Vinyals et al., 2015a).
In the Encoder-Decoder framework, the source se-
quence X = [x1, ..., xTS ] is converted into a fixed
length vector c by the encoder RNN, i.e.

ht = f(xt,ht‚àí1); c = œÜ({h1, ...,hTS}) (1)
where {ht} are the RNN states, c is the so-called
context vector, f is the dynamics function, and œÜ
summarizes the hidden states, e.g. choosing the
last state hTS . In practice it is found that gated
RNN alternatives such as LSTM (Hochreiter and
Schmidhuber, 1997) or GRU (Cho et al., 2014) of-
ten perform much better than vanilla ones.

The decoder RNN is to unfold the context vec-
tor c into the target sequence, through the follow-
ing dynamics and prediction model:

st = f(yt‚àí1, st‚àí1, c)
p(yt|y<t, X) = g(yt‚àí1, st, c)

(2)

where st is the RNN state at time t, yt is the pre-
dicted target symbol at t (through function g(¬∑))
with y<t denoting the history {y1, ..., yt‚àí1}. The
prediction model is typically a classifier over the
vocabulary with, say, 30,000 words.

2.2 The Attention Mechanism

The attention mechanism was first introduced to
Seq2Seq (Bahdanau et al., 2014) to release the
burden of summarizing the entire source into a
fixed-length vector as context. Instead, the atten-
tion uses a dynamically changing context ct in the
decoding process. A natural option (or rather ‚Äúsoft
attention‚Äù) is to represent ct as the weighted sum
of the source hidden states, i.e.

ct =
TS‚àë
œÑ=1

Œ±tœÑhœÑ ; Œ±tœÑ =
eŒ∑(st‚àí1,hœÑ )‚àë
œÑ ‚Ä≤ e

Œ∑(st‚àí1,hœÑ ‚Ä≤ )
(3)

where Œ∑ is the function that shows the correspon-
dence strength for attention, approximated usually
with a multi-layer neural network (DNN). Note
that in (Bahdanau et al., 2014) the source sen-
tence is encoded with a Bi-directional RNN, mak-
ing each hidden state hœÑ aware of the contextual
information from both ends.

3 COPYNET

From a cognitive perspective, the copying mech-
anism is related to rote memorization, requiring
less understanding but ensuring high literal fi-
delity. From a modeling perspective, the copying
operations are more rigid and symbolic, making
it more difficult than soft attention mechanism to
integrate into a fully differentiable neural model.
In this section, we present COPYNET, a differen-
tiable Seq2Seq model with ‚Äúcopying mechanism‚Äù,
which can be trained in an end-to-end fashion with
just gradient descent.

3.1 Model Overview

As illustrated in Figure 1, COPYNET is still an
encoder-decoder (in a slightly generalized sense).
The source sequence is transformed by Encoder
into representation, which is then read by Decoder
to generate the target sequence.

Encoder: Same as in (Bahdanau et al., 2014), a
bi-directional RNN is used to transform the source
sequence into a series of hidden states with equal
length, with each hidden state ht corresponding to
word xt. This new representation of the source,
{h1, ...,hTS}, is considered to be a short-term
memory (referred to as M in the remainder of the
paper), which will later be accessed in multiple
ways in generating the target sequence (decoding).

1632



hello    ,     my     name   is    Tony  Jebara   . 

Attentive	Read

hi     ,     Tony  Jebara

<eos>   hi     ,     Tony

h1 h2 h3 h4 h5

s1 s2 s3 s4

h6 h7 h8
‚ÄúTony‚Äù

DNN

Embedding 
for ‚ÄúTony‚Äù
Selective Read 
for ‚ÄúTony‚Äù

(a) Attention-based Encoder-Decoder (RNNSearch)
(c) State Update

s4

SourceVocabulary

Softmax
Prob(‚ÄúJebara‚Äù) = Prob(‚ÄúJebara‚Äù, g) + Prob(‚ÄúJebara‚Äù, c)

‚Ä¶ ...

(b) Generate-Mode & Copy-Mode

ùúå

M

M

Figure 1: The overall diagram of COPYNET. For simplicity, we omit some links for prediction (see
Sections 3.2 for more details).

Decoder: An RNN that reads M and predicts
the target sequence. It is similar with the canoni-
cal RNN-decoder in (Bahdanau et al., 2014), with
however the following important differences

‚Ä¢ Prediction: COPYNET predicts words based
on a mixed probabilistic model of two modes,
namely the generate-mode and the copy-
mode, where the latter picks words from the
source sequence (see Section 3.2);

‚Ä¢ State Update: the predicted word at time t‚àí1
is used in updating the state at t, but COPY-
NET uses not only its word-embedding but
also its corresponding location-specific hid-
den state in M (if any) (see Section 3.3 for
more details);

‚Ä¢ Reading M: in addition to the attentive read
to M, COPYNET also has‚Äúselective read‚Äù
to M, which leads to a powerful hybrid of
content-based addressing and location-based
addressing (see both Sections 3.3 and 3.4 for
more discussion).

3.2 Prediction with Copying and Generation
We assume a vocabulary V = {v1, ..., vN}, and
use UNK for any out-of-vocabulary (OOV) word.
In addition, we have another set of words X , for
all the unique words in source sequence X =
{x1, ..., xTS}. Since X may contain words not
in V , copying sub-sequence in X enables COPY-
NET to output some OOV words. In a nutshell,
the instance-specific vocabulary for source X is
V ‚à™ UNK ‚à™ X .

Given the decoder RNN state st at time t to-
gether with M, the probability of generating any
target word yt, is given by the ‚Äúmixture‚Äù of proba-
bilities as follows

p(yt|st, yt‚àí1, ct,M) = p(yt, g|st, yt‚àí1, ct,M)
+ p(yt, c|st, yt‚àí1, ct,M) (4)

where g stands for the generate-mode, and c the
copy mode. The probability of the two modes are
given respectively by

p(yt, g|¬∑)=

Ô£±Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£≥
1
Z
eœàg(yt), yt ‚àà V

0, yt ‚àà X ‚à© VÃÑ
1
Z
eœàg(UNK) yt 6‚àà V ‚à™ X

(5)

p(yt, c|¬∑)=
{ 1
Z

‚àë
j:xj=yt

eœàc(xj), yt ‚àà X
0 otherwise

(6)

where œàg(¬∑) and œàc(¬∑) are score functions for
generate-mode and copy-mode, respectively, and
Z is the normalization term shared by the two
modes, Z =

‚àë
v‚ààV‚à™{UNK} e

œàg(v) +
‚àë

x‚ààX e
œàc(x).

Due to the shared normalization term, the two
modes are basically competing through a softmax
function (see Figure 1 for an illustration with ex-
ample), rendering Eq.(4) different from the canon-
ical definition of the mixture model (McLachlan
and Basford, 1988). This is also pictorially illus-
trated in Figure 2. The score of each mode is cal-
culated:

1633



unk

ùëã ùëâùëã ‚à© ùëâ

'
( exp ùúì- ùë£/ 	 |	ùë£/ = ùë¶4

'
(
‚àë exp ùúì6 ùë•8 	 	| 	ùë•8= ùë¶4  9:

'
(
‚àë exp ùúì6 ùë•8 +9: exp ùúì- ùë£/ 	|	ùë•8 = ùë¶4,ùë£/ = ùë¶4

'
( exp	[ùúì- unk ]

*Z	is	the	normalization	term.

Figure 2: The illustration of the decoding proba-
bility p(yt|¬∑) as a 4-class classifier.

Generate-Mode: The same scoring function as
in the generic RNN encoder-decoder (Bahdanau et
al., 2014) is used, i.e.

œàg(yt = vi) = v>i Wost, vi ‚àà V ‚à™ UNK (7)

where Wo ‚àà R(N+1)√óds and vi is the one-hot in-
dicator vector for vi.

Copy-Mode: The score for ‚Äúcopying‚Äù the word
xj is calculated as

œàc(yt = xj) = œÉ
(
h>j Wc

)
st, xj ‚àà X (8)

where Wc ‚àà Rdh√óds , and œÉ is a non-linear ac-
tivation function, considering that the non-linear
transformation in Eq.( 8) can help project st and hj
in the same semantic space. Empirically, we also
found that using the tanh non-linearity worked
better than linear transformation, and we used that
for the following experiments. When calculating
the copy-mode score, we use the hidden states
{h1, ...,hTS} to ‚Äúrepresent‚Äù each of the word in
the source sequence {x1, ..., xTS} since the bi-
directional RNN encodes not only the content, but
also the location information into the hidden states
in M. The location informaton is important for
copying (see Section 3.4 for related discussion).
Note that we sum the probabilities of all xj equal
to yt in Eq. (6) considering that there may be mul-
tiple source symbols for decoding yt. Naturally
we let p(yt, c|¬∑) = 0 if yt does not appear in the
source sequence, and set p(yt, g|¬∑) = 0 when yt
only appears in the source.

3.3 State Update
COPYNET updates each decoding state st with
the previous state st‚àí1, the previous symbol yt‚àí1
and the context vector ct following Eq. (2) for the
generic attention-based Seq2Seq model. However,
there is some minor changes in the yt‚àí1‚àí‚Üíst path
for the copying mechanism. More specifically,
yt‚àí1 will be represented as [e(yt‚àí1); Œ∂(yt‚àí1)]>,

where e(yt‚àí1) is the word embedding associated
with yt‚àí1, while Œ∂(yt‚àí1) is the weighted sum of
hidden states in M corresponding to yt

Œ∂(yt‚àí1) =
‚àëTS

œÑ=1
œÅtœÑhœÑ

œÅtœÑ =

{ 1
K
p(xœÑ , c|st‚àí1,M), xœÑ = yt‚àí1

0 otherwise

(9)

where K is the normalization term which equals‚àë
œÑ ‚Ä≤:xœÑ ‚Ä≤=yt‚àí1

p(xœÑ ‚Ä≤ , c|st‚àí1,M), considering there
may exist multiple positions with yt‚àí1 in the
source sequence. In practice, œÅtœÑ is often con-
centrated on one location among multiple appear-
ances, indicating the prediction is closely bounded
to the location of words.

In a sense Œ∂(yt‚àí1) performs a type of read to
M similar to the attentive read (resulting ct) with
however higher precision. In the remainder of
this paper, Œ∂(yt‚àí1) will be referred to as selective
read. Œ∂(yt‚àí1) is specifically designed for the copy
mode: with its pinpointing precision to the cor-
responding yt‚àí1, it naturally bears the location of
yt‚àí1 in the source sequence encoded in the hidden
state. As will be discussed more in Section 3.4,
this particular design potentially helps copy-mode
in covering a consecutive sub-sequence of words.
If yt‚àí1 is not in the source, we let Œ∂(yt‚àí1) = 0.

3.4 Hybrid Addressing of M

We hypothesize that COPYNET uses a hybrid
strategy for fetching the content in M, which com-
bines both content-based and location-based ad-
dressing. Both addressing strategies are coordi-
nated by the decoder RNN in managing the atten-
tive read and selective read, as well as determining
when to enter/quit the copy-mode.

Both the semantics of a word and its location
in X will be encoded into the hidden states in M
by a properly trained encoder RNN. Judging from
our experiments, the attentive read of COPYNET is
driven more by the semantics and language model,
therefore capable of traveling more freely on M,
even across a long distance. On the other hand,
once COPYNET enters the copy-mode, the selec-
tive read of M is often guided by the location in-
formation. As the result, the selective read often
takes rigid move and tends to cover consecutive
words, including UNKs. Unlike the explicit de-
sign for hybrid addressing in Neural Turing Ma-
chine (Graves et al., 2014; Kurach et al., 2015),
COPYNET is more subtle: it provides the archi-

1634



tecture that can facilitate some particular location-
based addressing and lets the model figure out the
details from the training data for specific tasks.

Location-based Addressing: With the location
information in {hi}, the information flow

Œ∂(yt‚àí1)
update‚àí‚àí‚àí‚Üí st predict‚àí‚àí‚àí‚Üí yt sel. read‚àí‚àí‚àí‚àí‚Üí Œ∂(yt)

provides a simple way of ‚Äúmoving one step to the
right‚Äù on X . More specifically, assuming the se-
lective read Œ∂(yt‚àí1) concentrates on the `th word
in X , the state-update operation Œ∂(yt‚àí1)

update‚àí‚àí‚àí‚Üíst
acts as ‚Äúlocation ‚Üê location+1‚Äù, making st
favor the (`+1)th word in X in the prediction

st
predict‚àí‚àí‚àí‚Üí yt in copy-mode. This again leads to

the selective read hÃÇt
sel. read‚àí‚àí‚àí‚àí‚ÜíŒ∂(yt) for the state up-

date of the next round.

Handling Out-of-Vocabulary Words Although
it is hard to verify the exact addressing strategy as
above directly, there is strong evidence from our
empirical study. Most saliently, a properly trained
COPYNET can copy a fairly long segment full of
OOV words, despite the lack of semantic infor-
mation in its M representation. This provides a
natural way to extend the effective vocabulary to
include all the words in the source. Although this
change is small, it seems quite significant empiri-
cally in alleviating the OOV problem. Indeed, for
many NLP applications (e.g., text summarization
or spoken dialogue system), much of the OOV
words on the target side, for example the proper
nouns, are essentially the replicates of those on the
source side.

4 Learning
Although the copying mechanism uses the ‚Äúhard‚Äù
operation to copy from the source and choose to
paste them or generate symbols from the vocab-
ulary, COPYNET is fully differentiable and can
be optimized in an end-to-end fashion using back-
propagation. Given the batches of the source and
target sequence {X}N and {Y }N , the objectives
are to minimize the negative log-likelihood:

L = ‚àí 1
N

N‚àë
k=1

T‚àë
t=1

log
[
p(y(k)t |y(k)<t , X(k))

]
, (10)

where we use superscripts to index the instances.
Since the probabilistic model for observing any
target word is a mixture of generate-mode and
copy-mode, there is no need for any additional
labels for modes. The network can learn to co-
ordinate the two modes from data. More specif-
ically, if one particular word y(k)t can be found

in the source sequence, the copy-mode will con-
tribute to the mixture model, and the gradient will
more or less encourage the copy-mode; otherwise,
the copy-mode is discouraged due to the compe-
tition from the shared normalization term Z. In
practice, in most cases one mode dominates.

5 Experiments

We report our empirical study of COPYNET on the
following three tasks with different characteristics

1. A synthetic dataset on with simple patterns;
2. A real-world task on text summarization;
3. A dataset for simple single-turn dialogues.

5.1 Synthetic Dataset
Dataset: We first randomly generate transforma-
tion rules with 5‚àº20 symbols and variables x &
y, e.g.

a b x c d y e f ‚àí‚Üí g h x m,
with {a b c d e f g h m} being regular symbols
from a vocabulary of size 1,000. As shown in the
table below, each rule can further produce a num-
ber of instances by replacing the variables with
randomly generated subsequences (1‚àº15 sym-
bols) from the same vocabulary. We create five
types of rules, including ‚Äúx ‚Üí ‚àÖ‚Äù. The task is
to learn to do the Seq2Seq transformation from
the training instances. This dataset is designed to
study the behavior of COPYNET on handling sim-
ple and rigid patterns. Since the strings to repeat
are random, they can also be viewed as some ex-
treme cases of rote memorization.

Rule-type Examples (e.g. x = i h k, y = j c)

x‚Üí ‚àÖ a b c d x e f‚Üí c d g
x‚Üí x a b c d x e f‚Üí c d x g
x‚Üí xx a b c d x e f‚Üí x d x g
xy‚Üí x a b y d x e f‚Üí x d i g
xy‚Üí xy a b y d x e f‚Üí x d y g

Experimental Setting: We select 200 artificial
rules from the dataset, and for each rule 200 in-
stances are generated, which will be split into
training (50%) and testing (50%). We compare
the accuracy of COPYNET and the RNN Encoder-
Decoder with (i.e. RNNsearch) or without atten-
tion (denoted as Enc-Dec). For a fair compari-
son, we use bi-directional GRU for encoder and
another GRU for decoder for all Seq2Seq models,
with hidden layer size = 300 and word embedding
dimension = 150. We use bin size = 10 in beam
search for testing. The prediction is considered

1635



Rule-type x x x xy xy
‚Üí ‚àÖ ‚Üí x ‚Üí xx ‚Üí x ‚Üí xy

Enc-Dec 100 3.3 1.5 2.9 0.0
RNNSearch 99.0 69.4 22.3 40.7 2.6

COPYNET 97.3 93.7 98.3 68.2 77.5

Table 1: The test accuracy (%) on synthetic data.

correct only when the generated sequence is ex-
actly the same as the given one.

It is clear from Table 1 that COPYNET signifi-
cantly outperforms the other two on all rule-types
except ‚Äúx‚Üí ‚àÖ‚Äù, indicating that COPYNET can ef-
fectively learn the patterns with variables and ac-
curately replicate rather long subsequence of sym-
bols at the proper places.This is hard to Enc-Dec
due to the difficulty of representing a long se-
quence with very high fidelity. This difficulty can
be alleviated with the attention mechanism. How-
ever attention alone seems inadequate for handling
the case where strict replication is needed.

A closer look (see Figure 3 for example) re-
veals that the decoder is dominated by copy-mode
when moving into the subsequence to replicate,
and switch to generate-mode after leaving this
area, showing COPYNET can achieve a rather pre-
cise coordination of the two modes.

Pattern: 
705 502 X 504 339 270 584 556

‚Üí
510 771 581 557 022 230 X 115 
102 172 862 X 950

* Symbols are represented by 
their indices from 000 to 999

** Dark color represents large 
value. 5

10
 

77
1 

58
1 

55
7 

26
3 

02
2 

23
0

97
0 

99
1 

57
5 

32
5 

68
8

11
5 

10
2 

17
2 

86
2 

97
0 

99
1 

57
5 

32
5 

68
8

95
0

705 
502 

970 
991 

575 
325 

688
504 

339 
270 

584 
556 

Th
e 

So
ur

ce
 S

eq
ue

nc
e

<0
>

51
0 

77
1 

58
1 

55
7 

26
3 

02
2 

23
0

97
0 

99
1 

57
5 

32
5 

68
8

11
5 

10
2 

17
2 

86
2 

97
0 

99
1 

57
5 

32
5 

68
8

Input:

The Target Sequence
Predict:

Figure 3: Example output of COPYNET on the
synthetic dataset. The heatmap represents the ac-
tivations of the copy-mode over the input sequence
(left) during the decoding process (bottom).

5.2 Text Summarization
Automatic text summarization aims to find a con-
densed representation which can capture the core
meaning of the original document. It has been
recently formulated as a Seq2Seq learning prob-
lem in (Rush et al., 2015; Hu et al., 2015), which
essentially gives abstractive summarization since
the summary is generated based on a represen-
tation of the document. In contrast, extractive
summarization extracts sentences or phrases from
the original text to fuse them into the summaries,
therefore making better use of the overall struc-
ture of the original document. In a sense, COPY-
NET for summarization lies somewhere between

two categories, since part of output summary is ac-
tually extracted from the document (via the copy-
ing mechanism), which are fused together possi-
bly with the words from the generate-mode.

Dataset: We evaluate our model on the recently
published LCSTS dataset (Hu et al., 2015), a large
scale dataset for short text summarization. The
dataset is collected from the news medias on Sina
Weibo1 including pairs of (short news, summary)
in Chinese. Shown in Table 2, PART II and III are
manually rated for their quality from 1 to 5. Fol-
lowing the setting of (Hu et al., 2015) we use Part
I as the training set and and the subset of Part III
scored from 3 to 5 as the testing set.

Dataset PART I PART II PART III

no. of pairs 2,400,591 10,666 1106
no. of score ‚â• 3 - 8685 725

Table 2: Some statistics of the LCSTS dataset.

Experimental Setting: We try COPYNET that is
based on character (+C) and word (+W). For the
word-based variant the word-segmentation is ob-
tained with jieba2. We set the vocabulary size to
3,000 (+C) and 10,000 (+W) respectively, which
are much smaller than those for models in (Hu
et al., 2015). For both variants we set the em-
bedding dimension to 350 and the size of hidden
layers to 500. Following (Hu et al., 2015), we
evaluate the test performance with the commonly
used ROUGE-1, ROUGE-2 and ROUGE-L (Lin,
2004), and compare it against the two models in
(Hu et al., 2015), which are essentially canonical
Encoder-Decoder and its variant with attention.

Models ROUGE scores on LCSTS (%)
R-1 R-2 R-L

RNN +C 21.5 8.9 18.6
(Hu et al., 2015) +W 17.7 8.5 15.8
RNN context +C 29.9 17.4 27.2
(Hu et al., 2015) +W 26.8 16.1 24.1

COPYNET +C 34.4 21.6 31.3+W 35.0 22.3 32.0

Table 3: Testing performance of LCSTS, where
‚ÄúRNN‚Äù is canonical Enc-Dec, and ‚ÄúRNN context‚Äù
its attentive variant.

It is clear from Table 3 that COPYNET beats
the competitor models with big margin. Hu
et al. (2015) reports that the performance of a
word-based model is inferior to a character-based

1www.sina.com
2https://pypi.python.org/pypi/jieba

1636



Input(1):  ‰ªäÂ§©‰∏äÂçà 9 ÁÇπÂçäÔºåÂ§çÊó¶ÊäïÊØíÊ°àÂ∞ÜÂú®‰∏äÊµ∑‰∫å‰∏≠Èô¢ÂÖ¨ÂºÄÂÆ°ÁêÜ„ÄÇË¢´ÂÆ≥Â≠¶ÁîüÈªÑÊ¥ãÁöÑ‰∫≤Â±ûÂ∑≤‰ªéÂõõÂ∑ùÊäµËææ‰∏äÊµ∑ÔºåÂÖ∂Áà∂Áß∞ÂæÖÂàë‰∫ãÈÉ®ÂàÜÁªìÊùüÂêéÔºåÂÜçÊèêÊ∞ë‰∫ãËµîÂÅøÔºåÈªÑÊ¥ã 92 Â≤ÅÁöÑÂ•∂Â•∂‰æùÁÑ∂

‰∏çÁü•ÊÉÖ„ÄÇ‰ªäÂπ¥ 4 ÊúàÔºåÂú®Â§çÊó¶‰∏äÊµ∑ÂåªÂ≠¶Èô¢ËØªÁ†îÁ©∂ÁîüÁöÑÈªÑÊ¥ãÁñëÈÅ≠ÂÆ§ÂèãÊûóÊ£ÆÊµ©ÊäïÊØíÔºå‰∏çÂπ∏Ë∫´‰∫°„ÄÇÊñ∞Ê∞ëÁΩë
Today 9:30, the Fudan poisoning case will be will on public trial at the Shanghai Second Intermediate Court. The relatives of the murdered student Huang Yang has arrived at Shanghai from Sichuan. His father said that they will start 
the lawsuit for civil compensation after the criminal section. HuangYang 92-year-old grandmother is still unaware of his death. In April, a graduate student at Fudan University Shanghai Medical College, Huang Yang is allegedly 
poisoned and killed by his roommate Lin Senhao. Reported by Xinmin
______________________________________________________________
Golden: ÊûóÊ£ÆÊµ©ÊäïÊØíÊ°à‰ªäÊó•ÂºÄÂÆ° 92 Â≤ÅÂ•∂Â•∂Â∞ö‰∏çÁü•ÊÉÖ
the case of Lin Senhao poisoning is on trial today, his 92-year-old grandmother is still unaware of this

RNN context:  Â§çÊó¶ÊäïÊØíÊ°àÔºöÈªÑÊ¥ãÁñëÈÅ≠ÂÆ§ÂèãÊäïÊØíÂá∂ÊâãÂ∑≤‰ªéÂõõÂ∑ùÈ£ûÂæÄ‰∏äÊµ∑ÔºåÁà∂‰∫≤ÂëΩÊ°àÂè¶Êúâ4‰∫∫Ë¢´ÈÄöÁü•ÂÆ∂Â±û‰∏çÊ≤ªÔºü

CopyNet:  Â§çÊó¶ÊäïÊØíÊ°à‰ªäÂú®Ê≤™‰∏äÂÖ¨ÂºÄÂÆ°ÁêÜ
the Fudan poisoning case is on public trial today in Shanghai

Input(2):  ÂçéË∞äÂÖÑÂºüÔºà 300027ÔºâÂú®Êò®Êó•Êî∂ÁõòÂêéÂèëÂ∏ÉÂÖ¨ÂëäÁß∞ÔºåÂÖ¨Âè∏Êãü‰ª•Ëá™ÊúâËµÑÈáë 3.978 ‰∫øÂÖÉÊî∂Ë¥≠ÊµôÊ±üÊ∞∏‰πêÂΩ±ËßÜËÇ°‰ªΩÊúâÈôêÂÖ¨Âè∏Ëã•Âπ≤ËÇ°‰∏úÊåÅÊúâÁöÑÊ∞∏‰πêÂΩ±ËßÜ 51 % ÁöÑËÇ°ÊùÉ„ÄÇÂØπ‰∫éÊ≠§È°πÊî∂Ë¥≠Ôºå

ÂçéË∞äÂÖÑÂºüËë£ÁßòËÉ°ÊòéÊò®Êó•Ë°®Á§∫Ôºö‚ÄúÂíåÊ∞∏‰πêÂΩ±ËßÜÁöÑÂêàÂπ∂ÊòØÂØπÂçéË∞äÂÖÑÂºüÁîµËßÜÂâß‰∏öÂä°ÁöÑ‰∏Ä‰∏™Âä†Âº∫„ÄÇ
Huayi Brothers (300027) announced that the company intends to buy with its own fund 397.8 million 51% of Zhejiang Yongle Film LTD's stake owned by a number of shareholders of Yongle Film LTD. For this acquisition, the 
secretary of the board, Hu Ming, said yesterday: "the merging with Yongle Film is to strengthen Huayi Brothers on TV business".
______________________________________________________________
Golden: ÂçéË∞äÂÖÑÂºüÊãüÊî∂Ë¥≠Ê∞∏‰πêÂΩ±ËßÜÔºïÔºëÔºÖËÇ°ÊùÉ
Huayi Brothers intends to acquire 51% stake of Zhejiang Yongle Film

RNN context:  ÂçéË∞äÂÖÑÂºüÊî∂Ë¥≠Ê∞∏‰πêÂΩ±ËßÜ51%ËÇ°ÊùÉÔºö‰∏éÊ∞∏‰πêÂΩ±ËßÜÂêàÂπ∂‰∏∫‚ÄúÂíåÂîê‚ÄùÂΩ±ËßÜÂêàÂπ∂ÁöÑ‚ÄúUNK‚ÄùÂíå‚ÄúUNK‚ÄùÁöÑÂå∫Âà´Ôºü

CopyNet:  ÂçéË∞äÂÖÑÂºüÊãü 3.978 ‰∫øÊî∂Ë¥≠Ê∞∏‰πêÂΩ±ËßÜËë£ÁßòÁß∞Âä†Âº∫ÁîµËßÜÂâß‰∏öÂä°
Huayi Brothers is intended to 397.8 million acquisition of Yongle Film secretaries called to strengthen the TV business 

Input(3):  Â∑•ÂéÇÔºåÂ§ßÈó®Á¥ßÈîÅÔºåÁ∫¶ 20 ÂêçÂ∑•‰∫∫Êï£ÂùêÂú®Ê†ëËç´‰∏ã„ÄÇ‚ÄúÊàë‰ª¨Â∞±ÊòØÊôÆÈÄöÂ∑•‰∫∫ÔºåÂú®ËøôÈáåÁ≠âÂ∑•ËµÑ„ÄÇ‚ÄùÂÖ∂‰∏≠‰∏Ä‰∫∫ËØ¥ÈÅì„ÄÇ 7 Êúà 4 Êó•‰∏äÂçàÔºåËÆ∞ËÄÖÊäµËææÊ∑±Âú≥ÈæôÂçéÂå∫Ê∏ÖÊπñË∑Ø‰∏äÁöÑÊ∑±Âú≥ÊÑøÊôØ

ÂÖâÁîµÂ≠êÊúâÈôêÂÖ¨Âè∏„ÄÇÊ≠£Â¶Ç‰º†Ë®Ä‰∏ÄËà¨ÔºåÊÑøÊôØÂÖâÁîµÂ≠êÂÄíÈó≠‰∫ÜÔºåÂ§ßËÇ°‰∏úÈÇ¢ÊØÖ‰∏çÁü•ÊâÄË∏™„ÄÇ
The door of factory is locked. About 20 workers are scattered to sit under the shade. ‚ÄúWe are ordinary workers, waiting for our salary‚Äù one of them said. In the morning of July 4th, reporters arrived at Yuanjing Photoelectron 
Corporation located at Qinghu Road, Longhua District, Shenzhen. Just as the rumor, Yuanjing Photoelectron Corporation is closed down and the big shareholder Xing Yi is missing.
______________________________________________________________
Golden: Ê∑±Âú≥‰∫øÂÖÉÁ∫ß LED ‰ºÅ‰∏öÂÄíÈó≠ÁÉàÊó•‰∏ãÂ∑•‰∫∫Ëã¶Á≠âËÄÅÊùø
Hundred-million CNY worth LED enterprise is closed down and workers wait for the boss under the scorching sun

RNN context:  Ê∑±Âú≥‚Äú<UNK>‚ÄùÔºöÊ∑±Âú≥<UNK><UNK>Ôºå<UNK>Ôºå<UNK>Ôºå<UNK> 

CopyNet: ÊÑøÊôØÂÖâÁîµÂ≠êÂÄíÈó≠ 20 ÂêçÂ∑•‰∫∫Êï£ÂùêÂú®Ê†ëËç´‰∏ã
Yuanjing Photoelectron Corporation is closed down, 20 workers are scattered to sit under the shade

Input(4):  Êà™Ëá≥ 2012 Âπ¥ 10 ÊúàÂ∫ïÔºåÂÖ®ÂõΩÁ¥ØËÆ°Êä•ÂëäËâæÊªãÁóÖÁóÖÊØíÊÑüÊüìËÄÖÂíåÁóÖ‰∫∫ 492191‰æã„ÄÇÂç´ÁîüÈÉ®Áß∞ÔºåÊÄß‰º†Êí≠Â∑≤Êàê‰∏∫ËâæÊªãÁóÖÁöÑ‰∏ªË¶Å‰º†Êí≠ÈÄîÂæÑ„ÄÇËá≥ 2011 Âπ¥ 9 ÊúàÔºåËâæÊªãÁóÖÊÑüÊüìËÄÖÂíåÁóÖ‰∫∫Êï∞Á¥Ø

ËÆ°Êä•ÂëäÊï∞ÊéíÂú®Ââç 6 ‰ΩçÁöÑÁúÅ‰ªΩ‰æùÊ¨°‰∏∫‰∫ëÂçó„ÄÅÂπøË•ø„ÄÅÊ≤≥Âçó„ÄÅÂõõÂ∑ù„ÄÅÊñ∞ÁñÜÂíåÂπø‰∏úÔºåÂç†ÂÖ®ÂõΩÁöÑ 75.8 % „ÄÇ„ÄÇ
At the end of October 2012, the national total of reported HIV infected people and AIDS patients is 492,191 cases. The Health Ministry saids exual transmission has become the main route of transmission of AIDS. To September 
2011, the six provinces with the most reported HIV infected people and AIDS patients were Yunnan, Guangxi, Henan,Sichuan, Xinjiang and Guangdong, accounting for 75.8% of the country.
______________________________________________________________
Golden: Âç´ÁîüÈÉ®ÔºöÊÄß‰º†Êí≠ÊàêËâæÊªãÁóÖ‰∏ªË¶Å‰º†Êí≠ÈÄîÂæÑ
Ministry of Health: Sexually transmission became the main route of transmission of AIDS

RNN context:  ÂÖ®ÂõΩÁ¥ØËÆ°Êä•ÂëäËâæÊªãÁóÖÊÇ£ËÄÖÂíåÁóÖ‰∫∫<UNK>‰æãËâæÊªãÁóÖÊÇ£ËÄÖÂç†ÂÖ®ÂõΩ<UNK>%ÔºåÊÄß‰º†Êí≠ÊàêËâæÊªãÁóÖÈ´òÂèë‰∫∫Áæ§ Ôºü

CopyNet:  Âç´ÁîüÈÉ®ÔºöÊÄß‰º†Êí≠Â∑≤Êàê‰∏∫ËâæÊªãÁóÖ‰∏ªË¶Å‰º†Êí≠ÈÄîÂæÑ
Ministry of Health: Sexually transmission has become the main route of transmission of AIDS

Input(5):  ‰∏≠ÂõΩÂèçÂûÑÊñ≠Ë∞ÉÊü•È£éÊö¥ÁªßÁª≠Â∏≠Âç∑Ê±ΩËΩ¶Ë°å‰∏öÔºåÁªßÂæ∑ÂõΩËΩ¶‰ºÅÂ••Ëø™ÂíåÁæéÂõΩËΩ¶‰ºÅÂÖãËé±ÊñØÂãí‚ÄúÊ≤¶Èô∑‚Äù‰πãÂêéÔºåÂèàÊúâ 12 ÂÆ∂Êó•Êú¨Ê±ΩËΩ¶‰ºÅ‰∏öÂç∑ÂÖ•Êº©Ê∂°„ÄÇËÆ∞ËÄÖ‰ªé‰∏öÂÜÖ‰∫∫Â£´Ëé∑ÊÇâÔºå‰∏∞Áî∞Êóó‰∏ãÁöÑ

Èõ∑ÂÖãËê®ÊñØËøëÊúüÊõæË¢´ÂèëÊîπÂßîÁ∫¶Ë∞à„ÄÇ
Chinese antitrust investigation continues to sweep the automotive industry. After Germany Audi car and the US Chrysler "fell", there are 12 Japanese car companies involved in the whirlpool. Reporters learned from the insiders 
that Toyota's Lexus has been asked to report to the Development and Reform Commission recently.
______________________________________________________________
Golden: ÂèëÊîπÂßîÂÖ¨Â∏ÉÊ±ΩËΩ¶ÂèçÂûÑÊñ≠ËøõÁ®ãÔºö‰∏∞Áî∞Èõ∑ÂÖãËê®ÊñØËøëÊúüË¢´Á∫¶Ë∞à
the investigation by Development and Reform Commission: Toyota's Lexus has been asked to report

RNN context:  ‰∏∞Áî∞Èõ∑ÂÖãËê®ÊñØÈÅ≠ÂèëÊîπÂßîÁ∫¶Ë∞àÔºöÊõæË¢´Á∫¶Ë∞à‰∏∞Áî∞Êóó‰∏ãÁöÑÈõ∑ÂÖãËê®ÊñØÈÅ≠ÂèëÊîπÂßîÁ∫¶Ë∞àË¥ü‰∫∫Ë¢´Á∫¶Ë∞à

CopyNet:  ‰∏≠ÂõΩÂèçÂûÑÊñ≠ÁªßÁª≠Â∏≠Âç∑Ê±ΩËΩ¶Ë°å‰∏ö 12 ÂÆ∂Êó•Êú¨Ê±ΩËΩ¶‰ºÅ‰∏öË¢´ÂèëÊîπÂßîÁ∫¶Ë∞à
Chinese antitrust investigation continues to sweep the automotive industry. 12 Japanese car companies are asked to report to he Development and Reform Commission

Input(6):  ÈïÅÁ¶ªÂ≠êÁîµÊ±†Áõ∏ÊØîÈîÇÁîµÊ±†ËÉΩÈáèÂØÜÂ∫¶ÊèêÂçá‰∫ÜËøë‰∏ÄÂÄçÔºåËøôÊÑèÂë≥ÁùÄ‰ΩøÁî®‰∫ÜÈïÅÁîµÊ±†ÁöÑÁîµÂä®ËΩ¶ÔºåÁ∫ØÁîµÁª≠Ëà™‰πüÂ∞ÜÊúâË¥®ÁöÑÊèêÂçá„ÄÇ‰ΩÜÁõÆÂâçÁî±‰∫éÁîµËß£Ë¥®Á≠âÊäÄÊúØÂ£ÅÂûíÔºåË¶ÅÂ§ßËßÑÊ®°Èáè‰∫ßÂπ∂Âèñ‰ª£ÈîÇÁîµÊ±†Ëøò‰∏∫Êó∂ËøáÊó©„ÄÇ
The energy density of Magnesium ion batteries almost doubles that of lithium battery, which means that for the electric vehicles using of magnesium batteries will last longer even at pure electric power. But currently due to the 
technical barriers to the electrolyte, it is still too early for the mass production of it and replacing lithium batteries..
______________________________________________________________
Golden: ÈîÇÁîµÊ±†ÊàñÂ∞ÜË¢´Ê∑òÊ±∞ ËÉΩÈáèÂØÜÂ∫¶Êõ¥È´òÁöÑÈïÅÁîµÊ±†‰∫¶Â§ßÂäøÊâÄË∂ã
Lithium batteries will be phased out, magnesium battery with energy density higher will be the future trend

RNN context:  <UNK>„ÄÅ<UNK>„ÄÅ<UNK>„ÄÅ<UNK>„ÄÅ<UNK>„ÄÅ<UNK>„ÄÅ<UNK>„ÄÅ<UNK>„ÄÅ<UNK>„ÄÅ<UNK>„ÄÅ<UNK>„ÄÅ<UNK>ÁîµÊ±†‰∫Ü

CopyNet:  ÈïÅÁ¶ªÂ≠êÁîµÊ±†ÈóÆ‰∏ñÔºöÂ§ßËßÑÊ®°Èáè‰∫ßÂèñ‰ª£ÈîÇÁîµÊ±†
Magnesium ion battery is developed : mass production of it will replace lithium batteries 

Input(7):  1 . ÊéåÊè°ÊäÄÂ∑ßËûç‰ºöË¥ØÈÄöÔºõ 2 . Â≠¶‰ºöËûçËµÑÔºõ 3 . ÊáÇÊ≥ïÂæãÔºõ 4 . ‰øùÊåÅËá™‰ø°Ôºõ 5 . ÊµãËØï + Â∞ùËØïÔºõ 6 . ‰∫ÜËß£ÂÆ¢Êà∑ÁöÑÈúÄÊ±ÇÔºõ 7 . È¢ÑÊµã + Ë°°Èáè + Á°Æ‰øùÔºõ 8 . ÂÅöÂ•Ω‰∏éÂêÑÁßçÂ∞è bugÂÅöÊñó‰∫âÁöÑÂøÉÊÄÅÔºõ 9 . 

ÂèëÁé∞Êú∫ÈÅá‰øùÊåÅÂàõ‰∏öÊøÄÊÉÖ„ÄÇ
1. master the skills; 2 Learn to finance ; 3. understand the law; 4. Be confident; 5. test+ trial; 6. understand the need of customers; 7 forecast + measure + ensure; 8. mentally prepared to fight all kinds of small bugs ; 9 discover 
opportunities and keep the passion of start-up.

______________________________________________________________
Golden: ÂàùÊ¨°Âàõ‰∏öËÄÖÂøÖÁü•ÁöÑ 10 ‰∏™ÊäÄÂ∑ß
The 10 tips for the first time start-ups 

RNN context:  6‰∏™ÊñπÊ≥ïËÆ©‰Ω†Âàõ‰∏öÁöÑ6‰∏™<UNK>‰∏é<UNK>Ôºå‰Ω†ÊÄé‰πàÁúãÊáÇ‰Ω†ÁöÑÂàõ‰∏öÊïÖ‰∫ãÂêóÔºüÔºà6ÂÆ∂Ôºâ

CopyNet:  Âàõ‰∏öÊàêÂäüÁöÑ 9 ‰∏™ÊäÄÂ∑ß
The 9 tips for success in start-up

Input(8):  9 Êúà 3 Êó•ÔºåÊÄªÈÉ®‰Ωç‰∫éÊó•ÂÜÖÁì¶ÁöÑ‰∏ñÁïåÁªèÊµéËÆ∫ÂùõÂèëÂ∏É‰∫Ü„Ää 2014 - 2015 Âπ¥ÂÖ®ÁêÉÁ´û‰∫âÂäõÊä•Âëä„ÄãÔºåÁëûÂ£´ËøûÁª≠ÂÖ≠Âπ¥‰ΩçÂ±ÖÊ¶úÈ¶ñÔºåÊàê‰∏∫ÂÖ®ÁêÉÊúÄÂÖ∑Á´û‰∫âÂäõÁöÑÂõΩÂÆ∂ÔºåÊñ∞Âä†Âù°ÂíåÁæéÂõΩÂàÜÂàóÁ¨¨‰∫å

‰ΩçÂíåÁ¨¨‰∏â‰Ωç„ÄÇ‰∏≠ÂõΩÊéíÂêçÁ¨¨ 28 ‰ΩçÔºåÂú®ÈáëÁ†ñÂõΩÂÆ∂‰∏≠ÊéíÂêçÊúÄÈ´ò„ÄÇ
On September 3, the Geneva based World Economic Forum released ‚Äú The Global Competitiveness Report 2014-2015‚Äù. Switzerland topped the list for six consecutive years , becoming the world‚Äòs most competitive country. Singapore 
and the United States are in the second and third place respectively. China is in the 28th place, ranking highest among the BRIC countries.

______________________________________________________________
Golden: ÂÖ®ÁêÉÁ´û‰∫âÂäõÊéíË°åÊ¶ú‰∏≠ÂõΩÂ±Ö 28 ‰ΩçÂ±ÖÈáëÁ†ñÂõΩÂÆ∂È¶ñ‰Ωç
The Global competitiveness ranking list, China is in the 28th place, the highest among BRIC countries.

RNN context:  2014-2015Âπ¥ÂÖ®ÁêÉÁ´û‰∫âÂäõÊä•ÂëäÔºöÁëûÂ£´ËøûÁª≠6Âπ¥Â±ÖÊ¶úÈ¶ñ‰∏≠ÂõΩÂ±Ö28‰Ωç(È¶ñ/3‚Äî‚Äî‚ÄîËÆøÊ¶úÈ¶ñ)‰∏≠ÂõΩÊéíÂêçÁ¨¨28‰Ωç

CopyNet:  2014 - 2015 Âπ¥ÂÖ®ÁêÉÁ´û‰∫âÂäõÊä•ÂëäÔºöÁëûÂ£´Â±ÖÈ¶ñ‰∏≠ÂõΩÁ¨¨ 28
2014--2015 Global Competitiveness Report: Switzerland topped and China the 28th

Figure 4: Examples of COPYNET on LCSTS compared with RNN context. Word segmentation is
applied on the input, where OOV words are underlined. The highlighted words (with different colors)
are those words with copy-mode probability higher than the generate-mode. We also provide literal
English translation for the document, the golden, and COPYNET, while omitting that for RNN context
since the language is broken.

1637



one. One possible explanation is that a word-
based model, even with a much larger vocabulary
(50,000 words in Hu et al. (2015)), still has a large
proportion of OOVs due to the large number of en-
tity names in the summary data and the mistakes
in word segmentation. COPYNET, with its ability
to handle the OOV words with the copying mech-
anism, performs however slightly better with the
word-based variant.

5.2.1 Case Study
As shown in Figure 4, we make the following
interesting observations about the summary from
COPYNET: 1) most words are from copy-mode,
but the summary is usually still fluent; 2) COPY-
NET tends to cover consecutive words in the orig-
inal document, but it often puts together seg-
ments far away from each other, indicating a so-
phisticated coordination of content-based address-
ing and location-based addressing; 3) COPYNET
handles OOV words really well: it can gener-
ate acceptable summary for document with many
OOVs, and even the summary itself often con-
tains many OOV words. In contrast, the canonical
RNN-based approaches often fail in such cases.

It is quite intriguing that COPYNET can often
find important parts of the document, a behav-
ior with the characteristics of extractive summa-
rization, while it often generate words to ‚Äúcon-
nect‚Äù those words, showing its aspect of abstrac-
tive summarization.

5.3 Single-turn Dialogue

In this experiment we follow the work on neural
dialogue model proposed in (Shang et al., 2015;
Vinyals and Le, 2015; Sordoni et al., 2015), and
test COPYNET on single-turn dialogue. Basically,
the neural model learns to generate a response to
user‚Äôs input, from the given (input, response) pairs
as training instances.

Dataset: We build a simple dialogue dataset
based on the following three instructions:

1. Dialogue instances are collected from Baidu
Tieba3 with some coverage of conversations
of real life e.g., greeting and sports, etc.

2. Patterns with slots like
hi, my name is x‚Üí hi, x

are mined from the set, with possibly multi-
ple responding patterns to one input.

3http://tieba.baidu.com

3. Similar with the synthetic dataset, we enlarge
the dataset by filling the slots with suitable
subsequence (e.g. name entities, dates, etc.)

To make the dataset close to the real conversations,
we also maintain a certain proportion of instances
with the response that 1) do not contain entities or
2) contain entities not in the input.

Experimental Setting: We create two datasets:
DS-I and DS-II with slot filling on 173 collected
patterns. The main difference between the two
datasets is that the filled substrings for training and
testing in DS-II have no overlaps, while in DS-I
they are sampled from the same pool. For each
dataset we use 6,500 instances for training and
1,500 for testing. We compare COPYNET with
canonical RNNSearch, both character-based, with
the same model configuration in Section 5.1.

DS-I (%) DS-II (%)

Models Top1 Top10 Top1 Top10

RNNSearch 44.1 57.7 13.5 15.9
COPYNET 61.2 71.0 50.5 64.8

Table 4: The decoding accuracy on the two testing
sets. Decoding is admitted success only when the
answer is found exactly in the Top-K outputs.

We compare COPYNET and RNNSearch on
DS-I and DS-II in terms of top-1 and top-10 ac-
curacy (shown in Table 4), estimating respectively
the chance of the top-1 or one of top-10 (from
beam search) matching the golden. Since there
are often many good responses to an input, top-
10 accuracy appears to be closer to the real world
setting.

As shown in Table 4, COPYNET significantly
outperforms RNNsearch, especially on DS-II. It
suggests that introducing the copying mechanism
helps the dialogue system master the patterns in
dialogue and correctly identify the correct parts of
input, often proper nouns, to replicate in the re-
sponse. Since the filled substrings have no over-
laps in DS-II, the performance of RNNSearch
drops significantly as it cannot handle words un-
seen in training data. In contrast, the performance
of COPYNET only drops slightly as it has learned
to fill the slots with the copying mechanism and
relies less on the representation of the words.

5.3.1 Case Study
As indicated by the examples in Figure 5, COPY-
NET accurately replicates the critical segments
from the input with the copy-mode, and generates

1638



Input(1):  ÊàëÂá∫ÁîüÂú®‰∏äÊµ∑„ÄÇ
/I was born in Shanghai

Golden:   ÁúüÂ∑ßÔºåÊàë‰πüÊù•Ëá™‰∏äÊµ∑ÔºåËÄÅ‰π°Âïä!   /What a coincidence, I also come from Shanghai, we are fellow townsman!
RNNSearch:   ÁúüÂ∑ßÔºåÊàë‰πüÊù•Ëá™Êµ∑ÂçóÔºåËÄÅ‰π°Âïä!   /What a coincidence, I also come from Hainan, we are fellow townsman!

CopyNet:   ÁúüÂ∑ßÔºåÊàë‰πüÊù•Ëá™‰∏äÊµ∑ÔºåËÄÅ‰π°Âïä!   /What a coincidence, I also come from Shanghai, we are fellow townsman!

Input(2):  ‰ªäÂ§©ÂéªÁà¨ÈªÑÂ±±ÂêóÔºü
/Shall we climbing Huangshan?

Golden:   ÂéªÁà¨Ê≥∞Â±±ÂêßÔºåÈªÑÂ±±Ê≤°ÊÑèÊÄù!  /Climb Taishan. Huangshan is boring!
RNNSearch:   ÂéªÁà¨ÈïøÁôΩÂ±±ÂêßÔºåË°°Â±±Ê≤°ÊÑèÊÄù!       /Climb Changbaishan, Hengshan is boring!

CopyNet:   ÂéªÁà¨Âµ©Â±±ÂêßÔºåÈªÑÂ±±Ê≤°ÊÑèÊÄù!   /Climb Songshan, Huangshan is boring!

Input(3):  ‰Ω†ÂñúÊ¨¢ÂñùÈõ™Á¢ßËøòÊòØÂñúÊ¨¢ÂèØ‰πêÔºü
/Would you like to drink Sprite or Coke?

Golden:    ÈÉΩ‰∏çÂñúÊ¨¢ÔºåË∞¢Ë∞¢!                            /Not at all, thanks!
RNNSearch:    ÊàëÂñúÊ¨¢ÂñùËúÇËúú! /I like to drink honey!

CopyNet:    ÊàëÂñúÊ¨¢ÂñùÂèØ‰πê!   /I like to drink coke!

Input(4):  ÊàëÂ§ßÈùíÂ≤õÂõõÂ≠£ÂÆú‰∫∫
/Our great Qingdao has a good weather!

Golden:   Â£ÆÊàëÂ§ßÈùíÂ≤õÔΩûÔΩûÔΩû /Our Qingdao is great~~~
RNNSearch:   Â£ÆÊàëÂ§ßÂìàÂ∞îÊª®ÔΩûÔΩûÔΩû /Our Harbin is great~~~

CopyNet:   Â£ÆÊàëÂ§ßÈùíÂ≤õÔΩûÔΩûÔΩû /Our Qingdao is great~~~

Input(5):  Â•≥ÂÑø‰ºöÊ∏∏Ëù∂Ê≥≥
/Daughter can swim butterfly?

Golden:   ÊàëËøûÊ∏∏Ê≥≥ÈÉΩ‰∏ç‰ºöT^T                      /I cannot swim at all T^T
RNNSearch:   ÊàëËøûÊ∏∏Ê≥≥ÈÉΩ‰∏ç‰ºöT^T                 /I cannot swim at all T^T

CopyNet:   ÊàëËøûÊ∏∏Ê≥≥ÈÉΩ‰∏ç‰ºöT^T                  /I cannot swim at all T^T

Input(6):  ËãèÊúâÊúãÊòØË∞Å? 
/Who is Su You Peng?

Golden:   ËãèÊúâÊúãÊòØ‰∏Ä‰∏™Áî∑ÊòéÊòü„ÄÇ /Su You Peng is a male star.

CopyNet:   ËãèÊúâÊòØ‰∏Ä‰∏™Áî∑ÊòéÊòü„ÄÇ /Su You is a male star

RNNSearch: Âê¥‰∫¶Âá°ÊòØ‰∏Ä‰∏™Áî∑ÊòéÊòü„ÄÇ /Wu Yifan is a male star.

Figure 5: Examples from the testing set of DS-II shown as the input text and golden, with the outputs
of RNNSearch and CopyNet. Words in red rectangles are unseen in the training set. The highlighted
words (with different colors) are those words with copy-mode probability higher than the generate-mode.
Green cirles (meaning correct) and red cross (meaning incorrect) are given based on human judgment on
whether the response is appropriate.

the rest of the answers smoothly by the generate-
mode. Note that in (2) and (3), the decoding se-
quence is not exactly the same with the standard
one, yet still correct regarding to their meanings.
In contrast, although RNNSearch usually gener-
ates answers in the right formats, it fails to catch
the critical entities in all three cases because of the
difficulty brought by the unseen words.

6 Related Work
Our work is partially inspired by the recent work
of Pointer Networks (Vinyals et al., 2015a), in
which a pointer mechanism (quite similar with the
proposed copying mechanism) is used to predict
the output sequence directly from the input. In ad-
dition to the difference with ours in application,
(Vinyals et al., 2015a) cannot predict outside of
the set of input sequence, while COPYNET can
naturally combine generating and copying.

COPYNET is also related to the effort to solve
the OOV problem in neural machine translation.
Luong et al. (2015) introduced a heuristics to post-
process the translated sentence using annotations
on the source sentence. In contrast COPYNET ad-
dresses the OOV problem in a more systemic way
with an end-to-end model. However, as COPY-
NET copies the exact source words as the output, it
cannot be directly applied to machine translation.
However, such copying mechanism can be natu-
rally extended to any types of references except
for the input sequence, which will help in appli-
cations with heterogeneous source and target se-

quences such as machine translation.
The copying mechanism can also be viewed as

carrying information over to the next stage without
any nonlinear transformation. Similar ideas are
proposed for training very deep neural networks in
(Srivastava et al., 2015; He et al., 2015) for clas-
sification tasks, where shortcuts are built between
layers for the direct carrying of information.

Recently, we noticed some parallel efforts to-
wards modeling mechanisms similar to or related
to copying. Cheng and Lapata (2016) devised a
neural summarization model with the ability to ex-
tract words/sentences from the source. Gulcehre
et al. (2016) proposed a pointing method to han-
dle the OOV words for summarization and MT. In
contrast, COPYNET is more general, and not lim-
ited to a specific task or OOV words. Moreover,
the softmaxCOPYNET is more flexible than gating
in the related work in handling the mixture of two
modes, due to its ability to adequately model the
content of copied segment.

7 Conclusion and Future Work
We proposed COPYNET to incorporate copy-
ing into the sequence-to-sequence learning frame-
work. For future work, we will extend this idea to
the task where the source and target are in hetero-
geneous types, for example, machine translation.

Acknowledgments
This work is supported in part by the China Na-
tional 973 Project 2014CB340301.

1639



References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.
arXiv preprint arXiv:1603.07252.

Kyunghyun Cho, Bart Van MerrieÃànboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Alex Graves, Greg Wayne, and Ivo Danihelka.
2014. Neural turing machines. arXiv preprint
arXiv:1410.5401.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallap-
ati, Bowen Zhou, and Yoshua Bengio. 2016.
Pointing the unknown words. arXiv preprint
arXiv:1603.08148.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Deep residual learning for image recog-
nition. arXiv preprint arXiv:1512.03385.

Sepp Hochreiter and JuÃàrgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735‚Äì1780.

Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. Lc-
sts: a large scale chinese short text summarization
dataset. arXiv preprint arXiv:1506.05865.

Karol Kurach, Marcin Andrychowicz, and Ilya
Sutskever. 2015. Neural random-access machines.
arXiv preprint arXiv:1511.06392.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74‚Äì81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.

Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,
and Wojciech Zaremba. 2015. Addressing the rare
word problem in neural machine translation. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 11‚Äì19,
Beijing, China, July. Association for Computational
Linguistics.

Geoffrey J McLachlan and Kaye E Basford. 1988.
Mixture models. inference and applications to clus-
tering. Statistics: Textbooks and Monographs, New
York: Dekker, 1988, 1.

Alexander M Rush, Sumit Chopra, and Jason We-
ston. 2015. A neural attention model for ab-
stractive sentence summarization. arXiv preprint
arXiv:1509.00685.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. arXiv preprint arXiv:1503.02364.

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2015. Minimum
risk training for neural machine translation. CoRR,
abs/1512.02433.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. arXiv preprint
arXiv:1506.06714.

Rupesh Kumar Srivastava, Klaus Greff, and JuÃàrgen
Schmidhuber. 2015. Highway networks. arXiv
preprint arXiv:1505.00387.

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104‚Äì3112.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015a. Pointer networks. In Advances in Neural
Information Processing Systems, pages 2674‚Äì2682.

Oriol Vinyals, ≈Åukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015b. Gram-
mar as a foreign language. In Advances in Neural
Information Processing Systems, pages 2755‚Äì2763.

1640


