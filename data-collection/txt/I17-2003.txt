



















































Transferring Semantic Roles Using Translation and Syntactic Information


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 13–19,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Transferring Semantic Roles Using Translation and Syntactic Information

Maryam Aminian1, Mohammad Sadegh Rasooli2, Mona Diab1
1Department of Computer Science, The George Washington University, Washington

2Department of Computer Science, Columbia University, New York
1{aminian,mtdiab}@gwu.edu
2rasooli@cs.columbia.edu

Abstract

Our paper addresses the problem of anno-
tation projection for semantic role label-
ing for resource-poor languages using su-
pervised annotations from a resource-rich
language through parallel data. We pro-
pose a transfer method that employs in-
formation from source and target syntactic
dependencies as well as word alignment
density to improve the quality of an iter-
ative bootstrapping method. Our experi-
ments yield a 3.5 absolute labeled F-score
improvement over a standard annotation
projection method.

1 Introduction

Semantic role labeling (SRL) is the task of auto-
matically labeling predicates and arguments of a
sentence with shallow semantic labels character-
izing “Who did What to Whom, How, When and
Where?” (Palmer et al., 2010). These rich se-
mantic representations are useful in many applica-
tions such as question answering (Shen and Lap-
ata, 2007) and information extraction (Christensen
et al., 2011), hence gaining a lot of attention in re-
cent years (Zhou and Xu, 2015; Täckström et al.,
2015; Roth and Lapata, 2016; Marcheggiani et al.,
2017). Since the process of creating annotated re-
sources needs significant manual effort, SRL re-
sources are available for a relative small num-
ber of languages such as English (Palmer et al.,
2005), German (Erk et al., 2003), Arabic (Za-
ghouani et al., 2010) and Hindi (Vaidya et al.,
2011). However, most languages still lack SRL
systems. There have been some efforts to use
information from a resource-rich language to de-
velop SRL systems for resource-poor languages.
Transfer methods address this problem by trans-
ferring information from a resource-rich language

(e.g. English) to a resource-poor language.
Annotation projection is a popular transfer

method that transfers supervised annotations from
a source language to a target language through
parallel data. Unfortunately this technique is not
as straightforward as it seems, e.g. translation
shifts lead to erroneous projections and accord-
ingly affecting the performance of the SRL system
trained on these projections. Translation shifts are
typically a result of the differences in word order
and the semantic divergences between the source
and target languages. In addition to translation
shifts, there are errors that occur in translations,
automatic word alignments as well as automatic
semantic roles, hence we observe a cascade of er-
ror effect.

In this paper, we introduce a new approach for
a dependency-based SRL system based on anno-
tation projection without any semantically anno-
tated data for a target language. We primarily fo-
cus on improving the quality of annotation projec-
tion by using translation cues automatically dis-
covered from word alignments. We show that ex-
clusively relying on partially projected data does
not yield good performance. We improve over
the baseline by filtering irrelevant projections, it-
erative bootstrapping with relabeling, and weight-
ing each projection instance differently with data-
dependent cost-sensitive training.

In short, contributions of this paper can be sum-
marized as follows; We introduce a weighting al-
gorithm to improve annotation projection based
on cues obtained from syntactic and translation
information. In other words, instead of utilizing
manually-defined rules to filter projections, we de-
fine and use a customized cost function to train
over noisy projected instances. This newly de-
fined cost function helps the system weight some
projections over other instances. We then utilize
this algorithm in a bootstrapping framework. Un-

13



like traditional bootstrapping, ours relabels every
training instance (including labeled data) in every
self-training round. Our final model on transfer-
ring from English to German yields a 3.5 absolute
improvement labeled F-score over a standard an-
notation projection method.

2 Our Approach

We aim to develop a dependency-based SRL sys-
tem which makes use of training instances pro-
jected from a source language (SLang) onto a tar-
get language (TLang) through parallel data. Our
SRL system is formed as a pipeline of classifiers
consisting of a predicate identification and dis-
ambiguation module, an argument identification
module, and an argument classification module.
In particular, we use our re-implementation of the
greedy (local) model of Björkelund et al. (2009)
except that we use an averaged perceptron algo-
rithm (Freund and Schapire, 1999) as the learning
algorithm.

2.1 Baseline Model

As our baseline, we apply automatic word align-
ment on parallel data and preserve the intersected
alignments from the source-to-target and target-
to-source directions. As our next step, we de-
fine a projection density criteria to filter some of
the projected sentences. Given a target sentence
from TLang with w words where f words have
alignments (f ≤ w), if the source sentence from
SLang has p predicates for which p′ of them are
projected (p′ ≤ p), we define projection den-
sity as (p′ × f)/(p × w) and prune out sentences
with a density value less than a certain threshold.
The threshold value is empirically determined dur-
ing tuning experiments performed on the develop-
ment data. In this criteria, the denominator shows
the maximum number of training instances that
could be obtained by projection and the nomina-
tor shows the actual number of relevant instances
that are used in our model. In addition to speed-
ing up the training process, filtering sparse align-
ments helps remove sentence pairs with a signif-
icant translation shifts. Thereafter, a supervised
model is trained directly on the projected data.

2.2 Model Improvements

As already mentioned, the quality of projected
roles is highly dependent on different factors in-
cluding translation shifts, errors in automatic word

alignments and the SLang supervised SRL sys-
tem. In order to address these problems, we apply
the following techniques to improve learning from
partial and noisy projections, thereby enhancing
the performance of our model:

• Bootstrapping to make use of unlabeled data;

• Determining the quality of a particular pro-
jected semantic dependency based on two
factors: 1) source-target syntactic correspon-
dence; and, 2) projection completeness de-
gree. We utilize the above constraints in the
form of a data-dependent cost-sensitive train-
ing objective. This way the classifier would
be able to learn translation shifts and erro-
neous instances in the projected data, hence
enhancing the overall performance of the sys-
tem.

Bootstrapping Bootstrapping (or self-training)
is a simple but very useful technique that makes
use of unlabeled data. A traditional self-training
method (McClosky et al., 2006) labels unlabeled
data (in our case, fill in missing SRL decisions)
and adds that data to the labeled data for further
training. We report results for this setting in §3.1
as fill–in. Although fill–in method is shown to be
very useful in previous work (Akbik et al., 2015),
empirically, we find that it is better to relabel all
training instances (including the already labeled
data) instead of only labeling unlabeled raw data.
Therefore, the classifier is empowered to discover
outliers (resulting from erroneous projections) and
change their labels during the training process.
Figure 1 illustrates our algorithm. It starts with
training on the labeled data and uses the trained
model to label the unlabeled data and relabel the
already labeled data. This process repeats for a
certain number of epochs until the model con-
verges, i.e., reaches its maximum performance.

Data-dependent cost-sensitive training In our
baseline approach, we use the standard perceptron
training. In other words, whenever the algorithm
sees a training instance xi with its corresponding
label yi, it updates the weight vector θ for itera-
tion t based on the difference between the feature
vector φ(xi, yi) of the gold label and the feature
vector φ(xi, y∗i ) of the predicted label y

∗
i (Eq. 1).

θt = θt−1 + φ(xi, yi)− φ(xi, y∗i ) (1)
In Eq. 1, the algorithm assumes that every data

point xi in the training data {x1, · · · , xn} has the

14



Inputs: 1) Projected dataD = DL∪DU whereDL andDU
indicate labeled and unlabeled instances in the projected
data; 2) Number of self-training iterations m.

Algorithm:
Train model θ0 on DL
for i = 1 to m do
DU ← Label data DU with model θi−1.
DL ← Relabel data DL with model θi−1.
Train model θi on DL ∪ DU

Output: The model parameters θm.

Figure 1: The iterative bootstrapping algorithm for
training SRL on partially projected data.

same importance and the cost of wrongly predict-
ing the best label for each training instance is uni-
form. We believe this uniform update is problem-
atic especially for the transfer task in which dif-
ferent projected instances have different qualities.
To mitigate this issue, we propose a simple mod-
ification, we introduce a cost λi ∈ [0, 1] for each
training instance xi. Therefore, Eq. 1 is modified
as follows in Eq. 2.

θt = θt−1 + λi (φ(xi, yi)− φ(xi, y∗i )) (2)
In other words, the penalty of making a mis-

take by the classifier for each training instance de-
pends on the importance of that instance defined
by a certain cost. The main challenge is to de-
fine an effective cost function, especially in our
framework where we don’t have supervision. Ac-
cordingly, we experiment with the following cost
definitions:

- Projection completeness: Our observation
shows that the density of projection is a very
important indicator of projection quality. We
view it as a rough indicator of translation
shifts: the more alignments from source to
target, the less we have a chance of having
translation shifts. As an example, consider
the sentence pair extracted from English–
German Europarl corpus: “I sit here be-
tween a rock and a hard place” and its Ger-
man translation “Zwei Herzen wohnen ach
in meiner Brust” which literally reads as
“Two hearts dwell in my chest”. The only
words that are aligned (based on the output of
Giza++) are the English word “between” and
the German word “in”. In fact, the German
sentence is an idiomatic translation of the
English sentence. Consequently predicate–
argument structure of these sentences vary

tremendously; The word “sit” is predicate of
the English sentence while “wohnen (dwell)”
is the predicate of the German sentence.

We use the definition of completeness from
Akbik et al. (2015) to define the sparsity cost
(λcomp): this definition deals with the propor-
tion of a verb or direct dependents of verbs in
a sentence that are labeled.

- Source-target syntactic dependency
match: We observe that when the depen-
dency label of a target word is different from
its aligned source word, there is a higher
chance of a projection mistake. However,
given the high frequency of source-target de-
pendency mismatches, it is harmful to prune
those projections that have dependency
mismatch; instead, we define a different
cost if we see a training instance with a
dependency mismatch. For an argument xi
that is projected from source argument sxi ,
we define the cost λdepi according to the
dependency of the source and target words
dep(xi) and dep(sxi) as Eq. 3.

λdepi =

{
1 if dep(xi) = dep(sxi)
0.5 otherwise

(3)

As an example, consider Fig. 2 that demon-
strates an English-German sentence pair from
EuroParl “I would urge you to endorse this”
with its German translation that literally
reads as “I ask for your approval”. As we can
see, there is a shift in translation of English
clausal complement “to endorse this” into
German equivalent “um Zustimmung (your
approaval)” which leads the difference in the
syntactic structure of source and target sen-
tences. Therefore, neither the predicate label
of English verb “endorse” nor the argument
“A2” should not be projected to the German
noun “Zustimmung”. Dashed edges between
sentences show intersected word alignments.
Here, projecting semantic role of ”endorse“
(A2) to to the word ”Zustimmmung“ through
alignment will lead to the wrong semantic
role for this word.

- Completeness + syntactic match: We em-
ploy the average of λdep and λcomp values as
defined above. This way, we simultaneously
encode both the completeness and syntactic
similarity information.

15



I would urge you to endorse this
A0 urge.01 A1 A2

A0 endorse.01 A1

Ich bitte Sie um Zustimmung

nsubj

aux

nsubj

aux

xcomp

dobj

nsubj dobj

adpmod

adpobj

Figure 2: Example of English-German sentences
from Europarl with dependency structure. Dif-
ferent dependencies are shown with dashed arcs.
Predicate–argument structure of the English sen-
tence is shown bellow each word.

3 Experiments

Data and Setting We use English as the source
and German as the target language. In our setting,
we assume to have supervised part-of-speech tag-
ging and dependency parsing models for both the
source (SLang) and target (TLang) languages. We
use the Universal part-of-speech tagset of Petrov
et al. (2011) and the Google Universal Treebank
(McDonald et al., 2013). We ignore the projection
of the AM roles to German since this particular
role does not appear in the German dataset.

We use the standard data splits in the CoNLL
shared task on SRL (Hajič et al., 2009) for eval-
uation. We replace the POS and dependency
information with the predictions from the Yara
parser (Rasooli and Tetreault, 2015) trained on the
Google Universal Treebank.1 We use the parallel
Europarl corpus (Koehn, 2005) and Giza++ (Och
and Ney, 2003) for extracting word alignments.
Since predicate senses are projected from English
to German, comparing projected senses with the
gold German predicate sense is impossible. To ad-
dress this, all evaluations are conducted using the
Gold predicate sense.

After filtering projections with density criteria
of §2.1, 29417 of the sentences are preserved.
The number of preserved sentences after filtering
sparse alignments is roughly one percent of the

1Our ideal setting is to transfer to more languages but
because of the semantic label inconsistency across CoNLL
datasets, we find it impossible to evaluate our model on more
languages. Future work should work on defining a reli-
able conversion scheme to unify the annotations in different
datasets.

Model Cost Lab. F1
Baseline × 60.3
Bootstrap–fill-in × 61.6
Bootstrap–relabel × 62.4
Bootstrap–relabel comp. 63.0(+1.0)
Bootstrap–relabel dep. 63.4(+1.8)
Bootstrap–relabel comp.+dep. 63.8(+1.3)
Supervised – 79.5

Table 1: Labeled F-score for different models
in SRL transfer from English to German using
gold predicates. Cost columns shows the use of
cost-sensitive training using projection complete-
ness (“comp.”), source-target dependency match
(“dep.”) and both (“comp.+dep.”). The numbers in
parenthesis show the absolute improvement over
the Bootstrap-fill-in method.

original parallel data (29K sentences out of 2.2M
sentences). Density threshold is set to 0.4 deter-
mined based on our tuning experiments on devel-
opment data.

3.1 Results and Discussion
Table 1 shows the results of different models
on the German evaluation data. As we can see
in the table, bootstrapping outperforms the base-
line. Interestingly, relabelling all training in-
stances (Bootstrap–relabel) gives us 0.8 absolute
improvement in F-score compared to when we
just predict over instances without a projected la-
bel (Bootstrap–fill-in). Here, the fill-in approach
would label only the German word ”um“ in Fig. 2
that does not have any projected label from the En-
glish side. While the relabeling method will over-
write all projected labels with less noisy predicted
labels.

We additionally observe that the combination
of the two cost functions improves the quality
further. Overall, the best model yields 3.5 ab-
solute improvement F-score over the baseline.
As expected, none of the approaches improves
over supervised performance. We further ana-
lyzed the effects of relabeling approach on iden-
tification and classification of non–root seman-
tic dependencies. Figure 3 shows precision, re-
call and F–score of the two most frequent seman-
tic dependencies (predicate pos + argument la-
bel): VERB+A0, VERB+A1 throughout relabel-
ing iterations. As demonstrated in the graph, both
precision and recall improve by cost-sensitive re-

16



2 4 6

0.5

0.55

0.6

VERB+A0

2 4 6
0.4

0.45

0.5

VERB+A1

precision recall f-score

Figure 3: Precision, recall and F–score of
VERB+A0 and VERB+A1 during relabeling iter-
ations on the German development data. Horizon-
tal axis shows the number of iterations and vertical
axis shows values of precision, recall and F–score.

labeling for VERB+A0. In fact, cost-sensitive
training helps the system refine irrelevant projec-
tions at each iteration and assigns more weight on
less noisy projections, hence enhancing precision.
Our analysis on VERB+A0 instances shows that
source–target dependency match percentage also
increases during iterations leading to increase the
recall. In other words, weighting projection in-
stances based on dependency match helps classi-
fier label some of the instances which were dis-
missed during projection, thereby will increase the
recall. While similar improvement in precision
is observed for VERB+A1, Figure 3 shows that
the recall is almost descending by relabeling. Our
analysis shows that unlike VERB+A0, percentage
of source–target dependency match remains al-
most steady for VERB+A1. This means that cost-
sensitive relabeling for this particular semantic de-
pendency has not been very successful in labeling
unlabeled data.

4 Related Work

There have been several studies on transferring
SRL systems (Padó and Lapata, 2005, 2009;
Mukund et al., 2010; van der Plas et al., 2011,
2014; Kozhevnikov and Titov, 2013; Akbik et al.,
2015). Padó and Lapata (2005), as one of the earli-
est studies on annotation projection for SRL using
parallel resources, apply different heuristics and
techniques to improve the quality of their model
by focusing on having better word and constituent
alignments. van der Plas et al. (2011) improve an
annotation projection model by jointly training a
transfer system for parsing and SRL. They solely
focus on fully projected annotations and train only
on verbs. In this work, we train on all predicates

as well as exploit partial annotation. Kozhevnikov
and Titov (2013) define shared feature representa-
tions between the source and target languages in
annotation projection. The benefit of using shared
representations is complementary to our work en-
couraging us to use it in future work.

Akbik et al. (2015) introduce an iterative self-
training approach using different types of linguis-
tic heuristics and alignment filters to improve the
quality of projected roles. Unlike our work that
does not use any external resources, Akbik et al.
(2015) make use of bilingual dictionaries. Our
work also leverages self-training but with a differ-
ent approach: first of all, ours does not apply any
heuristics to filter out projections. Second, it trains
and relabels all projected instances, either labeled
or unlabeled, at every epoch and does not grad-
ually introduce new unlabeled data. Instead, we
find it more useful to let the target language SRL
system rule out noisy projections via relabeling.

5 Conclusion

We described a method to improve the perfor-
mance of annotation projection in the dependency-
based SRL task utilizing a data-dependent cost-
sensitive training. Unlinke previous studies that
use manually-defined rules to filter projections, we
benefit from information obtained from projection
sparsity and syntactic similarity to weigh projec-
tions. We utilize a bootstrapping algorithm to train
a SRL system over projections. We showed that
we can get better results if we relabel the entire
train data in each iteration as opposed to only la-
beling instances without projections.

For the future work, we consider experiment-
ing with newly published Universal Proposition
Bank (Wang et al., 2017) that provides a uni-
fied labeling scheme for all languages. Given the
recent success in SRL systems with neural net-
works (Marcheggiani et al., 2017; Marcheggiani
and Titov, 2017), we plan to use them for further
improvement. We expect a similar trend by apply-
ing the same ideas in a neural SRL system.

Acknowledgments

This work has been partly funded by DARPA
LORELEI Grant and generous support by Leidos
Corp. for the 1st and 3rd authors. We would like to
acknowledge the useful comments by three anony-
mous reviewers who helped in making this publi-
cation more concise and better presented.

17



References
Alan Akbik, laura chiticariu, Marina Danilevsky, Yun-

yao Li, Shivakumar Vaithyanathan, and Huaiyu Zhu.
2015. Generating high quality proposition banks for
multilingual semantic role labeling. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 397–407. Associa-
tion for Computational Linguistics.

Anders Björkelund, Love Hafdell, and Pierre Nugues.
2009. Proceedings of the Thirteenth Conference on
Computational Natural Language Learning (CoNLL
2009): Shared Task, chapter Multilingual Semantic
Role Labeling. Association for Computational Lin-
guistics.

Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2011. An analysis of open informa-
tion extraction based on semantic role labeling. In
Proceedings of the Sixth International Conference
on Knowledge Capture, K-CAP ’11, pages 113–120,
New York, NY, USA. ACM.

Katrin Erk, Andrea Kowalski, Sebastian Padó, and
Manfred Pinkal. 2003. Towards a resource for lexi-
cal semantics: A large german corpus with extensive
semantic annotation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 537–544, Sapporo, Japan. Asso-
ciation for Computational Linguistics.

Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277–296.

Jan Hajič, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Antònia Maria Martı́, Lluı́s
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Štepánek, Pavel Straňák, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. Proceedings of
the Thirteenth Conference on Computational Nat-
ural Language Learning (CoNLL 2009): Shared
Task, chapter The CoNLL-2009 Shared Task: Syn-
tactic and Semantic Dependencies in Multiple Lan-
guages. Association for Computational Linguistics.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5, pages 79–86.

Mikhail Kozhevnikov and Ivan Titov. 2013. Cross-
lingual transfer of semantic role labeling models.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1190–1200. Association for
Computational Linguistics.

Diego Marcheggiani, Anton Frolov, and Ivan Titov.
2017. A simple and accurate syntax-agnostic neural
model for dependency-based semantic role labeling.
In Proceedings of the 21st Conference on Computa-
tional Natural Language Learning (CoNLL 2017),

pages 411–420. Association for Computational Lin-
guistics.

Diego Marcheggiani and Ivan Titov. 2017. Encoding
sentences with graph convolutional networks for se-
mantic role labeling. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1507–1516. Association
for Computational Linguistics.

David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference.

Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuz-
man Ganchev, Keith Hall, Slav Petrov, Hao
Zhang, Oscar Täckström, Claudia Bedini, Núria
Bertomeu Castelló, and Jungmee Lee. 2013. Uni-
versal dependency annotation for multilingual pars-
ing. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 92–97, Sofia, Bulgaria.
Association for Computational Linguistics.

Smruthi Mukund, Debanjan Ghosh, and Rohini Sri-
hari. 2010. Using cross-lingual projections to gener-
ate semantic role labeled annotated corpus for urdu
- a resource poor language. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 797–805. Coling
2010 Organizing Committee.

Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).

Sebastian Padó and Mirella Lapata. 2005. Cross-
lingual bootstrapping of semantic lexicons: The case
of framenet. In Proceedings of the National Con-
ference on Artificial Intelligence, volume 20, pages
1087–1092.

Sebastian Padó and Mirella Lapata. 2009. Cross-
lingual annotation projection for semantic
roles. Journal of Artificial Intelligence Research,
36(1):307–340.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Computational Linguistics, Vol-
ume 31, Number 1, March 2005.

Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic role labeling. Synthesis Lectures
on Human Language Technologies, 3(1):1–103.

Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.

Lonneke van der Plas, Marianna Apidianaki, and Chen-
hua Chen. 2014. Global methods for cross-lingual

18



semantic role and predicate labelling. In Proceed-
ings of COLING 2014, the 25th International Con-
ference on Computational Linguistics: Technical
Papers, pages 1279–1290. Dublin City University
and Association for Computational Linguistics.

Lonneke van der Plas, Paola Merlo, and James Hender-
son. 2011. Scaling up automatic cross-lingual se-
mantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 299–304. Association for Computational Lin-
guistics.

Mohammad Sadegh Rasooli and Joel Tetreault. 2015.
Yara parser: A fast and accurate dependency parser.
arXiv preprint arXiv:1503.06733.

Michael Roth and Mirella Lapata. 2016. Neural se-
mantic role labeling with dependency path embed-
dings. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1192–1202. Associa-
tion for Computational Linguistics.

Dan Shen and Mirella Lapata. 2007. Using seman-
tic roles to improve question answering. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 12–21, Prague, Czech Republic. As-
sociation for Computational Linguistics.

Oscar Täckström, Kuzman Ganchev, and Dipanjan
Das. 2015. Efficient inference and structured learn-
ing for semantic role labeling. Transactions of the
Association of Computational Linguistics, 3:29–41.

Ashwini Vaidya, Jinho Choi, Martha Palmer, and Bhu-
vana Narasimhan. 2011. Analysis of the hindi
proposition bank using dependency structure. In
Proceedings of the 5th Linguistic Annotation Work-
shop, pages 21–29, Portland, Oregon, USA. Associ-
ation for Computational Linguistics.

Chenguang Wang, Alan Akbik, laura chiticariu, Yun-
yao Li, Fei Xia, and Anbang Xu. 2017. Crowd-in-
the-loop: A hybrid approach for annotating seman-
tic roles. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1914–1923, Copenhagen, Denmark. As-
sociation for Computational Linguistics.

Wajdi Zaghouani, Mona Diab, Aous Mansouri, Sameer
Pradhan, and Martha Palmer. 2010. The revised ara-
bic propbank. In Proceedings of the Fourth Linguis-
tic Annotation Workshop, pages 222–226, Uppsala,
Sweden. Association for Computational Linguistics.

Jie Zhou and Wei Xu. 2015. End-to-end learning of
semantic role labeling using recurrent neural net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguis-
tics and the 7th International Joint Conference on

Natural Language Processing (Volume 1: Long Pa-
pers), pages 1127–1137. Association for Computa-
tional Linguistics.

19


