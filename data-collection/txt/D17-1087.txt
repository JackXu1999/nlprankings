



















































Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 835–844
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Two-Stage Synthesis Networks for Transfer Learning in Machine
Comprehension

David Golub∗
Stanford University

golubd@cs.stanford.edu

Po-Sen Huang and Xiaodong He
Microsoft Research

{pshuang, xiaohe}@microsoft.com

Li Deng†
Citadel Securities, LLC

l.deng@ieee.org

Abstract

We develop a technique for transfer learn-
ing in machine comprehension (MC) us-
ing a novel two-stage synthesis network
(SynNet). Given a high-performing MC
model in one domain, our technique aims
to answer questions about documents in
another domain, where we use no labeled
data of question-answer pairs. Using the
proposed SynNet with a pretrained model
on the SQuAD dataset, we achieve an
F1 measure of 46.6% on the challeng-
ing NewsQA dataset, approaching perfor-
mance of in-domain models (F1 measure
of 50.0%) and outperforming the out-of-
domain baseline by 7.6%, without use of
provided annotations.1

1 Introduction

Machine comprehension (MC), the ability to an-
swer questions over a provided context paragraph,
is a key task in natural language processing. The
rise of high-quality, large-scale human-annotated
datasets for this task (Rajpurkar et al., 2016;
Trischler et al., 2016) has allowed for the train-
ing of data-intensive but expressive models such
as deep neural networks (Wang et al., 2016; Xiong
et al., 2017; Seo et al., 2017). Moreover, these
datasets have the attractive quality that the answer
is a short snippet of text within the paragraph,
which narrows the search space of possible answer
spans.

However, many of these models rely on large
amounts of human-labeled data for training. Yet

∗Work performed while interning at Microsoft Research.
†Work performed when the author was at Microsoft Re-

search.
1Code will be available at https://github.com/

davidgolub/QuestionGeneration

data collection is a time-consuming and expensive
task. Moreover, direct application of a MC model
trained on one domain to answer questions over
paragraphs from another domain may suffer per-
formance degradation.

While understudied, the ability to transfer a MC
model to multiple domains is of great practical im-
portance. For instance, the ability to quickly use
a MC model trained on Wikipedia to bootstrap a
question-answering system over customer support
manuals or news articles, where there is no labeled
data, can unlock a great number of practical appli-
cations.

In this paper, we address this problem in MC
through a two-stage synthesis network (SynNet).
The SynNet generates synthetic question-answer
pairs over paragraphs in a new domain that are
then used in place of human-generated annotations
to finetune a MC model trained on the original do-
main.

The idea of generating synthetic data to aug-
ment insufficient training data has been explored
before. For example, for the target task of trans-
lation, Sennrich et al. (2016) present a method
to generate synthetic translations given real sen-
tences to refine an existing machine translation
system.

However, unlike machine translation, for tasks
like MC, we need to synthesize both the question
and answers given the context paragraph. More-
over, while the question is a syntactically fluent
natural language sentence, the answer is mostly a
salient semantic concept in the paragraph, e.g., a
named entity, an action, or a number, which is of-
ten a single word or short phrase.2 Since the an-
swer has a very different linguistic structure com-
pared to the question, it may be more appropri-

2This assumption holds for MC datasets such as SQuAD
and NewsQA, but there are exceptions in certain subdomains
of MSMARCO.

835



…
…

…

w
1

w
2

w
N

…

Passage Text Paragraph Answer Network

…

q
1

q
2

q
M

w
3

O B OI

Question Network

O

Attention Context Vector

Figure 1: Illustration of the two-stage SynNet. The SynNet
is trained to synthesize the answer and the question, given the
paragraph. The first stage of the model, an answer synthesis
module, uses a bi-directional LSTM to predict IOB tags on
the input paragraph, which mark out key semantic concepts
that are likely answers. The second stage, a question syn-
thesis module, uses a uni-directional LSTM to generate the
question, while attending on embeddings of the words in the
paragraph and IOB ids. Although multiple spans in the para-
graph could be identified as potential answers, we pick one
span when generating the question.

ate to view answers and questions as two different
types of data. Hence, the synthesis of a (question,
answer) tuple is needed.

In our approach, we decompose the process of
generating question-answer pairs into two steps,
answer generation conditioned on the paragraph,
and question generation conditioned on the para-
graph and answer. We generate the answer first be-
cause answers are usually key semantic concepts,
while questions can be viewed as a full sentence
composed to inquire the concept.

Using the proposed SynNet, we are able to
outperform a strong baseline of directly apply-
ing a high-performing MC model trained on an-
other domain. For example, when we apply our
algorithm using a pretrained model on the Stan-
ford Question-Answering Dataset (SQuAD) (Ra-
jpurkar et al., 2016), which consists of Wikipedia
articles, to answer questions on the NewsQA
dataset (Trischler et al., 2016), which consists of
CNN/Daily Mail articles, we improve the per-
formance of the SQuAD baseline from 39.0%

to 46.6% F1 and approach results of previously
published work of Trischler et al. (2016) (50.0%
F1), without use of labeled data in the new do-
main. Moreover, an error analysis reveals that we
achieve higher accuracy over the baseline on all
common question types.

2 Related Work

2.1 Question Answering

Question answering is an active area in natural lan-
guage processing with ongoing research in many
directions (Berant et al., 2013; Hill et al., 2015;
Golub and He, 2016; Chen et al., 2016; Hermann
et al., 2015). Machine comprehension, a form of
extractive question answering where the answer is
a snippet or multiple snippets of text within a con-
text paragraph, has recently attracted a lot of atten-
tion in the community. The rise of large-scale hu-
man annotated datasets with over 100,000 realistic
question-answer pairs such as SQuAD (Rajpurkar
et al., 2016), NewsQA (Trischler et al., 2016), and
MSMARCO (Nguyen et al., 2016), has led to a
large number of successful deep learning models
(Lee et al., 2016; Seo et al., 2017; Xiong et al.,
2017; Dhingra et al., 2017; Wang and Jiang, 2016).

2.2 Semi-Supervised Learning

Semi-supervised learning has a long history (c.f.
Chapelle et al. (2009) for an overview), and has
been applied to many tasks in natural language
processing such as dependency parsing (Koo et al.,
2008), sentiment analysis (Yang et al., 2015),ma-
chine translation (Sennrich et al., 2016), and se-
mantic parsing (Berant and Liang, 2014; Wang
et al., 2015; Jia and Liang, 2016). Recent
work generated synthetic annotations on unsuper-
vised data to boost the performance of both read-
ing comprehension and visual question answering
models (Yang et al., 2017; Ren et al., 2015), but on
domains with some form of annotated data. There
has also been work on generating high-quality
questions (Yuan et al., 2017; Serban et al., 2016;
Labutov et al., 2015), but not how to best use them
to train a model. In contrast, we use the two-stage
SynNet to generate data tuples to directly boost
performance of a model on a domain with no an-
notations.

2.3 Transfer Learning

Transfer learning (Pan and Yang, 2010) has been
successfully applied to numerous domains in ma-

836



chine learning, such as machine translation (Zoph
et al., 2016), computer vision, (Sharif Razavian
et al., 2014), and speech recognition (Doulaty
et al., 2015). Specifically, object recognition mod-
els trained on the large-scale ImageNet challenge
(Russakovsky et al., 2015) have proven to be ex-
cellent feature extractors for diverse tasks such as
image captioning (i.e., Lu et al. (2017); Fang et al.
(2015); Karpathy and Fei-Fei (2015)) and visual
question answering (i.e., Zhou et al. (2015); Xu
and Saenko (2016); Fukui et al. (2016); Yang et al.
(2016)), among others. In a similar fashion, we
use a model pretrained on the SQuAD dataset as a
generic feature extractor to bootstrap a QA system
on NewsQA.

3 The Transfer Learning Task for MC

We formalize the task of machine comprehension
below. Our MC model takes as input a tokenized
question q = {q0, q1, ...qn}, a context paragraph
p = {p0, p1, ...pn}, where qi, pi are words, and
learns a function f(p, q) 7→ {astart, aend} where
astart and aend are pointer indices into paragraph
p, i.e., the answer a = pastart ...paend .

Given a collection of labeled paragraph, ques-
tion, answer triples {p, q, a}ni=1 from a particular
domain s, i.e., Wikipedia articles, we can learn a
MC model fs(p, q) that is able to answer questions
in that domain.

However, when applying the model trained in
one domain to answer questions in another, the
performance may degrade. On the other hand, la-
beling data to train a model in the new domain is
expensive and time-consuming.

In this paper, we propose the task of transferring
a MC system fs(p, q) that is trained in a source do-
main to answer questions over another target do-
main, t. In the target domain t, we are given an
unlabeled set pt = {p}ki=1 of k paragraphs. Dur-
ing test time, we are given an unseen set of para-
graphs, p∗, in the target domain, over which we
would like to answer questions.

4 The Model

4.1 Two-Stage SynNet
To bootstrap our model fs we use a SynNet (Fig-
ure 1), which consists of answer synthesis and
question synthesis modules, to generate data on
pt. Our SynNet learns the conditional probability
of generating answer a = {astart, aend} and ques-
tion q = {q1, ...qn} given paragraph p, P (q, a|p).

We decompose the joint probability distribution
P (q, a|p) into a conditional probability distribu-
tion P (q|p, a)P (a|p), where we first generate the
answer a, followed by generating the question q
conditioned on the answer and paragraph.

4.1.1 Answer Synthesis Module
In our answer synthesis module we train a simple
IOB tagger to predict whether each word in the
paragraph is part of an answer or not.

More formally, given a set of words in a para-
graph p = {p1...pn}, our IOB tagging model
learns the conditional probability of labels y1...yn,
where y1 ∈ IOBSTART, IOBMID, IOBEND if a word
pi is marked as an answer by the annotator in our
train set, NONE otherwise.

We use a bi-directional Long-Short Term Mem-
ory Network (Bi-LSTM) (Hochreiter and Schmid-
huber, 1997) for tagging. Specifically, we project
each word pi 7→ p∗i into a continuous vector
space via pretrained GloVe embeddings (Penning-
ton et al., 2014). We then run a Bi-LSTM over the
word embeddings p∗1, ...p∗n to produce a context-
dependent word representation h1, ...hn, which
we feed into two fully connected layers followed
by a softmax to produce our tag likelihoods for
each word.

We select all consecutive spans where y 6=
NONE produced by the tagger as our candidate
answer chunks, which we feed into our question
synthesis module for question generation.

4.1.2 Question Synthesis Module
Our question synthesis module learns
the conditional probability of generating
question q = {q1, ...qn} given answer
a = astart, aend and paragraph p = p1...pn,
P (q1, ...qn|p1...pn, astart, aend). We decompose
the joint probability distribution of generating
all the question words q1, ...qn into gener-
ating the question one word at a time, i.e.∏n

i=1 P (qi|p, a, q1...i−1).
The model is similar to an encoder-decoder

network with attention (Bahdanau et al., 2014),
which computes the conditional probability
P (qi|p1...pn, astart, aend, q1...i−1). We run a
Bi-LSTM over the paragraph to produce context-
dependent word representations h = {h1, ...hn}.
To model where the answer is in the paragraph,
similar to Yang et al. (2017), we insert answer
information by appending a zero/one feature
to the paragraph word embeddings. Then, at

837



each time step i, a decoder network attends to
both h and the previously generated question
token qi−1 to produce a hidden representation ri.
Since paragraphs may often have named entities
and rare words not present during training, we
incorporate a copy mechanism into our models
(Gu et al., 2016).

We use an architecture motivated by latent pre-
dictor networks (Ling et al., 2016) to force the
model to learn when to copy vs. directly predict
the word, without direct supervision of what ac-
tion to choose. Specifically, at every time step i,
two latent predictors generate the probability of
generating word wi, a pointer network Cp (Vinyals
et al., 2015) which can copy a word from the
context paragraph, and a vocabulary predictor Vp
which directly generates a probability distribution
of choosing a word wi from a predefined vocab-
ulary. The likelihood of choosing predictor k at
time step i is proportional to wkri, and the like-
lihood of predicting question token qi is given by
q∗i = p

vlv(wi) + (1 − pv)lc(wi), where v rep-
resents the vocabulary predictor and c represents
the copy predictor, and l(wi) is the likelihood of
the word given by the predictor.3 For training,
since no direct supervision is given as to which
predictor to choose, we minimize the cross en-
tropy loss of producing the correct question to-
kens

∑n
j=1−log(q∗j ) by marginalizing out latent

variables using a variant of the forward-backward
algorithm (see Ling et al. (2016) for full details).

During inference, to generate a question q1...qn,
we use greedy decoding in the following manner.
At time step i, we select the most likely predictor
(Cp or Vp), followed by the most likely word qi
given the predictor. We feed the predicted word
as input at the next timestep back into the decoder
until we predict the end symbol, END, after which
we stop decoding.

4.2 Machine Comprehension Model

Our machine comprehension model f(p, q) 7→ a
learns the conditional likelihood of predicting an-
swer pointers a = {astart, aend} given paragraph
p and question q, P (a|p, q). In our experiments we
use the open-source Bi-directional Attention Flow
(BiDAF) network (Seo et al., 2017)4 since it is
one of the best-performing models on the SQuAD

3Since we only have two predictors, pc = 1− pv
4See https://github.com/allenai/bi-att-flow

Algorithm 1: Training Algorithm
Input : xs = {ps, qs, as}ni=1 triplets from

source domain s; pretrained MC
model on s,
fs(p, q) 7→ {astart, aend};
paragraphs from target domain t,
pmj=1

Output: MC model on target domain,
ft(p, q) 7→ {astart, aend}

1 Train SynNet g to maximize P (q, a|p) on
source s;

2 Generate samples xt = (q, a|p)ki=1 on text in
target domain t;

3 Use xs ∪ xt to finetune MC model fs on
domain t. For every batch sampled from xt,
sample k batches from xs;

dataset,5 although we note that our algorithm for
data synthesis can be used with any MC model.

4.3 Algorithm Overview

Having given an overview of our SynNet and a
brief overview of the MC model we describe our
training procedure, which is illustrated in Algo-
rithm 1.

4.4 Training

Our approach for transfer learning consists of sev-
eral training steps. First, given a series of labeled
examples xs = {ps, qs, as}ni=1 from domain s,
paragraphs pmj=1 from domain t, and pretrained
MC model fs(p, q), we train the SynNet gs to
maximize the likelihood of the question-answer
pairs in s.

Second, we fix our SynNet gs and we sample
xt = {pt, qt, at}ki=1 question-answer pairs on the
paragraphs in domain t. Several examples of gen-
erated questions can be found in Table 1.

We then transfer the MC model originally
learned on the source domain to the target domain
t using SGD on the synthetic data. However, since
the synthetic data is usually noisy, we alternatively
train the MC model with mini-batches from xs and
xt, which we call data-regularization. Every k
batches from x, we sample 1 batch of synthetic
data from x′, where k is a hyper-parameter, which
we set to 4. Letting the model encounter many ex-
amples from source domain s serves to regularize

5See https://rajpurkar.github.io/SQuAD-explorer/ for lat-
est results

838



Snippet of context paragraph (answer in bold) Generated questions (bold) vs. human questions
...At this point, some of these used-luxe models have
been around so long that they almost qualify as vin-
tage throwback editions. Recently, Consumer Re-
port magazine issued its list of best and worst used
cars, and divvied them up by price range ...

What magazine made best used cars in the
USAF?
Who released a list of best and worst used cars

...A high court in northern India on Friday acquitted
a wealthy businessman facing the death sentence for
the killing of a teen in a case dubbed ”the house of
horrors.“ Moninder Singh Pandher was sentenced to
death by a lower court in February. The teen was one
of 19 victims – children and ...

How many victims were in India ?
What was the amount of children murdered ?

Joe Pantoliano has met with the Obama and Mc-
Cain camps to promote mental health and recov-
ery. Pantoliano, founder and president of the eight-
month-old advocacy organization No Kidding, Me
Too, released a teaser of his new film about various
forms of mental illness...

Which two groups did Joe Pantoliano meet with?
Who did he meet with to discuss the issue?

...Former boxing champion Vernon Forrest , 38 , was
shot and killed in southwest Atlanta , Georgia , on
July 25 . A grand jury indicted the three suspects –
Charman Sinkfield , 30 ; Demario Ware , 20 ; and
Jquante...

Where was the first person to be shot ?
Where was Forrest killed?

Table 1: Randomly sampled paragraphs and corresponding synthetic vs. human questions from the
NewsQA train set. Human-selected answers from the train set were used as input.

the distribution of the synthetic data in the target
domain with real data from s. We checkpoint fine-
tuned model f∗s every i mini-batches, i = 1000 in
our experiments, and save a copy of the model at
each checkpoint.

At test time, to generate an answer, we feed
paragraph p = {p0, p1, ...pn} and question q
through our finetuned MC model f∗(p, q) to get
P (pi = astart), P (pi = aend) for all i ∈
1...n. We then use dynamic programming (Seo
et al., 2017) to find the optimal answer span
{astart, aend}. To improve the stability of using
our model for inference, we average the predicted
answer likelihoods from model copies at differ-
ent checkpoints prior to running the dynamic pro-
gramming algorithm.

5 Experimental Setup

We summarize the datasets we use in our experi-
ments, parameters for our model architectures, and
training details.

The SQuAD dataset consists of approximately
100,000 question-answer pairs on Wikipedia,
87,600 of which are used for training, 10,570 for
development, and an unknown number in a hidden
test set. The NewsQA dataset consists of 92,549
train, 5,166 development and 5,165 test questions
on CNN/Daily Mail news articles. Both the do-
main type (i.e., news) and question types differ
between the two datasets. For example, an analy-

sis of a randomly generated sample of 1,000 ques-
tions from both NewsQA and SQuAD (Trischler
et al., 2016) reveals that approximately 74.1% of
questions in SQuAD require word matching or
paraphrasing to retrieve the answer, as opposed to
59.7% in NewsQA. As our test metrics, we report
two numbers, exact match (EM) and F1 score.

We train a BIDAF model on the SQuAD train
dataset and use a two-stage SynNet to finetune it
on the NewsQA train dataset.

We initialize word-embeddings for the BIDAF
model, answer synthesis module, and question
synthesis module with 300-dimensional-GloVe
vectors (Pennington et al., 2014) trained on the
840 Billion Words Common Crawl corpus. We set
all embeddings of unknown word tokens to zero.

For both the answer synthesis and question
synthesis module, we use a vocabulary of size
110,179. We use LSTMs with hidden states of size
150 for the answer module vs. those of size 100
for the question module since the answer module
is less memory intensive than the question module.

We train both the answer and question module
with Adam (Kingma and Ba, 2015) and a learning
rate of 1e-2. We train a BIDAF model with the de-
fault hyperparameters provided in the open-source
repository. To stop training of the question synthe-
sis module, after each epoch, we monitor both the
loss as well as the quality of questions generated
on the SQuAD development set. To stop training

839



of the answer synthesis module, we similarly mon-
itor predictions on the SQuAD development set.

To train the question synthesis module, we only
use the questions provided in the SQuAD train set.
However, to train the answer synthesis module,
we further augment the human-annotated labels
of each paragraph with tags from a simple NER
system6 because labels of answers provided in the
train set are underspecified, i.e., many words in the
paragraph that could be potential answers are not
labeled. Therefore, we assume any named entities
could also be potential answers of certain ques-
tions, in addition to the answers explicitly labeled
by annotators.

To generate question-answer pairs on the
NewsQA train set using the SynNet, we first
run every paragraph through our answer synthe-
sis module. We then randomly sample up to 30
candidate answers extracted by our module, which
we feed into the question synthesis module. This
results in 250,000 synthetic question-answer pairs
that we can use to finetune our MC model.

6 Experimental Results

We report the main results on the NewsQA test set
(Table 2), report brief results on SQuAD (Table 3),
conduct ablation studies (Table 4), and conduct an
error analysis.

6.1 Results

We compare to the best previously published
work, which trains BARB (Trischler et al., 2016)
and Match-LSTM (Wang and Jiang, 2016) ar-
chitectures, and a BIDAF model we train on
NewsQA. Directly applying a BIDAF model
trained on SQuAD to predict on NewsQA leads to
poor performance with an F1 measure of 39.0%,
13.2% lower than one trained on labeled NewsQA
data. Using the 2-stage SynNet already leads
to a slight boost in performance (F1 measure of
44.3%), which implies that having exposure to
the new domain via question-answer pairs pro-
vides important signal for the model during train-
ing. When we augment the answers from our an-
swer synthesis module with those from a generic
NER system to produce questions, we have an ad-
ditional 2.3% performance boost. Finally, when
we ensemble with the original model, we boost
the EM further by 0.2%. Our final system achieves
an F1 measure of 46.6%, approaching previously

6https://spacy.io/

published results of 50.0%. The results demon-
strate that using the proposed architecture and
training procedure, we can transfer a MC model
from one domain to another, without use of anno-
tated data.

We also evaluate the SynNet on the NewsQA-
to-SQuAD direction. We directly apply the best
setting from the other direction and report the re-
sult in Table 3. The SynNet improves over the
baseline by 1.6% in EM and 0.7% in F1. Lim-
ited by space, we leave out ablation studies in this
direction.

6.2 Ablation Studies

To better understand how various components in
our training procedure and model impact overall
performance we conduct several ablation studies,
as summarized in Table 4.

6.2.1 Answer Synthesis
We experiment with using the answer chunks
given in the train set, Aoracle, to generate syn-
thetic questions, versus those from an NER sys-
tem, Aner. Results in Table 4(A) show that us-
ing human-annotated answers to generate ques-
tions leads to a significant performance boost over
using answers from an answer generation module.
This supports the hypothesis that the answers hu-
mans choose to generate questions for provide im-
portant linguistic cues for finetuning the machine
comprehension model.

6.2.2 Question Synthesis
To see how copying impacts performance, we ex-
plore using the entire paragraph to generate the
question vs. only the two sentences before and
one sentence after the answer span and report re-
sults in Table 4(B). On the NewsQA train set, syn-
thetic questions that use 2 sentences contain an
average of 3.0 context words within 10 words to
the left and right of the answer chunk, those that
use the entire context have 2.1 context words, and
human generated questions only have 1.7 words.
Training with generated questions that have a large
amount of overlap with words close to the an-
swer span (i.e., those that use 2-sentences vs. en-
tire context for generation) leads to models that
perform worse, especially with synthetic answer
spans and no data regularization (35.6% F1 vs.
34.3% F1). One possible reason is that, accord-
ing to analysis in Trischler et al. (2016), signifi-
cantly more questions in the NewsQA dataset re-

840



Method System EM F1

Transfer Learning Msq (baseline) 24.9 39.0
Msq + Agen + Qgen 30.6 44.3
Msq + Agen + Aner + Qgen 32.8 46.6
Msq + Agen + Aner + Qgen + M∗sq 33.0 46.6

Supervised Learning Barb-LSTM on NewsQA (Trischler et al., 2016) 34.9 50.0
Match-LSTM on NewsQA (Trischler et al., 2016) 34.1 48.2
BIDAF on NewsQA 37.1 52.3
BIDAF on SQuAD finetuned on NewsQA 37.3 52.2

Table 2: Main Results. Exact match (EM) and span F1 scores on the NewsQA test set of a BIDAF
model finetuned with our SynNet. Msq refers to a baseline BIDAF model trained on SQuAD, Agen,
Qgen refers to using answers generated from our SynNet respectively to finetune the model on NewsQA,
Aner refers to using answers extracted from a standard NER system to generate questions. M∗sq refers to
using the baseline SQUAD model in the ensemble.

System EM F1
Mnewsqa 46.3 60.8
Mnewsqa + Snet 47.9 61.5

Table 3: NewsQA to SQuAD. Exact match (EM)
and span F1 results on SQuAD development set of
a NewsQA BIDAF model baseline vs. one fine-
tuned on SQuAD using the data generated by a
2-stage SynNet (Snet).

quire paraphrase, inference, and synthesis as op-
posed to word-matching.

6.2.3 Model Finetuning

To see how the quantity of synthetic questions
encountered during training impacts performance,
we use k = {0, 2, 4} mini-batches from SQuAD
for every synthetic mini-batch from NewsQA to
finetune our model, and average the prediction
of 4 checkpointed models during testing. As
we see from the results, letting the model to en-
counter data from human annotations, although
from another domain, serves as a key form of data-
regularization, yielding consistent improvement as
k increases. We hypothesize this is because the
data distribution of machine-generated questions
is different than human-annotated ones; our batch-
ing scheme provides a simple way to prevent over-
fitting to this distribution.

A) EM F1 B) EM F1
k=0 27.2 40.5 2s + Aner 22.8 36.1
k=2 29.8 43.9 all + Aner 27.2 40.5
k=4 30.4 44.3 2s + Aoracle 31.3 45.2

all + Aoracle 32.5 46.8

Table 4: Ablation Studies. Exact match (EM) and
span F1 results on NewsQA test set of a BIDAF
model finetuned with a 2-stage SynNet. In study
A, we vary k, the number of mini-batches from
SQuAD for every batch in NewsQA. In study B,
we set k = 0, and vary the answer type and how
much of the paragraph we use for question synthe-
sis. 2 − sent refers to using two sentences before
answer span, while all refers to using the entire
paragraph. Aner refers to using an NER system
and Aor refers to using the human-annotated an-
swers to generate questions.

6.3 Error Analysis

In this section we provide a qualitative analysis of
some of our components to help guide further re-
search in this task.

6.3.1 Answer Synthesis
We randomly sample and present a paragraph with
answers extracted by our answer synthesis module
(Tables 5 and 6). Although the module appears to
have high precision, i.e., it picks up entities such as
the “Atlantic Paranormal Society”, it misses clear
entities such as “David Schrader”, which suggests
training a system with full NER/POS tags as la-

841



They are ghost hunters , or , as they prefer to be called , para-
normal investigators . “ Ghost-Hunters ”, which airs a spe-
cial live show at 7 p.m. Halloween night , is helping lift
the stigma once attached to paranormal investigators . The
show has become so popular that the group featured in each
episode – Atlantic Paranormal Society - has spawned im-
itators across United States and affiliates in countries .
TAPS , as the “ Hunters” group is informally known , even
has its own “ Reality Radio” show , magazine , lecture tours
, T-shirts – and groupies . “ Hunters” has made creepy cool
, says David Schrader , a paranormal investigator and co-host
of “ Radio ”, a radio show that investigates paranormal activ-
ity.

Table 5: Sample predictions from our answer syn-
thesis module.

What is Oklahoma’s unemployment rate until Oklahoma City
?
What was the manager of the Oklahoma City agency ?
How many companies are in Oklahoma City ?
How many workers may Oklahoma have as fair hold ?
Who said the bureau has already hired civilians to choose
What was the average hour manager of Oklahoma City ?
How much would Oklahoma have a year to be held
What year did Oklahoma ’s census build job industry ?

Table 6: Predictions from the question synthesis
module on a subset of a paragraph.

bels would yield better results, and also explains
why augmenting synthetic data generated by Syn-
Net with such tags leads to improved performance.

6.3.2 Question Synthesis
We randomly sample synthetic questions gener-
ated by our module and present our results in Ta-
ble 6. Due to the copy mechanism, our module
has the tendency to directly use many words from
the paragraph, especially common entities, such
as “Oklahoma” in the example. Thus, one way to
generate higher-quality questions may be to intro-
duce a cost function that promotes diversity during
decoding, especially within a single paragraph. In
turn, this would expose the RC model to a larger
variety of training examples in the new domain,
which can lead to better performance.

6.3.3 Machine Comprehension Model
We examine the performance over various ques-
tion types of a finetuned BIDAF on NewsQA
vs. one trained on NewsQA vs. one trained
on SQuAD (Figure 2). Finetuning with Syn-
Net improves performance over all question types
given, with the largest performance boost on lo-
cation and person-identification questions. Simi-
larly, models trained on synthetic questions tend to

0 20 40 60 80 100
Percentage of questions with correct answers

what is (436)

what did (411)

how many (296)

who is (238)

what does (217)

what was (206)

who was (162)

where did (102)

what are (92)

where was (89)

To
p 

N-
gr

am
s

Figure 2: NewsQA accuracy of baseline BIDAF
model trained on SQuAD (light green), vs. model
finetuned with our method (red) vs. one trained
from scratch on NewsQA (dark grey).

approach in-domain performance on numeric and
person-identification questions, but still struggle
with questions that require higher-order reasoning,
i.e. those starting with “what was” or “what did”.
Designing a question generator that explicitly re-
quires such reasoning may be one way to further
bridge the gap in performance.

7 Conclusion

We introduce a two-stage SynNet for the task of
transfer learning for machine comprehension, a
task which is both challenging and of practical im-
portance. With our network and a simple training
algorithm where we generate synthetic question-
answer pairs on the target domain, we are able to
generalize a MC model from one domain to an-
other with no annotated data. We present strong
results on the NewsQA test set, improving perfor-
mance of a baseline BIDAF model by over 7.6%
F1. Through ablation studies and error analysis,
we provide insights into our methodology on the

842



SynNet and MC models that can help guide fur-
ther research in this task.

Acknowledgments

We would like to thank Yejin Choi and Luke
Zettlemoyer for helpful discussions concerning
this work.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations (ICLR).

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Empirical Methods for
Natural Language Processing (EMNLP).

Jonathan Berant and Percy Liang. 2014. Semantic
parsing via paraphrasing. In Association for Com-
putational Linguistics (ACL).

Olivier Chapelle, Bernhard Scholkopf, and Alexander
Zien. 2009. Semi-supervised learning. IEEE Trans-
actions on Neural Networks .

Danqi Chen, Jason Bolton, and Christopher D. Man-
ning. 2016. A thorough examination of the
CNN/Daily Mail reading comprehension task. In
Association for Computational Linguistics (ACL).

Bhuwan Dhingra, Hanxiao Liu, William W Cohen, and
Ruslan Salakhutdinov. 2017. Gated-attention read-
ers for text comprehension. In Association for Com-
putational Linguistics (ACL).

Mortaza Doulaty, Oscar Saz, and Thomas Hain.
2015. Data-selective transfer learning for multi-
domain speech recognition. arXiv preprint
arXiv:1509.02409 .

Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K
Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xi-
aodong He, Margaret Mitchell, and John C Platt.
2015. From captions to visual concepts and back. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR).

Akira Fukui, Dong Huk Park, Daylen Yang, Anna
Rohrbach, Trevor Darrell, and Marcus Rohrbach.
2016. Multimodal compact bilinear pooling for
visual question answering and visual grounding.
arXiv preprint arXiv:1606.01847 .

David Golub and Xiaodong He. 2016. Character-
level question answering with attention. In Em-
pirical Methods in Natural Language Processing
(EMNLP).

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Association for
Computational Linguistics (ACL).

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems (NIPS).

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2015. The Goldilocks principle: Reading
children’s books with explicit memory representa-
tions. arXiv preprint arXiv:1511.02301 .

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Computation .

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In Association for Com-
putational Linguistics (ACL).

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In International
Conference on Learning Representations (ICLR).

Terry Koo, Xavier Carreras Pérez, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Association for Computational Linguistics (ACL).

Igor Labutov, Sumit Basu, and Lucy Vanderwende.
2015. Deep questions without deep understanding.
In Association for Computational Linguistics (ACL).

Kenton Lee, Tom Kwiatkowski, Ankur Parikh, and Di-
panjan Das. 2016. Learning recurrent span repre-
sentations for extractive question answering. arXiv
preprint arXiv:1611.01436 .

Wang Ling, Edward Grefenstette, Karl Moritz Her-
mann, Tomáš Kočiskỳ, Andrew Senior, Fumin
Wang, and Phil Blunsom. 2016. Latent predictor
networks for code generation. In Association for
Computational Linguistics (ACL).

Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard
Socher. 2017. Knowing when to look: Adaptive at-
tention via a visual sentinel for image captioning. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR).

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. MS MARCO: A human generated machine
reading comprehension dataset. arXiv preprint
arXiv:1611.09268 .

Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering .

843



Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods for Nat-
ural Language Processing (EMNLP).

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Empirical Meth-
ods for Natural Language Processing (EMNLP).

Mengye Ren, Ryan Kiros, and Richard Zemel. 2015.
Exploring models and data for image question an-
swering. In Advances in Neural Information Pro-
cessing Systems (NIPS).

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, et al.
2015. Imagenet large scale visual recognition chal-
lenge. In International Journal of Computer Vision
(IJCV). Springer.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation models
with monolingual data. In Association for Compu-
tational Linguistics (ACL).

Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In International
Conference on Learning Representations (ICLR).

Iulian Vlad Serban, Alberto Garcı́a-Durán, Caglar
Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron
Courville, and Yoshua Bengio. 2016. Generating
factoid questions with recurrent neural networks:
The 30M factoid question-answer corpus. In Asso-
ciation for Computational Linguistics (ACL).

Ali Sharif Razavian, Hossein Azizpour, Josephine Sul-
livan, and Stefan Carlsson. 2014. CNN features off-
the-shelf: an astounding baseline for recognition. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition Workshops.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. NewsQA: A machine compre-
hension dataset. arXiv preprint arXiv:1611.09830 .

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems (NIPS).

Shuohang Wang and Jing Jiang. 2016. Machine com-
prehension using match-LSTM and answer pointer.
arXiv preprint arXiv:1608.07905 .

Yushi Wang, Jonathan Berant, and Percy Liang. 2015.
Building a semantic parser overnight. In Associa-
tion for Computational Linguistics (ACL).

Zhiguo Wang, Haitao Mi, Wael Hamza, and Radu Flo-
rian. 2016. Multi-perspective context matching for
machine comprehension. In Association for Com-
putational Linguistics (ACL).

Caiming Xiong, Victor Zhong, and Richard Socher.
2017. Dynamic coattention networks for question
answering. In International Conference on Learn-
ing Representations (ICLR).

Huijuan Xu and Kate Saenko. 2016. Ask, attend and
answer: Exploring question-guided spatial attention
for visual question answering. In European Confer-
ence on Computer Vision (ECCV).

Min Yang, Wenting Tu, Ziyu Lu, Wenpeng Yin, and
Kam-Pui Chow. 2015. LCCT: a semisupervised
model for sentiment classification. In North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL).

Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and
William W Cohen. 2017. Semi-supervised QA with
generative domain-adaptive nets. In Association for
Computational Linguistics (ACL).

Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,
and Alex Smola. 2016. Stacked attention networks
for image question answering. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Xingdi Yuan, Tong Wang, Caglar Gulcehre, Alessan-
dro Sordoni, Philip Bachman, Sandeep Subrama-
nian, Saizheng Zhang, and Adam Trischler. 2017.
Machine comprehension by text-to-text neural ques-
tion generation. arXiv preprint arXiv:1705.02012 .

Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar,
Arthur Szlam, and Rob Fergus. 2015. Simple base-
line for visual question answering. arXiv preprint
arXiv:1512.02167 .

Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
neural machine translation. In Association for Com-
putational Linguistics (ACL).

844


