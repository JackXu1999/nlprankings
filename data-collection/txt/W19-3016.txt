



















































Overcoming the bottleneck in traditional assessments of verbal memory: Modeling human ratings and classifying clinical group membership


Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology, pages 137–147
Minneapolis, Minnesota, June 6, 2019. c©2019 Association for Computational Linguistics

137

Overcoming the bottleneck in traditional assessments of verbal memory:
Modeling human ratings and classifying clinical group membership

Chelsea Chandler1, Peter W. Foltz1,2, Jian Cheng3, Jared C. Bernstein3, Elizabeth P. Rosenfeld3,
Alex S. Cohen4, Terje B. Holmlund5, and Brita Elvevåg5,6

1University of Colorado Boulder, {chelsea.chandler, peter.foltz}@colorado.edu
2Pearson 3Analytic Measures Inc.

4Louisiana State University
5University of Tromsø 6Norwegian Centre for eHealth Research

Abstract

Verbal memory is affected by numerous clin-
ical conditions and most neuropsychological
and clinical examinations evaluate it. How-
ever, a bottleneck exists in such endeavors be-
cause traditional methods require expert hu-
man review, and usually only a couple of test
versions exist, thus limiting the frequency of
administration and clinical applications. The
present study overcomes this bottleneck by
automating the administration, transcription,
analysis and scoring of story recall. A large
group of healthy participants (n = 120) and pa-
tients with mental illness (n = 105) interacted
with a mobile application that administered
a wide range of assessments, including ver-
bal memory. The resulting speech generated
by participants when retelling stories from the
memory task was transcribed using automatic
speech recognition tools, which was compared
with human transcriptions (overall word er-
ror rate = 21%). An assortment of surface-
level and semantic language-based features
were extracted from the verbal recalls. A fi-
nal set of three features were used to both pre-
dict expert human ratings with a ridge regres-
sion model (r = 0.88) and to differentiate pa-
tients from healthy individuals with an ensem-
ble of logistic regression classifiers (accuracy
= 76%). This is the first ‘outside of the labo-
ratory’ study to showcase the viability of the
complete pipeline of automated assessment of
verbal memory in naturalistic settings.

1 Introduction

Assessing human memory is one of the most im-
portant ways in which neurocognitive function
is established. Memory is of central interest
in numerous neurodevelopmental, neurodegener-
ative and neuropsychiatric conditions, as well as
in brain injuries that affect cortical and subcortical
brain systems (Baddeley and Wilson, 2002).

Given the importance of verbal memory, it is a
core component of the globally employed Wech-
sler Memory Scale (Wechsler, 1997). The Logi-
cal Memory subtest requires the repetition of short
stories that have been spoken by the examiner,
both immediately and after a delay. Administering
these assessments requires participants to be phys-
ically present with the examiner, who then gives
scores manually by assigning points for key words
or thematic units correctly recalled. The required
time-consuming human review combined with the
availability of only a couple of test versions limits
their use and as such contributes to the bottleneck
in the assessment of verbal memory. This unfortu-
nately translates into infrequent assessments.

Automating certain aspects of such assessments
holds promise of enabling more regular assess-
ments as well as remote ones, which may be ben-
eficial for monitoring treatment effectiveness and
may also avert tragedy. Given that the verbal
memory task is spoken, it is well-suited for autom-
atization by leveraging recent advances in speech
technology and machine learning. It has become
possible to assess not just the words generated,
but deeper measures of semantic understanding,
which can be used to develop objective and sen-
sitive metrics from the speech of patients with
dementia (Fraser and Hirst, 2016; Yancheva and
Rudzicz, 2016; Zhou et al., 2016), aphasia (Fraser
et al., 2013), autism (Losh and Gordon, 2014;
Prud’hommeaux et al., 2017; Goodkind et al.,
2018), and mental illness (Elvevåg et al., 2007,
2010; Rosenstein et al., 2015; Bedi et al., 2015;
Iter et al., 2018; Corcoran et al., 2018; Holm-
lund et al., 2019b). However, it is now time to
move beyond simple proof of concept and trans-
late such findings into viable clinical tools (Foltz
et al., 2016). Indeed, since machine learning based
approaches make it possible to mimic the actual
assessment processes employed by expert humans,



138

the modeling and prediction of cognitive functions
can be done by a machine in much the same man-
ner as by humans. Thus, the entire pipeline can be
automated from administration and transcription,
to analysis and actual scoring of memory recall.

In the present research, we applied computa-
tional approaches to the speech generated from
participants retelling stories from the verbal recall
task in order to characterize the quality of their re-
call and determine the accuracy of this characteri-
zation. The approach developed natural language
processing (NLP) measures that were designed
to align with features related to verbal memory
and story recall in order to best assess the data.
This study focused on two computational tasks:
1) automatically assigning ratings to participants’
retells based on how much of the content from the
original story they remembered, and 2) performing
a classification task to distinguish psychiatric pa-
tients from healthy participants. The study further
examined how well these measures can be incor-
porated into a full analysis pipeline starting from
data collection on a mobile platform outside of
the traditional laboratory (thus in the real-world,
perhaps noisy, environment), to automated speech
recognition (ASR), and then to the conversion of
the language to predictions of recall quality.

2 Related Work

NLP has been used in a range of clinical appli-
cations from detecting depression in twitter feeds
(Coppersmith et al., 2015) to analyzing coherence
in patient-clinician interactions (Elvevåg et al.,
2007). In each case, text is reduced to a set of
variables to relate to clinical measures of interest.

There are several classes of variables that can
encode characteristics of texts. One class of mea-
sures are considered surface features of language.
This includes counts of words, phrases, and words
related to cognitive and affective processes (Pen-
nebaker et al., 2015; Prud’hommeaux and Roark,
2011). A second class of measures examines
structural features of language, such as parses of
the syntactic structure, the probabilities that word
pairs would likely occur together (e.g., n-grams),
and the cohesion and coherence of a text. Finally,
semantic features assess the meaning expressed in
texts, such as choice of words as they relate to a
specific topic, as well as encoding the underlying
meaning of words, sentences, or whole passages.
Such measures are often based on corpora that en-

code general knowledge of the world or a domain
to measure meaning at a conceptual level rather
than through the counting of direct overlap.

Previous studies have measured story recall by
computing the distance between two pieces of text
(Lehr et al., 2012, 2013). For example, a partic-
ipant’s retell can be compared against the orig-
inal story to determine the amount of informa-
tion retained. One approach to measuring this
distance is computing a word alignment between
the texts, which relies on participants using exact
words and phrases to achieve a high memory score
(Prud’hommeaux and Roark, 2011). A more ro-
bust approach is to measure the distance in a de-
rived embedding space between two pieces of text.
Latent Semantic Analysis (LSA) (Landauer et al.,
1998) applies a singular value decomposition to
a matrix of word-document co-occurrences in a
large corpus. It then uses the cosine distance be-
tween representations which is able to account for
semantic relationships in which a participant may
make small changes in concepts such as “store”
and “market”. Recent studies (Dunn et al., 2002;
Rosenstein et al., 2014) have used LSA to success-
fully model recall data from the Logical Memory
subtest of the Wechsler Memory Scale to quantify
the degradation of performance with increasing re-
trieval intervals.

More recently, word embedding models have
been applied to assessing clinical discourse. Iter
et al. (2018) modeled the coherence of patient
discourse using LSA, word2vec (Mikolov et al.,
2013) and GloVe (Pennington et al., 2014). While
LSA derives its semantic context from a bag-of-
words across documents, the word2vec word em-
bedding model derives its representation by con-
sidering the contexts in which each word appears
by examining the window of words around each
word. This window measures context, either tak-
ing into account the order of the words in that win-
dow or independent of word order. An advantage
of the latter approach is that the method learns
both semantic associations and syntactic word or-
der.

3 Data

The present study was the result of data collection
through a mobile application for the purpose of
longitudinally tracking the mental state of psychi-
atric patients (Cohen et al., 2019; Holmlund et al.,
2019a). The application is composed of a num-



139

ber of assessment tasks that engage participants
in spoken and touch-based interactions in order
to capture daily measures of cognition, affect and
clinical state.

As part of the overall examination, participants’
verbal memory was assessed. Stories were pre-
sented orally in a male voice and the participant
was then immediately asked to retell the story
with as many details as possible. After a delay
of approximately one day, they were prompted to
retell the same story. Each participant was pre-
sented with one new story per session and all sto-
ries were sampled across participants. There were
a total of 24 different stories developed to be struc-
turally similar to the Logical Memory subtest of
the Wechsler Memory Scale-III (Wechsler, 1997).
Multiple versions were created to enable frequent
administration, as there exist only two test ver-
sions in the Wechsler Memory Scale which lim-
its the frequency of administration and hence its
clinical application. The stories were narrative in
nature and ranged from 61 to 82 words in length.
They each had two characters, a setting, an action
that caused a problem, and a resolution. An exam-
ple story is as follows:

“On Monday morning, the woman woke
up more tired than usual. When she
walked downstairs to make herself a cup
of coffee, she found her husband in the
kitchen. She was surprised because he
usually left an hour before she woke up.
Her husband greeted her and reminded
her that daylight savings time was over.
Realizing the clocks were wrong, she
happily ran upstairs and jumped back
into bed.”

Since this research concerned itself with evalu-
ating the viability of leveraging speech technolo-
gies to automate a traditional verbal memory task,
our focus was on usability engineering to ensure
a robust design that could be implemented on a
large scale, out of the controlled laboratory, and
self-administered by the participant themselves.
Therefore, the traditional matching of groups was
not considered a priority, and nor is this feasi-
ble in machine learning studies that seek sample
sizes in the thousands. Our participants comprised
105 stable patients with mental illness at a sub-
stance use treatment program and 120 undergrad-
uate students at Louisiana State University pre-
sumed to be healthy (henceforth termed ‘healthy

participants’; see Holmlund et al., 2019a for de-
tails). This research program was approved by the
relevant ethics committee (LSU Institutional Re-
view Board #3618) and participants provided their
informed written consent to this study. The 105
patients produced 750 retell responses, of which
575 were immediate retells and 175 were delayed
retells. Each patient produced between 2 and 19
retells, with an average of 7.35 and standard de-
viation of 4.50. The 120 healthy participants pro-
duced 427 retell responses, of which 216 were im-
mediate retells and 211 were delayed responses.
Each produced between 2 and 15 retells with an
average of 4.97 and standard deviation of 2.76.
The scale of the collected data was impressive in
size and quality given that an experimenter was not
present during administration.

4 Human Rating of Story Recall

The audio of the memory recalls were transcribed
by humans. Trained human raters read the tran-
scriptions and assigned scores on the quality and
amount of concepts and themes recalled, including
characters, events, dates, descriptors, and feelings.
The scores assigned were on a scale from 1 to 6,
with 1 indicating no details were recalled, and 6
indicating all major and almost all minor concepts
and themes were recalled. The responses were
rated by three trained human raters with clinical
experience. A subset (326) of the responses were
rated by two independent raters in order to verify
inter-rater reliability (r = 0.92). The high degree
of agreement suggests that the rating rubric was
reliable and thus appropriate for use in training a
machine learning algorithm.

Over all the ratings, healthy participants gener-
ally received higher ratings for the amount of con-
tent recalled from the original story. For the im-
mediate retells, they received an average rating of
4.31 (SD = 1.38) as compared to patients’ aver-
age rating of 3.15 (SD = 1.44, t = 9.5, p < .001).
The biggest differentiator between the two groups
was in delayed retell (healthy participants average
= 3.95, SD = 1.45; patients average = 2.24, SD =
1.66; t = 9.8, p < .001). Figure 1 shows that the
average ratings assigned to patients on both the
immediate and delayed retell were significantly
lower than the average ratings assigned to healthy
participants. The wide error bars indicate a large
variability in the averages among both groups.

The two groups of participants differed both in



140

Figure 1: Average ratings by participant type. Error
bars represent standard deviations of the samples.

the number of words typically spoken in a retell,
and also in the relevance of their retell to the orig-
inal story. While the histograms in figure 2 are
somewhat biased since there was an uneven break-
down in the number of samples analyzed between
the two groups, they do show that the peak of
the distribution for patients is skewed more to the
lower word counts than the peak for the healthy
participants.

A noteworthy observation from the data is the
amount of missing or silent responses. The tasks
were self-administered by the participant outside
of a traditional controlled setting, and there were
several responses that were either silent or along
the lines of “I don’t remember”. As expected, this
type of missing data was more prevalent among
patients than healthy participants, with 5% of the
patient immediate retells and 19% of the patient
delayed retells being less than 5 words or silent.
While this is a constraint in live data collection in
uncontrolled real world settings, it is a trait of re-
alistic data that it will never be perfect and forced
the creation of models capable of generating pre-
dictions on imperfect data. Instead of including
silent responses (and thus allowing a classifier to
learn that this is a trait common to patients), all
silent responses were eliminated in order to create
models that learn based on the language produc-
tion, not the lack of any language.

5 Overview of Analysis Approach

There were four major components to this study.
The first was feature engineering in order to de-
termine a set of features that could be instanti-
ated through computational NLP approaches and
would assess important aspects of recall. We nar-
rowed the large feature set down to only those

most relevant to the constructs of story memory.
Second, we built a regression model that could
predict the ratings an expert human would assign
to a story recall. Third, in order to show the pre-
dictive power of our data, we used the same fea-
tures in a classification model to predict whether
a participant was a patient or healthy participant.
Fourth, in order to fully automate the pipeline,
these analyses were completed on transcripts de-
rived using ASR rather than the human transcrip-
tions.

6 Feature Engineering

In designing NLP-based features to assess recall,
it was critical to consider what aspects were most
significant. A retell can be characterized by the
amount of information recalled, the level of de-
tail, changes in structure, as well as the quality
of expressed language. Linguistic surface features
provide indications of the overall amount of in-
formation recalled. Overuse of particular parts
of speech, such as determiners, have been shown
to provide indications of language ability, in that
certain language constructions may indicate more
sophisticated ability (Bedi et al., 2015). Retells,
however, are affected by transformations of words
within semantic memory (Kintsch, 1988). Indeed,
surface features of a story (e.g., exact wording) are
quickly lost in memory, but the gist is retained. Al-
though a story may contain the word “market”, a
person may recall it as a “store”. Thus, features
that can account for semantics may be more ef-
fective at measuring the degree to which a mem-
ory has changed, with subtle effects of synonymy.
Therefore, we investigated a variety of feature
types ranging from linguistic surface features such
as word counts to semantic features like cosine
similarity between embedded representations.

The surface features included either raw or
normalized counts of the number of tokens
(word count), types (unique word count), n-grams
(counts of word sequences of length n), or partic-
ular parts of speech. The surface features, while
not the most sophisticated, nonetheless proved to
be highly predictive. For instance, a simple count
of the tokens informed how detailed the retell was.
Whether the details aligned with the original story
or not was revealed by the more advanced surface
and semantic features. We further explored the use
of specific parts of speech and ambiguous pronoun
usage as Iter et al. (2018) concluded these are



141

Figure 2: Immediate (left) and delayed (right) retell word count histograms by participant type.

traits of disordered speech. Since our data were
composed of short responses that were fairly con-
strained in content, these features did not prove to
be especially useful.

A step beyond raw counts is overlapping sur-
face features, e.g. alignment, between the original
story and the retell (Prud’hommeaux and Roark,
2012). The number of overlapping types between
the original story and the retell measured how
many concepts were remembered. For instance, if
a retell stated that the event took place on “an af-
ternoon” when it was actually “a rainy afternoon”
in the original story, the type overlap can pick up
on a missing detail. These counts offered a seman-
tic relatedness indicator since recall of words from
the original prompt was a good measure of mem-
ory, however, more interesting were metrics that
could measure semantic similarity directly, some-
what independent of surface features.

Semantic features can be analyzed by using dif-
ferent types of embedded representations and met-
rics to score the distance between these represen-
tations. Word embeddings are widely employed to
represent the semantic content as well as syntactic
relationships of variable-length pieces of text. In
this study, we tested pre-trained word embeddings,
including word2vec and GloVe, and found that the
pre-trained word2vec Google News corpus word
embedding model (3 million 300-dimensional em-
beddings) produced results most correlated with
our data.

Calculating the cosine distance between the av-
erage (both tf-idf weighted and unweighted) of
the word embedding representations of two doc-
uments is a standard metric in NLP. We tested this
in the current study, as well as the word mover’s
distance (WMD). Cosine distance was not as ef-
fective as WMD as it tends to smooth out the im-
portance of individual words.

WMD is a good metric for analysing recall data
as it captures word meaning and how semantically

distant each word in a document is to its closest
aligned word in another document. Thus, for ver-
bal memory assessment, it provides a way to char-
acterize how much semantic change there is from
the original story to the recalled story. Put simply,
WMD finds a mapping from each word in a doc-
ument to its closest counterpart in the other and
the distance is calculated as the sum of all Eu-
clidean distances between matched words. Figure
3 illustrates the WMD calculation on document 1
(D1) and document 2 (D2) from a single source
document (D0). Ignoring stop words, the model
first finds a pairing between the most semantically
similar words in the two documents. The arrows
drawn between words in the documents represent
a matching and are labeled with their distance con-
tribution. WMD calculates a total distance as a
function of all word pairings. D1 and D2 have
an equal ‘bag of words’ distance of 0 from D0 as
there are no overlapping content tokens, but se-
mantically, D1 is much closer than D2. WMD is
a more sophisticated method than cosine distance
and has been shown to outperform it in many clas-
sification tasks (Kusner et al., 2015). For exam-
ple, we compared the embedded representation for
each participant’s retell to the embedded represen-
tation for each original story using both the cosine
metric and WMD, and overall the WMD metric
correlated -0.82 with the human ratings while the
cosine metric correlated -0.72.

A final feature considered was retell structure.
Prior work has shown that language coherence can
be useful clinically and predict risk of psychosis
onset. To measure coherence, word embeddings
are generated of n contiguous words in the retell
and the semantic similarity to the embedded rep-
resentation of the next n words is computed. Then
the window is moved ahead by one word to make
the next comparison, and then all the semantic
similarities are averaged (Elvevåg et al., 2007).
This approach provides a smoothed metric of the



142

Figure 3: Example adapted from original paper by Kus-
ner et al. (2015). The source sentence D0 and two
query sentences, D1 and D2, aligned by words, and
document distance computed by some function of to-
tal word pair distances.

cohesiveness of a retell, in that if the response is
tangential or switches topics, it was assigned a
lower overall coherence. In the present study, us-
ing a window size of four words on the retells cor-
related at 0.39 to the human ratings of the retells,
indicating that better retells tended to be more co-
herent.

7 Human Rating Prediction Models

To fully automate the modeling of memory recall,
a regression model was created that assigned a per-
formance score to a story retell, treating immedi-
ate and delayed retells as the same task. Using a
combination of univariate statistical tests and re-
cursive feature elimination on the feature set, we
identified the best combination of 3 features. They
were not collinear and accounted for aspects of
the rating task that align well with attributes that
trained humans look for when rating recall. The
features assessed the overall amount of content
generated, the direct overlap of word types with
the original story and the overall semantic change.

A ridge regression model was trained with a
regularization parameter set to 0.01. We chose
only three features to incorporate into the model
in order to derive a system that is simple and
interpretable. The three features used to gener-
ate ratings were the common types between the
original story and retell (mean regression coeffi-
cient of 3.14), the word type count in the retell
(mean regression coefficient of 2.47), and the
word mover’s distance between the original story
and retell (mean regression coefficient of -2.71).
4 shows the correlations of each of the features
to the rating given to the retell. The overall aver-
age correlation (Pearson r) with the human rating
over 10-fold cross-validation through the data was
0.88. This average correlation of 0.88 of the model

to the average human rating was in line with the
0.9 correlation between human raters. The impli-
cation is that automated assessment performs on
par with humans, and additionally is an unbiased
and convenient method. Success notwithstanding,
it should be noted that the model performed poorly
on responses that should have received low scores
because key details of the original story were not
recalled, but achieved a high word count, token
overlap, and a reasonable word mover’s distance.
For instance, when a participant was prompted to
retell the “balloon story” yet could not remem-
ber much, since prompted to talk about balloons,
they were nonetheless able to ramble on about bal-
loons, in essence ‘fooling’ the regression model.

8 Classification of Clinical Group
Membership

The ability to automatically score recall is most
definitely noteworthy, but the predictive power of
the features was additionally demonstrated with
a classification task which successfully identified
the clinical group membership of the participant.
Given that participants recalled each story twice,
three classes of features were derived from the
data: (i) how similar the initial retell was to the
original story, (ii) how similar the delayed retell
was to the original story, and (iii) how similar the
initial retell was to the delayed retell.

As mentioned in the data section above, a goal
of the current study was for the model to perform
well in participants who were unable to complete
both parts of the task. Therefore, an ensemble
classifier was necessary to retain data for partial
task completion. Each classifier made a classifica-
tion based on features derived from a single ses-
sion and the resulting subject-level classification
was made from a combination of the individual
session’s prediction probabilities. This allowed
silent or missing retells to be discarded yet still
make predictions based on language data.

Prior applications of computational approaches
in the cognitive health field have tended to perform
classifications on a session-level (Prud’hommeaux
and Roark, 2011; Rosenstein et al., 2014) rather
than examining recall over multiple sessions. It
was a goal of this research program to build
a longitudinal model of behavior of an individ-
ual participant, so while the classifiers generated
probability calculations at the session-level, all of
these probabilities were aggregated over time and



143

Figure 4: Scatter plots of our top features with human ratings. The number of common word types between the
original story and the retell has a Pearson r correlation to average human ratings of 0.86, the number of word types
in the retell has a Pearson r correlation of 0.82, and the word mover’s distance between the original story and the
retell has a Pearson r correlation of -0.82.

Patient Healthy
# immediate retells 575 216

# delayed retells 175 211
Average retells 7.35, 4.97,
per participant SD = 4.50 SD = 2.76

Range of retells [2,19] [2,15]
per participant

Table 1: Breakdown of retell counts.

tasks to make a final prediction at the subject-
level. Leave-one-out cross-validation was per-
formed across data from individual participants,
training on all participants but one, and then sub-
sequently testing on the one who was left out.
Table 1 below contains a detailed breakdown of
how many retells constituted profiles of the differ-
ent groups, excluding any silent responses or re-
sponses with less than 5 words, which resulted in a
disproportionate loss of delayed retells in patients.

The features used in the retell classifier were
the number of unique types in the retell, the num-
ber of overlapping types between the original and
retell, and the word mover’s distance between the
original and retell. Unsurprisingly, word mover’s
distance proved to be the most significant feature
in the classifier. The delayed retell classifier was
composed of the same features, but with calcula-
tions made on the delayed retell in lieu of the im-
mediate retell. The last classifier, which focused
on the change between the initial and delayed
retell utilized two features: the number of com-
mon types between the immediate and delayed
retell and the word mover’s distance between the
immediate and delayed retell.

The workflow for the ensemble classifier is
shown in figure 5. The three classifiers were lo-
gistic regression classifiers optimized individually

Figure 5: Ensemble Classifier Diagram.

at a session-level. For instance, the retell classifier
was trained on all retell features in the data, and
predicted only on these inputs. Each classifier re-
turned a tuple for each session of the probability
that the session belonged to a healthy participant,
Px(h), and to a patient, Px(p), where h repre-
sented the healthy class and p represented the pa-
tient class, x represented the classifier type, either
retell, reretell, or change, and Px(h)+Px(p) =
1. The model then summed the probabilities com-
ing from each classifier and normalized the sum-
mation to reach a final class membership probabil-
ity.

P (p) = Pretell(p) + Preretell(p) + Pchange(p)
P (h) = Pretell(h) + Preretell(h) + Pchange(h)

The final prediction was a patient if P (p)> P (h),
otherwise it was a healthy participant. Put simply,
the class with the largest overall probability was
taken as the prediction.

The model correctly classified 78% of the pa-
tients and 74% of the healthy participants. Table 2
shows the confusion matrix from this classification
model. The delayed retell classifier was the most
accurate of the three as it was the biggest differen-
tiator in performance between the two classes.

Patients misclassified as healthy had highly



144

Predicted Predicted Total
Healthy Patient

True Healthy 89 31 120
True Patient 23 82 105

Total 112 113

Table 2: Confusion matrix of ensemble classification
model. Model accuracy = 76%, precision = 73%, recall
= 80%, F1 score = 76%.

rated retells that overlapped both semantically and
at a word level with the original stories. Healthy
participants misclassified as patients had multiple
“I don’t remember” or silent responses so their
memory performance was characterised as poor.

9 Automated Speech Recognition

These results demonstrate that transcribed retells
can be accurately characterized through computa-
tional methods. However, humans transcriptions
in real-time are not practically viable. There-
fore, to test how the methods would work in a
fully automated pipeline, the same retells as gener-
ated by ASR were assessed. The audio files were
run through two systems: (i) the latest Google
Speech API (https://cloud.google.com/speech-to-
text) which is a deep learning-based model trained
on general English language, and (ii) a task-
specific model that used a Deep Neural Network
- Hidden Markov Model (Zhang et al., 2014) con-
taining 5 hidden layers and 350 p-norm (p = 2)
non-linearity neurons with a group size G = 10 per
hidden layer, trained using Librispeech’s (Panay-
otov et al., 2015) 960 hours of clean native (L1)
reading data (Cheng, 2018). No speech data from
the current dataset were used to train this acous-
tic model, but a 5-gram model based on the retells
was used for the recognition.

Using Google’s acoustic model, the average
word error rate compared to human transcriptions
across all patient retells was 26.51% and 16.38%
across all healthy participant retells, totalling
20.90% on average. Using the task-specific model,
the average patient word error rate was substan-
tially less at 13.36% and the average healthy par-
ticipant word error rate was 5.90% with an over-
all average of 10.79%. Some word errors were
due to different word normalizations, for example
“hashbrowns” versus “hash browns”. Although
transcriptions derived via ASR strayed somewhat
from human transcriptions, the same ridge regres-

sion model described above, employing the same
parameters, was then applied to the ASR-derived
transcripts. As compared to the correlation r =
0.88 on a human transcription trained and tested
regression model, the Google ASR trained and
tested model achieved an r = 0.86, and the cus-
tom ASR trained and tested model achieved an r =
0.87. The change in performance on the classifica-
tion ensemble model was similar; compared with
an accuracy of 76% on the human model, both the
Google ASR model and the custom ASR model
achieved an accuracy of 74%. Thus, even with 10-
20% word error rate, the model’s predictive per-
formance only lost a few percentage points, likely
because it captures multiple aspects of the ex-
pressed language and so is highly robust to small
errors if the overall sense is retained. The impor-
tant implication is that audio collected from par-
ticipants over mobile devices in realistic environ-
ments can be automatically transcribed with suf-
ficient accuracy to provide useful predictions. Of
note however, the nature of the current task and the
fact that the retells had all been transcribed by hu-
mans who could screen for any potentially identi-
fying information, ensured that there was zero risk
of any identifying information being uploaded to
the Google ASR system and thus critically main-
tained participant privacy. However, research that
includes sensitive information (e.g., discussion of
symptoms or things of a personal nature) must take
additional measures to comply with relevant legis-
lation and privacy protection rules.

10 Assessment Pipeline

This study - as illustrated in Figure 6 - demon-
strates the solution to the bottleneck caused by
time-consuming human review that is required in
traditional settings and the resulting infrequent ad-
ministration of verbal memory tests in current as-
sessment practice. Our methodology enables the
frequent and remote assessment of verbal mem-
ory and provides metrics of significant value in the
new era of personalized medicine (Insel, 2017).

11 Conclusion

In conclusion, this study has overcome a classic
bottleneck in traditional assessment practice and
demonstrated that the promise of a truly person-
alized medicine approach to verbal memory as-
sessment is realistic. The current study has val-
idated the metrics on scores from expert human



145

Figure 6: The complete pipeline of automated verbal memory assessment: It begins with a participant verbally
retelling a story previously presented. Next, the retell is transcribed by an automatic speech recognition system.
Once the speech is converted to text, various features are extracted, and sent to both a regression and classification
model for ratings and categorization. Once complete, actionable inferences about cognitive state can be taken.

raters, and validated the actual assessment tool in
terms of its functionality and usability. The de-
sign is demonstrably and sufficiently robust that
this assessment tool is now ready to be applied
within clinical settings to track patients longitu-
dinally and inform clinicians accordingly. Fu-
ture studies need to ‘close the triage’ by providing
semi-immediate feedback from the assessment to
the relevant entity. However, establishing the clin-
ical and behavioral implications of such new met-
rics - such that they are calibrated correctly - re-
mains an extremely complex empirical task which
will necessitate the incorporation and modeling of
multiple and dynamic data streams, as variables
should not be interpreted in isolation when action-
able clinical inferences are to be made.

12 Acknowledgements

This project was funded by grant 231395 from
the Research Council of Norway awarded to Brita
Elvevåg.

References
Alan Baddeley and Barbara A. Wilson. 2002. Prose

recall and amnesia: implications for the struc-
ture of working memory. Neuropsychologia,
(40(10)):1737–1743.

Gillinder Bedi, Facundo Carrillo, Guillermo A. Cec-
chi, Diego Fernández-Slezak, Mariano Sigman, Na-
talia B. Mota, Sidarta Ribeiro, Daniel C. Javitt,
Mauro Copelli, and Cheryl M. Corcoran. 2015.
Automated analysis of free speech predicts psy-
chosis onset in high-risk youths. npj Schizophrenia,
(1:15030).

Jian Cheng. 2018. Real-time scoring of an oral reading
assessment on mobile devices. In Proceedings In-
terspeech, Hyderabad, India, September 2–6, pages
1621–1625.

Alex S. Cohen, Taylor Fedechko, Elana Schwartz,
Thanh Le, Peter W. Foltz, Jared Bernstein, Jian

Cheng, Elizabeth Rosenfeld, Terje B. Holmlund,
and Brita Elvevåg. 2019. Ambulatory vocal acous-
tics, temporal dynamics, and serious mental illness.
Journal of Abnormal Psychology, (128):97–105.

Glen Coppersmith, Mark Dredze, Craig Harman, and
Kristy Hollingshead. 2015. From ADHD to SAD:
Analyzing the language of mental health on Twitter
through self-reported diagnoses. In Proceedings of
the 2nd ACL Workshop on Computational Linguis-
tics and Clinical Psychology: From Linguistic Sig-
nal to Clinical Reality.

Cheryl M. Corcoran, Facundo Carrillo, Diego
Fernández-Slezak, Gillinder Bedi, Casimir
Klim, Daniel C. Javitt, Carrie E. Bearden, and
Guillermo A. Cecchi. 2018. Prediction of psy-
chosis across protocols and risk cohorts using
automated language analysis. World Psychiatry,
(17(1)):67–75.

John C. Dunn, Osvaldo P. Almeida, Lee Barclay, Anna
Waterreus, and Leon Flicker. 2002. Latent seman-
tic analysis: A new method to measure prose recall.
Journal of Clinical and Experimental Neuropsychol-
ogy, (24(1)):26–35.

Brita Elvevåg, Peter W. Foltz, Mark Rosenstein, and
Lynn DeLisi. 2010. An automated method to ana-
lyze language use in patients with schizophrenia and
their first-degree relatives. Journal of Neurolinguis-
tics, (23):270–284.

Brita Elvevåg, Peter W. Foltz, Daniel R. Weinberger,
and Terry E. Goldberg. 2007. Quantifying inco-
herence in speech: An automated methodology and
novel application to schizophrenia. Schizophrenia
Research, (93(1-3)):304–316.

Peter W. Foltz, Mark Rosenstein, and Brita Elvevåg.
2016. Detecting clinically significant events through
automated language analysis: Quo imus? npj
Schizophrenia, (2:15054).

Kathleen Fraser and Graeme Hirst. 2016. Detecting
semantic changes in alzheimer’s disease with vector
space models. In LREC.

Kathleen Fraser, Frank Rudzicz, Naida Graham, and
Elizabeth Rochon. 2013. Automatic speech recogni-
tion in the diagnosis of primary progressive aphasia.

https://doi.org/10.1016/S0028-3932(01)00146-4
https://doi.org/10.1016/S0028-3932(01)00146-4
https://doi.org/10.1016/S0028-3932(01)00146-4
https://doi.org/10.1038/npjschz.2015.30
https://doi.org/10.1038/npjschz.2015.30
https://doi.org/10.21437/Interspeech.2018-34
https://doi.org/10.21437/Interspeech.2018-34
https://doi.org/10.1037/abn0000397
https://doi.org/10.1037/abn0000397
https://doi.org/10.3115/v1/W15-1201
https://doi.org/10.3115/v1/W15-1201
https://doi.org/10.3115/v1/W15-1201
https://doi.org/10.1002/wps.20491
https://doi.org/10.1002/wps.20491
https://doi.org/10.1002/wps.20491
https://doi.org/10.1076/jcen.24.1.26.965
https://doi.org/10.1076/jcen.24.1.26.965
https://doi.org/10.1016/j.jneuroling.2009.05.002
https://doi.org/10.1016/j.jneuroling.2009.05.002
https://doi.org/10.1016/j.jneuroling.2009.05.002
https://doi.org/10.1016/j.schres.2007.03.001
https://doi.org/10.1016/j.schres.2007.03.001
https://doi.org/10.1016/j.schres.2007.03.001
https://doi.org/10.1038/npjschz.2015.54
https://doi.org/10.1038/npjschz.2015.54


146

In 4th Workshop on Speech and Language Process-
ing for Assistive Technologies, pages 47–54.

Adam Goodkind, Michelle Lee, Gary E. Martin, Molly
Losh, and Klinton Bicknell. 2018. Detecting lan-
guage impairments in autism: A computational anal-
ysis of semi-structured conversations with vector se-
mantics. In Proceedings of the Society for Compu-
tation in Linguistics, volume 1.

Terje B. Holmlund, Jian Cheng, Peter W. Foltz, Alex S.
Cohen, and Brita Elvevåg. 2019b. Updating verbal
fluency analysis for the 21st century: Applications
for psychiatry. Psychiatry Research.

Terje B. Holmlund, Peter W. Foltz, Alex S. Co-
hen, H. D. Johansen, R. Sigurdsen, P. Fugelli,
D. Bergsager, Jian Cheng, Jared Bernstein, Eliza-
beth Rosenfeld, and Brita Elvevåg. 2019a. Mov-
ing psychological assessment out of the controlled
laboratory setting and into the hands of the indi-
vidual: Practical challenges. Psychological Assess-
ment, (31(3)):292–303.

Thomas R Insel. 2017. Digital phenotyping: Tech-
nology for a new science of behavior. In JAMA,
318(13), pages 1215–1216.

Dan Iter, Jong H. Yoon, and Dan Jurafsky. 2018. Auto-
matic detection of incoherent speech for diagnosing
schizophrenia. In Proceedings of the Fifth Workshop
on Computational Linguistics and Clinical Psychol-
ogy, pages 136–146.

Walter Kintsch. 1988. The role of knowledge in dis-
course comprehension: a construction-integration
model. Psychological Review, (95):163–182.

Matt Kusner, Yu Sun, Nicholas I. Kolkin, and Kil-
lian Q. Weinberger. 2015. From word embed-
dings to document distances. In Proceedings of the
32nd International Conference on Machine Learn-
ing, pages 957–966.

Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. Introduction to latent semantic analysis.
Discourse Processes, (25):259–284.

Maider Lehr, Emily Prud’hommeaux, Izhak Shafran,
and Brian Roark. 2012. Fully automated neuropsy-
chological assessment for detecting mild cognitive
impairment. Proceedings of the 13th Annual Con-
ference of the International Speech Communication
Association., 2.

Maider Lehr, Izhak Shafran, Emily Prud’hommeaux,
and Brian Roark. 2013. Discriminative joint model-
ing of lexical variation and acoustic confusion for
automated narrative retelling assessment. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
211–220.

Molly Losh and Peter C. Gordon. 2014. Quantify-
ing narrative ability in autism spectrum disorder: A
computational linguistic analysis of narrative coher-
ence. Journal of Autism and Developmental Disor-
ders.

Thomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR.

Vassil Panayotov, Guoguo Chen, Daniel Povey, and
Sanjeev Khudanpur. 2015. Librispeech: an ASR
corpus based on public domain audio books. In
ICASSP, pages 5206–5210.

James W. Pennebaker, Ryan L. Boyd, Kayla Jordan,
and Kate Blackburn. 2015. The development and
psychometric properties of LIWC2015. Austin, TX:
University of Texas at Austin.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Empiri-
cal Methods in Natural Language Processing, pages
1532–1543.

Emily T. Prud’hommeaux and Brian Roark. 2011. Ex-
traction of narrative recall patterns for neuropsycho-
logical assessment. In Proceedings of the 12th An-
nual Conference of the International Speech Com-
munication Association (Interspeech), pages 3021–
3024.

Emily T. Prud’hommeaux and Brian Roark. 2012.
Graph-based alignment of narratives for automated
neurological assessment. In Proceedings of the 2012
Workshop on Biomedical Natural Language Pro-
cessing.

Emily T. Prud’hommeaux, Jan van Santen, and Dou-
glas Gliner. 2017. Vector space models for evalu-
ating semantic fluency in autism. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Short Papers), pages 32–37.

Mark Rosenstein, Catherine Diaz-Asper, Peter W.
Foltz, and Brita Elvevåg. 2014. A computa-
tional language approach to modeling prose recall
in schizophrenia. Cortex, (55):148–166.

Mark Rosenstein, Peter W. Foltz, Lynn E DeLisi, and
Brita Elvevåg. 2015. Language as a biomarker in
those at high-risk for psychosis. Schizophrenia Re-
search, (165):249–250.

David Wechsler. 1997. Wechsler Memory Scale - Third
Edition, WMS-III: Administration and scoring man-
ual. The Psychological Corporation.

Maria Yancheva and Frank Rudzicz. 2016. Vector-
space topic models for detecting alzheimer’s dis-
ease. In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics,
pages 2337–2346.

https://doi.org/10.7275/R56W988P
https://doi.org/10.7275/R56W988P
https://doi.org/10.7275/R56W988P
https://doi.org/10.7275/R56W988P
https://doi.org/10.1016/j.psychres.2019.02.014
https://doi.org/10.1016/j.psychres.2019.02.014
https://doi.org/10.1016/j.psychres.2019.02.014
https://doi.org/10.1037/pas0000647
https://doi.org/10.1037/pas0000647
https://doi.org/10.1037/pas0000647
https://doi.org/10.1037/pas0000647
https://doi.org/10.1001/jama.2017.11295
https://doi.org/10.1001/jama.2017.11295
https://doi.org/10.18653/v1/W18-0615
https://doi.org/10.18653/v1/W18-0615
https://doi.org/10.18653/v1/W18-0615
https://doi.org/10.1037/0033-295X.95.2.163
https://doi.org/10.1037/0033-295X.95.2.163
https://doi.org/10.1037/0033-295X.95.2.163
https://doi.org/10.1007/s10803-014-2158-y
https://doi.org/10.1007/s10803-014-2158-y
https://doi.org/10.1007/s10803-014-2158-y
https://doi.org/10.1007/s10803-014-2158-y
https://doi.org/10.1109/ICASSP.2015.7178964
https://doi.org/10.1109/ICASSP.2015.7178964
https://doi.org/10.15781/T29G6Z
https://doi.org/10.15781/T29G6Z
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.18653/v1/P17-2006
https://doi.org/10.18653/v1/P17-2006
https://doi.org/10.1016/j.cortex.2014.01.021
https://doi.org/10.1016/j.cortex.2014.01.021
https://doi.org/10.1016/j.cortex.2014.01.021
https://doi.org/10.1016/j.schres.2015.04.023
https://doi.org/10.1016/j.schres.2015.04.023
https://doi.org/10.18653/v1/P16-1221
https://doi.org/10.18653/v1/P16-1221
https://doi.org/10.18653/v1/P16-1221


147

Xiaohui Zhang, Jan Trmal, Daniel Povey, and Sanjeev
Khudanpur. 2014. Improving deep neural network
acoustic models using generalized maxout networks.
In ICASSP, pages 215–219.

Luke Zhou, Kathleen Fraser, and Frank Rudzicz. 2016.
Speech recognition in alzheimers disease and in its
assessment. In Interspeech 2016, pages 1948–1952.

https://doi.org/10.1109/ICASSP.2014.6853589
https://doi.org/10.1109/ICASSP.2014.6853589
https://doi.org/10.21437/Interspeech.2016-1228
https://doi.org/10.21437/Interspeech.2016-1228

