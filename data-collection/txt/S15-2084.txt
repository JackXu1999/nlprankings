



















































SHELLFBK: An Information Retrieval-based System For Multi-Domain Sentiment Analysis


Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 502–509,
Denver, Colorado, June 4-5, 2015. c©2015 Association for Computational Linguistics

SHELLFBK: An Information Retrieval-based System For Multi-Domain
Sentiment Analysis

Mauro Dragoni
Fondazione Bruno Kessler

Via Sommarive 18
Povo, Trento

dragoni@fbk.eu

Abstract

This paper describes the SHELLFBK system
that participated in SemEval 2015 Tasks 9,
10, and 11. Our system takes a supervised
approach that builds on techniques from in-
formation retrieval. The algorithm populates
an inverted index with pseudo-documents that
encode dependency parse relationships ex-
tracted from the sentences in the training set.
Each record stored in the index is annotated
with the polarity and domain of the sentence
it represents. When the polarity or domain of
a new sentence has to be computed, the new
sentence is converted to a query that is used
to retrieve the most similar sentences from the
training set. The retrieved instances are scored
for relevance to the query. The most rele-
vant training instant is used to assign a polarity
and domain label to the new sentence. While
the results on well-formed sentences are en-
couraging, the performance obtained on short
texts like tweets demonstrate that more work
is needed in this area.

1 Introduction

Sentiment analysis is a natural language processing
task whose aim is to classify documents according to
the opinion (polarity) they express on a given sub-
ject (Pang et al., 2002). Generally speaking, sen-
timent analysis aims at determining the attitude of
a speaker or a writer with respect to a topic or the
overall tonality of a document. This task has created
a considerable interest due to its wide applications.
In recent years, the exponential increase of the Web
for exchanging public opinions about events, facts,

products, etc., has led to an extensive usage of senti-
ment analysis approaches, especially for marketing
purposes.

By formalizing the sentiment analysis problem, a
“sentiment” or “opinion” has been defined by (Liu
and Zhang, 2012) as a quintuple:

〈oj , fjk, soijkl, hi, tl〉, (1)
where oj is a target object, fjk is a feature of the
object oj , soijkl is the sentiment value of the opinion
of the opinion holder hi on feature fjk of object oj
at time tl. The value of soijkl can be positive (by
denoting a state of happiness, bliss, or satisfaction),
negative (by denoting a state of sorrow, dejection,
or disappointment), or neutral (it is not possible to
denote any particular sentiment), or a more granular
rating. The term hi encodes the opinion holder, and
tl is the time when the opinion is expressed.

Such an analysis, may be document-based, where
the positive, negative, or neutral sentiment is as-
signed to the entire document content; or it may be
sentence-based where individual sentences are ana-
lyzed separately and classified according to the dif-
ferent polarity values. In the latter case, it is often
desirable to find with a high precision the entity at-
tributes towards which the detected sentiment is di-
rected.

In the classic sentiment analysis problem, the po-
larity of each term within the document is com-
puted independently of the domain which the doc-
ument’s domain. However, conditioning term po-
larity by domain has been found to improve perfor-
mance (Blitzer et al., 2007). We illustrate the intu-
ition behind domain specific term polarity. Let us

502



consider the following example concerning the ad-
jective “small”:

1. The sideboard is small and it is not able to con-
tain a lot of stuff.

2. The small dimensions of this decoder allow to
move it easily.

In the first sentence, we considered the Furnishings
domain and, within it, the polarity of the adjective
“small” is, for sure, “negative” because it highlights
an issue of the described item. On the other hand, in
the second sentence, where we considered the Elec-
tronics domain, the polarity of such an adjective may
be considered “positive”.

Unlike the approaches already discussed in the lit-
erature (and presented in Section 2), we address the
multi-domain sentiment analysis problem by apply-
ing Information Retrieval (IR) techniques for repre-
senting information about the linguistic structure of
sentences and by taking into account both their po-
larity and the domain.

The rest of the work is structured as follows. Sec-
tion 2 presents a survey on works about sentiment
analysis. Section 3 provides a description of the
SHELLFBK system by described how information
are stored during the training phase and exploited
during the test one. Section 4 reports the system
evaluation performed on the Tasks 9, 10, and 11 pro-
posed at SemEval 2015 and, finally, Section 5 con-
cludes the paper.

2 Related Work

The topic of sentiment analysis has been studied ex-
tensively in the literature (Pang and Lee, 2008; Liu
and Zhang, 2012), where several techniques have
been proposed and validated.

Machine learning techniques are the most com-
mon approaches used for addressing this problem,
given that any existing supervised methods can be
applied to sentiment classification. For instance,
in (Pang et al., 2002) and (Pang and Lee, 2004), the
authors compared the performance of Naive-Bayes,
Maximum Entropy, and Support Vector Machines in
sentiment analysis on different features like consid-
ering only unigrams, bigrams, combination of both,
incorporating parts of speech and position informa-
tion or by taking only adjectives. Moreover, beside

the use of standard machine learning method, re-
searchers have also proposed several custom tech-
niques specifically for sentiment classification, like
the use of adapted score function based on the eval-
uation of positive or negative words in product re-
views (Dave et al., 2003), as well as by defining
weighting schemata for enhancing classification ac-
curacy (Paltoglou and Thelwall, 2010).

An obstacle to research in this direction is the
need of labeled training data, whose preparation is
a time-consuming activity. Therefore, in order to re-
duce the labeling effort, opinion words have been
used for training procedures. In (Tan et al., 2008)
and (Qiu et al., 2009b), the authors used opinion
words to label portions of informative examples for
training the classifiers. Opinion words have been ex-
ploited also for improving the accuracy of sentiment
classification, as presented in (Melville et al., 2009),
where a framework incorporating lexical knowledge
in supervised learning to enhance accuracy has been
proposed. Opinion words have been used also for
unsupervised learning approaches like the ones pre-
sented in (Taboada et al., 2011) and (Turney, 2002).

Another research direction concerns the exploita-
tion of discourse-analysis techniques. (Somasun-
daran, 2010) and (Asher et al., 2008) discuss some
discourse-based supervised and unsupervised ap-
proaches for opinion analysis; while in (Wang and
Zhou, 2010), the authors present an approach to
identify discourse relations.

The approaches presented above are applied at the
document-level, i.e., the polarity value is assigned
to the entire document content. However, for im-
proving the accuracy of the sentiment classification,
a more fine-grained analysis of the text, i.e., the sen-
timent classification of the single sentences, has to
be performed. In the case of sentence-level senti-
ment classification, two different sub-tasks have to
be addressed: (i) to determine if the sentence is sub-
jective or objective, and (ii) in the case that the sen-
tence is subjective, to determine if the opinion ex-
pressed in the sentence is positive, negative, or neu-
tral. The task of classifying a sentence as subjec-
tive or objective, called “subjectivity classification”,
has been widely discussed in the literature (Riloff et
al., 2006; Wiebe et al., 2004; Wilson et al., 2004;
Wilson et al., 2006; Yu and Hatzivassiloglou, 2003).
Once subjective sentences are identified, the same

503



methods as for sentiment classification may be ap-
plied. For example, in (Hatzivassiloglou and Wiebe,
2000) the authors consider gradable adjectives for
sentiment spotting; while in (Kim and Hovy, 2007)
and (Kim et al., 2006) the authors built models to
identify some specific types of opinions.

The growth of product reviews was the perfect
floor for using sentiment analysis techniques in mar-
keting activities. However, the issue of improving
the ability of detecting the different opinions con-
cerning the same product expressed in the same re-
view became a challenging problem. Such a task
has been faced by introducing “aspect” extraction
approaches that were able to extract, from each
sentence, which is the aspect the opinion refers
to. In the literature, many approaches have been
proposed: conditional random fields (CRF) (Jakob
and Gurevych, 2010; Lafferty et al., 2001), hid-
den Markov models (HMM) (Freitag and McCal-
lum, 2000; Jin and Ho, 2009; Jin et al., 2009), se-
quential rule mining (Liu et al., 2005), dependency
tree kernels (Wu et al., 2009), and clustering (Su et
al., 2008). In (Qiu et al., 2009a; Qiu et al., 2011), a
method was proposed to extract both opinion words
and aspects simultaneously by exploiting some syn-
tactic relations of opinion words and aspects.

A particular attention should be given also to the
application of sentiment analysis in social networks.
More and more often, people use social networks
for expressing their moods concerning their last pur-
chase or, in general, about new products. Such a
social network environment opened up new chal-
lenges due to the different ways people express their
opinions, as described by (Barbosa and Feng, 2010)
and (Bermingham and Smeaton, 2010), who men-
tion “noisy data” as one of the biggest hurdles in
analyzing social network texts.

One of the first studies on sentiment analysis on
micro-blogging websites has been discussed in (Go
et al., 2009), where the authors present a distant
supervision-based approach for sentiment classifica-
tion.

At the same time, the social dimension of the
Web opens up the opportunity to combine com-
puter science and social sciences to better recognize,
interpret, and process opinions and sentiments ex-
pressed over it. Such multi-disciplinary approach
has been called sentic computing (Cambria and Hus-

sain, 2012b). Application domains where sentic
computing has already shown its potential are the
cognitive-inspired classification of images (Cambria
and Hussain, 2012a), of texts in natural language,
and of handwritten text (Wang et al., 2013).

Finally, an interesting recent research direction is
domain adaptation, as it has been shown that senti-
ment classification is highly sensitive to the domain
from which the training data is extracted. A classi-
fier trained using opinionated documents from one
domain often performs poorly when it is applied or
tested on opinionated documents from another do-
main, as we demonstrated through the example pre-
sented in Section 1. The reason is that words and
even language constructs used in different domains
for expressing opinions can be quite different. To
make matters worse, the same word in one domain
may have positive connotations, but in another do-
main may have negative connotations; therefore, do-
main adaptation is needed. In the literature, dif-
ferent approaches related to the Multi-Domain sen-
timent analysis have been proposed. Briefly, two
main categories may be identified: (i) the transfer
of learned classifiers across different domains (Yang
et al., 2006; Blitzer et al., 2007; Pan et al., 2010;
Bollegala et al., 2013; Xia et al., 2013; Yoshida et
al., 2011), and (ii) the use of propagation of labels
through graph structures (Ponomareva and Thelwall,
2013; Tsai et al., 2013; Tai and Kao, 2013; Huang et
al., 2014). Independently of the kind of approach,
works using concepts rather than terms for repre-
senting different sentiments have been proposed.

3 The SHELLFBK System

The proposed system is based on the implementa-
tion of an IR approach for inferring both the polarity
of a sentence and, if requested, the domain to which
the sentence belongs to. The rational behind the us-
age of such an approach is that by using indexes, the
computation of the Retrieval Status Value (RSV) (da
Costa Pereira et al., 2012) of a term or expression,
automatically takes into account which are the ele-
ments that are more significant in each index with
respect to the ones that, instead, are not important
with respect to the index content. In this section, we
present the steps we carried out to implement our IR
based sentiment and theme classification system.

504



3.1 Indexes Construction
The proposed approach, with respect to a classic IR
system, does not use a single index for containing
all information, but a set of indexes are created in
order to facilitate the identification of the correct po-
larity and domain, of a sentence during the valida-
tion phase. In particular, we built the following set
of indexes:

• Polarity Indexes: from the training set, the
positive, negative, and neutral sentences have
been indexed separately.

• Domain Indexes:: a different index has been
built for each domain identified in the training
set. This way, it is possible to store information
about which terms, or expression, are relevant
for each domain.

• Mixed Indexes: by considering the multi-
domain nature of the system, this further set of
indexes allows to have, for each domain, infor-
mation about the correlation between the do-
main and the polarities. This way, we are able
to know if the same term, or expression, has the
same polarity in different domains or not.

For each sentence of the training set, we exploited
the Stanford NLP Library for extracting the depen-
dencies between the terms. Such dependencies are
then used as input for the indexing procedure.

As example, let’s consider the following sentence
extracted from the training set of the Task 9:

“I came here to reflect my happiness by fishing.”

This sentence has a positive polarity and belongs
to the “outdoor activity” domain. By applying the
Stanford parser, the dependencies that are extracted
are the following ones:

nsubj(came-2, I-1)
nsubj(reflect-5, I-1)
root(ROOT-0, came-2)
advmod(came-2, here-3)
aux(reflect-5, to-4)
xcomp(came-2, reflect-5)
poss(happiness-7, my-6)
dobj(reflect-5, happiness-7)
prep_by(reflect-5, fishing-9)

Each dependency is composed by three elements:
the name of the “relation” (R), the “governor” (G)
that is the first term of the dependency, and the “de-
pendent” (D) that is the second one. We extract,

from each dependency, the structure “field - content”
shown in Table 1 by using as example the depen-
dency “dobj(reflect-5, happiness-7)”. Such a struc-
ture is then given as input to the index.

Field Name Content
RGD “dobj-reflect-happiness”
RDG “dobj-happiness-reflect”
GD “reflect-happiness”
DG “happiness-reflect”
G “reflect”
D “happiness”

Table 1: Field structure and corresponding content stored
in the index.

The structure shown in Table 1 is created for each
dependency extracted from the sentence and the ag-
gregation of all structures are stored as final record
in the index.

3.2 Polarity and Domain Computation

Once the indexes are built, both the polarity and the
domain of each sentence that need to be evaluated,
are computed by performing a set of queries on the
indexes. In our approach, we implemented a varia-
tion of classic IR scoring formula for our purposes.
In the classical TF-IDF IR model (van Rijsbergen,
1979), the inverse document frequency value is used
for identifying which are the most significant docu-
ments with respect to a particular query. This value
is useful when we want to identify the uniqueness
of a document with respect to a term contained in
a query, with respect to the other documents stored
into the index. In our case, the scenario is different
because if a term, or expression, occurs often in the
index, this aspect has to be emphasized instead of
being discriminated. Therefore, in our scoring for-
mula we consider, as final score of a term or an ex-
pression, the document frequency (DF) value (i.e.,
the inverse of the IDF). This way, we are able to in-
fer if a particular term or expression is significant or
not for a given polarity value or domain.

The queries are built with the same procedure
used for creating the records stored in the indexes.
For each sentence to evaluate, a set of queries, one
for each dependency extracted from the sentence is
performed on the indexes and the results are aggre-

505



gated for inferring both the polarity and domain of
the sentence.

As example of how the system works, let’s con-
sider the following sentence:

“I feel good and I feel healthy.”
For simplicity, we only consider the following

two extracted dependencies:
acomp(feel-2, good-3)
acomp(feel-6, healthy-7)

From these two dependencies, we generate the
following two queries:
Q1: "RGD:"acomp-feel-good"

OR RDG:"acomp-good-feel"
OR GD:"feel-good" OR DG:"good-feel"
OR G:"feel" OR D:"good"

Q2: "RGD:"acomp-feel-healthy"
OR RDG:"acomp-healthy-feel"
OR GD:"feel-healthy" OR DG:"healthy-feel"
OR G:"feel" OR D:"healthy"

For computing the polarity of the sentence, the
queries are performed on the three indexes con-
taining polarized records: positive (POS), negative
(NEG), and neutral (NEU ). From the computed
ranks, we extract only the DF associated to each
field F contained in the query:

DF (F ) = 1/IDF (F ) (2)

where DF is the value extracted.
As a direct consequence, for each index I , the

value representing the RSV of a sentence is:

RSV (I) = DF (RGDQ1) + DF (RDGQ1)+
DF (GDQ1) + DF (DGQ1) + DF (GQ1)+

DF (DQ1) + DF (RGDQ2) + DF (RDGQ2)+
DF (GDQ2) + DF (DGQ2) + DF (GQ2)+

DF (DQ2)
(3)

Finally, the polarity of the sentence S is inferred
by considering the maximum RSV computed over
the three indexes:

Polarity(S) =
argmaxP∈POS,NEU,NEG RSV(S, P )

(4)

In case of domain assignment, given a set D of k
domains, the domain is computed by:

Domain(S) = argmax i∈1...k RSV(S, Di) (5)

4 Results

The SHELLFBK system participated in three Se-
mEval 2015 tasks: 9, 10, and 11. All three tasks
were about the sentiment analysis topic with the fol-
lowing differences:

• Task 9 (Russo et al., 2015): this task is based on
a dataset of events annotated as instantiations of
pleasant and unpleasant events previously col-
lected in psychological researches as the ones
on which human judgments converge (Lewin-
sohn and Amenson, 1978),(MacPhillamy. and
Lewinsohn, 1982). Task 9 concerns classifica-
tion of the events that are pleasant or unpleas-
ant for a person writing in first person. This
task was organized around two subtasks: (A)
identification of the polarity value associated
to an event instance, and (B) identification of
both the event instantiations and the associated
polarity values. The SHELLFBK system has
been tested on both tasks.

• Task 10 (Rosenthal et al., 2015): this task
aims to identify sentiment polarities in short
text messages contained in the Twitter micro-
blog. This task contains five subtasks: (A)
expression-level, (B) message-level, (C) topic-
related, (D) trend, and (E) a task on prior polar-
ity of terms. The SHELLFBK has been tested
only on the subtask (B).

• Task 11 (Ghosh et al., 2015): this task consists
in the classification of tweets containing irony
and metaphors. Given a set of tweets that are
rich in metaphor and irony, the goal is to deter-
mine whether the user has expressed a positive,
negative, or neutral sentiment in each, and the
degree to which this sentiment has been com-
municated. With respect to the other tasks, here
the polarity is expressed through a fine-grained
scale in the interval [-5, 5].

In the following subsections, we will briefly re-
port the performance obtained on each task.

4.1 Task 9
Table 2 reports the results obtained in Task 9. This
task consisted in the identification of the polarity of
a sentence written in first person (subtask A) and in

506



Task Precision Recall F-Measure
Subtask A 0.555 0.384 0.454
Subtask B 0.261 0.155 0.197

Table 2: Results obtained by the SHELLFBK system on
Task 9.

the identification of both the polarity and the do-
main of the sentence (subtask B). Precision, recall
and F-Measure have been computed. As expected,
the accuracy obtained on the sole prediction of the
sentence polarity is higher with respect to the one
obtained on the subtask combining the inference of
both the domain and the polarity itself. Unfortu-
nately, the recall values obtained on both subtasks
are quite low, especially for the subtask B.

4.2 Task 10

Performance obtained by the SHELLFBK system
on Task 10 have been reported in Table 3. For this
task, the SHELLFBK system has been tested only
on the message-level polarity subtask (B). By ob-
serving either the overall f-measure and the ones ob-
tained on the different portions of the dataset, the
performance of the system are too low for consider-
ing it a reliable solution for being used in contexts
where short texts are taken into account.

4.3 Task 11

Results of the proposed system concerning Task 11
are shown in Table 4. In this task, due to the fine-
grained nature of the polarity predictions, the cosine
similarity and the mean square error with respect to
the gold standard have been computed. In the first
result-line, the values obtained on the four figura-
tive categories are reported, while in the second one,
the overall results. By observing the results, for the
“Sarcasm” and “Irony” topics the obtained results
are acceptable; while, for the “Metaphor” and for
the “Other” category, both the cosine similarity and
the MSE are significantly worse with respect to the
first two. These results, either with the ones obtained
on Task 10, confirm that the analysis of short texts
is the first issue to address for improving the general
quality of the system.

5 Conclusion

In this paper, we described the SHELLFBK system
presented at SemEval 2015 that participated in Se-
mEval 2015 Tasks 9, 10, and 11. Our system makes
use of IR techniques to classify sentences by polar-
ity, domain and the joint prediction of polarity and
domain, effectively providing domain specific senti-
ment analysis. The results demonstrated that, while
on well-formed sentences the system obtained good
performance, the method performs less well on short
texts like tweets. Therefore, future work will focus
on the improvement of the system in this direction.
In future work, we intend to explore the integration
of sentiment knowledge bases (Dragoni et al., 2014)
in order to move toward a more cognitive approach.

Acknowledgments

We would like to thank the anonymous reviewers for
their helpful suggestions and comments.

References

Nicholas Asher, Farah Benamara, and Yvette Yannick
Mathieu. 2008. Distilling opinion in discourse: A
preliminary study. In COLING (Posters), pages 7–10.

Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In COLING (Posters), pages 36–44.

Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: is brevity an advan-
tage? In CIKM, pages 1833–1836.

John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
ACL, pages 187–205.

Danushka Bollegala, David J. Weir, and John A. Carroll.
2013. Cross-domain sentiment classification using a
sentiment sensitive thesaurus. IEEE Trans. Knowl.
Data Eng., 25(8):1719–1731.

Erik Cambria and Amir Hussain. 2012a. Sentic album:
Content-, concept-, and context-based online personal
photo management system. Cognitive Computation,
4(4):477–496.

Erik Cambria and Amir Hussain. 2012b. Sentic Com-
puting: Techniques, Tools, and Applications, volume 2
of SpringerBriefs in Cognitive Computation. Springer,
Dordrecht, Netherlands.

Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction

507



Task Live Journal 2014 SMS 2013 Twitter 2013 Twitter 2014 Twitter 2014 Sarcasm
Progress Test 0.3406 0.2614 0.3214 0.3220 0.3558
Task F-Measure - - - -
Overall 0.3245 - - - -

Table 3: Results obtained by the SHELLFBK on Task 10.

Mean Square Error Cosine Similarity
Sarcasm Irony Metaphor Other Sarcasm Irony Metaphor Other

Detailed Results 4.375 4.516 9.219 12.16 0.669 0.625 0.35 0.167
MSE Cosine - - - - - -

General Result 7.701 0.431 - - - - - -

Table 4: Results obtained by the SHELLFBK on Task 11.

and semantic classification of product reviews. In
WWW, pages 519–528.

Mauro Dragoni, Andrea G.B.Tettamanzi, and Célia
da Costa Pereira. 2014. Propagating and aggregating
fuzzy polarities for concept-level sentiment analysis.
Cognitive Computation, pages 1–12.

Dayne Freitag and Andrew McCallum. 2000. In-
formation extraction with hmm structures learned by
stochastic optimization. In AAAI/IAAI, pages 584–
589.

Aniruddha Ghosh, Guofu Li, Tony Veale, Paolo Rosso,
Ekaterina Shutova, Antonio Reyes, and John Barnden.
2015. SemEval-2015 task 11: Sentiment analysis of
figurative language in twitter. In Proceedings of the
9th International Workshop on Semantic Evaluation,
SemEval ’2015, Denver, Colorado, June.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Standford University.

Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299–305.

Sheng Huang, Zhendong Niu, and Chongyang Shi.
2014. Automatic construction of domain-specific sen-
timent lexicon based on constrained label propagation.
Knowl.-Based Syst., 56:191–200.

Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single and cross-domain setting
with conditional random fields. In EMNLP, pages
1035–1045.

Wei Jin and Hung Hay Ho. 2009. A novel lexicalized
HMM-based learning framework for web opinion min-
ing. In Proceedings of the 26th Annual International
Conference on Machine Learning, ICML ’09, pages
465–472, New York, NY, USA. ACM.

Wei Jin, Hung Hay Ho, and Rohini K. Srihari. 2009.
Opinionminer: a novel machine learning system for

web opinion mining and extraction. In KDD, pages
1195–1204.

Soo-Min Kim and Eduard H. Hovy. 2007. Crystal: An-
alyzing predictive opinions on the web. In EMNLP-
CoNLL, pages 1056–1064.

Soo-Min Kim, Patrick Pantel, Timothy Chklovski, and
Marco Pennacchiotti. 2006. Automatically assessing
review helpfulness. In EMNLP, pages 423–430.

John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282–289.

Peter M. Lewinsohn and Christopher S. Amenson.
1978. Some relations between pleasant and unpleas-
ant events and depression. Journal of Abnormal Psy-
chology, 87:644–654.

Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In C. C. Aggarwal and
C. X. Zhai, editors, Mining Text Data, pages 415–463.
Springer.

Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the web. In WWW, pages 342–351.

Douglas MacPhillamy, and Peter M. Lewinsohn. 1982.
The pleasant event schedule: Studies on reliability, va-
lidity, and scale intercorrelation. Journal of Counsel-
ing and Clinical Psychology, 50:363–380.

Prem Melville, Wojciech Gryc, and Richard D.
Lawrence. 2009. Sentiment analysis of blogs by com-
bining lexical knowledge with text classification. In
KDD, pages 1275–1284.

Georgios Paltoglou and Mike Thelwall. 2010. A study of
information retrieval weighting schemes for sentiment
analysis. In ACL, pages 1386–1395.

Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-

508



ment classification via spectral feature alignment. In
WWW, pages 751–760.

Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACL, pages 271–278.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP
2002, pages 79–86, Philadelphia, July.

Natalia Ponomareva and Mike Thelwall. 2013. Semi-
supervised vs. cross-domain graphs for sentiment anal-
ysis. In RANLP, pages 571–578.

Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009a.
Expanding domain sentiment lexicon through double
propagation. In IJCAI, pages 1199–1204.

Likun Qiu, Weishi Zhang, Changjian Hu, and Kai Zhao.
2009b. Selc: a self-supervised model for sentiment
classification. In CIKM, pages 929–936.

Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Linguis-
tics, 37(1):9–27.

Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe.
2006. Feature subsumption for opinion analysis. In
EMNLP, pages 440–448.

Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif M Mohammad, Alan Ritter, and Veselin Stoy-
anov. 2015. SemEval-2015 task 10: Sentiment analy-
sis in twitter. In Proceedings of the 9th International
Workshop on Semantic Evaluation, SemEval ’2015,
Denver, Colorado, June.

Irene Russo, Tommaso Caselli, and Carlo Strapparava.
2015. SemEval-2015 Task 9: CLIPEval Implicit Po-
larity of Events. In Proceedings of the 9th Inter-
national Workshop on Semantic Evaluation, SemEval
’2015, Denver, Colorado, June.

Swapna Somasundaran. 2010. Discourse-level relations
for Opinion Analysis. Ph.D. thesis, University of Pitts-
burgh.

Qi Su, Xinying Xu, Honglei Guo, Zhili Guo, Xian Wu,
Xiaoxun Zhang, Bin Swen, and Zhong Su. 2008. Hid-
den sentiment association in chinese web opinion min-
ing. In WWW, pages 959–968.

Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly D. Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computational
Linguistics, 37(2):267–307.

Yen-Jen Tai and Hung-Yu Kao. 2013. Automatic
domain-specific sentiment lexicon generation with la-
bel propagation. In iiWAS, pages 53:53–53:62. ACM.

Songbo Tan, Yuefen Wang, and Xueqi Cheng. 2008.
Combining learn-based and lexicon-based techniques
for sentiment detection without using labeled exam-
ples. In SIGIR, pages 743–744.

Angela Charng-Rurng Tsai, Chi-En Wu, Richard Tzong-
Han Tsai, and Jane Yung jen Hsu. 2013. Building a
concept-level sentiment dictionary based on common-
sense knowledge. IEEE Int. Systems, 28(2):22–30.

Célia da Costa Pereira, Mauro Dragoni, and Gabriella
Pasi 2012. Multidimensional relevance: Prioritized
aggregation in a personalized Information Retrieval
setting. Inf. Process. Manage., 48(2):340–357.

Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In ACL, pages 417–424.

Cornelis Joost van Rijsbergen. 1979. Information Re-
trieval. Butterworth.

Hongling Wang and Guodong Zhou. 2010. Topic-driven
multi-document summarization. In IALP, pages 195–
198.

Qiu Feng Wang, Erik Cambria, Cheng Lin Liu, and Amir
Hussain. 2013. Common sense knowledge for hand-
written chinese recognition. Cognitive Computation,
5(2):234–242.

Janyce Wiebe, Theresa Wilson, Rebecca F. Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277–308.

Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004.
Just how mad are you? finding strong and weak opin-
ion clauses. In AAAI, pages 761–769.

Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2006.
Recognizing strong and weak opinion clauses. Com-
putational Intelligence, 22(2):73–99.

Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In EMNLP, pages 1533–1541.

Rui Xia, Chengqing Zong, Xuelei Hu, and Erik Cambria.
2013. Feature ensemble plus sample selection: Do-
main adaptation for sentiment classification. IEEE Int.
Systems, 28(3):10–18.

Hui Yang, Jamie Callan, and Luo Si. 2006. Knowledge
transfer and opinion detection in the TREC 2006 blog
track. In TREC.

Yasuhisa Yoshida, Tsutomu Hirao, Tomoharu Iwata,
Masaaki Nagata, and Yuji Matsumoto. 2011. Trans-
fer learning for multiple-domain sentiment analysis—
identifying domain dependent/independent word po-
larity. In AAAI, pages 1286–1291.

Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP 2003, pages 129–
136, Stroudsburg, PA, USA.

509


