




































Self-Discriminative Learning for Unsupervised Document Embedding


Proceedings of NAACL-HLT 2019, pages 2465–2474
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

2465

Self-Discriminative Learning for Unsupervised Document Embedding

Hong-You Chen∗1, Chin-Hua Hu∗1, Leila Wehbe2, Shou-De Lin1
1Department of Computer Science and Information Engineering, National Taiwan University

2Machine Learning Department, Carnegie Mellon University
{b03902128, r07922028}@ntu.edu.tw,

lwehbe@cmu.edu, sdlin@csie.ntu.edu.tw

Abstract

Unsupervised document representation learn-
ing is an important task providing pre-trained
features for NLP applications. Unlike most
previous work which learn the embedding
based on self-prediction of the surface of text,
we explicitly exploit the inter-document infor-
mation and directly model the relations of doc-
uments in embedding space with a discrimi-
native network and a novel objective. Exten-
sive experiments on both small and large pub-
lic datasets show the competitiveness of the
proposed method. In evaluations on standard
document classification, our model has errors
that are relatively 5 to 13% lower than state-of-
the-art unsupervised embedding models. The
reduction in error is even more pronounced in
scarce label setting.

1 Introduction

Rapid advance in deep methods for natural lan-
guage processing has contributed to a growing
need for vector representation of documents as in-
put features. Applications for such vector repre-
sentations include machine translation (Sutskever
et al., 2014), text classification (Dai and Le, 2015),
image captioning (Mao et al., 2015), multi-lingual
document matching (Pham et al., 2015), question
answering (Rajpurkar et al., 2016), and more. This
work studies unsupervised training for encoders
that can efficiently encode long paragraph of text
into compact vectors to be used as pre-trained fea-
tures. Existing solutions are mostly based on the
assumption that a good document embedding can
be learned through modeling the intra-document
information by predicting the occurrence of terms
inside the document itself. We argue that such an
assumption might not be sufficient to obtain mean-

∗ Equally contribution.

ingful a document embedding as they do not con-
sider inter-document relationships.

Traditional document representation models
such as Bag-of-words (BoW) and TF-IDF show
competitive performance in some tasks (Wang and
Manning, 2012). However, these models treat
words as flat tokens which may neglect other use-
ful information such as word order and semantic
distance. This in turn can limit the models effec-
tiveness on more complex tasks that require deeper
level of understanding. Further, BoW models suf-
fer from high dimensionality and sparsity. This is
likely to prevent them from being used as input
features for downstream NLP tasks.

Continuous vector representations for docu-
ments are being developed. A successful thread
of work is based on the distributional hypothesis,
and use contextual information for context-word
predictions. Similar to Word2Vec (Mikolov et al.,
2013), PV (Le and Mikolov, 2014) is optimized
by predicting the next words given their contexts
in a document, but it is conditioned on a unique
document vector. Word2Vec-based methods for
computing document embeddings achieve state-
of-the-art performance on document embedding.
Such methods rely on one strong underlying as-
sumption: it is necessary to train the document
embedding to optimize the prediction of the tar-
get words in the document. In other words, the
objective requires the model to learn to predict the
target words in surface text. We argue that there
are several concerns with such a self-prediction as-
sumption.

The strategy of predicting target words there-
fore only exploits in-document information, and
do not explicitly model the inter-document dis-
tances. We believe an ideal embedding space
should also infer the relations among training doc-
uments. For example, if all documents in the
corpus are about machine learning, then the con-



2466

cept of machine learning becomes less critical in
the embedding. However, if the corpus contains
documents from different areas of computer sci-
ence, then the concept of machine learning should
be encoded in any document relevant to it. We
therefore claim that the embedding of a document
should not only depend on the document itself
but also the other documents in the corpus, even
though previous work seldom makes this consid-
eration.

In addition, accurate predictions at the lexicon
or word level do not necessarily reflect that the
”true semantics” have been learned. For example,
in IMDB dataset review No.10007:

”... the father did such a good job.”

Obviously, good can be replaced with synonyms
like nice without significantly altering the mean-
ing of the sentence. However, since the syn-
onyms are treated as independent tokens in PV and
Doc2VecC, the lexicon good must be predicted ex-
actly. Moreover, to accurately predict the final
word job, the embedding probably only needs to
know that did a good job is a very common phrase,
without having to understand the true meaning of
job. This example shows that in order to accu-
rately predict a local lexicon, the embedding might
opt to encode the syntactic relationship instead of
true semantics. Enforcing document embeddings
to make predictions at the word level could be too
strong of an objective. More specifically, we argue
that the true semantics should not only depend on
a small context, but also the relations with other
training documents at document level.

To address the above concerns we propose a
novel model for learning document embedding un-
supervisedly. In contrast with previous work (PV
and Doc2Vec), we model documents according to
two aspects.

First, we abandon the concept of context word
prediction when training an embedding model.
Instead we propose a self-supervision learning
framework to model inter-document information.
Conceptually, we use the embedding to determine
whether a sentence belongs to a document. Our
encoder is equipped with a discriminator to clas-
sify whether a sentence embedding is derived from
a document given that document’s embedding.
This explicitly enforces documents to be spread
reasonably in the embedding space without any la-
bels so that they can be discriminated. To the best

of our knowledge, this is the first deep embedding
work to explicitly model the inter-document rela-
tionship.

Second, in our approach the predictions are in-
ferred at the sentence level. This avoids the effect
of only predicting the surface meaning in word
level (e.g. good vs. nice). Unlike previous work,
our model is explicitly optimized to represent doc-
uments as combinations of sequence embedding
beyond words seen in training.

Below we summarize the key contributions:

• We present a deep and general framework
and a novel objective for learning document
representation unsupervisedly. Our models
are end-to-end, easy to implement, and flexi-
ble to extend.

• We perform experiments through sentiment
analysis and topic classification to show that
our model, referred to as self-discriminative
document embedding (SDDE), is competitive
to the state-of-the-art solutions based on tra-
ditional context-prediction objectives.

• Our extensive experiments quantitatively and
qualitatively show that SDDE learns more ef-
fective features that capture more document-
level information. To the best of our knowl-
edge, SDDE is the first deep network to
model inter-instance information at docu-
ment level.

• We further propose to evaluate unsupervised
document embedding models in weakly-
supervised classification. That is, lots of un-
labeled documents with only few labels at-
tached to some of them, which is a realistic
scenario that unsupervised embedding could
be particularly useful.

2 Related Work

Here we give an overview of other related meth-
ods on learning unsupervised text representations.
Besides BoW and TF-IDF, Latent Dirichlet Al-
location models (Deerwester et al., 1990; Blei
et al., 2003) leverage the orthogonality of high-
dimensional BoW features by clustering a prob-
abilistic BoW for latent topics.

Several models extend from Word2Vec
(Mikolov et al., 2013), using context-word
predictions for training document embedding
end-to-end. PV (Le and Mikolov, 2014) keeps



2467

a document embedding matrix in memory and is
jointly trained. The required training parameters
are linear to the number of documents and thus
prohibit PV from being trained on a large corpus.
Moreover, expensive inference for new documents
is required during testing. To address the above
concerns, Doc2VecC (Chen, 2017) combines PV
and a denoising autoencoder (DEA) (Chen et al.,
2012) with BoW vectors as global document in-
formation instead. The final document embedding
are then produced by simply averaging the jointly
trained word embedding.

Another thread of work uses two-stage pipelines
to construct sentence/document embedding from
pre-trained word embedding. Arora et al. (2017)
propose post-processing weighting strategies on
top of word embedding to build sentence represen-
tations. WME (Wu et al., 2018) propose a random
feature kernel method based on distance between
pairs of words, which also shows inter-document
information helps. However, the cost scales with
the size of training samples such that it is hard to
be applied on large-scale dataset.

There have been more embedding work on sen-
tences compared to documents. These approaches
mostly learn the sentence embedding by model-
ing the sentence-level (Kiros et al., 2015; Tang
et al., 2017b,a; Logeswaran and Lee, 2018) or
word-level (Pagliardini et al., 2018; Kenter et al.,
2016; Arora et al., 2018) distribution hypothesis
(Harris, 1954; Polajnar et al., 2015) in a large or-
dered corpus. We note that the main difference be-
tween learning embedding for sentences and doc-
uments is that documents are not ordered in a cor-
pus. Some other work model sentences with RNN
autoencoders (Hill et al., 2016a; Gan et al., 2017).
Documents often refer to long-length text contain-
ing multiple sentences, which might be hard to
model with RNNs (Pascanu et al., 2013; Jing et al.,
2017) and time-consuming on large corpus.

3 Model and Design Rationale

To facilitate downstream tasks, a document em-
bedding is required to compress useful features
into a compact representation. It is not an easy
task to learn discriminable features unsupervisedly
since validation information is not accessible for
training. We first introduce some notations:

• V: the training corpus vocabulary of size |V|;

• X = {X1, · · · , Xn}: a training corpus of docu-

ment size n = |X |, in which each document Xi
is a set of sentences Si;

• Si = {s1i , · · · , s
|xi|
i }: a document divided into

a set of sentences, of set size |Si|, in which
each sentence sji ∈ R|V|×Tj contains a sequence
of variable length Tj of word one-hot vectors
w1j , · · · ,w

Tj
j , each in R|V|×1. S is the set of

total sentences
n⋃

i=1
Si in the training corpus, of

size m = |S|;

• hw: the size of the word embedding and U ∈
Rhw×|V|: the word embedding projection ma-
trix. We use uw to denote the column in U for
word w

• hs: the size of the sentence embedding and es ∈
Rhs : the embedding of sentence s.

• di ∈ Rhs : document Xi’s embedding.

Our goal is to learn a function F : X → Rhs×n
that maps document Xi to di unsupervisedly.

Next, we formulate how SDDE represents a
document, then introduce our self-discriminative
learning procedure we use to train it.

3.1 Document Representation

We consider a document as mean of sentences,
i.e., breaking a document into several subse-
quences. We demonstrate several benefits of the
design in SDDE. First, decomposing long docu-
ments into shorter but reasonable semantic unit
(e.g., sentences) makes encoding easier and faster
since they can be processed in parallel. Simi-
lar concepts of modeling documents hierarchically
have shown benefits in some supervised tasks such
as text classification (Yang et al., 2016). It also
makes the model insensitive to document length,
which is important because length varies greatly
in real documents (see Table 1).

In training, we further propose to represent
a document during training using the average
of only a subset of its sentences. This special
sentence-level dropout is beneficial for training
by creating up to

(|Xi|
q

)
combinations for each doc-

ument, where q is the number of sentences to keep.
This enforces the local sentence representations to
capture global information of a document by rep-
resenting it with a subset of sentences in it. The
word embedding is used as globally shared build-
ing blocks for sentence embedding.



2468

For a document Xi = {sji}, or Si, the embed-
ding is derived from averaging the respective rep-
resentations of subsequences. Noted as:

di =
1

q

q∑
j=1,

s∼PSi (s)

ejs, (1)

where
es = E(s), (2)

where a sentence encoder E is introduced to pro-
duce sentence embedding for sji . In practice, sen-
tences can be obtained by simply segmenting doc-
uments with punctuation. In testing, the document
embedding is obtained by averaging all the sen-
tences in which:

d =
1

|Si|
∑
∀s∈Si

es. (3)

We note that averaging subsequences differs
from averaging of words in two aspects. First,
each sentence is encoded individually before be-
ing averaged, allowing incorporation of word or-
der into design rationale at least in a reasonable
range. Second, subsequences may have different
lengths that reveal syntactic information. To il-
lustrate, BoW/mean-of-word models suffer from
ambiguously modeling two different documents
which are similar in word distributions but differ in
some aspects of interest. Mean-of-sentence model
avoids such concern by modeling documents at the
sentence level. It could be expected that it is much
less likely to find two documents with similar sen-
tence distribution than similar word distribution.
Mean-of-sentences formulation can be smoothly
reduced to mean-of-word models (by treating each
word as a sentence) or pure sequence models (by
treating each document as a very long sentence).

3.2 Self-Discriminative Learning
Unlike PV or Doc2VecC which emphasize mod-
eling distributional information within individual
documents, we model relations across documents.
The basic idea is that we hope to learn an embed-
ding for each sentence in the document as well as
a discriminator that determines whether a sentence
belongs to a document. Self-discriminative learn-
ing uses a discriminator network D to determine
whether a sentence belongs to a document. The
aim is to learn a suitable embedding and a good
discriminator to determine if a sentence belongs to

Algorithm 1 Self-Discriminative Learning for
Unsupervised Document Embedding

Input: Documents X = {Xi}n1 , p, k, hw, hs.
Output: Function F : X → Rhs that maps
text of a document to an embedding d ∈ Rhs .

1: Compute S =
n⋃

i=1
Si from set Si of each Xi.

2: Create and initialize D, U.
3: Create and initialize E as in Eq. 5 or Eq. 6.
4: while not converge do:
5: for i = 1, . . . , n do
6: Sample s from Si.
7: Get eps ← E(s) as a positive sample.
8: Sample s1i , . . . , s

q
i from Si \ {s}.

9: for j = 1, . . . , q do
10: Get ejs ← E(sji )
11: end for
12: Get di with Eq. 1 given {ejs}qj=1.
13: for ` = 1, . . . , k do
14: Sample s′` from S \ Si.
15: Get e`s′ ← E(s′`) as a negative sample.
16: end for
17: Compute Eq. 4 given di, e

p
s , {e`s′}k`=1.

18: Backprop and update for E, D, U.
19: end for
20: end while
21: Return F (E,U),

a document. The overall procedure is summarized
in Algorithm 1.

We propose an objective that explicitly opti-
mizes SDDE towards representing a document
with mean of (encoded) sentences. To optimize
the discriminator D, we formulate it as a binary
classifier that takes pairs of document embedding
d of a document Xi and a sentence embedding es,
(d, es), as inputs. The discriminator is asked to
discriminate using d whether the sentence s be-
longs to the document Xi or the other documents
X ′ ∈ X \ {Xi}. The loss then becomes:

log(1−D(d, eps))+
k∑

`=1

E
s′∼PS ,
s′ /∈Si

[
log(D(d, e`s′))

]
,

(4)
for each document with one positive sample eps
and k negative samples of sentences es′ , where s′

are not in the sentence set Si of Xi, as s′ /∈ Si.
Note that eps is not used for d otherwise it would
be trivial to be solved by the discriminator.

The spirits of self-discriminative learning can



2469

be understood as unsupervisedly mimicking su-
pervised inference without knowledge of any label
information by treating sentences from other doc-
uments as fake/negative samples. One main con-
cern that it is possible to find similar sentences in
two different documents. Our discriminator par-
ticularly addresses this issue by optimizing for the
most discriminative sentences rather than similar
ones that might not be critical to shape the em-
bedding. To minimize the loss, the encoder would
tend to preserve the most essential feature to facil-
itate the discriminator to push away any two doc-
uments, which should encourage the embedding
points spread even more widely across the space.
This in turn should result in more ease in down-
stream tasks: for example in learning a decision
hyperplane in a classification task.

3.3 Sentence Encoder

Next, we narrow down to sentence encoder E.
Given a sequence of word one-hot vectors as a sen-
tence s = [w1, . . . ,wT ], we project them into an
embedding layer U to retrieve their corresponding
word embedding. Note that the word embedding
are trained jointly.

Our first method uses:

E(s) = φ(ReLU(G(Ust))), (5)

where G is a single-layer RNN encoder using
GRU cells to process the word embedding se-
quences and φ is a linear transform for dimension
hs of sentence embedding.

Our second method, we use a schema of mean-
of-word for advantage of fast generation, we av-
erage the word embedding w within a sentence s
along time axis as AVG encoder:

E(s) = φ(ReLU(
1

|s|

|s|∑
i=1

Uwti)), (6)

Let us stress that the role of encoderE is to extract
local feature from every sentence, and the over-
all objective encourages SDDE to represent docu-
ments as mean of sentence embedding.

3.4 Discriminator

An undesired pitfall comes from a learned weak
encoder with a powerful discriminator causing the
embedding produced by the encoder useless for
downstream tasks. To avoid such a pitfall, we

Dataset #Class #Train / #Test Doc Length Sent Length

IMDB 2 75k / 25k 124.6±8,856.7 11.6±105.5
Yelp P. 2 560k / 38k 70.0±4,117.8 8.2±48.7
AG’s News 4 120k / 7.6k 27.2±66.1 11.8±66.7
DBPedia 14 560k / 70k 32.8±231.3 9.7±68.6

Table 1: Statistics of the datasets. Length of document
and sentence in words (mean±variance), which could
be high for real-world scenarios such as online reviews.

Dataset k p hw hs

IMDB 1 3 100 100
Yelp P. 1 3 300 500
AG’s News 3 1 100 100
DBPedia 4 2 300 500

Table 2: Hyperparameters used in experiments. The
document embedding trained with the same hyperpa-
rameters are used for all the evaluations without task-
specific tuning.

adopt lightweight network structures for discrimi-
nators. For the IMDB datasets, we find inner prod-
uct (dV)tE(s) with a learnable matrix V suffi-
cient. For the other datasets in Table 1, two fully-
connected layers with ReLU activations in latent
are used.

3.5 Sampling Sentences

We relate our method to the Negative Sampling
(Mikolov et al., 2013) technique which is a simpli-
fied objective of softmax approximation (Mikolov
et al., 2013; Mnih and Teh, 2012; Zoph et al.,
2016). Negative sampling has been used as an
efficient and effective technique in learning word
embedding. We reformulate it to train document
embedding by sampling in sentence level, which
is easy to implement and efficient to train just like
Word2Vec (Mikolov et al., 2013).

In practice, when training with mini-batches the
documents for negative samples are from the same
mini-batch, which requires small extra computa-
tion efforts.

SDDE requires a similar number of parameters
as Doc2VecC does, but much less than PV. In ad-
dition, the sentence encoder is flexible and can in-
corporate other techniques of text processing such
as attention methods.

4 Experiments

4.1 Setup

Public datasets on sentiment analysis and topic
classification across diverse domains including
online reviews, news, and Wiki pages are used in-
cluding IMDB dataset (Maas et al., 2011) and the
others from Zhang et al. (2015). Table 1 provides



2470

a summary. Only the training splits are used in
training embedding with subsampled training split
for cross-validations.

We preprocess the datasets by normalizing the
text to lower class and replacing words appearing
less than 10 times with a UNK token. Out-of-
vocabulary words in testing set are also replaced.
All our baseline models use the same input data.
To define the sentences for experiment, we uti-
lize the sentence tokenizer from NLTK. For the
documents containing only one sentence we sim-
ply divide it into multiple subsequences for sam-
pling. We use RMSProp method for optimiza-
tion. Dropout 50% of input to the discriminator.
Weights are random-uniformly initialized between
[-1, 1].

The other hyperparameters are summarized in
Table 2. All the models use the same embedding
size for fair comparison. The trained document
embedding are used for all the evaluations without
specific tuning.

4.2 Evaluation

Generally, it is not easy to evaluate an unsuper-
vised embedding model. In Section 4.3 and 4.4,
we evaluate the performance on standard docu-
ment classification following the common prac-
tice used by previous work (Chen, 2017; Wu et al.,
2018; Arora et al., 2017): a classification task with
a linear SVM (Fan et al., 2008) trained on the la-
bels in each dataset. Next, we study unsupervised
document embedding on two novel aspects. In
Section 4.5, we study a weakly-supervised clas-
sification setting that fits the realistic scenario of
using unsupervised embedding with only a few la-
bels. In Section 4.6, we provide a metric to eval-
uate the effectiveness of modeling inter-document
information.

4.3 Sentiment Analysis with IMDB Dataset

We first compare our models with the others state-
of-the-art competitors. RNN-LM (Mikolov et al.,
2010) and Skip-thought (Kiros et al., 2015) are
RNN-based. SIF (Arora et al., 2017), W2V-AVG
(Mikolov et al., 2013), and WME (Wu et al., 2018)
are two-stage approach that post-processing on
word embedding. We collect the results reported
on the widely-used benchmark sentiment classifi-
cation dataset IMDB. For PV, we use Gensim im-
plementation (Řehůřek and Sojka, 2010); versions

https://www.nltk.org/

Model Error%

Skip-thought* (Kiros et al., 2015) 17.4
SIF (GloVe) (Arora et al., 2017) 15.0
RNN-LM* (Mikolov et al., 2010) 13.6
W2V-AVG* (Mikolov et al., 2013) 12.7
DEA* (Chen et al., 2012) 12.5
PV-DM 20.0
PV-DBoW 12.0
WME (Wu et al., 2018) 11.5
Doc2VecC* (Chen, 2017) 11.7

SDDE-AVG 10.6
SDDE-RNN 10.2

Table 3: Sentiment Classification on IMDB Bench-
mark. *Results are collected from Chen (2017).

Pred SDDE-RNN Doc2VecCTrue False True False

#Sent 14.8 14.7 14.5 14.8

Table 4: Mean of #sentence per document in IMDB
dataset, in groups of classification correctness.

of both Distributed Memory (DM) and Distributed
Bag of Words (DBoW) are reported. For differ-
ent encoders in SDDE, AVG is for averaging word
embedding and RNN is for the RNN encoder.

Self-Discriminative Learning is Effective
From the experiment result in Table 3, we can see
that our self-discriminative learning is effective
and superior on the document embedding models
for both AVG and RNN versions. SDDE-RNN
achieves best accuracy on IMDB dataset 1.5%
margin against Doc2VecC.

Study the Property of SDDE Unlike previous
work modeling documents on the word or short
context level, SDDE operates on the sentence
level. We study the false and true predictions
output by SVM upon SDDE in comparison with
Doc2VecC. Table 5 show some examples that have
the largest difference. We observed SDDE can
better capture contradicting or contrasting opin-
ions. We observe some wrong predictions (Row
3) are due to the ambivalent reviews. SDDE is in-
sensitive to the number of sentences; we found the
effect of the number of sentences per document
was trivial as shown in Table 4.

4.4 Large-Scale Document Classification on
More Dataset

Next, we borrow some public large-scale dataset
in Table 1 to further validate the effectiveness
of SDDE compared to the other models. For
Doc2vecC and SIF, we use the code from the au-



2471

Label: 1 i don t even like watching those late night talk shows , but i found this one really interesting . i imagine it s probably close to the truth
— it feels like an honest account , if that means anything . kinda feel for the people somewhat when you watch it . a nice movie for a
saturday night .

SDDE: 0.89
Doc2VecC: 0.27

Label: 0 i m a boorman fan but this is arguably his least successful film . comedy has never been his strong suit , and here his attempts at
screwball farce are clumsily done . still , it s almost worth seeing for boorman s eye for talent : this is one of uma thurman s first
starring roles , and as always she is ravishing to watch . on a sad side note boorman wrote the script with his daughter , <UNK> who
died a couple years ago .

SDDE: 0.12
Doc2VecC: 0.75

Label: 0 michael dudikoff stars as joe armstrong a martial artist who fights ninjas who are stealing weapons from the u s army , in this
entertaining yet admittedly brainless martial arts actioner , which is hampered by too many long pauses without action , but
helped by some high energy action <UNK> as well as steve james performance .

SDDE: 0.77
Doc2VecC: 0.10

Table 5: IMDB Examples with scores 0 to 1 for negative to positive assigned by SVM.

Model Yelp P. AG DBP.

PV-DM 17.6 39.6 21.6
PV-DBoW 12.1 16.8 10.4
Word2Vec AVG 8.0 14.2 2.7
SIF 8.4 13.8 2.9
Doc2VecC 7.4 10.4 2.0

SDDE-RNN 8.7 12.7 8.6
SDDE-AVG 6.7 9.8 1.8

Table 6: Testing error rate (%) with standard classifi-
cation on public datasets. Bold text indicates passing
hypothesis test with p-value < 0.05 with different ran-
dom initialization.

Model IMDB Yelp P. AG DBP.

Doc2VecC 12.5 11.5 13.9 3.6
PV-DBoW 13.7 18.6 13.9 13.1

SDDE 11.7 10.0 10.0 2.9

Table 7: Testing error (%) in weakly-supervised set-
ting. Only 1k labeled data per class were used. 4.5).

thors. We use SIF to generate document embed-
ding with Word2Vec trained on each dataset as its
inputs.

Results are shown in Table 6. SDDE-AVG
performs slightly better across different dataset.
We hypothesis SDDE gets larger improvement on
IMDB dataset since SDDE can handle longer doc-
uments better by exploiting sentence embeddings.
On the other hand, the RNN version of SDDE per-
forms significantly worse than the word-averaging
version. We may remind the reader that state-of-
the-art unsupervised document embedding models
are not RNN-based. The effects of word order are
still unclear. Wieting and Gimpel (2017) provides
a study of sentence embedding. We hypothesize
that it may be difficult for an RNN encoder to
learn to incorporate multi-domain information in
datasets with many classes (e.g., DBpedia) unsu-
pervisedly. This would be our future work.

4.5 Classification with Few Labeled Data

Next, we consider a more real-world weakly-
supervised learning scenario: classification on the

https://github.com/mchen24/iclr2017
https://github.com/PrincetonML/SIF

(a) IMDB

(b) AG’s news

Figure 1: t-SNE embedding Visualizations of weakly-
supervised learning experiments in Section 4.5. Col-
ored points are labeled training data in the experiments
and gray points are the unlabeled testing documents.

datasets we have used in previous experiments, but
this time only when very few labels are available.

We hypothesize that SDDE is particularly use-
ful for classification with few labels since the self-
discriminative learning has exploited the possible
features to map the text onto the embedding space
properly during the representation learning phase.
The embedding is expected to be more discrim-
inable to facilitate finding the classification deci-
sion hyperplanes with fewer labeled data.

We randomly sample equal number of instances
from each class to train a SVM and verify with
the whole testing set. PV-DBoW, Doc2VecC, and
SDDE-AVG are examined in this experiment. We
use the same pre-trained document embedding as
in the previous experiments. We repeat the whole
procedure 30 times and report the means and tune
the penalty parameter C to find the best value for
each model. Results in Figure 2 and Table 7 show
SDDEs outperform PV and Doc2VecC. We visu-
alize the training points with t-SNE. As shown in
Figure 1, SDDE seems to be able to spread the em-
bedded data more widely, which eventually leads
to better usage of scarce data for classification.



2472

Figure 2: Weakly-supervised learning on datasets in Table 1. Training with [10, 20, 30, 40, 50, 100, 1000] instances
per class (X-axis) and computing the accuracy on the whole testing set (Y-axis).

Model IMDB Yelp P. AG DBP.

Metric A E A-E A E A-E A E A-E A E A-E

Doc2VecC .78 .76 .02 .52 .49 .03 .57 .46 .11 .60 .40 .20
Word2Vec AVG .86 .85 .01 .54 .51 .03 .57 .44 .13 .75 .61 .14
PV-DBoW .28 .27 .01 .11 .10 .01 .71 .60 .09 .45 .36 .09

SDDE .36 .27 .09 .14 .08 .06 .20 .01 .19 .58 -.03 .61

Table 8: Distances of Intra & Inter-class cosine similarity. A for IntraCos and E for InterCos, note that they cannot
be compared across different models. Instead, distance A-E defined in Equation 7 is reported to study a method’s
effectiveness of modeling inter-document features. The higher the number the better.

4.6 Distance of Intra&Inter-Class Pairwise
Cosine Similarity

Definition We examine our assumption of the
ability of SDDE to model inter-document feature.
Similar to (Hill et al., 2016b), we consider pair-
wise cosine similarity between documents with
topic labels, this allows us to quantitatively eval-
uate unsupervised document embedding at inter-
document level. Our assumption is that: if pair-
wise similarity between documents is calculated
based on different kinds of embedding, the bet-
ter embedding results should comply with the
properties of both high similarities between those
documents within the same underlying class, de-
noted as IntraCos(d, d′) and low similarities be-
tween document pairs from different classes, or
InterCos(d, d̃). The mean distance:

mean(IntraCos)−mean(InterCos), (7)

is considered as our metric to avoid simply max-
imizing IntraCos or minimizing InterCos.

SDDE Provides High Separation Table 8 shows
the evaluation. The distances (Eq. 7) for the base-
line models are small, which support our assump-
tion that these methods are not able to model inter-
document features properly. On the other hand,
distances for SDDE are significantly larger. With
the classification experiments, we believe SDDE
better preserves meaningful inter-document fea-
tures. Figure 3 shows some meaningful clusters in
SDDE in the World class as cohesive sub-classes.

Figure 3: SDDE-AVG t-SNE visualization of class
”World” in AG News dataset testing set. Titles of docu-
ments are shown. Meaningful clusters such as (A) dis-
aster, (B) election, and (C) war.

5 Conclusion

Compared to mainstream unsupervised document
embedding models (trained to perform predictions
on the lexicon level) SDDE embeddings capture
information at the inter-document level, as they
are trained to maximize the distance between a
sentence and a corresponding document. We hope
the underlying idea of SDDE offers the document-
embedding community a new investigation direc-
tion. Self-discriminative learning shows potential



2473

for real-world scarcely-labeled scenarios, and our
future work will focus on joint training of repre-
sentations for semi-supervised learning.

Acknowledgements

This material is based upon work supported by Mi-
crosoft Research Asia (MSRA) grant, and by Tai-
wan Ministry of Science and Technology (MOST)
under grant number 108-2634-F-002 -019.

References
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.

A simple but tough-to-beat baseline for sentence em-
beddings. In ICLR.

Sanjeev Arora, Yingyu Liang, Tengyu Ma, Mikhail
Khodak, Nikunj Saunshi, and Brandon Stewart.
2018. A la carte embedding: Cheap but effective in-
duction of semantic feature vectors. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics, ACL 2018, Melbourne,
Australia, July 15-20, 2018, Volume 1: Long Papers,
pages 12–22.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.

Minmin Chen. 2017. Efficient vector representation for
documents through corruption. In ICLR.

Minmin Chen, Zhixiang Eddie Xu, Kilian Q. Wein-
berger, and Fei Sha. 2012. Marginalized denoising
autoencoders for domain adaptation. In ICML.

Andrew M. Dai and Quoc V. Le. 2015. Semi-
supervised sequence learning. In NIPS, pages 3079–
3087.

Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391–407.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. JMLR, pages
1871–1874.

Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li,
Xiaodong He, and Lawrence Carin. 2017. Learning
generic sentence representations using convolutional
neural networks. In EMNLP.

Zellig S. Harris. 1954. Distributional structure.
WORD, 10(2-3):146–162.

Felix Hill, Kyunghyun Cho, and Anna Korhonen.
2016a. Learning distributed representations of sen-
tences from unlabelled data. In NAACL HLT, pages
1367–1377.

Felix Hill, KyungHyun Cho, Anna Korhonen, and
Yoshua Bengio. 2016b. Learning to understand
phrases by embedding the dictionary. TACL, 4:17–
30.

Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen,
Max Tegmark, Marin Soljacic, and Yoshua Bengio.
2017. Gated orthogonal recurrent units: On learning
to forget. CoRR, abs/1706.02761.

Tom Kenter, Alexey Borisov, and Maarten de Rijke.
2016. Siamese CBOW: optimizing word embed-
dings for sentence representations. In ACL.

Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,
Richard S. Zemel, Antonio Torralba, Raquel Urta-
sun, and Sanja Fidler. 2015. Skip-thought vectors.
In NIPS, pages 3294–3302.

Quoc V. Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML,
pages 1188–1196.

Lajanugen Logeswaran and Honglak Lee. 2018. An
efficient framework for learning sentence represen-
tations. In ICLR.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In The 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, Proceedings of the Conference, 19-24
June, 2011, Portland, Oregon, USA, pages 142–150.

Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng
Huang, and Alan Yuille. 2015. Deep captioning
with multimodal recurrent neural networks (m-rnn).
In ICLR.

Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan
Cernocký, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, 11th Annual Conference of the
International Speech Communication Association,
Makuhari, Chiba, Japan, September 26-30, 2010,
pages 1045–1048.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In NIPS, pages 3111–3119.

Andriy Mnih and Yee Whye Teh. 2012. A fast and sim-
ple algorithm for training neural probabilistic lan-
guage models. In Proceedings of the 29th Inter-
national Conference on Machine Learning, ICML
2012, Edinburgh, Scotland, UK, June 26 - July 1,
2012.

Matteo Pagliardini, Prakhar Gupta, and Martin Jaggi.
2018. Unsupervised learning of sentence embed-
dings using compositional n-gram features. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,



2474

NAACL-HLT 2018, New Orleans, Louisiana, USA,
June 1-6, 2018, Volume 1 (Long Papers), pages 528–
540.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. In ICML, pages 1310–1318.

Hieu Pham, Minh-Thang Luong, and Christopher D.
Manning. 2015. Learning distributed representa-
tions for multilingual text sequences. In NAACL-
HLT, pages 88–94.

Tamara Polajnar, Laura Rimell, and Stephen Clark.
2015. An exploration of discourse-based sentence
spaces for compositional distributional semantics.
In Proceedings of the First Workshop on Linking
Computational Models of Lexical, Sentential and
Discourse-level Semantics, pages 1–11. Association
for Computational Linguistics.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In EMNLP.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45–50, Val-
letta, Malta. ELRA. http://is.muni.cz/
publication/884893/en.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS, pages 3104–3112.

Shuai Tang, Hailin Jin, Chen Fang, Zhaowen Wang,
and Virginia R. de Sa. 2017a. Rethinking skip-
thought: A neighborhood based approach. In Pro-
ceedings of the 2nd Workshop on Representation
Learning for NLP, Rep4NLP@ACL 2017, Vancou-
ver, Canada, August 3, 2017, pages 211–218.

Shuai Tang, Hailin Jin, Chen Fang, Zhaowen
Wang, and Virginia R. de Sa. 2017b. Trim-
ming and improving skip-thought vectors. CoRR,
abs/1706.03148.

Sida Wang and Christopher D. Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In The 50th Annual Meeting of the As-
sociation for Computational Linguistics, Proceed-
ings of the Conference, July 8-14, 2012, Jeju Island,
Korea - Volume 2: Short Papers, pages 90–94.

John Wieting and Kevin Gimpel. 2017. Revisiting re-
current networks for paraphrastic sentence embed-
dings. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics, ACL
2017, Vancouver, Canada, July 30 - August 4, Vol-
ume 1: Long Papers, pages 2078–2088.

Lingfei Wu, Ian En-Hsu Yen, Kun Xu, Fangli
Xu, Avinash Balakrishnan, Pin-Yu Chen, Pradeep
Ravikumar, and Michael J. Witbrock. 2018. Word
mover’s embedding: From word2vec to document

embedding. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Pro-
cessing, Brussels, Belgium, October 31 - November
4, 2018, pages 4524–4534.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchical
attention networks for document classification. In
NAACL HLT, pages 1480–1489.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In NIPS, pages 649–657.

Barret Zoph, Ashish Vaswani, Jonathan May, and
Kevin Knight. 2016. Simple, fast noise-contrastive
estimation for large RNN vocabularies. In NAACL
HLT, pages 1217–1222.


