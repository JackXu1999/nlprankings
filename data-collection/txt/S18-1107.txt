



















































KNU CI System at SemEval-2018 Task4: Character Identification by Solving Sequence-Labeling Problem


Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 655–659
New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics

 

   

 KNU CI System at SemEval-2018 Task4: Character Identification by 

Solving Sequence-Labeling Problem 

 

Cheoneum Park*, Heejun Song**, Changki Lee* 

 
*Department of Computer Science, Kangwon National University, South Korea 

**Samsung Research, Samsung Electronics Co., Ltd., South Korea 
*{parkce, leeck}@kangwon.ac.kr 

**heejun7.song@samsung.com 

 

 

 

 

Abstract 

Character identification is an entity-linking 

task that finds words referring to the same 

person among the nouns mentioned in a 

conversation and turns them into one entity. 

In this paper, we define a sequence-labeling 

problem to solve character identification, 

and propose an attention-based recurrent 

neural network (RNN) encoder–decoder 

model. The input document for character 

identification on multiparty dialogues con-

sists of several conversations, which in-

crease the length of the input sequence. The 

RNN encoder–decoder model suffers from 

poor performance when the length of the in-

put sequence is long. To solve this problem, 

we propose applying position encoding and 

the self-matching network to the RNN en-

coder–decoder model. Our experimental 

results demonstrate that of the four models 

proposed, Model 2 showed an F1 score of 

86.00% and a label accuracy of 85.10% at 

the scene-level. 

1 Introduction 

In this paper, we define character identification (CI) 

(Chen et al., 2017) as a sequence-labeling problem 

and use a recurrent neural network (RNN) en-

coder–decoder (Enc–Dec) model (Cho et al., 2014) 

based on the attention mechanism (Bahdanau et al., 

2015) to solve it. An Enc–Dec is an extension of 

the RNN model; it generates an encoding vector 

using an RNN in the encoder when an input se-

quence is given and performs decoding using the 

encoding vector. The attention mechanism calcu-

lates the alignment score for the two sequences and 

performs the input sequence and weighted sum so 

that they can focus more on the position that affects 

the output result. The self-matching network 

(Wang et al., 2017) is used to calculate an attention 

weight for itself and a context vector by using a 

weighted sum, after which the weights of similar 

words in the RNN sequence can be applied to aid 

in coreference resolution. Position encoding (PE) 

(Sukhbaatar et al., 2015, Park and Lee, 2017, Vas-

wani et al., 2017) is a method of applying weights 

differently, according to the positions of words ap-

pearing in a sequence. Training and prediction are 

performed by multiplying a weight vector by a vec-

tor of positions to be identified in a given input se-

quence. 

In an Enc–Dec model, a long input sequence re-

sults in performance degradation due to loss of in-

formation in the front portion of the input sequence 

when encoding is performed. In this paper, we pro-

pose four models that apply PE, attention mecha-

nism, and self-matching network to Enc–Dec mod-

els to solve the problem of performance degrada-

tion due to long input sequences.  

To summarize, the main contributions of this pa-

per are as follows:  

1. In this paper, we define CI task as sequence-

labeling problem, and perform training and 

prediction in end-to-end model. 

2. We propose four models using Enc-Dec 

based on attention mechanism and achieve 

high performance. 

2 System Description 

An Enc–Dec model maximizes P(𝑦|𝑥)  using an 
RNN. The encoder generates an encoder hidden 

655



 

   

state by encoding the input sequence, and the de-

coder generates an output sequence that maximizes 

P(𝑦|𝑥)  using the hidden state of the decoder, 
which was generated until this time step, with the 

encoder hidden state. The attention mechanism is a 

method of determining which part of the target 

class should be focused using the hidden state of 

the decoder and the hidden state of the encoder 

when performing decoding.  

2.1 Model 1: Attention-based Enc–Dec 

model 

The first model proposed in this paper is a general 

attention mechanism-based Enc–Dec model, as 

shown in Figure 1. 

The input of the encoder is one document that 

contains 𝑆  sentences (multiparty dialogue). Each 
sentence 𝑆 consists of 𝑛𝑆 words, and the input se-
quence 𝑋𝑖𝑛𝑝𝑢𝑡  is 𝑋𝑖𝑛𝑝𝑢𝑡 = {𝑥1, 𝑥2, … , 𝑥𝑛𝑆} . The 

input to the decoder is 𝑌𝑖𝑛𝑝𝑢𝑡 = {𝑦𝑖0, 𝑦𝑖1, … , 𝑦𝑖𝑚} 

consisting of the positions of the words given in the 

gold mentions, and the output sequence accord-

ingly becomes 𝑌𝑜𝑢𝑡𝑝𝑢𝑡 = {𝑦𝑜0, 𝑦𝑜1, … , 𝑦𝑜𝑚} con-

sisting of the character number, which is corre-

sponded with the decoder’s input mentions.  

We use word embedding and adopt the K-di-

mensional word embedding 𝑒𝑖
𝑘 , 𝑘 ∈ [1, 𝐾]  for all 

input words, where 𝑖 is the word index in the input 
sequence. We perform feature embedding for three 

features — speaker, named entity recognition 

(NER) tags, and capitalization — and concatenate 

them to make �̃�𝑖.  

 The uppercase feature is a binary feature (1 

or 0) that verifies whether the uppercase is 

included in the word. 

 10-dimensional speaker embedding for a to-
tal of 205 different types of speakers in-

cluded by “unknown”. 

 19-dimensional NER embedding for a total 
of 19 different types of NER tags. 

We use bidirectional gated recurrent unit 

(BiGRU) (Cho et al., 2014) for the encoder. The 

hidden state of the encoder for the input (word) se-

quence is defined as ℎ𝑖
𝑁.  

 𝑒𝑖 = 𝑊𝑒𝑥𝑖   (1) 

 �̃�𝑖 = [𝑒𝑖; 𝑢𝑐𝑖; 𝑠𝑝𝑘𝑖; 𝑁𝐸𝑅𝑖]  (2) 

 ℎ𝑖 = 𝑏𝑖𝐺𝑅𝑈(�̃�𝑖 , ℎ𝑖−1)  (3) 

where ℎ⃗ 𝑖  and ℎ⃗⃖𝑖  are forward and backward net-

works, respectively, and ℎ𝑖
𝑁 concatenates ℎ⃗ 𝑖 and ℎ⃗⃖𝑖.  

The decoder of our model uses the GRU as fol-

lows.  

 ℎ𝑡 = 𝐺𝑅𝑈(ℎ𝑦𝑖𝑡
𝑁 , ℎ𝑡−1)  (4) 

The input of the decoder is the hidden state ℎ𝑖
𝑁 

generated by the encoder corresponding to each po-

sition of 𝑌𝑖𝑛𝑝𝑢𝑡 which is the gold mention sequence. 

The hidden state ℎ𝑡 of the current decoder receives 

the hidden state ℎ𝑖
𝑁 of the encoder corresponding to 

the output position of the previous decoder and the 

previous hidden state of the decoder.  

 𝛼𝑖
𝑡 =

exp(𝑠𝑐𝑜𝑟𝑒𝑎(ℎ𝑡,ℎ𝑦𝑖𝑖
𝑁 ))

∑ exp(𝑠𝑐𝑜𝑟𝑒𝑎(ℎ𝑡,ℎ𝑦𝑖𝑗
𝑁 ))𝑗

  (5) 

𝑠𝑐𝑜𝑟𝑒𝑎 (ℎ𝑡, ℎ𝑦𝑖
𝑁

𝑖
) = vt

𝑇tanh⁡(𝑊𝑎 [ℎ𝑡; ℎ𝑦𝑖
𝑁

𝑖
; ℎ𝑦𝑖𝑡

𝑁 ]) (6) 

 𝑦𝑡 = argmax𝑖(𝑎𝑖
𝑡)  (7) 

 𝑐𝑡 = {
∑ 𝑎𝑖

𝑡ℎ𝑖
𝑁

𝑖 ,⁡⁡⁡𝑠𝑜𝑓𝑡⁡𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛

ℎ𝑦𝑡
𝑁 ,⁡⁡⁡⁡⁡⁡⁡⁡⁡⁡⁡⁡⁡ℎ𝑎𝑟𝑑⁡𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛

  (8) 

At the attention layer of the decoder, we use the 

attention weight 𝛼𝑖
𝑡 to compute the alignment score 

for the gold mention input into the decoder and the 

encoder hidden state ℎ𝑖
𝑁 input. The attention layer 

acts as a coreference resolution for each gold men-

tion and input sequence. After calculating the atten-

tion weights, we create the context vector 𝑐𝑡. We 
use soft attention and hard attention in Eq. (8). Soft 

attention 𝑐𝑡 = ∑ 𝑎𝑖
𝑡ℎ𝑖

𝑁
𝑖   is an attention-pooling 

vector of the whole input sentence of the encoder 

(ℎ𝑁). The other attention-pooling vector is hard at-

tention 𝑐𝑡 = ℎ𝑦𝑡
𝑁  , which is based on the argmax 

function Eq. (7) for attention weight 𝛼𝑖
𝑡 to choose 

the position with high score for the decoder input 

as the gold mention.  

𝑠𝑐𝑜𝑟𝑒𝑧(ℎ𝑡 , 𝑐𝑡 , ℎ𝑦𝑖𝑡
𝑁 ) = 𝑊𝑧2

𝑇 ReLU⁡(𝑊𝑧[ℎ𝑡; 𝑐𝑡; ℎ𝑦𝑖𝑡
𝑁 ])  (9) 

 

Figure 1: Attention-based Enc-Dec. 

656



 

   

𝑦𝑜𝑡 = argmax𝑡 (𝑠𝑜𝑓𝑡𝑚𝑎𝑥 (𝑠𝑐𝑜𝑟𝑒𝑧(ℎ𝑡 , 𝑐𝑡 , ℎ𝑦𝑖𝑡
𝑁 ))) 

 (10) 

After calculating the context vector between the 

input of the encoder and the input of the decoder, 

we calculate 𝑠𝑐𝑜𝑟𝑒𝑧, using which the context vec-
tor 𝑐𝑡, decode hidden state ℎ𝑡 and encoder hidden 
state ℎ𝑁  are concatenated in the decoder hidden 
layer. Next, the softmax function is used to calcu-

late the alignment score for 𝑠𝑐𝑜𝑟𝑒𝑧, and then the 
character index (𝑌𝑜𝑢𝑡𝑝𝑢𝑡 ) for the CI task corre-

sponding to the input of the decoder is obtained us-

ing the argmax function.  

2.2 Model 2: Attention–based Enc–Dec w/ 

model with PE 

The second model is based on the first model but 

uses PE which is a method of applying a weight to 

an input sequence of an RNN according to the word 

order. Among the words in the coreference resolu-

tion, the antecedent has a feature that appears mainly 

in the preceding context. In this paper, we apply PE 

with a feature to the encoder input sequence, and use 

the weight according to the word order as the feature. 

As shown in Eq. (11), PE information is concate-

nated to Eq. (2) to produce �̃�𝑖, and PE is calculated 
as shown in Eq. (12).  

 �̃�𝑖 = [𝑒𝑖; 𝑃𝐸𝑖; 𝑢𝑐𝑖; 𝑠𝑝𝑘𝑖; 𝑁𝐸𝑅𝑖]  (11) 

 𝑃𝐸𝑖 = (1 − 𝑖/𝑛𝑠) − (𝑠/𝑘)(1 − 2𝑖/𝑛𝑠)  (12) 

In PE, 𝑖 is the index of the word, 𝑛𝑠 is the total 
length of the input sequence, 𝑠 is the position of the 
sentence, and 𝑘 is the number of dimensions of the 
word expression. The weight of PE is calculated as 

a real value that gradually decreases between 1 and 

0, and is applied to the input of the encoder to take 

advantage of the feature that the predecessor pre-

cedes the current mention. In Eq. (12), (1 −
2𝑖/𝑛𝑠) denotes the order of words. If it is a front 
word, it has a higher value than the next word. 

(𝑠/𝑘) is a weight based on the sentence order, and 
when the sentence is different, the weight reduction 

rate difference is calculated to be higher than the 

value decreasing in the sentence. The expression 

for the encoder and decoder are the same as for 

model 1.  

2.3 Model 3: Self-matching Network-based 
Enc-Dec model 

The third model is also based on the first model, 

but performs encoding by using the self-matching 

network in the encoder without using PE, as shown 

in Figure 2. The self-matching network is used for 

calculating the alignment score for a given RNN 

sequence and itself, and then for performing a 

weighting sum with itself to create a context vector. 

While using the self-matching network for encod-

ing, attention weights are weighted with high align-

ment scores between similar words. For example, 

if “Rachel’s child-hood best friend” and “Monica” 

appear in a sentence, a high alignment score be-

tween them is calculated by the self-matching net-

work. 

The input sequence of the encoder becomes �̃�𝑖 
in Eq. (2), and a feed-forward neural network is 

used, as in Eq. (13). Next, we use the self-matching 

network to compute the attention weight for the t 

sequence and create a context vector 𝑐𝑖
𝑠𝑒𝑙𝑓

 that re-

flects the self-attention (Eqs. 14–16).  

 ℎ𝑖
𝑠𝑟𝑐 = 𝑊𝑠𝑟𝑐�̃�𝑖 + b𝑠𝑟𝑐  (13) 

 𝛼𝑗
𝑖 =

exp(𝑠𝑐𝑜𝑟𝑒𝑠𝑒𝑙𝑓(ℎ𝑗
𝑠𝑟𝑐 ,ℎ𝑖

𝑠𝑟𝑐))

∑ exp(𝑠𝑐𝑜𝑟𝑒𝑠𝑒𝑙𝑓(ℎ𝑡
𝑠𝑟𝑐,ℎ𝑖

𝑠𝑟𝑐))𝑡
  (14) 

𝑠𝑐𝑜𝑟𝑒𝑠𝑒𝑙𝑓(ℎ𝑗
𝑠𝑟𝑐 , ℎ𝑖

𝑠𝑟𝑐) = 𝑣𝑖
𝑇tanh⁡(𝑊𝑎

𝑠𝑟𝑐[ℎ𝑗
𝑠𝑟𝑐; ℎ𝑖

𝑠𝑟𝑐])  

 (15) 

 𝑐𝑖
𝑠𝑒𝑙𝑓

= ∑ 𝛼𝑗
𝑖

𝑗 ℎ𝑗
𝑠𝑟𝑐   (16) 

Subsequently, we construct [ℎ𝑖
𝑠𝑟𝑐; 𝑐𝑖

𝑠𝑒𝑙𝑓
]  by 

concatenating the context vector 𝑐𝑖
𝑠𝑒𝑙𝑓

, created us-

ing the self-matching network, and the hidden state 

ℎ𝑖
𝑠𝑟𝑐; it is then fed to the input of the hidden layer 

of the encoder to perform BiGRU.  

We apply [ℎ𝑖
𝑠𝑟𝑐; 𝑐𝑖

𝑠𝑒𝑙𝑓
] to the additional gate to 

make [ℎ𝑖
𝑠𝑟𝑐; 𝑐𝑖

𝑠𝑒𝑙𝑓
]
∗
, this determines whether or not 

 

Figure 2: Self-matching–Network-based Enc–

Dec. 

657



 

   

the information of [ℎ𝑖
𝑠𝑟𝑐; 𝑐𝑖

𝑠𝑒𝑙𝑓
]  is transmitted to 

the encoder hidden layer input. The equation is as 

follows:  

 𝑔𝑡 = 𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝑊𝑔[ℎ𝑖
𝑠𝑟𝑐; 𝑐𝑖

𝑠𝑒𝑙𝑓
])  (17) 

 [ℎ𝑖
𝑠𝑟𝑐; 𝑐𝑖

𝑠𝑒𝑙𝑓
]
∗
= 𝑔𝑡⨀[ℎ𝑖

𝑠𝑟𝑐; 𝑐𝑖
𝑠𝑒𝑙𝑓

]  (18) 

The decoder of model 3 performs training and 

prediction using a decoder such as the one used in 

model 1 (Eqs. 4–8) based on the hidden state where 

encoding is performed as above.  

2.4 Model 4: Self–matching Network–based 

RNN Enc-Dec model with PE 

Model 4 is based on model 3 using the self-match-

ing network; it additionally uses PE, which was 

also used for model 2, as a feature to confirm the 

word order.  

3 Experimental Results 

We evaluate the entity linking performance of the 

models using label accuracy and macro-F1 (Chen et 

al., 2017), and the coreference resolution perfor-

mance using CoNLL F1 (Rahman and Ng, 2009).  

The word representation used in this paper is a 

data–set provided by LDC, which is learned by a 

neural network language model (Bengio et al., 

2003, Lee et al., 2014), and is set to 50 dimensions. 

The experiments were performed with cross vali-

dation. The hyper parameters used in the experi-

ment are as follows. We used tanh for the encoder 

and decoder, and ReLU for the attention layer. The 

hidden layers had 150 dimension, and the dropout 

of all layers was set to 0.3. The learning was done 

using RMSprop (Hinton et al., 2012) and the learn-

ing rate was reduced by 50% for every 5 epochs 

without performance improvement starting at 0.1. 

The decoder attention functions of the models used 

in the experiments are all based on hard attention, 

and are compared with soft attention in Table 1.  

Table 1 shows a comparison between the CI per-

formances of the models on the trial set. M 2’ is a 

model in which PE proposed by Vaswani et al. 

(2017) is applied, and M 4’ is a model in which soft 

attention is applied to M 4. At the episode-level, M 

3 showed the best Main F1 performance (86.30%) 

and M 1 showed the best All F1 performance 

(23.33%). At the scene-level, M 4 showed the high-

est Main F1 performance (87.41%), and M 4 

showed the highest All F1 performance (23.92%). 

In the case of M 2 and M 2’, we can see that the 

proposed PE method resulted in a better overall 

performance. 

At the episode-level, M 4’ showed a better Main 

F1 performance (1.83%) than M 4, whereas M 4 

showed a better All F1 performance (by 2.67%). At 

the scene-level, M 4 showed a better Main F1 per-

formance (by 1.18%) than M 4’, whereas M 4’ 

 Episode-Level Scene-Level 

 Main F1 All F1 Main F1 All F1 

M 1 85.57 23.33 83.45 21.13 

M 2 82.91 23.15 85.58 22.40 

M 2’ 82.46 22.04 83.56 19.67 

M 3 86.30 22.93 84.10 21.84 

M 4 83.65 22.13 87.41 22.06 

M 4’ 85.48 19.46 86.23 23.92 

Table 1: Entity-linking results on the trial set (in %). 

Main and All in column mean main and other char-

acters, and all characters. 

 

 

 Characters Main + Other All 

Model Chandler Joey Monica Phoebe Rachel Ross Others F1 Acc F1 Acc 

E 

M 1 81.17 76.39 86.86 84.70 87.13 81.03 72.86 81.45 80.32 15.75 67.19 

M 2 85.77 81.59 85.19 87.67 59.64 84.79 80.42 85.01 84.36 16.47 68.42 

M 3 82.76 82.06 86.36 84.21 88.76 81.78 77.40 83.33 82.38 17.02 66.65 

M 4 83.52 79.92 87.17 86.43 86.72 83.73 78.46 83.71 82.96 15.19 65.83 

S 

M 1 83.55 79.19 90.99 86.43 90.01 84.11 76.08 84.34 83.08 15.93 68.59 

M 2 84.94 79.67 91.16 88.09 92.49 85.86 79.79 86.00 85.10 16.98 69.49 

M 3 83.87 84.30 88.24 86.10 88.79 79.65 76.23 83.88 82.34 16.99 67.31 

M 4 78.21 83.06 84.42 84.30 89.94 79.08 90.41 85.33 84.64 15.43 67.68 

Amore - - - - - - - 79.36 77.23 41.05 74.72 

Kamp. - - - - - - - 73.51 73.36 37.37 59.45 

zuma - - - - - - - 43.15 46.07 14.42 25.81 

Table 2: Entity-linking results on the evaluation set (in %). The F1 score is reported for each character. E/S: 

episode/scene level. F1 is macro-average F1 score. Acc is character label accuracy. 

 

 
658



 

   

showed a better All F1 performance (by 1.86%). 

Thus, it can be seen that the use of hard attention 

results in a better performance. 

Table 2 presents the experimental results of the 

test set (episode- and scene-level) for the method 

proposed in this paper, and the performance com-

parison with other competing models, namely 

AMORE UPF (Amore), Kampfpudding (Kamp.), 

and zuma. In the Main + Other character evalua-

tions at episode-level, M 2 showed the best perfor-

mance among all models (F1 of 85.01%, Acc of 

84.36%), whereas in the All character evaluations, 

M 3 showed the best F1 performance (17.02%) and 

M 2 showed the best Acc performance (68.42%). 

At the scene-level, M 2 showed the best perfor-

mance in both the Main + Other and the All char-

acter evaluation. The proposed method showed a 

lower overall performance in the All character 

evaluation compared with other competing models, 

but showed a higher performance in the Main + 

Other character evaluations. The reason for the 

lower performance in the All character evaluation 

is that the number of data points is smaller than that 

of the main characters. 

4 Conclusion 

In this paper, we defined the entity-linking problem 

of SemEval-2018 Task 4 as a sequence-labeling 

problem and proposed four models to solve it. Ex-

perimental results showed that M 2 shows the best 

performance in the test set scene-level (Main + 

Other characters), with an F1 of 86.00% and Acc 

of 85.10%. In the Main entities + Others evaluation 

of SemEval-2018 Task 4, it ranked 1st with an F1 

of 83.37% and Acc of 82.13%. In All Entities + 

Others, it ranked 2nd with an F1 of 13.53% and Acc 

of 68.55%. 

In future work, we will apply character CNN to 

solve the unknown word problem, and we will add 

word expressions such as GloVe (Pennington et al., 

2014) and ELMo (Peters et al., 2018). We will also 

enhance the performance by tightening the model 

with less data by adding the features used in the 

task 4-based model. 

References  

Henry Y.  Chen, Ethan Zhou, and Jinho D. Choi. 2017. 

Robust Coreference Resolution and Entity Linking 

on Dialogues: Character Identification on TV Show 

Transcripts. In: Proceedings of the 55th Annual 

Meeting of the Association for Computational Lin-

guistics. 

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-

cehre, Dzmitry Bahdanau, Fethi Bougares, Holger 

Schwenk, and Yoshua Bengio. 2014. Learning 

Phrase Representations using RNN Encoder-De-

coder for Statistical Machine Translation. 

arXiv:1406.1078. 

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly 

learning to align and translate. Proc. of ICLR’ 15, 

arXiv:1409.0473. 

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, 

and Ming Zhou.  2017. Gated Self-Matching Net-

works for Reading Comprehension and Question 

Answering, In: Proceedings of the 55th Annual 

Meeting of the Association for Computational Lin-

guistics (Volume 1: Long Papers), pages 189-198. 

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, 

and Rob Fergus. 2015. End-To-End Memory Net-

works. arXiv:1503.08895. 

Cheoneum Park and Changki Lee. 2017. Korean Co-

reference Resolution using Pointer Networks based 

on Position Encoding. In: Proceedings Of the KIISE 

and the KBS Joint Symposium, pages 76-78. 

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob 

Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz 

Kaiser, and Illia Polosukhin. 2017. Attention Is All 

You Need. arXiv:1706.03762. 

Altaf Rahman and Vincent Ng. 2009. Supervised Mod-

els for Coreference Resolution. In: Proceedings of 

the 2014 Conference on Empirical Methods in Nat-

ural Language Processing (EMNLP), pages 968-

977. 

Changki Lee, Junseok Kim, and Jeonghee Kim. 2014. 

Korean Dependency Parsing using Deep Learning. 

In: Proceedings of the KIISE for HCLT, pages 87-

37. 

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and 

Christian Jauvin. 2003. A Neural Probabilistic Lan-

guage Model. Journal of machine learning research, 

pages 1137-1155. 

Geoffrey Hinton, Nitish Srivastava, and Kevin 

Swersky. 2012. Rmsprop: Divide the gradient by a 

running average of its recent magnitude. Neural net-

works for machine learning, Coursera lecture 6e. 

Jeffrey Pennington, Richard Socher, and Christopher 

Manning. 2014. Glove: Global vectors for word rep-

resentation. Proceedings of the 2014 conference on 

empirical methods in natural language processing 

(EMNLP), pages. 1532-1543. 

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt 

Gardner, Christopher Clark, Kenton Lee, and Luke 

Zettlemoyer. 2018. Deep contextualized word 

prepresentations. arXiv:1802.05365. 

659


