















































Attribute Extraction from Synthetic Web Search Queries


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 401–409,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Attribute Extraction from Synthetic Web Search Queries

Marius Paşca
Google Inc.

Mountain View, California 94043
mars@google.com

Abstract

The accuracy and coverage of existing
methods for extracting attributes of in-
stances from text in general, and Web
search queries in particular, are limited
by two main factors: availability of in-
put textual data to which the methods
can be applied, and inherent limitations
of the underlying assumptions and algo-
rithms being used. This paper proposes
a weakly-supervised approach for the ac-
quisition of attributes of instances from in-
put data available in the form of synthetic
queries automatically generated from sub-
mitted queries. The generated queries al-
low for the acquisition of additional at-
tributes, leading to extracted lists of at-
tributes of higher quality than with com-
parable previous methods.

1 Introduction

Motivation: The availability of larger textual data
sources has allowed research in information ex-
traction to shift its focus towards robust methods
that require little or no annotated data, operate at
large scale with lower computational costs, and
acquire open-domain information (Banko and Et-
zioni, 2008). The information is usually targeted
at three levels of granularity: classes (e.g., giant
planets), class elements or instances (e.g., jupiter,
uranus, saturn), and relations among instances.
Since these types of information would form the
backbone of knowledge bases acquired automati-
cally from text (Mooney and Bunescu, 2005), their
acquisition has received increased attention over
recent years.

Among other types of relations targeted by vari-
ous extraction methods, attributes (e.g., escape ve-

locity, diameter and surface gravity) have emerged
as one of the more popular types, as they capture
quantifiable properties of their respective classes
(giant planets) and instances (jupiter). A variety
of attribute extraction methods mine textual data
sources ranging from unstructured (Tokunaga et
al., 2005) or structured (Cafarella et al., 2008) text
within Web documents, to human-compiled ency-
clopedia (Wu et al., 2008; Cui et al., 2009) and
Web search query logs (Alfonseca et al., 2010),
attempting to extract, for a given class or instance,
a ranked list of attributes that is as comprehen-
sive and accurate as possible. The accuracy and
coverage of existing methods (Raju et al., 2008;
Alfonseca et al., 2010) for extracting attributes of
instances are limited by two main factors: avail-
ability of input textual data to which the methods
can be applied; and inherent limitations of the un-
derlying assumptions and algorithms being used.
For example, a simple but effective method was
proposed in (Paşca and Van Durme, 2007) for ex-
tracting attributes of an instance, by applying a
small set of extraction patterns (e.g., A of I) to Web
search queries (e.g., “escape velocity of jupiter”).
If the input set of queries increased, additional
candidate attributes would be extracted.

Contributions: This paper introduces a weakly-
supervised approach for the acquisition of at-
tributes of instances from query logs, by auto-
matically expanding the set of known (organic)
queries from which attributes are extracted with
additional, inferred (synthetic), not-yet-submitted
queries. The focus on expanding the input textual
data gives a generally-applicable approach, which
can be applied to existing methods for attribute ac-
quisition from query logs, to increase coverage. In
particular, the application of previously-proposed
extraction patterns (Paşca and Van Durme, 2007)

401



to the expanded set of queries allows for the ac-
quisition of additional attributes that would other-
wise not be acquired only from the set of known
queries.

In order to infer new queries, known queries
are aggregated into query templates (e.g., “lyrics
of ⋆ beatles”) associated with known phrase
fillers (e.g., ⋆→{yesterday, hey jude}). The
known phrase fillers of each query template are
then expanded into new candidate phrase fillers.
In contrast to previous work on query genera-
tion (Mitkov and Ha, 2006; Heilman and Smith,
2010), new queries are generated based on query
analysis alone, as opposed to individual docu-
ment analysis. This has the potential advantages
of scalability and robustness when applied to ar-
bitrary, inherently-noisy queries. Among the in-
ferred queries, the ones of higher interest to at-
tribute extraction are those derived from a query
template that fixes either a potential attribute (e.g.,
“surface gravity of ⋆”) or a potential instance
(e.g., “⋆ of jupiter”). In experiments using a
large set of anonymized search queries, the in-
ferred queries allow for the acquisition of accurate
attributes over an evaluation set of 75 instances in-
troduced in previous work (Alfonseca et al., 2010).
Applications: Attributes are useful in infor-
mation retrieval, e.g., for suggesting related
queries (Bellare et al., 2007) and recommending
products (Probst et al., 2007). They plan an im-
portant role in knowledge acquisition and repre-
sentation (Guarino, 1992), for example as build-
ing blocks in the manual compilation of infoboxes
in Wikipedia (Remy, 2002). Furthermore, the
availability of a larger number of more accurate
attributes allows for the development of better
search interfaces geared towards structured search.
Examples of such interfaces are Wolfram Alpha
and Google Squared, two search tools that can take
as input instances and return lists of attributes and
their values.

2 Attribute Extraction

2.1 Intuitions and Scope

Intuitions: Our attribute extraction method is in-
spired by several intuitions. First, phrases of
the same class (car models, search engines, base-
ball players etc.) share similar properties or at-
tributes, enter similar relations and satisfy simi-
lar constraints. For example, car models have re-
placement parts, are assembled by some maker in

some year, and have an estimated current value.
Consequently, known queries that refer to similar
phrases may overlap significantly, or even become
equal once the phrases have been replaced by a
common slot filled by the phrases. Thus, “kel-
ley blue book value of 2008 dodge charger” and
“kelley blue book value of 2008 honda civic” can
be grouped into a shared query template “kelley
blue book of 2008 ⋆”, whose slot ⋆ is filled by the
names of car models. Second, known queries that
can be grouped into a shared query template often
provide a small sample of, rather than comprehen-
sive coverage of, all phrases that would meaning-
fully fill the template. Therefore, new queries can
be generated by filling the slots of query templates
with new phrase fillers similar to known fillers.
For instance, “kelley blue book value of 2008
chrysler sebring” can be inferred as a new candi-
date query if chrysler sebring is known to be sim-
ilar to dodge charger and/or honda civic. Third,
a new candidate query is meaningful if the new
phrase filler is similar to the known fillers not only
statically, but also in the context of the query tem-
plate. This is particularly important for query tem-
plates with few, ambiguous phrase fillers. Con-
sider the query template “lyrics of ⋆ beatles”, with
the known fillers come together, hey jude and yes-
terday. Although phrases such as gather together,
earlier today and last friday are highly similar to
the known fillers, they are not meaningful new
fillers for “lyrics of ⋆ beatles” and would there-
fore produce spurious new queries. In contrast,
lovely rita and here comes the sun are similar to
the known fillers both statically and in the context
of the query template.

Scope: Following the above intuitions, attributes
can be extracted from generated queries. In turn,
generated queries are inferred by essentially re-
placing phrases from known queries with mean-
ingful, similar phrases. The form (e.g., long
and complex, vs. short and simple) and scope
(e.g., open-domain vs. domain-specific) of known
queries determine the form and scope of inferred
queries. While this prevents arbitrarily complex
queries from being generated, it has the advantage
of homogeneity of new queries relative to known
queries. Also, this should have little, if any, im-
pact on extracted attributes, since the latter are ac-
tually extracted from queries whose form is rela-
tively simple rather than complex. The scope of
new queries is further influenced by the availabil-

402



ity of new phrases that are similar to phrases from
known queries. For example, chrysler sebring
must be available as a phrase similar to dodge
charger and/or honda civic, in order to potentially
generate “kelley blue book of 2008 chrysler se-
bring” from the known queries “kelley blue book
of 2008 dodge charger” and “kelley blue book of
2008 honda civic”.

2.2 Extraction from Generated Queries

Aggregation into Query Templates: The input
to the method is a set of Web search queries.
As described in (Paşca, 2011), the sequence of
terms available in each query is split into all com-
binations of triples of a prefix, non-empty infix
and postfix. Queries that share a common pre-
fix and common postfix are aggregated into a
query template, where the input infixes are the
known phrase fillers of the template. For example,
queries such as “lyrics of yesterday beatles” and
“lyrics of come together beatles” are aggregated
into the template “lyrics of ⋆ beatles”, where the
template filler ⋆ corresponds to the set of known
phrase fillers, i.e., infixes from the input queries:
{yesterday, come together}. An input query may
contribute to the creation of multiple query tem-
plates, via different infixes. For example, another
template created from “lyrics of yesterday beat-
les” “lyrics of yesterday toni braxton” is “lyrics
of yesterday ⋆”.

Like the subsequent stages of processing, gener-
ating all possible infixes of all input queries, espe-
cially for large input sets of queries, is a non-trivial
computational challenge. However, the compu-
tation can be translated into parallelizable opera-
tions in a distributed computing framework such
as Hadoop (White, 2010) or MapReduce (Dean
and Ghemawat, 2004). In particular, the aggre-
gation of queries into templates can be performed
in a single MapReduce step. The mapper takes
as input queries, and splits them into one or more
mappings from a query template (key) to a corre-
sponding infix, i.e., a known phrase filler (value).
For each query template, the reducer simply ag-
gregates its phrase fillers into a set.

Generation of Candidate Phrase Fillers: In or-
der to generate new queries, the set of known
phrase fillers is expanded into additional candi-
date phrases that may fill the query template. As a
prerequisite to generating candidate phrase fillers,
distributionally similar phrases (Lin and Pantel,

2002; Lin and Wu, 2009; Pantel et al., 2009) and
their scores are collected in advance. The assump-
tion is that phrases that appear in similar contexts
have similar meanings. A phrase is represented
as a vector of its contextual features. A feature
is a token, collected from windows of three to-
kens centered around the occurrences of the phrase
in sentences across Web documents (Lin and Wu,
2009). Alternatively, the context could be approx-
imated via linguistic dependencies detected with
noun chunking (Pantel et al., 2009) and syntactic
parsing (Lin and Pantel, 2002). In the contextual
vector of a phrase, the weight of a feature is the
pointwise-mutual information (Lin and Wu, 2009)
between the phrase P and the feature F :

PMI(P, F ) = log
Freq(P, F )×N

Freq(P )× Freq(F ) (1)

where Freq(P, F ) is the frequency of the feature
F occurring with the phrase P , and N is the fea-
ture vocabulary size. The distributional similarity
score between two phrases P1 and P2 is the co-
sine similarity between the contextual vectors of
the two phrases. Alternatively, vector similarity
could be computed via the Jaccard or Dice coef-
ficients (Pantel et al., 2009). The lists DS(P ) of
most distributionally similar phrases of a phrase
P are thus compiled offline, by ranking the similar
phrases of P in decreasing order of their DSscore
relative to P .

The most distributionally similar (Lin and Pan-
tel, 2002; Pantel et al., 2009) phrases, of a known
phrase filler Ki from a query template T , are con-
sidered to be candidate phrase filler U of the re-
spective query template. The score of a candidate
relative to the entire set of known fillers is the aver-
age of the distributional similarity scores between
the candidate and each known filler. For each tem-
plate T , its candidate phrase fillers U are ranked
in decreasing order of their scores. Known phrase
fillers of T are discarded from the resulting list of
candidate phrase fillers of T .

The generation of candidate phrase fillers trans-
lates into two MapReduce steps. The first step re-
arranges the mappings from a query template to
a set of known phrase fillers, into mappings from
a known phrase filler to a set of query templates.
Concretely, the mapper takes as input mappings
from a query template (key) to its set of known
phrase fillers (value), as they were output after
the aggregation into query templates. The mapper
emits mappings from a known phrase filler (key)

403



to a query template (value). For each phrase filler,
the reducer aggregates its query templates into a
set. The second MapReduce step takes this data,
and joins it with distributional similarity data. The
latter is available as mappings from a phrase (key)
to a list of scored similar phrases (value). The
mapper selects similar phrases as candidate phrase
fillers for the template, as explained earlier. The
output of the second step consists in mappings
from a query template (key) to its set of known
phrase fillers, as well as to a list of scored candi-
date phrase fillers (value).

Filtering of Candidate Phrase Fillers: Candi-
date phrase fillers generated via distributional sim-
ilarities are similar to known phrases only stati-
cally. To also take the context of the query tem-
plate into account, the candidates are filtered using
the input queries. More precisely, the list of can-
didate phrases of a query template T is filtered, by
retaining only known phrases of some other tem-
plates T ′ that are equivalent to T . Templates are
deemed equivalent if they become identical after
removal of stop words and other linking particles
(prepositions, conjunctions etc.), term stemming
and term reordering. For example, if the unfiltered
candidate phrase eleanor rigby for the template
“lyrics of ⋆ beatles” appears as a known phrase
filler of the template “lyrics for ⋆ by the beatles”,
then eleanor rigby is retained after filtering for the
template “lyrics of ⋆ beatles”.

The filtering of candidate phrases using equiv-
alent templates is modeled as two MapReduce
steps. The first step takes as input mappings from
a query template (key) to its set of known phrase
fillers and list of scored candidate phrase fillers
(value), as they were output after the generation
of candidate phrase fillers. The mapper converts
the query template into the corresponding equiv-
alent template that contains the individual terms
in lexicographic order. It emits mappings from
an equivalent template (key), to a query template
with its set of known phrase fillers and its list
of scored candidate phrase fillers (value). For
each equivalent template, the reducer simply ag-
gregates this data. The second step takes as input
mappings from an equivalent template (key) to its
query templates with their sets of known phrase
fillers and lists of scored candidate phrase fillers
(value). For each equivalent template, the map-
per iterates over its query templates. It checks
which of its candidate phrase fillers occur among

the known phrase fillers of the other query tem-
plates. The candidate phrase fillers that pass this
test are retained as filtered phrase fillers. The map-
per emits mappings from a query template (key)
to its set of known phrase fillers, list of scored
unfiltered phrase fillers, and list of scored filtered
phrase fillers. The reducer merely emits its input,
without modifications.

The relative ranking of candidate phrases from
the list of inferred unfiltered phrase fillers (before
filtering) is preserved in the list of inferred filtered
phrase fillers (after filtering). Each filtered phrase
filler inferred for a query template corresponds to
a new query, generated by filling the phrase into
the slot filler of the query template.
Attribute Extraction: Extraction patterns such as
“A of I”, introduced in previous work (Paşca and
Van Durme, 2007) to extract a candidate attribute
A for a candidate instance I from queries, can be
immediately applied to inferred queries. Thus, ad-
ditional candidate attributes can be extracted from
inferred queries, where the inferred queries are ob-
tained via two types of query templates:
• query templates that specify a potential in-

stance, and leave the attribute (e.g., A in “A of I”)
as a phrase filler being inferred:

surface gravity of jupiter
europa of jupiter
composition of atmosphere of jupiter
father of jupiter





“⋆ of jupiter”

Each inferred phrase filler (e.g., core temper-
ature, luminosity) is collected as a candidate at-
tribute of the phrase specified in the query tem-
plate (jupiter). In this case, attribute extraction
is equivalent to transferring an instance associated
with a noisy set of attributes that are known phrase
fillers of a template, to be associated with new at-
tributes that are inferred phrase fillers of the tem-
plate.
• query templates that specify a potential at-

tribute, and leave the instance (e.g., I in “A of I”)
as a phrase filler being inferred:

mass of positronium
mass of planets in order
mass of steel beams
mass of planet uranus





“mass of ⋆”

For each inferred phrase filler (e.g., cesium 137,
water drop), the phrase specified in the query tem-
plate (mass) is collected as a candidate attribute.
In this case, attribute extraction is equivalent to
transferring an attribute associated with a noisy set

404



Phrase Ranked List Available in Data Repository

caesium [cesium, rubidium, strontium, barium, thallium, lanthanum, potassium, cerium, yt-
trium, bismuth, indium, gallium, europium, cadmium, antimony, ammonium,..]

ch3br [ch3cl, ch4, nh3, ch 4, c2h4, ch3i, nh 3, ch3oh, c2h2, ch3f, c2h6, hcho, no2, h2s,
hcn, ch30h, n2o, n20, ch3cn, hcooh, ethane, cc14, ethene, propene, hzo, c02, ch3sh,
chbr3,..]

exxon valdez [torrey canyon, amoco cadiz, sea empress, braer, 1989 exxon valdez, valdez oil,
exxon valdez oil tanker, chernobyl nuclear, cosco busan, valdez oil spill, cher-
nobyl,..]

fragile x [fragile x, klinefelter syndrome, rett syndrome, fxtas, turner syndrome, down syn-
drome, friedreich ataxia, myotonic dystrophy, huntington’s disease, williams syn-
drome, trisomy 21,..]

medal [congressional medal of honor, navy cross, distinguished service cross, victoria
cross, distinguished flying cross,air force cross, bronze star, purple heart, soldier’s
medal, medal honor, military cross,..]

men and
women

[women and men, men and woman, males and females, men & women, young
men, women, people, men, individuals, boys and girls, girls and boys, adults, sexes,
females and males,..]

nerve [ganglion, preganglionic, postganglionic, peripheral nerve, sciatic, sciatic nerve,
axon, afferent, nerve root, dorsal root, ganglionic, vagus, trigeminal, median
nerve,..]

yesterday [last week, earlier today, last friday, last night, two days ago, yesturday, last thurs-
day, earlier this week, just yesterday, last wednesday, yesteday, last monday, last
month, two weeks ago,..]

whey [protein powder, whey protein isolate, whey protein powder, soy protein, whey
isolate, protein whey, casein protein, creatine, creatine monohydrate, isopure,..]

Table 1: Examples of ranked lists of similar phrases, available in the phrase similarity repository for
various phrases

of instances that are known phrase fillers of a tem-
plate, to be associated with new instances that are
inferred phrase fillers of the template.

Regardless of whether they are specified man-
ually or derived automatically, extraction pat-
terns used in information extraction are imper-
fect (Kozareva et al., 2008). The pattern A of I for
attribute extraction is no exception. The two types
of query templates from above have known phrase
fillers that are not true attributes (e.g., europa for
the instance jupiter) and instances (e.g., planets
in order for the attribute mass) respectively. This
phenomenon is not a defect of this particular ap-
proach, but is inherited from and shared with any
methods using such patterns for attribute extrac-
tion, as well as with any methods that rely on seed
attributes, when the seeds are noisy rather than
clean.

Attribute Ranking: As explained earlier, the
score of an inferred phrase filler is computed as
the average of similarity scores relative to known

phrase fillers. The score is assigned to the pair
of an attribute and instance extracted from the
phrase filler. Attributes extracted for an instance
are ranked in decreasing order of the scores.

3 Experimental Setting

Textual Data Sources: The acquisition of in-
stance attributes relies on a random sample of
around 100 million fully-anonymized queries in
English submitted by Web users in 2010. Each
query is accompanied by its frequency of occur-
rence in the query logs.

A phrase similarity repository is derived follow-
ing (Pantel et al., 2009), from unstructured text
available within a sample of around 200 million
documents in English. The repository provides
data for each of around 1 million phrases that oc-
cur as full-length queries in the input query logs.
It contains ranked lists of the top 200 phrases
computed to be the most distributionally similar,
for each phrase. Table 1 illustrates the actual

405



aaa, ac compressors, acheron, acrocyanosis,
adelaide cbd, african population, agua caliente
casino, al hirschfeld, alessandro nesta, ameri-
can fascism, american society for horticultural
science, ancient babylonia, angioplasty, an-
napolis harbor, antarctic region, arlene martel,
arrabiata sauce, artificial intelligence, bangla
music, baquba, bb gun, berkshire hathaway,
bicalutamide, blue jay, boulder colorado, brit-
tle star, capsicum, carbonate, carotid arter-
ies, chester arthur, christian songs, cloxacillin,
cobol, communicable diseases, contemporary
art, cortex, ct scan, digital fortress, eartha
kitt, eating disorders, file sharing, final fan-
tasy vii, forensics, habbo hotel, halogens,
halophytes, ho chi minh trail, icici pruden-
tial, jane fonda, juan carlos, karlsruhe, kidney
stones, lipoma, loss of appetite, lucky ali, ma-
jorca, martin frobisher, mexico city, pancho
villa, phosphorus, playing cards, prednisone,
right to vote, robotics, rouen, scientific revolu-
tion, self-esteem, spandex, strattera, u.s., vida
guerra, visual basic, web hosting, windsurfing,
wlan

Table 2: Set of 75 target instances, used in the
evaluation of instance attribute extraction

ranked lists available in the repository for various
phrases. The underlying similarity score between
two phrases is the cosine between their vectors of
context windows.

Target Instances: The performance of attribute
extraction is computed over a standard set of 75
instances, previously introduced in (Alfonseca et
al., 2010). As shown in Table 2, the set of
instances ensures varied experimentation across
multiple domains.

Experimental Runs: The experiments consist of
several individual runs. Runs RU and RF acquire
attributes from queries inferred via the first type of
target query templates (e.g., “⋆ of jupiter”), before
filtering (RU ) and after filtering (RF ). Run RI uses
queries inferred via the second type of target query
templates (e.g., “mass of ⋆”), after filtering.

In order to compare with existing work, a pre-
vious extraction method from (Paşca and Van
Durme, 2007), which uses extraction patterns, is
implemented in a baseline run RP . For consis-
tency, the data source to the run RP is the same
set of input queries described at the beginning of

Label Value Examples of Attributes

vital 1.0 capsicum: calorie count
cloxacillin: side effects
lucky ali: album songs

okay 0.5 jane fonda: musical theatre
contributions
mexico city: cathedral
robotics: three laws

wrong 0.0 acheron: kingdom
berkshire hathaway: tax ex-
clusion
contemporary art: urban in-
stitute

Table 3: Correctness labels manually assigned to
attributes extracted for various instances

the section.
The per-instance ranked lists of attributes pro-

duced by the individual runs from above are con-
catenated in a series of combination runs. For ex-
ample, run RFP concatenates the attributes output
by RF and by RP , in this order.
Evaluation Procedure: The evaluation focuses
on the assessment of accuracy of the ranked list
of attributes generated for each instance. To re-
move any undesirable bias towards higher-ranked
attributes, the attributes of each list to be evaluated
are sorted alphabetically into a merged list. Each
attribute of the merged list is manually assigned a
correctness label relative to its respective instance.
In accordance with previously introduced method-
ology, an attribute is vital if it must be present in
an ideal list of attributes of the instance (e.g., side
effects for cloxacillin); okay if it provides useful
but non-essential information; and wrong if it is
incorrect (Paşca, 2007). Thus, a correctness label
is manually assigned to a total of 4,833 attributes
extracted for the 75 target instances.

To compute the precision score over a ranked
list of attributes, the correctness labels are con-
verted to numeric values (vital to 1, okay to 0.5
and wrong to 0), as shown in Table 3. Precision
at some rank N in the list is measured as the sum
of the correctness values of the attributes extracted
up to rank N , divided by the number of those at-
tributes.

4 Evaluation Results

Attribute Accuracy: Table 4 compares precision
at various ranks, in the ranked lists of attributes

406



Run Precision of Ranked Attributes
@1 @5 @10 @20 @50

Average (over all):
RP 0.61 0.65 0.65 0.67 0.65
RI 0.61 0.62 0.60 0.61 0.60
RU 0.45 0.37 0.33 0.31 0.30
RF 0.64 0.58 0.57 0.55 0.51

RIP 0.68 0.68 0.67 0.67 0.66
RIF 0.64 0.64 0.63 0.64 0.62
RFP 0.77 0.69 0.68 0.65 0.62
RFI 0.77 0.69 0.68 0.65 0.62
Average (over non-empty):
RP 0.66 0.71 0.71 0.73 0.71
RI 0.67 0.67 0.65 0.67 0.65
RU 0.59 0.48 0.43 0.41 0.38
RF 0.83 0.75 0.74 0.71 0.66

RIP 0.69 0.69 0.67 0.68 0.67
RIF 0.68 0.68 0.66 0.67 0.66
RFP 0.83 0.74 0.72 0.70 0.67
RFI 0.81 0.73 0.72 0.69 0.65

Table 4: Comparative accuracy of the ranked lists
of attributes extracted in various runs, as an aver-
age over the entire set of 75 instances; and as an
average over the (variable) subsets of instances for
which some attributes were extracted

extracted by various runs. In the upper half of the
table, average precision scores penalize instances
for which no attributes are extracted. In contrast,
the average scores in the lower half of the table
only consider the instances for which some at-
tributes are extracted.

The runs RI , RU and RF operate directly over
generated queries. One of the two types of query
templates that produce attributes performs better,
as illustrated by lower scores with RF than with
RI . The difference in scores between RU and RF ,
which are about twice as high at rank 50 for the
latter, illustrates the positive impact of filtering the
candidate phrase fillers inferred from the query
templates.

The benefit of combining the output from indi-
vidual runs is illustrated by the generally higher
scores given by combination runs in Table 4, rela-
tive to individual runs that they combine. Among
combination runs, RFP gives the highest scores at
most ranks. When considering the accuracy of all
runs, the highest scores over the entire evaluation
set of instances are given by the combination run
RFP . Over subsets of instances with non-empty

Instance [Ranked List of Inferred At-
tributes]

aaa [symptoms, epidemiology,
differential diagnosis, mor-
tality, risk, surgical repair,
stenting, resection, benefits,
stent placement,..]

ac compres-
sors

[manufacturer]

berkshire
hathaway

[net asset value, closing price,
total assets, dividend yield,
par value, fair value, shares
outstanding, class a shares,
current price, common
stock,..]

martin
frobisher

[time line, routes, voyage,
bibliography, explorations,
ships, discoveries, travel,..]

scientific
revolution

[negative effects, social im-
pacts, social impact, theories,
positive effects, accomplish-
ments, long term effects, in-
fluences, cause and effects,
scientific discoveries,..]

wlan [limitations, risks, architec-
tures, benefits, configura-
tions, throughput, concepts,
config, physical layer, secu-
rity vulnerabilities,..]

Table 5: Examples of ranked lists of attributes ex-
tracted in run RF from inferred filtered queries.
None of these attributes are extracted for the re-
spective instances in the baseline run RP

attribute lists, the accuracy of the individual run
RF is higher than all other individual runs, at par
with the combination run RFP . Table 5 shows
the ranked lists of attributes extracted by RF for
a sample of instances.

5 Related Work

Previous work on attribute extraction uses a va-
riety of types of textual data as sources for min-
ing attributes. Taking advantage of structured
and semi-structured text available within Web
documents, the method introduced in (Yoshinaga
and Torisawa, 2007) assembles and submits list-
seeking queries to general-purpose Web search
engines, and analyzes the retrieved documents
to identify common structural (HTML) patterns

407



around class labels given as input, and potential
attributes. Similarly, layout (e.g., font color and
size) and other HTML tags serve as clues to ac-
quire attributes from either domain-specific doc-
uments such as those from product and auction
Web sites (Wong et al., 2008) or from arbitrary
documents, optionally relying on the presence of
explicit itemized lists or tables (Cafarella et al.,
2008). As an alternative to Web documents, ar-
ticles within online encyclopedia can also be ex-
ploited as sources of structured text for attribute
extraction, as illustrated by previous work using
infoboxes and category labels (Suchanek et al.,
2007; Nastase and Strube, 2008; Wu and Weld,
2008) associated with articles within Wikipedia.

Working with unstructured text within Web
documents, the method described in (Tokunaga
et al., 2005) applies manually-created lexico-
syntactic patterns to document sentences in or-
der to extract candidate attributes, given vari-
ous class labels as input. The candidate at-
tributes are ranked using several frequency statis-
tics. If the documents are domain-specific, such as
documents containing product reviews, additional
heuristically-motivated filters and scoring metrics
can be used to extract and rank the attributes (Raju
et al., 2008). In (Bellare et al., 2007), the extrac-
tion is guided by a small set of manually-provided
seed instances and attributes rather than manually-
created patterns, with the purpose of generating
training data and extract new pairs of instances and
attributes from text.

Web search queries have also been considered
as a textual data source for attribute extraction,
using extraction patterns (Paşca and Van Durme,
2007) or seed attributes (Paşca, 2007) to guide the
extraction, and leading to attributes of higher ac-
curacy than those extracted with equivalent tech-
niques from Web documents. If the input data
includes query sessions in addition to sets of
search queries, extracted attributes have higher
quality (Paşca et al., 2010). Given an instance
(e.g., nissan gt-r) and a numerical attribute (e.g.,
width) extracted with a method like ours, the ac-
quisition of the corresponding values (e.g., 1.9m)
is the aim of other research endeavors (Davidov
and Rappoport, 2010; Bakalov et al., 2011).

6 Conclusion

The role of Web search queries in information ex-
traction has been previously explored. In this pa-

per, synthetic search queries inferred from existing
queries are used to acquire attributes. The queries
lead to ranked lists of attributes whose accuracy
is higher than with equivalent methods operating
over queries. Current work investigates alterna-
tive methods for combining attributes from mul-
tiple individual runs; the expansion of the target
query templates used to extract attributes; and fur-
ther applications of the inferred queries, in infor-
mation extraction and beyond.

References
E. Alfonseca, M. Paşca, and E. Robledo-Arnuncio. 2010.

Acquisition of instance attributes via labeled and related
instances. In Proceedings of the 33rd International Con-
ference on Research and Development in Information Re-
trieval (SIGIR-10), pages 58–65, Geneva, Switzerland.

A. Bakalov, A. Fuxman, P. Talukdar, and S. Chakrabarti.
2011. Scad: collective discovery of attribute values.
In Proceedings of the 20th World Wide Web Conference
(WWW-11), pages 447–456, Hyderabad, India.

M. Banko and O. Etzioni. 2008. The tradeoffs between open
and traditional relation extraction. In Proceedings of the
46th Annual Meeting of the Association for Computational
Linguistics (ACL-08), pages 28–36, Columbus, Ohio.

K. Bellare, P. Talukdar, G. Kumaran, F. Pereira, M. Liber-
man, A. McCallum, and M. Dredze. 2007. Lightly-
supervised attribute extraction. In Proceedings of the
21st Annual Conference on Neural Information Process-
ing Systems (NIPS-07). Workshop on Machine Learning
for Web Search, Whistler, British Columbia.

M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. WebTables: Exploring the power of tables on the
Web. In Proceedings of the 34th Conference on Very
Large Data Bases (VLDB-08), pages 538–549, Auckland,
New Zealand.

G. Cui, Q. Lu, W. Li, and Y. Chen. 2009. Automatic acqui-
sition of attributes for ontology construction. In Proceed-
ings of the 22nd International Conference on Computer
Processing of Oriental Languages, pages 248–259, Hong
Kong.

D. Davidov and A. Rappoport. 2010. Extraction and ap-
proximation of numerical attributes from the Web. In Pro-
ceedings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL-10), Uppsala, Sweden.

J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In Proceedings of the
6th Symposium on Operating Systems Design and Imple-
mentation (OSDI-04), pages 137–150, San Francisco, Cal-
ifornia.

N. Guarino. 1992. Concepts, attributes and arbitrary rela-
tions. Data and Knowledge Engineering, 8:249–261.

M. Heilman and N. Smith. 2010. Good question! Statisti-
cal ranking for question generation. In Proceedings of the
2010 Conference of the North American Association for
Computational Linguistics (NAACL-HLT-10), pages 609–
617, Los Angeles, California.

408



Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
class learning from the Web with hyponym pattern link-
age graphs. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics (ACL-08),
pages 1048–1056, Columbus, Ohio.

D. Lin and P. Pantel. 2002. Concept discovery from
text. In Proceedings of the 19th International Confer-
ence on Computational linguistics (COLING-02), pages
1–7, Taipei, Taiwan.

D. Lin and X. Wu. 2009. Phrase clustering for discrimina-
tive learning. In Proceedings of the 47th Annual Meeting
of the Association for Computational Linguistics (ACL-
IJCNLP-09), pages 1030–1038, Singapore.

R. Mitkov and L. Ha. 2006. A computer-aided environment
for generating multiple-choice test items. Natural Lan-
guage Engineering, 12(2):177–194.

R. Mooney and R. Bunescu. 2005. Mining knowledge from
text using information extraction. SIGKDD Explorations,
7(1):3–10.

V. Nastase and M. Strube. 2008. Decoding Wikipedia cat-
egories for knowledge acquisition. In Proceedings of
the 23rd National Conference on Artificial Intelligence
(AAAI-08), pages 1219–1224, Chicago, Illinois.

M. Paşca and B. Van Durme. 2007. What you seek is what
you get: Extraction of class attributes from query logs. In
Proceedings of the 20th International Joint Conference on
Artificial Intelligence (IJCAI-07), pages 2832–2837, Hy-
derabad, India.

M. Paşca, E. Alfonseca, E. Robledo-Arnuncio, R. Martin-
Brualla, and K. Hall. 2010. The role of query sessions
in extracting instance attributes from web search queries.
In Proceedings of the 32nd European Conference on Infor-
mation Retrieval (ECIR-10), pages 62–74, Milton Keynes,
United Kingdom.

M. Paşca. 2007. Organizing and searching the World Wide
Web of facts - step two: Harnessing the wisdom of the
crowds. In Proceedings of the 16th World Wide Web Con-
ference (WWW-07), pages 101–110, Banff, Canada.

M. Paşca. 2011. Asking what no one has asked before: Us-
ing phrase similarities to generate synthetic web search
queries. In Proceedings of the 20th International Confer-
ence on Information and Knowledge Management (CIKM-
11), Glasgow, United Kingdom.

P. Pantel, E. Crestan, A. Borkovsky, A. Popescu, and V. Vyas.
2009. Web-scale distributional similarity and entity
set expansion. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Processing
(EMNLP-09), pages 938–947, Singapore.

K. Probst, R. Ghani, M. Krema, A. Fano, and Y. Liu. 2007.
Semi-supervised learning of attribute-value pairs from
product descriptions. In Proceedings of the 20th Interna-
tional Joint Conference on Artificial Intelligence (IJCAI-
07), pages 2838–2843, Hyderabad, India.

S. Raju, P. Pingali, and V. Varma. 2008. An unsupervised ap-
proach to product attribute extraction. In Proceedings of
the 31st International Conference on Research and Devel-
opment in Information Retrieval (SIGIR-08), pages 35–42,
Singapore.

M. Remy. 2002. Wikipedia: The free encyclopedia. Online
Information Review, 26(6):434.

F. Suchanek, G. Kasneci, and G. Weikum. 2007. Yago:
a core of semantic knowledge unifying WordNet and
Wikipedia. In Proceedings of the 16th World Wide Web
Conference (WWW-07), pages 697–706, Banff, Canada.

K. Tokunaga, J. Kazama, and K. Torisawa. 2005. Automatic
discovery of attribute words from Web documents. In
Proceedings of the 2nd International Joint Conference on
Natural Language Processing (IJCNLP-05), pages 106–
118, Jeju Island, Korea.

Tom White. 2010. Hadoop: the Definitive Guide. O’Reilly
Media, 2nd edition.

T. Wong, W. Lam, and T. Wong. 2008. An unsuper-
vised framework for extracting and normalizing product
attributes from multiple Web sites. In Proceedings of the
31st International Conference on Research and Develop-
ment in Information Retrieval (SIGIR-08), pages 35–42,
Singapore.

F. Wu and D. Weld. 2008. Automatically refining the
Wikipedia infobox ontology. In Proceedings of the 17th
World Wide Web Conference (WWW-08), pages 635–644,
Beijing, China.

F. Wu, R. Hoffmann, and D. Weld. 2008. Information extrac-
tion from Wikipedia: Moving down the long tail. In Pro-
ceedings of the 14th ACM SIGKDD Conference on Knowl-
edge Discovery and Data Mining (KDD-08), pages 731–
739.

N. Yoshinaga and K. Torisawa. 2007. Open-domain
attribute-value acquisition from semi-structured texts. In
Proceedings of the 6th International Semantic Web Con-
ference (ISWC-07), Workshop on Text to Knowledge: The
Lexicon/Ontology Interface (OntoLex-2007), pages 55–
66, Busan, South Korea.

409


