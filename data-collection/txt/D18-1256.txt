











































Decoupling Strategy and Generation in Negotiation Dialogues


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2333–2343
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2333

Decoupling Strategy and Generation in Negotiation Dialogues

He He and Derek Chen and Anusha Balakrishnan and Percy Liang
Computer Science Department, Stanford University

{hehe,derekchen14,anusha,pliang}@cs.stanford.edu

Abstract

We consider negotiation settings in which
two agents use natural language to bargain
on goods. Agents need to decide on both
high-level strategy (e.g., proposing $50) and
the execution of that strategy (e.g., generat-
ing “The bike is brand new. Selling for just
$50!”). Recent work on negotiation trains
neural models, but their end-to-end nature
makes it hard to control their strategy, and
reinforcement learning tends to lead to de-
generate solutions. In this paper, we pro-
pose a modular approach based on coarse di-
alogue acts (e.g., propose(price=50)) that de-
couples strategy and generation. We show
that we can flexibly set the strategy using su-
pervised learning, reinforcement learning, or
domain-specific knowledge without degener-
acy, while our retrieval-based generation can
maintain context-awareness and produce di-
verse utterances. We test our approach on the
recently proposed DEALORNODEAL game,
and we also collect a richer dataset based on
real items on Craigslist. Human evaluation
shows that our systems achieve higher task
success rate and more human-like negotiation
behavior than previous approaches.

1 Introduction

A good negotiator needs to decide on the strat-
egy for achieving a certain goal (e.g., proposing
$6000) and the realization of that strategy via gen-
eration of natural language (e.g., “I really need a
car so I can go to work, but all I have is 6000, any
more and I won’t be able to feed my children.”).

Most past work in NLP on negotiation focuses
on strategy (dialogue management) with either no
natural language (Cuayáhuitl et al., 2015; Cao
et al., 2018) or canned responses (Keizer et al.,
2017; Traum et al., 2008). Recently, end-to-end
neural models (Lewis et al., 2017; He et al., 2017)
are used to simultaneously learn dialogue strategy

and language realization from human-human di-
alogues, following the trend of using neural net-
work models on both goal-oriented dialogue (Wen
et al., 2017a; Dhingra et al., 2017) and open-
domain dialogue (Sordoni et al., 2015; Li et al.,
2017; Lowe et al., 2017). However, these models
have two problems: (i) it is hard to control and in-
terpret the strategies, and (ii) directly optimizing
the agent’s goal through reinforcement learning
often leads to degenerate solutions where the utter-
ances become ungrammatical (Lewis et al., 2017)
or repetitive (Li et al., 2016).

To alleviate these problems, our key idea is to
decouple strategy and generation, which gives us
control over the strategy such that we can achieve
different negotiation goals (e.g., maximizing util-
ity, achieving a fair deal) with the same language
generator. Our framework consists of three com-
ponents shown in Figure 1: First, the parser iden-
tifies keywords and entities to map each utter-
ance to a coarse dialogue act capturing the high-
level strategic move. Then, the dialogue man-
ager chooses a responding dialogue act based on a
sequence-to-sequence model over coarse dialogue
acts learned from parsed training dialogues. Fi-
nally, the generator produces an utterance given
the dialogue act and the utterance history.

Our framework follows that of traditional goal-
oriented dialogue systems (Young et al., 2013),
with one important difference: coarse dialogue
acts are not intended to and cannot capture the
full meaning of an utterance. As negotiation di-
alogues are fairly open-ended, the generator needs
to depend on the full utterance history. For exam-
ple, consider the first turn in Figure 1. We can-
not generate a response given only the dialogue
act inform; we must also look at the previous ques-
tion. However, we still optimize the dialogue man-
ager in the coarse dialogue act space using super-
vised learning, reinforcement learning, or domain-



2334

inquire inform propose(price=125)
counter
(price=160)

counter
(price=150) agree

Buyer: Are you 
willing to lower 
the price at all?

Seller: yes

Buyer:  I would 
like to pay 125 for 
it.  How does that 
sound?

Buyer: How 
about 150? That 
is a bit lower.

Seller: i’ll 
accept 160

Seller: i can 
do that!

Manager

Parser
Generator

Words

Coarse 
dialogue acts

ztz<t

xt

x<t

Figure 1: Our modular framework consists of three components similar to traditional goal-oriented di-
alogue systems. (1) The parser maps received utterances to coarse dialogue acts (an intent and its ar-
guments) that capture the high-level dialogue flow. (2) The manager generates the next coarse dialogue
act zt conditioned on past dialogue acts z<t. (3) The generator then produces a response conditioned on
both the predicted coarse dialogue act zt and the dialogue history x<t. Importantly, unlike in traditional
systems, coarse dialogue acts only capture the rough shape of a dialogue, not the full meaning of its
utterances, e.g., inform does not specify the answer to the question.

specific knowledge.
Existing human-human negotiation datasets are

grounded in closed-domain games with a fixed set
of objects such as Settlers of Catan (lumber, coal,
brick, wheat, and sheep) (Afantenos et al., 2012;
Asher et al., 2016) or item division (book, hat,
and ball) (DeVault et al., 2015; Lewis et al., 2017).
These objects lack the richness of the real world.
To study human negotiation in more open-ended
settings that involve real goods, we scraped post-
ings of items for sale from craigslist.org
as our negotiation scenario. By hiring workers
on Amazon Mechanical Turk (AMT) to play the
role of buyers and sellers, we collected a new
dataset (CRAIGSLISTBARGAIN) of negotiation di-
alogues.1 Compared to existing datasets, our more
realistic scenario invites richer negotiation behav-
ior involving open-ended aspects such as cheap
talk or side offers.

We evaluate two families of systems modeling
coarse dialogue acts and words respectively, which
are optimized by supervised learning, reinforce-
ment learning, or domain knowledge. Each sys-
tem is evaluated on our new CRAIGSLISTBAR-
GAIN dataset and the DEALORNODEAL dataset
of Lewis et al. (2017) by asking AMT workers to
chat with the system in an A/B testing setting. We
focus on two metrics: task-specific scores (e.g.,
utility) and human-likeness. We show that rein-
forcement learning on coarse dialogue acts avoids

1 Available at https://stanfordnlp.github.
io/cocoa.

degenerate solutions, which was a problem in Li
et al. (2016); Lewis et al. (2017). Our modular
model maintains reasonable human-like behavior
while still optimizes the objective. Furthermore,
we find that models trained over coarse dialogue
acts are stronger negotiators (even with only su-
pervised learning) and produce more diverse ut-
terances than models trained over words. Finally,
the interpretability of coarse dialogue acts allows
system developers to combine the learned dia-
logue policy with hand-coded rules, thus imposing
stronger control over the desired strategy.

2 Craigslist Negotiation Dataset

Previous negotiation datasets were collected in the
context of games. For example, Asher et al. (2016)
collected chat logs from online Settlers of Catan.
Lewis et al. (2017) asked two people to divide a
set of hats, books, and balls. While such games
are convenient for grounding and evaluation, it re-
stricts the dialogue domain and the richness of the
language. Most utterances are direct offers such as
“has anyone got wood for me?” and “I want the
ball.”, whereas real-world negotiation would in-
volve more information gathering and persuasion.

To encourage more open-ended, realistic ne-
gotiation, we propose the CRAIGSLISTBARGAIN
task. Two agents are assigned the role of a buyer
and a seller; they are asked to negotiate the price
of an item for sale on Craigslist given a descrip-
tion and photos. As with the real platform, the
listing price is shown to both agents. We addition-

craigslist.org
https://stanfordnlp.github.io/cocoa
https://stanfordnlp.github.io/cocoa


2335

JVC HD-ILA 1080P 70 Inch TV

Tv is approximately 10 years old.
Just installed new lamp. There are 2
HDMI inputs. Works and looks like
new.
Listing price: $275
Buyer’s target price: $192

Agent Utterance Dialogue Act

Buyer Hello do you still have the TV? greet
Seller Hello, yes the TV is still available greet

Buyer What condition is it in? Any scratches or problems? I see itrecently got repaired inquire

Seller It is in great condition and works like a champ! I just installeda new lamp in it. There aren’t any scratches or problems. inform

Buyer All right. Well I think 275 is a little high for a 10 year old TV.Can you lower the price some? How about 150? propose(150)

Seller
I am willing to lower the price, but $150 is a little too low.
How about $245 and if you are not too far from me, I will
deliver it to you for free?

counter(245)

Buyer It’s still 10 years old and the technology is much older. Willyou do 225 and you deliver it. How’s that sound? counter(225)

Seller Okay, that sounds like a deal! agree
Buyer Great thanks! agree
Seller OFFER $225.0 o↵er(225)
Buyer ACCEPT accept

Table 1: Example dialogue between two people negotiating the price of a used TV.

ally suggest a private price to the buyer as a tar-
get. Agents chat freely in alternating turns. Either
agent can enter an offer price at any time, which
can be accepted or rejected by the partner. Agents
also have the option to quit, in which case the task
is completed with no agreement.

To generate the negotiation scenarios, we
scraped postings on sfbay.craigslist.org
from the 6 most popular categories (housing, fur-
niture, cars, bikes, phones, and electronics). Each
posting produces three scenarios with the buyer’s
target prices at 0.5x, 0.7x and 0.9x of the listing
price. Statistics of the scenarios are shown in Ta-
ble 2.

We collected 6682 human-human dialogues on
AMT using the interface shown in Appendix A
Figure 2. The dataset statistics in Table 3 show
that CRAIGSLISTBARGAIN has longer dialogues
and more diverse utterances compared to prior
datasets. Furthermore, workers were encouraged
to embellish the item and negotiate side offers
such as free delivery or pick-up. This highly re-
latable scenario leads to richer dialogues such as
the one shown in Table 1. We also observed vari-
ous persuasion techniques listed in Table 4 such as
embellishment, side offers, and appeals to sympa-
thy.

3 Approach

3.1 Motivation
While end-to-end neural models have made
promising progress in dialogue systems (Wen
et al., 2017a; Dhingra et al., 2017), we find they

# of unique postings 1402
% with images 80.8
Avg # of tokens per description 42.6
Avg # of tokens per title 33.8
Vocab size 12872

Table 2: Statistics of CRAIGSLISTBARGAIN sce-
narios.

CB DN SoC

# of dialogues 6682 5808 1081
Avg # of turns 9.2 6.6 8.5
Avg # of tokens per turn 15.5 7.6 4.2
Vocab size 13928 2719 4921
Vocab size (excl. numbers) 11799 2623 4735

Table 3: Comparison of dataset statistics of
CRAIGSLISTBARGAIN (CB), DEALORN-
ODEAL (DN), and SETTLERSOFCATAN (SoC).
CRAIGSLISTBARGAIN contains longer, more
diverse dialogues on average.

struggle to simultaneously learn the strategy and
the rich utterances necessary to succeed in the
CRAIGSLISTBARGAIN domain, e.g., Table 8(a)
shows a typical dialogue between a human and
a sequence-to-sequence-based bot, where the bot
easily agrees. We wish to now separate negoti-
ation strategy and language generation. Suppose
the buyer says: “All right. Well I think 275 is a
little high for a 10 year old TV. Can you lower the
price some? How about 150?” We can capture the
highest-order bit with a coarse dialogue act pro-
pose(price=150). Then, to generate the seller’s
response, the agent can first focus on this coarse

sfbay.craigslist.org


2336

Phenomenon Example

Embellishment It is in great condition and works like a champ! I just installed a new lamp in it. There aren’t anyscratches or problems.
Cheap talk How about i give you $20 and you keep the helmet. its for my daughter for her job, she deliverslemonade.
Side offers Throw in a couple of movies with that DVD player, and you have yourself a deal.

Appeal to sympathy I would love to have this for my mother, she is very sick and this would help her and with me takingcare of her and having to take a leave from work I can’t pay very much of it
World knowledge For a Beemer 5 series in this condition, I really can’t go that low.

Table 4: Rich negotiation language in our CRAIGSLISTBARGAIN dataset.

dialogue act rather than having to ingest the free-
form text all at once. Once a counter price is de-
cided, the rest is open-ended justification for the
proposed price, e.g., emphasizing the quality of
the TV despite its age.

Motivated by these observations, we now de-
scribe a modular framework that extracts coarse
dialogue acts from utterances, learns to optimize
strategy in the dialogue act space, and uses re-
trieval to fill in the open-ended parts conditioned
on the full dialogue history.

3.2 Overview
Our goal is to build a dialogue agent that takes
the dialogue history, i.e. a sequence of utterances
x1, . . . , xt�1 along with the dialogue scenario c
(e.g., item description), and produces a distribu-
tion over the responding utterance xt.

For each utterance xt (e.g., “I am willing to
pay $15”), we define a coarse dialogue act zt
(e.g., propose(price=15)); the coarse dialogue act
serves as a logical skeleton which does not attempt
to capture the full semantics of the utterance. Fol-
lowing the strategy of traditional goal-oriented di-
alogue systems (Young et al., 2013), we broadly
define our model in terms of the following three
modules:

1. A parser that (deterministically) maps an in-
put utterance xt�1 into a coarse dialogue act
zt�1 given the dialogue history x<t and z<t,
as well as the scenario c.

2. A manager that predicts the responding di-
alogue act zt given past coarse dialogue acts
z<t and the scenario c.

3. A generator that turns the coarse dialogue
act zt to a natural language response xt given
the full dialogue history x<t.

Because coarse dialogue acts do not capture the
full semantics, the parser and the generator main-
tains full access to the dialogue history. The main

restriction is the manager examining the dialogue
acts, which we show will reduce the risk of degen-
eracy during reinforcement learning Section 4.4.
We now describe each module in detail (Figure 1).

3.3 Parser
Our framework is centered around the coarse dia-
logue act z, which consists of an intent and a set
of arguments. For example, “I am willing to pay
$15” is mapped to propose(price=15). The fact
that our coarse dialogue acts do not intend to cap-
ture the full semantics of a sentence allows us to
use a simple rule-based parser. It detects the intent
and its arguments by regular expression matching
and a few if-then rules. Our parser starts by de-
tecting entities (e.g., prices, objects) and matching
keyword patterns (e.g., “go lower”). These sig-
nals are checked against an ordered list of rules,
where we choose the first matched intent in the
case of multiple matches. An unknown act is out-
put if no rule is triggered. The list of intent parsing
rules used are shown in Table 5. Please refer to
Appendix B for argument parsing based on entity
detection.

3.4 Manager
The dialogue manager decides what action zt the
dialogue agent should take at each time step t
given the sequence of past coarse dialogue acts
z<t and the scenario c. Below, we describe three
ways to learn the dialogue manager with increas-
ing controllability: modeling human behavior in
the training corpus (supervised learning), explic-
itly optimizing a reward function (reinforcement
learning), and injecting hand-coded rules (hybrid
policy).

Supervised learning. Given a parsed train-
ing corpus, each training example is a se-
quence of coarse dialogue acts over one dialogue,
z1, . . . , zT . We learn the transition probabilities



2337

Generic Rules

Intent Matching Patterns

greet hi, hello, hey, hiya, howdy
disagree no, not, n’t, nothing, dont

agree
not disagree and ok, okay, great, perfect,
deal, that works, i can do that

insist
the same offer as the previous one is
detected

inquire
starts with an interrogative word (e.g., what,
when, where) or particle (e.g., do, are)

CRAIGSLISTBARGAIN Rules

Intent Matching Patterns

intro greet or how are you, interested
propose first price mention

vague-price
no price mention and come down, highest,
lowest, go higher/lower, too high/low

counter new price detected
inform previous coarse dialogue act was inquire

DEALORNODEAL Rules

Intent Matching Patterns

propose items and respective counts are detected

Table 5: Rules for intent detection in the parser.

p✓(zt | z<t, c) by maximizing the likelihood of
the training data.

We use a standard sequence-to-sequence model
with attention. Each coarse dialogue act is repre-
sented as a sequence of tokens, i.e. an intent fol-
lowed by each of its arguments, e.g., “o↵er 150”.
During the agent’s listening turn, an LSTM en-
codes the received coarse dialogue act; during its
speaking turn, another LSTM decodes the tokens
in the coarse dialogue act. The hidden states are
carried over the entire dialogue to provide full his-
tory.

The vocabulary of coarse dialogue acts is much
smaller than the word vocabulary. For example,
our implementation includes fewer than 10 intents
and argument values are normalized and binned
(see Section 4.2).

Reinforcement learning. Supervised learning
aims to mimic the average human behavior, but
sometimes we want to directly optimize for a par-
ticular dialogue goal. In reinforcement learning,
we define a reward R(z1:T ) on the entire sequence
of coarse dialogue acts. Specifically, we experi-
ment with three reward functions:

• Utility is the objective of a self-interested
agent. For CRAIGSLISTBARGAIN, we set the
utility function to be a linear function of the
final price, such that the buyer has a utility of

1 at their target price, the seller has a utility
of 1 at the listing price, and both agents have
a utility of zero at the midpoint of the list-
ing price and the buyer’s target price, making
it a zero-sum game. For DEALORNODEAL,
utility is the total value of objects given to the
agent.

• Fairness aims to achieve equal outcome for
both agents, i.e. the difference between two
agents’ utilities.

• Length is the number of utterances in a dia-
logue, thus encourages agents to chat as long
as possible.

The reward is �1 if no agreement is reached.
We use policy gradient (Williams, 1992) for op-

timization. Given a sampled trajectory z1:T and
the final reward r, let ai be the i-th generated to-
ken (i.e. “action” taken by the policy) along the
trajectory. We update the parameters ✓ by

✓  ✓ � ⌘
X

i

r✓ log p✓(ai | a<i, c)(r � b) (1)

where ⌘ is the learning rate and b is a baseline es-
timated by the average return so far for variance
reduction.

Hybrid policy. Given the interpretable coarse
dialogue acts, a simple option is to write a rule-
based manager with domain knowledge, e.g., if
zt�1 = greet, then zt = greet. We combine
these rules with a learned manager to fine-tune the
dialogue policy. Specifically, the dialogue man-
ager predicts the intent from a learned sequence
model but fills in the arguments (e.g., price) using
rules. For example, given a predicted intent pro-
pose, we can set the price to be the average of the
buyer’s and seller’s current proposals (a split-the-
difference strategy).

3.5 Generator
We use retrieval-based generation to condition on
both the coarse dialogue act and the dialogue his-
tory. Each candidate in our database for retrieval is
a tuple of an utterance xt and its dialogue context
xt�1, represented by both templates and coarse di-
alogue acts. i.e. (d(xt�1), zt�1, d(xt), zt), where
d is the template extractor. Specifically, given a
parsed training set, each utterance is converted to a
template by delexicalizing arguments in its coarse
dialogue act. For example, “How about $150?”



2338

becomes “How about [price]?”, where [price] is a
placeholder to be filled in at generation time.

At test time, given zt from the dialogue man-
ager, the generator first retrieves candidates with
the same intent as zt and zt�1. Next, candi-
dates are ranked by similarity between their con-
text templates and the current dialogue context.
Specifically, we represent the context d(xt�1) as
a TF-IDF weighted bag-of-words vector and sim-
ilarity is computed by a dot product of two con-
text vectors. To encourage diversity, the generator
samples an utterance from the top K candidates
according to the distribution given by a trigram
language model estimated on the training data.

4 Experiments

4.1 Tasks
We test our approach on two negotiation tasks.
CRAIGSLISTBARGAIN (Section 2) asks a buyer
and a seller to negotiate the price of an item
for sale given its Craigslist post. DEALORN-
ODEAL (Lewis et al., 2017) asks two agents to
divide a set of items given their private utility func-
tions.

4.2 Models
We compare two families of models: end-to-end
neural models that directly map the input dialogue
context to a sequence of output words, and our
modular models that use coarse dialogue acts as
the intermediate representation.

We start by training the word-based model
and the act-based model with supervised learning
(SL).

• SL(word): a sequence-to-sequence model
with attention over previous utterances and
the scenario, both embedded as a continuous
Bag-of-Words;

• SL(act): our model described in Section 3
with a rule-based parser, a learned neural di-
alogue manager, and a retrieval-based gener-
ator.

To handle the large range of argument values
(prices) in CRAIGSLISTBARGAIN for act-based
models, we normalize the prices such that an
agent’s target price is 1 and the bottomline price
is 0. For the buyer, the target is given and the
bottomline is the listing price. For the seller, the
target is the listing price and the bottomline is set
to 0.7x of the listing price. The prices are then

Model z Parser Manager Generator

SL/RL(word) vector learned learned generative
SL/RL(act) logical rules learned retrieval
SL(act)+rule logical rules hybrid retrieval

Table 6: Comparison of different implementation
of the core modules in our framework.

binned according to their approximate values with
two digits after the decimal point.

Next, given the pretrained SL models, we
fine-tune them with the three reward functions
(Section 3.4), producing RLutility, RLfairness, and
RLlength.

In addition, we compare with the hybrid model,
SL(act)+rule. It predicts the next intent using
a trigram language model learned over intent se-
quences in the training data, and fills in the argu-
ments with hand-coded rules. For CRAIGSLIST-
BARGAIN, the only argument is the price. The
agent always splits the difference when making
counter proposals, rejects an offer if it is worse
than its bottomline and accepts otherwise. For
DEALORNODEAL, the agent maintains an esti-
mate of the partner’s private utility function. In
case of disagreement, it gives up the item with the
lowest value of (own utility � partner utility) and
takes an item of estimated zero utility to the part-
ner. The agent agrees whenever a proposal is bet-
ter than the last one or its predefined target. A
high-level comparison of all models is shown in
Table 6.

4.3 Training Details

CRAIGSLISTBARGAIN For SL(word), we use
a sequence-to-sequence model with attention over
3 previous utterances and the negotiation sce-
nario (embedded as a continuous Bag-of-Words).
For both SL(word) and SL(act), we use 300-
dimensional word vectors initialized by pretrained
GloVe word vectors (Pennington et al., 2014), and
a two-layer LSTM with 300 hidden units for both
the encoder and the decoder. Parameters are ini-
tialized by sampling from a uniform distribution
between -0.1 and 0.1. For optimization, we use
AdaGrad (Duchi et al., 2010) with a learning rate
of 0.01 and a mini-batch size of 128. We train the
model for 20 epochs and choose the model with
the lowest validation loss.

For RL, we first fit a partner model using su-
pervised learning (e.g., SL(word)), then run RL



2339

CRAIGSLISTBARGAIN DEALORNODEAL
Hu Ut Fa Ag Len Hu Ut Fa Ag Len

Human 4.3 -0.07 -0.14 0.91 10.2 4.6 5.5 vs. 5.3 -0.2 0.78 5.8

SL(word) 3.0 -0.32 -0.64 0.75 7.8 3.8 4.7 vs. 5.0 -0.3 0.70 5.0
SL(act) 3.3 0.06 -0.12 0.84 14.0 3.2 5.2 vs. 5.0 -0.2 0.67 7.0
SL(act)+rule 3.6 0.23 -0.46 0.75 11.4 4.2 5.2 vs. 5.2 0 0.72 8.0

RLutility(word) 1.7 1.00 -2.00 0.31 2.5 1.7 2.9 vs. 1.8 -1.1 0.33 10.4
RLutility(act) 2.8 1.00 -2.00 0.22 6.7 2.8 3.3 vs. 2.3 -1.0 0.38 9.5

RLfairness(word) 1.8 -0.62 -1.24 0.75 9.4 3.2 5.7 vs. 5.9 -0.2 0.79 4.0
RLfairness(act) 3.0 -0.28 -0.56 0.68 7.1 3.5 4.2 vs. 5.4 -1.2 0.77 7.6

RLlength(word) 1.9 -0.79 -1.58 0.85 13.8 1.6 3.4 vs. 2.9 -0.5 0.48 9.2
RLlength(act) 3.0 0.89 -1.78 0.40 11.8 2.5 2.5 vs. 3.1 -0.6 0.54 11.0

Table 7: Human evaluation results on human-likeness (Hu), agreement rate (Ag), and RL objectives,
including agent utility (Ut), deal fairness (Fa), and dialogue length (Len). Results are grouped by the
optimization objective. For each group of RL models, the column of the optimization objective is high-
lighted. For human-likeness, scores that are better than others in the same group with statistical signifi-
cance (p < 0.05 given by paired t-tests) are in bold. Overall, with SL, all models are human-like, how-
ever, act-based models better matches human statistics across all metrics; with RL, word-based models
becomes degenerate, whereas act-based models optimize the reward while maintaining human-likeness.

against it. One agent is updated by policy gradi-
ent and the partner model is fixed during training.
We use a learning rate of 0.001 and train for 5000
episodes (dialogues). The model with the highest
reward on the validation set is chosen.

DEALORNODEAL For act-based models, we
use the same parameterization as CRAIGSLIST-
BARGAIN. For word-based models, we use the
implementation from Lewis et al. (2017).2 Note
that for fair comparison, we did not apply SL in-
terleaving during RL training and rollouts during
inference.

4.4 Human Evaluation
We evaluated each system on two metrics: task-
specific scores (e.g., utility) and human-likeness.
The scores tell us how well the system is play-
ing the game, and human-likeness tells us whether
the bot deviates from human behavior, presumably
due to over-optimization.

We put up all 9 systems online and hired work-
ers from AMT to chat with the bots. Each worker
was randomly paired with one of the bots or an-
other worker, so as to compare the bots with hu-
man performance under the same conditions. At

2https://github.com/facebookresearch/
end-to-end-negotiator

the end of a chat, workers were asked the question
“Do you think your partner demonstrated reason-
able human behavior?”. They provided answers
on a Likert scale from 1 (not at all) to 5 (defi-
nitely). Table 7 shows the human evaluation re-
sults on CRAIGSLISTBARGAIN and DEALORN-
ODEAL respectively. We also show example
human-bot dialogues in Table 8 and Appendix C.

SL(act) learns more human-like behavior. We
first compare performance of SL models over
words and coarse dialogue acts. Both SL(word)
and SL(act) achieved similar scores on human-
likeness (no statistically significant difference).
However, SL(word) better matched human statis-
tics such as dialogue length and utility. For in-
stance, SL(word) tended to produce short, generic
utterances as shown in Table 8(a); they also agreed
on a deal more quickly because utterances such
as “deal” and “I can do that” are frequent in
negotiation dialogues. This behavior is reflected
by the shorter dialogue length and lower utility of
SL(word) models.

RL(word) leads to degeneracy. On
CRAIGSLISTBARGAIN, all RL(word) mod-
els clearly have low scores on human-likeness
in Table 7. They merely learned to repeat
a few sentences: The three most frequent

https://github.com/facebookresearch/end-to-end-negotiator
https://github.com/facebookresearch/end-to-end-negotiator


2340

Title: Fitbit Flex + 2 Bands Activity Tracker Listing Price: $60
Description: Hi, I’m selling a Fitbit Activity Tracker. Track your day including steps, distance, calories burned, active minutes.
Track your night including hours slept, times to woken up, sleep quality, Wake with a silent vibrating alarm. You can set a ...

(a) A: SL(word) (Buyer) B: Human (Seller) (b) A: SL(act) (Buyer) B: Human (Seller)

A: hi, i ’m interested in your fitbit.
B: Great. It really is a good deal.
A: great.
B: Do you have any questions about it?
A: i ’m interested in your fitbit. i ’m not sure i ’m not sure
i ’m interested.
B: It has so many functions that are great. I particularly like
the sleep tracking. Plus it is waterproof. How about I will
deliver it to you and you give me 60 dollars?
A: i can do $52 if you come pick it up.
B: Well I am selling it. So I will deliver it for 60. Deal?
A: yes, i can do that.
B: OFFER($60) A: ACCEPT

A: how we doing today
B: Hi!
A: yes, i really need one, what color is it?
B: It has a black band and a white band.
A: i see, so you bought an extra one by mistake?
B: No, it comes with 2 bands
A: i would like to make you a offer of 36
B: Ooooh....yeah, I can’t. Sorry. It’s wireless, has Bluetooth,
extra bands, and is water resistant. I could do $50.
A: 49?
B: You know..I’d do $49.
A: thank you so much!
B: OFFER($49) A: ACCEPT

(c) A: RLlength(word) (Buyer) B: Human (Seller) (d) A: RLlength(act) (Buyer) B: Human (Seller)

A: hello
B: Hi how are you?
A: i am.
B: you are interested in the fitbit flex and the 2 trackers?
A: it’s in good condition condition.
B: yes
A: ok, that sounds good.
B: I am selling it at $60?
A: it’s in good condition condition.
B: yes
A: ok, that sounds good.
B: $60
A: i can go that low.

A: hi, i saw your ad about the item.
B: Okay great, i’m selling a Fitbit Flex plus 2 bands
A: how old is the item?
B: I’ve only had it for about a few months.
A: but it does work correct?
B: yes it does work, it’s in great condition.
A: excellent. would you consider taking 36 for it?
B: I’m selling for $60 but $36 would be way too low.
A: how about 36?
B: no I cannot accept $36, I’m sorry
A: i’m sorry. would you accept 36?
B: I am not going lower than $50
A: OFFER($36) A: REJECT

Table 8: Example human-bot chats on CRAIGSLISTBARGAIN, where bot utterances are in bold.
SL(word) produced generic responses, while SL(act) is more human-like. RLlength(word) devolved into
degenerate behavior repeating itself while RLlength(act) maintained coherency. Only the first half of the
item description and the RLlength(word) chat are shown due to space limit.

sentences of RLutility(word), RLfairness(word),
and RLlength(word) account for 81.6%, 100%
and 100% of all utterances. For example,
RLutility(word) almost always opened with “i
can pick it up”, then offer its target price.
RLlength(word) repeated generic sentences un-
til the partner submitted a price. While they
scored high on the reward being optimized, the
conversations are unnatural.

On DEALORNODEAL, we have observed sim-
ilar patterns. A general strategy learned by
RL(word) was to pick an offer depending on its
objective, then repeat the same utterance over
and over again (e.g., “i need the ball.”), result-
ing in low human-likeness scores. One excep-
tion is RLfairness(word), since most of its offers
were reasonable and agreed on immediately (it has
the shorted dialogue length), the conversations are
natural.

RL(act) optimizes different negotiation goals
while being human-like. On both tasks,
RL(act) models optimized their rewards while
maintaining reasonable human-likeness scores.
We now show that different models demon-
strated different negotiation behavior. Two main
strategies learned by RLlength(act) were to ask
questions and to postpone offer submission. On
CRAIGSLISTBARGAIN, when acting as a buyer,
42.4% of its utterances were questions, compared
to 30.2% for other models. On both tasks, it
tended to wait for the partner to submit an offer
(even after a deal was agreed on), compared to
RLmargin(act) which almost always submitted
offers first. For RLfairness(act), it aimed to agree
on a price in the middle of the listing price and the
buyer’s target price for CRAIGSLISTBARGAIN.
Since the buyer’s target was hidden, when the
agent was the seller, it tended to wait for the buyer
to propose prices first. Similary, on DEALORN-



2341

ODEAL it waited to hear the parter’s offer and
sometimes changed its offer afterwards, whereas
the other models often insisted on one offer.

On both tasks, RLutility(act) learned to insist on
its offer and refuse to budge. This ended up frus-
trating many people, which is why it has a low
agreement rate. The problem is that our human
model is simply a SL model trained on human-
human dialogues, which may not accurately re-
flects real human behavior during human-bot chat.
For example, the SL model often agrees after a few
turns of insistence on a proposal, whereas humans
get annoyed if the partner is not willing to make
compromises at all. However, by injecting domain
knowledge to SL(act)+rule, e.g., making a small
compromise is better than stubbornly being fixed
on a single price, we were able to achieve high
utility and human-likeness on both CRAIGSLIST-
BARGAIN and DEALORNODEAL.

5 Related Work and Discussion

Recent work has explored the space between
goal-oriented dialogue and open-domain chit-chat
through collaborative or competitive language
games, such as collecting cards in a maze (Potts,
2012), finding a mutual friend (He et al., 2017), or
splitting a set of items (DeVault et al., 2015; Lewis
et al., 2017). Our CRAIGSLISTBARGAIN dialogue
falls in this category, but exhibits richer and more
diverse language than prior datasets. Our dataset
calls for systems that can handle both strategic
decision-making and open-ended text generation.

Traditional goal-oriented dialogue systems
build a pipeline of modules (Young et al., 2013;
Williams et al., 2016). Due to the laborious dia-
logue state design and annotation, recent work has
been exploring ways to replace these modules with
neural networks and end-to-end training while still
having a logical backbone (Wen et al., 2017a; Bor-
des and Weston, 2017; He et al., 2017). Our
work is closely related to the Hybrid Code Net-
work (Williams et al., 2017), but the key difference
is that Williams et al. (2017) uses a neural dialogue
state, whereas we keep a structured, interpretable
dialogue state which allows for stronger top-down
control. Another line of work tackles this prob-
lem by introducing latent stochastic variables to
model the dialogue state (Wen et al., 2017b; Zhao
et al., 2017; Cao and Clark, 2017). While the la-
tent discrete variable allows for post-hoc discov-
ery of dialogue acts and increased utterance diver-

sity, it does not provide controllability over the di-
alogue strategy.

Our work is also related to a large body of liter-
ature on dialogue policies in negotiation (English
and Heeman, 2005; Efstathiou and Lemon, 2014;
Hiraoka et al., 2015; Cao et al., 2018). These work
mostly focus on learning good negotiation policies
in a domain-specific action space, whereas our
model operates in an open-ended space of natural
language. An interesting future direction is to con-
nect with game theory (Brams, 2003) for complex
multi-issue bargaining. Another direction is learn-
ing to generate persuasive utterances, e.g., through
framing (Takuya et al., 2014) or accounting for the
social and cultural context (Elnaz et al., 2012).

To conclude, we have introduced CRAIGSLIST-
BARGAIN, a rich dataset of human-human nego-
tiation dialogues. We have also presented a mod-
ular approach based on coarse dialogue acts that
models a rough strategic backbone as well allow-
ing for open-ended generation. We hope this work
will spur more research in hybrid approaches that
can work in open-ended, goal-oriented settings.

Acknowledgments. This work is supported by
DARPA Communicating with Computers (CwC)
program under ARO prime contract no. W911NF-
15-1-0462. We thank members of the Stan-
ford NLP group for insightful discussion and the
anonymous reviewers for constructive feedback.

Reproducibility. All code, data, and
experiments for this paper are avail-
able on the CodaLab platform: https:
//worksheets.codalab.org/worksheets/

0x453913e76b65495d8b9730d41c7e0a0c/.

References
S. Afantenos, N. Asher, F. Benamara, A. Cadilhac,

C. Dégremont, P. Denis, M. Guhe, S. Keizer, A. Las-
carides, O. Lemon, et al. 2012. Modelling strategic
conversation: Model, annotation design and corpus.
In Proceedings of SemDial 2012: Workshop on the
Semantics and Pragmatics of Dialogue, pages 167–
168.

N. Asher, J. Hunter, M. Morey, F. Benamara, and
S. Afantenos. 2016. Discourse structure and dia-
logue acts in multiparty dialogue: the STAC corpus.
In Language Resources and Evaluation Conference
(LREC).

A. Bordes and J. Weston. 2017. Learning end-to-end
goal-oriented dialog. In International Conference
on Learning Representations (ICLR).

https://worksheets.codalab.org/worksheets/0x453913e76b65495d8b9730d41c7e0a0c/
https://worksheets.codalab.org/worksheets/0x453913e76b65495d8b9730d41c7e0a0c/
https://worksheets.codalab.org/worksheets/0x453913e76b65495d8b9730d41c7e0a0c/


2342

S. J. Brams. 2003. Negotiation Games: Applying
Game Theory to Bargaining and Arbitration. Psy-
chology Press.

K. Cao and S. Clark. 2017. Latent variable dialogue
models and their diversity. In European Association
for Computational Linguistics (EACL).

K. Cao, A. Lazaridou, M. Lanctot, J. Z. Leibo,
K. Tuyls, and S. Clark. 2018. Emergent commu-
nication through negotiation. In International Con-
ference on Learning Representations (ICLR).

H. Cuayáhuitl, S. Keizer, and O. Lemon. 2015. Strate-
gic dialogue management via deep reinforcement
learning. In Advances in Neural Information Pro-
cessing Systems (NIPS).

D. DeVault, J. Mell, and J. Gratch. 2015. Toward natu-
ral turn-taking in a virtual human negotiation agent.
In Association for the Advancement of Artificial In-
telligence (AAAI).

B. Dhingra, L. Li, X. Li, J. Gao, Y. Chen, F. Ahmed,
and L. Deng. 2017. End-to-end reinforcement learn-
ing of dialogue agents for information access. In As-
sociation for Computational Linguistics (ACL).

J. Duchi, E. Hazan, and Y. Singer. 2010. Adaptive sub-
gradient methods for online learning and stochastic
optimization. In Conference on Learning Theory
(COLT).

I. Efstathiou and O. Lemon. 2014. Learning non-
cooperative dialogue behaviours. In Special Interest
Group on Discourse and Dialogue (SIGDIAL).

N. Elnaz, G. Kallirroi, and T. David. 2012. A cultural
decision-making model for negotiation based on in-
verse reinforcement learning. In The Annual Meet-
ing of the Cognitive Science Society.

M. S. English and P. A. Heeman. 2005. Learn-
ing mixed initiative dialog strategies by using re-
inforcement learning on both conversants. In Em-
pirical Methods in Natural Language Processing
(EMNLP).

H. He, A. Balakrishnan, M. Eric, and P. Liang. 2017.
Learning symmetric collaborative dialogue agents
with dynamic knowledge graph embeddings. In
Association for Computational Linguistics (ACL),
pages 1766–1776.

T. Hiraoka, K. Georgila, E. Nouri, and D. Traum. 2015.
Reinforcement learning in multi-party trading dia-
log. In Special Interest Group on Discourse and Di-
alogue (SIGDIAL).

S. Keizer, M. Guhe, H. Cuayahuitl, I. Efstathiou,
K. Engelbrecht, M. Dobre, A. Lascarides, and
O. Lemon. 2017. Evaluating persuasion strategies
and deep reinforcement learning methods for nego-
tiation dialogue agents. In European Association for
Computational Linguistics (EACL).

M. Lewis, D. Yarats, Y. N. Dauphin, D. Parikh, and
D. Batra. 2017. Deal or no deal? end-to-end learn-
ing for negotiation dialogues. In Empirical Methods
in Natural Language Processing (EMNLP).

J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and
J. Gao. 2016. Deep reinforcement learning for dia-
logue generation. In Empirical Methods in Natural
Language Processing (EMNLP).

J. Li, W. Monroe, T. Shi, A. Ritter, and D. Jurafsky.
2017. Adversarial learning for neural dialogue gen-
eration. arXiv preprint arXiv:1701.06547.

R. T. Lowe, N. Pow, I. Serban, L. Charlin, C. Liu, and
J. Pineau. 2017. Training end-to-end dialogue sys-
tems with the ubuntu dialogue corpus. Dialogue and
Discourse, 8.

J. Pennington, R. Socher, and C. D. Manning. 2014.
GloVe: Global vectors for word representation. In
Empirical Methods in Natural Language Processing
(EMNLP), pages 1532–1543.

C. Potts. 2012. Goal-driven answers in the Cards dia-
logue corpus. In Proceedings of the 30th West Coast
Conference on Formal Linguistics, pages 1–20.

A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji,
M. Mitchell, J. Nie, J. Gao, and B. Dolan. 2015.
A neural network approach to context-sensitive
generation of conversational responses. In North
American Association for Computational Linguis-
tics (NAACL).

H. Takuya, N. Graham, S. Sakriani, T. Tomoki, and
N. Satoshi. 2014. Reinforcement learning of coop-
erative persuasive dialogue policies using framing.
In International Conference on Computational Lin-
guistics (COLING).

D. Traum, S. C. Marsella, J. Gratch, J. Lee, and
A. Hartholt. 2008. Multi-party, multi-issue, multi-
strategy negotiation for multi-modal virtual agents.
In International Workshop on Intelligent Virtual
Agents, pages 117–130.

T. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona,
P. Su, S. Ultes, D. Vandyke, and S. Young. 2017a.
A network-based end-to-end trainable task-oriented
dialogue system. In European Association for Com-
putational Linguistics (EACL), pages 438–449.

T. Wen, Y. Miao, P. Blunsom, and S. Young. 2017b.
Latent intention dialogue models. In International
Conference on Machine Learning (ICML).

J. D. Williams, K. Asadi, and G. Zweig. 2017. Hy-
brid code networks: Practical and efficient end-to-
end dialog control with supervised and reinforce-
ment learning. In Association for Computational
Linguistics (ACL).

J. D. Williams, A. Raux, and M. Henderson. 2016. The
dialog state tracking challenge series: A review. Di-
alogue and Discourse, 7.



2343

R. J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3):229–256.

S. Young, M. Gašić, B. Thomson, and J. D. Williams.
2013. POMDP-based statistical spoken dialog sys-
tems: A review. In Proceedings of the IEEE, 5,
pages 1160–1179.

T. Zhao, R. Zhao, and M. Eskenazi. 2017. Learning
discourse-level diversity for neural dialog models
using conditional variational autoencoders. In As-
sociation for Computational Linguistics (ACL).


