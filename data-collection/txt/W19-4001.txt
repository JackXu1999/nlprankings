



















































Crowdsourced Hedge Term Disambiguation


Proceedings of the 13th Linguistic Annotation Workshop, pages 1–5
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

1

Crowdsourced Hedge Term Disambiguation

Morgan Ulinski and Julia Hirschberg
Department of Computer Science

Columbia University
New York, NY, USA

{mulinski,julia}@cs.columbia.edu

Abstract

We address the issue of acquiring quality an-
notations of hedging words and phrases, lin-
guistic phenomenona in which words, sounds,
or other constructions are used to express am-
biguity or uncertainty. Due to the limited
availability of existing corpora annotated for
hedging, linguists and other language scien-
tists have been constrained as to the extent
they can study this phenomenon. In this pa-
per, we introduce a new method of acquir-
ing hedging annotations via crowdsourcing,
based on reformulating the task of labeling
hedges as a simple word sense disambiguation
task. We also introduce a new hedging corpus
we have constructed by applying this method,
a collection of forum posts annotated using
Amazon Mechanical Turk. We found that the
crowdsourced judgments we obtained had an
inter-annotator agreement of 92.89% (Fleiss’
Kappa=0.751) and, when comparing a sub-
set of these annotations to an expert-annotated
gold standard, an accuracy of 96.65%.

1 Introduction

Hedging refers to the use of words, sounds, or
constructions that add ambiguity or uncertainty to
spoken or written language. Hedging can indicate
a speaker’s lack of commitment to what they are
saying or an attempt to distance themselves from
the proposition they are communicating. Identi-
fying hedging behavior in conversational speech
and text can also reveal information about social
and power relations between conversants. Addi-
tionally, since hedging can be indicative of a lack
of speaker commitment, identifying hedging is of
interest to the information extraction community,
to determine the extent to which statements have
been believed by the writer or speaker.

A major challenge in identifying hedges is that
many hedge words and phrases are ambiguous.

For example, in (1a), appear is used as a hedge
word, but not in (1b).

(1) a. The problem appears to be a bug in the
software.

b. A man suddenly appeared in the door-
way.

Currently there are few corpora annotated for
hedging, and these are available in a limited num-
ber of genres. In particular, there is currently no
corpus of informal language annotated with hedge
behavior. Acquiring expert annotations on text in
other genres can be time consuming and may be
cost prohibitive, which is an impediment to ex-
ploring how hedging can help with applications
based on text in other genres. To address these
issues, we have developed a method of acquir-
ing hedge annotations through crowdsourcing, by
framing the hedge identification task as a simple
word sense disambiguation problem. In this pa-
per, we describe this method and also our use of
Amazon Mechanical Turk to construct a corpus of
forum posts labeled with hedge information.

In Section 2, we discuss related work. In Sec-
tion 3, we describe how we constructed our dic-
tionary of hedge terms and created the hedge and
non-hedge definitions for each. Section 4 de-
scribes the crowdsourcing task in more detail and
discusses the resulting corpus. We conclude in
Section 5.

2 Related Work

Currently, there is limited material available for
studying hedging. The CoNLL-2010 shared task
on learning to detect hedges (Farkas et al., 2010)
used the BioScope corpus (Vincze et al., 2008) of
biomedical abstracts and articles and a Wikipedia
corpus annotated for “weasel words”. Because
of the domain-specific nature of these corpora,



2

they can be difficult to apply to other text genres,
such as social media or blogs. Additionally, the
Wikipedia definition of a weasel word is slightly
different than that of a hedge. Weasel words in-
clude language referring to personal opinions and
subjectivity (e.g. excellent, best) in addition to un-
certainty and lack of speaker commitment. Thus,
it may be difficult to use the Wikipedia corpus to
study hedging as a phenomenon that is distinct
from subjectivity. Both the BioScope corpus and
the Wikipedia corpus were annotated by experts
and/or trained linguists; as with any annotation
task, acquiring new expert-annotated data can be
time- and cost-prohibitive. Our work differs from
these in that we annotate a corpus of documents
containing more informal language —a collection
of forum posts. Additionally, rather than relying
on the availability of trained linguists to annotate
the corpus, our work explores how we can use
crowdsourcing to obtain hedge annotations.

To facilitate annotation by non-experts, we
frame the annotation task as a word sense dis-
ambiguation problem rather than asking directly
about hedging. Note that there is a precedent for
reformulating hedge detection in this way: as a
follow-up to the CoNLL-2010 hedge classifica-
tion task, Velldal (2011) described a new approach
to classification in which hedge detection was
viewed as a simple disambiguation task, restricted
to words that have previously been observed as
hedge cues. Velldal transformed the CoNLL data
for the binary classification task by defining the
dictionary of potential hedge terms as any tokens
that appeared as hedge cues in the training data; all
unlabeled instances of these terms were assumed
to be non-hedge usages. A classifier trained using
this approach was found to outperform the sys-
tems presented at CoNLL-2010, which relied on
standard methods of token-by-token or sentence-
level classification. Our work extends the word
sense disambiguation approach to the problem of
obtaining hedging annotations on new corpora.

Crowdsourcing has been successfully used in
the past for collecting annotations for word sense
disambiguation. Chklovski and Mihalcea (2002)
had users select the WordNet sense that most
closely matched the definition of a word as used
in a given sentence. Likewise, Akkaya et al.
(2010) used Amazon Mechanical Turk (AMT) to
annotate Subjectivity Word Sense Disambigua-
tion (SWSD), a coarse-grained word sense disam-

Relational Hedges
according to, appear, arguably, assume, believe,
consider, could, doubt, estimate, expect, feel,
find, guess, hear, I mean, I would say, imag-
ine, impression, in my mind, in my opinion, in
my understanding, in my view, know, likely,
look like, looks like, may, maybe, might, my
thinking, my understanding, necessarily, per-
haps, possibly, presumably, probably, read, say,
seem, seemingly, should, sound like, sounds
like, speculate, suggest, suppose, sure, tend,
think, understand, unlikely, unsure
Propositional Hedges
a bit, a bunch, a couple, a few, a little, a whole
bunch, about, allegedly, among others, and all
that, and so forth, and so on, and suchlike, ap-
parently, approximately, around, at least, basic,
basically, completely, et cetera, etc, fair, fairly,
for the most part, frequently, general, gener-
ally, in a way, in part, in some ways, kind of,
kinda, largely, like, mainly, more or less, most,
mostly, much, occasionally, often, partial, par-
tially, partly, possible, practically, pretty, pretty
much, probable, rarely, rather, really, relatively,
rough, roughly, seldom, several, something or
other, sort of, to a certain extent, to some ex-
tent, totally, usually, virtually

Table 1: List of (potential) hedge words and phrases

biguation task. In a much easier task, Snow et al.
(2008) had users select from among three different
senses of the word president. Our work follows
these examples by presenting hedging and non-
hedging definitions and asking users to choose be-
tween them.

3 Hedging Dictionary

We compiled a dictionary of 117 potential hedge
words and phrases. We began with the hedge
terms identified during the CoNLL-2010 shared
task (Farkas et al., 2010), along with synonyms
of these terms. This list was further expanded
and edited through consultation with the LDC and
other linguists, to ensure representation of hedge
terms from more informal text.

The full list of hedge words and phrases in our
dictionary is shown in Table 1. This hedging
dictionary is divided into relational and proposi-
tional hedges. As described in Prokofieva and
Hirschberg (2014), relational hedges have to do



3

Hedge Term Hedge Definition Non-Hedge Definition
about • almost; approximately (“There are

about 10 million packages in transit
right now.”)

• on the subject of; concerning (“We
need to talk about Mark.”)
• located in a particular area (“He is

about the house.”)
• on the verge of (“He was about to

leave.”)
practically • virtually; almost; nearly (“Their

provisions were practically gone.” “It
has rained practically every day.”)

• in a practical manner; realistically;
sensibly (“Practically speaking, the
plan is not very promising.” “He
purchased as many items as he could
practically afford.”)

suppose • to believe or assume as true (“It is
generally supposed that his death was
an accident.”)
• to think or hold as an opinion (“I

suppose the package will arrive next
week.”)

• to be expected or designed; to be
required or permitted (“The machine
is supposed to make noise.” “I’m
supposed to call if I’m going to be
late.”)

think • to have an opinion, belief, or idea
about someone or something (“I think
it’s an important issue.” “John doesn’t
think he will win the election.”)
• to have as a plan or intention (“I

thought that I would go.”)

• to use one’s mind actively to form
ideas (“Think carefully before you
begin.” “I didn’t think of the solution
in time.”)
• to direct one’s mind toward something

or someone (“I was thinking about
you.”)

Table 2: Example definitions from our hedging dictionary

with the speaker’s relation to the propositional
content, while propositional hedges are those that
introduce uncertainty into the propositional con-
tent itself. The examples in (2) demonstrate rela-
tional and propositional hedges.

(2) a. I think the ball is blue.
b. The ball is sort of blue.

In (2a), think is a relational hedge. In (2b), sort of
is a propositional hedge.

For each hedge term in our dictionary, we cre-
ated definitions for the hedging and non-hedging
usages of the term, including examples for each
case. We attempted to keep these definitions as
simple as possible while still providing enough
direction for workers completing the AMT task.
These definitions were revised as we tested the
AMT task with real-world users and received feed-
back pointing out ambiguities or other problems
with the definitions. We did find that some words
were too complicated or that the differences in
senses was too nuanced to reduce definitions to
short hedge and non-hedge definitions: in par-

ticular, hear, read, and say were identified as
such. For example, the sentences in (3) differ only
slightly, but hear is being used a hedge in the first
and not in the second:

(3) a. I heard that there was an arrest.
b. I heard about the arrest.

For these words, it might be more effective to de-
velop a separate AMT task that provides a more
comprehensive set of definitions and examples
rather than trying to reduce them to a simple bi-
nary choice. Another option would be to ask AMT
workers more directly about how the speaker is us-
ing a term: e.g. whether the usage reflects uncer-
tainty or lack of commitment to a proposition.

Table 2 shows some examples of hedging and
non-hedging definitions. The complete dictionary
of hedge terms, definitions, and examples is avail-
able from the authors upon request. Note that for
34 entries in our dictionary, the non-hedge defi-
nition is simply “Other”. These are cases where
the word or phrase is generally unambiguous ex-
cept for extremely rare instances (generally, typos



4

Figure 1: Instructions for Amazon Mechanical Turk Task

Figure 2: Example of AMT word sense disambiguation task

or other errors).

4 Corpus Annotation

We began with a collection of discussion forum
posts from the 2014 Deft Committed Belief Cor-
pora (Release No. LDC2014E55, LDC2014E106,
and LDC2014E125). These posts were originally
collected for the DARPA BOLT program and were
selected according to a variety of criteria, includ-
ing that the posts should contain primarily infor-
mal discussion and that the main focus of the
threads should be discussion of dynamic events or
personal anecdotes (Garland et al., 2012).

We located all instances of the hedges from our
dictionary in these corpora and presented each of
these instances as a potential hedge to workers
on AMT. The hedge term was shown as a high-
lighted word or phrase within a sentence; below
this sentence, we displayed definitions and exam-
ples of the hedging and non-hedging uses of the
term. We asked workers which definition they felt
most closely matched the meaning of the word
highlighted in the sentence. To avoid bias based
on the placement of the choices, we varied the or-
der in which the hedging and non-hedging defi-
nitions appeared. Each Human Intelligence Task
(HIT) asked for judgments on 10 sentences, with
one being a gold-standard check judgment. If the
worker failed to answer the check judgment cor-
rectly, we discarded their data and republished the

6 4 5 7 4 4 5 4

11 10

50

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0

20

40

60

# hedge usages / # total usages

Nu
m

be
r o

f v
al

ue
s

Figure 3: Distribution of proportion of hedge usages
out of all occurrences of each term.

HIT. We obtained 5 judgments for every potential
hedge word and picked the majority vote as the
label for that instance. Figure 1 shows the instruc-
tions given to workers. An example of the task for
the word fairly is shown in Figure 2.

The resulting corpus has a total of 20,683 an-
notated potential hedge terms, although the data
set is very unbalanced, with some hedge terms ap-
pearing many more times than others. For exam-
ple, about appears 2,124 times but in some ways,
et cetera, and to a certain extent each appear only
once. The number of hedge usages vs. non-hedge
usages for each term also varied. Figure 3 shows
the distribution of the proportion of times a term
was used a hedge out of all occurrences of that
term. Overall agreement among the AMT workers
was 92.89%, with Fleiss’ Kappa equal to 0.751.



5

0.0 0.5 1.0
0.75

0.80

0.85

0.90

0.95

1.00

# hedge usages / # total usages

Ag
re

em
en

t (
%

)

Figure 4: Plot of agreement vs. proportion of hedge
usages out of total occurrences for each term.

The agreement varied depending on the hedge
term. Figure 4 shows a scatterplot of the agree-
ment percentage vs. how often each term is used
a hedge. As one might expect, the general trend
shows that agreement is higher for terms that are
almost always used as hedges (or as non-hedges)
than for the more ambiguous terms.

To get a sense of the quality of the crowd-
sourced judgments, we annotated a subset of the
corpus ourselves. This subset was constructed by
randomly selecting two instances for each hedge
term. Each instance received two judgments, one
by each of the two authors of this paper. As
one would expect, inter-annotator agreement was
higher, 94.73% overall, with Cohen’s Kappa equal
to 0.857. For most hedge terms, agreement was
100%; however, 11 hedge terms had an agree-
ment of 50%. We adjudicated the questions for
which we disagreed to create a single gold stan-
dard answer. We then compared our gold standard
answers for this subset to the majority vote judg-
ments obtained from AMT workers for the same
questions. The crowdsourced majority vote judg-
ment differed from the gold standard on only 7
questions, for an overall accuracy of 96.65%.

5 Summary

We have described a new method of using crowd-
sourcing to annotate a corpus with hedging infor-
mation, by framing the hedge detection task as
a word sense disambiguation problem. We have
used this method to annotate a corpus of forum
posts, which we hope to make generally available
through the LDC. We have shown that annotations
obtained using this method can in fact be very ac-
curate; when comparing the crowdsourced judg-
ments to an expert-annotated subset of the corpus,
we obtained an accuracy of 96.65%.

Acknowledgments

This paper is based upon work supported by the
DARPA DEFT program. The views expressed
here are those of the author(s) and do not reflect
the official policy or position of the Department of
Defense or the U.S. Government.

References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and

Rada Mihalcea. 2010. Amazon Mechanical Turk
for Subjectivity Word Sense Disambiguation. In
Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon’s
Mechanical Turk, pages 195–203, Los Angeles. As-
sociation for Computational Linguistics.

Timothy Chklovski and Rada Mihalcea. 2002. Build-
ing a Sense Tagged Corpus with Open Mind Word
Expert. In Proceedings of the ACL-02 Workshop on
Word Sense Disambiguation: Recent Successes and
Future Directions, pages 116–122. Association for
Computational Linguistics.

Richárd Farkas, Veronika Vincze, György Móra, János
Csirik, and György Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 1–12, Uppsala,
Sweden. Association for Computational Linguistics.

Jennifer Garland, Stephanie Strassel, Safa Ismael,
Zhiyi Song, and Haejoong Lee. 2012. Linguistic
resources for genre-independent language technolo-
gies: User-generated content in BOLT. In @NLP
Can u Tag #user generated content ?! Via Lrec-
Conf.Org (NLP4UGC 2012).

Anna Prokofieva and Julia Hirschberg. 2014. Hedging
and speaker commitment. In Proceedings of the 5th
International Workshop on Emotion, Social Signals,
Sentiment & Linked Open Data, pages 10–13, Reyk-
javik, Iceland.

Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and Fast – But is it
Good? Evaluating Non-Expert Annotations for
Natural Language Tasks. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 254–263, Honolulu,
Hawaii. Association for Computational Linguistics.

Erik Velldal. 2011. Predicting speculation: A sim-
ple disambiguation approach to hedge detection in
biomedical literature. Journal of Biomedical Se-
mantics, 2(5):S7.

Veronika Vincze, György Szarvas, Richárd Farkas,
György Móra, and János Csirik. 2008. The Bio-
Scope corpus: Biomedical texts annotated for un-
certainty, negation and their scopes. BMC Bioinfor-
matics, 9(11):S9.

https://www.aclweb.org/anthology/W10-0731
https://www.aclweb.org/anthology/W10-0731
https://doi.org/10.3115/1118675.1118692
https://doi.org/10.3115/1118675.1118692
https://doi.org/10.3115/1118675.1118692
https://www.aclweb.org/anthology/W10-3001
https://www.aclweb.org/anthology/W10-3001
https://www.aclweb.org/anthology/W10-3001
https://www.aclweb.org/anthology/D08-1027
https://www.aclweb.org/anthology/D08-1027
https://www.aclweb.org/anthology/D08-1027
https://doi.org/10.1186/2041-1480-2-S5-S7
https://doi.org/10.1186/2041-1480-2-S5-S7
https://doi.org/10.1186/2041-1480-2-S5-S7
https://doi.org/10.1186/1471-2105-9-S11-S9
https://doi.org/10.1186/1471-2105-9-S11-S9
https://doi.org/10.1186/1471-2105-9-S11-S9

