



















































Neural Hidden Markov Model for Machine Translation


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 377–382
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

377

Neural Hidden Markov Model for Machine Translation

Weiyue Wang, Derui Zhu, Tamer Alkhouli, Zixuan Gan, Hermann Ney
Human Language Technology and Pattern Recognition, Computer Science Department

RWTH Aachen University, 52056 Aachen, Germany
<surname>@i6.informatik.rwth-aachen.de

Abstract

This work aims to investigate alterna-
tive neural machine translation (NMT)
approaches and thus proposes a neural
hidden Markov model (HMM) consist-
ing of neural network-based alignment
and lexicon models. The neural models
make use of encoder and decoder compo-
nents, but drop the attention component.
The training is end-to-end and the stand-
alone decoder is able to provide compa-
rable performance with the state-of-the-art
attention-based models on three different
translation tasks.

1 Introduction

Attention-based neural translation models (Bah-
danau et al., 2015; Luong et al., 2015) attend
to specific positions on the source side to gen-
erate translation. Using the attention component
provides significant improvements over the pure
encoder-decoder sequence-to-sequence approach
(Sutskever et al., 2014) that uses no such attention
mechanism. In this work, we aim to compare the
performance of attention-based models to another
baseline, namely, neural hidden Markov models.

The neural HMM has been successfully applied
in the literature on top of conventional phrase-
based systems (Wang et al., 2017). In this work,
our purpose is to explore its application in stan-
dalone decoding, i.e. the model is used to gener-
ate and score candidates without assistance from a
phrase-based system. Because translation is done
standalone using only neural models, we still re-
fer to this as NMT. In addition, while Wang et al.
(2017) applied feedforward networks to model
alignment and translation, the recurrent structures
proposed in this work surpass the feedforward
variants by up to 1.3% in BLEU.

By comparing neural HMM and attention-based
NMT, we shed light on the role of the attention
component. To this end, we use an alignment-
based model that has a recurrent bidirectional en-
coder and a recurrent decoder, but use no atten-
tion component. We replace the attention mecha-
nism by a first-order HMM alignment model. At-
tention levels are deterministic normalized simi-
larity scores part of the architecture design of an
otherwise fully supervised classifier. HMM-style
alignments on the other hand are discrete ran-
dom variables and (unlike attention levels) must be
marginalized. Once alignments are marginalized,
which is tractable for a first-order HMM, parame-
ters can be estimated to attain a local optimum of
log-likelihood of observations as usual.

2 Motivation

In attention-based approaches, the alignment dis-
tribution is used to select the positions in the
source sentence that the decoder attends to dur-
ing translation. Thus the alignment model can be
considered as an implicit part of the translation
model. On the other hand, separating the align-
ment model from the lexicon model has its own
advantages: First of all, this leads to more flexi-
bility in modeling and training: The models can
not only be trained separately, but they can also
have different model types, such as neural mod-
els, count-based models, etc. Second, the separa-
tion avoids propagating errors from one model to
another. In attention-based systems, the transla-
tion score is based on the alignment distribution,
in which errors can be propagated from the align-
ment part to the translation part. Third, probabilis-
tic treatment to alignments in NMT typically im-
plies an extended degree of interpretability (e.g.
one can inspect posteriors) and control over the
model (e.g. one can impose priors over alignments



378

and lexical distributions).

3 Neural Hidden Markov Model

Given a source sentence fJ1 = f1...fj ...fJ and a
target sentence eI1 = e1...ei...eI , where j = bi is
the source position aligned to the target position
i, we model translation using an alignment model
and a lexicon model:

p(eI1|fJ1 ) =
∑
bI1

p(eI1, b
I
1|fJ1 ) (1)

:=
∑
bI1

I∏
i=1

p(ei|bi1, ei−10 , f
J
1 )︸ ︷︷ ︸

lexicon model

· p(bi|bi−11 , e
i−1
0 , f

J
1 )︸ ︷︷ ︸

alignment model

(2)

Instead of predicting the absolute source
position bi, we use an alignment model
p(∆i|bi−11 , e

i−1
0 , f

J
1 ) that predicts the jump

∆i = bi − bi−1.
Wang et al. (2017) applied feedforward neu-

ral networks for modeling the lexicon and align-
ment probabilities. In this work, we would like
to model these distributions using recurrent neu-
ral networks (RNN). RNNs have been shown to
outperform feedforward variants in language and
translation modeling. This is mainly due to that
RNN can handle arbitrary input lengths and thus
include unbounded context information. Unfortu-
nately, the recurrent hidden layer cannot be eas-
ily applied for the neural hidden Markov model,
since it will significantly complicate the compu-
tation of forward-backward messages when run-
ning Baum-Welch. Nevertheless, we can apply
long short-term memory (LSTM) (Hochreiter and
Schmidhuber, 1997) structure for source and tar-
get words embedding. With this technique we can
take the essence of LSTM RNN and do not break
any sequential generative model assumptions.

Our models are close in structure to the model
proposed in Luong et al. (2015), where we have
a component that encodes the source sentence,
and another that encodes the target sentence. As
shown in Figure 1, we use a source side bidi-
rectional LSTM embedding hj =

−→
h j +

←−
h j ,

where
−→
h j = LSTM(W, fj ,

−→
h j−1) and

←−
h j =

LSTM(V, fj ,
←−
h j+1), as well as a target side

LSTM embedding si−1 = LSTM(U, ei−1, si−2).
hj ,
−→
h j ,
←−
h j and si−1, si−2 are vectors, W , V

and U are weight matrices. Before the non-linear
hidden layers, there is a projection layer which

f1

· · ·
fj−1 fj fj+1

· · ·
fJ e1

· · ·
ei−2 ei−1

−→s i−1· · ·

· · · · · ·

· · · · · ·

· · · · · ·
−→
h j

←−
h j

⊕⊕⊕

p(ei|hj, si−1, ei−1)

Figure 1: The architecture of our neural networks
with LSTM RNN on source and target side.

concatenates hj , si−1 and ei−1. Then the neural
network-based lexicon model is given by

p(ei|bi1, ei−10 , f
J
1 ) := p(ei|hj , si−1, ei−1) (3)

and the neural network-based alignment model

p(bi|bi−11 , e
i−1
0 , f

J
1 ) := p(∆i|hj′ , si−1, ei−1) (4)

where j′ = bi−1.
The training criterion is the logarithm of sen-

tence posterior probabilities over training sentence
pairs (Fr, Er), r = 1, ..., R:

arg max
θ

{∑
r

log pθ(Er|Fr)

}
(5)

The derivative for a single sentence pair (F,E) =
(fJ1 , e

I
1) is:

∂

∂θ
log pθ(E|F )

=
∑
j′,j

∑
i

pi(j
′, j|fJ1 , eI1; θ)

· ∂
∂θ

log p(j, ei|j′, ei−10 , f
J
1 ; θ)

(6)

with HMM posterior weights pi(j′, j|fJ1 , eI1; θ),
which can be computed using the forward-
backward algorithm.

The entire training procedure can be summa-
rized as backpropagation in an EM framework:

1. compute:
• the posterior HMM weights
• the local gradients (backpropagation)

2. update neural network weights



379

4 Decoding

In the decoding stage we still calculate the sum
over alignments and apply a target-synchronous
beam search for the target string.

The auxiliary quantity for each unknown partial
string ei0 is specified as Q(i, j; e

i
0). During search,

the partial hypothesis is extended from ei−10 to e
i
0:

Q(i, j; ei0)

=
∑
j′

[
p(j, ei|j′, ei−10 , f

J
1 ) ·Q(i− 1, j′; ei−10 )

]
(7)

The decoder is shown in Algorithm 1. In the in-
nermost loop (line 11-13), alignments are hypoth-
esized and used to calculate the auxiliary quantity
Q(i, j; ei0). Then for each source position j, the
lexical distribution over the full target vocabulary
is computed (line 14). The distributions are ac-
cumulated (Q(i; ei0) =

∑
j Q(i, j; e

i
0), line 16),

then sorted (line 18) and the best candidate trans-
lations (arg maxei Q(i; e

i
0)) lying within the beam

are used to expand the partial hypotheses (line
19-23). cache is a two-dimensional list of size
J × |Vsrc| (source vocabulary size), which is used
to cache the current quantities.

Whenever a partial hypothesis in the beam
ends with the sentence end symbol (<EOF>), the
counter will be increased by 1 (line 26-28). The
translation is terminated if the counter reaches the
beam size or hypothesis sentence length reaches
three times the source sentence length (line 6). If
a hypothesis stops but its score is worse than other
hypotheses, it is eliminated from the beam, but it
still contests non-terminated hypotheses. During
comparison the scores are normalized by hypothe-
sis sentence length. Note that we have no explicit
coverage constraints. This means that a source po-
sition can be revisited many times, whereby creat-
ing one-to-many alignment cases. This also allows
unaligned source words.

In the neural HMM decoder, word alignments
are estimated and scored according to the dis-
tribution calculated by the neural network align-
ment model, leading alignment decisions to be-
come part of the beam search. The search space
consists of both alignment and translation deci-
sions. In contrast, the search space in attention-
based decoding consists only of translation deci-
sions.

The decoding complexity is O(J2 · I) (J =
source sentence length, I = target sentence length)

Algorithm 1 Neural HMM Decoder

1: function TRANSLATE(fJ1 , beam size)
2: count = 0
3: i = 1
4: hyps = {e0}
5: new hyps = {}
6: while count < beam size and i < 3 · J do
7: for hyp in hyps do
8: sum dist = [0] ∗ |V |src
9: for j from 1 to J do

10: sum = 0
11: for j′ from 1 to J do
12: sum = sum + SCORES(hyp, j′)

·palign(fj′ , j − j′)
13: end for
14: cache[j] = sum · lex dist(fj)
15: #Element wise addition
16: sum dist = sum dist⊕ cache[j]
17: end for
18: dist = SORT(sum dist, beam size)
19: for word in dist[:beam size] do
20: new hyp = EXTEND(hyp, word)
21: SETSCORES(new hyp, cache)
22: new hyps.INSERT(new hyp)
23: end for
24: end for
25: PRUNE(new hyps, beam size)
26: for <EOF> in new hyps do
27: count = count + 1
28: end for
29: hyps = new hyps
30: i = i+ 1
31: end while
32: return GETBEST(hyps)
33: end function

compared to O(J · I) for attention-based models.
These are theoretical complexities of decoding on
a CPU only considering source and target sentence
lengths. In practice, the size of the neural net-
work must also be taken into account, and there
are some optimized matrix multiplications for de-
coding on a GPU. In general, the decoding speed
of our model is about 3 times slower than that of a
standard attention model (1.07 sentences per sec-
ond vs. 3.00 sentences per second) on a single
GPU. This is still an initial decoder and we did
not spend much time on accelerating its decoding
yet. The optimization of our decoder would be a
promising future work.



380

5 Experiments

The experiments are conducted on the WMT 2017
German↔English and Chinese→English transla-
tion tasks, which consist of 5M and 23M paral-
lel sentence pairs respectively. Translation quality
is measured with the case sensitive BLEU (Pap-
ineni et al., 2002) and TER (Snover et al., 2006)
metric on newstests 2017, which contain 3004
(German↔English) and 2001 (Chinese→English)
sentence pairs.

For German and English preprocessing, we use
the Moses tokenizer with hyphen splitting, and
perform truecasing with Moses scripts (Koehn
et al., 2007). For German↔English subword seg-
mentation (Sennrich et al., 2016), we use 20K
joint BPE operations. For the Chinese data, we
segment it using the Jieba1 segmenter. We then
learn a BPE model on the segmented Chinese,
also using 20K merge operations. During train-
ing, sentences with a length greater than 50 sub-
words are filtered out.

5.1 Attention-Based System
The attention-based systems are trained with Sock-
eye (Hieber et al., 2017), which implement an
attentional encoder-decoder with small modifica-
tions to the model in Bahdanau et al. (2015). The
encoder and decoder word embeddings are of size
620. The encoder consists of a bidirectional layer
with 1000 LSTMs with peephole connections to
encode the source side. We use Adam (Kingma
and Ba, 2015) as optimizer with a learning rate
of 0.001, and a batch size of 50. The network
is trained with 30% dropout for up to 500K it-
erations and evaluated every 10K iterations on the
development set with BLEU. Decoding is done us-
ing beam search with a beam size of 12. In the end
the four best models are averaged as described in

1https://github.com/fxsjy/jieba

the beginning of Junczys-Dowmunt et al. (2016).

5.2 Neural Hidden Markov Model
The entire neural hidden Markov model is imple-
mented in TensorFlow (Abadi et al., 2016). The
feedforward models have three hidden layers of
sizes 1000, 1000 and 500 respectively, with a 5-
word source window and a 3-gram target history.
200 nodes are used for word embeddings.

The output layer of the neural lexicon model
consists of around 25K nodes for all subword
units, while the neural alignment model has a
small output layer with 201 nodes, which reflects
that the aligned position can jump within the scope
from −100 to 100.

Apart from the basic projection layer, we also
applied LSTM layers for the source and target
words embedding. The embedding layers have
350 nodes and the size of the projection layer
is 800 (400 + 200 + 200, Figure 1). We use
Adam as optimizer with a learning rate of 0.001.
Neural lexicon and alignment models are trained
with 30% dropout and the norm of the gradient is
clipped with a threshold 1 (Pascanu et al., 2014).
In decoding we use a beam size of 12 and the
element-wise average of all weights of the four
best models also results in better performance.

5.3 Results
We compare the neural HMM approach (Subsec-
tion 5.2) with the state-of-the-art attention-based
approach (Subsection 5.1) on different translation
tasks. The results are presented in Table 1. Com-
pare to the model presented in Wang et al. (2017),
switching to LSTM models has a clear advantage,
which improves the FFNN-based system by up
to 1.3% BLEU and 1.8% TER. It seems that the
HMM model benefits from richer features, such
as LSTM states, which are very similar to what an
attention mechanism would require. We actually

WMT 2017
# free German→English English→German Chinese→English

parameters BLEU[%] TER[%] BLEU[%] TER[%] BLEU[%] TER[%]

FFNN-based neural HMM 1 33M 28.3 51.4 23.4 58.8 19.3 64.8
LSTM-based neural HMM 2 52M 29.6 50.5 24.6 57.0 20.2 63.7
Attention-based neural network 3 77M 29.5 50.8 24.7 57.4 20.2 63.8

Table 1: Experimental results on WMT 2017 German↔English and Chinese→English test sets.
All models are trained without synthetic data. Single model is used for decoding.
1 (Wang et al., 2017) but applied in decoding instead of rescoring
2 This work
3 (Bahdanau et al., 2015) with small modifications (Section 5.1)



381

er wollte
nie an irgendeiner

Art
von

Auseinandersetzung

teilnehmen

.
he

never

wanted

to

be

in

any

kind

of

altercation

.

Attention-based NMT

er wollte
nie an irgendeiner

Art
von

Auseinandersetzung

teilnehmen

.
he

never

wanted

to

be

in

any

kind

of

altercation

.

GIZA++ alignment

er wollte
nie an irgendeiner

Art
von

Auseinandersetzung

teilnehmen

.
he

never

wanted

to

be

in

any

kind

of

altercation

.

Neural HMM

Figure 2: Attention weight and alignment matrices visualized in heat map form. Generated by the
attention NMT baseline, GIZA++ and the neural hidden Markov model.

expected it to do with less, the reason being that
alignment distributions get refined a posteriori and
so they do not have to be as strong a priori. We can
also observe that the performance of our approach
is comparable with the state-of-the-art attention-
based system with 25M more parameters on all
three tasks.

5.4 Alignment Analysis
We show an example from the German→English
newstest 2017 in Figure 2, along with the atten-
tion and alignment matrices. We can observe that
the neural network-based HMM could generate a
more clear alignment path compared to the atten-
tion weights. In this example, it can exactly esti-
mate the alignment positions for words wanted
and of.

6 Discussion

We described a novel formulation for a neural
network-based machine translation system, which
applied neural networks to the conventional hid-
den Markov model. The training is end-to-end, the
model is monolithic and can be used as a stand-
alone decoder. This results in a more modern
and efficient way to use HMM in machine trans-
lation and enables neural networks to benefit from
HMMs.

Experiments show that replacing attention with
alignment does not improve the translation perfor-
mance of NMT significantly. One possible reason
is that alignment may fail to capture relevant con-
texts as attention does. While alignment aims to
identify translation equivalents between two lan-

guages, attention is designed to find relevant con-
text for predicting the next target word. Source
words with high attention weights are not neces-
sarily translation equivalents of the target word.
Although using alignment does not lead to signif-
icant improvements in terms of BLEU over atten-
tion, we think alignment-based NMT models are
still useful for automatic post editing and develop-
ing coverage-based models. These might be inter-
esting future directions to explore.

Acknowledgments

This project has received
funding from the Eu-
ropean Union’s Horizon
2020 research and inno-
vation programme under

grant agreement no 645452 (QT21), and from the
European Research Council (ERC) under the Eu-
ropean Unions Horizon 2020 research and inno-
vation programme, grant agreement no 694537
(SEQCLAS). The work reflects only the authors’
views and neither the European Research Coun-
cil Executive Agency nor the European Commis-
sion is responsible for any use that may be made
of the information it contains. The GPU cluster
used for the experiments was partially funded by
Deutsche Forschungsgemeinschaft (DFG) Grant
INST 222/1168-1. Tamer Alkhouli was partly
funded by the 2016 Google PhD Fellowship for
North America, Europe and the Middle East.



382

References
Martı́n Abadi, Ashish Agarwal, and Paul Barham et

al. 2016. TensorFlow: Large-Scale Machine Learn-
ing on Heterogeneous Distributed Systems. CoRR
abs/1603.04467.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural Machine Translation by Jointly
Learning to Align and Translate. In Proceedings of
the 3rd International Conference on Learning Rep-
resentations. San Diego, CA, USA.

Felix Hieber, Tobias Domhan, Michael Denkowski,
David Vilar, Artem Sokolov, Ann Clifton, and
Matt Post. 2017. Sockeye: A Toolkit for
Neural Machine Translation. ArXiv e-prints
https://arxiv.org/abs/1712.05690.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long Short-Term Memory. Neural Computation
9(8):1735–1780.

Marcin Junczys-Dowmunt, Tomasz Dwojak, and Rico
Sennrich. 2016. The AMU-UEDIN Submission
to the WMT16 News Translation Task: Attention-
based NMT Models as Feature Functions in Phrase-
based SMT. In Proceedings of the First Conference
on Machine Translation, Volume 2: Shared Task Pa-
pers. Berlin, Germany, pages 319–325.

Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam:
A method for stochastic optimization. In Proceed-
ings of the Third International Conference on Learn-
ing Representations. San Diego, CA, USA.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Compan-
ion Volume Proceedings of the Demo and Poster Ses-
sions. Prague, Czech Republic, pages 177–180.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Conference on
Empirical Methods in Natural Language Process-
ing. Lisbon, Portugal, pages 1412–1421.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics. Philadelphia, PA, USA,
pages 311–318.

Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho,
and Yoshua Bengio. 2014. How to Construct Deep
Recurrent Neural Networks. In Proceedings of the
Second International Conference on Learning Rep-
resentations. Banff, Canada.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural Machine Translation of Rare Words
with Subword Units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics. Berlin, Germany, pages 1715–1725.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of the Conference of the As-
sociation for Machine Translation in the Americas.
Cambridge, MA, USA, pages 223–231.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to Sequence Learning with Neural Net-
works. In Proceedings of the Advances in Neu-
ral Information Processing Systems 27. Montréal,
Canada, pages 3104–3112.

Weiyue Wang, Tamer Alkhouli, Derui Zhu, and Her-
mann Ney. 2017. Hybrid Neural Network Align-
ment and Lexicon Model in Direct HMM for Sta-
tistical Machine Translation. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics. Vancouver, Canada, pages 125–
131.

https://arxiv.org/abs/1712.05690
https://arxiv.org/abs/1712.05690
https://arxiv.org/abs/1712.05690

