



















































Identifying Predictive Causal Factors from News Streams


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2338–2348,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2338

Identifying Predictive Causal Factors from News Streams

Ananth Balashankar1, Sunandan Chakraborty2, Samuel Fraiberger1,3, and
Lakshminarayanan Subramanian1

1Courant Institute of Mathematical Sciences, New York University
2School of Informatics and Computing, Indiana University-Indianapolis

3World Bank, Washington DC
ananth@nyu.edu, sunchak@iu.edu, sfraiberger@worldbank.org, lakshmi@nyu.edu

Abstract

We propose a new framework to uncover the
relationship between news events and real
world phenomena. We present the Predictive
Causal Graph (PCG) which allows to detect
latent relationships between events mentioned
in news streams. This graph is constructed
by measuring how the occurrence of a word
in the news influences the occurrence of an-
other (set of) word(s) in the future. We show
that PCG can be used to extract latent fea-
tures from news streams, outperforming other
graph-based methods in prediction error of 10
stock price time series for 12 months. We
then extended PCG to be applicable for longer
time windows by allowing time-varying fac-
tors, leading to stock price prediction error
rates between 1.5% and 5% for about 4 years.
We then manually validated PCG, finding that
67% of the causation semantic frame argu-
ments present in the news corpus were directly
connected in the PCG, the remaining being
connected through a semantically relevant in-
termediate node.

1 Introduction

Contextual embedding models (Devlin et al.,
2018) have managed to produce effective repre-
sentations of words, achieving state-of-the-art per-
formance on a range of NLP tasks. In this pa-
per, we consider a specific task of predicting vari-
ations in stock prices based on word relationships
extracted from news streams. Existing word em-
bedding techniques are not suited to learn relation-
ships between words appearing in different docu-
ments and contexts (Le and Mikolov, 2014). Ex-
isting work on stock price prediction using news
have typically relied on extracting features from
financial news (Falinouss, 2007; Hagenau et al.,
2013), or sentiments expressed on Twitter (Mao
et al., 2011; Rao and Srivastava, 2012; Bernardo
et al., 2018), or by focusing on features present in

a single document (Kalyani et al., 2016; Shynke-
vich et al., 2015). However, relationships between
events affecting stock prices can be quite com-
plex, and their mentions can be spread across mul-
tiple documents. For instance, market volatility is
known to be triggered by recessions; this relation-
ship may be reflected with a spike in the frequency
of the word ”recession” followed by a spike in
the frequency of the word ”volatility” a few weeks
later. Existing methods are not well-equipped to
deal with these cases.

This paper aims to uncover latent relationships
between words describing events in news streams,
allowing us to unveil hidden links between events
spread across time, and integrate them into a
news-based predictive model for stock prices. We
propose the Predictive Causal Graphs (PCG), a
framework allowing us to detect latent relation-
ships between words when such relationships are
not directly observed. PCG differs from existing
relationship extraction (Das et al., 2010) and rep-
resentational frameworks (Mikolov et al., 2013)
across two dimensions. First, PCG identifies un-
supervised causal relationships based on consis-
tent time series prediction instead of association,
allowing us to uncover paths of influence between
news items. Second, PCG finds inter-topic in-
fluence relationships outside the “context” or the
confines of a single document. Construction of
PCG naturally leads to news-dependent predictive
models for numerous variables, like stock prices.

We construct PCG by identifying Granger
causal pairs of words (Granger et al., 2000) and
combining them to form a network of words using
the Lasso Granger method (Arnold et al., 2007). A
directed edge in the network therefore represents a
potential influence between words. While predic-
tive causality is not true causality (Maziarz, 2015),
identification of predictive causal factors which
prove to be relevant predictors over long periods of



2339

time provides guidance for future causal inference
studies. We achieve this consistency by proposing
a framework for Longitudinal Predictive Causal
Factor identification based on methods of honest
estimation (Athey and Imbens, 2016). Here, we
first estimate a universe of predictive causal fac-
tors on a relatively long time series and then iden-
tify time-varying predictive causal factors based
on constrained estimation on multiple smaller time
series. We also augment our model with an or-
thogonal spike correction ARIMA (Brockwell and
Davis, 2002) model, allowing us to overcome the
drawback of slow recovery in smaller time series.

We constructed PCG from news streams of
around 700, 000 articles from Google News API
and New York Times spread across over 6 years
and evaluated it to extract features for stock price
predictions. We obtained two orders lower predic-
tion error compared to a similar semantic causal
graph-based method (Kang et al., 2017). The lon-
gitudinal PCG provided insights into the variation
in importance of the predictive causal factors over
time, while consistently maintaining a low predic-
tion error rate between 1.5-5% in predicting 10
stock prices. Using full text of more than 1.5 mil-
lion articles of Times of India news archives for
over 10 years, we performed a fine-grained qual-
itative analysis of PCG and validated that 67%
of the semantic causation arguments found in the
news text is connected by a direct edge in PCG
while the rest were linked by a path of length 2.
In summary, PCG provides a powerful framework
for identifying predictive causal factors from news
streams to accurately predict and interpret price
fluctuations.

2 Related Work

Online news articles are a popular source
for mining real-world events, including extrac-
tion of causal relationships. Radinsky and
Horvitz (Radinsky and Horvitz, 2013) proposed
a framework to find causal relationships between
events to predict future events from News but
caters to a small number of events. Causal re-
lationships extracted from news using Granger
causality have also been used for predicting vari-
ables, such as stock prices (Kang et al., 2017;
Verma et al., 2017; Darrat et al., 2007). A similar
causal relationship generation model has been pro-
posed by Hashimoto et al. (2015) to extract causal
relationships from natural language text. A simi-

lar approach can be observed in (Kozareva, 2012;
Do et al., 2011), whereas CATENA system (Mirza
and Tonelli, 2016) used a hybrid approach consist-
ing of a rule-based component and a supervised
classifier. PCG differs from these approaches as
it explores latent inter-topic causal relationships in
an unsupervised manner from the entire vocabu-
lary of words and collocated N-grams.

Apart from using causality, there are many other
methods explored to extract information from
news and are used in time series based forecasting.
Amodeo et al. (Amodeo et al., 2011) proposed a
hybrid model consisting of time-series analysis, to
predict future events using the New York Times
corpus. FBLG (Cheng et al., 2014) focused on
discovering temporal dependency from time series
data and applied it to a Twitter dataset mention-
ing the Haiti earthquake. Similar work by Luo et
al. (Luo et al., 2014) showed correlations between
real-world events and time-series data for inci-
dent diagnosis in online services. Other similar
works like, Trend Analysis Model (TAM) (Kawa-
mae, 2011) and Temporal-LDA (TM-LDA) (Wang
et al., 2012) model the temporal aspect of topics in
social media streams like Twitter. Structured data
extraction from news have also been used for stock
price prediction using techniques of information
retrieval in (Ding et al., 2014; Xie et al., 2013;
Ding et al., 2015; Chang et al., 2016; Ding et al.,
2016). Vaca et al. (Vaca et al., 2014) used a collec-
tive matrix factorization method to track emerg-
ing, fading and evolving topics in news streams.
PCG is inspired by such time series models and
leverages the Granger causality detection frame-
work for the trend prediction task.

Deriving true causality from observational stud-
ies has been studied extensively. One of the
most widely used algorithm is to control for vari-
ables which satisfy the backdoor criterion (Pearl,
2009). This however, requires a knowledge of the
causal graph and the unconfoundedness assump-
tion that there are no other unobserved confound-
ing variables. While the unconfoundedness as-
sumption is to some extent valid when we ana-
lyze all news streams (under the assumption that
all significant events are reported), it is still hard
to get away from the causal graph requirement.
Propensity score based matching aims to control
for most confounding variables by using an ex-
ternal method for estimating and controlling for
the likelihood of outcomes (Olteanu et al., 2017).



2340

More recently, (Wang and Blei, 2018) showed that
with multiple causal factors, it is possible to lever-
age the correlation of those multiple causal fac-
tors and deconfound using a latent variable model.
This setting is similar to the one we consider, and
is guaranteed to be truly causal if there is no con-
founder which links a single cause and the out-
come. This assumption is less strict than the un-
confoundedness assumption and makes the case
for using predictive causality in such scenarios.
Another approach taken by (Athey and Imbens,
2016) estimates heterogeneous treatment effects
by honest estimation where the model selection
and factor weight estimation is done on two sub-
populations of data by extending regression trees.

Our work is motivated by these works and ap-
plies methodologies for time series data extracted
from news streams. PCG can offer the following
benefits for using news for predictive analytics –
(1) Detection of influence path, (2) Unsupervised
feature extraction, (3) Hypothesis testing for ex-
periment design.

3 Predictive Causal Graph

Predictive Causal Graph (PCG) addresses the dis-
covery of influence between words that appear in
news text. The identification of influence link be-
tween words is based on temporal co-variance,
that can help answer questions of the form: “Does
the appearance of word x influence the appear-
ance of word y after δ days?”. The influence of
one word on another is determined based on pair-
wise causal relationships and is computed using
the Granger causality test. Following the identifi-
cation of Granger causal pairs of words, such pairs
are combined together to form a network of words,
where the directed edges depict potential influence
between words. In the final network, an edge or a
path between a word pair represents a flow of in-
fluence from the source word to the final word and
this influence depicts an increase in the appearance
of the final words when the source word was ob-
served in news data.

Construction of PCG from the raw unstructured
news data, finding pairwise causal links and even-
tually building the influence network involves nu-
merous challenges. In the rest of the section, we
discuss the design methodologies used to over-
come these challenges and describe some proper-
ties of the PCG.

3.1 Selecting Informative Words:
Only a small percentage of the words appearing
in news can be used for meaningful information
extraction and analysis (Manning et al., 1999; Ho-
vold, 2005). Specifically, we eliminated too fre-
quent (at least once in more than 50% of the days)
or too rare (appearing in less than 100 articles)
(Manning et al., 2008). Many common English
nouns, adjectives and verbs, whose contribution
to semantics is minimal (Forman, 2003) were also
removed from the vocabulary. However, named-
entities were retained for their newsworthiness and
a set of “trigger” words were retained that de-
pict events (e.g. flood, election) using an existing
“event trigger” detection algorithm (Ahn, 2006).
The vocabulary set was enhanced by adding bi-
grams that are significantly collocated in the cor-
pus, such as, ‘fuel price’ and ‘prime minister’ etc.

3.2 Time-series Representation of Words:
Consider a corpus D of news articles indexed by
time t, such that Dt is the collection of news arti-
cles published at time t. Each article d ∈ D is a
collection of words Wd, where ith word wd,i ∈
Wd is drawn from a vocabulary V of size N .
The set of articles published at time t can be ex-
pressed in terms of the words appearing in the
articles as {αt1, αt2, ..., αtN}, where αti is the sum
of frequency of the word wi ∈ V across all ar-
ticles published at time t. αti corresponding to

wi ∈ V is defined as, αti =
µti∑T
t=1 µ

t
i

where µti =∑|Dt|
d=1 TF (wd,i). α

t
i is normalized by using the

frequency distribution of wi in the entire time pe-
riod. T (wi) represents the time series of the word
wi, where i varies from 1 to N , the vocabulary
size.

3.3 Measuring Influence between Words
Given two time-series X and Y , the Granger
causality test checks whether the X is more effec-
tive in predicting Y , than using just Y and if this
holds then the test concludes X “Granger-causes”
Y (Granger et al., 2000). However, if both X and
Y are driven by a common third process with dif-
ferent lags, one might still fail to reject the alter-
native hypothesis of Granger causality. Hence, in
PCG, we explore the possibility of causal links be-
tween all word pairs and detect triangulated rela-
tions to eliminate the risk of ignoring confounding
variables, otherwise not considered in the Granger
causality test.



2341

slum rehabilitation

coordinate committee provided relief

2

19

21

Figure 1: PCG highlighting the underlying cause

However, constructing PCG using an exhaus-
tive set of word pairs does not scale, as even af-
ter using a reduced set of words and including the
collocated phrases, the vocabulary size is around
39, 000. One solution to this problem is consid-
ering the Lasso Granger method (Arnold et al.,
2007) that applies regression to the neighborhood
selection problem for any word, given the fact that
the best regressor for that variable with the least
squared error will have non-zero coefficients only
for the lagged variables in the neighborhood. The
Lasso algorithm for linear regression is an incre-
mental algorithm that embodies a method of vari-
able selection (Tibshirani, 1994).

If we define V to be the input vocabulary from
the news dataset, N is the vocabulary size, x is the
list of all lagged variables (each word is multivari-
ate with a maximum lag of 30 days per word) of
the vocabulary,w is the weight vector denoting the
influence of each variable, y is the predicted time
series variable and λ is a sparsity constraint hyper-
parameter to be fine-tuned, then minimizing the
regression loss below leads to weights that charac-
terize the influential links between words in x that
predicts y,

w = argmin
1

N
Σ(x,y)∈V |w.x− y|2 + λ||w|| (1)

To set λ, we use the method based on consistent
estimation used in (Meinshausen and Bühlmann,
2006). We select the variables that have non-zero
co-efficients and choose the best lag for a given
variable based on the maximum absolute value of
a word’s co-efficient. We then, draw an edge from
all these words to the predicted word with the an-
notations of the optimal time lag (in days) and in-
crementally construct the graph as illustrated in
Figure 1.

3.4 Topic Influence Compression

To arrive at a sparse graphical representation of
PCG, we compress the graph based on topics (50
topics in our case). Topics are learned from the
original news corpus using unsupervised Latent

Dirichlet Allocation (LDA)(Blei et al., 2003). In-
fluence is generalized to topic level by calculating
the weight of inter-topic influence relationships as
a total number of edges between vertices of two
topics. If we define θu and θv to be two topics
in our topic model and |θu| represents the size of
topic θu, i.e. the number of words in the topic
whose topic-probability is greater than a thresh-
old (0.001), then the strength of influence between
topics θu and θv is defined as,

Φ(θu, θv) =
# Edges between words in θu and θv

(|θu| × |θv|)
(2)

Φ(θu, θv) is termed as strong if its value is in
the 99th percentile of Φ for all topics. Any
edge in the original PCG is removed if there are
no strong topic edges between the corresponding
word nodes. This filtered topic graph has only
edges between topics which have high influence
strength. This combination of inter-document
temporal and intra-document distributional simi-
larity is critical to obtaining temporally and se-
mantically consistent predictive causal factors.

4 Prediction Models using PCG

In this section, we present three approaches for
building prediction models using PCG namely (1)
direct estimation using PCG (2) longitudinal pre-
diction which incorporates short term temporal
variations and (3) spike augumented prediction
which estimates spikes over a longer time window.

4.1 Direct Prediction from PCG

One straightforward way of using PCG for predic-
tion modeling is to use the Lasso regression equa-
tion used for identifying the predictive causal fac-
tors directly. We first adopt this approach by re-
stricting the construction of PCG to the nodes of
concern, which significantly speeds up the com-
putation. This inherently ignores any predictive
causal factor which only has an indirect link to
the outcome node, as theorized by the Granger
Causality framework. In this case, we split the
data into a contiguous training data, and evaluate
on the remaining testing data. If y represents the
target stock time series variable and x represents a
multivariate vector of all lagged feature variables,
w represents the coefficient weight vector indexed
by the feature variable z ∈ x and time lag m, p, q
in days, a represent a bias constant and �t repre-



2342

sents an i.i.d noise variable, then we predict future
values of y as follows.

yt = a+

m∑
i=1

wy,iyt−i +
∑
z∈x

q∑
j=p

wz,jzt−j + �t (3)

4.2 Longitudinal Prediction via Honest
Estimation

In scenarios where heterogenous causal effects are
to be estimated, it is important to adjust by parti-
tioning the time series into subpopulations which
vary in the magnitude of the causal effects (Athey
and Imbens, 2016). In a news stream, this amounts
to constructing the word influence networks given
a context specified by a time window. This naive
extension however can be quite computationally
expensive and can limit the speed of inference
considerably. However, if the set of potential
causal factors are identified over a larger time se-
ries, learning their time varying weights over a
shorter training period can significantly decrease
the computation required.

Hence, we do a two staged honest estimation
approach similar to (Athey and Imbens, 2016). In
the first stage, multiple sets (instead of trees as
in (Athey and Imbens, 2016)) of predictive causal
factors F (Trm), that provide overall reduction
in root mean squared error (RMSE), over train-
ing data Trm, are gathered for model selection
through repeated random initialization of regres-
sion weights. For any f ∈ F (Trm), we define f
to be a set of predictive factors which when trained
over data Trm to predict future time series values
of the target y, achievesRMSE(Trm, f) < δ, for
a hyperparameter δ > 0.

F (Trm) =
⋃

f
{RMSE(Trm, f) < δ} (4)

From these sets of predictive causal fac-
tors F (Trm), we choose the set of features
fLPC(Trm, T re), which gives the least expected
root mean squared error on time windows w of
length W uniformly sampled from the unseen
training data used for estimation Tre through
cross validation. This model selection procedure
depends on the size of the smaller time windows
W used to fine-tune the factors. The size of this
time window is a hyperparameter to trade off long-
term stability and short-term responsiveness of the
predictive causal factors. We chose the time win-
dow based of 30 days for our stock price predic-
tion due to prior knowledge that many financial

indicators have a monthly cycle and hence respon-
siveness within that cycle is desired.

E(Tre, f) =
√

E
w∈Tre,|w|=W

MSE(w, f) (5)

fLPC(Trm, T re) = minf∈F (Trm)

(
E(Tre, f)

)
(6)

We then evaluate the model on an unseen time
series Te, where the learnt predictive causal fac-
tors and their weights are used for inference.

4.3 Spike Prediction
One drawback of using a specific time window for
estimating the weights of the predictive causal fac-
tors is the lack of representative data in the win-
dow used for training. This could mean that pre-
dicting abrupt drops or spikes in the data would
be hard. To overcome this limitation, we train a
composite model to predict the residual error from
honest estimation by training on differences in
consecutive values of the time series. Let (∆y =
yt − yt−1,∆f = ft − ft−1) denote time series of
the differences of the consecutive values of the la-
bels and the feature variables and let [∆y,∆f ] de-
note the concatenated input variables of the model.
We use a multivariate ARIMA model M of or-
der (p, d, q) (Brockwell and Davis, 2002) where
p controls the number of time lags included in
the model, q denotes the number of times differ-
ences are taken between consecutive values and r
denotes the time window of the moving average
taken to incorporate sudden spikes in values. Let
the actual values of the time series of label y be
y∗, the predictions of the honest estimation model
be ŷ, a training sample of significantly longer time
window Trs with |Trs| >> W , then the com-
posite model is trained to predict the residuals,
res = y∗ − ŷ = E(Trs, f).

M = ARIMA(p, d, q) (7)
M.fit(E(Trs, f), [∆y,∆f ]) (8)

ˆres = M.predict(Tre) (9)

Augmenting this predicted residual ( ˆres) back
to ŷTre gives us the spike-corrected estimate ŷs.

ŷs = ŷTre + ˆres (10)

5 Results

In this section, we present the results from direc-
tion prediction models from PCG, followed by im-
provement in stock price prediction due to longitu-
dinal and spike prediction from news streams and



2343

compare it to a manually tuned semantic causal
graph method. We analyze the time varying fac-
tors to explain the gains achieved via honest esti-
mation.

5.1 Data and Metrics

The news dataset1 we used for stock price predic-
tion contains news crawled from 2010 to 2013 us-
ing Google News APIs and New York Times data
from 1989 to 2007. We construct PCG from the
time series representation of its 12,804 unigrams
and 25,909 bigrams over the entire news corpora
of more than 23 years, as well as the 10 stock
prices2 from 2010 to 2012 for training and 2013
as test data for prediction. The prediction is done
with varying step sizes (1,3,5), which indicates the
time lag between the news data and the day of the
predicted stock price in days. The results shown in
Table 1 is the root mean squared error (RMSE) in
predicted stock value calculated on a 30 day win-
dow averaged by moving it by 10 days over the pe-
riod and directly comparable to our baseline (Kang
et al., 2017). To evaluate the time-varying fac-
tors over a larger time window, we present average
monthly cross validation RMSE % sampled over a
4 year window of 2010-13 in Table 3. Please note
that the results in Table 3 are not comparable with
(Kang et al., 2017) as we report a cross validation
error over a longer time window.

5.2 Prediction Performance of PCG

To evaluate the causal links generated by PCG, we
use it to extract features for predicting stock prices
using the exact data and prediction setting used in
Kang et al. (2017) as our baseline.

Baseline: Kang et al. (2017) extract relevant
variables based on a semantic parser - SEMAFOR
(Das et al., 2010) by filtering causation related
frames from news corpora, topics and sentiments
from tweets. To overcome the problem of low re-
call, they adopt a topic-based knowledge base ex-
pansion. This expanded dataset is then used to
train a neural reasoning model which generates se-
quence of cause-effect statements using an atten-
tion model where the words are represented using
word2vec vectors. (Kang et al., 2017)’s CGRAPH
based forecasting model - Cbest model uses the
top 10 such generated cause features, given the
stock name as the effect and apply a vector auto-

1https://github.com/dykang/cgraph
2https://finance.yahoo.com

Table 1: 30 day windowed average of stock price pre-
diction error using PCG

Step size Cbest PCGuni PCGbi PCGboth
1 1.96 0.022 0.023 0.020
3 3.78 0.022 0.023 0.022
5 5.25 0.022 0.023 0.021

regressive model on the combined time series of
text and historical stock values.

Comparison with the baseline: Compared to
the baseline, note that our features and topics were
chosen purely based on distributional semantics of
the word time series. Once the features are ex-
tracted from PCG, we use the past values of stock
prices and time series corresponding to the incom-
ing word edges of PCG to predict the future values
of the stock prices using the multivariate regres-
sion equation used to determine Granger Causal-
ity. As compared to their best error, PCG from uni-
grams, bigrams or both obtain two orders lower er-
ror and significantly outperforms Cbest. The mean
absolute error (MAE) for the same set of evalu-
ations is within 0.003 of the RMSE, which indi-
cates that the variance of the errors is also low.
We attribute this gain to the flexibility of PCG’s
Lasso Granger method to produce sparse graphs
as compared to CGRAPH’s Vector Auto Regres-
sive model which used a fixed number (10) of in-
coming edges per node already pre-filtered by a
semantic parser. This imposes an artificial bound
on sparsity thereby losing valuable latent informa-
tion. We overcome this in PCG using a suitable
penalty term (λ) in the Lasso method.

Key PCG factors for 2013: The causal links
in PCG are more generic (Table 2) than the ones
described in CGRAPH, supporting the hypothesis
that latent word relationships do exist that go be-
yond the scope of a single news article. The nodes
of CGRAPH are tuples extracted from a seman-
tic parser (SEMAFOR (Das et al., 2010)) based
on evidence of causality in a sentence. PCG poses
no such restriction and derives topical (unfriended,
FB) and inter-topical (healthcare, AMZN), sparse,
latent and semantic relationships.

Inspecting the links and paths of PCG gives us
qualitative insights into the context in which the
word-word relationships were established. Since
PCG is also capable of representing other stock
time series as potential influencers in the network,
we can use this to model the propagation of shocks
in the market as shown in Figure 2. However,



2344

Table 2: Stock price predictive factors for 2013 in PCG

Stock symbol Prediction indicators
AAPL workplace, shutter, music
AMZN healthcare, HBO, cloud
FB unfriended, troll, politician
GOOG advertisers, artificial intelligence, shake-up
HPQ China, inventions, Pittsburg
IBM 64 GB, redesign, outage
MSFT interactive, security, Broadcom
ORCL corporate, investing, multimedia
TSLA prices, testers, controversy
YHOO entertainment, leadership, investment

HP AAPL

AMZN

FB

14

8

8

Figure 2: Inter-stock influencer links where one stock’s
movement indicates future movement of other stocks
(time lag annotated edges)

these links were not used for prediction perfor-
mance to maintain parity with our baseline.

5.3 Time Varying Causal Analysis

We quantitatively evaluate the time varying variant
of PCG by using it to extract features for stock
price prediction for a longer time window.

We present average root mean squared errors in
Table 3 for different values of time windows of
size W (50,100). For model selection, we used
50% of the time series and then used multiple time
series of length W , disjoint from the ones used for
time-varying factor selection and took average of
the test error on the next 30 data points using the
weights learnt. We repeat this using K-fold cross
validation (K=10) for choosing the model selec-
tion data and present the average errors. The vari-
ation in importance weights of predictive causal
factors for stock prices (“podcast”, AAPL) and
(“unfollowers”, GOOG) is shown in Figure 3
which illustrates several peaks (for weeks) when
the factor in the news was particularly predictive
of the company’s stock price and not significant
during other weeks.

The time series and error graph shown for mul-
tiple stocks shows that the RMSE errors range be-
tween 1.5% 5% for all the test time series as shown
in Figure 4. However, sudden spikes tend to dis-
play higher error rates due to the lack of train-
ing data which contain spikes. This issue is mit-

0 10 20 30 40 50 60 70 80

time (in weeks)

0.000

0.001

0.002

0.003

0.004

0.005

0.006

0.007

0.008

0.009

Im
p
o
rt

a
n
ce

 w
e
ig

h
t

(a) “podcast” for predicting AAPL stock

0 10 20 30 40 50 60 70 80

time (in weeks)

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

Im
p
o
rt

a
n
ce

 w
e
ig

h
t

(b) “unfollowers” for predicting GOOG stock

Figure 3: Temporal variation in importance weight of
predictive causal factors

igated when the time window for training is in-
creased. Increasing the window more than 100 did
not improve the RMSE and came at the cost of
training time. But, incorporating the spike resid-
ual PCG model, which predicts the leftover price
value from the first model, provides significant im-
provements over the model without spike correc-
tion as seen in the last column on Table 3. Thus,
we are able to achieve significant gains with an
unsupervised approach without any domain spe-
cific feature engineering by estimating using an
ARIMA model (p, d, q) = (30, 1, 10).

6 Interpretation of Predictive Causal
Factors

In order to qualitatively validate that the latent
inter-topic edges learnt from the news stream is
also humanly interpretable, we constructed PCG
from the online archives of Times of India (TOI)
3, the most circulated Indian English newspaper.
We used this dataset as, unlike the previous dataset

3https://timesofindia.indiatimes.com/archive.cms



2345

0 100 200 300 400 500 600 700 800 900

time (days)

1.0

1.5

2.0

2.5

3.0

3.5

4.0

4.5

5.0

5.5

R
M

S
E

50

60

70

80

90

100

110

st
o
ck

 p
ri

ce

AAPL Stock Prediction

0 100 200 300 400 500 600 700 800 900

time (days)

0.0

0.5

1.0

1.5

2.0

2.5

3.0

R
M

S
E

5

6

7

8

9

10

11

12

13

14

st
o
ck

 p
ri

ce

HPQ Stock Prediction

0 100 200 300 400 500 600 700 800 900

time (days)

0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

R
M

S
E

24

26

28

30

32

34

36

38

st
o
ck

 p
ri

ce

ORCL Stock Prediction

Figure 4: RMSE of stocks for longitudinal causal factor prediction without spike correction. Spikes in RMSE can
be seen along with spikes in stock prices like HPQ.

Table 3: Variation in stock price prediction error
(RMSE) % with window size and spike correction

Stock W=50 W=100 W=100 + spike
AABA 2.87 2.07 1.61
AAPL 2.95 2.84 2.28
AMZN 3.03 2.99 2.41
GOOG 2.67 2.36 1.91
HPQ 6.77 3.34 2.44
IBM 2.19 2.07 1.65
MSFT 3.03 9.45 4.80
ORCL 2.94 2.21 1.65
TSLA 5.56 5.52 4.32

which provided just the time series of words, we
also have the raw text of the articles, which al-
lowed us to perform manual causal signature ver-
ification. This dataset contains all the articles
published in their online edition between Jan-
uary 1, 2006 and December 31, 2015 containing
1,538,932 articles.

6.1 Inter-topic edges of PCG
The influence network we constructed from the
TOI dataset has 18,541 edges and 7,190 uni-grams
and bi-gram vertices. We were interested in the
inter-topic non-associative relationships that PCG
is expected to capture. We observe that a few top-
ics (5) influence or are influenced by a large num-
ber of topics. Some of these highly influential top-
ics are composed of words describing “Agricul-
ture”, “Politics”, “Crime”, etc. The ability of PCG
to learn these edges between topical word clusters
purely based on temporal predictive causal predic-
tion further validates its use for design of extensive
causal inference experiments.

6.2 Causal evidence in PCG
To validate the causal links in PCG, we extracted
56 causation semantic frame (Baker et al., 1998)
arguments which depict direct causal relationships
in the news corpus. We narrowed down the search
to words surrounding verbs which depict the no-

tion of causality like “caused”, “effect”, “due to”
and manually verified that these word pairs were
indeed causal. We then searched the shortest path
in PCG between these word pairs. For example,
one of the news article mentioned that “Gujarat
government has set aside a suggestion for price
hike in electricity due to the Mundra Ultra Mega
Power Project.” and these corresponding causa-
tion arguments were captured by a direct link in
PCG as shown in Table 4. 67% of the word pairs
which were manually identified to be causal in
the news text through causal indicator words such
as “caused”, were linked in PCG through direct
edges, while the rest were linked through an in-
termediate relevant node. As seen in Table 4, the
bi-gram involving the words and the intermediate
words in the path provide the relevant context un-
der which the causality is established. The time
lags in the path show that the influence between
events are at different time lags. We also qualita-
tively verified that two unrelated words are either
not connected or have a path length greater than
2, which makes the relationship weak. The abil-
ity of PCG to validate such humanly understood
causal pairs with temporal predictive causality can
be used for better hypothesis testing.

Table 4: Comparison with manually identified influ-
ence from news articles

Pairs in news Relevant paths in PCG
price, project price-hike –(19)– power-project
land, budget allot-land –(22)– railway-budget
price, land price-hike –(12)– land
strike, law terror-strike –(25)– law ministry
land, bill land-reform –(25)– bill-pass
election, strike election –(21)– Kerala government –(10)– strike
election, strike election –(18)– Mumbai University –(14)– strike
election, strike election –(20)– Shiromani Akali –(13)– strike

7 Conclusion

We presented PCG, a framework for building pre-
dictive causal graphs which capture hidden rela-



2346

tionships between words in text streams. PCG
overcomes the limitations of contextual represen-
tation approaches and provides the framework to
capture inter-document word and topical relation-
ships spread across time to solve complex min-
ing tasks from text streams. We demonstrated
the power of these graphs in providing insights to
answer causal hypotheses and extracting features
to provide consistent, interpretable and accurate
stock price prediction from news streams through
honest estimation on unseen time series data.

References
David Ahn. 2006. The stages of event extrac-

tion. In Proceedings of the Workshop on Annotat-
ing and Reasoning about Time and Events. As-
sociation for Computational Linguistics, Sydney,
Australia, 1–8. https://www.aclweb.org/
anthology/W06-0901

Giuseppe Amodeo, Roi Blanco, and Ulf Brefeld. 2011.
Hybrid Models for Future Event Prediction (CIKM
’11). 1981–1984.

Andrew Arnold, Yan Liu, and Naoki Abe. 2007.
Temporal Causal Modeling with Graphical Granger
Methods. In Proceedings of the 13th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining (KDD ’07). ACM, New York, NY,
USA, 66–75. https://doi.org/10.1145/
1281192.1281203

Susan Athey and Guido Imbens. 2016. Recursive par-
titioning for heterogeneous causal effects: Table
1. Proceedings of the National Academy of Sci-
ences 113, 7353–7360. https://doi.org/
10.1073/pnas.1510489113

Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Pro-
ceedings of the 36th Annual Meeting of the Asso-
ciation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguis-
tics - Volume 1 (ACL ’98/COLING ’98). Associ-
ation for Computational Linguistics, Stroudsburg,
PA, USA, 86–90. https://doi.org/10.
3115/980845.980860

Ivo Bernardo, Roberto Henriques, and Victor Lobo.
2018. Social Market: Stock Market and Twit-
ter Correlation. In Intelligent Decision Technologies
2017, Ireneusz Czarnowski, Robert J. Howlett, and
Lakhmi C. Jain (Eds.). Springer International Pub-
lishing, Cham, 341–356.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. J. Mach. Learn.
Res. 3 (March 2003), 993–1022.

Peter J. Brockwell and Richard A. Davis. 2002. In-
troduction to Time Series and Forecasting (2nd ed.).
Springer.

Ching-Yun Chang, Yue Zhang, Zhiyang Teng, Zahn
Bozanic, and Bin Ke. 2016. Measuring the Informa-
tion Content of Financial News. In COLING. ACL,
3216–3225.

Dehua Cheng, Mohammad Taha Bahadori, and Yan
Liu. 2014. FBLG: A Simple and Effective Ap-
proach for Temporal Dependence Discovery from
Time Series Data (KDD ’14). 382–391. https:
//doi.org/10.1145/2623330.2623709

Ali F. Darrat, Maosen Zhong, and Louis T.W.
Cheng. 2007. Intraday volume and volatility re-
lations with and without public news. Jour-
nal of Banking and Finance 31, 9 (2007), 2711
– 2729. https://doi.org/10.1016/j.
jbankfin.2006.11.019

Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic Frame-
Semantic Parsing. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics. Association for Com-
putational Linguistics, Los Angeles, California,
948–956. https://www.aclweb.org/
anthology/N10-1138

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training
of Deep Bidirectional Transformers for Language
Understanding. CoRR abs/1810.04805 (2018).
arXiv:1810.04805 http://arxiv.org/abs/
1810.04805

Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan.
2014. Using Structured Events to Predict Stock
Price Movement: An Empirical Investigation.

Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan.
2015. Deep Learning for Event-Driven Stock Pre-
diction. In IJCAI. AAAI Press, 2327–2333.

Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan.
2016. Knowledge-Driven Event Embedding for
Stock Prediction. In COLING. ACL, 2133–2142.

Quang Xuan Do, Yee Seng Chan, and Dan Roth.
2011. Minimally Supervised Event Causal-
ity Identification. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ’11). Association for
Computational Linguistics, Stroudsburg, PA, USA,
294–303. http://dl.acm.org/citation.
cfm?id=2145432.2145466

Pegah Falinouss. 2007. Stock Trend Prediction using
News Events. Masters thesis (2007).

George Forman. 2003. An extensive empirical study
of feature selection metrics for text classification.
Journal of machine learning research 3, Mar (2003),
1289–1305.

https://www.aclweb.org/anthology/W06-0901
https://www.aclweb.org/anthology/W06-0901
https://doi.org/10.1145/1281192.1281203
https://doi.org/10.1145/1281192.1281203
https://doi.org/10.1073/pnas.1510489113
https://doi.org/10.1073/pnas.1510489113
https://doi.org/10.3115/980845.980860
https://doi.org/10.3115/980845.980860
https://doi.org/10.1145/2623330.2623709
https://doi.org/10.1145/2623330.2623709
https://doi.org/10.1016/j.jbankfin.2006.11.019
https://doi.org/10.1016/j.jbankfin.2006.11.019
https://www.aclweb.org/anthology/N10-1138
https://www.aclweb.org/anthology/N10-1138
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
http://dl.acm.org/citation.cfm?id=2145432.2145466
http://dl.acm.org/citation.cfm?id=2145432.2145466


2347

Clive WJ Granger, Bwo-Nung Huangb, and Chin-Wei
Yang. 2000. A bivariate causality between stock
prices and exchange rates: evidence from recent
Asianflu. The Quarterly Review of Economics and
Finance 40, 3 (2000), 337–354.

Michael Hagenau, Michael Liebmann, and Dirk Neu-
mann. 2013. Automated news reading: Stock price
prediction based on financial news using context-
capturing features. Decision Support Systems 55,
3 (2013), 685 – 697. https://doi.org/10.
1016/j.dss.2013.02.006

Chikara Hashimoto, Kentaro Torisawa, Julien Kloet-
zer, and Jong-Hoon Oh. 2015. Generating Event
Causality Hypotheses Through Semantic Relations.
In Proceedings of the Twenty-Ninth AAAI Confer-
ence on Artificial Intelligence (AAAI’15). AAAI
Press, 2396–2403. http://dl.acm.org/
citation.cfm?id=2886521.2886654

Johan Hovold. 2005. Naive Bayes Spam Filtering Us-
ing Word-Position-Based Attributes.. In CEAS. 41–
48.

Joshi Kalyani, H. N. Bharathi, and Rao Jyothi.
2016. Stock trend prediction using news sen-
timent analysis. CoRR abs/1607.01958 (2016).
arXiv:1607.01958 http://arxiv.org/abs/
1607.01958

Dongyeop Kang, Varun Gangal, Ang Lu, Zheng Chen,
and Eduard Hovy. 2017. Detecting and Explaining
Causes From Text For a Time Series Event. In Pro-
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Copenhagen, Den-
mark, 2758–2767. https://www.aclweb.
org/anthology/D17-1292

Noriaki Kawamae. 2011. Trend analysis model: trend
consists of temporal words, topics, and timestamps
(WSDM ’11). 317–326.

Zornitsa Kozareva. 2012. Cause-effect Relation Learn-
ing. In Workshop Proceedings of TextGraphs-7 on
Graph-based Methods for Natural Language Pro-
cessing (TextGraphs-7 ’12). Association for Com-
putational Linguistics, Stroudsburg, PA, USA, 39–
43. http://dl.acm.org/citation.cfm?
id=2392954.2392961

Quoc V. Le and Tomas Mikolov. 2014. Distributed
Representations of Sentences and Documents.
CoRR abs/1405.4053 (2014). arXiv:1405.4053
http://arxiv.org/abs/1405.4053

Chen Luo, Jian-Guang Lou, Qingwei Lin, Qiang Fu,
Rui Ding, Dongmei Zhang, and Zhe Wang. 2014.
Correlating Events with Time Series for Incident Di-
agnosis (KDD ’14). 1583–1592. https://doi.
org/10.1145/2623330.2623374

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schtze. 2008. Scoring, term weighting,
and the vector space model. Cambridge University

Press, 100123. https://doi.org/10.1017/
CBO9780511809071.007

Christopher D Manning, Hinrich Schütze, et al. 1999.
Foundations of statistical natural language process-
ing. Vol. 999. MIT Press.

Huina Mao, Scott Counts, and Johan Bollen. 2011.
Predicting Financial Markets: Comparing Survey,
News, Twitter and Search Engine Data. Arxiv
preprint (2011).

Mariusz Maziarz. 2015. A review of the Granger-
causality fallacy. The Journal of Philosoph-
ical Economics 8, 2 (2015), 6. https:
//EconPapers.repec.org/RePEc:bus:
jphile:v:8:y:2015:i:2:n:6

Nicolai Meinshausen and Peter Bühlmann. 2006.
High-dimensional graphs and variable selection with
the lasso. The annals of statistics (2006), 1436–
1462.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed Repre-
sentations of Words and Phrases and Their Com-
positionality. In Proceedings of the 26th Interna-
tional Conference on Neural Information Processing
Systems - Volume 2 (NIPS’13). Curran Associates
Inc., USA, 3111–3119. http://dl.acm.org/
citation.cfm?id=2999792.2999959

Paramita Mirza and Sara Tonelli. 2016. CATENA:
CAusal and TEmporal relation extraction from
NAtural language texts. In Proceedings of COL-
ING 2016, the 26th International Conference
on Computational Linguistics: Technical Pa-
pers. The COLING 2016 Organizing Commit-
tee, 64–75. http://www.aclweb.org/
anthology/C16-1007

Alexandra Olteanu, Onur Varol, and Emre Kiciman.
2017. Distilling the Outcomes of Personal Experi-
ences: A Propensity-scored Analysis of Social Me-
dia. In Proceedings of The 20th ACM Conference on
Computer-Supported Cooperative Work and Social
Computing (computer-supported cooperative work
and social computing ed.). Association for Comput-
ing Machinery, Inc.

Judea Pearl. 2009. Causal inference in statistics: An
overview. Statistics Surveys (2009).

Kira Radinsky and Eric Horvitz. 2013. Mining the web
to predict future events (WSDM ’13). ACM, 255–
264.

Tushar Rao and Saket Srivastava. 2012. Ana-
lyzing Stock Market Movements Using Twitter
Sentiment Analysis. In Proceedings of the 2012
International Conference on Advances in Social
Networks Analysis and Mining (ASONAM 2012)
(ASONAM ’12). IEEE Computer Society, Washing-
ton, DC, USA, 119–123. https://doi.org/
10.1109/ASONAM.2012.30

https://doi.org/10.1016/j.dss.2013.02.006
https://doi.org/10.1016/j.dss.2013.02.006
http://dl.acm.org/citation.cfm?id=2886521.2886654
http://dl.acm.org/citation.cfm?id=2886521.2886654
http://arxiv.org/abs/1607.01958
http://arxiv.org/abs/1607.01958
https://www.aclweb.org/anthology/D17-1292
https://www.aclweb.org/anthology/D17-1292
http://dl.acm.org/citation.cfm?id=2392954.2392961
http://dl.acm.org/citation.cfm?id=2392954.2392961
http://arxiv.org/abs/1405.4053
https://doi.org/10.1145/2623330.2623374
https://doi.org/10.1145/2623330.2623374
https://doi.org/10.1017/CBO9780511809071.007
https://doi.org/10.1017/CBO9780511809071.007
https://EconPapers.repec.org/RePEc:bus:jphile:v:8:y:2015:i:2:n:6
https://EconPapers.repec.org/RePEc:bus:jphile:v:8:y:2015:i:2:n:6
https://EconPapers.repec.org/RePEc:bus:jphile:v:8:y:2015:i:2:n:6
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://www.aclweb.org/anthology/C16-1007
http://www.aclweb.org/anthology/C16-1007
https://doi.org/10.1109/ASONAM.2012.30
https://doi.org/10.1109/ASONAM.2012.30


2348

Y. Shynkevich, T. M. McGinnity, S. Coleman, and
A. Belatreche. 2015. Stock price prediction based
on stock-specific and sub-industry-specific news ar-
ticles. In 2015 International Joint Conference on
Neural Networks (IJCNN). 1–8. https://doi.
org/10.1109/IJCNN.2015.7280517

Robert Tibshirani. 1994. Regression Shrinkage and Se-
lection Via the Lasso. Journal of the Royal Statisti-
cal Society, Series B 58 (1994), 267–288.

Carmen K Vaca, Amin Mantrach, Alejandro Jaimes,
and Marco Saerens. 2014. A time-based collective
factorization for topic discovery and monitoring in
news (WWW ’14). 527–538.

Ishan Verma, Lipika Dey, and Hardik Meisheri.
2017. Detecting, Quantifying and Accessing Im-
pact of News Events on Indian Stock Indices.
In Proceedings of the International Conference
on Web Intelligence (WI ’17). ACM, New York,
NY, USA, 550–557. https://doi.org/10.
1145/3106426.3106482

Yu Wang, Eugene Agichtein, and Michele Benzi. 2012.
Tm-lda: efficient online modeling of latent topic
transitions in social media (KDD ’12). ACM, 123–
131.

Yixin Wang and David M. Blei. 2018. The Blessings of
Multiple Causes. CoRR abs/1805.06826. http:
//dblp.uni-trier.de/db/journals/
corr/corr1805.html#abs-1805-06826

Boyi Xie, Rebecca J. Passonneau, Leon Wu, and
Germán Creamer. 2013. Semantic Frames to Predict
Stock Price Movement. In ACL (1). The Association
for Computer Linguistics, 873–883.

https://doi.org/10.1109/IJCNN.2015.7280517
https://doi.org/10.1109/IJCNN.2015.7280517
https://doi.org/10.1145/3106426.3106482
https://doi.org/10.1145/3106426.3106482
http://dblp.uni-trier.de/db/journals/corr/corr1805.html#abs-1805-06826
http://dblp.uni-trier.de/db/journals/corr/corr1805.html#abs-1805-06826
http://dblp.uni-trier.de/db/journals/corr/corr1805.html#abs-1805-06826

