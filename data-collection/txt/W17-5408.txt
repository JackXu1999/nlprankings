



















































ACTSA: Annotated Corpus for Telugu Sentiment Analysis


Proceedings of the First Workshop on Building Linguistically Generalizable NLP Systems, pages 54–58
Copenhagen, Denmark, September 8, 2017. c©2017 Association for Computational Linguistics

ACTSA: Annotated Corpus for Telugu Sentiment Analysis

Sandeep Sricharan Mukku and Radhika Mamidi
Language Technologies Research Center

KCIS, IIIT Hyderabad
sandeep.mukku@research.iiit.ac.in, radhika.mamidi@iiit.ac.in

Abstract

Sentiment analysis deals with the task
of determining the polarity of a doc-
ument or sentence and has received
a lot of attention in recent years for
the English language. With the rapid
growth of social media these days, a
lot of data is available in regional lan-
guages besides English. Telugu is one
such regional language with abundant
data available in social media, but it’s
hard to find a labelled data of sen-
tences for Telugu Sentiment Analysis.
In this paper, we describe an effort to
build a gold-standard annotated cor-
pus of Telugu sentences to support Tel-
ugu Sentiment Analysis. The corpus,
named ACTSA (Annotated Corpus for
Telugu Sentiment Analysis) has a col-
lection of Telugu sentences taken from
different sources which were then pre-
processed and manually annotated by
native Telugu speakers using our anno-
tation guidelines. In total, we have an-
notated 5410 sentences, which makes
our corpus the largest resource cur-
rently available. The corpus and an-
notation guidelines are made publicly
available.

1 Introduction
Now-a-days, people are commonly found writ-
ing comments, reviews, blog posts in social me-
dia about trending activities in their regional
languages. Unlike English, many regional lan-
guages lack NLP tools and resources to ana-
lyze these activities. Moreover, English has
many datasets available, however, it is not the
same with Telugu.

The annotation of Telugu data has not re-
ceived a lot of attention in sentiment analysis
community. While there is a wealth of raw
corpora with opinionated information, no cor-
pora with annotated sentences in Telugu are
publicly available as far as we know.

Telugu has a special status as an official
standard language in the twin states of Andhra
Pradesh and Telangana of India. There are
a large variety of dialects that constitute the
mother tongues of Telugu speakers. Major
Telugu print media, journalism, and electronic
media follow the dialects of Krishna and Go-
davari since it has been conceived as arguably
standard and easy to reach the rest of the Tel-
ugu speakers (Krishnamurthi, 1961). We built
our corpus over this dialect as this dialect is
most prominent and has a strong online pres-
ence today on news websites, blogs, forums,
and user/reader commentaries.

In this work, we present a dedicated gold
standard corpus of polarity annotated Telugu
sentences. To our knowledge, our corpus is
the largest source of polarity annotated Telugu
sentences to date. This data also motivates
the development of new techniques for Telugu
sentiment analysis. The corpus and annota-
tion guidelines are publicly available here1.

2 Related Work

There is a growing interest within the Natural
Language Processing community to build cor-
pora for Indian languages from the data avail-
able on the web. (Kaur and Gupta, 2013) sur-
veyed sentiment analysis for different Indian
languages including Telugu, but never men-
tioned about the corpus used. (Mukku et al.,
2016) did sentiment classification for Telugu

1https://goo.gl/M9rkUX

54



Figure 1: Process of building the resource

text using various ML techniques, but no data
was publicly made available.

(Wiebe et al., 2005) describes a corpus an-
notation project to study issues in the man-
ual annotation of opinions, emotions, senti-
ments, speculations, evaluations and other pri-
vate states in language. This was the first at-
tempt to manually annotate the 10,000 sen-
tence corpus of articles from the news. (Alm
et al., 2005) have manually annotated 1580
sentences extracted from 22 Grimms’ tales for
the task of emotion annotation at the sentence
level.

(Arora, 2013) performed sentiment analy-
sis task for the Hindi Language with limited
corpus made manually annotated by the na-
tive Hindi speakers. (Das and Bandyopad-
hyay, 2010b) aims to manually annotate the
sentences in a web-based Bengali blog corpus
with the emotional components such as emo-
tional expression (word/phrase), intensity, as-
sociated holder and topic(s).

(Das and Bandyopadhyay, 2010a) built a
lexicon of words to support the task of Tel-
ugu sentiment analysis and is made available
to the public. (Das and Bandyopadhay, 2010)
created an interactive gaming to technology
(Dr. Sentiment) to create and validate Senti-
WordNet for Telugu.

3 Data Collection
In this section, we will explore the different
resources where raw data was obtained from
and how processing of that data was done, as
shown in Figure 1.

Currently, most of the corpora available
for Sentiment Analysis are harvested from
sources like review data from e-commerce web-
sites where customers express their opinion on
products freely, posts from social networking

sites like Twitter and Facebook. Although
the news genre has received much less atten-
tion within the Sentiment Analysis commu-
nity, news plays an important role in exhibit-
ing the reality and has a strong influence on
social practices. Also, a lot of Telugu data
is available mostly on news websites. These
reasons motivated us to select news genre for
building our corpus. We scraped and har-
vested our raw data from five different Telugu
news websites viz., Andhrabhoomi2, Andhra-
jyothi3, Eenadu4, Kridajyothi5 and Sakshi6.
In total we have collected over 453 news ar-
ticles and filtered down to 321 which were rel-
evant to our work.

The extracted data was cleaned in a pre-
processing step, e.g. by removing headings
and sub-headings, eliminating sentences with
non-Telugu words and cleaning any extra dots,
extra spaces, URLs, and other garbage values.
Later Sentence Segmentation is done where
this data was split into individual sentences.

The sentences thus obtained were now
tested for objectivity manually. Objective sen-
tences are sentences where no sentiment, opin-
ion, etc. is expressed. They state a fact confi-
dently and has an evidence to support it. For
example, sentence (1) is an objective sentence
as it is a verifiable fact with evidence.

అ క ం రత శఅధ ప (1)

Transliteration: Abdul kalāṁ bhāratadēśa
adhyakṣuḍigā panicēśāru
English: Abdul Kalam served as the presi-
dent of India

2http://www.andhrabhoomi.net/
3 http://www.andhrajyothy.com/
4 http://www.eenadu.net/
5 http://www.andhrajyothy.com/pages/sports
6 http://www.sakshi.com/

55



Table 1: Example annotations
ID Original Sentence English Translation A1 A2 V F

1

అమె అధ
టం వరణ
ఒప ందం ం అమె
వై ల ం

US President Donald
Trump withdrew the
US from the Paris Cli-
mate Agreement

Neg Obj Obj Obj

2 ఇం ఎవ అభ ంతరంఉండనవసరం
There is no need for
any objection to any-
one in this

Neu Neu NA Neu

3
రత ప నమం న ం-

ద అలరపై
స ం ం

India’s Prime Minister
Narendra Modi has re-
acted severely to the
Kashmir riots

Neg Neg NA Neg

4 ఫ లపై మం సం షంఉ
The minister is happy
on the results Pos Pos NA Pos

Pos = Positive, Neg = Negative, Neu = Neutral, Obj = Objective, NA = Not Applicable,
A1 = Annotator 1, A2 = Annotator 2, V = Validation, F = Final Result

These sentences do not contain any senti-
ment/polarity and are not useful for sentiment
analysis. The objective sentences thus sepa-
rated with objectivity test are removed from
the data.

4 Annotation

In this section, we describe the process fol-
lowed for annotating the sentences (refer Fig-
ure 1). First, we built a team of seven ed-
ucated native Telugu speakers for the task
of polarity tagging of the extracted Telugu
sentences. Then, we developed an annota-
tion schema for this task and the annotators
were instructed to thoroughly understand the
concepts mentioned in the schema for a pre-
cise/perfect annotation. Each sentence is an-
notated by two annotators.

The annotators were required to tag the sen-
tences with three polarities: positive, negative,
neutral. For example, sentence (2) should be
tagged positive as it expresses positive senti-
ment by the use of కృతజత (gratitude).

మం ,ఆయన ఎ న ం , (2)

పజల కృతజతవ కం
Transliteration: Mantri, āyananu ennukun-
nanduku, prajalaku kr�tajñata vyaktaṁ cēśāru
English: The minister expressed gratitude to

the people for electing him

On the other hand sentence (3) should be
tagged negative because it expresses negative
sentiment with ఆం ళన (concern).
రంతర తలపై పజ ఆం ళనవ కం

(3)

Transliteration: Nirantara vidyut kōtalapai
prajalu āndōḷana vyaktaṁ cēśāru
English: People have expressed concern over
continuous power cuts

However, sentence (4) is a neutral sentence
as it is a speculation about the future. Even
though it doesn’t contain any sentiment, it
is not an objective sentence because it is not
a verifiable fact or not something which hap-
pened in the past. It is speculating something
to happen in the future.

ప వ నెల చై సంద ంచ (4)
Transliteration: Pradhāni vaccē nelalō
cainānu sandarśin̄canunnāru
English: The prime minister is expected to
visit China next month

If in any case annotators were unsure or felt
ambiguous about the polarity of a sentence
they can label it uncertain. If they feel the
sentence is objective but was not removed in

56



Table 2: Agreement for Sentences in ACTSA

Annotator 1
Annotator 2 Positive Negative Neutral Total

Positive 1463 31 103 1597
Negative 23 1421 116 1560
Neutral 112 127 2427 2666
Total 1598 1579 2646 5823

the pre-processing step, they can mark it ob-
jective.

Annotators labelled all the sentences, with
each sentence annotated by exactly two anno-
tators. We call it annotation step. The sen-
tences marked uncertain by at least one anno-
tator were discarded to avoid any ambiguous
sentences in the corpus.

The sentences which had a clash between
the two annotators’ labels were sent for a third
independent annotation which we call as a val-
idation step. The most common label among
the three annotators was considered as the fi-
nal label for the sentence. If even after the
third annotation the disagreement prevailed,
such sentences were discarded as we consid-
ered them too ambiguous for getting three dif-
ferent labels by three different annotators. If
there were any objective sentences after the
validation step, they were discarded.

Table 1 shows some example annotations
from the corpus.

5 Agreement Study

After annotation task, we measured how re-
liable our annotation scheme was. To mea-
sure the reliability of our polarity annota-
tion scheme, we conducted an inter-annotator
agreement study on the annotated sentences.
Table 2 shows the agreement for the two anno-
tators’ judgments for each sentence. We used
Cohens´ kappa, κ which is calculated using
formula (5)

κ =
po − pe
1− pe (5)

where po is the relative observed agreement
and pe is the agreement by chance. In general,
κ values between 0.6 and 0.8 are considered
a substantial agreement. To our surprise we
got the κ value to be 0.87, which is in perfect

agreement and is an indication of the reliabil-
ity of the annotations.

Table 3: Statistics about the data
News articles 321

Cleaned Sentences 11952
Objective Sentences (Removed) 4327
Uncertain Sentences (Removed) 1802

Disagreement Sentences 512
Classified 99
Removed 413

Positive sentences 1489
Negative sentences 1441
Neutral sentences 2475
Total sentences 5410

6 Corpus Statistics
In this section, we present the statistics about
our data from raw data collection to final sen-
tences. We scraped several websites for the
data. We collected 453 news articles and fil-
tered down to 321 which were relevant for our
work. After pre-processing this raw data, we
have 11952 sentences. We tested the sentences
for subjectivity (as explained in section 3) and
removed 4327 objective sentences after which
we were left with 7812 sentences. These sen-
tences were given to the annotators for the an-
notation as mentioned in section 4. 1802 sen-
tences were removed where at least one anno-
tator marked it uncertain. In the remaining
5823 sentences, 512 were with disagreement
and were sent for third independent annota-
tion. After the third annotation, 413 sentences
were discarded if the disagreement prevailed or
if they are objective. The final 5410 sentences
forms the required annotated corpus, ACTSA.
Statistics about our complete corpus can be
found in Table 3.

57



7 Experiments and Evaluation

A strategy that can give very useful hints
about the reliability of the annotated data is
the comparison between the results of auto-
mated classification and human annotation.

(Mukku et al., 2016) described a method
to perform automated classification of Telugu
sentences into polarity tags: positive, nega-
tive and neutral. We followed this method to
evaluate our data. We used 2000 sentences
from our human automated corpus to train the
model for automated classification.

To test the reliability of our annotated data,
we compared the classification expressed by
humans and that of the automated classifier
trained above. The testing was done on the
remaining 3410 sentences and the error rate
was observed to be 12.3% which hints the
quality and reliability of the annotated corpus,
ACTSA.

8 Conclusion

In this work, we presented a gold standard cor-
pus of Telugu sentences taken from different
resources, which were then cleaned and an-
notated by native Telugu speakers. For each
sentence, we have a polarity label attached
with it. We described our annotation pro-
cess and gave an overview of our annotation
schema. The results from our evaluation study
show that our corpus has a reasonable inter-
annotator agreement. The corpus and guide-
lines are publicly available. In future, we try
to automate the task of annotation for new
sentences with the help of ACTSA. We would
also like to perform sentiment analysis task for
Telugu, using this corpus.

Acknowledgements

We would like to thank all the members of
annotation group (native Telugu speakers) for
their tireless effort in this process. The work
reported in this paper was supported by Kohli
Center on Intelligent Systems.

References
Cecilia Ovesdotter Alm, Dan Roth, and Richard

Sproat. 2005. Emotions from text: machine
learning for text-based emotion prediction. In

Proceedings of the conference on human lan-
guage technology and empirical methods in nat-
ural language processing. Association for Com-
putational Linguistics, pages 579–586.

Piyush Arora. 2013. Sentiment analysis for hindi
language. MS by Research in Computer Science,
IIIT Hyderabad .

Amitava Das and S Bandyopadhay. 2010. Dr sen-
timent creates sentiwordnet (s) for indian lan-
guages involving internet population. In Pro-
ceedings of Indo-wordnet workshop.

Amitava Das and Sivaji Bandyopadhyay. 2010a.
Sentiwordnet for indian languages. Asian Fed-
eration for Natural Language Processing, China
pages 56–63.

Dipankar Das and Sivaji Bandyopadhyay. 2010b.
Labeling emotion in bengali blog corpus–a fine
grained tagging at sentence level. In Proceed-
ings of the 8th Workshop on Asian Language
Resources. page 47.

Amandeep Kaur and Vishal Gupta. 2013. A sur-
vey on sentiment analysis and opinion mining
techniques. Journal of Emerging Technologies
in Web Intelligence 5(4):367–371.

Bh Krishnamurthi. 1961. Telugu verbal bases: A
comparative and descriptive study.

Sandeep Sricharan Mukku, Nurendra Choudhary,
and Radhika Mamidi. 2016. Enhanced senti-
ment classification of telugu text using ml tech-
niques. In 4th Workshop on Sentiment Anal-
ysis where AI meets Psychology, 25th Inter-
national Joint Conference on Artificial Intelli-
gence. page 29.

Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and
emotions in language. Language resources and
evaluation 39(2):165–210.

58


