



















































A Spatial Model for Extracting and Visualizing Latent Discourse Structure in Text


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2268–2277
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

2268

A Spatial Model for Extracting and Visualizing Latent Discourse
Structure in Text

Shashank Srivastava∗
Carnegie Mellon University
Pittsburgh, PA 15213, USA
ssrivastava@cmu.edu

Nebojsa Jojic
Microsoft Research

Redmond, WA 98052, USA
jojic@microsoft.com

Abstract

We present a generative probabilistic model
of documents as sequences of sentences,
and show that inference in it can lead to
extraction of long-range latent discourse
structure from a collection of documents.
The approach is based on embedding se-
quences of sentences from longer texts into
a 2- or 3-D spatial grids, in which one or
two coordinates model smooth topic transi-
tions, while the third captures the sequen-
tial nature of the modeled text. A signif-
icant advantage of our approach is that
the learned models are naturally visualiz-
able and interpretable, as semantic simi-
larity and sequential structure are modeled
along orthogonal directions in the grid. We
show that the method can capture discourse
structures in narrative text across multiple
genres, including biographies, stories, and
newswire reports. In particular, our method
can capture biographical templates from
Wikipedia, and is competitive with state-of-
the-art generative approaches on tasks such
as predicting the outcome of a story, and
sentence ordering.

1 Introduction

The ability to identify discourse patterns and nar-
rative themes from language is useful in a wide
range of applications and data analysis. From a
perspective of language understanding, learning
such latent structure from large corpora can pro-
vide background information that can aid machine
reading. For example, computers can use such
knowledge to predict what is likely to happen next

∗*Work done while first author was an intern at Microsoft
Research

in a narrative (Mostafazadeh et al., 2016), or rea-
son about which narratives are coherent and which
do not make sense (Barzilay and Lapata, 2008).
Similarly, knowledge of discourse is increasingly
important for language generation models. Modern
neural generation models, while good at capturing
surface properties of text – by fusing elements of
syntax and style – are still poor at modeling long
range dependencies that go across sentences (Li
and Jurafsky, 2017; Wang et al., 2017). Models of
long range flow in the text can thus be useful as
additional input to such methods.

Previously, the question of modeling discourse
structure in language has been explored through
several lenses, including from perspectives of lin-
guistics, cognitive science and information re-
trieval. Prominent among linguistic approaches
are Discourse Representation Theory (Asher,
1986) and Rhetorical Structure Theory (Mann and
Thompson, 1988); which formalize how discourse
context can constrain the semantics of a sentence,
and lay out ontologies of discourse relation types
between parts of a document. This line of research
has been largely constrained by the unavailability
of corpora of discourse relations, which are ex-
pensive to annotate. Another line of research has
focused on the task of automatic script induction,
building on earlier work in the 1970’s (Schank and
Abelson, 1977). More recently, methods based
on neural distributed representations have been ex-
plored (Li and Hovy, 2014; Kalchbrenner and Blun-
som, 2013; Le and Mikolov, 2014) to model the
flow of discourse. While these methods have had
varying degrees of success, they are largely opaque
and hard to interpret. In this work, we seek to pro-
vide a scalable model that can extract latent sequen-
tial structures from a collection of documents, and
can be naturally visualized to provide a summary
of the learned semantics and discourse trajectories.

In this work, we present an approach for extract-



2269

Figure 1: Modeling principle for Sequential Count-
ing Grids. We design the method to capture se-
mantic similarities between documents along XY
planes (e.g., biographies might be more similar to
literary fiction than news reports), as well extract
sequential trajectories along the Z axes similar to
those shown. The sequence of sentences in a doc-
ument is latently aligned to positions in the grid,
such that the model prefers alignments of contigu-
ous sentences to grid cells that are spatially close.

ing and visualizing sequential structure from a col-
lection of text documents. Our method is based
on embedding sentences in a document in a 3-
dimensional grid, such that contiguous sentences in
the document are likely to be embedded in the same
order in the grid. Further, sentences across docu-
ments that are semantically similar are also likely to
be embedded in the same neighborhood in the grid.
By leveraging the sequential order of sentences in
a large document collection, the method can induce
lexical semantics, as well as extract latent discourse
trajectories in the documents. Figure 1 shows a con-
ceptual schematic of our approach. The method
can learn semantic similarity (across XY planes),
as well as sequential discourse chains (along the
Z-axis). The parameters and latent structure of the
grid are learned by optimizing the likelihood of a
collection of documents under a generative model.
Our method outperforms state-of-the-art generative
methods on two tasks: predicting the outcome of a
story and coherence prediction; and is seen to yield
a flexible range of interpretable visualizations in
different domains of text. Our method is scalable,
and can incorporate a broad range of features. In
particular, the approach can work on simple tok-
enized text.

The remainder of this paper is organized as fol-
lows. In Section 2, we briefly summarize other
related work. In Section 3, we describe our method

in detail. We present experimental results in Sec-
tion 4, and conclude with a brief discussion.

2 Related work

Building on linguistic theories of discourse and
text coherence, several computational approaches
have attempted to model discourse structure from
multiple perspectives. Prominent among these are
Narrative Event Chains (Chambers and Jurafsky,
2008) which learn chains of events that follow a
pattern in a unsupervised framework, and the Entity
grid model (Barzilay and Lapata, 2008), which rep-
resents sentences in a context in terms of discourse
entities occurring in them and trains coherence clas-
sifiers over this representation. Other work extends
these using better models of events and discourse
entities (Lin et al., 2011; Pichotta and Mooney,
2015). Louis and Nenkova (2012) use manually
provided syntactic patterns for sentence representa-
tion, and model transitions in text as Markov prob-
abilities, which is related to our work. However,
while they use simple HMMs over discrete topics,
our method allows for a richer model that also cap-
tures smooth transition across them. Approaches
such as Kalchbrenner and Blunsom (2013); Li et al.
(2014); Li and Jurafsky (2017) model text through
recurrent neural architectures, but are hard to in-
terpret and visualize. Other approaches have ex-
plored applications related to modeling narrative
discourse in context of limited tasks such as story
cloze (Mostafazadeh et al., 2016) and identifying
similar narratives (Chaturvedi et al., 2018).

From a large scale document-mining perspective,
the question of extracting intra-document structure
remains largely underexplored. While early mod-
els such as LDA completely ignore ordering and
discourse elements of a documents, other methods
that use distributed embeddings of documents are
opaque (Le and Mikolov, 2014), even while they
can in principle model sequential structure within
a document. Methods such as HMM-LDA (Grif-
fiths et al., 2005) and Topics-over-time (Wang and
McCallum, 2006) address the related question of
topic evolution in a stream of documents, but these
approaches are too coarse to model intra-document
sequential structure. In terms of our technical ap-
proach, we build on previous research on grid-
based models (Jojic and Perina, 2011), which have
previously been used for topic-modeling for images
and text as unstructured bags-of-features.



2270

3 Sequential CG model

In this section, we present our method, which we
call Sequential Counting Grids, or Sequential
CG. We first present our notation, model formu-
lation and training approach. We discuss how the
method is designed to incorporate smoothness and
sequential structure, and how the method can be
efficiently scaled to train on large document collec-
tions. In Section 3.2, we present a mixture model
variant that combines Sequential CG with a uni-
gram language model.

3.1 Model description
We represent a document as a sequence s of sen-
tences, s = {s1, s2 . . . sD}, where D represents
the number of sentences in the document. In gen-
eral, we assume each sentence is represented as
a multiset of features si = {cz}i, where ciz repre-
sents the count of the feature indexed by z in the
ith sentence in the sequence.1

The Sequential CG consists of a 3-D grid G of
size Ex × Ey × Ez , where Ex, Ey and Ez denote
the extent of the grid along the X, Y and Z-axes
(see Figure 1). Let us denote an index of a position
in the grid by an integer-valued vector i = (ixiyiz).
The three components of the index together spec-
ify a XY location as well as a depth in the grid.
The Sequential CG model is parametrized by two
sets of parameters, πi,z and Pij. Here, πi,z repre-
sents a multinomial distribution over the vocabu-
lary of features z for each cell in the grid G, i.e.∑

z πi,z = 1 ∀ i ∈ G. To induce smoothness
across XY planes, we further define histogram dis-
tributions hi,z , which average the π distributions
in a 2-D neighborhood Wi (of size specified by
W = [Wx,Wy]) around the grid position i. This
notation follows Jojic and Perina (2011).

hi,z =
1

WxWy

∑
i′∈Wi

πi′,z (1)

The generative model assumes that individual sen-
tences in a document are generated by h distribu-
tions in the grid. Movements from one position i
to another j in the grid are modeled as transition
probabilities Pij. The generative process consists
of the following. We uniformly sample a starting
location i1 in the grid. We sample words in the first

1These may simply consist of tokens (words, entities and
MWEs) in the sentence, but can include additional informa-
tion, such as sentiment or event annotations, or other discrete
sentence-level representations

sentence s1 from πi1, and sample the next posi-
tion i2 from the distribution Pi1,:, and so on till we
generate sD. The alignments I = [i1, i2 . . . iD] of
individual sentences in a document with positions
in the grid are latent variables in our model.

Given the sequence of alignments I for a doc-
ument, the conditional likelihood of generating s
is given as a product of generating individual sen-
tences:

p(s| I) =
D∏
d

p({cdz}| id) =
D∏
d=1

∏
z

(hid,z)
cdz

(2)

Since the alignments of sequences to their posi-
tions in the grids I are latent, we marginalize over
these to maximize the likelihood of an observed col-
lection of documents S := {st}Tt=1. Here, T is the
total number of documents, and t is an index over
individual documents. Using Jensen’s inequality,
any distributions qtI over the hidden alignments It
provide lower-bounds on the data log-likelihood.

∑
t

log p(st|π) =
∑
t

log
(∑
I
p(st, I|π)

)
=
∑
t

log
(∑
I
qtI
p(st|I)p(I))

qtI

)
≥ −

∑
t

∑
I
qtI log q

t
I

+
∑
t

∑
I
qtI log

(
p(s|I, π)p(I))

(3)

Here, qtI denotes a variational distribution for each
of the data sequences st. The learning algorithm
consists of an iterative generalized EM procedure
(which can be interpreted as a block-coordinate
ascent in the latent variables qtI and the model pa-
rameters π and P). We maximize the lower bound
in Eqn 3 exactly by setting qtI to the posterior dis-
tribution of the data for the current values of the
parameters π (standard E step). Thus, we have

qtI ∝ p(s|I)p(I)

=
[ D∏
d=1

∏
z

(hid,z)
cdz(t)

][ D∏
d=2

Pid−1,id
] (4)

We do not need to explicitly compute the poste-
rior distribution qtI = p(I|s) at this point, but only
use it to compute the relevant expectation statistics
in the M-step. This can be done efficiently, as we



2271

see next. In the M-step, we consider qtI as fixed,
and maximize the objective in terms of the model
parameters π. Substituting this in Eqn 3, and focus-
ing on terms that depend on the model parameters
(π and P), we get

L(π,P) ≥
∑
t

∑
I
qtI log

(
p(s|I, π)p(I)) +Hq

=
∑
t

∑
I
qtI

(∑
d

∑
z

cdz(t) log hid,z

+
∑
d

logPid−1,id
)

=
∑
t

∑
I

EqtI
[∑

d

∑
z

Iitd=ic
d
z(t) log hid,z

]
+
∑
t

∑
I

EqtI
[∑

d

Iitd−1=i,itd=j logPij
]

(5)

Maximizing the likelihood w.r.t. P leads to the
following updates for the transition probabilities:2

Pij =
∑

t

∑
d P (i

t
d−1 = i, i

t
d = j)∑

t

∑
d P (i

t
d−1 = i)

(6)

Here, the pairwise state-probabilities P (itd−1 =
i, itd = j) for adjacent sentences in a sequence
can be efficiently calculated using the Forward-
Backward algorithm. In Equation 5, rewriting the
term containing h in terms of π using Eqn 1 (and
ignoring constant terms WxWy), we get:

∑
t

∑
I

EqtI
[∑

d

∑
z

Iitd=ic
d
z(t) log

∑
i′∈Wi

πi′,z

]
=
∑
t

∑
I

∑
d

P (itd = i)
∑
z

cdz(t) log
∑
i′∈Wi

πi′,z

(7)

The presence of a summation inside of a loga-
rithm makes maximizing this objective for π harder.
For this, we simply use Jensen’s inequality intro-
ducing an additional variational distribution (for
the latent grid positions within window Wi ), and
maximize the lower bound. The final M-step up-
date for π becomes:

πi,z ∝
(∑

t

∑
d

cdz(t)
∑

k|i∈Wk

P (itd = k)

hk,z

)
πi,z

(8)
2Since the optimal value for the concave problem∑

j yj log xj s.t.
∑
j xj = 1 occurs when x

∗
j ∝ yj

As before, the state-probabilities P (itd = i) can be
computed using the Forward Backward algorithm.

Intuitively, the expected alignments in the
E-step are distributions over sequences of positions
in the grid that best explain the structure of
documents for the current value of Sequential
CG parameters. In the M-step, we assume these
distributions embedding documents into various
parts of the grid as given, and update the multi-
nomial parameters and transition probabilities.
Modeling the transitions as having a Markov
property allows us to use a dynamic programming
approach (Forward Backward algorithm) to exactly
compute the posterior probabilities required for
parameter updates. We note that at the onset of
the procedure, we need to initialize π randomly to
break symmetries. Unless otherwise stated, in all
experiments, we run EM to 200 iterations.

Correlating space with sequential structure:
The use of histogram distributions h to generate
data forces smoothness in the model along XY
planes due to adjacent cells in the grid sharing a
large number of parameters that contribute to their
histograms (due to overlapping windows). On the
other hand, in order to induce spatial proximity in
the grid to mimic the sequential flow of discourse
in documents, we constrain the transition matrix
P (which specifies transition preferences from one
position in the grid to another) to a sparse banded
matrix. In particular, a position i = (ix, iy, iz) in
the grid can only transition to itself, its 4 neighbors
in the same XY plane, and two cells in the suc-
ceeding two layers along the Z-axis ( (ix, iy, iz+1)
and (ix, iy, iz+2)). This is enforced by fixing other
elements in the transition matrix to 0, and only
updating allowable transitions.

As an important note about implementation de-
tails, we observe here that the Forward-Backward
procedure (which is repeatedly invoked during
model training) can be naturally formulated in
terms of matrix operations.3 This allows training
for the Sequential CG approach to be scalable for
large document collections.

In our formulation, we have presented a Sequen-
tial CG model for a 3-D grid. This can be adapted
to learn 2-D grids (trellis) by setting Ey = 1. In
our experiments, we found 3-D grids to be better

3To explain, if fd1×G are forward probabilities for step
d, and Od+1G×G are observation probabilities for step d + 1,
fd+1 = fd ×P ×Od computes forward probabilities for the
next step in the sequence



2272

in terms of task performance and visualization (for
a comparable number of parameters).

3.2 Mixture model
The Sequential CG model described above can be
combined with other generative models (e.g., lan-
guage models) to get a mixture model. Here, we
show how a unigram language model can be com-
bined with Sequential CG. The rationale behind
this is that since the Sequential CG is primarily
designed to explain elements of documents that re-
flect sequential discourse structures, mixing with
a context-agnostic distribution can allow it to fo-
cus specifically on elements that reflect sequential
regularities. In experimental evaluation, we find
that such a mixture model shows distinctly differ-
ent behavior (see Section 4.1.1). Next, we briefly
describe updates for this approach.

Let µz denote the multinomial distribution over
features for the unigram model to be mixed with
the CG. Let βz be the mixing proportion for the
feature z, i.e. an occurrence of z is presumed to
come from the Sequential CG with probability βz ,
and from the unigram distribution with probability
1 − βz . Further, let αtz be binary variable that
denotes whether a particular instance of z comes
from the Sequential CG, or the unigram model.
Then, Equation 2 changes to:

p(s| I, α) =
∏
z,d

(
(hid,z)

cdzβz

)αtz(
µc

d
z
z (1−βz)

)1−αtz
Since we do not observe αtz (i.e., which distribu-
tion generated a particular feature in a particular
document), they are additional latent variables in
the model. Thus, we need to introduce a Bernoulli
variational distribution qαzt . Doing this modifies
relevant parts (containing qαzt) of Equation 5 to:

∑
t

∑
I
qtI

(∑
z

qαzt log
(
βz
∏
d

h
cdz(t)
id,z

)
+ (1− qαzt) log

((
1− βz)µ

∑
d c
d
z

z

)
+
∑
d

logPid−1,id
)
+Hqαzt

(9)

This leads to the following additional updates for
estimating qαzt (in the E-step)

4 and βz (in the M-
step).

4Since the optimal value for the concave problem∑
j xj log

yj
xj

s.t.
∑
j xj = 1 occurs when x

∗
j ∝ yj

qαzt =
exp

(∑I
i P (i

t
d=i)c

d
z(t) log hid,z

)
βz

exp

(∑I
i P (i

t
d=i)c

d
z(t) log hid,z

)
βz+µ

∑
d c
d
z

z (1−βz)

In the M-step, βz can be estimated simply as the
fraction of times z is generated from the Sequential
CG component.

βz =
∑
t qαzt∑
t Iz

4 Evaluation

In this section, we analyze the performance of
our approach on text collections from several do-
mains (including short stories, newswire text and
biographies). We first qualitatively evaluate our
generative method on a dataset of biographical ex-
tracts from Wikipedia, which visually illustrates
biographical trajectories learned by the model, op-
erationalizing our model concept from Figure 1 in
real data (see Figure 2). Next, we evaluate our
method on two standard tasks requiring document
understanding: story cloze evaluation and sentence
ordering. Since our method is completely unsu-
pervised and is not tailored to specific tasks, com-
petitive performance on these tasks would indicate
that the method learns helpful regularities in text
structure, useful for general-purpose language un-
derstanding.

4.1 Visualizing Wikipedia biographies
We now qualitatively explore models learned by
our method on a dataset of biographies from
Wikipedia.5 For this, we use the data previously
collected and processed by Bamman and Smith
(2014). In all, the original dataset consists of ex-
tracts from biographies of about 240,000 individu-
als. For ease of training, we trained our method on
a subset of the 50,0000 shortest documents from
this set. The original paper uses the numerical or-
der of dates mentioned in the biographies to extract
biographical templates, but we do not use this infor-
mation. Figure 2 visualizes a Sequential CG model
learned on this dataset for on a grid of dimensions
E = 8 × 8 × 5, and a histogram window W of
dimensions 3× 3 . In general, we found that using
larger grids leads to smoother transitions and learn-
ing more intricate patterns including hierarchies
of trajectories, but here we show a model with a

5For all our experimental evaluation, we tokenize and lem-
matize text using the Stanford CoreNLP pipeline, but retain
entity-names and contiguous text-spans representing MWEs
as single units



2273

Figure 2: Visualization of a Sequential-CG model with grid size of 8×8×5, trained on 50,000 documents
from the Wikipedia biographies dataset. Cells in the grid show words with highest probabilities (empty
cells may indicate that no word has a substantially higher probability than others).

smaller grid for ease of visualization. Here, the
words in each cell in the grid denote the highest
probability assignments in that cell. Larger fonts
within a cell indicate higher probabilities.

We observe that the method successfully extracts
various biographical trajectories, as well as capture
a notion of similarity between them. To explain,
we observe that the lower-right part of the learned
grid largely models documents about sportpersons
(with discernable regions focusing on sports like
soccer, American football and ice-hockey). On
the other hand, the left-half of the grid is domi-
nated by biographies of people from the arts and
humanities (inlcuding artists, writers, musicians,
etc.). The top-center of the grid focuses on aca-
demicians and scientists, while the top-right repre-
sents biographies of political and military leaders.
We note smooth transitions between different re-
gions, which is precisely what we would expect
from the use of the smoothing filter that incorpo-
rates parameter sharing across cells in the method.
Further, as we go across the layers in the figure,
we note the biographical trajectories learned by the
model across the entire grid. For example, from
the grid, the life trajectory of a football player can
be visualized as being drafted, signing and playing
for a team, and eventually becoming a head-coach
or a hall-of-fame inductee.

4.1.1 Effects of mixing
The Sequential-CG method can be combined with
other generative models in a mixture model, fol-

lowing the approach previously described in Sec-
tion 3.2. A major reason to do this might be to
allow the base model to handle general content,
while allowing the Sequential-CG method to focus
on modeling context-sensitive words only. Here,
we empirically characterize the mixing behavior
for different categories of words.

Figure 3 shows the mixing proportion of differ-
ent words when the Sequential-CG model is com-
bined with a unigram model. In the figure, the
X-axis corresponds to words in the dataset with
decreasing frequency of occurrence, whereas the Y-
axis denotes the mixing proportions βz learned by
the mixture model. We note that the mixture model
learns to explain frequent as well as the long-tail
of rare words using the simple unigram model (as
seen from low mixing proportion of Sequential-CG
method). These regimes correspond to (1) stop-
words and very common nouns, and (2) rare words
respectively. In turn, this allows the Sequential-
CG component to preserve more probability mass
to explain the intermediate content words. Thus,
the Sequential-CG component only needs to model
words that reflect useful statistical sequential pat-
terns, without expending modeling effort on back-
ground content (common words) or noise (rare
words). For the long tail of infrequent words, we
observe that Sequential CG is much more likely to
generate verbs and adjectives, rather than nouns.
This is as we would expect, since verbs and adjec-
tives often denote events and sentiments, which can



2274

Figure 3: Learned mixing proportion (βz) in the
mixture model of Section 3.2 for words of different
frequencies. βz denotes the probability of a word
being generated from the Sequential CG model
(rather than from the Unigram model). The Se-
quential CG learns to model content words (with
intermediate ranks), and conserves modeling effort
by avoiding modeling both very common words
(that occur across contexts), as well as rare words.

be important elements in discourse trajectories.

4.2 Story-cloze
We next evaluate our method on the story-cloze task
presented by Mostafazadeh et al. (2016), which
tests common-sense understanding in context of
children stories. The task consists of identifying
the correct ending to a four-sentence long story
(called context in the original paper) and two possi-
ble ending options. The dataset for the task consists
of a collection of around 45K unlabeled 5-sentence
long stories as well as 3742 5-sentence stories with
two provided endings, with one labeled as the cor-
rect ending. For this task, we train our method on
grids of dimension 15× 15× 6 (E), and histogram
windows W of size 5× 5 on the unlabeled collec-
tion of stories. At test time, for each story, we are
provided two versions (a story-version v consists
of the provided context c, followed by a possible
ending e1, i.e. v = [c, e] ). For prediction, we need
to define a goodness score Sv for a story-version.

In the simplest case, this score can simply be the
log-likelihood log pSCG(v) of the story-version, ac-
cording to the Sequential-CG model. However, this
is problematic since this is biased towards choos-
ing shorter endings. To alleviate this, we define the
goodness score by discounting the log-likelihood
by the probability of the ending e itself, under a

Accuracy
Our Method variants
Sequential CG + Unigram Mixture 0.602
Sequential CG + Brown clustering 0.593
Sequential CG + Sentiment 0.581
Sequential CG 0.589
Sequential CG (unnormalized) 0.531
DSSM 0.585
GenSim 0.539
Skip-thoughts 0.552
Narrative-Chain(Stories) 0.494
N-grams 0.494

Table 1: Performance of our approach on story-
cloze task from Mostafazadeh et al. (2016) com-
pared with other unsupervised approaches (accu-
racy numbers as reported in Mostafazadeh et al.
(2016)).

simple unigram model.

Sv = log pSCG(c, e)− log puni(e)

The predicted ending is the story-version with
a higher score. Table 1 shows the performance
of variants of our approach for the task. Our base-
lines include previous approaches for the same task:
DSSM is a deep-learning based approach, which
maps the context and ending to the same space,
and is the best-performing method in Mostafazadeh
et al. (2016). GenSim and N-gram return the end-
ing that is more similar to the context based on
word2vec embeddings (Mikolov et al., 2013) and
n-grams, respectively. Narrative-Chains computes
the probability of each alternative based on event-
chains, following the approach of Chambers and
Jurafsky (2008).

We note that our method improves on the pre-
vious best unsupervised methods for the task.
This is quite surprising, since our Sequential-CG
model in this case is trained on bag-of-lemma
representations, and only needs sentence segmen-
tation, tokenization and lemmatization for pre-
processing. On the other hand, approaches such
as Narrative-Chains require parsing and event-
recognition, while approaches such as GenSim re-
quire learning word embeddings on large text cor-
pora for training. Further, we note that predicting
the ending without normalizing for the probability
of the words in the ending results in significantly
weaker performance, as expected. We train another



2275

Figure 4: Illustrative story-cloze examples where
the model correctly identifies the appropriate end-
ing (model score in parentheses).

variant of Sequential-CG with the sentence-level
sentiment annotation (from Stanford CoreNLP)
also added as a feature. This does not improve per-
formance, consistent with findings in Mostafazadeh
et al. (2016). We also experiment with a variant
where we perform Brown clustering (Brown et al.,
1992) of words in the unlabeled stories (K = 500
clusters), and include cluster-annotations as fea-
tures for training the method. Doing this explicitly
incorporates lexical similarity into the model, lead-
ing to a small improvement in performance. Finally,
a mixture model consisting of the Sequential-CG
and a unigram language model leads to a further
improvement in performance. The performance of
our unsupervised approach on this task indicates
that it can learn discourse structures that are helpful
for general language understanding.

The story-cloze task has recently also been ad-
dressed as a shared task at EACL (Mostafazadeh
et al., 2017) with a significantly expanded dataset,
and achieving much higher performance. How-
ever, we note that the proposed best-performing ap-
proaches (Chaturvedi et al., 2017; Schwartz et al.,
2017) for this task are all supervised, and hence not
included here for comparison.

Figure 4 shows examples where the model cor-
rectly identifies the ending. These show a mix
of behavior such as sentiment coherence (iden-
tifying dissonance between ‘wonderful surprise’
and ‘stolen’) and modeling causation (police being
called after being suspected).

4.3 Sentence Ordering

We next evaluate our method on the sentence order-
ing task, which requires distinguishing an original

Accidents Earthquakes
Sequential CG 0.813 0.946
VLV-GM (2017) 0.770 0.931
HMM (2012) 0.822 0.938
HMM+Entity (2012) 0.842 0.911
HMM+Content (2012) 0.742 0.953
Discriminative approaches
DM (2017) 0.930 0.992
Recursive (2014) 0.864 0.976
Entity-Grid (2008) 0.904 0.872
Graph (2013) 0.846 0.635

Table 2: Performance of our approach on sentence
ordering dataset from Barzilay and Lapata (2008).

document from a version consisting of permuta-
tions of sentences of the original (Barzilay and
Lapata, 2008; Louis and Nenkova, 2012). For this,
we use two datasets of documents and their per-
mutations from Barzilay and Lapata (2008), which
are used as standard evaluation for coherence pre-
diction tasks. These consist of (i) reports of ac-
cidents from the National Transportation Safety
Bureau (we refer to this data as accidents), and (ii)
newswire reports about earthquake events from the
Associated press (we refer to this as earthquakes).
Each dataset consists of 100 training documents,
and about 100 documents for testing. Also pro-
vided are about 20 generated permutations for each
document (resulting in 1986 test pairs for accidents,
and 1955 test pairs for earthquakes). Documents
in accidents consist of between 6 and 19 sentences
each, with a median of 11 sentences. Documents in
earthquakes consist of between 4 and 30 sentences
each, with a median of 10 sentences.

Since the datasets for these tasks only have a
relatively small number of training documents (100
each), we use Sequential-CG with smaller grids
(3×3×15), and don’t train a mixture model (which
needs to learn a parameter βz for each word in the
vocabulary). Further, we train for a much smaller
number of iterations to prevent overfitting (K = 3,
chosen through cross-validation on the training set).
During testing, since provided article pairs are sim-
ply permutations of each other and identical in
content, we do not need to normalize as needed in
Section 4.2. The score of a provided article is sim-
ply calculated as its log-likelihood. The article with
higher likelihood is predicted to be the original.

Table 2 shows performance of the method com-
pared with other approaches for coherence predic-
tion. We note that Sequential-CG performs com-



2276

Figure 5: Example of newswire report about an
earthquake event. Bold fonts represent words that
align particularly well with the learned model at
corresponding points in the narrative.

petitively with the state-of-the-art for generative
approaches for the task, while needing no other
annotation. In comparison, the HMM based ap-
proaches use significant annotation and syntactic
features. Sequential-CG also outperforms several
discriminative approaches for the task. In Figure 5
we illustrate the learned discourse trajectories in
terms of the most salient features in each sentence.
Words in bold are those identified by the model
to be most context-appropriate at the correspond-
ing point in the narrative. This is done by ranking
words by the ratio between their probabilities (π:,z)
in the grid weighted by alignment locations of the
document (qtI), and unigram probabilities.

5 Conclusion

We have presented a simple model for extracting
and visualizing latent discourse structure from un-
labeled documents. The approach is coarse, and
does not have explicit models for important ele-
ments such as entities and events in a discourse.
However, the method outperforms some previous
approaches on document understanding tasks, even
while ignoring syntactic structure within sentences.
The ability to visualize learning is a key component
of our method, which can find significant applica-
tions in data mining and data-discovery in large text
collections. More generally, similar approaches
can explore a wider range of scenarios involving
sequences of text. While here our focus was on
learning discourse structures at the document level,
similar methods can also be used at other scales,
such as for syntactic or morphological analysis.

References
Nicholas Asher. 1986. Belief in discourse repre-

sentation theory. Journal of Philosophical Logic,
15(2):127–189.

David Bamman and Noah Smith. 2014. Unsupervised
discovery of biographical structure from text. Trans-
actions of the Association for Computational Lin-
guistics, 2:363–376.

Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1–34.

Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J Della Pietra, and Jenifer C Lai. 1992. Class-
based n-gram models of natural language. Compu-
tational linguistics, 18(4):467–479.

Nathanael Chambers and Daniel Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In ACL,
pages 789–797. The Association for Computer Lin-
guistics.

Snigdha Chaturvedi, Haoruo Peng, and Dan Roth.
2017. Story comprehension for predicting what hap-
pens next. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1603–1614.

Snigdha Chaturvedi, Shashank Srivastava, and Dan
Roth. 2018. ‘Where have I heard this story before?’
: Identifying narrative similarity in movie remakes.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.

Thomas L Griffiths, Mark Steyvers, David M Blei, and
Joshua B Tenenbaum. 2005. Integrating topics and
syntax. In Advances in neural information process-
ing systems, pages 537–544.

Camille Guinaudeau and Michael Strube. 2013. Graph-
based local coherence modeling. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 93–103.

Nebojsa Jojic and Alessandro Perina. 2011. Multidi-
mensional counting grids: Inferring word order from
disordered bags of words. In Proceedings of the
Twenty-Seventh Conference on Uncertainty in Arti-
ficial Intelligence, pages 547–556. AUAI Press.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. ACL 2013, page 119.

Quoc V. Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Pro-
ceedings of the 31th International Conference on
Machine Learning, ICML 2014, Beijing, China, 21-
26 June 2014, pages 1188–1196.

https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/371
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/371
http://dblp.uni-trier.de/db/conf/acl/acl2008.html#ChambersJ08
http://dblp.uni-trier.de/db/conf/acl/acl2008.html#ChambersJ08
http://jmlr.org/proceedings/papers/v32/le14.html
http://jmlr.org/proceedings/papers/v32/le14.html


2277

Jiwei Li and Eduard Hovy. 2014. A model of coher-
ence based on distributed sentence representation.
In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 2039–2048.

Jiwei Li and Dan Jurafsky. 2017. Neural net models
of open-domain discourse coherence. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 198–209.

Jiwei Li, Rumeng Li, and Eduard H Hovy. 2014.
Recursive deep models for discourse parsing. In
EMNLP, pages 2061–2069.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 997–1006. Association for Computational
Linguistics.

Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1157–1168. As-
sociation for Computational Linguistics.

William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text-Interdisciplinary Jour-
nal for the Study of Discourse, 8(3):243–281.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016. A cor-
pus and cloze evaluation for deeper understanding
of commonsense stories. In Proceedings of NAACL-
HLT, pages 839–849.

Nasrin Mostafazadeh, Michael Roth, Annie Louis,
Nathanael Chambers, and James Allen. 2017. LS-
DSem 2017 shared task: The story cloze test. In
Proceedings of the 2nd Workshop on Linking Mod-
els of Lexical, Sentential and Discourse-level Seman-
tics, pages 46–51, Valencia, Spain.

Karl Pichotta and Raymond J Mooney. 2015. Learning
statistical scripts with LSTM recurrent neural net-
works. In AAAI.

Roger C Schank and Robert P Abelson. 1977. Scripts,
plans, goals and understanding: an inquiry into hu-
man knowledge structures. Erlbaum.

Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila
Zilles, Yejin Choi, and Noah A Smith. 2017. The
effect of different writing tasks on linguistic style: A

case study of the roc story cloze task. arXiv preprint
arXiv:1702.01841.

Di Wang, Nebojsa Jojic, Chris Brockett, and Eric Ny-
berg. 2017. Steering output style and topic in neu-
ral response generation. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2140–2150, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Xuerui Wang and Andrew McCallum. 2006. Topics
over time: A non-markov continuous-time model
of topical trends. In Proceedings of the 12th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’06, pages 424–
433, New York, NY, USA. ACM.

http://aclweb.org/anthology/W17-0906.pdf
http://aclweb.org/anthology/W17-0906.pdf
https://doi.org/10.1145/1150402.1150450
https://doi.org/10.1145/1150402.1150450
https://doi.org/10.1145/1150402.1150450

