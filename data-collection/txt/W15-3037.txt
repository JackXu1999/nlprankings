



















































QUality Estimation from ScraTCH (QUETCH): Deep Learning for Word-level Translation Quality Estimation


Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 316–322,
Lisboa, Portugal, 17-18 September 2015. c©2015 Association for Computational Linguistics.

QUality Estimation from ScraTCH (QUETCH):
Deep Learning for Word-level Translation Quality Estimation

Julia Kreutzer and Shigehiko Schamoni
Computational Linguistics

Heidelberg University
69120 Heidelberg, Germany

{kreutzer,schamoni}@cl.uni-heidelberg.de

Stefan Riezler
Computational Linguistics & IWR

Heidelberg University
69120 Heidelberg, Germany

riezler@cl.uni-heidelberg.de

Abstract

This paper describes the system submit-
ted by the University of Heidelberg to the
Shared Task on Word-level Quality Esti-
mation at the 2015 Workshop on Statis-
tical Machine Translation. The submit-
ted system combines a continuous space
deep neural network, that learns a bilin-
gual feature representation from scratch,
with a linear combination of the manually
defined baseline features provided by the
task organizers. A combination of these
orthogonal information sources shows sig-
nificant improvements over the combined
systems, and produces very competitive
F1-scores for predicting word-level trans-
lation quality.

1 Introduction

This paper describes the University of Heidel-
berg submission to the Shared Task on Word-
level Quality Estimation (QE Task 2) at the
2015 Workshop on Statistical Machine Transla-
tion (WMT15). The task consists of predict-
ing the word-level quality level (“OK”/“BAD”) of
English-to-Spanish machine translations, without
the use of human references, and without insight
into the translation derivations, that is, by treat-
ing the Machine Translation (MT) system that pro-
duced the translations as a black box.

The task organizers provided training and de-
velopment data comprising tokenized MT outputs
that were automatically annotated for errors as edit
operations (replacements, insertions, or deletions)
with respect to human post-edits (Snover et al.,
2006). Furthermore, a set of 25 baseline features
that operate on source and target translation, but
do not use features of the SMT pipeline that pro-
duced the translations, was provided. Even though
the distribution of binary labels is skewed towards

“OK” labels, even more so than in the previous
QE task at WMT141, the most common approach
is to treat the problem as a supervised classifica-
tion task. Furthermore, most approaches rely on
manually designed features, including source and
target contexts, alignments, and generalizations by
linguistic categories (POS, syntactic dependency
links, WordNet senses) as reported by Bojar et al.
(2014), similar to the 25 feature templates pro-
vided by the organizers.

We apply the framework of Collobert et al.
(2011) to learn bilingual correspondences “from
scratch”, i.e. from raw input words. To this
aim, a continuous space deep neural network is
pre-trained by initializing the lookup-table with
distributed word representations (Mikolov et al.,
2013b), and fine-tuned for the QE classification
task by back-propagating word-level prediction er-
rors using stochastic gradient descent (Rumelhart
et al., 1986). Moreover, we train a linear combi-
nation of the manually defined baseline features
provided by the task organizers. A combination of
the orthogonal information based on the continu-
ous space features and the manually chosen base-
line features shows significant improvements over
the combined systems, and produces very compet-
itive F1 scores for predicting word-level transla-
tion quality.

2 Deep Learning for Quality Estimation

Continuous space neural network models are cred-
ited with the advantage of superior modeling
power by replacing discrete units such as words
or n-grams by vectors in continuous space, allow-
ing similar words to have similar representations,
and avoiding data sparsity issues. These advan-
tages have been demonstrated experimentally by
showcasing meaningful structure in vector space

1A factor of 4.22 on WMT15 train, and 4.21 on WMT15
dev, as opposed to 1.84 for WMT14 train and 1.81 for
WMT14 test for the same language pair.

316



representations (Mikolov et al. (2013c), Penning-
ton et al. (2014) inter alia), or by producing state-
of-the-art performance in applications such as lan-
guage modeling (Bengio et al. (2003), Mikolov et
al. (2010), inter alia) or statistical machine trans-
lation (Kalchbrenner and Blunsom (2013), Bah-
danau et al. (2015), inter alia). The property
that makes these models most attractive for vari-
ous applications is the ability to learn continuous
space representations “from scratch”(Collobert et
al., 2011), and to infuse the representation with
non-linearity. The deep layers of the neural net-
work capture these representations – even a single
hidden layer is sufficient (Hornik et al., 1989).

We present an approach to address the chal-
lenges of word-level translation quality estimation
by learning these continuous space bilingual rep-
resentations instead of relying on manual feature
engineering. While the neural network architec-
ture presented by Collobert et al. (2011) is lim-
ited to monolingual word-labeling tasks, we ex-
tend it to the bilingual context of QE. The multi-
layer feedforward neural network is pre-trained in
an unsupervised fashion by initializing the lookup-
table with word2vec representations (Mikolov et
al., 2013b). This is not only an effective way of
guiding the learning towards minima that still al-
low good generalization in non-convex optimiza-
tion (Bengio, 2009; Erhan et al., 2010), but it also
proves to yield considerably better results in our
application. In addition, we train a linear com-
bination of the manually defined baseline features
provided by the task organizers. We combine these
orthogonal information sources and find signifi-
cant improvements over each individual system.

3 QUETCH

Our QUality Estimation from scraTCH
(QUETCH) system is based on a neural net-
work architecture built with Theano (Bergstra
et al., 2010). We design a multilayer percep-
tron (MLP) architecture with one hidden layer,
non-linear tanh activation functions and a lookup-
table layer as proposed by Collobert et al. (2011).
The lookup-table has the function of mapping
word to continuous vectors and is updated during
training. Figure 1 illustrates the connections
between the input, hidden lookup-table and linear
layer, and the output.

Training is done by optimizing the log-
likelihood of the model given the training data

Input Layer

Hidden Layers

Lookup-Table
Layer

Linear Layer
+ non-linear
transformation

Output Layer

target
source

t1 t2 t3 t4 t5 s1 s2 s3 s4 s5

t2  t3  t4 s3  s4  s5

align

|V|

dwrd

concatenate

tanh(W1   +b1)

tanh(W2   +b2)

"OK"     "BAD"

Figure 1: Neural network architecture for predict-
ing word-level translation quality given aligned
source and target sentences. The lookup-table ma-
trix M contains dwrd-dimensional vectors for each
word in the vocabulary V. In this example, the con-
text window sizes |winsrc| and |wintgt| are set to
three and the target word t3 is classified “OK”.

via back-propagation and stochastic gradient de-
scent (Rumelhart et al., 1986). Trainable param-
eters are the bias vectors (b1,b2) and weight ma-
trices (W1,W2) of the linear layers and the matrix
M ∈ Rdwrd×|V | that represents the lookup-table.
Tunable hyper-parameters are the number of units
of the hidden linear layer, the lookup-table dimen-
sionality dwrd and the learning rate. The number
of output units is set to two, since the QE task 2
requires binary classification. The softmax over
the activation of these output units is interpreted
as score for the two classes.

3.1 Bilingual Representation Learning
Given a target word, we consider bilingual con-
text information: From the target sentence we ex-
tract a fixed-size word window wintgt centered at
the target word. From the aligned source sentence
we extract a fixed-size word window winsrc cen-
tered at a position that is either estimated heuristi-
cally or via word alignments. Concatenating target
and source windows, we obtain a bilingual context
vector for a given target word. This context vector
is the input for the lookup-table layer, which maps

317



each context word to a dwrd-dimensional vector2.
All lookup-table output vectors are concatenated
to form the input to the MLP hidden layer. Since
the lookup-table representations of words are up-
dated during training, QUETCH learns represen-
tations of words in bilingual contexts that are op-
timized for QE.

3.2 Unsupervised Pre-training

Usually, the parameters of a neural network are
initialized with zeros or random numbers, i.e. no
a-priori knowledge is captured in the network.
However, the learning process can benefit from
knowledge that is encoded into the architecture
prior to training (Saxe et al., 2011). In case of
QE, we want the model to know what well-written
source and target sentences look like – before actu-
ally seeing translations. word2vec (Mikolov et
al., 2013b; Mikolov et al., 2013a) offers efficient
methods to pre-train word representations in an
unsupervised fashion such that they reflect word
similarities and relations. Initializing the lookup-
table with pre-trained word2vec vectors allows
us to incorporate prior linguistic knowledge about
source and target language into QUETCH. During
the learning process, these representations are fur-
ther optimized for QE and the vocabulary encoun-
tered during training.

4 Baseline Features and System
Combination

In contrast to word-based quality estimation tasks
from previous years, this year’s data addition-
ally provides a number of baseline features. A
straightforward approach would be to integrate
the baseline features in the deep learning sys-
tem on the same level as word-features and train
lookup-tables for each feature class (Collobert et
al., 2011). While this certainly works for word-
similar features like POS-tags, this is not suitable
for continuous numerical features. Preliminary
tests of extending QUETCH with a lookup-table
for POS-tags did not result in better F1 scores.
Also, training took considerably longer, because
of (1) the additional lookup-table to train and (2)
the larger dimensionality of the vector represent-
ing a target word with its context. If we added all

2All words are indexed within a vocabulary V . The vo-
cabulary contains the entire training, development and test
data of the QE task and is realized as a gensim dictionary
(Řehůřek and Sojka, 2010).

25 features for each target word in the context win-
dow, the input to the first linear layer would grow
by 25 ∗ |wintgt| ∗ dwrd dimensions.

For these reasons, we decided to design a sys-
tem combination that treats the QUETCH system
and the baseline features individually and inde-
pendently. For many complex applications, sys-
tem combination has proven to be effective strat-
egy to boost performance. In machine transla-
tion tasks, Heafield and Lavie (2011) and Karakos
et al. (2008), inter alia, increased overall perfor-
mance by cleverly combining the outputs of sev-
eral MT systems. In cross-lingual information re-
trieval, Schamoni and Riezler (2015) empirically
showed that it is more beneficial to combine sys-
tems that are most dissimilar than those that have
highest single scores.

Our approach is to train separate systems, one
based on the deep learning approach described in
Section 3, and one based solely on the baseline
features provided for the shared task. In a final
step, we combine both systems together with bina-
rized versions of selected baseline features. From
this modular combination of both systems, we can
furthermore gain knowledge about their individual
contribution to the combined system which will
help to understand their usefulness for the QE task.

4.1 Baseline Features System
To obtain a system for baseline features that is
most complementary to QUETCH, we used the
Vowpal Wabbit (VW) toolkit (Goel et al., 2008) to
train a linear classifier, i.e. a single-layer percep-
tron. We built new features by “pairing” baseline
features, thus we quadratically expand the feature
space and learn a weight for each possible pair.

Assuming two feature vectors p ∈ {0, 1}P and
q ∈ {0, 1}Q of sizes P and Q where the nth di-
mension indicates the occurrence of the nth fea-
ture, we define our linear model as

f(p,q) = p>Wq =
P∑

i=1

Q∑
j=1

piWijqj ,

where W ∈ RP×Q encodes a feature matrix (Bai
et al., 2010; Schamoni et al., 2014). The value
of f(·, ·) is the prediction of the classifier given a
target vector p and a vector of related features q.

To address the problem of data sparsity, we
reduced the number of possible feature pairs by
restricting the feature expansion to two groups:
(1) target words are combined with target context

318



words and source aligned words, and (2) target
POS tags are combined with source aligned POS
tags. In total, we observed 3.5M different features
during training of the VW model.

4.2 System Combination

For the final system combination, we reused the
VW toolkit. The combined systems comprises 82
features: the QUETCH-score, the VW-score, and
the remaining 80 features are binary features de-
rived from the baseline feature set. The QUETCH-
score is the system’s prediction combined with its
likelihood, for VW we directly utilize the raw pre-
dictions with clipping at ±1. Binarized features
were inserted to enrich the classifier with addi-
tional non-linearity. They consist of (1) the bi-
nary features from the baseline feature set, and
(2) binned versions of the numerical features from
the same set. For small groups of discrete val-
ues we assigned a binary feature to each possible
value, for larger groups and real-valued features
we heuristically defined intervals (“bins”) contain-
ing roughly the same number of instances. The in-
tegration of the single components for the system
combination is illustrated in Figure 2.

T

Baseline Features

QUETCH

q

Context, POS

binarized

w     + b

“OK” / “BAD”

W

p

target

source

Systems

Combination

Figure 2: Architecture of the QUETCHPLUS sys-
tem combination.

5 Experiments

5.1 WMT14
We first ran experiments on the WMT14 task 2
data to compare QUETCH’s performance with the
WMT14 submissions. With outlook to this year’s
task we considered only the binary classification
task where words are labeled either ”BAD” or
”OK”.

In contrast to the WMT15 data, the WMT14’s
data covers not only English to Spanish transla-
tions (en-es) but also German to English (de-en)
and vice versa. Since the plain QUETCH sys-
tem does not rely on language-specific features,
we simply use the same deep learning architecture
for all of these language pairs.

QUETCH is trained on the WMT14 training
set, with a source and target window size of 3,
a lookup-table dimensionality of 10, 300 hid-
den units, and a constant learning rate of 0.001.
Test and training data were lowercased. The
alignments used for positioning the target win-
dow as described in Section 3.1 were created with
fast align from the cdec toolkit (Dyer et al.,
2010). The collection of corpora provided with
WMT13’s translation task3 is utilized as source for
unsupervised pre-training: Europarl v7 (Koehn,
2005), Common Crawl corpus, and News Com-
mentary. Note that we did not use these corpora
because of their parallel structure, but because
they are large, multilingual, and are commonly
used in WMT submissions.

Following the WMT14 evaluation (Bojar et al.,
2014), we report on accuracy and BAD F1-score,
the latter being the task’s primary evaluation met-
ric. The WMT14 baselines trivially predict either
only BAD or only OK labels. Table 1 presents the
best F1-scores during training and the according
accuracies for QUETCH under different configu-
rations.

The plain QUETCH system yields an accept-
able accuracy, but the BAD F1-scores are not com-
petitive. Adding alignment information further
improves the accuracy for all language pairs but
de-en. It improves the F1-score only for es-en
and en-de, which indicates that the model is still
prone to local optima. It is in fact pre-training that
boosts the BAD F1-score – this initial positioning
in the parameter space appears to have a larger im-
pact on the training outcome than the introduction

3http://www.statmt.org/wmt13/
translation-task.html

319



configuration BAD F1 Accuracy
en

-e
s

(v) 0.4378 0.5087
(a) 0.4164 0.5107
(p) 0.5206 0.4026

(a), (p) 0.5228 0.4196

es
-e

n

(v) 0.2197 0.7604
(a) 0.2470 0.7749
(p) 0.3203 0.8051

(a), (p) 0.3396 0.8076

en
-d

e

(v) 0.3743 0.6090
(a) 0.4197 0.6381
(p) 0.4684 0.6060

(a), (p) 0.4863 0.6271

de
-e

n

(v) 0.2482 0.7001
(a) 0.2426 0.6837
(p) 0.3734 0.6657

(a), (p) 0.3791 0.6792

Table 1: QUETCH results on WMT14 task 2 test data un-
der different configurations: (v)anilla system, (p)retraining
of word embeddings, (a)lignments from an SMT system.

of translation knowledge via alignments. How-
ever, we can achieve further improvement when
combining both pre-training and alignments. As
a result, QUETCH outperforms the official win-
ning systems of the WMT14 QE task (see Table
2) and the trivial baselines for all language pairs.
The fact that the overall tendencies are consistent
across languages proves that QUETCH is capable
of language-independent quality estimation.

submission BAD F1 Acc.

en-es FBK-UPV-UEDIN/RNN 0.4873 0.6162
es-en RTM-DCU/RTU-GLMd 0.2914 0.8298
en-de RTM-DCU/RTU-GLM 0.4530 0.7297
de-en RTM-DCU/RTU-GLM 0.2613 0.7614

Table 2: Winning submissions of the WMT14 Quality Esti-
mation Task 2 (Bojar et al., 2014).

5.2 WMT15

With the insights from the experiments on the
WMT14 data we proceed to the experiments on
the WMT15 en-es data. We introduce a weight
w for BAD training samples, such that QUETCH
is trained on each BAD sample w times. In this
way, we easily counterbalance the skewed distri-
bution of labels, without modifying the classifier’s
loss function. Also, we utilize the larger and non-
parallel Wikicorpus (Reese et al., 2010) in English

and Spanish for pre-training. As described in Sec-
tion 1, 25 baseline features are supplied with train-
ing, development and test data. This allows us to
evaluate the approach for system combination in-
troduced in Section 4.

During training of the VW-system, we ex-
perimented with various loss functions (hinge,
squared, logistic) and found the model trained on
squared loss to return the highest accuracy. Un-
wanted collisions in VW’s hashed weight vector
were reduced by increasing the size of the hash to
28 bits. To prevent the model from degenerating
towards OK-labels, we utilized VW’s option to set
the weight for each training instance individually
and increased the weights of the BAD-labeled in-
stances to 4.0.

The VW-system and the system combination
were trained in a 10-fold manner, i.e. the VW-
system was trained on 9 folds and the weights for
system combination were tuned on the 10th fold of
the training data. The final weights of the model
for evaluation were averaged among all 10 folds.

Table 3 presents the results on the WMT15
data for both QUETCH, the baseline feature VW
model, and the system combination referred to
as QUETCH+. The QUETCH results were pro-
duced under the same parameter conditions as in
the WMT14 experiments, and the newly intro-
duced w is set to 2 for the submitted and the com-
bined model, and 5 for another model that was ex-
plicitely designed for a high BAD F1-score.

configuration BAD F1 Accuracy

Q
U

E
T

C
H

(v) 0.2535 0.7104
(a) 0.2628 0.7099
(p) 0.2535 0.7668

(a), (p) 0.2793 0.7716
†(a), (p), (w) 0.3527 0.7508
(a), (p), (w) 0.3876 0.6031
‡(a), (p), (w) 0.2985 0.7888
‡VW 0.4084 0.7335

†QUETCH+ 0.4305 0.6977

Table 3: QUETCH results on en-es WMT15 task 2 test
data under different configuration setting: (v)anilla model
vs. models using (p)re-training, (a)lignments from an SMT-
System, and (w)eighting of the BAD-instances. Submit-
ted systems are preceded by †, components of the final
QUETCH+ system are marked with ‡.

Although proceeding in the same manner as in
the WMT14 experiments, we see slightly different
tendencies here: Adding alignments has a positive

320



effect on the BAD F1-score, whereas pre-training
improves mainly the accuracy. Still, the combina-
tion of both yields both a high BAD F1-score and a
high accuracy, which indicates that QUETCH suc-
ceeds in integrating both contributions in a com-
plementary way. Adding BAD weights further-
more improves the BAD F1-score, yet losing some
accuracy. Further increasing the weight up to 5
strengthens this effect, such that we obtain a model
with very high BAD F1-score, but rather low ac-
curacy.

The stand-alone VW model yields gener-
ally higher BAD F1-score, but does not reach
QUETCH’s accuracy. To enhance the orthogonal-
ity of the two models for combination, we select
a QUETCH model with extremely high accuracy
for the system combination4. Interestingly, the
system combination appears to profit from both
models, resulting in the overall best BAD F1-
score. The resulting VW weights of 1.188 for
QUETCH and 0.951 for VW underline each sys-
tem’s contribution. The next most important fea-
tures for the combination were pseudo reference
and is proper noun with weights of 0.2208 and
0.1557, respectively.

system BAD F1 OK F1 All F1
baseline 0.1678 0.8893 0.7531
QUETCH 0.3527 0.8456 0.7526
QUETCH+ 0.4305 0.7942 0.7256
UAlacant/OnLine-SBI-Baseline 0.4312 0.7807 0.7147

Table 4: Official test results on WMT15 task 2 for word level
translation quality. The All F1-score is the weighted aver-
age of BAD F1 and OK F1, where the weights are deter-
mined by the frequency of the classes in the test data. The
UAlacant/OnLine-SBI-Baseline and the QUETCH+ predic-
tions show no significant difference at p=0.05 and are both
announced official winners.

Table 4 shows the final test results on the
WMT15 task 2 for the main evaluation metric
of F1 for predicting BAD word level translation
quality, the F1 for predicting OK translations and
their weighted average. Both submitted systems,
QUETCH and QUETCH+, yield considerable im-
provements over the baseline. The QUETCH+
system that combines the neural network with the
linearly weighted baseline features is nominally

4We observe that the training process first produces high
BAD F1-score models, then further improves the accuracy
whilst slowly decreasing the BAD F1-score. This is due to
the fact that we do not optimize on the BAD F1-score di-
rectly, but the log-likelihood of the data, which is skewed to-
wards the OK label. This behavior allows us to select models
with individual trade-offs between BAD F1-score and accu-
racy at different stages of training.

outperformed by one other system by 0.07% BAD
F1 points, but their difference is not significant at
p=0.05.

6 Conclusion

We successfully applied a continuous space deep
neural network to the task of quality estimation.
With QUETCH we built a language-independent
neural network architecture that learns representa-
tions for words in bilingual contexts from scratch.
Furthermore we showed how this architecture ben-
efits from unsupervised pre-training on large cor-
pora. Winning the WMT15 QE task we found
evidence that the combination of such a contin-
uous space deep model with a discrete shallow
model benefits from their orthogonality and pro-
duces very competitive F1-scores for quality es-
timation. Further work will address the transfer
to sentence-based predictions and the introduction
of convolution and recurrence into the neural net-
work architecture.

Acknowledgments.

This research was supported in part by DFG
grant RI-2221/1-2 “Weakly Supervised Learning
of Cross-Lingual Systems”.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the International Conference on Learning Represen-
tations (ICLR), San Diego, CA.

Bing Bai, Jason Weston, David Grangier, Ronan Col-
lobert, Kunihiko Sadamasa, Yanjun Qi, Olivier
Chapelle, and Kilian Weinberger. 2010. Learning
to rank with (a lot of) word features. Information
Retrieval Journal, 13(3):291–314.

Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic
language model. Joural of Machine Learning Re-
search, 3:1137–1155.

Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and trends R© in Machine Learning,
2(1):1–127.

James Bergstra, Olivier Breuleux, Frédéric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), Austin, TX.

321



Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, et al. 2014. Findings of the 2014
workshop on statistical machine translation. In Pro-
ceedings of the Ninth Workshop on Statistical Ma-
chine Translation (WMT), Baltimore, MD.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, Uppsala, Sweden.

Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? The Journal of Machine Learn-
ing Research, 11:625–660.

Sharad Goel, John Langford, and Alexander L. Strehl.
2008. Predictive indexing for fast search. In Ad-
vances in Neural Information Processing Systems
(NIPS), Vancouver, Canada.

Kenneth Heafield and Alon Lavie. 2011. CMU sys-
tem combination in WMT 2011. In Proceedings
of the EMNLP 2011 Sixth Workshop on Statistical
Machine Translation, Edinburgh, Scotland, United
Kingdom.

Kurt Hornik, Maxwell Stinchcombe, and Halber
White. 1989. Multilayer feedforward networks are
universal approximators. Neural Networks, 2:359–
366.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), Seattle, WA.

Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation
system combination using itg-based alignments. In
Proceedings of ACL-08: HLT, Short Papers, Colum-
bus, Ohio.

Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings of
Machine Translation Summit X, Phuket, Thailand.

Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Pro-
ceedings of Interspeech, Makuhari, Chiba, Japan.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR, Scottsdale, AZ.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed represen-
tations of words and phrases and their composition-
ality. In Advances in neural information processing
systems (NIPS), Lake Tahoe, CA.

Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT), Atlanta, Geor-
gia.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Doha, Qatar.

Samuel Reese, Gemma Boleda, Montse Cuadros, Llus
Padr, and German Rigau. 2010. Wikicorpus: A
word-sense disambiguated multilingual wikipedia
corpus. In Proceedings of 7th Language Resources
and Evaluation Conference (LREC), La Valleta,
Malta.

Radim Řehůřek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, Valletta,
Malta.

David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323.

Andrew Saxe, Pang W Koh, Zhenghao Chen, Maneesh
Bhand, Bipin Suresh, and Andrew Y Ng. 2011. On
random weights and unsupervised feature learning.
In Proceedings of the 28th International Conference
on Machine Learning (ICML), Bellevue, WA.

Shigehiko Schamoni and Stefan Riezler. 2015.
Combining Orthogonal Information in Large-Scale
Cross-Language Information Retrieval. In Proceed-
ings of the 38th Annual ACM SIGIR Conference (SI-
GIR), Santiago, Chile.

Shigehiko Schamoni, Felix Hieber, Artem Sokolov,
and Stefan Riezler. 2014. Learning translational
and knowledge-based similarities from relevance
rankings for cross-language retrieval. In Proceed-
ings of the Association of Computational Linguistics
(ACL), Baltimore, MD.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA), Cambridge, MA.

322


