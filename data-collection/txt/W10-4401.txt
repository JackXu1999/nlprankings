




































Incremental Parsing in Bounded Memory

William Schuler

Department of Lingustics

The Ohio State University

schuler@ling.osu.edu

Abstract

This tutorial will describe the use of a fac-

tored probabilistic sequence model for parsing

speech and text using a bounded store of three

to four incomplete constituents over time, in

line with recent estimates of human short-

term working memory capacity. This formu-

lation uses a grammar transform to minimize

memory usage during parsing. Incremental

operations on incomplete constituents in this

transformed representation then define an ex-

tended domain of locality similar to those de-

fined in mildly context-sensitive grammar for-

malisms, which can similarly be used to pro-

cess long-distance and crossed-and-nested de-

pendencies.

1 Introduction

This paper will describe the derivation of a fac-

tored probabilistic sequence model from a proba-

bilistic context-free grammar (PCFG). The result-

ing sequence model can incrementally parse input

sentences approximately as accurately as a bottom-

up CKY-styple parser, incrementally estimating the

contents of a bounded memory store of intended

constituents, consisting of only three to four work-

ing memory elements, in line with recent esti-

mates of human short-term working memory capac-

ity (Cowan, 2001). The detailed derivation of this

model is intended to illustrate how probabilistic de-

pendencies from an original PCFG (or other types of

syntax-derived dependencies) can be preserved in a

processing model with human-like working memory

constraints.

1.1 Notation

This paper will associate variables for syntactic cate-

gories c, trees or subtrees τ , and string yields x̄ with
constituents in phrase structure trees, identified us-

ing subscripts that describe the path from the root of

the tree containing this constituent to the constituent

itself. These paths may consist of left branches (in-

dicated by ‘0’s in the path) and right branches (in-

dicated by ‘1’s), concatenated into sequences η (or ι
or κ). Thus, if a path η identifies a constituent, that
constituent’s left child would be identified by η0,
and that constituent’s right child would be identified

by η1. The empty path ² will be sued to identify the
root of a tree.

The probabilistic parsers defined here will also

use an indicator function J⋅K to denote deterministic
probabilities: JφK = 1 if φ is true, 0 otherwise.

2 Parsing

For a phrase structure subtree rooted at a constituent

of category cη with yield x̄η (a sequence of individ-
ual words x), the task of parsing will require the
calculation of the inside likelihood probability or

Viterbi (best subtree) probabilities. When the do-

main X of words x is a subset of the domain C of
category labels c, these can be calculated using rule
probabilities in a probabilistic context-free grammar

(PCFG) model θG, notated:

PθG(cη → cη0 cη1) = PθG(cη0 cη1 ∣ cη) (1)
Any yield x̄η can be decomposed into prefix x̄η0

and suffix x̄η1 yields:

x̄η = x̄η0x̄η1 (2)

Incremental Parsing in Bounded Memory

1



Therefore, inside likelihood probabilities can be de-

fined by marginalizing over all such decomposi-

tions:

PθIns(G)(x̄η ∣ cη) =⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩

if ∣x̄η ∣ = 1 ∶ Jx̄η = cηK
otherwise ∶ ∑

x̄η0cη0,x̄η1cη1

PθG(cη → cη0 cη1)⋅PθIns(G)(x̄η0 ∣ cη0)⋅PθIns(G)(x̄η1 ∣ cη1)
(3)

and Viterbi scores (the probability of the best tree)

can be defined by maximizing over all such decom-

positions:

PθVit(G)(x̄η ∣ cη) =⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩

if ∣x̄η ∣ = 1 ∶ Jx̄η = cηK
otherwise ∶ max

x̄η0cη0,x̄η1cη1
PθG(cη → cη0 cη1)⋅PθVit(G)(x̄η0 ∣ cη0)⋅PθVit(G)(x̄η1 ∣ cη1)

(4)

From these, it is possible to obtain the probability of

a sentence x̄²:

P(x̄²) =∑
c²

PθIns(G)(x̄² ∣ c²) ⋅ P(c²) (5)
and the most likely tree:

τ̂² = argmax
τ²

PθVit(G)(x̄² ∣ cτ²) ⋅ P(cτ²) (6)
3 Incremental Parsing using Incomplete

Constituents

Note that any prefix of a yield x̄η can be rewritten
as an ‘incomplete yield’ consisting of the complete

yield lacking some suffix of that yield x̄ηι:

x̄η = (x̄η/x̄ηι)x̄ηι (7)
It is therefore possible to decompose any inside like-

lihood or Viterbi probability into two parts: first, an

‘incomplete constituent’ probability of generating

this incomplete yield x̄η/x̄ηι along with an awaited
constituent of category cηι, given an active con-
stituent of category cη; and second, an ordinary in-
side or Viterbi probability (or ‘complete constituent’

probability) of generating x̄ηι given cηι:

PθIns(G)(x̄η ∣ cη) = ∑
ι∈1+,x̄ηι,cηι

PθIC(G)(x̄η/x̄ηι, cηι ∣ cη)
⋅ PθIns(G)(x̄ηι ∣ cηι) (8)

This decomposition can be represented graphically

as a transformation of the structure of a recurrence

from the standard PCFG recurrence above, corre-

sponding to PCFG dependencies, to one involv-

ing incomplete constituents (in particular, this will

transform the end of a right-expanding sequence into

the beginning of a left-expanding sequence of in-

complete constituents):

cη

x̄η/x̄ηι cηι
x̄ηι

⇒

cη

cη/cηι
x̄η/x̄ηι

cηι

x̄ηι

(9)

This decomposition gives incomplete constituent

probabilities that can then be decomposed into other

incomplete constituent probabilities (this will trans-

form the middle of a right-expending sequence into

the middle of a left-expanding sequence of incom-

plete constituents):

PθIC(G)(x̄η/x̄ηι1, cηι1 ∣ cη)
= ∑

x̄ηι,cηι

PθIC(G)(x̄η/x̄ηι, cηι ∣ cη)
⋅ PθIC(G)(x̄ηι/x̄ηι1, cηι1 ∣ cηι) (10)

= ∑
x̄ηι,cηι

PθIC(G)(x̄η/x̄ηι, cηι ∣ cη)
⋅ ∑
x̄ηι0,cηι0

PθG(cηι → cηι0 cηι1)⋅ PθIns(G)(x̄ηι0 ∣ cηι0) (11)
represented graphically:

cη

x̄η/x̄ηι cηι
cηι0

x̄ηι0

cηι1

x̄ηι1

⇒

cη

cη/cηι1
cη/cηι
x̄η/x̄ηι

cηι0

x̄ηι0

x̄ηι1
(12)

or into the product of a grammar rule probability and

an inside probability at the end of a sequence of such

decompositions (this will transform the beginning of

a right-expanding sequence into the end of a left-

expanding sequence of incomplete constituents):

PθIC(G)(x̄η/x̄η1, cη1 ∣ cη) =
∑

x̄η0,cη0

PθG(cη → cη0 cη1) ⋅ PθIns(G)(x̄η0 ∣ cη0) (13)

William Schuler

2



represented graphically:

cη

cη0

x̄η0

cη1

x̄η1

⇒

cη

cη/cη1
cη0

x̄η0

x̄η1
(14)

Essentially this transformation turns right-

expanding sequences of constituents (with sub-

script addresses ending in ‘1’) into left-expanding

sequences of incomplete constituents, which can

be composed together as they are recognized incre-

mentally from left to right. The decompositions of

Equations 8–13, taken together, are a model-based

right-corner transform (Schuler, 2009).

This transformation can then be defined on depth-

specific rules, allowing the sequence model to keep

track of a bounded amount of center-embedded

phrase structure.

PθIns(G),d(x̄η ∣ cη) = ∑
ι∈1+,x̄ηι,cηι

PθIC(G),d(x̄η/x̄ηι, cηι ∣ cη)
⋅ PθIns(G),d(x̄ηι ∣ cηι) (15)

PθIC(G),d(x̄η/x̄ηι1, cηι1 ∣ cη)
= ∑

x̄ηι,cηι

PθIC(G),d(x̄η/x̄ηι, cηι ∣ cη)
⋅ PθIC(G),d(x̄ηι/x̄ηι1, cηι1 ∣ cηι) (16)

= ∑
x̄ηι,cηι

PθIC(G),d(x̄η/x̄ηι, cηι ∣ cη)
⋅ ∑
x̄ηι0,cηι0

PθG-R,d(cηι → cηι0 cηι1)
⋅ PθIns(G),d+1(x̄ηι0 ∣ cηι0) (17)

PθIC(G),d(x̄η/x̄η1, cη1 ∣ cη) =
∑

x̄η0,cη0

PθG-L,d(cη → cη0 cη1) ⋅ PθIns(G),d(x̄η0 ∣ cη0)
(18)

Estimating probabilities for the beginning and

ending of these transformed left-expanding se-

quences will require the estimation of expected

counts for repeated recursive decompositions of yet-

unrecognized incomplete constituents according to

Equation 8, marginalizing over values of x̄. Since
left children and right children are decomposed dif-

ferently, the expected counts θG-RL∗,d will use PCFG
probabilities θG-R,d and θG-L,d that are conditioned

on whether the expanding constituent is a right or

left child, and on the center-embedding depth d of
the expanding constituent. The expected counts

θG-RL∗,d are of constituent categories cηι anywhere in
the left progeny of a right child of category cη:

EθG-RL∗,d
(cη 0→ cη0 ...) =∑

cη1

PθG-R,d(cη → cη0 cη1)
(19)

EθG-RL∗,d
(cη k→ cη0k0 ...) = ∑

c
η0k

EθG-RL∗,d
(cη k−1→ cη0k ...)

⋅ ∑
c
η0k1

PθG-L,d(cη0k → cη0k0 cη0k1) (20)

EθG-RL∗,d
(cη ∗→ cηι ...) = ∞∑

k=0

EθG-RL∗,d
(cη k→ cηι ...)

(21)

EθG-RL∗,d
(cη +→ cηι ...) = EθG-RL∗,d(cη ∗→ cηι ...)

− EθG-RL∗,d(cη 0→ cηι ...)
(22)

In practice the infinite sum is estimated to some con-

stant K using value iteration (Bellman, 1957).

These expected counts can then be used to calcu-

late left progeny probabilities:

PθG-RL∗,d
(cη ∗→ cηι ...) = EθG-RL∗,d(cη

∗
→ cηι ...)

∑cηι EθG-RL∗,d(cη ∗→ cηι ...)
(23)

which can be used to calculate forward or Viterbi

probabilities for all incomplete constituents in the

memory store, along with their yields:

PθFwd((x̄ηd/x̄ηdιd1)Dd=1, (cηd)Dd=1, (cηdιd1)Dd=1) =
D∏
d=1

PθG-RL∗,d
(cηd−1ιd−11 ∗→ cηd ...)

⋅ PθIC(G)(x̄ηd/x̄ηdιd1, cηdιd1 ∣ cηd) (24)
Putting all the pieces together, probabilities for

stores of incomplete constituents can now be de-

fined in terms of transitions from possible previ-

ous stores of incomplete constituents (set apart by

parentheses in the equation below) and reductions of

incomplete constituents and terminal symbols into

complete constituents (set apart by square brackets).

These transitions either:

Incremental Parsing in Bounded Memory

3



• add a layer of structure below an awaited con-

stituent at some depth level (the first case be-

low),

• add a layer of structure to the top of an active

constituent at some depth level (the second case

below), or

• carry forward the probability of an incomplete

constituent at a depth at which no transition

takes place (the third case below):

PθFwd((x̄ηd/x̄ηdιd1)Dd=1, (cηd)Dd=1, (cηdιd1)Dd=1) =

D∏
d=1

⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩

if cηd+1 = x̄ηd+1 , ιd ≠ ² ∶∑x̄ηdιd ,cηdιd
⋅ ⎛⎝
PθG-RL∗,d

(cηd−1ιd−11 ∗→ cηd ...)⋅PθIC(G),d(x̄ηd/x̄ηdιd , cηdιd ∣ cηd)
⎞
⎠

⋅∑x̄ηdιd0,cηdιd0∑κ s.t. x̄ηdιd0κ=cηdιd0κ⎛
⎝
PθG-RL∗,d+1

(cηdιd ∗→ cηdιd0 ...)⋅PθIC(G),d+1(x̄ηdιd0/x̄ηdιd0κ, cηdιd0κ ∣ cηdιd0)
⎞
⎠

⋅
⎡⎢⎢⎢⎢⎢⎢⎣

Pθ
G-RL∗,d+1

(cηdιd
0
→cηdιd0 ...)

Pθ
G-RL∗,d+1

(cηdιd
∗

→cηdιd0 ...)⋅Jx̄ηdιd0κ = cηdιd0κK

⎤⎥⎥⎥⎥⎥⎥⎦
PθG-R,d

(cηdιd→cηdιd0 cηdιd1)

Eθ
G-RL∗,d+1

(cηdιd
0
→cηdιd0 ...)

if cηd+1 = x̄ηd+1 , ιd = ² ∶∑x̄ηd0,cηd0∑κ s.t. x̄ηd0κ=cηd0κ⎛
⎝
PθG-RL∗,d

(cηd−1ιd−11 ∗→ cηd0 ...)⋅PθIC(G),d(x̄ηd0/x̄ηd0κ, cηd0κ ∣ cηd0)
⎞
⎠

⋅
⎡⎢⎢⎢⎢⎢⎢⎣

Pθ
G-RL∗,d

(cηd−1ιd−11
+

→cηd0 ...)

Pθ
G-RL∗,d

(cηd−1ιd−11
∗

→cηd0 ...)⋅Jx̄ηd0κ = cηd0κK

⎤⎥⎥⎥⎥⎥⎥⎦
⋅ PθG-RL∗,d(cηd−1ιd−11

∗

→cηd ...)

Pθ
G-RL∗,d

(cηd−1ιd−11
+

→cηd0 ...)⋅PθG-L,d(cηd → cηd0 cηd1)
if cηd+1 ≠ x̄ηd+1 ∶⎛
⎝
PθG-RL∗,d

(cηd−1ιd−11 ∗→ cηd ...)⋅PθIC(G),d(x̄ηd/x̄ηdιd , cηdιd ∣ cηd)
⎞
⎠

(25)

Note that the left progeny probabilities above can-

cel out over time, leaving only the relevant original

PCFG probabilities at the end of each sentence.

4 Parsing in a Factored Sequence Model

Store elements can now be abstracted away from

(i.e. marginalized over) individual constituent struc-

ture addresses. Store elements are therefore de-

fined to contain the only the active and awaited (cAsdt
and cWsdt ) constituent categories necessary to compute
an incomplete constituent probability:

sdt
def
= ⟨cAsdt , cWsdt ⟩ (26)
def
= ⟨cηd , cηdιd⟩ s.t. ιd ∈ 1+, ηd ∈ ηd−1ιd−10+ (27)

Reduction states are defined to contain only the

complete constituent category crdt necessary to com-
pute an inside likelihood probability, as well as a

flag frdt indicating whether a reduction has taken
place (to end a sequence of incomplete constituents):

rdt
def
= ⟨crdt , frdt ⟩ (28)
def
= ⟨cηd , Jx̄ηd = cηdK⟩ (29)

Since ιd ∈ 1
+, it follows that:

x1..t = (x̄ηd/x̄ηdιd)Dd=1 (30)
This allows store elements to be abstracted away

from (marginalized over) tree addresses in the cal-

culation of forward or Viterbi probabilities:

PθFwd(x1..t, s1..Dt ) =
∑

η1..D,ι1..D

PθFwd((x̄ηd/x̄ηdιd1)Dd=1, (cηd)Dd=1, (cηdιd1)Dd=1)
(31)

Forward probabilities can then be factored into con-

tributions from previous store states (θFwd at s
1..D
t−1

below, parenthesized in Equation 25), reductions

of terminal symbols (θB below, bracketed in Equa-
tion 25), and transition operations (θA below, not set
apart in Equation 25):

PθFwd(x1..t, s1..Dt ) =
∑
s1..Dt−1

PθFwd(x1..t−1, s1..Dt−1 ) ⋅ PθA(s1..Dt ∣ s1..Dt−1 )
⋅ PθB(xt ∣ s1..Dt ) (32)

Probabilities for recognized sequences of incom-

plete constituents (Equations 8 through 13), and

expected left progeny counts for unrecognized se-

quences of incomplete constituents (Equations 19

William Schuler

4



through 22) can be combined in a probabilistic push-

down automaton with a bounded pushdown store (to

simulate the bounded working memory store of a hu-

man comprehender). This is essentially an extension

of a Hierarchical Hidden Markov Model (HHMM)

(Murphy and Paskin, 2001), which obtains a most

likely sequence of hidden store states ŝ1..D1..T of some
length T and some maximum depth D, given a se-
quence of observations (e.g. words) x1..T :

ŝ1..D1..T
def
= argmax

s1..D1..T

T∏
t=1

PθA(s1..Dt ∣ s1..Dt−1 )⋅PθB(xt ∣ s1..Dt )
(33)

The model generates each successive store only after

considering whether each nested sequence of incom-

plete constituents has completed and reduced:

PθA(s1..Dt ∣ s1..Dt−1 )def=
∑

r1t ..r
D
t

D∏
d=1

PθR(rdt ∣ rd+1t sdt−1sd−1t−1 )
⋅ PθS(sdt ∣ rd+1t rdt sdt−1sd−1t ) (34)

The model probabilities for these store elements

and reduction states can then be defined (from Mur-

phy and Paskin 2001) to expand a new incomplete

constituent after a reduction has taken place (frdt =
1), transition along a sequence of store elements if
no reduction has taken place (frdt =0):

PθS(sdt ∣ rd+1t rdt sdt−1sd−1t )def=⎧⎪⎪⎪⎨⎪⎪⎪⎩
if frd+1t =1, frdt =1 ∶ PθS-E,d(sdt ∣ sd−1t )
if frd+1t =1, frdt =0 ∶ PθS-T,d(sdt ∣ rd+1t rdt sdt−1sd−1t )
if frd+1t =0, frdt =0 ∶ Jsdt = sdt−1K

(35)

and possibly reduce a store element (terminate a

sequence) if the store state below it has reduced

(frd+1t =1):

PθR(rdt ∣ rd+1t sdt−1sd−1t−1 )def=
{if frd+1t =0 ∶ Jrdt = rK

if frd+1t =1 ∶ PθR-R,d(rdt ∣ rd+1t sdt−1 sd−1t−1 ) (36)
where s0t = s⊺ and r

D+1
t = r⊺ for constants s⊺ (an

incomplete root constituent), r⊺ (a complete lexical
constituent) and r (a null state resulting from the

. . .

. . .

. . .

. . .

r3t−1

r2t−1

r1t−1
s1t−1

s2t−1

s3t−1

xt−1

r3t

r2t

r1t
s1t

s2t

s3t

xt

Figure 1: Graphical representation of the dependency

structure in a standard Hierarchic Hidden Markov Model

with D = 3 hidden levels that can be used to parse syntax.
Circles denote random variables, and edges denote con-

ditional dependencies. Shaded circles denote variables

with observed values.

failure of an incomplete constituent to complete).

The model is shown graphically in Figure 1.

These pushdown automaton operations are then

refined for right-corner parsing (from Schuler 2009),

distinguishing active transitions θS-T-A,d (in which
an incomplete constituent is completed, but not re-

duced, and then immediately expanded to a new

incomplete constituent in the same store element)

from awaited transitions θS-T-W,d (which involve no
completion):

PθS-T,d(sdt ∣ rd+1t rdt sdt−1sd−1t )def=
{if rdt =r ∶ PθS-T-A,d(sdt ∣ sdt−1rd+1t )

if rdt ≠r ∶ PθS-T-W,d(sdt ∣ sd−1t rdt ) (37)

PθR-R,d(rdt ∣ rd+1t sdt−1sd−1t−1 )def=
{if crd+1t =xt ∶ PθR-R,d(rdt ∣ sdt−1sd−1t−1 )

if crd+1t ≠xt ∶ Jrdt = rK (38)
These right-corner parsing operations then con-

struct a full inside probability decompositions, us-

ing Equations 8 through 13 and Equations 19

through 22, marginalizing out all constituents that

are not required in each term:

• for expansions:

PθS-E,d(⟨cηι, c′ηι⟩ ∣ ⟨−, cη⟩)def=
EθG-RL∗,d

(cη ∗→ cηι ...) ⋅ Jxηι = c′ηι = cηιK (39)

Incremental Parsing in Bounded Memory

5



a) binarized phrase structure tree:

S

NP

NP

JJ

strong

NN

demand

PP

IN

for

NP

NPpos

NNP

NNP

new

NNP

NNP

york

NNP

city

POS

’s

NNS

JJ

general

NNS

NN

obligation

NNS

bonds

VP

VBN

VBN

propped

PRT

up

NP

DT

the

NN

JJ

municipal

NN

market

b) result of right-corner transform:

S

S/NN

S/NN

S/NP

S/VP

NP

NP/NNS

NP/NNS

NP/NNS

NP/NP

NP/PP

NP

NP/NN

JJ

strong

NN

demand

IN

for

NPpos

NPpos/POS

NNP

NNP/NNP

NNP/NNP

NNP

new

NNP

york

NNP

city

POS

’s

JJ

general

NN

obligation

NNS

bonds

VBN

VBN/PRT

VBN

propped

PRT

up

DT

the

JJ

municipal

NN

market

Figure 2: (a) Sample binarized phrase structure tree for the sentence Strong demand for New York City’s general

obligations bonds propped up the municipal market, and (b) a right-corner transform of this binarized tree.

• for awaited transitions, from Equation 11:

PθS-T-W,d(⟨cη, cηι1⟩ ∣ ⟨c′η, cηι⟩ cηι0)def=
Jcη = c

′
ηK ⋅ PθG-R,d(cηι → cηι0 cηι1)

EθG-RL∗,d
(cηι 0→ cηι0 ...) (40)

• for active transitions, from Equations 8 and 13:

PθS-T-A,d(⟨cηι, cηι1⟩ ∣ ⟨−, cη⟩ cηι0)def=
EθG-RL∗,d

(cη ∗→ cηι ...) ⋅ PθG-L,d(cηι → cηι0 cηι1)
EθG-RL∗,d

(cη +→ cηι0 ...)
(41)

• for cross-element reductions:

PθR-R,d(cηι,1 ∣ ⟨−, cη⟩ ⟨c′ηι,−⟩)def=
Jcηι = c

′
ηιK ⋅ EθG-RL∗,d(cη

0
→ cηι ...)

EθG-RL∗,d
(cη ∗→ cηι ...) (42)

• for in-element reductions:

PθR-R,d(cηι,0 ∣ ⟨−, cη⟩ ⟨c′ηι,−⟩)def=
Jcηι = c

′
ηιK ⋅ EθG-RL∗,d(cη

+
→ cηι ...)

EθG-RL∗,d
(cη ∗→ cηι ...) (43)

A sample phrase structure tree is shown as a right-

corner transformed recursive structure in Figure 2,

William Schuler

6



d=1

d=2

d=3

word

t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8 t=9 t=10 t=11 t=12 t=13 t=14 t=15

strong

dem
and

for

new

york

city

’s

general

obligations

bonds

propped

up the

m
unicipal

m
arket

− − − − − − − − − − − − − − −

− − − −

N
N

P
/N

N
P

N
N

P
/N

N
P

N
P
pos/P

O
S

− − − −

V
B

N
/P

R
T

− − −

−

N
P
/N

N

N
P
/P

P

N
P
/N

P

N
P
/N

P

N
P
/N

P

N
P
/N

P

N
P
/N

N
S

N
P
/N

N
S

N
P
/N

N
S

S
/V

P

S
/V

P

S
/N

P

S
/N

N

S
/N

N

Figure 3: Sample tree from Figure 2 mapped to sdt variable positions of an HHMM at each depth level d (vertical) and
time step t (horizontal). This tree uses at most only two memory store elements. Values for reduction states rdt are not
shown.

and as a sequence of store states in Figure 3, cor-

responding to the output of Viterbi most likely se-

quence estimator. This estimation runs in linear time

on the length of the input, so the parser can be run

continuously on unsegmented or unpunctuated in-

put. An ordinary phrase structure tree can be ob-

tained by applying the transforms from Section 3, in

reverse, to the right-corner recursive phrase structure

tree represented in the sequence of store states.

5 Discussion

This paper has presented a derivation of a fac-

tored probabilistic sequence model from a proba-

bilistic context-free grammar, using a right-corner

transform to minimize memory usage in incremen-

tal processing. This sequence model characteriza-

tion is attractive as a cognitive model because it

does not posit any internal representation of com-

plex phrasal structure beyond the pair of categories

in each incomplete constituent resulting from the

application of a right-corner transform; and be-

cause these incomplete constituents represent con-

tiguous connected chunks of phrase structure, in line

with characterizations of chunking in working mem-

ory (Miller, 1956). Experiments on large phrase-

structure annotated corpora (Marcus et al., 1993)

show this model could process the vast majority of

sentences in a typical newspaper using only three or

four store elements (Schuler et al., 2008; Schuler

et al., 2010), in line with recent estimates of hu-

man short-term working memory capacity (Cowan,

2001).

This derivation can be applied to efficiently in-

crementalize any PCFG model, preserving the prob-

abilistic dependencies in the original model. But,

since the model is ultimately defined through transi-

tions on entire working memory stores, it is also pos-

sible to relax the independence assumptions from

the original PCFG model, and introduce additional

dependencies across store elements that do not cor-

respond to context-free dependencies. These ad-

ditional dependencies might be used approximate

dependencies of mildly context-sensitive grammar

formalisms like tree adjoining grammars (Joshi,

1985), e.g. to model long-distance dependencies in

filler-gap constructions (Kroch and Joshi, 1986), or

crossed and nested dependencies in languages like

Dutch (Shieber, 1985).

Figure 4 shows a sample store sequence corre-

sponding to a parse of a noun phrase modified by an

object relative clause. Here, the affix ‘-xNP’ is used

to identify a phrase containing an extracted NP con-

stituent, and the category label ‘GCnpS’ is used to

identify the maximal projection of a gapped clause.

If the GC constituent (and only this constituent) is

associated with the referent or dependency informa-

tion of the filler constituent (the bike in this exam-

ple), this information can be made available dur-

ing processing from the immediately superior in-

complete constituent at the gap position in the rel-

ative clause, without passing dependency informa-

tion down the tree as any kind of feature (Pollard

and Sag, 1994), even though this position is not ad-

jacent to the filler in the phrase structure tree (they

are separated by the referent or dependency informa-

tion of the bridge verb ‘said’). This is important to

Incremental Parsing in Bounded Memory

7



d=1

d=2

d=3

word

t=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8 t=9

the

bike

you

said

your

friend

got

for

christm
as

− − − − −

N
P/N

N

− − −

− − −

SxN
P/V

PxN
P

SxN
P/SxN

P

SxN
P/SxN

P
SxN

P/V
PxN

P

SxN
P/PP

SxN
P/N

P

−

N
P/N

N

N
P/G

C
npS

N
P/G

C
npS

N
P/G

C
npS

N
P/G

C
npS

N
P/G

C
npS

N
P/G

C
npS

N
P/G

C
npS

Figure 4: Sample store sequence containing long-distance dependency in a filler-gap construction.

d=1

d=2

d=3

word

t=1 t=2 t=3 t=4 t=5 t=6

Jan

Piet

M
arie

zag

laten

zw
em

m
en

− − −

S/V
P

S/V
P

S/V
P

− −

S/V
P

S/V
P

S/V
P

S/S

−

S/V
P

S/V
P

S/V
P

S/S

S/S

Figure 5: Sample store sequence containing crossed and

nested dependencies.

satisfy claims that the model constructs chunks only

for contiguous dependency structures (which is not

true of propagated ‘slash’ features.

Figure 5 shows a sample store sequence corre-

sponding to a parse of a Dutch sentence containing

crossed and nested dependencies, featuring reduc-

tions across non-adjacent depth levels. This requires

a more severe relaxation of PCFG independence as-

sumptions, and is beyond the capacity of the HHMM

as defined above, but this does preserve the notion

of incomplete constituents and general composition

established above, and is not beyond the capacity of

factored sequence models with human-like memory

bounds in general (note that the example sentence

can still be parsed with only three store elements).

This suggests a promising avenue of generalizing

this model to learn parsing transitions that may be

broader than those of a strict pushdown automaton.

References

Richard Bellman. 1957. Dynamic Programming.

Princeton University Press, Princeton, NJ.

Nelson Cowan. 2001. The magical number 4 in short-

term memory: A reconsideration of mental storage ca-

pacity. Behavioral and Brain Sciences, 24:87–185.

Aravind K. Joshi. 1985. How much context sensitiv-

ity is necessary for characterizing structural descrip-

tions: Tree adjoining grammars. In L. Karttunen

D. Dowty and A. Zwicky, editors, Natural language

parsing: Psychological, computational and theoreti-

cal perspectives, pages 206–250. Cambridge Univer-

sity Press, Cambridge, U.K.

Anthony S. Kroch and Aravind K. Joshi. 1986. The lin-

guistic relevance of tree adjoining grammars. Linguis-

tics and Philosophy.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann

Marcinkiewicz. 1993. Building a large annotated cor-

pus of English: the Penn Treebank. Computational

Linguistics, 19(2):313–330.

George A. Miller. 1956. The magical number seven, plus

or minus two: Some limits on our capacity for process-

ing information. Psychological Review, 63:81–97.

Kevin P. Murphy and Mark A. Paskin. 2001. Linear time

inference in hierarchical HMMs. In Proc. NIPS, pages

833–840, Vancouver, BC, Canada.

Carl Pollard and Ivan Sag. 1994. Head-driven Phrase

Structure Grammar. University of Chicago Press,

Chicago.

William Schuler, Samir AbdelRahman, Tim Miller, and

Lane Schwartz. 2008. Toward a psycholinguistically-

motivated model of language. In Proceedings of COL-

ING, pages 785–792, Manchester, UK, August.

William Schuler, Samir AbdelRahman, Tim Miller, and

Lane Schwartz. 2010. Broad-coverage incremental

parsing using human-like memory constraints. Com-

putational Linguistics, 36(1).

William Schuler. 2009. Parsing with a bounded stack us-

ing a model-based right-corner transform. In Proceed-

ings of NAACL, pages 344–352, Boulder, Colorado.

Stuart Shieber. 1985. Evidence against the context-

freeness of natural language. Linguistics and Philoso-

phy, 8:333–343.

William Schuler

8


