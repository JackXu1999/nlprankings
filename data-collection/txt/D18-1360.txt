



















































Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219–3232
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

3219

Multi-Task Identification of Entities, Relations, and Coreference
for Scientific Knowledge Graph Construction

Yi Luan Luheng He Mari Ostendorf Hannaneh Hajishirzi
University of Washington

{luanyi, luheng, ostendor, hannaneh}@uw.edu

Abstract

We introduce a multi-task setup of identifying
and classifying entities, relations, and coref-
erence clusters in scientific articles. We cre-
ate SCIERC, a dataset that includes annota-
tions for all three tasks and develop a uni-
fied framework called Scientific Information
Extractor (SCIIE) for with shared span rep-
resentations. The multi-task setup reduces
cascading errors between tasks and leverages
cross-sentence relations through coreference
links. Experiments show that our multi-task
model outperforms previous models in scien-
tific information extraction without using any
domain-specific features. We further show that
the framework supports construction of a sci-
entific knowledge graph, which we use to ana-
lyze information in scientific literature.1

1 Introduction

As scientific communities grow and evolve, new
tasks, methods, and datasets are introduced and
different methods are compared with each other.
Despite advances in search engines, it is still hard
to identify new technologies and their relationships
with what existed before. To help researchers more
quickly identify opportunities for new combina-
tions of tasks, methods and data, it is important to
design intelligent algorithms that can extract and
organize scientific information from a large collec-
tion of documents.

Organizing scientific information into structured
knowledge bases requires information extraction
(IE) about scientific entities and their relationships.
However, the challenges associated with scientific
IE are greater than for a general domain. First, an-
notation of scientific text requires domain expertise
which makes annotation costly and limits resources.

1Data and code are publicly available at: http://nlp.
cs.washington.edu/sciIE/

Figure 1: Example annotation: phrases that refer to
the same scientific concept are annotated into the
same coreference cluster, such as MORphological
PAser MORPA, it and MORPA (marked as red).

In addition, most relation extraction systems are de-
signed for within-sentence relations. However, ex-
tracting information from scientific articles requires
extracting relations across sentences. Figure 1 il-
lustrates this problem. The cross-sentence relations
between some entities can only be connected by
entities that refer to the same scientific concept,
including generic terms (such as the pronoun it,
or phrases like our method) that are not informa-
tive by themselves. With co-reference, context-free
grammar can be connected to MORPA through the
intermediate co-referred pronoun it. Applying ex-
isting IE systems to this data, without co-reference,
will result in much lower relation coverage (and a
sparse knowledge base).

In this paper, we develop a unified learning
model for extracting scientific entities, relations,
and coreference resolution. This is different from
previous work (Luan et al., 2017b; Gupta and Man-
ning, 2011; Tsai et al., 2013; Gábor et al., 2018)
which often addresses these tasks as independent

http://nlp.cs.washington.edu/sciIE/
http://nlp.cs.washington.edu/sciIE/


3220

components of a pipeline. Our unified model is
a multi-task setup that shares parameters across
low-level tasks, making predictions by leveraging
context across the document through coreference
links. Specifically, we extend prior work for learn-
ing span representations and coreference resolution
(Lee et al., 2017; He et al., 2018). Different from a
standard tagging system, our system enumerates all
possible spans during decoding and can effectively
detect overlapped spans. It avoids cascading errors
between tasks by jointly modeling all spans and
span-span relations.

To explore this problem, we create a dataset SCI-
ERC for scientific information extraction, which
includes annotations of scientific terms, relation
categories and co-reference links. Our experiments
show that the unified model is better at predict-
ing span boundaries, and it outperforms previous
state-of-the-art scientific IE systems on entity and
relation extraction (Luan et al., 2017b; Augenstein
et al., 2017). In addition, we build a scientific
knowledge graph integrating terms and relations
extracted from each article. Human evaluation
shows that propagating coreference can signifi-
cantly improve the quality of the automatic con-
structed knowledge graph.

In summary we make the following contribu-
tions. We create a dataset for scientific information
extraction by jointly annotating scientific entities,
relations, and coreference links. Extending a previ-
ous end-to-end coreference resolution system, we
develop a multi-task learning framework that can
detect scientific entities, relations, and coreference
clusters without hand-engineered features. We use
our unified framework to build a scientific knowl-
edge graph from a large collection of documents
and analyze information in scientific literature.

2 Related Work

There has been growing interest in research on au-
tomatic methods for information extraction from
scientific articles. Past research in scientific IE
addressed analyzing citations (Athar and Teufel,
2012b,a; Kas, 2011; Gabor et al., 2016; Sim et al.,
2012; Do et al., 2013; Jaidka et al., 2014; Abu-
Jbara and Radev, 2011), analyzing research com-
munity (Vogel and Jurafsky, 2012; Anderson et al.,
2012), and unsupervised methods for extracting sci-
entific entities and relations (Gupta and Manning,
2011; Tsai et al., 2013; Gábor et al., 2016).

More recently, two datasets in SemEval 2017

and 2018 have been introduced, which facilitate
research on supervised and semi-supervised learn-
ing for scientific information extraction. SemEval
17 (Augenstein et al., 2017) includes 500 para-
graphs from articles in the domains of computer
science, physics, and material science. It includes
three types of entities (called keyphrases): Tasks,
Methods, and Materials and two relation types:
hyponym-of and synonym-of. SemEval 18 (Gábor
et al., 2018) is focused on predicting relations be-
tween entities within a sentence. It consists of six
relation types. Using these datasets, neural mod-
els (Ammar et al., 2017, 2018; Luan et al., 2017b;
Augenstein and Søgaard, 2017) are introduced for
extracting scientific information. We extend these
datasets by increasing relation coverage, adding
cross-sentence coreference linking, and removing
some annotation constraints. Different from most
previous IE systems for scientific literature and gen-
eral domains (Miwa and Bansal, 2016; Xu et al.,
2016; Peng et al., 2017; Quirk and Poon, 2017;
Luan et al., 2018; Adel and Schütze, 2017), which
use preprocessed syntactic, discourse or corefer-
ence features as input, our unified framework does
not rely on any pipeline processing and is able to
model overlapping spans.

While Singh et al. (2013) show improvements
by jointly modeling entities, relations, and coref-
erence links, most recent neural models for these
tasks focus on single tasks (Clark and Manning,
2016; Wiseman et al., 2016; Lee et al., 2017; Lam-
ple et al., 2016; Peng et al., 2017) or joint entity
and relation extraction (Katiyar and Cardie, 2017;
Zhang et al., 2017; Adel and Schütze, 2017; Zheng
et al., 2017). Among those studies, many papers as-
sume the entity boundaries are given, such as (Clark
and Manning, 2016), Adel and Schütze (2017) and
Peng et al. (2017). Our work relaxes this constraint
and predicts entity boundaries by optimizing over
all possible spans. Our model draws from recent
end-to-end span-based models for coreference res-
olution (Lee et al., 2017, 2018) and semantic role
labeling (He et al., 2018) and extends them for the
multi-task framework involving the three tasks of
identification of entity, relation and coreference.

Neural multi-task learning has been applied to
a range of NLP tasks. Most of these models share
word-level representations (Collobert and Weston,
2008; Klerke et al., 2016; Luan et al., 2016, 2017a;
Rei, 2017), while Peng et al. (2017) uses high-order
cross-task factors. Our model instead propagates



3221

cross-task information via span representations,
which is related to Swayamdipta et al. (2017).

3 Dataset

Our dataset (called SCIERC) includes annotations
for scientific entities, their relations, and corefer-
ence clusters for 500 scientific abstracts. These ab-
stracts are taken from 12 AI conference/workshop
proceedings in four AI communities from the Se-
mantic Scholar Corpus2. SCIERC extends pre-
vious datasets in scientific articles SemEval 2017
Task 10 (SemEval 17) (Augenstein et al., 2017) and
SemEval 2018 Task 7 (SemEval 18) (Gábor et al.,
2018) by extending entity types, relation types, rela-
tion coverage, and adding cross-sentence relations
using coreference links. Our dataset is publicly
available at: http://nlp.cs.washington.
edu/sciIE/. Table 1 shows the statistics of SCI-
ERC.

Annotation Scheme We define six types for an-
notating scientific entities (Task, Method, Metric,
Material, Other-ScientificTerm and Generic) and
seven relation types (Compare, Part-of, Conjunc-
tion, Evaluate-for, Feature-of, Used-for, Hyponym-
Of). Directionality is taken into account except
for the two symmetric relation types (Conjunction
and Compare). Coreference links are annotated
between identical scientific entities. A Generic en-
tity is annotated only when the entity is involved
in a relation or is coreferred with another entity.
Annotation guidelines can be found in Appendix A.
Figure 1 shows an annotated example.

Following annotation guidelines from Qasem-
iZadeh and Schumann (2016) and using the BRAT
interface (Stenetorp et al., 2012), our annotators
perform a greedy annotation for spans and always
prefer the longer span whenever ambiguity occurs.
Nested spans are allowed when a subspan has a
relation/coreference link with another term outside
the span.

Human Agreements One domain expert anno-
tated all the documents in the dataset; 12% of the
data is dually annotated by 4 other domain experts
to evaluate the user agreements. The kappa score
for annotating entities is 76.9%, relation extraction
is 67.8% and coreference is 63.8%.

2These conferences include general AI (AAAI, IJCAI),
NLP (ACL, EMNLP, IJCNLP), speech (ICASSP, Interspeech),
machine learning (NIPS, ICML), and computer vision (CVPR,
ICCV, ECCV) at http://labs.semanticscholar.
org/corpus/

Statistics SCIERC SemEval 17 SemEval 18

#Entities 8089 9946 7483
#Relations 4716 672 1595
#Relations/Doc 9.4 1.3 3.2
#Coref links 2752 - -
#Coref clusters 1023 - -

Table 1: Dataset statistics for our dataset SCIERC
and two previous datasets on scientific information
extraction. All datasets annotate 500 documents.

Comparison with previous datasets SCIERC
is focused on annotating cross-sentence relations
and has more relation coverage than SemEval 17
and SemEval 18, as shown in Table 1. SemEval 17
is mostly designed for entity recognition and only
covers two relation types. The task in SemEval 18
is to classify a relation between a pair of entities
given entity boundaries, but only intra-sentence re-
lations are annotated and each entity only appears
in one relation, resulting in sparser relation cover-
age than our dataset (3.2 vs. 9.4 relations per ab-
stract). SCIERC extends these datasets by adding
more relation types and coreference clusters, which
allows representing cross-sentence relations, and
removing annotation constraints. Table 1 gives a
comparison of statistics among the three datasets.
In addition, SCIERC aims at including broader
coverage of general AI communities.

4 Model

We develop a unified framework (called SCIIE)
to identify and classify scientific entities, relations,
and coreference resolution across sentences. SCIIE
is a multi-task learning setup that extends previous
span-based models for coreference resolution (Lee
et al., 2017) and semantic role labeling (He et al.,
2018). All three tasks of entity recognition, re-
lation extraction, and coreference resolution are
treated as multinomial classification problems with
shared span representations. SCIIE benefits from
expressive contextualized span representations as
classifier features. By sharing span representations,
sentence-level tasks can benefit from information
propagated from coreference resolution across sen-
tences, without increasing the complexity of infer-
ence. Figure 2 shows a high-level overview of the
SCIIE multi-task framework.

4.1 Problem Definition

The input is a document represented as a sequence
of words D = {w1, . . . , wn}, from which we de-
rive S = {s1, . . . , sN}, the set of all possible

http://nlp.cs.washington.edu/sciIE/
http://nlp.cs.washington.edu/sciIE/
http://labs.semanticscholar.org/corpus/
http://labs.semanticscholar.org/corpus/


3222

MORPA

MORPA is a

a fully implemented parser

parser

MORphological Parser MORPA

MORPA is a fully implemented parser developed for ……, the MORphological Parser MORPA is provided with a …

MORphological

MORPA is

is provided with

NULL

Hyponym-of Used-for NULL
MethodTask NULL

Sentences

BiLSTM outputs

Span  
Representations

Entity 
Recognition

+Span Features

Coreference 
Resolution

Relation 
Extraction

… …

parserMORPA
MORphological

MORphological Parser MORPA

…

… …

Figure 2: Overview of the multitask setup, where all three tasks are treated as classification problems on
top of shared span representations. Dotted arcs indicate the normalization space for each task.

within-sentence word sequence spans (up to a rea-
sonable length) in the document. The output con-
tains three structures: the entity types E for all
spans S, the relations R for all pair of spans S×S,
and the coreference links C for all spans in S. The
output structures are represented with a set of dis-
crete random variables indexed by spans or pairs
of spans. Specifically, the output structures are
defined as follows.
Entity recognition is to predict the best entity type
for every candidate span. Let LE represent the set
of all possible entity types including the null-type �.
The output structure E is a set of random variables
indexed by spans: ei ∈ LE for i = 1, . . . , N .
Relation extraction is to predict the best relation
type given an ordered pair of spans (si, sj). Let LR
be the set of all possible relation types including
the null-type �. The output structure R is a set of
random variables indexed over pairs of spans (i, j)
that belong to the same sentence: rij ∈ LR for
i, j = 1, . . . , N .
Coreference resolution is to predict the best an-
tecedent (including a special null antecedent) given
a span, which is the same mention-ranking model
used in Lee et al. (2017). The output structure
C is a set of random variables defined as: ci ∈
{1, . . . , i− 1, �} for i = 1, . . . , N .

4.2 Model Definition

We formulate the multi-task learning setup as
learning the conditional probability distribution
P (E,R,C|D). For efficient training and inference,
we decompose P (E,R,C|D) assuming spans are

conditionally independent given D:

P (E,R,C | D) = P (E,R,C, S | D) (1)

=

N∏
i=1

P (ei | D)P (ci | D)
N∏
j=1

P (rij | D),

where the conditional probabilities of each random
variable are independently normalized:

P (ei = e | D) =
exp(ΦE(e, si))∑

e′∈LE exp(ΦE(e
′, si))

(2)

P (rij = r | D) =
exp(ΦR(r, si, sj))∑

r′∈LR exp(ΦR(r
′, si, sj))

P (ci = j | D) =
exp(ΦC(si, sj))∑

j′∈{1,...,i−1,�} exp(ΦC(si, sj′))
,

where ΦE denotes the unnormalized model score
for an entity type e and a span si, ΦR denotes the
score for a relation type r and span pairs si, sj ,
and ΦC denotes the score for a binary coreference
link between si and sj . These Φ scores are further
decomposed into span and pairwise span scores
computed from feed-forward networks, as will be
explained in Section 4.3.

For simplicity, we omit D from the Φ functions
and S from the observation.

Objective Given a set of all documents D, the
model loss function is defined as a weighted sum of
the negative log-likelihood loss of all three tasks:

−
∑

(D,R∗,E∗,C∗)∈D

{
λE logP (E

∗ | D) (3)

+ λR logP (R
∗ | D) + λC logP (C∗ | D)

}



3223

where E∗, R∗, and C∗ are gold structures of the en-
tity types, relations, and coreference, respectively.
The task weights λE, λR, and λC are introduced as
hyper-parameters to control the importance of each
task.

For entity recognition and relation extraction,
P (E∗ | D) and P (R∗ | D) are computed with
the definition in Equation (2). For coreference
resolution, we use the marginalized loss follow-
ing Lee et al. (2017) since each mention can have
multiple correct antecedents. Let C∗i be the set
of all correct antecedents for span i, we have:
logP (C∗ | D) =

∑
i=1..N log

∑
c∈C∗i

P (c | D).

4.3 Scoring Architecture
We use feedforward neural networks (FFNNs) over
shared span representations g to compute a set
of span and pairwise span scores. For the span
scores, φe(si) measures how likely a span si has
an entity type e, and φmr(si) and φmc(si) measure
how likely a span si is a mention in a relation or a
coreference link, respectively. The pairwise scores
φr(si, sj) and φc(si, sj) measure how likely two
spans are associated in a relation r or a coreference
link, respectively. Let gi be the fixed-length vec-
tor representation for span si. For different tasks,
the span scores φx(si) for x ∈ {e,mc,mr} and
pairwise span scores φy(si, sj) for y ∈ {r, c} are
computed as follows:

φx(si) =wx · FFNNx(gi)
φy(si, sj) =wy · FFNNy([gi,gj ,gi � gj ]),

where � is element-wise multiplication, and
{wx,wy} are neural network parameters to be
learned.

We use these scores to compute the different Φ:

ΦE(e, si) = φe(si) (4)

ΦR(r, si, sj) = φmr(si) + φmr(sj) + φr(si, sj)

ΦC(si, sj) = φmc(si) + φmc(sj) + φc(si, sj)

The scores in Equation (4) are defined for entity
types, relations, and antecedents that are not the
null-type �. Scores involving the null label are
set to a constant 0: ΦE(�, si) = ΦR(�, si, sj) =
ΦC(si, �) = 0.

We use the same span representations g from
(Lee et al., 2017) and share them across the three
tasks. We start by building bi-directional LSTMs
(Hochreiter and Schmidhuber, 1997) from word,
character and ELMo (Peters et al., 2018) embed-
dings.

For a span si, its vector representation gi is con-
structed by concatenating si’s left and right end
points from the BiLSTM outputs, an attention-
based soft “headword,” and embedded span width
features. Hyperparameters and other implementa-
tion details will be described in Section 6.

4.4 Inference and Pruning

Following previous work, we use beam pruning to
reduce the number of pairwise span factors from
O(n4) to O(n2) at both training and test time,
where n is the number of words in the document.
We define two separate beams: BC to prune spans
for the coreference resolution task, and BR for rela-
tion extraction. The spans in the beams are sorted
by their span scores φmc and φmr respectively, and
the sizes of the beams are limited by λCn and λRn.
We also limit the maximum width of spans to a
fixed number W , which further reduces the num-
ber of span factors to O(n).

5 Knowledge Graph Construction

We construct a scientific knowledge graph from
a large corpus of scientific articles. The corpus
includes all abstracts (110k in total) from 12 AI
conference proceedings from the Semantic Scholar
Corpus. Nodes in the knowledge graph correspond
to scientific entities. Edges correspond to scientific
relations between pairs of entities. The edges are
typed according to the relation types defined in Sec-
tion 3. Figure 4 shows a part of a knowledge graph
created by our method. For example, Statistical
Machine Translation (SMT) and grammatical error
correction are nodes in the graph, and they are con-
nected through a Used-for relation type. In order
to construct the knowledge graph for the whole
corpus, we first apply the SCIIE model over sin-
gle documents and then integrate the entities and
relations across multiple documents (Figure 3).

Extracting nodes (entities) The SCIIE model
extracts entities, their relations, and coreference

Abstract(1)
<latexit sha1_base64="plPMi3PhbjdexTOfOHuoBb/buP8=">AAAB+3icbVBNS8NAEN34WetXrEcvi0Wol5KIoHiqePFYwX5AG8pmu2mXbjZhdyItIX/FiwdFvPpHvPlv3LQ5aOuDgcd7M8zM82PBNTjOt7W2vrG5tV3aKe/u7R8c2keVto4SRVmLRiJSXZ9oJrhkLeAgWDdWjIS+YB1/cpf7nSemNI/kI8xi5oVkJHnAKQEjDexKH9gU0ltfgyIUspp7PrCrTt2ZA68StyBVVKA5sL/6w4gmIZNABdG65zoxeClRwKlgWbmfaBYTOiEj1jNUkpBpL53fnuEzowxxEClTEvBc/T2RklDrWeibzpDAWC97ufif10sguPZSLuMEmKSLRUEiMEQ4DwIPuWIUxMwQQhU3t2I6JnkIJq6yCcFdfnmVtC/qrlN3Hy6rjZsijhI6Qaeohlx0hRroHjVRC1E0Rc/oFb1ZmfVivVsfi9Y1q5g5Rn9gff4Aq5qUJg==</latexit><latexit sha1_base64="plPMi3PhbjdexTOfOHuoBb/buP8=">AAAB+3icbVBNS8NAEN34WetXrEcvi0Wol5KIoHiqePFYwX5AG8pmu2mXbjZhdyItIX/FiwdFvPpHvPlv3LQ5aOuDgcd7M8zM82PBNTjOt7W2vrG5tV3aKe/u7R8c2keVto4SRVmLRiJSXZ9oJrhkLeAgWDdWjIS+YB1/cpf7nSemNI/kI8xi5oVkJHnAKQEjDexKH9gU0ltfgyIUspp7PrCrTt2ZA68StyBVVKA5sL/6w4gmIZNABdG65zoxeClRwKlgWbmfaBYTOiEj1jNUkpBpL53fnuEzowxxEClTEvBc/T2RklDrWeibzpDAWC97ufif10sguPZSLuMEmKSLRUEiMEQ4DwIPuWIUxMwQQhU3t2I6JnkIJq6yCcFdfnmVtC/qrlN3Hy6rjZsijhI6Qaeohlx0hRroHjVRC1E0Rc/oFb1ZmfVivVsfi9Y1q5g5Rn9gff4Aq5qUJg==</latexit><latexit sha1_base64="plPMi3PhbjdexTOfOHuoBb/buP8=">AAAB+3icbVBNS8NAEN34WetXrEcvi0Wol5KIoHiqePFYwX5AG8pmu2mXbjZhdyItIX/FiwdFvPpHvPlv3LQ5aOuDgcd7M8zM82PBNTjOt7W2vrG5tV3aKe/u7R8c2keVto4SRVmLRiJSXZ9oJrhkLeAgWDdWjIS+YB1/cpf7nSemNI/kI8xi5oVkJHnAKQEjDexKH9gU0ltfgyIUspp7PrCrTt2ZA68StyBVVKA5sL/6w4gmIZNABdG65zoxeClRwKlgWbmfaBYTOiEj1jNUkpBpL53fnuEzowxxEClTEvBc/T2RklDrWeibzpDAWC97ufif10sguPZSLuMEmKSLRUEiMEQ4DwIPuWIUxMwQQhU3t2I6JnkIJq6yCcFdfnmVtC/qrlN3Hy6rjZsijhI6Qaeohlx0hRroHjVRC1E0Rc/oFb1ZmfVivVsfi9Y1q5g5Rn9gff4Aq5qUJg==</latexit><latexit sha1_base64="plPMi3PhbjdexTOfOHuoBb/buP8=">AAAB+3icbVBNS8NAEN34WetXrEcvi0Wol5KIoHiqePFYwX5AG8pmu2mXbjZhdyItIX/FiwdFvPpHvPlv3LQ5aOuDgcd7M8zM82PBNTjOt7W2vrG5tV3aKe/u7R8c2keVto4SRVmLRiJSXZ9oJrhkLeAgWDdWjIS+YB1/cpf7nSemNI/kI8xi5oVkJHnAKQEjDexKH9gU0ltfgyIUspp7PrCrTt2ZA68StyBVVKA5sL/6w4gmIZNABdG65zoxeClRwKlgWbmfaBYTOiEj1jNUkpBpL53fnuEzowxxEClTEvBc/T2RklDrWeibzpDAWC97ufif10sguPZSLuMEmKSLRUEiMEQ4DwIPuWIUxMwQQhU3t2I6JnkIJq6yCcFdfnmVtC/qrlN3Hy6rjZsijhI6Qaeohlx0hRroHjVRC1E0Rc/oFb1ZmfVivVsfi9Y1q5g5Rn9gff4Aq5qUJg==</latexit>

Abstract(2)
<latexit sha1_base64="oH+E80xlkwVGYgNSnJz+yJi1Lw8=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSxCvZSkCIqnihePFewHtKFstpt26WYTdifSEvpXvHhQxKt/xJv/xk2bg7Y+GHi8N8PMPD8WXIPjfFuFjc2t7Z3ibmlv/+DwyD4ut3WUKMpaNBKR6vpEM8ElawEHwbqxYiT0Bev4k7vM7zwxpXkkH2EWMy8kI8kDTgkYaWCX+8CmkN76GhShMK/WLwZ2xak5C+B14uakgnI0B/ZXfxjRJGQSqCBa91wnBi8lCjgVbF7qJ5rFhE7IiPUMlSRk2ksXt8/xuVGGOIiUKQl4of6eSEmo9Sz0TWdIYKxXvUz8z+slEFx7KZdxAkzS5aIgERginAWBh1wxCmJmCKGKm1sxHZMsBBNXyYTgrr68Ttr1muvU3IfLSuMmj6OITtEZqiIXXaEGukdN1EIUTdEzekVv1tx6sd6tj2VrwcpnTtAfWJ8/rR+UJw==</latexit><latexit sha1_base64="oH+E80xlkwVGYgNSnJz+yJi1Lw8=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSxCvZSkCIqnihePFewHtKFstpt26WYTdifSEvpXvHhQxKt/xJv/xk2bg7Y+GHi8N8PMPD8WXIPjfFuFjc2t7Z3ibmlv/+DwyD4ut3WUKMpaNBKR6vpEM8ElawEHwbqxYiT0Bev4k7vM7zwxpXkkH2EWMy8kI8kDTgkYaWCX+8CmkN76GhShMK/WLwZ2xak5C+B14uakgnI0B/ZXfxjRJGQSqCBa91wnBi8lCjgVbF7qJ5rFhE7IiPUMlSRk2ksXt8/xuVGGOIiUKQl4of6eSEmo9Sz0TWdIYKxXvUz8z+slEFx7KZdxAkzS5aIgERginAWBh1wxCmJmCKGKm1sxHZMsBBNXyYTgrr68Ttr1muvU3IfLSuMmj6OITtEZqiIXXaEGukdN1EIUTdEzekVv1tx6sd6tj2VrwcpnTtAfWJ8/rR+UJw==</latexit><latexit sha1_base64="oH+E80xlkwVGYgNSnJz+yJi1Lw8=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSxCvZSkCIqnihePFewHtKFstpt26WYTdifSEvpXvHhQxKt/xJv/xk2bg7Y+GHi8N8PMPD8WXIPjfFuFjc2t7Z3ibmlv/+DwyD4ut3WUKMpaNBKR6vpEM8ElawEHwbqxYiT0Bev4k7vM7zwxpXkkH2EWMy8kI8kDTgkYaWCX+8CmkN76GhShMK/WLwZ2xak5C+B14uakgnI0B/ZXfxjRJGQSqCBa91wnBi8lCjgVbF7qJ5rFhE7IiPUMlSRk2ksXt8/xuVGGOIiUKQl4of6eSEmo9Sz0TWdIYKxXvUz8z+slEFx7KZdxAkzS5aIgERginAWBh1wxCmJmCKGKm1sxHZMsBBNXyYTgrr68Ttr1muvU3IfLSuMmj6OITtEZqiIXXaEGukdN1EIUTdEzekVv1tx6sd6tj2VrwcpnTtAfWJ8/rR+UJw==</latexit><latexit sha1_base64="oH+E80xlkwVGYgNSnJz+yJi1Lw8=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSxCvZSkCIqnihePFewHtKFstpt26WYTdifSEvpXvHhQxKt/xJv/xk2bg7Y+GHi8N8PMPD8WXIPjfFuFjc2t7Z3ibmlv/+DwyD4ut3WUKMpaNBKR6vpEM8ElawEHwbqxYiT0Bev4k7vM7zwxpXkkH2EWMy8kI8kDTgkYaWCX+8CmkN76GhShMK/WLwZ2xak5C+B14uakgnI0B/ZXfxjRJGQSqCBa91wnBi8lCjgVbF7qJ5rFhE7IiPUMlSRk2ksXt8/xuVGGOIiUKQl4of6eSEmo9Sz0TWdIYKxXvUz8z+slEFx7KZdxAkzS5aIgERginAWBh1wxCmJmCKGKm1sxHZMsBBNXyYTgrr68Ttr1muvU3IfLSuMmj6OITtEZqiIXXaEGukdN1EIUTdEzekVv1tx6sd6tj2VrwcpnTtAfWJ8/rR+UJw==</latexit>

Abstract(m)
<latexit sha1_base64="PuH458bIm0iyjkYW01I+k4X1XVk=">AAAB+3icbVBNS8NAEN34WetXrEcvi0Wol5KIoHiqePFYwX5AG8pmu2mX7iZhdyItIX/FiwdFvPpHvPlv3LQ5aOuDgcd7M8zM82PBNTjOt7W2vrG5tV3aKe/u7R8c2keVto4SRVmLRiJSXZ9oJnjIWsBBsG6sGJG+YB1/cpf7nSemNI/CR5jFzJNkFPKAUwJGGtiVPrAppLe+BkUoZDV5PrCrTt2ZA68StyBVVKA5sL/6w4gmkoVABdG65zoxeClRwKlgWbmfaBYTOiEj1jM0JJJpL53fnuEzowxxEClTIeC5+nsiJVLrmfRNpyQw1steLv7n9RIIrr2Uh3ECLKSLRUEiMEQ4DwIPuWIUxMwQQhU3t2I6JnkIJq6yCcFdfnmVtC/qrlN3Hy6rjZsijhI6Qaeohlx0hRroHjVRC1E0Rc/oFb1ZmfVivVsfi9Y1q5g5Rn9gff4ABtWUYg==</latexit><latexit sha1_base64="PuH458bIm0iyjkYW01I+k4X1XVk=">AAAB+3icbVBNS8NAEN34WetXrEcvi0Wol5KIoHiqePFYwX5AG8pmu2mX7iZhdyItIX/FiwdFvPpHvPlv3LQ5aOuDgcd7M8zM82PBNTjOt7W2vrG5tV3aKe/u7R8c2keVto4SRVmLRiJSXZ9oJnjIWsBBsG6sGJG+YB1/cpf7nSemNI/CR5jFzJNkFPKAUwJGGtiVPrAppLe+BkUoZDV5PrCrTt2ZA68StyBVVKA5sL/6w4gmkoVABdG65zoxeClRwKlgWbmfaBYTOiEj1jM0JJJpL53fnuEzowxxEClTIeC5+nsiJVLrmfRNpyQw1steLv7n9RIIrr2Uh3ECLKSLRUEiMEQ4DwIPuWIUxMwQQhU3t2I6JnkIJq6yCcFdfnmVtC/qrlN3Hy6rjZsijhI6Qaeohlx0hRroHjVRC1E0Rc/oFb1ZmfVivVsfi9Y1q5g5Rn9gff4ABtWUYg==</latexit><latexit sha1_base64="PuH458bIm0iyjkYW01I+k4X1XVk=">AAAB+3icbVBNS8NAEN34WetXrEcvi0Wol5KIoHiqePFYwX5AG8pmu2mX7iZhdyItIX/FiwdFvPpHvPlv3LQ5aOuDgcd7M8zM82PBNTjOt7W2vrG5tV3aKe/u7R8c2keVto4SRVmLRiJSXZ9oJnjIWsBBsG6sGJG+YB1/cpf7nSemNI/CR5jFzJNkFPKAUwJGGtiVPrAppLe+BkUoZDV5PrCrTt2ZA68StyBVVKA5sL/6w4gmkoVABdG65zoxeClRwKlgWbmfaBYTOiEj1jM0JJJpL53fnuEzowxxEClTIeC5+nsiJVLrmfRNpyQw1steLv7n9RIIrr2Uh3ECLKSLRUEiMEQ4DwIPuWIUxMwQQhU3t2I6JnkIJq6yCcFdfnmVtC/qrlN3Hy6rjZsijhI6Qaeohlx0hRroHjVRC1E0Rc/oFb1ZmfVivVsfi9Y1q5g5Rn9gff4ABtWUYg==</latexit><latexit sha1_base64="PuH458bIm0iyjkYW01I+k4X1XVk=">AAAB+3icbVBNS8NAEN34WetXrEcvi0Wol5KIoHiqePFYwX5AG8pmu2mX7iZhdyItIX/FiwdFvPpHvPlv3LQ5aOuDgcd7M8zM82PBNTjOt7W2vrG5tV3aKe/u7R8c2keVto4SRVmLRiJSXZ9oJnjIWsBBsG6sGJG+YB1/cpf7nSemNI/CR5jFzJNkFPKAUwJGGtiVPrAppLe+BkUoZDV5PrCrTt2ZA68StyBVVKA5sL/6w4gmkoVABdG65zoxeClRwKlgWbmfaBYTOiEj1jM0JJJpL53fnuEzowxxEClTIeC5+nsiJVLrmfRNpyQw1steLv7n9RIIrr2Uh3ECLKSLRUEiMEQ4DwIPuWIUxMwQQhU3t2I6JnkIJq6yCcFdfnmVtC/qrlN3Hy6rjZsijhI6Qaeohlx0hRroHjVRC1E0Rc/oFb1ZmfVivVsfi9Y1q5g5Rn9gff4ABtWUYg==</latexit>

. . .
<latexit sha1_base64="BD1XKLi2MLyNy/k+DR9W3roijvs=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisYD+gDWWz2bRrN9mwOxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzglQKg6777ZTW1jc2t8rblZ3dvf2D6uFR26hMM95iSirdDajhUiS8hQIl76aa0ziQvBOMb2d+54lrI1TygJOU+zEdJiISjKKV2n0ZKjSDas2tu3OQVeIVpAYFmoPqVz9ULIt5gkxSY3qem6KfU42CST6t9DPDU8rGdMh7liY05sbP59dOyZlVQhIpbStBMld/T+Q0NmYSB7Yzpjgyy95M/M/rZRhd+7lI0gx5whaLokwSVGT2OgmF5gzlxBLKtLC3EjaimjK0AVVsCN7yy6ukfVH33Lp3f1lr3BRxlOEETuEcPLiCBtxBE1rA4BGe4RXeHOW8OO/Ox6K15BQzx/AHzucPubePMA==</latexit><latexit sha1_base64="BD1XKLi2MLyNy/k+DR9W3roijvs=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisYD+gDWWz2bRrN9mwOxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzglQKg6777ZTW1jc2t8rblZ3dvf2D6uFR26hMM95iSirdDajhUiS8hQIl76aa0ziQvBOMb2d+54lrI1TygJOU+zEdJiISjKKV2n0ZKjSDas2tu3OQVeIVpAYFmoPqVz9ULIt5gkxSY3qem6KfU42CST6t9DPDU8rGdMh7liY05sbP59dOyZlVQhIpbStBMld/T+Q0NmYSB7Yzpjgyy95M/M/rZRhd+7lI0gx5whaLokwSVGT2OgmF5gzlxBLKtLC3EjaimjK0AVVsCN7yy6ukfVH33Lp3f1lr3BRxlOEETuEcPLiCBtxBE1rA4BGe4RXeHOW8OO/Ox6K15BQzx/AHzucPubePMA==</latexit><latexit sha1_base64="BD1XKLi2MLyNy/k+DR9W3roijvs=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisYD+gDWWz2bRrN9mwOxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzglQKg6777ZTW1jc2t8rblZ3dvf2D6uFR26hMM95iSirdDajhUiS8hQIl76aa0ziQvBOMb2d+54lrI1TygJOU+zEdJiISjKKV2n0ZKjSDas2tu3OQVeIVpAYFmoPqVz9ULIt5gkxSY3qem6KfU42CST6t9DPDU8rGdMh7liY05sbP59dOyZlVQhIpbStBMld/T+Q0NmYSB7Yzpjgyy95M/M/rZRhd+7lI0gx5whaLokwSVGT2OgmF5gzlxBLKtLC3EjaimjK0AVVsCN7yy6ukfVH33Lp3f1lr3BRxlOEETuEcPLiCBtxBE1rA4BGe4RXeHOW8OO/Ox6K15BQzx/AHzucPubePMA==</latexit><latexit sha1_base64="BD1XKLi2MLyNy/k+DR9W3roijvs=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisYD+gDWWz2bRrN9mwOxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzglQKg6777ZTW1jc2t8rblZ3dvf2D6uFR26hMM95iSirdDajhUiS8hQIl76aa0ziQvBOMb2d+54lrI1TygJOU+zEdJiISjKKV2n0ZKjSDas2tu3OQVeIVpAYFmoPqVz9ULIt5gkxSY3qem6KfU42CST6t9DPDU8rGdMh7liY05sbP59dOyZlVQhIpbStBMld/T+Q0NmYSB7Yzpjgyy95M/M/rZRhd+7lI0gx5whaLokwSVGT2OgmF5gzlxBLKtLC3EjaimjK0AVVsCN7yy6ukfVH33Lp3f1lr3BRxlOEETuEcPLiCBtxBE1rA4BGe4RXeHOW8OO/Ox6K15BQzx/AHzucPubePMA==</latexit> . . .

<latexit sha1_base64="BD1XKLi2MLyNy/k+DR9W3roijvs=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisYD+gDWWz2bRrN9mwOxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzglQKg6777ZTW1jc2t8rblZ3dvf2D6uFR26hMM95iSirdDajhUiS8hQIl76aa0ziQvBOMb2d+54lrI1TygJOU+zEdJiISjKKV2n0ZKjSDas2tu3OQVeIVpAYFmoPqVz9ULIt5gkxSY3qem6KfU42CST6t9DPDU8rGdMh7liY05sbP59dOyZlVQhIpbStBMld/T+Q0NmYSB7Yzpjgyy95M/M/rZRhd+7lI0gx5whaLokwSVGT2OgmF5gzlxBLKtLC3EjaimjK0AVVsCN7yy6ukfVH33Lp3f1lr3BRxlOEETuEcPLiCBtxBE1rA4BGe4RXeHOW8OO/Ox6K15BQzx/AHzucPubePMA==</latexit><latexit sha1_base64="BD1XKLi2MLyNy/k+DR9W3roijvs=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisYD+gDWWz2bRrN9mwOxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzglQKg6777ZTW1jc2t8rblZ3dvf2D6uFR26hMM95iSirdDajhUiS8hQIl76aa0ziQvBOMb2d+54lrI1TygJOU+zEdJiISjKKV2n0ZKjSDas2tu3OQVeIVpAYFmoPqVz9ULIt5gkxSY3qem6KfU42CST6t9DPDU8rGdMh7liY05sbP59dOyZlVQhIpbStBMld/T+Q0NmYSB7Yzpjgyy95M/M/rZRhd+7lI0gx5whaLokwSVGT2OgmF5gzlxBLKtLC3EjaimjK0AVVsCN7yy6ukfVH33Lp3f1lr3BRxlOEETuEcPLiCBtxBE1rA4BGe4RXeHOW8OO/Ox6K15BQzx/AHzucPubePMA==</latexit><latexit sha1_base64="BD1XKLi2MLyNy/k+DR9W3roijvs=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisYD+gDWWz2bRrN9mwOxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzglQKg6777ZTW1jc2t8rblZ3dvf2D6uFR26hMM95iSirdDajhUiS8hQIl76aa0ziQvBOMb2d+54lrI1TygJOU+zEdJiISjKKV2n0ZKjSDas2tu3OQVeIVpAYFmoPqVz9ULIt5gkxSY3qem6KfU42CST6t9DPDU8rGdMh7liY05sbP59dOyZlVQhIpbStBMld/T+Q0NmYSB7Yzpjgyy95M/M/rZRhd+7lI0gx5whaLokwSVGT2OgmF5gzlxBLKtLC3EjaimjK0AVVsCN7yy6ukfVH33Lp3f1lr3BRxlOEETuEcPLiCBtxBE1rA4BGe4RXeHOW8OO/Ox6K15BQzx/AHzucPubePMA==</latexit><latexit sha1_base64="BD1XKLi2MLyNy/k+DR9W3roijvs=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisYD+gDWWz2bRrN9mwOxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzglQKg6777ZTW1jc2t8rblZ3dvf2D6uFR26hMM95iSirdDajhUiS8hQIl76aa0ziQvBOMb2d+54lrI1TygJOU+zEdJiISjKKV2n0ZKjSDas2tu3OQVeIVpAYFmoPqVz9ULIt5gkxSY3qem6KfU42CST6t9DPDU8rGdMh7liY05sbP59dOyZlVQhIpbStBMld/T+Q0NmYSB7Yzpjgyy95M/M/rZRhd+7lI0gx5whaLokwSVGT2OgmF5gzlxBLKtLC3EjaimjK0AVVsCN7yy6ukfVH33Lp3f1lr3BRxlOEETuEcPLiCBtxBE1rA4BGe4RXeHOW8OO/Ox6K15BQzx/AHzucPubePMA==</latexit>

. . .
<latexit sha1_base64="BD1XKLi2MLyNy/k+DR9W3roijvs=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisYD+gDWWz2bRrN9mwOxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzglQKg6777ZTW1jc2t8rblZ3dvf2D6uFR26hMM95iSirdDajhUiS8hQIl76aa0ziQvBOMb2d+54lrI1TygJOU+zEdJiISjKKV2n0ZKjSDas2tu3OQVeIVpAYFmoPqVz9ULIt5gkxSY3qem6KfU42CST6t9DPDU8rGdMh7liY05sbP59dOyZlVQhIpbStBMld/T+Q0NmYSB7Yzpjgyy95M/M/rZRhd+7lI0gx5whaLokwSVGT2OgmF5gzlxBLKtLC3EjaimjK0AVVsCN7yy6ukfVH33Lp3f1lr3BRxlOEETuEcPLiCBtxBE1rA4BGe4RXeHOW8OO/Ox6K15BQzx/AHzucPubePMA==</latexit><latexit sha1_base64="BD1XKLi2MLyNy/k+DR9W3roijvs=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisYD+gDWWz2bRrN9mwOxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzglQKg6777ZTW1jc2t8rblZ3dvf2D6uFR26hMM95iSirdDajhUiS8hQIl76aa0ziQvBOMb2d+54lrI1TygJOU+zEdJiISjKKV2n0ZKjSDas2tu3OQVeIVpAYFmoPqVz9ULIt5gkxSY3qem6KfU42CST6t9DPDU8rGdMh7liY05sbP59dOyZlVQhIpbStBMld/T+Q0NmYSB7Yzpjgyy95M/M/rZRhd+7lI0gx5whaLokwSVGT2OgmF5gzlxBLKtLC3EjaimjK0AVVsCN7yy6ukfVH33Lp3f1lr3BRxlOEETuEcPLiCBtxBE1rA4BGe4RXeHOW8OO/Ox6K15BQzx/AHzucPubePMA==</latexit><latexit sha1_base64="BD1XKLi2MLyNy/k+DR9W3roijvs=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisYD+gDWWz2bRrN9mwOxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzglQKg6777ZTW1jc2t8rblZ3dvf2D6uFR26hMM95iSirdDajhUiS8hQIl76aa0ziQvBOMb2d+54lrI1TygJOU+zEdJiISjKKV2n0ZKjSDas2tu3OQVeIVpAYFmoPqVz9ULIt5gkxSY3qem6KfU42CST6t9DPDU8rGdMh7liY05sbP59dOyZlVQhIpbStBMld/T+Q0NmYSB7Yzpjgyy95M/M/rZRhd+7lI0gx5whaLokwSVGT2OgmF5gzlxBLKtLC3EjaimjK0AVVsCN7yy6ukfVH33Lp3f1lr3BRxlOEETuEcPLiCBtxBE1rA4BGe4RXeHOW8OO/Ox6K15BQzx/AHzucPubePMA==</latexit><latexit sha1_base64="BD1XKLi2MLyNy/k+DR9W3roijvs=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUDwVvHisYD+gDWWz2bRrN9mwOxFK6H/w4kERr/4fb/4bt20O2vpg4PHeDDPzglQKg6777ZTW1jc2t8rblZ3dvf2D6uFR26hMM95iSirdDajhUiS8hQIl76aa0ziQvBOMb2d+54lrI1TygJOU+zEdJiISjKKV2n0ZKjSDas2tu3OQVeIVpAYFmoPqVz9ULIt5gkxSY3qem6KfU42CST6t9DPDU8rGdMh7liY05sbP59dOyZlVQhIpbStBMld/T+Q0NmYSB7Yzpjgyy95M/M/rZRhd+7lI0gx5whaLokwSVGT2OgmF5gzlxBLKtLC3EjaimjK0AVVsCN7yy6ukfVH33Lp3f1lr3BRxlOEETuEcPLiCBtxBE1rA4BGe4RXeHOW8OO/Ox6K15BQzx/AHzucPubePMA==</latexit>

Document-level KGs

Scientific KG

Merging

SciIE
<latexit sha1_base64="dkMVQqBbljsC87ORSph5l08iusY=">AAAB9XicbVDLSgNBEJz1GeMr6tHLYBA8hV0RFE8BEfQW0TwgWcPspDcZMju7zPSqYcl/ePGgiFf/xZt/4+Rx0MSChqKqm+6uIJHCoOt+OwuLS8srq7m1/PrG5tZ2YWe3ZuJUc6jyWMa6ETADUiiookAJjUQDiwIJ9aB/MfLrD6CNiNUdDhLwI9ZVIhScoZXuWwhPaHh2y8X15bBdKLoldww6T7wpKZIpKu3CV6sT8zQChVwyY5qem6CfMY2CSxjmW6mBhPE+60LTUsUiMH42vnpID63SoWGsbSmkY/X3RMYiYwZRYDsjhj0z643E/7xmiuGZnwmVpAiKTxaFqaQY01EEtCM0cJQDSxjXwt5KeY9pxtEGlbcheLMvz5PacclzS97NSbF8Po0jR/bJATkiHjklZXJFKqRKONHkmbySN+fReXHenY9J64Izndkjf+B8/gC0ypKa</latexit><latexit sha1_base64="dkMVQqBbljsC87ORSph5l08iusY=">AAAB9XicbVDLSgNBEJz1GeMr6tHLYBA8hV0RFE8BEfQW0TwgWcPspDcZMju7zPSqYcl/ePGgiFf/xZt/4+Rx0MSChqKqm+6uIJHCoOt+OwuLS8srq7m1/PrG5tZ2YWe3ZuJUc6jyWMa6ETADUiiookAJjUQDiwIJ9aB/MfLrD6CNiNUdDhLwI9ZVIhScoZXuWwhPaHh2y8X15bBdKLoldww6T7wpKZIpKu3CV6sT8zQChVwyY5qem6CfMY2CSxjmW6mBhPE+60LTUsUiMH42vnpID63SoWGsbSmkY/X3RMYiYwZRYDsjhj0z643E/7xmiuGZnwmVpAiKTxaFqaQY01EEtCM0cJQDSxjXwt5KeY9pxtEGlbcheLMvz5PacclzS97NSbF8Po0jR/bJATkiHjklZXJFKqRKONHkmbySN+fReXHenY9J64Izndkjf+B8/gC0ypKa</latexit><latexit sha1_base64="dkMVQqBbljsC87ORSph5l08iusY=">AAAB9XicbVDLSgNBEJz1GeMr6tHLYBA8hV0RFE8BEfQW0TwgWcPspDcZMju7zPSqYcl/ePGgiFf/xZt/4+Rx0MSChqKqm+6uIJHCoOt+OwuLS8srq7m1/PrG5tZ2YWe3ZuJUc6jyWMa6ETADUiiookAJjUQDiwIJ9aB/MfLrD6CNiNUdDhLwI9ZVIhScoZXuWwhPaHh2y8X15bBdKLoldww6T7wpKZIpKu3CV6sT8zQChVwyY5qem6CfMY2CSxjmW6mBhPE+60LTUsUiMH42vnpID63SoWGsbSmkY/X3RMYiYwZRYDsjhj0z643E/7xmiuGZnwmVpAiKTxaFqaQY01EEtCM0cJQDSxjXwt5KeY9pxtEGlbcheLMvz5PacclzS97NSbF8Po0jR/bJATkiHjklZXJFKqRKONHkmbySN+fReXHenY9J64Izndkjf+B8/gC0ypKa</latexit><latexit sha1_base64="dkMVQqBbljsC87ORSph5l08iusY=">AAAB9XicbVDLSgNBEJz1GeMr6tHLYBA8hV0RFE8BEfQW0TwgWcPspDcZMju7zPSqYcl/ePGgiFf/xZt/4+Rx0MSChqKqm+6uIJHCoOt+OwuLS8srq7m1/PrG5tZ2YWe3ZuJUc6jyWMa6ETADUiiookAJjUQDiwIJ9aB/MfLrD6CNiNUdDhLwI9ZVIhScoZXuWwhPaHh2y8X15bBdKLoldww6T7wpKZIpKu3CV6sT8zQChVwyY5qem6CfMY2CSxjmW6mBhPE+60LTUsUiMH42vnpID63SoWGsbSmkY/X3RMYiYwZRYDsjhj0z643E/7xmiuGZnwmVpAiKTxaFqaQY01EEtCM0cJQDSxjXwt5KeY9pxtEGlbcheLMvz5PacclzS97NSbF8Po0jR/bJATkiHjklZXJFKqRKONHkmbySN+fReXHenY9J64Izndkjf+B8/gC0ypKa</latexit>

SciIE
<latexit sha1_base64="dkMVQqBbljsC87ORSph5l08iusY=">AAAB9XicbVDLSgNBEJz1GeMr6tHLYBA8hV0RFE8BEfQW0TwgWcPspDcZMju7zPSqYcl/ePGgiFf/xZt/4+Rx0MSChqKqm+6uIJHCoOt+OwuLS8srq7m1/PrG5tZ2YWe3ZuJUc6jyWMa6ETADUiiookAJjUQDiwIJ9aB/MfLrD6CNiNUdDhLwI9ZVIhScoZXuWwhPaHh2y8X15bBdKLoldww6T7wpKZIpKu3CV6sT8zQChVwyY5qem6CfMY2CSxjmW6mBhPE+60LTUsUiMH42vnpID63SoWGsbSmkY/X3RMYiYwZRYDsjhj0z643E/7xmiuGZnwmVpAiKTxaFqaQY01EEtCM0cJQDSxjXwt5KeY9pxtEGlbcheLMvz5PacclzS97NSbF8Po0jR/bJATkiHjklZXJFKqRKONHkmbySN+fReXHenY9J64Izndkjf+B8/gC0ypKa</latexit><latexit sha1_base64="dkMVQqBbljsC87ORSph5l08iusY=">AAAB9XicbVDLSgNBEJz1GeMr6tHLYBA8hV0RFE8BEfQW0TwgWcPspDcZMju7zPSqYcl/ePGgiFf/xZt/4+Rx0MSChqKqm+6uIJHCoOt+OwuLS8srq7m1/PrG5tZ2YWe3ZuJUc6jyWMa6ETADUiiookAJjUQDiwIJ9aB/MfLrD6CNiNUdDhLwI9ZVIhScoZXuWwhPaHh2y8X15bBdKLoldww6T7wpKZIpKu3CV6sT8zQChVwyY5qem6CfMY2CSxjmW6mBhPE+60LTUsUiMH42vnpID63SoWGsbSmkY/X3RMYiYwZRYDsjhj0z643E/7xmiuGZnwmVpAiKTxaFqaQY01EEtCM0cJQDSxjXwt5KeY9pxtEGlbcheLMvz5PacclzS97NSbF8Po0jR/bJATkiHjklZXJFKqRKONHkmbySN+fReXHenY9J64Izndkjf+B8/gC0ypKa</latexit><latexit sha1_base64="dkMVQqBbljsC87ORSph5l08iusY=">AAAB9XicbVDLSgNBEJz1GeMr6tHLYBA8hV0RFE8BEfQW0TwgWcPspDcZMju7zPSqYcl/ePGgiFf/xZt/4+Rx0MSChqKqm+6uIJHCoOt+OwuLS8srq7m1/PrG5tZ2YWe3ZuJUc6jyWMa6ETADUiiookAJjUQDiwIJ9aB/MfLrD6CNiNUdDhLwI9ZVIhScoZXuWwhPaHh2y8X15bBdKLoldww6T7wpKZIpKu3CV6sT8zQChVwyY5qem6CfMY2CSxjmW6mBhPE+60LTUsUiMH42vnpID63SoWGsbSmkY/X3RMYiYwZRYDsjhj0z643E/7xmiuGZnwmVpAiKTxaFqaQY01EEtCM0cJQDSxjXwt5KeY9pxtEGlbcheLMvz5PacclzS97NSbF8Po0jR/bJATkiHjklZXJFKqRKONHkmbySN+fReXHenY9J64Izndkjf+B8/gC0ypKa</latexit><latexit sha1_base64="dkMVQqBbljsC87ORSph5l08iusY=">AAAB9XicbVDLSgNBEJz1GeMr6tHLYBA8hV0RFE8BEfQW0TwgWcPspDcZMju7zPSqYcl/ePGgiFf/xZt/4+Rx0MSChqKqm+6uIJHCoOt+OwuLS8srq7m1/PrG5tZ2YWe3ZuJUc6jyWMa6ETADUiiookAJjUQDiwIJ9aB/MfLrD6CNiNUdDhLwI9ZVIhScoZXuWwhPaHh2y8X15bBdKLoldww6T7wpKZIpKu3CV6sT8zQChVwyY5qem6CfMY2CSxjmW6mBhPE+60LTUsUiMH42vnpID63SoWGsbSmkY/X3RMYiYwZRYDsjhj0z643E/7xmiuGZnwmVpAiKTxaFqaQY01EEtCM0cJQDSxjXwt5KeY9pxtEGlbcheLMvz5PacclzS97NSbF8Po0jR/bJATkiHjklZXJFKqRKONHkmbySN+fReXHenY9J64Izndkjf+B8/gC0ypKa</latexit>

SciIE
<latexit sha1_base64="dkMVQqBbljsC87ORSph5l08iusY=">AAAB9XicbVDLSgNBEJz1GeMr6tHLYBA8hV0RFE8BEfQW0TwgWcPspDcZMju7zPSqYcl/ePGgiFf/xZt/4+Rx0MSChqKqm+6uIJHCoOt+OwuLS8srq7m1/PrG5tZ2YWe3ZuJUc6jyWMa6ETADUiiookAJjUQDiwIJ9aB/MfLrD6CNiNUdDhLwI9ZVIhScoZXuWwhPaHh2y8X15bBdKLoldww6T7wpKZIpKu3CV6sT8zQChVwyY5qem6CfMY2CSxjmW6mBhPE+60LTUsUiMH42vnpID63SoWGsbSmkY/X3RMYiYwZRYDsjhj0z643E/7xmiuGZnwmVpAiKTxaFqaQY01EEtCM0cJQDSxjXwt5KeY9pxtEGlbcheLMvz5PacclzS97NSbF8Po0jR/bJATkiHjklZXJFKqRKONHkmbySN+fReXHenY9J64Izndkjf+B8/gC0ypKa</latexit><latexit sha1_base64="dkMVQqBbljsC87ORSph5l08iusY=">AAAB9XicbVDLSgNBEJz1GeMr6tHLYBA8hV0RFE8BEfQW0TwgWcPspDcZMju7zPSqYcl/ePGgiFf/xZt/4+Rx0MSChqKqm+6uIJHCoOt+OwuLS8srq7m1/PrG5tZ2YWe3ZuJUc6jyWMa6ETADUiiookAJjUQDiwIJ9aB/MfLrD6CNiNUdDhLwI9ZVIhScoZXuWwhPaHh2y8X15bBdKLoldww6T7wpKZIpKu3CV6sT8zQChVwyY5qem6CfMY2CSxjmW6mBhPE+60LTUsUiMH42vnpID63SoWGsbSmkY/X3RMYiYwZRYDsjhj0z643E/7xmiuGZnwmVpAiKTxaFqaQY01EEtCM0cJQDSxjXwt5KeY9pxtEGlbcheLMvz5PacclzS97NSbF8Po0jR/bJATkiHjklZXJFKqRKONHkmbySN+fReXHenY9J64Izndkjf+B8/gC0ypKa</latexit><latexit sha1_base64="dkMVQqBbljsC87ORSph5l08iusY=">AAAB9XicbVDLSgNBEJz1GeMr6tHLYBA8hV0RFE8BEfQW0TwgWcPspDcZMju7zPSqYcl/ePGgiFf/xZt/4+Rx0MSChqKqm+6uIJHCoOt+OwuLS8srq7m1/PrG5tZ2YWe3ZuJUc6jyWMa6ETADUiiookAJjUQDiwIJ9aB/MfLrD6CNiNUdDhLwI9ZVIhScoZXuWwhPaHh2y8X15bBdKLoldww6T7wpKZIpKu3CV6sT8zQChVwyY5qem6CfMY2CSxjmW6mBhPE+60LTUsUiMH42vnpID63SoWGsbSmkY/X3RMYiYwZRYDsjhj0z643E/7xmiuGZnwmVpAiKTxaFqaQY01EEtCM0cJQDSxjXwt5KeY9pxtEGlbcheLMvz5PacclzS97NSbF8Po0jR/bJATkiHjklZXJFKqRKONHkmbySN+fReXHenY9J64Izndkjf+B8/gC0ypKa</latexit><latexit sha1_base64="dkMVQqBbljsC87ORSph5l08iusY=">AAAB9XicbVDLSgNBEJz1GeMr6tHLYBA8hV0RFE8BEfQW0TwgWcPspDcZMju7zPSqYcl/ePGgiFf/xZt/4+Rx0MSChqKqm+6uIJHCoOt+OwuLS8srq7m1/PrG5tZ2YWe3ZuJUc6jyWMa6ETADUiiookAJjUQDiwIJ9aB/MfLrD6CNiNUdDhLwI9ZVIhScoZXuWwhPaHh2y8X15bBdKLoldww6T7wpKZIpKu3CV6sT8zQChVwyY5qem6CfMY2CSxjmW6mBhPE+60LTUsUiMH42vnpID63SoWGsbSmkY/X3RMYiYwZRYDsjhj0z643E/7xmiuGZnwmVpAiKTxaFqaQY01EEtCM0cJQDSxjXwt5KeY9pxtEGlbcheLMvz5PacclzS97NSbF8Po0jR/bJATkiHjklZXJFKqRKONHkmbySN+fReXHenY9J64Izndkjf+B8/gC0ypKa</latexit>

Figure 3: Knowledge graph construction process.



3224

Figure 4: A part of an automatically constructed
scientific knowledge graph with the most frequent
neighbors of the scientific term statistical machine
translation (SMT) on the graph. For simplicity we
denote Used-for (Reverse) as Uses, Evaluated-for
(Reverse) as Evaluated-by, and replace common
terms with their acronyms. The original graph and
more examples are given Figure 10 in Appendix B.

clusters within one document. Phrases are heuris-
tically normalized (described in Section 6) using
entities and coreference links. In particular, we
link all entities that belong to the same coreference
cluster to replace generic terms with any other non-
generic term in the cluster. Moreover, we replace
all the entities in the cluster with the entity that has
the longest string. Our qualitative analysis shows
that there are fewer ambiguous phrases using coref-
erence links (Figure 5). We calculate the frequency
counts of all entities that appear in the whole cor-
pus. We assign nodes in the knowledge graph by
selecting the most frequent entities (with counts
> k) in the corpus, and merge in any remaining
entities for which a frequent entity is a substring.

Assigning edges (relations) A pair of entities
may appear in different contexts, resulting in differ-
ent relation types between those entities (Figure 6).
For every pair of entities in the graph, we calculate
the frequency of different relation types across the
whole corpus.We assign edges between entities by
selecting the most frequent relation type.

6 Experimental Setup

We evaluate our unified framework SCIIE on SCI-
ERC and SemEval 17. The knowledge graph for

detection

object detection

face detection

human detection

pedestrian detection

action detection

1237

585

258

124

90

87

1297

510

177

84

57

63 With Coref. Without Coref.

Figure 5: Frequency of detected entities with and
without coreferece resolution: using coreference
reduces the frequency of the generic phrase detec-
tion while significantly increasing the frequency of
specific phrases. Linking entities through corefer-
ence helps disambiguate phrases when generating
the knowledge graph.

Conju
nctio

n
Used

for

Used
for (R

evers
e)

0

20

40

60

80
80

10
4#

R
el

at
io

n
Tr

ip
le

s MT-ASR

Hypo
nym

of
Conju

nctio
n
Used

for

Used
for (R

evers
e)

0

10

20

30
25

4
2 2

CRF-GM

Figure 6: Frequency of relation types between pairs
of entities: (left) automatic speech recognition
(ASR) and machine translation (MT), (right) con-
ditional random field (CRF) and graphical model
(GM). We use the most frequent relation between
pairs of entities in the knowledge graph.

scientific community analysis is built using the Se-
mantic Scholar Corpus (110k abstracts in total).

6.1 Baselines
We compare our model with the following base-
lines on SCIERCdataset:

• LSTM+CRF The state-of-the-art NER sys-
tem (Lample et al., 2016), which applies CRF
on top of LSTM for named entity tagging, the
approach has also been used in scientific term
extraction (Luan et al., 2017b).

• LSTM+CRF+ELMo LSTM+CRF with
ELMO as an additional input feature.

• E2E Rel State-of-the-art joint entity and re-
lation extraction system (Miwa and Bansal,
2016) that has also been used in scientific lit-
erature (Peters et al., 2017; Augenstein et al.,
2017). This system uses syntactic features
such as part-of-speech tagging and depen-
dency parsing.



3225

• E2E Rel(Pipeline) Pipeline setting of E2E
Rel. Extract entities first and use entity results
as input to relation extraction task.

• E2E Rel+ELMo E2E Rel with ELMO as an
additional input feature.

• E2E Coref State-of-the-art coreference sys-
tem Lee et al. (2017) combined with ELMO.
Our system SCIIE extends E2E Coref with
multi-task learning.

In the SemEval task, we compare our model
SCIIE with the best reported system in the SemEval
leaderboard (Peters et al., 2017), which extends
E2E Rel with several in-domain features such as
gazetteers extracted from existing knowledge bases
and model ensembles. We also compare with the
state of the art on keyphrase extraction (Luan et al.,
2017b), which applies semi-supervised methods to
a neural tagging model.3

6.2 Implementation details
Our system extends the implementation and hyper-
parameters from Lee et al. (2017) with the follow-
ing adjustments. We use a 1 layer BiLSTM with
200-dimensional hidden layers. All the FFNNs
have 2 hidden layers of 150 dimensions each. We
use 0.4 variational dropout (Gal and Ghahramani,
2016) for the LSTMs, 0.4 dropout for the FFNNs,
and 0.5 dropout for the input embeddings. We
model spans up to 8 words. For beam pruning,
we use λC = 0.3 for coreference resolution and
λR = 0.4 for relation extraction. For constructing
the knowledge graph, we use the following heuris-
tics to normalize the entity phrases. We replace all
acronyms with their corresponding full name and
normalize all the plural terms with their singular
counterparts.

7 Experimental Results

We evaluate SCIIE on SCIERC and SemEval 17
datasets. We provide qualitative results and human
evaluation of the constructed knowledge graph.

7.1 IE Results
Results on SciERC Table 2 compares the result
of our model with baselines on the three tasks: en-
tity recognition (Table 2a), relation extraction (Ta-
ble 2b), and coreference resolution (Table 2c). As
evidenced by the table, our unified multi-task setup

3We compare with the inductive setting results.

Dev Test

Model P R F1 P R F1

LSTM+CRF 67.2 65.8 66.5 62.9 61.1 62.0
LSTM+CRF+ELMo 68.1 66.3 67.2 63.8 63.2 63.5
E2E Rel(Pipeline) 66.7 65.9 66.3 60.8 61.2 61.0
E2E Rel 64.3 68.6 66.4 60.6 61.9 61.2
E2E Rel+ELMO 67.5 66.3 66.9 63.5 63.9 63.7
SCIIE 70.0 66.3 68.1 67.2 61.5 64.2

(a) Entity recognition.

Dev Test

Model P R F1 P R F1

E2E Rel(Pipeline) 34.2 33.7 33.9 37.8 34.2 35.9
E2E Rel 37.3 33.5 35.3 37.1 32.2 34.1
E2E Rel+ELMO 38.5 36.4 37.4 38.4 34.9 36.6
SCIIE 45.4 34.9 39.5 47.6 33.5 39.3

(b) Relation extraction.

Dev Test

Model P R F1 P R F1

E2E Coref 59.4 52.0 55.4 60.9 37.3 46.2
SCIIE 61.5 54.8 58.0 52.0 44.9 48.2

(c) Coreference resolution.

Table 2: Comparison with previous systems on
the development and test set for our three tasks.
For coreference resolution, we report the average
P/R/F1 of MUC, B3, and CEAFφ4 scores.

SCIIE outperforms all the baselines. For entity
recognition, our model achieves 1.3% and 2.4%
relative improvement over LSTM+CRF with and
without ELMO, respectively. Moreover, it achieves
1.8% and 2.7% relative improvement over E2E Rel
with and without ELMO, respectively. For rela-
tion extraction, we observe more significant im-
provement with 13.1% relative improvement over
E2E Rel and 7.4% improvement over E2E Rel with
ELMO. For coreference resolution, SCIIE outper-
forms E2E Coref with 4.5% relative improvement.
We still observe a large gap between human-level
performance and a machine learning system. We
invite the community to address this challenging
task.

Ablations We evaluate the effect of multi-task
learning in each of the three tasks defined in our
dataset. Table 3 reports the results for individual
tasks when additional tasks are included in the
learning objective function. We observe that per-
formance improves with each added task in the
objective. For example, Entity recognition (65.7)
benefits from both coreference resolution (67.5)
and relation extraction (66.8). Relation extrac-



3226

Task Entity Rec. Relation Coref.

Multi Task (SCIIE) 68.1 39.5 58.0

Single Task 65.7 37.9 55.3
+Entity Rec. - 38.9 57.1
+Relation 66.8 - 57.6
+Coreference 67.5 39.5 -

Table 3: Ablation study for multitask learning on
SCIERC development set. Each column shows
results for the target task.

tion (37.9) significantly benefits when multi-tasked
with coreference resolution (7.1% relative improve-
ment). Coreference resolution benefits when multi-
tasked with relation extraction, with 4.9% relative
improvement.

Results on SemEval 17 Table 4 compares the
results of our model with the state of the art on the
SemEval 17 dataset for tasks of span identification,
keyphrase extraction and relation extraction as well
as the overall score. Span identification aims at
identifying spans of entities. Keyphrase classifi-
cation and relation extraction has the same setting
with the entity and relation extraction in SCIERC.
Our model outperforms all the previous models
that use hand-designed features. We observe more
significant improvement in span identification than
keyphrase classification. This confirms the bene-
fit of our model in enumerating spans (rather than
BIO tagging in state-of-the-art systems). More-
over, we have competitive results compared to the
previous state of the art in relation extraction. We
observe less gain compared to the SCIERC dataset
mainly because there are no coference links, and
the relation types are not comprehensive.

7.2 Knowledge Graph Analysis

We provide qualitative analysis and human evalua-
tions on the constructed knowledge graph.

Scientific trend analysis Figure 7 shows the his-
torical trend analysis (from 1996 to 2016) of the
most popular applications of the phrase neural net-
work, selected according to the statistics of the
extracted relation triples with the ‘Used-for’ rela-
tion type from speech, computer vision, and NLP
conference papers. We observe that, before 2000,
neural network has been applied to a greater per-
centage of speech applications compared to the
NLP and computer vision papers. In NLP, neural
networks first gain popularity in language modeling

1,995 2,000 2,005 2,010 2,015

0

0.2

0.4

0.6 Language Modeling
Machine Translation

POS Tagging

1,995 2,000 2,005 2,010 2,015

0

0.2

0.4

0.6 Speech Recognition
Speech Synthesis

Speaker Recognition

1995 2000 2005 2010 2015

0

0.2

0.4
Object Recognition

Object Detection
Image Segmentation

Figure 7: Historical trend for top applications of the
keyphrase neural network in NLP, speech, and CV
conference papers we collected. y-axis indicates
the ratio of papers that use neural network in the
task to the number of papers that is about the task.

0 20 40 60 80 100

84

86

88

90

92

Pseudo-recall %

Pr
ec

is
io

n
%

With Coref.
Without Coref.

Figure 8: Precision/pseudo-recall curves for human
evaluation by varying cut-off thresholds. The AUC
is 0.751 with coreference, and 0.695 without.

and then extend to other tasks such as POS Tag-
ging and Machine Translation. In computer vision,
the application of neural networks gains popularity
in object recognition earlier (around 2010) than
the other two more complex tasks of object detec-
tion and image segmentation (hardest and also the
latest).

Knowledge Graph Evaluation Figure 8 shows
the human evaluation of the constructed knowl-
edge graph, comparing the quality of automatically
generated knowledge graphs with and without the
coreference links. We randomly select 10 frequent
scientific entities and extract all the relation triples
that include one of the selected entities leading to
1.5k relation triples from both systems. We ask
four domain experts to annotate each of these ex-



3227

Span Indentification Keyphrase Extraction Relation Extraction Overall

Model P R F1 P R F1 P R F1 P R F1

(Luan 2017) - - 56.9 - - 45.3 - - - - - -
Best SemEval 55 54 55 44 43 44 36 23 28 44 41 43
SCIIE 62.2 55.4 58.6 48.5 43.8 46.0 40.4 21.2 27.8 48.1 41.8 44.7

Table 4: Results for scientific keyphrase extraction and extraction on SemEval 2017 Task 10, comparing
with previous best systems.

tracted relations to define ground truth labels. Each
domain expert is assigned 2 or 3 entities and all of
the corresponding relations. Figure 8 shows preci-
sion/recall curves for both systems. Since it is not
feasible to compute the actual recall of the systems,
we compute the pseudo-recall (Zhang et al., 2015)
based on the output of both systems. We observe
that the knowledge graph curve with coreference
linking is mostly above the curve without corefer-
ence linking. The precision of both systems is high
(above 84% for both systems), but the system with
coreference links has significantly higher recall.

8 Conclusion

In this paper, we create a new dataset and develop a
multi-task model for identifying entities, relations,
and coreference clusters in scientific articles. By
sharing span representations and leveraging cross-
sentence information, our multi-task setup effec-
tively improves performance across all tasks. More-
over, we show that our multi-task model is better at
predicting span boundaries and outperforms previ-
ous state-of-the-art scientific IE systems on entity
and relation extraction, without using any hand-
engineered features or pipeline processing. Using
our model, we are able to automatically organize
the extracted information from a large collection
of scientific articles into a knowledge graph. Our
analysis shows the importance of coreference links
in making a dense, useful graph.

We still observe a large gap between the perfor-
mance of our model and human performance, con-
firming the challenges of scientific IE. Future work
includes improving the performance using semi-
supervised techniques and providing in-domain
features. We also plan to extend our multi-task
framework to information extraction tasks in other
domains.

Acknowledgments

This research was supported by the Office of Naval
Research under the MURI grant N00014-18-1-

2670, NSF (IIS 1616112, III 1703166), Allen Dis-
tinguished Investigator Award, and gifts from Allen
Institute for AI, Google, Amazon, and Bloomberg.
We are grateful to Waleed Ammar and AI2 for
sharing the Semantic Scholar Corpus. We also
thank the anonymous reviewers, UW-NLP group
and Shoou-I Yu for their helpful comments.

References
Amjad Abu-Jbara and Dragomir Radev. 2011. Co-

herent citation-based summarization of scientific pa-
pers. In Proc. Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies. volume 1, pages 500–509.

Heike Adel and Hinrich Schütze. 2017. Global normal-
ization of convolutional neural networks for joint en-
tity and relation classification. In Proc. Conf. Empir-
ical Methods Natural Language Process. (EMNLP).
pages 1723–1729.

Waleed Ammar, Dirk Groeneveld, Chandra Bhagavat-
ula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-
son Dunkelberger, Ahmed Elgohary, Sergey Feld-
man, Vu Ha, et al. 2018. Construction of the litera-
ture graph in semantic scholar. In Proc. Conf. North
American Assoc. for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT), (Indus-
try Papers). pages 84–91.

Waleed Ammar, Matthew Peters, Chandra Bhagavat-
ula, and Russell Power. 2017. The ai2 system at
semeval-2017 task 10 (scienceie): semi-supervised
end-to-end entity and relation extraction. In Proc.
Int. Workshop on Semantic Evaluation (SemEval).
pages 592–596.

Ashton Anderson, Dan McFarland, and Dan Jurafsky.
2012. Towards a computational history of the ACL:
1980-2008. In Proc. ACL Special Workshop on Re-
discovering 50 Years of Discoveries. pages 13–21.

Awais Athar and Simone Teufel. 2012a. Context-
enhanced citation sentiment detection. In Proc.
Conf. North American Assoc. for Computational Lin-
guistics: Human Language Technologies (NAACL-
HLT). pages 597–601.

Awais Athar and Simone Teufel. 2012b. Detection of
implicit citations for sentiment detection. In Proc.



3228

ACL Workshop on Detecting Structure in Scholarly
Discourse. pages 18–26.

Isabelle Augenstein, Mrinal Das, Sebastian Riedel,
Lakshmi Vikraman, and Andrew McCallum. 2017.
Semeval 2017 task 10: ScienceIE - extracting
keyphrases and relations from scientific publications.
In Proc. Int. Workshop on Semantic Evaluation (Se-
mEval).

Isabelle Augenstein and Anders Søgaard. 2017. Multi-
task learning of keyphrase boundary classification.
In Proc. Annu. Meeting Assoc. for Computational
Linguistics (ACL). pages 341–346.

Kevin Clark and Christopher D. Manning. 2016.
Improving coreference resolution by learning
entity-level distributed representations. CoRR
abs/1606.01323.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Proc.
Int. Conf. Machine Learning (ICML). pages 160–
167.

Huy Hoang Nhat Do, Muthu Kumar Chandrasekaran,
Philip S Cho, and Min Yen Kan. 2013. Extracting
and matching authors and affiliations in scholarly
documents. In Proc. ACM/IEEE-CS Joint Confer-
ence on Digital libraries. pages 219–228.

Kata Gábor, Davide Buscaldi, Anne-Kathrin Schu-
mann, Behrang QasemiZadeh, Haı̈fa Zargayouna,
and Thierry Charnois. 2018. Semeval-2018 Task 7:
Semantic relation extraction and classification in sci-
entific papers. In Proc. Int. Workshop on Semantic
Evaluation (SemEval).

Kata Gabor, Haifa Zargayouna, Davide Buscaldi, Is-
abelle Tellier, and Thierry Charnois. 2016. Se-
mantic annotation of the ACL anthology corpus for
the automatic analysis of scientific literature. In
Proc. Language Resources and Evaluation Confer-
ence (LREC).

Kata Gábor, Haı̈fa Zargayouna, Isabelle Tellier, Davide
Buscaldi, and Thierry Charnois. 2016. Unsuper-
vised relation extraction in specialized corpora using
sequence mining. In International Symposium on In-
telligent Data Analysis. Springer, pages 237–248.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Proc. Annu. Conf. Neural In-
form. Process. Syst. (NIPS).

Sonal Gupta and Christopher D Manning. 2011. An-
alyzing the dynamics of research by extracting key
aspects of scientific papers. In Proc. IJCNLP. pages
1–9.

Luheng He, Kenton Lee, Omer Levy, and Luke Zettle-
moyer. 2018. Jointly predicting predicates and argu-
ments in neural semantic role labeling. In ACL.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation 9(8):1735–
1780.

Kokil Jaidka, Muthu Kumar Chandrasekaran, Beat-
riz Fisas Elizalde, Rahul Jha, Christopher Jones,
Min-Yen Kan, Ankur Khanna, Diego Molla-Aliod,
Dragomir R Radev, Francesco Ronzano, et al. 2014.
The computational linguistics summarization pilot
task. In Proc. Text Analysis Conference.

Miray Kas. 2011. Structures and statistics of citation
networks. Technical report, DTIC Document.

Arzoo Katiyar and Claire Cardie. 2017. Going out
on a limb: Joint extraction of entity mentions
and relations without dependency trees. In Proc.
Annu. Meeting Assoc. for Computational Linguistics
(ACL). volume 1, pages 917–928.

Sigrid Klerke, Yoav Goldberg, and Anders Søgaard.
2016. Improving sentence compression by learning
to predict gaze. In HLT-NAACL.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proc. Conf. North American Assoc. for Compu-
tational Linguistics (NAACL).

Kenton Lee, Luheng He, Mike Lewis, and Luke S.
Zettlemoyer. 2017. End-to-end neural coreference
resolution. In EMNLP.

Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018.
Higher-order coreference resolution with coarse-to-
fine inference. In NAACL.

Yi Luan, Chris Brockett, Bill Dolan, Jianfeng Gao,
and Michel Galley. 2017a. Multi-task learning for
speaker-role adaptation in neural conversation mod-
els. In Proc. IJCNLP.

Yi Luan, Yangfeng Ji, Hannaneh Hajishirzi, and
Boyang Li. 2016. Multiplicative representations
for unsupervised semantic role induction. In Proc.
Annu. Meeting Assoc. for Computational Linguistics
(ACL). page 118.

Yi Luan, Mari Ostendorf, and Hannaneh Hajishirzi.
2017b. Scientific information extraction with semi-
supervised neural tagging. In Proc. Conf. Empirical
Methods Natural Language Process. (EMNLP).

Yi Luan, Mari Ostendorf, and Hannaneh Hajishirzi.
2018. The uwnlp system at semeval-2018 task 7:
Neural relation extraction model with selectively in-
corporated concept embeddings. In Proc. Int. Work-
shop on Semantic Evaluation (SemEval). pages 788–
792.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Proc. Annu. Meeting Assoc. for Com-
putational Linguistics (ACL). pages 1105–1116.



3229

Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina
Toutanova, and Wen-tau Yih. 2017. Cross-sentence
n-ary relation extraction with graph lstms. Trans.
Assoc. for Computational Linguistics (TACL) 5:101–
115.

Matthew Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language models.
In Proc. Annu. Meeting Assoc. for Computational
Linguistics (ACL). volume 1, pages 1756–1765.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In NAACL.

Behrang QasemiZadeh and Anne-Kathrin Schumann.
2016. The ACL RD-TEC 2.0: A language resource
for evaluating term extraction and entity recognition
methods. In LREC.

Chris Quirk and Hoifung Poon. 2017. Distant su-
pervision for relation extraction beyond the sen-
tence boundary. In Proc. European Chapter Assoc.
for Computational Linguistics (EACL). pages 1171–
1182.

Marek Rei. 2017. Semi-supervised multitask learning
for sequence labeling. In Proc. Annu. Meeting As-
soc. for Computational Linguistics (ACL).

Yanchuan Sim, Noah A Smith, and David A Smith.
2012. Discovering factions in the computational lin-
guistics community. In Proc. ACL Special Workshop
on Rediscovering 50 Years of Discoveries. pages 22–
32.

Sameer Singh, Sebastian Riedel, Brian Martin, Jiaping
Zheng, and Andrew McCallum. 2013. Joint infer-
ence of entities, relations, and coreference. In Proc.
of the 2013 workshop on Automated knowledge base
construction. ACM, pages 1–6.

Pontus Stenetorp, Sampo Pyysalo, Goran Topić,
Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsu-
jii. 2012. Brat: a web-based tool for nlp-assisted
text annotation. In Proc. European Chapter Assoc.
for Computational Linguistics (EACL). pages 102–
107.

Swabha Swayamdipta, Sam Thomson, Chris Dyer, and
Noah A. Smith. 2017. Frame-semantic parsing with
softmax-margin segmental rnns and a syntactic scaf-
fold. CoRR abs/1706.09528.

Chen-Tse Tsai, Gourab Kundu, and Dan Roth. 2013.
Concept-based analysis of scientific literature. In
Proc. ACM Int. Conference on Information & Knowl-
edge Management. ACM, pages 1733–1738.

Adam Vogel and Dan Jurafsky. 2012. He said, she said:
Gender in the ACL anthology. In Proc. ACL Special
Workshop on Rediscovering 50 Years of Discoveries.
pages 33–41.

Sam Wiseman, Alexander M. Rush, and Stuart M.
Shieber. 2016. Learning global features for coref-
erence resolution. In HLT-NAACL.

Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen,
Yangyang Lu, and Zhi Jin. 2016. Improved rela-
tion classification by deep recurrent neural networks
with data augmentation. In Proc. Int. Conf. Compu-
tational Linguistics (COLING). pages 1461–1470.

Congle Zhang, Stephen Soderland, and Daniel S. Weld.
2015. Exploiting parallel news streams for unsuper-
vised event extraction. TACL 3:117–129.

Meishan Zhang, Yue Zhang, and Guohong Fu. 2017.
End-to-end neural relation extraction with global op-
timization. In Proc. Conf. Empirical Methods Natu-
ral Language Process. (EMNLP). pages 1730–1740.

Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing
Hao, Peng Zhou, and Bo Xu. 2017. Joint extrac-
tion of entities and relations based on a novel tag-
ging scheme. In Proc. Annu. Meeting Assoc. for
Computational Linguistics (ACL). volume 1, pages
1227–1236.



3230

A Annotation Guideline

A.1 Entity Category
• Task: Applications, problems to solve, sys-

tems to construct.

E.g. information extraction, machine reading
system, image segmentation, etc.

• Method: Methods , models, systems to use,
or tools, components of a system, frameworks.

E.g. language model, CORENLP, POS parser,
kernel method, etc.

• Evaluation Metric: Metrics, measures, or
entities that can express quality of a sys-
tem/method.

E.g. F1, BLEU, Precision, Recall, ROC curve,
mean reciprocal rank, mean-squared error, ro-
bustness, time complexity, etc.

• Material: Data, datasets, resources, Corpus,
Knowledge base.

E.g. image data, speech data, stereo images,
bilingual dictionary, paraphrased questions,
CoNLL, Panntreebank, WordNet, Wikipedia,
etc.

• Evaluation Metric: Metric measure or term
that can express quality of a system/method.

E.g. F1, BLEU, Precision, Recall, ROC
curve, mean reciprocal rank, mean-squared
error,robustness, compile time, time complex-
ity...

• Generic: General terms or pronouns that may
refer to a entity but are not themselves infor-
mative, often used as connection words.

E.g model, approach, prior knowledge, them,
it...

A.2 Relation Category
Relation link can not go beyond sentence boundary.
We define 4 asymmetric relation types (Used-for,
Feature-of, Hyponym-of, Part-of ), together with 2
symmetric relation types (Compare, Conjunction).
B always points to A for asymmetric relations

• Used-for: B is used for A, B models A, A is
trained on B, B exploits A, A is based on B.
E.g.

The TISPER system has been designed
to enable many text applications.

Our method models user proficiency.
Our algorithms exploits local soothness.

• Feature-of: B belongs to A, B is a feature of
A, B is under A domain. E.g.

prior knowledge of the model
genre-specific regularities of discourse
structure
English text in science domain

• Hyponym-of: B is a hyponym of A, B is a
type of A. E.g.

TUIT is a software library
NLP applications such as machine trans-
lation and language generation

• Part-of: B is a part of A... E.g.

The system includes two models: speech
recognition and natural language under-
standing
We incorporate NLU module to the sys-
tem.

• Compare: Symmetric relation (use blue to
denote entity). Opposite of conjunction, com-
pare two models/methods, or listing two op-
posing entities. E.g.

Unlike the quantitative prior, the qualita-
tive prior is often ignored...
We compare our system with previous
sequential tagging systems...

• Conjunction: Symmetric relation (use blue
to denote entity). Function as similar role or
use/incorporate with. E.g.

obtained from human expert or knowl-
edge base
NLP applications such as machine trans-
lation and language generation

A.3 Coreference
Two Entities that points to the same concept.

• Anaphora and Cataphora:

We introduce a machine reading system...
The system...
The prior knowledge include...Such
knowledge can be applied to...

• Coreferring noun phrase:

We develop a part-of-speech tagging sys-
tem...The POS tagger...



3231

A.4 Notes
1. Entity boundary annotation follows the

ACL RD-TEC Annotation Guideline (Qasem-
iZadeh and Schumann, 2016), with the exten-
tion that spans can be embedded in longer
spans, only if the shorter span is involved in a
relation.

2. Do not include determinators (such as the, a),
or adjective pronouns (such as this,its, these,
such) to the span. If generic phrases are not
involved in a relation, do not tag them.

3. Do not tag relation if one entity is:

• Variable bound:
We introduce a neural based approach..
Its benefit is...
• The word which:

We introduce a neural based approach,
which is a...

4. Do not tag coreference if the entity is

• Generically-used Other-ScientificTerm:
...advantage gained from local smooth-
ness which... We present algorithms ex-
ploiting local smoothness in more aggres-
sive ways...
• Same scientific term but refer to different

examples:
We use a data structure, we also use an-
other data structure...

5. Do not label negative relations:

X is not used in Y or X is hard to be applied
in Y

B Annotation and Knowledge Graph
Examples

Here we take a screen shot of the BRAT interface
for an ACL paper in Figure 9. We also attach the
original figure of Figure 3 in Figure 10. More
examples can be found in the project website4.

4http://nlp.cs.washington.edu/sciIE/

http://nlp.cs.washington.edu/sciIE/


3232

Figure 9: Annotation example 1 from ACL

Figure 10: An example of our automatically generated knowledge graph centered on statistical machine
translation. This is the original figure of Figure 4.


