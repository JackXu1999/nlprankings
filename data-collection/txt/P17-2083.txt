



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–529
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2083

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–529
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2083

A Generative Attentional Neural Network Model for Dialogue Act
Classification

Quan Hung Tran and Ingrid Zukerman and Gholamreza Haffari
Faculty of Information Technology

Monash University, Australia
hung.tran,ingrid.zukerman,gholamreza.haffari@monash.edu

Abstract

We propose a novel generative neural net-
work architecture for Dialogue Act clas-
sification. Building upon the Recurrent
Neural Network framework, our model in-
corporates a new attentional technique and
a label-to-label connection for sequence
learning, akin to Hidden Markov Mod-
els. Our experiments show that both of
these innovations enable our model to out-
perform strong baselines for dialogue-act
classification on the MapTask and Switch-
board corpora. In addition, we analyse
empirically the effectiveness of each of
these innovations.

1 Introduction

Dialogue Act (DA) classification is a sequence-
to-sequence learning task where a sequence of
utterances is mapped into a sequence of DAs.
Some works in DA classification treat each ut-
terance as an independent instance (Julia et al.,
2010; Gambäck et al., 2011), which leads to ig-
noring important long-range dependencies in the
dialogue history. Other works have captured
inter-utterance relationships using models such as
Hidden Markov Models (HMMs) (Stolcke et al.,
2000; Surendran and Levow, 2006) or Recur-
rent Neural Networks (RNNs) (Kalchbrenner and
Blunsom, 2013; Ji et al., 2016), where RNNs have
been particularly successful.

In this paper, we present a generative model of
utterances and dialogue acts which conditions on
the relevant part of the dialogue history. To this
effect, we use the attention mechanism (Bahdanau
et al., 2014) developed originally for sequence-to-
sequence models, which has proven effective in
Machine Translation (Bahdanau et al., 2014; Lu-
ong et al., 2015) and DA classification (Shen and

Lee, 2016). The intuition is that different parts of
an input sequence have different levels of impor-
tance with respect to the objective, and this mech-
anism enables the selection of the important parts.
However, the traditional attention mechanism suf-
fers from the attention-bias problem (Wang et al.,
2016), where the attention mechanism tends to fa-
vor the inputs at the end of a sequence. To address
this problem, we propose a gated attention mech-
anism, where the attention signal is represented as
a gate over the input vector.

In addition, when generating a dialogue act, we
capture its direct dependence on the previous di-
alogue act — a reasonable source of information,
which, surprisingly, has not been explored in the
RNN literature for DA classification.

Our experiments show that our model signifi-
cantly outperforms variants that do not have our
innovations, i.e., the gated attention mechanism
and direct label-to-label dependency.

2 Model Description

Assume that we have a training dataset D com-
prising a collection of dialogues, where each dia-
logue consists of a sequence of utterances {yt}Tt=1
and the corresponding sequence of dialogue acts
{zt}Tt=1. Each utterance yt is a sequence of to-
kens, and its n-th token is denoted yt,n.

We propose a generative neural model for
dialogue PΘΘΘ(y1:T , z1:T ), which specifies a
joint probability distribution over a sequence
of utterances y1:T and the corresponding
sequence of dialogue acts z1:T . This gener-
ative model is then trained discriminatively
by maximising the conditional log-likelihood∑

(z1:T ,y1:T )∈D logPΘΘΘ(z1:T |y1:T ):

arg max
ΘΘΘ

∑

(y1:T ,z1:T )∈D
log

PΘΘΘ(y1:T , z1:T )∑
z′1:T

PΘΘΘ(y1:T , z
′
1:T )

524

https://doi.org/10.18653/v1/P17-2083
https://doi.org/10.18653/v1/P17-2083


Figure 1: Graphical model representation of our model. Red connections depict dialogue-act genera-
tion (1); purple connections (dashed and continuous) depict utterance generation (2).

where ΘΘΘ represents all neural network param-
eters. Discriminative training is employed in
order to match the use of the model for pre-
dicting dialogue acts during test time, using
arg maxz′1:T PΘΘΘ(z

′
1:T |y1:T ).

The generative story of our model is as follows:
(1) generate the dialogue act of the current dia-
logue turn conditioned on the previous dialogue
act and the previous utterance PΘΘΘ(zt|zt−1,yt−1);
and (2) generate the current utterance condi-
tioned on the previous utterance and the current
dialogue act PΘΘΘ(yt|zt,yt−1). In other words,
PΘΘΘ(z1:T ,y1:T ) is decomposed as:

T∏

t=1

PΘΘΘ(zt|zt−1,yt−1)PΘΘΘ(yt|zt,yt−1). (1)

Furthermore, each utterance is generated by a
sequential process whereby each token yt,n is con-
ditioned on all the previously generated tokens
yt,<n, as well as the external conditioning context
consisting of the dialogue act zt and the previous
turn’s utterance yt−1, i.e.,

PΘΘΘ(yt|zt,yt−1) =
|yt|∏

n=1

PΘΘΘ(yt,n|yt,<n, zt,yt−1).

(2)

Importantly, the decomposition of the joint dis-
tribution in Equation 1 allows dynamic program-
ming for exact decoding (§2.2). One possible
extension of our framework is to investigate a
higher-order Markov model, although one needs
to be conscious about the trade-off between the in-
crease in the computational complexity of train-
ing/decoding with higher-order Markov models
versus the potential gain in classification quality.

We now turn our attention to the neural architec-
ture used to realise the components of our prob-
abilistic model (Figure 1). We define the neural

model for the conditional probability of the next
dialogue act as follows:

PΘΘΘ(zt|zt−1,yt−1) =
softmax(W (zt−1)cz ct + b

(zt−1)
z ),

(3)

where ct is the context vector summarising the in-
formation from the previous utterance yt−1, and
W

(zt−1)
cz and b

(zt−1)
z are the softmax parameter

gated on the previous dialogue act zt−1. Due to
gating, the number of parameters of the model
may increase significantly; therefore, we have also
explored a variant where only the bias term b(zt−1)z
is gated. We define the neural model for generat-
ing the tokens of the current utterance as follows:

PΘΘΘ(yt,n|yt,<n, zt,yt−1) =
softmax(W (zt)hy ht,n−1 + Wcct + by),

(4)

where the weight matrix W (zt)hy is gated based
on zt, ct summarises the previous utterance, and
ht,n−1 is the state of an utterance-level RNN sum-
marising all the previously generated tokens:

ht,n−1 = fff(ht,n−2,EEEyt,n−1), (5)

where EEEyt,n−1 provides the embedding of the to-
ken yt,n−1 from the embedding table EEE, and fff
can be any non-linear function, i.e., the sim-
ple sigmoid applied to elements of a vec-
tor, or the more complex Long-Short-Term-
Memory unit (LSTM) (Graves, 2013; Hochreiter
and Schmidhuber, 1997), or the Gated-Recurrent-
Unit (GRU) (Chung et al., 2014; Cho et al., 2014).

In what follows, we elaborate on how to best
summarise the information from the previous ut-
terance in ct, and how to decode for the best se-
quence of dialogue acts given a trend model.

525



2.1 The Gated Attention Mechanism
Given a sequence of words in an utterance
{y1, . . . , yn}, we would like to compress its infor-
mation in c, which is then used in the conditioning
contexts of other components of the model. Typ-
ically, the last hidden state of the utterance-level
RNN is taken to be the summary vector: c = hn.
However, it has been shown that attending to all
RNN states is more effective.

The traditional attention mechanism (Bahdanau
et al., 2014) employs a probability vector a over
the words of the input utterance to summarise it.
The attention elements in a are typically calcu-
lated from the current input yn, and the previous
hidden state hn−1:

αn = g(hn−1,EEEyn) , an =
eαn∑n

n′=1 e
αn′

,

where g is a non-linear function. Once the atten-
tion is defined, the representation of the input is
constructed as

c =
∑

n

anhhhn. (6)

The problem with this traditional attention
model is that the final hidden state is a function
of all the inputs, hence it is usually more “infor-
mative” than the earlier hidden states due to se-
mantic accumulation (Wang et al., 2016). Thus,
most of the attention signal is assigned to the hid-
den states toward the end of a sequence. In DA
classification, this may not be desirable, since an
important token with respect to a dialogue act can
appear anywhere in an utterance. We call this the
attention bias problem.

We propose a novel gated attention mechanism,
which is inspired by the gating mechanism in
LSTMs, to fix the attention bias problem. Simi-
lar to the forget gate of LSTMs, we use the avail-
able information to calculate an attention gate that
learns whether to allow the whole input signal to
pass through or to forget all or a part of the input
signal:

an = ggg(hn−1,EEEyn) (7)

xn = an �EEEyn (8)
hn = fff(hn−1,xn) (9)

where � represents element-wise multiplication.
After filtering the important signal from the in-

put token, the information from our tokens is accu-
mulated in the last hidden state of the RNN, which

we take as the summary vector c = hhhn. Note that
since the gated attention is applied to the input be-
fore the RNN calculations, it is not affected by the
attention bias.

2.2 Inference: Viterbi Decoding

For prediction, we choose the sequence of dia-
logue acts with the highest posterior probability:

arg max
z′1:T

PΘΘΘ(z
′
1:T |y1:T )=arg max

z′1:T
PΘΘΘ(z

′
1:T ,y1:T )

Since the joint probability is decomposed fur-
ther according to Equation 1, we can make use of
dynamic programming to find the highest prob-
ability sequence of dialogue acts. Specifically,
the model endows each latent variable zt with a
unary potential PΘΘΘ(yt|zt,yt−1) and binary poten-
tial PΘΘΘ(zt|zt−1,yt−1) functions. PΘΘΘ(yt|zt,yt−1)
and PΘΘΘ(zt|zt−1,yt−1) are akin to the emission and
transition functions of an HMM, and are calcu-
lated using Equations 2 and 3 respectively. Fur-
thermore, the model has been carefully designed
so that the hidden states in the RNNs encod-
ing the utterances to form the context vector ct
(the representation of the previous utterance) are
not affected by the sequence of dialogue acts,
which is crucial to making the inference amenable
to dynamic programming. The resulting infer-
ence algorithm is akin to the Viterbi algorithm for
HMMs.

3 Experiments

Datasets. We conduct our experiments on the
MapTask and Switchboard corpora. The MapTask
Dialog Act corpus (Anderson et al., 1991) con-
sists of 128 conversations and more than 27000
utterances in an instruction-giving scenario. There
are 13 DA types in this corpus. For the experi-
ments, the available data is split into three parts,
train/test/validation with 103, 13 and 12 conversa-
tions respectively.

The Switchboard Dialog Act corpus (Jurafsky
et al., 1997) consists of 1155 transcribed telephone
conversations with around 205000 utterances. In
contrast with the MapTask conversations, which
are task-oriented, the Switchboard corpus con-
sists mostly of general topic conversations. The
Switchboard tag set has 42 DAs.1

1The original size of the tag set for Switchboard is 226,
which was then collapsed into 42

526



without gate bias gate all
HMM HMM HMM

no attn. 60.97% 64.60% 63.55%
traditional 61.72% 64.73% 65.19%
gated attn. 62.21% 65.94% 65.94%

Table 1: Comparison of our model variants on the
MapTask corpus.

Baselines. On MapTask, to the best of our
knowledge, there is no standard data split, thus, we
make the comparison against our implementation
of strong baselines such as HMM-trigram (Stol-
cke et al., 2000) and instance-based random forest
classifier (1/2/3-gram features). Ji et al.’s (2016)
results for this corpus are obtained by running
their publicly available code with the same hyper
parameters as those used by our models. We also
report the results of Julia et al. (2010)2 and Suren-
dran et al. (2006). However, the experimental
setup of these two works differs from ours, hence
their results are not directly comparable to ours.

On Switchboard, we compare our results with
strong baselines using the experimental setup from
Kalchbrenner and Blunsom (2013) and Stolcke et
al. (2000).3

Our Model Configurations. We experiment
with several variants of our model to explore
the effectiveness of our two improvements: the
HMM-like connection and the gated attention
mechanism. For the HMM connection, we con-
sider three choices: gating all parameters (Equa-
tion 3), gating only the bias, and no connection.
For the attention, we consider three choices: our
new gated attention mechanism, the traditional at-
tention, and no attention. Thus, in total, we ex-
plore nine model variants.

All the model variants are implemented with the
CNN package4 and trained with Adagrad (Duchi
et al., 2011) using dropout (Srivastava et al., 2014).
They share the same word-embedding size (128)
and hidden vector size (64).5

2Julia et al. (2010) employed both text transcription and
audio signal. Here, we report the results obtained with the
transcription.

3There have been other works with different experimental
setups (Gambäck et al., 2011; Webb and Ferguson, 2010) that
obtained accuracies ranging from 77.85% to 80.72%. How-
ever, these results are not directly comparable to ours.

4https://github.com/clab/cnn-v1.
5The experiments were executed on an Intel Xeon E5-

2667 CPU with 16GB of RAM. The training time for each
MapTask model is less than a day, the training time for each
Switchboard model takes up to four weeks.

Models Accuracy
Julia et al. (2010) 55.40%
Surendran et al. (2006) 59.10%
HMM (Stolcke et al. (2000)) 51.40%
Random Forest (n-gram) 55.72%
Ji et al. (2016) 60.97%
Our model

gated attn. + gated HMM bias 65.94%
gated attn. + gated HMM all 65.94%

Table 2: Results on MapTask data.

Models Accuracy
Stolcke et al. (2000) 71.0%
Kalchbrenner and Blunsom (2013) 73.9%
Ji et al. (2016) 72.5%
Shen and Lee (2016) 72.6%
our model

gated attn. + gated HMM bias 74.2%
gated attn. + gated HMM all 74.0%

Table 3: Results on Switchboard data.

Results and Analysis. Table 1 shows the classi-
fication accuracy of the nine variants of our model
on the MapTask corpus. The classification accu-
racy of the two best variants of our model and the
baselines appears in Tables 2 and 3 for MapTask
and Switchboard respectively. The bold numbers
in each table show the best accuracy achieved by
the systems. As seen in these tables, our best mod-
els outperform strong baselines for both corpora.6

Table 1 shows that adding the attention mecha-
nism is beneficial, as the traditional attention mod-
els always outperform their non-attention coun-
terparts. The gated attention configurations, in
turn, outperform those with the traditional atten-
tion mechanism by 0.49%-1.21%. Interestingly,
the accuracy of Shen and Lee’s (2016) classifier,
which employs an attention mechanism, is lower
than that obtained by Kalchbrenner and Blun-
som (2013), whose mechanism does not use atten-
tion. We believe that the difference in performance
is not due to the attention mechanism being inef-
fective, but because Shen and Lee (2016) treat the
classification of each utterance independently. In
contrast, Kalchbrenner and Blunsom (2013) take

6Ji et al. (2016) reported an accuracy of 77.0% on the
Switchboard corpus, but their paper does not provide enough
information about the experimental setup to replicate this re-
sult (hyper-parameters, train/test/development split). Thus,
we ran the paper’s publicly available code with our experi-
mental settings, and report the result in our comparison.

527



the sequential nature of dialog acts into account,
and run an RNN across the conversation, which
conditions the generation of a dialogue act on the
dialogue acts and utterances in all the previous di-
alogue turns.

As seen in Table 1, the performance gain from
the HMM connection is larger than the gain from
the attention mechanism. Without the attention
mechanism, the HMM connection brings an in-
crease of 3.63% with the gated bias HMM config-
uration and 2.58% with the fully gated HMM con-
figuration. With the use of traditional attention,
the improvement is 3.01% for the bias HMM con-
figuration and 3.47% for the gated HMM config-
uration. Finally with the gated attention in place,
the two HMM configurations improve the accu-
racy by 3.73%.

We used McNemar’s test to determine the statis-
tical significance between the predictions of differ-
ent models, and found that our model with both in-
novations (HMM connections and gated attention)
is statistically significantly better than the variant
without these innovations with α < 0.01.

4 Conclusions

In this work, we have proposed a new gated at-
tention mechanism and a novel HMM-like con-
nection in a generative model of utterances and
dialogue acts. Our experiments show that these
two innovations significantly improve the accu-
racy of DA classification on the MapTask and
Switchboard corpora. In the future, we plan to
apply these two innovations to other sequence-to-
sequence learning tasks. Furthermore, DA classi-
fication itself can be seen as a preprocessing step
in a dialogue system’s pipeline. Thus, we also plan
to investigate the effect of improvements in DA
classification on the downstream components of a
dialogue system.

References

Anne H Anderson, Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
Jim Miller, et al. 1991. The HCRC map task corpus.
Language and speech 34(4):351–366.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .

Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. arXiv preprint arXiv:1409.1259 .

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555 .

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research 12(Jul):2121–2159.

Björn Gambäck, Fredrik Olsson, and Oscar Täckström.
2011. Active learning for dialogue act classification.
In Interspeech 2011 – Proceedings of the Interna-
tional Conference on Spoken Language Processing.
pages 1329–1332.

Alex Graves. 2013. Generating sequences with
recurrent neural networks. arXiv preprint
arXiv:1308.0850 .

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Yangfeng Ji, Gholamreza Haffari, and Jacob Eisen-
stein. 2016. A latent variable recurrent neural net-
work for discourse-driven language models. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. pages
332–342. http://www.aclweb.org/anthology/N16-
1037.

Fatema N Julia, Khan M Iftekharuddin, and ATIQ U
ISLAM. 2010. Dialog act classification using
acoustic and discourse information of maptask data.
International Journal of Computational Intelligence
and Applications 9(04):289–311.

Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca. 1997. Switchboard SWBD-DAMSL Shallow-
Discourse-Function Annotation Coders Manual,
Draft 13. Technical report, University of Colorado.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. arXiv preprint arXiv:1306.3584 .

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings
of the 2015 Conference on Empirical Methods in
Natural Language Processing. pages 1412–1421.
http://aclweb.org/anthology/D15-1166.

Sheng-syun Shen and Hung-yi Lee. 2016. Neural at-
tention models for sequence classification: Analysis
and application to key term extraction and dialogue
act detection. arXiv preprint arXiv:1604.00077 .

528



Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov.
2014. Dropout: A simple way to prevent
neural networks from overfitting. Journal of
Machine Learning Research 15(1):1929–1958.
http://dl.acm.org/citation.cfm?id=2627435.2670313.

Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational linguistics 26(3):339–373.

Dinoj Surendran and Gina-Anne Levow. 2006. Dia-
log act tagging with support vector machines and
hidden markov models. In Interspeech 2006 – Pro-
ceedings of the International Conference on Spoken
Language Processing. pages 1950–1953.

Bingning Wang, Kang Liu, and Jun Zhao. 2016. In-
ner attention based recurrent neural networks for an-
swer selection. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). pages 1288–1297.
http://www.aclweb.org/anthology/P16-1122.

Nick Webb and Michael Ferguson. 2010. Automatic
extraction of cue phrases for cross-corpus dialogue
act classification. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters. pages 1310–1317.

529


	A Generative Attentional Neural Network Model for Dialogue Act Classification

