



















































Recurrent Residual Learning for Sequence Classification


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 938–943,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Recurrent Residual Learning for Sequence Classification

Yiren Wang∗
University of Illinois at Urbana-Champaign

yiren@illinois.edu

Fei Tian
Microsoft Research

fetia@microsoft.com

Abstract

In this paper, we explore the possibility of
leveraging Residual Networks (ResNet), a
powerful structure in constructing extremely
deep neural network for image understanding,
to improve recurrent neural networks (RNN)
for modeling sequential data. We show that
for sequence classification tasks, incorporat-
ing residual connections into recurrent struc-
tures yields similar accuracy to Long Short
Term Memory (LSTM) RNN with much fewer
model parameters. In addition, we propose
two novel models which combine the best
of both residual learning and LSTM. Experi-
ments show that the new models significantly
outperform LSTM.

1 Introduction

Recurrent Neural Networks (RNNs) are powerful
tools to model sequential data. Among various
RNN models, Long Short Term Memory (LSTM)
(Hochreiter and Schmidhuber, 1997) is one of the
most effective structures. In LSTM, gating mech-
anism is used to control the information flow such
that gradient vanishing problem in vanilla RNN is
better handled, and long range dependency is bet-
ter captured. However, as empirically verified by
previous works and our own experiments, to obtain
fairly good results, training LSTM RNN needs care-
fully designed optimization procedure (Hochreiter
et al., 2001; Pascanu et al., 2013; Dai and Le, 2015;
Laurent et al., 2015; He et al., 2016; Arjovsky et

∗This work was done when the author was visiting Mi-
crosoft Research Asia.

al., 2015), especially when faced with unfolded very
deep architectures for fairly long sequences (Dai
and Le, 2015).

From another perspective, for constructing very
deep neural networks, recently Residual Networks
(ResNet) (He et al., 2015) have shown their ef-
fectiveness in quite a few computer vision tasks.
By learning a residual mapping between layers with
identity skip connections (Jaeger et al., 2007),
ResNet ensures a fluent information flow, leading to
efficient optimization for very deep structures (e.g.,
with hundreds of layers). In this paper, we explore
the possibilities of leveraging residual learning to
improve the performances of recurrent structures, in
particular, LSTM RNN, in modeling fairly long se-
quences (i.e., whose lengths exceed 100). To sum-
marize, our main contributions include:

1. We introduce residual connecting mechanism
into the recurrent structure and propose recur-
rent residual networks for sequence learning.
Our model achieves similar performances to
LSTM in text classification tasks, whereas the
number of model parameters is greatly reduced.

2. We present in-depth analysis of the strengths
and limitations of LSTM and ResNet in respect
of sequence learning.

3. Based on such analysis, we further propose two
novel models that incorporate the strengths of
the mechanisms behind LSTM and ResNet. We
demonstrate that our models outperform LSTM
in many sequence classification tasks.

938



2 Background

RNN models sequences by taking sequential input
x = {x1, · · · , xT } and generating T step hidden
states h = {h1, · · · , hT }. At each time step t, RNN
takes the input vector xt ∈ Rn and the previous hid-
den state vector ht−1 ∈ Rm to produce the next hid-
den state ht.

Based on this basic structure, LSTM avoids gradi-
ent vanishing in RNN training and thus copes better
with long range dependencies, by further augment-
ing vanilla RNN with a memory cell vector ct ∈ Rm
and multiplicative gate units that regulate the infor-
mation flow. To be more specific, at each time step
t, an LSTM unit takes xt, ct−1, ht−1 as input, gen-
erates the input, output and forget gate signals (de-
noted as it, ot and ft respectively), and produces the
next cell state ct and hidden state ht:

c̃t = tanh(Wc[ht−1, xt] + bc)

it = σ(Wi[ht−1, xt] + bi)

ft = σ(Wf [ht−1, xt] + bf )

ot = σ(Wo[ht−1, xt] + bo)

ct = ft ⊗ Ct−1 + it ⊗ c̃t
ht = ot ⊗ tanh(ct)

(1)

where ⊗ refers to element-wise product. σ(x) is the
sigmoid function σ(x) = 1/(1+exp(−x)). Wj(j ∈
{i, o, f, c}) are LSTM parameters. In the following
part, such functions generating ht and ct are denoted
as ht, ct = LSTM(xt, ht−1, ct−1).

Residual Networks (ResNet) are among the pio-
neering works (Szegedy et al., 2015; Srivastava et
al., 2015) that utilize extra identity connections to
enhance information flow such that very deep neural
networks can be effectively optimized. ResNet (He
et al., 2015) is composed of several stacked resid-
ual units, in which the lth unit takes the following
transformation:

hl+1 = f(g(hl) + F(hl;Wl)) (2)

where hl and hl+1 are the input and output for the
lth unit respectively. F is the residual function with
weight parametersWl. f is typically the ReLU func-
tion (Nair and Hinton, 2010). g is set as identity
function, i.e., g(hl) = hl. Such an identity con-
nection guarantees the direct propagation of signals

among different layers, thereby avoids gradient van-
ishing. The recent paper (Liao and Poggio, 2016)
talks about the possibility of using shared weights in
ResNet, similar to what RNN does.

3 Recurrent Residual Learning

The basic idea of recurrent residual learning is to
force a direct information flow in different time steps
of RNNs by identity (skip) connections. In this sec-
tion, we introduce how to leverage residual learning
to 1) directly construct recurrent neural network in
subsection 3.1; 2) improve LSTM in subsection 3.2.

3.1 Recurrent Residual Networks (RRN)
The basic architecture of Recurrent Residual Net-
work (RRN for short) is illustrated in Figure 1, in
which orange arrows indicate the identity connec-
tions from each ht−1 to ht, and blue arrows rep-
resent the recurrent transformations taking both ht
and xt as input. Similar to equation (2), the recur-
rent transformation in RRN takes the following form
(denoted as ht = RRN(xt, ht−1) in the following
sections):

ht = f(g(ht−1) + F(ht−1, xt;W )), (3)
where g is still the identity function s.t. g(ht−1) =
ht−1, corresponding to the orange arrows in Figure
1. f is typically set as tanh. For function F with
weight parameters W (corresponding to the blue ar-
rows in Figure 1), inspired by the observation that
higher recurrent depth tends to lead to better perfor-
mances (Zhang et al., 2016), we impose K deep
transformations in F :

yt1 = σ(xtW1 + ht−1U1 + b1)

yt2 = σ(xtW2 + y
t
1U2 + b2)

· · ·
ytK = σ(xtWK + y

t
K−1UK + bK)

F(ht−1, xt) = ytK

(4)

where xt is taken at every layer such that the input
information is better captured, which works simi-
larly to the mechanism of highway network (Sri-
vastava et al., 2015). K is the recurrent depth de-
fined in (Zhang et al., 2016). The weights Wm
(m ∈ {1, · · · ,K}) are shared across different time
steps t.

939



Figure 1: The basic structure of Recurrent Residual Networks.

RRN forces the direct propagation of hidden state
signals between every two consecutive time steps
with identity connections g. In addition, the mul-
tiple non-linear transformations in F guarantees its
capability in modelling complicated recurrent rela-
tionship. In practice, we found that K = 2 yields
fairly good performances, meanwhile leads to half
of LSTM parameter size when model dimensions
are the same.

3.2 Gated Residual RNN
Identity connections in ResNet are important for
propagating the single input image information to
higher layers of CNN. However, when it comes to
sequence classification, the scenario is quite differ-
ent in that there is a new input at every time step.
Therefore, a forgetting mechanism to “forget” less
critical historical information, as is employed in
LSTM (controlled by the forget gate ft), becomes
necessary. On the other hand, while LSTM benefits
from the flexible gating mechanism, its parametric
nature brings optimization difficulties to cope with
fairly long sequences, whose long range informa-
tion dependencies could be better captured by iden-
tity connections.

Inspired by the success of the gating mechanism
of LSTM and the residual connecting mechanism
with enhanced information flow of ResNet, we fur-
ther propose two Gated Residual Recurrent models
leveraging the strengths of the two mechanisms.

3.2.1 Model 1: Skip-Connected LSTM
(SC-LSTM)

Skip-Connected LSTM (SC-LSTM for short) in-
troduces identity connections into standard LSTM to
enhance the information flow. Note that in Figure 1,
a straightforward approach is to replace F with an
LSTM unit. However, our preliminary experiments

do not achieve satisfactory results. Our conjecture is
that identity connections between consecutive mem-
ory cells, which are already sufficient to maintain
short-range memory, make the unregulated informa-
tion flow overly strong, and thus compromise the
merit of gating mechanism.

To reasonably enhance the information flow for
LSTM network while keeping the advantage of gat-
ing mechanism, starting from equation (1), we pro-
pose to add skip connections between two LSTM
hidden states with a wide range of distance L (e.g.,
L = 20), such that ∀t = {1, 1+L, 1+ 2L, · · · , 1+
bT−L−1L cL}:

ht+L = tanh(ct+L)⊗ ot+L + αht (5)
Here α is a scalar that can either be fixed as 1
(i.e., identity mapping) or be optimized during train-
ing process as a model parameter (i.e., parametric
skip connection). We refer to these two variants as
SC-LSTM-I and SC-LSTM-P respectively. Note
that in SC-LSTM, the skip connections only exist
in time steps 1, 1 + L, 1 + 2L, · · · , 1 + bT−L−1L cL.
The basic structure is shown in Figure 2.

Figure 2: The basic structure of Skip-Connected LSTM.

3.2.2 Model 2: Hybrid Residual LSTM (HRL)
Since LSTM generates sequence representations

out of flexible gating mechanism, and RRN gener-
ates representations with enhanced residual histori-
cal information, it is a natural extension to combine
the two representations to form a signal that bene-
fits from both mechanisms. We denote this model as
Hybrid Residual LSTM (HRL for short).

In HRL, two independent signals, hLSTMt gen-
erated by LSTM (equation (1)) and hRRNt gener-
ated by RRN (equation (3)), are propagated through
LSTM and RRN respectively:

hLSTMt , ct = LSTM(xt, h
LSTM
t−1 , ct−1)

hRRNt = RRN(xt, h
RRN
t−1 )

(6)

940



The final representation hHRLT is obtained by the
mean pooling of the two “sub” hidden states:

hHRLT =
1

2
(hLSTMT + h

RRN
T ) (7)

hHRLT is then used for higher level tasks such as pre-
dicting the sequence label. Acting in this way, hHRLT
contains both the statically forced and dynamically
adjusted historical signals, which are respectively
conveyed by hRRNt and h

LSTM
t .

4 Experiments

We conduct comprehensive empirical analysis on
sequence classification tasks. Listed in the ascend-
ing order of average sequence lengths, several public
datasets we use include:

1. AG’s news corpus1,a news article corpus with
categorized articles from more than 2, 000
news sources. We use the dataset with 4 largest
classes constructed in (Zhang et al., 2015).

2. IMDB movie review dataset2, a binary senti-
ment classification dataset consisting of movie
review comments with positive/negative senti-
ment labels (Maas et al., 2011).

3. 20 Newsgroups (20NG for short), an email
collection dataset categorized into 20 news
groups. Simiar to (Dai and Le, 2015), we use
the post-processed version3, in which attach-
ments, PGP keys and some duplicates are re-
moved.

4. Permuted-MNIST (P-MNIST for short). Fol-
lowing (Le et al., 2015; Arjovsky et al., 2015),
we shuffle pixels of each MNIST image (Le-
Cun et al., 1998) with a fixed random per-
mutation, and feed all pixels sequentially into
recurrent network to predict the image label.
Permuted-MNIST is assumed to be a good
testbed for measuring the ability of modeling
very long range dependencies (Arjovsky et al.,
2015).

1http://www.di.unipi.it/~gulli/AG corpus of news
articles.html

2http://ai.stanford.edu/~amaas/data/sentiment/
3http://ana.cachopo.org/datasets-for-single-label-text-

categorization

Detailed statistics of each dataset are listed in
Table 1. For all the text datasets, we take every
word as input and feed word embedding vectors
pre-trained by Word2Vec (Mikolov et al., 2013) on
Wikipedia into the recurrent neural network. The
top most frequent words with 95% total frequency
coverage are kept, while others are replaced by the
token “UNK”. We use the standard training/test
split along with all these datasets and randomly
pick 15% of training set as dev set, based on which
we perform early stopping and for all models
tune hyper-parameters such as dropout ratio (on
non-recurrent layers) (Zaremba et al., 2014),
gradient clipping value (Pascanu et al., 2013) and
the skip connection length L for SC-LSTM (cf.
equation (5)). The last hidden states of recurrent
networks are put into logistic regression classifiers
for label predictions. We use Adadelta (Zeiler,
2012) to perform parameter optimization. All our
implementations are based on Theano (Theano De-
velopment Team, 2016) and run on one K40 GPU.
All the source codes and datasets can be down-
loaded at https://publish.illinois.
edu/yirenwang/emnlp16source/.

We compare our proposed models mainly with
the state-of-art standard LSTM RNN. In addition, to
fully demonstrate the effects of residual learning in
our HRL model, we employ another hybrid model
as baseline, which combines LSTM and GRU (Cho
et al., 2014), another state-of-art RNN variant, in a
similar way as HRL. We use LSTM+GRU to de-
note such a baseline. The model sizes (word embed-
ding size × hidden state size) configurations used
for each dataset are listed in Table 2. In Table 2,
“Non-Hybrid” refers to LSTM, RRN and SC-LSTM
models, while “Hybrid” refers to two methods that
combines two basic models: HRL and LSTM+GRU.
The model sizes of all hybrid models are smaller
than the standard LSTM. All models have only one
recurrent layer.

4.1 Experimental Results

All the classification accuracy numbers are listed in
Table 3. From this table, we have the following ob-
servations and analysis:

1. RRN achieves similar performances to stan-
dard LSTM in all classification tasks with only

941



Dataset Ave. Len Max Len #Classes #Train : #Test
AG’s News 34 211 4 120, 000 : 7, 600

IMDB 281 2, 956 2 25, 000 : 25, 000
20NG 267 11, 924 20 11, 293 : 7, 528

P-MNIST 784 784 10 60, 000 : 10, 000
Table 1: Classification Datasets.

AG’s News IMDB 20NG P-MNIST
Non-Hybird 256× 512 256× 512 500× 768 1× 100

Hybrid 256× 384 256× 384 256× 512 1× 80
Table 2: Model Sizes on Different Dataset.

Model/Task AG’s News IMDB 20NG P-MNIST
LSTM 91.76% 88.88% 79.21% 90.64%
RRN 91.19% 89.13% 79.76% 88.63%

SC-LSTM-P 92.01% 90.74% 82.98% 94.46%
SC-LSTM-I 92.05% 90.67% 81.85% 94.80%
LSTM+GRU 91.05% 89.23% 80.12% 90.28%

HRL 91.90% 90.92% 81.73% 90.33%
Table 3: Classification Results (Test Accuracy).

half of the model parameters, indicating that
residual network structure, with connecting
mechanism to enhance the information flow, is
also an effective approach for sequence learn-
ing. However, the fact that it fails to sig-
nificantly outperform other models (as it does
in image classification) implies that forgetting
mechanism is desired in recurrent structures to
handle multiple inputs.

2. Skip-Connected LSTM performs much better
than standard LSTM. For tasks with shorter se-
quences such as AG’s News, the improvement
is limited. However, the improvements get
more significant with the growth of sequence
lengths among different datasets4, and the per-
formance is particularly good in P-MNIST with
very long sequences. This reveals the impor-
tance of skip connections in carrying on histor-
ical information through a long range of time
steps, and demonstrates the effectiveness of our
approach that adopts the residual connecting
mechanism to improve LSTM’s capability of
handling long-term dependency. Furthermore,
SC-LSTM is robust with different hyperparam-

4t-test on SC-LSTM-P and SC-LSTM-I with p value <
0.001.

eter values: we test L = 10, 20, 50, 75 in P-
MNIST and find the performance is not sensi-
tive w.r.t. these L values.

3. HRL also outperforms standard LSTM with
fewer model parameters5. In comparison, the
hybrid model of LSTM+GRU cannot achieve
such accuracy as HRL. As we expected, the ad-
ditional long range historical information prop-
agated by RRN is proved to be good assistance
to standard LSTM.

5 Conclusion

In this paper, we explore the possibility of lever-
aging residual network to improve the performance
of LSTM RNN. We show that direct adaptation of
ResNet performs well in sequence classification. In
addition, when combined with the gating mecha-
nism in LSTM, residual learning significantly im-
prove LSTM’s performance. As to future work,
we plan to apply residual learning to other se-
quence tasks such as language modeling, and RNN
based neural machine translation (Sutskever et al.,
2014) (Cho et al., 2014).

5t-test on HRL with p value < 0.001.

942



References

Martin Arjovsky, Amar Shah, and Yoshua Bengio. 2015.
Unitary evolution recurrent neural networks. arXiv
preprint arXiv:1511.06464.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learn-
ing phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Andrew M Dai and Quoc V Le. 2015. Semi-supervised
sequence learning. In Advances in Neural Information
Processing Systems, pages 3061–3069.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Deep residual learning for image recogni-
tion. arXiv preprint arXiv:1512.03385.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Identity mappings in deep residual net-
works. arXiv preprint arXiv:1603.05027.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9(8):1735–
1780.

Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jr-
gen Schmidhuber. 2001. Gradient flow in recurrent
nets: the difficulty of learning long-term dependen-
cies.

Herbert Jaeger, Mantas Lukoševičius, Dan Popovici, and
Udo Siewert. 2007. Optimization and applications
of echo state networks with leaky-integrator neurons.
Neural Networks, 20(3):335–352.

César Laurent, Gabriel Pereyra, Philémon Brakel, Ying
Zhang, and Yoshua Bengio. 2015. Batch nor-
malized recurrent neural networks. arXiv preprint
arXiv:1510.01378.

Quoc V Le, Navdeep Jaitly, and Geoffrey E Hin-
ton. 2015. A simple way to initialize recurrent
networks of rectified linear units. arXiv preprint
arXiv:1504.00941.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE,
86(11):2278–2324.

Qianli Liao and Tomaso Poggio. 2016. Bridging the gaps
between residual learning, recurrent neural networks
and visual cortex. arXiv preprint arXiv:1604.03640.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan
Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 142–150, Portland, Oregon, USA,
June. Association for Computational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality.
In Advances in neural information processing systems,
pages 3111–3119.

Vinod Nair and Geoffrey E Hinton. 2010. Rectified lin-
ear units improve restricted boltzmann machines. In
Proceedings of the 27th International Conference on
Machine Learning (ICML-10), pages 807–814.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. In Proceedings of The 30th International
Conference on Machine Learning, pages 1310–1318.

Rupesh K Srivastava, Klaus Greff, and Jürgen Schmid-
huber. 2015. Training very deep networks. In
Advances in Neural Information Processing Systems,
pages 2368–2376.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in neural information processing systems,
pages 3104–3112.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-
manet, Scott Reed, Dragomir Anguelov, Dumitru Er-
han, Vincent Vanhoucke, and Andrew Rabinovich.
2015. Going deeper with convolutions. In Proceed-
ings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1–9.

Theano Development Team. 2016. Theano: A Python
framework for fast computation of mathematical ex-
pressions. arXiv e-prints, abs/1605.02688, May.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization. arXiv
preprint arXiv:1409.2329.

Matthew D Zeiler. 2012. Adadelta: an adaptive learning
rate method. arXiv preprint arXiv:1212.5701.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text classi-
fication. In Advances in Neural Information Process-
ing Systems, pages 649–657.

Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin,
Roland Memisevic, Ruslan Salakhutdinov, and Yoshua
Bengio. 2016. Architectural complexity mea-
sures of recurrent neural networks. arXiv preprint
arXiv:1602.08210.

943


