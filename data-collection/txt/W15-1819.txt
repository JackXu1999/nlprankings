






















Topic Models: Accounting Component Structure of Bigrams

Michael Nokel
Lomonosov Moscow State University,

Russian Federation
mnokel@gmail.com

Natalia Loukachevitch
Lomonosov Moscow State University,

Russian Federation
louk nat@mail.ru

Abstract

The paper describes the results of an em-
pirical study of integrating bigram col-
locations and similarities between them
and unigrams into topic models. First of
all, we propose a novel algorithm PLSA-
SIM that is a modification of the original
algorithm PLSA. It incorporates bigrams
and maintains relationships between uni-
grams and bigrams based on their com-
ponent structure. Then we analyze a va-
riety of word association measures in or-
der to integrate top-ranked bigrams into
topic models. All experiments were con-
ducted on four text collections of different
domains and languages. The experiments
distinguish a subgroup of tested measures
that produce top-ranked bigrams, which
demonstrate significant improvement of
topic models quality for all collections,
when integrated into PLSA-SIM algo-
rithm.

1 Introduction

Topic modeling is one of the latest applications
of machine learning techniques to the natural lan-
guage processing. Topic models identify which
topics relate to each document and which words
form each topic. Each topic is defined as a multi-
nomial distribution over terms and each document
is defined as multinomial distribution over top-
ics (Blei et al., 2003). Topic models have achieved
noticeable success in various areas such as infor-
mation retrieval (Wei and Croft, 2006), including
such applications as multi-document summariza-
tion (Wang et al., 2009), text clustering and cat-
egorization (Zhou et al., 2009), and other natural
language processing tasks such as word sense dis-
ambiguation (Boyd-Graber et al., 2007), machine
translation (Eidelman et al., 2012). Among most

well-known models are Latent Dirichlet Alloca-
tion (LDA) (Blei et al., 2003), which is based on
Dirichlet prior distribution, and Probabilistic La-
tent Semantic Analysis (PLSA) (Hofmann, 1999),
which is not connected with any parametric prior
distribution.

One of the main drawback of the topic models
is that they utilize “bag-of-words” model that dis-
cards word order and is based on the word inde-
pendence assumption. There are numerous stud-
ies, where the integration of collocations, n-grams,
idioms and multi-word terms into topic models
is investigated. However, it often leads to a de-
crease in the model quality due to increasing size
of a vocabulary or to a serious complication of the
model (Wallach, 2006; Griffiths et al., 2007; Wang
et al., 2007).

The paper proposes a novel approach taking
into account bigram collocations and relationship
between them and unigrams in topic models (such
as citizen – citizen of country – citizen of union
– European citizen – state citizen; categorization
– document categorization – term categorization
– text categorization). This allows us to create a
novel method of integrating bigram collocations
into topic models that does not consider bigrams
being as “black boxes”, but maintains the rela-
tionship between unigrams and bigrams based on
their component structure. The proposed algo-
rithm leads to significant improvement of topic
models quality measured in perplexity and topic
coherence (Newman et al., 2010) without compli-
cations of the model.

All experiments were carried out using PLSA
algorithm and its modifications on four corpora
of different domains and languages: the English
part of Europarl parallel corpus, the English part of
JRC-Acquis parallel corpus, ACL Anthology Ref-
erence corpus, and Russian banking magazines.

The rest of the paper is organized as follows. In
the section 2 we focus on related work. Section 3

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 145



proposes a novel algorithm PLSA-SIM that incor-
porates bigrams and similarities between them and
unigrams into topic models. Section 4 describes
the datasets used in experiments, all preprocessing
steps and metrics used to evaluate the quality. In
the section 5 we perform an extensive analysis of a
variety of measures for integrating top-ranked bi-
grams into topic models. And in the last section
we draw conclusions.

2 Related Work

The idea of using collocations in topic models is
not a novel one. Nowadays there are two kinds of
methods proposed to deal with this problem: cre-
ation of a unified probabilistic model and prelim-
inary extraction of collocations and n-grams with
further integration into topic models.

Most studies belong to the first kind of meth-
ods. So, the first movement beyond “bag-of-
words” assumption has been made by Wallach
(2006), where the Bigram Topic Model was pre-
sented. In this model word probabilities are con-
ditioned on the immediately preceding word. The
LDA Collocation Model (Griffiths et al., 2007) ex-
tends the Bigram Topic Model by introducing a
new set of variables and thereby giving a flexibil-
ity to generate both unigrams and bigrams. Wang
et al. (2007) proposed the Topical N-Gram Model
that adds a layer of complexity to allow the for-
mation of bigrams to be determined by the con-
text. Hu et al. (2008) proposed the Topical Word-
Character Model challenging the assumption that
the topic of an n-gram is determined by the top-
ics of composite words within the collocation.
This model is mainly suitable for Chinese lan-
guage. Johnson (2010) established connection be-
tween LDA and Probabilistic Context-Free Gram-
mars and proposed two probabilistic models com-
bining insights from LDA and Adaptor Grammars
to integrate collocations and proper names into the
topic model.

While all these models have a theoretically ele-
gant background, they are very complex and hard
to compute on real datasets. For example, Bigram
Topic Model has W2T parameters, compared to
WT for LDA and WT + DT for PLSA, where W
is the size of vocabulary, D is the number of doc-
uments, and T is the number of topics. Therefore
such models are mostly of theoretical interest.

The algorithm proposed in Lau et al. (2013) be-
longs to the second type of methods that use col-

locations in topic models. The authors extract bi-
gram collocations via t-test and replace separate
units by top-ranked bigrams at the preprocessing
step. They use two metrics of topic quality: per-
plexity and topic coherence (Newman et al., 2010)
and conclude that incorporating bigram colloca-
tions into topics results in worsening perplexity
and improving topic coherence.

Our current work also belongs to the second
type of methods and distinguishes from previ-
ous papers such as Lau et al. (2013) in that our
approach does not consider bigrams as “black
boxes”, but maintains information about the inner
structure of bigrams and relationships between bi-
grams and component unigrams, which leads to
improvement in both metrics: perplexity and topic
coherence.

The idea to utilize prior natural language knowl-
edge in topic models is not a novel one. So, An-
drzejewski et al. (2009) incorporated domain-
specific knowledge by Must-Link and Cannot-
Link primitives represented by a novel Dirichlet
Forest prior. These primitives control that two
words tend to be generated by the same or sep-
arate topics. However, this method can result in
an exponential growth in the encoding of Cannot-
Link primitives and thus has difficulty in process-
ing a large number of constraints (Liu, 2012). An-
other method of incorporating such knowledge is
presented in Zhai (2010) where a semi-supervised
EM-algorithm was proposed to group expressions
into some user-specified categories. To provide a
better initialization for EM-algorithm the method
employs prior knowledge that expressions shar-
ing words and synonyms are likely to belong to
the same group. Our current work distinguishes
from these ones in that we incorporate similar-
ity links between unigrams and bigrams into the
topic model in a very natural way counting their
co-occurrences in documents. The proposed ap-
proach does not increase the complexity of the
original PLSA algorithm.

3 PLSA-SIM algorithm

As mentioned above, original topic models utilize
the “bag-of-words” assumption that assumes word
independence. And bigrams are usually added to
topic models as “black boxes” without any ties
with other words. So, bigrams are added to the
vocabulary as single tokens and in each document
containing any of added bigrams the frequencies

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 146



of unigram components are decreased by the fre-
quencies of bigrams (Lau et al., 2013). Thus “bag-
of-words” assumption holds.

However, there are many similar unigrams and
bigrams that share the same lemmas (i.e, correc-
tion – correction of word – error correction –
spelling correction; rail – rail infrastructure – rail
transport – use of rail) and others in documents.
We should note such bigrams do not only have
identical words, but many of them maintain se-
mantic and thematic similarity. At the same time
other bigrams with the same words (i.e., idioms)
can have significant semantic differences. To take
into account these different situations, we hypoth-
esized that similar bigrams sharing the same uni-
gram components should often belong to the same
topics, if they often co-occur within the same texts.

To verify this hypothesis we precompute sets
of similar unigrams and bigrams sharing the same
lemmas and propose novel PLSA-SIM algorithm
that is the modification of the original PLSA al-
gorithm. We will rely on the description found
in Vorontsov and Potapenko (2014) and use the
following notations (further in the paper we will
use notation “term” when speaking about both un-
igrams and bigrams):

• D – the collection of documents;
• T – the set of inferred topics;
• W – the vocabulary (the set of unique terms

found in the collection D);
• Φ = {φwt = p(w|t)} – the distribution of terms

w over topics t;
• Θ = {θtd = p(t|d)} – the distribution of topics

t over documents d;
• S = {S w} – the sets of similar terms (S w is

the set of terms similar to w, that is S w =
{w⋃

v
wv
⋃
v

vw}, where w is the lemmatized
unigram, while wv and vw are lemmatized bi-
grams);
• ndw, nds – the number of occurrences of the

terms w, s in the document d;
• n̂wt – the estimate of frequency of the term w

in the topic t;
• n̂td – the estimate of frequency of the topic t

in the document d;
• n̂t – the estimate of frequency of the topic t in

the text collection D;
• nd – the number of words in the document d.

The pseudocode of PLSA-SIM algorithm is pre-
sented in the Algorithm 1. The only modifications

of the original algorithm concern lines 6 and 9,
where we introduce auxiliary variable fdw, which
takes into account pre-computed sets of similar
terms. Thus, the weight of such terms is increased
within each document.

Algorithm 1: PLSA-SIM algorithm: PLSA
with similar terms
Input: collection of documents D, number of

topics |T |, initial distributions Θ and
Φ, sets of similar terms S

Output: distributions Θ and Φ
1 while not meet the stop criterion do
2 for d ∈ D, w ∈W, t ∈ T do
3 n̂wt = 0, n̂td = 0, n̂t = 0, nd = |d|
4 for d ∈ D, w ∈W do
5 Z =

∑
t
φwtθtd,

6

fdw = ndw +
∑

s∈S w
nds

7 for t ∈ T do
8 if φwtθtd > 0 then
9 δ = fdwφwtθtd/Z

10 n̂wt = n̂wt +δ, n̂td = n̂d +δ,
n̂t = n̂t +δ

11 for w ∈W, t ∈ T do
12 φwt = n̂wt/n̂t
13 for d ∈ D, t ∈ T do
14 θtd = n̂td/nd

So, if similar unigrams and bigrams co-occur
within the same document, we try to carry them
to the same topics. We consider such terms hav-
ing semantic and thematic similarities. However,
if unigrams and bigrams from the same set S w do
not co-occur within the same document, we do no
modifications to the original algorithm PLSA. We
consider such terms having semantic differences.

4 Datasets and Evaluation

4.1 Datasets and Preprocessing
In our experiments we used English and Russian
text collections obtained from different sources:

• For the English part of our study we took
three different collections:

– Europarl multilingual parallel corpus.
It was extracted from the proceed-
ings of the European Parliament (http:

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 147



//www.statmt.org/europarl). The
English part includes almost 54 mln.
words and 9672 documents.

– JRC-Acquis multilingual parallel cor-
pus. It represents selected texts of the
EU legislation written between the
1950s and 2005 (http://ipsc.jrc.
ec.europa.eu/index.php?id=198).
The English part contains almost 45
mln. words and 23545 documents.

– ACL Anthology Reference Corpus. It
contains scholarly publications about
Computational Linguistics (http://
acl-arc.comp.nus.edu.sg/). The
corpus includes almost 42 mln. words
and 10921 documents.

• For the Russian part of our study we
took 10422 Russian articles from several
economics-oriented magazines such as Au-
ditor, RBC, Banking Magazine, etc. These
documents contain almost 18.5 mln. words.

At the preprocessing step documents were
processed by morphological analyzers. For
the English corpus we used Stanford CoreNLP
tools (http://nlp.stanford.edu/software/
corenlp.shtml), while for the Russian corpus
we used our own morphological analyzer. All
words were lemmatized. We consider only Ad-
jectives, Nouns, Verbs and Adverbs since function
words do not play significant role in forming top-
ics. Besides, we excluded words occurring less
than five times per the whole text collection.

In addition, we extracted all bigrams in forms of
Noun + Noun, Adjective + Noun and Noun + of +
Noun for all English collections, and Noun + Noun
in Genitive and Adjective + Noun for the Russian
collection. We consider only such bigrams since
topics are mainly identified by nouns and noun
groups (Wang et al., 2007).

4.2 Evaluation Framework

As for the inferred topics quality, we consider four
different intrinsic measures. The first measure is
Perplexity since it is the standard criterion of topic
models quality (Daud et al., 2010):

Perplexity(D) = exp

−1n∑
d∈D

∑
w∈d

ndw ln p(w|d)
,
(1)

where n is the number of all considered words in
the collection, D is the set of documents in the col-
lection, ndw is the number of occurrences of the
word w in the document d, p(w|d) is the probabil-
ity of appearing the word w in the document d.

The less the value of perplexity is the better
the model predicts words w in documents D. Al-
though there were numerous studies arguing that
perplexity is not suited to topic model evalua-
tion (Chang et al., 2009; Newman et al., 2010),
it is still commonly used for comparing different
topic models. Since it is well-known that perplex-
ity computed on the same training collection is
susceptible to over-fitting and can give optimisti-
cally low values (Blei et al., 2003) we use the stan-
dard method of computing hold-out perplexity de-
scribed in Asuncion et al. (2009). In our exper-
iments we split the collections randomly into the
training sets D, on which models are trained, and
the validation sets D′, on which hold-out perplex-
ity is computed.

Another method of evaluating topic model qual-
ity is using expert opinions. We provided anno-
tators with inferred topics from the same text col-
lections and instructed them to decide whether the
topic was to some extent coherent, meaningful and
interpretable. The indicator of topic usefulness is
the ease by which one could think of a short label
to describe a topic (Newman et al., 2010). In the
Table 1 we present incoherent topic that can not be
given any label and coherent one with label given
by experts.

Top words from topic Label
have, also, commission, state, more, however –

vessel, fishing, fishery, community, catch, board fishing

Table 1: Examples of incoherent and coherent top-
ics

Since involving experts is time-consuming and
expensive, there were several attempts to propose
a method for automatic evaluation of topic mod-
els quality that would go beyond perplexity and
would be correlated with expert opinions. The for-
mulation of such a problem is very complicated
since experts can quite strongly disagree with each
other. However, it was recently shown that it is
possible to evaluate topic coherence automatically
using word semantics with precision, almost coin-
ciding with experts (Newman et al., 2010; Mimno
et al., 2011). The proposed metric measures in-
terpretability of topics based on human judge-

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 148



ment (Newman et al., 2010). As topics are usu-
ally presented to users via their top-N topic terms,
the topic coherence evaluates whether these top
terms correspond to the topic or not. Newman et
al. (2010) proposed an automated variation of the
coherence score based on pointwise mutual infor-
mation (TC-PMI):

TC-PMI(t) =
10∑
j=2

j−1∑
i=1

log
P(w j,wi)

P(w j)P(wi)
, (2)

where (w1,w2, . . . ,w10) are the top-10 terms in a
topic, P(wi) and P(w j) are probabilities of uni-
grams wi and w j respectively, while P(w j,wi) is
the probability of bigram (w j,wi). The final mea-
sure of topic coherence is calculated by averaging
TC-PMI(t) measure by all topics t.

This score is proven to demonstrate high cor-
relation with human judgement (Newman et al.,
2010). The proposed metric considers only top-
10 words in each topic since they usually pro-
vide enough information to form the subject of the
topic and distinguishing features from other top-
ics. Topic coherence is becoming more widely
used to evaluate topic model quality along with
perplexity. For example, Stevens et al. (2012)
showed that this metric is strongly correlated with
expert estimates. Also Andrzejewski et al. (2011)
simply used it for evaluating topic model quality.

Following the approach proposed by Mimno et
al. (2011) we compute probabilities by dividing
the number of documents where the unigram or
bigram occurred by the number of documents in
the collection. To avoid optimistically high values
we use external corpus for this purpose – namely,
Russian and English Wikipedia. We should note
that we do not consider another variation of topic
coherence based on log conditional probability
(TC-LCP) proposed by Mimno et al. (2011) since
it was shown in Lau et al. (2013) that it works sig-
nificantly worse than TC-PMI.

We should note that while incorporating the
knowledge of similar unigrams and bigrams into
topic models in the proposed algorithm, we en-
courage such terms to be in the top-10 terms in
inferred topics. Therefore, we increase TC-PMI
metric unintentionally since such terms are likely
to co-occur within the same documents. So we
decided to use also modification of this metric to
consider not top-10 terms in topics but top-10 non-
similar terms in topics (this metric will be further
called as TC-PMI-nSIM).

5 Integrating bigrams into topic models

To compare proposed algorithm with the original
one we extracted all bigrams found in each docu-
ment of collections. For ranking bigrams we uti-
lized Term Frequency (TF) or one of the following
19 word association measures:

1. Mutual Information (MI) (Church and Hanks,
1990);

2. Augmented MI (Zhang, 2008);
3. Normalized MI (Bouma, 2009);
4. True MI (Deane, 2005);
5. Cubic MI (Daille, 1995);
6. Symmetric Conditional Probability (Lopes

and Silva, 1999);
7. Dice Coefficient (DC) (Smadja et al., 1996);
8. Modified DC (Kitamura and Matsumoto,

1996);
9. Lexical Cohesion (Park et al., 2002);

10. Gravity Count (Daudarvičius and
Marcinkevičiené, 2003);

11. Simple Matching Coefficient (Daille, 1995);
12. Kulczinsky Coefficient (Daille, 1995);
13. Ochiai Coefficient (Daille, 1995);
14. Yule Coefficient (Daille, 1995);
15. Jaccard Coefficient (Jaccard, 1901);
16. T-Score;
17. Z-Score;
18. Chi Square;
19. Loglikelihood Ratio (Dunning, 1993).

According to the results of Lau et al. (2013)
we decided to integrate top-1000 bigrams into all
topic models under consideration. We should note
that in all experiments described in the paper we
fixed the number of topics and the number of iter-
ations of algorithms to 100.

We conducted experiments with all 20 afore-
mentioned measures on all four text collections in
order to compare the quality of the original algo-
rithm PLSA, PLSA with top-1000 bigrams added
as “black boxes”, and PLSA-SIM algorithm with
the same top-1000 bigrams.

According to the results of experiments we have
revealed two groups of measures.

The first group contains MI, Augmented MI,
Normalized MI, DC, Chi-Square, Symmetrical
Conditional Probability, Simple Matching Coef-
ficient, Kulczinsky Coefficient, Yule Coefficient,
Ochiai Coefficient, Jaccard Coefficient, Z-Score,
and Loglikelihood Ratio. We got nearly the same
levels of perplexity and topic coherence when top

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 149



bigrams ranked by these measures were integrated
into all tested topic models. This is explained by
the fact that these measures rank up very special,
non-typical and low frequency bigrams. In the Ta-
ble 2 we present results of integrating top-1000 bi-
grams ranked by MI for all text collections.

Corpus Model Perplexity TC-
TC-

PMI-
PMI nSIM

Banking

PLSA 1724.2 86.1 86.1
PLSA 1714.1 84.2 84.2

+ bigrams
PLSA-SIM 1715.4 84.1 84.1
+ bigrams

Europarl

PLSA 1594.3 53.2 53.2
PLSA 1584.6 55 55

+ bigrams
PLSA-SIM 1591.3 55.2 55.2
+ bigrams

JRC

PLSA 812.1 67 67
PLSA 815.4 66.3 66.3

+ bigrams
PLSA-SIM 815.6 66.4 66.4
+ bigrams

ACL

PLSA 2134.7 74.8 74.8
PLSA 2138.1 75.5 75.5

+ bigrams
PLSA-SIM 2144.8 75.8 75.8
+ bigrams

Table 2: Results of integrating top-1000 bigrams
ranked by MI into topic models

The second group includes TF, Cubic MI, True
MI, Modified DC, T-Score, Lexical Cohesion and
Gravity Count. We got worsened perplexity
and improved topic coherence, when top bigrams
ranked by these measures were integrated into
PLSA algorithm as “black boxes”. But when they
were used in PLSA-SIM topic models, it led to
significant improvement of all metrics under con-
sideration. This is explained by the fact that these
measures rank up high frequent, typical bigrams.
In the Table 3 we present results of integrating top-
1000 bigrams ranked by TF for all text collections.

So, we succeed to achieve better quality for both
languages using the proposed algorithm and the
second group of measures.

For the expert evaluation of topic model qual-
ity we invited two linguistic experts and gave them
topics inferred by the original PLSA algorithm and
by the proposed PLSA-SIM algorithm with top-
1000 bigrams ranked by TF (term frequency). The
task was to classify given topics into 2 classes:
whether they can be given a subject name (we will
further mark such topics as ’+’) or not (we will
further mark such topics as ’–’). In the Table 4 we

Corpus Model Perplexity TC-
TC-

PMI-
PMI nSIM

Banking

PLSA 1724.2 86.1 86.1
PLSA 2251.8 98.8 98.8

+ bigrams
PLSA-SIM 1450.6 156.5 102.6
+ bigrams

Europarl

PLSA 1594.3 53.2 53.2
PLSA 1993.5 57.3 57.3

+ bigrams
PLSA-SIM 1431.6 127.7 84.7
+ bigrams

JRC

PLSA 812.1 67 67
PLSA 1038.9 72 72

+ bigrams
PLSA-SIM 743.7 108.4 76.9
+ bigrams

ACL

PLSA 2134.7 74.8 74.8
PLSA 2619.3 73.7 73.7

+ bigrams
PLSA-SIM 1806.4 152.7 87.8
+ bigrams

Table 3: Results of integrating top-1000 bigrams
ranked by TF into topic models

present results for all text collections except ACL
Anthology Reference Corpus because for the cor-
rect markup advance knowledge in computational
linguistics is required.

Corpus Model Expert 1 Expert 2
+ – + –

Banking

PLSA 93 7 92 8
PLSA 92 8 95 5

+ bigrams
PLSA-SIM 95 5 97 3
+ bigrams

JRC

PLSA 92 8 90 10
PLSA 94 6 97 3

+ bigrams
PLSA-SIM 97 3 100 0
+ bigrams

Europarl

PLSA 97 3 99 1
PLSA 95 5 99 1

+ bigrams
PLSA-SIM 98 2 100 0
+ bigrams

Table 4: Results of expert markup of topics

As we can see, in the case of PLSA-SIM al-
gorithm with top-1000 bigrams ranked by TF the
amount of inferred topics, for which labels can be
given, is increased for all text collections. It is also
worth noting that adding bigrams as “black boxes”
does not increase the amount of such inferred top-
ics. This result also confirms that the proposed
algorithm improves the quality of topic models.

In the Table 5 we present top-5 words from
one random topic for each corpus for original

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 150



PLSA and PLSA-SIM algorithms with top-1000
bigrams ranked by TF. Within each text collection
we present topics discussing the same subject.

Banking Europarl
PLSA PLSA-SIM PLSA PLSA-SIM

Banking Financial Financial Economicsystem crisis

Bank Financial Crisis Financialmarket crisis

Sector Financial Have Europeansector economy

Financial Financial European Time ofcrisis

System Financial Market Crisisinstitute
JRC-Acquis ACL

PLSA PLSA-SIM PLSA PLSA-SIM
Transport Transport Tag Tag

Road Transport Word Tagservice set

Nuclear Road Corpus Tagtransport sequence

Vehicle Transport Tagger Unknownsector word

Material Air Tagging Speechtransport tag

Table 5: Top-5 words from topics inferred by
PLSA and PLSA-SIM algorithms

We should note that we used only intrinsic mea-
sures of topic model quality in the paper. In the
future we would like to test improved topic mod-
els in such applications of information retrieval as
text clustering and categorization.

6 Conclusion

The paper presents experiments on integrating bi-
grams and similarities between them and unigrams
into topic models. At first, we propose the novel
algorithm PLSA-SIM that incorporates similar un-
igrams and bigrams into topic models and main-
tains relationships between bigrams and unigram
components. The experiments were conducted on
the English parts of Europarl and JRC-Acquis par-
allel corpora, ACL Anthology Reference corpus
and Russian banking articles distinguished two
groups of measures ranking bigrams. The first
group produces top bigrams, which, if added to
topic models either as “black boxes” or not, re-
sults in nearly the same quality of inferred topics.
However, the second group produces top bigrams,
which, if added to the proposed PLSA-SIM al-
gorithm, results in significant improvement in all
metrics under consideration.

Acknowledgements

This work is partially supported by RFBR grant
N14-07-00383.

References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.

2009. Incorporating Domain Knowledge into Topic
Modeling via Dirichlet Forest Priors. Proceedings
of the 26th Annual International Conference on Ma-
chine Learning: 25–32.

David Andrzejewski and David Buttler. 2011. Latent
Topic Feedback for Information Retrieval. Proceed-
ings of the 17th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining:
600–608.

Arthur Asuncion, Max Welling, Padhraic Smyth, Yee
Whye Teh. 2009. On Smoothing and Inference for
Topic Models. Proceedings of the 25th International
Conference on Uncertainty in Artificial Intelligence:
27–34.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, volume 3: 993–1022.

Gerlof Bouma. 2009. Normalized (Pointwise) Mu-
tual Information. Proceedings of the Biennial GSCL
Conference: 31–40.

Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A Topic Model for Word Sense Disambigua-
tion. Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning:
1024–1033.

Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, David M. Blei. 2009. Reading Tea
Leaves: How Human Interpret Topic Models. Pro-
ceedings of the 24th Annual Conference on Neural
Information Processing Systems: 288–296.

Kenneth Ward Church, and Patrick Hanks. 1990. Word
Association Norms, Mutual Information, and Lexi-
cography. Computational Linguistics, volume 16:
22–29.

Beatrice Daille. 1995. Combined Approach for Ter-
minology Extraction: Lexical Statistics and Linguis-
tic Filtering PhD Dissertation. University of Paris,
Paris.

Ali Daud, Juanzi Li, Lizhu Zhou, Faqir Muhammad.
2010. Knowledge discovery through directed prob-
abilistic topic models: a survey. Frontiers of Com-
puter Science in China, 4(2): 280–301.

Vidas Daudarvičius and Rūta Marcinkevičiené. 2003.
Gravity Counts for the Boundaries of Collocations.
International Journal of Corpus Linguistics, 9(2):
321–348.

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 151



Paul Deane. 2005. A Nonparametric Method for Ex-
traction of Candidate Phrasal Terms. Proceedings
of the 43rd Annual Meeting of the ACL: 605–613.

Ted Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. International Jour-
nal of Computational Linguistics, 19(1): 61–74.

Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic Models for Dynamic Transla-
tion Model Adaptation. Proceedings of the 50th An-
nual Meeting of the Association of Computational
Linguistics, volume 2: 115–119.

Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in Semantic Represen-
tation. Psychological Review, 114(2): 211–244.

Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. Proceedings of the 22nd Annual Inter-
national SIGIR Conference on Research and Devel-
opment in Information Retrieval: 50–57.

Wei Hu, Nobuyuki Shimizu, Hiroshi Nakagawa, and
Huanye Shenq. 2008. Modeling Chinese Docu-
ments with Topical Word-Character Models. Pro-
ceedings of the 22nd International Conference on
Computational Linguistics: 345–352.

Paul Jaccard. 1901. Distribution de la flore alpine
dans le Bassin des Dranses et dans quelques regions
voisines. Bull. Soc. Vaudoise sci. Natur. V. 37. Bd.
140: 241–272.

Mark Johnson M. 2010. PCFGs, Topic Models, Adap-
tor Grammars and Learning Topical Collocations
and the Structure of Proper Names. Proceedings of
the 48th Annual Meeting of the ACL: 1148–1157.

Mihoko Kitamura, and Yuji Matsumoto. 1996. Au-
tomatic Extraction of Word Sequence Correspon-
dences in Parallel Corpora. Proceedings of the 4th
Annual Workshop on Very Large Corpora: 79–87.

Jey Han Lau, Timothy Baldwin, and David Newman.
2013. On Collocations and Topic Models. ACM
Transactions on Speech and Language Processing,
10(3): 1–14.

Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies, Morgan & Claypool Publishers.

Jose Gabriel Pereira Lopes, and Joaquim Ferreira da
Silva. 1999. A Local Maxima Method and a Fair
Dispersion Normalization for Extracting Multiword
Units. Proceedings of the 6th Meeting on the Math-
ematics of Language: 369–381.

David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, Andrew McCallum. 2011. Op-
timizing Semantic Coherence in Topic Models. Pro-
ceedings of EMNLP’11: 262–272.

David Newman, Jey Han Lau, Karl Grieser, and Timo-
thy Baldwin. 2010. Automatic Evaluation of Topic
Coherence. Proceedings of Human Language Tech-
nologies: The 11th Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: 100–108.

Youngja Park, Roy J. Byrd, and Branimir K. Boguraev.
2002. Automatic Glossary Extraction: Beyond Ter-
minology Identification. Proceedings of the 19th In-
ternational Conference on Computational Linguis-
tics: 1–7.

Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating Collocations
for Bilingual Lexicons: A Statistical Approach.
Computational Linguistics, 22(1): 1–38.

Keith Stevens, Philip Kegelmeyer, David Adnrzejew-
ski, and David Buttler. 2012. Exploring Topic Co-
herence over Many Models and Many Topics. Pro-
ceedings of EMNLP-CoNLL’12: 952–961.

Konstantin V. Vorontsov, and Anna A. Potapenko.
2014. Tutorial on Probabilistic Topic Modeling:
Additive Regularization for Stochastic Matrix Fac-
torization. Proceedings of AIST’2014. LNCS,
Springer Verlag-Germany, volume CCIS 439: 28–
45.

Hanna M. Wallach. 2006. Topic Modeling: Beyond
Bag-of-Words. Proceedings of the 23rd International
Conference on Machine Learning: 977–984.

Xuerui Wang, Andrew McCallum, and Xing Wei.
2007. Topical N-grams: Phrase and Topic Discov-
ery, with an Application to Information Retrieval.
Proceedings of the 2007 Seventh IEEE International
Conference on Data Mining: 697–702.

Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Multi-Document Summarization us-
ing Sentence-based Topic Models. Proceedings of
the ACL-IJCNLP 2009 Conference Short Papers:
297–300.

Xing Wei and W. Bruce Croft. 2006. LDA-Based Doc-
ument Models for Ad-hoc Retrieval. Proceedings of
the 29th International Conference on Research and
Development in Information Retrieval: 178–185.

Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2010.
Grouping Product Features Using Semi-Supervised
Learning with Soft-Constraints. Proceedings of
the 23rd International Conference on Computational
Linguistics: 1272–1280.

Wen Zhang, Taketoshi Yoshida, Tu Bao Ho, and Xi-
jin Tang. 2008. Augmented Mutual Information for
Multi-Word Term Extraction. International Journal
of Innovative Computing, Information and Control,
8(2): 543–554.

Shibin Zhou, Kan Li, and Yushu Liu. 2009. Text
Categorization Based on Topic Model. International
Journal of Computational Intelligence Systems, vol-
ume 2, No. 4: 398–409.

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 152


