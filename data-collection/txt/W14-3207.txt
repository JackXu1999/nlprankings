



















































Quantifying Mental Health Signals in Twitter


Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 51–60,
Baltimore, Maryland USA, June 27, 2014. c©2014 Association for Computational Linguistics

Quantifying Mental Health Signals in Twitter

Glen Coppersmith Mark Dredze Craig Harman
Human Language Technology Center of Excellence

Johns Hopkins University
Balitmore, MD, USA

Abstract

The ubiquity of social media provides a
rich opportunity to enhance the data avail-
able to mental health clinicians and re-
searchers, enabling a better-informed and
better-equipped mental health field. We
present analysis of mental health phe-
nomena in publicly available Twitter data,
demonstrating how rigorous application of
simple natural language processing meth-
ods can yield insight into specific disor-
ders as well as mental health writ large,
along with evidence that as-of-yet undis-
covered linguistic signals relevant to men-
tal health exist in social media. We present
a novel method for gathering data for
a range of mental illnesses quickly and
cheaply, then focus on analysis of four in
particular: post-traumatic stress disorder
(PTSD), depression, bipolar disorder, and
seasonal affective disorder (SAD). We in-
tend for these proof-of-concept results to
inform the necessary ethical discussion re-
garding the balance between the utility of
such data and the privacy of mental health
related information.

1 Introduction

While mental health issues pose a significant
health burden on the general public, mental health
research lacks the quantifiable data available to
many physical health disciplines. This is partly
due to the complexity of the underlying causes
of mental illness and partly due to longstanding
societal stigma making the subject all but taboo.
Lack of data has hampered mental health research
in terms of developing reliable diagnoses and ef-
fective treatment for many disorders. Moreover,
population-level analysis via traditional methods
is time consuming, expensive, and often comes
with a significant delay.

In contrast, social media is plentiful and has
enabled diverse research on a wide range of top-
ics, including political science (Boydstun et al.,
2013), social science (Al Zamal et al., 2012), and
health at an individual and population level (Paul
and Dredze, 2011; Dredze, 2012; Aramaki et al.,
2011; Hawn, 2009). Of the numerous health top-
ics for which social media has been considered,
mental health may actually be the most appropri-
ate. A major component of mental health research
requires the study of behavior, which may be man-
ifest in how an individual acts, how they com-
municate, what activities they engage in and how
they interact with the world around them includ-
ing friends and family. Additionally, capturing
population level behavioral trends from Web data
has previously provided revolutionary capabilities
to health researchers (Ayers et al., 2014). Thus,
social media seems like a perfect fit for study-
ing mental health in both individual and overall
trends in the population. Such topics have already
been the focus of several studies (Coppersmith et
al., 2014; De Choudhury et al., 2014; De Choud-
hury et al., 2013d; De Choudhury et al., 2013b;
De Choudhury et al., 2013c; Ayers et al., 2013).

What can we expect to learn about mental health
by studying social media? How does a service like
Twitter inform our knowledge in this area? Nu-
merous studies indicate that language use, social
expression and interaction are telling indicators of
mental health. The well-known Linguistic Inquiry
Word Count (LIWC), a validated tool for the psy-
chometric analysis of language data (Pennebaker
et al., 2007), has been repeatedly used to study
language associated with all types of disorders
(Resnik et al., 2013; Alvarez-Conrad et al., 2001;
Tausczik and Pennebaker, 2010). Furthermore, so-
cial media is by nature social, which means that
social patterns, a critical part of mental health and
illness, may be readily observable in raw Twitter
data. Thus, Twitter and other social media provide

51



a unique quantifiable perspective on human behav-
ior that may otherwise go unobserved, suggesting
it as a powerful tool for mental health researchers.

The main vehicle for studying mental health in
social media has been the use of surveys, e.g.,
depression battery (De Choudhury, 2013) or per-
sonality test (Schwartz et al., 2013), to deter-
mine characteristics of a user coupled with analyz-
ing their corresponding social media data. Work
in this area has mostly focused on depression
(De Choudhury et al., 2013d; De Choudhury et al.,
2013b; De Choudhury et al., 2013c), and the num-
ber of users is limited by those that can complete
the appropriate survey. For example, De Choud-
hury et al. (2013d) solicited Twitter users to take
the CES-D and to share their public Twitter pro-
file, analyzing linguistic and behavioral patterns.
While this type of study has produced high qual-
ity data, it is limited in size (by survey respon-
dents) and scope (to diagnoses which have a bat-
tery amenable to administration over the internet).

In this paper we examine a range of mental
health disorders using automatically derived sam-
ples from large amounts of Twitter data. Rather
than rely on surveys, we automatically identify
self-expressions of mental illness diagnoses and
leverage these messages to construct a labeled data
set for analysis. Using this dataset, we make the
following contributions:

• We demonstrate the effectiveness of our au-
tomatically derived data by showing that sta-
tistical classifiers can differentiate users with
four different mental health disorders: de-
pression, bipolar, post traumatic stress disor-
der and seasonal affective disorder.

• We conduct a LIWC analysis of each dis-
order to measure deviations in each illness
group from a control group, replicating pre-
vious findings for depression and providing
new findings for bipolar, PTSD and SAD.

• We conduct an open-vocabulary analysis that
captures language use relevant to mental
health beyond what is captured with LIWC.

Our results open the door to a range of large scale
analysis of mental health issues using Twitter.

2 Related Work

For a good retrospective and prospective sum-
mary of the role of social media in mental health

research, we refer the reader to De Choudhury
(2013). De Choudhury identifies ways in which
NLP has and can be used on social media data to
produce what the relevant mental health literature
would predict, both at an individual level and a
population level. She proceeds to identify ways
in which these types of analyses can be used in
the near and far term to influence mental health
research and interventions alike.

Differences in language use have been observed
in the personal writing of students who score
highly on depression scales (Rude et al., 2004),
forum posts for depression (Ramirez-Esparza et
al., 2008), self narratives for PTSD (He et al.,
2012; D’Andrea et al., 2011; Alvarez-Conrad et
al., 2001), and chat rooms for bipolar (Kramer
et al., 2004). Specifically in social media, dif-
ferences have previously been observed between
depressed and control groups (as assessed by
internet-administered batteries) via LIWC: de-
pressed users more frequently use first person pro-
nouns (Chung and Pennebaker, 2007) and more
frequently use negative emotion words and anger
words on Twitter, but show no differences in posi-
tive emotion word usage (Park et al., 2012). Simi-
larly, an increase in negative emotion and first per-
son pronouns, and a decrease in third person pro-
nouns, (via LIWC) is observed, as well as many
manifestations of literature findings in the pattern
of life of depressed users (e.g., social engagement,
demographics) (De Choudhury et al., 2013d). Dif-
ferences in language use in social media via LIWC
have also been observed between PTSD and con-
trol groups (Coppersmith et al., 2014).

For population-level analysis, surveys such as
the Behavioral Risk Factor Surveillance System
(BRFSS) are conducted via telephone (Centers
for Disease Control and Prevention (CDC), 2010).
Some of these surveys cover relatively few par-
ticipants (often in the thousands), have significant
cost, and have long delays between data collec-
tion and dissemination of the findings. However,
De Choudhury et al. (2013c) presents a promising
population-level analysis of depression that high-
lights the role of NLP and social media.

3 Data

All data we obtain is public, posted between
2008 and 2013, and made available from Twitter
via their application programming interface (API).
Specifically, this does not include any data that has

52



Genuine Statements of Diagnosis

In loving memory my mom, she was only 42, I was 17 & taken away from me. I was diagnosed with having P.T.S.D LINK
So today I started therapy, she diagnosed me with anorexia, depression, anxiety disorder, post traumatic stress disorder and
wants me to
@USER The VA diagnosed me with PTSD, so I can’t go in that direction anymore
I wanted to share some things that have been helping me heal lately. I was diagnosed with severe complex PTSD and... LINK

Disingenuous Statements of Diagnosis

“I think I’m I’m diagnosed with SAD. Sexually active disorder” -anonymous
LOL omg my bro the “psychologist” just diagnosed me with seasonal ADHD AHAHAHAAAAAAAAAAA IM DYING.
The winter blues: Yesterday I was diagnosed with seasonal affective disorder. Now, this sounds a lot more dramat... LINK

Table 1: Examples found via regular expression keyword search for diagnosis tweets.

been marked as ‘private’ by the author or any di-
rect messages.

Diagnosed Group We seek users who publicly
state that they have been diagnosed with various
mental illnesses. Users may make such a state-
ment to seek support from others in their social
network, to fight the taboo of mental illness, or
perhaps as an explanation of some of their behav-
ior. Tweets were obtained using regular expres-
sions on a large multi-year health related collec-
tion, e.g. “I was diagnosed with X.” We searched
for four conditions: depression, bipolar disorder,
post traumatic stress disorder (PTSD) and sea-
sonal affective disorder (SAD). The matched diag-
nosis tweets were manually labeled as to whether
the tweet contained a genuine statement of a men-
tal health diagnosis. Table 1 shows examples of
both genuine statements of diagnosis and disin-
genuous statements (often jokes or quotes).

Next, we retrieved the most recent tweets (up
to 3200) for each user with a genuine diagnosis
tweet. We then filtered the users to remove those
with fewer than 25 tweets and those whose tweets
were not at least 75% in English (measured using
the Compact Language Detector1). These filter-
ing steps left us with users that were considered
positive examples. Table 2 indicates the number
of users and tweets found for each of the mental
health categories examined. We manually exam-
ined and annotated only half the diagnosis state-
ments for depression – indicating there are likely
800-900 depression users available via these auto-
matic methods from our collection, compared to
the 117 obtained via the methods of De Choud-
hury et al. (2013d). Additionally, we emphasize
the low cost and effort of our automated effort
as compared to their crowdsourced survey meth-

1https://code.google.com/p/cld2/

ods. The difference in collection methods also
suggests that the two have a reasonable chance of
being complementary. This is especially signif-
icant when considering disorders with lower in-
cidence rates than depression (arguably the high-
est), where respondents to crowdsourced surveys
or self-stated diagnoses alike are rare.

This method is similar in spirit to that of De
Choudhury et al. (2013c), where they inferred
a tweet-level classifier for depression from user-
level labels (specifically, tweets from the past three
months from users scoring highly on CES-D for
the positive class and conversely for the negative).

Control Group To build models for analysis
and to validate the data, we also need a sample of
the general population to use as an approximation
of community controls. We follow a similar pro-
cess: randomly select 10k usernames from a list
of Twitter users who posted to a separate random
historical collection within a selected two week
window, downloaded the 3200 most recent tweets
from these users, and apply our two filters: at least
25 tweets and 75% English. This yields a control
group of 5728 random users, whose 13.7 million
tweets were used as negative examples.

Caveats Our method for finding users with
mental health diagnoses has significant caveats: 1)
the method may only capture a subpopulation of
each disorder (i.e., those who are speaking pub-
licly about what is usually a very private mat-
ter), which may not truly represent all aspects of
the population as a whole. 2) This method in
no way verifies whether this diagnosis is genuine
(i.e., people are not always truthful in self-reports).
However, given the stigma often associated with
mental illness, it seems unlikely users would tweet
that they are diagnosed with a condition they do
not have. 3) The control group is likely contami-

53



Match Users Tweets
Bipolar 6k 394 992k
Depression 5k 441 1.0m
PTSD 477 244 573k
SAD 389 159 421k
Control 10k 5728 13.7m

Table 2: Number of users matching the diagnosis regular
expression, users labeled with genuine diagnoses and tweets
retrieved from diagnosed users for each mental health condi-
tion.

nated by the presence of users that are diagnosed
with the various conditions investigated. We make
no attempt to remove these users, and if we as-
sume that the prevalence of each disorder in the
general population is similar in our control groups,
we likely have hundreds of such diagnosed users
contaminating our control training data. 4) Twitter
users are not an entirely representative sample of
the population as a whole. Despite these caveats,
we find that this method yielded promising results
as discussed in the next sections.

Comorbidity Since some of these disorders
have high comorbidity, there are some users in
more than one class (e.g., those that state a diagno-
sis for PTSD and depression): Bipolar and depres-
sion have 19 users in common (4.8% of the bipo-
lar users, 4.3% of the depression users), PTSD and
depression share 10 (4.0% of PTSD, 2.2% of de-
pression), and bipolar and PTSD share 9 (2.2% of
bipolar, 3.6% of PTSD). Two users state diagnosis
of bipolar, PTSD and depression (less than 1% of
each set). No users stated diagnoses of both SAD
and any other condition investigated.

4 Methods

We quantify various aspects of each user’s lan-
guage usage and pattern of life via automated
methods, extracting features for subsequent ma-
chine learning. We use these to (1) replicate pre-
vious findings, (2) build classifiers to separate di-
agnosed from control users, and (3) introspect on
those classifiers. Introspection here shows us what
quantified signals in the content the classifiers base
their decision on, and thus we can gain intuition
about what signals are present in the content rele-
vant to mental health.

4.1 Linguistic Inquiry Word Count (LIWC)
LIWC provides clinicians with a tool for gather-
ing quantitative data regarding the state of a pa-
tient from the patient’s writing (Pennebaker et al.,

2007). Previous work has found signal in the ‘pos-
itive affect’ and ‘negative affect’ categories of the
LIWC when applied to social media (including
Twitter), so we examine their correlations sepa-
rately, as well as in the context of other LIWC
categories (De Choudhury et al., 2013a). In all,
we examine some of the LIWC categories directly
(Swear, Anger, PosEmo, NegEmo, Anx) and com-
bine pronoun classes by linguistic form: I and We
classes are combined to form Pro1, You becomes
Pro2 and SheHe and They become Pro3. Each of
these classes provides one feature used by subse-
quent machine learning and our other analyses.

4.2 Language Models (LMs)
Language models are commonly used to estimate
how likely a given sequence of words is. Gener-
ally, an n-gram language model refers to a model
that examines strings of up to n words long. This
is less than ideal for applications in social me-
dia: spelling errors, shortenings, space removal,
and other aspects of social media data (especially
Twitter) confounds many traditional word-based
approaches. Thus, we employ two LMs, first a
traditional 1-gram LM (ULM) that examines the
probability of each whole word. Second, a char-
acter 5-gram LM (CLM) to examine sequences of
up to 5 characters.

LMs model the likelihood of sequences from
training data. In our case, we build one of each
model from the positive class (tweets from one
class of diagnosed users – e.g., PTSD), yield-
ing ULM+ and CLM+. We also build one of
each model from the negative class (control users),
yielding ULM− and CLM−. We score each tweet
by computing these probabilities and classifying it
according to which model has a higher probability
(e.g., for a given tweet, is ULM+ > ULM−?).

4.3 Pattern of Life Analytics
For brevity, we only briefly discuss the pattern of
life analytics, since they do not depend on sig-
nificant NLP. They examine how correlates found
to be significant in the mental health literature
may manifest and be measured in social media
data. These are all imperfect proxies for the find-
ings from the literature, but our experiments will
demonstrate that they do collectively provide in-
formation relevant to mental health.

For each of the following analytics we extract
one feature to use in subsequent machine learn-
ing. Social engagement has been correlated with

54



●●

●

●

●

●

●

●

●●

●

●

●

●

● ●●

●

●●

●

●●●●●●●●●●●

●

●●●●●●●●●● ●●●●●

●

●

●

●●●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●

●

●●

●
●

●
●

●

●

●

●

●●

●

●

●

●

●●

●
●

●

●●●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●●

●

●●

●

●

●
●

●
●

●

●

●

●

●
●●

●●●

●

● ●

●

●
●

●●
●●

● ●●●

●

●●●●

●

●●

●●
●●

●

●●
●

●●

●

●

●

●

●

●
●

●

●

●

● ●

●

●
●●
●
●
● ●

●
●

●

●●●●

●

●

●

●

●

●

●

●

●

●●

●
●

●●●

●

●
●

●

●

●

●

●

●

●

●

●

●●

●

●

●
●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●●

●
●

●

●

●

●●

●

●

●

●
●

●●●●●●●

●

●●●

●

●

●●

●

●

●

●

●

●●

●
●●

●

●

●●●●●

●

●

●

●●●●●

●

● ●

●

●

●●

●

●

●

●

●

●●

●

●

●●●

●
●

●

●
●

●

●

●

●●

●

●

●

●

●●
●

●●●

●

●

●

● ●●●●

●

●

●

●

●

●

●●

●

●

●

● ●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●●

●

●

●
●●

●

●●

●

●

●●

●

●

●●

●

●

●

●●●●●●●

●

●

●

●

●

●

●●

●

●

●

●

●

●

●

●●

●

●

●●●●

●

●●●●●●

●

●●●●●

●

●

●

●

●

●

0.
00

0.
05

0.
10

0.
15 Pro1

* *
Pro2

*
Pro3

*
Swear

*
Anger

*
PosEmo NegEmo

*
Anxiety

****

0
0.
00
5

0.
01

0.
01
5

Figure 1: Box and whiskers plot of proportion of tweets each user has (y-axis) matching various LIWC categories. Each
bar represents one LIWC category for one condition – PTSD in purple, depression in blue, SAD in orange, bipolar in red and
control in gray. Anxiety occurs an order of magnitude less often than the others, so its proportion is on the right y-axis (and thus
not comparable to the others). Statistically significant deviations from control users are denoted by asterisks.

positive mental health outcomes (Greetham et al.,
2011; Berkman et al., 2000; Organization, 2001;
De Choudhury et al., 2013d), which is difficult
to measure directly so we examine various ways
in which this may be manifest in a user’s tweet
stream: Tweet rate measures how often a twit-
ter user posts (a measure of overall engagement
with this social media platform) and Proportion
of tweets with @mentions measures how often
a user posts ‘in conversation’ (for lack of better
terms) with other users. Number of @mentions is
a measure of how often the user in question en-
gages other users, while Number of self @men-
tions is a measure of how often the user responds
to mentions of themselves (since users rarely in-
clude their own username in a tweet). To estimate
the size of a user’s social network, we calculate
Number of unique users @mentioned and Number
of users @mentioned at least 3 times, respectively.

For each of the following analytics, we calcu-
late the proportion of a user’s tweets that the ana-
lytic finds evidence in: Insomnia and sleep distur-
bance is often a symptom of mental health disor-
ders (Weissman et al., 1996; De Choudhury et al.,
2013d), so we calculate the proportion of tweets
that a user makes between midnight and 4am ac-
cording to their local timezone. Exercise has
also been correlated with positive mental health
outcomes (Penedo and Dahn, 2005; Callaghan,
2004), so we examine tweets mentioning one of a
small set of exercise-related terms. We also use an
English sentiment analysis lexicon from Mitchell
et al. (2013) to score individual tweets according
to the presence and valence of sentiment words.

We apply no thresholds, so any tweet with a senti-
ment score above 0 was considered positive, below
0 was considered negative, and those with score 0
were considered to have no sentiment. Thus we
use the proportion of Insomnia, Exercise, Positive
Sentiment and Negative Sentiment tweets as fea-
tures in subsequent machine learning and analysis.

5 Results

We present three types of experiments to evalu-
ate the quality and character of these data, and to
demonstrate some quantifiable mental health sig-
nals in Twitter. First, we validate our method for
obtaining data by replicating previous findings us-
ing LIWC. Next, we build classifiers to distinguish
each group from the control group, demonstrating
that there is useful signal in the language of each
group, and compare these classifiers. Finally, we
analyze the correlations between our analytics and
classifiers to uncover relationships between them
and derive insight into quantifiable and relevant
mental health signals in Twitter.

Validation First, we provide some validation
for our novel method for gathering samples. We
demonstrate that language use, as measured by
LIWC, is statistically significantly different be-
tween control and diagnosed users. Figure 1
shows the proportion of tweets from each user
that scores positively on various LIWC categories
(i.e., have at least one word from that category).
Box-and-whiskers plots (Tukey, 1977)2 summa-
rize a distribution of observations and ease com-

2For a modern implementation see Wickham (2009).

55



False Alarm: 0.1 0.2
Bipolar 0.64 0.82

Depression 0.48 0.68

PTSD 0.67 0.81

SAD 0.42 0.65

Figure 2: ROC curves for separating diagnosed from con-
trol users, compared across disorders: bipolar in red, depres-
sion in blue, PTSD in purple, SAD in orange. The preci-
sion (diagnosed, correctly labeled) for each disorder at false
alarm (control, labeled as diagnosed) rates of 10% and 20%
are shown to the right of the ROC curve. Chance performance
is indicated by the dotted black line.

parison between them (here, each observation is
the proportion of a user’s tweets that score posi-
tively on LIWC). The median of the distribution
is the black horizontal line in the middle of the
bar, the bar covers the inter quartile range (where
50% of the observations lie), the whiskers are a
robust estimate of the extent of the data, with out-
liers plotted as circles beyond the whiskers. An
approximation of statistical significance is indi-
cated by the pinched in notches on each bar. If
the notches on the bars do not overlap, the dif-
ferences between those distributions is different
(α<0.05, 95% confidence interval). Each bar is
colored according to diagnosis, and each group
of 5 bars notes the scores for one LIWC cat-
egory. Differences that reach statistical signifi-
cance from the control group are noted with as-
terisks (e.g., Pro1, Swear, Anger, NegEmo and
Anxiety are statistically significantly different for
the depression group). Importantly, this repli-
cates previous findings of significant differences
between depressed users (according to an internet-
administered diagnostic battery): significant in-
creases are expected in NegEmo, Anger, Pro1 and
Pro3 and no change in PosEmo, given all previous
work (Park et al., 2012; Chung and Pennebaker,
2007; De Choudhury et al., 2013d). We repli-
cate all these findings except the increase in Pro3
(which only De Choudhury et al. (2013d) found),
which validates our data collection methods.

Classification We next explore the ability of
the various analytics to separate diagnosed from
control users and assess performance on a leave-
one-out cross-validation task. We train a log lin-
ear classifier on the features described in §4 using
scikit-learn (Pedregosa et al., 2011).

Bipolar Depression

PTSD SAD

Figure 3: ROC curves of performance of individual analyt-
ics for each disorder: LIWC in blue, pattern of life in yellow,
CLM in red, ULM in green, all in black. Chance performance
is indicated by the dotted black line.

The receiver operating characteristic (ROC)
curves in Figures 2 and 3 demonstrate perfor-
mance of the various classifiers at the task of sepa-
rating diagnosed from control groups. In all cases,
the correct detections (or hits) are on the y-axis
and the false detections (or false alarms) are on
the x-axis. Figure 2 compares performance across
diagnoses, one line per disorder.

Figure 3 shows one plot per mental health con-
dition, with the performance of the various an-
alytics, individually and in concert as individual
ROC curves. A few trends emerge – 1) All an-
alytics show some ability to separate the classes,
indicating they are finding useful signals. 2) The
LMs provide superior performance to the other an-
alytics, indicating there are more signals present
in the language than are captured by LIWC and
pattern-of-life analytics. For readability we do not
show the performance of all combinations of an-
alytics, but they perform as expected: any set of
them perform equal to or better than their indi-
vidual components. Taken together, this indicates
that there is information relevant to separating di-
agnosed users from controls in all the analytics
discussed here. Furthermore, this highlights that
there remains significant signals to be uncovered
and understood in the language of social media.

These trends also allow us to compare the dis-
orders as manifest in language usage, though this

56



tends to raise more questions than it answers. Gen-
erally, the pattern-of-life analytics and LIWC are
on par, but this is decidedly not true for depres-
sion, where pattern-of-life seems to perform espe-
cially poorly, and for SAD, where pattern-of-life
seems to perform especially well. This indicates
that the depression users have patterns-of-life that
look more similar to the controls than is the case
for the other disorders (perhaps especially surpris-
ing given the inclusion of the sentiment lexicon)
and that there may be significant correlation be-
tween pattern-of-life factors and SAD.

5.1 Analytic Introspection

To examine correlations between the analytics and
the linguistic content they depend on, we scored
a random subset of 1 million tweets from control
users with each of the linguistic analytics, and plot
their Pearson’s correlation coefficients (r) in Fig-
ure 4. A simple overlap of wordlists is not suf-
ficient to assess the true utility of these methods
since it does not take into account the frequency
of occurrence of each word, nor the correlation be-
tween these words in real data (e.g., does a classi-
fier based on the LIWC category Swear provide
redundant information to the sentiment analysis).
Each row and column in Figure 4 represents one of
the 17 analytics, in the same order. Colors denote
Bonferroni-corrected Pearson’s r for statistically
significant correlations between the analytic on the
row and column. Correlations that do not reach
statistical significance are in aquamarine (corre-
sponding to r=0). Excluded for brevity is a sanity
check of a χ2 test between the analytics to assert
they were scoring significantly differently.

The strong correlations between the various
LIWC analytics, notably Swear, Anger and
NegEmo, likely indicates that the analytics are
triggered by the same word(s) – in this case pro-
fanity. Similarly for LIWC’s PosEmo and the sen-
timent lexicon – ‘happy’ for example. The corre-
lation between CLM for various diagnoses is par-
ticularly intriguingly, as it is in line with known
patterns of comorbidity: major depressive disor-
der, PTSD, and bipolar all have observed comor-
bidity (Brady et al., 2000; Campbell et al., 2007;
McElroy et al., 2001) while SAD is currently con-
sidered a specifier of major depressive disorder or
bipolar disorder (American Psychiatric Associa-
tion, 2013; Lurie et al., 2006), without published
findings indicating comorbidity. Indeed our small

Figure 4: Pearson’s r correlations between various analyt-
ics, color indicates the strength of statistically significant cor-
relations, or 0 (aquamarine) otherwise. Bonferroni corrected,
each comparison is significant only if α<0.0002). Rows and
columns represent the analytics in the same order, so the di-
agonal is self-correlation.

sample dataset follows the same trends, where
we observed users with multiple diagnoses exist
within depression, PTSD, and bipolar, but none
exist with SAD. The correlation observed is too
large to be solely attributed to those users shared
between the groups, though (correlations at most
r = 0.05 would be attributable to that alone). Fur-
thermore, when taken in combination with the dif-
ferent patterns exhibited by the groups as seen in
Figure 1, this correlation is not solely attributable
to LIWC categories either. At its core, these cor-
relations seem to suggest that similar language
is employed by users diagnosed with these occa-
sionally comorbid disorders, and dissimilar lan-
guage by users with SAD. This should be taken as
merely suggestive of the type of analysis one could
do, though, since the literature does not present a
strong and clear prediction for the comorbidity and
exhibited symptoms (to include language use).

Interestingly, the lack of (or negative) correla-
tion between most of the analytics again highlights
the complexity of the mental illnesses and the di-
vergent signals it presents. Additionally, the lack
of correlation between ULM and the other models
is to be expected, since they are basing their scores
on significantly more words (or different signals as
is the case for CLM). Each one of these analytics is
highly imperfect, and often give contradictory ev-
idence, but when combined, the machine learning
algorithms are able to sort through the conflicting
signals with some success.

57



Analytic Example Tweet Text
Bipolar LM I’m insecure because being around your ex of 4 years little sister, makes me feel a slight bit uncom-

fortable. Ok.
Depression LM Pain has a weird way of working. You’re still the same person from before the pain, but that person is

underneath & doesn’t come out.
PTSD LM Don’t wanna get out my bed but I really need to get up & prepare myself for work

Sentiment(+) NAME is absolutely unbelievable, he just gets better and better every time I see him. The best play in
the world, no doubt about it.

Sentiment(-) I hate losing people in my life. I try so hard to not let it happen
PosEmo Wowee...that was a hectic day... Got more done than expected but so glad to be in bed now. Grateful

for my supportive husband & loving pooch
Functioning if i had a dollar for all the grammatical errors ive ever typed, my college tuition, book cost, and dorm

rent would be paid in full
NegEmo My tooth hurts, my neck hurts, my mouth hurts, my toungue hurts, my head hurts...kill me now.

Anx don’t stress over someone who is going to stress over you..
Anger Ugly n arrogant sums everytin up.shdnt hv ffd her seff

Table 3: Example high scoring tweets from each analytic.

6 Conclusion

We demonstrate quantifiable signals in Twitter
data relevant to bipolar disorder, major depres-
sive disorder, post-traumatic-stress disorder and
seasonal affective disorder. We introduce a novel
method for automatic data collection and validate
its veracity by 1) replicating observations of sig-
nificant differences between depressed and control
user groups and 2) constructing classifiers capa-
ble of separating diagnosed from control users for
each disorder. This data allows us to demonstrate
equivalent differences in language use (according
to LIWC) for bipolar, PTSD, and SAD. Further-
more, we provide evidence that more information
relevant to mental health is encoded in language
use in social media (above and beyond that cap-
tured by methods based on the mental health lit-
erature). By examining correlations between the
various analytics investigated, we provide some
insight into what quantifiable linguistic informa-
tion is captured by our classifiers. We finally
demonstrate the utility of examining multiple dis-
orders simultaneously and other larger analyses,
difficult or impossible with other methods.

Crucially, we expect that these novel data col-
lection methods can provide complementary infor-
mation to existing survey-based methods, rather
than supplant them. For many disorders rarer
than depression (which has comparatively high in-
cidence rates), we suspect that finding any data
will be a challenge, in which case combining
these methods with the existing survey collection
methods may be the best way to obtain sufficient
amounts of data for statistical analyses.

Since the LMs take more information into ac-
count when modeling the language usage of di-

agnosed and control users, it is unsurprising that
they outperform LIWC and pattern-of-life analy-
ses alone, but this is evidence of as-of-yet undis-
covered linguistic differences between diagnosed
and control users for all disorders investigated.
Uncovering and interpreting these signals can be
best accomplished through collaboration between
NLP and mental health researchers.

Naturally, some caveats come with these re-
sults: while identifying genuine self-statements of
diagnosis in Twitter works well for some condi-
tions, others exist for which there were few or
no diagnoses stated. For Alzheimer’s, the demo-
graphic with the majority of diagnoses does not
frequently use Twitter (or likely any social me-
dia). Eating disorders are also elusive via this
method, though related automatic methods (e.g.,
using disorder-related hashtags) may address this.
Finally, those willing to publicly reveal a mental
health diagnosis may not be representative of the
population suffering from that mental illness.

All these experiments, taken together, indicate
that there are a diverse set of quantifiable signals
relevant to mental health observable in Twitter.
They indicate that individual- and population-level
analyses can be made cheaper and more timely
than current methods, yet there remains as-of-yet
untapped information encoded in language use –
promising a rich collaboration between the fields
of natural language processing and mental health.

Acknowledgments: The authors would like to
thank Kristy Hollingshead for thoughtful com-
ments and contributions throughout this research.

58



References
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.

Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media (ICWSM).

Jennifer Alvarez-Conrad, Lori A. Zoellner, and
Edna B. Foa. 2001. Linguistic predictors of trauma
pathology and physical health. Applied Cognitive
Psychology, 15(7):S159–S170.

American Psychiatric Association. 2013. Diagnostic
Statistical Manual 5. American Psychiatric Associ-
ation.

Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza
epidemics using twitter. In Empirical Natural Lan-
guage Processing Conference (EMNLP).

John W. Ayers, Benjamin M. Althouse, Jon-Patrick
Allem, J. Niels Rosenquist, and Daniel E. Ford.
2013. Seasonality in seeking mental health infor-
mation on google. American journal of preventive
medicine, 44(5):520–525.

John W. Ayers, Benjamin M. Althouse, and Mark
Dredze. 2014. Could behavioral medicine lead the
web data revolution? Journal of the American Med-
ical Association (JAMA), February 27.

Lisa F. Berkman, Thomas Glass, Ian Brissette, and
Teresa E. Seeman. 2000. From social integration
to health: Durkheim in the new millennium? Social
Science & Medicine, 51(6):843–857, September.

Amber Boydstun, Rebecca Glazier, Timothy Jurka, and
Matthew Pietryka. 2013. Examining debate effects
in real time: A report of the 2012 React Labs: Ed-
ucate study. The Political Communication Report,
23(1), February. [Online; accessed 25-February-
2014].

Kathleen T. Brady, Therese K. Killeen, Tim Brewerton,
and Sylvia Lucerini. 2000. Comorbidity of psy-
chiatric disorders and posttraumatic stress disorder.
Journal of Clinical Psychiatry.

Patrick Callaghan. 2004. Exercise: a neglected inter-
vention in mental health care? Journal of Psychi-
atric and Mental Health Nursing, 11:476–483.

Duncan G. Campbell, Bradford L. Felker, Chuan-Fen
Liu, Elizabeth M. Yano, JoAnn E. Kirchner, Domin
Chan, Lisa V. Rubenstein, and Edmund F. Chaney.
2007. Prevalence of depression-PTSD comorbidity:
Implications for clinical practice guidelines and pri-
mary care-based interventions. Journal of General
Internal Medicine, 22(6):711–718.

Centers for Disease Control and Prevention (CDC).
2010. Behavioral risk factor surveillance system
survey data.

Cindy Chung and James Pennebaker. 2007. The psy-
chological functions of function words. Social com-
munication, pages 343–359.

Glen A. Coppersmith, Craig T. Harman, and Mark
Dredze. 2014. Measuring post traumatic stress
disorder in Twitter. In Proceedings of the Interna-
tional AAAI Conference on Weblogs and Social Me-
dia (ICWSM).

Wendy D’Andrea, Pearl H. Chiu, Brooks R. Casas,
and Patricia Deldin. 2011. Linguistic predictors of
post-traumatic stress disorder symptoms following
11 September 2001. Applied Cognitive Psychology,
26(2):316–323, October.

Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013a. Major life changes and behav-
ioral markers in social media: Case of childbirth. In
Proceedings of the ACM Conference on Computer
Supported Cooperative Work and Social Computing
(CSCW).

Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013b. Predicting postpartum changes in
emotion and behavior via social media. In Proceed-
ings of the ACM Annual Conference on Human Fac-
tors in Computing Systems (CHI), pages 3267–3276.
ACM.

Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013c. Social media as a measurement
tool of depression in populations. In Proceedings of
the Annual ACM Web Science Conference.

Munmun De Choudhury, Michael Gamon, Scott
Counts, and Eric Horvitz. 2013d. Predicting de-
pression via social media. In Proceedings of the In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM).

Munmun De Choudhury, Andres Monroy-Hernandez,
and Gloria Mark. 2014. ” narco” emotions: Affect
and desensitization in social media during the mexi-
can drug war.

Munmun De Choudhury. 2013. Role of social media
in tackling challenges in mental health. In Proceed-
ings of the 2nd International Workshop on Socially-
Aware Multimedia, pages 49–52.

Mark Dredze. 2012. How social media will change
public health. IEEE Intelligent Systems, 27(4):81–
84.

Danica Vukadinovic Greetham, Robert Hurling,
Gabrielle Osborne, and Alex Linley. 2011. Social
networks and positive and negative affect. Procedia
- Social and Behavioral Sciences, 22:4–13, January.

Carleen Hawn. 2009. Take Two Aspirin And Tweet
Me In The Morning: How Twitter, Facebook, And
Other Social Media Are Reshaping Health Care.
Health Affairs, 28(2):361–368.

59



Qiwei He, Bernard P. Veldkamp, and Theo de Vries.
2012. Screening for posttraumatic stress disorder
using verbal features in self narratives: A text min-
ing approach. Psychiatry Research.

Adam D. I. Kramer, Susan R. Fussell, and Leslie D.
Setlock. 2004. Text analysis as a tool for analyz-
ing conversation in online support groups. In Pro-
ceedings of the ACM Annual Conference on Human
Factors in Computing Systems (CHI).

Stephen J. Lurie, Barbara Gawinski, Deborah Pierce,
and Sally J. Rousseau. 2006. Seasonal affective dis-
order. American family physician, 74(9).

Susan L. McElroy, Lori L. Altshuler, Trisha Suppes,
Paul E. Keck, Mark A. Frye, Kirk D. Denicoff,
Willem A. Nolen, Ralph W. Kupka, Gabriele S. Lev-
erich, Jennifer R. Rochussen, A. John Rush Rush,
and Robert M. Post Post. 2001. Axis I psychi-
atric comorbidity and its relationship to historical ill-
ness variables in 288 patients with bipolar disorder.
American Journal of Psychiatry, 158(3):420–426.

Margaret Mitchell, Jacqueline Aguilar, Theresa Wil-
son, and Benjamin Van Durme. 2013. Open domain
targeted sentiment. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1643–1654.

World Health Organization. 2001. The world health
report 2001 - Mental health: New understanding,
new hope. Technical report, Genf, Schweiz.

Minsu Park, Chiyoung Cha, and Meeyoung Cha. 2012.
Depressive moods of users portrayed in Twitter. In
Proceedings of the ACM SIGKDD Workshop on
Healthcare Informatics (HI-KDD).

Michael J. Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing Twitter for public health. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media (ICWSM).

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
and Matthieu Perrot Édouard Duchesnay. 2011.
scikit-learn: Machine learning in Python. The Jour-
nal of Machine Learning Research, 12:2825–2830.

Frank J. Penedo and Jason R. Dahn. 2005. Exer-
cise and well-being: a review of mental and phys-
ical health benefits associated with physical activ-
ity. Current Opinion in Psychiatry, 18(2):189–193,
March.

James W. Pennebaker, Cindy K. Chung, Molly Ire-
land, Amy Gonzales, and Roger J. Booth. 2007.
The development and psychometric properties of
LIWC2007.

Nairan Ramirez-Esparza, Cindy K. Chung, Ewa
Kacewicz, and James W. Pennebaker. 2008. The
psychology of word use in depression forums in En-
glish and in Spanish: Testing two text analytic ap-
proaches. In Proceedings of the International AAAI
Conference on Weblogs and Social Media (ICWSM).

Philip Resnik, Anderson Garron, and Rebecca Resnik.
2013. Using topic modeling to improve prediction
of neuroticism and depression. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural, pages 1348–1353.

Stephanie S. Rude, Eva-Maria Gortner, and James W.
Pennebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition
& Emotion, 18(8):1121–1133, December.

H. Andrew Schwartz, Johannes C. Eichstaedt, Mar-
garet L. Kern, Lukasz Dziurzynski, Stephanie M.
Ramones, Megha Agrawal, Achal Shah, Michal
Kosinski, David Stillwell, Martin E. P. Seligman,
and Lyle H. Ungar. 2013. Personality, gender,
and age in the language of social media: The open-
vocabulary approach. PLOS One, 8(9).

Yla R. Tausczik and James W. Pennebaker. 2010. The
psychological meaning of words: LIWC and com-
puterized text analysis methods. Journal of Lan-
guage and Social Psychology, 29(1):24–54.

John W. Tukey. 1977. Box-and-whisker plots. Ex-
ploratory Data Analysis, pages 39–43.

Myrna M. Weissman, Roger C. Bland, Glorisa J.
Canino, Carlo Faravelli, Steven Greenwald, Hai-
Gwo Hwu, Peter R. Joyce, Eile G. Karam, Chung-
Kyoon Lee, Joseph Lellouch, Jean-Pierre Lépine,
Stephen C. Newman, Maritza Rubio-Stipec, J. Elis-
abeth Wells, Priya J. Wickramaratne, Hans-Ulrich
Wittchen, and Eng-Kung Yeh. 1996. Cross-national
epidemiology of major depression and bipolar dis-
order. Journal of the American Medical Association
(JAMA), 276(4):293–299.

Hadley Wickham. 2009. ggplot2: elegant graphics for
data analysis. Springer.

60


