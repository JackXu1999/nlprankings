



















































TIGEr: Text-to-Image Grounding for Image Caption Evaluation


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2141–2152,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2141

TIGEr: Text-to-Image Grounding for Image Caption Evaluation

Ming Jiang1, Qiuyuan Huang3, Lei Zhang3, Xin Wang2
Pengchuan Zhang3, Zhe Gan3, Jana Diesner1, Jianfeng Gao3

1University of Illinois at Urbana-Champaign, 2University of California, Santa Barbara
3Microsoft Research, Redmond

{mjiang17,jdiesner}@illinois.edu, xwang@cs.ucsb.edu
{qihua,leizhang,penzhan,zhe.gan,jfgao}@microsoft.com

Abstract

This paper presents a new metric called TIGEr
for the automatic evaluation of image caption-
ing systems. Popular metrics, such as BLEU
and CIDEr, are based solely on text match-
ing between reference captions and machine-
generated captions, potentially leading to bi-
ased evaluations because references may not
fully cover the image content and natural
language is inherently ambiguous. Building
upon a machine-learned text-image ground-
ing model, TIGEr allows to evaluate caption
quality not only based on how well a cap-
tion represents image content, but also on
how well machine-generated captions match
human-generated captions. Our empirical
tests show that TIGEr has a higher consistency
with human judgments than alternative exist-
ing metrics. We also comprehensively assess
the metric’s effectiveness in caption evaluation
by measuring the correlation between human
judgments and metric scores.

1 Introduction

Image captioning is a research topic at the nexus of
natural language processing and computer vision,
and has a wide range of practical applications (Ho-
dosh et al., 2013; Fang et al., 2015), such as image
retrieval and human-machine interaction. Given
an image as input, the task of image captioning is
to generate a text that describes the image content.
Overall, prior studies of this task have been focus-
ing on two aspects: 1) building large benchmark
datasets (Lin et al., 2014; Hodosh et al., 2013),
and 2) developing effective caption generation al-
gorithms (Karpathy and Li, 2015; Bernardi et al.,
2016; Gan et al., 2017). Remarkable contributions
that have been made, but assessing the quality of
the generated captions is still an insufficiently ad-
dressed issue. Since numerous captions with vary-
ing quality can be produced by machines, it is im-

Figure 1: An example of caption evaluation challenge.
Given two candidate captions: (a) correctly describes
image content, but most words in this caption are not
contained in references, while (b) overlaps with refer-
ences (“is holding a bottle of beer”), but involves wrong
information (“a woman with straight hair”). Prior met-
rics based only on text-level comparisons fail to as-
sess the quality of candidate captions, while our met-
ric (TIGEr) can detect this inconsistency by matching
a caption with image content. To explain the TIGEr re-
sult, we show the grounding weights of two illustrative
image regions with each candidate. The proper text in-
formation matched with each region is highlighted by
colors.

portant to propose an automatic evaluation metric
that is highly consistent with human judges, easy
to implement and interpret.

Prior evaluation metrics typically considered
several aspects, including content relevance, in-
formation correctness, grammaticality, and if ex-
pressions are human-like (Hodosh et al., 2013;
Bernardi et al., 2016). Rule-based metrics inspired
by linguistic features such as n-gram overlapping
are commonly used to evaluate machine-generated
captions (Chen et al., 2018; Karpathy and Li,
2015; Huang et al., 2019). However, these metrics
primarily evaluate a candidate caption based on
references without taking image content into ac-
count, and the possible information loss caused by
references may bring biases to the evaluation pro-
cess. Moreover, the ambiguity inherent to natural
language presents a challenge for rule-based met-



2142

rics that are based on text overlapping. As shown
in Figure 1, although Caption (a) describes the im-
age properly, most words in this sentence are dif-
ferent from the references. In contrast, Caption
(b) contains wrong information (“a woman with
straight hair”), but overlaps with human-written
references (“is holding a bottle of beer”). Con-
sequently, all existing metrics improperly assign a
higher score to Caption (b).

In order to address the aforementioned chal-
lenges, we propose a novel Text-to-Image Ground-
ing based metric for image caption Evaluation
(TIGEr), which considers both image content and
human-generated references for evaluation. First,
TIGEr grounds the content of texts (both refer-
ence and generated captions) in a set of image re-
gions by using a pre-trained image-text ground-
ing model. Based on the grounding outputs,
TIGEr calculates a score by comparing both the
relevance ranking and distribution of grounding
weights among image regions between the refer-
ences and the generated caption. Instead of eval-
uating image captions by exact n-gram matching,
TIGEr compares the candidate caption with refer-
ences by mapping them into vectors in a common
semantic space. As a result, TIGEr is able to de-
tect the paraphrases based on the semantic mean-
ing of caption sentences. A systematic compari-
son of TIGEr and other commonly used metrics
shows that TIGEr improves the evaluation perfor-
mance across multiple datasets and demonstrates a
higher consistency with human judgments. For ex-
ample, Figure 1 presents a case where only TIGEr
is able to assign a higher score to the caption can-
didate that is more consistent with human judg-
ments. Our main contributions include:

• We propose a novel automatic evaluation
metric, TIGEr1, to assess the quality of image
captions by grounding text captions in image
content.

• By performing an empirical study, we show
that TIGEr outperforms other commonly
used metrics, and demonstrates a higher and
more stable consistency with human judg-
ments.

• We conduct an in-depth analysis of the
assessment of metric effectiveness, which

1Code is released at https://github.com/
SeleenaJM/CapEval.

deepens our understanding of the character-
istics of different metrics.

2 Related Work

Caption Evaluation The goal of image caption
evaluation is to measure the quality of a generated
caption given an image and human-written refer-
ence captions (Bernardi et al., 2016).

In general, prior solutions to this task can be di-
vided into three groups. First, human evaluation
is typically conducted by employing human anno-
tators to assess captions (e.g., via Amazons Me-
chanical Turk) (Bernardi et al., 2016; Aditya et al.,
2015a; Wang et al., 2018b).

Second, automatic rule-based evaluation mea-
sures assess the similarity between references
and generated captions. Many metrics in this
group were extended from other related tasks (Pa-
pineni et al., 2002; Banerjee and Lavie, 2005;
Lin, 2004). BLEU (Papineni et al., 2002) and
METEOR (Banerjee and Lavie, 2005) were ini-
tially developed to evaluate machine translation
outcomes based on n-gram precision and recall.
ROUGE (Lin, 2004) was originally used in text
summarization, which measures the overlap of n-
grams using recall. Recently, two metrics were
specifically built for visual captioning: 1) CIDEr
(Vedantam et al., 2015) measures n-gram simi-
larity based on TF-IDF; and 2) SPICE (Ander-
son et al., 2016) quantifies graph similarity based
on the scene graphs built from captions. Overall,
these metrics focus on text-level comparisons, as-
suming that the information contained in human-
written references can well represent the image
content. Differing from prior work, we argue in
this study that references may not fully cover the
image content because both references and cap-
tions are incomplete and selective translations of
image contents made by human judges or auto-
mated systems. The ground truth can only be fully
revealed by taking the images themselves into ac-
count for evaluation.

Finally, machine-learned metrics use a trained
model to predict the likelihood of a testing cap-
tion as a human-generated description (Cui et al.,
2018; Dai et al., 2017). Prior studies have applied
learning-based evaluation for related text gener-
ation tasks, e.g., machine translation (Corston-
Oliver et al., 2001; Lowe et al., 2017; Kulesza
and Shieber, 2004). Most recently, Cui et al.
(2018) trained a hybrid neural network model for

https://github.com/SeleenaJM/CapEval
https://github.com/SeleenaJM/CapEval


2143

caption evaluation based on image and text fea-
tures. This work mainly focuses on the generation
of adversarial data used for model training. De-
spite improving the consistency with human de-
cisions, this approach may involve high compu-
tational cost and lead to overfitting (Gao et al.,
2019). Besides, the interpretability of the eval-
uation is limited due to the black-box nature of
end-to-end learning models. An even more serious
problem of using machine-learned metrics is the
so-called “gaming of the metric”: if a generation
system is optimized directly for a learnable met-
ric, then the system’s performance is likely to be
over-estimated. See a detailed discussion in Gao
et al. (2019).

Though caption evaluation is similar to tradi-
tional text-to-text generation evaluation in terms of
testing targets and evaluation principles, this task
additionally need to synthesize image and refer-
ence contents as ground truth information for fur-
ther evaluation, which is hard to achieve by using
a rule-based method with a strong human prior. In
this study, we propose a new metric that combines
the strengths of learning-based and rule-based ap-
proaches in terms of: 1) utilizing a pre-trained
grounding model2 to combine the image and text
information; and 2) scoring captions by defining
principle rules based on the grounding outputs to
make the evaluation interpretable.

Image-Text Matching The task of image-text
matching can be defined as the measurement of
semantic similarity between visual data and text
data. The main strategy of solutions designed for
this task is to map image data and text data into a
common semantic vector space (Fang et al., 2015;
Wang et al., 2018a; Lee et al., 2018). Prior studies
usually consider this issue from two perspectives:
From the perspective of data encoding, prior work
either regards an image/text as a whole or applies
bottom-up attention to encode image regions and
words/phrases (Karpathy et al., 2014; Kiros et al.,
2014). From the perspective of algorithm develop-
ment, prior work has often applied a deep learning
framework to build a joint embedding space for
images and texts, and some of these studies train
a model with ranking loss (Fang et al., 2015; Lee
et al., 2018; Frome et al., 2013) and some use clas-

2We find in our experiments that we can simply use the
pre-trained model as is to evaluate systems developed on dif-
ferent datasets, and the pre-trained model is very efficient to
run. Thus, the cost of computing TIGEr is not much higher
than that of computing other traditional metrics.

Figure 2: TIGEr framework.

sification loss (e.g., softmax function) (Jabri et al.,
2016; Fukui et al., 2016). In this work, we take
advantage of a state-of-the-art model for image-
text matching (Lee et al., 2018) and propose an
automatic evaluation metric for image captioning
based on the matching results. Our goal is to cap-
ture comprehensive information from input data
while also providing an explainable method to as-
sess the quality of an image description.

3 The TIGEr Metric

The overall framework of TIGEr is shown in Fig-
ure 2. With the assumption that a good machine-
generated caption C should generate a description
of an image V like a human would, we compare
C against a set of reference captions produced by
humansR = {R1, ..., Rk} in two stages.

The first stage is text-image grounding, where
for each caption-image pair, we compute a vector
of grounding scores, one for each specific region
of the image, indicating how likely the caption is
grounded in the region. The grounding vector can
also be interpreted as estimating how much atten-
tion a human judge or the to-be-evaluated image
captioning system pays on each image region in
generating the caption. A good system is expected
to distribute its attention among different regions
of an image similarly to that of human judges. For
example, if a human judge wrote a caption solely
based on a particular region of an image (e.g.,
a human face or a bottle as shown in Figure 1)
while ignoring the rest of the image, then a good
machine-generated caption would be expected to
describe only the objects in the same region.

The second stage is grounding vector compari-
son, where we compare the grounding vector be-
tween an image V and the machine-generated cap-
tion C, denoted by s(V,C), and that between V
and reference captions R, denoted by s(V,R).
The more similar these two vectors are, the higher
quality of C is. The similarity is measured by



2144

Figure 3: Overview of TIGEr calculation. For each pair of image and caption sentence, the pre-trained SCAN
model generates a similarity vector where each dimension denotes the grounding relevance between an image
region and the caption sentence in this region context. Given two similarity vectors denoting the grounding out-
comes of reference versus candidate captions, we measure: 1) region rank disagreement, and 2) the similarity of
two grounding weight distributions.

using two metric systems. The first one mea-
sures how similarly these image regions are or-
dered (by their grounding scores) in the two vec-
tors. The second one measures how similarly the
attention (indicated by grounding scores) is dis-
tributed among different regions of the image in
the two vectors. The TIGEr score is the average
score of the resulting two similarity scores.

In the rest of this section, we describe the two
stages in detail.

3.1 Text-Image Grounding

To compute the grounding scores of an image-
caption pair, we need to map the image and
its paired caption into vector representations in
the same vector space. We achieve this by us-
ing a Stacked Cross Attention Neural Network
(SCAN) (Lee et al., 2018), which is pre-trained
on the 2014 MS-COCO train/val dataset with de-
fault settings3. The training data contains 121,287
images, of which each is annotated with five text
descriptions. The architecture of SCAN is shown
in Figure 3.

We first encode caption C or R, which is a
sequence of m words, into a sequence of d-
dimensional (d = 300) word embedding vec-
tors, {w1, ...,wm}, using a bi-directional recur-
rent neural network (RNN) models with gated re-
current units (GRU) (Bahdanau et al., 2015). We
then map image V into a sequence of feature vec-
tors in two steps. First, we extract from V a set
of n = 36 region-level 2048-dimensional fea-
ture vectors using a pre-trained bottom-up atten-
tion model (Anderson et al., 2018). Then, we use
linear projection to convert these vectors into a

3Github link: https://github.com/kuanghuei/SCAN

sequence of d-dimensional image region vectors
{v1, ...,vn}.

We compute how much caption C, represented
as {w1, ...,wm}, is grounded to each image re-
gion vi, i = 1...n, as follows. First, we compute
an attention feature vector for each vi as

ai =
m∑
j=1

αijwj (1)

αij =
exp(λsim(vi,wj))∑m
k=1 exp(λsim(vi,wk))

(2)

where λ is a smoothing factor, and sim(v,w) is a
normalized similarity function defined as

sim(vi,wj) =
max(0, score(vi,wj))√∑n
k=1max(0, score(vk,wj))2

(3)

score(vi,wj) =
vi ·wj

‖ vi ‖‖ wj ‖
(4)

Then, we get the grounding vector of C and V as

s(V,C) = {s1, ..., sn} (5)

si := s(vi, C) =
vi · ai

‖ vi ‖‖ ai ‖
(6)

where the absolute value of si indicates how much
caption C is grounded to the i-th image region
or, in other words, to what degree C is generated
based on the i-th image region.

Given an image V , in order to evaluate the qual-
ity of a machine-generated caption C based on
a set of reference captions R = {R1, ..., Rk},
we generate two grounding vectors, s(V,C) as in
Equation 5 and s(V,R) which is a mean ground-
ing vector over all references inR as

s(V,R) = avg([s(V,R1), ..., s(V,Rk)]). (7)



2145

3.2 Grounding Vector Comparison

The quality of C is measured by comparing
s(V,C) and s(V,R) using two metric systems,
Region Rank Similarity (RRS) and Weight Distri-
bution Similarity (WDS).

3.2.1 Region Rank Similarity (RRS)
RRS is based on Discounted Cumulative Gain
(DCG) (Järvelin and Kekäläinen, 2002), which is
widely used to measure document ranking qual-
ity of web search engines. Using a graded rele-
vance scale of documents in a rank list returned by
a search engine, DCG measures the usefulness, or
gain, of a document based on its position in the
ranked list.

In the image captioning task, we view a cap-
tion as a query, an image as a collection of docu-
ments, one for each image region, and the ground-
ing scores based on reference captions as human-
labeled relevance scores. Note that s(V,C) con-
sists of a set of similarity scores {s1, s2, ..., sn}
for all image regions. If we view s(v, C) as a rel-
evance score between caption (query) C and im-
age region (document) v, we can sort these image
regions by their scores to form a ranked list, simi-
lar to common procedure in Information Retrieval
of documents from data collections via search en-
gines.

Then, the quality of the ranked list, or equiva-
lently the quality of C, can be measured via DCG,
which is the sum of the graded relevance values of
all image regions in V , discounted based on their
positions in the rank list derived from s(V,C):

DCGs(V,C) =
n∑

k=1

relk
log2(k + 1)

(8)

where relk is the human-labeled graded relevance
value of the image region at position k in the
ranked list.

Similarly, we can generate an ideal ranked list
from s(V,R) where all regions are ordered based
on their human-labeled graded relevance values.
We compute the Ideal DCG (IDCG) as

IDCGs(V,R) =
n∑

k=1

relk
log2(k + 1)

(9)

Finally, RRS between s(V,C) and s(V,R) is de-
fined as Normalized DCG (NDCG) as

RRS(V,C,R) := NDCG(V,C,R) =
DCGs(V,C)
IDCGs(V,R)

(10)

The assumption made in using RRS is that C
is generated based mainly on a few highly impor-
tant image regions, rather than the whole image.
A high-quality C should be generated based on a
similar set or the same set of highly important im-
age regions based on which human judges write
reference captions. One limitation of RRS is that
it is not suitable to measure the quality of captions
that are generated based on many equally impor-
tant image regions. In these cases, we assume that
humans produce captions by distributing their at-
tention more or less evenly across all image re-
gions rather than focusing on a small number of
highly important regions. The remedy to this lim-
itation is the new metric we describe next.

3.2.2 Weight Distribution Similarity (WDS)

WDS measures how similarly a system and hu-
man judges distribute their attention among differ-
ent regions of an image when generating captions.
Let P and Q be the attention distributions derived
from the grounding scores in s(V,R) and s(V,C),
respectively. We measure the distance between
the two distributions via KL Divergence (Kullback
and Leibler, 1951) as

DKL(P ||Q) =
n∑

k=1

P (k) log
P (k)

Q(k)
(11)

where P (k) = softmax(s(vk,R)) and Q(k) =
softmax(s(vk, C)).

In addition, we also find it useful to capture
the difference in caption-image relevance between
(V,C) and (V,R). The relevance value of a
caption-image pair can be approximated by using
the module of each grounding vector. We useDrel
to denote the value difference term, defined as

Drel((V,R)||(V,C)) = log
‖s(V,R)‖
‖s(V,C)‖

(12)

Finally, letting D(R||C) = DKL(P ||Q) +
Drel((V,R)||(V,C)), we get WDS between two
grounding vectors using a sigmoid function as

WDS(V,C,R) := 1−
exp (τD(R||C))

exp (τD(R||C)) + 1
(13)

where τ is the to-be-tuned temperature.



2146

3.2.3 TIGEr Score
The TIGEr score is defined as the average value4of
RRS and WDS:

TIGEr(V,C,R) :=
RRS(V,C,R)+WDS(V,C,R)

2 (14)

The score is a real value of [0, 1]. A higher TIGEr
score indicates a better caption as it matches better
with the human generated captions for the same
image.

4 Experiments

4.1 Dataset

Composite Dataset The multisource dataset
(Aditya et al., 2015a) we used contains testing
captions for 2007 MS-COCO images, 997 Flickr
8k pictures, and 991 Flickr 30k images. In total,
there are 11,985 candidates graded by annotators
on the description relevance from 1 (not relevant)
to 5 (very relevant). Each image has three candi-
date captions, where one is extracted from human-
written references, and the other two are generated
by recently proposed captioning models (Karpathy
and Fei-Fei, 2015; Aditya et al., 2015b).

Flickr 8K The Flickr 8K dataset was collected
by Hodosh et al. (2013), and contains 8092 im-
ages. Each picture is associated with five reference
captions written by humans. This dataset also in-
cludes 5822 testing captions for 1000 images. Un-
like the aforementioned two datasets, where test-
ing captions are directly generated based on an
image, candidates in Flickr 8K were selected by
an image retrieval system from a reference cap-
tion pool. For grading, native speakers were hired
to give a score from 1 (not related to the image
content) to 4 (very related). Each caption was
graded by three human evaluators, and the inter-
coder agreement was around 0.73. Because 158
candidates are actual references of target images,
we excluded these for further analysis.

Pascal-50S The PASCAL-50S dataset (Vedan-
tam et al., 2015) contains 1000 images from the
UIUC PASCAL Sentence dataset, of which 950
images are associated with 50 human-written cap-
tions per image as references, and the remainder
of the images has 120 references for each picture.
This dataset also contains 4000 candidate caption

4We selected arithmetic mean by empirically observing
the value variance between RRS and WDS in the [0, 1] inter-
val.

pairs with human evaluation, where each annota-
tor was asked to select one candidate per pair that
is closer to the given reference description. Can-
didate pairs are grouped by four types: 1) human-
human correct (HC) contains two human-written
captions for the target image, 2) human-human in-
correct (HI) includes two captions written by hu-
man but describing different images, 3) the group
of human-machine (HM) is contains a human-
written and a machine-generated caption, and 4)
machine-machine (MM) includes two matching-
generated captions focusing on the same image.

4.2 Compared Metrics
Given our emphasis on metric interpretability and
efficiency, we selected six rule-based metrics that
have been widely used to evaluate image captions
for comparison. The metrics are BLEU-1, BLEU-
4, ROUGE-L, METEOR, CIDEr, and SPICE. We
use MS COCO evaluation tool5 to implement all
metrics. Before testing, the input texts were lower-
cased and tokenized by using the ptbtokenizer.py
script from the same tool package.

4.3 Evaluation
Our examination of metric performances mainly
focuses on the caption-level correlation with hu-
man judgments. Following prior studies (Ander-
son et al., 2016; Elliott and Keller, 2014), we use
Kendall’s tau (τ ) and Spearman’s rho (ρ) rank cor-
relation to evaluate pairwise scores between met-
rics and human decisions in the Composite and
Flickr 8K datasets.

Composite Flickr8k

τ ρ τ ρ

BLEU-1 0.280 0.353 0.323 0.404
BLEU-4 0.205 0.352 0.138 0.387
ROUGE-L 0.307 0.383 0.323 0.404
METEOR 0.379 0.469 0.418 0.519
CIDEr 0.378 0.472 0.439 0.542
SPICE 0.419 0.514 0.449 0.596

Ours
RRS 0.388 0.479 0.418 0.521
WDS 0.433 0.526 0.464 0.572
TIGEr 0.454 0.553 0.493 0.606

Table 1: Caption-level correlation between metrics and
human grading scores in Composite and Flickr 8K
dataset by using Kendall tau and Spearman rho. All
p-values < 0.01.

Since human annotation of the Pascal-50S data
is a pairwise ranking instead of scoring, we kept

5https://github.com/tylin/coco-caption



2147

Figure 4: Average metric score based on human score
group.

consistency with the prior work (Vedantam et al.,
2015; Anderson et al., 2016) that evaluates metrics
by accuracy. Different from Anderson et al. (2016)
who considered equally-scored pairs as correct
cases, our definition of a correct case is that the
metric should assign a higher score to a candidate
that was preferred by human annotators.

4.4 Result on Composite & Flickr 8K

Metric Performance Table 1 displays the cor-
relation between metrics and human judgments in
terms of τ and ρ. Based on both correlation co-
efficients, we achieved a noticeable improvement
in the assessment of caption quality on Compos-
ite and Flick8K compared to the previous best re-
sults produced with existing metrics (Anderson
et al., 2016; Elliott and Keller, 2014). Regard-
ing the isolated impact of two similarity measures,
we observe that WDS contributes more to caption
evaluation than RRS. This finding indicates that
the micro-level comparison of grounding similar-
ity distribution is more sensitive to human judges
than the macro-level contrast of image region rank
by grounding scores.

TIGEr Score Analysis To understand the eval-
uation results of TIGEr, we further analyzed our
metric scores based on the group of human scoring
on caption quality. As shown in Figure 4, our met-
ric scores are increasing according to the growth
of human scores in both datasets. Overall, the
growth rate of RRS is similar with the WDS score
per dataset. Interestingly, the value of the metric
scores increase more slowly in high-scored cap-
tion groups than in low-scored groups (Flickr8k),
which suggests that the difference in caption rele-
vance between adjacent high-scored groups (e.g.,
3 vs. 4) is smaller than the descriptions in nearby
low-scored groups. A more detailed error analy-
sis and qualitative analysis is addressed in the Ap-
pendix.

HC HI HM MM All

BLEU-1 51.20 95.70 91.20 58.20 74.08
BLEU-4 53.00 92.40 86.70 59.40 72.88
ROUGE-L 51.50 94.50 92.50 57.70 74.05
METEOR 56.70 97.60 94.20 63.40 77.98
CIDEr 53.00 98.00 91.50 64.50 76.75
SPICE 52.60 93.90 83.60 48.10 69.55

TIGEr (ours) 56.00 99.80 92.80 74.20 80.70

Table 2: Accuracy of metrics at matching human judg-
ments on PASCAL-50S with 5 reference captions. The
highest accuracy per pair type is shown in bold font.
Column titles are explained in Section 4.1.

4.5 Result on Pascal-50S

Fixed Reference Number Table 2 reports the
evaluation accuracy of metrics on PASCAL-50S
with five reference captions per image. Our metric
achieves higher accuracy in most pair groups ex-
cept for HM and HC. Given all instances, TIGEr
improves the closeness to human judgment by
2.72% compared to the best prior metric (i.e.,
METEOR). Among the four considered candidate
groups, identifying irrelevant human-written cap-
tions in HI is relatively easy for all metrics, and
TIGEr achieves the highest accuracy (99.80%).
In contrast, judging the quality of two correct
human-annotated captions in HC is difficult with a
lower accuracy per metric compared to other test-
ing groups. For this pair group, TIGEr (56.00%)
shows a comparable performance with the best al-
ternative metric (METEOR: 56.70%). More im-
portantly, TIGEr reaches a noteworthy improve-
ment in judging machine-generated caption pairs
(MM) with an increasing in terms of accuracy by
about 10.00% compared to the best prior metric
(CIDEr: 64.50%).

Changing Reference Number In order to ex-
plore the impact of reference captions on metric
performance, we changed the number of refer-
ences from 1 to 50. As shown in Figure 5, TIGEr
outperforms prior metrics in All and MM candi-
date pairs by: 1) achieving higher accuracy, es-
pecially for small numbers of references; and 2)
obtaining more stable performance results across
varied reference sizes. Our findings suggest that
TIGEr has low reference dependency. Compared
with prior work (Vedantam et al., 2015), the slight
differences in results might be caused by the ran-
dom choices for reference subsets.



2148

Figure 5: Pairwise comparison accuracy (y-axis) of metrics at matching human judgments with 1-50 reference
captions (x-axis). TIGEr (solid line in light-purple) is the best performing metric in both all pair group and
machine-machine pair group. The number of reference captions is obviously beneficial to all existing metrics
except TIGEr, which is less dependent on excessive references and can therefore greatly reduce human annotation
costs.

Figure 6: Visualization of text-to-image grounding.

4.6 Text Component Sensitivity

We also explore the metrics’ capability to identify
specific text component errors (i.e., object, action,
and property). We randomly sampled a small set
of images from Composite and Pascal-50S. Given
an image, we pick a reference caption and gener-
ate two candidates by replacing a text component.
For example, we replaced the action “walk” from
a reference “People walk in a city square” with a
similar action “stroll” and a different action“are
running” as a candidate pair. We then calcu-
lated the accuracy of pairwise ranking per metric
for each component. As Figure 7 shows, TIGEr
is sensitive to recognizing object-level changes
while comparatively weak in detecting action dif-
ferences. This implies that text-to-image ground-
ing is more difficult at the action-level than the
object-level. Similarly, SPICE also has a lower ca-
pability to identify action inconsistencies, which
shows the limitation of scene graphs to capture
this feature. In contrast, the n-gram-based metrics
prefer to identify movement changes. To further
study the sensitivity of TIGEr to property-level
(i.e., object attributes) differences, we manually
grouped the compared pairs based on their asso-
ciated attribute category such as color. Our results
show that states (e.g., sad, happy), counting (e.g.,

Figure 7: Metric accuracy at three text component lev-
els.

one, two), and color (e.g., red, green) are top at-
tribute groups in the sampled data, where TIGEr
can better identify the differences of the counting
description (acc. 84.21%) compared to the expres-
sion of state (69.70%) and color (69.23%).

4.7 Grounding Analysis

To analyze the process of text-to-image ground-
ing behind TIGEr in depth, we visualize a set of
matching cases of image-caption pairs. Figure 6
provides two illustrative examples. Interestingly,
both captions per image are correct, but focus on
different image parts. For instance, in the left im-
age, caption 1 mentions “chairs”, which matches
region a, while caption 2 relates to region b by ad-



2149

dressing “garage”. According to our observation,
the image region typically has a higher ground-
ing weight with the corresponding caption than the
other unrelated region. Our observations suggest
that since captions are typically short, they may
not cover the information of all regions of an im-
age and hence taking image content into account
for captioning evaluation is necessary.

5 Conclusion and Future Work

We have presented a novel evaluation metric
called TIGEr for image captioning by utilizing
text-to-image grounding result based on a pre-
trained model. Unlike traditional metrics that are
solely based on text matching between reference
captions and machine-generated captions, TIGEr
also takes the matching between image contents
and captions into account , and the similarity be-
tween captions generated by human judges and
automated systems, respectively. The presented
experiments with three benchmark datasets have
shown that TIGEr outperforms existing popular
metrics, and has a higher and more stable corre-
lation with human judgments. In addition, TIGEr
is a fine-grained metric in that it identifies descrip-
tion errors at the object level.

Though TIGEr was build upon SCAN, our met-
ric is not tied to this grounding model. The
improvements with grounding models focusing
on the latent correspondence between object-level
image regions and descriptions will allow TIGEr
to be further improved in the future. Since the pre-
trained data mainly contains photo-based images,
TIGEr primarily focuses on the caption evaluation
in this image domain. For other domains, we can
retrain the SCAN model using the ground-truth
image-caption pairs, where the process is similar
to applying a learning-based metric. In our future
work, we plan to extend the metric to other visual-
text generation tasks such as storytelling.

Acknowledgments

We appreciate anonymous reviewers for their con-
structive comments and insightful suggestions.
This work was mostly done when Ming Jiang was
interning at Microsoft Research.

References
Somak Aditya, Yezhou Yang, Chitta Baral, Cor-

nelia Fermuller, and Yiannis Aloimonos. 2015a.

From images to sentences through scene description
graphs using commonsense reasoning and knowl-
edge. arXiv preprint arXiv:1511.03292.

Somak Aditya, Yezhou Yang, Chitta Baral, Cor-
nelia Fermuller, and Yiannis Aloimonos. 2015b.
From images to sentences through scene description
graphs using commonsense reasoning and knowl-
edge. arXiv preprint arXiv:1511.03292.

Peter Anderson, Basura Fernando, Mark Johnson, and
Stephen Gould. 2016. Spice: Semantic proposi-
tional image caption evaluation. In ECCV.

Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei
Zhang. 2018. Bottom-up and top-down attention for
image captioning and vqa. In CVPR.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the ACL workshop.

Raffaella Bernardi, Ruket Cakici, Desmond Elliott,
Aykut Erdem, Erkut Erdem, Nazli Ikizler-Cinbis,
Frank Keller, Adrian Muscat, and Barbara Plank.
2016. Automatic description generation from im-
ages: A survey of models, datasets, and evalua-
tion measures. Journal of Artificial Intelligence Re-
search.

Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi,
and Cho-Jui Hsieh. 2018. Attacking visual language
grounding with adversarial examples: A case study
on neural image captioning. In ACL.

Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A machine learning approach to
the automatic evaluation of machine translation. In
ACL.

Yin Cui, Guandao Yang, Andreas Veit, Xun Huang,
and Serge Belongie. 2018. Learning to evaluate im-
age captioning. In CVPR.

Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin.
2017. Towards diverse and natural image descrip-
tions via a conditional gan. In ICCV.

Desmond Elliott and Frank Keller. 2014. Comparing
automatic evaluation measures for image descrip-
tion. In ACL.

Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K
Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xi-
aodong He, Margaret Mitchell, John C Platt, et al.
2015. From captions to visual concepts and back.
In CVPR.



2150

Andrea Frome, Greg S Corrado, Jon Shlens, Samy
Bengio, Jeff Dean, Tomas Mikolov, et al. 2013. De-
vise: A deep visual-semantic embedding model. In
NeurIPS.

Akira Fukui, Dong Huk Park, Daylen Yang, Anna
Rohrbach, Trevor Darrell, and Marcus Rohrbach.
2016. Multimodal compact bilinear pooling for vi-
sual question answering and visual grounding. In
EMNLP.

Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu,
Kenneth Tran, Jianfeng Gao, Lawrence Carin, and
Li Deng. 2017. Semantic compositional networks
for visual captioning. In CVPR.

Jianfeng Gao, Michel Galley, and Lihong Li. 2019.
Neural approaches to conversational ai. Founda-
tions and Trends in Information Retrieval.

Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of Ar-
tificial Intelligence Research.

Qiuyuan Huang, Zhe Gan, Asli Celikyilmaz, Dapeng
Wu, Jianfeng Wang, and Xiaodong He. 2019. Hier-
archically structured reinforcement learning for top-
ically coherent visual story generation. In AAAI.

Allan Jabri, Armand Joulin, and Laurens van der
Maaten. 2016. Revisiting visual question answering
baselines. In ECCV.

Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumu-
lated gain-based evaluation of ir techniques. ACM
Transactions on Information Systems.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In CVPR.

Andrej Karpathy, Armand Joulin, and Li F Fei-Fei.
2014. Deep fragment embeddings for bidirectional
image sentence mapping. In NeurIPS.

Andrej Karpathy and Fei-Fei Li. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In CVPR.

Ryan Kiros, Ruslan Salakhutdinov, and Richard S.
Zemel. 2014. Unifying visual-semantic embeddings
with multimodal neural language models. arXiv
preprint arXiv:1411.2539.

Alex Kulesza and Stuart M Shieber. 2004. A learn-
ing approach to improving sentence-level mt evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation.

Solomon Kullback and Richard A Leibler. 1951. On
information and sufficiency. The annals of mathe-
matical statistics.

Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu,
and Xiaodong He. 2018. Stacked cross attention for
image-text matching. In ECCV.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In ECCV.

Ryan Lowe, Michael Noseworthy, Iulian Vlad Ser-
ban, Nicolas Angelard-Gontier, Yoshua Bengio, and
Joelle Pineau. 2017. Towards an automatic Turing
test: Learning to evaluate dialogue responses. In
ACL.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
scription evaluation. In CVPR.

Liwei Wang, Yin Li, Jing Huang, and Svetlana Lazeb-
nik. 2018a. Learning two-branch neural networks
for image-text matching tasks. PAMI.

Xin Wang, Wenhu Chen, Yuan-Fang Wang, and
William Yang Wang. 2018b. No metrics are perfect:
Adversarial reward learning for visual storytelling.
In ACL.



2151

A Appendices

A.1 Error Analysis
To have a better understanding of inconsistent
score ranks between TIGEr and human evalua-
tion assigned to candidates in the Composite and
Flick8k datasets, we further conducted an error
analysis. Considering the score of TIGEr is con-
tinuous [0, 1], while human scoring is done on
an n-point scale, we map our metric score onto
the n-point scale by sorting instances according to
TIGEr results, and assigning a score group per in-
stance based on the distribution of human scores.
Figure 8 shows the distribution of score group
differences between TIGEr and human evaluation
for both datasets. We observe that true positives
(i.e., the score group of TIGEr is equal to the hu-
man score for a given caption) achieve higher fre-
quency than each error group. On the other hand,
inconsistent cases with the small differences of as-
signed scores (e.g., TIGEr assigns 3, while human
assigns 4 to a testing caption) get a higher fre-
quency than the notable difference between two
metric scores, especially for the test cases from the
Flickr8k dataset. Our finding suggests that cap-
tions with subtle quality difference are more diffi-
cult to be discriminated than instances with signif-
icant gaps.

A.2 Qualitative Analysis
To illustrate some characteristics of TIGEr more
in depth, we show human and TIGEr scores for
a set of caption examples in Table 3. In order to
make the result of two metrics comparable, both
the score group and actual grading value of TIGEr
are displayed. Note that we provide an equal num-
ber of examples for each type of comparison result
(i.e., either human or TIGEr score is higher than
the other one & both scores are equal) in Table 3.

We find that TIGEr is able to measure a caption
quality by considering the semantic information
of image contents. For example, human-written
references in case (e) primarily provide an over-
all description of all people shown in the image,
while the candidate caption specifically describes
two walking women in the image. Despite such
difference at the text-level, TIGEr assigns a high
score to this sentence - just like human evaluators.
Case (b) also supports this finding to some ex-
tent. Unlike the references that specially described
a baseball payer in the game, the candidate caption
in this example provides a general description that

is matching with the image content (e.g., “base-
ball game”, “at night”). This observation may help
to explain why TIGEr gave this sentence a high
score.

Besides, we find two challenges for caption
evaluation based on TIGEr. First, though our met-
ric improved evaluation performance by consid-
ering semantic information from both images and
references, objects with closed semantic meaning
such as ”laptop” and ”computer” in example (a) is
limited to differentiate. Second, human interpreta-
tion inspired by the image is hard to be judged by
an automatic evaluation metric. For example, in
the case (d), “makes a wish as he blows the little
pinwheels into the air” is a reasonable imagination
of an human annotator, which cannot be explicitly
observed from the image.



2152

Figure 8: Distribution of group score differences between TIGEr and human evaluation

Table 3: Examples of scores given by TIGEr. In the column of TIGEr, we display both score group and the actual
grading value. The score group is assigned to be comparable with human scores, where we map our metric score
into the n-point scale like human evaluation. The detailed mapping process is shown in Section A.1.


