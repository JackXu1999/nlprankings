















































Efficient Large-Scale Neural Domain Classification with Personalized Attention


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2214–2224
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

2214

Efficient Large-Scale Neural Domain Classification
with Personalized Attention

Young-Bum Kim Dongchan Kim Anjishnu Kumar Ruhi Sarikaya
Amazon Alexa

{youngbum,dongchan,anjikum,rsarikaya}@amazon.com

Abstract

In this paper, we explore the task of map-
ping spoken language utterances to one of
thousands of natural language understand-
ing domains in intelligent personal digital
assistants (IPDAs). This scenario is ob-
served in mainstream IPDAs in industry
that allow third parties to develop thou-
sands of new domains to augment built-
in first party domains to rapidly increase
domain coverage and overall IPDA ca-
pabilities. We propose a scalable neu-
ral model architecture with a shared en-
coder, a novel attention mechanism that
incorporates personalization information
and domain-specific classifiers that solves
the problem efficiently. Our architecture
is designed to efficiently accommodate
incremental domain additions achieving
two orders of magnitude speed up com-
pared to full model retraining. We con-
sider the practical constraints of real-time
production systems, and design to min-
imize memory footprint and runtime la-
tency. We demonstrate that incorporating
personalization significantly improves do-
main classification accuracy in a setting
with thousands of overlapping domains.

1 Introduction

Intelligent personal digital assistants (IPDAs) are
one of the most advanced and successful artifi-
cial intelligence applications that have spoken lan-
guage understanding (SLU). Many IPDAs have
recently emerged in industry including Amazon
Alexa, Google Assistant, Apple Siri, and Mi-
crosoft Cortana (Sarikaya, 2017). IPDAs have tra-
ditionally supported only dozens of well-separated
domains, each defined in terms of a specific ap-

plication or functionality such as calendar and lo-
cal search (Tur and de Mori, 2011; Sarikaya et al.,
2016). To rapidly increase domain coverage and
extend capabilities, some IPDAs have released
Software Development Toolkits (SDKs) to allow
third-party developers to quickly build and inte-
grate new domains, which we refer to as skills
henceforth. Amazon’s Alexa Skills Kit (Kumar
et al., 2017a), Google’s Actions and Microsoft’s
Cortana Skills Kit are all examples of such SDKs.
Alexa Skills Kit is the largest of these services
with over 40,000 skills.

For IPDAs, finding the most relevant skill to
handle an utterance is an open problem for three
reasons. First, the sheer number of skills makes
the task difficult. Unlike traditional systems that
have on the order of 10-20 built-in domains, large-
scale IPDAs can have up to 40,000 skills. Sec-
ond, the number of skills is rapidly expanding
with 100+ new skills added per week. Large-
scale IPDAs should be able to accommodate new
skills efficiently without compromising perfor-
mance. Third, unlike traditional built-in domains
that are carefully designed to be disjoint by a cen-
tral team, skills are built independently by differ-
ent developers and can cover overlapping func-
tionalities. For instance, there are over 50 recipe
skills in Alexa that can handle recipe-related utter-
ances.

One simple solution to this problem has been to
require the user to explicitly mention a skill name
and follow a strict invocation pattern as in ”Ask
{Uber} to {get me a ride}.” However, this signif-
icantly limits the natural interaction with IPDAs.
Users have to remember skill names and invoca-
tion patterns, and it places a cognitive burden on
users who tend to forget both. Skill discovery is
difficult with a pure voice user interface, it is hard
for users to know the capabilities of thousands of
skills a priori, which may leads to limited user en-



2215

gagement with skills and potentially with IPDAs.
In this paper, we propose a solution that ad-

dresses all three practical challenges without re-
quiring skill names or invocation patterns. Our
approach is based on a scalable neural model ar-
chitecture with a shared encoder, a skill atten-
tion mechanism and skill-specific classification
networks that can efficiently perform large-scale
skill classification in IPDAs using a weakly su-
pervised training dataset. We demonstrate that
our model achieves a high accuracy on a manu-
ally transcribed test set after being trained with
weak supervision. Moreover, our architecture is
designed to efficiently integrate new skills that ap-
pear in-between full model retraining cycles into
the model. Besides accuracy, we also keep prac-
tical constraints in mind and focus on minimiz-
ing memory footprint and runtime latency, while
ensuring architecture is scalable to thousands of
skills, all of which are important for real-time pro-
duction systems. Furthermore, we investigate two
different ways of incorporating user personaliza-
tion information into the model, our naive base-
line method adds the information as a 1-bit flag in
the feature space of the skill-specific networks, the
personalized attention technique computes a con-
vex combination of skill embeddings for the user’s
enabled skills and significantly outperforms the
naive personalization baseline. We show the ef-
fectiveness of our approach with extensive exper-
iments using 1,500 skills from a deployed IPDA
system.

2 Related Work

Traditional multi-domain SLU/NLU systems are
designed hierarchically, starting with domain clas-
sification to classify an incoming utterance into
one of many possible domains, followed by fur-
ther semantic analysis with domain-specific intent
classification and slot tagging (Tur and de Mori,
2011). Traditional systems have typically been
limited to a small number of domains, designed
by specialists to be well-separable. Therefore,
domain classification has been considered a less
complex task compared to other semantic anal-
ysis such as intent and slot predictions. Tradi-
tional domain classifiers are built using simple lin-
ear models such as Multinomial Logistic Regres-
sion or Support Vector Machines in a one-versus-
all setting for multi-class prediction. The models
typically use word n-gram features and also those

based on static lexicon match, and there have been
several recent studies applying deep learning tech-
niques (Xu and Sarikaya, 2014).

There is also a line of prior work on enhanc-
ing sequential text classification or tagging. Hier-
archical character-to-word level LSTM (Hochre-
iter and Schmidhuber, 1997) architectures similar
to our models have been explored for the Named
Entity Recognition task by Lample et al. (2016).
Character-informed sequence models have also
been explored for simple text classification with
small sets of classes by Xiao and Cho (2016).
Joulin et al. (2016) explored highly scalable text
classification using a shared hierarchical encoder,
but their hierarchical softmax-based output formu-
lation is unsuitable for incremental model updates.
Work on zero-shot domain classifier expansion by
Kumar et al. (2017b) struggled to rank incom-
ing domains higher than training domains. The
attention-based approach of Kim et al. (2017d)
does not require retraining from scratch, but it
requires keeping all models stored in memory
which is computationally expensive. Multi-Task
learning was used in the context of SLU by Tur
(2006) and has been further explored using neu-
ral networks for phoneme recognition (Seltzer and
Droppo, 2013) and semantic parsing (Fan et al.,
2017; Bapna et al., 2017). There have been many
other pieces of prior work on improving NLU sys-
tems with pre-training (Kim et al., 2015b; Celiky-
ilmaz et al., 2016; Kim et al., 2017e), multi-task
learning (Zhang and Wang, 2016; Liu and Lane,
2016; Kim et al., 2017b), transfer learning (El-
Kahky et al., 2014; Kim et al., 2015a,c; Chen et al.,
2016a; Yang et al., 2017), domain adaptation (Kim
et al., 2016; Jaech et al., 2016; Liu and Lane, 2017;
Kim et al., 2017d,c) and contextual signals (Bhar-
gava et al., 2013; Chen et al., 2016b; Hori et al.,
2016; Kim et al., 2017a).

3 Weakly Supervised Training Data
Generation

Our model addresses the domain classification
task in SLU systems. In traditional IPDA systems,
these domains are hand-crafted by experts to be
well separable and can easily be annotated by hu-
mans because they are small in number. The emer-
gence of self-service SLU results in a large num-
ber of potentially mutually overlapping SLU do-
mains. This means that eliciting large volumes of
high quality human annotations to train our model



2216

Char-level
Bidirectional

LSTM

Word-level
Bidirectional

LSTM

Character
embedding

𝑤" 𝑤#

𝑐%," 𝑐%,# 𝑐%,'

𝑤%

…

𝑐#," 𝑐#,# 𝑐#,'𝑐"," 𝑐",# 𝑐",'

Word
embedding

…

…
𝜙)*
𝜙+*

… …

𝜙)*
𝜙+*

𝜙)*
𝜙+*

…
𝜙)*
𝜙+*

𝜙)*
𝜙+*

𝜙)*
𝜙+* …

𝜙)*
𝜙+*

𝜙)*
𝜙+*

𝜙)*
𝜙+*…

𝜙)*
𝜙+*

𝜙)*
𝜙+*

𝜙)*
𝜙+*

Utterance

…

…
𝑑" 𝑑# 𝑑-

Domain
Feedforward𝑑.

𝑑" 𝑑. 𝑑/

Enabled	Domains

Domain
embedding

Domain	1 Domain	2 Domain	3 Domain	K

…

Figure 1: The overall architecture of the personalized dynamic domain classifier.

is no longer feasible, and we cannot assume that
domains are designed to be well separable.

Instead we can generate training data by adopt-
ing the weak supervision paradigm introduced by
(Hoffmann et al., 2011), which proposes using
heuristic labeling functions generate large num-
bers of noisy data samples. Clean data generation
with weak supervision is a challenging problem,
so we address it by decomposing it into two sim-
pler problems, of candidate generation and noise
suppression, however it remains important for our
model to be noise robust.

3.1 Data Programming
The key insight of the Data Programming ap-
proach is that O(1) simple labeling functions can
be used to approximate O(n) human annotated
data points with much less effort. We adopt the
formalism used by (Ratner et al., 2016) to treat
each of instance data generation rule as a rich gen-
erative model, defined by a labeling function λ and
describe different families of labeling functions.
Our data programming pipeline is analogous to the
noisy channel model proposed for spelling correc-
tion by (Kernighan et al., 1990), and consists of
a set of candidate generation and noise detection
functions.

arg max
µ

P (µ|si) = arg max
µ

P (si|µ). P (µ)

where µ and si represent utterances and the ith
skill respectively. P (si|µ) the probability of a skill

being valid for an utterance is approximated by
simple functions that act as candidate data genera-
tors λg ∈ Λg based on recognitions produced by a
family of query patterns λq ∈ Λq. P (µ) is repre-
sented by a family of simple functions that act as
noise detectors λn ∈ Λn, which mark utterances
as likely being noise.

We apply the technique to the query logs of a
popular IPDA, which has support for personalized
third party domains. Looking at the structure of
utterances that match query pattern λq, each utter-
ance of form ”Ask {Uber} to {get me a car}” can
be considered as being parametrized by the under-
lying latent command µz , that is ”Get me a car”,
a target domain corresponding to service st, which
in this case is Uber and the query recognition pat-
tern λq, in this case ”Ask {st} to {µz}”. Next we
assume that the distribution of latent commands
over domains are independent of the query pattern.

P (µz, st) ≈ P (µ, st, λq)

Making this simple distributional approximation
allows us to generate a large number of noisy train-
ing samples. The family of generator functions
λg ∈ Λg is thus defined such that uz = λig(µ, λiq)

3.2 Noise Reduction
The distribution defined above contains a large
number of noisy positive samples. Related to
P (µ) in the noisy channel in the spell correction
context, we defined a small family of heuristic
noise detection functions λn ∈ Λn that discards



2217

training data instances that are not likely to be well
formed. For instance,

• λ1n requires u to contain a minimum thresh-
old of information by removing those with
µz that has token length fewer than 3. Utter-
ances shorter than this mostly consist of non-
actionable commands.

• λ2n discards all data samples below a certain
threshold of occurrences in live traffic, since
utterances that are rarely observed are more
likely to be ASR errors or unnatural.

• λ3n discards the data samples for a domain if
they come from an overly broad pattern with
a catch-all behavior.

• λ4n discards utterances that belong to shared
intents provided by the SLU SDK.

The end result of this stage is to retain utter-
ances such as ‘call me a cab’ from ‘Ask Uber to
call me a cab’ but discard ‘Boston’ from ‘Ask Ac-
cuweather for Boston’. One can easily imagine
extending this framework with other high recall
noise detectors, for example, using language mod-
els to discard candidates that are unlikely to be
spoken sentences.

4 Model Architecture

Our model consists of a shared encoder network
consisting of an orthography-sensitive hierarchical
LSTM encoder that feeds into a set of domain spe-
cific classification layers trained to make a binary
decision for each output label.

Our main novel contribution is the extension
of this architecture with a personalized attention
mechanism which uses the attention mechanism
of (Bahdanau et al., 2014) to attend to memory lo-
cations corresponding to the specific domains en-
abled by a user, and allows the system to learn
semantic representations of each domain via do-
main embeddings. As we will show, incorporat-
ing personalization features is key to disambiguat-
ing between multiple overlapping domains1, and
the personalized attention mechanism outperforms
more naive forms of personalization. The person-
alized attention mechanism first computes an at-
tention weight for each of enabled domains, per-
forms a convex combination to compute a context

1We assume that users can customize their IPDA settings
to enable certain domains.

vector and then concatenates this vector to the en-
coded utterance before the final domain classifica-
tion. Figure 1 depicts the model in detail.

Our model can efficiently accommodate new
domains not seen during initial training by keep-
ing the shared encoder frozen, bootstrapping a do-
main embedding based on existing ones, then opti-
mizing a small number of network parameters cor-
responding to domain-specific classifier, which is
orders of magnitude faster and more data efficient
than retraining the full classifier.

We make design decisions to ensure that our
model has a low memory and latency footprint.
We avoid expensive large vocabulary matrix mul-
tiplications on both the input and output stages,
and instead use a combination of character embed-
dings and word embeddings in the input stage.2

The output matrix is lightweight because each
domain-specific classifier is a matrix of only
201×2 parameters. The inference task can be triv-
ially parallelized across cores since there’s no re-
quirement to compute a partition function across
a high-dimensional softmax layer, which is the
slowest component of large label multiclass neu-
ral networks. Instead, we achieve comparability
between the probability scores generated by indi-
vidual models by using a customized loss formu-
lation.3

4.1 Shared Encoder

First we describe our shared hierarchical utterance
encoder, which is marked by the almond colored
box in Figure 1. Our hierarchical character to
word to utterance design is motivated by the need
to make the model operate on an open vocabulary
in terms of words and to make it robust to small
changes in orthography resulting from fluctuations
in the upstream ASR system, all while avoiding
expensive large matrix multiplications associated
with one-hot word encoding in large vocabulary
systems.

We denote an LSTM simply as a mapping φ :
Rd × Rd′ → Rd′ that takes a d dimensional in-
put vector x and a d′ dimensional state vector h
to output a new d′ dimensional state vector h′ =

2Using a one-hot representation of word vocabulary size
60,000 and hidden dimension 100 would require learning a
matrix of size 60000 x 100 - using 100-dim word embeddings
requires only a O(1) lookup followed by a 100 x 100 matrix,
thus allowing our model to be significantly smaller and faster
despite having what is effectively an open vocabulary

3Current inference consumes 50MB memory and the p99
latency is 15ms.



2218

φ(x, h). Let C denote the set of characters andW
the set of words in a given utterance. Let⊕ denote
the vector concatenation operation. We encode an
utterance using BiLSTMs, and the model parame-
ters Θ associated with this BiLSTM layer are

• Char embeddings ec ∈ R25 for each c ∈ C
• Char LSTMs φCf , φCb : R25 × R25 → R25

• Word embeddings ew ∈ R50 for each w ∈ W
• Word LSTMs φWf , φWb : R100 × R50 → R50

Let w1 . . . wn ∈ W denote a word sequence
where wordwi has characterwi(j) ∈ C at position
j. First, the model computes a character-sensitive
word representation vi ∈ R100 as

fCj = φ
C
f

(
ewi(j), f

C
j−1
)

∀j = 1 . . . |wi|
bCj = φ

C
b

(
ewi(j), b

C
j+1

)
∀j = |wi| . . . 1

vi = f
C
|wi| ⊕ b

C
1 ⊕ ewi

for each i = 1 . . . n.4 These word representa-
tion vectors are encoded by forward and backward
LSTMs for word φWf , φ

W
b as

fWi = φ
W
f

(
vi, f

W
i−1
)

∀i = 1 . . . n
bWi = φ

W
b

(
vi, b

W
i+1

)
∀i = n . . . 1

and induces a character and context-sensitive word
representation hi ∈ R100 as

hi = f
W
i ⊕ bWi

for each i = 1 . . . n. For convenience, we write
the entire operation as a mapping BiLSTMΘ:

(h1 . . . hn)← BiLSTMΘ(w1 . . . wn)

h̄ =
n∑
i=1

hi (1)

4.2 Domain Classification
Our Multitask domain classification formulation is
motivated by a desire to avoid computing the full
partition function during test time, which tends to
be the slowest component of a multiclass neural
network classifer, as has been documented before
by (Jozefowicz et al., 2016) and (Mikolov et al.,
2013), amongst others.

4For simplicity, we assume some random initial state vec-
tors such as fC0 and bC|wi|+1 when we describe LSTMs.

However, we also want access to reliable proba-
bility estimates instead of raw scores - we accom-
plish this by constructing a custom loss function.
During training, each domain classifier receives
in-domain (IND) and out-of-domain (OOD) utter-
ances, and we adapt the one-sided selection mech-
anism of (Kubat et al., 1997) to prevent OOD ut-
terances from overpowering IND utterances, thus
an utterance in a domain d ∈ D is considered as
an IND utterance in the viewpoint of domain d and
OOD for all other domains.

We first use the shared encoder to compute the
utterance representation h̄ as previously described.
Then we define the probability of domain d̃ for the
utterance by mapping h̄ to a 2-dimensional out-
put vector with a linear transformation for each
domain d̃ as

zd̃ = σ(W d̃ · h̄+ bd̃)

p(d̃|h̄) ∝

exp
(

[zd̃]IND

)
, if d̃ = d

exp
(

[zd̃]OOD

)
, otherwise

where σ is scaled exponential linear unit (SeLU)
for normalized activation outputs (Klambauer
et al., 2017) and [zd̃]IND and [zd̃]OOD denote the
values in the IND and OOD position of vector zd̃.

We define the joint domain classification loss
LD as the summation of positive (LP ) and neg-
ative (LN ) class loss functions 5:

LP
(

Θ,Θd̃
)

= − log p
(
d̃|h̄
)

LN
(

Θ,Θd̃
)

= − 1
k − 1

 ∑
d̄∈D,d̄ 6=d̃

log p
(
d̄|h̄
)

LD
(

Θ,Θd̃
)

= LP
(

Θ,Θd̃
)

+ LN
(

Θ,Θd̃
)

where k is the total number of domains. We di-
vide the second term by k − 1 so that LP and LN
are balanced in terms of the ratio of the training
examples for a domain to those for other domains.
While a softmax over the entire domains tends to
highlight only the ground-truth domain while sup-
pressing all the rest, the our joint domain classifi-
cation with a softmax over two classes is designed
to produce a more balanced confidence score per
domain independent of other domains.

5Θd̃ denotes the additional parameters in the classification
layer for domain d̃.



2219

4.3 Personalized Attention
We explore encoding a user’s domain preferences
in two ways. Our baseline method is a 1-bit
flag that is appended to the input features of each
domain-specific classifier. Our novel personalized
attention method induces domain embeddings by
supervising an attention mechanism to attend to
a user’s enabled domains with different weights
depending on their relevance. The domain em-
bedding matrix in Figure 1 represents the embed-
dings of a user’s enabled domains. We hypothe-
size that attention enables the network learn richer
representations of user preferences and domain
co-occurrence features.

Let eD(d̃) ∈ R100 and h̄ ∈ R100 denote the
domain embeddings for domain d̃ and the utter-
ance representation calculated by Eq. (1), respec-
tively. The domain attention weights for a given
user u who has a preferred domain list d(u) =(
d̃

(u)
1 , . . . , d̃

(u)
k

)
are calculated by the dot-product

operation,

ai = h̄ · eD
(
d̃

(u)
i

)
∀i = 1 . . . k

The final, normalized attention weights ā are ob-
tained after normalization via a softmax layer,

āi =
exp(ai)∑k
j=1 exp(aj)

∀i = 1 . . . k

The weighted combination of domain embeddings
is

S̄attended =
k∑
i=1

(
āi · eD

(
d̃

(u)
i

))
Finally the two representations of enabled do-
mains, namely the attention model and 1-bit flag
are then concatenated with the utterance represen-
tation and used to make per-domain predictions
via domain-specific affine transformations:

z̄att = h̄⊕ S̄attended

z̄1bit = h̄⊕ I(d̃ ∈ enabled)

Here I(d̄ ∈ enabled) is a 1-bit indicator for
whether the domain is enabled by the user or not.
z̄att and z̄1bit represent the encoded hidden state
of the Attention and 1-Bit Flag configura-
tions of the model from the experiment section. In
our experiments we will compare these two ways
of encoding personalization information, as well

as evaluate a variant that combines the two. In this
way we can ascertain whether the two personal-
ization signals are complementary via an ablation
study on the full model.

4.4 Domain Bootstrapping
Our model separates the responsibilities for utter-
ance representation and domain classification be-
tween the shared encoder and the domain-specific
classifiers. That is, the shared encoder needs to
be retrained only if it cannot encode an utter-
ance well (e.g., a new domain introduces com-
pletely new words) and the existing domain clas-
sifiers need to be retrained only when the shared
encoder changes. For adding new domains effi-
ciently without full retraining, the only two com-
ponents in the architecture need to be updated for
each new domain d̃new, are the domain embed-
dings for the new domain and its domain-specific
classifier.6 We keep the weights of the encoder
network frozen and use the hidden state vector h̄,
calculated by Eq. 1, as a feature vector to feed
into the downstream classifiers. To initialize the
m-dimensional domain embeddings ed̃new , we use
the column-wise average of all utterance vectors in
the training data h̄avg, and project it to the domain
embedding space using a matrix U ∈ Rm×m.
Thus,

ed̃new = U
∗ · h̄avg

The parameters of U∗ are learned using the
column-wise average utterance vectors h̄avgj and
learned domain vectors for all existing domains dj

U∗ = arg min
U

||U · h̄avgj − edj || ∀dj ∈ D

This is a write-to-memory operation that creates
a new domain representation after attending to all
existing domain representations. We then train the
parameters of the domain-specific classifier with
the new domain’s data while keeping the encoder
fixed. This mechanism allows us to efficiently
support new domains that appear in-between full
model deployment cycles without compromising
performance on existing domains. A full model
refresh would require us to fully retrain with the
domains that have appeared in the intermediate pe-
riod.

6We have assumed that the shared encoder covers most
of the vocabulary of new domains; otherwise, the entire net-
work may need to be retrained. Based on our observation of
live usage data, this assumption is reasonable since the shared
encoder after initial training is still shown to cover 95% of the
vocabulary of new domains added in the subsequent week.



2220

WEAK Mturk
Top-1 Top-3 Top-5 Top-1 Top-3 Top-5

Binary 78.29 87.90 88.92 73.79 85.35 86.45
MultiClass 78.58 87.12 88.11 73.78 84.54 85.55
MultiTask 80.46 89.27 90.16 75.66 86.48 87.66
1-Bit Flag 91.97 95.89 96.68 86.50 92.47 93.09
Attention* 94.83 97.11 98.35 89.64 95.39 96.70
1-Bit + Att 95.19 97.32 98.64 89.65 95.79 96.98

Table 1: The performance of different variants of our neural
model in terms of top-N accuracy. Binary trains a separate
binary classifier for each skill. MultiClass has a shared
encoder followed by a softmax. MultiTask replaces the
softmax with per-skill classifiers. 1-Bit Flag adds a sin-
gle bit for personalization to each skill classifier in MultiTask.
Attention extends MultiTask with personalized attention.
The last 3 models are personalized. *Best single encoding.

5 Experiments

In this section we aim to demonstrate the effec-
tiveness of our model architecture in two settings.
First, we will demonstrate that attention based per-
sonalization significantly outperforms the baseline
approach. Secondly, we will show that our model
new domain bootstrapping procedure results in ac-
curacies comparable to full retraining while re-
quiring less than 1% of the orignal training time.

5.1 Experimental Setup
Weak: This is a weakly supervised dataset was
generated by preprocessing utterances with strict
invocation patterns according to the setup men-
tioned in Section 3. The dataset consists of 5.34M
utterances from 637,975 users across 1,500 differ-
ent skills. Since we are interested in capturing the
temporal effects of the dataset as well as personal-
ization effects, we partitioned the data based both
on user and time. Our core training data for the ex-
periments in this paper was drawn from one month
of live usage, the validation data came from the
next 15 days of usage, and the test data came from
the subsequent 15 days. The training, validation
and test sets are user-independent, and each user
belongs to only one of the three sets to ensure no
leakage of information.

MTurk: Since the Weak dataset is generated by
weak supervision, we verified the performance of
our approach with human generated utterances. A
random sample of 12,428 utterances from the test
partition of users were presented to 300 human
judges, who were asked to produce two natural
ways to issue the same command. This dataset
is treated as a representative clean held out test set
on which we can observe the generalization of our
weakly supervised training and validation data to

natural language.

New Skills: In order to simulate the scenario
in which new skills appear within a week be-
tween model updates, we selected 250 new skills
which do not overlap with the skills in the Weak
dataset. The vocabulary size of 1,500 skills is
200K words, and on average, 5% of the vocabu-
lary for new skills is not covered. We randomly
sampled 4,000 unique utterances for each skill us-
ing the same weak supervision method, and split
them into 3,000 utterances for training and 1,000
for testing.

5.2 Results and Discussion

Generalization from Weakly Supervised to
Natural Utterances We first show the progres-
sion of model performance as we add more com-
ponents to show their individual contribution. Sec-
ondly, we show that training our models on a
weakly supervised dataset can generalize to nat-
ural speech by showing their test performance on
the human-annotated test data. Finally, we com-
pare two personalization strategies.

The full results are summarized in Table 1,
which shows the top-N test results separately for
the Weak dataset (weakly supervised) and MTurk
dataset (human-annotated). We report top-N ac-
curacy to show the potential for further re-ranking
or disambiguation downstream. For top-1 results
on the Weak dataset, using a separate binary clas-
sifier for each domain (Binary) shows a prediction
accuracy at 78.29% and using a softmax layer on
top of the shared encoder (MultiClass) shows a
comparable accuracy at 78.58%. The performance
shows a slight improvement when using the Mul-
titask neural loss structure, but adding personal-
ization signals to the Multitask structure showed
a significant boost in performance. We noted the
large difference between the 1-bit and attention ar-
chitecture. At 94.83% accuracy, attention resulted
in 35.6% relative error reduction over the 1-bit
baseline 91.97% on the Weak validation set and
23.25% relative on the MTurk test set. We hypoth-
esize that this may be because the attention mecha-
nism allows the model to focus on complementary
features in case of overlapping domains as well as
learning domain co-occurrence statistics, both of
which are not possible with the simple 1-bit flag.

When both personalization representations
were combined, the performance peaked at
95.19% for the Weak dataset and a more modest



2221

Time Accuracy
Binary 34.81 78.13
Expand 30.34 94.03
Refresh 5300.18 94.58

Table 2: Comparison of per-epoch training time (seconds)
and top-1 accuracy (%) on an NVIDIA Tesla M40 GPU.

89.65% for the MTurk dataset. The improvement
trend is extremely consistent across all top-N re-
sults for both of the Weak and MTurk datasets
across all experiments. The disambiguation task
is complex due to similar and overlapping skills,
but the results suggest that incorporating person-
alization signals equip the models with much bet-
ter discriminative power. The results also suggest
that the two mechanisms for encoding personal-
ization provide a small amount of complementary
information since combining them together is bet-
ter than using them individually. Although the per-
formance on the Weak dataset tends to be more
optimistic, the best performance on the human-
annotated test data is still close to 90% for top-1
accuracy, which suggests that training our model
with the samples derived from the invocation pat-
terns can generalize well to natural utterances.

Rapid Bootstrapping of New Skills We show
the results when new domains are added to an
IPDA and the model needs to efficiently accom-
modate them with a limited number of data sam-
ples. We show the classification performance
on the skills in the New Skills dataset while as-
suming we have access to the WEAK dataset to
pre-train our model for transfer learning. In the
Binary setting, a domain-specific binary classi-
fier is trained for each domain. Expand describes
the case in which we incrementally train on top
of an existing model. Refresh is the setting in
which the model is fully retrained from scratch
with the new data - which would be ideal in case
there were no time constraints.

We record the average training time for each
epoch and the performance is measured with top-1
classification accuracy over new skills. The exper-
iment results can be found in Table 2. Adapting a
new skill is two orders of magnitude faster (30.34
seconds) than retraining the model (5300.18 sec-
onds) while achieving 94.03% accuracy which is
comparable to 94.58% accuracy of full retraining.
The first two techniques can also be easily paral-
lelized unlike the Refresh configuration.

Top-1 Top-3 Top-5
Full 6.17 14.30 20.41

Enabled 85.62 96.15 98.06

Table 3: Top-N prediction accuracy (%) on the full skill set
(Full) and only enabled skills (Enabled).

Behavior of Attention Mechanism Our expec-
tation is that the model is able to learn to attend
the relevant skills during the inference process.
To study the behavior of the attention layer, we
compute the top-N prediction accuracy based on
the most relevant skills defined by the attention
weights. In this experiment, we considered the
subset of users who had enabled more than 20 do-
mains to exclude trivial cases7. The results are
shown in Table 3. When the model attends to
the entire set of 1500 (Full), the top-5 prediction
accuracy is 20.41%, which indicates that a large
number of skills can process the utterance, and
thus it is highly likely to miss the correct one in
the top-5 predictions. This ambiguity issue can
be significantly improved by users’ enabled do-
main lists as proved by the accuracies (Enabled):
85.62% for top-1, 96.15% for top-3, and 98.06%
for top-5.8 Thus the attention mechanism can thus
be viewed as an initial soft selection which is then
followed by a fine-grained selection at the classifi-
cation stage.

End-to-End User Evaluation All intermediate
metrics on this task are proxies to a human cus-
tomer’s eventual evaluation. In order to assess the
user experience, we need to measure its end-to-
end performance. For a brief end-to-end system
evaluation, 983 utterances from 283 domains were
randomly sampled from the test set in the large-
scale IPDA setting. 15 human judges (male=12,
female=3) rated the system responses, 1 judge per
utterance, on a 5-point Likert scale with 1 being
Terrible and 5 being Perfect. The judgment score
of 3 or above was taken as SUCCESS and 2 or be-
low as DEFECT. The end-to-end SUCCESS rate,

7Thus, the random prediction accuracy on enabled do-
mains is less than 5% and across the Full domain list is
0.066%

8Visual inspection of the embeddings confirms that mean-
ingful clusters are learned. We see clusters related to home
automation, commerce, cooking, trivia etc, we show some
examples in Figure 2, 3 and 4. However there are still other
clusters where the the relationships cannot be established as
easily. An example of these is show in Figure 5. The per-
sonalized attention mechanism is learned using the semantic
content as well as personalization signals, so we hypothesize
clusters like this may be capturing user tendencies to enable
these domains in a correlated manner.



2222

Figure 2: Embeddings of different domain categories visualized in 2D using TSNE (van der Maaten and Hinton, 2008).
Different colors represent different categories, for e.g. the large blue cluster on the left is Home Automation.

thus user satisfaction, was shown to be 95.52%.
The discrepancy between this score and the score
produced on MTurk dataset indicates that even in
cases in which the model makes classification mis-
takes, some of these interpretations remain percep-
tually meaningful to humans.

Figure 3: A large cluster of home automation domains.

Figure 4: A cluster of domains related to cooking.

Figure 5: A mixed cluster with several different domain
categories represented.

6 Conclusions

We have described a neural model architecture to
address large-scale skill classification in an IPDA
used by tens of millions of users every day. We
have described how personalization features and
an attention mechanism can be used for handling
ambiguity between domains. We have also shown
that the model can be extended efficiently and in-
crementally for new domains, saving multiple or-
ders of magnitude in terms of training time. The
model also addresses practical constraints of hav-
ing a low memory footprint, low latency and be-
ing easily parallelized, all of which are important
characteristics for real-time production systems.
In future work, we plan to incorporate various
types of context (e.g. anaphora, device-specific
capabilities) and dialogue history into a large-scale
NLU system.



2223

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Ankur Bapna, Gokhan Tur, Dilek Hakkani-Tur, and
Larry Heck. 2017. Towards zero shot frame seman-
tic parsing for domain scaling. In Interspeech 2017.

A. Bhargava, Asli Celikyilmaz, Dilek Z. Hakkani-
Tur, and Ruhi Sarikaya. 2013. Easy contex-
tual intent prediction and slot detection. IEEE
International Conference on Acoustics, Speech and
Signal Processing, pages 8337–8341.

Asli Celikyilmaz, Ruhi Sarikaya, Dilek Hakkani-Tür,
Xiaohu Liu, Nikhil Ramesh, and Gökhan Tür. 2016.
A new pre-training method for training deep learn-
ing models with application to spoken language un-
derstanding. In Interspeech, pages 3255–3259.

Yun-Nung Chen, Dilek Hakkani-Tür, and Xiaodong
He. 2016a. Zero-shot learning of intent embeddings
for expansion by convolutional deep structured se-
mantic models. In Acoustics, Speech and Signal
Processing (ICASSP), 2016 IEEE International
Conference on, pages 6045–6049.

Yun-Nung Chen, Dilek Hakkani-Tür, Gokhan Tur,
Jianfeng Gao, and Li Deng. 2016b. End-to-
end memory networks with knowledge carryover
for multi-turn spoken language understanding. In
Interspeech.

Ali El-Kahky, Xiaohu Liu, Ruhi Sarikaya, Gokhan Tur,
Dilek Hakkani-Tur, and Larry Heck. 2014. Ex-
tending domain coverage of language understand-
ing systems via intent transfer between domains us-
ing knowledge graphs and search query click logs.
In IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages
4067–4071. IEEE.

Xing Fan, Emilio Monti, Lambert Mathias, and Markus
Dreyer. 2017. Transfer learning for neural semantic
parsing. CoRR, abs/1706.04326.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extrac-
tion of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 541–550. Associa-
tion for Computational Linguistics.

Chiori Hori, Takaaki Hori, Shinji Watanabe, and
John R Hershey. 2016. Context-sensitive and role-
dependent spoken language understanding using
bidirectional and attention lstms. Interspeech, pages
3236–3240.

Aaron Jaech, Larry Heck, and Mari Ostendorf. 2016.
Domain adaptation of recurrent neural networks for
natural language understanding. In Interspeech.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efficient text
classification. arXiv preprint arXiv:1607.01759.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring
the limits of language modeling. arXiv preprint
arXiv:1602.02410.

Mark D Kernighan, Kenneth W Church, and William A
Gale. 1990. A spelling correction program based on
a noisy channel model. In Proceedings of the 13th
conference on Computational linguistics-Volume 2,
pages 205–210. Association for Computational Lin-
guistics.

Young-Bum Kim, Minwoo Jeong, Karl Stratos, and
Ruhi Sarikaya. 2015a. Weakly supervised slot tag-
ging with partially labeled sequences from web
search click logs. In Proceedings of the 2015
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 84–92.

Young-Bum Kim, Sungjin Lee, and Ruhi Sarikaya.
2017a. Speaker-sensitive dual memory networks
for multi-turn slot tagging. In Automatic Speech
Recognition and Understanding Workshop (ASRU),
2017 IEEE, pages 547–553. IEEE.

Young-Bum Kim, Sungjin Lee, and Karl Stratos.
2017b. Onenet: Joint domain, intent, slot prediction
for spoken language understanding. In Automatic
Speech Recognition and Understanding Workshop
(ASRU), 2017 IEEE, pages 547–553. IEEE.

Young-Bum Kim, Karl Stratos, and Dongchan Kim.
2017c. Adversarial adaptation of synthetic or stale
data. In Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics,
pages 1297–1307. Association for Computational
Linguistics.

Young-Bum Kim, Karl Stratos, and Dongchan Kim.
2017d. Domain attention with an ensemble of ex-
perts. In Annual Meeting of the Association for
Computational Linguistics.

Young-Bum Kim, Karl Stratos, and Ruhi Sarikaya.
2015b. Pre-training of hidden-unit crfs. In
Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing, volume 2, pages 192–198.

Young-Bum Kim, Karl Stratos, and Ruhi Sarikaya.
2016. Frustratingly easy neural domain adap-
tation. In Proceedings of COLING 2016, the
26th International Conference on Computational
Linguistics: Technical Papers, pages 387–396.



2224

Young-Bum Kim, Karl Stratos, and Ruhi Sarikaya.
2017e. A framework for pre-training hidden-unit
conditional random fields and its extension to long
short term memory networks. Computer Speech &
Language, 46:311–326.

Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, and
Minwoo Jeong. 2015c. New transfer learning tech-
niques for disparate label sets. In Proceedings of
the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing,
volume 1, pages 473–482.

Gunter Klambauer, Thomas Unterthiner, Andreas
Mayr, and Sepp Hochreiter. 2017. Self-normalizing
neural networks. CoRR, abs/1706.02515.

Miroslav Kubat, Stan Matwin, et al. 1997. Address-
ing the curse of imbalanced training sets: one-sided
selection. In ICML, volume 97, pages 179–186.
Nashville, USA.

Anjishnu Kumar, Arpit Gupta, Julian Chan, Sam
Tucker, Bjorn Hoffmeister, and Markus Dreyer.
2017a. Just ask: Building an architecture for ex-
tensible self-service spoken language understand-
ing. arXiv preprint arXiv:1711.00549.

Anjishnu Kumar, Pavankumar Reddy Muddireddy,
Markus Dreyer, and Björn Hoffmeister. 2017b.
Zero-shot learning across heterogeneous overlap-
ping domains. Proc. Interspeech 2017, pages 2914–
2918.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL-HLT, pages 260–270.

Bing Liu and Ian Lane. 2016. Attention-based recur-
rent neural network models for joint intent detection
and slot filling. In Interspeech, pages 685–689.

Bing Liu and Ian Lane. 2017. Multi-domain adversar-
ial learning for slot filling in spoken language un-
derstanding. In NIPS Workshop on Conversational
AI.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing high-dimensional data using t-sne.
Journal of Machine Learning Research, 9:2579–
2605.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Alexander J Ratner, Christopher M De Sa, Sen Wu,
Daniel Selsam, and Christopher Ré. 2016. Data
programming: Creating large training sets, quickly.
In Advances in Neural Information Processing
Systems, pages 3567–3575.

Ruhi Sarikaya. 2017. The technology behind per-
sonal digital assistants: An overview of the sys-
tem architecture and key components. IEEE Signal
Processing Magazine, 34(1):67–81.

Ruhi Sarikaya, Paul A Crook, Alex Marin, Minwoo
Jeong, Jean-Philippe Robichaud, Asli Celikyilmaz,
Young-Bum Kim, Alexandre Rochette, Omar Zia
Khan, Xiaohu Liu, et al. 2016. An overview of
end-to-end language understanding and dialog man-
agement for personal digital assistants. In Spoken
Language Technology Workshop (SLT), 2016 IEEE,
pages 391–397. IEEE.

Michael L Seltzer and Jasha Droppo. 2013. Multi-
task learning in deep neural networks for im-
proved phoneme recognition. In Acoustics, Speech
and Signal Processing (ICASSP), 2013 IEEE
International Conference on, pages 6965–6969.
IEEE.

Gokhan Tur. 2006. Multitask learning for spo-
ken language understanding. In Acoustics,
Speech and Signal Processing, 2006. ICASSP 2006
Proceedings. 2006 IEEE International Conference
on, volume 1, pages I–I. IEEE.

Gokhan Tur and Renato de Mori. 2011. Spoken
Language Understanding: Systems for Extracting
Semantic Information from Speech. New York, NY:
John Wiley and Sons.

Yijun Xiao and Kyunghyun Cho. 2016. Efficient
character-level document classification by combin-
ing convolution and recurrent layers. arXiv preprint
arXiv:1602.00367.

Puyang Xu and Ruhi Sarikaya. 2014. Contextual do-
main classification in spoken language understand-
ing systems using recurrent neural network. In
Acoustics, Speech and Signal Processing (ICASSP),
2014 IEEE International Conference on, pages 136–
140. IEEE.

Zhilin Yang, Ruslan Salakhutdinov, and William W
Cohen. 2017. Transfer learning for se-
quence tagging with hierarchical recurrent net-
works. International Conference on Learning
Representation (ICLR).

Xiaodong Zhang and Houfeng Wang. 2016. A joint
model of intent determination and slot filling for
spoken language understanding. In International
Joint Conference on Artificial Intelligence (IJCAI),
pages 2993–2999.


