



















































Improving Coreference Resolution by Learning Entity-Level Distributed Representations


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 643–653,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Improving Coreference Resolution by Learning Entity-Level Distributed
Representations

Kevin Clark
Computer Science Department

Stanford University
kevclark@cs.stanford.edu

Christopher D. Manning
Computer Science Department

Stanford University
manning@cs.stanford.edu

Abstract

A long-standing challenge in coreference
resolution has been the incorporation of
entity-level information – features defined
over clusters of mentions instead of men-
tion pairs. We present a neural net-
work based coreference system that pro-
duces high-dimensional vector represen-
tations for pairs of coreference clusters.
Using these representations, our system
learns when combining clusters is de-
sirable. We train the system with a
learning-to-search algorithm that teaches
it which local decisions (cluster merges)
will lead to a high-scoring final corefer-
ence partition. The system substantially
outperforms the current state-of-the-art on
the English and Chinese portions of the
CoNLL 2012 Shared Task dataset despite
using few hand-engineered features.

1 Introduction

Coreference resolution, the task of identifying
which mentions in a text refer to the same real-
world entity, is fundamentally a clustering prob-
lem. However, many recent state-of-the-art coref-
erence systems operate solely by linking pairs
of mentions together (Durrett and Klein, 2013;
Martschat and Strube, 2015; Wiseman et al.,
2015).

An alternative approach is to use agglomera-
tive clustering, treating each mention as a single-
ton cluster at the outset and then repeatedly merg-
ing clusters of mentions deemed to be referring
to the same entity. Such systems can take advan-
tage of entity-level information, i.e., features be-
tween clusters of mentions instead of between just
two mentions. As an example for why this is use-
ful, it is clear that the clusters {Bill Clinton} and

{Clinton, she} are not referring to the same entity,
but it is ambiguous whether the pair of mentions
Bill Clinton and Clinton are coreferent.

Previous work has incorporated entity-level in-
formation through features that capture hard con-
straints like having gender or number agreement
between clusters (Raghunathan et al., 2010; Dur-
rett et al., 2013). In this work, we instead train a
deep neural network to build distributed represen-
tations of pairs of coreference clusters. This cap-
tures entity-level information with a large number
of learned, continuous features instead of a small
number of hand-crafted categorical ones.

Using the cluster-pair representations, our net-
work learns when combining two coreference
clusters is desirable. At test time it builds up coref-
erence clusters incrementally, starting with each
mention in its own cluster and then merging a pair
of clusters each step. It makes these decisions with
a novel easy-first cluster-ranking procedure that
combines the strengths of cluster-ranking (Rah-
man and Ng, 2011) and easy-first (Stoyanov and
Eisner, 2012) coreference algorithms.

Training incremental coreference systems is
challenging because the coreference decisions fac-
ing a model depend on previous decisions it
has already made. We address this by using a
learning-to-search algorithm inspired by SEARN
(Daumé III et al., 2009) to train our neural net-
work. This approach allows the model to learn
which action (a cluster merge) available from the
current state (a partially completed coreference
clustering) will eventually lead to a high-scoring
coreference partition.

Our system uses little manual feature engineer-
ing, which means it is easily extended to multiple
languages. We evaluate our system on the English
and Chinese portions of the CoNLL 2012 Shared
Task dataset. The cluster-ranking model signifi-
cantly outperforms a mention-ranking model that

643



does not use entity-level information. We also
show that using an easy-first strategy improves the
performance of the cluster-ranking model. Our fi-
nal system achieves CoNLL F1 scores of 65.29 for
English and 63.66 for Chinese, substantially out-
performing other state-of-the-art systems.1

2 System Architecture

Our cluster-ranking model is a single neural net-
work that learns which coreference cluster merges
are desirable. However, it is helpful to think of
the network as being composed of distinct sub-
networks. The mention-pair encoder produces
distributed representations for pairs of mentions
by passing relevant features through a feedforward
neural network. The cluster-pair encoder pro-
duces distributed representations for pairs of clus-
ters by applying a pooling operation over the rep-
resentations of relevant mention pairs, i.e., pairs
where one mention is in each cluster. The cluster-
ranking model then scores pairs of clusters by
passing their representations through a single neu-
ral network layer.

We also train a mention-ranking model that
scores pairs of mentions by passing their repre-
sentations through a single neural network layer.
Its parameters are used to initialize the cluster-
ranking model, and the scores it produces are
used to prune which candidate cluster merges
the cluster-ranking model considers, allowing the
cluster-ranking model to run much faster. The sys-
tem architecture is summarized in Figure 1.

	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	

Mention-Pair 
Encoder 

Cluster-Pair 
Encoder 

Cluster-Ranking 
Model 

Mention-Ranking 
Model 

Pretraining, 

Search space 
pruning 

Figure 1: System architecture. Solid arrows indi-
cate one neural network is used as a component of
the other; the dashed arrow indicates other depen-
dencies.

3 Building Representations

In this section, we describe the neural networks
producing distributed representations of pairs of

1Code and trained models are available at https://
github.com/clarkkev/deep-coref.

	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	 	

Candidate 
Antecedent  
Embeddings 

Candidate 
Antecedent  
Features 
 

Mention 
Features 
 

Mention 
Embeddings 

Hidden Layer h2 

Mention-Pair Representation rm 
	

Input Layer h0 

Hidden Layer h1 

ReLU(W1h0 + b1) 

ReLU(W2h1 + b2) 
 

ReLU(W3h2 + b3) 
 

Pair and 
Document 
Features 
 

Figure 2: Mention-pair encoder.

mentions and pairs of coreference clusters. We as-
sume that a set of mentions has already been ex-
tracted from each document using a method such
as the one in Raghunathan et al. (2010).

3.1 Mention-Pair Encoder

Given a mention m and candidate antecedent a,
the mention-pair encoder produces a distributed
representation of the pair rm(a,m) ∈ Rd with a
feedforward neural network, which is shown in
Figure 2. The candidate antecedent may be any
mention that occurs before m in the document
or NA, indicating that m has no antecedent. We
also experimented with models based on Long
Short-Term Memory recurrent neural networks
(Hochreiter and Schmidhuber, 1997), but found
these to perform slightly worse when used in
an end-to-end coreference system due to heavy
overfitting to the training data.

Input Layer. For each mention, the model ex-
tracts various words and groups of words that
are fed into the neural network. Each word
is represented by a vector wi ∈ Rdw . Each
group of words is represented by the average
of the vectors of each word in the group. For
each mention and pair of mentions, a small
number of binary features and distance fea-
tures are also extracted. Distances and men-
tion lengths are binned into one of the buck-
ets [0, 1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+] and
then encoded in a one-hot vector in addition to be-
ing included as continuous features. The full set
of features is as follows:

Embedding Features: Word embeddings of the
head word, dependency parent, first word, last
word, two preceding words, and two following
words of the mention. Averaged word embed-
dings of the five preceding words, five following

644



words, all words in the mention, all words in the
mention’s sentence, and all words in the mention’s
document.

Additional Mention Features: The type of the
mention (pronoun, nominal, proper, or list), the
mention’s position (index of the mention divided
by the number of mentions in the document),
whether the mentions is contained in another men-
tion, and the length of the mention in words.

Document Genre: The genre of the mention’s doc-
ument (broadcast news, newswire, web data, etc.).

Distance Features: The distance between the men-
tions in sentences, the distance between the men-
tions in intervening mentions, and whether the
mentions overlap.

Speaker Features: Whether the mentions have the
same speaker and whether one mention is the other
mention’s speaker as determined by string match-
ing rules from Raghunathan et al. (2010).

String Matching Features: Head match, exact
string match, and partial string match.

The vectors for all of these features are concate-
nated to produce an I-dimensional vector h0, the
input to the neural network. If a = NA, the fea-
tures defined over mention pairs are not included.
For this case, we train a separate network with an
identical architecture to the pair network except
for the input layer to produce anaphoricity scores.

Our set of hand-engineered features is much
smaller than the dozens of complex features typ-
ically used in coreference systems. However, we
found these features were crucial for getting good
model performance. See Section 6.1 for a feature
ablation study.

Hidden Layers. The input gets passed through
three hidden layers of rectified linear (ReLU) units
(Nair and Hinton, 2010). Each unit in a hidden
layer is fully connected to the previous layer:

hi(a,m) = max(0,Wihi−1(a,m) + bi)

where W1 is a M1 × I weight matrix, W2 is a
M2 ×M1 matrix, and W3 is a d×M2 matrix.

The output of the last hidden layer is the
vector representation for the mention pair:
rm(a,m) = h3(a,m).

	
	
	
 
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	

Cluster-Pair 
Representation 

Mention-Pair 
Representations 

Pooling 

!!! 
 

c2  
 

c1  

Mention-Pair 
Encoder 

!!! 
 

!!! 
 

rc(c1, c2) 

Rm(c1, c2) 

!!! 
 

Figure 3: Cluster-pair encoder.

3.2 Cluster-Pair Encoder

Given two clusters of mentions ci =
{mi1,mi2, ...,mi|ci|} and cj = {m

j
1,m

j
2, ...,m

j
|cj |},

the cluster-pair encoder produces a distributed
representation rc(ci, cj) ∈ R2d. The architecture
of the encoder is summarized in Figure 3.

The cluster-pair encoder first combines
the information contained in the matrix of
mention-pair representations Rm(ci, cj) =
[rm(mi1,m

j
1), rm(m

i
1,m

j
2), ..., rm(m

i
|ci|,m

j
|cj |)]

to produce rc(ci, cj). This is done by applying a
pooling operation. In particular it concatenates
the results of max-pooling and average-pooling,
which we found to be slightly more effective than
using either one alone:

rc(ci, cj)k =

{
max {Rm(ci, cj)k,·} for 0 ≤ k < d
avg {Rm(ci, cj)k−d,·} for d ≤ k < 2d

4 Mention-Ranking Model

Rather than training a cluster-ranking model from
scratch, we first train a mention-ranking model
that assigns each mention its highest scoring can-
didate antecedent. There are two key advantages
of doing this. First, it serves as pretraining for the
cluster-ranking model; in particular the mention-
ranking model learns effective weights for the
mention-pair encoder. Second, the scores pro-
duced by the mention-ranking model are used to
provide a measure of which coreference decisions
are easy (allowing for an easy-first clustering strat-
egy) and which decisions are clearly wrong (these
decisions can be pruned away, significantly reduc-
ing the search space of the cluster-ranking model).

The mention-ranking model assigns a score
sm(a,m) to a mention m and candidate an-

645



tecedent a representing their compatibility for
coreference. This is produced by applying a sin-
gle fully connected layer of size one to the repre-
sentation rm(a,m) produced by the mention-pair
encoder:

sm(a,m) = Wmrm(a,m) + bm

where Wm is a 1× d weight matrix. At test time,
the mention-ranking model links each mention
with its highest scoring candidate antecedent.

Training Objective. We train the mention-
ranking model with the slack-rescaled max-
margin training objective from Wiseman et al.
(2015), which encourages separation between the
highest scoring true and false antecedents of the
current mention. Suppose the training set consists
of N mentions m1,m2, ...,mN . Let A(mi) de-
note the set of candidate antecedents of a men-
tion mi (i.e., mentions preceding mi and NA), and
T (mi) denote the set of true antecedents of mi
(i.e., mentions preceding mi that are coreferent
with it or {NA} if mi has no antecedent). Let t̂i
be the highest scoring true antecedent of mention
mi:

t̂i = argmax
t∈T (mi)

sm(t,mi)

Then the loss is given by

N∑
i=1

max
a∈A(mi)

∆(a,mi)(1 + sm(a,mi)− sm(t̂i,mi))

where ∆(a,mi) is the mistake-specific cost func-
tion

∆(a,mi) =


αFN if a = NA ∧ T (mi) 6= {NA}
αFA if a 6= NA ∧ T (mi) = {NA}
αWL if a 6= NA ∧ a /∈ T (mi)
0 if a ∈ T (mi)

for “false new,” “false anaphoric,” “wrong link,”
and correct coreference decisions. The different
error penalties allow the system to be tuned
for coreference evaluation metrics by biasing it
towards making more or fewer coreference links.

Finding Effective Error Penalties. We fix
αWL = 1.0 and search for αFA and αFN out of
{0.1, 0.2, ..., 1.5} with a variant of grid search.
Each new trial uses the unexplored set of hy-
perparameters that has the closest Manhattan

distance to the best setting found so far on
the dev set. We stopped the search when all
immediate neighbors (within 0.1 distance) of
the best setting had been explored. We found
(αFN, αFA, αWL) = (0.8, 0.4, 1.0) to be best for
English and (αFN, αFA, αWL) = (0.7, 0.4, 1.0) to
be best for Chinese on the CoNLL 2012 data.
We attribute our smaller false new cost from the
one used by Wiseman et al. (they set αFN = 1.2)
to using more precise mention detection, which
results in fewer links to NA.

Training Details. We initialized our word em-
beddings with 50 dimensional ones produced by
word2vec (Mikolov et al., 2013) on the Giga-
word corpus for English and 64 dimensional ones
provided by Polyglot (Al-Rfou et al., 2013) for
Chinese. Averaged word embeddings were held
fixed during training while the embeddings used
for single words were updated. We set our hid-
den layer sizes to M1 = 1000,M2 = d = 500
and minimized the training objective using RMS-
Prop (Hinton and Tieleman, 2012). To regularize
the network, we applied L2 regularization to the
model weights and dropout (Hinton et al., 2012)
with a rate of 0.5 on the word embeddings and the
output of each hidden layer.

Pretraining. As in Wiseman et al. (2015), we
found that pretraining is crucial for the mention-
ranking model’s success. We pretrained the
network in two stages, minimizing the following
objectives from Clark and Manning (2015):

All-Pairs Classification

−
N∑
i=1

[
∑

t∈T (mi)
log p(t,mi) +

∑
f∈F(mi)

log(1− p(f,mi))]

Top-Pairs Classification

−
N∑
i=1

[ max
t∈T (mi)

log p(t,mi) + min
f∈F(mi)

log(1− p(f,mi))]

WhereF(mi) is the set of false antecedents formi
and p(a,mi) = sigmoid(s(a,mi)). The top pairs
objective is a middle ground between the all-pairs
classification and mention ranking objectives: it
only processes high-scoring mentions, but is prob-
abilistic rather than max-margin. We first pre-
trained the network with all-pairs classification for
150 epochs and then with top-pairs classification
for 50 epochs. See Section 6.1 for experiments on

646



the two-stage pretraining.

5 Cluster-Ranking Model

Although a strong coreference system on its own,
the mention-ranking model has the disadvantage
of only considering local information between
pairs of mentions, so it cannot consolidate infor-
mation at the entity-level. We address this prob-
lem by training a cluster-ranking model that scores
pairs of clusters instead of pairs of mentions.

Given two clusters of mentions ci and cj , the
cluster-ranking model produces a score sc(ci, cj)
representing their compatibility for coreference.
This is produced by applying a single fully con-
nected layer of size one to the representation
rc(ci, cj) produced by the cluster-pair encoder:

sc(ci, cj) = Wcrc(ci, cj) + bc

where Wc is a 1 × 2d weight matrix. Our
cluster-ranking approach also uses a measure of
anaphoricity, or how likely it is for a mention m to
have an antecedent. This is defined as

sNA(m) = WNArm(NA,m) + bNA

where WNA is a 1× d matrix.
5.1 Cluster-Ranking Policy Network
At test time, the cluster ranker iterates through ev-
ery mention in the document, merging the current
mention’s cluster with a preceding one or perform-
ing no action. We view this procedure as a sequen-
tial decision process where at each step the algo-
rithm observes the current state x and performs
some action u.

Specifically, we define a state x = (C,m) to
consist of C = {c1, c2, ...}, the set of existing
coreference clusters, and m, the current mention
being considered. At a start state, each cluster in
C contains a single mention. Let cm ∈ C be the
cluster containing m and A(m) be a set of candi-
date antecedents for m: mentions occurring previ-
ously in the document. Then the available actions
U(x) from x are

• MERGE[cm, c], where c is a cluster contain-
ing a mention in A(m). This combines cm
and c into a single coreference cluster.

• PASS. This leaves the clustering unchanged.
After determining the new clustering C ′ based on
the existing clustering C and action u, we con-

sider another mention m′ to get the next state
x′ = (C ′,m′).

Using the scoring functions sc and sNA, we de-
fine a policy network π that assigns a probability
distribution over U(x) as follows:

π(MERGE[cm, c]|x) ∝ esc(cm,c)
π(PASS|x) ∝ esNA(m)

During inference, π is executed by taking the
highest-scoring (most probable) action at each
step.

5.2 Easy-First Cluster Ranking

The last detail needed is the ordering in which
to consider mentions. Cluster-ranking models in
prior work order the mentions according to their
positions in the document, processing them left-
to-right (Rahman and Ng, 2011; Ma et al., 2014).
However, we instead sort the mentions in de-
scending order by their highest scoring candidate
coreference link according to the mention-ranking
model. This causes inference to occur in an easy-
first fashion where hard decisions are delayed until
more information is available. Easy-first orderings
have been shown to improve the performance of
other incremental coreference strategies (Raghu-
nathan et al., 2010; Stoyanov and Eisner, 2012)
because they reduce the problem of errors com-
pounding as the algorithm runs.

We also find it beneficial to prune the set of
candidate antecedents A(m) for each mention m.
Rather than using all previously occurring men-
tions as candidate antecedents, we only include
high-scoring ones, which greatly reduces the size
of the search space. This allows for much faster
learning and inference; we are able to remove over
95% of candidate actions with no decrease in the
model’s performance. For both of these two pre-
processing steps, we use s(a,m) − s(NA,m) as
the score of a coreference link between a and m.

5.3 Deep Learning to Search

We face a sequential prediction problem where fu-
ture observations (visited states) depend on previ-
ous actions. This is challenging because it violates
the common i.i.d. assumption made in machine
learning. Learning-to-search algorithms are effec-
tive for this sort of problem, and have been applied
successfully to coreference resolution (Daumé III
and Marcu, 2005; Clark and Manning, 2015) as

647



Algorithm 1 Deep Learning to Search
for i = 1 to num epochs do

Initialize the current training set Γ = ∅
for each example (x, y) ∈ D do

Run the policy π to completion from start state x to obtain a trajectory of states {x1, x2, ..., xn}
for each state xi in the trajectory do

for each possible action u ∈ U(xi) do
Execute u on xi and then run the reference policy πref until reaching an end state e
Assign u a cost by computing the loss on the end state: l(u) = L(e, y)

end for
Add the state xi and associated costs l to Γ

end for
end for
Update π with gradient descent, minimizing

∑
(x,l)∈Γ

∑
u∈U(x) π(u|x)l(u).

end for

well as other structured prediction tasks in natu-
ral language processing (Daumé III et al., 2014;
Chang et al., 2015a).

We train the cluster-ranking model using a
learning-to-search algorithm inspired by SEARN
(Daumé III et al., 2009), which is described in Al-
gorithm 1. The algorithm takes as input a dataset
D of start states x (in our case documents with
each mention in its own singleton coreference
cluster) and structured labels y (in our case gold
coreference clusters). Its goal is to train the pol-
icy π so when it executes from x, reaching a fi-
nal state e, the resulting loss L(e, y) is small. We
use the negative of the B3 coreference metric for
this loss (Bagga and Baldwin, 1998). Although
our system evaluation also includes the MUC (Vi-
lain et al., 1995) and CEAFφ4 (Luo, 2005) metrics,
we do not incorporate them into the loss because
MUC has the flaw of treating all errors equally and
CEAFφ4 is slow to compute.

For each example (x, y) ∈ D, the algorithm ob-
tains a trajectory of states x1, x2, ..., xn visited by
the current policy by running it to completion (i.e.,
repeatedly taking the highest scoring action until
reaching an end state) from the start state x. This
exposes the model to states at train time similar to
the ones it will face at test time, allowing it to learn
how to cope with mistakes.

Given a state x in a trajectory, the algorithm
then assigns a cost l(u) to each action u ∈ U(x)
by executing the action, “rolling out” from the
resulting state with a reference policy πref until
reaching an end state e, and computing the result-
ing loss L(e, y). This rolling out procedure allows
the model to learn how a local action will affect the

final score, which cannot be otherwise computed
because coreference evaluation metrics do not de-
compose over cluster merges. The policy network
is then trained to minimize the risk associated with
taking each action:

∑
u∈U(x) π(u|x)l(u).

Reference policies typically refer to the gold la-
bels to find actions that are likely to be beneficial.
Our reference policy πref takes the action that in-
creases the B3 score the most each step, breaking
ties randomly. It is generally recommended to
use a stochastic mixture of the reference policy
and the current learned policy during rollouts
when the reference policy is not optimal (Chang
et al., 2015b). However, we find only using the
reference policy (which is close to optimal) to be
much more efficient because it does not require
neural network computations and is deterministic,
which means the costs of actions can be cached.

Training details. We update π using RMSProp
and apply dropout with a rate of 0.5 to the in-
put layer. For most experiments, we initialize the
mention-pair encoder component of the cluster-
ranking model with the learned weights from the
mention-ranking model, which we find to greatly
improve performance (see Section 6.2).

Runtime. The full cluster-ranking system runs
end-to-end in slightly under 1 second per docu-
ment on the English test set when using a GPU
(including scoring all pairs of mentions with the
mention-ranking model for search-space pruning).
This means the bottleneck for the overall system is
the syntactic parsing required for mention detec-
tion (around 4 seconds per document).

648



Model English F1 Chinese F1

Full Model 65.52 64.41
– MENTION –1.27 –0.74
– GENRE –0.25 –2.91
– DISTANCE –2.42 –2.41
– SPEAKER –1.26 –0.93
– MATCHING –2.07 –3.44

Table 1: CoNLL F1 scores of the mention-ranking
model on the dev sets without mention, docu-
ment genre, distance, speaker, and string matching
hand-engineered features.

6 Experiments and Results

Experimental Setup. We run experiments on
the English and Chinese portions of the CoNLL
2012 Shared Task data (Pradhan et al., 2012).
The models are evaluated using three of the most
popular coreference metrics: MUC, B3, and
Entity-based CEAF (CEAFφ4). We generally
report the average F1 score (CoNLL F1) of the
three, which is common practice in coreference
evaluation. We used the most recent version of the
CoNLL scorer (version 8.01), which implements
the original definitions of the metrics.

Mention Detection. Our experiments were run
using system-produced predicted mentions. We
used the rule-based mention detection algorithm
from Raghunathan et al. (2010), which first
extracts pronouns and maximal NP projections
as candidate mentions and then filters this set
with rules that remove spurious mentions such as
numeric entities and pleonastic it pronouns.

6.1 Mention-Ranking Model Experiments

Feature Ablations. We performed a feature ab-
lation study to determine the importance of the
hand-engineered features included in our model.
The results are shown in Table 1. We find the
small number of non-embedding features substan-
tially improves model performance, especially the
distance and string matching features. This is un-
surprising, as the additional features are not eas-
ily captured by word embeddings and historically
such features have been very important in corefer-
ence resolvers (Bengtson and Roth, 2008).

The Importance of Pretraining. We evaluate
the benefit of the two-step pretraining for the

All-Pairs Top-Pairs English F1 Chinese F1

Yes Yes 65.52 64.41
Yes No –0.36 –0.24
No Yes –0.54 –0.33
No No –3.58 –5.43

Table 2: CoNLL F1 scores of the mention-ranking
model on the dev sets with different pretraining
methods.

Model English F1 Chinese F1

Full Model 66.01 64.86
– PRETRAINING –5.01 –6.85
– EASY-FIRST –0.15 –0.12
– L2S –0.32 –0.25

Table 3: CoNLL F1 scores of the cluster-ranking
model on the dev sets with various ablations.
– PRETRAINING: initializing model parameters
randomly instead of from the mention-ranking
model, – EASY-FIRST: iterating through mentions
in order of occurrence instead of according to their
highest scoring candidate coreference link, – L2S:
training on a fixed trajectory of correct actions in-
stead of using learning to search.

mention-ranking model and report results in Ta-
ble 2. Consistent with Wiseman et al. (2015), we
find pretraining to greatly improve the model’s ac-
curacy. We note in particular that the model ben-
efits from using both pretraining steps from Sec-
tion 4, which more smoothly transitions the model
from a mention-pair classification objective that is
easy to optimize to a max-margin objective better
suited for a ranking task.

6.2 Cluster-Ranking Model Experiments

We evaluate the importance of three key details of
the cluster ranker: initializing it with the mention-
ranking model’s weights, using an easy-first order-
ing of mentions, and using learning to search. The
results are shown in Table 3.

Pretrained Weights. We compare initializing
the cluster-ranking model randomly with initial-
izing it with the weights learned by the mention-
ranking model. Using pretrained weights greatly
improves performance. We believe the cluster-
ranking model has difficulty learning effective
weights from scratch due to noise in the signal
coming from cluster-level decisions (an overall
bad cluster merge may still involve a few cor-

649



rect pairwise links) and the smaller amount of
data used to train the cluster-ranking model (many
possible actions are pruned away during prepro-
cessing). We believe the score would be even
lower without search-space pruning, which stops
the model from considering many bad actions.

Easy-First Cluster Ranking. We compare the ef-
fectiveness of easy-first cluster-ranking with the
commonly used left-to-right approach. Using a
left-to-right strategy simply requires changing the
preprocessing step ordering the mentions so men-
tions are sorted by their position in the document
instead of their highest scoring coreference link
according to the mention-ranking model. We find
the easy-first approach slightly outperforms us-
ing a left-to-right ordering of mentions. We be-
lieve this is because delaying hard decisions until
later reduces the problem of early mistakes caus-
ing later decisions to be made incorrectly.

Learning to Search. We also compare learning to
search with the simpler approach of training the
model on a trajectory of gold coreference deci-
sions (i.e., training on a fixed cost-sensitive clas-
sification dataset). Using this approach signifi-
cantly decreases performance. We attribute this to
the model not learning how to deal with mistakes
when it only sees correct decisions during training.

6.3 Capturing Semantic Similarity

Using semantic information to improve corefer-
ence accuracy has had mixed in results in previous
research, and has been called an “uphill battle” in
coreference resolution (Durrett and Klein, 2013).
However, word embeddings are well known for
being effective at capturing semantic relatedness,
and we show here that neural network coreference
models can take advantage of this.

Perhaps the case where semantic similarity is
most important is in linking nominals with no head
match (e.g., “the nation” and “the country”). We
compare the performance of our neural network
model with our earlier statistical system (Clark
and Manning, 2015) at classifying mention pairs
of this type as being coreferent or not. The neu-
ral network shows substantial improvement (18.9
F1 vs. 10.7 F1) on this task compared to the more
modest improvement it gets at classifying any pair
of mentions as coreferent (68.7 F1 vs. 66.1 F1).
Some example wins are shown in Table 4. These
types of coreference links are quite rare in the
CoNLL data (about 1.2% of the positive coref-

Antecedent Anaphor

the country’s leftist rebels the guerrillas
the company the New York firm
the suicide bombing the attack
the gun the rifle
the U.S. carrier the ship

Table 4: Examples of nominal coreferences with
no head match that the neural model gets correct,
but the system from Clark and Manning (2015)
gets incorrect.

erence links in the test set), so the improvement
does not significantly contribute to the final sys-
tem’s score, but it does suggest progress on this
difficult type of coreference problem.

6.4 Final System Performance

In Table 5 we compare the results of our system
with state-of-the-art approaches for English and
Chinese. Our mention-ranking model surpasses
all previous systems. We attribute its improvement
over the neural mention ranker from Wiseman et
al. (2015) to our model using a deeper neural net-
work, pretrained word embeddings, and more so-
phisticated pretraining.

The cluster-ranking model improves results fur-
ther across both languages and all evaluation met-
rics, demonstrating the utility of incorporating
entity-level information. The improvement is
largest in CEAFφ4 , which is encouraging because
CEAFφ4 is the most recently proposed metric, de-
signed to correct flaws in the other two (Luo,
2005). We believe entity-level information is par-
ticularly useful for preventing bad merges between
large clusters (see Figure 4 for an example). How-
ever, it is worth noting that in practice the much
more complicated cluster-ranking model brings
only fairly modest gains in performance.

7 Related Work

There has been extensive work on machine learn-
ing approaches to coreference resolution (Soon et
al., 2001; Ng and Cardie, 2002), with mention-
ranking models being particularly popular (Denis
and Baldridge, 2007; Durrett and Klein, 2013;
Björkelund and Kuhn, 2014).

We train a neural mention-ranking model in-
spired by Wiseman et al. (2015) as a starting point,
but then use it to pretrain a cluster-ranking model
that benefits from entity-level information. Wise-

650



MUC B3 CEAFφ4
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Avg. F1

CoNLL 2012 English Test Data

Clark and Manning (2015) 76.12 69.38 72.59 65.64 56.01 60.44 59.44 52.98 56.02 63.02
Peng et al. (2015) – – 72.22 – – 60.50 – – 56.37 63.03
Wiseman et al. (2015) 76.23 69.31 72.60 66.07 55.83 60.52 59.41 54.88 57.05 63.39
Wiseman et al. (2016) 77.49 69.75 73.42 66.83 56.95 61.50 62.14 53.85 57.70 64.21

NN Mention Ranker 79.77 69.10 74.05 69.68 56.37 62.32 63.02 53.59 57.92 64.76
NN Cluster Ranker 78.93 69.75 74.06 70.08 56.98 62.86 62.48 55.82 58.96 65.29

CoNLL 2012 Chinese Test Data

Chen & Ng (2012) 59.92 64.69 62.21 60.26 51.76 55.69 51.61 58.84 54.99 57.63
Björkelund & Kuhn (2014) 69.39 62.57 65.80 61.64 53.87 57.49 59.33 54.65 56.89 60.06

NN Mention Ranker 72.53 65.72 68.96 65.49 56.87 60.88 61.93 57.11 59.42 63.09
NN Cluster Ranker 73.85 65.42 69.38 67.53 56.41 61.47 62.84 57.62 60.12 63.66

Table 5: Comparison with the current state-of-the-art approaches on the CoNLL 2012 test sets. NN
Mention Ranker and NN Cluster Ranker are contributions of this work.
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	

Russian 
President 
Vladimir Putin, his, 

the Russian  
President,  

President 
Clinton’s, Bill Clinton, Mr. Clinton’s he … , { { } } 

incorrect link predicted by 
the mention-ranking model 

… , … , 

Figure 4: Thanks to entity-level information, the cluster-ranking model correctly declines to merge these
two large clusters when running on the test set. However, the mention-ranking model incorrectly links
the Russian President and President Clinton’s, which greatly reduces the final precision score.

man et al. (2016) extend their mention-ranking
model by incorporating entity-level information
produced by a recurrent neural network running
over the candidate antecedent-cluster. However,
this is an augmentation to a mention-ranking
model, and not fundamentally a clustering model
as our cluster ranker is.

Entity-level information has also been incorpo-
rated in coreference systems using joint inference
(McCallum and Wellner, 2003; Poon and Domin-
gos, 2008; Haghighi and Klein, 2010) and systems
that build up coreference clusters incrementally
(Luo et al., 2004; Yang et al., 2008; Raghunathan
et al., 2010). We take the latter approach, and
in particular combine the cluster-ranking (Rah-
man and Ng, 2011; Ma et al., 2014) and easy-first
(Stoyanov and Eisner, 2012; Clark and Manning,
2015) clustering strategies. These prior systems
all express entity-level information in the form of
hand-engineered features and constraints instead
of entity-level distributed representations that are
learned from data.

We train our system using a learning-to-search
algorithm similar to SEARN (Daumé III et al.,
2009). Learning-to-search style algorithms have
been employed to train coreference resolvers on
trajectories of decisions similar to those that would

be seen at test-time by Daumé et al. (2005), Ma et
al. (2014), and Clark and Manning (2015). Other
works use structured perceptron models for the
same purpose (Stoyanov and Eisner, 2012; Fer-
nandes et al., 2012; Björkelund and Kuhn, 2014).

8 Conclusion

We have presented a coreference system that cap-
tures entity-level information with distributed rep-
resentations of coreference cluster pairs. These
learned, dense, high-dimensional feature vectors
provide our cluster-ranking coreference model
with a strong ability to distinguish beneficial clus-
ter merges from harmful ones. The model is
trained with a learning-to-search algorithm that al-
lows it to learn how local decisions will affect
the final coreference score. We evaluate our sys-
tem on the English and Chinese portions of the
CoNLL 2012 Shared Task and report a substantial
improvement over the current state-of-the-art.

Acknowledgments

We thank Will Hamilton, Jon Gauthier, and the
anonymous reviewers for their thoughtful com-
ments and suggestions. This work was supported
by NSF Award IIS-1514268.

651



References
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.

2013. Polyglot: Distributed word representations
for multilingual NLP. Conference on Natural Lan-
guage Learning (CoNLL), pages 183–192.

Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563–566.

Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Empirical Methods in Natural Language Processing
(EMNLP), pages 294–303.

Anders Björkelund and Jonas Kuhn. 2014. Learn-
ing structured perceptrons for coreference resolu-
tion with latent antecedents and non-local features.
In Association of Computational Linguistics (ACL),
pages 47–57.

Kai-Wei Chang, He He, Hal Daumé III, and John Lang-
ford. 2015a. Learning to search for dependencies.
arXiv preprint arXiv:1503.05615.

Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agar-
wal, Hal Daumé III, and John Langford. 2015b.
Learning to search better than your teacher. In Inter-
national Conference on Machine Learning (ICML).

Chen Chen and Vincent Ng. 2012. Combining the
best of two worlds: A hybrid approach to multilin-
gual coreference resolution. In Proceedings of the
Joint Conference on Empirical Methods in Natural
Language Processing and Conference on Computa-
tional Natural Language Learning - Shared Task,
pages 56–63.

Kevin Clark and Christopher D. Manning. 2015.
Entity-centric coreference resolution with model
stacking. In Association for Computational Linguis-
tics (ACL).

Hal Daumé III and Daniel Marcu. 2005. A large-
scale exploration of effective global features for a
joint entity detection and tracking model. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 97–104.

Hal Daumé III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
Learning, 75(3):297–325.

Hal Daumé III, John Langford, and Stephane Ross.
2014. Efficient programmable learning to search.
arXiv preprint arXiv:1406.1837.

Pascal Denis and Jason Baldridge. 2007. A ranking
approach to pronoun resolution. In International
Joint Conferences on Artificial Intelligence (IJCAI),
pages 1588–1593.

Greg Durrett and Dan Klein. 2013. Easy victories
and uphill battles in coreference resolution. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1971–1982.

Greg Durrett, David Leo Wright Hall, and Dan Klein.
2013. Decentralized entity-level modeling for coref-
erence resolution. In Association for Computational
Linguistics (ACL), pages 114–124.

Eraldo Rezende Fernandes, Cı́cero Nogueira Dos San-
tos, and Ruy Luiz Milidiú. 2012. Latent structure
perceptron with feature induction for unrestricted
coreference resolution. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Conference on Computa-
tional Natural Language Learning - Shared Task,
pages 41–48.

Aria Haghighi and Dan Klein. 2010. Coreference
resolution in a modular, entity-centered model. In
Human Language Technology and North American
Association for Computational Linguistics (HLT-
NAACL), pages 385–393.

Geoffrey Hinton and Tijmen Tieleman. 2012. Lecture
6.5-RmsProp: Divide the gradient by a running av-
erage of its recent magnitude. COURSERA: Neural
Networks for Machine Learning, 4.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Association for Computational
Linguistics (ACL), page 135.

Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Empirical Methods in Natural
Language Processing (EMNLP), pages 25–32.

Chao Ma, Janardhan Rao Doppa, J Walker Orr,
Prashanth Mannem, Xiaoli Fern, Tom Dietterich,
and Prasad Tadepalli. 2014. Prune-and-score:
Learning for greedy coreference resolution. In Em-
pirical Methods in Natural Language Processing
(EMNLP).

Sebastian Martschat and Michael Strube. 2015. La-
tent structures for coreference resolution. Transac-
tions of the Association for Computational Linguis-
tics (TACL), 3:405–418.

Andrew McCallum and Ben Wellner. 2003. Toward
conditional models of identity uncertainty with ap-
plication to proper noun coreference. In Proceed-
ings of the IJCAI Workshop on Information Integra-
tion on the Web.

652



Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems (NIPS), pages 3111–3119.

Vinod Nair and Geoffrey E. Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In International Conference on Machine Learning
(ICML), pages 807–814.

Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Association of Computational Linguistics (ACL),
pages 104–111.

Haoruo Peng, Kai-Wei Chang, and Dan Roth. 2015. A
joint framework for coreference resolution and men-
tion head detection. Conference on Natural Lan-
guage Learning (CoNLL), 51:12.

Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with markov logic.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 650–659.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceed-
ings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Confer-
ence on Computational Natural Language Learning
- Shared Task, pages 1–40.

Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A
multi-pass sieve for coreference resolution. In Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 492–501.

Altaf Rahman and Vincent Ng. 2011. Narrowing the
modeling gap: a cluster-ranking approach to coref-
erence resolution. Journal of Artificial Intelligence
Research (JAIR), pages 469–521.

Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521–544.

Veselin Stoyanov and Jason Eisner. 2012. Easy-
first coreference resolution. In International Con-
ference on Computational Linguistics (COLING),
pages 2519–2534.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th conference on Message understand-
ing, pages 45–52.

Sam Wiseman, Alexander M Rush, Stuart M Shieber,
and Jason Weston. 2015. Learning anaphoricity and

antecedent ranking features for coreference resolu-
tion. In Association of Computational Linguistics
(ACL), pages 92–100.

Sam Wiseman, Alexander M Rush, Stuart M Shieber,
and Jason Weston. 2016. Learning global fea-
tures for coreference resolution. In Human Lan-
guage Technology and North American Association
for Computational Linguistics (HLT-NAACL).

Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. An entity-mention
model for coreference resolution with inductive
logic programming. In Association of Computa-
tional Linguistics (ACL), pages 843–851.

653


