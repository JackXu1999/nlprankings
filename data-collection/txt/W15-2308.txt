



















































General Perspective on Distributionally Learnable Classes


Proceedings of the 14th Meeting on the Mathematics of Language (MoL 14), pages 87–98,
Chicago, USA, July 25–26, 2015. c©2015 Association for Computational Linguistics

General Perspective on Distributionally Learnable Classes

Ryo Yoshinaka
Kyoto University, Japan
ry@i.kyoto-u.ac.jp

Abstract

Several algorithms have been proposed to
learn different subclasses of context-free
grammars based on the idea generically
called distributional learning. Those tech-
niques have been applied to many formalisms
richer than context-free grammars like mul-
tiple context-free grammars, simple context-
free tree grammars and others. The learning
algorithms for those different formalisms are
actually quite similar to each other. We in
this paper give a uniform view on those algo-
rithms.

1 Introduction

Approaches based on the idea generically called dis-
tributional learning have been making great success
in the algorithmic learning of various subclasses of
context-free grammars (CFGs) (Clark, 2010c; Yoshi-
naka, 2012). Those techniques are applied to richer
formalisms as well. The formalisms studied so far
include multiple CFGs (Yoshinaka, 2011a), simple
context-free tree grammars (CFTGs) (Kasprzik and
Yoshinaka, 2011), second-order abstract categorial
grammars (Yoshinaka and Kanazawa, 2011), par-
allel multiple CFGs (Clark and Yoshinaka, 2014),
conjunctive grammars (Yoshinaka, 2015) and oth-
ers. The goal of this paper is to present a uniform
view on those algorithms.

Every grammar formalism for which distribu-
tional learning techniques have been proposed so
far generate their languages through context-free
derivation trees, whose nodes are labeled by produc-
tion rules. The formalism and grammar rules deter-

mine how a context-free derivation tree τ is mapped
to a derived object τ̃ = d. A context-free deriva-
tion tree τ can be decomposed into a subtree σ and
a tree-context χ so that τ = χ[σ]. The subtree deter-
mines a substructure s = σ̃ of d and the tree-context
determines a contextual structure c = χ̃ in which
the substructure is plugged to form the derived ob-
ject d = c ⊙ s, where we represent the plugging
operation by ⊙. In the CFG case, c is a string pair
⟨l, r⟩ and s is a string u and ⟨l, r⟩ ⊙ u = lur, which
may correspond to a derivation I ∗⇒ lXr ∗⇒ lur
where I is the initial symbol and X is a nontermi-
nal symbol. In richer formalisms those substructures
and contexts may have richer structures, like tuples
of strings or λ-terms. A learner does not know how
a given example d is derived by a hidden grammar
behind the observed examples. A learner based on
distributional learning simply tries all the possible
decompositions of a positive example into arbitrary
two parts c′ and s′ such that d = c′ ⊙ s′ where
some grammar may derive d thorough a derivation
tree τ ′ = χ′[σ′] with χ̃′ = c′ and σ̃′ = s′. Based
on observation on the relation between substructures
and contexts collected from given examples, a hy-
pothesis grammar is computed. We call properties
on grammars with which distributional learning ap-
proaches work distributional properties.

This paper first formally defines grammar for-
malisms based on context-free derivation trees. We
then show that grammars with different distribu-
tional properties are learnable by standard distri-
butional learning techniques if the formalism sat-
isfies some conditions, which include polynomial-
time decomposability of objects into contexts and

87



substructures. In addition, we discuss cases where
we cannot enumerate all of the possible contexts and
substructures.

2 Σ-grammars

There is a number of ways to represent a language,
a subset of an object set O∗, whose elements are
typically strings, trees but anythings encodable are
eligible. Formalisms this paper discusses generate
objects in O∗ through context-free derivation trees
τ , which are mapped to an element d ∈ O∗ in a
uniform way. The map is inductively defined and
computed. Each derivation subtree τ ′ of τ also de-
termines an object, which we call a substructure
of d. Each substructure is not necessarily a mem-
ber of O∗. For example, nonterminal symbols of
multiple CFGs (Seki et al., 1991) derive n-tuples of
strings, where the value n is unique to each non-
terminal, while the languages generated by multiple
CFGs are still simply string sets. A generalization of
the CFG formalism is specified by kinds of objects
that each nonterminal generates and admissible op-
erations over those objects.

Let O be a set of objects, which are identified with
their codes of finite length. We have a set Ω of fi-
nite representations O which are interpreted as sub-
sets OO of O through an effective procedure. By
a sort we flexibly refer to O ∈ Ω or OO ⊆ O.
We also have an indexed family of computable func-
tions from tuples of objects of some sorts to objects
of some sort. Let F be a set of function names or
function indices f , which represent functions f̃ . By
O1× · · ·×On → O0 we denote the set of functions
whose domain is O1 × · · · × On and codomain is
O0. By FO0,O1,...,On , we denote the set of function
names f ∈ F with f̃ ∈ OO1 × · · · × OOn → OO0 .
We assume that the domain sorts O1, . . . , On and the
codomain sort O0 are easily computed from f . We
specify a class of grammars by a triple, which we
call a signature, Σ = ⟨Ω, F, O∗⟩ where O∗ ∈ Ω is a
special sort of objects. We write O∗ for OO∗ .

A context-free Σ-grammar (Σ-grammar for
short) is a tuple G = ⟨N, σ, F, P, I⟩ where N is a
finite set of nonterminal symbols, I ⊆ N is a set of
initial symbols, σ ∈ N → Ω is a sort assignment on
nonterminals such that σ(X) = O∗ for all X ∈ I ,
F ⊆ F is a finite set of function names, and P is a

finite set of production rules, which are elements of
N × F ×N∗. Each production rule is denoted as

X0 ← f⟨X1, . . . , Xn⟩

where X0, . . . , Xn ∈ N and f ∈ FO0,O1,...,On for
σ(Xi) = Oi. For each O ∈ Ω, NO = σ−1(O) ⊆ N
is the set of O-nonterminals which are assigned
the sort O. By G(Σ) we denote the class of Σ-
grammars.

A Σ-grammar defines its language via derivation
trees, which are recursively defined as follows.

• If τi are Xi-derivation trees for i = 1, . . . , n and
ρ is a rule of the form X0 ← f⟨X1, . . . , Xn⟩,
then the term τ0 = ρ[τ1, . . . , τn] is an X0-
derivation tree. Its yield τ̃0 is f̃(τ̃1, . . . , τ̃n) ∈
Oσ(X0) where τ̃i is the yield of τi.

The case where n = 0 gives the base of this re-
cursive definition. An X-derivation tree is com-
plete if X ∈ I . The yield of any X-derivation tree
is called an X-substructure. By S(G,X) we de-
note the set of X-substructures. The language of
G is L(G) = ∪X∈I S(G,X), which we call a Σ-
language. In other words, L(G) is the set of the
yields of complete derivation trees. The class of Σ-
languages is denoted by L(Σ).

Distributional learning is concerned with what X-
derivation contexts represent. An X-derivation con-
text is obtained by replacing an occurrence of an
X-derivation tree in a complete derivation tree by
a special symbol □σ(X). Accordingly the yield χ̃ of
an X-derivation context χ should be a finite repre-
sentation of a function that gives χ̃[τ ] when applied
to τ̃ for any X-derivation tree τ . We assume to have
a set EO of representations of functions from OO to
O∗ for O ∈ Ω to which the yields of derivation con-
texts belong.

• □X is an X-derivation context for all X ∈ I and
its yield □O∗ ∈ EO∗ represents the identity func-
tion on O∗,
• For an X-derivation context χ0, a rule ρ =

X ← f⟨X1, . . . , Xn⟩ and Xi-derivation
trees τi for i ∈ {1, . . . , n} − {j}, the
term χ obtained by replacing □X in χ0
by ρ[τ1, . . . , τj−1,□Xj , τj+1, . . . , τn] is an Xj-
derivation context. Its yield χ̃ ∈ Eσ(Xj), which

88



is denoted as

χ̃ = χ̃0⊙ f̃(τ̃1, . . . , τ̃j−1, □σ(Xj), τ̃j−1, . . . , τ̃n),

represents the function ϕ ∈ Oσ(Xj) → O∗ such
that for all s ∈ Oσ(Xj),

ϕ(s) = ϕ0(f̃(τ̃1, . . . , τ̃j−1, s, τ̃j+1, . . . , τ̃n)) ,

where ϕ0 is the function represented by χ̃0.
The yield of any X-derivation context is called

an X-context. By C(G,X) we denote the set of X-
contexts. For c ∈ C(G,X) and s ∈ S(G,X), c ⊙ s
is the result of the application of the function repre-
sented by c to s.

3 Context-substructure relation

By S and C we denote the set of substructures and
contexts, respectively, which can be obtained by
some grammar in G(Σ):

S =
∪

O∈Ω
SO and C =

∪

O∈Ω
CO where

SO =
∪
{ S(G,X) | X is an O-nonterminal

of some G ∈ G(Σ) }
CO =

∪
{ C(G,X) | X is an O-nonterminal

of some G ∈ G(Σ) }.

We write S∗ for SO∗ . Note that the above definition
is relative to Σ. Even if OO1 = OO2 for different
O1, O2 ∈ Ω, it can be the case that SO1 ̸= SO2 and
CO1 ̸= CO2 . Though usually OO has a definition
independent from Σ, it is possible to specify OO in
terms of the signature so that SO = OO. Clearly if
s ∈ SO and c ∈ CO, then there is a grammar G ∈
G(Σ) generating c⊙s using a nonterminal of sort O.
Therefore, c ⊙ s is well defined for any s ∈ SO and
c ∈ CO without specifying a particular Σ-grammar.
Similarly c⊙ f̃(s1, . . . , sj−1, □Oj , sj+1, sn) is well
defined for any c ∈ CO0 , f ∈ FO0,...,On and si ∈
SOi . This operation is generalized to sets S ⊆ SO
and C ⊆ CO in the straightforward way, like C ⊙
S = { c⊙ s | c ∈ C and s ∈ S }.

Hereafter, whenever we write c ⊙ s and
f̃(s1, . . . , sn), we assume they are well-formed.
That is, the domains of the functions repre-
sented by c and f match the sorts to which s
and s1, . . . , sn belong, respectively. Accordingly

we drop the subscript O from □O and write
f̃(s1, . . . , sj−1, □, sj+1, . . . , sn). When we have a
substructure set S, we assume S ⊆ SO for some
O ∈ Ω. We often identify s with {s} unless confu-
sion arises. Also we assume SO ̸= ∅ for all O ∈ Ω.
The same assumptions apply to contexts.

We are interested in whether the composition c⊙s
belongs to a concerned language L ∈ L(Σ). Clark
(2010b) has introduced syntactic concept lattices to
analyze the context-substring relation on string lan-
guages and particularly to design a distributional
learning algorithm for CFLs. Generalizing his dis-
cussion, we define an O-concept lattice BO(L) of a
language L ⊆ O∗ for respective sorts O ∈ Ω. As-
suming L and O understood from the context, let us
write

S‡ = { c ∈ CO | c⊙ S ⊆ L } ,
C† = { s ∈ SO | C ⊙ s ⊆ L }

for S ⊆ SO and C ⊆ CO. We write S† for S‡† and
C‡ for C†‡ .

We call a pair ⟨S,C⟩ ⊆ SO × CO a concept iff
S† = C and C‡ = S. For any S ⊆ SO and C ⊆
CO, ⟨S†, S‡⟩ and ⟨C†, C‡⟩ are concepts. We call
them the concepts induced by S and C, respectively.
For two concepts ⟨S1, C1⟩ and ⟨S2, C2⟩ in BO(L),
we write ⟨S1, C1⟩ ≤OL ⟨S2, C2⟩ if S1 ⊆ S2, which
is equivalent to C2 ⊆ C1. With this partial order,
BO(L) is a complete lattice.

We can introduce a partial order to substructure
sets based on the concepts that they induce. Let us
write S1 ≤OL S2 if S

‡
2 ⊆ S‡1. The relation represents

the substitutability of S1 for S2.

Lemma 1. The following three are equivalent for
S, T ⊆ SO:
• S ≤OL T ,
• c⊙ T ⊆ L implies c⊙ S ⊆ L for all c ∈ CO,
• T ‡ ⊙ S ⊆ L.
If Si ≤OiL Ti for i = 1, . . . , n, then for any

f ∈ FO0,O1,...,On , we have f(S1, . . . , Sn) ≤O0L
f(T1, . . . , Tn) .

If S1 ≤L S2 and S2 ≤L S1, we write S1 ≡L S2.

89



4 Conditions to be distributionally
learnable

Distributional learning algorithms decompose ex-
amples d ∈ S∗ into contexts c ∈ CO and substruc-
tures s ∈ SO so that c ⊙ s = d. Then a primal
approach uses substructures or sets of substructures
as nonterminals of a conjecture grammar. We want
each nonterminal [[S]] indexed by S ⊆ SO to sat-
isfy S(G, [[S]]) = S†. On the other hand, a dual
approach uses contexts or sets of contexts as non-
terminals where the semantics of the nonterminal is
S(G, [[C]]) = C†. For an object d ∈ S∗, O ∈ Ω and
O⃗ = (O0, . . . , On) with Oi ∈ Ω, we define

SO|d = { s ∈ SO | c⊙ s = d for some c ∈ CO }
CO|d = { c ∈ CO | c⊙ s = d for some s ∈ SO }
FO⃗|d = { f ∈ FO⃗ | c⊙ f(s1, . . . , sm) = d

for some c ∈ CO0|d and si ∈ SOi|d } ,

S|D =
∪d∈D

O∈Ω SO|d, CO|D =
∪d∈D

O∈Ω CO|d and F|D =∪d∈D
O⃗∈Ω∗ FO⃗|d for D ⊆ O∗. Let Ω|D =

∪
d∈D Ω|d for

Ω|d = {O | SO|d ̸= ∅ }.
We require G(Σ) to be a tractable formalism such

that composition and decomposition can be done ef-
ficiently.
Assumption 1. There are polynomial-time algo-
rithms which

• decide whether s ∈ SO from s ∈ S and O ∈ Ω,
• compute f̃(s1, . . . , sn) from si ∈ SOi and f ∈
FO0,O1,...,On ,
• decide whether c ∈ CO from c ∈ C and O ∈ Ω,
• compute c⊙ s from c ∈ CO and s ∈ SO for any
O ∈ Ω.
• decide whether s ∈ L(G) from s ∈ S∗ and G ∈
G(Σ),

Assumption 2. There is p ∈ N such that the arity of
every f ∈ F is at most p.
Assumption 3. There are polynomial-time algo-
rithms that compute SUB(d), CON(d) and FUN(d)
from d ∈ S∗ such that S|d ⊆ SUB(d) ⊆ S, C|d ⊆
CON(d) ⊆ C and F|d ⊆ FUN(d) ⊆ F.

Actually by Assumptions 2 and 3, one can de-
rive the polynomial-time uniform membership de-
cidability. Moreover, it is easy to filter out nonmem-
bers of S|d, C|d and F|d from SUB(d), CON(d) and

FUN(d), respectively, but it is not necessary. As-
sumption 3 implies |Ω|d| is polynomially bounded,
since O ∈ Ω|d iff FO,O1,...,On ̸= ∅ for some
O1, . . . , On.

We write SUBO(D) = SUB(D) ∩ SO,
CONO(D) = CON(D) ∩ CO and FUNO⃗(D) =
FUN(D) ∩ FO⃗.

It is often the case that elements of Ω repre-
sents pairwise disjoint sets. Actually for any sig-
nature Σ, one can find Σ′ = ⟨Ω′, F′, O∗⟩ that sat-
isfies this condition such that L(Σ) = L(Σ′). Let
Ω′ = {O′ | O ∈ Ω } ∪ {O∗} and OO′ = OO ×{O}
for each O ∈ Ω − {O∗}. For f ∈ FO0,...,On with
f̃(s1, . . . , sn) = s0, we have f ′ ∈ F′O′0,...,O′n with
f̃ ′(s′1, . . . , s

′
n) = s

′
0 where s

′
i = (si, Oi) if Oi ∈

Ω − {O∗} and s′i = si if Oi = O∗. Clearly every
Σ-grammar has an equivalent Σ′-grammar. More-
over, this makes it clear that from s ∈ S one can
immediately specify the unique sort O′ ∈ Ω′ such
that s ∈ OO′ . Similarly we may assume that each
c ∈ C has unique O ∈ Ω such that c ∈ CO and find-
ing that O is a trivial task. Hereafter we work under
this assumption. By O and f we mean OO and f̃ for
notational convenience.

Example 1. A right regular grammar over an alpha-
bet ∆ is a Σreg-grammar for Σreg = ⟨{∆∗}, F, ∆∗⟩.
F has nullary functions which are members of ∆ ∪
{ε} and unary functions fa for fa(w) = aw for all
w ∈ Σ∗ for some a ∈ ∆. Clearly the class of right
regular grammars satisfies Assumptions 1, 2 and 3.

Example 2. A CFG is a Σcfg-grammar for Σcfg =
⟨{∆∗}, F, ∆∗⟩ where each f ∈ F is represented as
an (n + 1)-dimension vector of strings ⟨u0, . . . , un⟩
such that f(v1, . . . , vn) = u0v1u1 . . . vnun for all
vi ∈ ∆∗. The class of CFGs itself satisfies Assump-
tion 1 but not Assumptions 2 and 3, since we have
no limit on n. But several normal forms fulfill As-
sumptions 2 and 3.

Example 3. Let Ω = {O1, O2, . . . } where Om
denotes the set of m-tuples of strings. Linear
context-free rewriting systems, equivalent to non-
deleting multiple CFGs, are Σmcfg-grammars where
O∗ = O1 = ∆∗ and every f ∈ FOm0 ,Om1 ,...,Omn
concatenates strings ui,j occurring in an input
⟨⟨u1,1, . . . , u1,m1⟩, . . . , ⟨un,1, . . . , un,mn⟩⟩ in some
way to form an m0-tuple of strings. The uni-
form membership problem of this class is PSPACE-

90



complete (Kaji et al., 1992). There are infinitely
many ways to decompose a string d into substruc-
tures and contexts as Om ∈ Ω|d for all m. Assump-
tions 1 and 3 will be fulfilled when we restrict ad-
missible functions so that FOm0 ,...,Omn ̸= ∅ only if
n ≤ p and mi ≤ q for all i.

As is the case for multiple CFGs, Assumption 2 is
often needed to make the uniform membership prob-
lem solvable in polynomial-time (Assumption 1).

5 Learning models

Learning algorithms in this paper work under three
different learning models.

A positive presentation (text) of a language L∗ ⊆
O∗ is an infinite sequence d1, d2, · · · ∈ O∗ such that
L∗ = { di | i ≥ 1 }. In the framework of identifica-
tion in the limit from positive data, a learner is given
a positive presentation of the language L∗ = L(G∗)
of the target grammar G∗ and each time a new ex-
ample di is given, it outputs a grammar Gi computed
from d1, . . . , di. We say that a learning algorithm A
identifies G∗ in the limit from positive data if for any
positive presentation d1, d2, . . . of L(G∗), there is
an integer n such that Gn = Gm for all m ≥ n and
L(Gn) = L(G∗). We say that A identifies a class
G of grammars in the limit from positive data iff A
identifies all G ∈ G in the limit from positive data.

We say thatA identifies a class G of grammars in
the limit from positive data and membership queries
when we allow A to ask membership queries (MQs)
to an oracle when it computes a hypothesis grammar.
An instance of an MQ is an object d ∈ O∗ and the
oracle answers whether d ∈ L∗ in constant time.

The third model is the learning with a minimally
adequate teacher (MAT). A learner is not given
a positive presentation but it may ask equivalence
queries (EQs) to an oracle in addition to MQs. An in-
stance of an EQ is a grammar G. If L(G) = L∗, the
oracle answers “Congratulations!” and the learn-
ing process ends. Otherwise, the oracle returns a
counerexample d ∈ (L∗ − L(G)) ∪ (L(G) − L∗),
which is called positive if d ∈ L∗ − L(G) and neg-
ative if d ∈ L(G)− L∗.

When we have an oracle, the learning task itself
is trivial unless we show some favorable property on
the learning efficiency.

6 Learnable subclasses

This section presents how Σ-grammars with distri-
butional properties can be learned. Note that all of
those properties are relative to Σ. We assume Σ-
grammars G∗ = ⟨N∗, σ∗, F∗, P∗, I∗⟩ in this section
have no useless nonterminals or functions. That is,
S(G∗, X) ̸= ∅, C(G∗, X) ̸= ∅ for all X ∈ N∗ and
every f ∈ F∗ appears in some rule in P∗.

6.1 Substitutable Languages
Definition 1 (Clark and Eyraud (2007)). A language
L ∈ L(Σ) is said to be substitutable if for any O ∈
Ω, c1, c2 ∈ CO and s1, s2 ∈ SO,
c1 ⊙ s1, c1 ⊙ s2, c2 ⊙ s1 ∈ L implies c2 ⊙ s2 ∈ L .

The definition can be rephrased as follows:

s‡1 ∩ s‡2 ̸= ∅ implies s1 ≡L s2 .
Example 4. Yoshinaka (2008) has proposed a learn-
ing algorithm for k, l-substitutable CFLs, which sat-
isfy the following property:

x1uy1vz1, x1uy2vz1, x2uy1vz2 ∈ L
=⇒ x2uy2vz2 ∈ L

for any xi, yi, zi ∈ ∆∗, u ∈ ∆k and v ∈ ∆l. We
define a signature Σk,l = ⟨Ωk,l, Fk,l, O∗⟩ as fol-
lows. Let Ωk,l = {O∗} ∪ {Ou,v | u ∈ ∆k and v ∈
∆l }∪{Ou | u ∈ ∆<k+l }, where O∗ = ∆∗, Ou,v =
{uwv | w ∈ ∆∗ } and Ou = {u}. Here we put
overlines to make elements of Ω pairwise disjoint.
Let Fk,l = {+α,β | α, β ∈ Ω − {∆∗} } ∪ {□O∗α |
α ∈ Ω−{O∗} }∪∆. The binary function +α,β con-
catenates two strings from sorts α and β and gives
the right sort in Ω − {O∗}. For example, +α,β ∈ F
with α = Ou,v, β = Ow has codomain Ou,x where
x is the suffix of vw of length l. The unary opera-
tion □O∗α ∈ FO∗,α simply removes the overline and
“promotes” ū ∈ α to u ∈ O∗. ∆ consists of the
nullary functions giving a single letter from ∆. It is
not hard to see that every CFG has an equivalent Σk,l-
grammar. Note that O∗-nonterminals never occur
on the right hand side of a rule in a Σk,l-grammar.
Hence CO∗ is just the singleton {□O∗} such that
□O∗ ⊙ u = u for all u ∈ ∆∗, whereas Cα ̸= CO∗
contains arbitrary pairs of strings ⟨l, r⟩ ∈ ∆∗ ×∆∗
such that ⟨l, r⟩ ⊙ ū = lur for any ū ∈ α. The Σk,l-
substitutability is exactly the k, l-substitutability.

91



Theorem 1. The class of substitutable Σ-languages
is identifiable in the limit from positive data.

The theorem follows Lemmas 2 and 3 below.
From a finite set D of positive examples, Al-

gorithm 1 computes the grammar SUBSTP(D) =
⟨N, σ, F, P, I⟩ defined as follows:
• NO = { [[s]] | s ∈ SUBO(D) } for O ∈ Ω|D,
• I = { [[s]] | s ∈ D },
• F = FUN(D),
• P consists of the rules of the form

[[s0]]← f⟨[[s1]], . . . , [[sn]]⟩
where f ∈ FUNO0,...,On(D) for [[si]] ∈ NOi if
there is c ∈ CONO(D) such that

c⊙ s0, c⊙ f(s1, . . . , sn) ∈ D .

Since we assume elements of Ω are pairwise dis-
joint, each [[s]] belongs to a unique sort. Otherwise,
each nonterminal should be tagged with a sort like
[[s,O]].

Algorithm 1 Learning substitutable Σ-grammars
Data: A positive presentation d1, d2, . . .
Result: A sequence of grammars G1, G2, . . .
let Ĝ be a grammar such that L(Ĝ) = ∅;
for n = 1, 2, . . . do

let D = {d1, . . . , dn};
if D ⊈ L(Ĝ) then

let Ĝ = SUBSTP(D);
end if
output Ĝ as Gn;

end for

An alternative way to construct a grammar is to
use contexts rather than substructures for nontermi-
nals. One can replace SUBSTP(D) in the algorithm
by SUBSTD(D) which is defined as follows.
• NO = { [[c]] | c ∈ CONO(D) } for O ∈ Ω|D,
• I = { [[□O∗ ]] },
• F = FUN(D),
• P consists of the rules of the form

[[c0]]← f⟨[[c1]], . . . , [[cn]]⟩
where f ∈ FUNO0,...,On for [[ci]] ∈ NOi if there
are si ∈ SUBOi(D) such that
ci ⊙ si ∈ D for all i and c0 ⊙ f(s1, . . . , sn) ∈ D .

The existing algorithms for different classes of
substitutable languages (Clark and Eyraud, 2007;
Yoshinaka, 2008; Yoshinaka, 2011a) are based on
slight variants of SUBSTP. This paper shows the
correctness of the algorithm using SUBSTD.

Lemma 2. Let D be a finite subset of a Σ-
substitutable language L∗ and G the grammar out-
put by SUBSTD(D). Then L(G) ⊆ L∗.

Proof. One can show by induction on the deriva-
tion that if s ∈ S(G, [[c]]) then c ⊙ s ∈ L∗. Sup-
pose that G has a rule [[c]] ← f⟨[[c1]], . . . , [[cn]]⟩,
si ∈ S(G, [[ci]]) and s = f(s1, . . . , sn). The induc-
tion hypothesis says ci⊙si ∈ L∗ for all i. By the rule
construction, there are ti for i = 1, . . . , n such that
ci ⊙ ti ∈ D ⊆ L∗ and c⊙ f(t1, . . . , tn) ∈ D ⊆ L∗.
We have si ≡L∗ ti since they occur in the same con-
text ci. By Lemma 1, c⊙ f(s1, . . . , sn) ∈ L∗.

Let G∗ = ⟨N∗, σ∗, F∗, P∗, I∗⟩ be a Σ-grammar
generating L∗. Fix sX ∈ S(G∗, X) and cX ∈
C(G∗, X) where cX = □O∗ for X ∈ I∗. Define
D∗ by

D∗ = { cX ⊙ sX | X ∈ N∗ }
∪ { cX0 ⊙ f(sX1 , . . . , sXn)

| X0 ← f⟨X1, . . . , Xn⟩ ∈ P∗ } .

Lemma 3. If D∗ ⊆ D, then S(G∗, X) ⊆
S(SUBSTD(D), [[sX ]]) for all X .

Proof. Let G = SUBSTD(D). If G∗ has a rule
X0 ← f⟨X1, . . . , Xn⟩ then G has the correspond-
ing rule [[cX0 ]]← f⟨[[cX1 ]], . . . , [[cXn ]]⟩, since

cX0 ⊙ f(sX1 , . . . , sXn), cXi ⊙ sXi ∈ D .

In particular since cX for X ∈ I is the identity func-
tion □O∗ , the corresponding nonterminal [[cX ]] =
[[□O∗ ]] is the initial symbol of G, too.

This shows that we do not need too many data to
achieve a right grammar, since |D∗| ≤ |P∗| + |N∗|,
where | · | denotes the cardinality of a set. Moreover,
it is easy to see Algorithm 1 updates its conjecture in
polynomial time in the total size of D by Assump-
tions 1, 2 and 3.

92



6.2 Finite kernel property
Definition 2 (Clark et al. (2009), Yoshinaka
(2011b)). A nonempty finite set S ⊆ Sσ(X) is called
a k-kernel of a nonterminal X if |S| ≤ k and

S(G,X) ≡L(G) S .

A Σ-grammar G is said to have the k-finite kernel
property (k-FKP) if every nonterminal X has a k-
kernel SX .

Theorem 2. Under Assumptions 1, 2 and 3, Algo-
rithm 2 identifies Σ-grammars with the k-FKP in the
limit from positive data and membership queries.

Algorithm 2 Learning Σ-grammars with k-FKP
Data: A positive presentation d1, d2, . . . of L∗;
Result: A sequence of Σ-grammars G1, G2, . . . ;
let D := K := F := J := ∅;
let Ĝ := PRIMALk(K, F, J);
for n = 1, 2, . . . do

let D := D ∪ {dn}; J := CON(D);
if D ⊈ L(Ĝ) then

let K := SUB(D) and F := FUN(D);
end if
output Ĝ = PRIMALk(K,F, J) as Gn;

end for

The conjecture grammar PRIMALk(K, F, J) =
⟨N, σ, F, P, I⟩ of Algorithm 2 is defined from finite
sets of substructures K ⊆ S, functions F ⊆ F and
contexts J ⊆ C. The subsets of those sets corre-
sponding to respective sorts are denoted as KO =
K ∩ SO, JO = J ∩ CO and FO⃗ = F ∩ FO⃗.
• NO = { [[S]] | S ⊆ KO with 1 ≤ |S| ≤ k } for

each O ∈ Ω|D,
• I = { [[S]] ∈ NO∗ | S ⊆ L },
• P consists of the rules of the form

[[S0]]← f⟨[[S1]], . . . , [[Sn]]⟩

where f ∈ FO0,O1,...,On for [[Si]] ∈ NOi if

(S0
‡ ∩ JO0)⊙ f(S1, . . . , Sn) ⊆ L∗ . (1)

The grammar is constructed by the aid of finitely
many MQs. PRIMALk(K,F, J) can be computed
in polynomial time by Assumptions 1, 2 and 3.
A rule [[S0]] ← f⟨[[S1]], . . . , [[Sn]]⟩ is compatible

with the semantics of the nonterminals if S†0 ⊇
f(S†1, . . . , S

†
n), which is equivalent to

S0
‡ ⊙ f(S1, . . . , Sn) ⊆ L∗ (2)

by Lemma 1. However, this condition (2) cannot
be checked by finitely many MQs. The condition
(1) can be seen as an approximation of (2), which
is decidable by finitely many MQs. Clearly (2) im-
plies (1) but not vice versa. If a rule satisfies (1) but
not (2), we call the rule incorrect. If a rule is in-
correct, there is a witness c ∈ CO0 − JO0 such that
c⊙ S0 ∈ L∗ and c⊙ f(S1, . . . , Sn) /∈ L∗.
Lemma 4. For every finite K ⊆ S and F ⊆ F there
is J ⊆ C such that Ĝ = PRIMAL(K, F, J) has no
incorrect rules and |J | ≤ |F ||K|k(p+1), in which
case L(Ĝ) ⊆ L∗.

Let SX be a k-kernel of each nonterminal X of a
grammar G∗ = ⟨N∗, σ∗, F∗, P∗, I∗⟩ generating L∗.
Lemma 5. There is a finite subset D ⊆ L∗ such
that SX ⊆ S|D for all X ∈ N∗, F∗ ⊆ F|D and
|D| ≤ k|N∗| + |P∗|. Moreover, if SX ⊆ K for all
X ∈ N∗ and F∗ ⊆ F , then L∗ ⊆ L(Ĝ).

We prove Theorem 2 discussing the efficiency.

Proof of Theorem 2. Clearly Algorithm 2 updates
its conjecture in polynomial time in the data size.
Polynomially (in the size of G∗) many positive ex-
amples will stabilize K and F by Lemma 5. After
K and F stabilized, all the incorrect rules will be re-
moved with at most polynomially (in |K||F |) many
examples by Lemma 4. After that point Algorithm 2
never changes the conjecture, which generates the
target language L∗.

6.3 Congruential grammars
Definition 3 (Clark (2010a)). A Σ-grammar G is
said to be congruential if every s ∈ S(G,X) is a
1-kernel of every X ∈ N .

Congruential Σ-grammars have the 1-FKP. Un-
der the following additional assumption, this special
case will be polynomial-time learnable with a mini-
mally adequate teacher.

Assumption 4. For any derivation tree τ , the size of
its yield τ̃ is polynomially bounded by that of τ .

Theorem 3. Under Assumptions 1, 2, 3 and 4, Al-
gorithm 3 learns any language L∗ generated by a

93



congruential Σ-grammar G∗ with a minimally ad-
equate teacher in time polynomial in |N∗|, |F∗|, ℓ
where ℓ is the total size of counterexamples given
to the learner.

Algorithm 3 Learning congruential Σ-grammars
let K := F := J := ∅;
let Ĝ := PRIMAL1(K,F, J);
for n = 1, 2, . . . do

if L(Ĝ) = L∗ (equivalence query) then
output Ĝ and halt;

else if the given counterexample d is positive
(d ∈ L∗ − L(Ĝ)) then

let K := K∪SUB(d) and F := F∪FUN(d);
else

let J := J ∪WITNESSP(τd,□O∗) where τd
is an (implicit) parse tree of d by Ĝ

end if
let Ĝ = PRIMAL1(K, F, J);

end for

Algorithm 3 uses the same grammar construction
PRIMAL as Algorithm 2 where the parameters K
and F are calculated from positive counterexam-
ples given by the oracle. On the other hand, J is
computed in a different way. By Lemma 4, when
the oracle answers a negative counterexample d to-
wards an EQ, our conjecture Ĝ must use an incorrect
rule to derive d. To find and remove such an incor-
rect rule, Algorithm 3 calls a subroutine WITNESSP
with input (τd,□), where τd is a derivation tree of
Ĝ whose yield is d. To be precise, τd does not
have to be a derivation tree. Rather what we re-
quire is that for each s ∈ S|d, one can compute at
least one tuple of s1, . . . , sn ∈ S|d and f ∈ F|d
such that s = f(s1, . . . , sn) and the height of the
lowest derivation tree of each si is strictly lower
than that of s. Indeed one can do this in polyno-
mial time by a dynamic programming method from
SUB(d) and FUN(d). Yet for explanatory easiness,
we treat such information as an (implicit) derivation
tree τd. The procedure WITNESSP returns a con-
text that witnesses an incorrect rule that contributes
to generating d by searching τd recursively calling
itself. The procedure WITNESSP in general takes a
pair (τ, c) such that τ is an [[s]]-derivation tree of Ĝ
and c ∈ s‡ − τ̃ ‡. Let τ = ρ(τ1, . . . , τn) where ρ =
[[s]]← f⟨[[s1]], . . . , [[sn]]⟩. If c⊙ f(s1, . . . , sn) /∈ L∗

then the rule ρ is incorrect. So WITNESSP returns c
which witnesses the incorrectness of the rule. Oth-
erwise, we have

c⊙ f(s1, . . . , sn) ∈ L∗
c⊙ f(τ̃1, . . . , τ̃n) /∈ L∗

for the yields τ̃i of τi. One can find i such that

c⊙ f(s1, . . . , si−1, si, τ̃i+1, . . . , τ̃n) ∈ L∗
c⊙ f(s1, . . . , si−1, τ̃i, τ̃i+1, . . . , τ̃n) /∈ L∗ .

This means an incorrect rule is in τi. We call
WITNESSP(τi, c⊙ f(s1, . . . , si−1, □, τ̃i+1, . . . , τ̃n)).
Lemma 6. The procedure WITNESSP(τd, □) runs
in polynomial time in ℓ and |d|.

Proof. The number of recursive calls of WITNESSP
is no more than the height of τd, which is at most
|S|d|. Let the instance of the j-th recursive call be
(τj , cj) and χj the derivation context for c = χ̃j .
χj+1 is obtained from χj by replacing at most p
subtrees by a derivation tree whose yield is an el-
ement of K. By Assumption 4, the size of cj and
thus the size of an instance of an MQ is polynomi-
ally bounded by |d|ℓ. WITNESSP runs in polynomial
time.

Lemma 7. Each time Algorithm 3 receives a neg-
ative counterexample, at least one incorrect rule is
removed.

Lemma 8. Let G∗ = ⟨N∗, σ, F∗, P∗, I∗⟩ be a con-
gruential grammar generating L∗. Each time Algo-
rithm 3 receives a positive counterexample, the car-
dinality of the set {X ∈ N∗ | K ∩L(G∗, X) = ∅ }
∪ (F∗ − F ) decreases strictly.

Proof of Theorem 3. Time between an EQ and an-
other is polynomially bounded by Lemma 6. By
Lemmas 5 and 8, Algorithm 3 gets at most |N∗| +
|F∗| positive counterexamples. The grammar Ĝ =
PRIMAL(K,F, J) is constructed from those positive
counterexamples, so it has polynomially many rules.
Therefore, by Lemma 7, after getting polynomially
many negative counterexamples, which suppress all
the incorrect rules, Algorithm 3 gets a right grammar
representing L∗.

94



6.4 Finite context property
Definition 4 (Clark (2010b), Yoshinaka (2011b)1).
A nonempty finite set C ⊆ C is called a k-context
of a nonterminal X if |C| ≤ k and

S(G,X) ≡L(G) C† .

A Σ-grammar G is said to have the k-(weak) finite
context property (k-FCP) if every nonterminal X has
a k-context CX .

Theorem 4. Under Assumptions 1, 2 and 3, Algo-
rithm 4 identifies Σ-grammars with the k-FCP in the
limit from positive data and membership queries.

The theorem can be shown by an argument similar
to the proof of Theorem 2 based on Lemmas 9 and
10 below. The discussion on the learning efficiency
of Algorithm 2 is applied to Algorithm 4 as well.

Algorithm 4 Learning Σ-grammars with k-FCP
Data: A positive presentation d1, d2, . . . of L∗;
Result: A sequence of Σ-grammars G1, G2, . . . ;
let D := J := F := K := ∅;
let Ĝ := DUALk(J, F, K);
for n = 1, 2, . . . do

let D := D ∪ {dn}; K := SUB(D);
if D ⊈ L(Ĝ) then

let J := CON(D) and F := FUN(D);
end if
output Ĝ = DUALk(J, F,K) as Gn;

end for

The conjecture grammar DUALk(J, F, K) =
⟨N, σ, F, P, I⟩ of Algorithm 4 is defined from finite
sets of contexts J ⊆ C, functions F ⊆ F and sub-
structures K ⊆ S. For each C ⊆ JO, we write C(K)
to mean C†∩KO. This set can be seen as a finite ap-
proximation of C†, which is computable with MQs.
• NO = { [[C]] | C ⊆ JO with 1 ≤ |C| ≤ k } for

O ∈ Ω|D,
• I = { [[{□∗}]] },
• P consists of the rules of the form

[[C0]]← f⟨[[C1]], . . . , [[Cn]]⟩

where f ∈ FO0,...,On for [[Ci]] ∈ NOi if C0 ⊙
f(C

(K)
1 , . . . , C

(K)
m ) ⊆ L∗.

1We adopt the definition by Yoshinaka, which is slightly
weaker than Clark’s.

We say that a rule [[C0]] ← f([[C1]], . . . , [[Cn]]) is
incorrect if C0⊙ f(C†1, . . . , C†n) ⊈ L∗. In that case,
there are si ∈ C†i such that C0⊙f(s1, . . . , sn) ⊈ L∗.
Lemma 9. For every finite J ⊆ C and F ⊆ F there
is K ⊆ S such that Ĝ = DUAL(J, F, K) has no
incorrect rules and |K| ≤ p|F ||J |k(p+1), in which
case L(Ĝ) ⊆ L∗.

Let G∗ = ⟨N∗, σ∗, F∗, P∗, I∗⟩ generate L∗ and
CX a k-context of each nonterminal X ∈ N∗.
Lemma 10. There is a finite subset D ⊆ L∗ such
that C|D ⊇ CX for all X ∈ N∗, F|D ⊇ F∗ and
|D| ≤ k|N∗| + |P∗|. Moreover, if J ⊇ CX for all
X ∈ N∗ and F ⊇ F∗, then L∗ ⊆ L(Ĝ).

6.5 Context-deterministic grammars
Definition 5 (Shirakawa and Yokomori (1993),
Yoshinaka (2012)2). A Σ-grammar G is said to be
(weakly) context-deterministic if every c ∈ C(G,X)
is a 1-context of every X ∈ NO.

Differently from Theorem 3, we do not need As-
sumption 4 for learning context-deterministic gram-
mars with a minimally adequate teacher.

Theorem 5. Under Assumptions 1, 2 and 3, Al-
gorithm 3 learns any language L∗ generated by a
context-deterministic Σ-grammar G∗ with a min-
imally adequate teacher in time polynomial in
|N∗|, |F∗|, ℓ where ℓ is the total size of counterex-
amples given to the learner.

Proof. By Lemmas 11, 12 and 13 below.

Algorithm 5 uses the same grammar construc-
tion DUAL as Algorithm 4. By Lemma 9, when
the oracle answers a negative counterexample d to-
wards an EQ, our conjecture Ĝ must use an incor-
rect rule to derive d. To find and remove such
an incorrect rule, Algorithm 5 calls a subroutine
WITNESSD with a derivation tree τd of Ĝ whose
yield is d. The procedure WITNESSD returns a fi-
nite set of substructures that witnesses an incorrect
rule that contributes to generating d. An input given
to WITNESSD is in general a [[c]]-derivation tree τ
such that c ⊙ τ̃ /∈ L∗. Let τ = ρ[τ1, . . . , τn] where
ρ = [[c]] ← f⟨[[c1]], . . . , [[cn]]⟩. If there is i such that
ci ⊙ τ̃i /∈ L∗, we recursively call WITNESSD(τi).

2We adopt the definition by Yoshinaka, which is slightly
weaker than Shirakawa and Yokomori’s.

95



Algorithm 5 Learning context-deterministic Σ-
grammars

let J := F := K := ∅;
let Ĝ := DUAL1(J, F, K);
for n = 1, 2, . . . do

if L(Ĝ) = L∗ (equivalence query) then
output Ĝ and halt;

else if the given counterexample d is positive
(d ∈ L∗ − L(Ĝ)) then

let J := J ∪CON(d) and F := F ∪FUN(d);
else

let K := K ∪WITNESSD(τ) where τ is an
(implicit) parse tree of d by Ĝ

end if
let Ĝ = DUAL1(J, F, K);

end for

Otherwise, τ̃i ∈ ci† for all i, which means the
rule ρ is incorrect. WITNESSD(τ) returns the set
{ τ̃1, . . . , τ̃n }. Differently from the case of WIT-
NESSP, an instance of a recursive call is always an
(implicit) derivation tree of some s ∈ S|d. This ex-
plains why we do not need Assumption 4 in this
case.

Lemma 11. Time between an EQ and another is
polynomially bounded.

Lemma 12. Each time Algorithm 5 receives a neg-
ative counterexample, at least one incorrect rule is
removed.

Lemma 13. Let G∗ = ⟨N∗, σ, F∗, P∗, I∗⟩ be a
context-deterministic grammar for L∗. Each time
Algorithm 5 receives a positive counterexample, the
set {X ∈ N∗ | J ∩ C(G∗, X) = ∅ } ∪ (F∗ − F )
gets shrunk.

6.6 Combined approaches
By combining primal and dual approaches, one
can obtain stronger approaches (Yoshinaka, 2012).
The class of Σ-grammars whose nonterminals ad-
mit either a k-kernel or l-context can be learned by
combining the techniques presented in Sections 6.2
and 6.4 under Assumptions 1, 2 and 3. Also
Σ-grammars whose nonterminals satisfy either the
requirement to be congruential or to be context-
deterministic can be learned with a minimally ade-
quate teacher under Assumptions 1, 2, 3 and 4 (Sec-
tions 6.3 and 6.5).

7 Restricted cases

In some grammar classes, it may be the case that
only (supersets of) C|d and F|d are computable in
polynomial-time but S|d is not, or the other way
around: S|d and F|d are efficiently computable but
C|d is not. For example, in non-permuting paral-
lel multiple CFGs (Seki et al., 1991), elements of
S|d for a string d are tuples of strings of the form
⟨v1, . . . , vm⟩ for d = u0v1u1 . . . vmum and such
substrings are polynomially many if m is fixed.
However, C|d contains exponentially many contexts.
Clark and Yoshinaka (2014) showed that still a dual
approach works for parallel multiple CFGs if nonter-
minals are known to have k-contexts belonging to
a certain subset C ⊆ C such that C|d = C|d ∩ C
is polynomial-time computable. A symmetric re-
sult of a primal approach has also been obtained by
Kanazawa and Yoshinaka (2015) targeting a certain
kind of tree grammars. This section does not postu-
late Assumption 3.

Definition 6. A Σ-grammar G is said to have the
(k,S)-FKP if every nonterminal admits a k-kernel
which is a subset of S.

Assumption 5. There are polynomial-time algo-
rithms that compute SUB(d), CON(d) and FUN(d)
such that S|d ⊆ SUB(d) ⊆ S, C|d ⊆ CON(d) ⊆ C
and F|d ⊆ FUN(d) ⊆ F, where S|d = S ∩ S|d.

It is not hard to see that Algorithm 2 works for
learning Σ-grammars with (k,S)-FKP under As-
sumptions 1, 2 and 5. All discussions in Section 6.2
hold for this restricted case.

The symmetric definition and assumption are as
follows.

Definition 7. A Σ-grammar G is said to have the
(k,C)-FCP if every nonterminal admits a k-context
which is a subset of C.

Assumption 6. There are polynomial-time algo-
rithms that compute SUB(d), CON(d) and FUN(d)
such that S|d ⊆ SUB(d) ⊆ S, C|d ⊆ CON(d) ⊆ C
and F|d ⊆ FUN(d) ⊆ F.

It is not hard to see that under Assumptions 1, 2
and 6, Algorithms 4 work for learning Σ-grammars
with (k,C)-FCP Σ-grammars. All discussions in
Section 6.4 hold for this restricted case.

When learning substitutable languages, even a
weaker assumption suffices.

96



Assumption 7. There are sets S ⊆ S and C ⊆ C
such that for every nonterminal X of G ∈ G(Σ),
we have S(G, X) ∩ S ̸= ∅ and C(G,X) ∩C ̸= ∅.
Moreover, there are polynomial-time algorithms that
compute SUB(d), CON(d) and FUN(d) such that
S|d ⊆ SUB(d) ⊆ S, C|d ⊆ CON(d) ⊆ C and
F|d ⊆ FUN(d) ⊆ F.

Under Assumptions 1, 2 and 7, Algorithm 1
works using either SUBSTP or SUBSTD.

On the other hand, the results on the polynomial-
time MAT learnability of congruential and context-
deterministic Σ-grammars do not hold anymore un-
der any of Assumptions 5, 6 and 7.

8 Extending learnable classes

This section compares learnable classes of Σ-
languages for different Σ with the same special sort
O∗. For Σ1 and Σ2 with Σi = ⟨Ωi, Fi, O∗⟩, if
Ω1 ⊆ Ω2 and F1 ⊆ F2, every Σ1-grammar is a
Σ2-grammar, so L(Σ1) ⊆ L(Σ2). However, since
the distributional properties defined so far are rela-
tive to a signature, a Σ1-grammar with a distribu-
tional property under Σ1 does not necessarily have
the corresponding property under Σ2. Yet if SO and
CO are preserved by moving from Σ1 to Σ2, the dis-
tributional properties other than the substitutability
are preserved.

Let us define the direct union Σ0 = ⟨Ω0, F0, O∗⟩
of arbitrary signatures Σ1 and Σ2 by Ω0 = {O∗} ∪
{ (O, i) | O ∈ Ωi with i ∈ {1, 2} } where O(O,i) =
{ (s, i) | s ∈ O } and F0 = G1 ∪ G2 ∪ {□1, □2},
where Gi is a trivial variant of Fi working on the
new domain and codomain of the form (O, i) and
□i(s, i) = s for all s ∈ O∗. Then every Σi-grammar
G can be seen as a special type of Σ0-grammar by
adding a new initial symbol Z and rules of the form
Z ← □Σi⟨X⟩ for all initial symbols X of G. We
have L(Σ1) ∪ L(Σ2) ⊆ L(Σ0). Every Σi-grammar
that is congruential, context-deterministic, with the
k-FKP or with the k-FCP for i = 1, 2 can be seen as
a Σ0-grammar with those properties. Note that CO∗
is the singleton of the identity function in Σ0, which
means any element of L(G) is a 1-kernel of the new
initial symbol Z. In this way, from two signatures,
one can obtain a richer learnable class of languages.

The above argument on signature generalization
does not hold for substitutable case. Rather the op-

posite holds. If Ω1 ⊆ Ω2 and F1 ⊆ F2, then a lan-
guage substitutable under Σ2 is substitutable under
Σ1 but not vice versa.

Let us say that Σ2 is finer than Σ1 if every sort
of Ω1 is partitioned into finite number of sorts in Ω2

and every function of F2 is a subfunction of some
function in F1 which accords with the partition. That
is, every sort O of Ω1 has a finite set Ω2O ⊆ Ω2 such
that O =

∪
Ω2O and F1O0,...,On =

∪{F2O′0,...,O′n |
O′i ∈ Ω2Oi }. For instance, Σk,l is finer than Σk′,l′
for k′ ≤ k and l′ ≤ l in Example 4. If Σ2 is
finer than Σ1, L(Σ1) = L(Σ2) holds. Every lan-
guage substitutable under Σ1 is substitutable under
Σ2 but not vice versa. Moreover, every congruen-
tial (resp. context-deterministic) Σ1-grammar has an
equivalent congruential (resp. context-deterministic)
Σ2-grammar but not vice versa.

9 Grammars with partial functions

Yoshinaka (2015) showed that a dual approach can
be applied to the learning of conjunctive grammars.
Conjunctive grammars (Okhotin, 2001) are CFGs ex-
tended with the conjunctive operation & so that one
can extract the intersection of the languages of non-
terminals. For example, a conjunctive rule A0 →
A1&A2 means that if both A1 and A2 generate the
same string u then so does A0. Conjunctive gram-
mars cannot be seen as Σ-grammars, since the con-
junctive operation & is a partial function whose do-
main is not represented as the direct product of two
sorts, which is not legitimate in the general frame-
work of Σ-grammars.

A partial signature is a triple Π = ⟨Ω, F, O∗⟩
which is defined in the way similar to a (total) sig-
nature but F may have partial functions. Accord-
ingly contexts in C will be partial functions. We do
not have C(G,X) ⊙ S(G,X) ⊆ L(G) any more,
since c ⊙ s may not be defined for some elements
c ∈ C(G,X) and s ∈ S(G, X). The correspon-
dence between O-concept lattices and Σ-grammars
collapses. This prevents the application of the theory
of distributional learning developed in this paper to
Π-grammars. Still we can generalize the discussion
on the learning of conjunctive grammars.

Definition 8. A Π-grammar G is said to have the
strong k-FCP if for any X ∈ NO, there is a finite set

97



CX ⊆ CO with |CX | ≤ k such that

S(G, X) = { s | c⊙ s ∈ L for all c ∈ CX } .

Definition 8 requires every c ∈ CX to be to-
tal on S(G,X). One can learn Π-grammars with
the strong k-FCP under Assumptions 1, 2 and 6,
where C consists of total functions only. The gram-
mar construction DUALk should be modified so that
we have a rule [[C0]] ← f⟨[[C1]], . . . , [[Cn]]⟩ if c ⊙
f(s1, . . . , sn) ∈ L∗ for any c ∈ C0 and si ∈ C(K)i
such that f(s1, . . . , sn) is defined. One might think
that one can naturally define context-deterministic
grammars accordingly: Every c ∈ C(G, X) should
be a 1-context of X . However, this means that func-
tions in such a Π-grammar are essentially total.

Acknowledgments

The view presented in this paper has been sharpened
through the interactions and discussions with sev-
eral researchers with whom I worked on the distri-
butional learning of generalized CFGs. I would like
to show my deepest gratitude to Alexander Clark,
Makoto Kanazawa, Anna Kasprzik and Gregory Ko-
bele. Without those people this work would have
been hard to accomplish. Any insufficiency or er-
rors in this paper are of course of my own.

References

Alexander Clark and Rémi Eyraud. 2007. Polynomial
identification in the limit of substitutable context-free
languages. Journal of Machine Learning Research,
8:1725–1745.

Alexander Clark and Ryo Yoshinaka. 2014. Distribu-
tional learning of parallel multiple context-free gram-
mars. Machine Learning, 96(1-2):5–31.

Alexander Clark, Rémi Eyraud, and Amaury Habrard.
2009. A note on contextual binary feature grammars.
In EACL 2009 workshop on Computational Linguistic
Aspects of Grammatical Inference, pp. 33–40.

Alexander Clark. 2010a. Distributional learning of
some context-free languages with a minimally ade-
quate teacher. In J. Sempere and P. Garcı́a, editors,
ICGI, LNCS 6339, pp. 24–37. Springer.

Alexander Clark. 2010b. Learning context free gram-
mars with the syntactic concept lattice. In J. Sempere
and P. Garcı́a, editors, ICGI, LNCS 6339, pp. 38–51.
Springer.

Alexander Clark. 2010c. Towards general algorithms
for grammatical inference. In M. Hutter, F. Stephan,
V. Vovk, and T. Zeugmann, editors, ALT, LNCS 6331,
pp. 11–30. Springer.

Yuichi Kaji, Ryuichi Nakanishi, Hiroyuki Seki, and
Tadao Kasami. 1992. The universal recognition prob-
lems for parallel multiple context-free grammars and
for their subclasses. IEICE Transaction on Informa-
tion and Systems, E75-D(7):499–508.

Makoto Kanazawa and Ryo Yoshinaka. 2015. Distribu-
tional learning and context/substructure enumerability
in non-linear tree grammars. In Formal Grammar -
20th International Conference, FG 2015, Barcelona,
Spain, August 8-9, 2015. Proceedings. to appear.

Anna Kasprzik and Ryo Yoshinaka. 2011. Distribu-
tional learning of simple context-free tree grammars.
In J. Kivinen, C. Szepesvári, E. Ukkonen, and T.
Zeugmann, editors, ALT, LNCS 6925, pp. 398–412.
Springer.

Alexander Okhotin. 2001. Conjunctive grammars.
Journal of Automata, Languages and Combinatorics,
6(4):519–535.

Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free gram-
mars. Theoretical Computer Science, 88(2):191–229.

Hiromi Shirakawa and Takashi Yokomori. 1993.
Polynomial-time MAT learning of c-deterministic
context-free grammars. Transaction of Information
Processing Society of Japan, 34:380–390.

Ryo Yoshinaka and Makoto Kanazawa. 2011. Distribu-
tional learning of abstract categorial grammars. In S.
Pogodalla and J.-P. Prost, editors, LACL, LNCS 6736,
pp. 251–266. Springer.

Ryo Yoshinaka. 2008. Identification in the limit of
k, l-substitutable context-free languages. In A. Clark,
F. Coste, and L. Miclet, editors, ICGI, LNCS 5278,
pp. 266–279. Springer.

Ryo Yoshinaka. 2011a. Efficient learning of multiple
context-free languages with multidimensional substi-
tutability from positive data. Theoretical Computer
Science, 412(19):1821–1831.

Ryo Yoshinaka. 2011b. Towards dual approaches for
learning context-free grammars based on syntactic
concept lattices. In G. Mauri and A. Leporati, editors,
DLT, LNCS 6795, pp. 429–440. Springer.

Ryo Yoshinaka. 2012. Integration of the dual approaches
in the distributional learning of context-free grammars.
In A. H. Dediu and C. Martı́n-Vide, editors, LATA,
LNCS 7183, pp. 538–550. Springer.

Ryo Yoshinaka. 2015. Learning conjunctive grammars
and contextual binary feature grammars. In A. H.
Dediu, E. Formenti, C. Martı́n-Vide, and B. Truthe,
editors, LATA, LNCS 8977, pp. 623–635. Springer.

98


