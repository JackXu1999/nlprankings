



















































AmritaNLP at SemEval-2018 Task 10: Capturing discriminative attributes using convolution neural network over global vector representation.


Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 1003–1007
New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics

AmritaNLP at SemEval-2018 Task 10: Capturing discriminative
attributes using convolution neural network over global vector

representation.
Vivek Vinayan, Anand Kumar M, Soman K P

Center for Computational Engineering and Networking (CEN)
Amrita School of Engineering, Coimbatore

Amrita Vishwa Vidyapeetham, India
cb.en.p2cen16018@cb.students.amrita.edu,

m anandkumar@cb.amrita.edu

Abstract

The ”Capturing Discriminative Attributes”
sharedtask is the tenth task, conjoint with Se-
mEval2018. The task is to predict if a word
can capture distinguishing attributes of one
word from another. We use GloVe word em-
bedding, pre-trained on openly sourced cor-
pus for this task. A base representation is
initially established over varied dimensions.
These representations are evaluated based on
validation scores over two models, first on an
SVM based classifier and second on a one di-
mension CNN model. The scores are used to
further develop the representation with vector
combinations, by considering various distance
measures. These measures correspond to off-
set vectors which are concatenated as features,
mainly to improve upon the F1score, with the
best accuracy. The features are then further
tuned on the validation scores, to achieve high-
est F1score. Our evaluation narrowed down to
two representations, classified on CNN mod-
els, having a total dimension length of 1204 &
1203 for the final submissions. Of the two, the
latter feature representation delivered our best
F1score of 0.658024 (as per result1.)

1 Introduction

As famously quoted by firth ”You shall know a
word by the company it keeps” that is, the se-
mantic information embedded in a representation
can only be described by the words surrounding it.
This can only get you somewhere when, company
itself is unambiguous and a representation goes
through capturing ”hypothetically” every sense of
the word over a corpus. The capturing discrim-
inative attributes sharedtask, conducted with Se-
mEval(2018) is a task proposed by alicia kerbs
and denis paperno (2016). It describes, how lexi-
cal similarity may not be enough to access qualita-
tively, the semantic information for a multitude of
tasks. Wherein they propose that, with this task, a

1Results/Evaluation under the team name ”AmritaNLP”

system can be modelled for effectively extracting
certain semantic differences in the words for un-
derstanding the sense embedded within them. This
is provided as a proof of concept dataset for this
sharedtask, where a certain word is used to check
if it can distinguish between a pair of words. The
dataset in itself seems simple where, in the training
set a label information for the two classes, positive
or negative are provided making this a binary clas-
sification task.

The three words that are provided in each instance
are given in the order as, a pivot word followed
by a compare word and ending with a attribute or
feature word, that may or may not be associated
with the pivot word. Based on the last word it
is decided, if that attribute word actually is a dis-
tinguishing feature that is able to discriminate the
pivot word from that of the compare word. e.g (ap-
ple,banana,red) here apple is the pivot word, ba-
nana the compare word and red, the word which
decides if this is a feature that can be associate
with apple to distinguish it from banana. This is a
rather oversimplified example to a human, as from
a very young age we are taught to distinguish ob-
jects based on visual aid, which simplifies the task
for us as we have embedded subconsciously to dif-
ferentiate the fruits mainly based on their color
or size. This information is seldom used to de-
scribe the fruits when illustrated in written form,
thus lacking that visual form of information for
a machine to make this judgment call, making it
that much more difficult to take an informed deci-
sion. Their work is based on a method, that was
presented by Lazaridou et al. (2016) for predic-
tion of distinguishing feature with use of image as
reference for visual discrimination attribute iden-
tification task, more prominently it was related to
capturing of lexical information using offset vec-
tors.

1003



Figure 1: SVM architecture for feature representation.

2 Dataset

The dataset in the sharedtask2018 (Krebs and Pa-
perno, 2018) is divided into three sets namely train
test and validation. The training set contains auto-
matically generated examples which are not man-
ually curated. Whereas, the test and validation set
are manually verified examples which include just
over 5000 instances. The test set instances are
made keeping in consideration that feature word
overlap between the words in train and test are
minimal. The validation set is similar to that of
the test set and is used for parameter tuning of the
models.

There are in total 17782 instances in the train-
ing set, 2722 in the validation set and 2340 in the
test set. With the automated nature of the data, the
training set is noisier in comparison to that of the
validation and test set.
In the dataset, positive examples are anno-
tated with the label ’1’, signifying that the at-
tribute/feature word is a positive association only
to the pivot word in the order presented and
not vice verse. e.g. (airplane,helicopter,wings)
here ’wings’ is an attribute only associated to
’airplane’, whereas (helicopter,airplane,wings) is
an invalid entry. The combination of (heli-
copter,airplane) in this order will only be added if
the concept ’helicopter’ has a feature that airplane
does not have in this set.
On the other hand, the negative examples are an-
notated with label ’0’ at the end. These are con-
sidered when the attribute/feature words are either
similar to both pivot and the compare word or are
dissimilar to them, e.g. (Tractor,scooter,wheels),
(Spider,elephant,legs) e.t.c.

In the training dataset, there is a total of 508
unique concepts (pivot) words, of which 375
words have positive attributes and 505 of these
have negative attributes, seeing the big contrast be-
tween the two labeled attributes we can infer that

not every concept word has an equal proportion of
labeled instances.

Wp Pivot word
Wc Compare word
Wa Attribute word

Cos p cosine similarity (WpWa)
Cos c cosine similarity (WcWa)
Dis p kulsinski(WpWa)
Dis c kulsinski(WcWa)
Min p minkowski(WpWap=1)
Min c minkowski(WcWap=1)

Coref p corrcoef(WpWa)
Coref c corrcoef(WcWa)

Sqeu sqeuclidean(W,Wa)

Table 1: Nomenclature for feature representation.

Figure 2: Convolution neural network architecture for
feature representation.

3 Methodology

Here discussed are methods which are considered
in our implementation. On a cursory look at the
dataset, we decided to go with a pre-trained rep-
resentation of the words, rather than preparing a
word embedded representation of the dataset. This
is devised with a notion that, word pair associated
models on this dataset would not help educate the

1004



SN
GloVe

Pre-trained
word Vectors

Representation
Representation

Length

Validation

Accuracy F1score

1

6B 50d

Wp,Wc,Wa 150 51.16 42.20
2 Wp,Wa,Wc,Wa 200 51.08 42.10

3
Wp,Wa,Wc,Wa,

Cos p
201 51.48 51.10

4
Wp,Wa,Wc,Wa,

Cos p, Dis p
202 51.37 50.90

5 CR 1 200 50.28 41.90
6 CR 2,Cos 202 47.59 46.00
7 CR 2,Cos,Min c,Min p 204 49.61 46.80
8

6B 300d
Wp,Wc,Wa 900 52.04 45.80

9 Wp,Wa,Wc,Wa 1200 51.64 45.57
10

840B 300d
Wp,Wc,Wa 900 51.10 41.00

11 Wp,Wa,Wc,Wa 1200 51.41 40.34

Table 2: Validation accuracy for varied dimension GloVe representation using SVM.

SN
GloVe

Pre-trained
word Vectors

Representation
Representation

Length

Validation

Accuracy F1score

1
6B 300d

Wp,Wc,Wa 900 51.3 41.1
2 Wp,Wa,Wc,Wa 1200 50.9 43.8
3 CR 1 1200 50.8 42.2
4

840B 300d
Wp,Wc,Wa 900 52.0 51.0

5 Wp,Wa,Wc,Wa 1200 52.0 45.3
6 CR 1 1200 53.8 48.4

Table 3: Validation accuracy for varied dimension GloVe representation using CNN.

embedding. Further, using the pre-trained embed-
dings, the representation are evaluated based on
validation accuracy with machine learning tech-
niques like SVM, where we use ten fold ten cross
with linear kernel for validation. This algorithm
was earlier explored for sense disambiguation of
a native language (Tamil), having rich feature rep-
resentation presented in his work by Anand Ku-
mar et al. (2014a), and is also implemented in
his work (2014b). A simple one dimension con-
volution neural networks model is also illustrated
upon, based on the works by Vinayakumar et al.
(2017). The CNN model is fixed on an empiri-
cal method where the representation is convoluted
with twenty filters, of size three, on a batch size
of sixty-four, with activation ReLU over a way-
ward ten epochs, which are flattened and reduced
to thirty-two and later to one at the final layer for
evaluation. The architectures for the models, are
as shown in Figure 1 & 2 respectively.
Moving ahead, a GloVe pre-trained word embed-

ding (Pennington et al., 2014) of various dimen-
sions are considered, which is learned over pub-
lic data, available under the PDDL.2 (100, 300 di-
mension word representation, embedded over 6B,
840B sizes common crawl corpus are considered).
The focus is on using one of these representations
for our base method. Upon these embedding, vari-
ous distance, dissimilarity and similarity measures
are considered, to provide a measure between vec-
tors or in our case between the words.

In Table 1, provided are abbreviations that we
used through out the upcoming discussion regard-
ing the methods and the representations. With
the implementation of pre-trained vectors, we re-
fer few vector measurement technique that could
be used to measure a sense of semantic similar-
ity among them. These vector carry within them
a spacial correlation between words which has
be discussed in their work by (Pennington et al.,

2 Public Domain Dedication and License v1.0.
http://www.opendatacommons.org/licenses/pddl/1.0/.

1005



SN Conditional representation (CR)
If : Else :

1 Wp,(Wp + Wa),Wc,Wa Wp,(Wp – Wa),Wc,(Wc – Wa)
2 Wp,(Wp+ Wa),Wc,Wa,(Dis c – Dis p) Wp,(Wp– Wa),Wc,(Wc – Wa),(Dis c – Dis p)

Table 4: Various feature representation taken for the classification task.

SN
GloVe

Pre-trained
word Vectors

Representation
Representation

length
Validation

Accuracy F1score
1

840B 300d

CR 1, Dis 1201 53.1 47.3
2 CR 1, Cos 1201 50.3 50.1
3 CR 1, Dis, Cos 1202 55.1 53.1
4 CR 1, Min, Dis p, Dis c 1203 50.9 51.6
5 CR 1, (Coref p – Coref c), Min c, Min p 1203 51.1 56.8
6 CR 1, Cos, Min c, Min p 1203 54.6 58.9
7 CR 2 1201 51.3 45.6
8 CR 2, (Coref p – Coref c), Min c, Min p 1204 55.7 56.4
9 CR 2, (Min c – Min p) 1202 61.5 55.8
10 CR 2, Cos, Min c, Min p 1204 60.1 56.1
11 CR 2, Min c, Min p 1203 61.1 60.2
12 CR 2, (Cos p – Cos c), Min c, Min p 1204 58.9 51.3
13 CR 2, Cos, Min c, Min p, Sqeu 1205 54.4 54.9

Table 5: Validation accuracy of 300 dimension, GloVe representations on 840B common crawl tokens using CNN.

2014).
Initially, a simple concatenation of the three

words is considered as an instance, which are the
pivot(Wp), compare(Wc) and attribute(Wa) words,
for the entire dataset. The same representation is
taken of two different dimensions lengths as men-
tioned earlier. Based on the model fit across train-
ing data, the validation accuracy and F1score are
measured, these are as shown in Table 2. Simi-
larly, these representations are also passed on to a
convolution neural network, where their respective
accuracy and F1scores are measured and shown in
Table 3.
With an empirical approach, the representations
are further extended by appending (Wa) to (Wp)
and (Wc) sequentially and passing it to the two
models(As shown by representation two in the Ta-
ble 2). The SVM model did not show any sig-
nificant improvement in the score, over the rep-
resentations. In comparison, the CNN model ob-
served a slight improvement in scores on the same
representation. Word embedding being a vec-
tor representation in higher dimensional space,
has proved (Pennington et al., 2014) to captures
spatial information, that can be employed to use
as features for the representation. This is ex-
erted by using certain measures between the (Wp),
(Wa) and (Wc), (Wa). These measures are cal-

culated using Scipy libraries (Jones et al., 2001)
and Sklearn library (Pedregosa et al., 2011) to find
the distance, similarity and dissimilarity measure
between the two 1-D array words. The similar-
ity of the two words indicates how similarly as-
sociated these words are, this measure is calcu-
lated using the cosine distance which is a scalar
representation that signifies, larger the number be-
tween the two words the more similar they are.
Whereas, the dissimilarity is the vice-versa of this
measure. Of the various distance measures ex-
plored, we considered euclidean, chebyshev, sqeu-
clidean, minkowski and for dissimilarity measures
jaccard, kulsinski, Hamming and these are imple-
mented using the Scipy (Jones et al., 2001) li-
brary. Amongst the measures considered, kulsin-
ski dissimilarity gave the nearest disambiguation
between the comparison of Wp, Wa and Wc, Wa,
thus we chose it as the threshold measured for dif-
ferentiating the representations between a positive
and a negative instances i.e if the dissimilarity of
Wc, Wa is greater than that of the Wp, Wa then
the Wa were added to the WP and concatenated
to form a representation. Otherwise the second
representation is considered where the Wa is sub-
tracted from both the words. This is as shown in
the first conditional representation (CR) of the Ta-
ble 4.

1006



The CR based representation accuracy de-
creased for SVM models. Whereas, the F1score
and accuracy increased for the CNN model over
the initial representations shown in Table 3. Thus,
the further representation where improved on the
CNN model to achieve better F1score with good
accuracy. Comparing the two GloVe pre-trained
vectors of 300 dimension for varied corpus size
shown in Table 3, the 840B 300d trained model
has achieved better F1score and accuracy com-
pared to the other, thus moving along further with
word embedding.

In Table 5 we see that subsequent represen-
tations, built upon the simple representation of
CR1 are concatenated with kulsinski(Dis3) dis-
tance and Cosine similarity (Cos3) have improved
the F1score. As show in the third representa-
tion, where the F1score increased to 53.1% with a
considerable accuracy over the previous iteration.
Further improvisation on CR1 representation with
different features like correlation coefficient have
increased the F1score to 56.8% but brought down
the accuracy. Representation six is the next fea-
ture representation for which the accuracy, as well
as the F1score, increases to 54.6% and 58.9% re-
spectively. After many iterations of adding fea-
tures, the representation eleven is the one that gave
the highest F1score with the best accuracy, and
this representation based model is submitted along
with the representation ten, which also had good
F1score, but a lower accuracy on the validation
dataset.

4 Results & Conclusion

The tenth and eleventh representation of Table 5
are the two feature set based on CNN models,
which are predicted on the test set and submit-
ted for the competition. The results published for
our models showed that the first set was scored at
0.52, where as the second set was scored at 0.66
for F1score. Comparing the predicted labels of
the two systems with that of the gold standard, we
see that our system fit over the tenth representation
predicted correctly only 399 of 1293 as negative
example and 855 of 1047 as the positive example.
On the eleventh representation it gave 857 of 1293
and 687 of 1047 for negative and positive example
respectively. Comparing the outcomes of the sys-

3These representations are same as earlier mentioned in
Table 1 wherein here the pivot word based measure is taken
for the ’if’ condition and the compare word based on the other

tems we see that majority of negative sample are
mis-classified for system ten, on the other hand,
the eleventh system improved upon this classifica-
tion of the negative samples which increased the
F1score for the system.

References
M Anand Kumar, S Rajendran, and KP Soman. 2014a.

Tamil word sense disambiguation using support vec-
tor machines with rich features. International Jour-
nal of Applied Engineering Research, 9(20):7609–
20.

M Anand Kumar and KP Soman. 2014b. Amrita-cen@
fire-2014: Morpheme extraction and lemmatization
for tamil using machine learning. In ACM Interna-
tional Conference Proceeding Series, pages 112–20.

Eric Jones, Travis Oliphant, Pearu Peterson, et al. 2001.
SciPy: Open source scientific tools for Python.

Alicia Krebs and Denis Paperno. 2016. Capturing dis-
criminative attributes in a distributional space: Task
proposal. In Proceedings of the 1st Workshop on
Evaluating Vector-Space Representations for NLP,
pages 51–54.

Alicia Krebs and Denis Paperno. 2018. Semeval-2018
Task 10: Capturing discriminative attributes. In
Proceedings of International Workshop on Semantic
Evaluation (SemEval-2018).

Angeliki Lazaridou, Nghia The Pham, and Marco Ba-
roni. 2016. The red one!: On learning to refer
to things based on their discriminative properties.
arXiv preprint arXiv:1603.02618.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in python. Journal of machine
learning research, 12(Oct):2825–2830.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

R. Vinayakumar, K. P. Soman, and Prabaharan Poor-
nachandran. 2017. Applying convolutional neural
network for network intrusion detection. 2017 In-
ternational Conference on Advances in Computing,
Communications and Informatics (ICACCI), pages
1222–1228.

1007


