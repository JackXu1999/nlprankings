



















































Research on attention memory networks as a model for learning natural language inference


Proceedings of the Workshop on Structured Prediction for NLP, pages 18–24,
Austin, TX, November 5, 2016. c©2016 Association for Computational Linguistics

Research on attention memory networks as a model for learning natural
language inference

Zhuang Liu and Degen Huang and Jing Zhang and Kaiyu Huang
School of Computer Science and Technology

Dalian University of Technology, Dalian, P.R. China
huangdg@dlut.edu.cn

{zhuangliu,zhangjingqf}@mail.dlut.edu.cn
huangkaiyucs@foxmail.com

Abstract
Natural Language Inference (NLI) is a fun-
damentally important task in natural language
processing that has many applications. It is
concerned with classifying the logical rela-
tion between two sentences. In this paper, we
propose attention memory networks (AMNs)
to recognize entailment and contradiction be-
tween two sentences. In our model, an atten-
tion memory neural network (AMNN) has a
variable sized encoding memory and support-
s semantic compositionality. AMNN captures
sentence level semantics and reasons relation
between the sentence pairs; then we use a S-
parsemax layer over the output of the gener-
ated matching vectors (sentences) for classifi-
cation. Our experiments on the Stanford Nat-
ural Language Inference (SNLI) Corpus show
that our model outperforms the state of the art,
achieving an accuracy of 87.4% on the test da-
ta.

1 Introduction

Natural Language Inference (NLI) refers to the
problem of determining entailment and contradic-
tion relationships between two sentences. The chal-
lenge in Natural Language Inference, also known
as Recognizing Textual Entailment (RTE), is to cor-
rectly decide whether a sentence (called a hypoth-
esis) entails or contradicts or is neutral in respect
to another sentence (referred to as a premise). Pro-
vided with a premise sentence, the task is to judge
whether the hypothesis can be inferred (Entailment)
or the hypothesis cannot be true (Contradiction) or
the truth is unknown (Neutral). Few examples are
illustrated in Table 1.

NLI is the core of natural language understand-
ing and has wide applications in NLP, e.g., automat-
ic text summarization (Yan et al., 2011a; RuiYan et
al., 2011b); and question answering (Harabagiu and
Hickl, 2006). Moreover, NLI is also related to oth-
er tasks of sentence pair modeling, including rela-
tion recognition of discourse units (YangLiu et al.,
2016), paraphrase detection (Hu et al., 2014), etc.

Bowman released the Stanford Natural Language
Inference (SNLI) corpus for the purpose of encour-
aging more learning centered approaches to NLI
(Bowman et al., 2015). Published SNLI corpus
makes it possible to use deep learning methods to
solve NLI problems. So far proposed work based
on neural networks for text similarity tasks includ-
ing NLI have been published in recent years (Hu
et al., 2014; Wang and Jiang, 2015; Rocktaschel
et al., 2016; Yin et al., 2016);. The core of these
models is to build deep sentence encoding model-
s, for example, with convolutional networks (LeCun
et al., 1990) or long short-term memory networks
(Hochreiter and Schmidhuber, 1997) with the goal
of deeper semantic encoders. Recurrent neural net-
works (RNNs) equipped with internal short mem-
ories, such as long short-term memories (LSTMs)
have achieved a notable success in sentence encod-
ing. LSTMs are powerful because it learns to control
its short term memories. However, the short term
memories in LSTMs are a part of the training pa-
rameters. This imposes some practical difficulties in
training and modeling long sequences with LSTMs.

In this paper, we proposed a deep learning frame-
work for natural language inference, which mainly
consists of two layers. As we can see from the fig-

18



Premise Hypothesis Label
A person throwing a yellow ball in the air. The ball sails through the air. Entailment
A person throwing a yellow ball in the air. The person throws a square. Contradiction
A person throwing a yellow ball in the air. The ball is heavy. Neutral

Table 1: Three NLI examples from SNLI. Relations between a Premise and a Hypothesis: Entailment, Contradiction, and Neutral
(irrelevant).

ure 1, from top to bottom are: (A) The sentence en-
coding layer (Figure 1a); (B) The sentence matching
layer (Figure 1b). In the sentence encoding layer,
we introduce an attention memory neural network
(AMNN), which has a variable sized encoding mem-
ory and naturally supports semantic compositionali-
ty. The encoding memory evolves over time, whose
size can be altered depending on the length of in-
put sequences. In the sentence matching layer, we
directly model the relation between two sentences
to extract relations between premise and hypothesis,
and dont generate sentence representations. In addi-
tion, we introduce the Sparsemax (Yin and Schutze,
2015) , a new activation function similar to the tra-
ditional Softmax, but is able to output sparse prob-
ability distributions; then, we present a new smooth
and convex loss function, Sparsemax loss function,
which is the Sparsemax analogue of the logistic loss.
We will explain the two layers in detail in the follow-
ing subsection.

Figure 1: High-level architectures of attention memory neural
networks. (a) The sentence encoding layer: Individual sentence

modeling via AMNN. (b) The sentence matching layer: Sen-

tence pair modeling, after which a Sparsemax layer is applied

for output.

2 Proposed Approach

In our model, we adopt a two-step strategy to clas-
sify the relation between two sentences. Concretely,

our model comprises two parts:

• The sentence encoding layer (Figure 1a). This
part is mainly a sentence semantic encoder,
aiming to capture general semantics of sen-
tences.

• The sentence matching layer (Figure 1b). This
part mainly introduces how vector representa-
tions are combined to capture the relation be-
tween the premise and hypothesis for classifi-
cation.

2.1 The sentence encoding layer: AMNN

In this layer, we introduce an attention memory neu-
ral network (AMNN), which implements an atten-
tion controller and a variable sized encoding mem-
ory, and naturally supports semantic compositional-
ity. AMNN has four main components: Input, Out-
put and Attention memory modules, and an encod-
ing memory. We then examine each module in detail
and give intuitions about its formulations.

Suppose we are given an set {Xi, Y i}Ni=1, where
the input Xi is a sequence wi1, w

i
2, ..., w

i
Ti

of token-
s, and Y i can be an output sequence. The encoding
memory M ∈ Sd×l has a variable number of slot-
s, where d is the embedding dimension and l is the
length of the input sequence. Each memory slot vec-
tor mt ∈ Sd corresponds to the vector representation
of wt. In particular, the memory is initialized with
the raw embedding vector at time t=0. As Attention
memory module reads more input content in time,
the initial memory evolves over time and refines the
encoded sequence.

Input module reads an embedding vector. Atten-
tion memory module looks for the slots related to
the input by computing semantic similarity between
each memory slot and the hidden state. We calculate
the similarity by the dot product and transform the
similarity scores to the fuzzy key vector by normal-
izing with Softmax function. Since our key vector is

19



fuzzy, the slot to be composed is retrieved by taking
weighted sum of the all slots. In this process, our
memory is analogous to the soft attention mechanis-
m. We compose the retrieved slot with the current
hidden state and map the resulting vector to the en-
coder output space. Finally, we write the new repre-
sentation to the memory location pointed by the key
vector.

In our recurrent network, we use a gated recurrent
network (Cho et al., 2014a; Chung et al., 2014). We
also explored the more complex LSTM (Hochreit-
er and Schmidhuber, 1997) but it performed simi-
larly and is more computationally expensive. Both
work much better than the standard tanh RNN and
we postulate that the main strength comes from
having gates that allow the model to suffer less
from the vanishing gradient problem (Hochreiter
and Schmidhuber, 1997).

Concretely, let vl ∈ Rl and vd ∈ Rd be vectors,
and given a input function fGRUinput , a output function
fGRUoutput, and the key vector output at, the output state
ht and the encoding memory Mt in time step t as

ot = fGRUinput(xt) (1)

at = Softmax(oTt Mt−1) (2)

mt = aTt Mt−1 (3)

ht = fGRUoutput(ot, mt) (4)

Mt = Mt−1(1− (at ⊗ vd)T )
+(ht ⊗ vl)(at ⊗ vd)T (5)

where the input function fGRUinput and the output
function fGRUoutput are neural networks, also are the
training parameters in the model. We abbreviate the
above computation with Mt = GRU(xt, Mt−1). E-
quation (1) is a matrix of ones, ⊗ denotes the outer
product which duplicates its left vector l or d times
to form a matrix. The function fGRUinput sequentially
maps the word embeddings to the internal space of
the memory wt−1. Then Equation (2),(3),(4),and (5)
retrieves a memory slot mt that is semantically asso-
ciated with the current input word wt, and combines
the slot mt with the input wt, and then transforms

the composition vector to the encoding memory and
rewrites the resulting new representation into the s-
lot location of the memory space. The slot location
(ranging from 1 to d) is defined by a key vector at
which the Input module emits by attending over the
memory slots. In GRU (xt, Mt−1), the slot that was
retrieved is erased and then the new representation is
located. Attention memory module performs this it-
erative process until all words in the input sequence
is read, and performs the input and output opera-
tions in every time step. The encoding memories
{M}Tt=1 and output states {h}Tt=1 are further used
for the tasks.

2.2 The sentence matching layer

Combining sentences encoding: In this part, we
introduce how vector representations of individual
sentences are combined to capture the relation be-
tween the premise and hypothesis. Three matching
methods were applied to extract relations.

• Concatenation of the two representations

• Element-wise product

• Element-wise difference

This matching architecture was first used by (Mou
et al., 2015) The first matching method follows the
most standard procedure of the Siamese architec-
tures, while the latter two are certain measures of
similarity or closeness. This matching process is fur-
ther concatenated (Figure 1b), given by

Vc = [(VpVh; Vp − Vh; Vp � Vh] (6)

where Vp and Vh are the sentence vectors of the
premise and hypothesis, respectively; � denotes
element-wise product; semicolons refer to column
vector concatenation. Vc is the generated matching
vector of the matching layer.

We would like to point out that, with subsequen-
t linear transformation, element-wise difference is a
special case of concatenation. If we assume the sub-
sequent transformation takes the form of W [VpVh]T ,
where W=[W1W2] is the weights for concatenated
sentence representations, then element-wise differ-
ence can be viewed as such that W0(Vp − Vh) W0

20



is the weights corresponding to element-wise differ-
ence). Thus, our third heuristic can be absorbed in-
to the first one in terms of model capacity. How-
ever, as will be shown in the experiment, explic-
itly specifying this heuristic significantly improves
the performance, indicating that optimization dif-
fers, despite the same model capacity. Moreover,
word embedding studies show that linear offset of
vectors can capture relationships between two words
(Mikolov et al., 2013b), but it has not been exploit-
ed in sentence-pair relation recognition. Although
element-wise distance is used to detect paraphrase in
(He et al., 2015), it mainly reflects similarity infor-
mation. Our study verifies that vector offset is useful
in capturing generic sentence relationships, akin to
the word analogy task.
Sparsemax Transformation: In this part, we in-
troduce the Sparsemax transformation, which has
similar properties to the traditional Softmax, but
is able to output sparse probability distribution-
s. This transformation was first used by Andre
(Martins and Astudillo, 2016). Let 4K−1 :=
p ∈ RK |1T p = 1, p ≥ 0 be the (K-1)-dimensional
simplex. We are interested in functions that map
vectors in RK to probability distributions in4K−1.
Such functions are useful for converting a vector of
real weights (e.g., label scores) to a probability dis-
tribution (e.g. posterior probabilities of labels). The
Sparsemax function, defined componentwise as:

Sparsemax(z) := argmax
p∈4K−1

‖p− z‖2 (7)

Sparsemax has the distinctive feature that it can re-
turn sparse posterior distributions, that is, it may as-
sign exactly zero probability to some of its output
variables. This property makes it appealing to be
used as a filter for large output spaces, to predict
multiple labels, or as a component to identify which
of a group of variables are potentially relevant for a
decision, making the model more interpretable. Cru-
cially, this is done while preserving most of the at-
tractive properties of Softmax: we show that Sparse-
max is also simple to evaluate, it is even cheaper to
differentiate, and that it can be turned into a convex
loss function.

We present the Sparsemax loss, a new loss func-
tion that is the Sparsemax analogue of logistic re-
gression. We show that it is convex, everywhere d-

ifferentiable, and can be regarded as a multi-class
generalization of the Huber classification loss, an
important tool in robust statistics (Zhang and Tong,
2004). We apply the Sparsemax loss to train multi-
label linear classifiers. Finally, we use a Sparsemax
layer over the output of a non-linear projection of
the generated matching vector for classification.

3 Experiments

3.1 Dataset

To evaluate the performance of our model, we
conducted our experiments on Stanford Natu-
ral Language Inference (SNLI) corpus (Bowman
et al., 2015). The dataset, which consists of
549,367/9,842/9,824 premise-hypothesis pairs for
train/dev/test sets and target label indicating their re-
lation. Each pair consists of a premise and a hy-
pothesis, manually labeled with one the labels EN-
TAILMENT, CONTRADICTION, or NEUTRAL.
We used the provided training, development, and
test splits.

3.2 Hyper-Parameter Settings

In this section, we provide details about training the
neural network. The model is implemented using
open-source framework the TensorFlow (Abadi et
al., 2015). The training objective of our model is
cross-entropy loss, and we use mini-batch stochastic
gradient descent (SGD) with the Rmsprop (Hinton,
2012) for optimization. We set the batch size to 128,
the initial learning rate to 3e-4 and l2 regularizer
strength to 3e-5, and train each model for 60 epochs,
and fix dropout rate at 0.3 for all dropout layers. In
our neural layers, we used pretrained 300D Glove
840B vectors (Pennington et al., 2014) to initialize
the word embedding. Out-of-vocabulary words in
the training set are randomly initialized by sampling
values uniformly from (0.02, 0.02). All of these
embedding are not updated during training. Each
hyper-parameter setting was run on a single machine
with 10 asynchronous gradient-update threads, us-
ing Adagrad (Duchi et al., 2011) for optimization.

3.3 Results and Qualitative Analysis

Table 1 compares the results of our models with the
previous art-of-the-state baseline results. We com-
pare our models against several baselines, including

21



Method Train acc. (%) Test acc. (%) Params.
Previous non-neural network results

Lexicalized Classifier(Bowman et al., 2015) 99.7 78.2 -
Previous neural network results

LSTM LSTM RNN encoders(Bowman et al., 2016) 83.9 80.6 3.0M
Tree-based CNN encoders (Mou et al., 2015) 83.3 82.1 3.5M
SPINN-NP encoders (Bowman et al., 2016) 89.2 83.2 3.7M

LSTM with attention (Rocktaschel et al., 2016) 85.3 83.5 252K
mLSTM (Wang and Jiang, 2015) 92.0 86.1 1.9M

LSTMN with deep attention fusion (Cheng et al., 2016) 88.5 86.3 3.4M
Decomposable attention model (Parikh et al., 2016) 90.5 86.8 582K

Our results
AMNs-G (AMNN with GRU) 89.1 87.4 3.5M
AMNs-L (AMNN with LSTM) 89.3 87.0 3.2M

Table 2: Train/test accuracies on the SNLI dataset and the approximate number of trained parameters (excluding embeddings) for
each approach. “-G“ and “-L“ denote GRU and LSTM, resp.

the strongest published non-neural network-based
result from Bowman et al. (2015) and previous neu-
ral network models built around several types of sen-
tence encoders. Here AMNs-G, AMNs-L denote
the neural networks of AMNN GRU RNN and LST-
M RNN, respectively. When we experimented with
the AMNN model instead of some previous model-
s, Tree-based CNN by (Mou et al., 2015), SPINN-
NP by Bowman et al. 2016, LSTM with attention
by Rocktaschel et al. 2016, as initial sentence rep-
resentations of the premise and the hypothesis. As
seen, both AMNs-G and AMNs-L further slightly
improved the result. Our models are able to outper-
form the previous state-of-the-art in terms of the ac-
curacy at test time, by approximately 0.6%.

4 Related Work

Language inference or entailment recognition can be
viewed as a task of sentence pair modeling. Most
neural networks in this field involve a sentence-level
model, followed by one or a few matching modules.
Our method is motivated by the central role played
by sentence-level modeling (Yin and Schutze, 2015;
Mou et al., 2016; Wan et al., 2015; Parikh et al.,
2016; YangLiu et al., 2016; Rocktaschel et al.,
2016) and previous approaches to semantic encoder
(Graves et al., 2014; Weston et al., 2015; Sukhbaatar
et al., 2015; Kumar et al., 2016; Bahdanau et al.,
2015). (Yin and Schutze, 2015) and (Mou et al.,
2016) apply convolutional neural networks (CNNs)
as the individual sentence model, where a set of fea-
ture detectors over successive words are designed to

extract local features. (Wan et al., 2015) and (Yan-
gLiu et al., 2016) build sentence pair models upon
recurrent neural networks (RNNs) to iteratively in-
tegrate information along a sentence.

The neural counterpart to sentence similarity
modeling, attention and external memory, which are
the key part of our approach, was originally pro-
posed and has been predominantly used to attemp-
t to extend deep neural networks with an external
memory (NTM) (Graves et al., 2014). NTM im-
plements a centralized controller and a fixed-sized
random access memory. The controller uses atten-
tion mechanisms to access the memory. The work
of (Sukhbaatar et al., 2015) combines the soft atten-
tion with Memory Networks (MemNNs) (Graves et
al., 2014). Although MemNNs are designed with
non-writable memories, it constructed layered mem-
ory representations and showed promising results on
both artificial and real question answering tasks. An-
other variation of MemNNs is Dynamic Memory
Network (DMN) (Kumar et al., 2016) which is e-
quipped with an episodic memory and seems to be
flexible in different settings.

In contrast, our use of external memory is based
on variable sized semantic encoder and our method
use the attention mechanism to access the external
memory. The size of the memory can be altered
depending on the input length, i.e., we use a larger
memory for long sequences and a smaller memory
for short sequences. Our models are suitable for N-
LI and can be trained easily by any gradient descent
optimizer.

22



5 Conclusion and future work

In this paper, we proposed attention memory net-
works (AMNs) to solve the natural language infer-
ence (NLI) problem. Firstly, we present the atten-
tion memory neural network (AMNN) that uses at-
tention mechanism and has a variable sized seman-
tic memory. AMNN captures sentence-level seman-
tics; then we directly model the relation with com-
bining two sentence vectors to aggregate informa-
tion between premise and hypothesis. Finally, we
introduce the Sparsemax, a new activation function
similar to the traditional Softmax, but is able to out-
put sparse probability distributions. We use the S-
parsemax layer over the generated matching vector
for output. The attention memory networks (AMN-
s) over the premise provides further improvements
to the predictive abilities of the system, resulting in
a new state-of-the-art accuracy for natural language
inference on the Stanford Natural Language Infer-
ence corpus.

Our model can be easily adapted to other
sentence-matching models. There are several di-
rections for our future work: (1) Employ this ar-
chitecture on other sentence matching tasks such
as Text Summarization, Paraphrase Text Similarity
and Question Answer etc. (2) Try more heuristics
matching methods to make full use of the individual
sentence vectors. (3) Extend AMNN to produce en-
coding memory and representation vector of entire
documents.

Acknowledgments

We thank Kai-Wei Chang, Ming-Wei Chang,
Alexander Rush, Vivek Srikumar, and the anony-
mous EMNLP reviewers for their helpful comments
on drafts of this paper. This research is support-
ed by National Natural Science Foundation of Chi-
na(No.61672127No. 61173100).

References
M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,

C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin,
S. Ghemawat, I. Goodfellow, A. Harp, G. Irving,
M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kud-
lur, J. Levenberg, D. Mane, R. Monga, S. Moore,
D. Murray, C. Olah, M. Schuster, J. Shlens, B. Stein-
er, I. Sutskever, K. Talwar, P. Tucker, V. Vanhouck-

e, V. Vasudevan, O. Vinyals F. Viegas, P. Warden,
M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. 2015.
Tensorflow: Large-scale machine learning on hetero-
geneous systems. Software available from tensor-
flow.org.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Samuel R. Bowman, Gabor Angeli, Christopher Potts, ,
and Christopher D. Manning. 2015. A large annotat-
ed corpus for learning natural language inference. In
Proceedings of EMNLP.

Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi,
Raghav Gupta, Christopher D. Manning, and Christo-
pher Potts. 2016. A fast unified model for parsing and
sentence understanding. In Proceedings of ACL.

KyungHyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, , and Yoshua Bengio. 2014a. On the prop-
erties of neural machine translation: Encoder-decoder
approaches. CoRR, abs/1409.1259.

Junyoung Chung, Calar Gulcehre, Kyunghyun Cho, and
Yoshua Bengio. 2014. Empirical evaluation of gat-
ed recurrent neural networks on sequence modeling.
Technical Report Arxiv report 1412.3555.

John Duchi, Elad Hazan, , and Yoram Singer. 2011.
Adaptive subgradient methods for online learning and
stochastic optimization. The Journal of Machine
Learning Research, (12):2121–2159.

Alex Graves, Greg Wayne, and Ivo Danihelka. 2014.
Neural turing machines. arXiv preprint arX-
iv:1410.5401.

Sanda Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain question
answering. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
Annual Meeting of the Association for Computational
Linguistics, pages 905–912.

Hua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi-
perspective sentence similarity modeling with convo-
lutional neural networks. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 17–21.

G. Hinton. 2012. Lecture 6.5: rmsprop: divide the gra-
dient by a running average of its recent magnitude.
coursera: Neural networks for machine learning.

Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9(8):1735–
1780.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen.
2014. Convolutional neural network architectures for
matching natural language sentences. In Advances in
Neural Information Processing Systems, pages 2042–
2050.

23



Ankit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury,
Robert English, Brian Pierce, Peter Ondruska, Ishaan
Gulrajani, and Richard Socher. 2016. Ask me any-
thing: Dynamic memory networks for natural lan-
guage processing. CoRR, abs/1506, 2016.

Y. LeCun, B. Boser, J.S. Denker, D. Henderson, R.E.
Howard, W. Hubbard, and L.D.Jackel. 1990. Hand-
written digit recognition with a back-propagation net-
work. In Advances in NIPS, 1990.

Andre F. T. Martins and Ramon F. Astudillo. 2016.
From softmax to sparsemax:a sparse model of atten-
tion and multi-label classification. arXiv preprint arX-
iv:1602.02068v2.

Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In NAACL-HLT, pages 746–
751.

Lili Mou, Men Rui, Ge Li, Yan Xu, Lu Zhang, Rui Yan,
and Zhi Jin. 2015. Natural language inference by tree-
based convolution and heuristic matching. In Proceed-
ings of ACL (short papers).

Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan,
and Zhi Jin. 2016. Recognizing entailment and con-
tradiction by tree-based convolution. In ACL 2016.

Ankur P Parikh, Oscar Tackstrom, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. arXiv preprint
arXiv:1606.01933.

Tim Rocktaschel, Edward Grefenstette, Karl Moritz Her-
mann, Tomas Kocisky, and Phil Blunsom. 2016. Rea-
soning about entailment with neural attention. In Pro-
ceedings of ICLR.

RuiYan, XiaojunWan, JahnaOtterbacher, LiangKong, X-
iaoming Li, , and Yan Zhang. 2011b. Evolution-
ary timeline summarization: A balanced optimization
framework via iterative substitution. In Proceedings
of the 34th international ACM SIGIR conference on
Research and development in Information Retrieval,
pages 745–754.

Sainbayar Sukhbaatar, Jason Weston, and Rob Fergus
et al. 2015. End-to-end memory networks. In NIP-
S 2015, pages 2431–2439.

Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang
Pang, and Xueqi Cheng. 2015. A deep architecture for
semantic matching with multiple positional sentence
representations. arXiv preprint arXiv:1511.08277.

Shuohang Wang and Jing Jiang. 2015. Learning natu-
ral language inference with lstm. In Proceedings of
NAACL.

Jason Weston, Sumit Chopra, and Antoine Bordes. 2015.
Memory networks. In ICML 2015.

Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, X-
iaoming Li, and Yan Zhang. 2011a. Timeline gener-

ation through evolutionary trans-temporal summariza-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 433–
443.

YangLiu, SujianLi, XiaodongZhang, and ZhifangSui.
2016. Implicit discourse relation classification vi-
a multi-task neural networks. In Proceedings of the
Thirtieth AAAI Conference on Artificial Intelligence.

Wenpeng Yin and Hinrich Schutze. 2015. Convolutional
neural network for paraphrase identification. In Pro-
ceedings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 901–
911.

Wenpeng Yin, Hinrich Schutze, Bing Xiang, and Bowen
Zhou. 2016. Abcnn: Attention-based convolutional
neural network for modeling sentence pairs. In Trans-
actions of the Association of Computational Linguis-
tics.

Zhang and Tong. 2004. Statistical behavior and consis-
tency of classification methods based on convex risk
minimization. Annals of Statistics, pages 56–85.

24


