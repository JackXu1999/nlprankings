



















































UFRGS: Identifying Categories and Targets in Customer Reviews


Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 725–729,
Denver, Colorado, June 4-5, 2015. c©2015 Association for Computational Linguistics

UFRGS: Identifying Categories and Targets in Customer Reviews

Anderson Kauer
Institute of Informatics – UFRGS

Porto Alegre – RS – Brazil
aukauer@inf.ufrgs.br

Viviane P. Moreira
Institute of Informatics – UFRGS

Porto Alegre – RS – Brazil
viviane@inf.ufrgs.br

Abstract

This paper reports on our participation in
SemEval-2015 Task 12, which was devoted
to Aspect-Based Sentiment Analysis. Partic-
ipants were required to identify the category
(entity and attribute), the opinion target, and
the polarity of customer reviews. The system
we built relies on classification algorithms to
identify aspect categories and on a set of rules
to identify the opinion target. We propose a
two-phase classification approach for category
identification and use a simple method for po-
larity detection. Our results outperform the
baseline in many cases, which means our sys-
tem could be used as an alternative for aspect
classification.

1 Introduction

Aspect Based Sentiment Analysis aims at discover-
ing the opinions or sentiments expressed by a user
on the different aspects of a given entity (Hu and
Liu, 2004; Liu, 2012). Recently, a number of meth-
ods and techniques have been developed to tackle
this task and some of them rely on syntactic depen-
dencies to locate the opinion target (Kim and Hovy,
2004; Qiu et al., 2011; Liu et al., 2013). A syntac-
tic parser takes a natural language sentence as input
and outputs the relationships between the words in
the sentence. Figure 1 shows the dependency tree
for the sentence “The phone has a good screen.” and
the grammatical relations of each token (det, subj,
mod, obj). We explore using grammatical relations
to help identify the opinion targets.

In this paper, we describe a system which took
part on SemEval-2015, and the way it was applied

by an opinion word “good” (which is known a priori). The
knowledge can be represented by the logic rule:

aspect(T) :- depends(T,mod,O),
opinionW(O),pos(T,nn).

where :- is understood as logic if, aspect(T) denotes that
the word T is an aspect, opinionW(O) that the word O
is an opinion word, pos(T,nn) that the POS of T is NN,
depends(T,mod,O) that T is modified by O.

The extraction process inevitably produces errors because
it uses only syntactical information. In many existing works
[21], [26], infrequent candidate aspects are pruned. However,
this method may result in significant loss in precision or recall.
Typically, a threshold is used to tell whether a frequency is
high or low. As a result, to improve the precision, we need to
raise the threshold, which will hurt the recall, and vice versa.
To improve precision and recall, methods other than simple
frequency threshold have to be used.

We observed that there is a large class of words which are
so general that in very few cases they are aspects. Normally,
we will not take these words as aspects. As an example, in “I
can’t write enough good things about this camera,” “things”
is extracted as an aspect because it is modified by the opinion
word “good.” However, “things” is very unlikely to be a
product aspect and thus should be pruned. We propose to
use WordNet [28] to automatically generate a list of general
words using three typical general words “thing,” “person,”
and “place” as seeds. By extending the DP method with the
knowledge that a general word is normally not an aspect, we
obtain a major improvement in the precision with almost no
drop in recall on a widely used benchmark data set.

In summary, we make two contributions: (1) We propose
to employ Answer Set Programming (ASP) – a variant of
Logic Programming – to implement syntactical approach based
aspect extraction. Our implementation of the DP method is
more elegant and efficient, and it has only 8 rules, while a
Java implementation has about 510 lines of code. The ASP
based implementation can process about 3000 sentences per
second, while the Java implementation only processes about
300 sentences per second. The preciseness and simplicity of
the logic programming rules enable the sharing of knowledge
used in aspect extraction and the reproducibility of experi-
mental results. (2) We introduce the concept of general words
based on WordNet and augment the DP method with the
knowledge that general words normally should not be taken
as aspects, which results in more accurate aspect extraction.
Again, the general words and new knowledge can be naturally
implemented using ASP.

The remaining of the paper is organized as follows: we
present background and related work in Section II and an
overview of our logic programming approach in Section III.
The ASP rules to implement the DP method for extracting
explicit aspects are described in Section IV. Our new approach
to aspect pruning is presented in Section V. We present the
experiments in Section VI and conclude the paper and discuss
future work in Section VII.

II. BACKGROUND AND RELATED WORK

In this section we introduce the basics of aspect extraction
and Answer Set Programming.

A. Aspect Extraction

An object is an entity which can be a product, service,
person, event, organization, or topic. It is associated with a set
of components or attributes, called aspects of the object. Each
component may have its own set of aspects.

For example, a particular brand of cellular phone, say
iPhone, is an object. It has a set of components, e.g., battery
and screen, and also a set of attributes, e.g., voice quality, size,
and weight. These components and attributes are aspects of the
phone.

An opinion is simply a positive or negative view, attitude,
or emotion about an object or an aspect of the object from a
person or an organization. Given a collection of opinion texts
on an object, the aspect extraction problem is to produce the
aspects of the object from these documents.

As mentioned earlier, there are two main methods for as-
pect extraction. In this paper, we focus only on the syntactical
approach as it has been shown to perform better than the
statistical approach [26]. For related work on the statistical
approach, please refer to the recent book [1]. In the syntactical
approach, explicit aspect extraction consists of two phases:
candidate aspect extraction and incorrect aspect pruning.

For candidate aspect extraction, we focus on the double
propagation method [26] which is based on the following
observations. The first is that it is easy to identify (a priori)
a set of opinion words such as “good” and “bad,” etc. The
next is that opinion words are usually associated with aspects
(opinion targets) under certain syntactic relations. For example,
in the sentence “This camera is good,” “good” is an opinion
word. The “camera,” a noun modified by “good,” is clearly an
aspect. Therefore from a given set of opinion words, we can
derive a set of aspects in terms of syntactic relations. Similarly,
syntactic clues can help extract new aspects from the extracted
aspects, and new opinion words from the extracted aspects.
This propagation process continues until no more opinion
words or aspects can be extracted.

Dependency grammar is adopted to represent the syntactic
relations used in the propagation. See the picture below for an
example of the dependency tree for the sentence “The phone
has a good screen.” � � �� � � �� � � 	 
� � 
�  � � � �� � � 
 
 	��  � � � �� � � �

A direct dependency indicates that one word depends on
another word without any additional words in their dependency
path or they both depend on a third word directly. The
DP method considers only direct dependencies as complex
relations can make the method vulnerable to parsing errors.
Opinion words are assumed to be adjectives and aspects nouns
or noun phrases. Thus the potential POS tags for opinion
words are JJ (adjectives), JJR (comparative adjectives) and
JJS (superlative adjectives) while those for aspects are NN
(singular nouns) and NNS (plural nouns). The dependency

Figure 1: Example of a dependency tree (Liu et al., 2013).

to category and polarity classification. Our system
participated in all subtasks from Task 12 (Aspect
Based Sentiment Analysis). For more details on this
task, please refer to Pontiki et al. (2015). Our sys-
tem combines classification algorithms, coreference
resolution tools, and a syntactic parser. One of our
goals was to minimize the use of external resources.

The remainder of this paper is organized as fol-
lows: Our system is described in Section 2. Sec-
tion 3 reports on the evaluation results. Finally, sec-
tion 4 concludes the paper.

2 Description of the System

In this section, we describe the different components
of the system.

2.1 Pre-processing
A distinctive characteristic of Web content is the
high prevalence of noise. This directly impacts
the quality of the results generated by a syntactic
parser. In our system, we used the StanfordNLP
Core toolkit (Manning et al., 2014).

The training sentences provided by the organizers
were sometimes composed by more than one sen-
tence. Thus, before submitting them to the parser,

725



a cleaning step based on regular expressions was
performed. In this step, we replaced all punctua-
tion marks by commas and removed non-alphabetic
characters.

Then, the standard pre-processing tools available
from the StanfordNLP Core were applied (tokeniza-
tion, sentence splitting, part-of-speech tagging, mor-
phological analysis, syntactic parsing, coreference
resolution, and sentiment analysis).

2.2 Aspect Category Identification
We treated the problem of identifying aspect cate-
gories as a classification task. Thus, we made use
of the classifiers available from Weka (Hall et al.,
2009) to build models based on the training data.
In Task 12, categories are formed by a pair En-
tity#Attribute. The organizers have provided a list
of possible entities and, for each entity, a list of at-
tributes.

For each entity, we built a binary classifier where
each instance contains the lemmas on the sentence
and coreference lemmas to the previous sentences.
The class indicates whether the instance belongs to
the entity (i.e., positive means that the instance be-
longs to the entity and negative means it does not
belong to the entity). For each entity, the features
were selected using the InfoGainAttributeEval with
Ranker as a search method (available from Weka).
The threshold set up to Ranker was 0, which means
that the words selected by the method must con-
tribute to identify the class.

We used two approaches to classify the sen-
tences. In the first approach, one-phase classifi-
cation, for each entity dataset we trained six clas-
sifiers using all the sentences. These six clas-
sifiers (namely IBk, ThresholdSelector, Bayesian-
LogisticRegression, Logistic, MultiClassClassifier,
and SMO) were the top performers on our experi-
ments on the training data. We will refer to those
as Category classifiers, as they will be used to ac-
tually determine the class. Since the classifiers for
each category are independent, it is possible that a
sentence is predicted as belonging to more than one
category.

Classifiers were also built for each attribute be-
longing to that entity using only the sentences con-
taining the entity. We call these Attribute classifiers,
as they will be used to generate features for the Cat-

egory classifiers.
In the two-phase approach (Figure 2), first we

train n Attribute classifiers using all sentences but
the current. In the experiments reported in Section 3,
we used twenty Attribute classifiers (n=20). Then,
the outputs from each of the n Attribute classifiers
were used as features for the Category classifiers
(second phase). This phase requires significant pro-
cessing time since a new dataset is created for each
instance and the models have to be updated. This
method assumes that the features in each instance
contain “what the others tell about it” using differ-
ent prediction models.

set of instances (sentences)

subset of 

instances
current instance

build attribute 

classifier 1 to n
classification 1 to n

current instance with n features

Figure 2: Two-phase classification pipeline.

To classify a new unseen instance, first it needs to
be processed so that its lemmas and coreferences are
identified. Then, word frequencies are selected and
the n Attribute classifiers generate the values of the
features for the second phase.

The final predicted class is the top scoring (i.e.,
with the highest sum of scores) obtained from the
results of the six Category classifiers. Although this
has not happened in our experiments, a tie between
the scores of the positive and negative classes is pos-
sible. In such a case, the sentence will be assigned to
the positive class (i.e., as belonging to the category).

2.3 Opinion Target Identification

The opinion target is detected after the category has
been identified. For each pair Entity#Attribute dis-

726



covered in the sentence, the candidate words are se-
lected in order of information gain for that category.
The words from attribute classification are concate-
nated with the words for entity classification. The
assumption is that the words from attribute classifi-
cation are more significant than the words from en-
tity classification (which are more generic).

We select the word pairs which are directly as-
sociated (on the dependency tree) by a grammati-
cal relation such as adjectival modifier, noun com-
pound modifier, and nominal subject. We consider
the opinion targets to be nouns/noun phrases as this
has been widely adopted in the related literature (Hu
and Liu, 2004; Qiu et al., 2011; Liu et al., 2013).
Thus, the potential POS tags for targets are NN (sin-
gular nouns) and NNS (plural nouns). In order to
identify incorrect targets, we rely on a list of 5k
words assembled by Qian (2013). This exceptions
list contains words with little or no meaning and that
normally are not an aspect. The main target is the
first candidate noun which is not in such a list.

If no nouns are found among the candidates, we
find the nouns in the same sentence that are indi-
rectly related to the candidate words (i.e. by transi-
tivity), then we select the first noun. When still no
nouns are found, then the opinion is set to NULL (it
does not exist in the sentence). Target expressions
are obtained using noun compound modifier (nn) as-
sociations.

A current limitation is that we do not identify mul-
tiple target expressions for the same category. We
assume that for each category found, there is only
one target in the sentence. However, since a sen-
tence may be assigned to several categories, in these
cases, more than one target may be identified and
returned.

2.4 Sentiment Polarity Attribution
For this subtask, we used a simple approach that as-
signs the polarity of the target as the general polarity
of the sentence. Stanford NLP Core provides senti-
ment analysis based on a compositional model over
trees using deep learning (Socher et al., 2013). The
nodes of a binarized tree of each sentence are as-
signed a sentiment score.

We opted for this approach to minimize the exter-
nal resources in the our system, such as sentiment
lexicons or reviews collected from other sources.

The underlying model for Stanford NLP Core Sen-
timent Analysis was built on a corpus consisting of
11,855 sentences extracted from movie reviews. We
have made no attempt to change the model to adapt
to our reviews and used it as is to determine the
polarity of the sentences. Our contribution in this
phase was just the benchmarking of an existing tool.

3 Evaluation

We experimented with all three datasets from Task
12, namely Restaurants (Res), Laptops (Lap), and
Hidden (Hid) for which the domain was unknown.
Details on the datasets are in Pontiki et al. (2015).

The evaluation occurs in two phases. In the first
phase, participating systems were evaluated on cat-
egory detection for Restaurants and Laptops. Ad-
ditionally, identifying opinion target and the pair
(category, target) was requested for the Restau-
rants domain. In the second phase, the systems were
evaluated on polarity detection on all three domains.

3.1 Opinion Category and Target Detection
When evaluating opinion category and target detec-
tion (first phase), three measures were taken into ac-
count: precision, recall, and F1. For both category
and target detection, the baseline methodologies are
presented in Pontiki et al. (2015). Table 1 shows the
results obtained using our approach compared to the
baseline for aspect category detection, whereas Ta-
ble 2 outlines the results regarding aspect target de-
tection. The results for the pair (category, target)
are presented in Table 3.

Table 1: Opinion Category detection.
Domain Method P R F1

Res 2Phase 0.6556 0.4323 0.5210
Res 1Phase 0.6835 0.4181 0.5188
Res Baseline 0.5133
Res 1Phase-coref 0.6821 0.4180 0.5184
Res 2Phase-coref 0.6509 0.4090 0.5023
Lap Baseline 0.4631
Lap 1Phase 0.5066 0.4040 0.4495
Lap 2Phase 0.4773 0.4209 0.4473
Lap 1Phase-coref 0.4834 0.4462 0.4640
Lap 2Phase-coref 0.4689 0.4388 0.4534

The system outperforms the baseline on both ap-
proaches for the Restaurants domain. In this do-
main, the two-phase approach was superior to the

727



one-phase approach. For the laptop domain, how-
ever, we scored lower than the baseline. We attribute
that to the increased difficulty the coreference reso-
lution step had when processing the review texts in
this domain because of the large number of out of
vocabulary words (CPU, HD, RAM, etc). Table 1
shows that the results improve when the coreference
resolution step is not performed. Nevertheless, for
the Restaurant domain, it brought improvements.

Table 2: Opinion Target detection.
Domain Method P R F1

Res 2Phase 0.5656 0.4373 0.4932
Res 1Phase 0.5764 0.4244 0.4888
Res Baseline 0.4807
Res 2Phase-exc. 0.5632 0.4354 0.4911
Res 1Phase-exc. 0.5739 0.4225 0.4867

Considering the results for opinion target detec-
tion, both versions of our system outperformed the
baseline. The two-phase classification achieved bet-
ter recall in both category and target detection, but
worse precision compared to one-phase classifica-
tion.

We ran some additional experiments to evaluate
the use of the exceptions list during target identifi-
cation. These runs in which the exceptions list were
not used are labelled 1Phase-exc and 2Phase-exc in
Table 2. The results show that using such a list did
not impact the results.

Table 3: Opinion Category and Target pair detection.
Domain Method P R F1

Res 2Phase 0.4852 0.2722 0.3487
Res Baseline 0.3444
Res 1Phase 0.4521 0.2734 0.3407
Res 1Phase-coref 0.4694 0.2639 0.3378
Res 2Phase-coref 0.4496 0.2591 0.3288

As for the results for the pair (category, target)
the two-phase classification outperforms both the
baseline and the one-phase classification. The gain
in terms of precision is three percentage points,
while recall was slightly reduced. The best configu-
ration was using coreference resolution and the ex-
ceptions list.

3.2 Opinion Polarity Detection

Table 4 shows the results in terms of accuracy
on opinion polarity. Here, the methodology for
the baseline is similar to the ones used for as-
pect category detection (also described in Pontiki et
al. (2015)). In this subtask, we submitted only the
results for the one-phase classification.

Table 4: Opinion Polarity detection.
Domain Method Accuracy

Res 1Phase 0.7172
Res Baseline 0.5373
Lap 1Phase 0.6733
Lap Baseline 0.5701
Hid Baseline 0.7168
Hid 1Phase 0.6578

The Stanford Core Toolkit uses a model trained
on movie reviews, and this was not the same do-
main of the datasets in the task. Still, the classifi-
cation results outperformed the baseline on Restau-
rants and Laptops. However, for the Hidden domain,
we scored lower than the baseline.

3.3 Error Analysis

The results obtained with our system are ranked be-
tween the 5th (out of 15) and the 14th (out of 22)
places. A case by case analysis was performed to
identify the most frequent causes of errors. In the
task of aspect category classification, the choice of
the threshold used during feature selection by the
Ranker (0) may have negatively impacted the re-
sults. Nevertheless, some feature selection method
is necessary since the use of all the words as features
greatly increases the processing time.

We used words selected by their Information Gain
as seeds to identify the target expression. In our ex-
periments, in many cases, the target was next to the
words selected by this strategy. This happens be-
cause the positive class had fewer instances than
the negative class, and the Information Gain tends
to select words that characterize the least frequent
class. However, most classification errors happened
because this strategy failed to identify infrequent
words that corresponded to the expected categories.
One possible alternative to mitigate this problem
could be the use of synonyms.

728



The method we used for polarity detection con-
sidered the entire sentence. The limitation here is
that many sentences contain more than one opinion,
which may not convey the same polarity. This could
be solved by identifying the context (i.e., a region
around the target) and limit the polarity attribution
to that region.

4 Conclusion

This paper reports on the experiments that we con-
ducted while taking part on SemEval-2015 Task 12.
We showed that classification algorithms, corefer-
ence resolution tools, and a syntactic parser may
be combined in a category/target detection sys-
tem.We employed a two-phase approach to classify
instances. Our results show that this approach can
be an alternative to classify sentences without us-
ing lexicons, improving recall with a small decay in
precision. As future work, we plan to improve the
coreference resolution of review texts so as to fur-
ther improve recall.

Acknowledgments

This work has been partially funded by CNPq-Brazil
project 478979/2012-6. Anderson Kauer receives a
scholarship from CNPq-Brazil. We would like to
thank the anonymous reviewers for their helpful sug-
gestions and comments.

References
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard

Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10–18, November.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth

ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168–177,
New York, NY, USA. ACM.

Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Qian Liu, Zhiqiang Gao, Bing Liu, and Yuanlin Zhang.
2013. A logic programming approach to aspect
extraction in opinion mining. In Web Intelligence
(WI) and Intelligent Agent Technologies (IAT), 2013
IEEE/WIC/ACM International Joint Conferences on,
volume 1, pages 276–283, Nov.

Bing Liu. 2012. Sentiment analysis and opinion mining.
Synthesis Lectures on Human Language Technologies,
5(1):1–167.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics: System Demonstrations, pages 55–60, Baltimore,
Maryland. Association for Computational Linguistics.

Maria Pontiki, Dimitrios Galanis, Haris Papageogiou,
Suresh Manandhar, and Ion Androutsopoulos. 2015.
Semeval-2015 task 12: Aspect based sentiment analy-
sis. In Proceedings of the 9th International Workshop
on Semantic Evaluation (SemEval 2015), Denver, Col-
orado.

Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Linguis-
tics, 37(1):9–27.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
1631–1642.

729


