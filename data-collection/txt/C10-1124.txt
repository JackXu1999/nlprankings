Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1101–1109,

Beijing, August 2010

1101

Large Scale Parallel Document Mining for Machine Translation

Jakob Uszkoreit

Jay M. Ponte Ashok C. Popat Moshe Dubiner

{uszkoreit,ponte,popat,moshe}@google.com

Google, Inc.

Abstract

A distributed system is described that re-
liably mines parallel text from large cor-
pora.
The approach can be regarded
as cross-language near-duplicate detec-
tion, enabled by an initial,
low-quality
batch translation. In contrast to other ap-
proaches which require specialized meta-
data, the system uses only the textual con-
tent of the documents. Results are pre-
sented for a corpus of over two billion web
pages and for a large collection of digi-
tized public-domain books.

Introduction

1
While the World Wide Web provides an abun-
dance of readily available monolingual text, par-
allel data is still a comparatively scarce resource,
yet plays a crucially important role in training sta-
tistical machine translation systems.

We describe an approach to mining document-
aligned parallel text to be used as training data
for a statistical machine translation system. Pre-
vious approaches have focused on rather homo-
geneous corpora and relied on metadata such as
publication dates (Munteanu and Marcu, 2005;
Munteanu and Marcu, 2006; Udupa et al., 2009;
Do et al., 2009; Abdul-Rauf and Schwenk, 2009)
or information about document structure (Resnik
and Smith, 2003; Chen and Nie, 2000). In large
and unstructured collections of documents such as
the Web, however, metadata is often sparse or un-
reliable. Our approach, in contrast, scales com-
putationally to very large and diverse collections
of documents and does not require metadata. It is

based solely on the textual contents of the input
documents.

Casting the problem as one of cross-language
near duplicate detection, we use a baseline ma-
chine translation system to translate all input doc-
uments into a single language. However,
the
words and phrases that are most discriminatory
for the purposes of information retrieval and du-
plicate detection are the relatively rare ones, pre-
cisely those that are less likely to be translated
well by the baseline translation system.

Our approach to circumvent this problem and
to avoid the prohibitive quadratic computational
complexity of the naive approach of performing a
comparison of every possible pair of input docu-
ments is similar to previous work in near duplicate
detection (Broder, 2000; Henzinger, 2006; Man-
ber, 1994) and noisy data retrieval (Harding et al.,
1997).

We use shingles consisting of word n-grams to
construct relatively rare features from more com-
mon, in-vocabulary words. For each input doc-
ument, we identify a comparatively small set of
candidate pairings with documents sharing at least
a certain number of such features. We then per-
form a more expensive comparison between each
document and all documents in its candidate set
using lower order n-gram features that would typ-
ically be too frequent to be used efﬁciently in
forming candidate pairings, but provide a higher
coverage of the scored document pairs. Another
important aspect of our approach is that it can be
implemented in a highly parallel way, as we de-
scribe in the following section.

1102

2 System Description

The input is a set of documents from diverse
sources such as web pages and digitized books.
In a ﬁrst stage, all documents are independently
translated into English using a baseline statistical
machine translation system.

We then extract two different sets of n-grams
from the translated documents: matching n-grams
that are used to construct the candidate sets as well
as scoring n-grams used only in the computation
of a score for a given pair of documents. This
stage generates two indexes: a forward index list-
ing all extracted scoring n-grams, indexed by doc-

Figure 1: Architecture of the Parallel Text Mining
System.

ument; and an inverted index referencing all doc-
uments from which we extracted a given match-
ing n-gram, indexed by n-grams. The inverted
index is also used to accumulate global informa-
tion about scoring n-grams, such as their docu-
ment frequency, yet for scoring n-grams we do
not accumulate a posting list of all documents in
which they occur.

In the next step, the system generates all possi-
ble pairs of documents for each matching n-gram
posting list in the inverted index. Since we keep
only those pairs of documents that originated in
different languages, we can discard posting lists
from the inverted index that contain only a single
document, i.e. those of singleton n-grams, or only
documents in a single language.

Crucially, we further discard posting lists for
matching n-grams whose frequency exceeds a
certain threshold. When choosing a sufﬁciently
large order for the matching n-grams, their long-
tailed distribution causes only a small fraction of
matching n-grams to be ﬁltered out due to fre-
quency, as we show empirically in Section 5. It
is this ﬁltering step that causes the overall runtime
of the system to be linear in the size of the input
data and allows the system to scale to very large
document collections.

In parallel, global information about scoring n-
grams accumulated in the inverted index that is
required for pairwise scoring, such as their doc-
ument frequency, is folded into the forward in-
dex by iterating over all forward index entries, re-
questing the respective per-feature quantities from
the inverted index and storing them with each oc-
currence of a scoring n-gram in an updated for-
ward index.

In the next stage, we compute pairwise scores
for all candidate document pairs, accessing the
forward index entry of each of the two scored doc-
uments to obtain the respective scoring n-grams.
Document pairs with a score below a given thresh-
old are discarded. For each input document, this
results in one n-best list per language. In the last
step we retain only those document pairs where
each document is contained in the n-best list of
the other document for its original language. Fi-
nally we perform a join of our identiﬁed transla-
tion pairs with the original text by making another

Documents 
in Multiple 
Languages

Machine translate input data

English 
Translations

Extract n-grams

Forward Index

Filter inverted index

by document frequency and 
number of original languages

Fold global, per-scoring n-gram 
information from inverted index 

into forward index

Inverted Index

Generate all pairs of documents 

sharing matching n-grams

Score unique document pairs,
querying the forward Index

Per-document n-best lists

Discard non-symmetric pairs

Evaluate on reference document 

alignments

Join with original input data

1103

pass over the original, untranslated input data
where the contents of document pairs with sufﬁ-
ciently high scores are then aggregated and out-
put. Document pairings involving all languages
are identiﬁed simultaneously. Each stage of the
system ﬁts well into the MapReduce program-
ming model (Dean and Ghemawat, 2004). The
general architecture is shown in Figure 1.

2.1 Pairwise Scoring
For scoring a pair of documents d and d0, the
forward index is queried for the entries for both
documents. Let Fd = {f1, f2, ...fn} and Fd0 =
{f01, f02, ...f0n0} be the sets of scoring n-grams in
the forward index entries of d and d0, respectively.
Let idf(f ) = log |D|
df (f ) be the inverse document
frequency of a scoring n-gram f, where |D| is
the number of documents in the input corpus and
df (f ) is the number documents from which we
extracted the feature f. Interpreting Fd and Fd0 as
incidence vectors in the vector space of n-grams
and replacing each non-zero component f with
idf(f ), we compute the score of the document pair
as the inverse document frequency weighted co-
sine similarity of Fd and Fd0

score(d, d0) =

Fd · Fd0

||Fd|| · ||Fd0||

(1)

The per-document n-best lists are sorted ac-
cording to this score and document pairs for which
the score is below a threshold are discarded com-
pletely.

We do not use term frequency in the scoring
metric.
In preliminary experiments, incorporat-
ing the term frequency to yield basic tf/idf as
well as using other information retrieval ranking
functions incorporating term frequencies such as
BM25 (Robertson et al., 1995) resulted in a degra-
dation of performance compared to the simpler
scoring function described above. We believe this
is due to the fact that, in contrast to the standard
information retrieval setting, the overall length of
our queries is on par with that of the documents in
the collection.

The scoring is completely agnostic regarding
the scoring n-grams’ positions in the documents.
Since especially for long documents such as

books this may produce spurious matches, we ap-
ply an additional ﬁlter to remove document pairs
for which the relative ordering of the matching
scoring n-grams is very different. Together with
each scoring n-gram we also extract its relative
position in each document and store it in the for-
ward index. When scoring a document pair, we
compute the normalized permutation edit distance
(Cormode et al., 2001) between the two sequences
of overlapping n-grams sorted by their position in
the respective document. If this distance exceeds
a certain threshold, we discard the document pair.

2.2 Computational Complexity
By limiting the frequency of matching n-grams,
the complexity becomes linear. Let the tunable
parameter c be the maximum occurrence count for
matching n-grams to be kept in the inverted in-
dex. Let m be the average number of matching
n-grams extracted from a single document whose
count is below c and D be the set of documents
in the input corpus. Then the system generates up
to |D| · m · c candidate pairings. Scoring a given
candidate document pair according to cosine sim-
ilarity involves computing three dot-products be-
tween sparse vectors with one non-zero compo-
nent per scoring n-gram extracted and not ﬁltered
from the respective document. Let s be the av-
erage number of such scoring n-grams per docu-
ment, which is bounded by the average document
length. Then the time complexity of the entire
document alignment is in

O(|D| · m · c · s)

(2)

and therefore linear in the number of input doc-
uments in the corpus and the average document
size.

The space complexity is dominated by the size
of the inverted and forward indexes, both of which
are linear in the size of the input corpus.

2.3 Sentence-Level Alignment
Further ﬁltering is performed on a per-sentence
basis during per-document-pair sentence align-
ment of the mined text with a standard dynamic
programming sentence alignment algorithm using
sentence length and multilingual probabilistic dic-
tionaries as features. Afterwards we crudely align

1104

words within each pair of aligned source and tar-
get sentences. This crude alignment is used only
to ﬁlter nonparallel sentences. Let S be the set
of source words, T the set of target words and
S × T the set of ordered pairs. Let the source
sentence contain words S0 ⊂ S and the target
sentence contain words T0 ⊂ T . An alignment
A0 ⊂ S0 × T0 will be scored by
score(A0) = X(s,t)∈A0

p(s) p(t)

p(s, t)

(3)

ln

where the joint probabilities p(s, t) and marginal
probabilities p(s), p(t) are taken to be the respec-
tive empirical distributions (without smoothing)
in an existing word aligned corpus. This is greed-
ily maximized and the result is divided by its ap-
proximate expected value

X(s,t)∈S0×T

p(s, t)
p(s)

ln

p(s, t)

p(s) p(t)

(4)

We discard sentence pairs for which the ratio be-
tween the actual and the expected score is less
than 1/3. We also drop sentence pairs for which
both sides are identical, or a language detector de-
clares them to be in the wrong language.

2.4 Baseline Translation System
To translate the input documents into English we
use phrase-based statistical machine translation
systems based on the log-linear formulation of the
problem (Och and Ney, 2002).

(Koehn, 2002),

We train the systems on the Europarl Cor-
pus
the DGT Multilingual
Translation Memory (European Commission
Directorate-General for Translation, 2007) and
the United Nations ODS corpus (United Nations,
2006). Minimum error rate training (Macherey
et al., 2008) under the BLEU criterion is used
to optimize the feature function weights on de-
velopment data consisting of the nv-dev2007 and
news-dev2009 data sets provided by the organiz-
ers of the 2007 and 2009 WMT shared translation
tasks1. We use a 4-gram language model trained
on a variety of large monolingual corpora. The
BLEU scores of our baseline translation system

1available at http://statmt.org

on the test sets from various WMT shared trans-
lation tasks are listed in Table 5. An empirical
analysis of the impact of the baseline translation
system quality on the data mining system is given
in Section 6.3.

Input Document Collections

3
We evaluate the parallel text mining system on
two input data sets:

web A collection of 2.5 Billion general pages
crawled from the Web, containing only pages
in Czech, English, French, German, Hungar-
ian and Spanish

books A collection of 1.5 Million public domain
books digitized using an optical character
recognition system. The collection consists
primarily of English, French and fewer Span-
ish volumes

3.1 Reference Sets
We created reference sets of groups of docu-
ments in multiple languages which are true trans-
lations of one another for both the web and the
books data set. Due to the presence of duplicates,
each reference pairing can contain more than a
single alternative translation per language. The
web reference set was constructed by exploiting
the systematic hyperlink structure of the web-site
http://america.gov/, that links pages in
one language to their respective translations into
one or more other languages. The resulting refer-
ence set contains documents in Arabic, Chinese,
English, French, Russian and Spanish, however,
for most English pages there is only one transla-
tion into one of the other languages. Overall, the
reference set contains 6,818 documents and 7,286
translation pairs.

The books reference set contains 30 manually
aligned groups of translations covering a total of
103 volumes in English and French.

4 Evaluation Metrics
The fact that the system outputs pairs of docu-
ments and the presence of duplicate documents in
the corpus motivate the use of modiﬁed versions
of precision and recall.

1105

Let C be a set of candidate parallel document
pairs and let R be a possibly incomplete reference
set of groups of parallel documents known to exist
in the corpus. Consider the following two subsets
of C:

• Matching pairs which are in some reference

cluster.

• Touching pairs which are non-matching but
have at least one document in some reference
cluster.

t
e
S
a
t
a
D

t
s
e
T
b
e
w
n
o

e
r
o
c
S
1
F

0.96

0.95

0.94

0.93

0.92

0.91

0.9

0.89

We deﬁne

2-gram matching
3-gram matching
4-gram matching
5-gram matching

2

3
4
Scoring n-gram Order

5

Precision =

and

|CMatching|

|CMatching| + |CTouching|

Recall = |CMatching|

(5)

|R|

5 Parameter Selection
We conducted a series of small-scale experiments
on only those documents contained in the web ref-
erence data set to empirically determine good set-
tings for the tunable parameters of the text min-
ing system. Among the most important parame-
ters are the orders of the n-grams used for pair-
ing documents as well as scoring them. Aside
from the obvious impact on the quality of the out-
put, these parameters have a very large inﬂuence
on the overall computational performance of the
system. The choice of the order of the extracted
matching n-grams is mainly a trade-off between
recall and efﬁciency.
If the order is too large
the system will miss valid pairs; if too small the
the threshold on matching n-gram frequency will
need to be increased.

Figure 2 shows the F1-scores obtained run-
ning only on the documents contained in the web
reference set with different orders of matching
and scoring n-grams. Figure 3 shows the corre-
sponding number of pairwise comparisons made
when using different orders of matching n-grams.
While there is a drop of 0.01 in F1 score between
using 2-grams and 5-grams as matching n-grams,
this drop in quality seems to be well worth the 42-
fold reduction in resulting pairwise comparisons.

Figure 2: F1 scores on the web reference set for
different scoring and matching n-gram orders.

s
n
o
s
i
r
a
p
m
o
C
e
s
i
w

r
i
a
P
f
o

r
e
b
m
u
N

107

106

105

2

3
4
Matching n-gram Order

5

Figure 3: Number of pairwise comparisons made
when using matching n-grams of different orders.

The largest portion of the loss in F1 score is in-
curred when increasing the matching n-gram or-
der from 4 to 5, the reduction in pairwise compar-
isons, however, is still more than twofold.

Table 1 shows the precision and recall on the
web reference set when running only on docu-
ments in the reference set using 5-grams as match-
ing n-grams and bigrams for scoring for differ-
ent values of the threshold on the cosine similar-
ity score. In this setting as well as in large-scale
experiments on both complete data sets described
in section 6.1, a threshold of 0.1 yields the highest
F1 score.

1106

score threshold
precision
recall

0.06
0.92
0.91

0.10
0.97
0.91

0.12
0.98
0.90

0.16
0.99
0.89

0.20
0.99
0.83

Table 1: Precision and recall on the web reference
set when running only on documents contained in
the reference set.

6 Evaluation

We run the parallel text mining system on the web
and books data sets using 5-grams for matching
and bigrams for scoring. In both cases we discard
matching n-grams which occurred in more than
50 documents and output only the highest scoring
candidate for each document.

In case of the web data set, we extract every 5-
gram as potential matching feature. For the books
data set, however, we downsample the number
of candidate matching 5-grams by extracting only
those whose integer ﬁngerprints under some hash
function have four speciﬁc bits set, thus keeping
on average only 1/16 of the matching n-grams.
Here, we also restrict the total number of match-
ing n-grams extracted from any given document
to 20,000. Scoring bigrams are dropped from
the forward index if their document frequency ex-
ceeds 100,000, at which point their inﬂuence on
the pairwise score would be negligible.

Running on the web data set, the system on
average extracts 250 matching 5-grams per doc-
ument, extracting a total of approximately 430
Billion distinct 5-grams. Of those, 78% are
singletons and 21% only occur in a single lan-
guage. Only approximately 0.8% of all match-
ing n-grams are ﬁltered due to having a docu-
ment frequency higher than 50. The forward in-
dex initially contains more than 500 Billion bi-
gram occurrences; after pruning out singletons
and bigrams with a document frequency larger
than 100,000, the number of indexed scoring fea-
ture occurrences is reduced to 40%. During scor-
ing, approximately 50 Billion pairwise compar-
isons are performed.

In total the n-gram extraction, document scor-
ing and subsequent ﬁltering takes less than 24
hours on a cluster of 2,000 state-of-the-art CPUs.
The number of words after sentence-level ﬁl-
tering and alignment that the parallel text mining

Czech
French
German
Hungarian
Spanish

books
0

web
271.9 M

baseline
27.5 M
479.8 M 228.5 M 4,914.3 M
3,787.6 M
54.2 M
198.9 M
26.9 M
441.0 M 15.0 M
4,846.8 M

0
0

Table 2: The number of words per language in the
baseline training corpora and extracted from the
two different data sets.

system extracted for the different languages from
each dataset are listed in Table 2.

score threshold
precision
recall

0.06
0.88
0.68

0.10
0.93
0.65

0.12
0.95
0.63

0.16
0.97
0.52

0.20
0.97
0.38

Table 3: Precision and recall on the reference set
when running on the complete web data set with
different score thresholds.

score threshold
precision
recall

0.06
0.95
0.71

0.10
1.00
0.71

0.12
1.00
0.71

0.16
1.00
0.48

0.20
1.00
0.38

Table 4: Precision and recall on the reference set
when running on the complete books data set with
different score thresholds.

6.1 Precision and Recall
Tables 3 and 4 show precision and recall on the re-
spective reference sets for the web and the books
input data sets. While the text mining system
maintains a very high precision, recall drops sig-
niﬁcantly compared to running only on the doc-
uments in the reference set. One reason for this
behavior is that the number of n-grams in the test
data set which are sufﬁciently rare to be used as
queries drops with increasing amounts of input
data and in particular short documents which only
share a small number of matching n-grams any-
way, may happen to only share matching n-grams
with a too high document frequency. Further anal-
ysis shows that another, more signiﬁcant factor is
the existence of multiple, possibly partial transla-
tions and near-duplicate documents which cause
symmetrization to discard valid document pairs
because each document in the pair is determined
by the document pair score to be more similar to
a different translation of a near-duplicate or sub-

1107

Language Pair
Czech English

German English

Hungarian English

French English

Spanish English

English Czech

English German

English Hungarian

English French

English Spanish

Training Data WMT 2007 news commentary WMT 2008 news WMT 2009 news
baseline
web
baseline
web
baseline
web
baseline
books
web
baseline
books
web
baseline
web
baseline
web
baseline
web
baseline
books
web
baseline
books
web

21.59
29.26 (+7.67)
27.99
32.35 (+4.36)
-
-
34.26
34.73 (+0.47)
36.65 (+2.39)
43.67
44.07 (+0.40)
46.21 (+2.54)
14.78
20.65 (+5.86)
19.89
23.49 (+3.60)
-
-
31.59
31.92 (+0.33)
34.35 (+2.76)
42.05
42.05
45.21 (+3.16)

14.59
20.16 (+5.57)
20.34
23.22 (+2.88)
10.21
12.92 (+2.71)
22.14
22.39 (+0.25)
23.22 (+1.08)
24.15
24.32 (+0.17)
25.52 (+1.37)
12.45
18.70 (+6.25)
14.67
16.78 (+2.11)
07.93
10.16 (+2.23)
22.29
22.42 (+0.13)
23.56 (+1.27)
24.65
24.79 (+0.14)
26.46 (+1.81)

16.46
23.25 (+6.76)
20.03
23.35 (+3.32)
11.02
14.68 (+3.66)
26.39
27.15 (+0.76)
28.34 (+1.95)
26.88
27.16 (+0.28)
28.50 (+1.62)
11.62
16.60 (+4.98)
14.31
16.96 (+2.65)
08.52
11.42 (+2.90)
25.14
25.46 (+0.32)
27.05 (+1.91)
25.85
26.07 (+0.22)
27.79 (+1.94)

Table 5: BLEU scores of the translation systems trained on the automatically mined parallel corpora
and the baseline training data.

set of the document. This problem seems to affect
news articles in particular where there are often
multiple different translations of large subsets of
the same or slightly changed versions of the arti-
cle.

6.2 Translation Quality

Arabic English
Baseline (UN ODS)
Munteanu and Marcu
Present work
Chinese English
Baseline (UN ODS)
Munteanu and Marcu
Present work

42.79
43.86
43.64

NIST 2006 NIST 2008
44.31
45.13
44.72
NIST 2006 NIST 2008
25.71
28.11
28.08

19.79
21.69
22.02

Table 6: BLEU scores of the Chinese and Arabic
to English translation systems trained on the base-
line UN ODS corpus and after adding either the
Munteanu and Marcu corpora or the training data
mined using the presented approach.

We trained a phrase-based translation system
on the mined parallel data sets and evaluated it
on translation tasks for the language pairs Czech,
French, German, Hungarian and Spanish to and
from English, measuring translation quality with

the BLEU score (Papineni et al., 2002). The trans-
lation tasks evaluated are the WMT 2007 news
commentary test set as well the WMT 2008 and
2009 news test sets.

The parallel data for this experiment was mined
using the general settings described in the previ-
ous section and a threshold of 0.1 on the pairwise
score. We ensure that the test data is not included
in the training data by ﬁltering out all sentences
from the training data that share more than 30%
of their 6-grams with any sentence from one of
the test corpora.

Table 5 shows the BLEU scores of the differ-
ent translation systems. The consistent and signif-
icant improvements in BLEU score demonstrate
the usefulness of the mined document pairs in
training a translation system.

Even though the presented approach works
on a less granular level than the sentence-level
approach of Munteanu and Marcu (2005), we
compare results on the same input data2 used
by those authors to automatically generate the

2LDC corpora LDC2005T12,

and
LDC2006T02, the second editions of the Arabic, Chinese
and English Gigaword corpora.

LDC2005T14

1108

Sampling Rate WMT 2007 news commentary
En→Cz
20.65
20.55
20.61
20.39
20.44
20.28

degraded Cz→En
21.59
20.12
18.59
16.69
14.72
12.60

1.0
0.5
0.25
0.125
0.0625
0.0312

29.26
29.16
29.09
29.10
29.04
28.75

WMT 2008 news

WMT 2009 news

degraded Cz→En
14.59
13.65
12.79
11.87
10.87
09.71

20.16
20.16
20.09
20.07
20.06
19.97

En→Cz
18.70
18.71
18.58
18.48
18.49
18.45

degraded Cz→En
16.46
15.44
14.35
13.05
11.62
10.43

23.25
23.16
23.18
23.06
23.11
23.04

En→Cz
16.60
16.56
16.50
16.53
16.44
16.41

Table 7: BLEU scores of the degraded Czech to English baseline systems used for translating Czech
documents from the web data set as well as those of Czech to and from English systems trained on data
mined using translations of varying quality created by sampling from the training data.

Arabic English and Chinese English sentence-
aligned parallel LDC corpora LDC2007T08 and
LDC2007T09. We trained Arabic and Chinese
English baseline systems on the United Nations
ODS corpus (United Nations, 2006); we also use
these to translate the non-English portions of the
input data to English. We then evaluate the effects
of also training on either the LDC2007T08 and
LDC2007T09 corpora or the parallel documents
mined by our approach in addition to the United
Nations ODS corpus on the NIST 2006 and 2008
MT evaluation test sets. The results are presented
in Table 6.

The approach proposed in (Munteanu and
Marcu, 2005) relies critically on the existence
of publication dates in order to be computation-
ally feasible, yet it still scales superlinearly in the
amount of input data. It could therefore not easily
be applied to much larger and less structured input
data collections. While our approach neither uses
metadata nor operates on the sentence level, in all
but one of the tasks, the system trained on the data
mined using our approach performs similarly or
slightly better.

Impact of Baseline Translation Quality

6.3
In order to evaluate the impact of the translation
quality of the baseline system on the quality of
the mined document pairs, we trained artiﬁcially
degraded Czech to English translation systems by
sampling from the baseline training data at de-
creasing rates. We translate the Czech subset of
the web document collection into English with
each of the degraded systems and apply the paral-
lel data mining system in the same conﬁguration.
Table 7 shows the BLEU scores of the degraded
baseline systems and those resulting from adding

the different mined data sets to the non-degraded
Czech English and English Czech systems. De-
grading the input data translation quality by up to
8.9% BLEU results in a consistent but only com-
paratively small decrease of less than 0.6% BLEU
in the scores obtained when training on the mined
document pairs. This does not only show that the
impact of variations of the baseline system quality
on the data mining system is limited, but also that
the data mining system will already work with a
rather low quality baseline system.

7 Conclusion

We presented a scalable approach to mining paral-
lel text from collections of billions of documents
with high precision. The system makes few as-
sumptions about the input documents. We demon-
strated that it works well on different types of
data: a large collection of web pages and a col-
lection of digitized books. We further showed that
the produced parallel corpora can signiﬁcantly im-
prove the quality of a state-of-the-art statistical
machine translation system.

8 Acknowledgments

We thank the anonymous reviewers for their in-
sightful comments.

References
Abdul-Rauf, Sadaf and Holger Schwenk. 2009. On
the use of comparable corpora to improve SMT per-
formance. In EACL, pages 16–23.

Broder, Andrei Z. 2000. Identifying and ﬁltering near-
duplicate documents. In COM ’00: Proceedings of
the 11th Annual Symposium on Combinatorial Pat-

1109

Munteanu, Dragos Stefan and Daniel Marcu. 2005.
Improving machine translation performance by ex-
ploiting non-parallel corpora. Comput. Linguist.,
31(4):477–504.

Munteanu, Dragos Stefan and Daniel Marcu. 2006.
Extracting parallel sub-sentential fragments from
non-parallel corpora. In ACL.

Och, Franz Josef and Hermann Ney.

2002. Dis-
criminative training and maximum entropy models
for statistical machine translation.
In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 295–302,
Philadelphia, PA, USA.

Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation.
In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
311–318, Philadelphia, PA, USA.

Resnik, Philip and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29:349–380.

Robertson, S E, S Walker, S Jones, M M Hancock-
Beaulieu, and M Gatford. 1995. Okapi at TREC–3.
In Proceedings of the Third Text REtrieval Confer-
ence (TREC-3).

Udupa, Raghavendra, K. Saravanan, A. Kumaran, and
Jagadeesh Jagarlamudi. 2009. Mint: A method
for effective and scalable mining of named entity
transliterations from large comparable corpora. In
EACL, pages 799–807.

United Nations.

2006. ODS UN parallel corpus.

http://ods.un.org/.

tern Matching, pages 1–10, London, UK. Springer-
Verlag.

Chen, Jiang and Jian-Yun Nie. 2000. Parallel web
text mining for cross-language IR. In In In Proc. of
RIAO, pages 62–77.

Cormode,

Graham,

S. Muthukrishnan,

and
S¨uleyman Cenk Sahinalp.
Permutation
In ICALP
editing and matching via embeddings.
’01: Proceedings of the 28th International Collo-
quium on Automata, Languages and Programming,,
pages 481–492, London, UK. Springer-Verlag.

2001.

Dean, Jeffrey and Sanjay Ghemawat. 2004. MapRe-
duce: Simpliﬁed data processing on large clusters.
In Proceedings of the Sixth Symposium on Operat-
ing System Design and Implementation (OSDI-04),
San Francisco, CA, USA.

Do, Thi-Ngoc-Diep, Viet-Bac Le, Brigitte Bigi, Lau-
rent Besacier Eric, and Castelli. 2009. Mining a
comparable text corpus for a Vietnamese - French
statistical machine translation system. In Proceed-
ings of the 4th EACL Workshop on Statistical Ma-
chine Translation, pages 165–172, Athens, Greece,
March.

European Commission Directorate-General for Trans-
lation.
DGT-TM parallel corpus.
http://langtech.jrc.it/DGT-TM.html.

2007.

Harding, Stephen M., W. Bruce Croft, and C. Weir.
1997. Probabilistic retrieval of OCR degraded text
using n-grams.
In ECDL ’97: Proceedings of
the First European Conference on Research and
Advanced Technology for Digital Libraries, pages
345–359, London, UK. Springer-Verlag.

Henzinger, Monika.

2006. Finding near-duplicate
web pages: a large-scale evaluation of algorithms.
In SIGIR ’06: Proceedings of the 29th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 284–
291, New York, NY, USA. ACM.

Koehn, Philipp. 2002. Europarl: A multilingual cor-

pus for evaluation of machine translation. Draft.

Macherey, Wolfgang, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
725–734, Honolulu, Hi, October. Association for
Computational Linguistics.

Manber, Udi. 1994. Finding similar ﬁles in a large ﬁle
system. In Proceedings of the USENIX Winter 1994
Technical Conferenc.

