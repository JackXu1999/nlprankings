Proceedings of the Biomedical NLP Workshop associated with RANLP 2017, pages 49–54,

Varna, Bulgaria, 8 September 2017.

https://doi.org/10.26615/978-954-452-044-1_007

49

One model per entity: using hundreds of machine learning models to

recognize and normalize biomedical names in text

Victor Bellon

MINES ParisTech,

PSL-Research University,

CBIO-Centre de bio-informatique

Raul Rodriguez-Esteban

Roche Innovation Center Basel

raul.rodriguez-esteban@roche.com

victor.bellon@mines-paristech.fr

Abstract

We explored a new approach to named
entity recognition based on hundreds of
machine learning models, each trained to
distinguish a single entity, and showed
its application to gene name identiﬁca-
tion (GNI). The rationale for our approach,
which we named “one model per entity”
(OMPE), was that increasing the num-
ber of models would make the learn-
ing task easier for each individual model.
Our training strategy leveraged freely-
available database annotations instead of
manually-annotated corpora. While its
performance in our proof-of-concept was
disappointing, we believe that
there is
enough room for improvement that such
approaches could reach competitive per-
formance while eliminating the cost of
creating costly training corpora.

1 Background
Recognizing names in text is a longstanding task
in natural language processing (NLP) known as
named-entity recognition (NER). In biomedical
text mining (or BioNLP), the focus of NER is on
certain technical names (terms) such as those of
chemical compounds, genes, species and anatom-
ical parts. Recognizing such names alone, how-
ever, is of limited application as, in practice, they
often need to be linked to other facts. This can
be done by ﬁrst mapping them to unique name
identiﬁers—a task known as normalization or
grounding. Recognizing and normalizing names
of genes and gene products,
in particular, has
drawn much attention from the BioNLP commu-
nity (Leser and Hakenberg, 2005). These names
are usually considered a single class of terms
due to their overlapping vocabularies (Hatzivas-
siloglou et al., 2001). Thus, here we refer to them
as simply gene names.

The tasks of gene name recognition (GNR) and
gene name normalization (GNN) involve, respec-
tively, the recognition and normalization of gene
names found in text. Gene name identiﬁcation
(GNI) is the combination of gene name recogni-
tion and normalization (GNR + GNN) (see the
framework by Krauthammer and Nenadic (2004)).
State-of-the-art GNI methods involve machine
learning algorithms, such as conditional random
ﬁelds (CRF), trained under supervised learning.
Supervised learning requires gold-standard train-
ing and testing sets, which for GNI typically are
sets of documents (corpora) that have been man-
ually annotated for gene names by expert cura-
tors. Several community challenges have been or-
ganized to foster the improvement of GNI algo-
rithms (Morgan et al., 2008; Lu et al., 2011). How-
ever, despite such efforts, even the best algorithms
suffer from an important weakness. Namely that
their performance has been shown to degrade out-
side of their training and testing corpora, decaying
to levels barely above those of rule-based systems
involving dictionary-matching rules together with
ﬁltering of noisy names (Rebholz-Schuhmann
et al., 2013; Rodriguez-Esteban, 2016b,a).

It has been suggested that the shortcomings of
current GNI machine learning algorithms could
be addressed in two ways:
(1) by training
models with different, diverse corpora (Rebholz-
Schuhmann et al., 2013) (see, in that respect, the
work of Kaewphan et al. (2016) with cell line
names), and (2) by using “domain adaptation”
techniques, which consist in adapting machine
learning models to the characteristics of the in-
put text. The limited size and number of exist-
ing gold-standard corpora, and the cost of creating
new ones, represent, however, a bottleneck for (1).
For (2), experiments with domain adaptation in
biomedical text have led, thus far, only to modest
improvements in performance (Miwa et al., 2012).
Here we describe an alternative approach that

50

can be applied to problems that require the iden-
tiﬁcation of large but ﬁnite sets of entities, par-
ticularly in biomedicine. To begin with, instead
of using a gold-standard corpus as training set, we
propose utilizing the wealth of manual annotations
that currently exist in biomedical databases.
In-
deed, several freely-available databases provide a
growing number of annotations concerning gene
name identiﬁers associated to biomedical docu-
ments. The main drawback of these annotations is
that they are weakly labeled, as they do not specify
the precise location in which the genes are men-
tioned within the documents. However, there are
ways to infer these locations (Jain et al., 2016).

While past GNI studies have not leveraged an-
notations from biomedical databases, there are ex-
amples of their use for GNN (Wermter et al., 2009;
Zwick, 2015; Chen et al., 2015).
In these stud-
ies contextual features were created out of the an-
notated biomedical documents to resolve ambigu-
ous gene mentions. Besides for GNN, weakly-
labeled database annotations have been used in
BioNLP for identifying protein-speciﬁc residues
(Ravikumar et al., 2012) and annotating Med-
line abstracts with Gene Ontology terms (Gob-
eill et al., 2013).
In another example, Furrer et
al. (2014) used a biomedical database called Bi-
oGRID (Chatr-Aryamontri et al., 2015) for the
purpose of training and testing an algorithm for
extracting protein-protein interactions (PPI).

Leveraging database annotations for GNI is not
straightforward. We have implemented our ap-
proach in a way that, as far as we can tell,
has not been described in the NER literature be-
fore (biomedical or otherwise). Our method in-
volves training many machine learning models,
each model trained to identify a single entity (i.e.
a single gene) rather than, as it is commonplace,
training one or a handful of models to identify all
entities. We call this approach “one model per en-
tity” (OMPE).

2 Methods

As building block for our OMPE system we used
BANNER (Leaman and Gonzalez, 2008), which
is a machine learning algorithm for NER built on
CRF. While BANNER is based on a generic model
that can be trained to identify any class of terms,
it has shown state-of-the-art performance in GNR
(Kabiljo et al., 2009). Our strategy consists in us-
ing multiple BANNER models, each model being

responsible for detecting the mentions of a single
gene. That means that a gene name mention that
is recognized by a BANNER model can be auto-
matically mapped to the gene for which the model
was trained.

2.1 Training set
To create our
training set we built ﬁrst a
database of positive training examples contain-
ing sentences that mention gene names. Each
sentence in the database was associated to a
gene identiﬁer (NCBI Gene ID), correspond-
ing to a gene mentioned in the sentence, and
to a document
identiﬁer (PubMed ID), corre-
sponding to the document source of the sen-
The {NCBI Gene ID, PubMed ID}
tence.
pairs came from the following publicly-available
databases:
gene2pubmed, UniProt, BioGRID
(Chatr-Aryamontri et al., 2015) and Gene Refer-
ence into Function (GeneRIF) (see Table 1).

Source
gene2pubmed
UniProt
BioGRID
GeneRIF

Genes Documents Mentions
1 087 465
34 004
21 383
68 966
66 358
11 832
17 462
641 354

493 620
22 539
23 925
386 927

Table 1: Statistics of the different datasets used.

Because these databases do not specify the loca-
tion of the gene mentions in the source documents,
we retrieved each source document from the Med-
line baseline 2015 and attempted to ﬁnd their loca-
tions. In order to do that we leveraged gene names
and synonyms from the NCBI Gene database.
This database, however, does not include all the
gene name variations and synonyms that authors
use in practice (Hirschman et al., 2002; Liu et al.,
2006). To increase recall we therefore expanded
the list of gene names and synonyms following
Schuemie et al. (2007). By using this expanded list
to look up gene names in the source documents we
created a set of positive examples for each of the
genes annotated in the aforementioned databases.
For training (and testing) we only considered
genes for which we had at least 32 positive ex-
amples (this cut-off was a compromise between
coverage and amount of training data available),
which totaled 2180 with a median of 281 positive
examples per gene. These genes covered approxi-
mately 80% of all gene mentions appearing in the
test corpus.

51

Negative training examples were selected ac-
cording to different strategies. First, we created
certain modiﬁed versions of the positive examples.
Modiﬁcations consisted in the deletion of words
within the gene names that made reference to a
certain function, such as receptor, inhibitor, en-
hancer. For example, while TNF-α receptor refers
to gene ID 7132, TNF-α corresponds to gene ID
7124.

The second type of negative examples that we
selected consisted in positive examples belonging
to genes that share synonyms. For example, the
gene name FAT may refer to gene ID 2195 or
948. Thus, positive examples for gene 948 can be
used as negative examples for gene 2195. Finally,
we included as negative examples randomly se-
lected sentences from the English Wikipedia (not
from any particular domain) and positive exam-
ples from randomly selected genes.

2.2 Test set
For testing the performance of our OMPE sys-
tem we used a modiﬁed dataset based on the gold
standard from the BioCreative 2 Gene Normal-
ization (BC2GN) challenge (Morgan et al., 2008).
The BC2GN training set covers 281 abstracts and
684 gene annotations, and the testing set covers
262 abstracts and 785 gene annotations. As we
used an independent training set based on freely-
available database annotations we could employ
both BC2GN training and testing datasets to create
our BC2GNmod test dataset.

When building the BC2GNmod dataset we only
considered gene annotations for 621 unique genes
from the 1156 genes present
in the original
BC2GN datasets—those for which we had more
than 32 positive examples in our training database.
Thus, BC2GNmod contained a total of 841 human
gene annotations.

We compared our OMPE system against GNAT
(Hakenberg et al., 2008, 2011), which is a state-
of-the-art system for GNI (Rebholz-Schuhmann
et al., 2013). We evaluated the prediction qual-
ity of our system according to the number of true
positives (TP), false positives (FP) and false neg-
atives (FN), and according to precision (P), recall
(R) and F-measure (F).

2.3 Computation
We made use of two different computational con-
ﬁgurations for training and testing. First, we used
a server with 40 CPU cores at 2.4 GHz and 567 GB

RAM. This server was used for both generating
the training set and making the ﬁnal predictions.
As training the models is the most computationally
demanding task, we used a cluster computer with
164 nodes, each node possessing 2 CPUs with 12
cores (Intel Xeon Processor E5-2680 v3) and 256
GB of memory. In this conﬁguration the median
model training time was 212 seconds.

3 Results

Two versions of the OMPE system were tested and
compared against the output of GNAT. The ﬁrst
version (OMPE1) used the standard BANNER im-
plementation, in which the most probable class
is associated to every token. The second version
(OMPE2) used a modiﬁed BANNER that required
the probability of a token being a mention to be
larger than a certain threshold, which we set to
0.95.

Results for predictions over the BC2GNmod
dataset can be seen in Table 2. GNAT showed
a high performance, with a recall of 0.762 and a
precision of 0.881, corresponding to 892 TPs and
only 121 FPs. The OMPE1 system achieved, on
the other hand, a recall of 0.331 and a precision of
0.215 caused by the large number of FPs, 1413.

Method TP
892
GNAT
OMPE1
387
355
OMPE2

FP
121
1413
575

FN
278
783
815

P
.881
.215
.382

R
.762
.331
.303

F
.817
.261
.338

Table 2: Performance of the 3 different methods.

To reduce the number of FPs we set a thresh-
old to the probability of accepting a prediction
(OMPE2). By using the threshold we dramatically
reduced the number of FPs from 1413 to 575, in-
creasing the precision to 0.382 while slightly de-
creasing the recall to 0.303.

In Figure 1 we show a comparison of each
method’s individual performance.
In this ﬁgure,
the ﬁrst row compares OMPE1 and GNAT, while
the second row compares OMPE2 and GNAT. The
last row compares OMPE1 and OMPE2. Light
colors represent the individual performance of the
methods and dark colors the difference between
them. The ﬁrst and second column show the pre-
cision and recall, respectively. Genes were or-
dered according to performance differences be-
tween methods.

52

Figure 2: Cumulative frequency distribution of
genes at each precision and recall level. (A) Re-
call for prediction of genes in the test dataset. (B)
Precision for prediction of genes in the test dataset.
(C) Precision for algorithms focused on the genes
known to be present in the test dataset.

without the need for retraining the entire sys-
tem (all models in our case), unlike in interac-
tive single-model approaches such as tagtog (Ce-
juela et al., 2014). Another advantage of OMPE
is its robustness, as it is not trained on a particular
hand-selected corpus. Thus, our results with the
BC2GNmod corpus are not biased by the training
set utilized.

A challenge for training the OMPE system is
the selection of negative examples.
It is impor-
tant to select negative examples that are as simi-
lar as possible to the positive examples, meaning
examples that are closest to the class separation
boundary—analogous to what support vectors rep-
resent for support vector machines (SVMs). One
of our approach’s limitations is its reduced recall
due to the low number of positive examples that
exist for many genes. Such genes are, on the other
hand, less likely to be mentioned in the biomedical
literature and, as biomedical databases continue to
grow, the number of positive examples for those
genes will keep increasing as well.

Finally, our focus was only on human genes.
The identiﬁcation of genes from additional species
would have required greater computational re-
sources. An OMPE system that covered all
protein-expressing genes would need to be trained
for around 20 000 genes (Ezkurdia et al., 2014).

Figure 1: Difference in precision and recall of the
different methods on an individual gene basis. The
ﬁrst row compares OMPE1 and GNAT, while the
second row compares OMPE2 and GNAT. The last
row compares OMPE1 and OMPE2. The ﬁrst and
second column show the precision and recall, re-
spectively. Light colors represent the individual
performance of the methods and dark colors the
difference between them. Genes were ordered ac-
cording to performance differences between meth-
ods.

Figure 1 shows that there is a set of genes in
which one of the algorithms works well but the
other algorithms do not. Moreover, the use of a
threshold in OMPE2 leads to an increase in the
precision over a large number of genes and to a de-
crease in only a small number of them. Figure 2,
on the other hand, shows the cumulative frequency
distribution of precision and recall for genes pre-
dicted in the test datasets with the different meth-
ods.

4 Discussion
An advantage of the OMPE approach is that it
allows targeted performance improvements with
respect to speciﬁc gene names. Positive exam-
ples and synonyms belonging to particularly chal-
lenging gene names can be modiﬁed interactively

53

In this sense, and in the reliance on large, grow-
ing biomedical databases, our approach has a fu-
turistic stance, meaning that it will become more
feasible with time. As “Big Computing” infras-
tructure, such as cloud computing, becomes in-
creasingly available and more powerful, it will be-
come more practical to implement systems such
as OMPE. It is important to stress that computa-
tional requirements differ greatly between training
an OMPE system and deploying it for prediction,
which requires far lower computational power.

Beyond the GNI example shown here, OMPE
can be used to identify other types of entities
with a ﬁnite cardinality, such as (in the BioNLP
ﬁeld) diseases, cell types, cell lines and anatomical
parts. We have focused here on GNI because it has
been already widely investigated and has multiple
applications, such as the tracking of biomedical
facts and trends (Cokol and Rodriguez-Esteban,
2008; Cokol et al., 2007; Rodriguez-Esteban and
Loging, 2013). Beyond NER, the OMPE approach
could also be applied to other classiﬁcation prob-
lems in which the class cardinality is below a
computationally-feasible threshold. The rationale
would again be that increasing the number of mod-
els could ease (”relieve”) the learning task to each
individual model.

5 Conclusion

In this study we have shown a new approach for
GNI that takes advantage of the decreasing costs
of computing and the increasing availability of an-
notated data to train hundreds of machine learn-
ing models. Our proof of concept did not reach
acceptable performance levels but, due to the fact
that there remains ample room for potential im-
provements, such strategies could become com-
petitive for GNI and other domains in the future.
Following the remarks from Halevy et al. (2009)
in “The unreasonable effectiveness of data,” we
should learn to “use available large-scale data
rather than hoping for annotated data that isn’t
available.”

References

JM Cejuela, P McQuilton, L Ponting, SJ Mary-
gold, R Stefancsik, GH Millburn, B Rost,
and FlyBase Consortium.
tagtog:
interactive
anno-
in PLOS full-text
tation of gene mentions

text-mining-assisted

2014.

and

articles.
https://doi.org/10.1093/database/bau033.

Database

(Oxford)

0(bau033).

A Chatr-Aryamontri, B J Breitkreutz, R Oughtred,
L Boucher, S Heinicke, D Chen, C Stark,
A Breitkreutz, N Kolas, L O’Donnell, T Reg-
uly, J Nixon, L Ramage, A Winter, A Sellam,
C Chang, J Hirschman, C Theesfeld, J Rust,
M S Livstone, K Dolinski, and M Tyers. 2015.
The BioGRID interaction database: 2015 update.
Nucleic Acids Res 43(Database issue):D470–478.
https://doi.org/10.1093/nar/gku1204.

G Chen,

J Sun,
J Zhao, T Cohen, C Tao,
H Xu, E V Bernstam, A Lawson,
J Zeng,
A M Johnson, V Holla, A M Bailey, H Lara-
Guerra, B Litzenburger, F Meric-Bernstam, and
W Jim Zheng. 2015. Using ontology ﬁngerprints
to disambiguate gene name entities in the biomed-
ical literature. Database (Oxford) 2015:bav034.
https://doi.org/10.1093/database/bav034.

M Cokol and R Rodriguez-Esteban. 2008.

Vi-
sualizing evolution and impact of biomedical
ﬁelds.
J Biomed Inform 41(6):1050–1052.
https://doi.org/10.1016/j.jbi.2008.05.002.

M Cokol, R Rodriguez-Esteban, and A Rzhetsky. 2007.
A recipe for high impact. Genome Biol 8(5):406.
https://doi.org/10.1186/gb-2007-8-5-406.

I Ezkurdia, D Juan,

J Rodriguez, A Frankish,
M Diekhans, J Harrow, J Vazquez, A Valencia, and
M Tress. 2014. Multiple evidence strands suggest
that there may be as few as 19 000 human protein-
coding genes. Hum Mol Genet 23(22):5866–5878.
https://doi.org/10.1093/hmg/ddu309.

L Furrer, S Clematide, H Marques, R Rodriguez-
Esteban, M Romacker, and F Rinaldi. 2014.
Collection-wide extraction of protein-protein in-
teractions.
6th International Symposium on
Semantic Mining in Biomedicine pages 61–66.
https://doi.org/10.5167/uzh-101472.

J Gobeill,

E Pasche, D Vishnyakova,

P Ruch. 2013.
data-driven go category assignment
while
increases.
https://doi.org/10.1093/database/bat041.

and
Managing the data deluge:
improves
functional
annotation
(Oxford) 2013:bat041.

of
Database

complexity

J Hakenberg, M Gerner, M Haeussler,

I Solt,
C Plake, M Schroeder, G Gonzalez, G Ne-
nadic, and C M Bergman. 2011.
The GNAT
and remote gene mention
library for
normalization.
Bioinformatics 27:2769–2771.
https://doi.org/10.1093/bioinformatics/btr455.

local

J Hakenberg, C Plake, R Leaman, M Schroeder, and
G Gonzalez. 2008.
Inter-species normalization of
gene mentions with GNAT. Bioinformatics 24:126–
132. https://doi.org/10.1093/bioinformatics/btn299.

54

A Halevy,

P Norvig,

unreasonable
Intelligent

The
IEEE
https://doi.org/10.1109/MIS.2009.36.

effectiveness

and F Pereira. 2009.
data.
of
24(2):8–12.

Systems

V Hatzivassiloglou, P A Dubou, and A Rzhet-
Disambiguating proteins, genes,
in text:
learning ap-
Bioinformatics 17 Suppl 1:S97–106.

sky. 2001.
and rna
proach.
https://doi.org/10.1093/bioinformatics/17.suppl 1.S97.

a machine

L Hirschman, AA Morgan, and AS Yeh. 2002.
Rutabaga by any other name: extracting biolog-
ical names.
J Biomed Inform 35(4):247–259.
https://doi.org/10.1016/S1532-0464(03)00014-5.

S Jain, K R, T T Kuo, S Bhargava, G Lin,
and C N Hsu. 2016. Weakly supervised learn-
ing of biomedical information extraction from cu-
rated data.
BMC Bioinformatics 17 Suppl 1:1.
https://doi.org/10.1186/s12859-015-0844-1.

R Kabiljo, A B Clegg,

2009.
ods
from free text.
https://doi.org/10.1186/1471-2105-10-233.

and A Shepherd.
of meth-
interactions
BMC Bioinformatics 10:233.

assessment
gene/protein

A realistic
extracting

for

S Kaewphan, S Van Landeghem, T Ohta, Y Van de
Cell
the
in
synthetic
can-
Bioinformatics 32(2):276–282.

Peer, F Ginter, and S Pyysalo. 2016.
line
name
identiﬁcation
cer
https://doi.org/10.1093/bioinformatics/btv570.

recognition
of

support
lethality

from text.

of
in

M Krauthammer and G Nenadic. 2004.
biomedical

identiﬁcation
ture.
https://doi.org/10.1016/j.jbi.2004.08.004.

Term
litera-
Inform 37(6):512–526.

J Biomed

the

in

R Leaman and G Gonzalez. 2008. BANNER: an exe-
cutable survey of advances in biomedical named en-
tity recognition. Pac Symp Biocomput pages 652–
663. https://doi.org/10.1142/9789812776136 0062.

U Leser and J Hakenberg. 2005. What makes a
gene name?
Named entity recognition in the
biomedical literature. Brief Bioinform 6(4):357–
369. https://doi.org/10.1093/bib/6.4.357.

2006.

H Liu, ZZ Hu, M Torii, C Wu, and C Fried-
of
tag-
J Am Med Inform Assoc 13(5):497–507.

man.
dictionary-based
ging.
https://doi.org/10.1197/jamia.M2085.

Quantitative
protein

assessment

named

entity

Z Lu, H Y Kao, C H Wei, M Huang, J Liu, C J Kuo,
C N Hsu, R T Tsai, H J Dai, N Okazaki, H C Cho,
M Gerner, I Solt, S Agarwal, F Liu, D Vishnyakova,
P Ruch, M Romacker, F Rinaldi, S Bhattacharya,
P Srinivasan, H Liu, M Torii, S Matos, D Cam-
pos, K Verspoor, K M Livingston, and W J Wilbur.
2011.
The gene normalization task in BioCre-
ative III.
BMC Bioinformatics 12 Suppl 8:S2.
https://doi.org/10.1186/1471-2105-12-S8-S2.

M Miwa, P Thompson, and S Ananiadou. 2012.
Boosting automatic event extraction from the
literature using domain adaptation and corefer-
ence resolution.
Bioinformatics 28:1759–1765.
https://doi.org/10.1093/bioinformatics/bts237.

A A Morgan, Z Lu, X Wang, A M Cohen, J Fluck,
P Ruch, A Divoli, K Fundel, R Leaman, J Haken-
berg, C Sun, H H Liu, R Torres, M Krauthammer,
W W Lau, H Liu, C N Hsu, M Schuemie, K B Co-
hen, and L Hirschman. 2008. Overview of BioCre-
ative II gene normalization. Genome Biol 9 Suppl
2:S3. https://doi.org/10.1186/gb-2008-9-s2-s3.

K Ravikumar, H Liu, J D Cohn, M E Wall, and K Ver-
spoor. 2012. Literature mining of protein-residue
associations with graph rules learned through dis-
tant supervision. J Biomed Semantics 3 Suppl 3:S2.
https://doi.org/10.1186/2041-1480-3-S3-S2.

D Rebholz-Schuhmann, S Kafkas, J H Kim, C Li,
A Jimeno Yepes, R Hoehndorf, R Backofen, and
I Lewin. 2013.
Evaluating gold standard cor-
pora against gene/protein tagging solutions and
lexical
J Biomed Semantics 4:28.
https://doi.org/10.1186/2041-1480-4-28.

resources.

R Rodriguez-Esteban. 2016a. Additional knowledge-
based analysis approaches.
In W Loging, ed-
itor, Bioinformatics and Computational Biology
in Drug Discovery and Development, Cambridge
University Press, Cambridge, United Kingdom.
https://doi.org/10.1017/CBO9780511989421.011.

R Rodriguez-Esteban. 2016b.

is text mining?

Understanding hu-
man disease knowledge through text mining:
What
In W Loging, ed-
itor, Bioinformatics and Computational Biology
in Drug Discovery and Development, Cambridge
University Press, Cambridge, United Kingdom.
https://doi.org/10.1017/cbo9780511989421.004.

R Rodriguez-Esteban and W T Loging. 2013.
re-
29:2918–2924.

Quantifying
search.
https://doi.org/10.1093/bioinformatics/btt505.

Bioinformatics

of medical

complexity

the

M J Schuemie, B Mons, M Weeber, and J A Kors.
2007. Evaluation of techniques for increasing re-
call in a dictionary approach to gene and protein
name identiﬁcation. J Biomed Inform 40:316–324.
https://doi.org/10.1016/j.jbi.2006.09.002.

and U Hahn. 2009.
J Wermter, K Tomanek,
gene
normaliza-
Bioinformatics 25:815–821.

High-performance
tion with GeNo.
https://doi.org/10.1093/bioinformatics/btp071.

name

M Zwick. 2015.

Automated curation of gene
name normalization results using the Konstanz in-
formation miner.
J Biomed Inform 53:58–64.
https://doi.org/10.1016/j.jbi.2014.08.016.

