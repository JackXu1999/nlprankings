



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 698–707
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1065

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 698–707
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1065

Sequence-to-Dependency Neural Machine Translation

Shuangzhi Wu†∗, Dongdong Zhang‡ , Nan Yang‡ , Mu Li‡ , Ming Zhou‡
†Harbin Institute of Technology, Harbin, China

‡Microsoft Research
{v-shuawu, dozhang, nanya, muli, mingzhou}@microsoft.com

Abstract

Nowadays a typical Neural Machine
Translation (NMT) model generates trans-
lations from left to right as a linear
sequence, during which latent syntactic
structures of the target sentences are not
explicitly concerned. Inspired by the suc-
cess of using syntactic knowledge of tar-
get language for improving statistical ma-
chine translation, in this paper we propose
a novel Sequence-to-Dependency Neural
Machine Translation (SD-NMT) method,
in which the target word sequence and its
corresponding dependency structure are
jointly constructed and modeled, and this
structure is used as context to facilitate
word generations. Experimental results
show that the proposed method signifi-
cantly outperforms state-of-the-art base-
lines on Chinese-English and Japanese-
English translation tasks.

1 Introduction

Recently, Neural Machine Translation (NMT)
with the attention-based encoder-decoder frame-
work (Bahdanau et al., 2015) has achieved sig-
nificant improvements in translation quality of
many language pairs (Bahdanau et al., 2015; Lu-
ong et al., 2015a; Tu et al., 2016; Wu et al., 2016).
In a conventional NMT model, an encoder reads
in source sentences of various lengths, and trans-
forms them into sequences of intermediate hidden
vector representations. After weighted by atten-
tion operations, combined hidden vectors are used
by the decoder to generate translations. In most of
cases, both encoder and decoder are implemented
as recurrent neural networks (RNNs).

∗Contribution during internship at Microsoft Research.

Many methods have been proposed to further
improve the sequence-to-sequence NMT model
since it was first proposed by Sutskever et al.
(2014) and Bahdanau et al. (2015). Previous work
ranges from addressing the problem of out-of-
vocabulary words (Jean et al., 2015), designing at-
tention mechanism (Luong et al., 2015a), to more
efficient parameter learning (Shen et al., 2016),
using source-side syntactic trees for better encod-
ing (Eriguchi et al., 2016) and so on. All these
NMT models employ a sequential recurrent neu-
ral network for target generations. Although in
theory RNN is able to remember sufficiently long
history, we still observe substantial incorrect trans-
lations which violate long-distance syntactic con-
straints. This suggests that it is still very challeng-
ing for a linear RNN to learn models that effec-
tively capture many subtle long-range word de-
pendencies. For example, Figure 1 shows an in-
correct translation related to the long-distance de-
pendency. The translation fragment in italic is lo-
cally fluent around the word is, but from a global
view the translation is ungrammatical. Actually,
this part of translation should be mostly affected
by the distant plural noun foreigners rather than
words Venezuelan government nearby.

Fortunately, such long-distance word corre-
spondence can be well addressed and modeled by
syntactic dependency trees. In Figure 1, the head
word foreigners in the partial dependency tree (top
dashed box) can provide correct structural con-
text for the next target word, with this informa-
tion it is more likely to generate the correct word
will rather than is. This structure has been suc-
cessfully applied to significantly improve the per-
formance of statistical machine translation (Shen
et al., 2008). On the NMT side, introducing tar-
get syntactic structures could help solve the prob-
lem of ungrammatical output because it can bring
two advantages over state-of-the-art NMT models:

698

https://doi.org/10.18653/v1/P17-1065
https://doi.org/10.18653/v1/P17-1065


a) syntactic trees can be used to model the gram-
matical validity of translation candidates; b) par-
tial syntactic structures can be used as additional
context to facilitate future target word prediction.

Source : 他还说 , 来委外国人若攻击委内瑞拉政府
会面临严重后果 , 将被驱逐出境 .

partial tree

decoder

Ref :    He added that foreign visitors to Venezuela who criticize 
the Venezuelan government will face serious consequences 
and will be deported .

NMT : He also said that foreigners to Venezuela who attack 
the Venezuelan government is facing serious consequences, 
will be deported . 

…

foreigners to Venezuela who  attack   the    Venezuelan  government

attack the    Venezuelan  government

…

is

ungrammatical structure

Figure 1: Dependency trees help the prediction of
the next target word. “NMT” refers to the trans-
lation result from a conventional NMT model,
which fails to capture the long distance word re-
lation denoted by the dashed arrow.

However, it is not trivial to build and leverage
syntactic structures on the target side in current
NMT framework. Several practical challenges
arise:

(1) How to model syntactic structures such as
dependency parse trees with recurrent neural net-
work;

(2) How to efficiently perform both target word
generation and syntactic structure construction
tasks simultaneously in a single neural network;

(3) How to effectively leverage target syntactic
context to help target word generation.

To address these issues, we propose and empir-
ically evaluate a novel Sequence-to-Dependency
Neural Machine Translation (SD-NMT) model in
our paper. An SD-NMT model encodes source in-
puts with bi-directional RNNs and associates them
with target word prediction via attention mecha-
nism as in most NMT models, but it comes with
a new decoder which is able to jointly generate
target translations and construct their syntactic de-
pendency trees. The key difference from conven-
tional NMT decoders is that we use two RNNs,
one for translation generation and the other for de-
pendency parse tree construction, in which incre-
mental parsing is performed with the arc-standard
shift-reduce algorithm proposed by Nivre (2004).

We will describe in detail how these two RNNs
work interactively in Section 3.

We evaluate our method on publicly avail-
able data sets with Chinese-English and Japanese-
English translation tasks. Experimental results
show that our model significantly improves trans-
lation accuracy over the conventional NMT and
SMT baseline systems.

2 Background

2.1 Neural Machine Translation

As a new paradigm to machine translation, NMT
is an end-to-end framework (Sutskever et al.,
2014; Bahdanau et al., 2015) which directly mod-
els the conditional probability P (Y |X) of target
translation Y = y1,y2,...,yn given source sentence
X = x1,x2,...,xm. An NMT model consists of two
parts: an encoder and a decoder. Both of them
utilize recurrent neural networks which can be a
Gated Recurrent Unit (GRU) (Cho et al., 2014)
or a Long Short-Term Memory (LSTM) (Hochre-
iter and Schmidhuber, 1997) in practice. The en-
coder bidirectionally encodes a source sentence
into a sequence of hidden vectorsH = h1,h2,...,hm
with a forward RNN and a backward RNN. Then
the decoder predicts target words one by one with
probability

P (Y |X) =
n∏

j=1

P (yj|y<j, H) (1)

Typically, for the jth target word, the probability
P (yj |y<j , H) is computed as

P (yj|y<j, H) = g(sj, yj−1, cj) (2)

where g is a nonlinear function that outputs the
probability of yj , and sj is the RNN hidden state.
The context cj is calculated at each timestamp j
based on H by the attention network

cj =
m∑

k=1

ajkhk (3)

ajk =
exp(ejk)∑m
i=1 exp(eji)

(4)

ejk = v
T
a tanh(Wasj−1 + Uahk) (5)

where va, Wa, Ua are the weight matrices. The
attention mechanism is effective to model the cor-
respondences between source and target.

699



2.2 Dependency Tree Construction
We use a shift-reduce transition-based dependency
parser to build the syntactic structure for the target
language in our work. Specially, we adopt the arc-
standard algorithm (Nivre, 2004) to perform incre-
mental parsing during the translation process. In
this algorithm, a stack and a buffer are maintained
to store the parsing state over which three kinds of
transition actions are applied. Let w0 and w1 be
two topmost words in the stack, and w̄ be the cur-
rent new word in a sequence of input, three transi-
tion actions are described as below.

• Shift(SH) : Push w̄ to the stack.

• Left-Reduce(LR(d)) : Link w0 and w1 with
dependency label d as w0

d−→w1, and reduce
them to the head w0.

• Right-Reduce(RR(d)) : Link w0 andw1 with
dependency label d as w0

d←−w1, and reduce
them to the head w1.

During parsing, an specific structure is used to
record the dependency relationship between dif-
ferent words of input sentence. The parsing fin-
ishes when the stack is empty and all input words
are consumed. As each word must be pushed to
the stack once and popped off once, the number
of actions needed to parse a sentence is always
2n, where n is the length of the sentence (Nivre,
2004). Because each valid transition action se-
quence corresponds to a unique dependency tree,
a dependency tree can also be equivalently repre-
sented by a sequence of transition actions.

3 Sequence-to-Dependency Neural
Machine Translation

An SD-NMT model is an extension to the con-
ventional NMT model augmented with syntactic
structural information of target translation. Given
a source sentenceX = x1,x2,..,xm, its target trans-
lation Y = y1,y2,..,yn and Y ’s dependency parse
tree T , the goal of the extension is to enable us to
compute the joint probability P (Y, T |X). As in
most structural learning tasks, the full prediction
of Y and T is further decomposed into a chain of
smaller predictions. For translation Y , it is gen-
erated in the left-to-right order as y1, y2, .., yn fol-
lowing the way in a normal sequence-to-sequence
model. For Y ’s parse tree T , instead of directly
modeling the tree itself, we predict a parsing ac-
tion sequence A which can map Y to T . Thus at

top level our SD-NMT model can be formulated
as

P (Y, T |X) = P (Y,A|X)
= P (y1y2..yn, a1, a2..al|X)(6)

where A = a1,a2,..,aj ,..,al 1 with length l (l =
2n), aj ∈ {SH,RR(d),LR(d)}2.

Two recurrent neural networks, Word-RNN and
Action-RNN, are used to model generation pro-
cesses of translation sequence Y and parsing ac-
tion sequence A respectively. Figure 2 shows an
example how translation Y and its parsing actions
are predicted step by step.

<s>

Word RNN

Action RNN

𝑎𝑟𝑒 𝑦𝑜𝑢

SH

𝑤ℎ𝑜 𝑎𝑟𝑒

<s> SH SH

LR

LR

RRSH

SH

𝑤ℎ𝑜

𝑎𝑟𝑒

𝑦𝑜𝑢

SH

𝑤ℎ𝑜

…

…

Figure 2: Decoding example of our SD-NMT
model for target sentence “who are you” with tran-
sition action sequence “SH SH LR SH RR”. The
ending symbol EOS is omitted.

Because the lengths of Word-RNN and Action-
RNN are different, they are designed to work in
a mutually dependent way: a target word is only
allowed to be generated when the SH action is
predicted in the action sequence. In this way, we
can perform incremental dependency parsing for
translation Y and at the same time track the par-
tial parsing status through the translation genera-
tion process.

For notational clarity, we introduce a virtual
translation sequence Ŷ =ŷ1,ŷ2,..,ŷj ,..,ŷl for Word-
RNN which has the same length l with transition
action sequence. ŷj is defined as

ŷj =

{
yvj δ(SH, aj) = 1
yvj−1 δ(SH, aj) = 0

where δ(SH, aj) is 1 when aj = SH, otherwise
0. vj is the index of Y , computed by vj =∑j

i=1 δ(SH, ai). Apparently the mapping from Ŷ
1In the rest of this paper, aj represents the transition ac-

tion, rather than the attention weight in Equation 4.
2RR(d) refers to a set of RR actions augmented with de-

pendency labels so as to LR(d).

700



a

…𝐸𝑤0 𝑏0𝑙

𝐾𝑗

𝐸𝑤1

𝐸𝑤0 𝐸𝑤0𝑙

𝑏1𝑟

𝐸𝑤1 𝐸𝑤1𝑟

𝑤0𝑤1… stack

parsing configuation

… 𝑤1𝑙 … 𝑤1𝑟 … 𝑤0𝑙 … 𝑤0𝑟

𝑢𝑛𝑖𝑔𝑟𝑎𝑚

𝑏𝑖𝑔𝑟𝑎𝑚

construction of 𝐾𝑗

Attention

partial tree

 𝑦1  𝑦2  𝑦3  𝑦4 …  𝑦𝑗−1

𝑇𝑖𝑚𝑒𝑠𝑡𝑎𝑚𝑝 1 2 3 4 𝑗 − 1

𝑎𝑗−1

𝑎𝑗

𝑎1 𝑎2 𝑎3 𝑎𝑗−2

⊕

δ

𝑎1 𝑎2 𝑎3 𝑎4 … 𝑎𝑗−1

 𝑦0

𝑎0

𝑥1 𝑥2 𝑥3 … 𝑥𝑚

0

1Word RNN

Action RNN

Encoder

𝑦1 𝑦2 𝑦3 … 𝑦𝑣𝑗−1

…

…

…

 𝑦1  𝑦3  𝑦𝑗−2  𝑦𝑗−1 𝑦2

𝑌

 𝑌

𝐾𝑗

 𝑦𝑗

𝑦𝑣𝑗

𝒋

(𝑎) (𝑏)

Figure 3: (a) is the overview of SD-NMT model. The dashed arrows mean copying previous recurrent
state or word. The two RNNs use the same source context for prediction. aj ∈ {SH,RR(d),LR(d)}. The
bidirection arrow refers to the interaction between two RNNs. (b) shows the construction of syntactic
context. The gray box means the concatenation of vectors

to Y is deterministic, and Y can be easily derived
given Ŷ and A.

With the notation of Ŷ , the sequence probability
of Y and A can be written as

P (A|X, Ŷ<l) =
l∏

j=1

P (aj|a<j, X, Ŷ<j) (7)

P (Ŷ |X,A≤l) =
l∏

j=1

P (ŷj|ŷ<j, X,A≤j)δ(SH,aj)

(8)

where Ŷ<j refers to the subsequence
ŷ1, ŷ2, .., ŷj−1, and A≤j to a1, a2, .., aj . Based on
Equation 7 and 8, the overall joint model can be
computed as

P (Y, T |X) = P (A|X, Ŷ<l)× P (Ŷ |X,A≤l)
(9)

As we have two RNNs in our model, the termina-
tion condition is also different from a conventional
NMT model. In decoding, we maintain a stack
to track the parsing configuration, and our model
terminates once the Word-RNN predicts a special
ending symbol EOS and all the words in the stack
have been reduced.

Figure 3 (a) gives an overview of our SD-NMT
model. Due to space limitation, the detailed inter-
connections between two RNNs are only illus-
trated at timestamp j. The encoder of our model

follows standard bidirectional RNN configuration.
At timestamp j during decoding, our model first
predicts an action aj by Action-RNN, then Word-
RNN checks the condition gate δ according to aj .
If aj = SH, the Word-RNN will generate a new
state (solid arrow) and predict a new target word
yvj , otherwise it just copies previous state (dashed
arrow) to the current state. For example, at times-
tamp 3, a3 6= SH, the state of Word-RNN is copied
from its previous one. Meanwhile, ŷ3 = y2 is used
as the immediate proceeding word in translation
history.

When computing attention scores, we extend
Equation 5 by replacing the decoder hidden state
with the concatenation of Word-RNN hidden state
s and Action-RNN hidden state s′ (gray boxes in
Figure 3). The new attention score is then updated
as

ejk = v
T
a tanh(Wa[sj−1; s

′
j−1] + Uahk) (10)

3.1 Syntactic Context for Target Word
Prediction

Syntax has been proven useful for sentence gen-
eration task (Dyer et al., 2016). We propose to
leverage target syntax to help translation genera-
tion. In our model, the syntactic context Kj at
timestamp j is defined as a vector which is com-
puted by a feed-forward network based on current

701



parsing configuration of Action-RNN. Denote that
w0 and w1 are two topmost words in the stack, w0l
and w1l are their leftmost modifiers in the partial
tree,w0r andw1r their rightmost modifiers respec-
tively. We define two unigram features and four
bigram features. The unigram features are w0 and
w1 which are represented by the word embedding
vectors. The bigram features are w0w0l, w0w0r,
w1w1l and w1w1r. Each of them is computed by
bhc = tanh(WbEwh + UbEwhc), h ∈ {0, 1},
c ∈ {l, r}. These kinds of feature template have
beeb proven effective in dependency parsing task
(Zhang and Clark, 2008). Based on these features,
the syntactic context vector Kj is computed as

Kj = tanh(Wk[Ew0;Ew1] + Uk[b0l; b0r; b1l; b1r])
(11)

where Wk, Uk, Wb, Ub are the weight matrices,
E stands for the embedding matrix. Figure 2 (b)
gives an overview of the construction of Kj . Note
that zero vector is used for padding the words
which are not available in the partial tree, so that
all the K vectors have the same input size in com-
putation.

Adding Kj to Equation 2, the probabilities of
transition action and word in Equation 7 and 8 are
then updated as

P (aj|a<j, X, Ŷ<j) = g(s′j, aj−1, cj,Kj) (12)
P (ŷj|ŷ<j, X,A≤j) = g(sj, ŷj−1, cj,Kj) (13)

After each prediction step in Word-RNN and
Action-RNN, the syntax context vector K will be
updated accordingly. Note that K is not used to
calculate the recurrent states s in this work.

3.2 Model Training and Decoding
For SD-NMT model, we use the sum of log-
likelihoods of word sequence and action sequence
as objective function for training algorithm, so that
the joint probability of target translations and their
parsing trees can be maximized:

J(θ) =
∑

(X,Y,A)∈D
log P (A|X, Ŷ<l)+

log P (Ŷ |X,A≤l) (14)

We also use mini-batch for model training. As
the target dependency trees are known in the bilin-
gual corpus during training, we pre-compute the
partial tree state and syntactic context at each time

stamp for each training instance. Thus it is easy for
the model to process multiple trees in one batch.

In the decoding process of an SD-NMT model,
the score of each search path is the sum of log
probabilities of target word sequence and transi-
tion action sequence normalized by the sequence
length:

score =
1

l

l∑

j=1

log P (aj |a<j , X, Ŷ<j)+

1

n

l∑

j=1

δ(SH, aj) log P (ŷj |ŷ<j , X,A≤j) (15)

where n is word sequence length and l is action
sequence length.

4 Experiments

The experiments are conducted on the Chinese-
English task as well as the Japanese-English trans-
lation tasks where the same data set from WAT
2016 ASPEC corpus (Nakazawa et al., 2016) 3

is used for a fair comparison with other work. In
addition to evaluate translation performance, we
also investigate the quality of dependency parsing
as a by-product and the effect of parsing quality
against translation quality.

4.1 Setup
In the Chinese-English task, the bilingual training
data consists of a set of LDC datasets, 4 which
has around 2M sentence pairs. We use NIST2003
as the development set, and the testsets contain
NIST2005, NIST2006, NIST2008 and NIST2012.
All English words are lowercased.

In the Japanese-English task, we use top 1M
sentence pairs from ASPEC Japanese-English cor-
pus. The development data contains 1,790 sen-
tences, and the test data contains 1,812 sentences
with single reference per source sentence.

To train SD-NMT model, the target dependency
tree references are needed. As there is no golden
annotation of parse trees over the target training
data, we use pseudo parsing results as the tar-
get dependency references, which are got from an
in-house developed arc-eager dependency parser
based on work in (Zhang and Nivre, 2011).

3http://orchid.kuee.kyoto-u.ac.jp/ASPEC/
4LDC2003E14, LDC2005T10, LDC2005E83,

LDC2006E26, LDC2006E34, LDC2006E85, LDC2006E92,
LDC2003E07, LDC2002E18, LDC2005T06, LDC2003E07,
LDC2004T07, LDC2004T08, LDC2005T06

702



Settings NIST 2005 NIST 2006 NIST 2008 NIST 2012 Average
HPSMT 35.34 33.56 26.06 27.47 30.61
RNNsearch 38.07 38.95 31.61 28.95 34.39
SD-NMT\K 38.83 39.23 31.92 29.72 34.93
SD-NMT 39.38 41.81 33.06 31.43 36.42

Table 1: Evaluation results on Chinese-English translation task with BLEU% metric. The “Average”
column is the averaged result of all test sets. The numbers in bold indicate statistically significant differ-
ence (p < 0.05) from baselines.

In the neural network training, the vocabulary
size is limited to 30K high frequent words for
both source and target languages. All low fre-
quent words are normalized into a special token
unk and post-processed by following the work in
(Luong et al., 2015b). The size of word embed-
ding and transition action embedding is set to 512.
The dimensions of the hidden states for all RNNs
are set to 1024. All model parameters are initial-
ized randomly with Gaussian distribution (Glorot
and Bengio, 2010) and trained on a NVIDIA Tesla
K40 GPU. The stochastic gradient descent (SGD)
algorithm is used to tune parameters with a learn-
ing rate of 1.0. The batch size is set to 96. In the
update procedure, Adadelta (Zeiler, 2012) algo-
rithm is used to automatically adapt the learning
rate. The beam sizes for both word prediction and
transition action prediction are set to 12 in decod-
ing.

The baselines in our experiments are a phrasal
system and a neural translation system, denoted
by HPSMT and RNNsearch respectively. HPSMT
is an in-house implementation of the hierarchical
phrase-based model (Chiang, 2005), where a 4-
gram language model is trained using the mod-
ified Kneser-Ney smoothing (Kneser and Ney,
1995) algorism over the English Gigaword corpus
(LDC2009T13) plus the target data from the bilin-
gual corpus. RNNsearch is an in-house implemen-
tation of the attention-based neural machine trans-
lation model (Bahdanau et al., 2015) using the
same parameter settings as our SD-NMT model
including word embedding size, hidden vector di-
mension, beam size, as well as the same mecha-
nism for OOV word processing.

The evaluation results are reported with the
case-insensitive IBM BLEU-4 (Papineni et al.,
2002). A statistical significance test is performed
using the bootstrap resampling method proposed
by Koehn (2004) with a 95% confidence level.
For Japanese-English task, we use the official eval-

uation procedure provided by WAT 2016.5, where
both BLEU and RIBES (Isozaki et al., 2010) are
used for evaluation.

4.2 Evaluation on Chinese-English
Translation

We evaluate our method on the Chinese-English
translation task. The evaluation results over all
NIST test sets against baselines are listed in Table
1. Generally, RNNsearch outperforms HPSMT
by 3.78 BLEU points on average while SD-NMT
surpasses RNNsearch 2.03 BLUE point gains on
average, which shows that NMT models usually
achieve better results than SMT models, and our
proposed sequence-to-dependency NMT model
performs much better than traditional sequence-to-
sequence NMT model.

We also investigate the effect of syntactic
knowledge context by excluding its computation
in Equation 12 and 13. The alternative model
is denoted by SD-NMT\K. According to Table
1, SD-NMT\K outperforms RNNsearch by 0.54
BLEU points but degrades SD-NMT by 1.49
BLEU points on average, which demonstrates that
the long distance dependencies captured by the
target syntactic knowledge context, such as left-
most/rightmost children together with their depen-
dency relationships, really bring strong positive
effects on the prediction of target words.

In addition to translation quality, we compare
the perplexity (PPL) changes on the development
set in terms of numbers of training mini-batches
for RNNsearch and SD-NMT in Figure 4. We can
see that the PPL of SD-NMT is initially higher
than that of RNNsearch, but decreased to be lower
over time. This is mainly because the quality
of parse tree is too poor at the beginning which
degrades translation quality and leads to higher
PPL. After some training iterations, the SD-NMT

5http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index
.html

703



BLEU RIBES System Description
SMT Hiero 18.72 0.6511 Moses’ Hierarchical Phrase-based SMT
SMT Phrase 18.45 0.6451 Moses’ Phrase-based SMT
SMT S2T 20.36 0.6782 Moses’ String-to-Tree Syntax-based SMT
Cromieres (2016)(Single model) 22.86 - Single-layer NMT model without ensemble
Cromieres (2016)(Self-ensemble) 24.71 0.7508 Self-ensemble of 2-layer NMT model
Cromieres (2016)(4-Ensemble) 26.22 0.7566 Ensemble of 4 single-layer NMT models
RNNsearch 23.50 0.7459 Single-layer NMT model
SD-NMT 25.93 0.7540 Single-layer SD-NMT model

Table 2: Evaluation results on Japanese-English translation task.

model learns reasonable inferences of parse trees
which begins to help target word generation and
leads to lower PPL.

iter RNNsearch SD-NMT

1 39.39 46.57

2 37.78 42.5

3 33.73 37.43

4 27.4 29.21

5 27.5 26.67

6 25.09 24.22

7 24.99 23.7

8 24.1 23.5

9 23.94 24.66

10 25.92 23.19

11 24.41 23.35

12 25.67 20.38

13 24.28 21

14 23.14 18.49

15 23.73 19.57

16 20.51 17.58

17 19.58 16.43

18 20.98 17.13

19 18.43 17

20 19.25 17.31

21 18.87 16.75

22 20.18 17.57

23 19.27 16.6

24 17.8 15.2

25 17.26 15.74

26 18.76 16.58

27 17.62 15.88

14

16.5

19

21.5

24

26.5

29

31.5

34

36.5

39

41.5

44

46.5

49

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27

P
P
L

Mini-batches(×2000)

RNNsearch

SD-NMT

Figure 4: Perplexity (PPL) changes in terms of
numbers of training mini-batches.

In our experiments, the time cost of SD-NMT
is two times of that for RNNsearch due to a more
complicated model structure. But we think it is a
worthy trade to pursue high quality translations.

4.3 Evaluation on Japanese-English
Translation

In this section, we report results on the Japanese-
English translation task. To ensure fair compar-
isons, we use the same training data and follow the
pre-processing steps recommended in WAT 20166.
Table 2 shows the comparison results from 8 sys-
tems with the evaluation metrics of BLEU and
RIBES. The results in the first 3 rows are pro-
duced by SMT systems taken from the official
WAT 2016. The remaining results are produced by
NMT systems, among which the bottom two row
results are taken from our in-house NMT systems
and others refer to the work in (Cromieres, 2016;

6http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/data
PreparationJE.html

Cromieres et al., 2016) that are the competitive
NMT results on WAT 2016. According to Table
2, NMT results still outperform SMT results simi-
lar to our Chinese-English evaluation results. The
SD-NMT model significantly outperforms most
other NMT models, which shows that our pro-
posed approach to modeling target dependency
tree benefit NMT systems since our RNNsearch
baseline achieves comparable performance with
the single layer attention-based NMT system in
(Cromieres, 2016). Note that our SD-NMT gets
comparable results with the 4 single-layer ensem-
ble model in (Cromieres, 2016; Cromieres et al.,
2016). We believe SD-NMT can get more im-
provements with an ensemble of multiple models
in future experiments.

4.4 Effect of the Parsing Accuracy upon
Translation Quality

The interaction effect between dependency tree
conduction and target word generation is investi-
gated in this section. The experiments are con-
ducted on the Chinese-English task over multiple
test sets. We evaluate how the quality of depen-
dency trees affect the performance of translation.
In the decoding phase of SD-NMT, beam search
is applied to the generations of both transition and
actions as illustrated in Equation 15. Intuitively,
the larger the beam size of action prediction is, the
better the dependency tree quality is. We fix the
beam size for generating target words to 12, and
change the beam size for action prediction to see
the difference. Figure 5 shows the evaluation re-
sults of all test sets. There is a tendency for BLEU
scores to increase with the growth of action pre-
diction beam size. The reason is that the transla-
tion quality increases as the quality of dependency
tree improves, which shows the construction of de-
pendency trees can boost the generation of target

704



beamsize NIST2005 NIST2006 NIST2008 NIST2012

2 37.56 39.3 30.69 29.41

4 38.77 40.64 32.06 30.63

6 38.93 41.32 32.63 31.07

8 39.34 41.52 32.88 31.32

10 39.32 41.65 32.82 31.41

12 39.38 41.81 33.06 31.43

37.56

38.77
38.93

39.34 39.32 39.38

37

37.5

38

38.5

39

39.5

2 4 6 8 10 12

B
LE

U
(%

)

Beam size of action prediction

NIST2005

39.3

40.64

41.32
41.52 41.65

41.81

39

39.5

40

40.5

41

41.5

42

2 4 6 8 10 12

B
LE

U
(%

)

Beam size of action prediction

NIST2006

30.69

32.06

32.63
32.88 32.82

33.06

30

30.5

31

31.5

32

32.5

33

33.5

2 4 6 8 10 12

B
LE

U
(%

)

Beam size of action prediction

NIST2008

29.41

30.63

31.07
31.32 31.41 31.43

29

29.5

30

30.5

31

31.5

2 4 6 8 10 12

B
LE

U
(%

)

Beam size of action prediction

NIST2012

Figure 5: Translation performance against the
beam size of action prediction.

words, and vice versa we believe.

4.5 Quality Estimation of Dependency Tree
Construction

As a by-product, the quality of dependency trees
not only affects the performance of target word
generation, but also influences the possible down-
stream processors or tasks such as text analyses.
The direct evaluation of tree quality is not feasible
due to the unavailable golden references. So we
resort to estimating the consistency between the
by-products and the parsing results of our stand-
alone dependency parser with state-of-the-art per-
formance. The higher the consistency is, the closer
the performance of by-product is to the stand-
alone parser. To reduce the influence of ill-formed
data as much as possible, we build the evaluation
data set by heuristically selecting 360 SD-NMT
translation results together with their dependency
trees from NIST test sets where both source- and
target-side do not contain unk and have a length
of 20-30. We then take the parsing results of the
stand-alone parser for these translations as ref-
erences to indirectly estimate the quality of by-
products. We get a UAS (unlabeled attachment
score) of 94.96% and a LAS (labeled attachment
score) of 93.92%, which demonstrates that the de-
pendency trees produced by SD-NMT are much
similar with the parsing results from the stand-
alone parser.

4.6 Translation Example

In this section, we give a case study to explain
how our method works. Figure 6 shows a trans-
lation example from the NIST testsets. SMT and
RNNsearch refer to the translation results from the

baselines HPSMT and NMT. For our SD-NMT
model, we list both the generated translation and
its corresponding dependency tree. We find that
the translation of SMT is disfluent and ungram-
matical, whereas RNNsearch is better than SMT.
Although the translation of RNNsearch is locally
fluent around word “have” in the rectangle, both
its grammar is incorrect and its meaning is inaccu-
rate from a global view. The word “have” should
be in a singular form as its subject is “safety”
rather than “workers”. For our SD-NMT model,
we can see that the translation is much better than
baselines and the dependency tree is reasonable.
The reason is that after generating the word “work-
ers”, the previous subtree in the gray region is
transformed to the syntactic context which can
guide the generation of the next word as illustrated
by the dashed arrow. Thus our model is more
likely to generate the correct verb “is” with sin-
gular form. In addition, the global structure helps
the model correctly identify the inverted sentence
pattern of the former translated part and make bet-
ter choices for the future translation (“only when
.. can ..” in our translation, “only when .. will ..”
in the reference), which remains a challenge for
conventional NMT model.

5 Related Work

Incorporating linguistic knowledge into machine
translation has been extensively studied in Statistic
Machine Translation (SMT) (Galley et al., 2006;
Shen et al., 2008; Liu et al., 2006). Liu et al.
(2006) proposed a tree-to-string alignment tem-
plate for SMT to leverage source side syntactic in-
formation. Shen et al. (2008) proposed a target
dependency language model for SMT to employ
target-side structured information. These methods
show promising improvement for SMT.

Recently, neural machine translation (NMT)
has achieved better performance than SMT in
many language pairs (Luong et al., 2015a; Zhang
et al., 2016; Shen et al., 2016; Wu et al., 2016;
Neubig, 2016). In a vanilla NMT model, source
and target sentences are treated as sequences
where the syntactic knowledge of both sides is
neglected. Some effort has been done to incor-
porate source syntax into NMT. Eriguchi et al.
(2016) proposed a tree-to-sequence attentional
NMT model where source-side parse tree was
used and achieved promising improvement. In-
tuitively, adding source syntactic information to

705



[Source]         只有施工人员的安全得到了保证 , 才能继续施工 .
[Reference]   only when the safety of the workers is guaranteed will they continue with the project .
[HPSMT]        only safety is assured of construction personnel , to continue construction .
[RNNsearch] only when the safety of construction workers have been guaranteed to continue construction .
[SD-NMT]      only when the safety of the workers is guaranteed can we continue to work . 

nsubjpass

nsubj

auxpass punctadvmod

pobj

auxprep xcomp

the of

workers

safety is continue

guaranteed

can workwe

.only when

the

to

dep

aux

det

det

ccomp

Figure 6: Translation examples of SMT, RNNsearch and our SD-NMT on Chinese-English transla-
tion task. The italic words on the arrows are dependency labels. The ending symbol EOS is omitted.
RNNsearch fails to capture the long dependency which leads to an ungrammatical result. Whereas with
the help of the syntactic tree, our SD-NMT can get a much better translation.

NMT is straightforward, because the source sen-
tence is definitive and easy to attach extra informa-
tion. However, it is non-trivial to add target syn-
tax as target words are uncertain in decoding pro-
cess. Up to now, there is few work that attempts to
build and leverage target syntactic information for
NMT.

There has been work that incorporates syntactic
information into NLP tasks with neural networks.
Dyer et al. (2016) presented a RNN grammar for
parsing and language modeling. They replaced SH
with a set of generative actions to generate words
under a Stack LSTM framework (Dyer et al.,
2015), which achieves promising results for lan-
guage modeling on the Penn Treebank data. In
our work, we propose to involve target syntactic
trees into NMT model to jointly learn target trans-
lation and dependency parsing where target syn-
tactic context over the parse tree is used to improve
the translation quality.

6 Conclusion and Future Work

In this paper, we propose a novel string-to-
dependency translation model over NMT. Our
model jointly performs target word generation and
arc-standard dependency parsing. Experimental
results show that our method can boost the two
procedures and achieve significant improvements
on the translation quality of NMT systems.

In future work, along this research direction, we
will try to integrate other prior knowledge, such as

semantic information, into NMT systems. In addi-
tion, we will apply our method to other sequence-
to-sequence tasks, such as text summarization, to
verify the effectiveness.

Acknowledgments

We are grateful to the anonymous reviewers for
their insightful comments. We also thank Shujie
Liu and Zhirui Zhang for the helpful discussions.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. ICLR 2015 .

David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL 2005.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings
of ENMLP 2014.

Fabien Cromieres. 2016. Kyoto-nmt: a neural machine
translation implementation in chainer. In Proceed-
ings of COLING 2016.

Fabien Cromieres, Chenhui Chu, Toshiaki Nakazawa,
and Sadao Kurohashi. 2016. Kyoto university par-
ticipation to wat 2016. In Proceedings of the 3rd
Workshop on Asian Translation (WAT2016). pages
166–174.

706



Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of ACL 2015.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A. Smith. 2016. Recurrent neural network
grammars. In Proceedings of the NAACL 2016.

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tsuruoka. 2016. Tree-to-sequence attentional neu-
ral machine translation. In Proceedings of ACL
2016.

Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of ACL 2006.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Aistats. volume 9, pages 249–256.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation 9(8).

Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic eval-
uation of translation quality for distant language
pairs. In Proceedings of EMNLP.

Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation. In
Proceedings of ACL 2015.

Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language model-
ing. In Acoustics, Speech, and Signal Processing,
1995. ICASSP-95., 1995 International Conference
on. IEEE, volume 1, pages 181–184.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP. Cite-
seer, pages 388–395.

Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of ACL 2006.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015a. Effective approaches to attention-
based neural machine translation. In Proceedings
of EMNLP 2015.

Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,
and Wojciech Zaremba. 2015b. Addressing the rare
word problem in neural machine translation. In Pro-
ceedings of ACL 2015.

Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-
moto, Masao Utiyama, Eiichiro Sumita, Sadao
Kurohashi, and Hitoshi Isahara. 2016. Aspec:
Asian scientific paper excerpt corpus. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,

Thierry Declerck, Marko Grobelnik, Bente Mae-
gaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings
of the Ninth International Conference on Language
Resources and Evaluation (LREC 2016). European
Language Resources Association (ELRA), Portoroz,
Slovenia, pages 2204–2208.

Graham Neubig. 2016. Lexicons and minimum risk
training for neural machine translation: NAIST-
CMU at WAT2016. In Proceedings of the 3nd
Workshop on Asian Translation (WAT2016). Osaka,
Japan.

Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL
2002.

Libin Shen, Jinxi Xu, and Ralph M Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In ACL. pages 577–585.

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. In Pro-
ceedings of ACL 2016.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Modeling coverage for neural
machine translation. In Proceedings of ACL 2016.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144 .

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701 .

Biao Zhang, Deyi Xiong, jinsong su, Hong Duan, and
Min Zhang. 2016. Variational neural machine trans-
lation. In Proceedings of EMNLP 2016.

Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing. In
EMNLP2008.

Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL 2011.

707


	Sequence-to-Dependency Neural Machine Translation

