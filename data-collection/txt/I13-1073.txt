










































Generalized Abbreviation Prediction with Negative Full Forms and Its Application on Improving Chinese Web Search


International Joint Conference on Natural Language Processing, pages 641–647,
Nagoya, Japan, 14-18 October 2013.

Generalized Abbreviation Prediction with Negative Full Forms and Its
Application on Improving Chinese Web Search

Xu Sun†, Wenjie Li‡, Fanqi Meng‡, Houfeng Wang†,
†Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, China

‡Department of Computing, The Hong Kong Polytechnic University
xusun@pku.edu.cn cswjli@comp.polyu.edu.hk mengfanqi928@163.com wanghf@pku.edu.cn

Abstract

In Chinese abbreviation prediction, prior
studies are limited on positive full forms.
This lab assumption is problematic in real-
world applications, which have a large
portion of negative full forms (NFFs). We
propose solutions to solve this problem of
generalized abbreviation prediction. Ex-
periments show that the proposed unified
method outperforms baselines, with the
full-match accuracy of 79.4%. Moreover,
we apply generalized abbreviation predic-
tion for improving web search quality. Ex-
perimental results on web search demon-
strate that our method can significantly im-
prove the search results, with the search
F-score increasing from 35.9% to 64.9%.
To our knowledge, this is the first study on
generalized abbreviation prediction and its
application on web search.

1 Introduction

Abbreviations increase the ambiguity in a text.
Associating abbreviations with their fully expand-
ed forms is important in various natural language
processing applications (Pakhomov, 2002; Yu et
al., 2006; HaCohen-Kerner et al., 2008). Chi-
nese abbreviations represent fully expanded forms
(e.g., the left side of Figure 1) through the use of
shortened forms (e.g., the right side of Figure 1).
Chinese abbreviations are derived via a genera-
tive lexical process. Although native speakers may
possess intuitions of the generative process, it can-
not be adequately explained by any linguistic theo-
ry (Chang and Lai, 2004; Chang and Teng, 2007).

Abbreviation prediction (i.e., predicting abbre-
viation of a given full form) is important in Chi-
nese natural language processing applications. For
example, it is helpful for information retrieval if
we can estimate the abbreviation of a query. For

European Economic and Monetary Union

欧洲经济与货币联盟 欧盟

Full Form Abbr.

Figure 1: An example of abbreviation prediction.

the data of one month’s People’s Daily, only 17%
of the documents contain the full form in Figure 1,
while more than 70% of the articles contain on-
ly the abbreviation in Figure 1. It is expected
that abbreviation prediction can improve the re-
call in information retrieval. In addition, (Yang et
al., 2009b) in speech studies showed that Chinese
abbreviation prediction can improve voice-based
search quality.

The study of Chinese abbreviation prediction is
still in an early stage. Chinese abbreviation predic-
tion is quite different from English ones, because
of its specific characteristics (Sun et al., 2008;
Huang et al., 1994; Chang and Teng, 2007; Yang
et al., 2009b; Yang et al., 2009a). For example,
Chinese abbreviations are not necessarily from the
initials of words. They frequently take non-initial
characters from the words in the full form. In ad-
dition, the Chinese full form does not have word
boundaries.

To our knowledge, all of the prior studies of
Chinese abbreviation prediction (Sun et al., 2008;
Sun et al., 2009; Sun et al., 2013; Huang et al.,
1994; Chang and Teng, 2007; Yang et al., 2009b;
Yang et al., 2009a) have focused on positive full
forms with valid abbreviations. This implicit lab
assumption is quite limited in real-world applica-
tions, because real-world Chinese full forms con-
tain a large portion of negative full forms (NFFs),
which have no abbreviation at all. Abbreviation
prediction becomes more difficult by considering
NFFs, because of the strong noise. This difficulty
is one of the reason of the lab setting on consid-
ering only positive full forms. Another reason is

641



probably the difficulty of data collection. To our
knowledge, there is no existing collection of ab-
breviation prediction data with NFFs.

We aim at solving this abbreviation prediction
problem with generalized assumption (hereinafter
generalized abbreviation prediction). We manu-
ally collected a large dataset for this study, which
contains 10,786 entries including NFFs. To deal
with the strong noise from NFFs, we propose a
variety of solutions. We also apply generalized ab-
breviation prediction for web search and we show
that it can significantly improve web search quali-
ty.

2 Proposed Method

2.1 Preprocessing

The preprocessing step includes word segmenta-
tion and part-of-speech tagging for the input ab-
breviation prediction full forms. The word seg-
mentation and part-of-speech tagging is done via
the tool ICTCLAS 1.

2.2 Abbreviation Prediction

2.2.1 Simple Heuristic System
The simple heuristic system means always choos-
ing initial characters of words in the segmented
full form. This is because the most natural abbre-
viating heuristic is to produce the first character of
each word in the original full form. This is just the
simplest baseline.

2.2.2 Unified System
We present a unified system for generalized ab-
breviation prediction with NFFs. The unified sys-
tem can conduct the abbreviation prediction with
a single step. We cast abbreviation prediction as
a sequential labeling task. Following (Sun et al.,
2013), each character in the full form is tagged
with a label, y ∈ {P,S}, where the label P pro-
duces the current character and the label S skips
the current character.

As for NFFs, we need a special encoding of la-
beling EEE to represent “no valid abbreviation”. S-
ince there is no prior work, we need to study this
“no valid abbreviation” issue. Given a full form
FFF , its valid abbreviation AAA should have the char-
acter number constraints: 0 < |AAA| < |FFF |. On the
other hand, we can assume that a negative full for-
m has an “invalid abbreviation” AAA with |AAA| = 0

1http://ictclas.org/

or |AAA| = |FFF |. Those two kinds of interpretation-
s actually represent two different answers to the
question “why some full forms do not have valid
abbreviations”:

• Assumption-1 with |AAA| = 0: It as-
sumes that a negative full form F is “abbre-
viated” to nothing, i.e., with the abbreviation
A =NULL.

• Assumption-2 with |AAA| = |FFF |: It as-
sumes that a negative full form F is “abbre-
viated” to itself, i.e., with the abbreviation
A = F .

We want to find out which assumption leads to bet-
ter performance.

With those interpretations, invalid abbreviations
are treated as special forms of abbreviations, thus
positive and negative full forms can be modeled in
a unified framework via sequential labeling. For
simplicity, we use the well-known conditional ran-
dom fields (CRFs) (Lafferty et al., 2001) for se-
quential labeling. Assuming a feature function
that maps a pair of observation sequence xxx (char-
acters of a full form) and label sequence yyy (label
encoding based on abbreviations) to a feature vec-
tor fff , the probability function is defined as fol-
lows:

P (yyy|xxx,www) =
exp

[
www⊤fff(yyy,xxx)

]∑
∀y′y′y′ exp

[
www⊤fff(y′y′y′,xxx)

] , (1)
where www is a parameter vector.

Given a training set consisting of n labeled se-
quences, (xxxi, yyyi), for i = 1 . . . n, parameter esti-
mation is performed by maximizing the objective
function,

L(www) =
n∑

i=1

log P (yyyi|xxxi,www)−R(www). (2)

The second term is a regularizer, typically an L2
(Gaussian) norm, R(www) = ||www||

2

2σ2
.

We use features as follows:

• Character feature This feature
records the input characters xi−1, xi and
xi+1.

• Character bi-gram The character bi-
grams starting at (i− 2) . . . i.

• Numeral Whether or not the xi is a numer-
al.

642



• Organization name suffix
Whether or not the xi is a suffix of tra-
ditional Chinese organization names.

• Location name suffix Whether or
not the xi is a suffix of traditional Chinese
location names.

• Word segmentation information
After the word segmentation step, whether
or not the xi is the beginning character of a
word.

• Part-of-speech information The
part-of-speech tag information of xi.

i denotes the current position for extracting fea-
tures.

The character unigram and bi-gram feature is to
capture character-based information in the abbre-
viating process. For example, some special char-
acters are more likely to be chosen in abbreviating.
The named entity suffix features are used because
a named entity suffix character is more likely to
be chosen in abbreviating. The word segmenta-
tion information is also important because the be-
ginning character of a word is more likely to be
chosen in abbreviating.

2.3 Label Encoding with Global Information

As a common practice to reduce complexity, on-
ly local information based on Markov assumption
is used for sequential labeling. Nevertheless, the
Chinese abbreviation generation process is highly
dependent on global information. An example of
global information is the number of characters of
the generated abbreviations.

For better performance, we try to model global
information to make the system be “aware” of the
number of characters being generated. We use a
simple, effective, and tractable solution for model-
ing global information in abbreviation prediction:
label encoding with global information (GI) (Sun
et al., 2013).

In this approach, the label yi at position i will
be encoded with the global information of its pre-
vious labels, y1, y2, . . . , yi−1. Note that, while di-
rectly increasing the Markov order is untractable,
the GI label encoding is tractable. More detailed
description of the GI method is in (Sun et al.,
2013).

Category Portion (%)
Noun Phrase 52.01%
Verb Phrase 13.72%
Organization Name 26.84%
Location Name 5.28%
Person Name 0.32%
Others 1.80%

Table 1: Distribution of the full forms in the data.

2.4 Abbreviation Prediction for Web Search

Abbreviation prediction should be helpful for in-
formation search, but we find there is almost no
prior work on this. It is probably because the tradi-
tional abbreviation prediction is not so applicable
in real-world data, which includes lots of NFFs.
Since we have solved this problem via generalized
abbreviation prediction, we hope to apply gener-
alized abbreviation prediction on improving infor-
mation search.

In particular, we apply generalized abbreviation
prediction for “query expansion” in Chinese we-
b search. In this method, the original queries are
treated as full forms (with NFFs) for generating
abbreviations. Given a query as an input, the gen-
eralized abbreviation prediction system outputs an
abbreviation candidate or a NULL string. If the
output is an empty string, it means the query is
an NFF. Finally, the derived abbreviations, togeth-
er with the original query terms, are used for we-
b search, and their search results (web pages) are
simply added together. For the negative full form-
s, only the original full forms are used for the web
search. The simple architecture of the query ex-
pansion system is summarized in Figure 2.

In addition, to make clear the role of the pre-
dicted abbreviations in web search, we can remove
the full form information in web search and check
the difference. In this way, the method turns to
a “query alternation” method, which uses the pre-
dicted abbreviation to replace the positive full for-
m for the web search. For a negative full form,
the query alternation acts the same like the query
expansion.

3 Experiments

Here we describe our collected data for general-
ized abbreviation prediction. First, we extract long
phrases and terms from Chinese natural language
processing corpora, including People’s Daily cor-

643



Query
Search 

engine

Generalized 

abbreviation 

prediction

Abbreviation

NULL

Figure 2: Generalized abbreviation prediction for improving web search.

Positive/Negative Full Forms Abbreviation

磷酸氢二钠 X

君主专制制 X

珠穆朗玛峰 珠峰

天公不作美 X

中国社会科学院 中国社科院

新时期的总任务 X

自由民主党 自民党

车辆发动机 X

复员退伍军人安置办公室 复退办

车尔尼雪夫斯基 X

持谨慎态度 X

土产日用品杂品公司 土杂公司

一叶蔽目不见泰山 X

打击黑势力扫除恶势力 打黑扫恶

Figure 3: Samples of the collected data with NFFs.
The “X” means no valid abbreviation.

pora2 and SIGHAN word segmentation corpora3.
Then, we classify the collected phrases and terms
into positive and negative full forms. For the neg-
ative full forms, no further annotation is required.
For the positive full forms, their abbreviations are
annotated.

We build a dataset containing 10,786 full forms,
including 8,015 positive full forms and 2,771 neg-
ative full forms. Samples of the data are shown in
Figure 3. The dataset is made up of phrases and
terms, including noun phrases, verb phrases, orga-
nization names, location names, and so on. The
distribution is shown in Table 1. For experiments,
we randomly sampled 8,629 samples (80% of the
full dataset) for training and 2,157 (20% of the full
dataset) for testing.

For experiments on web search, we simply use
the 2,157 testing samples as query terms for web

2http://icl.pku.edu.cn/icl_res
3http://www.sighan.org/bakeoff2005

search. The evaluation is on the news domain4 of
the well-known web search engine “baidu.com”5.
The Baidu news search engine has two alternative
options: “title search” and “full content search”.
Since abbreviations are more common in news ti-
tles, we adopt the option of title search.

3.1 Experimental Settings

For evaluating abbreviation prediction quality, the
systems are evaluated using the following two
metrics:

• All-match accuracy (All-Acc): The number
of correct outputs (i.e., label strings) generat-
ed by the system divided by the total number
of full forms in the test set.6

• Character accuracy (Char-Acc): The num-
ber of correct labels (i.e., a classification on
a character) generated by the system divided
by the total number of characters in the test
set.

For evaluating web search quality based on a
given query, the following metrics are used:

• Precision P : The number of correct search
results returned by the query divided by the
total number of search results returned by the
query.

• Recall R: The number of correct search re-
sults returned by the query divided by the to-
tal number of existing correct search results
based on the query.

• F-Score F : F = 2PR/(P + R).
4We choose news domain because abbreviations are main-

ly from named entities, and named entities are important in
news domain.

5http://news.baidu.com
6There is only one label string for a full form, and a label

string corresponds to a unique abbreviation candidate. A la-
bel string is deemed as correct if and only if all of the labels
are correct.

644



Method Discriminate Acc (%) Overall All-Acc Overall Char-Acc
Heuristic System 73.20 25.77 65.79
Unified-Assum.1 (Perc) 87.48 54.89 87.02
Unified-Assum.1 (MEMM) 86.97 50.16 85.92
Unified-Assum.1 (CRF-ADF) 87.80 56.69 87.20
Unified-Assum.1-GI (Perc) 91.93 75.42 90.23
Unified-Assum.1-GI (MEMM) 88.59 70.32 88.21
Unified-Assum.1-GI (CRF-ADF) 91.05 79.46 91.61
Unified-Assum.2 (Perc) 86.83 55.86 82.20
Unified-Assum.2 (MEMM) 87.52 56.18 82.27
Unified-Assum.2 (CRF-ADF) 87.11 56.97 82.54
Unified-Assum.2-GI (Perc) 90.35 71.85 88.04
Unified-Assum.2-GI (MEMM) 87.99 63.74 83.77
Unified-Assum.2-GI (CRF-ADF) 90.77 74.78 89.19

Table 2: Results on comparing different methods on generalized abbreviation prediction. Assum.1 and
Assum.2 represent the two assumptions on NFFs discussed in Section 2.2.2. GI means the integration
with global information. As we can see, the Unified-Assum.1-GI (CRF) system has the best performance.

For evaluating web search quality based on a set of
queries, we use the macro-averaging and micro-
averaging of the precision, recall, and F-score
based on a single query. Hence, we finally have six
metrics: macro-precision, macro-recall, macro-F-
score, micro-precision, micro-recall, and micro-F-
score. We use the novel training method, adaptive
online gradient descent based on feature frequen-
cy information (ADF) (Sun et al., 2012), for fast
and accurate training of the CRF model.

To study the performance of other machine
learning models, we also implement on other well-
known sequential labeling models, including max-
imum entropy Markov models (MEMMs) (Mc-
Callum et al., 2000) and averaged perceptrons
(Perc) (Collins, 2002).

3.2 Results on Abbreviation Prediction

The experimental results are shown in Table 2.
In the table, the overall accuracy is most impor-
tant and it means the final accuracy achieved by
the systems in generalized abbreviation prediction
with NFFs. For the completeness of experimental
information, we also show the discriminate accu-
racy. The discriminate accuracy checks the ac-
curacy of discriminating positive and negative full
forms, without comparing the generated abbrevia-
tions with the gold-standard abbreviations.

As we can see from Table 2, first, the best
system is the system Unified-Assum.1-GI (CRF).
Results demonstrate that incorporating global in-
formation can always improve the accuracy for

the unified methods. Second, the unified system
with assumption-1 has better accuracy than the
one with assumption-2. This result suggests that
assumption-1 works better in practice. It is inter-
esting that assumption-1 is more useful. A prob-
able reason is that those negative full forms have
no similar patterns with the real abbreviations. For
example, the number of characters in NFFs is very
different compared with that of real abbreviation-
s. Also, the NFFs contain more formal word u-
nits. Real abbreviations contain much less word
units. Thus, assumption-2 will have the incon-
sistency problem between abbreviations generat-
ed from NFFs and real abbreviations. As a re-
sult, assumption-2 works worse than assumption-
1 which gives no abbreviations. Finally, the CRF
model outperforms the MEMM and averaged per-
ceptron models. To summarize, the unified system
with assumption-1, global information, and CRF
model has the best performance.

3.3 Results on Web Search

We use the 2,157 testing samples as query terms
for web search. We test the original query terms,
the query alternation, and the query expansion
methods. Some search results actually do not
match the query. For example, given a query abc,
some search results do not contain abc, but with
the expression “ab . . . c” or “a . . . bc”, where “. . . ”
means other characters. In this case, the search re-
sults are incorrect. Since the number of the search
results is massive, we need to evaluate the web

645



Method Micro Prec Micro Rec Micro F1 Macro Prec Macro Rec Macro F1
Original query 48.51 18.14 26.41 47.84 28.76 35.92
Query alternation 47.73 54.04 50.69 62.84 61.12 61.97
Query expansion 47.93 72.18 57.60 53.70 82.02 64.90

Table 3: Results on comparing different methods on web search quality.

Method Micro Prec Micro Rec Micro F1 Macro Prec Macro Rec Macro F1
Original query 48.51 18.14 26.41 47.84 28.76 35.92
Query alternation (gold-standard) 72.31 81.86 76.79 83.07 79.11 81.04
Query expansion (gold-standard) 66.40 100.00 79.81 65.56 100.00 79.19

Table 4: Results on comparing different methods on web search quality.

search quality in an efficient way. The correctness
of the search results is evaluated by an automatic
postprocessing scoring system, which crawls the
search results from the baidu.com site. Then,
the system runs text matching analysis to check if
the search query matches the retrieved web pages.

In traditional web search studies, many queries
are phrases (e.g., NP+VP) with ambiguous sens-
es. In this case, improving search precision vi-
a contextual information is important. Howev-
er, for abbreviation processing, most abbreviations
are from named entities (the data contains phrases
but most of them are NFFs), and the major prob-
lem of named entities is variational expressions. In
this case, the search recall is more important. To
calculate the recall rate in web search, we need to
estimate the total number of correct web pages N
relating to a query Q and its abbreviation A. We
can estimate N via summing up the correct web
pages of Q and the correct web pages of the gold-
standard abbreviation A.

The precision, recall, F-scores are shown in
Table 3. As we can see, the query expansion
method based on generalized abbreviation predic-
tion achieves significantly better F-scores on web
search quality than using the original queries. We
find the major improvement is from the recalls. As
expected, the query alternation has lower recal-
l rate than the query expansion method, because
the full form information is removed. Neverthe-
less, the query alternation method is also better
than using the original queries. This result em-
phasizes that the abbreviations are helpful in title-
based news search.

Finally, we check the “up-bound” of the per-
formance of generalized abbreviation prediction
for web search. The up-bound is achieved by the
100% correct “gold-standard” system, in which

gold-standard abbreviations labeled in the data are
used. The results are shown in Table 4. As we
can see, the up-bound of the micro-F-score and the
macro-F-score is 79.81% and 81.04%, respective-
ly. Thus, the web search quality of automatic gen-
eralized abbreviation prediction still has a large s-
pace to be improved, possibly via a larger training
data set in the future.

4 Conclusions and Future Work

This paper is dedicated on generalized abbrevia-
tion prediction and its application on improving
web search. Experiments demonstrate that the
unified system based on global information out-
performs the baselines. Experiments also demon-
strate that generalized abbreviation prediction can
improve web search qualities. As future work,
we try to improve the performance via collecting
more training data or via semi-supervised learning
methods.

Acknowledgments

This work was supported by the Hong Kong
Polytechnic University Internal Grant (4-ZZD5),
Major National Social Science Fund of Chi-
na (No.12&ZD227), National High Technolo-
gy Research and Development Program of Chi-
na (863 Program) (No.2012AA011101), and
National Natural Science Foundation of China
(No.91024009).

References

Jing-Shin Chang and Yu-Tso Lai. 2004. A preliminary
study on probabilistic models for chinese abbrevia-
tions. In Proceedings of the Third SIGHAN Work-
shop on Chinese Language Learning, pages 9–16.

646



Jing-Shin Chang and Wei-Lun Teng. 2007. Mining
atomic chinese abbreviation pairs: A probabilistic
model for single character word recovery. Language
Resources and Evaluation, 40:367–374.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP’02, pages 1–8.

Yaakov HaCohen-Kerner, Ariel Kass, and Ariel Peretz.
2008. Combined one sense disambiguation of ab-
breviations. In Proceedings of ACL’08: HLT, Short
Papers, pages 61–64, June.

C.R. Huang, W.M. Hong, , and K.J. Chen. 1994. Suox-
ie: An information based lexical rule of abbrevia-
tion. In Proceedings of the Second Pacific Asia Con-
ference on Formal and Computational Linguistics II,
pages 49–52.

John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning (ICML’01),
pages 282–289.

Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov model-
s for information extraction and segmentation. In
Proc. 17th International Conf. on Machine Learn-
ing, pages 591–598. Morgan Kaufmann, San Fran-
cisco, CA.

Serguei Pakhomov. 2002. Semi-supervised maximum
entropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of
ACL’02, pages 160–167.

Xu Sun, Houfeng Wang, and Bo Wang. 2008. Pre-
dicting chinese abbreviations from definitions: An
empirical learning approach using support vector re-
gression. Journal of Computer Science and Tech-
nology, 23(4):602–611.

Xu Sun, Naoaki Okazaki, and Jun’ichi Tsujii. 2009.
Robust approach to abbreviating terms: A discrimi-
native latent variable model with global information.
In Proceedings of the ACL’09, pages 905–913, Sun-
tec, Singapore, August.

Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast on-
line training with frequency-adaptive learning rates
for chinese word segmentation and new word detec-
tion. In Proceedings of ACL’12, pages 253–262.

Xu Sun, Naoaki Okazaki, Jun’ichi Tsujii, and Houfeng
Wang. 2013. Learning abbreviations from chinese
and english terms by modeling non-local informa-
tion. ACM Trans. Asian Lang. Inf. Process., 12(2):5.

Dong Yang, Yi-Cheng Pan, and Sadaoki Furui. 2009a.
Automatic chinese abbreviation generation using
conditional random field. In Proceedings of HLT-
NAACL’09 (Short Papers), pages 273–276.

Dong Yang, Yi-Cheng Pan, and Sadaoki Furui. 2009b.
Vocabulary expansion through automatic abbrevia-
tion generation for chinese voice search. In Pro-
ceedings of INTERSPEECH’09, pages 728–731. IS-
CA.

Hong Yu, Won Kim, Vasileios Hatzivassiloglou, and
John Wilbur. 2006. A large scale, corpus-based ap-
proach for automatically disambiguating biomedical
abbreviations. ACM Transactions on Information
Systems (TOIS), 24(3):380–404.

647


