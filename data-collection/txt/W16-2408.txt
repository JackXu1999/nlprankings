



















































On the Correspondence between Compositional Matrix-Space Models of Language and Weighted Automata


Proceedings of the ACL Workshop on Statistical NLP and Weighted Automata, pages 70–74,
Berlin, Germany, August 12, 2016. c©2016 Association for Computational Linguistics

On the Correspondence between Compositional Matrix-Space Models of
Language and Weighted Automata

Shima Asaadi∗ and Sebastian Rudolph
Faculty of Computer Science

Technische Universität Dresden
firstname.lastname@tu-dresden.de

Abstract

Compositional matrix-space models of
language were recently proposed for the
task of meaning representation of complex
text structures in natural language process-
ing. These models have been shown to be
a theoretically elegant way to model com-
positionality in natural language. How-
ever, in practical cases, appropriate meth-
ods are required to learn such models
by automatically acquiring the necessary
token-to-matrix assignments. In this pa-
per, we introduce graded matrix gram-
mars of natural language, a variant of the
matrix grammars proposed by Rudolph
and Giesbrecht (2010), and show a close
correspondence between this matrix-space
model and weighted finite automata. We
conclude that the problem of learning
compositional matrix-space models can
be mapped to the problem of learning
weighted finite automata over the real
numbers.

1 Introduction

Quantitative models of language have recently re-
ceived considerable research attention in the field
of Natural Language Processing (NLP). In the ap-
plication of meaning representation of text in NLP,
much effort has been spent on semantic Vector
Space Models (VSMs). Such models capture word
meanings quantitatively, based on their statisti-
cal co-occurrences in the documents. The basic
idea is to represent words as vectors in a high-
dimensional space, where each dimension corre-
sponds to a separate feature. In this way, seman-
tic similarities can be computed based on mea-
suring the distance between vectors in the vector

∗Supported by DFG Graduiertenkolleg 1763 (QuantLA)

space (Mitchell and Lapata, 2010). Vectors which
are close together in this space have similar mean-
ings and vectors which are far away are distant in
meaning (Turney and Pantel, 2010).

VSMs typically represent each word separately,
without considering representations of phrases or
sentences. So, the compositionality properties of
the language is lost in VSMs (Mitchell and Lapata,
2010). Recently, some approaches have been de-
veloped in the area of compositionality and distri-
butional semantics in NLP. These approaches in-
troduce different word representations and ways
of combining those words. Mitchell and Lapata
(2010) propose a framework for vector-based se-
mantic composition. They define additive or mul-
tiplicative function for the composition of two vec-
tors and show that compositional approaches gen-
erally outperform non-compositional approaches
which treat the phrase as the union of single lexi-
cal items. However, VSMs still have some limita-
tions in the task of modeling complex conceptual
text structures. For example, in the bag-of-words
model, the words order and therefore the structure
of the language is lost.

To overcome the limitations of VSMs, Rudolph
and Giesbrecht (2010) proposed Compositional
Matrix-Space Models (CMSM) as a recent al-
ternative model to work with distributional ap-
proaches. These models employ matrices instead
of vectors and make use of iterated matrix multi-
plication as the only composition operation. They
show that these models are powerful enough to
subsume many known models, both quantitative
(vector-space models with diverse composition
operations) and qualitative ones (such as regular
languages). It is also proved theoretically that this
framework is an elegant way to model composi-
tional, symbolic and distributional aspects of nat-
ural language.

However, in practical cases, methods are needed

70



to automatically acquire the token-to-matrix as-
signments from available data. Therefore, meth-
ods for training such models should be developed
e.g. by leveraging appropriate machine learning
methods.

In this paper, we are concerned with Graded
Matrix Grammars, a variant of the Matrix Gram-
mars of Rudolph and Giesbrecht (2010), where in-
stead of the “yes or no” decision, if a sequence is
part of a language, a real-valued score is assigned.
This is a popular task in NLP, used, e.g., in sen-
timent analysis settings (Yessenalina and Cardie,
2011).

Generally, in many tasks of NLP, we need to es-
timate functions which map arbitrary sequence of
words (e.g. sentences) to some semantical space.
Using Weighted Finite Automata (WFA), an ex-
tensive class of these functions can be defined,
which assign values to these sequences (Balle and
Mohri, 2012).

Herein, inspired by the definition of weighted fi-
nite automata (Sakarovitch, 2009) and their appli-
cations in NLP (Knight and May, 2009), we show
a tight correspondence between graded matrix
grammars and weighted finite automata. Hence,
we argue that the problem of learning CMSMs can
be mapped to the problem of learning WFA.

The rest of the paper is organized as follows.
Section 2 provides the basic notions of weighted
automata and the matrix-space model. A detailed
description of correspondence between CMSM
and WFA is presented in Section 3, followed by
related work in Section 4 and conclusion and fu-
ture work in Section 5.

2 Preliminaries

In this section, we provide the definitions of
weighted automata in (Balle and Mohri, 2015;
Sakarovitch, 2009) and matrix-space models of
language in (Rudolph and Giesbrecht, 2010).

2.1 Weighted Finite Automata

Weighted finite automata generalize classical au-
tomata in which transitions and states carry
weights. These weights can be considered as
the cost of the transitions or amount of resources
needed to execute the transitions. Let Σ be a fi-
nite alphabet. A weighted automaton A is a tu-
ple of (QA, λ, α, β) and defined over a semi-ring
(S,⊕,⊗, 0̄, 1̄). QA is a finite set of states, λ :
Σ → SQA×QA is the transition weight function,

{A, 1, 0.5} {B, 2, 0.2}

{C, 3, 0.8}

b/3

a/1.5

a/4

b/2

b/3

Figure 1: Example of WFA A.

and, α : Σ → S and β : Σ → S are two functions
assigning to every state its initial and final weight.
Thereby, for each transition e = (q, σ, q′), λ(σ)q,q′
denotes the weight of the label σ associated with
the transition e between q and q′, which are the
source and target state of the transition. Moreover,
A path P in A is a sequence of transitions labeled
with σ1 · · ·σn, in more detail:

P := p0 σ1−→ p1 σ2−→ · · · σn−→ pn
with pi ∈ QA. The weight of P is defined as
the ⊗-product of the weights of the starting state,
its transitions, and final state: ω(P) = α(p0) ⊗
λ(σ1)p0,p1⊗· · ·⊗λ(σn)pn−1,pn⊗β(pn). Now, the
weight of a word x = σ1 · · ·σn ∈ Σ? is the cumu-
lative weight of all paths labeled with the sequence
σ1 · · ·σn which is computed as the ⊕-sum of the
weights of the corresponding paths, also known as
a rational power series:

fA(σ1 · · ·σn) =
⊕

P∈PA(σ1···σn)
ω(P), (1)

where PA(σ1 · · ·σn) denotes the (finite) set of
paths in A labeled with σ1 · · ·σn. So, the func-
tion fA maps the set of strings in Σ? to S. In this
work, we will assume that S is the set of the real
numbers R with the usual multiplication and addi-
tion. Figure 1 illustrates an example of WFA over
Σ = {a, b}. Inside each state there is a tuple of
the name, initial and final weight of the state, re-
spectively. As an example, for x = ab we have:
fA(x) = 1× 1.5× 3× 0.8 + 1× 1.5× 2× 0.2 +
2× 4× 3× 0.5.
2.2 Compositionality and Compositional

Matrix-Space Model
The general principle of compositionality is that
the meaning of a complex expression is a function
of the meaning of its constituent tokens and some
rules used to combine them (Frege, 1884). More

71



formally, according to Rudolph and Giesbrecht
(2010), the underlying idea can be described as
follows: “Given a mapping J·K : Σ → S from a
set of tokens in Σ into some semantical space S,
the composition operation is defined by mapping
sequences of meanings to meanings: ./: S? →
S. So, the meaning of the sequence of tokens
σ1 · · ·σn can be obtained by first applying the
function J·K to each token and then ./ to the se-
quence Jσ1K · · · JσnK, as shown in Figure 2”.

σ1 σ2 . . . σn σ1σ2 · · ·σn

Jσ1K Jσ2K · · · JσnK Jσ1 · · ·σnK

Concatenation ·

J·K J·K J·K J·K

Composition ./

Figure 2: Principle of compositionality, illustra-
tion taken from Rudolph and Giesbrecht (2010)

In compositional matrix-space models, this gen-
eral idea is instantiated as follows: we have
S = Rn×n, i.e., the semantical space consists of
quadratic matrices of real numbers. The mapping
function J·K maps the tokens into matrices so that
the semantics of simple tokens is expressed by ma-
trices. Then, using the standard matrix multiplica-
tion as the only composition operation ./, the se-
mantics of complex phrases are also described by
matrices.

Rudolph and Giesbrecht (2010) showed theoret-
ically that by employing matrices instead of vec-
tors, CMSMs subsume a wide range of linguis-
tic models such as statistical models (vector-space
models and word space models).

3 Graded Matrix Grammars and
Weighted Finite Automata

In some applications of NLP, we need to derive
the meaning of a sequence of words in a language,
which can be done with CMSMs as described in
Section 2.2. In this section, we introduce the no-
tion of a graded matrix grammar which consti-
tutes a slight variation of matrix grammars as in-
troduced by Rudolph and Giesbrecht (2010).

Definition 1 (Graded Matrix Grammars). Let Σ be

an alphabet. A graded matrix grammarM of de-
gree n is defined as the tuple 〈J·K,Σ, α, β〉 whereJ·K is a function mapping tokens in Σ to n × n
matrices of real numbers. Moreover, α, β ∈ Rn.
Then we map each sequence of tokens σ1 · · ·σn ∈
Σ? to a real number (called the value of the se-
quence) using the target function ϕ : Σ? → R
defined by:

ϕ(σ1 · · ·σn) = αJσ1K · · · JσnKβ>. (2)
However, as discussed before, to be used in

practice, some learning methods are needed to ex-
tract graded matrix grammars from textual data.
Hence, the target function ϕ can be generalized to
all texts in the language and handle unseen word
compositions. To this end, we show the corre-
spondence between the CMSM and WFA, with
the consequence that existing learning methods for
WFA can be applied to learning CMSMs.

As discussed in Section 2.1, in WFA, for a ratio-
nal power series f , a value f(x) is the sum of all
possible paths labeled with x = σ1 · · ·σn ∈ Σ?.
However, this computation can be described via
matrices by the fact that a walk over a graph cor-
responds to a matrix multiplication (Sakarovitch,
2009). More precisely, for any σ ∈ Σ, let Aσ ∈
RQA×QA be the transition matrix of σ: [Aσ]pq =∑

e∈PA(p,σ,q) λ(σ)p,q, where PA(p, σ, q) is the set
of all transitions labeled with σ from p to q. Also,
the vectors αA ∈ RQA and βA ∈ RQA are the
start and final weights of the states in QA, respec-
tively. Then, Equation 1 can be equally expressed
as follows in terms of matrices with entries in R
(Balle and Mohri, 2015):

fA(σ1 · · ·σn) = α>AAσ1 · · ·AσnβA (3)
Hence, we see the correspondence between Equa-
tion 2 and 3. In more detail, consider each phrase
p and its value r in a natural language. If we ex-
tract the words of the language as a finite alphabet
Σ in an automaton, then p would be a string in Σ?.
The J·K function inM applied over the words con-
structs n×n transition matrices of the alphabets in
the automaton. Here, n can be the number of states
of the automaton. So, estimating the function ϕ
in graded matrix grammar corresponds to estimat-
ing the target function of the automaton fA, which
computes exactly the value of a string translated
from the phrase p in a language. That is, the repre-
sentation of a string is done with multiplication of
transition matrices of its tokens, which results in a

72



new representation matrix for the string. Then, the
suitable predefined vectors α and β translate the
resulting matrix to a real number which denotes
the value of associated phrase p in the natural lan-
guage.

The problem of learning WFAs is finding a
WFA closely estimating a target function, using
for training a finite sample of strings labeled with
their target values (Balle and Mohri, 2015). By
learning WFAs, one obtains an automaton that is
a tuple A = 〈α, β, {Aa}a∈Σ〉, and one can com-
pute the target function fA(x). Since WFA encode
CMSMs, and based on this close correspondence
between them, learning a graded matrix grammar
to estimate the value of phrases can be mapped to
the problem of learning a weighted automaton.

4 Related Work

An application of CMSM has been shown in the
work of Yessenalina and Cardie (2011). They
proposed a learning-based approach for phrase-
level sentiment analysis. Inspired by the work of
Rudolph and Giesbrecht (2010) they use CMSMs
to model composition, and present an algorithm
for learning a matrix for each word via ordered lo-
gistic regression, which is evaluated with promis-
ing results. However, it is not trivial to learn a
matrix-space model. Since the final optimization
problem is non-convex, the matrix initialization
for this method is not done perfectly.

Socher et al. (2012) introduce a matrix-vector
recursive neural network (MV-RNN) model that
learns compositional vector representations for
phrases and sentences. The model assigns a vec-
tor and a matrix to every node in a parse tree. The
vector represents the meaning of the constituent,
while the matrix captures how it affects the mean-
ing of neighboring constituent. The model needs
to parse the tree to learn the vectors and matrices.

Recently, new approaches are proposed in
learning weighted finite automata in NLP. Balle
and Mohri (2012) and Balle et al. (2014) introduce
a new family of algorithms for learning general
WFA and stochastic WFA based on the combi-
nation of matrix completion problem and spectral
methods. These algorithms are designed for learn-
ing an arbitrary weighted automaton from sample
data of strings and assigned labels. They formu-
late the missing information from the sample data
as a Hankel matrix completion problem. Then, the
spectral learning is applied to the resulting Hankel

matrix to obtain WFA. Balle et al. (2014) also, of-
fer the main results in spectral learning which are
an interesting alternative to the classical EM al-
gorithms in the context of grammatical inference
and show the computational efficiency of these al-
gorithms.

Moreover, Balle and Mohri (2015) discuss mod-
ern learning methods (spectral methods) for an ar-
bitrary WFA in different scenarios. They provide
WFA reconstruction algorithms and standardiza-
tion. it is theoretically guaranteed that for a Han-
kel matrix with a finite rank, representing a ratio-
nal power series, there is a corresponding WFA
with the number of states equal to this rank and it
is minimal.

5 Conclusion and Future Work

In this paper, we introduced a graded matrix gram-
mar for compositionality in language where com-
positional matrix-space models are employed in
different tasks of NLP. However, we need to pro-
pose a learning method to train this model for
value assignments in NLP. For this purpose, we
showed the close correspondence between matrix
grammars and weighted automata. So, the prob-
lem of learning the CMSM can be encoded as the
problem of learning WFA.

Our future goal is to review the existing meth-
ods in learning WFA, and adapt them to solve the
task of sentiment analysis/meaning representation
in NLP. Using learning methods, they allow to au-
tomatically learn CMSM and induce the graded
matrix grammar.

References
Borja Balle and Mehryar Mohri. 2012. Spectral learn-

ing of general weighted automata via constrained
matrix completion. In Advances in neural informa-
tion processing systems, pages 2168–2176.

Borja Balle and Mehryar Mohri, 2015. Learning
Weighted Automata, pages 1–21. Springer Interna-
tional Publishing.

Borja Balle, Xavier Carreras, Franco M Luque, and
Ariadna Quattoni. 2014. Spectral learning of
weighted automata. Machine Learning, 96(1):33–
63.

Gottlob Frege. 1884. Die Grundlagen der Arithmetik
eine logisch-mathematische Untersuchung über den
Begriff der Zahl. Verlage Wilhelm Koebner, Bres-
lau.

73



Kevin Knight and Jonathan May. 2009. Applications
of weighted automata in natural language process-
ing. In Manfred Droste, Werner Kuich, and Heiko
Vogler, editors, Handbook of Weighted Automata,
pages 571–596. Springer Berlin Heidelberg, Berlin,
Heidelberg.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.

Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ’10,
pages 907–916, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Jacques Sakarovitch. 2009. Rational and recognisable
power series. In Manfred Droste, Werner Kuich, and
Heiko Vogler, editors, Handbook of Weighted Au-
tomata, pages 105–174. Springer Berlin Heidelberg,
Berlin, Heidelberg.

Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
EMNLP-CoNLL ’12, pages 1201–1211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141–188.

Ainur Yessenalina and Claire Cardie. 2011. Com-
positional matrix-space models for sentiment analy-
sis. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
’11, pages 172–182, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

74


