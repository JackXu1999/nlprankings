



















































Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5297–5306,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5297

Improved Word Sense Disambiguation
Using Pre-Trained Contextualized Word Representations

Christian Hadiwinoto, Hwee Tou Ng, and Wee Chung Gan
Department of Computer Science, National University of Singapore

christian.hadiwinoto@u.nus.edu, nght@comp.nus.edu.sg,
gan weechung@u.nus.edu

Abstract

Contextualized word representations are able
to give different representations for the same
word in different contexts, and they have been
shown to be effective in downstream natu-
ral language processing tasks, such as ques-
tion answering, named entity recognition, and
sentiment analysis. However, evaluation on
word sense disambiguation (WSD) in prior
work shows that using contextualized word
representations does not outperform the state-
of-the-art approach that makes use of non-
contextualized word embeddings. In this pa-
per, we explore different strategies of integrat-
ing pre-trained contextualized word represen-
tations and our best strategy achieves accura-
cies exceeding the best prior published accura-
cies by significant margins on multiple bench-
mark WSD datasets.

1 Introduction

Word sense disambiguation (WSD) automatically
assigns a pre-defined sense to a word in a text. Dif-
ferent senses of a word reflect different meanings a
word has in different contexts. Identifying the cor-
rect word sense given a context is crucial in natural
language processing (NLP). Unfortunately, while
it is easy for a human to infer the correct sense of
a word given a context, it is a challenge for NLP
systems. As such, WSD is an important task and it
has been shown that WSD helps downstream NLP
tasks, such as machine translation (Chan et al.,
2007a) and information retrieval (Zhong and Ng,
2012).

A WSD system assigns a sense to a word by tak-
ing into account its context, comprising the other
words in the sentence. This can be done through
discrete word features, which typically involve
surrounding words and collocations trained using
a classifier (Lee et al., 2004; Ando, 2006; Chan
et al., 2007b; Zhong and Ng, 2010). The classifier

can also make use of continuous word represen-
tations of the surrounding words (Taghipour and
Ng, 2015; Iacobacci et al., 2016). Neural WSD
systems (Kågebäck and Salomonsson, 2016; Ra-
ganato et al., 2017b) feed the continuous word rep-
resentations into a neural network that captures the
whole sentence and the word representation in the
sentence. However, in both approaches, the word
representations are independent of the context.

Recently, pre-trained contextualized word rep-
resentations (Melamud et al., 2016; McCann et al.,
2017; Peters et al., 2018; Devlin et al., 2019)
have been shown to improve downstream NLP
tasks. Pre-trained contextualized word represen-
tations are obtained through neural sentence en-
coders trained on a huge amount of raw texts.
When the resulting sentence encoder is fine-tuned
on the downstream task, such as question answer-
ing, named entity recognition, and sentiment anal-
ysis, with much smaller annotated training data, it
has been shown that the trained model, with the
pre-trained sentence encoder component, achieves
new state-of-the-art results on those tasks.

While demonstrating superior performance in
downstream NLP tasks, pre-trained contextual-
ized word representations are still reported to give
lower accuracy compared to approaches that use
non-contextualized word representations (Mela-
mud et al., 2016; Peters et al., 2018) when eval-
uated on WSD. This seems counter-intuitive, as
a neural sentence encoder better captures the sur-
rounding context that serves as an important cue to
disambiguate words. In this paper, we explore dif-
ferent strategies of integrating pre-trained contex-
tualized word representations for WSD. Our best
strategy outperforms prior methods of incorpo-
rating pre-trained contextualized word represen-
tations and achieves new state-of-the-art accuracy
on multiple benchmark WSD datasets.

The following sections are organized as follows.



5298

Section 2 presents related work. Section 3 de-
scribes our pre-trained contextualized word repre-
sentation. Section 4 proposes different strategies
to incorporate the contextualized word represen-
tation for WSD. Section 5 describes our experi-
mental setup. Section 6 presents the experimental
results. Section 7 discusses the findings from the
experiments. Finally, Section 8 presents the con-
clusion.

2 Related Work

Continuous word representations in real-valued
vectors, or commonly known as word embed-
dings, have been shown to help improve NLP
performance. Initially, exploiting continuous rep-
resentations was achieved by adding real-valued
vectors as classification features (Turian et al.,
2010). Taghipour and Ng (2015) fine-tuned
non-contextualized word embeddings by a feed-
forward neural network such that those word em-
beddings were more suited for WSD. The fine-
tuned embeddings were incorporated into an SVM
classifier. Iacobacci et al. (2016) explored differ-
ent strategies of incorporating word embeddings
and found that their best strategy involved expo-
nential decay that decreased the contribution of
surrounding word features as their distances to the
target word increased.

The neural sequence tagging approach has also
been explored for WSD. Kågebäck and Salomons-
son (2016) proposed bidirectional long short-term
memory (LSTM) (Hochreiter and Schmidhuber,
1997) for WSD. They concatenated the hidden
states of the forward and backward LSTMs and
fed the concatenation into an affine transforma-
tion followed by softmax normalization, similar to
the approach to incorporate a bidirectional LSTM
adopted in sequence labeling tasks such as part-of-
speech tagging and named entity recognition (Ma
and Hovy, 2016). Raganato et al. (2017b) pro-
posed a self-attention layer on top of the concate-
nated bidirectional LSTM hidden states for WSD
and introduced multi-task learning with part-of-
speech tagging and semantic labeling as auxiliary
tasks. However, on average across the test sets,
their approach did not outperform SVM with word
embedding features. Subsequently, Luo et al.
(2018) proposed the incorporation of glosses from
WordNet in a bidirectional LSTM for WSD, and
reported better results than both SVM and prior
bidirectional LSTM models.

A neural language model (LM) is aimed at pre-
dicting a word given its surrounding context. As
such, the resulting hidden representation vector
captures the context of a word in a sentence. Mela-
mud et al. (2016) designed context2vec, which is
a one-layer bidirectional LSTM trained to maxi-
mize the similarity between the hidden state rep-
resentation of the LSTM and the target word em-
bedding. Peters et al. (2018) designed ELMo,
which is a two-layer bidirectional LSTM language
model trained to predict the next word in the for-
ward LSTM and the previous word in the back-
ward LSTM. In both models, WSD was evaluated
by nearest neighbor matching between the test and
training instance representations. However, de-
spite training on a huge amount of raw texts, the
resulting accuracies were still lower than those
achieved by WSD approaches with pre-trained
non-contextualized word representations.

End-to-end neural machine translation (NMT)
(Sutskever et al., 2014; Bahdanau et al., 2015)
learns to generate an output sequence given an
input sequence, using an encoder-decoder model.
The encoder captures the contextualized represen-
tation of the words in the input sentence for the
decoder to generate the output sentence. Follow-
ing this intuition, McCann et al. (2017) trained an
encoder-decoder model on parallel texts and ob-
tained pre-trained contextualized word representa-
tions from the encoder.

3 Pre-Trained Contextualized Word
Representation

The contextualized word representation that we
use is BERT (Devlin et al., 2019), which is a
bidirectional transformer encoder model (Vaswani
et al., 2017) pre-trained on billions of words of
texts. There are two tasks on which the model is
trained, i.e., masked word and next sentence pre-
diction. In both tasks, prediction accuracy is de-
termined by the ability of the model to understand
the context.

A transformer encoder computes the represen-
tation of each word through an attention mecha-
nism with respect to the surrounding words. Given
a sentence xn1 of length n, the transformer com-
putes the representation of each word xi through a
multi-head attention mechanism, where the query
vector is from xi and the key-value vector pairs are
from the surrounding words xi′ (1 ≤ i′ ≤ n). The
word representation produced by the transformer



5299

captures the contextual information of a word.
The attention mechanism can be viewed as

mapping a query vector q and a set of key-value
vector pairs (k,v) to an output vector. The at-
tention function A(·) computes the output vector
which is the weighted sum of the value vectors and
is defined as:

A(q,K,V, ρ) =
∑

(k,v)∈(K,V)

α(q,k, ρ)v (1)

α(q,k, ρ) =
exp(ρk>q)∑

k′∈K exp(ρk
′>q)

(2)

where K and V are matrices, containing the key
vectors and the value vectors of the words in the
sentence respectively, and α(q,k, ρ) is a scalar at-
tention weight between q and k, re-scaled by a
scalar ρ.

Two building blocks for the transformer en-
coder are the multi-head attention mechanism
and the position-wise feed-forward neural net-
work (FFNN). The multi-head attention mecha-
nism with H heads leverages the attention func-
tion in Equation 1 as follows:

MH(q,K,V, ρ) = WMH
H⊕
η=1

headη(q,K,V, ρ)

(3)

headη(q,K,V, ρ) = A(WQη q,W
K
η K,W

V
η V, ρ)

(4)

where ⊕ denotes concatenation of vectors,
WMH ∈ Rdmodel×Hdv , WQη ,WKη ∈ Rdk×dmodel ,
and WVη ∈ Rdv×dmodel . The input vector q ∈
Rdmodel is the hidden vector for the ambiguous
word, while input matrices K,V ∈ Rdmodel×n
are the concatenation of the hidden vectors of all
words in the sentence. For each attention head, the
dimension of both the query and key vectors is dk
while the dimension of the value vector is dv. The
encoder model dimension is dmodel.

The position-wise FFNN performs a non-linear
transformation on the attention output correspond-
ing to each input word position as follows:

FF(u) = WFF2(max(0,WFF1u+ bFF1)) + bFF2
(5)

in which the input vector u ∈ Rdmodel is trans-
formed to the output vector with dimension dmodel
via a series of linear projections with the ReLU
activation function.

For the hidden layer l (1 ≤ l ≤ L), the self-
attention sub-layer output f li is computed as fol-
lows:

f li = LayerNorm(χ
l
h,i + h

l−1
i )

χlh,i = MH
l
h(h

l−1
i ,h

l−1
1:n ,h

l−1
1:n ,

1√
dv

)

where LayerNorm refers to layer normalization
(Ba et al., 2016) and the superscript l and sub-
script h indicate that each encoder layer l has an
independent set of multi-head attention weight pa-
rameters (see Equations 3 and 4). The input for
the first layer is h0i = E(xi), which is the non-
contextualized word embedding of xi.

The second sub-layer consists of the position-
wise fully connected FFNN, computed as:

hli = LayerNorm(FF
l
h(f

l
i ) + f

l
i )

where, similar to self-attention, an independent set
of weight parameters in Equation 5 is defined in
each layer.

4 Incorporating Pre-Trained
Contextualized Word Representation

As BERT is trained on the masked word predic-
tion task, which is to predict a word given the sur-
rounding (left and right) context, the pre-trained
model already captures the context. In this sec-
tion, we describe different techniques of leverag-
ing BERT for WSD, broadly categorized into near-
est neighbor matching and linear projection of hid-
den layers.

4.1 Nearest Neighbor Matching

A straightforward way to disambiguate word sense
is through 1-nearest neighbor matching. We com-
pute the contextualized representation of each
word in the training data and the test data through
BERT. Given a hidden representation hLi at the L-
th layer for word xi in the test data, nearest neigh-
bor matching finds a vector h∗ in the L-th layer
from the training data such that

h∗ = argmax
h′

cos(hLi ,h
′) (6)

where the sense assigned to xi is the sense of the
word whose contextualized representation is h∗.
This WSD technique is adopted in earlier work
on contextualized word representations (Melamud
et al., 2016; Peters et al., 2018).



5300

(a) (b)

Figure 1: Illustration of WSD models by linear projection of (a) the last layer and (b) the weighted sum of all
layers.

4.2 Linear Projection of Hidden Layers

Apart from nearest neighbor matching, we can
perform a linear projection of the hidden vector hi
by an affine transformation into an output sense
vector, with its dimension equal to the number of
senses for word xi. The output of this affine trans-
formation is normalized by softmax such that all
its values sum to 1. Therefore, the predicted sense
si of word xi is formulated as

si = softmax(Wlexelt(xi)hi + blexelt(xi)) (7)

where si is a vector of predicted sense distribution
for word xi, while Wlexelt(xi) and blexelt(xi) are
respectively the projection matrix and bias vector
specific to the lexical element (lexelt) of word xi,
which consists of its lemma and optionally its part-
of-speech tag. We choose the sense corresponding
to the element of si with the maximum value.

Training the linear projection model is done
by the back-propagation algorithm, which updates
the model parameters to minimize a cost function.
Our cost function is the negative log-likelihood
of the softmax output value that corresponds to
the tagged sense in the training data. In addi-
tion, we propose two novel ways of incorporating
BERT’s hidden representation vectors to compute
the sense output vector, which are described in the
following sub-subsections.

4.2.1 Last Layer Projection
Similar to the nearest neighbor matching model,
we can feed the hidden vector of BERT in the last
layer, hLi , into an affine transformation followed
by softmax. That is, hi in Equation 7 is instan-

tiated by hLi . The last layer projection model is
illustrated in Figure 1(a).

4.2.2 Weighted Sum of Hidden Layers
BERT consists of multiple layers stacked one after
another. Each layer carries a different representa-
tion of word context. Taking into account different
hidden layers may help the WSD system to learn
from different context information encoded in dif-
ferent layers of BERT.

To take into account all layers, we compute the
weighted sum of all hidden layers, hli, where 1 ≤
l ≤ L, corresponding to a word position i, through
attention mechanism. That is, hi in Equation 7 is
replaced by the following equation:

hi = A(m,W
sh1:Li ,h

1:L
i , 1) (8)

where m ∈ Rdmodel is a projection vector to obtain
scalar values with the key vectors. The model with
weighted sum of all hidden layers is illustrated in
Figure 1(b).

4.2.3 Gated Linear Unit
While the contextualized representations in the
BERT hidden layer vectors are features that deter-
mine the word sense, some features are more use-
ful than the others. As such, we propose filtering
the vector values by a gating vector whose values
range from 0 to 1. This mechanism is known as
the gated linear unit (GLU) (Dauphin et al., 2017),
which is formulated as

GLU(h) = (h+Whh+bh)�σ(Wgh+bg) (9)

where Wh and Wg are separate projection matri-
ces and bh and bg are separate bias vectors. The



5301

symbols σ(·) and � denote the sigmoid function
and element-wise vector multiplication operation
respectively.

GLU transforms the input vector h by feeding it
to two separate affine transformations. The second
transformation is used as the sigmoid gate to filter
the input vector, which is summed with the vector
after the first affine transformation.

5 Experimental Setup

We conduct experiments on various WSD tasks.
The description and the statistics for each task are
given in Table 1. For English, a lexical element
(lexelt) is defined as a combination of lemma and
part-of-speech tag, while for Chinese, it is simply
the lemma, following the OntoNotes setup.

We exploit English BERTBASE for the English
tasks and Chinese BERT for the Chinese task.
We conduct experiments with different strategies
of incorporating BERT as described in Section 4,
namely 1-nearest neighbor matching (1-nn) and
linear projection. In the latter technique, we ex-
plore strategies including simple last layer projec-
tion, layer weighting (LW), and gated linear unit
(GLU).

In the linear projection model, we train the
model by the Adam algorithm (Kingma and Ba,
2015) with a learning rate of 10−3. The model
parameters are updated per mini-batch of 16 sen-
tences. As update progresses, we pick the best
model parameters from a series of neural network
updates based on accuracy on a held-out develop-
ment set, disjoint from the training set.

The state-of-the-art supervised WSD approach
takes into account features from the neighboring
sentences, typically one sentence to the left and
one to the right apart from the current sentence that
contains the ambiguous words. We also exploit
this in our model, as BERT supports inputs with
multiple sentences separated by a special [SEP]
symbol.

For English all-words WSD, we train our WSD
model on SemCor (Miller et al., 1994), and test
it on Senseval-2 (SE2), Senseval-3 (SE3), Se-
mEval 2013 task 12 (SE13), and SemEval 2015
task 13 (SE15). This common benchmark, which
has been annotated with WordNet-3.0 senses (Ra-
ganato et al., 2017a), has recently been adopted
in English all-words WSD. Following (Raganato
et al., 2017b), we choose SemEval 2007 Task
17 (SE07) as our development data to pick the

Task # Instances # Lexelts
English all-words
- SemCor (train) 226,036 22,436
- SemEval-2007 (dev) 455 330
- Senseval-2 (test) 2,282 1,093
- Senseval-3 (test) 1,850 977
- SemEval 2013 (test) 1,664 751
- SemEval 2015 (test) 1,022 512
English lexical sample
- Senseval-2 (train) 8,611 177
- Senseval-2 (test) 4,328 146
- Senseval-3 (train) 8,022 57
- Senseval-3 (test) 3,944 57
Chinese OntoNotes
- Train 66,409 431
- Dev 9,523 341
- BC (test) 1,769 160
- BN (test) 3,227 253
- MZ (test) 1,876 223
- NW (test) 1,483 143
- All (test) 8,355 324

Table 1: Statistics of the datasets used for the English
all-words task, English lexical sample task, and Chi-
nese OntoNotes WSD task in terms of the number of
instances and the number of distinct lexelts. For Chi-
nese WSD task, “All” refers to the concatenation of all
genres BC, BN, MZ, and NW.

best model parameters after a number of neural
network updates, for models that require back-
propagation training.

We also evaluate on Senseval-2 and Senseval-
3 English lexical sample tasks, which come with
pre-defined training and test data. For each word
type, we pick 20% of the training instances to be
our development set and keep the remaining 80%
as the actual training data. Through this develop-
ment set, we determine the number of epochs to
use in training. We then re-train the model with the
whole training dataset using the number of epochs
identified in the initial training step.

While WSD is predominantly evaluated on En-
glish, we are also interested in evaluating our ap-
proach on Chinese, to evaluate the effectiveness
of our approach in a different language. We use
OntoNotes Release 5.01, which contains a num-
ber of annotations including word senses for Chi-
nese. We follow the data setup of Pradhan et al.
(2013) and conduct an evaluation on four genres,
i.e., broadcast conversation (BC), broadcast news
(BN), magazine (MZ), and newswire (NW), as
well as the concatenation of all genres. While the
training and development datasets are divided into
genres, we train on the concatenation of all genres
and test on each individual genre.

1LDC2013T19



5302

System SE07 SE2 SE3 SE13 SE15 Avg
Reported in previous papers

MFS baseline 54.5 65.6 66.0 63.8 67.1 65.6
IMS (Zhong and Ng, 2010) 61.3 70.9 69.3 65.3 69.5 68.8
IMS+emb (Iacobacci et al., 2016) 60.9 71.0 69.3 67.3 71.3 69.7
SupWSD (Papandrea et al., 2017) 60.2 71.3 68.8 65.8 70.0 69.0
SupWSD+emb (Papandrea et al., 2017) 63.1 72.7 70.6 66.8 71.8 70.5
BiLSTMatt+LEX (Raganato et al., 2017b) 63.7 72.0 69.4 66.4 72.4 70.1
GASext Concat (Luo et al., 2018) – 72.2 70.5 67.2 72.6 70.6
context2vec (Melamud et al., 2016) 61.3 71.8 69.1 65.6 71.9 69.6
ELMo (Peters et al., 2018) 62.2 71.6 69.6 66.2 71.3 69.7

BERT nearest neighbor (ours)
1nn (1sent) 64.0 73.0 69.7 67.8 73.3 71.0
1nn (1sent+1sur) 63.3 73.8 71.6 69.2 74.4 72.3

BERT linear projection (ours)
Simple (1sent) 67.0 75.0 71.6 69.7 74.4 72.7
Simple (1sent+1sur) 69.3∗ 75.9∗ 73.4 70.4∗ 75.1 73.7∗

LW (1sent) 66.7 75.0 71.6 69.9 74.2 72.7
LW (1sent+1sur) 69.0∗ 76.4∗ 74.0∗ 70.1∗ 75.0 73.9∗

GLU (1sent) 64.9 74.1 71.6 69.8 74.3 72.5
GLU (1sent+1sur) 68.1∗ 75.5∗ 73.6∗ 71.1∗ 76.2∗ 74.1∗

GLU+LW (1sent) 65.7 74.0 70.9 68.8 73.6 71.8
GLU+LW (1sent+1sur) 68.5∗ 75.5∗ 73.4∗ 71.0∗ 76.2∗ 74.0∗

Table 2: English all-words task results in F1 measure (%), averaged over three runs. SemEval 2007 Task 17
(SE07) test set is used as the development set. We show the results of nearest neighbor matching (1nn) and linear
projection, by simple last layer linear projection, layer weighting (LW), and gated linear units (GLU). Apart
from BERT representation of one sentence (1sent), we also show BERT representation of one sentence plus one
surrounding sentence to the left and one to the right (1sent+1sur). The best result in each dataset is shown in bold.
Statistical significance tests by bootstrap resampling (∗: p < 0.05) compare 1nn (1sent+1sur) with each of Simple
(1sent+1sur), LW (1sent+1sur), GLU (1sent+1sur), and GLU+LW (1sent+1sur).

For Chinese WSD evaluation, we train IMS
(Zhong and Ng, 2010) on the Chinese OntoNotes
dataset as our baseline. We also incorporate pre-
trained non-contextualized Chinese word embed-
dings as IMS features (Taghipour and Ng, 2015;
Iacobacci et al., 2016). The pre-trained word em-
beddings are obtained by training the word2vec
skip-gram model on Chinese Gigaword Fifth Edi-
tion2, which after automatic word segmentation
contains approximately 2 billion words. Follow-
ing (Taghipour and Ng, 2015), we incorporate the
embedding features of words within a window sur-
rounding the target ambiguous word. In our ex-
periments, we take into account 5 words to the left
and right.

2LDC2011T13

6 Results

We present our experimental results and compare
them with prior baselines.

6.1 English All-Words Tasks

For English all-words WSD, we compare our ap-
proach with three categories of prior approaches.
Firstly, we compare our approach with the su-
pervised SVM classifier approach, namely IMS
(Zhong and Ng, 2010). We compare our approach
with both the original IMS without word embed-
ding features and IMS with non-contextualized
word embedding features, that is, word2vec with
exponential decay (Iacobacci et al., 2016). We
also compare with SupWSD (Papandrea et al.,
2017), which is an alternative implementation of
IMS. Secondly, we compare our approach with
the neural WSD approaches that leverage bidi-
rectional LSTM (bi-LSTM). These include the



5303

bi-LSTM model with attention trained jointly
with lexical semantic labeling task (Raganato
et al., 2017b) (BiLSTMatt+LEX) and the bi-
LSTM model enhanced with gloss representation
from WordNet (GAS). Thirdly, we show compari-
son with prior contextualized word representations
for WSD, pre-trained on a large number of texts,
namely context2vec (Melamud et al., 2016) and
ELMo (Peters et al., 2018). In these two models,
WSD is treated as nearest neighbor matching as
described in Section 4.1.

Table 2 shows our WSD results in F1 mea-
sure. It is shown in the table that with the near-
est neighbor matching model, BERT outperforms
context2vec and ELMo. This shows the effective-
ness of BERT’s pre-trained contextualized word
representation. When we include surrounding sen-
tences, one to the left and one to the right, we get
improved F1 scores consistently.

We also show that linear projection to the
sense output vector further improves WSD per-
formance, by which our best results are achieved.
While BERT has been shown to outperform other
pre-trained contextualized word representations
through the nearest neighbor matching experi-
ments, its potential can be maximized through lin-
ear projection to the sense output vector. It is
worthwhile to note that our more advanced linear
projection, by means of layer weighting (§4.2.2
and gated linear unit (§4.2.3) gives the best F1
scores on all test sets.

All our BERT WSD systems outperform gloss-
enhanced neural WSD, which has the best overall
score among all prior systems.

6.2 English Lexical Sample Tasks

For English lexical sample tasks, we compare our
approach with the original IMS (Zhong and Ng,
2010) and IMS with non-contextualized word em-
bedding features. The embedding features incor-
porated into IMS include CW embeddings (Col-
lobert et al., 2011), obtained from a convolu-
tional language model, fine-tuned (adapted) to
WSD (Taghipour and Ng, 2015) (+adapted CW),
and word2vec skip-gram (Mikolov et al., 2013)
with exponential decay (Iacobacci et al., 2016)
(+w2v+expdecay). We also compare our ap-
proach with the bi-LSTM, on top of which sense
classification is defined (Kågebäck and Salomon-
sson, 2016), and context2vec (Melamud et al.,
2016), which is a contextualized pre-trained bi-

LSTM model trained on 2B words of text. Fi-
nally, we also compare with a prior multi-task and
semi-supervised WSD approach learned through
alternating structure optimization (ASO) (Ando,
2006), which also utilizes unlabeled data for train-
ing.

System SE2 SE3
Reported

IMS 65.2 72.3
IMS+adapted CW 66.2 73.4
IMS+w2v+expdecay 69.9 75.2
BiLSTM 66.9 73.4
context2vec – 72.8
ASO+multitask+semisup – 74.1

BERT nearest neighbor (ours)
1nn (1sent) 67.7 72.7
1nn (1sent+1sur) 71.5 75.7

BERT linear projection (ours)
Simple (1sent) 73.3 78.6
Simple (1sent+1sur) 76.9∗ 80.0∗
LW (1sent+1sur) 76.7∗ 80.0∗
GLU (1sent+1sur) 75.1∗ 79.4∗

GLU+LW (1sent+1sur) 74.2∗ 79.8∗

Table 3: English lexical sample task results in accu-
racy (%), averaged over three runs. Best accuracy in
each dataset is shown in bold. Statistical significance
tests by bootstrap resampling (∗: p < 0.05) compare
1nn (1sent+1sur) with each of Simple (1sent+1sur),
LW (1sent+1sur), GLU (1sent+1sur), and GLU+LW
(1sent+1sur).

As shown in Table 3, our BERT-based WSD
approach with linear projection model outper-
forms all prior approaches. context2vec, which
is pre-trained on a large amount of texts, per-
forms worse than the prior semi-supervised ASO
approach on Senseval-3, while our best result out-
performs ASO by a large margin.

Neural bi-LSTM performs worse than IMS
with non-contextualized word embedding fea-
tures. Our neural model with pre-trained contex-
tualized word representations outperforms the best
result achieved by IMS on both Senseval-2 and
Senseval-3.

6.3 Chinese OntoNotes WSD

We compare our approach with IMS without and
with word embedding features as the baselines.
The results are shown in Table 4.

Across all genres, BERT outperforms the
baseline IMS with word embedding (non-
contextualized word representation) features
(Taghipour and Ng, 2015). The latter also
improves over the original IMS without word
embedding features (Zhong and Ng, 2010). Linear



5304

System BC BN MZ NW All
Baseline

IMS 79.8 85.7 83.0 91.0 84.8
IMS+w2v 80.2 86.4 82.3 92.0 85.2

BERT
1nn 81.7 88.5 85.1 93.3 87.1
Simple 84.6∗ 90.4∗ 88.9∗ 93.9 89.5∗
LW 84.7∗ 90.3∗ 88.8∗ 94.0 89.5∗
GLU 84.6∗ 90.0∗ 88.3∗ 93.3 89.0∗

GLU+LW 84.6∗ 90.2∗ 88.2∗ 93.4 89.2∗

Table 4: Chinese OntoNotes WSD results in accu-
racy (%), averaged over three runs, for each genre. All
BERT results in this table are obtained from the rep-
resentation of one sentence plus one surrounding sen-
tence to the left and to the right (1sent+1sur). We show
results of various BERT incorporation strategy, namely
nearest neighbor matching (1nn), simple projection,
projection with layer weighting (LW) and gated linear
unit (GLU). Best accuracy in each genre is shown in
bold. Statistical significance tests by bootstrap resam-
pling (∗: p < 0.05) compare 1nn with each of Simple,
LW, GLU, and GLU+LW.

projection approaches consistently outperform
nearest neighbor matching by a significant margin,
similar to the results on English WSD tasks.

The best overall result for the Chinese
OntoNotes test set is achieved by the models with
simple projection and hidden layer weighting.

7 Discussion

Across all tasks (English all-words, English lex-
ical sample, and Chinese OntoNotes), our ex-
periments demonstrate the effectiveness of BERT
over various prior WSD approaches. The best re-
sults are consistently obtained by linear projection
models, which project the last hidden layer or the
weighted sum of all hidden layers to an output
sense vector.

We can view the BERT hidden layer outputs as
contextual features, which serve as useful cues in
determining the word senses. In fact, the attention
mechanism in transformer captures the surround-
ing words. In prior work like IMS (Zhong and Ng,
2010), these contextual cues are captured by the
manually-defined surrounding word and colloca-
tion features. The features obtained by the hidden
vector output are shown to be more effective than
the manually-defined features.

We introduced two advanced linear projection
techniques, namely layer weighting and gated lin-
ear unit (GLU). While Peters et al. (2018) showed
that the second biLSTM layer results in better
WSD accuracy compared to the first layer (nearer

to the individual word representation), we showed
that taking into account different layers by means
of the attention mechanism is useful for WSD.
GLU as an activation function has been shown to
be effective for better convergence and to over-
come the vanishing gradient problem in the convo-
lutional language model (Dauphin et al., 2017). In
addition, the GLU gate vector, with values ranging
from 0 to 1, can be seen as a filter for the features
from the hidden layer vector.

Compared with two prior contextualized word
representations models, context2vec (Melamud
et al., 2016) and ELMo (Peters et al., 2018), BERT
achieves higher accuracy. This shows the effec-
tiveness of the attention mechanism used in the
transformer model to represent the context.

Our BERT WSD models outperform prior neu-
ral WSD models by a large margin. These prior
neural WSD models perform comparably with
IMS with embeddings as classifier features, in ad-
dition to the discrete features. While neural WSD
approaches (Kågebäck and Salomonsson, 2016;
Raganato et al., 2017b; Luo et al., 2018) exploit
non-contextualized word embeddings which are
trained on large texts, the hidden layers are trained
only on a small amount of labeled data.

8 Conclusion

For the WSD task, we have proposed novel strate-
gies of incorporating BERT, a pre-trained con-
textualized word representation which effectively
captures the context in its hidden vectors. Our ex-
periments show that linear projection of the hidden
vectors, coupled with gating to filter the values,
gives better results than the prior state of the art.
Compared to prior neural and feature-based WSD
approaches that make use of non-contextualized
word representations, using pre-trained contextu-
alized word representation with our proposed in-
corporation strategy achieves significantly higher
scores.

References

Rie Kubota Ando. 2006. Applying alternating struc-
ture optimization to word sense disambiguation. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 77–84.

Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E.
Hinton. 2016. Layer normalization. CoRR,
abs/1607.06450.



5305

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the 3rd International Conference on Learning Rep-
resentations.

Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007a. Word sense disambiguation improves sta-
tistical machine translation. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, pages 33–40.

Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007b.
NUS-PT: Exploiting parallel texts for word sense
disambiguation in the English all-words tasks. In
Proceedings of the Fourth International Workshop
on Semantic Evaluations, pages 253–256.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

Yann N. Dauphin, Angela Fan, Michael Auli, and
David Grangier. 2017. Language modeling with
gated convolutional networks. In Proceedings of the
Thirty-Fourth International Conference on Machine
Learning, pages 933–941.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 4171–4186.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2016. Embeddings for word sense
disambiguation: An evaluation study. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics, pages 897–907.

Mikael Kågebäck and Hans Salomonsson. 2016. Word
sense disambiguation using a bidirectional LSTM.
In Proceedings of the 5th Workshop on Cognitive As-
pects of the Lexicon, pages 51–56.

Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam:
A method for stochastic optimization. In Proceed-
ings of the 3rd International Conference on Learn-
ing Representations.

Yoong Keok Lee, Hwee Tou Ng, and Tee Kiah Chia.
2004. Supervised word sense disambiguation with
support vector machines and multiple knowledge
sources. In Proceedings of SENSEVAL-3, the Third
International Workshop on the Evaluation of Sys-
tems for the Semantic Analysis of Text, pages 137–
140.

Fuli Luo, Tianyu Liu, Qiaolin Xia, Baobao Chang, and
Zhifang Sui. 2018. Incorporating glosses into neural
word sense disambiguation. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics, pages 2473–2482.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end
sequence labeling via bi-directional LSTM-CNNs-
CRF. In Proceedings of the 54th Annual Meeting
of the Association for Computational Linguistics,,
pages 1064–1074.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In Proceedings of the
Thirty-First Conference on Neural Information Pro-
cessing Systems, pages 6297–6308.

Oren Melamud, Jacob Goldberger, and Ido Dagan.
2016. context2vec: Learning generic context em-
bedding with bidirectional LSTM. In Proceedings
of the 20th SIGNLL Conference on Computational
Natural Language Learning, pages 51–61.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at the International Conference on Learning Repre-
sentations.

George A. Miller, Martin Chodorow, Shari Landes,
Claudia Leacock, and Robert G. Thomas. 1994. Us-
ing a semantic concordance for sense identification.
In Human Language Technology: Proceedings of
a Workshop held at Plainsboro, New Jersey, pages
240–243.

Simone Papandrea, Alessandro Raganato, and Claudio
Delli Bovi. 2017. SupWSD: A flexible toolkit for
supervised word sense disambiguation. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing: System Demon-
strations, pages 103–108.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 2227–2237.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Hwee Tou Ng, Anders Björkelund, Olga Uryupina,
Yuchen Zhang, and Zhi Zhong. 2013. Towards ro-
bust linguistic analysis using OntoNotes. In Pro-
ceedings of the Seventeenth Conference on Com-
putational Natural Language Learning, pages 143–
152.

Alessandro Raganato, Jose Camacho-Collados, and
Roberto Navigli. 2017a. Word sense disambigua-
tion: A unified evaluation framework and empirical
comparison. In Proceedings of the 15th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 99–110.



5306

Alessandro Raganato, Claudio Delli Bovi, and Roberto
Navigli. 2017b. Neural sequence learning models
for word sense disambiguation. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1156–1167.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the Twenty-Eighth Con-
ference on Neural Information Processing Systems,
pages 3104–3112.

Kaveh Taghipour and Hwee Tou Ng. 2015. Semi-
supervised word sense disambiguation using word
embeddings in general and specific domains. In
Human Language Technologies: The 2015 Annual
Conference of the North American Chapter of the
ACL, pages 314–323.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of the Thirty-First Con-
ference on Neural Information Processing Systems,
pages 5998–6008.

Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
a wide-coverage word sense disambiguation system
for free text. In Proceedings of the ACL 2010 System
Demonstrations, pages 78–83.

Zhi Zhong and Hwee Tou Ng. 2012. Word sense dis-
ambiguation improves information retrieval. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 273–282.


