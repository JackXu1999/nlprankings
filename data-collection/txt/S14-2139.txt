



















































UoW: NLP techniques developed at the University of Wolverhampton for Semantic Similarity and Textual Entailment


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 785–789,
Dublin, Ireland, August 23-24, 2014.

UoW: NLP Techniques Developed at the University of Wolverhampton for
Semantic Similarity and Textual Entailment

Rohit Gupta, Hanna Béchara, Ismail El Maarouf and Constantin Orăsan
Research Group in Computational Linguistics,

Research Institute of Information and Language Processing,
University of Wolverhampton, UK

{R.Gupta, Hanna.Bechara, I.El-Maarouf, C.Orasan}@wlv.ac.uk

Abstract

This paper presents the system submit-
ted by University of Wolverhampton for
SemEval-2014 task 1. We proposed a ma-
chine learning approach which is based
on features extracted using Typed Depen-
dencies, Paraphrasing, Machine Transla-
tion evaluation metrics, Quality Estima-
tion metrics and Corpus Pattern Analysis.
Our system performed satisfactorily and
obtained 0.711 Pearson correlation for the
semantic relatedness task and 78.52% ac-
curacy for the textual entailment task.

1 Introduction

The SemEval task 1 (Marelli et al., 2014a) in-
volves two subtasks: predicting the degree of re-
latedness between two sentences and detecting the
entailment relation holding between them. The
task uses SICK dataset (Marelli et al., 2014b),
consisting of 10000 pairs, each annotated with re-
latedness in meaning and entailment relationship
holding between them. Similarity measures be-
tween sentences are required in a wide variety of
NLP applications. In applications like Informa-
tion Retrieval (IR), measuring similarity is a vi-
tal step in order to determine the best result for
a related query. Other applications such as Para-
phrasing and Translation Memory (TM) rely on
similarity measures to weight results. However,
computing semantic similarity between sentences
is a complex and difficult task, due to the fact that
the same meaning can be expressed in a variety of
ways. For this reason it is necessary to have more
than a surface-form comparison.

We present a method based on machine learning
which exploits available NLP technology. Our ap-

This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/

proach relies on features inspired by deep seman-
tics (such as parsing and paraphrasing), machine
translation quality estimation, machine translation
evaluation and Corpus Pattern Analysis (CPA1).

We use the same features to measure both se-
mantic relatedness and textual entailment. Our hy-
pothesis is that each feature covers a particular as-
pect of implicit similarity and entailment informa-
tion contained within the pair of sentences. Train-
ing is performed in a regression framework for se-
mantic relatedness and in a classification frame-
work for textual entailment.

The remainder of the paper is structured as fol-
lows. In Section 2, we review the work related
to our study and the existing NLP technologies
used to measure sentence similarity. In Sections 3
and 4, we describe our approach and the similarity
measures we used. In Section 5, we present the re-
sults and an analysis of our runs based on the test
and training data provided by the SemEval-2014
task. Finally, our work is summed up in Section 6
with perspectives for future work we would like to
explore.

2 Related Work

The areas of semantic relatedness and entailment
have received extensive interest from the research
community in the last decade. Earlier work in
relatedness (Banerjee and Pedersen, 2003; Li et
al., 2006) exploited WordNet in various ways to
extract the semantic relatedness. Banerjee and
Pedersen (2003) presented a measure using ex-
tended gloss overlap. This measure takes two
WordNet synsets as input and uses the overlap
of their WordNet glosses to compute their degree
of semantic relatedness. Li et al. (2006) pre-
sented a semantic similarity metric based on the
semantic similarity of words in a sentence. Re-
cently, Wang and Cer (2012) presented an ap-

1http://pdev.org.uk

785



proach that uses probabilistic edit-distance to mea-
sure semantic similarity. The approach uses prob-
abilistic finite state and pushdown automata to
model weighted edit-distance where state transi-
tions correspond to edit-operations. In some as-
pects, our work is similar to Bär et al. (2012),
who presented an approach which combines var-
ious text similarity measures using a log-linear re-
gression model.

Entailment has been modelled using various ap-
proaches. The main approaches are based on
logic inferencing (Moldovan et al., 2003), ma-
chine learning (Hickl et al., 2006; Castillo, 2010)
and tree edit-distance (Kouylekov and Magnini,
2005). Most of the recent approaches employ var-
ious syntactic or tree edit models (Heilman and
Smith, 2010; Mai et al., 2011; Rios and Gelbukh,
2012; Alabbas and Ramsay, 2013). Recently, Al-
abbas and Ramsay (2013) presented a modified
tree edit distance approach, which extends tree
edit distance to the level of subtrees. The ap-
proach extends Zhang-Shasha’s algorithm (Zhang
and Shasha, 1989).

3 Features

Our system uses the same 31 features for both sub-
tasks. This section explains them and the code
which implements most of them can be found on
GitHub2.

3.1 Language Technology Features

We used existing language processing tools to ex-
tract features. Stanford CoreNLP3 toolkit provides
lemma, parts of speech (POS), named entities, de-
pendencies relations of words in each sentence.

We calculated Jaccard similarity on surface
form, lemma, dependencies relations, POS and
named entities to get the feature values. The Jac-
card similarity computes sentence similarity by di-
viding the overlap of words on the total number of
words of both sentences.

Sim(s1, s2) =
|s1 ∩ s2|
|s1 ∪ s2| (1)

where in equation (1), Sim(s1, s2) is the Jaccard
similarity between sets of words s1 and s2.

We used the same toolkit to identify corefer-
ence relations and determine clusters of corefer-
ential entities. The coreference feature value was

2https://github.com/rohitguptacs/wlvsimilarity
3http://nlp.stanford.edu/software/corenlp.shtml

calculated using clusters of coreferential entities.
The intuition is that sentences containing corefer-
ential entities should have some semantic related-
ness. In order to extract clusters of coreferential
entities, the pair of sentences was treated as a doc-
ument. The coreference feature value using these
clusters was calculated as follows:

V aluecoref =
CC

TC
(2)

where CC is the number of clusters formed by the
participation of entities (at least one entity from
each sentence of the pair) in both sentences and
TC is the total number of clusters.

We calculated two separate feature values for
dependency relations: the first feature concate-
nated the words involved in a dependency relation
and the second used grammatical relation tags. For
example, for the sentence pair “the kids are play-
ing outdoors” and “the students are playing out-
doors” the Jaccard similarity is calculated based
on concatenated words “kids::the, playing::kids,
playing::are, ROOT::playing, playing::outdoors”
and “students::the, playing::students, playing::are,
ROOT::playing, playing::outdoors” to get the
value for the first feature and “det, nsubj, aux, root,
dobj” and “det, nsubj, aux, root, dobj” to get the
value for the second feature.

These language technology features try to cap-
ture the token based similarity and grammatical
similarity between a pair of sentences.

3.2 Paraphrasing Features
We used the PPDB paraphrase database (Ganitke-
vitch et al., 2013) to get the paraphrases. We used
lexical and phrasal paraphrases of “L” size. For
each sentence of the pair, we created two sets of
bags of n-grams (1 ≤ n ≤ length of the sentence).
We extended each set with paraphrases for each n-
gram available from paraphrase database. We then
calculated the Jaccard similarity (see Section 3.1)
between these extended bag of n-grams to get the
feature value. This feature capture the cases where
one sentence is a paraphrase of the other.

3.3 Negation Feature
Our system does not attempt to model similar-
ity with negation, but since negation is an impor-
tant feature for contradiction in textual entailment,
we designed a non-similarity feature. The system
checks for the presence of a negation word such as
‘no’, ‘never’ and ‘not’ in the pair of sentences and

786



returns “1” (“0” otherwise) if both or none of the
sentences contain any of these words.

3.4 Machine Translation Quality Estimation
Features

Seventeen of the features consist of Machine
Translation Quality Estimation (QE) features,
based on the work of (Specia et al., 2009) and used
as a baseline in recent QE tasks (such as (Callison-
Burch et al., 2012)). We extracted these features
by treating the first set of sentences as the Machine
Translation (MT) “source”, and the second set of
sentences as the MT “target”. In Machine Trans-
lation, these features are used to access the quality
of MT “target”. The QE features include shallow
surface features such as the number of punctua-
tion marks, the average length of words, the num-
ber of words. Furthermore, these features include
n-gram frequencies and language model probabil-
ities. A full list of the QE features is provided in
the documentation of the QE system4 (Specia et
al., 2009).

QE features relate to well-formedness and syn-
tax, and are not usually used to compute seman-
tic relatedness between sentences. We have used
them in the hope that the surface features at least
will show us some structural similarity between
sentences.

3.5 Machine Translation Evaluation Features
Additionally, we used BLEU (Papineni et al.,
2002), a very popular machine translation evalu-
ation metric, as a feature. BLEU is based on n-
gram counts. It is meant to capture the similarity
between translated text and references for machine
translation evaluation. The BLEU score over sur-
face, lemma and POS was calculated to get three
feature values. In a pair of sentences, one side was
treated as a translation and another as a reference.
We applied it at the sentence level to capture the
similarity between two sentences.

3.6 Corpus Pattern Analysis Features
Corpus Pattern Analysis (CPA) (Hanks, 2013) is
a procedure in corpus linguistics that associates
word meaning with word use by means of seman-
tic patterns. CPA is a new technique for map-
ping meaning onto words in text. It is currently
being used to build a “Pattern Dictionary of En-
glish Verbs”(PDEV5). It is based on the Theory of

4https://github.com/lspecia/quest
5http://pdev.org.uk

Norms and Exploitations (Hanks, 2013).
There are two features extracted from PDEV.

They both make use of a derived resource called
the CPA network (Bradbury and El Maarouf,
2013). The CPA network links verbs according
to similar semantic patterns (e.g. both ‘pour’ and
‘trickle’ share an intransitive use where the subject
is “liquid”).

The first feature value compares the main verbs
in both sentences. When both verbs share a pat-
tern, the system returns a value of “1” (otherwise
“0”). The second feature extends the CPA network
to compute the probability of a PDEV pattern,
given a word. This probability is computed over
the portion of the British National Corpus which is
manually tagged with PDEV patterns. The prob-
ability of a pattern given each word of a sentence
of the dataset is obtained by the product of those
probabilities. The feature value is the (normalised)
number of common patterns from the three most
probable patterns in each sentence. These features
try to capture similarity based on semantic pat-
terns.

4 Predicting Through Machine Learning

4.1 Model Description

We used a support vector machine in order to build
a regression model to predict semantic relatedness
and a classification model to predict textual entail-
ment. For the actual implementation we used Lib-
SVM6 (Chang and Lin, 2011).

We used a regression model for the related-
ness task that estimates a continuous score be-
tween 1 and 5 for each sentence. For the entail-
ment task, we trained a classification model which
assigns one of three different labels (ENTAIL-
MENT, CONTRADICTION, NEUTRAL) to each
sentence pair. We trained both systems on the
4500 sentence training set, augmented with the
500 sentence trial data. The values of C and γ
have been optimised through a grid-search which
uses a 5-fold cross-validation method.

The RBF kernel proved to be the best for both
tasks.

5 Results and Analysis

We submitted 4 runs of our system (Run-1 to Run-
4). Run-1 was submitted as primary run. Run-2,
Run-3 and Run-4 systems were identical except

6http://www.csie.ntu.edu.tw/ cjlin/libsvm/

787



Run-1 Run-2 Run-3 Run-4
C 8 8 2 2
γ 0.0441 0.0441 0.125 0.125

Pearson 0.7111 0.7166 0.6968 0.6975

Table 1: Results: Relatedness.

for some parameter differences for SVM train-
ing and the replacement of the values which were
outside the boundaries (1-5). If relatedness val-
ues predicted by the system were less than 1 or
greater than 5, these values were replaced by 1
and 5 respectively for Run-1, Run-2 and Run-4
and 1.5 and 4.5 respectively for Run-3. Our pri-
mary run also used one extra feature for related-
ness, which was obtained by considering entail-
ment judgement as a feature. Our hypothesis was
that entailment judgement may help in measur-
ing relatedness. In the actual test this feature was
not helpful and we obtained Pearson correlation of
0.711 for the primary run, compared to 0.716 for
Run-2. The details of runs are given in Table 1 and
2.

After training both models, we ran a feature
selection algorithm to determine which features
yielded the highest accuracy, and therefore had the
highest impact on our system. Perhaps unsurpris-
ingly, the QE features were not very useful in pre-
dicting semantic similarity or entailment. How-
ever, despite their focus on fluency rather than se-
mantic correctness, the QE features still managed
to contribute to some improvements in the textual
entailment task (increasing accuracy by 1%), and
the semantic relatedness task (0.027 increase in
Pearson correlation).

In the entailment (classification) task, the
strongest feature proved to be the negation fea-
ture with 70% accuracy (on the training set) when
training on this feature only. This suggests that
some measure of negation is crucial in determin-
ing whether a sentence contradicts or entails an-
other sentence. Other strong features were lemma,
paraphrasing and dependencies.

In the relatedness (regression) task, the lemma,
surface, paraphrasing, dependencies, PDEV fea-
tures were the strongest contributors to accuracy.

Run-1 Run-2 Run-3 Run-4
C 16 16 8 8
γ 0.0625 0.0625 0.5 0.5

Accuracy 78.526 78.526 78.343 78.343

Table 2: Results: Entailment.

6 Conclusion and Future Work

We have presented an efficient approach to calcu-
late semantic relatedness and textual entailment.
One noticeable point of our approach is that we
have used the same features for both tasks and
our system performed well in each of these tasks.
Therefore, our system captures reasonably good
models to compute semantic relatedness and tex-
tual entailment.

In the future we would like to explore more fea-
tures and particularly those based on tree edit dis-
tance, WordNet and PDEV. Our intuition suggests
that tree edit distance seems to be more helpful for
entailment, whereas WordNet and PDEV seem to
be more helpful for similarity measurement. Ad-
ditionally, we would like to combine our tech-
niques for measuring relatedness and entailment
with MT evaluation techniques. We would fur-
ther like to apply these techniques cross-lingually,
moving into other areas like machine translation
evaluation and quality estimation.

Acknowledgement

The research leading to these results has received
funding from the People Programme (Marie Curie
Actions) of the European Union’s Seventh Frame-
work Programme FP7/2007-2013/ under REA
grant agreement no. 317471 and partly supported
by an AHRC grant “Disambiguating Verbs by Col-
location project, AH/J005940/1, 2012-2015”.

References
Maytham Alabbas and Allan Ramsay. 2013. Natural

language inference for Arabic using extended tree
edit distance with subtrees. Journal of Artificial In-
telligence Research, 48:1–22.

Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In IJCAI, volume 3, pages 805–810.

Daniel Bär, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In First Joint Conference on

788



Lexical and Computational Semantics, Association
for Computational Linguistics, pages 435–440.

Jane Bradbury and Ismaı̈l El Maarouf. 2013. An
empirical classification of verbs based on Semantic
Types: the case of the ’poison’ verbs. In Proceed-
ings of the Joint Symposium on Semantic Process-
ing. Textual Inference and Structures in Corpora,
pages 70–74.

Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia, editors.
2012. Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation. Association for Com-
putational Linguistics, Montréal, Canada, June.

Julio J. Castillo. 2010. Recognizing textual en-
tailment: experiments with machine learning al-
gorithms and RTE corpora. Special issue: Natu-
ral Language Processings and its Applications, Re-
search in Computing Science, 46:155–164.

Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1–27:27.

Juri Ganitkevitch, Van Durme Benjamin, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In Proceedings of NAACL-HLT, pages
758–764, Atlanta, Georgia.

Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. Mit Press.

Michael Heilman and Noah A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In The 2010 An-
nual Conference of the North American Chapter of
the ACL, number June, pages 1011–1019.

Andrew Hickl, Jeremy Bensley, John Williams, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Rec-
ognizing textual entailment with LCC’s GROUND-
HOG system. In Proceedings of the Second PAS-
CAL Challenges Workshop.

Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance
algorithms. In Proceedings of the First Challenge
Workshop Recognising Textual Entailment, pages
17–20.

Yuhua Li, David McLean, Zuhair A Bandar, James D
O’shea, and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
Knowledge and Data Engineering, IEEE Transac-
tions on, 18(8):1138–1150.

Zhewei Mai, Y Zhang, and Donghong Ji. 2011. Rec-
ognizing text entailment via syntactic tree match-
ing. In Proceedings of NTCIR-9 Workshop Meeting,
pages 361–364, Tokyo, Japan.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014).

Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014b. A sick cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC 2014.

Dan Moldovan, Christine Clark, Sanda Harabagiu, and
Steve Maiorano. 2003. COGEX : A Logic Prover
for Question Answering. In Proceedings of HLT-
NAACL, number June, pages 87–93.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
ACL, pages 311–318.

Miguel Rios and Alexander Gelbukh. 2012. Recog-
nizing Textual Entailment with a Semantic Edit Dis-
tance Metric. In 11th Mexican International Confer-
ence on Artificial Intelligence, pages 15–20. IEEE.

Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In 13th Conference of the European Asso-
ciation for Machine Translation, pages 28–37.

Mengqiu Wang and Daniel Cer. 2012. Stanford: prob-
abilistic edit distance metrics for STS. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics, pages 648–654.

Kaizhong Zhang and Dennis Shasha. 1989. Simple
Fast Algorithms for the Editing Distance between
Trees and Related Problems. SIAM Journal on Com-
puting, 18(6):1245–1262.

789


