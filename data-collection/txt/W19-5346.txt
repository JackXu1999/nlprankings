



















































IITP-MT System for Gujarati-English News Translation Task at WMT 2019


Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 407–411
Florence, Italy, August 1-2, 2019. c©2019 Association for Computational Linguistics

407

IITP-MT System for Gujarati-English News Translation Task
at WMT 2019

Sukanta Sen, Kamal Kumar Gupta, Asif Ekbal, Pushpak Bhattacharyya
Department of Computer Science and Engineering

Indian Institute of Technology Patna
{sukanta.pcs15,kamal.pcs17,asif,pb}@iitp.ac.in

Abstract

We describe our submission to WMT 2019
News translation shared task for Gujarati-
English language pair. We submit con-
strained systems, i.e, we rely on the data
provided for this language pair and do not
use any external data. We train Transformer
based subword-level neural machine transla-
tion (NMT) system using original parallel cor-
pus along with synthetic parallel corpus ob-
tained through back-translation of monolin-
gual data. Our primary systems achieve BLEU
scores of 10.4 and 8.1 for Gujarati→English
and English→Gujarati, respectively. We ob-
serve that incorporating monolingual data
through back-translation improves the BLEU
score significantly over baseline NMT and
SMT systems for this language pair.

1 Introduction

In this paper, we describe the system that we
submit to the WMT 20191 news translation
shared task (Bojar et al., 2019). We par-
ticipate in Gujarati-English language pair and
submit two systems: English→Gujarati and
Gujarati→English. Gujarati language belongs to
Indo-Aryan language family and is spoken pre-
dominantly in the Indian state of Gujarat. It
is a low-resource language as only a few thou-
sands parallel sentences are available, which are
not enough to train a neural machine translation
(NMT) system as well statistical machine trans-
lation (SMT) system. Gujarati-English is a dis-
tant language pair and they have different linguis-
tic properties including syntax, morphology, word
order etc. English follows subject-verb-object or-
der while Gujarati follows subject-object-verb or-
der.

1http://www.statmt.org/wmt19/
translation-task.html

NMT (Kalchbrenner and Blunsom, 2013; Cho
et al., 2014; Sutskever et al., 2014; Bahdanau et al.,
2015) has recently become dominant paradigm
for machine translation (MT) achieving state-of-
the-art on standard benchmark data sets for many
language pairs. As opposed to SMT, NMT sys-
tems are trained in an end-to-end manner. Train-
ing an effective NMT requires a huge amount of
high-quality parallel corpus and in absence of that,
an NMT system tends to perform poorly (Koehn
and Knowles, 2017). However, back-translation
(Sennrich et al., 2016) has been shown to im-
prove NMT systems in such a situation. In this
work, we train a SMT system and an NMT system
for both English→Gujarati and Gujarati→English
using the original training data. SMT systems
are also used to generate synthetic parallel cor-
pora through back-translation of monolingual data
from English news crawl and Gujarati Wikipedia
dumps. These corpora along with the original
training corpora are used to improve the baseline
NMT systems. All the SMT and NMT systems are
trained at subword level.

Our SMT systems are standard phrase-based
SMT systems (Koehn et al., 2003), and NMT sys-
tems are based on Transformer (Vaswani et al.,
2017) architecture. Experiments show that NMT
systems achieve BLEU (Papineni et al., 2002)
scores of 10.4 and 8.1 for Gujarati→English
and English→Gujarati, respectively, outperform-
ing the baseline SMT systems even in the absence
of enough-sized parallel data.

Rest of the paper is arranged in following man-
ner: Section 2 gives brief introduction of the
Transformer architecture that we used for NMT
training, Section 3 describes the task, Section 4 de-
scribes the submitted systems, Section 5 gives var-
ious evaluation scores for English-Gujarati trans-
lation pair, and finally, Section 6 concludes the
work.

http://www.statmt.org/wmt19/translation-task.html
http://www.statmt.org/wmt19/translation-task.html


408

2 Transformer Architecture

Recurrent neural network based encoder-decoder
NMT architecture (Cho et al., 2014; Sutskever
et al., 2014; Bahdanau et al., 2015) deals with in-
put/output sentences word-by-word sequentially,
which prevents the model from parallel com-
putation. Vaswani et al. (2017) came up with
a highly parallelizable architecture called Trans-
former which uses the self-attention to better en-
code a sequences. Self-attention is used in the ar-
chitecture to calculate attention between a word
and the other words in the sentence itself. En-
coder and decoder both are stack of 6 identical lay-
ers. Each layer in encoder has two sub-layers: i.
multi-head self attention mechanism and ii. posi-
tion wise feed forward network. Each sub-layer
is associated with residual connections, followed
by layer normalization. Multi-head attention com-
putes the attention multiple times for each word.
Since their is no sequence to sequence encoding,
positional encoding is used to encode the sequence
information.

3 Task Description

This task focuses on translating news domain cor-
pus and this year, Gujarati language is introduced
for the first time in a WMT shared task. Gujarati
is a low-resource language and not many results
have been reported in machine translation involv-
ing this language. Also, there was no standard test
set for this language pair. So introduction of this
language pair will help in further research for this
language pair.

As Gujarati does not have enough parallel
data, the data that are provided for this shared
task are mainly from WikiTitles which con-
sists of only 11,671 parallel titles. Apart from
that, few publicly available domain specific par-
allel data that are provided are: Bible corpus
(Christodouloupoulos and Steedman, 2015); a lo-
calization extracted from OPUS2; parallel corpus
extracted from Wikipedia; crawled corpus pro-
duced for this task; and monolingual Wikipedia
dumps.

4 System Description

We participated in Gujarati-English pair only and
we submit for both directions: English→Gujarati

2http://opus.nlpl.eu

and Gujarati→English. As Gujarati is a low-
resource language and only a little amount of
parallel data is available, we explore the back-
translation technique for this pair. Also our mod-
els are based on Transformer as it has become state
of the art for machine translation for many lan-
guage pairs. We train systems at subword level.
For back-translation, we train a phrase-based SMT
(Koehn et al., 2003) system for each system in re-
verse direction. Using these SMT systems, mono-
lingual sentences (for both Gujarati and English)
are translated to create synthetic parallel data hav-
ing original monolingual sentences at target and
translated sentences at source side. These syn-
thetic parallel data, along with the original paral-
lel data are used to train a transformer based NMT
system for each direction.

4.1 Dataset

Sources #Sentences
Parallel

Bible 7,807
govin-clean.gu-en.tsv 10,650
opus.gu-en.tsv 107,637
wikipedia.gu-en.tsv 18,033
wikititles-v1.gu-en.tsv 11,671
Total 155,798

Monolingual
Gujarati (Wikipedia dump) 382,881
English (News crawl) 1,000,000

Table 1: Training data sources and number of sen-
tences.

The datasets that we use for training are
shown in the Table 1, which combine to a to-
tal of 155,798 parallel sentences. These par-
allel data are compiled from different sources.
The compiled datasets are Bible3, govin-clean.gu-
en.tsv4, opus.gu-en.tsv5, wikipedia.gu-en.tsv6 and
wikititles-v1.gu-en.tsv7. We use newsdev2019 for
tuning the model, which has 1,998 parallel sen-
tences.

3http://data.statmt.org/wmt19/translation-task/bible.gu-
en.tsv.gz

4http://data.statmt.org/wmt19/translation-task/govin-
raw.gu-en.tsv.gz

5http://data.statmt.org/wmt19/translation-task/opus.gu-
en.tsv.gz

6http://data.statmt.org/wmt19/translation-
task/wikipedia.gu-en.tsv.gz

7http://data.statmt.org/wikititles/v1/wikititles-v1.gu-
en.tsv.gz



409

System BLEU BLEU-cased TER CharactTER
English→Gujarati

PBSMT 5.2 5.2 0.987 0.782
Transformer 4.0 4.0 1.005 0.884
Transformer + Synth 8.1 8.1 0.919 0.763

Gujarati→English
PBSMT 7.3 6.3 0.883 0.817
Transformer 5.5 5.1 0.905 0.859
Transformer + Synth 10.4 9.4 0.828 0.774

Table 2: BLEU scores of different SMT and NMT based systems. Synth: Synthetic data

Apart from these parallel data, we use
monolingual English (news crawl) and Gujarati
(Wikipedia dumps) sentences for synthetic par-
allel data creation. After training two models
i.e. English→Gujarati and Gujarati→English us-
ing the parallel data mentioned in Table 1, En-
glish and Gujarati monolingual sentences are back
translated respectively.

4.2 Experimental Setup

We train phrase based statistical system (PB-
SMT) (Koehn et al., 2003) as well as Transformer
(Vaswani et al., 2017) based neural system for
comparing their performance under low-resource
conditions. In addition to that, PBSMT are used to
genrate synthetic parallel data. PBSMT systems
are trained only on original training data, while
neural based models are trained on original train-
ing data (Transfomer in Table 2), and also with
synthetic parallel data in addition to original data
(Transfomer+Synth in Table 2). Synthetic paral-
lel data are obtained through back-translation of a
target monolingual corpus into source using PB-
SMT system. We use Moses (Koehn et al., 2007)
toolkit for PBSMT training and Sockeye (Hieber
et al., 2017) toolkit for NMT training. Some pre-
processing of data is required before using it for
experiment. English data is tokenized using moses
tokenizer, and truecased. For tokenizing Gujarati
data, we use indic nlp library8. After tokeninza-
tion and truecasing, we subword (Sennrich et al.,
2015) all original data. We apply 10,000 BPE
merge operations over English and Gujarati data
independently.

For back-translation of monolingual data,
two PBSMT models English→Gujarati and
Gujarati→English are trained over original avail-
able parallel subworded corpora. 4-gram lan-

8https://github.com/anoopkunchukuttan/indic nlp library

guage model is trained using KenLM (Heafield,
2011). For word alignment, we use GIZA++ (Och
and Ney, 2003) with grow-diag-final-and heuris-
tics. Model is tuned with Minimum Error Rate
Training (Och, 2003). After these two models
are trained, monolingual subworded data from
both English and Gujarati are back-translated us-
ing English→Gujarati and Gujarati→English PB-
SMT model, respectively. We merge the back
translated data with original parallel data to have
larger parallel corpora for Gujarati→English and
English→Gujarati translation directions.

Finally, with the augmented parallel corpora,
we train one Transformer based NMT model for
each direction. We use the following hyper-
parameters values of Sockeye toolkit: 6 layers in
both encoder and decoder, word embedding size of
512, hidden size of 512, maximum input length of
50 tokens, Adam optimizer, word batch size 1000,
attention type is dot, learning rate of 0.0002. The
rest of the hyper-parameters are set to the default
values in Sockeye. We use early stopping criteria
for terminating the training on the validation set of
1,998 parallel sentences.

5 Results

The official automatic evaluation uses the follow-
ing metrics: BLEU (Papineni et al., 2002), TER
(Snover et al., 2006), CharactTER (Wang et al.,
2016). The official scores are shown in the Ta-
ble 2. Phrase-base SMT (PBSMT) obtains BLEU
scores of 5.2 and 7.3 for English→Gujarati and
Gujarati→Englsih, respectively. Whereas, base-
line NMT (Transformer) obtains lower BLEU
scores of 4.0 and 5.5 for the same directions.
Though, SMT systems outperforms baseline NMT
systems trained using small amount of original
parallel data only. We observe from the Table 2
that Transformer with synthetic (Transformer +



410

English→German
Ave. Ave. z System
90.3 0.347 Facebook-FAIR
93.0 0.311 Microsoft-WMT19-sent-doc
92.6 0.296 Microsoft-WMT19-doc-level
90.3 0.240 HUMAN
87.6 0.214 MSRA-MADL
88.7 0.213 UCAM
89.6 0.208 NEU
87.5 0.189 MLLP-UPV
87.5 0.130 eTranslation
86.8 0.119 dfki-nmt
84.2 0.094 online-B
86.6 0.094 Microsoft-WMT19-sent-level
87.3 0.081 JHU
84.4 0.077 Helsinki-NLP
84.2 0.038 online-Y
83.7 0.010 lmu-ctx-tf-single
84.1 0.001 PROMT-NMT
82.8 −0.072 online-A
82.7 −0.119 online-G
80.3 −0.129 UdS-DFKI
82.4 −0.132 TartuNLP-c
76.3 −0.400 online-X
43.3 −1.769 en-de-task

Gujarati→English
Ave. Ave. z System
64.8 0.210 NEU
61.7 0.126 UEDIN
59.4 0.100 GTCOM-Primary
60.8 0.090 CUNI-T2T-transfer
59.4 0.066 aylien-mt-multilingual
59.3 0.044 NICT
51.3 −0.189 online-G
50.9 −0.192 IITP-MT
48.0 −0.277 UdS-DFKI
47.4 −0.296 IIITH-MT
41.1 −0.598 Ju-Saarland

English→Gujarati
Ave. Ave. z System
73.1 0.701 HUMAN
72.2 0.663 online-B
66.8 0.597 GTCOM-Primary
60.2 0.318 MSRA-CrossBERT
58.3 0.305 UEDIN
55.9 0.254 CUNI-T2T-transfer
52.7 −0.079 Ju-Saarland-clean-num-135-bpe
35.2 −0.458 IITP-MT
38.8 −0.465 NICT
39.1 −0.490 online-G
33.1 −0.502 online-X
33.2 −0.718 UdS-DFKI

Kazakh→English
Ave. Ave. z System
72.2 0.270 online-B
70.1 0.218 NEU
69.7 0.189 rug-morfessor
68.1 0.133 online-G
67.1 0.113 talp-upc-2019
67.0 0.092 NRC-CNRC
65.8 0.066 Frank-s-MT
65.6 0.064 NICT
64.5 0.003 CUNI-T2T-transfer
48.9 −0.477 UMD
32.1 −1.058 DBMS-KU

Lithuanian→English
Ave. Ave. z System
77.4 0.234 GTCOM-Primary
77.5 0.216 tilde-nc-nmt
77.0 0.213 NEU
76.4 0.206 MSRA-MASS
76.4 0.202 tilde-c-nmt
73.8 0.107 online-B
69.4 −0.056 online-A
69.2 −0.059 TartuNLP-c
62.8 −0.284 online-G
62.4 −0.337 JUMT
59.1 −0.396 online-X

German→English
Ave. Ave. z System
81.6 0.146 Facebook-FAIR
81.5 0.136 RWTH-Aachen
79.0 0.136 MSRA-MADL
79.9 0.121 online-B
79.0 0.086 JHU
80.1 0.067 MLLP-UPV
79.0 0.066 dfki-nmt
78.0 0.066 UCAM
76.6 0.050 online-A
78.4 0.039 NEU
79.0 0.027 HUMAN
77.4 0.011 uedin
77.9 0.009 online-Y
74.8 0.006 TartuNLP-c
72.9 −0.051 online-G
71.8 −0.128 PROMT-NMT
69.7 −0.192 online-X

English→Czech
Ave. Ave. z System
91.2 0.642 HUMAN
86.0 0.402 CUNI-DocTransformer-T2T
86.9 0.401 CUNI-Transformer-T2T-2018
85.4 0.388 CUNI-Transformer-T2T-2019
81.3 0.223 CUNI-DocTransformer-Marian
80.5 0.206 uedin
70.8 −0.156 online-Y
71.4 −0.195 TartuNLP-c
67.8 −0.300 online-G
68.0 −0.336 online-B
60.9 −0.594 online-A
59.3 −0.651 online-X

Finnish→English
Ave. Ave. z System
78.2 0.285 MSRA-NAO
77.8 0.265 online-Y
77.6 0.261 GTCOM-Primary
76.4 0.245 USYD
72.5 0.107 online-B
73.3 0.105 Helsinki-NLP
69.2 0.012 online-A
68.4 −0.044 online-G
68.0 −0.053 TartuNLP-c
67.3 −0.071 online-X
61.9 −0.209 parfda
53.3 −0.516 apertium-uc

English→Finnish
Ave. Ave. z System
94.8 1.007 HUMAN
82.6 0.586 GTCOM-Primary
80.2 0.570 MSRA-NAO
70.9 0.275 online-Y
65.8 0.199 NICT
65.7 0.09 Helsinki-NLP
63.1 0.072 online-G
63.0 0.037 online-B
54.5 −0.125 TartuNLP-c
48.3 −0.384 online-A
47.1 −0.398 online-X
47.9 −0.522 Helsinki-NLP-rule-based
16.9 −1.260 apertium-uc

English→Kazakh
Ave. Ave. z System
81.5 0.746 HUMAN
67.6 0.262 UAlacant-NMT
63.8 0.243 online-B
63.8 0.222 UAlacant-NM
63.8 0.222 RBMT
63.3 0.126 NEU
63.3 0.108 MSRA-CrossBERT
60.4 0.097 CUNI-T2T-transfer
61.7 0.078 online-G
55.2 −0.049 rug-bpe
49.0 −0.328 talp-upc-2019
41.4 −0.493 NICT
11.6 −1.395 DBMS-KU

English→Lithuanian
Ave. Ave. z System
90.5 1.017 HUMAN
72.8 0.388 tilde-nc-nmt
69.1 0.387 MSRA-MASS-uc
68.0 0.262 tilde-c-nmt
68.2 0.259 MSRA-MASS-c
67.7 0.155 GTCOM-Primary
62.7 0.036 eTranslation
59.6 −0.054 NEU
57.4 −0.061 online-B
47.8 −0.383 TartuNLP-c
38.4 −0.620 online-A
39.2 −0.666 online-X
32.6 −0.805 online-G

English→Russian
Ave. Ave. z System
89.5 0.536 HUMAN
88.5 0.506 Facebook-FAIR
83.6 0.332 USTC-MCC
82.0 0.279 online-G
80.4 0.269 online-B
79.0 0.223 NEU
80.2 0.219 PROMT-NMT
78.5 0.156 online-Y
71.7 −0.188 rerank-er
67.9 −0.268 online-A
68.8 −0.310 TartuNLP-u
62.1 −0.363 online-X
35.7 −1.270 NICT

English→Chinese
Ave. Ave. z System
82.5 0.368 HUMAN
83.0 0.306 KSAI
83.3 0.280 Baidu
80.5 0.209 NEU
80.3 0.052 online-A
79.9 0.042 xzl-nmt
79.0 0.017 UEDIN
77.8 0.009 BTRANS
76.9 0.000 NICT
74.6 −0.125 online-B
75.6 −0.218 online-Y
72.6 −0.262 online-G
69.5 −0.553 online-X

Russian→English
Ave. Ave. z System
81.4 0.156 Facebook-FAIR
80.7 0.134 online-G
80.4 0.122 eTranslation
80.1 0.121 online-B
81.4 0.115 NEU
80.4 0.102 MSRA-SCA
79.8 0.084 rerank-re
79.2 0.076 online-Y
79.0 0.029 online-A
76.8 0.012 afrl-syscomb19
76.8 −0.039 afrl-ewc
76.2 −0.040 TartuNLP-u
74.5 −0.097 online-X
69.3 −0.303 NICT

Chinese→English
Ave. Ave. z System
83.6 0.295 Baidu-system
82.7 0.266 KSAI-system
81.7 0.203 MSRA-MASS
81.5 0.195 MSRA-MASS
81.5 0.193 NEU
80.6 0.186 BTRANS
80.7 0.161 online-B
79.2 0.103 BTRANS-ensemble
77.9 0.054 UEDIN
78.0 0.049 online-Y
77.4 0.001 NICT
75.3 −0.065 online-A
72.4 −0.202 online-G
66.9 −0.483 online-X
56.4 −0.957 Apprentice-c

Table 8: Preliminary results of WMT19 News Translation Task. Systems ordered by DA score z-score; systems within a
cluster are considered tied; lines indicate clusters according to Wilcoxon rank-sum test p < 0.05; grayed entry indicates
resources that fall outside the constraints provided.

Table 3: Preliminary official results of WMT 2019
news translation task for Gujarati-English pair. Sys-
tems ordered by DA score z-score; systems within a
cluster are considered tied; lines indicate clusters ac-
cording to Wilcoxon rank-sum test p < 0.05; grayed
entry indicates resources that fall outside the con-
straints provided.

Synth) data obtained through back-translation of
monolingual data, outperforms the baseline SMT
systems with a margin of 2.9 and 3.1 BELU
points. Also, as a result of augmenting back-
translated data with original training data, we ob-
tain improvement of of 4.7 and 5.3 BLEU points
over baseline NMT for English→Gujarati and
Gujarati→English, respectively. The official pre-
liminary human evaluation results are shown in the
Table 3.

6 Conclusion

In this paper, we described our submission to
the WMT 2019 News translation shared task for
Gujarati-English language pair. This is the first
time Gujarati language is introduced in a WMT
shared task. We submit Transformer based NMT
systems for English-Gujarati language pair. Since

the number of parallel sentences in training set
are very less and many sentences have length
of only 2-3 tokens, BLEU scores for English-
Gujarati pair using only available parallel corpus
are very low (4.0 and 5.1 for English→Gujarati
and Gujarati→English, respectively). So we
use monolingual sentences for both languages
to create synthetic parallel data through back-
translation, and merged them with original parallel
data. We obtained improved BLEU scores of 8.1
and 10.4, respectively.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural Machine Translation by Jointly
Learning to Align and Translate. In Proceedings of
the 3rd International Conference on Learning Rep-
resentation (ICLR 2015).

Ondřej Bojar, Christian Federmann, Mark Fishel,
Yvette Graham, Barry Haddow, Matthias Huck,
Philipp Koehn, Christof Monz, Mathias Müller, and
Matt Post. 2019. Findings of the 2019 conference on
machine translation (wmt19). In Proceedings of the
Fourth Conference on Machine Translation, Volume
2: Shared Task Papers, Florence, Italy. Association
for Computational Linguistics.

Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the Properties
of Neural Machine Translation: Encoder-decoder
Approaches. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in
Statistical Translation, pages 103–111.

Christos Christodouloupoulos and Mark Steedman.
2015. A massively parallel corpus: the Bible in
100 languages. Language resources and evaluation,
49(2):375–395.

Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187–197. Association for Computational Linguis-
tics.

Felix Hieber, Tobias Domhan, Michael Denkowski,
David Vilar, Artem Sokolov, Ann Clifton, and Matt
Post. 2017. Sockeye: A toolkit for neural machine
translation. arXiv preprint arXiv:1712.05690.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Continuous Translation Models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2013), pages
1700–1709.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source



411

toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177–180. Association for Computational Linguis-
tics.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Pro-
ceedings of the First Workshop on Neural Machine
Translation, pages 28–39, Vancouver. Association
for Computational Linguistics.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54. Association for Computa-
tional Linguistics.

Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160–167. As-
sociation for Computational Linguistics.

Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational linguistics, 29(1):19–51.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th annual meeting on association for com-
putational linguistics, pages 311–318, Philadelphia,
Pennsylvania.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2016, August 7-12, 2016,
Berlin, Germany.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, volume 200.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to Sequence Learning with Neural Net-
works. In Proceedings of Advances in neural in-
formation processing systems (NIPS 2014), pages
3104–3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998–6008.

Weiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl,
and Hermann Ney. 2016. Character: Translation
edit rate on character level. In Proceedings of the
First Conference on Machine Translation: Volume
2, Shared Task Papers, volume 2, pages 505–510.


