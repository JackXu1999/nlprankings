



















































UTU: Disease Mention Recognition and Normalization with CRFs and Vector Space Representations


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 807–811,
Dublin, Ireland, August 23-24, 2014.

UTU: Disease Mention Recognition and Normalization with CRFs and
Vector Space Representations

Suwisa Kaewphan1,2,3∗, Kai Hakaka1∗, Filip Ginter1
1 Dept. of Information Technology, University of Turku, Finland

2 Turku Centre for Computer Science (TUCS), Turku, Finland
3 The University of Turku Graduate School (UTUGS), University of Turku, Finland

sukaew@utu.fi, kahaka@utu.fi, ginter@cs.utu.fi

Abstract

In this paper we present our system par-
ticipating in the SemEval-2014 Task 7
in both subtasks A and B, aiming at
recognizing and normalizing disease and
symptom mentions from electronic medi-
cal records respectively. In subtask A, we
used an existing NER system, NERsuite,
with our own feature set tailored for this
task. For subtask B, we combined word
vector representations and supervised ma-
chine learning to map the recognized men-
tions to the corresponding UMLS con-
cepts. Our system was placed 2nd and 5th
out of 21 participants on subtasks A and B
respectively showing competitive perfor-
mance.

1 Introduction

The SemEval 2014 task 7 aims to advance the de-
velopment of tools for analyzing clinical text. The
task is organized by providing the researchers an-
notated clinical records to develop systems that
can detect the mentions of diseases and symptoms
in medical records. In particular, the SemEval task
7 comprises two subtasks, recognizing the men-
tions of diseases and symptoms (task A) and map-
ping the mentions to unique concept identifiers
that belong to the semantic group of disorders in
the Unified Medical Language System (UMLS).

Our team participated in both of these sub-
tasks. In subtask A, we used an existing named
entity recognition (NER) system, NERsuite, sup-
plemented with UMLS dictionary and normaliza-
tion similarity features. In subtask B, we com-
bined compositional word vector representations

∗These authors contributed equally.
This work is licensed under a Creative Commons At-

tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/

with supervised machine learning to map the rec-
ognized mentions from task A to the UMLS con-
cepts. Our best systems, evaluated on strict match-
ing criteria, achieved F-score of 76.6% for the sub-
task A and accuracy of 60.1% for the subtask B,
showing competitive performance in both tasks.

2 Task A: Named Entity Recognition
with NERSuite

The ML approach based on conditional random
fields (CRFs) has shown to have state-of-the-art
performance in recognizing the biological entities.
We thus performed task A by using NERsuite,
an existing NER toolkit with competitive perfor-
mance on biological entity recognition (Campos
et al., 2013).

NERsuite is a NER system that is built on
top of the CRFsuite (Okazaki, 2007). It con-
sists of three language processing modules: a to-
kenizer, a modified version of the GENIA tagger
and a named entity recognizer. NERsuite allows
user-implemented features in addition to dictio-
nary matching and features shown to benefit the
systems such as raw token, lemma, part-of-speech
(POS) and text chunk.

Prior to detecting the disease mentions by the
recognizer module of NERsuite, the clinical text
is split into sentences by using GENIA Sentence
Splitter, a supervised ML system that is known to
be well optimized for biomedical texts (Sætre et
al., 2007). The sentences are subsequently tok-
enized and POS tagged.

To represent the positive entities, the “BIO”
model was used in our system. The first tokens
of positive mentions are labeled with “B” and the
rest with “I”. Negative examples, non-entities, are
thus labeled with “O”. This model was used for
both contiguous and discontiguous entities.

The features include the normalization similar-
ity (see Section 3.3), types of medical records (dis-
charge, echo, radiology and ecg), and UMLS dic-

807



Trained Model Precision Recall F-score
train + positive samples 77.3% 72.4% 74.8%
train + development 76.7 % 76.5% 76.6%

Table 1: The results of our different NERsuite
models, announced by the organizers.

tionary matching in addition to NERsuite’s own
feature generation.

The UMLS dictionary is prepared by extract-
ing the UMLS database for the semantic types de-
manded by the task. In addition to those 11 seman-
tic types, “Finding” was also included in our dic-
tionary since, according to its definition, the con-
cept is also deemed relevant for the task. Due to
the common use of acronyms, which are not ex-
tensively provided by UMLS, we also extended
the coverage of our prepared UMLS dictionary
by extracting medical acronyms from the UMLS
database using regular expression.

We assessed the effect of dictionary matching
by training the models with and without the com-
piled UMLS dictionary and evaluating against the
development set. The model trained with dictio-
nary features outperformed the one without. The
best model was obtained by training the NERsuite
with UMLS dictionary in case-number-symbol
normalization mode. In this mode, all letters,
numbers and symbols are converted to lower case,
zero (0) and underscore ( ) respectively.

The regularization parameter (C2) was selected
by using development set to evaluate the best
model. The default parameter (C2 = 1.0) gave
the best performing system and thus was used
throughout the work.

Finally, for the NER task, we submitted two
models. The first model was trained with the orig-
inal training data and duplicates of sentences with
at least one entity mention. The second model
was trained by using the combination of the first
model’s training data and development set.

2.1 Results and Discussions

Our NER system from both submissions benefited
from the increased number of training examples
while the more diverse training data set gave a bet-
ter performance. The official results are shown in
table 1.

The analysis of our best performing NER sys-
tem is not possible since the gold standard of the
test data is not publicly available. We thus simply
analyze our second NER system based on the eval-

uation on the development data. The F-score of the
system was 75.1% and 88.0% for the strict and re-
laxed evaluation criteria respectively. Among all
the mistakes made by the system, the discontigu-
ous entities were the most challenging ones for the
NERsuite. In development data, the discontiguous
entities contribute about 10% of all entities, how-
ever, only 2% were recognized correctly. On the
contrary, the system did well for the other types as
73% were correctly recognized under strict crite-
ria. This demonstrates that the “BIO” model has
limitations in representing the discontiguous enti-
ties. Improving the model to better represent the
discontiguous entities can possibly boost the per-
formance of the NER system significantly.

3 Task B: Normalization with
Compositional Vector Representations

Our normalization approach is based on con-
tinuous distributed word vector representations,
namely the state-of-the-art method word2vec
(Mikolov et al., 2013a). Our word2vec model
was trained on a subset of abstracts and full ar-
ticles from the PubMed and PubMed Central re-
sources. This data was used as it was readily
available to us from the EVEX resource (Van Lan-
deghem et al., 2013). Before training, all non-
alphanumeric characters were removed and all to-
kens were lower-cased. Even though a set of unan-
notated clinical reports was provided in the task
to support unsupervised learning methods, our ex-
periments on the development set showed better
performance with the model trained with PubMed
articles. This might be due to the size of the cor-
pora, as the PubMed data included billions of to-
kens whereas the provided clinical reports totaled
in over 200 million tokens.

The dimensionality of the word vectors was set
to 300 and we used the continuous skip-gram ap-
proach. For other word2vec parameters default
values were used.

One interesting feature demonstrated by
Mikolov et al. (2013b; 2013c) is that the vectors
conserve some of the semantic characteristics in
element-wise addition and subtraction. In this task
we used the same approach of simply summing
the word-level vectors to create compositional
vectors for multi-word entities and concepts, i.e.
we looked up the vectors for every token appear-
ing in a concept name or entity and summed them
to form a vector to represent the whole phrase.

808



We then formed a lexicon including all preferred
terms and synonyms of all the concepts in the
subset of UMLS defined in the task guidelines.
This lexicon is a mapping from the compositional
vector representations of the concept names into
the corresponding UMLS identifiers. To select the
best concept for a recognized entity we calculated
cosine similarity between the vector representa-
tion of the given entity and all the concept vectors
in the lexicon and the concept with the highest
similarity was chosen.

Word2vec is generally able to relate different
forms of the same word to each other, but we no-
ticed a small improvement in accuracy when pos-
sessive suffixes were removed and all tokens were
lemmatized.

3.1 Detecting CUI-less Mentions

As some of the mentions in the training data do not
have corresponding concepts in the semantic cat-
egories listed in the task guidelines, they are an-
notated as “CUI-less”. However, our normaliza-
tion approach will always find the nearest match-
ing concept, thus getting penalized for wrong pre-
dictions in the official evaluation. To overcome
this problem, we implemented three separate steps
for detecting the “CUI-less” mentions. As the
simplest approach we set a fixed cosine similarity
threshold and if the maximal similarity falls below
it, the mention is normalized to “CUI-less”. The
threshold value was selected using a grid search to
optimize the performance on the official develop-
ment set. Although this method resulted in decent
performance, it is not capable of coping with cases
where the mention has very high similarity or even
exact match with a concept name. For instance
our system normalized “aspiration” mentions into
UMLS concept “Pulmonary aspiration” which has
a synonym “Aspiration”, thus resulting in an exact
match. To resolve this kind of cases, we used sim-
ilar approach as in the DNorm system (Leaman et
al., 2013b), where the “CUI-less” mentions occur-
ring several times in the training data were added
to the concept lexicon with concept ID “CUI-less”.
As the final step we trained a binary SVM classi-
fier to distinguish the “CUI-less” mentions. The
classifier utilized bag-of-word features as well as
the compositional vectors. The performance im-
provement provided by each of these steps is pre-
sented in table 2. This evaluation shows that each
step increases the performance considerably, but

Method Strict accuracy
B 43.6
T 48.4
T + L 53.5
T + L + C 55.4
O 59.3

Table 2: Evaluation of the different approaches
to detect CUI-less entities on the official develop-
ment set compared to a baseline without CUI-less
detection and an oracle method with perfect de-
tection. This evaluation was done with the entities
recognized by our NER system instead of the gold
standard entities. B = baseline without CUI-less
detection, T = similarity threshold, L = Lexicon-
based method, C = classifier, O = Oracle.

the overall performance is still 3.9pp below per-
fect detection.

3.2 Acronym Resolution

Abbreviations, especially acronyms, form a con-
siderable portion of the entity mentions in clini-
cal reports. One of the problems in normalizing
the acronyms is disambiguation as one acronym
can be associated with multiple diseases. Previ-
ous normalization systems (Leaman et al., 2013b)
handle this by selecting the matching concept with
most occurrences in the training data. However,
this approach does not resolve the problem of
non-standard acronyms, i.e. acronyms that are not
known in the UMLS vocabulary or in other medi-
cal acronym dictionaries. Our goal was to resolve
both of these problems by looking at the other enti-
ties found in the same document instead of match-
ing the acronym against the concept lexicon. With
this approach for instance entity mention “CP”
was on multiple occasions correctly normalized
into the concept “Chest Pain”, even though UMLS
is not aware of this acronym for the given concept
and in fact associates it with several other con-
cepts such as “Chronic Pancreatitis” and “Cerebral
Palsy”. However, the overall gain in accuracy ob-
tained from this method was only minor.

3.3 Normalization Feedback to Named
Entity Recognition

While basic exact match dictionary features pro-
vide usually a large improvement in NER perfor-
mance, they are prone to bias the system to high
precision and low recall. As both noun and ad-
jective forms of medical concepts, e.g. “atrium”
and “atrial”, are commonly used in clinical texts,

809



the entities may not have exact dictionary matches.
Moreover the different forms of medical terms
may not share a common morphological root dis-
covered by simple stemming methods, thus com-
plicating approximate matching. In this task we
tried to boost the recall of our entity recognition by
feeding back the normalization similarity informa-
tion as features. These features included the max-
imum similarity between the token and the UMLS
concepts as a numerical value as well as a boolean
feature describing whether the similarity exceeded
a certain threshold.

In addition we experimented by calculating the
similarities for bigrams and trigrams in a slid-
ing window around the tokens, but these features
did not provide any further performance improve-
ments.

3.4 Other Directions Explored

The DNorm system utilizes TF-IDF vectors to rep-
resent the entities and concepts but instead of cal-
culating cosine similarity, the system trains a rank-
ing algorithm to measure the maximal similarity
(Leaman et al., 2013a). Their evaluation, carried
out on the NCBI disease corpus (Doğan et al.,
2014), showed a notable improvement in perfor-
mance compared to cosine similarity. In our anal-
ysis we noticed that in 39% of the false predic-
tions made by our normalization system, the cor-
rect concept was in the top 10 most similar con-
cepts. This strongly suggested that a similar rank-
ing method might be beneficial with our system as
well. To test this we trained a linear SVM to rerank
the top 10 concepts with highest cosine similarity,
but we were not able to increase the overall per-
formance of the system. However, due to the strict
time constraints of the task, we cannot conclude
whether this approach is feasible or not.

As our compositional vectors are formed by
summing the word vectors, each word has an equal
weight in the sum. Due to this our system made
various errors where the entity was a single word
matching closely to several concepts with longer
names. For instance entity “hypertensive” was
falsely normalized to concept “Hypertensive car-
diopathy” whereas the correct concept was “Hy-
pertensive disorder”. These mistakes could have
been prevented to some extent if the more impor-
tant words had had a larger weight in the sum, e.g.
word “disorder” is of low significance when try-
ing to distinguish different disorders. However,

Team Strict accuracy Relaxed accuracy
UTH CCB 74.1 87.3
UWM 66.0 90.9
RelAgent 63.9 91.2
IxaMed 60.4 86.2
UTU 60.1 78.3

Table 3: Official evaluation results for the top 5
teams in the normalization task.

weighting the word vectors with their IDF values,
document in this case being an UMLS concept, did
not improve the performance.

3.5 Results

The official results for the normalization task are
shown in table 3. Our system achieved accuracy
of 60.1% when evaluated with the official strict
evaluation metric. This result suggests that com-
positional vector representations are a competitive
approach for entity normalization. However, the
best performing team surpassed our performance
by 14.0pp, showing that there is plenty of room for
other teams to improve. It is worth noting though
that their recall in the NER task tops ours by 8.2pp
thus drastically influencing the normalization re-
sults as well. To evaluate the normalization sys-
tems in isolation from the NER task, a separate
evaluation set with gold standard entities should
be provided.

4 Conclusions

Overall, our NER system can perform well with
the same default settings of NERsuite for gene
name recognition. The performance improves
when relevant features, such as UMLS dictionary
matching and word2vec similarity are added. We
speculated that representing the nature of the data
with more suitable model can improve the system
performance further. As a part of a combined sys-
tem, the improvement on NER system can result
in the increased performance of normalization sys-
tem.

Our normalization system showed competitive
results as well, indicating that word2vec-based
vector representations are a feasible way of solv-
ing the normalization task. As future work we
would like to explore different methods for cre-
ating the compositional vectors and reassess the
applicability of the reranking approach described
in section 3.4.

810



Acknowledgements

Computational resources were provided by CSC
— IT Center for Science Ltd, Espoo, Finland. This
work was supported by the Academy of Finland.

References
David Campos, Sérgio Matos, and José Luı́s Oliveira.

2013. Gimli: open source and high-performance
biomedical name recognition. BMC bioinformatics,
14(1):54.

Rezarta Islamaj Doğan, Robert Leaman, and Zhiyong
Lu. 2014. NCBI disease corpus: a resource for dis-
ease name recognition and concept normalization.
Journal of Biomedical Informatics, 47:1–10, Feb.

Robert Leaman, Rezarta Islamaj Doğan, and Zhiyong
Lu. 2013a. DNorm: disease name normaliza-
tion with pairwise learning to rank. Bioinformatics,
29(22):2909–2917.

Robert Leaman, Ritu Khare, and Zhiyong Lu.
2013b. NCBI at 2013 ShARe/CLEF eHealth Shared
Task: Disorder normalization in clinical notes with
DNorm. In Proceedings of the Conference and Labs
of the Evaluation Forum.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In ICLR Workshop.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS, pages 3111–3119.

Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746–
751.

Naoaki Okazaki. 2007. CRFsuite: a fast im-
plementation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.

Rune Sætre, Kazuhiro Yoshida, Akane Yakushiji,
Yusuke Miyao, Y Matsubyashi, and Tomoko Ohta.
2007. AKANE system: protein-protein interaction
pairs in BioCreAtIvE2 challenge, PPI-IPS subtask.
In Proceedings of the BioCreative II, pages 209–
212.

Sofie Van Landeghem, Jari Björne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, and Filip Ginter. 2013. Large-
scale event extraction from literature with multi-
level gene normalization. PLoS ONE, 8(4):e55814.

811


