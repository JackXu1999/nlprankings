



















































Learning the Extraction Order of Multiple Relational Facts in a Sentence with Reinforcement Learning


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 367–377,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

367

Learning the Extraction Order of Multiple Relational Facts in a Sentence
with Reinforcement Learning

Xiangrong Zeng1,2,3, Shizhu He1,2, Daojian Zeng4, Kang Liu1,2, Jun Zhao1,2
1NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China

2University of Chinese Academy of Sciences, Beijing, 100049, China
3Unisound AI Technology Co.,Ltd, Beijing, 100000, China

4Changsha University of Science & Technology, Changsha, 410114, China
{xiangrong.zeng, shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn

zengdj@csust.edu.cn

Abstract

The multiple relation extraction task tries to
extract all relational facts from a sentence. Ex-
isting works didn’t consider the extraction or-
der of relational facts in a sentence. In this pa-
per we argue that the extraction order is impor-
tant in this task. To take the extraction order
into consideration, we apply the reinforcement
learning into a sequence-to-sequence model.
The proposed model could generate relational
facts freely. Widely conducted experiments on
two public datasets demonstrate the efficacy of
the proposed method.

1 Introduction

Relation extraction (RE) is a core task in natural
language processing (NLP). RE can be used in in-
formation extraction (Wu and Weld, 2010), ques-
tion answering (Yih et al., 2015; Dai et al., 2016)
and other NLP tasks. Most existing works as-
sumed that a sentence only contains one relational
facts (a relational fact, or a triplet, contains a rela-
tion and two entities). But in fact, a sentence of-
ten contains multiple relational facts (Zeng et al.,
2018b). The multiple relation extraction task tries
to extract all relational facts from a sentence.

Existing works on multiple relation extraction
task can be divided into five genres. 1) Pseu-
doPipeline genre, including Miwa and Bansal
(2016); Sun et al. (2018). They first recognized all
the entities of the sentence, then extracted features
for each entity pair and predicted their relation.
They trained the entity recognition model and re-
lation prediction model together instead of sepa-
rately. Therefore, we call them PseudoPipeline
methods. 2) TableFilling genre, including Miwa
and Sasaki (2014); Gupta et al. (2016) and Zhang
et al. (2017). They maintained a entity-relation
table and predicted a semantic tag (either entity
tags or relation tags) for each cell in the table.

According to the predicted tags, they can recog-
nize the entities and the relation between each en-
tity pair. 3) NovelTagging genre, including Zheng
et al. (2017). This method can be seen as a de-
velopment of TableFilling method. They assigned
a pre-defined semantic tag to each word of the
sentence and collected triplets based on the tags.
Their tags include both entity and relation in-
formation. Therefore, they don’t need to main-
tain a entity-relation table. 4) MultiHeadSelec-
tion genre, including Bekoulis et al. (2018a) and
Bekoulis et al. (2018b). They first recognized the
entities, then they formulated the relation extrac-
tion task as a multi-head selection problem. For
each entity, they calculated the score between it
and every other entities for a given relation. The
combination of the entity pair and relation with
the score exceeding a threshold will be kept as a
triplet. 5) Generative genre, including Zeng et al.
(2018b). They directly generate triplets one by
one by a sequence-to-sequence model with copy
mechanism (Gu et al., 2016; Vinyals et al., 2015).
To generate a triplet, they first generated the rela-
tion, then they copy the first entity and the second
entity from the source sentence.

However, none of them have considered the ex-
traction order of multiple triplets in a sentence.
Given a sentence, the PseudoPipeline methods ex-
tract relations of different entity pairs separately.
Although they jointly training the entity model
and relation model, they ignore the influence be-
tween triplets actually. The TableFilling, NovelT-
agging and MultiHeadSelection methods extract
the triplets in the word order of this sentence. They
firstly deal with the first word, then the second one
and so on. The generative method could generate
triplets in any order actually. However, Zeng et al.
(2018b) randomly choose the extraction order of
the triplets in each sentence. Sorting the triplets
in the sentences of training data beforehand with



368

Sentence
Cubanelle is in Arros negre, a dish from the 
Catalonia region. 

Relational 
Facts

#1 ��: <Arros negre, food_region, Catalonia>

#2 ��: <Arros negre, ingredient, Cubanelle>

Figure 1: Example of multiple relation extraction. In
this example, it’s easier to extract F2 first. The extrac-
tion of F1 can be benefit from F2.

global rules (e.g., alphabetical order) is straight-
forward. But one global sorting rule may not fit
every sentences.

In this paper, we argue that the extraction order
of triplets in a sentence is important. Take Figure
1 as example. It’s difficult to extract F1 first be-
cause we don’t know what “Arros negre” is in the
first place. Extracting F2 is more straightforward
as the key words “dish”, “region” in the sentence is
helpful. F2 can help us to extract F1 because now
we are confident that “Arros negre” is some kind of
food, so that “ingredient” is a suitable relation be-
tween “Arros negre” and “Cubanelle”. From this
intuitive example, we can see that the extracted
triplets could influence the extraction of the re-
maining triplets.

To automatically learning the extraction order
of multiple relational facts in a sentence, we pro-
pose a sequence-to-sequence model and apply re-
inforcement learning (RL) on it. we follow the
Generative genre because such a model could ex-
tract triplets in various order, which is convenient
for us to explore the influence of triplets extrac-
tion order. Our model reads in a raw sentence and
generates triplets one by one. Thus, all triplets in
a sentence could be extracted. To take the triplets
extraction order into consideration, we convert the
triplets generation process as a RL process. The
sequence-to-sequence model is regarded as the RL
policy. The action is what we generate in each
time step. We assume that a better generation
order could lead to more valid generated triplets.
The RL reward is related to the generated triplets.
In general, the more triplets are correctly gener-
ated, the higher the reward. Unlike supervised
learning with negative log likelihood (NLL) loss,
which forces the model to generate triplets in the
order of the ground truth, reinforcement learning
allows the model generate triplets freely to achieve
higher reward.

The main contributions of this work are:

• We discuss the triplets extraction order prob-

lem in the multiple relation extraction task. In
our knowledge, this problem has never been
addressed before.

• We apply reinforcement learning method on
a sequence-to-sequence model to handle this
problem.

• We conduct widely experiments on two pub-
lic datasets. Experimental results show that
the proposed method outperform the strong
baselines with 3.4% and 5.5% improvements
respectively.

2 Related Work

Given a sentence with two annotated entities (an
entity pair), the relation classification task aims
to identify the predefined relation between these
two entities. Zeng et al. (2014) was among the
first to apply neural networks in relation classifi-
cation task. They adopted the Convolutional Neu-
ral Network (CNN) to learn the sentence represen-
tation automatically. In the following, dos San-
tos et al. (2015); Xu et al. (2015a) also applied
CNN to extract relation. Xu et al. (2015b) uti-
lized shortest dependency path between two en-
tities with a LSTM (Hochreiter and Schmidhuber,
1997) based recurrent neural network. Zhou et al.
(2016) applied attention mechanism to learn dif-
ferent weights for each word and used LSTM to
represent sentence. These methods all assumed
that the entity pair is given beforehand and a sen-
tence only contains two entities.

To extract both entities and relation from sen-
tence, early works like Zelenko et al. (2003); Chan
and Roth (2011) adopted pipeline methods. How-
ever, such pipeline methods neglect the relevance
between entities and relation. Latter works fo-
cused on joint models that extract entities and rela-
tion jointly. Yu and Lam (2010); Li and Ji (2014);
Miwa and Bansal (2016) relied on NLP tools to do
feature engineering, which suffered from the error
propagation problem. Miwa and Sasaki (2014);
Gupta et al. (2016); Zhang et al. (2017) applied
neural networks to jointly extract entities and rela-
tions. They converted the relation extraction task
into a table filling task. Zheng et al. (2017) took a
step further and converted this task into a tagging
task. They assigned a semantic tag to each word in
the sentence and collected triplets according to the
tag information. Bekoulis et al. (2018b,a) model
the relation extraction task as a multi-head selec-



369

tion problem. However, these models can not take
triplet’s extraction order into consideration. Sun
et al. (2018) proposed a joint learning paradigm
based on minimum risk training. Their method ig-
nore the influence between relational facts. Zeng
et al. (2018b) proposed an sequence-to-sequence
model with copy mechanism to handle the over-
lapping problem in multiple relation extraction.
They randomly choose a extraction order for each
sentence.

RL has attracted lot of attention recently. It has
been successfully applied in many games (Mnih
et al., 2015; Silver et al., 2016). Narasimhan et al.
(2015); He et al. (2016) applied RL on text based
games. Narasimhan et al. (2016) employed deep
Q-network to optimize a reward function that re-
flects the extraction accuracy while penalizing ex-
tra effort. Li et al. (2016) applied policy gra-
dient method to model future reward in chatbot
dialogue. They designed a reward to promote
three conversational properties: informativity, co-
herence and ease of answering. Su et al. (2016)
using on-line activate reward learning for policy
optimization in spoken dialogue systems because
the user feedback is often unreliable and costly to
collect. Yu et al. (2017) applied RL method to
overcome the limitations that the Generative Ad-
versarial Net (GAN) in generating sequences of
discrete tokens. Our work is related to Li et al.
(2016); Yu et al. (2017) since we also apply RL to
generate better sequences.

There are several works that related to both re-
lation extraction and RL, which are also related to
our work. Zeng et al. (2018a); Feng et al. (2018);
Qin et al. (2018) applied RL to distantly super-
vised relation extraction task. Zeng et al. (2018a)
turned the bag relation prediction into an RL pro-
cess. They assumed that the relation of the bag is
determined by the relation of sentences from the
bag. They set the final reward to +1 or -1 by com-
paring the predict bag relation with the gold re-
lation. Feng et al. (2018) adopted policy gradi-
ent method to select high-quality sentences from
the bag. The selected sentences are feed to the
relation classifier and the relation classifier pro-
vides rewards to the instance selector. Similarly,
Qin et al. (2018) explored a deep RL strategy to
generate the false-positive indicator. Our work is
different from them since we focus on supervised
relation extraction task.

3 Method

We first introduce our basic model and then intro-
duce how to apply RL on it. Similar to Zeng et al.
(2018b), our neural model is also a sequence-to-
sequence model with copy mechanism. It reads
in a raw sentence and generates triplets one by
one. Instead of training the model with NLL
loss, we regard the triplets generation process as
a RL process and optimize the model with REIN-
FORCE (Williams, 1992) algorithm. Therefore,
we don’t have to determine the triplets order of
each sentence beforehand, we let the model gen-
erate triplets freely. We show the RL process in
Figure 2.

3.1 Sequence-to-Sequence Model with Copy
Mechanism

The sequence-to-sequence model with copy mech-
anism is a kind of CopyNet (Gu et al., 2016)
or PointerNetwork (Vinyals et al., 2015). Two
components included in this model: encoder and
decoder. The encoder is a bi-directional recur-
rent neural network, which is used to encode a
variable-length sentence into a fixed-length vec-
tor. We denote the outputs of encoder as OE =
[oE1 , ..., oEn ], where oEi denotes the output of i-th
word of the encoder and n is the sentence length.

The decoder is another recurrent neural net-
work, which is used to generate triplets one by
one. The NA-triplets will be generated if the valid
triplets number is less than the maximum triplets
number.1 It takes three-time steps to generate one
triplet. That is, in time step t (t = 1, 2, 3, ..., T ),
if t%3 = 1, we predict the relation. If t%3 = 2,
we copy the first entity and if t%3 = 0, we copy
the second entity. T is the maximum decode time
step. Note that T is always divisible by 3.

Suppose there are m predefined valid relations,
in time step t (t = 1, 4, 7, ...), we calculate the
confidence score for each valid relation:

qrt = selu(o
D
t ·Wrt + brt ) (1)

where oDt is the output of decoder in time step t;
Wrt is the weight matrix and brt is the bias in time
step t; selu(·) (Klambauer et al., 2017) is activa-
tion function. To allow the model to generate NA-
triplet, we also calculate the confidence score for

1NA-triplet is a special triplet proposed in Zeng et al.
(2018b). It’s similar to the “eos” symbol in neural sentence
generation.



370

generate

calculate

optimize

sequence-to-sequence model with copy mechanism

Model

Triplets

Triplet 2Triplet 1

capital, Sudan, Khartoum, capital, Sudan, Khartoum

[1,1,1,0,0,0]

RewardSentence

News of the list’s existence unnerved 
officials in Khartoum, Sudan ’s capital.

generate

calculate

optimize

sequence-to-sequence model with copy mechanism

Model

Triplets

Triplet 2Triplet 1

capital, Sudan, Khartoum, capital, Sudan, Khartoum

[1,1,1,0,0,0]

RewardSentence

News of the list’s existence unnerved 
officials in Khartoum, Sudan ’s capital.

Figure 2: The RL process. The model reads in a raw sentence and generates triplets. Then, a reward is assigned to
each time step based on the generated triplets. Lastly, the rewards is used to optimize the model.

NA relation:

qNAt = selu(o
D
t ·WNAt + bNAt ) (2)

where WNAt and bNAt are parameters in time step
t. Then we concatenate qrt and qNAt and perform
softmax to obtain the probability distribution:

prt = softmax([q
r
t ;q

NA
t ]) (3)

To copy the first entity in time step t (t =
2, 5, 8, ...), we calculate the confidence score of
each word in source sentence:

qeti = selu([o
D
t ; o

E
i ] · wet ) (4)

where qeti is the confidence score of i-th word and
wet is the weight vector, in time step t. Similarly,
to take the NA-triplet into consideration, we also
calculate the confidence score for NA entity with
Eq 2. We concatenate them and perform softmax
to obtain the probability distribution:

pet = softmax([[q
e
t1, ..., q

e
tn];q

NA
t ]) (5)

Copy the second entity in time step t (t =
3, 6, 9, ...) is almost the same as the first entity.
The only difference is we also apply the mask
(Zeng et al., 2018b) to avoid the copied two en-
tities are the same.

Our model is similar to OneDecoder model and
MultiDecoder model in Zeng et al. (2018b). Com-
pared with OneDecoder model, our model using
different linear transformation parameters in dif-
ferent decoding time step. Compared with Multi-
Decoder model, our model using only one decoder
cell to decode all triplets. In our model, we didn’t
using attention mechanism because we found that
the attention mechanism makes no difference to
the results.

3.2 Reinforcement Learning Process

We regard the triplets generation process as RL
process. The loop in Figure 2 represents a RL
episode. In each RL episode, the model reads in
the raw sentence and generate output sequence.
Then we gain triplets from the output sequence
and calculate rewards based on them. Finally, we
optimize the model with REINFORCE algorithm.

State
We use st to denote the state of sentence x in
decoding time step t. The state st contains the
already generated tokens ŷ<t, the information of
source sentence x and the model parameters θ.

st = (ŷ
<t, x, θ) (6)

Action
The action is what we predict (or copy) in each
time step. In time step t and t%3 = 1, the model
(policy) is required to determine the relation of the
triplet; In time step t where t%3 = 2 or 0, the
model is required to determine the first or second
entity, which is copied from the source sentence.
Therefore, the action space A is varied in different
time step t.

A =

{
R, t%3 = 1

P, t%3 = 2, 0
(7)

where R is the predefined relations and P is the
positions of source sentence. We denote the ac-
tion sequence of the source sentence as a =
[a1, ..., aT ].



371

Algorithm 1 Reward Assignment
Input: Sampled action sequence a = [a1, ..., aT ];
Gold triplets set G; NA-triplet NA.
Output: Rewards of each action r = [r1, ..., rT ].

1: Number of generated triplets K = T/3;
Generated triplets list F = [F1, ..., FK ];
Already generated triplets set V = {}.

2: for each i ∈ [1,K] do
3: Fi = [a3∗i−2, a3∗i−1, a3∗i]
4: end for
5: for each i ∈ [1,K] do
6: r3∗i−2 = r3∗i−1 = r3∗i = 0
7: if i ≤ |G| then
8: if Fi ∈ G and Fi /∈ V then
9: Add Fi to V

10: r3∗i−2 = r3∗i−1 = r3∗i = 1
11: end if
12: else if Fi = NA then
13: r3∗i−2 = r3∗i−1 = r3∗i = 0.5
14: end if
15: end for

Reward
The reward is used to guide the training, which is
critical to RL training. However, we can’t assign
a reward to each step directly during the genera-
tion since we don’t know whether each action we
choose is good or not before we finish the gen-
eration. Remind that we could obtain a triplet in
every three steps. Once we obtained a triplet, we
can compare it with the gold triplets and know
if this triplet is good or not. A well generated
triplet means it’s the same with one of the gold
triplets and not the same with any already gener-
ated triplets.

When we obtained a good triplet after three
steps, we assign reward 1 to each of these three
steps. Otherwise, we assign reward 0 to them. Af-
ter generating valid triplets, we may need to gen-
erate NA-triplets. We assign reward 0.5 to each
of these three steps if we correctly generate NA-
triplet and reward 0 otherwise. We show the de-
tails of the reward assignment in Algorithm 12.

3.3 Training

The model can be trained with either supervised
learning loss or reinforcement learning loss. How-
ever, the supervised learning forces the model to

2How to determine the reward in RL is difficult. We tried
several different reward assignments but only this one works.

generate triplets in the order of the ground truth
while the reinforcement learning allows the model
generate triplets freely.

NLL Loss
Training the model with NLL loss requires a pre-
defined ground truth sequence for each sentence.
Suppose T is the maximum time step of de-
coder, we denote the ground truth sequence as
[y1, ..., yt, ..., yT ]. Them the NLL loss for sentence
x can be defined as:

L =
1

T

T∑
t=1

−log(p(yt|ŷ<t, x, θ)) (8)

where ŷ<t is the already generated tokens; p(·|·) is
the conditional probability; θ is the parameters of
the entire model.

RL Loss
Training the model with reinforcement learning
only require the ground truth triplets for each sen-
tence. The RL loss for sentence x is:

L =
1

T

T∑
t=1

−log(p(ŷt|ŷ<t, x, θ)) ∗ rt (9)

where ŷt is the sampled action and rt is the reward,
in time step t.

4 Experiments

4.1 Dataset
Following Zeng et al. (2018b), we test our method
on two open datasets: New York Times (NYT)
dataset and WebNLG dataset.

NYT dataset is proposed by Riedel et al. (2010).
This dataset is produced by distant supervision
method which automatically aligns Freebase with
New York Times news articles. Like Zheng et al.
(2017); Zeng et al. (2018b) do, we ignore the noise
in this dataset and use it as a supervised dataset.
We use the pre-processed dataset used in Zeng
et al. (2018b), which contains 5000 sentences in
the test set and 5000 sentences in the validation
set and 56195 sentences in the train set. In the
train set, there are 36868 sentences that contain
one triplet, 19327 sentences that contain multiple
triplets. In the test set, the sentence number are
3244 and 1756, respectively. There are 24 rela-
tions in total.

WebNLG dataset is proposed by Gardent et al.
(2017). This data set is originally created for Nat-
ural Language Generation (NLG) task. Given a



372

group of triplets, annotators are asked to write
a sentence which contains the information of all
triplets in this group. We use the dataset pre-
processed by Zeng et al. (2018b) and the train set
contains 5019 sentences, the test set contains 703
sentences and the validation set contains 500 sen-
tences. In the train set, there are 1596 sentences
that contain one triplet, 3423 sentences contain
multiple triplets. In the test set, the sentence num-
ber are 266 and 437, respectively. There are 246
different relations.

4.2 Settings
Zeng et al. (2018b) only use LSTM as the model
cell. In this paper, we report the results of both
LSTM and GRU (Cho et al., 2014). We follow the
most settings from Zeng et al. (2018b). The cell
unit number is set to 1000; The embedding dimen-
sion is set to 100; The batch size is 100; The max-
imum time step T is 15, that is, we will extract 5
triplets for each sentence; We use Adam (Kingma
and Ba, 2015) to optimize parameters and stop the
training when we find the best result in validation
set. For the NLL training, the learning rate in both
dataset is 0.001. For the RL training, we first pre-
train the model with NLL training (pretrain model
achieves 80%-90% of the best NLL training per-
formance), then training the model with RL. The
RL learning rate is 0.0005.

4.3 Evaluation Metrics
We follow the evaluation metrics in Zeng et al.
(2018b). Our model can only copy one word
for each entity and we use the last word
of each entity to represent them. Triplet
is regarded as correct when its relation, the
first entity and the second entity are all cor-
rect. For example, suppose the gold triplets
is < Barack Obama, president, USA >, <
Obama, president, USA > is regarded as cor-
rect while < Obama, locate, USA > and <
Barack, prsident, USA > are not. A triplet is
regarded as NA-triplet when and only when its re-
lation is NA relation and it has a NA entity pair.
The predicted NA-triplet will be excluded. We use
the standard micro Precision, Recall and F1 score
to evaluate the results.

4.4 Results of Different Extraction Order
To find out if the triplets extraction order of a sen-
tence can make difference in multiple relation ex-
traction task, we conduct widely experiments on

both NYT and WebNLG dataset. We show the re-
sults of different extraction order of different mod-
els with LSTM cell in Table 1. The results of mod-
els with GRU cell are shown in Appendix B. We
box the best results of a model and the bold values
are the best results in this dataset.

CNN denotes the baseline with CNN classifier.
We use the NLTK toolkit3 to recognize the en-
tities first. Then we combine every two entities
as an entity pair. Every two entities can lead to
two different entity pairs. For each entity pair, we
apply a CNN classifier (Zeng et al., 2014) to de-
termine the relation. We leave the details of this
model in Appendix A. ONE and MULTI denotes
the OneDecoder model and MultiDecoder model
in Zeng et al. (2018b).4 NLL means the model
is trained with NLL loss, which requires a prede-
fined ground truth sequence for each sentence. For
a sentence with N triplets, there are N ! (the facto-
rial of N ) possible extraction order, which lead to
N ! valid sequences. Shuffle means we randomly
select one valid sequence as the ground truth se-
quence in every training epoch for a sentence. Fix-
Unsort means we randomly select one valid se-
quence before training, and use the selected one
as ground truth sequence during training. This
strategy is used in Zeng et al. (2018b). Alpha-
betical means we sort the triplets of a sentence in
alphabetical order and build ground truth sequence
based on the sorted triplets. Frequency means we
sort the triplets of a sentence based on the relation
frequency. We count the relation frequency from
the training set. RL means the model is trained
with reinforcement learning. In NYT dataset, we
using Alphabetical strategy to pretrain the model,
and in WebNLG dataset, we pretrain the model
wtih Frequency strategy.

Form Table 1, we can observe that: (a) The
CNN baseline is not performing well because this
model neglect the influence between triplets. (b)
Compared with FixUnsort strategy, simply change
the ground truth sequence in different training
epoch (the Shuffle strategy) is also not good. The
performance of OneDecoder drops from 0.566
to 0.552 in NYT dataset and 0.305 to 0.283 in
WebNLG dataset. (c) In both dataset and for all
models trained with NLL loss, sort the triplets
in some order (Alphabetical or Frequency order)

3https://www.nltk.org/
4We reimplement these two models and find that the at-

tention mechanism is not important. Therefore, we report the
results without applying the attention mechanism.



373

Model
NYT WebNLG

Precision Recall F1 Precision Recall F1
CNN 0.468 0.620 0.533 0.304 0.417 0.352
ONE+NLL (Shuffle) 0.635 0.488 0.552 0.400 0.219 0.283
ONE+NLL (FixUnsort) 0.606 0.530 0.566 0.320 0.291 0.305
ONE+NLL (Alphabetical) 0.616 0.553 0.583 0.305 0.292 0.298
ONE+NLL (Frequency) 0.609 0.546 0.576 0.323 0.311 0.317
ONE+RL 0.715 0.533 0.610 0.668 0.307 0.421
MULTI+NLL (Shuffle) 0.629 0.522 0.570 0.363 0.278 0.315
MULTI+NLL (FixUnsort) 0.600 0.562 0.580 0.434 0.431 0.432
MULTI+NLL (Alphabetical) 0.685 0.647 0.665 0.482 0.481 0.481
MULTI+NLL (Frequency) 0.648 0.604 0.625 0.519 0.518 0.518
MULTI+RL 0.742 0.639 0.687 0.611 0.523 0.564
Our+NLL (Shuffle) 0.670 0.545 0.601 0.417 0.310 0.356
Our+NLL (FixUnsort) 0.645 0.591 0.617 0.490 0.488 0.489
Our+NLL (Alphabetical) 0.717 0.678 0.697 0.533 0.537 0.535
Our+NLL (Frequency) 0.688 0.652 0.669 0.581 0.587 0.584
Our+RL 0.779 0.672 0.721 0.633 0.599 0.616

Table 1: Results of different extraction order of models with LSTM cell.

can lead to better performance. For example, our
model achieves 0.617 F1 score under FixUnsort
strategy, while 0.697 and 0.669 F1 score under Al-
phabetical and Frequency strategy in NYT dataset.
This observation verifies that the triplets extracting
order of a sentence is important in multiple rela-
tion extraction task. (d) Another interesting obser-
vation is that the NLL trained model can achieve
the best F1 score in NYT dataset if we sort the
triplets in alphabetical order, while in WebNLG
dataset, we need to sort the triplets in relation
frequency order. This observation demonstrates
that a global sorting rule may not fit for every
dataset. The Alphabetical strategy is better for
NYT dataset while the Frequency strategy is bet-
ter for WebNLG dataset. (e) We can also observe
that the model trained with RL can achieve better
result than any NLL sorting strategy. For example,
in WebNLG dataset, MultiDecoder model only
achieves 0.481 and 0.518 F1 score with Alpha-
betical and Frequency strategy, it achieves 0.564
F1 score with RL. Our model trained with RL
achieves the best performance on both NYT and
WebNLG dataset, which is 0.721 and 0.616. It’s
3.4% and 5.5% improvements compared with the
best global sorting rule on these two datasets.

4.5 Extraction Order Comparison

This experiment compares the extraction order of
our model trained with RL. We test our model

Comparison LSTM GRU
FUNLL-FURL 0.326 0.390
FreqRL-FURL 0.446 0.435

Table 2: The extraction order comparison.

on WebNLG dataset. The generated triplets (ex-
cluding NA-triplets) sequence of our model which
is pretrained with FixUnsort strategy then trained
with RL, is denoted as FURL. Similarly, the
triplets sequence of our model which is pretrained
with Frequency strategy then trained with RL is
denoted as FreqRL. And the triplets sequence of
our model which is trained with FixUnsort strat-
egy is denoted as FUNLL.

Suppose A = [Fa, Fb, Fc] is the generated
triplets sequence of FURL for sentence x, B =
[Fa, Fc] is the generated triplets sequence of
FUNLL for the same sentence x. Fa, as well as Fb
and Fc, is a triplet. The first triplet of the sequence
A is Fa, which is the same with the first triplet of
the sequence B. But the second triplet of A is dif-
ferent from B. Therefore, there are only 1 triplet
is in the same position for A and B. The triplets
number is the maximum triplets number of A and
B, which is 3 in this example. We calculate the
order comparison of sentence x as 1/3 = 0.333.
The order comparison of FUNLL and FURL (de-
note as FUNLL-FURL) is the mean value of all



374

Precision Recall F1
0

1

0.732
0.765 0.748

0.672

0.730
0.700

0.657

0.737
0.695

0.674

0.748
0.7090.713

0.735 0.724

Sentences with One Triplet

Precision Recall F1
0

1

0.605

0.398

0.480

0.620

0.499

0.553

0.749

0.648
0.6950.700

0.588

0.639

0.839

0.629

0.719

Sentences with Multiple Triplets
NLL (Shuffle)
NLL (FixUnsort)
NLL (Alphabetical)
NLL (Frequency)
RL

Figure 3: Results of our model with LSTM cell under different training strategies on NYT dataset.

sentences.
We show the order comparison results of our

model with LSTM and GRU cell in Table 2. As
we can see, although FURL model is pretrained
by FUNLL, FURL is more alike FreqRL (0.446
and 0.435), rather then FUNLL (0.326 and 0.390).

This experiment verified that after RL training,
the model trend to generate triplets in the same or-
der.

4.6 Multiple Relation Extraction

To verify the ability of extracting multiple rela-
tional facts, we conduct the experiment on NYT
dataset of our model with LSTM cell. We show
the results in Figure 3. The left part of this fig-
ure shows the performance of sentences with one
triplet. The right part shows the performance of
sentences with multiple triplets.

As we can see, when the sentence only con-
tains one triplet, our model trained with RL can
achieve comparative performance with the strong
baselines. When there are multiple triplets for a
sentence, our model trained with RL outperform
all baselines significantly. By training with RL,
our model could extract triplets more precisely.
Although the recall value is slightly lower then
NLL training with Frequency strategy, it exceeds
other baselines significantly. These observations
demonstrate that RL training is effective to handle
the multiple relation extraction task.

5 Weakness

Although we overcome all strong baselines by
training the model with RL, there are still some
weaknesses in our method.

The first weakness is the decrease in recall. Ta-
ble 1 shows that NLL training with Alphabeti-
cal or Frequency strategy achieves the highest re-
call in most cases. Training the model with RL
achieves the highest precision and relatively low
recall. This phenomenon demonstrates that the
model trained with RL generates relatively fewer
triplets. Although we can extract triplets more ac-
curate, it is still a weakness of our method since
we try to extract all triplets from a sentence.

The second weakness is our model can only
copy one word for each entity. Following Zeng
et al. (2018b), we only copy the last word of an
entity. But in reality, most entities contains more
than one word. In the future, we will consider how
to extract the complete entity. For example, we
could add the BIO tag prediction in the encoder
and train the BIO loss together with current loss.
Therefore, we can recognize the complete entity
with the help of BIO tags. Or, we can take two
steps to generate one entity, one step for the head
word and the other for the tail word.

6 Conclusions

In this paper, we discuss the multiple triplets ex-
traction order problem in the multiple relation ex-
traction task. We propose a sequence-to-sequence
model with reinforcement learning to take the ex-
traction order into consideration. Widely experi-
ments on NYT dataset and WebNLG dataset are
conducted and verified that the proposed method
is effective in handling this problem.



375

7 Acknowledgments

This work is supported by the National Natu-
ral Science Foundation of China (No.61533018,
No.61702512) and the independent research
project of National Laboratory of Pattern Recog-
nition.

References
Giannis Bekoulis, Johannes Deleu, Thomas Demeester,

and Chris Develder. 2018a. Adversarial training for
multi-context joint entity and relation extraction.

Giannis Bekoulis, Johannes Deleu, Thomas Demeester,
and Chris Develder. 2018b. Joint entity recogni-
tion and relation extraction as a multi-head selection
problem.

Yee Seng Chan and Dan Roth. 2011. Exploiting
syntactico-semantic structures for relation extrac-
tion. In Proceedings of ACL, pages 551–560.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings
of the EMNLP, pages 1724–1734.

Zihang Dai, Lei Li, and Wei Xu. 2016. Cfo: Condi-
tional focused neural question answering with large-
scale knowledge bases. In Proceedings of ACL,
pages 800–810.

Jun Feng, Minlie Huang, Li Zhao, Yang Yang, and Xi-
aoyan Zhu. 2018. Reinforcement learning for rela-
tion classification from nosy data. In Proceedings of
AAAI.

Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini. 2017. Creating train-
ing corpora for nlg micro-planners. In Proceedings
of ACL, pages 179–188.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K.
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
ACL, pages 1631–1640.

Pankaj Gupta, Hinrich Schtze, and Bernt Andrassy.
2016. Table filling multi-task recurrent neural net-
work for joint entity and relation extraction. In Pro-
ceedings of COLING, pages 2537–2547.

Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Li-
hong Li, Li Deng, and Mari Ostendorf. 2016. Deep
reinforcement learning with a natural language ac-
tion space. In Proceedings of ACL, pages 1621–
1630.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Diederik P. Kingma and Jimmy Lei Ba. 2015. Adam:
a Method for Stochastic Optimization. In Proceed-
ings of ICLR, pages 1–15.

Günter Klambauer, Thomas Unterthiner, Andreas
Mayr, and Sepp Hochreiter. 2017. Self-normalizing
neural networks. In Advances in NIPS, pages 971–
980.

Jiwei Li, Will Monroe, Alan Ritter, Michel Galley,
Jianfeng Gao, and Dan Jurafsky. 2016. Deep Re-
inforcement Learning for Dialogue Generation. In
Proceedings of EMNLP, pages 1192–1202.

Qi Li and Heng Ji. 2014. Incremental joint extraction
of entity mentions and relations. In Proceedings of
ACL, pages 402–412.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Proceedings of ACL, pages 1105–
1116.

Makoto Miwa and Yutaka Sasaki. 2014. Modeling
joint entity and relation extraction with table repre-
sentation. In Proceedings of EMNLP, pages 1858–
1869.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas Fidjeland,
Georg Ostrovski, et al. 2015. Human-level con-
trol through deep reinforcement learning. Nature,
518(7540):529–533.

Karthik Narasimhan, Tejas Kulkarni, and Regina
Barzilay. 2015. Language understanding for text-
based games using deep reinforcement learning. In
Proceedings of EMNLP, pages 1–11.

Karthik Narasimhan, Adam Yala, and Regina Barzilay.
2016. Improving information extraction by acquir-
ing external evidence with reinforcement learning.
In Proceedings of EMNLP, pages 2355–2365.

Pengda Qin, Weiran Xu, and William Yang Wang.
2018. Robust distant supervision relation extraction
via deep reinforcement learning. In Proceedings of
ACL.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML PKDD,
pages 148–163.

Cicero dos Santos, Bing Xiang, and Bowen Zhou.
2015. Classifying relations by ranking with convo-
lutional neural networks. In Proceedings of ACL,
pages 626–634.

David Silver, Aja Huang, Chris J Maddison, Arthur
Guez, Laurent Sifre, George Van Den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, et al. 2016. Mastering
the game of go with deep neural networks and tree
search. Nature, 529(7587):484–489.



376

Pei-Hao Su, Milica Gasic, Nikola Mrki, Lina M. Ro-
jas Barahona, Stefan Ultes, David Vandyke, Tsung-
Hsien Wen, and Steve Young. 2016. On-line ac-
tive reward learning for policy optimisation in spo-
ken dialogue systems. In Proceedings of ACL, pages
2431–2441.

Changzhi Sun, Yuanbin Wu, Man Lan, Shiliang Sun,
Wenting Wang, Kuang-Chih Lee, and Kewen Wu.
2018. Extracting entities and relations with joint
minimum risk training. In Proceedings of EMNLP,
pages 2256–2265.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In C. Cortes, N. D.
Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in NIPS, pages 2692–2700.

Ronald J Williams. 1992. Simple Statistical Gradient-
Following Algorithms for Connectionist Reinforce-
ment Learning. Machine Learning, 8:229–256.

Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In Proceedings of ACL,
pages 118–127.

Kun Xu, Yansong Feng, Songfang Huang, and
Dongyan Zhao. 2015a. Semantic relation classifi-
cation via convolutional neural networks with sim-
ple negative sampling. In Proceedings of EMNLP,
pages 536–540.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015b. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In Proceedings of EMNLP, pages
1785–1794.

Wentau Yih, Mingwei Chang, Xiaodong He, and Jian-
feng Gao. 2015. Semantic parsing via staged query
graph generation: Question answering with knowl-
edge base. In Proceedings of ACL, pages 1321–
1331.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. Seqgan: Sequence generative adversarial nets
with policy gradient. In Proceedings of AAAI, pages
2852–2858.

Xiaofeng Yu and Wai Lam. 2010. Jointly identifying
entities and extracting relations in encyclopedia text
via a graphical model approach. In Proceedings of
COLING, pages 1399–1407.

Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. J. Mach. Learn. Res., 3:1083–1106.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
COLING, pages 2335–2344.

Xiangrong Zeng, Shizhu He, Kang Liu, and Jun Zhao.
2018a. Large scaled relation extraction with rein-
forcement learning. In Proceedings of AAAI.

Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu,
and Jun Zhao. 2018b. Extracting relational facts by
an end-to-end neural model with copy mechanism.
In Proceedings of the ACL.

Meishan Zhang, Yue Zhang, and Guohong Fu. 2017.
End-to-end neural relation extraction with global op-
timization. In Proceedings of EMNLP, pages 1730–
1740.

Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing
Hao, Peng Zhou, and Bo Xu. 2017. Joint extrac-
tion of entities and relations based on a novel tagging
scheme. In Proceedings of ACL, pages 1227–1236.

Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
based bidirectional long short-term memory net-
works for relation classification. In Proceedings of
ACL, pages 207–212.

A The Details of the CNN Baseline

In this section, we will describe the details of the
CNN baseline. This baseline is a pipeline method.
For a sentence, we use the NLTK toolkit to recog-
nize the entities first. Then, we combine each two
entities as an entity pair and use a CNN relation
classifier to predict their relation.

For example, suppose we recognize 3 entities in
sentence s, which denoted as e1, e2, e3. There are
6 different entity pairs (remind that< e1, e2 > and
< e2, e1 > are different).

The CNN classifier is basically the same with
Zeng et al. (2014). Each word is turned into a em-
bedding which including it’s word embedding and
position embedding. After the convolution layer,
we apply a maxpooling layer on it. Then we ap-
ply a two layer softmax classify layer to obtain the
final results. We train the model with NLL loss.

Specifically, the word embedding dimension is
100, the position embedding dimension is 5, we
use 128 filters and the filter size is 3. The hid-
den layer size of softmax classifier is 100 and we
use tanh as the activation function. We optimize
the model with Adam optimizer (Kingma and Ba,
2015).

During evaluation, if the entity pair is classi-
fied into NA relation, we will exclude this triplet.
Otherwise, the triplet will be regarded as a predict
triplet. If the predicted triplet is the same as one
of the gold triples, it will be regarded as correct
triplet. To be fair, when comparing the entities in
the triplets, we only compare the last word of each
entity. As long as the last word of the extract entity
is the same as the gold one, we regard it as correct.



377

Model
NYT WebNLG

Precision Recall F1 Precision Recall F1
ONE+NLL (Shuffle) 0.617 0.471 0.534 0.360 0.214 0.268
ONE+NLL (FixUnsort) 0.597 0.511 0.551 0.311 0.294 0.302
ONE+NLL (Alphabetical) 0.570 0.526 0.547 0.310 0.292 0.300
ONE+NLL (Frequency) 0.609 0.531 0.567 0.311 0.302 0.306
ONE+RL 0.723 0.540 0.618 0.613 0.300 0.403
MULTI+NLL (Shuffle) 0.617 0.517 0.562 0.377 0.275 0.318
MULTI+NLL (FixUnsort) 0.586 0.544 0.564 0.404 0.403 0.403
MULTI+NLL (Alphabetical) 0.652 0.635 0.643 0.470 0.471 0.471
MULTI+NLL (Frequency) 0.636 0.593 0.614 0.500 0.499 0.500
MULTI+RL 0.738 0.636 0.683 0.615 0.478 0.538
OUR+NLL (Shuffle) 0.644 0.545 0.591 0.419 0.312 0.358
OUR+NLL (FixUnsort) 0.657 0.577 0.615 0.488 0.485 0.486
OUR+NLL (Alphabetical) 0.701 0.683 0.692 0.521 0.516 0.518
OUR+NLL (Frequency) 0.680 0.642 0.660 0.569 0.576 0.572
OUR+RL 0.770 0.671 0.717 0.628 0.555 0.589

Table 3: Results of different extraction order of models with GRU cell.

B Results of GRU Cell

We show the results of different extraction order
of models with GRU cell in Table 3.


