



















































A General-Purpose Algorithm for Constrained Sequential Inference


Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 482–492
Hong Kong, China, November 3-4, 2019. c©2019 Association for Computational Linguistics

482

A General-Purpose Algorithm for Constrained Sequential Inference

Daniel Deutsch,∗† Shyam Upadhyay,∗‡� and Dan Roth†
†Department of Computer and Information Science, University of Pennsylvania

‡Google, New York
{ddeutsch,danroth}@seas.upenn.edu

shyamupa@google.com

Abstract
Inference in structured prediction involves
finding the best output structure for an input,
subject to certain constraints. Many current
approaches use sequential inference, which
constructs the output in a left-to-right man-
ner. However, there is no general framework
to specify constraints in these approaches. We
present a principled approach for incorporat-
ing constraints into sequential inference algo-
rithms. Our approach expresses constraints us-
ing an automaton, which is traversed in lock-
step during inference, guiding the search to
valid outputs. We show that automata can
express commonly used constraints and are
easily incorporated into sequential inference.
When it is more natural to represent con-
straints as a set of automata, our algorithm
uses an active set method for demonstrably
fast and efficient inference. We experimentally
show the benefits of our algorithm on con-
stituency parsing and semantic role labeling.
For parsing, unlike unconstrained approaches,
our algorithm always generates valid output,
incurring only a small drop in performance.
For semantic role labeling, imposing con-
straints using our algorithm corrects common
errors, improving F1 by 1.5 points. These ben-
efits increase in low-resource settings. Our ac-
tive set method achieves a 5.2x relative speed-
up over a naive approach.1

1 Introduction

The key challenge in structured prediction prob-
lems (like sequence tagging and parsing) is infer-
ence (also known as decoding), which involves
identifying the best output structure y for an in-
put instance x from an exponentially large search

1All code available at https://cogcomp.seas.
upenn.edu/page/publication_view/884

∗Equal contribution, authors listed alphabetically
�Work done while at University of Pennsylvania.

input: Alice Smith gave a flower to Bob
gold tags: B-A0 I-A0 O O B-A1 O B-A2
predicate: gave
legal args: A0, A1, A2 (from PropBank)
spans: [Alice Smith] gave a [flower] to [Bob]
invalid tags: B-A0 I-A0 O O B-A0 O B-A2
reason: duplicate A0
invalid tags: B-A5 I-A5 O O B-A1 O B-A2
reason: illegal arg A5 for predicate “gave”
invalid tags: B-A0 O O O B-A1 O B-A2
reason: span [Alice Smith] should have single label

Figure 1: An example SRL instance (input, predicate,
gold tags) with different invalid tag sequences and the
constraints they violate (details in §4.2).

space Y (Taskar, 2004; Tsochantaridis et al.,
2005). The search is restricted to set of valid
y ∈ Yx ⊆ Y for x by imposing constraints during
inference. Figure 1 shows examples of constraints
used in Semantic Role Labeling (SRL) (Gildea
and Jurafsky, 2002).

Currently, inference for most structured pre-
diction problems is solved sequentially, predict-
ing the output in a left-to-right manner (Sutskever
et al., 2014; Luong et al., 2016). Such sequential
inference approaches enforce constraints in differ-
ent ways. For example, a shift-reduce parser con-
sults a stack to determine which action sequences
produce valid trees (Nivre et al., 2014), while an
SRL model penalizes tag sequences which violate
constraints during inference (Punyakanok et al.,
2008; Täckström et al., 2015).

At present, inference algorithms are designed
to handle task-specific constraints, and there is no
general formulation for constrained sequential in-
ference. This contrasts with the state of affairs in
NLP before deep learning, when constrained in-
ference approaches used general formulations like
Integer Linear Programming (ILP) (Roth and Yih,
2004; Clarke and Lapata, 2008, inter alia).

We present a simple, general-purpose sequen-
tial inference algorithm that takes a model and

https://cogcomp.seas.upenn.edu/page/publication_view/884
https://cogcomp.seas.upenn.edu/page/publication_view/884


483

O

B
-
O
R
G

I-ORG

O

B-PER

B-LOC

B-
LO
C

I-LOC

O

B
-
P
E
R

B-ORG

B-PER
I-PER

O

B
-
L
O
C

B-ORG

(a) A FSA for BIO tag sequences for NER.

(S/(NP/(VP, push $

X
X

,d
o

no
th

in
g

(S/(NP/(VP, push (

XX, do nothing

), if ) then pop

(S/(NP/(VP, push (

), if $ then pop

(b) A PDA that accepts valid parse trees like (S (NP
XX)(VP XX XX)) and (S (VP XX)). The edge la-
bel “P/Q/R, A” denotes consuming either P, Q or R as
input and changing the stack per A. “If T” indicates that
edge is only valid if T is on top of the stack.

Figure 2: An example FSA and PDA. Red arrows mark start states, and double circles mark accepting states.

an automaton expressing the constraints as input,
and outputs a structure that satisfies the constraints
(§2.2). The automaton guides the inference to
always produce a valid output by reshaping the
model’s probability distribution such that actions
deemed invalid by the automaton are not taken.

In some situations, it is more natural to express
the constraints as a set of automata. However,
naively enforcing multiple automata by fully in-
tersecting them is potentially expensive. Instead,
our algorithm lazily intersects the automata us-
ing an efficient active set method, reminiscent of
the cutting-plane algorithm (Tsochantaridis et al.,
2005) (§2.4).

The choice of using automata to express con-
straints has several benefits. First, automata
are capable of expressing constraints used in a
wide variety of NLP tasks. Indeed, in §3, we
show that task-specific constrained inference ap-
proaches implicitly use an automaton. Second, au-
tomata can be naturally incorporated into any se-
quential inference algorithm such as beam search.
Finally, automata make enforcing multiple con-
straints straightforward — only the automata for
individual constraints need to be specified, which
are then intersected at inference time.

Our algorithm is a principled approach for en-
forcing constraints and has many desirable prop-
erties. It decouples the constraints from the in-
ference algorithm, making it generally applicable
to many problems. Further, it guarantees valid
output and allows for the seamless addition of
constraints at inference time without modifying
the inference code. We experimentally demon-
strate the benefits of our algorithm on two struc-

tured prediction tasks, constituency parsing (§5.1)
and semantic role labeling (§5.2). Our results in
constituency parsing show that our algorithm al-
ways outputs valid parses, incurring only a small
drop in F1. In SRL, constrained inference using
our algorithm corrects common errors produced
by unconstrained inference, resulting in a 1.5 F1
improvement. This increase in performance is
more prominent in low-resource settings. Finally,
the active set method for enforcing multiple con-
straints achieves a 5.2x speed-up over the naive ap-
proach of fully intersecting the relevant automata.

2 Constrained Inference with Automata

We briefly review automata that we use for repre-
senting constraints in our algorithm.

2.1 Brief Review of Automata Theory

For the purposes of this work, an automaton is a
(possibly weighted) directed graph that compactly
encodes a set of strings, known as its language.
The two types of automata used in this work are
finite-state automata (FSA) and push-down au-
tomata (PDA).2

In an FSA, each edge is labeled with a sym-
bol y from an alphabet Σ, and any traversal from
the starting state to the final state(s) represents a
unique string y in the language. A PDA is an
extension of an FSA in which the traversal can
maintain a stack that is used for computation, and
the edges may manipulate or examine the state of
the stack. Any language that can be expressed us-
ing regular expressions or context-free grammars

2We only consider deterministic automata.



484

has an equivalent accepting FSA or PDA, respec-
tively (Sipser, 1997). An example of each type of
automata is depicted in Figure 2.

Our inference algorithm views an automaton as
an abstract stateful function, denoted as A, which
accepts strings from its languageL(A). After con-
suming the prefix y1:i of a string y, A provides a
score A(yi+1 | y1:i) for every symbol y ∈ Σ. In-
voking A.accepts(s) tests if a string s is in L(A).

2.2 Sequential Inference
The traditional inference problem for structured
prediction can be formalized as solving

ŷ = argmax
y∈Yx

log pθ(y | x) (1)

where x and y are the input and output structures,
Yx is the set of valid output structures for x, and
θ are the parameters of the model pθ(y | x). A
common way to solve this problem is to decom-
pose the objective as follows:

ŷ = argmax
y∈Yx

∑
i

log pθ(yi | x,y1:i−1) (2)

This decomposition is adopted by popu-
lar approaches, such as seq2seq models,
SEARN (Daumé et al., 2009), etc. Under
this decomposition, the inference problem is
often solved by an inexact sequential inference
algorithm, such as beam search.

2.3 Imposing Constraints with Automata
We are interested in versions of Equation 2 in
which the output space is described by the lan-
guage of an automaton. Formally,

ŷ = argmax
y

∑
i

log pθ(yi | x,y1:i−1)

such that y ∈ L(Ax)
(3)

where L(Ax) is the language of an automaton Ax
describing the valid output space for instance x.
This framework is capable of expressing many
constraints which are common in NLP applica-
tions, described in detail in §3.

Equation 3 can be rewritten as:

ŷ = argmax
y

∑
i

log p̃θ(yi | x,y1:i−1) (4)

log p̃θ(yi | x,y1:i−1) , log pθ(yi | x,y1:i−1)
+Ax(yi | y1:i−1)

Algorithm 1 Constrained Sequential Greedy Inference

Input: Model pθ , automaton Ax, sequence x
Output: Prediction ŷ ∈ L(Ax)

1: procedure CONSTRAINED-SEARCH(pθ , Ax, x)
2: ŷ← [BOS] . Beginning of sequence token
3: while ŷ is not finished do
4: log p̃θ(y′ |x, ŷ)=log pθ(y′ |x, ŷ)+Ax(y′ | ŷ)
5: y ← arg maxy′ log p̃θ(y′ | x, ŷ)
6: ŷ← ŷ + y . Extend current sequence
7: return ŷ

Inference

yi+1
<latexit sha1_base64="ue18fuwzYNKaZZT5v9WT2QLrWMI=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBZBEEqigh6LXjxWsLbQhrLZTtqlm03Y3Qgh9Ed48aCIV3+PN/+N2zYHbX0w8Hhvhpl5QSK4Nq777ZRWVtfWN8qbla3tnd296v7Bo45TxbDFYhGrTkA1Ci6xZbgR2EkU0igQ2A7Gt1O//YRK81g+mCxBP6JDyUPOqLFSO+vn/Myb9Ks1t+7OQJaJV5AaFGj2q1+9QczSCKVhgmrd9dzE+DlVhjOBk0ov1ZhQNqZD7FoqaYTaz2fnTsiJVQYkjJUtachM/T2R00jrLApsZ0TNSC96U/E/r5ua8NrPuUxSg5LNF4WpICYm09/JgCtkRmSWUKa4vZWwEVWUGZtQxYbgLb68TB7P695F3b2/rDVuijjKcATHcAoeXEED7qAJLWAwhmd4hTcncV6cd+dj3lpyiplD+APn8wcA949X</latexit>

yi�1
<latexit sha1_base64="6yO3IUclX4/tm74nTaF9Ym+08fg=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBiyVRQY9FLx4rWFtoQ9lsJ+3SzSbsboQQ+iO8eFDEq7/Hm//GbZuDtj4YeLw3w8y8IBFcG9f9dkorq2vrG+XNytb2zu5edf/gUcepYthisYhVJ6AaBZfYMtwI7CQKaRQIbAfj26nffkKleSwfTJagH9Gh5CFn1FipnfVzfuZN+tWaW3dnIMvEK0gNCjT71a/eIGZphNIwQbXuem5i/Jwqw5nASaWXakwoG9Mhdi2VNELt57NzJ+TEKgMSxsqWNGSm/p7IaaR1FgW2M6JmpBe9qfif101NeO3nXCapQcnmi8JUEBOT6e9kwBUyIzJLKFPc3krYiCrKjE2oYkPwFl9eJo/nde+i7t5f1ho3RRxlOIJjOAUPrqABd9CEFjAYwzO8wpuTOC/Ou/Mxby05xcwh/IHz+QMEA49Z</latexit>

yi
<latexit sha1_base64="fA+ZJK1EyQbvufKNnkf6r0xzIAs=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lU0GPRi8cKpi20oWy2m3bpZhN2J0IJ/Q1ePCji1R/kzX/jts1BWx8MPN6bYWZemEph0HW/ndLa+sbmVnm7srO7t39QPTxqmSTTjPsskYnuhNRwKRT3UaDknVRzGoeSt8Px3cxvP3FtRKIecZLyIKZDJSLBKFrJn/RzMe1Xa27dnYOsEq8gNSjQ7Fe/eoOEZTFXyCQ1puu5KQY51SiY5NNKLzM8pWxMh7xrqaIxN0E+P3ZKzqwyIFGibSkkc/X3RE5jYyZxaDtjiiOz7M3E/7xuhtFNkAuVZsgVWyyKMkkwIbPPyUBozlBOLKFMC3srYSOqKUObT8WG4C2/vEpaF3Xvsu4+XNUat0UcZTiBUzgHD66hAffQBB8YCHiGV3hzlPPivDsfi9aSU8wcwx84nz8nn47n</latexit>

ŷ
<latexit sha1_base64="cKmPPdsLA9KyE329ijh5al0YHK8=">AAAB+XicbVDLSsNAFL2pr1pfUZduBovgqiQq6LLoxmUF+4AmlMl00g6dTMLMpBBC/sSNC0Xc+ifu/BsnbRZaPTBwOOde7pkTJJwp7ThfVm1tfWNzq77d2Nnd2z+wD496Kk4loV0S81gOAqwoZ4J2NdOcDhJJcRRw2g9md6Xfn1OpWCwedZZQP8ITwUJGsDbSyLa9Kda5F2E9DcI8K4qR3XRazgLoL3Er0oQKnZH96Y1jkkZUaMKxUkPXSbSfY6kZ4bRoeKmiCSYzPKFDQwWOqPLzRfICnRlljMJYmic0Wqg/N3IcKZVFgZksI6pVrxT/84apDm/8nIkk1VSQ5aEw5UjHqKwBjZmkRPPMEEwkM1kRmWKJiTZlNUwJ7uqX/5LeRcu9bDkPV832bVVHHU7gFM7BhWtowz10oAsE5vAEL/Bq5daz9Wa9L0drVrVzDL9gfXwDWDWUHQ==</latexit>

Model … …

p✓(yi+1 | x, ŷ1:i)
<latexit sha1_base64="ZaKfDt4t2ZNOVcZXF2PYOFfdmdk=">AAACInicbVDJSgNBEO2JW4xb1KOXxiBElDCjgssp6MVjBLNAJgw9nZ6kSc9Cd404DPMtXvwVLx4U9ST4MXYWRBMfdPN4r4qqem4kuALT/DRyc/MLi0v55cLK6tr6RnFzq6HCWFJWp6EIZcsligkesDpwEKwVSUZ8V7CmO7ga+s07JhUPg1tIItbxSS/gHqcEtOQUzyPHhj4DUk6clB9YGbZ93tUfgb7rpffZ4Q+3+wTSJMuc1Lrg2b5TLJkVcwQ8S6wJKaEJak7x3e6GNPZZAFQQpdqWGUEnJRI4FSwr2LFiEaED0mNtTQPiM9VJRydmeE8rXeyFUr8A8Ej93ZESX6nEd3XlcF017Q3F/7x2DN5ZJ+VBFAML6HiQFwsMIR7mhbtcMgoi0YRQyfWumPaJJBR0qgUdgjV98ixpHFWs44p5c1KqXk7iyKMdtIvKyEKnqIquUQ3VEUUP6Am9oFfj0Xg23oyPcWnOmPRsoz8wvr4Bu+ikaQ==</latexit>

p̃✓(yi+1 | x, ŷ1:i)
<latexit sha1_base64="bAs4JQ0t0NT2RFEW9FikJClOLb8=">AAACKnicbVDJSgQxEE27O26jHr0EB0FRhm4VFE8uF48KjgrTQ5NOVzth0gtJtdiE/h4v/ooXD4p49UPMjIO4PUh4vFdFVb0wl0Kj6746I6Nj4xOTU9O1mdm5+YX64tKlzgrFocUzmanrkGmQIoUWCpRwnStgSSjhKuyd9P2rW1BaZOkFljl0EnaTilhwhlYK6kc+ChmByavAxy4gWy8DIza9ivqJiOzHsBvG5q7a+uJ+l6Epqyow3oGoNoJ6w226A9C/xBuSBhniLKg/+VHGiwRS5JJp3fbcHDuGKRRcQlXzCw054z12A21LU5aA7pjBqRVds0pE40zZlyIdqN87DEu0LpPQVvbX1b+9vvif1y4w3u8YkeYFQso/B8WFpJjRfm40Ego4ytISxpWwu1LeZYpxtOnWbAje75P/ksvtprfTdM93G4fHwzimyApZJevEI3vkkJySM9IinNyTR/JMXpwH58l5dd4+S0ecYc8y+QHn/QPbzKgf</latexit>

Constraints
Ax(yi+1 | ŷ1:i)

<latexit sha1_base64="zELc0MiJaBdSSkCg4kh4vLzZCEM=">AAACJXicbVDLSgMxFM34rPVVdekmWISKUGZEUMRF1Y3LCvYBnWHIpJk2NPMgyYhDmJ9x46+4cWERwZW/YqYdQVsPBE7OuZd77/FiRoU0zU9jYXFpeWW1tFZe39jc2q7s7LZFlHBMWjhiEe96SBBGQ9KSVDLSjTlBgcdIxxvd5H7ngXBBo/BepjFxAjQIqU8xklpyK5d2gOQQI6auMldNPp6vHrOslrqKHluZHdA+/NHtIZIqzXSldUGzI7dSNevmBHCeWAWpggJNtzK2+xFOAhJKzJAQPcuMpaMQlxQzkpXtRJAY4REakJ6mIQqIcNTkygweaqUP/YjrF0o4UX93KBQIkQaerszXFbNeLv7n9RLpnzuKhnEiSYing/yEQRnBPDLYp5xgyVJNEOZU7wrxEHGEpQ62rEOwZk+eJ+2TumXWrbvTauO6iKME9sEBqAELnIEGuAVN0AIYPIEX8AbGxrPxarwbH9PSBaPo2QN/YHx9A8bupp4=</latexit><latexit sha1_base64="zELc0MiJaBdSSkCg4kh4vLzZCEM=">AAACJXicbVDLSgMxFM34rPVVdekmWISKUGZEUMRF1Y3LCvYBnWHIpJk2NPMgyYhDmJ9x46+4cWERwZW/YqYdQVsPBE7OuZd77/FiRoU0zU9jYXFpeWW1tFZe39jc2q7s7LZFlHBMWjhiEe96SBBGQ9KSVDLSjTlBgcdIxxvd5H7ngXBBo/BepjFxAjQIqU8xklpyK5d2gOQQI6auMldNPp6vHrOslrqKHluZHdA+/NHtIZIqzXSldUGzI7dSNevmBHCeWAWpggJNtzK2+xFOAhJKzJAQPcuMpaMQlxQzkpXtRJAY4REakJ6mIQqIcNTkygweaqUP/YjrF0o4UX93KBQIkQaerszXFbNeLv7n9RLpnzuKhnEiSYing/yEQRnBPDLYp5xgyVJNEOZU7wrxEHGEpQ62rEOwZk+eJ+2TumXWrbvTauO6iKME9sEBqAELnIEGuAVN0AIYPIEX8AbGxrPxarwbH9PSBaPo2QN/YHx9A8bupp4=</latexit><latexit sha1_base64="zELc0MiJaBdSSkCg4kh4vLzZCEM=">AAACJXicbVDLSgMxFM34rPVVdekmWISKUGZEUMRF1Y3LCvYBnWHIpJk2NPMgyYhDmJ9x46+4cWERwZW/YqYdQVsPBE7OuZd77/FiRoU0zU9jYXFpeWW1tFZe39jc2q7s7LZFlHBMWjhiEe96SBBGQ9KSVDLSjTlBgcdIxxvd5H7ngXBBo/BepjFxAjQIqU8xklpyK5d2gOQQI6auMldNPp6vHrOslrqKHluZHdA+/NHtIZIqzXSldUGzI7dSNevmBHCeWAWpggJNtzK2+xFOAhJKzJAQPcuMpaMQlxQzkpXtRJAY4REakJ6mIQqIcNTkygweaqUP/YjrF0o4UX93KBQIkQaerszXFbNeLv7n9RLpnzuKhnEiSYing/yEQRnBPDLYp5xgyVJNEOZU7wrxEHGEpQ62rEOwZk+eJ+2TumXWrbvTauO6iKME9sEBqAELnIEGuAVN0AIYPIEX8AbGxrPxarwbH9PSBaPo2QN/YHx9A8bupp4=</latexit><latexit sha1_base64="zELc0MiJaBdSSkCg4kh4vLzZCEM=">AAACJXicbVDLSgMxFM34rPVVdekmWISKUGZEUMRF1Y3LCvYBnWHIpJk2NPMgyYhDmJ9x46+4cWERwZW/YqYdQVsPBE7OuZd77/FiRoU0zU9jYXFpeWW1tFZe39jc2q7s7LZFlHBMWjhiEe96SBBGQ9KSVDLSjTlBgcdIxxvd5H7ngXBBo/BepjFxAjQIqU8xklpyK5d2gOQQI6auMldNPp6vHrOslrqKHluZHdA+/NHtIZIqzXSldUGzI7dSNevmBHCeWAWpggJNtzK2+xFOAhJKzJAQPcuMpaMQlxQzkpXtRJAY4REakJ6mIQqIcNTkygweaqUP/YjrF0o4UX93KBQIkQaerszXFbNeLv7n9RLpnzuKhnEiSYing/yEQRnBPDLYp5xgyVJNEOZU7wrxEHGEpQ62rEOwZk+eJ+2TumXWrbvTauO6iKME9sEBqAELnIEGuAVN0AIYPIEX8AbGxrPxarwbH9PSBaPo2QN/YHx9A8bupp4=</latexit>

Figure 3: At each time step, the automata Ax re-
shapes the model’s probability distribution pθ to p̃θ,
from which the next output label yi+1 is determined.

where Ax reshapes pθ to p̃θ at each time step.
To impose hard constraints, we set this score to
−∞ for invalid yi and constant for all valid yi.3
Equation 4 allows natural extensions of sequential
inference algorithms, where an automaton is tra-
versed in lock-step to guide the search to always
output a valid structure. The necessary extension
of greedy search is presented in Algorithm 1. Con-
cretely, when predicting yi, the inference selects
the most probable action under the reshaped prob-
ability distribution p̃θ (line 5). It is straightforward
to similarly extend other sequential inference al-
gorithms.

2.4 Imposing Multiple Constraints

The formulation above assumes that the con-
straints are described using a single automaton.
However, in some scenarios, it is more natural to
impose multiple constraints by representing them
as a set of automata.

A Motivating Example. Consider Figure 1
which illustrates the SRL problem. The following
constraints should be obeyed by the tag sequence:
the predicate cannot have multiple arguments of
the same type (A0–A5), each predicate may only
accept a certain set of argument types, and certain
spans derived from a constituency parse should re-

3Soft constraints can also be imposed, but are not ex-
plored in this work.



485

ceive the same label.4 Each of these constraints
can be easily expressed using a separate automa-
ton. In contrast, directly writing a single automa-
ton to enforce these constraints simultaneously
could be impractical and difficult.

Issues with Naive Approaches. Naively ex-
tending sequential inference algorithms to impose
a set of constraints by traversing multiple au-
tomata in parallel fails. There is no guarantee that
a valid structure will be found, even if the intersec-
tion of the automata’s languages is non-empty (a
proof by counter-example is provided in Appendix
A). The alternative solution of intersecting the au-
tomata into a single automaton may be intractable,
as the size of the intersected automaton grows ex-
ponentially in the number of constraints (Hopcroft
and Ullman, 1979).

An Active Set Method. Intuitively, intersecting
all of the automata may not be necessary because
it is possible for a constraint to be satisfied with-
out it being enforced. This is the basis for ac-
tive set methods (such as the cutting-plane algo-
rithm (Kelley, 1960; Tsochantaridis et al., 2005)),
which maintain a set during inference that contains
currently active (i.e., enforced) constraints. When
a constraint is violated by the current output, it en-
ters (i.e., is added to) the active set.

We present an active set method for imposing
multiple constraints represented by a set of au-
tomata Sx in Algorithm 2. Our algorithm is in-
spired by the active set algorithm of Tromble and
Eisner (2006) for finite-state transducers.

For an instance x, Algorithm 2 maintains a ac-
tive set5 W corresponding to all violated con-
straints so far.W is represented by the intersection
AW of the relevant automata, which is initialized
with an automata Σ∗ that accepts any sequence
(line 1). On each iteration, the algorithm runs
a constrained inference algorithm (such as Algo-
rithm 1) that uses AW (line 3) to find an output ŷ.
Then, FIND-VIOLATION checks if ŷ violates any
of the constraints that are not currently in the ac-
tive set, Sx \ W (line 4). If ŷ is accepted by all of
the automata (line 5), it is valid and subsequently
returned (line 6). Otherwise, the first violated con-
straint is added to W (line 8), its automaton A′
intersected with AW (line 9), and constrained in-
ference is re-run (line 3).

4These constraints are described in detail in §4.
5Also known as a working set.

Algorithm 2 An Active Set Method for Multiple Constraints

Input: Model pθ , set of automata Sx, sequence x
Output: Prediction ŷ ∈

⋂
A∈Sx L(A)

1: W ← ∅, AW ← Σ∗
2: while True do
3: ŷ← CONSTRAINED-SEARCH(pθ , AW , x)
4: A′ ← FIND-VIOLATION(Sx \W , ŷ)
5: if A′ is null then . No constraint is violated
6: return ŷ . return current output
7: else
8: W ←W ∪ {A′} . update working set
9: AW ← AW ∩ A′ . automata intersection

10: procedure FIND-VIOLATION(K, y)
11: for each A in K do
12: if not A.accepts(y) then
13: return A . the first violated constraint
14: return null

Algorithm 2 is guaranteed to terminate with a
valid output. In the worst case, all of the con-
straints will be eventually enter the active set and
inference will run with a fully intersected au-
tomata. Although the cost of this worst case is
exponential in the number of constraints, this oc-
curs infrequently in practice. Moreover, we found
that Algorithm 2 is faster than naively computing
the full intersection despite running inference mul-
tiple times.

3 Representing Constraints as Automata

We now illustrate the expressibility of automata by
showing how they can represent commonly used
constraints in various NLP applications.

Text Generation. Text generation tasks like im-
age captioning, machine translation, sentence sim-
plification, etc., often require that the output must
contain specific words or phrases (Anderson et al.,
2017; Hokamp and Liu, 2017; Post and Vilar,
2018; Zhang et al., 2017) in order to incorpo-
rate prior knowledge (e.g., the caption must have
the word “chair” as the object was detected in
the image). Similarly, constraints can disallow
invalid sequences, such as words which do not
rhyme (Ghazvininejad et al., 2016, 2018) or do
not appear in a dictionary (Deutsch et al., 2018).
These constraints can be represented as FSAs
where all paths to an accepting state contain the
required sequences and do not contain any disal-
lowed sequences. Note that the size of the au-
tomata is not a function of the output vocabulary
but the vocabulary that participates in the con-
straints.



486

Sequence Tagging. Many sequence tagging
problems (such as NER, shallow parsing, etc.) re-
quire that the output tags are valid under the spe-
cific tagging scheme, such as BIO, which marks
each token as beginning, inside, or outside of a
phrase. One can easily write an FSA that recog-
nizes the language of valid tag sequences for these
schemes, such as the automaton in Figure 2a.

Other constraints commonly applied to se-
quence tagging are specific to the particular task.
In SRL, each argument type can appear exactly
one time. For instance, for the label A0, an FSA
with 3 states (before-A0, emitting-A0 and after-
A0) can be written to enforce this constraint. See
Tromble and Eisner (2006) for examples.

Syntactic Parsing. Syntactic parsing (depen-
dency or constituency) tasks require that the out-
put forms a valid tree, a constraint commonly en-
forced using the shift-reduce algorithm (Zhu et al.,
2013; Nivre et al., 2014; Dyer et al., 2016). Shift-
reduce inference inspects the state of a stack to de-
cide which next actions are valid (e.g., if the stack
is empty, reduce is invalid). Shift-reduce inference
is implicitly using an automaton which is the inter-
section of a PDA (that counts how many shift and
reduce actions have occurred) and an FSA (that
restricts the maximum number of actions based on
the input sentence length).

Semantic Parsing and Code Generation. In
semantic parsing and code generation, constraints
ensure both the syntactic validity and the exe-
cutability of the output. For instance, for the predi-
cate ISADJACENT (which compares two countries
for adjacency) the syntactic constraint ensures the
predicate receives two arguments, whereas the ex-
ecutability ensures they are properly typed (e.g.,
that they are countries). Krishnamurthy et al.
(2017) use a type-constrained grammar to ensure
the output logical form is well-typed. Similarly,
Ling et al. (2016) use a stack while decoding to
ensure the validity of the generated code. Both
these constraints can be encoded in a PDA derived
from the grammar of the language (Sipser, 1997).

4 Experimental Setup

We elaborate on two constrained inference tasks
from the previous section, constituency parsing
and semantic role labeling, which serve as case
studies for showing the applicability and versa-
tility of our approach. Our goal is not to beat

the state-of-the-art, but to illustrate the practical-
ity and benefits of our approach.

4.1 Models, Datasets, and Evaluation

Constituency Parsing. We follow the experi-
mental setup of Vinyals et al. (2015) and train a
seq2seq model with attention using standard splits
on the Penn Treebank. This setup was chosen to
ensure that no constraints are built into the model
for a truly unconstrained baseline. The input is a
sequence of tokens, and the model outputs a lin-
earized parse tree (“gold parse” in Figure 4).

We compare our approach (CONSTRAINED)
to unconstrained inference (UNCONSTRAINED),
which runs beam search and selects the highest-
scoring output. We also compare to Vinyals
et al. (2015)’s inference approach (henceforth
POSTHOC). Vinyals et al. (2015) removes parses
without the correct number of preterminals at
the end of beam search and adds parentheses to
almost-valid parses.6 The algorithms are evalu-
ated using F1 as reported by EVALB, a standard
evaluation toolkit.7 However, because EVALB ig-
nores invalid trees during evaluation (which can
artificially improve the performance of models
which violate constraints), we also report cover-
age, the percentage of valid outputs. We use a
beam size of 10.

Semantic Role Labeling. For SRL, the uncon-
strained model is an off-the-shelf implementation
of He et al. (2017). The input to the model is
the sentence and the predicate for which the ar-
guments need to be identified, and the model out-
puts a tag sequence (such as Figure 1). To iso-
late the effect of constraints, we assume the gold
predicates are available, unlike He et al. (2017).
We use the standard train and development splits
from the CoNLL 2005 shared task and the same
model parameters as He et al. (2017). We report
the CoNLL F1 score8 computed for the core argu-
ments (namely A0–A5) and use greedy inference.

4.2 Constraints

Constituency Parsing. The constraints used in
parsing disallow invalid parses, such as the ex-
amples in Figure 4. The constraints ensure
that (i) the parentheses are balanced (BAL), (ii)
there is exactly one preterminal per input token

6personal communication with Vinyals et al. (2015)
7nlp.cs.nyu.edu/evalb/
8www.lsi.upc.es/˜srlconll/srl-eval.pl

nlp.cs.nyu.edu/evalb/
www.lsi.upc.es/~srlconll/srl-eval.pl


487

input: John kissed Mary
gold parse: (S (NP XX)(VP XX (NP XX)))
invalid parse: (S (NP )(VP XX XX (NP XX)))
reason: empty phrase
invalid parse: (S (VP XX (NP XX)))
reason: incorrect number of preterminals
invalid parse: (S (NP XX)(VP XX (NP XX))))
reason: unbalanced parentheses

Figure 4: An example parsing instance with different
invalid parses and the constraints they violate (details
in §4.2). XX denotes POS tags.

(#PRETERM), and (iii) every phrase contains at
least one preterminal (NONEMPTY).

Semantic Role Labeling. The constraints used
in SRL disallow invalid sequences, such as the
ones in Figure 1. The constraints ensure that (i)
a predicate cannot have multiple arguments of the
same type (NODUP). This constraint is expressed
using one FSA per argument label for a total of
5 automata; (ii) a predicate can only take a cer-
tain set of argument types (LEGALARGS), accord-
ing to PropBank v3.1 (Palmer et al., 2005). This
constraint is expressed using one FSA; (iii) certain
spans in the sentence should be assigned the same
argument label type (SPANLABEL). These spans
are identified from the predicate and the con-
stituency parse of the sentence using the heuris-
tic of Xue and Palmer (2004).9 This constraint is
expressed using one FSA.

Ease of Implementation. All of the constraints
were expressed using an FSA, with the exception
of BAL which requires a PDA. The automata were
implemented using Pynini (Gorman, 2016). We
use the same inference code for both tasks, except
for the automata generation.

5 Experimental Results

We show the practicality of our approach and the
benefits over unconstrained inference in the exper-
iments below. In order to illustrate the benefits of
constrained inference in low-resource settings, we
simulate different levels of supervision.

5.1 Constituency Parsing

We experimentally show the need for constraints
and compare inference strategies for parsing.

Necessity of Constraints. In order to demon-
strate that constraints are necessary to guarantee

9We use Kitaev and Klein (2018)’s parser.

0 20 40 60 80 100

40

60

80

100

Percentage of Training Data

Pe
rc

en
tS

at
is

fie
d

NONEMPTY
BAL

#PRETERM
All

Figure 5: Percentage of output test parses that satisfy
the constituency parsing constraints in §4.2 when us-
ing unconstrained inference as the amount of training
data is increased. “All” measures the percent of parses
which satisfy all of the constraints. Even with 100% of
the training data, the model fails to always output the
correct number of preterminals.

structurally valid output, we vary the amount of
training data for a seq2seq model and measure
how frequently the output is an invalid parse tree.

The results in Figure 5 show that while the
model learns some constraints with little data, oth-
ers are frequently violated even with full supervi-
sion. For instance, the model outputs parse trees
with balanced parentheses for over 94% of in-
stances, after training on as little as 5% of the
training data. However, the model frequently vi-
olates the #PRETERM constraint, outputting an in-
correct number of preterminals on about 7% of
parses even after training on 100% of the data.
Therefore, we cannot expect seq2seq models to
learn to produce valid outputs, even when it has
access to 40k labeled parse trees, let alone in low-
resource settings.

Comparing Inference Approaches. Figure 6
shows the performance of all the three inference
techniques discussed in §4.1, namely, UNCON-
STRAINED, POSTHOC, and CONSTRAINED.

At first glance, it appears that UNCON-
STRAINED is slightly better than both POSTHOC
and CONSTRAINED at 100% supervision. How-
ever, EVALB ignores any invalid parse trees when
computing F1, and therefore it is necessary to take
coverage (the percent of valid output parses) into
account when comparing inference approaches.
It is evident from Figure 6 (left) that CON-
STRAINED always ensures 100% coverage, and
POSTHOC only reaches near-100% coverage with
full supervision. In contrast, UNCONSTRAINED



488

0 20 40 60 80 100
0

20

40

60

80

100

Percentage of Training Data

F 1
/P

er
ce

nt

Coverage F1

UNCONSTRAINED

CONSTRAINED

POSTHOC

0 20 40 60 80 100
0

20

40

60

80

100

Percentage of Training Data

F 1
/P

er
ce

nt

Coverage F1

UNCONSTRAINED

CONSTRAINED

POSTHOC

Figure 6: F1 and coverage (# valid output parses / # test instances) for UNCONSTRAINED, POSTHOC, and CON-
STRAINED on the entire test data (left) and test sentences of length ≥30 (right). The UNCONSTRAINED and
POSTHOC inference algorithms have a harder time producing valid parse trees for the longer input sentences.

achieves consistently low coverage, with the best
model reaching 7% lower coverage than the CON-
STRAINED.

The increased coverage explains the apparent
drop in F1 for constrained approaches. Intuitively,
longer sentences are inherently harder to parse.
CONSTRAINED and POSTHOC produce a valid,
but potentially incorrect parse for these sentences
and get penalized. In contrast, UNCONSTRAINED
is more likely to produce invalid parses for these
sentences, and is thus effectively evaluated on a
test set containing short sentences. To verify this,
we re-evaluated the inference approaches on test
sentences which are ≥30 tokens (Figure 6, right).
Under this setting, every approach has worse F1
and coverage at all levels of supervision (Figure 6
left vs right).

5.2 Semantic Role Labeling

We show that we can improve the performance of
a trained model by incorporating constraints at in-
ference time which address common errors (Daza
and Frank, 2018) made by SRL models. This ex-
periment also shows the efficiency of the active set
method in enforcing multiple constraints.

Correcting Common Errors. We now show
how some common SRL errors like the ones
discussed in Figure 1, can be corrected using
constraints added at inference time. Start-
ing with an unconstrained baseline model
(UNCONSTRAINED), we successively add the
NODUP, LEGALARGS and SPANLABEL con-
straints, in that order.

10 20 30 40 50 60 70 80 90 100

65

70

75

80

Percentage of Training Data

C
oN

L
L

F 1

UNCONSTRAINED
+NODUP

+LEGALARGS
+SPANLABEL (Fully Constrained)

Figure 7: Learning curve for SRL. Starting with un-
constrained inference, we incrementally add NODUP,
LEGALARGS and SPANLABEL constraints. The con-
straints improve performance at all levels of supervi-
sion, with the largest improvements in low supervision
settings.

Figure 7 shows that using the constraints im-
proves performance over UNCONSTRAINED in all
settings. Constraints like NODUP and SPANLA-
BEL give significant gains, demonstrating their
value. At 100% supervision, fully constrained in-
ference improves by 1.5 F1 points over uncon-
strained inference (83.03 vs 81.53). In fact, con-
strained inference at only 50% data achieves sim-
ilar F1 score as UNCONSTRAINED at 100%, with
larger gains in low supervision regimes (≤40%).

Active Set Size and Efficiency. For SRL, the
maximum possible size of the active set W is 7
for any test instance. Intersecting all 7 automata
would lead to an automaton with 1043 states and



489

10 20 30 40 50 60 70 80 90 100

60

65

70

75

80

85

90

95

100

Percentage of Training Data

Si
ze

D
is

tr
ib

ut
io

n
of
W

0 violation 1 violation 2 violation
3 violation 4 violation 5 violation

10 20 30 40 50 60 70 80 90 100
3.5x

4x

5x

5.5x

5.2x

Percentage of Training Data

sp
ee

d-
up

Figure 8: Top: Size distribution of the final working set
for the active set method. With a model trained on 30%
of the data, 72% of the test instances have an empty fi-
nal working set (i.e., with 0 violations). Bottom: Rel-
ative speed-up in decoding time using the active set
method over naively computing the full intersection.

2022 arcs. We now measure the size of the active
set observed in practice.

Figure 8 shows the size distribution of the final
W under different amounts of supervision. With a
fully supervised model, 80% of the test instances
had an empty W when inference terminated (i.e.,
no constraints entered W). Under the same set-
ting,W contains 1 and 2 constraints for 10.6% and
8.6% of the instances, respectively. Under any set-
ting,W was empty for >60% of the instances. On
average, the active set automaton had 34 states and
66 arcs, a 30x reduction over the full intersection.

To evaluate the efficiency of Algorithm 2, Fig-
ure 8 (bottom) plots the relative decoding times of
the active set method over naively computing the
full intersection. The active set method is consis-
tently faster, with the largest speed-up (5.2x) at the
highest level of supervision as the average active
set size is the smallest at this setting.

Factors Affecting Speed-up. In general, the
amount of speed-up provided by the active set
method depends on several factors, including the

number of constraints, the size of the constraint
automata, and the cost of computing the softmax
during inference. The largest gains will come
when the former two factors are most expensive,
as the active set will only incur the intersection
cost as needed. If the output vocabulary is large,
softmax computation may outweigh the cost of
fully intersecting the constraint automata.

6 Related Work

Traditional Constrained Inference. Tradi-
tional constrained inference approaches enforced
constraints using general combinatorial opti-
mization frameworks, for instance linear (Taskar
et al., 2004) and integer linear programs (Roth
and Yih, 2005, 2007; Clarke and Lapata, 2008;
Martins et al., 2009), SAT solvers (Richardson
and Domingos, 2006; Poon and Domingos, 2008),
etc. Unlike these approaches which find the best
output that satisfies a set of constraints, our work
attempts to provide a similar general framework
for sequential inference algorithms that will find
the approximate-best output.

Unconstrained Data-driven Approaches.
Many sequential inference approaches do not
enforce constraints at all, in the hope that they will
be learned from data (Lample et al., 2016; Choe
and Charniak, 2016; Suhr et al., 2018). While the
model can potentially “impose” some constraints
which are well-represented in the data, there is no
guarantee that the output structure will be valid.
In contrast, our work guarantees valid output.

Post-Hoc Constraint Satisfaction. Some ap-
proaches first run unconstrained inference to find
the top-k structures and then identify valid struc-
tures in a post-hoc manner (Andreas et al., 2013;
Vinyals et al., 2015; Kiddon et al., 2016; Upad-
hyay et al., 2018). Such techniques also cannot
guarantee validity of the output structure when all
top-k structures are invalid, whereas our model en-
sures all top-k are valid.

Our Work. We draw from work in NLP that
uses automata (Mohri, 1997; Karttunen, 2000, in-
ter alia) and recent work in constrained text gen-
eration (discussed in §3). Our work is also re-
lated to Anderson et al. (2017) who impose lexi-
cal constraints for image captioning by maintain-
ing a beam for each state in an FSA, an impracti-
cal strategy for automata with thousands of states
(like those in §5.2). Tromble and Eisner (2006)



490

was the inspiration for the active set method. A
very recent work, Lee et al. (2019), also impose
constraints in sequential inference by including
them in the objective function and modifying the
model’s weights until the constraints are satisfied.

7 Conclusion and Future Work

We presented a principled, general-purpose con-
strained sequential inference algorithm. Key to
our algorithm is using automata to represent con-
straints, which we showed are capable of express-
ing popularly used constraints in NLP. Our ap-
proach is an attractive alternative to task-specific
constrained inference approaches currently in use.
Using a fast active set method, we can seamlessly
incorporate multiple constraints at inference time
without modifying the inference code. The exper-
imental results showed the value of our approach
over unconstrained inference, with the gains be-
coming more prominent in low-resource settings.

Acknowledgments

The authors would like to thank Sampath Kannan,
Caleb Stanford, Jordan Kodner, and the anony-
mous reviewers for their useful comments and
suggestions.

This work was supported by Contract HR0011-
15-C-0113 and Contract HR0011-18-2-0052 with
the US Defense Advanced Research Projects
Agency (DARPA). Approved for Public Release,
Distribution Unlimited. The views expressed are
those of the authors and do not reflect the official
policy or position of the Department of Defense or
the U.S. Government.

References
Peter Anderson, Basura Fernando, Mark Johnson, and

Stephen Gould. 2017. Guided Open Vocabulary
Image Captioning with Constrained Beam Search.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
936–945, Copenhagen, Denmark. Association for
Computational Linguistics.

Jacob Andreas, Andreas Vlachos, and Stephen Clark.
2013. Semantic Parsing as Machine Translation. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 47–52, Sofia, Bulgaria. Asso-
ciation for Computational Linguistics.

Do Kook Choe and Eugene Charniak. 2016. Parsing
as Language Modeling. In Proceedings of the 2016

Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2331–2336, Austin, Texas.
Association for Computational Linguistics.

James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research, 31:399–429.

Hal Daumé, John Langford, and Daniel Marcu. 2009.
Search-based Structured Prediction. Machine learn-
ing, 75(3):297–325.

Angel Daza and Anette Frank. 2018. A Sequence-to-
Sequence Model for Semantic Role Labeling. In
Proceedings of The Third Workshop on Representa-
tion Learning for NLP, pages 207–216. Association
for Computational Linguistics.

Daniel Deutsch, John Hewitt, and Dan Roth. 2018. A
Distributional and Orthographic Aggregation Model
for English Derivational Morphology. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1938–1947. Association for Computa-
tional Linguistics.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A. Smith. 2016. Recurrent neural network
grammars. In Proceedings of the 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 199–209, San Diego, Califor-
nia. Association for Computational Linguistics.

Marjan Ghazvininejad, Yejin Choi, and Kevin Knight.
2018. Neural Poetry Translation. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 2 (Short
Papers), pages 67–71, New Orleans, Louisiana. As-
sociation for Computational Linguistics.

Marjan Ghazvininejad, Xing Shi, Yejin Choi, and
Kevin Knight. 2016. Generating Topical Poetry.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183–1191, Austin, Texas. Association for Compu-
tational Linguistics.

Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245–288.

Kyle Gorman. 2016. Pynini: A Python Library for
Weighted Finite-state Grammar Compilation. In
Proceedings of the SIGFSM Workshop on Statistical
NLP and Weighted Automata, pages 75–80, Berlin,
Germany. Association for Computational Linguis-
tics.

Luheng He, Kenton Lee, Mike Lewis, and Luke Zettle-
moyer. 2017. Deep Semantic Role Labeling: What
Works and What’s Next. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages

https://www.aclweb.org/anthology/D17-1098
https://www.aclweb.org/anthology/D17-1098
http://www.aclweb.org/anthology/P13-2009
https://aclweb.org/anthology/D16-1257
https://aclweb.org/anthology/D16-1257
http://aclweb.org/anthology/W18-3027
http://aclweb.org/anthology/W18-3027
http://aclweb.org/anthology/P18-1180
http://aclweb.org/anthology/P18-1180
http://aclweb.org/anthology/P18-1180
http://www.aclweb.org/anthology/N16-1024
http://www.aclweb.org/anthology/N16-1024
http://www.aclweb.org/anthology/N18-2011
https://aclweb.org/anthology/D16-1126
http://anthology.aclweb.org/W16-2409
http://anthology.aclweb.org/W16-2409
https://doi.org/10.18653/v1/P17-1044
https://doi.org/10.18653/v1/P17-1044


491

473–483. Association for Computational Linguis-
tics.

Chris Hokamp and Qun Liu. 2017. Lexically Con-
strained Decoding for Sequence Generation Using
Grid Beam Search. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1535–
1546, Vancouver, Canada. Association for Compu-
tational Linguistics.

John E Hopcroft and Jeffrey D Ullman. 1979. Intro-
duction to automata theory, languages, and compu-
tation. Addison-Wesley.

Lauri Karttunen. 2000. Applications of finite-state
transducers in natural language processing. In In-
ternational Conference on Implementation and Ap-
plication of Automata, pages 34–46. Springer.

James E Kelley, Jr. 1960. The Cutting-Plane Method
for Solving Convex Programs. Journal of the society
for Industrial and Applied Mathematics, 8(4):703–
712.

Chloé Kiddon, Luke Zettlemoyer, and Yejin Choi.
2016. Globally Coherent Text Generation with Neu-
ral Checklist Models. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 329–339, Austin, Texas.
Association for Computational Linguistics.

Nikita Kitaev and Dan Klein. 2018. Constituency
Parsing with a Self-Attentive Encoder. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2676–2686. Association for Computa-
tional Linguistics.

Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gard-
ner. 2017. Neural semantic parsing with type con-
straints for semi-structured tables. In EMNLP.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural Architectures for Named Entity Recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 260–270, San Diego, California. Association
for Computational Linguistics.

Jay Yoon Lee, Sanket Vaibhav Mehta, Michael Wick,
Jean-Baptiste Tristan, and Jaime G. Carbonell. 2019.
Gradient-based inference for networks with output
constraints. In AAAI 2019.

Wang Ling, Phil Blunsom, Edward Grefenstette,
Karl Moritz Hermann, Tomáš Kočiský, Fumin
Wang, and Andrew Senior. 2016. Latent predictor
networks for code generation. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
599–609. Association for Computational Linguis-
tics.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2016. Multi-task Se-
quence to Sequence Learning. In Proc. of ICLR.

André FT Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise Integer Linear Programming Formu-
lations for Dependency Parsing. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL).

Mehryar Mohri. 1997. Finite-state Transducers in Lan-
guage and Speech Processing. Computational lin-
guistics, 23(2):269–311.

Joakim Nivre, Yoav Goldberg, and Ryan McDonald.
2014. Constrained Arc-Eager Dependency Parsing.
Computational Linguistics, 40(2).

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational linguistics,
31(1):71–106.

Hoifung Poon and Pedro Domingos. 2008. Joint
Unsupervised Coreference Resolution with Markov
Logic. In Proceedings of the Conference on Em-
pirical Methods for Natural Language Processing
(EMNLP), pages 650–659.

Matt Post and David Vilar. 2018. Fast Lexically Con-
strained Decoding with Dynamic Beam Allocation
for Neural Machine Translation. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), pages 1314–1324. Association for Computa-
tional Linguistics.

Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008.
The Importance of Syntactic Parsing and Inference
in Semantic Role Labeling. Computational Linguis-
tics, 34(2).

Matthew Richardson and Pedro Domingos. 2006.
Markov Logic Networks. Machine Learning Jour-
nal, 62(1-2):107–136.

Dan Roth and Scott Wen-tau Yih. 2004. A linear
programming formulation for global inference in
natural language tasks. In Proc. of the Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 1–8. Association for Computational
Linguistics.

Dan Roth and Scott Wen-tau Yih. 2005. Integer Lin-
ear Programming Inference for Conditional Random
Fields. In Proc. of the International Conference on
Machine Learning (ICML), pages 737–744.

Dan Roth and Wen-tau Yih. 2007. Global Inference
for Entity and Relation Identification via a Linear
Programming Formulation.

Michael Sipser. 1997. Introduction to the Theory of
Computation, volume 2. PWS Publishing.

http://aclweb.org/anthology/P17-1141
http://aclweb.org/anthology/P17-1141
http://aclweb.org/anthology/P17-1141
https://aclweb.org/anthology/D16-1032
https://aclweb.org/anthology/D16-1032
http://aclweb.org/anthology/P18-1249
http://aclweb.org/anthology/P18-1249
http://www.aclweb.org/anthology/N16-1030
https://doi.org/10.18653/v1/P16-1057
https://doi.org/10.18653/v1/P16-1057
http://aclweb.org/anthology/N18-1119
http://aclweb.org/anthology/N18-1119
http://aclweb.org/anthology/N18-1119
https://doi.org/http://dx.doi.org/10.1007/s10994-006-5833-1


492

Alane Suhr, Srinivasan Iyer, and Yoav Artzi. 2018.
Learning to Map Context-Dependent Sentences to
Executable Formal Queries. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Pa-
pers), pages 2238–2249. Association for Computa-
tional Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to Sequence Learning with Neural Net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Oscar Täckström, Kuzman Ganchev, and Dipanjan
Das. 2015. Efficient inference and structured learn-
ing for semantic role labeling. Transactions of the
Association for Computational Linguistics, 3:29–41.

B. Taskar. 2004. Learning Structured Prediction Mod-
els: A Large Margin Approach. Ph.D. thesis, Stan-
ford University. Computer Science TR-280.

B. Taskar, C. Guestrin, and D. Koller. 2004. Max-
margin markov networks. In The Conference on
Advances in Neural Information Processing Systems
(NIPS).

Roy Tromble and Jason Eisner. 2006. A fast finite-
state relaxation method for enforcing global con-
straints on sequence decoding. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 423–430, New
York City, USA. Association for Computational Lin-
guistics.

I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6:1453–1484.

Shyam Upadhyay, Jordan Kodner, and Dan Roth. 2018.
Bootstrapping Transliteration with Constrained Dis-
covery for Low-Resource Languages. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 501–511.
Association for Computational Linguistics.

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a Foreign Language. In Advances in Neural
Information Processing Systems, pages 2773–2781.

Nianwen Xue and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In Proceed-
ings of the Conference on Empirical Methods for
Natural Language Processing (EMNLP), pages 88–
94, Barcelona, Spain.

Yaoyuan Zhang, Zhenxu Ye, Yansong Feng, Dongyan
Zhao, and Rui Yan. 2017. A Constrained Sequence-
to-Sequence Neural Model for Sentence Simplifica-
tion. CoRR, abs/1704.02312.

Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
434–443, Sofia, Bulgaria. Association for Computa-
tional Linguistics.

https://doi.org/10.18653/v1/N18-1203
https://doi.org/10.18653/v1/N18-1203
http://www.aclweb.org/anthology/N/N06/N06-1054
http://www.aclweb.org/anthology/N/N06/N06-1054
http://www.aclweb.org/anthology/N/N06/N06-1054
http://aclweb.org/anthology/D18-1046
http://aclweb.org/anthology/D18-1046
http://www.aclweb.org/anthology/P13-1043
http://www.aclweb.org/anthology/P13-1043

