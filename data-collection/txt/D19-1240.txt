



















































Deep Reinforcement Learning-based Text Anonymization against Private-Attribute Inference


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2360–2369,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2360

Deep Reinforcement Learning-based Text Anonymization against
Private-Attribute Inference

Ahmadreza Mosallanezhad Ghazaleh Beigi
Computer Science and Engineering

Arizona State University
{amosalla, gbeigi, huanliu}@asu.edu

Huan Liu

Abstract

User-generated textual data is rich in con-
tent and has been used in many user be-
havioral modeling tasks. However, it could
also leak user private-attribute information that
they may not want to disclose such as age
and location. User’s privacy concerns man-
date data publishers to protect privacy. One
effective way is to anonymize the textual data.
In this paper, we study the problem of textual
data anonymization and propose a novel Re-
inforcement Learning-based Text Anonymi-
zor, RLTA, which addresses the problem of
private-attribute leakage while preserving the
utility of textual data. Our approach first ex-
tracts a latent representation of the original text
w.r.t. a given task, then leverages deep rein-
forcement learning to automatically learn an
optimal strategy for manipulating text repre-
sentations w.r.t. the received privacy and util-
ity feedback. Experiments show the effective-
ness of this approach in terms of preserving
both privacy and utility.

1 Introduction

Social media users generate a tremendous amount
of data such as profile information, network con-
nections and online reviews and posts. Online
vendors use this data to understand users prefer-
ences and further predict their future needs. How-
ever, user-generated data is rich in content and ma-
licious attackers can infer users’ sensitive infor-
mation. AOL search data leak in 2006 is an ex-
ample of privacy breaches which results in users
re-identification according to the published AOL
search logs and queries (Pass et al., 2006). There-
fore, these privacy concerns mandate that data be
anonymized before publishing. Recent research
has shown that textual data alone may contain suf-
ficient information about users’ private-attributes
that they do not want to disclose such as age, gen-
der, location, political views and sexual orienta-

tion (Mukherjee and Liu, 2010; Volkova et al.,
2015). Little attention has been paid to protect
users textual information (Li et al., 2018; Zhang
et al., 2018; Anandan et al., 2012; Saygin et al.,
2006).

Anonymizing textual information comes at the
cost of losing utility of data for future applications.
Some existing work shows the degraded quality of
textual information (Anandan et al., 2012; Zhang
et al., 2018; Saygin et al., 2006). Another re-
lated problem setting is when the latent represen-
tation of the user generated texts is shared for
different tasks. It is very common to use recur-
rent neural networks to create a representation of
user generated text to use for different machine
learning tasks. Hitaj el al. show text represen-
tations can leak users’ private information such
as location (Hitaj et al., 2017). This work aims
to anonymize users’ textual information against
private-attribute inference attacks.

Adversarial learning is the state-of-the-art ap-
proach for creating a privacy preserving text em-
bedding (Li et al., 2018; Coavoux et al., 2018).
In these methods, a model is trained to create a
text embedding, but we cannot control the privacy-
utility balance. Recent success of reinforcement
learning (RL) (Paulus et al., 2017; Sun and Zhang,
2018) shows a feasible alternative: by leveraging
reinforcement learning, we can include feedback
of attackers and utility in a reward function that al-
lows for the control of the privacy-utility balance.
Furthermore, an RL agent can perturb parts of an
embedded text for preserving both utility and pri-
vacy, instead of retraining an embedding as in ad-
versarial learning. Therefore, we propose a novel
Reinforcement Learning-based Text Anonymizer,
namely, RLTA, composed of two main compo-
nents: 1) an attention based task-aware text repre-
sentation learner to extract latent embedding rep-
resentation of the original text’s content w.r.t. a



2361

given task, and 2) a deep reinforcement learn-
ing based privacy and utility preserver to convert
the problem of text anonymization to a one-player
game in which the agent’s goal is to learn the
optimal strategy for text embedding manipulation
to satisfy both privacy and utility. The Deep Q-
Learning algorithm is then used to train the agent
capable of changing the text embedding w.r.t. the
received feedback from the privacy and utility sub-
components.

We investigate the following challenges: 1)
How could we extract the textual embedding w.r.t.
a given task? 2) How could we perturb the ex-
tracted text embedding to ensure that user private-
attribute information is obscured? and 3) How
could we preserve the utility of text embedding
during anonymization? Our main contributions
are: (1) we study the problem of text anonymiza-
tion by learning a reinforced task-aware text
anonymizer, (2) we corporate a data-utility task-
aware checker to ensure that the utility of textual
embeddings is preserved w.r.t. a given task, and
(3) we conduct experiments on real-world data to
demonstrate the effectiveness of RLTA in an im-
portant natural language processing task.

2 Related Work

Reinforcement Learning (RL) has applications in
natural language processing and recommendation
systems. For example, a recent paper (Paulus
et al., 2017) combines RL with a supervised
method to get a readable and informative article
summary. Another work uses RL to solve the
problem of adversarial generative models for text
generation (Shi et al., 2018). Sun et al. also uses
RL in a recommendation system which recom-
mends items according to the users’ feedbacks and
preferences (Sun and Zhang, 2018)

Textual data is rich in content and recent re-
search has shown that users’ private-attributes can
be easily inferred from the text (Beretta et al.,
2015; Mukherjee and Liu, 2010; Volkova et al.,
2015), however, few papers consider user privacy
w.r.t. such data. Anandan et al. (2012) introduce
t-Plausibility which uses an information theoretic
based approach to sanitize documents heuristi-
cally. This method does not preserves the utility of
data during anonymization process. Another work
focuses on leveraging differential privacy (Dwork
et al., 2017) to make the extracted Term Frequency
Inverse Document (TF-IDF) textual vectors pri-

vate (Zhang et al., 2018). It has been shown that
TF-IDF cannot accurately capture semantic mean-
ing of the text which can hurt its usefulness for
different tasks (Lan et al., 2005).

Two recent similar works Li et al. (2018);
Coavoux et al. (2018) convert textual data
anonymization into a minimax problem. These
works use the idea of adversarial learning to
create a text embedding which satisfies utility
and protects users against private-attribute leak-
age. Our scenario is similar to the work of (Li
et al., 2018) as they have considered several
attribute-inference attackers as adversaries to cre-
ate a privacy-preserving text embedding. (Beigi
et al., 2019b,a) propose a method for privacy pre-
serving text representation. In this method, they
try to add noise to an existing text representation
in a way that it does not change the meaning of the
text and it preserves the user’s private attributes.

Our work is different from the existing works.
First, we consider the task that the textual infor-
mation will be used for and protect users against
leakage of private-attributes. Second, we incorpo-
rate deep RL to anonymize the extracted text em-
bedding by receiving privacy and utility feedbacks
and automatically learning the optimal strategy for
proper manipulation of text embeddings.

3 Problem Statement

Let X = {x1, x2, ..., xN} denotes a set of N doc-
uments and each document xi is composed of a
sequence of words. We denote vi ∈ Rd×1 as the
embedded representation of the original document
xi. Let P = {p1, p2, ..., pm} denotes a set of m
private-attributes that users do not want to disclose
such as age, gender, location, etc. The goal of re-
inforced task-aware text anonymizer is to learn an
embedding representation of each document and
then anonymize it such that 1) users privacy is
preserved by preventing any potential attacker to
infer users’ private-attribute information from the
textual embedding data, and 2) utility of the text
embedding is maintained for a given task T which
incorporates such data, e.g., classification. In this
paper, we study the following problem:

Problem 3.1. Given a set of documents X , set
of private-attributes P , and given task T , learn
an anonymizer f that can learn a private embed-
ded representation vi from the original document
xi so that, 1) the adversary cannot infer the tar-
geted user’s private-attributes P from the private



2362

Figure 1: The architecture of RLTA method

text representation vi, and 2) the generated private
representation vi is good for the given task T . The
problem can be formally defined as:

vi = f(xi,P, T ) (1)

Due to the success of Reinforcement Learn-
ing (Shi et al., 2018; Paulus et al., 2017), we
use RL to address the aforementioned problem.
RL (Sutton and Barto, 2018) formulates the prob-
lem within the framework of Markov Decision
Process (MDP), and learns an action-selection pol-
icy based on past observations of transition data.
An MDP is defined by state space S = {s}, ac-
tion space A = {a}, transition probability func-
tion P : S × A × S → [0, 1] and reward function
r : S ×A× S → R.

4 Proposed Method
We discuss the reinforced task-aware text
anonymizer framework. The input of this private
system is the user generated text, and the output
is a privacy-preserving text representation. As in
Figure. 1, this framework consists of two major
components: 1) an attention based task-aware
text representation learner, and 2) a deep RL
based privacy and utility preserver. The text rep-
resentation learner aims to extract the embedded
representation of a document w.r.t. a given task
by minimizing the task’s loss function. Then, the
deep RL preserver manipulates the embedded text
representation by learning the optimal strategy
so that both privacy and utility of the embedded
representation are preserved. It includes two
sub-components: 1) private-attribute inference at-
tacker DP , and 2) data-utility task-aware checker

DU . The former seeks to infer user private-
attribute information based on their embedded
text representation. The latter incorporates the
given manipulated embedded text representation
for a given task T and investigates the usefulness
of the latent representation for T .

The RL component then utilizes the feedback
of the two sub-components to guide the data ma-
nipulation process by ensuring that the new text
embedding does not leak user private-attributes by
confusing the adversary in DP and the changes
made to the representation does not destroy the se-
mantic meaning for T .

4.1 Extracting Textual Embedding

Let x = {w1, ..., wm} be a document with m
words. Attention mechanism has shown to be ef-
fective in capturing embedding of textual informa-
tion w.r.t. a given task (Pennington et al.; Vaswani
et al., 2017). It allows the model to attend to dif-
ferent parts of the given original document at each
step and then learns what to attend based on the in-
put document and what it has produced as embed-
ding representation so far, as shown in Figure. 1.

We use a bi-directional recurrent neural net-
work (RNN) to encode the given document into
an initial embedding representation. RNN has
been shown to be effective for summarizing and
learning semantic of unstructured noisy short
texts (Cho et al., 2014; Shang et al., 2015). We use
GloVe 100d (Pennington et al.) to exchange each
word wi with its corresponding word vector, note
that different dimensionality can be used. This
process produces a matrix of text x′ ∈ Rm∗100.

We employ the gated recurrent unit (GRU) as
the cell type to build the RNN, which is designed
in a manner to have a more persisted memory (Cho
et al., 2014). The bi-directional GRU will read the
text forward and backwards, then outputs two hid-
den states hfwt ,h

bw
t and an output ot. We then

concatenate two hidden states as the initial en-
coded embedding of the given original document:

Ht = Concat(h
fw
t ,h

bw
t ) (2)

After calculating the initial context vector Ht, we
seek to pinpoint specific information within the
Ht, which helps the classifier to predict the labels
with higher confidence (Luong et al., 2015) We
use the location-based attention layer based on the
work of Luong et al. (2015). The attention layer
calculates a vector at including a weight for each



2363

element in the Ht, showing the importance of that
element. The context vector vt is calculated:

vt =

m∑
i=1

at,iHi (3)

The vector vt is then fed to a neural network clas-
sifier for the given utility task. Classification is
one of the common tasks for textual data. Based
on the output of the classifier and loss function, we
update the three networks so that the output of the
attention layer is an useful context that can be used
for a utility task (Ranzato et al., 2015).

4.2 Reinforced Task-Aware Text Anonymizer

Here, we discuss the details of the second compo-
nent which seeks to preserve privacy and utility.

4.2.1 Protecting Private-Attributes
Textual information is rich in content and pub-
lishing textual embedding representation without
proper anonymization leads to privacy breach and
revealing the private-attributes of an individual
such age, gender and location. It is thus essen-
tial to protect the textual information before pub-
lishing it. The goal of our model is to manipu-
lated learned embedded representation such that
any potential adversary cannot infer users’ private-
attribute information. However, a challenge is
that the text anonymizer does not know the ad-
versary’s attack model. To address this challenge,
we add a private-attribute inference attacker DP
sub-component to our text anonymizer. This sub-
component learns a classifier that can accurately
identify the private information of users from their
embedded text representations vu. We incorporate
this sub-component to understand how the textual
embedded representation should be anonymized to
obfuscate the private information.

Inspired by the success of RL (Kaelbling et al.,
1996; Mnih et al., 2013; Van Hasselt et al., 2016),
we model this problem using RL to automatically
learn how to anonymize the text representations
w.r.t. the private-attribute inference attacker. In
our RL model, one agent is trained to change a
randomly selected text embedding representation.
Then, the agent keeps interacting with the environ-
ment and changes the text embedding accordingly
based on its current state and received rewards so
that the private-attribute inference attacker cannot
correctly identify user’s private-attribute informa-
tion given his embedding. In this part, we define

the main four parts of RL environment in our prob-
lem, i.e., environment, state, action and reward.

• Environment: Environment in our problem in-
cludes the private-attribute inference attackers
DP and the text embedding vu. Note that DP
is trained beforehand.

• State: State describes the current situation.
Here, state is the current text embedding vector
vu,t which reflects the results of the agents’ ac-
tions on vu up to time t.

• Actions: Action is define as selecting one el-
ement such as vu,k in text embedding vector
vu = {vu,1, ..., vu,m} and changing it to a value
near −1, 0 or 1. This results in 3.m actions
where m is the size of the embedding vector.

Changing value to near 1: In this action, the
agent changes the value of vu,k to a value be-
tween 0.9 to 1.0. As vu,k will be multiplied by a
classifier’s weight, the output will be the weight
as is. In another word, the value vu,k will be-
come important to the classifier.

Changing value to near 0: In this action the
vu,k will be changed to a value between −0.01
to 0.01. This action makes vu,k seem neutral and
unimportant to a classifier as it will result in a 0
when multiplied by a weight.

Changing value to near -1: In this action, the
agent changes vu,k to a value between −1.0 to
−0.9. This action will make vu,k important to a
classifier, but, in a negative way.

• Reward: Reward in our problem is defined
based on how successfully the agent obfuscated
the private-attribute information against the at-
tacker so far. In particular, we defined the reward
function at state st+1 according to the confi-
dence of private-attribute inference attacker Cpk
for private-attribute pk given the resultant text
embedding at state st+1, i.e., vt+1. Consider-
ing the classifier’s input data as vu and its cor-
rect label as i, we define the confidence for a
multi-class classifier as the difference between
the probability of actual value of the private-
attribute and the minimum probability of other
values of the private-attribute:

Cpk = Pr(l = i|vu)−min
j
Pr(l = j|vu) (4)

Where l indicates label. For each private-
attribute attacker pk, the confidence score Cpk is



2364

within the range [−1, 1]. Positive value demon-
strates that the attacker has predicted private-
attribute accurately, and negative value indicates
that the attacker was not able to infer user’s
private-attribute. According to this definition,
the reward will be positive if action at has
caused information hiding, and will be nega-
tive if the action at was not able to hide sensi-
tive information. Having confidence of private-
attribute inference attackers, reward function at
state st+1 is defined as:

rt+1(st+1) = −
∑

pk∈DP

Cpk(st+1) (5)

The reward rt is calculated according to the
state st+1 which associated with the transition
of agent from state st after applying action at.
Note that the goal of agent is to maximize the
amount of received rewards so that the mean of
rewards r over time t ∈ [0, T ] (T is the terminal
time) will be positive and above 0.

4.2.2 Preserving Utility of Text Embedding
Thus far, we have discussed how to 1) learn tex-
tual embeddings from the given original document
w.r.t. the given task, and 2) prevent leakage of
private-attribute information by developing a re-
inforcement learning environment which incorpo-
rates a private-attribute inference attacker and ma-
nipulates the initial given text embedding accord-
ingly to fool the attacker. However, data obfus-
cation comes at the cost of data utility loss. Util-
ity is defined as the quality of the given data for a
given task. Neglecting the utility of the text em-
bedding while manipulating it, may destroy the
semantic meaning of the text data for the given
task. Classification is one of the common tasks.
In order to preserve the utility of data, we need
to ensure that preserving privacy of data does not
destroy the semantic meaning of the text embed-
ding representation w.r.t. the given task. We ap-
proach this challenge by changing the agent’s re-
ward function w.r.t. the data utility. We add a util-
ity sub-component, i.e., classifier DU , to the rein-
forcement learning environment which its goal is
to assess the quality of resultant embedding repre-
sentation. We use the confidence of the classifier
for the given task to measure the utility of embed-
ding representation using the text embedding vec-
tor vu the its correct label i.

C = Pr(l = i|vu)−min
j
Pr(l = j|vu) (6)

The agent can then use the feedback from the util-
ity classifier to make decision when taking actions.
We thus modify the reward function in order to in-
corporate the confidence of utility sub-component.
Reward function at state st+1 can be defined as:

rt+1(st+1) =αCDU (st+1)− (7)

−(1− α)
∑

pk∈DP

Cpk(st+1)− B

where CDU and Cpk represent the confidence of
utility sub-component and private-attribute infer-
ence attacker, respectively. Moreover, B demon-
strates a baseline reward which forces the agent
to reach a minimum reward value. The coeffi-
cient α also control the amount of contribution
from both private-attribute inference and utility
sub-components in the Eq. 7.

4.3 Optimization Algorithm

Given the formulation of states and actions, we
aim to learn the optimal strategy via manipulat-
ing text representations w.r.t. the private-attribute
attackers and utility sub-component feedbacks.
We manipulate the text embeddings by repeatedly
choosing an action at given current state st, and
then applying actions on current state to transit to
the new one st+1. The agent then receives reward
rt+1 as a consequence of interacting with the en-
vironment. The goal of agent is to manipulate text
embedding vu,k in a way that maximizes its reward
according to Eq. 7. Moreover, the agent updates its
action selection policy π(s) so that it can achieve
the maximum reward over time.

In RLTA we use Deep Q-Learning which is a
variant of Q-Learning. In this algorithm the goal
is to find the following function:

Q∗(st, at) = Est+1 [rt+1 + γmax
a′

Q∗(st+1, a
′)] (8)

where Q(s, a) corresponds to the Q-function for
extracting actions and it is defined as the expected
return based on state s and action a. More-
over,Q∗(s, a) denotes the optimal action-value Q-
function which has the maximum expected return
using the optimal policy π(s). Rewards are also
discounted by a factor of γ per time step. The
agent keeps interacting with the environment till
it reaches the terminal time T .

Since it is not feasible to estimate Q∗(s, a)
in Eq.8, we use a function approximator to esti-
mate the state-action value function Q∗(s, a) ≈



2365

Algorithm 1 The Learning Process of RLTA
Require: v, DP , DU , α, γ, B, T .

1: Initialize replay memory M with size N
2: while training is not terminal do
3: st ← v
4: for t ∈ {0, 1, ..., T} do
5: Choose action at using �-greedy
6: Perform at on st and get (st+1, rt+1)
7: M ←M + (st, at, rt+1, st+1)
8: st ← st+1
9: Sample mini-batch b from memroy M

10: for (s, a, s′, r) ∈ b do
11: Update DQN weights using Eq. 11
12: end for
13: end for
14: end while

Q(s, a; θ). Given neural networks as excellent
function approximators (Cybenko, 1989), we lver-
age a deep neural network function approximator
with parameters θ, or a Deep Q-Network (DQN)
(Mnih et al., 2013) by minimizing the following:

L(θ) = Est,at,rt+1,st+1 [(y −Q(s, a; θ))2] (9)

in which y is the target for the current iteration:

y = Est+1 [rt+1 + γmax
a′

Q(st+1, a
′; θp)] (10)

θp is the parameters from the previous iteration.
We update the DQN according to the derivation

of Eq. 9 with respect to the parameter θ:

∇θL(θ) = Est,at,rt+1,st+1 [(r (11)
+γmax

a′
Q(st+1, a

′; θp)

−Q(st, at; θ))∇θQ(st, at; θ)]

Algorithm 1 shows the optimization process.

5 Experiments

Experiments are designed to answer the fol-
lowing questions: Q1(Privacy): How well RLTA
can obscure users’ private-attribute information?
Q2(Utility): How well RLTA can preserve util-
ity of the textual data w.r.t. the given task?
Q3(Privacy-Utility Relation): How does improv-
ing user privacy affects loss of utility?

To answer the first question (Q1), we use inves-
tigate the robustness of resultant text embedding
against private-attribute inference attacks. We
consider two private-attribute information, i.e., lo-
cation and gender. To answer the second question

(Q2), we report experimental results w.r.t. a well-
known task, sentiment analysis. Sentiment analy-
sis has many applications in user-behavioral mod-
eling and Web (Zafarani et al., 2014). In particular,
we predict sentiment of the given textual embed-
ding. To answer the final question (Q3), we exam-
ine the privacy improvement against utility loss.

5.1 Data

We use a real-world dataset from Trustpilot (Hovy
et al., 2015). This dataset includes user reviews
along with users private-attribute information such
as location and gender. We remove non-English
reviews based on LANGID.py1 (Lui and Bald-
win, 2012) and only keep reviews classified as En-
glish. Then, we consider English reviews associ-
ated with location of US and UK and create a sub-
set of data with 10k users. Each review is associ-
ated with a rating score. We consider the review’s
sentiment as positive if its rating score is {4, 5}
and consider it as negative if rating is {1, 2, 3}

5.2 Implementation Details

For extracting the initial textual embedding, we
use a bi-directional RNNs which their hidden sizes
are set to 25. This makes the size of the final hid-
den vector Ht as 50. We also use a logistic re-
gression with a linear network as the classifier in
the attention mechanism. We use a 3-layer net-
work for the Deep Q-network, i.e., input, hidden
and output layers. Dimensions of the input and
hidden layers are set to 50 and 700, respectively.
Dimension of the last layer, i.e., output, is also set
as 150. This layer outputs the state-action values
which we execute the action with the best value.

For each of the private-attribute attackers and
utility sub-components, we use feed-forward net-
work with a single hidden layer with dimension of
100 which gets the textual embedding as input and
uses a Softmax function as output.

We first train both private-attribute inference at-
tacker DP and utility sub-component DU on the
training set. These sub-components do not change
after that. Then, we train an agent on each se-
lected data for 5000 episodes. The reward dis-
count for agents is γ = 0.99 and batch size b = 32.
We also set the terminal time T = 25. We run
RLTA for 5 times and select the best agent based
on the cumulative reward. We also vary α as
α = {0, 0.25, 0.5, 0.75, 1}. The higher values of

1https://github.com/saffsd/langid.py



2366

(a) Gender inference attack (b) Location inference attack (c) Sentiment prediction

Figure 2: AUC scores for private-attribute and sentiment prediction tasks for different values of α. Lower
AUC for private-attribute inference attacks shows higher privacy, while higher AUC for the sentiment pre-
diction task indicates higher utility.

α indicate more utility contribution in RLTA.

5.3 Experimental Design
We use 10-fold cross validation of RLTA for eval-
uating both private-attribute inference attacker and
an utility task with the following baselines:

• ORIGINAL: This baseline is a variant of pro-
posed RLTA which does not change the original
user text embeddings vu and publishes it as is.

• ADV-ALL: This adversarial method has two
main components, i.e., generator and discrimi-
nator, and creates a text representation that has
high quality for a given task, but has poor quality
for inferring private-attributes (Li et al., 2018).

• ENC-DEC: Using an auto-encoder is one of
the effective methods to create a text embed-
ding (Nallapati et al., 2016). We modify this
simple method to create a privacy-preserving
text embedding. This method gets the origi-
nal text x and outputs a re-constructed text x̄.
The following loss function is used to train the
model. After training, we use the encoder’s out-
put as the text representation vu (Cho et al.,
2014).

loss = −
∑
x∈X

logPr(x̄|x)+ (12)

+ α((
∑
pk

Cpk)− CDU )

In which α is the privacy budget.
To examine the privacy of final text embed-

ding, we apply the trained private-attribute at-
tacker sub-component DP to the output of each
method to evaluate the users’ privacy. We con-
sider two private attributes, i.e., location and gen-
der. We then compute the attacker’s AUC. Lower
attacker’s AUC indicates that textual embeddings
have higher privacy after anonymization against

the private-attribute inference attacker. We also
report experimental results w.r.t. the utility. In
particular, we predict sentiment (positive and neg-
ative) of the given textual embedding by applying
trained utility sub-component DU to the resultant
text embedding from test set for each method. We
then compute AUC score for sentiment prediction
task. Higher values of AUC demonstrate that the
utility of textual embedding has been preserved.

5.4 Experimental Results

We answer the three question Q1, Q2 and Q3 to
evaluate our proposed method RLTA.We use a
natural language processing task, sentiment pre-
diction, using a three layer neural network.
Privacy (Q1). Figure. 2 (a-b) demonstrates the
results of private-attribute inference attack w.r.t.
gender and location attributes. The lower the value
of AUC is, the more privacy user has in terms of
obscuring private attributes. We also report the
performance of RLTA for different values of α.

We observe that ORIGINAL is not robust against
private-attribute inference attack for both gender
and location attributes. This confirms leakage of
users private information from their textual data.
Moreover, RLTA has significantly lower AUC
score for both gender and location attributes in
comparison to other methods. This demonstrates
the effectiveness of RL for obfuscating private at-
tributes. In RLTA, the AUC score for private-
attribute inference attack increases for both at-
tributes with the increase of α which shows the
degradation in user privacy. The reason is because
of the fact that agent pays less attention to privacy
by increasing the value of α.

In the ENC-DEC method, as the value of α in-
creases, the encoder tries to generate a text repre-
sentation that is prune to inference attacks but it



2367

does not lose its utility w.r.t. the given task DU .
The results show that as α increases, the AUC of
inference attackers will decrease.

Utility (Q2). To answer the second question, we
investigate the utility of embeddings w.r.t. senti-
ment prediction. Results for different values of α
are demonstrated in Figure. 2(c). The higher the
value of the AUC is, the higher utility is preserved.

The ORIGINAL approach has the highest AUC
score which shows the utility of the text embed-
dings before any anonymization. We observe that
the results for RLTA is comparable to the ORIGI-
NAL approach which shows that RLTA preserves
the utility of text embedding. Moreover, RLTA
outperforms ADV-ALL which confirms the effec-
tiveness of reinforced task-aware text anonymiza-
tion approach in preserving utility of the textual
embeddings. We also observe that the AUC of
RLTA w.r.t. sentiment prediction task increases
with the increase of value of α. This is because
with the increase of α, the agent pays more atten-
tion to the feedbacks of utility sub-component.

We also observe a small utility loss after apply-
ing RLTA when α = 1. This is because the agent
keeps changing the text embedding until it reaches
the terminal time. These changes result in loss of
utility even when the α = 1.

Finally, in the ENC-DEC method, as both utility
and attackers have the same importance, trying to
preserving privacy would result in huge utility loss
as we increase the value of α.

Privacy-Utility Relation (Q3). Results show that
the ORIGINAL achieves the highest AUC score for
both utility task and private-attribute inference at-
tack. This shows that ORIGINAL has the highest
utility which comes at the cost of significant user
privacy loss. However, comparing results of pri-
vacy and utility for α = 0.5, we observe RLTA
has achieved the lowest AUC score for attribute
inference attacks in comparison to other baselines,
thus has the highest privacy. It also reaches the
higher utility level in comparison to the ADV-ALL.
RLTA also has comparable utility results to the
ORIGINAL approach. We also observe that in-
creasing the α reduces the performance of RLTA
in terms of privacy but increases its performance
for utility. However, with α = 1, RLTA pre-
serves both user privacy and utility in comparison
to ORIGINAL, ENC-DEC, and ADV-ALL.

Method Location Gender Utility
ORIGINAL 84.77 86.54 58.57
ENC-DEC 71.55 58.35 53.78
ADV-ALL 70.37 57.15 52.15
RLTA 53.34 56.41 54.83
RLTA-GEN 56.64 55.02 56.67
RLTA-LOC 52.04 56.64 54.13

Table 1: Impact of different private-attribute in-
ference attackers on RLTA when α = 0.5. With
α = 0.5, privacy and utility will contribute equally.

5.4.1 Impact of Different Components
Here, we investigate the impact of different
private-attribute inference attackers. We define
two variants of our proposed model, RLTA-GEN
and RLTA-LOC. In each of these variants, we
train the agent in RLTA w.r.t. the one of private-
attribute attackers, e.g., RLTA-GEN is trained to
solely hide gender attribute. For this experiment
we set α = 0.5 as in this case privacy and utility
sub-components contribute equally during train-
ing phase (Eq. 7). Results are shown in Table 1.

RLTA-LOC and RLTA-GEN have the best per-
formance amongst all methods in obfuscating lo-
cation and gender private-attributes, respectively.
Results show that using RLTA-LOC could also
help improve privacy on gender and likewise for
(RLTA-GEN) in comparison to other approaches.

RLTA-GEN performs better in terms of utility,
in comparison to RLTA which incorporates both
gender and location attackers. Moreover, results
show that both RLTA-GEN and RLTA-LOC have
better utility than other baselines.

To sum-up, these results indicate that although
using one private-attribute attacker in the training
process can help in preserving more utility, it can
compromise obscuring other private-attributes.

Parameter Analysis: Our proposed method
RLTA has an important parameter α to change
the level of privacy and utility. We illustrate the
effect of this parameter by changing it as α ∈
{0.0, 0.1, 0.25, 0.5, 0.75, 1.0}. According to the
Figure 2, when the α parameter increases, the pri-
vacy loss will decrease, but, the utility loss will
increase. This shows the utility and the privacy
have an association with each other. Hence, the
more privacy loss decreases, the utility loss in-
creases. Choosing the right value for α depends
on the application and usage of this method. Ac-
cording to the results, choosing α = 0.5 would
result in a balanced privacy-utility. In some appli-
cations where the privacy of users are important
and critical, we can set the α parameter above 0.5.



2368

Figure 3: The average of agent’s rewards

On the other hand, if the users privacy is not top
priority, this parameter can be set to a lower value
than 0.5 which although it does not protect users’
private attribute as good as when α >= 0.5, but it
does protect users’ private attribute at a reasonable
level.

5.4.2 Rewards Convergence
To evaluate the convergence of rewards, we con-
sider agent’s reward during training phase for each
episode, shown in Figure. 3. The result indicates
that agent’s average reward is low at the begin-
ning and then it increases afterward. This is be-
cause agent performs many random actions at the
beginning to explore the action state space. We
also observe that after several episodes, the reward
converges to the baseline reward B. This confirms
that the agent has learned a proper action selection
policy π(s) to preserve both utility and privacy by
satisfying the objectives of Eq. 7.

6 Conclusion

In this paper, we propose a deep reinforcement
learning based text anonymization, RLTA, which
creates a text embedding such that does not leak
user’s private-attribute information while preserv-
ing its utility w.r.t. a given task. RLTA has two
main components: (1) an attention based task-
aware text representation learner, and (2) a deep
RL based privacy and utility preserver. Our re-
sults illustrate the effectiveness of RLTA in pre-
serving privacy and utility. One future direction is
to generate privacy preserving text rather than em-
beddings. We also adopt deep Q-learning to train
the agent. A future direction is to apply different
RL algorithms and investigate how it impacts re-
sults. It would be also interesting to adopt RLTA
for other types of data.

7 Acknowledgements

The authors would like to thank Jundong Li for
his help throughout the paper. This material is
based upon the work supported, in part, by NSF
1614576, ARO W911NF-15-1-0328 and ONR
N00014-17-1-2605.

References
Balamurugan Anandan, Chris Clifton, Wei Jiang,

Mummoorthy Murugesan, Pedro Pastrana-
Camacho, and Luo Si. 2012. t-plausibility:
Generalizing words to desensitize text. Trans. Data
Privacy, 5(3):505–534.

Ghazaleh Beigi, Kai Shu, Ruocheng Guo, Suhang
Wang, and Huan Liu. 2019a. I am not what i
write: Privacy preserving text representation learn-
ing. arXiv preprint arXiv:1907.03189.

Ghazaleh Beigi, Kai Shu, Ruocheng Guo, Suhang
Wang, and Huan Liu. 2019b. Privacy preserving text
representation learning. Proceedings of the 30th on
Hypertext and Social Media (HT19). ACM.

Valentina Beretta, Daniele Maccagnola, Timothy Crib-
bin, and Enza Messina. 2015. An interactive method
for inferring demographic attributes in twitter. In
ACM Conference on Hypertext & Social Media.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Maximin Coavoux, Shashi Narayan, and Shay B Co-
hen. 2018. Privacy-preserving neural representa-
tions of text. arXiv preprint arXiv:1808.09408.

George Cybenko. 1989. Approximation by superposi-
tions of a sigmoidal function. Mathematics of con-
trol, signals and systems, 2(4):303–314.

Cynthia Dwork, Frank McSherry, Kobbi Nissim, and
Adam Smith. 2017. Calibrating noise to sensitiv-
ity in private data analysis. Journal of Privacy and
Confidentiality, 7(3):17–51.

Briland Hitaj, Giuseppe Ateniese, and Fernando Pérez-
Cruz. 2017. Deep models under the gan: informa-
tion leakage from collaborative deep learning. In
Proceedings of the 2017 ACM SIGSAC Conference
on Computer and Communications Security, pages
603–618. ACM.

Dirk Hovy, Anders Johannsen, and Anders Søgaard.
2015. User review sites as a resource for large-scale
sociolinguistic studies. In Proceedings of the 24th
International Conference on World Wide Web.



2369

Leslie Pack Kaelbling, Michael L Littman, and An-
drew W Moore. 1996. Reinforcement learning: A
survey. Journal of artificial intelligence research.

Man Lan, Sam-Yuan Sung, Hwee-Boon Low, and
Chew-Lim Tan. 2005. A comparative study on term
weighting schemes for text categorization. In IEEE
International Joint Conference on Neural Networks.

Yitong Li, Timothy Baldwin, and Trevor Cohn. 2018.
Towards robust and privacy-preserving text repre-
sentations. In The 56th Annual Meeting of the As-
sociation for Computational Linguistics.

Marco Lui and Timothy Baldwin. 2012. langid. py: An
off-the-shelf language identification tool. In Pro-
ceedings of the ACL 2012 system demonstrations.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412–1421.

Volodymyr Mnih, Koray Kavukcuoglu, David Sil-
ver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. 2013. Playing atari
with deep reinforcement learning. arXiv preprint
arXiv:1312.5602.

Arjun Mukherjee and Bing Liu. 2010. Improving gen-
der classification of blog authors. In Empirical
Methods in natural Language Processing (EMNLP).

Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre,
Bing Xiang, et al. 2016. Abstractive text summa-
rization using sequence-to-sequence rnns and be-
yond. arXiv preprint arXiv:1602.06023.

Greg Pass, Abdur Chowdhury, and Cayley Torgeson.
2006. A picture of search. In 1st international con-
ference on Scalable information systems (InfoScale).

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. Glove: Global vectors for word represen-
tation. In Proceedings of conference on Empirical
Methods in natural Language Processing (EMNLP).

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015. Sequence level train-
ing with recurrent neural networks. arXiv preprint
arXiv:1511.06732.

Yücel Saygin, Dilek Hakkini-Tur, and Gökhan Tur.
2006. Sanitization and anonymization of document
repositories. In Web and information security.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics.

Zhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing
Huang. 2018. Toward diverse text generation with
inverse reinforcement learning. In 27th Interna-
tional Joint Conference on Artificial Intelligence.

Yueming Sun and Yi Zhang. 2018. Conversational rec-
ommender system. In ACM SIGIR Conference on
Research & Development in Information Retrieval.

Richard S Sutton and Andrew G Barto. 2018. Rein-
forcement learning: An introduction. MIT press.

Hado Van Hasselt, Arthur Guez, and David Silver.
2016. Deep reinforcement learning with double q-
learning. In 30th AAAI Conference.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Svitlana Volkova, Yoram Bachrach, Michael Arm-
strong, and Vijay Sharma. 2015. Inferring latent
user properties from texts published in social media.
In 29th AAAI Conference on Artificial Intelligence.

Reza Zafarani, Mohammad Ali Abbasi, and Huan Liu.
2014. Social media mining: an introduction. Cam-
bridge University Press.

Jinxue Zhang, Jingchao Sun, Rui Zhang, Yanchao
Zhang, and Xia Hu. 2018. Privacy-preserving so-
cial media data outsourcing. In IEEE INFOCOM
Conference on Computer Communications.


