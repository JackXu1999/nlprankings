



















































Scientific Discovery as Link Prediction in Influence and Citation Graphs


Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12), pages 1–6
New Orleans, Louisiana, June 6, 2018. c©2018 Association for Computational Linguistics

Scientific Discovery as Link Prediction in Influence and Citation Graphs

Fan Luo Marco Valenzuela-Escárcega Gus Hahn-Powell Mihai Surdeanu
University of Arizona, Tucson, AZ, USA

{fanluo, marcov, hahnpowell, msurdeanu}@email.arizona.edu

Abstract

We introduce a machine learning approach for
the identification of “white spaces” in scien-
tific knowledge. Our approach addresses this
task as link prediction over a graph that con-
tains over 2M influence statements such as
“CTCF activates FOXA1”, which were au-
tomatically extracted using open-domain ma-
chine reading. We model this prediction task
using graph-based features extracted from the
above influence graph, as well as from a ci-
tation graph that captures scientific commu-
nities. We evaluated the proposed approach
through backtesting. Although the data is
heavily unbalanced (50 times more negative
examples than positives), our approach pre-
dicts which influence links will be discovered
in the “near future” with a F1 score of 27
points, and a mean average precision of 68%.

1 Introduction

The amount of scientific knowledge that is pub-
licly available has increased dramatically in the
past few years. For example, PubMed, a search
engine of biomedical publications,1 now indexes
over 25 million papers, 17 million of which were
published between 1990 and the present. This in-
formation overload yields two critical problems.
First, this exceeds the human capacity to aggregate
and interpret the fragments of knowledge pub-
lished in these papers, which may result in ex-
isting solutions to critical problems being over-
looked. Swanson (1986) described this problem
as “undiscovered public knowledge”. Second, this
vast amount of available information complicates
the identification of “white spaces” in science,
i.e., topics that are insufficiently studied and may
lead to important scientific discoveries.

While the first problem has been addressed re-
cently with efforts that combine machine read-

1http://www.ncbi.nlm.nih.gov/pubmed

ing and assembly with existing data analysis al-
gorithms (Valenzuela-Escarcega et al., 2017; Poon
et al., 2015, inter alia), the second problem is
largely unstudied.

In this work we propose a first enabling step
towards addressing the problem of white space
discovery from literature (Sebastian et al., 2017;
Cameron, 2014) with an approach inspired from
the field of link prediction (Liben-Nowell and
Kleinberg, 2007; Leskovec et al., 2010). In par-
ticular, our method operates over two graphs: (a)
a graph of positive/negative influence relations
such as the relation “CTCF activates FOXA1”
between the two proteins, which were extracted
using an existing, open-domain machine read-
ing tool (Hahn-Powell et al., 2017) from over
100K biomedical publications2; and (b) the ci-
tation graph between the corresponding papers
where these findings were published. The pro-
posed approach approximates the task of white
space discovery by predicting new influence rela-
tions that do not exist in the influence graph at a
given time (hence the white space) but will emerge
in future (thus somebody identified the missing
knowledge as important).

The contributions of this work are:

(1) We propose a novel machine learning (ML)
framework for this prediction task that uses fea-
tures extracted from both the influence graph (e.g.,
the connectivity of relevant concepts in the graph)
and the citation graph (e.g., the affinity between
related influence relations measured by member-
ship to communities in citation space).

(2) We evaluate the proposed method on an
influence graph extracted from over 100K pa-

2These relations were extracted using a grammar that
identifies causal statements in text. However, we prefer the
term “influence” to “causality” in this work because here we
simply rely on the text and do not demonstrate that these find-
ings are truly causal.

1



pers, which contains 1,564,748 concepts (e.g.,
“astrocytes”, “proinflammatory cytokines”) and
2,395,944 influence relations (e.g., “VEGF in-
creases Akt”). Our method obtains an F1 score of
27 points and a mean average precision of 68%.
This outperforms considerably methods that ex-
tract features only from one of the two graphs.

(3) To promote future work on this topic, we
release a dataset containing both the influence and
the citation graphs used in this paper, available at:
https://github.com/clulab/releases/tree/

master/textgraphs2018-discovery.

2 Data

As mentioned, the primary graph this method op-
erates on is a graph of influence relations ex-
tracted from a corpus of 119,667 PubMed Open
Access publications. These papers were previ-
ously selected to be relevant to the topic of chil-
dren’s health, which spans multiple domains, and
includes issues such as stunting, wasting, and mal-
nutrition.

All these papers were processed using the ma-
chine reading and assembly software of Hahn-
Powell et al. (2017). In order to address the multi-
domain nature of the children’s health, Hahn-
Powell et al. followed the OpenIE-style approach
of Banko et al. (2007) for entity extraction by con-
sidering expanded noun phrases as a coarse ap-
proximation of the concepts relevant to the topic.
For event extraction, the authors adapted a sub-
set of REACH grammars (Valenzuela-Escarcega
et al., 2017) from the biomolecular domain that
capture influence statements (e.g., positive and
negative regulations). The adaptation removed
selectional restrictions on the arguments of each
event predicate. That is, they extract any lexi-
calized variation of “A causes B” where A and
B are concepts identified in the entity extraction
step. For example, when processing the sentence
“Chronic infection may lead to malnutrition and
malabsorption”, the system extracts the following
entities: “Chronic infection”, “malnutrition”, and
“malabsorption”. In this particular case, the ex-
tracted entities participate in two promotes re-
lations: the first between “Chronic infection” and
“malnutrition”, and the second between “Chronic
infection” and “malabsorption”.

This machine reading approach was used to
read the entire content of these publications (in-
cluding abstract and body of paper). To reduce

Figure 1: Intuition of the backtesting framework: we predict
if links such as A→ C will be added to the influence graph
after time t, using information that exists before time t such
as A→B, and B→C. Setting t = 2012 yields 5,015 positive
examples, i.e., A→ C links that were added after 2012, and
274,251 negative examples, i.e., A → C links that were not
added to the graph between 2012 and the end of 2017.

noise we kept only relations extracted at least
twice, and which occur between concepts with
an inverse document frequency (IDF) larger than
1. The resulting influence graph (IG) contains
1,564,748 distinct nodes, connected by 2,395,944
influence relations.

Each match to these rules produces a directed
influence relation3 that encodes polarity (i.e., in-
crease or decrease). Finally, the relation instances
are then consolidated through a conservative dedu-
plication procedure.

Because the hypotheses studied in these publi-
cations are generally expressed using causal lan-
guage, we believe this influence graph (IG) cap-
tures the essence of the scientific knowledge in this
domain.

The above IG is accompanied by a citation
graph (CG), which contains outgoing citations
from the above papers, at a total of 5,523,759 cita-
tion links.

We model the discovery of important white
spaces in this knowledge base as link prediction:
we predict which influence links will be added to
the IG after time t, using only information avail-
able before time t. We believe this is a reason-
able approximation for the discovery of impor-
tant white spaces in science knowledge: influence
links that will be added in the future indicate that
somebody identified the missing information to be
important enough to be studied and published. To
limit search space, in this work we focus on the
prediction of A → C influence links, when A →
B and B → C exist in the graph before time t, for
at least one node B. Figure 1 visualizes this proce-
dure.

3It should be noted that this approach only reads such
statements from publications, and does not attempt to verify
these findings directly through separate modeling. In other
words, the authors statements are assumed to be correct.

2



t (year) positive negative

2017 - 998,586
2016 319 979,709
2015 1,706 881,689
2014 3,767 696,406
2013 5,208 481,671
2012 5,015 274,251
2011 3,741 151,460
2010 2,448 73,226
2009 1,521 36,839
2008 782 16,372
2007 444 7,843

Table 1: Total number of positive and negative examples for
different values of the threshold t.

Positive Negative
Examples Examples

Training 3,011 164,551
Development 1,002 54,850
Testing 1,002 54,850

Table 2: Number of positive and negative examples in the
training, development, and testing partitions.

Note that transitivity cannot be assumed to be
true in this graph due to the fact that influence
relations extracted from text usually oversimplify
complex causal processes. We show in Section 4
that relying on this transitivity assumption leads to
poor predictions.

We implemented the above task through back-
testing. That is, we look at an arbitrary point in
time in the past (t), and create positive training ex-
amples from A → C links that were added to IG af-
ter t. Similarly, we create negative examples from
A → C links that do not appear between time t and
the present. Note that these negative examples are
an approximation: some of these may correspond
to inventions that will be published at future dates
that are beyond the coverage of our dataset. In this
paper we used t = 2012. This choice of t is jus-
tified in Table 1, which shows the distribution of
positive and negative examples for different values
of t. We chose t = 2012 because this provided a
large enough number of positive examples, while
still maintaining a realistic distribution of negative
examples.

The dataset was split into train-
ing/development/testing as indicated in Table 2,
using a 60-20-20% split. During the partitioning,
we made sure that identical influence links coming
from different papers are all allocated to the same
partition.

3 Approach

We model link prediction as i.i.d. classification on
the above dataset, exploring multiple classifiers in
Section 4. One key contribution of this work is the
feature set used by these classifiers, which is sum-
marized in Table 3. At a higher level, these fea-
tures capture the connectivity of both the IG and
CG around a candidate link, under the assumption
that the more connected the corresponding graph
is around A and C, the more likely it is that the
link A → C will be discovered in the near fu-
ture. In particular, from the IG we extract the in-
and out-degrees of the source/destination nodes,
and statistics from the path(s) connecting the two
nodes such as the length of the shortest path con-
necting the source and destination nodes, or the
inverse document frequency (IDF) scores of the
nodes on these paths.

From the CG, we derive features based on
the probabilities that papers containing A → B
(pA→B) and B → C (pB→C) belong to the same
community/ies, motivated by the idea that discov-
eries are easier to be made if the individual frag-
ments that form the puzzle (A → B and B → C
here) come from the same or related discipline(s).
We model the probability that two papers, p1 and
p2, belong to the same community P (p1, p2) us-
ing two configurations of the Coda community de-
tection algorithm (Yang et al., 2014), one in which
detects 100 communities, and another where it de-
tects 300. Because influence links may be re-
ported in more than one paper, we derived the
max/min/avg P(pA→B ,pB→C) features, which are
computed across all possible combinations of pa-
pers pA→B and pB→C .

Lastly, we add a series of features (bottom
part of Table 3) extracted from the collection
of biomedical publications used in these experi-
ments, such as IDF scores of the relevant concept
nodes and the counts for the number of papers that
mention a given influence link.

4 Results

Table 4 lists the results of several classifiers on
the test partition,4 compared against two base-
lines. The first baseline randomly creates positive
links following the distribution of positive exam-
ples from the training partition. The second base-

4All classifier hyper parameters were tuned on the de-
velopment partition. All classification results were averaged
over 5 runs.

3



Feature Name Description

CA.outdegree Out-degree of source concept node A, i.e., number of influence relations starting on A

CA.indegree In-degree of source concept node A, i.e., number of influence relations ending on A

CC .outdegree Out-degree of destination concept node C

CC .indegree In-degree of destination concept node C

Cinbetween.outdegree Out-degree of nodes in all the shortest paths that connect A to C but do not pass through B

Cinbetween.indegree In-degree of nodes in all the shortest paths that connect A to C but do not pass through B

shortest path length The length of the shortest path that connects A to C but does not pass through B; 0 if nosuch path exists
shortest path count The number of shortest paths that connect A to C but do not pass through B

Cinbetween.avg-idf
Average inverse document frequency (IDF) of nodes in-between A and C in all the above
shortest paths

rinbetween.avg-seen Average number of papers containing an edge in the above shortest paths

max P(pA→B ,pB→C )
Maximum probability of papers pA→B and pB→C being related based on their membership
to multi-communities detected by the Coda algorithm; pr refers to any paper that contains
influence relation r.

min P(pA→B ,pB→C )
Minimum probability of papers pA→B and pB→C being related based on their membership
to multi-communities detected by the Coda algorithm

avg P(pA→B ,pB→C )
Average probability of papers pA→B and pB→C being related based on their membership
to multi-communities detected by the Coda algorithm

Jaccard(pA→B ,pB→C )
Jaccard similarity between the set of papers that contain the link A → B (pA→B) and the
set of papers that contain B → C (pB→C )

Inter-citation ratio The number of citations between the two sets pA→B and pB→C normalized by the size of
the union of the two sets.

CA.idf IDF of the lemmatized terms of source concept node A

CB .idf IDF of the lemmatized terms of intermediate concept node B

CC .idf IDF of the lemmatized terms of destination concept node C

r1.seen Number of papers that contain the influence relation A→ B
r2.seen Number of papers that contain the influence relation B→ C

Table 3: List of features used by the link prediction classifier that classifies the candidate link A → C, given an intermediate
node B. The top part of table lists the features derived from the influence graph. The middle part lists features extracted from
the citation graph. The bottom part contains features extracted from the collection of papers.

line assumes that all candidate links are positive,
i.e., candidate A → C is always correct if A → B
and B → C exist for some B.

This table yields several observations. First, the
performance of the first (random) baseline is very
low, indicating that this is indeed a hard task that is
exacerbated by the biased label distribution. Sec-
ond, the precision of the second baseline is also
very low, confirming that the transitive closure as-
sumption is not supported on this realistic influ-
ence graph. Third, all classifiers considerably out-
perform the baseline, indicating that capturing the
structure of the IG and CG is indeed indicative of
the likelihood that an influence link will be dis-
covered in the near future. Fourth, a linear support
vector machines (SVM) classifier did not converge
on this data, indicating that, while it is possible to
learn a model for this link prediction task, the re-
sulting model is more complex than a linear func-
tion. All in all, the best non-linear model (Ad-

F1 Precision Recall P@10 MAP

Baseline (random) 0.02 0.02 0.02 - -
Baseline (all positive) 0.035 0.018 1 - -

Neural Network 0.27 0.398 0.206 0.8 0.537
AdaBoost 0.27 0.536 0.178 0.9 0.681
Random Forest 0.23 0.244 0.216 0.5 0.309

Table 4: Unranked scores –precision, recall, and F1– and
ranked scores –precision at 10 (P@10), and mean average
precision (MAP)– of several classifiers for the prediction of
influence links using backtesting at time t = 2012. The base-
line predicts that every A → C link will be discovered after
time t, if A→ B and B → C exist before time t.

aBoost) obtained an F1 score of 0.27, an order of
magnitude higher than the baseline, and a mean
average precision (MAP) of 0.68, indicating that
most correct predictions are ranked closer to the
top.

Table 5 shows the results of an ablation ex-
periment in which we measured the drop in per-
formance when each feature was individually re-
moved from the full AdaBoost model. This exper-
iment indicates that, importantly, both the influ-

4



Removed Feature F1 Precision Recall

Full model 0.268 0.528 0.176

− CA.outdegree 0.234 0.678 0.14
− CA.indegree 0.246 0.44 0.17
− CC .outdegree 0.214 0.248 0.186
− CC .indegree 0.2 0.232 0.172
− Cinbetween.outdegree 0.22 0.272 0.182
− Cinbetween.indegree 0.234 0.3 0.192
− Cinbetween.avg-idf 0.214 0.272 0.18
− rinbetween.avg-seen 0.23 0.29 0.192
− shortest path count 0.222 0.29 0.182
− shortest path length 0.204 0.28 0.16
− max P(pA→B ,pB→C ) (c=100) 0.226 0.3 0.18
− min P(pA→B ,pB→C ) (c=100) 0.228 0.302 0.184
− avg P(pA→B ,pB→C ) (c=100) 0.232 0.306 0.186
− max P(pA→B ,pB→C ) (c=300) 0.232 0.314 0.182
− min P(pA→B ,pB→C ) (c=300) 0.23 0.298 0.18
− avg P(pA→B ,pB→C ) (c=300) 0.232 0.326 0.182
− Jaccard(pA→B ,pB→C ) 0.226 0.29 0.184
− Inter-citation ratio 0.23 0.318 0.18
− CA.idf 0.248 0.478 0.168
− CB .idf 0.216 0.258 0.19
− CC .idf 0.22 0.256 0.194
− r1.seen 0.226 0.284 0.19
− r2.seen 0.228 0.298 0.184

Table 5: Ablation experiment, which removed one feature
at a time from the full AdaBoost model. This experiment was
performed on the development partition.

ence and citation graphs contribute to the overall
performance. Removing individual features from
either group impacts performance. Several fea-
tures have a high impact, including Cinbetween.avg-
idf, shortest path length, which are extracted from
the influence graph, and max P(pA→B ,pB→C) and
Jaccard(pA→B ,pB→C), which are extracted from
the citation graph. These results demonstrate that
the task of scientific discovery requires a multi-
faceted approach that analyzes several graphs, in-
cluding graphs that model the content of publica-
tions (our IG), as well as citation graphs.

Lastly, we rank the discoveries made by the
proposed approach using the NN model, using a
scoring function that combines the classifier con-
fidence and redundancy (i.e., how many times we
saw A → C with different intermediate nodes B)
using a Noisy-Or formula:

Score(A→ C) = (1−
∏

B

(1− prob(A→ C|B)))

where B loops overall intermediate nodes that
support the predicted relation, and prob(A →
C|B) is the classifier’s output probability given
one intermediate node B.5 Table 6 lists the top
10 prediction of our approach under this scoring
function. 80% of these predictions are correct (i.e.,
they are discovered after time t = 2012). Addi-
tionally, the table shows that the predictions are

5This ranking function was also used to generate the
ranked evaluation in Table 4.

Predicted discovery Score Gold label
antibodies→ apoptosis 1 1
apoptosis→ ROS 1 1
TGF-beta→ apoptosis 1 1
TLR→ cascade 1 1
apoptosis→ insulin 1 0
apoptosis→ enzymes 1 0
antibodies→ receptor 1 1
IL-6→ tumor 1 1
mutations→ inflammation 0.999 1
macrophages→ tumor 0.999 1

Table 6: Top 10 predicted links by the neural network model,
sorted in descending order of their informativeness score.

indeed informative: they capture fragments of pro-
tein signaling pathways, and links to biological
processes (e.g., apoptosis). A few predicted links
such as “TLR → cascade” are not informative, but
this could be attributed to limitations in the ma-
chine reader, which failed to capture meaningful
content from the destination concept (“cascade”).

5 Conclusion

We proposed a novel strategy for the identification
of white spaces in scientific knowledge, which are
topics that are insufficiently studied and may hide
important scientific discoveries. We addressed this
task with a link prediction method that operates
over two graphs: a graph of influence relations that
were automatically extracted from over 100K pa-
pers on children’s health using a machine reading
tool, and which summarize the scientific knowl-
edge in this domain, and a graph of citations orig-
inating from these papers. Using a backtesting
methodology, we showed that our method is capa-
ble of predicting which influence links will be dis-
covered in the future with a F1 score of 27 points,
and a mean average precision of 68%. An ablation
analysis experiment demonstrated that features ex-
tracted from both graphs contribute to overall per-
formance. We believe this work is relevant to
many actors involved in scientific discovery in-
cluding researchers and program managers.

Acknowledgements

Marco Valenzuela-Escárcega, Gus Hahn-Powell,
and Mihai Surdeanu declare a financial interest
in lum.ai. This interest has been properly dis-
closed to the University of Arizona Institutional
Review Committee and is managed in accordance
with its conflict of interest policies. This work was
funded by the Bill and Melinda Gates Foundation
HBGDki Initiative.

5



References
Michele Banko, Michael J Cafarella, Stephen Soder-

land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In
IJCAI, volume 7, pages 2670–2676.

Delroy Huborn Cameron. 2014. A context-driven
subgraph model for literature-based discovery.
Ph.D. thesis, Wright State University.

Gus Hahn-Powell, Marco A Valenzuela-Escarcega,
and Mihai Surdeanu. 2017. Swanson linking
revisited: Accelerating literature-based discovery
across domains using a conceptual influence graph.
Proceedings of ACL 2017, System Demonstrations,
pages 103–108.

Jure Leskovec, Daniel Huttenlocher, and Jon Klein-
berg. 2010. Predicting positive and negative links in
online social networks. In Proceedings of the 19th
international conference on World wide web, pages
641–650. ACM.

David Liben-Nowell and Jon Kleinberg. 2007. The
link-prediction problem for social networks. journal
of the Association for Information Science and
Technology, 58(7):1019–1031.

Hoifung Poon, Kristina Toutanova, and Chris Quirk.
2015. Distant supervision for cancer pathway ex-
traction from text.

Yakub Sebastian, Eu-Gene Siew, and Sylvester O Ori-
maye. 2017. Emerging approaches in literature-
based discovery: techniques and performance re-
view. The Knowledge Engineering Review, 32.

Don R Swanson. 1986. Undiscovered public knowl-
edge. The Library Quarterly, 56(2):103–118.

Marco A. Valenzuela-Escarcega, Ozgun Babur, Gus
Hahn-Powell, Dane Bell, Thomas Hicks, Enrique
Noriega-Atala, Xia Wang, Mihai Surdeanu, Emek
Demir, and Clayton T. Morrison. 2017. Large-scale
automated reading with Reach discovers new can-
cer driving mechanisms. In Proceedings of the Sixth
BioCreative Challenge Evaluation Workshop, pages
201–203.

Jaewon Yang, Julian McAuley, and Jure Leskovec.
2014. Detecting cohesive and 2-mode communities
indirected and undirected networks. In Proceedings
of the 7th ACM international conference on Web
search and data mining, pages 323–332. ACM.

6


