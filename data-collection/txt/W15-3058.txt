



















































Dependency Analysis of Scrambled References for Better Evaluation of Japanese Translation


Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 450–456,
Lisboa, Portugal, 17-18 September 2015. c©2015 Association for Computational Linguistics.

Dependency Analysis of Scrambled References
for Better Evaluation of Japanese Translations

Hideki Isozaki and Natsume Kouchi∗
Okayama Prefectural University

111 Kuboki, Soja-shi, Okayama, 719-1197, Japan

isozaki@cse.oka-pu.ac.jp

Abstract
In English-to-Japanese translation,
BLEU (Papineni et al., 2002), the de facto
standard evaluation metric for machine
translation (MT), has very weak corre-
lation with human judgments (Goto et
al., 2011; Goto et al., 2013). Therefore,
RIBES (Isozaki et al., 2010; Hirao et
al., 2014) was proposed. RIBES mea-
sures similarity of the word order of a
machine-translated sentence and that of a
corresponding human-translated reference
sentence.
RIBES has much stronger correlation than
BLEU but most Japanese sentences have
alternative word orders (scrambling), and
one reference sentence is not sufficient for
fair evaluation. Isozaki et al. (2014) pro-
posed a solution to this problem. This
solution generates semantically equiva-
lent word orders of reference sentences.
Automatically generated word orders are
sometimes incomprehensible or mislead-
ing, and they introduced a heuristic rule
that filters out such bad sentences. How-
ever, their rule is too conservative and
generated alternative word orders for only
30% of reference sentences.
In this paper, we present a rule-free
method that uses a dependency parser to
check scrambled sentences and generated
alternatives for 80% of sentences. The ex-
perimental results show that our method
improves sentence-level correlation with
human judgments. In addition, strong
system-level correlation of single refer-
ence RIBES is not damaged very much.
We expect this method can be applied to
other languages such as German, Korean,
∗This work was done while the second author was a grad-

uate student of Okayama Prefectural University.

Spearman’s ρ with adequacy

NTCIR-7 JE RIBES
JE BLEU

NTCIR-9 JE RIBES
JE BLEU
EJ RIBES
EJ BLEU

NTCIR-10 JE RIBES
JE BLEU
EJ RIBES
EJ BLEU

0.0 0.2 0.4 0.6 0.8 1.0

Figure 1: RIBES has better correlation with ade-
quacy than BLEU (system-level correlation)

Turkish, Hindi, etc.

1 Introduction

For translation among European languages,
BLEU (Papineni et al., 2002) has strong cor-
relation with human judgments and almost all
MT papers use BLEU for evaluation of trans-
lation quality. However, BLEU has very weak
correlation with human judgments in English-to-
Japanese/Japanese-to-English translation, and a
new metric RIBES (Isozaki et al., 2010; Hirao
et al., 2014) has strong correlation with human
judgments. RIBES measures similarity of the
word order of a machine translated sentence and
that of a human-translated reference sentence.
Figure 1 compares RIBES and BLEU in terms of
Spearman’s ρ with human judgments of adequacy
based on NTCIR-7/9/10 data (Isozaki et al., 2010;
Goto et al., 2011; Goto et al., 2013).

Japanese and English have completely different
word order, and phrase-based SMT systems tend
to output bad word orders. RIBES correctly points
out their word order problems.

In this paper, we propose a method to improve
“sentence-level correlation”, which is useful for
MT developers to find problems of their MT sys-
tems. If the sentence-level correlation is strong,
low RIBES scores indicate bad translations, and
we will find typical failure patterns from them.

450



katta

John ga Tokyo de PC wo

(a) “John ga Tokyo de PC wo katta”

katta

John gaTokyo dePC wo

(b) “PC wo Tokyo de John ga katta”

atta

ato ni Alice kara denwa ga

katta

John ga PC wo

(c) “John ga PC wo katta ato ni Alice kara denwa
ga atta”

atta

ato ni denwa ga

katta

Alice kara John ga PC wo

(d) “Alice kara John ga PC wo katta ato ni denwa
ga atta”

Figure 2: Dependency trees

However, improvement of sentence-level corre-
lation is more difficult than system-level correla-
tion and current automatic evaluation metrics do
not have strong correlation. (Leusch et al., 2003;
Stanojević and Sima’an, 2014; Echizen-ya and
Araki, 2010; Callison-Burch et al., 2012)

1.1 Scrambling
As for Japanese translation, however, we should
consider “scrambling” or acceptable reordering of
phrases. For example, “John ga Tokyo de PC wo
katta” (John bought a PC in Tokyo) consists of the
main verb “katta” (bought) and its modifiers. “Ga”,
“de”, and “wo” are case markers.

• “Ga” is a nominative case marker.
• “De” is a locative case marker.
• “Wo” is an accusative case marker.
This sentence can be reordered as follows.
1. John ga Tokyo de PC wo katta . (1.00)
2. John ga PC wo Tokyo de katta . (0.86)
3. Tokyo de John ga PC wo katta . (0.86)

4. Tokyo de PC wo John ga katta . (0.71)
5. PC wo John ga Tokyo de katta . (0.71)
6. PC wo Tokyo de John ga katta . (0.57)

All of the above sentences are acceptable and
have the same meaning, and this is called “scram-
bling”. However, RIBES outputs different scores
for these sentences. When we use the first one as
the reference sentence, RIBES output scores in the
parentheses. Human judges will give almost equal
scores to all of them, and we should improve these
RIBES scores for better evaluation.

Scrambling is also observed in other lan-
guages such as German (Maier et al., 2014), Ko-
rean (Chun, 2013), Turkish (ldız et al., 2014),
Hindi (Sharma and Paul, 2014), etc.

Figure 2 (a) shows the dependency tree of “John
ga Tokyo de PC wo katta”. Each box indicates a bun-
setsu (chunk). Arrows indicate modification rela-
tions. The source node of an arrow modifies the
target node of the arrow. The root “katta” has three
modifiers (children), “John ga”, “Tokyo de”, and
“PC wo”. We can generate 3! = 6 word orders by
post-order traversal of this tree because the order
of siblings does not matter. Figure 2 (b) shows a
permutation and its dependency tree. In this case,
all permutations are acceptable.

However, more complex dependency trees tend
to generate misleading/incomprehensible sen-
tences. Figure 2 (c) shows such a sentence: “John
ga PC wo katta ato ni Alice kara denwa ga atta”. (Af-
ter John bought a PC, there was a phone call from
Alice). “X ato ni Y” means “After X, Y”. “Denwa”
means “a phone call”. “Atta” means “there was”.

This tree has 2! × 3! = 12 post-order permuta-
tions. Some of them are misleading. For exam-
ple, “Alice kara John ga PC wo katta ato ni denwa ga
atta” sounds like “After John bought a PC from
Alice, there was a phone call” because “Alice kara”
(from Alice) precedes “katta” (bought). This sen-
tence will have a dependency tree in Figure 2 (d).

1.2 Rule-based filtering of bad sentences
Isozaki et al. (2014) tried to solve the above prob-
lem by automatic generation of reordered sen-
tences and use of a heuristic rule (constraint) to
filter out bad sentences.

• Use a Japanese dependency parser to get de-
pendency trees of reference sentences.
• Check the dependency trees and manually

correct wrong ones because sentence-level
accuracy of dependency analyzers are still

451



low.
• In order to get Japanese-like head final sen-

tences, output words in the corrected depen-
dency tree in post-order. That is, recursively
output all child nodes before a mother node.
They called this method “postOrder”.
• The above “postOrder” generates misleading/

incomprehensible sentences. In order to in-
hibit them, they introduced the following rule
called “Simple Case Marker Constraint”:

If a reordered sentence has a case
marker phrase of a verb that pre-
cedes another verb before the verb,
the sentence is rejected. “wo” case
markers can precede adjectives be-
fore the verb.

Here, we call this “rule2014”.

This “rule2014” improved sentence-level corre-
lation of NTCIR-7 EJ data. However, rule2014 is
so conservative that only 30% of reference sen-
tences obtained alternative word orders. In the
next section, we present a method that covers more
reference sentences.

2 Methodology

2.1 Our idea
We do not want to introduce more rules to cover
more sentences. Instead we present a rule-free
method. Our idea is simple: if a reordered sen-
tence is misleading or incomprehensible, a depen-
dency parser will output a dependency tree differ-
ent from the original dependency tree. That is, use
a dependency parser for detecting misleading sen-
tences.

We apply a dependency parser to the reordered
reference sentences. If the dependency parser out-
puts the same dependency tree with the original
reference sentence except sibling orders, accept
the word order as a new reference. Otherwise, it is
a misleading word order and reject it. (We do not
parse MT output because it is often broken and de-
pendency analysis will fail.)

For example, “PC wo Tokyo de John ga katta” has
the dependency tree in Figure 2 (b). This tree is
the same as (a) except the order of three siblings.
We don’t care about the order of siblings, and ac-
cept this as a new reference sentence. On the other
hand, the parser shows that “Alice kara John ga PC
wo katta ato ni denwa ga atta” has the dependency
tree in (d), which is different from (c) and we

reject this sentence. We call this method “com-
pDep” because it compares dependency trees of
reordered reference sentences with the original de-
pendency tree.

Each MT output sentence is evaluated by the
best of RIBES scores for remaining reordered ref-
erence sentences. This is a sentence-level score. A
system’s score (system-level score) is the average
of sentence-level scores of all test sentences.

2.2 Data and tools
We use NTCIR-7 PatentMT EJ data (Fujii et al.,
2008) and NTCIR-9 PatentMT EJ data (Goto et
al., 2011).1 NTCIR-7 EJ human judgment data
consists of 100 sentences × five MT systems.
NTCIR-9 EJ human judgment data consists of 300
sentences × 17 MT systems. NTCIR provided
only one reference sentence for each sentence.
When we use only the provided reference sen-
tences, we call it “single ref”.

We apply a popular Japanese dependency parser
CaboCha2 to the reference sentences, and man-
ually corrected its output just like Isozaki et al.
(2014). 40% of NTCIR-7 dependency trees and
50% of NTCIR-9 dependency trees were cor-
rected.

Based on the corrected dependency trees, we
generate all post-order permutations. Then we ap-
ply CaboCha to these reordered sentences. We
compare the dependency tree of the original ref-
erence sentence with that of a reordered reference
sentence.

We accept a reordered reference sentence only
when its tree is the same as that of the original
reference sentence except the sibling order.

This tree comparison is implemented by remov-
ing word IDs and chunk IDs from the trees keep-
ing their dependency structures and sorting chil-
dren of each node by their surface strings. These
sorted dependency trees are compared recursively
from their roots.

3 Experimental Results

Table 1 shows that our compDep method succeeded
in generating more reordered sentences (permuta-
tions) than rule2014. The column with #perms =
1 indicates failure of generation of reordered sen-
tences. As for NTCIR-7, rule2014 failed for 70%

1NTCIR-8 did not provide human judgments. NTCIR-10
submission data was not publicly available yet at the time of
writing this paper.

2http://code.google.com/p/cabocha/

452



NTCIR-7 EJ
#perms 1 2–10 11–100 101–1000 >1000 total
single ref 100 0 0 0 0 100
rule2014 70 30 0 0 0 100
compDep 20 61 15 4 0 100
postOrder 1 41 41 13 4 100

NTCIR-9 EJ
#perms 1 2–10 11–100 101–1000 >1000 total
single ref 300 0 0 0 0 300
rule2014 267 25 7 1 0 300
compDep 41 189 63 5 2 300
postOrder 0 100 124 58 18 300

Table 1: Distribution of the number of generated
permutations (#perms=1 indicates the number of
sentences for which the method didn’t generate al-
ternative word orders)

tsbmt
moses

NTT
NICT-ATR

kuro

Spearman’s ρ with adequacy

single ref
compDep
rule2014

0.0 0.2 0.4 0.6 0.8 1.0

Figure 3: Improvement of sentence-level correla-
tion with adequacy (NTCIR-7 EJ)

of reference sentences while compDep failed for
only 20%. As for NTCIR-9, rule2014 failed for
89% (267/300) while compDep failed for only 14%
(41/300).

From the viewpoint of the number of such fail-
ures, postOrder (§1.2) is the best method, but pos-
tOrder does not filter out bad sentences, and it leads
to the loss of system-level correlation with ade-
quacy (See §3.2).
3.1 Sentence-level correlation
Here, we focus on adequacy because it is easy
to generate fluent sentences if we disregard ade-
quacy. Figure 3 shows NTCIR-7 EJ results. our
compDep succeeded in improving sentence-level
correlation with adequacy for four MT systems
among five. The average of ρ was improved from
single ref’s 0.558 to 0.606.

Figure 4 shows NTCIR-9 EJ results. our com-
pDep succeeded in improving sentence-level cor-
relation of all 17 MT systems. The average of ρ
was improved from single ref’s 0.385 and rule2014’s
0.396 to compDep’s 0.420. The improvement from
single ref to compDep is statistically significant with
p = 0.000015 (two-sided sign test) for NTCIR-9
data. The improvement from rule2014 to compDep
is also statistically significant with p = 0.01273.

Spearman’s ρ with adequacy

NTT-UT-1
NTT-UT-3

RBMT6
JAPIO

RBMT4
RBMT5

ONLINE1
BASELINE1

TORI
BASELINE2

KLE
FRDC

ICT
UOTTS

KYOTO-2
KYOTO-1

BJTUX

single ref
compDep
rule2014

0.0 0.2 0.4 0.6 0.8 1.0

Figure 4: Improvement of sentence-level correla-
tion with adequacy (NTCIR-9 EJ)

3.2 System-level correlation
Isozaki et al. (2014) pointed out that postOrder
loses system-level correlation with adequacy be-
cause it also generates bad word orders.

Figure 5 shows that system-level correlation of
compDep is comparable to that of single ref and
rule2014. Spearman’s ρ of compDep in NTCIR-
7 (0.90) looks slightly worse than single ref and
rule2014 (1.00). However, this is not a big prob-
lem because the NTCIR-7 correlation is based on
only five systems as described in §2.2, and the
NTCIR-9 correlation based on 17 systems did not
degrade very much (compDep: 0.690, single ref:
0.695, rule2014: 0.668).

Table 2 shows details of system-level correla-
tion of NTCIR-7 EJ. Single reference RIBES and
rule2014 completely follows the order of adequacy.
On the other hand, compDep slightly violates this
order at the bottom of the table. NICT-ATR and
kuro is swapped.

The “single ref” and “rule2014” scores of this ta-
ble are slightly different from that of Table 5 of
Isozaki et al. (2014). This difference is caused
by the difference of normalization of punctuation
symbols and full-width/half-width alphanumeric
letters.

Figure 6 shows that the effects of manual cor-
rection of dependency trees. The average of sin-

453



Pearson with adequacy

NTCIR-7 single ref
compDep

rule2014
postOrder

NTCIR-9 single ref
compDep

rule2014
postOrder

0.0 0.2 0.4 0.6 0.8 1.0

Spearman’s ρ with adequacy

NTCIR-7 single ref
rule2014

compDep
postOrder

NTCIR-9 single ref
rule2014

compDep
postOrder

0.0 0.2 0.4 0.6 0.8 1.0

Figure 5: System-level correlation with adequacy
Adequacy Averaged RIBES

single ref rule2014 compDep
tsbmt 3.527 0.722 0.726 0.750
Moses 2.897 0.707 0.720 0.745
NTT 2.740 0.670 0.682 0.722
NICT-ATR 2.587 0.658 0.667 0.706
kuro 2.420 0.633 0.643 0.711

Table 2: Details of system-level RIBES scores
(NTCIR-7 EJ)

gle ref, compDep, and compDep without correction are
0.388, 0.422, and 0.420, respectively. Thus, the
difference between compDep (with correction) and
compDep without correction is very small and we can
skip the manual correction step.

We used dependency analysis twice in the above
method. First, we used it for generation of re-
ordered reference sentences. Second, we used it
for detecting misleading word orders.

In the first usage, we manually corrected depen-
dency trees of the given reference sentences. In the
second usage, however, we did not correct depen-
dency trees of reordered reference sentences be-
cause some sentences have thousands of permuta-
tions (Table 1) and it is time-consuming to cor-
rect all of them manually. Moreover, some re-
ordered sentences are meaningless or incompre-
hensible, and we cannot make their correct depen-
dency trees. Therefore, we did not correct them.
Our experimental results have shown that we can
omit correction in the first step.

4 Related Work

Our method uses syntactic information. Use of
syntactic information in MT evaluation is not a

Spearman’s ρ with adequacy

NTT-UT-1
NTT-UT-3

RBMT6
JAPIO

RBMT4
RBMT5

ONLINE1
BASELINE1

TORI
BASELINE2

KLE
FRDC

ICT
UOTTS

KYOTO-2
KYOTO-1

BJTUX

single ref
compDep
compDep w/o

correction

0.0 0.2 0.4 0.6 0.8 1.0

Figure 6: Effects of manual correction on comp-
Dep’s correlation with adequacy (NTCIR-9 EJ)

new idea.
Liu and Gildea (2005) compared parse trees

of reference sentences and MT output sentences.
They proposed four methods: STM, TKM,
HWCM, DSTM, and DTKM. STM measures sim-
ilarity by the number of matching subtrees. TKM
uses Tree Kernel for the measurement. HWCM
uses n-gram matches in dependency trees. DSTM
and DTKM are dependency tree versions of STM
and TKM respectively.

Owczarzak et al. (2007) used LFG-based typed
dependency trees. They also introduced process-
ing of paraphrases.

Chan and Ng (2008) proposed MAXSIM that is
based on a bipartite graph matching algorithm and
assigns different weights to matches. Dependency
relation are used as a factor in this framework.

Zhu et al. (2010) proposed an SVM-based MT
metric that uses different features in different gran-
ularities. Dependency relations are used as a fea-
ture in this framework.

We designed our method not to parse MT out-
puts because some MT outputs are broken and it is
difficult to parse them. Our method does not parse
MT outputs and we expect our method is more ro-
bust than these methods.

Recently, Yu et al. (2014) proposed RED, an
evaluation metric based on reference dependency
trees. They also avoided parsing of “results of

454



noisy machine translations” and used only depen-
dency trees of reference sentences. However, their
research motivation is completely different from
ours. They did not mention scrambling at all,
and they did not try to generate reordered ref-
erence sentences, but it is closely related to our
method. It might be possible to make a better eval-
uation method by combining our method and their
method.

Some readers might think that adequacy is not
very reliable. WMT-2008 (Callison-Burch et al.,
2008) gave up using adequacy as a human judge-
ment score because of unreliability. NTCIR or-
ganizers used relative comparison to improve re-
liability of adequacy. The details are described in
Appendix A of Goto et al. (2011).

5 Conclusions

RIBES (Isozaki et al., 2010) is a new evaluation
metric of translation quality for distant language
pairs. It compares the word order of an MT output
sentence with that of a corresponding reference
sentence. However, most Japanese sentences can
be reordered and a single reference sentence is not
sufficient for fair evaluation. Isozaki et al. (2014)
proposed a rule-based method for this problem but
it succeeded in generating alternative word orders
for only 11–30% of reference sentences.

In this paper, we proposed a method that uses a
dependency parser to detect misleading reordered
sentences. Only when a reordered sentence has
the same dependency tree with its original refer-
ence sentence except the order of siblings, we ac-
cept the reordered sentence as a new reference sen-
tence. This method succeeded in generating al-
ternative word orders for 80–89% and improved
sentence-level correlation of RIBES with ade-
quacy and its system-level correlation is compa-
rable to the single reference RIBES.

In conventional MT evaluations, we have to pre-
pare multiple references for better evaluation. This
paper showed that we can automatically generate
multiple references without much effort.

Future work includes use of the generated refer-
ence sentences in other metrics such as BLUE. We
expect that this method is applicable to other lan-
guages such as German, Korean, Turkish, Hindi,
etc. because they have scrambling.

References
Chris Callison-Burch, Cameron Fordyce, Philipp

Koehn, Christof Monz, and Josh Schroeder. 2008.
Futther meta-evaluation of machine translation. In
Proc. of the Workshop on Statistical Machine Trans-
lation, pages 70–106.

Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proc. of the Workshop on Sta-
tistical Machine Translation, pages 10–51.

Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A
maximum similarity metric for machine translation
evaluation. In Proc. of the Annual Meeting of the As-
sociation of Computational Linguistics (ACL), pages
55–62.

Jihye Chun. 2013. Verb Cluster, Non-Projectivity,
and Syntax-Topology Interface in Korean. In Proc.
of the Second International Conference on Depen-
dency Linguistics, pages 51–59.

Hiroshi Echizen-ya and Kenji Araki. 2010. Auto-
matic evaluation method for machine translation us-
ing noun-phrase chunking. In Proc. of the Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 108–117.

Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Work-
ing Notes of the NTCIR Workshop Meeting (NTCIR),
pages 389–400.

Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Working Notes of the NTCIR Workshop Meeting
(NTCIR).

Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K. Tsou. 2013. Overview of the patent
machine translation task at the NTCIR-10 work-
shop. In Working Notes of the NTCIR Workshop
Meeting (NTCIR).

Tsutomu Hirao, Hideki Isozaki, Katsuhito Sudoh,
Kevin Duh, Hajime Tsukada, and Masaaki Nagata.
2014. Evaluating translation quality with word or-
der correlations (in Japanese). Journal of Natural
Language Processing, 21(3):421–444.

Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
evaluation of translation quality for distant lan-
guage pairs. In Proc. of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 944–952.

Hideki Isozaki, Natsume Kouchi, and Tsutomu Hirao.
2014. Dependency-based automatic enumeration of
semantically equivalent word orders for evaluating
Japanese translations. In Proc. of the Workshop on
Statistical Machine Translation, pages 287–292.

Olcay Taner Yıldız, Ercan Solak, Onur Görg̈ , and Ra-
zieh Ehsani. 2014. Constructing a Turkish-English
Parallel TreeBank. In Proc. of the Annual Meet-
ing of the Association of Computational Linguistics
(ACL), pages 112–117.

Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2003. A novel string-to-string distance measure

455



with applications to machine translation evaluation.
In Machine Translation Summit, pages 240–247.

Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation,
pages 25–32.

Wolfgang Maier, Miriam Kaeshammer, Peter Bau-
mann, and Sandra Kubler. 2014. Discosuite - A
parser test suite for German discontinuous struc-
tures. In Proc. of the Language Resources and Eval-
uation Conference (LREC), pages 2905–2912.

Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Dependency-based automatic evalu-
ation for machine translation. In Proceedings of
SSST, NAACL-HLT 2007 / AMTA Workshop on Syn-
tax and Structure in Statistical Translation,, pages
80–87.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 311–318.

Rahul Sharma and Soma Paul. 2014. A hybrid ap-
proach for automatic clause boundary identification
in hindi. In Proc. of the 5th Workshop on South and
Southeast Asian NLP, pages 43–49.

Miloš Stanojević and Khalil Sima’an. 2014. Fit-
ting sentence level translation evaluation with many
dense features. In Proc. of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 202–206.

Hui Yu, Xiaofeng Wu, Jun Xie Wenbin Jiang, Qun Liu,
and Shouxun Lin. 2014. RED: A Reference De-
pendency Based MT Evaluation Metric. In Proc. of
the International Conference on Computational Lin-
guistics (COLING), pages 2042–2051.

Junguo Zhu, Muyun Yang, Bo Wang, Sheng Li, and
Tiejun Zhao. 2010. All in Strings: a Power-
ful String-based Automatic MT Evaluation Metric
with Multiple Granularities. In Proc. of the Inter-
national Conference on Computational Linguistics
(COLING), pages 1533–1540.

456


