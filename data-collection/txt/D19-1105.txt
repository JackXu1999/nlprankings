



















































Similarity Based Auxiliary Classifier for Named Entity Recognition


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1140–1149,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1140

Similarity Based Auxiliary Classifier for Named Entity Recognition

Shiyuan Xiao1,2, Yuanxin Ouyang1,2, Wenge Rong1,2,
Jianxin Yang1,2, Zhang Xiong1,2

1Engineering Research Center of Advanced Computer Application Technology,
Ministry of Education, Beihang University, Beijing, China

2School of Computer Science and Engineering, Beihang University, Beijing, China
{xiaoshiyuan, oyyx, w.rong, yjx17, xiongz}@buaa.edu.cn

Abstract

The segmentation problem is one of the fun-
damental challenges associated with name en-
tity recognition (NER) tasks that aim to reduce
the boundary error when detecting a sequence
of entity words. A considerable number of
advanced approaches have been proposed and
most of them exhibit performance deteriora-
tion when entities become longer. Inspired
by previous work in which a multi-task strat-
egy is used to solve segmentation problems,
we design a similarity based auxiliary classi-
fier (SAC), which can distinguish entity words
from non-entity words. Unlike conventional
classifiers, SAC uses vectors to indicate tags.
Therefore, SAC can calculate the similarities
between words and tags, and then compute a
weighted sum of the tag vectors, which can
be considered a useful feature for NER tasks.
Empirical results are used to verify the ratio-
nality of the SAC structure and demonstrate
the SAC model’s potential in performance im-
provement against our baseline approaches.

1 Introduction

Named entity recognition (NER) focuses on ex-
tracting a specific sequence and then classifying
the sequence as a predefined category. As a funda-
mental and important task in natural language un-
derstanding and information extraction, NER can
be considered a sequence labelling task, which
includes part-of-speech (POS) tagging, chunk-
ing, and semantic role labelling (SRL) (Collobert
et al., 2011). Owing to the growing popularity
of deep learning techniques, a considerable num-
ber of neural network-based approaches and tradi-
tional conditional random fields (CRF) have been
proposed and are widely used in many sequence
labelling tasks to obtain magnificent results (Ye
and Ling, 2018; Wu et al., 2018; Ghaddar and
Langlais, 2018).

2019/3/3 example1.xml

1/1

State Street Bank and CompanyTrust

Organization 

Organization? Organization? 

Figure 1: The segmentation problem in NER. “State
Street Bank and Trust Company” is an organization en-
tity with six words, it would be unsatisfactory if we
consider “State Street Bank” and “Trust Company” to
be two entities

To enhance NER’s performance, significant ef-
fort has been devoted in this area, and one direc-
tion is to employ useful information at the input
end. This mechanism can be realized by includ-
ing hand-crafted features (Collobert et al., 2011;
Passos et al., 2014; Huang et al., 2015). To even-
tually reduce heavy feature engineering, some re-
searchers have argued for the exclusion of task-
specific features and for the automatic mining of
character level features and word level features
(Ma and Hovy, 2016; Liu et al., 2018). Such
end-to-end models can potentially improve perfor-
mance with the exclusion of hand-crafted features.

While character and word level features demon-
strate promising results, Ye and Ling (2018) deter-
mined that such methods (i.e. LM-BiLSTM-CRF
(Liu et al., 2018)) will be relatively less effec-
tive for longer entity recognition. As indicated
by Zhuo et al. (2016), NER is a segment-level
task, and such NER models should be able to ex-
tract segment information while word-level labels
might tend to learn more word level information
than segment level information. Such limitations
can manifest as entity boundary errors, referred to
as the segmentation problem. This problem is il-
lustrated in Figure 1.

There have been many attempts to address the
segmentation problem. Some researchers em-
ployed segment level information, e.g., Semi-CRF



1141

(SCRF) (Zhuo et al., 2016), to address this chal-
lenge. Though SCRF exhibits promising results in
the case of longer entity recognition, it requires ad-
ditional feature extractors and exhibits some limi-
tations in the case of shorter entity recognition. Ye
and Ling (2018) proposed using CRF jointly with
SCRF to address this problem. On the other hand,
researchers have also adopted a multi-task strat-
egy to predict boundaries. For example, Stratos
(2016) divides NER into a two-step task in which
the boundary is first predicted and then the entity
type of the boundary is determined. Though this
method is easy to implement, it relies heavily on
the accuracy of the first step. Another alternative
thought is to employ auxiliary tasks to address this
problem. For example, Aguilar et al. (2017) cre-
ated two auxiliary tasks to help determine entity
boundaries and types. However, the authors con-
sidered the auxiliary tasks to be supervisors, which
means that the outputs of the auxiliary tasks are
not fully utilized.

In this study, inspired by the research indi-
cated previously, we design an auxiliary classifi-
cation task that aims to distinguish entity words
from non-entity words before executing the main
NER task. Unlike prior studies, the outputs of
the auxiliary task do not directly determine the
entity boundaries but rather serve as features for
assisting the main task. Therefore, the auxiliary
task is more like an “advisor” rather than supervi-
sor to the entire model. Our auxiliary classifier
is different from conventional classifiers, which
compute the probability of an entity word us-
ing the softmax function. Instead, we initialize
two trainable vectors for the classifier to identify
“True” and “False” tags, and then we determine
the similarity scores between each word and the
two tag vectors to indicate the probability whether
the word is an entity word. The hypothesis is
that tag vectors can serve as helpful features that
are shared by a majority of entity words and non-
entity words in NER tasks. Finally, we obtain
the weighted sum of the tag vectors by using the
similarity scores. In this study, this process is
referred to as a similarity based auxiliary classi-
fier (SAC).1 The empirical results verify the ra-
tionality of SAC’s structure and demonstrate that
the SAC model can reduce segmentation problems
and exhibit good performance in recognizing enti-

1code available at https://github.com/XiaoShiyuan/NCRF-
SAC

ties of different lengths.
The contributions of this paper are as follows:

1) We introduce a similarity based auxiliary clas-
sifier (SAC), which allows the model to determine
whether a word is an entity word before predict-
ing its entity label to overcome the segmentation
problem. 2) By analyzing the experimental results,
we verify the rationality of SAC’s structure and
demonstrate the advantages of using the weighted
sum of tag vectors compared to a list of scores de-
noting classification results. 3) Without using any
hand-crafted features, NeuralCRF+SAC outper-
forms existing state-of-the-art end-to-end models
on CoNLL 2003 and Ontonotes 5.0. When using
external resource like ELMo, NeuralCRF+SAC
still exhibits comparable results.

2 Related work

NER is a fundamental task, and traditional meth-
ods usually adopt statistical machine learning
models e.g., Hidden Markov Models (HMM),
Maximum-entropy Markov Models (MEMM),
and Conditional Random Fields (CRF) (Bikei
et al., 1998; McCallum et al., 2000; Lafferty et al.,
2001). To improve overall performance, many
hand-crafted features such as POS chunking tags
and gazetteers are required and integrated into the
task (Chieu and Ng, 2002; Florian et al., 2003;
Ando and Zhang, 2005).

With the growing popularity of deep learning
techniques, neural networks have been widely
adopted for NER tasks. Collobert et al. (2011) ini-
tiated the trend of using neural networks for se-
quence labelling tasks. Huang et al. (2015) com-
bined BiLSTM with CRF for NER tasks. Because
of the powerful feature mining ability of neural
networks, some end-to-end models employ neural
structures to extract character features and obtain
state-of-the-art results without using hand-crafted
features (Lample et al., 2016; Ma and Hovy, 2016;
Liu et al., 2018). Hand-crafted features such as
POS tags, word capitalization, and lexicons are
commonly used (Chiu and Nichols, 2016; Ghad-
dar and Langlais, 2018; Wu et al., 2018). It was
recently proved that using contextualized repre-
sentations from pretrained language models can
significantly improve performance (Peters et al.,
2017, 2018; Devlin et al., 2018).

To address the segmentation problem, several
methods have been proposed, among which Semi-
CRF(SCRF) is a successful solution for segment-

https://github.com/XiaoShiyuan/NCRF-SAC
https://github.com/XiaoShiyuan/NCRF-SAC


1142

2019/3/3 Figure1.xml

1/1

EU rejects German call

Word representations

SAC BiLSTM

concatenation and linear 

CRF

B-LOC O B-LOC O

input

embedding layer

feature layer

CRF layer

output

Figure 2: Overall architecture of NeuralCRF+SAC

level modeling (Zhuo et al., 2016). Ye and Ling
(2018) proposed an improved version of SCRF,
namely HSCRF. Using a multi-task strategy to
combine HSCRF and CRF, the HSCRF model ex-
hibits state-of-the-art results without using exter-
nal resources. In particular, HSCRF resolves the
segmentation problem by matching entity lengths.
In addition, there are other approaches that con-
sider multi-task learning to predict entity bound-
aries. Stratos (2016) divides NER into two tasks.
First, CRF is used to predict the location and
boundary of entities and to classify the entities.
Aguilar et al. (2017) used the outputs of their fea-
ture extractor to perform segmentation, catego-
rization, and then sequential classification.

Our method exploits the features of multi-task
learning and proposes a similarity based auxiliary
classifier (SAC). Compared to existing studies that
usually consider auxiliary tasks to be supervisors,
we also use the output of SAC to support the NER
task. Meanwhile, we employ vectors to denote
“True” and “False” tags, which contain more use-
ful information compared to simply using “1” and
“0” to denote tags.

3 Methodology

3.1 Overall Architecture

We implement the framework based on BiLSTM-
CNN-CRF(NeuralCRF) (Ma and Hovy, 2016), a
robust and widely used neural network in En-
glish NER tasks, and we build our SAC parallel
to BiLSTM. The overall architecture of Neural-
CRF+SAC is presented in Figure 2.
Embedding layer. Denoting input sequences as
X = {x1, x2, ..., xT }, we first obtain the word-

level vector W = {w1, w2, ..., wT }. Next, a
Char-CNN is employed to obtain character-level
features C = {cw1 , cw2 , ..., cwT } (Santos and
Zadrozny, 2014; Chiu and Nichols, 2016; Ma and
Hovy, 2016). Finally, we obtain [wi; cwi ] to denote
xi, where [;] indicates concatenation.
Feature layer. In this layer, we employ BiLSTM
to extract contextual features and SAC to gener-
ate weighted vectors. The calculations can be ex-
pressed as follows:

−→
hi = LSTM(

−−→
hi−1, [wi; cwi ]) (1)

←−
hi = LSTM(

←−−
hi−1, [wi; cwi ]) (2)

hai = SAC([wi; cwi ]) (3)

Here, we use SAC to denote all operations of the
classifier and we will introduce it in detail in Sec-
tion 3.2. The three outputs are then concatenated
and operated as zi = Linear([

−→
hi ;
←−
hi ;hai ]), where

Linear denotes a linear calculation.
CRF layer. Because of the ability to utilize in-
formation from neighboring labels, CRF is widely
employed in sequence tagging tasks. Denoting la-
bel sequences as Y = {y1, y2, ..., yT }, the collec-
tion of all possible labels as Y , and semantic fea-
ture sequence as Z = {z1, z2, ..., zT }, CRF can be
described as a family of conditional probabilities
p(y|Z):

p(y|Z) =
∏T

i=1 φi(yi−1, yi, zi)∑
y′∈Y

∏T
i=1 φi(y

′
i−1, y

′
i, zi)

(4)

where φi(yi−1, yi, zi) = exp(Wyi−1,yzi+byi−1,yi)
is the potential function and Z = {z1, z2, ..., zT }
are the outputs of the feature layer.

For the training process, we minimize the nega-
tive log likelihood:

LCRF = −
∑
i

log(p(yi|Z)) (5)

while the Viterbi algorithm (Forney, 1973) is used
to decode the best label sequence.

3.2 Similarity based Auxiliary Classifier
We believe that SAC can concentrate on one word
and on the nearest neighbors of the word rather
than on long-term dependency because we believe
that information from the word itself and its near-
est neighbors contribute the most in determining
whether the word is an entity word. Compared
to RNN, CNN exhibits a distinct advantage that



1143

2019/3/3 Figure2.xml

1/1

False True

Embeddings with position

concatenate and
linear

output

tanh

concatenate and linear 
k=2k=1 k=3

1-D convolution, k=1

Similarity score used
to represent

classification results

Auxiliary
Classifier

   tag 
vectors

Figure 3: The architecture of SAC. Notice that the con-
volutional block (lower left) includes 4 1-dimentional
convolutional neural networks, which share the same
filter settings. Kernel size is marked out.

it can accurately control the length of the context,
which satisfies our requirement. Therefore, SAC
is based on CNN. The structure of this module is
presented in Figure 3. To prevent the loss of po-
sitional information while using CNN, we imple-
ment a position encoder (Vaswani et al., 2017) to
generate position vectors pi ∈ Rd, where d is con-
figurable and pi is concatenated with [wi; cwi ] to
form new embeddings [wi; cwi ; pi]. The new em-
beddings are then sent to the convolutional block
to extract uni-gram, bi-gram and tri-gram infor-
mation. We denote the output of this convolu-
tional block as Hc = {hc1 , hc2 , ..., hcT }, where
hci ∈ Rn.

Intuitively, we adopt a multiplicative attention
mechanism (Luong et al., 2015) for computing
similarity. For the purpose of classifying en-
tity words and non-entity words, we use ti, (i ∈
{0, 1}) to denote non-entity words and entity
words, respectively. Then, we randomly initialize
two tag vectors vi ∈ Rdh , (i ∈ {0, 1}) as semantic
representations of ti, (i ∈ {0, 1}). We express the
computation as follows:

eij = v
T
i Wnhcj (6)

aij =
exp(eij)∑1

k=0 exp(ekj)
(7)

sj =
1∑

i=0

aijvi (8)

haj = tanh(Wa[hcj ; sj ] + ba) (9)

where Wn ∈ Rdh×n, Wa ∈ Rn×(n+dh) and ba ∈
Rn. Innovatively, we use aij , the similarity score
between ith tag and hcj as the classification result.

Dataset train dev test

CoNLL 2003 #tok 204567 51578 46666#ent 23499 5942 5648

Ontonotes 5.0 #tok 1088503 147724 152728#ent 81828 11066 11257

WNUT 2017 #tok 62729 15733 23394#ent 3160 1250 1589

Table 1: Statistics of these three datasets, #tok denotes
tokens and #ent denotes entities.

For instance, if the jth word is an entity word,
its feature representation hcj must have a higher
similarity score with v1. Therefore, SAC oper-
ates in a manner that is similar to a “supervised
cluster,” separating entity words from non-entity
words, and denoting two “clustering centers” with
two tag vectors. According to Equation 8 and 9,
classification information related to the jth word
can be expressed as a weighted sum sj and is then
concatenated with hcj . Finally, we obtain the out-
put Ha = {ha1 , ha2 , ..., haT }.

To train this auxiliary classifier, we define atten-
tion loss as follows:

LA = −
1

T

T∑
j=1

1∑
i=0

tilog(aij) (10)

By combiningLCRF andLA, we obtain the loss
function:

L = LCRF + λLA (11)

We obtain the best and most stable λ = 0.05 when
using the IOBES tag scheme.

4 Experiments

4.1 Experiments Setup
Dataset. We performed experiments on CoNLL
2003 (Tjong Kim Sang and De Meulder, 2003),
Ontonotes 5.0 (Hovy et al., 2006) and WNUT
2017 shared tasks (Derczynski et al., 2017).
CoNLL 2003 is a widely used dataset compris-
ing data extracted from Reuters news articles.
Ontonotes 5.0 is a large-scale dataset comprising
data collected from news, weblogs, etc. WNUT
2017 is a small dataset consisting primarily of
emerging entities. All datasets have been sepa-
rated into training/dev/test sets and they include
4/18/6 entity types, respectively. Table 1 presents
some statistics of the 3 datasets.
Configuration. All models were implemented
with TensorFlow 2. We used GloVe (Pennington

2www.tensorflow.org

www.tensorflow.org


1144

Module Parameter Value
Word embedding dims 300

Char-CNN
char dims 100
kernel size 3
filters 50

BiLSTM dims 300

Auxiliary Classifier
pos dims 40
tag-vector dims 25
CNN-block filters 300

Others

learning rate 0.001
batch size 20
epoch 50
drop rate 0.5

Table 2: Hyper-parameters

structure Accuracy F1(max)
BiLSTM 96.97 91.20
CNN 97.87 91.65

Table 3: Comparing different structure of SAC. Accu-
racy indicates that of an auxiliary classification task and
F1 for an NER task

et al., 2014) (300-dim, trained on 840B corpus3)
pretrained word embeddings, and the same hyper-
parameters on BiLSTM-CNN-CRF(NeuralCRF)
and NeuralCRF+SAC. To determine performance
using external resources, we included ELMo rep-
resentations. All hyper-parameters, most of which
are empirical parameters, are listed in Table 2 and
an early stopping strategy was adopted. The main
experiments were tested for 5 times to obtain the
mean and standard deviation of F1 score.
Evaluation. After training, the latest 50 check-
points were preserved and we obtained the check-
point with the highest word-level F1-score to make
predictions on the test set. Following this, we
used the official CoNLL evaluation script on the
CoNLL 2003 dataset and ontonotes 5.0 and the
WNUT evaluation script on the WNUT 2017
dataset.

4.2 Development experiments

To verify the rationality of the structure of SAC
and the role of the tag vectors, we designed two
development experiments on the CoNLL 2003
dataset. One experiment compared the CNN struc-
ture in SAC with the BiLSTM structure. The other
one compared SAC with a modified TextCNN.
Comparing with an RNN structure We be-
lieve that if a word is an entity word, its near-
est neighbors are very likely to be entity words
as well. Although there are many one-word enti-

3http://nlp.stanford.edu/data/glove.840B.300d.zip

1 2 3 4 5 ≥6
entity length

50

60

70

80

90

F1
 sc

or
e

F1 against entity length

BiLSTM
CNN

Figure 4: Comparing the ability of SAC with different
structures to identify different length entities.

2019/3/3 Figure3.xml

1/1

Embeddings with position

concatenate 
k=2k=1 k=3

1-D convolution, k=1

linear and
softmax

output

Figure 5: The structure of our TextCNN.

ties, these entities always share an obvious feature
such as capital letters, which can be easily cap-
tured by a char-CNN. On the contrary, if a word
is a non-entity word, under most circumstances,
there would be at most one entity word among the
adjacent words. However, when considering long-
term dependency, a word can be affected by sev-
eral former or latter words, which could weaken
the influence of adjacent words and introduce un-
necessary noise. Therefore, we chose CNN in-
stead of RNN in SAC and we set the max kernel
size to 3.

To verify if a CNN structure is more suitable
in SAC, we replace the convolutional block with
BiLSTM. In Table 3, we can observe that the max-
imum F1 value of the entire model is 91.65 when
employing a CNN structure compared to 91.20
when using BiLSTM. To determine the reason for
a large performance drop, we compare the perfor-
mances of the two structures in the face of differ-
ent entity lengths. From Figure 4, we can observe
that if SAC uses a BiLSTM structure, the over-
all performance will still deteriorate as entities be-
come long, indicating that SAC appears to be re-
dundant for the entire model. Maybe it is because
BiLSTM also exists in NeuralCRF, and using BiL-
STM in SAC may cause NeuralCRF and SAC to
obtain duplicate features.

http://nlp.stanford.edu/data/glove.840B.300d.zip


1145

structure Accuracy F1(max)

NeuralCRF+TextCNN 98.29 91.47

NeuralCRF+SAC 97.87 91.65

Table 4: Compare SAC with TextCNN. Accuracy is for
the auxiliary classification task and F1 for NER task.

Models type F1(±std)

Zhuo et al. (2016) reported 88.12

Lample et al. (2016) reported 90.94

Ma and Hovy (2016) reported 91.21

Strubell et al. (2017) reported 90.54

Liu et al. (2018)
mean 91.24(±0.12)
max 91.35

Ye and Ling (2018)
mean 91.38(±0.10)
max 91.53

NeuralCRF
mean 91.22(±0.09)
max 91.30

NeuralCRF+SAC
mean (91.48±0.07)
max 91.65

Table 5: Results on CoNLL 2003 NER task without
external resources.

Comparing with TextCNN In addition, we em-
ployed a conventional classifier like TextCNN
(Kim, 2014) to replace SAC. As is known,
TextCNN predicts a set of scores for each sentence
according to the number of categories. The po-
sition of the largest score indicates the category
of the sentence. As is shown in Figure 5, to en-
able TextCNN to predict scores for each word,
and to make a fair comparison with SAC, we re-
move the pooling layer of TextCNN and adopt the
same convolutional structure as SAC. Meanwhile,
both outputs of TextCNN and SAC include fea-
tures mined by their convolutional layers. The dif-
ference lies in the other part of their outputs in
which TextCNN includes a series of scores while
SAC provides the weighted sum of the tag vectors.

Table 4 demonstrates that NeuralCRF+SAC ob-
tains a maximum F1 score of 91.65 while Neural-
CRF+TextCNN receives a score of 91.47. How-
ever, we can observe that TextCNN exhibits higher
accuracy on its own classification task. We must
declare that the accuracy of SAC has different
meanings in terms of the accuracy of TextCNN.
For TextCNN, an accuracy score of 98.29 only
means that TextCNN assigns 98.29% of the words
with the right tags. However, for SAC, an accu-
racy score of 97.87% also means that 97.87% of

Models resources F1(±std)

Chiu and Nichols (2016) Lexicons 91.62(±0.33)

Peters et al. (2017) TagLM† 91.93(±0.19)

Ghaddar and Langlais (2018) gazetteer, LS 91.73

Wu et al. (2018) POS,etc. 91.89(±0.23)

Peters et al. (2018) ELMo† 92.22(±0.10)

Devlin et al. (2018) BERT(BASE)† 92.40

Devlin et al. (2018) BERT(LARGE)† 92.80

NeuralCRF ELMo† 92.20(±0.06)

NeuralCRF+SAC ELMo† 92.57(±0.07)

Table 6: Results of using the CoNLL 2003 NER task
with ELMo representations. † denotes the language
model trained on external resources.

Models resources F1(±std)

Shen et al. (2017) None 86.63(±0.49)

Strubell et al. (2017) None 86.84(±0.19)

NeuralCRF None 86.65(±0.10)

NeuralCRF+SAC None 87.24(±0.07)

Chiu and Nichols (2016) Lexicons 86.28(±0.26)

Ghaddar and Langlais (2018) gazetteer, LS 87.95

NeuralCRF ELMo 88.41(±0.06)

NeuralCRF+SAC ELMo 89.05(±0.18)

Table 7: Results on Ontonotes 5.0 dataset.

the words share common traits in which a majority
of entity words are similar to the “True” tag vector,
while the majority of non-entity words are similar
to “False” tag vector. Therefore, because of the
higher F1 score obtained from NeuralCRF+SAC,
the two tag vectors can be viewed as features that
are helpful for the NER task. Just like our base-
line approach, NeuralCRF, uses char-CNN to mine
character-level features. SAC automatically mines
features that are shared by entity words or non-
entity words.

4.3 Results on CoNLL 2003 dataset
End-to-end. Table 5 presents the experimental
results4 on the CoNLL 2003 datasets without ex-
ternal resources. We can observe that our model
exhibits a mean F1 score improvement of 0.26
compared to the base model and outperforms all
existing end-to-end models.
With ELMo representations. In addition, be-

4The first author of (Liu et al., 2018) indicates
errors pertaining to the original reported results be-
cause of some bugs and suggests citing fixed results.
(https://github.com/LiyuanLucasLiu/LM-LSTM-CRF)

https://github.com/LiyuanLucasLiu/LM-LSTM-CRF


1146

Models F1(entity) F1(surface)
Lin et al. (2017) 40.42 37.62
von Däniken et al. (2017) 40.78 39.33
Aguilar et al. (2017) 41.86 40.24
NeuralCRF 43.93 41.80
NeuralCRF+SAC 44.77 43.29

Table 8: Results of the WNUT 2017 shared task

Length Ye and Ling (2018) NeuralCRF+SAC ∆

1 91.73 91.75 +0.02

2 92.03 92.70 +0.67

3 83.78 83.26 -0.52

4 77.27 78.57 +0.30

5 79.66 80.00 +0.34

≥ 6 76.55 76.92 +0.37

overall 91.38 91.48 +0.10

Table 9: Comparing performance with Ye and Ling
(2018).

cause of the popularity of contextualized repre-
sentation, we introduce EMLo5 in the embedding
layer as a form of external resources and results,
which are presented in Table 6. Observe that
our model still exhibits a 0.37 F1 improvement
over the base model, which verifies the effects of
the SAC. Despite using BERT(LARGE), we still
obtain state-of-the-art results, even though it re-
quires a considerable number of resources to be
expended. In contrast, our method is flexible and
resources friendly.

4.4 Results on Ontonotes 5.0 dataset
We present the experimental results on Ontonotes
5.0 in Table 7. Without using external resources,
our model exhibits a mean F1 score of 87.24,
which still exceeds the previous end-to-end model.
When employing ELMo as external resources, we
obtain a mean F1 score of 89.05. Comparing
to NeuralCRF, SAC brings 0.59 and 0.64 F1 im-
provements respectively.

4.5 Results on WNUT 2017 shared task
Experimental results on the WNUT17 dataset are
presented in Table 8. Owing to the facts that this
task focuses on unusual and previously-unseen en-
tities, all existing models incorporate some hand-
crafted features. We include ELMo representa-
tions in this experiment. SAC exhibits a 0.84 im-
provement in F1(entity) and 1.49 improvement in

5https://allennlp.org/elmo

1 2 3 4 5 ≥6
entity length

50

60

70

80

90

F1
 sc

or
e

NCRF
NCRF+SAC

(a) F1 against entity length(CoNLL 2003)

1 2 3 4 5 ≥6
entity length

75

80

85

90

F1
 sc

or
e

NCRF
NCRF+SAC

(b) F1 against entity length(Ontonotes 5.0)

1 2 3 4 ≥5
entity length

0
10
20
30
40
50

F1
 sc

or
e

NCRF
NCRF+SAC

(c) F1 against entity length(WNUT 2017, with ELMo)

Figure 6: F1 against length test on all three datasets.

F1(surface).

4.6 Analysis
Performance against entity length. We believe
that before using CRF to predict the final la-
bel of a word, using a classifier to determine
whether the word is an entity word helps in ad-
dressing problems related to segmentation, partic-
ularly when entities become long. Following Ye
and Ling (2018)’s study, we compared the per-
formance of NeuralCRF and NeuralCRF+SAC in
identifying entities of different lengths on three
datasets. The experimental results of the CoNLL
2003, Ontonotes 5.0, and WNUT 2017 datasets
are presented in Figure 6. To eliminate the impact
of external resources, we use the end-to-end ap-
proach as far as possible. But because the WNUT
2017 datasets are relatively small and most of
them are emerging entities, which makes both

https://allennlp.org/elmo


1147

Ground truth Models Results

the Financial Times of Londom
NeuralCRF correspondent for the Financial Times(ORG) of London(GPE)...

NeuralCRF+SAC correspondent for the Financial Times of Londom(ORG)...

What Else is Making News
NeuralCRF Watch What Else(creative-work) is Making News

NeuralCRF+SAC Watch What Else is Making News(creative-work)

Monopolies and Mergers Commission

NeuralCRF
...Monopolies(ORG) and Mergers Commission(ORG) unless

the carriers complied...

NeuralCRF+SAC
...Monopolies and Mergers Commission(ORG) unless the

carriers complied...

Table 10: Three examples, identified entities are highlighted

	��������
��� ��� ��� �����	��� ����� �� ���
��

����

�����

���	� ���� ���� �� ����� ����

����

�����

���������� ��
 ������ ���������� ������ ��� 	������� 	������


����

�����

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

Figure 7: Similarity scores of three examples

models incapable of recognizing entities consist-
ing of 4 or more words, we employed ELMo in
WNUT 2017.

Observe that Figure 6(a) indicates a big im-
provement when entity lengths are equal to or
greater than 4 and Figures 6(b) and 6(c) illustrate
even improvements against different lengths.

In addition, we compared NeuralCRF+SAC
with Ye and Ling (2018)’ work in Table 9. The
two models perform comparably when the entity
length is less than 4, but NeuralCRF+SAC exhibits
even improvements when the entity length is equal
to or larger than 4, which demonstrates that SAC
can improve the overall performance in long entity
recognition.

Case study. We examined the manner in which
SAC handles long entities. Table 10 presents three
cases from 3 datasets, respectively. We observe
that these 3 cases are quite similar and NeuralCRF
makes almost the same mistake on each of them.
Generally speaking, character level features are
quite powerful for the NER task that words written
in uppercase letters are easily recognized as entity
words. In addition, if all the letters of a word are
lowercase letters, they are more likely to be clas-
sified as non-entity words. Therefore, we can ob-
serve that cases in which meeting words like “of”,
“is” and “and” are too normal to be considered en-
tity words, it is very easy for NeuralCRF to predict
an “O” label for these words. In fact, many long
entities exhibit a pattern in which almost all words
are written in uppercase letters except one. There-
fore, it is common that the segmentation problem
grows in severity as entities become longer. How-
ever, these three examples indicate that SAC can
address this problem to some extent. We obtain
the similarity scores of these examples and present
them in Figure 7. It is obvious that SAC assigns a
high similarity score to “of”, “is” and “and” even
though they hardly include any features of entity
words.

A possible reason is that the CNN structure of
SAC plays a role. As discussed in Section 4.2,
CNN can control the length of the context. When
the kernel sizes are set to 1, 2 and 3, CNN lay-
ers can extract uni-gram, bi-gram and tri-gram fea-
tures that enable SAC to learn a considerable num-
ber of samples that exhibit the same structure as
“Time of London.” Therefore, SAC reduces trun-
cation errors for long entities caused by lowercase
words.



1148

5 Conclusion

In this work, we extended the BiLSTM-CNN-CRF
architecture with SAC to address the segmenta-
tion problem. By computing the similarity scores
between words and two tag vectors, respectively,
SAC provides the weighted sum of tag vectors to-
gether with features it mined to support the main
NER task. Because of the higher accuracy of SAC,
the two tag vectors can be considered features that
are shared between entity words and non-entity
words. By reducing the boundary errors, SAC ad-
dresses the segmentation problem and improves
overall performance, particularly when entities be-
come long. Experimental results demonstrate that
NeuralCRF+SAC surpasses previous state-of-the-
art end-to-end models on the CoNLL 2003 and
Ontonotes 5.0 datasets without using external re-
sources. Moreover, when including ELMo, SAC
still improves overall performance and is compa-
rable to current state-of-the-art models.

Acknowledgments

This work was partially supported by the Na-
tional Natural Science Foundation of China (No.
61977002)

References
Gustavo Aguilar, Suraj Maharjan, A Pastor López-

Monroy, and Thamar Solorio. 2017. A multi-task
approach for named entity recognition in social me-
dia data. W-NUT 2017, page 148.

Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. Journal of Machine Learning
Research, 6(Nov):1817–1853.

Daniel M. Bikei, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1998. Nymble: High-
performance learning name-finder. Anlp, pages
194–201.

Hai Leong Chieu and Hwee Tou Ng. 2002. Named en-
tity recognition: a maximum entropy approach using
global information. In Proceedings of the 19th inter-
national conference on Computational linguistics-
Volume 1, pages 1–7. Association for Computational
Linguistics.

Jason PC Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional lstm-cnns. Transac-
tions of the Association for Computational Linguis-
tics, 4:357–370.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.

2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

Pius von Däniken, AG SpinningBytes, and Mark
Cieliebak. 2017. Transfer learning and sentence
level features for named entity recognition on
tweets. W-NUT 2017, page 166.

Leon Derczynski, Eric Nichols, Marieke van Erp, and
Nut Limsopatham. 2017. Results of the wnut2017
shared task on novel and emerging entity recogni-
tion. W-NUT 2017, page 140.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In Proceedings of
the seventh conference on Natural language learn-
ing at HLT-NAACL 2003-Volume 4, pages 168–171.
Association for Computational Linguistics.

G David Forney. 1973. The viterbi algorithm. Pro-
ceedings of the IEEE, 61(3):268–278.

Abbas Ghaddar and Phillippe Langlais. 2018. Robust
lexical features for improved neural network named-
entity recognition. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 1896–1907.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90\% solution. In Proceedings of the hu-
man language technology conference of the NAACL,
Companion Volume: Short Papers.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

John Lafferty, Andrew Mccallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. Proceedings of Icml, 3(2):282–289.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL-HLT, pages 260–270.

Bill Y Lin, Frank F Xu, Zhiyi Luo, and Kenny Q Zhu.
2017. Multi-channel bilstm-crf model for emerging
named entity recognition in social media. W-NUT
2017, page 160.

L. Liu, J. Shang, F. Xu, X. Ren, H. Gui, J. Peng, and
J. Han. 2018. Empower sequence labeling with task-
aware neural language model. In AAAI.



1149

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412–1421.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1064–1074.

Andrew McCallum, Dayne Freitag, and Fernando CN
Pereira. 2000. Maximum entropy markov models
for information extraction and segmentation. In
Icml, volume 17, pages 591–598.

Alexandre Passos, Vineet Kumar, and Andrew Mc-
Callum. 2014. Lexicon infused phrase embed-
dings for named entity resolution. arXiv preprint
arXiv:1404.5367.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Matthew Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language models.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), volume 1, pages 1756–1765.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), volume 1,
pages 2227–2237.

Cicero D Santos and Bianca Zadrozny. 2014. Learning
character-level representations for part-of-speech
tagging. In Proceedings of the 31st International
Conference on Machine Learning (ICML-14), pages
1818–1826.

Yanyao Shen, Hyokun Yun, Zachary C Lipton, Yakov
Kronrod, and Animashree Anandkumar. 2017.
Deep active learning for named entity recognition.
arXiv preprint arXiv:1707.05928.

Karl Stratos. 2016. Entity identification as multitask-
ing. arXiv preprint arXiv:1612.02706.

Emma Strubell, Patrick Verga, David Belanger, and
Andrew McCallum. 2017. Fast and accurate en-
tity recognition with iterated dilated convolutions.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2670–2680.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the Seventh Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2003), pages 142–147.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Minghao Wu, Fei Liu, and Trevor Cohn. 2018. Evalu-
ating the utility of hand-crafted features in sequence
labeling. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2850–2856.

Zhixiu Ye and Zhen-Hua Ling. 2018. Hybrid semi-
markov crf for neural sequence labeling. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 235–240.

Jingwei Zhuo, Yong Cao, Jun Zhu, Bo Zhang, and Za-
iqing Nie. 2016. Segment-level sequence modeling
using gated recursive semi-markov conditional ran-
dom fields. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1413–
1423.


