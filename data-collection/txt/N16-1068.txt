



















































Structured Prediction with Output Embeddings for Semantic Image Annotation


Proceedings of NAACL-HLT 2016, pages 552–557,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Structured Prediction with Output Embeddings
for Semantic Image Annotation

Ariadna Quattoni1, Arnau Ramisa2, Pranava Swaroop Madhyastha3

Edgar Simo-Serra4, Francesc Moreno-Noguer2
1Xerox Research Center Europe, ariadna.quattoni@xrce.xerox.com

2 Institut de Robòtica i Informàtica Industrial (CSIC-UPC), {aramisa,fmoreno}@iri.upc.edu
3TALP Research Center, Universitat Politècnica de Catalunya, pranava@cs.upc.edu

4Waseda University, esimo@aoni.waseda.jp

Abstract

We address the task of annotating images with
semantic tuples. Solving this problem requires
an algorithm able to deal with hundreds of
classes for each argument of the tuple. In
such contexts, data sparsity becomes a key
challenge. We propose handling this spar-
sity by incorporating feature representations
of both the inputs (images) and outputs (ar-
gument classes) into a factorized log-linear
model.

1 Introduction
Many important problems in machine learning can
be framed as structured prediction tasks where the
goal is to learn functions that map inputs to struc-
tured outputs such as sequences, trees or general
graphs. A wide range of applications involve learn-
ing over large state spaces, e.g., if the output is a
labeled graph, each node of the graph may take val-
ues over a potentially large set of labels. Data spar-
sity then becomes a challenge, as there will be many
classes with very few training examples.

Within this context, we are interested in the task
of predicting semantic tuples for images. That is,
given an input image we seek to predict what are
the events or actions (referred here as predicates),
who and what are the participants (referred here as
actors) of the actions and where is the action taking
place (referred here as locatives). For example, an
image might be annotated with the semantic tuples:
〈run, dog, park〉 and 〈play, dog, grass〉. We call
each field of a tuple an argument.

To handle the data sparsity challenge imposed by
the large state space, we will leverage an approach

that has proven to be useful in multiclass and mul-
tilabel prediction tasks (Weston et al., 2010; Akata
et al., 2013). The idea is to represent a value for
an argument a using a feature vector representation
φ ∈ IRn. We will integrate this argument represen-
tation into the structured prediction model.

In summary, our main contribution is to pro-
pose an approach that incorporates feature represen-
tations of the outputs into a structured prediction
model, and apply it to the problem of annotating
images with semantic tuples. We present an experi-
mental study using different output feature represen-
tations and analyze how they affect performance for
different argument types.

2 Semantic Tuple Image Annotation
Task: We will address the task of predicting se-
mantic tuples for images. Following Farhadi et al.
(2010), we will focus on a simple semantic repre-
sentation that considers three basic arguments: pred-
icate, actors and locatives. For example, in the tuple
〈play, dog, grass〉, “play” is the predicate, “dog” is
the actor and “grass” is the locative.

Given this representation, we can formally de-
fine our problem as that of learning a function
θ : X × P × A × L → IR that scores the
compatibility between images and semantic tuples.
Here X is the space of images; P , A and L are dis-
crete sets of predicate, actor and locative arguments
respectively, and 〈p a l〉 is a specific tuple instance.
The overall learning process is illustrated in Fig. 1.

Dataset: For our experiments we used a subset
of the Flickr8k dataset, proposed in Hodosh et al.
(2013). This dataset (subset B in Fig. 1) consists
of 8,000 images from Flickr of people and animals

552



Training'Image'x (From!Q)'

Training'Senten2al'Descrip2ons'(From'Q)'

Seman2c'Tuple'Extractor'
(Trained)using)L)))

Seman2c'Tuples'

Image'Features'

A)brown)dog)is)running)in)a)grassy)plain.)
A)brown)dog)runs)along)a)path)in)the)grass.)
Dog)running)in)field.))
Dog)running)in)narrow)dirt)path.)
The)dog)is)running)through)the)uncut)grass.)

<act=dog,)pre=run,)loc=plain>)
<act=dog,)pre=run,)loc=grass>)
<act=dog,)pre=run,)loc=field>)
<act=dog,)pre=run,)loc=path>)
<act=dog,)pre=run,)loc=grass>))

< ϕA(x), ϕP(x), ϕL(x) >  

Convolu2onal'NN'
(Trained)using)U)))

Image'to'
Seman2c'Tuple'

Predictor'

U) (Imagenet):) Images) annotated) with)
keywords) (cheap,) exploits) readily) available)
dataset))
Q) (Flickr8K):) Images) annotated) with)
descripJve) sentences) (relaJvely) cheap,)
exploits)readily)available)resources))
L) (SPMDataset):) Images) annotated) with)
descripJve) sentences) and) semanJc) tuples)
(this) is) a) small) subset) of) Q,) expensive)
annotaJon,)requires)experJse))

Training'Data'

Embedded'CRF''
(Implicitly)induces)
embedding)of)image)

features)and)arguments))

Figure 1: Overview of our approach. First, images x ∈ A are represented using image features φs(x), and semantic tuples are
obtained applying our semantic tuple extractor (learned from the subset C) to their corresponding captions. The resulting enlarged
training set, is used to train our embedded CRF model that maps images to semantic tuples.

(mostly dogs) performing some action, with five
crowd-sourced descriptive captions for each one.

We first manually annotated 1,544 captions, cor-
responding to 311 images (approximately one third
of the development set (subset C in Fig. 1), produc-
ing more than 2,000 semantic tuples of predicate, ac-
tor and locative. For the experiments we partitioned
the images and annotations into training, validation
and test sets of 150, 50 and 100 images respectively.

Data augmentation: To enlarge the manually an-
notated dataset we trained a model able to predict
semantic tuples from captions using standard shal-
low and deep linguistic features (e.g., POS tags, de-
pendency parsing, semantic role labeling). We ex-
tract the predicates by looking at the words tagged
as verbs by the POS tagger. Then, the extraction of
arguments for each predicate is resolved as a classi-
fication problem.

More specifically, for each detected predicate in a
sentence we regard each noun as a positive or neg-
ative training example of a given relation depend-
ing on whether the candidate noun is or is not an
argument of the predicate. We use these examples
to train a SVM classifier that predicts if a candidate
noun is an argument of a given predicate based on
several linguistic features computed over the syntac-
tic path of the dependency tree that connects them.
We run the learned tuple predictor model on all the
captions of the Fickr8k dataset to obtain a larger
dataset of 8,000 images paired with semantic tuples.

3 Bilinear Models with Output Features
In this section we explain how we incorporate out-
put feature representations into a factorized linear
model. For simplicity, we will consider factorized

sequence models over sequences of fixed length.
However, it should not be hard to see that all the
ideas presented here can be easily generalized to
other structured prediction settings.

Let y = [y1. . .yT ] be a set of labels and S =
[S1, . . ., ST ] be the set of possible label values,
where yi∈Si. We are interested in learning a model
that computes P (y|x), i.e., the conditional probabil-
ity of a sequence y given some input x. We will
consider factorized log-linear models that take the
form:

P (y|x) = exp
θ(x,y)∑

y exp
θ(x,y)

(1)

The scoring function θ(x, y) is modeled as a sum of
unary and binary bilinear potentials and is defined
as:

θ(x, y) =
T∑
t=1

v>ytWtφ(x, t) +
T∑
t=1

v>ytZtvyt+1 (2)

where vyt ∈ IRnt is a nt−dimensional feature rep-
resentation of label arguments yt∈St and φ(x, t) ∈
IRdt is a dt−dimensional feature representation of
the tth input factor of x.

The first set of terms in the above equation are
usually referred as unary potentials and measure the
compatibility between a single state at t and the fea-
ture representation of input factor t. The second set
of terms are the binary potentials and measure the
compatibility between pairs of states at adjacent fac-
tors. The scoring θ(x, y) function is fully parameter-
ized by the unary parameter matrices Wt∈IRnt×dt
and the binary parameter matrices Zt∈IRnt×nt .

The main idea is to define a feature space where
semantically similar labels will be close. Like in the

553



multilabel scenario (Weston et al., 2010; Akata et
al., 2013), having full feature representations for ar-
guments will allow us to share information across
different classes and generalize better. With a good
output feature representation, our model should be
able to make sensible predictions about pairs of ar-
guments that it has not observed at training. This is
easy to see: consider a case were we have a pair of
arguments represented with feature vectors a1 and
a2 and suppose that we have not observed the factor
a1, a2 in our training data but we have observed the
factor b1, b2. Then if a1 is close in the feature space
to argument b1 and a2 is close to b2 our model will
predict that a1 and a2 are compatible. That is it will
assign probability to the factor a1, a2 which seems
a natural generalization from the observed training
data.

Now we show that the rank of W and Z have use-
ful interpretations. Let W = UΣV be the singular
value decomposition ofW . We can then write unary
potentials: v>y Wφ(x, t) as: v>y U Σ [V φ(x, t)] Thus
we can regard the bilinear form as a function com-
puting a weighted inner product over some real em-
bedding v>y U representing state y and some real em-
bedding [V φ(x, t)] representing input factor t. The
rank of W gives us the intrinsic dimensionality of
the embedding. Thus if we want to induce shared
low-dimensional embeddings across different states
it seems reasonable to impose a low rank penalty on
W . Similarly, let Z = UΣV be now the singular
value decomposition of Z. We can write the binary
potentials v>y Zvy′ as: v>y U Σ V vy′ and thus the bi-
nary potentials compute a weighted inner product
between a real embedding of state y and a real em-
bedding of state y′. As before, the rank of Z gives us
the intrinsic dimensionality of the embedding and,
to induce a low dimensional embedding for binary
potentials, we will impose a low rank penalty on Z.

After having described the type of scoring func-
tions we are interested in, we now turn our at-
tention to the learning problem. That is, given
a training set D = {〈x y〉} of pairs of in-
puts x and output sequences y we need to learn
the parameters {W} and {Z}. For this purpose
we will do standard max-likelihood estimation and
find the parameters that minimize the conditional
negative log-likelihood of the data in D. That
is, we will find the {W} and {Z} that mini-

mize the following loss function L(D, {W}, {Z}):
−∑〈x y〉∈D logP (y|x; {W}, {Z}) Recall that we
are interested in learning low-rank unary and binary
potentials. To achieve this we take a common ap-
proach which is to use as the nuclear norm |W |∗ and
|Z|∗ as a convex approximation of the rank function,
the final optimization problem becomes:

min{W}L(D, {W}) +
∑
t

α|Wt|∗ + β|Zt|∗ (3)

where L(D, {W}) = ∑d∈D loss(d, {W}) is the
negative log likelihood function and α and β are
two constants that control the trade off between
minimizing the loss and the implicit dimensionality
of the embeddings. We use a simple optimization
scheme known as Forward Backward Splitting, or
FOBOS (Duchi and Singer, 2009).

For our task we will consider a simple factor-
ized scoring function: θ(x, 〈p a l〉) that has one
factor associated with the locative-predicate pair
and one factor associated with the predicate-actor
pair. Since this corresponds to a chain structure,
argmaxt∈T θ(x; 〈p a l〉) can be efficiently computed
using Viterbi decoding in time O(N2), where N =
max(|P |, |A|, |L|). Similarly, we can also find the
top k predictions in O(kN2). Thus for this appli-
cation the scoring function of the bilinear CRF will
take the form of:

θ(x, 〈p a l〉) = λloc(l)>Wlocφloc(l)
+λpre(p)>Wpreφpre(p)
+λact(a)>Wactφact(a)
+φloc(l)>W locpreφpre(p)

+φpre(p)>W
pre
act φact(a) (4)

The unary potentials measure the compatibility be-
tween an image and a semantic argument, the first
binary potential measures the compatibility between
a locative and a predicate, and the second binary po-
tential measures the compatibility between a predi-
cate and an actor. The scoring function is fully pa-
rameterized by the unary parameter matricesWloc ∈
IRdl×nl , Wpre ∈ IRdp×np and Wa ∈ IRda×na and
the binary parameter matrices W locpre ∈ IRnl×np and
W preact ∈ IRnp×na . Where, nl, np and na are the di-
mensionality of the locatives, predicates and actors
feature representations, respectively and dl, dp and

554



da are the dimensionality of the image representa-
tions. Notice that if we let the argument representa-
tion φt(r) ∈ IR|St| be an indicator vector for label
argument t, we obtain the usual parametrization of
a standard factorized linear model, while having a
dense feature representations for arguments instead
of indicator vectors will allow us to share informa-
tion across different classes.
4 Representing Semantic Arguments
We will conduct experiments with two differ-
ent feature representations: 1) Fully unsupervised
Skip-Gram based Continuous Word Representations
(SCWR) representation (Mikolov et al., 2013) and
2) A feature representation computed using the
〈caption, semantic-tuples〉 pairs, that we call Se-
mantic Equivalence Representation (SER).

We decided to exploit the dataset of captions
paired with semantic tuples to induce a useful fea-
ture representation for arguments. The idea is quite
simple: we wish to leverage the fact that any pair
of semantic tuples associated with the same image
will be likely describing the same event. Thus,
they are in essence different ways of lexicalizing
the same underlying concept. Let’s look at a con-
crete example. Imagine that we have an image an-
notated with the tuples: 〈play, dog, water〉 and
〈play, dog, river〉. Since both tuples describe the
same image, it is quite likely that both “river” and
“water” refer to the same real world entity, i.e,
“river” and “water” are ’semantically equivalent’
for this image. Using this idea we can build a rep-
resentation φloc(i) ∈ IR|L| where the j-th dimen-
sion corresponds to the number of times the argu-
ment j has been semantically equivalent to argu-
ment i. More precisely, we compute the probabil-
ity that argument j can be exchanged with argument
i as: [i,j]sr∑

j [i,j]sr
Where [i, j]sr is the number of times

that i and j have appeared as annotations of the same
image and with the same other arguments. For ex-
ample, for the actor arguments [i, j]sr represents the
number of time that actor i and actor j have appeared
with the same locative and predicate as descriptions
of the same image.
5 Related Work
In recent years, some works have tackled the prob-
lem of generating rich textual descriptions of im-
ages. One of the pioneers is (Kulkarni et al., 2011),

where a CRF model combines the output of several
vision systems to produce input for a language gen-
eration method. In Farhadi et al. (2010), the authors
find the similarity between sentences and images in
a “meaning” space, represented by semantic tuples
which are very similar to our triplets. Other works
focus on a simplified problem: ranking of human-
generated captions for images. Hodosh et al. (2013)
propose to use Kernel Canonical Correlation Anal-
ysis to project images and their captions into a joint
representation space, in which images and captions
can be related and ranked to perform illustration and
annotation tasks. Socher et al. (2014) also address
the ranking of images given a sentence and vice-
versa using a common subspace learned via Recur-
sive Neural Networks. Other recent works also ex-
ploit deep networks to address the problem (Vinyals
et al., 2015; Karpathy and Fei-Fei, 2015). Using la-
bel embeddings combined with bilinear forms has
been previously proposed in the context of multi-
class and multilabel image classification (Weston et
al., 2010; Akata et al., 2013).

6 Experiments
For image features we use the 4,096-dimensional
second to last layer of BVLC implementation of
‘AlexNet’ ImageNet model, a Convolutional Neu-
ral Network (CNN) as described in Jia et al. (2014).
To test our method we used the 100 test images that
were annotated with ground-truth semantic tuples.
To measure performance we first predict the top tu-
ple for each image and then measure accuracy for
each argument type (i.e. the number of correct pre-
dictions among the top 1 triplets). The regularization
parameters of each model were set using the valida-
tion set. We compare the performance of the fol-
lowing models: 1) Baseline Separate Predictors (S-
Pred): We consider a baseline made of independent
predictors for each argument type.

More specifically we train one-vs-all SVMs (we
also tried multi-class SVMs but they did not im-
prove performance) to independently predict loca-
tives, predicates and actors. For each argument type
and candidate label we have a score computed by
the corresponding SVM. Given an image we gener-
ate the top tuples that maximize the sum of scores for
each argument type; 2) Baseline KCCA: This model
implements the Kernel Canonical Correlation Anal-

555



<act=dog,pre=run,loc=beach>3

<act=girl,pre=sit,loc=pool>3

<act=dog,pre=run,loc=grass>3

<act=man,pre=stand,loc=street>3

<act=man,3pre=ride,3loc=street>3

<act=boy,pre=play,loc=field>3

<act=people,pre=sit,loc=camera>3

<act=dog,pre=run,loc=water>3 <act=boy,pre=sit,loc=pool>3

<act=dog,3pre=run,3loc=water>3 <act=dog,pre=stand,loc=field>3<act=dog,pre=perform,loc=air>3 <act=woman,pre=sit,loc=pool>3 <act=player,pre=hold,loc=football>3

Incorrect(loca+ve(&(ac+on( Incorrect(actor( Incorrect(actor( Incorrect(actor(&(ac+on( Incorrect(actor(&(loca+ve(

TRAINING(SENTENCES(
A3guy3is3doing3a3skateboard3trick3in3front3of3a3crowd3
A3man3is3skateboarding3in3front3of3a3group3of3people.3
A3skateboarder3performs3a3trick3in3front3of3a3large3crowd3.3333
A3skateboarder3leaping3from3a3pool3in3front3of3a3crowd.333
Skateboarder3does3tricks3in3front3of3crowd3while3photographer33
watches3333

3
3
3
3
3
3
3

<act=people,pre=perform,loc=air>3
<act=people,pre=jump,loc=air>3
<act=people,pre=wear,loc=air>3
<act=people,pre=watch,loc=air>3
<act=people,pre=perform,loc=pool>3
<act=people,pre=sit,loc=air>3
<act=people,pre=gather,loc=air>3

Figure 2: Samples of predicted tuples. Top-left: Examples of visually correct predictions. Bottom: Typical errors on one or
several arguments. Top-right: Sample image and its top predicted tuples. The tuples in blue were not observed neither in the
SP-Dataset nor in the automatically enlarged dataset. Note that all of them are descriptive of what is occurring in the scene.

ysis approach of Hodosh et al. (2013). We first note
that this approach is able to rank a list of candidate
captions but cannot directly generate tuples. To gen-
erate tuples for test images, we first find the caption
in the training set that has the highest ranking score
for that image and then extract the corresponding se-
mantic tuples from that caption; 3) Indicator Fea-
tures (IND), this is a standard factorized log-linear
model that does not use any feature representation
for the outputs; 4) A model that uses the skip-gram
continuous word representation of outputs (SCWR);
5) A model that uses that semantic equivalence rep-
resentation of outputs (SER); 6) A combined model
that makes predictions using the best feature repre-
sentation for each argument type (COMBO).

S-Pred KCCA IND SCWR SER COMBO
LOC 15 23 32 28 33
PRED 11 20 24 33 25
ACT 30 25 52 51 50
MEAN 18.6 22.6 36 37.3 36 39.3

Table 1: Comparison of Output Feature Representation.

Table 1 shows the results. We observe that our
proposed method performs significantly better than
the baselines. The second observation is that the
best performing output feature representation is dif-
ferent for different argument types, for the locatives
the best representation is SER, for the predicates is
the SCWR and for the actors using an output fea-
ture representation actually hurts performance. The

biggest improvement we get is on the predicate ar-
guments, where we improve almost by 10% in aver-
age precision over the baseline using the skip-gram
word representation. Overall, the model that uses
the best representation performs better than the indi-
cator baseline.

Regarding the rank of the parameter matrices,
we observed that the learned models can work well
even if we drop the rank to 10% of its maximum
rank. This shows that the learned models are effi-
cient in the sense that they can work well with low-
dimensional projections of the features.

7 Conclusion
In this paper we have presented a framework for ex-
ploiting input and output embeddings in the context
of structured prediction. We have applied this frame-
work to the problem of predicting compositional se-
mantic descriptions of images. Our results show the
advantages of using output embeddings and induc-
ing low-dimensional embeddings for handling large
state spaces in structured prediction problems. The
framework we propose is general enough to consider
additional sources of information.

8 Acknowledgments
This work was partly funded by the Spanish
MINECO project RobInstruct TIN2014-58178-R
and by the ERA-net CHISTERA projects VISEN
PCIN- 2013-047 and I-DRESS PCIN-2015-147.
The authors are grateful to the Nvidia donation pro-
gram for its support with GPU cards.

556



References
[Akata et al.2013] Zeynep Akata, Florent Perronnin, Zaid

Harchaoui, and Cordelia Schmid. 2013. Label-
embedding for attribute-based classification. In Proc.
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

[Duchi and Singer2009] John Duchi and Yoram Singer.
2009. Efficient online and batch learning using for-
ward backward splitting. Journal of Machine Learn-
ing Research (JMLR), 10:2899–2934.

[Farhadi et al.2010] Ali Farhadi, Mohsen Hejrati,
MohammadAmin Sadeghi, Peter Young, Cyrus
Rashtchian, Julia Hockenmaier, and David Forsyth.
2010. Every picture tells a story: Generating sen-
tences from images. In Proc. European Conference
on Computer Vision (ECCV).

[Hodosh et al.2013] Micah Hodosh, Peter Young, and Ju-
lia Hockenmaier. 2013. Framing image descrip-
tion as a ranking task: Data, models and evaluation
metrics. Journal of Artificial Intelligence Research
(JAIR), 47:853–899.

[Jia et al.2014] Yangqing Jia, Evan Shelhamer, Jeff Don-
ahue, Sergey Karayev, Jonathan Long, Ross Girshick,
Sergio Guadarrama, and Trevor Darrell. 2014. Caffe:
Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093.

[Karpathy and Fei-Fei2015] Andrej Karpathy and Li Fei-
Fei. 2015. Deep visual-semantic alignments for gen-

erating image descriptions. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition (CVPR).

[Kulkarni et al.2011] Girish Kulkarni, Visruth Premraj,
Sagnik Dhar, Siming Li, Yejin Choi, Alexander C
Berg, and Tamara L Berg. 2011. Baby talk: Un-
derstanding and generating image descriptions. In
Proc. IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR).

[Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013. Efficient estimation
of word representations in vector space. In Proc. In-
ternational Conference on Learning Representations
(ICLR).

[Socher et al.2014] Richard Socher, Andrej Karpathy,
Quoc V. Le, Christopher D. Manning, and Andrew Y.
Ng. 2014. Grounded compositional semantics for
finding and describing images with sentences. Trans-
actions of the Association of Computational Linguis-
tics (TACL), 2:207–218.

[Vinyals et al.2015] Oriol Vinyals, Alexander Toshev,
Samy Bengio, and Dumitru Erhan. 2015. Show and
tell: A neural image caption generator. In Proc. IEEE
Conference on Computer Vision and Pattern Recogni-
tion (CVPR).

[Weston et al.2010] Jason Weston, Samy Bengio, and
Nicolas Usunier. 2010. Large scale image annota-
tion: Learning to rank with joint word-image embed-
dings. In Proc. European Conference on Computer
Vision (ECCV).

557


