



















































Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2182‚Äì2192
Melbourne, Australia, July 15 - 20, 2018. c¬©2018 Association for Computational Linguistics

2182

Deep Dyna-Q: Integrating Planning for
Task-Completion Dialogue Policy Learning

Baolin Peng? Xiujun Li‚Ä† Jianfeng Gao‚Ä† Jingjing Liu‚Ä† Kam-Fai Wong?‚Ä°
‚Ä†Microsoft Research, Redmond, WA, USA

?The Chinese University of Hong Kong, Hong Kong
‚Ä°MoE Key Lab of High Confidence Software Technologies, China

{blpeng, kfwong}@se.cuhk.edu.hk
{xiul,jfgao,jingjl}@microsoft.com

Abstract

Training a task-completion dialogue agent
via reinforcement learning (RL) is costly
because it requires many interactions with
real users. One common alternative is to
use a user simulator. However, a user
simulator usually lacks the language com-
plexity of human interlocutors and the bi-
ases in its design may tend to degrade the
agent. To address these issues, we present
Deep Dyna-Q, which to our knowledge
is the first deep RL framework that inte-
grates planning for task-completion dia-
logue policy learning. We incorporate into
the dialogue agent a model of the envi-
ronment, referred to as the world model,
to mimic real user response and gener-
ate simulated experience. During dialogue
policy learning, the world model is con-
stantly updated with real user experience
to approach real user behavior, and in
turn, the dialogue agent is optimized using
both real experience and simulated expe-
rience. The effectiveness of our approach
is demonstrated on a movie-ticket booking
task in both simulated and human-in-the-
loop settings1.

1 Introduction

Learning policies for task-completion dialogue is
often formulated as a reinforcement learning (RL)
problem (Young et al., 2013; Levin et al., 1997).
However, applying RL to real-world dialogue sys-
tems can be challenging, due to the constraint that
an RL learner needs an environment to operate
in. In the dialogue setting, this requires a dia-
logue agent to interact with real users and adjust

1The source code of this work is available at https://
github.com/MiuLab/DDQ

its policy in an online fashion, as illustrated in Fig-
ure 1(a). Unlike simulation-based games such as
Atari games (Mnih et al., 2015) and AlphaGo (Sil-
ver et al., 2016a, 2017) where RL has made its
greatest strides, task-completion dialogue systems
may incur significant real-world cost in case of
failure. Thus, except for very simple tasks (Singh
et al., 2002; GasÃåicÃÅ et al., 2010, 2011; Pietquin
et al., 2011; Li et al., 2016a; Su et al., 2016b), RL
is too expensive to be applied to real users to train
dialogue agents from scratch.

One strategy is to convert human-interacting di-
alogue to a simulation problem (similar to Atari
games), by building a user simulator using human
conversational data (Schatzmann et al., 2007; Li
et al., 2016b). In this way, the dialogue agent can
learn its policy by interacting with the simulator
instead of real users (Figure 1(b)). The simulator,
in theory, does not incur any real-world cost and
can provide unlimited simulated experience for re-
inforcement learning. The dialogue agent trained
with such a user simulator can then be deployed to
real users and further enhanced by only a small
number of human interactions. Most of recent
studies in this area have adopted this strategy (Su
et al., 2016a; Lipton et al., 2016; Zhao and Eske-
nazi, 2016; Williams et al., 2017; Dhingra et al.,
2017; Li et al., 2017; Liu and Lane, 2017; Peng
et al., 2017b; Budzianowski et al., 2017; Peng
et al., 2017a).

However, user simulators usually lack the con-
versational complexity of human interlocutors,
and the trained agent is inevitably affected by bi-
ases in the design of the simulator. Dhingra et al.
(2017) demonstrated a significant discrepancy in
a simulator-trained dialogue agent when evalu-
ated with simulators and with real users. Even
more challenging is the fact that there is no uni-
versally accepted metric to evaluate a user simula-
tor (Pietquin and Hastie, 2013). Thus, it remains

https://github.com/MiuLab/DDQ
https://github.com/MiuLab/DDQ


2183

User

Human 
Conversational Data

Policy 
Model Direct 

Reinforcement 
Learning

Acting

Imitation
Learning

(a) Learning with real users

Simulator

Human 
Conversational Data

Policy 
Model Direct 

Reinforcement 
Learning

Acting

Imitation
Learning

Supervised
Learning

(b) Learning with user simulators

Policy 
Model

User
World 
Model

Real
Experience

Direct
Reinforcement 

Learning

World model 
learning

Planning

Acting

Human 
Conversational Data

Imitation
Learning

Supervised
Learning

(c) Learning with real users via DDQ

Figure 1: Three strategies of learning task-completion dialogue policies via RL.

controversial whether training task-completion di-
alogue agent via simulated users is a valid ap-
proach.

We propose a new strategy of learning dialogue
policy by interacting with real users. Compared
to previous works (Singh et al., 2002; Li et al.,
2016a; Su et al., 2016b; Papangelis, 2012), our di-
alogue agent learns in a much more efficient way,
using only a small number of real user interac-
tions, which amounts to an affordable cost in many
nontrivial dialogue tasks.

Our approach is based on the Dyna-Q frame-
work (Sutton, 1990) where planning is integrated
into policy learning for task-completion dialogue.
Specifically, we incorporate a model of the envi-
ronment, referred to as the world model, into the
dialogue agent, which simulates the environment
and generates simulated user experience. During
the dialogue policy learning, real user experience
plays two pivotal roles: first, it can be used to im-
prove the world model and make it behave more
like real users, via supervised learning; second, it
can also be used to directly improve the dialogue
policy via RL. The former is referred to as world
model learning, and the latter direct reinforcement
learning. Dialogue policy can be improved ei-
ther using real experience directly (i.e., direct re-
inforcement learning) or via the world model in-
directly (referred to as planning or indirect re-
inforcement learning). The interaction between
world model learning, direct reinforcement learn-
ing and planning is illustrated in Figure 1(c), fol-
lowing the Dyna-Q framework (Sutton, 1990).

The original papers on Dyna-Q and most its
early extensions used tabular methods for both
planning and learning (Singh, 1992; Peng and
Williams, 1993; Moore and Atkeson, 1993; Ku-
vayev and Sutton, 1996). This table-lookup repre-
sentation limits its application to small problems

only. Sutton et al. (2012) extends the Dyna ar-
chitecture to linear function approximation, mak-
ing it applicable to larger problems. In the dia-
logue setting, we are dealing with a much larger
action-state space. Inspired by Mnih et al. (2015),
we propose Deep Dyna-Q (DDQ) by combining
Dyna-Q with deep learning approaches to repre-
senting the state-action space by neural networks
(NN).

By employing the world model for planning, the
DDQ method can be viewed as a model-based RL
approach, which has drawn growing interest in the
research community. However, most model-based
RL methods (Tamar et al., 2016; Silver et al.,
2016b; Gu et al., 2016; RacanieÃÄre et al., 2017) are
developed for simulation-based, synthetic prob-
lems (e.g., games), but not for human-in-the-loop,
real-world problems. To these ends, our main con-
tributions in this work are two-fold:
‚Ä¢ We present Deep Dyna-Q, which to the best

of our knowledge is the first deep RL frame-
work that incorporates planning for task-
completion dialogue policy learning.
‚Ä¢ We demonstrate that a task-completion dia-

logue agent can efficiently adapt its policy on
the fly, by interacting with real users via RL.
This results in a significant improvement in
success rate on a nontrivial task.

2 Dialogue Policy Learning via Deep
Dyna-Q (DDQ)

Our DDQ dialogue agent is illustrated in Fig-
ure 2, consisting of five modules: (1) an LSTM-
based natural language understanding (NLU)
module (Hakkani-TuÃàr et al., 2016) for identifying
user intents and extracting associated slots; (2) a
state tracker (MrksÃåicÃÅ et al., 2016) for tracking the
dialogue states; (3) a dialogue policy which selects



2184

NLG

NLU ùëú1 ùëú2

Dialogue State Tracker

ùëúùë°

Dialogue Policy

Dialogue Manager

System Action 
(Policy)

ùë†ùë°

ùë†1 ùë†2 ùë†ùëõ

ùëé1 ùëé2 ùëéùëò

‚Ä¶‚Ä¶

‚Ä¶

Semantic 
Frame

State Representation

ùëé‚àó = max
ùëé

ùúã ùëé|ùë†

World 
Model

User
Goal

Figure 2: Illustration of the task-completion DDQ
dialogue agent.

the next action2 based on the current state; (4) a
model-based natural language generation (NLG)
module for converting dialogue actions to natural
language response (Wen et al.); and (5) a world
model for generating simulated user actions and
simulated rewards.

As illustrated in Figure 1(c), starting with an
initial dialogue policy and an initial world model
(both trained with pre-collected human conversa-
tional data), the training of the DDQ agent con-
sists of three processes: (1) direct reinforcement
learning, where the agent interacts with a real user,
collects real experience and improves the dialogue
policy; (2) world model learning, where the world
model is learned and refined using real experience;
and (3) planning, where the agent improves the di-
alogue policy using simulated experience.

Although these three processes conceptually
can occur simultaneously in the DDQ agent,
we implement an iterative training procedure, as
shown in Algorithm 1, where we specify the or-
der in which they occur within each iteration. In
what follows, we will describe these processes in
details.

2.1 Direct Reinforcement Learning

In this process (lines 5-18 in Algorithm 1) we use
the DQN method (Mnih et al., 2015) to improve
the dialogue policy based on real experience. We
consider task-completion dialogue as a Markov
Decision Process (MDP), where the agent inter-

2In the dialogue scenario, actions are dialogue-acts, con-
sisting of a single act and a (possibly empty) collection of
(slot = value) pairs (Schatzmann et al., 2007).

acts with a user in a sequence of actions to ac-
complish a user goal. In each step, the agent ob-
serves the dialogue state s, and chooses the action
a to execute, using an ÔøΩ-greedy policy that selects a
random action with probability ÔøΩ or otherwise fol-
lows the greedy policy a = argmaxa‚Ä≤Q(s, a

‚Ä≤; Œ∏Q).
Q(s, a; Œ∏Q) which is the approximated value func-
tion, implemented as a Multi-Layer Perceptron
(MLP) parameterized by Œ∏Q. The agent then re-
ceives reward3 r, observes next user response au,
and updates the state to s‚Ä≤. Finally, we store the
experience (s, a, r, au, s‚Ä≤) in the replay buffer Du.
The cycle continues until the dialogue terminates.

We improve the value function Q(s, a; Œ∏Q) by
adjusting Œ∏Q to minimize the mean-squared loss
function, defined as follows:

L(Œ∏Q) = E(s,a,r,s‚Ä≤)‚àºDu [(yi ‚àíQ(s, a; Œ∏Q))2]
yi = r + Œ≥max

a‚Ä≤
Q‚Ä≤(s‚Ä≤, a‚Ä≤; Œ∏Q‚Ä≤) (1)

where Œ≥ ‚àà [0, 1] is a discount factor, and Q‚Ä≤(.) is
the target value function that is only periodically
updated (line 42 in Algorithm 1). By differentiat-
ing the loss function with respect to Œ∏Q, we arrive
at the following gradient:

‚àáŒ∏QL(Œ∏Q) = E(s,a,r,s‚Ä≤)‚àºDu [(r+
Œ≥max

a‚Ä≤
Q‚Ä≤(s‚Ä≤, a‚Ä≤; Œ∏Q‚Ä≤)‚àíQ(s, a; Œ∏Q))

‚àáŒ∏QQ(s, a; Œ∏Q)]

(2)

As shown in lines 16-17 in Algorithm 1, in each
iteration, we improve Q(.) using minibatch Deep
Q-learning.

2.2 Planning

In the planning process (lines 23-41 in Algo-
rithm 1), the world model is employed to generate
simulated experience that can be used to improve
dialogue policy. K in line 24 is the number of
planning steps that the agent performs per step of
direct reinforcement learning. If the world model
is able to accurately simulate the environment, a
big K can be used to speed up the policy learn-
ing. In DDQ, we use two replay buffers, Du for
storing real experience and Ds for simulated ex-
perience. Learning and planning are accomplished

3In the dialogue scenario, reward is defined to measure
the degree of success of a dialogue. In our experiment, for
example, success corresponds to a reward of 80, failure to a
reward of‚àí40, and the agent receives a reward of‚àí1 at each
turn so as to encourage shorter dialogues.



2185

Algorithm 1 Deep Dyna-Q for Dialogue Policy
Learning
Require: N , ÔøΩ, K, L, C, Z
Ensure: Q(s, a; Œ∏Q), M(s, a; Œ∏M )
1: initialize Q(s, a; Œ∏Q) and M(s, a; Œ∏M ) via pre-training

on human conversational data
2: initialize Q‚Ä≤(s, a; Œ∏Q‚Ä≤) with Œ∏Q‚Ä≤ = Œ∏Q
3: initialize real experience replay buffer Du using Reply

Buffer Spiking (RBS), and simulated experience replay
buffer Ds as empty

4: for n=1:N do
5: # Direct Reinforcement Learning starts
6: user starts a dialogue with user action au

7: generate an initial dialogue state s
8: while s is not a terminal state do
9: with probability ÔøΩ select a random action a

10: otherwise select a = argmaxa‚Ä≤Q(s, a
‚Ä≤; Œ∏Q)

11: execute a, and observe user response au and re-
ward r

12: update dialogue state to s‚Ä≤

13: store (s, a, r, au, s‚Ä≤) to Du

14: s = s‚Ä≤

15: end while
16: sample random minibatches of (s, a, r, s‚Ä≤) from Du

17: update Œ∏Q via Z-step minibatch Q-learning according
to Equation (2)

18: # Direct Reinforcement Learning ends
19: # World Model Learning starts
20: sample random minibatches of training samples

(s, a, r, au, s‚Ä≤) from Du

21: update Œ∏M via Z-step minibatch SGD of multi-task
learning

22: # World Model Learning ends
23: # Planning starts
24: for k=1:K do
25: t = FALSE, l = 0
26: sample a user goal G
27: sample user action au from G
28: generate an initial dialogue state s
29: while t is FALSE ‚àß l ‚â§ L do
30: with probability ÔøΩ select a random action a
31: otherwise select a = argmaxa‚Ä≤Q(s, a

‚Ä≤; Œ∏Q)
32: execute a
33: world model responds with au, r and t
34: update dialogue state to s‚Ä≤

35: store (s, a, r, s‚Ä≤) to Ds

36: l = l + 1, s = s‚Ä≤

37: end while
38: sample random minibatches of (s, a, r, s‚Ä≤) from

Ds

39: update Œ∏Q via Z-step minibatch Q-learning ac-
cording to Equation (2)

40: end for
41: # Planning ends
42: every C steps reset Œ∏Q‚Ä≤ = Œ∏Q
43: end for

by the same DQN algorithm, operating on real ex-
perience in Du for learning and on simulated ex-
perience in Ds for planning. Thus, here we only
describe the way the simulated experience is gen-
erated.

Similar to Schatzmann et al. (2007), at the be-
ginning of each dialogue, we uniformly draw a
user goal G = (C,R), where C is a set of con-

straints and R is a set of requests (line 26 in Al-
gorithm 1). For movie-ticket booking dialogues,
constraints are typically the name and the date
of the movie, the number of tickets to buy, etc.
Requests can contain these slots as well as the
location of the theater, its start time, etc. Ta-
ble 3 presents some sampled user goals and di-
alogues generated by simulated and real users,
respectively. The first user action au (line 27)
can be either a request or an inform dialogue-
act. A request, such as request(theater;
moviename=batman), consists of a request
slot and multiple (> 1) constraint slots, uni-
formly sampled from R and C, respectively.
An inform contains constraint slots only. The
user action can also be converted to natural lan-
guage via NLG, e.g., "which theater will
show batman?"

In each dialogue turn, the world model takes
as input the current dialogue state s and the last
agent action a (represented as an one-hot vector),
and generates user response au, reward r, and a
binary variable t, which indicates whether the di-
alogue terminates (line 33). The generation is ac-
complished using the world model M(s, a; Œ∏M ), a
MLP shown in Figure 3, as follows:

h = tanh(Wh(s, a) + bh)

r = Wrh+ br

au = softmax(Wah+ ba)

t = sigmoid(Wth+ bt)

where (s, a) is the concatenation of s and a, and
W and b are parameter matrices and vectors, re-
spectively.

Task-Specific Representation

s: state a: agent action

au r t

Shared
layers

1

Figure 3: The world model architecture.



2186

2.3 World Model Learning

In this process (lines 19-22 in Algorithm 1),
M(s, a; Œ∏M ) is refined via minibatch SGD using
real experience in the replay buffer Du. As shown
in Figure 3,M(s, a; Œ∏M ) is a multi-task neural net-
work (Liu et al., 2015) that combines two classi-
fication tasks of simulating au and t, respectively,
and one regression task of simulating r. The lower
layers are shared across all tasks, while the top lay-
ers are task-specific.

3 Experiments and Results

We evaluate the DDQ method on a movie-ticket
booking task in both simulation and human-in-the-
loop settings.

3.1 Dataset

Raw conversational data in the movie-ticket book-
ing scenario was collected via Amazon Mechani-
cal Turk. The dataset has been manually labeled
based on a schema defined by domain experts, as
shown in Table 4, which consists of 11 dialogue
acts and 16 slots. In total, the dataset contains 280
annotated dialogues, the average length of which
is approximately 11 turns.

3.2 Dialogue Agents for Comparison

To benchmark the performance of DDQ, we have
developed different versions of task-completion
dialogue agents, using variations of Algorithm 1.
‚Ä¢ A DQN agent is learned by standard DQN,

implemented with direct reinforcement learn-
ing only (lines 5-18 in Algorithm 1) in each
epoch.
‚Ä¢ The DDQ(K) agents are learned by DDQ of

Algorithm 1, with an initial world model pre-
trained on human conversational data, as de-
scribed in Section 3.1. K is the number of
planning steps. We trained different versions
of DDQ(K) with different K‚Äôs.
‚Ä¢ The DDQ(K, rand-init Œ∏M ) agents are

learned by the DDQ method with a randomly
initialized world model.
‚Ä¢ The DDQ(K, fixed Œ∏M ) agents are learned

by DDQ with an initial world model pre-
trained on human conversational data. But
the world model is not updated afterwards.
That is, the world model learning part in Al-
gorithm 1 (lines 19-22) is removed. The
DDQ(K, fixed Œ∏M ) agents are evaluated in
the simulation setting only.

‚Ä¢ The DQN(K) agents are learned by DQN,
but with K times more real experiences than
the DQN agent. DQN(K) is evaluated in the
simulation setting only. Its performance can
be viewed as the upper bound of its DDQ(K)
counterpart, assuming that the world model
in DDQ(K) perfectly matches real users.

Implementation Details All the models in these
agents (Q(s, a; Œ∏Q), M(s, a; Œ∏M )) are MLPs with
tanh activations. Each policy network Q(.) has
one hidden layer with 80 hidden nodes. As shown
in Figure 3, the world model M(.) contains two
shared hidden layers and three task-specific hid-
den layers, with 80 nodes in each. All the agents
are trained by Algorithm 1 with the same set of
hyper-parameters. ÔøΩ-greedy is always applied for
exploration. We set the discount factor Œ≥ = 0.95.
The buffer sizes of both Du and Ds are set to
5000. The target value function is updated at
the end of each epoch. In each epoch, Q(.) and
M(.) are refined using one-step (Z = 1) 16-tuple-
minibatch update. 4 In planning, the maximum
length of a simulated dialogue is 40 (L = 40).
In addition, to make the dialogue training effi-
cient, we also applied a variant of imitation learn-
ing, called Reply Buffer Spiking (RBS) (Lipton
et al., 2016). We built a naive but occasionally suc-
cessful rule-based agent based on human conver-
sational dataset (line 1 in Algorithm 1), and pre-
filled the real experience replay buffer Du with
100 dialogues of experience (line 2) before train-
ing for all the variants of agents.

3.3 Simulated User Evaluation

In this setting the dialogue agents are optimized
by interacting with user simulators, instead of real
users. Thus, the world model is learned to mimic
user simulators. Although the simulator-trained
agents are sub-optimal when applied to real users
due to the discrepancy between simulators and real
users, the simulation setting allows us to perform
a detailed analysis of DDQ without much cost and
to reproduce the experimental results easily.

4We found in our experiments that setting Z > 1 im-
proves the performance of all agents, but does not change
the conclusion of this study: DDQ consistently outperforms
DQN by a statistically significant margin. Conceptually, the
optimal value of Z used in planning is different from that in
direct reinforcement learning, and should vary according to
the quality of the world model. The better the world model
is, the more aggressive update (thus bigger Z) is being used
in planning. We leave it to future work to investigate how to
optimize Z for planning in DDQ.



2187

Agent Epoch = 100 Epoch = 200 Epoch = 300Success Reward Turns Success Reward Turns Success Reward Turns
DQN .4260 -3.84 31.93 .5308 10.78 22.72 .6480 27.66 22.21
DDQ(5) .6056 20.35 26.65 .7128 36.76 19.55 .7372 39.97 18.99
DDQ(5, rand-init Œ∏M ) .5904 18.75 26.21 .6888 33.47 20.36 .7032 36.06 18.64
DDQ(5, fixed Œ∏M ) .5540 14.54 25.89 .6660 29.72 22.39 .6860 33.58 19.49
DQN(5) .6560 29.38 21.76 .7344 41.09 16.07 .7576 43.97 15.88
DDQ(10) .6624 28.18 24.62 .7664 42.46 21.01 .7840 45.11 19.94
DDQ(10, rand-init Œ∏M ) .6132 21.50 26.16 .6864 32.43 21.86 .7628 42.37 20.32
DDQ(10, fixed Œ∏M ) .5884 18.41 26.41 .6196 24.17 22.36 .6412 26.70 22.49
DQN(10) .7944 48.61 15.43 .8296 54.00 13.09 .8356 54.89 12.77

Table 1: Results of different agents at training epoch = {100, 200, 300}. Each number is averaged
over 5 runs, each run tested on 2000 dialogues. Excluding DQN(5) and DQN(10) which serve as the
upper bounds, any two groups of success rate (except three groups: at epoch 100, DDQ(5, rand-init Œ∏M )
and DDQ(10, fixed Œ∏M ), at epoch 200, DDQ(5, rand-init Œ∏M ) and DDQ(10, rand-init Œ∏M ), at epoch
300, DQN and DDQ(10, fixed Œ∏M )) evaluated at the same epoch is statistically significant in mean with
p < 0.01. (Success: success rate)

0 50 100 150 200 250 300 350 400
Epoch

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Su
cc

es
s 

ra
te

DQN
DDQ(2)
DDQ(5)
DDQ(10)
DDQ(20)

Figure 4: Learning curves of the DDQ(K) agents
withK = 2, 5, 10, 20. The DQN agent is identical
to a DDQ(K) agent with K = 0.

User Simulator We adapted a publicly avail-
able user simulator (Li et al., 2016b) to the task-
completion dialogue setting. During training, the
simulator provides the agent with a simulated user
response in each dialogue turn and a reward sig-
nal at the end of the dialogue. A dialogue is
considered successful only when a movie ticket
is booked successfully and when the information
provided by the agent satisfies all the user‚Äôs con-
straints. At the end of each dialogue, the agent
receives a positive reward of 2 ‚àó L for success, or
a negative reward of ‚àíL for failure, where L is
the maximum number of turns in each dialogue,
and is set to 40 in our experiments. Furthermore,
in each turn, the agent receives a reward of ‚àí1,
so that shorter dialogues are encouraged. Read-
ers can refer to Appendix B for details on the user
simulator.

0 50 100 150 200 250 300 350 400
Epoch

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Su
cc

es
s 

ra
te

DQN
DDQ(10)
DDQ(10, rand-init)
DDQ(10, fixed)
DQN(10)

Figure 5: Learning curves of DQN, DDQ(10),
DDQ(10, rand-init Œ∏M ), DDQ(10, fixed Œ∏M ), and
DQN(10).

Results The main simulation results are reported
in Table 1 and Figures 4 and 5. For each agent,
we report its results in terms of success rate, av-
erage reward, and average number of turns (aver-
aged over 5 repetitions of the experiments). Re-
sults show that the DDQ agents consistently out-
perform DQN with a statistically significant mar-
gin. Figure 4 shows the learning curves of differ-
ent DDQ agents trained using different planning
steps. Since the training of all RL agents started
with RBS using the same rule-based agent, their
performance in the first few epochs is very close.
After that, performance improved for all values of
K, but much more rapidly for larger values. Re-
call that the DDQ(K) agent with K=0 is identical
to the DQN agent, which does no planning but re-
lies on direct reinforcement learning only. Without
planning, the DQN agent took about 180 epochs
(real dialogues) to reach the success rate of 50%,



2188

Agent Epoch = 100 Epoch = 150 Epoch = 200Success Reward Turns Success Reward Turns Success Reward Turns
DQN .0000 -58.69 39.38 .4080 -5.730 30.38 .4545 0.350 30.38
DDQ(5) .4620 00.78 31.33 .5637 15.05 26.17 .6000 19.84 26.32
DDQ(5, rand-init Œ∏M ) .3600 -11.67 31.74 .5500 13.71 26.58 .5752 16.84 26.37
DDQ(10) .5555 14.69 25.92 .6416 25.85 24.28 .7332 38.88 20.21
DDQ(10, rand-init Œ∏M ) .5010 6.27 29.70 .6055 22.11 23.11 .7023 36.90 21.20

Table 2: The performance of different agents at training epoch = {100, 150, 200} in the human-in-the-
loop experiments. The difference between the results of all agent pairs evaluated at the same epoch is
statistically significant (p < 0.01). (Success: success rate)

and DDQ(10) took only 50 epochs.
Intuitively, the optimal value of K needs to be

determined by seeking the best trade-off between
the quality of the world model and the amount
of simulated experience that is useful for improv-
ing the dialogue agent. This is a non-trivial opti-
mization problem because both the dialogue agent
and the world model are updated constantly during
training and the optimal K needs to be adjusted
accordingly. For example, we find in our experi-
ments that at the early stages of training, it is fine
to perform planning aggressively by using large
amounts of simulated experience even though they
are of low quality, but in the late stages of train-
ing where the dialogue agent has been signif-
icantly improved, low-quality simulated experi-
ence is likely to hurt the performance. Thus, in our
implementation of Algorithm 1, we use a heuris-
tic5 to reduce the value of K in the late stages of
training (e.g., after 150 epochs in Figure 4) to mit-
igate the negative impact of low-qualify simulated
experience. We leave it to future work how to op-
timize the planning step size during DDQ training
in a principled way.

Figure 5 shows that the quality of the world
model has a significant impact on the agent‚Äôs
performance. The learning curve of DQN(10)
indicates the best performance we can expect
with a perfect world model. With a pre-trained
world model, the performance of the DDQ agent
improves more rapidly, although eventually, the
DDQ and DDQ(rand-init Œ∏M ) agents reach the
same success rate after many epochs. The world
model learning process is crucial to both the ef-
ficiency of dialogue policy learning and the final
performance of the agent. For example, in the
early stages (before 60 epochs), the performances
of DDQ and DDQ(fixed Œ∏M ) remain very close to
each other, but DDQ reaches a success rate almost

5The heuristic is not presented in Algorithm 1. Readers
can refer to the released source code for details.

0 50 100 150 200
Epoch

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Su
cc

es
s 

ra
te

DQN
DDQ(5)
DDQ(5, rand-init)
DDQ(10)
DDQ(10, rand-init)

Figure 6: Human-in-the-loop dialogue policy
learning curves in four different agents.

10% better than DDQ(fixed Œ∏M ) after 400 epochs.

3.4 Human-in-the-Loop Evaluation

In this setting, five dialogue agents (i.e., DQN,
DDQ(10), DDQ(10, rand-init Œ∏M ), DDQ(5), and
DDQ(5, rand-init Œ∏M )) are trained via RL by in-
teracting with real human users. In each dialogue
session, one of the agents was randomly picked to
converse with a user. The user was presented with
a user goal sampled from the corpus, and was in-
structed to converse with the agent to complete the
task. The user had the choice of abandoning the
task and ending the dialogue at any time, if she or
he believed that the dialogue was unlikely to suc-
ceed or simply because the dialogue dragged on
for too many turns. In such cases, the dialogue ses-
sion is considered failed. At the end of each ses-
sion, the user was asked to give explicit feedback
whether the dialogue succeeded (i.e., whether the
movie tickets were booked with all the user con-
straints satisfied). Each learning curve is trained
with two runs, with each run generating 150 dia-
logues (andK‚àó150 additional simulated dialogues
when planning is applied). In total, we collected
1500 dialogue sessions for training all five agents.

The main results are presented in Table 2 and



2189

Simulation Sample Real User Sample
movie-ticket booking user goal:
{

‚Äúrequest slots‚Äù: { ‚Äúconstraint slots‚Äù: {
‚Äúticket‚Äù: ‚Äú?‚Äù ‚Äúnumberofpeople‚Äù:‚Äú2‚Äù
‚Äútheater‚Äù: ‚Äú?‚Äù ‚Äúmoviename‚Äù: ‚Äúdeadpool‚Äù
‚Äústarttime‚Äù: ‚Äú?‚Äù ‚Äúcity‚Äù: ‚Äúseattle‚Äù
‚Äúdate‚Äù: ‚Äú?‚Äù }
}
}

movie-ticket booking user goal:
{

‚Äúrequest slots‚Äù: { ‚Äúconstraint slots‚Äù: {
‚Äúticket‚Äù: ‚Äú?‚Äù ‚Äúdate‚Äù:‚Äúthis weekend‚Äù
‚Äútheater‚Äù: ‚Äú?‚Äù ‚Äúnumberofpeople‚Äù: ‚Äú1‚Äù
‚Äústarttime‚Äù: ‚Äú?‚Äù ‚Äúmoviename‚Äù: ‚Äúbatman‚Äù
} }
}

DQN agent
1 usr: Hi, what date can I book 2 tickets for deadpool? usr: Hi, which theater will show batman? I want 1 ticket.
2 agt: Which city would you like? agt: Which city would you like?
3 usr: I want to watch at seattle. usr: Boston.
4 agt: What time would you like to see it? agt: What time would you like to see it?
5 usr: What start time is available? usr: What start time is available?
6 agt: Which theater would you like? agt: What date would you like to see it?
7 usr: Which theater is available? usr: This weekend.
8 agt: What time would you like to see it? agt:Which theater would you like?
9 usr: What start time is available? usr: Which theater is available?
10 agt: Which theater would you like? agt: Which theater would you like?
11 usr: Which theater is available? usr: Which theater is available?
12 agt: How many tickets do you need? agt: What time would you like to see it?
13 usr: I want 2 tickets please! usr: What start time is available?
14 agt: Which theater would you like? agt: Which theater would you like?
15 usr: Which theater is available? usr: Thanks, I do not need the ticket.
16 agt: What time would you like to see it? agt: Thank you.

...
39 agt: Which theater would you like?
40 usr: Which theater is available?

Failure: Agent does not book the right ticket within themax turns. Failure:
User terminates the dialogue before reaching the
max turns because the agent repeatly asks the same
question.

DDQ(10) agent
1 usr: Hi, when is deadpool playing at seattle? usr: Hi, which theater will show batman? I want 1 ticket.
2 agt: Tonight. agt: Century Eastport 16.
3 usr: What is the start time? usr: What is the start time?
4 agt: Which theater would you like? agt: 9PM.
5 usr: I want to watch it at amc pacific place 11. usr: Can I buy a ticket here?
6 agt: 10pm is available. agt: This weekend?
7 usr: I want 2 tickets please! usr: Yes, one ticket please!

8 agt: Great, here are 2 deadpool tickets for youat amc pacific place 11 in seattle tonight 10 pm. agt:
Sure, here it is - batman at Century Eastport 16,
9PM this weekend.

9 usr: Thank you. usr: Thank you.
10 agt: Thank you. agt: Thank you.

Success Success

Table 3: Two sample dialogue sessions by DQN and DDQ(10) agents trained at epoch 100: Left: simu-
lated user experiments; Right: human-in-the-loop experiments. (agt: agent, usr: user)

Figure 6, with each agent averaged over two in-
dependent runs. The results confirm what we ob-
served in the simulation experiments. The conclu-
sions are summarized as below:

‚Ä¢ The DDQ agent significantly outperforms
DQN, as demonstrated by the comparison be-
tween DDQ(10) and DQN. Table 3 presents
four example dialogues produced by two di-
alogue agents interacting with simulated and
human users, respectively. The DQN agent,
after being trained with 100 dialogues, still
behaved like a naive rule-based agent that re-

quested information bit by bit in a fixed or-
der. When the user did not answer the request
explicitly (e.g., usr: which theater
is available?), the agent failed to re-
spond properly. On the other hand, with plan-
ning, the DDQ agent trained with 100 real
dialogues is much more robust and can com-
plete 50% of user tasks successfully.
‚Ä¢ A largerK leads to more aggressive planning

and better results, as shown by DDQ(10) vs.
DDQ(5).
‚Ä¢ Pre-training world model with human con-



2190

versational data improves the learning effi-
ciency and the agent‚Äôs performance, as shown
by DDQ(5) vs. DDQ(5, rand-init Œ∏M ), and
DDQ(10) vs. DDQ(10, rand-init Œ∏M ).

4 Conclusion

We propose a new strategy for a task-completion
dialogue agent to learn its policy by interacting
with real users. Compared to previous work, our
agent learns in a much more efficient way, us-
ing only a small number of real user interactions,
which amounts to an affordable cost in many non-
trivial domains. Our strategy is based on the Deep
Dyna-Q (DDQ) framework where planning is in-
tegrated into dialogue policy learning. The ef-
fectiveness of DDQ is validated by human-in-the-
loop experiments, demonstrating that a dialogue
agent can efficiently adapt its policy on the fly by
interacting with real users via deep RL.

One interesting topic for future research is ex-
ploration in planning. We need to deal with the
challenge of adapting the world model in a chang-
ing environment, as exemplified by the domain ex-
tension problem (Lipton et al., 2016). As pointed
out by Sutton and Barto (1998), the general prob-
lem here is a particular manifestation of the con-
flict between exploration and exploitation. In a
planning context, exploration means trying actions
that may improve the world model, whereas ex-
ploitation means trying to behave in the optimal
way given the current model. To this end, we want
the agent to explore in the environment, but not so
much that the performance would be greatly de-
graded.

Additional Authors

Shang-Yu Su (National Taiwan University, Room
524, CSIE Bldg., No. 1, Sec. 4, Roo-
sevelt Rd., Taipei 10617, Taiwan. email:
shangyusu.tw@gmail.com)

Acknowledgments

We would like to thank Chris Brockett, Yun-Nung
Chen, Michel Galley and Lihong Li for their in-
sightful comments on the paper. We would like
to acknowledge the volunteers from Microsoft Re-
search for helping us with the human-in-the-loop
experiments. This work was done when Baolin
Peng and Shang-Yu Su were visiting Microsoft.
Baolin Peng is in part supported by Innovation

and Technology Fund (6904333), and General Re-
search Fund of Hong Kong (12183516).

References
Pawel Budzianowski, Stefan Ultes, Pei-Hao Su, Nikola

Mrksic, Tsung-Hsien Wen, Inigo Casanueva, Lina
Rojas-Barahona, and Milica Gasic. 2017. Sub-
domain modelling for dialogue management with
hierarchical reinforcement learning. arXiv preprint
arXiv:1706.06210 .

Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao,
Yun-Nung Chen, Faisal Ahmed, and Li Deng. 2017.
Towards end-to-end reinforcement learning of dia-
logue agents for information access. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). volume 1, pages 484‚Äì495.

Milica GasÃåicÃÅ, Filip JurcÃåƒ±ÃÅcÃåek, Simon Keizer, FrancÃßois
Mairesse, Blaise Thomson, Kai Yu, and Steve
Young. 2010. Gaussian processes for fast policy op-
timisation of pomdp-based dialogue managers. In
Proceedings of the 11th Annual Meeting of the Spe-
cial Interest Group on Discourse and Dialogue. As-
sociation for Computational Linguistics, pages 201‚Äì
204.

Milica GasÃåicÃÅ, Filip JurcÃåƒ±ÃÅcÃåek, Blaise Thomson, Kai Yu,
and Steve Young. 2011. On-line policy optimisation
of spoken dialogue systems via live interaction with
human subjects. In Automatic Speech Recognition
and Understanding (ASRU), 2011 IEEE Workshop
on. IEEE, pages 312‚Äì317.

Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and
Sergey Levine. 2016. Continuous deep q-learning
with model-based acceleration. In International
Conference on Machine Learning. pages 2829‚Äì
2838.

Dilek Hakkani-TuÃàr, Gokhan Tur, Asli Celikyilmaz,
Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-
Yi Wang. 2016. Multi-domain joint semantic frame
parsing using bi-directional RNN-LSTM. In Pro-
ceedings of The 17th Annual Meeting of the Interna-
tional Speech Communication Association.

Leonid Kuvayev and Richard S Sutton. 1996. Model-
based reinforcement learning with an approximate,
learned model. In in Proceedings of the Ninth Yale
Workshop on Adaptive and Learning Systems. Cite-
seer.

Esther Levin, Roberto Pieraccini, and Wieland Eck-
ert. 1997. Learning dialogue strategies within the
markov decision process framework. In Automatic
Speech Recognition and Understanding, 1997. Pro-
ceedings., 1997 IEEE Workshop on. IEEE, pages
72‚Äì79.

Jiwei Li, Alexander H Miller, Sumit Chopra,
Marc‚ÄôAurelio Ranzato, and Jason Weston. 2016a.



2191

Dialogue learning with human-in-the-loop. arXiv
preprint arXiv:1611.09823 .

Xiujun Li, Zachary C Lipton, Bhuwan Dhingra, Lihong
Li, Jianfeng Gao, and Yun-Nung Chen. 2016b. A
user simulator for task-completion dialogues. arXiv
preprint arXiv:1612.05688 .

Xuijun Li, Yun-Nung Chen, Lihong Li, Jianfeng Gao,
and Asli Celikyilmaz. 2017. End-to-end task-
completion neural dialogue systems. In Proceed-
ings of the The 8th International Joint Conference
on Natural Language Processing. pages 733‚Äì743.

Zachary C Lipton, Jianfeng Gao, Lihong Li, Xiujun
Li, Faisal Ahmed, and Li Deng. 2016. Efficient
exploration for dialogue policy learning with bbq
networks & replay buffer spiking. arXiv preprint
arXiv:1608.05081 .

Bing Liu and Ian Lane. 2017. Iterative policy learning
in end-to-end trainable task-oriented neural dialog
models. In Proceedings of 2017 IEEE Workshop on
Automatic Speech Recognition and Understanding.

Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,
Kevin Duh, and Ye-Yi Wang. 2015. Representation
learning using multi-task deep neural networks for
semantic classification and information retrieval .

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level
control through deep reinforcement learning. Na-
ture 518(7540):529‚Äì533.

Andrew W Moore and Christopher G Atkeson. 1993.
Prioritized sweeping: Reinforcement learning with
less data and less time. Machine learning
13(1):103‚Äì130.

Nikola MrksÃåicÃÅ, Diarmuid O SeÃÅaghdha, Tsung-Hsien
Wen, Blaise Thomson, and Steve Young. 2016.
Neural belief tracker: Data-driven dialogue state
tracking. arXiv preprint arXiv:1606.03777 .

Alexandros Papangelis. 2012. A comparative study of
reinforcement learning techniques on dialogue man-
agement. In Proceedings of the Student Research
Workshop at the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
pages 22‚Äì31.

Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu,
Yun-Nung Chen, and Kam-Fai Wong. 2017a. Ad-
versarial advantage actor-critic model for task-
completion dialogue policy learning. arXiv preprint
arXiv:1710.11277 .

Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao,
Asli Celikyilmaz, Sungjin Lee, and Kam-Fai Wong.
2017b. Composite task-completion dialogue policy

learning via hierarchical deep reinforcement learn-
ing. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing.
pages 2221‚Äì2230.

Jing Peng and Ronald J Williams. 1993. Efficient
learning and planning within the dyna framework.
Adaptive Behavior 1(4):437‚Äì454.

Olivier Pietquin, Matthieu Geist, Senthilkumar Chan-
dramohan, et al. 2011. Sample efficient on-
line learning of optimal dialogue policies with
kalman temporal differences. In IJCAI Proceedings-
International Joint Conference on Artificial Intelli-
gence. volume 22, page 1878.

Olivier Pietquin and Helen Hastie. 2013. A survey on
metrics for the evaluation of user simulations. The
knowledge engineering review .

SeÃÅbastien RacanieÃÄre, TheÃÅophane Weber, David Re-
ichert, Lars Buesing, Arthur Guez, Danilo Jimenez
Rezende, AdriaÃÄ PuigdomeÃÄnech Badia, Oriol
Vinyals, Nicolas Heess, Yujia Li, et al. 2017.
Imagination-augmented agents for deep reinforce-
ment learning. In Advances in Neural Information
Processing Systems. pages 5694‚Äì5705.

Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based user
simulation for bootstrapping a pomdp dialogue sys-
tem. In NAACL 2007; Companion Volume, Short
Papers. Association for Computational Linguistics,
pages 149‚Äì152.

David Silver, Aja Huang, Chris J Maddison, Arthur
Guez, Laurent Sifre, George Van Den Driessche, Ju-
lian Schrittwieser, Ioannis Antonoglou, Veda Pan-
neershelvam, Marc Lanctot, et al. 2016a. Mastering
the game of go with deep neural networks and tree
search. Nature 529(7587):484‚Äì489.

David Silver, Julian Schrittwieser, Karen Simonyan,
Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian
Bolton, et al. 2017. Mastering the game of go with-
out human knowledge. Nature 550(7676):354.

David Silver, Hado van Hasselt, Matteo Hessel, Tom
Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-
Arnold, David Reichert, Neil Rabinowitz, Andre
Barreto, et al. 2016b. The predictron: End-
to-end learning and planning. arXiv preprint
arXiv:1612.08810 .

Satinder Singh, Diane Litman, Michael Kearns, and
Marilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experiments
with the njfun system. Journal of Artificial Intelli-
gence Research 16:105‚Äì133.

Satinder P Singh. 1992. Reinforcement learning with
a hierarchy of abstract models. In Proceedings of
the National Conference on Artificial Intelligence.
JOHN WILEY & SONS LTD, 10, page 202.



2192

Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-
Barahona, Stefan Ultes, David Vandyke, Tsung-
Hsien Wen, and Steve Young. 2016a. Continu-
ously learning neural dialogue management. arXiv
preprint arXiv:1606.02689 .

Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-
Barahona, Stefan Ultes, David Vandyke, Tsung-
Hsien Wen, and Steve Young. 2016b. On-line active
reward learning for policy optimisation in spoken di-
alogue systems. arXiv preprint arXiv:1605.07669 .

Richard S Sutton. 1990. Integrated architectures for
learning, planning, and reacting based on approx-
imating dynamic programming. In Proceedings
of the seventh international conference on machine
learning. pages 216‚Äì224.

Richard S Sutton and Andrew G Barto. 1998. Introduc-
tion to reinforcement learning, volume 135. MIT
press Cambridge.

Richard S Sutton, Csaba SzepesvaÃÅri, Alborz Geram-
ifard, and Michael P Bowling. 2012. Dyna-style
planning with linear function approximation and pri-
oritized sweeping. arXiv preprint arXiv:1206.3285
.

Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine,
and Pieter Abbeel. 2016. Value iteration networks.
In Advances in Neural Information Processing Sys-
tems. pages 2154‚Äì2162.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-
hao Su, David Vandyke, and Steve J. Young. ????
Semantically conditioned LSTM-based natural lan-
guage generation for spoken dialogue systems. In
EMNLP 2015, Lisbon, Portugal, September 17-21,
2015. pages 1711‚Äì1721.

Jason D Williams, Kavosh Asadi, and Geoffrey Zweig.
2017. Hybrid code networks: Practical and efficient
end-to-end dialog control with supervised and rein-
forcement learning. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics.

Steve Young, Milica GasÃåicÃÅ, Blaise Thomson, and Ja-
son D Williams. 2013. Pomdp-based statistical spo-
ken dialog systems: A review. Proceedings of the
IEEE 101(5):1160‚Äì1179.

Tiancheng Zhao and Maxine Eskenazi. 2016. To-
wards end-to-end learning for dialog state tracking
and management using deep reinforcement learning.
arXiv preprint arXiv:1606.02560 .

A Dataset Annotation Schema

Table 4 lists all annotated dialogue acts and slots
in details.

Annotations
request, inform, deny, confirm question,

Intent confirm answer, greeting, closing, not sure,
multiple choice, thanks, welcome

Slot

city, closing, date, distanceconstraints,
greeting, moviename, numberofpeople,
price, starttime, state, taskcomplete, theater,
theater chain, ticket, video format, zip

Table 4: The data annotation schema

B User Simulator

In the task-completion dialogue setting, the entire
conversation is around a user goal implicitly, but
the agent knows nothing about the user goal ex-
plicitly and its objective is to help the user to ac-
complish this goal. Generally, the definition of
user goal contains two parts:
‚Ä¢ inform slots contain a number of slot-value

pairs which serve as constraints from the user.
‚Ä¢ request slots contain a set of slots that user

has no information about the values, but
wants to get the values from the agent dur-
ing the conversation. ticket is a default slot
which always appears in the request slots
part of user goal.

To make the user goal more realistic, we add
some constraints in the user goal: slots are split
into two groups. Some of slots must appear in the
user goal, we called these elements as Required
slots. In the movie-booking scenario, it includes
moviename, theater, starttime, date, num-
berofpeople; the rest slots are Optional slots, for
example, theater chain, video format etc.

We generated the user goals from the labeled
dataset mentioned in Section 3.1, using two mech-
anisms. One mechanism is to extract all the slots
(known and unknown) from the first user turns (ex-
cluding the greeting user turn) in the data, since
usually the first turn contains some or all the re-
quired information from user. The other mech-
anism is to extract all the slots (known and un-
known) that first appear in all the user turns,
and then aggregate them into one user goal. We
dump these user goals into a file as the user-goal
database. Every time when running a dialogue, we
randomly sample one user goal from this user goal
database.


