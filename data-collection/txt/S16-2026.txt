



















































Approximating Givenness in Content Assessment through Distributional Semantics


Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 209–218,
Berlin, Germany, August 11-12, 2016.

Approximating Givenness in Content Assessment
through Distributional Semantics

Ramon Ziai Kordula De Kuthy Detmar Meurers
Collaborative Research Center 833

University of Tübingen
{rziai,kdk,dm}@sfs.uni-tuebingen.de

Abstract

Givenness (Schwarzschild, 1999) is one of
the central notions in the formal pragmatic
literature discussing the organization of dis-
course. In this paper, we explore where
distributional semantics can help address
the gap between the linguistic insights into
the formal pragmatic notion of Givenness
and its implementation in computational
linguistics.

As experimental testbed, we focus on short
answer assessment, in which the goal is to
assess whether a student response correctly
answers the provided reading comprehen-
sion question or not. Current approaches
only implement a very basic, surface-based
perspective on Givenness: A word of the
answer that appears as such in the question
counts as GIVEN.

We show that an approach approximating
Givenness using distributional semantics
to check whether a word in a sentence is
similar enough to a word in the context to
count as GIVEN is more successful quanti-
tatively and supports interesting qualitative
insights into the data and the limitations
of a basic distributional semantic approach
identifying Givenness at the lexical level.

1 Introduction

Givenness is one of the central notions in the formal
pragmatic literature discussing the organization of
discourse. The distinction between given and new
material in an utterance dates back at least to Hal-
liday (1967) where given is defined as “anaphori-
cally recoverable” and the notion is used to predict
patterns of prosodic prominence. Schwarzschild
(1999) proposes to define Givenness in terms of

the entailment of the existential f-closure between
previously mentioned material and the GIVEN ex-
pression, hereby also capturing the occurrence of
synonyms and hyponyms as given.

On the theoretical linguistic side, a foundational
question is whether an approach to Information
Structure should be grounded in terms of a Given-
New or a Focus-Background dichotomy, or whether
the two are best seen as complementing each other.
Computational linguistic research on short answer
assessment points in the direction of both perspec-
tives providing performance gains (Ziai and Meur-
ers, 2014). On the empirical side, the characteristic
problem of obtaining high inter-annotator agree-
ment in focus annotation (Ritz et al., 2008; Cal-
houn et al., 2010) can be overcome through an
incremental annotation process making reference
to questions as part of an explicit task context (Ziai
and Meurers, 2014; De Kuthy et al., 2016).

In short answer assessment approaches determin-
ing whether a student response correctly answers
a provided reading comprehension question, the
practical value of excluding material that is men-
tioned in the question from evaluating the content
of the answer has been clearly established (Meurers
et al., 2011; Mohler et al., 2011). Yet these com-
putational linguistic approaches only implement a
very basic, completely surface-based perspective
on Givenness: A word of the answer that appears
as such in the question counts as GIVEN.

Such a surface-based approach to Givenness fails
to capture that the semantic notion of Givenness

i) may be transported by semantically similar
words,

ii) entailment rather than identity is at stake, and

iii) so-called bridging cases seem to involve se-
mantically related rather than semantically
similar words.

209



Computational linguistic approaches to classify-
ing Givenness (Hempelmann et al., 2005; Nissim,
2006; Rahman and Ng, 2011; Cahill and Riester,
2012) have concentrated on the information sta-
tus of noun phrases, without taking into account
other syntactic elements. Furthermore, they do not
explicitly make use of similarity and relatedness
between lexical units as we propose in this paper.
Our approach thus explores a new avenue in com-
putationally determining Givenness.

Theoretical linguistic proposals spelling out
Givenness are based on formal semantic for-
malisms and notions such as logical entailment,
type shifting, and existential f-closure, which do
not readily lend themselves to extending the com-
putational linguistic approaches. As already al-
luded to by the choice of words “semantically simi-
lar” and “semantically related” above, in this paper
we want to explore whether distributional seman-
tics can help address the gap between the linguis-
tic insights into Givenness and the computational
linguistic realizations. In place of surface-based
Givenness checks, as a first step in this direction we
developed an approach integrating distributional se-
mantics to check whether a word in a sentence is
similar enough to a word in the context to count as
GIVEN.

In section 2, we provide the background on
Schwarzschild’s notion of Givenness and conceptu-
ally explore what a distributional semantic perspec-
tive may offer. Section 3 then introduces the appli-
cation domain of content assessment as our experi-
mental sandbox and the CoMiC system (Meurers
et al., 2011) we extended. The distributional model
for German used in extending the baseline system
is built in section 4. In section 5 we then turn to
the experiments we conducted using the system
extended with the distributional Givenness com-
ponent and provide quantitative results. Section 6
then presents the qualitative perspective, discussing
examples to probe into the connection between the
theoretical linguistic notion of Givenness and its
distributional semantic approximation, and where it
fails. Finally, section 7 concludes with a summary
of the approach and its contribution.

2 Linking Givenness and the
distributional semantic perspective

Before turning to the computational realization and
a quantitative and qualitative evaluation of the idea,
let us consider which classes of data are handled

by the theoretical linguistic approach to Givenness
and where an approximation of Givenness using
distributional semantics can contribute.

Let us first define Givenness according to
Schwarzschild (1999, p. 151): an utterance U
counts as GIVEN iff it has a salient antecedent A
and either i) A and U co-refer or ii) A entails the
Existential F-Closure of U . In turn, the Existential
F-Closure of U is defined as “the result of replacing
F-marked phrases in U with variables and existen-
tially closing the result, modulo existential type
shifting” (Schwarzschild, 1999, p. 150).

Schwarzschild uses Givenness to predict where
in an utterance the prosodic prominence falls. Con-
sider the question-answer pair in (1), example (12)
of Schwarzschild (1999).

(1) John drove Mary’s red convertible. What did
he drive before that?
A: He drove her BLUE convertible.

Here the prominence does not fall on convertible
as the rightmost expression answering the question,
as generally is the case in English, but instead on
the adjective blue because the convertible is GIVEN
and thus is de-accented according to Schwarzschild.
With respect to our goal of automatically identify-
ing Givenness, such cases involving identical lexi-
cal material that is repeated (here: convertible) are
trivial for a surface-based or distributional semantic
approach.

A more interesting case of Givenness involves
semantically similar words such as synonyms and
hypernyms, as exemplified by violin and string
instrument in (2), mentioned as example (7) by
Büring (2007).

(2) (I’d like to learn the violin,) because I LIKE
string instruments.

The existence of a violin entails the existence of
a string instrument, so string instrument is GIVEN
and deaccented under Schwarzschild’s approach.
Such examples are beyond a simple surface-based
approach to the identification of Givenness and mo-
tivate the perspective pursued in this paper: investi-
gating whether a distributional semantic approach
to semantic similarity can be used to capture them.

Before tackling these core cases, let us complete
the empirical overview of the landscape of cases
that the Givenness notion is expected to handle. A
relevant phenomenon in this context is bridging.
It can be exemplified using (3), which is example
(29) of Schwarzschild (1999).

210



(3) a. John got the job.
b. I KNOW. They WANTed a New Yorker.

The part of the formal definitions that is intended
to capture the deaccenting of New Yorker in a con-
text where John is known to be from that city sim-
ply refers to salience (Schwarzschild, 1999: “An
utterance U counts as GIVEN iff it has a salient
antecedent A . . . ”), which Schwarzschild readily
admits is not actually modeled: “Exactly which
propositions count as in the background for these
purposes remains to be worked out”. While beyond
the scope of our experiments, approaches comput-
ing semantic similarity in more local contexts, such
as Dinu and Lapata (2010), may be able to provide
an avenue for handling such narrowly contextual-
ized notions of common ground in the evolving,
dynamic discourse.

A more straightforward case arises when such
bridging examples involve semantic relatedness
between expressions that are richly represented in
corpora. For example, the fact that Giuliani was the
mayor of New York and thus can be identified as
semantically related to New Yorker in (4) is within
reach of a distributional semantic approach.

(4) a. Giuliani got the job.
b. I KNOW. They WANTed a New Yorker.

When exactly such bridging based on semanti-
cally related material results in GIVEN material and
its deaccenting, as far as we are aware, has not been
systematically researched and would be relevant to
explore in the future.

An interesting case related to bridging that adds
a further challenge for any Givenness approach is
exemplified by (5), originating as example (4) in
Büring (2007). The challenge arises from the fact
that it does not seem to involve an apparent se-
mantic relation such as entailment – yet the accent
falling on strangle can only be explained if butcher
is GIVEN, i.e., entailed by the context.

(5) a. Did you see Dr. Cremer to get your root
canal?

b. (Don’t remind me.) I’d like to STRANgle
the butcher.

The linguistic approaches to Givenness do not
formally tackle this since the lexical semantic spec-
ification and contextual disambiguation of butcher
as a particular (undesirable type of) dentist is be-
yond their scope. The fact that butcher counts as

GIVEN is not readily captured by a general distri-
butional semantic approach either since it is de-
pendent on the specific context and the top-down
selection of the meaning of butcher as referring to
people who brutally go about their job. Distribu-
tional semantic approaches distinguishing specific
word senses (Iacobacci et al., 2015) could be appli-
cable for extending the core approach worked out
in this paper to cover such cases.

Overall, at the conceptual level, a realization of
Givenness in terms of distributional semantics can
be seen as nicely complementing the theoretical
linguistic approach in terms of the division of labor
of formal and distributional factors.

3 Content Assessment: Baseline System
and Gold Standard Data

To be able to test the idea we conceptually moti-
vated above, we chose short answer assessment as
our experimental testbed. The content assessment
of reading comprehension exercises is an authen-
tic task including a rich, language-based context.
This makes it an interesting real-life challenge for
research into the applicability of formal pragmatic
concepts such as Givenness. Provided a text and
a question, the content assessment task is to de-
termine whether a particular response actually an-
swers the question or not.

In such a setting, the question typically intro-
duces some linguistic material about which addi-
tional information is required. The material intro-
duced is usually not the information required in a
felicitous answer. For example, in a question such
as ‘Where was Mozart born?’, we are looking for
a location. Consequently, in an answer such as
‘Mozart was born in Salzburg’, we can disregard
the words ‘Mozart’, ‘was’ and ‘born’ on account
of their previous mention, leaving only the relevant
information ‘in Salzburg’.

Short answer assessment is thus a natural testbed
since the practical value of excluding material that
is mentioned in the question from evaluating the
content of the answer has been clearly established
(Meurers et al., 2011; Mohler et al., 2011) – yet
these approaches only integrated a basic surface-
based perspective on Givenness. The CoMiC sys-
tem (Meurers et al., 2011) is freely available, so
we used it as baseline approach and proceeded to
replaced its surface-based Givenness filter with our
distributional semantic approach to Givenness.

211



3.1 Baseline system

CoMiC is an alignment-based Content Assessment
system which assesses student answers by analyz-
ing the quantity and quality of alignment links it
finds between the student and the target answer. For
content assessment, it extracts several numeric fea-
tures based on the number and kind of alignments
found between non-GIVEN answer parts. The only
change we made to the baseline setup is to replace
the TiMBL (Daelemans et al., 2007) implementa-
tion of k-nearest-neighbors with the WEKA pack-
age (Hall et al., 2009), setting k to 5 following the
positive results of Rudzewitz (2016).

The CoMiC system we use as baseline for our
research employs a surface-based Givenness filter,
only aligning tokens not found in the question. The
surface-based Givenness filter thus ensures that
parts of the answer already occurring in the ques-
tion are not counted (or could be fed into separate
features so that the machine learner making the
final assessment can take their discourse status into
account).

3.2 Gold-standard content assessment corpus

The data we used for training and testing our ex-
tension of the CoMiC system are taken from the
CREG corpus (Ott et al., 2012), a task-based cor-
pus consisting of answers to reading comprehen-
sion questions written by American learners of
German at the university level. It was collected
at Kansas University (KU) and The Ohio State Uni-
versity (OSU). The overall corpus includes 164
reading texts, 1,517 reading comprehension ques-
tions, 2,057 target answers provided by the teach-
ers, and 36,335 learner answers.

The CREG-5K subset used for the present ex-
periments is an extended version of CREG-1032
(Meurers et al., 2011), selected using the same
criteria after the overall, four year corpus collec-
tion effort was completed. The criteria include
balancedness (same number of correct and incor-
rect answers), a minimum answer length of four
tokens, and a language course level at the interme-
diate level or above.

4 Creating a distributional model

To model Givenness as distributional similarity, we
need an appropriate word vector model. As there
is no such model readily available for German, we
trained one ourselves.

As empirical basis, we used the DeWAC corpus

(Baroni et al., 2009) since it is a large corpus that is
freely available and it is already lemmatized, both
of which have been argued to be desirable for word
vector models. Further preprocessing consisted
of excluding numbers and other undesired words
such as foreign language material and words the
POS tagger had labelled as non-words. The whole
corpus was converted to lowercase to get rid of
unwanted distinctions between multiple possible
capitalizations.

To select an implementation for our purpose,
we compared two of the major word vector toolk-
its currently available, word2vec (Mikolov et al.,
2013) and GloVe (Pennington et al., 2014). While
word2vec is a prediction-based approach that opti-
mizes the probability of a word occurring in a cer-
tain context, GloVe is a counting approach based
on co-occurrences of words.

We compared the two on the lexical substitution
task designed for GermEval 2015 (Miller et al.,
2015). The task can be seen as related to recogniz-
ing Givenness: deciding what a good substitute for
a word in context is requires similar mechanisms
to deciding whether the meaning of a word is al-
ready present in previous utterances. For GloVe,
we used the models trained by Dima (2015), which
were also trained on a large German web corpus
and were shown to perform well. However, re-
sults on the lexical substitution task put both of
word2vec’s training approaches, continuous bag-
of-words (CBOW) and skip-gram, ahead of GloVe
using the models previously mentioned, so we con-
tinued with word2vec.

Finally, to select the optimal training algorithm
for word2vec for our purpose, we again used the
GermEval task as a benchmark. We explored both
CBOW and skip-gram with negative sampling and
hierarchical softmax, yielding four combinations.
Among these, CBOW with hierarchical softmax
significantly outperformed all other combinations,
so we chose it as our training algorithm.

The German model we obtained has a vocabulary
of 1,825,306 words and uses 400 dimensions for
each, the latter being inspired by Iacobacci et al.
(2015).

5 Experiment and Quantitative Results

Now that we have a baseline content assessment
system (section 3) and a distributional model for
German (section 4) in place, we have all the com-
ponents to quantitatively and qualitatively evaluate

212



the idea to model Givenness through semantic sim-
ilarity measures. To do so, we simply replaced
the surface-based Givenness filter of the base-
line CoMiC system with a distributional-semantics
based Givenness filter based on the model de-
scribed in the previous section. For this we must
make concrete, how exactly distributional-semantic
distances are used to determine the words in an an-
swer counting as GIVEN.

The parameters to be estimated relate to two dif-
ferent ways one can determine semantic relatedness
using word vectors for two words w1 and w2:

I. Calculate cosine similarity of w1 and w2 and
require it to be at least equal to a threshold t.

II. Calculate n nearest words to w1 and check
whether w2 is among them.

For the first method, one needs to estimate the
threshold t, while for the second method one needs
to determine how many neighbors to calculate (n).
We explored both methods. For the threshold pa-
rameter t, we experimented with values from 0.1 to
0.9 in increments of 0.1. For the number of nearest
neighbors n, we used a space from 2 to 20 with
increments of 2.

To cleanly separate our test data from the data
used for training and parameter estimation, we ran-
domly sampled approximately 20% of the CREG-
5K data set and set it aside as the final test set. The
remaining 80% was used as training set. All param-
eter estimation was done before running the final
system on the test set and using only the training
data.

Table 1 shows the results in terms of classifica-
tion accuracy for 10-fold cross-validation on the
training data. The table includes the performance
of the system without a Givenness filter as well
as with the basic surface-based approach. Train-
ing and testing was done separately for the two

KU OSU
# answers 1466 2670
Without Givenness 75.4% 76.7%
Surface Givenness 82.4% 83.0%
Best threshold t 0.3 0.5
Accuracy using t 82.7% 83.6%
Best n nearest-words 20 10
Accuracy using n 83.2% 83.6%

Table 1: Content Assessment results on training set

sub-corpora of CREG-5K corresponding to the uni-
versities where they were collected, KU and OSU.

First, the results confirm that an alignment-based
content assessment system such as CoMiC greatly
benefits from a Givenness filter, as demonstrated
by the big gap in performance between the no-
Givenness and surface-Givenness conditions. Sec-
ond, both the threshold method and the nearest-
words method outperform the surface baseline, if
only by a small margin.

Turning to the actual testing, we wanted to
find out whether the improvements found for the
distributional-semantic Givenness filters carry over
to the untouched test set. We trained the classifier
on the full training set and used the best parameters
from the training set. The results thus obtained are
summarized in Table 2.

KU OSU
# answers 348 654
No Givenness 74.7% 74.2%
Surface Givenness 80.7% 81.2%
Accuracy using t 81.0% 81.8%
Accuracy using n 81.9% 81.0%

Table 2: Content Assessment results on test set

We can see that results on the test set are gen-
erally lower, but the general picture for the test
set is the same as what we found for the 10-fold
CV on the training data: Surface-based-Givenness
easily outperforms the system not employing a
Givenness filter, and at least one of the systems
employing a distributional semantic Givenness
filter (marginally) outperforms the surface-based
method.

Interestingly, the two data sets seem to differ in
terms of which relatedness method works best for
recognizing Givenness: while the threshold method
works better for OSU, the n-nearest-words method
is the optimal choice for the KU data set. This may
be due to the fact that the OSU data set is generally
more diverse in terms of lexical variation and thus
presents more opportunities for false positives, i.e.,
words that are somewhat related but should not be
counted as given. Such cases are better filtered out
using a global threshold. The KU data set, on the
other hand, contains less variation and hence prof-
its from the more local n-nearest-words method,
which always returns a list of candidates for any
known word in the vocabulary, no matter whether
the candidates are globally very similar or not.

213



6 Qualitative Discussion

While the quantitative results provide a useful ball-
park measure of how well a Givenness filter based
on distributional semantics performs and that it can
improve the content assessment of reading com-
prehension questions, the relatively small and het-
erogeneous nature of the data set for a complex
task such as the content assessment of reading com-
prehension means that such quantitative results by
themselves are best interpreted cautiously. For the
conceptual side of our proposal, it is more inter-
esting to see whether semantic similarity can ad-
equately capture the different types of Givenness
that we discussed in section 2.

6.1 Successfully identifying Givenness
through distributional semantics

To illustrate how exactly the Givenness filter in the
CoMiC system ensures that only the material that
is not already present in the question is aligned for
assessing the similarity of a student and a target
answer, let us start by taking a look at a simple ex-
ample from CREG where the answers repeat lexical
material from the question, as shown Figure 1.

Q: Wer
who

war
was

an
at

der
the

Tür
door

?

Identifying
Givenness

TA: Drei
three

Soldaten
soldiers

waren
were

an
at

der
the

Tür.
door

Alignment for
Assessment

SA: Drei
three

Soldaten
soldiers

waren
were

an
at

der
the

Tür.
door

Figure 1: Simple Givenness alignment

The dotted arrows show which words in the ques-
tion trigger Givenness marking of which items in
the target and the student answer. The solid arrows
illustrate the actual alignments between words in
the target and the student answer used in the con-
tent assessment.

The Givenness filter ensures that the words
waren (was), an (at), der (the), and Tür (door)
of the student (SA) and the target (TA) answers
are marked as GIVEN with respect to the question
and are thus not aligned in order to calculate the

similarity of the two answers.
A type of Givenness that a surface-based Given-

ness filter cannot handle, but that is captured by our
distributional similarity approach, occurs in exam-
ples where parts of the question are picked up by
semantically similar words in the target and student
answer. This is illustrated by Figure 2.

The verbs glaubte (believed) and meinte
(thought) are semantically close enough to the verb
verstand (understood) in the question for them to
be identified as GIVEN. They consequently can be
excluded from the content assessment of the stu-
dent answer (SA) in relation to the target answer
(TA).

The core idea to use semantic similarity as iden-
tified by distributional semantics to identify the
words which are GIVEN in a context thus nicely
captures real cases in authentic data.

6.2 Overidentifying Givenness

At the same time, there are two aspects of distribu-
tional semantics that can also lead to overidentifi-
cation of Givenness.

Entailment is not symmetric, but semantic sim-
ilarity and relatedness are The first difficulty
arises from the fact that semantic similarity and
semantic relatedness are symmetric, whereas the
entailment relation used to define Givenness is
not. As a result, our distributional semantic model
wrongly identifies a word as GIVEN that is more
specific than, i.e., a hyponym of the word in the
context as illustrated in Figure 3.

The entire NP praktische Erfahrung im Control-
ling eines Finanzservice-Unternehmens (practical
experience in controlling of a financial service com-
pany) consists of new material in both the target
answer and the student answer and should thus be
aligned for the content assessment of the student
answer. But since Finanzservice-Unternehmen (fi-
nancial service company) is semantically similar
to the noun Firma (company) occurring in the ques-
tion, it is marked as GIVEN under the current set-
ting of our distributional similarity approach and
incorrectly excluded from the content assessment.

Under the notion of Givenness as defined by
Schwarzschild, Finanzservice-Unternehmen (finan-
cial service company) would not count as GIVEN,
since the mentioning of company in the prior dis-
course does not entail the existence of a financial
service company.

214



Q: Wie
how

verstand
understood

Julchen
Julchen

die
the

Silbergeschichte?
silver story

Identifying Givenness

TA: Sie
she

glaubte
believed

, irgendjemand
someone

war
had

gekommen
come

und
and

hatte
had

den
the

Puppenwagen
doll’s pram

gebracht.
brought

[...]

Alignment
for Assessment

SA: Julchen
Julchen

meinte
thought

, dass
that

irgendjemand
someone

hatte
had

den
the

Puppenwagen
doll’s pram

gebracht
brought

, [...]

Figure 2: CREG example illustrating Semantic similarity

Q: Welche
which

Qualifikationen
qualifications

sind
are

der
for the

Firma
company

wichtig
important

?

Identifying Givenness

TA: Praktische
practical

Erfahrung
experience

im
in

Controlling
controlling

eines
of a

Finanzservice-Unternehmens
financial service company

Alignment for Assessment

SA: Ein
a

Mann
man

musste
had to

praktische
practical

Erfahrung
experience

im
in

Controlling
controlling

eines
of a

Finanzservice-Unternehmens
financial service company

haben.
have

Figure 3: CREG example illustrating entailment in wrong direction

Q: Von
by

wem
whom

wird
is

der
the

Vorstand
managm. board

gewählt?
elected

Identifying Givenness

TA: Der
the

Vorstand
board

wird
is

vom
by

Aufsichtsrat
superv. board

gewählt
elected

Alignment for Assessment

SA: Der
the

Vorstand
m. board

wird
is

vom
by

Aufsichtsrat
superv. board

gewählt
elected

Figure 4: CREG example illustrating Semantic Relatedness

215



Q: Ist
is

die
the

Wohnung
flat

in
in

einem
a

Neubau
new building

oder
or

einem
an

Altbau?
old building

Identifying Givenness

TA: Die
The

Wohnung
flat

ist
is

in
in

einem
a

Neubau.
new building

Alignment for Assessment

SA: Die
The

Wohnung
flat

ist
is

in
in

einem
a

Neubau.
new building

Figure 5: CREG example illustrating overidentification by Givenness filter

Semantic relatedness is not semantic similarity
Second, it is difficult for distributional semantic ap-
proaches to distinguish semantic similarity from se-
mantic relatedness (cf., e.g., Kolb, 2009). In the dis-
cussion of bridging in section 2 we saw that cases
such as (4) could arguably benefit from the use of
semantic relatedness to identify Givenness. Yet,
allowing all semantic related material to count as
GIVEN clearly overestimates what counts as GIVEN
and can therefore be deaccented. As a result, our
approach wrongly identifies some semantic related-
ness cases as Givenness. Consider the semantically
related words Vorstand (management board) and
Aufsichtsrat (supervisory board) in the example
shown in Figure 4.

The Givenness filter ensures that the lexical ma-
terial der (the), Vorstand (management board),
wird (is), gewählt (elected) that is repeated in the
answers is marked as GIVEN and thus excluded
from the content assessment. But under the current
setting of our distributional similarity approach,
the noun Aufsichtsrat (supervisory board) that is
semantically related to the noun Vorstand (advisory
board) is also marked as GIVEN and thus excluded
from the content assessment. As a consequence
all material in the answers is excluded from the
alignment and the CoMiC system fails to classify
the student answer as a correct answer. A general
solution to this kind of misidentification seems to
be beyond the scope of an analysis based on the
word level – an issue which also turns out to be a
problem in another, systematic set of cases, which
we turn to next.

Comparing lexical units not enough The
Givenness filter under both approaches, surface-
based Givenness as well as distributional similarity,

sometimes also overidentifies Givenness because
the analysis is based on lexical units rather than
entailment between sentence meanings. Recall that
the way this filter works is to exclude tokens from
alignment which are GIVEN in the question. But
what if the lexical material required by the question
is actually explicitly spelled out as an option by
the question itself? This actually happens system-
atically for alternative questions, where one has
to pick one out of an explicitly given set of alter-
natives. Consider the example in Figure 5, where
target and student answer happen to be identical
(and for visual transparency only the arcs between
question and target answer are shown, not also the
identical arcs that link the question and the student
answer).

The question asks whether the apartment is in
a new or in an old building. Both alternatives are
GIVEN in the question, however only one is correct,
namely that the apartment is in a new building. The
student correctly picked that alternative, but the
Givenness filter excludes all material from align-
ment for content assessment. Hence, classification
fails to mark this as a correct answer. As a simple
fix, one could integrate an automatic identification
of question types and switch off the Givenness filter
for alternative questions. More interesting would
be an approach that explores when material pro-
vided by the question constitutes alternatives in the
sense of focus alternatives (Krifka, 2007), from
which a selection in the answer should be counted
as informative. This essentially would replace the
Givenness filter with an approach zooming in to the
material in Focus in the answer in the context of
the question. At the same time, realizing this idea
would require development of an approach auto-
matically identifying Focus, an alternative avenue

216



to pursue in future research.

7 Conclusion

The paper investigated how the formal pragmatic
notion of Givenness can be approximated using cur-
rent computational linguistic methods, and whether
this can capture a number of distinct conceptual
subcases. We tested the idea in a real-life compu-
tational linguistic task with an established external
evaluation criterion, content assessment of learner
answers to reading comprehension questions.

In place of a surface-based Givenness filter as
employed in previous content assessment work, we
developed an approach based on distributional se-
mantics to check whether a word in an answer is
similar enough to a word in the question to count
as GIVEN. The quantitative evaluation confirms the
importance of a Givenness filter for content assess-
ment and improved content assessment accuracy
for the distributional approach. We experimented
with absolute cosine similarity thresholds and with
calculating the nearest n words for a candidate
word and found that which of the two works better
potentially depends on data set characteristics such
as lexical diversity.

In the qualitative evaluation, we confirmed that
the approximation of Givenness through semantic
similarity does indeed capture a number of con-
ceptual cases that a pure surface-based Givenness
approach cannot handle, such as bridging-cases in-
volving semantically related words – though this
can also lead to over-identification. In future re-
search, integrating more context-sensitive notions
of semantic similarity, such as proposed by Dinu
and Lapata (2010), may provide a handle on a more
narrowly contextualized notion of Givenness in the
common ground of discourse participants.

Acknowledgments

We would like to thank Mohamed Balabel for his
work on training and selecting the distributional
model. Furthermore, we are grateful to the anony-
mous reviewers of JerSem and *Sem for their com-
ments, which were very helpful in revising the pa-
per.

References

Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web: A

collection of very large linguistically processed web-
crawled corpora. Journal of Language Resources
and Evaluation, 3(43):209–226.

Daniel Büring. 2007. Intonation, semantics and infor-
mation structure. In Gillian Ramchand and Charles
Reiss, editors, The Oxford Handbook of Linguistic
Interfaces. Oxford University Press.

Aoife Cahill and Arndt Riester. 2012. Automati-
cally acquiring fine-grained information status dis-
tinctions in german. In Proceedings of the 13th An-
nual Meeting of the Special Interest Group on Dis-
course and Dialogue, pages 232–236. Association
for Computational Linguistics.

Sasha Calhoun, Jean Carletta, Jason Brenier, Neil
Mayo, Dan Jurafsky, Mark Steedman, and David
Beaver. 2010. The NXT-format switchboard cor-
pus: A rich resource for investigating the syntax, se-
mantics, pragmatics and prosody of dialogue. Lan-
guage Resources and Evaluation, 44:387–419.

Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch, 2007. TiMBL: Tilburg
Memory-Based Learner Reference Guide, ILK Tech-
nical Report ILK 07-03. Induction of Linguistic
Knowledge Research Group Department of Commu-
nication and Information Sciences, Tilburg Univer-
sity, Tilburg, The Netherlands, July 11. Version 6.0.

Kordula De Kuthy, Ramon Ziai, and Detmar Meurers.
2016. Focus annotation of task-based data: a com-
parison of expert and crowd-sourced annotation in
a reading comprehension corpus. In Proceedings
of the 10th Edition of the Language Resources and
Evaluation Conference (LREC).

Corina Dima. 2015. Reverse-engineering language: A
study on the semantic compositionality of german
compounds. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1637–1642, Lisbon, Portugal, September.
Association for Computational Linguistics.

Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing distributional similarity in context. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1162–1172,
Cambridge, MA, October. Association for Computa-
tional Linguistics.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
In The SIGKDD Explorations, volume 11, pages 10–
18.

Michael Halliday. 1967. Notes on transitivity and
theme in english. part 1 and 2. Journal of Linguis-
tics, 3:37–81, 199–244.

Christian F. Hempelmann, David Dufty, Philip M.
McCarthy, Arthur C. Graesser, Zhiqiang Cai, and
Danielle S. McNamara. 2005. Using LSA to au-
tomatically identify Givenness and Newness of noun

217



phrases in written discourse. In B. G. Bara, L. Barsa-
lou, and M. Bucciarelli, editors, Proceedings of the
27th Annual Meeting of the Cognitive Science Soci-
ety, pages 941–949, Stresa, Italy. Erlbaum.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. Sensembed: learning sense
embeddings for word and relational similarity. In
Proceedings of ACL, pages 95–105.

Peter Kolb. 2009. Experiments on the difference be-
tween semantic similarity and relatedness. In Kristi-
ina Jokinen and Eckhard Bick, editors, Proceedings
of the 17th Nordic Conference on Computational
Linguistics (NODALIDA), pages 81–88.

Manfred Krifka. 2007. Basic notions of information
structure. In Caroline Fery, Gisbert Fanselow, and
Manfred Krifka, editors, The notions of information
structure, volume 6 of Interdisciplinary Studies on
Information Structure (ISIS), pages 13–55. Univer-
sitätsverlag Potsdam, Potsdam.

Detmar Meurers, Ramon Ziai, Niels Ott, and Janina
Kopp. 2011. Evaluating answers to reading compre-
hension questions in context: Results for German
and the role of information structure. In Proceed-
ings of the TextInfer 2011 Workshop on Textual En-
tailment, pages 1–9, Edinburgh, July. ACL.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Tristan Miller, Darina Benikova, and Sallam Abual-
haija. 2015. GermEval 2015: LexSub – A shared
task for German-language lexical substitution. In
Proceedings of GermEval 2015: LexSub, pages 1–9,
sep.

Michael Mohler, Razvan Bunescu, and Rada Mihal-
cea. 2011. Learning to grade short answer questions
using semantic similarity measures and dependency
graph alignments. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
752–762, Portland, Oregon, USA, June. Association
for Computational Linguistics.

Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of the 2006 Con-
ference on Emprical Methods in Natural Language
Processing, Sydney, Australia.

Niels Ott, Ramon Ziai, and Detmar Meurers. 2012.
Creation and analysis of a reading comprehension
exercise corpus: Towards evaluating meaning in con-
text. In Thomas Schmidt and Kai Wörner, edi-
tors, Multilingual Corpora and Multilingual Cor-
pus Analysis, Hamburg Studies in Multilingualism
(HSM), pages 47–69. Benjamins, Amsterdam.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543. Associa-
tion for Computational Linguistics.

Altaf Rahman and Vincent Ng. 2011. Learning the
information status of noun phrases in spoken dia-
logues. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1069–1080, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.

Julia Ritz, Stefanie Dipper, and Michael Götze. 2008.
Annotation of information structure: An evaluation
across different types of texts. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation, pages 2137–2142, Mar-
rakech, Morocco.

Björn Rudzewitz. 2016. Exploring the intersection
of short answer assessment, authorship attribution,
and plagiarism detection. In Proceedings of the 11th
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, San Diego, CA.

Roger Schwarzschild. 1999. GIVENness, AvoidF and
other constraints on the placement of accent. Natu-
ral Language Semantics, 7(2):141–177.

Ramon Ziai and Detmar Meurers. 2014. Focus an-
notation in reading comprehension data. In Pro-
ceedings of the 8th Linguistic Annotation Workshop
(LAW VIII, 2014), pages 159–168, Dublin, Ireland.
COLING, Association for Computational Linguis-
tics.

218


