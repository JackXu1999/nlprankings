



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 1–6
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2001

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 1–6
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2001

Classifying Temporal Relations by Bidirectional LSTM over Dependency
Paths

Fei Cheng and Yusuke Miyao
Research Center for Financial Smart Data

National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan
{fei-cheng, yusuke}@nii.ac.jp

Abstract

Temporal relation classification is becom-
ing an active research field. Lots of meth-
ods have been proposed, while most of
them focus on extracting features from
external resources. Less attention has
been paid to a significant advance in a
closely related task: relation extraction.
In this work, we borrow a state-of-the-art
method in relation extraction by adopting
bidirectional long short-term memory (Bi-
LSTM) along dependency paths (DP). We
make a “common root” assumption to ex-
tend DP representations of cross-sentence
links. In the final comparison to two state-
of-the-art systems on TimeBank-Dense,
our model achieves comparable perfor-
mance, without using external knowledge
and manually annotated attributes of enti-
ties (class, tense, polarity, etc.).

1 Introduction

Recently, the need for extracting temporal infor-
mation from text is motivated rapidly by many
NLP tasks such as: question answering (QA), in-
formation extraction (IE), etc. Along with the
TimeBank1 (Pustejovsky et al., 2003) and other
temporal information annotated corpora, a se-
ries of temporal evaluation challenges (TempEval-
1,2,3) (Verhagen et al., 2009, 2010; UzZaman
et al., 2012) are attracting growing research ef-
forts.

Temporal relation classification is a task to iden-
tify the pairs of temporal entities (events or tem-
poral expressions) that have a temporal link and
classify the temporal relations between them. For
instance, we show an event-event (E-E) link with
‘DURING’ type in (i), an event-time (E-T) link

1https://catalog.ldc.upenn.edu/LDC2006T08

with ‘INCLUDES’ type in (ii) and an event-DCT
(document creation time, E-D) with ‘BEFORE’
type in (iii).

(i) There was no hint of trouble in the last con-
versation between controllers and TWA pilot
Steven Snyder.

(ii) In Washington today, the Federal Aviation
Administration released air traffic control
tapes.

(iii) The U.S. Navy has 27 ships in the maritime
barricade of Iraq.

Marcu and Echihabi (2002) propose an ap-
proach considering word-based pairs as useful fea-
tures. The following researchers (Laokulrat et al.,
2013; Chambers et al., 2014; Mani et al., 2006;
D’Souza and Ng, 2013) focus on extracting lex-
ical, syntactic or semantic information from var-
ious external knowledge bases such as: Word-
Net (Miller, 1995) and VerbOcean (Chklovski and
Pantel, 2004). However, these feature based meth-
ods rely on hand-crafted efforts and external re-
sources. In addition, these works require the fea-
tures of entity attributes (class, tense, polarity,
etc.), which are manually annotated to achieve
high performance. Consequently, they are hard to
obtain in practical application scenarios.

In relation extraction, there is an explosion of
the works done with the dependency path (DP)
based methods, which employ various models
along dependency paths (Bunescu and Mooney,
2005; Plank and Moschitti, 2013). In recent years,
the DP-based neural networks (Socher et al., 2011;
Xu et al., 2015a,b) show state-of-the-art perfor-
mance, with less requirements on explicit features.
Intuitively, the DP-based approaches have the po-
tential to classify temporal relations.

Both relation extraction and temporal relation
classification require the identification of relation-

1

https://doi.org/10.18653/v1/P17-2001
https://doi.org/10.18653/v1/P17-2001


Figure 1: An example of the sentences with entity attributes annotated in TimeBank.

ship between entities in texts. However, temporal
relation classification is more challenging, since it
includes three different type of entities: ‘event’,
‘time expression’ and DCT. Cross-sentence links
also add additional complexity into the task. Due
to the outstanding performance of DP-based neu-
ral networks revealed in relation extraction, we
borrow this state-of-the-art approach to temporal
relation classification.

In Section 2 of this paper, we review related
work and introduce TimeBank-Dense. We dis-
cuss the cross-sentence link problem and the ar-
chitectures of our E-E, E-T and E-D classifiers in
Section 3. In Section 4, the experiments are per-
formed on TimeBank-Dense and we compare our
model to the baseline and two state-of-the-art sys-
tems. The final conclusion is made in Section 5.

2 Background

2.1 Related Work

Current state-of-the-art temporal relation classi-
fiers exploit a variety of features. Laokulrat et al.
(2013); Chambers et al. (2014) extract lexical
and morphological features derived from Word-
Net synsets. Mani et al. (2006); D’Souza and
Ng (2013) incorporate semantic relations between
verbs from VerbOcean as features. In addition,
most of the systems include the entity attributes
(Figure 1) specified in TimeML 2 as basic features,
which actually need heavy human annotations.

In this work, we push this work into a more
practical level by using only word, part-of-speech
(POS), dependency parsing information, without
incorporating entity attributes, as well as any other
external resources.

In relation extraction, Bunescu and Mooney
(2005) propose an observation that a relation
can be captured by the shortest dependency path

2http://timeml.org/

Figure 2: An example of the DP representation of
a cross-sentence link between the two sentences in
Figure 1.

(SDP) between the two entities in the entire depen-
dency graph. Plank and Moschitti (2013) extract
syntactic and semantic information in a tree ker-
nel. Following this line, researchers (Socher et al.,
2011; Xu et al., 2015a,b) achieve state-of-the-art
performance by building various neural networks
over dependency path.

Our system is similar to the work by Xu et al.
(2015b). They perform LSTM with max pooling
separately on each feature channel along depen-
dency path. In contrast, our system adopts bidi-
rectional LSTM on the concatenation of feature
embeddings.

2.2 TimeBank-Dense

In the original TimeBank, temporal links have
been created on those pairs with semantic connec-
tions, which led to a sparse annotation style. Cas-
sidy et al. (2014) 3 propose a mechanism to force
annotators to create complete graphs over the enti-
ties in neighboring sentences. Compared to 6,418
links in 183 TimeBank documents, TimeBank-
Dense achieves greater density with 12,715 links
in 36 documents.

We follow a similar experiment setting to the
other two systems (Mirza and Tonelli, 2016;
Chambers et al., 2014) with the same 9 documents

3https://www.usna.edu/Users/cs/nchamber/caevo

2



as test data and the others as training data (15%
of training data is split as validation data for early
stopping).

3 The Proposed Method

3.1 Cross-sentence Dependency Paths
Intuitively, the dependency path based idea can
be introduced into the temporal relation classifica-
tion task. However, around 64% E-E, E-T links in
TimeBank-Dense are with the ends in two neigh-
boring sentences, called cross-sentence links.

A crucial obstacle is how to represent the depen-
dency path of a cross-sentence link. In this work,
we make a naive assumption that two neighbor-
ing sentences share a “common root”. Therefore,
a cross-sentence dependency path can be repre-
sented as two shortest dependency path branches
from the ends to the “common root”, as shown in
Figure 2.

Stanford CoreNLP4 is used to parsing syntactic
structures of sentences in this work.

3.2 Temporal Relation Classifiers
Long short-term memory (LSTM) (Hochreiter and
Schmidhuber, 1997) is a natural choice for pro-
cessing sequential dependency paths. As the re-
versed order also takes useful information, a back-
ward representation can be achieved by feed-
ing LSTM with the same input in reverse. We
adopt the concatenation of the forward and back-
ward LSTMs outputs, referred to as bidirectional
LSTM (Graves and Schmidhuber, 2005).

Figure 3a shows the neural network architec-
ture of our E-E, E-T classifier. Given an E-E or
E-T temporal link, our system first generates two
SDP branches: 1) the source entity to common
root, 2) the target entity to common root. For
each word along a SDP branch, concatenation of
word, POS and dependency relation (DEP) em-
beddings (word-level) is fed into Bi-LSTM. The
forward and backward outputs of both source and
target branches are all concatenated, and fed into a
fully connected hidden units layer. The final Soft-
max layer generates multi-class predictions. Since
an E-D link contains single event SDP branch,
our system applies a similar architecture, but with
single branch Bi-LSTM with outputs fed into the
penultimate hidden layer, as shown in Figure 3b.

In this work, we use word2vec5 (Mikolov et al.,
4http://stanfordnlp.github.io/CoreNLP/
5https://code.google.com/archive/p/word2vec/

LINK type E-D E-E E-T
AFTER .493 .477 .350
BEFORE .552 .380 .311
SIMULTANEOUS - - -
INCLUDES .305 .185 .254
IS INCLUDED .513 .296 .204
VAGUE .482 .656 .616
Overall .491 .544 .480

Table 1: The best sentence-level 5-fold CV per-
formance (Micro-average Overall F1-score).

2013a,b) to train 200-dimensions word embed-
dings on English Gigaword 4th edition with skip-
gram model and other default settings. For ei-
ther of POS or DEP, we adopt the 50-dimensions
lookup table initialized randomly.

4 Experiments

4.1 Hyper-parameters and Cross-validation

The grid search exploring a full hyper-parameter
space takes time for three classifiers (E-E, E-T and
E-D). Empirically, we set each single LSTM out-
put with the same dimensions (equal to 300) as the
concatenation of word, POS, DEP embeddings.
The hidden layer is set as 200-dimensions.

Our system adopts dependency paths as input,
which means that the entities in the same sen-
tences contain highly covered word sequence in-
put. Simple cross-validation (CV) on links can not
reflect the generalization ability of our model cor-
rectly. We use a grouped 5-fold CV based on the
source entity ids (document id + sentence id) of
links. This schema can reduce bias separately in
either the source SDP or the target SDP. Although
document level CV can avoid this issue, it’s not
feasible for TimeBank-Dense because it contains
only 27 training documents.

Early stopping is used to save the best model
based on the validation data. In each run of the
5-fold cross-validation, we split 80% of ‘original
training’ as ‘tentative training’ and 20% as ‘ten-
tative test’. 85% of ‘tentative training’ is used to
learning and 15% is used for validation. We also
adopt early stopping in the final system on the val-
idation data (15% of ‘original training’). The pa-
tience is set as 10.

Dropout (Srivastava et al., 2014) recently is
proved to be an useful approach to prevent neu-
ral networks from over-fitting. We adopt dropout

3



(a) E-E, E-T classifier (b) E-D classifier

Figure 3: The DP-based Bi-LSTM temporal relation classifier.

Our Mirza Our Mirza
LINK type E-D E-D E-E E-E
AFTER .582 .466 .440 .430
BEFORE .634 .671 .460 .471
SIMULTA. - - - -
INCLUDES .056 .250 .025 .049
IS INCLUD. .595 .600 .170 .250
VAGUE .526 .502 .624 .613
Overall .546 .534 .529 .519

Table 2: The detailed comparison of E-E and E-T
against relation types to Mirza and Tonelli (2016)
(Micro-average Overall F1-score) on test data.

separately after the following layers: embeddings,
LSTM, and hidden layer to investigate the impact
of dropout on performance. Table 1 shows the best
CV results recorded in tuning dropout. The hyper-
parameter setting with the best CV performance is
adopted in the final system.

4.2 Overall Performance

Recently, Mirza and Tonelli (2016) report state-of-
the-art performance on TimeBank-Dense. They
show the new attempt to mine the value of low-
dimensions word embeddings by concatenating
them with sparse traditional features. Their tra-
ditional features include entity attributes, tempo-
ral signals, semantic information of WordNet, etc.,
which means it’s a hard setting for challenging
their performance. In Table 2 and 3, ‘Mirza’ de-
notes their system.

Table 2 shows the detailed comparison to

Systems E-D E-E E-T Overall
Baseline .471 .502 .437 .486
Proposed .546 .529 .471 .520
Mirza .534 .519 .468 .512
CAEVO .553 .494 .494 .502

Table 3: The final comparison of E-E, E-T and E-
D to the baseline and two state-of-the-art systems
on test data.

their work. Our system achieves higher perfor-
mance on ‘AFTER’, ‘VAGUE’, while lower on
‘BEFORE’, ‘INCLUDES’ (5% of all data) and
‘IS INCLUDED’ (4% of all data). It is likely that
their rich traditional features help the classifiers to
capture more minority-class links. On the whole,
our system reaches better ‘Overall’ on both E-E
and E-D. As their E-T classifier does not include
word embeddings, the E-T results are not listed.

The final comparison is shown in Table 3. An
one-layer fully connected hidden units baseline
(200-dimensions) with word, POS embeddings as
input (without any dependency information) is
provided. The significant out-performance of our
proposed model over the baseline indicates the ef-
fectiveness of the dependency path information
and our Bi-LSTM in classifying temporal links.
As a hybrid system, ‘CAEVO’ (Chambers et al.,
2014) includes hand-crafted rules for their E-T and
E-D classifiers. For instance, the temporal prepo-
sitions in, on, over, during, and within indicate
‘IN INCLUDED’ relations. Their system is supe-
rior in E-T and E-D. ’Miza’ takes the pure feature-

4



based methods and performs slightly better in E-
E and overall, compared to ‘CAEVO’. Our sys-
tem shows the highest scores in E-E and overall
among the four systems. In general, our system
achieves comparable performance to two state-of-
the-art systems, without using any hand-crafted
features, rules, or external resources.

5 Conclusion

We borrow the idea of the dependency path based
neural networks into temporal relation classifica-
tion. A “common root” assumption adapts our
model to cross-sentence links. Our model adopts
bidirectional LSTM for capturing both forward
and backward orders information. We observe
the significant benefit of the DP-based Bi-LSTM
model by comparing it to the baseline. Our model
achieves comparable performance to two state-of-
the-art systems without using any explicit features
(class, tense, polarity, etc.) or external resources,
which indicates that our model can capture such
information automatically.

6 Acknowledgments

We thank the anonymous reviewers for the insight-
ful comments.

References
Razvan Bunescu and Raymond Mooney. 2005. A

shortest path dependency kernel for relation ex-
traction. In Proceedings of Human Language
Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing.
Association for Computational Linguistics, Van-
couver, British Columbia, Canada, pages 724–
731. http://www.aclweb.org/anthology/H/H05/H05-
1091.

Taylor Cassidy, Bill McDowell, Nathanael Cham-
bers, and Steven Bethard. 2014. An annotation
framework for dense event ordering. In Pro-
ceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume
2: Short Papers). Association for Computational
Linguistics, Baltimore, Maryland, pages 501–506.
http://www.aclweb.org/anthology/P14-2082.

Nathanael Chambers, Taylor Cassidy, Bill McDowell,
and Steven Bethard. 2014. Dense event ordering
with a multi-pass architecture. Transactions of the
Association for Computational Linguistics 2:273–
284. http://aclweb.org/anthology/Q/Q14/Q14-
1022.pdf.

Timothy Chklovski and Patrick Pantel. 2004. Ver-
bocean: Mining the web for fine-grained se-

mantic verb relations. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP
2004. Association for Computational Lin-
guistics, Barcelona, Spain, pages 33–40.
https://www.aclweb.org/anthology/W/W04/W04-
3205.pdf.

Jennifer D’Souza and Vincent Ng. 2013. Classify-
ing temporal relations with rich linguistic knowl-
edge. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Atlanta, Georgia, pages 918–927.
http://www.aclweb.org/anthology/N13-1112.

Alex Graves and Jürgen Schmidhuber. 2005.
Framewise phoneme classification with bidi-
rectional lstm and other neural network ar-
chitectures. Neural Networks 18(5):602–610.
https://doi.org/10.1016/j.neunet.2005.06.042.

Sepp Hochreiter and Jürgen Schmidhu-
ber. 1997. Long short-term memory.
Neural computation 9(8):1735–1780.
https://doi.org/10.1162/neco.1997.9.8.1735.

Natsuda Laokulrat, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Uttime: Tem-
poral relation classification using deep syntactic fea-
tures. In Second Joint Conference on Lexical and
Computational Semantics (* SEM). volume 2, pages
88–92. http://aclweb.org/anthology/S/S13/S13-
2015.pdf.

Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006.
Machine learning of temporal relations. In Pro-
ceedings of the 21st International Conference
on Computational Linguistics and 44th Annual
Meeting of the Association for Computational
Linguistics. Association for Computational Lin-
guistics, Sydney, Australia, pages 753–760.
https://doi.org/10.3115/1220175.1220270.

Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of 40th Annual Meet-
ing of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Philadelphia, Pennsylvania, USA, pages 368–375.
https://doi.org/10.3115/1073083.1073145.

Tomas Mikolov, Kai Chen, Greg Corrado,
and Jeffrey Dean. 2013a. Efficient es-
timation of word representations in vec-
tor space. arXiv preprint arXiv:1301.3781
http://arxiv.org/pdf/1301.3781v3.pdf.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. 2013b. Distributed
representations of words and phrases and their
compositionality. In Advances in neural in-
formation processing systems. pages 3111–3119.
https://arxiv.org/pdf/1310.4546.pdf.

5



George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM 38(11):39–
41. https://doi.org/10.1145/219717.219748.

Paramita Mirza and Sara Tonelli. 2016. On the con-
tribution of word embeddings to temporal relation
classification. In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers. The COLING 2016
Organizing Committee, Osaka, Japan, pages 2818–
2828. http://aclweb.org/anthology/C16-1265.

Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In Pro-
ceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Association for Computational
Linguistics, Sofia, Bulgaria, pages 1498–1507.
http://www.aclweb.org/anthology/P13-1147.

James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day,
Lisa Ferro, et al. 2003. The timebank cor-
pus. In Corpus linguistics. volume 2003, page 40.
https://catalog.ldc.upenn.edu/LDC2006T08.

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
2011 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Edinburgh, Scotland, UK., pages
151–161. http://www.aclweb.org/anthology/D11-
1014.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov.
2014. Dropout: a simple way to prevent
neural networks from overfitting. Journal of
Machine Learning Research 15(1):1929–1958.
http://jmlr.org/papers/v15/srivastava14a.html.

Naushad UzZaman, Hector Llorens, James Allen,
Leon Derczynski, Marc Verhagen, and James
Pustejovsky. 2012. Tempeval-3: Evaluat-
ing events, time expressions, and temporal
relations. arXiv preprint arXiv:1206.5333
http://aclweb.org/anthology/S/S13/S13-2001.pdf.

Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James
Pustejovsky. 2009. The tempeval challenge:
identifying temporal relations in text. Lan-
guage Resources and Evaluation 43(2):161–179.
https://doi.org/10.1007/s10579-009-9086-z.

Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th interna-
tional workshop on semantic evaluation. Associa-
tion for Computational Linguistics, pages 57–62.
http://www.aclweb.org/anthology/S10-1010.

Kun Xu, Yansong Feng, Songfang Huang, and
Dongyan Zhao. 2015a. Semantic relation clas-
sification via convolutional neural networks with
simple negative sampling. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Lisbon, Portugal, pages 536–540.
http://aclweb.org/anthology/D15-1062.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao
Peng, and Zhi Jin. 2015b. Classifying relations
via long short term memory networks along short-
est dependency paths. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Lisbon, Portugal, pages 1785–1794.
http://aclweb.org/anthology/D15-1206.

6


	Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths

