



















































Towards Abstractive Multi-Document Summarization Using Submodular Function-Based Framework, Sentence Compression and Merging


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 418–424,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Towards Abstractive Multi-Document Summarization Using Submodular
Function-Based Framework, Sentence Compression and Merging

Yllias Chali
University of Lethbridge

Lethbridge, Alberta, Canada
chali@cs.uleth.ca

Moin Tanvee
University of Lethbridge

Lethbridge, Alberta, Canada
tanvee@uleth.ca

Mir Tafseer Nayeem
University of Lethbridge

Lethbridge, Alberta, Canada
mir.nayeem@uleth.ca

Abstract

We propose a submodular function-based
summarization system which integrates
three important measures namely impor-
tance, coverage, and non-redundancy to
detect the important sentences for the sum-
mary. We design monotone and submod-
ular functions which allow us to apply an
efficient and scalable greedy algorithm to
obtain informative and well-covered sum-
maries. In addition, we integrate two
abstraction-based methods namely sen-
tence compression and merging for gener-
ating an abstractive sentence set. We de-
sign our summarization models for both
generic and query-focused summarization.
Experimental results on DUC-2004 and
DUC-2007 datasets show that our generic
and query-focused summarizers have out-
performed the state-of-the-art summariza-
tion systems in terms of ROUGE-1 and
ROUGE-2 recall and F-measure.

1 Introduction

Existing multi-document summarization tech-
niques mainly fall into two categories: extrac-
tive and abstractive. Extractive approach selects
important source sentences to cover the overall
concepts of the document set (Erkan and Radev,
2004; Lin and Bilmes, 2010; Boudin et al., 2015;
Parveen and Strube, 2015; Parveen et al., 2015;
Cheng and Lapata, 2016; Nallapati et al., 2017).
This method is very popular because of its sim-
plicity and speed. But it mostly generates less con-
densed summaries with redundant information.
On the other hand, abstractive summarization is
a way of natural language generation and using
this approach, it is possible to produce human-like
summaries (Rush et al., 2015; Chopra et al., 2016;

Wang and Ling, 2016). It requires deep language
understanding. Though this technique is complex
and less popular than the extractive approach, it
is possible to produce more informative and fluent
summary. For generating abstractive summaries,
researchers often try to modify the candidate sen-
tences by either shortening and compressing it
(Knight and Marcu, 2000; Berg-Kirkpatrick et al.,
2011; Filippova et al., 2015) or by merging several
sentences which is called sentence fusion (Barzi-
lay and McKeown, 2005; Cheung and Penn, 2014;
Bing et al., 2015).

In this paper, we divide the whole task of sum-
marization in two main phases: document shrink-
ing and summarization. In the first phase, we ap-
ply sentence compression and merging to produce
concise and new candidate sentences for the sum-
mary. In the second phase, we represent sum-
marization as a submodular function maximiza-
tion problem under budgeted constraints. While
generating summaries, our system considers three
important measures namely importance, coverage,
and non-redundancy to ensure summary quality.
We design three submodular functions for each
these measures. The importance property of the
summary considers how much relevant informa-
tion present in a summary. The coverage measure
ranks the sentences based on the fact of how rep-
resentative they are of the document cluster. The
third objective function is designed for measur-
ing non-redundancy of the summaries. This met-
ric assigns a score to a sentence based on how
many distinct concepts it contains and how dis-
similar it is with the other summary sentences. We
design the summarization model for both generic
and query-focused summarization. Finally, a mod-
ified greedy algorithm is applied which obtains
near optimal summaries guaranteed to be within
(1− 1/√e) of the optimal solution.

418



2 Related Work

Most of the research on document summarization
are extractive which principally based on two im-
portant objectives, namely maximizing the rele-
vance and minimizing the redundancy (Carbonell
and Goldstein, 1998; Erkan and Radev, 2004). Be-
sides, formulation of summarization as a maxi-
mum coverage problem with knapsack constraint
(MCKP) (Takamura and Okumura, 2009; Morita
et al., 2011) have been used. Recently, summa-
rization has also been considered as a submodu-
lar function maximization (Lin and Bilmes, 2010,
2011; Dasgupta et al., 2013) where greedy algo-
rithms were adopted to achieve near optimal sum-
maries. However, the main drawback of all the ex-
tractive approaches is that they can not avoid the
inclusion of insignificant information which de-
grades the summary quality.

On the other hand, the abstractive approach
in a multi-document setting aims at generating
summaries by deeply understanding the contents
of the document set and rewriting the most rel-
evant information in natural language. Two re-
cent abstractive techniques are most commonly
used to accomplish the task: sentence compres-
sion (Knight and Marcu, 2000) and sentence fu-
sion (Barzilay and McKeown, 2005). In the re-
cent years, sentence compression is jointly used
with the extractive system to improve summary
quality (Berg-Kirkpatrick et al., 2011; Martins and
Smith, 2009). In addition, sentence fusion-based
models have also been proposed where sentence
fragments from multiple sentences are combined
to cover more information in a concise manner
(Barzilay and McKeown, 2005; Filippova et al.,
2015; Ganesan et al., 2010; Thadani and McKe-
own, 2013; Cheung and Penn, 2014; Bing et al.,
2015).

3 Document Shrinking

In this phase, we used sentence compression and
sentence merging to prepare a better and more
concise document set before approaching the ac-
tual summarization task.

3.1 Sentence Compression
Sentence compression is a technique of shorten-
ing sentences which can be used with the extrac-
tive system to improve summary quality. Consider
the following example sentence as a candidate sen-
tence of the summary:

“According to a newspaper report, a total of
4,299 political opponents died or disappeared dur-
ing Pinochet’s term.”

In this sentence, we can see the part shown
in the italic font is not carrying much signifi-
cance and can be removed. We removed these
sort of insignificant sub-parts of sentences follow-
ing Berg-Kirkpatrick et al., (2011)’s compression
technique.

In addition, we removed the sub-clauses related
to the reporting verbs from sentences following
(Chali and Uddin, 2016), like in the following ex-
ample sentence:

Cambodian parties agreed to a Coalition gov-
ernment led by Hun Sen, the official said.

We considered mostly used reporting verbs such
as said, told, reported, and announced to find out
subclause. It is known that the sentence which
contains a reporting verb is always the ‘root’ of the
dependency tree. Following this rule, we traversed
the tree to find out the subclause related to the re-
porting verb and removed it from the sentence.

3.2 Sentence Merging

Sentence merging is a technique to create a more
informative sentence by merging the information
from different source sentences. According to
Bing et al., (2015), human summary writers usu-
ally merge the important facts in different verb
phrases (VPs) about the same entity into a sin-
gle sentence. Based on this assumption, we de-
sign a sentence merging technique. While Bing et
al., (2015), took phrases as the basic linguistic unit
and merge phrases to produce a summary, we take
sentences as the basic linguistic units and merge
them to generate new sentences for the summary.
For example, the following sentences: (1) Cam-
bodian prime minister Hun Sen has ruled through
violence, (2) Hun Sen threatened to eliminate op-
ponents can be merged as (3) Hun Sen has ruled
through violence and threatened to eliminate op-
ponents. For merging two sentences, we identify
the sentences which start with a coreferent subject
in order to preserve the gramaticality of the newly
generated sentence, which is a key challenge in ab-
stractive summarization.

Our system first applies Stanford Coreference
Resolution engine (Lee et al., 2013) on each sen-
tence of a document. From this step, we obtain
a set of clusters containing the noun phrases that
refer to the same entity in a document. A new sen-

419



tence is generated from two sentences if they share
a coreferent NP as the subject but have different
VPs. We picked the sentences closest to each other
for merging and produced the new sentences. The
natural order of the sentences has thus been pre-
served.

After this phase, we obtain a cluster of doc-
uments containing concise sentences. Now, this
document set is the input of our document sum-
marization phase.

4 Document Summarization

We consider text summarization as a budgeted
submodular function maximization problem sim-
ilar to the recent works of (Lin and Bilmes, 2011),
but our proposed monotone submodular objective
function is significantly different from their work,
which is discussed in this section.

4.1 Problem Definition

Suppose U be the finite set of all textual-units
(sentence) in the documents. Our task of summa-
rization is to select a subset S ⊆ U that maximizes
the submodular function. Since there is a length
constraint in standard summarization tasks (e.g.,
DUC1 evaluations), we consider the problem as a
submodular function maximization with budgeted
constraints:

max
S⊆U

{
f(S) :

∑
i∈S costi ≤ Bmax

}
(1)

where, costi is the non-negative cost of selecting
the textual-unit i and Bmax is the budget. The
value of Bmax could be the number of words or
bytes in the summary. f(S) is the submodular ob-
jective function that scores the summary quality.

4.1.1 Generic Summarization
We design a monotone submodular objective func-
tion composed of three important objectives for
document summarization. These objectives are
responsible for measuring summary’s importance,
coverage and non-redundancy property. The pro-
posed objective function is:

f(S) = αr(S) + βc(S) + Λh(S) (2)

where, r(S) measures summary’s importance
quality, c(S) measures summary’s coverage qual-
ity, h(S) measures summary’s non-redundancy

1http://www-nlpir.nist.gov/projects/duc/index.html

quality and α, β, and Λ are non-negative trade-off
coefficients which can be tuned empirically2.

As we know, the linear combination of the sub-
modular functions is submodular (Lin and Bilmes,
2011) and all the proposed subparts of our objec-
tive function are submodular, the function f(S) is
also submodular.

One of the basic requirements of a good sum-
mary is that it should contain the most impor-
tant information across multiple documents. To
model this property, we introduce a new mono-
tone nondecreasing submodular function based on
the atomic concept. In our definition, atomic con-
cepts are the atomic terms that bear significance in
a sentence. Our system, therefore, considers only
verbs, named-entities, and adjectives as atomic
concepts (excluding the stop words). Our pro-
posed submodular function is:

r(S) =
N∑
i=1

1
pos(Si)

Ωi.λSi (3)

where, λSi ∈ {0, 1}, λSi = 1 if sentence Si is in
the summary, otherwise λSi = 0. Ωi is the impor-
tance score of sentence Si and pos(Si) denotes the
position of sentence Si in the document.

We consider the relevance of the summary as
the summation of the importance scores of the
sentences in it. First, we utilize the Markov ran-
dom walk model used by (Hong and Nenkova,
2014; Mihalcea and Tarau, 2004) to score each
concept from the document set. Then we score
every sentence based on the weight of the con-
stituent words in the sentence. We only decrease
the weight of the constituent concepts when it
appears in multiple sentences in the summary.
While sentence similarity-based approaches (Lin
and Bilmes, 2011) do not consider the individual
word’s importance to model the importance prop-
erty, our proposed submodular function is based
on the atomic concept and this model encourages
coverage of most of the important concepts across
the documents.

A good summary has the capability to cover
most of the important aspects of a document set.
To formulate this, we consider a submodular ob-
jective function which utilizes the following ‘sen-
tence similarity-based approach’ based on “facil-

2The values for the coefficients are 1.0, 1.0 and 5.0 for α,
β, and Λ respectively, as found empirically using DUC-2003
development set during the experiments.

420



ity location objective” (Cornuejols et al., 1977).

d(S) =
∑
i∈V

maxj∈Ssim(i, j) (4)

where, sim(i, j) denotes the deep semantic sen-
tence similarity between sentence i and j. For
measuring the similarity between sentences, we
used the Word2Vec sentence similarity measure
(Mikolov et al., 2013). We first remove all the
stop words3 which do not add much meaning to
the sentence and then run Word2Vec4 on the words
in both sentences. We calculate the average vector
for all words in both sentences and use cosine sim-
ilarity between vectors to find the semantic simi-
larity between sentences. Finally, following equa-
tion (4), a sentence’s eligibility to be included in
the summary depends on how similar it is with all
the other sentences in the document cluster.

Minimizing redundant information in the sum-
mary is handled by the following submodular
function:

h(S) =
∑

Ck∈η(S)
σ(Ck)−

∑
i,j∈S,i6=j

sim(i, j) (5)

where, sim(i, j) is the deep semantic sentence
similarity between summary sentence i and j,
σ(Ck) is the weight of k-th concept term, and
η(S) is the set of all distinct terms in the summary.

The first part of the function h(S) is based
on atomic-concept which scores the summary by
measuring the weighted sum of the unique con-
cept terms in the summary. In the second part,
we penalize the summary redundancy by measur-
ing semantic similarity among the summary sen-
tences. Finally, our task is to maximize the pro-
posed submodular function f(S) to produce a rel-
evant, well-covered, and non-redundant summary
using the modified greedy algorithm for submod-
ular function (Lin and Bilmes, 2010).

The reason behind choosing this algorithm is
that a solution is guaranteed to be within a constant
factor (1−1/√e) of the optimal solution when the
objective function is monotone submodular. Since
the scoring function f(s) of our proposed summa-
rizer is non-decreasing monotone submodular, we
thus use the following greedy algorithm to obtain
the near optimal solution.

3http://jmlr.org/papers/volume5/lewis04a/a11-smart-
stop-list/english.stop

4https://code.google.com/archive/p/word2vec/

Algorithm 1 A Greedy algorithm for maximizing
the objective function
Require: A minimization LP in standard form.
Ensure: Integral solution, IR1 to the LP.

1: S ← ∅,M ← {1, ..., N}
2: while M 6= 0 do
3: q ← argmaxp∈M f(S∪{p})−f(S)(cp)r
4: if

∑
j∈S Cj + Cq ≤ Bmax and f(S ∪

{q})− f(S) ≥ 0 then
5: S ← S ∪ {q}
6: end if
7: M ←M \ {q}
8: end while
9: t∗ ← argmaxt∈{1,...,N},ct≤Bmaxf({t})

10: if f(t∗) > f(S) then
11: return t∗
12: else return S
13: end if

4.2 Query-focused Summarization

For the query-focused summarization phase, we
propose the following objective function:

f(S) = αr(S) + Υq(S) + Λh(S) (6)

where, r(S) measures summary’s importance
quality, q(S) measures summary’s query rele-
vance quality, h(S) measures summary’s non-
redundancy quality and α, Υ, and Λ are non-
negative trade-off coefficients which can be tuned
empirically5.
We keep the importance and non-redundancy re-
ward function similar to the generic summarizer
described in the previous section. In addition, we
design a query relevance objective function which
considers the two important aspects: (1) how re-
lated summary sentences are with the query?, and
(2) how much query dependent information is cov-
ered in the summary?

q(S) = ψ.
∑
j∈S

Sim(q, sj) + θ.nj,q (7)

where, Sim(i, j) is the similarity between sum-
mary sentence j and query q, here similarity
means the cosine similarity of the average word
vectors obtained from Word2Vec (Mikolov et al.,
2013) for the query and the summary sentence.
nj,q is the number of query terms present in the

5The values for the coefficients are 1.0, 10.0, and 5.0 for
α, Υ, and Λ respectively, as found empirically using the de-
velopment set DUC-2006 during the experiments.

421



summary sentence j. ψ, and θ are non-negative
trade-off coefficients which have been tuned em-
pirically during the experiments6.

5 Experiments
To evaluate our generic and query-focused sum-
maries, we use DUC-2004 and DUC-2007
datasets, respectively. We perform some basic pre-
processing on all the documents such as tokeniza-
tion, part-of-speech tagging and document coref-
erence resolution using Stanford CoreNLP (Man-
ning et al., 2014). We also use Porter’s stem-
mer (Porter, 1999) for stemming all the words and
remove all the stop words from the smart stop
words list7. For query-focused summarization, we
use word vectors from Word2Vec (Mikolov et al.,
2013) which allows us to obtain better similarity
scores between the sentences and the queries. We
evaluate our system generated summaries using
the automatic evaluation toolkit ROUGE version
1.5.5 (Lin, 2004).

We compare the results of our systems (i.e.,
document shrinking + summarization or document
summarization + shrinking) with other state-of-
the-art generic summarization methods. The com-
parison is shown in Table 1 where we report the
values of ROUGE-1 recall and F-1 measure8 of
different approaches. From the table, we can see
that our generic multi-document summarizer (doc-
ument shrinking + summarization) significantly
outperforms those systems in all measures. This
result suggests the effectiveness of sentence com-
pression and merging phase in our system. It also
shows the effectiveness of using semantic similar-
ity measures to select important sentences in the
summary. Moreover, our system also uses a sepa-
rate redundancy function which also helps to gen-
erate summaries with less redundancy compared
to the systems which only concentrate on sum-
mary’s coverage and relevance. These results also
confirm that the proposed strategy can improve
summary quality.

We compare our query-focused summarizer
with other state-of-the-art query summarization
methods. Table 2 shows the comparison in terms

6The values for the query relevance coefficients are 4.0
and 2.0 for ψ and θ respectively, as found empirically using
the development set DUC-2006 during the experiments.

7http://jmlr.org/papers/volume5/lewis04a/a11-smart-
stop-list/english.stop

8ROUGE runtime arguments for DUC-2004: ROUGE -a
-c 95 -b 665 -m -n 4 -w 1.2

Systems R-1 F-1
Best system in DUC-04 (peer 65) 0.3828 0.3794
(Takamura and Okumura, 2009) 0.385 -
(Lin and Bilmes, 2011) 0.3935 0.389
(McDonald, 2007) 0.362 0.338
(Wang et al., 2009) 0.3907 -
Document Shrinking + Summarization 0.4127 0.4133
Document Summarization + Shrinking 0.3874 0.3882

Table 1: Results on DUC-2004 Datasets

Systems R-2 F-2
Best system in DUC-07 (peer 15) 0.1245 0.1229
(Lin and Bilmes, 2011) 0.1238 0.1233
(Toutanova et al., 2007) 0.1189 0.1189
(Haghighi and Vanderwende, 2009) 0.118 -
Document Shrinking + Summarization 0.1258 0.1264
Document Summarization + Shrinking 0.1133 0.1149

Table 2: Results on DUC-2007 Datasets

of ROUGE scores9 between our system and the
best performing systems. From the table, we can
say that our query-focused multi-document sum-
marizer (document shrinking + summarization)
outperforms the best-known systems in DUC-
2007. It is notable that the best system in DUC-
2007 takes the topic title as a query and uses Ya-
hoo search engine to get a ranked set of retrieved
documents which were used later to calculate the
query relevance score (Pingali et al., 2007). How-
ever, our system is totally unsupervised and does
not use any external source for the summary gen-
eration.

6 Conclusion
In this paper, we proposed a new summarization
framework using different submodular functions
with deep semantic features and abstraction-based
methods. Abstraction-based methods help the sys-
tem to obtain concise and more informative can-
didate summary sentences. We selected the best
sentences for the summary by maximizing the sub-
modular objective function. The empirical results
show that our generic and query-focused summa-
rization model outperform the state-of-the-art sys-
tems.

Acknowledgments
This research was supported by the Natu-
ral Sciences and Engineering Research Council
(NSERC) of Canada and the University of Leth-
bridge.

9ROUGE runtime arguments for DUC-2007: ROUGE -n
2 -x -m -2 4 -u -c 95 -r 1000 -f A-p 0.5-t 0-d

422



References
R. Barzilay and K. R. McKeown. 2005. Sentence fu-

sion for multidocument news summarization. Com-
putational Linguistics, 31(3):297–328.

T. Berg-Kirkpatrick, D. Gillick, and D. Klein. 2011.
Jointly learning to extract and compress. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 481–490. As-
sociation for Computational Linguistics.

L. Bing, P. Li, Y. Liao, W. Lam, W. Guo, and R. J. Pas-
sonneau. 2015. Abstractive multi-document sum-
marization via phrase selection and merging. Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing.

F. Boudin, H. Mougard, and B. Favre. 2015. Concept-
based summarization using integer linear program-
ming: From concept pruning to multiple optimal
solutions. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1914–1918. Association for Compu-
tational Linguistics.

J. Carbonell and J. Goldstein. 1998. The use of mmr,
diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the
21st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 335–336. ACM.

Y. Chali and M. Uddin. 2016. Multi-document summa-
rization based on atomic semantic events and their
temporal relationships. In Advances in Information
Retrieval, pages 366–377. Springer.

J. Cheng and M. Lapata. 2016. Neural summariza-
tion by extracting sentences and words. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 484–494, Berlin, Germany. Association
for Computational Linguistics.

J. C. Cheung and G. Penn. 2014. Unsupervised sen-
tence enhancement for automatic summarization.
In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 775–786. Association for Compu-
tational Linguistics.

S. Chopra, M. Auli, A. M. Rush, and S. Harvard. 2016.
Abstractive sentence summarization with attentive
recurrent neural networks. Proceedings of NAACL-
HLT16, pages 93–98.

G. Cornuejols, M. L. Fischer, and G. L. Nemhauser.
1977. Location of bank accounts to optimize float:
An analytic study of exact and approximate algo-
rithms. Management Science, 23(8):789–810.

A. Dasgupta, R. Kumar, and S. Ravi. 2013. Summa-
rization through submodularity and dispersion. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics ACL, pages
1014–1022.

G. Erkan and D. R. Radev. 2004. Lexrank: graph-
based lexical centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research,
pages 457–479.

C. Fellbaum, editor. 1998. Wordnet: An Electronic
Database. MIT Press.

K. Filippova, E. Alfonseca, C. A. Colmenares,
L. Kaiser, and O. Vinyals. 2015. Sentence compres-
sion by deletion with lstms. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 360–368.

K. Ganesan, C. Zhai, and J. Han. 2010. Opinosis: a
graph-based approach to abstractive summarization
of highly redundant opinions. In Proceedings of the
23rd international conference on computational lin-
guistics, pages 340–348. Association for Computa-
tional Linguistics.

A. Haghighi and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 362–370. Association for Computa-
tional Linguistics.

K. Hong and A. Nenkova. 2014. Improving the estima-
tion of word importance for news multi-document
summarization. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 712–721. Associ-
ation for Computational Linguistics.

K. Knight and D. Marcu. 2000. Statistics-based sum-
marization - step one: Sentence compression. In
Proceedings of the 17th National Conference on Ar-
tificial Intelligence, Austin.

H. Lee, A. Chang, Y. Peirsman, N. Chambers,
M. Surdeanu, and D. Jurafsky. 2013. Determin-
istic coreference resolution based on entity-centric,
precision-ranked rules. Computational Linguistics,
39(4):885–916.

C.-Y. Lin. 2004. Rouge: A package for automatic eval-
uation of summaries. In Proceedings of the 42nd
Annual Meeting of the Association for Computa-
tional Linguistics, Workshop on Text Summarization
Branches Out.

H. Lin and J. Bilmes. 2010. Multi-document summa-
rization via budgeted maximization of submodular
functions. In Proceedings of the 2010 Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 912–920. Associ-
ation for Computational Linguistics.

423



H. Lin and J. Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1. Association for Computa-
tional Linguistics.

C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J.
Bethard, and D. McClosky. 2014. The stanford
corenlp natural language processing toolkit. In Pro-
ceedings of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics System Demon-
strations.

A. Martins and N. A. Smith. 2009. Summarization
with a joint model for sentence extraction and com-
pression. In Proceedings of the Workshop on Inte-
ger Linear Programming for Natural Language Pro-
cessing, pages 1–9. Association for Computational
Linguistics.

R. McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Eu-
ropean Conference on Information Retrieval, pages
557–564. Springer.

R. Mihalcea and P. Tarau. 2004. Textrank: Bringing
order into texts. In Proceedings of EMNLP 2004,
pages 404–411, Barcelona, Spain. Association for
Computational Linguistics.

T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. 2013. Distributed representations of words
and phrases and their compositionality. In In Ad-
vances in neural information processing systems,
pages 3111–3119.

H. Morita, T. Sakai, and M. Okumura. 2011. Query
snowball: a co-occurrence-based approach to multi-
document summarization for question answering. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
223–229. Association for Computational Linguis-
tics.

R. Nallapati, F. Zhai, and B. Zhou. 2017. Sum-
marunner: An interpretable recurrent neural network
model for extractive summarization. In Proceed-
ings of the 31st AAAI Conference on Artificial In-
telligence, San Francisco. AAAAI Press.

D. Parveen, H. M. Ramsl, and M. Strube. 2015. Top-
ical coherence for graph-based extractive summa-
rization. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1949–1954. Association for Compu-
tational Linguistics.

D. Parveen and M. Strube. 2015. Integrating im-
portance, non-redundancy and coherence in graph-
based extractive summarization. In Proceedings of
the 24th International Joint Conference on Artificial
Intelligence, page 12981304.

P. Pingali, K. Rahul, and V. Varma. 2007. Iiit hyder-
abad at duc 2007. In Proceedings of the Document
Understanding Conference.

M.F. Porter. 1999. An algorithm for suffix stripping.
Program, 14(3):130–137.

M. A. Rush, S. Chopra, and J. Weston. 2015. A neu-
ral attention model for abstractive sentence summa-
rization. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 379–389. Association for Computational
Linguistics.

H. Takamura and M. Okumura. 2009. Text summa-
rization model based on maximum coverage prob-
lem and its variant. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 781–789. As-
sociation for Computational Linguistics.

K. Thadani and K. McKeown. 2013. Supervised sen-
tence fusion with single-stage inference. In Pro-
ceedings of the International Joint Conference on
Natural Language Processing, pages 1410–1418.

K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The pythy
summarization system: Microsoft research at duc
2007. In Proceedings of the Document Understand-
ing Conference.

D. Wang, S. Zhu, T. Li, and Y. Gong. 2009.
Multi-document summarization using sentence-
based topic models. In Proceedings of the ACL-
IJCNLP 2009 Conference, pages 297–300. Associa-
tion for Computational Linguistics.

L. Wang and W. Ling. 2016. Neural network-based
abstract generation for opinions and arguments. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 47–57, San Diego, California. Association for
Computational Linguistics.

424


