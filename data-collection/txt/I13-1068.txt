










































A Hierarchical Semantics-Aware Distributional Similarity Scheme


International Joint Conference on Natural Language Processing, pages 596–604,
Nagoya, Japan, 14-18 October 2013.

A Hierarchical Semantics-Aware Distributional Similarity Scheme∗

Shuqi Sun1, Ke Sun2, Shiqi Zhao2, Haifeng Wang2, Muyun Yang1, and Sheng Li1

1Harbin Institute of Technology, Harbin, China

{sqsun,ymy}@mtlab.hit.edu.cn, lisheng@hit.edu.cn
2Baidu, Beijing, China

{sunke,zhaoshiqi,wanghaifeng}@baidu.com

Abstract

The context type and similarity calcula-

tion are two essential features of a distri-

butional similarity scheme (DSS). In this

paper, we propose a hierarchical semantic-

aware DSS that exploits semantic rela-

tion words as extra context information to

guide the similarity calculation. First, we

define and extract five types of semantic

relations, and then develop relation-based

similarities from the distributional similar-

ities among the top-ranked relation words.

Finally, we integrate various similarities

using learning-to-rank technique. Exper-

iments show that semantic relations are

beneficial to predicting accurate similar-

ity. On 6904 pairwise similarity compar-

isons, the predictive accuracy of our ap-

proach reaches 83.9%, which significantly

outperforms the baseline approaches. We

also conduct intrinsic analysis by varying

the quality of semantic relations and the

usage of individual similarities.

1 Introduction

Distributional similarity is an essential measure of

the semantic relatedness between a pair of linguis-

tic objects, including words, phrases, or even sen-

tences. Confident distributional similar objects,

are useful in various NLP applications, such as

word sense disambiguation (Lin, 1997), lexical

substitution (McCarthy and Navigli, 2009), para-

phrase ranking (Dinu and Lapata, 2010), text clas-

sification (Baker and McCallum, 1998), etc. In

this paper, we focus on the semantic similarity be-

tween words.

Well-known implementations of word-level dis-

tributional similarity scheme (DSS) mainly fall

∗This work was done when the first author was visiting
Baidu.

into two categories according to the choice of

the context: a) text-window based, and b) depen-

dency path based. The former has the advantages

of language-independence and computational effi-

ciency, while the latter captures finer word-word

relationships. However, both approaches focus on

the usage aspect of words’ meanings, which is

only indirect indicator of the underlying semantic

interactions.

Meanwhile, a great many successful efforts

have been made to extract word-level semantic

knowledge from the text, including synonyms /

antonyms, sibling terms, hypernyms / hyponyms,

holonyms / meronyms, etc. Despite of such delib-

erated studies, there are few considerations about

how these semantics-oriented outcomes could

contribute to the construction of DSS.

To realize the full potential of such semantic ev-

idences, we propose a semantics-aware DSS by

using semantic relation words to guide the sim-

ilarity calculation. Our motivation is that words

having similar semantic relational words, e.g. sib-

ling terms or hypernyms, tend to be more semanti-

cally similar. The proposed DSS has a hierarchical

layout, in which text-window based distributional

similarity is first established from the corpus serv-

ing as the basis of the relation-specific similarities

for 5 semantic relations. Finally, these similari-

ties are linearly combined using learning-to-rank

technique into a single measure, capturing both the

context distributions and the semantic relations of

the target words.

Our contribution is three-fold. First, by deriving

semantic relation based similarities from distribu-

tional similarity, we develop a semantics-aware

DSS in a hierarchical fashion. The DSS eventually

fuses these similarities, and yields significant im-

provement over several baseline approaches. Sec-

ond, our design of DSS relies solely on the same

corpus where distributional similarity can be de-

rived. It is adaptable to different languages, in

596



which distributional similarity and semantic rela-

tions are available. Third, our DSS’s hierarchi-

cal nature allows us to individually replace each

component with better implementations to adapt

to specific applications or new languages.

2 Related Work

Distributional similarity (a.k.a. contextual similar-

ity) has been elaborately studied to predict seman-

tic similarity, and the type of the context is a main

concern. A variety of context types have been

proposed to capture the underlying semantic inter-

actions between linguistic objects, including text-

window based collocations (Rapp, 2003; Agirre et

al., 2009), lexico-syntactic patterns (Turney, 2006;

Baroni and Lenci, 2010), grammatical dependen-

cies (Lin, 1998; Padó and Lapata, 2007; Thater

et al., 2010), click-through data (Jain and Pen-

nacchiotti, 2010), selectional preferences (Erk and

Padó, 2008), synsets in thesaurus (Agirre et al.,

2009), and latent topics (Dinu and Lapata, 2010).

There are also researches that focus on distribution

compositions (Mitchell and Lapata, 2008; Grefen-

stette et al., 2013) or context constrained similarity

calculation (Erk and Padó, 2008).

Extracting sibling or hierarchical semantic re-

lations from corpora forms a different track of

research, in which exist ample efforts. Most of

them make use of hand-crafted or automatically

bootstrapped patterns. Various types of patterns

have been tried out, including plain texts (Hearst,

1992; Paşca, 2004), semi-structured HTML tags

(Shinzato and Torisawa, 2007), or their combi-

nations (Shi et al., 2010). Bootstrapping ap-

proach is shown useful given a number of seeds,

which could be either relation instances(Snow et

al., 2004; Pantel and Pennacchiotti, 2006), or ini-

tial patterns(Pantel et al., 2004). To improve the

quality of raw extraction results, some studies also

resort to optimizing one relation using other rela-

tions (Zhang et al., 2011; Kozareva et al., 2011) or

using distributional similarity (Shi et al., 2010).

Despite the great progress made in the field of

semantic relation extraction, few studies explic-

itly use semantic relations to guide the similarity

calculation. In this paper, we use instance-pattern

iteration on a massive corpus to populate seman-

tic relation instances, and derive relation-specific

similarities on top of text-window based distribu-

tional similarity. Indeed, previous studies did re-

sort to a closed set of lexical patterns that indicate

sibling / hypernym / hyponym relations (Baroni

and Lenci, 2010; Bansal and Klein, 2012), concept

properties (Baroni et al., 2010), and attribute infor-

mation (Baroni and Lenci, 2010). Compared with

these studies, our approach systematically exploits

specific semantic relations instead of counting co-

occurrence under surface patterns. We also de-

velop a hierarchical similarity fusion architecture,

rather than blending the heterogeneous evidences

in a single distribution vector (Baroni and Lenci,

2010). It is also notable that sibling term extrac-

tion, in which various semantic evidences (e.g. hy-

pernyms) also help, is not in the track of our study.

Sibling term extraction focuses on words sharing

the same super concept, and does not quantify the

pair-wise similarity between them. Nevertheless,

sibling terms work fine as an evidence of semantic

similarity, as shown in our experiments.

Machine learning based integration of multiple

evidences are shown useful in semantic class con-

struction and semantic similarity calculation. Pen-

nacchiotti and Pantel (2009) use gradient boost-

ing decision tree to combine evidences from Web

page, query log, Web tablet, and Wikipedia to

populate instances of Actors, Athletes, and Mu-

sicians. There are also studies that combine dis-

tribution and pattern information in lexical en-

tailment (Mirkin et al., 2006) and word cluster-

ing (Kaji and Kitsuregawa, 2008). Close to our

work, Agirre et al. (2009) train SVM classification

models to combine individual similarities derived

from dependency path, text-window, and WordNet

synsets. The synsets are highly accurate in repre-

senting words’ meanings. However, the size of the

thesaurus is limited, and not equally available in

different languages. Although Agirre et al. (2009)

tried machine translation techniques to tackle with

this issue, abundant named entities and translation

errors in the Web corpus still challenge the perfor-

mance of their approach.

3 Hierarchical Semantics-aware DSS

Our proposed DSS has a four-layer structure, as

shown in Figure 1. The bottom layer is the corpus

layer where a massive Web page repository is pre-

processed. Upwards, we build a distribution layer

to obtain basic text-window based distributional

similarity between any pair of target words. The

distribution layer provides a distributional similar-

ity database upon which a semantics layer takes

effect. At this layer we adopt an extraction system

597



Corpus

Distribution

Semantics

Fusion

Semantic relation extraction

Distributional similarity

Relation-based similarity

Text-window based context

Learning-

to-rank

Billion-Webpage collection

Word segmentation, POS tagging, 
NE recognition

Figure 1: Hierarchical approach to semantics-

aware DSS.

that iteratively populates instances for 5 types of

semantic relations. Then, for each type of seman-

tic relation, we develop a relation-specific similar-

ity measure. Finally at the fusion layer, the simi-

larities at the distribution and semantics layers are

integrated linearly using learning-to-rank.

3.1 Corpus Layer

The corpus we work on is a repository of Chinese

Web pages collected in 2011. It contains 1.1 bil-

lion pages (5.8 × 1011 words in total), and takes
4.7TB of storage. All pages are de-tagged, leav-

ing only their titles and textual content, and then

segmented with a word segmentor based on dictio-

naries plus conditional random field (CRF) mod-

els. The segmentor is efficiently implemented,

and is able to process 40K words per second with

an accuracy around 98%. Based on the segmen-

tation results, two more steps of pre-processing:

POS tagging and named entity (NE) recognition

are also performed.

3.2 Distribution Layer

The most widely studied types of context for dis-

tributional similarity are text-window based con-

text and grammatical context. The construction of

the latter requires syntactic or dependency pars-

ing, which is highly language-dependant and may

be extremely time-consuming on large corpora.

Therefore, at the distribution layer, we build

the distributional similarity database using simple

text-window based co-occurrences. Two different

lengths of the text-window are experimented: 3

words and 6 words. The window slides one word

per step from the beginning of each sentence to

the end. Thus, the 3-length and 6-length windows

capture two and five words at most on each side

of the target word respectively. For each pair of

words w and w′, four association measures are

tried out, covering a range of common practices,

including (1) raw number of co-occurrence, (2)

point mutual information (PMI), (3) Jaccard in-

dex, and (4) local mutual information (LMI) (Ev-

ert, 2008). To reduce the amount of computa-

tion, we preserve top 5000 context words for each

target word. Processing the whole corpus yields

∼6.5 million unique target words1. For any pair
of words w and w′ in the vocabulary, the distribu-

tion layer provides it a cosine similarity between

their context distributions, denoted by ds(w,w′).

3.3 Semantics Layer

We deem that semantic relationship is a more di-

rect clue of a word’s meaning than either its text-

window co-occurrences or syntactic dependen-

cies. Therefore, we introduce a semantics layer

upon the distribution layer to exploit semantic re-

lations. Specifically, we adopt an extraction sys-

tem to populate semantic relation instances, and

then derive relation-based similarities from the

system’s output.

3.3.1 Relation Extraction

Here, we present a fully-featured, yet lightweight

semantic relation extraction system that is capa-

ble to conduct in-depth mining in massive corpora.

The system follows the line of instance-pattern it-

eration on a massive corpus, and can be substituted

by any implementation of such fashion.

We define and extract five types of semantic re-

lations, as listed in Table 1. The extraction starts

in the first iteration with a number of seed relation

instances. A relation instance is defined as a triple

(w, r,w′), which means words w and w′ have the
relation r.

r Relation Description

r1 w [Sibling]is w
′ w, w′ are sibling terms

r2 w [Hyponym]is w
′ w

′ “is a” w
w′ is a “Instance-of” w

r3 w [Hypernym]is w
′ w “is a” w

′

w is a “Instance-of” w′

r4 w [Meronym]is w
′

w′ is a “Part-of” w
w′ is a “Member-of” w
w′ is a “Substance-of” w

r5 w [Holonym]is w
′

w is a “Part-of” w′

w is a “Member-of” w′

w is a “Substance-of” w′

Table 1: List of semantic relations.

In a nutshell, during a full iteration, the system

1This is much larger than the number of typical Chinese
words, which is mainly caused by the huge amount of NEs in
the Web corpus, plus typos and word segmentation errors.

598



first uses seed instances to populate initial patterns

that match them from the corpus, and assigns a

unique semantic relation to each pattern accord-

ing to the seeds it matches. Then, the system uses

these patterns to extract new word pairs, and as-

signs relations to the word pairs according to the

patterns that extracted them. The whole procedure

is described in detail as the following four steps:

(1) Pattern initialization Find all sentences that

contain the words w and w′ in any seed relation

instance (w, r,w′). Each sentence is then split
into prefix, infix, and suffix by w and w′. The to-

tal length limit of these three parts is 10 words.

Within this limitation, the system exhaustively

enumerates all possible prefixes and suffixes (the

infix remains unchanged). For example, in the sen-

tence “A B w C w′ D E”, the prefixes can be {‘A’,
‘B’, ‘A B’, ‘ ’} and the suffixes can be {‘D’, ‘E’, ‘D
E’, ‘ ’}. Then, each combination of “prefix SLOT1
infix SLOT2 suffix” forms the word level of a pat-

tern. Plus the POS and NE tags, a pattern finally

contains three levels of information. In addition,

to increase the recall of the patterns, named enti-

ties at the word level are replaced by their NE tags.

This means at these positions, the pattern would

match an arbitrary word as long as the word’s NE

tags are matched. For instance, say the follow-

ing sentence matches a seed (苹果(Apple), [Sib-

ling]is,三星(Samsung)):

近日，[苹果]和[三星]在美国进行了专利诉讼。
(Recently, [Apple] and [Samsung] conducted patent

litigation in the U.S.)

A pattern derived from this sentence could be:

Word:

Recently

近日
{SLOT1}

and

和
{SLOT1}

in

在
U.S.
LOC

POS: t nz c nz p ns
NE: NOR BRD NOR BRD NOR LOC

where BRD, LOC stand for brand and location,

and NOR means the word is not a NE.

(2) Pattern-relation mapping For pattern p,

consider all seed instances (w, r,w′) it matched.
For each relation r in those seeds, count it w.r.t. p.

Then, r is scored by tf idf , where tf is r’s count,

and df is the number of relations that have a non-

zero count. Finally, map p to the relation rmax
with the highest score smax, and assign smax to p

as its score.

(3) Instance extraction For each semantic rela-

tion r, extract word pairs using top scored 1,000

patterns that are mapped to r. For each sentence, if

it matches a pattern p’s word sequence and meets

the POS / NE tag constraints at SLOT1 and SLOT2
in p, then the words falling into the two slots are

extracted.

Note that different patterns (even those with dif-

ferent relations) may extract the same word pair.

To determine the final relation of a word pair <w,

w′>, the system traverses all the patterns that can

extract it. The patterns are then grouped by the

relations they map to. Within each group, the

patterns’ scores are added up. The relation r⋆

whose group has the highest sum score S is se-

lected. Then, with r⋆, a new instance (w, r⋆, w′)
is generated, with S as its weight. The system will

also generates a reversed instance (w′, r⋆−1, w).
E.g., for (Intel, [Sibling]is AMD) and (Intel, [Hy-

pernym]is Company) , the system also gener-

ates (AMD, [Sibling]is Intel) and (Company, [Hy-

ponym]is Intel) respectively.

(4) New seed generation Add the top-weighted

relation instances obtained in step (3) into the seed

set. In the current setting, each relation’s top-500

weighted instances are added as new seeds in each

iteration.

In practice, the seed set used is small ontology

of totally 222k instances of the five relations The
system produces 27M instances after 2 iterations,
which are used in our experiments. For each word

w, we define its relation words as the words ap-

pearing in the slot “w [relation]is ”. E.g., w’s

[Hypernym]is relation words are its hypernyms.

3.3.2 Similarity

Recall that at the distribution layer, each pair
of words has a distributional similarity, denoted
byds(·, ·). On top of this, we individually develop
a relation-based similarity rsi(·, ·) for each seman-
tic relation ri ∈ {r1, r2, . . . , r5}. For two words
w and w′, rsi(w,w

′) is defined as the average of
non-zero distributional similarities between their
top-N (at most) relation words under ri (denoted

by rNi (·)):

rsi(w, w
′) =

1

|{(u, v)|ds(u, v) > 0}|

∑

u∈r
N

i
(w)

v∈r
N

i
(w′)

ds(u, v)

(1)

In our experiments, we universally set N to 10.

One alternate practice is to directly calculate the

traditional cosine similarity between the relation

word distributions of w and w′. We do not take

this approach because such manner suffers from

data sparseness. In particular, sometimes the rela-

tion words of w and w′ are quite similar, but none

599



of them are shared by w and w′ (this means the

cosine similarity will be 0). For instance:

• hand and head may not share any meronym,
e.g. hand only has meronym finger while

head has eye, nose, . . . ;

• Carmel (a small city in IN, U.S.) may only
have the hypernym {small city} while New
York may have {city, big city}.

In our approach, owing to the non-zero ds(·, ·)
between finger / eye, or between small city / big

city, the two pairs of words will have positive

relation-based similarities.

3.4 Fusion Layer: Learning-to-rank

Eventually, we fuse ds(·, ·) and rsi(·, ·) together
to get the final similarity prediction of each pair
of words. We choose a straightforward manner by
linearly combining ds(·, ·) and rsi(·, ·):

FUSE(w, w′) = αd · ds(w, w
′) +

∑

i

αi · rsi(w, w
′) (2)

Note that the relation-based similarities are built

upon ds(·, ·) (Eq. 1), so FUSE is essentially a hier-
archical combination of ds(·, ·) guided by seman-
tic relations. Linear combination is simple, but

turns out to be effective through the experiments.

More elaborated fusion method may be invested in

future studies.

To get the weights αd and αi, we adopt pair-

wise learning-to-rank technique rather than regres-

sion. This is because it is difficult to assign an

absolute score of semantic similarity to a pair of

words, especially when seasoned linguists are not

available. On the other hand, given two word pairs

<A, B> and <A,C>, it is relatively easier to tell

whether <A, B> is more similar than <A,C>, or

vice versa.

We use the ranking option of SVMlight v6.02

(Joachims, 1999) with linear kernel to optimize

the weights against human judgements. The goal

of the learning process is to minimize the num-

ber of wrong pair-wise similarity comparisons. In

the testing phase, the model assigns to each testing

sample a real-value prediction, which is exactly a

linear fusion of the corresponding sub-similarities.

As for the technical details in SVMlight, each word

pair <X, Y> yields a sample. If the human judge-

ment suggests <A, B> is more similar than <A,

C>, then the corresponding samples will be as-

signed to an unique sample group, with the tar-

get values 1 and 0 respectively. If a sub-similarity

value does not exist due to out-of-vocabulary is-

sue, the corresponding feature is set to “missing”.

The judgement is obtained from a Chinese

thesaurus (HIT-SCIR, 2006), containing 77,458

words that are manually grouped according to a

five-level category hierarchy. Words grouped to-

gether at the lowest(fifth) level include both syn-

onyms (e.g. sea / ocean) and comparable terms

(e.g. Germany / France). The lower the level two

words appear in together, the more semantically

related they are. We directly use this clue to deter-

mine the semantic similarity between words. For

instance, words that appear together in a level-3

category but not in any level-4 category have a

similarity of 3. Words do no appear together in

any category have a zero-similarity.

After a pilot study, we found that words with

similarities 0-2 are indistinguishably dissimilar.

So we merged these similarity levels together as

zero-similarity. Moreover, to further distinguish

semantically-similar and comparable words, we

set the similarity between synonyms to 6 instead of

5 for comparable terms. Finally, we got five simi-

larity levels: 0, 3, 4, 5, and 6. To make the experi-

ment manageable, we randomly sample 200 nouns

from the thesaurus and extract their similar words

at every level, and arrange them as a serial of simi-

larity judgements like sim(w,w′) > sim(w,w′′).
The whole dataset contains 2,204 words and 6,904

judgements. To avoid the randomness in data, we

adopt five-fold cross-validation on it. In each fold,

we use 3 parts of the data to train the model, and

tune / test it using the other two parts.

4 Evaluation

4.1 Experiment Settings

We compare our fused similarity with three base-

lines. The first one is classical text-window based

distributional similarity ds(·, ·). The other two
baseline approaches are listed as follows:

Lin’s similarity (Lin, 1998) (LIN98). LIN98

combines PMI values from different distributions

linearly. The formula uses dependency paths, and

we extend it to semantic relations extracted as in

Section 3.3.1. As a by-product in the extraction

phase, words’ co-occurrence counts under each se-

mantic relation are acquired to compute the PMI

values. The text-window based distribution is also

included in the combination.

Joint cosine similarity (JCS). There is also pre-

vious work that uses pattern-constrained context

600



information as extra clue of semantic similarity

(Baroni and Lenci, 2010). Different from LIN98,

words’ co-occurrence counts under each semantic

relation are replaced by the number of patterns that

extract them (Baroni and Lenci, 2010). Here, the

text-window and the relation based distributions

are mingled into a single distribution, and cosine

similarity is obtained. Baroni and Lenci (2010)

uses LMI in relation based distributions, but we

found PMI achieves better performance.

The text-window distribution used in both

LIN98 and JCS is based on 3-length window and

PMI, since this configuration shows the best per-

formance in our experiments. For the relation

based distribution in LIN98 and JCS, we initially

use the whole (noisy) relation extraction result,

and make further analysis by varying the amount

(and quality) of the relation instances. LIN98 and

JCS also generate integrated similarities based on

multiple evidences, we will compare their effec-

tiveness with our approach.

Two evaluations metrics are used:

Accuracy of comparison (Acc.). We say a

system makes a correct comparison if it returns

S(A,B) > S(A,C) that coincides with human
judgement. The overall accuracy is defined as the

percentage of correct comparisons over the whole

dataset.

Spearman’s ρ. For each word pair <A, B>, we

count the number of word pairs <A, x> that are

judged less similar than <A, B>, and use it as an

absolute score of similarity between A and B. This

allows us to compare similarity predictions with

such scores globally, and get the ρ coefficient.

To get meaningful conclusions, we use approxi-

mate randomization (Noreen, 1989) to test the sig-

nificance of Acc. comparison, and Steiger’s Z-test

(Steiger, 1980) for Spearman’s ρ comparison.

4.2 ds(·, ·) Configurations

Distributional similarity ds(·, ·) is an important
baseline. Moreover, by substituting Eq. 1 into

Eq. 2, one will find that ds(·, ·) is also the basic
building block of the fused similarity. With the

multiple choices of the text-window lengths (3 and

6) and association measures (Raw co-occurrence,

PMI, Jaccard, and LMI) listed in subsection 3.2,

we now try to find out an optimal configuration

of ds(·, ·). The results are obtained based on the
whole dataset, as shown in Table 2. For the ds(·, ·)
configurations (the first 8 rows), the subscripts are

the text-window’s length and the superscripts are

the association measures used. Performance of

LIN98 and JCS is also included (rows 9∼10).

Acc. ρ

dsRaw cooc
3wd (·, ·) 77.0 0.458

dsRaw cooc
6wd (·, ·) 75.2 0.427

dsPMI
3wd (·, ·) 80.8 0.522

dsPMI
6wd (·, ·) 77.4 0.438

dsJac.
3wd (·, ·) 80.1 0.527

dsJac.
6wd (·, ·) 79.0 0.501

dsLMI
3wd (·, ·) 80.0 0.544

dsLMI
6wd (·, ·) 78.2 0.497

LIN98 79.4 0.496

JCS 82.2 0.553

Table 2: Performance of ds(·, ·), LIN98, and JCS
In both Acc. and ρ, ds(·, ·) with the 3-length

window significantly (p < 0.01) outperforms that
with the 6-length window, except when using Jac-

card (p = 0.12 for Acc.). Although the win-
dow length is easy to choose, it remains unclear

which association measure is the most appropri-

ate. With the 3-length window, the performance

of PMI, Jaccard, and LMI are comparable. Thus,

we will have to try out all PMI, LMI and Jaccard

in the fusion phase.

As for the other two baseline approaches, JCS

significantly outperforms all ds(·, ·) configura-
tions in both Acc. (p < 0.05) and ρ (p < 0.07)
as shown in bold font, but LIN98 does not.

4.3 Similarity Fusion

In similarity fusion (Eq. 2), for the sake of con-

ciseness, we use the same ds(·, ·) configuration to
compute both the distributional similarity and the

relation-based similarities (Eq. 1). The Acc. and

ρ of the fused similarity (denoted by FUSE) using

different ds(·, ·) configurations are shown in Ta-
ble 3. Recall that because of the indistinguishable

Acc., three configurations need to be examined:

dsPMI
3wd (·, ·), ds

LMI
3wd (·, ·), and ds

Jac.
3wd (·, ·).

ds(·, ·) configuration used
dsPMI

3wd (·, ·) ds
LMI
3wd (·, ·) ds

Jac.
3wd (·, ·)

Acc. 83.9 81.9 78.9

ρ 0.591 0.558 0.500

Table 3: Performance of our proposed fused sim-

ilarity (FUSE) using different ds(·, ·) configura-
tions.

In both Acc. and ρ, dsPMI
3wd (·, ·) based FUSE has

601



significantly (p < 0.005) superior performance
(shown in bold font). In both metrics, it also

significantly (p < 0.01) outperforms all baseline
approaches, including all ds(·, ·) configurations,
LIN98, and JCS. The results suggest that on our

dataset, the most suitable ds(·, ·) to use in FUSE
is dsPMI

3wd (·, ·), which achieves 83.9% accuracy in
predicting whether a word pair <A, B> is more

similar than <A, C>.

As a global comparison, we have the following

performance rankings:

Acc. : LIN98 <0.05 ds
PMI

3wd (·, ·) <0.05 JCS <0.01 FUSE
ρ : LIN98 <0.01 ds

LMI

3wd (·, ·) <0.01 JCS <0.01 FUSE

where the subscripts show the significance level.

JCS outperforms the best ds(·, ·) configurations in
both Acc. and ρ, confirming the contribution of the

semantic evidences obtained by the in-depth min-

ing in the corpus. Moreover, FUSE achieves even

better performance, showing the effectiveness of

the design of relation-based similarity (Eq. 1) and

the linear combination mechanism (Eq. 2).

Ideally, an effective fusion should have worked

for all ds(·, ·) configurations. However, FUSE us-
ing dsJac.

3wd (·, ·) yields bad performance. Through
intrinsic analysis we found that dsJac.

3wd (·, ·) is more
sensitive to the noise in the relation data than

dsPMI
3wd (·, ·).

4.4 Quality of Semantic Relations

Initially, we use all of the extracted relation in-

stances in the experiments without threshold based

filtering. Without doubt, there is much noise in the

bottom of the extraction results. Through control-

ling the weight threshold of the relation instances,

we now shrink the global extraction results to top

∼5%, ∼10%, ∼30%, and ∼60% subsets to see
how their quality and coverage change, and how

they affect the performance of FUSE, LIN98, and

JCS.

FUSE uses top 10 relation words to calculate

the relation-based similarity. Thus, instead of ex-

amining the global extraction results, we focus on

the top 10 relation words of the target words in our

dataset, because FUSE’s performance is our main

concern.

The full evaluation is expensive. There are to-

tally 2,204 target words in the dataset, involving

70,000 relation words. So we randomly sample

200 words from the 2,204 words, and evaluate the

accuracy of their relation words by varying the

amount of the global extraction results. The re-

sults are summarized in Table 4. While the amount

78.0

79.0

80.0

81.0

82.0

83.0

84.0

~5% ~10% ~30% ~60% 100%

LIN98 JCS

FUSE w/ ds_3wd_PMI FUSE w/ ds_3wd_LMI

FUSE w/ ds_3wd_Jac.

Figure 2: Accuracy of LIN98, JCS, and FUSE

varying the amount of the global extraction results.

of global extraction results shrink, low-weight re-

lation words are gradually removed, and the cov-

erage of the relation words decreases. Only the

top ∼5% results have acceptable accuracies. In-
trinsic study shows that holonyms and meronyms

concentrate to location names due to the bias in

the seeds. This causes a significantly low quality

and coverage for these two relations.

The low-quality extraction results pose an aus-

tere challenge. Here, we re-examine LIN98,

JCS, and FUSE (5-fold CV using dsPMI
3wd (·, ·),

dsLMI
3wd (·, ·), or ds

Jac.
3wd (·, ·)) based on the four sub-

sets of the global extraction results, and assemble

the performance figures in Figure 2. For the sake

of space limit, we only include the Acc. results.

Spearman’s ρ shows a similar trend.

The results further confirms the ranking listed

in subsection 4.3. A common finding is that

the bottom 40% of the global extraction results

are hardly useful. FUSE based on dsPMI
3wd (·, ·)

handles the noise in the data quite well. FUSE

based on dsLMI
3wd (·, ·), or ds

Jac.
3wd (·, ·) seems par-

tial to high-quality data. Similar to dsPMI
3wd (·, ·)

based FUSE, performance of LIN98 and JCS

drops while shrinking the number of relation in-

stances. This indicates they prefer recall to preci-

sion of the extraction results.

4.5 Feature Analysis

We have shown that the fusion of dsPMI
3wd (·, ·)

and rsi(·, ·) shows superior performance, yet each
sub-similarity’s contribution remains unclear. Us-

ing 100% global extraction results, we incremen-

tally add relation-based similarities to dsPMI
3wd (·, ·),

and report the fusion’s cross-validation perfor-

mance in Table 5. The order of addition is coin-

cide to the quality of the relations (see Table 4).

602



∼5% ∼10% ∼30% ∼60% 100%
Relation words # wd. Acc. # wd. Acc. # wd. Acc. # wd. Acc. # wd. Acc.

r1 w [Sibling]is 1,115 96.6 1,325 88.3 1,532 80.9 1,615 78.7 1,648 77.9
r2 w [Hyponym]is 263 77.6 627 38.8 1,018 27.8 1,227 23.9 1,378 21.8
r3 w [Hypernym]is 608 73.8 1,059 52.0 1,376 44.6 1,488 42.7 1,561 40.9
r4 w [Meronym]is 266 41.7 416 29.6 586 22.7 741 19.4 972 15.1
r5 w [Holonym]is 141 55.3 161 50.9 197 42.6 388 23.2 462 20.1

Table 4: Quantity and quality analysis of the 200 sampled words’ relation words.

Feature set Acc. ρ

dsPMI3wd 80.8 0.522

dsPMI3wd + rs1 83.1 0.559
dsPMI3wd + rs1 + rs3 83.4 0.587
dsPMI3wd + rs1 + rs3 + rs2 83.6 0.587
dsPMI3wd + rs1 + rs3 + rs2 + rs5 83.7 0.590
dsPMI3wd + rs1∼5 83.9 0.591

dsPMI3wd + rs2 81.3 0.522
dsPMI3wd + rs3 82.3 0.574
dsPMI3wd + rs4 81.2 0.532
dsPMI3wd + rs5 81.1 0.550

Table 5: FUSE’s performance on sub feature sets.

Unsurprisingly, rs1(·, ·), i.e. [Sibling]is based
similarity is the most effective, owing to its high

quality. rs3(·, ·) ([Hypernym]is) dominates the
rest of the performance improvement, and adding

it alone to dsPMI
3wd (·, ·) also largely improves the

performance. It is reasonable since comparing

the sibling terms or hypernyms (i.e. “what is

it”) are natural ways to compare words’ mean-

ings. Though “masked” by [Sibling]is and [Hyper-

nym]is, other relations also show their contribu-

tion (yet small) when added to dsPMI
3wd (·, ·) alone.

rs2(·, ·) ([Hyponym]is) is an exception, and its
weight is also negative in the trained models. This

indicates that hyponyms may not be an adequate

evidence for semantic similarity.

Given the bad quality of [Meronym]is and

[Holonym]is relations, their effectiveness seems

bizarre. In fact, though a great number of relation

words are not correct, they can be considered as

special context words. Owing to the design of the

relation-based similarity (Eq. 1), the distributional

similarities of those words still contribute to the

target words’ similarity calculation. This finding

allows us to relax the quality restriction of seman-

tic relation extraction. Our hierarchical approach

to semantics-aware distributional similarity would

work on the basis of noisy relation databases.

5 Conclusion

In this paper, we propose a hierarchical semantics-

aware distributional similarity scheme (DSS). We

introduce a semantic layer over the classical dis-

tribution layer by employing a semantic relation

extraction system and a mechanism that computes

words’ relation-specific similarities based on sim-

ple distributional similarity. Finally, the fusion of

the distributional and relation-based similarities is

completed by learning-to-rank.

Experiments show that the in-depth mining in

the corpus provides effective evidences for seman-

tic similarity. On our dataset, the fused similar-

ity significantly improves distributional similar-

ity, and also outperforms the baseline approaches

that blend the heterogeneous evidences in a sin-

gle vector. Additionally, intrinsic analysis shows

that [Sibling]is and [Hypernym]is relations are the

most effective semantic clues.

In future studies, we will experiment on more

elaborated combination similarity fusion mech-

anisms other that linear combination. We will

also explore more types of semantic evidences,

e.g. synonym, antonym, semantic attribute, or the-

matic relations such as agent / patient relations.

Acknowledgments

This work was supported by (1) the National High

Technology Research and Development Program

of China (863 Program, No. 2011AA01A207),

(2) the Natural Science Foundation of China (No.

61272384 & 61105072), (3) China Postdoctoral

Science Foundation (No. 2012M510220), and (4)

Beijing Postdoctoral Research Foundation (No.

2012ZZ-99).

References

Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Paşca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distri-
butional and wordnet-based approaches. In NAACL
’09, pages 19–27.

L. Douglas Baker and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classification. In SIGIR ’98, pages 96–103.

Mohit Bansal and Dan Klein. 2012. Coreference se-
mantics from web features. In ACL ’12, pages 389–
398.

603



Marco Baroni and Alessandro Lenci. 2010. Distri-
butional memory: A general framework for corpus-
based semantics. Comput. Linguist., 36(4):673–721.

Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based seman-
tic model based on properties and types. Cognitive
Science, 34(2):222–254.

Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In EMNLP ’10,
pages 1162–1172.

Katrin Erk and Sebastian Padó. 2008. A structured
vector space model for word meaning in context. In
EMNLP ’08, pages 897–906.

Stefan Evert. 2008. Corpora and collocations. In
A. Lüeling and M. Kytö, editors, Corpus Linguistics.
An International Handbook. Mouton de Gruyter.

Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Ba-
roni. 2013. Multi-step regression learning for
compositional distributional semantics. CoRR,
abs/1301.6939.

Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING ’92,
pages 539–545.

HIT-SCIR. 2006. Retrieved from: http:
//ir.hit.edu.cn/phpwebsite/index.

php?module=pagemaster&PAGE_user_

op=view_page&PAGE_id=162.

Alpa Jain and Marco Pennacchiotti. 2010. Open entity
extraction from web search query logs. In COLING
’10, pages 510–518.

Thorsten Joachims. 1999. Advances in kernel meth-
ods. chapter Making large-scale support vector ma-
chine learning practical, pages 169–184. MIT Press.

Nobuhiro Kaji and Masaru Kitsuregawa. 2008. Using
hidden markov random fields to combine distribu-
tional and pattern-based word clustering. In COL-
ING ’08, pages 401–408.

Zornitsa Kozareva, Konstantin Voevodski, and Shang-
Hua Teng. 2011. Class label enhancement via re-
lated instances. In EMNLP ’11, pages 118–128.

Dekang Lin. 1997. Using syntactic dependency as lo-
cal context to resolve word sense ambiguity. In ACL
’97, pages 64–71.

Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In ACL ’98, pages 768–774.

Diana McCarthy and Roberto Navigli. 2009. The en-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139–159.

Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similar-
ity methods for lexical entailment acquisition. In
COLING-ACL ’06, pages 579–586.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236–244.

Eric W. Noreen. 1989. Computer-intensive methods
for testing hypotheses. A Wiley-Interscience publi-
cation. Wiley, New York, NY [u.a.].

Marius Paşca. 2004. Acquisition of categorized named
entities for web search. In CIKM ’04, pages 137–
145.

Sebastian Padó and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Comput. Linguist., 33(2):161–199.

Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: leveraging generic patterns for automat-
ically harvesting semantic relations. In ACL ’06,
pages 113–120.

Patrick Pantel, Deepak Ravichandran, and Eduard
Hovy. 2004. Towards terascale knowledge acqui-
sition. In COLING ’04.

Marco Pennacchiotti and Patrick Pantel. 2009. Entity
extraction via ensemble semantics. In EMNLP ’09,
pages 238–247.

Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
Ninth Machine Translation Summit, pages 315–322.

Shuming Shi, Huibin Zhang, Xiaojie Yuan, and Ji-
Rong Wen. 2010. Corpus-based semantic class
mining: distributional vs. pattern-based approaches.
In COLING ’10, pages 993–1001.

Keiji Shinzato and Kentaro Torisawa. 2007. A Simple
WWW-based Method for Semantic Word Class Ac-
quisition. In RANLP 2005, volume 292 of Current
Issues in Linguistic Theory, pages 207–216. John
Benjamins.

Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS.

James H. Steiger. 1980. Tests for comparing ele-
ments of a correlation matrix. Psychological Bul-
letin, 87(2):245–251, March.

Stefan Thater, Hagen Fürstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In ACL
’10, pages 948–957.

Peter D. Turney. 2006. Similarity of semantic rela-
tions. Comput. Linguist., 32(3):379–416, Septem-
ber.

Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, and
Chin-Yew Lin. 2011. Nonlinear evidence fusion
and propagation for hyponymy relation mining. In
ACL ’11, pages 1159–1168.

604


