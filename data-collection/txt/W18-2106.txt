






































Marcello Federico
MMT Srl / FBK  Trento, Italy

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 207



Symbiotic Human and Machine Translation

MT seamlessly 
• adapts to user data
• learns from post-editing

user enjoys 
• enhanced productivity
• better user experience

3

Usable technology for the translation industry

• easy to install and deploy
• fast to set-up for a new project
• effective, also on small projects
• scalable with data and users
• works with commodity hardware

4

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 208



The Modern MT way

(1)  connect your CAT with a plug-in
(2)  drag & drop your private TMs
(3)  start translating!

5

Modern MT in a nutshell

zero training time 
adapts to context
learns from user corrections
scales with data and users

6

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 209



Training data is a dynamic collection of Translation Memories

At any time: 

● new TMs are added 
● existing TMs are extended

Training time comparable to uploading time!

7

Context aware translation

party

CONTEXT

We are going out.

TRANSLATION

fête

SENTENCE

CONTEXT

We approved the law 

TRANSLATION

parti

8

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 210



   
 

9

      requests

Machine 
Translation   suggestions

    post-edits Incremental Learning

Simple. Adaptive. Neural. 

Core technology [original plan]

context analyser
phrase-based decoder
adaptive models
incremental structures
parallel processing 

10

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 211



Simple. Adaptive. Neural. 

Language support

● 45 languages

● fast pre-/post-processing

● simple interfaces

● tags and XML management

● localization of expressions

● TM cleaning

Simple. Adaptive. Neural. 

Context Analyzer

A
B
C

50%

45%

5%

● analyze input text

● retrieve best matching TMs

● compute matching scores

● dynamic structure

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 212



Simple. Adaptive. Neural. 

Adaptive Phrase Table

1000
Suffix Array with 

Ranked Sampling

● suffix array indexed with TMs 

● phrases sampled on demand

● priority sampling over TMs 

● dynamic structure 

Simple. Adaptive. Neural. 

Adaptive Language Model

A

B

C

∑ w • p
● large static background model

● n-grams stats indexed with TMs  

● combination of active TM LMs

● TM LMs computed on the fly

● dynamic structure 

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 213



15

M. Cettolo, et al. (2016), The IWSLT 2016 Evaluation Campaign, IWSLT. 

TED Talks  
English-French

Simple. Adaptive. Neural. 

Second Prototype  (0.14 January 2017) 

Domains: ECB, Gnome, JRC, KDE, 
OpenOffice, PHP, Ubuntu, UN-TM

Open benchmark:

- Training speed:
12x Moses - 100x NMT

- MT quality (BLEU):
+1 vs Moses   
-0.5 vs NMT Ada

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 214



Simple. Adaptive. Neural. 

What happened

Research on adaptive neural MT 

Believed PBMT was competitive on technical translation

 Finally realised superiority of NMT quality

Completed PBMT release and switched to NMT

 Data collection for 14 translation directions 

Simple. Adaptive. Neural. 

Roadmap from last review meeting 

2015 Q2 2016 Q2 2016 Q4 2017 Q4

minimum 
viable product

context aware 
1 lang pair

first alpha 
release

fast training,
context aware,
distributed,
1 lang pair

first beta 
release

online learning
plug-in,
3 lang pairs

final release 

neural MT,
enterprise 
ready,
14 lang pairs

technology 
switch

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 215



Multi-user scenario

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 216



Multi-user scenario

Multi-user scenario

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 217



All we need is a memory

24

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 218



All we need is a memory

25

All we need is a memory

26

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 219



All we need is a memory

27

All we need is a memory

28

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 220



Multi-user adaptive NMT

29

Multi-user adaptive NMT

30

Instances are 
selected by 
combining 
context scores and 
similarity scores 

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 221



Adaptation, too! Source: Farajian et al, “Multi-Domain Neural MT 
through Unsupervised Adaptation”, Proc. WMT 2017.

31

Farajian et al. (2017) “Multi-domain NMT through unsupervised adaptation”, WMT.

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 222



33

Sep: integration of MateCat
Oct: NMT code released 
Nov: co-development 

release of 14 engines
Dec: performance boost

34

Relative BLEU 
scores wrt
Google Translate 

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 223



35

Performance of 
generic MMT 
1-6 scale 
(w/o adaptation)

Progression in one month on English-Italian

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 224



37

38

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 225



Simple. Adaptive. Neural. 

Noisy training data

EN: What history teaches us

IT: === Storia ==================================

Simple. Adaptive. Neural. 

Data Cleaning

We added a simple QE module to filter out bad examples:

● Apply Fast-Align in two directions
● Compute Model 1 scores in two directions
● Combine and normalize scores
● Filter out on the distribution of scores

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 226



Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 227



43

44

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 228



45

We compare:

● Generic MT:  production engine [En-It]
● Custom MT: Generic MT tuned on TM [takes hours]
● +Adaptive MT: Generic MT adapted on TM [real-time]
● +Incremental MT: TM updated with simulated PE

46

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 229



47

48

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 230



49

50

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 231



Use post-editing as new training instances 

Perform one/more iterations 

Can be combined with a priori adaptation

Updates generic or adapted model

 

Turchi et al. (2017), Continuous learning from human post-edits for NMT, EAMT.

52

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 232



53

54

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 233



55

56

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 234



57

Online-learning contribution is consistent

Does it scale with number of domains?

Incremental learning contributes marginally

Probably depends on test set size 

We are not always able to beat specialized models 

How to improve further adaptation ? 

 

Source: Turchi et al. (2017) “Continuous learning from human post-edits for NMT”, EAMT.

58

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 235



Can improve MT without touching it inside 

We can adapt an “external” MT  service!

Similar to NMT: two inputs (src,mt), one output (ape)

Can be trained with less data than NMT

We can deploy instance based adaptation 

 
Chatterjee  et al. (2017), Multi-source Neural APE: FBK’s participation …. , WMT.

60

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 236



61

Neural APE uses two 
encoders and two 
attention models, 
which are merged and 
used by one decoder. 

62

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 237



63

64

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 238



65

66

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 239



67

Can improve on top of static and adaptive engine! 

Uses incremental learning, adaptation and online learning

Portable (in principle) on the multi-domain setting  

Limited gain on top of full-fledged adaptive NMT 

Can be an extra component to manage

 

68

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 240



Multi-user scenario goes beyond simple domain adaptation  

We need to handle multiple evolving domains

Domain customization is not an option

 Real-time adaptation/learning works! 

But, there is still room for improvement! 

 

70

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 241



Thank You

Website
www.ModernMT.eu

Github
github.com/ModernMT/MMT

Proceedings for AMTA 2018 Workshop: Translation Quality Estimation and Automatic Post-Editing Boston, March 21, 2018   |   Page 242


	AMTA_2018_Workshop_Proceedings_QEAPE
	Wks3_Front_Material

	AMTA_2018_Workshop_Proceedings_QEAPE_3
	405_update
	JoaoGraca_qeape2018_footer
	MaximKhalilov_qeape2018_footer
	MarcinJunczys-Dowmunt_qeape2018_enlarge_footer
	MarcelloFederico_qeape2018_footer
	406_footer
	403_footer



