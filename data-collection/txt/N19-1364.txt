



















































Let's Make Your Request More Persuasive: Modeling Persuasive Strategies via Semi-Supervised Neural Nets on Crowdfunding Platforms


Proceedings of NAACL-HLT 2019, pages 3620–3630
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3620

Let’s Make Your Request More Persuasive: Modeling Persuasive
Strategies via Semi-Supervised Neural Nets on Crowdfunding Platforms

Diyi Yang∗, Jiaao Chen∗, Zichao Yang, Dan Jurafsky, Eduard Hovy
Georgia Institute of Technology, Carnegie Mellon University, Stanford University

diyi.yang@cc.gatech.edu
{jiaaoc, zichaoy, hovy}@andrew.cmu.edu

jurafsky@stanford.edu

Abstract

Modeling what makes a request persuasive—
eliciting the desired response from a reader—
is critical to the study of propaganda, behav-
ioral economics, and advertising. Yet current
models can’t quantify the persuasiveness of re-
quests or extract successful persuasive strate-
gies. Building on theories of persuasion, we
propose a neural network to quantify persua-
siveness and identify the persuasive strategies
in advocacy requests. Our semi-supervised hi-
erarchical neural network model is supervised
by the number of people persuaded to take ac-
tions and partially supervised at the sentence
level with human-labeled rhetorical strategies.
Our method outperforms several baselines,
uncovers persuasive strategies—offering in-
creased interpretability of persuasive speech—
and has applications for other situations with
document-level supervision but only partial
sentence supervision.

1 Introduction

Crowdfunding platforms are a popular way to
raise funds for projects. For example, Kiva, a peer-
to-peer lending platform, has crowd-funded more
than a million loans, totaling over $1 billion since
2005. Kickstarter, another online crowdfunding
platform, successfully funded 110,270 projects
with a total of over 2 billion dollars. Yet most
projects still suffer from low success rates. How
can we help requesters craft persuasive and suc-
cessful pitches to convince others to take actions?

Persuasive communication has the potential to
shape and change people’s attitudes and behaviors
(Hovland et al., 1953), and has been widely re-
searched in various fields such as social psychol-
ogy, marketing, behavioral economics, and politi-
cal campaigning (Shrum et al., 2012). One of the

∗Equal contribution. This work was done when the first
two authors were students at CMU.

most influential theories in the advertising liter-
ature is Chaiken’s systematic-heuristic dual pro-
cessing theory, which suggests that people pro-
cess persuasive communication by evaluating the
quality of arguments or by relying on inferential
rules. Some such heuristic rules are commonly
used in consumer behaviors; commercial websites
may highlight the limited availability of their items
“In high demand - only 2 left on our site!” or em-
phasize the person in authority “Speak to our head
of sales—he has over 15 years’ experience sell-
ing properties” to attract potential consumers. Al-
though numerous studies on persuasion have been
conducted (Chaiken, 1980), we still know little
about the way how persuasion functions in the
wild and how it can be modeled computationally.

In this work, we utilize neural-network based
methods to computationally model persuasion in
requests from crowdfunding websites. We build
on theoretical models of persuasion to operational-
ize persuasive strategies and ensure generalizabil-
ity. We propose to identify the persuasive strat-
egy employed in each sentence in each request.
However, constructing a large dataset with persua-
sion strategies labeled at the sentence level is time-
consuming and expensive. Instead, we propose to
use a small amount of hand-labeled sentences to-
gether with a large number of requests automati-
cally labeled at the document level by the number
of persuaded support actions. Our model is a semi-
supervised hierarchical neural network that iden-
tifies the persuasive strategies employed in each
sentence, where the supervision comes from the
overall persuasiveness of the request. We propose
that the success of requests could have substan-
tive explanatory power to uncover their persuasive
strategies. We also introduce an annotated cor-
pus with sentence-level persuasion strategy labels
and document-level persuasiveness labels, to fa-
cilitate future work on persuasion. Experiments



3621

show that our semi-supervised model outperforms
several baselines. We then apply this automated
model to unseen requests from different domains
and obtain nuanced findings of the importance of
different strategies on persuasion success. Our
model can be useful in any situation in which we
have exogenous document-level supervision, but
only small amounts of expensive human-annotated
sentence labels.

2 Related Work
Computational argumentation has received much
recent attention (Ghosh et al., 2016; Stab and
Gurevych, 2017; Peldszus and Stede, 2013; Stab
et al., 2018; Ghosh et al., 2014). Most work has ei-
ther identified the arguments in news articles (Sar-
dianos et al., 2015) or user-generated web con-
tent (Habernal and Gurevych, 2017; Musi et al.,
2018), or classified argument components (Zhang
and Litman, 2015) into claims and premises, sup-
porting and opposing claims, or backings, re-
buttals and refutations . For example, Stab
and Gurevych (2014) proposed structural, lex-
ical, syntactic and contextual features to iden-
tify convincing components of Web arguments in-
cluding claim, major claim, and premise. Sim-
ilarly, Zhang and Litman (2015) studied stu-
dent essay revisions and classified a set of ar-
gumentative actions associated with successful
writing such as warrant/reasoning/backing, rebut-
tal/reservation, and claims/ideas. Habernal and
Gurevych (2016) investigated the persuasiveness
of arguments in any given argument pair using
bidirectional LSTM. Hidey et al., (2017) utilized
the persuasive modes—ethos, logos, pathos—to
model premises and the semantic types of argu-
ment components in an online persuasive forum.

While most computational argumentation fo-
cuses on the relational support structures and fac-
tual evidence to make claims, persuasion focuses
more on language cues aimed at shaping, rein-
forcing and changing people’s opinions and be-
liefs. How language changes people’s attitudes
and behaviors have received less attention from
the computational community than argumentation,
although there have been important preliminary
work (Persing and Ng, 2017; Carlile et al., 2018).
Farra et al., (2015) built regression models to pre-
dict essay scores based on features extracted from
opinion expressions and topical elements. Chat-
terjee et al., (2014) used verbal descriptors and
para-verbal markers of hesitation to predict speak-

ers’ persuasiveness on website housing videos of
product reviews. When looking at persuasion in
the context of online forum discussions (Wei et al.,
2016), Tan et al., (2016) found that on the Change
My View subreddit, interaction dynamics such as
the language interplay between opinion holders
and other participants provides highly predictive
cues for persuasiveness. Using the same dataset,
Wel et al., (2016) extracted a set of textual infor-
mation and social interaction features to identify
persuasive posts.

Recently, Pryzant et al., (2017) introduced a
neural network with an adversarial objective to
select text features that are predictive of some
outcomes but decorrelated with others and fur-
ther analyzed the narratives highlighted by such
text features. Further work extended the model
to induce narrative persuasion lexicons predictive
of enrollment from course descriptions and sales
from product descriptions (Pryzant et al., 2018a),
and the efficacy of search advertisements (Pryzant
et al., 2018b). Similar to their settings, we use
the outcomes of a persuasive description to su-
pervise the learning of persuasion tactics, and our
model can similarly induce lexicons associated
with successful narrative persuasion by examining
highly attentional words associated with persua-
sion outcomes. Our work differs both in our semi-
supervised method and also because we explicitly
draw on the theoretical literature to model the per-
suasion strategy for each sentence in requests, al-
lowing requests to have multiple persuasion strate-
gies; our induced lexicons can thus be very spe-
cific to different persuasion strategies.

Other lines of persuasion work predict the suc-
cess of requests on peer-to-peer lending or crowd-
funding platforms, and mainly exploit request at-
tributes like project description (Greenberg et al.,
2013), project videos (Dey et al., 2017), and so-
cial predictors such as the number of backers (Et-
ter et al., 2013) or specific types of project up-
dates (Xu et al., 2014). Among them, only a
few investigated the effect of language on the
success of requests. Althoff et al., (2014) stud-
ied donations in Random Acts of Pizza on Red-
dit, using the social relations between recipient
and donor plus linguistic factors to predict the
success of these altruistic requests. Based on a
corpus of 45K crowd-funded projects, Mitra and
Gilbert (2014) found that 9M phrases commonly
present in crowd-funding have reasonable predic-



3622

tive power in accounting for variance around suc-
cessful funding, suggesting that language does ex-
hibit some general principles of persuasion. Al-
though this prior work offers predictive and in-
sightful models, most studies chose their persua-
sion labels or variables without reference to a tax-
onomy of persuasion techniques nor to a princi-
pled method of choosing them. Some exceptions
include Yang and Kraut (2017), Dey et al., (2017),
and Rosenthal and McKeown (2017). For exam-
ple, Yang and Kraut (2017) looked at the effective-
ness of a set of persuasive cues in Kiva requests
and found that certain heuristic cues are positively
correlated with lenders’ contribution.

Inspired by these prior work, we operational-
ize persuasive strategies based on theories of per-
suasion and aim to learn local structures/labels
of sentences based on the global labels of para-
graphs/requests. Our task is different from most
previous work on semi-supervised learning for
NLP (Liang, 2005; Yang et al., 2017) that focuses
on the setting with partial data labels. While in
computer vision, there is a lot of prior work in us-
ing image global labels to uncover local pixel level
labels and bounding boxes of objects (Oquab et al.,
2015; Pinheiro and Collobert, 2015), the investiga-
tion of this task in NLP, to the best of our knowl-
edge, is novel and could potentially have much
broader applications.

3 Research Context
We situate this research within the team forums
of Kiva1, the largest peer-to-peer lending web-
site. These self-organized lending teams are built
around common interests, school affiliation or lo-
cation. In such teams, members can post messages
in their team discussion board to persuade other
members to lend to a particular borrower. One
such message is shown in Figure 1. A borrower,
Sheila, posted a message on Kiva to request loans
for woman-led group. As highlighted in the fig-
ure, she made use of several persuasion strategies
such as commitment, concreteness, and impact to
render her request more persuasive. We define the
persuasiveness score of a request message as the
number of team members (in log-scale) who read
the message and make loans to the mentioned bor-
rower. We then regard this overall persuasiveness
of messages as high-level supervision for training
our model to determine which persuasion strategy

1https://www.kiva.org/

Figure 1: An anonymized advocating message that per-
suaded 5 members to lend to the mentioned borrower.
Persuasion strategies are highlighted.

is used in each sentence inside each message.

4 Persuasion Strategies
Numerous studies have investigated the basic prin-
ciples that govern getting compliance from people
(Cialdini and Garde, 1987; Petty et al., 1983). In
this work, we utilized Chiaken’s 1980 systemic-
heuristic model of social information processing,
which suggests that people process persuasive re-
quests by assessing the quality of arguments (sys-
tematic processing) or by relying on heuristic rules
(heuristic processing). Building on that, we first
borrow several commonly used heuristic princi-
ples (Cialdini and Garde, 1987) that are also suit-
able for our context as below.

• Scarcity states that people tend to value an
item more as soon as it becomes rare, distinct
or limited. For example, take the use of ‘ex-
pire’in this message: “This loan is going to
expire in 35 mins...”.

• The principle of Emotion says that mak-
ing messages full of emotional valence and
arousal affect (e.g., describing a miserable
situation or a happy moment) can make peo-
ple care and act, e.g., “The picture of widow
Bunisia holding one of her children in front of
her meager home brings tears to my eyes..”,
similar to Sentiment and Politeness used by
Althoff et al., (2014) and Tan et al., (2016),
and Pathos used by Hidey et al., (2017).

• Commitment states that once we make a
choice, we will encounter pressures that
cause us to respond in ways that justify our
earlier decision, and to convince others that
we have made the correct choice. Here it
could be mentioning their contribution in the
message, e.g., “I loaned to her already.”

• Social Identity refers to people’s self-
concept of their membership in a social

https://www.kiva.org/


3623

group, and people have an affinity for their
groups over others, similar to name mentions
in Rosenthal and McKeown (2017). Thus if
a loan request comes from their own groups,
they are more likely to contribute, such as
“For those of you in our team who love bread,
here is a loan about bakery.”

• Concreteness refers to providing concrete
facts or evidence, such as “She wishes to
have a septic tank and toilet, and is 51%
raised and needs $825”, similar to Claim
and Evidence (Zhang et al., 2016; Stab
and Gurevych, 2014)), Evidentiality (Althoff
et al., 2014), and Logos (Hidey et al., 2017).

We also propose a new strategy to capture impor-
tance or impact on these requests:

• Impact and Value emphasizes the impor-
tance or bigger impact of this loan, such as
“... to grow organic rice. Then, she can pro-
vide better education for her daughter”.

Note that other persuasion tactics such as Reci-
procity — “feel obligated to return something af-
ter receiving something of value from another”
— and Authority — “comply with the requests
of authority in an unthinking way to guide their
decisions”— are also widely used in persuasive
communication. However, in this context, we did
not observe enough instances of them.

5 Semi-supervised Neural Net

Given a message M = {S0, S1, ..., SL} consist-
ing of L sentences that the author posted to advo-
cate for a loan, our task is to predict the persua-
sion strategies pi employed in each sentence Si,
i ∈ [0, L]. However, purely constructing a large-
scale dataset that contains such labels of sentence-
level persuasion strategy is often time-consuming
and expensive. Instead, we propose to utilize a
small amount of labeled and a large amount of un-
labeled data. We design a semi-supervised hier-
archical neural network to identify the persuasive
strategies employed in each sentence, where the
supervision comes from the sentence-level labels
g in a small portion of data and the overall persua-
siveness scores y of messages. The overall archi-
tecture of our method is shown in Figure 2.

5.1 Sentence Encoder
Given a sentence Si with words wi,j , j ∈ [0, l]
and l is the sentence length, a GRU (Bahdanau

Figure 2: The overall model architecture. The blue part
describes the sentence encoder. Sentences with labels
of persuasion strategies are highlighted with dark blue
like p1. The orange part shows the document encoder.

et al., 2014) is used to incorporate contextual cues
of words into hidden state hi,j . This GRU reads
the sentence Si from wi,1 to wi,l and encodes each
word wi,j with its context into hidden state hi,j :

hi,j = GRU(Wewi,j , hi,j−1), j ∈ [0, l]. (1)

where We is the word embedding matrix. To learn
the characteristic words associated with the per-
suasive strategy in a sentence, we apply an at-
tention mechanism (Bahdanau et al., 2014; Yang
et al., 2016). The representation of those words
are then aggregated to form the sentence vector si.
We formulated this word level attention as follows:

ui,j = tanh(Wwhi,j + bw) (2)

αi,j =
exp(uᵀi,juw)∑
k exp(u

ᵀ
i,kuw)

(3)

si =
∑
j

αi,jhi,j (4)

where uw is a context vector that queries the char-
acteristic words associated with different persua-
sion strategies. It is randomly initialized and
jointly learned from data.



3624

5.2 Latent Persuasive Strategies
We assume that each sentence instantiates only
one type of persuasion strategy. For example, a
sentence “She is 51% raised and needs $825 in 3
days” employs Scarcity, trying to emphasize lim-
ited time availability. We propose to use the high
level representation of each sentence to predict the
latent variable:

pi = softmax(Wvsi + bv) (5)

5.3 Document Encoder
After obtaining the sentence vector pi, we can get
a document vector in a similar way:

hi = GRU(pi, hi−1), i ∈ [0, L] (6)

whereL denotes the number of sentences in a mes-
sage. Similarly, we introduce an attention mecha-
nism to measure the importance of each sentence
and its persuasion strategy via a context vector us:

ui = tanh(Wshi + bs) (7)

αi =
exp(uᵀi us)∑
k exp(u

ᵀ
kus)

(8)

v =
∑
i

αihi (9)

5.4 Semi-Supervised Learning Objective
The document vector v is a high-level representa-
tion of the document and can be used as a set of
features for predicting ỹ, the persuasiveness of a
message, i.e., how many team members will make
loans to the project mentioned in this message. We
also include a context vector c to further assist the
prediction of making loans. For instance, c could
represent the number of team members in a team,
the total amount of money contributed by this team
in the past, etc.

ỹ =Wf [v, c] + bf (10)

We then can use the mean squared error be-
tween the predicted and ground truth persuasive-
ness as training loss. To take advantage of the la-
beled subset that has sentence level annotation of
persuasive strategies, we reformulate this problem
as a semi-supervised learning task:

l =γ
∑
d∈CL

(yd − ỹd)2 − β
∑
−gi log pi (11)

+ (1− γ)
∑

d′∈CU

(yd′ − ỹd′)2 (12)

Here, CL refers to the document corpus with sen-
tence level persuasion labels. CU denotes those
without any sentence labels. gi refers to the per-
suasion strategy in sentence Si, and pi is predicted
by our model. γ and β are used as re-weight fac-
tors to trade off the penalization and reward intro-
duced by different components.

6 Experiment
6.1 Dataset

Our collaboration with Kiva provided us access to
all public data dumps of the team discussion fo-
rums on Kiva. Here we only focused on messages
that have explicit links because in most cases,
members need to include the loan link to better
direct others to a specific loan or borrower. Af-
ter removing messages that do not contain any
links, we obtained 41,666 messages that contain
loan advocacy. We used Amazon’s Mechanical
Turk (MTurk) to construct a reliable, hand-coded
dataset to obtain the persuasion strategy label for
each sentence. To increase annotation quality, we
required Turkers to have a United States location
with 98% approval rate for their previous work
on MTurk. Since messages often contain differ-
ent numbers of sentences, which might be asso-
ciated with different sets of persuasion strategies,
we sampled 200 messages for each fixed message
length ranging from one sentence to six sentences,
in order to guarantee that our hand-coded dataset
reasonably represents the data. Messages with at
most six sentences accounted for 89% percentages
among all messages in our corpus. Each sentence
in a message was labeled by two Mechanical Turk
Master Workers 2. To assess the reliability of the
judges’ ratings, we computed the intra-class cor-
relation (ICC), and obtained an overall ICC score
of 0.524, indicating moderate agreement among
annotators (Cicchetti, 1994). The distribution for
each persuasion strategy in the annotated corpus is
described in the blue line in Figure 3. We assigned
a persuasion label to a sentence if two annotators
gave consistent labels for it, and filtered out sen-
tences that annotators disagreed on the label.

In the final annotated corpus, there were 1200
messages, with 2898 sentences. The average num-
ber of sentences is 2.4 and the average number
of words per sentence is 17.3. For predicting the
persuasive strategy in each sentence, we randomly

2https://www.mturk.com/worker/help:
What-is-a-Mechanical-Turk-Master-Worker

https://www.mturk.com/worker/help:What-is-a-Mechanical-Turk-Master-Worker
https://www.mturk.com/worker/help:What-is-a-Mechanical-Turk-Master-Worker


3625

Figure 3: The distribution of each persuasion strategy
in the annotation corpus and in the whole unlabeled
corpus after prediction.

split 80% of this annotated corpus as the training
set (2271 sentences in 1060 messages), 10% as
the validation set (322 sentences in 70 messages),
and 10% as the testing set (305 sentences in 70
messages). To further utilize supervision from the
persuasiveness score of each message, we merged
1060 documents with sentence labels and 40,466
unlabeled messages, using it as the final training
set for training semi-supervised models.

6.2 Model Setup and Baselines
We split documents into sentences and tokenize
each sentence using Stanford’s CoreNLP (Man-
ning et al., 2014). Words appearing less than 5
times were replaced with a special UNK token.
We trained the hyperparameters of the models on
the validation set using Adam (Kingma and Ba,
2014). Specifically, we set the word embedding
dimension to be 128, where the word embeddings
are initialized randomly, and GRU dimension to
be 256. The learning rate is set to be 5e-5. The
balancer γ is the ratio of labeled data in a batch of
training data. The balancer β is selected via grid
search, searching in a set of (5, 10, 20, 50, 100),
resulting in β=10.

We propose several baselines to predict the sen-
tence level persuasion strategies for comparison
with our model. (1) SVM + BoW is a SVM classi-
fier with RBF kernel using bag-of-words features
(one-hot). (2) GRU uses the hidden state at the last
word as features to classify persuasive strategies,
a special case of our SH Net model without the su-
pervision from the overall persuasiveness scores.
(3) bi-GRU uses bi-directional GRU.

H Net is a hierarchical GRU for classifying
strategies with the supervision from the overall

persuasiveness scores as shown in Figure 2, but it
only adopts all the annotated messages. We denote
our semi-supervised hierarchical model as SH Net
(Semi Hierarchical Net), which utilizes both an-
notated messages and unlabeled corpus. Semi-Att
Net builds on SH Net by incorporating both word-
level and sentence-level attention. In addition to
the textual cues in the advocation message, per-
suasive requests also depend on the context. We
introduced a set of contextual descriptors into our
semi-supervised hierarchical network, denoted as
SH-Att Plus Net. Such features include the number
of borrowers in this message, the number of team
members in a team, the total amount of money
contributed by this team, the number of messages
ever posted in the discussion board of this team,
and the amount of money requested in this loan.

6.3 Results
We evaluated the baselines and our hierarchical
neural network models using accuracy, macro-
averaged F1 score, macro-averaged precision and
macro-averaged recall, as well as RMSE for eval-
uating the message level persuasiveness score pre-
diction. As we can see in Table 1, when pre-
dicting the persuasive strategies (6 types of per-
suasive strategies plus an Other strategy), BoW +
SVM gives a performance of 0.347 and a macro F1
of 0.229. A direct neural network GRU boosted
the accuracy to 0.518, demonstrating the effec-
tiveness of neural networks for sentence classifi-
cation. When bi-directional contextual informa-
tion is used, the sentence level prediction per-
formance is 52.1%. Our hierarchical neural net-
work achieved an accuracy of 48.2% and a macro
F1 of 0.432. When incorporating the whole cor-
pus of unlabeled messages, our semi-supervised
neural network achieved an accuracy of 56.1%
(16.4% improvement over H Net). This indi-
cates that our semi-supervised model effectively
takes advantage of the supervision from the small
amount of labeled data and the overall persuasive-
ness scores. Moreover, we noticed that this semi-
supervised neural network not only helps predict
the sentence level persuasion strategies, but also
assist the prediction of messages’ overall persua-
siveness with a 9% RMSE decrease. Semi-Att out-
performed SH Net with an accuracy of 56.9%,
and a macro F1 score of 0.518. Although the
improvement from attention is minor (but signif-
icant), it’s important for visualizing associations
between words, persuasion strategies and persua-



3626

Evaluating Sentence Level Strategies Doc Level
Model Accuracy Macro F1 Macro Precision Macro Recall RMSE

SVM (RBF) + BoW 0.347 0.229 0.364 0.167 -
GRU 0.518 0.479 0.479 0.479 -

bi-GRU 0.521 0.440 0.445 0.436 -
Hierarchical Net (H Net) 0.482 0.432 0.430 0.432 1.15

Semi Net (SH Net)* 0.561 0.513 0.504 0.522 1.05
Semi-Att Net* 0.569 0.518 0.512 0.534 1.04

Semi-Att Plus Net 0.552 0.513 0.515 0.512 0.87

Table 1: Results of different models. * indicates that the model is significantly better than the one above it.

Figure 4: The accuracy for each persuasion strategy
evaluated via GRU, SH Net and Semi-Att.

Figure 5: Model performances with different portion of
unlabeled data (a) and labeled data (b).

sion outcomes. Interestingly, incorporating con-
textual descriptors did not help the prediction of
persuasion strategies. However, such contextual
information strongly predicted the overall persua-
siveness, decreasing RMSE to 0.84 from 1.04.

Strategy-Level Performance: We also report
the accuracy per persuasion strategy category via
Semi-Att, SH Net and simple GRU in Figure 4. It
seems that overall neural models are better at cap-
turing persuasion strategies such as concreteness,
identity and scarcity. This might be because peo-
ple are concrete by using specific terms such as
numbers or entities that are easy to model. Simi-

Strategy Top Ranked Keywords
Commitment joined, lenders, loaning, lend, loan

just, join, loaned, made, lent
Concreteness women, married, old, heads, year-old

money, sells, years, business, number
Emotion hard, thank, better, grief, great

maybe, help, please, thanks, happy
Identity promotion, shall, captain, form, number

spirits, lenders, member, team
Impact improve, new, better, products, money

to, use, business, more, order
Scarcity minutes, there’s, now, soon, go

expire, hours, days, number, left

Table 2: Top ranked keywords for persuasion strategies

lar principles might also occur for social identity
and scarcity where the use of words such as “we”,
“our” and “expire”, “left” can reveal a lot about
the persuasion strategies.

Different Percentage of Labeled Data: To fig-
ure out the importance of supervision from mas-
sages’ overall persuasiveness scores, we experi-
ment on SH Net with all the labeled messages.
To this end, we include all the labeled messages,
and vary the percentage of unlabeled corpus from
0%, 25%, 50%, 75%, to 100%, in Figure 5 (a).
We found that as the amount of unlabeled mes-
sages increases, the accuracy of sentence level pre-
diction increases as well, which further validates
the effectiveness of the semi-supervised setting for
persuasion strategy prediction. Similarly, to in-
vestigate the predictive power introduced by the
sentence level labels, we also vary the percent-
age of labeled messages from 25%, 50%, 75%,
to 100% when including the whole unlabeled cor-
pus, as shown in Figure 5 (b). As expected, hav-
ing more training data about sentence-level anno-
tation increases the prediction performance. Over-
all, these experiments demonstrate the effective-
ness of semi-supervised models for predicting sen-
tence level persuasion strategies. This enables us



3627

Scarcity 5 days left $3475 needed .
Concreteness We can do this ! Rosa sells natural

fruit juices for daily needs .
Concreteness Her business income is able only to

pay for the costs of maintaining her
home.

Impact She is requesting a new loan in
order to buy more product to cover
all of her customer demands

Impact All of these subjects help them better
their lives both economically and developmentally

Other Thank you kiva investors .

(a) Predicted persuasiveness: 2.56 (after natural logarithm)

Other I found a caterer for the celebration .
Scarcity and she has only one day left to

raise $775 HELP !
Concreteness Each day she surprises her customers with a

new flavour and aroma .
Concreteness She can give us a menu of rice,

beans. She is 38 years old and has
3 children for whom she works hard to

provide better future
Concreteness Their education is the most important thing to

her .
Concreteness She is an enterprising women with dreams .

(b) Predicted persuasiveness: 1.75

Figure 6: Attention Visualization. Left-most columns in red represent sentence-level attention and the remaining
columns in blue are word-level attention.

to obtain sentence level labels for any given para-
graphs by using a small amount of labeled data.

6.4 Visualization
To validate whether our semi-supervised model
captures characteristic words and sentences in re-
quests, we visualize the attention in a sentence in
Figure 6. We show the predicted persuasion la-
bel for each sentence in a message in red (left-
most columns in Figure 6(a) and 6(b)), with the
color scale indicating its learned attention weight.
Word-level attention is highlighted in blue (re-
maining columns). As we can see in Figure 6(b),
our model places emphasis on Scarcity, and high-
lights words such as “left” and “day” that carry
the scarcity meaning. Similarly, in the second
message— 5 days left 3475 needed—our model
first labeled the sentence as Scarcity, and then
picked words such as “days” and “left”. Sentences
predicted as Concreteness seem to contain specific
entities and concepts such as “business”, “her”,
and “home”. For Impact, our model accurately lo-
calizes words like “in order to” and “cover”.

To demonstrate that our model can learn repre-
sentative words associated with different persua-
sion strategies, we show the 10 highest-scoring
words from sentences with different labels in Ta-
ble 2. Interestingly, Commitment is highly associ-
ated with words such as “made” and “loan”. Ex-
plicit mentions of “thanks” and “hard” were found
in sentences with Emotional labels. Sentences that
emphasize their “team” as a whole were labeled as
Identity. Overall, this validates that our model is
able to select informative words associated with
different persuasion strategies.

For further illustration, we visualized the at-
tention weight distributions of different persua-
sion strategies. Since the number of sentences
inside each message is intertwined with attention
weights, we only plotted the distributions for mes-

Figure 7: Attention weight distributions of persuasion
strategies in requests with 2-3 sentences.

sages with two or three sentences in Figure 7. We
observed that Scarcity, Identity, and Impact seem
to play a relatively more important role for influ-
encing the success of requests, whereas Emotional
language, Commitment and Concreteness seem to
concentrate more on the lower weight ranges.

7 Importance of Persuasive Strategies

After applying the semi-supervised hierarchical
neural network to the unlabeled 40552 messages,
we obtained their sentence-level persuasive strate-
gies usages. We showed the distribution of each
persuasive strategy in the whole corpus in Figure
3, as described by the orange line. To further in-
vestigate how important each persuasive strategy
is for convincing others to make loans, in this sec-
tion, we present results on which of them are pre-
dictive via linear regression. All variables are stan-
dardized before entering the regression model. We
controlled for the number of team members in a
team, the total amount of money contributed by
this team, the number of messages posted in the
discussion board of this team. Since those vari-
ables are highly correlated with each other, we av-



3628

Persuasion Kiva RAOP
Strategy (Coef.) (Odds ratio)
Concreteness 0.041*** 1.111***
Commitment -0.015** 1.062
Emotional 0.030*** 1.145***
Identity 0.087*** 1.104**
Impact/Value 0.024*** 1.084*
Scarcity -0.076*** 1.118***

Table 3: The influences of different persuasive strate-
gies on request success on Kiva and RAOP. Here,
p<0.001:***; p<0.01:**; p<0.05:*.

eraged them into a single variable to capture these
team level attributes. We also controlled for the
amount of money the borrower requested. We
represented each message as a 6-dimensional vec-
tor to capture the amount of each persuasive strat-
egy, which is calculated by selecting the maximum
probability associated with each strategy from all
sentences in this message. The persuasive strat-
egy features significantly improve the model fit, as
indicated by a 11.8% improvement in adjusted R-
squared from 0.152 to 0.170. To demonstrate the
generalizability of our persuasion strategies and
the resulted semi-supervised model, we also ap-
plied our Semi-Att model to 5671 textual requests
for pizza from the Reddit community “Random
Acts of Pizza” (RAOP). Specifically, we used the
data released by Althoff et al., (2014) where each
request asked for a free pizza and the outcome
whether its author received a pizza or not was pro-
vided in the dataset. Via Semi-Att, we were able
to obtain the persuasive strategy used in each sen-
tence of each request. Similarly, we built a logistic
regression model to predict whether a request will
receive the pizza or not, controlling for the com-
munity age of the requester, the number of subred-
dits the requester participated in, his/her number
of posts as well as the votes (upvotes - downvotes)
this requester had received.

As shown in the column of RAOP in Table 3,
concreteness is significantly correlated with suc-
cess on both datasets. This demonstrated that pro-
viding more evidence might help readers know
the situation better, consistent with the effect of
Evidentiality in Althoff et al., (2014). Similarly,
making the request full of emotions (β=0.030,
Odds ratio (OR) =1.145), mentioning the simi-
larity between potential readers and the requester
(β=0.087, OR=1.104), and talking about the po-

tential impact and value for others (β=0.024,
OR=1.084) are all significantly associated with
increases in the persuasiveness of these requests
across two contexts. In contrast, highlighting the
urgency of the requests and emphasizing existing
contribution to loans (β=-0.015) negatively cor-
relate with request success (β=-0.076) on Kiva,
confirming prior work (Yang and Kraut, 2017).
This communicates to us that some of those loans
might have expired before others read the request
and took action given the limited time available,
or it could be that members thought their actions
might not help if the remaining money needed is
high and the time left is low, different from the
“limited-time offer” tactics widely used in com-
mercial advertising. To sum up, the two analy-
ses demonstrated that certain persuasive strategies
such as Identity and Impact are consistently ef-
fective across two datasets, whereas Scarcity and
Commitment contribute differently and need to be
used with caution for different contexts.

8 Conclusion

In this work, we operationalized a set of persua-
sive strategies widely used in micro-lending plat-
forms based on theories of persuasion, and devel-
oped an annotated corpus for identifying persua-
sion strategies. We designed a semi-supervised hi-
erarchical neural network to identify the persua-
sive strategies contained in loan requests. Results
show that our model improves accuracy consid-
erably. We also showed how different persuasive
strategies contribute to request success. In the fu-
ture, we plan to build a richer taxonomy of per-
suasion strategies and incorporate additional neu-
ral architectures such as variational autoencoders
to better represent sentences in each message to
further assist the modeling of persuasiveness. Be-
yond the text, images and even audios may provide
additional insights on the successes of persuasive
requests. Our model also has important applica-
tions to other domains, such as in computational
advertisements, micro-funding platforms and po-
litical campaigns.

Acknowledgement

The authors would like to thank Jason Eisner for
his help at the brainstorm stage and insightful fol-
lowup suggestions, and the anonymous reviewers
for their helpful comments. Diyi Yang was sup-
ported by Facebook Fellowship.



3629

References
Tim Althoff, Cristian Danescu-Niculescu-Mizil, and

Dan Jurafsky. 2014. How to ask for a favor: A case
study on the success of altruistic requests. In Pro-
ceedings of ICWSM.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Winston Carlile, Nishant Gurrapadi, Zixuan Ke, and
Vincent Ng. 2018. Give me more feedback: Anno-
tating argument persuasiveness and related attributes
in student essays. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
621–631.

Shelly Chaiken. 1980. Heuristic versus systematic in-
formation processing and the use of source versus
message cues in persuasion. Journal of personality
and social psychology, 39(5):752.

Moitreya Chatterjee, Sunghyun Park, Han Suk Shim,
Kenji Sagae, and Louis-Philippe Morency. 2014.
Verbal behaviors and persuasiveness in online multi-
media content. In Proceedings of the Second Work-
shop on Natural Language Processing for Social
Media (SocialNLP), pages 50–58.

Robert B Cialdini and Nathalie Garde. 1987. Influence,
volume 3. A. Michel.

Domenic V Cicchetti. 1994. Guidelines, criteria, and
rules of thumb for evaluating normed and standard-
ized assessment instruments in psychology. Psycho-
logical assessment, 6(4):284.

Sanorita Dey, Brittany Duff, Karrie Karahalios, and
Wai-Tat Fu. 2017. The art and science of persua-
sion: Not all crowdfunding campaign videos are the
same. In Proceedings of the 2017 ACM Conference
on Computer Supported Cooperative Work and So-
cial Computing, pages 755–769. ACM.

Vincent Etter, Matthias Grossglauser, and Patrick Thi-
ran. 2013. Launch hard or go home!: Predicting the
success of kickstarter campaigns. In Proceedings
of the First ACM Conference on Online Social Net-
works, COSN ’13, pages 177–182, New York, NY,
USA. ACM.

Noura Farra, Swapna Somasundaran, and Jill Burstein.
2015. Scoring persuasive essays using opinions and
their targets. In Proceedings of the Tenth Workshop
on Innovative Use of NLP for Building Educational
Applications, pages 64–74.

Debanjan Ghosh, Aquila Khanam, Yubo Han, and
Smaranda Muresan. 2016. Coarse-grained argu-
mentation features for scoring persuasive essays. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, pages 549–
554.

Debanjan Ghosh, Smaranda Muresan, Nina Wacholder,
Mark Aakhus, and Matthew Mitsui. 2014. Analyz-
ing argumentative discourse units in online interac-
tions. In Proceedings of the First Workshop on Ar-
gumentation Mining, pages 39–48.

Michael D Greenberg, Bryan Pardo, Karthic Hariharan,
and Elizabeth Gerber. 2013. Crowdfunding support
tools: predicting success & failure. In CHI’13 Ex-
tended Abstracts on Human Factors in Computing
Systems, pages 1815–1820. ACM.

Ivan Habernal and Iryna Gurevych. 2016. Which ar-
gument is more convincing? analyzing and predict-
ing convincingness of web arguments using bidirec-
tional lstm. In ACL, pages 1589–1599, Berlin, Ger-
many. Association for Computational Linguistics.

Ivan Habernal and Iryna Gurevych. 2017. Argumenta-
tion mining in user-generated web discourse. Com-
putational Linguistics, 43(1):125–179.

Christopher Hidey, Elena Musi, Alyssa Hwang,
Smaranda Muresan, and Kathy McKeown. 2017.
Analyzing the semantic types of claims and
premises in an online persuasive forum. In Proceed-
ings of the 4th Workshop on Argument Mining, pages
11–21.

Carl I Hovland, Irving L Janis, and Harold H Kelley.
1953. Communication and persuasion; psychologi-
cal studies of opinion change.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master’s thesis, Massachusetts Insti-
tute of Technology.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Tanushree Mitra and Eric Gilbert. 2014. The language
that gets people to give: Phrases that predict success
on kickstarter. In Proceedings of the 17th ACM con-
ference on Computer supported cooperative work &
social computing, pages 49–61. ACM.

Elena Musi, Debanjan Ghosh, and Smaranda Mure-
san. 2018. Changemyview through concessions: Do
concessions increase persuasion? arXiv preprint
arXiv:1806.03223.

Maxime Oquab, Léon Bottou, Ivan Laptev, and Josef
Sivic. 2015. Is object localization for free?-weakly-
supervised learning with convolutional neural net-
works. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
685–694.



3630

Andreas Peldszus and Manfred Stede. 2013. From ar-
gument diagrams to argumentation mining in texts:
A survey. International Journal of Cognitive Infor-
matics and Natural Intelligence (IJCINI), 7(1):1–31.

Isaac Persing and Vincent Ng. 2017. Why can’t you
convince me? modeling weaknesses in unpersua-
sive arguments. In Proceedings of the 26th Inter-
national Joint Conference on Artificial Intelligence,
pages 4082–4088. AAAI Press.

Richard E Petty, John T Cacioppo, and David Schu-
mann. 1983. Central and peripheral routes to adver-
tising effectiveness: The moderating role of involve-
ment. Journal of consumer research, 10(2):135–
146.

Pedro O Pinheiro and Ronan Collobert. 2015. From
image-level to pixel-level labeling with convolu-
tional networks. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition,
pages 1713–1721.

Reid Pryzant, Young-Joo Chung, and Dan Jurafsky.
2017. Predicting sales from the language of prod-
uct descriptions. In Proceedings of the SIGIR 2017
Workshop on eCommerce (ECOM 17).

Reid Pryzant, Kelly Shen, Dan Jurafsky, and Stefan
Wagner. 2018a. Deconfounded lexicon induction
for interpretable social science. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers), volume 1, pages 1615–1625.

Reid Pryzant, Kazoo Sone, and Sugato Basu. 2018b.
Interpretable neural architectures for attributing an
ad’s performance to its writing style. In EMNLP
Workshop on BlackboxNLP.

Sara Rosenthal and Kathleen Mckeown. 2017. De-
tecting influencers in multiple online genres. ACM
Trans. Internet Technol., 17(2):12:1–12:22.

Christos Sardianos, Ioannis Manousos Katakis, Geor-
gios Petasis, and Vangelis Karkaletsis. 2015. Argu-
ment extraction from news. In Proceedings of the
2nd Workshop on Argumentation Mining, pages 56–
66.

LJ Shrum, Min Liu, Mark Nespoli, and Tina M Lowrey.
2012. Persuasion in the marketplace. The SAGE
Handbook of Persuasion, page 314.

Christian Stab and Iryna Gurevych. 2014. Identify-
ing argumentative discourse structures in persuasive
essays. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 46–56, Doha, Qatar. Association
for Computational Linguistics.

Christian Stab and Iryna Gurevych. 2017. Parsing ar-
gumentation structures in persuasive essays. Com-
putational Linguistics, 43(3):619–659.

Christian Stab, Tristan Miller, and Iryna Gurevych.
2018. Cross-topic argument mining from hetero-
geneous sources using attention-based neural net-
works. arXiv preprint arXiv:1802.05758.

Chenhao Tan, Vlad Niculae, Cristian Danescu-
Niculescu-Mizil, and Lillian Lee. 2016. Winning
arguments: Interaction dynamics and persuasion
strategies in good-faith online discussions. In Pro-
ceedings of the 25th International Conference on
World Wide Web, WWW ’16, pages 613–624, Re-
public and Canton of Geneva, Switzerland. Interna-
tional World Wide Web Conferences Steering Com-
mittee.

Zhongyu Wei, Yang Liu, and Yi Li. 2016. Is this post
persuasive? ranking argumentative comments in on-
line forum. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 195–200, Berlin,
Germany. Association for Computational Linguis-
tics.

Anbang Xu, Xiao Yang, Huaming Rao, Wai-Tat Fu,
Shih-Wen Huang, and Brian P. Bailey. 2014. Show
me the money!: An analysis of project updates dur-
ing crowdfunding campaigns. In Proceedings of the
SIGCHI Conference on Human Factors in Comput-
ing Systems, CHI ’14, pages 591–600, New York,
NY, USA. ACM.

Diyi Yang and Robert E Kraut. 2017. Persuading
teammates to give: Systematic versus heuristic cues
for soliciting loans. Proceedings of the ACM on
Human-Computer Interaction, 1:114.

Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and
Taylor Berg-Kirkpatrick. 2017. Improved varia-
tional autoencoders for text modeling using dilated
convolutions. arXiv preprint arXiv:1702.08139.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489, San Diego, California. Associa-
tion for Computational Linguistics.

Fan Zhang and Diane Litman. 2015. Annotation and
classification of argumentative writing revisions. In
Proceedings of the Tenth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 133–143.

Fan Zhang, Diane Litman, and Katherine Forbes-Riley.
2016. Inferring discourse relations from pdtb-style
discourse labels for argumentative revision classi-
fication. In Proceedings of COLING 2016, the
26th International Conference on Computational
Linguistics: Technical Papers, pages 2615–2624.


