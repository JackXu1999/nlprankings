



















































Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer Learning


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2195–2204
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2195

Neural Relation Extraction via Inner-Sentence Noise Reduction and
Transfer Learning

Tianyi Liu1, Xinsong Zhang1, Wanhao Zhou1 and Weijia Jia2,1
1Department of Computer Science and Engineering, Shanghai Jiao Tong University

2Department of Computer and Information Science, University of Macau
{liutianyi, xszhang0320, whzhou, jiawj}@sjtu.edu.cn

Abstract
Extracting relations is critical for knowledge
base completion and construction in which
distant supervised methods are widely used
to extract relational facts automatically with
the existing knowledge bases. However, the
automatically constructed datasets comprise
amounts of low-quality sentences containing
noisy words, which is neglected by current
distant supervised methods resulting in unac-
ceptable precisions. To mitigate this problem,
we propose a novel word-level distant super-
vised approach for relation extraction. We first
build Sub-Tree Parse (STP) to remove noisy
words that are irrelevant to relations. Then we
construct a neural network inputting the sub-
tree while applying the entity-wise attention to
identify the important semantic features of re-
lational words in each instance. To make our
model more robust against noisy words, we
initialize our network with a priori knowledge
learned from the relevant task of entity classi-
fication by transfer learning. We conduct ex-
tensive experiments using the corpora of New
York Times (NYT) and Freebase. Experiments
show that our approach is effective and im-
proves the area of Precision/Recall (PR) from
0.35 to 0.39 over the state-of-the-art work.

1 Introduction

Relation extraction aims to extract relations be-
tween pairs of marked entities in raw texts. Tradi-
tional supervised methods are time-consuming for
the requirement of large-scale manually labeled
data. Thus, Mintz et al. (2009) propose the distant
supervised relation extraction, in which amounts
of sentences are crawled from web pages of New
York Times (NYT) and labeled with a known
knowledge base automatically. The method as-
sumes that if two entities have a relation in a
known knowledge base, all instances that mention
these two entities will express the same relation.
Obviously, this assumption is too strong, since
a sentence that mentions the two entities does

not necessarily express the relation contained in
a known knowledge base. As described in Riedel
et al. (2010), the assumption leads to the wrong
labeling problem. In order to tackle the wrong
labeling problem, various multi-instance learning
methods are adopted by mitigating noise between
sentences (Hoffmann et al., 2011; Surdeanu et al.,
2012; Zeng et al., 2015; Lin et al., 2016). De-
spite the wrong labeling problem, distant super-
vised methods may suffer from the low quality of
sentences which derive from the large-scale au-
tomatically constructed dataset by crawling web
pages (Yang et al., 2017). To handle the problem
of low-quality sentences, we have to face two ma-
jor challenges: (1) Reduce word-level noise within
sentences; (2) Improve the robustness of relation
extraction against noise.

To explain the influence of word-level noise
within sentences, we consider the following sen-
tence as an example: [It is no accident that
the main event will feature the junior welter-
weight champion miguel cotto, a puerto rican,
against Paul Malignaggi, an Italian American
from Brooklyn.], where Paul Malignaggi and
Brooklyn are two corresponding entities. The sub-
sentence [Paul Malignaggi, an Italian American
from Brooklyn.] keeps enough words to express
the relation /people/person/place of birth, and the
other words could be regarded as noise that may
hamper the extractor’s performance. Meanwhile,
as shown in Figure 1, half of the original sentences
are longer than 40 words, which means that there
are many irrelevant words inside sentences. To be
more detail, there are about 12 noisy words in each
sentence on average, and 99.4% of sentences in the
NYT-10 dataset have noise. Although the Short-
est Dependency Path (SDP) proposed by Xu et al.
(2015) tries to get rid of irrelevant words for rela-
tion extraction, it is not suitable to handle such in-
formal sentences. Moreover, word-level attention
has been leveraged to alleviate the impact of noisy



2196

words (Zhou et al., 2016), but it weakens the im-
portance of entity features for relation extraction.

0 20 40 60 80 100 120 140
Sentence Length

0

5000

10000

15000

20000

25000

Nu
m

be
r o

f s
en

te
nc

es

Original Data
Parsed Data

Figure 1: Comparison of sentence length distribution
between original data and parsed data.

As for the second challenge, a robust model
could extract precise relation features even from
low-quality sentences containing noisy words.
However, previous neural methods are always
lacking in robustness because parameters are ini-
tialized randomly and hard to tune with noisy
training data, resulting in the poor performance
of extractors. Inspired by Kumagai (2016), ini-
tializing neural networks with a priori knowledge
learned from relevant tasks by transfer learning
could improve the robustness of the target task.
For the relation extraction, entity type classifica-
tion can be used as the relevant task since en-
tity types provide abundant background knowl-
edge. For instance, the sentence [Alfead Kahn,
the Cornell-University economist who led the fight
to deregulate airplanes.] has a relation busi-
ness/person/company, which is hard to decide
without the information that Alfead Kahn is a per-
son and Cornell-University is a company. There-
fore, type features learned from entity type classi-
fication are proper a priori knowledge to initialize
the relation extractor.

In this paper, we propose a novel word-level ap-
proach for distant supervised relation extraction
by reducing inner-sentence noise and improving
robustness against noisy words. To reduce inner-
sentence noise, we utilize a novel Sub-Tree Parse
(STP) method to remove irrelevant words by in-
tercepting a subtree under the parent of entities’
lowest common ancestor. As shown in Figure 1,
the average length of the parsed sentences is much
shorter. Furthermore, the entity-wise attention is

adopted to alleviate the influence of noisy words
in the subtree and emphasize the task-relevant fea-
tures. To tackle the second challenge, we initial-
ize our model parameters with a priori knowledge
learned from the entity type classification task by
transfer learning. The experimental results show
that our model can achieve satisfactory perfor-
mance among the state-of-the-art works. Our con-
tributions are summarized as follows:

• To handle the problem of low-quality sen-
tences, we propose the STP to remove noisy
words of sentences and the entity-wise atten-
tion mechanism to enhance semantic features
of relational words.

• We first propose to initialize the neural
relation extractor with a priori knowledge
learned from entity type classification, which
strengthens its robustness against low-quality
corpus.

• Our model achieves significant results for
distant supervised relation extraction, which
improves the Precision/Recall (PR) curve
area from 0.35 to 0.39 and increases top 100
predictions by 6.3% over the state-of-the-art
work.

2 Related Work

The distant supervised method plays an increas-
ingly essential role in relation extraction due to
its less requirement of human labor (Mintz et al.,
2009). However, an evident drawback of the
method is the wrong labeling problem. Thus,
multi-instance and multi-label learning methods
are proposed to address this issue (Riedel et al.,
2010; Hoffmann et al., 2011; Surdeanu et al.,
2012). Meanwhile, other researches (Angeli et al.,
2014; Han and Sun, 2016) incorporate human-
designed features and leverage Natural Language
Processing (NLP) tools.

As neural networks have been widely used, an
increasing number of researches have been pro-
posed. Zeng et al. (2015) use a piecewise convo-
lutional neural network with multi-instance learn-
ing. Furthermore, selective attention over in-
stances with the neural network is proposed (Lin
et al., 2016). Making use of entity description, Ji
et al. (2017) assign more precise attention weights.
Focused on the imbalance of datasets, a soft label
method has been proposed by Liu et al. (2017).



2197

Recently, reinforcement learning and adversarial
learning are widely used to select the valid in-
stances for relation extraction (Feng et al., 2018;
Qin et al., 2018b,a).

However, above methods ignore inner-sentence
noise. To better remove irrelevant words, the
SDP between entities is proved to be effective
(De Marneffe and Manning, 2008; Chen and Man-
ning, 2014; Xu et al., 2015; Miwa and Bansal,
2016). Nevertheless, in our observation, the SDP
deals with informal texts difficultly (See Section
3.1 for details). Furthermore, word-level attention
is adopted to focus on relational words for relation
extraction (Zhou et al., 2016), but it hinders the
effect of entity words.

Transfer learning proposed by Pratt (1993) pro-
vides a new approach to leverage knoweldge ex-
tracted by related tasks to enhance the perfor-
mance of the target task. Furthermore, parameter
transfer learning is proved to be effective to im-
prove the stability of models by initializing model
parameters reasonably (Pan and Yang, 2010; Ku-
magai, 2016).

3 Methodology

In this section, we present our methodology for
distant supervised relation extraction. Figure 2
shows the overall architecture of our model. Our
model is divided into three parts:

Sub-Tree Parser. Input instances are parsed
to dependency parse trees by the Stanford parser1

(Chen and Manning, 2014) at first. Then words in
the STP and relative positions are transformed to
distributed representations.

Entity-Wise Neural Extractor. Given the rep-
resentation of each subtree, Bidirectional Gated
Recurrent Unit (BGRU) extracts specific features.
Then, entity-wise attention combined with word-
level attention is applied to reducing irrelevant fea-
tures for relation extraction. Finally, the sentence-
level attention is used to alleviate the influence of
wrong labeling sentences.

Parameter-Transfer Initializer. The transfer
learning method pre-trains our model parameters
from the task of entity type classification aiming
at boosting the performance of relation extraction.

3.1 Sub-Tree Parser

Each instance is put into the dependency parse
module to build the dependency parse tree in the

1https://nlp.stanford.edu/software/lex-parser.shtml

first place. Then we can tailor the sentences based
on the STP method. Finally, we transform word
tokens and position tokens of each instance to dis-
tributed representations by embedding matrixes.

Sub-Tree Parse
In order to reduce inner-sentence noise and ex-
tract relational words, we propose the STP method
which intercepts the subtree of each instance un-
der the parent of entities’ lowest common ancestor.
For instance, in Figure 2(b), China and Shanghai
are entities connected directly with the appositive
relation. The instance [In 1990, he lives in Shang-
hai, China.] will be transformed to [in Shang-
hai, China.] on the basis of the STP, where in is
the parent of Shanghai and China lowest common
ancestor and kept as important information for
expressing the relation location/location/contain.
Words connected by the imaginary line indicat-
ing the extracted subtree are reorganized into their
original sequence order to form network inputs.

Among the parse tree, the SDP has been widely
used by Chen and Manning (2014) and Xu et al.
(2015) to help models focus on relational words.
However, in our observation, the SDP is not appro-
priate in the condition that key relation words are
not in the SDP. Although additional information
(dependency relations between words) is adopted
to enhance the performance of SDP, we found
they have the minor effect through our experiment.
Thus, we do not make use of other types of linguis-
tic information. As Figure 2(b) shows, in the SDP
method, the original sentence will be transformed
to [Shanghai China] because Shanghai and China
are connected with each other directly in the de-
pendency parse tree, which results in deleting the
keyword in and may confuse the model when ex-
tracting relations. Compared with SDP, the STP
method is more appropriate to extract useful in-
formation in informal sentences where relational
words are always not in the SDP.

Word and Position Embeddings
The inputs of the network are word and position
tokens, which are transformed to the distributed
representations before they are fed into the neu-
ral model. We map jth word in the ith instance
to a vector of k dimensions denoted as xwij ∈ Rk
through the Skip-Gram model (Mikolov et al.,
2013). Like Zeng et al. (2014), we leverage Pos1
and Pos2 to specify entity pairs, which are defined
as the relative distances of current word from head



2198

Parameter
-Transfer
Initializer

Sub-Tree
Parser

Entity-Wise
Neural Extractor

lived

In

1990

,

he

in

[Shanghai]
,

[China] .

Sub-Tree Parse

Embedding Layer

Bidirectional GRU

Source
task Y_head Y_tail

Target
task Y_relation

Word

Pos1

Pos2

Attention

𝑯"𝒉𝒆𝒂𝒅;	𝑯"𝒕𝒂𝒊𝒍; 𝑯"𝒂𝒗𝒆	𝑯"𝒉𝒆𝒂𝒅;	𝑯"𝒕𝒂𝒊𝒍	

in , [Shanghai] , [China]

GRU GRU GRU GRU GRU

Hidden Layer

H1 H2 H3 H4 H5
(a)

(b)

In 1990 , he lived [Shanghai] [China] .,

-8 -7 -6

-3

-5

-2

-4

0

-2

1

-1

2

0

3

1

-6 -5 -4

in

-1

-3

Figure 2: Overall architecture of our model is used for distant supervised relation extraction, expressing the process
of handling instances. There are two modules described in detail: (a) One is the BGRU; (b) Another is the STP,
where words in the red brackets represent entities (better viewed in color).

entity and tail entity. For instance, in Figure 2 rel-
ative distances of lived from Shanghai and China
are -2 and -4 respectively. Then, the position to-
ken of each word is transformed to a vector in l
dimensions. Position embeddings are denoted as
xp1ij ∈ Rl and x

p2
ij ∈ Rl respectively. Finally,

the input representation for xij is concatenated
by word embedding xwij , position embeddings x

p1
ij

and xp2ij , which is denoted as xij = [x
w
ij ;x

p1
ij ;x

p2
ij ]

where xij ∈ Rk+2l.

3.2 Entity-Wise Neural Extractor

As shown in Figure 2, we transform the STP into
feature vectors by BGRU at first. Next, entity-wise
attention combined with the hierarchical-level at-
tention mechanism is applied to enhancing seman-
tic features of each instance.

BGRU over STP

Since the transfer learning and entity-wise atten-
tion require the specific features of entities in tree
parsed instances as their input, we adopt Gated Re-
current Unit (GRU) (Cho et al., 2014) to be our
based relation extractor, which can extract global
information of each word by pointing out its corre-
sponding position in the sequence. It can be briefly

described as below:

hit = GRU(xit) (1)

where xit is the tth word representation in the ith

parsed instance as described in the input layer, and
hit ∈ Rm is the hidden state of GRU in m dimen-
sions.

Furthermore, BGRU implementing GRU in a
different direction can access future as well as
past context. Under our circumstance, BGRU
combined with the STP can extract semantic and
syntactic features adequately. Figure 2(a) shows
the processing of BGRU over STP. The following
equation defines the operation mathematically.

hit = [
−→
hit ⊕

←−
hit] (2)

In above equation, the tth word output hit ∈ Rm
of BGRU is the element-wise addition of the tth

hidden states of forward GRU and backward one.

Entity-wise Attention
To reduce noise within sentences, we propose the
entity-wise attention mechanism to help our model
focus on relational words, especially entity words
for relation extraction. Assume that Hi is the
ith instance matrix consisting of T word vectors
[hi1, hi2, · · · , hiT ] produced by BGRU.



2199

Not all words contribute equally to the represen-
tation of the sentence. Entity words are of great
importance because they are significantly benefi-
cial to relation extraction. In our model, entity-
wise attention assigns the weight αeit to focus on
the target entity and removes noise further. It is
defined as follows:

αeit =

{
1 t = head, tail

0 others
(3)

In the above equation, αeit = 1 if t
th word belongs

to the head or tail entity.

Hierarchical-level Attention
To reduce inner-sentence noise further and de-
emphasize noisy sentences, we incorporate word-
level attention and sentence-level attention as
hierarchical-level attention which is introduced in
Yang et al. (2016).

Word-level Attention. It assigns an additional
weight αwit to relational word hit due to its rele-
vance to the relation as described by Zhou et al.
(2016). It can be described as follows:

αwit =
exp(hitA

wrw)∑T
t=1 exp(hitA

wrw)
(4)

where Aw is a weighted matrix, and vector rw can
be seen as a high level representation in a fixed
query what is the informative word over the other
words.

The ith sentence representation Si ∈ Rm is
computed as a weighted sum of hit:

Si =
T∑
t=1

(αwit + α
e
it)hit (5)

Sentence-level Attention. After we get the in-
stance representation Si, we adopt the selective at-
tention mechanism over instances to de-emphasize
the noisy sentence (Lin et al., 2016), which is de-
scribed as follows:

S =
∑
i

αsiSi (6)

αsi =
exp(SiA

srs)∑
i exp(SiA

srs)
(7)

whereAs is a weighted matrix, rs is the query vec-
tor associated with the relation, and S ∈ Rm is the
output of the sentence-level attention layer.

3.3 Parameter-Transfer Initializer
The transfer learning method pre-trains our model
parameters in the entity type classification task,
which in turn contributes to the relation extraction.

Pre-learn the Entity Type
As entity type information plays a significant role
in detecting relation types, the entity type classi-
fication task is considered to be the source task,
which is learned before the relation extraction
task. According to Eq. 6, outputs of the sentence-
level attention layer for the head entity and tail en-
tity task are Shead and Stail respectively. They are
ultimately fed into the softmax layer:

p̂i = softmax(WiSi+bi); i ∈ {head, tail} (8)

whereWi and bi are the weight and bias for the en-
tity type classification task respectively, p̂i ∈ Rzt
is the predicted probability of each class and zt is
the number of entity classes. The loss function of
the source task is the negative log-likelihood of the
true labels:

Je(θ0, θhead, θtail) = β‖θ0‖2

+
∑
t

(− 1
zt
λt

zt∑
i=1

yti log(p̂
t
i) + β‖θt‖2)

t ∈ {head, tail}

(9)

where λt is the weight of each task, θ0 is the
shared model parameters, θhead and θtail are indi-
vidual parameters for the head and tail entity clas-
sification tasks respectively, yt ∈ Rzt is the one-
hot vector representing ground truth, and β is the
hyper-parameter for L2 regularization.

Train the Relation Extractor
Based on the pre-trained model in the entity type
classification task, the relation extractor initializes
shared parameters θ0 within the best state of the
pre-trained model and independent parameters θr
randomly. Same as the entity type classification
task, the output Sr of the attention layer for the re-
lation extraction task is finally fed into the softmax
layer and the loss is calculated by cross entropy,
which is defined as follows:

p̂ = softmax(WrSr + br) (10)

Jr(θ0, θr) = −
1

zr

zr∑
i=1

yilog(p̂i)

+β(‖θ0‖2 + ‖θr‖2)
(11)



2200

where Wr, br, y ∈ Rzr , p̂ ∈ Rzr , θr and β are
defined similarly in the entity type classification
task.

As shown in Figure 2, two tasks share all layers
except attention and output layers. Assume that
the set of total model parameters is θ. Thus, θ, θ0,
θr, θhead and θtail have a relationship described in
the following equations:

θ = θ0 ∪ θhead ∪ θtail ∪ θr (12)
θi = {Awi , rwi , Asi , rsi ,Wi, bi}

i ∈ {head, tail, r}
(13)

whereAwi , r
w
i , A

s
i , r

s
i , Wi and bi are parameters in

attention and output layers.

Optimize the Objective Function
At first, we minimize J to obtain θ0 at the best
model state θ̂0 for entity type classification. Then
we minimize Jr for the best performance of rela-
tion extraction under the initialization of θ0 to be
θ̂0. Above process can be summarized as the fol-
lowing equation:

min J(θ) =λJe(θ0, θhead, θtail)+

(1− λ)Jr(θ0, θr)
(14)

where λ ∈ (0, 1) is the hyperparameter to deter-
mine the importance of each task at different train-
ing steps. We use Adam (Kingma and Ba, 2014)
optimizer to minimize the objective J(θ).

4 Experiments

Our experiments are designed to demonstrate that
our model alleviates the influence of word-level
noise arising from low-quality sentences. In this
section, we first introduce the dataset and evalua-
tion metrics. Next, we describe parameter settings.
Then we evaluate effects of the STP, entity-wise
attention and the parameter-transfer initializer. Fi-
nally, we compare our model with the state-of-the-
art works by several evaluation metrics.

4.1 Dataset and Evaluation Metrics
To evaluate the performance of our model, we
adopt a widely used dataset NYT-10 developed by
Riedel et al. (2010). NYT-10 dataset is constructed
by aligning relational facts in Freebase (Bollacker
et al., 2008) with the NYT corpus, where sen-
tences from 2005-2006 are used as training set,
and sentences from 2007 are used for testing. For
training data, there are 522,611 sentences, 281,270

entity pairs, and 18,252 relational facts; for testing
data, there are 172,448 sentences, 96,678 entity
pairs and 1,950 relational facts. There are 53 rela-
tions including a special relation NA, which means
that there is no relation between the entity pair in
the instance. Meanwhile, all relations in Freebase
are defined on head types and tail types. There-
fore, we can construct datasets for type prediction
tasks with the same dataset. The dataset has 29
head types and 26 tail types.

Like previous works, we evaluate our model
with the held-out metrics, which compare rela-
tions found by models with those in Freebase. The
held-out evaluation provides a convenient way to
assess models. We report both the PR curve and
Precision at top N predictions (P@N) at various
numbers of instances under each entity pair:

One: For each entity pair, we randomly select
one instance to represent the relation.

Two: For each entity pair, we randomly select
two instances to represent the relation.

All: For each entity pair, we select all instances
to represent the relation.

4.2 Experimental Settings

In the experiment, we utilize word2vec2 to train
word embeddings on NYT corpus. We use
the cross-validation to tune our model and grid
search to determine model parameters. The
grid search approach is used to select opti-
mal learning rate lr for Adam optimizer among
{0.1, 0.001, 0.0005, 0.0001}, GRU size m ∈
{100, 160, 230, 400}, position embedding size l ∈
{5, 10, 15, 20}. Table 1 shows all parameters for
our task. We follow experienced settings for other
parameters because they make little influence to
our model performance.

GRU size m 230
Word embedding dimension k 50
POS embedding dimension l 5
Batch size n 50
Entity-Task weights(λhead, λtail) 0.5,0.5
Entity-Relation Task weight λ 0.3
Learning rate lr 0.001
Dropout probability p 0.5
l2 penalty β 0.0001

Table 1: Parameter Settings

2https://code.google.com/p/word2vec/



2201

4.3 Effect of Various Model Parts

In this section, we utilize the PR curve to evalu-
ate the effects of three main parts in our model:
the STP, entity-wise attention and the parameter-
transfer initializer.

Effect of the STP
To demonstrate the effect of the STP, we adopt
BGRU with Word-Level Attention (WLA) pro-
posed by Zhou et al. (2016) as our base model. We
compare the performance of BGRU, BGRU+STP,
and BGRU+SDP.

0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
Recall

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Pr
ec

isi
on

BGRU+STP
BGRU+SDP
BGRU

Figure 3: PR curves for BGRU, BGRU+SDP and
BGRU+STP.

From Figure 3, we can observe that the model
with the STP performs best, and the SDP model
obtains an even worse result than the pure one.
The PR curve areas of BGRU+SDP and BGRU
are about 0.332 and 0.337 respectively, while
BGRU+STP increases it to 0.366. The result indi-
cates: (1) Our STP can get rid of irrelevant words
in each instance and obtain more precise sentence
representation for relation extraction. It proves
that our STP module is effective. (2) The SDP
method is not appropriate to handle low-quality
sentences where key relation words are not in the
SDP.

Effect of Entity-wise Attention

Test Settings PR Curve Area
Dataset Original Data STP Data
BGRU 0.337 0.366

-WLA+EWA 0.365 0.375
+EWA 0.372 0.383

Table 2: PR curve areas for BGRU, BGRU-
WLA+EWA and BGRU+EWA on various datasets.

To evaluate the effect of entity-wise attention
combined with word-level attention, we utilize
BGRU in three settings on our tree parsed data
and original data. One setting is to use WLA
mechanism only (BGRU). The second one is to re-
place WLA with the Entity-Wise Attention (EWA)
mechanism (BGRU-WLA+EWA). The third one
is to incorporate two mechanisms (BGRU+EWA).

0.00 0.05 0.10 0.15 0.20 0.25 0.30
Recall

0.5

0.6

0.7

0.8

0.9

1.0

Pr
ec

isi
on

BGRU+STP+EWA
BGRU-WLA+STP+EWA
BGRU+STP
BGRU+EWA
BGRU-WLA+EWA
BGRU

Figure 4: PR curves for BGRU, BGRU-WLA+EWA
and BGRU+EWA on various datasets.

From Table 2 and Figure 4, we can obtain: (1)
Regardless of the dataset that we employ, BGRU-
WLA(+STP)+EWA outperforms BGRU(+STP).
To be more specific, the PR curve area has a rel-
ative improvement of over 2.3%, which demon-
strates that entity-wise hidden states in the BGRU
present more precise relational features than other
word states. (2) BGRU(+STP)+EWA achieves
further improvements and outperforms the base-
line by over 4.6%, because it considers more infor-
mation than entity or relational words alone. Thus,
it indicates that entity words are essential for rela-
tion extraction, but they can not represent features
of the whole sentence without other words.

Effect of Parameter-Transfer Initializer
To evaluate the effect of the parameter-transfer ini-
tializer in our model, we leverage BGRU under
four circumstances. The first one is to directly ap-
ply it on the original dataset. The second one tests
BGRU combined with Transfer Learning (TL) on
the original dataset. The third one uses BGRU
on our STP dataset. The fourth one examines
BGRU+TL on our STP dataset.

From Figure 5, we can conclude: (1) Regard-
less of the dataset that we use, models with TL
achieve better performance, which improve the PR
curve area by over 4.7%. It demonstrates that



2202

Test Settings One Two All
P@N 100 200 300 Mean 100 200 300 Mean 100 200 300 Mean
Mintz 35.0 37.5 37.3 36.6 51.0 42.0 43.3 45.4 54.0 50.5 45.3 49.9

MultiR 64.0 61.5 53.7 59.7 62.0 61.5 58.7 61.1 75.0 65.0 62.0 67.3
MIML 62.0 59.0 54.7 58.6 69.0 59.5 59.0 62.5 70.0 64.5 60.3 64.9
PCNN 73.3 64.8 56.8 65.0 70.3 67.2 63.1 66.9 72.3 69.7 64.1 68.7

PCNN+ATT 78.0 68.0 60.7 68.9 75.0 74.0 66.3 71.8 82.0 74.0 69.0 75.0
BGRU 72.0 62.5 59.0 64.5 70.0 64.0 64.7 66.2 74.0 68.0 65.0 69.0
+STP 73.0 63.0 60.7 65.6 83.0 72.5 68.0 74.5 86.0 76.0 70.3 77.4
+EWA 82.0 71.5 66.3 73.3 84.0 79.5 70.3 77.9 86.0 81.5 75.3 80.9
+TL 83.0 75.5 67.0 75.2 85.0 81.0 72.3 79.4 87.0 83.0 78.0 82.7

Table 3: P@N for relation extraction in the entity pairs with different number of sentences

0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
Recall

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Pr
ec

isi
on

BGRU+STP+TL
BGRU+STP
BGRU+TL
BGRU

Figure 5: PR curves for BGRU, BGRU+TL,
BGRU+STP and BGRU+STP+TL

transfer learning helps our model become more ro-
bust against noise. (2) BGRU+STP+TL achieves
the best performance and increases the area to
0.383, while areas of BGRU, BGRU+STP and
BGRU+TL are 0.337, 0.366 and 0.372 respec-
tively. It means that the TL method works well
with the STP and can resist noisy words further.

4.4 Comparison with Baselines

To evaluate our approach, we select the following
six methods as our baseline:

Mintz (Mintz et al., 2009) proposes the human-
designed feature model.

MultiR (Hoffmann et al., 2011) puts forward a
graphical model.

MIML (Surdeanu et al., 2012) proposes a
multi-instance multi-label model.

PCNN (Zeng et al., 2015) puts forward a piece-
wise CNN for relation extraction.

PCNN+ATT (Lin et al., 2016) proposes the se-
lective attention mechanism with PCNN.

BGRU (Zhou et al., 2016) proposes a BGRU
with the word-level attention mechanism.

0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
Recall

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0
Pr

ec
isi

on
BGRU+STP+EWA+TL
BGRU+STP+EWA
MINTZ
MULTIR
MIML
PCNN
PCNN+ATT
BGRU

Figure 6: Performance comparison of the proposed
method with baselines.

As Figure 6 shows, we can observe: (1)
BGRU+STP+EWA achieves the best PR curve
over baselines, which improves the area to 0.38
over 0.33 of PCNN, 0.34 of BGRU and 0.35
of PCNN+ATT. At the recall rate of 0.25, our
model can still achieve a precision rate above
0.6. It demonstrates that BGRU+STP+EWA is
effective because the STP and entity-wise at-
tention combined with word-level attention can
reduce inner-sentence noise at a fine-grained
level. (2) Integrated with transfer learning,
BGRU+STP+EWA+TL performs much better and
increases the PR curve area to 0.392. It means that
the model is pre-trained for better parameter ini-
tialization so the TL model becomes more robust
against noisy words. Parameter transfer learning
can be applied in better feature extractors for fur-
ther improvement.

Following previous works, we adopt P@N as



2203

a quantitative indicator to compare our model
with baselines based on various instances un-
der each relational tuple. In Table 3, we re-
port P@100, P@200, P@300 and the mean of
them for each model in the held-out evalua-
tion. We can find: (1) Compared with baselines,
BGRU+STP+EWA+TL achieves the best perfor-
mance in all test settings, which increases the
performance of PCNN+ATT in three settings by
6.3%, 7.6%, and 7.7% respectively. It demon-
strates that the integrated model is the most ef-
fective; (2) Our STP and entity-wise attention
combined with word-level attention reduce inner-
sentence noise effectively, and outperform base-
lines by over 5%; (3) Our neural extractor initial-
ized with a priori knowledge learned from entity
type classification is more robust against word-
level noise where BGRU+STP+EWA+TL has an
improvement of 2% over BGRU+STP+EWA.

5 Conclusion

In this paper, we propose a novel word-level ap-
proach for distant supervised relation extraction.
It aims at tackling the low-quality corpus by re-
ducing inner-sentence noise and improving the ro-
bustness against noisy words. To alleviate the
influence of word-level noise, we propose the
STP. Meanwhile, entity-wise attention combined
with word-level attention helps the model focus
more on relational words. Furthermore, parame-
ter transfer learning makes our model more robust
against noise by reasonable initialization of pa-
rameters. The experimental results show that our
model significantly and consistently outperforms
the state-of-the-art method.

In the future, we will incorporate the SDP and
STP to obtain more precise shortened sentences.
Furthermore, we will conduct research in how to
utilize entity information to assign more appropri-
ate initial parameters of the relation extractor.

Acknowledgments

This work is supported by FDCT 0007/2018/A1,
DCT-MoST Joint-project No. (025/2015/AMJ)
of SAR Macau; University of Macau Funds
Nos: CPG2018-00032-FST & SRG2018-00111-
FST; Chinese National Research Fund (NSFC)
Key Project No. 61532013 and National China
973 Project No. 2015CB352401.

References
Gabor Angeli, Julie Tibshirani, Jean Wu, and Christo-

pher D Manning. 2014. Combining distant and par-
tial supervision for relation extraction. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1556–1567. Association for Computational Linguis-
tics.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data, pages 1247–1250. ACM.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750. Association for Compu-
tational Linguistics.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734. Association for Computational Linguistics.

Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1–8. Association for Com-
putational Linguistics.

Jun Feng, Minlie Huang, Li Zhao, Yang Yang, and Xi-
aoyan Zhu. 2018. Reinforcement learning for rela-
tion classification from noisy data. In The Thirty-
Second AAAI Conference on Artificial Intelligence
(AAAI-18), pages 5779–5786.

Xianpei Han and Le Sun. 2016. Global distant super-
vision for relation extraction. In Proceedings of the
Thirtieth AAAI Conference on Artificial Intelligence
(AAAI-16), pages 2950–2956.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
- Volume 1, pages 541–550. Association for Compu-
tational Linguistics.

Guoliang Ji, Kang Liu, Shizhu He, Jun Zhao, et al.
2017. Distant supervision for relation extraction
with sentence-level attention and entity descriptions.
In Proceedings of the Thirty-First AAAI Conference
on Artificial Intelligence (AAAI-17), pages 3060–
3066.



2204

Diederik P Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Wataru Kumagai. 2016. Learning bound for parameter
transfer learning. In Advances in Neural Informa-
tion Processing Systems, pages 2721–2729.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 2124–2133. Association for
Computational Linguistics.

Tianyu Liu, Kexiang Wang, Baobao Chang, and Zhi-
fang Sui. 2017. A soft-label method for noise-
tolerant distantly supervised relation extraction.
In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1790–1795. Association for Com-
putational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1105–1116. Association for Computa-
tional Linguistics.

Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10):1345–1359.

Lorien Y Pratt. 1993. Discriminability-based transfer
between neural networks. In Advances in Neural In-
formation Processing Systems, pages 204–211.

Pengda Qin, Weiran XU, and William Yang Wang.
2018a. Dsgan: Generative adversarial training for
distant supervision relation extraction. In Proceed-
ings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 496–505. Association for Computa-
tional Linguistics.

Pengda Qin, Weiran XU, and William Yang Wang.
2018b. Robust distant supervision relation extrac-
tion via deep reinforcement learning. In Proceed-
ings of the 56th Annual Meeting of the Association

for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2137–2147. Association for Computa-
tional Linguistics.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Joint European Conference
on Machine Learning and Knowledge Discovery in
Databases, pages 148–163. Springer.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 455–465. Association for Computa-
tional Linguistics.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 1785–1794. Associa-
tion for Computational Linguistics.

Wenmian Yang, Na Ruan, Wenyuan Gao, Kun Wang,
Wensheng Ran, and Weijia Jia. 2017. Crowdsourced
time-sync video tagging using semantic association
graph. In Multimedia and Expo (ICME), 2017 IEEE
International Conference on, pages 547–552. IEEE.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
piecewise convolutional neural networks. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 1753–1762. Association for Computational
Linguistics.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2335–2344.

Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
based bidirectional long short-term memory net-
works for relation classification. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
volume 2, pages 207–212. Association for Compu-
tational Linguistics.


