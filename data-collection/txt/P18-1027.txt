











































Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 284–294
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

284

Sharp Nearby, Fuzzy Far Away: How Neural
Language Models Use Context

Urvashi Khandelwal, He He, Peng Qi, Dan Jurafsky
Computer Science Department

Stanford University
{urvashik,hehe,pengqi,jurafsky}@stanford.edu

Abstract

We know very little about how neural lan-
guage models (LM) use prior linguistic
context. In this paper, we investigate the
role of context in an LSTM LM, through
ablation studies. Specifically, we ana-
lyze the increase in perplexity when prior
context words are shuffled, replaced, or
dropped. On two standard datasets, Penn
Treebank and WikiText-2, we find that the
model is capable of using about 200 to-
kens of context on average, but sharply
distinguishes nearby context (recent 50 to-
kens) from the distant history. The model
is highly sensitive to the order of words
within the most recent sentence, but ig-
nores word order in the long-range context
(beyond 50 tokens), suggesting the distant
past is modeled only as a rough seman-
tic field or topic. We further find that the
neural caching model (Grave et al., 2017b)
especially helps the LSTM to copy words
from within this distant context. Overall,
our analysis not only provides a better un-
derstanding of how neural LMs use their
context, but also sheds light on recent suc-
cess from cache-based models.

1 Introduction

Language models are an important component
of natural language generation tasks, such as
machine translation and summarization. They
use context (a sequence of words) to estimate
a probability distribution of the upcoming word.
For several years now, neural language models
(NLMs) (Graves, 2013; Jozefowicz et al., 2016;
Grave et al., 2017a; Dauphin et al., 2017; Melis
et al., 2018; Yang et al., 2018) have consistently
outperformed classical n-gram models, an im-

provement often attributed to their ability to model
long-range dependencies in faraway context. Yet,
how these NLMs use the context is largely unex-
plained.

Recent studies have begun to shed light on the
information encoded by Long Short-Term Mem-
ory (LSTM) networks. They can remember sen-
tence lengths, word identity, and word order (Adi
et al., 2017), can capture some syntactic structures
such as subject-verb agreement (Linzen et al.,
2016), and can model certain kinds of semantic
compositionality such as negation and intensifica-
tion (Li et al., 2016).

However, all of the previous work studies
LSTMs at the sentence level, even though they can
potentially encode longer context. Our goal is to
complement the prior work to provide a richer un-
derstanding of the role of context, in particular,
long-range context beyond a sentence. We aim
to answer the following questions: (i) How much
context is used by NLMs, in terms of the number
of tokens? (ii) Within this range, are nearby and
long-range contexts represented differently? (iii)
How do copy mechanisms help the model use dif-
ferent regions of context?

We investigate these questions via ablation stud-
ies on a standard LSTM language model (Merity
et al., 2018) on two benchmark language modeling
datasets: Penn Treebank and WikiText-2. Given a
pretrained language model, we perturb the prior
context in various ways at test time, to study how
much the perturbed information affects model per-
formance. Specifically, we alter the context length
to study how many tokens are used, permute to-
kens to see if LSTMs care about word order in
both local and global contexts, and drop and re-
place target words to test the copying abilities of
LSTMs with and without an external copy mech-
anism, such as the neural cache (Grave et al.,
2017b). The cache operates by first recording tar-



285

get words and their context representations seen
in the history, and then encouraging the model to
copy a word from the past when the current con-
text representation matches that word’s recorded
context vector.

We find that the LSTM is capable of using about
200 tokens of context on average, with no observ-
able differences from changing the hyperparame-
ter settings. Within this context range, word or-
der is only relevant within the 20 most recent to-
kens or about a sentence. In the long-range con-
text, order has almost no effect on performance,
suggesting that the model maintains a high-level,
rough semantic representation of faraway words.
Finally, we find that LSTMs can regenerate some
words seen in the nearby context, but heavily rely
on the cache to help them copy words from the
long-range context.

2 Language Modeling

Language models assign probabilities to se-
quences of words. In practice, the probability can
be factorized using the chain rule

P (w
1

, . . . , wt) =
tY

i=1

P (wi|wi�1, . . . , w1),

and language models compute the conditional
probability of a target word wt given its preced-
ing context, w

1

, . . . , wt�1.
Language models are trained to minimize the

negative log likelihood of the training corpus:

NLL = � 1
T

TX

t=1

logP (wt|wt�1, . . . , w1),

and the model’s performance is usually evaluated
by perplexity (PP) on a held-out set:

PP = exp(NLL).

When testing the effect of ablations, we focus
on comparing differences in the language model’s
losses (NLL) on the dev set, which is equivalent to
relative improvements in perplexity.

3 Approach

Our goal is to investigate the effect of contextual
features such as the length of context, word or-
der and more, on LSTM performance. Thus, we
use ablation analysis, during evaluation, to mea-
sure changes in model performance in the absence
of certain contextual information.

PTB Wiki
Dev Test Dev Test

# Tokens 73,760 82,430 217,646 245,569
Perplexity (no cache) 59.07 56.89 67.29 64.51
Avg. Sent. Len. 20.9 20.9 23.7 22.6

Table 1: Dataset statistics and performance rele-
vant to our experiments.

Typically, when testing the language model on a
held-out sequence of words, all tokens prior to the
target word are fed to the model; we call this the
infinite-context setting. In this study, we observe
the change in perplexity or NLL when the model
is fed a perturbed context �(wt�1, . . . , w1), at test
time. � refers to the perturbation function, and we
experiment with perturbations such as dropping
tokens, shuffling/reversing tokens, and replacing
tokens with other words from the vocabulary.1 It
is important to note that we do not train the model
with these perturbations. This is because the aim is
to start with an LSTM that has been trained in the
standard fashion, and discover how much context
it uses and which features in nearby vs. long-range
context are important. Hence, the mismatch in
training and test is a necessary part of experiment
design, and all measured losses are upper bounds
which would likely be lower, were the model also
trained to handle such perturbations.

We use a standard LSTM language model,
trained and finetuned using the Averaging SGD
optimizer (Merity et al., 2018).2 We also augment
the model with a cache only for Section 6.2, in
order to investigate why an external copy mech-
anism is helpful. A short description of the ar-
chitecture and a detailed list of hyperparameters is
listed in Appendix A, and we refer the reader to
the original paper for additional details.

We analyze two datasets commonly used for
language modeling, Penn Treebank (PTB) (Mar-
cus et al., 1993; Mikolov et al., 2010) and
Wikitext-2 (Wiki) (Merity et al., 2017). PTB
consists of Wall Street Journal news articles with
0.9M tokens for training and a 10K vocabulary.
Wiki is a larger and more diverse dataset, con-
taining Wikipedia articles across many topics with
2.1M tokens for training and a 33K vocabulary.
Additional dataset statistics are provided in Ta-

1Code for our experiments available at https://
github.com/urvashik/lm-context-analysis

2Public release of their code at https://github.
com/salesforce/awd-lstm-lm

https://github.com/urvashik/lm-context-analysis
https://github.com/urvashik/lm-context-analysis
https://github.com/salesforce/awd-lstm-lm
https://github.com/salesforce/awd-lstm-lm


286

ble 1.
In this paper, we present results only on the dev

sets, in order to avoid revealing details about the
test sets. However, we have confirmed that all re-
sults are consistent with those on the test sets. In
addition, for all experiments we report averaged
results from three models trained with different
random seeds. Some of the figures provided con-
tain trends from only one of the two datasets and
the corresponding figures for the other dataset are
provided in Appendix B.

4 How much context is used?
LSTMs are designed to capture long-range depen-
dencies in sequences (Hochreiter and Schmidhu-
ber, 1997). In practice, LSTM language models
are provided an infinite amount of prior context,
which is as long as the test sequence goes. How-
ever, it is unclear how much of this history has a
direct impact on model performance. In this sec-
tion, we investigate how many tokens of context
achieve a similar loss (or 1-2% difference in model
perplexity) to providing the model infinite context.
We consider this the effective context size.

LSTM language models have an effective con-
text size of about 200 tokens on average. We
determine the effective context size by varying the
number of tokens fed to the model. In particular,
at test time, we feed the model the most recent n
tokens:

�truncate(wt�1, . . . , w1) = (wt�1, . . . , wt�n), (1)

where n > 0 and all tokens farther away from
the target wt are dropped.3 We compare the dev
loss (NLL) from truncated context, to that of the
infinite-context setting where all previous words
are fed to the model. The resulting increase in loss
indicates how important the dropped tokens are for
the model.

Figure 1a shows that the difference in dev loss,
between truncated- and infinite-context variants of
the test setting, gradually diminishes as we in-
crease n from 5 tokens to 1000 tokens. In particu-
lar, we only see a 1% increase in perplexity as we
move beyond a context of 150 tokens on PTB and
250 tokens on Wiki. Hence, we provide empirical
evidence to show that LSTM language models do,
in fact, model long-range dependencies, without
help from extra context vectors or caches.

3Words at the beginning of the test sequence with fewer
than n tokens in the context are ignored for loss computation.

Changing hyperparameters does not change
the effective context size. NLM performance
has been shown to be sensitive to hyperparame-
ters such as the dropout rate and model size (Melis
et al., 2018). To investigate if these hyperpa-
rameters affect the effective context size as well,
we train separate models by varying the follow-
ing hyperparameters one at a time: (1) number
of timesteps for truncated back-propogation (2)
dropout rate, (3) model size (hidden state size,
number of layers, and word embedding size). In
Figure 1b, we show that while different hyperpa-
rameter settings result in different perplexities in
the infinite-context setting, the trend of how per-
plexity changes as we reduce the context size re-
mains the same.

4.1 Do different types of words need different
amounts of context?

The effective context size determined in the pre-
vious section is aggregated over the entire cor-
pus, which ignores the type of the upcoming word.
Boyd-Graber and Blei (2009) have previously in-
vestigated the differences in context used by dif-
ferent types of words and found that function
words rely on less context than content words.
We investigate whether the effective context size
varies across different types of words, by catego-
rizing them based on either frequency or parts-of-
speech. Specifically, we vary the number of con-
text tokens in the same way as the previous sec-
tion, and aggregate loss over words within each
class separately.

Infrequent words need more context than fre-
quent words. We categorize words that appear
at least 800 times in the training set as frequent,
and the rest as infrequent. Figure 1c shows that
the loss of frequent words is insensitive to missing
context beyond the 50 most recent tokens, which
holds across the two datasets. Infrequent words,
on the other hand, require more than 200 tokens.

Content words need more context than function
words. Given the parts-of-speech of each word,
we define content words as nouns, verbs and adjec-
tives, and function words as prepositions and de-
terminers.4 Figure 1d shows that the loss of nouns
and verbs is affected by distant context, whereas
when the target word is a determiner, the model
only relies on words within the last 10 tokens.

4We obtain part-of-speech tags using Stanford
CoreNLP (Manning et al., 2014).



287

(a) Varying context size. (b) Changing model hyperparameters.

(c) Frequent vs. infrequent words. (d) Different parts-of-speech.

Figure 1: Effects of varying the number of tokens provided in the context, as compared to the same
model provided with infinite context. Increase in loss represents an absolute increase in NLL over the
entire corpus, due to restricted context. All curves are averaged over three random seeds, and error bars
represent the standard deviation. (a) The model has an effective context size of 150 on PTB and 250 on
Wiki. (b) Changing model hyperparameters does not change the context usage trend, but does change
model performance. We report perplexities to highlight the consistent trend. (c) Infrequent words need
more context than frequent words. (d) Content words need more context than function words.

Discussion. Overall, we find that the model’s ef-
fective context size is dynamic. It depends on
the target word, which is consistent with what we
know about language, e.g., determiners require
less context than nouns (Boyd-Graber and Blei,
2009). In addition, these findings are consistent
with those previously reported for different lan-
guage models and datasets (Hill et al., 2016; Wang
and Cho, 2016).

5 Nearby vs. long-range context

An effective context size of 200 tokens allows for
representing linguistic information at many lev-
els of abstraction, such as words, sentences, top-
ics, etc. In this section, we investigate the impor-
tance of contextual information such as word order
and word identity. Unlike prior work that studies
LSTM embeddings at the sentence level, we look
at both nearby and faraway context, and analyze

how the language model treats contextual informa-
tion presented in different regions of the context.

5.1 Does word order matter?
Adi et al. (2017) have shown that LSTMs are
aware of word order within a sentence. We investi-
gate whether LSTM language models are sensitive
to word order within a larger context window. To
determine the range in which word order affects
model performance, we permute substrings in the
context to observe their effect on dev loss com-
pared to the unperturbed baseline. In particular,
we perturb the context as follows,

�permute(wt�1, . . . , wt�n) =

(wt�1, .., ⇢(wt�s1�1, .., wt�s2), .., wt�n)
(2)

where ⇢ 2 {shu✏e, reverse} and (s
1

, s
2

] denotes
the range of the substring to be permuted. We re-
fer to this substring as the permutable span. For



288

(a) Perturb order locally, within 20 tokens of each point. (b) Perturb global order, i.e. all tokens in the context before a
given point, in Wiki.

Figure 2: Effects of shuffling and reversing the order of words in 300 tokens of context, relative to an
unperturbed baseline. All curves are averages from three random seeds, where error bars represent the
standard deviation. (a) Changing the order of words within a 20-token window has negligible effect on
the loss after the first 20 tokens. (b) Changing the global order of words within the context does not
affect loss beyond 50 tokens.

the following analysis, we distinguish local word
order, within 20-token permutable spans which
are the length of an average sentence, from global
word order, which extends beyond local spans to
include all the farthest tokens in the history. We
consider selecting permutable spans within a con-
text of n = 300 tokens, which is greater than the
effective context size.

Local word order only matters for the most re-
cent 20 tokens. We can locate the region of con-
text beyond which the local word order has no rel-
evance, by permuting word order locally at various
points within the context. We accomplish this by
varying s

1

and setting s
2

= s
1

+ 20. Figure 2a
shows that local word order matters very much
within the most recent 20 tokens, and far less be-
yond that.

Global order of words only matters for the most
recent 50 tokens. Similar to the local word or-
der experiment, we locate the point beyond which
the general location of words within the context
is irrelevant, by permuting global word order. We
achieve this by varying s

1

and fixing s
2

= n. Fig-
ure 2b demonstrates that after 50 tokens, shuffling
or reversing the remaining words in the context has
no effect on the model performance.

In order to determine whether this is due to in-
sensitivity to word order or whether the language
model is simply not sensitive to any changes in

the long-range context, we further replace words
in the permutable span with a randomly sampled
sequence of the same length from the training set.
The gap between the permutation and replacement
curves in Figure 2b illustrates that the identity of
words in the far away context is still relevant, and
only the order of the words is not.

Discussion. These results suggest that word or-
der matters only within the most recent sentence,
beyond which the order of sentences matters for
2-3 sentences (determined by our experiments on
global word order). After 50 tokens, word or-
der has almost no effect, but the identity of those
words is still relevant, suggesting a high-level,
rough semantic representation for these faraway
words. In light of these observations, we define 50
tokens as the boundary between nearby and long-
range context, for the rest of this study. Next, we
investigate the importance of different word types
in the different regions of context.

5.2 Types of words and the region of context
Open-class or content words such as nouns, verbs,
adjectives and adverbs, contribute more to the
semantic context of natural language than func-
tion words such as determiners and prepositions.
Given our observation that the language model
represents long-range context as a rough seman-
tic representation, a natural question to ask is how
important are function words in the long-range



289

Figure 3: Effect of dropping content and function
words from 300 tokens of context relative to an un-
perturbed baseline, on PTB. Error bars represent
95% confidence intervals. Dropping both content
and function words 5 tokens away from the target
results in a nontrivial increase in loss, whereas be-
yond 20 tokens, only content words are relevant.

context? Below, we study the effect of these
two classes of words on the model’s performance.
Function words are defined as all words that are
not nouns, verbs, adjectives or adverbs.

Content words matter more than function
words. To study the effect of content and func-
tion words on model perplexity, we drop them
from different regions of the context and compare
the resulting change in loss. Specifically, we per-
turb the context as follows,

�drop(wt�1, . . . , wt�n) =

(wt�1, .., wt�s1 , fpos(y, (wt�s1�1, .., wt�n)))

(3)

where f
pos

(y, span) is a function that drops all
words with POS tag y in a given span. s

1

denotes
the starting offset of the perturbed subsequence.
For these experiments, we set s

1

2 {5, 20, 100}.
On average, there are slightly more content words
than function words in any given text. As shown in
Section 4, dropping more words results in higher
loss. To eliminate the effect of dropping differ-
ent fractions of words, for each experiment where
we drop a specific word type, we add a control
experiment where the same number of tokens are
sampled randomly from the context, and dropped.

Figure 3 shows that dropping content words as
close as 5 tokens from the target word increases
model perplexity by about 65%, whereas dropping

the same proportion of tokens at random, results in
a much smaller 17% increase. Dropping all func-
tion words, on the other hand, is not very differ-
ent from dropping the same proportion of words
at random, but still increases loss by about 15%.
This suggests that within the most recent sentence,
content words are extremely important but func-
tion words are also relevant since they help main-
tain grammaticality and syntactic structure. On the
other hand, beyond a sentence, only content words
have a sizeable influence on model performance.

6 To cache or not to cache?

As shown in Section 5.1, LSTM language models
use a high-level, rough semantic representation for
long-range context, suggesting that they might not
be using information from any specific words lo-
cated far away. Adi et al. (2017) have also shown
that while LSTMs are aware of which words ap-
pear in their context, this awareness degrades with
increasing length of the sequence. However, the
success of copy mechanisms such as attention and
caching (Bahdanau et al., 2015; Hill et al., 2016;
Merity et al., 2017; Grave et al., 2017a,b) suggests
that information in the distant context is very use-
ful. Given this fact, can LSTMs copy any words
from context without relying on external copy
mechanisms? Do they copy words from nearby
and long-range context equally? How does the
caching model help? In this section, we investi-
gate these questions by studying how LSTMs copy
words from different regions of context. More
specifically, we look at two regions of context,
nearby (within 50 most recent tokens) and long-
range (beyond 50 tokens), and study three cate-
gories of target words: those that can be copied
from nearby context (Cnear), those that can only be
copied from long-range context (Cfar), and those
that cannot be copied at all given a limited context
(Cnone).

6.1 Can LSTMs copy words without caches?

Even without a cache, LSTMs often regenerate
words that have already appeared in prior context.
We investigate how much the model relies on the
previous occurrences of the upcoming target word,
by analyzing the change in loss after dropping and
replacing this target word in the context.

LSTMs can regenerate words seen in nearby
context. In order to demonstrate the usefulness



290

(a) Dropping tokens (b) Perturbing occurrences of target word in context.

Figure 4: Effects of perturbing the target word in the context compared to dropping long-range context
altogether, on PTB. Error bars represent 95% confidence intervals. (a) Words that can only be copied
from long-range context are more sensitive to dropping all the distant words than to dropping the target.
For words that can be copied from nearby context, dropping only the target has a much larger effect
on loss compared to dropping the long-range context. (b) Replacing the target word with other tokens
from vocabulary hurts more than dropping it from the context, for words that can be copied from nearby
context, but has no effect on words that can only be copied from far away.

of target word occurrences in context, we experi-
ment with dropping all the distant context versus
dropping only occurrences of the target word from
the context. In particular, we compare removing
all tokens after the 50 most recent tokens, (Equa-
tion 1 with n = 50), versus removing only the
target word, in context of size n = 300:

�drop(wt�1, . . . , wt�n) =

fword(wt, (wt�1, . . . , wt�n)),
(4)

where fword(w, span) drops words equal to w in a
given span. We compare applying both perturba-
tions to a baseline model with unperturbed context
restricted to n = 300. We also include the target
words that never appear in the context (Cnone) as a
control set for this experiment.

The results show that LSTMs rely on the rough
semantic representation of the faraway context to
generate Cfar, but direclty copy Cnear from the
nearby context. In Figure 4a, the long-range con-
text bars show that for words that can only be
copied from long-range context (Cfar), removing
all distant context is far more disruptive than re-
moving only occurrences of the target word (12%
and 2% increase in perplexity, respectively). This
suggests that the model relies more on the rough
semantic representation of faraway context to pre-
dict these Cfar tokens, rather than directly copy-
ing them from the distant context. On the other
hand, for words that can be copied from nearby

context (Cnear), removing all long-range context
has a smaller effect (about 3.5% increase in per-
plexity) as seen in Figure 4a, compared to remov-
ing the target word which increases perplexity by
almost 9%. This suggests that these Cnear tokens
are more often copied from nearby context, than
inferred from information found in the rough se-
mantic representation of long-range context.

However, is it possible that dropping the tar-
get tokens altogether, hurts the model too much
by adversely affecting grammaticality of the con-
text? We test this theory by replacing target words
in the context with other words from the vocab-
ulary. This perturbation is similar to Equation 4,
except instead of dropping the token, we replace
it with a different one. In particular, we exper-
iment with replacing the target with <unk>, to
see if having the generic word is better than not
having any word. We also replace it with a word
that has the same part-of-speech tag and a simi-
lar frequency in the dataset, to observe how much
this change confuses the model. Figure 4b shows
that replacing the target with other words results
in up to a 14% increase in perplexity for Cnear,
which suggests that the replacement token seems
to confuse the model far more than when the to-
ken is simply dropped. However, the words that
rely on the long-range context, Cfar, are largely
unaffected by these changes, which confirms our
conclusion from dropping the target tokens: Cfar



291

witnesses in the morris film </s> served up as a solo however the music lacks the UNK provided by a context within another
medium </s> UNK of mr. glass may agree with the critic richard UNK 's sense that the NUM music in twelve parts is as UNK and
UNK as the UNK UNK </s> but while making the obvious point that both UNK develop variations from themes this comparison
UNK the intensely UNK nature of mr. glass

</s> snack-food UNK increased a strong NUM NUM in the third quarter while domestic profit increased in double UNK mr.
calloway said </s> excluding the british snack-food business acquired in july snack-food international UNK jumped NUM NUM
with sales strong in spain mexico and brazil </s> total snack-food profit rose NUM NUM </s> led by pizza hut and UNK bell
restaurant earnings increased about NUM NUM in the third quarter on a NUM NUM sales increase </s> UNK sales for pizza hut
rose about NUM NUM while UNK bell 's increased NUM NUM as the chain continues to benefit from its UNK strategy </s> UNK
bell has turned around declining customer counts by permanently lowering the price of its UNK </s> same UNK for kentucky fried
chicken which has struggled with increased competition in the fast-food chicken market and a lack of new products rose only NUM
NUM </s> the operation which has been slow to respond to consumers ' shifting UNK away from fried foods has been developing a
UNK product that may be introduced nationally at the end of next year </s> the new product has performed well in a market test in
las vegas nev. mr. calloway

send a delegation of congressional staffers to poland to assist its legislature the UNK in democratic procedures </s> senator pete
UNK calls this effort the first gift of democracy </s> the poles might do better to view it as a UNK horse </s> it is the vast shadow
government of NUM congressional staffers that helps create such legislative UNK as the NUM page UNK reconciliation bill that
claimed to be the budget of the united states </s> maybe after the staffers explain their work to the poles they 'd be willing to come
back and do the same for the american people </s> UNK UNK plc a financially troubled irish maker of fine crystal and UNK china
reported that its pretax loss for the first six months widened to NUM million irish punts $ NUM million from NUM million irish
punts a year earlier </s> the results for the half were worse than market expectations which suggested an interim loss of around
NUM million irish punts </s> in a sharply weaker london market yesterday UNK shares were down NUM pence at NUM pence
NUM cents </s> the company reported a loss after taxation and minority interests of NUM million irish

sim has set a fresh target of $ NUM a share by the end of </s> reaching that goal says robert t. UNK applied 's chief financial officer
will require efficient reinvestment of cash by applied and UNK of its healthy NUM NUM rate of return on operating capital </s> in
barry wright mr. sim sees a situation very similar to the one he faced when he joined applied as president and chief operating officer
in NUM </s> applied then a closely held company was UNK under the management of its controlling family </s> while profitable it
was n't growing and was n't providing a satisfactory return on invested capital he says </s> mr. sim is confident that the drive to
dominate certain niche markets will work at barry wright as it has at applied </s> he also UNK an UNK UNK to develop a corporate
culture that rewards managers who produce and where UNK is shared </s> mr. sim considers the new unit 's operations
fundamentally sound and adds that barry wright has been fairly successful in moving into markets that have n't interested larger
competitors </s> with a little patience these businesses will perform very UNK mr. sim

was openly sympathetic to swapo </s> shortly after that mr. UNK had scott stanley arrested and his UNK confiscated </s> mr.
stanley is on trial over charges that he violated a UNK issued by the south african administrator general earlier this year which made
it a crime punishable by two years in prison for any person to UNK UNK or UNK the election commission </s> the stanley affair
does n't UNK well for the future of democracy or freedom of anything in namibia when swapo starts running the government </s> to
the extent mr. stanley has done anything wrong it may be that he is out of step with the consensus of world intellectuals that the
UNK guerrillas were above all else the victims of UNK by neighboring south africa </s> swapo has enjoyed favorable western
media treatment ever since the u.n. general assembly declared it the sole UNK representative of namibia 's people in </s> last year
the u.s. UNK a peace settlement to remove cuba 's UNK UNK from UNK and hold free and fair elections that would end south africa
's control of namibia </s> the elections are set for nov. NUM </s> in july mr. stanley

july snack-food international UNK jumped NUM NUM with sales strong in spain mexico and brazil </s> total snack-food profit rose
NUM NUM </s> led by pizza hut and UNK bell restaurant earnings increased about NUM NUM in the third quarter on a NUM
NUM sales increase </s> UNK sales for pizza hut rose about NUM NUM while UNK bell 's increased NUM NUM as the chain
continues to benefit from its UNK strategy </s> UNK bell has turned around declining customer counts by permanently lowering the
price of its UNK </s> same UNK for kentucky fried chicken which has struggled with increased competition in the fast-food
chicken market and a lack of new products rose only NUM NUM </s> the operation which has been slow to respond to consumers '
shifting UNK away from fried foods has been developing a UNK product that may be introduced nationally at the end of next year
</s> the new product has performed well in a market test in las vegas nev. mr. calloway said </s> after a four-year $ NUM billion
acquisition binge that brought a major soft-drink company soda UNK a fast-food chain and an overseas snack-food giant to pepsi mr.
calloway

of london 's securities traders it was a day that started nervously in the small hours </s> by UNK the selling was at UNK fever </s>
but as the day ended in a UNK wall UNK rally the city UNK a sigh of relief </s> so it went yesterday in the trading rooms of london
's financial district </s> in the wake of wall street 's plunge last friday the london market was considered especially vulnerable </s>
and before the opening of trading here yesterday all eyes were on early trading in tokyo for a clue as to how widespread the fallout

Figure 5: Success of neural cache on PTB. Brightly shaded region shows peaky distribution.

management equity participation </s> further many institutions today holding troubled retailers ' debt securities will be UNK to
consider additional retailing investments </s> it 's called bad money driving out good money said one retailing UNK </s>
institutions that usually buy retail paper have to be more concerned </s> however the lower prices these retail chains are now
expected to bring should make it easier for managers to raise the necessary capital and pay back the resulting debt </s> in addition
the fall selling season has generally been a good one especially for those retailers dependent on apparel sales for the majority of their
revenues </s> what 's encouraging about this is that retail chains will be sold on the basis of their sales and earnings not liquidation
values said joseph e. brooks chairman and chief

offerings outside the u.s. </s> goldman sachs & co. will manage the offering </s> macmillan said berlitz intends to pay quarterly
dividends on the stock </s> the company said it expects to pay the first dividend of NUM cents a share in the NUM first quarter </s>
berlitz will borrow an amount equal to its expected net proceeds from the offerings plus $ NUM million in connection with a credit
agreement with lenders </s> the total borrowing will be about $ NUM million the company said </s> proceeds from the borrowings
under the credit agreement will be used to pay an $ NUM million cash dividend to macmillan and to lend the remainder of about $
NUM million to maxwell communications in connection with a UNK note </s> proceeds from the offering will be used to repay
borrowings under the short-term parts of a credit agreement </s> berlitz which is based in princeton n.j. provides language
instruction and translation services through more than NUM language centers in NUM countries </s> in the past five years more
than NUM NUM of its sales have been outside the u.s. </s> macmillan has owned berlitz since NUM </s> in the first six months

said that despite losses on ual stock his firm 's health is excellent </s> the stock 's decline also has left the ual board in a UNK </s>
although it may not be legally obligated to sell the company if the buy-out group ca n't revive its bid it may have to explore
alternatives if the buyers come back with a bid much lower than the group 's original $ 300-a-share proposal </s> at a meeting sept.
NUM to consider the labor-management bid the board also was informed by its investment adviser first boston corp. of interest
expressed by buy-out funds including kohlberg kravis roberts & co. and UNK little & co. as well as by robert bass morgan stanley 's
buy-out fund and pan am corp </s> the takeover-stock traders were hoping that mr. davis or one of the other interested parties might
UNK with the situation in disarray or that the board might consider a recapitalization </s> meanwhile japanese bankers said they
were still UNK about accepting citicorp 's latest proposal </s> macmillan inc. said it plans a public offering of NUM million shares
of its berlitz international inc. unit at $ NUM to $ NUM a share

capital markets to sell its hertz equipment rental corp. unit </s> there is no pressing need to sell the unit but we are doing it so we
can concentrate on our core business UNK automobiles in the u.s. and abroad said william UNK hertz 's executive vice president
</s> we are only going to sell at the right price </s> hertz equipment had operating profit before depreciation of $ NUM million on
revenue of $ NUM million in NUM </s> the closely held hertz corp. had annual revenue of close to $ NUM billion in NUM of
which $ NUM billion was contributed by its hertz rent a car operations world-wide </s> hertz equipment is a major supplier of rental
equipment in the u.s. france spain and the UNK </s> it supplies commercial and industrial equipment including UNK UNK UNK
and electrical equipment UNK UNK UNK and trucks </s> UNK inc. reported a net loss of $ NUM million for the fiscal third quarter
ended aug. NUM </s> it said the loss resulted from UNK and introduction costs related to a new medical UNK equipment system
</s> in the year-earlier quarter the company reported net income of $ NUM or

acquisition of nine businesses that make up the group the biggest portion of which was related to the NUM purchase of a UNK co.
unit </s> among other things the restructured facilities will substantially reduce the group 's required amortization of the term loan
portion of the credit facilities through september NUM mlx said </s> certain details of the restructured facilities remain to be
negotiated </s> the agreement is subject to completion of a definitive amendment and appropriate approvals </s> william p. UNK
mlx chairman and chief executive said the pact will provide mlx with the additional time and flexibility necessary to complete the
restructuring of the company 's capital structure </s> mlx has filed a registration statement with the securities and exchange
commission covering a proposed offering of $ NUM million in long-term senior subordinated notes and warrants </s> dow jones &
co. said it acquired a NUM NUM interest in UNK corp. a subsidiary of oklahoma publishing co. oklahoma city that provides
electronic research services </s> terms were n't disclosed </s> customers of either UNK or dow jones UNK are able to access the
information on both services </s> dow jones is the publisher of the wall street

video games electronic information systems and playing cards posted a NUM NUM unconsolidated surge in pretax profit to NUM
billion yen $ NUM million from NUM billion yen $ NUM million for the fiscal year ended aug. NUM </s> sales surged NUM NUM
to NUM billion yen from NUM billion </s> net income rose NUM NUM to NUM billion yen from NUM billion </s> UNK net fell
to NUM yen from NUM yen because of expenses and capital adjustments </s> without detailing specific product UNK UNK
credited its bullish UNK in sales including advanced computer games and television entertainment systems to surging UNK sales in
foreign markets </s> export sales for leisure items alone for instance totaled NUM billion yen in the NUM months up from NUM
billion in the previous fiscal year </s> domestic leisure sales however were lower </s> hertz corp. of park UNK n.j. said it retained
merrill lynch capital markets to sell its hertz equipment rental corp. unit </s> there is no pressing need to sell the unit but we are
doing it so we can concentrate on our core business UNK automobiles in the u.s. and abroad said william UNK hertz 's executive
vice president

so-called road show to market the package around the world </s> an increasing number of banks appear to be considering the option

Figure 6: Failure of neural cache on PTB. Lightly shaded regions show flat distribution.

words are predicted from the rough representation
of faraway context instead of specific occurrences
of certain words.

6.2 How does the cache help?
If LSTMs can already regenerate words from
nearby context, how are copy mechanisms help-
ing the model? We answer this question by ana-
lyzing how the neural cache model (Grave et al.,
2017b) helps with improving model performance.
The cache records the hidden state ht at each
timestep t, and computes a cache distribution over
the words in the history as follows:

P
cache

(wt|wt�1, . . . , w1;ht, . . . , h1)

/
t�1X

i=1

[wi = wt] exp(✓h
T
i ht),

(5)

where ✓ controls the flatness of the distribution.
This cache distribution is then interpolated with
the model’s output distribution over the vocabu-
lary. Consequently, certain words from the history
are upweighted, encouraging the model to copy
them.

Caches help words that can be copied from
long-range context the most. In order to study
the effectiveness of the cache for the three
classes of words (Cnear, Cfar, Cnone), we evaluate
an LSTM language model with and without a
cache, and measure the difference in perplexity for
these words. In both settings, the model is pro-
vided all prior context (not just 300 tokens) in or-

Figure 7: Model performance relative to using a
cache. Error bars represent 95% confidence inter-
vals. Words that can only be copied from the dis-
tant context benefit the most from using a cache.

der to replicate the Grave et al. (2017b) setup. The
amount of history recorded, known as the cache
size, is a hyperparameter set to 500 past timesteps
for PTB and 3,875 for Wiki, both values very sim-
ilar to the average document lengths in the respec-
tive datasets.

We find that the cache helps words that can
only be copied from long-range context (Cfar)
more than words that can be copied from nearby
(Cnear). This is illustrated by Figure 7 where with-
out caching, Cnear words see a 22% increase in
perplexity for PTB, and a 32% increase for Wiki,
whereas Cfar see a 28% increase in perplexity
for PTB, and a whopping 53% increase for Wiki.
Thus, the cache is, in a sense, complementary to
the standard model, since it especially helps regen-
erate words from the long-range context where the
latter falls short.



292

However, the cache also hurts about 36% of
the words in PTB and 20% in Wiki, which are
words that cannot be copied from context (Cnone),
as illustrated by bars for “none” in Figure 7. We
also provide some case studies showing success
(Fig. 5) and failure (Fig. 6) modes for the cache.
We find that for the successful case, the cache
distribution is concentrated on a single word that
it wants to copy. However, when the target is
not present in the history, the cache distribution
is more flat, illustrating the model’s confusion,
shown in Figure 6. This suggests that the neural
cache model might benefit from having the option
to ignore the cache when it cannot make a confi-
dent choice.

7 Discussion

The findings presented in this paper provide a
great deal of insight into how LSTMs model con-
text. This information can prove extremely use-
ful for improving language models. For instance,
the discovery that some word types are more im-
portant than others can help refine word dropout
strategies by making them adaptive to the different
word types. Results on the cache also show that
we can further improve performance by allowing
the model to ignore the cache distribution when it
is extremely uncertain, such as in Figure 6. Dif-
ferences in nearby vs. long-range context suggest
that memory models, which feed explicit context
representations to the LSTM (Ghosh et al., 2016;
Lau et al., 2017), could benefit from representa-
tions that specifically capture information orthog-
onal to that modeled by the LSTM.

In addition, the empirical methods used in this
study are model-agnostic and can generalize to
models other than the standard LSTM. This opens
the path to generating a stronger understanding of
model classes beyond test set perplexities, by com-
paring them across additional axes of information
such as how much context they use on average, or
how robust they are to shuffled contexts.

Given the empirical nature of this study and the
fact that the model and data are tightly coupled,
separating model behavior from language charac-
teristics, has proved challenging. More specifi-
cally, a number of confounding factors such as vo-
cabulary size, dataset size etc. make this separa-
tion difficult. In an attempt to address this, we
have chosen PTB and Wiki - two standard lan-
guage modeling datasets which are diverse in con-

tent (news vs. factual articles) and writing style,
and are structured differently (eg: Wiki articles are
4-6x longer on average and contain extra informa-
tion such as titles and paragraph/section markers).
Making the data sources diverse in nature, has pro-
vided the opportunity to somewhat isolate effects
of the model, while ensuring consistency in re-
sults. An interesting extension to further study this
separation would lie in experimenting with differ-
ent model classes and even different languages.

Recently, Chelba et al. (2017), in proposing a
new model, showed that on PTB, an LSTM lan-
guage model with 13 tokens of context is similar
to the infinite-context LSTM performance, with
close to an 8% 5 increase in perplexity. This is
compared to a 25% increase at 13 tokens of con-
text in our setup. We believe this difference is
attributed to the fact that their model was trained
with restricted context and a different error propa-
gation scheme, while ours is not. Further investi-
gation would be an interesting direction for future
work.

8 Conclusion

In this analytic study, we have empirically shown
that a standard LSTM language model can effec-
tively use about 200 tokens of context on two
benchmark datasets, regardless of hyperparame-
ter settings such as model size. It is sensitive to
word order in the nearby context, but less so in
the long-range context. In addition, the model is
able to regenerate words from nearby context, but
heavily relies on caches to copy words from far
away. These findings not only help us better un-
derstand these models but also suggest ways for
improving them, as discussed in Section 7. While
observations in this paper are reported at the to-
ken level, deeper understanding of sentence-level
interactions warrants further investigation, which
we leave to future work.

Acknowledgments

We thank Arun Chaganty, Kevin Clark, Reid
Pryzant, Yuhao Zhang and our anonymous review-
ers for their thoughtful comments and suggestions.
We gratefully acknowledge support of the DARPA
Communicating with Computers (CwC) program
under ARO prime contract no. W911NF15-1-
0462 and the NSF via grant IIS-1514268.

5Table 3, 91 perplexity for the 13-gram vs. 84 for the
infinite context model.



293

References
Yossi Adi, Einat Kermany, Yonatan Belinkov,

Ofer Lavi, and Yoav Goldberg. 2017. Fine-
grained analysis of sentence embeddings using
auxiliary prediction tasks. International Con-
ference on Learning Representations (ICLR)

https://openreview.net/pdf?id=BJh6Ztuxl.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. International
Conference on Learning Representations (ICLR)

https://arxiv.org/pdf/1409.0473.pdf.

Jordan Boyd-Graber and David Blei. 2009. Syn-
tactic topic models. In Advances in neu-
ral information processing systems. pages 185–
192. https://papers.nips.cc/paper/3398-syntactic-
topic-models.pdf.

Ciprian Chelba, Mohammad Norouzi, and Samy
Bengio. 2017. N-gram language model-
ing using recurrent neural network esti-
mation. arXiv preprint arXiv:1703.10724
https://arxiv.org/pdf/1703.10724.pdf.

Yann N Dauphin, Angela Fan, Michael Auli, and
David Grangier. 2017. Language modeling
with gated convolutional networks. Interna-
tional Conference on Machine Learning (ICML)

https://arxiv.org/pdf/1612.08083.pdf.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in neural informa-
tion processing systems (NIPS). pages 1019–1027.
https://arxiv.org/pdf/1512.05287.pdf.

Shalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy,
Tom Dean, and Larry Heck. 2016. Contextual lstm
(clstm) models for large scale nlp tasks. Work-
shop on Large-scale Deep Learning for Data Min-

ing, KDD https://arxiv.org/pdf/1602.06291.pdf.

Edouard Grave, Moustapha M Cisse, and Ar-
mand Joulin. 2017a. Unbounded cache model
for online language modeling with open vo-
cabulary. In Advances in Neural Information
Processing Systems (NIPS). pages 6044–6054.
https://papers.nips.cc/paper/7185-unbounded-
cache-model-for-online-language-modeling-with-
open-vocabulary.pdf.

Edouard Grave, Armand Joulin, and Nicolas Usunier.
2017b. Improving Neural Language Mod-
els with a Continuous Cache. International
Conference on Learning Representations (ICLR)

https://openreview.net/pdf?id=B184E5qee.

Alex Graves. 2013. Generating se-
quences with recurrent neural net-
works. arXiv preprint arXiv:1308.0850
https://arxiv.org/pdf/1308.0850.pdf.

Felix Hill, Antoine Bordes, Sumit Chopra, and
Jason Weston. 2016. The goldilocks princi-
ple: Reading children’s books with explicit
memory representations. International Con-
ference on Learning Representations (ICLR)

https://arxiv.org/pdf/1511.02301.pdf.

Sepp Hochreiter and Jürgen Schmidhu-
ber. 1997. Long short-term memory.
Neural computation 9(8):1735–1780.
https://doi.org/10.1162/neco.1997.9.8.1735.

Hakan Inan, Khashayar Khosravi, and Richard Socher.
2017. Tying word vectors and word classifiers:
A loss framework for language modeling. Inter-
national Conference on Learning Representations

(ICLR) https://openreview.net/pdf?id=r1aPbsFle.

Rafal Jozefowicz, Oriol Vinyals, Mike Schus-
ter, Noam Shazeer, and Yonghui Wu. 2016.
Exploring the limits of language mod-
eling. arXiv preprint arXiv:1602.02410
https://arxiv.org/pdf/1602.02410.pdf.

Jey Han Lau, Timothy Baldwin, and Trevor Cohn.
2017. Topically Driven Neural Language Model.
Association for Computational Linguistics (ACL)

https://doi.org/10.18653/v1/P17-1033.

Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan
Jurafsky. 2016. Visualizing and understanding
neural models in nlp. North American As-
sociation of Computational Linguistics (NAACL)

http://www.aclweb.org/anthology/N16-1082.

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the ability of lstms to learn
syntax-sensitive dependencies. Transactions of the
Association for Computational Linguistics (TACL)

http://aclweb.org/anthology/Q16-1037.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural lan-
guage processing toolkit. In Proceedings of 52nd
annual meeting of the association for computational

linguistics: system demonstrations. pages 55–60.
https://doi.org/10.3115/v1/P14-5010.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large
annotated corpus of english: The penn tree-
bank. Computational linguistics 19(2):313–330.
http://aclweb.org/anthology/J93-2004.

Gabor Melis, Chris Dyer, and Phil Blunsom.
2018. On the State of the Art of Evalua-
tion in Neural Language Models. International
Conference on Learning Representations (ICLR)

https://openreview.net/pdf?id=ByJHuTgA-.

Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2018. Regularizing and Optimizing
LSTM Language Models. International Con-
ference on Learning Representations (ICLR)

https://openreview.net/pdf?id=SyyGPP0TZ.

https://openreview.net/pdf?id=BJh6Ztuxl
https://openreview.net/pdf?id=BJh6Ztuxl
https://openreview.net/pdf?id=BJh6Ztuxl
https://openreview.net/pdf?id=BJh6Ztuxl
https://arxiv.org/pdf/1409.0473.pdf
https://arxiv.org/pdf/1409.0473.pdf
https://arxiv.org/pdf/1409.0473.pdf
https://papers.nips.cc/paper/3398-syntactic-topic-models.pdf
https://papers.nips.cc/paper/3398-syntactic-topic-models.pdf
https://papers.nips.cc/paper/3398-syntactic-topic-models.pdf
https://papers.nips.cc/paper/3398-syntactic-topic-models.pdf
https://arxiv.org/pdf/1703.10724.pdf
https://arxiv.org/pdf/1703.10724.pdf
https://arxiv.org/pdf/1703.10724.pdf
https://arxiv.org/pdf/1703.10724.pdf
https://arxiv.org/pdf/1612.08083.pdf
https://arxiv.org/pdf/1612.08083.pdf
https://arxiv.org/pdf/1612.08083.pdf
https://arxiv.org/pdf/1512.05287.pdf
https://arxiv.org/pdf/1512.05287.pdf
https://arxiv.org/pdf/1512.05287.pdf
https://arxiv.org/pdf/1512.05287.pdf
https://arxiv.org/pdf/1602.06291.pdf
https://arxiv.org/pdf/1602.06291.pdf
https://arxiv.org/pdf/1602.06291.pdf
https://papers.nips.cc/paper/7185-unbounded-cache-model-for-online-language-modeling-with-open-vocabulary.pdf
https://papers.nips.cc/paper/7185-unbounded-cache-model-for-online-language-modeling-with-open-vocabulary.pdf
https://papers.nips.cc/paper/7185-unbounded-cache-model-for-online-language-modeling-with-open-vocabulary.pdf
https://papers.nips.cc/paper/7185-unbounded-cache-model-for-online-language-modeling-with-open-vocabulary.pdf
https://papers.nips.cc/paper/7185-unbounded-cache-model-for-online-language-modeling-with-open-vocabulary.pdf
https://papers.nips.cc/paper/7185-unbounded-cache-model-for-online-language-modeling-with-open-vocabulary.pdf
https://openreview.net/pdf?id=B184E5qee
https://openreview.net/pdf?id=B184E5qee
https://openreview.net/pdf?id=B184E5qee
https://arxiv.org/pdf/1308.0850.pdf
https://arxiv.org/pdf/1308.0850.pdf
https://arxiv.org/pdf/1308.0850.pdf
https://arxiv.org/pdf/1308.0850.pdf
https://arxiv.org/pdf/1511.02301.pdf
https://arxiv.org/pdf/1511.02301.pdf
https://arxiv.org/pdf/1511.02301.pdf
https://arxiv.org/pdf/1511.02301.pdf
https://doi.org/10.1162/neco.1997.9.8.1735
https://doi.org/10.1162/neco.1997.9.8.1735
https://openreview.net/pdf?id=r1aPbsFle
https://openreview.net/pdf?id=r1aPbsFle
https://openreview.net/pdf?id=r1aPbsFle
https://arxiv.org/pdf/1602.02410.pdf
https://arxiv.org/pdf/1602.02410.pdf
https://arxiv.org/pdf/1602.02410.pdf
https://doi.org/10.18653/v1/P17-1033
https://doi.org/10.18653/v1/P17-1033
http://www.aclweb.org/anthology/N16-1082
http://www.aclweb.org/anthology/N16-1082
http://www.aclweb.org/anthology/N16-1082
http://aclweb.org/anthology/Q16-1037
http://aclweb.org/anthology/Q16-1037
http://aclweb.org/anthology/Q16-1037
https://doi.org/10.3115/v1/P14-5010
https://doi.org/10.3115/v1/P14-5010
https://doi.org/10.3115/v1/P14-5010
http://aclweb.org/anthology/J93-2004
http://aclweb.org/anthology/J93-2004
http://aclweb.org/anthology/J93-2004
http://aclweb.org/anthology/J93-2004
https://openreview.net/pdf?id=ByJHuTgA-
https://openreview.net/pdf?id=ByJHuTgA-
https://openreview.net/pdf?id=ByJHuTgA-
https://openreview.net/pdf?id=SyyGPP0TZ
https://openreview.net/pdf?id=SyyGPP0TZ
https://openreview.net/pdf?id=SyyGPP0TZ


294

Stephen Merity, Caiming Xiong, James Brad-
bury, and Richard Socher. 2017. Pointer
Sentinel Mixture Models. International Con-
ference on Learning Representations (ICLR)

https://openreview.net/pdf?id=Byj72udxe.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan
Černockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In
Eleventh Annual Conference of the International

Speech Communication Association.

Ofir Press and Lior Wolf. 2017. Using the output em-
bedding to improve language models. European
Chapter of the Association for Computational Lin-

guistics http://aclweb.org/anthology/E17-2025.

Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun,
and Rob Fergus. 2013. Regularization of neural
networks using dropconnect. In International Con-
ference on Machine Learning (ICML). pages 1058–
1066.

Tian Wang and Kyunghyun Cho. 2016. Larger-Context
Language Modelling with Recurrent Neural Net-
work. Association for Computational Linguistics
(ACL) https://doi.org/10.18653/v1/P16-1125.

Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and
William W Cohen. 2018. Breaking the softmax
bottleneck: a high-rank rnn language model. Inter-
national Conference on Learning Representations

(ICLR) https://openreview.net/pdf?id=HkwZSG-
CZ.

https://openreview.net/pdf?id=Byj72udxe
https://openreview.net/pdf?id=Byj72udxe
https://openreview.net/pdf?id=Byj72udxe
http://aclweb.org/anthology/E17-2025
http://aclweb.org/anthology/E17-2025
http://aclweb.org/anthology/E17-2025
https://doi.org/10.18653/v1/P16-1125
https://doi.org/10.18653/v1/P16-1125
https://doi.org/10.18653/v1/P16-1125
https://doi.org/10.18653/v1/P16-1125
https://openreview.net/pdf?id=HkwZSG-CZ
https://openreview.net/pdf?id=HkwZSG-CZ
https://openreview.net/pdf?id=HkwZSG-CZ
https://openreview.net/pdf?id=HkwZSG-CZ

