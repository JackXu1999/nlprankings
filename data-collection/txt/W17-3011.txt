



















































Technology Solutions to Combat Online Harassment


Proceedings of the First Workshop on Abusive Language Online, pages 73–77,
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

Hack Harassment: Technology Solutions to Combat Online Harassment

George W. Kennedy III
Intel

Hillsboro, OR
george.w.kennedy@intel.com

Andrew W. McCollough
EdgeRock Technology Partners

Hillsboro, OR

Edward Dixon
Intel

Cork, Ireland
edward.dixon@intel.com

Alexie Bastidas
Intel

Santa Clara, CA

John Ryan
Intel

Cork, Ireland

Chris Loo
Intel

Santa Clara, CA

Saurav Sahay
Intel

Santa Clara, CA

Abstract

This work is part of a new initiative to
use machine learning to identify online
harassment in social media and comment
streams. Online harassment goes under-
reported due to the reliance on humans to
identify and report harassment, reporting
that is further slowed by requirements to
fill out forms providing context. In addi-
tion, the time for moderators to respond
and apply human judgment can take days,
but response times in terms of minutes
are needed in the online context. Though
some of the major social media compa-
nies have been doing proprietary work in
automating the detection of harassment,
there are few tools available for use by the
public. In addition, the amount of labeled
online harassment data and availability of
cross platform online harassment datasets
is limited. We present the methodology
used to create a harassment dataset and
classifier and the dataset used to help the
system learn what harassment looks like.

1 Introduction

Online harassment has been a problem to a greater
or lesser extent since the early days of the inter-
net. Previous work has applied anti-spam tech-
niques like machine learning based text classifica-

tion (Reynolds et al., 2011) to detecting harassing
messages. However, existing public datasets are
limited in size, with labels of varying quality.

The #HackHarassment (Harassment, 2017) ini-
tiative (an alliance of tech companies and NGOs
devoted to fighting bullying on the internet) has
begun to address this issue by creating a web tool
to collect and label data, and using the tool to gen-
erate a large, high-quality, cross-platform dataset.
The release of this tool is scheduled for Sum-
mer 2017. As we complete further rounds of la-
belling with a public audience, later iterations of
this dataset will increase the available samples by
at least an order of magnitude and enable corre-
sponding improvements in the quality of machine
learning models we have built for harassment de-
tection. In this paper, we introduce an improved
cross-platform harassment dataset and a machine
learning model built on the dataset.

2 Related Work

Previous work in the area by (Bayzick et al.,
2011) showed that natural language processing in
combination with a rule-based system could detect
bullying messages on an online forum, but with
very poor accuracy. However, the same work also
made clear that the limiting factor on such mod-
els was the availability of a suitable quantity of la-
beled examples, e.g. the Bayzick work relied on a
dataset of 2,696 samples, only 196 of which were

73



found to be examples of bullying behavior. Addi-
tionally, this work relied on classical decision-tree
models like J48 and JRIP, and k-nearest neigh-
bors classifiers like IBk, as opposed to modern
ensemble methods or deep neural-network-based
approaches. In addition, Intel’s #HackHarassment
team published work (Bastidas et al., 2016) show-
ing results for harassment detection using a variety
of model types on a new dataset of comments and
posts which their team had labelled.

More recently, major internet companies have
focused efforts on combating various forms of
harassment online. Yahoo researchers have de-
veloped machine learning models for detecting
abusive language (Nobata et al., 2016) and a
Google Jigsaw team partnered with the Wikime-
dia Foundation to develop solutions for reducing
personal attacks or toxic comments, in Wikimedia
editing (Wulczyn et al., 2017). Nobata outper-
formed state-of-the-art deep learning approaches
with their supervised learning approach using a
combination of linguistic, n-gram (including char-
acter n-grams), syntactic (POS), and semantic (us-
ing comment embeddings similar to word2vec)
features. In addition, the Yahoo team has released
the longitudinal New Feed data set used in the
study on (Webscope, 2017). Wulczyn demon-
strated that their machine models can perform as
well as three human graders in identifying toxic
comments in Wikipedia editing wars, and in addi-
tion released the Perspective API to enable devel-
opers to utilize their solution. However, see (Hos-
seini et al., 2017) for comments on adversarial at-
tacks and the resultant fragility of the model - and
other models that depend on token-level features.
We extend these results and others by developing a
system architecture for crowdsourcing sample la-
beling, a crosssocial-media-platform dataset, and
providing an open source classifier for developers
to build upon. The classifier is intended to be open
sourced in Summer 2017.

3 Methods

In this work, we build upon our initial results using
version 1.0 of our dataset (Bastidas et al., 2016).
We followed a supervised classification method
that uses a data with gold-standard labeled com-
ments and a set discriminating linguistic proper-
ties, or features, of each comment to predict the
class membership of new or untrained comments.
Our features consisted primarily of n-gram and a

small set of linguistic features on datasets drawn
from The Guardian, Reddit, and Twitter. We per-
formed no significant pre-processing on the data
other than tokenization, though in the future we
anticipate adding further feature-reduction steps,
such as stemming, to improve model performance.

4 Data Source Selection

Three initial data sources were selected: The
Guardian, Reddit, and Twitter. Text from each
data source were extracted in several ways in Sum-
mer 2016. Comments on polarizing or hot-button
news articles were extracted from the Guardian,
an online news source. Comments from Reddit, a
popular social media site, were selected from com-
ment which had received at least 100 down votes.
Short texts from Twitter, tweets were hand-curated
from an initially machine-selected data set from
Twitter, and then further tweets scraped by search-
ing on polarizing or hot-button topics.

4.1 Reddit

Comments from Reddit were downloaded from
a publicly available dataset on Google BigQuery,
reddit comments all 2015. These comments were
then filtered to those that had received at least 100
down votes. We used our initial version of the
classifier to label these comments. The resulting
5700 harassing comments were then further manu-
ally labeled by an in-house team of analysts. Ana-
lysts were given instructions and examples for an-
notation of harassment or non-harassment. In ad-
dition, the raters were provided with an additional
set of more fine-grained labels but instruction on
annotation was not provided.). Each post was la-
beled independently by at least five Intel Security
Web Analysts. A perfect consensus was relatively
rare, and so we rated a post as harassing if 40%, 2
of our 5 raters, consider it to be harassing.

4.2 Twitter

Data were comprised of two sources: manual cu-
ration and annotation of a pre-existing machine-
annotated dataset and a set of scraped tweets us-
ing proprietary sampling methods. The sampling
should not be considered unbiased. The initial
5000 tweets were sourced from an online repos-
itory of tweets at. Additional tweets were scraped
directly from Twitter during July 2016 using a cus-
tom twitterbot that queried on hot-button topics
as keywords to the Twitter API. These additional

74



Figure 1: Corpus Comment Counts by Source

Figure 2: Inter-annotator agreement for Hack Ha-
rassment (HH) compared to Yahoo. HH uses Krip-
pendorfs Alpha and Yahoo uses Kappa. Agree-
ment is an average pairwise agreement.

tweets were first labeled by our early classifier and
then manually labeled by our team (Hart, 2016).

4.3 The Guardian

Comments were scraped from 15 articles covering
hot-button or polarizing topics. We believe that
minimal harassing comments were found in the
Guardian dataset as Guardian comments are cu-
rated by a team of moderators in accordance with
their content policy. Therefore, minimal or no ha-
rassing comments should be expected, as we con-
firmed in the dataset.

Figure 1 shows that the current data set is rea-
sonably unbalanced overall with a 1 : 4 ratio of
non-harassing to harassing comments. In addi-
tion, the categories are unbalanced across source
as well as category within source, such that Red-
dit, despite being only 28% of the total comments
contributed 56% of the harassing comments.

As shown in Figure 2 and Figure 3, aver-
age agreement is below 90% for the Guardian

Figure 3: Average Percent of Agreement Among
Amazon Mechanical Turk (AMT) Annotators

and Twitter surveys, with an average across all
Qualtrics surveys only .875. This is well be-
low what is typically suggested for raw agreement
scores.

Guardian URLs
https://www.theguardian.com/discussion/p/4pcq2
https://www.theguardian.com/discussion/p/4pgek
https://www.theguardian.com/discussion/p/4an9q
https://www.theguardian.com/discussion/p/4p76x
https://www.theguardian.com/discussion/p/4pdqd
https://www.theguardian.com/discussion/p/4phck
https://www.theguardian.com/discussion/p/4pf70
https://www.theguardian.com/discussion/p/4pfe3
https://www.theguardian.com/discussion/p/4k4tx
https://www.theguardian.com/discussion/p/4pd76
https://www.theguardian.com/discussion/p/4jmg2
https://www.theguardian.com/discussion/p/4pg57
https://www.theguardian.com/discussion/p/4p6dt
https://www.theguardian.com/discussion/p/4p6gn
https://www.theguardian.com/discussion/p/4pgbx

Table 1: Guardian URLs used to scrape initial
comments.

5 Data Ingest and Annotation Methods

Data ingest process and annotation were heteroge-
neous in nature. Manual curation was combined
with machine annotation in several iterated steps
to produce a final annotated dataset. The comment
dataset was simply annotated with a Boolean in-
dicating harassment. Harassment was determined
on the gold data through a percent voting method:
the reported metrics are for 40% and above simple
agreement among raters that a given comment is
harassment.

All preprocessing, training and evaluation was
carried out in Python, using the popular SciK-
itLearn (for feature engineering and linear models)
in combination with Numpy3 (for matrix opera-
tions) (Pedregosa et al., 2011; van der Walt et al.,

75



2011).

6 Feature Selection

Features were generated by tokenizing each com-
ment, hashing the resulting n-grams, and comput-
ing a TF/IDF value for each token. The resultant
feature vectors were used to train a Random Forest
classifier. We used the following features:

• Unigram and Bigram TF-IDF: this is a stan-
dard feature used in text-categorization. We
used unigrams and bigrams. Trigrams were
not used because the size of the dataset meant
almost all trigrams were too rare for their
presence and absence to reach statistical sig-
nificance.

• Character N-Gram TF-IDF from 3 to 6 char-
acters: The goal with this was to target com-
mon alternative spellings of words, particu-
larly frequent in online communication.

• Unigram Token Count: we utilized NLTKs
Twitter Tokenizer to tokenize the tokens and
count the number of tokens. The Twitter To-
kenizer handles URLs and Hashtags much
better than a standard punctuation based tok-
enizers found in NLTK or Sck-kit Learn. Our
assumption behind using token count is that
harassing texts tend to be brief assaults rather
than long diatribes.

• Source: In combination with the token count,
we selected a dummy coefficient (toggled as
1 or 0) to highlight if a comment is sourced
from Twitter or not.

• Sentiment Polarities: we utilized NLTKs
VADER Sentiment Analyzer to generate sen-
timent polarities for positive, neutral, and
negative sentiment. Our assumption was that
harassing comments tend to have more neg-
ative sentiment, whereas non-harassing com-
ments tend to have more positive sentiment.

7 Training Dataset

The current training dataset contains: 20,432
unique comments. Of these comments, 4136 are
labeled as harassment, 16296 are labeled as non-
harassment. 12,049 comments are sourced from
Twitter, with the remaining 8383 being from Red-
dit or the Guardian.

8 Machine Learning Model

Bastidas tested a variety of algorithms, includ-
ing SVM, Decision Tree, Random Forest (ensem-
ble Decision Trees), and Multinomial Nave Bayes
(Bastidas et al., 2016). We increased the size
of the Reddit dataset and included labeled com-
ments that were sampled from Twitter and The
Guardian. We collected performance results using
this larger, cross-platform dataset, described in the
Data Source Selection section, and a Scikit-Learn
Random Forest classifier. For our hyperparame-
ters we limited the number of trees to 200 and left
the tree depth unbounded. Subsequently, the data
were trained on the Random Forest by splitting the
dataset into 80/20 training and evaluation sets, and
then the training data were further split into Kfold
(n=10) folds for cross-validation and the average
results reported in Table 2. Of primary concern to
us is to optimize for high recall. We want to mini-
mize our false-negative rate for harassment.

Class Precision Recall F1 Score
Not Harassing 0.93 0.95 0.94
Harassing 0.75 0.68 0.71
Average 0.89 0.90 0.90

Table 2: Random forest classifier results.

9 Future Work

New work from Facebook and OpenAI on
text classification suggests obvious next steps.
Bytelevel deep neural nets are capable of state-of-
theart results on large datasets, can exploit unla-
beled data, as described in recent work from Ope-
nAI (Radford et al., 2017) and have the poten-
tial to resist the ”adversarial” tokens described in
(Hosseini et al., 2017). Using OpenAI’s approach
with a large, unlabeled dataset for pre-training is
an obvious next step. A contrasting approach that
requires further evaluation is the FastText model
from Facebook’s Advanced Research Lab, which,
as described in (Joulin et al., 2016) and (Bo-
janowski et al., 2016), is competitive with deep
convolutional neural networks and can exploit un-
labeled data using pre-trained WordVectors, while
requiring vastly less training time than competitive
alternatives.

10 Conclusion

We have presented to our cross-platform harass-
ment dataset, machine learning model. We intend

76



to open our labeling platform to the public to ex-
pand the Hack Harassment cross platform dataset.
As we complete further rounds of labelling with a
public audience, later iterations of this dataset will
increase the available samples by at least an order
of magnitude, enabling corresponding improve-
ments in the quality of machine learning mod-
els for harassment detection. We look forward
to both the availability of a larger, cross-social-
mediaplatform harassment dataset and seeing the
development of classifiers that improve upon our
work. We welcome partners able to contribute to
expanding the dataset and improving the model-
ing.

References
A. Bastidas, E. Dixon, C. Loo, and J. Ryan. 2016.

Harassment detection: a benchmark on the hackha-
rassment dataset. In Proceedings of the Collabora-
tive European Research Conference (CERC 2016),
Cork. pages 76–79.

J. Bayzick, A. Kontostathis, and L. Edwards. 2011.
Detecting the presence of cyberbullying using com-
puter software.

P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov.
2016. Enriching word vectors with subword infor-
mation. arXiv.org.

Hack Harassment. 2017. Hack harassment. https:
//www.hackharassment.com/.

M. Hart. 2016. Twitterclassifier. https:
//github.com/HackHarassment/
TwitterClassifier.

H. Hosseini, S. Kannan, B. Zhang, and R. Poovendran.
2017. Deceiving google’s perspective api built for
detecting toxic comments. arXiv.org.

A. Joulin, E. Grave, P. Bojanowski, and Mikolov. T.
2016. Bag of tricks for efficient text classification.
arXiv.org.

Chikashi Nobata, Joel Tetreault, Achint Thomas,
Yashar Mehdad, and Yi Chang. 2016. Abusive lan-
guage detection in online user content. In Proceed-
ings of the 25th International Conference on World
Wide Web. International World Wide Web Confer-
ences Steering Committee, Republic and Canton of
Geneva, Switzerland, WWW ’16, pages 145–153.
https://doi.org/10.1145/2872427.2883062.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, and et al. 2011. Scikit-learn:
Machine learning in python. Journal of Machine
Learning Research 12:145–153.

A. Radford, R. Jozefowicz, and I. Sutskever. 2017.
Learning to generate reviews and discovering sen-
timent.

K. Reynolds, A. Kontostathis, and L. Edwards.
2011. Using machine learning to detect cyberbul-
lying. In Presented at the 2011 Tenth Interna-
tional Conference on Machine Learning and Appli-
cations(ICMLA 2011), IEEE.. pages 241–244.

S. van der Walt, S. C. Colbert, and G. Varoquaux. 2011.
The numpy array: A structure for efficient numerical
computation. Computing in Science & Engineering
13(2):22–30.

Webscope. 2017. Webscope. https:
//webscope.sandbox.yahoo.com/.

E. Wulczyn, D. Taraborelli, N. Thain, and L Dixon.
2017. Algorithms and insults: Scaling up
our understanding of harassment on wikipedia.
https://medium.com/jigsaw/algorithms-and-insults-
scaling-up-ourunderstanding-of-harassment-on-
wikipedia-6cc417b9f7ff.

77


