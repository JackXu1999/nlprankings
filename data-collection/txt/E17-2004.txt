



















































Robust Training under Linguistic Adversity


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 21–27,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Robust Training under Linguistic Adversity

Yitong Li and Trevor Cohn and Timothy Baldwin
Department of Computing and Information Systems

The University of Melbourne, Australia
yitongl4@student.unimelb.edu.au, {tcohn,tbaldwin}@unimelb.edu.au

Abstract

Deep neural networks have achieved re-
markable results across many language
processing tasks, however they have been
shown to be susceptible to overfitting and
highly sensitive to noise, including ad-
versarial attacks. In this work, we pro-
pose a linguistically-motivated approach
for training robust models based on ex-
posing the model to corrupted text exam-
ples at training time. We consider sev-
eral flavours of linguistically plausible cor-
ruption, include lexical semantic and syn-
tactic methods. Empirically, we evaluate
our method with a convolutional neural
model across a range of sentiment analy-
sis datasets. Compared with a baseline and
the dropout method, our method achieves
better overall performance.

1 Introduction

Deep learning has achieved state-of-the-art results
across a range of computer vision (Krizhevsky
et al., 2012), speech recognition (Graves et al.,
2013), and natural language processing tasks
(Bahdanau et al., 2015; Kalchbrenner et al., 2014;
Bitvai and Cohn, 2015). However, deep mod-
els tend to be overconfident in their predictions
over noisy test instances, including adversarial ex-
amples (Szegedy et al., 2014; Goodfellow et al.,
2015). A range of methods have been proposed to
train models to be more robust, such as injecting
noise into the data and hidden layers (Jiang et al.,
2009), dropout (Srivastava et al., 2014), and the
incorporation of explicit regularization terms into
the training objective (Ng, 2004; Li et al., 2016).

In this work, we propose a linguistically-
motivated method customised to text applications,
based on injecting different kinds of word- and

20

40

60

80

0 1 2 3 4
Noise [%]

Te
st

A
cc

ur
ac

y
[%

]

Figure 1: Accuracy (%) drops as we increase ad-
versarial noise to word embeddings, as evaluated
on binary classification dataset MR.

sentence-level linguistic noise into the input text,
inspired by adversarial examples (Goodfellow et
al., 2015). Our method has its origins in computer
vision, where it has been shown that small pixel
perturbations indiscernible to humans can signif-
icantly distort the predictions of state-of-the-art
deep models (Szegedy et al., 2014; Nguyen et al.,
2015), an observation that has been harnessed in
recent work on adversarial training (Goodfellow et
al., 2015). This kind of noise is cheap to generate
for images and is transferable between different
models, but it is less clear how to generate analo-
gous textual noise while preserving the fidelity of
the training data, due to text being discrete and se-
quential in nature, with latent syntactic structure.
Based on the same linguistic intuition, adversarial
evaluation for natural language processing mod-
els was proposed by Smith (2012). Also, adver-
sarial learning for text, such as perceptron learn-
ing (Søgaard, 2013) and unsupervised estimation
methods (Smith and Eisner, 2005), have been stud-
ied in the language area.

Word embeddings learned from WORD2VEC

21



(Mikolov et al., 2013) and GLOVE (Pennington
et al., 2014) are now widely used as input to lan-
guage processing models, however these represen-
tations are highly susceptible to noise. For ex-
ample, Figure 1 shows that as we add adversarial
noise η = �∇xLoss(x, y, θ) to WORD2VEC rep-
resentations, classification accuracy for a convolu-
tional model (Kim, 2014) over a sentiment classi-
fication task (Pang and Lee, 2008) drops apprecia-
bly, such that with only 1% perturbations, a state-
of-the-art model drops to the level of a random
classifier.

Word embeddings are not an intuitive represen-
tation of human language, and it is not immedi-
ately clear how to generate adversarial noise over
the raw text input without affecting the fidelity of
the data. In human-to-human textual communi-
cation such as chat and microblogs, humans are
remarkably resilient to “noise”, in terms of typos,
lexical and syntactic disfluencies, and the large va-
riety of semantically-equivalent ways of express-
ing the same content (Han and Baldwin, 2011;
Eisenstein, 2013; Baldwin et al., 2013; Pavlick
and Callison-Burch, 2016). These observations
are the inspiration for this work, in proposing a
training strategy based on the explicit generation
of linguistic corruption over the source training in-
stances, to train robust text models. Empirically,
we demonstrate the effectiveness of our method
over a range of sentiment analysis datasets us-
ing a state-of-the-art convolutional neural network
model (Kim, 2014). In this, we show that our
method is superior to a baseline and dropout (Sri-
vastava et al., 2014) using MAP training.1

2 Generating Text Noise

Our method involves the explicit generation of
several kinds of linguistic corruption, to train more
robust deep models. The first question is how to
generate the linguistic noise, focusing on English
for the purposes of this paper. We focus on the
generation of two classes of text noise: (1) syntac-
tic noise; and (2) semantic noise.2

Syntactic Noise The first class of linguistic
noise is syntactic, focusing on the syntactic struc-

1The implementation is freely available at https://
github.com/lrank/Linguistic_adversity.

2We also experimented with a method which generates
lexical noise, but for the purposes of our experiments here, as
the vast majority of the generated candidates are OOV words,
it is largely equivalent to word dropout, and omitted from this
paper.

ture of the input, either through explicit parsing
and generation using a deep linguistic parser, or
sentence compression.

For the deep linguistic parser, we use the LinGO
English Resource Grammar (“ERG”: Copestake
and Flickinger (2000)) with the ACE parser, based
on pyDelphin.3 The ERG supports both pars-
ing and generation, via the semantic formalism
of Minimal Recursion Semantics (“MRS”: Copes-
take et al. (2005)). To generate paraphrases with
the ERG, we simply parse a given input, select
the preferred parse using a pretrained parse selec-
tion model (Oepen et al., 2002), and exhaustively
generate from the resultant MRS. We then use uni-
form random sampling to select from the genera-
tor outputs, which potentially numbers in the thou-
sands of variants. To handle unknown words dur-
ing parsing and generation, we use POS mapping
and introduce a unique relation for each unknown
word, which we use to substitute the unknown
word back in to the generation output. In practice,
the primary sources of “noise” introduced by the
ERG are due to topicalisation, adjective ordering,
fronting of adverbial phrases, and relativisation of
modifiers.

The second approach to syntactic noise is based
on sentence compression (“COMP”: Knight and
Marcu (2000)), which aims to “trim” an input of
peripheral content, while maintaining grammati-
cality, and also the syntax of the original as much
as possible. While the state-of-the-art in sentence
compression is based on deep learning methods
such as recurrent neural networks (Filippova et
al., 2015), we implement a simple parser-based
model, due to the lack of large-scale annotated
data for training and the fact that a relative lack
of precision in the output may ultimately help our
method. First, we parse the sentence using the
Stanford CoreNLP constituency parser (Chen and
Manning, 2014). Next, we model the conditional
probability of deleting a sub-tree C with label S
given its parent node with labelR by p(C|S,R) =

p(C,S,R)
ΣCp(C,S,R)

, trained on the sentence compression
corpora of Clarke and Lapata (2006),4 made up of
a few hundred labelled instances.

Semantic Noise The second class of linguistic
noise is semantic noise. Semantic noise is more
subtle than syntactic noise, as we must be careful

3https://github.com/delph-in/pydelphin
4http://jamesclarke.net/research/

resources/

22



not to impact on the fidelity of the original labels,
which can readily occur with full paraphrasing or
abstractive summarisation. As such, we focus on
lexical substitution of near-synonyms of words in
the original text, and experiment with two methods
for generating near-synonyms.

Our approach to generating semantic noise pro-
ceeds as follows. First, we apply filters to iden-
tify words which should not be candidates for lex-
ical substitution, namely words which are parts
of named entities or function words. As such,
we use the Stanford CoreNLP POS tagger and
named entity recogniser (Finkel et al., 2005; Chen
and Manning, 2014), and identify “substitutable
words” as those which are nouns, verbs, adjectives
or adverbs, and not part of a named entity. For
each substitutable word w, we generate the set of
substitution candidates s(w). For each candidate
wi ∈ {w}∪ s(w) we allow the original word to be
preserved with p(wi) = α, and share the remain-
ing 1−α proportional to the language model score
based on substituting wi into the original text. For
this, we use the pre-trained US English language
model from the CMU Sphinx Speech Recognition
toolkit.5 Finally, we sample from the probability
distribution {p(wi) : wi ∈ {w} ∪ s(w)} for each
substitutable word w to generate a semantically-
corrupted version of the original.

We experiment with two approaches to gen-
erating the substitution candidates. The first is
based on Princeton WordNet (“WN”: Miller et
al. (1990)), over all synsets that a given substi-
tutable word occurs in, using the NLTK API (Bird,
2006). The second is based on the “counter-
fitting” method of Mrkšić et al. (2016) (“CFIT”),
whereby word embeddings from WORD2VEC are
projected based on a supervised objective func-
tion which penalises similarity between antonym
pairs, and rewards similarity between synonym
pairs, as trained on 10k English news sentences
from WMT14 (Bojar et al., 2014).

Word Dropout As a standard approach to train-
ing robust models, we use word dropout (Srivas-
tava et al., 2014; Pham et al., 2014). Dropout can
be viewed as a method for zeroing out noise, and
is first-order equivalent to an `2 regularizer applied
after feature scaling (Wager et al., 2013).

5https://sourceforge.net/projects/
cmusphinx/

Method Example

Original The cat sat on the mat .
ERG On the mat sat the cat .
COMP The cat sat on � mat �
WN The

:::
kat sat on the

:::::::
flatness .

CFIT The
:::
pet

:::::
stood

::::
onto the mat .

Table 1: Examples of generated sentences across
four proposed methods. Modified words are
marked by “

:::::::::
underwave” and omitted words are de-

noted with a “�”.

Table 1 shows an example sentence and sample
corrupted outputs after applying each type of lin-
guistic noise. The ERG seldom changes words,
and instead tends to reorder the words based on
syntactic alternation. COMP performs like word
dropout in that it tends to remove tokens with low
semantic content and to generate complete sen-
tences. WN and CFIT both only modify the text at
the word level, based on near-synonyms and words
with similar semantic function, respectively.

3 Models and Training

We evaluate our methods on several sentence clas-
sification tasks, using a convolutional neural net-
work (“CNN”) model (Kim, 2014). Note that our
method corrupts the input directly, and is thus eas-
ily transferrable to other classes of models (e.g.,
other deep learning or linear models).

Convolutional Neural Network The CNN op-
erates at the sentence level by first embedding
each word using a lookup table which is stacked
into the sentence matrix ES. A 1d convolutional
layer is then applied to ES, which applies a se-
ries of filters over each window of t words, with
each filter employing a rectifier transform func-
tion. MaxPooling is applied over each set of fil-
ter outputs to result in a fixed-size sentence repre-
sentation.6 The sentence vector is fed into a final
Softmax layer to generate a probability distribu-
tion over classification labels.

The model is trained to minimise the cross-
entropy between the ground-truth and the model
prediction, using the Adam Optimizer (Kingma
and Ba, 2015) with learning rate 10−4 and a

6We use window widths of size t ∈ {3, 4, 5}, and 128 fil-
ters for each size. MaxPooling is applied to each of the three
sizes separately, and the resulting vectors are concatenated to
form the sentence representation.

23



batch size of 128. We initialise the embedding
with dimension m = 300 Google pre-trained
WORD2VEC word embeddings (Mikolov et al.,
2013). Words not in the pre-trained vocabulary are
initialized randomly using a uniform distribution
U([−0.25, 0.25)m).
Injecting Noise during Training Our proposed
method involves corrupting the training input with
adversarial noise of various kinds. All the meth-
ods are non-deterministic, involving random sam-
pling. They are applied afresh every epoch, such
that each time an instance is processed, it will
have a different input form.7 The two semantic
approaches (WN and CFIT) support configurable
noise rates in terms of the proportion of substi-
tutable words that are corrupted. Accordingly,
we experiment with two thresholds on the random
variable for substitution of each word: low (“lo”;
α = 0.5) and high (“hi”; α = 0). Besides the
above methods which employ a single type noise,
we experiment with a combination (COMB) of the
four different noise types (ERG + COMP + WNlo
+ CFITlo), by uniformly randomly choosing one
of the four methods for noise generation each time
we process a training instance.

Datasets We experiment on the following
datasets:
• MR: sentence polarity dataset from movie re-

views (Pang and Lee, 2008)8

• CR: customer review dataset (Hu and Liu,
2004)9

• Subj: subjectivity dataset (Pang and Lee,
2005)8

• SST: Stanford Sentiment Treebank, using
the 2-class configuration (Socher et al.,
2013)10

We evaluate using classification accuracy, based
on both in-domain evaluation11 and a cross-
domain setting, in which we evaluate a model
trained on MR and tested on CR, and vice versa.
This last setting characterises a realistic applica-

7Using a single application of noise is less effective, but
still yields improvements over baseline methods including
dropout.

8https://www.cs.cornell.edu/people/
pabo/movie-review-data/

9http://www.cs.uic.edu/˜liub/FBS/
sentiment-analysis.html

10http://nlp.stanford.edu/sentiment/
11Where there is no pre-defined training/test split for a

given dataset, we use 10-fold cross validation. See Kim
(2014) for more details on the datasets and evaluation set-
tings.

tion scenario, where robustness to vocabulary shift
and other differences in the input is paramount.

4 Experimental Results and Analysis

Table 2 presents the results of training with dif-
ferent sources of linguistic corruption in the in-
domain and cross-domain settings. In general, the
proposed methods perform better than the base-
line and dropout, and semantic noise using WN
achieves consistent improvements across all set-
tings. The COMB method uniformly outperforms
the other methods for all in-domain evaluations,
indicating that the improvements from training
with different types of noise are orthogonal. Note
that improvements are smaller on SST and MR
than CR and Subj for all methods. Almost ev-
ery method improves over word dropout, except
counter-fitting at a high noise level. Also surpris-
ing is the fact that dropout shows no improvement
over standard training, and is overall mildly detri-
mental.

Our intuition behind why WN consistently out-
performs the baseline methods and other single
sources of noise is it sometimes performs sim-
ilarity to dropout, in replacing common words
with rare ones, and sometimes substitutes frequent
words for frequent words, leading to better gen-
eralisation in the word embeddings. To test this
hypothesis, we computed nearest neighbours in
the word embedding space for both the baseline
method and the WN method. For example, the
top-3 nearest neighbours for superior in CR are
exceptional, excellent and unmatched for WN,
while for the baseline, they are inferior, excep-
tional and excellent. That is, similar to the intu-
ition behind counter-fitting, the methods appears
to learn to differentiate between synonyms and
antonyms, in a manner which is sensitised to the
target domain.

Although similar in function to WN, the
counter-fitting based method performs unexpect-
edly poorly. This appears to be a consequence of
the training of these embeddings, namely that the
corpus was much smaller than that used for the
WORD2VEC training, and consequently coverage
on our corpora was substantially lower, leading
to the approach making inappropriate substitutions
and not aiding model robustness.

Sentence compression was found to be highly
effective. To illustrate by example, the sentence
Player has a problem with dual-layer dvd’s such

24



Method
In domain Cross domain

MR CR Subj SST MR/CR CR/MR

baseline 80.4 82.6 92.4 84.5 67.0 67.2

dropout 80.1 82.4 92.6 84.5 67.7 67.4

ERG 80.0 82.8 92.9 84.4 68.1 67.3
COMP 79.5 83.1 93.2 84.3 68.1 67.5
WNlo 80.9 83.2 93.1 84.3 68.5 67.3
WNhi 81.2 83.8 92.9 84.6 67.9 67.5

CFITlo 79.8 82.7 92.6 84.1 68.9 67.3
CFIThi 76.2 78.9 91.0 80.3 67.4 64.2

COMB 81.4 84.3 93.6 84.8 68.4 67.4

Table 2: Accuracy (%) of the CNN, in four in-domain settings, and two cross-domain settings, with
word dropout (“dropout”), or linguistic corruption based on different sources of syntactic and semantic
corruption. The best result for each dataset is indicated in bold.

as Alias seasons 1 and season 2 is compressed
into has a problem with dual-layer dvd which pre-
serves the key information that we expect to be
useful for model learning. This allows the model
to better learn the components of the input that are
predictive of sentiment.

Syntactic paraphrasing (ERG) tends to primar-
ily corrupt the word order, with fewer lexical sub-
stitutions. Thus, the model is less prone to over-
fitting to local n-gram features, and focuses on
learning words and phrases that are genuinely pre-
dictive of sentiment.

5 Conclusions

In this paper, we present a training method that
corrupts training examples with linguistic noise,
in order to learn more robust models. Based on
evaluation over several sentiment analysis datasets
with convolutional neural networks, we show that
this method outperforms standard training and
dropout, both for in-domain and out-of-domain
application. Our approach has wide-spread po-
tential to also benefit other types of discriminative
model and in a range of other language processing
tasks.

Acknowledgments

We are grateful to the anonymous reviewers for
their helpful feedback and suggestions, and to Ned
Letcher for assistance in running the ERG. This
research was supported in part by the Australian
Research Council.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the International Conference on Learning Represen-
tations, San Diego, USA.

Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy social
media text, how diffrnt social media sources? In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing, pages 356–
364, Nagoya, Japan.

Steven Bird. 2006. NLTK: The natural language
toolkit. In Proceedings of the COLING/ACL 2006
Interactive Presentation Sessions, pages 69–72,
Sydney, Australia.

Zsolt Bitvai and Trevor Cohn. 2015. Non-linear text
regression with a deep convolutional neural network.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 2: Short Papers), pages
180–185, Beijing, China.

Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleš
Tamchyna. 2014. Findings of the 2014 Workshop
on Statistical Machine Translation. In Proceedings
of the Ninth Workshop on Statistical Machine Trans-
lation, pages 12–58, Baltimore, USA.

Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing, pages 740–750, Doha, Qatar.

25



James Clarke and Mirella Lapata. 2006. Models
for sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 377–384, Sydney, Australia.

Ann Copestake and Dan Flickinger. 2000. An
open source grammar development environment and
broad-coverage english grammar using HPSG. In
Proceedings of the Second International Conference
on Language Resources and Evaluation, Athens,
Greece.

Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl
Pollard. 2005. Minimal recursion semantics: An
introduction. Journal of Research on Language and
Computation, 3(2–3):281–332.

Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 359–369, Atlanta,
USA.

Katja Filippova, Enrique Alfonseca, A. Carlos Col-
menares, Lukasz Kaiser, and Oriol Vinyals. 2015.
Sentence compression by deletion with LSTMs. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
360–368, Lisbon, Portugal.

Rose Jenny Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 363–370, Ann Arbor, USA.

Ian J. Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2015. Explaining and harnessing ad-
versarial examples. In Proceedings of the Interna-
tional Conference on Learning Representations, San
Diego, USA.

Alan Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. 2013. Speech recognition with deep recur-
rent neural networks. In Proceedings of the IEEE
International Conference on Acoustics, Speech and
Signal Processing, pages 6645–6649, Vancouver,
Canada.

Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 368–378, Port-
land, USA.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168–177,
Seattle, USA.

Yulei Jiang, Richard M. Zur, Lorenzo L. Pesce, and
Karen Drukker. 2009. A study of the effect of
noise injection on the training of artificial neural net-
works. In International Joint Conference on Neural
Networks, pages 1428–1432, Atlanta, USA.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
655–665, Baltimore, USA.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1746–1751, Doha, Qatar.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference on Learning Repre-
sentations, San Diego, USA.

Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. In Proceedings of the 18th Annual Conference
on Artificial Intelligence, pages 703–710, Austin,
USA.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2012. ImageNet classification with deep con-
volutional neural networks. In Advances in Neu-
ral Information Processing Systems 25, pages 1097–
1105, Lake Tahoe, USA.

Yitong Li, Trevor Cohn, and Timothy Baldwin. 2016.
Learning robust representations of text. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 1979–1985,
Austin, USA.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119, Lake Tahoe, USA.

George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller.
1990. Introduction to WordNet: an on-line lexical
database. International Journal of Lexicography,
3(4):235–244.

Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thom-
son, Milica Gašić, Lina M. Rojas-Barahona, Pei-
Hao Su, David Vandyke, Tsung-Hsien Wen, and
Steve Young. 2016. Counter-fitting word vectors
to linguistic constraints. In Proceedings of the 2016
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 142–148, San Diego,
USA.

Andrew Y. Ng. 2004. Feature selection, L1 vs. L2 reg-
ularization, and rotational invariance. In Proceed-
ings of the Twenty-first International Conference on
Machine Learning, Banff, Canada.

26



Anh Nguyen, Jason Yosinski, and Jeff Clune. 2015.
Deep neural networks are easily fooled: High con-
fidence predictions for unrecognizable images. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 427–436,
Boston, USA.

Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and Thorsten
Brants. 2002. The LinGO Redwoods Tree-
bank: Motivation and preliminary applications. In
Proceedings of the 19th International Conference
on Computational Linguistics, pages 1253–1257,
Taipei, Taiwan.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 115–124, Ann Arbor,
USA.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1–135.

Ellie Pavlick and Chris Callison-Burch. 2016. Simple
PPDB: A paraphrase database for simplification. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 143–148, Berlin, Germany.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1532–1543, Doha, Qatar.

Vu Pham, Théodore Bluche, Christopher Kermorvant,
and Jérôme Louradour. 2014. Dropout improves
recurrent neural networks for handwriting recogni-
tion. In 14th International Conference on Frontiers
in Handwriting Recognition, pages 285–290, Crete,
Greece.

Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics,
pages 354–362, Ann Arbor, USA.

Noah A. Smith. 2012. Adversarial evaluation
for models of natural language. arXiv preprint
arXiv:1207.0245.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642, Seattle, USA.

Anders Søgaard. 2013. Part-of-speech tagging with
antagonistic adversaries. In Proceedings of the 51st

Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
640–644, Sofia, Bulgaria.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15:1929–1958.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. 2014. Intriguing properties of neu-
ral networks. In Proceedings of the International
Conference on Learning Representations, Banff,
Canada.

Stefan Wager, Sida Wang, and Percy S. Liang. 2013.
Dropout training as adaptive regularization. In Ad-
vances in Neural Information Processing Systems
26, pages 351–359, Lake Tahoe, USA.

27


