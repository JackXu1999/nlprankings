



















































Idiom Savant at Semeval-2017 Task 7: Detection and Interpretation of English Puns


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 103–108,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

Idiom Savant at Semeval-2017 Task 7: Detection and Interpretation of
English Puns

Samuel Doogan Aniruddha Ghosh Hanyang Chen Tony Veale
Department of Computer Science and Informatics

University College Dublin
Dublin, Ireland

{samuel.doogan, aniruddha.ghosh, hanyang.chen}@ucdconnect.ie
tony.veale@ucd.ie

Abstract

This paper describes our system, entitled
Idiom Savant, for the 7th Task of the Se-
meval 2017 workshop, “Detection and in-
terpretation of English Puns”. Our system
consists of two probabilistic models for
each type of puns using Google n-grams
and Word2Vec. Our system achieved f-
score of 0.84, 0.663, and 0.07 in ho-
mographic puns and 0.8439, 0.6631, and
0.0806 in heterographic puns in task 1,
task 2, and task 3 respectively.

1 Introduction

A pun is a form of wordplay, which is often pro-
filed by exploiting polysemy of a word or by
replacing a phonetically similar sounding word
for an intended humorous effect. From Shake-
speare’s works to modern advertisement catch-
phrases (Tanaka, 1992), puns have been widely
used as a humorous and rhetorical device. For
a polysemous word, the non-literal meaning is
addressed when contextual information has low
accordance with it’s primary or most prominent
meaning (Giora, 1997). A pun can be seen as a
democratic form of literal and non-literal meaning.
In using puns, the author alternates an idiomatic
expression to a certain extent or provides enough
context for a polysemous word to evoke non-literal
meaning without attenuating literal meaning com-
pletely (Giora, 2002).

Task 7 of the 2017 SemEval workshop (Miller
et al., 2017) involves three subtasks. The first sub-
task requires the system to classify a given context
into two binary categories: puns and non-puns.
The second subtask concerns itself with finding
the word producing the punning effect in a given
context. The third and final subtask involves an-
notating puns with the dual senses with which the

punning effect is being driven.
In a written context, puns are classified into 2

categories. Homographic puns shown in exam-
ple 1, exploits polysemy of the language by us-
ing a word or phrase which has multiple coher-
ent meanings given its context; And heterographic
puns shown in example 2, humorous effect is often
induced by adding incongruity by replacing a pho-
netically similar word which is semantically dis-
tant from the context.

(1) Tires are fixed for a flat rate.

(2) A dentist hates having a bad day at the
orifice.

The rest of the paper is organized as fol-
lows. Section 2 give a general description of
our approach. Section 3 and 4 illustrate the de-
tailed methodologies used for detecting and locat-
ing Heterographic and Homographic puns respec-
tively. In section 5, we provided an analysis of the
system along with experimental results and finally
section 6 contains some closing remarks and con-
clusion.

2 General Approach

We argue that the detection of heterographic puns
rests on two assumptions. Firstly, the word be-
ing used to introduce the punning effect is pho-
netically similar to the intended word, so that the
reader can infer the desired meaning behind the
pun. Secondly, the context in which the pun takes
place is a subversion of frequent or idiomatic lan-
guage, once again so that the inference appropri-
ately facilitated. This introduces two computa-
tional tasks - designing a model which ranks pairs
of words based on their phonetic similarity, and in-
troducing a means by which we can determine the
normativeness of the context in question. The sys-

103



tem is attempting to recreate how a human mind
might recognize a pun. Take this example:

(3) “Acupuncture is a jab well done”

It is immediately noticeable that this sentence is
not a normative use of language. However, we
can easily recognize the familiar idiom “a job well
done”, and it is easy to make this substitution due
to the phonetic overlap between the words “job”
and “jab”. Our system is therefore trying to mimic
two things: the detection of an infrequent (or even
semantically incoherent) use of language, and the
detection of the intended idiom by means of pho-
netic substitution. To model the detection of sub-
verted uses of idioms, we use the Google n-gram
corpus (Brants and Franz, 2006). We assume that
the normativeness of a context is represented by
the n-gram frequency provided by this corpus. The
system then replaces phonetically similar words in
the non-normative context in an attempt to pro-
duce an idiomatic use of language. We determine
an idiomatic use of language to be one that has an
adequately high frequency in the Google n-gram
corpus. We argue that if, by replacing a word
in an infrequent use of language with a phonet-
ically similar word, we arrive at a very frequent
use of language, we have derived an indicator for
the usage of puns. For example, the quadgram
“a jab well done” occurs 890 times in the cor-
pus. By replacing the word “jab” with “job”, the
new quadgram occurs 203575 times. This increase
in frequency suggests that a pun is taking place.
The system uses several methods to examine such
changes in frequency, and outputs a “score”, or the
estimated likelihood that a pun is being used. The
way in which these scores are computed is detailed
below.

Homographic puns are generally figurative in
nature. Due to identical spelling, interpretation of
literal and non-literal meaning is solely dependent
on the context information. Literal and non-literal
meaning of a polysemous word are referred by dif-
ferent slices of context, which is termed as “double
grounding” by Feyaerts and Brône (2002). Con-
sidering example 1, it is easily noticeable that two
coherent meanings of ‘flat’, ‘a deflated pneumatic
tire’ and ‘commercially inactive’, have been re-
ferred by ‘Tires’ and ‘rate’ respectively. Thus de-
tection of homographic puns involves establish-
ing links between concepts present in context with
meanings of polysemous word.

From the general description of different types
of puns, it is evident that detection of pun is com-
plex and challenging. To keep the complexity at
its minimum, Idiom Savant contains two distinct
models to handle homographic and heterographic
pun tasks.

3 Heterographic Puns

Idiom Savant calculates scores for all possible
ngram pairs for a given context. To generate pairs,
the system first separates the context into n-grams.
For each of these original n-grams, the corpus is
searched for n-grams that are at most one word
different. The pairs are then scored using the met-
ric described below. The scores for these pairs are
then used to tackle each subtask, which is covered
below. Since heterographic puns are fabricated by
replacing phonetically similar words, classifica-
tion and identification requires a phonetic knowl-
edge of the language. To obtain phonetic represen-
tation of a word, CMU pronouncing dictionary1

was used. We have ignored the lexical stresses in
the pronunciation, as experimentation showed that
coarser definitions led to better results. To mea-
sure the phonetic distance between a phoneme rep-
resentation of a pair of words, we have employed
three different strategies which use Levenshtein
distances. The first distance formula, dph, calcu-
lates Levenshtein distance between two words by
considering each CMU phoneme of a word as a
single unit. Take the pun word and intended word
from example 2:

dph({AO, F, AH, S}, {AO, R, AH, F, AH, S}) = 2

Our second strategy treats the phonetic repre-
sentation as a concatenated string and calculates
Levenshtein distance dphs.

dphs(“AOFAHS”, “AORAHFAHS”) = 3

With this metric, the distance reflects the simi-
larity between phonemes such as “AH” and “AA”,
which begin with the same vowel sounds. The fall-
back method for out-of-vocabulary words uses the
original Levenshtein string distance.

dch(“office”, “orifice”) = 2
1http://www.speech.cs.cmu.edu/cgi-bin/

cmudict

104



The system normalizes these distances with re-
spect to the length of the phonetic representation
of the target words to reduce the penalty caused
by word length. By converting distance measures
into similarity ratios, longer words remain candi-
dates for possible puns, even though Levenshtein
distances will be greater than the shorter counter-
parts. The system chooses the maximum positive
ratio from all possible phonetic representations.
If no positive ratio exists, the target word is dis-
carded as a possible candidate.

ratiof (w1, w2) =
min

w∈w1,w2
||w||f−df (w1,w2)
min

w∈w1,w2
||w||f

wheref ∈ {ph, phs, ch}

ratio = max(ratioph, ratiophs, ratioch)

We choose the maximum ratio in order to min-
imize the drawbacks inherent in each metric. The
assumption is that the maximum ratio between all
three methods is the most reflective of the real pho-
netic similarity between a pair of words. The final
score is calculated as the inverted ratio subtracted
from the difference between the ngram pair’s fre-
quency.

score = (freqngram′ − freqngram)− 1
ration

Deducting the original n-gram’s frequency from
the new frequency effectively ignores normal uses
of language which do not relate to pun language.
The value of the exponent introduces a trade off
between phonetic similarity and frequency. The
frequencies of certain n-grams are so high that if
n is too low, even words with very little phonetic
similarity will score high using this method. In our
experiments, an optimal value of 10 was found for
this trade off.

3.1 Binary Classification
Tto classify a context as a pun or non pun, Idiom
Savant finds the maximum score from all possi-
ble n-gram pairs. If the maximum score found ex-
ceeds a threshold value, the context is classified as
a pun.

Finding the correct threshold value to accurately
classify contexts is discussed below in the Experi-
ments and Results section.

3.2 Locating Pun Word
By maximizing the score when replacing all po-
tential lexical units, the system also produces a
candidate word. Whichever replacement word
used to produce the top n-gram pair is returned as
the candidate word. The system only examines the
last two ngrams. Those grams, the system anno-
tates the POS tag and only the content words —
nouns, verbs, adverbs and adjectives— are con-
sidered as candidate words. The system uses a fall
back by choosing the last word in the context when
no adequate substitution is found.

3.3 Annotating senses for pun meanings
Subtask 3 introduces an added difficulty with re-
gards to heterographic puns. The system needs to
correctly identify the two senses involved of pun,
which is based on the accuracy of selecting tar-
get words. The system produce a ranked list of
n-gram pairs using single word substitution. The
highest ranked pair then contains the new or re-
placed word with which we search for a sense
in WordNet (Fellbaum, 1998). For this particular
task, the pun word are already given, so the sys-
tem chooses only the n-grams which contain this
word, and only needs to replace this word in order
to produce pairs.

Once both words are found, we apply the se-
mantic similarity measure akin to the one used in
our systems approach to homographic puns de-
scribed in Section 4. Both the original and target
word is compared to a list of wordnet glosses cor-
responding to the senses available for each word.
Idiom Savant uses Word2Vec cosine similarity be-
tween the words and their sense glosses to choose
the best sense key.

3.4 Tom Swifties
“Tom Swifty” (Lessard and Levison, 1992) is one
type of pun often found in the test data set. An
example found is “ It’s March 14th, Tom said
piously”. Such puns frequently use adverbs to
introduce the contextual ties inherent in hetero-
graphic puns. Despite that, most of these adverbs
occurred in the test data set show little connec-
tion with their contexts, rather they are specifi-
cally used for ironic purpose. As such, our system
did not adequately recognize these instances, so
we designed a separate procedure for these cases.
To flag whether a pun might be of a Tom Swifty
type, the system uses a Part of Speech tagger from

105



NLTK (Bird, 2006) and also analyses the suffixes
of the last word in the context (for example, words
ending in “ly”).

With relation to tasks 1, an amalgamation of
this approach and the original is performed. If the
highest score does not exceed the threshold, we
check to see if the pun is of type Tom Swifty. If
this is the case, then we mark the context as a pun.
Task 2 operates similarly - if the pun is flagged as
a Tom Swifty, then the last adverb is returned as a
candidate. For task 3 however, we need to trans-
form the adverb into the intended word in order to
get the appropriate sense entry in WordNet.

To do so we build two prefix trees: one is a pho-
netic prefix tree based on CMU pronunciation dic-
tionary; the other is a string prefix tree, to cover
the exception cases where the adverb is not present
in the CMU. If the word is in the phonetic prefix
tree, the program returns all words which share at
least two common prefix phonemes. For example,
given the adverb “punctually”, the words “punc-
ture”, “punk”, “pun” and so on will be returned as
candidates. If the string prefix tree is used, the pro-
gram returns words which share at least the first
three characters found in th input word. For the
word “dogmatically”, “dogmatic”, “dogma”, and
“dog” will be returned as candidates. The list of
such candidates is then used to replace the ngrams
in which they occur, and the new ngram pairs are
ranked according to the metric described at the be-
ginning of 3. The highest scoring prefix is then
used to search the appropriate WordNet sense tags.

4 Homographic Puns

Since polysemous words have identical spelling
but different meanings, detecting homographic
puns is solely dependent on context information.
Following double grounding theory, if the ith word
of input sentence W = w1:n, has a higher possi-
bility to be the punning word, two senses of wi
should infer a higher similarity score with two dif-
ferent components in its context ci = w1:i−1,i+1:n.
In the baseline model we design, the pun potential
score of a word wi is computed as the sum of co-
sine similarities between the word wi and every
word in context wj ∈ ci, using distributed repre-
sentation Word2Vec (Mikolov et al., 2013). The
word with highest score is returned as the punning
word.

Furthermore, as additional context information,
wi were replaced with set of gloss information ex-

tracted from its different senses, noted as gi, ob-
tained from WordNet. While calculating similar-
ity between gi and ci, two different strategies were
employed. In the first strategy, the system com-
putes similarities between every combination of gi
and ci, and sum of similarity scores is the score for
wi. In the second strategy, similarity score were
calculated between gi and gj , the gloss of wj ∈ ci.
In most of the cases, pun words and their ground-
ing words in the context do not share the same
part-of-speech (POS) tags. In the latter strategy,
we added a POS damping factor, noted as pij of
0.2 if the POS tags of wi and wj are equal. Follow-
ing Optimal Innovation hypothesis, the similarity
of a punning word and its grounding word should
neither be too high or too low in order to evoke the
non-literal meaning. We applied following correc-
tion on computed similarities.

fws(x) =

{
0 x < 0.01
1− x x >= 0.01

In puns, punning words and grounding words in
context are often not adjacent. Thus the system
does not consider the adjacent words of the candi-
date word. The system also ignored stopwords of-
fered by NLTK. We noticed that words with high
frequency other than stopwords overshadow low
frequency words since every word with high fre-
quency poses certain similarity score with every
other phrases. Thus we added a frequency damp-
ing factor(fij) of 0.1 to the score for whose words
have frequencies more than 100 in Brown Cor-
pus (Francis and Kucera, 1979). The final scoring
function is shown as follows.

score(W, i) =
n∑

j=1

pijfij

l∑
k=1

q∑
m=1

fws(
gkgm
|gk||gm|)

n is the number of words in ci and l and q is num-
ber of senses of wi and wj . gk and gm are gloss of
the kth sense and mth sense of wi and wj respec-
tively.

For task 3, in order to obtain the sense keys
of intended meaning from Wordnet, we chose the
top two glosses of the pun word based on similar-
ity score between gloss and word in context. For
adding more context, instead of comparing only
with words in context, we performed similarity
measurement among the glosses.

For subtask 1, for each word we calculated sim-
ilarity with other words and we averaged the top

106



two similarity score. We have considered a word
as a pun if the average score is more than threshold
of 0.6, which we chose empirically after observing
a number of examples. For subtask 3, we chose
the top two senses of the word ranked by the gloss
similarity as candidate senses of punned word.

5 Experiment results and analysis

5.1 Heterographic Puns Processing

ID Method P R F

1
Infrequent Quadgram 0.90 0.71 0.79

Trigram Score 0.82 0.87 0.84

2
Last Word 0.55 0.55 0.55

BestQuadGramPairs 0.68 0.68 0.68

3
TopSenses 0.14 0.11 0.12
GlossSim 0.08 0.07 0.07

Table 1: The precision, recall, and F-score value
of heterographic pun subtasks

The experiment results for the heterographic
pun subtasks are shown in Table 5.1. For subtask
1, the baseline infrequent quadgram is created: if
a pun contains no infrequent quadgrams, which
have a frequency less than 150 in Ngram corpus,
then it is labeled as a non pun. The system uses
trigram in subtask 1 because it is computationally
feasible to search the ngram space, whilst still be-
ing representative of typical uses of language. We
set a balanced threshold value of −14 by observ-
ing the first 200 samples in the test set.

The high precision score indicates the underly-
ing mechanism behind such puns: a mutation of a
typical use of language needs to take place. How-
ever the recall for this baseline is poor. A large
portion of puns de facto use frequent language us-
ages as targets for linguistic perversion, which this
baseline method fails.

Our system outperforms the baseline about five
percentage of F-score. The largest factor regard-
ing improper classifications of our model is false
positives. Not all infrequent uses of language
are heterographic puns. Idiom Savant’s technique
would sometimes misread a context, modify an
infrequent trigram that was not the source of a
pun to produce a much more frequent trigram.
These false positives are the result of the enormous
amount of possible uses in the English language.
Infrequent yet “normal” trigrams are an important
caveat when using frequency based techniques
such as Idiom Savant. Hence we see the differ-

ence between our model and the simple baseline:
although the puns that were detected were very
precise, the baseline failed to detect more subtle
puns, where normal uses of language are still us-
ing phonetic translations to introduce ambiguity.

For subtask 2, Idiom Savant uses quadgrams to
produce the scores. This is possible because the
system employs a number of methods to reduce
the search space created when attempting to re-
place quadgrams. Firstly, the system won’t search
the Tom Swifty puns in ngrams corpus. Analysing
the first 200 samples in the test data, which is not
Tom Swifty puns, we found that roughly half all
pun words are the last words in the context. Us-
ing this method on the whole corpus produced the
LastWord baseline seen above. When expanding
that to quadgrams and thus enlarging the window,
an even greater ratio presents itself. Of the same
200 samples, three fourth of punning words are
present in the last quadgram. In the gold stan-
dard, ninety percent of pun words appear in the
last quadgram. We apply the same scoring tech-
nique as described above and achieved the per-
formance presented in the table. We find an in-
crease of 13% as compared to the last word base-
line across the board.

To create a baseline for subtask 3, we followed
the approach described in (Miller and Gurevych,
2015). and choose the top WordNet senses for
each word selected as pun word. As WordNet
ranks each sense with their associated frequency
of usage, the baseline simply selects the most fre-
quent sense for the pun word and replaced word
respectively. As the replaced word are produced
by the system, the possibility of error even with
the baseline approach is affected by the accuracy
of previous steps. When an incorrect word is pro-
duced, the sense key attached is by default incor-
rect and thus the precision, recall, and F scores
suffer. The baseline outperforms our system to
choose the best sense keys by approximately 6 per-
centage points. Our method involves Word2Vec is
insufficient for solving this subtask, which is evi-
dently much more difficult than the previous sub-
tasks.

5.2 Homographic Pun Processing
For homographic pun processing, we participated
in subtask 2 and 3. We calculated scores of sub-
task 1 on test data after task. For subtask 1, our
system achieves 0.84 F-score, which outperforms

107



the all positive baseline. For subtask 2, our sys-
tem achieves 0.66 F-score. We observed that our
system performed well on long sentences. How-
ever, for short sentences, most frequent word in
the sentence were selected as pun word. This may
be caused by lack of context.

Our system does not perform well on subtask 3
as it could not pick the apt sense intended in the
pun. We noticed that the system can not pinpoint
the apt senses whose glosses are not long enough.

Task Method P R F-score

Task 1
AllPositive 0.71 1.00 0.83
WordPairSim 0.73 0.98 0.84

Task 2
WordSim 0.57 0.54 0.55
WordGlossSim 0.66 0.66 0.66

Task 3 GlossSim 0.08 0.08 0.08

Table 2: The precision, recall, and F-score value
of homographic pun processing subtasks

6 Concluding Remarks

We introduced Idiom Savant, a computational sys-
tem that capable of classifying and analyzing het-
erographic and homographic puns. We show that
using n-grams in combination with the CMU dic-
tionary can accurately model heterographic pun.

There are however a number of drawbacks to
this approach. We hypothesize that using a larger
corpus would increase the performance of hetero-
grahic pun processing. And we may combine dif-
ferent length grams to search for these idiomatic
uses of language, which would more accurately
model how human recognizes heterographic puns.
Furthermore, the system has no means of checking
whether the candidate words offered up by Idiom
Savant are correlated to the rest of the context. Our
system suffers intensely for short sentences and
short gloss information, since Word2Vec doesn’t
offer context information.

References
Steven Bird. 2006. NLTK: The Natural Language

Toolkit. In Proceedings of the COLING/ACL 2006
Interactive Presentation Sessions. Association for
Computational Linguistics, Sydney, Australia, pages
69–72.

Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. Linguistic Data Consortium .

Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Massachusetts Institute of
Technology.

Kurt Feyaerts and Geert Brône. 2002. Humor through
double grounding: Structural interaction of optimal-
ity principles. Odense Working Papers in Language
and Communication (23):312–336.

W. Nelson Francis and Henry Kucera. 1979. Brown
corpus manual. Brown University .

Rachel Giora. 1997. Understanding figurative and
literal language: The graded salience hypothesis.
Cognitive Linguistics (includes Cognitive Linguistic
Bibliography) 8(3):183–206.

Rachel Giora. 2002. Optimal innovation and plea-
sure. In Stock, O., Strapparva, C. and A. Nijholt
(eds.) Processing of The April Fools Day Workshop
on Computational Humour. Citeseer, pages 11–28.

Greg Lessard and Michael Levison. 1992. Computa-
tional modelling of linguistic humour: Tom swifties.
In ALLC/ACH Joint Annual Conference, Oxford.
pages 175–178.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.

Tristan Miller and Iryna Gurevych. 2015. Automatic
disambiguation of english puns. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers). Association for Computa-
tional Linguistics, Beijing, China, pages 719–729.

Tristan Miller, Christian F. Hempelmann, and Iryna
Gurevych. 2017. SemEval-2017 Task 7: Detec-
tion and interpretation of English puns. In Proceed-
ings of the 11th International Workshop on Semantic
Evaluation (SemEval-2017).

Keiko Tanaka. 1992. The pun in advertising: A prag-
matic approach. Lingua 87(1):91 – 102.

108


