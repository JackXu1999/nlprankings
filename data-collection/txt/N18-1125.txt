



















































Target Foresight Based Attention for Neural Machine Translation


Proceedings of NAACL-HLT 2018, pages 1380–1390
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Target Foresight based Attention for Neural Machine
Translation∗

Xintong Li†, Lemao Liu‡, Zhaopeng Tu‡, Shuming Shi‡, Max Meng†

†The Chinese University of Hong Kong
{xtli, qhmeng}@ee.cuhk.edu.hk

‡Tencent AI Lab
{redmondliu, zptu, shumingshi}@tencent.com

Abstract

in neural machine translation, an attention
model is used to identify the aligned source
words for a target word （target foresight
word）in order to select translation con-
text, but it does not make use of any in-
formation of this target foresight word at
all. previous work proposed an approach
to improve the attention model by explic-
itly accessing this target foresight word
and demonstrated the substantial gains in
alignment task. however, this approach
is useless in machine translation task on
which the target foresight word is unavail-
able. in this paper, we propose a new
attention model enhanced by the implicit
information of target foresight word ori-
ented to both alignment and translation
tasks. empirical experiments on chinese-
to-english and japanese-to-english datasets
show that the proposed attention model
delivers significant improvements in terms
of both alignment error rate and bleu.

1 Introduction

Since neural machine translation (NMT) was
proposed (Bahdanau et al., 2014), it has
been attracted increasing interests in machine
translation community (Luong et al., 2015b;
Tu et al., 2016; Feng et al., 2016; Cohn
et al., 2016). NMT not only yields impressive
translation performance in practice, but also
has appealing model architecture in essence.
Compared with traditional statistical machine
translation (Koehn et al., 2003; Chiang, 2005),
one of advantages in NMT is that its archi-
tecture combines language model, translation
model and alignment between source and tar-
get words in a unified manner rather than a

∗Work done when X. Li interning at Tencent AI
Lab. L. Liu is the corresponding author.

fă guó shī yè rén shù zài dù huí shēng </S>
法国 失业 人数 再度 回升 </S>

French unemployment rate rises again </S>

(a) Baseline

fă guó shī yè rén shù zài dù huí shēng </S>
法国 失业 人数 再度 回升 </S>

French unemployment rises again </S>
JJ NN VBZ RB EOS

(b) TFA-NMT

French unemployment rises again </S>

(c) Reference

Figure 1: A running example to motivate the pro-
posed model. (a) The baseline obtains a transla-
tion error due to the incorrect attention. (b) With
the help of the target foresight information “VBZ”,
TFA-NMT is likely to figure out the exact transla-
tion as the reference in (c). The light font denotes
the target words to be translated in future. Both
dashed or solid arrowed lines denote the alignments
and solid one denotes the 1-best alignment.

pipeline manner, and it thereby has the poten-
tial to alleviate the issue of error propagation.

In NMT, the attention mechanism plays an
important role. It calculates the alignments of
a target word with respect to the source words
for translation context selection. Although the
source words are always available in inference,
the target word, called target foresight word, 1

1Note that the concept of foresight word in our
translation task is not exactly the same as the original
concept in alignment task (Peter et al., 2017). How-
ever, both of them share a common idea that foresight
word should be at a later time step, and thus we re-
spect the work in Peter et al. (2017) and maintain the
same concept for easier understanding.

1380



i.e. the first light color word in Figure 1(a), is
not known but to be translated at the next
time step. Therefore, this may lead to inade-
quate modeling for attention mechanism (Liu
et al., 2016a; Peter et al., 2017). Regarding to
this, Peter et al. (2017) explicitly feed this tar-
get word into the attention model, and demon-
strate the significant improvements in align-
ment accuracy. Unfortunately, this approach
relies on the premise that the target foresight
word is available in advance in its alignment
scenario, and thus it can not be used in the
translation scenario.

To address this issue, in this paper, we pro-
pose a target foresight based attention (TFA)
model oriented to both alignment and trans-
lation tasks. Its basic idea includes two steps:
it firstly designs an auxiliary mechanism to
predict some information for the target fore-
sight word which is helpful for alignment; and
then it feeds the predicted result into the at-
tention model for translation. For the sake
of efficiency, instead of predicting the target
foresight word with large vocabulary size, we
only predict its partial information, i.e. part-
of-speech tag, which is proved to be helpful for
word alignment (Liu et al., 2005). Figure 1(b)
shows the main idea of TFA based on NMT. In
order to remit the negative effects due to the
prediction errors, we feed the distribution of
the prediction result instead of the maximum
a posteriori result into the attention model. In
addition, since the target foresight words are
available during the training, we jointly learn
the prediction model for the target foresight
words and the translation model in a super-
vised manner.

This paper makes the following contribu-
tions:

• It proposes a novel TFA-NMT for neural
machine translation by using an auxiliary
mechanism to predict the target foresight
word which is subsequently used to en-
hance the attention model.

• It empirically shows that the proposed
TFA-NMT can lead to better align-
ment accuracy, and achieves signifi-
cant improvements on both Chinese-to-
English and Japanese-to-English transla-
tion tasks.

2 Background
Given a source sentence x = {x1, . . . , xm}
with length m and a target sentence y =
{y1, . . . , yn} with length n, neural machine
translation aims to model the conditional
probability P (y | x):

P (y | x) =
n∏

i=1

P (yi | y<i,x) , (1)

where y<i = {y1, . . . , yi−1} denotes a prefix of
y with length i− 1.

To achieve this, neural machine transla-
tion adopts recurrent neural network (RNN)
under the encoder-decoder framework (Bah-
danau et al., 2014). In encoding, an en-
coder reads the source sentence x into a se-
quence of representation vectors by a bidirec-
tional recurrent neural network. Suppose hi
denotes the representation vector for xi, and
let h = {h1, . . . , hm}. In decoding, a decoder
sequentially generates a target word according
to P (yi | y<i,x) by using another RNN.

In Eq.(1), the distribution P (yi | y<i,x) is
used to generate yi as follows:

P (yi | y<i,x) = softmax (ϕ (yi−1, si, ci)) ,
(2)

where ϕ represents a feedforward neural net-
work, ci is the context vector from h to infer
yi, and si denotes the hidden state at times-
tamp i via the decoding RNN represented by
f :

si = f (si−1, yi−1, ci) . (3)
Bahdanau et al. (2014) propose an atten-

tion model to define the context ci, inspired
by the alignment model in statistical machine
translation.

Given the last hidden state si−1 and the en-
coding vectors h, an attention model is based
on a distribution consisting of αij as follows:

αij =
exp (eij)∑m

k=1 exp (eik)
,

where eij is computed by a feedforward neural
network represented by a:

eij = a (si−1, hj) . (4)

The quantity αij denotes the possibility of tar-
get word yi aligns to the source word xj en-
coded by hj . According to αij , the context

1381



vector ci is defined as the weighted sum of h:

ci =

m∑

j=1

αijhj . (5)

In this way, when translating the target word
yi, the decoder will pay more attention to its
aligned source words with respect to the dis-
tribution αi = {αi1, · · · , αim}. Figure 2 shows
a slice of the entire architecture for NMT at
timestamp i.

sisi−1

ciαi

h

· · · · · ·

yiyi−1

x

Figure 2: One slice of the architecture of Neural
Machine Translation based on a generic attention.

Unfortunately, even though the entire trans-
lation y is available in training, during the in-
ference it is unknown in advance but to be
generated sequentially. Specifically, when cal-
culating αi, one can make use of the informa-
tion only from x and y<i but nothing from
yi. Therefore, it is difficult to certainly spec-
ify which source words should be aligned to
an unknown target word yi. This might lead
to the inadequacy of the attention model (Liu
et al., 2016a; Peter et al., 2017), as explained
in Figure 1(a).

3 Target Foresight Attention

In order to alleviate the issue of inadequate
modeling for attention in NMT, in this sec-
tion, we propose the target foresight attention
for NMT, which foresees some related infor-
mation of the unknown target foresight word
to improve its alignments regarding to source
words. The basic idea of the proposed atten-
tion model includes two steps as following:

• It firstly introduce a model to predict
some information of the target foresight
word. (§3.1)

• It then feeds the predicted result about
the foresight target word into the atten-
tion as an additional input. (§3.2)

Therefore, as shown in Figure 1(b), when
translating the third word, if the prediction
model shows it to be a “VBZ”, the attention
model is likely to align it to the verb words
such as “huí shēng” rather than “rén shù” in
the source side, and then the corrected word
“rises” will be translated.

3.1 Target Foresight Prediction
Ideally, it is possible to build a model to di-
rectly predict the target foresight word itself.
In practice, it will be inefficient due to its large
vocabulary size. As a result, we instead build a
model to predict the partial information of the
target foresight word, such as part-of-speech
(POS) tag or word cluster, which has limited
vocabulary size. In this paper, we use the POS
tag as the partial information of a target fore-
sight word because POS tag is helpful to word
alignment proved by Liu et al. (2005). Fur-
thermore, predicting a POS tag is easier than
a target foresight word, so the predicted re-
sult will be more reliable for the downstream
application on attention.

Suppose ui denotes a variable indicating the
POS tag of a target foresight word yi. Our aim
is to define a prediction model of ui prior to
calculate the attention probability. For sim-
plicity, this prediction model is generally rep-
resented as βi = P (ui | y<i,x). We consider
three variant prediction models in a coarse-to-
fine manner as follows.

3.1.1 Model 1
It is straightforward to define this prediction
model directly based on the hidden states of
the RNN in decoder by using a neural network.
Formally, one can use the following equation:
βi = P (ui | y<i,x) = softmax (ψ(yi−1, si−1)) ,

(6)
where ψ is implemented by a feedforward neu-
ral network. Note that Eq.(6) only depends
on the decoding RNN hidden state si−1 and it
is very simple to implementation. Figure 3(a)
shows its architecture.

3.1.2 Model 2
Unlike Eq.(6) relying on the same hidden si−1
as the decoder, we design a specialized RNN

1382



βisi−1

(a) model 1

titi−1

βi

· · · · · ·

yi−1

(b) model 2

titi−1

βi

c′i

· · · · · ·

yi−1

(c) model 3

Figure 3: The prediction coarse-to-fine models for target foresight information: (a) Model 1 using only
the decoding hidden state si−1. (b) Model 2 using a hidden state ti from a specialized RNN. (c) Models
using a hidden state from a specialized RNN enhanced by the representation vector c′i of x similar to
Eq.(5).

to provide a particular hidden state for pre-
diction of ui. This improved prediction model
is defined as follows:

βi = P (ui | y<i,x) = softmax (ψ(yi−1, ti)) ,
(7)

where ti is the hidden state of the special-
ized RNN defined by a GRU unit, i.e. ti =
g(ti−1, yi−1). This prediction model architec-
ture is shown in Figure 3(b).

3.1.3 Model 3
In model 2, the specialized RNN for ui only
cares about the target sentence y and ignores
the information from the source sentence x.
We define a fine-grained model by taking a
context vector c′i from x as an additional in-
put:

βi = P (ui | y<i,x) = softmax
(
ψ(yi−1, ti, c′i)

)
,

(8)
where c′i is a context vector extracted from x
in a way similar to ci in Eq.(5),2 and ti =
g(ti−1, yi−1, c′i) is the hidden state of the spe-
cialized RNN. The architecture of this model
is shown in Figure 3(c).

3.2 Feeding the Prediction Model
Suppose we have the prediction result P (ui |
y<i,x), then we consider to feed it into the at-
tention model. Firstly, it is natural to feed the
prediction into attention by using maximum a
posteriori (MAP) strategy:

eij = a(si−1, hj , zi), (9)
2In our preliminary experiments, we tried ci, but we

found c′i performs better.

where a is the function for attention similar
to Eq.(4) but includes an additional input zi,
which is the MAP result of P (ui | y<i,x):

zi = z
(

argmax
ui

P (ui | y<i,x)
)
, (10)

where z denotes the embeddings of the POS
tags of target foresight words, and z(ui) re-
turns the embedding of a particular POS tag
ui.

Note that in Eq.(10) the accuracy of P (ui |
y<i,x) is important to the attention model.
For example, suppose at timestamp i, the
ground-truth POS tag is “NN”, but one has
P (ui = NN | y<i,x) = 0.4 and P (ui = VV |
y<i,x) = 0.41. In this case, the prediction
model selects “VV” as the POS tag of the
target foresight word and ignores the ground-
truth tag “NN”. Then the attention model
takes this error signal and may align the target
foresight word to a verb word. Subsequently,
this might lead to a translation error.

Therefore, we propose another method to
integrate the expected embedding of ui ac-
cording to P (ui | y<i,x) into attention as fol-
lows:

zi =
∑

ui

z(ui)P (ui | y<i,x) . (11)

In this way, zi can take into account all pos-
sible POS tags ui including the ground-truth
result.

Until now, we can obtain the entire archi-
tecture of the proposed target foresight at-
tention based NMT (TFA-NMT), as shown in
Figure 4. Comparing Figure 4 with Figure 2,
the only difference is the variable zi, which is

1383



sisi−1

ciαizi

h

· · · · · ·

yiyi−1

xz

βi

Figure 4: Neural machine translation with target
Foresight attention. βi is derived from Figure 3, zi
is from Eq.(10-11), and other nodes are similar to
ones in Figure 2.

obtained from Eq.(10-11) and the prediction
model as shown in Figure 3.

Note that the proposed TFA-NMT models
the target foresight word, which is a future
word regarding to the current time step, to
conduct attention calculation. In this sense, it
employs the idea of modeling future and thus
resembles to the work in (Zheng et al., 2017).
The main difference is that TFA-NMT models
the future from the target side whereas Zheng
et al. (2017) models the future from the source
side. In addition, Weng et al. (2017) imposes
a regularization term by using future words
during training. Unlike our approach, their
approach does not use future words during the
inference because these words are unavailable.
Anyway, it is possible to put both their ap-
proach and our approach together for further
improvements.

3.3 Learning and Inference
Suppose a set of training data is denoted by{⟨

xk,yk,uk
⟩

| k = 1, · · · ,K
}

. Here xk, yk
and uk denotes a source sentence, a target sen-
tence and a POS tag sequence of yk, respec-
tively. Then one can jointly train both the
translation model for yk and the prediction
model for uk by minimizing the loss function:

ℓ = −
∑

k

∑

i

(
logP (yki | yk<i,xk)+

λ logP (uki | yk<i,xk)
)
, (12)

where P (yki | yk<i,xk) is the translation model
similar to Eq.(2) with target foresight atten-
tion, and P (uki | yk<i,xk) is the target fore-
sight prediction model as defined in Eq.(6-8),

respectively. λ ≥ 0 is a hyper-parameter that
balances the preference between the trans-
lation model and target foresight prediction
model.

According to the training objective, the pro-
posed TFA-NMT resembles to the multi-task
learning, since it jointly learns two tasks simi-
lar to (Evgeniou and Pontil, 2004; Luong et al.,
2015a). The difference of our approach is ob-
viously: in this work the prediction result of
one model is integrated into the other model,
while in their works, two models only share
some common hidden states.

In inference, we implement two different de-
coding methods according two different ways
to integrate the foresight prediction model into
attention as described in §3.2. For the MAP
feeding style, we optimize ui according to the
loss function in Eq.(12) by beam search be-
sides optimizing yi. However, for the expec-
tation feeding style, we maintain the standard
beam search algorithm only regarding to the
translation model, i.e. by setting λ = 0.

4 Experiments

We conduct experiments on Chinese-to-
English and Japanese-to-English translation
tasks. The specific analyses are based on
Chinese-to-English task, and the generaliza-
tion ability is shown by Japanese-to-English
task. Case-insensitive 4-gram BLEU is used
to evaluate translation quality, and the multi-
bleu.perl is adopted as its implementation.

4.1 Setup
Data The training data for Chinese-to-
English task consists of 1.8M sentence pairs
from NIST2008 Open Machine Campaign,
with 40.1M Chinese words and 48.3M En-
glish words respectively. The development set
is chosen as NIST2002 (878 sentences) and
the test sets are NIST2005 (1082 sentences),
NIST2006 (1664 sentences), and NIST2008
(1357 sentences).

For Japanese-to-English translation, we
adopt the data sets from NTCIR-9 patent
translation task (Goto et al., 2013). The
training data consists of 2.0M sentence pairs
with 53.4M Japanese words and 49.3M English
words, the development and test sets respec-
tively contain 2000 sentences with a single ref-

1384



Model # Para. Speed Performance
Train Decode BLEU FPA

Nematus 105M 2858.8 86.6 38.65 –
+2-Layer +6M 2522.5 84.1 38.57 –
+Model1 +2M 1844.9 72.0 38.83 69.03
+Model2 +12M 1666.1 70.1 39.26 69.95
+Model3 +27M 1485.2 59.1 40.63 71.91

Table 1: Speeds and performances of the proposed models. “Speed” is measured in words/second for
both training and decoding, and performances are measured in terms of BLEU scores (“BLEU”) and
foresight prediction accuracy (“FPA”) on the development set. Higher BLEU and FPA scores denote
better performance.

erence, following (Goto et al., 2013; Liu et al.,
2016b) for further comparison.

Implementation We compare the proposed
models with two strong baselines from SMT
and NMT:

• Moses (Koehn et al., 2007): an open
source phrased based translation system
with default configuration.

• Nematus (Sennrich et al., 2017): an
generic attention based NMT.

We implement the proposed models on top of
Nematus. We use Stanford Log-linear Part-
Of-Speech Tagger (Toutanova et al., 2003)
to produce POS tags for the English side.
For both Chinese-to-English and Japanese-to-
English tasks, we limit the vocabularies to the
most frequent 30K words for both sides. All
the out-of-vocabulary words are mapped to a
spacial token “UNK”. Only the sentences of
length up to 50 words are used in training,
with 80 sentences in a batch. The dimen-
sion of word embedding is 620. The dimen-
sions of both feed forward NN and RNN hid-
den layer are 1000. The beam size for decod-
ing is 12, and the cost function is optimized
by Adadelta with hyper-parameters suggested
by Zeiler (2012). Particularly for TFA-NMT,
the foresight embedding is also 620, and the
hyper-parameter λ is 1.

4.2 Impact of Components
We conduct analyses on Chinese-to-English
translation task, to investigate the impact of
the added components and to figure out their
best configuration for further testing in the
next subsection.

4.2.1 Model Architectures
Table 1 lists the speeds and performances of
the proposed models. Clearly the proposed
approach improves the translation quality in
all cases, although there are still considerable
differences among the proposed variants.

Model Complexity The proposed models
introduce a few parameters to the NMT base-
line system Nematus, which has 105M pa-
rameters. The most complex model (i.e.,
Model3) introduces 27M new parameters,
which are small compared with the baseline
model. As seen, the proposed models signifi-
cantly slows down the training speed, which
we attribute to the new softmax operation
over the foresight tags and more gradient op-
erations associated with the new training ob-
jective, i.e., Eq.(12). For decoding, the most
complex model reduces speed by around 30%,
which is the cost of the proposed approach for
improving translation quality.

Performance We measure the performance
with BLEU and the result is shown in Ta-
ble 1. Model1 marginally improves perfor-
mance by guiding the decoder states to em-
bed information for predicting foresight tags.
Model2 achieves further improvement by in-
troducing a new specific hidden layer to ex-
plicitly separate the predict function from the
decoder states. Model3 achieves the best
performance by adopting an independent at-
tention model to attend corresponding source
parts for foresight prediction, which may not
be the same as the attended source parts for
translation. We conduct the significant test
using Kevin Gimpel’s toolkit (Clark et al.,
2011). We found that Model1 is not signif-

1385



Type Perc. FPA AER
Ours Base Ours

Noun 30.13% 77.49 28.97 26.50
Verb 12.39% 71.94 37.06 33.93
Adj. 9.43% 55.99 34.67 31.86

Prep. 14.66% 79.40 84.04 76.95
Dete. 10.08% 72.06 80.15 76.51

Punc. 8.01% 74.89 91.74 66.51
Others 15.30% 81.22 53.64 39.11

All 100% 74.87 49.67 42.56

Table 2: Performances on syntactic categories.
“Base” denotes “Nematus”, and Ours denotes
the proposed model.

icantly better than baseline, but Model2 is
significantly better with p<0.05 and Model3
is significantly better with p<0.01. Given that
simply introducing an additional layer (“+2-
Layer”) does not produce any improvement
on this data, we believe the gain of our model
is not only from the more introduced param-
eters. Besides, we augment the word em-
bedding by concatenating the POS tag em-
bedding, proposed by (Sennrich and Haddow,
2016), the BLEU is 38.96, which indicating the
improvement of our model is not only from
the POS tagging. In order to further validate
the improvements of variant proposed mod-
els, we evaluate the foresight prediction accu-
racy (FPA) for three proposed prediction mod-
els. We found that the fine-grained Model3
achieves the best FPA, indicating a good es-
timated foresight is very important to obtain
the gains in terms of BLEU.

4.2.2 Analysis on Syntactic Categories
In this experiment, we investigate which cat-
egory of generated words benefit most from
the proposed approach in terms of alignments
measured by alignment error rate (AER)
(Och, 2003). We carry out experiments on the
evaluation dataset from (Liu and Sun, 2015),
which contains 900 manually aligned Chinese-
English sentence pairs. Following (Luong
et al., 2015b), we force-decode both the bilin-
gual sentences including source and reference
sentences to obtain the attention matrices,
and then we extract one-to-one alignments by
picking up the source word with the high-
est alignment confidence as the hard align-

Train (λ) Decode BLEU ▽
1 Exp 40.63 –
0 Exp 39.36 -1.27
1 Map 40.34 -0.29

Table 3: Effect of foresight supervision signal in
training (i.e., λ) and foresight representations in
decoding: Exp for expectation and Map for max-
imum a posteriori.

ment. As shown in Table 2, the AER improve-
ments are modest for content words such as
Noun, Verb, and adjective (“Adj.”) words; but
there are substantial improvements for func-
tion words such as preposition words (“Prep.”)
and punctuations (“Punc.”).

The reason can be explained as follows. The
content words are easy to align with AER un-
der 38 as shown in Table 2, and thus it is
more difficult to gain over the BASE. On the
other hand, as depicted in Table 2, function
words are inherently more difficult than con-
tent words. These findings satisfy the linguis-
tic intuition: content words tend to be less in-
volved in multiple potential correspondences
than function words, and function words tend
to be attached to content words, as pointed
out by Pianta and Bentivogli (2004). Fortu-
nately, TFA-NMT can predict the POS tag
for target foresight word with high confidence
and thus it can improve the alignment qual-
ity by using of POS tags, which is useful for
alignment task (Liu et al., 2005).

It is surprising that the AER for “Prep.”,
“Det.” and “Punc.” is relatively low especially
for Base. The main reason can be explained
from the quantities yi−1, si, and ci in Eq.(2) as
follows. These highly frequent function words
are usually easy to be translated by using the
history information from yi−1 and si even if
ci is not confident enough. For example, it is
relatively easy to guess the “comma” by us-
ing the history words in language model task,
where there are no bilingual information at all.
Therefore, during the training, the model tries
to adjust the parameters for highly frequent
words from yi−1 and si while neglecting the
attention model.

4.2.3 Foresight Strategies
Table 3 shows the performances of different
foresight strategies in both training and de-

1386



coding. Without an explicit objective to guide
the training of foresight prediction model (i.e.,
λ = 0), the performance decreases by 1.27
BLEU points. When feeding the best fore-
sight predicted result to the attention model
(i.e., Map), the performance decreases by 0.29
BLEU points. We attribute this to the prop-
agation of prediction errors, which can be al-
leviated by using a weighted representation of
all predicted results (i.e., Exp).

In the following experiments, we use “λ = 1
and Exp” as the default setting for the final
system TFA-NMT.

4.3 Main Results
Chinese-to-English Task Table 4 shows
the translation performances for the Chinese-
to-English translation task. As seen, the pro-
posed approach significantly outperforms the
baseline system (i.e., Nematus) in all cases,
demonstrating the effectiveness and university
of our model.

Japanese-to-English Task Table 5 shows
the translation quality of the NMT baseline
and our TFA-NMT on Japanese-to-English
task. From the table, we can see that our
model still achieves a significant improvement
of 1.22 and 1.31 BLEU points on the devel-
opment and test set, respectively. This shows
that the proposed approach works well across
different language pairs.

5 Related Work

Attention model becomes a standard compo-
nent for many applications due to its ability of
dynamically selecting the informative context
from sequential representations. For example,
Xu et al. (2015) propose an attention based
neural network for image caption task and ad-
vance the state-of-the-art results; Yin et al.
(2015) put the attention structure between a
pair of convolution networks for answer se-
lection, paraphrase identification and textual
entailment tasks. In the context of machine
translation, the idea of attention based neu-
ral networks has been pioneered by Bahdanau
et al. (2014); Luong et al. (2015b) and achieved
impressive results over the traditional statis-
tical machine translation. Since then many
research works have been devoted to improve

the neural machine translation by enhancing
attention models.

Tu et al. (2016) design a coverage vector
for the translation history and then integrates
it into the attention model. Similarly, Meng
et al. (2016) maintain a tag vector to keep
track of the attention history and Sankaran
et al. (2016) memorize historical alignments
and accumulate them as temporal memory to
improve the attention model. In addition,
Zhang et al. (2017) improve the attention with
a gated operator for encoding states and a
decoding state, and and Dutil et al. (2017)
enhance attention through a planning mecha-
nism. Furthermore, Feng et al. (2016) adopt a
recurrent structure for attention to take long-
term dependencies into account, Zhou et al.
(2017) propose a look-ahead attention by addi-
tionally modeling the translation history, and
Cohn et al. (2016) incorporate structural bi-
ases into attention models. Recently Chen
et al. (2017) introduce the syntactic knowledge
into attention models. These works are essen-
tially similar to the propose approach, since
we introduce auxiliary information from a tar-
get foresight word into the attention model.
However, there is a significant difference be-
tween our approach and their approaches. Our
auxiliary information biases to the word to be
translated at next timestep while theirs biases
to the information available so far at the cur-
rent timestep, and thereby our approach is or-
thogonal to theirs.

The works mentioned above improve the at-
tention models by access auxiliary informa-
tion, and thus they modify the structure of at-
tention models in both inference and learning.
In contrast, Mi et al. (2016); Liu et al. (2016b);
Chen et al. (2016) maintain the structure of
the attention models in inference but utilize
some external signals to supervise the outputs
of attention models during the learning. They
improve the generalization abilities of atten-
tion models by use of the external aligners as
the signals, which typically yield alignment re-
sults accurate enough to guide the learning of
attention.

6 Conclusion

It has been argued that the traditional atten-
tion model in neural machine translation suf-

1387



System Model Dev MT05 MT06 MT08 Ave.

(Liu et al., 2016b) Moses – 35.4 33.7 25.0 31.37
NMT-J – 36.8 36.9 28.5 34.07

(Liu et al., 2016a) SA-NMT 40.0 37.8 37.6 29.9 35.10

This work Nematus 38.65 36.32 36.10 28.24 33.55
TFA-NMT 40.63 37.70 38.01 30.12 35.28

Table 4: Evaluation of translation performance on Chinese-to-English task.

System Model Dev Test

(Liu et al., 2016b)
Moses 28.6 30.2
NMT-J 33.0 34.1

This work Nematus 33.92 35.01TFA-Nmt 35.14 36.32

Table 5: Evaluation of translation performance on
Japanese-to-English task.

fers from model inadequacy due to the lack
of information from the target foresight word
(Peter et al., 2017; Liu et al., 2016a). To ad-
dress this issue, this paper proposes a new at-
tention model, which can serve for both align-
ment and translation tasks, by implicitly mak-
ing use of the target foresight word. Em-
pirical experiments on Chinese-to-English and
Japanese-to-English tasks demonstrate that
the proposed attention based NMT delivers
substantial gains in terms of both BLEU and
AER scores.

In future work, it is promising to exploit
other target foresight information such as word
cluster besides the POS tags in this paper, and
it is also interesting to apply this idea on top
of other attention models such as the local at-
tention in Luong et al. (2015b).

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua

Bengio. 2014. Neural machine translation by
jointly learning to align and translate. arXiv
preprint arXiv:1409.0473 .

Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro
Sumita, and Tiejun Zhao. 2017. Syntax-directed
attention for neural machine translation. arXiv
preprint arXiv:1711.04231 .

Wenhu Chen, Evgeny Matusov, Shahram Khadivi,
and Jan-Thorsten Peter. 2016. Guided align-
ment training for topic-aware neural machine
translation. arXiv preprint arXiv:1607.01628 .

David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In
Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics. Associ-
ation for Computational Linguistics, pages 263–
270.

Jonathan H Clark, Chris Dyer, Alon Lavie, and
Noah A Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Control-
ling for optimizer instability. In Proceedings of
the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies: short papers-Volume 2. Associa-
tion for Computational Linguistics, pages 176–
181.

Trevor Cohn, Cong Duy Vu Hoang, Ekate-
rina Vymolova, Kaisheng Yao, Chris Dyer,
and Gholamreza Haffari. 2016. Incorporat-
ing structural alignment biases into an atten-
tional neural translation model. arXiv preprint
arXiv:1601.01085 .

Francis Dutil, Caglar Gulcehre, Adam Trischler,
and Yoshua Bengio. 2017. Plan, attend, gener-
ate: Planning for sequence-to-sequence models.
arXiv preprint arXiv:1711.10462 .

Theodoros Evgeniou and Massimiliano Pontil.
2004. Regularized multi–task learning. In Pro-
ceedings of the tenth ACM SIGKDD interna-
tional conference on Knowledge discovery and
data mining. ACM, pages 109–117.

Shi Feng, Shujie Liu, Nan Yang, Mu Li, Ming
Zhou, and Kenny Q Zhu. 2016. Improving at-
tention modeling with implicit distortion and
fertility for machine translation. In COLING.
pages 3082–3092.

Isao Goto, Ka-Po Chow, Bin Lu, Eiichiro Sumita,
and Benjamin K Tsou. 2013. Overview of the
patent machine translation task at the ntcir-10
workshop. In NTCIR.

Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, et al. 2007. Moses: Open
source toolkit for statistical machine transla-
tion. In Proceedings of the 45th annual meeting

1388



of the ACL on interactive poster and demon-
stration sessions. Association for Computational
Linguistics, pages 177–180.

Philipp Koehn, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based transla-
tion. In Proceedings of the 2003 Conference
of the North American Chapter of the Associ-
ation for Computational Linguistics on Human
Language Technology-Volume 1. Association for
Computational Linguistics, pages 48–54.

Lemao Liu, Masao Utiyama, Andrew Finch, and
Eiichiro Sumita. 2016a. Neural machine trans-
lation with supervised attention. arXiv preprint
arXiv:1609.04186 .

Lemao Liu, Masao Utiyama, Andrew M Finch, and
Eiichiro Sumita. 2016b. Agreement on target-
bidirectional neural machine translation. In
HLT-NAACL. pages 411–416.

Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceed-
ings of the 43rd Annual Meeting on Association
for Computational Linguistics. Association for
Computational Linguistics, pages 459–466.

Yang Liu and Maosong Sun. 2015. Contrastive un-
supervised word alignment with non-local fea-
tures. In AAAI . pages 2295–2301.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever,
Oriol Vinyals, and Lukasz Kaiser. 2015a. Multi-
task sequence to sequence learning. arXiv
preprint arXiv:1511.06114 .

Minh-Thang Luong, Hieu Pham, and Christo-
pher D Manning. 2015b. Effective approaches
to attention-based neural machine translation.
arXiv preprint arXiv:1508.04025 .

Fandong Meng, Zhengdong Lu, Hang Li, and
Qun Liu. 2016. Interactive attention for
neural machine translation. arXiv preprint
arXiv:1610.05011 .

Haitao Mi, Zhiguo Wang, and Abe Ittycheriah.
2016. Supervised attentions for neural machine
translation. arXiv preprint arXiv:1608.00112 .

Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics-Volume
1. Association for Computational Linguistics,
pages 160–167.

Jan-Thorsten Peter, Arne Nix, and Hermann Ney.
2017. Generating alignments using target fore-
sight in attention-based neural machine transla-
tion. The Prague Bulletin of Mathematical Lin-
guistics 108(1):27–36.

Emanuele Pianta and Luisa Bentivogli. 2004.
Knowledge intensive word alignment with
knowa. In Proceedings of the 20th international
conference on Computational Linguistics. As-
sociation for Computational Linguistics, page
1086.

Baskaran Sankaran, Haitao Mi, Yaser Al-Onaizan,
and Abe Ittycheriah. 2016. Temporal attention
model for neural machine translation. arXiv
preprint arXiv:1608.02927 .

Rico Sennrich, Orhan Firat, Kyunghyun Cho,
Alexandra Birch, Barry Haddow, Julian
Hitschler, Marcin Junczys-Dowmunt, Samuel
Läubli, Antonio Valerio Miceli Barone, Jozef
Mokry, et al. 2017. Nematus: a toolkit for
neural machine translation. arXiv preprint
arXiv:1703.04357 .

Rico Sennrich and Barry Haddow. 2016. Linguistic
input features improve neural machine transla-
tion. arXiv preprint arXiv:1606.02892 .

Kristina Toutanova, Dan Klein, Christopher D
Manning, and Yoram Singer. 2003. Feature-rich
part-of-speech tagging with a cyclic dependency
network. In Proceedings of the 2003 Conference
of the North American Chapter of the Associ-
ation for Computational Linguistics on Human
Language Technology-Volume 1. Association for
Computational Linguistics, pages 173–180.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua
Liu, and Hang Li. 2016. Modeling coverage
for neural machine translation. arXiv preprint
arXiv:1601.04811 .

Rongxiang Weng, Shujian Huang, Zaixiang Zheng,
XIN-YU DAI, and Jiajun CHEN. 2017. Neural
machine translation with word predictions. In
Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing. As-
sociation for Computational Linguistics, Copen-
hagen, Denmark, pages 136–145.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun
Cho, Aaron Courville, Ruslan Salakhudinov,
Rich Zemel, and Yoshua Bengio. 2015. Show, at-
tend and tell: Neural image caption generation
with visual attention. In International Confer-
ence on Machine Learning. pages 2048–2057.

Wenpeng Yin, Hinrich Schütze, Bing Xiang, and
Bowen Zhou. 2015. Abcnn: Attention-based
convolutional neural network for modeling sen-
tence pairs. arXiv preprint arXiv:1512.05193 .

Matthew D Zeiler. 2012. Adadelta: an adap-
tive learning rate method. arXiv preprint
arXiv:1212.5701 .

Biao Zhang, Deyi Xiong, and Jinsong Su. 2017.
A gru-gated attention model for neural machine
translation. arXiv preprint arXiv:1704.08430 .

1389



Zaixiang Zheng, Hao Zhou, Shujian Huang, Lili
Mou, Xinyu Dai, Jiajun Chen, and Zhaopeng
Tu. 2017. Modeling past and future for
neural machine translation. arXiv preprint
arXiv:1711.09502 .

Long Zhou, Jiajun Zhang, and Chengqing Zong.
2017. Look-ahead attention for generation in
neural machine translation. arXiv preprint
arXiv:1708.09217 .

1390


