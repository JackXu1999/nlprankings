



















































Adaptation of Hierarchical Structured Models for Speech Act Recognition in Asynchronous Conversation


Proceedings of NAACL-HLT 2019, pages 1326–1336
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1326

Adaptation of Hierarchical Structured Models for Speech Act Recognition
in Asynchronous Conversation

Tasnim Mohiuddin1∗ and Thanh-Tung Nguyen1? and Shafiq Joty1,2?
?Nanyang Technological University, Singapore

†Salesforce Research Asia, Singapore
{mohi0004@e.,NG0155NG@e.,srjoty@}ntu.edu.sg

Abstract

We address the problem of speech act recog-
nition (SAR) in asynchronous conversations
(forums, emails). Unlike synchronous conver-
sations (e.g., meetings, phone), asynchronous
domains lack large labeled datasets to train
an effective SAR model. In this paper, we
propose methods to effectively leverage abun-
dant unlabeled conversational data and the
available labeled data from synchronous do-
mains. We carry out our research in three main
steps. First, we introduce a neural architec-
ture based on hierarchical LSTMs and condi-
tional random fields (CRF) for SAR, and show
that our method outperforms existing methods
when trained on in-domain data only. Second,
we improve our initial SAR models by semi-
supervised learning in the form of pretrained
word embeddings learned from a large unla-
beled conversational corpus. Finally, we em-
ploy adversarial training to improve the results
further by leveraging the labeled data from
synchronous domains and by explicitly mod-
eling the distributional shift in two domains.

1 Introduction

With the ever-increasing popularity of Internet
and mobile technologies, communication media
like emails and forums have become an inte-
gral part of people’s daily life where they discuss
events, issues and experiences. Participants inter-
act with each other asynchronously in these me-
dia by writing at different times, generating a type
of conversational discourse that is different from
synchronous conversations such as meeting and
phone conversations (Louis and Cohen, 2015). In
the course of the interactions, the participants per-
form certain communicative acts like asking ques-
tions, requesting information, or suggesting some-
thing, which are known as speech acts (Austin,

∗All authors contibuted equally.

C1:hoping to do the XinJiang Tibet Highway. [Statement]
Has anyone done it? [Question]
Am hoping to hire a 4-wheel drive. [Statement]
I know the roads are bad and I would need an experienced
driver and guide - any recommendations? [Question]

C2:I never done this routine,however i been to Xinjiang
twice,in my opinion the local people not friendly, not safe
to do this. [Response]
I still have relative stay in Xinjiang, however don’t know
what they can offer for help... [Response]

C3:I’m not sure if travelling overland from Xinjiang to Tibet
is officially legal yet. [Response]
You might want to post your question on the North-
East Asia branch of Lonely Planet’s ThornTree forum for
more (useful) answers. [Suggestion]

C4:a frend and i are trying this route as well, we will likely
be in urumuqi and northern part of xinjiang from 8th apr
to end apr; looking at doing the xin jiang tibet highway
from end apr. (truncated) [Statement]
contact me at [email] if you want to hook up for possible
transport sharing [Suggestion]
cheers. [Polite]

Figure 1: Example of speech acts in a forum thread.

1962). For example, consider the forum conversa-
tion in Figure 1. The participant who posted the
initial comment C1, describes his situation and
asks a couple of questions. Other participants re-
spond to the initial post with more information and
provide suggestions. In this process, the partici-
pants get into a conversation by taking turns, each
of which consists of one or more speech acts.

Speech act recognition (SAR) is an impor-
tant step towards deep conversational analysis,
and can benefit many downstream applications.
Availability of large labeled datasets such as the
Switchboard-DAMSL (SWBD) (Jurafsky et al.,
1997) and the Meeting Recorder Dialog Act
(MRDA) (Dhillon et al., 2004) corpora has fos-
tered research in data-driven SAR methods in syn-
chronous domains. However, such large corpora



1327

are not available in the asynchronous domains,
and many of the existing (small-sized) ones use
task-specific tagsets as opposed to a standard one.
The unavailability of large annotated datasets with
standard tagsets is one of the main reasons for
SAR not getting much attention in asynchronous
domains, and it is often quite expensive to anno-
tate such datasets for each domain of interest.

SAR methods proposed before the neural
‘tsunami’, e.g., (Qadir and Riloff, 2011; Jeong
et al., 2009; Tavafi et al., 2013), used mostly
bag-of-ngram representation (e.g., unigram, bi-
gram) of a sentence, and most of these methods
disregard conversational dependencies (discourse
structure) between sentences. Recently, Joty and
Hoque (2016) proposed a neural-CRF framework
for SAR in forum conversations. In their ap-
proach, a bi-LSTM (trained on the SAR task) first
encodes the sentences separately into task-specific
embeddings, which are then used in a separate
CRF model to capture the conversational depen-
dencies between sentences. They also use la-
beled data from the MRDA meeting corpus, with-
out which their LSTMs perform worse than simple
feed-forward networks. Although their method at-
tempts to model sentence structure (using LSTM)
and conversational dependencies (using CRF), the
approach has several limitations.

First, the LSTM-CRF framework was disjoint,
and thus cannot be trained end-to-end. Second,
when using the MRDA meeting data, their method
simply concatenates it with the target domain data
assuming they have the same distribution. How-
ever, asynchronous domains (forum, email) dif-
fer from synchronous (MRDA) in their underlying
conversational structure (Louis and Cohen, 2015),
in style (spoken vs. written), and in vocabulary us-
age (meetings on some focused agenda vs. conver-
sations on any topic of interests in a public forum).
Therefore, we hypothesize that to make the best
use of labeled data from synchronous domains,
one needs to model the shift in domains.

In this work, we advance the state-of-the-art of
SAR in asynchronous conversations in three main
steps. First, we introduce an end-to-end neural
architecture based on a hierarchical LSTM en-
coder with a Softmax or CRF output layer. Sec-
ond, we improve our initial SAR model by semi-
supervised learning in the form of word embed-
dings learned from a large unlabeled conversa-
tional corpus. Most importantly, we adapt our hi-

erarchical LSTM encoder using domain adversar-
ial training (Ganin et al., 2016) to leverage the la-
beled data from synchronous domains by explic-
itly modeling the shift in the two domains.

We evaluate our models on three different asyn-
chronous datasets containing forum and email
conversations, and on the MRDA meeting cor-
pus. Our main findings are: (i) the hierarchi-
cal LSTMs outperform existing methods when
trained on in-domain data for both synchronous
and asynchronous domains, setting a new state-
of-the-art; (ii) conversational word embeddings
yield significant improvements over off-the-shelf
ones; and (iii) domain adversarial training im-
proves the results by inducing domain-invariant
features. The source code, the conversational
word embeddings, and the datasets are avail-
able at https://ntunlpsg.github.io/
demo/project/speech-act/.

2 Related Work

Previous studies on SAR in asynchronous con-
versation have used supervised, semi-supervised
and unsupervised methods. Cohen et al. (2004)
classify emails into acts like ‘deliver’ and ‘meet-
ing’. Their approach however does not take email
context into account. Carvalho and Cohen (2005)
use an iterative algorithm containing two different
classifiers: the content classifier that only looks at
the content of the message, and the context clas-
sifier that takes into account both the content and
contextual speech acts in the email thread struc-
ture. Other supervised approaches use classifiers
and sequence taggers with hand-crafted features
(Qadir and Riloff, 2011; Tavafi et al., 2013).

Jeong et al. (2009) use semi-supervised boost-
ing to induce informative patterns from labeled
spoken domains (MRDA, SWBD). Given a sen-
tence represented as a set of trees (dependency,
POS tags, n-grams), the boosting algorithm iter-
atively learns the sub-tree features. This approach
does not consider the dependencies between the
act types, something we successfully exploit in our
work. Also, we leverage labeled data from syn-
chronous conversations while adapting our model
to account for the domain shift. Joty and Hoque
(2016) use a bi-LSTM to encode a sentence, then
use a separate CRF to model conversational de-
pendencies. To learn an effective bi-LSTM model,
they use the MRDA meeting data; however, with-
out modeling the domain differences.

https://ntunlpsg.github.io/demo/project/speech-act/
https://ntunlpsg.github.io/demo/project/speech-act/


1328

The unsupervised methods use variations of
Hidden Markov Models (HMM) including HMM-
Topic (Ritter et al., 2010), HMM-Mix (Joty et al.,
2011), and Mixed Membership (Paul, 2012).

Several neural methods have been proposed
in recent years for SAR in synchronous con-
versations. Kalchbrenner and Blunsom (2013)
use a simple recurrent neural network (RNN) to
model sequential dependencies between act types
in phone conversations. They use a convolutional
network to compose sentence representations from
word vectors. Lee and Dernoncourt (2016) use
a similar model, but also experiment with RNNs
to compose sentence representations. Khanpour
et al. (2016) use a stacked LSTM to compose
word vectors into a sentence vector. Kumar et al.
(2018) also use a hierarchical LSTM-CRF. How-
ever, none of these methods were applied to asyn-
chronous conversations, where not much labeled
data is available. Also to the best of our knowl-
edge, no prior work attempted to do domain adap-
tation from the synchronous conversation, which
is our main contribution in this paper.

3 The Base Model
We use a bidirectional long short-term memory or
bi-LSTM (Hochreiter and Schmidhuber, 1997) to
encode each sentence into a vector representation.
Given an input sentence xi = (w1, · · · , wm) of
length m, we first map each word wt to its cor-
responding vector representation vt by looking up
the word embedding matrix. The LSTM recurrent
layer then computes a compositional representa-
tion zt at every time step t by performing nonlin-
ear transformations of the current input vt and the
output of the previous time step zt−1. The output
of the last time step zm is considered as the repre-
sentation of the sentence. A bi-LSTM composes a
sentence in two directions: left-to-right and right-
to-left, yielding a representation hi = [−→zm;←−zm],
where ‘;’ denotes concatenation. Similar to (Joty
and Hoque, 2016), we could use hi to classify sen-
tence xi into one of the speech act types using a
Softmax output layer. However, in that case, we
would disregard the discourse-level dependencies
between sentences in a conversation. To take con-
versational dependencies into account, we explore
two methods as we describe below.

3.1 Hierarchical LSTM
We consider a conversation as a sequence of utter-
ances (sentences). Given an input sequence of n

Figure 2: Hierarchical bi-LSTM-CRF model with do-
main adversarial training for speech act recognition.

sentences X = (x1, · · · ,xn), the sentence-level
bi-LSTM generates a sequence of n vectors H =
(h1, · · · ,hn). To consider interdependencies be-
tween sentences, we place another bi-LSTM layer
on top of H to connect the sentence vectors se-
quentially in both directions, and encode each sen-
tence within its left and right contexts. As shown
in Figure 2, the upper bi-LSTM combines the cur-
rent input hi with its previous hidden state −→u i−1
(resp., ←−u i+1) to generate a representation for the
current sentence −→u i (resp., ←−u i). The hierarchi-
cally encoded sentence vectors U = (u1, · · · ,un)
(where ui = [−→ui;←−ui]) are fed into a Softmax clas-
sifier for speech act classification.

p(yi = k|X,W, θ) =
exp (wTk ui)∑K
k=1 exp (w

T
k ui)

where W are the classifier weights, and θ are the
parameters of the hierarchical LSTM encoder. We
train the model by minimizing the cross entropy:

Lc(W, θ) = −
n∑
i=1

K∑
k=1

yi,k log p(yi = k|X,W, θ)

with yi,k being the one-hot encoding of the label.

3.2 Hierarchical LSTM with CRF
The hierarchical LSTM (H-LSTM) captures con-
textual information by propagating information
through hidden layers, and has been shown to be
effective in similar tasks such as context encoding
in dialog systems (Serban et al., 2016). Despite
this, its modeling strength is limited compared



1329

to structured models that use global inference to
model consistency in the output, especially when
there are strong dependencies between output la-
bels (Collobert et al., 2011). Therefore, instead of
classifying sentences independently with a Soft-
max layer, our second method is to model them
jointly with a CRF layer (Lafferty et al., 2001).
For an input-output sequence pair (X,y), we de-
fine the joint probability distribution:

p(y|X) = 1
Z(U, A, V, θ)

n∏
i=1

ψn(yi|ui, V )︸ ︷︷ ︸
node factor

n∏
i=0

ψe(yi,i+1|A)︸ ︷︷ ︸
edge factor

where U = (u1, · · · ,un) is the hierarchically en-
coded sentence vectors as before, and ψn(yi =
k|ui, V ) = exp(V Tk ui) is the node-level score
with V being the weight matrix, ψe is the tran-
sition matrix parameterized by A, and Z(.) is the
global normalization constant that ensures a valid
probability distribution. The cross entropy loss for
the (X,y) sequence pair can be written as:

Lc(V,A, θ)=−
n∑
i=1

logψn(yi|ui, V )−
n∑
i=0

logAi,i+1+logZ

We use Viterbi decoding to infer the most probable
tag sequence for an input sequence of sentences,
y∗ = argmaxy p(y|X, V, A, θ). We will demon-
strate later in our experiments that a CRF layer
helps the H-LSTM to adapt quickly (i.e., with less
labeled data) to a target domain by exploiting the
tag dependencies in the source domain.

4 Adaptation Methods

The hierarchical models have many parameters.
Given enough training data, they should be able to
encode a sentence, capturing its syntactic and se-
mantic properties, and discourse-level dependen-
cies. However, when it comes to SAR in asyn-
chronous domains, not many large annotated cor-
pora are available. Because of the large number
of parameters, the models usually overfit when
trained on small datasets of asynchronous conver-
sations (shown in Sec. 6). We propose two solu-
tions to address this problem. Our first (simple but
effective) solution is to leverage large unlabeled
conversational corpus to learn better task-agnostic
word embeddings, and use it to initialize our mod-
els for better generalization. In the interests of co-
herence, we present this method in Section 5.

Our second solution is to leverage data from
synchronous domains for which large annotated

corpus is available (e.g., MRDA corpus). How-
ever, as we will see, simple concatenation of the
datasets is not quite effective in our case, be-
cause the conversations in synchronous and asyn-
chronous domains differ in their conversational
structures, modality (spoken vs. written), and vo-
cabulary usage. To get the best out of the available
synchronous domain data, we need to adapt our
models by explicitly modeling the domain shift.
More precisely, our goal is to adapt the hierarchi-
cal encoder so that it learns to encode sentence
representations U (i.e., features used for classi-
fication) that is not only discriminative for the
act classification, but also invariant across the do-
mains. We propose to use the domain adversarial
training proposed by Ganin et al. (2016).

Let DS = {Xp,yp}Pp=1 denote the set of P la-
beled training conversations in the source domain
(MRDA). We consider two adaptation scenarios.

(i) Unsupervised adaptation: In this scenario,
we have only unlabeled examples in the target do-
main (e.g., forum). Let DuT = {Xp}

Q
p=P+1 be the

set of (Q−P − 1) unlabeled training instances in
the target domain with Q being the total number
of training instances in the two domains.

(ii) Semi-supervised/supervised adaptation:
In addition to the unlabeled instancesDuT , here we
have access to some labeled training instances in
the target domain, DlT = {Xp,yp}Rp=Q+1, with R
being the total number of training examples in the
two domains. Depending on the amount of labeled
data in the target domain, this setting is referred to
as semi-supervised or supervised adaptation.

4.1 Unsupervised Adaptation

The dashed lines in Figure 2 show the extension
of our base model for adaptation. The input con-
versation X is sampled either from a synchronous
domain (e.g., meeting) or from an asynchronous
domain (e.g., forum). Our goal is to adapt the
H-LSTM encoder (parameterized by θ) to gener-
ate U such that it is not only informative for the
SAR task but also invariant across domains. Upon
achieving this, we can use the adapted encoder to
encode a target sentence, and use the source clas-
sifier (Softmax or CRF) to classify the sentences.

We achieve this by adding a domain discrimina-
tor (dashed lines in Figure 2), another neural net-
work that takes U as input, and tries to discrimi-
nate the domains of the input conversation X (e.g.,
meeting vs. forum). The output of the discrimina-



1330

tor is defined by a sigmoid function:

d̂ω = p(d = 1|ui, ω, θ) = sigm(wTd hd) (1)

where d ∈ {0, 1} denotes the domain (1 for meet-
ing, 0 for forum), wd are the final layer weights of
the discriminator, and hd = g(Udui) defines the
hidden layer of the discriminator with Ud being
the layer weights, and g(.) being the activations.
We use cross entropy as the discrimination loss:

Ld(ω, θ) = −d log d̂ω − (1− d) log
(
1− d̂ω

)
(2)

The composite network has three players: the hier-
archical LSTM encoder, the classifier (Softmax
or CRF), and the domain discriminator. Dur-
ing training, the encoder and the classifier play a
co-operative game, while the encoder and the dis-
criminator play an adversarial game. The training
objective L(W, θ, ω) of the composite model is:

P∑
p=1

Lpc(W, θ)︸ ︷︷ ︸
act classif (src)

−λ
[ P∑
p=1

Lpd(ω, θ)︸ ︷︷ ︸
domain disc (src)

+

Q∑
p=P+1

Lpd(ω, θ)︸ ︷︷ ︸
domain disc (tar)

]
(3)

where θ are the parameters of the encoder, W
are the classifier weights, and ω = {Ud,wd} are
the parameters of the discriminator.1 The hyper-
parameter λ controls the relative strength of the
act classifier and the discriminator. We learn θ that
optimizes the following min-max criterion:

θ∗ = argmin
W,θ

max
Ud,wd

L(W, θ, ω) (4)

Note that the updates of the shared encoder
for the two networks (classifier and discrimina-
tor) work adversarially with respect to each other.
Algorithm 1 provides pseudocode of our training
method. The main challenge in adversarial train-
ing is to balance the networks (Arjovsky et al.,
2017). In our experiments, we found the discrim-
inator to be weaker initially. To balance the two
components, we would need the error signals from
the discriminator to be fairly weak initially, with
full power unleashed only as the classification er-
rors start to dominate. We follow the weighting
schedule proposed in (Ganin et al., 2016, p. 21),
which initializes λ to 0, and then changes it grad-
ually to 1 as training progresses.

1For simplicity, we describe adaptation of the encoder
with Softmax output, but this generalizes naturally to CRF.

Algorithm 1: Adversarial training with SGD.
Input : Data DS , DuT , and batch size b
Output: Adapted model parameters θ, W
1. Initialize model parameters;
2. repeat

(a) Randomly sample b
2

labeled examples from DS
(b) Randomly sample b

2
unlabeled examples from

DuT
(c) Compute Lc(W, θ) and Ld(ω, θ)
(d) Set λ = 2

1+exp(−10∗p) − 1; p is the training
progress linearly changing form 0 to 1.

// Classifier & Encoder
(e) Take a gradient step for 2

b
∇W,θLc(W, θ)

// Discriminator

(f) Take a gradient step for 2λ
b
∇Ud,wdLd(ω, θ)

// Gradient reversal

(g) Take a gradient step for − 2λ
b
∇θLd(ω, θ)

until convergence;

4.2 Semi-supervised/supervised Adaptation
It is straight-forward to extend our adaptation
method to a semi-supervised/supervised setting.
Similar to the instances in the source domain, the
labeled instances in the target domainDlT are used
for act classification and domain discrimination.
The total training loss in this case is

L(W, θ, ω) =
P∑
p=1

Lpc(W, θ)︸ ︷︷ ︸
act classif. (source)

+

R∑
p=Q+1

Lpc(W, θ)︸ ︷︷ ︸
act classif. (target)

−

λ
[ P∑
p=1

Lpd(ω, θ)︸ ︷︷ ︸
dom classif. (source)

+

R∑
n=P+1

Lpd(ω, θ)︸ ︷︷ ︸
dom classif. (target)

]

where the second term is the classification loss
on the target dataset DlT , and the last term is the
discrimination loss on both labeled and unlabeled
data in the target domain.

5 Corpora

We now describe the datasets and the act tagset
that we use, and the conversational word embed-
dings that we learn from a large unlabeled corpus.

5.1 Labeled Datasets
As mentioned, asynchronous domains lack large
corpora that are annotated with a standard speech
act tagset. Jeong et al. (2009) annotated sentences
in TripAdvisor (TA) forum threads with the stan-
dard 12 act types defined in MRDA. They also
remapped the BC3 email corpus (Ulrich et al.,
2008) according to these tags. Subsequent studies
(Tavafi et al., 2013; Oya and Carenini, 2014; Joty



1331

Asynchronous Synchronous
TA BC3 QC3 MRDA

Total # of conversations 200 39 47 73
Avg. # of comments/conv 4.02 6.54 13.32 N.A
Avg. # of sentences/conv 18.56 34.15 33.28 955.10
Avg. # of words/sen 14.90 12.61 19.78 10.11

Table 1: Basic statistics about our corpora.

Asynchronous Synchronous
Tag Description TA BC3 QC3 MRDA
SU Suggestion 7.71 5.48 17.38 5.97
R Response 2.4 3.75 5.24 15.63
Q Questions 14.71 8.41 12.59 8.62
P Polite 9.57 8.63 6.13 3.77
ST Statement 65.62 73.72 58.66 66.00

Table 2: Distribution of speech acts in our corpora.

and Hoque, 2016) used these datasets but grouped
the 12 acts into 5 coarser classes. Joty and Hoque
(2016) also created a new dataset of QatarLiving2

forum threads called QC3.3 We use these three
asynchronous datasets in our experiments. For our
experiments on synchronous domains, we use the
MRDA meeting corpus that was also used in re-
lated studies (Jeong et al., 2009; Joty and Hoque,
2016). Tables 1 and 2 show some basic statis-
tics of the datasets and the tag distributions. Note
that the tagset used by us and other related studies
in asynchronous (written) conversation is differ-
ent from the one used in synchronous spoken con-
versations (Lee and Dernoncourt, 2016; Khanpour
et al., 2016; Kumar et al., 2018). The later tagset
contains acts like backchannel, filter and disrup-
tion that are more specific to speech.

The train-dev-test splits of the asynchronous
datasets are done uniformly at random at the con-
versation level. Since the asynchronous datasets
are quite small in size, to have a reliable test set,
we create the train:test splits with an equal number
of conversations (Table 3). Joty and Hoque (2016)
also created conversation level datasets to train and
test their CRF models. Their test sets however
contain only 20% of the conversations, providing
only 5 conversations for QC3 and BC3, and 20
for TA. Our experiments on these small test sets
showed unstable results for all the models. There-
fore, we use a larger test set (50%), and we report
more general results on the whole corpus based on
2-fold cross-validation, where the second fold was

2http://www.qatarliving.com/
3https://ntunlpsg.github.io/project/speech-act/

Train Dev. Test
Source MRDA 59 (50865) 6 (8366) 8 (10492)

Target
TA 90 (1667) 20 (427) 90 (1617)
QC3 20 (675) 7 (230) 20 (660)
BC3 16 (557) 7 (233) 16 (542)

Total 126 (2899) 34 (890) 126 (2819)

Table 3: Train, dev. and test sets for the datasets. Num-
bers in parentheses indicate the number of sentences.

# of Threads # of Tokens # of Words
W3C 23,940 21,465,830 546,921
TripAdvisor 25,000 2,037,239 127,333
Qatar Living 219,690 103,255,922 1,157,757

MRDA - 675,110 18,514
SWBD - 1,131,516 57,075

Table 4: Datasets and their statistics used for training
the conversational word embeddings.

created by interchanging the train and test splits in
Table 3. The same development set was used to
tune the hyperparameters of the models for exper-
iments on each fold. For experiments on MRDA,
we use the same train:test:dev split as in (Jeong
et al., 2009; Joty and Hoque, 2016).

5.2 Conversational Word Embeddings
One simple and effective approach to semi-
supervised learning is to use word embeddings
pretrained from a large unlabeled corpus. In our
work, we use generic off-the-shelf pretrained em-
beddings to boost the performance of our models.
In addition, we have also trained word embeddings
from a large conversational corpus to get more rel-
evant conversational word embeddings.

We use Glove (Pennington et al., 2014) to train
our word embeddings from a corpus that con-
tains 24K email threads from W3C (w3c.org),
25K threads from TripAdvisor, 220K threads from
QatarLiving, and all conversations from SWBD
and MRDA (a total of 120M tokens). Table 4
shows some statistics of the datasets used for train-
ing the conversational word embeddings. We
also trained skip-gram word2vec (Mikolov et al.,
2013), but its performance was worse than Glove.

6 Experiments

We followed similar preprocessing steps as Joty
and Hoque (2016); specifically: normalize all
characters to lower case, spell out digits and
URLs, and tokenize the texts using TweetNLP
(Gimpel et al., 2011). For performance compar-

http://www.qatarliving.com/
https://ntunlpsg.github.io/project/speech-act/


1332

QC3 TA BC3 MRDA
SVMc-gl 16.96±0.00 20.17±0.00 17.20±0.00 31.47±0.00
FFNc-gl 48.29±0.25 61.36±0.21 39.58±0.26 71.12±0.13
FFNskip-th 50.80±1.21 61.44±0.92 47.67±0.74 71.73±0.48

B-LSTMrand 50.25±0.57 62.11±0.64 45.08±1.03 70.72±0.02
B-LSTMgl 53.21±0.77 63.23±0.80 49.04±0.90 72.23±0.18
B-GRUc-gl 60.50±0.36 67.23±0.76 55.45±1.05 72.04±0.35
B-LSTMc-gl 61.01±0.60 67.23±0.70 55.32±0.68 72.42±0.14
S-LSTMc-gl 56.70±0.58 62.28±1.23 52.31±0.86 71.32±0.28

H-LSTMc-gl 60.76±0.99 68.38±0.65 57.17±0.87 72.91±0.14
H-LSTM-CRFc-gl 59.83±1.27 68.10±0.68 56.37±0.61 72.77±0.17

Table 5: Macro-F1 scores for in-domain training.

ison, we use accuracy and macro-F1. Like other
related studies, we consider macro-F1 as the main
metric (more appropriate when class distributions
are imbalanced), and select our model based on
the best F1 on the development set. Due to space
limitations, we report only macro-F1 here. Please
refer to the Appendix for the accuracy numbers.

6.1 Experiments on In-domain Training

We first evaluate our base models on in-domain
datasets by comparing with state-of-the-art mod-
els. In the next subsection, we evaluate our adap-
tation method in the three adaptation scenarios.

Settings. To validate the efficacy of our model,
we compare it with two baselines: a Support Vec-
tor Machine (SVM) and a feed-forward network
(FFN). In one setting, we use the concatenated
word vectors as the input sentence representa-
tion, while in another, we use the pretrained skip-
thought vectors (Kiros et al., 2015). We also com-
pare our models with the bi-LSTM (B-LSTM)
model of Joty and Hoque (2016) and the stacked
LSTM (S-LSTM) of Khanpour et al. (2016).

We use the Adam optimizer (Kingma and Ba,
2014) with a learning rate of 0.001, and use
dropout to avoid over-fitting. We use the Xavier
initializer (Glorot and Bengio, 2010) to initialize
the weights, and uniform U(−0.05, 0.05) to ini-
tialize the word vectors randomly. For pretrained
word embeddings, we experiment with off-the-
shelf embeddings that come with Glove as well
as with our conversational word embeddings. For
both random and pretrained initialization, we fine-
tune our word embeddings on the SAR task.

We construct sequences from the chronological
order of the sentences in a conversation. Since
MRDA conversations are much longer compared
to those in asynchronous domains (955 vs. 18-34

sentences in Table 1), we split the MRDA con-
versations into smaller parts containing a maxi-
mum of 100 sentences.4 The number of epochs
and batch size were fixed to 30 and 5 (conver-
sations), respectively. We ran each experiment
5 times, each time with a different random seed,
and report the average of the (2-fold×5=10) runs
along with the standard deviation. Recently, Crane
(2018) show that the main source of variability in
results for neural models come from the random
seed, and the author has recommended to report
the distribution of results from a range of seeds.

Results. We present the results in Table 5. From
the first block of results, we notice that both SVM
and FFN baselines perform poorly compared to
other models that tune the word embeddings and
learn the sentence representation on the SAR task.

The second block contains five LSTM variants:
(i) B-LSTMrand, referring to bi-LSTM with ran-
dom initialization; (ii) B-LSTMgl, referring to bi-
LSTM initialized with off-the-shelf Glove embed-
dings; (iii) B-GRUc-gl, referring to bidirectional
Gated Recurrent Unit (Cho et al., 2014) initialized
with our conversational Glove; (iv) B-LSTMc-gl,
referring to bi-LSTM initialized with conversa-
tional Glove, and (v) S-LSTMc-gl, referring to a 2-
layer stacked LSTM with conversational Glove.5

From the results, we can make the following con-
clusions. First, B-LSTMrand overfits extremely on

4In a different setting, we created sequences by connect-
ing each non-initial comment with the initial comment gen-
erating many 2-comment sequences. This is considering the
fact that in many QA forums, users mostly answer to the
questions asked in the initial post. In our experiments on
in-domain training, we found this competitive with our ‘one
long-chain’ structure. However, the adaptation in this setting
was much worse because of the mismatch in discourse struc-
tures of synchronous and asynchronous conversations.

5Increasing the number of layers in S-LSTMc-gl did not
give any gain (see Table 2 in the Appendix).



1333

the asynchronous datasets, giving the worst results
among the LSTMs. Second, pretrained vectors
help to achieve better results, however, compared
to the off-the-shelf vectors, our conversational
word vectors yield much higher F1, especially,
in the asynchronous datasets that are smaller in
size (5 - 11% absolute gains). This demonstrates
that pretrained word embeddings provide an effec-
tive method to perform semi-supervised learning,
when they are learned from relevant datasets.

The last block shows the results of our models.
It is evident that both H-LSTM and H-LSTM-CRF
outperform other models in all the datasets except
QC3 where the difference is very small. They also
give the best F1 reported so far on MRDA, outper-
forming the B-LSTM models of Joty and Hoque
(2016) and S-LSTM model of Khanpour et al.
(2016). When we compare the two models, we
notice that H-LSTM outperforms H-LSTM-CRF
in all the datasets. A reason for this could be that
the contextual dependency is already captured by
the upper LSTM layer and the data may be too
small for the CRF to offer anything more.

6.2 Experiments on Domain Adaptation

Settings. We compare our adversarial adapta-
tion method with three baseline methods: Trans-
fer, Merge and Fine-tune. Transfer models are
trained on the source (MRDA) and tested on the
target (QC3, TA, BC3). Our adversarial unsu-
pervised adaptation method is comparable to the
transfer method as they use labeled data only from
the source domain. In Merge, models are trained
on the concatenated training set of source and tar-
get datasets. Fine-tune is a widely used adapta-
tion method for neural models (Chu and Wang,
2018). In this method, we first train a model on
the source domain until convergence, then we fine-
tune it on the target by training it further on the
target dataset. Both merge and fine-tune are com-
parable to our semi-supervised/supervised adap-
tation as these methods use labeled data from the
target domain. For semi-supervised experiments,
we take smaller subsets (e.g., 25%, 50%, and 75%
of the labeled data) from the target domain.

We also compare our method with Neural SCL
(Ziser and Reichart, 2017), which is another do-
main adaption method in the neural framework.
We used the implementation made available by
the authors.6 For training our adaptation models,

6https://github.com/yftah89/structural-correspondence-
learning-SCL

Method Model QC3 TA BC3

Transfer

SVM 17.78±0.00 20.44±0.00 17.85±0.00
FFN 46.91±0.00 56.30±0.00 46.74±0.00
S-LSTM 49.89±1.29 62.52±1.49 36.36±1.28
B-LSTM 50.50±0.91 65.47±0.62 35.92±0.62
H-LSTM 50.22±0.64 64.43±0.52 35.11±1.64
H-LSTM-CRF 50.83±0.70 63.80±0.81 34.45±1.42

Unsup.
adapt

Neural SCL 37.73±0.92 53.98±0.33 46.90±0.89
Adv-S-LSTM 43.36±1.44 48.51±0.51 42.05±0.21
Adv-B-LSTM 47.39±0.74 58.49±1.29 32.86±1.35
Adv-H-LSTM 46.53±1.48 52.90±1.20 31.36±1.91
Adv-H-LSTM-CRF 47.06±1.24 61.58±0.78 29.54±1.06

Merge
(50%)

S-LSTM 55.39±0.29 67.79±0.15 50.82±0.98
B-LSTM 55.08±0.67 68.99±0.35 51.05±0.60
H-LSTM 51.74±0.44 69.09±0.64 47.82±1.47
H-LSTM-CRF 50.92±0.48 68.66±0.12 48.58±0.59

Fine-tune
(50%)

S-LSTM 53.94±1.04 66.07±0.67 51.73±1.53
B-LSTM 54.81±0.48 68.43±0.51 52.26±0.82
H-LSTM 54.34±0.71 69.16±0.66 50.81±0.97
H-LSTM-CRF 54.97±0.87 69.91±0.53 51.42±1.24

Semisup.
adapt
(50%)

Neural SCL 41.46±0.75 58.85±0.27 48.32±0.19
Adv-S-LSTM 60.20±0.32 68.71±0.75 57.97±0.39
Adv-B-LSTM 58.57±0.80 66.51±0.81 54.39±1.28
Adv-H-LSTM 60.19±1.10 69.43±0.64 58.39±1.12
Adv-H-LSTM-CRF 61.81±0.63 70.34±0.62 59.43±1.41

Merge
(100%)

S-LSTM 59.18±1.40 66.93±1.70 54.87±1.47
B-LSTM 58.33±0.84 70.12±0.39 55.89±0.89
H-LSTM 59.85±0.57 70.40±0.41 57.19±0.87
H-LSTM-CRF 59.53±0.66 69.88±0.68 56.04±1.15

Fine-tune
(100%)

S-LSTM 56.67±0.85 67.41±0.34 56.40±0.44
B-LSTM 59.74±0.53 69.87±0.82 57.09±1.14
H-LSTM 60.12±0.44 70.96±0.61 58.09±1.03
H-LSTM-CRF 59.95±0.59 70.44±0.76 57.17±0.97

Sup.
adapt

Neural SCL 43.35±0.30 60.40±0.23 48.88±0.70
Adv-S-LSTM 61.15±0.48 70.33±0.66 59.19±0.67
Adv-B-LSTM 60.60±0.68 69.30±0.50 60.12±1.26
Adv-H-LSTM 63.10±0.83 72.82±0.59 60.38±1.07
Adv-H-LSTM-CRF 62.24±0.74 73.04±0.38 59.84±0.75

Table 6: Domain adaptation results on our datasets. All
models use conversational word embeddings. Results
are averaged over (2 folds×5) 10 runs.

we use SGD (Algorithm 1 in the Appendix) with
a momentum term of 0.9 and a dynamic learning
rate as suggested by Ganin et al. (2016).
Results. The adaptation results are shown in Ta-
ble 6. We observe that without any labeled data
from the target (Unsup. adap), our adversarial
adapted models (Adv-H-LSTM, Adv-H-LSTM-
CRF) perform worse than the transfer baseline in
all three datasets. In this case, since the out-of-
domain labeled dataset (MRDA) is much larger,
it overwhelms the model inducing features that
are not relevant for the task in the target do-
main. However, when we provide the models with
some labeled in-domain examples in the semi-
supervised (50%) setting, we observe about 11%
absolute gains in QC3 and BC3 over the corre-
sponding Merge baselines, and 7 - 8% gains over
the corresponding Fine-tune baselines. As we
add more target labels (100%), performance of
our adapted models (Sup. adap) improve further,
yielding sizable improvements (∼ 3% absolute)
over the corresponding baselines in all datasets.

https://github.com/yftah89/structural-correspondence-learning-SCL
https://github.com/yftah89/structural-correspondence-learning-SCL


1334

0 25 50 75 100

30

50

60

70

Amount of Target Labeled Data [%]

F
1

Sc
or

e

QC3Adv-H-LSTM
QC3Adv-H-LSTM-CRF
TAAdv-H-LSTM
TAAdv-H-LSTM-CRF
BC3Adv-H-LSTM
BC3Adv-H-LSTM-CRF

Figure 3: F1 with varying amount of target labels.

Also notice that our adversarial adaptation outper-
forms Merge and Fine-tune methods for all models
over all datasets, showing its effectiveness.

Figure 3 presents the F1 scores of our adapted
models with varying amount of labeled data in the
target domain. We notice that the largest improve-
ments for all three datasets come from the first
25% of the target labels. The gains from the sec-
ond quartile are also relatively higher than the last
two quartiles for TA and BC3. Another interesting
observation is that H-LSTM-CRF performs better
in unsupervised and semi-supervised settings (i.e.,
with less target labels). In other words, H-LSTM-
CRF adapts better than H-LSTM with small target
datasets by exploiting the tag dependencies in the
source. As we include more labeled data from the
target, H-LSTM catches up with H-LSTM-CRF.
Surprisingly, Neural SCL performs the worst. We
suspect this is due to the mismatches between
pivot features of the source and target domains.

If we compare our adaptation results with the
in-domain results in Table 5, we notice that us-
ing the same amount of labeled data in the target,
our supervised adaptation gives 3-4% gains across
the datasets. Our semi-supervised adaptation us-
ing half of the target labels (50%) also outperforms
the in-domain models that use all the target labels.

To further analyze the cases where our adapted
models make a difference, Figure 4 shows the con-
fusion matrices for the adapted H-LSTM and the
non-adapted H-LSTM on the concatenated test-
sets of QC3, TA, and BC3. In general, our clas-
sifiers get confused between Response and State-
ment, and between Suggestion and Statement the
most. We noticed similar phenomena in the hu-
man annotations, where annotators had difficulties

(a) Non-adapted H-LSTM

(b) Adapted H-LSTM

Figure 4: Confusion matrices on the combined test sets.

with these three acts. It is however noticeable that
the adapted H-LSTM is less affected by class im-
balance, and it can detect the Suggestion and Polite
acts more correctly than the non-adapted one.

7 Conclusion

We proposed an adaptation framework for speech
act recognition in asynchronous conversation. Our
base model is a hierarchical LSTM encoder with
a Softmax or CRF output layer, which achieves
state-of-the-art results for in-domain training.
Crucial to its performance is the conversational
word embeddings. We adapted our base model
with adversarial training to effectively leverage
out-of-domain meeting data, and to improve the
results further. A comparison with existing meth-
ods and baselines in different training scenarios
demonstrates the effectiveness of our approach.

Acknowledgments

Shafiq Joty would like to thank the funding sup-
port from MOE Tier-1 (Grant M4011897.020).



1335

References
Martı́n Arjovsky, Soumith Chintala, and Léon Bottou.

2017. Wasserstein GAN. CoRR, abs/1701.07875.

John Langshaw Austin. 1962. How to do things with
words. Harvard University Press.

Vitor R. Carvalho and William W. Cohen. 2005. On
the collective classification of email ”speech acts”.
In SIGIR ’05, pages 345–352, New York, NY, USA.

Kyunghyun Cho, Bart van Merriënboer, Çalar
Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In EMNLP’14,
Doha, Qatar.

Chenhui Chu and Rui Wang. 2018. A survey of do-
main adaptation for neural machine translation. In
Proceedings of the 27th International Conference on
Computational Linguistics, pages 1304–1319. Asso-
ciation for Computational Linguistics.

William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
“speech acts”. In EMNLP’04, pages 309–316.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR, 12:2493–2537.

Matt Crane. 2018. Questionable answers in question
answering research: Reproducibility and variability
of published results. TACL’18, 6:241–252.

Rajdip Dhillon, Sonali Bhagat, Hannah Carvey, and
Elizabeth Shriberg. 2004. Meeting Recorder
Project: Dialog Act Labeling Guide. Technical re-
port, ICSI Tech. Report.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor Lempitsky.
2016. Domain-adversarial training of neural net-
works. JMLR, 17(1):2096–2030.

Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In HLT’11: short papers-Volume 2, pages 42–47.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In JMLR, Sardinia, Italy.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Semi-supervised speech act recognition in
emails and forums. In EMNLP’09, pages 1250–
1259, Singapore.

Shafiq Joty, Giuseppe Carenini, and Chin-Yew Lin.
2011. Unsupervised Modeling of Dialog Acts in
Asynchronous Conversations. In IJCAI’11, pages
1–130, Barcelona.

Shafiq Joty and Enamul Hoque. 2016. Speech act mod-
eling of written asynchronous conversations with
task-specific embeddings and conditional structured
models. In ACL ’16, pages 1746–1756, Berlin, Ger-
many.

Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997.
Switchboard SWBD-DAMSL Shallow-Discourse-
Function Annotation Coders Manual. Technical re-
port, UC Boulder & SRI International.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. In CVSC’13, pages 119–126.

Hamed Khanpour, Nishitha Guntakandla, and Rod-
ney Nielsen. 2016. Dialogue act classification in
domain-independent conversations using a deep re-
current neural network. In COLING’16, pages
2012–2021.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,
Richard S. Zemel, Antonio Torralba, Raquel Ur-
tasun, and Sanja Fidler. 2015. Skip-thought vec-
tors. In Proceedings of the 28th International Con-
ference on Neural Information Processing Systems -
Volume 2, NIPS’15, pages 3294–3302, Cambridge,
MA, USA.

Harshit Kumar, Arvind Agarwal, Riddhiman Dasgupta,
Sachindra Joshi, and Arun Kumar. 2018. Dia-
logue act sequence labeling using hierarchical en-
coder with CRF. In AAAI’18.

John Lafferty, Andrew K. McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.

Ji Young Lee and Franck Dernoncourt. 2016. Sequen-
tial short-text classification with recurrent and con-
volutional neural networks. In HLT’16, pages 515–
520.

Annie P Louis and Shay B Cohen. 2015. Conversa-
tion trees: A grammar model for topic structure in
forums. In ACL ’15.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic Regularities in Continuous Space
Word Representations. In NAACL-HLT ’13, pages
746–751, Atlanta, GA, USA.

Tatsuro Oya and Giuseppe Carenini. 2014. Extrac-
tive summarization and dialogue act modeling on
email threads: An integrated probabilistic approach.
In SIGDIAL’14, page 133–140, Philadelphia, PA,
U.S.A.

https://doi.org/10.1145/1076034.1076094
https://doi.org/10.1145/1076034.1076094
http://www.aclweb.org/anthology/D14-1179
http://www.aclweb.org/anthology/D14-1179
http://www.aclweb.org/anthology/D14-1179
http://aclweb.org/anthology/C18-1111
http://aclweb.org/anthology/C18-1111
http://aclweb.org/anthology/Q18-1018
http://aclweb.org/anthology/Q18-1018
http://aclweb.org/anthology/Q18-1018
http://www.aclweb.org/anthology/P16-1165
http://www.aclweb.org/anthology/P16-1165
http://www.aclweb.org/anthology/P16-1165
http://www.aclweb.org/anthology/P16-1165
https://web.stanford.edu/~jurafsky/ws97/manual.august1.html
https://web.stanford.edu/~jurafsky/ws97/manual.august1.html
http://www.aclweb.org/anthology/W13-3214
http://www.aclweb.org/anthology/W13-3214
http://www.aclweb.org/anthology/W13-3214
http://www.aclweb.org/anthology/C16-1189
http://www.aclweb.org/anthology/C16-1189
http://www.aclweb.org/anthology/C16-1189
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
http://dl.acm.org/citation.cfm?id=2969442.2969607
http://dl.acm.org/citation.cfm?id=2969442.2969607
https://doi.org/10.18653/v1/N16-1062
https://doi.org/10.18653/v1/N16-1062
https://doi.org/10.18653/v1/N16-1062
http://www.aclweb.org/anthology/N13-1090
http://www.aclweb.org/anthology/N13-1090
http://www.aclweb.org/anthology/W14-4318
http://www.aclweb.org/anthology/W14-4318
http://www.aclweb.org/anthology/W14-4318


1336

Michael J. Paul. 2012. Mixed Membership Markov
Models for Unsupervised Conversation Modeling.
In EMNLP-CoNLL ’12, pages 94–104, Stroudsburg,
PA, USA.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP’14, pages 1532–1543,
Doha, Qatar.

Ashequl Qadir and Ellen Riloff. 2011. Classifying sen-
tences as speech acts in message board posts. In
EMNLP’11, pages 748–758, Edinburgh, Scotland,
UK.

A. Ritter, C. Cherry, and B. Dolan. 2010. Unsuper-
vised modeling of twitter conversations. In HLT:
NAACL’10, LA, California.

Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In AAAI’16.

Maryam Tavafi, Yashar Mehdad, Shafiq Joty, Giuseppe
Carenini, and Raymond Ng. 2013. Dialogue act
recognition in synchronous and asynchronous con-
versations. In SIGDIAL’13, Metz, France.

Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.
2008. A publicly available annotated corpus for
supervised email summarization. In AAAI’08,
Chicago, USA.

Yftah Ziser and Roi Reichart. 2017. Neural structural
correspondence learning for domain adaptation. In
CoNLL 2017, pages 400–410.

http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D11-1069
http://www.aclweb.org/anthology/D11-1069
http://dl.acm.org/citation.cfm?id=3016387.3016435
http://dl.acm.org/citation.cfm?id=3016387.3016435
http://dl.acm.org/citation.cfm?id=3016387.3016435
http://www.aclweb.org/anthology/W13-4017
http://www.aclweb.org/anthology/W13-4017
http://www.aclweb.org/anthology/W13-4017

