



















































Integrated sentence generation using charts


Proceedings of The 10th International Natural Language Generation conference, pages 139–143,
Santiago de Compostela, Spain, September 4-7 2017. c©2017 Association for Computational Linguistics

Integrated sentence generation with charts

Alexander Koller and Nikos Engonopoulos
Saarland University, Saarbrücken, Germany

{koller|nikos}@coli.uni-saarland.de

Abstract

Integrating surface realization and the gen-
eration of referring expressions (REs) into a
single algorithm can improve the quality of
the generated sentences. Existing algorithms
for doing this, such as SPUD and CRISP,
are search-based and can be slow or incom-
plete. We offer a chart-based algorithm for
integrated sentence generation which supports
efficient search through chart pruning.

1 Introduction

It has long been argued (Stone et al., 2003) that
the strict distinction between surface realization and
sentence planning in the classical NLG pipeline (Re-
iter and Dale, 2000) can cause difficulties for an
NLG system. Generation decisions that look good to
the sentence planner may be hard or impossible for
the realizer to express in natural language. Further-
more, a standalone sentence planner must compute
each RE separately, thus missing out on opportuni-
ties for succinct REs that are ambiguous in isolation
but correct in context (Stone and Webber, 1998).

Algorithms such as SPUD (Stone et al., 2003) and
CRISP (Koller and Stone, 2007) perform surface re-
alization and parts of sentence planning, including
RE generation, in an integrated fashion. Such inte-
grated algorithms for sentence generation can bal-
ance the needs of the realizer and the sentence plan-
ner and take advantage of opportunities for succinct
realizations. However, integrated sentence planning
multiplies the complexities of two hard combinato-
rial problems, and thus existing, search-based algo-
rithms can be inefficient or fail to find a good solu-

tion; SPUD’s greedy search strategy may even find
no solution at all, even when one exists.

By contrast, chart-based algorithms have been
shown in parsing to remain efficient and accurate
even for large inputs because they support structure
sharing and very effective pruning techniques. Chart
algorithms have been successfully applied to surface
realization (White, 2004; Gardent and Kow, 2005;
Carroll and Oepen, 2005; Schwenger et al., 2016),
but in RE generation, most algorithms are not chart-
based, see e.g. (Dale and Reiter, 1995; Krahmer et
al., 2003). One exception is the chart-based RE gen-
eration of Engonopoulos and Koller (2014).

In this paper, we present a chart-based algorithm
for integrated surface realization and RE generation.
This makes it possible – to our knowledge, for the
first time – to apply chart-based pruning techniques
to integrated sentence generation. Our algorithm ex-
tends the chart-based RE generation algorithm of
Engonopoulos and Koller (2014) by keeping track of
the semantic content that has been expressed by each
chart item. Because it is modular on the string side,
the same algorithm can be used to generate with
context-free grammars or TAGs, with or without fea-
ture structures, at no expense in runtime efficiency.
An open-source implementation of our algorithm,
based on the Alto system (Gontrum et al., 2017), can
be found at bitbucket.org/tclup/alto.

2 Chart-based integrated generation

We first describe the grammar formalism we use.
Then we explain the sentence generation algorithm
and discuss its runtime performance.

139



(a) (b) (c)

{e}
IR←−−−−−− sleepe,r2

def r2

rabbitr2

whiter2

IN−−−→ {sleep(e, r2), rabbit(r2), white(r2)}
[∩2]1

uniqr2

∩1

whiterabbit

sleep

]

]

white(r2)rabbit(r2)

sleep(e, r2)

(d) IS : •(•(the, •(white, rabbit)), sleeps) = ”the white rabbit sleeps”
Figure 1: A derivation tree (b) with its interpretations (a, c, d).

2.1 Semantically interpreted grammars

We describe the integrated sentence generation
problem in terms of semantically interpreted gram-
mars (SIGs) (Engonopoulos and Koller, 2014), a
special case of Interpreted Regular Tree Grammars
(Koller and Kuhlmann, 2011). We introduce SIGs
by example, and refer to Engonopoulos and Koller
(2014) for detailed definitions.

An example SIG grammar is shown in Fig. 2. At
the core of each grammar rule is a rule of the form
Aa → f(Bb, . . . ,Zz). The symbols A, . . . ,Z are
nonterminals such as S, NP, VP, and a, . . . , z are se-
mantic indices, i.e. constants for individuals indicat-
ing to which object in the model a natural-language
expression is intended to refer. These core rules al-
low us to recursively derive a derivation tree, such
as the one shown in Fig. 1b, representing the abstract
syntactic structure of a natural-language expression.

Each derivation tree t is mapped to a string
through a function IS . This function is defined re-
cursively for each rule of the grammar. In the ex-
ample grammar of Fig. 2, we have IS(whiter2) =
white, i.e. the word “white”. Given a string w1,
we have IS(rabbitr2)(w1) = w1 • rabbit, where
“•” is string concatenation. This means that given
a subtree t′ which evaluates to the string w1, a
string for the tree rabbitr2(t

′) is constructed by ap-
pending the word “rabbit” after w1. For the com-
plete derivation tree t in Fig. 1, we obtain IS(t) =
“the white rabbit sleeps”.

At the same time and in the same way, the deriva-
tion tree is also evaluated to a set of referents through
a function IR. Constants, such as rabbit and sleep,
are interpreted as subsets of and relations between
the individuals in a given model. For instance, in
the model of Fig. 3, rabbit denotes the set {r1, r2},
whereas in denotes the relation {(r1, h1), (f1, h2)}.

for all e, a ∈ sleep:
Se → sleepe,a(NPa)
IS(sleepe,a)(w1) = w1 • sleeps
IR(sleepe,a)(R1) = [sleep ∩2 uniqa(R1)]1
IN (sleepe,a)(N1) = {sleep(e, a)} ]N1
for all a ∈ rabbit:
Na → rabbita(Adja)
IS(rabbita)(w1) = w1 • rabbit
IR(rabbita)(R1) = rabbit ∩1 R1
IN (rabbita)(N1) = {rabbit(a)} ]N1
for all a ∈ U :
NPa → def a(Na)
IS(def a)(w1) = the • w1
IR(def a)(R1) = R1
IN (def a)(N1) = N1

for all a ∈ U :
Na → thinga(Adja)
IS(thinga)(w1) = w1 • thing
IR(thinga)(R1) = R1
IN (thinga)(N1) = N1

for all a ∈ white:
Adja → whitea
IS(whitea) = white
IR(whitea) = white
IN (whitea) = {white(a)}

for all a ∈ U :
Adja → nopa
IS(nopa) = �
IR(nopa) = U
IN (nopa) = ∅

for all e, a, b ∈ takefrom(e, a, b):
Se → takefrome,a,b(NPa,NPb)
IS(takefrome,a,b)(w1, w2) = take • w1 • from • w2
IR(takefrome,a,b)(R1, R2) = [(takefrom ∩2 uniqa(R1 ∩1 [in ∩2
R2]1)) ∩3 uniqb(R2 ∩1 [in ∩1 R1]2)]1
IN (takefrome,a,b)(N1, N2) = {takefrom(e, a, b)} ]N1 ]N2

Figure 2: An example grammar.

Figure 3: An example model.

These relations are then combined using intersec-
tion R1 ∩i R2 (yielding the subset of elements of
R1 whose i-th component is an element of the set
R2), projection [R]i (yielding the set of i-th compo-
nents of the tuples in R), and the uniqueness checker
uniqa(R) (yielding R if R = {a} and ∅ otherwise).
Under this interpretation, the derivation tree in Fig. 1
maps to the set {e}, given the model in Fig. 3.

Observe, finally, that each rule is annotated with a
“for all” clause, which creates instances of the rule
for each tuple of individuals that satisfies the con-
dition. We also mention that although we only use
unary attributes such as “rabbit” and “white” in this
example grammar, SIGs deal easily with relational
attributes such as “in” or “next to”; see Engonopou-
los and Koller (2014).

2.2 Integrated sentence generation with SIGs

Engonopoulos and Koller (2014) describe an algo-
rithm which, given a SIG grammar G and a set R

140



[B1, R1, N1] . . . [Bk, Rk, Nk]
A→ r(B1, ..., Bk) in G

[A, IR(r)(R1, . . . , Rk), IN (r)(N1, . . . , Nk)]
Figure 4: The chart computation algorithm.

of target referents, will compute a chart describing
all derivations t of G with IR(t) = R – that is, all
semantically valid REs for R.

Here we extend both SIGs and this algorithm to
include surface realization. We assume that the
generation algorithm is given a set N of seman-
tic atoms in addition to the grammar G and refer-
ent set R, and should return only derivations that
refer to R while expressing at least all the atoms
in N . We achieve this by adding an interpreta-
tion IN to SIGs, such that IN (t) will return a set
of semantic atoms expressed by the derivation tree
t. We have added such IN clauses to the gram-
mar in Fig. 2. For example, the rule for whiter2
expresses the set {white(r2)} of semantic atoms.
The rule for rabbitr2 evaluates to the disjoint union
of {rabbit(r2)} with whatever its “Adj” subtree ex-
pressed. Thus, the IN interpretation keeps track of
the semantic atoms expressed by each subtree of a
derivation tree; in the example of Fig. 1, we see that
the derivation tree as a whole expresses the semantic
atoms {sleep(e, r2), rabbit(r2), white(r2)}.

Given a grammar G, target referent set R, and tar-
get semantic content N , we can now compute a chart
that describes all derivation trees t of G such that
IR(t) = R and IN (t) ⊇ N ; thus this algorithm
performs surface realization and RE generation at
the same time. The algorithm, shown in Fig. 4 in
the form of a parsing schema (Shieber et al., 1995),
computes chart items [A, R, N ] in a bottom-up fash-
ion. Such an item states that there is a tree t such that
t can be derived from A, and we have IR(t) = R
and IN (t) = N . Given k items for subtrees derived
from the nonterminals B1, . . . , Bk as premises and a
rule r that can combine these into a nonterminal A,
the algorithm creates a new item for A in which the
IR and IN functions for that rule were applied to the
referent sets and semantic contents of the subtrees.
Given the inputs R and N , we define a goal item
to be an item [S, R, N ′] where S is the start symbol
of the grammar and N ′ ⊇ N . Each goal item the
algorithm discovers thus represents a sentence that
achieves the given communicative goals.

An example run of the algorithm, for the inputs

1 [Adjr2 , {r2}, {whiter2}] (whiter2 )
2 [Adjr2 , U, ∅] (nopr2 )
3 [Nr2 , {r2}, {whiter2 , rabbitr2}] (rabbitr2 , 1)
4 [Nr2 , {r2}, {whiter2}] (thingr2 , 1)
5 [Nr2 , {r1, r2}, {rabbitr2}] (rabbitr2 , 2)
6 [NPr2 , {r2}, {whiter2 , rabbitr2}] (ther2 , 3)
7 [NPr2 , {r2}, {whiter2}] (ther2 , 4)
8 [NPr2 , {r1, r2}, {rabbitr2}] (ther2 , 5)
9 [Se, {e}, {whiter2 , rabbitr2 , sleepe,r2}] (sleepe, 6)
10 [Se, {e}, {whiter2 , sleepe,r2}] (sleepe, 7)
11 [Se, ∅, {rabbitr2 , sleepe,r2}] (sleepe, 8)

Figure 5: Excerpt from the chart for “The white rabbit sleeps.”

R = {e} and N = {rabbit(r2)}, is shown in Fig. 5.
Each row in the chart corresponds to one application
of the rule in Fig. 4. The grammar rule that was used,
along with any premises, is given in brackets to the
right. Observe that the only goal item, (9), corre-
sponds to the derivation in Fig. 1, and hence the out-
put string “the white rabbit sleeps”; the derivation
can be reconstructed by following the backpointers
to the premise items recursively. Observe also that
(10) – for “the white thing sleeps” – is not a goal
item because its semantic content is not a superset
of N . The item (11) – for “the rabbit sleeps” – is
not a goal item either: Its referent set is empty be-
cause (8) is not a unique RE for r2, and thus the term
uniqr2(R1) in the “sleep” rule evaluates to the empty
set. Thus, the algorithm performs both surface real-
ization and RE generation.

2.3 Generating succinct REs in context

One advantage of integrating surface realization
with RE generation is that REs can be more succinct
in the context of a larger grammatical construction
than in isolation. The shortest standalone RE for r1
in Fig. 3 is “the brown rabbit”, but it is perfectly fe-
licitous to say “take the rabbit from the hat”. Stone
and Webber (1998) explain this in terms of the pre-
suppositions of the verb “take X from Y”, which say
that X must be in Y, and thus the REs X and Y can
mutually constrain each other. They also show how
the SPUD algorithm can generate such succinct REs
in the context of the verb, by global reasoning over
the referent sets of all REs in the sentence.

Our algorithm can generate such REs as well,
and can do it in an efficient, chart-based way. As-
sume R = {e2} and N = {takefrom(e2, r1, h1)}
and the grammar in Fig. 2. The chart algo-
rithm will construct items for the sub-derivation-

141



trees t1 = def r1(rabbitr1(nopr1)) (“the rabbit”)
and t2 = def h1(hath1(noph1)) (“the hat”), with
R1 = IR(t1) = {r1, r2} and R2 = IR(t2) =
{h1, h2}; thus, these REs are not by themselves
unique. These trees are then combined with the
rule takefrome2,r1,h1 . This rule intersects R1 with
the set of things that are “in” an element of R2, en-
coding the presupposition of “take X from Y”. Thus
R1 ∩1 [in ∩2 R2]1 evaluates to {r1}, satisfying the
uniqueness condition. Thus, the algorithm returns
t = takefrome2,r1,h1(t1, t2) as a valid realization.

Note that we achieved the ability to let REs mutu-
ally constrain each other by moving the requirement
for semantic uniqueness to the verb that subcatego-
rizes for the RE. This is in contrast to the standard
assumption that it is the definite article that requires
uniqueness, but permits us a purely grammar-based
treatment of mutually constraining REs which re-
quires no further reasoning capabilities.

2.4 Chart generation with heuristics

Our algorithm can enumerate all subsets N of the
true semantic atoms in the model, and thus has
worst-case exponential runtime. This is probably
unavoidable, given that surface realization and the
generation of shortest REs are both NP-complete
(Koller and Striegnitz, 2002; Dale and Reiter, 1995).

However, because it is a chart-based algorithm,
we can use heuristics to avoid computing the whole
chart, and thus speed up the search for the best so-
lution. To get an impression of this, assume that we
are looking for a short sentence; other optimization
criteria are also possible. We first compute the full
chart CR for the IR part of the input alone, using
essentially the same algorithm as Engonopoulos and
Koller (2014). From CR we compute the distance
of each chart item to a goal item, i.e. the minimal
number of rules that must be applied to the item
to produce a goal item. We then refine the items
of CR by adding the IN parts to each chart item.
Unprocessed chart items [A, R, N ] are organized on
an agenda which is ordered lexicographically by the
number of atoms in the target semantic content that
are not yet realized in N and then the distance of
[A, R] to a goal item in CR. We stop the chart com-
putation once the first goal item has been found.

Using this pruning strategy, we measured
runtimes with problems from the GIVE Chal-

lenge (Koller et al., 2010) on an 2.9 GHz Intel
Core i5 CPU.1 We compared the performance of our
system against CRISP, which uses the FF planner
(Hoffmann and Nebel, 2001) to perform the search.
For CRISP, we only measured the time spent in run-
ning the planner. On the most complex scene from
GIVE that we tried, our system took 13 ms to gen-
erate the sentence “Push the button to the left of the
flower”, outperforming CRISP which generated the
same sentence in 50 ms. Note that it is possible to
construct (not entirely realistic) inputs for the gener-
ator on which FF’s much more sophisticated search
strategy outperforms the heuristic described above.
By incorporating such a heuristic into chart genera-
tion, e.g. as in Schwenger et al. (2016), our system
could be accelerated further.

3 Conclusion

We have presented a chart-based algorithm for inte-
grated surface realization and RE generation. Com-
pared to earlier approaches to integrated sentence
generation, our algorithm can exploit the capabil-
ities of charts for structure-sharing and pruning to
achieve higher runtime performance in practice. We
have only presented a simple pruning strategy here,
but it would be astraightforward extension to in-
corporate pruning strategies from surface realization
(White, 2004; Schwenger et al., 2016).

One advantage of our algorithm is that it is ag-
nostic of the grammar formalism that is used on
the string side. We have used context-free rules for
reading off string representations from the gener-
ated derivation trees, but because SIGs are special
case of IRTGs, we could instead use a tree-adjoining
grammar to construct strings instead (Koller and
Kuhlmann, 2012). In fact, the runtime experiments
in Section 2.4 were based on a TAG grammar to al-
low direct comparison with CRISP.

With the algorithm presented here, it may be-
come feasible for the first time to perform integrated
sentence generation in the context of practical ap-
plications. So far, grammars that support this lag
far behind grammars for surface realization in size
and complexity. It would thus be interesting to ei-
ther convert existing surface realization grammars to
SIGs, or to learn such grammars from data.

1We let the Java VM warm up before measuring runtimes.

142



References
John Carroll and Stephan Oepen. 2005. High efficiency

realization for a wide-coverage unification grammar.
In Proceedings of the 2nd IJCNLP.

Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean Maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233–263.

Nikos Engonopoulos and Alexander Koller. 2014. Gen-
erating effective referring expressions using charts. In
Proceedings of the 8th International Conference on
Natural Language Generation (INLG), Philadelphia.

Claire Gardent and Eric Kow. 2005. Generating and se-
lecting grammatical paraphrases. In Proceedings of
ENLG.

Johannes Gontrum, Jonas Groschwitz, Alexander Koller,
and Christoph Teichmann. 2017. Alto: Rapid proto-
typing for parsing and translation. In Proceedings of
the EACL Demo Session.

Jörg Hoffmann and Bernhard Nebel. 2001. The FF
planning system: Fast plan generation through heuris-
tic search. Journal of Artificial Intelligence Research,
14:253–302.

Alexander Koller and Marco Kuhlmann. 2011. A gener-
alized view on parsing and translation. In Proceedings
of the 12th International Conference on Parsing Tech-
nologies (IWPT).

Alexander Koller and Marco Kuhlmann. 2012. Decom-
posing TAG algorithms using simple algebraizations.
In Proceedings of the 11th TAG+ Workshop.

Alexander Koller and Matthew Stone. 2007. Sentence
generation as a planning problem. In Proceedings of
the 45th ACL.

Alexander Koller and Kristina Striegnitz. 2002. Genera-
tion as dependency parsing. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 17–24, Philadelphia, PA, USA.

Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-
tine Cassell, Robert Dale, Johanna Moore, and Jon
Oberlander. 2010. The First Challenge on Generating
Instructions in Virtual Environments. In E. Krahmer
and M. Theune, editors, Empirical Methods in Natural
Language Generation, number 5790 in LNCS, pages
337–361. Springer.

Emiel Krahmer, Sebastiaan van Erk, and André Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53–72.

Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge University
Press, Cambridge, England.

Maximilian Schwenger, Álvaro Torralba, Jörg Hoffmann,
David M. Howcroft, and Vera Demberg. 2016. From
OpenCCG to AI planning: Detecting infeasible edges

in sentence generation. In Proceedings of the 26th In-
ternational Conference on Computational Linguistics
(COLING).

Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24(1–2):3–
36.

Matthew Stone and Bonnie Webber. 1998. Textual econ-
omy through close coupling of syntax and semantics.
In Proceedings of the 9th INLG.

Matthew Stone, Christine Doran, Bonnie Webber, Tonia
Bleam, and Martha Palmer. 2003. Microplanning
with communicative intentions: The SPUD system.
Computational Intelligence, 19(4):311–381.

Michael White. 2004. Reining in CCG chart realization.
In Anja Belz, Roger Evans, and Paul Piwek, editors,
Proceedings of the Third International Conference on
Natural Language Generation (INLG04).

143


