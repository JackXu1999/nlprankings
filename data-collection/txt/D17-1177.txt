



















































Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1689–1694
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Combining Generative and Discriminative Approaches to Unsupervised
Dependency Parsing via Dual Decomposition∗

Yong Jiang, Wenjuan Han and Kewei Tu
{jiangyong,hanwj,tukw}@shanghaitech.edu.cn

School of Information Science and Technology
ShanghaiTech University, Shanghai, China

Abstract

Unsupervised dependency parsing aims to
learn a dependency parser from unanno-
tated sentences. Existing work focus-
es on either learning generative model-
s using the expectation-maximization al-
gorithm and its variants, or learning dis-
criminative models using the discrimina-
tive clustering algorithm. In this paper, we
propose a new learning strategy that learn-
s a generative model and a discriminative
model jointly based on the dual decom-
position method. Our method is simple
and general, yet effective to capture the ad-
vantages of both models and improve their
learning results. We tested our method on
the UD treebank and achieved a state-of-
the-art performance on thirty languages.

1 Introduction

Dependency parsing is an important task in nat-
ural language processing. It identifies dependen-
cies between words in a sentence, which have been
shown to benefit other tasks such as semantic role
labeling (Lei et al., 2015) and sentence classifica-
tion (Ma et al., 2015). Supervised learning of a
dependency parser requires annotation of a train-
ing corpus by linguistic experts, which can be time
and resource consuming. Unsupervised dependen-
cy parsing eliminates the need for dependency an-
notation by directly learning from unparsed text.

Previous work on unsupervised dependency
parsing mainly focuses on learning generative
models, such as the dependency model with va-
lence (DMV) (Klein and Manning, 2004) and
combinatory categorial grammars (CCG) (Bisk
and Hockenmaier, 2012). Generative models have

∗This work was supported by the National Natural Sci-
ence Foundation of China (61503248).

many advantages. For example, the learning ob-
jective function can be defined as the marginal
likelihood of the training data, which is typical-
ly easy to compute in a generative model. In
addition, many types of inductive bias, such as
those favoring short dependency arcs (Smith and
Eisner, 2006), encouraging correlations between
POS tags (Cohen et al., 2008; Cohen and Smith,
2009; Berg-Kirkpatrick et al., 2010; Jiang et al.,
2016), and limiting center embedding (Noji et al.,
2016), can be incorporated into generative models
to achieve better parsing accuracy. However, due
to the strong independence assumption in most
generative models, it is difficult for these models
to utilize context information that has been shown
to benefit supervised parsing.

Recently, a feature-rich discriminative model
for unsupervised parsing is proposed that captures
the global context information of sentences (Grave
and Elhadad, 2015). Inspired by discriminative
clustering, learning of the model is formulated as
convex optimization of both the model parameters
and the parses of training sentences. By utilizing
language-independent rules between pairs of POS
tags to guide learning, the model achieves state-of-
the-art performance on the UD treebank dataset.

In this paper we propose to jointly train two
state-of-the-art models of unsupervised dependen-
cy parsing: a generative model called LC-DMV
(Noji et al., 2016) and a discriminative model
called Convex-MST (Grave and Elhadad, 2015).
We employ a learning algorithm based on the dual
decomposition (Dantzig and Wolfe, 1960) infer-
ence algorithm, which encourages the two models
to influence each other during training.

We evaluated our method on thirty languages
and found that the jointly trained models surpass
their separately trained counterparts in parsing ac-
curacy. Further analysis shows that the two models
positively influence each other during joint train-

1689



ing by implicitly sharing the inductive bias.

2 Preliminaries

2.1 DMV
The dependency model with valence (DMV) (K-
lein and Manning, 2004) is the first generative
model that outperforms the left-branching base-
line in unsupervised dependency parsing. In D-
MV, a sentence is generated by recursively apply-
ing three types of grammar rules to construct a
parse tree from the top down. The probability of
the generated sentence and parse tree is the prob-
ability product of all the rules used in the genera-
tion process. To learn the parameters (rule prob-
abilities) of DMV, the expectation maximization
algorithm is often used. Noji et al. (2016) ex-
ploited two universal syntactic biases in learning
DMV: restricting the center-embedding depth and
encouraging short dependencies. They achieved a
comparable performance with state-of-the-art ap-
proaches.

2.2 Convex-MST
Convex-MST (Grave and Elhadad, 2015) is a dis-
criminative model for unsupervised dependency
parsing based on the first-order maximum span-
ning tree dependency parser (McDonald et al.,
2005). Given a sentence, whether each possible
dependency exists or not is predicted based on a
set of handcrafted features and a valid parse tree
closest to the prediction is identified by the mini-
mum spanning tree algorithm.

For each sentence x, a first-order dependency
graph is built over the words of the sentence. The
weight of each edge is calculated by wT f(x, i, j),
where w is the parameters and f(x, i, j) is the
handcrafted feature vector of the dependency from
the i-th word to the j-th word in sentence x. For
sentence x of length n, we can represent it as ma-
trix X where each raw is a feature vector. The
parse tree y is a spanning tree of the graph and
can be represented as a binary vector with length
n×nwhere each element is 1 if the corresponding
arc is in the tree and 0 otherwise.

Learning is based on discriminative clustering
with the following objective function:

1
N

N∑
α=1

(
1

2nα
||yα −Xαw||22 − µvTyα

)
+
λ

2
||w||22

where Xα is a matrix where each row is a feature
representation f(xα, i, j) of an edge in the depen-

dency graph of sentence xα, v represents whether
each dependency arc in yα satisfies a set of pre-
specified linguistic rules, and λ and µ are hyper-
parameters. The Frank-Wolfe algorithm is em-
ployed to optimize the objective function.

2.3 Dual Decomposition

Dual decomposition (Dantzig and Wolfe, 1960), a
special case of Lagrangian relaxation, is an opti-
mization method that decomposes a hard problem
into several small sub-problems. It has been wide-
ly used in machine learning (Komodakis et al.,
2007) and natural language processing (Koo et al.,
2010; Rush and Collins, 2012).

Komodakis et al. (2007) proposed using dual
decomposition to do MAP inference for Markov
random fields. Koo et al. (2010) proposed a new
dependency parser based on dual decomposition
by combining a graph based dependency model
and a non-projective head automata. In the work
of Rush et al. (2010), they showed that dual de-
composition can effectively integrate two lexical-
ized parsing models or two correlated tasks.

2.4 Agreement based Learning

Liang et al. (2008) proposed agreement based
learning that trains several tractable generative
models jointly and encourages them to agree on
certain latent variables. To effectively train the
system, a product EM algorithm was used. They
showed that the joint model can perform better
than each independent model on the accuracy or
convergence speed. They also showed that the ob-
jective function of the work of Klein and Manning
(2004) is a special case of the product EM algo-
rithm for grammar induction. Our approach has a
similar motivation to agreement based learning but
has two important advantages. First, while their
approach only combines generative models, our
approach can make use of both generative and dis-
criminative models. Second, while their approach
requires the sub-models to share the same dynam-
ic programming structure when performing decod-
ing, our approach does not have such restriction.

3 Joint Training

We minimize the following objective function that
combines two different models of unsupervised

1690



dependency parsing:

J(MF,MG)

=
N∑
α=1

min
yα∈Yα

(F (xα,yα; MF) +G(xα,yα; MG))

where N is the size of training data, MF and MG
are the parameters of the first and second model
respectively, F and G are their respective learn-
ing objectives, and Yα is the set of valid depen-
dency parses of sentence xα. While in princi-
ple this objective can be used to combine many
different types of models, here we consider two
state-of-the-art models of unsupervised dependen-
cy parsing, a generative model LC-DMV (Noji
et al., 2016) and a discriminative model Convex-
MST (Grave and Elhadad, 2015). We denote the
parameters of LC-DMV by Θ and the parameters
of Convex-MST by w. Their respective objective
functions are,

F (xα,yα; Θ) = − log (PΘ(xα,yα)f(xα,yα))
G(xα,yα; w)

=
1

2nα
||yα −Xαw||22 +

λ

2N
||w||22 − µvTy

where PΘ(xα,yα) is the joint probability of sen-
tence xα and parse yα, f is a constraint factor, and
the notations in the second objective function are
explained in section 2.2.

3.1 Learning
We use coordinate descent to optimize the param-
eters of the two models. In each iteration, we first
fix the parameters and find the best dependency
parses of the training sentences (see section 3.2);
we then fix the parses and optimize the parameter-
s. The detailed algorithm is shown in Algorithm
1.

Pretraining of the two models is done by run-
ning their original learning algorithms separate-
ly. When the parses of the training sentences are
fixed, it is easy to show that the parameters of the
two models can be optimized separately. Updat-
ing the parameters Θ of LC-DMV can be done by
simply counting the number of times each rule is
used in the parse trees and then normalizing the
counts to get the maximum-likelihood probabili-
ties. The parameters w of Convex-MST can be
updated by stochastic gradient descent. After up-
dating Θ and w at each iteration, we additional-
ly train each model separately for three iterations,
which we find further improves learning.

Algorithm 1 Parameter Learning
Input: Training sentence x1,x2, ...,xN
Pre-train Θ and w
repeat

Fix Θ and w and solve the decoding problem
to get yα, α = 1, 2, . . . , N

Fix the parses and update Θ and w
until Convergence

Algorithm 2 Decoding via Dual Decomposition
Input: Sentence x, fixed parameters w and Θ
Initialize vector u of size n× n to 0
repeat

ŷ = arg miny∈Y F (x,y; Θ) + uTy
ẑ = arg minz∈Y G(x, z; w)− uT z
if ŷ = ẑ then

return ŷ
else

u = u− τ (ŷ − ẑ)
end if

until Convergence

3.2 Joint Decoding
Given a training sample x and parameters w,Θ,
the goal of decoding is to find the best parse tree:

ŷ = arg min
y∈Y

1
2n
||y−Xw||22−µvTy−logPΘ(x,y)

We employ the dual decomposition algorithm to
solve this problem (shown in Algorithm 2), where
τ represents the step size.

The most important part of the algorithm is
solving the two separate decoding problems:

ŷ = arg min
y∈Y
− log(PΘ(x,y)f(x,y)) + uTy

ẑ = arg min
z∈Y

1
2n
||z−Xw||22 − µvT z− uT z

The first decoding problem can be solved by a
modified CYK parsing algorithm that takes into
account the information in vector u. The second
decoding problem can be solved using the same al-
gorithm of Grave and Elhadad (2015) (we use the
projective version in our approach).

4 Experiments

4.1 Setup
We use UD Treebank 1.4 as our datasets. We sort-
ed the datasets in the treebank by the number of

1691



training sentences of length ≤ 15 and selected the
top thirty datasets, which is similar to the setup of
Noji et al. (2016). For each dataset, we trained our
method on the training data with length ≤ 15 and
tested our method on the testing data with length≤
40. We tuned the hyper-parameters of our method
on the dataset of the English language and re-
ported the results on the thirty datasets without
any further parameter tuning. We compared our
method with four baselines. The first two base-
lines are Convex-MST and LC-DMV that are in-
dependently trained. To construct the third base-
line, we used the independently trained Convex-
MST baseline to parse all the training sentences
and then used the parses to initialize the training
of LC-DMV. This can be seen as a simple method
to combine two different approaches. On the other
hand, we did not use the LC-DMV baseline to ini-
tialize Convex-MST training because the objective
function of Convex-MST is convex and therefore
the initialization does not matter.

4.2 Results
In Table 1, we compare our jointly trained models
with the four baselines. We can see that with joint
training and independent decoding, LC-DMV and
Convex-MST can achieve superior overall perfor-
mance than when they are separately trained with
or without mutual initialization. Joint decoding
with our jointly trained models performs worse
than independent decoding. We made the same
observation when applying joint decoding to the
separately trained models (not shown in the table).
We believe this is because unsupervised parsers
have relatively low accuracy and forcing them to
reconcile would not lead to better parses. On the
other hand, joint decoding during training helps
propagate useful inductive biases between models
and thus leads to better trained models.

4.3 Analysis of Parsing Results
We analyze the parsing results from the two mod-
els to see how they benefit each other with join-
t training. Note that LC-DMV limits the depth
of center embedding and encourages shorter de-
pendency length, while Convex-MST encourages
dependencies satisfying pre-specified linguistic
rules. Therefore, we would like to see whether
the jointly-trained LC-DMV produces more de-
pendencies satisfying the linguistic priors than it-
s separately-trained counterpart, and whether the
jointly-trained Convex-MST produces parse trees

Language M D D-I M-J D-J DD
A Greek 43.4 33.1 38.8 44.2 44.9 38.9
A Greek-P 50.4 43.0 44.7 50.8 52.9 44.9
Basque 50.0 45.4 54.2 52.1 55.7 50.2
Bulgarian 61.6 62.4 60.3 64.7 73.8 64.8
Czech 48.6 17.4 53.9 48.7 54.0 53.5
Czech-CAC 50.4 53.0 53.9 55.6 62.3 50.2
Dutch 45.3 34.1 56.7 48.2 43.5 40.7
Dutch-LS 42.4 27.0 16.4 43.2 41.2 36.3
English 54.0 56.0 49.8 57.3 60.1 53.4
Estonian 49.4 31.8 47.5 48.7 44.0 44.4
Finnish 44.7 26.9 39.0 44.2 43.5 31.2
Finnish-FTB 49.9 31.0 47.9 47.7 48.0 36.5
French 62.0 48.6 57.0 54.5 57.0 55.5
German 51.4 50.5 54.1 49.3 55.7 48.6
Gothic 52.7 49.9 47.3 59.6 56.4 58.0
Hindi 56.8 54.2 48.4 52.1 60.0 49.1
Italian 69.1 71.1 67.4 62.8 70.3 64.5
Japanese 44.8 43.8 43.8 42.8 45.8 41.0
Latin-ITTB 38.8 38.6 42.3 47.0 42.2 40.3
Latin-PROIEL 44.3 34.8 38.7 46.8 41.8 42.9
Norwegian 55.3 45.5 51.4 57.4 60.8 46.6
Old Church S 56.4 26.6 51.3 58.3 58.6 42.0
Polish 63.4 63.7 61.5 70.7 74.2 68.9
Portuguese 57.9 67.2 60.1 56.1 62.9 57.4
Portuguese-BR 59.3 63.1 62.0 65.5 68.8 58.3
Russian-STR 47.6 51.7 56.5 52.1 64.4 52.6
Slovak 57.4 59.3 51.9 61.7 65.9 58.7
Slovenian 54.0 49.5 56.3 65.5 69.6 56.1
Spanish 61.9 61.9 60.3 57.4 68.0 60.2
Spanish-AC 59.4 59.5 56.4 56.8 65.2 57.6
Average 52.7 47.2 50.3 54.2 56.5 49.6
Average ≤ 15 55.4 48.9 54.9 57.3 60.2 53.8

Table 1: Directed dependency accuracy on thirty datasets
with test sentences of length ≤ 40. The last row indicates
the average directed accuracy on sentences of length ≤ 15.
M (Convex-MST) and D (LC-DMV) are the independently
trained baselines. D-I is the third baseline in which the LC-
DMV training is initialized by the parses produced from the
trained Convex-MST model. With our jointly trained models,
M-J and D-J denote separate decoding and DD denotes joint
decoding.

with less center embedding and shorter dependen-
cies than its separately-trained counterpart.

Figure 1 shows the percentages of dependencies
satisfying linguistic rules when using the separate-
ly and jointly trained LC-DMV to parse the test
sentences in the English dataset. As we can see,
with joint training, LC-DMV is indeed influenced
by Convex-MST and produces more dependencies
satisfying linguistic rules.

Table 2 shows the average dependency length
when using the separately and jointly trained
Convex-MST to parse the English test dataset. The
dependency length can be seen to decrease with
joint training, showing the influence from LC-
DMV. As to center embedding depth, we find that
separately trained Convext-MST already produces
very few center embeddings of depth 2 or more,

1692



D
ep

en
de

nc
y 

A
rc

 P
er

ce
nt

ag
e

20

30

40

50

60

Noun Verb All

59.8

32.3

26.4

55.9

31.7

23.2

Separate Training Joint Training

Figure 1: Percentages of dependencies satisfying linguistic
rules in the LC-DMV parses of the English test dataset. Noun
and Verb denote dependencies headed by nouns and verbs.

Methods Average Dependency Length
Separate Training 1.673

Joint Training 1.627

Table 2: Average dependency length in the Convex-MST
parses of the English test dataset.

so the influence from the center embedding con-
straint of LC-DMV during joint training is not ob-
vious. We note that the influence on Convex-MST
from LC-DMV during joint training is relatively
small, which may contribute to the much small-
er accuracy improvement (1.5%) of Convex-MST
with joint training in comparison with the 9.3%
improvement of LC-DMV. We conducted an ad-
ditional experiment that scaled down the Convex-
MST objective in joint training in order to in-
crease the influence of LC-DMV. The results show
that LC-DMV indeed influences Convex-MST to
a greater degree, but the parsing accuracies of the
two models decrease.

5 Conclusion

In this paper, we proposed a new learning strategy
for unsupervised dependency parsing that learns a
generative model and a discriminative model joint-
ly based on dual decomposition. We show that
with joint training, two state-of-the-art models can
positively influence each other and achieve better
performance than their separately trained counter-
parts.

References

Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association

for Computational Linguistics, pages 582–590. As-
sociation for Computational Linguistics.

Yonatan Bisk and Julia Hockenmaier. 2012. Simple ro-
bust grammar induction with combinatory categorial
grammars.

Shay B Cohen, Kevin Gimpel, and Noah A Smith.
2008. Logistic normal priors for unsupervised prob-
abilistic grammar induction. In Advances in Neural
Information Processing Systems, pages 321–328.

Shay B Cohen and Noah A Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying
in unsupervised grammar induction. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistic-
s, pages 74–82. Association for Computational Lin-
guistics.

George B Dantzig and Philip Wolfe. 1960. Decom-
position principle for linear programs. Operations
research, 8(1):101–111.

Edouard Grave and Noémie Elhadad. 2015. A con-
vex and feature-rich discriminative approach to de-
pendency grammar induction. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1375–1384, Beijing,
China. Association for Computational Linguistics.

Yong Jiang, Wenjuan Han, and Kewei Tu. 2016. Un-
supervised neural dependency parsing. In Proceed-
ings of the 2016 Conference on Empirical Method-
s in Natural Language Processing, pages 763–771,
Austin, Texas. Association for Computational Lin-
guistics.

Dan Klein and Christopher D Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 478. Association for Com-
putational Linguistics.

Nikos Komodakis, Nikos Paragios, and Georgios Tzir-
itas. 2007. Mrf optimization via dual decomposi-
tion: Message-passing revisited. In Computer Vi-
sion, 2007. ICCV 2007. IEEE 11th International
Conference on, pages 1–8. IEEE.

Terry Koo, Alexander M Rush, Michael Collins, Tom-
mi Jaakkola, and David Sontag. 2010. Dual de-
composition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1288–1298. Association for Compu-
tational Linguistics.

Tao Lei, Yuan Zhang, Lluı́s Màrquez, Alessandro
Moschitti, and Regina Barzilay. 2015. High-order
low-rank tensors for semantic role labeling. In
Proceedings of the 2015 Conference of the North

1693



American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1150–1160, Denver, Colorado. Association
for Computational Linguistics.

Percy S Liang, Dan Klein, and Michael I. Jordan. 2008.
Agreement-based learning. In J. C. Platt, D. Koller,
Y. Singer, and S. T. Roweis, editors, Advances in
Neural Information Processing Systems 20, pages
913–920. Curran Associates, Inc.

Mingbo Ma, Liang Huang, Bing Xiang, and Bowen
Zhou. 2015. Dependency-based convolutional neu-
ral networks for sentence embedding. arXiv preprint
arXiv:1507.01839.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd an-
nual meeting on association for computational lin-
guistics, pages 91–98. Association for Computation-
al Linguistics.

Hiroshi Noji, Yusuke Miyao, and Mark Johnson. 2016.
Using left-corner parsing to encode universal struc-
tural constraints in grammar induction. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 33–43,
Austin, Texas. Association for Computational Lin-
guistics.

Alexander M. Rush and Michael Collins. 2012. A tuto-
rial on dual decomposition and lagrangian relaxation
for inference in natural language processing. J. Ar-
tif. Int. Res., 45(1):305–362.

Alexander M Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1–11. Association for Computa-
tional Linguistics.

Noah A Smith and Jason Eisner. 2006. Annealing
structural bias in multilingual weighted grammar in-
duction. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 569–576. Association for
Computational Linguistics.

1694


