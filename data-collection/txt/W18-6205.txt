



















































Creating a Dataset for Multilingual Fine-grained Emotion-detection Using Gamification-based Annotation


Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 24–30
Brussels, Belgium, October 31, 2018. c©2018 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17

24

Creating a Dataset for Multilingual Fine-grained Emotion-detection
Using Gamification-based Annotation

Emily Öhman Kaisla Kajava Jörg Tiedemann Timo Honkela

University of Helsinki
firstname.lastname@helsinki.fi

Abstract

This paper introduces a gamified framework
for fine-grained sentiment analysis and emo-
tion detection. We present a flexible tool, Sen-
timentator, that can be used for efficient an-
notation based on crowd sourcing and a self-
perpetuating gold standard. We also present
a novel dataset with multi-dimensional anno-
tations of emotions and sentiments in movie
subtitles that enables research on sentiment
preservation across languages and the creation
of robust multilingual emotion detection tools.
The tools and datasets are public and open-
source and can easily be extended and applied
for various purposes.

1 Introduction

Sentiment analysis and emotion detection is a cru-
cial component in many practical applications but
also defines a great challenge in natural language
processing and artificial intelligence. Detecting
emotions is crucial in human-computer interac-
tion, and human behavior in communication is to
a large degree affected by the emotional states that
are created in a message. These states are typi-
cally fine-grained and fuzzy covering various di-
mensions of human feelings and attitudes. Nev-
ertheless, it is often the practice to consider sen-
timents and emotions as very coarse and discrete
features that can be detected with simple classi-
fiers on a small scale of a few classes.

In our work, we focus on a high-dimensional
model of emotions that allows a more natural and
fine-grained classification and, furthermore, we
tackle emotion detection in a multilingual setting.
One of the biggest issues that stand in the way of
creating reliable emotion detection algorithms is
the lack of properly annotated datasets for training
and testing purposes, especially in the case of the
dimensionality that we consider and the multilin-
gual support that we envision. This is the reason

why we created Sentimentator, a new annotation
tool that facilitates the efficient creation of appro-
priate datasets (Öhman and Kajava, 2018).

The main contribution of the paper is the frame-
work based on a gamified environment that we
develop to efficiently build large-scale resources.
Our setup results in a self-perpetuating gold stan-
dard, which is initialized by seed sentences that are
annotated by experts and augmented by crowd an-
notators. A combination of correlation-based scor-
ing and ranking makes it possible to build datasets
with weighted judgments based on the annotator
confidence that we measure. Initial rankings are
based on the comparison to seed annotation only
but they will be adjusted dynamically once the cor-
relation between crowd annotators allows to esti-
mate further reliability scores. The main idea is
that we can trust annotators that provide identical
or at least similar judgments as other reliable an-
notators. With this scheme, we can move away
from the use of limited seed sentences for con-
fidence estimation to a more dynamic and self-
perpetuating gold standard.

Another fundamental decision in our setup is
the use of multilingual material on which to base
our annotations. We are interested in the cross-
lingual use of emotions and the development of
multilingual classifiers (see Öhman et al., 2016).
Therefore, we start with sentences extracted from
movie subtitles (English originals in our case) for
which we also have plenty of translations into a
large number of languages. Movies contain a lot
of emotional content and, as a side effect, it is in-
teresting to see how that is reflected in subtitles
and their translations.

Before presenting Sentimentator itself, we will
first discuss related work and the theoretical
framework we work with. The presentation of the
seed/pilot dataset and its application for emotion
detection follows the description of the tool and



25

ends with a concluding discussion.

2 Related Work

Sentiment analysis is a widely studied task in nat-
ural language processing. Most of the existing
datasets applied in sentiment analysis use binary
or ternary annotation schemes (positive-negative,
or positive, negative, and neutral) (Andreevskaia
and Bergler, 2007), or some kind of combination
of these (i.e. the addition of e.g. ”mixed” (Saif
et al., 2013)). This is not enough if the aim is to
detect emotions rather than overarching sentiment
(de Albornoz et al., 2012; Li and Hovy, 2017;
Cambria et al., 2013). Furthermore, many of the
existing datasets or tools (Munezero et al., 2015;
Eryigit et al., 2013; Musat et al., 2012; Kakko-
nen and Kakkonen, 2011; Calefato et al., 2017;
Saif et al., 2013; Abdul-Mageed and Ungar, 2017)
are domain-dependent (often Twitter data) and/or
document-level. Very few of these are also open
data or open source.

An important questions is whether to show
wider context or not. Boland et al. (2013) show
that context can lead to the effect of double
weighting for fine-grained annotations. For that
reason, we also opted for the annotation of isolated
sentences even though our tools would easily sup-
port other setups.

2.1 Crowd-sourcing Annotations

The annotation of datasets can be very costly and
time consuming (Andreevskaia and Bergler, 2007;
Devitt and Ahmad, 2008) if done by expert anno-
tators. Crowd-sourcing can often be a cheaper al-
ternative to hiring expert annotators, and has been
used successfully by several researchers to create
different types of datasets (Turney, 2002; Green-
hill et al., 2014; Mohammad and Turney, 2013).

However, one issue with using non-experts to
solicit annotations is that there is a risk of the qual-
ity suffering. Our solution to annotation-reliability
related issues is gamification, which will be dis-
cussed in detail in section 3.

2.2 Theory of Emotion

The underlying theory of emotion for Sentimen-
tator is Plutchik’s theory of emotion (Plutchik,
1980). The eight core emotions he proposes are
anger, anticipation, disgust, fear, joy, sadness,
surprise, and trust. He uses a wheel, or flower,
to illustrate these emotions. For a more intuitive

interface, we have inverted the wheel (see figure
1).

Figure 1: Inverted wheel of emotions

Although Sentimentator takes into account the
intensity of the emotions, the complexity of the
annotation task does not increase linearly with the
number of classes this produces. It is possible to
annotate for all 24+1 emotions, but only use the
eight core emotions for classification, which was
done very successfully by Abdul-Mageed and Un-
gar (2017).

Using Euclidean distance on the inverted wheel
to calculate the similarity of annotations, we can
see that annotations for neutral and low intensity
emotions are in fact quite similar. This means we
can avoid unnecessarily dismissing an annotation
as noise, as might be the case if a more traditional
interface was used where neutral was a separate
category from low-intensity emotions.

2.3 Classification

Table 1 shows the accuracies achieved by a
few other multidimensional approaches (generally
those of Ekman (1971)) using various classifi-
cation methods such as SVMs, neural networks,
maximum entropy, Naı̈ve Bayes, and k-Nearest
Neighbor to name a few. When more classes are
included in a model, accuracies achieved are typi-
cally lower than what binary or ternary models of
sentiment analysis achieve (Purver and Battersby,
2012; Tokuhisa et al., 2008), with the exception
of Abdul-Mageed and Ungar (2017) who apply a
gated recurrent neural network model.

1Quoted in Purver and Battersby (2012)



26

Study classes accuracy
Go et al. (2009) 2 82.2-83%
Danisman and Alpkocak (2008) 6 32%
Chuang and Wu (2004) 6+1 56-74%
Chuang and Wu (2004) 6+1, audio 81.5%
Ansari 20101 6 81%
Purver and Battersby (2012) 6 varies
Seol et al. (2008) 8 45-65%
Abdul-Mageed and Ungar (2017) 8 95.68%
Tokuhisa et al. (2008) 10 up to 80%
Abdul-Mageed and Ungar (2017) 24 87.58%

Table 1: Accuracies achieved by previous studies

Although the data and methods are different,
it seems reasonable to expect accuracies between
30-70% depending on the category for an initial
multiclass classification. Depending on the avail-
ability, we will try to apply our model to the same
datasets that have been used in the studies listed in
table 1 in order to directly compare results.

3 Gamifying the Annotation Platform

Our goal is to implement efficient crowd-sourcing
through gamification. Gamification refers the use
of game elements in environments that are not typ-
ically games (Deterding et al., 2011; Hamari and
Koivisto, 2013). Previous research shows that one
can achieve a high number of quality annotations
by non-experts by using carefully considered gam-
ified aspects such as (1) Relatedness (connected
to other players), (2) Competence (mastering the
game problems), and (3) Autonomy (control of
own life) (Musat et al., 2012).

Robson et al. (2015) posits that gamification
can change behavior by tapping into motivational
drivers of human behavior: reinforcements and
emotions. The emotions we want to elicit are
of course enjoyment, but negative emotions such
as disappointment can also increase commitment
and a desire to increase one’s competence. Sim-
ilarly, both positive and negative reinforcements
increase repetitive behavior in players (Robson
et al., 2015).

Our platform offers players leaderboards and
statistics about both their immediate and longterm
progress (relatedness). Progress, rank and prestige
are important measures that help players feel com-
pensated for the work they are doing within the
game (competence). Rank has an additional func-
tion in our platform; as we lack a gold standard
against which to compare the annotations we re-
ceive, we use rank to determine noisy annotations

and noisy annotators. Furthermore, the player can
see how each of their choice affects their standing
in the ranks.

The dataset that the experiments in this paper
rely on is our validated seed sentences. These
sentences will be used as a type of seeded gold
standard. What this means is that annotators will
annotate both non-seed sentences and seed sen-
tences. They receive a score from their annota-
tion based on similarity with gold annotations that
determines their rank. In practice, rank is equiv-
alent with confidence level. With enough partic-
ipants, players will also be ranked according to
how closely their annotations match those of high-
ranked annotators. We, thus, have the option to
include only the least noisy, highest quality anno-
tations in our dataset.

In order to compare annotations, we map them
on the inverted Plutchik’s wheel we propose in
Figure 1 projected on a standard two-dimensional
space with coordinates [x, y], where the least in-
tense emotions are at the center and the most in-
tense at the tips of the petals. The origin of our
emotion space is located in the center of the wheel
and represents the case of a neutral expression.

We can then calculate the distance D(Gx, Gy)
between any pair of points corresponding to emo-
tion labels GX and Gy by computing the Eu-
clidean distance normalized by the maximum dis-
tance that can be observed between opposite emo-
tions with maximum intensity. Assuming that ge-
ometric location expresses the relatedness of emo-
tions, this distance metric takes into account all
different types of similarities/dissimilarities be-
tween annotations, including labels that combine
neighboring emotions.

The distance metric is the basis for the compu-
tation of annotation confidence Cx for new anno-
tation Gx that we obtain. We define annotation
confidence as

Cx = Rannotator ∗
1

N

n∑
n=1

(1− Cn ∗D(Gx, Gn))

where Gn ∈ {G1, .., GN} are annotations of
the same instance (sentence) from the current gold
standard with corresponding confidence scores
{C1, .., CN}.2 In other words, we add an aver-
aged penalty for annotations that differ from exist-
ing gold annotations weighted by their confidence
scores. Note that our seed annotations Gs obtain

2Note that we set Cx = Rannotator if N = 0.



27

a perfect confidence Cs = 1. Another component
of the confidence score is the rank of the annota-
tor based on the score Rannotator. This score is
initialized with one and will be updated by each
submitted annotation. Currently, we use a simple
average over annotation confidence scores of that
particular annotator:

Rannotator =
1

M

M∑
m=1

Cm

Our self-perpetuating gold standard with
ranking-based confidence ratings will reduce the
need for manual screening and will ensure that we
can receive consistent emotion annotations with a
measurable confidence attached to them.

4 Creating the Dataset

For the seed data, we used the following proce-
dure: On completed expert annotation, another ex-
pert annotates the same sentences with the data or-
der randomized. Ambiguous sentences were re-
viewed and the correct class was agreed upon. In
some cases where no agreement could be reached
the sentence was excluded from the seed dataset.

Our data collection will be unique in that it will
provide a fine-grained multi-dimensional open
source dataset for sentiment analysis and emo-
tion detection in various languages. Annotation is
on-going and the first real dataset will be avail-
able later in 2018. For now we have a set of
sentences with validated annotations that we will
use as our seed data to get the gamified annota-
tion started. This dataset has already been used
to investigate sentiment preservation in Finnish,
French, and Italian (Kajava, 2018).

We wanted to make the dataset as useful as pos-
sible to as many researchers as possible from the
beginning. This is why we selected an open par-
allel corpus, namely the OPUS movie subtitles
corpus (Tiedemann, 2012; Lison and Tiedemann,
2016). From this collection, approximately 9,000
English sentences were annotated into the follow-
ing emotion classes: anger, anticipation, disgust,
joy, fear, sadness, surprise, and trust. This yielded
a preliminary dataset of between 649 to 908 sen-
tences per class (see table 4). For the classification
experiments, we did not take into account the mea-
sure of emotion intensity that we introduce later in
our annotation framework, which is also part of
the seed sentence annotation.

ang ant dis fea joy sad sur tru Total
train 816 739 775 615 876 633 583 737 5,774
test 92 84 87 70 99 72 66 83 653

Table 2: Emotion distribution in seed data

We also keep the metadata and therefore all
sentences can be paired with a particular movie,
genre, time period, as well as its counterpart in an-
other language. This is valuable information for
future research avenues.

We expect to have a full dataset by the end of the
year as we will be collecting at least 100 000 anno-
tations in September-October of 2018 from crowd
annotators (students). Snow et al. (2008) suggest
using four non-experts to match the quality of one
expert annotator, however, gamification, seed sen-
tences, and rank-validation means that fewer an-
notations per sentence might be sufficient using
our platform. Inter-annotator agreement is on av-
erage around 70-90% depending on the type of
annotation and who is doing the annotation work
(expert vs. non-expert) (Nowak and Rüger, 2010),
and this is where annotator agreement was for our
data as well, varying between classes.

Based on initial timed annotations, a typical an-
notator can be expected to annotate up to 10 sen-
tences per minute. This means that it only takes
just over one hour to annotate around 600 sen-
tences. If every annotator is asked to annotate
1000 sentences, this should not take more than
a few hours each on average taking the learning
curve into account. The students are encouraged to
annotate in languages other than English as well,
resulting in at least two or three separate datasets
with an expected minimum of 40 000 annotations
each. We currently have preliminary datasets for
English, Italian, French, and Finnish.

5 Validation of the Data Quality

In order to test the quality of the data for the pur-
pose of developing an automatic emotion detector
we ran some initial experiments using our anno-
tated seed data for training, and evaluating stan-
dard multi-class classifiers. The data was tok-
enized and lowercased as a preprocessing step. We
selected Multinomial Naı̈ve Bayes (NB) and Mul-
tilayer Perceptron (MLP) classifiers for our exper-
iments. For the classifiers we use the scikit-learn
(Pedregosa et al., 2011) machine learning toolkit.
In both classification scenarios, the data was split
into class-stratified training and test sets of 90%



28

and 10%, respectively.
The MLP network used in this work is a three-

layer network. The model creates a lexicon from
the dataset using a bag-of-words approach, em-
ploying it for extracting a set of features for each
class. We use Adam for training the network
and apply Rectified Linear Unit (ReLu) activiation
functions in the hidden layers of feed-forward net-
work.

Classifier Accuracy
NB 0.5069
MLP 0.5023

Table 3: Overall classification accuracy

As can be seen in table 3, the baseline classi-
fiers perform reasonably well for such a small data
set and such a fine-grained task. Note that we did
spend any time on optimizing features and hyper-
parameters to obtain a better performance. The
purpose of this study is entirely to test the feasi-
bility of fine-grained classification and validity of
our seed data.

In the confusion matrix for the best perform-
ing classifier (see Table 4), we can see that there
is some significant confusion between anger and
fear, between disgust and sadness and also surpris-
ingly between trust and fear. These are the same
classes that others have struggled to distinguish
(e.g. (Purver and Battersby, 2012)), which is re-
assuring that our data is in good shape. With this
promising performance on our pilot data set (de-
spite its limited size) we are encouraged to pro-
ceed with future experiments and the more fine-
grained distinctions we propose that take intensity
into account.

These experiments also demonstrate that the
seed data is sufficient for initial classifications and
that we can go ahead in developing our gami-
fied strategy of getting more annotations based on
correlations between annotators and their level of
trust, which will initially be based on the compar-
ison to the validated seed sentences.

6 Conclusions

In this paper, we present an open annotation tool
for fine-grained emotion detection and a dataset
of seed sentences that can be used to gamify the
annotation efforts. The classification results show
that the dataset is reliable enough to be used as
seed sentences, indicating that gamification based

ang ant dis fea joy sad sur tru <- classified as
62 3 10 3 5 1 3 5 anger
9 43 6 1 5 1 5 14 anticipation
25 6 29 5 9 3 3 7 disgust
11 8 6 20 2 6 5 12 fear
2 4 5 1 77 2 3 5 joy
7 2 8 5 12 30 2 6 sadness
5 4 10 4 13 1 25 4 surprise
5 3 5 3 16 3 2 45 trust

Table 4: Confusion matrix for NB-based classification
of the test set.

on the seed data is a viable option for compil-
ing sentiment datasets, and that multidimensional
classification yields acceptable results even with
a small dataset. However, already with our lim-
ited validation experiments we can see that the
choice of model and learning algorithm influences
the quality of the resulting classifier. This is an im-
portant outcome that needs to be considered when
designing tools with scarce resources. We will
continue to monitor performance to measure the
impact of gamification and cross-lingual transfer
on classification performance.

7 Discussion and Future Work

As our system collects both coarse (ternary) senti-
ment annotations, and fine-grained emotion anno-
tations, a future option could be to apply the suc-
cessful approach demonstrated by Tokuhisa et al.
(2008), and utilize the coarser sentiment polarity
to pre-classify data before emotion classification,
and then implement a k-nearest neighbors algo-
rithm on the larger dataset. Our platform does not
show context by default, however, it is easy to add
the context of the target segment to be annotated if
required for a different type of project.

It might be valuable to re-annotate parts of
the data showing additional context to check the
impact on annotation and annotator confidence.
Tokuhisa et al. (2008) found that in their data,
context-dependent samples were useful for train-
ing their classifier and yielded slightly higher ac-
curacies than their non-context-dependent data.
Finally, we will also consider models that make
use of sentence-internal relations to improve the
classification results. In particular, we will inves-
tigate the use of sequence models and gated recur-
rent networks as proposed by Abdul-Mageed and
Ungar (2017).



29

References
Muhammad Abdul-Mageed and Lyle Ungar. 2017.

Emonet: Fine-grained emotion detection with gated
recurrent neural networks. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 718–728.

Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gervás. 2012. Sentisense: An easily scalable
concept-based affective lexicon for sentiment anal-
ysis.

Alina Andreevskaia and Sabine Bergler. 2007. Clac
and clac-nb: Knowledge-based and corpus-based
approaches to sentiment tagging. In Proceedings
of the 4th international workshop on semantic eval-
uations, pages 117–120. Association for Computa-
tional Linguistics.

Katarina Boland, Andias Wira-Alam, and Reinhard
Messerschmidt. 2013. Creating an annotated corpus
for sentiment analysis of german product reviews.

Fabio Calefato, Filippo Lanubile, and Nicole Novielli.
2017. Emotxt: a toolkit for emotion recognition
from text. arXiv preprint arXiv:1708.03892.

Erik Cambria, Björn Schuller, Yunqing Xia, and
Catherine Havasi. 2013. New avenues in opinion
mining and sentiment analysis. IEEE Intelligent
Systems, 28(2):15–21.

Ze-Jing Chuang and Chung-Hsien Wu. 2004. Multi-
modal emotion recognition from speech and text.
International Journal of Computational Linguistics
& Chinese Language Processing, Volume 9, Num-
ber 2, August 2004: Special Issue on New Trends of
Speech and Language Processing, 9(2):45–62.

Taner Danisman and Adil Alpkocak. 2008. Feeler:
Emotion classification of text using vector space
model. In AISB 2008 Convention Communica-
tion, Interaction and Social Intelligence, volume 1,
page 53.

Sebastian Deterding, Miguel Sicart, Lennart Nacke,
Kenton O’Hara, and Dan Dixon. 2011. Gamifi-
cation. using game-design elements in non-gaming
contexts. In CHI’11 extended abstracts on human
factors in computing systems, pages 2425–2428.
ACM.

Ann Devitt and Khurshid Ahmad. 2008. Sentiment
analysis and the use of extrinsic datasets in evalu-
ation. In LREC.

Paul Ekman. 1971. Universals and cultural differences
in facial expressions of emotion. In Nebraska sym-
posium on motivation. University of Nebraska Press.

Gülsen Eryigit, Fatih Samet Cetin, Meltem Yanik,
Tanel Temel, and Ilyas Çiçekli. 2013. Turksent:
A sentiment annotation tool for social media. In
LAW@ ACL, pages 131–134.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, 1(12).

Anita Greenhill, Kate Holmes, Chris Lintott, Brooke
Simmons, Karen Masters, Joe Cox, and Gary Gra-
ham. 2014. Playing with science: Gamised aspects
of gamification found on the online citizen science
project-zooniverse. In GAMEON’2014. EUROSIS.

Juho Hamari and Jonna Koivisto. 2013. Social moti-
vations to use gamification: An empirical study of
gamifying exercise. In ECIS, page 105.

Kaisla Kajava. 2018. Cross-lingual sentiment preser-
vation in binary and multi-dimensional classifica-
tion.

Tuomo Kakkonen and Gordana Galić Kakkonen. 2011.
Sentiprofiler: creating comparable visual profiles of
sentimental content in texts. Language Technolo-
gies for Digital Humanities and Cultural Heritage,
62:189–204.

Jiwei Li and Eduard Hovy. 2017. Reflections on senti-
ment/opinion analysis. In A Practical Guide to Sen-
timent Analysis, pages 41–59. Springer.

Pierre Lison and Jörg Tiedemann. 2016. Opensub-
titles2016: Extracting large parallel corpora from
movie and tv subtitles.

Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a word-emotion association lexicon.
29(3):436–465.

Myriam Munezero, Calkin Suero Montero, Maxim
Mozgovoy, and Erkki Sutinen. 2015. Emotwitter–a
fine-grained visualization system for identifying en-
during sentiments in tweets. In International Con-
ference on Intelligent Text Processing and Compu-
tational Linguistics, pages 78–91. Springer.

Claudiu-Cristian Musat, Alireza Ghasemi, and Boi
Faltings. 2012. Sentiment analysis using a novel hu-
man computation game. In Proceedings of the 3rd
Workshop on the People’s Web Meets NLP: Collabo-
ratively Constructed Semantic Resources and Their
Applications to NLP, pages 1–9, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Stefanie Nowak and Stefan Rüger. 2010. How reliable
are annotations via crowdsourcing: a study about
inter-annotator agreement for multi-label image an-
notation. In Proceedings of the international con-
ference on Multimedia information retrieval, pages
557–566. ACM.

Emily Öhman, Timo Honkela, and Jörg Tiedemann.
2016. The challenges of multi-dimensional senti-
ment analysis across languages. PEOPLES 2016,
page 138.

Emily Öhman and Kaisla Kajava. 2018. Sentimenta-
tor: Gamifying fine-grained sentiment annotation.
In Digital Humanities in the Nordic Countries 2018.
CEUR Workshop Proceedings.



30

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

Robert Plutchik. 1980. A general psychoevolutionary
theory of emotion. Theories of emotion, 1:3–31.

Matthew Purver and Stuart Battersby. 2012. Experi-
menting with distant supervision for emotion classi-
fication. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 482–491. Association
for Computational Linguistics.

Karen Robson, Kirk Plangger, Jan H. Kietzmann, Ian
McCarthy, and Leyland Pitt. 2015. Is it all a game?
understanding the principles of gamification. Busi-
ness Horizons, 58(4):411 – 420.

Hassan Saif, Miriam Fernandez, Yulan He, and Harith
Alani. 2013. Evaluation datasets for twitter senti-
ment analysis: a survey and a new dataset, the sts-
gold.

Yong-Soo Seol, Dong-Joo Kim, and Han-Woo Kim.
2008. Emotion recognition from text using
knowledge-based ann. In ITC-CSCC: International
Technical Conference on Circuits Systems, Comput-
ers and Communications, pages 1569–1572.

Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast—but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the conference
on empirical methods in natural language process-
ing, pages 254–263. Association for Computational
Linguistics.

Jorg Tiedemann. 2012. Parallel data, tools and inter-
faces in opus. In Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC’12), Istanbul, Turkey. European Lan-
guage Resources Association (ELRA).

Ryoko Tokuhisa, Kentaro Inui, and Yuji Matsumoto.
2008. Emotion classification using massive exam-
ples extracted from the web. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 881–888. Association
for Computational Linguistics.

Peter D. Turney. 2002. Thumbs up or thumbs down?:
Semantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 417–424, Stroudsburg, PA, USA.
Association for Computational Linguistics.


