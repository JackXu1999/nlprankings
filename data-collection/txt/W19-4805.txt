



















































Multi-Granular Text Encoding for Self-Explaining Categorization


Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 41–45
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

41

Multi-Granular Text Encoding for Self-Explaining Categorization
Zhiguo Wang1, Yue Zhang2, Mo Yu1, Wei Zhang1, Lin Pan1

Linfeng Song3, Kun Xu3, Yousef El-Kurdi1
1IBM T.J. Watson Research Center, Yorktown Heights, NY 10598

2School of Engineering, Westlake University, China
3Tencent AI Lab

zgw.tomorrow@gmail.com

Abstract

Self-explaining text categorization requires a
classifier to make a prediction along with sup-
porting evidence. A popular type of evidence
is sub-sequences extracted from the input text
which are sufficient for the classifier to make
the prediction. In this work, we define multi-
granular ngrams as basic units for explanation,
and organize all ngrams into a hierarchical
structure, so that shorter ngrams can be reused
while computing longer ngrams. We lever-
age a tree-structured LSTM to learn a context-
independent representation for each unit via
parameter sharing. Experiments on medical
disease classification show that our model is
more accurate, efficient and compact than BiL-
STM and CNN baselines. More importantly,
our model can extract intuitive multi-granular
evidence to support its predictions.

1 Introduction

Increasingly complex neural networks have
achieved highly competitive results for many
NLP tasks (Vaswani et al., 2017; Devlin et al.,
2018), but they prevent human experts from
understanding how and why a prediction is made.
Understanding how a prediction is made can be
very important for certain domains, such as the
medical domain. Recent research has started to
investigate models with self-explaining capability,
i.e. extracting evidence to support their final
predictions (Li et al., 2015; Lei et al., 2016;
Lin et al., 2017; Mullenbach et al., 2018). For
example, in order to make diagnoses based on
the medical report in Table 1, the highlighted
symptoms may be extracted as evidence.

Two methods have been proposed on how to
jointly provide highlights along with classifica-
tion. (1) an extraction-based method (Lei et al.,
2016), which first extracts evidences from the
original text and then makes a prediction solely
based on the extracted evidences; (2) an attention-
based method (Lin et al., 2017; Mullenbach et al.,
2018), which leverages the self-attention mecha-

Medical Report: The patient was admitted to the
Neurological Intensive Care Unit for close observa-
tion. She was begun on heparin anticoagulated

carefully secondary to the petechial bleed . She
started weaning from the vent the next day. She
was started on Digoxin to control her rate and her
Cardizem was held. She was started on antibiotics
for possible aspiration pneumonia . Her chest x-

ray showed retrocardiac effusion . She had some
bleeding after nasogastric tube insertion .

Diagnoses: Cerebral artery occlusion; Unspecified es-
sential hypertension; Atrial fibrillation; Diabetes melli-
tus.

Table 1: A medical report snippet and its diagnoses.

nism to show the importance of basic units (words
or ngrams) through their attention weights.

However, previous work has several limitations.
Lin et al. (2017), for example, take single words as
basic units, while meaningful information is usu-
ally carried by multi-word phrases. For instance,
useful symptoms in Table 1, such as “bleeding af-
ter nasogastric tube insertion”, are larger than a
single word. Another issue of Lin et al. (2017)
is that their attention model is applied on the rep-
resentation vectors produced by an LSTM. Each
LSTM output contains more than just the infor-
mation of that position, thus the real range for
the highlighted position is unclear. Mullenbach
et al. (2018) defines all 4-grams of the input text
as basic units and uses a convolutional layer to
learn their representations, which still suffers from
fixed-length highlighting. Thus the explainability
of the model is limited. Lei et al. (2016) intro-
duce a regularizer over the selected (single-word)
positions to encourage the model to extract larger
phrases. However, their method can not tell how
much a selected unit contributes to the model’s de-
cision through a weight value.

In this paper, we study what the meaningful
units to highlight are. We define multi-granular
ngrams as basic units, so that all highlighted symp-
toms in Table 1 can be directly used for explain-
ing the model. Different ngrams can have over-
lap. To improve the efficiency, we organize all



42

… … … … …

Text representation

Input text

Text Encoder

Basic unit representation

./(

2

/" /# /3

4" 4# 4( 43

… …
Prediction

Figure 1: A generic architecture.

ngrams into a hierarchical structure, such that the
shorter ngram representations can be reused to
construct longer ngram representations. Experi-
ments on medical disease classification show that
our model is more accurate, efficient and compact
than BiLSTM and CNN baselines. Furthermore,
our model can extract intuitive multi-granular evi-
dence to support its predictions.

2 Generic architecture and baselines

Our work leverages the attention-based self-
explaining method (Lin et al., 2017), as shown in
Figure 1. First, our text encoder (§3) formulates an
input text into a list of basic units, learning a vec-
tor representation for each, where the basic units
can be words, phrases, or arbitrary ngrams. Then,
the attention mechanism is leveraged over all basic
units, and sums up all unit representations based
on the attention weights {α1, ..., αn}. Eventually,
the attention weight αi will be used to reveal how
important a basic unit hi is. The last prediction
layer takes the fixed-length text representation t
as input, and makes the final prediction.

Baselines: We compare two types of baseline
text encoders in Figure 1. (1) Lin et al. (2017)
(BiLSTM), which formulates single word posi-
tions as basic units, and computes the vector hi
for the i-th word position with a BiLSTM; (2) Ex-
tension of Mullenbach et al. (2018) (CNN). The
original model in (Mullenbach et al., 2018) only
utilizes 4-grams. Here we extend this model to
take all unigrams, bigrams, and up to n-grams as
the basic units.

For a fair comparison, both our approach and
the baselines share the same architecture, and the
only difference is the text encoder used.

(a) tree (b) pyramid

(c) left-branching forest

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

EMNLP 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

w1 w2 w3 w4

w1 w2 w3 w4

w1 w2 w3 w4

w2 w3 w4

w3 w4

000

001

002

003

004

005

006

007

008

009

010

011

012

013

014

015

016

017

018

019

020

021

022

023

024

025

026

027

028

029

030

031

032

033

034

035

036

037

038

039

040

041

042

043

044

045

046

047

048

049

050

051

052

053

054

055

056

057

058

059

060

061

062

063

064

065

066

067

068

069

070

071

072

073

074

075

076

077

078

079

080

081

082

083

084

085

086

087

088

089

090

091

092

093

094

095

096

097

098

099

EMNLP 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

w1 w2 w3 w4

w1 w2 w3 w4

w1 w2 w3 w4

w2 w3 w4

w3 w4

!"!# !$!%

!"!#!$!%

!"!# !$!%

!#!$!%!"!#!$

!"!#!$!%

!#!$

(d) right-branching forest

2

100

101

102

103

104

105

106

107

108

109

110

111

112

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

128

129

130

131

132

133

134

135

136

137

138

139

140

141

142

143

144

145

146

147

148

149

150

151

152

153

154

155

156

157

158

159

160

161

162

163

164

165

166

167

168

169

170

171

172

173

174

175

176

177

178

179

180

181

182

183

184

185

186

187

188

189

190

191

192

193

194

195

196

197

198

199

EMNLP 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Disease type: cardiovascular diseases
Document: Seven-pathogen tricuspid endocarditis in an intravenous drug abuser . Pitfalls in labo-
ratory diagnosis . Polymicrobial endocarditis is being reported with increasing frequency in drug
abusers . However , the full extent of infection may be unrecognized with routine blood culture
techniques because of the overgrowth of more fastidious organisms by other pathogens . This report
documents an intravenous drug abuser with the first reported case of tricuspid valve endocarditis in-
volving seven pathogens , discusses pitfalls of routine blood cultures and examines the role of the
laboratory in microbiologic diagnosis .

Table 1: Highlighted ngrams by our model, where darker colors means higher weights.

w1 w2 w3 w4

Figure 8: Right Branch.

w1 w2 w3

Figure 9: Right Branch.

w1 w2

Figure 10: Right Branch.

w1 w2 w3 w4

Figure 11: Left-branch forest.

w1 w2 w3 w4

Figure 12: Pyramid Structure.

2

100

101

102

103

104

105

106

107

108

109

110

111

112

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

128

129

130

131

132

133

134

135

136

137

138

139

140

141

142

143

144

145

146

147

148

149

150

151

152

153

154

155

156

157

158

159

160

161

162

163

164

165

166

167

168

169

170

171

172

173

174

175

176

177

178

179

180

181

182

183

184

185

186

187

188

189

190

191

192

193

194

195

196

197

198

199

EMNLP 2018 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.

Disease type: cardiovascular diseases
Document: Seven-pathogen tricuspid endocarditis in an intravenous drug abuser . Pitfalls in labo-
ratory diagnosis . Polymicrobial endocarditis is being reported with increasing frequency in drug
abusers . However , the full extent of infection may be unrecognized with routine blood culture
techniques because of the overgrowth of more fastidious organisms by other pathogens . This report
documents an intravenous drug abuser with the first reported case of tricuspid valve endocarditis in-
volving seven pathogens , discusses pitfalls of routine blood cultures and examines the role of the
laboratory in microbiologic diagnosis .

Table 1: Highlighted ngrams by our model, where darker colors means higher weights.

w1 w2 w3 w4

Figure 8: Right Branch.

w1 w2 w3

Figure 9: Right Branch.

w1 w2

Figure 10: Right Branch.

w1 w2 w3 w4

Figure 11: Left-branch forest.

w1 w2 w3 w4

Figure 12: Pyramid Structure.

!"!#!$!% !"!#!$!%

!"!#!$ !"!#!$!#!$!% !#!$!%

!"!# !"!# !#!$!#!$ !$!% !$!%

Figure 2: Structures for a sentence w1w2w3w4, where
each node corresponds to a phrase or ngram.

3 Multi-granular text encoder

We propose the multi-granular text encoder to deal
with drawbacks (as mentioned in the third para-
graph of Section 1) of our baselines.

Structural basic units: We define basic units
for the input text as multi-granular ngrams, orga-
nizing ngrams in four different ways. Taking a
synthetic sentence w1w2w3w4 as the running ex-
ample, we illustrate these structures in Figure 2
(a), (b), (c) and (d), respectively. The first is a
tree structure (as shown in Figure 2(a)) that in-
cludes all phrases from a (binarized) constituent
tree over the input text, where no cross-boundary
phrases exists. The second type (as shown in Fig-
ure 2 (b,c,d)) includes all possible ngrams from the
input text, which is a superset of the tree structure.
In order to reuse representations of smaller ngrams
while encoding bigger ngrams, all ngrams are or-
ganized into hierarchical structures in three differ-
ent ways. First, inspired by Zhao et al. (2015),
a pyramid structure is created for all ngrams as
shown in Figure 2(b), where leaf nodes are uni-
grams of the input text, and higher level nodes cor-
respond to higher-order ngrams. A disadvantage
of the pyramid structure is that some lower level
nodes may be used repeatedly while encoding
higher level nodes, which may improperly aug-
ment the influence of the repeated units. For exam-
ple, when encoding the trigram node “w1w2w3”,
the unigram node “w2” is used twice through two
bigram nodes “w1w2” and “w2w3”. To tackle
this issue, a left-branching forest structure is con-
structed for all ngrams as shown in Figure 2(c),
where ngrams with the same prefix are grouped
together into a left-branching binary tree, and, in
this arrangement, multiple trees construct a forest.
Similarly, we construct a right-branching forest as
shown in Figure 2(d).



43

Encoding: We leverage a tree-structured LSTM
composition function (Tai et al., 2015; Zhu et al.,
2015; Teng and Zhang, 2016) to compute node
embeddings for all structures in Figure 2. For-
mally, the state of each node is represented as a
pair of one hidden vector h and one memory rep-
resentation c, which are calculated by composing
the node’s label embedding x and states of its left
child 〈h l, c l〉 and right child 〈h r, c r〉 with gated
functions:

i = σ(W 1x+ U1l h
l + U1rh

r + b1) (1)

f l = σ(W 2x+ U2l h
l + U2rh

r + b2) (2)

f r = σ(W 3x+ U3l h
l + U3rh

r + b3) (3)

o = σ(W 4x+ U4l h
l + U4rh

r + b4) (4)

u = tanh(W 5x+ U5l h
l + U5rh

r + b5) (5)

c = i� u+ f l � h l + f r � h r (6)
h = o� tanh(c) (7)

where σ is the sigmoid activation function, � is
the elementwise product, i is the input gate, f l

and f r are the forget gates for the left and right
child respectively, and o is the output gate. We
set x as the pre-trained word embedding for leaf
nodes, and zero vectors for other nodes. The rep-
resentations for all units (nodes) can be obtained
by encoding each basic unit in a bottom-up order.

Comparison with baselines Our encoder is
more efficient than CNN while encoding big-
ger ngrams, because it reuses representations of
smaller ngrams. Furthermore, the same parame-
ters are shared across all ngrams, which makes our
encoder more compact, whereas the CNN base-
line has to define different filters for different or-
der of ngrams, so it requires much more parame-
ters. Experiments show that using basic units up
to 7-grams to construct the forest structure is good
enough, which makes our encoder more efficient
than BiLSTM. Since in our encoder, all ngrams
with the same order can be computed in parallel,
and the model needs at most 7 iterative steps along
the depth dimension for representing a given text
of arbitrary length.

4 Experiments

Dataset: We experiment on a public medical text
classification dataset.1 Each instance consists of
a medical abstract with an average length of 207

1https://github.com/SnehaVM/Medical-Text-
Classification

Figure 3: Influence of n-gram order.

Model Train Time Eval Time ACC #Param.

CNN 57.0 2.6 64.8 848,228
BiLSTM 92.1 4.6 64.5 147,928
LeftForest 30.3 1.4 66.2 168,228

Table 2: Efficiency evaluation.

tokens, and one label out of five categories in-
dicating which disease this document is about.
We randomly split the dataset into train/dev/test
sets by 8:1:1 for each category, and end up with
11,216/1,442/1,444 instances for each set.

Hyperparameters We use the 300-dimensional
GloVe word vectors pre-trained from the 840B
Common Crawl corpus (Pennington et al., 2014),
and set the hidden size as 100 for node embed-
dings. We apply dropout to every layer with a
dropout ratio 0.2, and set the batch size as 50. We
minimize the cross-entropy of the training set with
the ADAM optimizer (Kingma and Ba, 2014), and
set the learning rate is to 0.001. During training,
the pre-trained word embeddings are not updated.

4.1 Properties of the multi-granular encoder

Influence of the n-gram order: For CNN and our
LeftForest encoder, we vary the order of ngrams
from 1 to 9, and plot results in Figure 3. For
BiLSTM, we draw a horizontal line according
to its performance, since the ngram order does
not apply. When ngram order is less than 3,
both CNN and LeftForest underperform BiLSTM.
When ngram order is over 3, LeftForest outper-
forms both baselines. Therefore, in terms of accu-
racy, our multi-granular text encoder is more pow-
erful than baselines.

Efficiency: We set ngram order as 7 for both
CNN and our encoder. Table 2 shows the time
cost (seconds) of one iteration over the training set
and evaluation on the development set. BiLSTM
is the slowest model, because it has to scan over
the entire text sequentially. LeftForest is almost
2x faster than CNN, because LeftForest reuses
lower-order ngrams while computing higher-order



44

Model Accuracy

BiLSTM 62.7
CNN 62.5

Tree 63.8
Pyramid 63.7
LeftForest 64.6
RightForest 64.5
BiForest 65.2

Table 3: Test set results.

Figure 4: Effectiveness of the extracted evidence.

ngrams. This result reveals that our encoder is
more efficient than baselines.

Model size: In Table 2, the last two columns
show the accuracy and number of parameters for
each model. LeftForest contains much less param-
eters than CNN, and it gives a better accuracy than
BiLSTM with only a small amount of extra param-
eters. Therefore, our encoder is more compact.

4.2 Model performance

Table 3 lists the accuracy on the test set, where
BiForest represents each ngram by concatenat-
ing representations of this ngram from both the
LeftForest and the RightForest encoders. We get
several interesting observations: (1) Our multi-
granular text encoder outperforms both the CNN
and BiLSTM baselines regardless of the structure
used; (2) The LeftForest and RightForest encoders
work better than the Tree encoder, which shows
that representing texts with more ngrams is helpful
than just using the non-overlapping phrases from
a parse tree; (3) The LeftForest and RightForest
encoders give better performance than the Pyra-
mid encoder, which verifies the advantages of or-
ganizing ngrams with forest structures; (4) There
is no significant difference between the LeftFor-
est encoder and the RightForest encoder. How-
ever, by combining them, the BiForest encoder
gets the best performance among all models, in-
dicating that the LeftForest encoder and the Right-
Forest encoder complement each other for better
accuracy.

4.3 Analysis of explainability

Qualitative analysis The following text is a
snippet of an example from the dev set. We lever-
age our BiForest model to extract ngrams whose
attention scores are higher than 0.05, and use the
bold font to highlight them. We extracted three
ngrams as supporting evidence for its predicted
category “nervous system diseases”. Both the
spontaneous extradural spinal hematoma and the
spinal arteriovenous malformation are diseases re-
lated to the spinal cord, therefore they are good
evidence to indicate the text is about “nervous sys-
tem diseases”.

Snippet: Value of magnetic resonance imaging in spon-
taneous extradural spinal hematoma due to vascular mal-
formation : case report . A case of spinal cord compression

due to spontaneous extradural spinal hematoma is reported
. A spinal arteriovenous malformation was suspected on the
basis of magnetic resonance imaging. Early surgical explo-

ration allowed a complete neurological recovery .

Quantitative analysis For each instance in the
training set and the dev set, we utilize the atten-
tion scores from BiForest to sort all ngrams, and
create different copies of the training set and de-
velopment set by only keeping the first n impor-
tant words. We then train and evaluate a BiLSTM
model with the newly created dataset. We vary the
number of words n among {1, 2, 3, 4, 5, 6, 7, 8,
9, 10, 20, 30, 40, 50}, and show the corresponding
accuracy with the green triangles in Figure 4. We
define a Random baseline by randomly selecting a
sub-sequence containing n words, and plot its ac-
curacy with blue squares in Figure 4. We also take
a BiLSTM model trained with the entire texts as
the upper bound (the horizontal line in Figure 4).
When using only a single word for representing
instances, single words extracted from our BiFor-
est model are significantly more effective than ran-
domly picked single words. When utilizing up to
five extracted words from our BiForest model for
representing each instance, we can obtain an accu-
racy very close to the upper bound. Therefore, the
extracted evidence from our BiForest model are
truly effective for representing the instance and its
corresponding category.

5 Conclusion

We proposed a multi-granular text encoder for
self-explaining text categorization. Comparing
with the existing BiLSTM and CNN baselines, our



45

model is more accurate, efficient and compact. In
addition, our model can extract effective and intu-
itive evidence to support its predictions.

References
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and

Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.
Rationalizing neural predictions. arXiv preprint
arXiv:1606.04155.

Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.
2015. Visualizing and understanding neural models
in nlp. arXiv preprint arXiv:1506.01066.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. arXiv preprint arXiv:1703.03130.

James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng
Sun, and Jacob Eisenstein. 2018. Explainable pre-
diction of medical codes from clinical text. arXiv
preprint arXiv:1802.05695.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075.

Zhiyang Teng and Yue Zhang. 2016. Bidirectional
tree-structured lstm with head lexicalization. arXiv
preprint arXiv:1611.06788.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 6000–6010.

Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015.
Self-adaptive hierarchical sentence model. In IJCAI,
pages 4069–4076.

Xiaodan Zhu, Parinaz Sobihani, and Hongyu Guo.
2015. Long short-term memory over recursive
structures. In International Conference on Machine
Learning, pages 1604–1612.


