



















































A Computational Approach for Generating Toulmin Model Argumentation


Proceedings of the 2nd Workshop on Argumentation Mining, pages 45–55,
Denver, Colorado, June 4, 2015. c©2015 Association for Computational Linguistics

A Computational Approach for Generating Toulmin Model Argumentation

Paul Reisert Naoya Inoue Naoaki Okazaki Kentaro Inui

Tohoku University
Graduate School of Information Sciences

6-6 Aramaki Aza Aoba, Aobaku, Sendai, Miyagi 980-8579, Japan
{preisert,naoya-i,okazai,inui}@ecei.tohoku.ac.jp

Abstract

Automatic generation of arguments is an im-
portant task that can be useful for many ap-
plications. For instance, the ability to gener-
ate coherent arguments during a debate can be
useful when determining strengths of support-
ing evidence. However, with limited technolo-
gies that automatically generate arguments,
the development of computational models for
debates, as well as other areas, is becoming
increasingly important. For this task, we fo-
cused on a promising argumentation model:
the Toulmin model. The Toulmin model is
both well-structured and general, and has been
shown to be useful for policy debates. In this
preliminary work we attempted to generate,
with a given topic motion keyword or phrase,
Toulmin model arguments by developing a
computational model for detecting arguments
spanned across multiple documents. This pa-
per discusses our subjective results, observa-
tions, and future work.

1 Introduction

Given an input motion, or claim, the task of auto-
matic argumentation generation is to generate co-
herent and logically structured argumentation in
various scenarios. In this paper, we examined two
extreme types of scenarios: (i) an input claim should
be supported, and (ii) a counterclaim should be sup-
ported. For example, with the House should ban
alcohol in schools as an input claim, our goal was
to automatically generate supportive output, such as
“The input claim is valid because alcohol causes
brain damage. Brain damage loses concentration

for study.”; and with the House should not ban alco-
hol in schools as our counterclaim, our goal, like be-
fore, was to generate supportive output, such as “The
counterclaim is valid because alcohol makes people
fun. Sociality can be developed by pleasure.”. The
automatic generation of arguments is a challenging
problem that is not only useful for identifying evi-
dence of certain claims but also for why the evidence
of certain claims is significant.

As a basis for generating logically structured out-
put, we required the utilization of a structured frame-
work ideal for debate arguments. A promising op-
tion for accomplishing this goal includes the inte-
gration of the Toulmin model [18], which consists of
three main components (claim, data, and warrant),
where a claim is something an individual believes,
data is support or evidence to the claim, and a war-
rant is the hypothetical link between the claim and
data. When considering this structure for debate
topic motions, such as alcohol should be banned,
then data such as alcohol causes liver disease and a
warrant such as if alcohol causes liver disease, then
it should be banned can be supportive for the claim,
as the data’s relevance to the claim is provided by
the warrant. Although many possibilities exist for
constructing a Toulmin model, we refer to a single
possibility as a Toulmin instantiation; and due to its
promising usefulness in policy debates [1], we ex-
plored the Toulmin model for argumentation gener-
ation. As such, no previous work has experimented
with automatically constructing Toulmin instantia-
tions through computational modeling.

As an information source of argumentation gen-
eration, we aggregate statements relevant to the in-

45



put claim spanned across multiple documents on the
Web. One can exploit one single document that in-
cludes the input claim; however, it may not include
information sufficient to organize a logically struc-
tured answer comprehensively.

The most challenging part of automatic construc-
tion of a Toulmin instantiation is to construct a co-
herent and well-organized argumentation from the
relevant pieces of statements from multiple docu-
ments. In this paper, we manually give relations be-
tween each Toulmin component in terms of causal-
ity and the sentiment polarity of their participants.
We focus on two extreme causality relations, namely
PROMOTE or SUPPRESS in this paper. By utilizing
these relations, our task is reduced to finding rela-
tion tuples that can satisfy the definitions. We use
our evaluation results as a basis of justification as to
whether or not the these relation tuples are sufficient
for argumentation construction. To ensure the co-
herency of overall argumentation, we find contextu-
ally similar relations. In future work, we plan to ap-
ply state-of-the-art technologies from discourse rela-
tion recognition and QAs for generating each Toul-
min component, where a significant amount of re-
search has been done [20, 15, 13, 8, 17].

The rest of the paper is as follows. We first de-
scribe related work in Section 2 and an overview of
the Toulmin model in Section 3. In Section 4, we de-
scribe our methodology for generating patterns for
Toulmin construction. In Section 5, we experiment
with constructing Toulmin instantiations for a given
claim and report our findings. In Section 6, we dis-
cuss our results. Finally, in Section 7, we conclude
our work and describe our future work.

2 Related Work

To the best of our knowledge, no prior work has de-
veloped a computation model for automatically con-
structing Toulmin instantiations. However, various
components of the Toulmin model have individually
been researched and are discussed below.

The most similar work to ours is the automatic
detection of enthymemes using Walton [21]’s argu-
mentation schemes [5]. Similarly, we aim to dis-
cover enthymemes in the Toulmin model explicitly
through computational modeling in order to assist
with generating constructive debate speeches. In fu-

ture work, we plan to adopt different, less general
argumentation theories.

Given a motion-like topic, previous work has
found relevant claims to support the topic [8]. Other
work has utilized a list of controversial topics in or-
der to find relevant claim and evidence segments uti-
lizing discourse markers [17]. Previous Why-QA
work [20, 15, 13] has dealt with finding answers for
questions such as Why should alcohol be banned?.
In this case, a passage such as Alcohol causes heart
disease can be retrieved; however, the passage is
not necessarily concerned with Why is heart disease
negative? which can act as a link between the ques-
tion and answer. In this work, in addition to a claim
and it data, or evidence, we explore finding the link,
or warrant, and its backing, in order to strengthen
the relationship between the claim and data, one of
the aspects of the Toulmin model.

In terms of determining stance, previous work has
utilized attack or support claims in user comments
as a method for determining stance [3]. Inspired by
Hashimoto et al. [6]’s excitatory and inhibitory tem-
plates, in this work, we similarly compose a manual
list of PROMOTE(X,Y) and SUPPRESS(X,Y) rela-
tions and rely on these relations, coupled with posi-
tive and negative sentiment values, as a means to sig-
nify stance. Simultaneously, not only does this assist
with stance, but it is an important feature for argu-
ment construction in our first round of constructing
automatic Toulmin instantiations.

Finally, we generate arguments spanned across
multiple documents using the PROMOTE(X,Y) and
SUPPRESS(X,Y) relations. Previous work such as
Cross Document Structure theory [16] has organized
information from multiple documents via relations.

Furthermore, the Statement Map [14] project, for
a given query, has detected agreeing and conflict-
ing support which are spanned across multiple doc-
uments. In this work, we attempt to construct an im-
plicit Warrant and generate its Backing for a Claim
(query) and its Data (support).

3 Toulmin Model

Toulmin was the first to believe that most arguments
could simply be modeled using the following six
components: claim, data, warrant, backing, qual-
ifier, and rebuttal [18]. This model is referred to as

46



the Toulmin model and is shown in Figure 1, along
with an instantiation. In this work, we focus on con-
structing an argument consisting of a claim, data,
warrant, as these three components make up the
bare minimum of the Toulmin model. The claim
consists of the argument an individual wishes for
others to believe. Data consists of evidence to sup-
port the claim. However, in the event the data is
considered unrelated to the claim by another indi-
vidual, such as a member of a negative team in a pol-
icy debate, the warrant, although typically implicit,
can explicitly be mentioned to state the relevance of
the data with the claim.

Alcohol causes liver disease.
PROMOTE(alcohol, liver disease)

if  alcohol causes liver disease, then the House should ban it.
if  PROMOTE(alcohol, liver disease) then SUPPRESS(House, alcohol)	

According to a recent study, liver disease causes 10,000 deaths a year on average.
PROMOTE(arg2(data), death)	

This House believes alcohol should be banned.
SUPPRESS(House, alcohol)	

Data	

Warrant	

Claim	

Backing	

This House believes alcohol should be banned.
Input	  (Mo6on)	

Output:	  Toulmin	  Instan6a6on	

Figure 1: An Instantiation of the Toulmin Model. The
underlined word represents negative sentiment.

In addition to the basic components, one indi-
vidual may require more information to support the
warrant. This component is referred to as backing,
and we attempt to generate backing as evidence to
the warrant. By generating a warrant and its back-
ing, we can strengthen the data in relation to the
claim which can be important for determining the
relevancy of the data in a debate. Additional Toul-
min components consist of a rebuttal, which is an
exception to a claim, and a qualifier, which is a
component, such as a sentence or word, in which
affects the degree of the claim.

4 Methodology

As shown in Figure 1, our task consists of the fol-
lowing: given a topic motion in the form PRO-
MOTE(House,Y) or SUPPRESS(House, Y), where Y
is a topic motion keyword, we instantiate a Toul-
min model by first recognizing the topic motion
as a Toulmin model claim, and through computa-
tional modeling, we generate the remaining Toulmin
model arguments.

For instantiating a Toulmin model through com-

putational modeling given a motion, or claim in the
Toulmin model, we need to recognize the semantic
relation between sentences in a corpus. For exam-
ple, to find data of the claim, we need find a set of
sentences that can serve as the evidence of the claim.
However, as described in Section 1, there are still a
lot of challenging problems in this research area.

Therefore, our idea is to focus on the sen-
tences that can be represented by an excitation re-
lation, namely PROMOTE(X, Y) or SUPPRESS(X,
Y), which is inspired by [6]. Focusing on such sen-
tences, we can recast the problem of semantic rela-
tion recognition between sentences as a simple pat-
tern matching problem. For example, suppose we
are given the claim SUPPRESS(government, riot).
Then, we can find the supporting evidence of this
claim by searching for sentences that match PRO-
MOTE(riot, Z), where the sentiment polarity of Z is
negative.

1. Junk food causes 
obesity for many students
2. Junk food causes 
environmental pollution    

This House believes junk food in 
schools should be banned.

Data	 Claim	

This House believes junk food in 
schools should be banned.

Input	  (Mo0on)	

Output:	  Toulmin	  Instan0a0on	

Figure 2: An example of contextual information for argu-
ment generation. The selected data is shown in bold.

One of the challenges of argument generation is
the ability to produce coherent arguments. Figure 2
shows an example of this challenge. In the claim
in Figure 2, one can see that opposed to banning
all junk food in the world, the claim is limited to
banning junk food in schools only. If we were to
discover that junk food causes obesity for many stu-
dents and junk food causes environmental pollution
as data, then we would like to choose the data which
is most likely related to the claim. Therefore, we
also account for contextual similarity when generat-
ing arguments. In the case of Figure 2, we would
prefer the first data over the second, given the sim-
ilarity between student and school. More details re-
garding our contextual similarity calculation method
are described in Section 4.3.

47



4.1 Overview
We develop a two-staged framework for the auto-
matic construction of Toulmin instantiations. First,
we extract a set of claims represented by two-
place predicates (e.g., cause(alcohol, cancer)) from
a text corpus and generalize them into an excitation
relation, namely either PROMOTE(X, Y) or SUP-
PRESS(X, Y). We then store the generalized rela-
tions into a database, which we call a knowledge
base. In addition to the PROMOTE(X, Y) and SUP-
PRESS(X, Y) relation extraction, we also append di-
rect object sentiment and first-order dependency in-
formation for our relations. This is further elabo-
rated in Section 4.2.

23	  

Warrant Construction 
Stage 

if something causes cancer, 
it should be banned... 

Data Selection Stage 
Alcohol causes cancer 

Booze cause liver disease 
... 

Knowledge 
Base  

PR causes 
alcohol cancer 

-1.0 ... 
SP decrease it 
cancer alcohol 

-1.0 ... 

Backing Selection Stage 
cancer promotes death 

... 

Toulmin Instantiation Stage	

Motion: ban(House,alcohol) 

  Knowledge Base 
Construction Stage	

Relation Search 
Input = Promote/Suppress 

Predicates 

Sentiment Calculation & 
First Order Dependency 

Extraction Stage 

Web Corpus	

Figure 3: Overall framework

Second, given the motion claim that is also rep-
resented by a two-place predicate (e.g., ban(house,
alcohol)) by the user, we find relevant relations from
the knowledge base to generate data, warrant, and
backing for the input motion claim. For counter-
claims, we apply a simple hypothesis for reversing
an original input motion (ban(house, alcohol) to not
ban(house, alcohol)) and repeat the Toulmin con-
struction stage for the new input. In the rest of this
section, we elaborate on the two processes one by
one.

4.2 Knowledge Base Construction
For constructing a knowledge base of PRO-
MOTE(X,Y) and SUPPRESS(X,Y) relations, we rely
on a manually created list of verbs representing
PROMOTE/SUPPRESS relations and parsed depen-
dency output. Similar to Open Information Extrac-
tion systems [23, 4, 10, etc.], we extract a set of

triples (A1, R, A2), where R is a verb matching a
PROMOTE/SUPPRESS-denoting verb, A1 is a noun
phrase (NP) serving as the surface subject of R, and
A2 is an NP serving as the surface object of R.

In our experiment, we used a collection of web
pages extracted from ClueWeb12 as a source corpus
of knowledge base construction. ClueWeb121 con-
sists of roughly 733 million Web documents ranging
from blogs to news articles. All web pages contain-
ing less than 30 words were filtered out which re-
sulted in 222 million total web pages. From these
web pages, we extract 22,973,104 relations using a
manually composed list of 40 PROMOTE (e.g. in-
crease, cause, raise) and 76 SUPPRESS (e.g. harm,
kill, prevent) predicates. We parse each document
using Stanford CoreNLP [9] in order to acquire both
dependency, named entity, and coreference resolu-
tion features. In the case of coreference resolution,
in order to reduce parsing time, the search distance
was restricted to the previous two sentences.

At this time, we limit our extraction on a simple
noun subject/direct objects opposed to passive sen-
tences (e.g. cancer is caused by smoking). In future
work, we will integrate more state of the art relation
extraction methods for handling such cases.

4.2.1 Sentiment Polarity Calculation
For calculating the sentiment of each argument’s

head noun, we use SentiWordNet [2], Takamura
et al. [19]’s sentiment corpus, and the Subjectivity
Lexicon [22]. For each corpus, we assign a value of
1.0 if the sentiment is positive, -1.0 if negative, or
otherwise neutral. We base positive and negative as
a value greater than 0 and less than 0, respectively.
In the case of SentiWordNet, we focus only on
the top-ranked synset polarity value for each noun.
Afterwards, we combine the values per noun and
calculate sentiment using the following:

sp(w)=


pos if num pos votes(w) ≥ 2
neg if num neg votes(w) ≤ −2
neutral otherwise

,
where w is the head noun of the direct object in each
PROMOTE and SUPPRESS relation. The functions
num pos votes(w) and num neg votes(w) refer
to the total number of positive sentiment votes
and the total number of negative sentiment votes,

1http://www.lemurproject.org/clueweb12.
php/

48



respectively, for w.
The results of our knowledge base construction

are shown in Table 1. Positive, Negative, and Neu-
tral refer to the number of relations in which a rela-
tion’s A2 sentiment is positive, negative, and neutral,
respectively.

Table 1: PROMOTE (PR) and SUPPRESS (SP) relations
from our data set.

Type Positive Negative Neutral Total
PR 2,039,644 755,695 17,504,201 20,299,540
SP 115,895 163,408 2,394,261 2,673,564
Total 2,155,539 919,103 19,898,462 22,973,104

From Table 1, we recognize an abundance
of PROMOTE(X,Y) relations opposed to SUP-
PRESS(X,Y) relations. In addition, there are a con-
siderable amount of neutral sentiment values. In
our future work, we will focus on generating argu-
ments with relations containing neutral direct object.
For now, we limit our argument generation on re-
lations with positive or negative direct object senti-
ment only.

4.3 Contextual Similarity

For calculating the contextual similarity between
two sentences, we use first-order dependency tree
information for an extracted relation’s arguments’
head and predicate. In the event a first-order node
is a named entity, we also extract any of its children
with named entity information attached.

We then calculate the average pairwise similarity
between each relation between sentences using the
cosine similarity of word vectors.

We adopt the following hypotheses for contextual
similarity for our full model:

• if determining contextual similarity between
claim and data, we calculate similarity be-
tween a claim’s predicate first-order depen-
dency information with data’s predicate first-
order dependency information, and claims’s
A2 first-order dependency information with
data’s A1 first-order dependency information

• if determining contextual similarity between
data and backing, we calculate similarity be-
tween a data’s A2 first-order dependency in-

formation with backing’s A1 first-order depen-
dency information, and data’s predicate first-
order dependency information with backing’s
predicate first-order dependency information

Figure 4: A dependency graph used for contextual simi-
larity

The rabies virus causes disease in the brain

det

nn nsubj dobj

prep in

det

An example is as follows. In the case of the sen-
tence the rabies virus causes disease in the brain, the
following first-order dependency extractions will be
produced for subject (rabies virus), object (disease),
and predicate (cause), respectively: {det: the, nn:
rabies}, {}, {nsubj: virus, prep in: brain, dobj: dis-
ease}.
4.4 Finding Toulmin Arguments
Below we present our hypotheses for generating
claim, data, warrant, and backing.

4.4.1 Data
Given the motion in the form of a triplet I =

(A1, R, A2), we first extract a set D of candidate
triplets of data for the input motion I from the con-
structed knowledge base. As described in Section 3,
data is defined as a statement that supports the input
motion, otherwise known as the claim. We find a set
of data triplets based on the following hypotheses:

• if the input motion is PROMOTE(X, Y), the sup-
porting data can be in the following two forms:
(i) PROMOTE(Y’, Z), where the sentiment po-
larity of Z (henceforth, sp(Z)) is positive, or (ii)
SUPPRESS(Y’, Z), where sp(Z) is negative. Y’
may also be a full hyponym2 of Y or Y itself.

• if the input motion is SUPPRESS(X, Y), the
supporting data can be either (i) PROMOTE(Y’,
Z), where sp(Z) is negative, or (ii) SUP-
PRESS(Y’, Z), where sp(Z) is positive. Y’ may
also be a full hyponym of Y or Y itself.

2We limit hyponyms to the top 10 most similar hyponyms to
Y (Z in the case of backing)

49



For example, given the input motion ban(house, al-
cohol), where ban is a SUPPRESS relation, we ex-
tract (i) all PROMOTE relations in which its A1 is
alcohol, or a full hyponym of alcohol, and sp(A2)
is negative (e.g., cause(alcohol, liver disease)), and
(ii) SUPPRESS relations in which its A1 is alcohol,
or a full hyponym of alcohol, and sp(A2) is positive
(e.g., decrease(alcohol, life expectancy)).

After we collect a set of candidate triplets, we
then cluster by the head noun of each relation’s Z
which is represented as D = Dn1 , Dn2 , ..., Dnm ,
where ni is the head noun and m is the total size of
unique Z. This is in order to diversify our arguments
by different topics.

4.4.2 Warrant and Backing

Given that warrant is a hypothetical, bridgelike
statement [18], we use a simple combination of a
data relation and a claim using an if...then construct.
Therefore, with the claim this House should ban al-
cohol and a data of alcohol causes liver disease, we
generate a warrant of if alcohol causes liver disease,
then the House should ban it. In future work, we
will work on expanding this rule.

For each d ∈ D,D ∈ D, we extract a set Bd
of candidate backings using the similar hypotheses
in the data extraction step. As described in Sec-
tion 3, backing serves as the supporting evidence of
the warrant. For example, we would like to find a
statement that further provides reason to a warrant
of if alcohol promotes lung cancer, then it should be
banned (in this case, a statement such as lung cancer
causes death can be a backing).

To capture backing of a warrant, we apply the
following hypotheses if the input motion I is PRO-
MOTE(X, Y) and data is d:

• if d is PROMOTE(Y, Z), where sp(Z) is pos-
itive, the backing can be either: (i) PRO-
MOTE(Z’, V), where sp(V ) is positive, or (ii)
SUPPRESS(Z’, V), where sp(V ) is negative. Z’
may also be a full hyponym of Z or Z itself.

• if d is SUPPRESS(Y, Z), where sp(Z) is neg-
ative, the backing can be either: (i) PRO-
MOTE(Z’, V), where sp(V ) is negative, or (ii)
SUPPRESS(Z’, V), where sp(V ) is positive. Z’
may also be a full hyponym of Z or Z itself.

Similarly, if the input motion I is SUPPRESS(X, Y),
the following rules are applied:

• if d is PROMOTE(Y, Z), where sp(Z) is neg-
ative, the backing can be either: (i) PRO-
MOTE(Z’, V), where sp(V ) is negative, or (ii)
SUPPRESS(Z’, V), where sp(V ) is positive. Z’
may also be a full hyponym of Z or Z itself.

• if d is SUPPRESS(Y, Z), where sp(Z) is pos-
itive, the backing can be either: (i) PRO-
MOTE(Z’, V), where sp(V ) is positive, or (ii)
SUPPRESS(Z’, V), where sp(V ) is negative. Z’
may also be a full hyponym of Z or Z itself.

For example, for the input motion ban(house, al-
cohol) and data cause(alcohol, liver disease), we
would have as a result cause(liver disease, death)
and suppress(liver disease, metabolism) as a back-
ing.

After we collect a set of candidate triplets, we
then cluster by the head noun of each relation’s V
which is represented as W = Wn1 , Wn2 , ...,Wnm ,
where ni is the head noun and m is the total size of
unique V. Similar to data, this is in order to diversify
our generated arguments by topic.

4.4.3 Counterclaim
For the purpose of debating, we would like to cre-

ate a Toulmin instantiation which conflicts with the
original claim; that is, which is initialized with a
counterclaim. For example, if the original input mo-
tion, and thus claim, is ban(house, alcohol), then we
would ideally like to construct an independent Toul-
min instantiation with the following counterclaim:
not ban(house, alcohol). As such, the following two
hypotheses are applied:

• if the original input motion is PROMOTE(X, Y),
then the claim will be the new input motion
SUPPRESS(X,Y)

• if the original input motion is SUPPRESS(X, Y),
then the claim will be the new input motion
PROMOTE(X,Y)

4.4.4 Toulmin Instantiation
So far, we have a set D of candidate data clus-

ters, and for each d ∈ D,D ∈ D, we have a set
Bd of backing clusters. For generating argumen-
tation, we first select representative data candidate

50



repr(D) for each D ∈ D based on the following
score function:

repr(D) = arg max
d∈D

score(d; c) (1)

score(x; y) = w1 · (cs(arg0(x), arg1(y))
+cs(pred(x), pred(c)))

+w2 · as(arg0(x), arg1(y))
+w3 · rel(clust(x))− w4 · spec(x), (2)

where cs(x, y) and as(x, y) are functions represent-
ing contextual similarity and relation argument simi-
larity, respectively. rel(clust(x)) determines the re-
liability of cluster clust(x) based on its total num-
ber of items. spec(x) determines the specificity of a
given entry x. Both are defined as follows:

spec(e) =
ene size
etokens

+ log esent len (3)

rel(X) = log Xnum items (4)

, where ene size is the total number of named en-
tities in entry e, etokens is the total number of to-
kens, esent len is the sentence length of entry e, and
Xnum items is the number of entries in cluster X .

Contextual similarity is described in Section 4.3.
For relation argument similarity, we simply calcu-
late the average between relation argument surfaces
using word vectors. We utilize the Google News
dataset created by Mikolov et al. [11], which con-
sists of 300-dimensional vectors for 3 million words
and phrases. For each representative data candi-
date d ∈ R, we select the most likely backing from
B ∈ Bd based on the following:

backing(B) = arg max
b∈B

score(b; d) (5)

In order to determine appropriate weights for our
ranking function, we create a development set for
the motion ban(House,alcohol in America) and tune
the weights until we discover a suitable value for
our results. We determine the following empirical
weights for our scoring function which we utilize in
our experiment section: w1 = .5, w2 = .15, w3 =
.3, and w4 = .5. We choose the relation with
the highest score for our data selection stage and,
similarly, our backing selection stage. Finally, we
would like to mention that Equation 2 represents our
ranking function for our full model which accounts

for predicate similarity between our target argument
(data or backing) and original claim. Our baseline
model does not include predicate similarity between
the targeted argument and original claim.

5 Experiment and Discussion

Given the five topic motion phrases animal testing,
death penalty, cosmetic surgery, smoking in public
places, and junk food from schools that were ran-
domly selected from the iDebate, a popular, well-
structured online debate platform, Top 100 Debate
list3, we construct 5 Toulmin instantations for the
topic motion ban(House, Y)), where Y is a topic mo-
tion phrase. Similarly, we construct 5 Toulmin in-
stantations for the topic motion not ban(House, Y)),
which serves as a counterclaim.

For each topic motion, we use WordNet [12] to
collect the full hyponyms and lemmas of the topic
motion keyword. Next, we calculate the surface sim-
ilarity between the keyword and its hypoynms, and
we use the top 10 most similar hyponyms in order
to collect more relations with subjects similar to the
main keyword. After hyponym expansion, we fil-
ter out passages containing a question mark to avoid
non-factual arguments , and we cluster by a rela-
tion’s direct object head noun. This is in order to
diversify our generated arguments by unique topics.
Furthermore, we use the Lesk algorithm [7] to dis-
ambiguate a sentence using the hyponym synset or
original motion topic synset in order to obtain sen-
tences with similar semantic meaning. For instance,
for the hypoynm face lift of cosmetic surgery, we
filter out sentences referring to a renovation face lift
opposed to a cosmetic face lift.

For each cluster, we use the appropriate scoring
function shown in Section 4.4.4 to rank the rela-
tions. After each cluster item is scored, we collect
10 clusters, if available, with the top scores each to
represent data. However, as shown from our re-
sults in Tables 2 and 3, some topics generated less
than 10 data. For each data argument we generate,
we repeat our hyponym expansion step for each di-
rect object in our data relation, generate clusters and
use the appropriate equations from Section 4.4.4 for
generating backing for the constructed hypothetical
warrant.

3http://idebate.org/view/top_100_debates

51



Table 2: Precision of baseline model consistency
ban(A1, A2) Data Backing
A2=animal testing - -
A2=cosmetic surgery 0.20 (1/5) 0.00 (0/4)
A2=death penalty 0.20 (1/5) 0.00 (0/5)
A2=junk food in schools 0.75 (6/8) 0.25 (2/8)
A2=smoking in public places 1.00 (2/2) 0.00 (0/1)
Average 0.50 0.11
not ban(A1, A2) Data Backing
A2=animal testing 0.33 (1/3) 0.00 (0/3)
A2=cosmetic surgery 0.83 (5/6) 0.00 (0/6)
A2=death penalty 0.67 (4/6) 0.17 (1/6)
A2=junk food in schools - -
A2=smoking in public places - -
Average 0.67 0.07

5.1 Results
We subjectively evaluate our output based on the fol-
lowing criteria: i) Does data support the claim?, and
ii) Does the backing properly support the warrant?
In Tables 2 and 3, we represent i and ii as data and
backing, respectively.

Table 3: Precision of full model consistency
ban(A1, A2) Data Backing
A2=animal testing - -
A2=cosmetic surgery 0.20 (1/5) 0.00 (0/4)
A2=death penalty 0.20 (1/5) 0.00 (0/5)
A2=junk food in schools 0.75 (6/8) 0.25 (2/8)
A2=smoking in public places 1.00 (2/2) 0.00 (0/1)
Average 0.50 0.11
not ban(A1, A2) Data Backing
A2=animal testing 0.33 (1/3) 0.00 (0/3)
A2=cosmetic surgery 0.83 (5/6) 0.00 (0/6)
A2=death penalty 0.67 (4/6) 0.00 (0/6)
A2=junk food in schools - -
A2=smoking in public places - -
Average 0.67 0.00

We achieved almost identical results for our base-
line model and full model; however, for the claim
the death penalty should not be banned, our base-
line model generated death penalty will eliminate
sins and sin makes men accomplices of one another
and causes concupiscence, violence, and injustice
to reign among them as data and backing, respec-
tively. On the other hand, our full model generated
the same data argument, but generated the incorrect

backing of any bloggers promoted this, not me, giv-
ing people the idea it was making them money and
they too should join.

Overall, our low precision signifies that many is-
sues still remain with our computational model.

Table 4: Sample of one valid Toulmin instantiation con-
structed by our model

Argument Sentence
Claim This House should ban junk food in schools.
Data Junk food will cause acne.
Warrant If junk food causes acne, then it should be

banned.
Backing Although acne developing in other parts of body

is more severe than facial acne , facial acne
greatly hurts ones self esteem due to ugly facial
complexion.

Shown in Table 4 is an example of a valid Toul-
min instantiation generated by our model. For the
claim this house should ban junk food in schools,
the data junk food will cause acne was generated.
Using the claim and data, the warrant if junk food
causes acne, then it should be banned was gener-
ated. Finally, to support the warrant, the backing
above was generated, thus generating a full Toulmin
instantiation.

Table 5: Sample of incorrect output. In the second exam-
ple, backing only is incorrect.

Argument Sentence
Data Smoking causes bad health and

it is very deadly .
Data Capital punishment gives peace

of mind to the victim ’s family
and friends .

Backing But let us also be prepared to
point out , as McGowan does
, that peace can make claims
on pragmatists at least as com-
pelling as war .

From the output in Table 5, we recognized further
improvements must be made to our knowledge base.
For instance, for the generated data smoking causes
bad health and it is very deadly, the object health’s
sentiment polarity was labeled as positive; however,
the phrase bad health implies negative sentiment. In

52



future work, we must consider an object’s adjective
modifiers in our sentiment polarity calculation algo-
rithm.

The second example in Table 5 demonstrates the
difficulty in generating backing. In this example,
the relation PR(capital punishment, peace) gener-
ated the data; however, searching for relations in
our knowledge base with a subject of peace resulted
in several unrelated sentences. Therefore, our model
generated an unrelated backing. In our future work,
we will address this issue, as this accounted for most
of errors in backing.

6 Discussion

From our results in the previous section, it is ap-
parent that we must make significant effort for im-
proving our generated output precision in our future
work. We learned that while our current knowledge
base looks promising for argument generation, we
must further modify its construction. In addition to
the errors discussed in the previous section, we rec-
ognize that another consideration when generating
arguments is the credibility of the source of infor-
mation. As our measure of reliability was based
on the frequency of relation occurrences, we also
need to incorporate the source of information into
our model. For example, if we find a passage such as
researchers mention that cancer causes pain, then it
is important to extract the source of information (e.g.
news article, personal blog, etc) as well as the entity
stating the fact (e.g. researchers). This can be espe-
cially important when determining for strengthening
an argument’s persuasiveness in a debate.

7 Conclusion and Future Work

In this work, we conducted a preliminary study for
the development a computational model for the in-
stantiation of a Toulmin model given a debate mo-
tion. We constructed a knowledge base of PRO-
MOTE(X,Y) and SUPPRESS(X,Y) relations and cre-
ated a set of rules for generating Toulmin data, war-
rant, and backing. From our results, we determined
that our model requires significant improvement for
the task of argument generation.

7.1 Future Work

As this work is a preliminary study for Toulmin in-
stantiations by taking a computational approach, we
recognize several areas for improvement. For ex-
ample, we are aware that a claim and its respec-
tive arguments can come in forms other than PRO-
MOTE(X,Y) and SUPPRESS(X,Y), such as a claim
in the form of the sky is blue.

We would also like to adopt previous strategies,
such as rhetorical structure theory for finding claim
and data within one document. We believe that
while not all Toulmin arguments may be explicitly
mentioned in a single document, we may be able to
detect multiple arguments for which we can utilize
for discovering the implicit arguments in another
document. For example, if one document states
drinking is dangerous because it can lead to liver
disease, then we can extract drinking is dangerous
as a claim and it can lead to liver disease as a data
from a single document, and, similarly to the strate-
gies in this work, find the remaining arguments from
other documents.

Credibility is also another important integration
we must account for in our future work. As we only
rely on frequency of relations for the reliability of
a relation, we ignore the source of information and
any entities stating facts containing our extracted re-
lations. Integrating credibility can help strengthen
the arguments our system generates which is benefi-
cial for policy debates.

Finally, we will expand upon our PROMOTE
and SUPPRESS keyword list, and we will experi-
ment with state-of-the-art relation extraction tech-
nologies, as our current implementation is based on
simple extraction rules.

Acknowledgments

The authors would like to give a special thanks to
Kohsuke Yanai and Toshihiko Yanase of Central Re-
search Laboratory, Hitachi, Limited. They would
also like to thank the workshop reviewers for their
invaluable comments. This work was supported by
the MEXT Research Student Scholarship and by
the JSPS KAKENHI Grant Numbers 15H05318 and
23240018.

53



References

[1] I. D. E. Association and R. Trapp. The De-
batabase Book: A Must-have Guide for Suc-
cessful Debate. International Debate Education
Association, 2009.

[2] S. Baccianella, A. Esuli, and F. Sebastiani.
Sentiwordnet 3.0: An enhanced lexical re-
source for sentiment analysis and opinion min-
ing. In in Proc. of LREC, 2010.

[3] F. Boltužić and J. Šnajder. Back up your
stance: Recognizing arguments in online dis-
cussions. In Proc. of the First Workshop on
Argumentation Mining, pages 49–58, 2014.

[4] A. Fader, S. Soderland, and O. Etzioni. Identi-
fying relations for open information extraction.
In Proc. of EMNLP, EMNLP ’11, pages 1535–
1545, 2011.

[5] V. W. Feng and G. Hirst. Classifying argu-
ments by scheme. In Proc. of ACL: HLT - Vol-
ume 1, HLT ’11, pages 987–996, 2011.

[6] C. Hashimoto, K. Torisawa, S. De Saeger, J.-H.
Oh, and J. Kazama. Excitatory or inhibitory: A
new semantic orientation extracts contradiction
and causality from the web. In Proc. of Joint
Conference on EMNLP and CoNLL, EMNLP-
CoNLL ’12, pages 619–630, 2012.

[7] M. Lesk. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell
a pine cone from an ice cream cone. In SIG-
DOC ’86: Proceedings of the 5th annual in-
ternational conference on Systems documenta-
tion, pages 24–26. ACM, 1986.

[8] R. Levy, Y. Bilu, D. Hershcovich, E. Aha-
roni, and N. Slonim. Proc. of COLING 2014:
Technical Papers, chapter Context Dependent
Claim Detection, pages 1489–1500. 2014.

[9] C. D. Manning, M. Surdeanu, J. Bauer,
J. Finkel, S. J. Bethard, and D. McClosky. The
Stanford CoreNLP natural language process-
ing toolkit. In Proc. of 52nd Annual Meeting of
the Association for Computational Linguistics:
System Demonstrations, pages 55–60, 2014.

[10] Mausam, M. Schmitz, R. Bart, S. Soderland,
and O. Etzioni. Open language learning for in-
formation extraction. In Proc. of the 2012 Joint

Conference on EMNLP and CoNLL, EMNLP-
CoNLL ’12, pages 523–534, 2012.

[11] T. Mikolov, I. Sutskever, K. Chen, G. Corrado,
and J. Dean. Distributed representations of
words and phrases and their compositionality.
CoRR, 2013.

[12] G. A. Miller. Wordnet: A lexical database for
english. Commun. ACM, pages 39–41, 1995.

[13] J. Mrozinski, E. Whittaker, and S. Furui. Col-
lecting a why-question corpus for development
and evaluation of an automatic QA-system. In
Proc. of ACL: HLT, pages 443–451, 2008.

[14] K. Murakami, E. Nichols, S. Matsuyoshi,
A. Sumida, S. Masuda, K. Inui, and Y. Mat-
sumoto. Statement map: Assisting information
credibility analysis by visualizing arguments.
In 3rd Workshop on Information Credibility on
the Web, 2008.

[15] J.-H. Oh, K. Torisawa, C. Hashimoto, M. Sano,
S. De Saeger, and K. Ohtake. Why-question
answering using intra- and inter-sentential
causal relations. In Proc. of ACL: long papers,
pages 1733–1743, 2013.

[16] D. R. Radev. A common theory of informa-
tion fusion from multiple text sources step one:
Cross-document structure. In Proceedings of
the 1st SIGdial Workshop on Discourse and
Dialogue - Volume 10, pages 74–83. Associ-
ation for Computational Linguistics, 2000.

[17] P. Reisert, J. Mizuno, M. Kanno, N. Okazaki,
and K. Inui. A corpus study for identifying ev-
idence on microblogs. In Proc. of LAW VIII,
pages 70–74, 2014.

[18] S. E. Toulmin. The Uses of Argument. Cam-
bridge University Press, 1958.

[19] H. Takamura, T. Inui, and M. Okumura. Ex-
tracting semantic orientations of words using
spin model. In Proc. of the 43rd Annual Meet-
ing on Association for Computational Linguis-
tics, ACL ’05, pages 133–140, 2005.

[20] S. Verberne. Developing an approach for why-
question answering. In Proc. of EACL: Student
Research Workshop, EACL ’06, pages 39–46,
2006.

54



[21] D. Walton, C. Reed, and F. Macagno. Ar-
gumentation Schemes. Cambridge University
Press, 2008.

[22] T. Wilson, J. Wiebe, and P. Hoffmann. Rec-
ognizing contextual polarity in phrase-level
sentiment analysis. In Proc. of HLT and
EMNLP, HLT ’05, pages 347–354. Associa-
tion for Computational Linguistics, 2005.

[23] A. Yates, M. Cafarella, M. Banko, O. Et-
zioni, M. Broadhead, and S. Soderland. Tex-
trunner: Open information extraction on the
web. In Proc. of HLT: NAACL: Demonstra-
tions, NAACL-Demonstrations ’07, pages 25–
26, 2007.

55


