



















































Sentiment Classification Using Document Embeddings Trained with Cosine Similarity


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 407–414
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

407

Sentiment Classification using Document Embeddings trained with
Cosine Similarity

Tan Thongtan
Department of Computer Engineering

Mahidol University International College
Mahidol University

Thailand
tan.thongtan1@gmail.com

Tanasanee Phienthrakul
Department of Computer Engineering

Faculty of Engineering
Mahidol University

Thailand
tanasanee.phi@mahidol.ac.th

Abstract

In document-level sentiment classification,
each document must be mapped to a fixed
length vector. Document embedding mod-
els map each document to a dense, low-
dimensional vector in continuous vector
space. This paper proposes training docu-
ment embeddings using cosine similarity in-
stead of dot product. Experiments on the
IMDB dataset show that accuracy is im-
proved when using cosine similarity com-
pared to using dot product, while using fea-
ture combination with Naı̈ve Bayes weighted
bag of n-grams achieves a new state of
the art accuracy of 97.42%. Code to
reproduce all experiments is available at
https://github.com/tanthongtan/dv-cosine.

1 Introduction

In document classification tasks such as sentiment
classification (in this paper we focus on binary
sentiment classification of long movie reviews, i.e.
determining whether each review is positive or
negative), the choice of document representation
is usually more important than the choice of clas-
sifier. The task of text representation aims at map-
ping variable length texts into fixed length vec-
tors, as to be valid inputs to a classifier. Document
embedding models achieve this by mapping each
document to a dense, real-valued vector.

This paper aims to improve existing document
embedding models (Le and Mikolov, 2014; Li
et al., 2016a) by training document embeddings
using cosine similarity instead of dot product. For
example, in the basic model of trying to predict -
given a document - the words/n-grams in the doc-
ument, instead of trying to maximize the dot prod-
uct between a document vector and vectors of the
words/n-grams in the document over the training
set, we’ll be trying to maximize the cosine simi-
larity instead.

The motivation behind this is twofold. Firstly,
cosine similarity serves as a regularization mech-
anism; by ignoring vector magnitudes, there is
less incentive to increase the magnitudes of the
input and output vectors, whereas in the case of
dot product, vectors of frequent document-n-gram
pairs can be made to have a high dot product sim-
ply by increasing the magnitudes of each vector.
The weights learned should be smaller overall.

Secondly, as cosine similarity is widely used
to measure document similarity (Singhal, 2001;
Dai et al., 2015), we believe our method should
more directly maximize the cosine similarity be-
tween similar document vectors. The angle be-
tween similar documents should be lower, and that
may encode useful information for distinguishing
between different types of documents. We’ll com-
pare the performance of our model on the IMDB
dataset (Maas et al., 2011) with dot product and
to determine if our model serves anything beyond
simple regularization, we’ll also compare it to dot
product using L2 regularization.

2 Related Work

Here we review methods of text representation,
in which there are two main categories: bag of
words models and neural embedding models.

The bag of words model (Joachims, 1998) rep-
resents text as a fixed length vector of length equal
to the number of distinct n-grams in the vocab-
ulary. Naive Bayes - Support Vector Machine
(NB-SVM) (Wang and Manning, 2012) utilizes
naı̈ve bayes weighted bag of n-grams vectors for
representing texts, feeding these vectors into a lo-
gistic regression or support vector machine classi-
fier.

The first example of a neural embedding
model is word embeddings which was proposed
by Bengio et al. in 2003, while objective functions



408

utilizing the negative sampling technique for effi-
cient training of word embeddings were proposed
in 2013 by Mikolov et al. (word2vec). The aim
of word embeddings is to map each word to a real
vector, whereby the dot product between two vec-
tors represents the amount of similarity in meaning
between the words they represent. There are two
versions of word2vec: continuous bag of words
(CBOW), in which a neural network is trained to
predict the next word in a piece of text given the
word’s context, and skip-gram, where it will try to
predict a word’s context given the word itself.

In a 2017 paper Arora et al. produce Sentence
Embeddings by computing the weighted average
of word vectors, where each word is weighted us-
ing smooth inverse frequency, and removing the
first principle component.

Paragraph Vector (Le and Mikolov, 2014)
may be seen as a modification to word embed-
dings in order to embed as vectors paragraphs as
opposed to words. Paragraph vector comes in
two flavors: the Distributed Memory Model of
Paragraph Vectors (PV-DM), and the Distributed
Bag of Words version of Paragraph Vector (PV-
DBOW). PV-DM is basically the same as CBOW
except that a paragraph vector is additionally av-
eraged or concatenated along with the context and
that whole thing is used to predict the next word.
In the PV-DBOW model a paragraph vector alone
is used/trained to predict the words in the para-
graph.

Document Vector by predicting n-grams
(DV-ngram) (Li et al., 2016a) trains paragraph
vectors to predict not only the words in the para-
graph, but n-grams in the paragraph as well.
Weighted Neural Bag of n-grams (W-Neural-
BON) (Li et al., 2016b) uses an objective function
similar to the one in DV-ngram, except that each
log probability term is weighted using a weighing
scheme which is similar to taking the absolute val-
ues of naive bayes weights.

In (Li et al., 2017), they introduce three main
methods of embedding n-grams. The first is con-
text guided n-gram representation (CGNR), which
is training n-gram vectors to predict its context n-
grams. The second is label guided n-gram rep-
resentation (LGNR), which is predicting given an
n-gram the label of the document to which it be-
longs. The last is text guided n-gram representa-
tion (TGNR), which is predicting given an n-gram
the document to which it belongs.

Embeddings from Language Models (ELMo)
(Peters et al., 2018) learns contextualized word
embeddings by training a bidirectional LSTM
(Hochreiter and Schmidhuber, 1997) on the lan-
guage modelling task of predicting the next word
as well as the previous word. Bidirectional
Encoder Representations from Transformers
(BERT) (Devlin et al., 2018) uses the masked lan-
guage model objective, which is predicting the
masked word given the left and right context, in
order to pre-train a multi-layer bidirectional Trans-
former (Vaswani et al., 2017). BERT also jointly
pre-trains text-pair representations by using a next
sentence prediction objective.

For the rest of this section we’ll look at other
research which replaces dot product with cosine
similarity. In the context of fully-connected neural
networks and convolutional neural networks, (Luo
et al., 2017) uses cosine similarity instead of dot
product in computing a layer’s pre-activation as a
regularization mechanism. Using a special dataset
where each instance is a paraphrase pair, (Wieting
et al., 2015) trains word vectors in such a way that
the cosine similarity between the resultant docu-
ment vectors of a paraphrase pair is directly max-
imized.

3 Proposed Model

In learning neural n-gram and document embed-
dings, a dot product between the input vector and
the output vector is generally used to compute the
similarity measure between the two vectors, i.e.
‘similar’ vectors should have a high dot product.
In this paper we explore using cosine similarity
instead of dot product in computing the similar-
ity measure between the input and output vectors.
More specifically we focus on modifications to
the PV-DBOW and the similar DV-ngram models.
The cosine similarity between a paragraph vector
and vectors of n-grams in the paragraph is maxi-
mized over the training set.

3.1 Architecture
A neural network is trained to be useful in pre-
dicting, given a document, the words and n-grams
in the document, in the process of doing so learn-
ing useful document embeddings. Formally, the
objective function to be minimized is defined as
follows: ∑

d∈D

∑
wo∈Wd

− log p(wo|d) (1)



409

where d is a document, D is the set of all docu-
ments in the dataset, wo is an n-gram and Wd is
the set of all n-grams in the document d. p(wo|d)
is defined using softmax:

p(wo|d) =
eα cos θwo∑
w∈W e

α cos θw
(2)

= softmax(α cos θwo) (3)

We have cos θw defined as follows:

cos θw =
vTd vw
‖vd‖‖vw‖

(4)

where vd and vw are vector representations of the
document d and the word/n-gram w respectively
and are parameters to be learned. α is a hyperpa-
rameter. W is the set of all n-grams in the vocab-
ulary.

Normally, the softmax of the dot product be-
tween the input and output vector is used to model
the conditional probability term as follows:

p(wo|d) =
ev

T
d vwo∑

w∈W e
vTd vw

(5)

Whereas dot product ranges from negative infinity
to positive infinity, since cosine similarity ranges
from -1 to 1, using the cosine similarity term alone
as an input to the softmax function may not be
sufficient in modeling the conditional probability
distribution. Therefore, we add a scaling hyperpa-
rameter α to increase the range of possible proba-
bility values for each conditional probability term.6/12/2019 architecture.drawio

1/1

 nodes in
hidden layer
N

M|D|×N M
′
|W|×N

 nodes in
output layer
|W |

vd vw

d vd

softmax   
for 

(α cos )θw
w ∈ W

Figure 1: Proposed Architecture.

Figure 1 shows the architecture of the neural
network used in learning the document embed-
dings. There is a hidden layer with N nodes cor-
responding to the dimensionality of the paragraph
vectors and an output layer with |W | nodes corre-
sponding to the number of distinct n-grams found
in the dataset. There are two weight parameter ma-
trices to be learned: M , which may be seen as a

collection of |D| document vectors each havingN
dimensions, and M ′, which is a collection of |W |
n-gram vectors each also having N dimensions.

An input document id d is used to select its
vector representation vd which is exactly output
through the N nodes of the first hidden layer. The
output of each node in the output layer represents
the probability p(w|d) of its corresponding n-gram
w, and is calculated as in (2) using softmax.

3.2 Negative Sampling
Since the weight update equations for minimizing
(1) implies that we must update each output vec-
tor corresponding to each feature in the feature set
W , with extremely large vocabularies, this com-
putation is impractical. In (Mikolov et al., 2013),
the negative sampling technique is introduced as a
means to speed up the learning process and it can
be shown that the updates for the negative sam-
pling version of (1) as shown in (6) approximates
the weight updates carried out in minimizing (1).
Therefore in practice, the document embeddings
are obtained by minimizing the following objec-
tive function with stochastic gradient descent and
backpropagation (Rumelhart et al., 1986):∑
d∈D

∑
wo∈Wd

[
− log σ (α cos θwo)

−
∑

wn∈Wneg

log σ (−α cos θwn)
]

(6)

where Wneg is a set of negatively sampled words;
the size of the set or the negative sampling size
as well as the distribution used to draw negatively
sampled words/n-grams are hyperparameters. σ is
the sigmoid function.

By contrast, in the case of dot product the ob-
jective function is:∑
d∈D

∑
wo∈Wd

[
− log σ

(
vTd vwo

)
−

∑
wn∈Wneg

log σ
(
−vTd vwn

)]
(7)

while in the case of L2R dot product, the objective
function used is:∑
d∈D

∑
wo∈Wd

[
− log σ

(
vTd vwo

)
+
λ

2
‖vd‖2 +

λ

2
‖vwo‖2

−
∑

wn∈Wneg

(
log σ

(
−vTd vwn

)
+
λ

2
‖vwn‖2

)]
(8)



410

Features Dot Product Dot Product Cosine
(DV-ngram) with L2R Similarity

(%) (%) (%)

Unigrams 89.60 87.15 (-2.45) 90.75 (+1.15)
Unigrams+Bigrams 91.27 91.72 (+0.45) 92.56 (+1.29)
Unigrams+Bigrams+Trigrams 92.14 92.45 (+0.31) 93.13 (+0.99)

Table 1: Experimental Results.

where λ is the regularization strength.

4 Experiments

The models are benchmarked on the IMDB dataset
(Maas et al., 2011), which contains 25,000 train-
ing documents, 25,000 test documents, and 50,000
unlabeled documents. The IMDB dataset is a bi-
nary sentiment classification dataset consisting of
movie reviews retrieved from IMDB; training doc-
uments in the dataset are highly polar. For labeled
documents, there is a 1:1 ratio between negative
and positive documents. The document vectors
are learned using all the documents in the dataset
(train, test and unlabeled documents). The dataset
consists of mainly long movie reviews.

In order to train the document vectors on un-
igrams to trigrams, the reviews are preprocessed
in such a way that tokens representing bigrams
and trigrams are simply appended to the original
unigrams of the review itself. An L2-regularized
logistic regression (LR) classifier is used to clas-
sify the documents at the end of each epoch us-
ing the predefined train-test split. However, the
results reported in this paper include only the ac-
curacy obtained from classifying documents in
the final epoch. For any java implementations of
the LR classifier we use the LIBLINEAR library
(Fan et al., 2008) while for python implementa-
tions we use Sci-kit learn (Pedregosa et al., 2011).
Code to reproduce all experiments is available at
https://github.com/tanthongtan/dv-cosine.

4.1 Optimal Hyperparameters

Grid search was performed using 20% of the train-
ing data as a validation set in order to determine
the optimal hyperparameters as well as whether to
use a constant learning rate or learning rate an-
nealing. Table 2 shows the optimal hyperparam-
eters for the models on the IMDB dataset. We
did not tune the N hyperparameter or the nega-
tive sampling size and left it the same as in (Li
et al., 2016a) and (Lau and Baldwin, 2016). We
did however tune the number of iterations from

[10, 20, 40, 80, 120], learning rate from [0.25,
0.025, 0.0025, 0.001] and α from [4, 6, 8]. A sen-
sible value of α should be around 6, since looking
at the graph of the sigmoid function, for input val-
ues greater than 6 and less than -6, the sigmoid
function doesn’t change much and has values of
close to 1 and 0, respectively. In the case of us-
ing L2 regularized dot product, λ (regularization
strength) was chosen from [1, 0.1, 0.01].

Hyperparameter Dot L2R Dot Cos.
Prod. Prod. Sim.

N (dimensionality) 500 500 500
Neg. Sampling Size 5 5 5
Iterations 10 20 120
Learning Rate 0.25 0.025 0.001
α - - 6
λ - 0.01 -
LR annealing true false false

Table 2: Optimal Hyperparameters.

The optimal learning rate in the case of cosine
similarity is extremely small, suggesting a chaotic
error surface. Since the learning rate is already
small to begin with, no learning rate annealing is
used. The model in turn requires a larger number
of epochs for convergence. For the distribution for
sampling negative words, we used the n-gram dis-
tribution raised to the 3/4th power in accordance
with (Mikolov et al., 2013). The weights of the
networks were initialized from a uniform distribu-
tion in the range of [-0.001, 0.001].

4.2 Results

Each experiment was carried out 5 times and the
mean accuracy is reported in Table 1. This is to
account for random factors such as shuffling doc-
ument and word ids, and random initialization.
From here we see that using cosine similarity in-
stead of dot product improves accuracy across the
board. The results are most apparent in the case
of unigrams + bigrams. However it is not to sug-
gest that switching from dot product to cosine sim-
ilarity alone improves accuracy as other minor ad-



411

Figure 2: PCA visualization of embeddings trained with (a) dot product, (b) L2R dot product and (c) cos. similarity.

justments and hyperparameter tuning as explained
was done. However it may imply that using cosine
similarity allows for a higher potential accuracy as
was achieved in these experiments.

Regardless, we believe the comparisons are fair
since each model is using its own set of optimal
hyperparameters, but for the full sake of compari-
son, leaving everything the same and only switch-
ing out dot product for cosine similarity (α = 1) as
well as switching it out and using a sensible value
of α at α = 6 both achieve an accuracy of around
50%. This is because our model fails whenever
the learning rate is too large. As seen during grid
search, whenever the initial learning rate was 0.25,
accuracy was always poor.

Introducing L2 regularization to dot product im-
proves accuracy for all cases except a depreciation
in the case of using unigrams only, lucikily cosine
similarity does not suffer from this same depreci-
ation.

4.3 Discussion

From table 3, the mean Euclidean norm of embed-
dings trained with cosine similarity is lower than
that of L2R dot product which is in turn lower than
in the case of using dot product; this suggests that
the method employing cosine similarity acts as a
regularization mechanism, preventing the weights
from getting too large. Large magnitudes of doc-
ument vectors may be harder for the end classifier
to fit in such a way that generalizes well, which
may be why cosine similarity and L2R dot prod-
uct perform better than dot product on the IMDB
dataset.

As predicted, the mean cosine similarity be-
tween all pairs of vectors in the same class (Same
Mean Cos. Sim.) is higher in the case of cosine
similarity than the other two models. Unfortu-

nately, the mean for all pairs in different classes
(Diff. Mean Cos. Sim.) is higher as well. Fur-
ther analysis and hopefully some formalism as to
why cosine similarity performs better is a planned
future work.

Embedding Dot L2R Dot Cos.
Statistic Prod. Prod. Sim.

Same Mean Cos. Sim. 0.23 0.20 0.35
Diff. Mean Cos. Sim. 0.21 0.17 0.32
Mean Norm 8.91 6.30 5.35

Table 3: Embedding statistics.

Figure 2 shows the projection of the embed-
dings along their first two principle components,
different colors corresponding to different classes.
Cosine similarity shows slightly better seperabil-
ity between the two classes, while dot product and
L2R dot product are quite similar.

4.4 Feature Combination
Another contribution of this paper is demonstrat-
ing the effectiveness of concatenating naive bayes
weighted bag of n-grams with DV-ngram, L2R dot
product, or document vectors trained with cosine
similarity, the last achieving state of the art accu-
racy on the IMDB dataset. We note that all models
utilize unigrams to trigrams and additional unla-
beled data if possible. Table 4 shows a compari-
son between our proposed models (shown in bold)
and previous state of the arts and other published
results.

5 Conclusion and Future Work

Our proposed model trains document embeddings
using cosine similarity as the similarity measure
and we show that sentiment classification perfor-
mance on the IMDB dataset is improved when



412

Model IMDB Dataset
Accuracy (%)

NB-SVM Bigrams 91.22
(Wang and Manning, 2012)
NB-SVM Trigrams 91.87
(Mesnil et al., 2015)
DV-ngram 92.14
(Li et al., 2016a)
Dot Product with 92.45
L2 Regularization
Paragraph Vector 92.58
(Le and Mikolov, 2014)
Document Vectors using 93.13
Cosine Similarity
W-Neural-BON Ensemble 93.51
(Li et al., 2016b)
TGNR Ensemble 93.51
(Li et al., 2017)
TopicRNN 93.76
(Dieng et al., 2017)
One-hot bi-LSTM 94.06
(Johnson and Zhang, 2016)
Virtual Adversarial 94.09
(Miyato et al., 2016)
BERT large finetune UDA 95.80
(Xie et al., 2019)
NB-weighted-BON + 96.95
DV-ngram
NB-weighted-BON + 97.17
L2R Dot Product
NB-weighted-BON + 97.42
Cosine Similarity

Table 4: Comparison with other models.

utilizing these embeddings as opposed to those
trained using dot-product. Cosine similarity may
help reduce overfitting to the embedding task, and
this regularization in turn produces more useful
embeddings. We also show that concatenating
these embeddings with Naı̈ve bayes weighed bag
of n-grams results in high accuracy on the IMDB
dataset.

An important future development is to carry out
experiments on other datasets. It is essential that
we benchmark on more than one dataset, to pre-
vent superficially good results by overfitting hy-
perparameters or the cosine similarity model itself
to the IMDB dataset. Other tasks and datasets in-
clude: (1) sentiment analysis - the Stanford senti-
ment treebank dataset (Socher et al., 2013), the po-
larity dataset v2.0 (Pang and Lee, 2004), (2) topic
classification - AthR, XGraph, BbCrypt (Wang
and Manning, 2012), and (3) semantic relatedness
tasks - datasets from the SemEval semantic textual
similarity (STS) tasks (Agirre et al., 2015).

References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel

Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada
Mihalcea, German Rigau, Larraitz Uria, and Janyce
Wiebe. 2015. Semeval-2015 task 2: Semantic tex-
tual similarity, english, spanish and pilot on inter-
pretability. In Proceedings of the 9th International
Workshop on Semantic Evaluation, pages 252–263.

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings. In Proceedings of the 5th International
Conference on Learning Representations.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3(Feb):1137–1155.

Andrew M. Dai, Christopher Olah, and Quoc V. Le.
2015. Document embedding with paragraph vec-
tors. arXiv preprint arXiv:1507.07998.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Adji B. Dieng, Chong Wang, Jianfeng Gao, and John
Paisley. 2017. Topicrnn: A recurrent neural network
with long-range semantic dependency. In Proceed-
ings of the 5th International Conference on Learning
Representations.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9(Aug):1871–1874.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. In Proceedings of the 10th European
Conference on Machine Learning, pages 137–142.

Rie Johnson and Tong Zhang. 2016. Supervised and
semi-supervised text categorization using lstm for
region embeddings. In Proceedings of the 4th Inter-
national Conference on Learning Representations.

Jey Han Lau and Timothy Baldwin. 2016. An empiri-
cal evaluation of doc2vec with practical insights into
document embedding generation. arXiv preprint
arXiv:1607.05368.

Quoc V. Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Pro-
ceedings of the 31st International Conference on
Machine Learning, pages 1188–1196.



413

Bofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, and
Zhe Zhao. 2016a. Learning document embeddings
by predicting n-grams for sentiment classification of
long movie reviews. In Proceedings of the 4th Inter-
national Workshop on Learning Representations.

Bofang Li, Tao Liu, Zhe Zhao, Puwei Wang, and Xi-
aoyong Du. 2017. Neural bag-of-ngrams. In Pro-
ceedings of the 31st AAAI Conference on Artificial
Intelligence, pages 3067–3074.

Bofang Li, Zhe Zhao, Tao Liu, Puwei Wang, and Xi-
aoyong Du. 2016b. Weighted neural bag-of-n-grams
model: New baselines for text classification. In Pro-
ceedings of the 26th International Conference on
Computational Linguistics, pages 1591–1600.

Chunjie Luo, Jianfeng Zhan, Lei Wang, and Qiang
Yang. 2017. Cosine normalization: Using cosine
similarity instead of dot product in neural networks.
arXiv preprint arXiv:1702.05870.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, pages 142–
150.

Grégoire Mesnil, Tomas Mikolov, Marc’Aurelio Ran-
zato, and Yoshua Bengio. 2015. Ensemble of gen-
erative and discriminative techniques for sentiment
analysis of movie reviews. In Proceedings of the
3rd International Workshop on Learning Represen-
tations.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of the 26th International Con-
ference on Neural Information Processing Systems,
pages 3111–3119.

Takeru Miyato, Andrew M. Dai, and Ian Goodfel-
low. 2016. Adversarial training methods for semi-
supervised text classification. In Proceedings of the
4th International Conference on Learning Represen-
tations.

Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity sum-
marization based on minimum cuts. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics, page 271.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in python. Journal of Machine
Learning Research, 12(Oct):2825–2830.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference

of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 2227–2237.

David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323(6088):533–536.

Amit Singhal. 2001. Modern information retrieval: A
brief overview. IEEE Data Engineering Bulletin,
24(4):35–43.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of the 31st International
Conference on Neural Information Processing Sys-
tems, pages 5998–6008.

Sida Wang and Christopher D. Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, pages 90–94.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2015. Towards universal paraphrastic sen-
tence embeddings. In Proceedings of the 4th Inter-
national Conference on Learning Representations.

Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Lu-
ong, and Quoc V. Le. 2019. Unsupervised data aug-
mentation. arXiv preprint arXiv:1904.12848.

A Obtaining the weight update equations
in the case of cosine similarity

To obtain the weight update equations for the input
and output vectors of our model in each iteration
of stochastic gradient descent, we must find the
gradient of the error function at a given training
example, which may be considered a document,
n-gram pair.

Let:

E =− log σ (α cos θwo)

−
∑

wn∈Wneg

log σ (−α cos θwn) (9)

where:

cos θw =
vTd vw
‖vd‖‖vw‖

(10)



414

be the objective function at a single training ex-
ample (d,wo). Then, to find the gradient of E first
differentiate E with respect to cos θw:

∂E

∂ cos θw
= α (σ (α cos θw)− t) (11)

where t = 1 if w = wo; 0 otherwise. We then
obtain the derivative of E w.r.t. the output n-gram
vectors:

∂E

∂vw
=

∂E

∂ cos θw
· ∂ cos θw

∂vw
(12)

∂E

∂vw
= α (σ (α cos θw)− t)

·

(
vd

‖vd‖‖vw‖
−

vw
(
vTd vw

)
‖vd‖‖vw‖3

)
(13)

This leads to the following weight update equa-
tion for the output vectors:

v(new)w = v
(old)
w − η

∂E

∂vw
(14)

where η is the learning rate. This equation needs
to be applied to all w ∈ {wo} ∪ Wneg in each
iteration.

Next, the errors are backpropagated and the in-
put document vectors are updated. Differentiating
E with respect to vd:

∂E

∂vd
=

∑
w∈{wo}∪Wneg

∂E

∂ cos θw
· ∂ cos θw

∂vd
(15)

=
∑

w∈{wo}∪Wneg

α (σ (α cos θw)− t)

·

(
vw

‖vd‖‖vw‖
−

vd
(
vTd vw

)
‖vd‖3‖vw‖

)
(16)

Thus, we obtain the weight update equation for
the input vector in each iteration:

v
(new)
d = v

(old)
d − η

∂E

∂vd
(17)

B Weight update equations in the case of
dot product

This section contains the weight update equations
for the input and output vectors of the dot prod-
uct model in each iteration of stochastic gradient
descent.

The following weight update equations for the
output vectors:

v(new)w = v
(old)
w − η

(
σ
(
vTd vw

)
− t
)
· vd (18)

where t = 1 if w = wo; 0 otherwise, needs to be
applied to all w ∈ {wo} ∪Wneg in each iteration.

The following weight update equation needs to
be applied to the input vector in each iteration:

v
(new)
d = v

(old)
d − η

∑
w∈{wo}∪Wneg

(
σ
(
vTd vw

)
− t
)
· vw

(19)

C Weight update equations in the case of
L2R dot product

This section contains the weight update equations
for the input and output vectors of the L2R dot
product model in each iteration of stochastic gra-
dient descent.

The following weight update equations for the
output vectors:

v(new)w = v
(old)
w − η

(
σ
(
vTd vw

)
− t
)
· vd

− ηλvw (20)

where t = 1 if w = wo; 0 otherwise, needs to be
applied to all w ∈ {wo} ∪Wneg in each iteration.

The following weight update equation needs to
be applied to the input vector in each iteration:

v
(new)
d = v

(old)
d − η

∑
w∈{wo}∪Wneg

(
σ
(
vTd vw

)
− t
)
· vw

− ηλvd (21)


