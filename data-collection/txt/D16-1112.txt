



















































Neural Headline Generation on Abstract Meaning Representation


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1054–1059,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Neural Headline Generation on Abstract Meaning Representation

Sho Takase† Jun Suzuki‡ Naoaki Okazaki† Tsutomu Hirao‡ Masaaki Nagata‡
Graduate School of Information Sciences, Tohoku University†

NTT Communication Science Laboratories, NTT Corporation‡

{takase, okazaki}@ecei.tohoku.ac.jp
{suzuki.jun, hirao.tsutomu, nagata.masaaki}@lab.ntt.co.jp

Abstract

Neural network-based encoder-decoder mod-
els are among recent attractive methodologies
for tackling natural language generation tasks.
This paper investigates the usefulness of struc-
tural syntactic and semantic information ad-
ditionally incorporated in a baseline neural
attention-based model. We encode results ob-
tained from an abstract meaning representa-
tion (AMR) parser using a modified version
of Tree-LSTM. Our proposed attention-based
AMR encoder-decoder model improves head-
line generation benchmarks compared with
the baseline neural attention-based model.

1 Introduction

Neural network-based encoder-decoder models are
cutting-edge methodologies for tackling natural lan-
guage generation (NLG) tasks, i.e., machine trans-
lation (Cho et al., 2014), image captioning (Vinyals
et al., 2015), video description (Venugopalan et al.,
2015), and headline generation (Rush et al., 2015).

This paper also shares a similar goal and moti-
vation to previous work: improving the encoder-
decoder models for natural language generation.
There are several directions for enhancement. This
paper respects the fact that NLP researchers have
expended an enormous amount of effort to develop
fundamental NLP techniques such as POS tagging,
dependency parsing, named entity recognition, and
semantic role labeling. Intuitively, this structural,
syntactic, and semantic information underlying in-
put text has the potential for improving the quality of
NLG tasks. However, to the best of our knowledge,

there is no clear evidence that syntactic and seman-
tic information can enhance the recently developed
encoder-decoder models in NLG tasks.

To answer this research question, this paper pro-
poses and evaluates a headline generation method
based on an encoder-decoder architecture on Ab-
stract Meaning Representation (AMR). The method
is essentially an extension of attention-based sum-
marization (ABS) (Rush et al., 2015). Our pro-
posed method encodes results obtained from an
AMR parser by using a modified version of Tree-
LSTM encoder (Tai et al., 2015) as additional in-
formation of the baseline ABS model. Conceptu-
ally, the reason for using AMR for headline gen-
eration is that information presented in AMR, such
as predicate-argument structures and named entities,
can be effective clues when producing shorter sum-
maries (headlines) from original longer sentences.
We expect that the quality of headlines will improve
with this reasonable combination (ABS and AMR).

2 Attention-based summarization (ABS)

ABS proposed in Rush et al. (2015) has achieved
state-of-the-art performance on the benchmark data
of headline generation including the DUC-2004
dataset (Over et al., 2007). Figure 1 illustrates the
model structure of ABS. The model predicts a word
sequence (summary) based on the combination of
the neural network language model and an input sen-
tence encoder.

Let V be a vocabulary. xi is the i-th indicator
vector corresponding to the i-th word in the input
sentence. Suppose we have M words of an input
sentence. X represents an input sentence, which

1054



<s>   canadian   prime  …   year <s>  canada     …     nato

Fx1 Fx3Fx2 FxM
Eyi�C+1 Eyi

E0yiE
0yi�C+1

nnlmenc

yi+1

input sentence headline

Figure 1: Model structure of ‘attention-based sum-
marization (ABS)’.

is represented as a sequence of indicator vectors,
whose length is M . That is, xi ∈ {0, 1}|V |, and
X = (x1, . . . , xM ). Similarly, let Y represent a
sequence of indicator vectors Y = (y1, . . . , yL),
whose length is L. Here, we assume L < M . YC,i is
a short notation of the list of vectors, which consists
of the sub-sequence in Y from yi−C+1 to yi. We
assume a one-hot vector for a special start symbol,
such as “⟨S⟩”, when i < 1. Then, ABS outputs a
summary Ŷ given an input sentence X as follows:

Ŷ = arg max
Y

{
log p(Y |X)

}
, (1)

log p(Y |X) ≈
L−1∑

i=0

log p(yi+1|X, YC,i), (2)

p(yi+1|X, YC,i)
∝ exp

(
nnlm(YC,i) + enc(X, YC,i)

)
, (3)

where nnlm(YC,i) is a feed-forward neural network
language model proposed in (Bengio et al., 2003),
and enc(X, YC,i) is an input sentence encoder with
attention mechanism.

This paper uses D and H as denoting sizes (di-
mensions) of vectors for word embedding and hid-
den layer, respectively. Let E ∈ RD×|V | be an
embedding matrix of output words. Moreover, let
U ∈ RH×(CD) and O ∈ R|V |×H be weight matri-
ces of hidden and output layers, respectively1. Using
the above notations, nnlm(YC,i) in Equation 3 can
be written as follows:

nnlm(YC,i) = Oh, h = tanh(Uỹc), (4)
1Following Rush et al. (2015), we omit bias terms through-

out the paper for readability, though each weight matrix also has
a bias term.

where ỹc is a concatenation of output embed-
ding vectors from i − C + 1 to i, that is, ỹc =
(Eyi−C+1 · · ·Eyi). Therefore, ỹc is a (CD) di-
mensional vector.

Next, F ∈ RD×|V | and E′ ∈ RD×|V | denote
embedding matrices of input and output words, re-
spectively. O′ ∈ R|V |×D is a weight matrix for the
output layer. P ∈ RD×(CD) is a weight matrix for
mapping embedding of C output words onto embed-
ding of input words. X̃ is a matrix form of a list
of input embeddings, namely, X̃ =

[
x̃1, . . . , x̃M

]
,

where x̃i = Fxi. Then, enc(X, YC,i) is defined as
the following equations:

enc(X,YC,i) = O
′X̄p, (5)

p ∝ exp(X̃TP ỹ′c), (6)

where ỹ′c is a concatenation of output embedding
vectors from i − C + 1 to i similar to ỹc, that is,
ỹ′c = (E

′yi−C+1 · · ·E′yi). Moreover, X̄ is a
matrix form of a list of averaged input word em-
beddings within window size Q, namely, X̄ =
[x̄1, . . . , x̄M ], where x̄i =

∑i+Q
q=i−Q

1
Q x̃q.

Equation 6 is generally referred to as the atten-
tion model, which is introduced to encode a rela-
tionship between input words and the previous C
output words. For example, if the previous C output
words are assumed to align to xi, then the surround-
ing Q words (xi−Q, . . . , xi+Q) are highly weighted
by Equation 5.

3 Proposed Method

Our assumption here is that syntactic and semantic
features of an input sentence can greatly help for
generating a headline. For example, the meanings
of subjects, predicates, and objects in a generated
headline should correspond to the ones appearing in
an input sentence. Thus, we incorporate syntactic
and semantic features into the framework of head-
line generation. This paper uses an AMR as a case
study of the additional features.

3.1 AMR

An AMR is a rooted, directed, acyclic graph that
encodes the meaning of a sentence. Nodes in an
AMR graph represent ‘concepts’, and directed edges
represent a relationship between nodes. Concepts

1055



“canadian”

name prime

country

announce

…

<s>     canada     …     nato

E0yiE0yi�C+1

tree

op
1

na
me

mod
a1

a2 a3

aj

…

…

Rush’s Model

yi+1

AMR of 
the input sentence

summary

“canadian”

name prime

country

announce

…

<s>     canada     …     nato

E0yiE0yi�C+1

op
1

na
me

mod
a1

a2 a3

aj

…

…

ABS

yi+1

AMR of 
the input sentence

headline

encAMR

Figure 2: Model structure of our proposed attention-
based AMR encoder; it outputs a headline using
ABS and encoded AMR with attention.

consist of English words, PropBank event predi-
cates, and special labels such as “person”. For
edges, AMR has approximately 100 relations (Ba-
narescu et al., 2013) including semantic roles based
on the PropBank annotations in OntoNotes (Hovy et
al., 2006). To acquire AMRs for input sentences,
we use the state-of-the-art transition-based AMR
parser (Wang et al., 2015).

3.2 Attention-Based AMR Encoder
Figure 2 shows a brief sketch of the model struc-
ture of our attention-based AMR encoder model. We
utilize a variant of child-sum Tree-LSTM originally
proposed in (Tai et al., 2015) to encode syntactic
and semantic information obtained from output of
the AMR parser into certain fixed-length embedding
vectors. To simplify the computation, we transform
a DAG structure of AMR parser output to a tree
structure, which we refer to as “tree-converted AMR
structure”. This transformation can be performed by
separating multiple head nodes, which often appear
for representing coreferential concepts, to a corre-
sponding number of out-edges to head nodes. Then,
we straightforwardly modify Tree-LSTM to also en-
code edge labels since AMR provides both node and
edge labels, and original Tree-LSTM only encodes
node labels.

Let nj and ej be N and E dimensional em-
beddings for labels assigned to the j-th node, and
the out-edge directed to its parent node2. Win,
Wfn, Won, Wun ∈ RD×N are weight matrices

2We prepare a special edge embedding for a root node.

for node embeddings nj3. Similarly, Wie, Wfe,
Woe, Wue ∈ RD×E are weight matrices for edge
embeddings ej . Wih, Wfh, Woh, Wuh ∈ RD×D
are weight matrices for output vectors connected
from child nodes. B(j) represents a set of nodes,
which have a direct edge to the j-th node in our tree-
converted AMR structure. Then, we define embed-
ding aj obtained at node j in tree-converted AMR
structure via Tree-LSTM as follows:

h̃j =
∑

k∈B(j)
ak, (7)

ij = σ
(
Winnj + Wieej + Wihh̃j

)
, (8)

fjk = σ
(
Wfnnj + Wfeej + Wfhak

)
, (9)

oj = σ
(
Wonnj + Woeej + Wohh̃j

)
, (10)

uj = tanh
(
Wunnj + Wueej + Wuhh̃j

)
, (11)

cj = ij ⊙ uj
∑

k∈B(j)
fjk ⊙ ck, (12)

aj = oj ⊙ tanh(cj). (13)

Let J represent the number of nodes in tree-
converted AMR structure obtained from a given in-
put sentence. We introduce A ∈ RD×J as a matrix
form of a list of hidden states aj for all j, namely,
A = [a1, . . . , aJ ]. Let O′′ ∈ R|V |×D be a weight
matrix for the output layer. Let S ∈ RD×(CD) be
a weight matrix for mapping the context embedding
of C output words onto embeddings obtained from
nodes. Then, we define the attention-based AMR
encoder ‘encAMR(A, YC,i)’ as follows:

encAMR(A, YC,i) = O
′′As, (14)

s ∝ exp(ATSỹ′c). (15)

Finally, we combine our attention-based AMR en-
coder shown in Equation 14 as an additional term of
Equation 3 to build our headline generation system.

4 Experiments

To demonstrate the effectiveness of our proposed
method, we conducted experiments on benchmark
data of the abstractive headline generation task de-
scribed in Rush et al. (2015).

3As with Equation 4, all the bias terms are omitted, though
each weight matrix has one.

1056



DUC-2004 Gigaword test data used Gigaword
in (Rush et al., 2015) Our sampled test data

Method R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L
ABS (Rush et al., 2015) 26.55 7.06 22.05 30.88 12.22 27.77 – – –
ABS (re-run) 28.05 7.38 23.15 31.26 12.46 28.25 32.93 13.43 29.80
ABS+AMR ∗28.80 ∗7.83 ∗23.62 31.64 ∗12.94 28.54 ∗33.43 ∗13.93 30.20
ABS+AMR(w/o attn) 28.28 7.21 23.12 30.89 12.40 27.94 31.32 12.83 28.46

Table 1: Results of methods on each dataset. We marked ∗ on the ABS+AMR results if we observed
statistical difference (p < 0.05) between ABS (re-run) and ABS+AMR on the t-test. (R-1: ROUGE-1, R-2:
ROUGE-2, R-L: ROUGE-L)

For a fair comparison, we followed their evalu-
ation setting. The training data was obtained from
the first sentence and the headline of a document
in the annotated Gigaword corpus (Napoles et al.,
2012)4. The development data is DUC-2003 data,
and test data are both DUC-2004 (Over et al., 2007)
and sentence-headline pairs obtained from the an-
notated Gigaword corpus as well as training data5.
All of the generated headlines were evaluated by
ROUGE (Lin, 2004)6. For evaluation on DUC-
2004, we removed strings after 75-characters for
each generated headline as described in the DUC-
2004 evaluation. For evaluation on Gigaword, we
forced the system outputs to be at most 8 words as
in Rush et al. (2015) since the average length of
headline in Gigaword is 8.3 words. For the pre-
processing for all data, all letters were converted to
lower case, all digits were replaced with ‘#’, and
words appearing less than five times with ‘UNK’.
Note that, for further evaluation, we prepared 2,000
sentence-headline pairs randomly sampled from the
test data section of the Gigaword corpus as our ad-
ditional test data.

In our experiments, we refer to the baseline neural
attention-based abstractive summarization method
described in Rush et al. (2015) as “ABS”, and our
proposed method of incorporating AMR structural
information by a neural encoder to the baseline
method described in Section 3 as “ABS+AMR”.
Additionally, we also evaluated the performance of

4Training data can be obtained by using the script distributed
by the authors of Rush et al. (2015).

5Gigaword test data can be obtained from https://
github.com/harvardnlp/sent-summary

6We used the ROUGE-1.5.5 script with option “−n2 −m
−b75 −d”, and computed the average of each ROUGE score.

the AMR encoder without the attention mechanism,
which we refer to as “ABS+AMR(w/o attn)”, to
investigate the contribution of the attention mech-
anism on the AMR encoder. For the parameter es-
timation (training), we used stochastic gradient de-
scent to learn parameters. We tried several val-
ues for the initial learning rate, and selected the
value that achieved the best performance for each
method. We decayed the learning rate by half if the
log-likelihood on the validation set did not improve
for an epoch. Hyper-parameters we selected were
D = 200, H = 400, N = 200, E = 50, C = 5, and
Q = 2. We re-normalized the embedding after each
epoch (Hinton et al., 2012).

For ABS+AMR, we used the two-step training
scheme to accelerate the training speed. The first
phase learns the parameters of the ABS. The second
phase trains the parameters of the AMR encoder by
using 1 million training pairs while the parameters
of the baseline ABS were fixed and unchanged to
prevent overfitting.

Table 1 shows the recall of ROUGE (Lin, 2004)
on each dataset. ABS (re-run) represents the perfor-
mance of ABS re-trained by the distributed scripts7.
We can see that the proposed method, ABS+AMR,
outperforms the baseline ABS on all datasets. In
particular, ABS+AMR achieved statistically signif-
icant gain from ABS (re-run) for ROUGE-1 and
ROUGE-2 on DUC-2004. However in contrast, we
observed that the improvements on Gigaword (the
same test data as Rush et al. (2015)) seem to be lim-
ited compared with the DUC-2004 dataset. We as-
sume that this limited gain is caused largely by the
quality of AMR parsing results. This means that the

7https://github.com/facebook/NAMAS

1057



•  a"
I(1): crown prince abdallah ibn abdel aziz left saturday at the head of 
saudi arabia 's delegation to the islamic summit in islamabad , the 
official news agency spa reported .
G: saudi crown prince leaves for islamic summit
A: crown prince leaves for islamic summit in saudi arabia
P: saudi crown prince leaves for islamic summit in riyadh

I(2): a massive gothic revival building once christened the lunatic 
asylum west of the <unk> was auctioned off for $ #.# million -lrb- 
euro# .# million -rrb- .
G: massive ##th century us mental hospital fetches $ #.# million at 
auction
A: west african art sells for $ #.# million in
P: west african art auctioned off for $ #.# million

I(3): brooklyn , the new bastion of cool for many new yorkers , is 
poised to go mainstream chic .
G: high-end retailers are scouting sites in brooklyn
A: new yorkers are poised to go mainstream with chic
P: new york city is poised to go mainstream chic

Figure 3: Examples of generated headlines on Giga-
word. I: input, G: true headline, A: ABS (re-run),
and P: ABS+AMR.

Gigaword test data provided by Rush et al. (2015)
is already pre-processed. Therefore, the quality of
the AMR parsing results seems relatively worse on
this pre-processed data since, for example, many
low-occurrence words in the data were already re-
placed with “UNK”. To provide evidence of this as-
sumption, we also evaluated the performance on our
randomly selected 2,000 sentence-headline test data
also taken from the test data section of the annotated
Gigaword corpus. “Gigaword (randomly sampled)”
in Table 1 shows the results of this setting. We found
the statistical difference between ABS(re-run) and
ABS+AMR on ROUGE-1 and ROUGE-2.

We can also observe that ABS+AMR achieved the
best ROUGE-1 scores on all of the test data. Ac-
cording to this fact, ABS+AMR tends to success-
fully yield semantically important words. In other
words, embeddings encoded through the AMR en-
coder are useful for capturing important concepts
in input sentences. Figure 3 supports this observa-
tion. For example, ABS+AMR successfully added
the correct modifier ‘saudi’ to “crown prince” in the
first example. Moreover, ABS+AMR generated a
consistent subject in the third example.

The comparison between ABS+AMR(w/o attn)
and ABS+AMR (with attention) suggests that the
attention mechanism is necessary for AMR encod-
ing. In other words, the encoder without the atten-
tion mechanism tends to be overfitting.

5 Related Work

Recently, the Recurrent Neural Network (RNN) and
its variant have been applied successfully to various
NLP tasks. For headline generation tasks, Chopra
et al. (2016) exploited the RNN decoder (and its
variant) with the attention mechanism instead of the
method of Rush et al. (2015): the combination of the
feed-forward neural network language model and
attention-based sentence encoder. Nallapati et al.
(2016) also adapted the RNN encoder-decoder with
attention for headline generation tasks. Moreover,
they made some efforts such as hierarchical atten-
tion to improve the performance. In addition to us-
ing a variant of RNN, Gulcehre et al. (2016) pro-
posed a method to handle infrequent words in nat-
ural language generation. Note that these recent
developments do not conflict with our method us-
ing the AMR encoder. This is because the AMR
encoder can be straightforwardly incorporated into
their methods as we have done in this paper, incor-
porating the AMR encoder into the baseline. We be-
lieve that our AMR encoder can possibly further im-
prove the performance of their methods. We will test
that hypothesis in future study.

6 Conclusion

This paper mainly discussed the usefulness of in-
corporating structural syntactic and semantic infor-
mation into novel attention-based encoder-decoder
models on headline generation tasks. We selected
abstract meaning representation (AMR) as syntac-
tic and semantic information, and proposed an
attention-based AMR encoder-decoder model. The
experimental results of headline generation bench-
mark data showed that our attention-based AMR
encoder-decoder model successfully improved stan-
dard automatic evaluation measures of headline gen-
eration tasks, ROUGE-1, ROUGE-2, and ROUGE-
L. We believe that our results provide empirical ev-
idence that syntactic and semantic information ob-
tained from an automatic parser can help to improve
the neural encoder-decoder approach in NLG tasks.

Acknowledgments

We thank the anonymous reviewers for their insight-
ful comments and suggestions.

1058



References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina

Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract Meaning Representation
for Sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop and Interoperability with Dis-
course, pages 178–186.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A Neural Probabilistic Lan-
guage Model. The Journal of Machine Learning Re-
search, 3:1137–1155.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. 2014. Learning Phrase Repre-
sentations using RNN Encoder–Decoder for Statistical
Machine Translation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2014), pages 1724–1734.

Sumit Chopra, Michael Auli, and Alexander M. Rush.
2016. Abstractive Sentence Summarization with At-
tentive Recurrent Neural Networks. In Proceedings
of the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT 2016),
pages 93–98.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,
Bowen Zhou, and Yoshua Bengio. 2016. Pointing the
Unknown Words. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2016), pages 140–149.

Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Im-
proving Neural Networks by Preventing Co-adaptation
of Feature Detectors. CoRR, abs/1207.0580.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics (HLT-NAACL 2006), pages 57–60.

Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Text Summariza-
tion Branches Out: Proceedings of the Association for
Computational Linguistics Workshop, pages 74–81.

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Caglar Gulcehre, and Bing Xiang. 2016. Abstrac-
tive Text Summarization using Sequence-to-sequence
RNNs and Beyond. In Proceedings of the 20th
SIGNLL Conference on Computational Natural Lan-
guage Learning (CoNLL 2016), pages 280–290.

Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated Gigaword. In Proceed-

ings of the Joint Workshop on Automatic Knowledge
Base Construction and Web-scale Knowledge Extrac-
tion (AKBC-WEKEX), pages 95–100.

Paul Over, Hoa Dang, and Donna Harman. 2007. DUC
in Context. Information Processing and Management,
43(6):1506–1520.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A Neural Attention Model for Abstractive Sen-
tence Summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2015), pages 379–389.

Kai Sheng Tai, Richard Socher, and Christopher D. Man-
ning. 2015. Improved Semantic Representations
From Tree-Structured Long Short-Term Memory Net-
works. In Proceedings of the 53rd Annual Meeting
of the Association for Computational Linguistics and
the 7th International Joint Conference on Natural Lan-
guage Processing (ACL-IJCNLP 2015), pages 1556–
1566.

Subhashini Venugopalan, Huijuan Xu, Jeff Donahue,
Marcus Rohrbach, Raymond Mooney, and Kate
Saenko. 2015. Translating Videos to Natural Lan-
guage Using Deep Recurrent Neural Networks. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2015), pages 1494–1504.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-
mitru Erhan. 2015. Show and Tell: A Neural Image
Caption Generator. In Proceedings of the Computer
Vision and Pattern Recognition (CVPR 2015), pages
3156–3164.

Chuan Wang, Nianwen Xue, and Sameer Pradhan. 2015.
A Transition-based Algorithm for AMR Parsing. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2015), pages 366–375.

1059


