



















































Translation Class Instruction as Collaboration in the Act of Translation


Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 34–42,
Baltimore, Maryland USA, June 26, 2014. c©2014 Association for Computational Linguistics

Translation Class Instruction as Collaboration 
in the Act of Translation 

 
Lars Ahrenberg 

Department of Computer and  
Information Science,  
Linköping University 

lars.ahrenberg@liu.se 

 Ljuba Tarvi 
University of Helsinki 

Helsinki, Finland 
ljuba.tarvi@welho.com 

 
  

 

Abstract 

The paper offers an effective way of 
teacher-student computer-based collabo-
ration in translation class. We show how 
a quantitative-qualitative method of 
analysis supported by word alignment 
technology can be applied to student 
translations for use in the classroom. The 
combined use of natural-language pro-
cessing and manual techniques enables 
students to ‘co-emerge’ during highly 
motivated collaborative sessions. Within 
the advocated approach, students are pro-
active seekers for a better translation 
(grade) in a teacher-centered computer-
based peer-assisted translation class.  

1 Introduction 

Tools for computer-assisted translation (CAT), 
including translation memories, term banks, and 
more, are nowadays standard tools for transla-
tors. The proper use of such tools and resources 
are also increasingly becoming obligatory parts 
of translator training. Yet we believe that transla-
tion technology has more to offer translator 
training, in particular as a support for classroom 
interaction. Our proposal includes a quantitative 
analysis of translations, supported by word 
alignment technology, to enable joint presenta-
tion, discussion, and assessment of individual 
student translation in class. For comparisons 
with related work, see section 4. 

From the pedagogical point of view, the sug-
gested procedure embraces at least four types of 
evaluation: students’ implied self-evaluation, a 
preliminary computer evaluation, teacher’s eval-
uation after manually correcting the imperfect 
computer alignment and assessment, and peer 

evaluation during the collaborative team work in 
class, when the versions produced by the stu-
dents are simultaneously displayed, discussed 
and corrected if necessary.  

Theoretically, translations are viewed here as 
mappings between two languages through emer-
gent conceptual spaces based on an intermediate 
level of representation (e.g., Honkela et. al., 
2010). In terms of praxis, the basic approach is 
rooted in the idea (Vinay & Darbelnet, 1958) of 
consecutive numbering of the tokens (words) in 
the original text. This simple technique enables -
finding and labeling, in accordance with a cho-
sen set of rules, certain isomorphic correspond-
ences between the source and target tokens. 
Finding such correspondences is what current 
machine translation approaches attempt to 
achieve by statistical means in the training 
phase. 

The quantitative-qualitative technique we use 
here is the Token Equivalence Method (TEM) 
(Tarvi 2004). The use of the TEM in translation 
teaching originated as an argument (involving 
the second author) in a teacher-student debate 
over the relevance of a grade. The considerable 
time spent on the manual preparation of texts for 
translation using the TEM proved to be fairly 
well compensated for by the evident objectivity 
of the grades - the argument that, say, only 65% 
of the original text has been retained in a transla-
tion is difficult to brush aside. Later, the method 
was applied in research. Tarvi (2004) compared 
the classical Russian novel in verse by A. Push-
kin Eugene Onegin (1837) with its nineteen Eng-
lish translations. Figures calculated manually on 
10% of the text of the novel showed an excellent 
fit with the results on the same material obtained 
elsewhere by conventional comparative meth-
ods. Thus, we believe that characterizations of 
relations between source and target texts in ob-

34



jective terms is a good thing for translation eval-
uation. 

1.1 The TEM: Basics and Example 

Methodologically, the TEM focuses not on 
translation ‘shifts’ but on what has been kept in 
translation. The basic frame for analysis in the 
TEM is the Token Frame (2.2.1), which accounts 
for the number of the original tokens retained in 
translations. The other four frames (2.2.2-3, 
2.3.1-2), although useful in gauging the compar-
ative merits of the translations and the individual 
strategies, are optional.  

To concisely illustrate the method, one sen-
tence will be used – the famous 13-token open-
ing sentence of Leo Tolstoy’s Anna Karenina:  
Vse  schastlivye  semyi  pohozhi drug na druga, 
kazhdaya neschastlivaya semya neschastliva  po 
svoemu. (All happy families resemble one an-
other, every unhappy  family is unhappy in its 
own way.) 

Eight English translations of this sentence 
(2.1) will be used for analysis.  

The source text and all its translations are to-
kenized and analyzed linguistically in different 
ways. NLP tools  such as lemmatizers, part-of-
speech taggers and parsers can be applied. Most 
importantly, however, to support the computa-
tion of the Token Frames (2.2.1), they must be 
word-aligned with the source text (2.6). The 
teacher or the students are expected to review the 
alignments and correct them if they are not ac-
ceptable.  Given the corrected alignments, the 
aligned data can be used by the teacher and the 
students in the classroom. 

After this introduction of the basics of the 
theoretical approach and relevant automatic 
methods for their implementation, the paper is 
built around the basic structural foci of any in-
struction unit: before class (2), in class (3), and 
outside class (4).  

2 Before class 

This section describes the techniques of pro-
cessing Source Texts (ST) and Target Texts (TT) 
by teachers and students.  

2.1 Token Numbering and Labeling 

The procedure starts with consecutive number-
ing of the original tokens:  
 
(1)Vse (2)schastlivye (3)semyi (4)pohozhi (5)drug (6)na 

(7)druga, (8)kazhdaya (9)neschastlivaya (10)semya 
(11)neschastliva (12)po (13)svoemu.  

 

The second step is establishing, via the proce-
dure of (corrected) alignment, the correspond-
ences between the Source tokens (St) and Target 
tokens (Tt). As a result, every corresponding TT 
token (Tt), if found, is designated with the num-
ber of its source counterpart (St). Besides, since 
no Tt may remain unlabeled, two types of Tts 
which have no counterparts in the ST are labeled 
as Extra tokens (2.3.1) and Formal tokens 
(2.3.2). Here are the eight translations of the ex-
ample sentence: 
 
Leo Wiener: (1899): 
(1)All (2)happy (3)families (4)resemble (5-6-7)one an-
other; (8)every (9)unhappy (10)family (Ft)is 
(11)unhappy (12)in (Ft)its (13)own way.  

 

Constance Garnett (1901):  
(2)Happy (3)families (Ft)are (1)all (4)alike; (8)every 

(9)unhappy (10)family (Ft)is (11)unhappy (12)in (Ft)its 
(13)own way. 
 
Rochelle S. Townsend (1912):  
(1)All (2)happy (3)families (Ft)are (Et)more (Et)or 
(Et)less (4)like (5-6-7)one another; (8)every (9)unhappy 

(10)family (Ft)is (11)unhappy (12)in (Ft)its (13)own 
(Et)particular way.  

 
Aylmer & Louise Maude (1918):   
(1)All (2)happy (3)families (4)resemble (5-6-7)one an-
other, (Et)but (8)each (9)unhappy (10)family (Ft)is 
(11)unhappy (12)in (Ft)its (13)own way. 
 

Rosemary Edmonds (1954):   
(1)All (2)happy (3)families (Ft)are (4)alike; (Et)but 

(Ft)an (9)unhappy (10)family (Ft)is (11)unhappy 
(12)after (Ft)its (13)own fashion. 
 
Joel Carmichael (1960):  
(2)Happy (3)families (Ft)are (1)all (4)alike; (8)every 
(9)unhappy (10)family (Ft)is (11)unhappy (12)in (Ft)its 

(13)own way. 
 

David Magarschack (1961):  
(1)All (2)happy (3)families (Ft)are (4)like (5-6-7)one 
another; (8)each (9)unhappy (10)family (Ft)is 
(11)unhappy (12)in (Ft)its (13)own way.  
 
Richard Pevear & Larisa Volokhonsky (2000):  

(1)All (2)happy (3)families (Ft)are (4)alike; (8)each 
(9)unhappy (10)family (Ft)is (11)unhappy (12)in (Ft)its 

(13)own way.  
 

As is seen, two of the versions are clones 
(Carmichael, Pevear-Volokhonsky), one transla-
tion (Garnett) differs from the original only by 
the choice of the adjective (St 8), while the re-
maining five versions are more diverse. Note the 
mode of denoting Tts suggested here: only the 

35



meaningful denotative tokens get labeled, e.g., 
are (4)alike, or is (11)unhappy; if not one Tt but 
a group of tokens is used as an isomorph to a 
single St, the whole group is underlined, e.g., 
(13)own way, or (13)own fashion.  

Although St 4 has been rendered as are alike 
(Edmonds, Pevear-Volokhonsky, Garnett, Car-
michael), are like (Townsend, Magarschack), 
and resemble (Wiener, the Maudes), all these 
rendering are viewed as retaining the denotative 
meaning of the original token. Or, for instance, 
St 12, whether rendered as after (Edmonds) or in 
(all the rest), is also viewed as retained in trans-
lation. The connotative shades of meaning most 
suitable for the outlined goals can be discussed 
in class (3.2).  

This mode of displaying the isomorphisms 
can be converted to the style of representation 
used in word alignment systems such as Giza++ 
(Och and Ney, 2003) as follows: Extra tokens 
and Formal tokens give rise to null links. Groups 
of tokens that correspond yield groups of links. 
Thus, the analysis for Wiener’s translation wo-
uld come out as below: 
1-1 2-2 3-3 4-4 5-5 5-6 6-5 6-6 7-5 7-6 8-7 9-8 
10-9 0-10 11-11 12-12 0-13 13-14 13-15. 

In gauging the content, two types of basic and 
optional analytical frames, content and formal, 
are used. Based on the way of calculating the 
results, the analytical frames will be considered 
here in two sections, percentage frames (2.2) and 
count frames (2.3).  
 
2.2  The TEM: Percentage Frames 
 
The results in these frames are calculated as per-
centages of the ST information retained in trans-
lations.  

2.2.1 Basic Content Frame (Token Frame) 

After finding the isomorphic counterparts, the 
percentages of the retained tokens are presented 
in Table 1 (column I). As one can see, Wiener, 
the Maudes, Magarschack and Townsend trans-
lated all thirteen tokens and, hence, scored 100% 
each; Garnett, Carmichael and Pevear-
Volokhonsky omitted Sts 5-6-7 and thus scored 
76%, while Edmonds left out four tokens, Sts 5-
6-7-8, thus retaining 69% of the original.  

2.2.2 Optional Formal Frame 1 (Morphology 
Frame) 

In this frame, if a token is rendered with the 
same part of speech as in the original, the Tt in 

question gets a count. As can be seen in Table 1 
(column II), only two translators, Wiener and the 
Maudes, kept the same type of predicate 1 (St 4) 
as in the original – resemble – while in the re-
maining six translations the type of predicate 1 
has been changed into a compound one: are 
alike (Edmonds, Pevear-Volokhonsky, Garnett, 
Carmichael), and are like (Townsend, 
Magarschack). Therefore, in this frame, Wiener 
and the Maudes get a 100% each; Edmonds, 
with her two changed parts of speech, gets 84%, 
while the remaining five translators, who 
changed one part of speech each, score 92%.  

2.2.3 Optional Formal Frame 2 (Syntax) 

Another possible way of gauging the ‘presence’ 
of the original in its translation is monitoring the 
syntactic changes. If at least two tokens are ren-
dered in the same sequence as in the original and 
preserve the same syntactic functions, they are 
considered syntactically kept. Non-translated Sts 
are viewed as non-kept syntactic positions. Table 
1 (column III) shows that Edmonds, who lost 
four syntactic positions, scores 76%, Garnett, 
Magarschack and Townsend get 92% each, the 
rest translators score a 100%.  
 
2.2.4 The Translation Quotient (TQ) 
As a result of either manual or computer-assisted 
translation processing, the teacher gets a tabulat-
ed picture (Table 1) of the three analyzed frames 
(columns I, II, III).  

In an attempt to combine the obtained figures 
in a meaningful whole, the Translation Quotient 
parameter (TQ, column IV) is used: it is the 
arithmetic mean of the percentages in the moni-
tored frames. If one adds up the percentage re-
sults in all three frames and divides the obtained 
figure by the number of frames, one gets a TQ, 
measured in percentage points (pp), which re-
flects a general quantitative picture of the con-
tent-form rendering of the original. This cumula-
tive parameter has shown a perfect fit with the 
results obtained by other methods of compara-
tive assessment (Tarvi 2004). Table 1 shows four 
groups of TQ results, from 100% (2 versions) 
through 97% (2) through 86% (3) to 74% (1). 
 
2.3 The TEM: Count Frames 
To further differentiate the translations in their 
closeness to the original, pure counts of some 
quantitative parameters can be added to the pic-
ture in Table 1: column V (extra tokens, Ets) and 
VI (formal Tokens, Fts).  

36



2.3.1 Optional Content Frame 1 

This frame is a useful tool of assessment, as it 
shows what has been added to the translation, 
i.e., the Tts that have no counterparts in the orig-
inal, labeled as extra Tokens (Et). Table 1 (col-
umn V) shows that Wiener, Magarschack, Gar-
nett, Carmachael, and Pevear-Volokhonsky add-
ed no extra Tokens (Ets), the Maudes and Ed-
monds added by one Et each, while Townsend – 
four.  

2.3.2 Optional Formal Frame 3 

In this frame, the center of attention is formal 
Tokens (Fts) – articles, tense markers, etc. Table 
1 (column VI) shows that Fts are employed in 
different quantities: Wiener and the Maudes used 
two Fts each, Edmonds used four, the rest trans-
lators – three Fts each.  

2.4 TEM Results: the 13-Token Sentence  

The table below gives a cumulative picture of 
the results in each of the five frames considered:  
 
Table 1. Cumulative Overall Table (13 tokens): Rank Order 
I II III IV V VI 
TF MF SF TQ Et Ft 
(2.2.1) (2.2.2) (2.2.3) (2.2.4) (2.2.5) (2.2.6) 
(%) (%) (%) (pp) (count) (count) 
Leo Wiener (1899)   
100  100  100  100 0 2 
Aylmer & Louise Maude (1918)  

100  100  100  100 1 2 
David Magarschack (1961)   
100  92  100  97 0 3 

Rochelle S. Townsend (1912)  
100  92  100  97 4 3 
Constance Garnett (1901)   
76  92  92  86 0 3 
Joel Carmichael (1960)   

76  92  92  86 0 3 
Pevear & Volokhonsky (2000)  
76  92  92  86 0 3 

Rosemary Edmonds (1954)  
69  84  69  74 1 4 
 

As is seen, there are four groups of the TQ re-
sults. In the 100% group, Wiener has a slight 
advantage (in terms of isomorphism) over the 
Maudes, since he introduced no extra tokens. In 
the 97% group, Townsends’s translation inferior-
ity (in terms of closeness) is expressed in four 
extra tokens as compared to no extra tokens in 
Magarschack’s version. In the 86% block, no 
distinctions can be made because they are word-
for-word clones, except for Pevear-
Volokhonsky’s use of ‘each’ instead of ‘every’ 
(St 8). Edmonds’ version (TQ = 74%) has a rec-
ord (for this sample) number of formal tokens, 

four. It does not imply that the translation is bad 
– this kind of judgment can arise only after a 
discussion in classroom (3.3).  

The one-sentence example, used here peda-
gogically to explain the TEM techniques, cannot 
be considered to be fairly representative of the 
quantitative parameters and their qualitative im-
plications of translated texts. Therefore, we offer 
the results obtained for a much bigger sample 
from Anna Karenina.  

2.4.1 TEM Results: the 405-Token Excerpt  

Sheldon (1997) performs a detailed conventional 
comparative analysis of the four ‘focal points’ of 
the novel: the opening sentence considered 
above (13 tokens), the ball scene (73 tokens), the 
seduction scene (103 tokens) and the suicide 
scene (216 tokens). He analyzed the seven trans-
lations considered here, except for the version by 
Pevear and Volokhonsky, which was published 
three years later. Sheldon concluded that it was 
Carmichael who showed the best fit with the 
original.  

Here are the quantitative results obtained with 
the TEM applied to the same excerpts.  
 
Table 2. Cumulative Overall Table (405 tokes): Rank Order 
Lost Kept TQ Ft Et 
tokens tokens  used  used 
(count) (count) (%) (count) (count) 
David Magarshack (1961)   
9 396 97,7 96 14 
Joe Carmichael (1960)   
18 387 95,5 95 15 
Constance Garnett (1901)   
20 385 95,0 90 8 
Aylmer & Louise Maude (1918)   
30 375 92,5 91 17 
Rosemary Edmonds (1954)   
34 371 91,6 87 14 
Leo Wiener (1899)    
57 348 85,9 74 20 
Rochelle S. Townsend (1912)   
69 336 82,9 79 42 
 

As is seen, the TQs range from 97,7% to 
82,9%. Since the TEM does not cover all aspects 
of Sheldon’s analysis, it favors Magarshack’s 
version, with Carmichael’s translation lauded by 
Sheldon following it closely.  

2.5 Language pair independence 

In our example with translation from Russian to 
English, there is an asymmetry in that formal 
tokens are largely to be seen only on the target 
side. However, the TEM frames can equally be 
applied in the reverse direction or to any lan-
guage pair. Whether or not we choose to exclude 
some formal tokens from the counting, the 

37



frames are applied in the same way to all transla-
tions and their relative differences will be re-
vealed. 

2.6 Computational analysis  

It has been suggested before that virtual learning 
environments are useful for translation teaching 
(e.g., Fictumova (2007)). Our point here is that 
fine-grained quantitative methods, such as the 
TEM, can be put to use given support from com-
putational linguistic tools. The proposed envi-
ronment consists of a central server and a num-
ber of client systems for the students. Communi-
cation between them is handled as in any e-
learning environment, where exercises, grades 
and other course materials can be stored and ac-
cessed. The server includes several modules for 
monolingual text analysis, such as sentence 
segmentation, tokenization, lemmatization and 
PoS-tagging. A parser may also be included to 
support the computation of the syntax frame. 
More importantly, there are modules for sen-
tence and word alignments, since this is what is 
required to support the TEM analysis. In addi-
tion, there are modules for reviewing and cor-
recting outputs from all analyzers. 

 
2.6.1 Tokenization 
In principle, tokenization, numbering and label-
ing of tokens (2.1), are processes that computers 
can handle with ease. It is important, though, 
that the tokenization is done in a way that sup-
ports the purpose to which it will be used. In this 
case, a tokenization module that only looks at 
spaces and separators will not be optimal, as the 
primary unit of TEM is semantic, and may span 
several text words. Moreover, punctuation marks 
are not treated as separate tokens in the TEM. 
This problem could be overcome by tokenizing 
in two steps. In the first step punctuation marks 
are removed, lexical tokens are identified using 
word lists and then formatted as character strings 
that have no internal spaces. In the second stage 
spaces are used to identify and number the to-
kens. Formal tokens can to a large extent be 
identified as part of this process, using word 
lists, but extra tokens cannot be identified until 
after the word alignment. 

 
2.6.2 Sentence alignment 
In some cases the translation task may require 
students not to change sentence boundaries and a 
one-to-one correspondence between source sen-
tences and sentences of the translations can be 
assumed to hold when translations are delivered. 

If not, a sentence alignment tool such as 
hunalign (Varga et al., 2005) can be used. 

 
2.6.3 Word alignment 
The accuracy of word alignment systems are 
quite far from 100%. The best performing sys-
tems are either statistical, such as Giza++ (Och 
& Ney, 2003), or hybrid (Moore et al., 2006) and 
require vast amounts of text to perform well. In 
the translation class context, the source text will 
be fairly short, perhaps a few thousand words as 
a maximum. Even with, say, 20 student transla-
tions, the total bitext, consisting of the source 
text repeated once for each student translation 
and sentence-aligned with it, will be too short for 
a statistical aligner to work well. For this reason, 
a hybrid system that relies on a combination of 
bilingual resources and statistics for the word 
alignment seems to be the best choice (cf. 
Ahrenberg & Tarvi, 2013). 

An advantage of having a short source text is 
that the teacher can develop a dictionary for it in 
advance to be used by the word aligner. While a 
teacher cannot predict all possible translations 
that a student may come up with, this is a re-
source that can be re-used and extended over 
several semesters and student groups. 

 
Table 3. Alignment performance on an excerpt from 
Anna Karenina using different combinations of statisti-
cal alignment and lexical resources. 

 Prec  Recall F-score 

Giza++ 0.499 0.497 0.498 

Wordlist based 0.881 0.366 0.517 

Combination 0.657 0.610 0.633 

Comb + filters 0.820 0.508 0.628 

 
Table 3 shows some results for the Russian-

English 405-token excerpt discussed above with 
different combinations of Giza++-output and 
lexicon-based alignments. Standard tokenization 
was used except that punctuation marks were 
deleted. The source then consists of eight itera-
tions of the excerpt, altogether 3304 tokens1 and 
the target text consisting of eight different trans-
lations has 4205 tokens. The files were lemma-
tized before alignment. 

The bilingual resources used are a word list of 
English function words such as articles and pos-
sessives that are likely to have no formal coun-
terpart in the source and a bilingual word list 
created by looking up content words in Google 

                                                
1 Standard tokenization does not recognize multitoken 
units.  

38



Translate. Not all translations suggested by 
Google have been included. The mean number 
of translations per Russian lemma is 1.5. In the 
combinations priority has been given to the 
alignments proposed by the word lists as they are 
deemed to have a higher precision.2 So, the third 
row means that Giza++ alignments have been 
replaced by null links and lexical links induced 
by the word lists in all cases where there was a 
contradiction. The fourth row is the result of ap-
plying a set of filters based on word frequencies 
in the corpus and alignment topology to the pre-
vious combination. 

Obviously, if a complete alignment is called 
for it is clear that the output of the system must 
be reviewed and hand-aligned afterwards. There 
are several interactive word-alignment tools that 
can be used for this purpose (Tiedemann, 2011), 
but it will still be time-consuming. However, the 
burden can be shared between teacher and stu-
dents, and efforts may be focused on a part of 
the text only. 

2.7 Workflow 

After selection of a ST to be used for a transla-
tion exercise, the system will have it segmented 
into sentences, tokenized, and numbered. Then 
the teacher checks the outcome and corrects it if 
necessary. The files are then sent to the students. 
Within the suggested approach, the students are 
asked to use the client version of the system for 
translation and then upload their translations to 
their teacher by a set date before class, or to 
bring them to class on memory sticks.  

When a student translation is in place in the 
server system, it can be aligned and graded au-
tomatically. Of course, the significance of the 
grades depends on the accuracy of the alignment, 
but both the student and the teacher can contrib-
ute to the reviewing. For instance, the teacher 
might have marked some words and phrases as 
especially significant and the student can review 
the alignments for them in the system for his or 
her particular translation.  

3 In Class 

When translations and their alignments are in 
place in the server system, they can be used as 
                                                
2 The fact that precision is not 100% for wordlist ba-
sed alignment has two major causes. First, some con-
tent words appear two or three times in a sentence and 
the system does not manage to pick the right occur-
rence. Also, some common English prepositions get 
aligned when they shouldn’t. 

input to various visualization tools. This we see 
as a further advantage of our approach which 
will stimulate discussion and reflections among 
the students. Students’ translations can be dis-
played individually or collectively, on a sentence 
basis or a phrase basis. Using again the opening 
sentence of Anna Karenina as our example, the 
outcome for one sentence can look as in Figure 
1, where also some of the token frames de-
scribed above are automatically computed from 
the alignment.3 Within this format, the teacher is 
acting as a post-editing human agent who can 
combine both manners of assessment – comput-
er-assisted and manual. 

Since the method combines human and com-
puter resources, it might raise the effectiveness 
of translation class instruction manifold 
(Lengyel 2006: 286). The TEM also depersonal-
izes the problem of grading.  

Figure 1. Alignment screenshot for Segment 1 of 
Translation 1 (Joel Carmichael, 1960) with metrics. 

 

3.1 From Translation Quotients to 
Grades 

As has been demonstrated, the TEM allows one 
to get a certain ‘cline of fidelity’ from the most 
faithful translation to the freest version. Based 
on these relative assessments, one can convert 
the cumulative figures obtained on a number of 
quantitative parameters to grades. It should be 
remembered that although the analytical ad-

                                                
3 Alignments of the excerpts from Anna Karenina can 
be accessed at http://www.ida.liu.se/~lah/AnnaK/ 

39



vantage of the frames is that they are minimally 
subjective, the step from TQ to grades is neither 
context- nor value-free but depends heavily on 
the translation task.  
 
Table 4. From TQs to Grades 

 TQ  Rank Grade 

Magarshack 97,7 1 Excellent 

Carmichael 95,5 2 Good  

Garnett 95,0 3 Good 

The Maudes 92,5 4 Good -  

Edmonds 91,6 5 Good - 

Wiener 85,9 6 Satisfactory 

Townsend 82,9 7 Satisfactory - 

 

3.2 Gauging Quality  

The highlight of the approach is class team 
work, in the course of which students are ex-
pected to have a chance to insert meaningful cor-
rections into their translations and thus improve 
their ‘home’ grades by the end of class. Because 
the tokens are numbered, the teacher can easily 
bring any St, or a group of Sts, on the screen to-
gether with all the versions of its or their transla-
tions. 

 It is at this stage that the qualitative side of 
the TEM comes into play with the aim of im-
proving the final quantitative grade. Let us, for 
instance, consider the way a group of two tokens 
from the sentence-example has been rendered. 
As can be seen here in the manual (Table 5) and 
computer (Figure 2) versions, this pair of source 
tokens has been rendered in three different ways. 
In a computer-equipped class, the required 
changes can be immediately introduced into the 
translated texts under discussion. 

 
3.3 Final Grading 

As was mentioned in Section 1, the suggested 
procedure embraces the four basic types of trans-
lation evaluation. The method generates absolute 
score (overall estimates) based on relative scores 
in separate frames (Table 1).  

The first monitoring gives a quantitative esti-
mate of students’ homework. After class discus-
sion, which is supposed, like any post-editing, to 
change the home translations for the better, one 
more monitoring is carried out, using the same 
frames. If the system is made incremental, the 
final grade, which is an arithmetic mean of the 
home and class grades, can be registered auto-
matically. If, at the end of class, the final grades 

are exhibited on screen in their ranking order, it 
might be the best possible motivation for stu-
dents to work diligently both at home and in 
class.  

 
Table 5. Renderings of Source tokens 12-13 
 
LW:  (12)in (Ft)its (13)own way. 
CG: (12)in (Ft)its (13)own way. 
ALM:  (12)in (Ft)its (13)own way. 
JC:  (12)in (Ft)its (13)own way. 
DM:  (12)in (Ft)its (13)own way.  
RPLV:  (12)in (Ft)its (13)own way.  
RE:  (12)after (Ft)its (13)own fashion.  
RT: (12)in (Ft)its (13)own (Et)particular way.  
 
Figure 2. Renderings of Source tokens 12-13 
(computed alignments) 
 

 

 

4 Outside Class  

Within machine translation research, work has 
been going on for several years, and is still very 
active, for the search of metrics that assess the 
similarity of a system translation with human 
reference translations. Metrics, such as BLEU 
(Papineni et al.. 2002), TER (Snover et al.. 
2006), and Meteor (Lavie and  Denkowski: 
2009), could also be included in the proposed 
environment. Published translations or transla-
tions that the teacher recognizes as particularly 
good can be used as reference translations. How-
ever, the scores of these metrics do not give as 
much qualitative information as the TEM 
frames.  

The role of corpora in translation and transla-
tion training is a topic of some interest (e.g. 
Zanettin et al.: 2003). In translator training, the 
corpora are mostly seen as resources for the stu-
dent to use when practicing translation (Lopez-

40



Rodriguez and Tercedor-Sanchez: 2008). This is 
orthogonal to what we are proposing here, i.e., 
enabling immediate comparisons and assess-
ments of students’ translations as a class-based 
activity. A system with a similar purpose is re-
ported in Shei and Pain (2002: 323) who de-
scribe it as an “intelligent tutoring system de-
signed to help student translators learn to appre-
ciate the distinction between literal and liberal 
translation”. Their system allows students to 
compare their own translations with reference 
translations and have them classified in terms of 
categories such as literal, semantic, and commu-
nicative. The comparisons are made one sen-
tence at a time, using the Dice coefficient, i.e., 
by treating the sentences as bags of words. Our 
proposal, in contrast, uses more advanced com-
putational linguistics tools and provides text lev-
el assessment based on word alignment. 

Michaud and McCoy (2013) describe a sys-
tem and a study where the goal, as in our pro-
posal, is to develop automatic support for trans-
lator training. They focus on the inverted TERp 
metric (Snover et al., 2009) for evaluation of 
student translations. TERp requires a reference 
translation but can represent the difference be-
tween a given translation and the reference in 
terms of editing operations such as insertion, 
deletion, change of word order and matches of 
different kinds. A weak positive correlation with 
instructor-based grades (using Pearson’s r) could 
be demonstrated in the study and the authors ar-
gue that TERp is sufficiently reliable to provide 
feed-back to students in a tutoring environment.  

The main difference between their proposal 
and ours is that we start with a metric that has 
been developed for the task of grading human 
translations, while TERp is originally an MT 
metric. Thus, TEM does not require reference 
translations, but on the other hand its computa-
tion has not been automated and so, that is where 
our current efforts are focused.  It should be em-
phasized that the teacher’s load within this ap-
proach remains quite heavy but the reviewing 
work may be shared between teachers and stu-
dents.  

Both the TEM and TERp provide quantitative 
measurements that can lay the foundation for 
qualitative discussions and feedback to students 
but as the TEM does not require a reference it 
gives the students more freedom in improving 
their work.  

As a more or less objective way of measuring 
the quantity with allowances made for quality, 
the method can also be used by teachers at ex-

ams, by editors for choosing a translation, by 
managers recruiting new in-house translators, by 
translators for self-monitoring, etc. The comput-
er-generated figures are obtained right on the 
spot – they may not be exactly accurate but they 
give a rough general picture at the level of con-
tent-form ‘presence’ of the original in its transla-
tions. 

Acknowledgement 

We are grateful to the anonymous reviewers who 
provided useful comments and additional refer-
ences. 
 
References 
Ahrenberg, Lars and Tarvi, Ljuba. 2013. Natural lan-

guage processing for the translation class. Proceed-
ings of the second workshop on NLP for comput-
er-assisted language learning at NODALIDA 2013 
(May 22, Oslo). NEALT Proceedings Series 17 / 
Linköping Electronic Conference Proceedings 86: 
1–10.  

Carmichael, Joel. 1960. Anna Karenina, by Lev Tol-
stoy. New York: Bantam Books, 1980.  

Edmonds, Rosemary. 1954. Anna Karenina, by Lev 
Tolstoy. London: The Folio Society, 1975.  

Garnett, Constance. 1901. Anna Karenina, by Leo 
Tolstoy, with revisions by Leonard J. Kent and Ni-
na Berberova. New York: Modern Library, 1993.  

Honkela, Timo et al. 2010. GIGA: Grounded 
Intersubjective Concept Analysis: A Method for 
Enhancing Mutual Understanding and Participa-
tion, Espoo: Aalto University School of Science 
and Technology.  

Lavie, Alon and Denkowski, Michael J. 2009. ‘The 
Meteor metric for automatic evaluation of machine 
translation,’ Machine Translation, Vol 23 (2-3) 
105-115. 

Lengyel, István. 2006. ‘Book reviews. Ljuba Tarvi: 
Comparative Translation Assessment: Quantifying 
Quality,’ Across Languages and Cultures 7 (2) 
2006, 284-286.  

Liang, Percy, Taskar, Ben, and Klein, Dan. 2006. 
‘Alignment by Agreement.’ In Proceedings of the 
Human Language Technology Conference of the 
North American Chapter of the Association for 
Computational Linguistics, 2006, 104-111. 

López-Rodríguez, Clara Inés and Tercedor-Sánchez, 
María Isabel. 2008. ‘Corpora and Students' Auton-
omy in Scientific and Technical Translation train-
ing,’ Journal of Specialized Translation 
(JoSTrans), Issue 09 (2008), 2-19. 

41



Magarschack, David. 1961. Anna Karenina, by Lev 
Tolstoy. New York: The New American Library.  

Maude, Louise and Maude, Aylmer. 1918. Anna 
Karenina, by Lev Tolstoy, a Norton Critical Edi-
tion, ed. George Gabian, with the Maude transla-
tion as revised by George Gibian. 2d edition. New 
York: Norton and Co, 1995.  

Michaud, Lisa N. and McCoy, Patricia Ann. 2013. 
Applying Machine Translation Metrics to Student-
Written Translations. Proceedings of the Eighth 
Workshop on Innovative Use of NLP for Building 
Educational Applications (BEA8), 2013, 306-311. 

Moore, Robert C, Yih, Wen-tau, and Bode, Anders. 
2006. ’Improved Discriminative Bilingual Word 
Alignment.’ In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics 
and 44th Annual Meeting of the ACL, 2006, 513-
520. 

Papineni, Kishore, Roukos, Salim, Ward, Todd, and 
Zhu, Wei-Jing. 2002. ‘BLEU: a method for auto-
matic evaluation of machine translation.’ In Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, 311-318. 

Pevear, Richard & Volokhonsky, Larissa. 2000. Leo 
Tolstoy. Anna Karenina. Penguin Books.  

Shei, Chi-Chiang and Pain, Helen. 2002. ‘Computer-
Assisted Teaching of Translation Methods.’ Liter-
ary & Linguistic Computing, Vol, 17, No 3 (2002), 
323-343. 

Sheldon, Richard. 1997. ‘Problems in the English 
Translation of Anna Karenina.’ Essays in the Art 
and Theory of Translation, Lewiston-Queenston-
Lampeter: The Edwin Mellen Press, 1997 

Snover, Matthew, Dorr, Bonnie, Schwartz, Richard, 
Micciulla, Linnea and Makhoul, John. 2006. ‘A 
Study of Translation Edit Rate with Targeted Hu-
man Annotation.’ Proceedings of the 7th Confer-
ence of the Association for Machine Translation in 
the Americas, 223-231. 

Snover, Matthew, Madnani, Nitin, Dorr, Bonnie J., 
and Schwartz, Richard. 2009. Fluency, adequacy, 
or HTER? Exploring different judgments with a 
tunable MT metric. Proceedings of the EACL 
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March 30-31, 2009: 259-268. 

Tarvi, Ljuba. 2004. Comparative Translation As-
sessment: Quantifying Quality, Helsinki: Helsinki 
University Press.  

Tiedemann. 2011. Bitext Alignment. Morgan & Clay-
pool Publishers. 

Townsend, Rochelle S. 1912. Anna Karenina, by 
Count Leo Tolstoi. London & Toronto: J.M. Dent 
& Sons; New york: E.P. Dutton and Co, 1928.  

Varga, Daniel, Németh, Laszlo, Halácsy, Peter, 
Kornai, Andras, Trón, Viktor, and Viktor Nagy, 
2005. Parallel corpora for medium density lan-
guages. Proceedings of RANLP 2005. 

Vinay, Jean-Paul & Darbelnet, Jean. 1995 [1958]. 
Comparative Stylistics of French and English. A 
Methodology for Translation, Amsterdam: John 
Benjamins.  

Wiener, Leo. 1899. Anna Karenina, by Lyof N. 
Tolstoi, vols II-IV: The Novels and Other Works of 
Lyof N. Tolstoi. New York: Charles Scribner’s 
Sons, 1904.  

Zanettin, Federico, Bernardini, Silvia, and Stewart, 
Dominic (eds.). 2003. Corpora in Translator Edu-
cation, Manchester. 

 

42


