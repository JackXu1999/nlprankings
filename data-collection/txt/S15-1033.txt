



















































Dependency-Based Semantic Role Labeling using Convolutional Neural Networks


Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 279–288,
Denver, Colorado, June 4–5, 2015.

Dependency-Based Semantic Role Labeling using
Convolutional Neural Networks

William R. Foland Jr.
OKRobotGo, Ltd.

5345 Dunraven Circle
Golden, Co, 80403, USA

bill.foland@okrobotgo.com

James H. Martin
Department of Computer Science and

Institute of Cognitive Science
University of Colorado

Boulder, CO 80309
James.Martin@colorado.edu

Abstract

We describe a semantic role labeler with state-
of-the-art performance and low computational
requirements, which uses convolutional and
time-domain neural networks. The system is
designed to work with features derived from
a dependency parser output. Various system
options and architectural details are discussed.
Incremental experiments were run to explore
the benefits of adding increasingly more com-
plex dependency-based features to the system;
results are presented for both in-domain and
out-of-domain datasets.

1 Introduction

Semantic role labeling (Gildea and Jurafsky [2002]),
the task of identifying and classifying the semantic
arguments of verbal and nominal predicates in text,
represents one of the most complex NLP tasks to
be addressed by supervised machine learning tech-
niques. In the standard supervised approach to
building SRL systems, collections of multiway clas-
sifiers are trained using annotated corpora such as
PropBank (Palmer et al. [2005]). In this approach,
classifiers are trained using features derived directly
from the original source text, as well as from subse-
quent syntactic and semantic processing.

As reported in several shared tasks (Carreras and
Màrquez [2004],Carreras and Màrquez [2005],Hajič
et al. [2009]), SRL systems trained in this manner
can achieve high performance. State-of-the-art sys-
tems employ classifiers such as support vector ma-
chines trained with large numbers of relatively com-
plex combinations of features, often combined with

re-ranking based on multiple syntactic analyses. Un-
fortunately, these approaches have a number of non-
trivial limitations including the computational cost
of the syntactic parsing and the sparse nature of the
complex features on which they rely. This latter lim-
itation is particularly critical since it leads to signif-
icant degradation in performance when the trained
system is applied to texts from new domains.

However, recent results using multilayer neu-
ral networks and pre-trained word embeddings
have demonstrated high performance using a much
smaller number of minimalist features. The archi-
tecture described by Collobert et al. [2011] com-
bines time delay convolutional neural networks
(Waibel et al. [1989]) and pre-trained word embed-
dings for a number of NLP tasks. They develop four
components and compare their performance to pre-
vious benchmarks, one of which is an SRL system
which uses features derived from a phrase-structure
parse as input, based on the CoNLL 2005 shared
task (Carreras and Màrquez [2005]).

The work described here adopts the basic archi-
tecture from Collobert et al. [2011] and explores
issues related to the use of this architecture in the
context of the CoNLL 2009 shared task. In partic-
ular, we present Daisy, a system that (1) employs
features derived from dependency parse as input,
(2) assigns semantic roles to both verbal and nom-
inal predicates, and (3) automatically assigns word
senses to the predicates as described in the CoNLL
2009 shared task (Hajič et al. [2009]).

The following sections will describe the architec-
ture of the Daisy system, present state-of-the-art per-
formance on the CoNLL 2009 shared task, and ex-

279



plore the utility of features derived from dependency
parses, including a version of the traditional SRL
syntactic path feature.

2 Experimental Setup

The CoNLL 2009 shared task consists of identify-
ing the sense and semantic arguments for each given
argument-bearing token (predicate). In addition to
the words themselves, the training data provides the
part of speech, syntactic head, and syntactic depen-
dency relation to the head for each word in the sen-
tence. Table 1 shows an example sentence and its
representation in the dataset. The PDEPREL and
PHEAD features are the head word and dependency
relation predicted automatically by a dependency
parser. In the example sentence, there are two pred-
icates identified for labeling: announce, and close.
The system should output two arguments for an-
nounce: results:A1 (Object), and after:AM-TEMP
(Temporal Marker). Similarly, market:A1 should be
output for the predicate close. In addition to role
identification, the word sense for each predicate is
output, in the example, the expected sense for an-
nounce is 01, and for close is 02.

The training, validation, and evaluation datasets
are annotated sentences from the Wall Street Jour-
nal. An additional out of domain dataset mostly
from the Brown corpus was also supplied. A com-
prehensive F1 score was generated for both role
labels and sense predictions using the provided
eval09.pl perl script.

3 Semantic Role Labeling System

The general block diagrams for the Daisy SRL sys-
tem are shown in Figure 1. The input to the system
is a list of words wi from w1 to wn, a list of pred-
icate positions, and dependency parse tree informa-
tion for the sentence. We treat role labeling and the
sense identification as two separate tasks. For each
predicate in a given sentence, the Role Subsystem
outputs the list of predicted role tags for all words
(SRLi), and the Sense Subsystem outputs the sense
tag of the predicate. The system is composed of five
major components:

• Word Preprocessing and Word Derived Feature
Convolution (Figure 2).
• Predicate Position Feature Convolution.

Feature Vector Convolution

Word 
Derived

Role Neural Network and
Viterbi Decoder

Predicate Position
and Path

Word 
Position

⌃

Sentence Words
wi

Dependency Tree

Word POS

Sentence SRL Tags

SRLi

A B C

D

(a) Role Subsystem

Feature Vector Convolution

Word 
Derived

Sense Neural Network

Predicate Position
and Path

⌃

Sentence Words
wi

Dependency Tree

Word POS

Predicate Senses

BA

E

Predicate Lemma

(b) Sense Subsystem

Figure 1: SRL Role and Sense Block Diagrams

• Word Position Feature Convolution.
• Neural Network and Viterbi (Figure 4).
• Predicate Sense Neural Network (Figure 5).

3.1 Word Derived Feature Convolution Section

The Word Derived Features and Convolution sec-
tion, shown in Figure 2, is sourced by five features
which are derived on a word by word basis.

The upper portion of Figure 2 depicts the process
of looking up features from the words and parse tree
information. The numeric information from the fea-
tures for each word is concatenated together to form
one long feature vector, shown in the diagram as a
multi-shaded set of rectangles. Three words of fea-
ture information (the word and its two neighbors)
from the Word Derived Feature Vector are multi-
plied by the the weights and bias of Θ4 and stored
in the Convolved Word Derived Feature Vector, for
each word in the sentence. For the default convo-
lution width of 300, this results in a long vector of
300 · n, where n is the number of words in the sen-
tence.

Each feature lookup table contains an entry for
PADDING. In order to allow the window to extend
beyond boundaries of the sentence for early and
late words the Feature Vector is padded with the
PADDING value from each lookup table. If a fea-

280



ID FORM LEMMA PLEMMA POS PPOS FEAT PFEAT HEAD PHEAD DEPREL PDEPREL FILLPRED PRED A[announce] A[close]

1 The the the DT DT 2 2 NMOD NMOD
2 results result result NNS NNS 3 3 SBJ SBJ A1
3 were be be VBD VBD 0 0 ROOT ROOT
4 announced announce announce VBN VBN 3 3 VC VC Y announce.01
5 after after after IN IN 4 4 TMP TMP AM-TMP
6 the the the DT DT 8 8 NMOD NMOD
7 stock stock stock NN NN 8 8 NMOD NMOD
8 market market market NN NN 9 9 SBJ SBJ A1
9 closed close close VBD VBD 5 5 SUB SUB Y close.02
10 . . . . . 3 3 P P

Table 1: CoNLL format SRL Dependency Parse Input Test Sentence Example

300

300 * n Convolved word feature vector

⇥4 = {W4, b4}

130K

5

450

Sentence
Words

⇥words

⇥caps

⇥dep

wi

...w1 w2 wnpad pad

5

5

50

ic

is } per wordiw
5

47
ih

Dependency
Tree

Word POS

195 = 3 * (50+5+5+5) 

⌃

300 * n

5
47

ih
⇥POS

⇥POSh

word pre-
processing
and index 
calculation

Figure 2: Word Preprocessing, Word Derived Features,
and Word Derived Feature Convolution. A in figures
1(a) and 1(b).

ture is in the table, the associated vector is output,
otherwise the vector corresponding to the special to-
ken UNKNOWN is output. The PADDING and UN-
KNOWN vectors are trained during supervised train-
ing.

To train the word representations from scratch, all
except the 0.63% least common unique words from
the training set are added to the lookup table. The
remaining words are therefore trained as the UN-
KNOWN word, which can then be used to represent
any word encountered outside the trained word list.
For other features, the representation for the most
probable token is used as the UNKNOWN represen-

tation.
The five types of word-derived features tested for

the SRL Dependency Parse tagger are:

• Word Embeddings
• Capitalization
• POS tag of word
• Dependency Relation
• POS tag of head

3.1.1 Word Pre-processing
The input data provided for the CoNLL 2009

task has already gone through some initial tokeniz-
ing. This prevents tokenization differences of differ-
ent systems from influencing the results, which are
meant to allow comparison of the SRL tagging ar-
chitecture itself. The Daisy pre-processor does not
split hyphenated input words, so each input word
will result in a single pre-processed word. Numeric
values are collapsed to the single common 0 token,
and words are lower-cased to create a word repre-
sentation lookup word.

3.1.2 Word Embeddings
Words are transformed to numeric representations

using a lookup table. Like all other feature lookup
tables in the system, the word representation vec-
tors can be initialized to small random values to start
with, and then trained using the supervised training
algorithm.

A method of training the word representations
from untagged databases has been very successfully
applied to create a starting set of vectors that can
be used to initialize a network, which is then fine-
tuned with supervised training to execute a specific
task. By ”pre-training” these word representations
using large amounts of untagged text, very infor-
mative word relationships can be inexpensively ex-
tracted, and later used as the starting point for task

281



specific application learning, see for example Hin-
ton et al. [2006], Bengio et al. [2007] and Weston
et al. [2012].

The word representations, or embeddings, used as
input to the Daisy SRL System for the experiments
described here were generated by Collobert et al.
[2011] and were created using a pairwise ranking
approach (Schapire and Singer [1998]).

3.1.3 Capitalization
Prior to lower casing, each word is checked for all

capitals, initial capital, any capital, or no capitals,
and this criteria is used to lookup a vector (default
length 5) from the caps table.

3.1.4 Predicted Dependency Relation
The PDEPREL column from the training data,

shown in table 1.

3.1.5 Predicted POS tag of word and of head
The Predicted Part-of-speech tag is provided in

PPOS column of the training data. The head part
of speech tag is found by following the PHEAD col-
umn and extracting the PPOS column. (see Table
1).

3.2 Predicate Position and Path Feature
Convolution Section

Predicate Position and optional Path features are ex-
tracted on a per word basis and convolved, once per
predicate (the outer loop of two).

3.2.1 Predicate Position Feature
The position of each word relative to the predicate

being evaluated is represented by 25 vectors, based
on distances of -12 to +12, and distances outside this
range are saturated.

3.2.2 Dependency Path Feature
Information about the path from each word to a

given predicate is maintained in a lookup table and is
provided in the Predicate Position Convolution sec-
tion as a per word feature.

Generic Path: The sequence of up and down
transitions to traverse the tree from a word to a given
predicate is referred to here as the Generic Path. The
dependency parse trees for each of the two predi-
cates from the example training sentence shown in
Table 1 are diagrammed in Figure 3. The Generic

uud ud d o u uuuu uuuu uuu uu ud
DT NNS VBD VBN IN DT NN NN VBD
The results were announced after the stock market closed

NMOD SBJ

root

VC TMP
NMOD

NMOD SBJ

SUB

A1

AM-TMP

announce.01

uuddd uddd ddd dd d uu uu u o uddd
DT NNS VBD VBN IN DT NN NN VBD
The results were announced after the stock market closed

NMOD SBJ

root

VC TMP
NMOD

NMOD SBJ

SUB

A1

close.02

Figure 3: Dependency Parse and Generic Paths

Path for each word is shown in the diagram, above
the part of speech tag for the word.

Labeled Path: These are path descriptions which
include both the arc direction (Generic Path) and
the dependency relation of the arc within the depen-
dency tree. After several rounds of experimentation,
we chose to include paths which occur at least five
times in the training data, which resulted in about
77K unique path types.

3.3 Word Position Feature Convolution Section

The position of every word with respect to the spe-
cific word being evaluated is extracted once per
word, per predicate (the inner loop of two). In a sim-
ilar fashion to the predicate position feature, the po-
sition of each word relative to the word being evalu-
ated is represented by 25 vectors, based on distances
of -12 to +12, and distances outside this range are
saturated.

3.4 Role Neural Network and Viterbi

Figure 4 shows the process of combining the Con-
volved Feature Vectors, processing with a neural
network, and finding the most likely role sequence
with a Viterbi detector. Both the Role and Sense
neural networks are constructed with a single non-
linear layer followed by an output layer. The param-
eters for each layer are referred to here as Θ, which
includes a matrix of weights,W , and a vector of bias
terms b. Each layer’s output, prior to the activation
function, can be calculated from the previous layer’s
activation output and parameters.

282



f lΘ = W
l−1f l−1Θ + b

l−1 (1)

The tanh function is used as the nonlinear activa-
tion function.

SRL 
Tag Scores

n (number of words) Sentence
SRL Tags
SRLi

Sequence Detector 
(Viterbi)
⇥initial

⇥transition

500

186
SRL Tag Scores

tanh

⇥out

(and �output)

300

Max

300 * n

per word, per verb

⌃

⇥1 = {W1, b1}

Sum of Convolved Features For:
• Word Derived
• Predicate Position
• Word Position

Figure 4: SRL Neural Network and Viterbi. D in figure
1(a).

The three Convolved Feature Vectors (dia-
grammed separately) are summed, then the maxi-
mum for each index within each group of 300 is
determined. This results in a 300 element vector
which will be the input to the Neural Network. A
single layer neural network followed by a single out-
put layer is used to create a ”score” for each possi-
ble role ”tag”, for the word and predicate being an-
alyzed. After running all words through the system
for a single predicate, a matrix of SRL roles scores
of size tags× words is created, which will be used
as the input to the Viterbi sequence decoder.

3.5 Sequence Decoder (Viterbi)
The Viterbi decoding algorithm input is a matrix
which consists of a vector of SRL role scores for
each word. The algorithm is initialized with a
learned set of weights per tag, and computes the log-
likelihood of transitioning from each state to the next
by applying a learned set of weights from the transi-
tion matrix.

3.6 Predicate Sense Neural Network
Figure 5 shows the process of combining the Con-
volved Feature Vectors, processing with a neural

network, and finding the most likely sense for a
given predicate. The neural network parameters for
the sense subsystem are managed with a lookup ta-
ble holding parameters for each lemma in the train-
ing set that is mapped to multiple senses.

300

20
Predicate Sense

tanh

300

Max

300 * n

⌃

⇥sns = {Wsns, bsns}

⇥sns out

Sum of Convolved Features For:
• Word Derived
• Predicate Position

(one set per Lemma)

Figure 5: SRL Neural Network for Predicate Sense. E
in figure1(b).

4 Sense Labeler Training and Forward
Model Creation

Both the Role and Sense subsystems are trained us-
ing stochastic gradient descent. A forward pass is
first run on the system, during which the indices of
the maximum values of the sum of the convolutions
layers (word-derived and predicate) are saved.

Back-propagation of the Sense Neural Network is
based on minimizing a log-likelihood objective:

log p(y|x,Θ) = f [x,Θ]y − log(
∑

j

e(f [x,Θ]j)) (2)

The two Sense and Role subsystems have the
same convolution structures (See figures 1(a) and
1(b)). Experiments run using a common structure
for both tasks resulted in about 0.5% worse perfor-
mance, so the the systems were kept independent.

A separate neural network was trained for each
lemma found in the training data set, and the param-
eters for each network were stored in a lookup table.
This results in very large memory requirements dur-
ing training, especially since Adagrad (Duchi et al.

283



[2011] was used to decrease training time. To min-
imize memory requirements and training time, the
sense for lemmas which always train to the same
sense in the training data are stored in a dictionary.
During forward processing, when a lemma is en-
countered that was not trained (and therefore is not
in the parameter lookup table), the sense from the
dictionary is output. If the lemma never occurred
during training, it won’t be in the dictionary, and the
most commonly occurring sense of ”01” is output by
default.

5 Role Labeler Training and Forward
Model Creation

During a forward pass, the activation layers and
maxIndices are saved and reused during training.

5.1 Cost Calculation
The Viterbi parameters for initial score and tran-
sition probabilities are trained using the Sentence
Level Log-Likelihood (SLL) cost function.

This cost function is based on Sentence Level
Likelihood and is similar to equation 2, except the
reference path score must be normalized by using
the sum of the exponential of all path scores (the
sum of unnormalized probabilities for all possible
paths, instead of for all possible tags). A recursive
method, developed in Rabiner [1989] and specified
in Collobert et al. [2011], provides an important and
efficient means of computing the sum of the expo-
nential of all path scores. An intermediate vector, δ,
is calculated, which will contain the unnormalized
log probability that any path through the trellis will
pass through a particular state k for the particular
word t. The δ vectors have a dimension of N, the
number of tags, and they are re-used for the gradient
calculation during back-propagation.

5.2 Back-propagation
The recursion described in Collobert et al. [2011] is
used to calculate Viterbi delta terms and gradients.
The error is then back-propagated through the sys-
tem in reverse, ending with the feature lookup ta-
bles. This is done for each word, for each predi-
cate, requiring two nested loops for training a full
sentence. The loop structure makes for long train-
ing times, roughly three days on a 2015 compute-
optimized AWS core.

6 Results

6.1 Benchmark
The best ConLL 2009, English, SRL F1 score,
is labeled Nugues, and the system is described in
Björkelund et al. [2009]. To the best of our knowl-
edge, the current state of the art for this dataset is
represented by these results, and we therefore use
them as a benchmark (See section 7). To generate
these benchmark results, 20 features were used for
argument identification, including the Dependency
Relation Path, and Part of Speech of Dependency
Relation Path. A reranker was then run on the out-
put of multiple system outputs.

Table 2 compares the benchmark with a complete
Daisy system using a labeled path, with a cutoff of
5, and two separate systems for sense and role la-
bels. F1 scores are 0.41% higher for the WSJ Eval
dataset, and 2.59% higher for the out of domain
(OOD) Brown dataset.

System Description WSJ F1 Brown F1

Benchmark
(CoNLL2009)

85.63% 73.31%

Daisy 86.04% 75.90%

Table 2: SRL Dependency Parse Test F1

6.2 Metrics
In all experiments, we strictly followed the standard
evaluation procedure of the CoNLL 2009 challenge.
A simple validation procedure using the specified
validation set was used to choose system hyper pa-
rameters, and the provided eval09.pl perl script was
used to generate all system F1 scores. The system
F1 score is the harmonic mean of precision and re-
call for both role and sense labels. Since we treated
the predicate sense disambiguation and the predicate
role assignment tasks as independent, it is interest-
ing to view the performance of the two tasks sep-
arately. The predicate sense task requires a label
for each given predicate, so a per predicate accuracy
was calculated (SenseAcc). Similarly we generated
a role label F1 score (RoleF1) that is independent of
the sense labels. These subsystem performance met-
rics were also calculated on the CoNLL 2009 bench-
mark results for comparison.

284



6.3 Incremental Experiments and Results
Feature abbreviations used in the descriptions are
shown in Table 3.

Abbrev. Feature Description

W words, initialized randomly prior to
training

C capitalization
P Part of Speech
HP Part of Speech of head word
DR Dependency Relation
GP Generic path
TW words, initialized with pre-trained

word embeddings prior to training
LP5 Labeled paths that occur at least five

times in the training data.

Table 3: Feature Abbreviations

81.0% 82.0% 83.0% 84.0% 85.0% 86.0%

Development systemF1

81.0%

82.0%

83.0%

84.0%

85.0%

86.0%

87.0%

W
S
J 
E
v
a
l 
sy

st
e
m

F1

 (85.63%)

WSJ (Eval) Incremental Feature Progression

Daisy(W,C)

Daisy(W,C,P)

Daisy(W,C,P,HP,DR)

Daisy(W,C,P,HP,DR,GP)

Daisy(TW,C,P,HP,DR,GP)

Daisy(TW,C,P,HP,DR,LP5)

Figure 6: Scatter Plot of Dev F1 vs. Eval F1 for Various
Feature Configurations (See also Table 4)

Incremental experiments were run to explore
the benefits of adding increasingly more complex
dependency-based features to the system.

We began with a basic configuration of only
words (randomly initialized) and capitalization
(W,C), Following this, a simple per-token part of
speech was added (W,C,P). Information from the
dependency parser is then added in two steps, first
the head word part of speech and dependency re-
lation (W,C,P,HP,DR), and next the generic path
(W,C,P,HP,DR,GP). The word representations were
then seeded with the pre-trained embeddings de-

81.0% 82.0% 83.0% 84.0% 85.0% 86.0%

Development systemF1

69.0%

70.0%

71.0%

72.0%

73.0%

74.0%

75.0%

76.0%

77.0%

78.0%

O
u
t 

o
f 

D
o
m

a
in

 s
y
st

e
m

F1

 (73.31%)

OOD (Brown) Incremental Feature Progression

Daisy(W,C)

Daisy(W,C,P)

Daisy(W,C,P,HP,DR)

Daisy(W,C,P,HP,DR,GP)

Daisy(TW,C,P,HP,DR,GP)

Daisy(TW,C,P,HP,DR,LP5)

Figure 7: Dev F1 vs. Brown (OOD) F1 for Various Fea-
ture Configurations (See also Table 5)

scribed in section 3.1.2 (TW,C,P,HP,DR,GP). Fi-
nally, the labeled path was used instead of the
generic path, still seeding the words with pre-trained
embeddings (TW,C,P,HP,DR,LP5).

For each system configuration, 12 role subsys-
tems and 8 sense subsystems were trained and
tested, using the WSJ development F1 score dur-
ing training to determine the best model parameter
state. After model generation, the WSJ development
scores for different systems don’t correlate well with
the WSJ eval or Brown scores. For example, mod-
els with high development scores don’t necessarily
correspond to best scoring models for the WSJ or
Brown data tests.

The CoNLL2009 results used as benchmarks
were given as single data points so statistics are not
available.

Figure 7 shows the relationship between the de-
velopment and Evaluation F1 scores, as well as the
general performance improvement as features were
added.

Tables 4 and 5 show the statistical performance of
the system with WSJ and Brown test data.

For the WSJ (evaluation) dataset, the role subsys-
tem F1 improves much more dramatically than the
sense subsystem as POS (+1.52%) and dependency
parser information (+1.68%) is added. The mean
System F1 score is -0.25% under the benchmark
without the pre-trained word embeddings. Adding
the embeddings boosts performance such that even
the lowest scoring systems beat the benchmark, and
the mean F1 score is about 0.41% higher.

285



System Description SystemF1 RoleF1 SenseAcc
Min Mean (∆) Max Mean (∆) Mean (∆)

Daisy(W,C) 82.86 83.03 83.24 77.47 94.92
Daisy(W,C,P) 83.83 84.12 (+1.09) 84.43 79.00 (+1.52) 95.15 (+0.23)

Daisy(W,C,P,HP,DR) 84.46 84.79 (+0.67) 85.10 79.92 (+0.92) 95.29 (+0.13)

Daisy(W,C,P,HP,DR,GP) 85.05 85.38 (+0.58) 85.78 80.69 (+0.76) 95.46 (+0.17)

Benchmark (CoNLL2009) 85.63
(+0.25)

81.00
(+0.31)

95.59
(+0.13)

Daisy(TW,C,P,HP,DR,GP) 85.64 85.92 (+0.29) 86.17 81.40 (+0.40) 95.66 (+0.07)

Daisy(TW,C,P,HP,DR,LP5) 85.77 86.04 (+0.13) 86.31 81.53 (+0.13) 95.77 (+0.11)

Table 4: Performance on WSJ Eval Dataset for Various System Configurations

System Description SystemF1 RoleF1 SenseAcc
Min Mean (∆) Max Mean (∆) Mean (∆)

Daisy(W,C) 70.50 71.70 72.43 65.49 85.08
Daisy(W,C,P) 72.45 73.13 (+1.43) 73.78 67.38 (+1.89) 85.66 (+0.59)

Benchmark (CoNLL2009) 73.31
(+0.18)

67.78
(+0.40)

85.23
(-0.43)

Daisy(W,C,P,HP,DR) 72.47 73.48 (+0.17) 74.43 67.87 (+0.09) 85.71 (+0.48)

Daisy(W,C,P,HP,DR,GP) 73.17 73.83 (+0.36) 74.23 68.21 (+0.34) 86.04 (+0.33)

Daisy(TW,C,P,HP,DR,GP) 74.85 75.80 (+1.97) 76.46 70.80 (+2.59) 86.72 (+0.68)

Daisy(TW,C,P,HP,DR,LP5) 75.19 75.90 (+0.09) 76.93 70.62 (-0.18) 87.40 (+0.69)

Table 5: Performance on Brown Dataset (OOD) for Various System Configurations

For the Brown (OOD) dataset, the role subsys-
tem F1 improves significantly with POS and depen-
dency parse information (+2.72%) while the sense
subsystem benefits less (0.96%). The role subsys-
tem dramatically improves when pre-trained words
are added (2.59%), due in large part to a better abil-
ity to handle unseen words. The mean System F1
scores are higher than the benchmark as soon as de-
pendency parser information is supplied, and the F1
is significantly better for the fully populated system
(+2.59%).

7 Related Work

The same Semantic Role Labeling system used to
generate the results used as our benchmark was
later tested using improved dependency parsing in
Björkelund et al. [2010]. Woodsend and Lapata
[2014] explore text rewriting and compare results
with the benchmark, which they accept as the cur-
rent state-of-the-art.

Kanerva and Ginter [2014] use the CoNLL 2009
data as a benchmark for investigating the use of Fin-

ish and English word vector relationships, and the
relationships of word vectors as they relate to se-
mantic roles.

In Socher et al. [2013], the authors present a Re-
cursive Neural Tensor Network (RNTN) which uses
word vectors as a primary input and which is used to
recursively generate a phrase tree structure for each
sentence. The resulting structures are then further
used to generate fine-grained sentiment analysis es-
timates.

Convolutional neural networks which include
character level structures have been effectively used
for sentiment analysis by dos Santos and Gatti
[2014]. The characters are not pre-trained, and syn-
tactic trees are not used as input to the network.

In Luong et al. [2013], words are broken down
into morphemes as the input to a recursive neural
network to capture morphological compositionality
with the goal of improving the vector representa-
tions of scarce words.

The characteristics and semantic expressive
power of various word embedding collections are in-

286



vestigated by Mikolov et al. [2013] and Chen et al.
[2013].

8 Conclusion and Future Work

We have presented a dependency-based semantic
role labeler using neural networks, inspired by Col-
lobert et al. [2011] and others to reduce the use
of hand-crafted features and make use of unsuper-
vised techniques. Experimental evaluations show
that our architecture improves the state of the art
performance for this task significantly, for both in
domain and out of domain test data. A key element
of the system’s performance is based on the use of
features derived from syntactic dependency parses.
The use of a dependency-based path feature, in par-
ticular, provides a significant boost in performance
over simpler feature sets.

Promising future directions suggested by these re-
sults include whether proxies for the dependency-
based features can be derived from a similar archi-
tecture without the direct need for a full dependency
analysis, thus eliminating the pre-processing parser
cost. Another future direction involves the pred-
icate disambiguation system. Although this sense
disambiguation task is part of the CoNLL 2009 SRL
evaluation, it is more properly a word sense disam-
biguation problem. A more thorough investigation
of sense disambiguation in the context of an SRL
system is warranted.

References

Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. Greedy layer-wise training of
deep networks. Advances in neural information
processing systems, 19:153, 2007.

Anders Björkelund, Love Hafdell, and Pierre
Nugues. Multilingual semantic role labeling.
In Proceedings of the Thirteenth Conference
on Computational Natural Language Learning:
Shared Task, pages 43–48. Association for Com-
putational Linguistics, 2009.

Anders Björkelund, Bernd Bohnet, Love Hafdell,
and Pierre Nugues. A high-performance syntactic
and semantic dependency parser. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics: Demonstrations, pages 33–

36. Association for Computational Linguistics,
2010.

Xavier Carreras and Lluı́s Màrquez. Introduction
to the conll-2004 shared task: Semantic role la-
beling. In Proceedings of the Eighth Confer-
ence on Computational Natural Language Learn-
ing, CoNLL 2004, Held in cooperation with HLT-
NAACL 2004, Boston, Massachusetts, USA, May
6-7, 2004, pages 89–97, 2004.

Xavier Carreras and Lluı́s Màrquez. Introduction
to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of the Ninth Conference
on Computational Natural Language Learning,
pages 152–164. Association for Computational
Linguistics, 2005.

Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, and
Steven Skiena. The expressive power of word em-
beddings. arXiv preprint arXiv:1301.3226, 2013.

Ronan Collobert, Jason Weston, Léon Bottou,
Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. Natural language processing (almost)
from scratch. The Journal of Machine Learning
Research, 12:2493–2537, 2011.

Cıcero Nogueira dos Santos and Maıra Gatti. Deep
convolutional neural networks for sentiment anal-
ysis of short texts. In Proceedings of the 25th
International Conference on Computational Lin-
guistics (COLING), Dublin, Ireland, 2014.

John Duchi, Elad Hazan, and Yoram Singer. Adap-
tive subgradient methods for online learning and
stochastic optimization. The Journal of Machine
Learning Research, 12:2121–2159, 2011.

Daniel Gildea and Daniel Jurafsky. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245–288, 2002.

Jan Hajič, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antònia Martı́,
Lluı́s Màrquez, Adam Meyers, Joakim Nivre, Se-
bastian Padó, Jan Štěpánek, et al. The conll-2009
shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning: Shared Task, pages 1–18.
Association for Computational Linguistics, 2009.

Geoffrey Hinton, Simon Osindero, and Yee-Whye
287



Teh. A fast learning algorithm for deep be-
lief nets. Neural computation, 18(7):1527–1554,
2006.

Jenna Kanerva and Filip Ginter. Post-hoc manip-
ulations of vector space models with application
to semantic role labeling. In Proceedings of the
2nd Workshop on Continuous Vector Space Mod-
els and their Compositionality (CVSC)@ EACL,
pages 1–10, 2014.

Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. Better word representations
with recursive neural networks for morphology.
CoNLL-2013, 104, 2013.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
Linguistic regularities in continuous space word
representations. In Proceedings of NAACL-HLT,
pages 746–751, 2013.

Martha Palmer, Paul Kingsbury, and Daniel Gildea.
The proposition bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 31(1):
71–106, 2005.

Lawrence Rabiner. A tutorial on hidden markov
models and selected applications in speech recog-
nition. Proceedings of the IEEE, 77(2):257–286,
1989.

William W Cohen Robert E Schapire and Yoram
Singer. Learning to order things. In Advances
in Neural Information Processing Systems 10:
Proceedings of the 1997 Conference, volume 10,
page 451. MIT Press, 1998.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models
for semantic compositionality over a sentiment
treebank. In Proceedings of the conference on
empirical methods in natural language process-
ing (EMNLP), volume 1631, page 1642. Citeseer,
2013.

Alex Waibel, Toshiyuki Hanazawa, Geoffrey Hin-
ton, Kiyohiro Shikano, and Kevin J Lang.
Phoneme recognition using time-delay neural net-
works. Acoustics, Speech and Signal Processing,
IEEE Transactions on, 37(3):328–339, 1989.

Jason Weston, Frédéric Ratle, Hossein Mobahi,
and Ronan Collobert. Deep learning via semi-
supervised embedding. In Neural Networks:

Tricks of the Trade, pages 639–655. Springer,
2012.

Kristian Woodsend and Mirella Lapata. Text rewrit-
ing improves semantic role labeling. Journal of
Artificial Intelligence Research, pages 133–164,
2014.

288


