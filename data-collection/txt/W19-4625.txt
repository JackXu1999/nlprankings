



















































LIUM-MIRACL Participation in the MADAR Arabic Dialect Identification Shared Task


Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 219–223
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

219

LIUM-MIRACL Participation in the MADAR Arabic Dialect
Identification Shared Task

Saméh Kchaou
University of Sfax, Tunisia

samehkchaou4@gmail.com

Fethi Bougares
University of Le Mans, France

fethi.bougares@univ-lemans.fr

Lamia Hadrich Belguith
University of Sfax, Tunisia

lamia.belguith@gmail.com

Abstract
This paper describes the joint participation of
the LIUM and MIRACL Laboratories at the
Arabic dialect identification challenge of the
MADAR Shared Task (Bouamor et al., 2019)
conducted during the Fourth Arabic Natu-
ral Language Processing Workshop (WANLP
2019). We participated to the Travel Domain
Dialect Identification subtask. We built several
systems and explored different techniques in-
cluding conventional machine learning meth-
ods and deep learning algorithms. Deep learn-
ing approaches did not perform well on this
task. We experimented several classification
systems and we were able to identify the di-
alect of an input sentence with an F1-score of
65.41% on the official test set using only the
training data supplied by the shared task orga-
nizers.

1 Introduction

Dialect can be defined as the language character-
istics of a specific community (Etman and Beex,
2015). For all their daily communications, Arabic
speakers use their local dialect. Dialects are
commonly known as spoken or colloquial Arabic,
acquired naturally as their mother tongue.

Being able to identify the dialect of a given sen-
tence is a fundamental step for various applica-
tions such as machine translation, speech recog-
nition and multiple Natural Language Processing
(NLP) related services. Therefore, the dialect
identification task has been the subject of several
earlier research and exploration activities. For in-
stance, Arabic dialect identification in speech tran-
scripts was introduced as a subtask of the Discrim-
inating between Similar Languages (DSL) Shared
Task of the Third, Fourth and the Fifth Workshop
on NLP for Similar Languages, Varieties and Di-
alects (VarDial) (Malmasi et al., 2016; Zampieri
et al., 2017, 2018).

In practice, the number of existing dialects are
as many as there are cities in the Arab world. Go-
ing to the city-level of granularity for dialect iden-
tification is a complex and expensive task. It is
for this reason that earlier work in dialect identifi-
cation generally study the problem at a region or
country level (Zaidan and Callison-Burch, 2014).
In this respect, dialects are generally classified into
five main groups: Maghrebi, Egyptian, Levantine,
Gulf, and Iraqi (El Haj et al., 2018; Elaraby and
Abdul-Mageed, 2018).

Quite recently, and in contrast with the overall
previous work, (Bouamor et al., 2018) presented
the MADAR corpus which is now the existing re-
source with the greatest dialectal coverage. In-
deed, the MADAR corpus includes 25 Arabic dif-
ferent dialects from east to west. The MADAR
shared task is organized to make the most efficient
use of this corpus. Dialect Identification (DID) is
already a hard task, even when taking into account
only 5 groups. This task became more perplex-
ing when taking into consideration 25 groups of
MADAR shared task. Indeed, taking into consid-
eration additional dialects will reduce the overall
classes dissimilarity and thus make the discrimi-
nation process harder. In the following sections
of this paper, we will describe our participation
to the MADAR Shared task. We investigate dif-
ferent classification techniques based on conven-
tional machine learning algorithms with different
kinds of features and various deep learning se-
quence2sequence architectures.

The paper is structured as follows: Section 2 de-
scribes the MADAR shared task and presents brief
description of the training data. Section 3 presents
a detailed overview of our systems and a discus-
sion of our results. Finally, section 4 will draw a
brief conclusion.



220

2 MADAR Shared Task

Arabic Dialect processing is a challenging task
since dialects are mainly spoken and do not have
an explicit written set of grammar rules. In this
context, the MADAR corpus (Bouamor et al.,
2018) is a valuable resource to push forward the
field Arabic Dialect processing. The MADAR Di-
alect Identification shared task is partially based
on this corpus. MADAR Shared task is the first
DID shared task to target a large set of dialects.
The challenge offered two subtasks: subtask 1
focuses on Travel Domain Dialect Identification,
whereas subtask 2 is centred around Twitter User
Dialect Identification. We will describe only the
subtask 1 in which we have participated.

Subtask 1 Dataset: The provided data-sets are
presented in table 1. The organizers provided a
training and development sets from two sources
created by translating the Basic Traveling Ex-
pression Corpus (BTEC) (Takezawa, 2006): (i)
corpus-6, a large-scale additional sentences of the
BTEC corpus of 5 regional representative dialects
and MSA, (ii) corpus-26, a smaller-scale parallel
corpus of 25 dialects in addition to MSA.

Corpus #lines #token
Corpus-26-train 41.6k 343.7k
Corpus-26-Dev 5.2k 43.7k
Corpus-6-train 54k 452.3k
Corpus-6-Dev 6k 49.6k
Corpus-26-Test 5.2k 43.1k

Table 1: Statistics of MADAR Subtask 1 Data Sets.

The shared task allowed participants to exploit
all the data presented in Table 1 for the develop-
ment of their DID systems1. Submission must
be constrained in the sense that external manu-
ally annotated data sets are prohibited. Submis-
sions are evaluated automatically on the test set
Corpus-26-Test, using F1 score. Both Corpus-26-
dev and Corpus-26-Test consist of 5.2k dialectal
sentences, uniformly distributed over the 26 ad-
dressed dialects (200 sentences for each dialect).
We note that we only use corpus-26 train and dev
to develop our DID systems.

1Training data from MADAR-Shared-Task-Subtask-2 are
also allowed but not used for our submission.

3 LIU-MIR Submission

3.1 Data pre-processing

All Arabic dialects came from the same source,
use the same character set, and share a large num-
ber of common words seen throughout their sub-
stantial vocabulary overlap. None of the existing
Arabic dialects, at least until now, has an official
status and none is regulated and taught in schools.
As there is at present no dialect-specific compu-
tationally motivated pre-processing methods, our
pre-processing is limited to a few steps of cleaning
up applied to all the data without distinction. This
includes the normalization of few arabic charac-
ters (


@,

�
@, @


, ø



, ø, ð2), the deletion of short vowels

and tatweel3 and the deletion of punctuation num-
bers and non arabic words.

3.2 Dialect Identification systems

In this section we present our experiments for
MADAR Travel Domain Dialect Identification
task. All our systems are constrained as we only
used the supplied data from table 1.

3.3 Baseline systems

As a baseline for our DID system, we tried to re-
produce the results presented in (Salameh et al.,
2018). Just like them, we trained a Multinomial
Naive Bayes (MNB) classifier using Word and
character n-gram features. We also used Term
Frequency-Inverse Document Frequency (Tf-Idf)
scores learned on extracted character n-grams
ranging from 1-grams to 5-grams.

N-Gram Features F1 score
Word Char Dev-26 Test-26

1. 1 - 59.64 57.42
2. - 1 10.96 9.99
3. - 1→5 56.44 54.34
4. 1 1 59.14 57.15
5. 1 1→3 60.07 58.51
6. 1 1→5 60.97 59.21

Table 2: MNB system using pre-processed training and
evaluation data..

Table 2 reports the results of our baseline
systems accuracy on the development set of
CORPUS-26 (Dev-26). We performed several ex-
periments using TF-IDF features of word and char

2This corresponds to >, |, <, y, and & with Buckwalter
transliteration.

3A type of justification using characters elongation.



221

level n-grams. The best identification accuracy
is obtained using uni-gram word level 1→5-gram
character level.

We have tested also to evaluate the above sys-
tems without any pre-processing or normalization
of the training data. The results are presented in
the following table.

N-Gram Features F1 score
Word Char Dev-26 Test-26

1 - 64.42 63.33
- 1 16.84 15.51
- 1→5 62.16 60.63
1 1 64.45 63.52
1 1→3 65.29 64.52
1 1→5 66.41 65.41

Table 3: MNB system using raw training and evalua-
tion data.

As shown in the table above, the dialect iden-
tification results are better when the systems are
trained using the raw data. Knowing that data were
created by translating sentences from English and
French, we suspect that the translation was per-
formed by one single person per dialect. If this is
true, This could explain this result since the sys-
tem learns as a side effect, to distinguish between
the style of the translators (i.e spelling, punctua-
tion, lexical choices, with or without vowels ...).

3.4 LM-based Systems
In addition to the baseline system presented in the
previous section, we evaluated the identification
performance using only n-gram word and charac-
ter level Language Models(LM) trained using only
corpus 26 training data. This has been done by
directly comparing LM perplexity for each input
sentence: Given an input sentence S to classify
into one of k dialects d1, d2, ..., dk we select the
dialect d∗ of the model that gives the lowest per-
plexity on this sentence (i.e equation 1).

d∗ = argmin
k

PP (S)
k

(1)

For this experiment, we considered forward and
backward word (LMWF, LMWB) and character-
level (LMCF, LMCB) LMs, trained using se-
quences of words and characters in the reverse
order. All LMs are 5-gram order, trained us-
ing KenLM toolkit with default parameters and
Kneser–Ney smoothing (Heafield, 2011).

Table 4 presents the results of the DID systems
with only LMs. While the character level LMs is

Word LMs Char LMs
LMWF LMWB LMCF LMCB

dev-26 60.34 60.34 61.07 61.36
test-26 60.07 60.15 60.21 60.63

Table 4: F1-scores of DID system using LM Scores

lightly better than word-level LMs, both shows a
lower accuracy compared to the MNB with word
n-gram features (line 1 in table 3). The best LM
based DID system is obtained using Backward
character level LM with F1-score of 60.63 on the
test-26 set.

3.5 LM Scores as Features

In this section we present our attempt to integrate
LMs scores as extra feature to the MNB classifier.
Each sentence is evaluated using the 26 trained
LMs presented in section 3.4 and their scores are
used as input features to the MNB.

N-Gram Feat. LM Feat. F1 score
Word Char Char Word Dev-26

1 1→5 - - 66.41 (65.41)
1 1→5 F - 65.97 (64.60)
1 1→5 B - 66.03 (64.58)
1 1→5 FB - 65.74 (63.73)
1 1→5 - F 45.19 (61.51)
1 1→5 - B 45.19 (61.78)
1 1→5 - FB 44.76 (61.49)
1 1→5 F F 46.09 (61.51)

Table 5: MNB system with N-Gram and LMs features.

As shown in table 5, adding LMs scores as fea-
tures to MNB results in a decrease of F1-score on
both dev-26 and test-26 sets. We tried both word
and character level LMs trained in either Forward
or Backward fashion. However, none of them and
not even their combination had a positive effect on
the system’s accuracy.

3.6 Analysis and Discussion

In this section, we present an analysis of the clas-
sification results of our DID system. Table 6
presents the details of the F1-score per dialect.
Overall, we can see that the system has a good pre-
diction (high F1 score) for several dialects wheres
the identification of other dialects are more chal-
lenging. This result is in accordance with the
rate of token dissimilarity presented in Figure 2 of
(Salameh et al., 2018). For example, ALG, SAN



222

and MOS dialects have high pairwise token dis-
similarity rate and they are also the easier to iden-
tify compared to CAI and RIY for instance.

Dialect F1-score Dialect F1-score
ALE 0.64 JED 0.62
ALG 0.78 JER 0.57
ALX 0.74 KHA 0.69
AMM 0.56 MOS 0.84
ASW 0.60 MSA 0.69
BAG 0.65 MUS 0.54
BAS 0.68 RAB 0.70
BEI 0.67 RIY 0.56
BEN 0.67 SAL 0.56
CAI 0.53 SAN 0.71

DAM 0.56 SFX 0.72
DOH 0.63 TRI 0.75
FES 0.68 TUN 0.70

Table 6: F1-score per dialect of the best system on the
test-26 set

After analyzing the full confusion matrix, we
figured out that identification confusion tends to
be bigger for geographical close cities. This is
expected since sentences from close cities has a
big vocabulary overlap and thus harder to discrim-
inate. In order to investigate further this, we con-
ducted a deeper analysis of two dialects belonging
to the same geographical area. The primary objec-
tive of this study is to measure the upper bound of
the classification accuracy for these dialects (i.e.
the best possible prediction accuracy).

We conducted this analysis for the Top-2
most confused dialects: TUN (TUNIS) and SFX
(SFAX). These dialects belongs to the same coun-
try, Tunisia, and present a high level of lexical
similarity. In addition, there are the only ones for
which we have native speakers. Table 7, presents
the TUN vs. SFX dialects confusion matrix.

Predicted
SFX TUN

A
ct

ua
l SFX 153 18

TUN 41 123

Table 7: TUN - SFX dialects Confusion matrix.

As shown in Table 7, TUN and SFX dialects are
source of confusion for the system. For instance,
from among all the 200 SFX test sentences, 153
were well predicted and 18 predicted TUN. Sim-
ilarly, 123 from the 200 TUN test sentences are
well identified whereas 41 are predicted SFX.

In order to understand this substantial SFX-
TUN confusion, we conducted a manual evalua-
tion of the “18 + 41” sentences. This evaluation
was performed as following: each of the incor-
rectly predicted 18 SFX sentences was presented
to a TUN native speaker who decide whether the
sentence seems to be a natural in his view, or
whether he will formulate it differently. Table 8
presents examples of sentences from the test-26
and their transliteration.

Sentences Label
? ú




	
Gñë 	áÓ I. K
Q

�
¯

�
Hñ

	
KAg AÔ

	
¯ TUN

fmA .hAnwt qryb mn hwny?
½

�
�
ªK
 , @ AJ


�
¯ ñK
AÓ I. m

�
	
' TUN

n.hb mAyw qyAs As, y‘ y∧sk
½

�
�
ªK
 hA

	
®
�
K I. m

�
	
' TUN

n.hb tfA.h y‘y∧sk
? é

�
JñJ. Ë @ Qº

�
�

�
A

�
J
�
¯ð SFX

wqtAy∧s tskr Albwsth?
? ú




	
Gñë 	áÓ I. K
Q

�
¯

	
àñ

�
K@Q�


�
Ë@ ÉJ


�
Kð , ù



ëAK. SFX

bAhy, wtyl Aly∧syrAtwn qryb mn hwny?
½

�
�
ªK
 , h. Ag. X SFX

djAj, y‘yy∧sk

Table 8: Examples of TUN and SFX mis-classified sen-
tences from test-26 with their ground truth label.

The evaluation has shown that, almost all the
“18 + 41” studied examples may belongs to both
dialect and hardly distinguishable even for native
speakers. This exemplifies the increasing com-
plexity the dialect identification task when we con-
sider close dialects of a large lexical overlap.

4 Conclusion

In this paper we described our participation to the
MADAR dialect identification task. We partici-
pated to the the Travel Domain Dialect Identifi-
cation subtask where the goal is to design a sys-
tem able to predict the correct dialect among 26
considered classes. We performed several exper-
iments showing that the DID is a very challeng-
ing task. We were able to reach a F1-score of
65.41 on the official corpus-26-test set. We also
conducted a manual assessment of the Top-2 most
confused classes (SFX and TUN). We have found
that almost all the confused SFX-TUN sentences
are cases for which even a native speaker can-
not decide. This shows that dialect identification
is reaching its effective limit when considered di-
alects have many commonalities.



223

References
Houda Bouamor, Nizar Habash, Mohammad Salameh,

Wajdi Zaghouani, Owen Rambow, Dana Abdul-
rahim, Ossama Obeid, Salam Khalifa, Fadhl Eryani,
Alexander Erdmann, and Kemal Oflazer. 2018. The
MADAR Arabic dialect corpus and lexicon. In Pro-
ceedings of the 11th Language Resources and Eval-
uation Conference, Miyazaki, Japan. European Lan-
guage Resource Association.

Houda Bouamor, Sabit Hassan, and Nizar Habash.
2019. The MADAR Shared Task on Arabic Fine-
Grained Dialect Identification. In Proceedings of the
Fourth Arabic Natural Language Processing Work-
shop (WANLP19), Florence, Italy.

Mahmoud El Haj, Paul Edward Rayson, and Mariam
Aboelezz. 2018. Arabic dialect identification in the
context of bivalency and code-switching. In LREC
2018, Eleventh International Conference on Lan-
guage Resources and Evaluation, pages 3622–3627.

Mohamed Elaraby and Muhammad Abdul-Mageed.
2018. Deep models for Arabic dialect identification
on benchmarked data. In Proceedings of the Fifth
Workshop on NLP for Similar Languages, Varieties
and Dialects (VarDial 2018), pages 263–274, Santa
Fe, New Mexico, USA. Association for Computa-
tional Linguistics.

A. Etman and A. A. L. Beex. 2015. Language and di-
alect identification: A survey. pages 220–231.

Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, WMT
’11, pages 187–197, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Shervin Malmasi, Marcos Zampieri, Nikola Ljubešić,
Preslav Nakov, Ahmed Ali, and Jörg Tiedemann.
2016. Discriminating between similar languages
and arabic dialect identification: A report on the
third dsl shared task. In Proceedings of the Third
Workshop on NLP for Similar Languages, Varieties
and Dialects (VarDial3), pages 1–14, Osaka, Japan.

Mohammad Salameh, Houda Bouamor, and Nizar
Habash. 2018. Fine-grained arabic dialect identi-
fication. In Proceedings of the International Con-
ference on Computational Linguistics (COLING),
pages 1332–1344, Santa Fe, New Mexico, USA.

Toshiyuki Takezawa. 2006. Multilingual spoken lan-
guage corpus development for communication re-
search. In Chinese Spoken Language Processing,
pages 781–791, Berlin, Heidelberg. Springer Berlin
Heidelberg.

Omar F. Zaidan and Chris Callison-Burch. 2014. Ara-
bic dialect identification. Comput. Linguist., 40(1).

Marcos Zampieri, Shervin Malmasi, Nikola Ljubešić,
Preslav Nakov, Ahmed Ali, Jörg Tiedemann, Yves
Scherrer, and Noëmi Aepli. 2017. Findings of the

vardial evaluation campaign 2017. In Proceedings
of the Fourth Workshop on NLP for Similar Lan-
guages, Varieties and Dialects (VarDial), pages 1–
15, Valencia, Spain. Association for Computational
Linguistics.

Marcos Zampieri, Shervin Malmasi, Preslav Nakov,
Ahmed Ali, Suwon Shon, James Glass, Yves Scher-
rer, Tanja Samardžić, Nikola Ljubešić, Jörg Tiede-
mann, Chris van der Lee, Stefan Grondelaers,
Nelleke Oostdijk, Antal van den Bosch, Ritesh Ku-
mar, Bornini Lahiri, and Mayank Jain. 2018. Lan-
guage identification and morphosyntactic tagging:
The second VarDial evaluation campaign. In Pro-
ceedings of the Fifth Workshop on NLP for Similar
Languages, Varieties and Dialects (VarDial), Santa
Fe, USA.

https://www.aclweb.org/anthology/L18-1535
https://www.aclweb.org/anthology/L18-1535
https://www.aclweb.org/anthology/W18-3930
https://www.aclweb.org/anthology/W18-3930
http://dl.acm.org/citation.cfm?id=2132960.2132986
http://dl.acm.org/citation.cfm?id=2132960.2132986

