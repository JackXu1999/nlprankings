



















































Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference


Proceedings of the 2nd Workshop on Evaluating Vector-Space Representations for NLP, pages 36–40,
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Recurrent Neural Network-Based Sentence Encoder with Gated Attention
for Natural Language Inference

Qian Chen
University of Science and Technology of China

cq1231@mail.ustc.edu.cn

Xiaodan Zhu
Queen’s University

xiaodan.zhu@queensu.ca

Zhen-Hua Ling
University of Science and Technology of China

zhling@ustc.edu.cn

Si Wei
iFLYTEK Research

siwei@iflytek.com

Hui Jiang
York University

hj@cse.yorku.ca

Diana Inkpen
University of Ottawa

diana@site.uottawa.ca

Abstract

The RepEval 2017 Shared Task aims to
evaluate natural language understanding
models for sentence representation, in
which a sentence is represented as a fixed-
length vector with neural networks and the
quality of the representation is tested with
a natural language inference task. This
paper describes our system (alpha) that is
ranked among the top in the Shared Task,
on both the in-domain test set (obtain-
ing a 74.9% accuracy) and on the cross-
domain test set (also attaining a 74.9%
accuracy), demonstrating that the model
generalizes well to the cross-domain data.
Our model is equipped with intra-sentence
gated-attention composition which helps
achieve a better performance. In addi-
tion to submitting our model to the Shared
Task, we have also tested it on the Stan-
ford Natural Language Inference (SNLI)
dataset. We obtain an accuracy of 85.5%,
which is the best reported result on SNLI
when cross-sentence attention is not al-
lowed, the same condition enforced in
RepEval 2017.

1 Introduction

The RepEval 2017 Shared Task aims to evaluate
language understanding models for sentence rep-
resentation with natural language inference (NLI)
tasks, where a sentence is represented as a fixed-
length vector.

Modeling inference in human language is very

challenging but is a basic problem in natural lan-
guage understanding. Specifically, NLI is con-
cerned with determining whether a hypothesis
sentence h can be inferred from a premise sen-
tence p.

Most previous top-performing neural network
models on NLI use attention models between a
premise and its hypothesis, while how much in-
formation can be encoded in a fixed-length vec-
tor without such cross-sentence attention deserves
some further understanding. In this paper, we
describe the model we submitted to the RepEval
2017 Shared Task (Nangia et al., 2017), which
achieves the top performance on both the in-
domain and cross-domain test set.

2 Related Work

Natural language inference (NLI), also named rec-
ognizing textual entailment (RTE) includes a large
bulk of early work on rather small datasets with
more conventional methods (Dagan et al., 2005;
MacCartney, 2009). More recently, the large
datasets are available, which makes it possible to
train natural language inference models based on
neural networks (Bowman et al., 2015; Williams
et al., 2017).

Natural language inference models based on
neural networks are mainly separated into two
kind of ways, sentence encoder-based models and
cross-sentence attention-based models. Among
them, Enhanced Sequential Inference Model
(ESIM) with cross-sentence attention represents
the state of the art (Chen et al., 2016b). How-
ever, in this paper we principally concentrate on

36



sentence encoder-based model. Many researchers
have studied sentence encoder-based model for
natural language inference (Bowman et al., 2015;
Vendrov et al., 2015; Mou et al., 2016; Bowman
et al., 2016; Munkhdalai and Yu, 2016a,b; Liu
et al., 2016; Lin et al., 2017). It is, however, not
very clear if the potential of the sentence encoder-
based model has been well exploited. In this pa-
per, we demonstrate that proposed models based
on gated-attention can achieve a new state-of-the-
art performance for natural language inference.

3 Methods

We present here the proposed natural language in-
ference networks which are composed of the fol-
lowing major components: word embedding, se-
quence encoder, composition layer, and the top-
layer classifier. Figure 1 shows a view of the archi-
tecture of our neural language inference network.

Figure 1: A view of our neural language inference
network.

3.1 Word Embedding

In our notation, a sentence (premise or hypothe-
sis) is indicated as x = (x1, . . . , xl), where l is
the length of the sentence. We concatenate em-
beddings learned at two different levels to rep-
resent each word in the sentence: the character
composition and holistic word-level embedding.
The character composition feeds all characters of
each word into a convolutional neural network
(CNN) with max-pooling (Kim, 2014) to obtain
representations c = (c1, . . . , cl). In addition, we
also use the pre-trained GloVe vectors (Penning-
ton et al., 2014) for each word as holistic word-
level embedding w = (w1, . . . , wl). Therefore,
each word is represented as a concatenation of the

character-composition vector and word-level em-
bedding e = ([c1;w1], . . . , [cl;wl]). This is per-
formed on both the premise and hypothesis, re-
sulting into two matrices: the ep ∈ Rn×dw for a
premise and the eh ∈ Rm×dw for a hypothesis,
where n and m are the length of the premise and
hypothesis respectively, and dw is the embedding
dimension.

3.2 Sequence Encoder
To represent words and their context in a premise
and hypothesis, sentence pairs are fed into sen-
tence encoders to obtain hidden vectors (hp and
hh). We use stacked bidirectional LSTMs (BiL-
STM) as the encoders. Shortcut connections are
applied, which concatenate word embeddings and
input hidden states at each layer in the stacked
BiLSTM except for the bottom layer.

hp = BiLSTM(ep) ∈ Rn×2d (1)
hh = BiLSTM(eh) ∈ Rm×2d (2)

where d is the dimension of hidden states of
LSTMs. A BiLSTM concatenate a forward and
backward LSTM on a sequence ht = [

−→
ht ;
←−
ht ],

starting from the left and the right end, respec-
tively. Hidden states of unidirectional LSTM (

−→
ht

or
←−
ht) are calculated as follows,

it
ft
ut
ot

 =


σ
σ

tanh
σ

 (Wxt + Uht−1 + b) (3)
ct = ft � ct−1 + it � ut (4)
ht = ot � tanh(ct) (5)

where σ is the sigmoid function, � is the element-
wise multiplication of two vectors, and W ∈
R4d×dw , U ∈ R4d×d, b ∈ R4d×1 are weight matri-
ces to be learned. For each input vector xt at time
step t, LSTM applies a set of gating functions—
the input gate it, forget gate ft, and output gate ot,
together with a memory cell ct, to control message
flow and track long-distance information (Hochre-
iter and Schmidhuber, 1997) and generate a hid-
den state ht at each time step.

3.3 Composition Layer
To transform sentences into fixed-length vector
representations and reason using those representa-
tions, we need to compose the hidden vectors ob-
tained by the sequence encoder layer (hp and hh).

37



We propose intra-sentence gated-attention to ob-
tain a fixed-length vector. Illustrated by the case
of hidden states of premise hp,

vpg =
n∑

t=1

‖it‖2∑n
j=1 ‖ij‖2

hpt (6)

or vpg =
n∑

t=1

‖1− ft‖2∑n
j=1 ‖1− fj‖2

hpt (7)

or vpg =
n∑

t=1

‖ot‖2∑n
j=1 ‖oj‖2

hpt (8)

where it, ft, ot are the input gate, forget gate,
and output gate in the BiLSTM of the top layer.
Note that the gates are concatenated by forward
and backward LSTM, i.e., it = [

−→
it ;
←−
it ], ft =

[
−→
ft ;
←−
ft ], ot = [−→ot ;←−ot ]. ‖∗‖2 indicates l2-norm,

which converts vectors to scalars. The idea of
gated-attention is inspired by the fact that human
only remember important parts after they read sen-
tences. (Liu et al., 2016; Lin et al., 2017) proposed
a similar “inner-attention” mechanism but it’s cal-
culated by an extra MLP layer which would re-
quire more computation than us.

We also use average-pooling and max-pooling
to obtain fixed-length vectors va and vm as in Chen
et al. (2016b). Then, the final fixed-length vector
representation of premise is vp = [vpg ; v

p
a; v

p
m]. As

for hidden states of hypothesis hh, we can obtain
vh through similar calculation procedure. Conse-
quently, both the premise and hypothesis are fed
into the composition layer to obtain fixed-length
vector representations respectively (vp, vh).

3.4 Top-layer Classifier
Our inference model feeds the resulting vectors
obtained above to the final classifier to determine
the overall inference relationship. In our mod-
els, we compute the absolute difference and the
element-wise product for the tuple [vp, vh]. The
absolute difference and element-wise product are
then concatenated with the original vectors vp and
vh (Mou et al., 2016).

vinp = [vp; vh; |vp − vh|; vp � vh] (9)
We then put the vector vinp into a final multi-

layer perceptron (MLP) classifier. The MLP has
2 hidden layers with ReLu activation with short-
cut connections and a softmax output layer in our
experiments. The entire model (all four compo-
nents described above) is trained end-to-end, and

the cross-entropy loss of the training set is mini-
mized.

4 Experimental Setup

Data RepEval 2017 use Multi-Genre NLI cor-
pus (MultiNLI) (Williams et al., 2017), which
focuses on three basic relationships between a
premise and a potential hypothesis: the premise
entails the hypothesis (entailment), they contradict
each other (contradiction), or they are not related
(neutral). The corpus has ten genres, such as fic-
tion, letters, telephone speech and so on. Training
set only has five genres of them, therefore there
are in-domain and cross-domain development/test
sets. SNLI (Bowman et al., 2015) corpus can
be used as an additional training/development set,
which includes content from the single genre of
image captions. However, we don’t use SNLI as
an additional training/development data in our ex-
periments.

Training We use the in-domain development
set to select models for testing. To help replicate
our results, we publish our code at https:
//github.com/lukecq1231/enc_nli
(the core code is also used or adapted for
a summarization (Chen et al., 2016a) and a
question-answering task (Zhang et al., 2017)).
We use the Adam (Kingma and Ba, 2014) for
optimization. Stacked BiLSTM has 3 layers, and
all hidden states of BiLSTMs and MLP have 600
dimensions. The character embedding has 15
dimensions, and CNN filters length is [1,3,5],
each of those is 100 dimensions. We use pre-
trained GloVe-840B-300D vectors (Pennington
et al., 2014) as our word-level embeddings and
fix these embeddings during the training process.
Out-of-vocabulary (OOV) words are initialized
randomly with Gaussian samples.

5 Results

Table 1 shows the results of different models. The
first group of models are copied from Williams
et al. (2017). The first sentence encoder is based
on continuous bag of words (CBOW), the second
is based on BiLSTM, and the third model is En-
hanced Sequential Inference Model (ESIM) (Chen
et al., 2016b) reimplemented by Williams et al.
(2017), which represents the state of the art on
SNLI dataset. However, ESIM uses attention be-
tween sentence pairs, which is not a sentence-
encoder based model.

38



Model In Cross
CBOW 64.8 64.5
BiLSTM 66.9 66.9
ESIM 72.3 72.1
TALP-UPC∗ 67.9 68.2
LCT-MALTA∗ 70.7 70.8
Rivercorners∗ 72.1 72.1
Rivercorners (ensemble)∗ 72.2 72.8
YixinNie-UNC-NLP∗ 74.5 73.5
Our ESIM 76.8 75.8
Single∗ 73.5 73.6
Ensembled∗ 74.9 74.9
Single (Input Gate)∗ 73.5 73.6
Single (Forget Gate) 72.9 73.1
Single (Output Gate) 73.7 73.4
Single - Gated-Att 72.8 73.6
Single - CharCNN 72.9 73.5
Single - Word Embedding 65.6 66.0
Single - AbsDiff/Product 69.7 69.2

Table 1: Accuracies of the models on MultiNLI.
Note that ∗ indicates that the model participate in
the competition on June 15th, 2017.

The second group of models are the results of
other teams which participate the RepEval 2017
Share Task competition (Nangia et al., 2017).

In addition, we also use our implementation of
ESIM, which achieves an accuracy of 76.8% in the
in-domain test set, and 75.8% in the cross-domain
test set, which presents the state-of-the-art results.
After removing the cross-sentence attention and
adding our gated-attention model, we achieve ac-
curacies of 73.5% and 73.6%, which ranks first
in the cross-domain test set and ranks second in
the in-domain test set among the single models.
When ensembling our models, we obtain accura-
cies 74.9% and 74.9%, which ranks first in both
test sets. Our ensembling is performed by averag-
ing the five models trained with different parame-
ter initialization.

We compare the performance of using different
gate in gate-attention in the fourth group of Ta-
ble 1. Note that we use attention based on input
gate on all other experiments.

To understand the importance of the different
elements of the proposed model, we remove some
crucial elements from our single model. If we
remove the gated-attention, the accuracies drop
to 72.8% and 73.6%. If we remove character-
composition vector, the accuracies drop to 72.9%
and 73.5%. If we remove word-level embedding,
the accuracies drop to 65.6% and 66.0%. If we re-

Model Test
LSTM (Bowman et al., 2015) 80.6
GRU (Vendrov et al., 2015) 81.4
Tree CNN (Mou et al., 2016) 82.1
SPINN-PI (Bowman et al., 2016) 83.2
NTI (Munkhdalai and Yu, 2016b) 83.4
Intra-Att BiLSTM (Liu et al., 2016) 84.2
Self-Att BiLSTM (Lin et al., 2017) 84.2
NSE (Munkhdalai and Yu, 2016a) 84.6
Gated-Att BiLSTM 85.5

Table 2: Accuracies of the models on SNLI.

move absolute difference and element-wise prod-
uct of the sentence representation vectors, the ac-
curacies drop to 69.7% and 69.2%.

In addition to testing on this shared task, we
have also applied our best single system (with-
out ensembling) on the SNLI dataset; our model
achieve an accuracy of 85.5%, which is best re-
sult reported on SNLI, outperforming all pre-
vious models when cross-sentence attention is
not allowed. The previous state-of-the-art sen-
tence encoder-based model (Munkhdalai and Yu,
2016b), called neural semantic encoders (NSE),
only achieved an accuracy of 84.6% on SNLI. Ta-
ble 2 shows the results of previous models and pro-
posed model.

6 Summary and Future Work

We describe our system that encodes a sentence
to a fixed-length vector for natural language in-
ference, which achieves the top performances on
both the RepEval-2017 and the SNLI dataset. The
model is equipped with a novel intra-sentence
gated-attention component. The model only uses
a common stacked BiLSTM as the building block
together with the intra-sentence gated-attention in
order to compose the fixed-length representations.
Our model could be used on other sentence encod-
ing tasks. Future work on NLI includes exploring
the usefulness of external resources such as Word-
Net and contrasting-meaning embedding (Chen
et al., 2015).

Acknowledgments

The first and the third author of this paper were
supported in part by the National Natural Science
Foundation of China (Grants No. U1636201) and
the Fundamental Research Funds for the Central
Universities (Grant No. WK2350000001).

39



References
R. Samuel Bowman, Gabor Angeli, Christopher Potts,

and D. Christopher Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, pages 632–642.
https://doi.org/10.18653/v1/D15-1075.

R. Samuel Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, D. Christopher Manning, and
Christopher Potts. 2016. A fast unified model for
parsing and sentence understanding. In Proceed-
ings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics,
pages 1466–1477. https://doi.org/10.18653/v1/P16-
1139.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
and Hui Jiang. 2016a. Distraction-based neural
networks for modeling document. In Subbarao
Kambhampati, editor, Proceedings of the Twenty-
Fifth International Joint Conference on Artificial In-
telligence, IJCAI 2016, New York, NY, USA, 9-15
July 2016. IJCAI/AAAI Press, pages 2754–2760.
http://www.ijcai.org/Abstract/16/391.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,
and Hui Jiang. 2016b. Enhanced LSTM for nat-
ural language inference. CoRR abs/1609.06038.
http://arxiv.org/abs/1609.06038.

Zhigang Chen, Wei Lin, Qian Chen, Xiaoping Chen,
Si Wei, Hui Jiang, and Xiaodan Zhu. 2015. Re-
visiting word embedding for contrasting meaning.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers). Associ-
ation for Computational Linguistics, pages 106–115.
https://doi.org/10.3115/v1/P15-1011.

Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In MLCW.

Sepp Hochreiter and Jürgen Schmidhu-
ber. 1997. Long short-term memory.
Neural Computation 9(8):1735–1780.
https://doi.org/10.1162/neco.1997.9.8.1735.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882 .

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR
abs/1412.6980. http://arxiv.org/abs/1412.6980.

Zhouhan Lin, Minwei Feng, Cı́cero Nogueira dos
Santos, Mo Yu, Bing Xiang, Bowen Zhou, and
Yoshua Bengio. 2017. A structured self-attentive
sentence embedding. CoRR abs/1703.03130.
http://arxiv.org/abs/1703.03130.

Yang Liu, Chengjie Sun, Lei Lin, and Xiao-
long Wang. 2016. Learning natural language
inference using bidirectional LSTM model
and inner-attention. CoRR abs/1605.09090.
http://arxiv.org/abs/1605.09090.

Bill MacCartney. 2009. Natural Language Inference.
Ph.D. thesis, Stanford University.

Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang,
Rui Yan, and Zhi Jin. 2016. Natural language
inference by tree-based convolution and heuris-
tic matching. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers). Associa-
tion for Computational Linguistics, pages 130–136.
https://doi.org/10.18653/v1/P16-2022.

Tsendsuren Munkhdalai and Hong Yu. 2016a. Neu-
ral semantic encoders. CoRR abs/1607.04315.
http://arxiv.org/abs/1607.04315.

Tsendsuren Munkhdalai and Hong Yu. 2016b. Neu-
ral tree indexers for text understanding. CoRR
abs/1607.04492. http://arxiv.org/abs/1607.04492.

Nikita Nangia, Adina Williams, Angeliki Lazaridou,
and Samuel R. Bowman. 2017. The repeval 2017
shared task: Multi-genre natural language inference
with sentence representations. In Proceedings of
RepEval 2017: The Second Workshop on Evaluating
Vector Space Representations for NLP. Association
for Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christo-
pher Manning. 2014. Glove: Global vectors
for word representation. In Proceedings of the
2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). Association
for Computational Linguistics, pages 1532–1543.
https://doi.org/10.3115/v1/D14-1162.

Ivan Vendrov, Ryan Kiros, Sanja Fidler, and
Raquel Urtasun. 2015. Order-embeddings of
images and language. CoRR abs/1511.06361.
http://arxiv.org/abs/1511.06361.

Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2017. A broad-coverage challenge corpus for
sentence understanding through inference. CoRR
abs/1704.05426. http://arxiv.org/abs/1704.05426.

Junbei Zhang, Xiaodan Zhu, Qian Chen, Lirong
Dai, Si Wei, and Hui Jiang. 2017. Ex-
ploring question understanding and adapta-
tion in neural-network-based question an-
swering. CoRR abs/arXiv:1703.04617v2.
https://arxiv.org/abs/1703.04617.

40


