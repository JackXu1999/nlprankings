















































Training Dependency Parsers from Partially Annotated Corpora


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 776–784,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Training Dependency Parsers from Partially Annotated Corpora
Daniel Flannery1, Yusuke Miyao2, Graham Neubig1, Shinsuke Mori1

1Graduate School of Informatics, Kyoto University
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan

2National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan

flannery@ar.media.kyoto-u.ac.jp, yusuke@nii.ac.jp,
neubig@ar.media.kyoto-u.ac.jp, forest@i.kyoto-u.ac.jp

Abstract

We introduce a maximum spanning tree
(MST) dependency parser that can be
trained from partially annotated corpora,
allowing for effective use of available lin-
guistic resources and reduction of the costs
of preparing new training data. This is
especially important for domain adapta-
tion in a real-world situation. We use a
pointwise approach where each edge in
the dependency tree for a sentence is es-
timated independently. Experiments on
Japanese dependency parsing show that
this approach allows for rapid training and
achieves accuracy comparable to state-of-
the-art dependency parsers trained on fully
annotated data.

1 Introduction

Parsing is one of the fundamental building blocks
of natural language processing, with applications
ranging from machine translation (Yamada and
Knight, 2001) to information extraction (Miyao
et al., 2009). However, while statistical parsers
achieve higher and higher accuracies on in-domain
text, the creation of data to train these parsers is
labor-intensive, which becomes a bottleneck for
smaller languages. In addition, it is also a well
known fact that accuracy plummets when tested
on sentences of a different domain than the train-
ing corpus (Gildea, 2001; Petrov et al., 2010), and
that in-domain data can be annotated to make up
for this weakness.

In this paper, we propose a maximum span-
ning tree (MST) parser that helps ameliorate these
problems by allowing for the efficient develop-
ment of training data. This is done through a com-
bination of an efficient corpus annotation strategy,
and a novel parsing method. For corpus construc-
tion, we use partial annotation, which allows an

annotator to skip annotation of unnecessary edges,
focusing their efforts only on the ones that will
provide the maximal gains in accuracy.

While partial annotation has been shown to be
an effective annotation strategy for a number of
tasks (Tsuboi et al., 2008; Sassano and Kurohashi,
2010; Neubig and Mori, 2010), traditional MST
parsers such as that of McDonald et al. (2005) can-
not be learned from partially annotated data. The
reason for this is that they use structural predic-
tion methods that must be learned from fully an-
notated sentences. However, a number of recent
works (Liang et al., 2008; Neubig et al., 2011)
have found that it is possible to ignore structure
and still achieve competitive accuracy on tasks
such as part-of-speech (POS) tagging.

Similarly, recent work on dependency parsing
(Spreyer and Kuhn, 2009; Spreyer et al., 2010)
has shown that training constraints can be relaxed
to allow MST parsers to be trained from partially
annotated sentences, with only a small reduction
in parsing accuracy. In this approach the scor-
ing function used to evaluate potential dependency
trees is modified so that it does not penalize trees
consistent with the partial annotations used for
training. Our formulation of an MST parser is
based on an even stronger independence assump-
tion, namely that the score of each edge is inde-
pendent of the other edges in the dependency tree.
While this does have the potential to decrease ac-
curacy, it has a number of advantages such as the
ability to use partially annotated data, faster speed,
and simple implementation.

We perform an evaluation of the proposed
method on a Japanese dependency parsing task.
First, we compare the proposed method to both a
traditional MST parser (McDonald et al., 2005),
and a deterministic parser (Nivre and Scholz,
2004). We find that despite the lack of structure
in our prediction method, the proposed method
is still able to achieve accuracy similar to that of

776



the traditional MST parser, while training and test-
ing speeds are similar to that of the deterministic
parser.

In addition, we perform a case-study of the
use of partial annotation in a practical scenario,
where we have data that follows a segmentation
standard that differs from the one we would like
to follow. In Japanese dependency parsing, tra-
ditionally phrase segments (bunsetsu) have been
used instead of words as the minimal unit for
parsing (Kudo and Matsumoto, 2002; Sassano
and Kurohashi, 2010), but these segments are of-
ten too large or unwieldy for applications such
as information extraction and machine translation
(Nakazawa and Kurohashi, 2008). In our case-
study, we demonstrate that a corpus labeled with
phrase dependencies can be used as a partially
annotated corpus in the development of a word-
based parser that is more appropriate for these ap-
plications. The use of a phrase-labeled corpus al-
lows us to increase the accuracy of a word-based
parser trained on a smaller word-labeled data set
by 2.75%.

2 Pointwise estimation for dependency
parsing

This work follows the standard setting of re-
cent work on dependency parsing (Buchholz and
Marsi, 2006). Given as input a sequence of words,
w = 〈w1, w2, . . . , wn〉, the goal is to output a de-
pendency tree d = 〈d1, d2, . . . , dn〉, where di ≡ j
when the head of wi is wj .1 We assume that di = 0
for some word wi in a sentence, which indicates
that wi is the head of the sentence.

2.1 A Pointwise MST Parser

The parsing model we pursue in this paper is Mc-
Donald et al. (2005)’s edge-factored model. A
score, σ(di), is assigned to each edge (i.e. de-
pendency) di, and parsing finds a dependency tree,
d̂, that maximizes the sum of the scores of all the
edges.

d̂ = argmax
d

∑

d∈d
σ(d). (1)

It is known that, given σ(d) for all possible
dependencies in a sentence, d̂ can be computed

1While we describe unlabeled dependency parsing for
simplicity, it is trivial to extend it to labeled dependency pars-
ing.

by the maximum spanning tree algorithm such as
Chu-Liu/Edmonds’ algorithm.

An important difference from McDonald et al.
(2005) is in the estimation of σ(d). McDonald et
al. (2005) applied a perceptron-like algorithm that
optimizes the score of entire dependency trees.
However, we stick to pointwise prediction: σ(di)
is estimated for each wi independently. A variety
of machine-learning-based classifiers can be ap-
plied to the estimation of σ(d), because it is es-
sentially an n-class classification problem.

In the experiments, we estimate a log-linear
model (Berger et al., 1996). We calculate the
probability of a dependency labeling p(di = j)
for a word wi from its context, which is a tuple
x = 〈w, t, i〉, where t = 〈t1, t2, . . . , tn〉 is a se-
quence of POS tags assigned to w by a tagger. The
conditional probability p(j|x) is given by the fol-
lowing equation.

p(j|x,θ) = exp (θ · φ(x, j))∑
j′∈J exp (θ · φ(x, j′))

(2)

The feature vector φ = 〈φ1, φ2, . . . , φm〉
is a vector of non-negative values calculated
from features on pairs (x, j), with correspond-
ing weights given by the parameter vector θ =
〈θ1, θ2, . . . , θm〉. We estimate θ from sentences
annotated with dependencies. It should be noted
that the probability p(di) depends only on i, j,
and the inputs w, t, which ensures that it is es-
timated independently for each wi. Because pa-
rameter estimation does not involve computing d̂,
we do not apply the maximum spanning tree algo-
rithm in training.

2.2 Features
Our current implementation uses the following
features for φ.

F1: The distance between a dependent word and
its candidate head.

F2: The surface forms of the dependent and head
words.

F3: The parts-of-speech of the dependent and
head words.

F4: The surface forms of up to three words to the
left of the dependent and head words.

F5: The surface forms of up to three words to the
right of the dependent and head words.

777



i 1 2 3 4 5 6 7 8 9
wi 政府 は 投資 に つなが る と 歓迎 し

Eng. Gov. subj. investment to leads ending that welcomes do
ti noun part. noun part. verb infl. part. noun verb
di 8
F1 6
F2 は 歓迎
F3 part. noun
F4 政府 つなが,る,と
F5 投資,に,つなが し
F6 noun verb, infl., part.
F7 noun, part., verb verb

The second word, case markerは (subj.), has two grammatically possible heads: the verbつなが (leads)
and the verb歓迎 (welcomes). In our framework, only this word needs to be annotated with its head.

Figure 1: An example of a partially annotated sentence and the features for a dependency between case
markerは (subj.) and the verb歓迎 (welcomes).

F6: The parts-of-speech of the words selected for
F4.

F7: The parts-of-speech of the words selected for
F5.

Figure 1 shows the values of these features for
a partially annotated example sentence where one
word, case markerは (subj.), has been annotated
with its head, the verb歓迎 (welcomes).

Pointwise prediction rather than structured pre-
diction has the potential to hurt parsing accuracy.
However, our method can enjoy greater flexibil-
ity, which allows for training from partially anno-
tated corpora as will be described in Section 3. It
also simplifies the implementation and reduces the
time necessary for training.

2.3 Solution Search

In the experiments, we target Japanese parsing.
Because Japanese is a head-final language, we as-
sume di > i for all i 6= n and dn = 0. This as-
sumption reduces the maximum spanning tree al-
gorithm to a simpler algorithm: for each word we
select the dependency with the maximum score.
As this never creates a loop of dependencies, a
recursive process as in Chu-Liu/Edmonds’ algo-
rithm is not necessary.

3 Domain Adaptation for MST Parsing

Assuming that the cost of annotation corresponds
roughly to the number of annotations performed,
out of all possible annotations to have annotators

perform for a target domain corpus we want to se-
lect the ones which provide the greatest benefit to
accuracy when training. The high cost of anno-
tation work is the primary motivation for this ap-
proach.

3.1 Partial Annotation for a Parser

In the context of dependency parsing, partial anno-
tation refers to annotating only certain dependen-
cies between words in a sentence. Dependencies
which are assumed to have little to no value for
training are left unannotated. Figure 1 shows an
example of a partially annotated sentence that can
be used as training data by our system.

Before text can be annotated with dependencies
for use in our system, it must first be tokenized and
labeled with POS tags.2 We assume that the results
of this tokenization and POS tagging are accurate
enough that we need to manually annotate only the
dependencies between the tokenized words.

3.2 Learning Feature Weights from Partial
Annotations

As explained in Section 2.1, edge scores, σ(di),
are estimated for each wi independently. This
means that the estimation of σ(di) requires only
a gold dependency of wi, and the other dependen-
cies in a sentence are not necessary. This allows us
to learn weights θ for features from partially anno-
tated corpora. When training data includes a gold

2We take a language-independent approach that does not
make any assumptions about the unit of tokenization or the
meaning of tags used.

778



phrase-based dependency corpus (fully annotated)
ID head phrase
01 02 党内/nounの/part. ?
02 07 議論/nounは/part.

?

03 04 「 /symbol保守/noun ?
04 05 二/noun党/suff. 論/nounは/part. ?
05 06 よろし /adj. く/infl.な/adj. い/infl. 。/symbol 」/symbolと/part. ?
06 07 い/verbう/infl. ?
07 – もの/nounだ/aux. 。/symbol

Figure 2: An example of phrase-based dependency annotation for a sentence.

dependency that wi depends on wj , a discrimina-
tive classifier can be trained by regarding di = j
as a positive sample and di = j′ s.t. j′ 6= j as
negative samples.

In the case of Japanese parsing, because j >
i for all di = j, negative samples are di =
j′ s.t. j′ 6= j ∧ j′ > i. For example, from the par-
tial annotation given in Figure 1, we can create a
training instance for w2,は (subj.), where the pos-
itive sample is d2 = 8 and the negative samples
are d2 = 3, 4, . . . , 7, 9.

3.3 Domain Adaptation with a Partially
Annotated Training Corpus

As a case study, we show how partial annotation
can be used as a low-cost method of converting
the annotation standard of an existing linguistic re-
source. As we mentioned in Section 1, traditional
frameworks for Japanese dependency parsing are
phrase-based. Many existing dependency corpora
use phrases as the unit of annotation, and these re-
sources are a valuable potential source of data for
mining word dependencies. However, phrase de-
pendencies alone do not provide enough informa-
tion for an automatic conversion to word depen-
dencies. One of the advantages of our parser is
that it can be trained on a partially annotated cor-
pus, so if we can derive even some word depen-
dencies from phrase dependencies we can quickly
and easily make use of existing resources.

To take advantage of these linguistic resources,
we created a number of rules to derive word-based
dependency annotations from phrase-based anno-
tations. Instead of trying to convert all phrase de-
pendencies, we focused on heuristics that provide
only reliable word dependencies. The word-based
dependency set produced by these rules is a partial
annotation of the original corpus.

For the domain adaptation experiments de-
scribed in Section 4, we used this procedure on

the NAIST Text Corpus (NTC) (Iida et al., 2007)
to create a small partially-annotated target domain
corpus. The NTC consists of newspaper articles
from the Mainichi Shimbun.3 Figure 2 shows an
example sentence from this corpus annotated with
phrase dependencies.

To aid the construction of conversion rules, we
chose three broad categories of words - content
words, function words, and punctuation symbols
- that provide clues to the structure of a phrase.
Before we explain our rules, we will give a short
explanation of these three categories.

We defined content words as nouns, verbs, ad-
jectives, interjections, prenominal adjectives, suf-
fixes, and prefixes. Function words are auxiliary
verbs, particles, inflections, and conjunctions. In
this context, punctuation symbols are both the En-
glish and Japanese versions of period and comma
characters. These three categories are used to de-
termine phrases which can be mined for relatively
accurate word dependencies.

Figure 3 shows an example of how the rules ex-
plained below are used to derive word-based de-
pendencies from phrase-based dependencies for
the sentence given in Figure 2.

The first two rules are inter-phrase rules, which
are concerned with the relationship between words
located in different phrases.

1. LAST: Given a dependent phrase and its head
phrase in the original annotation, set the head
of the last word in the dependent phrase to the
last content word in the head phrase. Note,
we only apply this rule if the head phrase
consists of a content word followed by zero
or more function words, followed by an op-
tional punctuation symbol.

3In addition to phrase dependency annotations, the NTC
also contains predicate-argument and coreference tags that
are useful for deriving reliable word dependencies.

779



word-based dependency corpus (partially annotated)
ID head word rule
01 02 党内/noun CF?
02 03 の/part. LAST?
03 04 議論/noun CF?
04 19 は/part. LAST

?

05 15 「 /symbol PAREN

?

06 保守/noun
07 二/noun
08 党/suff.
09 10 論/noun CF?
10 は/part.
11 12 よろし /adj. INFLECT?
12 く/infl.
13 14 な/adj. INFLECT?
14 15 い/infl. FUNCT?
15 」/symbol
16 17 と/part. LAST?
17 18 い/verb INFLECT?
18 19 う/infl. LAST?
19 もの/noun
20 21 だ/aux. FUNCT?
21 – 。/symbol

Figure 3: An example of word-based dependencies derived from phrase-based dependencies for a sen-
tence.

2. PAREN: Set the head of a left parenthesis (or
left bracket) word to the first right parenthe-
sis (or right bracket) that follows it in the sen-
tence.

The last four rules are intra-phrase rules that are
concerned with the dependencies between words
in the same phrase. The following rules were
found to be effective.

3. FFS: If a phrase consists of zero or more con-
tent words, function words, or punctuation
symbols followed by a sequence of two func-
tion words and a punctuation symbol, then set
the head of the first function word in the se-
quence to the second function word and the
head of the second function word to the punc-
tuation symbol.

4. CF: If a phrase consists of zero or more con-
tent words followed by a sequence of a con-
tent word and a function word, then set the
head of the content word to the function
word.

5. INFLECT: If a word that is inflected in
Japanese (verb, auxiliary verb, or adjective4)

4In Japanese there are two types of adjectives, i-type ad-
jectives and na-type adjectives. Both types are inflected.

is followed by an inflection, the first word de-
pends on the inflection.

6. FUNCT: If a function word is followed by a
punctuation symbol, set the head of the func-
tion word to the punctuation symbol.

4 Evaluation

As an evaluation of our parser, we measured pars-
ing accuracies of several systems on test corpora
in two domains: one is a general domain in which
a corpus fully annotated with word boundary and
dependency information is available, and the other
is a target domain assuming an adaptation situa-
tion in which only a partially annotated corpus is
available for quick and low-cost domain adapta-
tion.

4.1 Experimental Settings

In the experiments we used example sentences
from a dictionary (Keene et al., 1992) as the gen-
eral domain data, and newspaper articles as the
target domain data. We used business newspaper
articles (Nikkei), similar to the Wall Street Jour-
nal, for the target domain test set. For the domain
adaptation experiments, we used the partially-
annotated corpus mentioned in Section 3.3 as a

780



Table 1: Sizes of Corpora.

ID source usage #sentences #words #chars
EHJ-train example sentences training 11,700 145,925 197,941
EHJ-test from a dictionary test 1,300 16,348 22,207
NTC-train newspaper articles training 34,712 1,045,328 1,510,618
NKN-test newspaper articles test 1,002 29,038 43,695

NTC-train is a partially annotated corpus derived from phrase-based dependency annotations.

target domain training corpus. This corpus con-
sists of newspaper articles that are similar to the
target domain test set.

Usages and specifications of the various cor-
pora are shown in Table 1. All the sentences are
segmented into words manually and all the words
are annotated with their heads manually. The
Japanese data provided by the CoNLL organizers
(Buchholz and Marsi, 2006) are the result of an
automatic conversion from phrase (bunsetsu) de-
pendencies. For a more appropriate evaluation we
have prepared a word-based dependency data set.

The dependencies have no labels because al-
most all nouns are connected to a verb with a case
marker and many important labels are obvious.
The words are not annotated with POS tags, so
we used a Japanese POS tagger, KyTea (Neubig
et al., 2011), trained on about 40k sentences from
the BCCWJ (Maekawa, 2008).

For the general domain experiments we com-
pared the following systems, using projective pars-
ing algorithms for training because of the as-
sumptions about Japanese parsing outlined in Sec-
tion 2.1.

1. Malt: Nivre et al. (2006)’s MaltParser, us-
ing Nivre’s arc-eager algorithm and the op-
tion for strict root handling.

2. MST: McDonald et al. (2005)’s MST Parser,
using k-best parse size with k=5.

3. PW: Our system, where pointwise estimation
is used to estimate dependencies. Stochas-
tic gradient descent training was used to train
log-linear models.

4.2 With a Fully Annotated Training Corpus
For the first experiment, we measured the accu-
racy of each system on an in-domain test set when
training on a fully annotated corpus. The results
are shown in Table 2. Malt and MST have similar
accuracy. PW has slightly higher accuracy than

both of these systems, but the difference in accu-
racy is not statistically significant. We also mea-
sured the training time and the parsing speed of
each system. Table 3 shows the results. From this
table, first we see that MST is much slower than
Malt, as is well known. Our method, however,
is much faster than MST and the parsing speed is
approximately the same as the shift-reduce-based
Malt.

Theoretically the training time of our method
is proportional to the number of annotated depen-
dencies. In line with Kudo and Matsumoto (2002),
we make two assumptions about Japanese depen-
dency parsing. First, because Japanese is a head-
final language we assume that every word except
the final one in a sentence depends on one of the
words located to its right. Second, we assume
that all dependencies are projective, in other words
that edges in the dependency tree do not cross
each other. These assumptions limit the number
of candidate heads for a word, reducing the train-
ing time.

Because all parsers were trained with projective
algorithms, the first assumption is most likely the
main reason for the difference in training times be-
tween PW and MST. For other languages where
possible heads can be located both to the left and
right of a word, we expect training times to in-
crease. Our pointwise approach can be extended to
handle these languages by changing the constraint
on heads from j > i to j 6= i for all di = j. This
is an important direction for future work now that
we have confirmed that this approach is effective
for Japanese.

We performed a second experiment in the gen-
eral domain to measure the impact of the training
corpus size on parsing accuracy. To make smaller
training corpora, we set a fixed number of depen-
dency annotations and then sequentially selected
sentences from EHJ-train until the desired number
of dependency annotations were collected. The re-

781



Table 2: Parsing Accuracy on EHJ-test.

method EHJ-test
Malt 96.63%
MST 96.67%
PW 96.83%
All systems were
trained on EHJ-train.

sults are shown in Figure 4. Though PW achieves
the highest accuracy when the full training corpus
is used, Malt has higher accuracy than both of the
MST-based systems when the training corpus is
one-third or less the size of the full training cor-
pus. It can also be shown that both MST-based
systems improve at a similar rate for all sizes of
training corpora.

4.3 Domain Adaptation with a Partially
Annotated Training Corpus

Tasks that make use of parsers, such as ma-
chine translation (Yamada and Knight, 2001), of-
ten require word-based models. However, because
phrase-based approaches have traditionally been
used for Japanese dependency parsing (Kudo and
Matsumoto, 2002; Sassano and Kurohashi, 2010),
word-based linguistic resources for Japanese are
scarce. Preparing the fully annotated corpora re-
quired by existing word-based parsers such as Mc-
Donald et al. (2005)’s MST Parser is an expensive
and laborious task.

Our parser attempts to address these problems
by introducing a word-based framework for de-
pendency parsing that can use partially annotated
training data. Partial annotation is one way to effi-
ciently make use of existing resources in the target
domain without incurring high annotation costs.

We used each rule described in Section 3.3 in-
dividually to convert the annotations in the NTC-
train and produce a pool of word-based dependen-
cies. We then selected 5k of those dependencies
to add to EHJ-train, and measured the results on
NKN-test. We also used all rules simultaneously
to produce word-based dependencies and mea-
sured the results in the same way as the individ-
ual rules. The total size of the partial annotation
pool produced by using all rules was 248,148 de-
pendencies out of 1,010,648 annotation candidates
(not counting the last word of sentences, which
has no dependency). The baseline case only used
the EHJ-train with no partial annotations from the

Table 3: Training Time and Parsing Speed.

method training time parsing speed
Malt 14[s] 1.3[ms/sent.]
MST 1901[s] 32.7[ms/sent.]
PW 125[s] 2.8[ms/sent.]
All systems were trained on EHJ-train and
tested on EHJ-test. The machine used had
a 3.33GHz processor and 12GB of RAM.

pool. The results are shown in Figure 5.
It can be seen that the LAST rule is the most

effective, followed by the PAREN rule. This sug-
gests that the long-distance dependencies provided
by these rules are more useful for domain adapta-
tion than the short-distance dependency informa-
tion that the intra-phrase rules provide.

Combining all of the rules increases the accu-
racy on NKN-test to 88.44%, an increase of 2.75%
over the baseline. This combination of rules re-
sults in lower accuracy gains than the sum of
the gains from individual rules because different
rules may convert the same phrase dependencies.
These results show that our pointwise approach al-
lows for effective use of existing target domain re-
sources and increased parsing accuracy in the tar-
get domain through partial annotation.

5 Related Work

There has been a significant amount of work on
how to utilize in-domain data to improve the ac-
curacy of parsing. The majority of this work has
focused on using unlabeled data in combination
with self-training (Roark and Bacchiani, 2003;
McClosky et al., 2006) or other semi-supervised
learning methods (Blitzer et al., 2006; Nivre et al.,
2007; Suzuki et al., 2009).

Roark and Bacchiani (2003) also present work
on supervised domain adaptation, although this fo-
cuses on the utilization of an already-existing in-
domain corpus.

There has also been some work on efficient an-
notation of data for parsing (Tang et al., 2002; Os-
borne and Baldridge, 2004; Sassano and Kuro-
hashi, 2010). Most previous work focuses on
picking efficient sentences to annotate for parsing,
but Sassano and Kurohashi (2010) also present
a method for using partially annotated data with
deterministic dependency parsers, which can be
trivially estimated from partially annotated data.

782



0.950

0.955

0.960

0.965

0.970

 0  40000  80000  120000

D
e
p

e
n

d
e
n

c
y
 A

c
c
u

ra
c
y

Training Set Size (Dependencies)

Malt
MST
PW

Figure 4: Comparison of parsing accuracy for dif-
ferent parsers.

Other recent work (Spreyer and Kuhn, 2009;
Spreyer et al., 2010) has shown how both Nivre
et al. (2006)’s MaltParser and McDonald et al.
(2005)’s MST can be adapted to use partially an-
notated training data.

Traditional MST parsers such as McDonald et
al. (2005)’s use structured prediction methods.
Wang et al. (2007) showed that local classifica-
tion methods can be used to train structured pre-
dictors. Their approach also uses “dynamic” fea-
tures, where the predictions for some surrounding
edges are used as features when estimating a pos-
sible edge between a dependent and head word.

Our parser also makes use of local classification
methods for training, but in contrast to Wang et
al. (2007) we take a pointwise approach based on
the assumption that edge scores can be estimated
independently. This work follows in the thread of
Liang et al. (2008) and Neubig et al. (2011), who
demonstrated that these assumptions can be made
without a significant degradation in accuracy for
POS tagging. Here we demonstrated that the same
approach can be used for MST-based dependency
parsing.

6 Conclusion

We introduced an MST parser that evaluates the
score for each edge in a dependency tree indepen-
dently, which allows for the use of partially an-
notated corpora in training. We demonstrated that
target domain data annotated in this way can be
combined with available general domain data to
increase parsing accuracy in the target domain.

We evaluated state-of-the-art dependency
parsers on a Japanese dependency parsing task,

0.840

0.850

0.860

0.870

0.880

0.890

0.900

baseline LAST PAREN FFS CF INFLECT FUNCT all

D
e

p
e

n
d

e
n

c
y

 A
c

c
u

ra
c

y

Dependency Conversion Rule

Parsing Accuracy on NKN-test

Figure 5: Parsing Accuracy on NKN-test.

and found that our parser achieves accuracy
comparable to that of a traditional MST parser.
Additionally, the training and parsing speed of
our parser is much faster than the traditional one,
which allows it to be used for active learning in a
real-world domain adaptation situation.

References
Adam L. Berger, Vincent J. Della Pietra, and

Stephen A. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Computa-
tional Linguistics, 22(1):39–71.

John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120–128.

Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149–164.

Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 167–202.

Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference rela-
tions. In Proceedings of the Linguistic Annotation
Workshop, pages 132–139.

Donald Keene, Hiroyoshi Hatori, Haruko Yamada, and
Shouko Irabu. 1992. Japanese-English Sentence
Equivalents (in Japanese). Asahi Press, Electronic
book edition.

Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of the Sixth Conference on Computational
Natural Language Learning, volume 25, pages 1–7.

783



Percy Liang, Hal Daumé III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th International Conference
on Machine Learning, pages 592–599.

Kikuo Maekawa. 2008. Balanced corpus of con-
temporary written Japanese. In Proceedings of the
6th Workshop on Asian Language Resources, pages
101–102.

David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In Proceedings of the 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 337–344.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajič. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the 2005 Conference on Empirical Methods in
Natural Language Processing, pages 523–530.

Yusuke Miyao, Kenji Sagae, Rune Saetre, Takuya
Matsuzaki, and Jun’ichi Tsujii. 2009. Evalu-
ating contributions of natural language parsers to
protein-protein interaction extraction. Bioinformat-
ics, 25(3):394–400.

Toshiaki Nakazawa and Sadao Kurohashi. 2008.
Linguistically-motivated tree-based probabilis-
tic phrase alignment. In In Proceedings of the
Eighth Conference of the Association for Machine
Translation in the Americas (AMTA2008).

Graham Neubig and Shinsuke Mori. 2010. Word-
based partial annotation for efficient corpus con-
struction. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation.

Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
Japanese morphological analysis. In The 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies Short
Paper Track.

Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of the 20th International Conference on Computa-
tional Linguistics.

Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of the Fifth In-
ternational Conference on Language Resources and
Evaluation.

Joachim Nivre, Johan Hall, Sandra Kübler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007.

Miles Osborne and Jason Baldridge. 2004. Ensemble-
based active learning for parse selection. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 89–96.

Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate de-
terministic question parsing. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 705–713.

Brian Roark and Michiel Bacchiani. 2003. Super-
vised and unsupervised PCFG adaptation to novel
domains. In Proceedings of the 2003 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 126–133.

Manabu Sassano and Sadao Kurohashi. 2010. Using
smaller constituents rather than sentences in active
learning for Japanese dependency parsing. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 356–365.

Kathrin Spreyer and Jonas Kuhn. 2009. Data-driven
dependency parsing of new languages using incom-
plete and noisy training data. In Proceedings of
the Thirteenth Conference on Computational Nat-
ural Language Learning (CoNLL-2009), pages 12–
20.

Kathrin Spreyer, Lilja Øvrelid, and Jonas Kuhn. 2010.
Training parsers on partial trees: a cross-language
comparison. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation.

Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 551–560.

Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002.
Active learning for statistical natural language pars-
ing. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics,
pages 120–127.

Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki
Oda, and Yuji Matsumoto. 2008. Training condi-
tional random fields using incomplete annotations.
In Proceedings of the 22th International Conference
on Computational Linguistics.

Qin Iris Wang, Dekang Lin, and Dale Schuurmans.
2007. Simple training of dependency parsers via
structured boosting. In Proceedings of the Twenti-
ethInternational Joint Conference on Artificial Intel-
ligence, pages 1756–1762.

Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical machine translation model. In Pro-
ceedings of the 39th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 523–530.

784


