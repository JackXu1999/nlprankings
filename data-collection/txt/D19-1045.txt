



















































Hierarchical Attention Prototypical Networks for Few-Shot Text Classification


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 476–485,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

476

Hierarchical Attention Prototypical Networks for
Few-Shot Text Classification

Shengli Sun1∗ Qingfeng Sun1∗ Kevin Zhou2 † Tengchao Lv1
1Peking University 2Microsoft

slsun@ss.pku.edu.cn {sunqingfeng, lvtengchao}@pku.edu.cn
kezhou@microsoft.com

Abstract

Most of the current effective methods for text
classification task are based on large-scale la-
beled data and a great number of parame-
ters, but when the supervised training data are
few and difficult to be collected, these mod-
els are not available. In this paper, we pro-
pose a hierarchical attention prototypical net-
works (HAPN) for few-shot text classifica-
tion. We design the feature level, word level,
and instance level multi cross attention for our
model to enhance the expressive ability of se-
mantic space. We verify the effectiveness of
our model on two standard benchmark few-
shot text classification datasets - FewRel and
CSID, and achieve the state-of-the-art perfor-
mance. The visualization of hierarchical atten-
tion layers illustrates that our model can cap-
ture more important features, words, and in-
stances separately. In addition, our attention
mechanism increases support set augmentabil-
ity and accelerates convergence speed in the
training stage.

1 Introduction

The dominant text classification models in deep
learning (Kim, 2014; Zhang et al., 2015a; Yang
et al., 2016; Wang et al., 2018) require a consider-
able amount of labeled data to learn a large num-
ber of parameters. However, such methods may
have difficulty in learning the semantic space in
the case that only few data are available. Few-shot
learning has became an effective approach to solve
this challenge, it can train a neural network with a
few parameters using few data but achieve good
performance. A typical example of this approach
is prototypical networks (Snell et al., 2017), which
averages the vector of few support instances as the
class prototype and computes distance between
target query and each prototype, then classify the
query to the nearest prototype’s class. However,

∗ They contribute equally to this work.
† Corresponding author.

prototypical networks is rough and does not con-
sider the adverse effects of various noises in the
data, which weakens the discrimination and ex-
pressiveness of the prototype.

In this paper, we propose a hierarchical atten-
tion prototypical networks for few-shot text clas-
sification by using attention mechanism in three
levels. For feature level attention, we use convo-
lutional neural networks to get the feature scores
which is different for various classes. For word
level attention, we adopt an attention mechanism
to learn the importance of each word hidden state
in an instance. For instance level multi cross atten-
tion, with the help of multi cross attention between
support set and target query, we can determine the
importance of different instances in the same class
and enable the model to get a more discriminative
prototype of each class.

In the actual scenario, we apply HAPN on in-
tention detection of our open domain chatbots with
different character. If we create a chatbot for old
people, the user intentions will focus on children,
health or expectation, so we can define specific
intentions and supply related responses. And be-
cause of only few data are needed, we can expand
the number of classes quickly. The model helps
chatbot to identify user intentions precisely, makes
the dialogue process smoother, more knowledge-
able and more controllable.

There are three main parts of our contribution:
first of all, we propose a hierarchical attention pro-
totypical networks for few-shot text classification,
then we achieve state-of-the-art performance on
FewRel and CSID datasets, and the experiments
prove our model is faster and more extensible.

2 Related Works

2.1 Text Classification

Text Classification is an important task in Natu-
ral Language Processing, and many models are



477

Figure 1: Hierarchical Attention Prototypical Networks architecture

proposed to solve it. The traditional methods
mainly focus on feature engineerings such as bag-
of-words or n-grams (Wang and Manning, 2012)
or SVMs (Tang et al., 2015). The neural network
based methods like Kim (2014) applies convolu-
tional neural networks for sentence classification.
Then, Johnson and Zhang (2015) use a one-hot
word order CNN, and Zhang et al. (2015b) ap-
ply a character level CNN. C-LSTM (Zhou et al.,
2015) combines CNN and RNN for sentence rep-
resentation and text classification. Yang et al.
(2016) explore the hierarchical structure of docu-
ments classification, they use a GRU-based atten-
tion to build representations of sentences and an-
other GRU-based attention to aggregate them into
a document representation. But above supervised
learning methods require large-scale labeled data
and can’t classify unseen classes.

2.2 Few-Shot Learning

Few-Shot Learning (FSL) aims to solve classifi-
cation problems by training a classifier with few
instances in each class, and it can apply to un-
seen classes. The early works aim to use transfer
learning approaches, Caruana (1994) and Bengio
(2011) adopt the target task from the pre-trained
models. Then Koch et al. (2015) explore a method
for learning siamese neural networks which em-
ploys an unique structure to rank similarity be-
tween inputs. Vinyals et al. (2016) use matching
networks to map a small labeled support set and

an unlabelled example to its label, and obviate the
need for fine-tuning to adapt to new class types.
Prototypical networks (Snell et al., 2017) learns a
metric space in which the model can perform well
by computing distance between query and proto-
type representations of each class and classify the
query to the nearest prototype’s class. Sung et al.
(2018) propose a two-branch relation networks,
which learns to compare query against few-shot
labeled sample support data. Dual TriNet struc-
ture (Chen et al., 2018) can efficiently and directly
augment multi-layer visual features to boost the
few-shot classification.But all of the above works
mainly concentrate on computer vision field, the
research and applications in NLP field are ex-
tremely limited. Recently, Yu et al. (2018) propose
an adaptive metric learning approach that automat-
ically determines the best weighted combination
from a set of metrics obtained from meta-training
tasks for a newly seen few-shot task such as in-
tention classification, Han et al. (2018) present
a relation classification dataset - FewRel, and
adapt most recent state-of-the-art few-shot learn-
ing methods for it, Gao et al. (2019) propose a
hybrid attention-based prototypical networks for
noisy few-shot relation classification. However,
these methods do not consider mining semantic
information or reducing the impact of noise more
precisely. And in most of the realistic settings, we
may increase the number of instances gradually,
so model capacity needs more attention.



478

3 Task Definition

In few-shot text classification task, our goal is to
learn a function : G(D,S,x) → y. D is the la-
beled data, we divide D into three parts: Dtrain,
Dvalidation, and Dtest, and each part has specific
label space. We use Dtrain to optimize parame-
ters, Dvalidation to select best hyper parameters,
and Dtest to evaluate the model.

The “episode” training strategy that Vinyals
et al. (2016) proposed has proved to be effective.
For each training episode, we first sample a label
set L from Dtrain, then use L to sample the sup-
port set S and the query set Q, finally, we feed S
and Q to the model and minimize the loss. If L
includes N different classes and each class of S
contains K instances, we call the target problem
N -way K-shot learning. For this paper, we con-
sider N = 5 or 10, and K = 5 or 10.

For exactly, in an episode, we are given a sup-
port set S

S ={(x11, l1), (x21, l1), . . . , (xn11 , l1),
⋯,
(x1m, lm), (x2m, lm), . . . , (xnmm , lm)},
l1, l2,⋯, lm ∈ L

(1)

consists of ni text instances for each class li ∈ L,
xji means it is the j support instance belonging
to calss li, and instance x

j
i includes Ti,j words

{w1,w2, . . . ,wTi,j}.
Then x is an unlabeled instance of query set Q

to classify, and y ∈ L is the output label followed
by the prediction of G.

4 Method

4.1 Model Overview

The overall architecture of the Hierarchical Atten-
tion Prototypical Networks is shown in Figure 1.
We introduce different components in the follow-
ing subsections:
Instance Encoder Each instance in support set or
query set will be first represented to a input vector
by transforming each word into embeddings. Con-
sidering the lightweight and speed of the model,
we achieve this part with one layer convolutional
neural networks (CNN). For ease of comparison,
its details are the same as Han et al. (2018) pro-
posed.
Hierarchical Attention In order to get more im-
portant information from rare data, we adopt a hi-

erarchical attention mechanism. Feature level at-
tention enhances or reduces the importance of dif-
ferent feature in each class, word level attention
highlight the important words for meaning of the
instance, and instance level multi cross attention
can extract the important support instances for dif-
ferent query instances, these three attention mech-
anisms work together to improve the classification
performance of our model.
Prototypical Networks Prototypical networks
compute a prototype vector as the representation
of each class, and this vector is the mean vector of
the embedded support instances belonging to its
class. We compare the distance between all proto-
type vectors and a target query vector, then clas-
sify this query to the nearest one.

4.2 Instance Encoder
The instance encoder part consists of two layers:
embedding layer and instance encoding layer.

4.2.1 Embedding Layer
Given an instance x = {wt,w2, . . . ,wT } with T
words. We use an embedding matrix WE ,wt =
WEwt to embed each word to a vector

{w1,w2, . . . ,wT },wt ∈ Rd (2)
where d is the word embedding dimension.

4.2.2 Encoding Layer
Following we apply a convolutional neural net-
work Zeng et al. (2014) as encoding layer to get
the hidden annotations of each word by a convolu-
tion kernel with the window size m

ht = CNN(wt−m−1
2
, . . . ,wt−m+1

2
) (3)

Especially, if the word wt has a position embed-
ding pt, we should concat wt and pt

wpt = [wt ⊕ pt] (4)
where ⊕ is a concatation, the ht will be as follow

ht = CNN(wpt−m−1
2
, . . . ,wpt−m+1

2
) (5)

Then, we aggregate all ht to get the overall rep-
resentation of instance x

x = {h1,h2, . . . ,ht} (6)
Finally, we define those two layers as a compre-

hensive function

x = gθ(x) (7)
θ in this function are the networks parameters to
be learned.



479

4.3 Prototypical Networks

The prototypical networks (Snell et al., 2017) has
achieved excellent performance in few-shot image
classification and few-shot text classification (Han
et al., 2018; Gao et al., 2019) tasks respectively, so
our model is based on prototypical networks and
aims to get promotion.

The fundamental idea of prototypical networks
is simple but efficient: we can use a prototype vec-
tor ci as the representative feature of class li, each
prototype vector can be calculated by averaging all
the embedded instances in its support set

ci =
1

ni

ni

∑
j=1

gθ(xji ) (8)

Then the probability distribution over the
classes in L can be produced by a softmax func-
tion over distances between all prototypes vector
and the target query q

pθ(y = li∣q) =
exp(−d(gθ(q),ci)

Σ
∣L∣
l=1 exp(−d(gθ(q),cl)

(9)

As Snell et al. (2017) mentioned, squared Eu-
clidean distance is a reasonable choice, however,
we will introduce a more effective method in sec-
tion 4.4.1, which combines squared Euclidean dis-
tance with class feature scores, and achieves defi-
nite improvement.

4.4 Hierarchical Attention

We focus on sentence-level text classification in
this work. The proposed model gets a feature
scores vector and transfers the support set of each
class into a vector representation, on which we
build a classifier to perform few-shot text classi-
fication.

4.4.1 Feature Level Attention
Obviously, the same dimension belonging to dif-
ferent classes has different importance when we
calculate the euclidean distance. In other words,
some feature dimensions are more discriminative
for distinguishing specific class in the feature level
space, and other features are confusing and useless
at the same time.

So we apply a CNN-based feature attention
mechanism similar to Gao et al. (2019) proposed
as a class feature extractor. It depends on all the
instances in the support set of each class and will
dynamiclly change with different classes.

Given a support set Si ∈ Rni×T×d of class li as
the output of above instance encoder part

Si = {x1,x2, . . . ,xni} (10)

we apply a max pooling layer over each instance
in Si to get a new feature map Sci ∈ Rni×d. Then
we use three convolution layers to obtain λi ∈ Rd,
which is the scores vector of class li. The specific
structure of above class feature extractor is shown
in Table 1.

layer name kernel size stride output size

pool T × 1 1 × 1 K × d × 1

conv 1
K × 1 1 × 1 K × d × 32

ReLU

conv 2
K × 1 1 × 1 K × d × 64

ReLU

conv 3
K × 1 K × 1 1 × d × 1

ReLU

Table 1: Class feature extractor architecture

So we get a new distance calculation method as
follow

d(ci,q
′) = (ci − q

′)2 ⋅λi (11)

where q
′

is the query vector passed through the
word level attention mechanism which will be in-
troduced in the next subsection.

4.4.2 Word Level Attention
The importance of different words to the meanings
of an instance is unequal, thus it is worth pointing
out which words are useful and which words are
useless. Therefore, we apply an attention mech-
anism (Yang et al., 2016) to get those important
words and assemble them to compose a more in-
formative instance vector sj , and the definitions
are as follows

ujt = tanh(Wwh
j
t + bw) (12)

vjt = u
j
t
⊺uw (13)

αjt =
exp(vjt )

Σt exp(vjt )
(14)

sj =∑
t

αjth
j
t (15)

where hjt is the t hidden word embedding of in-
stance xj , it was encoded through the instance en-
coder, and has the same hidden size with xj .



480

Firstly, the Ww and bw followed by activation
function tanh make up a MLP layer to transform
hjt to the new hidden representation u

j
t . Immedi-

ately, we apply a dot product operation between
ujt and a word level weight vector uw to compute
similarity vjt as the importance weight of u

j
t . Then

we use a softmax function to normalize vjt to α
j
t .

Finally, we calculate the instance level vector sj

through the weighted sum of αjt and h
j
t . As mem-

ory networks (Sukhbaatar et al., 2015) proposed,
uw can help us to select the important words in
each instance, it will be randomly initialized at the
beginning of the training stage, and be optimized
together with the networks parameters θ.

4.4.3 Instance Level Multi Cross Attention
The previous prototypical networks use the mean
vector of support instances as the class prototype.
Because of the diversity and lack of the support in-
stances, the gap between each support vector and
prototype maybe wide, meanwhile, different query
instances can be expressed in several ways, so not
every instance in a support set contributes equally
to the class prototype when they face a target query
instance. To highlight the importance of support
instances which are useful clues to classify a query
instance correctly, we propose a multi cross atten-
tion mechanism.

Given a support set S
′

i ∈ Rni×d for class li and a
query vector q

′ ∈ Rd, they are all encoded through
the instance encoder and word level attention. We
consider each support vector sji in S

′

i has its own
weight βji to query q

′
. So the formula (8) will be

rewritten as follow

ci =
ni

∑
j=1

βji s
j
i (16)

where we define rji = β
j
i s
j
i as the weighted proto-

type vector and the definitions of βji are as follows

βji =
exp(γji )

Σnij=1 exp(γ
j
i )

(17)

γji = sum{σ(fϕ(mca))} (18)
mca = [sjiφ ⊕ q

′

φ ⊕ τ 1 ⊕ τ 2] (19)

τ 1 = ∣sjiφ − q
′

φ∣,τ 2 = s
j
iφ ⊙ q

′

φ (20)

sjiφ = fφ(s
j
i ),q

′

φ = fφ(q
′) (21)

where fφ is a linear layer, ∣ ⋅ ∣ is element-wise abso-
lute value and ⊙ is element-wise product, we use

these two operation to get the difference informa-
tion τ 1 and τ 2 between s

j
i and q

′
, then concate-

nate them all as the multi cross attention informa-
tion mca, then fϕ(⋅) is a linear layer, σ(⋅) is a
tanh activation function, sum{⋅} means a sum op-
eration of all elements in the vector. Finally, γji is
the weight of j instance in support set si, and we
use a softmax function to nomalize it to βji .

Through the multi cross attention mechanism,
the prototype can pay more attention to those
query-related support instances and improve the
capacity of support set.

5 Experiments

In this section, we will introduce the experiment
results of our model. Firstly, we evaluate our
model on FewRel dataset and CSID dataset, and
achieve state-of-the-art results, our model outper-
forms the best baselines models by 1.11% and
1.64% respectively on 10 way 5 shot setting. Then
we will show how our model works by case study
and visualization of attention layers. We fur-
ther demonstrate that the hierarchical attention in-
creases the augmentability of support set and the
convergence speed of the model.

5.1 Datasets

FewRel Few-Shot Relation Classification (Han
et al., 2018) is a new large-scale supervised
dataset1. It consists of 70000 instances on 100 re-
lations derived from Wikipedia, and each relation
includes 700 instances. It also marks the head and
tail entities in each instance, and the average num-
ber of tokens is 24.99. FewRel has 64 relations for
training, 16 relations for validation, and 20 rela-
tions for test separately.
CSID Character Studio Intention Detection is a
dataset extracted from a real-world open domain
chatbot. In character studio platform, this chatbot
should transform its character style sometime so it
can adapt to different user group and environment,
thus dialog query intention detection turns into an
important task. CSID consists of 24596 instances
for 128 intentions, and each intention includes 30
to 260 instances, the average number of tokens in
each instance is 11.52. We use 80, 18 and 30 inten-
tions for training, validation, and test respectively.

1https://github.com/thunlp/FewRel



481

Model 5 Way 5 Shot 5 Way 10 Shot 10 Way 5 Shot 10 Way 10 Shot
Finetune 69.43 ± 0.30 71.56 ± 0.33 58.31 ± 0.26 62.12 ± 0.38
kNN 71.94 ± 0.33 73.20 ± 0.35 61.69 ± 0.36 66.49 ± 0.42
MetaN 79.46 ± 0.35 83.63 ± 0.38 70.49 ± 0.52 72.84 ± 0.69
GNN 80.42 ± 0.56 83.50 ± 0.63 63.08 ± 0.70 64.81 ± 0.85
SNAIL 78.64 ± 0.19 81.22 ± 0.23 69.46 ± 0.25 71.29 ± 0.27
Proto 83.79 ± 0.12 86.05 ± 0.10 75.25 ± 0.14 77.14 ± 0.19
PHATT 86.96 ± 0.05 87.56 ± 0.08 78.62 ± 0.11 80.94 ± 0.14
HAPN-FA 86.53 ± 0.07 87.05 ± 0.07 78.23 ± 0.09 80.73 ± 0.12
HAPN-WA 87.91 ± 0.09 87.83 ± 0.12 79.31 ± 0.10 81.06 ± 0.17
HAPN-IMCA 88.07 ± 0.09 88.96 ± 0.11 80.02 ± 0.12 81.87 ± 0.15
HAPN 88.45 ± 0.06 89.72 ± 0.08 80.26 ± 0.11 82.68 ± 0.13

Table 2: Accuracies (%) of different models on the CSID dataset on four different settings.

5.2 Baselines

Firstly, we compare our model with several tradi-
tional models such as Finetune and kNN, Then we
compare our model with five state-of-the-art few-
shot learning models based on neural networks,
they are MetaN (Munkhdalai and Yu, 2017), GNN
(Garcia and Bruna, 2018), SNAIL (Mishra et al.,
2018), Proto (Snell et al., 2017) and PHATT (Gao
et al., 2019) respectively.

5.3 Implementation details

We compare our models with seven baselines, and
the implementation details are as follows.

For FewRel dataset, we cite the results reported
by Snell et al. (2017) which includes Finetune,
kNN, MetaN, GNN, and SNAIL, then we cite the
results reported by Gao et al. (2019) which in-
cludes Proto and PHATT. For a fair comparison, in
our model, we use the same word embeddings and
hyperparameters of instance encoder as PHATT
proposed. In detail, we use the Glove (Penning-
ton et al., 2014) consisting of 6B tokens and 400K
vocabulary as our initialized word representation,
and each word has a 50 dimensions vector. In
addition, the position embedding dimension of a
word is 10, the max length of each instance is 40.
Finally, we evaluate all models on 5 way 5 shot
and 10 way 5 shot settings.

For CSID dataset, we implement all above
seven baseline models and our models. we use the
Baidu Encyclopedia (Li et al., 2018) as our initial-
ized word representation, it includes 745M tokens
and 5422K vocabulary, and each word has a 300d
dimensions vector, the max length of each instance
is 20. Finally, we evaluate all models on 5 way 5
shot, 5 way 10 shot, 10 way 5 shot and 10 way 10

shot settings.
For the Finetune and kNN baselines, they learn

the parameters on the support set with the CNN
encoder. For the neural networks based baselines,
we use the same hyper parameters as Han et al.
(2018) proposed.

For our hierarchical attention prototypical net-
works, the window size of the CNN instance en-
coder is 3, the dimension of the hidden layer is
230, the learning rate is 0.1, the learning rate de-
cay step is 3000 and the decay rate is 0.1. In addi-
tion, we train our model 12000 episodes and each
episode consists of 20 classes.

In order to study the effects of different compo-
nents, we refer to our models as HAPN-{FA,WA,
IMCA}, FA indicates feature level attention, WA
indicates word level attention and IMCA indicates
instance level multi cross attention.

5.4 Results and analysis

The experimental accuracies on CSID and FewRel
are shown in Tabel 2 and Table 4 respectively. In
this subsection, we will show the effects of hier-
archical attention and support set augmentability
of three Proto-based models and the convergence
speed comparison.

5.4.1 Effects of hierarchical attention

Benefit from hierarchical attention, our model
achieves excellent performance.

The case study of word level attention and in-
stance level multi cross attention are shown in Ta-
ble 3, this is a 2 way 3 shot task on FewRel dataset.
The query instance is an instance of “mother”
class in fact, and our model should classify it into
“mother” class or “child” class. It is a difficult



482

Class Word Attention IMCAS

Support Set

(1) mother Cherie Gil is the daughter of Filipino actors Eddie Mesa and Rosemarie Gil, and sister
of fellow actors, Michael de Mesa and the late Mark Gil.

When they reachedadulthood, Pelias and Neleus found their mother Tyro and then killed
her stepmother, Sidero, for having mistreated her.

It was here that the Queen Consort Jetsun Pema gave birth to a son on 5 February 2016,
Jigme Namgyel Wangchuck.

(2) child In 1421 Mehmed died and his son Murad II refused to honour his father’s obligations to
the Byzantines.

Henry Norreys was a lifelong friend of Queen Elizabeth and was the father of six sons,
who included Sir John Norreys, a famous English soldier.
Jim Henson and his son Brian were impressed enough with Barron’s style to offer him a
job directing the pilot episode of “The Storyteller”.

Query

(1) or (2) From 1922 to 1963, Princess Dagmar of Demark, the daughter of Frederick VIII of
Denmark and Louise of Sweden, lived on Kongestlund.

Table 3: Visualization of word level and instance level multi cross attention scores (IMCAS) for 2 way 3 shot
setting, the bold words are head entities and tail entities.

Model 5 Way 5 Shot 10 Way 5 Shot
Finetune∗ 68.66 ± 0.41 55.04 ± 0.31
kNN∗ 68.77 ± 0.41 55.87 ± 0.31
MetaN∗ 80.57 ± 0.48 69.23 ± 0.52
GNN∗ 81.28 ± 0.62 64.02 ± 0.77
SNAIL∗ 79.40 ± 0.22 68.33 ± 0.25
Proto◇ 89.05 ± 0.09 81.46 ± 0.13
PHATT◇ 90.12 ± 0.04 83.05 ± 0.05
HAPN-FA 89.79 ± 0.13 82.47 ± 0.20
HAPN-WA 90.86 ± 0.12 83.79 ± 0.19
HAPN-IMCA 90.92 ± 0.11 84.07 ± 0.19
HAPN 91.02 ± 0.11 84.16 ± 0.18

Table 4: Accuracies (%) for 5 way 5 shot and 10 way
5 shot settings on FewRel test set. ∗ reported by Han
et al. (2018) and ◇ reported by Gao et al. (2019).

task because of there are many similarities be-
tween the expressions of two classes. With the
help of word level attention, we highlight the im-
portance of the word “daughter”, which appears in
the query instance and the first support instance of
class “mother” at the same time, then this support
instance get the highest attention score and con-
tributes more to the prototype vector of “mother”
class, finally our model can classify the query in-
stance into the correct class in this confusing task.

As shown in Figure 2, by using the feature level
attention, we also get the feature attention scores
of “mother” class and “child” class respectively.
The features with high scores have deep color, and

the features with low scores have light color. Ob-
viously, different classes may have different fea-
ture score vector, in other words, the same fea-
ture of different classes have different importance.
So our feature level attention can highlight impor-
tance of the useful features and weaken the im-
portance of the noise features, then the distance
between the prototype vector and the query vector
will measure the difference between them more ef-
ficiently.

(a) Feature attention scores of “mother” class

(b) Feature attention scores of “child” class

Figure 2: Feature attention scores of different classes

We treat the final prototype embedding vector
as the features of each instance, then we can get
the distribution of features by principal pompo-
nent analysis in feature space as shown in Figure
3. As we can see, the instances without hierarchi-
cal attention are more distributed and may cross
with each other, but the instances with hierarchi-
cal attention are more centralized and discrimina-
tive, which proves that our model learns a better
semantic space, which helps to distinguish confus-



483

ing data..

(a) Instances without
hierarchical attention

(b) Instances with
hierarchical attention

Figure 3: Instances distribution of embedding vector
without hierarchical attention (a) and with hierarchical
attention (b). The left blue points marked × are in-
stances of “mother” class and the right orange points
marked ● are instances of “child” class.

5.4.2 Augmentability of support set
More support instances can contribute more useful
information to the prototype vector, meanwhile,
more noise will be added in.

In this section, we define the support set aug-
mentability (SSA) as the additive value of accu-
racy when we increase the same number of the
support set for different models. So we compare
our model’s SSA with other models such as Proto
and PHATT on the 10 way FewRel task, and the
shot number ranges from 5 to 25.

By using the hierarchical attention, our model
obtaines a strong robustness and can pay more at-
tention to the important information of support set
and reduce those negative effects of noisy data,
thus as shown in Figure 4, the support set aug-
mentability of our model is larger than other mod-
els. Benefit from the above advantages, we can
deploy our model in the cold start stage, and grad-
ually accumulate labeled support data in practical
applications, then improve the performance of the
model day by day, and thus improve the utilization
rate of few data in realistic settings.

5.4.3 Convergence speed comparison
At the training stage, we also compare the conver-
gence speed between Proto, PHATT, and HAPN
on the 10 way 5 shot and 10 way 15 shot FewRel
task. As shown in Figure 5, our model can be op-
timized more quickly than the other models. From
10 way 5 shot task to 10 way 15 shot settings, the
Proto model takes almost twice time to achieve
70% accuracy on validation set, in other words,
the convergence speed will decrease sharply when
we increase the number of support instances, but

5 10 15 20 25
Shot Number

76

78

80

82

84

86

Ac
cu

ra
cy

 (%
)

Accuracy on FewRel Validation Set
Proto
PHATT
HAPN

Figure 4: Support set augmentability of Proto, PHATT
and HAPN on FewRel validation set.

this problem can be effectively alleviated when we
use hierarchical attention mechanism.

0 500 1000 1500 2000 2500 3000
Iter

0.5

1.0

1.5

2.0

2.5

3.0
Lo

ss

Proto-Loss
PHATT-Loss
HAPN-Loss

20

30

40

50

60

70

80

Ac
cu

ra
cy

 (%
)

10 Way 5 Shot Loss and Accuracy on FewRel Dataset

Proto-Acc
PHATT-Acc
HAPN-Acc

Figure 5: Training Proto, PHATT and HAPN on
FewRel dataset. Lines marked 2 denote loss on the
training set and lines marked △ denote accuracy on the
validation set.

6 Conclusion

Previous few-shot learning models for text classi-
fication roughly apply text representations or ne-
glect the noisy information. We propose to do hi-
erarchical attention prototypical networks consist-
ing of feature level, word level and instance level
multi cross attention, which highlight the impor-
tant information of few data and learn a more dis-
criminative prototype representation. In the ex-
periments, our model achieves the state-of-the-
art performance on FewRel and CSID datasets.
HAPN not only increases support set augmentabil-
ity but also accelerates convergence speed in the
training stage.

In the future, we will contribute new text dataset
to few-shot learning, explore better feature extrac-



484

tor networks and do some industrial application.

Acknowledgements

We would like to thank Sawyer Zeng and Yue Liu
for providing valuable hardware support and use-
ful advice, and thank Xuexiang Xu and Yang Bai
for helping us test online FewRel dataset. This
work is also supported by the National Key Re-
search and Development Program of China (No.
2018YFB1402902 and No. 2018YFB1403002)
and the Natural Science Foundation of Jiangsu
Province (No. BK20151132).

References
Yoshua Bengio. 2011. Deep learning of representa-

tions for unsupervised and transfer learning. In Un-
supervised and Transfer Learning - Workshop held
at ICML 2011, Bellevue, Washington, USA, July 2,
2011, pages 17–36.

Rich Caruana. 1994. Learning many related tasks at
the same time with backpropagation. In Advances
in Neural Information Processing Systems 7, [NIPS
Conference, Denver, Colorado, USA, 1994], pages
657–664.

Zitian Chen, Yanwei Fu, Yinda Zhang, Yu-Gang Jiang,
Xiangyang Xue, and Leonid Sigal. 2018. Semantic
feature augmentation in few-shot learning. volume
abs/1804.05298.

Tianyu Gao, Zhiyuan Liu Xu Han, and Maosong Sun.
2019. Hybrid attention-based prototypical networks
for noisy few-shot relation classification. In Pro-
ceedings of the Association for the Advancement of
Artificial Intelligence.

Victor Garcia and Joan Bruna. 2018. Few-shot learn-
ing with graph neural networks. In Proceedings of
ICLR.

Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao,
Zhiyuan Liu, and Maosong Sun. 2018. Fewrel: A
large-scale supervised few-shot relation classifica-
tion dataset with state-of-the-art evaluation. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, Brussels, Bel-
gium, October 31 - November 4, 2018, pages 4803–
4809.

Rie Johnson and Tong Zhang. 2015. Effective use
of word order for text categorization with convolu-
tional neural networks. In NAACL HLT 2015, The
2015 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies, Denver, Colorado,
USA, May 31 - June 5, 2015, pages 103–112.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. arXiv:1408.5822.

Gregory Koch, Richard Zemel, and Ruslan Salakhut-
dinov. 2015. Siamese neural networks for one-shot
image recognition. In ICML Deep Learning work-
shop.

Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, and
Xiaoyong Du. 2018. Analogical reasoning on chi-
nese morphological and semantic relations. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 138–143. Association for Computa-
tional Linguistics.

Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and
Pieter Abbeel. 2018. A simple neural attentive meta-
learner. In 6th International Conference on Learn-
ing Representations, ICLR 2018, Vancouver, BC,
Canada, April 30 - May 3, 2018, Conference Track
Proceedings.

Tsendsuren Munkhdalai and Hong Yu. 2017. Meta
networks. In Proceedings of the 34th International
Conference on Machine Learning, ICML 2017, Syd-
ney, NSW, Australia, 6-11 August 2017, pages 2554–
2563.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1532–1543.

Jake Snell, Kevin Swersky, and Richard S. Zemel.
2017. Prototypical networks for few-shot learning.
In Advances in Neural Information Processing Sys-
tems 30: Annual Conference on Neural Information
Processing Systems 2017, 4-9 December 2017, Long
Beach, CA, USA, pages 4080–4090.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. Yara parser: A fast and ac-
curate dependency parser. End-to-end memory net-
works, arXiv:1503.08895.

Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang,
Philip H. S. Torr, and Timothy M. Hospedales. 2018.
Learning to compare: Relation network for few-shot
learning. In 2018 IEEE Conference on Computer Vi-
sion and Pattern Recognition, CVPR 2018, Salt Lake
City, UT, USA, June 18-22, 2018, pages 1199–1208.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document
modeling with gated recurrent neural network for
sentiment classification. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portugal,
September 17-21, 2015, pages 1422–1432.

Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray
Kavukcuoglu, and Daan Wierstra. 2016. Match-
ing networks for one shot learning. In Advances in
Neural Information Processing Systems 29: Annual



485

Conference on Neural Information Processing Sys-
tems 2016, December 5-10, 2016, Barcelona, Spain,
pages 3630–3638.

Guoyin Wang, Chunyuan Li, Wenlin Wang, Yizhe
Zhang, Dinghan Shen, Xinyuan Zhang, Ricardo
Henao, and Lawrence Carin. 2018. Joint embed-
ding of words and labels for text classification. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2018,
Melbourne, Australia, July 15-20, 2018, Volume 1:
Long Papers, pages 2321–2331.

Sida I. Wang and Christopher D. Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In The 50th Annual Meeting of the As-
sociation for Computational Linguistics, Proceed-
ings of the Conference, July 8-14, 2012, Jeju Island,
Korea - Volume 2: Short Papers, pages 90–94.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alexander J. Smola, and Eduard H. Hovy. 2016. Hi-
erarchical attention networks for document classifi-
cation. In NAACL HLT 2016, The 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, San Diego California, USA, June 12-
17, 2016, pages 1480–1489.

Mo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni
Potdar, Yu Cheng, Gerald Tesauro, Haoyu Wang,
and Bowen Zhou. 2018. Diverse few-shot text clas-
sification with multiple metrics. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL-HLT 2018,
New Orleans, Louisiana, USA, June 1-6, 2018, Vol-
ume 1 (Long Papers), pages 1206–1215.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via
convolutional deep neural network. In COLING
2014, 25th International Conference on Computa-
tional Linguistics, Proceedings of the Conference:
Technical Papers, August 23-29, 2014, Dublin, Ire-
land, pages 2335–2344.

Xiang Zhang, Junbo Jake Zhao, and Yann LeCun.
2015a. Character-level convolutional networks for
text classification. In Advances in Neural Infor-
mation Processing Systems 28: Annual Conference
on Neural Information Processing Systems 2015,
December 7-12, 2015, Montreal, Quebec, Canada,
pages 649–657.

Xiang Zhang, Junbo Jake Zhao, and Yann LeCun.
2015b. Character-level convolutional networks for
text classification. In Advances in Neural Infor-
mation Processing Systems 28: Annual Conference
on Neural Information Processing Systems 2015,
December 7-12, 2015, Montreal, Quebec, Canada,
pages 649–657.

Chunting Zhou, Chonglin Sun, Zhiyuan Liu, and Fran-
cis C. M. Lau. 2015. A C-LSTM neural network for
text classification. volume abs/1511.08630.


