















































Answer-focused and Position-aware Neural Question Generation


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3930–3939
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

3930

Answer-focused and Position-aware Neural Question Generation

Xingwu Sun1, Jing Liu1, Yajuan Lyu1, Wei He1, Yanjun Ma1, Shi Wang2
1Baidu Inc., Beijing, China

2Institute of Computing Technology Chinese Academy of Sciences, Beijing, China
{sunxingwu,liujing46,lvyajuan,hewei06,mayanjun02}@baidu.com

wangshi@ict.ac.cn

Abstract

In this paper, we focus on the problem of ques-
tion generation (QG). Recent neural network-
based approaches employ the sequence-to-
sequence model which takes an answer and its
context as input and generates a relevant ques-
tion as output. However, we observe two ma-
jor issues with these approaches: (1) The gen-
erated interrogative words (or question words)
do not match the answer type. (2) The model
copies the context words that are far from
and irrelevant to the answer, instead of the
words that are close and relevant to the an-
swer. To address these two issues, we propose
an answer-focused and position-aware neural
question generation model. (1) By answer-
focused, we mean that we explicitly model
question word generation by incorporating the
answer embedding, which can help generate
an interrogative word matching the answer
type. (2) By position-aware, we mean that we
model the relative distance between the con-
text words and the answer. Hence the model
can be aware of the position of the context
words when copying them to generate a ques-
tion. We conduct extensive experiments to ex-
amine the effectiveness of our model. The ex-
perimental results show that our model signif-
icantly improves the baseline and outperforms
the state-of-the-art system.

1 Introduction

The task of question generation (QG) aims to gen-
erate questions for a given text, and it can benefit
several real applications: (1) In the area of educa-
tion, QG can help generate questions for reading
comprehension materials (Du et al., 2017). (2) QG
can enable the machine to actively ask questions in
a dialogue system. (3) QG can also aid in the de-
velopment of question answering datasets (Duan
et al., 2017). Typically, QG includes two sub-
tasks: (1) what to say, i.e. determining the targets

(e.g. sentences, phrases or words) that should be
asked; (2) how to say, i.e. producing the surface-
form of the question. In this paper, we focus on
the sub-task of surface-form realization of ques-
tions by assuming the targets are given.

Previous work of QG can be classified into two
categories: rule-based and neural network-based.
Compared to the rule-based approach, the neural
network-based approach does not rely on hand-
crafted rules, and it is instead data-driven and
trainable in an end-to-end fashion. The recent re-
lease of large-scale machine reading comprehen-
sion datasets, e.g. SQuAD (Rajpurkar et al., 2016)
and MARCO (Nguyen et al., 2016), also drives the
development of neural question generation.

Recent neural question generation ap-
proaches (Du et al., 2017; Zhou et al., 2017)
employ sequence-to-sequence model that takes
an answer and its context as input and outputs
a relevant question. Zhou et al. (2017) further
enrich the sequence-to-sequence model with rich
features (e.g. answer position and lexical features)
to generate answer focused questions, and incor-
porate copy mechanism that allows to copy words
from the context when generating questions. To
the best of our knowledge, it achieve the best re-
sults on SQuAD dataset so far (Zhou et al., 2017).
In this paper, we implement this approach and
carefully study its generation results. Specifically,
we randomly sample 130 questions generated by
the approach, and manually judge their quality by
comparing them with the references. We find 54
out of 130 questions are ill generated, and we ob-
serve two major issues with the 54 questions: (1)
20 (37.04% errors) questions contain the question
words that do not match the answer type, though
the answer position feature has been incorporated.
Because the model does not pay much attention to
the answer that is a key to question word genera-
tion. Table 1 gives an example. A when-question



3931

Context: The tax collector who arrested him rose to higher political office , and Thoreau ’s essay
was not published until after the end of the Mexican War.
Answer: the end of the Mexican War
Question generated by the baseline: Why was Thoreau’s essay published ?
Reference: When was Thoreau’s essay published ?

Table 1: A bad case where the generated question word does not match the answer type. A when-question should
be triggered for answer “the end of the Mexican War”, while a why-question is generated by the baseline.

Context: This mechanism is still the leading theory today; however, a second theory suggests that
most cpdna is actually linear and replicates through homologous recombination.
Answer: homologous recombination
Question generated by the baseline: What is the leading theory today ?
Reference: How does the second theory say most cpdna replicates ?

Table 2: A bad case where the model copies the context words far away from and irrelevant to the answer. The
baseline copies “leading theory” that is far away from and unrelated to the answer “homologous recombination”,
but neglects the phrase “second theory” that is close and relevant to the answer.

should be triggered for answer “the end of the
Mexican War” while a why-question is generated
by the model. (2) 11 (20.37% errors) questions
copies the context words that are far from and
irrelevant to the answer, instead of the words that
are close and relevant to the answer. Because the
model is not aware of the positions of context
words. Table 2 gives an example. The baseline
model copies “leading theory” that is far away
from and unrelated to the answer “homologous
recombination”, but neglects the phrase “second
theory” that is close and relevant to the answer.

To address these two issues, we propose an
answer-focused and position-aware neural ques-
tion generation model. (1) By answer-focused,
we mean that we explicitly model question word
generation by incorporating the answer embed-
ding, which can help generate a question word
matching the answer type. (2) By position-aware,
we mean that we model the relative distance be-
tween the context words and the answer. The rela-
tive distance is encoded as position embedding, on
which a position-aware attention is generated. The
position-aware attention help the model copy the
context words that are relatively close and relevant
to the answer. We further conduct extensive exper-
iments on SQuAD and MARCO dataset to exam-
ine the effectiveness of the answer-focused model
and position-aware model, respectively. The ex-
perimental results show that the combination of

our proposed answer-focused model and position-
aware model significantly improves the baseline
and outperforms the state-of-the-art system.

The contributions of this paper can be summa-
rized as follows:
• We analyze the generation results by the state-

of-the-art neural model, and find two major is-
sues with the model: (a) the generated question
words do not match the answer type; (b) the
model copies the context words that are far from
and irrelevant to the answer.
• To deal with these two issues, we propose an

answer-focused and position-aware neural ques-
tion generation model.
• We conduct extensive experiments to examine

the effectiveness of our proposed model.

2 Related Work

Question Generation Previous work of QG can
be classified into two categories: rule-based and
neural network-based. Regardless of the approach
taken, QG usually includes two sub-tasks: (1)
what to say, i.e. selecting the targets that should be
asked. (2) how to say, i.e. formulating the struc-
ture of the question and producing the surface re-
alization. This is similar to other natural language
generation tasks. In this paper, we focus on the
second sub-task, i.e. surface-form realization of
questions by assuming the targets are given.

The rule-based approaches usually include the



3932

following steps: (1) Preprocess the given text by
applying natural language processing techniques,
including syntactic parsing, sentence simplifica-
tion and semantic role labeling. (2) Identify
the targets that should be asked by using rules
or semantic roles. (3) Generate questions using
transformation rules or templates. (4) Rank the
over generated questions by well-designed fea-
tures (Heilman and Smith, 2009, 2010; Chali and
Hasan, 2015). The major drawbacks of rule-based
approaches include: (1) they rely on rules or tem-
plates that are expensive to manually create; (2)
the rules or templates lack diversity; (3) the targets
that they can deal with are limited.

To tackle the issues of rule-based approaches,
the neural network-based approaches are applied
to the task of QG. The neural network-based ap-
proaches do not rely on hand-crafted rules, and
they are instead data driven and trainable in an
end-to-end fashion. Serban et al. (2016) firstly in-
troduce an encoder-decoder framework with atten-
tion mechanism to generate factoid questions for
the facts (i.e. each fact is a triple composed of a
subject, a predicate and an object) from FreeBase.
Du et al. (2017) introduce sequence-to-sequence
model with attention mechanism to generate ques-
tions for the text from SQuAD dataset, which con-
tains large-scale manually annotated triples com-
posed of question, answer and the context (i.e. the
passage). Zhou et al. (2017) enrich the sequence-
to-sequence model with rich features, e.g. answer
position and lexical features, and incorporate copy
mechanism that allows it to copy words from the
context when generating a question. Their ex-
periments show the effectiveness of the rich fea-
tures and the copy mechanism. Duan et al. (2017)
propose to combine templates and sequence-to-
sequence model. Specifically, they mine question
patterns from a question answering community
and apply sequence-to-sequence to generate ques-
tion patterns for a given text. Tang et al. (2017)
model question answering and question generation
as dual tasks. It helps generate better questions
when training these two tasks together.

In this paper, we observe two major issues with
the exiting neural models: (1) The generated ques-
tion words do not match the answer type, since the
models do not pay much attention to the answers
that are critical to generate question words. (2)
The model copies the context words that are far
from and irrelevant to the answer, instead of the

words that are close and relevant to the answer,
since the models are not aware the positions of
the context words. To address these two issues,
we propose an answer-focused and position-aware
neural question generation model. As to position-
aware models, Zeng et al. (2014); Zhang et al.
(2017) introduce position feature in the task of re-
lation extraction. They apply this feature to en-
code the relative distance to the target noun pairs.
In the task of QG, Zhou et al. (2017) apply BIO
scheme to label answer position, which is a weak
representation of relative distance between answer
and its context words.
Sequence-to-sequence In recent years, the
sequence-to-sequence model has been widely
used in the area of natural language generation,
including the tasks of abstractive text summa-
rization, response generation in dialogue, poetry
generation, etc. Sutskever et al. (2014) propose
a sequence-to-sequence model and apply it
to the task of machine translation. Bahdanau
et al. (2014) introduce attention mechanism to
the sequence-to-sequence model and it greatly
improves the model performance on the task
of machine translation. To deal with the out of
vocabulary issue, several variants of the sequence-
to-sequence model have been proposed to copy
words from source text (Gu et al., 2016; Gulcehre
et al., 2016; Cao et al., 2017; See et al., 2017).

3 Our Models

In this section, we describe the details of our
models. We first describe the baseline model, a
feature-enriched pointer-generator model. Then,
we elaborate the proposed answer-focused model
and position-aware model to deal with the two is-
sues discussed in previous section. Finally, a hy-
brid model is introduced to combine these two
models.

3.1 Baseline (Feature-enriched
Pointer-generator Model)

Our baseline model is an attention-based pointer-
generator model (See et al., 2017) enhanced with
various rich features proposed by (Zhou et al.,
2017). These features include: named entity (NE),
part-of-speech (POS) and answer position in the
embedding layer of the encoder.

The encoder in our baseline is a bidirec-
tional LSTM, which takes the joint embedding
of word, answer position and lexical features



3933

(NE, POS) as input (w1, w2, ..., wTx) with wi ∈
Rdw+da+dn+dp , where Tx is the input length and
dw, da, dn, dp is the dimensionality of word em-
bedding, answer position embedding, NE embed-
ding and POS embedding respectively. It pro-
duces a sequence of hidden states (h1, h2, ..., hTx)
to represent its input, each of which is a concate-
nation of a forward and a backward LSTM repre-
sentation:

hi = [
←−
h i;
−→
h i],

←−
h i = LSTM(wi,

←−
h i+1),

−→
h i = LSTM(wi,

−→
h i−1)

(1)

where
←−
h i,
−→
h i are all dh-dimensional vectors.

The decoder is a unidirectional LSTM condi-
tioned on all encoded hidden states. At decoding
step t, the decoder reads an input word embedding
wt, previous attentional context vector ct−1 and
its previous hidden state st−1 to update its current
hidden state st ∈ Rdh :

st = LSTM([wt; ct−1], st−1) (2)

The context vector ct together with an attention
distribution αt are generated via attention mecha-
nism (Bahdanau et al., 2014). Attention can be re-
garded as a semantic match between encoder hid-
den states and the decoder hidden state. It de-
scribes how the model spread out the amount it
cares about different encoder hidden states during
decoding. At step t, the context vector ct and the
attention distribution αt are calculated as follows:

ct =

Tx∑
i=1

αtihi (3)

αti = softmax(eti) (4)

eti = v
T tanh(W Th hi +W

T
s st + b) (5)

where Wh,Ws, b and v are all trainable parame-
ters.

Our baseline is based on a pointer-generator
framework, which includes two complementary
modes: a generation mode and a copy mode. The
former mode generates words from a given vocab-
ulary as the vanilla sequence-to-sequence model:

Pvocab = softmax (g(st, ct)) (6)

where g(·) is a two-layer feed-forward network
with a maxout internal activation. Pvocab ∈ R|V |

denotes the vocabulary distribution with a vocab-
ulary size of |V |.

The latter mode copies words directly from the
source sequence. As the attention weights already
measure the relevance of each input word to the
partial decoding state, we treat αt as the copy
probability i.e. Pcopy(w) = αt. Both modes are
switched via a generation probability pgen as fol-
lows:

P (w) = pgenPvocab(w)+(1−pgen)Pcopy(w) (7)

where pgen is computed from the context vector
ct, decoder hidden state st and the decoder input
wt:

pgen = σ(f(ct, st, wt)) (8)

f(·) indicates a simple feed-forward neural net-
work that emits a single scalar value. The whole
network is thus trained end-to-end according to the
negative log likelihood loss of target word prob-
ability P (w∗). In the baseline, we apply BIO
scheme to label answer position, where B,I,O de-
note the begin of an answer, the non-begin of
the answer and the non-answer context words re-
spectively. Besides, we introduce dropout (Sri-
vastava et al., 2014) with maxout (Goodfellow
et al., 2013) to tackle over-fitting problem and pre-
trained global vectors (Glove) for word represen-
tation (Pennington et al., 2014). All these tech-
niques have been verified in their effectiveness in
our model.

3.2 Answer-focused Model
The mismatch between generated question words
and answer type is a major issue in neural ques-
tion generation (NQG). Even though answer is a
key to question word generation, most NQG mod-
els do not focus on answer or weakly emphasis
on it when generating question words. As Ta-
ble 1 shows, a when-question should be triggered
for the answer “the end of the Mexican War”, but
an answer-irrelevant why-question is generated by
the baseline. According to our analysis in Section
1, we discover that nearly 37% bad cases from our
baseline fall into this category. To deal with this
issue, we develop an answer-focused model.

We observe that the generation of question
words is mainly related to the answer and its sur-
rounding words. For example in Table 1, the an-
swer and its context “until after the end of the
Mexican War” already involve the essential infor-
mation to generate a question word “when”. This



3934

Context 
Vector

Attention 
Distribution

Vocabulary 
Distribution

×𝑝𝑔𝑒𝑛𝑣

Decoder 
Hidden States

×

Final Distribution

×𝑝𝑔𝑒𝑛q
Question Word 

Distribution

Position-aware
Attention Distribution

Position

Position-aware
Context Vector

Forbidden    City            is            located          at           China

Word
Lexical Features

Answer Position Feature

Encoder Hidden States

</s>             Where 

𝑤1 𝑤2 𝑤3 𝑤4 𝑤5 𝑤6

×𝑝𝑐𝑜𝑝𝑦

A A

A

P

Figure 1: The modules marked with A© and colored yellow describe the answer-focused model, while the ones
marked with P© and colored green describe the position-aware model. In answer-focused model, comparing to
pointer-generator model, we introduce a question word generation mode and generate question words in a restricted
vocabulary of question words. In position-aware model, we incorporate word position embeddings to gain a
position-aware attention for further generation. The hybrid model has two types of attention distribution: position-
aware and non-position-aware attention distribution and two types of context vector accordingly. Question word
distribution is generated from non-position-aware context vector while vocabulary distribution is calculated from
position-aware one.

suggests that the answer and its context can bene-
fit the generation of question words. We also ob-
serve that the number of question words is limited.
Therefore, we introduce a specific vocabulary of
question words to directly and explicitly model the
generation of question words.

As depicted in Figure 1, comparing to pointer-
generator model, we introduce an additional ques-
tion word generation mode to generate question
words based on a restricted vocabulary of ques-
tion words. This mode produces a question word
distribution based on an answer embedding vans,
the decoder state st and the context vector ct:

Pquestion word = softmax (g(vans, st, ct)) (9)

where Pquestion word is a |Vqw|-dimensional prob-
ability distribution, and |Vqw| is the size of vocab-
ulary of question words. We employ the encoded
hidden state at the answer start position as the an-
swer embedding, i.e. vans = hanswer start. We
argue that under bidirectional encoding, this an-
swer embedding has already memorized both the
left and the right contexts around the answer re-

gion, making it a desired choice. To control the
balance among different modes, we introduce a 3-
dimensional switch probability:

pgenv, pgenq, pcopy = softmax(f(ct,st,wt)) (10)

The final probability distribution is calculated via
a weighted sum of the three mode probability dis-
tributions:

P (w) = pgenvPvocab(w) + pcopyPcopy(w)

+ pgenqPquestion word(w)
(11)

3.3 Position-aware Model
Another main issue of NQG is that the generated
question copies the context words that are distant
from and irrelevant to the answer, instead of the
words that are close and relevant to the answer. In
other words, attention is distracted by irrelevant
words far away from the answer. As shown in
Table 2, the baseline model copies “leading the-
ory” that are far away from and unrelated to the
answer “homologous recombination”, but neglects
the words “second theory” and “linear and repli-
cates through” that are closer to the answer. In



3935

Section 1, we analyze the bad cases of our base-
line, and find that about 20% suffer from this phe-
nomenon. Further analysis on these cases indi-
cates that the closer a word is to the answer, the
more likely it should be copied.

Based on these evidences, we believe that an
important reason for the above phenomenon lies
in the lack of word position information inside the
NQG model. Following this direction, we propose
a position-aware model. The model aims at en-
forcing local attention, and adapting the attention
weights so as to put more emphasis on answer-
surrounded context words by incorporating word
position embeddings.

A straightforward solution to inject position
information is to directly incorporate relative
word position embeddings. This can inform the
model where the answer is and what context
words are close to it. As depicted in Figure 1,
we achieve this by feeding position embeddings
(dp1 , dp2 , ..., dpTx ) into the computation of atten-
tion distribution through a single layer network:

eti = v
T tanh(Wddpi +Whhi+Wsst+ b) (12)

αti = softmax(eti) (13)

where pi is the relative distance between the i-th
word and the answer, dpi is the embedding of pi,
which we call word position embedding. By opti-
mizing the attention parameters, our model is ex-
pected to discover the correlation between target
words and their relative distance from the answer.
As a result, distracted attention on irrelevant input
words can be avoided.

3.4 A Hybrid Model (Answer-focused and
Position-aware)

In Section 3.2 and 3.3, we describe the answer-
focused model and position-aware model respec-
tively. In this section, we combine these two
models to get a both answer-focused and position-
aware hybrid model. As depicted in Figure 1, the
hybrid model generates two types of attention dis-
tribution: position-aware and non-position-aware
distribution. Accordingly, the model has two
types of context vector: position-aware and non-
position-aware context vector. At time step t, we
calculate position-aware attention distribution α′t
as equation (12), (13), and non-position-aware one

αt as equation (4), (5). And we use ct, c′t to repre-
sent non-position-aware and position-aware con-
text vector calculated as equation (3) respectively.

The hybrid model has three modes as in the
answer-focused model. The question word dis-
tribution is computed from non-position-aware at-
tention distribution as equation (9), while vocabu-
lary distribution is calculated from position-aware
one. The final distribution is weighted sum of the
three mode probability distributions:

Pvocab = softmax
(
g(st, c

′
t))
)

(14)

P (w) = pgenvPvocab(w) + pcopyPcopy(w)

+ pgenqPquestion word(w)
(15)

where Pcopy(w) is the position-aware attention
distribution α′t.

4 Experiments

4.1 Experiment Settings

Dataset In this paper, we conduct the experiments
on SQuAD and MARCO. Since the test sets of
both data sets are not publicly available, we follow
Zhou et al. (2017) to randomly split the develop-
ment set into two parts and use them as the devel-
opment set and test set for the task of question gen-
eration. In SQuAD, there are 86, 635, 8, 965 and
8, 964 question-answer pairs in our training set,
development set and test set, respectively. We di-
rectly use the extracted features 1 shared by Zhou
et al. (2017). In MARCO, there are 74, 097, 4, 539
and 4, 539 question-answer pairs in our training
set, development set and test set, respectively. We
use Stanford CoreNLP 2 to extract lexical features.
We attach all the processed data sets in the supple-
mental materials.
Implementation Details In this paper, we set the
cutoff length of the input sequence as 100 words.
The vocabulary contains the most frequent 20, 000
words in each training set. The vocabulary of
question words contains 20 words. We use the
pre-trained Glove word vectors 3 with 300 dimen-
sions to initialize the word embeddings that will
be further fine-tuned in the training stage. The
representations of answer position feature and lex-
ical features at the embedding layer of the encoder

1https://res.qyzhou.me/redistribute.
zip

2https://nlp.stanford.edu/software/
3http://nlp.stanford.edu/data/glove.

6B.zip



3936

DataSet SQuAD MARCO
Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 BLEU-1 BLEU-2 BLEU-3 BLEU-4
NQG++ (Zhou et al., 2017) - - - 13.29 - - - -
Pointer-generator model See et al. (2017) 32.32 18.04 12.06 8.60 42.40 29.37 20.71 15.16
Feature-enriched pointer-generator model 40.49 26.11 18.94 14.34 44.45 31.85 23.32 17.90
Answer-focused model 42.10 27.52 20.14 15.36 46.59 33.46 24.57 18.73
Position-aware model 42.16 27.37 20.00 15.23 47.16 34.20 24.40 18.19
Hybrid model 43.02 28.14 20.51 15.64 48.24 35.95 25.79 19.45

Table 3: The main experimental results of baselines, answer-focused model, position-aware model and a hybrid
model on SQuAD and MARCO.

are randomly initialized as 32 dimensional vectors
that are trainable during training stage. The hid-
den size of both the encoder and decoder is 512.
We use dropout only in the encoder with a dropout
rate 0.5. The size of answer embedding in answer-
focused model is 512. The position, that indicates
the relative distance between the context words
and the answer, ranges from 0 to 80 and its embed-
ding size in position-aware model is 16. We use
the optimization algorithm Adagrad (Duchi et al.,
2011) with learning rate 0.15, an initial accumu-
lator value of 0.1 and batch size as 64. We use
gradient clipping with a maximum gradient norm
of 2. During training, we select the best model on
development set.
Evaluation Metrics We report BLEU (Papineni
et al., 2002) as the main evaluation metric of the
question generation systems.
Baselines In the experiments, we have three base-
lines for comparisons:
• NQG++ (Zhou et al., 2017) It is the state-

of-the-art neural question generation system
on SQuAD that incorporates rich features to
the embedding layer of a sequence-to-sequence
model and introduces copy mechanism pro-
posed by Gulcehre et al. (2016).
• Pointer-generator model (See et al., 2017) It is

a sequence-to-sequence model with copy mech-
anism that has different architecture from the
one proposed by Gulcehre et al. (2016). We
choose this model since its copy mechanism
shows better performance (See et al., 2017).
Note that we do not enable the coverage mech-
anism in this model to have a fair comparison.
• Feature-enriched pointer-generator model

We add the features to the embedding layer of
the pointer-generator model as described in Sec-
tion 3.1.

4.2 Main Results
Table 3 shows the main results, and we have the
following observations:

• The feature-enriched pointer-generator model
outperforms NQG++. Both of the two models
employ the sequence-to-sequence model with
copy mechanism and the same features. The
major difference between them is that their
copy mechanism has different architecture, and
pointer-generator shows better performance.
• The pointer-generator model without the fea-

tures does not perform well. This verifies the
effectiveness of the features extracted by Zhou
et al. (2017).
• Both answer-focused model and position-aware

model outperform the feature-enriched pointer-
generator model and NQG++. We will analyze
the effectiveness of these two models in the fol-
lowing two sections, respectively.
• The hybrid model shows the best perfor-

mance and it outperforms the two single mod-
els, answer-focused model and position-aware
model.

4.3 Answer-focused Model Analysis

As we discussed in Section 1, one major issue with
the neural question generation model is that the
generated question word does not match the an-
swer type. The design of answer-focused model is
explicitly modeling question word generation by
incorporating answer embedding. It is expected
that the answer-focused model can reduce such er-
rors. Recall the example shown in Table 1 (Section
1). The answer-focused model correctly predicts
the question word, though it copies wrong context
words that can be further corrected by the hybrid
model. The outputs of the answer-focused model
and the hybrid model for the case in Table 1 are as
follows:

• Answer-focused model: When did Thoreau’s
essay come to higher political office ?
• Answer-focused + Position-aware model:

When was Thoreau’s essay published ?

We further evaluate different systems in terms



3937

of the same question word ratio (SQWR). This
metric measures the ratio of the generated ques-
tions that have the same question words as the ref-
erence. Table 4 shows the SQWR of different sys-
tems on SQuAD. We can see that answer-focused
model outperforms the strong baseline, feature-
enriched pointer-generator model.

Model SQWR
Pointer-generator model (See et al., 2017) 53.17%
Feature-enriched pointer-generator model 71.58%
Answer-focused model 73.91%

Table 4: The answer-focused model has the highest
same question word ratio.

We re-analyze the 20 (37% of errors) questions
(discussed in Section 1) that have the answer type
mismatching problem. Our answer-focused model
can correct 7 out of the 20 bad cases. All the re-
solved cases have a commonality that we can eas-
ily figure out the answer type from the answer it-
self, as the case shown in Table 1. We also analyze
the remaining 13 of the 20 unresolved cases. (1)
4 cases show that the answer type is closely re-
lated to the context words other than the answer.
But these context words are far from the answer,
and the encoding of the answer by LSTM has lit-
tle memory of them. As shown in Table 5, the
answer “an attempt to avoid responsibility for her
actions” is useless to generate a question word
“why”, while the useful context word “because”
is far from the answer. (2) 3 cases show that the
answer itself has ambiguity to generate the right
question words. (3) 3 cases contain the answers
with the wrong named entity labels. (4) 2 cases
contain answers which are out of vocabulary. (5)
1 case is hard even for human.

4.4 Position-aware Model Analysis

As discussed in Section 1, another major issue
with the neural question generation model is that
the model copies the context words that are far
from and irrelevant to the answer. The design
of position-aware model is tackling this issue by
modeling the relative distance between context
words and the answer, so that the attention distri-
bution for copy mechanism will be biased towards
the context words that are close and relevant to
the answer. Recall the example in Table 2 (Sec-
tion 1). The question generated by the baseline
copies the wrong context words “leading theory”
instead of “second theory” and “linear and repli-

Context: This action was upheld because , ac-
cording to the U.S. court of appeals for the first
circuit , her statement suggested a lack of re-
morse , an attempt to avoid responsibility for
her actions , and even a likelihood of repeating
her illegal actions .
Answer: an attempt to avoid responsibility
for her actions
Reference: Why is giving a defiant speech
sometimes more harmful for the individual ?
Question generated by the answer-focused
model: What did the court of appeals reject
?

Table 5: A bad case of baseline remains unresolved by
applying answer-focused model because answer type
is closely related to the context word “because” instead
of the answer itself, but “because” is far from the an-
swer. Thus, the encoding of answer has little memory
of “because”.

cates through”. The outputs of our position-aware
and hybrid model for this case are as follows.

• Position-aware model: The second theory
suggests that most cpdna is actually linear and
replicates through what ?
• Position-aware + Answer-focused model:

Most cpdna is actually linear and replicates
through what ?

We can observe that the model can copy the
correct context words after introducing the posi-
tion embedding to the attention distribution. Fig-
ure 2 illustrates the attention distributions for
copy mechanism before and after introducing po-
sition embedding of context words. We can see
that “second” has much higher probability in the
position-aware attention distribution.

0
0.1
0.2
0.3
0.4
0.5
0.6
0.7

Baseline(Feature-enriched pointer-generator model)

Position-aware model

Answer

Figure 2: The two attention distributions for copy
mechanism when generating questions for the case in
Table 2. The position-aware model emphasizes more
on the “second” that is close to the answer.

The design of position-aware model aims to
help copy the context words close and relevant to



3938

the answer. We analyze the effects of position-
aware model on copying out of vocabulary (OOV)
words from the source sequence. We measure the
effects from two perspectives: average precision
and average recall. For one generated sequence,
the precision is defined as the ratio of the num-
ber of OOV words appearing in both the gener-
ated question and the reference, and the number of
OOV words in the generated question. The recall
is defined as the ratio of the number of OOV words
appearing in both the generated question and the
reference, and the number of OOV words in the
reference. The average precision (AP) is the mean
of precision of all generated questions, and the av-
erage recall (AR) is the mean of recall of all gen-
erated questions. As Table 6 shows, our position-
aware model can significantly improve the AP and
AR, which indicates our position-aware model can
help on copying OOV.

Model AP AR
Feature-enriched pointer-generator model 21.22% 18.87%
Position-aware model 22.79% 20.62%

Table 6: Our position-aware model can significantly
improve the average precision and recall of copied
OOV.

5 Conclusion

In this paper, we find two major issues with the ex-
isting neural question generation model. To tackle
the two issues, we propose an answer-focused and
position-aware model. We further conduct exten-
sive experiments on SQuAD and MARCO dataset.
The experimental results show that the combina-
tion of our proposed answer-focused model and
position-aware model significantly improves the
baseline and outperforms the state-of-the-art sys-
tem.

6 Acknowledgments

This work is supported by the National Basic
Research Program of China (973 program, No.
2014CB340505). We thank the anonymous re-
viewers for their constructive criticism of the
manuscript.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Ziqiang Cao, Chuwei Luo, Wenjie Li, and Sujian Li.
2017. Joint copying and restricted generation for
paraphrase. In AAAI, pages 3152–3158.

Yllias Chali and Sadid A Hasan. 2015. Towards topic-
to-question generation. Computational Linguistics,
41(1):1–20.

Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn-
ing to ask: Neural question generation for reading
comprehension. arXiv preprint arXiv:1705.00106.

Nan Duan, Duyu Tang, Peng Chen, and Ming Zhou.
2017. Question generation for question answering.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
866–874.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12(Jul):2121–2159.

Ian J Goodfellow, David Warde-Farley, Mehdi Mirza,
Aaron Courville, and Yoshua Bengio. 2013. Maxout
networks. arXiv preprint arXiv:1302.4389.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. arXiv preprint
arXiv:1603.06393.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallap-
ati, Bowen Zhou, and Yoshua Bengio. 2016.
Pointing the unknown words. arXiv preprint
arXiv:1603.08148.

Michael Heilman and Noah A Smith. 2009. Question
generation via overgenerating transformations and
ranking. Technical report, CARNEGIE-MELLON
UNIV PITTSBURGH PA LANGUAGE TECH-
NOLOGIES INST.

Michael Heilman and Noah A Smith. 2010. Good
question! statistical ranking for question genera-
tion. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 609–617. Association for Computational Lin-
guistics.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine
reading comprehension dataset. arXiv preprint
arXiv:1611.09268.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.



3939

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250.

Abigail See, Peter J Liu, and Christopher D Man-
ning. 2017. Get to the point: Summarization
with pointer-generator networks. arXiv preprint
arXiv:1704.04368.

Iulian Vlad Serban, Alberto Garcı́a-Durán, Caglar
Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron
Courville, and Yoshua Bengio. 2016. Generating
factoid questions with recurrent neural networks:
The 30m factoid question-answer corpus. arXiv
preprint arXiv:1603.06807.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Duyu Tang, Nan Duan, Tao Qin, and Ming Zhou. 2017.
Question answering and question generation as dual
tasks. arXiv preprint arXiv:1706.02027.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2335–2344.

Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor An-
geli, and Christopher D Manning. 2017. Position-
aware attention and supervised data improve slot fill-
ing. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
pages 35–45.

Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan,
Hangbo Bao, and Ming Zhou. 2017. Neural ques-
tion generation from text: A preliminary study.
In National CCF Conference on Natural Language
Processing and Chinese Computing, pages 662–671.
Springer.


