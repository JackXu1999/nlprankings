



















































Decipherment of Substitution Ciphers with Neural Language Models


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 869–874
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

869

Decipherment of Substitution Ciphers with Neural Language Models

Nishant Kambhatla, Anahita Mansouri Bigvand, Anoop Sarkar
School of Computing Science

Simon Fraser University
Burnaby, BC , Canada

{nkambhat,amansour,anoop}@sfu.ca

Abstract

Decipherment of homophonic substitution ci-
phers using language models (LMs) is a well-
studied task in NLP. Previous work in this
topic scores short local spans of possible plain-
text decipherments using n-gram LMs. The
most widely used technique is the use of beam
search with n-gram LMs proposed by Nuhn
et al. (2013). We propose a beam search al-
gorithm that scores the entire candidate plain-
text at each step of the decipherment using a
neural LM. We augment beam search with a
novel rest cost estimation that exploits the pre-
diction power of a neural LM. We compare
against the state of the art n-gram based meth-
ods on many different decipherment tasks. On
challenging ciphers such as the Beale cipher
we provide significantly better error rates with
much smaller beam sizes.

1 Introduction

Breaking substitution ciphers recovers the plain-
text from a ciphertext that uses a 1:1 or homo-
phonic cipher key. Previous work using pre-
trained language models (LMs) for decipherment
use n-gram LMs (Ravi and Knight, 2011; Nuhn
et al., 2013). Some methods use the Expectation-
Maximization (EM) algorithm (Knight et al.,
2006) while most state-of-the-art approaches for
decipherment of 1:1 and homophonic substitution
ciphers use beam search and rely on the clever
use of n-gram LMs (Nuhn et al., 2014; Hauer
et al., 2014). Neural LMs globally score the en-
tire candidate plaintext sequence (Mikolov et al.,
2010). However, using a neural LM for decipher-
ment is not trivial because scoring the entire candi-
date partially deciphered plaintext is computation-
ally challenging. We solve both of these problems
in this paper and provide an improved beam search
based decipherment algorithm for homophonic ci-
phers that exploits pre-trained neural LMs for the
first time.

2 Decipherment Model

We use the notation from Nuhn et al. (2013). Ci-
phertext fN1 = f1..fi..fN and plaintext e

N
1 =

e1..ei..eN consist of vocabularies fi ∈ Vf and
ei ∈ Ve respectively. The beginning tokens in the
ciphertext (f0) and plaintext (e0) are set to “$” de-
noting the beginning of a sentence. The substi-
tutions are represented by a function φ : Vf →
Ve such that 1:1 substitutions are bijective while
homophonic substitutions are general. A cipher
function φwhich does not have every φ(f) fixed is
called a partial cipher function (Corlett and Penn,
2010). The number of fs that are fixed in φ is
given by its cardinality. φ′ is called an extension
of φ, if f is fixed in φ′ such that δ(φ′(f), φ(f))
yields true ∀f ∈ Vf which are already fixed in φ
where δ is Kronecker delta. Decipherment is then
the task of finding the φ for which the probability
of the deciphered text is maximized.

φ̂ = argmax
φ

p(φ(f1)...φ(fN )) (1)

where p(.) is the language model (LM). Find-
ing this argmax is solved using a beam search al-
gorithm (Nuhn et al., 2013) which incrementally
finds the most likely substitutions using the lan-
guage model scores as the ranking.

2.1 Neural Language Model
The advantage of a neural LM is that it can be used
to score the entire candidate plaintext for a hypoth-
esized partial decipherment. In this work, we use a
state of the art byte (character) level neural LM us-
ing a multiplicative LSTM (Radford et al., 2017).

Consider a sequence S = w1, w2, w3, ..., wN .
The LM score of S is SCORE(S):

P (S) = P (w1, w2, w3, ..., wN )

P (S) =

N∏
i=1

P (wi | w1, w2, ..., wi−1))

SCORE(S) = −
N∑
i=1

log(P (wi | w<i))

(2)



870

2.2 Beam Search

Algorithm 1 is the beam search algorithm (Nuhn
et al., 2013, 2014) for solving substitution ci-
phers. It monitors all partial hypotheses in lists
Hs and Ht based on their quality. As the
search progresses, the partial hypotheses are ex-
tended, scored with SCORE and appended to
Ht. EXT LIMITS determines which extensions
should be allowed and EXT ORDER picks the next
cipher symbol for extension. The search continues
after pruning: Hs ← HISTOGRAM_PRUNE(Ht).
We augment this algorithm by updating the
SCORE function with a neural LM.

Algorithm 1 Beam Search for Decipherment
1: function (BEAM SEARCH (EXT ORDER, EXT LIM-

ITS))
2: initialize sets Hs, Ht
3: CARDINALITY = 0
4: Hs.ADD((∅,0))
5: while CARDINALITY < |Vf | do
6: f = EXT ORDER[CARDINALITY]
7: for all φ ∈ Hs do
8: for all e ∈ Ve do
9: φ’ := φ ∪ {(e, f)}

10: if EXT LIMITS(φ’) then
11: Ht.ADD(φ’,SCORE(φ’))
12: HISTOGRAM PRUNE(Ht)
13: CARDINALITY = CARDINALITY + 1
14: Hs = Ht
15: Ht.CLEAR()
16: return WINNER(Hs)

3 Score Estimation (SCORE)

Score estimation evaluates the quality of the
partial hypotheses φ. Using the example
from Nuhn et al. (2014), consider the vo-
cabularies Ve = {a, b, c, d} and Vf =
{A,B,C,D}, extension order (B,A,C,D), and
ciphertext $ ABDDCABCDADCABDC $. Let φ =
{(a,A), (b, B))} be the partial hypothesis. Then
SCORE(φ) scores this hypothesized partial deci-
pherment (only A and B are converted to plain-
text) using a pre-trained language model in the hy-
pothesized plaintext language.

3.1 Baseline

The initial rest cost estimator introduced by Nuhn
et al. nuhnbeam computes the score of hypothe-
ses only based on partially deciphered text that
builds a shard of n adjacent solved symbols. As a
heuristic, n-grams which still consist of unsolved
cipher-symbols are assigned a trivial estimate of
probability 1. An improved version of rest cost es-

timation (Nuhn et al., 2014) consults lower order
n-grams to score each position.

3.2 Global Rest Cost Estimation

The baseline scoring method greatly relies on local
context, i.e. the estimation is strictly based on par-
tial character sequences. Since this depends solely
on the n-gram LM, the true conditional probabil-
ity under Markov assumption is not modeled and,
therefore, context dependency beyond the window
of (n− 1) is ignored. Thus, attempting to utilize a
higher amount of context can lower the probability
of some tokens resulting in poor scores.

We address this issue with a new improved ver-
sion of the rest cost estimator by supplement-
ing the partial decipherment φ(fN1 ) with predicted
plaintext text symbols using our neural language
model (NLM). Applying φ = {(a,A), (b, B))} to
the ciphertext above, we get the following partial
hypothesis:
φ(fN1 ) = $a1b2...a6b7..a10..a13b14..$

We introduce a scoring function that is able to
score the entire plaintext including the missing
plaintext symbols. First, we sample1 the plaintext
symbols from the NLM at all locations depend-
ing on the deciphered tokens from the partial hy-
pothesis φ such that these tokens maintain their re-
spective positions in the sequence, and at the same
time are sampled from the neural LM to fit (prob-
abilistically) in this context. Next, we determine
the probability of the entire sequence including the
scores of sampled plaintext as our rest cost esti-
mate.

NLM

In our running example, this would yield a score
estimation of the partial decipherment, φ(fN1 ) :

φ(fN1 ) = $ a1b2d3c4c5a6b7c8d9a10d11d12a13b14d15c16 $

Thus, the neural LM is used to predict the score of
the full sequence. This method of global scoring
evaluates each candidate partial decipherment by
scoring the entire message, augmented by the sam-

1The char-level sampling is done incrementally from left
to right to generate a sequence that contains the deciphered
tokens from φ at the exact locations they occur in the above
φ(fN1 ). If the LM prediction contradicts the hypothesized
decipherment we stop sampling and start from the next char-
acter.



871

Cipher Length UniqueSymbols
Obs/

symbol
Zodiac-408 408 54 7.55
Beale Pt. 2 763 180 4.23

Table 1: Homophonic ciphers used in our experiments.

pled plaintext symbols from the NLM. Since more
terms participate in the rest cost estimation with
global context, we use the plaintext LM to provide
us with a better rest cost in the beam search.

3.3 Frequency Matching Heuristic

Alignment by frequency similarity (Yarowsky and
Wicentowski, 2000) assumes that two forms be-
long to the same lemma when their relative fre-
quency fits the expected distribution. We use
this heuristic to augment the score estimation
(SCORE):

FMH(φ′) =

∣∣∣∣log(ν(f)ν(e)
)∣∣∣∣ f ∈ Vf , e ∈ Ve

(3)
ν(f) is the percentage relative frequency of the ci-
phertext symbol f , while ν(e) is the percentage
relative frequency of the plaintext token e in the
plaintext language model. The closer this value to
0, the more likely it is that f is mapped to e.

Thus given a φ with the SCORE(φ), the exten-
sion φ′ (Algo. 1) is scored as:
SCORE(φ′) = SCORE(φ) + NEW(φ′)− FMH(φ′)

(4)
where NEW is the score for symbols that have been
newly fixed in φ′ while extending φ to φ′. Our
experimental evaluations show that the global rest
cost estimator and the frequency matching heuris-
tic contribute positively towards the accuracy of
different ciphertexts.

4 Experimental Evaluation

We carry out 2 sets of experiments: one on let-
ter based 1:1, and another on homophonic sub-
stitution ciphers. We report Symbol Error Rate
(SER) which is the fraction of characters in the
deciphered text that are incorrect.

The character NLM uses a single layer multi-
plicative LSTM (mLSTM) (Radford et al., 2017)
with 4096 units. The model was trained for a sin-
gle epoch on mini-batches of 128 subsequences of
length 256 for a total of 1 million weight updates.
States were initialized to zero at the beginning of
each data shard and persisted across updates to
simulate full-backprop and allow for the forward
propagation of information outside of a given sub-

sequence. In all the experiments we use a charac-
ter NLM trained on English Gigaword corpus aug-
mented with a short corpus of plaintext letters of
about 2000 words authored by the Zodiac killer2.

4.1 1:1 Substitution Ciphers

In this experiment we use a synthetic 1:1 let-
ter substitution cipher dataset following Ravi and
Knight (2008), Nuhn et al. (2013) and Hauer et
al. (2014). The text is from English Wikipedia
articles about history3, preprocessed by stripping
the text of all images, tables, then lower-casing all
characters, and removing all non-alphabetic and
non-space characters. We create 50 cryptograms
for each length 16, 32, 64, 128 and 256 using a
random Caesar-cipher 1:1 substitution.

Length Beam SER(%) 1 SER(%) 2
64 100 4.14 4.14

1,000 1.09 1.04
10,000 0.08 0.12
100,000 0.07 0.07

128 100 7.31 7.29
1,000 1.68 1.55
10,000 0.15 0.09
100,000 0.01 0.02

Table 2: Symbol Error Rates (SER) based on Neural
Language Model and beam size (Beam) for solving
1:1 substitution ciphers of lengths 64 and 128, respec-
tively. SER 1 shows beam search with global scoring,
and SER 2 shows beam search with global scoring with
frequency matching heuristic.

Figure 1: Symbol error rates for decipherment of 1:1
substitution ciphers of different lengths. The beam size
is 100k. Beam 6-gram model uses the beam search
from Nunh et al. (2013).

2https://en.wikisource.org/wiki/Zodiac Killer letters
3http://en.wikipedia.org/wiki/History



872

!2

I H A V E D E P O S I T E D I N T H E C O U N T Y O

F B E D F O R D A B O U T F O U R M I L E S F R O M

B U F O R D S I N A N E X C A V A T I O N O R V A U

L T S I X F E E T B E L O W T H E S U R F A C E O F

T H E G R O U N D T H E F O L L O W I N G A R T I C

L E S B E L O N G I N G J O I N T L Y T O T H E P A

115 73 24 807 37 52 49 17 31 62 647 22 7 15 140 47 29 107 79 84 56 239 10 26 811 5

196 308 85 52 160 136 59 211 36 9 46 316 554 122 106 95 53 58 2 42 7 35 122 53 31 82

77 250 196 56 96 118 71 140 287 28 353 37 1005 65 147 807 24 3 8 12 47 43 59 807 45 316

101 41 78 154 1005 122 138 191 16 77 49 102 57 72 34 73 85 35 371 59 196 81 92 191 106 273

60 394 620 270 220 106 388 287 63 3 6 191 122 43 234 400 106 290 314 47 48 81 96 26 115 92

158 191 110 77 85 197 46 10 113 140 353 48 120 106 2 607 61 420 811 29 125 14 20 37 105 28

Figure 2: First few lines from part two of the Beale cipher. The letters have been capitalized.

Fig 1 plots the results of our method for cipher
lengths of 16, 32, 64, 128 and 256 alongside Beam
6-gram (the best performing model) model (Nuhn
et al., 2013)

4.2 An Easy Cipher: Zodiac-408

Zodiac-408, a homophonic cipher, is commonly
used to evaluate decipherment algorithms.

Beam SER (%) 1 SER (%) 2
10k 3.92 3.18
100k 2.40 1.91
1M 1.47 1.22

Table 3: Symbol Error Rates (SER) based on Neu-
ral Language Model and beam size (Beam) for deci-
phering Zodiac-408, respectively. SER 1 shows beam
search with global scoring, and SER 2 shows beam
search with global scoring with the frequency match-
ing heuristic.

Our neural LM model with global rest cost es-
timation and frequency matching heuristic with a
beam size of 1M has SER of 1.2% compared to
the beam search algorithm (Nuhn et al., 2013) with
beam size of 10M with a 6-gram LM which gives
an SER of 2%. The improved beam search (Nuhn
et al., 2014) with an 8-gram LM, however, gets 52
out of 54 mappings correct on the Zodiac-408 ci-
pher.

4.3 A Hard Cipher: Beale Pt 2

Part 2 of the Beale Cipher is a more challeng-
ing homophonic cipher because of a much larger
search space of solutions. Nunh et al. (2014) were
the first to automatically decipher this Beale Ci-
pher.

With an error of 5% with beam size of 1M vs
5.4% with 8-gram LM and a pruning size of 10M,
our system outperforms the state of the art (Nuhn
et al., 2014) on this task.

!1

I L I K E K I L L I N G P E O P L

E B E C A U S E I T I S S O M U C

H F U N I T I S M O R E F U N T H
A

A N K I L L I N G W I L D G A M E

I N T H E F O R T E S T B E C A U

S E M A N I S T H E M O S T D A N

G E R T U E A N A M A L O F A L L

Figure 3: First 119 characters from deciphered Zodiac-
408 text. The letters have been capitalized.

Beam SER (%) 1 SER (%) 2
10k 41.67 48.33

100k 7.20 10.09
1M 4.98 5.50

Table 4: Symbol Error Rates (SER) based on Neural
Language Model and beam size (Beam) for decipher-
ing Part 2 of the Beale Cipher. SER 1 shows beam
search with global scoring, and SER 2 shows beam
search with global scoring with the frequency match-
ing heuristic.

5 Related Work

Automatic decipherment for substitution ciphers
started with dictionary attacks (Hart, 1994; Jakob-
sen, 1995; Olson, 2007). Ravi and Knight (2008)
frame the decipherment problem as an integer lin-
ear programming (ILP) problem. Knight et al.
(2006) use an HMM-based EM algorithm for solv-
ing a variety of decipherment problems. Ravi and
Knight (2011) extend the HMM-based EM ap-
proach with a Bayesian approach, and report the



873

first automatic decipherment of the Zodiac-408 ci-
pher.

Berg-Kirkpatrick and Klein (2013) show that
a large number of random restarts can help the
EM approach.Corlett and Penn (2010) presented
an efficient A* search algorithm to solve letter
substitution ciphers. Nuhn et al. (2013) produce
better results in faster time compared to ILP and
EM-based decipherment methods by employing a
higher order language model and an iterative beam
search algorithm. Nuhn et al. (2014) present var-
ious improvements to the beam search algorithm
in Nuhn et al. (2013) including improved rest cost
estimation and an optimized strategy for order-
ing decipherment of the cipher symbols. Hauer
et al. (2014) propose a novel approach for solv-
ing mono-alphabetic substitution ciphers which
combines character-level and word-level language
model. They formulate decipherment as a tree
search problem, and use Monte Carlo Tree Search
(MCTS) as an alternative to beam search. Their
approach is the best for short ciphers.

Greydanus (2017) frames the decryption pro-
cess as a sequence-to-sequence translation task
and uses a deep LSTM-based model to learn the
decryption algorithms for three polyalphabetic ci-
phers including the Enigma cipher. However,
this approach needs supervision compared to our
approach which uses a pre-trained neural LM.
Gomez et al. (2018) (CipherGAN) use a genera-
tive adversarial network to learn the mapping be-
tween the learned letter embedding distributions
in the ciphertext and plaintext. They apply this
approach to shift ciphers (including Vigenère ci-
phers). Their approach cannot be extended to ho-
mophonic ciphers and full message neural LMs as
in our work.

6 Conclusion

This paper presents, to our knowledge, the first
application of large pre-trained neural LMs to
the decipherment problem. We modify the beam
search algorithm for decipherment from Nuhn et
al. (2013; 2014) and extend it to use global scor-
ing of the plaintext message using neural LMs.
To enable full plaintext scoring we use the neural
LM to sample plaintext characters which reduces
the beam size required. For challenging ciphers
such as Beale Pt 2 we obtain lower error rates with
smaller beam sizes when compared to the state of
the art in decipherment for such ciphers.

Acknowledgments

We would like to thank the anonymous reviewers
for their helpful remarks. The research was also
partially supported by the Natural Sciences and
Engineering Research Council of Canada grants
NSERC RGPIN-2018-06437 and RGPAS-2018-
522574 and a Department of National Defence
(DND) and NSERC grant DGDND-2018-00025
to the third author.

References
Taylor Berg-Kirkpatrick and Dan Klein. 2013. Deci-

pherment with a million random restarts. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 874–
878.

Eric Corlett and Gerald Penn. 2010. An exact A*
method for deciphering letter-substitution ciphers.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1040–1047. Association for Computational Linguis-
tics.

Aidan N. Gomez, Scng Huang, Ivan Zhang, Bryan M.
Li, Muhammad Osama, and ukasz Kaiser. 2018.
Unsupervised cipher cracking using discrete gans.
arXiv preprint arXiv:1801.04883.

Sam Greydanus. 2017. Learning the enigma
with recurrent neural networks. arXiv preprint
arXiv:1708.07576.

George W Hart. 1994. To decode short cryptograms.
Communications of the ACM, 37(9):102–108.

Bradley Hauer, Ryan Hayward, and Grzegorz Kondrak.
2014. Solving substitution ciphers with combined
language models. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: Technical Papers, pages 2314–2325.

Thomas Jakobsen. 1995. A fast method for cryptanaly-
sis of substitution ciphers. Cryptologia, 19(3):265–
274.

Kevin Knight, Anish Nair, Nishit Rathod, and Kenji
Yamada. 2006. Unsupervised analysis for deci-
pherment problems. In Proceedings of the COL-
ING/ACL on Main conference poster sessions, pages
499–506. Association for Computational Linguis-
tics.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan
Černockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In
Eleventh Annual Conference of the International
Speech Communication Association.



874

Malte Nuhn, Julian Schamper, and Hermann Ney.
2013. Beam search for solving substitution ciphers.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1568–1576.

Malte Nuhn, Julian Schamper, and Hermann Ney.
2014. Improved decipherment of homophonic ci-
phers. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1764–1768.

Edwin Olson. 2007. Robust dictionary attack of
short simple substitution ciphers. Cryptologia,
31(4):332–342.

Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
2017. Learning to generate reviews and discovering
sentiment. arXiv preprint arXiv:1704.01444.

Sujith Ravi and Kevin Knight. 2008. Attacking de-
cipherment problems optimally with low-order n-
gram models. In proceedings of the conference on
Empirical Methods in Natural Language Process-
ing, pages 812–819. Association for Computational
Linguistics.

Sujith Ravi and Kevin Knight. 2011. Bayesian infer-
ence for zodiac and other homophonic ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 239–247. As-
sociation for Computational Linguistics.

David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of the 38th
Annual Meeting on Association for Computational
Linguistics, pages 207–216. Association for Com-
putational Linguistics.


