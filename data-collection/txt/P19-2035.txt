



















































Attention and Lexicon Regularized LSTM for Aspect-based Sentiment Analysis


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 253–259
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

253

Attention and Lexicon Regularized LSTM for Aspect-based Sentiment
Analysis

Lingxian Bao
Universitat Pompeu Fabra

lingxian.bao@upf.edu

Patrik Lambert
Universitat Pompeu Fabra
patrik.lambert

@upf.edu

Toni Badia
Universitat Pompeu Fabra
toni.badia@upf.edu

Abstract
Attention based deep learning systems have
been demonstrated to be the state of the art
approach for aspect-level sentiment analysis,
however, end-to-end deep neural networks
lack flexibility as one can not easily adjust the
network to fix an obvious problem, especially
when more training data is not available: e.g.
when it always predicts positive when seeing
the word disappointed. Meanwhile, it is less
stressed that attention mechanism is likely to
“over-focus” on particular parts of a sentence,
while ignoring positions which provide key
information for judging the polarity. In this
paper, we describe a simple yet effective ap-
proach to leverage lexicon information so that
the model becomes more flexible and robust.
We also explore the effect of regularizing at-
tention vectors to allow the network to have a
broader “focus” on different parts of the sen-
tence. The experimental results demonstrate
the effectiveness of our approach.

1 Introduction

Sentiment analysis (also called opinion mining)
has been one of the most active fields in NLP due
to its important value to business and society. It
is the field of study that tries to extract opinion
(positive, neutral, negative) expressed in natural
languages. Most sentiment analysis works have
been carried out at document level (Pang et al.,
2002; Turney, 2002) and sentence level (Wilson
et al., 2004), but as opinion expressed by words is
highly context dependent, one word may express
opposite sentiment under different circumstances.
Thus aspect-level sentiment analysis (ABSA) was
proposed to address this problem. It finds the po-
larity of an opinion associated with a certain as-
pect, such as food, ambiance, service, or price in a
restaurant domain.

Although deep neural networks yield significant
improvement across a variety of tasks compared to

previous state of the art methods, end-to-end deep
learning systems lack flexibility as one cannot eas-
ily adjust the network to fix an obvious problem:
e.g. when the network always predicts positive
when seeing the word disappointed, or when the
network is not able to recognize the word dun-
geon as an indication of negative polarity. It could
be even trickier in a low-resource scenario where
more labeled training data is simply not avail-
able. Moreover, it is less stressed that attention
mechanism is likely to over-fit and force the net-
work to “focus” too much on a particular part of
a sentence, while in some cases ignoring positions
which provide key information for judging the po-
larity. In recent studies, both Niculae and Blon-
del (2017) and Zhang et al. (2019) proposed ap-
proaches to make the attention vector more sparse,
however, it would only encourage the over-fitting
effect in such scenario.

In this paper, we describe a simple yet effec-
tive approach to merge lexicon information with
an attention LSTM model for ABSA in order to
leverage both the power of deep neural networks
and existing linguistic resources, so that the frame-
work becomes more flexible and robust without
requiring additional labeled data. We also explore
the effect of regularizing attention vectors by in-
troducing an attention regularizer to allow the net-
work to have a broader “focus” on different parts
of the sentence.

2 Related works

ABSA is a fine-grained task which requires the
model to be able to produce accurate prediction
given different aspects. As it is common that
one sentence may contain opposite polarities as-
sociated to different aspects at the same time,
attention-based LSTM (Wang et al., 2016) was
first proposed to allow the network to be able to as-



254

sign higher weights to more relevant words given
different aspects. Following this idea, a number of
researches have been carried out to keep improv-
ing the attention network for ABSA (Ma et al.,
2017; Tay et al., 2017; Cheng et al., 2017; He et al.,
2018; Zhu and Qian, 2018).

On the other hand, a lot of works have been
done focusing on leveraging existing linguistic re-
sources such as sentiment to enhance the perfor-
mance; however, most works are performed at
document and sentence level. For instance, at
document level, Teng et al. (2016) proposed a
weighted-sum model which consists of represent-
ing the final prediction as a weighted sum of the
network prediction and the polarities provided by
the lexicon. Zou et al. (2018) described a frame-
work to assign higher weights to opinion words
found in the lexicon by transforming lexicon po-
larity to sentiment degree.

At sentence level, Shin et al. (2017) used two
convolutional neural networks to separately pro-
cess sentence and lexicon inputs. Lei et al. (2018)
described a multi-head attention network where
the attention weights are jointly learned with lexi-
con inputs. Wu et al. (2018) proposed a new label-
ing strategy which breaks a sentence into clauses
by punctuation to produce more lower-level ex-
amples, inputs are then processed at different lev-
els with linguistic information such as lexicon and
POS, and finally merged back to perform sentence
level prediction. Meanwhile, some other similar
works that incorporate linguistic resources for sen-
timent analysis have been carried out (Rouvier and
Favre, 2016; Qian et al., 2017).

Regarding the attention regularization, instead
of using softmax and sparesmax, Niculae and
Blondel (2017) proposed fusemax as a regularized
attention framework to learn the attention weights;
Zhang et al. (2019) introducedLmax andEntropy
as regularization terms to be jointly optimized
with the loss. However, both approaches share the
same idea of shaping the attention weights to be
sharper and more sparse so that the advantage of
the attention mechanism is maximized.

In our work, different from the previously men-
tioned approaches, we incorporate polarities ob-
tained from lexicons directly into the attention-
based LSTM network to perform aspect-level sen-
timent analysis, so that the model improves in
terms of robustness without requiring extra train-
ing examples. Additionally, we find that the at-

tention vector is likely to over-fit which forces the
network to “focus” on particular words while ig-
noring positions that provide key information for
judging the polarity; and that by adding lexical
features, it is possible to reduce this effect. Fol-
lowing this idea, we also experimented reducing
the over-fitting effect by introducing an attention
regularizer. Unlike previously mentioned ideas,
we want the attention weights to be less sparse.
Details of our approach are in following sections.

3 Methodology

3.1 Baseline AT-LSTM
In our experiments, we replicate AT-LSTM pro-
posed by Wang et al. (2016) as our baseline sys-
tem. Comparing with a traditional LSTM network
(Hochreiter and Schmidhuber, 1997), AT-LSTM is
able to learn the attention vector and at the same
time to take into account the aspect embeddings.
Thus the network is able to assign higher weights
to more relevant parts of a given sentence with re-
spect to a specific aspect.

Formally, given a sentence S, let
{w1, w2, ..., wN} be the word vectors of each
word where N is the length of the sentence;
va ∈ Rda represents the aspect embeddings where
da is its dimension; let H ∈ Rd×N be a matrix of
the hidden states {h1, h2, ..., hN ∈ Rd} produced
by LSTM where d is the number of neurons of
the LSTM cell. Thus the attention vector α is
computed as follows:

M = tanh(

[
WhH

Wvva ⊗ eN

]
)

α = softmax(wTM)

r = HαT

where, M ∈ R(d+da)×N , α ∈ RN , r ∈ Rd,Wh ∈
Rd×d,Wv ∈ Rda×da , w ∈ Rd+da . α is a vector
consisting of attention weights and r is a weighted
representation of the input sentence with respect
to the input aspect. va⊗ eN = [va, va, ..., va], that
is, the operator repeatedly concatenates va for N
times. Then, the final representation is obtained
and fed to the output layer as below:

h∗ = tanh(Wpr +WxhN )

ŷ = softmax(Wsh
∗ + bs)

where, h∗ ∈ Rd, Wp and Wx are projection pa-
rameters to be learned during training; Ws and bs



255

are weights and biases in the output layer. The
prediction ŷ is then plugged into the cross-entropy
loss function for training, and L2 regularization is
applied.

loss = −
∑
i

yilog(ŷi) + λ‖Θ‖22 (1)

where i is the number of classes (three way clas-
sification in our experiments); λ is the hyper-
parameter for L2 regularization; Θ is the regular-
ized parameter set in the network.

3.2 ATLX

LSTM

w
1

h1
va

LSTM

w
2

h2
va

LSTM

w
3

h3
va

LSTM

w
N

hN
va

...

alpha

l1 l2 l3 lN

r
l

Word
Embeddings

H

Aspect
Embeddings

Attention

L

Figure 1: ATLX model diagram

3.2.1 Lexicon Build
Similar to Shin et al. (2017), but in a different way,
we build our lexicon by merging 4 existing lex-
icons to one: MPQA, Opinion Lexicon, Opener
and Vader. SentiWordNet was in the initial design
but was removed from the experiments as unnec-
essary noise was introduced, e.g. highly is an-
notated as negative. For categorical labels such as
negative, weakneg, neutral, both, positive, we con-
vert them to values in {−1.0,−0.5, 0.0, 0.0, 1.0}
respectively. Regarding lexicons with real value
annotations, for each lexicon, we adopt the an-
notated value standardized by the maximum po-
larity in that lexicon. Finally, the union U of
all lexicons is taken where each word wl ∈ U
has an associated vector vl ∈ Rn that repre-
sents the polarity given by each lexicon. n here
is the number of lexicons; average values across

all available lexicons are taken for missing values.
e.g. the lexical feature for word adorable is rep-
resented by [1.0, 1.0, 1.0, 0.55], which are taken
from MPQA(1.0), Opener(1.0), Opinion Lexi-
con(1.0) and Vader(0.55) respectively. For words
outside U , a zero vector of dimension n is sup-
plied.

3.2.2 Lexicon Integration
To merge the lexical features obtained from U into
the baseline, we first perform a linear transforma-
tion to the lexical features in order to preserve the
original sentiment distribution and have compat-
ible dimensions for further computations. Later,
the attention vector learned as in the baseline is
applied to the transformed lexical features. In the
end, all information is added together to perform
the final prediction.

Formally, let Vl ∈ Rn×N be the lexical matrix
for the sentence, Vl then is transformed linearly:

L = Wl · Vl

where L ∈ Rd×N ,Wl ∈ Rd×n. Later, the atten-
tion vector learned from the concatenation of H
and va ⊗ eN is applied to L:

l = L · αT

where l ∈ Rd, α ∈ RN . Finally h∗ is updated and
passed to output layer for prediction:

h∗ = tanh(Wpr +WxhN +Wll)

where Wl ∈ Rd×d is a projection parameter as
Wp and Wx. The model architecture is shown in
Figure 1.

3.3 Attention Regularization
As observed in both Figure 2 and Figure 3, the at-
tention weights in ATLX seem less sparse across
the sentence, while the ones in the baseline are fo-
cusing only on the final part of the sentence. It is
reasonable to think that the attention vector might
be over-fitting in some cases, causing the network
to ignore other relevant positions, since the atten-
tion vector is learned purely on training examples.
Thus we propose a simple attention regularizer to
further validate our hypothesis, which consists of
adding into the loss function a parameterized stan-
dard deviation or negative entropy term for the at-
tention weights. The idea is to avoid the attention
vector to have heavy weights in few positions, in-
stead, it is preferred to have higher weights for



256

more positions. Formally, the attention regular-
ized loss is computed as:

loss = −
∑
i

yilog(ŷi) + λ‖Θ‖22 + � ·R(α) (2)

compared to equation (1), a second regularization
term is added, where � is the hyper-parameter for
the attention regularizer; R stands for the regular-
ization term defined in (3) or (4); and α is the dis-
tribution of attention weights. Note that during
implementation, the attention weights for batch
padding positions are excluded from α.

We experiment two different regularizers, one
uses standard deviation of α defined in equation
(3); the other one uses the negative entropy of α
defined in equation (4).

R(α) = σ(α) (3)

R(α) = −[−
N∑
i

αi · log(αi)] (4)

4 Experiments

Figure 2: Comparison of attention weights between
baseline and ATLX; The rows annotated as ”Lexicon”
indicates the average polarity per word given by U .

4.1 Dataset
Same as Wang et al. (2016), we experiment on
SemEval 2014 Task 4, restaurant domain dataset.
The data consists of reviews of restaurants with
aspects: {food, price, service, ambience, miscel-
laneous} and associated polarities: {positive, neu-
tral, negative}. The objective is to predict the po-
larity given a sentence and an aspect. There are

Pos Neu Neg In Corpus
MPQA 2298 440 4148 908
OL 2004 3 4780 732
Opener 2298 440 4147 908
Vader 3333 0 4170 656
Merged U 5129 404 7764 1234

Table 1: Lexicon statistics of positive, neutral, negative
words and number of words covered in corpus.

3,518 training examples and 973 test examples in
the corpus. To initialize word vectors with pre-
trained word embeddings, the 300 dimensional
Glove vectors trained on 840b tokens are used, as
described in the original paper.

4.2 Lexicons

As shown in Table 1, we merge four existing and
online available lexicons into one. The merged
lexicon U as described in section 3.2.1 is used
for our experiments. After the union, the fol-
lowing postprocess is carried out: {bar, try, too}
are removed from U since they are unreason-
ably annotated as negative by MPQA and Opener;
{n′t, not} are added to U with −1 polarity for
negation.

4.3 Evaluation

Cross validation is applied to measure the perfor-
mance of each model. In all experiments, the train-
ing set is randomly shuffled and split into 6 folds
with a fixed random seed. According to the code
released by Wang et al. (2016), a development set
containing 528 examples is used, which is roughly
1
6 of the training corpus. In order to remain faithful
to the original implementation, we thus evaluate
our model with a cross validation of 6 folds.

As shown in Table 2, compared to the baseline
system, ATLX is not only able to improve in terms
of accuracy, but also the variance of the perfor-
mance across different sets gets significantly re-
duced. On the other hand, by adding attention
regularization to the baseline system without in-
troducing lexical features, both the standard devi-
ation regularizer (basestd) and the negative entropy
regularizer (baseent) are able to contribute posi-
tively; where baseent yields largest improvement.
By combining attention regularization and lexical
features together, although the model is able to fur-
ther improve, the difference is too small to draw
strong conclusion.



257

Figure 3: Comparison of attention weights between baseline, basestd, baseent and ATLX.

CV σCV TEST σTEST

base 75.27 1.420 81.48 1.157
basestd 74.67 1.688 81.57 0.915
baseent 75.93 1.467 82.24 0.863
ATLX 75.64 1.275 82.62 0.498
ATLXstd 75.64 1.275 82.68 0.559
ATLXent 75.53 1.265 82.86 1.115
ATLX* 74.99 1.638 82.03 1.409
baseLX 71.98 1.588 79.24 2.322

Table 2: Mean accuracy and standard deviation of cross
validation results on 6 folds of development sets and
one test set. Note that in our replicated baseline sys-
tem, test accuracy ranges from 80.06 to 83.45; 83.1 was
reported in the original paper.

5 Discussion

5.1 ATLX
As described in previously, the overall perfor-
mance of the baseline gets enhanced by leverag-
ing lexical features independent from the training
data, which makes the model more robust and flex-
ible. The example in Figure 2, although the base-
line is able to pay relatively high attention to the
word disappointed and dungeon, it is not able to
recognize these words as clear indicators of nega-
tive polarity; while ATLX is able to correctly pre-
dict positive for both examples. On the other hand,
it is worth mentioning that the computation of the
attention vector α does not take lexical features Vl
into account. Although it is natural to think that
adding Vl as input for computing α would be a
good option, the results of ATLX* in Table 2 sug-
gest otherwise.

In order to understand where does the improve-
ment of ATLX come from, lexical features or the
way we introduce lexical features to the system?
We conduct a support experiment to verify its im-
pact (baseLX), which consists of naively concate-

nating input word vector with its associated lexi-
cal vector and feed the extended embedding to the
baseline. As demonstrated in Table 2, by compar-
ing baseline with baseLX, we see that the simple
merge of lexical features with the network without
carefully designed mechanism, the model is not
able to leverage new information; and in contrast,
the overall performance gets decreased.

5.2 Attention Regularization

As shown in Figure 3, when comparing ATLX
with the baseline, we find that although the lexicon
only provides non-neutral polarity information for
three words, the attention weights of ATLX are
less sparse and less spread out than in the base-
line. Also, this effect is general as the standard de-
viation of the attention weights distribution for the
test set in ATLX (0.0219) are significantly lower
than in the baseline (0.0354).

Thus it makes us think that the attention weights
might be over-fitting in some cases as it is purely
learned on training examples. This could cause
that by giving too much weight to particular words
in a sentence, the network ignores other positions
which could provide key information for classify-
ing the polarity. For instance, the example in Fig-
ure 3 shows that the baseline which predicts posi-
tive is “focusing” on the final part of the sentence,
mostly the word easy; while ignoring the bad man-
ners coming before, which is key for judging the
polarity of the sentence given the aspect service.
In contrast, the same baseline model trained with
attention regularized by standard deviation is able
to correctly predict negative just by “focusing” a
little bit more on the ”bad manners” part.

However, the hard regularization by standard
deviation might not be ideal as the optimal min-
imum value of the regularizer will imply that all
words in the sentence have homogeneous weight,



258

Parameter name Value
� basestd 1e-3
� baseent 0.5
� ATLXstd 1e-4
� ATLXent 0.006

Table 3: Attention regularization parameter settings

which is the opposite of what the attention mech-
anism is able to gain.

Regarding the negative entropy regularizer, tak-
ing into account that the attention weights are out-
put of softmax which is normalized to sum up
to 1, although the minimum value of this term
would also imply homogeneous weight of 1N , it
is interesting to see that with almost evenly dis-
tributed α, the model remains sensitive to few po-
sitions with relatively higher weights; e.g. in Fig-
ure 3, the same sentence with entropy regulariza-
tion demonstrates that although most positions are
closely weighted, the model is still able to differ-
entiate key positions even with a weight difference
of 0.01 and predict correctly.

6 Parameter Settings

In our experiments, apart from newly introduced
parameter � for attention regularization, we follow
Wang et al. (2016) and their released code.

More specifically, we set batch size as 25; as-
pect embedding dimension da equals to 300, same
as Glove vector dimension; number of LSTM cell
d as 300; number of LSTM layers as 1; dropout
with 0.5 keep probability is applied to h∗; Ada-
Grad optimizer is used with initial accumulate
value equals to 1e-10; learning rate is set to 0.01;
L2 regularization parameter λ is set to 0.001; net-
work parameters are initialized from a random
uniform distribution with min and max values as -
0.01 and 0.01; all network parameters except word
embeddings are included in the L2 regularizer.
The hyperparmerter � for attention regularization
is shown in Table 3.

7 Conclusion and Future Works

In this paper, we describe our approach of di-
rectly leveraging numerical polarity features pro-
vided by existing lexicon resources in an aspect-
based sentiment analysis environment with an at-
tention LSTM neural network. Meanwhile, we
stress that the attention mechanism may over-fit
on particular positions, blinding the model from

other relevant positions. We also explore two reg-
ularizers to reduce this overfitting effect. The ex-
perimental results demonstrate the effectiveness of
our approach.

For future works, since the lexical features can
be leveraged directly by the network to boost per-
formance, a fine-grained lexicon which is domain
and aspect specific in principle could further im-
prove similar models. On the other hand, although
the negative entropy regularizer is able to reduce
the overfitting effect, a better attention framework
could be researched, so that the attention distribu-
tion would be sharp and spare but at the same time,
being able to “focus” on more positions.

References
Jiajun Cheng, Hui Wang, Shenglin Zhao, Xin Zhang,

Irwin King, and Jiani Zhang. 2017. Aspect-level
Sentiment Classification with HEAT (HiErarchical
ATtention) Network. In Proceedings of the 2017
ACM on Conference on Information and Knowledge
Management - CIKM ’17, pages 97–106.

Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel
Dahlmeier. 2018. Effective Attention Modeling
for Aspect-Level Sentiment Classification. In Pro-
ceedings of the 27th International Conference on
Computational Linguistics (COLING), pages 1121–
1131.

Sepp; Hochreiter and J?urgen Schmidhuber. 1997.
Long Short Term Memory. Neural Computation,
9(8):1735–1780.

Zeyang Lei, Yujiu Yang, and Min Yang. 2018. Senti-
ment Lexicon Enhanced Attention-Based LSTM for
Sentiment Classification. AAAI-2018-short paper,
pages 8105–8106.

Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng
Wang. 2017. Interactive attention networks for
aspect-level sentiment classification. In IJCAI Inter-
national Joint Conference on Artificial Intelligence,
pages 4068–4074.

Vlad Niculae and Mathieu Blondel. 2017. A Regular-
ized Framework for Sparse and Structured Neural
Attention. NIPS.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. Empirical Meth-
ods in Natural Language Processing (EMNLP),
10(July):79–86.

Qiao Qian, Minlie Huang, Jinhao Lei, and Xiaoyan
Zhu. 2017. Linguistically Regularized LSTM for
Sentiment Classification. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
1679–1689.

https://doi.org/10.1145/3132847.3133037
https://doi.org/10.1145/3132847.3133037
https://doi.org/10.1145/3132847.3133037
http://aclweb.org/anthology/C18-1096 http://www.aclweb.org/anthology/C18-1096
http://aclweb.org/anthology/C18-1096 http://www.aclweb.org/anthology/C18-1096
https://doi.org/10.1162/neco.1997.9.8.1735
www.aaai.org
www.aaai.org
www.aaai.org
http://arxiv.org/abs/1709.00893
http://arxiv.org/abs/1709.00893
http://arxiv.org/abs/1705.07704
http://arxiv.org/abs/1705.07704
http://arxiv.org/abs/1705.07704
https://doi.org/10.3115/1118693.1118704
https://doi.org/10.3115/1118693.1118704
https://doi.org/10.18653/v1/P17-1154
https://doi.org/10.18653/v1/P17-1154


259

Mickael Rouvier and Benoit Favre. 2016. SENSEI-
LIF at SemEval-2016 Task 4 : Polarity embedding
fusion for robust sentiment analysis. In Proceed-
ings of the 10th International Workshop on Semantic
Evaluation (SemEval-2016), pages 207–213.

Bonggun Shin, Timothy Lee, and Jinho D Choi. 2017.
Lexicon Integrated CNN Models with Attention for
Sentiment Analysis. ACL, pages 149–158.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2017.
Dyadic Memory Networks for Aspect-based Senti-
ment Analysis. In Proceedings of the 2017 ACM
on Conference on Information and Knowledge Man-
agement - CIKM ’17, pages 107–116.

Zhiyang Teng, Duy-Tin Vo, and Yue Zhang. 2016.
Context-Sensitive Lexicon Features for Neural Sen-
timent Analysis. EMNLP, pages 1629–1638.

Peter D Turney. 2002. Thumbs up or thumbs down?
Semantic Orientation applied to Unsupervised Clas-
sification of Reviews. Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL), (July):417–424.

Yequan Wang, Minlie Huang, Li Zhao, and Xiaoyan
Zhu. 2016. Attention-based LSTM for Aspect-level
Sentiment Classification. Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 606–615.

Theresa Wilson, Theresa Wilson, Janyce Wiebe,
Janyce Wiebe, Rebecca Hwa, and Rebecca Hwa.
2004. Just how mad are you? Finding strong and
weak opinion clauses. Proceedings of the National
Conference on Artificial Intelligence, pages 761–
769.

Ou Wu, Tao Yang, Mengyang Li, and Ming Li. 2018.
$$-hot Lexicon Embedding-based Two-level LSTM
for Sentiment Analysis.

Jiajun Zhang, Yang Zhao, Haoran Li, and Chengqing
Zong. 2019. Attention with sparsity regularization
for neural machine translation and summarization.
IEEE/ACM Transactions on Audio Speech and Lan-
guage Processing, 27(3):507–518.

Peisong Zhu and Tieyun Qian. 2018. Enhanced Aspect
Level Sentiment Classification with Auxiliary Mem-
ory. In COLING, pages 1077–1087.

Yicheng Zou, Tao Gui, Qi Zhang, and Xuanjing
Huang. 2018. A Lexicon-Based Supervised Atten-
tion Model for Neural Sentiment Analysis. In COL-
ING, pages 868–877.

A Supplemental Material

A.1 Resource Details
Lexical resources: MPQA1, Opinion Lexicon2,
Opener3, and Vader4. Glove vectors5. Code6 re-
leased by Wang et al. (2016). Experiments de-
scribed in this paper are implemented with Ten-
sorFlow7.

1http://mpqa.cs.pitt.edu/#subj lexicon
2https://www.cs.uic.edu/l̃iub/FBS/sentiment-

analysis.html#lexicon
3https://github.com/opener-project/VU-sentiment-

lexicon/tree/master/VUSentimentLexicon/EN-lexicon
4https://github.com/cjhutto/vaderSentiment
5https://nlp.stanford.edu/projects/glove/
6https://www.wangyequan.com/publications/
7https://www.tensorflow.org/

http://www.aclweb.org/anthology/S16-1030
http://www.aclweb.org/anthology/S16-1030
http://www.aclweb.org/anthology/S16-1030
http://arxiv.org/abs/1610.06272
http://arxiv.org/abs/1610.06272
https://doi.org/10.1145/3132847.3132936
https://doi.org/10.1145/3132847.3132936
https://www.aclweb.org/anthology/D16-1169
https://www.aclweb.org/anthology/D16-1169
https://doi.org/10.3115/1073083.1073153
https://doi.org/10.3115/1073083.1073153
https://doi.org/10.3115/1073083.1073153
https://doi.org/10.1.1.5.2078
https://doi.org/10.1.1.5.2078
http://arxiv.org/abs/1803.07771
http://arxiv.org/abs/1803.07771
https://doi.org/10.1109/TASLP.2018.2883740
https://doi.org/10.1109/TASLP.2018.2883740
http://aclweb.org/anthology/C18-1092
http://aclweb.org/anthology/C18-1092
http://aclweb.org/anthology/C18-1092
http://jkx.fudan.edu.cn/{~}qzhang/paper/coling2018.pdf http://aclweb.org/anthology/C18-1074
http://jkx.fudan.edu.cn/{~}qzhang/paper/coling2018.pdf http://aclweb.org/anthology/C18-1074

