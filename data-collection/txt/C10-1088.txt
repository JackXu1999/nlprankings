Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 779–787,

Beijing, August 2010

779

Evaluating Dependency Representation for Event Extraction

Makoto Miwa1 Sampo Pyysalo1 Tadayoshi Hara1

Jun’ichi Tsujii1,2,3

1Department of Computer Science, the University of Tokyo

2School of Computer Science, University of Manchester

3National Center for Text Mining

{mmiwa,smp,harasan,tsujii}@is.s.u-tokyo.ac.jp

Abstract

The detailed analyses of sentence struc-
ture provided by parsers have been applied
to address several information extraction
tasks. In a recent bio-molecular event ex-
traction task, state-of-the-art performance
was achieved by systems building specif-
ically on dependency representations of
parser output. While intrinsic evalua-
tions have shown signiﬁcant advances in
both general and domain-speciﬁc pars-
ing, the question of how these translate
into practical advantage is seldom con-
sidered.
In this paper, we analyze how
event extraction performance is affected
by parser and dependency representation,
further considering the relation between
intrinsic evaluation and performance at
the extraction task. We ﬁnd that good
intrinsic evaluation results do not always
imply good extraction performance, and
that the types and structures of differ-
ent dependency representations have spe-
ciﬁc advantages and disadvantages for the
event extraction task.

1 Introduction

Advanced syntactic parsing methods have been
shown to effective for many information extrac-
tion tasks. The BioNLP 2009 Shared Task, a re-
cent bio-molecular event extraction task, is one
such task: analysis showed that the application of
a parser correlated with high rank in the task (Kim

et al., 2009). The automatic extraction of bio-
molecular events from text is important for a num-
ber of advanced domain applications such as path-
way construction, and event extraction thus a key
task in Biomedical Natural Language Processing
(BioNLP).

Methods building feature representations and
extraction rules around dependency representa-
tions of sentence syntax have been successfully
applied to a number of tasks in BioNLP. Several
parsers and representations have been applied in
high-performing methods both in domain studies
in general and in the BioNLP’09 shared task in
particular, but no direct comparison of parsers or
representations has been performed. Likewise,
a number of evaluation of parser outputs against
gold standard corpora have been performed in the
domain, but the broader implications of the results
of such intrinsic evaluations are rarely considered.
The BioNLP’09 shared task involved documents
contained also in the GENIA treebank (Tateisi et
al., 2005), creating an opportunity for direct study
of intrinsic and task-oriented evaluation results.
As the treebank can be converted into various de-
pendency formats using existing format conver-
sion methods, evaluation can further be extended
to cover the effects of different representations.

In this this paper, we consider three types of de-
pendency representation and six parsers, evaluat-
ing their performance from two different aspects:
dependency-based intrinsic evaluation, and effec-
tiveness for bio-molecular event extraction with a
state-of-the-art event extraction system. Compar-
ison of intrinsic and task-oriented evaluation re-

780

guments corresponding to locations and sites con-
sidered in Task 2.

2.1 Event Extraction System
For evaluation, we apply the system of Miwa et al.
(2010b). The system was originally developed for
ﬁnding core events (Task 1) using the native out-
put of the Enju and GDep parsers. The system
consists of three supervised classiﬁcation-based
modules: a trigger detector, an event edge detec-
tor, and a complex event detector. The trigger
detector classiﬁes each word into the appropriate
event types, the event edge detector classiﬁes each
edge between an event and a candidate participant
into an argument type, and the complex event de-
tector classiﬁes event candidates constructed by
all edge combinations, deciding between event
and non-event. The system uses one-vs-all sup-
port vector machines (SVMs) for classiﬁcation.

The system operates on one sentence at a time,
building features for classiﬁcation based on the
syntactic analyses for the sentence provided by
the two parsers as well as the sequence of the
words in the sentence, including the target candi-
date. The features include the constituents/words
around entities (triggers and proteins), the depen-
dencies, and the shortest paths among the enti-
ties. The feature generation is format-independent
regarding the shared properties of different for-
mats, but makes use also of format-speciﬁc infor-
mation when available for extracting features, in-
cluding the dependency tags, word-related infor-
mation (e.g. a lexical entry in Enju format), and
the constituents and their head information.

We apply here a variant of the base system in-
corporating a number of modiﬁcations. The ap-
plied system performs feature selection removing
two classes of features that were found not to be
beneﬁcial for extraction performance, and applies
a reﬁnement of the trigger expressions of events.
The system is further extended to ﬁnd also sec-
ondary arguments (Task 2). For a detailed descrip-
tion of these improvements, we refer to Miwa et
al. (2010a).

3 Parsers and Representations

Six publicly available parsers and three depen-
dency formats are considered in this paper. The

Figure 1: Event Extraction.

sults shows that performance against gold stan-
dard annotations is not always correlated with
event extraction performance. We further ﬁnd
that the dependency types and overall structures
employed by the different dependency representa-
tions have speciﬁc advantages and disadvantages
for the event extraction task.

2 Bio-molecular Event Extraction

In this study, we adopt the event extraction task
deﬁned in the BioNLP 2009 Shared Task (Kim et
al., 2009) as a model information extraction task.
Figure 1 shows an example illustrating the task
of event extraction from a sentence. The shared
task provided common and consistent task deﬁ-
nitions, data sets for training and evaluation, and
evaluation criteria. The shared task deﬁned ﬁve
simple events (Gene expression, Transcription,
Protein catabolism, Phosphorylation, and Local-
ization) that take one core argument, a multi-
participant binding event (Binding), and three reg-
ulation events (Regulation, Positive regulation,
and Negative regulation) used to capture both bi-
ological regulation and general causation. The
participants of simple and Binding events were
speciﬁed to be of the general Protein type, while
regulation-type events could also take other events
as arguments, creating complex event structures.
We consider two subtasks, Task 1 and Task 2,
out of the three deﬁned in the shared task. Task 1
focuses on core event extraction, and Task 2
involves augmenting extracted events with sec-
ondary arguments (Kim et al., 2009). Events are
represented with a textual trigger, type, and ar-
guments, where the trigger is a span of text that
states the event in text. In Task 1 the event argu-
ments that need to be extracted are restricted to the
core Theme and Cause roles, with secondary ar-

… In this study we hypothesized that the phosphorylation of TRAF2inhibits 
binding to the CD40cytoplasmicdomain. …
Negative_regulation
inhibits
Cause
Theme
Binding
Phospholylation
binding
phosphorylation
Theme
Theme
Theme
TRAF2
CD40
TRAF2

781

Figure 2: Stanford basic dependency tree

Figure 3: CoNLL-X dependency tree

Figure 5: Format conversion dependencies in six
parsers. Formats adopted for the evaluation are
shown in solid boxes. SD: Stanford Dependency
format, CCG: Combinatory Categorial Grammar
output format, PTB: Penn Treebank format, and
PAS: Predicate Argument Structure in Enju for-
mat.

Figure 4: Predicate Argument Structure

parsers are GDep (Sagae and Tsujii, 2007), the
Bikel parser (Bikel) (Bikel, 2004), the Stanford
parser with two probabilistic context-free gram-
mar (PCFG) models1 (Wall Street Journal (WSJ)
model (Stanford WSJ) and “augmented English”
(Stanford eng))
model
(Klein and Manning,
2003),
the Charniak-Johnson reranking parser,
using David McClosky’s self-trained biomedi-
cal parsing model (MC) (McClosky, 2009), the
C&C CCG parser, adapted to biomedical text
(C&C) (Rimell and Clark, 2009), and the Enju
parser with the GENIA model (Miyao et al.,
2009).
The formats are Stanford Dependen-
cies (SD) (Figure 2), the CoNLL-X dependency
format (CoNLL) (Figure 3) and the predicate-
argument structure (PAS) format used by Enju
(Figure 4). With the exception of Stanford and
Enju, the analyses of these parsers were provided
by the BioNLP 2009 Shared Task organizers.

The six parsers operate in a number of different
frameworks, reﬂected in their analyses. GDep is
a native dependency parser that produces CoNLL
dependency trees, with dependency types similar
to those of CoNLL 2007. Bikel, Stanford, and MC

1Experiments showed no beneﬁt from using the lexical-

ized models with the Stanford parser.

are phrase-structure parsers trained on Penn Tree-
bank format (PTB) style treebanks, and they pro-
duce PTB trees. C&C is a deep parser based on
Combinatory Categorial Grammar (CCG), and its
native output is in a CCG-speciﬁc format. The
output of C&C can be converted into SD by a
rule-based conversion script (Rimell and Clark,
2009). Enju is deep parser based on Head-driven
Phrase Structure Grammar (HPSG) and produces
a format containing predicate argument structures
along with a phrase structure tree in Enju format,
which can be converted into PTB format (Miyao
et al., 2009).

For direct comparison and for the study of con-
tribution of the formats in which the six parsers
output their analyses to task performance, we ap-
ply a number of conversions between the out-
puts, shown in Figure 5. The Enju PAS output is
converted into PTB using the method introduced
by (Miyao et al., 2009). SD is generated from
PTB by the Stanford tools (de Marneffe et al.,
2006), and CoNLL generated from PTB by us-
ing Treebank Converter (Johansson and Nugues,
2007). With the exception of GDep, all CoNLL
outputs are generated by the conversion and thus
share dependency types. We note that all of these
conversions can introduce some errors in the con-
version process.

nn

prep

pobjcc
NFAT/AP-1 complex formed onlywith Pand
conj

nsubj

dep

P2

root

VMOD
PMODCOORD
ROOT
NFAT/AP-1 complex formed onlywith Pand
P2
NMOD
PMOD
CONJ

VMOD

prep_arg12
noun_arg1
arg1
arg2
NFAT/AP-1 complex formed onlywith Pand
coord_arg12
arg1

prep_arg12
arg1
adj_arg1
arg1

verb_arg1
arg1

P2
coord_arg12
arg2

SDConll-X

C&CGDep
McClosky-
Charniak
Bikel
Stanford
Enju

CCG
PASPTB

782

4 Evaluation Setting

4.1 Event Extraction Evaluation
Event extraction performance is evaluated using
the evaluation script provided by the BioNLP’09
shared task organizers for the development data
set, and the online evaluation system of the task
for the test data set2 . Results are reported under
the ofﬁcial evaluation criterion of the task, i.e. the
“Approximate Span Matching/Approximate Re-
cursive Matching” criterion.

The event extraction system described in Sec-
tion 2.1 is used with the default settings given in
(Miwa et al., 2010b). The C-values of SVMs are
set to 1.0, but the positive and negative examples
are balanced by placing more weight on the posi-
tive examples. The examples predicted with con-
ﬁdence greater than 0.5, as well as the examples
with the most conﬁdent labels, are extracted. Task
1 and Task 2 are solved at once for the evaluation.
Some of the parse results do not include word
base forms or part-of-speech (POS) tags, which
are required by the event extraction system. To
apply these parsers, the GENIA Tagger (Tsuruoka
et al., 2005) output is adopted to add this informa-
tion to the results.

4.2 Dependency Representation Evaluation
The parsers are evaluated with precision, recall,
and F-score for each dependency type. We note
that the parsers may produce more ﬁne-grained
word segmentations than that of the gold standard:
for example, two words “p70(S6)-kinase activa-
tion” in the gold standard tree (Figure 6 (a)) is
segmented into ﬁve words by Enju (Figure 6 (b)).
In the evaluation the word segmentations in the
gold tree are used, and dependency transfer and
word-based normalization are performed to match
parser outputs to these. Dependencies related to
the segmentations are transferred to the enclosing
word as follows. If one word is segmented into
several segments by a parser, all the dependencies
between the segments are removed (Figure 6 (c))
and the dependency between another word and
the segments is converted into the dependency be-
tween the two words (Figure 6 (d)).

2http://www-tsujii.is.s.u-tokyo.ac.jp/

GENIA/SharedTask/

The parser outputs in SD and CoNLL can be
assumed to be trees, so each node in the tree have
only one parent node. However, in the converted
tree nodes can have more than one parent. We
cannot simply apply accuracy, or (un)labeled at-
tachment score3. Word-based normalization is
performed to avoid negative impact by the word
segmentations by parsers. When (a) and (d) in
Figure 6 are compared, the counts of correct re-
lations will be 1.0 (0.5 for upper NMOD and 0.5
for lower NMOD in Figure 6 (d)) for the parser
(precision), and the counts of correct relations will
be 1.0 (for NMOD in Figure 6 (a)) for the gold
(recall). This F-score is a good approximation of
accuracy.

4.3 GENIA treebank processing
For comparison and evaluation, the texts in the
GENIA treebank (Tateisi et al., 2005) are con-
verted to the various formats as follows. To create
PAS, the treebank is converted with Enju, and for
trees that fail conversion, parse results are used in-
stead. The GENIA treebank is also converted into
PTB4, and then converted into SD and CoNLL as
described in Section 3. While based on manually
annotated gold data, the converted treebanks are
not always correct due to conversion errors.

5 Evaluation

This section presents evaluation results. Intrinsic
evaluation is ﬁrst performed in Section 5.1. Sec-
tion 5.2 considers the effect of different SD vari-
ants. Section 5.3 presents the results of experi-
ments with different parsers. Section 5.4 shows
the performance of different parsers. Finally, the
performance of the event extraction system is dis-
cussed in context of other proposed methods for
the task in Section 5.5.

Intrinsic Evaluation

5.1
We initially brieﬂy consider the results of an in-
trinsic evaluation comparing parser outputs to ref-
erence data automatically derived from the gold
standard treebank. Table 1 shows results for the
parsers whose outputs could be converted into the

3http://nextens.uvt.nl/∼conll/
4http://categorizer.tmit.bme.hu/

∼illes/genia ptb/

783

(a) Gold Word Segmen-
tations

(b) Parser Word Seg-
mentations

Inner Dependency

(c)
Removal

(d) Dependency Trans-
fer

Figure 6: Example of Word Segmentations of the words by gold and Enju and Dependency Transfer.

Bikel
SP WSJ
SP eng
C&C
MC
Enju

P
70.31
74.11
79.08
80.31
79.56
85.59

SD
R
70.37
73.94
78.89
78.04
79.63
85.62

Typed

F
70.34
74.03
78.98
79.16
79.60
85.60

P
77.81
81.41
84.92

88.13
88.59

CoNLL
R
77.56
81.47
84.82

-

87.87
89.51

F
77.69
81.44
84.87

88.00
89.05

P
80.54
81.36
84.16
84.91
87.43
88.28

SD
R
80.60
81.16
83.96
82.28
87.50
88.30

Untyped

F
80.57
81.26
84.06
83.57
87.47
88.29

P
82.43
84.05
86.54

89.81
90.24

CoNLL
R
82.18
84.05
86.47

-

89.42
90.77

F
82.31
84.05
86.51

89.62
90.50

Table 1: Comparison of precision, recall, and F-score results with ﬁve parsers (two models for Stanford)
in two different formats on the development data set (SP abbreviates for Stanford Parser). Results
shown separately for evaluation including dependency types and one eliminating them. Parser/model
combinations above the line do not use in-domain data, others do.

SD and CoNLL dependency representations us-
ing the Stanford tools and Treebank Converter, re-
spectively. For Stanford, both the Penn Treebank
WSJ section and “augmented English” (eng) mod-
els were tested; the latter includes biomedical do-
main data. The Enju results for PAS are 91.48
with types and 93.39 without in F-score. GDep
not shown as its output is not compatible with that
of Treebank Converter.

Despite numerical differences, the two repre-
sentations and two criteria (typed/untyped) all
produce largely the same ranking of the parsers.5
The evaluations also largely agree on the magni-
tude of the reduction in error afforded through the
use of in-domain training data for the Stanford
parser, with all estimates falling in the 15-19%
range. Similarly, all show substantial differences
between the parsers, indicating e.g. that the error
rate of Enju is 50% or less of that of Bikel.

These results serve as a reference point for ex-
trinsic evaluation results. However, it should be

5One larger divergence is between typed and untyped SD
results for MC. Analysis suggest one cause is frequent errors
in tagging hyphenated noun-modiﬁers such as NF-kappaB as
adjectives.

Task 1
Task 2

BD
55.60
53.94

CD
54.35
52.65

CDP
54.59
52.88

CTD
54.42
52.76

Table 2: Comparison of the F-score results with
different SD variants on the development data set
with the MC parser. The best score in each task is
shown in bold.

noted that as the parsers make use of annotated
domain training data to different extents, this eval-
uation does not provide a sound basis for direct
comparison of the parsers themselves.

5.2 Stanford Dependency Setting

SD have four different variants: basic depen-
dencies (BD), collapsed dependencies (CD), col-
lapsed dependencies with propagation of conjunct
dependencies (CDP), and collapsed tree depen-
dencies (CTD) (de Marneffe and Manning, 2008).
Except for BD, these variants do not necessarily
connect all the words in the sentence, and CD and
CDP do not necessarily form a tree structure. Ta-
ble 2 shows the comparison results with the MC
parser. Dependencies are generalized by remov-
ing expressions after “ ” of the dependencies (e.g.

p70(S6)-kinaseNMOD
activation

NMOD
P
-kinase
p70(S6)
activation
NMOD
PRNP

NMOD
-kinase
activation
p70(S6)
NMOD

NMOD
activation
p70(S6)-kinaseNMOD

784

“ with” in prep with) for better performance. We
ﬁnd that basic dependencies give the best perfor-
mance to event extraction, with little difference
between the other variants. This result is surpris-
ing, as variants other than basic have features such
as the resolution of conjunctions that are specif-
ically designed for practical applications. How-
ever, basic dependencies were found to consis-
tently provide best performance also for the other
parsers6. Thus, in the following evaluation, the
basic dependencies are adopted for all SD results.

5.3 Parser Comparison on Event Extraction

Results with different parsers and different for-
mats on the development data set are summarized
in Table 3. Baseline results are produced by re-
moving dependency information from the parse
results. The baseline results differ between the
representations as the word base forms and POS
tags produced by the GENIA tagger for use with
SD and CoNLL are different from PAS, and be-
cause head word information in the Enju format is
used. The evaluation ﬁnds best results for both
tasks with Enju, using its native output format.
However, as discussed in Section 2.1, the treat-
ment of PAS and the other two formats are slightly
different, this result does not necessarily indicate
that PAS is the best alternative for event extrac-
tion.

The Bikel and Stanford WSJ parsers, lacking
models adapted to the biomedical domain, per-
forms mostly worse than the other parsers. The
other parsers, even though trained on the treebank,
do not provide performance as high as that for
using the GENIA treebank, but, with the excep-
tion of Stanford eng with CoNLL, results with the
parsers are only slightly worse than results with
the treebank. The results with the data derived
from the GENIA treebank can be considered as
upper bounds for the parsers and formats at the
task, although conversion errors are expected to
lower these bounds to some extent. The results
suggest that there is relative little remaining ben-
eﬁt to be gained from improving parser perfor-
mance.

6Collapsed tree dependencies are not evaluated on the

C&C parser since the conversion is not provided.

5.4 Effects of Dependency Representation
Intrinsic evaluation results (Section 5.1) cannot
be used directly for comparing the parsers, since
some of the parsers contain models trained on the
GENIA treebank. To investigate the effects of the
evaluation results to the event extraction, we per-
formed event extraction with eliminating the de-
pendency types. Table 4 summarizes the results
with the dependency structures (without the de-
pendency types) on the development data set. In-
terestingly, we ﬁnd the performance increases in
Bikel and Stanford by eliminating the dependency
types. This implies that the inaccurate depen-
dency types shown in Table 1 confused the event
extraction system. SD and PAS drops more than
CoNLL, and Enju with CoNLL structures perform
best in total when the dependency types are re-
moved. This result shows that the formats have
their own strengths in ﬁnding events, and CoNLL
structure with SD or PAS types can be a good rep-
resentation for the event extraction.

By comparing Table 3, Table 1, and Table 4,
we found that the better dependency performance
does not always produce better event extraction
performance especially when the difference of the
dependency performance is small. MC and Enju
results show that performance in dependency is
important for event extraction. SD can be better
than CoNLL for the event extraction (shown with
the gold treebank data in Table 3), but the types
and relations of CoNLL were well predicted, and
MC and Enju performed better for CoNLL than
for SD in total.

5.5 Performance of Event Extraction System
Several systems are compared by the extraction
performance on the shared task test data in Ta-
ble 5. GDep and Enju with PAS are used for the
evaluation, which is the same evaluation setting
with the original system by Miwa et al. (2010b).
The performance of the best systems in the orig-
inal shared task is shown for reference ((Bj¨orne
et al., 2009) in Task 1 and (Riedel et al., 2009)
in Task 2). The event extraction system performs
signiﬁcantly better than the best systems in the
shared task, further outperforming the original
system. This shows that the comparison of the
parsers is performed with a state-of-the-art sys-

785

SD
51.05
53.29
53.51
55.02

-

55.60
56.09
55.48
56.34

Task 1
CoNLL

-

PAS
50.42

53.22
54.38
53.66
55.70
56.01

-

55.74
56.09

-
-
-
-
-
-

56.57
57.94

SD
49.17
51.40
52.02
53.41

-

53.94
54.27
54.06
55.04

Task 2
CoNLL

-

PAS
48.88

51.27
52.04
52.74
54.37
54.51

-

54.37
54.57

-
-
-
-
-
-

55.31
56.40

Baseline

Bikel

Stanford WSJ
Stanford eng

GDep
MC
C&C
Enju

GENIA

Table 3: Comparison of F-score results with six parsers in three different formats on the development
data set. Results without dependency information are shown as baselines. The results with the GENIA
treebank (converted into PTB and PAS) are shown for comparison. The best score in each task is shown
in bold, and the best score in each task and format is underlined.

SD

Task 1
CoNLL

PAS

SD

Task 2
CoNLL

PAS

Bikel

Stanford WSJ
Stanford eng

GDep
MC
C&C
Enju

GENIA

53.41 (+0.12)
53.03 (-0.48)
54.48 (-0.54)

-

54.22 (-1.38)
54.64 (-1.45)
53.74 (-1.74)
55.79 (-0.55)

53.92 (+0.70)
54.52 (+0.14)
54.02 (+0.36)
54.97 (-0.73)
55.24 (-0.77)

-

55.66 (-0.08)
55.64 (-0.45)

-
-
-
-
-
-

55.23 (-1.34)
56.42 (-1.52)

51.59 (+0.19)
51.43 (-0.59)
52.88 (-0.53)

-

52.73 (-1.21)
52.98 (-1.29)
52.29 (-1.77)
54.17 (-0.87)

52.21 (+0.94)
52.60 (-0.14)
52.28 (+0.24)
53.71 (-0.66)
53.42 (-1.09)

-

53.97 (-0.40)
53.83 (-0.74)

-
-
-
-
-
-

53.69 (-1.62)
55.34 (-1.06)

Table 4: Comparison of F-score results with six parsers in three different dependency structures (with-
out the dependency types) on the development data set. The changes from Table 3 are shown.

Simple

Ours
Miwa
Bj¨orne
Riedel
Baseline

Ours
Riedel
Baseline

66.84 / 78.22 / 72.08
65.31 / 76.44 / 70.44
64.21 / 77.45 / 70.21

62.94 / 68.38 / 65.55

65.43 / 75.56 / 70.13

60.88 / 63.78 / 62.30

N/A

N/A

Binding

Task 1
48.70 / 52.65 / 50.60
52.16 / 53.08 / 52.62
40.06 / 49.82 / 44.41
23.05 / 48.19 / 31.19
48.41 / 34.50 / 40.29
Task 2
46.42 / 50.31 / 48.29
22.35 / 46.99 / 30.29
44.99 / 31.78 / 37.25

Regulation

All

38.48 / 55.06 / 45.30
35.93 / 46.66 / 40.60
35.63 / 45.87 / 40.11
26.32 / 41.81 / 32.30
29.40 / 40.00 / 33.89

50.13 / 64.16 / 56.28
48.62 / 58.96 / 53.29
46.73 / 58.48 / 51.95
36.90 / 55.59 / 44.35
43.93 / 50.11 / 46.82

38.18 / 54.45 / 44.89
25.75 / 40.75 / 31.56
29.07 / 39.52 / 33.50

49.20 / 62.57 / 55.09
35.86 / 54.08 / 43.12
42.62 / 47.84 / 45.08

Table 5: Comparison of Recall / Precision / F-score results on the test data set. Results on simple,
binding, regulation, and all events are shown. GDep and Enju with PAS are used. Results by Miwa et
al. (2010b), Bj¨orne et al. (2009), Riedel et al. (2009), and Baseline for Task 1 and Task 2 are shown for
comparison. Baseline results are produced by removing dependency information from the parse results
of GDep and Enju. The best score in each result is shown in bold.

tem.

6 Related Work

Many approaches for parser comparison have
been proposed, and most comparisons have used
gold treebanks with intermediate formats (Clegg
and Shepherd, 2007; Pyysalo et al., 2007). Parser
comparison has also been proposed on speciﬁc

tasks such as unbounded dependencies (Rimell
et al., 2009) and textual entailment ( ¨Onder Eker,
2009)7. Among them, application-oriented parser
comparison across several formats was ﬁrst intro-
duced by Miyao et al. (2009), who compared eight
parsers and ﬁve formats for the protein-protein in-
teraction (PPI) extraction task. PPI extraction, the

7http://pete.yuret.com/

786

formats, further adding information provided by
other formats, such as the lexical entries of the
Enju format, from external resources. The results
of this paper are expected to be useful as a guide
not only for parser selection for biomedical infor-
mation extraction but also for the development of
event extraction systems.

The comparison in the present evaluation is
limited to the dependency representation. As fu-
ture work, it would be informative to extend the
comparison to other syntactic representation, such
as the PTB format. Finally, the evaluation showed
that the system fails to recover approximately
40% of events even when provided with manually
annotated treebank data, showing that other meth-
ods and resources need to be adopted to further
improve bio-molecular event extraction systems.
Such improvement is left as future work.

Acknowledgments

This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT,
Japan), Genome Network Project (MEXT, Japan),
and Scientiﬁc Research (C) (General) (MEXT,
Japan).

recognition of binary relations of between pro-
teins, is one of the most basic information ex-
traction tasks in the BioNLP ﬁeld. Our ﬁndings
do not conﬂict with those of Miyao et al. Event
extraction can be viewed as an additional extrin-
sic evaluation task for syntactic parsers, providing
more reliable and evaluation and a broader per-
spective into parser performance. An additional
advantage of application-oriented evaluation on
BioNLP shared task data is the availability of a
manually annotated gold standard treebank, the
GENIA treebank, that covers the same set of ab-
stracts as the task data. This allows the gold tree-
bank to be considered as an evaluation standard,
in addition to comparison of performance in the
primary task.

7 Conclusion

We compared six parsers and three formats on a
bio-molecular event extraction task with a state-
of-the-art event extraction system from two dif-
ferent aspects: dependency-based intrinsic eval-
uation and task-based extrinsic evaluation. The
speciﬁc task considered was the BioNLP shared
task, allowing the use of the GENIA treebank as
a gold standard parse reference. Five of the six
considered parsers were applied using biomedi-
cal models trained on the GENIA treebank, and
they were found to produce similar performance.
The comparison of the parsers from two aspects
showed slightly different results, and and the
dependency representations have advantages and
disadvantages for the event extraction task.

The contributions of this paper are 1) the com-
parison of intrinsic and extrinsic evaluation on
several commonly used parsers with a state-of-
the-art system, and 2) demonstration of the lim-
itation and possibility of the parser and system
improvement on the task. One limitation of this
study is that the comparison between the parsers
is not perfect, as the parsers are used with the pro-
vided models, the format conversions miss some
information from the original formats, and results
with different formats depend on the ability of
the event extraction system to take advantage of
their strengths. To maximize comparability, the
system was designed to extract features identi-
cally from similar parts of the dependency-based

787

References
Bikel, Daniel M. 2004. A distributional analysis of a
lexicalized statistical parsing model. In In EMNLP,
pages 182–189.

Bj¨orne, Jari, Juho Heimonen, Filip Ginter, Antti
Airola, Tapio Pahikkala, and Tapio Salakoski. 2009.
Extracting complex biological events with rich
graph-based feature sets.
In Proceedings of the
BioNLP’09 Shared Task on Event Extraction, pages
10–18.

Clegg, Andrew B. and Adrian J. Shepherd.

2007.
Benchmarking natural-language parsers for biolog-
ical applications using dependency graphs. BMC
Bioinformatics, 8.

de Marneffe, Marie-Catherine and Christopher D.
Manning. 2008. Stanford typed dependencies man-
ual. Technical report, September.

de Marneffe, Marie-Catherine, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the IEEE / ACL 2006 Workshop on
Spoken Language Technology.

Johansson, Richard and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proceedings of NODALIDA 2007, Tartu,
Estonia, May 25-26.

Kim, Jin-Dong, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview
of bionlp’09 shared task on event extraction.
In
BioNLP ’09: Proceedings of
the Workshop on
BioNLP, pages 1–9.

Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In ACL ’03: Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics, pages 423–430, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.

McClosky, David. 2009. Any Domain Parsing: Au-
tomatic Domain Adaptation for Natural Language
Parsing. Ph.D. thesis, Department of Computer Sci-
ence, Brown University.

Miwa, Makoto, Sampo Pyysalo, Tadayoshi Hara, and
Jun’ichi Tsujii. 2010a. A comparative study of syn-
tactic parsers for event extraction. In BioNLP2010:
Proceedings of the Workshop on BioNLP, Uppsala,
Sweden, July.

Miwa, Makoto, Rune Sætre, Jin-Dong Kim, and
Jun’ichi Tsujii. 2010b. Event extraction with com-
plex event classiﬁcation using rich features. Jour-
nal of Bioinformatics and Computational Biology
(JBCB), 8(1):131–146, February.

Miyao, Yusuke, Kenji Sagae, Rune Sætre, Takuya
Matsuzaki, and Jun ichi Tsujii.
2009. Evalu-
ating contributions of natural language parsers to
protein-protein interaction extraction. Bioinformat-
ics, 25(3):394–400.

¨Onder Eker. 2009. Parser evaluation using textual
entailments. Master’s thesis, Bo˘gazic¸i ¨Universitesi,
August.

Pyysalo, Sampo, Filip Ginter, Veronika Laippala, Ka-
tri Haverinen, Juho Heimonen, and Tapio Salakoski.
2007. On the uniﬁcation of syntactic annotations
under the stanford dependency scheme: A case
study on bioinfer and genia. In Biological, transla-
tional, and clinical language processing, pages 25–
32, Prague, Czech Republic, June. Association for
Computational Linguistics.

Riedel, Sebastian, Hong-Woo Chun, Toshihisa Takagi,
and Jun’ichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In BioNLP ’09:
Proceedings of the Workshop on BioNLP, pages 41–
49, Morristown, NJ, USA. Association for Compu-
tational Linguistics.

Rimell, Laura and Stephen Clark. 2009. Porting a
lexicalized-grammar parser to the biomedical do-
main. J. of Biomedical Informatics, 42(5):852–865.

Rimell, Laura, Stephen Clark, and Mark Steedman.
2009. Unbounded dependency recovery for parser
evaluation. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 813–821, Singapore, August. Asso-
ciation for Computational Linguistics.

Sagae, Kenji and Jun’ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In EMNLP-CoNLL 2007.

Tateisi, Yuka, Akane Yakushiji, Tomoko Ohta, and
Junﬁchi Tsujii. 2005. Syntax annotation for the ge-
nia corpus.
In Proceedings of the IJCNLP 2005,
Companion volume, pages 222–227, Jeju Island,
Korea, October.

Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun’ichi Tsujii. 2005. Developing a robust
part-of-speech tagger for biomedical text. In Boza-
nis, Panayiotis and Elias N. Houstis, editors, Pan-
hellenic Conference on Informatics, volume 3746 of
Lecture Notes in Computer Science, pages 382–392.
Springer.

