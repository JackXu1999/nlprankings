



















































Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks


Proceedings of NAACL-HLT 2016, pages 515–520,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Sequential Short-Text Classification with
Recurrent and Convolutional Neural Networks

Ji Young Lee∗
MIT

jjylee@mit.edu

Franck Dernoncourt∗
MIT

francky@mit.edu

Abstract
Recent approaches based on artificial neural
networks (ANNs) have shown promising re-
sults for short-text classification. However,
many short texts occur in sequences (e.g., sen-
tences in a document or utterances in a dia-
log), and most existing ANN-based systems
do not leverage the preceding short texts when
classifying a subsequent one. In this work,
we present a model based on recurrent neural
networks and convolutional neural networks
that incorporates the preceding short texts.
Our model achieves state-of-the-art results on
three different datasets for dialog act predic-
tion.

1 Introduction

Short-text classification is an important task in
many areas of natural language processing, includ-
ing sentiment analysis, question answering, or dia-
log management. Many different approaches have
been developed for short-text classification, such
as using Support Vector Machines (SVMs) with
rule-based features (Silva et al., 2011), combin-
ing SVMs with naive Bayes (Wang and Manning,
2012), and building dependency trees with Con-
ditional Random Fields (Nakagawa et al., 2010).
Several recent studies using ANNs have shown
promising results, including convolutional neural
networks (CNNs) (Kim, 2014; Blunsom et al., 2014;
Kalchbrenner et al., 2014) and recursive neural net-
works (Socher et al., 2012).

Most ANN systems classify short texts in iso-
lation, i.e., without considering preceding short

∗ These authors contributed equally to this work.

texts. However, short texts usually appear in se-
quence (e.g., sentences in a document or utter-
ances in a dialog), and therefore using information
from preceding short texts may improve the clas-
sification accuracy. Previous works on sequential
short-text classification are mostly based on non-
ANN approaches, such as Hidden Markov Models
(HMMs) (Reithinger and Klesen, 1997; Stolcke et
al., 2000; Surendran and Levow, 2006), maximum
entropy (Ang et al., 2005), naive Bayes (Lendvai
and Geertzen, 2007), and conditional random fields
(CRFs) (Kim et al., 2010; Quarteroni et al., 2011).

Inspired by the performance of ANN-based sys-
tems for non-sequential short-text classification, we
introduce a model based on recurrent neural net-
works (RNNs) and CNNs for sequential short-text
classification, and evaluate it on the dialog act classi-
fication task. A dialog act characterizes an utterance
in a dialog based on a combination of pragmatic, se-
mantic, and syntactic criteria. Its accurate detection
is useful for a range of applications, from speech
recognition to automatic summarization (Stolcke et
al., 2000). Our model achieves state-of-the-art re-
sults on three different datasets.

2 Model

Our model comprises two parts. The first part gener-
ates a vector representation for each short text using
either the RNN or CNN architecture, as discussed in
Section 2.1 and Figure 1. The second part classifies
the current short text based on the vector representa-
tions of the current as well as a few preceding short
texts, as presented in Section 2.2 and Figure 2.

We denote scalars with italic lowercases (e.g.,

515



Pooling

RNNRNNRNN RNN

Pooling

ConvConvConv Conv

Figure 1: RNN (left) and CNN (right) architectures for generating the vector representation s of a short text x1:`. For CNN, Conv
refers to convolution operations, and the filter height h = 3 is used in this figure.

FF1

S2VS2VS2V

FF1

S2VS2VS2V

FF1 FF1

FF2

FF1

S2VS2VS2V

FF1

FF2FF2

FF1

S2V

FF2

Figure 2: Four instances of the two-layer feedforward ANN used for predicting the probability distribution over the classes zi for
the ith short-text Xi. S2V stands for short text to vector, which is the RNN/CNN architecture that generates si from Xi. From left
to right, the history sizes (d1, d2) are (0, 0), (2, 0), (0, 2) and (1, 1). (0, 0) corresponds to the non-sequential classification case.

k, bf ), vectors with bold lowercases (e.g., s, xi),
and matrices with italic uppercases (e.g., Wf ). We
use the colon notation vi:j to denote the sequence of
vectors (vi,vi+1, . . . ,vj).

2.1 Short-text representation

A given short text of length ` is represented as the se-
quence of m-dimensional word vectors x1:`, which
is used by the RNN or CNN model to produce the
n-dimensional short-text representation s.

2.1.1 RNN-based short-text representation

We use a variant of RNN called Long Short Term
Memory (LSTM) (Hochreiter and Schmidhuber,
1997). For the tth word in the short-text, an LSTM
takes as input xt,ht−1, ct−1 and produces ht, ct
based on the following formulas:

it = σ(Wixt + Uiht−1 + bi)
ft = σ(Wfxt + Ufht−1 + bf )
c̃t = tanh(Wcxt + Ucht−1 + bc)
ct = ft � ct−1 + it � c̃t
ot = σ(Woxt + Uoht−1 + bo)
ht = ot � tanh(ct)

where Wj ∈ Rn×m, Uj ∈ Rn×n are weight matri-
ces and bj ∈ Rn are bias vectors, for j ∈ {i, f, c, o}.
The symbols σ(·) and tanh(·) refer to the element-
wise sigmoid and hyperbolic tangent functions, and
� is the element-wise multiplication. h0 = c0 = 0.

In the pooling layer, the sequence of vectors h1:`
output from the RNN layer are combined into a sin-
gle vector s ∈ Rn that represents the short-text, us-
ing one of the following mechanisms: last, mean,
and max pooling. Last pooling takes the last vector,
i.e., s = h`, mean pooling averages all vectors, i.e.,
s = 1`

∑`
t=1 ht, and max pooling takes the element-

wise maximum of h1:`.

2.1.2 CNN-based short-text representation
Using a filter Wf ∈ Rh×m of height h, a convolu-
tion operation on h consecutive word vectors start-
ing from tth word outputs the scalar feature

ct = ReLU(Wf •Xt:t+h−1 + bf )
where Xt:t+h−1 ∈ Rh×m is the matrix whose ith
row is xi ∈ Rm, and bf ∈ R is a bias. The symbol •
refers to the dot product and ReLU(·) is the element-
wise rectified linear unit function.

We perform convolution operations with n dif-
ferent filters, and denote the resulting features as

516



ct ∈ Rn, each of whose dimensions comes from a
distinct filter. Repeating the convolution operations
for each window of h consecutive words in the short-
text, we obtain c1:`−h+1. The short-text representa-
tion s ∈ Rn is computed in the max pooling layer,
as the element-wise maximum of c1:`−h+1.

2.2 Sequential short-text classification
Let si be the n-dimensional short-text representation
given by the RNN or CNN architecture for the ith

short text in the sequence. The sequence si−d1−d2 : i
is fed into a two-layer feedforward ANN that pre-
dicts the class for the ith short text. The hyperpa-
rameters d1, d2 are the history sizes used in the first
and second layers, respectively.

The first layer takes as input si−d1−d2 : i and out-
puts the sequence yi−d2 : i defined as

yj = tanh

(
d1∑

d=0

W−d sj−d + b1

)
, ∀j ∈ [i− d2, i]

where W0,W−1,W−2 ∈ Rk×n are the weight ma-
trices, b1 ∈ Rk is the bias vector, yj ∈ Rk is the
class representation, and k is the number of classes
for the classification task.

Similarly, the second layer takes as input the se-
quence of class representations yi−d2:i and outputs
zi ∈ Rk:

zi = softmax

 d2∑
j=0

W−j yi−j + b2


where U0, U−1, U−2 ∈ Rk×k and b2 ∈ Rk are the
weight matrices and bias vector.

The final output zi represents the probability dis-
tribution over the set of k classes for the ith short-
text: the jth element of zi corresponds to the proba-
bility that the ith short-text belongs to the jth class.

3 Datasets and Experimental Setup

3.1 Datasets
We evaluate our model on the dialog act classifica-
tion task using the following datasets:

• DSTC 4: Dialog State Tracking Challenge 4 (Kim
et al., 2015; Kim et al., 2016).

• MRDA: ICSI Meeting Recorder Dialog Act Cor-
pus (Janin et al., 2003; Shriberg et al., 2004). The
5 classes are introduced in (Ang et al., 2005).

• SwDA: Switchboard Dialog Act Corpus (Jurafsky
et al., 1997).

For MRDA, we use the train/validation/test splits
provided with the datasets. For DSTC 4 and SwDA,
only the train/test splits are provided.1 Table 1
presents statistics on the datasets.

Dataset |C| |V | Train Validation Test
DSTC 4 89 6k 24 (21k) 5 (5k) 6 (6k)

MRDA 5 12k 51 (78k) 11 (16k) 11 (15k)

SwDA 43 20k 1003 (193k) 112 (23k) 19 (5k)

Table 1: Dataset overview. |C| is the number of classes, |V |
the vocabulary size. For the train, validation and test sets, we
indicate the number of dialogs (i.e., sequences) followed by the
number of utterances (i.e., short texts) in parenthesis.

3.2 Training
The model is trained to minimize the negative log-
likelihood of predicting the correct dialog acts of the
utterances in the train set, using stochastic gradient
descent with the Adadelta update rule (Zeiler, 2012).
At each gradient descent step, weight matrices, bias
vectors, and word vectors are updated. For regular-
ization, dropout is applied after the pooling layer,
and early stopping is used on the validation set with
a patience of 10 epochs.

4 Results and Discussion

To find effective hyperparameters, we varied one hy-
perparameter at a time while keeping the other ones
fixed. Table 2 presents our hyperparameter choices.

Hyperparameter Choice Experiment Range
LSTM output dim. (n) 100 50 – 1000

LSTM pooling max max, mean, last

LSTM direction unidir. unidir., bidir.

CNN num. of filters (n) 500 50 – 1000

CNN filter height (h) 3 1 – 10

Dropout rate 0.5 0 – 1

Word vector dim. (m) 200, 300 25 – 300

Table 2: Experiments ranges and choices of hyperparameters.
Unidir refers to the regular RNNs presented in Section 2.1.1,
and bidir refers to bidirectional RNNs introduced in (Schuster
and Paliwal, 1997).

We initialized the word vectors with the 300-
dimensional word vectors pretrained with word2vec

1All train/validation/test splits can be found at https://
github.com/Franck-Dernoncourt/naacl2016

517



d1

d2 LSTM CNN
0 1 2 0 1 2

DSTC4
0 63.1 (62.4, 63.6) 65.7 (65.6, 65.7) 64.7 (63.9, 65.3) 64.1 (63.5, 65.2) 65.4 (64.7, 66.6) 65.1 (63.2, 65.9)

1 65.8 (65.5, 66.1) 65.7 (65.3, 66.1) 64.8 (64.6, 65.1) 65.3 (64.1, 65.9) 65.1 (62.1, 66.2) 64.9 (64.4, 65.6)
2 65.7 (65.0, 66.2) 65.5 (64.4, 66.1) 64.9 (64.6, 65.2) 65.7 (64.9, 66.3) 65.8 (65.2, 66.1) 65.4 (64.5, 66.0)

MRDA
0 82.8 (82.4, 83.1) 83.2 (82.9, 83.4) 82.9 (82.4, 83.4) 83.2 (83.0, 83.4) 83.5 (82.9, 84.0) 83.8 (83.4, 84.2)

1 83.2 (82.6, 83.7) 83.8 (83.5, 84.4) 83.6 (83.2, 83.8) 84.6 (84.5, 84.9) 84.6 (84.4, 84.8) 84.1 (83.8, 84.4)
2 84.1 (83.5, 84.4) 83.9 (83.4, 84.7) 83.3 (82.6, 84.2) 84.4 (84.1, 84.8) 84.6 (84.5, 84.7) 84.4 (84.2, 84.7)

SwDA
0 66.3 (65.1, 68.0) 67.9 (66.3, 68.6) 67.8 (66.7, 69.0) 67.0 (65.3, 68.7) 69.1 (68.5, 70.0) 69.7 (69.2, 70.9)

1 68.4 (67.8, 68.8) 67.8 (65.5, 68.9) 67.3 (65.5, 69.5) 69.9 (69.1, 70.9) 69.8 (69.3, 70.6) 69.9 (68.8, 70.6)

2 69.5 (68.9, 70.2) 67.9 (66.5, 69.4) 67.7 (66.9, 68.9) 71.4 (70.4, 73.1) 71.1 (70.2, 72.1) 70.9 (69.7, 71.7)

Table 3: Accuracy (%) on different architectures and history sizes d1, d2. For each setting, we report average (minimum, maximum)
computed on 5 runs. Sequential classification (d1 + d2 > 0) outperforms non-sequential classification (d1 = d2 = 0). Overall,
the CNN model outperformed the LSTM model for all datasets, albeit by a small margin except for SwDA. We also tried gated
recurrent units (GRUs) (Cho et al., 2014) and the basic RNN, but the results were generally lower than LSTM.

on Google News (Mikolov et al., 2013a; Mikolov
et al., 2013b) for DSTC 4, and the 200-dimensional
word vectors pretrained with GloVe on Twit-
ter (Pennington et al., 2014) for MRDA and
SwDA, as these choices yielded the best results
among all publicly available word2vec, GloVe,
SENNA (Collobert, 2011; Collobert et al., 2011)
and RNNLM (Mikolov et al., 2011) word vectors.

The effects of the history sizes d1 and d2 for the
short-text and the class representations, respectively,
are presented in Table 3 for both the LSTM and
CNN models. In both models, increasing d1 while
keeping d2 = 0 improved the performances by 1.3-
4.2 percentage points. Conversely, increasing d2
while keeping d1 = 0 yielded better results, but the
performance increase was less pronounced: incor-
porating sequential information at the short-text rep-
resentation level was more effective than at the class
representation level.

Using sequential information at both the short-
text representation level and the class representa-
tion level does not help in most cases and may even
lower the performances. We hypothesize that short-
text representations contain richer and more gen-
eral information than class representations due to
their larger dimension. Class representations may
not convey any additional information over short-
text representations, and are more likely to propa-
gate errors from previous misclassifications.

Table 4 compares our results with the state-of-the-
art. Overall, our model shows competitive results,
while requiring no human-engineered features. Rig-

orous comparisons are challenging to draw, as many
important details such as text preprocessing and
train/valid/test split may vary, and many studies fail
to perform several runs despite the randomness in
some parts of the training process, such as weight
initialization.

Model DSTC 4 MRDA SwDA
CNN 65.5 84.6 73.1
LSTM 66.2 84.3 69.6
Majority class 25.8 59.1 33.7

SVM 57.0 – –

Graphical model – 81.3 –

Naive Bayes – 82.0 –

HMM – – 71.0

Memory-based Learning – – 72.3

Interlabeler agreement – – 84.0

Table 4: Accuracy (%) of our models and other methods from
the literature. The majority class model predicts the most fre-
quent class. SVM: (Dernoncourt et al., 2016). Graphical model:
(Ji and Bilmes, 2006). Naive Bayes: (Lendvai and Geertzen,
2007). HMM: (Stolcke et al., 2000). Memory-based Learn-
ing: (Rotaru, 2002). All five models use features derived from
transcribed words, as well as previous predicted dialog acts ex-
cept for Naive Bayes. The interlabeler agreement could not be
obtained for MRDA, and DSTC 4 was labeled by a single an-
notator. For the CNN and LSTM models, the presented results
are the test set accuracy of the run with the highest accuracy on
the validation set.

5 Conclusion

In this article we have presented an ANN-based ap-
proach to sequential short-text classification. We
demonstrate that adding sequential information im-

518



proves the quality of the predictions, and the per-
formance depends on what sequential information is
used in the model. Our model achieves state-of-the-
art results on three different datasets for dialog act
prediction.

Acknowledgments

We warmly thank Regina Barzilay, Tommi Jaakkola
and the anonymous reviewers for their helpful feed-
back and suggestions.

References
[Ang et al.2005] Jeremy Ang, Yang Liu, and Elizabeth

Shriberg. 2005. Automatic dialog act segmentation
and classification in multiparty meetings. In ICASSP
(1), pages 1061–1064.

[Blunsom et al.2014] Phil Blunsom, Edward Grefenstette,
Nal Kalchbrenner, et al. 2014. A convolutional neu-
ral network for modelling sentences. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics. Proceedings of the 52nd
Annual Meeting of the Association for Computational
Linguistics.

[Cho et al.2014] Kyunghyun Cho, Bart van Merriënboer,
Dzmitry Bahdanau, and Yoshua Bengio. 2014. On
the properties of neural machine translation: Encoder-
decoder approaches. arXiv preprint arXiv:1409.1259.

[Collobert et al.2011] Ronan Collobert, Jason Weston,
Léon Bottou, Michael Karlen, Koray Kavukcuoglu,
and Pavel Kuksa. 2011. Natural language process-
ing (almost) from scratch. The Journal of Machine
Learning Research, 12:2493–2537.

[Collobert2011] Ronan Collobert. 2011. Deep learn-
ing for efficient discriminative parsing. In Interna-
tional Conference on Artificial Intelligence and Statis-
tics, number EPFL-CONF-192374.

[Dernoncourt et al.2016] Franck Dernoncourt, Ji Young
Lee, Trung H. Bui, and Hung H. Bui. 2016. Adobe-
MIT submission to the DSTC 4 Spoken Language Un-
derstanding pilot task. In 7th International Workshop
on Spoken Dialogue Systems (IWSDS).

[Hochreiter and Schmidhuber1997] Sepp Hochreiter and
Jürgen Schmidhuber. 1997. Long short-term memory.
Neural computation, 9(8):1735–1780.

[Janin et al.2003] Adam Janin, Don Baron, Jane Edwards,
Dan Ellis, David Gelbart, Nelson Morgan, Barbara Pe-
skin, Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke,
et al. 2003. The ICSI meeting corpus. In Acous-
tics, Speech, and Signal Processing, 2003. Proceed-
ings.(ICASSP’03). 2003 IEEE International Confer-
ence on, volume 1, pages I–364. IEEE.

[Ji and Bilmes2006] Gang Ji and Jeff Bilmes. 2006.
Backoff model training using partially observed data:
application to dialog act tagging. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 280–
287. Association for Computational Linguistics.

[Jurafsky et al.1997] Dan Jurafsky, Elizabeth Shriberg,
and Debra Biasca. 1997. Switchboard SWBD-
DAMSL shallow-discourse-function annotation
coders manual. Institute of Cognitive Science
Technical Report, pages 97–102.

[Kalchbrenner et al.2014] Nal Kalchbrenner, Edward
Grefenstette, and Phil Blunsom. 2014. A convolu-
tional neural network for modelling sentences. arXiv
preprint arXiv:1404.2188.

[Kim et al.2010] Su Nam Kim, Lawrence Cavedon, and
Timothy Baldwin. 2010. Classifying dialogue acts
in one-on-one live chats. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 862–871. Association for
Computational Linguistics.

[Kim et al.2015] Seokhwan Kim, Luis Fernando D’Haro,
Rafael E. Banchs, Jason Williams, and Matthew Hen-
derson. 2015. Dialog State Tracking Challenge 4:
Handbook.

[Kim et al.2016] Seokhwan Kim, Luis Fernando D’Haro,
Rafael E. Banchs, Jason Williams, and Matthew Hen-
derson. 2016. The Fourth Dialog State Tracking Chal-
lenge. In Proceedings of the 7th International Work-
shop on Spoken Dialogue Systems (IWSDS).

[Kim2014] Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. In Proceedings of
the 2014 Conference on Empirical Methods in Natural
Language Processing, pages 1746–1751. Association
for Computational Linguistics.

[Lendvai and Geertzen2007] Piroska Lendvai and Jeroen
Geertzen. 2007. Token-based chunking of turn-
internal dialogue act sequences. In Proceedings of the
8th SIGDIAL Workshop on Discourse and Dialogue,
pages 174–181.

[Mikolov et al.2011] Tomas Mikolov, Stefan Kombrink,
Anoop Deoras, Lukar Burget, and Jan Cernocky.
2011. Rnnlm-recurrent neural network language mod-
eling toolkit. In Proc. of the 2011 ASRU Workshop,
pages 196–201.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efficient estima-
tion of word representations in vector space. arXiv
preprint arXiv:1301.3781.

[Mikolov et al.2013b] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases and

519



their compositionality. In Advances in neural infor-
mation processing systems, pages 3111–3119.

[Nakagawa et al.2010] Tetsuji Nakagawa, Kentaro Inui,
and Sadao Kurohashi. 2010. Dependency tree-based
sentiment classification using CRFs with hidden vari-
ables. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
786–794. Association for Computational Linguistics.

[Pennington et al.2014] Jeffrey Pennington, Richard
Socher, and Christopher D Manning. 2014. GloVe:
global vectors for word representation. Proceedings
of the Empiricial Methods in Natural Language
Processing (EMNLP 2014), 12:1532–1543.

[Quarteroni et al.2011] Silvia Quarteroni, Alexei V
Ivanov, and Giuseppe Riccardi. 2011. Simultaneous
dialog act segmentation and classification from
human-human spoken conversations. In Acoustics,
Speech and Signal Processing (ICASSP), 2011 IEEE
International Conference on, pages 5596–5599. IEEE.

[Reithinger and Klesen1997] Norbert Reithinger and
Martin Klesen. 1997. Dialogue act classification
using language models. In EuroSpeech. Citeseer.

[Rotaru2002] Mihai Rotaru. 2002. Dialog act tagging us-
ing memory-based learning. Term project, University
of Pittsburgh, pages 255–276.

[Schuster and Paliwal1997] Mike Schuster and Kuldip K
Paliwal. 1997. Bidirectional recurrent neural net-
works. Signal Processing, IEEE Transactions on,
45(11):2673–2681.

[Shriberg et al.2004] Elizabeth Shriberg, Raj Dhillon,
Sonali Bhagat, Jeremy Ang, and Hannah Carvey.
2004. The ICSI meeting recorder dialog act (MRDA)
corpus. Technical report, DTIC Document.

[Silva et al.2011] Joao Silva, Luı́sa Coheur, Ana Cristina
Mendes, and Andreas Wichert. 2011. From symbolic
to sub-symbolic information in question classification.
Artificial Intelligence Review, 35(2):137–154.

[Socher et al.2012] Richard Socher, Brody Huval,
Christopher D Manning, and Andrew Y Ng. 2012.
Semantic compositionality through recursive matrix-
vector spaces. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 1201–1211. Association
for Computational Linguistics.

[Stolcke et al.2000] Andreas Stolcke, Klaus Ries, Noah
Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel
Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-
Dykema, and Marie Meteer. 2000. Dialogue act
modeling for automatic tagging and recognition of
conversational speech. Computational linguistics,
26(3):339–373.

[Surendran and Levow2006] Dinoj Surendran and Gina-
Anne Levow. 2006. Dialog act tagging with support
vector machines and hidden markov models. In IN-
TERSPEECH.

[Wang and Manning2012] Sida Wang and Christopher D
Manning. 2012. Baselines and bigrams: Simple, good
sentiment and topic classification. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics: Short Papers-Volume 2, pages
90–94. Association for Computational Linguistics.

[Zeiler2012] Matthew D Zeiler. 2012. Adadelta:
An adaptive learning rate method. arXiv preprint
arXiv:1212.5701.

520


