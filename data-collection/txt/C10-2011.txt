90

Coling 2010: Poster Volume, pages 90–98,

Beijing, August 2010

Automatic Acquisition of Lexical Formality

Julian Brooke, Tong Wang, and Graeme Hirst

Department of Computer Science

University of Toronto

{jbrooke,tong,gh}@cs.toronto.edu

Abstract

There has been relatively little work fo-
cused on determining the formality level
of individual lexical items. This study
applies information from large mixed-
genre corpora, demonstrating that signif-
icant improvement is possible over simple
word-length metrics, particularly when
multiple sources of information, i.e. word
length, word counts, and word associ-
ation, are integrated. Our best hybrid
system reaches 86% accuracy on an En-
glish near-synonym formality identiﬁca-
tion task, and near perfect accuracy when
comparing words with extreme formality
differences. We also test our word as-
sociation method in Chinese, a language
where word length is not an appropriate
metric for formality.

Introduction

1
The derivation of lexical resources for use in
computational applications has been focused pri-
marily on the denotational relationships among
words, e.g. the synonym and hyponym relation-
ships encapsulated in WordNet (Fellbaum, 1998).
Largely missing from popular lexical resources
such as WordNet and the General Inquirer (Stone
et al., 1966) is stylistic information;
there are,
for instance, no resources which provide com-
prehensive information about the formality level
of words, which relates to the appropriateness
of a word in a given context. Consider, for
example,
the problem of choice among near-
synonyms: there are only minor denotational dif-
ferences among synonyms such as get, acquire,

obtain, and snag, but it is difﬁcult to construct a
situation where any choice would be equally suit-
able. The key difference between these words is
their formality, with acquire the most formal and
snag the most informal.

In this work, we conceive of formality as
a continuous property.
This approach is in-
spired by resources such as Choose The Right
Word (Hayakawa, 1994), in which differences be-
tween synonyms are generally described in rela-
tive rather than absolute terms, as well as linguis-
tic literature in which the quantiﬁcation of stylis-
tic differences among genres is framed in terms of
dimensions rather than discrete properties (Biber,
1995). We begin by deﬁning the formality score
for a word as a real number value in the range 1
to −1, with 1 representing an extremely formal
word, and −1 an extremely informal word. A
formality lexicon, then, gives a FS score to every
word within its coverage.

The core of our approach to the problem of
classifying lexical formality is the automated cre-
ation of formality lexicons from large corpora. In
this paper, we focus on the somewhat low-level
task of identifying the relative formality of word
pairs; we believe, however, that a better under-
standing of lexical formality is relevant to a num-
ber of problems in computational linguistics, in-
cluding sub-ﬁelds such as text generation, error
correction of (ESL) writing, machine translation,
text classiﬁcation, text simpliﬁcation, word-sense
disambiguation, and sentiment analysis. One con-
clusion of our research is that formality variation
is omnipresent in natural corpora, but it does not
follow that the identiﬁcation of these differences
on the lexical level is a trivial one; nevertheless,

91

we are able to make signiﬁcant progress using the
methods presented here, in particular the applica-
tion of latent semantic analysis to blog corpora.

2 Related Work
As far as we are aware, there are only a few
lines of research explicitly focused on the ques-
tion of linguistic formality. In linguistics proper,
the study of register and genre usually involves
a number of dimensions or clines, sometimes
explicitly identiﬁed as formality (Leckie-Tarry,
1995; Carter, 1998), or decomposed into notions
such as informational versus interpersonal con-
tent (Biber, 1995). Heyligen and Dewaele (1998)
provide a part-of-speech based quantiﬁcation of
textual contextuality (which they argue is funda-
mental to the notion of formality); their metric
has been used, for instance, in a computational
investigation of the formality of online encyclo-
pedias (Emigh and Herring, 2005). In this kind
of quantiﬁcation, however, there is little, if any,
focus on individual elements of the lexicon.
In
computational linguistics, formality has received
attention in the context of text generation (Hovy,
1990); of particular note relevant to our research
is the work of Inkpen and Hirst (2006), who de-
rive boolean formality tags from Choose the Right
Word (Hayakawa, 1994). Like us, their focus was
improved word choice, though the approach was
much broader, also including dimensions such as
polarity. An intriguing example of formality rel-
evant to text classiﬁcation is the use of infor-
mal language (slang) to help distinguish true news
from satire (Burfoot and Baldwin, 2009).

Our approach to this task is inspired and in-
formed by automatic lexical acquisition research
within the ﬁeld of sentiment analysis (Turney
and Littman, 2003; Esuli and Sebastiani, 2006;
Taboada and Voll, 2006; Rao and Ravichandra,
2009). Turney and Littman (2003) apply latent
semantic analysis (LSA) (Landauer and Dumais,
1997) and pointwise mutual information (PMI) to
derive semantic orientation ratings for words us-
ing large corpora; like us, they found that LSA
was a powerful technique for deriving this lexical
information. The lexical database SentiWordNet
(Esuli and Sebastiani, 2006) provides 0–1 rank-
ings for positive, negative, and neutral polarity,

derived automatically using relationships between
words in WordNet (Fellbaum, 1998). Unfortu-
nately, WordNet synsets tend to cut across the for-
mal/informal distinction, and so the resource is
not obviously useful for our task.

The work presented here builds directly on a pi-
lot study (Brooke et al., 2010), the focus of which
was the construction of formality score (FS) lex-
icons.
In that work, we employed less sophis-
ticated forms of some of the methods used here
in a relatively small dataset (the Brown Corpus),
providing a proof of concept, but with poor cov-
erage, and with no attempt to combine the meth-
ods to maximize performance. However, the small
dataset allowed us to do a thorough test of certain
options associated with our task. In particular we
found that using a similarity metric based on LSA
gave good performance across our test sets, es-
pecially when the term-document matrix was bi-
nary (unweighted), the k-value used for LSA was
small, and the method used to derive a formality
score was cosine similarity to our seed terms. A
metric using total word counts in corpora with di-
vergent formality also showed promise, with both
methods performing above our word-length base-
line for words within their coverage. PMI, by
comparison, proved less effective, and we do not
pursue it further here.

3 Data and Resources

3.1 Word Lists
All the word lists discussed here are publicly
available.1 We begin with two, one formal and
one informal, that we use both as seeds for our
lexicon construction methods and as test sets for
evaluation (our gold standard). We assume that
all slang terms are by their very nature informal
and so our 138 informal seeds were taken primar-
ily from an online slang dictionary2 (e.g. wuss,
grubby) and also include some contractions and
interjections (e.g. cuz, yikes). The 105 formal
seeds were selected from a list of discourse mark-
ers (e.g. moreover, hence) and adverbs from a sen-
timent lexicon (e.g. preposterously, inscrutably);
these sources were chosen to avoid words with
1 http://www.cs.toronto.edu/∼jbrooke/FormalityLists.zip
2 http://onlineslangdictionary.com/

92

overt topic, and to ensure that there was some
balance of sentiment across formal and informal
seed sets. Part of speech, however, is not balanced
across our seed sets.

Another test set we use to evaluate our methods
is a collection of 399 pairs of near-synonyms from
Choose the Right Word (CTRW), a manual for as-
sisting writers with synonym word choice; each
pair was either explicitly or implicitly compared
for formality in the book. Implicit comparison in-
cluded statements such as this is the most formal
of these words; in those cases, and more gener-
ally, we avoided words appearing in more than
one comparison (there are no duplicate words in
our CTRW set), as well as multiword expressions
and words whose formality is strongly ambigu-
ous (i.e. word-sense dependent). An example of
this last phenomenon is the word cool, which is
used colloquially in the sense of good but more
formally as in the sense of cold. Partly as a re-
sult of this polysemy, which is clearly more com-
mon among informal words, our pairs are biased
toward the formal end of the spectrum; although
there are some informal comparisons, e.g. belly-
ache/whine, wisecrack/joke, more typical pairs
include determine/ascertain and hefty/ponderous.
Despite this imbalance, one obvious advantage
of using near-synonyms in our evaluation is that
factors other than linguistic formality (e.g. topic,
opinion) are less likely to inﬂuence performance.
In general, the CTRW allows for a more objective,
ﬁne-grained evaluation of our methods, and is ori-
ented towards our primary interest, near-synonym
word choice.

To test the performance of our unsupervised
method beyond English, one of the authors (a na-
tive speaker of Mandarin Chinese) created two
sets of Chinese two-character words, one formal,
one informal, based on but not limited to the
words in the English sets. The Chinese seeds in-
clude 49 formal seeds and 43 informal seeds.

pus. Although extremely small by modern cor-
pus standards (only 1 million words), the Brown
Corpus has the advantage of being compiled ex-
plicitly to represent a range of American English,
though it is all of the published, written variety.
The Switchboard (SW) Corpus is a collection of
American telephone conversations (Godfrey et al.,
1992), which contains roughly 2400 conversations
with over 2.6 million word tokens; we use it as an
informal counterpart to the Brown Corpus. Like
the Brown Corpus, The British National Corpus
(Burnard, 2000) is a manually-constructed mixed-
genre corpus; it is, however, much larger (roughly
100 million words). It contains a written portion
(90%), which we use as a formal corpus, and a
spontaneous spoken portion (4.3%), which we use
as an informal corpus. Our other mixed corpora
are two blog collections available to us: the ﬁrst,
which we call our development blog corpus (Dev-
Blog) contains a total of over 900,000 English
blogs, with 216 million tokens.3 The second is the
‘ﬁrst tier’ English blogs included in the publicly
available ICSWM 2009 Spinn3r Dataset (Burton
et al., 2009), a total of about 1.3 billion word to-
kens in 7.5 million documents. For our investiga-
tions in Chinese, we use the Chinese portion of the
ICSWM blogs, approximately 25.4 million char-
acter tokens in 86,000 documents.

4 Methods
4.1 Simple Formality Measures
The simplest kind of formality measure is based
on word length, which is often used directly as
an indicator of formality for applications such as
genre classiﬁcation (Karlgren and Cutting, 1994).
Here, we use logarithmic scaling to derive a FS
score based on word length. Given a maximum
word length L4 and a word w of length l, the for-
mality score function, FS(w), is given by:

FS(w) = −1 + 2

logl
logL

3.2 Corpora
Our corpora fall generally into three categories:
formal (written) copora, informal (spoken) cor-
pora, and mixed corpora. The Brown Corpus
(Francis and Kuˇcera, 1982), our development cor-
pus, is used here both as a formal and mixed cor-

3These blogs were gathered by the University of Toronto
Blogscope project (www.blogscope.net) over a week in May
2008.

4We use an upper bound of 28 characters, which is
the length of antidisestablishmentarianism, the prototypical
longest word in English; this value of L provides an appropri-
ate formality/informality threshold, between 5- and 6-letter
words

93

For hyphenated terms, the length of each compo-
nent is averaged. Though this metric works rela-
tively well for English, we note that it is problem-
atic in a language with signiﬁcant word aggluti-
nation (e.g. German) or without an alphabet (e.g.
Chinese, see below).

Another straightforward method is the assump-
tion that Latinate preﬁxes and sufﬁxes are indica-
tors of formality in English (Kessler et al., 1997),
i.e. informal words will not have Latinate afﬁxes
such as -ation and intra-. Here, we simply assign
words that appear to have such a preﬁx or sufﬁx
an FS of 1, and all other words an FS of −1.
Our frequency methods derive FS from word
counts in corpora. Our ﬁrst, naive approach as-
sumes a single corpus, where either formal words
are common and informal words are rare, or vice
versa. To smooth out the Zipﬁan distribution, we
use the frequency rank of words as exponentials;
for a corpus with R frequency ranks, the FS for a
word of rank r under the formal is rare assumption
is given by:

FS(w) = −1 + 2

e(r−1)
e(R−1)

Under the informal is rare assumption:

FS(w) = 1− 2

e(r−1)
e(R−1)

We have previously shown that these methods are
not particularly effective on their own (Brooke et
al., 2010), but we note that they provide useful
information for a hybrid system.

A more sophisticated method is to use two cor-
pora that are known to vary with respect to for-
mality and use the relative appearance of words in
each corpus as the metric. If word appears n times
in a (relatively) formal corpus and m times in an
informal corpus (and one of m, n is not zero), we
derive:

FS(w) = −1 + 2

n

m× N + n

Here, N is the ratio of the size (in tokens) of the
informal corpus (IC) to the formal corpus (FC).
We need the constant N so that an imbalance in
the size of the corpora does not result in an equiv-
alently skewed distribution of FS.

4.2 Latent Semantic Analysis
Next, we turn to LSA, a technique for extracting
information from a large corpus of texts by (dras-
tically) reducing the dimensionality of a term–
document matrix, i.e. a matrix where the row vec-
tors correspond to the appearance or (weighted)
frequency of words in a set of texts. In essence,
LSA simpliﬁes the variation of words across a col-
lection of texts, exploiting document–document
correlation to produce information about the k
most important dimensions of variation (k < to-
tal number of documents), which are generally
thought to represent semantic concepts, i.e. topic.
The mathematical basis for this transformation is
singular value decomposition5; for the details of
the matrix transformations, we refer the reader to
the discussion of Turney and Littman (2003). The
factor k, the number of columns in the compacted
matrix, is an important variable in any application
of LSA, one is generally determined by trial and
error (Turney and Littman, 2003).

LSA is computationally intensive; in order to
apply it to extremely large blog corpora, we need
to ﬁlter the documents and terms before build-
ing our term–document matrix. We adopt the
following strategy: to limit the number of docu-
ments in our term–document matrix, we ﬁrst re-
move documents less than 100 tokens in length,
with the rationale that these documents provide
less co-occurrence information. Second, we re-
move documents that either do not contain any
target words (i.e. one of our seeds or CTRW test
words), or contain only target words which are
among the most common 20 in the corpus; these
documents are less likely to provide us with use-
ful information, and the very common target terms
will be well represented regardless. We further
shrink the set of terms by removing all hapax
legomena; a single appearance in a corpus is not
enough to provide reliable co-occurrence informa-
tion, and roughly half the words in our blog cor-
pora appear only once. Finally, we remove sym-
bols and all words which are not entirely lower

5We use the implementation included in Matlab; we take
the rows of the decomposed U matrix weighted by the sin-
gular values in Σ for our word vectors. Using no weights
or Σ−1 generally resulted in worse performance, particularly
with the CTRW sets.

94

case; we are not interested, for instance, in num-
bers, acronyms, and proper nouns. We can esti-
mate the effect this ﬁltering has on performance
by testing it both ways in a development corpus.
Once a k-dimensional vector for each relevant
word is derived using LSA, a standard method is
to use the cosine of the angle between a word vec-
tor and the vectors of seed words to identify how
similar the distribution of the word is to the distri-
bution of the seeds. To begin, each formal seed is
assigned a FS value of 1, each informal seed a FS
value of −1, and then a raw seed similarity score
(FS0) is calculated for each word w:

FS0(w) = ∑
s∈S,s6=w

Ws × FS(s)× cos(θ (w,s))

S is the set of all seeds. Note that seed terms are
excluded from their own FS calculation, this is
equivalent to leave-one-out cross-validation. Ws
is a weight that depends on whether s is a formal
or informal seed, Wi (for informal seeds) is calcu-
lated as:

Wi =

∑ f∈F FS( f )

|∑i∈I FS(i)| + ∑ f∈F FS( f )

and Wf (for formal seeds) is:

Wf =

|∑i∈I FS(i)|

|∑i∈I FS(i)| + ∑ f∈F FS( f )

Here, I is the set of all informal seeds, and F is the
set of all formal seeds. These weights have the ef-
fect of countering any imbalance in the seed set,
as formal and informal seeds ultimately have the
same (potential) inﬂuence on each word, regard-
less of their count. This weighting is necessary for
the iterative extension of this method discussed in
the next section.

We calculate the ﬁnal FS score as follows:

FS(w) =

FS0(w)− FS0(r)

Nw

The word r is a reference term, a common func-
tion word that has no formality.6 This has the ef-
fect of countering any (moderate) bias that might
6The particular choice of this word is relatively unimpor-
tant; common function words all have essentially the same
LSA vectors because they appear at least once in nearly ev-
ery document of any size. For English, we chose r = and,
and for Chinese, r = yinwei (because); there does not seem
to be an obvious two-character, formality-neutral equivalent
to and in Chinese.

exist in the corpus; in the Brown Corpus, for in-
stance, function words have positive formality be-
fore this step, simply because formal words oc-
curred more often in the corpus. Nw is a normal-
ization factor, either

Nw = max

wi∈I0 |FS0(wi)− FS0(r)|

for all wi ∈ I0 or

Nw = max

w f∈F0|FS0(w f )− FS0(r)|

for all w f ∈ F0. I0 contains all words w such that
FS0(w)− FS0(r) < 0, and F0 contains all words w
such that FS0(w)− FS0(r) > 0. This ensures that
the resulting lexicon has terms exactly in the range
1 to −1, with the reference word r at the midpoint.
We also tested the LSA method in Chinese.
The only major relevant difference between Chi-
nese and English is word segmentation: Chinese
does not have spaces between words. To sidestep
this problem, we simply included all character bi-
grams found in our corpus. The drawback of this
approach in the inclusion of a huge number of
nonsense ‘words’ (1.3 million terms in just 86,000
documents), however we are at least certain to
identify all instances of our seeds.

4.3 Hybrid Methods
There are a number of ways to leverage the infor-
mation we derive from our basic methods. One
intriguing option is to use the basic FS measures
as the starting point for an iterative process using
the LSA cosine similarity. Under this paradigm,
all words in the starting FS lexicon are potential
seed words; we choose a cutoff value for inclu-
sion in the seed word set (e.g. words which have
at least .5 or −.5 FS), and then carry out the co-
sine calculations, as above, to derive new FS val-
ues (a new FS lexicon). We can repeat this process
as many times as required, with the idea that the
connections between various words (as reﬂected
in their LSA-derived vectors) will cause the sys-
tem to converge towards the true FS values.

A simple hybrid method that combines the two
word count models uses the ratio of word counts
in two corpora to deﬁne the center of the FS spec-
trum, but single corpus methods to deﬁne the ex-
tremes. Formally, if m and n (word counts for the

95

informal corpus IC and formal corpus FC, respec-
tively) are both non-zero, then FS is given by:

n

FS(w) = −0.5 +

m× N + n
However, if n is zero, FS is given by:
e√rIC−1
e√RIC−1

FS(w) = −1 + 0.5

where rIC is the frequency rank of the word in IC,
and RIC is the total number of ranks in IC. If m is
zero, FS is given by:

FS(w) = 1− 0.5

e√rFC−1
e√RFC−1

where i is the rank of the word in IC, and RIC is the
total number of frequency ranks in IC). This func-
tion is undeﬁned in the case where m and n are
both zero. Intuitively, this is a kind of backoff, re-
lying on the idea that words of extreme formality
are rare even in a corpus of corresponding formal-
ity, whereas words in the core vocabulary (Carter,
1998), which are only moderately formal, will ap-
pear in all kinds of corpora, and thus are amenable
to the ratio method.

Finally, we explore a number of ways to com-
bine lexicons directly. The motivation for this
is that the lexicons have different strengths and
weaknesses, representing partially independent
information. An obvious method is an averag-
ing or other linear combination of the scores, but
we also investigate vote-based methods (requiring
agreement among n dictionaries). Beyond these
simple options, we test support vector machines
and naive Bayes classiﬁcation using the WEKA
software suite (Witten and Frank, 2005), applying
10-fold cross-validation using default WEKA set-
tings for each classiﬁer. The features here are task
dependent (see Section 5); for the pairwise task,
we use the difference between the FS value of the
words in each lexicon, rather than their individ-
ual scores. Finally, we can use the weights from
the SVM model of the CTRW (pairwise) task to
interpolate an optimal formality lexicon.

5 Evaluation
We evaluate our methods using the gold standard
judgments from the seed sets and CTRW word

pairs. To differentiate the two, we continue to use
the term seed for the former; in this context, how-
ever, these ‘seed sets’ are being viewed as a test
set (recall that our LSA method is equivalent to
leave-one-out cross-validation).

We derive the following measures: ﬁrst, the
coverage (Cov.) is the percentage of words in the
set that are covered under the method. The class-
based accuracy (C-Acc.) of our seed sets is the
percentage of covered words which are correctly
classiﬁed as formal (FS > 0) or informal (FS <
0). The pair-based accuracy (P-Acc.) is the result
of exhaustively pairing words in the two seed sets
and testing their relative formality; that is, for all
wi ∈ I and w f ∈ F, the percentage of wi/w f pairs
where FS(wi) < FS(w f ). For the CTRW pairs
there are only two metrics, the coverage and the
pair-based accuracy; since the CTRW pairs repre-
sent relative formality of varying degrees, it is not
possible to calculate a class-based accuracy.

The ﬁrst section of Table 1 provides the re-
sults for the basic methods in various corpora.
The word length (1) and morphology-based (2)
methods provide good coverage, but poor accu-
racy, while the word count ratio methods (3–4) are
fairly accurate, but suffer from low coverage. The
LSA results in Table 1 are the best for each corpus
across the k values we tested. When both cover-
age and accuracy are considered, there is a clear
beneﬁt associated with increasing the amount of
data, though the difference between the Dev-Blog
and ICWSM suggests diminishing returns. The
performance of the ﬁltered Dev-Blog is actually
slightly better than the unﬁltered versions (though
there is a drop in coverage), suggesting that ﬁlter-
ing is a good strategy.

In our previous work (Brooke et al., 2010), we
noted that CTRW set performance in the Brown
dropped for k > 3, while performance on the seed
set was mostly steady as k increased. Figure 1
shows the pairwise performance of each test set
for various corpora across various k. The results
here are similar; all three corpora reach a CTRW
maximum at a relatively low k values (though
higher than Brown Corpus); however the seed set
performance in each corpus continues to improve
(though marginally) as k increases, while CTRW
performance drops. An explanation for this is that

96

Table 1: Seed coverage, class-based accuracy, pairwise accuracy, CTRW coverage, and pairwise accu-
racy for various FS lexicons and hybrid methods (%).

Method

Simple
(1) Word length
(2) Latinate afﬁx
(3) Word count ratio, Brown and Switchboard
(4) Word count ratio, BNC Written vs. Spoken
(5) LSA (k=3), Brown
(6) LSA (k=10), BNC
(7) LSA (k=20), Dev-Blog
(8) LSA (k=20), Dev-Blog, ﬁltered
(9) LSA (k=20), ICWSM, ﬁltered
Hybrid
(10) BNC ratio with backoff (4)
(11) Combined ratio with backoff (3 + 4)
(12) BNC weighted average (10,6), ratio 2:1
(13) Blog weighted average (9,7), ratio 4:1
(14) Voting, 3 agree (1, 6, 7, 9, 11)
(15) Voting, 2 agree (1, 11, 13)
(16) Voting, 2 agree (1, 12, 13)
(17) SVM classiﬁer (1, 2, 6, 7, 9, 11)
(18) Naive Bayes classiﬁer (1, 2, 6, 7, 9, 11)
(19) SVM (Seed, class) weighted (1, 2, 6, 7, 9, 11)
(20) SVM (CTRW) weighted (1, 6, 7, 9, 11)
(21) Average (1, 6, 7, 9, 11)

Seed set

Cov. C-Acc. P-Acc.

CTRW set
Cov. P-Acc.

100
100
38.0
60.9
51.0
94.7
100
99.0
100

97.1
97.1
97.1
100
92.6
86.8
87.7
100
100
100
100
100

86.4
74.5
81.5
89.2
87.1
83.0
91.4
92.1
93.0

78.8
79.2
83.5
93.8
99.1
99.1
98.6
97.9
97.5
98.4
93.0
95.9

91.8
46.3
85.7
97.3
94.2
98.3
96.8
97.0
98.4

75.7
79.9
90.0
98.5
99.9
100
100
99.9
99.8
99.8
99.0
99.5

100
100
36.0
38.8
59.6
96.5
99.0
97.7
99.7

97.0
97.5
97.0
99.7
87.0
81.5
82.7
100
100
100
100
100

63.7
32.6
78.2
74.3
73.9
69.4
80.5
80.5
81.9

78.8
79.9
83.2
83.4
91.6
96.9
97.3
84.2
83.9
80.5
86.0
84.5

the seed terms represent extreme examples of for-
mality; thus there are numerous semantic dimen-
sions to distinguish them. However, the CTRW
set includes near-synonyms, many with only rel-
atively subtle differences in formality; for these
pairs,
it is important to focus on the core di-
mensions relevant to formality, which are among
the ﬁrst discovered in a factor analysis of mixed-
register texts (Biber, 1995).

With regards to hybrid methods, we ﬁrst brieﬂy
summarize our testing with the iterative model,
which included extensive experiments using ba-
sic lexicons and the LSA vectors derived from
the Brown Corpus, and some targeted testing with
the blog corpora (iteration on these corpora is
extraordinarily time-consuming). In general, we
found only that there were only small, inconsis-
tent beneﬁts to be gained from the iterative ap-

Figure 1: Seed and CTRW pairwise accuracy,
LSA method for large corpora k, 10 ≤ k ≤ 200.

97

proach. More generally, the intuition behind the
iterative method, i.e. that performance would in-
crease with an drastic increase in the number of
seeds, was found to be ﬂawed: in other testing,
we found that we could randomly remove most
of the seeds without negatively affecting perfor-
mance. Even at relatively high k values, it seems
that a few seeds are enough to calibrate the model.
The ratio (with backoff) hybrid built from the
BNC (10) provides CTRW performance that is
comparable the best LSA models, though perfor-
mance in the seed sets is somewhat poor; supple-
menting with word counts from the Brown Cor-
pus and Switchboard Corpus provides a small im-
provement (11). The weighed hybrid dictionar-
ies in (12,13) demonstrate that it is possible to ef-
fectively combine lexicons built using two differ-
ent methods on the same corpus (12) or the same
method on different corpora (13); the former, in
particular, provides an impressive boost to CTRW
accuracy, indicating that word count and word as-
sociation methods are partially independent.

The remainder of Table 1 shows the best re-
sults using voting, averaging, and weighting. The
voting results (14–16) indicate that it is possible
to sacriﬁce some coverage for very high accu-
racy in both sets, including a near-perfect score
in the seed sets and signiﬁcant gains in CTRW
performance. In general, the best accuracy with-
out a signiﬁcant loss of coverage came from 2
of 3 voting (15–16), using dictionaries that rep-
resented our three basic sources of information
(word length, word count, and word associa-
tion). The machine learning hybrids (17–18) also
demonstrate a marked improvement over any sin-
gle lexicon, though it is important to note that
each accuracy score here reﬂects a different task-
speciﬁc model. Hybrid FS lexicons built with the
weights learned by the SVM models (19–20) pro-
vide superior performance on the task correspond-
ing to the model used, though the simple averag-
ing of the best dictionaries (21) also provides good
performance across all evaluation metrics.

Finally, the LSA results for Chinese are mod-
est but promising, given the relatively small scale
of our experiments: we saw a pairwise accuracy of
82.2%, with 79.3% class-based accuracy (k = 10).
We believe that the main reason for the generally

lower performance in Chinese (as compared to
English) is the modest size of the corpus, though
our simplistic character bigram term extraction
technique may also play a role. As mentioned,
smaller seed sets do not seem to be an issue. Inter-
estingly, the class-based accuracy is 10.8% lower
if no reference word is used to calibrate the divide
between formal and informal, suggesting a rather
biased corpus (towards informality); in English,
by comparison, the reference-word normalization
had a slightly negative effect on the LSA results,
though the effect mostly disappeared after hy-
bridization. The obvious next step is to integrate a
Chinese word segmenter, and use a larger corpus.
We could also try word count methods, though
ﬁnding appropriate (balanced) resouces similar to
the BNC might be a challenge; (mixed) blog cor-
pora, on the other hand, are easily collected.

6 Conclusion
In this work, we have experimented with a number
of different methods and source corpora for deter-
mining the formality level of lexical items, with
the implicit goal of distinguishing the formality of
near-synonym pairs. Our methods show marked
improvement over simple word-length metrics;
when multiple sources of information, i.e. word
length, word counts, and word association, are in-
tegrated, we are able to reach over 85% perfor-
mance on the near-synonym task, and close to
100% accuracy when comparing words with ex-
treme formality differences; our voting methods
show that even higher precision is possible. We
have also demonstrated that our LSA word associ-
ation method can be applied to a language where
word length is not an appropriate metric of for-
mality, though the results here are preliminary.
Other potential future work includes addressing a
wider range of phenomena, for instance assign-
ing formality scores to morphological elements,
syntactic cues, and multi-word expressions, and
demonstrating that a formality lexicon can be use-
fully applied to other NLP tasks.

Acknowledgements
This work was supported by Natural Sciences and
Engineering Research Council of Canada. Thanks
to Paul Cook for his ICWSM corpus API.

98

References
Biber, Douglas. 1995. Dimensions of Register Vari-
ation: A cross-linguistic comparison. Cambridge
University Press.

Brooke, Julian, Tong Wang, and Graeme Hirst. 2010.
Inducing lexicons of formality from corpora.
In
Proceedings of the Language Resources and Eval-
uation Conference (LREC ’10), Workshop on Meth-
ods for the automatic acquisition of Language Re-
sources and their evaluation methods.

Burfoot, Clint and Timothy Baldwin. 2009. Auto-
matic satire detection: Are you having a laugh? In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the Association for Computationsl
Linguistics and the 4th International Joint Confer-
ence on Nautral Language Processing of the Asian
Federation of Natural Language Processing (ACL-
IJCNLP ’09), Short Papers, Singapore.

Burnard, Lou. 2000. User reference guide for British
National Corpus. Technical report, Oxford Univer-
sity.

Burton, Kevin, Akshay Java, and Ian Soboroff. 2009.
The ICWSM 2009 Spinn3r Dataset. In Proceedings
of the Third Annual Conference on Weblogs and So-
cial Media (ICWSM 2009), San Jose, CA.

Carter, Ronald. 1998. Vocabulary: applied linguistic

perspectives. Routledge, London.

Emigh, William and Susan C. Herring. 2005. Col-
laborative authoring on the web: A genre analysis
of online encyclopedias. In Proceedings of the 38th
Annual Hawaii International Conference on System
Sciences (HICSS ’05).

Esuli, Andrea and Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Interna-
tion Conference on Language Resources and Eval-
uation(LREC), Genova, Italy.

Fellbaum, Christiane, editor. 1998. WordNet: An

Electronic Lexical Database. The MIT Press.

Francis, Nelson and Henry Kuˇcera. 1982. Frequency
Analysis of English Usage: Lexicon and Grammar.
Houghton Mifﬂin, Boston.

Godfrey, J.J., E.C. Holliman, and J. McDaniel. 1992.
Switchboard: telephone speech corpus for research
and development.
IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
1:517–520.

Hayakawa, S.I., editor. 1994. Choose the Right Word.
HarperCollins Publishers, second edition. Revised
by Eugene Ehrlich.

Heylighen, Francis and Jean-Marc Dewaele. 2002.
Variation in the contextuality of language: An em-
pirical measure. Foundations of Science, 7(3):293–
340.

Hovy, Eduard H. 1990. Pragmatics and natural lan-
guage generation. Artiﬁcial Intelligence, 43:153–
197.

Inkpen, Diana and Graeme Hirst. 2006. Building and
using a lexical knowledge base of near-synonym
differences. Computational Linguistics, 32(2):223–
262.

Karlgren, Jussi and Douglas Cutting. 1994. Recog-
nizing text genres with simple metrics using dis-
criminant analysis. In Proceedings of the 15th Con-
ference on Computational Linguistics, pages 1071–
1075.

Kessler, Brett, Geoffrey Nunberg,

and Hinrich
Sch¨utze. 1997. Automatic detection of text genre.
In Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics, pages
32–38.

Landauer, Thomas K. and Susan Dumais. 1997. A so-
lution to Plato’s problem: The latent semantic anal-
ysis theory of the acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104:211–240.

Leckie-Tarry, Helen. 1995. Language Context: a

functional linguistic theory of register. Pinter.

Rao, Delip and Deepak Ravichandra. 2009. Semi-
supervised polarity lexicon induction.
In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
gusitics, Athens, Greece.

Stone, Philip J., Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilivie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press.

Taboada, Maite and Kimberly Voll. 2006. Methods
In
for creating semantic orientation dictionaries.
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC), Gen-
ova, Italy.

Turney, Peter and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21:315–346.

Witten, Ian H. and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, San Francisco.

