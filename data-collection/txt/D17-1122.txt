



















































Inter-Weighted Alignment Network for Sentence Pair Modeling


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1179–1189
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Inter-Weighted Alignment Network for Sentence Pair Modeling

Gehui Shen Yunlun Yang Zhi-Hong Deng∗
Key Laboratory of Machine Perception (Ministry of Education),

School of Electronics Engineering and Computer Science, Peking University,
Beijing 100871, China

{jueliangguke, incomparable-lun, zhdeng}@pku.edu.cn

Abstract

Sentence pair modeling is a crucial prob-
lem in the field of natural language pro-
cessing. In this paper, we propose a model
to measure the similarity of a sentence
pair focusing on the interaction informa-
tion. We utilize the word level similarity
matrix to discover fine-grained alignment
of two sentences. It should be emphasized
that each word in a sentence has a different
importance from the perspective of seman-
tic composition, so we exploit two novel
and efficient strategies to explicitly calcu-
late a weight for each word. Although
the proposed model only use a sequen-
tial LSTM for sentence modeling with-
out any external resource such as syntactic
parser tree and additional lexicon features,
experimental results show that our model
achieves state-of-the-art performance on
three datasets of two tasks.

1 Introduction

Given two pieces of sentences S and T , sentence
pair modeling (SPM) is a fundamental task whose
applications include question answering (Lin,
2007), natural language inference (Bowman et al.,
2015), paraphrase identification (Socher et al.,
2011a) and sentence completion (Wan et al., 2016)
and so on. In general, each of the two sentences
are firstly mapped to a representation, and then
a model is designed to determine the relation be-
tween them. Traditional methods use lexicon fea-
tures such as Bag-of-Words(BOW) to map sen-
tences. As we know, features design and selection
are time-consuming and high dimensional features
may suffer from sparsity because of the varia-
tion of linguistic. Recently, deep learning tech-

∗Corresponding author

niques have been applied to develop end-to-end
models for NLP tasks, such as sentence model-
ing (Socher et al., 2011b; Kim, 2014), relation
classification (Socher et al., 2012) and machine
translation (Sutskever et al., 2014). These works
show that deep learning models can be compara-
ble with hand-crafted features based models and
often outperform them.

Existing DNN models are based on pre-trained
word embeddings which map each word to one
low dimensional vector and compose word em-
beddings to represent sentence. Some models
are developed directly from the sentence models.
They obtain single vector representation for each
sentence separately and then determine the rela-
tion based on two vectors (Huang et al., 2013; Qiu
and Huang, 2015; Palangi et al., 2016). Because
of the absence of interaction, these models can not
achieve state-of-the-art performance.

Inspired by attention mechanism in computer
vision and machine translation, some elaborate
models have been proposed (Rocktäschel et al.,
2016; Zhou et al., 2016; Wang and Jiang, 2016)
which take interaction information into consid-
eration. Meanwhile, to grasp the fine-grained
information for semantic similarity, some prior
works (Pang et al., 2016; He and Lin, 2016) firstly
compute a word level similarity matrix according
to word representation, and utilize multiple convo-
lution layers and extract features from the similar-
ity matrix in a perspective of image recognition.

In this paper, we focus on solving SPM problem
by measuring semantic similarity between two
sentences. We propose a new deep learning model
based on two facts that previous works always ne-
glected. As we know, in the aspect of semantic,
each word in the sentence is of different impor-
tance. When calculating a sentence representation
we should endow each word with a weight indi-
cating its importance. Taking following sentences

1179



as an example:
A: a man with a red helmet is riding a

motorbike along a roadway.
B: a man is riding a motorbike along a

roadway.
C: a man with a red helmet is riding a

bicycle along a roadway.
We can see that sentence A is more similar with
sentence B than with sentence C while a conven-
tional model probably makes an opposite conclu-
sion because the phrase ”with a red helmet” will
bias the meaning of A to C meanwhile the dif-
ference between ”motorbike” and ”bicycle” is not
large enough. If the model can realize that the
phrase ”with a red helmet” has little effect on se-
mantic composition, the mistake will be avoided.
Since we have to analyse a pair of sentences, the
weights should be related to not only the sentence
itself, but also its partner. From this point, we pro-
pose a novel inter-weighted layer to measure the
importance of each word.

On the other hand, the more similar two sen-
tences are, the more probably we can align each
word of sentence S with several words of sentence
T , and vice versa. On account of the variety of ex-
pression, the position and length of two aligned
parts are very likely different, so we apply soft-
alignment mechanism and build an effective align-
ment layer.

In summary, our contributions are as follows:

1. We propose an Inter-Weighted Alignment
Network (IWAN) for SPM, which builds an
alignment layer to compute similarity score
according to the degree of alignment.

2. Considering the importance of each word in
a sentence is different, we argue that an inter-
weighted layer for evaluating the weight of
each word is crucial to semantic composi-
tion. We propose two strategies for calcu-
lating weights. Experimental results demon-
strate their effectiveness.

3. Experimental results on semantic relatedness
benchmark dataset SICK and two answer se-
lection datasets show that proposed model
achieves state-of-the-art performances with-
out any external information.

2 Related Work

2.1 Sentence Models

For sentence modeling, RNN (Elman, 1990;
Mikolov et al., 2010) and CNN (Kim, 2014) are
both powerful and widely used. RNN models a
sentence sequentially by updating the hidden state
which represents context recurrently. As sentence
length grows, RNN will suffer from gradient van-
ishing problem. However, gated mechanism, such
as Long Short Term Memory(LSTM) (Hochre-
iter and Schmidhuber, 1997) is introduced to ad-
dress it. RecNN exploits syntatic information and
models sentences under a tree structure. Gated
mechanism can also improve the performance of
RecNN (Tai et al., 2015). CNN can extract
and combine important local context meanwhile
model sentences in a hierarchical way (Kim, 2014;
Kalchbrenner et al., 2014). All of the above mod-
els can be adapted to SPM by modeling two sen-
tences separately.

2.2 Attentive Models

Hermann et al. (2015) firstly introduces atten-
tion mechanism into question answering under an
RNN architecture. Rocktäschel et al. (2016) ap-
plies a similar model to natural language inference
which attends over the premise conditioned on the
hypothesis. Zhou et al. (2016) combines attention
mechanism with tree-structured RecNN encoder.
Some prior works (Wang et al., 2016b; Parikh
et al., 2016; Wang et al., 2017) compute soft-
alignment representation for each word in sen-
tences attentively with word level similarity and
then compose the alignment representations to de-
termine the relation. Our model is also under this
framework however we focus on explicitly calcu-
lating weights for each word to get more reason-
able semantic composition.

2.3 Similarity Matrix Based Models

Pang et al. (2016) adopts CNN on word level sim-
ilarity matrix to extract fine-grained matching pat-
terns from different text granularity. He and Lin
(2016) uses a similar architecture with a 19-layer
CNN in order to make full use of its power. Yin
and Schütze (2015) proposes a hierarchical archi-
tecture to model different granularity representa-
tion and computes several similarity matrices for
interaction.

1180



S

Word Embedding

BiLSTM
Similarity

Matrix &

Alignment 

Layer

T

Word Embedding

BiLSTM

Self-Attention Layer Inter-Weighted Layer Self-Attention Layer

Full Connection Layer

SOFTMAX

Figure 1: The architecture of IWAN. The blocks with same color have shared parameters.

3 Proposed Model

Given two sentences S and T , we aim to cal-
culate a score to measure their similarity. Fig-
ure 1 shows the architecture of IWAN model. To
learn representations with context information, we
firstly use a bi-direction LSTM sentence model
which takes word embeddings as inputs to obtain
a context-aware representation for each position
(Sec. 3.1). The context-aware representations are
used to compute the word level similarity matrix
(Sec. 3.2). Inspired by attention mechanism, we
exploit soft-alignment to find semantic counter-
part in one sentence for each position in the other
and compute a weighted sum vector of one sen-
tence as the alignment representation of each po-
sition of the other with an alignment layer (Sec.
3.3). Meanwhile, taking the context-aware repre-
sentation of S and T as inputs, we apply an inter-
weighted layer to compute a weight for each posi-
tion in S and T . We argue that this weight can in-
dicate the importance in semantic interaction and a
weighted summation of the representations at each
position is more interpretable than other compo-
sition method including max or average pooling
and LSTM layer. We propose two strategies for
computing those weights (Sec. 3.4). The weighted
vectors are fed to full connection layers and a soft-
max layer is used to give the final prediction(Sec.
3.5).

As Figure 1 illustrates, our model is symmetric
about S and T . So for simplicity, we only describe
the left part of IWAN model which is mainly about
modeling S from here. Right part is exactly same
except the roles of S and T exchange.

3.1 BiLSTM Layer

With pre-trained d dimension word embed-
ding, we can obtain sentence matrices Se =
[s1e, . . . , s

m
e ] and Te = [t

1
e, . . . , t

n
e ] where s

i
e ∈ Rd

is embedding of the i-th word in sentence S. m
and n are the length of S and T respectively. In
order to capture contextual information, we run a
bi-direction LSTM (Hochreiter and Schmidhuber,
1997) on two matrices. Let hidden layer dimen-
sion of LSTM be u. Given the word embedding
xt at time step t, previous hidden vector ht−1 and
cell state ct−1, LSTM recurrently computes ht and
ct as follows:

gt = ϕ(Wgxt + Vght−1 + bg),
it = σ(Wixt + Wiht−1 + bi),
ft = σ(Wfxt + Wfht−1 + bf ),
ot = σ(Woxt + Woht−1 + bo),
ct = gt ⊙ it + ct−1 ⊙ ft,
ht = ct ⊙ ot.

where all W ∈ Ru×d, V ∈ Ru×u and b ∈ Ru. σ
is sigmoid function and ϕ is tanh function. ⊙ indi-
cates the element-wise multiplication of two vec-
tors. The input gates i, forget gates f and output
gates o control information flow self-adaptively,
moreover cell state ct can memorize long-distance
information. ht is regarded as the representation
of time step t.

We feed Se and Te separately into a parame-
ter shared LSTM sentence model. If we run an
LSTM model on the sequence of Se from left to
right, we can get the forward hidden vector Sfh =
[s1fh, . . . , s

m
fh]. For applying bi-direction LSTM,

we also run another LSTM backward and get

1181



Sbh = [s1bh, . . . , s
m
bh]. Then we concatenate them

to one vector representation. So after bi-direction
LSTM layer, we obtain Sh = [s1h, . . . , s

m
h ] and

Th = [t1h, . . . , t
n
h] where s

i
h =

[
sifh
sibh

]
.

3.2 Word Level Similarity Matrix

As mentioned above, the word level similarity ma-
trix is crucial to making use of the interaction in-
formation. Pang et al. (2016) and Wang et al.
(2016b) compute the similarity matrix between
two word embeddings. We have argued that word
embedding can not express the word meaning in
context. From the view of RNN, sifh contains the
most semantic information about i-th word in S
and less about the leftmost words, while sibh also
contains the most semantic information about i-
th word in S and less about the rightmost words.
Therefore, the hidden vector of BiLSTM keeps the
most information of corresponding word as well
as integrated with the context information. Com-
puting similarity matrix between BiLSTM hidden
vectors is expected to improve the interaction re-
sults. We regard the inner dot of two vectors as
their similarity. For the similarity matrix M , its
element Mij indicates the similarity between sih
and tjh:

Mij = siTh · tjh.

3.3 Alignment Layer

We design the alignment layer for an intuitive idea:
more similar S and T are, more probably we can
find semantic counterpart in T for each part in
S, and vice versa. To some degree, people are
likely to find semantic correspondences between
two sentences and evaluate their similarity. He and
Lin (2016) are also inspired by similar intuition,
but they use deep CNN to recognize the alignment
patterns implicitly. However, for each sentence,
we explicitly calculate the alignment representa-
tion and alignment residual which we believe are
good indicators of sentence pair similarity.

For calculating the alignment representation,
we apply attention mechanism (Bahdanau et al.,
2014) to conduct a soft-alignment. The original at-
tention mechanism outputs the alignment weights
from an extra full connection layer while we think
the inner dot can represent the semantic related-
ness adequately. Therefore, we consider the i-th
row of M as the similarity between the i-th posi-
tion of S and each position in T and normalize it

as follows:

αij =
exp(Mij)∑n

k=1 exp(Mik)
, i = 1, . . . , m

while we also normalize each column of M for
T counterpart. αij always belongs to [0, 1] and
can be regarded as weight. Then the alignment
representation Sa = [s1a, . . . , s

m
a ] is computed as a

weighted sum of {tjh}:

sia =
n∑

k=1

αikt
k
h, i = 1, . . . , m

For T counterpart, the alignment representation is
Ta = [t1a, . . . , t

n
a ].

In order to measure the gap between the align-
ment representation and original representation, a
direct strategy is to compute the absolute value of
their difference: sir = |sih − sia|. We call Sr =
[s1r, . . . , s

m
r ] alignment residual which is consid-

ered as alignment feature for subsequent process-
ing.

We also utilize an orthogonal decomposition
strategy which is first proposed by Wang et al.
(2016b): the component sip of s

i
h parallel to s

i
a rep-

resents the alignment component and component
sio orthogonal to s

i
a represents alignment residual.

We compute these two component as follows:

sip =
sih · sia
sia · sia

sia, parallel component

sio = s
i
h − sip, orthogonal component

Then we can replace Sr with Sp and So to measure
the degree of alignment where Sp = [s1p, . . . , s

m
p ]

and So = [s1o, . . . , s
m
o ].

3.4 Inter-Weighted Layer

3.4.1 Inter-Attention Layer
(Lin et al., 2017) firstly proposes a self-attention
sentence model which explicitly computes a
weight for each word and uses the weighted sum-
mation of word representations as sentence em-
bedding. Inspired by this work, we apply a full
connection neural network to measure the impor-
tance to semantic interaction of every word. We
extend the self-attention model to inter-attention
layer in order to compute the weights combined
with interaction information which composing the
alignment representation benefits from. As the

1182



嫌捗朕沈貸怠 嫌捗朕沈 嫌捗朕沈袋怠

嫌長朕沈貸怠 嫌長朕沈 嫌長朕沈袋怠

ぐ

ぐ

ぐ

ぐ

嫌捗朕陳

嫌長朕陳

嫌捗朕怠

嫌長朕怠

concatenate

嫌鎚賃沈椎沈
Element-wise 

subtraction

建捗朕津

建長朕怠

Element-wise 

multiplication

sf鎚賃沈椎沈

Figure 2: The illustration of computing sf iskip.

name suggests, these weights of S are not only de-
pendent on S but also T and the parameters of the
inter-attention layer are shared for S and T .

Formally, we take Sh and Th as inputs and the
inter-attention layer outputs a vector ws with size
m for S:

ws = softmax(w2tanh(W1

[
Sh

(tavg ⊗ em)
]
)),

where tavg = 1n
∑n

k=1 t
k
h and Sh ∈ R2u×m. We

calculate the average of {tkh} as the representation
of T . We also try to replace average operator with
a self-attention layer (Lin et al., 2017) but get a
worse performance. em is a vector of 1s with size
m and ⊗ represents outer product operator. We
feed the concatenated matrix containing pairwise
information into a 2-layer neural network. The pa-
rameter W1 ∈ Rs×4u projects inputs into a hidden
layer with s units. The output layer is parameter-
ized by a vector w2 with size s and a softmax
operator ensures all the element of output sum up
to 1. Then we can use ws to sum up Sr, Sp and So
weightedly across the position dimension:

swr = Sr ∗ (ws)T ,
swp = Sp ∗ (ws)T ,
swo = So ∗ (ws)T .

We can get twr, twp and two in the same way.
We call these inter-features for final prediction.

3.4.2 Inter-Skip Layer
We also explore another novel strategy to com-
pute ws from the intuition that if the i-th word
in S has a low contribution to semantic composi-
tion, we will obtain a similar representation siskip

if we feed all word embeddings sequentially ex-
cept sie into BiLSTM. Unfortunately, the O(m

2)
complexity of running BiLSTM model m times is
too high so we exploit an approximate method to
compute {siskip}:

siskip =
[
si−1fh
si+1bh

]
Then we compute a skip feature as following:

sf iskip = (s
i
skip − Sih)⊙ th,

where th =
[
tnfh
t1bh

]
is the BiLSTM hidden repre-

sentation of T . Figure 2 illustrates how to com-
pute sf iskip. We think the difference between s

i
skip

and sih approximately reflects the contribution the
i-th word makes to semantic composition. On the
one hand, if the difference is small or even close
to zero, the importance of correspond word should
be small. On the other hand, if the difference (a
vector) is not similar to the representation of T ,
correspond word is probably of less importance
in measuring semantic similarity. From these two
points, we think sfskip = [sf1skip, . . . , sf

m
skip] is

a good feature to measure word importance. The
process of computing ws is similar:

ws = softmax(w2tanh(W1sfskip)),

We can use ws outputted by inter-skip layer to ob-
tain inter-features in the same way.

3.5 Output Layer
For more rich information, we combine alignment
information with sentence embeddings of S and T
for final prediction. We run the simple but effec-
tive self-attention (Lin et al., 2017) model on Sh
to obtain its embedding swh:

swh = Sh ∗ (softmax(w′2tanh(W ′1 ∗ Sh)))T ,

where W ′1 and w′2 are trainable. We compute swh
and twh with parameter shared self-attention layer
which is similar with the inter-attention layer ex-
cept inputs.

Following Tai et al. (2015), we compute their
element-wise product h× = swh ⊙ twh and their
absolute difference h+ = |swh − twh| as self-
features. If we use direct strategy, we combine
the features as follows:

hdi = [hT×; h
T
+; s

T
wr; t

T
wr]

T .

1183



Strategy SICK TrecQA WikiQA
r ρ MSE MAP MRR MAP MRR

DI 0.8774 0.8229 0.2374 0.815 0.882 0.724 0.739
OD 0.8810 0.8261 0.2289 0.822 0.889 0.730 0.744

Table 1: Performances of our model with differ-
ent strategies in alignment layer on three datasets.

If we use orthogonal decomposition strategy, we
combine the features as follows:

hod = [hT×; h
T
+; s

T
wp; t

T
wp; s

T
wo; t

T
wo]

T .

Following previous works, the sentence pair mod-
eling problem can always be considered as a clas-
sification task, so we finally calculate a probability
distribution with a 2-layer neural network:

p̂θ = softmax(V2ReLU(V1h + b1) + b2),

where h can be hdi or hod and the hidden size is l.
We use rectified linear units (ReLU) as activation
function.

4 Experimental Setup

4.1 Dataset and Evaluation Metric

To evaluate the proposed model, we conduct ex-
periments on two tasks: semantic relatedness and
answer selection.

For semantic relatedness task, we use the
Sentences Involving Compositional Knowledge
(SICK) dataset (Marelli et al., 2014), which con-
sists of 9927 sentence pairs in a 4500/500/4927
train/dev/test split. The sentences are derived from
existing image and video description and each sen-
tence pair has a relatedness score y ∈ [1, 5], where
the larger score indicates more similarity between
two sentences. As the goal of this task is to cal-
culate sentence pair similarity, we can directly
evaluate our model on SICK. Following previ-
ous works, we use Pearson’s Correlation r, Spear-
man’s Correlation ρ and mean square error (MSE)
as evaluation metrics.

For answer selection task, we experiment on
two datasets: TrecQA and WikiQA. The TrecQA
dataset (Wang et al., 2007) from the Text Retrieval
Conferences has been widely used for the answer
selection task during the past decade. The origi-
nal TrecQA train dataset consists of 1,229 ques-
tions with 53,417 question-answer pairs, 82 ques-
tions with 1,148 pairs in development set, and
100 questions with 1,517 pairs in test set. Recent
works (dos Santos et al., 2016; Rao et al., 2016;

Wang et al., 2016b) removed questions in develop-
ment and test set with no answers or with only pos-
itive/negative answers, thus there are 65 questions
with 1,117 pairs in Clean version development set
and 68 questions with 1,442 pairs in Clean ver-
sion test set. Rao et al. (2016) has showed the per-
formances on Original TrecQA and Clean version
TrecQA are not comparable. Therefore, for a fair
comparison, we only display the results on Clean
version TrecQA which are posted on the website
of Wiki of the Association for Computational Lin-
guistics1. The open domain question-answering
WikiQA (Yang et al., 2015) is constructed from
real queries of Bing and Wikipedia. We follow
Yang et al. (2015) to remove all questions with no
correct candidate answers. The excluded WikiQA
has 873/126/243 questions and 8627/1130/2351
question-answer pairs for train/dev/test split. To
adapt our model to this task, we use semantic sim-
ilarity to measure the probability of matching be-
tween a question and a candidate answer. We eval-
uate models by mean average precision (MAP)
and mean reciprocal rank (MRR).

4.2 Training Details
For experiments on SICK, we follows Tai et al.
(2015) to transform the relatedness score y to a
sparse target distribution p:

pi =


y − ⌊y⌋, i = ⌊y⌋+ 1
⌊y⌋+ 1− y, i = ⌊y⌋
0, otherwise

for 1 ≤ i ≤ 5. The training objective is to mini-
mize the KL-divergence loss between p and p̂θ:

loss =
1
|D|

|D|∑
k=1

KL(p(k) ∥ p̂θ(k))

where |D| is the number of training examples.
We regard the answer selection problem as

“yes” or “no” binary classification and the train-
ing objective is to minimize the negative log-
likelihood in training stage:

loss = − 1|D|
|D|∑
k=1

logp̂θ
(k)(y(k)|x(k))

where x(k) represents a question-answer pair and
y(k) indicates whether the candidate answer is cor-

1https://www.aclweb.org/aclwiki/index.php?title=Question
Answering (State of the art)

1184



rect to the question. In test stage, we sort condi-
date answers for same question in descending or-
der by probability of “yes” category and calculate
MAP and MRR.

In all experiments, we use 300-dimension
GloVe word embeddings2 (Pennington et al.,
2014) and fix the embeddings during training. The
LSTM hidden size u is set to 150. The hidden size
of inter-attention and self-attention layer s and full
connection network l are both set to 50. The L2
regularization strength is set to 3×10−5. We train
the model with Adagrad (Duchi et al., 2011) op-
timization algorithm with a learning rate of 0.05.
The minibatch size is always 25. We exploit early
stopping strategy according to MSE on develop-
ment set for SICK and MAP on development set
for TrecQA and WikiQA.

Model r ρ MSE
Meaning Factory (Jiménez et al.,
2014)

0.8268 0.7721 0.3224

ECNU (Zhao et al., 2014) 0.8414 - -
BiLSTM (Tai et al., 2015) 0.8567 0.7966 0.2736
Tree-LSTM (Tai et al., 2015) 0.8676 0.8083 0.2532
MPCNN (He et al., 2015) 0.8686 0.8047 0.2606

PWIM (He and Lin, 2016) 0.8784 0.8199 0.2329

Att Tree-LSTM (Zhou et al., 2016) 0.8730 0.8117 0.2426
Skip-thought+COCO∗ (Kiros et al.,
2015)

0.8655 0.7995 0.2561

MaLSTM∗◦ (Mueller and
Thyagarajan, 2016)

0.8822 0.8345 0.2286

IWAN-att (Proposed) 0.8810 0.8261 0.2289
IWAN-skip (Proposed) 0.8833 0.8263 0.2236

Table 2: Test results on SICK. The symbol ∗ indi-
cates the models with pre-training. The symbol ◦

indicates the models with data augmentation strat-
egy.

Model MAP MRR
Wang and Ittycheriah (2015) 0.746 0.820
QA-LSTM (Tan et al., 2015) 0.728 0.832
Att-pooling (dos Santos et al., 2016) 0.753 0.851
LDC (Wang et al., 2016b) 0.771 0.845
MPCNN (He et al., 2015) 0.777 0.836
PWIM (He and Lin, 2016) 0.738 0.827
NCE-CNN (Rao et al., 2016) 0.801 0.877
BiMPM (Wang et al., 2017) 0.802 0.875
IWAN-att (Proposed) 0.822 0.889
IWAN-skip (Proposed) 0.801 0.861

Table 3: Test results on Clean version TrecQA.

4.3 Results

Firstly, we evaluate the effectiveness of two strate-
gies in alignment layer. We use inter-attention
model in inter-weighted layer and we find or-

2http://nlp.stanford.edu/projects/glove/

Model MAP MRR
NASM (Miao et al., 2016) 0.689 0.707
Att-pooling (dos Santos et al., 2016) 0.689 0.696
LDC (Wang et al., 2016b) 0.706 0.723
MPCNN (He et al., 2015) 0.693 0.709
PWIM (He and Lin, 2016) 0.709 0.723
NCE-CNN (Rao et al., 2016) 0.701 0.718
IARNN◦ (Wang et al., 2016a) 0.734 0.742
BiMPM (Wang et al., 2017) 0.718 0.731
IWAN-att (Proposed) 0.730 0.744
IWAN-skip (Proposed) 0.733 0.750

Table 4: Test results on WikiQA. The symbol ◦

indicates the models with data augmentation strat-
egy.

thogonal decomposition (OD) strategy has a su-
perior performance to direct (DI) strategy on all
datasets. The comparison results are posted in Ta-
ble 1. In following experiments, we always choose
OD strategy in alignment layer.

Semantic Relatedness Table 3 shows the perfor-
mances of our model and compared models on
SICK dataset. IWAN-att and IWAN-skip repre-
sents our models using inter-attention layer and
inter-skip layer respectively. IWAN-skip outper-
forms IWAN-att in all metrics by a small margin.
The traditional feature engineering based mod-
els in first group have much poorer performances
than deep learning models. MaLSTM (Mueller
and Thyagarajan, 2016) benefits from the data
argumentation strategy with Wordnet informa-
tion and pre-training process. Ablation experi-
ments (Mueller and Thyagarajan, 2016) illustrates
a 0.04 degradation of Pearson’s r without data
argumentation strategy. Therefore it is unfair to
compare with this model directly, but our models
achieve a comparable performance with it. Our
models both outperform all other deep learning
models. IWAN-skip outperforms Attentive Tree-
LSTM (Zhou et al., 2016) by 0.01 in Pearson’s
r, over 0.01 in Spearman’s ρ and almost 0.02
in MSE, although it exploits syntactic parser in-
formation. Our model significantly outperforms
sentence modeling based models with CNN or
RNN which results from the absence of interac-
tion information. He and Lin (2016) proposes
Pairwise Word Interaction Model (PWIM) which
constructs 19-layer CNN on similarity matrix to
capture fine-grained interaction information and
shows most competitive. However our model out-
performs it in all metrics with much fewer parame-
ters (about 0.65 million versus 1.7 million (He and

1185



the animal with big eyes is sleeping the animal with big eyes is eating
words

0.00

0.05

0.10

0.15

0.20

0.25

0.30

0.35

in
te

r-
a
tt

e
n
ti

o
n
 w

e
ig

h
ts

sentence S

sentence T

Figure 3: Visualization of weights outputted by inter-weighted layer of words in a sentence pair in SICK
test set.

Lin, 2016)).

Answer Selection We compare our model with
several state-of-the-art models on Clean version
TrecQA and WikiQA in Table 3 and Table 4 re-
spectively. Our two models both have a state-
of-the-art performance on two datasets. IWAN-
att outperform all previous works on TrecQA and
make a significant improvement of state-of-the-
art. IWAN-skip and IARNN (Wang et al., 2016a)
which solves bias problem of attention mecha-
nism beat all other models on WikiQA, while the
latter is trained on an argumented dataset with
negative sampling. Wang et al. (2016b) first
proposes the orthogonal decomposition but their
LDC model compute the similarity matrix be-
tween word embeddings which are lack of con-
text information and IWAN-att outperforms it dra-
matically by 0.02-0.05 in MAP and MRR on
both datasets. The PWIM (He and Lin, 2016) is
still competitive on WikiQA but gets an inferior
performance on TrecQA. However, our models
both have state-of-the-art performances on three
datasets which demonstrates our models have ex-
cellent generalization ability in different datasets.

4.4 Ablation Tests

Table 5 show the results of ablations tests on SICK
dataset in r metric. From IWAN-att, we remove
or replace one component at a time and evalu-
ate performance of partial models. If removing
inter-features, the r degrades with a 0.013 decline
which proves the interaction information is cru-
cial for sentence pair modeling. Whereas, the
degradation from removing self-features is much

Ablation setting Pearson’s r
Full Model (IWAN-att) 0.8810
• w/o inter-features -0.0130
• w/o self-features -0.0070
• w/o BiLSTM layer -0.0387
• w/o inter-attention layer -0.0075
• Replace inter-attention weights with
self-attention weights -0.0063
• w/o parallel component -0.0037
• w/o orthogonal component -0.0046

Table 5: Ablation test on SICK dataset, removing
each component separately.

smaller. We found a large decline when remov-
ing BiLSTM layer, which confirms our conjecture
that context information is useful. It is worth men-
tioning that He and Lin (2016) posts the degra-
dation of their model from removing BiLSTM is
0.1225 in r which is much larger than 0.0387 of
us. Removing inter-attention layer means we per-
form a mean-pooling on inter-features instead of
a weighted summation. A 0.0075 r degradation
proves importance weighting can result in a signif-
icant improvement. If the weights are only about
single sentence information, the performance still
gets worse. The last two settings show both com-
ponents from orthogonal decomposition are infor-
mative. More or less unexpected, parallel compo-
nent is almost as useful as orthogonal component.

4.5 Visualization of Inter-Weighted Layer
In order to illustrate the effect of inter-weighted
layer in proposed model, we take a sentence pair
in SICK test set as an example and display the
weights outputted by inter-attention layer of each
word in Figure 3. The ground truth of this pair is

1186



3.2 and the prediction given by IWAN-att model
is 3.507 which is much more accurate than 4.356
given by the model without inter-attention layer.
We can find the inter-attention layer gives very
high weights over 0.25 (while the average weight
is about 0.14) to “sleeping” and “eating” which are
the only difference between two sentences. There-
fore, the difference will be attended in following
processing. Meanwhile, the weights of the arti-
cle “the” and the preposition “with” which are not
as important as other real words in semantic com-
position are much lower. These prove the inter-
weighted mechanism is reasonable and effective.

5 Conclusion

This work proposes a weighted alignment model
for sentence pair modeling. We utilize an align-
ment layer to measure the similarity of sen-
tence pairs according to their degree of alignment.
Moreover, we propose an inter-weighted layer to
measure the importance of different parts in sen-
tences. Two strategies for this layer have been ex-
plored which are both effective. The composition
of alignment features can benefit from the inter-
weighted weights. Experiment results shows that
proposed models achieve the state-of-the-art per-
formance on three datasets. In the future work, we
will improve the inter-weighted layer with more
sophisticated module and evaluate our model on
other large scale datasets.

Acknowledgments

This work is partially supported by the National
High Technology Research and Development Pro-
gram of China (Grant No. 2015AA015403) and
the National Natural Science Foundation of China
(Grant No. 61170091). We would also like to
thank the anonymous reviewers for their helpful
comments.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua

Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
ence. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2015, Lisbon, Portugal, September 17-
21, 2015, pages 632–642.

John C. Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159.

Jeffrey L. Elman. 1990. Finding structure in time.
Cognitive Science, 14(2):179–211.

Hua He, Kevin Gimpel, and Jimmy J. Lin. 2015. Multi-
perspective sentence similarity modeling with con-
volutional neural networks. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2015, Lisbon, Portu-
gal, September 17-21, 2015, pages 1576–1586.

Hua He and Jimmy J. Lin. 2016. Pairwise word in-
teraction modeling with deep neural networks for
semantic similarity measurement. In NAACL HLT
2016, The 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, San Diego
California, USA, June 12-17, 2016, pages 937–948.

Karl Moritz Hermann, Tomás Kociský, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in
Neural Information Processing Systems 28: Annual
Conference on Neural Information Processing Sys-
tems 2015, December 7-12, 2015, Montreal, Que-
bec, Canada, pages 1693–1701.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry P. Heck. 2013. Learning
deep structured semantic models for web search us-
ing clickthrough data. In 22nd ACM International
Conference on Information and Knowledge Man-
agement, CIKM’13, San Francisco, CA, USA, Oc-
tober 27 - November 1, 2013, pages 2333–2338.

Sergio Jiménez, George Dueñas, Julia Baquero, and
Alexander F. Gelbukh. 2014. UNAL-NLP: com-
bining soft cardinality features for semantic textual
similarity, relatedness and entailment. In Proceed-
ings of the 8th International Workshop on Semantic
Evaluation, SemEval@COLING 2014, Dublin, Ire-
land, August 23-24, 2014., pages 732–742.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2014, June 22-27, 2014,
Baltimore, MD, USA, Volume 1: Long Papers, pages
655–665.

Kim. 2014. Convolutional neural networks for sen-
tence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1746–1751.

1187



Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,
Richard S. Zemel, Raquel Urtasun, Antonio Tor-
ralba, and Sanja Fidler. 2015. Skip-thought vec-
tors. In Advances in Neural Information Process-
ing Systems 28: Annual Conference on Neural In-
formation Processing Systems 2015, December 7-
12, 2015, Montreal, Quebec, Canada, pages 3294–
3302.

Jimmy J. Lin. 2007. An exploration of the principles
underlying redundancy-based factoid question an-
swering. ACM Trans. Inf. Syst., 25(2).

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos
Santos, Mo Yu, Bing Xiang, Bowen Zhou, and
Yoshua Bengio. 2017. A structured self-attentive
sentence embedding. In Proceedings of the Inter-
national Conference on Learning Representations,
ICLR 2017.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. Semeval-2014 task 1: Evaluation
of compositional distributional semantic models on
full sentences through semantic relatedness and tex-
tual entailment. In Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval@COLING 2014, Dublin, Ireland, August 23-
24, 2014., pages 1–8.

Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neu-
ral variational inference for text processing. In Pro-
ceedings of the 33nd International Conference on
Machine Learning, ICML 2016, New York City, NY,
USA, June 19-24, 2016, pages 1727–1736.

Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan
Cernocký, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, 11th Annual Conference of the
International Speech Communication Association,
Makuhari, Chiba, Japan, September 26-30, 2010,
pages 1045–1048.

Jonas Mueller and Aditya Thyagarajan. 2016. Siamese
recurrent architectures for learning sentence simi-
larity. In Proceedings of the Thirtieth AAAI Con-
ference on Artificial Intelligence, February 12-17,
2016, Phoenix, Arizona, USA., pages 2786–2792.

Hamid Palangi, Li Deng, Yelong Shen, Jianfeng
Gao, Xiaodong He, Jianshu Chen, Xinying Song,
and Rabab K. Ward. 2016. Deep sentence em-
bedding using long short-term memory networks:
Analysis and application to information retrieval.
IEEE/ACM Trans. Audio, Speech & Language Pro-
cessing, 24(4):694–707.

Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu,
Shengxian Wan, and Xueqi Cheng. 2016. Text
matching as image recognition. In Proceedings of
the Thirtieth AAAI Conference on Artificial Intel-
ligence, February 12-17, 2016, Phoenix, Arizona,
USA., pages 2793–2799.

Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016, pages
2249–2255.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Xipeng Qiu and Xuanjing Huang. 2015. Convolutional
neural tensor network architecture for community-
based question answering. In Proceedings of the
Twenty-Fourth International Joint Conference on
Artificial Intelligence, IJCAI 2015, Buenos Aires,
Argentina, July 25-31, 2015, pages 1305–1311.

Jinfeng Rao, Hua He, and Jimmy J. Lin. 2016. Noise-
contrastive estimation for answer selection with
deep neural networks. In Proceedings of the 25th
ACM International on Conference on Information
and Knowledge Management, CIKM 2016, Indi-
anapolis, IN, USA, October 24-28, 2016, pages
1913–1916.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomás Kociský, and Phil Blunsom. 2016.
Reasoning about entailment with neural attention.
In Proceedings of the International Conference on
Learning Representations, ICLR 2016.

Cı́cero Nogueira dos Santos, Ming Tan, Bing Xiang,
and Bowen Zhou. 2016. Attentive pooling net-
works. CoRR, abs/1602.03609.

Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Advances in
Neural Information Processing Systems 24: 25th
Annual Conference on Neural Information Process-
ing Systems 2011. Proceedings of a meeting held
12-14 December 2011, Granada, Spain., pages 801–
809.

Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic compo-
sitionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing, EMNLP-CoNLL 2012, July 12-14, 2012, Jeju
Island, Korea, pages 1201–1211.

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
2011 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP 2011, 27-31 July
2011, John McIntyre Conference Centre, Edinburgh,

1188



UK, A meeting of SIGDAT, a Special Interest Group
of the ACL, pages 151–161.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural In-
formation Processing Systems 2014, December 8-
13 2014, Montreal, Quebec, Canada, pages 3104–
3112.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing of the Asian Federation of
Natural Language Processing, ACL 2015, July 26-
31, 2015, Beijing, China, Volume 1: Long Papers,
pages 1556–1566.

Ming Tan, Bing Xiang, and Bowen Zhou. 2015. Lstm-
based deep learning models for non-factoid answer
selection. CoRR, abs/1511.04108.

Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu,
Liang Pang, and Xueqi Cheng. 2016. A deep ar-
chitecture for semantic matching with multiple po-
sitional sentence representations. In Proceedings of
the Thirtieth AAAI Conference on Artificial Intel-
ligence, February 12-17, 2016, Phoenix, Arizona,
USA., pages 2835–2841.

Bingning Wang, Kang Liu, and Jun Zhao. 2016a. In-
ner attention based recurrent neural networks for an-
swer selection. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2016, August 7-12, 2016, Berlin, Ger-
many, Volume 1: Long Papers.

Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the jeopardy model? A quasi-
synchronous grammar for QA. In EMNLP-CoNLL
2007, Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
June 28-30, 2007, Prague, Czech Republic, pages
22–32.

Shuohang Wang and Jing Jiang. 2016. Learning natu-
ral language inference with LSTM. In NAACL HLT
2016, The 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, San Diego
California, USA, June 12-17, 2016, pages 1442–
1451.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences. CoRR, abs/1702.03814.

Zhiguo Wang and Abraham Ittycheriah. 2015. Faq-
based question answering via word alignment.
CoRR, abs/1507.02628.

Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah.
2016b. Sentence similarity learning by lexical de-
composition and composition. In COLING 2016,
26th International Conference on Computational
Linguistics, Proceedings of the Conference: Tech-
nical Papers, December 11-16, 2016, Osaka, Japan,
pages 1340–1349.

Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
Wikiqa: A challenge dataset for open-domain ques-
tion answering. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portugal,
September 17-21, 2015, pages 2013–2018.

Wenpeng Yin and Hinrich Schütze. 2015. Multi-
grancnn: An architecture for general matching of
text chunks on multiple levels of granularity. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing of the Asian Federation of Natural Lan-
guage Processing, ACL 2015, July 26-31, 2015, Bei-
jing, China, Volume 1: Long Papers, pages 63–73.

Jiang Zhao, Tiantian Zhu, and Man Lan. 2014. ECNU:
one stone two birds: Ensemble of heterogenous
measures for semantic relatedness and textual entail-
ment. In Proceedings of the 8th International Work-
shop on Semantic Evaluation, SemEval@COLING
2014, Dublin, Ireland, August 23-24, 2014., pages
271–277.

Yao Zhou, Cong Liu, and Yan Pan. 2016. Modelling
sentence pairs with tree-structured attentive encoder.
In COLING 2016, 26th International Conference on
Computational Linguistics, Proceedings of the Con-
ference: Technical Papers, December 11-16, 2016,
Osaka, Japan, pages 2912–2922.

1189


