



















































A Hierarchical Latent Structure for Variational Conversation Modeling


Proceedings of NAACL-HLT 2018, pages 1792–1801
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

A Hierarchical Latent Structure for Variational Conversation Modeling

Yookoon Park Jaemin Cho
Department of Computer Science and Engineering & Center for Superintelligence

Seoul National University, Korea
yookoonpark@vision.snu.ac.kr, {jaemin895,gunhee}@snu.ac.kr

http://vision.snu.ac.kr/projects/vhcr

Gunhee Kim

Abstract

Variational autoencoders (VAE) combined
with hierarchical RNNs have emerged as a
powerful framework for conversation model-
ing. However, they suffer from the notori-
ous degeneration problem, where the decoders
learn to ignore latent variables and reduce
to vanilla RNNs. We empirically show that
this degeneracy occurs mostly due to two rea-
sons. First, the expressive power of hierar-
chical RNN decoders is often high enough to
model the data using only its decoding distri-
butions without relying on the latent variables.
Second, the conditional VAE structure whose
generation process is conditioned on a context,
makes the range of training targets very sparse;
that is, the RNN decoders can easily overfit to
the training data ignoring the latent variables.
To solve the degeneration problem, we pro-
pose a novel model named Variational Hier-
archical Conversation RNNs (VHCR), involv-
ing two key ideas of (1) using a hierarchical
structure of latent variables, and (2) exploiting
an utterance drop regularization. With evalu-
ations on two datasets of Cornell Movie Dia-
log and Ubuntu Dialog Corpus, we show that
our VHCR successfully utilizes latent vari-
ables and outperforms state-of-the-art models
for conversation generation. Moreover, it can
perform several new utterance control tasks,
thanks to its hierarchical latent structure.

1 Introduction

Conversation modeling has been a long interest of
natural language research. Recent approaches for
data-driven conversation modeling mostly build
upon recurrent neural networks (RNNs) (Vinyals
and Le, 2015; Sordoni et al., 2015b; Shang et al.,
2015; Li et al., 2017; Serban et al., 2016). Ser-
ban et al. (2016) use a hierarchical RNN struc-
ture to model the context of conversation. Ser-
ban et al. (2017) further exploit an utterance latent

variable in the hierarchical RNNs by incorporat-
ing the variational autoencoder (VAE) framework
(Kingma and Welling, 2014; Rezende et al., 2014).

VAEs enable us to train a latent variable model
for natural language modeling, which grants us
several advantages. First, latent variables can
learn an interpretable holistic representation, such
as topics, tones, or high-level syntactic proper-
ties. Second, latent variables can model inher-
ently abundant variability of natural language by
encoding its global and long-term structure, which
is hard to be captured by shallow generative pro-
cesses (e.g. vanilla RNNs) where the only source
of stochasticity comes from the sampling of output
words.

In spite of such appealing properties of la-
tent variable models for natural language mod-
eling, VAEs suffer from the notorious degenera-
tion problem (Bowman et al., 2016; Chen et al.,
2017) that occurs when a VAE is combined with
a powerful decoder such as autoregressive RNNs.
This issue makes VAEs ignore latent variables,
and eventually behave as vanilla RNNs. Chen
et al. (2017) also note this degeneration issue by
showing that a VAE with a RNN decoder prefers
to model the data using its decoding distribution
rather than using latent variables, from bits-back
coding perspective. To resolve this issue, several
heuristics have been proposed to weaken the de-
coder, enforcing the models to use latent variables.
For example, Bowman et al. (2016) propose some
heuristics, including KL annealing and word drop
regularization. However, these heuristics cannot
be a complete solution; for example, we observe
that they fail to prevent the degeneracy in VHRED
(Serban et al., 2017), a conditional VAE model
equipped with hierarchical RNNs for conversation
modeling.

The objective of this work is to propose a novel
VAE model that significantly alleviates the degen-

1792



eration problem. Our analysis reveals that the
causes of the degeneracy are two-fold. First, the
hierarchical structure of autoregressive RNNs is
powerful enough to predict a sequence of utter-
ances without the need of latent variables, even
with the word drop regularization. Second, we
newly discover that the conditional VAE structure
where an utterance is generated conditioned on
context, i.e. a previous sequence of utterances, in-
duces severe data sparsity. Even with a large-scale
training corpus, there only exist very few target ut-
terances when conditioned on the context. Hence,
the hierarchical RNNs can easily memorize the
context-to-utterance relations without relying on
latent variables.

We propose a novel model named Variational
Hierarchical Conversation RNN (VHCR), which
involves two novel features to alleviate this prob-
lem. First, we introduce a global conversational la-
tent variable along with local utterance latent vari-
ables to build a hierarchical latent structure. Sec-
ond, we propose a new regularization technique
called utterance drop. We show that our hierar-
chical latent structure is not only crucial for facil-
itating the use of latent variables in conversation
modeling, but also delivers several additional ad-
vantages, including gaining control over the global
context in which the conversation takes place.

Our major contributions are as follows:
(1) We reveal that the existing conditional VAE

model with hierarchical RNNs for conversation
modeling (e.g. (Serban et al., 2017)) still suffers
from the degeneration problem, and this problem
is caused by data sparsity per context that arises
from the conditional VAE structure, as well as the
use of powerful hierarchical RNN decoders.

(2) We propose a novel variational hierarchical
conversation RNN (VHCR), which has two dis-
tinctive features: a hierarchical latent structure and
a new regularization of utterance drop. To the best
of our knowledge, our VHCR is the first VAE con-
versation model that exploits the hierarchical la-
tent structure.

(3) With evaluations on two benchmark datasets
of Cornell Movie Dialog (Danescu-Niculescu-
Mizil and Lee, 2011) and Ubuntu Dialog Corpus
(Lowe et al., 2015), we show that our model im-
proves the conversation performance in multiple
metrics over state-of-the-art methods, including
HRED (Serban et al., 2016), and VHRED (Ser-
ban et al., 2017) with existing degeneracy solu-

tions such as the word drop (Bowman et al., 2016),
and the bag-of-words loss (Zhao et al., 2017).

2 Related Work

Conversation Modeling. One popular approach
for conversation modeling is to use RNN-based
encoders and decoders, such as (Vinyals and Le,
2015; Sordoni et al., 2015b; Shang et al., 2015).
Hierarchical recurrent encoder-decoder (HRED)
models (Sordoni et al., 2015a; Serban et al., 2016,
2017) consist of utterance encoder and decoder,
and a context RNN which runs over utterance rep-
resentations to model long-term temporal structure
of conversation.

Recently, latent variable models such as VAEs
have been adopted in language modeling (Bow-
man et al., 2016; Zhang et al., 2016; Serban et al.,
2017). The VHRED model (Serban et al., 2017)
integrates the VAE with the HRED to model Twit-
ter and Ubuntu IRC conversations by introducing
an utterance latent variable. This makes a condi-
tional VAE where the generation process is condi-
tioned on the context of conversation. Zhao et al.
(2017) further make use of discourse act labels to
capture the diversity of conversations.

Degeneracy of Variational Autoencoders. For
sequence modeling, VAEs are often merged with
the RNN encoder-decoder structure (Bowman
et al., 2016; Serban et al., 2017; Zhao et al., 2017)
where the encoder predicts the posterior distribu-
tion of a latent variable z, and the decoder models
the output distributions conditioned on z. How-
ever, Bowman et al. (2016) report that a VAE
with a RNN decoder easily degenerates; that is,
it learns to ignore the latent variable z and falls
back to a vanilla RNN. They propose two tech-
niques to alleviate this issue: KL annealing and
word drop. Chen et al. (2017) interpret this degen-
eracy in the context of bits-back coding and show
that a VAE equipped with autoregressive models
such as RNNs often ignores the latent variable to
minimize the code length needed for describing
data. They propose to constrain the decoder to
selectively encode the information of interest in
the latent variable. However, their empirical re-
sults are limited to an image domain. Zhao et al.
(2017) use an auxiliary bag-of-words loss on the
latent variable to force the model to use z. That
is, they train an auxiliary network that predicts
bag-of-words representation of the target utterance
based on z. Yet this loss works in an opposite di-

1793



rection to the original objective of VAEs that min-
imizes the minimum description length. Thus, it
may be in danger of forcibly moving the informa-
tion that is better modeled in the decoder to the
latent variable.

3 Approach

We assume that the training set consists of N i.i.d
samples of conversations {c1, c2, ..., cN} where
each ci is a sequence of utterances (i.e. sentences)
{xi1,xi2, ...,xini}. Our objective is to learn the
parameters of a generative network θ using Maxi-
mum Likelihood Estimation (MLE):

argmax
θ

∑

i

log pθ(ci) (1)

We first briefly review the VAE, and explain the
degeneracy issue before presenting our model.

3.1 Preliminary: Variational Autoencoder

We follow the notion of Kingma and Welling
(2014). A datapoint x is generated from a latent
variable z, which is sampled from some prior dis-
tribution p(z), typically a standard Gaussian dis-
tribution N (z|0, I). We assume parametric fam-
ilies for conditional distribution pθ(x|z). Since
it is intractable to compute the log-marginal like-
lihood log pθ(x), we approximate the intractable
true posterior pθ(z|x) with a recognition model
qφ(z|x) to maximize the variational lower-bound:

log pθ(x) ≥ L(θ,φ;x) (2)
= Eqφ(z|x)[− log qφ(z|x) + log pθ(x, z)]
= −DKL(qφ(z|x)‖p(z))+Eqφ(z|x)[log pθ(x|z)]

Eq. 2 is decomposed into two terms: KL diver-
gence term and reconstruction term. Here, KL
divergence measures the amount of information
encoded in the latent variable z. In the extreme
where KL divergence is zero, the model com-
pletely ignores z, i.e. it degenerates. The expec-
tation term can be stochastically approximated by
sampling z from the variational posterior qφ(z|x).
The gradients to the recognition model can be ef-
ficiently estimated using the reparameterization
trick (Kingma and Welling, 2014).

3.2 VHRED

Serban et al. (2017) propose Variational Hierarchi-
cal Recurrent Encoder Decoder (VHRED) model

for conversation modeling. It integrates an ut-
terance latent variable zuttt into the HRED struc-
ture (Sordoni et al., 2015a) which consists of three
RNN components: encoder RNN, context RNN,
and decoder RNN. Given a previous sequence
of utterances x1, ...xt−1 in a conversation, the
VHRED generates the next utterance xt as:

henct−1 = f
enc
θ (xt−1) (3)

hcxtt = f
cxt
θ (h

cxt
t−1,h

enc
t−1) (4)

pθ(z
utt
t |x<t) = N (z|µt,σtI) (5)

where µt = MLPθ(h
cxt
t ) (6)

σt = Softplus(MLPθ(h
cxt
t )) (7)

pθ(xt|x<t) = fdecθ (x|hcxtt , zuttt ) (8)

At time step t, the encoder RNN f encθ takes the pre-
vious utterance xt−1 and produces an encoder vec-
tor henct−1 (Eq. 3). The context RNN f

cxt
θ models the

context of the conversation by updating its hidden
states using the encoder vector (Eq. 4). The con-
text hcxtt defines the conditional prior pθ(z

utt
t |x<t),

which is a factorized Gaussian distribution whose
mean µt and diagonal variance σt are given by
feed-forward neural networks (Eq. 5-7). Finally
the decoder RNN fdecθ generates the utterance xt,
conditioned on the context vector hcxtt and the la-
tent variable zuttt (Eq. 8). We make two important
notes: (1) the context RNN can be viewed as a
high-level decoder, and together with the decoder
RNN, they comprise a hierarchical RNN decoder.
(2) VHRED follows a conditional VAE structure
where each utterance xt is generated conditioned
on the context hcxtt (Eq. 5-8).

The variational posterior is a factorized Gaus-
sian distribution where the mean and the diago-
nal variance are predicted from the target utterance
and the context as follows:

qφ(z
utt
t |x≤t) = N (z|µ′t,σ′tI) (9)

where µ′t = MLPφ(xt,h
cxt
t ) (10)

σ′t = Softplus(MLPφ(xt,h
cxt
t )) (11)

3.3 The Degeneration Problem
A known problem of a VAE that incorporates
an autoregressive RNN decoder is the degeneracy
that ignores the latent variable z. In other words,
the KL divergence term in Eq. 2 goes to zero and
the decoder fails to learn any dependency between
the latent variable and the data. Eventually, the
model behaves as a vanilla RNN. This problem is

1794



0 5 10 15 20 25 30 35 40 45 50 55
Epoch

0.0

0.1

0.2

0.3

0.4

0.5

0.6
KL

 d
iv

er
ge

nc
e

KL divergence

0.0

0.2

0.4

0.6

0.8

1.0

KL
 m

ul
tip

lie
r

KL multiplier

Figure 1: Degeneration of VHRED. The KL di-
vergence term continuously decreases as training
proceeds, meaning that the decoder ignores the la-
tent variable zutt. We train the VHRED on Cornell
Movie Dialog Corpus with word drop and KL an-
nealing.

first reported in the sentence VAE (Bowman et al.,
2016), in which following two heuristics are pro-
posed to alleviate the problem by weakening the
decoder.

First, the KL annealing scales the KL diver-
gence term of Eq. 2 using a KL multiplier λ, which
gradually increases from 0 to 1 during training:

L̃(θ,φ;x) = −λDKL(qφ(z|x)‖p(z)) (12)
+Eqφ(z|x)[log pθ(x|z)]

This helps the optimization process to avoid lo-
cal optima of zero KL divergence in early training.
Second, the word drop regularization randomly re-
places some conditioned-on word tokens in the
RNN decoder with the generic unknown word to-
ken (UNK) during training. Normally, the RNN
decoder predicts each next word in an autoregres-
sive manner, conditioned on the previous sequence
of ground truth (GT) words. By randomly replac-
ing a GT word with an UNK token, the word drop
regularization weakens the autoregressive power
of the decoder and forces it to rely on the latent
variable to predict the next word. The word drop
probability is normally set to 0.25, since using a
higher probability may degrade the model perfor-
mance (Bowman et al., 2016).

However, we observe that these tricks do not
solve the degeneracy for the VHRED in conver-
sation modeling. An example in Fig. 1 shows that
the VHRED learns to ignore the utterance latent
variable as the KL divergence term falls to zero.

5 10 15 20 25 30 35 40 45
Epoch

0.0

0.5

1.0

1.5

2.0

2.5

ra
tio

ratio

0.0

0.5

1.0

1.5

KL
 d

iv
er

ge
nc

e

KL divergence

Figure 2: The average ratio E[σ2t ]/Var(µt) when
the decoder is only conditioned on zuttt . The ratio
drops to zero as training proceeds, indicating that
the conditional priors pθ(zuttt |x<t) degenerate to
separate point masses.

3.4 Empirical Observation on Degeneracy
The decoder RNN of the VHRED in Eq. 8 con-
ditions on two information sources: deterministic
hcxtt and stochastic z

utt. In order to check whether
the presence of deterministic source hcxtt causes
the degeneration, we drop the deterministic hcxtt
and condition the decoder only on the stochastic
utterance latent variable zutt:

pθ(xt|x<t) = fdecθ (x|zuttt ) (13)

While this model achieves higher values of KL di-
vergence than original VHRED, as training pro-
ceeds it again degenerates with the KL divergence
term reaching zero (Fig. 2).

To gain an insight of the degeneracy, we exam-
ine how the conditional prior pθ(zuttt |x<t) (Eq. 5)
of the utterance latent variable changes during
training, using the model above (Eq. 13). Fig. 2
plots the ratios of E[σ2t ]/Var(µt), where E[σ2t ]
indicates the within variance of the priors, and
Var(µt) is the between variance of the priors.
Note that traditionally this ratio is closely related
to Analysis of Variance (ANOVA) (Lomax and
Hahs-Vaughn, 2013). The ratio gradually falls to
zero, implying that the priors degenerate to sep-
arate point masses as training proceeds. More-
over, we find that the degeneracy of priors co-
incide with the degeneracy of KL divergence, as
shown in (Fig. 2). This is intuitively natural: if
the prior is already narrow enough to specify the
target utterance, there is little pressure to encode
any more information in the variational posterior
for reconstruction of the target utterance.

1795



z𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐

h1𝑐𝑐𝑐𝑐𝑐𝑐

z1𝑢𝑢𝑐𝑐𝑐𝑐

…

…x1 x𝑐𝑐

h𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐

z𝑐𝑐𝑢𝑢𝑐𝑐𝑐𝑐

Figure 3: Graphical representation of the Varia-
tional Hierarchical Conversation RNN (VHCR).
The global latent variable zconv provides a global
context in which the conversation takes place.

This empirical observation implies that the fun-
damental reason behind the degeneration may
originate from combination of two factors: (1)
strong expressive power of the hierarchical RNN
decoder and (2) training data sparsity caused by
the conditional VAE structure. The VHRED is
trained to predict a next target utterance xt con-
ditioned on the context hcxtt which encodes infor-
mation about previous utterances {x1, . . . ,xt−1}.
However, conditioning on the context makes the
range of training target xt very sparse; even in
a large-scale conversation corpus such as Ubuntu
Dialog (Lowe et al., 2015), there exist one or very
few target utterances per context. Therefore, hier-
archical RNNs, given their autoregressive power,
can easily overfit to training data without using the
latent variable. Consequently, the VHRED will
not encode any information in the latent variable,
i.e. it degenerates. It explains why the word drop
fails to prevent the degeneracy in the VHRED.
The word drop only regularizes the decoder RNN;
however, the context RNN is also powerful enough
to predict a next utterance in a given context even
with the weakened decoder RNN. Indeed we ob-
serve that using a larger word drop probability
such as 0.5 or 0.75 only slows down, but fails to
stop the KL divergence from vanishing.

3.5 Variational Hierarchical Conversation
RNN (VHCR)

As discussed, we argue that the two main causes
of degeneration are i) the expressiveness of the
hierarchical RNN decoders, and ii) the condi-
tional VAE structure that induces data sparsity.
This finding hints us that in order to train a non-
degenerate latent variable model, we need to de-
sign a model that provides an appropriate way to

regularize the hierarchical RNN decoders and al-
leviate data sparsity per context. At the same time,
the model should be capable of modeling complex
structure of conversation. Based on these insights,
we propose a novel VAE structure named Varia-
tional Hierarchical Conversation RNN (VHCR),
whose graphical model is illustrated in Fig. 3. Be-
low we first describe the model, and discuss its
unique features.

We introduce a global conversation latent vari-
able zconv which is responsible for generating a
sequence of utterances of a conversation c =
{x1, . . . ,xn}:

pθ(c|zconv) = pθ(x1, . . . ,xn|zconv) (14)

Overall, the VHCR builds upon the hierarchi-
cal RNNs, following the VHRED (Serban et al.,
2017). One key update is to form a hierarchical
latent structure, by using the global latent variable
zconv per conversation, along with local the latent
variable zuttt injected at each utterance (Fig. 3):

henct = f
enc
θ (xt) (15)

hcxtt =

{
MLPθ(zconv), if t = 0
f cxtθ (h

cxt
t−1,h

enc
t−1, z

conv), otherwise

pθ(xt|x<t, zuttt , zconv) = fdecθ (x|hcxtt , zuttt , zconv)
pθ(z

conv) = N (z|0, I) (16)
pθ(z

utt
t |x<t, zconv) = N (z|µt,σtI) (17)

where µt = MLPθ(h
cxt
t , z

conv) (18)

σt = Softplus(MLPθ(h
cxt
t , z

conv)). (19)

For inference of zconv, we use a bi-directional
RNN denoted by f conv, which runs over the utter-
ance vectors generated by the encoder RNN:

qφ(z
conv|x1, ...,xn) = N (z|µconv,σconvI) (20)

where hconv = f conv(henc1 , ...,h
enc
n ) (21)

µconv = MLPφ(hconv) (22)

σconv = Softplus(MLPφ(hconv)). (23)

The posteriors for local variables zuttt are then con-
ditioned on zconv:

qφ(z
utt
t |x1, ...,xn, zconv) = N (z|µ′t,σ′tI) (24)

where µ′t = MLPφ(xt,h
cxt
t , z

conv) (25)

σ′t = Softplus(MLPφ(xt,h
cxt
t , z

conv)).

Our solution of VHCR to the degeneration
problem is based on two ideas. The first idea is
to build a hierarchical latent structure of zconv for

1796



5 10 15 20 25 30
Epoch

0.0

0.1

0.2

0.3

0.4

0.5

0.6
KL

 d
iv

er
ge

nc
e

VHRED + w.d
VHCR + u.d

VHRED + u.d

Figure 4: The comparison of KL divergences. The
VHCR with the utterance drop shows high and sta-
ble KL divergence, indicating the active use of la-
tent variables. w.d and u.d denote the word drop
and the utterance drop, respectively.

a conversation and zuttt for each utterance. As z
conv

is independent of the conditional structure, it does
not suffer from the data sparsity problem. How-
ever, the expressive power of hierarchical RNN
decoders makes the model still prone to ignore la-
tent variables zconv and zuttt . Therefore, our second
idea is to apply an utterance drop regularization to
effectively regularize the hierarchical RNNs, in or-
der to facilitate the use of latent variables. That is,
at each time step, the utterance encoder vector henct
is randomly replaced with a generic unknown vec-
tor hunk with a probability p. This regularization
weakens the autoregressive power of hierarchical
RNNs and as well alleviates the data sparsity prob-
lem, since it induces noise into the context vector
hcxtt which conditions the decoder RNN. The dif-
ference with the word drop (Bowman et al., 2016)
is that our utterance drop depresses the hierarchi-
cal RNN decoders as a whole, while the word
drop only weakens the lower-level decoder RNNs.
Fig. 4 confirms that with the utterance drop with a
probability of 0.25, the VHCR effectively learns to
use latent variables, achieving a significant degree
of KL divergence.

3.6 Effectiveness of Hierarchical Latent
Structure

Is the hierarchical latent structure of the VHCR
crucial for effective utilization of latent variables?
We investigate this question by applying the ut-
terance drop on the VHRED which lacks any hi-
erarchical latent structure. We observe that the
KL divergence still vanishes (Fig. 4), even though

the utterance drop injects considerable noise in the
context hcxtt . We argue that the utterance drop
weakens the context RNN, thus it consequently
fail to predict a reasonable prior distribution for
zutt (Eq. 5-7). If the prior is far away from the re-
gion of zutt that can generate a correct target utter-
ance, encoding information about the target in the
variational posterior will incur a large KL diver-
gence penalty. If the penalty outweighs the gain
of the reconstruction term in Eq. 2, then the model
would learn to ignore zutt, in order to maximize
the variational lower-bound in Eq. 2.

On the other hand, the global variable zconv al-
lows the VHCR to predict a reasonable prior for
local variable zuttt even in the presence of the ut-
terance drop regularization. That is, zconv can act
as a guide for zutt by encoding the information for
local variables. This reduces the KL divergence
penalty induced by encoding information in zutt to
an affordable degree at the cost of KL divergence
caused by using zconv. This trade-off is indeed a
fundamental strength of hierarchical models that
provide parsimonious representation; if there ex-
ists any shared information among the local vari-
ables, it is coded in the global latent variable re-
ducing the code length by effectively reusing the
information. The remaining local variability is
handled properly by the decoding distribution and
local latent variables.

The global variable zconv provides other bene-
fits by representing a latent global structure of a
conversation, such as a topic, a length, and a tone
of the conversation. Moreover, it allows us to con-
trol such global properties, which is impossible for
models without hierarchical latent structure.

4 Results

We first describe our experimental setting, such as
datasets and baselines (section 4.1). We then re-
port quantitative comparisons using three differ-
ent metrics (section 4.2–4.4). Finally, we present
qualitative analyses, including several utterance
control tasks that are enabled by the hierarchal la-
tent structure of our VHCR (section 4.5). We defer
implementation details and additional experiment
results to the supplementary file.

4.1 Experimental Setting

Datasets. We evaluate the performance of conver-
sation generation using two benchmark datasets:
1) Cornell Movie Dialog Corpus (Danescu-

1797



Model NLL Recon. KL div.
HRED 3.873 - -

VHRED ≤ 3.912 3.619 0.293
VHRED + w.d ≤ 3.904 3.553 0.351
VHRED + bow ≤ 4.149 2.982 1.167

VHCR + u.d ≤ 4.026 3.523 0.503
(a) Cornell Movie Dialog

Model NLL Recon. KL div.
HRED 3.766 - -

VHRED ≤ 3.767 3.654 0.113
VHRED + w.d ≤ 3.824 3.363 0.461
VHRED + bow ≤ 4.237 2.215 2.022

VHCR + u.d ≤ 3.951 3.205 0.756
(b) Ubuntu Dialog

Table 1: Results of Negative Log-likelihood. The
inequalities denote the variational bounds. w.d and
u.d., and bow denote the word drop, the utterance
drop, and the auxiliary bag-of-words loss respec-
tively.

Cornell Ubuntu
Model Total zconv zutt Total zconv zutt

VHRED 0.351 - 0.351 0.461 - 0.461
VHCR 0.503 0.189 0.314 0.756 0.198 0.558

Table 2: KL divergence decomposition. VHRED
and VHCR are trained with word drop and utter-
ance drop respectively.

Niculescu-Mizil and Lee, 2011), containing
220,579 conversations from 617 movies. 2)
Ubuntu Dialog Corpus (Lowe et al., 2015), con-
taining about 1 million multi-turn conversations
from Ubuntu IRC channels. In both datasets, we
truncate utterances longer than 30 words.

Baselines. We compare our approach with four
baselines. They are combinations of two state-of-
the-art models of conversation generation with dif-
ferent solutions to the degeneracy. (i) Hierarchical
recurrent encoder-decoder (HRED) (Serban et al.,
2016), (ii) Variational HRED (VHRED) (Ser-
ban et al., 2017), (iii) VHRED with the word
drop (Bowman et al., 2016), and (iv) VHRED with
the bag-of-words (bow) loss (Zhao et al., 2017).

Performance Measures. Automatic evalua-
tion of conversational systems is still a challeng-
ing problem (Liu et al., 2016). Based on lit-
erature, we report three quantitative metrics: i)
the negative log-likelihood (the variational bound
for variational models), ii) embedding-based met-
rics (Serban et al., 2017), and iii) human evalua-
tion via Amazon Mechanical Turk (AMT).

4.2 Results of Negative Log-likelihood

Table 1 summarizes the per-word negative log-
likelihood (NLL) evaluated on the test sets of
two datasets. For variational models, we instead
present the variational bound of the negative log-
likelihood in Eq. 2, which consists of the recon-
struction error term and the KL divergence term.
The KL divergence term can measure how much
each model utilizes the latent variables.

We observe that the NLL is the lowest by the
HRED. Variational models show higher NLLs, be-
cause they are regularized methods that are forced
to rely more on latent variables. Independent of
NLL values, we later show that the latent variable
models often show better generalization perfor-
mance in terms of embedding-based metrics and
human evaluation. In the VHRED, the KL di-
vergence term gradually vanishes even with the
word drop regularization; thus, early stopping is
necessary to obtain a meaningful KL divergence.
The VHRED with the bag-of-words loss (bow)
achieves the highest KL divergence, however, at
the cost of high NLL values. That is, the vari-
ational lower-bound minimizes the minimum de-
scription length, to which the bow loss works in
an opposite direction by forcing latent variables to
encode bag-of-words representation of utterances.
Our VHCR achieves stable KL divergence without
any auxiliary objective, and the NLL is lower than
the VHRED + bow model.

Table 2 summarizes how global and latent vari-
able are used in the VHCR. We observe that
VHCR encodes a significant amount of informa-
tion in the global variable zconv as well as in the
local variable zutt, indicating that the VHCR suc-
cessfully exploits its hierarchical latent structure.

4.3 Results of Embedding-Based Metrics

The embedding-based metrics (Serban et al.,
2017; Rus and Lintean, 2012) measure the tex-
tual similarity between the words in the model
response and the ground truth. We represent
words using Word2Vec embeddings trained on
the Google News Corpus1. The average metric
projects each utterance to a vector by taking the
mean over word embeddings in the utterance, and
computes the cosine similarity between the model
response vector and the ground truth vector. The
extrema metric is similar to the average metric,
only except that it takes the extremum of each di-

1https://code.google.com/archive/p/word2vec/.

1798



Model Average Extrema Greedy
1-turn

HRED 0.541 0.370 0.387
VHRED 0.543 0.356 0.393

VHRED + w.d 0.554 0.365 0.404
VHRED + bow 0.555 0.350 0.411

VHCR + u.d 0.585 0.376 0.434
3-turn

HRED 0.556 0.372 0.395
VHRED 0.554 0.360 0.398

VHRED + w.d 0.566 0.369 0.408
VHRED + bow 0.573 0.360 0.423

VHCR + u.d 0.588 0.378 0.429
(a) Cornell Movie Dialog

Model Average Extrema Greedy
1-turn

HRED 0.567 0.337 0.412
VHRED 0.547 0.322 0.398

VHRED + w.d 0.545 0.314 0.398
VHRED + bow 0.545 0.306 0.398

VHCR + u.d 0.570 0.312 0.425
3-turn

HRED 0.559 0.324 0.402
VHRED 0.551 0.315 0.397

VHRED + w.d 0.551 0.309 0.399
VHRED + bow 0.552 0.303 0.398

VHCR + u.d 0.574 0.311 0.422
(b) Ubuntu Dialog

Table 3: Results of embedding-based metrics. 1-
turn and 3-turn responses of models per context.

mension, instead of the mean. The greedy metric
first finds the best non-exclusive word alignment
between the model response and the ground truth,
and then computes the mean over the cosine simi-
larity between the aligned words.

Table 3 compares the different methods with
three embedding-based metrics. Each model gen-
erates a single response (1-turn) or consecutive
three responses (3-turn) for a given context. For
3-turn cases, we report the average of metrics mea-
sured for three turns. We use the greedy decoding
for all the models.

Our VHCR achieves the best results in most
metrics. The HRED is the worst on the Cornell
Movie dataset, but outperforms the VHRED and
VHRED + bow on the Ubuntu Dialog dataset. Al-
though the VHRED + bow shows the highest KL
divergence, its performance is similar to that of
VHRED, and worse than that of the VHCR model.
It suggests that a higher KL divergence does not
necessarily lead to better performance; it is more
important for the models to balance the modeling
powers of the decoder and the latent variables. The
VHCR uses a more sophisticated hierarchical la-
tent structure, which better reflects the structure of

natural language conversations.

4.4 Results of Human Evaluation
Table 4 reports human evaluation results via Ama-
zon Mechanical Turk (AMT). The VHCR outper-
forms the baselines in both datasets; yet the per-
formance improvement in Cornell Movie Dialog
are less significant compared to that of Ubuntu.
We empirically find that Cornell Movie dataset is
small in size, but very diverse and complex in con-
tent and style, and the models often fail to gener-
ate sensible responses for the context. The perfor-
mance gap with the HRED is the smallest, sug-
gesting that the VAE models without hierarchical
latent structure have overfitted to Cornell Movie
dataset.

4.5 Qualitative Analyses
Comparison of Predicted Responses. Table 5
compares the generated responses of algorithms.
Overall, the VHCR creates more consistent re-
sponses within the context of a given conversation.
This is supposedly due to the global latent variable
zconv that provides a more direct and effective way
to handle the global context of a conversation. The
context RNN of the baseline models can handle
long-term context to some extent, but not as much
as the VHCR.

Interpolation on zconv. We present examples of
one advantage by the hierarchical latent structure
of the VHCR, which cannot be done by the other
existing models. Table 6 shows how the generated
responses vary according to the interpolation on
zconv. We randomly sample two zconv from a stan-
dard Gaussian prior as references (i.e. the top and
the bottom row of Table 6), and interpolate points
between them. We generate 3-turn conversations
conditioned on given zconv. We see that zconv con-
trols the overall tone and content of conversations;
for example, the tone of the response is friendly in
the first sample, but gradually becomes hostile as
zconv changes.

Generation on a Fixed zconv. We also study
how fixing a global conversation latent variable
zconv affects the conversation generation. Table 7
shows an example, where we randomly fix a ref-
erence zconv from the prior, and generate multiple
examples of 3-turn conversation using randomly
sampled local variables zutt. We observe that zconv

heavily affects the form of the first utterance; in
the examples, the first utterances all start with a
“where” phrase. At the same time, responses show

1799



Cornell Ubuntu
Opponent Wins Losses Ties Wins Losses Ties

VHCR vs HRED 28.5± 1.9 28.2± 1.9 43.3± 2.1 52.9± 2.1 42.2± 2.1 4.9± 0.9
VHCR vs VHRED + w.d 29.9± 1.9 28.0± 1.9 42.1± 2.1 48.1± 2.1 40.1± 3.6 11.9± 1.4
VHCR vs VHRED + bow 31.3± 2.0 26.9± 1.9 41.7± 2.1 46.1± 2.1 39.9± 2.1 14.0± 1.5

Table 4: Results of human evaluation via AMT. Human turkers are asked to choose which response is
more appropriate in a given context, without knowing which algorithms generate which responses. For
each pair of models, we carry out three evaluation batches, each of which consists of 100 random test
samples evaluated by five unique humans. We report mean preferences with ±90% confidence interval.

Context HRED VHRED VHRED+bow VHCR
i’m going home. → what’s wrong? → i don’t
know. one of these days, i’m gon na see louis.
my back’s killing me.

i don’t know. i don’t want to be here.
i’m going to be here.

are you going to go
now?

this isn’t a joke. you’re
going to be here.

how’s sleeping beauty? → still sleeping. → kiss
him, maybe he’ll turn into a frog.

he’s a good man. what? we won’t? maybe he’s dead.

when i mount my usb drive i get a permission
denied but i can easily see the files if i boot via
live cd any ideas on how i → the file permissions
of the ext3 file system are likely preventing you
→ thanks, any idea on options to fix?

no, i don’t know
how to fix it, but
i’m not sure how to
fix it

what kind of filesystem
is it on?

you did it just first? then
you’ll need to reboot
and then remount it up
again, if your problem
persists, i’d give you the

the only thing i can think
of is to fix the permis-
sions issue, try mount-
ing the drive with the uid
and gid options in the
forum

hello i need some support, anybody there? →
go ahead with your questions. <unk> are not
expected on irc. → i have an hp pavilion <unk>
here and i can’t get it to install ubuntu

what’s the prob-
lem?

<unk>. what’s the
problem?

i’m sorry. they should be sup-
ported by canonical.
you don’t need to use
them.

Table 5: Qualitative comparison of generated responses. Top two rows show the samples from Cornell
Movie Dialog, while the bottom two rows are from Ubuntu Dialog.

nice to meet you.
→ where’s my wife?
→ she’s asleep.
go on, frank.
→ i ll tell you what i’m doing. i’m a little tired.
→ why?
now, you’re not going.
→ not for a long time.
→ but you’re a little <unk>.
get out of here.
→ all right.
→ you want to go home?
get out of here!!!
→ it’s not your fault, is n’t it?
→ why? what’s wrong?

Table 6: An example of interpolated 3-turn re-
sponses over zconv on Cornell Movie Dialog.

variations according to different local variables
zutt. These examples show that the hierarchical
latent structure of VHCR allows both global and
fine-grained control over generated conversations.

5 Discussion

We introduced the variational hierarchical conver-
sation RNN (VHCR) for conversation modeling.
We noted that the degeneration problem in exist-
ing VAE models such as the VHRED is persis-
tent, and proposed a hierarchical latent variable
model with the utterance drop regularization. Our
VHCR obtained higher and more stable KL diver-
gences than various versions of VHRED models
without using any auxiliary objective. The empir-

where is she?
→ she’s the only one who knows where she is, she’s going to be all right.
→ oh, you’re the only one who’s gon na be. she’s a <unk>.
where’s my wife?
→ you’ve got to get out of here, you know? you’re the one who’s gon na
be here.
→ oh, that’s nice.
where are you?
→ well, i was just thinking about you and i know what you’re doing. i’m
going to have to go to the <unk> and i’m
→ i’m sorry.
where are you going?
→ to get you to the airport.
→ you’re going to be late?
where are you going?
→ to the <unk>. i am not going to tell you what i am. i am the only one
who has to be. i will be the
→ you’ve got to stop!

Table 7: An example of 3-turn responses condi-
tioned on sampled zutt for a single fixed zconv.

ical results showed that the VHCR better reflected
the structure of natural conversations, and outper-
formed previous models. Moreover, the hierarchi-
cal latent structure allowed both global and fine-
grained control over the conversation generation.

Acknowledgments

This work was supported by Kakao and Kakao
Brain corporations, and Creative-Pioneering Re-
searchers Program through Seoul National Univer-
sity. Gunhee Kim is the corresponding author.

1800



References
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-

drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2016. Generating sentences from a continu-
ous space. In CoNLL. https://doi.org/10.
18653/v1/K16-1002.

Xi Chen, Diederik P Kingma, Tim Salimans, Yan
Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. 2017. Variational
lossy autoencoder. In ICLR.

Cristian Danescu-Niculescu-Mizil and Lillian Lee.
2011. Chameleons in imagined conversations: A
new approach to understanding coordination of lin-
guistic style in dialogs. In CMCL Workshop.

Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational bayes. In ICLR.

Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter,
and Dan Jurafsky. 2017. Adversarial learning
for neural dialogue generation. arXiv preprint
arXiv:1701.06547 .

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. In EMNLP.

Richard G Lomax and Debbie L Hahs-Vaughn. 2013.
Statistical concepts: A second course. Routledge.

Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle
Pineau. 2015. The ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dia-
logue systems. In SIGDIAL.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. 2014. Stochastic backpropagation and ap-
proximate inference in deep generative models. In
ICML.

Vasile Rus and Mihai Lintean. 2012. A comparison of
greedy and optimal assessment of natural language
student input using word-to-word similarity metrics.
In Building Educational Applications Using NLP
Workshop. ACL. http://www.aclweb.org/
anthology/W12-2018.

Iulian Vlad Serban, Alessandro Sordoni, Yoshua Ben-
gio, Aaron C Courville, and Joelle Pineau. 2016.
Building end-to-end dialogue systems using gener-
ative hierarchical neural network models. In AAAI.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron C Courville,
and Yoshua Bengio. 2017. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. In AAAI.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. In ACL. https://doi.org/10.3115/
v1/P15-1152.

Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi,
Christina Lioma, Jakob Grue Simonsen, and Jian-
Yun Nie. 2015a. A hierarchical recurrent encoder-
decoder for generative context-aware query sugges-
tion. In CIKM.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan.
2015b. A neural network approach to context-
sensitive generation of conversational responses. In
NAACL-HLT . https://doi.org/10.3115/
v1/N15-1020.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. In ICML Deep Learning Workshop.

Biao Zhang, Deyi Xiong, Jinsong Su, Hong Duan, and
Min Zhang. 2016. Variational neural machine trans-
lation. In EMNLP. https://doi.org/10.
18653/v1/D16-1050.

Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.
2017. Learning discourse-level diversity for neu-
ral dialog models using conditional variational au-
toencoders. In ACL. https://doi.org/10.
18653/v1/P17-1061.

1801


