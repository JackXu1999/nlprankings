



















































Encoding Temporal Information for Time-Aware Link Prediction


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2350–2354,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Encoding Temporal Information for Time-Aware Link Prediction

Tingsong Jiang, Tianyu Liu, Tao Ge, Lei Sha, Sujian Li, Baobao Chang and Zhifang Sui
Key Laboratory of Computational Linguistics, Ministry of Education

School of Electronics Engineering and Computer Science, Peking University
Collaborative Innovation Center for Language Ability, Xuzhou 221009 China

{tingsong,tianyu0421,taoge,shalei,lisujian,chbb,szf}@pku.edu.cn

Abstract

Most existing knowledge base (KB) embed-
ding methods solely learn from time-unknown
fact triples but neglect the temporal informa-
tion in the knowledge base. In this paper,
we propose a novel time-aware KB embed-
ding approach taking advantage of the hap-
pening time of facts. Specifically, we use tem-
poral order constraints to model transforma-
tion between time-sensitive relations and en-
force the embeddings to be temporally consis-
tent and more accurate. We empirically eval-
uate our approach in two tasks of link predic-
tion and triple classification. Experimental re-
sults show that our method outperforms other
baselines on the two tasks consistently.

1 Introduction

Knowledge bases (KBs) such as Freebase (Bollacker
et al., 2008) and YAGO (Fabian et al., 2007) play a
pivotal role in many NLP related applications. KB-
s consist of facts in the form of triplets (ei, r, ej),
indicating that head entity ei and tail entity ej is
linked by relation r. Although KBs are large, they
are far from complete. Link prediction is to predict
relations between entities based on existing triplet-
s, which can alleviate the incompleteness of cur-
rent KBs. Recently a promising approach for this
task called knowledge base embedding (Nickel et
al., 2011; Bordes et al., 2011; Socher et al., 2013)
aims to embed entities and relations into a continu-
ous vector space while preserving certain informa-
tion of the KB graph. TransE (Bordes et al., 2013) is
a typical model considering relation vector as trans-

lating operations between head and tail vector, i.e.,
ei + r ≈ ej when (ei, r, ej) holds.

Most existing KB embedding methods solely
learn from time-unknown facts but ignore the use-
ful temporal information in the KB. In fact, there
are many temporal facts (or events) in the KB, e.g.,
(Obama, wasBornIn, Hawaii) happened at August
4, 1961. (Obama, presidentOf, USA) is true since
2009. Current KBs such as YAGO and Freebase
store such temporal information either directly or
indirectly. The happening time of time-sensitive
facts may indicate special temporal order of fact-
s and time-sensitive relations. For example, (Ein-
stein, wasBornIn, Ulm) happened in 1879, (Einstein,
wonPrize, Nobel Prize) happened in 1922, (Einstein,
diedIn, Princeton) occurred in 1955. We can infer
the temporal order of time-sensitive relations from
all such kinds of facts: wasBornIn → wonPrize →
diedIn. Traditional KB embedding models such as
TransE often confuse relations such as wasBornIn
and diedIn when predicting (person,?,location) be-
cause TransE learns only from time-unknown facts
and cannot distinguish relations with similar seman-
tic meaning. To make more accurate predictions, it
is non-trivial for existing KB embedding methods to
incorporate temporal order information.

This paper mainly focuses on incorporating the
temporal order information and proposes a time-
aware link prediction model. A new temporal di-
mension is added to fact triples, denoted as a quadru-
ple: (ei, r, ej , tr), indicating the fact happened at
time tr1. To make the embedding space compati-

1tr is the absolute beginning time when the fact is true, e.g.,
”1961-08-04” for (Obama, wasBornIn, Hawaii).

2350



ble with the observed triple in the fact dimension,
relation vectors behave as translations between enti-
ty vectors similarly as TransE models. To incorpo-
rate temporal order information between pair-wise
temporal facts, we assume that prior time-sensitive
relation vector can evolve into a subsequent time-
sensitive relation vector through a temporal tran-
sition. For example, we have two temporal fact-
s sharing the same head entity: (ei, ri, ej , t1) and
(ei, rj , ek, t2) and the temporal order constraint t1<
t2, i.e., ri happens before rj , then we propose the
assumption that prior relation ri after temporal tran-
sition should lie close to subsequent relation rj , i.e.,
riM ≈ rj , here matrix M captures the temporal
order information between relations. In this way,
both semantic and temporal information are embed-
ded into a continuous vector space during learning.
To the best of our knowledge, we are the first to con-
sider such temporal information for KB embedding.

We evaluate our approach on public available
datasets and our method outperforms state-of-the-art
methods in the time-aware link prediction and triple
classification tasks.

2 Time-Aware KB Embedding

Traditional KB embedding methods encode only ob-
served fact triples but neglect temporal constraints
between time-sensitive entities and facts. To deal
with this limitation, we introduce Time-Aware KB
Embedding which constrains the task by incorporat-
ing temporal constraints.

To consider the happening time of facts, we for-
mulate a temporal order constraint as an optimiza-
tion problem based on a manifold regularization ter-
m. Specially, temporal order of relations in time-
sensitive facts should affect KB representation. If ri
and rj share the same head entity ei, and ri occurs
before rj , then prior relation’s vector ri could evolve
into subsequent relation’s vector rj in the temporal
dimension.

To encode the transition between time-sensitive
relations, we define a transition matrix M ∈ Rn×n
between pair-wise temporal ordering relation pair
(ri, rj). Our optimization requires that positive tem-
poral ordering relation pairs should have lower s-
cores (energies) than negative pairs, so we define a

temporal order score function as

g(ri, rj) = ‖riM− rj‖1, (1)

which is expected to be a low score when the relation
pair is in chronological order, and high otherwise.

To make the embedding space compatible with
the observed triples, we make use of the triple set
∆ and follow the same strategy adopted in previous
methods such as TransE.

f(ei, r, ej) = ‖ei + r− ej‖1. (2)

For each candidate triple, it requires positive triples
to have lower scores than negative triples.

The optimization is to minimize the joint score
function,

L=
∑

x+∈∆

[ ∑

x−∈∆′
[γ1 + f(x

+)− f(x−)]+

+λ
∑

y+∈Ωei ,y
−∈Ω′ei

[γ2 + g(y
+)− g(y−)]+

] (3)

where x+ = (ei, ri, ej) ∈ ∆ is the positive triple
(quad), x−=(e′i, ri, e

′
j)∈∆′ is corresponding the

negative triple. y+ = (ri, rj)∈Ωei is the positive
relation ordering pair with respect to (ei, ri, ej , tx).
It’s defined as

Ωei = {(ri, rj)|(ei, ri, ej , tx)∈∆τ ,
(ei, rj , ek, ty)∈∆τ , tx< ty},

(4)

where ri and rj share the same head entity ei, and
y− = (rj , ri) ∈ Ω′ei are the corresponding negative
relation order pairs by inverse. In experiments, we
enforce constrains as ‖ei‖2 ≤ 1, ‖ri‖2 ≤ 1, ‖rj‖ ≤
1 and ‖riM‖2 ≤ 1.

The first term in Equation (3) enforces the resul-
tant embedding space compatible with all the ob-
served triples, and the second term further requires
the space to be temporally consistent and more accu-
rate. Hyperparameter λ makes a trade-off between
the two cases. Stochastic gradient descent (in mini-
batch mode) is adopted to solve the minimization
problem.

3 Experiments

We adopt the same evaluation metrics for time-
aware KB embedding in two tasks: link prediction
(Bordes et al., 2013) and triple classification (Socher
et al., 2013).

2351



Dataset #Rel #Ent #Train/#Valid/#Test #Trip. #Quad
YG15k 10 9513 13345/1320/1249 15914 15914
YG36k 10 9513 29757/3252/3058 36067 15914

Table 1: Statistics of data sets.

3.1 Datasets

We create two temporal datasets from YAGO2 (Hof-
fart et al., 2013), consisting of time-sensitive facts.
In YAGO2, MetaFacts contains all happening time
for facts. DateFacts contains all creation time for
entities. First, to make a pure time-sensitive dataset
where all facts have time annotations, we selected
the subset of entities that have at least 2 mentions in
MetaFacts and DateFacts. This resulted in 15,914
triples (quadruples) which were randomly split with
the ratio shown in Table 1. This dataset is denoted
YG15k. Second, to make a mixed dataset, we created
YG36k where 50% facts have time annotations and
50% do not. We will release the data upon request.

3.2 Link Prediction

Link prediction is to complete the triple (h, r, t)
when h, r or t is missing.

3.2.1 Entity Prediction
Evaluation protocol. For each test triple with

missing head or tail entity, various methods are used
to compute the scores for all candidate entities and
rank them in descending order. We use two metric-
s for our evaluation as in (Bordes et al., 2013): the
mean of correct entity ranks (Mean Rank) and the
proportion of valid entities ranked in top-10 (Hit-
s@10). As mentioned in (Bordes et al., 2013), the
metrics are desirable but flawed when a corrupted
triple exists in the KB. As a countermeasure, we fil-
ter out all these valid triples in the KB before rank-
ing. We name the first evaluation set as Raw and the
second as Filter.
Baseline methods. For comparison, we select trans-
lating methods such as TransE (Bordes et al., 2013),
TransH (Wang et al., 2014b) and TransR (Lin et al.,
2015b) as our baselines. We then use time-aware
embedding based on these methods to obtain the cor-
responding time-aware embedding models. A model
with time-aware embedding is denoted as ”tTransE”
for example.
Implementation details. For all methods, we cre-
ate 100 mini-batches on each data set. The di-

Mean Rank Hits@1 (%)
Metric Raw Filter Raw Filter
TransE 1.53 1.48 69.4 73.0

tTransE 1.42 1.35 71.1 75.7
TransH 1.51 1.37 70.5 72.2

tTransH 1.38 1.30 74.6 76.9
TransR 1.40 1.28 71.1 74.3

tTransR 1.27 1.12 74.5 78.9

Table 3: Evaluation results on relation prediction.

mension of the embedding n is set in the range of
{20,50,100}, the margin γ1 and γ2 are set in the
range {1,2,4,10}. The learning rate is set in the
range {0.1, 0.01, 0.001}. The regularization hy-
perparameter λ is tuned in {10−1,10−2,10−3,10−4}.
The best configuration is determined according to
the mean rank in validation set. The optimal config-
urations are n=100,γ1=γ2=4,λ=10−2, learning rate
is 0.001 and taking `1−norm.
Results. Table 2 reports the results on the test set.
From the results, we can see that time-aware em-
bedding methods outperform all the baselines on al-
l the data sets and with all the metrics. The im-
provements are usually quite significant. The Mean
Rank drops by about 75%, and Hits@10 rises about
19% to 30%. This demonstrates the superiority and
generality of our method. When dealing with s-
parse data YG15k, all the temporal information is u-
tilized to model temporal associations and make the
embeddings more accurate, so it obtains better im-
provement than mixing the time-unknown triples in
YG36k.

3.2.2 Relation Prediction
Relation prediction aims to predict relations giv-

en two entities. Evaluation results are shown
in Table 3 on only YG15K due to limited s-
pace, where we report Hits@1 instead of Hit-
s@10. Example prediction results for TransE
and tTransE are compared in Table 4. For ex-
ample, when testing (Billy Hughes,?,London,1862),
it’s easy for TransE to mix relations wasBornIn
and diedIn because they act similarly for a per-
son and a place. But known that (Billy Hughes,
isAffiliatedTo, National Labor Party) happened in
1916, and tTransE have learnt temporal order that
wasBornIn→isAffiliatedTo→diedIn, so the regular-
ization term |rbornT − raffiliated| is smaller than
|rdiedT− raffiliated|, so correct answer wasBornIn
ranks higher than diedIn.

2352



Dataset YG15k YG36k

Metric MeanRank Hits@10(%) MeanRank Hits@10(%)Raw Filter Raw Filter Raw Filter Raw Filter
TransE 990 971 26.6 29.5 179 163 65.7 75.6

tTransE 235 233 35.4 36.1 60 55 76.1 82.8
TransH 986 966 25.7 28.0 174 158 65.3 77.8

tTransH 232 230 36.1 37.2 61 54 76.6 82.9
TransR 976 955 29.5 30.2 175 153 68.3 80.1

tTransR 228 221 37.3 38.4 55 46 79.5 84.2

Table 2: Evaluation results on link prediction.

Testing quad TransE predictions tTransE predictions
(Billy Hughes,?,London,1862) diedIn,wasBornIn wasBornIn,diedIn
(John Schoenherr,?,Caldecott Medal,1988) owns,hasWonPrize hasWonPrize,owns
(John G. Thompson,?,University of Cambridge,1961) graduatedFrom,worksAt worksAt,graduatedFrom
(Tommy Douglas,?,New Democratic Party,1961) isMarriedTo,isAffiliatedTo isAffiliatedTo,worksAt

Table 4: Example results of relation prediction in descending order. Correct predictions are in bold.

3.3 Triple Classification

Triple classification aims to judge whether an un-
seen triple is correct or not.
Evaluation protocol. We follow the same evalua-
tion protocol used in Socher et al. (2013). To create
labeled data for classification, for each triple in the
test and validation sets, we construct a correspond-
ing negative triple by randomly corrupting the enti-
ties. To corrupt a position (head or tail), only entities
that have appeared in that position are allowed. Dur-
ing triple classification, a triple is predicted as posi-
tive if the score is below a relation-specific threshold
δr; otherwise as negative. We report averaged accu-
racy on the test sets.
Implementation details. We use the same hyperpa-
rameter settings as in the link prediction task. The
relation-specific threshold δr is determined by max-
imizing averaged accuracy on the validation sets.
Results. Table 5 reports the results on the test set-
s. The results indicate that time-aware embedding
outperforms all the baselines consistently. Temporal
order information may help to distinguish positive
and negative triples as different head entities may
have different temporally associated relations. If the
temporal order is the same with most facts, the reg-
ularization term helps it get lower energies and vice
versa.

4 Related Work

Many models have been proposed for KB embed-
ding (Nickel et al., 2011; Bordes et al., 2013; Socher
et al., 2013). External information is employed to
improve KB embedding such as text (Riedel et al.,

Datasets YG15K YG36K
TransE 63.9 71.9
tTransE 75.0 82.7
TransH 63.4 72.1
tTransH 75.1 82.3
TransR 64.5 74.9
tTransR 78.5 83.9

Table 5: Evaluation results on triple classification (%).

2013; Wang et al., 2014a; Zhao et al., 2015), enti-
ty type and relationship domain (Guo et al., 2015;
Chang et al., 2014), and relation path (Lin et al.,
2015a; Gu et al., 2015). However, these methods
solely rely on triple facts but neglect temporal or-
der constraints between facts. Temporal informa-
tion such as relation ordering in text has been ex-
plored (Talukdar et al., 2012; Chambers et al., 2014;
Bethard, 2013; Cassidy et al., 2014; Chambers et
al., 2007; Chambers and Jurafsky, 2008). This pa-
per proposes a time-aware embedding approach that
employs temporal order constraints to improve KB
embedding.

5 Conclusion and Future Work

In this paper, we propose a general time-aware KB
embedding, which incorporates creation time of en-
tities and imposes temporal order constraints on the
geometric structure of the embedding space and en-
force it to be temporally consistent and accurate. As
future work: (1) We will incorporate the valid time
of facts. (2) Some time-sensitive facts lack temporal
information in YAGO2, we will mine such temporal
information from texts.

2353



Acknowledgments

This research is supported by National Key Basic
Research Program of China (No.2014CB340504)
and National Natural Science Foundation of China
(No.61375074,61273318). The contact author for
this paper is Baobao Chang and Zhifang Sui.

References
Steven Bethard. 2013. Cleartk-timeml: A minimalist ap-

proach to tempeval 2013. In Second Joint Conference
on Lexical and Computational Semantics (*SEM), Vol-
ume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
10–14. Association for Computational Linguistics.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-
turge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In Proceedings of the 2008 ACM SIG-
MOD international conference on Management of da-
ta, pages 1247–1250. ACM.

Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured embed-
dings of knowledge bases. In Conference on Artificial
Intelligence, number EPFL-CONF-192344.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran,
Jason Weston, and Oksana Yakhnenko. 2013. Trans-
lating embeddings for modeling multi-relational data.
In Advances in Neural Information Processing System-
s, pages 2787–2795.

Taylor Cassidy, Bill McDowell, Nathanael Chambers,
and Steven Bethard. 2014. An annotation framework
for dense event ordering. In ACL.

Nathanael Chambers and Daniel Jurafsky. 2008. Un-
supervised learning of narrative event chains. ACL,
94305:789–797.

Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying temporal relations between events.
In Proceedings of the 45th Annual Meeting of the A-
CL on Interactive Poster and Demonstration Session-
s, pages 173–176. Association for Computational Lin-
guistics.

Nathanael Chambers, Taylor Cassidy, Bill McDowell,
and Steven Bethard. 2014. Dense event ordering with
a multi-pass architecture. Transactions of the Associ-
ation for Computational Linguistics, 2:273–284.

Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and Christo-
pher Meek. 2014. Typed tensor decomposition of
knowledge bases for relation extraction. In EMNLP,
pages 1568–1579.

MS Fabian, K Gjergji, and W Gerhard. 2007. Ya-
go: A core of semantic knowledge unifying wordnet

and wikipedia. In 16th International World Wide Web
Conference, WWW, pages 697–706.

Kelvin Gu, John Miller, and Percy Liang. 2015. Travers-
ing knowledge graphs in vector space. arXiv preprint
arXiv:1506.01094.

Shu Guo, Quan Wang, Bin Wang, Lihong Wang, and
Li Guo. 2015. Semantically smooth knowledge graph
embedding. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural
Language Processing, pages 84–94.

Johannes Hoffart, Fabian M Suchanek, Klaus Berberich,
and Gerhard Weikum. 2013. Yago2: A spatially and
temporally enhanced knowledge base from wikipedia.
Artificial Intelligence, 194:28–61.

Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2015a.
Modeling relation paths for representation learning of
knowledge bases. arXiv preprint arXiv:1506.00379.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015b. Learning entity and relation em-
beddings for knowledge graph completion.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In Proceedings of
the 28th international conference on machine learning
(ICML-11), pages 809–816.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas.

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In
Advances in Neural Information Processing Systems,
pages 926–934.

Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchel-
l. 2012. Acquiring temporal constraints between rela-
tions. In CIKM.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014a. Knowledge graph and text jointly em-
bedding. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP)., pages 1591–1601.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014b. Knowledge graph embedding by trans-
lating on hyperplanes. In Proceedings of the Twenty-
Eighth AAAI Conference on Artificial Intelligence,
pages 1112–1119.

Yu Zhao, Zhiyuan Liu, and Maosong Sun. 2015. Rep-
resentation learning for measuring entity relatedness
with rich information. In Proceedings of the 24th
International Conference on Artificial Intelligence,
pages 1412–1418. AAAI Press.

2354


