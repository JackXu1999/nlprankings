



















































Incremental Parsing with Minimal Features Using Bi-Directional LSTM


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 32–37,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Incremental Parsing with Minimal Features Using Bi-Directional LSTM

James Cross and Liang Huang
School of Electrical Engineering and Computer Science

Oregon State University
Corvallis, Oregon, USA

{crossj,liang.huang}@oregonstate.edu

Abstract

Recently, neural network approaches for
parsing have largely automated the combi-
nation of individual features, but still rely
on (often a larger number of) atomic fea-
tures created from human linguistic intu-
ition, and potentially omitting important
global context. To further reduce fea-
ture engineering to the bare minimum, we
use bi-directional LSTM sentence repre-
sentations to model a parser state with
only three sentence positions, which au-
tomatically identifies important aspects of
the entire sentence. This model achieves
state-of-the-art results among greedy de-
pendency parsers for English. We also in-
troduce a novel transition system for con-
stituency parsing which does not require
binarization, and together with the above
architecture, achieves state-of-the-art re-
sults among greedy parsers for both En-
glish and Chinese.

1 Introduction

Recently, neural network-based parsers have be-
come popular, with the promise of reducing the
burden of manual feature engineering. For ex-
ample, Chen and Manning (2014) and subsequent
work replace the huge amount of manual fea-
ture combinations in non-neural network efforts
(Nivre et al., 2006; Zhang and Nivre, 2011) by
vector embeddings of the atomic features. How-
ever, this approach has two related limitations.
First, it still depends on a large number of care-
fully designed atomic features. For example, Chen
and Manning (2014) and subsequent work such as
Weiss et al. (2015) use 48 atomic features from
Zhang and Nivre (2011), including select third-
order dependencies. More importantly, this ap-
proach inevitably leaves out some nonlocal in-
formation which could be useful. In particular,

though such a model can exploit similarities be-
tween words and other embedded categories, and
learn interactions among those atomic features, it
cannot exploit any other details of the text.

We aim to reduce the need for manual induction
of atomic features to the bare minimum, by us-
ing bi-directional recurrent neural networks to au-
tomatically learn context-sensitive representations
for each word in the sentence. This approach al-
lows the model to learn arbitrary patterns from the
entire sentence, effectively extending the general-
ization power of embedding individual words to
longer sequences. Since such a feature representa-
tion is less dependent on earlier parser decisions,
it is also more resilient to local mistakes.

With just three positional features we can build
a greedy shift-reduce dependency parser that is on
par with the most accurate parser in the published
literature for English Treebank. This effort is sim-
ilar in motivation to the stack-LSTM of Dyer et al.
(2015), but uses a much simpler architecture.

We also extend this model to predict phrase-
structure trees with a novel shift-promote-adjoin
system tailored to greedy constituency parsing,
and with just two more positional features (defin-
ing tree span) and nonterminal label embeddings
we achieve the most accurate greedy constituency
parser for both English and Chinese.

2 LSTM Position Features

f1;b1

w1;t1

f2;b2

w2;t2

f3;b3

w3;t3

f4;b4

w4;t4

f5;b5

w5;t5

Figure 1: The sentence is modeled with an LSTM
in each direction whose input vectors at each time
step are word and part-of-speech tag embeddings.

32



The central idea behind this approach is exploiting
the power of recurrent neural networks to let the
model decide what apsects of sentence context are
important to making parsing decisions, rather than
relying on fallible linguistic information (which
moreover requires leaving out information which
could be useful). In particular, we model an in-
put sentence using Long Short-Term Memory net-
works (LSTM), which have made a recent resur-
gence after being initially formulated by Hochre-
iter and Schmidhuber (1997).

The input at each time step is simply a vector
representing the word, in this case an embedding
for the word form and one for the part-of-speech
tag. These embeddings are learned from random
initialization together with other network param-
eters in this work. In our initial experiments, we
used one LSTM layer in each direction (forward
and backward), and then concatenate the output
at each time step to represent that sentence posi-
tion: that word in the entire context of the sen-
tence. This network is illustrated in Figure 1.

h1

f21 ;b
2
1

f11 ;b
1
1

w1;t1

h2

f22 ;b
2
2

f12 ;b
1
2

w2;t2

h3

f23 ;b
2
3

f13 ;b
1
3

w3;t3

h4

f24 ;b
2
4

f14 ;b
1
4

w4;t4

h5

f25 ;b
2
5

f15 ;b
1
5

w5;t5

Figure 2: In the 2-Layer architecture, the output
of each LSTM layer is concatenated to create the
positional feature vector.

It is also common to stack multiple such LSTM
layers, where the output of the forward and back-
ward networks at one layer are concatenated to
form the input to the next. We found that parsing
performance could be improved by using two bi-
directional LSTM layers in this manner, and con-
catenating the output of both layers as the posi-
tional feature representation, which becomes the
input to the fully-connected layer. This architec-

input: w0 . . . wn−1
axiom 〈�, 0〉: ∅

shift
〈S, j〉 : A

〈S|j, j + 1〉 : A j < n

rex
〈S|s1|s0, j〉 : A

〈S|s0, j〉 : A ∪ {s1xs0}
goal 〈s0, n〉: A

Figure 3: The arc-standard dependency parsing
system (Nivre, 2008) (rey omitted). Stack S is
a list of heads, j is the start index of the queue,
and s0 and s1 are the top two head indices on S.

dependency constituency

positional s1, s0, q0 s1, s0, q0, s1.left, s0.left

labels - s0.{left, right, root, head}
s1.{left, right, root, head}

Table 1: Feature templates. Note that, remarkably,
even though we do labeled dependency parsing,
we do not include arc label as features.

ture is shown in Figure 2.
Intuitively, this represents the sentence position

by the word in the context of the sentence up to
that point and the sentence after that point in the
first layer, as well as modeling the “higher-order”
interactions between parts of the sentence in the
second layer. In Section 5 we report results us-
ing only one LSTM layer (“Bi-LSTM”) as well as
with two layers where output from each layer is
used as part of the positional feature (“2-Layer Bi-
LSTM”).

3 Shift-Reduce Dependency Parsing

We use the arc-standard system for dependency
parsing (see Figure 4). By exploiting the LSTM
architecture to encode context, we found that we
were able to achieve competitive results using only
three sentence-position features to model parser
state: the head word of each of the top two trees
on the stack (s0 and s1), and the next word on the
queue (q0); see Table 1.

The usefulness of the head words on the stack
is clear enough, since those are the two words that
are linked by a dependency when taking a reduce
action. The next incoming word on the queue is
also important because the top tree on the stack
should not be reduced if it still has children which
have not yet been shifted. That feature thus allows

33



input: w0 . . . wn−1
axiom 〈�, 0〉: ∅

shift
〈S, j〉

〈S | j, j + 1〉 j < n

pro(X)
〈S | t, j〉
〈S | X(t), j〉

adjx
〈S | t | X(t1...tk), j〉
〈S | X(t, t1...tk), j〉

goal 〈s0, n〉
Figure 4: Our shift-promote-adjoin system for
constituency parsing (adjy omitted).

the model to learn to delay a right-reduce until the
top tree on the stack is fully formed, shifting in-
stead.

3.1 Hierarchical Classification

The structure of our network model after com-
puting positional features is fairly straightforward
and similar to previous neural-network parsing ap-
proaches such as Chen and Manning (2014) and
Weiss et al. (2015). It consists of a multilayer
perceptron using a single ReLU hidden layer fol-
lowed by a linear classifier over the action space,
with the training objective being negative log soft-
max.

We found that performance could be improved,
however, by factoring out the decision over struc-
tural actions (i.e., shift, left-reduce, or right-
reduce) and the decision of which arc label to as-
sign upon a reduce. We therefore use separate
classifiers for those decisions, each with its own
fully-connected hidden and output layers but shar-
ing the underlying recurrent architecture. This
structure was used for the results reported in Sec-
tion 5, and it is referred to as “Hierarchical Ac-
tions” when compared against a single action clas-
sifier in Table 3.

4 Shift-Promote-Adjoin
Constituency Parsing

To further demonstrate the advantage of our idea
of minimal features with bidirectional sentence
representations, we extend our work from depen-
dency parsing to constituency parsing. However,
the latter is significantly more challenging than the
former under the shift-reduce paradigm because:

S

VP

NP

5sports
NNS

6

3like
VBP

4 7
NP

1I
PRP

2

9 8
1shift (I) 6pro (NP)
2pro (NP) 7adjy
3shift (like) 8pro (S)
4pro (VP) 9adjx
5shift (sports)

Figure 5: Shift-Promote-Adjoin parsing example.
Upward and downward arrows indicate promote
and (sister-)adjunction actions, respectively.

• we also need to predict the nonterminal labels
• the tree is not binarized (with many unary

rules and more than binary branching rules)

While most previous work binarizes the con-
stituency tree in a preprocessing step (Zhu et
al., 2013; Wang and Xue, 2014; Mi and Huang,
2015), we propose a novel “Shift-Promote-
Adjoin” paradigm which does not require any bi-
nariziation or transformation of constituency trees
(see Figure 5). Note in particular that, in our
case only the Promote action produces a new tree
node (with a non-terminal label), while the Ad-
join action is the linguistically-motivated “sister-
adjunction” operation, i.e., attachment (Chiang,
2000; Henderson, 2003). By comparison, in pre-
vious work, both Unary-X and Reduce-L/R-X ac-
tions produce new labeled nodes (some of which
are auxiliary nodes due to binarization). Thus our
paradigm has two advantages:

• it dramatically reduces the number of possi-
ble actions, from 3X + 1 or more in previ-
ous work to 3 + X , where X is the number
of nonterminal labels, which we argue would
simplify learning;

• it does not require binarization (Zhu et al.,
2013; Wang and Xue, 2014) or compression
of unary chains (Mi and Huang, 2015)

There is, however, a more closely-related “shift-
project-attach” paradigm by Henderson (2003).
For the example in Figure 5 he would use the fol-
lowing actions:

shift(I), project(NP), project(S), shift(like),
project(VP), shift(sports), project(NP), attach,
attach.

34



The differences are twofold: first, our Promote ac-
tion is head-driven, which means we only promote
the head child (e.g., VP to S) whereas his Project
action promotes the first child (e.g., NP to S); and
secondly, as a result, his Attach action is always
right-attach whereas our Adjoin action could be ei-
ther left or right. The advantage of our method is
its close resemblance to shift-reduce dependency
parsing, which means that our constituency parser
is jointly performing both tasks and can produce
both kinds of trees. This also means that we use
head rules to determine the correct order of gold
actions.

We found that in this setting, we did need
slightly more input features. As mentioned, node
labels are necessary to distinguish whether a tree
has been sufficiently promoted, and are helpful in
any case. We used 8 labels: the current and im-
mediate predecessor label of each of the top two
stacks on the tree, as well as the label of the left-
and rightmost adjoined child for each tree. We also
found it helped to add positional features for the
leftmost word in the span for each of those trees,
bringing the total number of positional features to
five. See Table 1 for details.

5 Experimental Results

We report both dependency and constituency pars-
ing results on both English and Chinese.

All experiments were conducted with minimal
hyperparameter tuning. The settings used for
the reported results are summarized in Table 6.
Networks parameters were updated using gradi-
ent backpropagation, including backpropagation
through time for the recurrent components, using
ADADELTA for learning rate scheduling (Zeiler,
2012). We also applied dropout (Hinton et al.,
2012) (with p = 0.5) to the output of each LSTM
layer (separately for each connection in the case of
the two-layer network).

We tested both types of parser on the Penn Tree-
bank (PTB) and Penn Chinese Treebank (CTB-5),
with the standard splits for each of training, de-
velopment, and test sets. Automatically predicted
part of speech tags with 10-way jackknifing were
used as inputs for all tasks except for Chinese de-
pendency parsing, where we used gold tags, fol-
lowing the traditions in literature.

5.1 Dependency Parsing: English & Chinese

Table 2 shows results for English Penn Tree-
bank using Stanford dependencies. Despite the
minimally designed feature representation, rela-
tively few training iterations, and lack of pre-
computed embeddings, the parser performed on
par with state-of-the-art incremental dependency
parsers, and slightly outperformed the state-of-
the-art greedy parser.

The ablation experiments shown in the Table 3
indicate that both forward and backward contexts
for each word are very important to obtain strong
results. Using only word forms and no part-of-
speech input similarly degraded performance.

Parser
Dev Test

UAS LAS UAS LAS
C & M 2014 92.0 89.7 91.8 89.6
Dyer et al. 2015 93.2 90.9 93.1 90.9
Weiss et al. 2015 - - 93.19 91.18
+ Percept./Beam - - 93.99 92.05
Bi-LSTM 93.31 91.01 93.21 91.16
2-Layer Bi-LSTM 93.67 91.48 93.42 91.36

Table 2: Development and test set results for shift-
reduce dependency parser on Penn Treebank using
only (s1, s0, q0) positional features.

Parser UAS LAS
Bi-LSTM Hierarchical† 93.31 91.01
† - Hierarchical Actions 92.94 90.96
† - Backward-LSTM 91.12 88.72
† - Forward-LSTM 91.85 88.39
† - tag embeddings 92.46 89.81

Table 3: Ablation studies on PTB dev set (wsj
22). Forward and backward context, and part-of-
speech input were all critical to strong performace.

Figure 6 compares our parser with that of Chen
and Manning (2014) in terms of arc recall for var-
ious arc lengths. While the two parsers perform
similarly on short arcs, ours significantly outpe-
forms theirs on longer arcs, and more interestingly
our accuracy does not degrade much after length
6. This confirms the benefit of having a global
sentence repesentation in our model.

Table 4 summarizes the Chinese dependency
parsing results. Again, our work is competitive
with the state-of-the-art greedy parsers.

35



1 2 3 4 5 6 7 8 9 10 11 12 13 14 >14
Arc Length

0.75

0.80

0.85

0.90

0.95
R

e
ca

ll

Bi-LSTM (this work)
Chen and Manning

Figure 6: Recall on dependency arcs of various
lengths in PTB dev set. The Bi-LSTM parser is
particularly good at predicting longer arcs.

Parser
Dev Test

UAS LAS UAS LAS
C & M 2014 84.0 82.4 83.9 82.4
Dyer et al. 2015 87.2 85.9 87.2 85.7
Bi-LSTM 85.84 85.24 85.53 84.89
2-Layer Bi-LSTM 86.13 85.51 86.35 85.71

Table 4: Development and test set results for shift-
reduce dependency parser on Penn Chinese Tree-
bank (CTB-5) using only (s1, s0, q0) position fea-
tures (trained and tested with gold POS tags).

5.2 Constituency Parsing: English & Chinese

Table 5 compares our constituency parsing re-
sults with state-of-the-art incremental parsers. Al-
though our work are definitely less accurate than
those beam-search parsers, we achieve the highest
accuracy among greedy parsers, for both English
and Chinese.1,2

Parser b
English Chinese

greedy beam greedy beam

Zhu et al. (2013) 16 86.08 90.4 75.99 85.6
Mi & Huang (05) 32 84.95 90.8 75.61 83.9
Vinyals et al. (05) 10 - 90.5 - -
Bi-LSTM - 89.75 - 79.44 -
2-Layer Bi-LSTM - 89.95 - 80.13 -

Table 5: Test F-scores for constituency parsing on
Penn Treebank and CTB-5.

1The greedy accuracies for Mi and Huang (2015) are from
Haitao Mi, and greedy results for Zhu et al. (2013) come from
duplicating experiments with code provided by those authors.

2The parser of Vinyals et al. (2015) does not use an ex-
plicit transition system, but is similar in spirit since generat-
ing a right bracket can be viewed as a reduce action.

Dependency Constituency
Embeddings
Word (dims) 50 100
Tags (dims) 20 100
Nonterminals (dims) - 100
Pretrained No No
Network details
LSTM units (each direction) 200 200
ReLU hidden units 200 / decision 1000
Training
Training epochs 10 10
Minibatch size (sentences) 10 10
Dropout (LSTM output only) 0.5 0.5
L2 penalty (all weights) none 1× 10−8
ADADELTA ρ 0.99 0.99
ADADELTA � 1× 10−7 1× 10−7

Table 6: Hyperparameters and training settings.

6 Related Work

Because recurrent networks are such a natural fit
for modeling languages (given the sequential na-
ture of the latter), bi-directional LSTM networks
are becoming increasingly common in all sorts
of linguistic tasks, for example event detection in
Ghaeini et al. (2016). In fact, we discovered after
submission that Kiperwasser and Goldberg (2016)
have concurrently developed an extremely similar
approach to our dependency parser. Instead of ex-
tending it to constituency parsing, they also apply
the same idea to graph-based dependency parsing.

7 Conclusions

We have presented a simple bi-directional LSTM
sentence representation model for minimal fea-
tures in both incremental dependency and incre-
mental constituency parsing, the latter using a
novel shift-promote-adjoint algorithm. Experi-
ments show that our method are competitive with
the state-of-the-art greedy parsers on both parsing
tasks and on both English and Chinese.

Acknowledgments

We thank the anonymous reviewers for comments.
We also thank Taro Watanabe, Muhua Zhu, and
Yue Zhang for sharing their code, Haitao Mi for
producing greedy results from his parser, and
Ashish Vaswani and Yoav Goldberg for discus-
sions. The authors were supported in part by
DARPA FA8750-13-2-0041 (DEFT), NSF IIS-
1449278, and a Google Faculty Research Award.

36



References
Danqi Chen and Christopher D Manning. 2014. A fast

and accurate dependency parser using neural net-
works. In Empirical Methods in Natural Language
Processing (EMNLP).

David Chiang. 2000. Statistical parsing with an
automatically-extracted tree-adjoining grammar. In
Proc. of ACL.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. arXiv preprint arXiv:1505.08075.

Reza Ghaeini, Xiaoli Z. Fern, Liang Huang, and Prasad
Tadepalli. 2016. Event nugget detection with
forward-backward recurrent neural networks. In
Proc. of ACL.

James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Pro-
ceedings of NAACL.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors.
CoRR, abs/1207.0580.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidi-
rectional LSTM feature representations. CoRR,
abs/1603.04351.

Haitao Mi and Liang Huang. 2015. Shift-reduce con-
stituency parsing with dynamic programming and
pos tag lattice. In Proceedings of the 2015 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies.

Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proc. of LREC.

Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In Advances in Neural
Information Processing Systems, pages 2755–2763.

Zhiguo Wang and Nianwen Xue. 2014. Joint pos
tagging and transition-based constituent parsing in
chinese with non-local features. In Proceedings of
ACL.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural network
transition-based parsing. In Proceedings of ACL.

Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR, abs/1212.5701.

Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL, pages 188–193.

Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of ACL
2013.

37


