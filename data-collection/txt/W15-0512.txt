



















































Learning Sentence Ordering for Opinion Generation of Debate


Proceedings of the 2nd Workshop on Argumentation Mining, pages 94–103,
Denver, Colorado, June 4, 2015. c©2015 Association for Computational Linguistics

Learning Sentence Ordering for Opinion Generation of Debate

Toshihiko Yanase Toshinori Miyoshi
Research & Development Group, Hitachi, Ltd.

{toshihiko.yanase.gm, toshinori.miyoshi.pd, kohsuke.yanai.cs, misa.sato.mw}
@hitachi.com

Kohsuke Yanai Misa Sato

Makoto Iwayama Yoshiki Niwa
Research & Development Group, Hitachi, Ltd.
{makoto.iwayama.nw, yoshiki.niwa.tx}

@hitachi.com

Paul Reisert Kentaro Inui
Tohoku University

{preisert, inui}@ecei.tohoku.ac.jp

Abstract

We propose a sentence ordering method to
help compose persuasive opinions for debat-
ing. In debate texts, support of an opinion
such as evidence and reason typically follows
the main claim. We focused on this claim-
support structure to order sentences, and de-
veloped a two-step method. First, we select
from among candidate sentences a first sen-
tence that is likely to be a claim. Second,
we order the remaining sentences by using a
ranking-based method. We tested the effec-
tiveness of the proposed method by compar-
ing it with a general-purpose method of sen-
tence ordering and found through experiment
that it improves the accuracy of first sentence
selection by about 19 percentage points and
had a superior performance over all metrics.
We also applied the proposed method to a con-
structive speech generation task.

1 Introduction

There are increasing demands for information struc-
turing technologies to support decision making us-
ing a large amount of data. Argumentation in debat-
ing which composes texts in a persuasive manner is
a research target suitable for such information struc-
turing. In this paper, we discuss sentence ordering
for constructive speech generation of debate.

The following is an example of constructive
speech excerpts that provide affirmative opinions on
the banning of gambling1.

1This example is excerpted from De-
batabase (http://idebate.org/debatabase). Copyright 2005

Motion: This House should ban gambling.
(1) Poor people are more likely to gamble,
in the hope of getting rich.
(2) In 1999, the National Gambling Im-
pact Commission in the United States
found that 80 percent of gambling revenue
came from lower-income households.

We can observe a typical structure of constructive
speech in this example. The first sentence describes
a claim that is the main statement of the opinion and
the second sentence supports the main statement. In
this paper, we focus on this claim-support structure
to order sentences.

Regarding the structures of arguments, we can
find research on the modeling of arguments (Free-
ley and Steinberg, 2008) and on recognition such as
claim detection (Aharoni et al., 2014). To the best
of our knowledge, there is no research that exam-
ines the claim-support structure of debate texts for
the sentence ordering problem. Most of the pre-
vious works on sentence ordering (Barzilay et al.,
2002; Lapata, 2003; Bollegala et al., 2006; Tan et
al., 2013) focus on the sentence order of news ar-
ticles and do not consider the structures of argu-
ments. These methods mingle claim and supportive
sentences together, which decreases the persuasive-
ness of generated opinions.

In this paper, we propose a sentence ordering
method in which a motion and a set of sentences
are given as input. Ordering all paragraphs of de-
bate texts at once is a quite difficult task, so we have

International Debate Education Association. All Rights
Reserved.

94



Unordered sentence set
(A) In 1999, the National 
Gambling Impact Commission
in the United States found that
 80 percent of ....

(B) Gambling can become 
a psychologically addictive 
behavior in some people.

(C) Taxing gambling is 
a regressive tax, and ....

Claim

Support
(A) In 1999, the National Gambling Impact Commission in the United States found
that 80 percent of ....
(C) Taxing gambling is a regressive tax, and ....

Ordered sentence list

(B) Gambling can become a psychologically addictive behavior in some people.

Sentence ordering

Figure 1: Target sentence ordering problem.

simplified by assuming that all input sentences stand
for a single viewpoint regarding the motion.

We use this claim-support structure as a cue of
sentence ordering. We employ two-step ordering
based on machine learning, as shown in Fig. 1. First,
we select a first sentence that corresponds to a claim,
and second, we order the supportive sentences of
the claims in terms of consistency. For each step,
we design machine learning features to capture the
characteristics of sentences in terms of the claim-
support structure. The dataset for training and test-
ing is made up of content from an online debate site.

The remainder of this paper is structured as fol-
lows. The next section describes related works deal-
ing with sentence ordering. In the third section, we
examine the characteristics of debate texts. Next, we
describe our proposed method, explain the exper-
iments we performed to evaluate the performance,
and discuss the results. After that, we describe our
application of the proposed sentence ordering to au-
tomated constructive speech generation. We con-
clude the paper with a summary and a brief mention
of future work.

2 Related Works

Previous research on sentence ordering has been
conducted as a part of multi-document summariza-
tion. There are four major feature types to order
sentences: publication dates of source documents,
topical similarity, transitional association cues, and
rhetorical cues.

Arranging sentences by order of publication dates
of source documents is known as the chronological
ordering (Barzilay et al., 2002). It is effective for
news article summarization because descriptions of
a certain event tend to follow the order of publica-
tion. It is, however, not suitable for opinion gen-
eration because such generation requires statements
and evidence rather than the simple summarization
of an event.

Topical similarity is based on an assumption that
neighboring sentences have a higher similarity than
non-neighboring ones. For example, bag-of-words-
based cosine similarities of sentence pairs are used
in (Bollegala et al., 2006; Tan et al., 2013). Another
method, the Lexical Chain, models the semantic dis-
tances of word pairs on the basis of synonym dic-
tionaries such as WordNet (Barzilay and Elhadad,
1997; Chen et al., 2005). The effectiveness of this
feature depends highly on the method used to calcu-
late similarity.

Transitional association is used to measure the
likelihood of two consecutive sentences. Lapata
proposed a sentence ordering method based on a
probabilistic model (Lapata, 2003). This method
uses conditional probability to represent transitional
probability from the previous sentence to the target
sentence.

Dias et al. used rhetorical structures to order sen-
tences (de S. Dias et al., 2014). The rhetorical struc-
ture theory (RST) (Mann and Thompson, 1988) ex-
plains the textual organization such as background
and causal effect that can be useful to determine the
sentence order. For example, causes are likely to
precede results. However, it is important to restrict
the types of rhetorical relation because original RST
defines many relations and a large amount of data is
required for accurate estimation.

There has been research on integrating different
types of features. Bollegara et al. proposed ma-
chine learning-based integration of different kinds
of features (Bollegala et al., 2006) by using a bi-
nary classifier to determine if the order of a given
sentence pair is acceptable or not. Tan et al. for-
mulated sentence ordering as a ranking problem of
sentences (Tan et al., 2013). Their experimental re-
sults showed that the ranking-based method outper-
formed classification-based methods.

95



Viewpoint Debate News
Word overlap in neighbors 3.14 4.30

Word overlap in non-neighbors 3.09 4.22
Occurrence of named entity 0.372 0.832

Table 1: Characteristics of debate texts and news articles.

3 Characteristics of Debate Texts

Topical similarity can be measured by the word
overlap between two sentences. This metric assumes
that the closer a sentence pair is, the more word over-
lap exists. In order to examine this assumption, we
compared characteristics between debate texts and
news articles, as shown in Table 1. In the Debate col-
umn, we show the statistics of constructive speech of
Debatabase, an online debate site. Each constructive
speech item in the debate dataset has 7.2 sentences
on average. Details of the debate dataset are de-
scribed in the experiment section. In the News col-
umn, we show the statistics of a subset of Annotated
English Gigaword (Napoles et al., 2012). We ran-
domly selected 80,000 articles and extracted seven
leading sentences per article.

Overall, we found less word overlap in debate
texts than in news articles in both neighbor pairs and
non-neighbor pairs. This is mainly because debaters
usually try to add as much information as possible.
We assume from this result that conventional topical
similarity is less effective for debate texts and have
therefore focused on the claim-support structure of
debate texts.

We also examined the occurrence of named entity
(NE) in each sentence. We can observe that most
of the sentences in news articles contain NEs while
much fewer sentences in debate texts have NEs. This
suggests that debate texts deal more with general
opinions and related examples while news articles
describe specific events.

4 Proposed Method

4.1 Two-Step Ordering

In this study, we focused on a simple but common
style of constructive speech. We assumed that a con-
structive speech item has a claim and one or more
supporting sentences. The flow of the proposed or-
dering method is shown in Fig. 2. The system re-

Feature extraction

Training first sentence
selection

Training ranking-based
ordering

Ordered sentences

First sentence
selection model

Ranking-based
ordering model

Unordered sentences

Feature extraction

First sentence
selection

Ranking-based
ordering

Ordered sentences

Training Prediction

Figure 2: Flowchart of two-step ordering.

ceives a motion and a set of sentences as input and
then it outputs ordered sentences. First, syntactic
parsing is applied to the input texts, and then features
for the machine learning models are then extracted
from the results. Second, we select the first sen-
tence, which is likely to be the claim sentence, from
the candidate sentences. This problem is formulated
as a binary-classification problem, where first sen-
tences of constructive speech items are positive and
all others are negative. Third, we order the remain-
ing sentences on the basis of connectivity of pairs of
sentences. This problem is formulated as a ranking
problem, similarly to (Tan et al., 2013).

4.2 Feature Extraction

We obtained the part of speech, lemma, syntactic
parse tree, and NEs of each input sentence by using
the Stanford Core NLP (Manning et al., 2014).

The following features, which are commonly used
in sentence ordering methods to measure local co-
herence (Bollegala et al., 2006; Tan et al., 2013; La-
pata, 2003), are then extracted.

Sentence similarity: Cosine similarity between
sentence u and v. We simply counted the
frequency of each word to measure cosine
similarity. In addition to that, we also mea-
sured the cosine similarity between latter half
of u (denoted as latter(u)) and former half
of v (denoted as former(v)). The sentences

96



are separated by the most centered comma (if
exists) or word (if no comma exists).

Overlap: Commonly shared words of u and
v. Let overlapj(u, v) be the number of
commonly shared words of u and v, for
j = 1, 2, 3 representing lemmatized noun,
verb and adjective or adverb, respectively. We
calculated overlapj(u, v)/ min(|u|, |v|) and
overlapj(latter(u), former(v))/overlapj(u, v),
where |u| is the number of words of sentence
u. The value will be set to 0 if the denominator
is 0.

Expanded sentence similarity: Cosine similarity
between candidate sentences expanded with
synonyms. We used WordNet (Miller, 1995)
to expand the nouns and verbs into synonyms.

Word transitional probability: Calculate condi-
tional probability P (wv|wu), where wu, wv
denote the words in sentences u, v, respec-
tively. In the case of the first sentence, we
used P (wu). A probabilistic model based on
Lapata’s method (Lapata, 2003) was created.

The following features are used to capture the char-
acteristics of claim sentences.

Motion similarity: Cosine similarity between the
motion and the target sentence. This feature ex-
amines the existence of the motion keywords.

Expanded motion similarity: Cosine similarity of
the target sentence to the motion expanded with
synonyms.

Value relevance: Ratio of value expressions. In
this study, we defined human values as the top-
ics obviously considered to be positive or neg-
ative and highly relevant to people’s values and
then created a dictionary of value expressions.
For example, health, education, and the envi-
ronment are considered positive for people’s
values while crime, pollution, and high costs
are considered negative.

Sentiment: Ratio of positive or negative words.
The dictionary of sentimental words is from
(Hu and Liu, 2004). This feature is used to ex-
amine whether the stance of the target sentence
is positive, negative, or neutral.

Type 1st step 2nd step

Sentence similarity ✓
Expanded sentence similarity ✓

Overlap ✓
Word transitional probability ✓ ✓

Motion similarity ✓ ✓
Expanded motion similarity ✓ ✓

Value relevance ✓ ✓
Sentiment ✓ ✓

Concreteness ✓ ✓
Estimated first sentence similarity ✓

Table 2: Features used in each step.

Concreteness features are used to measure the rele-
vance of support.

Concreteness features: The ratio of tokens that are
a part of capital words, numerical expression,
NE, organization, person, location, or temporal
expression. These features are used to capture
characteristics of the supporting sentences.

We use the estimated results of the first step as a
feature of the second step.

Estimated first sentence similarity: Cosine simi-
larity between the target sentence and the es-
timated first sentence.

4.3 First Step: First Sentence Selection

In the first step, we choose a first sentence from in-
put sentences. This task can be formulated as a bi-
nary classification problem. We employ a machine
learning approach to solve this problem.

In the training phase, we extract N feature vectors
from N sentences in a document, and train a binary
classification function ffirst defined by

ffirst(si) =

{
+1 (i = 0)
−1 (i ̸= 0) , (1)

where si denotes the feature vector corresponding to
the i-th sentence. The function ffirst returns +1 if si
is the first sentence.

In the prediction phase, we applied ffirst to all sen-
tences and determined the first sentence that has the
maximum posterior probability of ffirst(si) = +1.

97



We used Classias2 (Okazaki, 2009), an implemen-
tation of logistic regression, as a binary classifier.

4.4 Second Step: Ranking-Based Ordering

In the second step, we assume that the first sen-
tence has already been determined. The number of
sentences in this step is Nsecond = N − 1. We
use a ranking-based framework proposed by Tan et
al. (2013) to order sentences.

In the training phase, we generate
Nsecond(Nsecond − 1) pairs of sentences from
Nsecond sentences in a document and train an
association strength function fpair defined by

fpair(si, sj) =

{
Nsecond − (j − i) (j > i)
0 (j ≤ i) . (2)

For forward direction pairs, the rank values are set to
N − (j− i). This means that the shorter the distance
between the pair is, the larger the rank value is. For
the backward direction pairs, the rank values are set
to 0.

In the prediction phase, the total ranking value of
a sentence permutation ρ is defined by

frank(ρ) =
∑

u,v;ρ(u)>ρ(v)

fpair(u, v), (3)

where ρ(u) > ρ(v) denotes that sentence u precedes
sentence v in ρ. A learning to rank algorithm based
on Support Vector Machine (Joachims, 2002) is used
as a machine learning model. We used svmrank 3 to
implement the training and the prediction of fpair.

We used the sentence similarity, the expanded
sentence similarity, the overlap, and the transitional
probability in addition to the same features as the
first step classification. These additional features are
defined by a sentence pair (u, v). We applied the fea-
ture normalization proposed by Tan et al. (2013) to
each additional feature. The normalization functions
are defined as

2http://www.chokkan.org/software/classias/
3http://www.cs.cornell.edu/people/tj/svm light/svm rank.html

Vi,1 = fi(u, v), (4)

Vi,2 =

{
1/2, if fi(u, v) + fi(v, u) = 0

fi(u,v)
fi(u,v)+fi(v,u)

, otherwise
(5)

Vi,3 =

{
1/|S|, if ∑y∈S\{u} fi(u, y) = 0

fi(u,v)∑
y∈S\{u} fi(u,y)

, otherwise
(6)

Vi,4 =

{
1/|S|, if ∑x∈S\{v} fi(x, v) = 0

fi(u,v)∑
x∈S\{v} fi(x,v)

, otherwise
(7)

where fi is the i-th feature function, S is a set of
candidate sentences, and |S| is the number of sen-
tences in S. Equation (4) is an original value of the
i-th feature function. Equation (5) examines the pri-
ority of (u, v) to its inversion (v, u). Equation (6)
measures the priority of (u, v) to the sentence pairs
that have u as a first element. Equation (7) the pri-
ority of (u, v) to the sentence pairs that have v as a
second element, similarly to Equation (6).

5 Experiments

5.1 Reconstructing Shuffled Sentences

We evaluated the proposed method by reconstruct-
ing the original order from randomly shuffled texts.
We compared the proposed method with the Ran-
dom method, which is a base line method that ran-
domly selects a sentence, and the Ranking method,
which is a form of Tan et al.’s method (Tan et al.,
2013) that arranges sentences using the same pro-
cedure as the second step of the proposed method
excluding estimated first sentence similarity feature.

Dataset

We created a dataset of constructive speech items
from Debatabase to train and evaluate the pro-
posed method. The speech item of this dataset
is a whole turn of affirmative/negative constructive
speech which consists of several ordered sentences.
Details of the dataset were shown in Table 3. The
dataset has 501 motions related to 14 themes (e.g.,
politics, education) and contains a total of 3,754
constructive speech items. The average sentence
length per item is 7.2. Each constructive speech item
has a short title sentence from which we extract the
value (e.g., “health”, “crime”) of the item.

98



Affirmative
no. of constructive speech items 1,939

no. of sentences 14,021

Negative
no. of constructive speech items 1,815

no. of sentences 13,041

Table 3: Details of constructive speech dataset created
from Debatabase.

Metrics
The overall performance of ordering sentences is

evaluated by Kendall’s τ , Spearman Rank Correla-
tion, and Average Continuity.

Kendall’s τ is defined by

τk = 1− 2ninv
N (N − 1) /2 , (8)

where N is the number of sentences and ninv is the
number of inversions of sentence pairs. The met-
ric ranges from −1 (inversed order) to 1 (identical
order). Kendall’s τ measures the efforts of human
readers to correct wrong sentence orders.

Spearman Rank Correlation is defined by

τs = 1− 6
N (N + 1) (N − 1)

N∑
i=1

d (i)2 , (9)

where d(i) is the difference between the correct rank
and the answered rank at the i-th sentence. Spear-
man Rank Correlation takes the distance of wrong
answers directly into account.

Average Continuity is based on the number of
matched n-grams, and is defined using Pn. Pn is
defined by

Pn =
m

N − n + 1 , (10)

where m is the number of matched n-grams. Pn
measures the ratio of correct n-grams in a sequence.
Average Continuity is then defined by

τa = exp

(
k∑

n=2

log (Pn + α)

)
, (11)

where k is the maximum n of n-grams, and α is a
small positive value to prevent divergence of score.
In this experiment, we used k = 4, α = 0.01 in
accordance with (Bollegala et al., 2006).

Method Mean accuracy [%] Std.
Random 17.9 0.81
Ranking 23.3 0.61
Proposed 42.6 1.58

Table 4: Results of the first sentence estimation.

Results

We applied 5-fold cross validation to each order-
ing method. The machine learning models were
trained by 3,003 constructive speech items and then
evaluated using 751 items.

The results of first sentence estimation are shown
in Table 4. The accuracy of the proposed method
is higher than that of Ranking, which represents
the sentence ranking technique without the first sen-
tence selection, by 19.3 percentage points. Although
the proposed method showed the best accuracy, we
observed that ffirst(s0) tended to be −1 rather than
1. This is mainly because the two classes were
unbalanced. The number of negative examples in
the training data was 6.2 times larger than that of
positive ones. We need to address the unbalanced
data problem for further improvement (Chawla et
al., 2004).

The results of overall sentence ordering are shown
in Table 5. We carried out a one-way analysis of
variance (ANOVA) to examine the effects of differ-
ent algorithms for sentence ordering. The ANOVA
revealed reliable effects with all metrics (p < 0.01).
We performed a Tukey Honest Significant Differ-
ences (HSD) test to compare differences among
these algorithms. In terms of Kendall’s τ and Spear-
man Rank Correlation, the Tukey HSD test revealed
that the proposed method was significantly better
than the rests (p < 0.01). In terms of Average Con-
tinuity, it was also significantly better than the Ran-
dom method, whereas it is not significantly different
from the Ranking method. These results show that
the proposed two-step ordering is also effective for
overall sentence ordering. However, the small dif-
ference of Average Continuity indicates that the or-
dering improvement is only regional.

5.2 Subjective Evaluation

In addition to our evaluation of the reconstruction
metrics, we also conducted a subjective evaluation

99



Method Kendall’s τ Spearman Average Continuity
Random −6.92× 10−4 −1.91× 10−3 5.92× 10−2
Ranking 6.22× 10−2 7.89× 10−2 7.13× 10−2
Proposed 1.17× 10−1 1.44× 10−1 8.36× 10−2

Table 5: Results of overall sentence ordering.

with a human judge. In this evaluation, we selected
target documents that were ordered uniquely by peo-
ple as follows. First, the judge ordered shuffled sen-
tences and then, we selected the correctly ordered
documents as targets. The number of target docu-
ments is 24.

Each ordering was awarded one of four grades:
Perfect, Acceptable, Poor or Unacceptable. The cri-
teria of these grades are the same as those of (Bol-
legala et al., 2006). A perfect text cannot be im-
proved by re-ordering. An acceptable text makes
sense and does not require revision although there
is some room for improvement in terms of read-
ability. A poor text loses the thread of the story in
some places and requires amendment to bring it up
to an acceptable level. An unacceptable text leaves
much to be improved and requires overall restructur-
ing rather than partial revision.

The results of our subjective evaluation are shown
in Figure 3. We have observed that about 70 % of
randomly ordered sentences are perfect or accept-
able. This is mainly because the target documents
contain only 3.87 sentences on average, and those
short documents are comprehensive even if they are
randomly shuffled.

There are four documents containing more than
six sentences in the targets. The number of unac-
ceptably ordered documents of the Random method,
the Ranking method, and the proposed method are
4, 3, and 1, respectively. We observed that the
proposed method selected the claim sentences suc-
cessfully and then arranged sentences related to the
claim sentences. These are the expected results of
the first sentence classification and the estimated
first sentence similarity in the second step. These
results show that the selection of the first sentence
plays an important role to make opinions compre-
hensive.

On the other hand, we did not observe the im-
provement of the number of the perfectly selected

0% 20% 40% 60% 80% 100%

Proposed

Ranking

Random

Perfect Acceptable Poor Unacceptable

Figure 3: Results of subjective evaluation.

Position Claim [%] Support [%]
1 62.5 3.93
2 8.93 19.7
3 7.14 20.2
4 5.36 17.4

5+ 16.1 38.7

Table 6: Sentence type annotation in constructive speech.

documents. We found misclassification of final sen-
tences as first sentences in the results of the proposed
method. Such final sentences described conclusions
similar to the claim sentences. We need to extend
the structure of constructive speech to handle con-
clusions correctly.

6 Discussion

6.1 Structures of Constructive Speech
We confirmed our assumption that claims are more
likely to be described in the first sentence than others
by manually examining constructive speech items.
We selected seven motions from the top 100 debates
in the Debatabase. These selected motions contain
a total of 56 constructive speech items. A human
annotator assigned claim tags and support tags for
the sentences. The results are shown in Table 6.

Here, we can see that about two-thirds of claim

100



# Text
1 The contributions of government funding have

been shown to be capable of sustaining the costs of

a museum, preventing those costs being passed on

to the public in the form of admissions charges.

2 The examples of the British Labour government

funding national museums has been noted above.

3 The National Museum of the American Indian in

Washington was set up partially with government

funding and partially with private funds, ensuring it

has remained free since its opening in 2004 (

Democracy Now , 2004 ).

4 In 2011 , China also announced that from 2012 all

of its national museums would become

publicly-funded and cease charging admissions

fees ( Zhu & Guo , 2011 ).

Table 7: A typical example ordered correctly by the pro-
posed method. The motion is “This House would make
all museums free of charge.” The motion and sentences
are from Debatabase.

sentences appeared at the beginning of constructive
speech items, and that more than 90 % of support-
ive sentences appeared from the second sentences or
later. This means that claims are followed by evi-
dence in more than half of all constructive speech
items.

6.2 Case Analysis

A typical example ordered correctly by the pro-
posed method is shown in Table 7. This constructive
speech item agrees with free admissions at muse-
ums. It has a clear claim-support structure. It first,
makes a claim related to the contributions of gov-
ernment funding and then gives three examples. The
first sentence has no NEs while the second and later
sentences have NEs to give details about the actual
museums and countries. Neighbor sentences were
connected with common words such as “museum,”
“charge,” and “government funding.”

7 Application to Automated Constructive
Speech Generation

We applied the proposed sentence ordering to the au-
tomated constructive speech generation.

Value Selection

Sentence Extraction

Sentence Ordering

Motion Analysis

Text Data
with Annotation

Value 
Dictionary

Figure 4: Flow of automated constructive speech genera-
tion.

System Description

The flowchart of constructive speech generation
is shown in Fig. 4. Here, we give a brief overview
of the system. The system is based on sentence ex-
traction and sentence ordering, which we explain
with the example motion “This House should ban
smoking in public spaces.” First, a motion analy-
sis component extracts keywords such as “smoking”
and “public spaces” from the motion. Second, a
value selection component searches for related sen-
tences with the motion keywords and human value
information. More specifically, it generates pairs
of motion keywords and values (such as (smok-
ing, health), (smoking, education), and (smoking,
crime)) and uses them as search queries. Then, it se-
lects the values of constructive speech in accordance
with the number of related sentences to values. In
the third step, a sentence extraction component ex-
amines the relevancy of each sentence with tex-
tual annotation such as promote/suppress relation-
ship and positive/negative relationship. Finally, a
sentence ordering component arranges the extracted
sentences for each value.

Ordering Results

The system outputs three paragraphs per motion.
Each paragraph is composed of seven sentences.
Currently, its performance is limited, as 49 out of the
150 generated paragraphs are understandable. To
focus on the effect of sentence ordering, we man-
ually extracted relevant sentences from generated
constructive speech and then applied the proposed
ordering method to them.

101



# Text
1 Smoking is a serious public health problem that

causes many diseases such as heart diseases, lung

diseases, eye problems, as well as risks for women

and babies.

2 Brendan McCormick, a spokesman for

cigarette-maker Philip Morris USA, said, “We

agree with the medical and scientific conclusions

that cigarette smoking causes serious diseases in

smokers, and that there is no such thing as a safe

cigarette.”

3 The study, released by the Rio de Janeiro State

University and the Cancer Institute, showed that

passive smoking could cause serious diseases, such

as lung cancer, cerebral hemorrhage, angina

pectoris, myocardial infection and coronary

thrombosis.

Table 8: A result of sentence ordering in automated con-
structive speech generation. The motion is “This House
would further restrict smoking.” The motion is from De-
batabase, and sentences are from Annotated English Gi-
gaword.

The results are shown in Table 84. We can observe
that the first sentence mentions the health problem of
smoking while the second and third sentences show
support for the problem, i.e., the names of authori-
ties such as spokesmen and institutes. The proposed
ordering method successfully ordered the types of
opinions that have a clear claim-support structure.

8 Conclusion

In this paper, we discussed sentence ordering for
debate texts. We proposed a sentence ordering
method that employs a two-step approach based on
the claim-support structure. We then constructed a
dataset from an on-line debate site to train and eval-
uate the ordering method. The evaluation results
of reconstruction from shuffled constructive speech

4These sentences are extracted from Annotated English
Gigaword. Portions c⃝1994-2010 Agence France Presse,
c⃝1994-2010 The Associated Press, c⃝1997-2010 Central

News Agency (Taiwan), c⃝1994-1998, 2003-2009 Los Angeles
Times-Washington Post News Service, Inc., c⃝1994-2010 New
York Times, c⃝2010 The Washington Post News Service with
Bloomberg News, c⃝1995-2010 Xinhua News Agency, c⃝2012
Matthew R. Gormley, c⃝2003, 2005, 2007, 2009, 2011, 2012
Trustees of the University of Pennsylvania

showed that our proposed method outperformed
a general-purpose ordering method. The subjec-
tive evaluation showed that our proposed method is
suitable for constructive speech containing explicit
claim sentences and supporting examples.

In this study, we focused on a very simple struc-
ture, i.e., claims and support. We will extend this
structure to handle different types of arguments in
the future. More specifically, we plan to take con-
clusion sentences into account as a component of the
structure.

References
Ehud Aharoni, Anatoly Polnarov, Tamar Lavee, Daniel

Hershcovich, Ran Levy, Ruty Rinott, Dan Gutfreund,
and Noam Slonim. 2014. A benchmark dataset for
automatic detection of claims and evidence in the con-
text of controversial topics. In Proceedings of the
First Workshop on Argumentation Mining, pages 64–
68. Association for Computational Linguistics.

Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In Proceedings of
the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10–17.

Regina Barzilay, Noemie Elhadad, and Kathleen R.
McKeown. 2002. Inferring strategies for sentence or-
dering in multidocument news summarization. J. Ar-
tif. Int. Res., 17(1):35–55, August.

Danushka Bollegala, Naoaki Okazaki, and Mitsuru
Ishizuka. 2006. A bottom-up approach to sentence
ordering for multi-document summarization. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics, ACL-
44, pages 385–392, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

Nitesh V. Chawla, Nathalie Japkowicz, and Aleksander
Kotcz. 2004. Editorial: Special issue on learning
from imbalanced data sets. SIGKDD Explor. Newsl.,
6(1):1–6, June.

Yan-Min Chen, Xiao-Long Wang, and Bing-Quan Liu.
2005. Multi-document summarization based on lexi-
cal chains. In Proceedings of 2005 International Con-
ference on Machine Learning and Cybernetics 2005,
volume 3, pages 1937–1942 Vol. 3, Aug.

Márcio de S. Dias, Valéria D. Feltrim, and Thiago
Alexandre Salgueiro Pardo. 2014. Using rhetorical
structure theory and entity grids to automatically eval-
uate local coherence in texts. Proceedings of the 11th
International Conference, PROPOR 2014, pages 232–
243.

102



Austin J. Freeley and David L. Steinberg. 2008. Argu-
mentation and Debate. WADSWORTH CENGAGE
Learning.

Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168–177.

Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’02, pages
133–142, New York, NY, USA. ACM.

Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ’03, pages 545–
552, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations, pages 55–60.

George A. Miller. 1995. Wordnet: A lexical database for
English. Communications of the ACM, 38(11):39–41.

Courtney Napoles, Matthew Gormley, and Benjamin Van
Durme. 2012. Annotated English Gigaword
ldc2012t21. AKBC-WEKEX ’12, pages 95–100. As-
sociation for Computational Linguistics.

Naoaki Okazaki. 2009. Classias: a collection of
machine-learning algorithms for classification.

Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2013. Learn-
ing to order natural language texts. Proceedings of the
51st Annual Meeting of the Association for Computa-
tional Linguistics, pages 87–91.

103


