



















































Right-truncatable Neural Word Embeddings


Proceedings of NAACL-HLT 2016, pages 1145–1151,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Right-truncatable Neural Word Embeddings

Jun Suzuki and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{suzuki.jun, nagata.masaaki}@lab.ntt.co.jp

Abstract

This paper proposes an incremental learning
strategy for neural word embedding methods,
such as SkipGrams and Global Vectors. Since
our method iteratively generates embedding
vectors one dimension at a time, obtained vec-
tors equip a unique property. Namely, any
right-truncated vector matches the solution of
the corresponding lower-dimensional embed-
ding. Therefore, a single embedding vector
can manage a wide range of dimensional re-
quirements imposed by many different uses
and applications.

1 Introduction

Word embedding vectors obtained from ‘neural
word embedding methods’, such as SkipGram,
continuous bag-of-words (CBoW) and the fam-
ily of vector log-bilinear (vLBL) models (Mnih
and Kavukcuoglu, 2013; Mikolov et al., 2013a;
Mikolov et al., 2013c; Mikolov et al., 2013b) have
now become an important fundamental resource for
tackling many natural language processing (NLP)
tasks. These NLP tasks include part-of-speech tag-
ging (Tsuboi, 2014; Ling et al., 2015), dependency
parsing (Chen and Manning, 2014; Dyer et al., 2015;
Alberti et al., 2015), semantic role labeling (Zhou
and Xu, 2015; Woodsend and Lapata, 2015), ma-
chine translation (Sutskever et al., 2014), sentiment
analysis (Kim et al., 2015), and question answer-
ing (Wang and Nyberg, 2015).

The main purpose of this paper is to further en-
hance the ‘usability’ of obtained embedding vectors
in actual use. To briefly explain our motivation, we
first introduce the following concept:

Definition 1 (D′-right-truncated vector1). Let w′
and w′′ be vectors, whose dimensions are D′ and
D′′, respectively. Namely, w′ = (w′1, . . . , w′D′)
and w′′ = (w′′1 , . . . , w′′D′′). Suppose w matches
the concatenation of w′ and w′′, that is, w =
(w′1, . . . , w′D′ , w

′′
1 , . . . , w

′′
D′′). Then, we define w

′ as
a D′-right-truncated vector of w.

This paper focuses on the fact that the appropriate
dimension of embedding vectors strongly depends
on applications and uses, and is basically determined
based on the performance and memory space (or
calculation speed) trade-off. Indeed, the actual di-
mensions of the previous studies listed above are di-
verse; often around 50, and at most 1000. It is worth
noting here that each dimension of embedding vec-
tors obtained by conventional methods has no inter-
pretable meaning. Thus, we basically need to re-
train D′-dimensional embedding vectors even if we
already have a well-trained D-dimensional vector.
In addition, we cannot take full advantage of freely
available high-quality pre-trained embedding vec-
tors2 since their dimensions are already given and
fixed, i.e., D=300.

To reduce the additional computational cost of the
retraining, and to improve the ‘usability’ of embed-
ding vectors, we propose a framework for incremen-
tally determining embeddings one dimension at a
time from 1 to D. As a result, our method always
offers the relation that ‘any D′-right-truncated em-

1The term ‘right-truncated’ is originally taken from ‘right-
truncatable prime’

2i.e., ‘GoogleNews-vectors-negative300’ ob-
tained from https://code.google.com/archive/p/word2vec/,
and ‘glove.840B.300d’ obtained from
http://nlp.stanford.edu/projects/glove/

1145



bedding vector is the solution for D′-dimensional
embeddings of our method’. Therefore, in actual
use, we only need to construct a relatively higher-
dimensional embedding vector ‘just once’, i.e., D =
1000, and then truncate it to an appropriate dimen-
sion for the application.

2 Neural Word Embedding Methods

Let U and V be two sets of predefined vocabularies
of possible inputs and outputs. Let |U| and |V| be
the number of words in U and V , respectively. Then,
neural word embedding methods generally assign a
D-dimensional vector to each word in U and V . We
denote ei as representing the i-th input vector, and
oj for the j-th output vector. In the rest of this paper,
for convenience the notation ‘i’ is always used as the
index of input vectors, and ‘j’ as the index of output
vectors, where 1 ≤ i ≤ |U| and 1 ≤ j ≤ |V|.

We introduce E and O that represent lists of
all input and output vectors, respectively. Namely,
E = (e1, · · · , e|U|) and O = (o1, · · · ,o|V|). X
represents training data. Then, embedding vectors
are obtained by solving the following form of a min-
imization problem defined in each neural word em-
bedding method:

(Ê, Ô) = arg min
E,O

{
Ψ(E,O | X )}, (1)

where Ψ represents the objective function, and Ê
and Ô are lists of solution embedding vectors.

Hereafter, we use Ψ as an abbreviation of
Ψ(E,O | X ). For example, the objective function Ψ
of ‘SkipGram with negative sampling (SGNS)’ can
be written in the following form3 :

Ψ =
∑
(i,j)

(
ci,jL(xi,j) + c′i,jL(−xi,j)

)
, (2)

where xi,j = ei · oj , and L(x) represents a logistic
loss function, namely, L(x) = log(1 + exp(−x)).
Moreover, ci,j and c′i,j represent co-occurrences of
the i-th input and j-th output words in training data
and negative sampling data, respectively.

Another example, the objective function Ψ of the
‘Global Vector (GloVe)’ can be written in the fol-

3We can obtain this form by a simple reformulation from the
original objective of SGNS (Mikolov et al., 2013b).

Input: X : training data, D: maximum number of di-
mensions (iterations)

1: E(0) ← ∅, O(0) ← ∅, and B(0) ← 0, d← 0
2: repeat
3: d← d+ 1
4: (q̄d, r̄d)←updateParams1D(X ,B(d−1)) // Eq. 5
5: E(d) ← appendVec(E(d−1), q̄d)
6: O(d) ← appendVec(O(d−1), r̄d)
7: B(d) ← updateBias(B(d−1), q̄d, r̄d) // Eq. 4
8: until d = D
Output: (E(D),O(D))

Figure 1: An algorithm for solving an iterative additional coor-
dinate optimization formulation for obtaining embedding vec-

tors.

lowing form (Pennington et al., 2014):

Ψ =
1
2

∑
(i,j)

βi,j(xi,j −mi,j)2, (3)

where mi,j and βi,j represent certain co-occurrence
and weighting factors of the i-th input and the j-
th output words, respectively. For example, βi,j =
min(1, (ci,j/xmax)γ), and mi,j = log(ci,j) are used
in (Pennington et al., 2014), where xmax and γ are
tunable hyper-parameters.

3 Incremental Construction of Embedding

This section explains our proposed method. The ba-
sic idea is very simple and clear: we convert the
minimization problem shown in Eq. 1 to a series
of minimization problems, each of whose individ-
ual problem determines one additional dimension of
each embedding vector. We refer to this formulation
of embedding problems as ‘ITerative Additional Co-
ordinate Optimization (ITACO)’ formulation. Fig. 1
shows our entire optimization algorithm for this for-
mulation.

3.1 Bias terms and optimization variables

Suppose d represents a discrete time step, where d ∈
{1, . . . , D}. Let B(d) be a matrix representation of
bias terms at the d-th time step, and b(d)i,j denote the

(i, j)-factor of B(d). Then, we define that b(d)i,j for all
(i, j) and d have the following recursive relation:

b
(d)
i,j =

d∑
k=1

ei,koj,k = b
(d−1)
i,j + ei,doj,d, (4)

1146



Figure 2: Relation of ei and q̄d used to represent input vectors
in this paper.

where we define b(0)i,j = 0 for all (i, j). This rela-
tion implies that the solutions of former optimiza-
tions are used as bias terms in latter optimizations.

Next, we define q̄d and r̄d as the vector represen-
tations of the concatenation of all the input and out-
put parameters at the d-th step, respectively, that is,
q̄d = (e1,d, . . . , e|U|,d) and r̄d = (o1,d, . . . , o|V|,d).
Note that ei used in the former part of this paper
is a D-dimensional vector while q̄d and r̄d defined
here are |U|-dimensional and |V|-dimensional vec-
tors, respectively. Moreover, there are relations that
ei,d is the d-th factor of ei, and, at the same time, the
i-th factor of q̄d.

Fig. 2 illustrates the relation of ei and q̄d in this
paper. We omit to explicitly show the relation of oj
and r̄d, which are used to represent output vectors
because of the space reason. However obviously,
they also have the same relation as ei and q̄d.

3.2 Individual optimization problem

Then, we define the d-th optimization problem in
our ITACO formulation as follows:

(q̄d, r̄d) = arg min
q̄,r̄

{
Ψ̄
(
q̄, r̄|X ,B(d−1))}

subject to:
|V|
|U| ||q̄||p = ||r̄||p,

(5)

where || · ||p represents the Lp-norm. We generally
assume that p = {1, 2,∞}, and often select p = 2.
Note that q̄d is optimization parameters in the d-th
optimization problem while B(d−1) is the constant.
Fig. 3 illustrates the relation of B(d−1) and q̄d.

We assume that the objective function Ψ̄ takes an
identical form as used in one of the conventional
methods such as SGNS and GloVe as shown by

Figure 3: Relation of B(d−1) and q̄d, which are the constant
and optimization parameters in the d-th optimization problems,

respectively.

Eqs. 2 and 3. The difference appears in the vari-
ables; our ITACO formulation uses xi,j =eioj + bi,j
rather than xi,j =ei ·oj as described in Sec. 2.
3.3 Improving stability of embeddings

The additional norm constraint in Eq. 5 is introduced
to improve stability. The optimization problems of
neural word embedding methods including SGNS
and GloVe can be categorized as a bi-convex op-
timization problem (Gorski et al., 2007); they are
convex with respect to the parameters E if the pa-
rameters O are assumed to be constants, and vice
versa. One well-known drawback of unconstrained
bi-convex optimization is that the optimization pa-
rameters can possibly diverge to ±∞ (See Exam-
ple 4.3 in (Gorski et al., 2007)). This is because the
objective function only cares about the inner prod-
uct value of two vectors. Therefore, each parameter
can easily have a much larger value, i.e., o1 = 109,
if e1 is smaller and approaches a zero value i.e.,
e1 = 10−10. This is mainly caused by inconsistent
scale problem. Thus, our norm constraint in Eq. 5
can eliminate this problem by maintaining the scale
of q̄ and r̄ at the same level.

3.4 Optimization algorithm

To solve Eq. 5, we employ the idea of the ‘Alternat-
ing Convex Optimization (ACO)’ algorithm (Gorski
et al., 2007). ACO and its variants have been widely
developed in the context of (non-negative) matrix
factorization, i.e., (Kim et al., 2014), and are empir-
ically known to be an efficient method in practice.
The main idea of ACO is that it iteratively and al-

1147



ternatively updates one parameter set, i.e., q̄, while
the other distinct parameter set is fixed, i.e., r̄. In
our case, ACO solves the following two optimiza-
tion problems iteratively and alternately:

q̄d = arg min
q̄

{
Ψ̄(q̄, r̄ | X ,B(d−1))} (6)

r̄d = arg min
r̄

{
Ψ̄(q̄, r̄ | X ,B(d−1))}. (7)

There are at least two advantages of using ACO; (1)
Eqs. 6 and 7 both become convex optimization prob-
lems. Therefore, the global optimum solution can be
obtained when ∂eiΨ̄ = 0 for all i and ∂oj Ψ̄ = 0 for
all j, respectively. (2) ACO guarantees to converge
to a stationary point (one of the local minima)4.

For example, by a simple reformulation of ∂eiΨ̄=
0, we obtain the closed form solution of Eq. 6 with
the GloVe objective, that is,

ei =

∑
j βi,j(mi,j − bi,j)oj∑

j βi,j(oj)2
∀i. (8)

Similarly, the closed form solution of Eq. 7 is:

oj =
∑

i βi,j(mi,j − bi,j)ei∑
i βi,j(ei)2

∀j. (9)

Thus, we can solve Eqs. 6 and 7 without performing
iterative estimation. Next, we obtain the following
equation by a simple reformulation of ∂eiΨ̄ = 0 for
the SGNS objective:∑

j

ci,joj =
∑
j

(ci,j + c′i,j)σ(eioj + bi,j)oj , (10)

where σ(x) represents a sigmoid function, that is,
σ(x) = 11+exp(−x) . Similarly, we also obtain the
following form of the equation for Eq. 7:∑

i

ci,jei =
∑
i

(ci,j + c′i,j)σ(eioj + bi,j)ei. (11)

These equations are efficiently solvable by a sim-
ple binary search procedure since each equation only
has a single parameter, that is, ei or oj ,

During the optimization, there is no guarantee that
the constraint |V||U| ||q̄||p = ||r̄||p always holds. Fortu-
nately, the following transformations always satisfy

4We somehow prevent the divergence of optimization pa-
rameters (Gorski et al., 2007)

Input: X : training data, B: matrix form of bias terms,
�: constant for convergence check

1: q̄← 1, and r̄← 0
2: repeat
3: r̄← updateIVec1D(r̄ | q̄,B) // Eq. 11 or 9
4: (q̄, r̄)← scaleVec(q̄, r̄) // Eq. 12
5: q̄← updateOVec1D(q̄ | r̄,B) // Eq. 10 or 8
6: (q̄, r̄)← scaleVec(q̄, r̄) // Eq. 12
7: until ConvergenceCheck(�)
Output: (q̄, r̄)

Figure 4: Procedure of updateParams1D in Fig. 1 using the
ACO-based algorithm.

this norm constraint:

ẽi =
|U|
|V|

ei
||q̄||p

( |V|
|U| ||q̄||p||r̄||p

) 1
2 ∀i

õj =
oj
||r̄||p

( |V|
|U| ||q̄||p||r̄||p

) 1
2 ∀j,

(12)

which also maintain ẽiõj = eioj , and the objective
value. Thus, we can safely apply them at any time
during the optimization.

Finally, Fig. 4 shows the optimization procedure
when using the ACO framework.

4 Experiments

As in previously reported neural word embedding
papers, our training data was taken from a Wikipedia
dump (Aug. 2014). We used hyperwords tool5

for our data preparation (Levy et al., 2015).
We compared our method, ITACO, with the

widely used conventional methods, SGNS and
GloVe. We used the word2vec implementa-
tion6 to obtain word embeddings of SGNS, and
glove implementation7 for GloVe. Many tunable
hyper-parameters were selected based on the rec-
ommended default values of each implementation,
or suggestion explained in (Levy et al., 2015). For
ITACO, we selected the Glove objective to solve
Eqs. 6 and 7 since it requires a lower calculation cost
than the SGNS objective.

We prepared three types of linguistic benchmark
tasks, namely word similarity estimation (Similar-
ity), word analogy estimation (Analogy), and sen-
tence completion (SentComp) tasks. We gathered

5https://bitbucket.org/omerlevy/hyperwords
6https://code.google.com/p/word2vec/ (We made a modifi-

cation to save the context vector as well as the word vector.)
7http://nlp.stanford.edu/projects/glove/

1148



Analogy
Methods D=10 50 100 300 500 1000

ITACO (trunc) ∗2.6 38.0 51.5 63.5 65.4 ∗65.6
SGNS (trunc) 0.0 11.8 34.6 57.4 61.9 63.2
GloVe (trunc) 0.1 20.1 42.3 60.7 63.2 ∗65.6
SGNS (retrain) 1.7 37.7 51.8 62.3 64.0 –
GloVe (retrain) ∗2.6 ∗42.6 ∗57.0 ∗65.6 ∗66.4 –

Similarity
Methods D=10 50 100 300 500 1000

ITACO (trunc) 41.6 55.2 58.5 61.4 62.2 62.9
SGNS (trunc) 29.0 46.1 52.5 60.7 61.8 ∗64.5
GloVe (trunc) 29.3 46.2 50.6 56.8 58.5 59.9
SGNS (retrain) ∗46.4 ∗58.2 ∗61.3 ∗63.9 ∗64.2 –
GloVe (retrain) 38.7 51.4 54.2 56.8 58.1 –

SentComp
Methods D=10 50 100 300 500 1000

ITACO (trunc) ∗29.2 ∗32.3 32.1 ∗34.3 ∗35.4 ∗36.6
SGNS (trunc) 24.2 26.2 29.9 32.1 32.3 36.1
GloVe (trunc) 22.8 24.3 26.7 26.7 28.2 27.7
SGNS (retrain) 24.8 29.7 ∗33.0 32.7 33.9 –
GloVe (retrain) 26.3 27.3 27.1 28.1 28.1 –

Table 1: Results of right-truncated embedding vectors (trunc),
and standard embedding vectors (retrain). ‘∗’ represents the

best results in the corresponding column.

nine datasets for Similarity (Rubenstein and Goode-
nough, 1965; Miller and Charles, 1991; Agirre et al.,
2009; Agirre et al., 2009; Bruni et al., 2014; Radin-
sky et al., 2011; Huang et al., 2012; Luong et al.,
2013; Hill et al., 2014), three for Analogy (Mikolov
et al., 2013a; Mikolov et al., 2013c) , and one for
SentComp (Mikolov et al., 2013a).

Table 1 shows all the results of our experiments8.
The rows labeled ‘(trunc)’ show the performance of
D-right-truncated embedding vectors, whose origi-
nal vector of dimension is D = 1000. Thus, they
were obtained from a single set of embedding vec-
tors with D= 1000 for each corresponding method.
Next, the rows labeled ‘(retrain)’ show the perfor-
mance provided by SGNS or GloVe that were in-
dependently constructed with using a standard set-
ting and corresponding D. Note that the results of
‘ITACO (retrain)’ are identical to those of ‘ITACO
(trunc)’. Moreover, ‘GloVe (trunc)’ and ‘GloVe (re-
train)’ in D = 1000 are equivalent, as are ‘SGNS
(trunc)’ and ‘SGNS (retrain)’. Thus, these results

8Results for SGNS and GloVe are the average performance
of ten runs as suggested in (Suzuki and Nagata, 2015)

were omitted from the table.
First, comparing ‘(retrain)’ and ‘(trunc)’ in SGNS

and GloVe, our experimental results first explicitly
revealed that SGNS and GloVe with the simple trun-
cation approach ‘(trunc)’ cannot provide effective
lower-dimensional embedding vectors. This obser-
vation strongly supports the significance of exis-
tence of our proposed method, ITACO.

Second, in most cases ITACO successfully pro-
vided almost the same performance level as the best
SGNS and GloVe (retrain) results. We emphasize
that ITACO constructed embedding vectors ‘just
once’, while SGNS and GloVe required us to retrain
embedding vectors in the corresponding times. In
addition, single run of ITACO for D = 1000 took
approximately 12,000 seconds in our machine envi-
ronment, which was almost equivalent to run 4 itera-
tions of SGNS and 8 iterations of GloVe. The results
of SGNS and GloVe in Table 1 were obtained by
10 iterations and 20 iterations, respectively, which
are one of the standard settings to run SGNS and
GloVe9. This fact verified that ITACO can run effi-
ciently as in the same level as SGNS and GloVe.

5 Conclusion

This paper proposed a method for generating in-
teresting right-truncatable word embedding vectors.
Our experiments revealed that the embedding vec-
tors obtained with our method, ITACO, in any lower
dimensions work as well as those obtained by SGNS
and Glove. In addition, ITACO can also be a good
alternative of SGNS and GloVe in terms of the exe-
cution speed of a single run. Now, we are free from
retraining different dimensions of embedding vec-
tors by using ITACO. Our method significantly re-
duces the total calculation cost and storage, which
improves the ‘usability’ of embedding vectors10.

Acknowledgment

We thank three anonymous reviewers for their help-
ful comments.

9The performance of 4 iterations of SGNS and 8 iterations
of GloVe were much lower than those of 10 iterations and 20
iterations of SGNS and GloVe shown in Table 1, respectively.

10The right-truncatable embedding vectors used in our exper-
iments will be available in author’s homepage

1149



References

Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Paşca, and Aitor Soroa. 2009. A
Study on Similarity and Relatedness Using Distribu-
tional and WordNet-based Approaches. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, NAACL
’09, pages 19–27, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Chris Alberti, David Weiss, Greg Coppola, and Slav
Petrov. 2015. Improved Transition-Based Parsing
and Tagging with Neural Networks. In Proceedings of
the 2015 Conference on Empirical Methods in Natural
Language Processing, pages 1354–1359, Lisbon, Por-
tugal, September. Association for Computational Lin-
guistics.

Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal Distributional Semantics. J. Artif. Int.
Res., 49(1):1–47, January.

Danqi Chen and Christopher Manning. 2014. A Fast and
Accurate Dependency Parser using Neural Networks.
In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 740–750, Doha, Qatar, October. Association for
Computational Linguistics.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
Based Dependency Parsing with Stack Long Short-
Term Memory. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Linguis-
tics and the 7th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 334–343, Beijing, China, July. Association for
Computational Linguistics.

Yoav Goldberg and Omer Levy. 2014. word2vec Ex-
plained: Deriving Mikolov et al.’s Negative-sampling
Word-embedding Method. CoRR, abs/1402.3722.

Jochen Gorski, Frank Pfeuffer, and Kathrin Klamroth.
2007. Biconvex Sets and Optimization with Biconvex
Functions: a Survey and Extensions. Math. Meth. of
OR, 66(3):373–407.

Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
SimLex-999: Evaluating Semantic Models with (Gen-
uine) Similarity Estimation. ArXiv e-prints, August.

Eric H. Huang, Richard Socher, Christopher D. Manning,
and Andrew Y. Ng. 2012. Improving Word Represen-
tations via Global Context and Multiple Word Proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers - Volume 1, pages 873–882. Association for
Computational Linguistics.

Jingu Kim, Yunlong He, and Haesun Park. 2014. Al-
gorithms for Nonnegative Matrix and Tensor Factor-
izations: a Unified View Based on Block Coordinate
Descent Framework. Journal of Global Optimization,
58(2):285–319.

Jonghoon Kim, Francois Rousseau, and Michalis Vazir-
giannis. 2015. Convolutional Sentence Kernel from
Word Embeddings for Short Text Categorization. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages 775–
780, Lisbon, Portugal, September. Association for
Computational Linguistics.

Omer Levy and Yoav Goldberg. 2014. Neural
Word Embedding as Implicit Matrix Factorization.
In Z. Ghahramani, M. Welling, C. Cortes, N.D.
Lawrence, and K.Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, pages
2177–2185. Curran Associates, Inc.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015.
Improving Distributional Similarity with Lessons
Learned from Word Embeddings. Transactions of the
Association for Computational Linguistics, 3.

Wang Ling, Chris Dyer, Alan W Black, Isabel Trancoso,
Ramon Fermandez, Silvio Amir, Luis Marujo, and
Tiago Luis. 2015. Finding Function in Form: Compo-
sitional Character Models for Open Vocabulary Word
Representation. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1520–1530, Lisbon, Portugal, Septem-
ber. Association for Computational Linguistics.

Thang Luong, Richard Socher, and Christopher Manning.
2013. Better Word Representations with Recursive
Neural Networks for Morphology. In Proceedings of
the Seventeenth Conference on Computational Natural
Language Learning, pages 104–113, Sofia, Bulgaria,
August. Association for Computational Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed Representa-
tions of Words and Phrases and their Compositionality.
In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahra-
mani, and K.Q. Weinberger, editors, Advances in Neu-
ral Information Processing Systems 26, pages 3111–
3119. Curran Associates, Inc.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic Regularities in Continuous Space
Word Representations. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 746–751, Atlanta, Georgia,
June. Association for Computational Linguistics.

1150



George A. Miller and Walter G. Charles. 1991. Contex-
tual Correlates of Semantic Similarity. Language &
Cognitive Processes, 6(1):1–28.

Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
Word Embeddings Efficiently With Noise-contrastive
Estimation. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems 26,
pages 2265–2273. Curran Associates, Inc.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global Vectors for Word
Represqentation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha, Qatar,
October. Association for Computational Linguistics.

Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich,
and Shaul Markovitch. 2011. A Word at a Time:
Computing Word Relatedness Using Temporal Se-
mantic Analysis. In Proceedings of the 20th Inter-
national Conference on World Wide Web, WWW ’11,
pages 337–346, New York, NY, USA. ACM.

Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Commun. ACM,
8(10):627–633, October.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Se-
quence to Sequence Learning with Neural Networks.
In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, pages
3104–3112. Curran Associates, Inc.

Jun Suzuki and Masaaki Nagata. 2015. A Unified Learn-
ing Framework of Skip-Grams and Global Vectors. In
Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics and the 7th
International Joint Conference on Natural Language
Processing (Volume 2: Short Papers), pages 186–191,
Beijing, China, July. Association for Computational
Linguistics.

Yuta Tsuboi. 2014. Neural Networks Leverage Corpus-
wide Information for Part-of-speech Tagging. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
938–950, Doha, Qatar, October. Association for Com-
putational Linguistics.

Di Wang and Eric Nyberg. 2015. A Long Short-
Term Memory Model for Answer Sentence Selection
in Question Answering. In Proceedings of the 53rd
Annual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 2: Short Pa-
pers), pages 707–712, Beijing, China, July. Associa-
tion for Computational Linguistics.

Kristian Woodsend and Mirella Lapata. 2015. Dis-
tributed Representations for Unsupervised Semantic

Role Labeling. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Process-
ing, pages 2482–2491, Lisbon, Portugal, September.
Association for Computational Linguistics.

Jie Zhou and Wei Xu. 2015. End-to-end Learning of
Semantic Role Labeling using Recurrent Neural Net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1127–1137, Beijing, China, July. Association
for Computational Linguistics.

Geoffrey Zweig and Christopher J.C. Burges. 2011. The
microsoft research sentence completion challenge.
Technical Report MSR-TR-2011-129, Microsoft Re-
search, December.

1151


