



















































A Regularization-based Framework for Bilingual Grammar Induction


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1423–1428,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1423

A Regularization-based Framework for Bilingual Grammar Induction∗

Yong Jiang�, Wenjuan Han†, Kewei Tu†
†School of Information Science and Technology, ShanghaiTech University, Shanghai, China

�Alibaba Group
yongjiang.jy@alibaba-inc.com

{hanwj,tukw}@shanghaitech.edu.cn

Abstract
Grammar induction aims to discover syntac-
tic structures from unannotated sentences. In
this paper, we propose a framework in which
the learning process of the grammar model of
one language is influenced by knowledge from
the model of another language. Unlike previ-
ous work on multilingual grammar induction,
our approach does not rely on any external re-
sources, such as parallel corpora, word align-
ments or linguistic phylogenetic trees. We pro-
pose three regularization methods that encour-
age similarity between model parameters, de-
pendency edge scores, and parse trees respec-
tively. We deploy our methods on a state-of-
the-art unsupervised discriminative parser and
evaluate it on both transfer grammar induction
and bilingual grammar induction. Empirical
results on multiple languages show that our
methods outperform strong baselines.

1 Introduction

Syntactic parsing is an important task in natural
language processing. Supervised parsing requires
manual labeling of gold parse trees, which is a
very labor-intensive task. On the other hand, unsu-
pervised parsing (a.k.a. grammar induction) does
not require labeled data and can make use of large
amounts of unlabeled data that are freely available.
However, grammar induction is very challenging
and its accuracy is still far below that of supervised
parsing. To compensate the lack of supervision in
grammar induction, some previous work consid-
ers multilingual grammar induction, i.e., simulta-
neously learning grammars of multiple languages
(Snyder et al., 2009; Berg-Kirkpatrick and Klein,
2010; Liu et al., 2013). Existing multilingual ap-
proaches require external resources such as paral-
lel corpora, word alignments, and linguistic phy-
logenetic trees.

∗ Yong Jiang contributed to this work when at Shang-
haiTech University. Kewei Tu is the corresponding author.

Language German English Spanish French Indonesian
Code DE EN ES FR ID
C-MST 60.2 62.3 68.8 72.3 69.7
D-Tran 59.9 – 65.3 67.8 45.7
∆ -0.3 – -3.5 -4.5 -24.0
Language Italian Japanese Korean Portuguese Swedish
Code IT JA KO PTBR SV
C-MST 64.3 57.5 59.0 68.3 66.2
D-Tran 63.1 54.6 50.0 66.2 67.8
∆ -1.2 -2.9 -9.0 -2.1 +1.6

Table 1: Directed dependency accuracy (DDA) on the
universal treebanks with universal POS tags, on sen-
tences of length ≤ 10. C-MST denotes the original
Convex-MST model. D-Tran denotes direct transfer.
∆ refers to the difference between C-MST and D-Tran.

In this paper, we aim at bilingual grammar in-
duction without external resource. We are moti-
vated by our observation that learning the unsu-
pervised Convex-MST model (Grave and Elhadad,
2015) on the English corpus and then directly ap-
plying it to parse other languages produces sur-
prisingly good results (Table 1). From the table,
we can see that even with this simplistic method
(which we call direct transfer), the dependency
accuracy on each language is often very close to
the accuracy of the model specifically trained on
the corpus of that language. For the Swedish lan-
guage, the accuracy of direct transfer is even bet-
ter than that of the specifically trained model. This
surprising result suggests that grammars of differ-
ent languages, even those from different language
families (e.g., English and Japanese), may have
non-trivial similarity that can be helpful in bilin-
gual grammar induction.

Inspired by this observation, we propose a
regularization-based framework to bilingual gram-
mar induction that encourage knowledge sharing
between models learned on a language pair. We
build our framework on top of Convex-MST, a
state-of-the-art unsupervised dependency parser,



1424

and propose three regularization terms that en-
courage similarity between model parameters,
edge scores, and parse trees respectively. We test
our methods on ten languages on the tasks of trans-
fer grammar induction and bilingual grammar in-
duction and show that our methods can achieve a
significant boost over strong baselines.

2 Background

2.1 Unsupervised Dependency Parsing

Dependency parsing is the task of mapping an in-
put sentence x = x1, x2, ..., xn of length n to an
output dependency structure y. A dummy root x0
is typically added at the beginning of the sentence
to denote the head of the dependency tree. There
are several approaches to represent the parse tree
y. In transition based dependency parsers, the de-
pendency tree can be regarded as a sequence of
actions. In graph based dependency parsers, the
dependency tree can be represented as a spanning
tree in the graph. In chart based parsers (a.k.a.,
grammar based parsers), the dependency tree is
denoted as a set of grammar rules. In unsuper-
vised graph based dependency parsers, since gold
trees are not available, carefully designed models
and objective functions are required for learning
a dependency parser. Regardless of model archi-
tectures, current unsupervised dependency models
usually use the following form of objective func-
tion,

J(w;X ) =
∑
x∈X

Oy∈Y(x)
(
D(w,x,y) +R(w)

)
where Y is the set of all possible dependency tree,
w is the model parameter, X is the unlabeled
training corpus, D is the measurement between
the parse y and model prediction on sentence x,
R(w) is the regularization term of parameter w,
O ∈ {min,∑} is an operator. Table 2 shows
the choices of O, D and R for several widely used
models.

2.2 Graph based Dependency Parsing

In this paper, we focus on graph based dependency
parsers, though we believe that our approaches
can be generalized to other types of parsers. Pre-
vious work on unsupervised graph based depen-
dency parsing utilizes the autoencoder structure
(Cai et al., 2017) or the discriminative clustering
techniques (Grave and Elhadad, 2015).

Following (McDonald et al., 2005), we can use
a discriminative model for dependency parsing
with first order factorization such that the score of
a dependency tree y is the sum of the scores of
its dependency edges. The score of an edge from
word h to wordm , sw(x, h,m), can be computed
as the inner product of a feature vector f(x, h,m)
and a parameter vector w. The optimal depen-
dency tree for sentence x be discovered in polyno-
mial time (Eisner, 1996; McDonald et al., 2005).

3 Bilingual Knowledge Sharing

Given non-parallel corpora of two languages Xs
and Xt, our goal is to learn two models with pa-
rameters ws and wt for the two languages. The
simplest learning objective function is,

J(ws,wt;Xs,Xt) = J(ws;Xs) + J(wt;Xt)

which contains no interaction between the two
models.

As suggested by our empirical observation in
Table 1, the model of one language may provide
a useful inductive bias in learning the model of
another language. Note that given a sentence, a
graph-based dependency parser has three levels of
representations: the model parameters, the scores
of dependency edges computed from the param-
eters, and the parse tree computed from the edge
scores. Therefore, we propose three different reg-
ularization terms to effectively encourage similar-
ity of the two models. An example is shown in
Figure 1.

Regularization of Weight Parameters (W-Reg)
Motivated by the approach of Berg-Kirkpatrick
and Klein (2010), we encourage the similarity be-
tween the two weight parameters ws and wt mea-
sured by l2 norm distance:

J(ws,wt;Xs,Xt) =
J(ws;Xs) + J(wt;Xt) + λ||ws −wt||22

in which λ is a hyper-parameter.

Regularization on Edge Scores (E-Reg) Di-
rectly encouraging weight similarity might result
in an inductive bias that is too strong, because the
difference between the two languages (e.g., differ-
ent word orders) may lead to different meanings of
each feature dimension. Therefore, we propose to
encourage similarity between the scores computed
by the two models for each dependency edge of



1425

Parsers O D R
DMV (Klein and Manning, 2004)

∑
negative log likelihood -

Convex-MST (Grave and Elhadad, 2015) min `2 distance `2 norm
LC-DMV (Noji et al., 2016)

∑
negative log likelihood `2 norm

NDMV (Jiang et al., 2016)
∑
,min negative log likelihood -

CRFAE (Cai et al., 2017) min negative conditional log likelihood `1 norm
D-NDMV (Han et al., 2019)

∑
,min negative (conditional) log likelihood -

Table 2: Notations of several widely used models.

each sentence, which can be seen as a soft version
of weight regularization.

J(ws,wt;Xs,Xt) = J(ws;Xs) + J(wt;Xt)+
λ
∑
x∈X ′

∑
(h,m)∈G(x)

||sws(x, h,m)− swt(x, h,m)||22

where X ′ = Xs∪Xt. G(x) is the weighted depen-
dency graph of sentence x.

Regularization on Parse Trees (T-Reg) An-
other alternative is to encourage similarity be-
tween the parse trees predicted by the two mod-
els. Motivated by the idea of knowledge distilla-
tion (Kim and Rush, 2016), in the learning objec-
tive of each model, we add a fourth term to en-
courage the parse tree to be close to the prediction
of the other model. Below we show the objective
function for ws.

J ′(ws,wt;Xs) =
∑
x∈Xs

Oy∈Y(x)
(
D(ws,x,y)

+ λD(wt,x,y)︸ ︷︷ ︸
T-Reg term

+R(ws)
)

J ′(wt,ws;Xt) =
∑
x∈Xt

Oy∈Y(x)
(
D(wt,x,y)

+ λD(ws,x,y)︸ ︷︷ ︸
T-Reg term

+R(wt)
)

J(ws,wt;Xs,Xt) =
J ′(ws,wt;Xs) + J ′(wt,ws;Xt)

We apply these regularization method to the
Convex-MST model (Grave and Elhadad, 2015).
Our three objective functions can be optimized
with coordinate descent in a similar way to
Convex-MST. In each iteration, we first fix parse

I eat sushi with Mary

y

s(eat ! with) = wT f(eat ! with)

Figure 1: Three levels of representations of the parser:
the parameter w, the edge score s, and the parse y.

y for each training sentence and update parameters
ws and wt by stochastic gradient descent; then we
fix ws and wt and update y of each sentence by
the Frank-Wolfe algorithm.

While our three methods are applicable to any
pair of languages, intuitively one may use weight
regularization only for similar languages, and use
edge regularization and tree regularization for an
arbitrary language pair.

4 Experiments

To enable direct comparison with the Convex-
MST model, we use the dataset used in their pa-
per (Grave and Elhadad, 2015), the universal tree-
banks version 2.01, introduced by McDonald et al.
(2013). The dataset contains ten different lan-
guages, which belong to five diverse families. In
additional, we test our methods on twelve lan-
guages from the more recent UD Treebank 1.42,
which is also used in previous grammar induction
work (Jiang et al., 2017; Li et al., 2019). Follow-
ing previous work, we train all the models on the
gold POS tags of sentences no longer than ten. We
tune hyper-parameters on the development dataset
and report the DDA on sentences no longer than
ten and all the sentences in the test dataset. As our
goal is to investigate the benefits of our regulariza-
tion methods, the two hyper-parameters µ and β

1https://github.com/ryanmcd/
uni-dep-tb. The version is not consistent with re-
cent releases of UD Treebanks.

2http://universaldependencies.org/

https://github.com/ryanmcd/uni-dep-tb
https://github.com/ryanmcd/uni-dep-tb
http://universaldependencies.org/


1426

CODE C-MST D-TRAN W-REG E-REG T-REG
DE 60.2 -0.3 -0.2 +0.2 -0.2
ES 68.8 -3.5 -3.5 +0.8 +0.3
FR 72.3 -4.5 -3.8 +0.3 +0.3
ID 69.7 -24.0 -21.4 -0.6 -1.2
IT 64.3 -1.2 -0.4 +0.3 +1.2
JA 57.5 -2.9 -3.4 +0.9 +2.3
KO 59.0 -9.0 -9.5 +1.3 +1.9

PTBR 68.3 -2.1 -2.1 +0.2 +0.3
SV 66.2 +1.6 +1.6 +2.7 +2.6
Avg 65.14 -5.10 -4.74 +0.68 +0.83

Avg-All 56.16 -4.63 -4.29 +0.47 +0.56

UD 1.4∗ 52.83 -0.30 -1.92 +1.06 +1.37

Table 3: Transfer grammar induction from English to
the other languages. We show accuracies on the test
sentences of length ≤ 10 (except the Avg-All row
which shows the average accuracies on all the sen-
tences). *: for the UD 1.4 dataset we show the average
results.

of Convex-MST are tuned on the English develop-
ment dataset and then fixed (µ = 0.1, β = 0.001)
while λ is selected from {10, 5, 1, 5e − 1, 1e −
1, 5e− 2, 1e− 2, 5e− 3, 1e− 3, 1e− 4} for each
language pair.

4.1 Experiments on Transfer Grammar
Induction

In transfer grammar induction, we train the first
model on the first language independent of the sec-
ond language; then, with the first model fixed, we
optimize our knowledge sharing objective with re-
spect to the second model; finally, we evaluate the
second model on the test set of the second lan-
guage. In this way, we want to test whether our
methods can transfer useful linguistic knowledge
from the first language to the second language. We
report the results of transfer grammar induction
from English to the other nine languages in Table
3. Our edge regularization and tree regularization
methods outperform the Convex-MST baseline in
almost all the cases. The weight regularization
method achieves worse results than Convex-MST
except for the Swedish language, which demon-
strates that directly regularizing weight parameters
may not work well in the transfer grammar induc-
tion task. For the Swedish language, although di-
rect transfer already achieves better performance
than Convex-MST, our regularization methods can
further boost the performance by a large mar-
gin. The Indonesian language is the only language
for which transfer grammar induction provides no
benefit, possibly because of its significant syntac-
tic difference from the English language. Our ad-
ditional experimental results on UD treebank 1.4

PAIR CODE BASE COMB W-REG E-REG T-REG

EN-DE EN 62.3 +0 +0.5 +0.1 +0.2DE 60.2 +0.1 -0.8 +0.5 +0.4

EN-ES EN 62.3 +0.2 +0.7 +0.4 +0.7ES 68.8 -1.8 +1.4 +1.1 +0.8

EN-FR EN 62.3 +0.4 +0.5 +0.1 +0.1FR 72.3 -3.6 -0.6 +1.4 +1.4

EN-ID EN 62.3 +0.7 +1.3 +0.1 +0.7ID 69.7 -19.2 -2.1 +0.5 +0.9

EN-IT EN 62.3 +0.7 +0.1 +0.6 +1.0IT 64.3 +0.7 +1.8 +0.4 +0.8

EN-JA EN 62.3 +1.0 +0.9 -0.1 -0.2JA 57.5 -4.3 -0.2 +2.7 +2.4

EN-KO EN 62.3 +0.3 +1.4 +0.5 +1.1KO 59.0 -3.8 +0.1 +1.0 +0.4

EN-PTBR EN 62.3 +1.0 +0.8 +0.6 +0.7PT-BR 68.3 -0.9 +1.4 +1.4 +1.2

EN-SV EN 62.3 -0.5 -0.9 +0.1 +0.1SV 66.2 +1.7 +1.9 +1.0 +1.1

Avg EN 62.30 +0.42 +0.58 +0.23 +0.49Other 65.14 -3.46 +0.32 +1.11 +1.04

Avg-All EN 52.10 -0.48 -0.40 +0.11 +0.42Other 56.16 -3.05 +0.64 +1.55 +1.21

UD 1.4∗ EN 53.50 -0.76 -0.09 + 0 +0.71Other 52.83 -0.09 +0.58 +1.92 +1.52

Table 4: Results of bilingual grammar induction on
test sentences no longer than 10 (except the Avg-All
row which shows the average accuracies on all the sen-
tences). BASE refers to the individually trained base-
line. COMB refers to learning a single model from the
combined training set of the two languages. *: for the
UD 1.4 dataset we show the average results.

show a similar trend.
We perform transfer grammar induction from

English to Swedish with different values of λ and
show the results in Figure 2. We can see that the
impact of different hyper-parameter values on the
accuracy generally follows the same tendency for
our three methods.

4.2 Experiments on Bilingual Grammar
Induction

In bilingual grammar induction, we jointly train
two models on two languages. In our experiments,
we pair English with each of the other nine lan-
guages. The results are reported in Table 4. It
can be seen that in most cases joint training leads
to better accuracies than the individually trained
models as well as the single model learned from
the combined training set. By comparing table 4
with table 3, we can also see that bilingual joint
training leads to better accuracies than transfer
grammar induction, which shows the benefit of
training two models simultaneously rather than se-
quentially. Again, our additional experimental re-
sults on UD treebank 1.4 show a similar trend.



1427

0.001 0.005 0.01 0.05 0.1 0.5 1.0 5.0 10.0
choice of lambda

0.62

0.63

0.64

0.65

0.66

0.67

0.68

0.69
de

pe
nd

en
cy

 a
cc

ur
ac

y

W-Reg on SV
E-Reg on SV
T-Reg on SV

Figure 2: Transfer grammar induction on the SV lan-
guage with different hyper-parameter values for the
three regularization methods

5 Related Work

Our work is related to many previous work.

Unsupervised Transfer Learning There has
been previous work aiming at solving an unsu-
pervised learning task of a target domain with the
help of knowledge learned from a source domain
(Dai et al., 2008; Wang et al., 2008; Pan and Yang,
2010). There is no labeled data in both the source
and the target domains during training. Our trans-
fer grammar induction setting can be seen as an
instance of unsupervised transfer learning.

Cross-lingual Supervised Dependency Parsing
This task focuses on learning a parser with unla-
beled training data and additional labeled training
data of a second language (McDonald et al., 2011;
Naseem et al., 2012; Guo et al., 2015). The main
difference between our approach and theirs is that
our approach is fully unsupervised. and do not uti-
lize external information like word alignments or
cross-lingual word embeddings.

Other Approaches to Multilingual Grammar
Induction To the best of our knowledge, this
task is first proposed by Kuhn (2004). They as-
sume that the syntax trees induced from paral-
lel sentences share structured regularities and uti-
lize the word alignments to guide parsing. From
then on, many approaches are proposed on both
constituency grammar induction and dependency
grammar induction (Snyder et al., 2009; Berg-
Kirkpatrick and Klein, 2010). We differ from
these approaches in that we do not make use of
any external rules or knowledge.

6 Conclusion

In this paper, we propose three regularization-
based knowledge sharing methods to bilingual
grammar induction problems. We test our meth-
ods on transfer grammar induction and bilingual
grammar induction and show that our methods
achieve better performance than the baselines. In
future work, we plan to investigate the effective-
ness of our approach in other types of induction
tasks.

Acknowledgments

This work was supported by the Major Program
of Science and Technology Commission Shanghai
Municipal (17JC1404102).

References
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylo-

genetic grammar induction. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1288–1297, Uppsala,
Sweden. Association for Computational Linguistics.

Jiong Cai, Yong Jiang, and Kewei Tu. 2017. Crf
autoencoder for unsupervised dependency parsing.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1638–1643, Copenhagen, Denmark. Association for
Computational Linguistics.

Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong
Yu. 2008. Self-taught clustering. In Proceedings of
the 25th international conference on Machine learn-
ing, pages 200–207. ACM.

Jason M Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Pro-
ceedings of the 16th conference on Computational
linguistics-Volume 1, pages 340–345. Association
for Computational Linguistics.

Edouard Grave and Noémie Elhadad. 2015. A con-
vex and feature-rich discriminative approach to de-
pendency grammar induction. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1375–1384, Beijing,
China. Association for Computational Linguistics.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2015. Cross-lingual depen-
dency parsing based on distributed representations.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 1234–1244.

http://www.aclweb.org/anthology/P10-1131
http://www.aclweb.org/anthology/P10-1131
https://www.aclweb.org/anthology/D17-1171
https://www.aclweb.org/anthology/D17-1171
http://www.aclweb.org/anthology/P15-1133
http://www.aclweb.org/anthology/P15-1133
http://www.aclweb.org/anthology/P15-1133


1428

Wenjuan Han, Yong Jiang, and Kewei Tu. 2019. En-
hancing unsupervised generative dependency parser
with contextual information. In Proceedings of the
57th Annual Meeting of the Association for Com-
putational Linguistics, pages 5315–5325, Florence,
Italy. Association for Computational Linguistics.

Yong Jiang, Wenjuan Han, and Kewei Tu. 2016. Un-
supervised neural dependency parsing. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 763–771,
Austin, Texas. Association for Computational Lin-
guistics.

Yong Jiang, Wenjuan Han, and Kewei Tu. 2017. Com-
bining generative and discriminative approaches to
unsupervised dependency parsing via dual decom-
position. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, Copenhagen, Denmark. Association for Com-
putational Linguistics.

Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1317–1327, Austin,
Texas. Association for Computational Linguistics.

Dan Klein and Christopher D Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 478. Association for Com-
putational Linguistics.

Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, page 470. Association for Computational
Linguistics.

Bowen Li, Jianpeng Cheng, Yang Liu, and Frank
Keller. 2019. Dependency grammar induction with
a neural variational transition-based parser. In AAAI
2019.

Kai Liu, Yajuan Lü, Wenbin Jiang, and Qun Liu. 2013.
Bilingually-guided monolingual dependency gram-
mar induction. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1063–
1072, Sofia, Bulgaria. Association for Computa-
tional Linguistics.

Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd an-
nual meeting on association for computational lin-
guistics, pages 91–98. Association for Computa-
tional Linguistics.

Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Os-
car Täckström, et al. 2013. Universal dependency

annotation for multilingual parsing. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), volume 2, pages 92–97.

Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the conference on empir-
ical methods in natural language processing, pages
62–72. Association for Computational Linguistics.

Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 629–637. Asso-
ciation for Computational Linguistics.

Hiroshi Noji, Yusuke Miyao, and Mark Johnson. 2016.
Using left-corner parsing to encode universal struc-
tural constraints in grammar induction. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 33–43,
Austin, Texas. Association for Computational Lin-
guistics.

Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345–1359.

Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 73–81, Suntec, Sin-
gapore. Association for Computational Linguistics.

Zheng Wang, Yangqiu Song, and Changshui Zhang.
2008. Transferred dimensionality reduction. In
Joint European Conference on Machine Learning
and Knowledge Discovery in Databases, pages 550–
565. Springer.

https://www.aclweb.org/anthology/P19-1526
https://www.aclweb.org/anthology/P19-1526
https://www.aclweb.org/anthology/P19-1526
https://aclweb.org/anthology/D16-1073
https://aclweb.org/anthology/D16-1073
https://aclweb.org/anthology/D16-1139
https://aclweb.org/anthology/D16-1139
http://www.aclweb.org/anthology/P13-1105
http://www.aclweb.org/anthology/P13-1105
https://aclweb.org/anthology/D16-1004
https://aclweb.org/anthology/D16-1004
http://www.aclweb.org/anthology/P/P09/P09-1009
http://www.aclweb.org/anthology/P/P09/P09-1009

