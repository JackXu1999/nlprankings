



















































Ternary Segmentation for Improving Search in Top-down Induction of Segmental ITGs


Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 86–93,
October 25, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Ternary Segmentation for Improving Search
in Top-down Induction of Segmental ITGs

Markus Saers DekaiWu
HKUST

Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology

{masaers|dekai}@cs.ust.hk

Abstract

We show that there are situations where
iteratively segmenting sentence pairs top-
down will fail to reach valid segments and
propose a method for alleviating the prob-
lem. Due to the enormity of the search
space, error analysis has indicated that it is
often impossible to get to a desired embed-
ded segment purely through binary seg-
mentation that divides existing segmental
rules in half – the strategy typically em-
ployed by existing search strategies – as
it requires two steps. We propose a new
method to hypothesize ternary segmenta-
tions in a single step, making the embed-
ded segments immediately discoverable.

1 Introduction

One of the most important improvements to sta-
tistical machine translation to date was the move
from token-basedmodel to segmental models (also
called phrasal). This move accomplishes two
things: it allows a flat surface-based model to
memorize some relationships between word real-
izations, but more importantly, it allows the model
to capture multi-word concepts or chunks. These
chunks are necessary in order to translate fixed ex-
pressions, or other multi-word units that do not
have a compositional meaning. If a sequence in
one language can be broken down into smaller
pieces which are then translated individually and
reassembled in another language, the meaning of
the sequence is compositional; if not, the only way
to translate it accurately is to treat it as a single unit
– a chunk. Existing surface-based models (Och et
al., 1999) have high recall in capturing the chunks,
but tend to over-generate, which leads to big mod-
els and low precision. Surface-based models have
no concept of hierarchical composition, instead
they make the assumption that a sentence consists

of a sequence of segments that can be individually
translated and reordered to form the translation.
This is counter-intuitive, as the who-did-what-to-
whoms of a sentence tends to be translated and re-
ordered as units, rather than have their components
mixed together. Transduction grammars (Aho and
Ullman, 1972; Wu, 1997), also called hierarchical
translation models (Chiang, 2007) or synchronous
grammars, address this through a mechanism sim-
ilar to context-free grammars. Inducing a segmen-
tal transduction grammar is hard, so the standard
practice is to use a similar method as the surface-
based models use to learn the chunks, which is
problematic, since that method mostly relies on
memorizing the relationships that the mechanics
of a compositional model is designed to general-
ize. A compositional translation model would be
able to translate lexical chunks, as well as gener-
alize different kinds of compositions; a segmen-
tal transduction grammar captures this by having
segmental lexical rules and different nonterminal
symbols for different categories of compositions.
In this paper, we focus on inducing the former:
segmental lexical rules in inversion transduction
grammars (ITGs).

One natural way would be to start with a token-
based grammar and chunk adjacent tokens to form
segments. The main problemwith chunking is that
the data becomes more and more likely as the seg-
ments get larger, with the degenerate end point of
all sentence pairs being memorized lexical items.
Zhang et al. (2008) combat this tendency by intro-
ducing a sparsity prior over the rule probabilities,
and variational Bayes to maximize the posterior
probability of the data subject to this symmetric
Dirichlet prior. To hypothesize possible chunks,
they examine the Viterbi biparse of the existing
model. Saers et al. (2012) use the entire parse for-
est to generate the hypotheses. They also boot-
strap the ITG from linear and finite-state transduc-
tion grammars (LTGs, Saers (2011), and FSTGs),

86



rather than initialize the lexical probabilities from
IBM models.
Another way to arrive at a segmental ITG is to

start with the degenerate chunking case: each sen-
tence pair as a lexical item, and segment the exist-
ing lexical rules into shorter rules. Since the start
point is the degenerate case when optimizing for
data likelihood, this approach requires a different
objective function to optimize against. Saers et al.
(2013c) proposes to use description length of the
model and the data given the model, which is sub-
sequently expressed in a Bayesian form with the
addition of a prior over the rule probabilities (Saers
andWu, 2013). The way they generate hypotheses
is restricted to segmenting an existing lexical item
into two parts, which is problematic, because em-
bedded lexical items are potentially overlooked.
There is also the option of implicitly defining

all possible grammars, and sample from that dis-
tribution. Blunsom et al. (2009) do exactly that;
they induce with collapsed Gibbs sampling which
keeps one derivation for each training sentence
that is altered and then resampled. The operations
to change the derivations are split, join, delete
and insert. The split-operator corresponds to bi-
nary segmentation, the join-operator corresponds
to chunking; the delete-operator removes an inter-
nal node, resulting in its parent having three chil-
dren, and the insert-operator allows a parent with
three children to be normalized to have only two.
The existence of ternary nodes in the derivation
means that the learned grammar contains ternary
rules. Note that it still takes three operations: two
split-operations and one delete-operation for their
model to do what we propose to do in a single
ternary segmentation. Also, although we allow for
single-step ternary segmentations, our grammar
does not contain ternary rules; instead the results of
a ternary segmentation is immediately normalized
to the 2-normal form. Although their model can
theoretically sample from the entire model space,
the split-operation alone is enough to do so; the
other operations were added to get the model to do
so in practice. Similarly, we propose ternary seg-
mentation to be able to reach areas of the model
space that we failed to reach with binary segmen-
tation.
To illustrate the problem with embedded lexi-

cal items, we will introduce a small example cor-
pus. Although Swedish and English are relatively
similar, with the structure of basic sentences being

identical, they already illustrate the common prob-
lem of rare embedded correspondences. Imagine
a really simple corpus of three sentence pairs with
identical structure:
he has a red book / han har en röd bok

she has a biology book / hon har en biologibok

it has begun / det har börjat

The main difference is that Swedish concate-
nates rather than juxtaposes compounds such as
biologibok instead of biology book. A bilingual
person looking at this corpus would produce bilin-
gual parse trees like those in Figure 1. Inducing
this relatively simple segmental ITG from the data
is, however, quite a challenge.
The example above illustrates a problem with

the chunking approach, as one of the most com-
mon chunks is has a/har en, whereas the linguis-
tically motivated chunk biology book/biologibok
occurs only once. There is very little in this data
that would lead the chunking approach towards the
desired ITG. It also illustrates a problem with the
binary segmentation approach, as all the bilingual
prefixes and suffixes, the biaffixes, are unique;
there is no way of discovering that all the above
sentences have the exact same verb.
In this paper, we propose a method to al-

low bilingual infixes to be hypothesized and used
to drive the minimization of description length,
which would be able to induce the desired ITG
from the above corpus.
The paper is structured so that we start by giv-

ing a definition of the grammar formalism we use:
ITGs (Section 2). We then describe the notion
of description length that we use (Section 3), and
how ternary segmentation differs from and com-
plements binary segmentation (Section 4). We
then present our induction algorithm (Section 5)
and give an example of a run through (Section 6).
Finally we offer some concluding remarks (Sec-
tion 7).

2 Inversion transduction grammars

Inversion transduction grammars, or ITGs (Wu,
1997), are an expressive yet efficient way to
model translation. Much like context-free gram-
mars (CFGs), they allow for sentences to be ex-
plained through composition of smaller units into
larger units, but where CFGs are restricted to gen-
erate monolingual sentences, ITGs generate sets
of sentence pairs – transductions – rather than
languages. Naturally, the components of differ-

87



hasshe a biology book

har en biologibokhon

hasit begun

har börjatdet

hashe a red

har en rödhan

book

bok

Figure 1: Possible inversion transduction trees over the example sentence pairs.

ent languages may have to be ordered differently,
which means that transduction grammars need to
handle these differences in order. Rather than al-
lowing arbitrary reordering and pay the price of ex-
ponential time complexity, ITGs allow only mono-
tonically straight or inverted order of the produc-
tions, which cuts the time complexity down to a
manageable polynomial.
Formally, an ITG is a tuple ⟨N,Σ,∆, R, S⟩,

where N is a finite nonempty set of nonterminal
symbols, Σ is a finite set of terminal symbols in
L0, ∆ is a finite set of terminal symbols in L1, R
is a finite nonempty set of inversion transduction
rules and S ∈ N is a designated start symbol. An
inversion transduction rule is restricted to take one
of the following forms:

S → [A] , A→ [ψ+] , A→ ⟨ψ+⟩
where S ∈ N is the start symbol, A ∈ N is a non-
terminal symbol, and ψ+ is a nonempty sequence
of nonterminals and biterminals. A biterminal is
a pair of symbol strings: Σ∗ ×∆∗, where at least
one of the strings have to be nonempty. The square
and angled brackets signal straight and inverted or-
der respectively. With straight order, both the L0
and the L1 productions are generated left-to-right,
but with inverted order, theL1 production is gener-
ated right-to-left. The brackets are frequently left
out when there is only one element on the right-
hand side, which means that S → [A] is shortened
to S → A.
Like CFGs, ITGs also have a 2-normal form,

analogous to the Chomsky normal form for CFGs,
where the rules are further restricted to only the
following four forms:

S → A, A→ [BC] , A→ ⟨BC⟩, A→ e/f

where S ∈ N is the start symbol, A,B,C ∈ N

are nonterminal symbols and e/f is a biterminal
string.

A bracketing ITG, or BITG, has only one non-
terminal symbol (other than the dedicated start
symbol), which means that the nonterminals carry
no information at all other than the fact that their
yields are discrete unit. Rather than make a proper
analysis of the sentence pair they only produce a
bracketing, hence the name.

A transduction grammar such as ITG can be
used in three modes: generation, transduction
and biparsing. Generation derives a bisentence, a
sentence pair, from the start symbol. Transduction
derives a sentence in one language from a sentence
in the other language and the start symbol. Bipars-
ing verifies that a given bisentence can be derived
from the start symbol. Biparsing is an integral part
of any learning that requires expected counts such
as expectation maximization, and transduction is
the actual translation process.

3 Description length

We follow the definition of description length from
Saers et al. (2013b,c,d,a); Saers and Wu (2013),
that is: the size of the model is determined by
counting the number of symbols needed to encode
the rules, and the size of the data given the model
is determined by biparsing the data with the model.
Formally, given a grammarΦ its description length
DL (Φ) is the sum of the length of the symbols
needed to serialize the rule set. For convenience
later on, the symbols are assumed to be uniformly
distributed with a length of−lg 1N bits each (where
N is the number of different symbols). The de-
scription length of the data D given the model is
defined as DL (D|Φ) = −lgP (D|Φ).

88



Figure 2: The four different kinds of binary seg-
mentation hypotheses.

Figure 3: The two different hypotheses that can
be made from an infix-to-infix link.

4 Segmenting lexical items

With a background in computer science it is tempt-
ing to draw the conclusion that any segmentation
can be made as a sequence of binary segmenta-
tions. This is true, but only relevant if the entire
search space can be exhaustively explored. When
inducing transduction grammars, the search space
is prohibitively large; in fact, we are typically af-
forded only an estimate of a single step forward
in the search process. In such circumstances, the
kinds of steps you can take start to matter greatly,
and adding ternary segmentation to the typically
used binary segmentation adds expressive power.
Figure 2 contains a schematic illustration of bi-

nary segmentation: To the left is a lexical item
where a good biaffix (anL0 prefix or suffix associ-
ated with anL1 prefix or suffix) has been found, as
illustrated with the solid connectors. To the right is
the segmentation that can be inferred. For binary
segmentation, there is no uncertainty in this step.
When adding ternary segmentation, there are

five more situations: one situation where an in-

Figure 4: The eight different hypotheses that can
bemade from the four different infix-to-affix links.

fix is linked to an infix, and four situations where
an infix is linked to an affix. Figure 3 shows the
infix-to-infix situation, where there is one addi-
tional piece of information to be decided: are the
surroundings linked straight or inverted? Figure 4
shows the situations where one infix is linked to an
affix. In these situations, there are twomore pieces
of information that needs to be inferred: (a) where
the sibling of the affix needs to be segmented, and
(b) how the two pieces of the sibling of the affix
link to the siblings of the infix. The infix-to-affix
situations require a second monolingual segmen-
tations decision to be made. As this is beyond the
scope of this paper, we will limit ourselves to the
infix-to-infix situation.

5 Finding segmentation hypotheses

Previous work on binary hypothesis generation
makes assumptions that do not hold with ternary
segmentation; this section explains why that is and
how we get around it. The basic problem with bi-
nary segmentation is that any bisegment hypothe-
sized to be good on its own has to be anchored to
either the beginning or the end of an existing biseg-
ment. An infix, by definition, does not.
While recording all affixes is possible, even for

non-toy corpora (Saers and Wu, 2013; Saers et al.,
2013b,c), recording all bilingual infixes is not, so
collecting them all is not an option (while there are

89



Algorithm 1 Pseudo code for segmenting an ITG.
Φ ▷ The ITG being induced.
Ψ ▷ The token-based ITG used to evaluate lexical rules.
hmax ▷ The maximum number of hypotheses to keep from a single lexical rule.
repeat
δ ← 0
H ′ ▷ Initial hypotheses
for all lexical rules A→ e/f do
p← parse(Ψ, e/f)
c ▷ Fractional counts of bispans
for all bispans s, t, u, v ∈ e/f do
c(s, t, u, v)← 0

H ′′ ← []
for all items Bs,t,u,v ∈ p do
c(s, t, u, v)← c(s, t, u, v) + α(Bs,t,u,v)β(Bs,t,u,v)/α(S0,T,0,V )
H ′′ ← [H ′′, ⟨s, t, u, v, c(s, t, u, v)⟩]

sort H ′′ on c(s, t, u, v)
for all ⟨s, t, u, v, c(s, t, u, v)⟩ ∈ H ′′[0..hmax] do
H ′(es..t/fu..v)← [H ′(es..t/fu..v), ⟨s, t, u, v, A→ e/f⟩]

H ▷ Evaluated hypotheses
for all bisegments es..t/fu..v ∈ keys(H ′) do

Φ′ ← Φ
R← []
for all bispan-rule pairs ⟨s, t, u, v, A→ e/f⟩ ∈ H ′(es..t/fu..v) do

Φ′ ← make_grammar_change(Φ′, e/f, s, t, u, v)
R← [R,A→ e/f ]

δ′ ← DL(Φ′)−DL(Φ) +DL(D|Φ′)−DL(D|Φ)
if δ′ < 0 then
H ← [H, ⟨es..t/fu..v, R, δ′⟩]

sort H on δ′
for all ⟨es..t/fu..v, R, δ′⟩ ∈ H do

Φ′ ← Φ
for all rules A→ e/f ∈ R ∩RΨ′ do

Φ′ ← make_grammar_change(Φ′, e/f, s, t, u, v)
δ′ ← DL(Φ′)−DL(Φ) +DL(D|Φ′)−DL(D|Φ)
if δ′ < 0 then

Φ← Φ′
δ ← δ + δ′

until δ ≤ 0
return Φ

only O
(
n2

)
possible biaffixes for a parallel sen-

tence of average length n, there are O
(
n4

)
possi-

ble bilingual infixes). A way to prioritize, within
the scope of a single bisegment, which infixes and
affixes to consider as hypotheses is crucial. In this
paper we use an approach similar to Saers et al.
(2013d), in which we use a token-based ITG to
evaluate the lexical rules in the ITG that is be-
ing induced. Using a transduction grammar has
the advantage of calculating fractional counts for

hypotheses, which allows both long and short hy-
potheses to compete on a level playing field.
In Algorithm 1, we start by parsing all the lex-

ical rules in the grammar Φ being learned using a
token-based ITG Ψ. For each rule, we only keep
the best hmax bispans. In the second part, all col-
lected bispans are evaluated as if they were the
only hypothesis being considered for changing Φ.
Any hypothesis with a positive effect is kept for
further processing. These hypotheses are sorted

90



and applied. Since the grammarmay have changed
since the effect of the hypothesis was estimated,
we have to check that the hypothesis would have
a positive effect on the updated grammar before
committing to it. All this is repeated as long as
there are improvements that can be made.
Themake_grammar_changemethod deletes the

old rule, and distributes its probability mass to
the rules replacing it. For ternary segmentation,
this will be three lexical rules, and two structural
rules (which happens to be identical in a bracket-
ing grammar, giving that one rule two shares of
the probability mass being distributed). For binary
segmentation it is two lexical rules and one struc-
tural rule.
Rather than calculating DL (D|Φ)− DL (D|Φ)

explicitly by biparsing the entire corpus, we es-
timate the change. For binary rules, we use the
same estimate as Saers and Wu (2013): multiply-
ing in the new rule probabilities and dividing out
the old. For ternary rules, we make the assump-
tion that the three new lexical rules are combined
using structural rules the way they would during
parsing, which means two binary structural rules
being applied. The infix-to-infix situation must be
generated either by two straight combinations or
by two inverted combinations, so for a bracketing
grammar it is always two applications of a single
structural rule. We thus multiply in the three new
lexical rules and the structural rule twice, and di-
vide out the old rule. In essence, both these meth-
ods are recreating the situations in which the parser
would have used the old rule, but now uses the new
rules.
Having exhausted all the hypotheses, we also

run expectation maximization to stabilize the pa-
rameters. This step is not shown in the pseudo
code.
Examining the pseudocode closer reveals that

the outer loop will continue as long as the grammar
changes; since the only way the grammar changes
is by making lexical rules shorter, this loop is guar-
anteed to terminate. Inside the outer loop there are
three inner loops: one over the rule set, one over
the set of initial hypothesesH ′ and one over the set
of evaluated hypothesesH . The sets of hypotheses
are related such that |H| ≤ |H ′|, which means that
the size of the initial set of hypotheses will dom-
inate the time complexity. The size of this initial
set of hypotheses is itself limited so that it cannot
contain more than hmax hypotheses from any one

rule. The dominating factor is thus the size of the
rule set, which we will further analyze.

The first thing we do is to parse the right-hand
side of the rule, which requires O

(
n3

)
with the

Saers et al. (2009) algorithm, where n is the av-
erage length of the lexical items. We then initial-
ize the counts, which does not actually require a
specific step in implementation. We then iterate
over all bispans in the parse, which has the same
upper bound as the parsing process, since the ap-
proximate parsing algorithm avoids exploring the
entire search space. We then sort the set of hy-
potheses derived from the current rule only, which
is asymptotically bound byO

(
n3lgn

)
, since there

is exactly one hypothesis per parse item. Finally,
there is a selection being made from the set of hy-
potheses derived from the current rule. In prac-
tice, the parsing is more complicated than the sort-
ing, making the time complexity of the whole inner
loop be dominated by the time it takes to parse the
rules.

6 Example

In this section we will trace through how the ex-
ample from the introduction fails to go through
binary segmentation, but succeeds when infix-to-
infix segmentations are an option.

The initial grammar consists of all the sentence
pairs as segmental lexical rules:

S → A 1
A→ he has a red bookhan har en röd bok 0.3

A→ she has a biology bookhon har en biologibok 0.3
A→ it has begundet har börjat 0.3

As noted before, there are no shared biaffixes
among the three lexical rules, so binary segmen-
tation cannot break this grammar down further.
There are, however, three shared bisegments rep-
resenting three different segmentation hypotheses:
has a/har en, has/har and a/en. In this example it
does not matter which hypothesis you choose, so
we will go with the first one, since that is the one
our implementation chose. Breaking out all occur-
rences of has a/har en gives the following gram-

91



mar:
S → A 1

A→ [AA] 0.36
A→ it has begun/det har börjat 0.09

A→ has a/har en 0.18
A→ he/han 0.09

A→ red book/röd bok 0.09
A→ she/hon 0.09

A→ biology book/biologibok 0.09
At this point there are two bisegments that occur
in more than one rule: has/har and a/en. Again,
it does not matter for the final outcome which of
the hypotheses we choose, so we will chose the
first one, again because that is the one our imple-
mentation chose. Breaking out all occurrences of
has/har gives the following grammar:

S → A 1
A→ [AA] 0.421
A→ he/han 0.053

A→ red book/röd bok 0.053
A→ she/hon 0.053

A→ biology book/biologibok 0.053
A→ has/har 0.158
A→ it/det 0.053

A→ begun/börjat 0.053
A→ a/en 0.105

There are no shared bisegments left in the gram-
mar now, so no more segmentations can be done.
Obviously, the probability of the data given this
new grammar is much smaller, but the grammar
itself has generalized far beyond the training data,
to the point where it largely agrees with the pro-
posed trees in Figure 1 (except that this grammar
binarizes the constituents, and treats red book/röd
bok as a segment).

7 Conclusions

We have shown that there are situations in which
a top-down segmenting approach that relies solely
on binary segmentation will fail to generalize, de-
spite there being ample evidence to a human that
a generalization is warranted. We have proposed
ternary segmentation as a solution to provide hy-
potheses that are considered good under a mini-
mum description length objective. And we have
shown that the proposed method could indeed per-
form generalizations that are clear to the human
eye, but not discoverable through binary segmen-
tation. The algorithm is comparable to previ-
ous segmentation approaches in terms of time and

space complexity, so scaling up to non-toy training
corpora is likely to work when the time comes.

Acknowledgements

This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT contract
nos. HR0011-12-C-0014 and HR0011-12-C-0016,
and GALE contract nos. HR0011-06-C-0022 and
HR0011-06-C-0023; by the European Union un-
der the FP7 grant agreement no. 287658; and by
the Hong Kong Research Grants Council (RGC)
research grants GRF620811, GRF621008, and
GRF612806. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of DARPA, the EU, or RGC.

References

Alfred V. Aho and Jeffrey D. Ullman. The The-
ory of Parsing, Translation, and Compiling.
Prentice-Halll, Englewood Cliffs, New Jersey,
1972.

Phil Blunsom, Trevor Cohn, Chris Dyer, andMiles
Osborne. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Joint Confer-
ence of the 47th Annual Meeting of the Asso-
ciation for Computational Linguistics and 4th
International Joint Conference on Natural Lan-
guage Processing of the AFNLP (ACL-IJCNLP
2009), pp. 782–790, Suntec, Singapore, August
2009.

David Chiang. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–
228, 2007.

Frans Josef Och, Christoph Tillmann, and Her-
mann Ney. Improved alignment models for sta-
tistical machine translation. In 1999 Joint SIG-
DAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Cor-
pora, pp. 20–28, University of Maryland, Col-
lege Park, Maryland, June 1999.

Markus Saers and Dekai Wu. Bayesian induc-
tion of bracketing inversion transduction gram-
mars. In Sixth International Joint Conference on
Natural Language Processing (IJCNLP2013),
pp. 1158–1166, Nagoya, Japan, October 2013.
Asian Federation of Natural Language Process-
ing.

92



Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In 11th International Conference
on Parsing Technologies (IWPT’09), pp. 29–32,
Paris, France, October 2009.

Markus Saers, Karteek Addanki, and Dekai Wu.
From finite-state to inversion transductions: To-
ward unsupervised bilingual grammar induc-
tion. In 24th International Conference on
Computational Linguistics (COLING 2012), pp.
2325–2340, Mumbai, India, December 2012.

Markus Saers, Karteek Addanki, and Dekai Wu.
Augmenting a bottom-up ITG with top-down
rules by minimizing conditional description
length. In Recent Advances in Natural Lan-
guage Processing (RANLP 2013), Hissar, Bul-
garia, September 2013.

Markus Saers, Karteek Addanki, and Dekai Wu.
Combining top-down and bottom-up search for
unsupervised induction of transduction gram-
mars. In Seventh Workshop on Syntax, Se-
mantics and Structure in Statistical Translation
(SSST-7), pp. 48–57, Atlanta, Georgia, June
2013.

Markus Saers, Karteek Addanki, and Dekai Wu.
Iterative rule segmentation under minimum
description length for unsupervised transduc-
tion grammar induction. In Adrian-Horia
Dediu, Carlos Martín-Vide, Ruslan Mitkov, and
Bianca Truthe, editors, Statistical Language and
Speech Processing, First International Confer-
ence, SLSP 2013, Lecture Notes in Artificial In-
telligence (LNAI). Springer, Tarragona, Spain,
July 2013.

Markus Saers, Karteek Addanki, and Dekai Wu.
Unsupervised transduction grammar induction
via minimum description length. In Second
Workshop on Hybrid Approaches to Transla-
tion (HyTra), pp. 67–73, Sofia, Bulgaria, Au-
gust 2013.

Markus Saers. Translation as Linear Trans-
duction: Models and Algorithms for Efficient
Learning in Statistical Machine Translation.
PhD thesis, Uppsala University, Department of
Linguistics and Philology, 2011.

Dekai Wu. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377–
403, 1997.

Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. Bayesian learning of non-
compositional phrases with synchronous pars-
ing. In 46th Annual Meeting of the Association
for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pp. 97–
105, Columbus, Ohio, June 2008.

93


