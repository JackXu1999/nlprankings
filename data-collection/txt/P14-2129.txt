



















































Transforming trees into hedges and parsing with "hedgebank" grammars


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 797–802,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Transforming trees into hedges and parsing with “hedgebank” grammars

Mahsa Yarmohammadi†, Aaron Dunlop† and Brian Roark◦
†Oregon Health & Science University, Portland, Oregon ◦Google, Inc., New York

yarmoham@ohsu.edu, {aaron.dunlop,roarkbr}@gmail.com

Abstract

Finite-state chunking and tagging meth-
ods are very fast for annotating non-
hierarchical syntactic information, and are
often applied in applications that do not
require full syntactic analyses. Scenar-
ios such as incremental machine transla-
tion may benefit from some degree of hier-
archical syntactic analysis without requir-
ing fully connected parses. We introduce
hedge parsing as an approach to recover-
ing constituents of length up to some max-
imum span L. This approach improves ef-
ficiency by bounding constituent size, and
allows for efficient segmentation strategies
prior to parsing. Unlike shallow parsing
methods, hedge parsing yields internal hi-
erarchical structure of phrases within its
span bound. We present the approach and
some initial experiments on different infer-
ence strategies.

1 Introduction

Parsing full hierarchical syntactic structures is
costly, and some NLP applications that could ben-
efit from parses instead substitute shallow prox-
ies such as NP chunks. Models to derive such
non-hierarchical annotations are finite-state, so in-
ference is very fast. Still, these partial annota-
tions omit all but the most basic syntactic segmen-
tation, ignoring the abundant local structure that
could be of utility even in the absence of fully con-
nected structures. For example, in incremental (si-
multaneous) machine translation (Yarmohammadi
et al., 2013), sub-sentential segments are trans-
lated independently and sequentially, hence the
fully-connected syntactic structure is not generally
available. Even so, locally-connected source lan-
guage parse structures can inform both segmen-
tation and translation of each segment in such a
translation scenario.

One way to provide local hierarchical syntactic

structures without fully connected trees is to fo-
cus on providing full hierarchical annotations for
structures within a local window, ignoring global
constituents outside that window. We follow the
XML community in naming structures of this type
hedges (not to be confused with the rhetorical de-
vice of the same name), due to the fact that they are
like smaller versions of trees which occur in se-
quences. Such structures may be of utility to var-
ious structured inference tasks, as well as within
a full parsing pipeline, to quickly constrain subse-
quent inference, much as finite-state models such
as supertagging (Bangalore and Joshi, 1999) or
chart cell constraints (Roark and Hollingshead,
2008; Roark et al., 2012) are used.

In this paper, we consider the problem of hedge
parsing, i.e., discovering every constituent of
length up to some span L. Similar constraints
have been used in dependency parsing (Eisner
and Smith, 2005; Dreyer et al., 2006), where the
use of hard constraints on the distance between
heads and dependents is known as vine parsing.
It is also reminiscent of so-called Semi-Markov
models (Sarawagi and Cohen, 2004), which allow
finite-state models to reason about segments rather
than just tags by imposing segment length limits.
In the XML community, trees and hedges are used
for models of XML document instances and for
the contents of elements (Brüggemann-Klein and
Wood, 2004). As far as we know, this paper is
the first to consider this sort of partial parsing ap-
proach for natural language.

We pursue this topic via tree transformation,
whereby non-root non-terminals labeling con-
stituents of span > L in the tree are recursively
elided and their children promoted to attach to
their parent. In such a way, hedges are sequen-
tially connected to the top-most non-terminal in
the tree, as demonstrated in Figure 1. After apply-
ing such a transform to a treebank, we can induce
grammars and modify parsing to search as needed
to recover just these constituents.

In this paper, we propose several methods to

797



a)

b)

Figure 1: a) Full parse tree, b) Hedge parse tree with maximum constituent span of 7 (L = 7).

parse hedge constituents and examine their accu-
racy/efficiency tradeoffs. This is compared with
a baseline of parsing with a typically induced
context-free grammar and transforming the result
via the hedge transform, which provides a ceiling
on accuracy and a floor on efficiency. We inves-
tigate pre-segmenting the sentences with a finite-
state model prior to hedge parsing, and achieve
large speedups relative to hedge parsing the whole
string, though at a loss in accuracy due to cas-
cading segmentation errors. In all cases, we find
it crucial that our “hedgebank” grammars be re-
trained to match the conditions during inference.

2 Methods

In this section, we present the details of our ap-
proach. First, we present the simple tree transform
from a full treebank parse tree to a (root attached)
sequence of hedges. Next, we discuss modifica-
tions to inference and the resulting computational
complexity gains. Finally, we discuss segmenting
to further reduce computational complexity.

2.1 Hedge Tree Transform
The hedge tree transform converts the original
parse tree into a hedge parse tree. In the resulting
hedge parse tree, every child of the top-most node
spans at most L words. To transform an original
tree to a hedge tree, we remove every non-terminal

with span larger than L and attach its children to its
parent. We label span length on each node by re-
cursively summing the span lengths of each node’s
children, with terminal items by definition having
span 1. A second top-down pass evaluates each
node before evaluating its children, and removes
nodes spanning > L words. For example, the span
of the non-root S, SBAR, ADJP, and VP nodes in
Figure 1(a) have spans between 10 and 13, hence
are removed in the tree in Figure 1(b).

If we apply this transform to an entire tree-
bank, we can use the transformed trees to induce
a PCFG for parsing. Figure 2 plots the percentage
of constituents from the original WSJ Penn tree-
bank (sections 2-21) retained in the transformed
version, as we vary the maximum span length pa-
rameter L. Over half of constituents have span 3 or
less (which includes frequent base noun phrases);
L = 7 covers approximately three quarters of the
original constituents, and L = 15 over 90%. Most
experiments in this paper will focus on L = 7,
which is short enough to provide a large speedup
yet still cover a large fraction of constituents.

2.2 Hedge Parsing

As stated earlier, our brute-force baseline ap-
proach is to parse the sentence using a full context-
free grammar (CFG) and then hedge-transform the
result. This method should yield a ceiling on

798



Pc
t.

of
co

ns
tit

ue
nt

s
re

ta
in

ed

0 5 10 15 20

50

60

70

80

90

100

Maximum  span  size  (L)

P
er
ce
nt
ag
e  
of
  c
on
st
itu
en
ts
  re

ta
in
ed

ce
ss

to
co

m
pa

re
d

w
ith

gr
am

m
ar

s
tr

ai
ne

d
on

tr
an

s-
fo

rm
ed

tr
ee

s;
bu

t
it

w
ill

be
sl

ow
er

to
pa

rs
e.

W
e

ai
m

to
dr

am
at

ic
al

ly
im

pr
ov

e
ef

fic
ie

nc
y

up
on

th
is

ba
se

lin
e

w
hi

le
lo

si
ng

as
lit

tle
ac

cu
ra

cy
as

po
ss

ib
le

.
Si

nc
e

w
e

lim
it

th
e

sp
an

of
no

n-
te

rm
in

al
la

-
be

ls
,w

e
ca

n
co

ns
tr

ai
n

th
e

se
ar

ch
pe

rf
or

m
ed

by
th

e
pa

rs
er

,g
re

at
ly

re
du

ce
th

e
C

Y
K

pr
oc

es
si

ng
tim

e.
In

es
se

nc
e,

w
e

pe
rf

or
m

no
w

or
k

in
ch

ar
tc

el
ls

sp
an

-
ni

ng
m

or
e

th
an

L
w

or
ds

,e
xc

ep
tf

or
th

e
ce

lls
al

on
g

th
e

pe
ri

ph
er

y
of

th
e

ch
ar

t,
w

hi
ch

ar
e

ju
st

us
ed

to
co

nn
ec

t
th

e
he

dg
es

to
th

e
ro

ot
.

C
on

si
de

r
th

e
fla

t
tr

ee
in

Fi
gu

re
1.

Fo
r

us
e

by
a

C
Y

K
pa

rs
in

g
al

-
go

ri
th

m
,t

re
es

ar
e

bi
na

ri
ze

d
pr

io
r

to
gr

am
m

ar
in

-
du

ct
io

n,
re

su
lti

ng
in

sp
ec

ia
ln

on
-t

er
m

in
al

s
cr

ea
te

d
by

bi
na

ri
za

tio
n.

O
th

er
th

an
th

e
sy

m
bo

la
tt

he
ro

ot
of

th
e

tr
ee

,t
he

on
ly

co
ns

tit
ue

nt
s

w
ith

sp
an

le
ng

th
gr

ea
te

rt
ha

n
L

in
th

e
bi

na
ri

ze
d

tr
ee

w
ill

be
la

be
le

d
w

ith
th

es
e

sp
ec

ia
lb

in
ar

iz
at

io
n

no
n-

te
rm

in
al

s.
Fu

r-
th

er
,i

f
th

e
bi

na
ri

za
tio

n
sy

st
em

at
ic

al
ly

gr
ou

ps
th

e

le
ft

m
os

to
rt

he
ri

gh
tm

os
tc

hi
ld

re
n

un
de

rt
he

se
ne

w
no

n-
te

rm
in

al
s

(t
he

m
os

t
co

m
m

on
st

ra
te

gy
),

th
en

co
ns

tit
ue

nt
s

w
ith

sp
an

gr
ea

te
r

th
an

L
w

ill
ei

th
er

be
gi

n
at

th
e

fir
st

w
or

d
(l

ef
tm

os
tg

ro
up

in
g)

or
en

d
at

th
e

la
st

w
or

d
(r

ig
ht

m
os

tg
ro

up
in

g)
,f

ur
th

er
co

n-
st

ra
in

in
g

th
e

nu
m

be
ro

fc
el

ls
in

th
e

ch
ar

tr
eq

ui
ri

ng
w

or
k. C
om

pl
ex

ity
of

pa
rs

in
g

w
ith

a
fu

ll
C

Y
K

pa
rs

er
is

O
(n

3
|G

|)
w

he
re

n
is

th
e

le
ng

th
of

in
pu

ta
nd

|G
|is

th
e

gr
am

m
ar

si
ze

co
ns

ta
nt

.
In

co
nt

ra
st

,c
om

pl
ex

-
ity

of
pa

rs
in

g
w

ith
a

he
dg

e
co

ns
tr

ai
ne

d
C

Y
K

is
re

-
du

ce
d

to
O

((
n
L

2
+

n
2
)|G

|).
To

se
e

th
at

th
is

is
th

e
ca

se
,c

on
si

de
rt

ha
tt

he
re

ar
e
O

(n
L

)c
el

ls
of

sp
an

L
or

le
ss

,a
nd

ea
ch

ha
s

a
m

ax
im

um
of

L
m

id
po

in
ts

,
w

hi
ch

ac
co

un
ts

fo
r

th
e

fir
st

te
rm

.
B

ey
on

d
th

es
e,

th
er

e
ar

e
O

(n
)

re
m

ai
ni

ng
ac

tiv
e

ce
lls

w
ith

O
(n

)
po

ss
ib

le
m

id
po

in
ts

,w
hi

ch
ac

co
un

ts
fo

rt
he

se
co

nd
te

rm
.

N
ot

e,
ho

w
ev

er
,t

ha
tt

he
w

or
k

in
th

es
e

la
tte

r
ce

lls
m

ay
be

le
ss

,
si

nc
e

th
e

se
t

of
po

ss
ib

le
no

n-
te

rm
in

al
s

is
re

du
ce

d.

It
is

po
ss

ib
le

to
pa

rs
e

w
ith

a
st

an
da

rd
ly

in
du

ce
d

PC
FG

us
in

g
th

is
so

rt
of

he
dg

e
co

ns
tr

ai
ne

d
pa

rs
-

in
g

th
at

on
ly

co
ns

id
er

s
a

su
bs

et
of

th
e

ch
ar

tc
el

ls
,

an
d

sp
ee

du
ps

ar
e

ac
hi

ev
ed

,h
ow

ev
er

th
is

is
cl

ea
rl

y
no

n-
op

tim
al

,s
in

ce
th

e
m

od
el

is
ill

-s
ui

te
d

to
co

m
-

bi
ni

ng
he

dg
es

in
to

fla
ts

tr
uc

tu
re

s
at

th
e

ro
ot

of
th

e
tr

ee
.

T
hi

s
re

su
lts

in
de

gr
ad

at
io

n
of

pa
rs

in
g

pe
r-

fo
rm

an
ce

by
te

ns
of

po
in

ts
of

F-
m

ea
su

re
ve

rs
us

st
an

da
rd

pa
rs

in
g.

In
st

ea
d,

in
al

l
sc

en
ar

io
s

w
he

re
a

th
e

ch
ar

ti
s

co
ns

tr
ai

ne
d

to
se

ar
ch

fo
rh

ed
ge

s,
w

e
le

ar
n

a
gr

am
m

ar
fr

om
a

he
dg

e
tr

an
sf

or
m

ed
tr

ee
-

ba
nk

,m
at

ch
ed

to
th

e
m

ax
im

um
le

ng
th

al
lo

w
ed

by
th

e
pa

rs
er

,
w

hi
ch

w
e

ca
ll

a
he

dg
eb

an
k

gr
am

m
ar

.
A

he
dg

eb
an

k
gr

am
m

ar
is

a
fu

lly
fu

nc
tio

na
lP

C
FG

an
d

it
ca

n
be

us
ed

w
ith

an
y

st
an

da
rd

pa
rs

in
g

al
-

go
ri

th
m

,
i.e

.,
th

es
e

ar
e

no
t

ge
ne

ra
lly

fin
ite

-s
ta

te
eq

ui
va

le
nt

m
od

el
s.

H
ow

ev
er

,
us

in
g

th
e

B
er

ke
-

le
y

gr
am

m
ar

le
ar

ne
r

(s
ee

Se
ct

io
n

3)
,w

e
fin

d
th

at
he

dg
eb

an
k

gr
am

m
ar

s
ar

e
ty

pi
ca

lly
sm

al
le

r
th

an
tr

ee
ba

nk
gr

am
m

ar
s,

al
so

co
nt

ri
bu

tin
g

to
pa

rs
in

g
sp

ee
du

p
vi

a
th

e
gr

am
m

ar
co

ns
ta

nt
.

A
un

iq
ue

pr
op

er
ty

of
he

dg
e

co
ns

tit
ue

nt
s

co
m

-
pa

re
d

to
co

ns
tit

ue
nt

s
in

th
e

or
ig

in
al

pa
rs

e
tr

ee
s

is
th

at
th

ey
ar

e
se

qu
en

tia
lly

co
nn

ec
te

d
to

th
e

TO
P

no
de

.
T

hi
s

pr
op

er
ty

en
ab

le
s

us
to

ch
un

k
th

e
se

nt
en

ce
in

to
se

gm
en

ts
th

at
co

rr
es

po
nd

to
co

m
-

pl
et

e
he

dg
es

,
an

d
pa

rs
e

th
e

se
gm

en
ts

in
de

pe
n-

de
nt

ly
(a

nd
si

m
ul

ta
ne

ou
sl

y)
in

st
ea

d
of

pa
rs

in
g

th
e

en
tir

e
se

nt
en

ce
.

In
se

ct
io

n
2.

3,
w

e
pr

es
en

to
ur

ap
-

pr
oa

ch
to

he
dg

e
se

gm
en

ta
tio

n.
N

ot
e

th
at

pa
rs

in
g

se
gm

en
ts

w
ith

a
gr

am
m

ar
tr

ai
ne

d
on

w
ho

le
st

ri
ng

s

Maximum span size (L)
Figure 2: Percentage of constituents retained at various
span length parameters

hedge-parsing accuracy, as it has access to rich
contextual information (as compared to grammars
trained on transformed trees). Naturally, inference
will be slow; we aim to improve efficiency upon
this baseline while minimizing accuracy loss.

Since we limit the span of non-terminal la-
bels, we can constrain the search performed by the
parser, greatly reduce the CYK processing time. In
essence, we perform no work in chart cells span-
ning more than L words, except for the cells along
the periphery of the chart, which are just used to
connect the hedges to the root. Consider the flat
tree in Figure 1(b). For use by a CYK parsing al-
gorithm, trees are binarized prior to grammar in-
duction, resulting in special non-terminals created
by binarization. Other than the symbol at the root
of the tree, the only constituents with span length
greater than L in the binarized tree will be labeled
with these special binarization non-terminals. Fur-
ther, if the binarization systematically groups the
leftmost or the rightmost children under these new
non-terminals (the most common strategy), then
constituents with span greater than L will either
begin at the first word (leftmost grouping) or end
at the last word (rightmost), further constraining
the number of cells in the chart requiring work.

Complexity of parsing with a full CYK parser is
O(n3|G|) where n is the length of input and |G| is
the grammar size constant. In contrast, complex-
ity of parsing with a hedge constrained CYK is re-
duced to O((nL2 + n2)|G|). To see that this is the
case, consider that there are O(nL) cells of span L
or less, and each has a maximum of L midpoints,
which accounts for the first term. Beyond these,
there are O(n) remaining active cells with O(n)
possible midpoints, which accounts for the second
term. Note also that these latter cells (spanning
> L words) may be less expensive, as the set of
possible non-terminals is reduced to only those in-
troduced by binarization.

It is possible to parse with a standardly induced

PCFG using this sort of hedge constrained pars-
ing that only considers a subset of the chart cells,
and speedups are achieved, however this is clearly
non-optimal, since the model is ill-suited to com-
bining hedges into flat structures at the root of the
tree. Space constraints preclude inclusion of tri-
als with this method, but the net result is a se-
vere degradation in accuracy (tens of points of F-
measure) versus standard parsing. Thus, we train
a grammar in a matched condition, which we call
it a hedgebank grammar. A hedgebank gram-
mar is a fully functional PCFG which is learned
from a hedge transformed treebank. A hedgebank
grammar can be used with any standard parsing
algorithm, i.e., these are not generally finite-state
equivalent models. However, using the Berke-
ley grammar learner (see §3), we find that hedge-
bank grammars are typically smaller than tree-
bank grammars, reducing the grammar constant
and contributing to faster inference.

A unique property of hedge constituents com-
pared to constituents in the original parse trees
is that they are sequentially connected to the top-
most node. This property enables us to chunk the
sentence into segments that correspond to com-
plete hedges, and parse the segments indepen-
dently (and simultaneously) instead of parsing the
entire sentence. In section 2.3, we present our ap-
proach to hedge segmentation.

In all scenarios where the chart is constrained
to search for hedges, we learn a hedgebank gram-
mar, which is matched to the maximum length al-
lowed by the parser. In the pre-segmentation sce-
nario, we first decompose the hedge transformed
treebank into its hedge segments and then learn a
hedgebank grammar from the new corpus.

2.3 Hedge Segmentation
In this section we present our segmentation model
which takes the input sentence and chunks it into
appropriate segments for hedge parsing. We treat
this as a binary classification task which decides
if a word can begin a new hedge. We use hedge
segmentation as a finite-state pre-processing step
for hedge context-free parsing.

Our task is to learn which words can begin
(B) a hedge constituent. Given a set of labeled
pairs (S, H) where S is a sentence of n words
w1 . . . wn and H is its hedge parse tree, word wb
belongs to B if there is a hedge constituent span-
ning wb . . . we for some e ≥ b and wb belongs to B̄
otherwise. To predict the hedge boundaries more
accurately, we grouped consecutive unary or POS-

799



tag hedges together under a new non-terminal la-
beled G. Unlabeled segmentation tags for the
words in the example sentence in Figure 1(b) are:

“Analysts/B are/B̄ concerned/B̄ that/B̄ much/B
of/B̄ the/B̄ high-yield/B̄ market/B̄ will/B
remain/B̄ treacherous/B̄ for/B̄ investors/B̄ ./B”

In addition to the simple unlabeled segmentation
with B and B̄ tags, we try a labeled segmenta-
tion with BC and B̄C tags where C is hedge con-
stituent type. We restrict the types to the most im-
portant types – following the 11 chunk types an-
notated in the CoNLL-2000 chunking task (Sang
and Buchholz, 2000) – by replacing all other types
with a new type OUT. Thus, “Analysts” is labeled
BG; “much”, BNP; “will”, BVP and so on.

To automatically predict the class of each word
position, we train a multi-class classifier from la-
beled training data using a discriminative linear
model, learning the model parameters with the av-
eraged perceptron algorithm (Collins, 2002). We
follow Roark et al. (2012) in the features they used
to label words as beginning or ending constituents.
The segmenter extracts features from word and
POS-tag input sequences and hedge-boundary tag
output sequences. The feature set includes tri-
grams of surrounding words, trigrams of surround-
ing POS tags, and hedge-boundary tags of the pre-
vious words. An additional orthographical fea-
ture set is used to tag rare1 and unknown words.
This feature set includes prefixes and suffixes of
the words (up to 4 characters), and presence of
a hyphen, digit, or an upper-case character. Re-
ported results are for a Markov order-2 segmenter,
which includes features with the output classes of
the previous two words.

3 Experimental Results
We ran all experiments on the WSJ Penn Tree-
bank corpus (Marcus et al., 1999) using section
2-21 for training, section 24 for development, and
section 23 for testing. We performed exhaustive
CYK parsing using the BUBS parser2 (Bodenstab
et al., 2011) with Berkeley SM6 latent-variable
grammars (Petrov and Klein, 2007) learned by the
Berkeley grammar trainer with default settings.
We compute accuracy from the 1-best Viterbi
tree extracted from the chart using the standard
EVALB script. Accuracy results are reported as
precision, recall and F1-score, the harmonic mean
between the two. In all trials, we evaluate accuracy
with respect to the hedge transformed reference

1Rare words occur less than 5 times in the training data.
2https://code.google.com/p/bubs-parser

Hedge Parsing Acc/Eff
Parser P R F1 w/s
Full w/full CYK 88.8 89.2 89.0 2.4
Hedgebank 87.6 84.4 86.0 25.7

Table 1: Hedge parsing results on section 24 for L = 7.

treebank, i.e., we are not penalizing the parser for
not discovering constituents longer than the max-
imum length. Segmentation accuracy is reported
as an F1-score of unlabeled segment bracketing.
We ran timing tests on an Intel 2.66GHz proces-
sor with 3MB of cache and 2GB of memory. Note
that segmentation time is negligible compared to
the parsing time, hence is omitted in reported time.
Efficiency results are reported as number of words
parsed per second (w/s).

Table 1 presents hedge parsing accuracy on
the development set for the full parsing baseline,
where the output of regular PCFG parsing is trans-
formed to hedges and evaluated, versus parsing
with a hedgebank grammar, with no segmenta-
tion of the strings. We find an order of magnitude
speedup of parsing, but at the cost of 3 percent F-
measure absolute. Note that most of that loss is
in recall, indicating that hedges predicted in that
condition are nearly as reliable as in full parsing.

Table 2 shows the results on the development
set when segmenting prior to hedge parsing. The
first row shows the result with no segmentation,
the same as the last row in Table 1 for ease of ref-
erence. The next row shows behavior with per-
fect segmentation. The final two rows show per-
formance with automatic segmentation, using a
model that includes either unlabeled or labeled
segmentation tags, as described in the last section.
Segmentation accuracy is better for the model with
labels, although overall that accuracy is rather low.
We achieve nearly another order of magnitude
speedup over hedge parsing without segmentation,
but again at the cost of nearly 5 percent F1.

Table 3 presents results of our best configura-
tions on the eval set, section 23. The results show
the same patterns as on the development set. Fi-
nally, Figure 3 shows the speed of inference, la-

Table 2: Hedge segmentation and parsing results on section
24 for L = 7.

Segmen- Seg Hedge Parsing Acc/Eff
tation F1 P R F1 w/s

None n/a 87.6 84.4 86.0 25.7
Oracle 100 91.3 88.9 90.1 188.6
Unlabeled 80.6 77.2 75.3 76.2 159.1
Labeled 83.8 83.1 79.5 81.3 195.8

800



Segmentation Grammar
Segmentation Acc Hedge Parsing Acc/Eff

P R F1 P R F1 w/s
None Full w/full CYK n/a 90.3 90.3 90.3 2.7
None Hedgebank n/a 88.3 85.3 86.8 26.2

Labeled Hedgebank 84.0 86.6 85.3 85.1 81.1 83.0 203.0

Table 3: Hedge segmentation and parsing results on test data, section 23, for L = 7.

W
or

ds
pe

rs
ec

on
d

Full  Parsing

Hedge  No  Seg

Hedge  With  Seg

0 5 10 15 20

0

200

400

600

800

Maximum  span  size  (L)

W
o
rd
s
  p
a
rs
e
d
  p
e
r  
s
e
c
o
n
d

W
or

ds
pe

rs
ec

on
d

Full  Parsing

Hedge  No  Seg

Hedge  With  Seg

0 5 10 15 20

0

200

400

600

800

Maximum  span  size  (L)

W
o
rd
s
  p
a
rs
e
d
  p
e
r  
s
e
c
o
n
d

H
ed

ge
Pr

ec
is

io
n

0 5 10 15 20

75

80

85

90

95

Maximum  span  size  (L)
H
e
d
g
e
  P
re
c
is
io
n

H
ed

ge
R

ec
al

l

0 5 10 15 20

75

80

85

90

95

Maximum  span  size  (L)

H
e
d
g
e
  R
e
c
a
ll

Figure 4: Hedge parsing a) efficiency, and b) accuracy on test data, section 23, for L = 3 � 20.

Maximum span size (L) Maximum span size (L)

a) b)
Figure 3: Hedge parsing a) efficiency, and b) accuracy on test data, section 23, for L = 3–20.

beled precision and labeled recall of annotating
hedge constituents on the test set as a function
of the maximum span parameter L, versus the
baseline parser. Keep in mind that the number
of reference constituents increases as L increases,
hence both precision and recall can decrease as
the parameter grows. Segmentation achieves large
speedups for smaller L values, but the accuracy
degradation is consistent, pointing to the need for
improved segmentation.

4 Conclusion and Future Work

We proposed a novel partial parsing approach for
applications that require a fast syntactic analysis
of the input beyond shallow bracketing. The span-
limit parameter allows tuning the annotation of in-
ternal structure as appropriate for the application
domain, trading off annotation complexity against
inference time. These properties make hedge pars-
ing potentially very useful for incremental text or
speech processing, such as streaming text analysis
or simultaneous translation.

One interesting characteristic of these anno-
tations is that they allow for string segmenta-
tion prior to inference, provided that the segment
boundaries do not cross any hedge boundaries. We
found that baseline segmentation models did pro-

vide a significant speedup in parsing, but that cas-
cading errors remain a problem.

There are many directions of future work to
pursue here. First, the current results are all for
exhaustive CYK parsing, and we plan to per-
form a detailed investigation of the performance
of hedgebank parsing with prioritization and prun-
ing methods of the sort available in BUBS (Bo-
denstab et al., 2011). Further, this sort of annota-
tion seems well suited to incremental parsing with
beam search, which has been shown to achieve
high accuracies even for fully connected parsing
(Zhang and Clark, 2011). Improvements to the
transform (e.g., grouping items not in hedges un-
der non-terminals) and to the segmentation model
(e.g., increasing precision at the expense of recall)
could improve accuracy without greatly reducing
efficiency. Finally, we intend to perform an ex-
trinsic evaluation of this parsing in an on-line task
such as simultaneous translation.

Acknowledgments

This work was supported in part by NSF grant
#IIS-0964102. Any opinions, findings, conclu-
sions or recommendations expressed in this pub-
lication are those of the authors and do not neces-
sarily reflect the views of the NSF.

801



References

Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237–265.

Nathan Bodenstab, Aaron Dunlop, Keith Hall, and
Brian Roark. 2011. Beam-width prediction for ef-
ficient context-free parsing. In Proceedings of the
49th Annual Meeting ACL: HLT, pages 440–449.

Anne Brüggemann-Klein and Derick Wood. 2004.
Balanced context-free grammars, hedge grammars
and pushdown caterpillar automata. In Extreme
Markup Languages.

Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1–8.

Markus Dreyer, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking
for speed and precision. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 201–205.

Jason Eisner and Noah A. Smith. 2005. Parsing with
soft and hard constraints on dependency length. In
Proceedings of the Ninth International Workshop on
Parsing Technology (IWPT), pages 30–41.

Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-
3. Linguistic Data Consortium, Philadelphia.

Slav Petrov and Dan Klein. 2007. Learning and infer-
ence for hierarchically split PCFGs. In Proceedings
of the 22nd national conference on Artificial intelli-
gence, pages 1663–1666.

Brian Roark and Kristy Hollingshead. 2008. Classi-
fying chart cells for quadratic complexity context-
free inference. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 745–751.

Brian Roark, Kristy Hollingshead, and Nathan Boden-
stab. 2012. Finite-state chart constraints for reduced
complexity context-free parsing pipelines. Compu-
tational Linguistics, 38(4):719–753.

Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task:
Chunking. In Proceedings of Conference on Com-
putational Natural Language Learning (CoNLL),
pages 127–132.

Sunita Sarawagi and William W. Cohen. 2004. Semi-
Markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems (NIPS), pages 1185–1192.

Mahsa Yarmohammadi, Vivek K. Rangarajan Sridhar,
Srinivas Bangalore, and Baskaran Sankaran. 2013.
Incremental segmentation and decoding strategies
for simultaneous translation. In Proceedings of the
6th International Joint Conference on Natural Lan-
guage Processing (IJCNLP), pages 1032–1036.

Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.

802


