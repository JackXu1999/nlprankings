



















































Syntactic Parsing of Web Queries


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1787–1796,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Syntactic Parsing of Web Queries

Xiangyan Sun
Fudan University

Haixun Wang
Facebook

Yanghua Xiao∗
Fudan University

Zhongyuan Wang
Microsoft Research

Abstract

Syntactic parsing of web queries is impor-
tant for query understanding. However, web
queries usually do not observe the grammar of
a written language, and no labeled syntactic
trees for web queries are available. In this pa-
per, we focus on a query’s clicked sentence,
i.e., a well-formed sentence that i) contains
all the tokens of the query, and ii) appears in
the query’s top clicked web pages. We ar-
gue such sentences are semantically consistent
with the query. We introduce algorithms to de-
rive a query’s syntactic structure from the de-
pendency trees of its clicked sentences. This
gives us a web query treebank without manual
labeling. We then train a dependency parser
on the treebank. Our model achieves much
better UAS (0.86) and LAS (0.80) scores than
state-of-the-art parsers on web queries.

1 Introduction

Syntactic analysis is important in understanding
a sentence’s grammatical constituents, parts of
speech, syntactic relations, and semantics. In this
paper, we are concerned with the syntactic structure
of a short text. The challenge is that short texts, for
example, web queries, do not observe grammars of
written languages (e.g., users often overlook capital-
ization, function words, and word order when creat-

∗ Correspondence author. This paper was supported
by National Key Basic Reserach Program of China un-
der No.2015CB358800, by National NSFC(No.61472085,
61171132, 61033010, U1509213), by Shanghai Municipal Sci-
ence and Technology Commission foundation key project under
No.15JC1400900.

ing a web query), and applying parsers trained on
standard treebanks on queries leads to poor results.

Syntactic structures are valuable for query under-
standing. Consider the following web queries and
their syntactic structures we would like to construct:

cover iphone 6 plus
NN NN CD NN

nn num amod

distance earth moon
NN NN NN

nn
nn

faucet adapter female
NN NN ADJ

nn nn

The syntactic structure of query cover iphone
6 plus tells us that the head token is cover, in-
dicating its intent is to shop for the cover of an
iphone, instead of iphones. With this knowledge,
search engines show ads of iphone covers instead of
iphones. For distance earth moon, the head
is distance, indicating its intent is to find the dis-
tance between the earth and the moon. For faucet
adapter female, the intent is to find a female
faucet adapter. In summary, correctly identifying
the head of a query helps identify its intent, and
correctly identifying the modifiers helps rewrite the
query (e.g., dropping non-essential modifiers).

Syntactic parsing of web queries is challenging
for at least two reasons. First, grammatical signals
from function words and word order are not avail-
able. Query distance earth moon is missing
function words between (preposition), and (coordi-
nator), and the (determiner) in conveying the intent

1787



distance between the earth and the moon. Also, it
is likely that queries {distance earth moon,
earth moon distance, earth distance
moon, · · · } have the same intent, which means they
should have the same syntactic structure. Second,
there is no labeled dependency trees (treebank) for
web queries, nor is there a standard for construct-
ing such dependency trees. It will take a tremendous
amount of time and effort to come up with such a
standard and a treebank for web queries.

In this paper, we propose an end-to-end solution
from treebank construction to syntactic parsing for
web queries. Our model achieves a UAS of 0.830
and an LAS of 0.747 on web queries, which is
dramatic improvement over state-of-the-art parsers
trained from standard treebanks.

2 Our Approach

The biggest challenge of syntactic analysis of web
queries is that they do not contain sufficient gram-
matical signals required for parsing. Indeed, web
queries can be very ambiguious. For example, kids
toysmay mean either toys for kids or kids
with toys, for which the dependency relation-
ships between toys and kids are totally opposite.

kids with toys
NN IN NN

prep pobj

toys for kids
NN IN NN

prep pobj

In view of this, why is syntactic parsing of web
queries a legitimate problem? We have shown some
example syntactic structures for 3 queries in Section
1. How do we know they are the correct syntactic
structures for the queries? We answer these ques-
tions here.

2.1 Derive syntax from semantics
In many cases, humans can easily determine the syn-
tax of a web query because its intent is easy to under-
stand. For example, for toys kids, we are pretty
sure as a web query, its intent is to look for toys for
kids, instead of the other way around. Thus, toys
should be the head of the query, and kids should be
its modifier. In other words, when the semantics of a
query is understood, we can often recover its syntax.

We may then manually annotate web queries.
Specifically, given a query, a human annotator forms
a sentence that is consistent with the meaning he

comes up for the query. Then, from the sentence’s
syntactic structure (which is well understood and
can be derived by a parser), the annotator derives the
syntactic structure of the query. For example, for
query thai food houston, the annotator may
formulate the following sentence:

... my favorite Thai food in Houston ...
PRP$ JJ NNP NN IN NN

poss

amod

nn prep pobj

Then we may project the dependency tree of the
sentence to the query:

thai food houston
NNP NN NN

nn nn

The above approach has two issues. First, food
and houston are not directly connected in the de-
pendency tree of the sentence. We connected them
in the query, but in general, it is not trivial to in-
fer synatx of the query from sentences in a consis-
tent way. There is no linguistic standard for doing
this. Second, annotation is very costly. A treebank
project takes years to accomplish.

2.2 Semantics of a web query
To avoid human annotation, we derive syntactic un-
derstanding of the query from semantic understand-
ing of the query. Our goal is to decide for any two
tokens x, y ∈ q, whether there is a dependency arc
between x and y, and if yes, what the dependency is.

Context-free signals. One approach to determine
the dependency between x and y is to directly model
P (e|x, y), where e denotes the dependency (x → y
or x ← y). It is context-free because we do not
condition on the query where x and y appear in.

To acquire P (e|x, y), we may consider annotated
corpora such as Google’s syntactic ngram (Goldberg
and Orwant, 2013). For any x and y, we count the
number of times that x is a dependent of y in the cor-
pus. One disadvantage of this approach is that web
queries and normal text differ significantly in distri-
bution. Another approach (Wang et al., 2014) is to
use search log to estimate P (e|x, y), where x and
y are nouns. Specifically, we find queries of pat-
tern x PREP y, where PREP is a preposition {of, in,
for, at, on, with, · · · }. We have P (x → y|x, y) =

1788



nx,y
nx,y+ny,x

where nx,y denotes the number of times
pattern x PREP y appears in the search log. The dis-
advantage is that the simple pattern only gives de-
pendency between two nouns.

Context-sensitive signals. The context-free ap-
proach has two major weaknesses: (1) It is risky to
decide the dependency between two tokens without
considering the context. (2) Context-free signals do
not reveal the type of dependency, that is, it does not
reveal the linguistic relationship between the head
and the modifier.

To take context into consideration, which means
estimating P (e|x, y, q) for any two tokens x, y ∈ q,
we are looking at the problem of building a parser
for web queries. This requires a training dataset (a
treebank). In this work, we propose to automati-
cally create such a treebank. The feasibility is cen-
tered on the following assumption: The intent of q
is contained in or consistent with the semantics of
its clicked sentences. We call sentence s a clicked
sentence of q if i) s appears in a top clicked page for
q, and ii) s contains all tokens in q. For instance, as-
sume sentence s = “... my favorite Thai
food in Houston ...” appears in one of the
most frequently clicked pages for query q = thai
food houston, then s is a clicked sentence of q.
It follows from the above assumption that the de-
pendency between any two tokens in q are likely to
be the same as the dependency between their corre-
sponding tokens in s. This allows to create a tree-
bank if we can project the dependency from sen-
tences to queries. However, since x and y may not
be directly connected by a dependency edge in s,
we need a method to derive the dependency between
x, y ∈ q from the (indirect) dependency between
x, y ∈ s. We propose such a method in Section 3.

3 Treebank for Web Queries

We create a web query treebank by projecting de-
pendency from clicked sentences to queries.

3.1 Inferring a dependency tree

A query q may have multiple clicked sentences. We
describe here how we project dependency to q from
such a sentence s. We describe how we aggregate
dependencies from multiple sentences in Sec 3.2.

Under our assumption, each token x ∈ q must
appear in sentence s. But x may appear multiple
times in s (especially when x is a function word).
As an example, for query apple watch stand,
we may get the following sentence:

Its apple watch charging stand is my favorite stand .
PRP$ NN NN NN NN VBZ PRP$ JJ NN .

poss

nn nn nn

nsubj
cop

poss
amod punct

Sentence s contains token stand twice, but only
one subtree contains each token in q exactly once.

apple watch charging stand
NN NN NN NN

nn nn nn

We use the following heuristics to derive a depen-
dency tree for query q from sentence s.

1. Let Ts denote all the subtrees of the depen-
dency tree of s.

2. Find the minimum subtree t ∈ Ts such that
each x ∈ q has one and only one match x′ ∈ t.

3. Derive dependency tree tq,s for q from t as fol-
lows. For any two tokens x and y in q:

(a) if there is an edge from x′ to y′ in t, we
create a same edge from x to y in tq,s.

(b) if there is a path1 from x′ to y′ in t, we
create an edge from x to y in tq,s, and label
it temporarily as dep.

We note the following. First, we argue that if the
dependency tree of s has a subtree that contains each
token in q once and only once, then it is very likely
that the subtree expresses the same semantics as the
query. On the other hand, if we cannot find such a
subtree, it is an indication that we cannot derive rea-
sonable dependency information from the sentence.

Second, it’s possible x′ and y′ are not connected
directly in s but through one or more other tokens.
Thus, we do not know the label of the derived edge.
We will decide on the label in Sec 3.3.

Third, we want to know whether it is meaningful
to connect x and y in q while x′ and y′ are not di-
rectly connected in s. We evaluated a few hundreds

1A path consists of edges of the same direction.

1789



of query-sentence pairs. Among the cases where de-
pendency trees for queries can be derived success-
fully, we found that x′ and y′ are connected in 5
possible ways (Table 1). We describe them in de-
tails next.

directly connected 46%
connected via function words 24%
connected via modifiers 24%
connected via a head noun 4%
connected via a verb 2%

Table 1: Dependency Projection

Directly connected. In this case, we copy the
edge and its label directly. Consider query party
supplies cheap’s clicked sentence below:

... selection of cheap party supplies is ...
NN IN JJ NN NNS VBZ

prep

amod

nn

pobj

Here both (party, supplies) and (supplies,
cheap) are directly connected. The query inherits
the dependencies, but note that tokens supplies
and cheap have different word orders in q and s:

party supplies cheap
NN NNS JJ

nn amod

Connected via function words. It is quite com-
mon prepositions are omitted in a query. Consider
query moon landing’s clicked sentence:

... first soft landing on moon in 37 years .
JJ JJ NN IN NN IN CD NNS .

amod
amod prep pobj

prep

num
pobj

We can derive the following dependency tree:

moon landing
NN NN

dep

For query side effects b12, suppose we
have the following sentence:

The side effects of vitamin b12 ...
DT NN NNS IN NN JJ

det
nn prep nn

pobj

The derived dependency tree should be:

side effects b12
NN NNS JJ

nn dep

For these two cases, we need to introduce a de-
rived edge for the query, which will be resolved later
to a specific dependency label.

Connected via modifiers. Many web queries are
noun compounds. Their clicked sentences may have
more modifiers. Depending on the bracketing, we
may or may not have direct dependencies.

For offshore work and its clicked sentence
below, missing drilling in the query does not
cause any problem: offshore and work are still
directly connected in the dependency tree.

... this offshore drilling work ...
DT JJ NN NN

amod

nn

But not for crude price and its clicked sentence.
Still, there is a path: crude← oil← price.

... crude oil price is rousing ...
JJ NN NN VBZ VBG

amod nn dep ccomp

In this case, we create a dependency between
crude and oil in the query and give it a tempo-
rary label dep. We will resolve it to a specific label
later.

crude price
NN NN

dep

Connected via a head noun. In some cases, the
head of a noun compound is missing. Consider
country singers and its clicked sentence:

... singers in country music ...
NNS IN NN NN

prep nn
pobj

Clearly they mean the same thing, but the head
(music) of the noun compound is missing in the
query. Still, a path exists from singers to
country, and we create a dependency:

1790



country singers
NN NN

dep

Connected via a verb. One common case is the
omission of copular verbs. Consider plants
poisonous to goats and its clicked sentence:

... many plants are poisonous to goats .
JJ NNS VBP JJ TO NNS .

amod

nsubj

cop prep
pobj

Here, the missing are does not cause any problem.
But for query pain between breasts and its
clicked sentence:

The pain that appears between the breasts ...
DT NN WDT VBZ IN DT NNS

det nsubj
rcmod

prep det

pobj

we need to introduce a derived edge, and it leads to:

pain between breasts
NN IN NNS

prep pobj

3.2 Inferring a unique dependency tree
A query corresponds to multiple clicked sentences.
From each sentence, we derive a dependency tree.
These dependency trees may not be the same, be-
cause i) dependency parsing for sentences is not per-
fect; ii) queries are ambiguous; or iii) some queries
do not have well-formed clicked sentences.

To choose a unique dependency tree for a query q,
we define a scoring function f to measure the qual-
ity of a dependency tree tq derived from q’s clicked
sentence s:

f(tq, s) =
∑

(x→y)∈tq
−αdist(x, y) + log count(x→ y)

count(x← y)
(1)

where (x→ y) is an edge in the tree tq, count(x→
y) is the occurrence count of the edge x → y in
the entire query dataset, dist(x, y) is the distance
of words x and y on the original sentence parsing
tree, and α is a parameter to adjust the importance
between the two measures (its value is empirically
determined).

The first term of the scoring function measures the
compactness of the query tree. Consider two clicked

Correct Wrong Query Sentence
side← effects side→ effects 1110:1 11257:17
benefits→ of benefits← of 144:63 5228:0
Full←Movie Full→Movie 128:5 1585:27
coconut← oil coconut→ oil 91:10 1507:46
credit← card credit→ card 96:2 4394:60

Table 2: Examples of globally inconsistent head
modifier relations

sentences for query deep learning:

... learning how to deep fry chicken ...

... JJ WRB NN IN NN IN ...

acl

advmod
mark

advmod dobj

... enjoy deep learning ...

... VBP JJ NN ...

dobj
amod

In the first sentence, deep and learning are
indirectly connected through fry so the total dis-
tance measure is 2. In the second query, the distance
is 1. Therefore, query aligned with the second sen-
tence is better than the first sentence.

The second term of the scoring function measures
the global consistency among head modifier direc-
tions. For a word pair (x, y), if in the dataset, the
number of edges x → y dominates the number of
edges x← y, then the latter is likely to be incorrect.

One important thing to note is word order. Word
order may influence the head-modifier relations be-
tween two words. For example, child of and
of child should definitely have different head-
modifier relations. Therefore, we treat two words
of different order as two different word pairs.

Table 2 shows some examples of conflicting de-
pendency edges and their corresponding occurrence
count in queries and sentences.

3.3 Label refinement
In Section 3.1, some dependencies are derived with
a placeholder label dep. Before we use the data
to train a parser, we must resolve dep to a true
label, otherwise they introduce inconsistency in
the training data. For example, consider a sim-
ple query crude price. From clicked sen-
tences that contain crude oil price, we de-

1791



rive crude
dep←−−price, but from those that contain

crude price, we derive crude amod←−−−price.
To resolve dep, we resort to majority vote first.

For any x
dep←−− y, we count the occurrence of x label←−−

y in the training data for each concrete label. If the
frequency of a certain label is dominating by a pre-
determined threshold (10 times more frequent than
any other label), then we resolve dep to that label.

With our training data, the above process is able
to resolve about 90% dependencies. We can simply
discard queries that contain unresolvable dependen-
cies. However, such queries still contain useful in-
formation, for example, the direction of this edge,
and the directions and labels of all the other edges.
We develop a bootstrapping method to preserve such
useful information. First, we train a parser on data
without dep labels. This skips about 10% queries in
our experiments. Second, we use the parser to pre-
dict the unknown label. If the prediction is consis-
tent with the annotation except for the dep label, we
use the predicted label. Third, we add the resolved
queries into the training data and train a final parser.
Experiments show the bootstrapping approach im-
proves the quality of the parser.

4 Dependency Parsing

We train a parser from the web query treebank
data. We also try to incorporate context-free head-
modifier signals into parsing. To make it easier to
incorporate such signals, we adopt a neural network
approach to train our POS tagger and parser.

4.1 Neural network POS tagger and parser

We first train a neural network POS tagger for web
queries. For each word in the sentence, we construct
features out of a fixed context window centered at
that word. The features include the word itself, case
(whether the first letter, any letter, or every letter in
the word, is in uppercase), prefix, and suffix (we rec-
ognize a pre-defined set of prefixes and suffixes, for
the rest we use a special token “UNK”). For the word
feature, we use pre-trained word2vec embeddings.
For word case and prefix/suffix, we use random ini-
tialization for the embeddings. The accuracy of the
trained POS tagger is similar to that of (Ganchev et
al., 2012), which outperforms POS taggers trained
on PTB data.

Buffer features
b1.wt, b2.wt, b3.wt
Stack features
s1.wt, s2.wt, s3.wt
Tree features
lc1(s1).wtl, lc2(s1).wtl, rc1(s1).wtl, rc2(s1).wtl
lc1(lc1(s1)).wtl, rc1(rc1(s1)).wtl
lc1(s2).wtl, lc2(s2).wtl, rc1(s2).wtl, rc2(s2).wtl
lc1(lc1(s2)).wtl, rc1(rc1(s2)).wtl

Table 3: The feature templates. si(i = 1, 2, ...) de-
note the ith top element of the stack, bi(i = 1, 2, ...)
denote the ith element on the buffer, lck(si) and
rck(si) denote the kth leftmost and rightmost chil-
dren of si, w denotes words, t denotes POS tag, l
denotes label.

We use the arc standard transition based depen-
dency parsing system (Nivre, 2004). The architec-
ture of the neural network dependency parser is sim-
ilar to that of (Chen and Manning, 2014) designed
for parsing sentences. The features used in parsing
are shown in Table 3.

4.2 Context free features

In Section 2.2, we discussed context-free signals
P (e|x, y) and context-sensitive signals P (e|x, y, q).
Previous work (Wang et al., 2014) uses context-free
signals for syntactic analysis of a query. Our ap-
proach outperforms the context-free approach.

An interesting question is, will context-free sig-
nals further improve our approach? The rationale is
that although context-sensitive signals P (e|x, y, q)
are more accurate in predicting the dependency be-
tween x and y, such signals are also very sparse. Do
context-free signals P (e|x, y) provide backoff infor-
mation in parsing?

It is not straightforward to include P (e|x, y) in
the neural network model. The head-modifier rela-
tions P (e|x, y) may exist between any pair of tokens
in the input query. Essentially, it is a pairwise graph-
ical model and it is difficult to directly incorporate
the signals in transition based dependency parsing.

We treat context-free signals as prior knowledge.
We train head-modifier embeddings for each to-
ken, and use such embeddings as pre-trained embed-
dings. Specifically, we use an approach similar to
training word2vec embeddings but focusing on head

1792



modifier relationships instead of co-occurrence rela-
tionships. More specifically, we train an one hidden
layer neural network classifier to determine whether
two words have head-modifier relations. The input
of the neural network is the concatenation of the em-
beddings of two words. The output is whether the
two words form a proper head-modifier relationship.
We obtain a large set of head-modifier data from text
corpus by mining “h PREP m” pattern in search log
where h and m are nouns. Then, for each known
head modifier pair h and m, we use (h,m) as pos-
itive example and (m,h) as negative example. For
each word, we also choose a few random words as
negative examples. During the training process, the
gradients are back propagated to the word embed-
dings. After training, the embeddings should con-
tain sufficient information to recover head modifier
relations between any word pairs.

But we did not observe improvement over the ex-
isting neural network that are trained on context sen-
sitive treebank data alone. The head-modifier em-
beddings has about 3% advantage in UAS over ran-
domized embeddings. However, using pretrained
word2vec embeddings, we also achieve 3% advan-
tage. Thus, it seems that context-sensitive signals
plus the generalizing power of embeddings contain
all the context-free signals already.

5 Experiments

In this section, we start with some case studies. Then
we describe data and compare models.

In experiments, we use the standard UAS (unla-
beled attachment score) and LAS (labeled attach-
ment score) score for measuring the quality of de-
pendency parsing. They are calculated as:

UAS =
# correct arc directions

# total arcs
(2)

LAS =
# correct arc directions and labels

# total arcs
(3)

5.1 Case Study

We compare dependency trees produced by our
QueryParser and Stanford Parser (Chen and Man-
ning, 2014) for some web queries (Stanford Parser
is trained from the standard PTB treebank). Table 4
shows that Stanford Parser heavily relies on gram-
mar signals such as function words and word or-

der, while QueryParser relies more on the seman-
tics of the query. For instance, in the 1st exam-
ple, QueryParser identifies toys as the head, re-
gardless of the word order, while Stanford parser
always assumes the last token as the head. In the
2nd example, the semantics of the query is a school
(vanguard school) at a certain location (lake
wales). QueryParser captures the semantics and
correctly identifies school as the head (root) of the
query, while Stanford parser treats the entire query
as a single noun compound (likely inferred from the
POS tags).

5.2 Clicked Sentences

For training data, we use one-month Bing query log
(between July 25, 2015 and August 24, 2015). From
the log, we obtain web query q and its top clicked
URLs {url1, url2, ..., urlm}. From the urls, we re-
trieve the clicked HTML document, and find sen-
tences {s1, s2, ..., sn} that contain all words (regard-
less to their order of occurrence) in q. Then we ex-
tract query-sentence tuples (q, s, count) to serve as
our training data to generate a web query treebank.
The size (# of distinct query-sentence pairs) of the
raw clicked sentences is 390,225,806.

5.3 Web Query Treebank

We evaluate the 3 steps of treebank generation. Af-
ter each step, we sample 100 queries from the result
and manually compute their UAS and LAS scores.
We also count the number of total query instances in
each step. The results are shown in Table 5.

• Inferring a dependency tree: For each (query,
sentence) pair, we project dependency from
the sentence to the query. The number of in-
stances shown in Table 5 are the input num-
ber of (query, sentence) pairs. It shows that
we obtain dependency trees for only 31% of the
queries, while the rest do not satisfy our filter-
ing criterion. This however is not a concern.
By sacrificing recall in this process, we ensure
high precision. Given that query log is large,
precision is more important.

• Inferring a unique dependency tree: In this
step, we group (query, sentence) pairs by
unique queries. Using the method in Section

1793



QueryParser Stanford parser

toys kids
NNS NNS

nn

kids toys
NNS NNS

nn

toys kids
NNS NNS

nn

kids toys
NNS NNS

nn

vanguard school lake wales
NN NN NN NNS

nn
nn

nn

vanguard school lake wales
NN NN NN NNS

nn

nn
nn

pretty little liars season 4 episode 6
RB JJ NNS NN CD NN CD

advmod
nn

nn
num

nn

num

pretty little liars season 4 episode 6
RB JJ NNS NN CD NN CD

advmod

nn
nn

num
nn num

interview questions contract specialist
NN NNS NN NN

nn nn

nn

contract specialist interview question
NN NN NN NN

nn nn nn

interview questions contract specialist
NN NNS NN NN

nn

nn

nn

contract specialist interview question
NN NN NN NN

nn

nn

nn

Table 4: Case study of parsers.

3.2, each group produces one or zero depen-
dency trees. The number of instances in Table
5 corresponds to the number of different query
groups. The overall success rate is high. This
is expected as the filtering process uses major-
ity voting, and we already have high precision
parsing trees after the first step.

• Label refinement: Dependency labels are re-
fined using the methodology in Section 3.3. It
shows that with majority voting and bootstrap-
ing, we are able to keep all the input.

5.4 Parser Performance

We compare QueryParser against three state-of-the-
art parsers: Stanford parser, which is a transition
based dependency parser based on neural network,
MSTParser (McDonald et al., 2005), which is a

graph based dependency parser based on minimum
spanning tree algorithms, and LSTMParser (Dyer et
al., 2015), which is a transition based dependency
parser based on stack long short-term memory cells.
Here, QueryParser is trained from our web query
treebank, while Stanford Parser and MSTParser are
trained from standard PTB treebanks.

For comparison, we manually labeled 1,000 web
queries to serve as a ground truth dataset2. We pro-
duce POS tags for the queries using our neural net-
work POS tagger. To specifically measure the ability
of QueryParser in parsing queries with no explicit
syntax structure, we split the entire dataset All into
two parts: NoFunc and Func, which correspond to
queries without any function word, and queries with
at least one function word. The number of queries

2https://github.com/wishstudio/queryparser

1794



Step Total Instances Produced Instances Success Rate UAS LAS
Inferring a dependency tree 3986300 1229860 31% 0.906 0.851
Inferring a unique tree 716261 680857 95% 0.910 0.851
Label refinement 680857 680857 100% 0.917 0.855

Table 5: Training dataset generation statistics

System
All (n=1000) NoFunc (n=900) Func (n=100)
UAS LAS UAS LAS UAS LAS

Stanford 0.694 0.602 0.670 0.568 0.834 0.799
MSTParser 0.699 0.616 0.683 0.691 0.799 0.766
LSTMParser 0.700 0.608 0.679 0.578 0.827 0.790
QueryParser + label refinement 0.829 0.769 0.824 0.761 0.858 0.818
QueryParser + word2vec 0.843 0.788 0.843 0.784 0.838 0.812
QueryParser + label refinement + word2vec 0.862 0.804 0.858 0.795 0.883 0.854

Table 6: Parsing performance on web queries

of the two datasets are 900 and 100, respectively.
Table 6 shows the results. We use 3 versions

of QueryParser. The first two use random word
embedding for initialization, and the first one does
not use label refinement. From the results, it can
be concluded that QueryParser consistently outper-
formed competitors on query parsing task. Pre-
trained word2vec embeddings improve performance
by 3-5 percent, and the postprocess of label refine-
ment also improves the performance by 1-2 percent.

Table 6 also shows that conventional depencency
parsers trained on sentence dataset relies much more
on the syntactic signals in the input. While Stanford
parser and MSTParser have similar performance to
our parser on Func dataset, the performance drops
significantly on All and NoFunc dataset, when the
majority of input has no function words.

6 Related Work

Some recent work (Ganchev et al., 2012; Barr et al.,
2008) investigated the problem of syntactic analysis
for web queries. However, current study is mostly
at postag rather than dependency tree level. Barr et
al. (2008) showed that applying taggers trained on
traditional corpora on web queries leads to poor re-
sults. Ganchev et al. (2012) propose a simple, ef-
ficient procedure in which part-of-speech tags are
transferred from retrieval-result snippets to queries
at training time. But they do not reveal syntactic
structures of web queries.

More work has focused on resolving simple re-
lations or structures in queries or short texts, par-
ticularly entity-concept relations (Shen et al., 2006;
Wang et al., 2015; Hua et al., 2015), entity-attribute
relations (Pasca and Van Durme, 2007; Lee et al.,
2013), head-modifier relations (Bendersky et al.,
2010; Wang et al., 2014). Such relations are impor-
tant but not enough. The general dependency rela-
tions we focus on is an important addition to query
understanding.

On the other hand, there is extensive work on syn-
tactic analysis of well-formed sentences (De Marn-
effe et al., 2006). Recently, a lot of work (Collobert
et al., 2011; Vinyals et al., 2015; Chen and Manning,
2014; Dyer et al., 2015) started using neural network
for this purpose. In this work, we use similar neural
network architecture for web queries.

7 Conclusion

Syntactic analysis of web queries is extremely im-
portant as it reveals actional signals to many down-
stream applications, including search ranking, ads
matching, etc. In this work, we first acquire well-
formed sentences that contain the semantics of the
query, and then infer the syntax of the query from
the sentences. This essentially creates a treebank for
web queries. We then train a neural network depen-
dency parser from the treebank. Our experiments
show that we achieve significant improvement over
traditional parsers on web queries.

1795



References

Cory Barr, Rosie Jones, and Moira Regelson. 2008. The
linguistic structure of english web-search queries. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ’08, pages
1021–1030, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Michael Bendersky, Donald Metzler, and W Bruce Croft.
2010. Learning concept importance using a weighted
dependence model. In Proceedings of the third ACM
international conference on Web search and data min-
ing, pages 31–40. ACM.

Danqi Chen and Christopher D Manning. 2014. A
fast and accurate dependency parser using neural net-
works. In EMNLP, pages 740–750.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493–
2537.

Marie-Catherine De Marneffe, Bill MacCartney, Christo-
pher D Manning, et al. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, volume 6, pages 449–454.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependeny parsing with stack long short-term
memory. In Proc. ACL.

Kuzman Ganchev, Keith Hall, Ryan McDonald, and Slav
Petrov. 2012. Using search-logs to improve query tag-
ging. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, ACL
’12, pages 238–242, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large corpus
of english books. In Second Joint Conference on Lexi-
cal and Computational Semantics (* SEM), volume 1,
pages 241–247.

Wen Hua, Zhongyuan Wang, Haixun Wang, Kai Zheng,
and Xiaofang Zhou. 2015. Short text understand-
ing through lexical-semantic analysis. In International
Conference on Data Engineering (ICDE).

Taesung Lee, Zhongyuan Wang, Haixun Wang, and
Seung-won Hwang. 2013. Attribute extraction and
scoring: A probabilistic approach. In International
Conference on Data Engineering (ICDE).

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajič. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,

pages 523–530. Association for Computational Lin-
guistics.

Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In Proceedings of the Workshop on
Incremental Parsing: Bringing Engineering and Cog-
nition Together, pages 50–57. Association for Compu-
tational Linguistics.

Marius Pasca and Benjamin Van Durme. 2007. What
you seek is what you get: Extraction of class attributes
from query logs. In IJCAI, volume 7, pages 2832–
2837.

Dou Shen, Jian-Tao Sun, Qiang Yang, and Zheng Chen.
2006. Building bridges for web query classification.
In Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 131–138. ACM.

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Grammar
as a foreign language. In Advances in Neural Informa-
tion Processing Systems, pages 2755–2763.

Zhongyuan Wang, Haixun Wang, and Zhirui Hu. 2014.
Head, modifier, and constraint detection in short texts.
In Data Engineering (ICDE), 2014 IEEE 30th Inter-
national Conference on, pages 280–291. IEEE.

Zhongyuan Wang, Kejun Zhao, Haixun Wang, Xiaofeng
Meng, and Ji-Rong Wen. 2015. Query understand-
ing through knowledge-based conceptualization. In
Proceedings of the Twenty-Fourth International Joint
Conference on Artificial Intelligence (IJCAI).

1796


