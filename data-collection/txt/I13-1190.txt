










































Unsupervised Extraction of Attributes and Their Values from Product Description


International Joint Conference on Natural Language Processing, pages 1339–1347,
Nagoya, Japan, 14-18 October 2013.

Unsupervised Extraction of Attributes and Their Values
from Product Description

Keiji Shinzato Satoshi Sekine
Rakuten Institute of Technology

{keiji.shinzato, satoshi.b.sekine}@mail.rakuten.com

Abstract

This paper describes an unsupervised
method for extracting product attributes
and their values from an e-commerce
product page. Previously, distant supervi-
sion has been applied for this task, but it
is not applicable in domains where no reli-
able knowledge base (KB) is available. In-
stead, the proposed method automatically
creates a KB from tables and itemizations
embedded in the product’s pages. This KB
is applied to annotate the pages automati-
cally and the annotated corpus is used to
train a model for the extraction. Because
of the incompleteness of the KB, the anno-
tated corpus is not as accurate as a manu-
ally annotated one. Our method tries to fil-
ter out sentences that are likely to include
problematic annotations based on statisti-
cal measures and morpheme patterns in-
duced from the entries in the KB. The
experimental results show that the perfor-
mance of our method achieves an average
F score of approximately 58.2 points and
that filters can improve the performance.

1 Introduction

E-commerce enables consumers to purchase a
large number of products from a variety of cat-
egories such as books, electronics, clothing and
foods. In recent years, this market has grown
rapidly around the world. Online shopping is re-
garded as a very important part of our daily lives.

Structured product data are most important for
online shopping malls. In particular, product at-
tributes and their values (PAVs) are crucial for
many applications such as faceted navigation and
recommendation. However, since structured infor-
mation is not always provided by the merchants,
it is important to build technologies to create this

structured information (such as PAVs) from un-
structured data (such as a product description).
Because of the wide variety of product types, such
technology should not rely on a manually anno-
tated corpus. One of the promising methods for
information extraction (IE) without a manually
annotated corpus is distant supervision (Mintz et
al., 2009), which leverages an existing knowledge
base (KB) such as Wikipedia or Freebase to an-
notate texts using the KB instances. These popu-
lar KBs, however, are not very helpful for distant
supervision in an e-commerce domain for the fol-
lowing reasons. (1) An infobox in a Wikipedia ar-
ticle is not always tailored towards e-commerce.
For instance, as of May 2013, the infobox at-
tributes of wine in English Wikipedia included
energy, carbohydrates, fat, protein and alcohol1.
These are not particularly useful for users seek-
ing their favorite wines through online shopping.
Instead, the grape variety, production area, and
vintage of the wine would be of greater interest.
(2) On the other hand, Freebase contains PAVs
for limited types of products such as digital cam-
eras2. However, since Freebase is currently only
available in English, we cannot use Freebase in
a distant supervision method for other languages.
Moreover, the number of categories whose PAVs
are available in Freebase is limited even in En-
glish.

In this paper, we propose a technique to extract
PAVs using an automatically induced KB. For the
induction, the method uses structured data such as
tables and itemizations embedded in the unstruc-
tured data. An annotated corpus is then automat-
ically constructed from the KB and unstructured
data, i.e. product pages. Since these texts are in
HTML format, we can extract the attribute can-
didates and their values using pattern matching
with tags and symbols. We can expect the KB to

1http://en.wikipedia.org/wiki/Wine
2http://www.freebase.com/view/digicams/digital camera

1339



have a certain level of accuracy because the tables
and itemizations are created by merchants who are
product experts. However, there may be a need to
group synonymous attribute names. We propose a
method for grouping synonymous attribute names
by observing the commonality of the attribute val-
ues and their co-occurrence statistics. The model
for extracting attribute values is then trained us-
ing the annotated corpus to find PAVs of the prod-
ucts. Because the KB is incomplete, the annotated
corpus may contain annotation mistakes; false-
positive and false-negative. Thus, our method tries
to filter out sentences that are likely to include
those problematic annotations based on statistical
measures and morpheme patterns induced from
the entries in the KB.

The contributions of our work are as follows:

1. Unsupervised and scalable methods for in-
ducing a KB consisting of PAVs, and for dis-
covering attribute synonyms.

2. Unsupervised method for improving the
quality of an automatically annotated cor-
pus by discarding false-positive and false-
negative annotations. These problematic an-
notations are always included in automati-
cally annotated corpora although such cor-
pora can be constructed without requiring ex-
pensive human effort.

3. Comprehensive evaluation of each compo-
nent: Automatically induced KBs, annota-
tion methodology, and the performance of
IE models based on the automatically con-
structed corpora. In particular, the annotation
methodology and the IE models are evaluated
by using a dataset comprising 1,776 manually
annotated product pages gathered from eight
product categories.

To summarize, as far as we know, this is a first
work to extract product attributes and their values
relying solely on product data, and completing all
the steps of this task, including KB induction, in a
purely unsupervised manner.

2 Related Work

2.1 Product Information Extraction
Bing et al. (2012) proposed an unsupervised
methodology for extracting product attribute-
values from product pages. Their method first gen-
erates word classes from product review data re-

lated to a category using Latent Dirichlet Alloca-
tion (LDA) (Blei et al., 2003). The method then
automatically constructs training data by match-
ing words in classes and those in product pages
for the category. After that, extraction models are
built using the training data. Their method does
not deal with attribute synonymy and problematic
annotations in their training data. They evaluated
the result of their extraction model only, while this
paper reports on evaluation results of not only ex-
traction models but also induced KBs and annota-
tion methodology. In addition, their method em-
ploys LDA for generating word classes. This may
involve the issue of scalability when running the
method on large size real-world data. On the other
hand, we employ a simple rule-based approach to
induce the KBs. We can then straightforwardly
apply the method on the large-scale data.

Mauge et al. (2012) also proposed methods to
extract product attribute-values using hand-crafted
patterns, and to group synonymous attributes in a
supervised manner. They, however, only evaluated
a part of the extracted attribute names, and aggre-
gated synonymous attribute names. They did not
evaluate the extracted attribute values.

Our work is also similar to Ghani et al. (2006),
who construct an annotated corpus using a manu-
ally tailored KB and then train models using the
corpus to extract attribute values. Probst et al.
(2007) and Putthividhya and Hu (2011) also pro-
posed a similar approach with the work of Ghani.
The main difference between these works and ours
is that our method does not require manually tai-
lored KBs. Instead, our method automatically in-
duces a KB of PAVs from structured data embed-
ded in product pages.

In addition to the above, many wrapper based
approaches have been proposed (Wong et al.,
2008; Dalvi et al., 2009; Gulhane et al., 2010;
Ferrez et al., 2013). The goal of these ap-
proaches is to extract information from documents
semi-structured by any mark up language such as
HTML. On the other hand, our method aims at ex-
tracting (product) information from full texts al-
though the method leverages semi-structured doc-
uments to induce KBs.

2.2 Knowledge Base Induction

There are many works for automatically induc-
ing KBs using syntactic parsing results (Rooth
et al., 1999; Pantel and Lin, 2002; Torisawa,
2001; Kazama and Torisawa, 2008), and semi-

1340



��� �� � � �� 	
 ��� �
� � 	� � � � � �

� � �� � � � � � 
�� ��
� � �  � � � � � � � �

� � �� � � � � � � � �� � 	 � �
�� � � � � � 


�� � � � 	� � ��� 
 �
� � � � � �� � � � 


� !" #$ � � � 	� � �% � 
 �  � � � & � �� �� 
 � �' C

(� � � �� �� 	
% � 
 � 

�� � � � � 	
 �
)� � * � � +

,attr1 - value1 .,attr2 - value2 .,attr1 - value3 ./
� � 0 � 	 � �

123 45 678 9 7 : 4 ;< 7= 5 >8 ?< > @ 7 = 12 3 4 5 > A 2 >B 4 B 2 = > > 7 5 >8 ?< > ?8 4

C " # ! " #$
� � � � � �� � � 	 	 � ��

123 4 @= < ; ? : @= 3 >2 D ; 45
7 8 @ > 4 9 @E 2 > @ 7= 5 F= 5 >8 ?< > ?8 4 :G 23 4

Figure 1: Overview of our approach.

structured data (Shinzato and Torisawa, 2004;
Wong et al., 2008; Yoshinaga and Torisawa,
2006; Mauge et al., 2012). As clues for
the induction of attribute-value KBs, previous
works employ lexico-syntactic patterns such as
<attribute>:<value> and <value> of <attribute>,
and layout patterns in semi-structured data such as
tables and itemizations marked up by HTML tags.

We employ similar clues as Yoshinaga and Tori-
sawa (2006) for KB induction. In addition to the
induction, our method tries to find synonymous at-
tributes in the induced KB. This is the difference
between Yoshinaga’s work and ours. The method
of Mauge et al. (2012) also finds attribute syn-
onyms, but it requires supervision to find these.

3 Data

We used Rakuten’s product pages comprising over
100 million products in 40,000 categories3. Each
product is assigned to one category by merchants
offering it on the Rakuten site. In contrast to
Amazon, Rakuten’s product pages are not well-
structured, because the product pages are designed
by the merchants. Although some pages include
tables for describing product information, major-
ity of pages describe product information in full
texts. That’s why we need to extract product in-
formation from the texts.

4 Approach

An overview of our approach is shown in Figure 1.
The input for the approach is a set of product pages
belonging to a single category like wine or sham-
poo. From the given pages, a value extraction
model for the category is generated, and then the

3http://www.rakuten.co.jp/

保存方法 (method of preservation), その他 (etc),商品説
明 (explanation), 広告文責 (responsibility for advertise-
ment),特徴 (characteristics),仕様 (specs.)

Figure 2: List of stop words for attributes.

P1: <T(H|D)>[ATTR]</T(H|D)><TD>[ANY]</TD>
P2: [P][ATTR][S][ANY][P]
P3: [P][ATTR][ANY][P]
P4: [ATTR][S][ANY][ATTR][S]

Figure 3: Patterns for extracting attribute values.
The [ATTR] tag denotes a collected attribute, the
[ANY] tag denotes a string, the [P] tag denotes
the prefix and open-braces, and the [S] tag de-
notes the suffix and close-braces. The prefix, suf-
fix and brace are defined in (Yoshinaga and Tori-
sawa, 2006). Attributes of HTML tags are re-
moved before running P1.

model is used to structure unstructured pages in
the category.

A detailed explanation of steps 1, 2 and 3 in Fig-
ure 1 is given in the remaining sections. Expla-
nation of step 4 is omitted because the extraction
model is simply applied to a given page.

4.1 Knowledge Base Induction

4.1.1 Extraction of Attribute-Values
A KB is induced from the tables and itemizations
embedded in the given product pages. We first
collected product attributes based on the assump-
tion that expressions that are often located in ta-
ble headers can be considered as attributes. The
regular expression <TH.*?>.+?</TH> is run on
the given pages, and expressions enclosed by the
tags are collected as attributes. (The <TH> tag is
used for a table header.) The collected candidate
set includes expressions that can not be regarded
as attributes. We excluded these using a small set
of stop words given in Figure 2.

To extract values corresponding to the collected
attributes, the regular expressions listed in Fig-
ure 3 are run on the given product pages. An ex-
pression that matches the position of [ANY] is ex-
tracted as a value of the attribute corresponding
to [ATTR]. The first appearance of [ATTR] is se-
lected as the attribute in P4. The extracted value
and its attribute are stored in the KB along with
the number of merchants that use these in the ta-
ble and itemization data. Henceforth, we refer to
this number as the merchant frequency (MF).

4.1.2 Attribute Synonym Discovery
The KBs constructed in the previous section still
contains numerous synonyms; that is, attributes

1341



with the same meaning, but different spellings, are
included. This is because merchants do not have
a standard method for describing products on their
own pages. For example, “Bordeaux” and “Tus-
cany” can be regarded as production areas of wine.
However, some merchants refer to “Bordeaux” as
the production area, while others consider “Tus-
cany” to be the region. If we use KBs that include
the incoherence as an annotation resource, cor-
pora containing annotation incoherence are con-
structed. To avoid this problem, we set out to iden-
tify synonymous attributes in the KBs.

We attempt to discover attribute synonyms ac-
cording to the assumption that attributes can be
seen as synonyms of one another if they have never
been included in the same structured data, and
they share an identical popular value. We re-
gard the MF of a value as a measure of popu-
larity. Based on this assumption, for all com-
binations of attribute pairs with an identical at-
tribute value whose MF exceeds N , we verify
whether they appear simultaneously in structured
data (i.e., table and itemization data). Attribute
pairs satisfying the condition are regarded as syn-
onyms. The value of N is defined by the equation
N = max(2,MS/100) where MS is the number
of merchants providing structured data in the cat-
egory. As a result, we obtain a vector of attributes
that can be regarded as synonyms, such as (re-
gion, country), and (grape, grape variety) for
the wine category. We refer to this set of synonym
vectors as Sattr .

Next, synonym vectors with high similarity are
iteratively aggregated. For example, the vector
(region, country, location) is generated from the
vectors (region, country) and (location, coun-
try). The similarity between vectors va and vb is
computed by the cosine measure: sim(va, vb) =
va · vb/|va||vb|. We iteratively aggregate the two
vectors with the highest similarity in set Sattr .
The aggregation process continues until the high-
est similarity is below the threshold thsim, which
we set to 0.5. The threshold value was determined
empirically.

Table 1 shows an example of the KB for the
wine category. We can see that the “variety” at-
tribute includes three Japanese variants.

4.2 Training Data Construction

An annotated corpus for training a model for at-
tribute value extraction is constructed from the
given product pages and the KB built from those

Table 1: Example of the KB for the wine category.
The top three attributes are listed with their top
four values. ([number] denotes the MF of a value.)
Attribute 容量 品種 タイプ

(volume) (variety) (type)
Attribute 内容量 ぶどう品種 —
synonyms (content) (grape variety)

ブドウ品種
(grape variety)
使用品種
(usage variety)

Attribute 750ML[147] シャルドネ [59] 辛口 [34]
values 720ML[64] (Chardonnay) (dry)

375ML[49] メルロー [36] 赤 [24]
500ML[41] (Merlot) (red)

シラー [29] 白 [23]
(Syrah) (white)
リースリング [29] フルボディ[23]
(Riesling) (full body)

# values 159 1,578 153

pages. Values in the KB are simply annotated for
the pages. Then, sentences possibly including in-
correct annotations or sentences where annotation
are missing are automatically filtered out from the
annotated pages to improve annotation quality.

4.2.1 Attribute Value Annotation
All given product pages are split into sentences
following block-type HTML tags, punctuation,
and brackets. Each sentence is then tokenized by
a morphological analyzer4. The longest attribute
value matching a sub-sequence of the token se-
quence is annotated. We employed the Start/End
tag model (Sekine et al., 1998) as chunk tags for
the matched sequence. If the matched value corre-
sponds to more than one attribute, the entry with
the largest MF is selected for annotation. Note
that if other attribute values are contained in the
matched sub-sequence, they are not annotated.

4.2.2 Incorrect Annotation Filtering
Some attribute values with low MFs are likely to
be incorrect. The quality of the corpus, and the
performance of extraction models based on the
corpus deteriorate if such values are frequently an-
notated. We detect incorrect value annotation in
the corpus according to the assumption that at-
tribute values with low MFs in structured data
(i.e., tables and itemizations) and high MFs in
unstructured data (i.e., product descriptions) are
likely to be incorrect. Thus, we designed the fol-
lowing score:

Score(v) =
MFD(v)/NM
MFS(v)/MS

4We used the Japanese morphological analyzer MeCab.
(http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html)

1342



T: Chateau Talbot is a famous winery in France .
A: O O O O O O O S-PA O
G: B-PR E-PR O O O O O S-PA O

Figure 4: Example of a sentence with missing an-
notations. The row T shows the tokens of the sen-
tence, the row A shows automatic annotations, and
the row G shows golden annotations. The ‘PR’
and ‘PA’ are abbreviations of Producer and Pro-
duction Area, respectively. Label ‘O’ means that a
token is not annotated with any label.

[NUMBER] [ML (milliliter)]
[シャトー (Chateau)] [ANY]
[LOCATION] [ANY] [LOCATION] [市 (city)]

Figure 5: Examples of morpheme patterns induced
from the KB for the wine category. The token
[ANY] matches a sequence consisting of 1 - 3 arbi-
trary tokens. Tokens [LOCATION] and [NUMBER]
are matched tokens whose part-of-speech is loca-
tion and number, respectively.

where MFD(v) and MFS(v) are the MFs of value
v in the product descriptions and structured data,
respectively, and NM is the number of merchants
offering products in the category, and MS is the
number of merchants providing structured data in
the category. The scoring function is designed so
that values with a low MF for structured data and a
high MF for item descriptions obtain high scores.
We regard attribute values with scores greater than
30 as incorrect, and remove sentences that include
annotation based on incorrect values from corpora.
The threshold value was decided empirically.

4.2.3 Missing Annotation Filtering
Because of the small coverage of the KB, sen-
tences with missing annotations are contained in
the corpus. For example, although the tokens
Chateau and Talbot in Figure 4 should be an-
notated as B-PRODUCER and E-PRODUCER5 re-
spectively, they are not annotated. These missing
annotations result in reduced performance (espe-
cially recall) of models based on the corpus since
they are considered to be other examples when
training the models. One of the possible way to re-
duce the influence of the missing annotations is to
discard sentences with missing annotations. Thus,
we removed such sentences from the corpus.

To detect missing annotations, we generate
morpheme patterns from values in the KB. Ex-

5The beginning and the end of a value for the attribute
called “Producer.”

Table 2: Features for training extraction models.
Basic features (BFs)
Feature Description
Token Surface form of the token.
Base Base form of the token.
PoS Part-of-speech tag of the token.
Char. types Types of characters contained in the token.

We defined Hiragana, Katakana, Kanji,
Alphabet, Number, and Other.

Prefix Double character prefix of the token.
Suffix Double character suffix of the token.
Context features
Feature Description
Context BFs of ±3 tokens surrounding the token.

amples of the patterns for the wine category are
given in Figure 5. First, attribute values are tok-
enized using a morphological analyzer, and then
the PrefixSpan algorithm (Pei et al., 2001) is exe-
cuted on the tokenized result to induce morpheme
patterns6. We employed patterns that do not start
and end with the [ANY] token, and which match
attribute values in the KB and the total number of
merchants corresponding to the matched values is
greater than one.

4.3 Extraction Model Training
Models for attribute value extraction are trained
using the corpus. We employed Conditional Ran-
dom Fields (CRFs), and used CRFsuite7 with
default parameter settings as the implementation
thereof. The features we used are listed in Table 2.

We built a single model for each category. In
other words, we did not build a separate model for
each attribute of each category.

5 Experiments

This section reports the experimental results. We
carried out the experiments on eight categories,
and evaluated them using the dataset discussed in
Section 5.1. An evaluation was conducted for each
component, that is, evaluation of the KB (Sec-
tion 5.2), evaluation of the automatically anno-
tated corpora (Section 5.3), and evaluation of the
extraction models (Section 5.4).

5.1 Construction of Evaluation Dataset
We created an evaluation dataset comprising 1,776
product pages gathered from the eight categories
in Table 3. In constructing the dataset, for each
category, we first listed top 300 merchants accord-
ing to the number of products offered by the mer-

6We used PrefixSpan-rel as the implementation of
PrefixSpan.(http://prefixspan-rel.sourceforge.jp/)

7http://www.chokkan.org/software/crfsuite/

1343



Table 3: Categories and their selected attributes. The symbol # denotes incorrect attributes. The symbol
∗ is attached to the attributes that are not aggregated into their synonyms. For example, “country of
origin” for wine is marked because it is not aggregated with “production area”.

Category Attributes (Each attribute is represented by one of its synonyms.)
Wine 容量 (volume),品種 (grape variety),タイプ (type),産地 (production area),アルコール度数 (alcohol),原

産国 (country of origin)∗,生産者 (producer),原材料 (material)
T-shirt(men) サイズ (size),素材 (material),色 (color),着丈 (length)∗,身幅 (body width)∗, M(M size)#,肩幅 (shoulder

width)∗, L(L size)#

Printer ink 容量 (volume),サイズ (size),カラー (color),重量 (weight),色 (color)∗,適応機種 (compatible model),材
質 (material),製造国 (production area)

Shampoo 容量 (volume), メーカー (manufacturer), 製造国 (production area), 成分 (constituent), 商品名 (product
name),区分 (category),サイズ (size),重量 (weight)

Golf ball コア (core),サイズ (size),カバー (cover),材質 (material),重さ (weight),原産国 (country of origin),ディ
ンブル形状 (shape of dimple),色 (color)

Video game サイズ (size),重さ (weight),材質 (material),付属品 (accessory),製造国 (production country),色 (color),
対応機種 (compatible model),ケーブル長 (length of cable)

Car spotlight サイズ (size),色温度 (color temperature),色 (color),材質 (material),重量 (weight),適合車種 (compatible
model),製造国 (production country),品番 (part number)

Cat food 内容量 (volume), 原産国 (production country), 粗繊維 (crude fiber), 粗脂肪 (crude fat), 粗灰分 (crude
ash),水分 (wet),粗タンパク質 (crude protein),重量 (weight)∗

Table 4: Statistics of the corpora. p#, s#, and v#

denote the number of annotated pages, the number
of annotated sentences, and the number of values,
respectively.
Category Test data Training data

(manually annotated) (automatically annotated)
p# s# v# p# s# v#

Wine 282 1,863 3,040 25,358 28,952 48,645
T-shirt(men) 259 2,580 5,534 14,978 18,018 41,954
Printer ink 273 1,230 4,029 8,473 13,562 42,969
Shampoo 233 1,518 4,352 18,669 30,263 53,294
Golf ball 160 555 719 1,114 2,109 2,760
Video game 212 807 1,088 19,292 29,356 35,230
Car spotlight 271 1,401 2,282 8,124 12,910 18,937
Cat food 86 276 452 4,915 7,375 8,843
Total 1,776 10,230 19,496 100,923 142,545 252,632

chant in the category. Then, we randomly picked
one from the product pages of each merchant. We
extracted titles and sentences from the pages based
on HTML tags. These texts were passed to the an-
notation process.

In selecting the attributes to be used for anno-
tation, we selected the top eight attributes in each
category according to the MFs of the attributes.
Then, we manually discarded incorrect attributes
and aggregated synonymous attributes that were
not automatically discovered. These attributes are
marked up with the symbols # and ∗ in Table 3,
and are not considered in the evaluation in Sec-
tions 5.3 and 5.4.

An annotator was asked to annotate expressions
in the text data, which could be considered as val-
ues of the selected attributes. The annotator was
also asked to discard pages that offered multiple
products and miscategorized products.

In this way, the evaluation dataset was con-
structed by one annotator. After the construction,

we checked the accuracy of the annotation. We
picked up 400 annotated pages, and then checked
them by another annotator whether annotations in
the pages were correct. The agreement of an-
notations between the two annotators was about
88.4%. Statistics of the dataset are given in the
Test data column in Table 4.

5.2 Evaluation of Knowledge Base
5.2.1 Evaluation of Extracted Attributes
We checked whether attributes extracted using our
method could be regarded as correct. We asked
two subjects to judge 411 expressions, all ex-
tracted attributes for the eight categories. The ratio
of attributes that were judged as correct by both
annotators was 0.776. The kappa statistics be-
tween the annotators was 0.581. This value is de-
fined as moderate agreement in (Landis and Koch,
1977). Majority of the attributes judged as incor-
rect were extracted from complex tables and tables
on miscategorized pages.

5.2.2 Evaluation of Attribute Synonyms
Next, we assessed the performance of our syn-
onym discovery. We asked the subjects to ag-
gregate synonymous attributes in KBs, and then
computed purity and inverse purity scores (Artiles
et al., 2007) using the data. We discarded at-
tributes judged as incorrect when computing these
scores. We computed macro-averaged scores for
each subject, and then averaged them.

As a result, the averaged purity and inverse pu-
rity were 0.920 and 0.813, respectively. The pu-
rity score is close to perfect, which means that
the merged expressions are mostly regarded as
synonyms. On the other hand, the score for in-

1344



Table 5: Accuracy of KBs. # shows the total num-
ber of KB entries with each MF.
MF of Wine Shampoo
pairs # Acc. [%] # Acc. [%]
≥ 1 (All) 3,940 75.3 (301/400) 6,798 67.5 (270/400)
≥ 2 751 97.2 (69/71) 2,307 90.8 (118/130)
≥ 3 384 97.1 (33/34) 1,543 97.3 (73/75)
≥ 5 215 95.5 (21/22) 931 98.1 (52/53)

Table 6: Accuracy of our annotation method.
Annotation method Prec. (%) Recall (%) F1 score
(1) Naive annotation 47.46 45.48 46.19
(2) (1) + incorrect 51.39 39.14 43.00
(3) (2) + missing 57.14 29.29 37.21

verse purity is less than 0.82. Improvement of the
methodology in terms of coverage is left for future
work.

5.2.3 Evaluation of Attribute Values
We evaluated the quality of the KBs for the wine
and shampoo categories only, because the evalua-
tion for all categories requires enormous human
effort. We randomly selected 400 values of at-
tributes listed in Table 3, and then asked the sub-
jects to judge whether the values could be regarded
as correct for the attributes. To judge the pair
<attr., value> in the KB for category C, we au-
tomatically generated the sentence: “value is an
expression that represents (attr. or S1attr or ... or
Snattr) of C.” Here, S

n
attr denotes the nth synonym

of attr. The subjects judged a pair to be correct
if the sentence generated from the pair was natu-
rally acceptable. For example, the pair <variety,
onion> in the KB for the wine category is deemed
incorrect because the sentence “onion is an expres-
sion that represents (variety or grape variety or us-
age variety) of wine.” is not acceptable.

The evaluation results are given in Table 5. The
kappa statistics between the annotators were 0.632
for the wine category, and 0.678 for the sham-
poo category, respectively. These values indicate
good agreement. We regarded a pair as correct
if the pair was judged as correct by both annota-
tors. We can see that the accuracy of each cate-
gory is promising. In particular, the accuracy of
pairs with MF greater than one is 90% or more.
This means that merchant frequency plays a cru-
cial role in constructing KBs from structured data
that are embedded in product pages by different
merchants.

5.3 Evaluation of the Annotated Corpora

We also checked the effectiveness of the proposed
annotation method by annotating the same product

KB match: Naive KB matching for corpora (same as (3)
in Table 6).
Model w/o filters: Training models based on corpora
naively annotated using KBs. That is, filters for incorrect
and missing annotations are not applied.
Model with incorrect annotation filter (Model with in-
correct only): Training models based on corpora where
only the filter for incorrect annotations is applied.
Model with missing annotation filter (Model with miss
only): Training models based on corpora where only the
filter for missing annotations is applied.

Figure 6: Alternative methods.

Table 7: Micro-averaged precision, recall and F
score of the models for proposed method and al-
ternatives.
Method Prec.(%) Recall (%) F1 score
Supervised Model 88.28 58.15 68.66
KB match 57.14 29.29 37.21
Model w/o filters 52.60 54.49 53.14
Model with incorrect only 60.46 54.23 56.84
Model with miss only 50.47 59.71 54.43
Model for proposed method 57.05 59.66 58.15

pages as those in the evaluation dataset, and then
checking overlaps between the manual and auto-
matic annotations. The experimental results are
given in Table 6. We judged an extracted value to
be correct if the value exactly matched the manu-
ally annotated one. The results are given as micro-
averaged precision, recall, and F1 scores for each
attribute in each category. We can see that the pro-
posed filtering methods improve the precision (an-
notation quality) at the expense of recall.

5.4 Evaluation of Extraction Model

We compared the performance of the extraction
models trained for each category with the alter-
native methods shown in Figure 6. We naively
matched entries in the KB for unlabeled product
pages, and then randomly picked 100,000 unique
sentences from the annotated pages. We refer to
the picked sentences as the Raw Corpus (RC).
Then, we ran the filters and training process on the
RC since we were limited by the RAM required by
CRFsuite. Some statistics of the corpora after ap-
plying the filters are shown in the column Training
data in Table 4.

The evaluation results are shown in Table 7.
Model w/o filters outperformed KB match by as
much as 15.9 points in F1 score. These improve-
ments are caused by improving the recall of the
method. This shows that contexts surrounding a
value and patterns of tokens in a value are suc-
cessfully captured. Model with incorrect only
also achieved higher performance than Model w/o

1345



Table 8: Types of errors.
Type # err.
Automatic Context dependent role 15
annotation Part of a compound word 12

Polysemous word 9
Incorrect KB entry 23
Over generation by learned patterns 15
Extraction from unrelated regions 12
Others 14

filters. Especially, the precision of the extrac-
tion models is improved by 7.9%. This means
that the incorrect annotation filter successfully re-
moved annotation based on incorrect KB entries
from the annotated corpora. In addition, Model
with miss only achieved higher performance than
Model w/o filters. In particular, the recall of
the method improved by 5.2%. This shows that
the missing annotation filter effectively works for
precisely training extraction models. As a result,
the precision and recall of the proposed method
are enhanced by employing both filters simultane-
ously, and the method achieved 58.15 points in F1
scores.

By comparison, the performance of the extrac-
tion models based on manually annotated corpora
is shown in Table 7. The supervised method was
evaluated with 10-fold cross validation. From the
table, we can see that the recall of our method out-
performs that of the supervised method while the
precision and F1 score of our method are lower
those of the supervised method.

6 Discussion

For the wine and shampoo categories, we ran-
domly picked up 50 attribute values that were
judged as incorrect. Then we classified them ac-
cording to their error types. The classification re-
sult is shown in Table 8. Ratios of errors based
on automatic annotation, and incorrect KB enti-
ties, over generation by learned patterns, and ex-
traction from unrelated region with products are
36%, 23%, 15% and 12%, respectively.

The errors stemming from automatic annota-
tion can be classified into three sub-types. Errors
that require understanding of the context when
annotating attribute values are the most common
sub-type. For example, in the wine domain, the
attribute-value pair <Production area, Bordeaux>
was extracted from the following sentence:

• 土壌が ボルドー<Productionarea> のポムロールと非
常に似ている。
(Soil is very similar with ones in Pomerol region of
Bordeaux<Productionarea>.)

Although the extracted pair can be regarded as a
correct KB entity for the wine categories, it is not
production areas of wine in the above sentence.
This type of error can be reduced if we can suc-
cessfully leverage the above sentence as a negative
example in the model training step. To generate
such negative examples is future work.

The second type of errors left for future work
occurs in annotation of compound words. For
example, automatically annotated corpora for the
shampoo category has the following sentence:

• ヒアルロン酸<Constituent> 以上の保水力がある。
(It has a higher water-holding ability than
hyaluronan<Constituent> has.)

This sort of annotation errors may decrease if we
omit the annotation of parts of compound words.

The third type of errors is annotation based
on polysemous words. For instance, although
<Alcohol, 10%> for the wine category is a correct
KB entry, the word “10%” is used for describing
various types of ratios. The following sentence is
one of the examples where the word 10% is used
with a meaning other than alcohol content in the
wine domain:

• 輸出は全体の 10 %<Alcohol> 程度。
(The amount of exports is approximately
10 %<Alcohol> of the total.)

A wrong extraction model for the alcohol attribute
is trained through the above sentence. Disam-
biguation of attribute values is required in the an-
notation step in order to train precise models. On
the task of extracting person names, a method for
disambiguation of names is proposed by Bollegala
et al. (2012). To employ similar disambiguation
methodology is one of our plans for future work.

7 Conclusion

We proposed a purely unsupervised methodology
for extracting attributes and their values from e-
commerce product pages. We showed that the
performance of our method attained an average F
score of approximately 58.2 points using manually
annotated corpora.

We believe the most important task for future
work is to improve annotation quality. Disam-
biguation of attribute values and construction of
wide coverage KBs are crucial to boost the quality.
Another important future task concerns synonymy.
We only tackled attribute synonymy. Discovery of
attribute value synonyms is also an important fu-
ture direction.

1346



References
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.

2007. The semeval-2007 weps evaluation: Estab-
lishing a benchmark for the web people search task.
In Proceedings of the Fourth International Work-
shop on Semantic Evaluations, pages 64–69.

Lidong Bing, Tak-Lam Wong, and Wai Lam. 2012.
Unsupervised extraction of popular product at-
tributes from web sites. In Proceedings of the
Eighth Asia Information Retrieval Societies Confer-
ence, pages 437–446.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.

Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2012. Automatic annotation of ambigu-
ous personal names on the web. Computational In-
telligence, 28(3):398–425.

Nilesh N. Dalvi, Philip Bohannon, and Fei Sha. 2009.
Robust web extraction: an approach based on a
probabilistic tree-edit model. In Proceedings of the
2009 ACM International Conference on Manage-
ment of Data, pages 335–348.

Remi Ferrez, Clement Groc, and Javier Couto. 2013.
Mining product features from the web: A self-
supervised approach. In Web Information Systems
and Technologies, volume 140 of Lecture Notes in
Business Information Processing, pages 296–311.

Rayid Ghani, Katharina Probst, Yan Liu, Marko
Krema, and Andrew Fano. 2006. Text mining for
product attribute extraction. ACM SIGKDD Explo-
rations Newsletter, 8(1):41–48.

Pankaj Gulhane, Rajeev Rastogi, Srinivasan H. Sen-
gamedu, and Ashwin Tengli. 2010. Exploiting con-
tent redundancy for web information extraction. In
Proceedings of the 19th International World Wide
Web Conference, pages 1105–1106.

Jun’ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 407–415.

J. Richard Landis and Gary. G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159–174.

Karin Mauge, Khash Rohanimanesh, and Jean-David
Ruvini. 2012. Structuring e-commerce inventory.
In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 805–
814.

Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the

Joint Conference of the 47th Annual Meeting of the
ACL and the Fourth International Joint Conference
on Natural Language Processing of the AFNLP,
pages 1003–1011.

Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the Eighth
ACM International Conference on Knowledge Dis-
covery and Data Mining, pages 577–583.

Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, Helen
Pinto, Qiming Chen, Umeshwar Dayal, and Mei-
Chun Hsu. 2001. Prefixspan: Mining sequen-
tial patterns efficiently by prefix-projected pattern
growth. In Proceedings of the 17th IEEE Interna-
tional Conference of Data Engineering, pages 215–
224.

Katharina Probst, Rayid Ghani, Marko Krema, Andrew
Fano, and Yan Liu. 2007. Semi-supervised learning
of attribute-value pairs from product descriptions. In
Proceedings of the 20th International Joint Confer-
ence in Artificial Intelligence, pages 2838–2843.

Duangmanee Putthividhya and Junling Hu. 2011.
Bootstrapped named entity recognition for product
attribute extraction. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1557–1567.

Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
Proceedings of the 37th annual meeting of the Asso-
ciation for Computational Linguistics on Computa-
tional Linguistics, pages 104–111.

Satoshi Sekine, Ralph Grishman, and Hiroyuki Shin-
nou. 1998. A decision tree method for finding and
classifying names in japanese texts. In In Proceed-
ings of the Sixth Workshop on Very Large Corpora.

Keiji Shinzato and Kentaro Torisawa. 2004. Acquiring
hyponymy relations from web documents. In Pro-
ceedings of Human Language Technology Confer-
ence/North American chapter of the Association for
Computational Linguistics annual meeting, pages
73–80.

Kentaro Torisawa. 2001. An unsupervised method for
canonicalization of japanese postpositions. In Pro-
ceedings of the Sixth Natural Language Processing
Pacific Rim Symposium, pages 211–218.

Tak-Lam Wong, Wai Lam, and Tik-Shun Wong. 2008.
An unsupervised framework for extracting and nor-
malizing product attributes from multiple web sites.
In Proceedings of the 31st ACM SIGIR Conference,
pages 35–42.

Naoki Yoshinaga and Kentaro Torisawa. 2006. Find-
ing specification pages according to attributes. In
Proceedings of the 15th International World Wide
Web Conference, pages 1021–1022.

1347


