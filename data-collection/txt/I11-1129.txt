















































Sentence Subjectivity Detection with Weakly-Supervised Learning


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1153–1161,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Sentence Subjectivity Detection with Weakly-Supervised Learning

Chenghua Lin1,2, Yulan He2 and Richard Everson1

1Department of Computer Science, University of Exeter, Exeter, EX4 4QF, UK
2Knowledge Media Institute, The Open University, Milton Keynes, MK7 6AA, UK

{C.Lin,Y.He}@open.ac.uk, R.M.Everson@exeter.ac.uk

Abstract
This paper presents a hierarchical
Bayesian model based on latent Dirichlet
allocation (LDA), called subjLDA, for
sentence-level subjectivity detection,
which automatically identifies whether
a given sentence expresses opinion or
states facts. In contrast to most of
the existing methods relying on either
labelled corpora for classifier train-
ing or linguistic pattern extraction for
subjectivity classification, we view the
problem as weakly-supervised generative
model learning, where the only input
to the model is a small set of domain
independent subjectivity lexical clues.
A mechanism is introduced to incor-
porate the prior information about the
subjectivity lexical clues into model
learning by modifying the Dirichlet
priors of topic-word distributions. The
subjLDA model has been evaluated on the
Multi-Perspective Question Answering
(MPQA) dataset and promising results
have been observed in the preliminary
experiments. We have also explored
adding neutral words as prior information
for model learning. It was found that
while incorporating subjectivity clues
bearing positive or negative polarity can
achieve a significant performance gain,
the prior lexical information from neutral
words is less effective.

1 Introduction

Subjectivity detection seeks to identify whether
the given text expresses opinions (subjective) or
reports facts (objective). Such a task of distin-
guishing subjective information from objective is
useful for many natural language processing ap-
plications. For instance, sentiment classification

often assumes that the input documents are opin-
ionated, and ideally only contain subjective state-
ments. Document summarization systems need
to summarize different perspectives and opinions.
For question answering systems, extracting and
presenting information of the appropriate type,
i.e., opinions or facts, is imperative according to
the specific question being asked (Yu and Hatzi-
vassiloglou, 2003; Wiebe and Riloff, 2005; Pang
and Lee, 2008).

Work on sentence-level subjectivity detection
is relatively sparse compared to document-level
sentiment classification. Early work used a boot-
strapping algorithm to learn subjective (Riloff
and Wiebe, 2003) or both subjective and objec-
tive (Wiebe and Riloff, 2005) expressions for
sentence-level subjectivity detection. In contrast
to bootstrapping, there has been some recent at-
tempts exploring various n-gram features and dif-
ferent level of lexical instantiation for detecting
subjective utterance from conversation data (Wil-
son and Raaijmakers, 2008; Raaijmakers et al.,
2008; Murray and Carenini, 2009).

However, the aforementioned line of work tack-
led subjectivity detection either as supervised or
semi-supervised learning, requiring labelled data
and extensive knowledge which are expensive to
acquire. On the other hand, both subjectivity
and sentiment are context sensitive and in general
quite domain dependent (Pang and Lee, 2008), so
that classifiers trained on one domain often fail
to produce satisfactory performance when shifted
to new domains (Gamon et al., 2005; Blitzer et
al., 2007). Moreover, user generated content from
web are often massive and evolve rapidly over
time, which imposes more challenges to the sub-
jectivity detection task. These observations have
thus motivated us to develop a subjectivity detec-
tion algorithm that is relatively simple compared
to existing methods (e.g., based on bootstrapping
or n-gram features), and yet can easily be trans-

1153



ferred between domains through unsupervised or
weakly-supervised learning without using any la-
belled data.

In this paper, we focus on the problem of
weakly-supervised sentence-level subjectivity de-
tection. Instead of learning subjective extrac-
tion patterns or exploring various n-gram features,
we view the problem as generative model learn-
ing with the proposed subjectivity detection LDA
(subjLDA) model. In this model, the generative
process involves: (1) three subjectivity labels for
sentences (i.e., sentence expresses subjective opin-
ions as being positive/negative, or states facts as
being objective); (2) a sentiment label for each
word in the sentence (either positive, negative, or
neutral), and (3) the words in the sentences.

We test the subjLDA model on the publicly
available Multi-Perspective Question Answering
(MPQA) dataset. Two lists of domain indepen-
dent subjectivity lexicons, namely the subjClue
and SentiWordNet lexicons (Esuli and Sebastiani,
2006), were incorporated as prior knowledge for
subjLDA model learning. Preliminary results
show that the weakly-supervised subjLDA model
is able to significantly outperform baseline. Fur-
thermore, it was found that while incorporating
subjectivity clues bearing positive or negative po-
larity can achieve a significant performance gain,
the prior lexical information from neutral words is
less effective for improving the classification ac-
curacy.

The rest of the paper is organized as follows.
Section 2 reviews the previous work on subjec-
tivity classification. Section 3 presents the sub-
jLDA model. Experimental setup and results on
the MPQA dataset are discussed in Sections 4
and 5, respectively. Finally, Section 6 concludes
the paper and outlines the future work.

2 Related Work

2.1 Subjectivity Detection

While sentiment classification and subjectivity de-
tection are closely related to each other, it has been
reported that separating subjective and objective
instances from text is more difficult than sentiment
classification, and the improvement of subjectivity
detection can benefit the latter as well (Mihalcea et
al., 2007).

Early work by Riloff and Wiebe (2003) fo-
cused on a bootstrapping method for sentence-
level subjectivity detection. They started with

high-precision subjectivity classifiers which auto-
matically identified subjective and objective sen-
tences in un-annotated texts. The subjective
expression patterns were learned from syntactic
structure output from the previously labelled high
confidence texts. The learned patterns were used
to automatically identify additional subjective sen-
tences, which enlarged the training set, and the en-
tire process was then iterated. Wiebe and Riloff
(2005) used very similar method for subjectivity
detection as Riloff and Wiebe (2003). But they
moved one step forward that they also learned ob-
jective expressions apart from subjective expres-
sons. As the subjective/objective expression pat-
terns are based on syntactic structures, they are
more flexible than single words or n-grams.

Wilson and Raaijmakers (2008) compared the
performance of classifiers trained using word n-
grams, character n-grams, and phoneme n-grams
for recognizing subjective utterances in multiparty
conversation. Raaijmakers et al. (2008) extended
the work in (Wilson and Raaijmakers, 2008) by
further analyzing the performance of detecting
subjectivity in meeting speech by combining a va-
riety of multimodal features including additional
prosodic features. More recently, Murray and
Carenini (2009) proposed to learn subjective ex-
pression patterns from both labeled and unlabeled
data using n-gram word sequences. Their ap-
proach for learning subjective expression patterns
is similar to (Raaijmakers et al., 2008) which re-
lies on n-grams, but goes beyond fixed sequences
of words by varying levels of lexical instantiation
as in (Riloff and Wiebe, 2003).

2.2 Weakly-supervised Sentiment
Classification

In this section, we first review some work in senti-
ment analysis using generative models as it partly
inspires our work of viewing subjectivity detec-
tion as generative model learning. We then dis-
cuss other weakly-supervised sentiment classifica-
tion approaches which also use prior word knowl-
edge.

Intuitively, sentiment or subjectivity are con-
text dependent. Therefore, modelling topic cou-
pled with sentiment should serve a critical func-
tion in sentiment analysis. There has seen sev-
eral lines of work pursuing this direction. Eguchi
and Lavrenko (2006) considered the topic depen-
dence of sentiment and combined sentiment mod-

1154



els with topic models for sentiment retrieval. Mei
et al. (2007) proposed the topic-sentiment mixture
(TSM) model for capturing mixture of topics and
sentiment simultaneously on Weblogs. The multi-
aspect sentiment (MAS) model by Titov and Mc-
Donald (2008) focused on aggregating sentiment
text for sentiment summary of rating aspects.

The more recently proposed joint sentiment-
topic (JST) model (Lin and He, 2009; Lin et al.,
2010) holds the closest paradigm to the proposed
subjLDA model. They targeted document-level
sentiment detection with weakly-supervised gen-
erative model learning, where the only knowledge
being incorporated was from generic sentiment
lexicons. In the JST model, topics are assumed
to be generated dependent on sentiment distribu-
tions and then words are generated conditioned on
sentiment-topic pairs. However, there are several
intrinsic differences between JST and subjLDA:
(1) JST mainly focused on document-level senti-
ment classification, while, in contrast, subjLDA
has a different scope of targeting sentence-level
subjectivity detection; (2) in JST the prior knowl-
edge was encoded during the Gibbs sampling by
assigning a word with its prior sentiment label if
that word appears in the sentiment lexicon. This
essentially places hard sentiment label to words
and can not resort the situation when words have
ambiguous sentiment polarity. Our proposed ap-
proach incorporates sentiment prior knowledge in
a more principled way, in that we use sentiment
lexicons to modify the topic-word Dirichlet priors
and essentially create an informed prior distribu-
tion for the sentiment labels.

Another common solution to weakly-
supervised sentiment classification is to make use
of prior word polarity knowledge, where one uses
a small number of seed words with known polarity
to infer the polarity of a large set of unidentified
terms. Turney and Littman (2002) classified
the sentiment orientation of other terms in the
corpus through mutual information, based on a
small set of positive/negative paradigm words.
Starting with a single seed word meaning “good”
and a negation check, Zagibalov and Carroll
(2008) derived a classifier through iteratively
retraining, and treated sentiment and subjectivity
as a continuum rather than distinct classes.

3 The SubjLDA Model

As shown in Figure 1(b), subjLDA is essentially
a four-layer Bayesian model. In order to gener-
ate a word wd,m,t (i.e., the tth word token of sen-
tence m within document d), one first chooses a
subjectivity label1 sd,m ∈ [1,K] for each sentence
in document d from the per-document subjectiv-
ity distribution πd. Following that, one chooses a
sentiment label ld,m,t ∈ [1, S] for each word in the
sentences from the per-sentence sentiment distri-
bution θsd,m . Finally, one draws a word from the
per-corpus word distribution ϕld,m,t conditioned
on the corresponding sentiment label. The clas-
sification of sentence subjectivity in subjLDA is
determined directly from the sentence subjectivity
label sd,m. The formal definition of the subjLDA
generative process is as follows:

• For each sentiment label l ∈ [1, S]

– Draw ϕl ∼ Dir(λl × βTl ).

• For each document d ∈ [1, D], choose distributions
πd ∼ Dir(γ).

• For each sentence m ∈ [1,Md] in document d,

– Sample a subjectivity label sd,m ∼ Mult(πd),
– Choose a distribution θd,m ∼ Dir(αsd,m),
– For each of the Nd,m word position,
∗ Choose a sentiment label
ld,m,t ∼ Mult(θsd,m),

∗ Choose a word wd,m,t ∼ Mult(ϕld,m,t).

In practice, it is quite intuitive that one classi-
fies a sentence as subjective if it contains one or
more strongly subjective clues (Riloff and Wiebe,
2003). However, the criterion for classifying ob-
jective sentences could be rather different, because
a sentence is likely to be objective if there are
no strongly subjective clues. In order to encode
this knowledge into the subjLDA model learning,
during the model initialization step, we initialized
sentence subjectivity label s based on the afore-
mentioned criterion with prior knowledge input
from the sentiment lexicon. If a sentence does not
match any sentiment words, its subjectivity label
will be randomly sampled.

1We have conducted another set of experiments modelling
only subjective and objective labels. It was found that sub-
jLDA performed slightly better with 3 subjectivity labels than
with binary labels. We do not report the binary label results
here due to page limit.

1155



w

 

!

"

z

#
Nd

D
T

(a)

w

 

!

 

l"

Nd,m
Md

s

#

K

$

D
S

%

(b)

Figure 1: (a) LDA model; (b) subjLDA model.

3.1 Incorporating Model Prior
Compared to the LDA model, besides adding a
sentence-level subjectivity label generation layer,
we also add an additional dependency link of ϕ
on the matrix λ of size S×V which we use to en-
code word prior sentiment information. The ma-
trix λ can be considered as a transformation ma-
trix which modifies the Dirichlet priors β so that
the word prior sentiment polarity can be captured.

Intuitively, λ is firstly initialized as a matrix
with all the elements taking a value of 1. Given
a sentiment lexicon, for each term w ∈ {1, ..., V }
in the corpus vocabulary, if w is found in the sen-
timent lexicon, then for each l ∈ {1, ..., S}, the
element λlw is updated as follows

λlw =

{
0.9 if S(w) = l
0.05 otherwise

, (1)

where the function S(w) returns the prior senti-
ment label of w in a sentiment lexicon, i.e., pos-
itive, negative or neutral. For example, the word
“excellent” with index wt has a positive senti-
ment polarity. The corresponding row vector λwt
is [0.05, 0.9, 0.05] with its elements representing
neutral, positive, and negative prior polarity. Mul-
tiplying β with λ, we can enforce that the word
“excellent”has much higher probability of being
drawn from the positive topic word distributions
generated from a Dirichlet distribution with pa-
rameter βlposwt .

The previously proposed DiscLDA (Lacoste-
Julien et al., 2008) and Labeled LDA (Ramage et
al., 2009) also utilize a transformation matrix to
modify Dirichlet priors by assuming the availabil-
ity of document class labels. In contrast, we use
word prior sentiment as supervised information to
modify the topic-word Dirichlet priors.

3.2 Model Inference

The total probability of the model is

P (w, l, s,θ,ϕ,π;α, β, γ) =
∏S

j=1 P (ϕj ;λ× β)·∏D
d=1 P (πd; γ)

∏Md
m=1 P (sd,m|πd)P (θd,m;αsd,m)·

∏Nd,m
t=1 P (ld,m,t|θd,m)P (wd,m,t|ϕld,m,t), (2)

where the bold-font variables denote vectors.
We use Gibbs sampling to estimate the posterior

of subjLDA by sequentially sampling each vari-
able of interest, ld,m,t and sd,m here, from the dis-
tribution over that variable given the current val-
ues of all other variables and the data. Letting the
index x = (d,m) and the subscript −x denote
a quantity that excludes counts in sentence m of
document d, the conditional posterior for sx is

P (sx = k|s−x, l,w, α, β, γ) ∝

(Nd,k + γk)− 1
(Nd +

∑K
k=1 γk)− 1

·
∏S

j=1

∏Nd,m,j−1
b=0 (b+ αsx,j)∏Nd,m−1

b=0 (b+
∑3

j=1 αsx,j)
,

(3)

where Nd,k denotes the frequency of sentences as-
signed to subjectivity label k in document d; Nd
is the total number of sentences in document d;
Nd,m,j is the total number of words in sentence m
of document d associated with sentiment label j;
Nd,m is the total number of words in sentence m
of document d.

In terms of the sentiment label, letting the index
y = (d,m, t) denote tth word in sentence m of
document d and the subscript −y denote a quan-
tity that excludes data from tth word position, the

1156



conditional posterior for lt is

P (ly = j|s, l−y,w, α, β, γ) ∝
Nd,m,j + αsd,m,j − 1

Nd,m +
∑S

j=1 αsd,m,j − 1
· Yj,wt + λj,wtβj,wt − 1
Yj +

∑V
r=1 λj,rβj,r − 1

,

(4)

where Yj,wt denotes the frequency of word wt as-
sociated with sentiment label j in the document
collection; Yj is the total number of words asso-
ciated with sentiment label j in the document col-
lection.

Equations 3-4 are the conditional probabilities
derived by marginalizing out the random variables
π, θ, and ϕ. Samples obtained from the Markov
chain are used to approximate the per-document
subjectivity distribution

πd,k =
Nd,k + γk

Nd +
∑K

k=1 γk
. (5)

The approximated per-sentence sentiment distri-
bution is

θd,m,j =
Nd,m,j + αsd,m,j

Nd,m +
∑S

j=1 αsd,m,j
. (6)

Finally, the per-corpus sentiment-word distribu-
tion is

ϕj,r =
Yj,r + λj,rβj,r

Yj +
∑V

r=1 λj,rβj,r
. (7)

4 Experimental Setup

4.1 Dataset

We tested our model on the MPQA dataset2 ver-
sion 1.2, which is derived from a variety of for-
eign news documents. The whole corpus consists
of 535 documents with a total number of 6,111
subjective and 5,001 objective sentences. We per-
formed a two-stage preprocessing on the dataset
by first removing stop words and non-word char-
acters, followed by standard stemming for reduc-
ing vocabulary size and minimizing sparse data
problems. After preprocessing, the MPQA dataset
contains 131,220 words with 10,511 distinct terms
(cf. the original dataset with 264,808 words and a
vocabulary size of 31,201 without any preprocess-
ing).

2http://www.cs.pitt.edu/mpqa/
databaserelease/

4.2 Lexical Prior Knowledge
We explored incorporating two subjectivity lexi-
cons as prior knowledge for subjLDA model learn-
ing, namely, the subjClue3 and SentiWordNet4

lexicons. We point out that the subjClue lexicon
is not related to the MPQA dataset as it was col-
lected from a number of sources, where some were
culled from manually developed resources and
others were identified automatically using both an-
notated and unannotated data (Wiebe and Riloff,
2005). We only extract the lexical clues that are
considered strongly subjective, with the weakly
subjective clues being discarded. The rationale be-
hind the filtering is that while a strongly subjective
clue is seldom used without a subjective meaning,
weakly subjective clues are ambiguous, often hav-
ing both subjective and objective uses. After stem-
ming, removing the duplicated lexical terms and
retaining those that have appeared in the corpus,
we finally obtained a lexicon subset of 477 posi-
tive and 917 negative words.

SentiWordNet provides a wide coverage of lex-
ical terms by tagging all the synsets of WordNet
with three sentiment labels, i.e., positive, negative
and neutral. In our experiment, we only use the
neutral words from SentiWordNet for investigat-
ing how neutral words would affect the subjLDA
model performance. After the same preprocess-
ing as performed on the subjClue lexicon, a total
of 193,871 neutral words were extracted. Further
mapping the extracted neutral words with the cor-
pus results in 6,457 neutral words.

5 Experimental Results

In this section, we first present the experimental
results of sentence-level subjectivity classification
on the MPQA dataset, and subsequently evaluate
the impact on the classification performance by
varying the proportion of prior information being
incorporated. All the results reported here are av-
eraged over 5 runs with 800 Gibbs sampling itera-
tions.

5.1 Overall Results
The baseline is calculated by counting the over-
lap of the prior lexicon with the dataset. We clas-
sify a sentence as subjective if it contains one or
more positive/negative sentiment words; if there
is no matching, the sentence will be classified as

3http://www.cs.pitt.edu/mpqa/
4http://sentiwordnet.isti.cnr.it/

1157



Table 1: Subjectivity classification results. (Boldface indicates the best results.)
Model Objective (%) Subjective (%) Overall (%)Recall Precision F-measure Recall Precision F-measure Accuracy
Baseline 46.5 74.1 57.1 76.7 63.7 69.6 63.1
subjLDA 59.7 71.6 65.1 80.9 71.0 75.6 71.2
LDA (Sent.) 60.5 65.7 63.0 74.2 69.7 72.0 68.1
LDA (Doc.) 51.4 68.7 58.8 80.6 67.0 73.2 67.6
Wiebe 05 77.6 68.4 72.7 70.6 79.4 74.7 73.8

200

1200

2200

3200

4200

5200

6200

7200

8200

9200

200 300 400 477 600 800 917

W
o
rd

 f
re
q
u
e
n
cy

Number of words

Postive Negative

(a)

3.5

4

4.5

5

5.5

o
rd

  
fr
e
q
u
e
n
cy
)

Positive Negative Neutral

2

2.5

3

3.5

4

4.5

5

5.5

200 300 400 477 600 800 917 1500 2500 3500 4500 5500 6457

Lo
g
(W

o
rd

  
fr
e
q
u
e
n
cy
)

Number of words

Positive Negative Neutral

(b)

Figure 2: (a) Positive and negative lexicon statistics; (b) Positive, negative and neutral lexicon statistics.

objective. The improvement over this baseline
will reflect how much subjLDA can learn from
data. The LDA model (Blei et al., 2003), as
shown in Figure 1(a), has been used as baseline
in document-level sentiment classification in pre-
vious research (Lin et al., 2010). Thus, we also
evaluated LDA on the sentence-level subjectivity
detection task by modelling a mixture of three sen-
timent topics, i.e., positive, negative and neutral.
For fair comparison, we encoded prior knowledge
of sentiment lexicon into LDA as identical to sub-
jLDA. Thus the LDA model here can be consid-
ered as a weakly-supervised version. Moreover,
we tested LDA under two different modes, i.e.,
modelling a normal document vs. treating each
individual sentence as a separate document. The
sentence sentiment is determined as follows.

(a) LDA in document mode: sentiment of sen-
tence m in document D is calculated using

P (l|m) ∝ P (m|l)P (l|d) =
∏

wt∈m
P (wt|l)P (l|d).

(8)

We define that sentence m is classified as an ob-
jective sentence if its probability of neutral label
given sentence P (l = neu.|m), is greater than
both P (l = pos.|m) and P (l = neg.|m). Oth-
erwise, the sentence is classified as subjective.

(b) LDA in sentence mode: sentence subjec-
tivity is directly determined based on the per-

sentence sentiment distribution θ, using identical
classification metrics to the document mode.

As can be seen from Table 1, a significant per-
formance gain was observed for both subjLDA
and LDA over the baseline. Particularly, more
than 8% gain was observed for subjLDA, giv-
ing the best overall accuracy of 71.2% which
is 3.1% and 3.6% higher than LDA(Sent.) and
LDA(Doc.), respectively. In addition, except
for objective recall, subjLDA outperforms LDA
in both the sentence and document modes for
all the other evaluation metrics, with more bal-
anced objective and subjective F-measures be-
ing attained compared to the other two models.
On the other hand, it was observed that while
LDA(Doc.) can achieve a comparable subjective
F-measure to LDA(Sent.), its objective F-measure
is nearly 5% lower, resulting in worse overall per-
formance. This is probably due to the fact that by
treating each individual sentence as a document,
LDA(Sent.) can avoid inferencing global senti-
ment topics and thus capture more accurate sen-
timent information from local topics. We mea-
sured the overall accuracy significance with paired
t-Test (critical P=0.01). Results show that the
improvements of subjLDA over both LDA(sent.)
and LDA(doc.) are highly statistically significant.
Thus, we conclude that subjLDA is superior than
LDA in the subjectivity detection task.

When compared to the previous proposed boot-

1158



0.650

0.700

0.750

cc
u
ra
cy

subLDA

LDA (Sent.)

LDA (Doc.)

0.450

0.500

0.550

0.600

0.650

O
v
e
ra
ll

 a
cc
u
ra
cy

0.450

0 65

0 4

e

(a)

0.8

0.9

1

ca
ll

0.3

0.4

0.5

0.6

0.7

0.8

O
b
je
ct
iv
e

 r
e
ca
ll

0.2

0.3

e

(b)

0.6

0.7

0.8

0.9

ti
v
e

 p
re
ci
si
o
n

0.3

0.4

0.5

0.6

O
b
je
ct
iv
e

 p
re

(c)

0 65

0.7

0 4

0.45

0.5

0.55

0.6

0.65

0.7

b
je
ct
iv
e

 F
!m

e
a
su
re

0.3

0.35

0.4

0.45

O
b
je
ct
iv
e

(d)

Figure 3: Subjectivity classification performance vs. different prior information by gradually adding
the subjective and neutral words. The vertical dashed line denotes the point where all the positive and
negative words have been incorporated into the model; 200P+400N denotes adding the least frequent 200
positive and 400 negative words; 500Neu denotes adding the least frequent 500 neutral words in addition
to all the positive and negative words.

strapping approach (Wiebe and Riloff, 2005), sub-
jLDA is about 2% lower in terms of overall ac-
curacy. However, it should be noted that, their
approach used a much larger training set for self-
training which consists of more than 100,000 sen-
tences. Moreover, apart from subjectivity clues,
they also used additional features such as subjec-
tive/obejctive pattern and POS for the Naive Bayes
sentence classifier training. In contrast, the pro-
posed subjLDA model is relatively simple with
only a small set of subjectivity clues being incor-
porated as prior knowledge.

5.2 Classification Results with Different
Priors

While positive/negative sentiment lexicon is com-
monly used in lexical approaches to sentiment
classification, the impact of incorporating neutral
words remains relatively unexplored. In this ex-
periment, we investigated the impact on the model
performance by incorporating additional knowl-
edge from neutral words. We started by first con-
sidering the positive and negative words only and
gradually increased the number of words starting
with the lowest frequency words. After all the pos-

itive and negative words have been incorporated,
we then gradually added additional neutral words
into the model also from the lowest frequency to
the highest. Figure 2 shows the lexicon statistics of
all the positive, negative and neutral words, where
the value on the x-axis represents the number of
words sorted by word frequency and the corre-
sponding y-axis value indicates the total number
of times those words appear in the corpus. For
instance, the 400 least frequent positive words ap-
pear a total of 1,826 times in the corpus as shown
in Figure 2(a).

Figure 3 depicts the subjectivity classification
results of subjLDA and LDA by varying the pro-
portion of lexical terms being incorporated. It is
quite obvious from the overall accuracy shown
in the figure that both subjLDA and LDA bene-
fit from incorporating the information of subjec-
tive words, and in general, the more lexical items
the better the results. Without using any neutral
words, all the three models achieved the best re-
sults when all the subjective words were incorpo-
rated. It was noted that subjLDA performed sim-
ilar to LDA when only a small number of low
frequency subjective words were used. However,

1159



with more higher frequency subjective words be-
ing incorporated, subjLDA shows stronger perfor-
mance boosting over LDA and gives the best ac-
curacy of 70.2% when all the subjective words
were incorporated, being 3.4% and 5.8% better
than LDA(Sent.) and LDA(Doc.), respectively, as
indicated by the vertical dashed line in the figure.

On the other hand, adding neutral words is also
beneficial, where performance gain was observed
for all the 3 models in addition to the best re-
sults using subjective words only (i.e., subjLDA
by 1%, LDA(Sent.) by 1.6% and LDA(Doc.) by
2.9%). Analyzing the objective recall and preci-
sion shown in Figure 3(b) and 3(c) reveals that,
while incorporating the 4,500 least frequent neu-
tral words considerably increases the objective re-
call, the objective precision does not drop much
which eventually leads to the overall improvement
of all the three models.

However, compared to the subjective words, the
classification improvements by incorporating ad-
ditional neutral words are less significant. This is
probably due to the fact that while the presence
of positive/negative words conveys clear subjec-
tive meanings, neutral words are relatively vague
which could bear objective or subjective sense un-
der different contexts. Furthermore, all three mod-
els experience a significant performance drop af-
ter the point of (4500Neu). Examining Figure 2
reveals that, while the 4,500 least frequent neutral
words appear 11,142 times in the corpus, the 1,957
most frequent words (i.e., from 4500 to 6457) ap-
pear 93,036 times, nearly 10 times as much as the
former. Thus, the high frequency neutral words
become dominant in the model and result in se-
vere classification bias towards the objective class.
Therefore, appropriate filtering of neutral words is
necessary in order to avoid introducing bias into
model learning.

5.3 Sentiment Topics

In subjLDA, we model three topics in the per-
corpus word distribution, each of which corre-
sponds to neutral, positive and negative sentiment.
Figure 4 shows the top 15 topic words of the sen-
timent topics extracted from the MPQA dataset by
subjLDA. It can be easily observed that while the
positive and negative sentiment topics consist of
clear sentiment bearing words, the neutral topic
contains mostly theme words with no sentiment,
which illustrates the effectiveness of subjLDA in

Sentiment topics

Neutral Positive Negative

countri state terror

presid right opposit

unit support concern

govern gener evil

intern want critic

bush interest question

report posit mean

elect move protest

china remark violat

militari hope accus

war alli refus

prison agre despit

taiwan live reject

minist provid fail

foreign consent impos

 

Figure 4: Sentiment topics extracted by subjLDA.

extracting sentiment bearing topics from text.

6 Conclusions and Future Work

This paper presents the subjectivity detection LDA
Model (subjLDA) for sentence-level subjectivity
classification. In contrast to most of the exist-
ing approaches requiring labelled corpora or lin-
guistic pattern extraction, we view this problem
as weakly-supervised generative model learning
where the only input to the model is a small
amount of domain independent subjective/neutral
words. The subjLDA model has been evaluated
on the MPQA dataset. Preliminary results show
that except slightly lower in objective recall, sub-
jLDA outperformed LDA over all other evaluation
metrics, and is comparable to the previously pro-
posed bootstrapping approach using a much larger
training set. Moreover, it was found that while in-
corporating more subjective words can generally
yield better results, the performance gain by em-
ploying extra neutral words is less significant.

There are several directions we would like
to pursue in the future. While word lexical
prior information is incorporated by modifying the
Dirichlet prior for topic-word distributions here, it
is also possible to explore other mechanisms to de-
fine expectation or posterior constraints. In ad-
dition, the current subjLDA model only models
bag-of-words features, another future step would
be extending subjLDA to include higher order in-
formation such as bigrams for improving model
performance.

1160



Acknowledgements

This work was supported in part by the EC-FP7
projects ROBUST (grant number 257859) and
VPH-Share (grant number 269978).

References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.

2003. Latent Dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022.

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the Association for Com-
putational Linguistics (ACL), pages 440–447.

Koji Eguchi and Victor Lavrenko. 2006. Sentiment
retrieval using generative models. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 345–354.

A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In Proceedings of LREC.

M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining customer opinions from free
text. Lecture Notes in Computer Science, 3646:121–
132.

S. Lacoste-Julien, F. Sha, and M.I. Jordan. 2008. Dis-
cLDA: Discriminative learning for dimensionality
reduction and classification. In NIPS.

Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of the ACM conference on Information and
knowledge management (CIKM).

Chenghua Lin, Yulan He, and Richard Everson.
2010. A comparative study of Bayesian models
for unsupervised sentiment detection. In Confer-
ence on Computational Natural Language Learning
(CoNLL).

Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of the 16th international conference on
World Wide Web (WWW), pages 171–180.

R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual
projections. In In Proceedings of the Association for
Computational Linguistics (ACL), page 976.

G. Murray and G. Carenini. 2009. Predicting subjec-
tivity in multimodal conversations. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1348–1357.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1–135.

S. Raaijmakers, K. Truong, and T. Wilson. 2008. Mul-
timodal subjectivity analysis of multiparty conver-
sation. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 466–474.

D. Ramage, D. Hall, R. Nallapati, and C.D. Manning.
2009. Labeled LDA: A supervised topic model for
credit attribution in multi-labeled corpora. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
248–256.

E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
the conference on Empirical methods in natural lan-
guage processing (EMNLP), pages 105–112.

Ivan Titov and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of the Aunal Meeting on Asso-
ciation for Computational Linguistics and the Hu-
man Language Technology Conference (ACL-HLT),
pages 308–316.

Peter D. Turney and Michael L. Littman. 2002. Un-
supervised learning of semantic orientation from a
hundred-billion-word corpus. ArXiv Computer Sci-
ence e-prints, cs.LG/0212012.

J. Wiebe and E. Riloff. 2005. Creating subjective
and objective sentence classifiers from unannotated
texts. In Proceedings of the Conference on Compu-
tational Linguistics and Intelligent Text Processing
(CICLing), volume 3406, pages 486–497. Springer.

T. Wilson and S. Raaijmakers. 2008. Comparing
word, character, and phoneme n-grams for subjec-
tive utterance recognition. In Proceedings of IN-
TERSPEECH, pages 1614–1617.

Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opin-
ion sentences. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 129–136.

T. Zagibalov and J. Carroll. 2008. Automatic seed
word selection for unsupervised sentiment classifi-
cation of Chinese text. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING), pages 1073–1080.

1161


