



















































Yimmon at SemEval-2019 Task 9: Suggestion Mining with Hybrid Augmented Approaches


Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1267–1271
Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics

1267

Yimmon at SemEval-2019 Task 9: Suggestion Mining with Hybrid
Augmented Approaches

Yimeng Zhuang
Samsung Research China - Beijing (SRC-B)

ym.zhuang@samsung.com

Abstract

Suggestion mining task aims to extract tips,
advice, and recommendations from unstruc-
tured text. The task includes many chal-
lenges, such as class imbalance, figurative ex-
pressions, context dependency, and long and
complex sentences. This paper gives a de-
tailed system description of our submission in
SemEval 2019 Task 9 Subtask A. We trans-
fer Self-Attention Network (SAN), a success-
ful model in machine reading comprehen-
sion field, into this task. Our model con-
centrates on modeling long-term dependency
which is indispensable to parse long and com-
plex sentences. Besides, we also adopt tech-
niques, such as contextualized embedding,
back-translation, and auxiliary loss, to aug-
ment the system. Our model achieves a per-
formance of F1=76.3, and rank 4th among 34
participating systems. Further ablation study
shows that the techniques used in our system
are beneficial to the performance.

1 Introduction

Suggestion mining is a trending research domain
that focuses on the extraction of extract tips,
advice, and recommendations from unstructured
text. To better recognize suggestions, instead of
only matching feature words, one must have the
ability to understand long and complex sentences.

SemEval-2019 Task 9 provides the suggestion
mining task (Negi et al., 2019). The task can be
recognized as a text classification task, given a
sentence collected from user feedback, participat-
ing systems are required to give a binary output by
marking it as suggestion or non-suggestion.

To address this problem, we focus on solving
long-term dependency on long and complex sen-
tences. Consequently, we transfer Self-Attention
Network (SAN), a successful model in machine
reading comprehension field, in which long-term

dependency is crucial, into this task. Furthermore,
we also utilize multiple techniques to improve the
suggestion mining system.

2 System description

In this paper, we consider suggestion mining as a
text classification task. Figure 1 gives an overview
of our model. First, the input text is converted into
word embeddings with linguistic features. Then,
we use several stacked semantic encoders to gen-
erate the hidden representations for each token.
On top of that, a softmax output layer estimates
the probability of the text being a suggestion.

2.1 Input encoding

The input encoding layer is in charge of encoding
each token of the input text to singular vectors. To-
kenization is completed during preprocessing. In
our work, we adopt WordPiece embedding (Wu
et al., 2016) and feed it into a pretrained lan-
guage model (LM) to generate contextualized em-
beddings. Compared to context independent word
vectors, such as widely used GloVe (Pennington
et al., 2014), SGNS (Mikolov et al., 2013), con-
textualized vectors show significant advantages in
disambiguation and sentence modeling. Besides,
well-pretrained language model also transfers ex-
ternal knowledge to this task, full use of transfer
learning is the key to the advance in modern neu-
ral natural language processing. On the choice
of the pretrained language model, we compared
two publicly available models ELMo (Peters et al.,
2018) and BERT (Devlin et al., 2018), and we fi-
nally choose BERT for better performance. Due to
the out-of-memory issues 1, we do not update the
parameters of BERT during training, thus we only
use it as a static feature extractor.

1https://github.com/google-research/
bert#out-of-memory-issues

https://github.com/google-research/bert#out-of-memory-issues
https://github.com/google-research/bert#out-of-memory-issues


1268

Text

LM
POS | TAG | NER

Pad

Concat

Highway

Position encoding

Layer norm

Conv

...

Layer norm

Self-Attention

Layer norm

Feedforward layer

X 3

X 2

Split

Linear & softmax

Score

In
pu

t 
en

co
d

in
g

M
o

de
l e

n
co

de
r

O
u

pu
t 

la
ye

r

Figure 1: Overview of our system

Also, linguistic features are extracted to im-
prove system performance further. In this work,
we extract part-of-speech (POS) and named en-
tities (NER) by spaCy 2. Two kinds of part-of-
speech granularity are used, primary POS and ex-
tended POS tag (TAG). In order to obtain linguis-
tic feature sequences with the same length as the
BERT outputs, we pad zero vectors at the start and
end position of the linguistic feature sequences.
Then, the contextualized vectors and linguistic
feature vectors are concatenated. A two layers
highway network (Srivastava et al., 2015) is also
adopted on top of this representation. The vectors

2https://spacy.io

are projected to d dimensions immediately.

2.2 Model encoder
The model encoder is the central part of our sys-
tem, and it is in charge of modeling long-term de-
pendency and extracting deep features. Because
of the success of Self-Attention Network (SAN)
(Shen et al., 2018) in various NLP tasks, we adopt
a structure from QANet (Yu et al., 2018), which is
a variant of Self-Attention Network, as our model
encoder.

As is shown in the middle part of Figure 1, the
model encoder is a combination of convolution,
self-attention and feed-forward layer. This struc-
ture is repeated three times. The input vectors of
this structure are firstly added by sinusoidal posi-
tion embedding (Gehring et al., 2017) to encode a
notion of the order in the sequence. The position
embedding is calculated as follows:

PEi,2j = sin(i/10000
2j/d)

PEi,2j+1 = cos(i/10000
2j/d)

(1)

After that there are convolution blocks, follow-
ing (Yu et al., 2018), depth-wise separable con-
volution layers are chosen for better generaliza-
tion and memory efficiency. The model encoder
is highly dependent on input layer normalization
and residual connections, each block is in a uni-
form structure: layer normalization / operation /
residual connection. In our system, we repeat con-
volution blocks two times for better and deeper lo-
cal feature extraction.

In the self-attention layer, the scaled dot-
product attention is computed:

Ai = softmax(
QKT√

d
) · V (2)

Where Q, K, and V are query, key, and value re-
spectively, they are the linear projection of each
position in the input. As in (Vaswani et al., 2017),
multi-head attention mechanism is adopted which
integrates multiple scaled dot-product attentions.

A = [A1; · · · ;Ai; · · · ;An] ·WA (3)

whereAi denotes the i-th head, [·] is concatenation
operator, WA is a trainable parameter.

At last, there is a fully connected block. In our
method, it is a little different from the original
work (Yu et al., 2018). We append a gate mech-
anism to refine tokens by their importance (Wang

https://spacy.io


1269

et al., 2017).

S = FFN2(LayerNorm(H))�G+H (4)
G = G∗/max(G∗) (5)

G∗ = sigmoid(LayerNorm(H) ·WG + bG)
(6)

where we assume the input of this block is H ∈
Rm×d, m is the sequence length, FFN2 denotes
a 2-layer non-linear feed-forward network, S ∈
Rm×d represents the output of this fully connected
block. G ∈ Rm×1 and G∗ ∈ Rm×1 are the output
weight of the gate. WG ∈ Rm×1 and bG ∈ R1
are trainable parameters. The maximum operation
aims to select the maximum element in G∗, and
the division operation normalizes these weights so
that the maximum weight in G is always one.

2.3 Output layer

Given the output S = [s1, s2, · · · , sm] of previ-
ous layers, this output layer converts these hidden
representations into the final probability. Since we
have adopted BERT in the input encoding layer,
the first vector of the sequence is a special clas-
sification token [CLS], which can be used as the
representation of the whole sentence. We split the
matrix S and take the first vector s1 ∈ Rd. The
probability of the input text being a suggestion text
is estimated as follows.

p = softmax(W p · s1 + bp) (7)

where W p ∈ R2×d and bp ∈ R2 are trainable pa-
rameters, p ∈ R2 denotes the output probabilities
including “yes” probability p1 and “no” probabil-
ity p0.

2.4 Loss

We treat this task as a text classification problem
and use log-loss as the loss function.

L0 =
1

N

N∑
i=1

yilog(p
1
i ) + (1− yi)log(p0i ) (8)

where N is the number of examples, yi ∈ {0, 1}
represents the label of i-th example, p1i and p

0
i are

the predictions.
Besides, in order to better recognize important

tokens and filter trivial tokens out, we add an aux-
iliary loss to discourage large weights in G in
Equation 4. Thus only those tokens that have

contributions to the classification have non-zero
weights in G.

L1 = β
1

N

N∑
i=1

‖Gi‖1 (9)

where ‖·‖1 denote 1-norm, β is a hyper-parameter,
we use β = 10−3 in this work. The final loss is the
sum of L0 and L1.

2.5 Class imbalance
As is pointed out in (Negi et al., 2019), sugges-
tions appear sparsely in online reviews and fo-
rums, and this makes class imbalance a critical
problem. For simplicity, we do not take measures
during training. In inference, we slightly adjust
the predicted probability and divide it by a priori,
which is the rate of positive examples in training
data.

p = softmax((W p · s1 + bp)× 10)
p1∗ = p1/priori

p0∗ = p0/(1− priori)
(10)

where p1∗ and p0∗ are the actual predictions for
inference, the system outputs “yes” when p1∗ is
larger than p0∗. Other symbols are the same as in
Equation 7.

2.6 Back-translation
Because the given training data set is not large, we
also utilize a data augmentation technique to en-
rich the training data. The data augmentation tech-
nique we used is back-translation (Yu et al., 2018).
Specifically, we first translate the given training
data into Chinese by a neural machine translation
system and then translate it back into English by
another neural machine translation system. The
two neural machine translation systems are trained
on a subset of the WMT18 data sets 3. Both origi-
nal training data and the augmented data are ap-
plied to train our text classification system, but
the augmented data is given a small weight (=0.2)
when calculating the loss.

3 Experiments

3.1 Setup
SemEval 2019 Task 9 Subtask A 4 provides a sug-
gestion mining data set collected from feedback

3http://statmt.org/wmt18
4https://competitions.codalab.org/

competitions/19955

http://statmt.org/wmt18
https://competitions.codalab.org/competitions/19955
https://competitions.codalab.org/competitions/19955


1270

posts on Universal Windows Platform. The data
set is split into an 8980-example training set, a
592-example validation set, and an 833-example
test set. Since the organizer does not limit the us-
age of the validation set, we merge it into training
data and train our model through the k-fold cross-
validation method. Specifically, we split all train-
ing data into eight subsets, and guarantee the rate
between the number of positive examples and the
number of negative examples is about 1:3 in each
subset. The preprocessing process is implemented
as the description in section 2.1.

The kernel size of convolution layers is 7, and
the hidden size d is 256, the number of heads
is 8 in multi-head self-attention layers. Adam
optimizer (Kingma and Ba, 2014) with learning
rate 0.0008 is used for tuning the model param-
eters. The mini-batch size is 32. For regulariza-
tion, the dropout rate is set to 0.1. The submission
predictions are obtained by integrating eight runs
through voting.

Approach Test F1
Baseline 26.8
1st ranked system 78.1
2nd ranked system 77.8
3rd ranked system 77.6
Our system 76.3
5th ranked system 74.9
Ablation - single model F1 ∆
Our full system 73.3 -
- Contextualized embedding 67.6 -5.7
- Back-translation 71.4 -1.9
- Priori 72.5 -0.8
- Linguistic features 72.6 -0.7
- Auxiliary loss 72.7 -0.6

Table 1: Performance of the Top 5 systems on the
leaderboard of subtask A, and our ablation experi-
ments.

3.2 Results
Table 1 shows the main results on the test set.
Compared with the rule baseline, participants im-
prove their performance with substantial gains.
Our system achieved F1=76.3 on the test set and
ranked 4th among all 34 teams.

In order to evaluate the individual contribution
of each feature, we run an ablation study. Con-
textualized embedding is most critical to the per-
formance, and it concludes that transferring com-
mon sense by learning large corpus is vital for this

task. Back-translation accounts for about 2% of
the performance degradation, which clearly shows
the effectiveness of data augmentation. Besides,
linguistic features, auxiliary loss, and priori are
also beneficial to the system.

Base Large
F1 F1 F1 F1

72.9 71.8 75.4 75.9
72.9 70.6 73.9 76.6
74.3 74.8 74.0 76.2
75.3 73.5 76.3 75.1

Ensemble
# F1 # F1
2 72.9 8 74.5
4 74.7 12 75.5
16 76.3 Oracle 79.6

Table 2: Performance of every single model and en-
semble models.

3.3 Ensemble models

Table 2 reports the effect of ensemble. To ob-
tain the submission predictions, we trained eight
models on the eight subsets mentioned in sec-
tion 3.1. Four of these models are based on a
110M parameters base BERT model, and the other
four are based on a 340M parameters large BERT
model. The performance of every single model is
reported. It is evident that different data partition
causes quite a large performance variance. Thus,
ensemble is necessary. The result shows that as
the number of models increases the ensemble ef-
fect improves. Also, we experiment by searching
the optimal model combination assuming the test
label is known to show the performance limitation
(the Oracle performance).

4 Conclusion

In this work, we adopt multiple techniques to im-
prove a suggestion mining system. The core of
our system is a variant of Self-Attention Network
(SAN), which originates from the machine reading
comprehension field. Based on this model, tech-
niques, such as contextualized embedding, back-
translation, linguistic features, and auxiliary loss,
are investigated to improve the system perfor-
mance further. Experimental results illustrate the
effect of our system. Our model achieves a perfor-
mance of F1=76.3, and rank 4th among 34 partic-
ipating systems.



1271

References
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and

Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional
sequence to sequence learning. In Proceedings
of the 34th International Conference on Machine
Learning-Volume 70, pages 1243–1252. JMLR. org.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Sapna Negi, Tobias Daudert, and Paul Buitelaar. 2019.
Semeval-2019 task 9: Suggestion mining from on-
line reviews and forums. In Proceedings of the
13th International Workshop on Semantic Evalua-
tion (SemEval-2019).

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), volume 1,
pages 2227–2237.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
and Chengqi Zhang. 2018. Bi-directional block self-
attention for fast and memory-efficient sequence
modeling. arXiv preprint arXiv:1804.00857.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Highway networks. arXiv
preprint arXiv:1505.00387.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
and Ming Zhou. 2017. Gated self-matching net-
works for reading comprehension and question an-
swering. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 189–198.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.

Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V
Le. 2018. Qanet: Combining local convolution
with global self-attention for reading comprehen-
sion. arXiv preprint arXiv:1804.09541.


