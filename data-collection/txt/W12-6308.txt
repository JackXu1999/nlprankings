










































Word Segmentation on Chinese Mirco-Blog Data with a Linear-Time Incremental Model


Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 41–46,
Tianjin, China, 20-21 DEC. 2012

Word Segmentation on Chinese Mirco-Blog Data
with a Linear-Time Incremental Model

Kaixu Zhang †

kareyzhang@gmail.com

Maosong Sun ‡
†Xiamen University, Fujian Province 361005, China

‡State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology

Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China

sunmaosong@gmail.com

Changle Zhou †

dozero@xmu.edu.cn

Abstract
This paper describes the model we de-
signed for the word segmentation bake-
off on Chinese micro-blog data in the 2nd
CIPS-SIGHAN joint conference on Chi-
nese language processing. We presented
a linear-time incremental model for word
segmentation where rich features includ-
ing character-based features, word-based
features as well as other possible features
can be easily employed. We report the per-
formances of our model on four datasets in
the SIGHAN bake-off 2005. After adding
more features designed for the micro-blog
data, the performance of our model is fur-
ther improved. The F-score of our model
for this bake-off is 0.9478 and 44.88% of
the sentences are segmented correctly.

1 Introduction

Chinese word segmentation is an important and
fundamental task for Chinese language process-
ing. General-purpose word segmentation is widely
studied. Micro-blog-related topic emergences and
becomes a new research topic in recent years.
Therefore researchers pay more and more atten-
tion to the word segmentation model for Chinese
micro-blog data.

Motivated by the linear-time incremental parser
proposed by Huang and Sagae (2010) and the
word-based word segmentation model proposed
by Zhang and Clark (2011), first we presented a
linear-time incremental word segmentation model.
Various features including character-based fea-
tures and word-based features can be employed
while exponentially many segmented results can
be tested in linear-time. We report the perfor-
mances of our model on four datasets in the
SIGHAN bake-off 2005.

One of the difficulties of training word segmen-
tation model on micro-blog data is the lack of an-

notated micro-blog data (only 500 sentences of
micro-blog data are provided and used by us). Fol-
lowing the annotation adaptation method proposed
by Jiang et al. (2009), we train a general-purpose
joint word segmentation and part-of-speech tag-
ging model using People’s Daily corpus. Then,
the decoding results of such a model are used as
features in the final word segmentation model for
micro-blog data.

Moreover, various lexicon features such as dic-
tionaries and word list of idioms are employed to
segment micro-blog data. Preprocessing is also
conducted to deal with URLs and special charac-
ters.

Finally, The F-score of our model for the bake-
off is 0.9478 and 44.88% of the sentences are seg-
mented correctly. The performance of our method
is still far from perfect. The lack of segmented
micro-blog data is one of the bottlenecks of our
model. If more training data is provided, our
model can reach better performance.

2 The Linear-Time Incremental Word
Segmentation Model

2.1 Word Segmentation Definition

First, we give a formal general definition of word
segmentation.

A raw sentence X is a Chinese sentence where
no spaces are presented to separate words, while
a segmented sentence Y is a sentence in which
words are separated by spaces. For example, “材
料利用率高” is a raw sentence, and “材料 利用
率高” is one of the possible segmented sentences
corresponding to the raw sentence.

Given a raw sentence X , a word segmenta-
tion model needs to find a segmented sentence Ŷ
among all possible segmented sentences GEN(X)
corresponding to the raw sentence. This can be

41



seen as an optimization problem:

Ŷ = arg max
Y ∈GEN(x)

f(Y,Λ) (1)

where the objective function f(Y,Λ) is used to
evaluate segmented sentences and Λ is the param-
eter.

In the following subsections, we will describe
the detail of this function and how to learn the pa-
rameter.

2.2 Word Segmentation as Action Sequence
Generation

In this paper, word segmentation is treated as ac-
tion sequence generation. Each action is cor-
responding to a character interval of the input
sentence. For an input sentence of |X| char-
acters, the corresponding action sequence A =
(a0, . . . , a|X|) has a length of |X| + 1 (includ-
ing the “intervals” at very beginning and very end
of the sentence). There are two kinds of actions
(ai ∈ {s,c}), namely separate (denoted as s) and
combine (denoted as c). The action ai = s means
that the i-th character and the i + 1-th character
in the input sentence are belong to two separated
words; while ai = c means that they are belong to
the same word.

Given A, the corresponding segmented sen-
tence Y is determined and denoted as YA. For ex-
ample, for the input sentence “材料利用率高”,
the action sequence (s, c, s, c, s, s, s) could gener-
ate a segmented sentence YA as “材料 利用 率
高”.

The problem of finding a best segmented sen-
tence is now equivalent to the problem of generat-
ing a best action sequence.

We introduce S = (s0, . . . , s|X|) determined by
A as a sequence of statuses to generate feature vec-
tors for the action sequence and then evaluate any
segmented sentence YA as

f(YA,Λ) =

|X|+1∑
i=0

Φ(si, X) · ΛTai (2)

where Φ(si, X) is a feature vector generated by
the input and status si corresponding to action ai.
And Λs and Λc are two weight vectors for two
kinds of actions.

The status sequence S can be defined in differ-
ent ways. In this paper, we define it as follows.

A status si in S is defined as a tuple 〈i, ui, vi〉,
where ui is the index of the last s action, and vi is

input X

axiom 〈0,−1,−1〉 : 0

s
〈i, u, 〉 : c

〈i+ 1, i, u〉 : c+ σ

c
〈i, u, v〉 : c

〈i+ 1, u, v〉 : c+ γ

goal 〈|X|+ 1, , 〉 : c

Figure 1: The deductive system used to describe
our model. In this system, i is the step, c is the
cost, σ = Φ(si, X) · ΛTs is the s cost and γ =
Φ(si, X) · ΛTc is the c cost. The best derivation is
the derivation of the goal with the highest cost.

Atom features Description

xj characters in X
ai−1, ai−2 last two actions

w0 current (partial) word
w−1 last determined word

Table 2: Atom features for the i-th action ai

the index of the second last s action. Thus given
A, si can be formally recursively calculated as

si =


〈i,−1,−1〉 if i = 0
〈i, i− 1, ui−1〉 if ai−1 = s
〈i, ui−1, vi−1〉 if ai−1 = c

(3)

Following Huang and Sagae (2010), the genera-
tion of the action sequence can also be formalized
as a deductive system described in Figure 2.2.

The next subsection will describe the feature
vector Φ(si, X) in detail.

2.3 Feature Templates
We define feature vectors by using feature tem-
plates. First, atom features are generated based
on si and X . All the feature templates can then be
generated by using atom features.

Atom features are shown in Table 2. The last
two actions ai−1 and ai−2 can be determined by
the status si. The (partial) word w0 is the string
between the index of last s action ui and the cur-
rent position i.

Feature templates are defined as tuples and
shown in Table 1. |w| is the length of the word
w. w[0] and w[−1] are the first and last character

42



action-based 〈a-1, ai−2, ai−1〉
character-based 〈c-1, xi−2, ai−1〉, 〈c-2, xi−1, ai−1〉, 〈c-3, xi, ai−1〉

〈c-4, xi−3, xi−2, ai−1〉, 〈c-5, xi−2, xi−1, ai−1〉,
〈c-6, xi−1, xi, ai−1〉, 〈c-7, xi, xi+1, ai−1〉

word-based 〈w-1,w0〉, 〈w-2, |w0|〉
〈w-3, |w0|,w0[0]〉, 〈w-4, |w0|,w0[−1]〉, 〈w-5,w0[0],w0[−1]〉
〈w-6,w−1[−1],w0[−1]〉, 〈w-7, |w−1|,w0〉, 〈w-8,w−1, |w0|〉
〈w-9,w0[0], xi〉, 〈w-10,w0[−1], xi〉

Table 1: Feature templates

of word w, respectively. Each tuple is correspond-
ing to one dimension of the feature vector and the
value of that dimension will be set to 1 if this cor-
responding feature was generated.

There are action-based, character-based and
word-based templates. Note that when only
action-based and character-based templates are
used, these feature templates are equivalent to the
templates used by conventional word segmenta-
tion models based on character tagging (Zhang
et al., 2011). And the word-based features are
mainly based on the work by Zhang and Clark
(2011).

2.4 Decoding and Learning Algorithms
We apply the decoding algorithm used by Huang
and Sagae (2010).

Beam search is used in the decoding algorithm,
while different hypotheses with the same status at
a certain step will be merged in a dynamic pro-
gramming manner. This decoding algorithm can
efficiently search exponentially many hypotheses
in linear-time (O(nb) where b is the width of the
beam). Comparatively, the time complexity of the
decoding algorithm using fully dynamic program-
ming is O(n3) ( or O(nL2) if the max length of
words L is specified).

The parameter Λ is trained using an average
perceptron algorithm (Collins, 2002). We also
tried early update (Collins and Roark, 2004) in the
learning algorithm. Although it is reported that
early update helps the learning of parsers, we do
not observe that early update helps the learning of
word segmentation models. So we do not imple-
ment early update in our experiments.

3 Word Segmentation for Micro-Blog
Data

In order to segment the micro-blog data better, we
modified the word segmentation model described

in the last section by adding a preprocessing and
more features.

We just perform feature engineering manually
for the development to decide which feature is use-
ful for segmenting micro-blog data 1.

3.1 Preprocessing

A rule-based preprocessing is conducted before
the statistical model. This preprocessing is mainly
used to reduce the search space of the statistical
model by assigning the action ai of certain posi-
tion before the decoding algorithm. Thus the de-
coding algorithm will only search either hypothe-
ses that ai = s or hypotheses that ai = c.

URLs and other micro-blog-specified charac-
ters (such as “@” means “at somebody” and “#”
means to annotate a tag) are first recognized. The
boundaries of these components are assigned to s,
while the inner character intervals of the URLs are
assigned to c.

Likewise, the punctuations (such as Chinese full
stop “。” and comma “，”) are recognized and the
boundaries of these are assigned to s. The intervals
between two Arabic numbers or two Latin letters
are assigned to c.

White spaces can also be found in the raw
micro-blog data between two English words or at
the end of a micro-blog user’s name after the ‘@’
character. The preprocessing will remove these
white spaces and assigned s for the left character
intervals.

3.2 Character-Type-Based Features

Since there are more non-standard uses of non-
Chinese characters in micro-blog data than in
news data and adding character-type-based fea-
tures can improve the performance of general-

1Word-based feature templates in Table 1 are also modi-
fied slightly for the word segmentation model for micro-blog
data.

43



Method
Dataset

AS CityU MSR PKU

Best05 0.952 0.943 0.964 0.950
(Asahara et al., 2005) (Tseng et al., 2005) (Chen et al., 2005)

(Wang et al., 2010) 0.956 0.956 0.972 0.957
(Zhang and Clark, 2011) 0.954 0.951 0.973 0.944

(Sun et al., 2012) NA 0.948 0.974 0.954
Our model 0.953 0.948 0.973 0.952

Table 3: F-scores of our model and models in related work on SIGHAN 05 bake-off data

purpose word segmentation model (Zhao et al.,
2006), we employ character-type-based features.

We define a function type(xi) that returns the
type of the characters

type(xi) =


C if xi is a Chinese character
L if xi is a Latin letter
A if xi is a Arabic numeric character
xi otherwise

(4)
The additional feature templates that we

use are 〈ct-1, type(xi)〉, 〈ct-2, type(xi−1)〉,
〈ct-3, type(xi+1)〉, 〈ct-4, type(xi−1), type(xi)〉
and 〈ct-5, type(xi), type(xi+1)〉.

3.3 Lexical Features

Lexical features are used as additional word-
based features for word segmentation for micro-
blog data. Each lexical feature template
〈lex-k, lexk(w0)〉 is based on a function whose
variable is a word.

Since we have various lexical resources, we can
define several functions lexk to create different
lexical feature templates. If the lexical resource
is just a word list, the lexk(w0) could just return
a binary value to indicate whether this word w0
is in the word list or not. If the lexical resource
is about the frequencies of words, lexk(w0) could
return log2 (freq(w0) + 1) where freq(w0) is the
frequency of word w0.

We use several word lists to add lexical feature
templates, including a word list of idioms from
Sun (2011), word lists based on People’s Daily
corpus, Yuwei Corpus and Tsinghua Treebank.
We also use words with frequencies counted from
the three mentioned segmented corpora.

Additionally, we add another lexical feature
template based on whether these four characters
xui , xui+1, xui+2 and xui+3 form an idiom.

3.4 Tagger-Based Features

The annotated micro-blog data contains only 500
micro-blogs. So more annotated data are required.
We train a character-based joint word segmenta-
tion and part-of-speech tagging model using the
People’s Daily corpus (Zhang, 2012)2, and then
use the decoding results of this model as features
for the word segmentation model for the micro-
blog data.

Three templates 〈tb-1, a′i〉, 〈tb-2, a′i,POSi−1〉
and 〈tb-3, a′i,POSi〉 are added. a′i is the action
based on the results of the tagger, and POSi is the
part-of-speech tag of the word that xi belongs to.

4 Experiments

We report the performances of our model on four
SIGHAN05 datasets (Emerson, 2005). Then we
report the performance our model on the micro-
blog data. We use 5-fold cross validation for the
development and use the whole dataset to train the
final model for the test.

The F-score is used to evaluate the performance,
which is the harmonic mean of precision (percent-
age of words that are correctly segmented in the
results) and recall (percentage of words that are
correctly segmented in the gold standard).

The results of our model and related work on
the SIGHAN05 datasets are listed in Table 3.

The results of the micro-blog data are listed in
Table 4. The first row is the final performance
on the test data, while the following rows show
the performances with different feature sets for
the cross validation using 500 micro-blog sen-
tences. We can see that the additional features of
the micro-blog data improve the performance.

2The code we use is a part of the tool THULAC (Tsinghua
University - Lexical Analyzer for Chinese) http://nlp.
csai.tsinghua.edu.cn/thulac/.

44



F-score

All features for test 0.9478
All features for cross validation 0.9413

w/o character-type-based features 0.9383
w/o lexical features 0.9201
w/o tagger-based features 0.9310

Table 4: Experiment results of our model on the
micro-blog data

For the annotated micro-blog data for the train-
ing is quite limited, the lexical features and tagger-
based features are important for the performance.
Note that the F-score for the test is better than the
F-score for the cross validation. This may caused
by that the training set for the former model is one-
quarter larger. It may imply that the performance
of our model is limited by the size of the training
data and the performance of our model will be im-
proved when larger training data was provided.

5 Discussion and Conclusion

In this paper, we describe the model we designed
for the word segmentation bake-off on Chinese
micro-blog data in the 2nd CIPS-SIGHAN joint
conference on Chinese language processing. We
presented a linear-time incremental word segmen-
tation model in which various features can be eas-
ily employed. After employing more features of
the micro-blog data, the performance of our model
is further improved. The final F-score of our
model on the test set is 0.9478 and 44.88% of the
micro-blogs are segmented correctly.

The performance of our model is still far from
perfect. Word segmentation for micro-blog data is
not as good as word segmentation for news data
(see Table 3 and Table 4). More manually anno-
tated data or employing semi-supervised method
can be used to improve the performance. We
also notice that outputting inconsistence words is a
problem for statistical word segmentation models.
Therefore a post-processing could be used to ad-
just the output for better performance. We spend
much time comparing the performances of combi-
nations of different feature templates. A more so-
phisticated method is needed for the selection of
feature templates.

Acknowledgments

The authors want to thank ZHANG Junsong from
the cognitive lab and SHI Xiaodong, CHEN Yi-
dong and SU Jinsong from the NLP lab of Xiamen
University for the support of experiments.

The authors are supported by NSFC under
Grant No. 61133012 and 61273338.

References
M. Asahara, K. Fukuoka, A. Azuma, C. L. Goh,

Y. Watanabe, Y. Matsumoto, and T. Tsuzuki. 2005.
Combination of machine learning methods for opti-
mum chinese word segmentation. In Proc. Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pages 134–137.

A. Chen, Y. Zhou, A. Zhang, and G. Sun. 2005. Un-
igram language model for chinese word segmenta-
tion. In Proceedings of the 4th SIGHAN Workshop
on Chinese Language Processing, pages 138–141.

Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL’04), Main Volume,
pages 111–118, Barcelona, Spain, July.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. pages 1–8.

Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, pages 123–133. Jeju Island, Korea.

Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077–
1086, Uppsala, Sweden, July. Association for Com-
putational Linguistics.

Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and POS tagging - a case study.
In Proceedings of the 47th ACL, pages 522–530,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.

Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast on-
line training with frequency-adaptive learning rates
for chinese word segmentation and new word detec-
tion. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 253–262, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.

Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech

45



tagging. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1385–
1394, Portland, Oregon, USA, June. Association for
Computational Linguistics.

H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and
C. Manning. 2005. A conditional random field
word segmenter for sighan bakeoff 2005. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chi-
nese Language Processing, pages 168–171. Jeju Is-
land, Korea.

Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010.
A character-based joint model for chinese word seg-
mentation. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010), pages 1173–1181, Beijing, China, August.
Coling 2010 Organizing Committee.

Y. Zhang and S. Clark. 2011. Syntactic processing
using the generalized perceptron and beam search.
Computational Linguistics, (Early Access):1–47.

Kaixu Zhang, Ruining Wang, Ping Xue, and Maosong
Sun. 2011. Extract chinese unknown words from
a large-scale corpus using morphological and dis-
tributional evidences. In Proceedings of 5th In-
ternational Joint Conference on Natural Language
Processing, pages 837–845, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.

Kaixu Zhang. 2012. Study on Chinese Word Segmenta-
tion and Part-of-Speech Tagging with Compact Rep-
resentations. Ph.D. thesis, Tsinghua University.

H. Zhao, C. N. Huang, and M. Li. 2006. An improved
chinese word segmentation system with conditional
random field. In Proceedings of the Fifth SIGHAN
Workshop on Chinese Language Processing, pages
162–165. Sydney: July.

46


