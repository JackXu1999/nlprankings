



















































Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation


Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 250–259,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Joint Learning of the Embedding of Words and Entities for Named Entity
Disambiguation

Ikuya Yamada1 4 Hiroyuki Shindo2 Hideaki Takeda3 Yoshiyasu Takefuji4
ikuya@ousia.jp shindo@is.naist.jp takeda@nii.ac.jp takefuji@sfc.keio.ac.jp

1Studio Ousia, 4489-105-221 Endo, Fujisawa, Kanagawa, Japan
2Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, Japan

3National Institute of Informatics, 2-1-2 Hitotsubashi, Chiyoda, Tokyo, Japan
4Keio University, 5322 Endo, Fujisawa, Kanagawa, Japan

Abstract

Named Entity Disambiguation (NED)
refers to the task of resolving multiple
named entity mentions in a document to
their correct references in a knowledge
base (KB) (e.g., Wikipedia). In this paper,
we propose a novel embedding method
specifically designed for NED. The pro-
posed method jointly maps words and enti-
ties into the same continuous vector space.
We extend the skip-gram model by us-
ing two models. The KB graph model
learns the relatedness of entities using the
link structure of the KB, whereas the an-
chor context model aims to align vectors
such that similar words and entities occur
close to one another in the vector space
by leveraging KB anchors and their con-
text words. By combining contexts based
on the proposed embedding with standard
NED features, we achieved state-of-the-
art accuracy of 93.1% on the standard
CoNLL dataset and 85.2% on the TAC
2010 dataset.

1 Introduction

Named Entity Disambiguation (NED) is the task
of resolving ambiguous mentions of entities to
their referent entities in a knowledge base (KB)
(e.g., Wikipedia). NED has lately been extensively
studied (Cucerzan, 2007; Mihalcea and Csomai,
2007; Milne and Witten, 2008b; Ratinov et al.,
2011) and used as a fundamental component in
numerous tasks, such as information extraction,
knowledge base population (McNamee and Dang,
2009; Ji et al., 2010), and semantic search (Blanco
et al., 2015). We use Wikipedia as KB in this pa-
per.

The main difficulty in NED is ambiguity in the

meaning of entity mentions. For example, the
mention “Washington” in a document can refer
to various entities, such as the state, or the capi-
tal of the US, the actor Denzel Washington, the
first US president George Washington, and so
on. In order to resolve these ambiguous men-
tions into references to the correct entities, early
approaches focused on modeling textual context,
such as the similarity between contextual words
and encyclopedic descriptions of a candidate en-
tity (Bunescu and Pasca, 2006; Mihalcea and Cso-
mai, 2007). Most state-of-the-art methods use
more sophisticated global approaches, where all
mentions in a document are simultaneously disam-
biguated based on global coherence among disam-
biguation decisions.

Word embedding methods are also becom-
ing increasingly popular (Mikolov et al., 2013a;
Mikolov et al., 2013b; Pennington et al., 2014).
These involve learning continuous vector repre-
sentations of words from large, unstructured text
corpora. The vectors are designed to capture the
semantic similarity of words when similar words
are placed near one another in a relatively low-
dimensional vector space.

In this paper, we propose a method to construct
a novel embedding that jointly maps words and en-
tities into the same continuous vector space. In
this model, similar words and entities are placed
close to one another in a vector space. Hence,
we can measure the similarity between any pair of
items (i.e., words, entities, and a word and an en-
tity) by simply computing their cosine similarity.
This enables us to easily measure the contextual
information for NED, such as the similarity be-
tween a context word and a candidate entity, and
the relatedness of entities required to model coher-
ence.

Our model is based on the skip-gram model
(Mikolov et al., 2013a; Mikolov et al., 2013b), a

250



recently proposed embedding model that learns to
predict each context word given the target word.
Our model consists of the following three models
based on the skip-gram model: 1) the conventional
skip-gram model that learns to predict neighboring
words given the target word in text corpora, 2) the
KB graph model that learns to estimate neighbor-
ing entities given the target entity in the link graph
of the KB, and 3) the anchor context model that
learns to predict neighboring words given the tar-
get entity using anchors and their context words in
the KB. By jointly optimizing these models, our
method simultaneously learns the embedding of
words and entities.

Based on our proposed embedding, we also de-
velop a straightforward NED method that com-
putes two contexts using the proposed embedding:
textual context similarity, and coherence. Textual
context similarity is measured according to vector
similarity between an entity and words in a docu-
ment. Coherence is measured based on the relat-
edness between the target entity and other entities
in a document. Our NED method combines these
contexts with several standard features (e.g., prior
probability) using supervised machine learning.

We tested the proposed method using two stan-
dard NED datasets: the CoNLL dataset and the
TAC 2010 dataset. Experimental results revealed
that our method outperforms state-of-the-art meth-
ods on both datasets by significant margins. More-
over, we conducted experiments to separately as-
sess the quality of the vector representation of enti-
ties using an entity relatedness dataset, and discov-
ered that our method successfully learns the qual-
ity representations of entities.

2 Joint Embedding of Words and
Entities

In this section, we first describe the conventional
skip-gram model for learning word embedding.
We then explain our method to construct an em-
bedding that jointly maps words and entities into
the same continuous d-dimensional vector space.
We extend the skip-gram model by adding the KB
graph model and the anchor context model.

2.1 Skip-gram Model for Word Similarity

The training objective of the skip-gram model is to
find word representations that are useful to predict
context words given the target word. Formally,
given a sequence of T words w1, w2, ..., wT , the

model aims to maximize the following objective
function:

Lw =
T∑

t=1

∑
−c≤j≤c,j 6=0

logP (wt+j |wt) (1)

where c is the size of the context window, wt de-
notes the target word, andwt+j is its context word.
The conditional probability P (wt+j |wt) is com-
puted using the following softmax function:

P (wt+j |wt) =
exp(Vwt>Uwt+j )∑
w∈W exp(Vwt>Uw)

(2)

where W is a set containing all words in the vo-
cabulary, and Vw ∈ Rd and Uw ∈ Rd denote the
vectors of word w in matrices V and U, respec-
tively.

The skip-gram model is trained to optimize the
above functionLw, and V are used as the resulting
vector representations of words.

2.2 Extending the Skip-gram Model
We extend the skip-gram model to learn the vector
representations of entities. We expand matrices V
and U to include the vectors of entities Ve ∈ Rd
and Ue ∈ Rd in addition to the vectors for words.
2.2.1 KB Graph Model
We use an internal link structure in KB to enable
the model to learn the relatedness between pairs of
entities. Wikipedia Link-based Measure (WLM)
(Milne and Witten, 2008a) is a method to measure
entity relatedness based on its link structure. It
has been used as a standard method to compute
the relatedness of entities for modeling coherence
in past NED studies. The relatedness between two
entities is computed using the following function:

WLM(e1, e2) = 1− log max(|Ce1 |,|Ce2 |)−log |Ce1∩Ce2 |log |E|−log min(|Ce1 |,|Ce2 |) (3)

where E is the set of all entities in KB and Ce is
the set of entities with a link to an entity e. In-
tuitively, WLM assumes that entities with similar
incoming links are related. Despite its simplicity,
WLM yields state-of-the-art performance (Hoffart
et al., 2012).

Inspired by WLM, the KB graph model simply
learns to place entities with similar incoming links
near one another in the vector space. We formalize
this as the following objective function:

Le =
∑
ei∈E

∑
eo∈Cei ,ei 6=eo

logP (eo|ei) (4)

251



We compute the conditional probability P (eo|ei)
using the following softmax function:

P (eo|ei) = exp(Vei
>Ueo)∑

e∈E exp(Vei>Ue)
(5)

We train the model to predict the incoming links
Ce given an entity e. Therefore, Ce plays a similar
role to context words in the skip-gram model.

2.2.2 Anchor Context Model
If we add only the KB graph model to the skip-
gram model, the vectors of words and entities do
not interact, and can be placed in different sub-
spaces of the vector space. To address this issue,
we introduce the anchor context model to place
similar words and entities near one another in the
vector space.

The idea underlying this model is to lever-
age KB anchors and their context words to train
the model. As mentioned in Section 1, we use
Wikipedia as a KB. It contains many internal an-
chors that can be safely treated as unambiguous
occurrences of referent KB entities. By using
these anchors, we can easily obtain many occur-
rences of entities and their corresponding context
words directly from the KB.

As in the skip-gram model, we simply train the
model to predict the context words of an entity
pointed to by the target anchor. The objective
function is as follows:

La =
∑

(ei,Q)∈A

∑
wo∈Q

logP (wo|ei) (6)

where A denotes a set of anchors in the KB, each
of which contains a pair of a referent entity ei and
a set of its context words Q. Here, Q contains the
previous c words and the next c words. Note that
|A| equals the number of internal anchors in the
KB. As in past models, the conditional probability
P (wo|ei) is computed using the softmax function:

P (wo|ei) = exp(Vei
>Uwo)∑

w∈W exp(Vei>Uw)
(7)

Using the proposed model, we align the vec-
tor representations of words and entities by plac-
ing words and entities with similar context words
close to one another in the vector space.

2.3 Training
Considering the three model components men-
tioned above, we propose the following objective

function by linearly combining the above objec-
tive functions:

L = Lw + Le + La (8)

The training of the model is intended to maximize
the above function, and the resulting matrix V is
used to embed words and entities.

One of the problems in training our model is
that the normalizers contained in the softmax func-
tions P (wt+j |wt), P (eo|ei), and P (wo|ei) are
computationally very expensive because they in-
volve summation over all words W or entities E.
To address this problem, we use negative sampling
(NEG) (Mikolov et al., 2013b) to convert original
objective functions into computationally feasible
ones. NEG is defined by the following objective
function:

log σ(Vwt>Uwt+j ) +
∑g

i=1 Ewi∼Pneg(w)
[
log σ(−Vwt>Uwi)

]
(9)

where σ(x) = 1/(1 + exp(−x)) and g is the
number of negative samples. We replace the
logP (wt+j |wt) term in Eq. (1) with the above ob-
jective function. Consequently, the objective func-
tion is transformed from that in Eq. (1) to a sim-
ple objective function of the binary classification
to distinguish the observed word wt from words
drawn from noise distribution Pneg(w). We also
replace logP (eo|ei) in Eq. (4) and logP (wo|ei)
in Eq. (6) in the same manner.

Note that NEG takes a negative distribution
Pneg(w) as a free parameter. Following (Mikolov
et al., 2013b), we use the unigram distribution
of words (U(w)) raised to the 3/4th power (i.e.,
U(w)3/4/Z, where Z is a normalization constant)
in the skip-gram model and the anchor context
model. In the KB graph model, we use a uniform
distribution over KB entitiesE as the negative dis-
tribution.

We use Wikipedia to train all the above mod-
els. Optimization is carried out simultaneously
to maximize the transformed objective function
by iterating over Wikipedia pages several times.
We use stochastic gradient descent (SGD) for the
optimization. The optimization is performed us-
ing a multiprocess-based implementation of our
model using Python, Cython, and NumPy config-
ured with OpenBLAS with storing matrices V and
U in the shared memory. To improve speed, we
decide not to introduce locks to the shared matri-
ces.

252



3 Named Entity Disambiguation Using
Embedding

In this section, we explain our NED method us-
ing our proposed embedding. Let us formally
define the task. Given a set of entity mentions
M = {m1,m2, ...,mN} in a document d with an
entity set E = {e1, e2, ..., eK} in the KB, the task
is defined as resolving mentions (e.g., “Washing-
ton”) into their referent entities (e.g., Washington
D.C.).

We introduce two measures that have been fre-
quently observed in past NED studies: entity prior
P (e) and prior probability P (e|m). We define en-
tity prior P (e) = |Ae,∗|/|A∗,∗| where A∗,∗ de-
notes all anchors in the KB and Ae,∗ is the set
of anchors that point to entity e. Prior probabil-
ity is defined as P (e|m) = |Ae,m|/|A∗,m| where
A∗,m represents all anchors with the same surface
as mention m in KB and Ae,m is a subset of A∗,m
that points to entity e.

We separate the NED task into two sub-tasks:
candidate generation and mention disambigua-
tion. In candidate generation, candidates of ref-
erent entities are generated for each mention. De-
tails of candidate generation are provided in Sec-
tion 4.3.1.

3.1 Mention Disambiguation

Given a document d and mention m with its can-
didate referent entities {e1, e2, ..., ek} generated in
the candidate generation step, the task is to disam-
biguate mention m by selecting the most relevant
entity from the candidate entities.

The key to improving the performance of this
task is to effectively model the context. We pro-
pose two novel methods to model the context us-
ing the proposed embedding. Further, we combine
these two models with several standard NED fea-
tures using supervised machine learning described
in 3.1.3.

3.1.1 Modeling Textual Context
Textual context is designed based on the assump-
tion that an entity is more likely to appear if the
context of a given mention is similar to that of the
entity.

We propose a method to measure the similarity
between textual context and entity using the pro-
posed embedding by first deriving the vector rep-
resentation of the context and then computing the
similarity between the context and the entity using

cosine similarity. To derive the vector of context,
we average the vectors of context words:

~vcw =
1

|Wcm |
∑

w∈Wcm
~vw (10)

where Wcm is a set of the context words of men-
tion m and ~vw ∈ V denotes the vector represen-
tation of word w. We use all noun words in docu-
ment d as context words.1 Moreover, we ignore a
context word if the surface of mention m contains
it.

We then measure the similarity between candi-
date entity and the derived textual context by using
cosine similarity between ~vcw and the vector of en-
tity ~ve.

3.1.2 Modeling Coherence
It has been revealed that effectively modeling co-
herence in the assignment of entities to mentions
is important for NED. However, this is a chicken-
and-egg problem because the assignment of enti-
ties to mentions, which is required to measure co-
herence, is not possible prior to performing NED.

Similar to past work (Ratinov et al., 2011), we
address this problem by employing a simple two-
step approach: we first train the machine learn-
ing model using the coherence score among unam-
biguous mentions2, in addition to other features,
and then retrain the model using the coherence
score among the predicted entity assignments in-
stead.

To estimate coherence, we first calculate the
vector representation of the context entities and
measure the similarity between the vector of the
context entities and that of the target entity e. Note
that context entities are unambiguous entities in
the first step, and predicted entities are used in-
stead in the second step.

To derive the vector representation of context
entities, we average their vector representations:

~vce =
1
|Ecm |

∑
e∗∈Ecm

~ve∗ (11)

where Ecm denotes the set of context entities de-
scribed above.

To estimate the coherence score, we again use
cosine similarity between the vector of entity ~ve
and that of context entities ~vce .

1We used Apache OpenNLP tagger to detect nouns.
https://opennlp.apache.org/

2We consider that mention m unambiguously refers to en-
tity e if its prior probability P (e|m) is greater than 0.95.

253



3.1.3 Learning to Rank
To combine the proposed contextual information
described above with standard NED features, we
employ a method of supervised machine learning
to rank the candidate entities given mentionm and
document d.

In particular, we use Gradient Boosted Regres-
sion Trees (GBRT) (Friedman, 2001), a state-
of-the-art point-wise learning-to-rank algorithm
widely used for various tasks, which has been re-
cently adopted for the sort of tasks for which we
employ it here (Meij et al., 2012). GBRT consists
of an ensemble of regression trees, and predicts
a relevance score given an instance. We use the
GBRT implementation in scikit-learn3 and the lo-
gistic loss is used as the loss function. The main
parameters of GBRT are the number of iterations
η, the learning rate β, and the maximum depth of
the decision trees ξ.

With regard to the features of machine learning,
we first use prior probability (P (e|m)) and entity
prior (P (e)). Further, we include a feature repre-
senting the maximum prior probability of the can-
didate entity e of all mentions in the document. We
also add the number of entity candidates for men-
tion m as a feature. The above set of four features
is called base features in the rest of the paper.

We also use several string similarity features
used in past work on NED (Meij et al., 2012).
These features aim to capture the similarity be-
tween the title of entity e and the surface of men-
tion m, and consist of the edit distance, whether
the title of entity e exactly equals or contains the
surface of mention m, and whether the title of en-
tity e starts or ends with the surface of mention m.

Finally, we include contextual features mea-
sured using the proposed embedding. We use co-
sine similarity between the candidate entity and
the textual context (see Section 3.1.1), and similar-
ity between an entity and contextual entities (see
Section 3.1.2). Furthermore, we include the rank
of entity e among candidate entities of mentionm,
sorted according to these two similarity scores in
descending order.

4 Experiments

In this section, we describe the setup and results
of our experiments. In addition to experiments on
the NED task, we separately assessed the quality
of pairwise entity relatedness in order to test the

3http://scikit-learn.org/

NDCG@1 NDCG@5 NDCG@10 MAP

Our Method 0.59 0.56 0.59 0.52
WLM 0.54 0.52 0.55 0.48

Table 1: Results of the entity relatedness task.

effectiveness of our method in capturing pairwise
similarity between pairs of entities. We first de-
scribe the details of the training of the embedding
and then present the experimental results.

4.1 Training for the Proposed Embedding

To train the proposed embedding, we used the
December 2014 version of the Wikipedia dump4.
We first removed the pages for navigation, main-
tenance, and discussion, and used the remaining
4.9 million pages. We parsed the Wikipedia pages
and extracted text and anchors from each page.
We further tokenized the text using the Apache
OpenNLP tokenizer. We also filtered out rare
words that appeared fewer than five times in the
corpus. We thus obtained approximately 2 billion
tokens and 73 million anchors. The total num-
ber of words and entities in the embedding were
approximately 2.1 million and 5 million, respec-
tively. Consequently, the number of rows of ma-
trices V and U were 7.1 million.

The number of dimensions d of the embed-
ding was set to 500. Following (Mikolov et al.,
2013b), we also used learning rate α = 0.025
which linearly decreased with the iterations of the
Wikipedia dump. Regarding the other parameters,
we set the size of the context window c = 10
and the negative samples g = 30. The model
was trained online by iterating over pages in the
Wikipedia dump 10 times. The training lasted ap-
proximately five days using a server with a 40-core
CPU on Amazon EC2.

4.2 Entity Relatedness

To test the quality of the vector representation
of entities, we conducted an experiment using a
dataset for entity relatedness created by Cecca-
relli et al. (Ceccarelli et al., 2013). The dataset
consists of training, test, and validation sets, and
we only use the test set for this experiment. The
test set contains 3,314 entities, where each entity
has 91 candidate entities with gold-standard la-
bels indicating whether the two entities are related.

4The dump was retrieved from Wikimedia Downloads.
http://dumps.wikimedia.org/

254



Following (Huang et al., 2015), we obtained the
ranked order of the candidate entities using cosine
similarity between the target entity and each of
the candidate entities, and computed the two stan-
dard measures: normalized discounted cumulative
gain (NDCG) (Järvelin and Kekäläinen, 2002) and
mean average precision (MAP) (Manning et al.,
2008). We adopted WLM as baseline.

Table 1 shows the results. The score for WLM
was obtained from Huang et al. (Huang et al.,
2015). Our method clearly outperformed WLM.
The results show that our method accurately cap-
tures pairwise entity relatedness.

4.3 Named Entity Disambiguation

4.3.1 Setup
We now explain our experimental setup for the
NED task. We tested the performance of our pro-
posed method on two standard NED datasets: the
CoNLL dataset and the TAC 2010 dataset. The de-
tails of these datasets are provided below. More-
over, as with the corpus used in the embedding, we
used the December 2014 version of the Wikipedia
dump as the referent KB, and to derive the prior
probability as well as the entity prior.

To find the best parameters for our machine
learning model, we ran a parameter search on the
CoNLL development set. We used η = 10, 000
trees, and tested all combinations of the learning
rate β = {0.01, 0.02, 0.03, 0.05} and the maxi-
mum depth of the decision trees ξ = {3, 4, 5}. We
computed their accuracy on the dataset, and found
that the parameters did not significantly affect per-
formance (1.0% at most). We used β = 0.02 and
ξ = 4 which yielded the best performance.

CoNLL The CoNLL dataset is a popular NED
dataset constructed by Hoffart et al. (Hoffart et al.,
2011). The dataset is based on NER data from the
CoNLL 2003 shared task, and consists of training,
development, and test sets, containing 946, 216,
and 231 documents, respectively. We trained our
machine learning model using the training set and
reported its performance using the test set. We also
used the development set for the parameter tuning
described above. Following (Hoffart et al., 2011),
we only used 27,816 mentions with valid entries in
the KB and reported the standard micro- (aggre-
gates over all mentions) and macro- (aggregates
over all documents) accuracies of the top-ranked
candidate entities to assess disambiguation perfor-
mance. For candidate generation, we use the fol-

lowing two resources: 1) a public dataset recently
built by Pershina et al. (Pershina et al., 2015) (de-
noted by PPRforNED) for the sake of compati-
bility with their state-of-the-art results, and 2) a
dictionary built using a standard YAGO means re-
lation dataset (Hoffart et al., 2011) (denoted by
YAGO). Moreover, we used PPRforNED for the
parameter tuning of the machine learning model
and for error analysis.

TAC 2010 The TAC 2010 dataset is another pop-
ular NED dataset constructed for the Text Analysis
Conference (TAC)5 (Ji et al., 2010). The dataset
is based on news articles from various agencies
and Web log data, and consists of a training and a
test set containing 1,043 and 1,013 documents, re-
spectively. Following past work (He et al., 2013;
Chisholm and Hachey, 2015), we used mentions
only with a valid entry in the KB, and reported the
micro-accuracy score of the top-ranked candidate
entities. We trained our model using the training
set and assessed its performance using the test set.
Consequently, we evaluated our model on 1,020
mentions contained in the test set. For candidate
generation, we used a dictionary that was directly
built from the Wikipedia dump mentioned previ-
ously. Similar to past work, we retrieved possible
mention surfaces of an entity from (1) the title of
the entity, (2) the title of another entity redirect-
ing to the entity, and (3) the names of anchors that
point to the entity. We retained the top 50 candi-
dates through their entity priors for computational
efficiency.

4.3.2 Comparison with State-of-the-art
Methods

We compared our method with the following re-
cently proposed state-of-the-art methods:

• Hoffart et al. (Hoffart et al., 2011) is a graph-
based approach that finds a dense subgraph
of entities in a document to address NED.

• He et al. (He et al., 2013) uses deep neural
networks to derive the representations of en-
tities and mention contexts and applies them
to NED.

• Chisholm and Hachey (Chisholm and
Hachey, 2015) uses a Wikilinks dataset
(Singh et al., 2012) to improve the perfor-
mance of NED.

5http://www.nist.gov/tac/

255



Micro
accuracy

Macro
accuracy

CoNLL (PPRforNED) 93.1 92.6
CoNLL (YAGO) 91.5 90.9
TAC 2010 85.2 -

Table 2: Experimental results of our proposed
NED method.

CoNLL

(Micro)

CoNLL

(Macro)

TAC10

(Micro)

Our Method 93.1 92.6 85.2
Hoffart et al., 2011 82.5 81.7 -
He et al., 2013 85.6 84.0 81.0
Chisholm & Hachey, 2015 88.7 - 80.7
Pershina et al., 2015 91.8 89.9 -

Table 3: Accuracy scores of the proposed method
and the state-of-the-art methods.

• Pershina et al. (Pershina et al., 2015) im-
proved NED by modeling coherence using
the personalized page rank algorithm, and
achieved the best-known accuracy on the
CoNLL dataset.

4.3.3 Results
Table 2 shows the experimental results of our pro-
posed method. Our method successfully achieved
enhanced performance on both the CoNLL and the
TAC 2010 datasets. Moreover, we found that the
choice of candidate generation method consider-
ably affected performance on the CoNLL dataset.

Further, Table 3 shows the experimental results
of our proposed method as well as those of state-
of-the-art methods. Our method outperformed all
the state-of-the-art methods on both datasets.

4.3.4 Feature Study
We conducted a feature study on our method. We
began with base features, added various features
to our system incrementally, and reported their im-
pact on performance. We then introduced our two-
step approach to achieve the final results.

Table 4 shows the results. Surprisingly, we
attained results comparable with those of some
state-of-the-art methods on the both datasets by
only using base features. Adding string similarity
features slightly further improved performance.

We observed significant improvement when
adding textual context features based on our pro-
posed embedding. Our method outperformed

Micro
accuracy

Macro
accuracy

CoNLL (PPRforNED):
Base 85.4 87.4
+String similarity 85.8 87.8
+Textual context 90.9 92.4
+Coherence 91.4 92.1
Two-step 93.1 92.6
CoNLL (YAGO):
Base 81.1 83.6
+String similarity 81.3 84.2
+Textual context 87.2 89.6
+Coherence 90.3 90.8
Two-step 91.5 90.9
TAC 2010:
Base 80.1 -
+String similarity 81.7 -
+Textual context 84.6 -
+Coherence 85.5 -
Two-step 85.2 -

Table 4: The results of our feature study.

some state-of-the-art methods without using co-
herence.

Further, coherence based on unambiguous en-
tity mentions and our two-step approach sig-
nificantly improved performance on the CoNLL
dataset. However, it did not contribute to perfor-
mance on the TAC 2010 dataset. This was because
of the significant difference in the density of en-
tity mentions between the datasets. The CoNLL
dataset contains approximately 20 entity mentions
per document, but the TAC 2010 only contains ap-
proximately one mention per document which is
unarguably insufficient to model coherence.

4.3.5 Error Analysis

We also conducted an error analysis on the
CoNLL test set with candidate generation using
PPRforNED dataset. We observed that approx-
imately 48.6% errors were caused by metonymy
mentions (Ling et al., 2015) (i.e., mentions with
more than one plausible annotation). In particu-
lar, our NED method often erred when an incor-
rect entity was highly popular and exactly matched
the mention surface (e.g., “South Africa” referring
to the entity South Africa national rugby union
team rather than the entity South Africa). This
makes sense because our machine learning model
uses the popularity statistics of the KB (i.e., prior

256



probability and entity prior), and the string simi-
larity between the title of the entity and the men-
tion surface. This problem is discussed further in
(Ling et al., 2015).

Furthermore, because our method depends on
the presence of KB anchors in order to learn en-
tity representation, it arguably fails to learn sat-
isfactory representations of tail entities (i.e., enti-
ties rarely referred to by anchors), thus resulting in
disambiguation errors. We discovered that nearly
9.6% errors were due to referent entities with less
than 10 inbound KB anchors, and 4.5% involved
entities with no inbound KB anchor. These errors
might be addressed using KB data other than KB
anchors, such as the description of the entities and
the KB categories in order to avoid dependence on
the KB anchors. This remains part of our future
work.

5 Related Work

Early NED methods addressed the problem as
a well-studied word sense disambiguation prob-
lem (Mihalcea and Csomai, 2007). These meth-
ods primarily focused on modeling the similar-
ity of textual (local) context. Most recent state-
of-the-art methods focus on modeling coherence
among disambiguated entities in the same docu-
ment (Cucerzan, 2007; Milne and Witten, 2008b;
Hoffart et al., 2011; Ratinov et al., 2011). These
approaches have also been called collective or
global approaches in the literature.

Learning the representations of entities for NED
has been addressed in past literature. Guo and
Barbosa (Guo and Barbosa, 2014) used random
walks on KB graphs to construct vector represen-
tations of entities and documents to address NED.
Blanco et al. (Blanco et al., 2015) proposed a
method to map entities into the word embedding
(i.e., Word2vec (Mikolov et al., 2013b)) space us-
ing entity descriptions in the KB and applied it for
NED. He et al. (He et al., 2013) used deep neu-
ral networks to compute representations of entities
and contexts of mentions directly from the KB.
Similarly, Sun et al. (Sun et al., 2015) proposed
a method based on deep neural networks to model
representations of mentions, contexts of mentions,
and entities. Huang et al. (Huang et al., 2015)
also leveraged deep neural networks to learn entity
representations such that the consequent pairwise
entity relatedness was more suitable than of a stan-
dard method (i.e., WLM) for NED. Further, Hu et

al. (Hu et al., 2015) used hierarchical information
in the KB to build entity embedding and applied
it to model coherence. Unlike these methods, our
proposed approach involves jointly learning vector
representations of entities as well as words, hence
enabling the accurate computation of the semantic
similarity among its items to model both the tex-
tual context and coherence.

Moreover, Yaghoobzadeh and Schutze
(Yaghoobzadeh and Schütze, 2015) addressed an
entity typing task by building an embedding of
words and entities on a corpus with annotated
entities (i.e., FACC1 (Gabrilovich et al., 2013))
using the skip-gram model. Compared to our
method, in addition to the significant difference
between their task and NED, their embedding
does not incorporate the link graph data of KB,
which is known to be highly important for NED.

Furthermore, in the context of knowledge graph
embedding, another tenor of recent works has been
published (Bordes et al., 2011; Socher et al., 2013;
Lin et al., 2015). These methods focus on learning
vector representations of entities to primarily ad-
dress the link prediction task that aims to predict
a new fact based on existing facts in KB. Particu-
larly, Wang et al. (Wang et al., 2014) have recently
revealed that the joint modeling of the embedding
of words and entities can improve performance
in several tasks including the link prediction task,
which is somewhat analogous to our experimental
results.

6 Conclusions

In this paper, we proposed an embedding method
to jointly map words and entities into the same
continuous vector space. Our method enables us
to effectively model both textual and global con-
texts. Further, armed with these context mod-
els, our NED method outperforms state-of-the-art
NED methods.

In future work, we intend to improve our model
by leveraging relevant knowledge, such as rela-
tions in a knowledge graph (e.g., Freebase).

References

Roi Blanco, Giuseppe Ottaviano, and Edgar Meij.
2015. Fast and Space-Efficient Entity Linking for
Queries. In Proceedings of the Eighth ACM Interna-
tional Conference on Web Search and Data Mining
(WSDM), pages 179–188.

257



Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning Structured Embed-
dings of Knowledge Bases. In Proceedings of the
25th Conference on Artificial Intelligence (AAAI),
pages 301–306.

Razvan Bunescu and Marius Pasca. 2006. Using
Encyclopedic Knowledge for Named Entity Disam-
biguation. In Proceedings of the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 9–16.

Diego Ceccarelli, Claudio Lucchese, Salvatore Or-
lando, Raffaele Perego, and Salvatore Trani. 2013.
Learning Relatedness Measures for Entity Linking.
In Proceedings of the 22nd ACM International Con-
ference on Information and Knowledge Manage-
ment (CIKM), pages 139–148.

Andrew Chisholm and Ben Hachey. 2015. Entity Dis-
ambiguation with Web Links. Transactions of the
Association for Computational Linguistics, 3:145–
156.

Silviu Cucerzan. 2007. Large-Scale Named Entity
Disambiguation Based on Wikipedia Data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708–716.

Jerome H. Friedman. 2001. Greedy Function Approx-
imation: A Gradient Boosting Machine. The Annals
of Statistics, 29(5):1189–1232.

Evgeniy Gabrilovich, Michael Ringgaard, and Amar-
nag Subramanya. 2013. FACC1: Freebase anno-
tation of ClueWeb corpora, Version 1 (Release date
2013-06-26, Format version 1, Correction level 0).

Zhaochen Guo and Denilson Barbosa. 2014. Entity
Linking with a Unified Semantic Representation. In
Proceedings of the 23rd International Conference on
World Wide Web (WWW), pages 1305–1310.

Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai
Zhang, and Houfeng Wang. 2013. Learning Entity
Representation for Entity Disambiguation. In Pro-
ceedings of the 51st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL) (Volume
2: Short Papers), pages 30–34.

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust Disambiguation of Named
Entities in Text. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 782–792.

Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen,
Martin Theobald, and Gerhard Weikum. 2012.
KORE: Keyphrase Overlap Relatedness for En-
tity Disambiguation. In Proceedings of the 21st
ACM International Conference on Information and
Knowledge Management (CIKM), pages 545–554.

Zhiting Hu, Poyao Huang, Yuntian Deng, Yingkai Gao,
and Eric Xing. 2015. Entity Hierarchy Embedding.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (ACL-IJCNLP) (Volume 1: Long
Papers), pages 1292–1300.

Hongzhao Huang, Larry Heck, and Heng Ji. 2015.
Leveraging Deep Neural Networks and Knowl-
edge Graphs for Entity Disambiguation. CoRR,
abs/1504.0.

Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20(4):422–
446.

Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the TAC 2010
Knowledge Base Population Track. In Proceeding
of Text Analytics Conference (TAC).

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning Entity and Relation Em-
beddings for Knowledge Graph Completion. In Pro-
ceedings of the 29th AAAI Conference on Artificial
Intelligence (AAAI).

Xiao Ling, Sameer Singh, and Daniel S. Weld. 2015.
Design Challenges for Entity Linking. Transactions
of the Association for Computational Linguistics,
3:315–328.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to Information
Retrieval. Cambridge University Press.

P McNamee and HT Dang. 2009. Overview of the
TAC 2009 Knowledge Base Population Track. In
Proceeding of Text Analysis Conference (TAC).

Edgar Meij, Wouter Weerkamp, and Maarten de Ri-
jke. 2012. Adding Semantics to Microblog Posts.
In Proceedings of the Fifth ACM International Con-
ference on Web Search and Data Mining (WSDM),
pages 563–572.

Rada Mihalcea and Andras Csomai. 2007. Wikify!:
Linking Documents to Encyclopedic Knowledge. In
Proceedings of the Sixteenth ACM Conference on
Information and Knowledge Management (CIKM),
pages 233–242.

Tomas Mikolov, Greg Corrado, Kai Chen, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of the
International Conference on Learning Representa-
tions (ICLR), pages 1–12.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013b. Distributed Represen-
tations of Words and Phrases and their Composition-
ality. In Advances in Neural Information Processing
Systems (NIPS), pages 3111–3119.

258



David Milne and Ian H. Witten. 2008a. An Effec-
tive, Low-Cost Measure of Semantic Relatedness
Obtained from Wikipedia Links. In Proceedings of
the First AAAI Workshop on Wikipedia and Artificial
Intelligence (WIKIAI).

David Milne and Ian H. Witten. 2008b. Learning to
Link with Wikipedia. In Proceeding of the 17th
ACM Conference on Information and Knowledge
Management (CIKM), pages 509–518.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global Vectors for Word
Representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543.

Maria Pershina, Yifan He, and Ralph Grishman. 2015.
Personalized Page Rank for Named Entity Disam-
biguation. In Proceedings of the 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL-HLT), pages 238–243.

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and Global Algorithms for Dis-
ambiguation to Wikipedia. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, volume 1, pages 1375–1384.

Sameer Singh, Amarnag Subramanya, Fernando
Pereira, and Andrew McCallum. 2012. Wikilinks:
A Large-scale Cross-Document Coreference Corpus
Labeled via Links to Wikipedia. Technical Report
UM-CS-2012-015.

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning With Neural Ten-
sor Networks for Knowledge Base Completion. In
Advances in Neural Information Processing Systems
(NIPS), pages 926–934.

Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou
Ji, and Xiaolong Wang. 2015. Modeling Mention,
Context and Entity with Neural Networks for En-
tity Disambiguation. In Proceedings of the Twenty-
Fourth International Joint Conference on Artificial
Intelligence (IJCAI), pages 1333–1339.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge Graph and Text Jointly
Embedding. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 1591–1601.

Yadollah Yaghoobzadeh and Hinrich Schütze. 2015.
Corpus-level fine-grained entity typing using con-
textual information. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 715–725.

259


