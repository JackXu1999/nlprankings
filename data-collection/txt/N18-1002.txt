



















































Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss


Proceedings of NAACL-HLT 2018, pages 16–25
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Neural Fine-Grained Entity Type Classification
with Hierarchy-Aware Loss

Peng Xu
Department of Computing Science

University of Alberta
Edmonton, Canada

pxu4@ualberta.ca

Denilson Barbosa
Department of Computing Science

University of Alberta
Edmonton, Canada

denilson@ualberta.ca

Abstract

The task of Fine-grained Entity Type Clas-
sification (FETC) consists of assigning types
from a hierarchy to entity mentions in text. Ex-
isting methods rely on distant supervision and
are thus susceptible to noisy labels that can be
out-of-context or overly-specific for the train-
ing sentence. Previous methods that attempt
to address these issues do so with heuristics or
with the help of hand-crafted features. Instead,
we propose an end-to-end solution with a neu-
ral network model that uses a variant of cross-
entropy loss function to handle out-of-context
labels, and hierarchical loss normalization to
cope with overly-specific ones. Also, previous
work solve FETC a multi-label classification
followed by ad-hoc post-processing. In con-
trast, our solution is more elegant: we use pub-
lic word embeddings to train a single-label that
jointly learns representations for entity men-
tions and their context. We show experimen-
tally that our approach is robust against noise
and consistently outperforms the state-of-the-
art on established benchmarks for the task.

1 Introduction

Fine-grained Entity Type Classification (FETC)
aims at labeling entity mentions in context with
one or more specific types organized in a hier-
archy (e.g., actor as a subtype of artist, which
in turn is a subtype of person). Fine-grained
types help in many applications, including rela-
tion extraction (Mintz et al., 2009), question an-
swering (Li and Roth, 2002), entity linking (Lin
et al., 2012), knowledge base completion (Dong
et al., 2014) and entity recommendation (Yu et al.,
2014). Because of the high cost in labeling large
training corpora with fine-grained types, current
FETC systems resort to distant supervision (Mintz
et al., 2009) and annotate mentions in the train-
ing corpus with all types associated with the en-
tity in a knowledge graph. This is illustrated in

Figure 1, with three training sentences about en-
tity Steve Kerr. Note that while the entity be-
longs to three fine-grained types (person, athlete,
and coach), some sentences provide evidence of
only some of the types: person and coach from
S1, person and athlete from S2, and just person
for S3. Clearly, direct distant supervision leads to
noisy training data which can hurt the accuracy of
the FETC model.

One kind of noise introduced by distant super-
vision is assigning labels that are out-of-context
(athlete in S1 and coach in S2) for the sentence.
Current FETC systems sidestep the issue by ei-
ther ignoring out-of-context labels or using simple
pruning heuristics like discarding training exam-
ples with entities assigned to multiple types in the
knowledge graph. However, both strategies are in-
elegant and hurt accuracy. Another source of noise
introduced by distant supervision is when the type
is overly-specific for the context. For instance, ex-
ample S3 does not support the inference that Mr.
Kerr is either an athlete or a coach. Since existing
knowledge graphs give more attention to notable
entities with more specific types, overly-specific
labels bias the model towards popular subtypes in-
stead of generic ones, i.e., preferring athlete over
person. Instead of correcting for this bias, most
existing FETC systems ignore the problem and
treat each type equally and independently, ignor-
ing that many types are semantically related.

Besides failing to handle noisy training data
there are two other limitations of previous FETC
approaches we seek to address. First, they rely on
hand-crafted features derived from various NLP
tools; therefore, the inevitable errors introduced
by these tools propagate to the FETC systems
via the training data. Second, previous systems
treat FETC as a multi-label classification problem:
during type inference they predict a plausibility
score for each type, and, then, either classify types

16



Figure 1: With distant supervision, all the three mentions of Steve Kerr shown are labeled with the same types in
oval boxes in the target type hierarchy. While only part of the types are correct: person and coach for S1, person
and athlete for S2, and just person for S3.

with scores above a threshold (Mintz et al., 2009;
Gillick et al., 2014; Shimaoka et al., 2017) or per-
form a top-down search in the given type hierarchy
(Ren et al., 2016a; Abhishek et al., 2017).

Contributions: We propose a neural network
based model to overcome the drawbacks of exist-
ing FETC systems mentioned above. With pub-
licly available word embeddings as input, we learn
two different entity representations and use bidi-
rectional long-short term memory (LSTM) with
attention to learn the context representation. We
propose a variant of cross entropy loss function to
handle out-of-context labels automatically during
the training phase. Also, we introduce hierarchical
loss normalization to adjust the penalties for corre-
lated types, allowing our model to understand the
type hierarchy and alleviate the negative effect of
overly-specific labels.

Moreover, in order to simplify the problem and
take advantage of previous research on hierar-
chical classification, we transform the multi-label
classification problem to a single-label classifica-
tion problem. Based on the assumption that each
mention can only have one type-path depending
on the context, we leverage the fact that type hier-
archies are forests, and represent each type-path
uniquely by the terminal type (which might not
be a leaf node). For Example, type-path root-
person-coach can be represented as just coach,
while root-person can be unambiguously repre-
sented as the non-leaf person.

Finally, we report on an experimental validation
against the state-of-the-art on established bench-

marks that shows that our model can adapt to noise
in training data and consistently outperform previ-
ous methods. In summary, we describe a single,
much simpler and more elegant neural network
model that attempts FETC “end-to-end” without
post-processing or ad-hoc features and improves
on the state-of-the-art for the task.

2 Related Work

Fine-Grained Entity Type Classification: The
first work to use distant supervision (Mintz et al.,
2009) to induce a large - but noisy - training set
and manually label a significantly smaller dataset
to evaluate their FETC system, was Ling and Weld
(2012) who introduced both a training and evalu-
ation dataset FIGER (GOLD). They used a linear
classifier perceptron for multi-label classification.
While initial work largely assumed that mention
assignments could be done independently of the
mention context, Gillick et al. (2014) introduced
the concept of context-dependent FETC where
the types of a mention are constrained to what
can be deduced from its context and introduced a
new OntoNotes-derived (Weischedel et al., 2011)
manually annotated evaluation dataset. In addi-
tion, they addressed the problem of label noise in-
duced by distant supervision and proposed three
label cleaning heuristics. Yogatama et al. (2015)
proposed an embedding-based model where user-
defined features and labels were embedded into a
low dimensional feature space to facilitate infor-
mation sharing among labels. Ma et al. (2016)
presented a label embedding method that incor-

17



Attentive AFET LNR AAA NFETC
no hand-crafted features — — —
uses attentive neural network — — —
adopts single label setting — — — —
handles out-of-context noise —
handles overly-specifc noise — —

Table 1: Summary comparison to related FETC work. FETC systems listed in the table: (1) Attentive (Shimaoka
et al., 2017); (2) AFET (Ren et al., 2016a); (3) LNR (Ren et al., 2016b); (4) AAA (Abhishek et al., 2017).

porates prototypical and hierarchical information
to learn pre-trained label embeddings and adpated
a zero-shot framework that can predict both seen
and previously unseen entity types.

Shimaoka et al. (2016) proposed an attentive
neural network model that used LSTMs to encode
the context of an entity mention and used an at-
tention mechanism to allow the model to focus on
relevant expressions in such context. Shimaoka
et al. (2017) summarizes many neural architec-
tures for FETC task. These models ignore the out-
of-context noise, that is, they assume that all labels
obtained via distant supervision are “correct” and
appropriate for every context in the training cor-
pus. In our paper, a simple yet effective variant of
cross entropy loss function is proposed to handle
the problem of out-of-context noise.

Ren et al. (2016a) have proposed AFET, an
FETC system, that separates the loss function for
clean and noisy entity mentions and uses label-
label correlation information obtained by given
data in its parametric loss function. Considering
the noise reduction aspects for FETC systems, Ren
et al. (2016b) introduced a method called LNR to
reduce label noise without data loss, leading to
significant performance gains on both the evalu-
ation dataset of FIGER(GOLD) and OntoNotes.
Although these works consider both out-of-context
noise and overly-specific noise, they rely on hand-
crafted features which become an impediment to
further improvement of the model performance.
For LNR, because the noise reduction step is sep-
arated from the FETC model, the inevitable errors
introduced by the noise reduction will be propa-
gated into the FETC model which is undesirable.
In our FETC system, we handle the problem in-
duced from irrelevant noise and overly-specific
noise seamlessly inside the model and avoid the
usage of hand-crafted features.

Most recently, following the idea from AFET,
Abhishek et al. (2017) proposed a simple neu-
ral network model which incorporates noisy la-
bel information using a variant of non-parametric

hinge loss function and gain great performance
improvement on FIGER(GOLD). However, their
work overlooks the effect of overly-specific noise,
treating each type label equally and independently
when learning the classifiers and ignores possible
correlations among types.

Hierarchical Loss Function: Due to the intrin-
sic type hierarchy existing in the task of FETC,
it is natural to adopt the idea of hierarchical loss
function to adjust the penalties for FETC mistakes
depending on how far they are in the hierarchy.
The penalty for predicting person instead of ath-
lete should less than the penalty for predicting or-
ganization. To the best of our knowledge, the first
use of a hierarchical loss function was originally
introduced in the context of document categoriza-
tion with support vector machines (Cai and Hof-
mann, 2004). However, that work assumed that
weights to control the hierarchical loss would be
solicited from domain experts, which is inappli-
cable for FETC. Instead, we propose a method
called hierarchical loss normalization which can
overcome the above limitations and be incorpo-
rated with cross entropy loss used in our neural
architecture.

Table 1 provides a summary comparison of our
work against the previous state-of-the-art in fine
grained entity typing.

3 Background and Problem

Our task is to automatically reveal the type infor-
mation for entity mentions in context. The input
is a knowledge graph Ψ with schema YΨ, whose
types are organized into a type hierarchy Y , and an
automatically labeled training corpus D obtained
by distant supervision with Y . The output is a
type-path in Y for each named entity mentioned
in a test sentence from a corpus Dt.

More precisely, a labeled corpus for entity type
classification consists of a set of extracted entity
mentions {mi}Ni=1 (i.e., token spans representing
entities in text), the context (e.g., sentence, para-
graph) of each mention {ci}Ni=1, and the candidate

18



type sets {Yi}Ni=1 automatically generated for each
mention.

We represent the training corpus using a set of
mention-based triples D = {(mi, ci,Yi)}Ni=1.

If Yi is free of out-of-context noise, the type la-
bels for each mi should form a single type-path in
Yi. However, Yi may contain type-paths that are
irrelevant to mi in ci if there exists out-of-context
noise.

We denote the type set including all terminal
types for each type-path as the target type set Yti .
In the example type hierarchy shown in Figure 1,
if Yi contains types person, athlete, coach, Yti
should contain athlete, coach, but not person.
In order to understand the trade-off between the
effect of out-of-context noise and the size of the
training set, we report on experiments with two
different training sets: Dfiltered only with triples
whose Yi form a single type-path in D, and Draw
with all triples.

We formulate fine-grained entity classification
problem as follows:

Definition 1 Given an entity mention mi =
(wp, . . . , wt) (p, t ∈ [1, T ], p ≤ t) and its context
ci = (w1, . . . , wT ) where T is the context length,
our task is to predict its most specific type ŷi de-
pending on the context.

In practice, ci is generated by truncating the
original context with words beyond the context
window size C both to the left and to the right of
mi. Specifically, we compute a probability distri-
bution over all theK = |Y| types in the target type
hierarchy Y . The type with the highest probability
is classified as the predicted type ŷi which is the
terminal type of the predicted type-path.

4 Methodology

This section details our Neural Fine-Grained En-
tity Type Classification (NFETC) model.

4.1 Input Representation
As stated in Section 3, the input is an entity men-
tion mi with its context ci. First, we transform
each word in the context ci into a real-valued vec-
tor to provide lexical-semantic features. Given a
word embedding matrix Wwrd of size dw × |V |,
where V is the input vocabulary and dw is the size
of word embedding, we map every wi to a column
vector wdi ∈ Rdw .

To additionally capture information about the
relationship to the target entities, we incorporate

word position embeddings (Zeng et al., 2014) to
reflect relative distances between the i-th word
to the entity mention. Every relative distance is
mapped to a randomly initialized position vector
in Rdp , where dp is the size of position embedding.
For a given word, we obtain the position vector
wpi . The overall embedding for the i-th word is
wEi = [(w

d
i )
>, (wpi )

>]>.

4.2 Context Representation

For the context ci, we want to apply a non-linear
transformation to the vector representation of ci to
derive a context feature vector hi = f(ci; θ) given
a set of parameters θ. In this paper, we adopt bidi-
rectional LSTM with ds hidden units as f(ci; θ).
The network contains two sub-networks for the
forward pass and the backward pass respectively.
Here, we use element-wise sum to combine the
forward and backward pass outputs. The output of
the i-th word in shown in the following equation:

hi = [
−→
hi ⊕

←−
hi ] (1)

Following Zhou et al. (2016), we employ
word-level attention mechanism, which makes our
model able to softly select the most informative
words during training. Let H be a matrix con-
sisting of output vectors [h1, h2, . . . , hT ] that the
LSTM produced. The context representation r is
formed by a weighted sum of these output vectors:

G = tanh(H) (2)

α = softmax(w>G) (3)

rc = Hα
> (4)

where H ∈ Rds×T , w is a trained parameter vec-
tor. The dimension ofw,α, rc are ds, T, ds respec-
tively.

4.3 Mention Representation

Averaging encoder: Given the entity mention
mi = (wp, . . . , wt) and its length L = t − p + 1,
the averaging encoder computes the average word
embedding of the words in mi. Formally, the av-
eraging representation ra of the mention is com-
puted as follows:

ra =
1

L

t∑

i=p

wdi (5)

19



Figure 2: The architecture of the NFETC model.

This relatively simple method for composing
the mention representation is motivated by it being
less prone to overfitting (Shimaoka et al., 2017).

LSTM encoder: In order to capture more se-
mantic information from the mentions, we add
one token before and another after the target en-
tity to the mention. The extended mention can be
represented as m∗i = (wp−1, wp, . . . , wt, wt+1).
The standard LSTM is applied to the mention se-
quence from left to right and produces the outputs
hp−1, . . . , ht+1. The last output ht+1 then serves
as the LSTM representation rl of the mention.

4.4 Optimization

We concatenate context representation and two
mention representations together to form the over-
all feature representation of the input R =
[rc, ra, rl]. Then we use a softmax classifier to
predict ŷi from a discrete set of classes for a en-
tity mention m and its context c with R as input:

p̂(y|m, c) = softmax(WR+ b) (6)
ŷ = arg max

y
p̂(y|m, c) (7)

where W can be treated as the learned type em-
beddings and b is the bias.

The traditional cross-entropy loss function is
represented as follows:

J(θ) = − 1
N

N∑

i=1

log(p̂(yi|mi, ci)) + λ‖Θ‖2 (8)

where yi is the only element in Yti and
(mi, ci,Yi) ∈ Dfiltered. λ is an L2 regulariza-
tion hyperparameter and Θ denotes all parameters
of the considered model.

In order to handle data with out-of-context noise
(in other words, with multiple labeled types) and
take full advantage of them, we introduce a simple
yet effective variant of the cross-entropy loss:

J(θ) = − 1
N

N∑

i=1

log(p̂(y∗i |mi, ci)) + λ‖Θ‖2 (9)

where y∗i = arg maxy∈Yti p̂(y|mi, ci) and
(mi, ci,Yi) ∈ Draw. With this loss function, we
assume that the type with the highest probability
among Yti during training as the correct type. If
there is only one element in Yti , this loss function
is equivalent to the cross-entropy loss function.
Wherever there are multiple elements, it can filter
the less probable types based on the local context
automatically.

20



4.5 Hierarchical Loss Normalization
Since the fine-grained types tend to form a for-
est of type hierarchies, it is unreasonable to treat
every type equally. Intuitively, it is better to pre-
dict an ancestor type of the true type than some
other unrelated type. For instance, if one exam-
ple is labeled as athlete, it is reasonable to predict
its type as person. However, predicting other high
level types like location or organization would be
inappropriate. In other words, we want the loss
function to penalize less the cases where types are
related. Based on the above idea, we adjust the
estimated probability as follows:

p∗(ŷ|m, c) = p(ŷ|m, c) + β ∗
∑

t∈Γ
p(t|m, c) (10)

where Γ is the set of ancestor types along the
type-path of ŷ, β is a hyperparameter to tune the
penalty. Afterwards, we re-normalize it back to
a probability distribution, a process which we de-
note as hierarchical loss normalization.

As discussed in Section 1, there exists overly-
specific noise in the automatically labeled training
sets which hurt the model performance severely.
With hierarchical loss normalization, the model
will get less penalty when it predicts the ac-
tual type for one example with overly-specific
noise. Hence, it can alleviate the negative effect
of overly-specific noise effectively. Generally, hi-
erarchical loss normalization can make the model
somewhat understand the given type hierarchy and
learn to detect those overly-specific cases. Dur-
ing classification, it will make the models prefer
generic types unless there is a strong indicator for
a more specific type in the context.

4.6 Regularization
Dropout, proposed by Hinton et al. (2012), pre-
vents co-adaptation of hidden units by randomly
omitting feature detectors from the network dur-
ing forward propagation. We employ both input
and output dropout on LSTM layers. In addition,
we constrain L2-norms for the weight vectors as
shown in Equations 8, 9 and use early stopping to
decide when to stop training.

5 Experiments

This section reports an experimental evaluation of
our NFETC approach using the previous state-of-
the-art as baselines.

FIGER(GOLD) OntoNotes
# types 113 89
# raw training mentions 2009898 253241
# raw testing mentions 563 8963
% filtered training mentions 64.46 73.13
% filtered testing mentions 88.28 94.00
Max hierarchy depth 2 3

Table 2: Statistics of the datasets

5.1 Datasets
We evaluate the proposed model on two standard
and publicly available datasets, provided in a pre-
processed tokenized format by Shimaoka et al.
(2017). Table 2 shows statistics about the bench-
marks. The details are as follows:

• FIGER(GOLD): The training data consists
of Wikipedia sentences and was automati-
cally generated with distant supervision, by
mapping Wikipedia identifiers to Freebase
ones. The test data, mainly consisting of
sentences from news reports, was manually
annotated as described by Ling and Weld
(2012).

• OntoNotes: The OntoNotes dataset con-
sists of sentences from newswire docu-
ments present in the OntoNotes text cor-
pus (Weischedel et al., 2013). DBpedia spot-
light (Daiber et al., 2013) was used to auto-
matically link entity mention in sentences to
Freebase. Manually annotated test data was
shared by Gillick et al. (2014).

Because the type hierarchy can be somewhat
understood by our proposed model, the quality
of the type hierarchy can also be a key factor to
the performance of our model. We find that the
type hierarchy for FIGER(GOLD) dataset follow-
ing Freebase has some flaws. For example, soft-
ware is not a subtype of product and government
is not a subtype of organization. Following the
proposed type hierarchy of Ling and Weld (2012),
we refine the Freebase-based type hierarchy. The
process is a one-to-one mapping for types in the
original dataset and we didn’t add or drop any type
or sentence in the original dataset. As a result, we
can directly compare the results of our proposed
model with or without this refinement.

Aside from the advantages brought by adopt-
ing the single label classification setting, we can
see one disadvantage of this setting based on Ta-
ble 2. That is, the performance upper bounds of

21



our proposed model are no longer 100%: for ex-
ample, the best strict accuracy we can get in this
setting is 88.28% for FIGER(GOLD). However,
as the strict accuracy of state-of-the-art methods
are still nowhere near 80% (Table 3), the evalua-
tion we perform is still informative.

5.2 Baselines

We compared the proposed model with state-of-
the-art FETC systems 1: (1) Attentive (Shimaoka
et al., 2017); (2) AFET (Ren et al., 2016a); (3)
LNR+FIGER (Ren et al., 2016b); (4) AAA (Ab-
hishek et al., 2017).

We compare these baselines with variants of
our proposed model: (1) NFETC(f): basic neu-
ral model trained on Dfiltered (recall Section 4.4);
(2) NFETC-hier(f): neural model with hierarich-
cal loss normalization trained on Dfiltered. (3)
NFETC(r): neural model with proposed vari-
ant of cross-entropy loss trained on Draw; (4)
NFETC-hier(r): neural model with proposed
variant of cross-entropy loss and hierarchical loss
normalization trained on Draw.

5.3 Experimental Setup

For evaluation metrics, we adopt the same crite-
ria as Ling and Weld (2012), that is, we evaluate
the model performance by strict accuracy, loose
macro, and loose micro F-scores. These measures
are widely used in existing FETC systems (Shi-
maoka et al., 2017; Ren et al., 2016b,a; Abhishek
et al., 2017).

We use pre-trained word embeddings that were
not updated during training to help the model gen-
eralize to words not appearing in the training set.
For this purpose, we used the freely available
300-dimensional cased word embedding trained
on 840 billion tokens from the Common Crawl
supplied by Pennington et al. (2014). For both
datasets, we randomly sampled 10% of the test set
as a development set, on which we do the hyper-
parameters tuning. The remaining 90% is used for
final evaluation. We run each model with the well-
tuned hyperparameter setting five times and report
their average strict accuracy, macro F1 and micro
F1 on the test set. The proposed model was imple-
mented using the TensorFlow framework. 2

1The results of the baselines are all as reported in their
corresponding papers.

2The code to replicate the work is available at: https:
//github.com/billy-inn/NFETC

Parameter FIGER(GOLD) OntoNotes
lr 0.0002 0.0002
dp 85 20
ds 180 440
pi 0.7 0.5
po 0.9 0.5
λ 0.0 0.0001
β 0.4 0.3

Table 4: Hyperparameter Settings

5.4 Hyperparameter Setting

In this paper, we search different hyperparameter
settings for FIGER(GOLD) and OntoNotes sepa-
rately, considering the differences between the two
datasets. The hyperparameters include the learn-
ing rate lr for Adam Optimizer, size of word po-
sition embeddings (WPE) dp, state size for LSTM
layers ds, input dropout keep probability pi and
output dropout keep probability po for LSTM lay-
ers 3, L2 regularization parameter λ and parame-
ter to tune hierarchical loss normalization β. The
values of these hyperparameters, obtained by eval-
uating the model performance on the development
set, for each dataset can be found in Table 4.

5.5 Performance comparison and analysis

Table 3 compares our models with other state-
of-the-art FETC systems on FIGER(GOLD) and
OntoNotes. The proposed model performs better
than the existing FETC systems, consistently on
both datasets. This indicates benefits of the pro-
posed representation scheme, loss function and hi-
erarchical loss normalization.

Discussion about Out-of-context Noise: For
dataset FIGER(GOLD), the performance of our
model with the proposed variant of cross-entropy
loss trained onDraw is significantly better than the
basic neural model trained on Dfiltered, suggest-
ing that the proposed variant of the cross-entropy
loss function can make use of the data with out-
of-context noise effectively. On the other hand,
the improvement introduced by our proposed vari-
ant of cross-entropy loss is not as significant for
the OntoNotes benchmark. This may be caused
by the fact that OntoNotes is much smaller than
FIGER(GOLD) and proportion of examples with-
out out-of-context noise are also higher, as shown
in Table 2.

3Following TensorFlow terminology.

22



FIGER(GOLD) OntoNotes
Model Strict Acc. Macro F1 Micro F1 Strict Acc. Macro F1 Micro F1
Attentive 59.68 78.97 75.36 51.74 70.98 64.91
AFET 53.3 69.3 66.4 55.1 71.1 64.7
LNR+FIGER 59.9 76.3 74.9 57.2 71.5 66.1
AAA 65.8 81.2 77.4 52.2 68.5 63.3
NFETC(f) 57.9± 1.3 78.4± 0.8 75.0± 0.7 54.4± 0.3 71.5± 0.4 64.9± 0.3
NFETC-hier(f) 68.0± 0.8 81.4± 0.8 77.9± 0.7 59.6± 0.2 76.1± 0.2 69.7± 0.2
NFETC(r) 56.2± 1.0 77.2± 0.9 74.3± 1.1 54.8± 0.4 71.8± 0.4 65.0± 0.4
NFETC-hier(r) 68.9± 0.6 81.9± 0.7 79.0± 0.7 60.2± 0.2 76.4± 0.1 70.2± 0.2

Table 3: Strict Accuracy, Macro F1 and Micro F1 for the models tested on the FIGER(GOLD) and OntoNotes
datasets.

Test Sentence Ground Truth
S1: Hopkins said four fellow elections is curious , considering the . . . Person
S2: . . . for WiFi communications across all the SD cards. Product
S3: A handful of professors in the UW Department of Chemistry . . . Educational Institution
S4: Work needs to be done and, in Washington state, . . . Province
S5: ASC Director Melvin Taing said that because the commission is . . . Organization

Table 5: Examples of test sentences in FIGER(GOLD) where the entity mentions are marked as bold italics.

Investigations on Overly-Specific Noise: With
hierarchical loss normalization, the performance
of our models are consistently better no matter
whether trained on Draw or Dfiltered on both
datasets, demonstrating the effectiveness of this
hierarchical loss normalization and showing that
overly-specific noise has a potentially significant
influence on the performance of FETC systems.

5.6 T-SNE Visualization of Type Embeddings
By visualizing the learned type embeddings (Fig-
ure 3), we can observe that the parent types are
mixed with their subtypes and forms clear distinct
clusters without hierarchical loss normalization,
making it hard for the model to distinguish sub-
types like actor or athlete from their parent types
person. This also biases the model towards the
most popular subtype. While the parent types tend
to cluster together and the general pattern is more
complicated with hierarchical loss normalization.
Although it’s not as easy to interpret, it hints that
our model can learn rather subtle intricacies and
correlations among types latent in the data with the
help of hierarchical loss normalization, instead of
sticking to a pre-defined hierarchy.

5.7 Error Analysis on FIGER(GOLD)
Since there are only 563 sentences for testing in
FIGER(GOLD), we look into the predictions for

all the test examples of all variants of our model.
Table 5 shows 5 examples of test sentence. With-
out hierarchical loss normalization, our model will
make too aggressive predictions for S1 with Politi-
cian and for S2 with Software. This kind of mis-
takes are very common and can be effectively re-
duced by introducing hierarchical loss normaliza-
tion leading to significant improvements on the
model performance. Using the changed loss func-
tion to handle multi-label (noisy) training data can
help the model distinguish ambiguous cases. For
example, our model trained on Dfiltered will mis-
classify S5 as Title, while the model trained on
Draw can make the correct prediction.

However, there are still some errors that can’t
be fixed with our model. For example, our model
cannot make correct predictions for S3 and S4 due
to the fact that our model doesn’t know that UW is
an abbreviation of University of Washington and
Washington state is the name of a province. In
addition, the influence of overly-specific noise can
only be alleviated but not eliminated. Sometimes,
our model will still make too aggressive or conser-
vative predictions. Also, mixing up very ambigu-
ous entity names is inevitable in this task.

23



−10 0 10 20

−
1

5
−

1
0

−
5

0
5

1
0

1
5

organization

location

event

person

organization location

product

building

art

organization
building

organization

organization

location

building

product

person
location

person

art

organization

building

location

product

person

locationperson

event

art

organization

location

product

location

building

organization

person

product

building

organization

organizationperson

person

person

person

art

person

person

product

productproduct

location

product

event

product

person

locationperson
location

person

organization

location

event

building

product

art

event

organization

location

event

event

organization

location

−10 −5 0 5 10

−
1

0
−

5
0

5
1

0
1

5

organization

location

event

person

organization

location

product

buildingart

organization

building

organization

organizationlocation

building

product

person

location

person

art

organization

building

location

product

person location

person

event
art

organization

location

product

location
building

organization

person

product

buildingorganization

organization

person

personperson

person

art

person

person

product

productproduct

location

product

event

product

person

location

person

location

person

organization

location

event

building

productart

event organization

location

event

event

organization

location

Figure 3: T-SNE visualization of the type embeddings
learned from FIGER(GOLD) dataset where subtypes
share the same color as their parent type. The seven
parent types are shown in the black boxes. The be-
low sub-figure uses the hierarchical loss normalization,
while the above not.

6 Conclusion and Further Work

In this paper, we studied two kinds of noise,
namely out-of-context noise and overly-specific
noise, for noisy type labels and investigate their
effects on FETC systems. We proposed a neural
network based model which jointly learns repre-
sentations for entity mentions and their context. A
variant of cross-entropy loss function was used to
handle out-of-context noise. Hierarchical loss nor-
malization was introduced into our model to alle-
viate the effect of overly-specific noise. Experi-
mental results on two publicly available datasets
demonstrate that the proposed model is robust to
these two kind of noise and outperforms previous
state-of-the-art methods significantly.

More work can be done to further develop hi-
erarchical loss normalization since currently it’s
very simple. Considering type information is valu-
able in various NLP tasks, we can incorporate re-
sults produced by our FETC system to other tasks,
such as relation extraction, to check our model’s
effectiveness and help improve other tasks’ per-

formance. In addition, tasks like relation extrac-
tion are complementary to the task of FETC and
therefore may have potentials to be digged to help
improve the performance of our system in return.

Acknowledgments

This work was supported in part by the Natu-
ral Sciences and Engineering Research Council of
Canada (NSERC).

References
Abhishek Abhishek, Ashish Anand, and Amit Awekar.

2017. Fine-grained entity type classification by
jointly learning representations and label embed-
dings. In Proceedings of the 15th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL) pages 797–807.

Lijuan Cai and Thomas Hofmann. 2004. Hierarchi-
cal document categorization with support vector ma-
chines. In Proceedings of the thirteenth ACM inter-
national conference on Information and knowledge
management. ACM, pages 78–87.

Joachim Daiber, Max Jakob, Chris Hokamp, and
Pablo N Mendes. 2013. Improving efficiency and
accuracy in multilingual entity extraction. Proceed-
ings of the 9th International Conference on Semantic
Systems pages 121–124.

Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowledge
vault: A web-scale approach to probabilistic knowl-
edge fusion. Proceedings of the 20th ACM SIGKDD
international conference on Knowledge discovery
and data mining pages 601–610.

Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse
Kirchner, and David Huynh. 2014. Context-
dependent fine-grained entity type tagging. arXiv
preprint arXiv:1412.1820 .

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580 .

Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of the 19th international con-
ference on Computational linguistics-Volume 1. As-
sociation for Computational Linguistics, pages 1–7.

Thomas Lin, Oren Etzioni, et al. 2012. No noun phrase
left behind: detecting and typing unlinkable enti-
ties. Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
pages 893–903.

24



Xiao Ling and Daniel S Weld. 2012. Fine-grained en-
tity recognition. AAAI .

Yukun Ma, Erik Cambria, and Sa Gao. 2016. La-
bel embedding for zero-shot fine-grained named en-
tity typing. In Proceedings of COLING 2016, the
26th International Conference on Computational
Linguistics: Technical Papers. pages 171–180.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP pages 1003–
1011.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. EMNLP 14(1532–1543).

Xiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng
Ji, and Jiawei Han. 2016a. Afet: Automatic fine-
grained entity typing by hierarchical partial-label
embedding. EMNLP 16(17).

Xiang Ren, Wenqi He, Meng Qu, Clare R Voss, Heng
Ji, and Jiawei Han. 2016b. Label noise reduction in
entity typing by heterogeneous partial-label embed-
ding. KDD .

Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and
Sebastian Riedel. 2016. An attentive neural ar-
chitecture for fine-grained entity type classification.
arXiv preprint arXiv:1604.05525 .

Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and
Sebastian Riedel. 2017. Neural architectures for
fine-grained entity type classification. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL) .

Ralph Weischedel, Eduard Hovy, Mitchell Mar-
cus, Martha Palmer, Robert Belvin, Sameer Prad-
han, Lance Ramshaw, and Nianwen Xue. 2011.
Ontonotes: A large training corpus for enhanced
processing. Handbook of Natural Language Pro-
cessing and Machine Translation. Springer .

Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, et al. 2013. Ontonotes release 5.0
ldc2013t19. Linguistic Data Consortium, Philadel-
phia, PA .

Dani Yogatama, Daniel Gillick, and Nevena Lazic.
2015. Embedding methods for fine grained entity
type classification. ACL (2) pages 291–296.

Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan
Gu, Bradley Sturt, Urvashi Khandelwal, Brandon
Norick, and Jiawei Han. 2014. Personalized entity
recommendation: A heterogeneous information net-
work approach. Proceedings of the 7th ACM inter-
national conference on Web search and data mining
pages 283–292.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
Jun Zhao, et al. 2014. Relation classification via
convolutional deep neural network. COLING pages
2335–2344.

Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
based bidirectional long short-term memory net-
works for relation classification. The 54th Annual
Meeting of the Association for Computational Lin-

guistics .

25


