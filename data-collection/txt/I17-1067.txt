



















































Measuring Semantic Relations between Human Activities


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 664–673,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Measuring Semantic Relations between Human Activities

Steven R. Wilson and Rada Mihalcea
University of Michigan

{steverw|mihalcea}@umich.edu

Abstract

The things people do in their daily lives
can provide valuable insights into their
personality, values, and interests. Unstruc-
tured text data on social media platforms
are rich in behavioral content, and auto-
mated systems can be deployed to learn
about human activity on a broad scale if
these systems are able to reason about the
content of interest. In order to aid in the
evaluation of such systems, we introduce
a new phrase-level semantic textual sim-
ilarity dataset comprised of human activ-
ity phrases, providing a testbed for auto-
mated systems that analyze relationships
between phrasal descriptions of people’s
actions. Our set of 1,000 pairs of activi-
ties is annotated by human judges across
four relational dimensions including simi-
larity, relatedness, motivational alignment,
and perceived actor congruence. We eval-
uate a set of strong baselines for the task
of generating scores that correlate highly
with human ratings, and we introduce sev-
eral new approaches to the phrase-level
similarity task in the domain of human ac-
tivities.

1 Introduction

Our everyday behaviors say a lot about who we
are. The things we do are related to our personality
(Ajzen, 1987), values (Rokeach, 1973), interests
(Goecks and Shavlik, 2000), and what we are go-
ing to do next (Ouellette and Wood, 1998). While
we cannot always directly observe what people are
doing on a day-to-day basis, we have access to
a large number of unstructured text sources that
describe real-world human activity, such as news
outlets and social media sites. Fiction and non-
fiction writings often revolve around the things
that people do, and even encyclopedic texts can

be rich in descriptions of human activities. Al-
though many common sources of text contain hu-
man activities, reasoning about these activities and
their relationships to one another is not a trivial
task. Descriptions of human actions are fraught
with ambiguity, subjectivity, and there are multi-
tudinous lexically distinct ways to express highly
similar events. If we want to gain useful insights
from these data, it should be beneficial to develop
effective systems that can successfully represent,
compare, and ultimately understand human activ-
ity phrases.

In this paper, we consider the task of automati-
cally determining the strength of a relationship be-
tween two human activities,1 which can be helpful
in reasoning about texts rich with activity-based
content. The relationship between activities might
be similarity in a strict sense, such as watching a
film and seeing a movie, or a more general related-
ness, such as the relationship between turn on an
oven and bake a pie. Another way to categorize a
pair of activities is by the degree to which they are
typically done with a similar motivation, like eat-
ing dinner with family and visiting relatives. Or,
in order to uncover which other behaviors a per-
son is likely to exhibit, it might be useful to deter-
mine how likely a person might be to do an activity
given some information about previous real-world
actions that they have taken.

Success on our proposed task will be a valu-
able step forward for multiple lines of research, es-
pecially within the computational social sciences
where human behavior and its relation to other
variables (e.g., personality traits, personal values,
or political orientation) is a key focus. Since the
language human activities is so varied, it is not
enough to store exact representations of activity

1Throughout this paper, we use the word “activity” to re-
fer to what a person does or has done. Unlike the typical use
of this term in the computer vision community, in this pa-
per we use it in a broad sense, to also encompass non-visual
activities such as “make vacation plans” or “have a dream”.

664



phrases that are unlikely to appear many times.
It would be useful to instead have methods that
can automatically find related phrases and group
them based on one (or more) of several dimen-
sions of interest. Moreover, the ability to auto-
matically group related activities will also bene-
fit research in video-based and multimodal human
activity recognition where there is need for infer-
ence about activities based on their relationships
to one another.

Reasoning about the relationships between ac-
tivity phrases brings with it many of the difficulties
often associated with phrase-level semantic simi-
larity tasks. It is not enough to know that the two
phrases share a root verb, as the semantic weight
of verbs can vary, such as the word “go” in the
phrases go to a bar and go to a church. While
these phrases have high lexical overlap and are
similar in that they both describe a traveling type
of activity, they are usually done for different mo-
tivations and are associated with different sets of
other activities. In this case, we could only con-
sider the main nouns (i.e., “bar” and “church”),
but that approach would cause difficulties when
dealing with other phrases such as sell a car and
drive a car, which both involve an automobile but
describe dissimilar actions. Therefore, successful
systems should be able to properly focus on the
most semantically relevant tokens with a phrase.
A final challenge when dealing with human activ-
ity phrase relations is evaluation. There should be
a good way to determine the effectiveness of a sys-
tem’s ability to measure relations between these
types of phrases, yet other commonly used se-
mantic similarity testbeds (e.g., those presented in
various Semeval tasks (Agirre et al., 2012, 2013;
Marelli et al., 2014)) are not specifically focused
on the domain of human activities. Currently, it is
unclear whether or not the top-performing systems
on general phrase similarity tasks will necessarily
lead to the best results when looking specifically
at human activity phrases.

To address these challenges, we introduce a new
task in automatically identifying the strength of
human activity phrase relations. We construct
a dataset consisting of pairs of activities report-
edly performed by actual people. The pairs that
we have collected aim specifically to showcase
diverse phenomena such as pairs containing the
same verb, a range of degrees of similarity and re-
latedness, pairs unlikely to be done by the same
type of person, and so forth. These pairs are each
annotated by multiple human judges across the

following four dimensions:

• Similarity: The degree to which the two ac-
tivity phrases describe the same thing. Here
we are seeking semantic similarity in a strict
sense. Example of high similarity phrases: to
watch a film and to see a movie.

• Relatedness: The degree to which the activ-
ities are related to one another. This relation-
ship describes a general semantic association
between two phrases. Example of strongly
related phrases: to give a gift and to receive a
present.

• Motivational Alignment: The degree to
which the activities are (typically) done with
similar motivations. Example of phrases with
potentially similar motivations: to eat dinner
with family members and to visit relatives.

• Perceived Actor Congruence: The degree
to which the activities are often done by
the same type of person. Put another way,
does knowing that a person often performs an
activity increase human judges’ expectation
that this person will also often do a second
activity? Example of activities that might be
expected to be done by the same person: to
pack a suitcase and to travel to another state.

These relational dimensions were selected to
cover a variety of types of relationships that may
hold between two activity phrases. This way, au-
tomated methods that capture slightly different no-
tions of similarity between phrases will potentially
be able to perform well when evaluated on dif-
ferent scales. While the dimensions are corre-
lated with one another, we show that they do in
fact measure different things. We provide a set of
benchmarks to show how well previously success-
ful phrase-level similarity systems perform on this
new task. Furthermore, we introduce several mod-
ifications and novel methods that lead to increased
performance on the task.

2 Related Work

Semantic similarity tasks have been recently dom-
inated by various methods that seek to embed
segments of text as vectors into some high-
dimensional space so that comparisons can be
made between them using cosine similarity or
other vector based metrics. While word embed-
dings have existed in various forms in the past
(Church and Hanks, 1990; Bengio et al., 2003),

665



many approaches used today draw inspiration di-
rectly from shallow neural network based models
such as those described in (Mikolov et al., 2013).2

In the common skip-gram variant of these neu-
ral embedding models, a neural network is trained
to predict a word given its context within some
fixed window size. (Levy and Goldberg, 2014a)
and (Bansal et al., 2014) extended the idea of con-
text to incorporate dependency structures into the
training process, leading to vectors that were able
to better capture certain types of long-distance
syntactic relationships. One of the major strengths
of neural word embedding methods is that they
are able to learn useful representations from ex-
tremely large corpora that can then be leveraged as
a source of semantic knowledge on other tasks of
interest, such as predicting word analogies (Pen-
nington et al., 2014) or the semantic similarity and
relatedness of word pairs (Huang et al., 2012).

Researchers have taken the powerful semi-
supervised ability of these word embedding meth-
ods to aid in tasks at the phrase-level, as well.
The most straightforward way to accomplish a
phrase-level representation is to use some binary
vector-level operation to compose pre-trained vec-
tor representations of individual words that belong
to a phrase (Mitchell and Lapata, 2010). Other
methods have sought to directly find embeddings
for larger sequences of words, such as (Le and
Mikolov, 2014) and (Kiros et al., 2015).

Semantic textual similarity tasks are often eval-
uated by computing the correlation between hu-
man judgements of similarity and machine out-
put. The wordsim353 (Finkelstein et al., 2001)
and simlex999 (Hill et al., 2016) resources provide
a set of human annotated pairs of words, labeled
for similarity and/or general association. Simverb-
3500 (Gerz et al., 2016) was introduced to pro-
vide researchers with a testbed for verb relations,
a specific yet important class of words that was
less common in earlier word-level similarity data
sets. SemEval has released a series of semantic
text similarity tasks at varying levels of granular-
ity, ranging from words to entire documents, such
as the SICK (Sentences Involving Compositional
Knowledge) dataset (Marelli et al., 2014) which is
specifically crafted to evaluate the ability of sys-
tems to effectively compose individual word se-
mantics in order to achieve the overall meaning of

2It is worth noting that (Levy and Goldberg, 2014b) show
that these embeddings are actually implicitly factorizing a
shifted version of a more traditional PMI word-context ma-
trix, which is similar to the word co-occurrence matrix fac-
torization approach used in (Pennington et al., 2014)).

a sentence. While many of these evaluation sets
contain human activities to some degree, they also
have contain other types of words or phrases due
to the way in which they were created. For exam-
ple, SICK contains actions done by animals such
as follow a fish. Similarly, Simverb-3500 contains
verbs that don’t necessarily describe human activ-
ities, like chirp and glow, and does not contain
phrase-level activities.

Several recent works have raised concerns over
the standard evaluation approaches used in seman-
tic textual similarity tasks. One potential issue is
the use of inadequate metrics depending on the
task that a practitioner is interested in tackling.
While the Pearson correlation between human-
judged similarity scores and predicted outputs is
often used, this type of correlation can be mis-
leading in the presence of outliers or nonlinear
relationships (Reimers et al., 2016). Remiers et
al. propose a framework for selecting a met-
ric for semantic text similarity tasks, which we
take into consideration when selecting our eval-
uation metric. Additionally, correlation with hu-
man judgments does not always give a good in-
dication of success on some downstream applica-
tions, the human ratings themselves are somewhat
subjective, and statistical significance is rarely re-
ported in comparisons of word embedding meth-
ods (Faruqui et al., 2016). However, our goal
in this work is not to evaluate the overall qual-
ity of distributional semantic models, but to find
a method that has high utility in the domain of hu-
man activity relations, and so we do rely on com-
parisons with human judges as a means of assess-
ment.

3 Data Collection and Annotation

One potential source of data containing people’s
self-reported descriptions of their activities is so-
cial media platforms, but these data are noisy and
require preprocessing steps that, being imperfect,
may propagate their own errors into the resulting
data. In order to get a set of cleaner activities
that people might actually talk about doing, we
directly asked Amazon Mechanical Turk (AMT)
workers to write short phrases describing five ac-
tivities that they had done in the past week. We
collected data from 1,000 people located in the
United States for a total of 5,000 activities. The ac-
tivity phrases were then normalized by converting
them to their infinitive form (without a preceding
”to”), correcting spelling errors, removing punc-
tuation, and converting all characters to lowercase.

666



Activity Prompt User Selection
pay the phone bill an activity that is EXTREMELY SIMILAR pay one’s student loan bill
play softball an activity that is SOMEWHAT SIMILAR go bowling
take a bath an activity that uses the SAME VERB take care of one’s ill spouse
smoke an activity that is RELATED, but not necessarily SIMILAR get sick and go to the doctor
go out for ice cream an activity that is NOT AT ALL SIMILAR cash a check

Table 1: Examples of activity/prompt pairs and the corresponding activities that were selected by the
annotators given the pair.

After removing duplicate entries (about 2,000) and
any phrases referring specifically to doing work on
AMT (e.g., those containing the tokens mTurk or
Turking, about 150 cases), we were left with a set
of 2,909 unique activity phrases.

We acknowledge that this methodology intro-
duces some bias since the workers all come from
the United States, and it is therefore likely that our
set of activity phrases describe things that are more
commonly done by Americans than people from
other regions. Furthermore, primacy and recency
effects (Murdock Jr, 1962) may bias the types of
items listed toward things done in the morning or
just before logging onto the AMT platform. Based
on this, we expect that our set of activities is not
necessarily a representative sample of everything
that people might do, but they are still descriptions
of actual activities that real humans have done and
are useful for our task.

3.1 Forming Pairs of Activities

Next, we sought to create pairs of activities that
showcase a variety of relationship types, includ-
ing varying degrees of similarity and relatedness.
To achieve this, we turned to another group to hu-
man annotators. After reading through a document
which oriented them to the task, the annotators
were given the full list of activities in addition to a
subset of randomly selected activity phrases. Each
of these phrases was randomly paired with one
of several possible prompts (see Table 1 for ex-
amples) which instructed the annotators how they
should select a second activity phrase from the
complete list in order to form a pair. Each prompt
was sampled an equal number of times in order to
make sure that the final set of pairs exhibited vari-
ous types of relationships to the same degree. All
annotators had access to a searchable copy of the
full list, but the order of the activities was shuffled
each time in order to avoid potential bias from the
annotators selecting phrases near the top of the list,
and a new shuffled version of the list was given
after every 25 pairs created. While a suitable sec-
ond activity phrase was not always present (e.g.,

no phrase in our dataset matches “an activity that
uses the SAME VERB” as choreograph a dance),
it is not crucial that all of these pairs fit the prompts
exactly since these are only intended to approxi-
mate various phenomena, and the final annotations
will be done without the knowledge of the prompts
used to generate the pairs. In total, 12 unique an-
notators created 1,000 pairs of phrases.

3.2 Annotating Activity Pairs

All of the activity phrase pairs were uploaded to
AMT in order to be labeled. For each pair, ten
workers were asked to rate the similarity, related-
ness, motivational alignment, and perceived actor
congruence on a 5-point Likert-type scales (a to-
tal of 40,000 annotated data points). The workers
were given a set of instructions that included de-
scriptions of the four types of relationships with
examples, including cases in which a pair might
be related but not similar, motivationally aligned
but not similar, etc. By asking the same set of
people to label all four relational dimensions for
a given pair, we hoped to make them cognizant of
the differences between the scales.

The first three relationships were prompted for
using the form: “To what degree are the two activ-
ities similar/related/of the same motivation?” and
were coded as 0 (e.g., for responses of “not at all
similar”) and the integers 1-4 with 4 representing
the strongest relationship. Perceived actor congru-
ence was solicited for using the form: “Person A
often does activity 1, while person B rarely does
activity 1. Who would you expect to do activity
2 more often?” with choices ranging from “Most
likely Person B” to “Most likely Person A.” Per-
ceived actor congruence ranges from -2 to 2 and
has the lowest score when Person B is chosen and
the highest when Person A is chosen. A score of
0 on this scale means that judges were unable to
determine whether Person A or Person B would
be more likely to perform the action being asked
about (i.e., activity 2). Each individual Human In-
telligence Task (HIT) posted to AMT required an
annotator to label 25 pairs so that we could reliably

667



Activity 1 Activity 2 SIM REL MA PAC
go jogging lift weights 1.67 2.22 2.89 1.11
read to one’s kids go to a bar 0 0 0 -1.29
take transit to work commute to work 3.38 3.5 3.38 0.5
make one’s bed organize one’s desk 0.58 1.29 1.57 0.71

Table 2: Sample activity phrase pairs and average human annotation scores given for the four dimensions:
Similarity (SIM), Relatedness (REL), Motivational Alignment (MA) and Perceived Actor Congruence
(PAC). SIM, REL, and MA are on a 0-4 scale, while PAC scores can range from -2 to 2.

SIM REL MA PAC
SIM 1.000 .962 .928 .735
REL 1.000 .932 .776
MA 1.000 .738
PAC 1.000

Table 3: Spearman correlations between the four
relational dimensions: Similarity (SIM), Related-
ness (REL), Motivational Alignment (MA) and
Perceived Actor Congruence (PAC).

compute agreement, and a worker could complete
as many HITs as they desired.

To remove potential spammers (annotators
seeking quick payment who do not follow the task
instructions), we first eliminated all annotations
by any AMT workers who left items blank or se-
lected the same score for every item for any of the
four relationships in any of their completed HITs.
Then, inter-annotator agreement was computed by
calculating the Spearman correlation coefficient ρ
between each annotator’s scores and the average
scores of all other AMT workers who completed
the HIT, excluding those already thrown out dur-
ing spammer removal. We then removed any an-
notations from workers whose agreement scores
were more than three standard deviations below
the mean agreement score for the HIT under the
assumption that these workers were not paying at-
tention to the pairs when selecting scores.

The final scores for each pair were assigned by
taking the average AMT worker score for each re-
lationship type. Some sample activities and their
ratings are shown in Table 2. Averaged across
all four relationship types, there is a good level
of inter-annotator agreement at ρ = .720 (recom-
puted after spammer removal). The highest levels
of agreement were found for similarity and relat-
edness (ρ = .768 for both), which is to be ex-
pected as these are somewhat less subjective than
motivational alignment (ρ = .745) and perceived
actor congruence (ρ = .620). These agreement
scores can be treated as an upper bound for perfor-
mance on this task; achieving a score higher than

SIM REL Activity 1 Activity 2
↑ ↑ call one’s mom call dad
↑ ↓ - -
↓ ↑ rake leaves mow the lawn
↓ ↓ go for a run shop at a thrift store

SIM MA Activity 1 Activity 2
↑ ↑ check facebook check twitter
↑ ↓ drive to missouri go on a road trip
↓ ↑ write a romantic letter kiss one’s spouse
↓ ↓ cut firewood trim one’s beard

SIM PAC Activity 1 Activity 2
↑ ↑ make a cherry pie bake a birthday cake
↑ ↓ have dinner with friends eat by oneself
↓ ↑ go to the gym take a shower
↓ ↓ read a novel go to a party

REL MA Activity 1 Activity 2
↑ ↑ gamble go to the casino
↑ ↓ go swimming clean the pool
↓ ↑ clean out old email vacuum the house
↓ ↓ study abstract algebra go to the state fair

REL PAC Activity 1 Activity 2
↑ ↑ eat cereal eat a lot of food
↑ ↓ homeschool one’s child drive one’s child to school
↓ ↑ cut the grass talk to neighbors
↓ ↓ eat at a restaurant cook beans from scratch

MA PAC Activity 1 Activity 2
↑ ↑ go to the dentist brush one’s teeth
↑ ↓ take the train to work drive to work
↓ ↑ walk one’s dog walk to the store
↓ ↓ read watch football all day

Table 4: Activity pairs from our dataset high-
lighting stark differences between the four rela-
tional dimensions. For each dimension, ↑ refers
to phrases rated at least one full point above the
middle value along the Likert scale, while ↓ indi-
cates a score at least one full point below the mid-
dle value. No pairs with high similarity and low
relatedness exist in the data.

these would mean that an automated system is as
good at ranking activity phrases as the average hu-
man annotator.

3.3 Relationships Between Dimensions
While the four relationship types being measured
are correlated with one another (Table 3.2), there
were certainly cases in which humans gave differ-
ent scores for each relationship type to the same
pair which shed light on the nuanced differences
between the dimensions. (Table 4). Therefore, it
is not necessarily the case that the best method for
capturing one dimension is also the most corre-

668



lated with human judgements across all four di-
mensions. However, it appears that similarity, re-
latedness, and motivational alignment are more
highly correlated with one another than perceived
actor congruence.

4 Methods

To determine how well automated systems are able
to model humans’ judgements of similarity, re-
latedness, motivational alignment, and perceived
actor congruence, we evaluate a group of seman-
tic textual similarity systems that are either com-
monly used or have shown state-of-the-art results.
Each method takes two texts of arbitrary length as
input and produces a continuous valued score as
output. All of the methods are trained on outside
data sources and many have been proposed as gen-
eralized embeddings that can be successful across
many tasks. The methods we assess fall into three
different categories: Composed Word-level Em-
beddings, Graph-based Embeddings, and Phrase-
level Embeddings.

Activity Phrase Pre-processing. For the first two
classes of methods, we experiment with several
variations in the set of words being passed to the
model as input in order remove the influence of
potentially less semantically important words. We
do not apply these pre-processing approaches to
the phrase-level embedding methods since those
methods are designed specifically to operate on
entire phrases (as opposed to the bag-of-words
view that the other methods take). The five varia-
tions of each phrase we consider are:
Full: The original phrase in its entirety.
Simplified: Starting with the Full phrase, we
remove several less semantically relevant edges
from a dependency parse3 of the phrase, includ-
ing the removal of determiners, coordinating con-
junctions, adjectival modifiers, adverbs, and par-
ticles. This step is somewhat similar to perform-
ing stopword removal. For example, this filtering
step would result in the bag of words containing
“clean”, “living” and “room” for full phrase: clean
up the living room.
Simplified - Light Verbs: Starting with the Sim-
plified set of words, we remove the root verb of
the activity if it is not the only word in the Sim-
plified phrase and if it belongs to the following list
of semantically light verbs (Kearns, 1988): “go”,
“make”, “do”, “have”, “get”, “give”, “take”, “let”,
“come”, and “put”. This means that we would

3We use the dependency parser from Stanford CoreNLP
(http://stanfordnlp.github.io/CoreNLP/).

convert the phrase go get a tattoo to just get a tat-
too, but read a novel would retain its verb and be-
come read novel (i.e., it will remain equivalent to
the Simplified variation).
Simplified - All Verbs: To compare against the
effect of removing light verbs, this approach takes
the Simplified phrase and removes the root verb
unless the Simplified phrase only contains that one
word. Performing this filtering step would convert
the phrase cook a sausage to simply sausage.
Core: This method seeks to reduce the phrase to
a single core concept. In many cases, this means
simply using the root verb from the dependency
parse. So, we might represent the phrase “clean
up the living room” using only the word embed-
ding for “clean”. However, we acknowledge that
semantically light verbs such as “go”, “have”, and
“do” would not adequately represent an entire ac-
tivity, and so in the case of light verbs we instead
select either the direct object or a nominal modi-
fier that is connected to the root verb. If the noun
selected as the core concept has another noun at-
tached by a compound relationship, we also in-
clude that noun. This means, for example, that we
would represent the phrase “go to an amusement
park” as just “amusement park” when we are con-
sidering just the core concept.

4.1 Composed Word-level Embeddings
The methods in this section are based on word-
level embeddings trained on some outside data.
Since they operate at a word level, we apply
a composition function to the words in a given
phrase in order to achieve an embedding for the
phrase. We tested both the arithmetic mean and
element-wise multiplication for composition func-
tions, but the former gave better performance and
thus we do not report results found when using the
element-wise product. Given an aggregate embed-
ding for a phrase, we generate a score for each pair
of activity phrases by computing the cosine simi-
larity between the embeddings for the two phrases.
We consider the following word-level methods:
Wiki-BOW: Skip Gram with Negative Sampling
Word Embeddings trained on Wikipedia data us-
ing a context window of size 2 (Wiki-BOW2) and
size 5 (Wiki-BOW5). These vectors are the same
ones used in (Levy and Goldberg, 2014a).
Wiki-DEP: Skip Gram with Negative Sam-
pling Word Embeddings trained on Wikipedia
data with dependency-based contexts (Wiki-DEP)
from (Levy and Goldberg, 2014a).
GoogleNews: Skip Gram with Negative Sampling
Word Embeddings trained on the Google News

669



corpus from (Mikolov et al., 2013).
Paragram: Embeddings trained on the Para-
phrase Database (Ganitkevitch et al., 2013) by fit-
ting the embeddings so that the difference between
the cosine similarity of actual paraphrases and that
of negative examples is maximized(Wieting et al.,
2015). We use the Paragram-Phrase XXL em-
beddings combined with the Paragram-SL999 em-
beddings, the latter of which has been tuned on
SimLex999 (Hill et al., 2016). We also use a
variation of Paragram Embeddings that employs
counter fitting (Paragram-CF). This method fur-
ther tunes the Paragram embeddings to capture a
more strict sense of similarity rather than general
association between words. This is accomplished
via optimization with the goal of increasing the
vectorspace differences between known antonyms
and altering synonym embeddings to make them
more similar to one another (Mrkšić et al., 2016).
Nondistributional vectors: Highly sparse vec-
tors that encode a huge number of binary variables
that capture interesting features about the words
such as part of speech, sentiment, and supersenses
(Faruqui and Dyer, 2015).

4.1.1 Graph-Based Embeddings
We also experiment with approaches that seek to
incorporate higher order relationships between ac-
tivity phrases by building semantic graphs that can
be exploited to discover relations that hold be-
tween the phrases. Each graph G is of the form
G = (V,E) where V is a set of human activity
phrases and E is some measure of semantic sim-
ilarity, which is computed differently depending
on the graph type. We run Node2vec (Grover and
Leskovec, 2016) using the default settings to gen-
erate an embedding for each node in the graph and
then measure the cosine similarity between nodes
(phrases) to get the final system output. The types
of graphs that we use are:
Similarity Graph: We first generate a fully con-
nected graph of all activities in our dataset us-
ing a high performing semantic similarity method
(Paragram in this case) as a way to generate edge
weights. Next, we prune all edges with a weight
less than some threshold. The results reported here
use a threshold of .5 (on a 0-1 continuous scale).
We also tried threshold values of .3, .4, and .6.,
but found them to produce inferior results for all
dimensions.
People Graph: For each activity, we know at least
four other activities that were done by the same
person because each person submitted five activ-
ities. We add an unweighted edge to the graph

for each pair of activities that were done by the
same person. On its own, this graph does not have
enough information to be competitive, so we only
report results for the combined graph.
Combined Graph: Here, we combine informa-
tion from both the Similarity Graph and the People
Graph. Since the People Graph is unweighted, we
follow the approach used in (Tripodi and Pelillo,
2016) and compute the average weight of all edges
in the Similarity Graph and assign this weight to
all edges in the People Graph. We then add the
edge weights of the two graphs, treating nonexis-
tant edges as edges with weight 0.

4.1.2 Phrase-level Embeddings
The methods in this section are designed to create
an embedding directly from phrases of arbitrary
length. Since these approaches are tailored toward
phrases in their entirety, we do not evaluate them
on the pre-processed variations of the phrases in
our dataset. The phrase-level approaches we con-
sider are:
Skip-thoughts vectors: This encoder-decoder
model induces sentence level vectors by learning
to predict surrounding sentences of each sentence
in a large corpus of books (Kiros et al., 2015).
The encoder is a recurrent neural network (RNN)
which creates a vector from the words in the in-
put sentence, and the RNN decoder generates the
neighboring sentences. The model also learns a
linear mapping from word-level embeddings into
the encoder space to handle rare words that may
not appear in the training corpus.
Charagram embeddings: Embeddings that rep-
resent character sequences (i.e., words or phrases)
based on an elementwise nonlinear transformation
of embeddings of the character n-grams that com-
prise the sequence (Wieting et al., 2016). Here we
use the pre-trained charagram-phrase model.

5 Results

Because human annotations should fall on an or-
dinal scale rather than a ratio scale, it would not be
fair to directly compare the average values human
judges gave to the systems’ output. Rather, the
systems should be evaluated based on their abil-
ity to rank the set of phrases in the same order as
the ranking given by the average human annota-
tions scores for each dimension. Therefore, we
calculate the Spearman Rank correlation between
scores given by the automated systems and the hu-
man judges our final score for each system. In a
previous study of evaluation metrics for intrinsic
semantic textual similarity tasks, this metric was

670



Method SIM REL MA PAC
Fu

ll
ph

ra
se

Wiki-BOW-2 .434 .395 .383 .230
Wiki-BOW-5 .480 .446 .431 .268
Wiki-DEP .388 ,346 ,339 .191
GoogleNews .550 .528 .514 .343
Paragram .578 .554 .530 .363
Paragram-CF .487 .455 .434 .276
Sim Graph .508 .489 .460 .330
+ People Graph .520 .502 .467 .340
Skip-thoughts .435 .408 .411 .276
Charagram .566 .550 .520 .381*

Si
m

pl
ifi

ed

Wiki-BOW-2 .532 .501 .475 .316
Wiki-BOW-5 .563 .537 .507 .342
Wiki-DEP .499 .463 .443 .284
GoogleNews .606* .582* .552* .383*
Paragram .616* .594* .560* .397*
Paragram-CF .617* .592* .556* .394*
Sim Graph .533 .520 .478 .340
+ People Graph .543 .533 .492 .350

-L
ig

ht
V

er
bs

Wiki-BOW-2 .523 .500 .481 .315
Wiki-BOW-5 .565 .545 .522 .350
Wiki-DEP .484 .457 .443 .280
GoogleNews .618* .599* .577* .394*
Paragram .639* .623* .595* .418*
Paragram-CF .637* .618* .587* .416*
Sim Graph .577 .572 .534 .360
+ People Graph .584 .576 .535 .375

-A
ll

V
er

bs

Wiki-BOW-2 .434 .436 .419 .334
Wiki-BOW-5 .482 .492 .469 .381*
Wiki-DEP .395 .392 .379 .290
GoogleNews .529 .542 .515 .425*
Paragram .547 .566 .541 .445*
Paragram-CF .522 .538 .510 .435*
Sim Graph .417 .452 .417 .363
+ People Graph .433 .468 .432 .379

C
or

e
O

nl
y

Wiki-BOW-2 .360 .321 .316 .153
Wiki-BOW-5 .402 .364 .363 .184
Wiki-DEP .319 .276 .274 .108
GoogleNews .436 .394 .393 .209
Paragram .444 .401 .402 .223
Paragram-CF .438 .397 .397 .225
Sim Graph .330 .281 .291 .146
+ People Graph .334 .283 .293 .134
Human Agree. .768 .768 .745 .620

Table 5: Spearman correlation between phrase
similarity methods and human annotations across
four annotated relations: Similarity (SIM), Relat-
edness (REL), Motivational Alignment (MA) and
Perceived Actor Congruence (PAC). Top perform-
ing methods for each dimension are in bold font. *
indicates correlation coefficient is not statistically
significantly lower than the best method for that
relational dimension (α = .05).

recommended for tasks in which the ranking of
all items is important (Reimers et al., 2016). Re-
sults for all methods using all phrase variations are
shown in Table (Table 5).

For our dataset, Paragram in the Simplified -
Light Verbs setting gives the best results for sim-
ilarity, relatedness, and motivational alignment.
It is somewhat expected that the same method
has the best performance for these three dimen-

sions as they are strongly correlated with one an-
other. Paragram in the Simplified - All Verbs set-
ting gives the best result on perceived actor con-
gruence. We can see that removing light verbs
is a helpful step for most methods when trying
to predict similarity, relatedness, and motivational
alignment indicating that light verbs mostly add
noise to the overall meaning of the phrases. In-
terestingly, the best results for pereceived actor
congruence come when ignoring all root verbs in
longer phrases. This was a filtering step that led
to decreased performance when ranking across the
other three dimensions. This suggests that for de-
termining perceived actor congruence, the context
of the action found within a phrase is more im-
portant than the action itself. Based on statis-
tical significance testing (Z-test using Fisher r-z
transformation, single-tailed), however, we cannot
be confident that all of these results will hold for
larger sets of human activity phrase pairs, as sev-
eral other methods had scores that were not found
to be significantly lower than the best methods.

6 Conclusion

In this paper, we addressed the task of measur-
ing semantic relations between human activity
phrases. We introduced a new dataset consisting
of human activity pairs that have been annotated
based on their similarity, relatedness, motivational
alignment, and perceived actor congruence. Using
this dataset, we evaluated a number of semantic
textual similarity methods to automatically deter-
mine scores for each of the four dimensions, and
found that similarity between averaged paragram
embeddings of the simplified phrases with light
verbs removed was most highly correlated with
human judgements of similarity, relatedness, and
motivational alignment. The method that yielded
the best result for the perceived actor congruence
dimension also used the paragram embeddings,
but when averaged across the simplified phrases
with all verbs removed.

We believe there is still plenty of room for im-
provement on this task, and we hope that the re-
lease of our data will encourage greater partici-
pation on this task. Future work should explore
methods to handle more subtle semantic differ-
ences between activities that we noticed are often
missed by the automated methods including the ef-
fects of function words and polysemy. It should
also be helpful to learn better weight-based com-
position methods (e.g., those proposed in (Yu and
Dredze, 2015)) rather than filtering out words in a

671



rule-based fashion.
We make our dataset, including all activ-

ity pairs and averaged human ratings, pub-
licly available at http://lit.eecs.umich.
edu/downloads.html.

Acknowledgments

This material is based in part upon work sup-
ported by the Michigan Institute for Data Sci-
ence, by the National Science Foundation (grant
#1344257), and by the John Templeton Founda-
tion (grant #48503). Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author and do not nec-
essarily reflect the views of the Michigan Insti-
tute for Data Science, the National Science Foun-
dation, or the John Templeton Foundation. We
would also like to thank members of the Univer-
sity of Michigan LIT lab for help with data anno-
tation.

References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-

Agirre, and Weiwei Guo. 2013. sem 2013 shared
task: Semantic textual similarity, including a pi-
lot on typed-similarity. In SEM 2013: The Second
Joint Conference on Lexical and Computational Se-
mantics. Association for Computational Linguistics.
Citeseer.

Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the First Joint Conference on Lexical and Computa-
tional Semantics-Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation, pages 385–393. Association for
Computational Linguistics.

Icek Ajzen. 1987. Attitudes, traits, and actions: Dis-
positional prediction of behavior in personality and
social psychology. Advances in experimental social
psychology, 20:1–63.

Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. Association for Computational
Linguistics.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research,
3(Feb):1137–1155.

Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.

Manaal Faruqui and Chris Dyer. 2015. Non-
distributional word vector representations. CoRR,
abs/1506.05230.

Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi,
and Chris Dyer. 2016. Problems with evaluation of
word embeddings using word similarity tasks. arXiv
preprint arXiv:1605.02276.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406–
414. ACM.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In HLT-NAACL, pages 758–764.

Daniela Gerz, Ivan Vulić, Felix Hill, Roi Reichart, and
Anna Korhonen. 2016. SimVerb-3500: A Large-
Scale Evaluation Set of Verb Similarity. In EMNLP.

Jeremy Goecks and Jude Shavlik. 2000. Learning
users’ interests by unobtrusively observing their nor-
mal behavior. In Proceedings of the 5th inter-
national conference on Intelligent user interfaces,
pages 129–132. ACM.

Aditya Grover and Jure Leskovec. 2016. node2vec:
Scalable feature learning for networks. In Proceed-
ings of the 22nd ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 855–864. ACM.

Felix Hill, Roi Reichart, and Anna Korhonen. 2016.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics.

Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.

Kate Kearns. 1988. Light verbs in english.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in neural information processing systems,
pages 3294–3302.

Quoc V Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML,
volume 14, pages 1188–1196.

Omer Levy and Yoav Goldberg. 2014a. Dependency-
based word embeddings. In ACL (2), pages 302–
308. Citeseer.

672



Omer Levy and Yoav Goldberg. 2014b. Neural word
embedding as implicit matrix factorization. In Ad-
vances in neural information processing systems,
pages 2177–2185.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A sick cure for the evaluation of com-
positional distributional semantic models. In LREC,
pages 216–223.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive sci-
ence, 34(8):1388–1429.

Nikola Mrkšić, Diarmuid O Séaghdha, Blaise Thom-
son, Milica Gašić, Lina Rojas-Barahona, Pei-
Hao Su, David Vandyke, Tsung-Hsien Wen, and
Steve Young. 2016. Counter-fitting word vec-
tors to linguistic constraints. arXiv preprint
arXiv:1603.00892.

Bennet B Murdock Jr. 1962. The serial position effect
of free recall. Journal of experimental psychology,
64(5):482.

Judith A Ouellette and Wendy Wood. 1998. Habit and
intention in everyday life: The multiple processes by
which past behavior predicts future behavior. Psy-
chological bulletin, 124(1):54.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, volume 14, pages 1532–
1543.

Nils Reimers, Philip Beyer, and Iryna Gurevych. 2016.
Task-oriented intrinsic evaluation of semantic tex-
tual similarity. In COLING, pages 87–96.

Milton Rokeach. 1973. The nature of human values.
Free press.

Rocco Tripodi and Marcello Pelillo. 2016. A game-
theoretic approach to word sense disambiguation.
Computational Linguistics.

John Wieting, Mohit Bansal, Kevin Gimpel, and
Karen Livescu. 2015. Towards universal para-
phrastic sentence embeddings. arXiv preprint
arXiv:1511.08198.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016. Charagram: Embedding words and
sentences via character n-grams. arXiv preprint
arXiv:1607.02789.

Mo Yu and Mark Dredze. 2015. Learning composition
models for phrase embeddings. Transactions of the
Association for Computational Linguistics, 3:227–
242.

673


