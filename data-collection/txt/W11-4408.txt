



















































A Practical Algorithm for Intersecting Weighted Context-free Grammars with Finite-State Automata


Proceedings of the 9th International Workshop on Finite State Methods and Natural Language Processing, pages 57–64,
Blois (France), July 12-15, 2011. c©2011 Association for Computational Linguistics

A Practical Algorithm for Intersecting Weighted Context-free Grammars
with Finite-State Automata

Thomas Hanneforth
Linguistics Dept.

University of Potsdam, Germany
Thomas.Hanneforth@uni-potsdam.de

Abstract

It is well known that context-free parsing
can be seen as the intersection of a context-
free language with a regular language (or,
equivalently, the intersection of a context-free
grammar with a finite-state automaton). The
present article provides a practical efficient
way to compute this intersection by converting
the grammar into a special finite-state automa-
ton (the GLR(0)-automaton) which is subse-
quently intersected with the given finite-state
automaton. As a byproduct, we present a gen-
eralisation of Tomita’s algorithm to recognize
several inputs simultaneously.

1 Introduction

At the least since the paper of Billot and Lang
(1989) which defined parsing as the intersection of a
context-free language (given by a grammar) with a
regular language (the “input”, which can be seen as
a simple finite-state automaton) the importance of
the notion of intersection for parsing purposes be-
came apparent. Intersection is first of all defined on
the language level: Intersect the (possibly infinite)
set of strings which the grammar generates with the
(possibly infinite) set of strings the finite-state au-
tomaton accepts. Since this may not be done effec-
tively due to the infiniteness of the involved sets, the
operation has to be lifted to the more compact level
of the devices generating the sets.

In principle, there are at least two ways to inter-
sect a context-free grammar G with a finite-state au-
tomaton A:

1. Convert G into a suitable pushdown automaton
(PDA) M and intersect A with M to get M ′.
Then extract the result grammar G′ from M ′.

2. Intersect G and A directly.

With respect to 1., it is well known that the class
of pushdown automata is closed under intersec-
tion with finite-state automata (henceforth FSA) (cf.
Hopcroft and Ullman (1979)). The standard con-
struction assumes a deterministic FSA (see next sec-
tion) and operates both automata in parallel: When-
ever the PDA makes a move on a certain input sym-
bol a, this move is combined with a corresponding
move of the FSA on a (see Hopcroft and Ullman
(1979) for details of the construction).

Method 1 works most efficiently in case of gram-
mars in Greibach normal form (GNF). Remember
that a context-free grammar is in GNF if each of its
rules conforms to the formatA→ aB1 . . . Bk where
a is an alphabet symbol, A is a nonterminal (phrase)
symbol and B1 . . . Bk is a possibly empty sequence
of nonterminals. Given a grammar in GNF, it is
very easy to construct a pushdown automaton from
it: when the PDA has A on its stack and next reads
an a, it replaces A by B1 . . . Bk. Every context-free
grammar can be converted into a weakly equivalent
grammar in GNF (cf. Hopcroft and Ullman (1979)),
but this changes the trees generated by the grammar
and may lead to a substiantial increase in grammar
size.

Method 2 was presented in Bar-Hillel et al. (1964)
and works in the following way: Consider a gram-
mar rule X0 → X1X2 . . . Xk. Then choose an arbi-
trary state sequence p0 . . . pk from the state set Q of
the FSA and create a new grammar rule

〈p0, X0, pk〉 →
〈p0, X1, p1〉 〈p1, X2, p2〉 . . . 〈pk−1, Xk, pk〉 .

(1)

57



Leaving the grammar fixed, the time complexity of
this method is in O(|Q|r+1) where r is the length
of the longest right-hand side of a grammar rule.
The drawback of the method is that it creates a great
amount of useless nonterminals and rules. Neder-
hof and Satta (2008) improved the algorithm by
adding simultaneous top-down and bottom-up filter-
ing mechanisms to prevent the creation of useless
symbols which of course does not change its worst-
case complexity. This upper bound can be reduced
to O(|Q|3) by binarising the grammar rules (which
naturally changes the generated trees). However, it
is not clear how the algorithm behaves in practice on
grammars with several thousands of rules ubiquitu-
ous in natural language processing.

In the present paper, we propose another algo-
rithm based on the creation of a GLR(0) automa-
ton which is subsumed by method 1 above. The
rest of the paper is organised as follows: Section 2
briefly defines the relevant notions. Section 3 de-
fines GLR(0) automata and presents an algorithm
how to intersect them with FSAs. In Section 4 we
report some experiments conducted with a big gram-
mar extracted from a treebank.

2 Preliminaries

An alphabet Σ is a finite set of symbols. A string
x = a1 . . . an over Σ is a finite concatenation of
symbols ai taken from Σ. The length of a string
x = a1 . . . an – symbolically |x| – is n. The empty
string is denoted by ε and has length zero. Let Σ∗

denote the set of all finite-length strings (including
ε) over Σ.
A finite-state automaton (FSA) A is a 5-tuple
〈Q,Σ, q0, δ, F 〉withQ being a finite set of states; Σ,
an alphabet, q0 ∈ Q, the start state; δ : Q×Σ 7→ 2Q,
the transition function; and F ⊆ Q, the set of final
states.
Given two states p, q ∈ Q, a path from p to q in A
– symbolically p  

A
q – is a sequence s0s1 . . . sk of

states, such that s0 = p, sk = q, and for all 1 ≤
i ≤ k: ∃a ∈ Σ : si ∈ δ(si−1, a). Given a path
π = p  

A
q, define labels(π) as the concatenation

of the symbols labeling the transitions along π. For
an empty path π = s0, labels(π) = ε. The length of
path π = s0s1 . . . sk – symbolically |π| – is k. We

use p k 
A
q to denote a path from p to q of length k

in A.
An FSA is called deterministic if for all symbol-
state pairs q, a, |δ(q, a)| ≤ 1. For a determin-
istic FSA, we may modify δ to be a partial func-
tion Q × Σ 7→ Q. When appropriate, we use δ as
a total function by adding a special element ⊥ to
Q denoting failure. For deterministic FSA, we use
sometimes the notation p a−→ q to denote transitions:
p

a−→ q if δ(p, a) = q.
Define δ∗ : Q × Σ 7→ G as the reflexive and tran-
sitive closure of δ: ∀q ∈ Q, δ∗(q, ε) = q and ∀q ∈
Q, a ∈ Σ, w ∈ Σ∗ : δ∗(q, aw) = δ∗(δ(q, a), w).
Given a (deterministic) FSA A = 〈Q,Σ, q0, δ, F 〉,
the language L(A) ofA is defined as: L(A) = {x ∈
Σ∗ | δ∗(q0, x) ∈ F}.
A semiringK is a 5-tuple (W,⊕,⊗, 0, 1) such that 1.
W is a non-empty set, the carrier set of the semiring,
2. (W,⊕, 0) is a commutative monoid, 3. (W,⊗, 1)
is a monoid, 4. ⊗ distributes over ⊕, and 5. 0 is an
annihilator for ⊗: ∀x ∈ W : x ⊗ 0 = 0 ⊗ x =
0 . In the following, we will identify a semiring K
with its carrier set W . Common semirings are the
tropical semiring T = 〈R,min,+,∞, 0〉 and the
probabilistic semiring P = 〈R,+, ·, 0, 1〉.

A weighted context-free grammar (WCFG) G
over a semiring K is a 4-tuple 〈N,Σ, S, P 〉: N
is a finite set, the non-terminals, Σ is an alphabet,
S ∈ N the start symbol, and P a finite set of pairs
〈A → β, c〉 ∈ (N × (Σ ∪ N)∗) × K, the set of
weighted rules. A WCFG without rule weights is
called a context-free grammar (CFG).

In particular, ifK is the probabilistic semiring and
if we define an additional condition on σ:

∀A ∈ N :
∑

〈A→β,c〉∈P
c = 1 , (2)

then G is called a probabilistic context-free gram-
mar (PCFG) (see also Nederhof and Satta (2008)).
Fig. 1 shows a toy PCFG.

1: S → NP VP / 1.0 2: NP → DET N / 0.6
3: NP → NE / 0.3 4: NP → NP PP / 0.1
5: PP → P NP / 1.0 6: VP → V / 0.5
7: VP → V NP / 0.4 8: VP → VP PP / 0.1

Figure 1: A toy PCFG with numbered rules. Probabilities
are stated after /.

58



Let G = 〈Σ, N, S, P 〉 be a context-free grammar,
and let V beN ∪Σ. Define a relation⇒ ⊆ V ∗×V ∗
as follows: α ⇒ β if α = xAγ, β = xψγ, x ∈
Σ∗, γ ∈ V ∗ and A→ ψ ∈ P .
Let ∗⇒ be equal to ⋃k≥0 k⇒. A leftmost derivation
of a string w ∈ Σ∗ is a sequence of elements from
⇒ such that S ⇒ X1 . . . Xk ⇒ . . . ⇒ w. We
abbreviate this to S ∗⇒ w. The language of a CFGG
– symbollically L(G) – is defined as L(G) = {w ∈
Σ∗ | S ∗⇒ w}. Finally, given a CFG G and an FSA
A, define the intersection of G and A as follows:
G∩ = G ∩A if L(G) ∩ L(A) = L(G∩).

The notion of a derivation and the language of a
CFG carry over to WCFGs, as well as the notion
of intersection. See Nederhof and Satta (2008) for
details.

3 The Intersection Algorithm

The main idea to compute the intersection of a
weighted context-free grammar G with a finite-state
automaton A is stated in Algorithm 1.

Algorithm 1: INTERSECTION OF A WCFG AND AN FSA
Input: WCFG G = 〈N,Σ, S, P 〉 over semiring K
Input: FSA A = 〈Q,Σ, q0, δ, F 〉
Output: WCFG G∩ = 〈N∩,Σ, S∩, P∩〉 over semiring K with

N∩ ⊆ N ×Q and P∩ ⊆ (N∩ × (N∩ ∪ Σ)∗)×K
1 Construct GLR(0) automaton M for G
2 Compute M∩, the intersection of M and A
3 Extract G∩ from M∩

In line 1, the WCFG is converted into a GLR(0)
automaton. Given a WCFG G = 〈N,Σ, S, P 〉
over K, a GLR(0) automaton (for Generalised LR)
M = 〈Q,∆, q0, F, δ, τ〉 over K is a finite-state au-
tomaton with Q, F and q0 defined as for FSAs; ∆
is N ∪ Σ. Since M is required to be determinis-
tic, δ : Q × ∆ 7→ Q is a partial transition function
which maps – when defined – a state q and a symbol
a ∈ ∆ to a follow state. τ : Q 7→ 2P is a mapping
from states to subsets of grammar rules (indices).1

GLR(0) automata are computed from grammars
by an algorithm adapted from a standard algorithm

1In the original definition of LR(k) automata, τ is a par-
tial function Q 7→ P . The presence of multiple reduce actions
would indicate an ambiguity (a reduce/reduce conflict) which
entails that the language of the underlying grammar G is not a
LR(k)-language. See Aho and Ullman (1972).

(cf. Aho et al. (1986, p. 216ff.)) which will be
explained in greater detail in Section 3.1.

Fig. 2 gives a example GLR(0) automaton for the
grammar from Fig. 1. A GLR(0) recognizer is con-

Figure 2: The GLR(0) automaton for the grammar in Fig.
1. “re n” means: “reduce with rule n”. A transition from
state p to state q corresponds to a (terminal or nontermi-
nal) shift operation.

trolled by a GLR(0) automaton M . Given some in-
put string w, it creates another GLR(0) automaton
M ′ as a result by repeatedly applying its two main
operations:2

• Shift: When the recognizer reads an input sym-
bol a ∈ Σ in state q, it adds a transition q a−→
δ(q, a) to M ′.

• Reduce: When while processing an input
string, the parser reaches a state q for which
τ(q) is defined, for each rule A → α ∈
τ(q), find all predecessor states p such that
labels(p  q) = α.3 Then add a transition
p

A−→ δ(p,A) to M ′ (which may be interpreted
as a nonterminal shift). δ(p,A) is also called a
GOTO state.

The GLR(0) recognizer starts in state q0 and scans
the input string from left to right. It applies the oper-
ations shift and reduce until either an accepting state

2Actually, the LR(k) method was invented for parsing deter-
ministic languages like programming languages. In these cases
it is not necessary to create an output automaton. Instead, a sim-
ple stack is used on which state symbols are pushed and from
which they a popped during a reduction.

3We momentarily disregard the rule weight here.

59



f ∈ F is reached (in which case f is also marked as
final in M ′) or the GLR(0) automaton blocks which
causes an error to be signaled.

Continuing with Algorithm 1, line 2 intersects the
GLR(0) automaton M with the FSA A, resulting in
a GLR(0) automaton M∩.

Finally, in line 3, the WCFG G∩ generating
L(G) ∩ L(A) is extracted from M∩.
The following subsections explain all three steps in
greater detail.

3.1 Efficient Construction of GLR(0) Automata
The construction of a GLR(0) automatonM is based
on the computation of the collection of LR(0) item
sets. Here, the crucial notion is that one of a dot-
ted rule. A dotted rule is a WCFG rule with a
dot somewhere in its right-hand side. This dot in-
dicates which part of the rule was already success-
fully applied and which part has yet to be matched.
An example with respect to the grammar in Fig. 1
is: NP → NP • PP. A dotted rule is also called
a LR(0) item. To compute M , we start with a set
containing only the LR(0) item S′ → •S where S′
is a new super start symbol. Then the main op-
eration of the algorithm – closure – is applied to
it. Basically, given a grammar G = 〈N,Σ, S, P 〉,
closure({A → α • Bβ}) (with A,B ∈ N and
α, β ∈ (N ∪ Σ)∗) computes on the basis of G the
symbols which are expected next given that the au-
tomaton’s expectation is to read a B.4

In our example case,

q0 = closure({S′ → • S}) =


S′ → • S
S → • NP VP
NP→ • DET N
NP→ • NE


(3)

The δ-function of M is computed as follows:

δ(q,B) = closure({A→ αB • β | A→ α •Bβ ∈ q}) .
(4)

For example, δ(q0,NE) = closure({NP →
NE •}) = {NP→ NE •} .

Let the state set Q of M be the set of all LR(0)
item sets that can be reached by recursively applying
δ and closure to q0 and all item sets originating from
it.

4Due to space limitations, we cannot state the definition of
the closure algorithm. Please refer to Aho et al. (1986, p. 223)
for the details.

If a state q contains an item A → α •, the rule
A → α is added to τ(q). M reaches an accepting
state f if the item set contains the LR(0) item S′ →
S • .

For grammars not having the LR(0) property (for
example, ambiguous grammars), the construction
introduces conflicts, see Aho and Ullman (1972).
Nevertheless, the algorithm leads to deterministic
GLR(0) automata for all grammars.

The naive approach representing LR(0) states as
sets of dotted rules leads to increased computation
times for bigger grammars, for example those ex-
tracted from treebanks. For example, the gram-
mar extracted from the TiGer treebank (Brants et al.
(2002)) has over 14,300 rules.

A better approach is replacing the dotted rule by a
pair consisting of the rule index and the current po-
sition of the dot. But even then quite big item sets
may result since treebank grammars often have sev-
eral thousand rules for expanding a single nontermi-
nal symbol.5 Since the right hand sides of grammar
rules expanding a given nonterminal symbol often
share common prefixes, left-factoring the grammar
(cf. Aho et al. (1986)) is an option when the struc-
ture of the parse tree is not of concern. In general,
this is not the case in using (weighted) grammars for
parsing natural languages.

Instead of altering the grammar, we prefer a more
sophisticated representation of the dotted rules. All
right hand sides αi of a set of rules {B → αi} ex-
panding B can be combined into a disjunctive reg-
ular expression r = α1 + α2 + . . . + αk. r can be
converted into a deterministic weighted finite-state
machine by standard techniques (cf. Hopcroft and
Ullman (1979)). The result is a trie-like left-factored
automaton AB representing the right hand sides of
the rules for B. For the final states of AB , we define
a function ρ : Q → I , where I is the set of rule in-
dices of the WCFG. For a given final state q of AB
(representing a fully found right hand side α of a
rule expanding B), ρ(q) = i if ∃c ∈ K : 〈B → α, c〉
is the ith rule of the grammar.

Fig. 3 shows the rule FSA AVP for the toy gram-
mar shown in Fig. 1.
An LR(0) item is then represented as a pair 〈B, q〉

5For example, the TiGer grammar used in Section 4 contains
2,378 rules for prepositional phrases and 3,475 rules for verbal
phrases.

60



Figure 3: The rule trie containing the right hand sides
of VP of the grammar in Fig. 1. Double circles indicate
final states and state the rule index after /.

where B is a nonterminal and q a state in the FSA
AB associated with B. When during the closure
operation a pair 〈B, q〉 is processed, the transitions
q

Xi−→ p leaving q in AB are enumerated and new
items 〈Xi, q0Xi 〉 (if Xi ∈ N ) are added to the clo-
sure items set.

3.2 Intersecting LR(0) Automata with
Finite-state Automata

Algorithm 2 computes the intersection of M and A.
The algorithm maintains a breadth-first queue L of
state pairs ∈ QM × QA. In line 4, a pair consisting
of the two start states is inserted intoL. In the while-
loop between lines 5 and 31, a state pair 〈qM , qA〉 is
removed from L. Then, two types of actions are ap-
plied to the current state pair 〈qM , qA〉: Reductions
and shifts. Line 9 checks whether M defines reduc-
tions for state qM . If true, the for-loop between lines
10 and 23 considers each rule B → α with weight
c. In line 11, the set of all ancestor nodes for cur-
rent state 〈qM , qA〉 is computed for which there are
paths π of length |α| to 〈qM , qA〉 (simultaneously,
we also record the labels of the paths between an an-
cestor state and 〈qM , qA〉). By definition of the con-
struction of a GLR(0) automaton, labels(π) equals
α (disregarding the indices).

Then, the for-loop between lines 12 and 23 op-
erates over each ancestor state 〈q, q′〉 of 〈qM , qA〉
and constructs new states and transitions which cor-
respond to the GOTO-actions of the GLR(0) recog-
nizer for nonterminal symbols. Before doing that, a
rule 〈BqA → α′, c〉 is added to the reduce actions of
state 〈qM , qA〉 in line 13. Note that α′ differs from
the original α in rule B → α in the indices carried
by the nonterminals.6 The subsequent steps are:

• In line 14, a new state 〈δM (q,B), qA〉 is created
6We will discuss the necessity of indexed nonterminalsBqA

below.

Algorithm 2: INTERSECTION OF A GLR(0) AU-
TOMATON AND AN FSA

Input: GLR(0) automaton
M = 〈QM ,∆M , q0M , FM , δM , τM 〉 over a semiring
K

Input: Deterministic FSA A = 〈QA,Σ, q0A , δA, FA〉
Output: GLR(0) automaton

M ′ = 〈QM′ ,∆M′ , q0M′ , FM′ , δM′ , τM′ 〉 over K
1 q0M′ ← 〈q0M , q0A 〉
2 QM′ ← {q0M′ }
3 FM′ ← ∅
4 Enqueue(q0M′ , L)

5 while L 6= ∅ do
6 〈qM , qA〉 ← Dequeue(L)
7 if qM ∈ FM ∧ qA ∈ FA then
8 FM′ ← FM′ ∪ {〈qM , qA〉}
9 if τM (qM ) 6= ⊥ then

// Perform reductions
10 for 〈B → α, c〉 ∈ τM (qM ) do
11 Vα ← {〈q, q′, labels(π)〉 | 〈q, q′〉 ∈

QM′ ∧ π = 〈q, q′〉
|α| 
M′
〈qM , qA〉}

12 for 〈q, q′, α′〉 ∈ Vα do
13 τM′ (〈qM , qA〉)←

τM′ (〈qM , qA〉) ∪ {〈BqA → α′, c〉}
14 if 〈δM (q,B), qA〉 /∈ QM′ then
15 QM′ ← QM′ ∪ {〈δM (q,B), qA〉}
16 δM′ (〈q, q′〉, BqA )←

〈δM (q,B), qA〉
17 ∆M′ ← ∆M′ ∪ {BqA}
18 Enqueue(〈δM (q,B), qA〉, L)
19 else
20 if δM′ (〈q, q′〉, BqA ) = ⊥ then
21 δM′ (〈q, q′〉, BqA )←

〈δM (q,B), qA〉
22 if 〈δM (q,B), qA〉 /∈ L then
23 Enqueue(〈δM (q,B), qA〉, L)

// Perform shifts
24 for a ∈ Σ do
25 if δM (qM , a) 6= ⊥ ∧ δA(qA, a) 6= ⊥ then
26 ∆M′ ← ∆M′ ∪ {a}
27 p← 〈δM (qM , a), δA(qA, a)〉
28 δM′ (〈qM , qA〉, a)← p
29 if p /∈ QM′ then
30 QM′ ← QM′ ∪ {p}
31 Enqueue(p, L)

32 return M ′

and checked whether it is present in the state
set (and inserted if it is not). Here, δM (q,B)
denotes the GOTO-state M defines for nonter-
minalB. Note that the second component qA of
〈δM (q,B), qA〉 is copied from the current state
pair 〈qM , qA〉 (the input index does not “move”
on after a reduction, so to speak).

61



• In line 20, it is checked whether a transition
leaving ancestor state 〈q, q′〉 with symbol BqA
already exists. Here, there are two subcases to
consider:

1. 〈δM (q,B), qA〉 is a new state
which entails that the transition
〈q, q′〉 BqA−−→ 〈δM (q,B), qA〉 also does not
exist (lines 15–18). This happens when
〈δM (q,B), qA〉 is encountered for the first
time. 〈δM (q,B), qA〉 is added to the state
set (line 15) and a new transition leading
to it is added to δM ′ (line 16). Finally,
BqA is added to the alphabet of M

′ (line
17) and the new state 〈δM (q,B), qA〉 is
inserted into the queue (line 18).

2. 〈δM (q,B), qA〉 already existed, but not
〈q, q′〉 BqA−−→ 〈δM (q,B), qA〉 (lines 21–
23). Here, we repeatedly encoun-
tered 〈δM (q,B), qA〉 during processing.
This happens in case of local ambigui-
ties where there exist multiple trees for
some subpart of A headed by the same
nonterminal. In terms of the graph
structure of M ′, we create a reentrant
node 〈δM (q,B), qA〉 with more than one
incoming transition (line 21). Since
〈δM (q,B), qA〉 may trigger further reduc-
tions (its ancestor set was changed), it is
reinserted into the queue (when not al-
ready present).

The shift operations performed in the for-loop be-
tween lines 24 and 31 are similar to the case of the
intersection of two FSAs: For every transition leav-
ing qA labeled a it is tried to find a corresponding
transition leaving qM . If this transition exists, a new
state pair 〈δM (qM , a), δA(qA, a)〉 is added to QM ′
and L, if not already present (line 30). In addition,
a transition 〈qM , qA〉 a−→ 〈δM (qA, a), δA(qA, a)〉 is
created (line 28).

Unsurprisingly, Algorithm 2 is simply a general-
isation of Tomita’s GLR algorithm (cf. Tomita and
Ng (1991)) to the recognition of the language of an
FSA instead of the recognition of a single sentence
(which can be seen as a simple, linear FSA with a
single final state). In the general case treated here,
A may have several final states and may contain an

infinite number of paths. Because of that, the non-
terminal symbols ofM ′ are indexed with the current
state qA of A. In that way, M ′ keeps track of the
different reductions made for different paths in A.

Fig. 4(b) shows the result of applying Algorithm
2 to the GLR(0) automaton of Fig. 2 and the FSA of
Fig. 4(a).

(a)

(b)

Figure 4: (a) A deterministic FSA representing three sen-
tences of the PCFG of Fig. 1. (b) The result of Algorithm
2 applied to the automata in Fig. 2 and Fig. 4(a). States
are labeled with pairs 〈qM , qA〉.

3.2.1 Complexity.
The outer while-loop of Algorithm 2 is bounded

by O(|QM × QA|). Since M and A are both de-
terministic, only a small subset of QM × QA will
be actually created in the average case. Concerning
the shift-actions (for-loop lines 24–31), each pair is
inserted exactly once into the queue. Looking at the
reduce-actions (lines 9–23), a state pair may be rein-
serted into the queue in case a new incoming transi-
tion is added for 〈δM (q,B), qA〉 (line 23). The num-
ber of reinsertions is bounded by the number of pos-

62



S’→ S4 / 1 S’→ S5 / 1
S4 → NP1 VP4 / 1 S4 → NP3 VP4 / 1
S5 → NP1 VP5 / 1 NP1 → NE / 0.3
NP3 → DET N / 0.6 NP4 → NE / 0.3
VP4 → V NP4 / 0.4 VP4 → V / 0.5
VP5 → V / 0.5

Figure 5: Rule set of the grammar extracted from Fig.
4(b).

sible reductions taking place at 〈qM , qA〉 which is in
turn bounded by |N |, the number of nonterminals of
G. The most expensive step is found in line 11. Let r
be the length of the longest right-hand side of a rule
inG. The number of state pairs 〈q, q′〉 created in line
11 and subsequently considered in the for-loop lines
12–23 is bounded by O(|QM × QA|r) (the number
of ancestors increases exponentially with respect to
the distance r). By applying the memoisation tech-
niques proposed in Kipps (1991), this bound can be
strengthend to O(|QM ×QA|2). Putting everything
together, the overall complexity of Algorithm 2 is in
O(|QM ×QA|3).

3.3 Extracting Grammar Rules

The last step, the extraction of G∩ = G ∩ A is easy
and stated in Algorithm 3.

Algorithm 3: EXTRACTING G∩ .
Input: A GLR(0) automaton

M = 〈QM ,∆M , q0M , FM , δM , τM 〉 over K
Output: A WCFG G∩ = 〈N,∆M \N,S′, P 〉 over K

1 N ← {Xi ∈ ∆M | i ∈ N} ∪ {S′}
2

P ← {〈S′ → Xi, 1〉 | ∃Xi ∈ N : δM (q0M , Xi) ∈ FM} ∪⋃
q∈QM∧τM (q)6=⊥

τM (q)

Algorithm 3 simply extracts the grammar rules from
the states q for which τM (q) is defined. Addition-
ally, a new start symbol S′ is introduced and triv-
ially weighted rules S′ → Xi are added to the rule
set such thatXi is labeling a transition from q0M to a
final state. Fig. 5 shows the grammar extracted from
the automaton shown in Fig. 4(b).

Automaton A1 A2 A3
|Q| 8 56 173
|δ| 9 64 259

Table 1: Sizes of the input FSA.

Phase Operation Time
(ms)

Avg.
time
per

sent

|Q| |δ|

1 Constr. of
M

3, 520 - 17, 279 1, 226, 776

2 M ∩A1 47 47 1, 841 14, 005
2 M ∩A2 3, 198 320 18, 355 389, 140
2 M ∩A3 17, 847 178 50, 332 1, 097, 768

Table 2: Results of the experiments with the TiGer gram-
mar.

4 Experiments

We implemented the algorithm from the last section
in the C++ programming language within the fsm2
framework (see Hanneforth (2009)). The grammar
used for the experiments has been extracted from the
TiGer treebank (cf. Brants et al. (2002)). It contains
14,379 rules7, and the sizes of the alphabet and the
nonterminal sets are 51 and 25, resp. The length
of the longest right-hand side of a rule is 17. For
the FSA operand of the intersection algorithm, we
created three minimal FSA accepting 1, 10 and 100
sentences (tag sequences) randomly taken from the
TiGer corpus. The sizes of the automata are sum-
marised in Table 1.
Table 2 shows the results of the experiments which
were carried out on a 2.8 GHz CPU. The columns
|Q| and |δ| contain the sizes of the automata result-
ing from the operation mentioned before.
Since treebank grammars tend to avoid recursive
rules and therefore assign flat structures to input
strings, they create a lot of readings with spurious
ambiguities. Johnson (1998) reports that approx-
imately 9% of the rules of the Penn treebank are
never used in a maximum likelihood setting since
these rules are subsumed by combinations of other
rules with a higher combined probability. We expect
even better intersection timings in the face of more
linguistically realistic grammars.

7Disregarding discontinous constructions like verb–particle
rules which are not directly representable in the context-free
grammar format.

63



5 Conclusion

Above, we presented a theoretical and practical al-
gorithm to intersect weighted grammars with FSAs
which can be used for parsing or language model
training purposes (cf. Nederhof (2005)). No gram-
mar transformation (for example, binarisation) is
necessary to achieve optimal cubic complexity. In-
stead, the binarisation is implicit by using the dotted
rule technique. However, the algorithm may suffer
from a big grammar dependent constant for artificial
grammars (see Johnson (1991) for details). This is
due to the implicit subset construction present in the
construction ofM ’s δ-function in Eq. (4). An option
to investigate for these artificial grammars would
be considering the construction of non-deterministic
GLR(0) automata.

References

Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice-Hall,
Inc., Upper Saddle River, NJ, USA.

A. V. Aho, R. Sethi, and J. D. Ullman. 1986. Compilers:
Principles, Techniques, and Tools. Addison-Wesley,
Reading, Mass.

Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On
Formal Properties of Simple Phrase Structure Gram-
mars. In Y. Bar-Hillel, editor, Language and Infor-
mation: Selected Essays on their Theory and Applica-
tion, pages 116–150. Addison-Wesley, Reading, Mas-
sachusetts.

S. Billot and B. Lang. 1989. The Structure of Shared
Forests in Ambiguous Parsing. In Proceedings of the
Twenty-Seventh Annual Meeting of the Association for
Computational Linguistics, pages 143–151, Vancou-
ver (British Columbia). Association for Computational
Linguistics (ACL).

Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER Tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories.

Thomas Hanneforth. 2009. fsm2 - A Scripting Language
Interpreter for Manipulating Weighted Finite-state Au-
tomata. In Anssi Yli-Jyrä et al. (eds): Finite-State
Methods and Natural Language Processing, 8th Inter-
national Workshop, pages 13–30, Berlin. Springer.

John E. Hopcroft and Jeffrey D. Ullman. 1979. Intro-
duction to Automata Theory, Languages and Compu-
tation. Addison-Wesley.

M. Johnson. 1991. The Computational Complexity of
GLR Parsing. In M. Tomita, editor, Generalized LR
Parsing, pages 35–42. Kluwer, Boston.

Mark Johnson. 1998. PCFG Models of Linguis-
tic Tree Representations. Computational Linguistics,
24(4):613–632.

J. R. Kipps. 1991. GLR Parsing in Time O(n3). In
M. Tomita, editor, Generalized LR Parsing, pages 43–
60. Kluwer, Boston.

Mark-Jan Nederhof and Giorgio Satta. 2008. Probabilis-
tic Parsing. In G. Bel-Enguix, M. Dolores Jimenez-
Lopez, and C. Martin-Vide, editors, New Develop-
ments in Formal Languages and Applications, pages
229–258. Springer.

Mark-Jan Nederhof. 2005. A General Technique to
Train Language Models on Language Models. Com-
putational Linguistics, 31:173–186, June.

M. Tomita and S.-K. Ng. 1991. The Generalized LR
Parsing Algorithm. In M. Tomita, editor, Generalized
LR Parsing, pages 1–16. Kluwer, Boston.

64


