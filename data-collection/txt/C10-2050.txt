436

Coling 2010: Poster Volume, pages 436–444,

Beijing, August 2010

Negative Feedback: The Forsaken Nature Available for Re-ranking

Yu Hong, Qing-qing Cai, Song Hua, Jian-min Yao, Qiao-ming Zhu 

School of Computer Science and Technology, Soochow University 

jyao@suda.edu.cn 

 

ABSTRACT 

Re-ranking  for  Information  Retrieval 
aims  to  elevate  relevant  feedbacks  and 
depress  negative  ones  in  initial  retrieval 
result  list.  Compared  to  relevance  feed-
back-based  re-ranking  method  widely 
adopted  in  the  literature,  this  paper  pro-
poses  a  new  method  to  well  use  three 
features  in  known  negative  feedbacks  to 
identify  and  depress  unknown  negative 
feedbacks.  The  features  include:  1)  the 
minor (lower-weighted) terms in negative 
feedbacks;  2)  hierarchical  distance  (HD) 
among  feedbacks  in  a  hierarchical  clus-
tering  tree;  3)  obstinateness  strength  of 
negative  feedbacks.  We  evaluate  the 
method  on  the  TDT4  corpus,  which  is 
made up of news topics and their relevant 
stories.  And  experimental  results  show 
that  our  new  scheme  substantially  out-
performs its counterparts. 

1.  INTRODUCTION 

When we start out an information retrieval jour-
ney on a search engine, the first step is to enter a 
query in the search box. The query seems to be 
the  most  direct  reflection  of  our  information 
needs. However, it is short and often out of stan-
dardized  syntax  and  terminology,  resulting  in  a 
large  number  of  negative  feedbacks.  Some  re-
searches focus on exploring long-term query logs 
to acquire query intent. This may be helpful for 
obtaining  information  relevant  to  specific  inter-
ests but not to daily real-time query intents. Es-
pecially  it  is  extremely  difficult  to  determine 
whether  the  interests  and  which  of  them  should 
be involved into certain queries. Therefore, given 
a query, it is important to “locally” ascertain its 
intent by using the real-time feedbacks. 

Intuitively  it  is  feasible  to  expand  the  query 
using the most relevant feedbacks (Chum et al., 
2007).  Unfortunately  search  engines  just  offer 
“farraginous”  feedbacks  (viz.  pseudo-feedback) 
which  may  involve  a  great  number  of  negative 
feedbacks.  And  these  negative  feedbacks  never 
honestly lag behind relevant ones in the retrieval 
results,  sometimes  far  ahead  because  of  their 
great  literal  similarity  to  query.  These  noisy 
feedbacks  often  mislead  the  process  of  learning 
query intent.   

For so long, there had no effective approaches 
to  confirm  the  relevance  of  feedbacks  until  the 
usage of the web click-through data (Joachims et 
al.,  2003).  Although  the  data  are  sometimes  in-
credible due to different backgrounds and habits 
of searchers, they are still the most effective way 
to  specify  relevant  feedbacks.  This  arouses  re-
cent  researches  about  learning  to  rank  based  on 
supervised  or  semi-supervised  machine  learning 
methods,  where  the  click-through  data,  as  the 
direct  reflection  of  query  intent,  offer  reliable 
training data to learning the ranking functions. 

Although  the  learning  methods  achieve  sub-
stantial improvements in ranking, it can be found 
that  lots  of  “obstinate”  negative  feedbacks  still 
permeate  retrieval  results.  Thus  an  interesting 
question  is  why  the  relevant  feedbacks  are  able 
to describe what we really need, but weakly repel 
what  we  do  not  need.  This  may  attribute  to  the 
inherent  characteristics  of  pseudo-feedback,  i.e. 
their  high  literal  similarity  to  queries.  Thus  no 
matter  whether  query  expansion  or  learning  to 
rank,  they  may  fall  in  the  predicament  that  “fa-
voring” relevant feedbacks may result in “favor-
ing”  negative  ones,  and  that  “hurting”  negative 
feedbacks may result in “hurting” relevant ones. 
However, there are indeed some subtle differ-
ences  between  relevant  and  negative  feedbacks, 
e.g. the minor terms (viz. low-weighted terms in 
texts). Although these terms are often ignored in 

437

relevance measurement because their little effect 
on mining relevant feedbacks that have the same 
topic or kernel, they are useful in distinguishing 
relevant feedbacks from negative ones. As a re-
sult,  these  minor  terms  provides  an  opportunity 
to  differentiate  the  true  query  intent  from  its 
counterpart  intents  (called  “opposite  intents” 
thereafter  in  this  paper).  And  the  “opposite  in-
tents” are adopted to depress negative feedbacks 
without “hurting” the ranks of relevant feedbacks. 
In addition, hierarchical clustering tree is helpful 
to  establish  the  natural  similarity  correlation 
among information. So this paper adopts the hi-
erarchical  distance  among  feedbacks  in  the  tree 
to enhance the “opposite intents” based division 
of  relevant  and  negative  feedbacks.  Finally,  an 
obstinateness  factor  is  also  computed  to  deal 
with  some  obstinate  negative  feedbacks  in  the 
top  list  of  retrieval  result  list.  In  fact,  Teevan 
(Teevan et al., 2008) observed that most search-
ers  tend  to  browse  only  a  few  feedbacks  in  the 
first one or two result pages. So our method fo-
cuses  on  improving  the  precision  of  highly 
ranked retrieval results.   

The rest of the paper is organized as follows. 
Section  2  reviews  the  related  work.  Section  3 
describes  our  new  irrelevance  feedback-based 
re-ranking scheme and the HD measure. Section 
4 introduces the experimental settings while Sec-
tion 5 reports experimental results. Finally, Sec-
tion 6 draws the conclusion and indicates future 
work. 
2.  RELATED WORK 

Our  work  is  motivated  by  information  search 
behaviors, such as eye-tracking and click through 
(Joachims,  2003).  Thereinto,  the  click-through 
behavior is most widely used for acquiring query 
intent.  Up  to    present,  several  interesting  fea-
tures,  such  as  click  frequency  and  hit  time  on 
click  graph  (Craswell  et  al.,  2007),  have  been 
extracted  from  click-through  data  to  improve 
search  results.  However,  although  effective  on 
query  learning,  they  fail  to  avoid  the  thorny 
problem that even when the typed query and the 
click-through data are the same, their intents may 
not be the same for different searchers.   

A  considerable  number  of  studies  have  ex-
plored  pseudo-feedback  to  learn  query  intent, 
thus  refining  page  ranking.  However,  most  of 
them focus on the relevant feedbacks. It is until 

recently that negative ones begin to receive some 
attention.  Zhang  (Zhang et  al.,  2009)  utilize  the 
irrelevance distribution to estimate the true rele-
vance model. Their work gives the evidence that 
negative  feedbacks  are  useful  in  the  ranking 
process. However, their work focuses on gener-
ating a better description of query intent to attract 
relevant  information,  but  ignoring  that  negative 
feedbacks have the independent effect on repel-
ling  their  own  kind.  That  is,  if  we  have  a  king, 
we  will  not  refuse  a  queen.  In  contrast,  Wang 
(Wang et al., 2008) benefit from the independent 
effect from the negative feedbacks. Their method 
represents the opposite of query intent by using 
negative  feedbacks  and  adopts  that  to  discount 
the relevance of each pseudo-feedback to a query. 
However,  their  work  just  gives  a  hybrid  repre-
sentation  of  opposite  intent  which  may  overlap 
much  with  the  relevance  model.  Although  an-
other  work  (Wang  et  al.,  2007)  of  them  filters 
query terms from the opposite intent, such filter-
ing makes little effect because of the sparsity of 
the query terms in pseudo-feedback. 

Other related work includes query expansion, 
term extraction and text clustering. In fact, query 
expansion techniques are often the chief benefi-
ciary  of  click-through  data  (Chum  et  al.,  2007). 
However,  the  query  expansion  techniques  via 
clicked  feedbacks  fail  to  effectively  repel  nega-
tive ones. This impels us to focus on un-clicked 
feedbacks.  Cao  (Cao  et  al.,  2008)  report  the  ef-
fectiveness of selecting good expansion terms for 
pseudo-feedback.  Their  work  gives  us  a  hint 
about the shortcomings of the one-sided usage of 
high-weighted terms. Lee (Lee et al., 2008) adopt 
a cluster-based re-sampling method to emphasize 
the core topic of a query. Their repeatedly feed-
ing process reveals the hierarchical relevance of 
pseudo-feedback. 
3.  RE-RANKING SCHEME 

3.1  Re-ranking Scheme 

The re-ranking scheme, as shown in Figure 1, 
consists of three components: acquiring negative 
feedbacks,  measuring  irrelevance  feedbacks  and 
re-ranking pseudo-feedback. 

Given a query and its search engine results, we 
start  off  the  re-ranking  process  after  a  trigger 
point.  The  point  may  occur  at  the  time  when 
searchers click on “next page” or any hyperlink. 

438

All  feedbacks  before  the  point  are  assumed  to 
have been seen by searchers. Thus the un-clicked 
feedbacks before the point will be treated as the 
known  negative  feedbacks  because  they  attract 
no attention of searchers. This may be questioned 
because  searchers  often  skip  some  hyperlinks 
that have the same contents as before, even if the 
links  are  relevant  to  their  interests.  However, 
such  skip  normally  reflects  the  true  searching 
intent  because  novel  relevant  feedbacks  always 
have more attractions after all. 

Figure 1. Re-ranking scheme 

 

I

_

_

_

=

R

Another crucial step after the trigger point is to 
generate the opposite intent by using the known 
negative  feedbacks.  But  now  we  temporarily 
leave the issue to Section 3.2 and assume that we 
have obtained a good representation of the oppo-
site  intent,  and  meanwhile  that  of  query  intent 
has been composed of the highly weighted terms 
in the known relevant feedbacks and query terms. 
Thus, given an unseen pseudo-feedback, we can 
calculate its overall ranking score predisposed to 
the opposite intent as follows: 
              (1) 
                   
⋅−
α
where the O_score is the relevance score to the 
opposite intent, I_score is that to the query intent 
and  α  is  a  weighting  factor.  On  the  basis,  we 
re-rank the unseen feedbacks in ascending order. 
That  is,  the  feedback  with  the  largest  score  ap-
pears at the bottom of the ranked list. 

score

score

score

O

It is worthwhile to emphasize that although the 
overall ranking score, i.e. R_score, looks similar 
to Wang (Wang et al., 2008) who adopts the in-
versely discounted value (i.e. the relevance score 
is  calculated  as 
)  to  re-rank 
feedbacks in descending order, they are actually 
quite different because our overall ranking score 
as shown in Equation (1) is designed to depress 
negative feedbacks, thereby achieving the similar 
effect to filtering. 

O _⋅α

score

score

I _

-

)

(

)

)

rqO
−−

3.2  Representing Opposite Intent 
It is necessary for the representation of opposite 
intent  to  obey  two  basic  rules:  1)  the  opposite 
intent  should  be  much  different  from  the  query 
intent;  and  2)  it  should  reflect  the  independent 
effect of negative feedbacks. 

intent  by  using 

Given  a  query,  it  seems  easy  to  represent  its 
a  vector  of 
opposite 
high-weighted 
terms  of  negative  feedbacks. 
However, the vector is actually a “close relative” 
of  query  intent  because  the  terms  often  have 
much  overlap  with  that  of  relevant  feedbacks. 
And the overlapping terms are exactly the source 
of  the  highly  ranked  negative  feedbacks.  Thus 
we  should  throw  off  the  overlapping  terms  and 
focus on the rest instead.   

In this paper, we propose two simple facilities 
in representing opposite intent. One is a vector of 
the  weighted  terms  (except  query  terms)  occur-
ring in the known negative feedbacks, named as 
,  while  another  further  filters  out  the 
( qO −
high-weighted  terms  occurring  in  the  known 
relevant feedbacks, named as 
. Although 
  filters  out  query  terms,  the  terms  are  so 
( qO −
sparse that they contribute little to opposite intent 
learning.  Thus,  we  will  not  explore 
  fur-
ther  in  this  paper  (Our  preliminary  experiments 
confirm our reasoning). In contrast, 
  not 
only differs from the representation of query in-
tent due to its exclusion of query terms but also 
emphasize  the  low-weighted  terms  occurring  in 
negative 
to  exclusion  of 
high-weighted  terms  occurring  in  the  known 
relevant feedbacks. 
3.3  Employing Opposite Intent 
Another  key  issue  in  our  re-ranking  scheme  is 
how  to  measure  the  relevance  of  all  the  feed-
backs to the opposite intent, i.e. O_score, thereby 
the  ranking  score  R_score.  For  simplicity,  we 
only  consider  Boolean  measures  in  employing 
opposite  intent  to  calculate  the  ranking  score 
R_score. 

feedbacks  due 

Assume  that  given  a  query,  there  are 

 
N
N   known nega-
known  relevant  feedbacks  and 
tive ones. First, we adopt query expansion to ac-
quire  the  representation  of  query  intent.  This  is 
done  by  pouring  all  terms  of  the 
  relevant 
feedbacks and query terms into a bag of words, 
where all the occurring weights of each term are 

rqO
−−

( qO −

N

)

)

(

439

(

)

(

I

r

q
++

accumulated,  and  extracting  n 
top-weighted 
. 
terms  to  represent  the  query  intent  as 
)
Then, we use the  N   negative feedbacks to rep-
the  n-dimensional  opposite 
resent 
intents 
. For any unseen pseudo-feedback u, we 
qO
r
−−
also  represent  it  using  an  n-dimensional  vector 
  which contains its n top-weighted terms. In 
)(uV
all  the  representation  processes,  the  TFIDF 
weighting is adopted. 
Thus,  for  an  unseen  pseudo-feedback  u,  the 
relevance scores to the query intent and the op-
posite intent can be measured as: 
uVB
q
r
  {
(
(
++
                       
qOuVB
(
(
  {
−−
where 

score
I
_
O
score
_
  indicates Boolean calculation: 
},{ ∗∗B
YXB
},{

              (2) 

u
)(
=
u
)(
=

}  )
r
}  )

I
  ),
  ),

  },

X

∈

                           

                  (3) 

=∑

Yxb
,{
i
if
x
,1
       
  
⎧
i
⎨
if
x
  
,0
      
i
⎩

x
i
Y
Y

∈
∉

Yxb
},{

i

=

(

)

(

)

I

r

r

−−

q
++

qO

In  particular,  we  simply  set  the  factor  α,  as 
mentioned in Equation (1), to 1 so as to balance 
the effect of query intent and its opposite intent 
on the overall ranking score. The intuition is that 
if an unseen pseudo-feedback has more overlap-
,  it  will 
ping  terms  with 
has  higher  probability  of  being  depressed  as  an 
negative feedback. 

  than 

Two  alternatives  to  the  above  Boolean  meas-
ure  are  to  employ  the  widely-adopted  VSM  co-
sine  measure  and  Kullback-Liebler  (KL)  diver-
gence  (Thollard  et  al.,  2000).  However,  such 
term-weighting  alternatives  will  seriously  elimi-
nate  the  effect  of  low-weighted  terms,  which  is 
core  of  our  negative  feedback-based  re-ranking 
scheme.   
3.4 Hierarchical Distance (HD) Measure   
The  proposed  method  in  Section  3.3  ignores 
two  key  issues.  First,  given  a  query,  although 
search  engine  has  thrown  away  most  opposite 
intents, 
the 
pseudo-feedback  still  involves  more  than  one 
opposite  intent.  However,  the  representation 
  has the difficulty in highlighting all the 
qO
opposite intents because the feature fusion of the 
representation smoothes the independent charac-
teristics  of  each  opposite  intent.  Second,  given 
several opposite intents, they have different lev-
els  of  effects  on  the  negative  score 
. 
u
)(
And the effects cannot be measured by the uni-
lateral score.   

unavoidable 

that 

score

−−

is 

it 

O

_

r

)

(

Figure 2. Weighted distance calculation 

 

To solve the issues, we propose a hierarchical 
distance  based  negative  measure,  abbr.  HD, 
which  measures  the  distances  among  feedbacks 
in  a  hierarchical  clustering  tree,  and  involves 
them into hierarchical division of relevance score. 
Given  two  random  leaves  u  and  v  in  the  tree, 
their HD score is calculated as: 
vu
rel
),(
                         
vuW
),(

                    (4) 

score

vu
),(

HD

=

_

rel

),( ∗∗

  indicates textual similarity, 

where 
 
),( ∗∗W
indicates the weighted distance in the tree, which 
is calculated as: 
                               

                          (5) 

vuW
),(

=

vuw
),(
i

∑

mi
∈

score

),( ∗∗W

),( kjW

),( ∗∗iw

),( ∗∗iw

where m is the total number of the edges between 
two  leaves, 
  indicates  the  weight  of  the 
i-th  edge.  In  this  paper,  we  adopt  CLUTO  to 
generate the hierarchical binary tree, and simply 
  be-
let  each 
comes to be the number of edges m, for example, 
the 

  equals 5 in Figure 2. 

  equal  1.  Thus  the 

its  modified 

On the basis, given an unseen feedback u, we 
re-ranking  score 
can  acquire 
  by  following  steps.  First,  we  regard 
R _ ′
each  known  negative  feedback  as  an  opposite 
intent, following the two generative rules (men-
tioned 
section  3.2) 
its 
n-dimensional  representation
.  Addition-
ally we represent both the known relevant feed-
backs 
as 
n-dimensional  term  vectors.  Second,  we  cluster 
these feedbacks to generate a hierarchical binary 
tree and calculate the HD score for each pair of 
, where  ∗   denotes a leaf in the tree except u. 
),( ∗u
Thus the modified ranking score is calculated as: 

to  generate 
qO
(
−−

feedback  u 

the  unseen 

and 

in 

r

)

∑

R
_ 
′

score

=

HDI

_

score

vu
,(
i

)

−

HDI

_

score

vu
,(

(6) 

)

j

Ni
∈
iv   indicates  the  i-th  known  negative 
where 
feedback in the leaves,  N   is the total number of 

∑

Nj
∈

440

  is the total number of 

j   indicates  the  j-th  known  relevant  feed-
v , 
v
v . Besides, we 
back, 
N
still adopt Boolean value to measure the textual 
similarity 
  in  both  clustering  process  and 
ranking  score  calculation,  thus  the  HD  score  in 
the formula (6) can be calculated as follows: 

),( ∗∗

rel

          

      

HD

_

score

vu
),(

=

         

          

         

HD

_

score

vu
),(

=

O

vVuVB
  {

}  )(

(
  ),
vuW
),(
score
_
vuW
),(

u
)(

              (7) 

3.5 Obstinateness Factor 
Additionally  we  involve  an  interesting  feature, 
i.e.  the  obstinate  degree,  into  our  re-ranking 
scheme. The degree is represented by the rank of 
negative  feedbacks  in  the  original  retrieval  re-
sults.  That  is,  the  more  “topping  the  list”  an 
negative feedback is, the more obstinate it is.   

Therefore  we  propose  a  hypothesis  that  if  a 
feedback  is  close  to  the  obstinate  feedback,  it 
should  be  obstinate  too.  Thus  given  an  unseen 
feedback u, its relevance to an opposite intent in 
HD can be modified as: 
                   
1(
+

              (8) 

score

score

u
)(

u
)(

=′

O

O

_

_

)

⋅

β
rnk

rnk

where 

  indicates the rank of the opposite 
intent  in  original  retrieval  results  (Note:  in  HD, 
every  known  negative  feedback  is  an  opposite 
β  is  a  smoothing  factor.  Because  as-
intent), 
cending order is used in our re-ranking process, 
, the 
by the weighting coefficient, i.e. 
feedback  close  to  the  obstinate  opposite  intents 
will  be  further  depressed.  But  the  coefficient  is 
not commonly used. In HD, we firstly ascertain 
the feedback closest to u, and if the feedback is 
known  to  be  negative,  set  to  max
,  we  will  use 
the  Equation  (8)  to  punish  the  pair  of  (u,  max
) 
alone, otherwise without any punishment. 
4.  EXPERIMENTAL SETTING 

rnkβ+

1(

v

v

)

/

4.1  Data Set 
We evaluate our methods with two TDT collec-
tions: TDT 2002 and TDT 2003. There are 3,085 
stories in the TDT 2002 collection are manually 
labeled  as  relevant  to  40  news  topics,  30,736 
ones  irrelevant  to  any  of  the  topics.  And  3,083 
news stories  in the TDT 2003 collection are la-
beled  as  relevant  to  another  40  news  topics, 
15833 ones irrelevant to them. In our evaluation, 

we  adopt  TDT  2002  as  training  set,  and  TDT 
2003 as test set. Besides, only English stories are 
used,  both  Mandarin  and  Arabic  ones  are  re-
placed  by  their  machine-translated  versions  (i.e. 
mttkn2 released by LDC). 
good 
26 
22 

Corpus 
TDT 2002 
TDT 2003 

fair 
7 
10 

poor 

7 
8 

Table 1. Number of queries referring to different 
types of feedbacks (Search engine: Lucene 2.3.2) 
In our experiments, we realize a simple search 
engine  based  on  Lucene  2.3.2  which  applies 
document  length  to  relevance  measure  on  the 
basis  of  traditional  literal  term  matching.  To 
emulate the real retrieval process, we extract the 
title  from  the  interpretation  of  news  topic  and 
regard it as a query, and then we run the search 
engine on the TDT sets and acquire the first 1000 
pseudo-feedback  for  each  query.  All  feedbacks 
will be used as the input of our re-ranking proc-
ess,  where  the  hand-crafted  relevant  stories  de-
fault to the clicked feedbacks. By the search en-
gine,  we  mainly  obtain 
types  of 
pseudo-feedback:  “good”,  “fair”  and  “poor”, 
where  “good”  denotes  that  more  than  5  clicked 
(viz. relevant) feedbacks are in the top 10, “fair” 
denotes more than 2 but less than 5, “poor” de-
notes less than 2. Table 1 shows the number of 
queries referring to different types of feedbacks. 
4.2  Evaluation Measure 
We use three evaluation measures in experiments, 
P@n, NDCG@n and MAP. Thereinto, P@n de-
notes  the  precision  of  top  n  feedbacks.  On  the 
basis, NDCG takes into account the influence of 
position to precision. NDCG at position n is cal-
culated as: 

three 

i∑ =

n
i

1

ur
(
)
2
log(
1
Z
n

−
i
+

1
)

1
Z
n

⋅

n

=

=

N

@

@

DCG

NDCG

           
      (9) 
where  i  is  the  position  in  the  result  list,  Zn  is  a 
normalizing  factor  and  chosen  so  that  for  the 
perfect list DCG at each position equals one, and 
r(ui) equals 1 when ui is relevant feedback, else 0. 
While MAP additionally takes into account recall, 
calculated as:   
1
               
      (10) 
m
where m is the total number of queries, so MAP 
gives the average measure of precision and recall 

∑ ∑=

))@()
⋅
i

(1
R
i

MAP

ur
(
i

m
i

k
j

1
=

=

p

1

j

j

441

feedbacks  and 

for  multiple  queries,  Ri  is  the  total  number  of 
feedbacks relevant to query i, and k is the num-
ber  of  pseudo-feedback  to  the  query.  Here  k  is 
indicated to be 1000, thus Map can give the av-
erage measure for all positions of result list. 
4.3  Systems 
We  conduct  experiments  using  four  main  sys-
tems,  in  which  the  search  engine  based  on  Lu-
cene 2.3.2, regarded as the basic retrieval system, 
provides  the  pseudo-feedback  for  the  following 
three re-ranking systems. 
Exp-sys: Query is expanded by the first N known 
relevant 
represented  by  an 
n-dimensional vector which consists of n distinct 
terms.  The  standard  TFIDF-weighted  cosine 
metric  is  used  to  measure  the  relevance  of  the 
unseen pseudo-feedback to query. And the rele-
vance-based descending order is in use. 
Wng-sys:  A  system  realizes  the  work  of  Wang 
(Wang  et  al.,  2008),  where  the  known  relevant 
feedbacks are used to represent query intent, and 
the  negative  feedbacks  are  used  to  generate  op-
posite intent. Thus, the relevance score of a feed-
back  is  calculated  as  I_scorewng-
wng, 
and the relevance-based descending order is used 
in re-ranking. 
Our-sys:  A  system  is  approximately  similar  to 
Wng-sys except that the relevance is measured by 
⋅α I_scoreour and the pseudo-feedback 
O_scoreour-
is re-ranked in ascending order.   

Additionally  both  Wng-sys  and  Our-sys  have 
three versions. We show them in Table 2, where 
“I”  corresponds  to  the  generation  rule  of  query 
intent, “O” to that of opposite intent, Rel. means 
relevance measure, u is an unseen feedback, v is 
a known relevant feedback,  v   is a known nega-
tive feedback. 
5.  RESULTS 

O_score

⋅wα

N

  and 

5.1  Main Training Result 
We evaluate the systems mainly in two circum-
N   equal  1  and 
stances:  when  both 
when they equal 5. In the first case, we assume 
that retrieval capability is measured under given 
few known feedbacks; in the second, we emulate 
the  first  page  turning  after  several  feedbacks 
have been clicked by searchers. Besides, the ap-
proximately  optimal  value  of  n  for  the  Exp-sys, 
which is trained to be 50, is adopted as the global 
value  for  all  other  systems.  The  training  results 
are shown in Figure 3, where the Exp-sys never 

gains much performance improvement when n is 
greater than 50. In fairness to effects of “I” and 
“O”  on  relevance  measure,  we  also  make  n  
equal  50.  In  addition,  all  the  discount  factors 
(viz.α,  αw2  and  αw3)  initially  equal  1,  and  the 
smoothing factor  β  is trained to be 0.5. 

n-dimensional vector for each v, Number of v in use is N

None 
N
i

1 ∑ =

=

(

1

Rel.

R

_

score
w

cos(

vu
/)),

N

 

Number of v in use is N, all v combine into a n-dimensional 
bag of words bw2

“I”
“O” Number  of  v   in  use  is  N  ,  all  v  combine  into  a 

n-dimensional words bag 
cos(

score
w

R

=

_

2

2wb
bu
,
w
2

 
)

−

α
w

2

⋅

cos(

bu
,

)

 

w

2

Similar  generation  rules  to  Wng-sys2  except  that  query 
3wb
terms are removed from bag of words 
⋅
α
w
3

  and 
bu
,

3wb
)
 
w
3

cos(

R

−

_

)

 

score
bu
,
cos(
=
w
w
3
3
q
I
r
(
)
  in section 3.3 
++
qO
r
)
(
  in section 3.2 
−−
score
score
O
_
_
=

⋅
α

R

−

_

I

score

 

The same generation rules to Our-sys1 

HD algorithm: 
∑
R
_ 
′

score

=

Ni
∈

HDI

_

score

vu
,(
i

)

−

HDI

_

score

vu
,(

)

j

∑

Nj
∈

The same generation rules to Our-sys1 

HD algorithm + obstinateness factor: 
O

score

u
)(

=′

1(

O

+

_

)

⋅

β
rnk

_

score

u
)(

 

Wng-sys1

Wng-sys2

Wng-sys3

Our-sys1

Our-sys2

Our-sys3

“I”
“O”

Rel.
“I”
“O”
Rel.

“I”
“O”
Rel.
“I”
“O”

Rel.

“I”
“O”

Rel.

Table 2. All versions of both Wngs and Ours 

 

For 

(such 
N

as  P@10 

Figure 3. Parameter training of Exp-sys 
all 

each  query  we 
including 

re-rank 
the 
pseudo-feedback, 
that  defined  as 
known,  so  P@20  and  NDCG@20  are  in  use  to 
avoid  over-fitting 
and 
N   equal  5  ). 
NDCG@10  given  both 
We  show  the  main  training  results  in  Table  3, 
where  our  methods  achieve  much  better  per-
formances than the re-ranking methods based on 
learning  when  N= N =5. 
relevant  feedback 
Thereinto,  our  basic  system,  i.e.  Our-sys1,  at 
least  achieves  approximate  5%  improvement  on 
P@20, 3% on NDCG@20 and 1% on MAP than 
the  optimal  wng-sys  (viz.  wng-sys1).  And  obvi-

  and 

442

ously  the  most  substantial  improvements  are 
contributed  by  the  HD  measure  which  even  in-
the  P@20  of  Our-sys1  by  8.5%, 
creases 
NDCG@20 by 13% and MAP by 9%. But it is 
slightly disappointing that the obstinateness fac-
tor only has little effectiveness on performance 
improvement,  although  Our-sys3  nearly  wins 
the  best  retrieval  results.  This  may  stem  from 
“soft” punishment on obstinateness, that is, for 
an  unseen  feedback,  only  the  obstinate  com-
panion  closest  to  the  feedback  is  punished  in 
relevance measure. 

- 

P@20 

Our-sys1  Our-sys2 Exp-sys  Wng-sys1
0.63125  0.7051
0.6603  0.8141
NDCG@20  0.7614  0.8587
0.8080  0.7797
0.6583  0.7928
0.5955  0.7010
Table 3. Main training results 

MAP 

Basic 
0.6588
0.6944
0.6440

N

=N

  and 

N ( =N

It is undeniable that all the re-ranking systems 
work  worse  than  the  basic  search  engine  when 
N =1. 
the known feedbacks are rare, such as 
This  motivates  an  additional  test  on  the  higher 
N =9),  as  shown 
values  of  both 
in Table 4. Thus it can be found that most of the 
re-ranking  systems  achieve  much  better  per-
formance  than  the  basic  search  engine.  An  im-
portant reason for this is that more key terms can 
be  involved  into  representations  of  both  query 
intent  and  its  opposite  intent.  So  it  seems  that 
more  manual  intervention  is  always  reliable. 
However  in practice,  seldom  searchers  are  will-
ing to use an unresponsive search engine that can 
only  offer  relatively  satisfactory  feedbacks  after 
lots  of  click-through  and  page  turning.  And  in 
fact  at  least  two  pages  (if one  page  includes  10 
pseudo-feedback) need to be turned in the train-
N   equal  9.  So 
ing  corpus  when  both 
we just regard the improvements benefiting from 
high  click-through  rate  as  an  ideal  status,  and 
still  adopt  the  practical  numerical  value  of 
 
N
and 
5.2  Constraint from Query 
that  Exp-sys  always 
A  surprising  result 
achieves the worst MAP value, even worse than 
the basic search engine even if high value of N is 
in use, such as the performance when N equal 9 
in Table 4. It seems to be difficult to question the 
reasonability  of  the  system  because  it  always 
selects the most key terms to represent query in-
tent  by  query  expansion. But  an  obvious  differ-
ence  between Exp-sys  and  other  re-ranking  sys-
tems  could  explain  the  result.  That  is  the  query 

N =5, to run following test. 

N , i.e. 

  and 

=N

is 

N

terms consistently involved in query representa-
tion by Exp-sys. 
systems
Basic 
Exp-sys 

Factor 

Wng-sys2

Wng-sys1

N = N

P@20 
0.6588
0.4388
0.5613
0.5653
0.6564
0.5436
0.5910
0.5436
0.5910
0.5628
0.7031
0.6474
0.7885
0.6026
0.7897
Table 4. Effects of 

-
1
5
1
5
1
5
1
5
1
5
1
5
1
5

Our-sys1

Our-sys2

Our-sys3

Wng-sys3

N
performance (when 

- 
- 
- 
- 
- 

NDCG@20 
0.6944 
0.4887 
0.6365 
0.6184 
0.7361 
0.6473 
0.7214 
0.6162 
0.6720 
0.6358 
0.7640 
0.6761 
0.8381 
0.6749 
0.8388 
  and 
=N

MAP 
0.6440 
0.3683 
0.5259 
0.5253 
0.6506 
0.4970 
2wα =1
0.5642 
2wα =1
0.4970 
3wα =1
0.5642 
3wα =1
0.4812  α=1 
0.6603  α=1 
0.5967  α=1 
0.7499  α=1 
0.5272  β=0.5
0.7464  β=0.5
N   on re-ranking 
N =9, n= n =50) 

In fact, Wng-sys1 never overly favor the query 
terms because they are not always the main body 
of  an  independent  feedback,  and  our  systems 
even  remove  the  query  terms  from  the  opposite 
intent directly. Conversely Exp-sys continuously 
enhances the weights of query terms which result 
in over-fitting and bias. The visible evidence for 
this  is  shown  in  Figure  4,  where  Exp-sys 
achieves  better  Precision  and  NDCG  than  the 
basic  search  engine  at  the  top  of  result  list  but 
worse  at  the  subsequent  parts.  The  results  illus-
trate  that  too  much  emphasis  placed  on  query 
terms  in  query  expansion  is  only  of  benefit  to 
elevating  the  originally  high-ranked  relevant 
feedback but powerless to pull the straggler out 
of the bottom of result list.   

 

Figure 4. MAP comparison (basic vs Exp) 

5.3  Positive Discount Loss 
Obviously Wang (Wang et al., 2008) has noticed 
the negative effects of query terms on re-ranking. 
Therefore his work (reproduced by Wng-sys1, 2, 
3 in this paper) avoids arbitrarily enhancing the 
terms  in  query  representation,  even  removes 
them as Wng-sys3. This indeed contributes to the 

443

improvement  of  the  re-ranking  system,  such  as 
the better performances of Wng-sys1, 2, 3 shown 
in  Table  3,  although  Wng-sys3  has  no  further 
improvement than Wng-sys2 because of the spar-
sity  of  query  terms.  On  the  basis,  the  work  re-
gards  the  terms  in  negative  feedbacks  as  noises 
and reduces their effects on relevance measure as 
much  as  possible.  This  should  be  a  reasonable 
scheme, but interestingly it does not work well in 
our 
although 
Wng-sys2 and Wng-sys3 eliminate the relevance 
score  calculated  by  using  the  terms  in  negative 
feedbacks,  they  perform  worse  than  Wng-sys1 
which never make any discount. 
∗α =1 
0.6603 
0.5642 
0.5642 

systems 
Our-sys1 
Wng-sys2 
Wng-sys3 

∗α =0.5 
0.4751 
0.6030 
0.6084 

experiments.  For 

∗α =2 
0.6901 
0.4739 
0.4739 

example, 

Table 5. Effects on MAP   

that 

2wα   and 

  Additionally  when  we  increase  the  discount 
factor 
3wα ,  as  shown  in  Table  5,  the 
performances (MAP) of Wng-sys2 and Wng-sys3 
further  decrease.  This 
the 
high-weighted  terms  of  high-ranked  negative 
feedbacks are actually not noises. Otherwise why 
do the feedbacks have high textual similarity to 
query  and  even  to  their  neighbor  relevant  feed-
backs?  Thus  it  actually  hurts  real  relevance  to 
discount the effect of the terms. 

illustrates 

Conversely  Our-sys1  can  achieve  further  im-
provement  when  the  discount  factor  α   in-
creases,  as  shown  in  Table  5.  It  is  because  the 
discount contributes to highlighting minor terms 
of  negative  feedbacks,  and  these  terms  always 
have  little  overlap  with  the  kernel  of  relevant 
feedbacks. Additionally the minor terms are used 
to  generate  the  main  body  of  opposite  intent  in 
our  systems,  thus  the  discount  can  effectively 
separate opposite intent from positive query rep-
resentation.  Thereby  we  can  use  relatively  pure 
representation  of  opposite  intent  to  detect  and 
repel subsequent negative feedbacks. 
5.4  Availability of Minor Terms 
Intuitively we can involve more terms into query 
representation  to  alleviate  the  positive  discount 
loss. But it does not work in practice. For exam-
ple, Wng-sys2 shown in Figure 5 has no obvious 
improvement no matter how many terms are in-
cluded 
representation.  Conversely 
Our-sys1  can  achieve  much  more  improvement 
when  it  involves  more  terms  into  the  opposite 

in  query 

intent.  For  example,  when  the  number  of  terms 
increases to 150, Our-sys1 has approximately 5% 
better MAP than Wng-sys2, shown in Figure 5. 

 
Figure 5. Effects on MAP in modifying the di-

mensionality n (when N= N =5,  α=1) 

  This  result  illustrates  that  minor  terms  are 
available  for  repelling  negative  feedbacks,  but 
too weak to recall relevant feedbacks. In fact, the 
minor  terms  are  just  the  low-weighted  terms  in 
text. Current text representation techniques often 
ignore  them  because  of  their  marginality.  How-
ever  minor  terms  can  reflect  fine  distinctions 
among  feedbacks,  even  if  they  have  the  same 
topic.  And  the  distinctions  are  of  great  impor-
tance  when  we  determine  why  searchers  say 
“Yes” to some feedbacks but “No” to others. 

global  Factor

systems

Wng-sys1

Our-sys1

Our-sys2

Our-sys3

fair 

poor 

MAP 
P@20 

metric 
P@20 

good
0.7682 0.5450  0.2643  0.6205
NDCG@20 0.8260 0.6437  0.4073  0.7041
0.6634 0.4541  0.9549  0.6620
0.8273 0.5700  0.2643  0.6603
NDCG@20 0.8679 0.6620  0.4017  0.7314
0.6740 0.4573  0.9184  0.6623
0.8523 0.7600  0.2714  0.7244
NDCG@20 0.8937 0.8199  0.4180  0.7894
0.7148 0.6313  0.9897  0.7427
0.8523 0.7600  0.2714  0.7244
NDCG@20 0.8937 0.8200  0.4180  0.7894
0.7145 0.6292  0.9897  0.7420

MAP 
Table 6. Main test results 

MAP 
P@20 

MAP 
P@20 

- 

α=2,
β=0.5

α=2,
β=0.5

α=2,
β=0.5

5.5  Test Result 
We run all systems on test corpus, i.e. TDT2003, 
but  only  report  four  main  systems:  Wng-sys1, 
Our-sys1, Our-sys2 and Our-sys3. Other systems 
are omitted because of their poor performances. 
The test results are shown in Table 6 which in-
cludes  not  only  global  performances  for  all  test 
queries but also local ones on three distinct types 
of queries, i.e. “good”, “fair” and “poor”. There-
into,  Our-sys2  achieves  the  best  performance 
around  all  types  of  queries.  So  it  is  believable 

444

that  hierarchical  distance  of  clustering  tree  al-
ways plays an active role in distinguishing nega-
tive feedbacks from relevant ones. But it is sur-
prising  that  Our-sys3  achieves  little  worse  per-
formance  than  Our-sys2.  This  illustrates  poor 
robustness of obstinateness factor. 

Interestingly, the four systems all achieve very 
high MAP scores but low P@20 and NDCG@20 
for  “poor”  queries.  This  is  because  the  queries 
have  inherently  sparse  relevant  feedbacks:  less 
than  6‰  averagely.  Thus  the  highest  p@20  is 
only  approximate  0.3,  i.e.  6/20.  And  the  low 
NDCG@20  is  in  the  same  way.  Besides,  all 
MAP scores for “fair” queries are the worst. We 
find that this type of query involves  more  mac-
roscopic  features  which  results  in  more  kernels 
of  negative  feedbacks.  Although  we  can  solve 
the issue by increasing the dimensionality of op-
posite  intent,  it  undoubtedly  impairs  the  effi-
ciency of re-ranking.   
6.  CONCLUSION 

This paper proposes a new re-ranking scheme 
to well explore the opposite intent. In particular, 
a  hierarchical  distance-based  (HD)  measure  is 
proposed to differentiate the opposite intent from 
the  true  query  intent  so  as  to  repel  negative 
feedbacks.  Experiments  show  substantial  out-
performance of our methods. 

in 

pseudo-feedback. 

Although  our  scheme  has  been  proven  effec-
tive in most cases, it fails on macroscopic queries. 
In fact, the key difficulty of this issue lies in how 
to ascertain the focal query intent given various 
kernels 
Fortunately, 
click-through data provide some useful informa-
tion  for  learning  real  query  intent.  Although  it 
seems feasible to generate focal intent represen-
tation  by  using  overlapping  terms  in  clicked 
feedbacks, such representation is just a reproduc-
tion of macroscopic query since the overlapping 
terms  can  only  reflect  common  topic  instead  of 
focal intent. Therefore, it is important to segment 
clicked  feedbacks  into  different  blocks,  and  as-
certain the block of greatest interest to searchers.   
References 
Allan,  J.,  Lavrenko,  V.,  and  Nallapati,  R.  2002. 
UMass  at  TDT  2002,  Topic  Detection  and 
Tracking: Workshop. 

Craswell, N., and Szummer, M. Random walks on 
the  click  graph.  2007.  In  Proceedings  of  the 
Conference  on  Research  and  Development  in 

Information  Retrieval.  SIGIR  '30.  ACM  Press, 
New York, NY, 239-246. 

Cao, G. H., Nie, J. Y., and Gao, J. F. 2008. Stephen 
Robertson. Selecting Good Expansion Terms for 
Pseudo-Relevance  Feedback.  In  Proceedings  of 
the Conference on Research and Development in 
Information  Retrieval.  SIGIR  '31.  ACM  Press, 
New York, NY, 243-250. 

Chum, O., Philbin, J., Sivic, J., and Zisserman, A. 
2007. Automatic query expansion with a genera-
tive  feature  model  for  object  retrieval.  In  Pro-
ceedings of the 11th International Conference on 
Computer Vision, Rio de Janeiro, Brazil, 1–8. 

Joachims, T., Granka, L., and Pan, B. 2003. Accu-
rately Interpreting Clickthrough Data as Implicit 
Feedback.  In  Proceedings  of  the  Conference  on 
Research  and  Development  in  Information  Re-
trieval. SIGIR '28. New York, NY, 154-161. 

Resampling 

Lee, K. S., Croft, W. B., and Allan, J. 2008 A Clus-
ter-Based 
for 
Pseudo-Relevance  Feedback.  In  Proceedings  of 
the Conference on Research and Development in 
Information  Retrieval.  SIGIR  '31.  ACM  Press, 
New York, NY, 235-242. 

Method 

Thollard,  F.,  Dupont,  P.,  and  Higuera,  L.2000. 
Probabilistic  DFA 
Inference  Using  Kull-
back-Leibler  Divergence  and  Minimality.  In 
Proceedings  of  the  17th  Int'l  Conf  on  Machine 
Learning.  San  Francisco:  Morgan  Kaufmann, 
975-982. 

Teevan,  J.  T.,  Dumais,  S.  T.,  and  Liebling,  D.  J. 
2008.  To  Personalize  or  Not  to  Personalize: 
Modeling Queries with Variation in User Intent. 
In  Proceedings  of  the  Conference  on  Research 
and  Development 
in  Information  Retrieval. 
SIGIR '31. New York, NY, 163-170. 

Wang,  X.  H.,  Fang,  H.,  and  Zhai,  C.  X.  2008.  A 
Study of Methods for Negative Relevance Feed-
back.  In  Proceedings  of  the  Conference  on  Re-
search  and  Development  in  Information  Re-
trieval. SIGIR '31. ACM Press, New York, NY, 
219-226. 

Wang, X. H., Fang, H., and Zhai, C. X. 2007. Im-
prove retrieval accuracy for difficult queries us-
ing  negative  feedback.  In  Proceedings  of  the 
sixteenth  ACM  conference  on  Conference  on 
information  and  knowledge  management.  ACM 
press, New York, NY, USA, 991-994. 

Zhang,  P.,  Hou,  Y.  X.,  and  Song,  D.  2009.  Ap-
proximating True Relevance Distribution from a 
Mixture  Model  based  on  Irrelevance  Data.  In 
Proceedings of the Conference on Research and 
Development  in  Information  Retrieval.  SIGIR 
'31. ACM Press, New York, NY, 107-114. 

