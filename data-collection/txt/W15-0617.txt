



















































Embarrassed or Awkward? Ranking Emotion Synonyms for ESL Learners' Appropriate Wording


Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 144–153,
Denver, Colorado, June 4, 2015. c©2015 Association for Computational Linguistics

Embarrassed or Awkward?  

Ranking Emotion Synonyms for ESL Learners' Appropriate Wording  

 Wei-Fan Chen 

Academia Sinica 

viericwf@iis.sinica.edu.tw 

Mei-Hua Chen 

Department of Foreign Language and 

Literature,  

Tunghai University 

chen.meihua@gmail.com 

Lun-Wei Ku 

Academia Sinica 

lwku@iis.sinica.edu.tw 

 

 

Abstract 

We introduce a novel framework based on the 

probabilistic model for emotion wording as-

sistance. The example sentences from the on-

line dictionary, Vocabulary.com are utilized as 

the training data; and the writings in a de-

signed ESL’s writing task are the testing cor-

pus. The emotion events are captured by 

extracting patterns of the example sentences. 

Our approach learns the joint probability of 

contextual emotion events and the emotion 

words from the training corpus. After extract-

ing patterns in the testing corpus, we then ag-

gregate their probabilities to suggest the 

emotion word that describes the ESL’s con-

text most appropriately. We evaluate the pro-

posed approach by the NDCG@5 of the 

suggested words for the writings in the testing 

corpus. The experiment result shows our ap-

proach can more appropriately suggest the 

emotion words compared to SVM, PMI and 

two representative on-line reference tools, 

PIGAI and Thesaurus.com. 

1 Introduction 

Most English-as-a-second-language (ESL) learners 

have been found to have difficulties in emotion 

vocabulary (Pavlenko, 2008). With limited lexical 

knowledge, learners tend to use common emotion 

words such as angry and happy to describe their 

feelings. Moreover, the learner’s first language 

seems to lead to inappropriate word choices (Altar-

riba and Basnight-Brown, 2012).  Many learners 

consult the thesaurus for synonyms of emotion 

words; typically, the synonyms suggested come 

with little or no definition or usage information. 

Moreover, the suggested synonyms seldom take 

into account contextual information. As a result, 

the thesaurus does not always help language learn-

ers select appropriately nuanced emotion words, 

and can even mislead learners into choosing im-

proper words that sometimes convey the wrong 

message (Chen et al., 2013). Take embarrassed 

and awkward for example: although they both de-

scribe situations where people feel uneasy or un-

comfortable, in practice they are used in different 

scenarios. According to Vocabulary.com, embar-

rassed is more self-conscious and can result from 

shame or wounded pride: for instance, He was too 

embarrassed to say hello to his drunken father on 

the street. On the other hand, awkward would be 

“socially uncomfortable” or “unsure and con-

strained in manner”: He felt awkward and reserved 

at parties. These examples illustrate not only the 

nuances between synonymous emotion words, but 

also the difficulty for language learners in deter-

mining proper words. There is a pressing need for 

a reference resource providing a sufficient number 

of emotion words and their corresponding usage 

information to help language learners expand their 

knowledge of emotion words and learn proper 

emotional expressions. 

To address this issue, we propose a novel ap-

proach to help differentiate synonyms of emotion 

words based on contextual clues—Ranking Emo-

tional SynOnyms for language Learners’ Vocabu-

lary Expansion (RESOLVE). This involves first 

the learning of emotion event scores between an 

event and an emotion word from a corpus: these 

144



scores quantify how appropriate an emotion word 

is to describe a given event. Subsequently, based 

on the emotion event in the learner’s text, 

RESOLVE suggests a list of ranked emotion words. 

2 Related Work 

Previous studies related to RESOLVE can be di-

vided into four groups: paraphrasing, emotion 

classification, word suggestion and writing as-

sessment. The aim of paraphrasing research is how 

to express the same information in various ways. 

Such alternative expressions of the same infor-

mation rely on paraphrase pairs which map an ex-

pression to a previously learned counterpart, or 

inference rules that re-structure the original sen-

tences. Most work uses machine translation tech-

niques such as statistical machine translation or 

multiple-sequence alignment to extract paraphrase 

pairs from monolingual corpora (Barzilay and 

McKeown, 2001; Keshtkar and Inkpen, 2010), or 

bilingual corpora (Bannard and Callison-Burch, 

2005; Callison-Burch, 2008; Chen et al., 2012). 

Approaches based on inference rules, on the other 

hand, derive these rules by analyzing the depend-

ency relations of paraphrase sentences (Lin and 

Pantel, 2001; Dinu and Lapata, 2010). Alternative 

expressions can be achieved by applying inference 

rules to rephrase the original sentence. In general, 

the focus of paraphrasing is sentence variation, 

which involves sentence re-structuring, phrase al-

ternation and word substitution. Generating an al-

ternative sentence without changing the sentence’s 

original meaning is the main concern. For 

RESOLVE, in contrast, rather than attempting 

preservation, the focus is on appropriate in-context 

word substitution. There are several online para-

phrasing tools. PREFER1 (Chen et al., 2012) is an 

online paraphrase reference interface that generates 

phrase-level paraphrases using a combination of 

graph and PageRank techniques. Chen shows a 

significant improvement in learner paraphrase 

writing performance. For RESOLVE, instead of 

pursuing participant improvements in semantically 

equivalent rephrasing, the aim is to suggest contex-

tually appropriate wording. Microsoft Contextual 

Thesaurus2 (MCT) is similar to PREFER: it is an 

online reference tool that smartly rephrases an in-

                                                           
1 http://service.nlpweb.org/PREFER 
2 http://labs.microsofttranslator.com/thesaurus/ 

put sentence into various alternative expressions, 

using both word-level and phrase-level substitution. 

However, we know of no study that evaluates 

learning effectiveness when using MCT. Finally, 

SPIDER (Barreiro, 2011) targets document-level 

editing; it relies on derivations from dictionaries 

and grammars to paraphrase sentences, aiming at 

reducing wordiness and clarifying vague or impre-

cise terms. In short, rather than offering better sug-

gestions, paraphrasing tools provide equivalent 

expressions. 

Emotion classification concerns approaches to 

detect the underlying emotion of a text. Related 

work typically attempts this using classifiers. 

These classifiers are trained with features such as 

n-grams (Tokuhisa et al., 2008), word-level 

pointwise mutual information (PMI) values 

(Agrawal et al., 2012; Bullinaria et al., 2007; and 

Church et al., 1990) or a combination of word POS 

and sentence dependency relations (Ghazi et al., 

2012). The remained works of emotion classifica-

tion in above mentioned research to deal with emo-

tions aroused by events inspires us to relate events 

to emotion words in RESOLVE. In addition, in 

terms of emotion classification, RESOLVE classi-

fies texts into fine-grained classes where each 

emotion word can be viewed as a single class; in 

contrast, most emotion classification work focuses 

only on coarse-grained (6 to 10 classes) emotion 

labeling. It is a challenging work. 

Word suggestion involves guessing a possible 

replacement for a given word in a sentence, or 

finding word collocations. A representative re-

search task for word suggestion is the SemEval 

2007 English Lexical Substitution task: the prob-

lem is to find a word substitute for the designated 

word given a sentence. Zhao et al. (2007) first uses 

rules to find possible candidates from WordNet 

and verifies the sentence after substitution using 

Web search results; Dahl et al. (2007) utilizes a 

more traditional n-gram model but uses statistics 

from  web 5-grams. Although closely related to our 

work, this task is different in several ways. First, 

the word for which a substitute is required is al-

ready an appropriate word, as it appears in a sen-

tence from a well-written English corpus, the 

Internet Corpus of English3. However, the goal of 

our work is to determine whether a word selected 

by ESL learners is appropriate, and if necessary to 

                                                           
3 http://corpus.leeds.ac.uk/internet.html 

145



suggest appropriate alternatives. Observation of 

our corpus has shown that typically, the word se-

lected by ESL learners is not the most appropriate 

one. This is in contrast to the cited related works in 

which the original in-context wording is usually 

the most appropriate one. However, in our research 

the context often does not support the way the ESL 

learner’s word(s) are used. Second, the context of 

the given word in SemEval is a sentence, while in 

this work it is a document. Third, annotators for 

SemEval were limited to at most three possible 

substitutions, all of which were to be equally ap-

propriate, while in our work annotators are asked 

to assign ranks to all candidates (synonyms of the 

given word). Fourth, in SemEval the words to be 

substituted come from various syntactic or seman-

tic categories, while we only suggest appropriate 

emotion words to the learners. 

For writing assessment, existing works are 

known as automatic essay assessment (AEA) sys-

tems, which analyze user compositions in terms of 

wording, grammar and organization. PIGAI4, tar-

geted at generating suggested revisions, suggests 

unranked synonyms for words. However, unranked 

synonyms easily confuse Chinese learners (Ma, 

2013). E-rater (Leading et al. 2005), a writing 

evaluation system developed by the Educational 

Testing Service (ETS), offers a prompt-specific 

vocabulary usage score, a scoring feature which 

evaluates the word choice and compares words in 

the writing with samples in low- to high-quality 

writings. Ma shows that students’ scores on PIGAI 

increase after using PIGAI, and that these results 

are in proportion to the frequency they use PIGAI. 

As for E-rater, to our best knowledge, its focus is 

on helping judges to score writing rather than on 

assisting learners. In contrast, the purpose of 

RESOLVE is to directly assist language learners in 

finding appropriate wording, especially for emo-

tion words. 

As context and events are crucial to appropriate 

emotion wording, both have been taken into ac-

count in the development of RESOLVE. For con-

text, learner writings describing emotions have 

been utilized to extract contextual clues. For events, 

Jeong and Myaeng (2012) find that in the well-

annotated TimeBank corpus, 65% of the event 

conveyance was accomplished using verbs; thus 

we detect events from verb phrases. In contrast to 

                                                           
4 http://www.pigai.org 

paraphrasing and emotion analysis, the goal of 

RESOLVE is to distinguish the nuances among 

emotion synonyms in order to aid in language 

learning: this makes it a novel research problem. 

3 Method 

We postulate that patterns can describe emotion 

events, and that event conveyance is accomplished 

primarily using verbs (Jeong and Myaeng, 2012). 

In RESOLVE, verb-phrase patterns are selected for 

use in differentiating emotion word synonyms, that 

is, candidate emotion words, using their relation-

ships with these patterns. Figure 1 shows the two-

stage RESOLVE framework. First we learn corpus 

patterns (patterns extracted from the corpus) and 

their emotion event scores, EES, for all emotion 

words, and then, given the target emotion word, we 

rank the candidate emotion words to suggest ap-

propriate wording using the writing patterns (pat-

terns extracted from learner writing) and associated 

emotion event scores learned in the first stage. To 

determine the similarity between corpus patterns 

and writing patterns, we also propose a pattern 

matching algorithm which takes into account the 

cost of wildcard matching. Finally, to verify the 

effectiveness of RESOLVE in aiding precise word-

ing, a learning experiment is designed. In an ex-

ample RESOLVE scenario, the learner writes the 

following: “I love guava but one day I ate a rotten 

guava with a maggot inside, which made me dis-

gust.” She is not sure about the wording so she 

turns to RESOLVE for help. She is given a ranked 

word suggestion list: repugnance, disgust, repul-

sion, loathing and revulsion; which are more ap-

propriate than the list Theasurus.com provides: 

antipathy, dislike, distaste, hatred and loathing. 

146



 
Figure 1: RESOLVE framework. 

3.1 Stage One: Learning Corpus Patterns for 
All Emotion Words 

In this stage, we learn patterns and their relations 

to emotion words from the corpus. Sentences are 

first pre-processed, after which patterns are ex-

tracted from the corpus and their emotion event 

scores calculated.  

Pre-processing. As compound sentences can 

be associated with more than one emotion event, 

they must be pre-processed before we extract pat-

terns. Compound sentences are first broken into 

clauses according to the Stanford phrase structure 

tree output (Klein and Manning, 2003). In the ex-

periments, these clauses are treated as sentences.  

Pattern Extraction. Emotion events are char-

acterized by verb-phrase patterns, derived from the 

output of the Stanford dependency parser (De 

Marneffe et al., 2006). This parser generates the 

grammatical relations of word pairs and determines 

the ROOT, which is often the verb, after parsing 

each sentence. We describe the extraction steps 

given the sentence “We gave the poor girl a new 

book.”. A total of 746,919 patterns were extracted 

in this process. 

Step1: Identify the ROOT (gave) and all its de-

pendents based on the parsing result. 

Step2: Replace the words having no dependency 

relation to the ROOT with wildcards; consecutive 

wildcards were combined into one.  

(we gave * girl * book) 

Step3: Filter out less informative dependents (i.e., 
those nodes that are not the children of the ROOT 

in the dependency parse tree) by replacing with 

wildcards the dependents in the following relations 

to the ROOT: subj, partmod, comp, parataxis, 

advcl, aux, poss, det, cc, advmod and dep. (* gave 

* girl * book) 

Step4: Generalize the grammatical objects by re-

placing them with their top-level semantic class in 

WordNet. (* gave * <person> * <artifact>) 

Step5: Lemmatize the verbs using WordNet. 

(* give * <person> * <artifact>) 

Step6: Removing the starting and ending wild-

cards. (give * <person> * <artifact>) 

Emotion Event Score Calculation. Once the 

patterns are extracted, RESOLVE learns their emo-

tion event scores (EES) to quantize their relations 

to each emotion word. Here we discover an inter-

esting issue: the extracted pattern may summarize 

an emotion event, but it may also tell the emotion 

it bears directly with emotion words. To determine 

whether patterns containing emotion words have 

different characteristics and effects on performance, 

we term them self-containing patterns. Hence two 

pattern sets are used in experiments: one that in-

cludes all extracted patterns (Pall), and the other 

that excludes all self-containing patterns (P-scPattern). 

As shown in equation (1), we define the emo-

tion event score (EESp,e) of a pattern p for an emo-

tion word e by the conditional probability of e 

given p. 

 , |p eEES P e p  (1) 

3.2 Stage Two: Ranking Synonyms of the 
Emotion Word to Suggest Appropriate 

Wording 

In previous stage we built a pattern set for each 

emotion word. In this stage, there are four tasks: 

enumerate candidate emotion words for the target 

emotion word, extract writing patterns, match the 

writing patterns to the corpus patterns of the candi-

dates, and rank the candidates. To enumerate the 

candidate emotion words, RESOLVE first looks up 

synonyms of the target emotion word in Word-

NetSynsets and in Merriam Webster’s Dictionary. 

Pattern Matching. For each candidate ei, 

RESOLVE compares writing patterns  

Pwriting=(pw1,pw2,…pwN) with corpus patterns Pcorpus, 

and returns the matching corpus patterns 

Pmatch=(p1,p2,…pN) and their corresponding emo-

147



tion event scores; where N is number of clauses in 

the writing. Edit distance is utilized to calculate the 

similarity between a writing pattern and a corpus 

pattern, where the matching corpus pattern is de-

fined as that with the maximum pattern similarity 

to the writing pattern (in a one-to-one matching). 

The emotion scores of this matched corpus pattern 

for different emotion words will be used as the 

writing pattern scores. 

We propose a variation of edit distance which 

accepts wildcards (that is, edit-distance with wild-

cards, EDW) that allows for partial matching, in-

cluding similar patterns, and hence increases hit 

rates. Therefore, we add a wildcard replacement 

cost (WRC) to the edit cost (general edit cost, 

GEC) in the traditional definition of the edit dis-

tance. For this purpose, a two-dimensional vector 

(GEC, WRC) which considers two edit costs sepa-

rately is used to measure the EDW between pat-

terns S1 and S2. EDW is defined as 

   ,1 2 ,  I JE D GECDW S WS RC   (2) 
where S1={s1(1), s1(2), s1(3),...,s1(I)} and S2={s2(1), 

s2(2), s2(3),..., s2(J)} are tokens of the corpus pat-

tern S1 and the writing pattern S2; I and J are the 

lengths of S1 and S2; i and j are the indices of S1 

and S2; and DI, J is recursively calculated from 1 to 

I and 1 to J  using the edit distance formula. Note 

that D0,0 is (0,0). A wildcard may be replaced with 

one or more tokens, or vice versa. When calculat-

ing EDW, if there is a wildcard replacement, the 

replacement cost is added to the WRC; for other 

cases, the edit cost is added to the GEC. 

We here define the value of the WRC. The tra-

ditional edit-distance algorithm takes into account 

only single-token costs, whereas wildcards in our 

patterns may replace more than one token. Wild-

card insertion and deletion costs hence depend on 

the number of tokens a wildcard may replace. Af-

ter some experiments, we empirically choose e 

(Euler’s number) as the cost of wildcard insertion 

and deletion. Note that e is also very close to the 

mean of the number of words replaced by one 

wildcard (positively skewed distribution). Table 1 

shows the costs of all EDW operations.  

 

 

 

 

 

 

Operation Cost 

Wildcard Insertion (ø → *) e 

Wildcard Deletion (* →ø) e 

Wildcard Replacement (* →token) 1 

Wildcard Replacement (token → *) e 

Table 1: Edit distance costs for EDW operations. 

Empirically, if no exact pattern is found, to rep-

resent the pattern we seek a more general pattern 

rather than a specific one. A general pattern’s 

meaning includes the meaning of the original pat-

tern, but a specific pattern’s meaning is part of the 

original. For example, consider the pattern “eat * 

tomorrow morning quickly.” If unable to find an 

exactly matching pattern, it would be better to use 

“eat * tomorrow * quickly” rather than “eat break-

fast tomorrow morning quickly” to represent it. 

Hence “*→token” wildcard replacements (“*

→morning” in the example) should be assigned a 

lower cost than “token→*” wildcard replace-

ments (“breakfast→*”  in the example), as a 
wildcard token may represent several general to-

kens: “token→*” wildcard replacement (token

→*) is equivalent to inserting more than zero to-

kens and “*→token” wildcard replacements are 
equivalent to deleting more than zero tokens. 

Therefore, we define the cost of “*→ token” 

wildcard replacement as 1 and “token→*” wild-
card replacement as e.  

 

2 2

arg  max  

        arg  max -

corpus corpus

corpus corpus

match corpus wiriting
p P

p P

p similarity p p

GEC WRC





 

 

 (3) 

The Euler equation, equation (3), takes into ac-

count both GEC and WRC to calculate the simi-

larity of two patterns. The matching corpus pattern 

is that with the maximum similarity. 

Candidate Emotion Word Ranking. The 

scoring function for ranking candidates S 

={e1,e2,…,eI} depends on the conditional probabil-

ity of candidate ei given writing patterns and can-

didates as defined in equation (4), which equals 

equation (5), assuming the patterns in Pwriting are 

mutually independent.  

   1 2| , | , ,..., ,i writing i NP e P S P e pw pw pw S  (4) 

148



 

   

1 2

1

| , ,..., ,

| , |

i N

N

i n i

n

P e pw pw pw S

P e pw S P e S
 

  
 


 (5) 

The second term in equation (5), P(ei|S)
1-N, de-

notes the learner’s preference with respect to writ-

ing topics. As we have no learner corpus, we 

assume that there are no such preferences and thus 

that P(ei|S) is uniformly distributed among ei in S. 

As a result, when ranking ei, P(ei|S)
1-N can be omit-

ted.  In addition, for the scores of the writing pat-

terns we must use the scores of the matching 

corpus pattern found by the EDW algorithm for the 

corpus. Therefore, we rewrite the first term of 

equation (5) as follows. 

 

 

 

 

 
 

 

 
 

| ,

, , ,
| ,

, , ,

, ,
| ,

, ,

i n

n

n i n

i n

n n i n

n i

i n

n n i

P e pw S

P pw e S P p S
P e p S

P p e S P pw S

P pw e S
P e p S

P p e S

  

 







 
(6) 

P(ei|pn,S) in equation (6) can be calculated by 

EES, and the similarity value from equation (3) is 

utilized in equation (7) to estimate the first term. 

Equation (8), its logarithmic form, is the final scor-

ing function for ranking. 

 
   

, ,

| ,n n

i writing

similarity p pw

i n

n

scr e P S

e P e p S


 
 (7) 

  

, 2 2

,

ln , ,

ln n i

n i

i

i writing

p e

n n

n p e

e S

scr e P S

EES
GEC WRC

EES


  
  

    
  

  




 (8) 

Modified EES. After observing the corpus 

characteristics, we further modified EES by adding 

the weighting factors ICZe (the Inverse Corpus-

siZe-ratio for emotion word e, where the corpus 

size denotes the number of patterns) and CTPlp (the 

emotion Category Transition Penalty for pattern p, 

where l denotes the level of the emotion word hier-

archy, as explained later) in equation (9). ICZ in 

equation (10) normalizes the effect of the emotion 

word corpus size. When an emotion word appears 

more frequently, more example sentences are col-

lected, resulting in a larger corpus. This can lead to 

a suggestion bias toward commonly seen emotion 

words. 

 
3

, ,

1

' 1 lp e p e e p
l

EES EES ICZ CTP


     (9) 

  1logeICZ P e   (10) 
The other weighting factor, CTP, takes into ac-

count emotion word similarity. As mentioned, 

emotion words are derived from WordNet-Affect 

and then extended via WordNetSynset and Webster 

Synonyms; as shown in Figure 2, we build a three-

layered hierarchy of emotion words. Level 1 is the 

six major emotion categories in WordNet-Affect 

(anger, disgust, fear, joy, sadness, and surprise), 

level 2 is the 1,000 emotion words from WordNet-

Affect, and level 3 is the synonyms of the level-2 

emotion words. 

 

Figure 2: The emotion word hierarchy.  

  

  

2

2
1

,

,0 1l lcp p
level

P p

P c p

CTP CTP
m



  


 
(11) 

Intuitively, patterns that co-occur with many 

different emotion words are less informative. To 

assign less importance to these patterns, CTP esti-

mates how often a pattern transits among emotion 

categories and adjusts its score accordingly in 

equation (11), where m is the number of categories 

in each level; c is the emotion category. High-CTP 

patterns appear in more emotion categories or are 

evenly distributed among emotion categories and 

are hence less representative. Note that categories 

in lower levels (for instance level 1) are less simi-

lar, and transitions among these make patterns less 

powerful. 

4 Experiment 

4.1 Emotion Words and Corpus 

The corpora used in this study include WordNet-

Affect (Strapparava and Valitutti, 2004), Word-

NetSynset (Fellbaum, 1999), Merriam Webster 

Dictionary, and Vocabulary.com. The WordNet-

149



Affect emotion list contains 1,113 emotion terms 

categorized into six major emotion categories: an-

ger, disgust, fear, joy, sadness, and surprise (Ek-

man, 1993). 113 of the 1,113 terms were excluded 

because they were emotion phrases as opposed to 

words; thus a total of 1,000 emotion words were 

collected. Then, to increase coverage, synonyms of 

these 1,000 emotion words from WordNetSynset 

and Merriam Webster Dictionary were included. 

Thus we compiled a corpus with 3,785 emotion 

words. For each of these 3,785 emotion words 

there was an average of 13.1 suggested synonyms, 

with a maximum of 57 and a minimum of 1. 

Moreover, we extracted from Vocabulary.com a 

total of 2,786,528 example sentences, each con-

taining emotion words. The maximum number of 

example sentences for a given emotion word was 

1,255; the minimum was 3. 

4.2 Testing Data and Gold Standard 

A writing task was designed for the evaluation. To 

create the testing data, 240 emotion writings writ-

ten by ESL learners were collected. The partici-

pants were non-native English speakers (native 

Chinese speakers), all undergraduates or higher. 

Each writing was a short story about one of the six 

emotions defined by Ekman, and each had three 

requirements: (1) a length of at least 120 words; (2) 

a consistent emotion throughout the story; and (3) 

a sentence at the end that contains an emotion 

word (hereafter referred to as the target emotion 

word) summarizing the feeling of the writing. The 

target emotion word and its synonyms were taken 

as candidates of the most appropriate word (hereaf-

ter termed candidate emotion words). From these, 

RESOLVE proposes for each writing the five most 

appropriate words.  

To create the gold standard, two native-speaker 

judges ranked the appropriateness of the emotion 

word candidates for each target emotion word giv-

en the writing. The judges scored the candidates 

ranging from 0 (worst) to 6 (best) based on contex-

tual clues. When two or more words were consid-

ered equally appropriate, equal ranks were allowed, 

i.e., skips were allowed in the ranking. For exam-

ple, given the synonym list angry, furious, enraged, 

mad and annoyed, if the judge considered enraged 

and furious to be equally appropriate, followed by 

angry, mad and annoyed, then the ranking scores 

from highest to lowest would be enraged-6, furi-

ous-6, angry-4, mad-3 and annoyed-2, respectively. 

In addition, words not in the top five but that fit the 

context were assigned 1 whereas those that did not 

fit the context were assigned 0.  

In order to gauge the quality of judges' ranks, 

Cohen’s KAPPA value was utilized to measure the 

inter-judge agreement. KAPPA (k) is calculated by 

considering the ranking score to be either zero (0) 

or non-zero (1-6). In addition, a weighted KAPPA 

value for ranked evaluation (kw) was adopted (Sim 

and Wright, 2005) to quantify the agreement be-

tween the native scores. On average, k=0.51, and 

kw=0.68; both values indicate substantial inter-

judge agreement. 

5 Performance of RESOLVE 

In this section, we first evaluate the performance of 

RESOLVE from several aspects: (1) the perfor-

mance of EDW and modified EES, (2) a compari-

son of RESOLVE with commonly-adopted mutual 

information and machine learning algorithms for 

classification, and (3) a comparison of RESOLVE 

with tools for ESL learners. Then we utilize and 

compare the pattern sets Pall and P-scPattern (no self-

containing patterns) introduced in Section 4.1. We 

adopt NDCG@5 as the evaluation metric, which 

evaluates the performance when viewing this work 

as a word suggestion problem. 

5.1 EDW and Modified EES 

We evaluate the effect of the pattern-matching al-

gorithm EDW, EES modified by three layers of 

CTP weighting, and ICZ weighting. First we com-

pare EDW matching with wildcard matching. For 

the baseline, we use conventional wildcard match-

ing with neither ICZ nor CTP. The results in Table 

2 show that EDW outperforms the baseline wild-

card matching algorithm. In addition, using ICZ to 

account for the influence of the corpus size im-

proves performance. Level-1 CTP performs best. 

Thus for the remaining experiments we use EDW 

and EES modified by ICZ weighting and level-1 

CTP. 

 

 

 

 

 

 

 

 

150



 RESOLVE Components NDCG@5 

Baseline 0.5107 

EDW 0.5138 

EDW + level-1 CTP 0.5150 

EDW + level-1 CTP + ICZ 0.5529 

EDW + level-1, 2 CTP + ICZ 0.5104 

EDW + level-1, 2, 3 CTP + ICZ 0.5098 

Table 2: Performance with various components. 

5.2 Comparison to MI/ML Methods 

After demonstrating that the proposed EDW and 

modified EES for RESOLVE yield the best per-

formance, we compare RESOLVE to representa-

tive methods in related work to demonstrate its 

superiority. As mentioned in Section 2, related 

works view similar research problems as emotion 

classification problems or word suggestion prob-

lems. Commonly-adopted approaches for the for-

mer are based on mutual information (MI) and the 

latter on machine learning (ML). To represent 

these two types of approaches, we selected PMI 

and SVM, respectively, to which we compare the 

performance of RESOLVE. 

PMI, SVM and RESOLVE all used the same 

corpus. Note that NAVA words (noun, adjective, 

verb and adverb) are the major sentiment-bearing 

terms (Agrawal and An, 2012). Hence for compar-

ison with the feature set of extracted patterns we 

selected NAVA words as the additional feature set. 

For the PMI approach we calculated PMI values (1) 

between NAVA words and emotion words, (2)  

between patterns and emotion words. The PMI 

values between features from the writing and one 

emotion word candidate are then summed as the 

ranking score of the candidate. For the SVM ap-

proach, we used libsvm (Chang et al., 2011). We 

used a linear kernel to train for a classifier for each 

emotion by selecting all positive samples and an 

equal number of randomly-selected negative sam-

ples. We ran tests using various SVM parameter 

settings and found the performance differences to 

be within 1%. PMI, SVM and RESOLVE were all 

trained on the prepared three feature sets. SVM 

simply classifies each emotion word candidate as 

fitting the context or not. The confidence value of 

each answer is used for ranking. 

From Table 3, we found the best features for 

the PMI and SVM approaches are NAVA words. 

NDCG@5 (BD) shows the binary decision per-

formance when giving a score of 1 to all candi-

dates with ranking scores from 1 to 6, and 0 other-

wise. Note that it is possible that SVM when using 

NAVA words is too sparse to ensure satisfactory 

performance, as the number of corpus-extracted 

patterns exceeds one million; thus the result is not 

shown here, as this leads to excessive feature 

counts for SVM. Experimental results show that 

RESOLVE achieves the best performance; the sig-

nificance test shows that RESOLVE (pattern) sig-

nificantly outperforms PMI (NAVA) and SVM 

(NAVA) at tail p-values of less than 0.001. 

Feature  PMI SVM RESOLVE 

NAVA 

word 

NDCG@5 0.4275 0.5122 0.5048 

NDCG@5(BD) 0.4778 0.5229 0.5236 

Pattern 
NDCG@5 0.4126 N/A 0.5529 

NDCG@5(BD) 0.4530 N/A 0.5627 

Table 3: NDCG@5 for various feature sets. 

As to RESOLVE, recall that there are two con-

figurations for testing the effectiveness of self-

containing patterns: RESOLVE including self-

containing patterns (RESOLVE-Pall), and 

RESOLVE excluding self-containing patterns 

(RESOLVE-P-scPattern). Six different emotion cate-

gories are analyzed individually to reveal their dif-

ferent characteristics (De Choudhury et al., 2012). 

Table 4 shows the NDCG@5 averaged by the 

number of writings in six emotion categories for 

PMI (NAVA), SVM (NAVA), and RESOLVE-Pall 

and RESOLVE-P-scPattern. A further analysis of the 

writings shows that when expressing disgust or 

sadness, extensive uses of emotion words are 

found. Therefore, RESOLVE-Pall yields better per-

formance. The remaining four emotions are ex-

pressed through descriptions of events rather than 

using emotion words.  These results conform to the 

conclusion from (De Choudhury, Counts and 

Gamon, 2012): negative moods tend to be de-

scribed in limited context. Based on the finding in 

Table 4, RESOLVE-Pall is used for emotion writ-

ings about disgust and sadness, and  

RESOLVE-P-scPattern is used for writings about an-

ger, fear, joy and surprise when building the final 

conditional RESOLVE system. 

 

 

 

 

151



Emotion PMI 

(NAVA) 

SVM 

(NAVA) 

RESOLVE 

-Pall 

RESOLVE 

-P-scPattern 

Anger 0.3295 0.4706 0.4886 0.5071 

Disgust 0.3103 0.3738 0.3773 0.2584 

Fear 0.4064 0.5381 0.5168 0.6152 

Joy 0.4849 0.5764 0.4456 0.5708 

Sadness 0.2863 0.3495 0.3999 0.3194 

Surprise 0.7346 0.7651 0.8037 0.8400 
Table 4 NDCG@5 for six emotion categories. 

5.3 Comparison to Tools for ESL Learners 

In the final part of the system evaluation, we show 

the effectiveness of RESOLVE by evaluating the 

performance of the most commonly-used tools by 

ESL learners. One traditional and handy tool is the 

thesaurus. For this evaluation we selected Roget’s 

Thesaurus5. Another tool is online language learn-

ing systems, of which PIGAI is the most well-

known online rating system for writing for Chinese 

ESL learners. This system can also suggest to 

learners several easily-confused words as substi-

tutes for several system-selected words. For evalu-

ation, we posted the experimental writing to PIGAI 

to check whether there were any suggested substi-

tutes for the target emotion word. Replacement 

suggestions were found for the target emotion 

word in 71 out of 240 writings. Therefore, we 

compared the performance of PIGAI and 

RESOLVE on these 71 writings. Note that what 

the thesaurus and PIGAI suggested are both appro-

priate word sets, where words are listed in alpha-

betic order. Learners must select by themselves (or 

most conveniently, simply select the first one). Ta-

ble 5 shows that RESOLVE provides a better set of 

top-5 suggestions than both the thesaurus and 

PIGAI. 

Tool 
NDCG@5 

 

NDCG@5 

(BD) 

Precision@5 

(BD) 

PIGAI 

(71/240) 
0.3300 0.3095 0.8732 

RESOLVE 

(71/240) 
0.4755 0.4728 0.9789 

Thesaurus 0.3708 0.4237 0.9146 

RESOLVE 0.5529 0.5627 0.9479 
Table 5: Performance using ESL learner tools. 

                                                           
5 http://Thesaurus.com 

6 Conclusion 

We presented a probabilistic model that can sug-

gest emotion word based on the context.  The mod-

ified EES that considered the distribution of 

emotion word help our algorithm rank the candi-

date emotion words better. Besides, the matching 

algorithm, EDW, can find the most similar emo-

tional event from the writings. Furthermore, the 

example sentences can be used as our training cor-

pus without any handcraft annotations. The evalua-

tion shows that the proposed approach can more 

appropriately suggest emotion words than other 

models and reference tools like PIGAI and Thesau-

rus. 

Acknowledgements 
Research of this paper was partially supported by 

National Science Council, Taiwan, under the con-

tract MOST 101-2628-E-001-005-MY3. 

References 
Agrawal, A., and An, A. 2012. Unsupervised Emotion 

Detection from Text using Semantic and Syntactic 

Relations. In Web Intelligence and Intelligent Agent 

Technology (WI-IAT), 2012 IEEE/WIC/ACM Interna-

tional Conferences on (Vol. 1, pp. 346-353). IEEE. 

Altarriba, J., and Basnight-Brown, D. M. 2012. The 

acquisition of concrete, abstract, and emotion words 

in a second language. International Journal of Bilin-

gualism, 16(4), 446-452. 

Bannard, C., and Callison-Burch, C. 2005. Paraphrasing 

with bilingual parallel corpora. In Proceedings of the 

43rd Annual Meeting on Association for Computa-

tional Linguistics (pp. 597-604). Association for 

Computational Linguistics. 

Barreiro, A. 2011. SPIDER: A System for Paraphrasing 

in Document Editing and Revision—Applicability in 

Machine Translation Pre-editing. In Computational 

Linguistics and Intelligent Text Processing (pp. 365-

376). Springer Berlin Heidelberg. 

Barzilay, R., and Lee, L. 2003. Learning to paraphrase: 

an unsupervised approach using multiple-sequence 

alignment. In Proceedings of the 2003 Conference of 

the North American Chapter of the Association for 

Computational Linguistics on Human Language 

Technology-Volume 1 (pp. 16-23). Association for 

Computational Linguistics. 

Barzilay, R., and McKeown, K. R. 2001. Extracting 

paraphrases from a parallel corpus. In Proceedings of 

the 39th Annual Meeting on Association for Compu-

tational Linguistics (pp. 50-57). Association for 

Computational Linguistics. 

152



Bullinaria, J. A., and Levy, J. P. 2007. Extracting se-

mantic representations from word co-occurrence sta-

tistics: A computational study. Behavior research 

methods, 39(3), 510-526. 

Callison-Burch, C. 2008. Syntactic constraints on para-

phrases extracted from parallel corpora. 

In Proceedings of the Conference on Empirical 

Methods in Natural Language Processing (pp. 196-

205). Association for Computational Linguistics. 

Chang, C. C., and Lin, C. J. 2011. LIBSVM: a library 

for support vector machines. ACM Transactions on 

Intelligent Systems and Technology (TIST), 2 (3), 27. 

Chen, M. H., Huang, S. T., Chang, J. S., and Liou, H. C. 

2013. Developing a corpus-based paraphrase tool to 

improve EFL learners' writing skills. Computer As-

sisted Language Learning, (ahead-of-print), 1-19. 

Chen, M. H., Huang, S. T., Huang, C. C., Liou, H. C., 

and Chang, J. S. 2012. PREFER: using a graph-based 

approach to generate paraphrases for language learn-

ing. In Proceedings of the Seventh Workshop on 

Building Educational Applications Using NLP (pp. 

80-85). Association for Computational Linguistics. 

Church, K. W., and Hanks, P. 1990. Word association 

norms, mutual information, and lexicogra-

phy. Computational linguistics, 16(1),  

Dahl, G., Frassica, A. M., and Wicentowski, R. (2007, 

June). SW-AG: Local context matching for English 

lexical substitution. In Proceedings of the 4th Inter-

national Workshop on Semantic Evaluations (pp. 

304-307). Association for Computational Linguistics. 

22-29. 

De Choudhury, M., Counts, S., and Gamon, M. 2012. 

Not all moods are created equal! exploring human 

emotional states in social media. In Sixth Interna-

tional AAAI Conference on Weblogs and Social Me-

dia. 

De Marneffe, M. C., MacCartney, B., and Manning, C. 

D. 2006. Generating typed dependency parses from 

phrase structure parses. In Proceedings of LREC (Vol. 

6, pp. 449-454). 

Dinu, G., and Lapata, M. 2010. Topic models for mean-

ing similarity in context. In Proceedings of the 23rd 

International Conference on Computational Linguis-

tics: Posters (pp. 250-258). Association for Compu-

tational Linguistics. 

Ekman, P. 1993. Facial expression and emo-

tion. American Psychologist, 48 (4), 384. 

Fellbaum, C. 1999. WordNet. Blackwell Publishing Ltd. 

Ghazi, D., Inkpen, D., and Szpakowicz, S. 2012. Prior 

versus Contextual Emotion of a Word in a Sentence. 

In Proceedings of the 3rd Workshop in Computation-

al Approaches to Subjectivity and Sentiment Analy-

sis (pp. 70-78). Association for Computational 

Linguistic 

Jeong, Y., and Myaeng, S. H. 2012. Using Syntactic 

Dependencies and WordNet Classes for Noun Event 

Recognition. In The 2nd Workhop on Detection, Rep-

resentation, and Exploitation of Events in the Seman-

tic Web in Conjunction with the 11th International 

Semantic Web Conference (pp. 41-50). 

Keshtkar, F., and Inkpen, D. 2010. A corpus-based 

method for extracting paraphrases of emotion terms. 

In Proceedings of the NAACL HLT 2010 Workshop 

on Computational approaches to Analysis and Gen-

eration of emotion in Text (pp. 35-44). Association 

for Computational Linguistics. 

Klein, D., and Manning, C. D. 2003. Accurate unlexi-

calized parsing. In Proceedings of the 41st Annual 

Meeting on Association for Computational Linguis-

tics-Volume 1 (pp. 423-430). Association for Compu-

tational Linguistics. 

Leading, L. L., Monaghan, W., and Bridgeman, B. 2005. 

E-rater as a Quality Control on Human Scores. 

Lin, D. 1998. An information-theoretic definition of 

similarity. In ICML (Vol. 98, pp. 296-304). 

Lin, D., and Pantel, P. 2001. DIRT—discovery of infer-

ence rules from text. In Proceedings of the seventh 

ACM SIGKDD international conference on 

Knowledge discovery and data mining (pp. 323-328). 

ACM. 

Ma, K. (2013). Improving EFL Graduate Students' Pro-

ficiency in Writing through an Online Automated Es-

say Assessing System. English Language 

Teaching, 6(7). 

Pavlenko, A. 2008. Emotion and emotion-laden words 

in the bilingual lexicon. BILINGUALISM 

LANGUAGE AND COGNITION, 11(2), 147. 

Sim, J., and Wright, C. C. 2005. The kappa statistic in 

reliability studies: use, interpretation, and sample size 

requirements. Physical therapy, 85(3), 257-268. 

Strapparava, C., and Valitutti, A. 2004. WordNet Affect: 

an Affective Extension of WordNet. In LREC (Vol. 4, 

pp. 1083-1086). 

Tokuhisa, R., Inui, K., and Matsumoto, Y. 2008. Emo-

tion classification using massive examples extracted 

from the web. In Proceedings of the 22nd Interna-

tional Conference on Computational Linguistics-

Volume 1 (pp. 881-888). Association for Computa-

tional Linguistics. 

Zhao, S., Zhao, L., Zhang, Y., Liu, T., and Li, S. (2007, 

June). Hit: Web based scoring method for english 

lexical substitution. In Proceedings of the 4th Inter-

national Workshop on Semantic Evaluations (pp. 

173-176). Association for Computational Linguistics. 

153


