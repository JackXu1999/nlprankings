










































Robust Transliteration Mining from Comparable Corpora with Bilingual Topic Models


International Joint Conference on Natural Language Processing, pages 261–269,
Nagoya, Japan, 14-18 October 2013.

Robust Transliteration Mining from Comparable Corpora with
Bilingual Topic Models

John Richardson† Toshiaki Nakazawa‡ Sadao Kurohashi†
†Graduate School of Informatics, Kyoto University, Kyoto 606-8501

‡Japan Science and Technology Agency, Kawaguchi-shi, Saitama 332-0012
john@nlp.ist.i.kyoto-u.ac.jp, nakazawa@pa.jst.jp, kuro@i.kyoto-u.ac.jp

Abstract

We present a high-precision, language-
independent transliteration framework
applicable to bilingual lexicon extrac-
tion. Our approach is to employ a
bilingual topic model to enhance the
output of a state-of-the-art grapheme-
based transliteration baseline. We
demonstrate that this method is able
to extract a high-quality bilingual lexi-
con from a comparable corpus, and we
extend the topic model to propose a so-
lution to the out-of-domain problem.

1 Introduction
A large, high-quality bilingual lexicon is of
great utility to any dictionary-based system
that processes bilingual data. The ability to
automatically generate such a lexicon with-
out relying on expensive training data or pre-
existing lexical resources allows us to find
translations for rare and unknown words with
high efficiency.

Transliteration1 is particularly important as
new words are often created by importing
words from other languages, especially En-
glish. It would be an almost impossible task
to create and maintain a dictionary of such
words by hand, as new words appear rapidly,
especially in online texts, and word usage can
vary over time.

In this paper we construct a language-
independent transliteration framework. Our
model builds on previous transliteration work,
improving extraction and generation precision
by including semantic as well as purely lexical
features. The proposed model can be trained

1This paper considers both ‘transliteration’ (EN–
XX) and ‘back-transliteration’ (XX–EN). For simplic-
ity we refer to both tasks as ‘transliteration’.

on comparable corpora, thereby not relying on
expensive or often unavailable parallel data.

The motivation behind the approach of com-
bining lexical and semantic features is that
these two components are largely independent,
greatly improving the effectiveness of their
combination. This is particularly important
for word-sense disambiguation. For example,
a purely lexical approach is not sufficient to
transliterate the Japanese ソース (soosu), as it
can mean either ‘sauce’ or ‘source’ depending
on the context.

2 Previous Work
Previous work has considered various methods
for transliteration, ranging from simple edit
distance and noisy-channel models (Brill et al.,
2001) to conditional random fields (Ganesh et
al., 2008) and finite state automata (Noeman
and Madkour, 2010). We construct a base-
line by modelling transliteration as a Phrase-
Based Statistical Machine Translation (PB-
SMT) task, a popular and well-studied ap-
proach (Matthews, 2007; Hong et al., 2009;
Antony et al., 2010).

The vast majority of previous work on
transliteration has considered only lexical fea-
tures, for example spelling similarity and
transliteration symbol mapping, however we
build on the inspiration of Li et al. (2007)
and later Hagiwara and Sekine (2012), who in-
troduced semantic features to a transliteration
model.

Li et al. (2007) proposed the concept of
‘semantic transliteration’, which is the con-
sideration of inherent semantic information in
transliterations. Their example is the influ-
ence of the source language and gender of for-
eign names on their transliterations into Chi-
nese. Hagiwara and Sekine (2012) expanded
upon this idea by considering a ‘latent class’

261



transliteration model considering translitera-
tions to be grouped into categories, such as
language of origin, which can give additional
information about their formation. For exam-
ple, if we know that a transliteration is of Ital-
ian origin, we are more likely to recover the
letter sequence ‘gli’ than if it were originally
French.

While these methods consider limited se-
mantic features, they do not make use of
the rich contextual information available from
comparable corpora. We show such contextual
information, in the form of bilingual topic dis-
tributions, to be highly effective in generating
transliterations.

Bilingual lexicon mining from non-parallel
data has been tackled in recent research such
as Tamura et al. (2012) and Haghighi et al.
(2008), and we build upon the techniques of
multilingual topic extraction from Wikipedia
pioneered by Ni et al. (2009). Previous re-
search in lexicon mining has tended to focus
on semantic features, such as context similar-
ity vectors and topic models, but these have
yet to be applied to the task of transliteration
mining. We use the word-topic distribution
similarities explored in Vulić et al. (2011) as
baseline word similarity measures.

In some cases it is possible to use mono-
lingual corpora for transliteration mining, as
English is often written alongside translitera-
tions (Kaji et al., 2011), however we consider
the more general setting where such informa-
tion is unavailable.

3 Baseline Transliteration Model

We begin by constructing a baseline transliter-
ation system trained only on lexical features.
This baseline system will allow us to compare
directly the effectiveness of the addition of a
semantic model to a traditional transliteration
framework.

Our baseline model is a grapheme-based
machine transliteration system. We model
transliteration as a machine translation task
on a character rather than word level, treat-
ing character groups as phrases. The model is
trained by learning phrase alignments such as
that shown in Figure 1. The field of phrase-
based SMT has been well studied and there ex-
ists a standard toolset enabling the construc-

コ　ン　ピ　ュ　ー　タ　ー
ko       n         p       yu       u         ta       a

 co 
    

m            p  u             t  e  r

Figure 1: Example of Japanese–English
transliteration phrase alignment.

tion of an easily reproducable baseline system.
We use the default configuration of Moses

(Koehn et al., 2007) to train our baseline sys-
tem, with the distortion limit set to 1 (as
transliteration requires monotonic alignment).
Character alignment is performed by GIZA++
(Och and Ney, 2003) with the ‘grow-diag-final’
heuristic for training. We apply standard tun-
ing with MERT (Och, 2003) on the BLEU
(Papineni et al., 2001) score. The language
model is built with SRILM (Stolcke, 2002) us-
ing Kneser-Ney smoothing (Kneser and Ney,
1995).

The system described above has been im-
plemented as specified in previous work such
as Matthews (2007) (Chinese and Arabic),
Hong et al. (2009) (Korean), and Antony et
al. (2010) (Kannada). We demonstrate that
this standard, highly-regarded baseline can be
greatly improved with our proposed method.

4 Semantic Model

Having set up the baseline system, we turn to
the task of combining a semantic model with
our transliteration engine. We employ the
method of bilingual LDA (Mimno et al., 2009),
an extension of monolingual Latent Dirichlet
Allocation (LDA) (Blei et al., 2003) as the se-
mantic model.

Monolingual LDA takes as its input a set of
monolingual documents and generates a word-
topic distribution ϕ classifying words appear-
ing in these documents into semantically sim-
ilar topics. Bilingual LDA extends this by
considering pairs of comparable documents in
each of two languages, and outputs a pair of
word-topic distributions ϕ and ψ, one for each
input language. The graphical model for bilin-
gual LDA is illustrated in Figure 2.

262



Figure 2: Graphical model for Bilingual LDA
with K topics, D document pairs and hyper-
parameters α and β. Topics for each document
are sampled from the common distribution θ,
and the two languages have word-topic distri-
butions ϕ and ψ.

4.1 Motivation for Bilingual LDA
We choose to employ a bilingual topic model
to measure semantic similarity (i.e. topic sim-
ilarity) of word pairs rather than the more in-
tuitive method of comparing monolingual con-
text similarity vectors (Rapp, 1995) for rea-
sons of robustness and scalability.

Measuring context similarity on a word level
requires a bilingual lexicon to match cross-
language word pairs and such bilingual data
is often expensive or unavailable. There are
also problems with directly comparing collo-
cations and word concurrence of distant lan-
guage pairs as they do not always correspond
predictably. Therefore our proposed method
provides a more robust approach using coarser
semantic features.

The use of topic models as a semantic sim-
ilarity measure is a scalable method because
document-aligned bilingual training data is
growing ever more widely available. Exam-
ples of such sources are Wikipedia, multilin-
gual newspaper articles and mined Web data.

4.2 Semantic Similarity Measures
In order to apply bilingual topic models to a
transliteration task, we must construct an ef-
fective word similarity measure for source and
target transliteration candidates. We improve
upon three natural similarity measures, Cos,
Cue and KL, based on those considered in
Vulić et al. (2011), by proposing two methods
of feature combination: reordering and SVM

combination.
The reranking method considers hybrid

scores Base+Cos, Base+Cue and Base+KL.
These are generated by reranking the top-
10 baseline (Base) transliteration candidates
by their respective semantic scores (Cos, Cue
or KL). We used 10 candidates for filtering
as we found this gave the best balance be-
tween volume and accuracy in preliminary ex-
periments. Approximately 75–85% of correct
transliterations (depending on language pair)
were within the top-10 candidates and this is
therefore an upper bound for the hybrid model
accuracy. As a comparison, the top-100 can-
didates contained roughly 80–85% of correct
transliterations, the remainder failing to be
identified by the baseline.

We additionally consider the combination
of all three semantic features with the base-
line (Moses) transliteration scores using a Sup-
port Vector Machine (SVM) (Vapnik, 1995).
The SVM is used to classify candidate pairs as
‘transliteration’ (positive) or ‘not translitera-
tion’ (negative), and we rerank the candidates
by SVM predicted values. The features used
for SVM training are baseline, Cos, Cue and
KL scores.

The similarity measures Cos, Cue and KL
are defined below.

4.2.1 Cos Similarity
The Cos method calculates the cosine simi-
larity of the topic distribution vectors ψk,we
and ϕk,wf for transliteration pair candidates
we and wf .

Cos(we, wf ) =

∑K
k=1 ψk,weϕk,wf√∑K

k=1 ψ
2
k,we

√∑K
k=1 ϕ

2
k,wf

(1)

4.2.2 Cue Similarity
The Cue method expresses the mean of the
two probabilities P (we | wf ) of a translitera-
tion we given some source language string wf
and P (wf | we) of the reverse. We define:

P (we | wf ) =
K∑
k=1

ψk,we
ϕk,wf
Normϕ

and likewise for P (we | wf ), with the

263



normalization factors given by Normϕ =∑K
k=1 ϕk,wf and Normψ =

∑K
k=1 ψk,we .

Finally, we consider:

Cue(we, wf ) =
1

2
(P (we | wf ) + P (wf | we))

(2)

4.2.3 KL Similarity
The KL method considers the averaged
Kullback-Leibler divergence:

KL(we, wf ) =
1

2
(KLe,f +KLf,e) (3)

KLe,f =
K∑
k=1

ϕk,we
Normϕ

log ϕk,we/Normϕ
ψk,wf /Normψ

KLf,e =

K∑
k=1

ψk,wf
Normψ

log
ψk,wf /Normψ

ϕk,we/Normϕ

using the same normalization factors as for
Cue similarity.

5 Experiments

In order to demonstrate the effectiveness of
our proposed model, we constructed an eval-
uation framework for a transliteration ex-
traction task. The language pairs English–
Japanese (EN–JA), Japanese–English (JA–
EN), English–Korean (EN–KO) and Korean–
English (KO–EN) were chosen to verify that
this method is effective for a variety of lan-
guages and in both transliteration directions.
Indeed, the methods introduced in this pa-
per could also be applied directly to other
languages with many transliterations, such as
Chinese, Arabic and Hindi.

While it is possible to make language-
specific optimizations, we decided only to pre-
process the data minimally (such as remov-
ing punctuation) in order to demonstrate that
our model works effectively in a language-
independent setting. Examples of language-
specific preprocessing techniques that we did
not perform include segmentation of Japanese
compound nouns (Nakazawa et al., 2005) and
splitting of Korean syllabic blocks (eumjeols)
into smaller components (jamo) (Hong et al.,
2009).

Language Pairs Train Tune Test
EN–JA/JA–EN 59K 1K 1K

KO–EN/EN–KO 21K 1K 1K

Table 1: Number of aligned word pairs in each
fold of data.

5.1 Data Set

We chose to build our data set from Wikipedia
articles, as they provide document-aligned
comparable data across a variety of languages.
Figure 3 shows how the Wikipedia data was
split.

5.1.1 Baseline Training Data
We trained our baseline system on aligned
Wikipedia page titles. This data consisted of
pairs of English and Japanese/Korean words
extracted from the freely available Wikipedia
XML dumps. The aligned titles were fil-
tered with hand-written rules2 to extract only
transliteration pairs, and the test data was
verified for correctness by hand. This data will
be made available to encourage comparison for
future transliteration research3.

The composition of this data is shown in
Table 1. Aligned word pairs were shuffled ran-
domly before splitting into the three folds to
ensure an even topic distribution across each
of ‘Train’, ‘Tune’ and ‘Test’.

5.1.2 Bilingual Topic Model
The bilingual topic model was trained on the
body text of Wikipedia articles aligned with
Wikipedia inter-language links. These corre-
spond to articles covering the same content,
however they are rarely of similar length and
not necessarily close transliterations.

We first pre-processed the most recent
Wikipedia XML dumps to remove all tags
and data other than plain text sentences, then
aligned articles with language links to generate
comparable document pairs. Words occurring
fewer than 10 or more than 100K times were
also removed to reduce noise and computation
time.

2Heuristic rules included extraction of Japanese
katakana, a script used primarily for transliterations,
and words aligned with proper nouns as defined in a
name dictionary.

3http://orchid.kuee.kyoto-u.ac.jp/~john

264



과일 (fruit), 열대 (tropical), 나무 (tree), ...바나나 (banana)

banana

Title pairs                                  Document pairs

이집트 (Egypt)

Egypt

.

.

.

yellow,  fruit,  cake, ...

나라 (country), 피라미드 (pyramid), ...

Nile,  economy,  Arabic, ...

.

.

.

Train
(baseline)

Tune
(baseline & SVM)

Test
(all models)

Train 
(topic model)

Figure 3: We extracted aligned title pairs (only transliterations) and aligned document pairs
from Wikipedia using inter-language links. The baseline was trained and tuned on title pairs
(‘Train’ and ‘Tune’), the topic model was trained on document pairs and the SVM was trained
on the title pairs ‘Tune’ fold.

5.1.3 SVM Hybrid Model
The training data for the proposed SVM hy-
brid model was built from the same data
used for the baseline (tuning fold). We first
generated the top-10 distinct transliteration
candidates for the tuning data using the ‘n-
best-list’ option in Moses. These candidates
were then labeled as ‘transliteration’ or ‘not-
transliteration’ and feature scores (Base, Cos,
Cue, KL) were generated for each candidate.
The SVM model was trained using these labels
and feature scores.

5.2 LDA Implementation Details
PolyLDA++, our implementation of multilin-
gual LDA, was based on GibbsLDA++ (Phan
et al., 2007), a toolkit for monolingual LDA.
This software is available for free4.

Each topic model was trained over 1000 iter-
ations, and the standard Dirichlet prior hyper-
parameters for the LDA model were set as
α = 50/K for K topics and β = 0.1.

The choice of number of topics is important,
as demonstrated in Figure 4, which shows the
top-1 accuracy of the SVM hybrid model us-
ing various numbers of topics K. The optimal
value of K seems to be between around 100
for this data.

The model accuracy gradually decreases
with adding more than 100 topics. We be-
lieve that this is because the granularity of

4http://orchid.kuee.kyoto-u.ac.jp/~john

Figure 4: Top-1 accuracy of SVM for various
K.

the topics becomes too fine to accommodate
for the wide differences in semantic usage of
English and Japanese/Korean transliteration
pairs. A higher number of topics could be
more suitable for more closely related language
pairs, such as Italian and English (Vulić et al.,
2011), because the higher similarity of word
usage would allow for topics of more limited
semantic scope. Such experiments are to be
considered in future work. The results below
are for K = 100.

5.3 Results

Table 2 compares the top-1 accuracy of our
proposed hybrid models to the baseline perfor-

265



JA–EN EN–JA KO–EN EN–KO
Base 0.334 0.363 0.296 0.421

Base+Cos 0.608 0.559 0.494 0.516
Base+Cue 0.608 0.551 0.507 0.504
Base+KL 0.365 0.398 0.261 0.373

SVM 0.610 0.572 0.504 0.551

Table 2: Top-1 accuracy of proposed model for each hybrid scoring method.

Figure 5: Precision-recall curve for SVM hy-
brid model.

mance. The SVM hybrid model outperformed
the baseline for every language pair, by as
much as 0.276 for JA–EN. This suggests that
the addition of a bilingual topic model signif-
icantly improves transliteration accuracy.

In general the SVM was the most effec-
tive hybrid score, outperforming Base+Cos,
Base+Cue and Base+KL in all but KO–EN,
where it performed very slightly worse than
Base+Cue.

Figure 5 shows the precision-recall curve
for the SVM hybrid model over the test set.
We vary recall by ranking the hybrid model
scores for all test pairs and selecting only
the highest scoring fraction to evaluate. This
simulates a lexicon extraction task where we
wish to sacrifice recall for precision. The re-
sults demonstrate that it is possible to im-
prove significantly the precision of a set of ex-
tracted transliterations by reducing the recall.
This large improvement is made possible be-
cause the topic similarity scores are particu-
larly effective at measuring confidence in each
transliteration candidate, allowing effective se-
lection of the correct transliterations.

5.4 Comparison with Previous Work
The results compare favorably to the top-1 ac-
curacy of similar existing systems, such as Di-
recTL+ (Jiampojamarn et al., 2010), which
also used Wikipedia titles (EN–JA 0.398), and
Hagiwara and Sekine (2012) (EN–JA 0.349).

Our baseline transliteration system can be
measured against previous work using Moses
and GIZA++ alignment, such as Matthews
(2007) (EN–AR 0.43, AR–EN 0.39, EN–ZH
0.38, ZH–EN 0.35) and Hong et al. (2009)
(EN–KO 0.45). These scores are consistent
with our baseline results.

While it is difficult to compare directly the
accuracy of transliteration systems across dif-
ferent languages and data sets, especially since
we use additional data to train the semantic
model, the results above show that our model
has made a considerable improvement over the
state-of-the-art baseline.

6 Extension to Out-of-Domain
Words

The model described in this paper revolves
around the use of a bilingual topic model to
improve transliteration quality. What hap-
pens then when a source word is not covered
by the topic model? This is a very important
problem in a practical setting, and we show
that even in such cases our model can improve
considerably upon the baseline system. We de-
fine ‘out-of-domain’ words as source language
words that did not appear in the topic model
training data and hence do not have a known
topic distribution.

6.1 Model Details
Our proposed approach is to consider not the
word-topic distribution of the source word we
itself, but rather that of the words in the sur-
rounding context. We consider two methods
for calculating the modified topic similarity

266



scores over the set of words We in the same
context as the source word.

Let S(we, wf ) be a basic topic similar-
ity score Cos, Cue or KL, then we define
the extended scores ExtMean(We, wf ) and
ExtWeight(We, wf ) as follows:

ExtMean(We, wf ) =

∑
we∈We S(we, wf )

|We|
(4)

ExtWeight(We, wf ) =

∑
we∈We c

′
weS(we, wf )∑

we∈We c
′
we

(5)
where c′we = (log cwe)

−1 for the frequency
cwe of we appearing in the semantic model
training data.

ExtMean corresponds to the mean topic
similarity for each word in the context We.
ExtWeight is weighted by the inverse log fre-
quency of each word, allowing consideration
of their semantic importance. These extended
scores are used to train the SVM in place of
the original scores.

6.2 Out-of-Domain Experiment
We performed an additional experiment where
we transliterated a set of 25 Japanese words
unknown to the topic model into English.
These words appeared in Wikipedia fewer than
10 times and therefore were not included in
our training data. We extracted the sen-
tences and documents in which these words oc-
curred, and back-transliterated the Japanese
words into English by hand. We considered
both sentence-level and document-level con-
texts for We, and evaluated each extended
metric ExtMean and ExtWeight.

The results of the out-of-domain experiment
are shown in Table 3, which gives the top-1
accuracy of the SVM hybrid model trained on
the ExtMean and ExtWeight counterparts of
Cos, Cue and KL similarities. Base is the top-
1 accuracy using only the Moses baseline.

The most effective settings were to use Ex-
tWeight on a sentence level context. There is
a balance between size and relevance of con-
text, with document-level context containing
too many misleading words. The improvement
of ExtWeight over ExtMean shows the impor-

Base ExtMean ExtWeight
Document 0.27 0.44 0.48Sentence 0.48 0.52

Table 3: Top-1 accuracy for out-of-domain
model extension (JA–EN).

tance of weighting contextual words based on
their importance (i.e. inverse log frequency).

The results show a large improvement
(+0.25) over the baseline scores that is compa-
rable to that of the in-domain model (+0.28,
see Table 2). This suggests that the proposed
model is an effective solution to the out-of-
domain problem.

7 Discussion and Error Analysis

An example of the top candidates for a suc-
cessful and an incorrect transliteration are
given in Tables 4 and 5 respectively. We can
see that the topic model has succeeded in find-
ing the correct transliteration of ‘batik’, a tra-
ditional Javanese fabric, however a low score
was given to the Korean transliteration of the
name ‘Bernard’ appearing in the training data.

The benefits of the addition of a topic model
is made clear with the example of ‘batik’ in Ta-
ble 4. The semantic similarity measures give
a higher score to ‘batik’ than ‘Batic’, a Slavic
surname, despite ‘Batic’ being the more likely
transliteration according to the baseline.

The improvement over the baseline for back-
transliteration (XX–EN), on average +0.24,
was considerably greater than that for translit-
eration (EN–XX), on average +0.17. We be-
lieve that this is due to the vast range of
transliteration spelling variations in the non-
English target languages. Since there is only
one correct spelling variation defined in our
test data and the topic distributions for each
spelling variation are very similar, it is not pos-
sible to guess correctly. For an example of this
problem, see Table 5.

7.1 Topic Alignment Difficulties
The majority of transliteration errors were
caused by unsuccessful topic alignment be-
tween the source and target words. This was
partly caused by the differences in usage of
the original English words and the transliter-
ated Japanese or Korean. For example, the

267



Candidate Baseline Cos Cue KL SVM
batik -1.29 0.989 2.54e-04 -0.327 1.10
baetic -1.32 0.0764 1.67e-06 -1.65 -1.39
batic -0.708 0.00 0.0 0.0 -1.48
batick -0.788 0.00 0.0 0.0 -1.53
butic -1.09 0.00 0.0 0.0 -1.68

Table 4: A good transliteration – バティック (batikku / ‘batik’) → batik.

Candidate Baseline Cos Cue KL SVM
베르나르 bereunareu -2.96 0.642 4.78e-04 -1.72 0.112

베르나르드 bereunareudeu -3.65 0.243 3.84e-05 -2.41 -0.909
베른하르트 bereunhareuteu -3.58 0.188 7.64e-05 -1.81 -0.969
베르나르트 bereunareuteu -4.24 0.217 8.24e-05 -2.69 -1.02

버나드 beonadeu -2.78 0.123 4.33e-05 -3.01 -1.23

Table 5: An incorrect transliteration – bernard → 베르나르트 (bereunareuteu).

Japanese バイキング (baikingu) is a transliter-
ation of ‘Viking’, however it is almost always
used to mean ‘buffet’, deriving from the Scan-
dinavian smorgasbord. In this case, we can ex-
pect the Japanese to be associated with food-
related topics, quite different from ‘Viking’.

There are also many cases where words that
do not clearly fit into one topic have unclear
distributions across many groups. For ex-
ample, the word 로마 (roma / ‘Rome’) could
be more strongly categorized with ‘cities’ and
‘sightseeing’ in English but ‘history’ and ‘clas-
sical civilization’ in Korean, giving a low over-
all topic correlation.

7.2 Effect of Word Length and
Frequency

We found that our model was more success-
ful at finding the correct transliteration of
longer words, as smaller words tend to have
more spelling variations and are orthographi-
cally more similar to other words. By remov-
ing words of length 5 characters or less from
the test data, we were able to improve the top-
1 accuracy (SVM) to 0.593 (KO–EN, +0.089)
and 0.721 (JA–EN, +0.111). In a practical
lexicon extraction task over the entirety of
Wikipedia this would cover roughly 35–45%
of words (depending on language).

There was almost no variation in transliter-
ation accuracy based on word frequency. The
baseline is relatively unaffected by word fre-
quency, with the exception of finding very rare
character phrases not in the training data, and

the topic model proved to be robust across
words of both high and low frequency.

8 Conclusion and Future Work
In this paper we demonstrated that the ad-
dition of semantic features can significantly
improve transliteration accuracy. Specifically,
it is possible to outperform the top-1 accu-
racy of a state-of-the-art phrase-based SMT
transliteration baseline through the addition
of a bilingual topic model.

Furthermore, our extended model is able to
produce a considerable improvement in accu-
racy even for out-of-domain source words that
have an unknown topic distribution. The ex-
perimental data set was constructed to simu-
late the task of extracting unknown word pairs
from a comparable corpus, however our exten-
sion model results suggest that it will be pos-
sible to extract high-quality transliterations
from larger and less comparable corpora than
ever before.

In the future we would like to explore in
depth the improvements to machine transla-
tion made possible by this approach.

Acknowledgements
We would like to thank the anonymous review-
ers for their feedback. The first author is sup-
ported by a Japanese Government (MEXT)
research scholarship.

268



References
P.J. Antony, V.P. Ajith, and K.P. Soman 2010.

Statistical Method for English to Kannada
Transliteration. BAIP 2010, CCIS 70, pp. 356–
362.

David Blei, Andrew Ng and Michael Jordan. 2003.
Latent Dirichlet Allocation. In The Journal of
Machine Learning Research, Volume 3.

Eric Brill, Gary Kacmarcik and Chris Brock-
ett. 2001. Automatically Harvesting Katakana-
English Term Pairs from Search Engine Query
Logs In Proceedings of the Sixth Natural Lan-
guage Processing Pacific Rim Symposium, 2001.

Surya Ganesh, Sree Harsha, Prasad Pingali, Va-
sudeva Varma. 2008. Statistical Transliteration
for Cross Language Information Retrieval using
HMM alignment model and CRF. In 2nd Inter-
national Workshop on Cross Language Informa-
tion Access, IJCNLP 2008.

Aria Haghighi, Percy Liang, Taylor Berg-
Kirkpatrick and Dan Klein. 2008. Learning
Bilingual Lexicons from Monolingual Corpora.
In ACL 2008.

Masato Hagiwara and Satoshi Sekine. 2011. La-
tent Class Transliteration based on Source Lan-
guage Origin. In ACL 2011.

Masato Hagiwara and Satoshi Sekine. 2012. La-
tent Semantic Transliteration using Dirichlet
Mixture. In ACL 2012.

Gumwon Hong, Min-Jeong Kim, Do-Gil Lee and
Hae-Chang Rim. 2009. A Hybrid Approach
to English-Korean Name Transliteration. In
Proceedings of 2009 Named Entities Workshop,
ACL-IJCNLP.

Sittichai Jiampojamarn, Kenneth Dwyer, Shane
Bergsma, Aditya Bhargava, Qing Dou, Mi-
Young Kim and Grzegorz Kondrak. 2010.
Transliteration Generation and Mining with
Limited Training Resources. In Proceedings of
the 2010 Named Entities Workshop, ACL 2010.

Nobuhiro Kaji and Masaru Kitsuregawa. 2011.
Splitting Noun Compounds via Monolingual and
Bilingual Paraphrasing: A Study on Japanese
Katakana Words. In EMNLP 2011.

Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language model-
ing. In Proceedings of the IEEE International
Conference on Acoustics, Speech, and Signal
Processing, Volume 1.

Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin and Evan Herbst. 2007.

Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In ACL 2007.

Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, Minghui
Dong. 2007. Semantic Transliteration of Per-
sonal Names. In ACL 2007.

David Matthews. 2007. Machine Transliteration
of Proper Names. Masters Thesis, School of In-
formatics, University of Edinburgh.

David Mimno, Hanna Wallach, Jason Naradowsky,
David Smith and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP 2009.

Toshiaki Nakazawa, Daisuke Kawahara and Sadao
Kurohashi. 2005. Automatic Acquisition of Ba-
sic Katakana Lexicon from a Given Corpus. In
IJCNLP 2005.

Xiaochuan Ni, Jian-Tao Sun, Jian Hu, Zheng
Chen. 2009. Mining Multilingual Topics from
Wikipedia. In WWW 2009.

Sara Noeman and Amgad Madkour. 2010. Lan-
guage independent transliteration mining sys-
tem using finite state automata framework. In
Proceedings of the 2010 Named Entities Work-
shop.

Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In ACL 2003.

Franz Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Align-
ment Models. In Computational Linguistics
2003.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: A method for auto-
matic evaluation of Machine Translation. Tech-
nical Report RC22176, IBM.

Xuan-Hieu Phan and Cam-Tu Nguyen. 2007.
GibbsLDA++: A C/C++ implementation of la-
tent Dirichlet allocation (LDA).

Reinhard Rapp. 1995. Identifying Word Transla-
tions in Non-Parallel Texts. In ACL 1995.

Andreas Stolcke. 2002. SRILM – An Extensible
Language Modeling Toolkit. In Proceedings of
ICSLP, Volume 2.

Akihiro Tamura, Taro Watanabe and Eiichiro
Sumita. 2012. Bilingual Lexicon Extraction
from Comparable Corpora Using Label Propa-
gation. In EMNLP-CoNLL 2012.

Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, New York.

Ivan Vulić, Wim De Smet and Marie-Francine
Moens. 2011. Identifying Word Translations
from Comparable Corpora Using Latent Topic
Models. In ACL 2011.

269


