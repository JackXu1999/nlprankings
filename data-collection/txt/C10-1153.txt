Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1362–1370,

Beijing, August 2010

1362

A Minimum Error Weighting Combination Strategy for Chinese

Semantic Role Labeling

Tao Zhuang and Chengqing Zong

National Laboratory of Pattern Recognition

Institute of Automation, Chinese Academy of Sciences

{tzhuang, cqzong}@nlpr.ia.ac.cn

Abstract

Many Semantic Role Labeling (SRL)
combination strategies have been pro-
posed and tested on English SRL task.
But little is known about how much Chi-
nese SRL can beneﬁt from system combi-
nation. And existing combination strate-
gies trust each individual system’s output
with the same conﬁdence when merging
them into a pool of candidates. In our ap-
proach, we assign different weights to dif-
ferent system outputs, and add a weighted
merging stage to the conventional SRL
combination architecture. We also pro-
pose a method to obtain an appropriate
weight for each system’s output by min-
imizing some error function on the devel-
opment set. We have evaluated our strat-
egy on Chinese Proposition Bank data set.
With our minimum error weighting strat-
egy, the F1 score of the combined result
achieves 80.45%, which is 1.12% higher
than baseline combination method’s re-
sult, and 4.90% higher than the best in-
dividual system’s result.

1 Introduction
In recent years, Chinese Semantic Role Labeling
has received much research effort (Sun and Juraf-
sky, 2004; Xue, 2008; Che et al., 2008; Ding and
Chang, 2008; Sun et al., 2009; Li et al., 2009).
And Chinese SRL is also included in CoNLL-
2009 shared task (Hajiˇc et al., 2009). On the data
set used in (Xue, 2008), the F1 score of the SRL
results using automatic syntactic analysis is still
in low 70s (Xue, 2008; Che et al., 2008; Sun et

al., 2009). As pointed out by Xue (Xue, 2008),
the SRL errors are mainly caused by the errors
in automatic syntactic analysis. In fact, Chinese
SRL suffers from parsing errors even more than
English SRL, because the state-of-the-art parser
for Chinese is still not as good as that for En-
glish. And previous research on English SRL
shows that combination is a robust and effective
method to alleviate SRL’s dependency on pars-
ing results (M`arquez et al., 2005; Koomen et
al., 2005; Pradhan et al., 2005; Surdeanu et al.,
2007; Toutanova et al., 2008). However, the ef-
fect of combination for Chinese SRL task is still
unknown. This raises two questions at least: (1)
How much can Chinese SRL beneﬁt from combi-
nation? (2) Can existing combination strategies
be improved? All existing combination strate-
gies trust each individual system’s output with the
same conﬁdence when putting them into a pool
of candidates. But according to our intuition, dif-
ferent systems have different performance. And
the system that have better performance should
be trusted with more conﬁdence. We can use our
prior knowledge about the combined systems to
do a better combination.

The observations above motivated the work in
this paper.
Instead of directly merging outputs
with equal weights, different outputs are assigned
different weights in our approach. An output’s
weight stands for the conﬁdence we have in that
output. We acquire these weights by minimizing
an error function on the development set. And
we use these weights to merge the outputs.
In
this paper, outputs are generated by a full parsing
based Chinese SRL system and a shallow parsing
based SRL system. The full parsing based system

1363

use multiple parse trees to generate multiple SRL
outputs. Whereas the shallow parsing based sys-
tem only produce one SRL output. After merging
all SRL outputs, we use greedy and integer lin-
ear programming combination methods to com-
bine the merged outputs.

We have evaluated our combination strategy on
Chinese Propbank data set used in (Xue, 2008)
and get encouraging results. With our minimum
error weighting (MEW) strategy,
the F1 score
of the combined result achieves 80.45%. This
is a signiﬁcant improvement over the best re-
ported SRL performance on this data set, which
is 74.12% in the literature (Sun et al., 2009).

2 Related work
A lot of research has been done on SRL combina-
tion. Most of them focused on English SRL task.
But the combination methods are general. And
they are closely related to the work in this paper.
Punyakanok et al. (2004) formulated an Integer
Linear Programming (ILP) model for SRL. Based
on that work, Koomen et al. (2005) combined sev-
eral SRL outputs using ILP method. M`arquez et
al. (2005) proposed a combination strategy that
does not require the individual system to give a
score for each argument. They used a binary clas-
siﬁer to ﬁlter different systems’ outputs. Then
they used a greedy method to combine the can-
didates that pass the ﬁltering process. Pradhan
et al. (2005) combined systems that are based on
phrase-structure parsing, dependency parsing, and
shallow parsing. They also used greedy method
when combining different outputs. Surdeanu et
al. (2007) did a complete research on a variety of
combination strategies. All these research shows
that combination can improve English SRL per-
formance by 2∼5 points on F1 score. However,
little is known about how much Chinese SRL can
beneﬁt from combination. And, as we will show,
existing combination strategies can still be im-
proved.

3 Individual SRL Systems
3.1 Full Parsing Based System
The full parsing based system utilize full syn-
tactic analysis to perform semantic role labeling.

We implemented a Chinese semantic role label-
ing system similar to the one described in (Xue,
2008). Our system consists of an argument identi-
ﬁcation stage and an argument classiﬁcation stage.
In the argument identiﬁcation stage, a number of
argument locations are identiﬁed in a sentence.
In the argument classiﬁcation stage, each location
identiﬁed in the ﬁrst stage is assigned a semantic
role label. The features used in this paper are the
same with those used in (Xue, 2008).

Maximum entropy classiﬁer is employed for
both the argument identiﬁcation and classiﬁcation
tasks. And Zhang Le’s MaxEnt toolkit1 is used for
implementation.

3.2 Shallow Parsing Based System
The shallow parsing based system utilize shal-
low syntactic information at the level of phrase
chunks to perform semantic role labeling. Sun
et al. (2009) proposed such a system on Chinese
SRL and reported encouraging results. The sys-
tem used in this paper is based on their approach.
For Chinese chunking, we adopted the method
used in (Chen et al., 2006), in which chunking is
regarded as a sequence labeling task with IBO2
representation. The features used for chunking
are the uni-gram and bi-gram word/POS tags with
a window of size 2. The SRL task is also re-
garded as a sequence labeling problem. For an
argument with label ARG*, we assign the label
B-ARG* to its ﬁrst chunk, and the label I-ARG*
to its rest chunks. The chunks outside of any argu-
ment are assigned the label O. The features used
for SRL are the same with those used in the one-
stage method in (Sun et al., 2009).

In this paper, we employ Tiny SVM along with
Yamcha (Kudo and Matsumoto, 2001) for Chi-
nese chunking, and CRF++2 for SRL.

3.3 Individual systems’ outputs
The maximum entropy classiﬁer used in full pars-
ing based system and the CRF model used in shal-
low paring based system can both output classi-
ﬁcation probabilities. For the full parsing based
system, the classiﬁcation probability of the ar-

1http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit

.html

2http://crfpp.sourceforge.net/

1364

gument classiﬁcation stage is used as the argu-
ment’s probability. Whereas for the shallow pars-
ing based system, an argument is usually com-
prised of multiple chunks. For example, an argu-
ment with label ARG0 may contain three chunks
labeled as: B-ARG0, I-ARG0, I-ARG0. And each
chunk has a label probability. Thus we have three
probabilities p1, p2, p3 for one argument. In this
case, we use the geometric mean of individual
chunks’ probabilities (p1 · p2 · p3)1/3 as the ar-
gument’s probability.
As illustrated in Figure 1, in an individual sys-
tem’s output, each argument has three attributes:
its location in sentence loc, represented by the
number of its ﬁrst word and last word; its semantic
role label l; and its probability p.

[

Sent: 外商 投资 企业 成为 中国 外贸 重要 增长点
Args:
]
loc:
l:
p:

ARG1
(4, 7)
ARG1
0.92

ARG0
(0, 2)
ARG0
0.94

] [pred] [

Figure 1: Three attributes of an output argument:
location loc, label l, and probability p.

So each argument outputted by a system is a
triple (loc, l, p). For example, the ARG0 in Fig-
ure 1 is ((0, 2), ARG0, 0.94). Because the outputs
of baseline systems are to be combined, we call
such triple a candidate for combination.

4 Approach Overview

As illustrated in Figure 2, the architecture of our
system consists of a candidates generation stage, a
weighted merging stage, and a combination stage.
In the candidates generation stage, the baseline
systems are run individually and their outputs are
collected. We use 2-best parse trees of Berkeley
parser (Petrov and Klein, 2007) and 1-best parse
tree of Bikel parser (Bikel, 2004) and Stanford
parser (Klein and Manning, 2003) as inputs to the
full parsing based system. The second best parse
tree of Berkeley parser is used here for its good
quality. So together we have four different out-
puts from the full parsing based system. From the
shallow parsing based system, we have only one
output.

Figure 2: The overall architecture of our system.

In the weighted merging stage, each system
output is assigned a weight according to our prior
knowledge obtained on the development set. De-
tails about how to obtain appropriate weights will
be explained in Section 6. Then all candidates
with the same loc and l are merged to one by
weighted summing their probabilities. Speciﬁ-
cally, suppose that there are n system outputs to
be combined, with the i-th output’s weight to be
wi. And the candidate in the i-th output with loc
and l is (loc, l, pi) (If there is no candidate with loc
and l in the i-th output, pi is 0.). Then the merged

candidate is (loc, l, p), where p =Pn

After the merging stage, a pool of merged can-
didates is obtained.
In the combination stage,
candidates in the pool are combined to form a
consistent SRL result. Greedy and integer lin-
ear programming combination methods are exper-
imented in this paper.

i=1 wipi.

Sentence

Berkeley

parser

Bikel
parser

Stanford

parser

Chunker

Full parsing based SRL system

Shallow parsing 
based SRL system

Output1

Output2

Output3

Output4

Output5

Weighted
merging

Candidates pool

Combination

Final results

Candidates
Generation

Stage

Weigthed
Merging

Stage

Combination

Stage

1365

5 Combination Methods

5.1 Global constraints
When combining the outputs, two global con-
straints are enforced to resolve the conﬂict be-
tween outputs. These two constraints are:

1. No duplication: There is no duplication for

key arguments: ARG0 ∼ ARG5.
with each other.

2. No overlapping: Arguments cannot overlap

We say two argument candidates conﬂict with
each other if they do not satisfy the two constraints
above.

5.2 Two combination methods
Under these constraints, two methods are explored
to combine the outputs. The ﬁrst one is a greedy
method. In this method, candidates with probabil-
ity below a threshold are deleted at ﬁrst. Then the
remaining candidates are inspected in descending
order according to their probabilities. And each
candidate will be put into a solution set if it does
not conﬂict with candidates already in the set.
This greedy combination method is very simple
and has been adopted in previous research (Prad-
han et al., 2005; M`arquez et al., 2005).

The second combination method is integer lin-
ear programming (ILP) method. ILP method was
ﬁrst applied to SRL in (Punyakanok et al., 2004).
Here we formulate an ILP model whose form is
different from the model in (Punyakanok et al.,
2004; Koomen et al., 2005). For convenience, we
denote the whole label set as {l1, l2, . . . , ln}. And
let l1 ∼ l6 stand for the key argument labels ARG0
∼ ARG5 respectively. Suppose there are m differ-
ent locations, denoted as loc1, . . . , locm, among
all candidates in the pool. And the probability of
assigning lj to loci is pij. A binary variable xij is
deﬁned as:

xij =‰ 1 if loci is assigned label lj,

0 otherwise.

The objective of the ILP model is to maximize the
sum of arguments’ probabilities:

max

mXi=1

nXj=1

(pij − T )xij

(1)

where T is a threshold to prevent including too
many candidates in solution. T is similar to the
threshold in greedy combination method. In this
paper, both thresholds are empirically tuned on
development data, and both are set to be 0.2.

The inequalities in equation (2) make sure that

each loc is assigned at most one label.

∀1 ≤ i ≤ m :

nXj=1

xij ≤ 1

(2)

The inequalities in equation (3) satisfy the No

duplication constraint.

∀1 ≤ j ≤ 6 :

mXi=1

xij ≤ 1

(3)

For any location loci, let Ci denote the index
set of the locations that overlap with it. Then
the No overlapping constraint means that if loci
is assigned a label, i.e.,Pn
j=1 xij = 1, then for
any k ∈ Ci, lock cannot be assigned any label,
i.e.,Pn
j=1 xkj = 0. A common technique in ILP
modeling to form such a constraint is to use a suf-
ﬁciently large auxiliary constant M. And the con-
straint is formulated as:

∀1 ≤ i ≤ m : Xk∈Ci

nXj=1

xkj ≤ (1 −

xij)M

nXj=1

(4)
In this case, M only needs to be larger than the
number of candidates to be combined. In this pa-
per, M = 500 is large enough. And we employ
lpsolve3 to solve the ILP model.

Note that the form of the ILP model in this
paper is different from that in (Punyakanok et
al., 2004; Koomen et al., 2005) in three as-
pects: (1) A special label class null, which means
no label is assigned, was added to the label set
in (Punyakanok et al., 2004; Koomen et al., 2005).
Whereas no such special class is needed in our
model, because if no label is assigned to loci,
j=1 xij = 0 would simply indicate this case.
This makes our model contain fewer variables.
(2) Without null class in our model, we need to
use a different technique to formulate the No-
overlapping constraint.
(3) In order to compare

Pn

3http://lpsolve.sourceforge.net/

1366

with the greedy combination method,
the ILP
model in this paper conforms to exactly the same
constraints as the greedy method. Whereas many
more global constraints were taken into account
in (Punyakanok et al., 2004; Koomen et al., 2005).

These weights are normalized, i.e.,Pn

6 Train Minimum Error Weights
The idea of minimum error weighting is straight-
forward.
Individual outputs O1, O2, . . . , On
are assigned weights w1, w2, . . . , wn respectively.
i=1 wi = 1.
An output’s weight can be seen as the conﬁdence
we have in that output. It is a kind of prior knowl-
edge we have about that output. We can gain this
prior knowledge on the development set. As long
as the data of the development set and the test set
are similar, this prior knowledge should be able
to help to guide SRL combination on test set. In
this section, we discuss how to obtain appropriate
weights.

6.1 Training model
Suppose the golden answer and SRL result on de-
velopment set are d and r respectively. An error
function Er(r, d) is a function that measures the
error contained in r in reference to d. An error
function can be deﬁned as the number of wrong
arguments in r. It can also be deﬁned using preci-
sion, recall, or F1 score. For example, Er(r, d) =
1 − P recision(r, d), or Er(r, d) = 1 − F1(r, d).
Smaller value of error function means less error in
r.

The combination process can also be seen as
a function, which maps the outputs and weights
1 ).
to the combined result r: r = Comb(On
1 , wn
Therefore, the error function of our system on de-
velopment set is:

Er(r, d) = Er(Comb(On

1 , wn

1 ), d)

(5)

From equation (5), it can be seen that: Given de-
velopment set d, if the outputs to be combined On
1
and the combination method Comb are ﬁxed, the
error function is just a function of the weights. So
we can obtain appropriate weights by minimizing
the error function:

ˆwn

1 = arg min
wn
1

Er(Comb(On

1 , wn

1 ), d)

(6)

6.2 Training algorithm

Algorithm 1 Powell Training Algorithm.
1: Input : Error function Er(w).
2: Initialize n directions d1, . . . , dn, and

a start point w in Rn.

α

f (wi + αdi)

f (w + αdn+1)

w1 ← w
for i ← 1, . . . , n:
αi ← arg min
wi+1 ← wi + αidi

3: Set termination threshold δ.
4: do:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: while ∆Er > δ
19: Output: The minimum error weights w.

dn+1 ← wn+1 − w
α∗ ← arg min
w0 ← w + α∗dn+1
∆Er ← Er(w) − Er(w0)
i ← arg max
1≤j≤n
if (α∗)2 ≥
Er(wi) − Er(wi+1):
for j ← i, . . . , n:
dj ← dj+1

Er(wj) − Er(wj+1)

w ← w0

∆Er

α

There are two difﬁculties to solve the optimiza-
tion problem in equation 6. The ﬁrst one is that
the error function cannot be written to an analyt-
ical form. This is because the Comb function,
which stands for the combination process, cannot
be written as an analytical formula. So the prob-
lem cannot be solved using canonical gradient-
based optimization algorithms, because the gradi-
ent function cannot be derived. The second difﬁ-
culty is that, according to our experience, the er-
ror function has many local optima, which makes
it difﬁcult to ﬁnd a global optima.

To resolve the ﬁrst difﬁculty, Modiﬁed Powell’s
method (Yuan, 1993) is employed to solve the op-
timization problem. Powell’s method is a heuris-
tic search method that does not require the objec-
tive function to have an explicit analytical form.
The training algorithm is presented in Algorithm
1. In Algorithm 1, the line search problem in steps
7 and 10 is solved using Brent’s method (Yuan,
1993). And the temination threshold δ is empiri-
cally set to be 0.001 in this paper.

1367

To resolve the second difﬁculty, we perform
multiple searches using different start points, and
then choose the best solution found.

7 Experiments

7.1 Experimental setup
We use Chinese Proposition Bank (CPB) 1.0 and
Chinese Tree Bank (CTB) 5.0 of Linguistic Data
Consortium corpus in our experiments. The train-
ing set is comprised of 648 ﬁles(chtb 081.ﬁd to
chtb 885.ﬁd). The development set is comprised
of 40 ﬁles(chtb 041.ﬁd to chtb 080.ﬁd). The
test set is comprised of 72 ﬁles(chtb 001.ﬁd to
chtb 040.ﬁd and chtb 900.ﬁd to chtb 931.ﬁd).

The same data setting has been used in (Xue,
2008; Ding and Chang, 2008; Sun et al., 2009).
Sun et al. (2009) used sentences with golden seg-
mentation and POS tags as input to their SRL
system. However, we use sentences with only
golden segmentation as input. Then we perform
automatic POS tagging using Stanford POS tag-
ger (Toutanova et al., 2003). In (Xue, 2008), the
parser used by the SRL system is trained on the
training and development set plus 275K words of
broadcast news.
In this paper, all parsers used
by the full parsing based system are trained on
the training set plus the broadcast news portion
of CTB6.0. And the chunker used in the shallow
parsing based system is trained just on the training
set.

7.2 Individual outputs’ performance
In this paper the four outputs of the full parsing
based system are represented by FO1 ∼ FO4 re-
spectively. Among them, FO1 and FO2 are the
outputs using the ﬁrst and second best parse trees
of Berkeley parser, FO3 and FO4 are the outputs
using the best parse trees of Stanford parser and
Bikel parser respectively. The output of the shal-
low parsing based system is represented by SO.
The individual outputs’ performance on develop-
ment and test set are listed in Table 1.

From Table 1 we can see that the performance
of individual outputs are similar on development
set and test set. On both sets, the F1 scores of
individual outputs are in the same order: FO1 >
FO2 > SO > FO3 > FO4.

Data set

Outputs

development

test

FO1
FO2
FO3
FO4
SO
FO1
FO2
FO3
FO4
SO

P (%)
79.17
77.89
72.57
75.60
73.72
80.75
79.44
73.95
75.89
75.69

R(%)
72.09
70.56
67.02
63.45
67.35
70.98
69.37
66.37
63.26
67.90

F1

75.47
74.04
69.68
69.00
70.39
75.55
74.06
70.00
69.00
71.59

Table 1: The results of individual systems on de-
velopment and test set.

7.3 Combining outputs of full parsing based

system

In order to investigate the beneﬁt that the full
parsing based system can get from using multi-
ple parsers, we combine the four outputs FO1 ∼
FO4. The combination results are listed in Ta-
ble 2.
In tables of this paper, “Grd” and “ILP”
stand for greedy and ILP combination methods re-
spectively, and “+MEW” means the combination
is performed with MEW strategy.

Grd
ILP

P (%)
82.68
82.21
Grd+MEW 81.30
ILP+MEW 81.27

R(%)
73.36
73.93
75.38
75.74

F1

77.74
77.85
78.23
78.41

Table 2: The results of combining outputs of full
parsing based system on test set.

Grd
ILP

Er
1 − F1
1 − F1

FO1
0.31
0.33

FO2
0.16
0.10

FO3
0.30
0.27

FO4
0.23
0.30

Table 3: The minimum error weights for the re-
sults in Table 2.

From Table 2 and Table 1, we can see that, with-
out MEW strategy, the F1 score of combination
result is about 2.3% higher than the best individ-
ual output. With MEW strategy, the F1 score is
improved about 0.5% further. That is to say, with
MEW strategy, the beneﬁt of combination is im-
proved by about 20%. Therefore, the effect of
MEW is very encouraging.

Here the error function for MEW training is
chosen to be 1 − F1. And the trained weights
for greedy and ILP methods are listed in Table 3

1368

separately. In tables of this paper, the column Er
corresponds to the error function used for MEW
strategy.

7.4 Combining all outputs
We have also combined all ﬁve outputs. The re-
sults are listed in Table 4. Compared with the re-
sults in Table 2, we can see that the combination
results is largely improved, especially the recall.

Grd
ILP

P (%)
83.64
83.31
Grd+MEW 83.34
ILP+MEW 83.02

R(%)
75.32
75.71
77.47
78.03

F1

79.26
79.33
80.30
80.45

Table 4: The results of combining all outputs on
test set.

From Table 4 and Table 1 we can see that with-
out MEW strategy, the F1 score of combination
result is about 3.8% higher than the best individ-
ual output. With MEW, the F1 score is improved
further by more than 1%. That means the bene-
ﬁt of combination is improved by over 25% with
MEW strategy.

Here the error function for MEW training is still
1 − F1, and the trained weights are listed in Ta-
ble 5.

Grd
ILP

Er
1 − F1
1 − F1

FO1
0.23
0.24

FO2
0.12
0.08

FO3
0.23
0.22

FO4
0.20
0.21

SO
0.22
0.25

Table 5: The minimum error weights for the re-
sults in Table 4.

7.5 Using alternative error functions for

minimum error weights training

In previous experiments, we use 1 − F1 as error
function. As pointed out in Section 6, the def-
inition of error function is very general. So we
have experimented with two other error functions,
which are 1 − P recision, and 1 − Recall. Ob-
viously, these two error functions favor precision
and recall separately. The results of combining
all ﬁve outputs using these two error functions are
listed in Table 6, and the trained weights are listed
in Table 7.

From Table 6 and Table 4, we can see that when
1 − P recison is used as error function, the pre-

P (%)
Er
Grd+MEW 1 − P
85.31
ILP+MEW 1 − P
85.62
Grd+MEW 1 − R 81.94
ILP+MEW 1 − R 79.74

R(%)
73.42
72.76
77.55
78.34

F1

78.92
78.67
79.68
79.03

Table 6: The results of combining all outputs with
alternative error functions.

Grd
ILP
Grd
ILP

FO1
Er
1 − P
0.25
1 − P
0.30
1 − R 0.21
1 − R 0.24

FO2
0.24
0.26
0.10
0.04

FO3
0.22
0.20
0.17
0.10

FO4
0.22
0.15
0.15
0.22

SO
0.07
0.09
0.37
0.39

Table 7: The minimum error weights for the re-
sults in Table 6.

cision of combination result is largely improved.
But the recall decreases a lot. Similar effect of the
error function 1 − Recall is also observed.
The results of this subsection reﬂect the ﬂex-
ibility of MEW strategy. This ﬂexibility comes
from the generality of the deﬁnition of error func-
tion. The choice of error function gives us some
control over the results we want to get. We can
deﬁne different error functions to favor precision,
or recall, or some error counts such as the number
of misclassiﬁed arguments.

7.6 Discussion
In this paper, the greedy and ILP combination
methods conform to the same simple constraints
speciﬁed in Section 5.
From the experiment
results, we can see that ILP method generates
slightly better results than greedy method.

In Subsection 7.4, we see that combining all
outputs using ILP method with MEW strategy
yields 4.90% improvement on F1 score over the
best individual output FO1.
In order to under-
stand each output’s contribution to the improve-
ment over FO1. We compare the differences be-
tween outputs.

Let CO denote the set of correct arguments in
an output O. Then we get the following statistics
when comparing two outputs A and B: (1) the
number of common correct arguments in A and
B, i.e., |CA ∩ CB| ; (2) the number of correct ar-
guments in A and not in B, i.e., |CA\ CB|; (3) the
number of correct arguments in B and not in A,
i.e., |CB \ CA|. The comparison results between

1369

some outputs on test set are listed in Table 8. In
this table, UF stands for the union of the 4 outputs
FO1 ∼ FO4.

A

FO1

UF

B
FO2
FO3
FO4
SO
SO

|CA ∩ CB|

5498
5044
4815
4826
5311

|CA \ CB|

508
962
1191
1180
1550

|CB \ CA|

372
552
512
920
435

Table 8: Comparison between outputs on test set.

From Table 8 we can see that the output SO
has 4826 common correct arguments with FO1,
which is relatively small. And, more importantly,
SO contains 920 correct arguments not in FO1,
which is much more than any other output con-
tains. Therefore, SO is more complementary to
FO1 than other outputs. On the contrary, FO2 is
least complementary to FO1. Even compared with
the union of FO1 ∼ FO4, SO still contains 435
correct arguments not in the union. This shows
that the output of shallow parsing based system is
a good complement to the outputs of full parsing
based system. This explains why recall is largely
improved when SO is combined in Subsection 7.4.
From the analysis above we can also see that the
weights in Table 5 are quite reasonable.
In Ta-
ble 5, SO is assigned the largest weight and FO2
is assigned the smallest weight.

In Subsection 7.3, the MEW strategy improves
the beneﬁt of combination by about 20%. And in
Subsection 7.4, the MEW strategy improves the
beneﬁt of combination by over 25%. This shows
that the MEW strategy is very effective for Chi-
nese SRL combination.

To our best knowledge, no results on Chinese
SRL combination has been reported in the litera-
ture. Therefore, to compare with previous results,
the top two results of single SRL system in the
literature and the result of our combination sys-
tem on this data set are listed in Table 9. For the
results in Table 9, the system of Sun et al. uses
sentences with golden POS tags as input. Xue’s
system and our system both use sentences with
automatic POS tags as input. The result of Sun
et al. (2009) is the best reported result on this data
set in the literature.

(Xue, 2008)

(Sun et al., 2009)

Ours

POS
auto
gold
auto

P (%)
76.8
79.25
83.02

R(%)
62.5
69.61
78.03

F1
68.9
74.12
80.45

Table 9: Previous best single system’s results and
our combination system’s result on this data set.

8 Conclusions

In this paper, we propose a minimum error
weighting strategy for SRL combination and in-
vestigate the beneﬁt that Chinese SRL can get
from combination. We assign different weights to
different system outputs and add a weighted merg-
ing stage to conventional SRL combination sys-
tem architecture. And we also propose a method
to train these weights on development set. We
evaluate the MEW strategy on Chinese Propbank
data set with greedy and ILP combination meth-
ods.

Our experiments have shown that the MEW
strategy is very effective for Chinese SRL combi-
nation, and the beneﬁt of combination can be im-
proved over 25% with this strategy. And also, the
MEW strategy is very ﬂexible. With different def-
initions of error function, this strategy can favor
precision, or recall, or F1 score. The experiments
have also shown that Chinese SRL can beneﬁt a
lot from combination, especially when systems
based on different syntactic views are combined.
The SRL result with the highest F1 score in this
paper is generated by ILP combination together
with MEW strategy. In fact, the MEW strategy is
easy to incorporate with other combination meth-
ods, just like incorporating with the greedy and
ILP combination methods in this paper.

Acknowledgment

The research work has been partially funded by
the Natural Science Foundation of China under
Grant No. 60975053, 90820303 and 60736014,
the National Key Technology R&D Program un-
der Grant No. 2006BAH03B02, the Hi-Tech Re-
search and Development Program (“863” Pro-
gram) of China under Grant No. 2006AA010108-
4, and also supported by the China-Singapore In-
stitute of Digital Media (CSIDM) project under
grant No. CSIDM-200804.

1370

Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic Role Labeling via Integer
Linear Programming Inference. In Proceedings of
COLING-2004.

Honglin Sun and Daniel Jurafsky. 2004. Shallow
In Proceedings of

semantic parsing of Chinese.
NAACL-2004.

Weiwei Sun, Zhifang Sui, Meng Wang, and Xin Wang.
2009. Chinese Semantic Role Labeling with Shal-
low Parsing. In Proceedings of EMNLP-2009.

Mihai Surdeanu, Llu´ıs M`arquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination Strategies for
Semantic Role Labeling. Journal of Artiﬁcial Intel-
ligence Research (JAIR), 29:105-151.

Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A Global Joint Model for Se-
mantic Role Labeling. Computational Linguistics,
34(2): 145-159.

Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of HLT-NAACL-2003.

Nianwen Xue. 2008. Labeling Chinese Predicates
with Semantic Roles. Computational Linguistics,
34(2): 225-255.

Yaxiang Yuan. 1993. Numerical Methods for Nonlin-
ear Programming. Shanghai Scientiﬁc and Techni-
cal Pulishers, Shanghai.

References
Daniel Bikel. 2004.

Intricacies of Collins Parsing
Model. Computational Linguistics, 30(4):480-511.

Wanxiang Che, Min Zhang, Ai Ti Aw, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. Using a Hybrid Con-
volution Tree Kernel for Semantic Role Labeling.
ACM Transactions on Asian Language Information
Processing, 2008, 7(4).

Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006. An empirical study of Chinese chunking. In
Proceedings of COLING/ACL-2006.

Weiwei Ding and Baobao Chang. 2008.

Improving
Chinese Semantic Role Classiﬁcation with Hierar-
chical Feature Selection Strategy. In Proceedings of
EMNLP-2008.

Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue and Yi Zhang. 2009. The CoNLL-
2009 Shared Task: Syntactic and Semantic Depen-
dencies in Multiple Languages. In Proceedings of
CoNLL-2009.

Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL-
2003.

Peter Koomen, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized Inference with
Multiple Semantic Role Labeling Systems. In Pro-
ceedings of CoNLL-2005 shared task.

Taku Kudo and Yuji Matsumoto. 2001. Chunking
In Proceedings of

with Support Vector Machines.
NAACL-2001.

Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu,
and Peide Qian. 2009.
Improving Nominal SRL
in Chinese Language with Verbal SRL Information
and Automatic Predicate Recognition. In Proceed-
ings of EMNLP-2009.

Llu´ıs M`arquez, Mihai Surdeanu, Pere Comas, and
Jordi Turmo. 2005. A Robust Combination Strat-
egy for Semantic Role Labeling. In Proceedings of
EMNLP-2005.

Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized parsing. In Proceedings of ACL-
2007.

Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic Role Labeling Using Different Syntactic
Views. In Proceedings of ACL-2005.

