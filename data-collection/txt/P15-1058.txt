



















































Multi-Objective Optimization for the Joint Disambiguation of Nouns and Named Entities


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 596–605,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Multi-Objective Optimization for the Joint Disambiguation of
Nouns and Named Entities

Dirk Weissenborn, Leonhard Hennig, Feiyu Xu and Hans Uszkoreit
Language Technology Lab, DFKI

Alt-Moabit 91c
Berlin, Germany

{dirk.weissenborn, leonhard.hennig, feiyu, uszkoreit}@dfki.de

Abstract

In this paper, we present a novel approach
to joint word sense disambiguation (WSD)
and entity linking (EL) that combines a set
of complementary objectives in an exten-
sible multi-objective formalism. During
disambiguation the system performs con-
tinuous optimization to find optimal prob-
ability distributions over candidate senses.
The performance of our system on nomi-
nal WSD as well as EL improves state-of-
the-art results on several corpora. These
improvements demonstrate the importance
of combining complementary objectives in
a joint model for robust disambiguation.

1 Introduction

The task of automatically assigning the correct
meaning to a given word or entity mention in
a document is called word sense disambiguation
(WSD) (Navigli, 2009) or entity linking (EL)
(Bunescu and Pasca, 2006), respectively. Suc-
cessful disambiguation requires not only an un-
derstanding of the topic or domain a document is
dealing with, but also a deep analysis of how an in-
dividual word is used within its local context. For
example, the meanings of the word “newspaper”,
as in the company or the physical product, often
cannot be distinguished by the global topic of the
document it was mentioned in, but by recogniz-
ing which type of meaning fits best into the local
context of its mention. On the other hand, for an
ambiguous entity mention such as a person name,
e.g., “Michael Jordan”, it is important to recognize
the domain or topic of the wider context to distin-
guish, e.g., between the basketball player and the
machine learning expert.

The combination of the two most com-
monly employed reference knowledge bases for
WSD and EL, WordNet (Fellbaum, 1998) and

Wikipedia, in BabelNet (Navigli and Ponzetto,
2012), has enabled a new line of research towards
the joint disambiguation of words and named en-
tities. Babelfy (Moro et al., 2014) has shown
the potential of combining these two tasks in
a purely knowledge-driven approach that jointly
finds connections between potential word senses
on a global, document level. On the other hand,
typical supervised methods (Zhong and Ng, 2010)
trained on sense-annotated datasets are usually
quite successful in dealing with individual words
in their local context on a sentence level. Hoffart
et al. (2011) recognize the importance of combin-
ing both local and global context for robust dis-
ambiguation. However, their approach is limited
to EL and optimization is performed in a discrete
setting.

We present a system that combines disambigua-
tion objectives for both global and local contexts
into a single multi-objective function. The result-
ing system is flexible and easily extensible with
complementary objectives. In contrast to prior
work (Hoffart et al., 2011; Moro et al., 2014) we
model the problem in a continuous setting based
on probability distributions over candidate mean-
ings instead of a binary treatment of candidate
meanings during disambiguation. Our approach
combines knowledge from various sources in one
robust model. The system uses lexical and ency-
clopedic knowledge for the joint disambiguation
of words and named entities, and exploits local
context information of a mention to infer the type
of its meaning. We integrate prior statistics from
surface strings to candidate meanings in a “nat-
ural” way as starting probability distributions for
each mention.

The contributions of our work are the following:

• a model for joint nominal WSD and EL that
outperforms previous state-of-the-art systems
on both tasks
• an extensible framework for multi-objective

596



disambiguation
• an extensive evaluation of the approach on

multiple standard WSD and EL datasets
• the first work that employs continuous op-

timization techniques for disambiguation (to
our knowledge)
• publicly available code, resources and

models at https://bitbucket.org/
dfki-lt-re-group/mood

2 Approach

Our system detects mentions in texts and disam-
biguates their meaning to one of the candidate
senses extracted from a reference knowledge base.
The integral parts of the system, namely mention
detection, candidate search and disambiguation
are described in detail in this section. The model
requires a tokenized, lemmatized and POS-tagged
document as input; the output are sense-annotated
mentions.

2.1 Knowledge Source

We employ BabelNet 2.5.1 as our reference
knowledge base (KB). BabelNet is a multilingual
semantic graph of concepts and named entities
that are represented by synonym sets, called Ba-
bel synsets. It is composed of lexical and encyclo-
pedic resources, such as WordNet and Wikipedia.
Babel synsets comprise several Babel senses, each
of which corresponds to a sense in another knowl-
edge base. For example the Babel synset of
“Neil Armstrong” contains multiple senses in-
cluding for example “armstrong#n#1” (WordNet),
“Neil Armstrong” (Wikipedia). All synsets are in-
terlinked by conceptual-semantic and lexical re-
lations from WordNet and semantic relations ex-
tracted from links between Wikipedia pages.

2.2 Mention Extraction & Entity Detection

We define a mention to be a sequence of tokens in
a given document. The system extracts mentions
for all content words (nouns, verbs, adjectives, ad-
verbs) and multi-token units of up to 7 tokens that
contain at least one noun. In addition, we apply
a NER-tagger to identify named entity (NE) men-
tions. Our approach distinguishes NEs from com-
mon nouns because there are many common nouns
also referring to NEs, making disambiguation un-
necessarily complicated. For example, the word
“moon” might refer to songs, films, video games,
etc., but we should only consider these meanings

if the occurrence suggests that it is used as a NE.

2.3 Candidate Search
After potential mentions are extracted, the sys-
tem tries to identify their candidate meanings, i.e.,
the appropriate synsets. Mentions without any
candidates are discarded. There are various re-
sources one can exploit to map surface strings to
candidate meanings. However, existing methods
or resources especially for NEs are either miss-
ing many important mappings1 or contain many
noisy mappings2. Therefore, we created a can-
didate mapping strategy that tries to avoid noisy
mappings while including all potentially correct
candidates. Our approach employs several heuris-
tics that aim to avoid noise. Their union yields
an almost complete mapping that includes the cor-
rect candidate meaning for 97-100% of the exam-
ples in the test datasets. Candidate mentions are
mapped to synsets based on similarity of their sur-
face strings or lemmas. If the surface string or
lemma of a mention matches the lemma of a syn-
onym in a synset that has the same part of speech,
the synset will be considered as a candidate mean-
ing. We allow partial matches for BabelNet syn-
onyms derived from Wikipedia titles or redirec-
tions. However, partial matching is restricted to
synsets that belong either to the semantic category
“Place” or “Agent”. We make use of the seman-
tic category information provided by the DBpe-
dia ontology3. A partial match allows the sur-
face string of a mention to differ by up to 3 to-
kens from the Wikipedia title (excluding every-
thing in parentheses) if the partial string occurred
at least once as an anchor for the corresponding
Wikipedia page. E.g., for the Wikipedia title Arm-
strong School District (Pennsylvania), the fol-
lowing surface strings would be considered
matches: “Armstrong School District (Pennsylva-
nia)”, “Armstrong School District”, “Armstrong”,
but not “School” or “District”, since they were
never used as an anchor. If there is no match we try
the same procedure applied to the lowercase forms
of the surface string or the lemma. For persons we
allow matches to all partial names, e.g., only first
name, first and middle name, last name, etc.

In addition to the aforementioned candidate ex-
traction we also match surface strings to candidate
entities mentioned on their respective disambigua-

1e.g., using only the synonyms of a synset
2e.g., partial matches for all synonyms of a synset
3http://wiki.dbpedia.org/Ontology

597



tion pages in Wikipedia4. For cases where ad-
jectives should be disambiguated as nouns, e.g.,
“English” as a country to “England”, we allow
candidate mappings through the pertainment rela-
tion from WordNet. Finally, frequently annotated
surface strings in Wikipedia are matched to their
corresponding entities, where we stipulate “fre-
quently” to mean that the surface string occurs at
least 100 times as anchor in Wikipedia and the en-
tity was either at least 100 times annotated by this
surface string or it was annotated above average.

The distinction between nouns and NEs im-
poses certain restrictions on the set of potential
candidates. Candidate synsets for nouns are noun
synsets considered as “Concepts” in BabelNet (as
opposed to “Named Entities”) in addition to all
synsets of WordNet senses. On the other hand,
candidate synsets for NEs comprise all nominal
Babel synsets. Thus, the range of candidate sets
for NEs properly contains the one for nouns. We
include all nominal synsets as potential candidates
for NEs because the distinction of NEs and sim-
ple concepts is not always clear in BabelNet. For
example the synset for “UN” (United Nations) is
considered a concept whereas it could also be con-
sidered a NE. Finally, if there is no candidate for a
potential nominal mention, we try to find NE can-
didates for it before discarding it.

2.4 Multi-Objective Disambiguation

We formulate the disambiguation as a continuous,
multi-objective optimization problem. Individual
objectives model different aspects of the disam-
biguation problem. Maximizing these objectives
means assigning high probabilities to candidate
senses that contribute most to the combined ob-
jective. After maximization, we select the candi-
date meaning with the highest probability as the
disambiguated sense. Our model is illustrated in
Figure 1.

Given a set of objectives O the overall objective
function O is defined as the sum of all normalized
objectives O ∈ O given a set of mentions M :

O(M) =
∑
O∈O

|MO|
|M | ·

O(M)
Omax(M)−Omin(M) .

(1)
The continuous approach has several advan-

tages over a discrete setting. First, we can ex-

4provided by DBpedia at http://wiki.dbpedia.
org/Downloads2014

Armstrong
- Armstrong_(crater) 0.6
- Neil_Armstrong 0.2
- Louis_Armstrong 0.1
...

jazz
- jazz_(music) 0.3
- jazz_(rhetoric) 0.3
- ...

Mentions M

play
- play_(game) 0.4
- play_(instrument) 0.2
- ...

Armstrong
- Armstrong_(crater) 0.3
- Neil_Armstrong 0.1
- Louis_Armstrong 0.5
- ...

Mentions MObjectives   

.

.

.

While not_converged or i < max_iterations

play
- play_(game) 0.1
- play_(instrument) 0.6
- ...

jazz
- jazz_(music) 0.8
- jazz_(rhetoric) 0.1
- ...

Figure 1: Illustration of our multi-objective ap-
proach to WSD & EL for the example sen-
tence: Armstrong plays jazz. Mentions are disam-
biguated by iteratively updating probability distri-
butions over their candidate senses with respect to
the given objective gradients∇Oi.

ploit well established continuous optimization al-
gorithms, such as conjugate gradient or LBFGS.
Second, by optimizing upon probability distribu-
tions we are optimizing the actually desired result,
in contrast to densest sub-graph algorithms where
normalized confidence scores are calculated after-
wards, e.g., Moro et al. (2014). Third, discrete
optimization usually works on a single candidate
per iteration whereas in a continuous setting, prob-
abilities are adjusted for each candidate, which is
computationally advantageous for highly ambigu-
ous documents.

We normalize each objective using the differ-
ence of its maximum and minimum value for a
given document, which makes the weighting of
the objectives different for each document. The
maximum/minimum values can be calculated ana-
lytically or, if this is not possible, by running the
optimization algorithm with only the given objec-
tive for an approximate estimate for the maximum
and with its negated form for an approximate min-
imum. Normalization is important for optimiza-
tion because it ensures that the individual gradi-
ents have similar norms on average for each ob-
jective. Without normalization, optimization is bi-
ased towards objectives with large gradients.

Given that one of the objectives can be applied
to only a fraction of all mentions (e.g., only nomi-
nal mentions), we scale each objective by the frac-
tion of mentions it is applied to.

Note that our formulation could easily be ex-
tended to using additional coefficients for each ob-

598



jective. However, these hyper-parameters would
have to be estimated on development data and
therefore, this method could hurt generalization.

Prior Another advantage of working with prob-
ability distributions over candidates is the easy in-
tegration of prior information. For example, the
word “Paris” without further context has a strong
prior on its meaning as a city instead of a per-
son. Our approach utilizes prior information in
form of frequency statistics over candidate synsets
for a mention’s surface string. These priors are
derived from annotation frequencies provided by
WordNet and Wikipedia. We make use of oc-
currence frequencies extracted by DBpedia Spot-
light (Daiber et al., 2013) for synsets containing
Wikipedia senses in case of NE disambiguation.
For nominal WSD, we employ frequency statis-
tics from WordNet for synsets containing Word-
Net senses. Laplace-smoothing is applied to all
prior frequencies. The priors serve as initializa-
tion for the probability distributions over candi-
date synsets. Note that we use priors “naturally”,
i.e., as actual priors for initialization only and not
during disambiguation itself. They should not be
applied during disambiguation because these pri-
ors can be very strong and are not domain inde-
pendent. However, they provide a good initializa-
tion which is important for successful continuous
optimization.

3 Disambiguation Objectives

3.1 Coherence Objective

Jointly disambiguating all mentions within a doc-
ument has been shown to have a large impact on
disambiguation quality, especially for named enti-
ties (Kulkarni et al., 2009). It requires a measure-
ment of semantic relatedness between concepts
that can for example be extracted from a semantic
network like BabelNet. However, semantic net-
works usually suffer from data sparsity where im-
portant links between concepts might be missing.
To deal with this issue, we adopt the idea of using
semantic signatures from Moro et al. (2014). Fol-
lowing their approach, we create semantic signa-
tures for concepts and named entities by running
a random walk with restart (RWR) in the seman-
tic network. We count the times a vertex is vis-
ited during RWR and define all frequently visited
vertices to be the semantic signature (i.e., a set of
highly related vertices) of the starting concept or

named entity vertex.
Our coherence objective aims at maximizing

the semantic relatedness among selected candidate
senses based on their semantic signatures Sc. We
define the continuous objective using probability
distributions pm(c) over the candidate set Cm of
each mention m ∈M in a document as follows:

Ocoh(M) =
∑

m∈M
c∈Cm

∑
m′∈M
m′ 6=m
c′∈Cm′

s(m, c,m′, c′)

s(m, c,m′, c′) = pm(c) · pm′(c′) · 1((c, c′) ∈ S)
pm(c) =

ewm,c∑
c′∈Cm e

wm,c′ , (2)

where 1 denotes the indicator function and pm(c)
is a softmax function. The only free, optimizable
parameters are the softmax weights wm. This ob-
jective includes all mentions, i.e., MOcoh = M . It
can be interpreted as finding the densest subgraph
where vertices correspond to mention-candidate
pairs and edges to semantic signatures between
candidate synsets. However, in contrast to a dis-
crete setup, each vertex is now weighted by its
probability and therefore each edge is weighted by
the product of its adjacent vertex probabilities.

3.2 Type Objective

One of the biggest problems for supervised ap-
proaches to WSD is the limited size and synset
coverage of available training datasets such as
SemCor (Miller et al., 1993). One way to cir-
cumvent this problem is to use a coarser set of se-
mantic classes that groups synsets together. Pre-
vious studies on using semantic classes for dis-
ambiguation showed promising results (Izquierdo-
Beviá et al., 2006). For example, WordNet pro-
vides a mapping, called lexnames, of synsets into
45 types, which is based on the syntactic cate-
gories of synsets and their logical groupings5. In
WordNet 13.5% of all nouns are ambiguous with
an average ambiguity of 2.79 synsets per lemma.
Given a noun and a type (lexname), the percentage
of ambiguous nouns drops to 7.1% for which the
average ambiguity drops to 2.33. This indicates
that exploiting type classification for disambigua-
tion can be very useful.

Similarly, for EL it is important to recognize
the type of an entity mention in a local context.

5http://wordnet.princeton.edu/man/
lexnames.5WN.html

599



For example, in the phrase “London beats Manch-
ester” it is very likely that the two city names refer
to sports clubs and not to the cities. We utilize an
existing mapping from Wikipedia pages to types
from the DBpedia ontology, restricting the set of
target types to the following: “Activity”, “Organ-
isation”, “Person”, “Event”, “Place” and “Misc”
for the rest.

We train a multi-class logistic regression model
for each set of types that calculates probability
distributions qm(t) over WN- or DBpedia-types t
given a noun- or a NE-mention m, respectively.
The features used as input to the model are the fol-
lowing:

• word embedding of mention’s surface string
• sum of word embeddings of all sentence

words excluding stopwords
• word embedding of the dependency parse

parent
• collocations of surrounding words as in

Zhong et al. (2010)
• POS tags with up to 3 tokens distance to m
• possible types of candidate synsets

We employed pre-trained word embeddings from
Mikolov et al. (2013) instead of the words them-
selves to increase generalization.

Type classification is included as an objective
in the model as defined in equation 3. It puts type
specific weights derived from type classification
on candidate synsets, enforcing candidates of fit-
ting type to have higher probabilities. The objec-
tive is only applied to noun, NE and verb men-
tions, i.e., MOtyp = Mn ∪MNE ∪Mv.

Otyp(M) =
∑

m∈MOtyp

∑
c∈Cm

qm(tc) · pm(c) (3)

3.3 Regularization Objective

Because candidate priors for NE mentions can be
very high, we add an additional L2-regularization
objective for NE mentions:

OL2(M) = −λ2
∑

m∈MNE
‖wm‖22 (4)

The regularization objective is integrated in the
overall objective function as it is, i.e., it is not nor-
malized.

Dataset |D| |M| KB
SemEval-2015-13 (Sem15) 4 757 BN
(to be published)
SemEval-2013-12 (Sem13) 13 1931 BN
SemEval-2013-12 (Sem13) 13 1644 WN
(Navigli et al., 2013)
SemEval-2007-17 (Sem07) 3 159 WN
(Pradhan et al., 2007)
Senseval 3 (Sen3) 4 886 WN
(Snyder and Palmer, 2004)
AIDA-CoNLL-testb (AIDA) 216 4530 Wiki
(Hoffart et al., 2011)
KORE50 (KORE) 50 144 Wiki
(Hoffart et al., 2012)

Table 1: List of datasets used in experiments with
information about their number of documents (D),
annotated noun and/or NE mentions (M ), and
their respective target knowledge base (KB): BN-
BabelNet, WN-WordNet, Wiki-Wikipedia.

4 Experiments

4.1 Datasets

We evaluated our approach on 7 different datasets,
comprising 3 WSD datasets annotated with Word-
Net senses, 2 datasets annotated with Wikipedia
articles for EL and 2 more recent datasets anno-
tated with Babel synsets. Table 1 contains a list of
all datasets.

Besides these test datasets we used SemCor
(Miller et al., 1993) as training data for WSD and
the training part of the AIDA CoNLL dataset for
EL.

4.2 Setup

For the creation of semantic signatures we choose
the same parameter set as defined by Moro et al.
(2014). We run the random walk with a restart
probability of 0.85 for a total of 1 million steps for
each vertex in the semantic graph and keep ver-
tices visited at least 100 times as semantic signa-
tures.

The L2-regularization objective for named enti-
ties is employed with λ = 0.001, which we found
to perform best on the training part of the AIDA-
CoNLL dataset.

We trained the multi-class logistic regression
model for WN-type classification on SemCor and
for DBpedia-type classification on the training
part of the AIDA-CoNLL dataset using LBFGS
and L2-Regularization with λ = 0.01 until con-
vergence.

Our system optimizes the combined multi-
objective function using Conjugate Gradient

600



System KB Description
IMS (Zhong and
Ng, 2010)

WN supervised, SVM

KPCS (Hoffart
et al., 2011)

Wiki greedy densest-subgraph on
combined mention-entity,
entity-entity measures

KORE (Hoffart
et al., 2012)

Wiki extension of KPCS with
keyphrase relatedness mea-
sure between entities

MW (Milne and
Witten, 2008)

Wiki Normalized Google Dis-
tance

Babelfy (Moro
et al., 2014)

BN greedy densest-subgraph on
semantic signatures

Table 2: Systems used for comparison during eval-
uation.

(Hestenes and Stiefel, 1952) with up to a maxi-
mum of 1000 iterations per document.

We utilized existing implementations from
FACTORIE version 1.1 (McCallum et al., 2009)
for logistic regression, NER tagging and Conju-
gate Gradient optimization. For NER tagging we
used a pre-trained stacked linear-chain CRF (Laf-
ferty et al., 2001).

4.3 Systems

We compare our approach to state-of-the-art re-
sults on all datasets and a most frequent sense
(MFS) baseline. The MFS baseline selects the
candidate with the highest prior as described in
section 2.4. Table 2 contains a list of all sys-
tems we compared against. We use Babelfy as our
main baseline, because of its state-of-the-art per-
formance on all datasets and because it also em-
ployed BabelNet as its sense inventory. Note that
Babelfy achieved its results with different setups
for WSD and EL, in contrast to our model, which
uses the same setup for both tasks.

4.4 General Results

We report the performance of all systems in terms
of F1-score. To ensure fairness we restricted
the candidate sets of the target mentions in each
dataset to candidates of their respective reference
KB. Note that our candidate mapping strategy en-
sures for all datasets a 97%−100% chance that the
target synset is within a mention’s candidate set.

This section presents results on the evaluation
datasets divided by their respective target KBs:
WordNet, Wikipedia and BabelNet.

WordNet Table 3 shows the results on three
datasets for the disambiguation of nouns to Word-

System Sens3 Sem07 Sem13
MFS 72.6 65.4 62.8
IMS 71.2 63.3 65.7
Babelfy 68.3 62.7 65.9
Our 68.8 66.0 72.8

Table 3: Results for nouns on WordNet annotated
datasets.

System AIDA KORE
MFS 70.1 35.4
KPCS 82.2 55.6
KORE-LSH-G 81.8 64.6
MW 82.3 57.6
Babelfy 82.1 71.5
Our 85.1 67.4

Table 4: Results for NEs on Wikipedia annotated
datasets.

Net. Our approach exhibits state-of-the-art re-
sults outperforming all other systems on two of the
three datasets. The model performs slightly worse
on the Senseval 3 dataset because of one docu-
ment in particular where the F1 score is very low
compared to the MFS baseline. On the other three
documents, however, it performs as good or even
better. In general, results from the literature are al-
ways worse than the MFS baseline on this dataset.
A strong improvement can be seen on the SemEval
2013 Task 12 dataset (Sem13), which is also the
largest dataset. Our system achieves an improve-
ment of nearly 7% F1 over the best other system,
which translates to an error reduction of roughly
20% given that every word mention gets anno-
tated. Besides the results presented in Table 3, we
also evaluated the system on the SemEval 2007
Task 7 dataset for coarse grained WSD, where it
achieved 85.5% F1 compared to the best previ-
ously reported result of 85.5% F1 from Ponzetto
et al. (2010) and Babelfy with 84.6%.

Wikipedia The performance on entity linking
was evaluated against state-of-the-art systems on
two different datasets. The results in Table 4
demonstrate that our model can compete with the
best existing models, showing superior results es-
pecially on the large AIDA CoNLL6 test dataset
comprising 216 news texts, where we achieve
an error reduction of about 16%, resulting in a
new state-of-the-art of 85.1% F1. On the other
hand, our system is slightly worse on the KORE
dataset compared to Babelfy (6 errors more in to-
tal), which might be due to the strong priors and

6the largest, freely available dataset for EL.

601



System Sem13 Sem15
MFS 66.7 71.1
Babelfy 69.2 –
Best other – 64.8
Our 71.5 75.4

Table 5: Results for nouns and NEs on BabelNet
annotated datasets.

System Sem13 Sem15 AIDA
MFS 66.7 71.1 70.1
Otyp 68.1 73.8 78.0
Ocoh + OL2 68.1 69.6 82.7
Ocoh + Otyp + OL2 71.5 75.4 85.1

Table 6: Detailed results for nouns and NEs on
BabelNet annotated datasets and AIDA CoNLL.

the small context. However, the dataset is rather
small, containing only 50 sentences, and has been
artificially tailored to the use of highly ambiguous
entity mentions. For example, persons are most
of the time only mentioned by their first names.
It is an interesting dataset because it requires the
system to employ a lot of background knowledge
about mentioned entities.

BabelNet Table 5 shows the results on the 2 ex-
isting BabelNet annotated datasets. To our knowl-
edge, our system shows the best performance on
both datasets in the literature. An interesting ob-
servation is that the F1 score on SemEval 2013
with BabelNet as target KB is lower compared to
WordNet as target KB. The reason is that ambigu-
ity rises for nominal mentions by including con-
cepts from Wikipedia that do not exist in WordNet.
For example, the Wikipedia concept “formal lan-
guage” becomes a candidate for the surface string
“language”.

4.5 Detailed Results
We also experimented with different objective
combinations, namely “type only” (Otyp), “coher-
ence only” (Ocoh +OL2) and “all” (Ocoh +Otyp +
OL2), to evaluate the impact of the different objec-
tives. Table 6 shows results of employing individ-
ual configurations compared to the MFS baseline.

Results for only using coherence or type exhibit
varying performance on the datasets, but still con-
sistently exceed the strong MFS baseline. Com-
bining both objectives always yields better results
compared to all other configurations. This find-
ing is important because it proves that the objec-
tives proposed in this work are indeed comple-
mentary, and thus demonstrates the significance of

combining complementary approaches in one ro-
bust framework such as ours.

An additional observation was that DBpedia-
type classification slightly overfitted on the AIDA
CoNLL training part. When removing DBpedia-
type classification from the type objective, results
increased marginally on some datasets except for
the AIDA CoNLL dataset, where results decreased
by roughly 3% F1. The improvements of using
DBpedia-type classification are mainly due to the
fact that the classifier is able to correctly clas-
sify names of places in tables consisting of sports
scores not to the “Place” type but to the “Organi-
zation” type. Note that the AIDA CoNLL dataset
(train and test) contains many of those tables. This
shows that including supervised objectives into the
system helps when data is available for the do-
main.

4.6 Generalization

We evaluated the ability of our system to gener-
alize to different domains based on the SemEval
2015 Task 13 dataset. It includes documents from
the bio-medical, the math&computer and general
domains. Our approach performs particularly well
on the bio-medical domain with 86.3% F1 (MFS:
77.3%). Results on the math&computer domain
(58.8% F1, MFS: 57.0%), however, reveal that
performance still strongly depends on the docu-
ment topic. This indicates that either the employed
resources do not cover this domain as well as oth-
ers, or that it is generally more difficult to dis-
ambiguate. Another potential explanation is that
enforcing only pairwise coherence does not take
the hidden concepts computer and maths into ac-
count, which connect all concepts, but are never
actually mentioned. An interesting point for future
research might be the introduction of an additional
objective or the extension of the coherence objec-
tive to allow indirect connections between candi-
date meanings through shared topics or categories.

Besides these very specific findings, the model’s
ability to generalize is strongly supported by its
good results across all datasets, covering a variety
of different topics.

5 Related Work

WSD Approaches to WSD can be distinguished
by the kind of resource exploited. The two main
resources for WSD are sense annotated datasets
and knowledge bases. Typical supervised ap-

602



proaches like IMS (Zhong and Ng, 2010) train
classifiers that learn from existing, annotated ex-
amples. They suffer from the sparsity of sense
annotated datasets that is due to the data acqui-
sition bottleneck (Pilehvar and Navigli, 2014).
There have been approaches to overcome this
issue through the automatic generation of such
resources based on bootstrapping (Pham et al.,
2005), sentences containing unambiguous rela-
tives of senses (Martinez et al., 2008) or exploit-
ing Wikipedia (Shen et al., 2013). On the other
hand, knowledge-based approaches achieve good
performances rivaling state-of-the-art supervised
systems (Ponzetto and Navigli, 2010) by using ex-
isting structured knowledge (Lesk, 1986; Agirre
et al., 2014), or take advantage of the structure of
a given semantic network through connectivity or
centrality measures (Tsatsaronis et al., 2007; Nav-
igli and Lapata, 2010). Such systems benefit from
the availability of numerous KBs for a variety of
domains. We believe that both knowledge-based
approaches and supervised methods have unique,
complementary abilities that need to be combined
for sophisticated disambiguation.

EL Typical EL systems employ supervised ma-
chine learning algorithms to classify or rank can-
didate entities (Bunescu and Pasca, 2006; Milne
and Witten, 2008; Zhang et al., 2010). Com-
mon features include popularity metrics based on
Wikipedia’s graph structure or on name mention
frequency (Dredze et al., 2010; Han and Zhao,
2009), similarity metrics exploring Wikipedia’s
concept relations (Han and Zhao, 2009), and
string similarity features. Mihalcea and Csomai
(2007) disambiguate each mention independently
given its sentence level context only. In contrast,
Cucerzan (2007) and Kulkarni et al. (Kulkarni
et al., 2009) recognize the interdependence be-
tween entities in a wider context. The most sim-
ilar work to ours is that of Hoffart et al. (2011)
which was the first that combined local and global
context measures in one robust model. However,
objectives and the disambiguation algorithm differ
from our work. They represent the disambigua-
tion task as a densest subgraph problem where the
least connected entity is eliminated in each itera-
tion. The discrete treatment of candidate entities
can be problematic especially at the beginning of
disambiguation where it is biased towards men-
tions with many candidates.

Babelfy (Moro et al., 2014) is a knowledge-
based approach for joint WSD and EL that also
uses a greedy densest subgraph algorithm for dis-
ambiguation. It employs a single coherence model
based on semantic signatures similar to our coher-
ence objective. The system’s very good perfor-
mance indicates that the semantic signatures pro-
vide a powerful resource for joint disambiguation.
However, because we believe it is not sufficient
to only enforce semantic agreement among nouns
and entities, our approach includes an objective
that also focuses on the local context of mentions,
making it more robust.

6 Conclusions & Future Work

We have presented a novel approach for the
joint disambiguation of nouns and named enti-
ties based on an extensible framework. Our sys-
tem employs continuous optimization on a multi-
objective function during disambiguation. The
integration of complementary objectives into our
formalism demonstrates that robust disambigua-
tion can be achieved by considering both the local
and the global context of a mention. Our model
outperforms previous state-of-the-art systems for
nominal WSD and for EL. It is the first system
that achieves such results on various WSD and EL
datasets using a single setup.

In future work, new objectives should be inte-
grated into the framework and existing objectives
could be enhanced. For example, it would be in-
teresting to express semantic relatedness contin-
uously rather than in a binary setting for the co-
herence objective. Additionally, using the entire
model during training could ensure better com-
patibility between the different objectives. At the
moment, the model itself is composed of different
pre-trained models that are only combined during
disambiguation.

Acknowledgment

This research was partially supported by the
German Federal Ministry of Education and
Research (BMBF) through the projects ALL
SIDES (01IW14002), BBDC (01IS14013E), and
by the German Federal Ministry of Economics
and Energy (BMWi) through the project SD4M
(01MD15007B), and by Google through a Fo-
cused Research Award granted in July 2013.

603



References
[Agirre et al.2014] Eneko Agirre, Oier Lopez de La-

calle, and Aitor Soroa. 2014. Random walks for
knowledge-based word sense disambiguation. Com-
putational Linguistics, 40(1):57–84.

[Bunescu and Pasca2006] Razvan C Bunescu and Mar-
ius Pasca. 2006. Using encyclopedic knowledge for
named entity disambiguation. In EACL, volume 6,
pages 9–16.

[Cucerzan2007] Silviu Cucerzan. 2007. Large-scale
named entity disambiguation based on wikipedia
data. In EMNLP-CoNLL, volume 7, pages 708–716.

[Daiber et al.2013] Joachim Daiber, Max Jakob, Chris
Hokamp, and Pablo N Mendes. 2013. Improving
efficiency and accuracy in multilingual entity extrac-
tion. In Proceedings of the 9th International Confer-
ence on Semantic Systems, pages 121–124. ACM.

[Dredze et al.2010] Mark Dredze, Paul McNamee,
Delip Rao, Adam Gerber, and Tim Finin. 2010.
Entity disambiguation for knowledge base popula-
tion. In Proc. of the 23rd International Conference
on Computational Linguistics, pages 277–285. As-
sociation for Computational Linguistics.

[Fellbaum1998] Christiane Fellbaum. 1998. WordNet.
Wiley Online Library.

[Han and Zhao2009] Xianpei Han and Jun Zhao.
2009. Named entity disambiguation by leverag-
ing wikipedia semantic knowledge. In Proc. of the
18th ACM conference on Information and knowl-
edge management, pages 215–224. ACM.

[Hestenes and Stiefel1952] Magnus Rudolph Hestenes
and Eduard Stiefel. 1952. Methods of conjugate
gradients for solving linear systems, volume 49. Na-
tional Bureau of Standards Washington, DC.

[Hoffart et al.2011] Johannes Hoffart, Mohamed Amir
Yosef, Ilaria Bordino, Hagen Fürstenau, Man-
fred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan
Thater, and Gerhard Weikum. 2011. Robust disam-
biguation of named entities in text. In Proc. of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 782–792. Association for
Computational Linguistics.

[Hoffart et al.2012] Johannes Hoffart, Stephan Seufert,
Dat Ba Nguyen, Martin Theobald, and Gerhard
Weikum. 2012. Kore: keyphrase overlap related-
ness for entity disambiguation. In Proc. of the 21st
ACM international conference on Information and
knowledge management, pages 545–554. ACM.

[Izquierdo-Beviá et al.2006] Rubén Izquierdo-Beviá,
Lorenza Moreno-Monteagudo, Borja Navarro,
and Armando Suárez. 2006. Spanish all-words
semantic class disambiguation using cast3lb corpus.
In MICAI 2006: Advances in Artificial Intelligence,
pages 879–888. Springer.

[Kulkarni et al.2009] Sayali Kulkarni, Amit Singh,
Ganesh Ramakrishnan, and Soumen Chakrabarti.
2009. Collective annotation of wikipedia entities in
web text. In Proc. of the 15th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 457–466. ACM.

[Lafferty et al.2001] John D. Lafferty, Andrew McCal-
lum, and Fernando C. N. Pereira. 2001. Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the Eighteenth International Conference on
Machine Learning, ICML ’01, pages 282–289, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.

[Lesk1986] Michael Lesk. 1986. Automatic sense dis-
ambiguation using machine readable dictionaries:
how to tell a pine cone from an ice cream cone. In
Proc. of the 5th annual international conference on
Systems documentation, pages 24–26. ACM.

[Martinez et al.2008] David Martinez, Oier Lopez
De Lacalle, and Eneko Agirre. 2008. On the use of
automatically acquired examples for all-nouns word
sense disambiguation. J. Artif. Intell. Res.(JAIR),
33:79–107.

[McCallum et al.2009] Andrew McCallum, Karl
Schultz, and Sameer Singh. 2009. FACTORIE:
Probabilistic programming via imperatively defined
factor graphs. In Neural Information Processing
Systems (NIPS).

[Mihalcea and Csomai2007] Rada Mihalcea and An-
dras Csomai. 2007. Wikify!: linking documents to
encyclopedic knowledge. In Proc. of the sixteenth
ACM conference on Conference on information and
knowledge management, pages 233–242. ACM.

[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases
and their compositionality. In Advances in Neural
Information Processing Systems, pages 3111–3119.

[Miller et al.1993] George A Miller, Claudia Leacock,
Randee Tengi, and Ross T Bunker. 1993. A se-
mantic concordance. In Proc. of the workshop on
Human Language Technology, pages 303–308. As-
sociation for Computational Linguistics.

[Milne and Witten2008] David Milne and Ian H Witten.
2008. Learning to link with wikipedia. In Proc. of
the 17th ACM conference on Information and knowl-
edge management, pages 509–518. ACM.

[Moro et al.2014] Andrea Moro, Alessandro Raganato,
and Roberto Navigli. 2014. Entity linking meets
word sense disambiguation: A unified approach.
Transactions of the Association for Computational
Linguistics, 2.

604



[Navigli and Lapata2010] Roberto Navigli and Mirella
Lapata. 2010. An experimental study of graph con-
nectivity for unsupervised word sense disambigua-
tion. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 32(4):678–692.

[Navigli and Ponzetto2012] Roberto Navigli and Si-
mone Paolo Ponzetto. 2012. Babelnet: The auto-
matic construction, evaluation and application of a
wide-coverage multilingual semantic network. Arti-
ficial Intelligence, 193:217–250.

[Navigli et al.2013] Roberto Navigli, David Jurgens,
and Daniele Vannella. 2013. Semeval-2013 task 12:
Multilingual word sense disambiguation. In Second
Joint Conference on Lexical and Computational Se-
mantics (SEM), volume 2, pages 222–231.

[Navigli2009] Roberto Navigli. 2009. Word sense dis-
ambiguation: A survey. ACM Computing Surveys
(CSUR), 41(2):10.

[Pham et al.2005] Thanh Phong Pham, Hwee Tou Ng,
and Wee Sun Lee. 2005. Word sense disambigua-
tion with semi-supervised learning. In Proc. of the
national conference on artificial intelligence, vol-
ume 20, page 1093. Menlo Park, CA; Cambridge,
MA; London; AAAI Press; MIT Press; 1999.

[Pilehvar and Navigli2014] Mohammad Taher Pilehvar
and Roberto Navigli. 2014. A large-scale
pseudoword-based evaluation framework for state-
of-the-art word sense disambiguation. Computa-
tional Linguistics, 40(4):837–881.

[Ponzetto and Navigli2010] Simone Paolo Ponzetto
and Roberto Navigli. 2010. Knowledge-rich
word sense disambiguation rivaling supervised
systems. In Proc. of the 48th annual meeting
of the association for computational linguistics,
pages 1522–1531. Association for Computational
Linguistics.

[Pradhan et al.2007] Sameer S Pradhan, Edward Loper,
Dmitriy Dligach, and Martha Palmer. 2007.
Semeval-2007 task 17: English lexical sample, srl
and all words. In Proc. of the 4th International
Workshop on Semantic Evaluations, pages 87–92.
Association for Computational Linguistics.

[Shen et al.2013] Hui Shen, Razvan Bunescu, and Rada
Mihalcea. 2013. Coarse to fine grained sense dis-
ambiguation in wikipedia. Proc. of SEM, pages 22–
31.

[Snyder and Palmer2004] Benjamin Snyder and Martha
Palmer. 2004. The english all-words task. In
Senseval-3: Third International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text, pages 41–43.

[Tsatsaronis et al.2007] George Tsatsaronis, Michalis
Vazirgiannis, and Ion Androutsopoulos. 2007.
Word sense disambiguation with spreading activa-
tion networks generated from thesauri. In IJCAI,
volume 7, pages 1725–1730.

[Zhang et al.2010] Wei Zhang, Jian Su, Chew Lim Tan,
and Wen Ting Wang. 2010. Entity linking leverag-
ing: automatically generated annotation. In Proc.
of the 23rd International Conference on Compu-
tational Linguistics, pages 1290–1298. Association
for Computational Linguistics.

[Zhong and Ng2010] Zhi Zhong and Hwee Tou Ng.
2010. It makes sense: A wide-coverage word sense
disambiguation system for free text. In Proc. of
the ACL 2010 System Demonstrations, pages 78–83.
Association for Computational Linguistics.

605


