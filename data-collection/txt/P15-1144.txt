



















































Sparse Overcomplete Word Vector Representations


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1491–1500,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Sparse Overcomplete Word Vector Representations

Manaal Faruqui Yulia Tsvetkov Dani Yogatama Chris Dyer Noah A. Smith
Language Technologies Institute

Carnegie Mellon University
Pittsburgh, PA, 15213, USA

{mfaruqui,ytsvetko,dyogatama,cdyer,nasmith}@cs.cmu.edu

Abstract

Current distributed representations of
words show little resemblance to theo-
ries of lexical semantics. The former
are dense and uninterpretable, the lat-
ter largely based on familiar, discrete
classes (e.g., supersenses) and relations
(e.g., synonymy and hypernymy). We pro-
pose methods that transform word vec-
tors into sparse (and optionally binary)
vectors. The resulting representations are
more similar to the interpretable features
typically used in NLP, though they are dis-
covered automatically from raw corpora.
Because the vectors are highly sparse, they
are computationally easy to work with.
Most importantly, we find that they out-
perform the original vectors on benchmark
tasks.

1 Introduction

Distributed representations of words have been
shown to benefit NLP tasks like parsing (Lazari-
dou et al., 2013; Bansal et al., 2014), named en-
tity recognition (Guo et al., 2014), and sentiment
analysis (Socher et al., 2013). The attraction of
word vectors is that they can be derived directly
from raw, unannotated corpora. Intrinsic evalua-
tions on various tasks are guiding methods toward
discovery of a representation that captures many
facts about lexical semantics (Turney, 2001; Tur-
ney and Pantel, 2010).

Yet word vectors do not look anything like the
representations described in most lexical seman-
tic theories, which focus on identifying classes of
words (Levin, 1993; Baker et al., 1998; Schuler,
2005) and relationships among word meanings
(Miller, 1995). Though expensive to construct,
conceptualizing word meanings symbolically is
important for theoretical understanding and also

when we incorporate lexical semantics into com-
putational models where interpretability is de-
sired. On the surface, discrete theories seem in-
commensurate with the distributed approach, a
problem now receiving much attention in compu-
tational linguistics (Lewis and Steedman, 2013;
Kiela and Clark, 2013; Vecchi et al., 2013; Grefen-
stette, 2013; Lewis and Steedman, 2014; Paperno
et al., 2014).

Our contribution to this discussion is a new,
principled sparse coding method that transforms
any distributed representation of words into sparse
vectors, which can then be transformed into binary
vectors (§2). Unlike recent approaches of incorpo-
rating semantics in distributional word vectors (Yu
and Dredze, 2014; Xu et al., 2014; Faruqui et al.,
2015), the method does not rely on any external
information source. The transformation results in
longer, sparser vectors, sometimes called an “over-
complete” representation (Olshausen and Field,
1997). Sparse, overcomplete representations have
been motivated in other domains as a way to in-
crease separability and interpretability, with each
instance (here, a word) having a small number
of active dimensions (Olshausen and Field, 1997;
Lewicki and Sejnowski, 2000), and to increase
stability in the presence of noise (Donoho et al.,
2006).

Our work builds on recent explorations of spar-
sity as a useful form of inductive bias in NLP and
machine learning more broadly (Kazama and Tsu-
jii, 2003; Goodman, 2004; Friedman et al., 2008;
Glorot et al., 2011; Yogatama and Smith, 2014,
inter alia). Introducing sparsity in word vector di-
mensions has been shown to improve dimension
interpretability (Murphy et al., 2012; Fyshe et al.,
2014) and usability of word vectors as features in
downstream tasks (Guo et al., 2014). The word
vectors we produce are more than 90% sparse; we
also consider binarizing transformations that bring
them closer to the categories and relations of lex-

1491



ical semantic theories. Using a number of state-
of-the-art word vectors as input, we find consis-
tent benefits of our method on a suite of standard
benchmark evaluation tasks (§3). We also evalu-
ate our word vectors in a word intrusion experi-
ment with humans (Chang et al., 2009) and find
that our sparse vectors are more interpretable than
the original vectors (§4).

We anticipate that sparse, binary vectors can
play an important role as features in statistical
NLP models, which still rely predominantly on
discrete, sparse features whose interpretability en-
ables error analysis and continued development.
We have made an implementation of our method
publicly available.1

2 Sparse Overcomplete Word Vectors

We consider methods for transforming dense word
vectors to sparse, binary overcomplete word vec-
tors. Fig. 1 shows two approaches. The one on the
top, method A, converts dense vectors to sparse
overcomplete vectors (§2.1). The one beneath,
method B, converts dense vectors to sparse and bi-
nary overcomplete vectors (§2.2 and §2.4).

Let V be the vocabulary size. In the following,
X ∈ RL×V is the matrix constructed by stack-
ing V non-sparse “input” word vectors of length
L (produced by an arbitrary word vector estima-
tor). We will refer to these as initializing vectors.
A ∈ RK×V contains V sparse overcomplete word
vectors of length K. “Overcomplete” representa-
tion learning implies that K > L.

2.1 Sparse Coding
In sparse coding (Lee et al., 2006), the goal is to
represent each input vector xi as a sparse linear
combination of basis vectors, ai. Our experiments
consider four initializing methods for these vec-
tors, discussed in Appendix A. Given X, we seek
to solve

arg min
D,A

‖X−DA‖22 + λΩ(A) + τ‖D‖22, (1)

where D ∈ RL×K is the dictionary of basis vec-
tors. λ is a regularization hyperparameter, and Ω is
the regularizer. Here, we use the squared loss for
the reconstruction error, but other loss functions
could also be used (Lee et al., 2009). To obtain
sparse word representations we will impose an `1

1https://github.com/mfaruqui/
sparse-coding

penalty on A. Eq. 1 can be broken down into loss
for each word vector which can be optimized sep-
arately in parallel (§2.3):

arg min
D,A

V∑
i=1

‖xi−Dai‖22 +λ‖ai‖1 +τ‖D‖22 (2)

where mi denotes the ith column vector of matrix
M. Note that this problem is not convex. We refer
to this approach as method A.

2.2 Sparse Nonnegative Vectors
Nonnegativity in the feature space has often been
shown to correspond to interpretability (Lee and
Seung, 1999; Cichocki et al., 2009; Murphy et al.,
2012; Fyshe et al., 2014; Fyshe et al., 2015). To
obtain nonnegative sparse word vectors, we use a
variation of the nonnegative sparse coding method
(Hoyer, 2002). Nonnegative sparse coding further
constrains the problem in Eq. 2 so that D and ai
are nonnegative. Here, we apply this constraint
only to the representation vectors {ai}. Thus, the
new objective for nonnegative sparse vectors be-
comes:

arg min
D∈RL×K≥0 ,A∈RK×V≥0

V∑
i=1

‖xi−Dai‖22+λ‖ai‖1+τ‖D‖22
(3)

This problem will play a role in our second ap-
proach, method B, to which we will return shortly.
This nonnegativity constraint can be easily incor-
porated during optimization, as explained next.

2.3 Optimization
We use online adaptive gradient descent (Ada-
Grad; Duchi et al., 2010) for solving the optimiza-
tion problems in Eqs. 2–3 by updating A and D.
In order to speed up training we use asynchronous
updates to the parameters of the model in parallel
for every word vector (Duchi et al., 2012; Heigold
et al., 2014).

However, directly applying stochastic subgradi-
ent descent to an `1-regularized objective fails to
produce sparse solutions in bounded time, which
has motivated several specialized algorithms that
target such objectives. We use the AdaGrad vari-
ant of one such learning algorithm, the regular-
ized dual averaging algorithm (Xiao, 2009), which
keeps track of the online average gradient at time
t: ḡt = 1t

∑t
t′=1 gt′ Here, the subgradients do not

include terms for the regularizer; they are deriva-
tives of the unregularized objective (λ = 0, τ = 0)

1492



XL

V

K

x

V

D
A K

K

x

V

D

V

KB

Sparse overcomplete vectors

Sparse, binary overcomplete 
vectors

Projection

Sparse coding

Non-negative sparse coding

Initial dense vectors

Figure 1: Methods for obtaining sparse overcomplete vectors (top, method A, §2.1) and sparse, binary
overcomplete word vectors (bottom, method B, §2.2 and §2.4). Observed dense vectors of length L (left)
are converted to sparse non-negative vectors (center) of lengthK which are then projected into the binary
vector space (right), where L � K. X is dense, A is sparse, and B is the binary word vector matrix.
Strength of colors signify the magnitude of values; negative is red, positive is blue, and zero is white.

with respect to ai. We define

γ = −sign(ḡt,i,j) ηt√
Gt,i,j

(|ḡt,i,j | − λ),

where Gt,i,j =
∑t

t′=1 g
2
t′,i,j . Now, using the av-

erage gradient, the `1-regularized objective is op-
timized as follows:

at+1,i,j =

{
0, if |ḡt,i,j | ≤ λ
γ, otherwise

(4)

where, at+1,i,j is the jth element of sparse vector
ai at the tth update and ḡt,i,j is the correspond-
ing average gradient. For obtaining nonnegative
sparse vectors we take projection of the updated ai
onto RK≥0 by choosing the closest point in RK≥0 ac-
cording to Euclidean distance (which corresponds
to zeroing out the negative elements):

at+1,i,j =


0, if |ḡt,i,j | ≤ λ
0, if γ < 0
γ, otherwise

(5)

2.4 Binarizing Transformation
Our aim with method B is to obtain word rep-
resentations that can emulate the binary-feature

X L λ τ K % Sparse
Glove 300 1.0 10−5 3000 91
SG 300 0.5 10−5 3000 92
GC 50 1.0 10−5 500 98
Multi 48 0.1 10−5 960 93

Table 1: Hyperparameters for learning sparse
overcomplete vectors tuned on the WS-353 task.
Tasks are explained in §B. The four initial vector
representations X are explained in §A.

hot, fresh, fish, 1/2, wine, salt
series, tv, appearances, episodes
1975, 1976, 1968, 1970, 1977, 1969
dress, shirt, ivory, shirts, pants
upscale, affluent, catering, clientele

Table 2: Highest frequency words in randomly
picked word clusters of binary sparse overcom-
plete Glove vectors.

space designed for various NLP tasks. We could

1493



state this as an optimization problem:

arg min
D∈RL×K

B∈{0,1}K×V

V∑
i=1

‖xi −Dbi‖22 + λ‖bi‖11 + τ‖D‖22

(6)
where B denotes the binary (and also sparse) rep-
resentation. This is an mixed integer bilinear pro-
gram, which is NP-hard (Al-Khayyal and Falk,
1983). Unfortunately, the number of variables in
the problem is ≈ KV which reaches 100 million
when V = 100, 000 and K = 1, 000, which is
intractable to solve using standard techniques.

A more tractable relaxation to this hard prob-
lem is to first constrain the continuous represen-
tation A to be nonnegative (i.e, ai ∈ RK≥0; §2.2).
Then, in order to avoid an expensive computation,
we take the nonnegative word vectors obtained us-
ing Eq. 3 and project nonzero values to 1, preserv-
ing the 0 values. Table 2 shows a random set of
word clusters obtained by (i) applying our method
to Glove initial vectors and (ii) applying k-means
clustering (k = 100). In §3 we will find that these
vectors perform well quantitatively.

2.5 Hyperparameter Tuning
Methods A and B have three hyperparameters: the
`1-regularization penalty λ, the `2-regularization
penalty τ , and the length of the overcomplete word
vector representationK. We perform a grid search
on λ ∈ {0.1, 0.5, 1.0} and K ∈ {10L, 20L}, se-
lecting values that maximizes performance on one
“development” word similarity task (WS-353, dis-
cussed in §B) while achieving at least 90% sparsity
in overcomplete vectors. τ was tuned on one col-
lection of initializing vectors (Glove, discussed in
§A) so that the vectors in D are near unit norm.
The four vector representations and their corre-
sponding hyperparameters selected by this proce-
dure are summarized in Table 1. There hyperpa-
rameters were chosen for method A and retained
for method B.

3 Experiments

Using methods A and B, we constructed sparse
overcomplete vector representations A, starting
from four initial vector representations X; these
are explained in Appendix A. We used one bench-
mark evaluation (WS-353) to tune hyperparame-
ters, resulting in the settings shown in Table 1;
seven other tasks were used to evaluate the quality
of the sparse overcomplete representations. The

first of these is a word similarity task, where the
score is correlation with human judgments, and
the others are classification accuracies of an `2-
regularized logistic regression model trained using
the word vectors. These tasks are described in de-
tail in Appendix B.

3.1 Effects of Transforming Vectors

First, we quantify the effects of our transforma-
tions by comparing their output to the initial (X)
vectors. Table 3 shows consistent improvements
of sparsifying vectors (method A). The exceptions
are on the SimLex task, where our sparse vectors
are worse than the skip-gram initializer and on par
with the multilingual initializer. Sparsification is
beneficial across all of the text classification tasks,
for all initial vector representations. On average
across all vector types and all tasks, sparse over-
complete vectors outperform their corresponding
initializers by 4.2 points.2

Binarized vectors (from method B) are also usu-
ally better than the initial vectors (also shown in
Table 3), and tend to outperform the sparsified
variants, except when initializing with Glove. On
average across all vector types and all tasks, bina-
rized overcomplete vectors outperform their cor-
responding initializers by 4.8 points and the con-
tinuous, sparse intermediate vectors by 0.6 points.

From here on, we explore more deeply the
sparse overcomplete vectors from method A (de-
noted by A), leaving binarization and method B
aside.

3.2 Effect of Vector Length

How does the length of the overcomplete vector
(K) affect performance? We focus here on the
Glove vectors, where L = 300, and report av-
erage performance across all tasks. We consider
K = αL where α ∈ {2, 3, 5, 10, 15, 20}. Figure 2
plots the average performance across tasks against
α. The earlier selection of K = 3, 000 (α = 10)
gives the best result; gains are monotonic in α to
that point and then begin to diminish.

3.3 Alternative Transformations

We consider two alternative transformations. The
first preserves the original vector length but

2We report correlation on a 100 point scale, so that the
average which includes accuracuies and correlation is equally
representatitve of both.

1494



Vectors
SimLex Senti. TREC Sports Comp. Relig. NP

Average
Corr. Acc. Acc. Acc. Acc. Acc. Acc.

Glove
X 36.9 77.7 76.2 95.9 79.7 86.7 77.9 76.2
A 38.9 81.4 81.5 96.3 87.0 88.8 82.3 79.4
B 39.7 81.0 81.2 95.7 84.6 87.4 81.6 78.7

SG
X 43.6 81.5 77.8 97.1 80.2 85.9 80.1 78.0
A 41.7 82.7 81.2 98.2 84.5 86.5 81.6 79.4
B 42.8 81.6 81.6 95.2 86.5 88.0 82.9 79.8

GC
X 9.7 68.3 64.6 75.1 60.5 76.0 79.4 61.9
A 12.0 73.3 77.6 77.0 68.3 81.0 81.2 67.2
B 18.7 73.6 79.2 79.7 70.5 79.6 79.4 68.6

Multi
X 28.7 75.5 63.8 83.6 64.3 81.8 79.2 68.1
A 28.1 78.6 79.2 93.9 78.2 84.5 81.1 74.8
B 28.7 77.6 82.0 94.7 81.4 85.6 81.9 75.9

Table 3: Performance comparison of transformed vectors to initial vectors X. We show sparse over-
complete representations A and also binarized representations B. Initial vectors are discussed in §A and
tasks in §B.

Figure 2: Average performace across all tasks
for sparse overcomplete vectors (A) produced by
Glove initial vectors, as a function of the ratio of
K to L.

achieves a binary, sparse vector (B) by applying:

bi,j =
{

1 if xi,j > 0
0 otherwise

(7)

The second transformation was proposed by
Guo et al. (2014). Here, the original vector length
is also preserved, but sparsity is achieved through:

ai,j =


1 if xi,j ≥M+
−1 if xi,j ≤M−

0 otherwise
(8)

where M+ (M−) is the mean of positive-valued
(negative-valued) elements of X. These vectors
are, obviously, not binary.

We find that on average, across initializing vec-
tors and across all tasks that our sparse overcom-
plete (A) vectors lead to better performance than
either of the alternative transformations.

4 Interpretability

Our hypothesis is that the dimensions of sparse
overcomplete vectors are more interpretable than
those of dense word vectors. Following Murphy
et al. (2012), we use a word intrusion experiment
(Chang et al., 2009) to corroborate this hypothesis.
In addition, we conduct qualitative analysis of in-
terpretability, focusing on individual dimensions.

4.1 Word Intrusion

Word intrusion experiments seek to quantify the
extent to which dimensions of a learned word rep-
resentation are coherent to humans. In one in-
stance of the experiment, a human judge is pre-
sented with five words in random order and asked
to select the “intruder.” The words are selected by
the experimenter by choosing one dimension j of
the learned representation, then ranking the words
on that dimension alone. The dimensions are cho-
sen in decreasing order of the variance of their
values across the vocabulary. Four of the words
are the top-ranked words according to j, and the
“true” intruder is a word from the bottom half of
the list, chosen to be a word that appears in the top
10% of some other dimension. An example of an
instance is:

naval, industrial, technological, marine, identity

1495



X: Glove SG GC Multi Average
X 76.2 78.0 61.9 68.1 71.0
Eq. 7 75.7 75.8 60.5 64.1 69.0
Eq. 8 (Guo et al., 2014) 75.8 76.9 60.5 66.2 69.8
A 79.4 79.4 67.2 74.8 75.2

Table 4: Average performance across all tasks and vector models using different transformations.

Vectors A1 A2 A3 Avg. IAA κ
X 61 53 56 57 70 0.40
A 71 70 72 71 77 0.45

Table 5: Accuracy of three human annotators on
the word intrusion task, along with the average
inter-annotator agreement (Artstein and Poesio,
2008) and Fleiss’ κ (Davies and Fleiss, 1982).

(The last word is the intruder.)
We formed instances from initializing vectors

and from our sparse overcomplete vectors (A).
Each of these two combines the four different ini-
tializers X. We selected the 25 dimensions d in
each case. Each of the 100 instances per condition
(initial vs. sparse overcomplete) was given to three
judges.

Results in Table 5 confirm that the sparse over-
complete vectors are more interpretable than the
dense vectors. The inter-annotator agreement on
the sparse vectors increases substantially, from
57% to 71%, and the Fleiss’ κ increases from
“fair” to “moderate” agreement (Landis and Koch,
1977).

4.2 Qualitative Evaluation of Interpretability

If a vector dimension is interpretable, the top-
ranking words for that dimension should display
semantic or syntactic groupings. To verify this
qualitatively, we select five dimensions with the
highest variance of values in initial and sparsi-
fied GC vectors. We compare top-ranked words in
the dimensions extracted from the two representa-
tions. The words are listed in Table 6, a dimension
per row. Subjectively, we find the semantic group-
ings better in the sparse vectors than in the initial
vectors.

Figure 3 visualizes the sparsified GC vectors for
six words. The dimensions are sorted by the aver-
age value across the three “animal” vectors. The
animal-related words use many of the same di-
mensions (102 common active dimensions out of
500 total); in constrast, the three city names use

X

combat, guard, honor, bow, trim, naval
’ll, could, faced, lacking, seriously, scored
see, n’t, recommended, depending, part
due, positive, equal, focus, respect, better
sergeant, comments, critics, she, videos

A

fracture, breathing, wound, tissue, relief
relationships, connections, identity, relations
files, bills, titles, collections, poems, songs
naval, industrial, technological, marine
stadium, belt, championship, toll, ride, coach

Table 6: Top-ranked words per dimension for ini-
tial and sparsified GC representations. Each line
shows words from a different dimension.

mostly distinct vectors.

5 Related Work

To the best of our knowledge, there has been no
prior work on obtaining overcomplete word vec-
tor representations that are sparse and categorical.
However, overcomplete features have been widely
used in image processing, computer vision (Ol-
shausen and Field, 1997; Lewicki and Sejnowski,
2000) and signal processing (Donoho et al., 2006).
Nonnegative matrix factorization is often used for
interpretable coding of information (Lee and Se-
ung, 1999; Liu et al., 2003; Cichocki et al., 2009).

Sparsity constraints are in general useful in NLP
problems (Kazama and Tsujii, 2003; Friedman
et al., 2008; Goodman, 2004), like POS tagging
(Ganchev et al., 2009), dependency parsing (Mar-
tins et al., 2011), text classification (Yogatama and
Smith, 2014), and representation learning (Ben-
gio et al., 2013). Including sparsity constraints
in Bayesian models of lexical semantics like LDA
in the form of sparse Dirichlet priors has been
shown to be useful for downstream tasks like POS-
tagging (Toutanova and Johnson, 2007), and im-
proving interpretation (Paul and Dredze, 2012;
Zhu and Xing, 2012).

1496



V
37
9

V
35
3

V
76

V
18
6

V
33
9

V
17
7

V
11
4

V
34
2

V
33
2

V
27
0

V
22
2

V
91

V
30
3

V
47
3

V
35
5

V
35
8

V
16
4

V
34
8

V
32
4

V
19
2

V
24

V
28
1

V
82
V
46

V
27
7

V
46
6

V
46
5

V
12
8

V
11

V
41
3

V
98

V
13
1

V
44
5

V
19
9

V
47
5

V
20
8

V
43
1

V
29
9

V
35
7

V
14
9

V
80

V
24
7

V
23
1

V
42
V
44

V
37
6

V
15
2

V
74

V
25
4

V
14
1

V
34
1

V
34
9

V
23
4

V
55

V
47
7

V
27
2

V
21
7

V
45
7

V
57

V
15
9

V
22
3

V
31
0

V
43
6

V
32
5

V
21
1

V
11
7

V
36
0

V
48
3

V
36
3

V
43
9

V
40
3

V
11
9

V
32
9

V
83

V
37
1

V
42
4

V
17
9

V
21
4

V
26
8

V
38

V
10
2

V
93
V
89
V
12

V
17
2

V
17
3

V
28
5

V
34
4

V
78

V
22
7

V
42
6

V
43
0

V
24
1

V
38
4

V
46
0

V
34
7

V
17
1

V
28
9

V
38
0
V
8
V
2
V
3
V
5
V
6
V
7

V
10
V
14
V
15
V
16
V
17
V
18
V
19
V
20
V
21
V
22
V
25
V
26
V
28
V
29
V
30
V
31
V
32
V
33
V
35
V
36
V
37
V
39
V
40
V
41
V
43
V
45
V
47
V
49
V
50
V
51
V
52
V
54
V
56
V
58
V
59
V
60
V
63
V
64
V
65
V
67
V
68
V
69
V
70
V
72
V
75
V
77
V
81
V
87
V
90
V
92
V
94
V
99

V
10
1

V
10
3

V
10
5

V
10
6

V
10
8

V
11
0

V
11
1

V
11
6

V
11
8

V
12
2

V
12
3

V
12
5

V
13
0

V
13
2

V
13
3

V
13
6

V
13
7

V
13
8

V
13
9

V
14
0

V
14
3

V
14
4

V
14
7

V
14
8

V
15
0

V
15
5

V
15
8

V
16
0

V
16
2

V
16
5

V
16
6

V
16
7

V
16
8

V
16
9

V
17
0

V
17
4

V
17
5

V
17
8

V
18
0

V
18
1

V
18
2

V
18
3

V
18
5

V
18
8

V
18
9

V
19
0

V
19
1

V
19
3

V
19
4

V
19
5

V
19
6

V
20
2

V
20
3

V
20
4

V
20
5

V
21
2

V
21
3

V
21
5

V
21
8

V
22
0

V
22
4

V
22
6

V
22
8

V
23
2

V
23
3

V
23
5

V
23
6

V
23
8

V
23
9

V
24
0

V
24
2

V
24
3

V
24
4

V
24
8

V
24
9

V
25
0

V
25
1

V
25
2

V
25
3

V
25
5

V
25
8

V
25
9

V
26
0

V
26
1

V
26
2

V
26
3

V
26
4

V
26
5

V
26
6

V
27
1

V
27
3

V
27
4

V
27
8

V
28
2

V
28
4

V
28
7

V
28
8

V
29
0

V
29
2

V
29
3

V
29
4

V
29
6

V
30
0

V
30
2

V
30
4

V
30
7

V
30
8

V
31
1

V
31
2

V
31
3

V
31
4

V
31
6

V
31
7

V
31
8

V
31
9

V
32
0

V
32
1

V
32
2

V
32
3

V
32
7

V
33
0

V
33
1

V
33
3

V
33
4

V
33
6

V
33
8

V
34
0

V
34
3

V
34
5

V
34
6

V
35
2

V
35
6

V
36
1

V
36
2

V
36
6

V
36
8

V
36
9

V
37
0

V
37
2

V
37
3

V
37
5

V
37
7

V
37
8

V
38
1

V
38
2

V
38
3

V
38
5

V
38
6

V
38
7

V
38
8

V
38
9

V
39
0

V
39
1

V
39
2

V
39
4

V
39
5

V
39
6

V
39
8

V
39
9

V
40
0

V
40
1

V
40
2

V
40
4

V
40
5

V
40
6

V
40
7

V
40
8

V
40
9

V
41
0

V
41
2

V
41
4

V
41
5

V
41
6

V
41
7

V
41
8

V
41
9

V
42
0

V
42
2

V
42
3

V
42
5

V
42
7

V
42
8

V
42
9

V
43
3

V
43
4

V
43
5

V
43
7

V
44
1

V
44
2

V
44
4

V
44
6

V
44
9

V
45
0

V
45
1

V
45
2

V
45
3

V
45
5

V
45
6

V
45
8

V
45
9

V
46
1

V
46
2

V
46
3

V
46
4

V
46
7

V
46
8

V
46
9

V
47
1

V
47
2

V
47
8

V
47
9

V
48
0

V
48
1

V
48
2

V
48
4

V
48
5

V
48
6

V
48
8

V
48
9

V
49
0

V
49
1

V
49
2

V
49
3

V
49
4

V
49
5

V
49
7

V
49
9

V
50
0

V
50
1

V
48
7

V
20
0

V
32
6
V
4

V
12
1

V
26
7

V
23
0

V
43
8

V
13
4

V
97

V
10
4

V
35
1

V
21
9

V
13
V
88

V
12
9

V
28
6

V
22
9

V
35
0

V
96

V
10
7

V
15
3

V
14
5

V
15
4

V
34

V
30
1

V
37
4

V
10
9

V
39
7

V
15
6

V
16
1

V
29
7

V
11
5

V
15
1

V
24
5

V
44
7

V
53

V
33
7

V
79

V
44
8

V
28
3

V
44
3

V
20
1

V
39
3

V
36
5

V
48

V
12
6

V
25
7

V
24
6

V
29
5

V
12
0

V
36
7

V
27

V
18
4

V
20
9

V
30
6

V
26
9

V
12
4

V
47
0

V
11
2

V
18
7

V
62

V
47
4

V
35
4

V
45
4

V
27
9

V
14
6

V
27
5

V
22
1

V
20
7

V
71

V
33
5

V
73
V
85

V
44
0

V
95
V
23

V
22
5

V
41
1

V
32
8

V
30
5

V
19
8

V
16
3
V
9

V
13
5

V
31
5

V
14
2

V
49
8

V
29
1

V
86

V
47
6

V
21
0

V
35
9

V
84

V
10
0

V
30
9

V
17
6

V
21
6

V
43
2

V
20
6

V
42
1

V
27
6

V
23
7

V
61

V
15
7

V
36
4

V
12
7

V
66

V
25
6

V
28
0

V
11
3

V
29
8

V
19
7

V
49
6

boston
seattle
chicago
dog
horse
fish

Figure 3: Visualization of sparsified GC vectors. Negative values are red, positive values are blue, zeroes
are white.

6 Conclusion

We have presented a method that converts word
vectors obtained using any state-of-the-art word
vector model into sparse and optionally binary
word vectors. These transformed vectors appear to
come closer to features used in NLP tasks and out-
perform the original vectors from which they are
derived on a suite of semantics and syntactic eval-
uation benchmarks. We also find that the sparse
vectors are more interpretable than the dense vec-
tors by humans according to a word intrusion de-
tection test.

Acknowledgments

We thank Alona Fyshe for discussions on vec-
tor interpretability and three anonymous review-
ers for their feedback. This research was sup-
ported in part by the National Science Foundation
through grant IIS-1251131 and the Defense Ad-
vanced Research Projects Agency through grant
FA87501420244. This work was supported in part
by the U.S. Army Research Laboratory and the
U.S. Army Research Office under contract/grant
number W911NF-10-1-0533.

A Initial Vector Representations (X)

Our experiments consider four publicly available
collections of pre-trained word vectors. They vary
in the amount of data used and the estimation
method.

Glove. Global vectors for word representations
(Pennington et al., 2014) are trained on aggregated
global word-word co-occurrence statistics from a
corpus. These vectors were trained on 6 billion
words from Wikipedia and English Gigaword and
are of length 300.3

3http://www-nlp.stanford.edu/projects/
glove/

Skip-Gram (SG). The word2vec tool (Mikolov
et al., 2013) is fast and widely-used. In this model,
each word’s Huffman code is used as an input to
a log-linear classifier with a continuous projection
layer and words within a given context window are
predicted. These vectors were trained on 100 bil-
lion words of Google news data and are of length
300.4

Global Context (GC). These vectors are
learned using a recursive neural network that
incorporates both local and global (document-
level) context features (Huang et al., 2012). These
vectors were trained on the first 1 billion words of
English Wikipedia and are of length 50.5

Multilingual (Multi). Faruqui and Dyer (2014)
learned vectors by first performing SVD on text
in different languages, then applying canonical
correlation analysis on pairs of vectors for words
that align in parallel corpora. These vectors were
trained on WMT-2011 news corpus containing
360 million words and are of length 48.6

B Evaluation Benchmarks

Our comparisons of word vector quality consider
five benchmark tasks. We now describe the differ-
ent evaluation benchmarks for word vectors.

Word Similarity. We evaluate our word repre-
sentations on two word similarity tasks. The first
is the WS-353 dataset (Finkelstein et al., 2001),
which contains 353 pairs of English words that
have been assigned similarity ratings by humans.
This dataset is used to tune sparse vector learning
hyperparameters (§2.5), while the remaining of the
tasks discussed in this section are completely held
out.

4https://code.google.com/p/word2vec
5http://nlp.stanford.edu/˜socherr/

ACL2012_wordVectorsTextFile.zip
6http://cs.cmu.edu/˜mfaruqui/soft.html

1497



A more recent dataset, SimLex-999 (Hill et al.,
2014), has been constructed to specifically focus
on similarity (rather than relatedness). It con-
tains a balanced set of noun, verb, and adjective
pairs. We calculate cosine similarity between the
vectors of two words forming a test item and re-
port Spearman’s rank correlation coefficient (My-
ers and Well, 1995) between the rankings pro-
duced by our model against the human rankings.

Sentiment Analysis (Senti). Socher et al.
(2013) created a treebank of sentences anno-
tated with fine-grained sentiment labels on phrases
and sentences from movie review excerpts. The
coarse-grained treebank of positive and negative
classes has been split into training, development,
and test datasets containing 6,920, 872, and 1,821
sentences, respectively. We use average of the
word vectors of a given sentence as feature for
classification. The classifier is tuned on the
dev. set and accuracy is reported on the test set.

Question Classification (TREC). As an aid to
question answering, a question may be classi-
fied as belonging to one of many question types.
The TREC questions dataset involves six differ-
ent question types, e.g., whether the question is
about a location, about a person, or about some nu-
meric information (Li and Roth, 2002). The train-
ing dataset consists of 5,452 labeled questions, and
the test dataset consists of 500 questions. An av-
erage of the word vectors of the input question is
used as features and accuracy is reported on the
test set.

20 Newsgroup Dataset. We consider three bi-
nary categorization tasks from the 20 News-
groups dataset.7 Each task involves categoriz-
ing a document according to two related cate-
gories with training/dev./test split in accordance
with Yogatama and Smith (2014): (1) Sports:
baseball vs. hockey (958/239/796) (2) Comp.:
IBM vs. Mac (929/239/777) (3) Religion: atheism
vs. christian (870/209/717). We use average of the
word vectors of a given sentence as features. The
classifier is tuned on the dev. set and accuracy is
reported on the test set.

NP bracketing (NP). Lazaridou et al. (2013)
constructed a dataset from the Penn Treebank
(Marcus et al., 1993) of noun phrases (NP) of

7http://qwone.com/˜jason/20Newsgroups

length three words, where the first can be an ad-
jective or a noun and the other two are nouns. The
task is to predict the correct bracketing in the parse
tree for a given noun phrase. For example, local
(phone company) and (blood pressure) medicine
exhibit right and left bracketing, respectively. We
append the word vectors of the three words in the
NP in order and use them as features for binary
classification. The dataset contains 2,227 noun
phrases split into 10 folds. The classifier is tuned
on the first fold and cross-validation accuracy is
reported on the remaining nine folds.

References
Faiz A. Al-Khayyal and James E. Falk. 1983. Jointly

constrained biconvex programming. Mathematics of
Operations Research, pages 273–286.

Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.

Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc. of
ACL.

Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proc. of ACL.

Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 35(8):1798–1828.

Jonathan Chang, Sean Gerrish, Chong Wang, Jordan L.
Boyd-Graber, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
NIPS.

Andrzej Cichocki, Rafal Zdunek, Anh Huy Phan, and
Shun-ichi Amari. 2009. Nonnegative Matrix and
Tensor Factorizations: Applications to Exploratory
Multi-way Data Analysis and Blind Source Separa-
tion. John Wiley & Sons.

Mark Davies and Joseph L Fleiss. 1982. Measuring
agreement for multinomial data. Biometrics, pages
1047–1051.

David L. Donoho, Michael Elad, and Vladimir N.
Temlyakov. 2006. Stable recovery of sparse over-
complete representations in the presence of noise.
IEEE Transactions on Information Theory, 52(1).

John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive subgradient methods for online learn-
ing and stochastic optimization. Technical Report
EECS-2010-24, University of California Berkeley.

1498



John C. Duchi, Alekh Agarwal, and Martin J. Wain-
wright. 2012. Dual averaging for distributed opti-
mization: Convergence analysis and network scal-
ing. IEEE Transactions on Automatic Control,
57(3):592–606.

Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proc. of EACL.

Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris
Dyer, Eduard Hovy, and Noah A. Smith. 2015.
Retrofitting word vectors to semantic lexicons. In
Proc. of NAACL.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In Proc. of WWW.

Jerome Friedman, Trevor Hastie, and Robert Tibshi-
rani. 2008. Sparse inverse covariance estimation
with the graphical lasso. Biostatistics, 9(3):432–
441.

Alona Fyshe, Partha P. Talukdar, Brian Murphy, and
Tom M. Mitchell. 2014. Interpretable semantic vec-
tors from a joint model of brain- and text- based
meaning. In Proc. of ACL.

Alona Fyshe, Leila Wehbe, Partha P. Talukdar, Brian
Murphy, and Tom M. Mitchell. 2015. A composi-
tional and interpretable semantic space. In Proc. of
NAACL.

Kuzman Ganchev, Ben Taskar, Fernando Pereira, and
João Gama. 2009. Posterior vs. parameter sparsity
in latent variable models. In NIPS.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proc. of
ICML.

Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In Proc. of NAACL.

E. Grefenstette. 2013. Towards a formal distributional
semantics: Simulating logical calculi with tensors.
arXiv:1304.5823.

Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Revisiting embedding features for sim-
ple semi-supervised learning. In Proc. of EMNLP.

Georg Heigold, Erik McDermott, Vincent Vanhoucke,
Andrew Senior, and Michiel Bacchiani. 2014.
Asynchronous stochastic optimization for sequence
training of deep neural networks. In Proc. of
ICASSP.

Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. CoRR, abs/1408.3456.

Patrik O. Hoyer. 2002. Non-negative sparse coding. In
Neural Networks for Signal Processing, 2002. Proc.
of IEEE Workshop on.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proc. of ACL.

Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proc. of EMNLP.

Douwe Kiela and Stephen Clark. 2013. Detecting
compositionality of multi-word expressions using
nearest neighbours in vector space models. In Proc.
of EMNLP.

J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159–174.

Angeliki Lazaridou, Eva Maria Vecchi, and Marco
Baroni. 2013. Fish transporters and miracle
homes: How compositional distributional semantics
can help NP parsing. In Proc. of EMNLP.

Daniel D. Lee and H. Sebastian Seung. 1999. Learning
the parts of objects by non-negative matrix factoriza-
tion. Nature, 401(6755):788–791.

Honglak Lee, Alexis Battle, Rajat Raina, and An-
drew Y. Ng. 2006. Efficient sparse coding algo-
rithms. In NIPS.

Honglak Lee, Rajat Raina, Alex Teichman, and An-
drew Y. Ng. 2009. Exponential family sparse cod-
ing with application to self-taught learning. In Proc.
of IJCAI.

Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.

Michael Lewicki and Terrence Sejnowski. 2000.
Learning overcomplete representations. Neural
Computation, 12(2):337–365.

Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the ACL, 1:179–192.

Mike Lewis and Mark Steedman. 2014. Combining
formal and distributional models of temporal and in-
tensional semantics. In Proc. of ACL.

Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proc. of COLING.

Weixiang Liu, Nanning Zheng, and Xiaofeng Lu.
2003. Non-negative matrix factorization for visual
coding. In Proc. of ICASSP.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19(2):313–330.

1499



André F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and Mário A. T. Figueiredo. 2011. Struc-
tured sparsity in structured prediction. In Proc. of
EMNLP.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space.

George A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39–41.

Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012. Learning effective and interpretable seman-
tic models using non-negative sparse embedding. In
Proc. of COLING.

Jerome L. Myers and Arnold D. Well. 1995. Research
Design & Statistical Analysis. Routledge.

Bruno A. Olshausen and David J. Field. 1997. Sparse
coding with an overcomplete basis set: A strategy
employed by v1? Vision Research, 37(23):3311 –
3325.

Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics. In
Proc. of ACL.

Michael Paul and Mark Dredze. 2012. Factorial LDA:
Sparse multi-dimensional text models. In NIPS.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proc. of EMNLP.

Karin Kipper Schuler. 2005. Verbnet: A Broad-
coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proc. of EMNLP.

Kristina Toutanova and Mark Johnson. 2007. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In NIPS.

Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning : Vector space models of seman-
tics. JAIR, 37(1):141–188.

Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proc. of ECML.

Eva Maria Vecchi, Roberto Zamparelli, and Marco Ba-
roni. 2013. Studying the recursive behaviour of
adjectival modification with compositional distribu-
tional semantics. In Proc. of EMNLP.

Lin Xiao. 2009. Dual averaging methods for regular-
ized stochastic learning and online optimization. In
NIPS.

Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang
Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. Rc-
net: A general framework for incorporating knowl-
edge into word representations. In Proc. of CIKM.

Dani Yogatama and Noah A Smith. 2014. Linguistic
structured sparsity in text categorization. In Proc. of
ACL.

Mo Yu and Mark Dredze. 2014. Improving lexical
embeddings with semantic knowledge. In Proc. of
ACL.

Jun Zhu and Eric P Xing. 2012. Sparse topical coding.
arXiv:1202.3778.

1500


