



















































Adaptive Knowledge Sharing in Multi-Task Learning: Improving Low-Resource Neural Machine Translation


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 656–661
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

656

Adaptive Knowledge Sharing in Multi-Task Learning: Improving
Low-Resource Neural Machine Translation

Poorya Zaremoodi Wray Buntine Gholamreza Haffari
Faculty of Information Technology, Monash University, Australia

first.last@monash.edu

Abstract

Neural Machine Translation (NMT) is no-
torious for its need for large amounts of
bilingual data. An effective approach to
compensate for this requirement is Multi-
Task Learning (MTL) to leverage differ-
ent linguistic resources as a source of
inductive bias. Current MTL architec-
tures are based on the SEQ2SEQ transduc-
tion, and (partially) share different com-
ponents of the models among the tasks.
However, this MTL approach often suffers
from task interference, and is not able to
fully capture commonalities among sub-
sets of tasks. We address this issue by ex-
tending the recurrent units with multiple
blocks along with a trainable routing net-
work. The routing network enables adap-
tive collaboration by dynamic sharing of
blocks conditioned on the task at hand, in-
put, and model state. Empirical evaluation
of two low-resource translation tasks, En-
glish to Vietnamese and Farsi, show +1
BLEU score improvements compared to
strong baselines.

1 Introduction

Neural Machine Translation (NMT) has shown re-
markable progress in recent years. However, it re-
quires large amounts of bilingual data to learn a
translation model with reasonable quality (Koehn
and Knowles, 2017). This requirement can be
compensated by leveraging curated monolingual
linguistic resources in a multi-task learning frame-
work. Essentially, learned knowledge from auxil-
iary linguistic tasks serves as inductive bias for the
translation task to lead to better generalizations.

Multi-Task Learning (MTL) is an effective ap-
proach for leveraging commonalities of related

tasks to improve performance. Various recent
works have attempted to improve NMT by scaf-
folding translation task on a single auxiliary task
(Domhan and Hieber, 2017; Zhang and Zong,
2016; Dalvi et al., 2017). Recently, (Niehues
and Cho, 2017) have made use of several linguis-
tic tasks to improve NMT. Their method shares
components of the SEQ2SEQ model among the
tasks, e.g. encoder, decoder or the attention mech-
anism. However, this approach has two limita-
tions: (i) it fully shares the components, and (ii) the
shared component(s) are shared among all of the
tasks. The first limitation can be addressed using
deep stacked layers in encoder/decoder, and shar-
ing the layers partially (Zaremoodi and Haffari,
2018). The second limitation causes this MTL ap-
proach to suffer from task interference or inabil-
ity to leverages commonalities among a subset of
tasks. Recently, (Ruder et al., 2017) tried to ad-
dress this issue; however, their method is restric-
tive for SEQ2SEQ scenarios and does not consider
the input at each time step to modulate parameter
sharing.

In this paper, we address the task interference
problem by learning how to dynamically control
the amount of sharing among all tasks. We ex-
tended the recurrent units with multiple blocks
along with a routing network to dynamically con-
trol sharing of blocks conditioning on the task at
hand, the input, and model state. Empirical results
on two low-resource translation scenarios, English
to Farsi and Vietnamese, show the effectiveness of
the proposed model by achieving +1 BLEU score
improvement compared to strong baselines.

2 SEQ2SEQ MTL Using Recurrent Unit
with Adaptive Routed Blocks

Our MTL is based on the sequential encoder-
decoder architecture with the attention mecha-



657

nism (Luong et al., 2015b; Bahdanau et al., 2014).
The encoder/decoder consist of recurrent units to
read/generate a sentence sequentially. Sharing the
parameters of the recurrent units among differ-
ent tasks is indeed sharing the knowledge for con-
trolling the information flow in the hidden states.
Sharing these parameters among all tasks may,
however, lead to task interference or inability to
leverages commonalities among subsets of tasks.
We address this issue by extending the recurrent
units with multiple blocks, each of which process-
ing its own information flow through the time. The
state of the recurrent unit at each time step is com-
posed of the states of these blocks. The recur-
rent unit is equipped with a routing mechanism to
softly direct the input at each time step to these
blocks (see Fig 1). Each block mimics an expert
in handling different kinds of information, coor-
dinated by the router. In MTL, the tasks can use
different subsets of these shared experts.

(Rosenbaum et al., 2018) uses a routing net-
work for adaptive selection of non-linear func-
tions for MTL. However, it is for fixed-size in-
puts based on a feed-forward architecture, and is
not applicable to SEQ2SEQ scenarios such as MT.
(Shazeer et al., 2017) uses Mixture-of-Experts
(feed-forward sub-networks) between stacked lay-
ers of recurrent units, to adaptively gate state in-
formation vertically. This is in contrast to our ap-
proach where the horizontal information flow is
adaptively modulated, as we would like to min-
imise the task interference in MTL.

Assuming there are n blocks in a recurrent unit,
we share n− 1 blocks among the tasks, and let the
last one to be task-specific1. Task-specific block
receives the input of the unit directly while shared
blocks are fed with modulated input by the routing
network. The state of the unit at each time-step
would be the aggregation of blocks’ states.

2.1 Routing Mechanism
At each time step, the routing network is responsi-
ble to softly forward the input to the shared blocks
conditioning on the input xt, and the previous hid-
den state of the unit ht−1 as follows:

st = tanh(Wx · xt +Wh · ht−1 + bs),
τt = softmax(Wτ · st + bτ ),

where W ’s and b’s are the parameters. Then,
the i-th shared block is fed with the input of the

1multiple recurrent units can be stacked on top of each
other to consist a multi-layer component

 Block 1

 Block 2

 Block 3

 Block 4

Routing 
Network

h(2)

h(1)

h(3)

ht-1

xt

ht

h(4)

h(2)

h(1)

h(3)

h(4)

t

t

t

t

t-1

t-1

t-1

t-1

Figure 1: High-level architecture of the proposed
recurrent unit with 3 shared blocks and 1 task-
specific.

unit modulated by the corresponding output of the
routing network x̃(i)t = τt[i]xt where τt[i] is the
scalar output of the routing network for the i-th
block.

The hidden state of the unit is the concatena-
tion of the hidden state of the shared and task-
specific parts ht = [h

(shared)
t ;h

(task)
t ]. The state

of task-specific part is the state of the correspond-
ing block h(task)t = h

(n+1)
t , and the state of the

shared part is the sum of states of shared blocks
weighted by the outputs of the routing network
h
(shared)
t =

∑n
i=1 τt[i]h

(i)
t .

2.2 Block Architecture

Each block is responsible to control its own flow of
information via a standard gating mechanism. Our
recurrent units are agnostic to the internal architec-
ture of the blocks; we use the gated-recurrent unit
(Cho et al., 2014) in this paper. For the i-th block
the corresponding equations are as follows:

z
(i)
t = σ(W

(i)
z x̃

(i)
t +U

(i)
z h

(i)
t−1 + b

(i)
z ),

r
(i)
t = σ(W

(i)
r x̃

(i)
t +U

(i)
r h

(i)
t−1 + b

(i)
r ),

h̃
(i)
t = tanh(W

(i)
h x̃

(i)
t +U

(i)
h h

(i)
t−1 + b

(i)
h ),

h
(i)
t = z

(i)
t � h

(i)
t−1 + (1− z

(i)
t )� h̃

(i)
t .

2.3 Training Objective and Schedule.

The rest of the model is similar to attentional
SEQ2SEQ model (Luong et al., 2015b) which
computes the conditional probability of the tar-
get sequence given the source Pθθθ(y|x) =∏
j Pθθθ(yj |y<jx). For the case of training M + 1

SEQ2SEQ transduction tasks, each of which is as-
sociated with a training set Dm := {(xi,yi)}Nmi=1,
the parameters of MTL architecture Θmtl =



658

{Θm}Mm=0 are learned by maximizing the follow-
ing objective:

Lmtl(Θmtl) :=
M∑
m=0

γm
|Dm|

∑
(x,y)∈Dm

logPΘm(y|x)

where |Dm| is the size of the training set for them-
th task, and γm is responsible to balance the influ-
ence of tasks in the training objective. We explored
different values in preliminary experiments, and
found that for our training schedule γ = 1 for all
tasks results in the best performance. Generally, γ
is useful when the dataset sizes for auxiliary tasks
are imbalanced (our training schedule handles the
main task).

Variants of stochastic gradient descent (SGD)
can be used to optimize the objective function.
In our training schedule, we randomly select a
mini-batch from the main task (translation) and
another mini-batch from a randomly selected aux-
iliary task to make the next SGD update. Selecting
a mini-batch from the main task in each SGD up-
date ensures that its training signals are not washed
out by auxiliary tasks.

3 Experiments

3.1 Bilingual Corpora
We use two language-pairs, translating from En-
glish to Farsi and Vietnamese. We have chosen
them to analyze the effect of multi-task learning
on languages with different underlying linguistic
structures2. We apply BPE (Sennrich et al., 2016)
on the union of source and target vocabularies for
English-Vietnamese, and separate vocabularies for
English-Farsi as the alphabets are disjoined (30K
BPE operations). Further details about the corpora
and their pre-processing is as follows:

• The English-Farsi corpus has ∼105K sentence
pairs. It is assembled from English-Farsi paral-
lel subtitles from the TED corpus (Tiedemann,
2012), accompanied by all the parallel news
text in LDC2016E93 Farsi Representative Lan-
guage Pack from the Linguistic Data Consor-
tium. The corpus has been normalized using
the Hazm toolkit3. We have removed sentences
with more than 80 tokens in either side (before
applying BPE). 3k and 4k sentence pairs were
held out for the purpose of validation and test.

2English and Vietnamese are SVO, and Farsi is SOV.
3www.sobhe.ir/hazm

• The English-Vietnamese has ∼133K training
pairs. It is the preprocessed version of the
IWSLT 2015 translation task provided by (Lu-
ong and Manning, 2015). It consists of sub-
titles and their corresponding translations of a
collection of public speeches from TED and
TEDX talks. The “tst2012” and “tst2013” parts
are used as validation and test sets, respec-
tively. We have removed sentence pairs which
had more than 300 tokens after applying BPE
on either sides.

3.2 Auxiliary Tasks

We have chosen the following auxiliary tasks to
leverage the syntactic and semantic knowledge to
improve NMT:

Named-Entity Recognition (NER). It is ex-
pected that learning to recognize named-entities
help the model to learn translation pattern by
masking out named-entites. We have used the
NER data comes from the CONLL shared task.4

Sentences in this dataset come from a collection
of newswire articles from the Reuters Corpus.
These sentences are annotated with four types of
named entities: persons, locations, organizations
and names of miscellaneous entities.

Syntactic Parsing. By learning the phrase struc-
ture of the input sentence, the model would be able
to learn better re-ordering. Specially, in the case of
language pairs with high level of syntactic diver-
gence (e.g. English-Farsi). We have used Penn
Tree Bank parsing data with the standard split
for training, development, and test (Marcus et al.,
1993). We cast syntactic parsing to a SEQ2SEQ
transduction task by linearizing constituency trees
(Vinyals et al., 2015).

Semantic Parsing. Learning semantic parsing
helps the model to abstract away the meaning from
the surface in order to convey it in the target trans-
lation. For this task, we have used the Abstract
Meaning Representation (AMR) corpus Release
2.0 (LDC2017T10)5. This corpus contains natu-
ral language sentences from newswire, weblogs,
web discussion forums and broadcast conversa-
tions. We cast this task to a SEQ2SEQ transduc-
tion task by linearizing the AMR graphs (Konstas
et al., 2017).

4https://www.clips.uantwerpen.be/conll2003/ner
5https://catalog.ldc.upenn.edu/LDC2017T10



659

English → Farsi English → Vietnamese
Dev Test Dev Test

Method PPL TER BLEU PPL TER BLEU PPL TER BLEU PPL TER BLEU
NMT 55.36 87.9 8.57 56.21 88.2 8.35 18.21 64.92 18.39 16.3 61.37 20.18

(Luong et al., 2015b)
MTL (Full) 47.43 85.92 8.97 48.23 87.3 8.73 14.56 61.52 20.55 12.5 57.6 22.6

(Niehues and Cho, 2017)
MTL (Partial) 42.6 80.16 10.58 43.09 81.94 10.54 13.32 59.55 22.2 11.34 55.84 24.65

(Zaremoodi and Haffari, 2018)
Our 37.95 76.30 12.06 38.57 78.18 11.95 12.38 58.52 23.06 10.52 54.33 25.65MTL (Routing)

Table 1: The performance measures of the baselines vs our MTL architecture on the bilingual datasets.

3.3 Models and Baselines
We have implemented the proposed MTL archi-
tecture along with the baselines in C++ using
DyNet (Neubig et al., 2017) on top of Mantis
(Cohn et al., 2016) which is an implementation
of the attentional SEQ2SEQ NMT model. For our
MTL architecture, we used the proposed recurrent
unit with 3 blocks in encoder and decoder. For the
fair comparison in terms the of number of parame-
ters, we used 3 stacked layers in both encoder and
decoder components for the baselines. We com-
pare against the following baselines:

• Baseline 1: The vanilla SEQ2SEQ model
(Luong et al., 2015a) without any auxiliary
task.

• Baseline 2: The MTL architecture proposed
in (Niehues and Cho, 2017) which fully
shares parameters in components. We have
used their best performing architecture with
our training schedule. We have extended
their work with deep stacked layers for the
sake of comparison.

• Baseline 3: The MTL architecture proposed
in (Zaremoodi and Haffari, 2018) which uses
deep stacked layers in the components and
shares the parameters of the top two/one
stacked layers among encoders/decoders of
all tasks6.

For the proposed MTL, we use recurrent units
with 400 hidden dimensions for each block. The
encoders and decoders of the baselines use GRU
units with 400 hidden dimensions. The attention
component has 400 dimensions. We use Adam
optimizer (Kingma and Ba, 2014) with the initial
learning rate of 0.003 for all the tasks. Learning

6In preliminary experiments, we have tried different shar-
ing scenarios and this one led to the best results.

0.2

0.25

0.3

0.35

0.4

0.45

0.5

Block	1 Block	2 Block	3

MT Semantic Syntactic NER

Figure 2: Average percentage of block usage for
each task.

rates are halved on the decrease in the performance
on the dev set of corresponding task. Mini-batch
size is set to 32, and dropout rate is 0.5. All mod-
els are trained for 50 epochs and the best models
are saved based on the perplexity on the dev set of
the translation task.

For each task, we add special tokens to the be-
ginning of source sequence (similar to (Johnson
et al., 2017)) to indicate which task the sequence
pair comes from.

We used greedy decoding to generate trans-
lation. In order to measure translation quality,
we use BLEU7 (Papineni et al., 2002) and TER
(Snover et al., 2006) scores.

3.4 Results and analysis

Table 1 reports the results for the baselines and
our proposed method on the two aforementioned
translation tasks. As expected, the performance of
MTL models are better than the baseline 1 (only
MT task). As seen, partial parameter sharing is
more effective than fully parameter sharing. Fur-
thermore, our proposed architecture with adaptive

7Using “multi-bleu.perl” script from Moses (Koehn et al.,
2007).



660

sharing performs better than the other MTL meth-
ods on all tasks, and achieve +1 BLEU score im-
provements on the test sets. The improvements in
the translation quality of NMT models trained by
our MTL method may be attributed to less inter-
ference with multiple auxiliary tasks.

Figure 2 shows the average percentage of block
usage for each task in an MTL model with 3 shared
blocks, on the English-Farsi test set. We have ag-
gregated the output of the routing network for the
blocks in the encoder recurrent units over all the
input tokens. Then, it is normalized by dividing
on the total number of input tokens. Based on
Figure 2, the first and third blocks are more spe-
cialized (based on their usage) for the translation
and NER tasks, respectively. The second block is
mostly used by the semantic and syntactic parsing
tasks, so specialized for them. This confirms our
model leverages commonalities among subsets of
tasks by dedicating common blocks to them to re-
duce task interference.

4 Conclusions

We have presented an effective MTL approach
to improve NMT for low-resource languages,
by leveraging curated linguistic resources on the
source side. We address the task interference is-
sue in previous MTL models by extending the re-
current units with multiple blocks along with a
trainable routing network. Our experimental re-
sults on low-resource English to Farsi and Viet-
namese datasets, show +1 BLEU score improve-
ments compared to strong baselines.

Acknowledgments

The research reported here was initiated at the
2017 Frederick Jelinek Memorial Summer Work-
shop on Speech and Language Technologies,
hosted at Carnegie Mellon University and spon-
sored by Johns Hopkins University with unre-
stricted gifts from Amazon, Apple, Facebook,
Google, and Microsoft. We are very grateful to the
workshop members for the insightful discussions
and data pre-processing. This work was supported
by the Multi-modal Australian ScienceS Imag-
ing and Visualisation Environment (MASSIVE)
(www.massive.org.au), and by the Aus-
tralian Research Council through DP160102686.
The first author was partly supported by CSIRO’s
Data61. We would like to thank the anonymous
reviewers for their constructive feedback.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734.

Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-
molova, Kaisheng Yao, Chris Dyer, and Gholamreza
Haffari. 2016. Incorporating structural alignment
biases into an attentional neural translation model.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 876–885.

Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan
Belinkov, and Stephan Vogel. 2017. Understanding
and improving morphological learning in the neu-
ral machine translation decoder. In Proceedings of
the Eighth International Joint Conference on Natu-
ral Language Processing, pages 142–151.

Tobias Domhan and Felix Hieber. 2017. Using target-
side monolingual data for neural machine translation
through multi-task learning. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, pages 1501–1506.

Melvin Johnson, Mike Schuster, Quoc V Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Viégas, Martin Wattenberg, Greg Corrado,
et al. 2017. Google’s multilingual neural machine
translation system: Enabling zero-shot translation.
Transactions of the Association of Computational
Linguistics, 5(1):339–351.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
ACL on Interactive Poster and Demonstration Ses-
sions, pages 177–180.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Pro-
ceedings of the First Workshop on Neural Machine
Translation, pages 28–39.

www.massive.org.au


661

Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin
Choi, and Luke Zettlemoyer. 2017. Neural amr:
Sequence-to-sequence models for parsing and gen-
eration. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 146–157.

Minh-Thang Luong and Christopher D. Manning.
2015. Stanford neural machine translation systems
for spoken language domain. In International Work-
shop on Spoken Language Translation.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015a. Effective approaches to attention-
based neural machine translation. In Proceedings of
the 2015 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP). Association for
Computational Linguistics.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015b. Effective Approaches to Attention-
based Neural Machine Translation. In Proceedings
of the 2015 Conference on Empirical Methods in
Natural Language Processing, pages 1412–1421.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Comput. Lin-
guist., 19(2):313–330.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, et al. 2017. Dynet: The
dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980.

Jan Niehues and Eunah Cho. 2017. Exploiting linguis-
tic resources for neural machine translation using
multi-task learning. In Proceedings of the Second
Conference on Machine Translation, pages 80–89.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318.

Clemens Rosenbaum, Tim Klinger, and Matthew
Riemer. 2018. Routing networks: Adaptive selec-
tion of non-linear functions for multi-task learning.
In International Conference on Learning Represen-
tations.

Sebastian Ruder, Joachim Bingel, Isabelle Augenstein,
and Anders Søgaard. 2017. Sluice networks: Learn-
ing what to share between loosely related tasks.
CoRR, abs/1705.08142.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural Machine Translation of Rare Words
with Subword Units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics, pages 1715–1725.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
Dean. 2017. Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer. arXiv
preprint arXiv:1701.06538.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas.

Jörg Tiedemann. 2012. Parallel data, tools and inter-
faces in opus. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Eval-
uation, pages 2214–2218.

Oriol Vinyals, Ł ukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In Advances in Neu-
ral Information Processing Systems 28, pages 2773–
2781.

Poorya Zaremoodi and Gholamreza Haffari. 2018.
Neural machine translation for bilingually scarce
scenarios: A deep multi-task learning approach. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1535–1545.

http://dl.acm.org/citation.cfm?id=972470.972475
http://dl.acm.org/citation.cfm?id=972470.972475
https://openreview.net/forum?id=ry8dvM-R-
https://openreview.net/forum?id=ry8dvM-R-
http://arxiv.org/abs/1705.08142
http://arxiv.org/abs/1705.08142
http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf
http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf
http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf

