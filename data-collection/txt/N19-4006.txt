



















































Train, Sort, Explain: Learning to Diagnose Translation Models


Proceedings of NAACL-HLT 2019: Demonstrations, pages 29–34
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

29

Train, Sort, Explain: Learning to Diagnose Translation Models

Robert Schwarzenberg1, David Harbecke1, Vivien Macketanz1,
Eleftherios Avramidis1, Sebastian Möller1,2

1German Research Center for Artificial Intelligence (DFKI), Berlin, Germany
2Technische Universität Berlin, Berlin, Germany

{firstname.lastname}@dfki.de

Abstract

Evaluating translation models is a trade-off be-
tween effort and detail. On the one end of
the spectrum there are automatic count-based
methods such as BLEU, on the other end lin-
guistic evaluations by humans, which arguably
are more informative but also require a dis-
proportionately high effort. To narrow the
spectrum, we propose a general approach on
how to automatically expose systematic dif-
ferences between human and machine trans-
lations to human experts. Inspired by adver-
sarial settings, we train a neural text classi-
fier to distinguish human from machine trans-
lations. A classifier that performs and general-
izes well after training should recognize sys-
tematic differences between the two classes,
which we uncover with neural explainability
methods. Our proof-of-concept implementa-
tion, DiaMaT, is open source. Applied to
a dataset translated by a state-of-the-art neu-
ral Transformer model, DiaMaT achieves a
classification accuracy of 75% and exposes
meaningful differences between humans and
the Transformer, amidst the current discussion
about human parity.

1 Introduction

A multi-dimensional diagnostic evaluation of per-
formance or quality often turns out to be more
helpful for system improvement than just consid-
ering a one-dimensional utilitarian metric, such as
BLEU (Papineni et al., 2002). This is exemplified
by, for instance, the pioneering work of Bahdanau
et al. (2014). The authors introduced the attention
mechanism responding to the findings of Cho et al.
(2014) who reported that neural translation qual-
ity degraded with sentence length. The attention
mechanism was later picked up by Vaswani et al.
(2017) for their attention-only Transformer model,
which still is state of the art in machine translation
(MT) (Bojar et al., 2018). Furthermore, while MT

output approaches human translation quality and
the claims for ”human parity” (Wu et al., 2016;
Hassan et al., 2018) increase, multi-dimensional
diagnostic evaluations can be useful to spot the
thin line between the machine and the human.

Diagnostic (linguistic) evaluations require
human-expert feedback, which, however, is very
time-consuming to collect. For this reason, there
is a need for tools that mitigate the effort, such as
the ones developed by Madnani (2011); Popović
(2011); Berka et al. (2012); Klejch et al. (2015).

In this paper we propose a novel approach for
developing evaluation tools. Contrary to the above
tools that employ string comparison methods such
as BLEU, implementations of the new approach
derive annotations based on a neural model of ex-
plainability. This allows both capturing of seman-
tics as well as focusing on the particular tenden-
cies of MT errors. Using neural methods for the
evaluation and juxtaposition of translations has al-
ready been done by Rikters et al. (2017). Their
method, however, can only be applied to attention-
based models and their translations. In contrast,
our approach generalizes to arbitrary machine and
even human translations. After first discussing the
abstract approach in the next section, we present a
concrete open-source implementation, “DiaMaT”
(from Diagnose Machine Translations).

2 Approach

The proposed approach consists of the three steps
(1) train, (2) sort, and (3) explain.

2.1 Step 1: Train
In a first step, inspired by generative adversar-
ial networks (Goodfellow et al., 2014; Wu et al.,
2017; Yang et al., 2017) we propose to train a
model to distinguish machine from human transla-
tions. The premise is that if the classifier general-
izes well after training, it has learned to recognize



30

systematic or frequent differences between the two
classes (herinafter also referred to as “class evi-
dence”). Class evidence may be, for instance, style
differences, overused n-grams but also errors. The
text classifier can be implemented through various
architectures, ranging from deep CNNs (Conneau
et al., 2017) to recurrent classifiers built on top of
pre-trained language models (Howard and Ruder,
2018).

2.2 Step 2: Sort
In a second step, we suggest letting the trained
classifier predict the labels of a test set which con-
tains human and machine translations and then
sort them by classification confidence. This is
based on the assumption that if the classifier is
very certain that a given translation was produced
by a machine (translation moved to the top of the
list in this step), then the translation should con-
tain strong evidence for a class, i.e. errors typical
for only the machine. Furthermore, even if we are
dealing with a very human-like MT output, which
means that our classifier may only slightly perform
above chance, sorting by classification confidence
should still move the few systematic differences
that the classifier identified to the top.

positive output

hidden layer
contributions

word vector
contributions

Figure 1: Contributions propagated from output to in-
put space. Colors represent positive (red) and negative
(blue) contributions. The Figure is adapted from Kin-
dermans et al. (2018).

2.3 Step 3: Explain
Arras et al. (2016, 2017a,b) demonstrated the data
exploratory power of explainability methods in
Natural Language Processing (NLP). This is why
in a third step, we propose to apply an explain-
ability method to uncover and visualize the class
evidence on which the classifier based its deci-
sions. Our definition of an explanation follows
Montavon et al. (2018), who define it as “the col-
lection of features of the interpretable domain, that

have contributed for a given example to produce
a decision (e.g. classification or regression).”1 In
our case the interpretable domain is the plain text
space. There exist several candidate explainability
methods, one of which we present in the following
as an example.

Figure 2: A heatmap of contribution scores in word
vector space over a sequence of tokens. Red means
positive contribution (score > 0), blue means negative
contribution (score < 0).

2.3.1 Explainability and Interpretability
Methods for Data Exploration

In their tutorial paper, Montavon et al. (2018)
discuss several groups of explainability methods.
One group, for instance, identifies how sensitively
a model reacts to a change in the input, others
extract patterns typical for a certain class. Here,
we discuss methods that propagate back contribu-
tions.

The contribution flow is illustrated in Fig. 1. At
the top, the depicted binary classifier produced a
positive output (input classified as class one). The
classification decision is based on the fact that in
the previous layer, the evidence for class one ex-
ceeded the evidence for class zero: The left and
the right neuron contributed positively to the de-
cision (reddish), whereas the middle neuron con-
tributed negatively (blueish). Several explainabil-
ity methods, such as Layerwise Relevance Prop-
agation (Bach et al., 2015) or PatternAttribution
(Kindermans et al., 2018), backtrack contributions
layer-wise. The methods have to preserve coher-
ence over highly non-linear activation functions.
Eventually, contributions are projected into the in-
put space where they reveal what the model con-
siders emblematic for a class. This is what we ex-
ploit in step 3.

1Montavon et al. (2018) distinguish between explainabil-
ity and interpretability. Interpretability methods also hold po-
tential for the approach. For brevity, we limit ourselves to
explainability methods here.



31

Figure 3: Screenshot of demonstrative results in DiaMaT. Filters that allow the user to analyse the corpus are not
shown. The bold label is the true label. The activations of the machine neuron are shown in brackets; on the left
the unnormalized logit activation, on the right the softmax activation. Positive logits and softmax probabilities
greater 0.5 indicate machine evidence, as do tokens highlighted in red. Consequently, blue indicates evidence for
the human. The more intense the colour, the stronger the evidence.

Explainability methods in NLP (Arras et al.,
2016, 2017a,b; Harbecke et al., 2018) are typi-
cally used to first project scores into word-vector
space resulting in heat maps as shown in Fig. 2.
To interpret them in plain text space, the scores
are summed over the word vector dimensions to
compute RGB values for each token, resulting in
plain text heatmaps as shown in Fig. 3.

3 Implementation

For step 1 (training phase), DiaMaT2 deploys a
CNN text classifier, the architecture of which is
depicted in Fig. 4. The classifier consumes three
embeddings: the embedding of a source and two
translations of the source, one by a machine and
one by a human. It then separately convolves
over the embeddings and subsequently applies
max pooling to the filter activations. The concate-
nated max features are then passed to the last layer,
a fully connected layer with two output neurons.
The left neuron fires if the machine translation was
passed to the left input layer, the right neuron fires
if the machine translation was passed to the right
input layer. Note that this layer allows the model
to combine features from all three inputs for its
classification decision.

For step 2 (sorting phase), DiaMaT offers to sort
by unnormalized logit activations or by softmax
activations. Furthermore, one can choose to use

2Source code, data and experiments are available at
https://github.com/dfki-nlp/diamat.

the machine neuron activation or the human neu-
ron activation as the sorting key.

For step 3 (explaining phase), DiaMaT employs
the iNNvestigate toolbox (Alber et al., 2018) in
the back-end that offers more than ten explainabil-
ity methods: Replacing one method with another
only requires to change one configuration value in
DiaMaT, before repeating step 3 again. In step
3, DiaMaT produces explanations in the form of
(token, score) tuple lists that are consumed by
a front-end server which visualizes the scores as
class evidence (see Fig. 3).3

Translation Source Translation

Conv Conv Conv

Max Pooling & Concatenation

FC2×F

Figure 4: Architecture of the text classifier.

4 Datasets and Experiments

We tested DiaMaT on a corpus translated by an
NMT Transformer engine (Vaswani et al., 2017)

3The front-end was inspired by the demo LRP server
of the Fraunhofer HHI insitute https://lrpserver.
hhi.fraunhofer.de/text-classification, last
accessed 2019-01-31.

https://github.com/dfki-nlp/diamat
https://lrpserver.hhi.fraunhofer.de/text-classification
https://lrpserver.hhi.fraunhofer.de/text-classification


32

conforming to the WMT14 data setup (Bojar et al.,
2014). The NMT model was optimized on the test-
set of WMT13 and an ensemble of 5 best models
was used. It was trained using OpenNMT (Klein
et al., 2017), including Byte Pair Encoding (Sen-
nrich et al., 2015) but no back-translation, achiev-
ing 32.68 BLEU on the test-set of WMT14.

Next, we trained the CNN text classifier
sketched in Fig. 4 for which we randomly drew
1M training samples (human references and ma-
chine translations alongside their sources) from
the WMT18 training data (Bojar et al., 2018), ex-
cluding the WMT14 training data. The valida-
tion set consisted of 100k randomly drawn sam-
ples from the same set and we drew another 100k
samples randomly for training the explainability
method of choice, PatternAttribution, which learns
explanations from data. All texts were embed-
ded using pre-trained fastText word vectors (Grave
et al., 2018).

We evaluated the classifier on around 20k sam-
ples drawn from the official test sets, excluding
WMT13. On this test set, the classifier achieved
an accuracy of 75%, which is remarkable, consid-
ering the ongoing discussion about human parity
(Wu et al., 2016; Hassan et al., 2018). We also
used this test set for steps 2 and 3. Thus, neither
the translation model, nor the text classifier, nor
the explainability method encountered this split
during training. For step 2, the machine transla-
tion was always passed to the right input layer and
contributions to the right output neuron were re-
trieved with PatternAttribution.4 We then sorted
the inputs by the softmax activation of the machine
neuron, which moved inputs for which the clas-
sifier is certain that it has identified the machine
correctly to the top.

5 Demonstration and Observations

We observed that the top inputs frequently con-
tained sentences in which DiaMaT considered the
token after a sentence-ending full stop strong ev-
idence for the human (Fig. 3, top segment). We
take this as evidence that DiaMaT correctly rec-
ognized that the human generated multiple sen-
tences instead of a single one more often than the
machine did. At this point, we cannot, however,
offer an explanation for why the token preceding
the punctuation mark is frequently considered ev-

4In order to visualize evidence for the human (blue), pos-
itive contributions in the left input needed to be inverted.

idence for the machine.
Furthermore, DiaMaT also regarded reduced

negations (“n’t”) as evidence for the human (see
Fig. 3, middle segment) which again is reflected in
the statistics. The machine tends to use the unre-
duced negation more frequently.

The last segment in Fig. 3 shows how DiaMaT
points to the fact that the machine more often pro-
duced sentence end markers than the human in
cases where the source contained no end marker.
The claims above are all statistically significant in
the test set, according to a χ2 test with α = 0.001.

6 Future Work

The inputs in Fig. 3 contain easily readable evi-
dence. There is, however, also much evidence that
is hard to read. In general, we can assume that with
increasing architectural complexity, more com-
plex class evidence can be uncovered, which may
come at the cost of harder readability.

In the future, it is worth exploring how different
architectures and model choices affect the quality,
complexity and readability of the uncovered ev-
idence. For instance, one direction would be to
to train the classifier on top of a pretrained lan-
guage model (Howard and Ruder, 2018; Devlin
et al., 2019) which could improve the classifica-
tion performance. Furthermore, other explainabil-
ity methods should also be tested.

7 Conclusion

We presented a new approach to analyse and juxta-
pose translations. Furthermore, we also presented
an implementation of the approach, DiaMaT. Dia-
MaT exploits the generalization power of neural
networks to learn systematic differences between
human and machine translations and then takes
advantage of neural explainability methods to un-
cover these. It learns from corpora containing mil-
lions of translations but offers explanations on sen-
tence level. In a stress test, DiaMaT was capa-
ble of exposing systematic differences between a
state-of-the-art translation model output and hu-
man translations.

Acknowledgements

This research was partially supported by the Ger-
man Federal Ministry of Education and Research
through the project DEEPLEE (01IW17001). We
would also like to thank the anonymous reviewers
and Leonhard Hennig for their helpful feedback.



33

References
Maximilian Alber, Sebastian Lapuschkin, Philipp

Seegerer, Miriam Hägele, Kristof T Schütt, Grégoire
Montavon, Wojciech Samek, Klaus-Robert Müller,
Sven Dähne, and Pieter-Jan Kindermans. 2018.
iNNvestigate neural networks! arXiv preprint
arXiv:1808.04260.

Leila Arras, Franziska Horn, Grégoire Montavon,
Klaus-Robert Müller, and Wojciech Samek. 2016.
Explaining Predictions of Non-Linear Classifiers in
NLP. In Proceedings of the 1st Workshop on Repre-
sentation Learning for NLP, pages 1–7.

Leila Arras, Franziska Horn, Grégoire Montavon,
Klaus-Robert Müller, and Wojciech Samek. 2017a.
”What is relevant in a text document?”: An inter-
pretable machine learning approach. PLOS ONE,
12(8).

Leila Arras, Grégoire Montavon, Klaus-Robert Müller,
and Wojciech Samek. 2017b. Explaining recurrent
neural network predictions in sentiment analysis. In
Proceedings of the EMNLP’17 Workshop on Com-
putational Approaches to Subjectivity, Sentiment &
Social Media Analysis (WASSA), pages 159–168.
Association for Computational Linguistics.

Sebastian Bach, Alexander Binder, Grégoire Mon-
tavon, Frederick Klauschen, Klaus-Robert Müller,
and Wojciech Samek. 2015. On pixel-wise explana-
tions for non-linear classifier decisions by layer-wise
relevance propagation. PLoS ONE, 10(7):e0130140.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Jan Berka, Ondřej Bojar, Mark Fishel, Maja Popović,
and Daniel Zeman. 2012. Automatic MT Error
Analysis: Hjerson Helping Addicter. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation, pages 2158–2163,
Istanbul, Turkey. European Language Resources As-
sociation.

Ondřej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleš
Tamchyna. 2014. Findings of the 2014 Workshop on
Statistical Machine Translation. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, pages 12–58, Baltimore, Maryland, USA. As-
sociation for Computational Linguistics.

Ondřej Bojar, Christian Federmann, Mark Fishel,
Yvette Graham, Barry Haddow, Matthias Huck,
Philipp Koehn, and Christof Monz. 2018. Find-
ings of the 2018 conference on machine translation
(wmt18). In Proceedings of the Third Conference
on Machine Translation, Volume 2: Shared Task Pa-
pers, pages 272–307, Belgium, Brussels. Associa-
tion for Computational Linguistics.

Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder–decoder ap-
proaches. Syntax, Semantics and Structure in Statis-
tical Translation, page 103.

Alexis Conneau, Holger Schwenk, Loı̈c Barrault, and
Yann Lecun. 2017. Very deep convolutional net-
works for text classification. In Proceedings of the
European Chapter of the Association for Computa-
tional Linguistic (EACL).

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
Deep Bidirectional Transformers for Language Un-
derstanding. Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Advances in neural information
processing systems, pages 2672–2680.

Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-
mand Joulin, and Tomas Mikolov. 2018. Learning
word vectors for 157 languages. In Proceedings
of the International Conference on Language Re-
sources and Evaluation (LREC 2018).

David Harbecke, Robert Schwarzenberg, and
Christoph Alt. 2018. Learning explanations
from language data. In Proceedings of the 2018
EMNLP Workshop BlackboxNLP: Analyzing and
Interpreting Neural Networks for NLP, pages
316–318.

Hany Hassan, Anthony Aue, Chang Chen, Vishal
Chowdhary, Jonathan Clark, Christian Feder-
mann, Xuedong Huang, Marcin Junczys-Dowmunt,
William Lewis, Mu Li, Shujie Liu, Tie-Yan Liu,
Renqian Luo, Arul Menezes, Tao Qin, Frank Seide,
Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce
Xia, Dongdong Zhang, Zhirui Zhang, and Ming
Zhou. 2018. Achieving Human Parity on Auto-
matic Chinese to English News Translation. CoRR,
abs/1803.05567.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model fine-tuning for text classification. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 328–339.

Pieter-Jan Kindermans, Kristof T. Schütt, Maximilian
Alber, Klaus-Robert Müller, Dumitru Erhan, Been
Kim, and Sven Dähne. 2018. Learning how to ex-
plain neural networks: PatternNet and PatternAttri-
bution. In International Conference on Learning
Representations (ICLR).

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander M Rush. 2017. OpenNMT:



34

Open-source toolkit for neural machine translation.
arXiv preprint arXiv:1701.02810.

Ondrej Klejch, Eleftherios Avramidis, Aljoscha Bur-
chardt, and Martin Popel. 2015. MT-ComparEval:
Graphical evaluation interface for Machine Transla-
tion development. The Prague Bulletin of Mathe-
matical Linguistics, 104(1):63–74.

Nitin Madnani. 2011. iBLEU: interactively debugging
and scoring statistical machine translation systems.
In 2011 IEEE Fifth International Conference on Se-
mantic Computing, pages 213–214.

Grégoire Montavon, Wojciech Samek, and Klaus-
Robert Müller. 2018. Methods for interpreting and
understanding deep neural networks. Digital Signal
Processing, 73:1–15.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Maja Popović. 2011. Hjerson: An Open Source
Tool for Automatic Error Classification of Machine
Translation Output. The Prague Bulletin of Mathe-
matical Linguistics, 96(-1):59–68.

Matı̄ss Rikters, Mark Fishel, and Ondřej Bojar. 2017.
Visualizing Neural Machine Translation Attention
and Confidence. The Prague Bulletin of Mathemat-
ical Linguistics, 109(1):39–50.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Neural Machine Translation of Rare Words
with Subword Units. CoRR, abs/1508.0.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Lijun Wu, Yingce Xia, Li Zhao, Fei Tian, Tao
Qin, Jianhuang Lai, and Tie-Yan Liu. 2017. Ad-
versarial Neural Machine Translation. CoRR,
abs/1704.06933.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
Neural Machine Translation System: Bridging the
Gap between Human and Machine Translation.
Computer Research Repository, abs/1609.0.

Zhen Yang, Wei Chen, Feng Wang, and Bo Xu.
2017. Improving Neural Machine Translation with
Conditional Sequence Generative Adversarial Nets.
CoRR, abs/1703.04887.


