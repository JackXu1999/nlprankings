



















































ICL-HD at SemEval-2016 Task 10: Improving the Detection of Minimal Semantic Units and their Meanings with an Ontology and Word Embeddings


Proceedings of SemEval-2016, pages 937–945,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

ICL-HD at SemEval-2016 Task 10: Improving the Detection of Minimal
Semantic Units and their Meanings with an Ontology and Word Embeddings

Angelika Kirilin and Felix Krauss and Yannick Versley

Institute for Computational Linguistics
Ruprecht-Karls-University

Heidelberg

{kirilin,krauss,versley}@cl.uni-heidelberg.de

Abstract

This paper presents our system submitted for
SemEval 2016 Task 10: Detecting Minimal
Semantic Units and their Meanings (DiM-
SUM; Schneider, Hovy, et al., 2016). We
extend AMALGrAM (Schneider and Smith,
2015) by tapping two additional information
sources. The first information source uses a
semantic knowledge base (YAGO3; Suchanek
et al., 2007) to improve supersense tagging
(SST) for named entities. The second in-
formation source employs word embeddings
(GloVe; Pennington et al., 2014) to cap-
ture fine-grained latent semantics and there-
fore improving the supersense identification
for both nouns and verbs. We conduct a de-
tailed evaluation and error analysis for our fea-
tures and come to the conclusion that both our
extensions lead to an improved detection for
SST.

1 Introduction

The SemEval 2016 Task 10 on Detecting Mini-
mal Semantic Units of Meaning (DiMSUM) is con-
cerned with the identification of semantic classes
called supersenses for single words as well as multi-
word expressions (MWEs).

Identifying supersenses in text allows for abstrac-
tions that characterize word meanings beyond su-
perficial orthography (Schneider and Smith, 2015)
as well as inferring representations that move to-
wards language independence (Schneider, Mohit, et
al., 2013). It has been used to extend named entity
recognition (Ciaramita and Johnson, 2003) and to
support supervised word sense disambiguation as it

provides partial disambiguation (Ciaramita and Al-
tun, 2006; Ciaramita and Johnson, 2003) as well as
in syntactic parse re-ranking (Koo et al., 2005) as
latent semantic features.

The addition of MWEs - idiosyncratic interpreta-
tions that cross word boundaries (Sag et al., 2002) -
takes into account that the supersense of a MWE is
usually not predictable from the meaning of the in-
dividual lexemes. The Task moreover distinguishes
between continuous MWEs like “high schooln.group”
and discontinuous (gappy) MWEs like “track peo-
ple downv.social”. The inventory of supersenses used
for the task - 41 supersense classes, consisting of
26 noun and 15 verb supersenses - is derived from
WordNet’s top-level hypernyms in the taxonomy.
They are designed to be broad enough to encompass
all nouns and verbs (Miller, 1990; Fellbaum, 1990).

2 Related Work

Schneider and Smith (2015) are the first to approach
SST and MWE detection jointly with a discrimina-
tive model. Most of the previous work focuses on
each of the tasks in separate. Sag et al. (2002) tried
to raise attention for the issue of MWEs in general
and analyzed different types of MWE. Baldwin et al.
(2003) employed latent semantic analysis to deter-
mine the decomposability of MWEs. Finally many
MWE lexicons have been built for different pur-
poses which is why Schneider et al. picked up the
issue to address MWE annotation for general pur-
poses (Schneider, Danchik, et al., 2014; Schneider,
Onuffer, et al., 2014).

Ciaramita and Johnson (2003) first trained and
tested a discriminative model for SST of unambigu-

937



ous nouns on data extracted from different versions
of WordNet and achieved an accuracy of slightly
over 52%. Curran (2005) applied an unsupervised
approach based on vector-space word similarity and
achieved 63% accuracy on the same data used by
Ciaramita and Johnson (2003). When revisiting the
task Ciaramita and Altun (2006) achieved between
70% and 77% F-score using a HMM sequence tag-
ger.

3 System Description

3.1 Baseline System

We use AMALGrAM 2.0 (Schneider and Smith,
2015) as our baseline system. The model uses a first-
order structured perceptron (Collins, 2002) with av-
eraging. It involves a linear scoring function, a
Viterbi algorithm that chooses the highest-scoring
valid output tag sequence and an online learning al-
gorithm that determines the best tagging given the
current model.

We contrast four feature sets for full SST: Schnei-
der and Smith (2015) most effective feature set
which is further described in section 3.2, those base-
line features plus YAGO feature (3.3), baseline fea-
tures plus GloVe word embeddings (3.4), and finally
a feature set that combines all the above mentioned.

3.2 Baseline Features

The feature set incorporates three components: the
first is Schneider, Danchik, et al. (2014) basic MWE
features, second is Brown clusters and the last com-
ponent is WordNet synset features (Schneider and
Smith, 2015). Basic MWE features analyze word n-
grams, character pre- and suffixes, and POS tags, as
well as lexicon entries that match lemmas of MWE
in the sentence. We use four of the ten available
lookup lexicons: semcor mwes including all MWEs
in SemCor (Miller et al., 1993), WordNet mwes con-
taining all MWEs from WordNet (Fellbaum, 1998),
phrases dot net which is a phrase idiom lexicon1,
wikimwe which is mined from English Wikipedia
(Hartmann et al., 2012), and enwikt consisting of
all MWE entries in English Wiktionary. The second
component of the baseline feature set provides un-
supervised distributional word clusters in the form

1http://www.phrases.net/

of Brown clusters (Brown et al., 1992). Those clus-
ters reflect lexical generalizations that are useful for
syntactic and semantic analysis tasks and are there-
fore suitable for our task. The last component of
the feature set helps predicting supersenses by cre-
ating possible supersense candidates from WordNet
synsets.

3.3 YAGO Feature
We implement a YAGO lookup component that pro-
vides semantic information as an additional feature
for the AMALGrAM system with the purpose of im-
proving SST for named entities. YAGO (Yet An-
other Great Anthology; Suchanek et al., 2007) is
a knowledge resource that combines the structural
benefits from the WordNet taxonomy with the rich-
ness of Wikipedia’s categories system. This renders
it ideal for our task by enabling us to retrieve Word-
Net hypernyms for many different named entities.
The following part describes how we look up poten-
tial concepts in YAGO, how we find potential names
of YAGO concepts within the text and how we en-
code the returned results as a feature for AMAL-
GrAM.
There is no guarantee that the named entities con-
tained in training or test data appear in the same sur-
face form as they are stored in YAGO (e.g. “Elvis”
vs. “Elvis Presley”). We apply some heuristics
should an initial query with a potential named entity
not yield a result:

1. Capitalize the initial character of each token 2

2. Try to retrieve the exact entry via Wikipedias
“redirectedFrom” links that are in-
cluded in YAGO (e.g. 〈Elvis Presley〉,
〈redirectedFrom〉, 〈“Elvis”@eng〉)

3. Drop all tokens except the first and repeat the
previous steps (only applicable for sequences)

For the detection of named entities during feature
extraction we rely on the gold POS tag annotation in
the provided data. Whenever a token appears with
a “PROPN” tag (e.g. Germany PROPN) we query it
on YAGO. If the token is followed by a continuous
sequence of further “PROPN” tagged tokens (e.g.

2We found that we get the best results when capitalizing ev-
ery candidate expression.

938



athlete fullback physical entity
back living thing player
causal agent object preserver
contestant organism running back
defender person whole
football player

Table 1: WordNet hypernyms contained in the YAGO-Entry:
“Jérôme Boateng”.

FCPROPN BayernPROPN MunichPROPN) the whole se-
quence linked with underscores is used as the search
query.

The feature extraction is done in the following
way: In a first step we iterate over all supersense
bearing singleton and MWE nouns in our training
data and try to query them in YAGO. If we have a
match we extract the WordNet hypernyms for the
found entity. An example for a successful query
can be seen in Table 1. We accumulate a count of
observed WordNet hypernyms for each supersense.
From the count we calculate a tf-idf whereby the
supersenses are seen as the “documents” and the
WordNet hypernyms as “words”. What we obtain
from this procedure is a tf-idf index that tells us the
significance of a WordNet hypernym for a given su-
persense based on our training data. We require that
a WordNet hypernym has to appear at least three
times with a supersense otherwise its tf-idf is set to
zero for this supersense. During feature extraction
we provide the information from extracted WordNet
hypernyms in two ways: The straightforward way is
to provide each WordNet hypernym as is. With the
second feature we make use of our precomputed tf-
idf index by calculating a supersense ranking when-
ever we find a candidate entity in YAGO. For each
supersense we add up the tf-idf values of the Word-
Net hypernyms found with the current candidate en-
tity. If the entity is linked to many WordNet hyper-
nyms with high tf-idf values for a certain supersense
the respective supersense will receive a high rank. 3

3.4 GloVe Feature
The second novel feature we provide is based on
word embeddings which are representations of the
meaning of words in terms of real-valued vectors in

3We achieve the best results when using the top ranked su-
persense as well as all additional supersenses that receive at
least half of the top ranked supersense’s tf-idf score.

a low-dimensional vector space. Because such em-
beddings provide generalizations over the meaning
of words, including words that do not occur in the
training data, they are often used as a general way to
improve accuracy in the form of extra word features
(Turian et al., 2010). Two of the most popular meth-
ods to create such a mapping include: global matrix
factorization and the local context window method.
Pennington et al. (2014) combine both methods in
the GloVe word embeddings. The available word
vectors are derived from a 2014 Wikipedia4 dump
and the Gigaword 5 corpus5. Both sources together
comprise 400,000 word types and there are four ver-
sions available that differ in the size of their di-
mensions: 50, 100, 200 and 300. GloVe word em-
beddings capture fine-grained semantic and syntac-
tic regularities using a global log-bilinear regression
model with a weighted least-squares objective. The
training objective of GloVe is to learn word vectors
such that their dot product equals the logarithm of
the words’ probability of co-occurrence.

We incorporate our feature set using a similar
method to the lookup lexicons. GloVe word embed-
dings are essentially a dictionary where each entry
consists of a word type and their word vector. If a
given lowercase token matches a GloVe lexicon en-
try then we extend the feature vector of that token
with its corresponding word embeddings. Turian
et al. (2010) mention that the weights in word em-
beddings are not necessarily in a bounded range. If
the range of the word embeddings is too large, they
will exert more influence than the remaining fea-
tures. To prevent the word embeddings from exert-
ing too much influence on the prediction when they
consist of an unbounded range of real numbers we
adopt Turian et al.’s (2010) method of scaling the
word vectors. Assuming that all word embeddings
are represented in a matrix E: Each row Ei contains
the word embeddings of a token, each column Ej
represents one dimension of the word embeddings
and each cell contains a word embedding Eij . We
scale each word vector dimension by a scaling con-
stant σ and the inverse of the standard deviation over

4http://dumps.wikimedia.org/enwiki/20140102/
5https://catalog.ldc.upenn.edu/LDC2011T07

939



the values across all words:

wnew =
σ ∗ Eij
stdev(Ej)

(1)

4 Experiments

In the following section we present the data we used,
then the tools and parameters and finally the results
of our experiments.

4.1 Data

Our training and test data consists of the data
made available for SemEval 2016 Task 10. The
training data includes three harmonized data-sets:
STREUSLE 2.1 (Schneider and Smith, 2015), Rit-
ter and Lowlands Twitter dataset (Johannsen et al.,
2014). The test set also consists of three sources:
online reviews from the TrustPilot corpus (Hovy et
al., 2015), tweets from the Tweebank corpus (Kong
et al., 2014) and TED talk transcripts (Cettolo et
al., 2012; Neubig et al., 2014). All datasets use the
17 Universal POS categories and the extended BIO
scheme from Schneider and Smith, 2015. For fea-
ture development we used a shuffled held-out por-
tion of the train set.

4.2 Experimental Setup

We conduct the experiments on the aforementioned
DiMSUM data sets. We use the AMALGrAM 2.0
tagger as baseline system with the following pa-
rameters:6 four training iterations, features that ap-
peared less than five times were cutoff, a constraint
for the decoding process that asserts that the “O” la-
bel is never followed by an “I”, including loss term
and a cost penalty of 100 for errors against recall.
Furthermore we use brown clusters and five MWE
lexica mentioned in section 3.2. Additionally we
create a new tagset that suits the DiMSUM tags. We
only consider tags occurring in the DiMSUM train-
ing set, yielding |Y | = 170 tags.

|{BbOo}|︸ ︷︷ ︸
4

× (|N |+ |V |+ |∅|)︸ ︷︷ ︸
26+15+1

+ |{Ii}|︸ ︷︷ ︸
2

= 170

All evaluation scores where obtained using the eval-
uation script for SemEval 2016 Task 10.

6We have been orienting ourselves towards the parameters
used in Schneider and Smith (2015).

baseline glove50 glove100 glove200 glove300
56

56.5

57

57.5

56.23

57.63

57.36
57.43

57.18

systems

F1
-s

co
re

Figure 1: Evaluation of the influence of high dimensional word
embeddings on the performance of the baseline system using

combined F1-scores.

4.3 YAGO Feature Experiments

The effects of the YAGO feature on AMALGrAM
can be seen in Table 5. Compared to the baseline
(BL) the WordNet hypernyms have almost no effect
on MWE detection (+0.02) while improving SST
(+0.64). The supersense rankings improve MWE
detection (+0.49) as well as SST (+0.62). Combin-
ing the features further improves the detection for
MWEs (+0.59) and SST (+1.13).

4.4 GloVe Feature Experiments

The first experiment involves the evaluation of dif-
ferent word embedding dimensions. Figure 1 shows
a comparison between the baseline system (BL)
which uses no word embeddings and our system
adopting GloVe word embeddings with four dimen-
sion sizes: 50, 100, 200 and 300. This experiment
uses word embeddings in their given real-valued
form without scaling them. All systems using word
embeddings show improved performance and are
approximately one percent higher than the baseline.
In this experiment the system performs best with 50
dimensional word embeddings, after which the per-
formance shows a slight decrease with growing vec-
tor dimensionality.

The second experiment evaluates the influence of
unscaled versus scaled word embeddings using the
method of Turian et al. (2010) which we described
in section 3.4. Table 2 compares the F1-scores of
four systems: the first - Glove50 - uses unscaled
word embedding whereas the remaining three sys-
tems scale those word embeddings with varying σ-
values (0.01, 0.1 and 1). According to Turian et al.
(2010) their method works best with a σ-value that
scales the standard derivation to 0.1. With our data

940



Glove50 σ=0.01 σ=0.1 σ=1

MWE 57.52 58.49 57.71 58.38
SST 57.65 57.14 57.04 57.98

combined 57.63 57.36 57.15 58.05

Table 2: Influence of unscaled (Glove50) versus scaled word
embeddings for MWE, SST and both tags (combined) using

F1-score. Last three columns use Turian et al.’s 2010 scaling

method with varying σ-values.

-brown +brown

BL Best BL Best

MWE 58.13 57.69 58.10 58.38
SST 56.28 57.43 55.87 57.98

combined 56.59 57.47 56.23 58.05

Table 3: Comparison of baseline (BL) and our most success-
ful system (Best) with (+brown) and without (-brown) Brown

cluster.

that value is σ = 0.1. However our results contradict
Turian et al. (2010) as our experiment shows that
σ = 0.01 is more successful in predicting MWEs
whereas σ = 1 is more suitable for the detection of
supersenses as well as both combined.

The third experiment evolves around the interac-
tion between Brown clusters and word embeddings.
As both methods have a similar aim - capturing the
semantic representation of words - it is of interest
to distinguish their influence on the system perfor-
mance. To accomplish this we train the baseline sys-
tem (BL) and our most successful system (Glove50,
σ = 1) with (+brown) and without using Brown
clusters (-brown). Table 3 represents the resulting
F1-scores. Our system profits from the Brown clus-
ters as the F1-scores for all categories (MWE, SST
and combined) improves.

4.5 Final System Comparison

Finally we evaluate the combined impact of our fea-
tures on the performance of the baseline system.
Table 5 compares the baseline system plus YAGO,
baseline plus GloVe and our combined system using
both YAGO and GloVe features (final)7 with var-

7This is a revised version of our submitted system for Se-
mEval 2016 where we used our YAGO combined and unscaled

system Acc P R F1

MWE YAGO 91.70 70.43 50.31 58.69
GloVe 91.99 73.47 48.43 58.38

final 91.70 71.74 47.35 57.05

SST YAGO 85.02 55.96 58.08 57.00
GloVe 85.34 56.82 59.20 57.98

final 85.15 56.52 59.09 57.78

combined YAGO 81.29 57.98 56.60 57.28
GloVe 81.81 58.97 57.15 58.05

final 81.57 58.49 56.86 57.66

Table 4: Accuracy (Acc), precision (P), recall (R) and F1-score
(F1) for MWE detection, SST and both (combined) for the base-

line system (BL) + YAGO, BL + GloVe and BL + YAGO +

GloVe (final).

ious measurements. Our GloVe feature produces
the overall best results except for the detection of
MWEs due to a decrease in recall. In this case the
YAGO feature improves all measures which leads
to the highest performance. Although the combina-
tion of both features (final) results in an improve-
ment of the baseline the performance mostly ranges
in between the results for the individual GloVe and
YAGO features.

5 Feature Analysis

To get a deeper understanding of the impact and ben-
efit of our features we conduct the following ana-
lyzes. First we compare the coverage of our YAGO
lookup with NLTK’s WordNet component and ex-
amine the accuracy of our ranking feature. Then we
analyze the coverage of GloVe on the provided data
set and give a detailed recall analysis for each tag.

5.1 YAGO coverage

For the coverage comparison we extract all super-
sense bearing nouns from the whole gold annotated
DiMSUM data set. We query the extracted nouns
and count the ones that are found exclusively with
the lookup component of the YAGO feature and not
by NLTK-WordNet. Figure 2 (see Appendix A)
displays the results for each supersense that has at

GloVe100 features. The results of our submitted system were
slightly better (combined F1-score 57.77%), but could not be
reproduced after revising our system.

941



BL Hyp Rank H+R

MWE 58.10 58.12 58.59 58.69
SST 55.87 56.61 56.49 57.00

combined 56.23 56.86 56.84 57.28

Table 5: Evaluation of the YAGO feature with F1-score com-
paring baseline (BL), baseline + WordNet hypernym features

(Hyp), baseline + ranking features (Rank) and baseline + both

additional features combined (H+R).

least 100 associated nouns. Looking at the results
we observe a decreased coverage of the WordNet
lookup for nouns associated to supersenses that tend
to have an increased proportion of named entities,
e.g. “n.location”, “n.group” or “n.person”. For the
YAGO lookup this correlation is inverted resulting
in an increased amount of additionally found super-
sense bearing nouns for the previously described su-
persenses.

We also investigate the YAGO feature’s ranking
component. First we extract a tf-idf index from the
DiMSUM training data set. We use the index to gen-
erate a ranking for each expression that we are able
to detect with our YAGO feature on the test data set.
Figure 3 (see Appendix A) shows the relative dis-
tribution of gold supersenses for the first five rank-
ing positions. In this evaluation, the correct super-
sense is present in the first two ranks for the ma-
jority of cases. This experiment also indicates how
well the detection of supersense bearing nouns per-
forms. E.g. out of 413 nouns in the DiMSUM test
set that are marked with the “n.person” supersense,
146 were detected and received a rank.

5.2 GloVe errors
We conduct two experiments to assess our GloVe
feature: The first experiment examines the number
of tokens for both the train and test set for which
there is a GloVe word vector. As described in sec-
tion 3.4 we use lowercase tokens and match them to
their corresponding GloVe word embeddings if pos-
sible. With this method we get a coverage of 97.35%
for the train set and 94.32% for the test set. Which
means that almost every word of the DiMSUM data
set is represented by GloVe word embeddings.

To fully assess our GloVe feature we investigate
the improvement and deterioration of each tag with

our second experiment. To accomplish this we com-
pare the difference between the F1-score of our sys-
tem and the F1-score of the baseline (F1-score dis-
crepancy) for each tag. Since there are two types
of tags - MWE and supersenses - we conduct an
analysis for each type. We provide separate evalu-
ations for tokens that have already been seen in the
training set (seen) and tokens that have never been
seen before (unseen). Firstly we examine the F1-
score discrepancy for all supersenses. Figure 4 (see
Appendix A) shows the result for all supersenses
that occur more than 50 times in the test data set
having a F1-score discrepancy that is higher than
|0.5|. Most supersenses - whether they have been
previously seen in the training set or not - improve
with the use of the GloVe feature. An exception is
“n.event” - a supersense whose F1-score decreases
by -8.2% for seen tokens. The average F1-score
discrepancy of all noun and verb supersense tags is
+0.59; this further confirms our claim that the major-
ity of the supersenses improve with the adoption of
the GloVe feature. Lastly we examine the F1-score
discrepancy for all MWE tags that occur more than
50 times in the test set. The results can be seen in
Figure 5 (see Appendix A). The detection of “I” on
seen tokens decreases by 13.2%, whereas the detec-
tion of “B” improves for unseen tokens by +8.2%.
Overall the GloVe feature has a slightly positive in-
fluence on MWE detection.

6 Conclusion

Both YAGO and GloVe are effective in improv-
ing the performance for SST and MWE detection.
Our experiments show that the GloVe word embed-
dings provide information in addition to Brown clus-
ters that help the system further distinguish between
tags. Unfortunately the benefits of our features don’t
add up but instead balance each other out. Learn-
ing the reason for this could be subject to future
work. One could also replace the heuristics we em-
ployed for the detection of named entities in the
text with more sophisticated named entity resolution
techniques. Another possibility would be the com-
parison of the effect of different word embeddings
on the performance. It might also be advantageous
to further research the scaling of word embeddings.

942



References

Baldwin, Timothy, Colin Bannard, Takaaki Tanaka,
and Dominic Widdows (2003). “An Empirical
Model of Multiword Expression Decomposabil-
ity”. In: Proceedings of the ACL 2003 Workshop
on Multiword Expressions: Analysis, Acquisition
and Treatment - Volume 18. MWE ’03. Sapporo,
Japan: Association for Computational Linguis-
tics, pp. 89–96.

Brown, Peter F, Peter V Desouza, Robert L Mer-
cer, Vincent J Della Pietra, and Jenifer C Lai
(1992). “Class-based n-gram models of natural
language”. In: Computational linguistics 18.4,
pp. 467–479.

Cettolo, Mauro, Christian Girardi, and Marcello
Federico (2012). “Wit3: Web inventory of tran-
scribed and translated talks”. In: Proceedings of
the 16th Conference of the European Association
for Machine Translation (EAMT), pp. 261–268.

Ciaramita, Massimiliano and Yasemin Altun (2006).
“Broad-coverage sense disambiguation and infor-
mation extraction with a supersense sequence tag-
ger”. In: Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing. Association for Computational Linguistics,
pp. 594–602.

Ciaramita, Massimiliano and Mark Johnson (2003).
“Supersense tagging of unknown nouns in Word-
Net”. In: Proceedings of the 2003 conference on
Empirical methods in natural language process-
ing. Association for Computational Linguistics,
pp. 168–175.

Collins, Michael (2002). “Discriminative training
methods for Hidden Markov Models: Theory and
experiments with perceptron algorithms”. In: Pro-
ceedings of the ACL-02 conference on Empirical
methods in natural language processing-Volume
10. Association for Computational Linguistics,
pp. 1–8.

Curran, James R (2005). “Supersense tagging of un-
known nouns using semantic similarity”. In: Pro-
ceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics. Association
for Computational Linguistics, pp. 26–33.

Fellbaum, Christiane (1990). “English verbs as a se-
mantic net”. In: International Journal of Lexicog-
raphy 3.4, pp. 278–301.

– (1998). WordNet: An Electronic Lexical
Database. MIT Press.

Hartmann, Silvana, György Szarvas, and Iryna
Gurevych (2012). “Mining multiword terms from
Wikipedia”. In: Semi-Automatic Ontology Devel-
opment: Processes and Resources, pp. 226–258.

Hovy, Dirk, Anders Johannsen, and Anders Søgaard
(2015). “User review sites as a resource for large-
scale sociolinguistic studies”. In: Proceedings of
the 24th International Conference on World Wide
Web. International World Wide Web Conferences
Steering Committee, pp. 452–461.

Johannsen, Anders, Dirk Hovy, Héctor Martinez,
Barbara Plank, and Anders Søgaard (2014).
“More or less supervised super-sense tagging of
Twitter”. In: The 3rd Joint Conference on Lexical
and Computational Semantics (*SEM). Dublin,
Ireland.

Kong, Lingpeng, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A Smith (2014). “A dependency parser for
tweets”. In: Proceedings of Conference on Em-
pirical Methods In Natural Language Processing
(EMNLP), pp. 1001–1012.

Koo, Terry and Michael Collins (2005). “Hidden-
variable models for discriminative reranking”. In:
Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Nat-
ural Language Processing. Association for Com-
putational Linguistics, pp. 507–514.

Miller, George A (1990). “Nouns in WordNet: a lex-
ical inheritance system”. In: International journal
of Lexicography 3.4, pp. 245–264.

Miller, George A, Claudia Leacock, Randee Tengi,
and Ross T Bunker (1993). “A semantic concor-
dance”. In: Proceedings of the workshop on Hu-
man Language Technology. Association for Com-
putational Linguistics, pp. 303–308.

Neubig, Graham, Katsuhito Sudoh, Yusuke Oda,
Kevin Duh, Hajime Tsukada, and Masaaki Nagata
(2014). “The NAIST-NTT Ted Talk Treebank”.
In: International Workshop on Spoken Language
Translation.

Pennington, Jeffrey, Richard Socher, and Christo-
pher D Manning (2014). “GloVe: Global Vectors
for Word Representation.” In: EMNLP. Vol. 14,
pp. 1532–1543.

943



Sag, Ivan A, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger (2002). “Multi-
word expressions: A pain in the neck for NLP”.
In: Computational Linguistics and Intelligent Text
Processing. Springer, pp. 1–15.

Schneider, Nathan, Emily Danchik, Chris Dyer, and
Noah A Smith (2014). “Discriminative lexical
semantic segmentation with gaps: running the
MWE gamut”. In: Transactions of the Association
for Computational Linguistics 2, pp. 193–206.

Schneider, Nathan, Dirk Hovy, Anders Johannsen,
and Marine Carpuat (2016). “SemEval 2016 Task
10: Detecting Minimal Semantic Units and their
Meanings (DiMSUM)”. In: Proc. of SemEval. San
Diego, California, USA.

Schneider, Nathan, Behrang Mohit, Chris Dyer, Ke-
mal Olflazer, and Noah A Smith (2013). “Super-
sense tagging for Arabic: the MT-in-the-middle
attack”. In: Association for Computational Lin-
guistics.

Schneider, Nathan, Spencer Onuffer, Nora Kazour,
Emily Danchik, Michael T Mordowanec, Henri-
etta Conrad, and Noah A Smith (2014). “Com-
prehensive annotation of multiword expressions
in a social web corpus”. In: Proceedings of the
Language Resources and Evaluation Conference
(LREC).

Schneider, Nathan and Noah A Smith (2015). “A
corpus and model integrating multiword expres-
sions and supersenses”. In: Proceedings of the
Conference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies, pp. 1537–1547.

Suchanek, Fabian M, Gjergji Kasneci, and Gerhard
Weikum (2007). “Yago: a core of semantic knowl-
edge”. In: Proceedings of the 16th international
conference on World Wide Web. ACM, pp. 697–
706.

Turian, Joseph, Lev Ratinov, and Yoshua Bengio
(2010). “Word representations: a simple and gen-
eral method for semi-supervised learning”. In:
Proceedings of the 48th annual meeting of the as-
sociation for computational linguistics. Associa-
tion for Computational Linguistics, pp. 384–394.

944



Appendix A

n.
st

at
e

(1
12

)

n.
po

ss
es

si
on

(1
35

)

n.
ph

en
om

en
on

(4
6)

n.
co

gn
iti

on
(5

07
)

n.
at

tr
ib

ut
e

(2
43

)

n.
bo

dy
(1

02
)

n.
qu

an
tit

y
(1

02
)

n.
ar

tif
ac

t(
77

8)

n.
fo

od
(3

34
)

n.
ac

t(
45

3)

n.
lo

ca
tio

n
(5

22
)

n.
tim

e
(1

98
)

n.
ev

en
t(

30
2)

n.
co

m
m

un
ic

at
io

n
(6

39
)

n.
pe

rs
on

(1
07

9)

n.
gr

ou
p

(9
32

)

0

0.2

0.4

0.6

0.8

1

supersense tags (total count)

re
la

tiv
e

co
ve

ra
ge

WordNet Yago

Figure 2: Additional nouns from the DiMSUM dataset found
with the YAGO feature.

1 2 3 4 5

0

0.2

0.4

0.6

0.8

1

ranking position

co
rr

ec
ts

up
er

se
ns

e
pr

op
or

tio
n

n.artifact (30)

n.communication (35)

n.event (13)

n.group (89)

n.location (42)

n.person (146)

Figure 3: Normalized distribution of gold supersenses over first
five ranks on test set.

n.
at

tr
ib

ut
e

(1
30

)

n.
co

gn
iti

on
(1

30
)

n.
ev

en
t(

84
)

n.
lo

ca
tio

n
(9

9)

n.
qu

an
tit

y
(5

2)

n.
tim

e
(1

80
)

v.
co

gn
iti

on
(2

50
)

v.
pe

rc
ep

tio
n

(7
0)

v.
po

ss
es

si
on

(7
9)

−8

−6

−4

−2

0

2

4

6

8

supersense tags (test count)

F1
-s

co
re

di
sc

re
pa

nc
y

unseen seen

Figure 4: F1-score discrepancy higher than |0.5| between base-
line and best system using GloVe feature for supersense tags

that occur more than 50 times in the test set distinguishing be-

tween seen and unseen tokens.

B I O

−14
−12
−10
−8
−6
−4
−2
0

2

4

6

8

bio tags

F1
-s

co
re

di
sc

re
pa

nc
y

unseen seen

Figure 5: F1-score discrepancy between baseline and best sys-
tem using GloVe feature for BIO tags that occur more than 50

times in the test set distinguishing between seen and unseen to-

kens.

945


