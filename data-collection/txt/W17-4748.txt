



















































The AFRL-OSU WMT17 Multimodal Translation System: An Image Processing Approach


Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 445–449
Copenhagen, Denmark, September 711, 2017. c©2017 Association for Computational Linguistics

The AFRL-OSU WMT17 Multimodal Translation System:
An Image Processing Approach

John Duselis, Michael Hutt,
Jeremy Gwinnup

Air Force Research Laboratory
{john.duselis,michael.hutt.ctr,

jeremy.gwinnup.1}@us.af.mil

James W. Davis
Joshua Sandvick

Ohio State University
{davis.1719,sandvick.6}@osu.edu

Abstract

This paper introduces the AFRL-OSU
Multimodal Machine Translation Task 1
system for submission to the Conference
on Machine Translation 2017 (WMT17).
This is an atypical MT system in that the
image is the catalyst for the MT results,
and not the textual content.

1 Introduction

Contemporary scientific meetings have examined
the potential benefits of fusing image information
with machine translation. For instance, the leading
international conference in this area, the Confer-
ence onMachine Translation (WMT), is approach-
ing its second year of competition on Multimodal
Machine Translation (MMT). First year results in
WMT16’s Multimodal Task 1 were varied in ap-
proaches, informative in their findings, and indi-
cated potential opprtunities for multimodal system
improvement. (Specia et al., 2016).
In the WMT16 submissions, the seemingly pre-

dominant focal point across the systems was the
fact that textual information was the driver for the
translation. The image features tended towards be-
ing ancilliary inputs or outputs (Libovický et al.,
2016; Guasch and Costa-Jussà, 2016; Caglayan
et al., 2016) or decision-type functions (Shah et al.,
2016) and not the main antagonist for translation
(Specia et al., 2016; Elliott et al., 2015). This is
sensible as it is an MT competition. However, ap-
proaching it from another direction, namely, hav-
ing the image as the driver for the translation
presents a different point of view worth investigat-
ing.

This work is sponsored by the Air Force Research Labo-
ratory under AFRL/711 Human PerformanceWing Chief Sci-
entist Funding.

The following sections will outline the seem-
ingly novel approach to MMT and give particulars
of this unconstrained system.

2 AFRL-OSU System

This section will outline the architecture of the sys-
tem. This is a first approximation into the pro-
cess but is expected to undergo further develop-
ment based on insights from this competition.

2.1 General Overview

Referencing Fig. 1, a generic example taken from
(Specia et al., 2016) shows a method where the
source caption and image are the drivers for the
multimodal translation. In some ofWMT16’s sub-
missions, the decomposition of the image is incor-
porated as an additional feature into the MMT sys-
tem, while others used the features as a function to
help pick the best translation.
AFRL-OSU’s system is pictorially represented

in Figure 2. Currently, there is much work in
image captioning systems (Socher et al., 2014;
Ghahramani et al., 2014; Mao et al., 2014; Kiros
et al., 2014; Vinyals et al., 2015), and WMT17 has
even set out a task in its competition for it. Our em-
phasis is not to try to produce a multilingual image
captioning system, rather to use one to accomplish
MT as the maturity of the caption engine research
progresses.
This system architecture assumes an image cap-

tion engine can be trained in a target language to
give meaningful output in the form of a set of the
most probable n target language candidate cap-
tions. A learned mapping function of the encoded
source language caption to the corresponding en-
coded target language candidate captions is thusly
employed. Finally, a distance function is applied
to retrieve the “nearest” candidate caption to be the
translation of the source caption.

445



Source	Caption:
A	brown	dog	is	running	
after	the	black	dog.

Translate Ein brauner Hund	läuft nachdem schwarzen Hund.

Evaluate

Reference:
Ein brauner Hund	rennt
dem schwarzen Hund	

hinterher.

Figure 1: A Text-based Model for Multimodal Machine Translation adapted from (Specia et al., 2016)

Image	generated	German	
candidate	captions:

Ein schwarzer und	ein …
Zwei hunde rennen …

…
Zwei Hunde tollen in…

Source	English	Caption:
A	brown	dog	is	running	after	

the	black	dog.

Source	to	Target	Transfer	Mapping

Functional
retrieval	of
appropriate
candidate
caption

Encode Encode

Ein brauner Hund	rennt dem schwarzen
Hund	hinterher.

Non-traditional	MT	determined	output

Figure 2: An Image-based Model for Multimodal Machine Translation.

2.2 Theoretical Overview
Details of the system architecture are illustrated in
Figure 3. Given an image i (top left), using an
image captioning engine trained in the target lan-
guage t, we produce n candidate captions: Ctij for
j = 1, ..., n.
After obtaining the candidate sentences, we

transform them into a fixed vector length encod-
ing with

vtij = G
t(Ctij ) (1)

where Gt(·) is the target encoder.
Similarly (from the top right of Figure 3), the

source language caption Csi is encoded using

vsi = G
s(Csi ) (2)

where Gs(·) is the source encoder.

At this point, both the target captions and the
source caption are encoded in separate monolin-
gual, monomodal subspaces. In order to exe-
cute the retrieval process, we develop a transfer
mapping of the source language encodings to the
space of target language encodings. We learn this
source-to-target mapping using training pairs of
source language encodings and target language en-
codings provided by traditional MT of the source
language examples (Sennrich et al., 2016). Hence
the mapping attempts to learn MT translation from
the encoding representations themselves. The ar-
chitecture employed is a multi-layer neural net-
work.

446



Figure 3: Architectural Diagram of the Processing Chain

2.3 Implementation

The actual AFRL-OSU unconstrained implemen-
tation went through many iterations of tool sets
before settling. The captioning engine selected
for this competition was Google’s Show and Tell
system (Vinyals et al., 2015). It was trained on
the WMT16 training and validation splits using
the MultiFlickr30K images and German (Elliott
et al., 2016) and ImageClef 2006-2008 multilin-
gual datasets (Rorissa et al., 2006). For testing,
1000 captions (Ctij for j = 1, ..., 1000) per image
were produced. Any caption with sentence length
less than five words was not considered, but was
not replaced. Captions were put into all lowercase
without punctuation.
The monolingual word encodings, Gt and Gs,

used to vector encode the source language caption
and target language captions employed the word
encodings compiled and published by Facebook
(Bojanowski et al., 2016). Because Facebook’s
data was chosen over any word encodings pro-
duced internally, vector length was fixed at 300.
This dataset was produced by Facebook by crawl-
ing and cleaning up data fromWikipedia pages us-
ing their fastText software and encoding algorithm

outlined in (Bojanowski et al., 2016). Sentence en-
codings used in the AFRL-OSU system were de-
rived from averaging of in-vocabulary constituent
word encodings.
To transform source encoded data into the tar-

get language encoded subspace, a multi-layer neu-
ral network was constructed. The WMT16 train-
ing/validation splits were used for the training En-
glish source captions (5 captions per image with
a total of 29000 images). These English cap-
tions were encoded into 300x1 vectors, each L2-
normalized. The training target outputs were gen-
erated using Edinburgh’sWMT16NeuralMT Sys-
tem (Sennrich et al., 2016) to translate captions
from English to German in the same 300x1 vec-
tor format, and again L2-normalized. The neural
network was configured with 1 hidden layer (500
nodes) and a mean squared error loss-function. To
test the approach 10% of the training data was kept
for evaluation. During training, 25% of the re-
maining training data was withheld for validation
with a maximum of 10000 epochs. The result-
ing network provides a source-to-target mapping
of the source caption encoding

v̂ti = Net(v
s
i ) (3)

447



100 200 300 400 500 600 700 800 900 1000

Number of Candidates

0

1

2

3

4

5

6

C
o

u
n
t

Histogram of Indices for Best Match

100 200 300 400 500 600 700 800 900 1000

Number of Candidates

0

100

200

300

400

C
o

u
n
t

Histogram of Indices for Sentence Lengths <5

Figure 4: Histogram analysis. Top: Histogram of
indices for the best match in the candidates. Bot-
tom: Histogram of indices for candidate sentences
with invalid (<5) length.

We lastly used the squared Euclidean Distance
between the source transformed English caption
encoding v̂ti to the collection of candidate target
caption encodings to select the best candidate sen-
tence index j∗

j∗ = argmin
j

||v̂ti − vtij ||22 (4)

The “best” match (to the source language cap-
tion) produced from the captioning engine is the
sentence Ctij∗ . From the test data (with ground
truth source-to-target labels), we received a top-1
of 77% and top-5 of 87%.

3 Results

The final submission consisted of generating 1000
captions per image with the top score being se-
lected. The minimum of 5 words per sentence dis-
counted 150963 candidate captions.
The top caption satisfying Eq.4 as the minimal

value was scored against the output from the Edin-
burgh WMT16 Neural MT system and had a ME-
TEOR (Denkowski and Lavie, 2014) score of 19.8
(Sennrich et al., 2016). Figure 4 provides some
trends for locations of zero vectors and top scoring
vectors.

4 Conclusion

Assuming sufficient baseline results from an
image-centric MMT system evaluated in this com-
petition, there exist several opportunities for un-

derstanding the implications of such a system and
also to improve its capabilities.
The captioning engine used is employed as a

black box and assumed meaningful output for pro-
cessing. Knowing the inner workings of the cap-
tion engine should allow tuning to produce more
meaningful results. The authors also look for-
ward to the results of this Multimodal Competi-
tion’s Task 2 to obtain a better captioning engine
(either improvements on the current system, or a
different method altogether).
The monolingual word encodings attained from

the Facebook models were constrained to 300 ele-
mental vector length. Exploration into not only the
size, but also construction of the data is warranted.
The cost function used, squared Euclidean Dis-

tance, is a first attempt. Looking at a variety of
functions may harvest better results.
The authors only submitted the top ranked cap-

tion for scoring in this competition. However,
33 candidate submissions received a 0.0 sentence
level METEOR score. Therefore, approaching a
selection from the Topm captions that would max-
imize the METEOR is worth investigating.
This paper outlined the AFRL-OSU WMT17

Multimodal Translation system where the image
is the focal point for MT. The authors hope that
it spurs some alternative thinking and research in
the area of multimodal MT.

Acknowlegements

The authors wish to thank Rebecca Young for her
involvement in the human evaluation portion of
the WMT17 Multimodal Translation task. The au-
thors also wish to thank Rico Sennrich for mak-
ing models and data available from the Edinburgh
WMT16 Neural MT system, saving valuable time
and effort during development.

References
2016. Proceedings of the First Conference on

Machine Translation, WMT 2016, colocated
with ACL 2016, August 11-12, Berlin, Ger-
many. The Association for Computer Linguistics.
http://aclweb.org/anthology/W/W16/.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-

Opinions, interpretations, conclusions and recommen-
dations are those of the authors and are not necessarily en-
dorsed by the United States Government. Cleared for public
release on 22 May 2017. Originator reference number RH-
17-117140. Case number 88ABW-2017-2503.

448



tors with subword information. arXiv preprint
arXiv:1607.04606 .

Ozan Caglayan, Walid Aransa, Yaxing Wang,
Marc Masana, Mercedes García-Martínez, Fethi
Bougares, Loïc Barrault, and Joost van de Weijer.
2016. Does multimodality help human and machine
for translation and image captioning? CoRR
abs/1605.09186. http://arxiv.org/abs/1605.09186.

Michael Denkowski andAlon Lavie. 2014. Meteor uni-
versal: Language specific translation evaluation for
any target language. In Proceedings of the EACL
2014 Workshop on Statistical Machine Translation.

D. Elliott, S. Frank, K. Sima’an, and L. Specia. 2016.
Multi30k: Multilingual english-german image de-
scriptions pages 70–74.

Desmond Elliott, Stella Frank, and Eva Hasler.
2015. Multi-language image description with
neural sequence models. CoRR abs/1510.04709.
http://arxiv.org/abs/1510.04709.

Zoubin Ghahramani, Max Welling, Corinna Cortes,
Neil D. Lawrence, and Kilian Q. Weinberger,
editors. 2014. Advances in Neural Informa-
tion Processing Systems 27: Annual Confer-
ence on Neural Information Processing Systems
2014, December 8-13 2014, Montreal, Quebec,
Canada. http://papers.nips.cc/book/advances-in-
neural-information-processing-systems-27-2014.

Sergio Rodríguez Guasch and Marta R. Costa-
Jussà. 2016. WMT 2016 multimodal trans-
lation system description based on bidirec-
tional recurrent neural networks with double-
embeddings. In (DBL, 2016), pages 655–
659. http://aclweb.org/anthology/W/W16/W16-
2362.pdf.

Ryan Kiros, Ruslan Salakhutdinov, and Richard S.
Zemel. 2014. Unifying visual-semantic embeddings
with multimodal neural language models. CoRR
abs/1411.2539. http://arxiv.org/abs/1411.2539.

Jindrich Libovický, Jindrich Helcl, Marek Tlustý,
Pavel Pecina, and Ondrej Bojar. 2016. CUNI sys-
tem for WMT16 automatic post-editing and mul-
timodal translation tasks. CoRR abs/1606.07481.
http://arxiv.org/abs/1606.07481.

JunhuaMao, Wei Xu, Yi Yang, JiangWang, and Alan L
Yuille. 2014. Explain images withmultimodal recur-
rent neural networks. NIPS Deep Learning Work-
shop .

Abebe Rorissa, Paul Clough, William Hersh, Abebe
Rorissa, and Miguel Ruiz. 2006. Imageclef
and imageclefmed: Toward standard test collec-
tions for image storage and retrieval research.
Proceedings of the American Society for In-
formation Science and Technology 43(1):1–6.
https://doi.org/10.1002/meet.14504301130.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Edinburgh neural machine translation
systems for WMT 16. CoRR abs/1606.02891.
http://arxiv.org/abs/1606.02891.

Kashif Shah, Josiah Wang, and Lucia Specia.
2016. Shef-multimodal: Grounding machine
translation on images. In (DBL, 2016), pages 660–
665. http://aclweb.org/anthology/W/W16/W16-
2363.pdf.

Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. TACL 2:207–218.

Lucia Specia, Stella Frank, Khalil Sima’an, and
Desmond Elliott. 2016. A shared task on multi-
modal machine translation and crosslingual image
description. In Proceedings of the First Conference
on Machine Translation. Association for Computa-
tional Linguistics, Berlin, Germany, pages 543–553.
http://www.aclweb.org/anthology/W/W16/W16-
2346.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In 2015 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).

449


