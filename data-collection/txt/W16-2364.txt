



















































DOCAL - Vicomtech's Participation in the WMT16 Shared Task on Bilingual Document Alignment


Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 666–671,
Berlin, Germany, August 11-12, 2016. c©2016 Association for Computational Linguistics

DOCAL - Vicomtech’s Participation in the WMT16 Shared Task on
Bilingual Document Alignment

Andoni Azpeitia and Thierry Etchegoyhen
Vicomtech-IK4

Mikeletegi Pasalekua, 57
Donostia / San Sebastian, Gipuzkoa, Spain

{aazpeitia, tetchegoyhen}@vicomtech.org

Abstract

This article presents the DOCAL system
for document alignment, which took part
in the WMT 2016 shared task on bilin-
gual document alignment. The system is
meant to offer a portable solution for var-
ied document alignment scenarios, from
parallel to comparable corpora, with mini-
mal deployment effort. Its main goal is to
provide an optimal balance between align-
ment precision and recall using minimal
resources and adaptation across alignment
scenarios. We describe and discuss the
performance of the system in the recall-
oriented shared task.

1 Introduction

Parallel corpora are essential to the develop-
ment of data-driven approaches to translation such
as statistical machine translation (Brown et al.,
1990). As it feeds further processes in the creation
of bitexts, multilingual document alignment plays
an important role in building accurate resources.

This article presents the DOCAL system for doc-
ument alignment, which took part in the WMT
2016 shared task on bilingual document align-
ment. The system is meant to offer a portable
solution for varied document alignment scenarios,
from parallel to comparable corpora.

The alignment of multilingual documents has
been performed with a variety of techniques over
the years, with alternatives targeting various sce-
narios, from parallel to weakly comparable cor-
pora.

Simple approaches based on file name match-
ing can provide fast document pairing, as they do
not rely on any analysis of the content of docu-
ments. Unfortunately, these approaches rely on

consistent file naming conventions, an assump-
tion which is often defeated in practice (Tiede-
mann, 2011). This approach is thus often com-
plemented with content-based alignment methods,
as in (Chen et al., 2004), whose system includes
a filename-based module and a semantic similar-
ity component based on a vector space model with
frequency-weighted term vectors.

The usefulness of document metadata for doc-
ument alignment has been explored in depth by
(Resnik and Smith, 2003), who exploit URL prop-
erties and structural tags to gather bilingual cor-
pora from HTML pages on the Web. (Chen and
Nie, 2000) is another example of an approach
that exploits URL properties, along with docu-
ment size and language identifiers. (Munteanu and
Marcu, 2005) use date-aligned documents as input
for their binary classification approach to compa-
rable sentence alignment.

To address comparable corpora specifically, dif-
ferent types of content-based approaches have
been proposed. (Fung and Cheung, 2004), for in-
stance, present the first exploration of very non-
parallel corpora using a document similarity mea-
sure based on bilingual lexical matching defined
over mutual information scores on word pairs.
(Patry and Langlais, 2005) present a feature-based
method based on an Ada-Boost classifier that in-
cludes features such as length, entities, and punc-
tuation, along with a filtering component to re-
move alignment duplicates. The BITS system is
another alternative proposed by (Ma and Liber-
man, 1999) for bilingual text mining on the Web,
measuring content similarity by counting the ratio
of token translation pairs over the total number of
tokens in the source document, where translation
pairs are determined within fixed windows of text.

Other general methods include (Ion et al.,
2011), who propose an approach based on
expectation-maximization using bilingual lexi-

666



cons, and (Li and Gaussier, 2013), whose com-
parability metric measures the overall proportion
of words for which a translation can be found in a
comparable corpus using bilingual dictionaries.

The Jaccard coefficient (Jaccard, 1901), which
is a core component of DOCAL, has been used for
instance by (Paramita et al., 2013) whose com-
parable document similarity measure is partially
based on this metric computed over a subset of
sentence pairs in the documents.

DOCAL (Etchegoyhen and Azpeitia, 2016) is
a simple method to measure multilingual docu-
ment similarity, whose main goal is to provide
an optimal balance between alignment precision
and recall with minimal resources and adaptation
across alignment scenarios. The next sections de-
scribe the system and its performance in the recall-
oriented shared task.

2 DOCAL

The core of the DOCAL approach relies on ex-
panded lexical translation sets, defined at the doc-
ument level, and the Jaccard coefficient com-
puted on those sets. Two token sets are thus ex-
tracted from each pair of documents, along with
two corresponding sets containing lexical transla-
tions of the tokens. The translation sets are then
augmented through set expansion operations, de-
scribed below, and similarity is computed as the
ratio of intersection over union on the original to-
ken sets and their corresponding translation sets.

Formally, the following components are gener-
ated for each document pair:

• di and dj : tokenised documents in languages
l1 and l2, respectively.

• Si: set of tokens in di.

• Sj : set of tokens in dj .

• Tij : set of expanded lexical translations into
l2 for all tokens in Si.

• Tji: set of expanded lexical translations into
l1 for all tokens in Sj .

From these elements, the similarity score is
computed as in Equation 1:

simdocal =

|Tij∩Sj |
|Tij∪Sj | +

|Tji∩Si|
|Tji∪Si|

2
(1)

In other words, the score is defined as the aver-
age of the document-level Jaccard similarity coef-
ficients computed in both translation directions.

Lexical translations are extracted from seed par-
allel corpora, with translation probabilities com-
puted according to IBM models (Brown et al.,
1993).1 For each token, the k-best translation op-
tions are selected among the alternatives ranked
according to their lexical translation probability.
The actual probability values are not used beyond
the ranking they enable, i.e. all selected transla-
tions are equally considered in the computation of
similarity. This is meant to abstract away from dif-
ferences in lexical distributions between the seed
corpora used to create translation tables and the
data in the domain at hand, which is often of a dif-
ferent nature.

No filtering is performed on the token sets, leav-
ing punctuation marks alongside functional and
content words, and the text is preserved with its
original capitalisation. Pre-processing is thus re-
duced to the minimal operation of tokenisation.

We now describe in turn the aforementioned set
expansion operations, the retrieval of alignment
candidates, and the available optimisations of the
core method.

2.1 Set Expansion

Since lexical translation tables cannot be expected
to cover a given domain satisfactorily, the transla-
tion sets are expanded with tokens that may be in-
dicators of similarity, although absent from trans-
lation tables. First, all capitalised tokens are added
to the sets if they are not found in the translation
tables.2 This simple operation, which we perform
at set creation time, provides coverage for named
entities, which can be viewed as important indica-
tors of content similarity given their low relative
frequency. The same process applies to numbers
as well, which can also be strong indicators of sim-
ilarity, in particular when they denote dates.

DOCAL includes an additional set expansion op-
eration based on longest common prefixes (LCP),
which are computed over the minimal sets of ele-
ments that may have a common stem, defined to be
the following two set differences: T ′ij = Tij − Sj

1We use GIZA++ (Och and Ney, 2003) to extract lexical
translation tables.

2Checking for their presence in lexical translation tables
allows one to distinguish between out-of-vocabulary tokens
and entities with an existing translation, e.g. Germany trans-
lated into Spanish Alemania.

667



and T ′ji = Tji − Si. For each element in T ′ij (re-
spectively T ′ji) and each element in Sj (respec-
tively Si), if a common prefix is found with an
empirically set minimal length of n characters, the
prefix is added to both sets. This specific expan-
sion operation is not included by default in the ac-
tual usage of the system, as it increases the over-
all computational cost and its benefits are largely
dependent on the specifics of the corpora and lan-
guage pairs at hand.

2.2 Alignment Candidates
Alignments are computed from source to target
documents, with the additional filtering described
in Section 2.3.

In some document alignment scenarios, an
alignment process based on the Cartesian prod-
uct of the document sets might be the optimal ap-
proach, as the alignment space is guaranteed to
be searched exhaustively. Since this approach has
quadratic complexity, it is however computation-
ally prohibitive if the volumes of documents reach
a certain amount.

For scenarios where the volume of documents
renders an exhaustive comparison unsustainable,
a standard cross-linguistic information retrieval
(CLIR) approach is provided. Target documents
are first indexed using the Lucene search engine3

and retrieved by building a query over the ex-
panded translation sets created from each source
document. This strategy drastically reduces the
overall processing time and resource consump-
tion, at the cost of missing some correct alignment
pairs.4

2.3 Alignment Filtering
As the alignment process is executed from source
to target documents, a given target document can
be selected as the best alignment for more than one
source document. This results in hidden correct
alignments, often with scores that are marginally
lower than the top alignment scores assigned by
the similarity metric.

A simple solution to this issue consists in re-
moving all alignments between a source docu-
ment and a target if the latter is aligned to a
different source document with a better similar-
ity score. That is, we remove alignment tuples
(di, dj , simij) between any two documents di and

3https://lucene.apache.org.
4In experiments on different datasets, the loss of correct

alignment pairs was minimal, at around 1% per test set.

dj if there exists a different tuple (dk, dj , simkj)
such that simkj > simij .

This process often produces large improve-
ments, as it allows previously hidden correct align-
ments to surface, and is included by default in DO-
CAL.

3 WMT 2016 Bilingual Document
Alignment Task

The WMT 2016 shared task on multilingual doc-
ument alignment5 consists in identifying pairs of
English and French documents from a given col-
lection of documents such that one document is
the translation of the other. Candidate pairs were
defined as all pairs of documents from the same
web domain for which the source side has been
identified as mostly English and the target side as
mostly French.

Participants were to submit a list of possible
pairings, with each source URL matched with at
most one target URL and vice-versa. The evalu-
ation metric was selected to be recall on the test
set, i.e. the percentage of the test-set pairs that a
participating system could find after enforcing the
1-1 alignment rule.

Our participation in the shared task was meant
to check the effectiveness of DOCAL in a new
large-scale document alignment task with no task-
specific adaptation, in accordance with our stated
aim at portability and ease of deployment across
document alignment scenarios. Thus, the system
was applied in its default configuration and the
provided training datasets were not used beyond
testing the processing tools provided for the task.
Document metadata or URL properties were not
exploited either, to strictly measure our content-
based approach to document alignment.

In the next section, we describe the setup for our
system, with results presented in Section 3.2.

3.1 System Setup
As mentioned above, DOCAL was applied in its de-
fault configuration. Lexical translation tables were
created with GIZA++ on the JRC-Acquis Commu-
nautaire corpus.6 For the English-French pair, the
training corpus consisted in 708.896 aligned sen-
tences. No experiments were made with different
translation tables, larger or more varied, although

5http://www.statmt.org/wmt16/bilingual-task.html.
6We used the latest available version of the cor-

pus, as of November 2015, in the OPUS repository:
http://opus.lingfil.uu.se/JRC-Acquis.php.

668



we view this research path as worth exploring in
future work.

We set k = 5 to define the range of k-best lex-
ical translations, as a compromise between larger
sets with less reliable translation candidates and
smaller sets which may miss translation alterna-
tives. Note that this value could have been tuned
on the provided training data, thus optimising the
setting to this specific task. However, as previ-
ously mentioned, our goal was to evaluate the ap-
proach with portability in mind, where no particu-
lar adaptation is performed; we therefore used this
default value for the k parameter.

Document content was tokenised using the
scripts provided in the Moses toolkit (Koehn et
al., 2007). For all but four web domains in the
test set, the set of possible alignment pairs was
computed using the Cartesian product of source-
target documents, as this guaranteed an exhaustive
search in the alignment space and the computation
was deemed practical for up to 260 million pos-
sible pairings.7 The remaining four domains fea-
tured potential pairs above the 300 million mark
and the CLIR approach using Lucene was used in
those cases.8

Finally, DOCAL was used with alignment filter-
ing, as described in Section 2.3, and without the
set expansion operation based on longest common
prefixes described in Section 2.1.

3.2 Results and Discussion

Overall, DOCAL ranked in 5th place on the official
test set, with 2128 pairs retrieved out of 2402 for a
recall score of 88.59%. It is interesting to note that
several systems, and in particular all four systems
with better scores, have submitted a significantly
larger number of pairs than DOCAL, which is in-
dicative of underlying differences in terms of pre-
cision and f-measure. However, without knowing
the correctness of the alignments outside the test
set pairs, it is obviously not possible to determine
whether these differences show better precision on
the part of DOCAL or not.

While performing an error analysis of the cases
where our system had retrieved the incorrect pair
according to the test set, we found 100 cases where
the test set contained what we consider to be incor-

7The documents were processed on a single server with
64G of RAM and 16 cores.

8The domains were: www.domainepechlaurier.com;
www.desmarais-robitaille.com; italiasullarete.it; and: egode-
sign.ca.

rect alignments. That is, in all 100 cases, shown in
Table 1,9 the target pair found by DOCAL seems
to us to be the correct one. In most of these cases,
the French documents in the test set and the one re-
trieved by DOCAL were nearly identical, with only
minor differences where the test set document was
missing a small portion of information from the
source document.10

These cases account for 4.16% of the test, and
impact the final results, as shown in Table 2.11 On
the corrected test set, DOCAL reaches a score of
92.76%, significantly better than its result on the
original test set.

It is of course entirely possible that other partic-
ipating systems had actually retrieved the correct
target documents as well in those cases, and that
the final ranking of systems would thus be unaf-
fected. Whether this is actually the case or not is
unknown to us at the time of this writing.

4 Conclusion

Overall, we found the results obtained by DOCAL
on the shared task to be satisfactory, in particu-
lar as a test case for the portability of the default
method in a new large-scale alignment scenario.

The system was developed to seek an opti-
mal balance between precision and recall, and
has shown promising results along these lines
in different scenarios involving both parallel and
comparable corpora (Etchegoyhen and Azpeitia,
2016). In future tasks, it would be interesting to
compare our approach to alternatives in terms of
f-measure as well, to fully assess the usefulness
of available methods for multilingual document
alignment.

Acknowledgments

This work was partially funded by the Spanish
Ministry of Economy and Competitiveness and the

9As many of the erroneous cases came from a single do-
main, namely www.lalettrediplomatique.fr, we indicate the
URL structure once where replacing the place-holder X with
one of the values in the last line forms the actual URL. Note
also that we indicate ranges with a dash, e.g., X = 15-17 in-
dicates that all values from 15 to 17 (included) lead to a URL
that is in the set of identified errors.

10For instance, 94 of the cases came from the domain
www.lalettrediplomatique.fr, where the English source doc-
ument content contains a date which is accurately translated
in the document retrieved by DOCAL, and incorrect in the tar-
get document in the test set.

11wmt2016 corr denotes the corrected version of the test
set.

669



Source: http://artfactories.net/Espace-Linga-Tere.html
Test set: http://artfactories.net/-Republique-centrafricaine-.html
Correct: http://artfactories.net/Espace-Linga-Tere-Bangui.html
Source: http://www.ipu.org/hr-e/169/Co121.htm
Test set: http://www.ipu.org/hr-f/168/Co121.htm
Correct: http://www.ipu.org/hr-f/169/Co121.htm
Source: http://www.lifegrid.fr/en/projets/projects/biomedicale-search.html
Test set: http://www.lifegrid.fr/fr/projets/appel-a-projets-e-nnovergne-lifegrid-2006/recherche-biomedicale.html
Correct: http://www.lifegrid.fr/fr/projets/31-recherche-biomedicale.html
Source: http://www.nserc-crsng.gc.ca/Prizes-Prix/Excellence-Excellence/Profiles-Profils eng.asp?ID=1008
Test set: http://www.nserc-crsng.gc.ca/Prizes-Prix/Herzberg-Herzberg/Profiles-Profils fra.asp?ID=1003
Correct: http://www.nserc-crsng.gc.ca/Prizes-Prix/Excellence-Excellence/Profiles-Profils fra.asp?ID=1008
Source: http://www.rfimusique.com/musiqueen/articles/060/article 6465.asp
Test set: http://www.rfimusique.com/musiquefr/articles/060/article 14625.asp
Correct: http://www.rfimusique.com/musiquefr/articles/060/article 13250.asp
Source: http://www.rfimusique.com/musiqueen/articles/129/article 8397.asp
Test set: http://www.rfimusique.com/musiquefr/articles/128/article 18057.asp
Correct: http://www.rfimusique.com/musiquefr/articles/129/article 18094.asp
Source: http://www.lalettrediplomatique.fr/contribution.php?choixlang=2&id=10&idrub=X
Test set: http://www.lalettrediplomatique.fr/contribution.php?id=10&idrub=X
Correct: http://www.lalettrediplomatique.fr/contribution.php?choixlang=1&id=10&idrub=X
X = 5, 7, 11-12, 15-17, 23, 28-31, 35, 37-39, 43, 45-46, 50-52, 56-58, 61-65, 69, 83-84, 86, 89, 91-94, 96-100,

103-107, 109-111, 114-115, 119-120, 123-125, 127-130, 133-135, 137-141, 144, 146, 149-152, 155-156,
158, 160-163, 165-167, 169, 173, 175, 177, 194, 197

Table 1: Identified likely errors in the test set

TEST SETS FOUND PAIRS SUBMITTED PAIRS PAIRS AFTER 1-1 RULE RECALL
wmt2016 2.128 191.993 191.993 88.592839

wmt2016 corr 2.228 191.993 191.993 92.756037

Table 2: DOCAL results

Department of Economic Development and Com-
petitiveness of the Basque Government through
the AdapTA (RTC-2015-3627-7) and TRADIN
(IG-2015/0000347) projects. We would like to
thank MondragonLingua Translation & Commu-
nication for their support as coordinator of these
projects, and the organisers of the shared task for
their work and support.

References

Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Fredrick Jelinek, John D Laf-
ferty, Robert L Mercer, and Paul S Roossin. 1990.
A statistical approach to machine translation. Com-
putational linguistics, 16(2):79–85.

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263–311.

Jiang Chen and Jian-Yun Nie. 2000. Parallel Web
Text Mining for Cross-language IR. In Content-
Based Multimedia Information Access - Volume 1,
RIAO ’00, pages 62–77, Paris, France, France. Cen-
tre des hautes tudes internationales d’informatique
documentaire.

Jisong Chen, Rowena Chau, and Chung-Hsing Yeh.
2004. Discovering parallel text from the world wide
web. In Proceedings of the second workshop on
Australasian information security, Data Mining and
Web Intelligence, and Software Internationalisation-
Volume 32, pages 157–161. Australian Computer
Society, Inc.

Thierry Etchegoyhen and Andoni Azpeitia. 2016. A
Portable Method for Parallel and Comparable Doc-
ument Alignment. Baltic Journal of Modern Com-
puting, 4(2):243–255. Special Issue: Proceedings
of EAMT 2016.

Pascale Fung and Percy Cheung. 2004. Mining Very
Non-Parallel Corpora: Parallel Sentence and Lexi-
con Extraction via Bootstrapping and E.M. In Pro-
ceedings of Empirical Methods in Natural Language
Processing, pages 57–63.

Radu Ion, Alexandru Ceauşu, and Elena Irimia. 2011.
An expectation maximization algorithm for textual
unit alignment. In Proceedings of the 4th Workshop
on Building and Using Comparable Corpora: Com-
parable Corpora and the Web, pages 128–135. As-
sociation for Computational Linguistics.

Paul Jaccard. 1901. Distribution de la flore alpine
dans le bassin des Dranses et dans quelques régions
voisines. Bulletin de la Société Vaudoise des Sci-
ences Naturelles, 37:241 – 272.

670



Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL,
pages 177–180. Association for Computational Lin-
guistics.

Bo Li and Eric Gaussier. 2013. Exploiting comparable
corpora for lexicon extraction: Measuring and im-
proving corpus quality. In Building and Using Com-
parable Corpora, pages 131–149. Springer.

Xiaoyi Ma and Mark Liberman. 1999. Bits: A method
for bilingual text search over the web. In Machine
Translation Summit VII, pages 538–542.

Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477–504.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19–51.

Monica Lestari Paramita, David Guthrie, Evangelos
Kanoulas, Rob Gaizauskas, Paul Clough, and Mark
Sanderson. 2013. Methods for collection and evalu-
ation of comparable documents. In Building and Us-
ing Comparable Corpora, pages 93–112. Springer.

Alexandre Patry and Philippe Langlais. 2005. Au-
tomatic identification of parallel documents with
light or without linguistic resources. In Proceedings
of the 18th Canadian Society Conference on Ad-
vances in Artificial Intelligence, AI’05, pages 354–
365, Berlin, Heidelberg. Springer-Verlag.

Philip Resnik and Noah A Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349–380.

Jörg Tiedemann. 2011. Bitext Alignment. Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool Publishers.

671


