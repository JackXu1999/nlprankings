



















































SemRegex: A Semantics-Based Approach for Generating Regular Expressions from Natural Language Specifications


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1608–1618
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1608

SemRegex: A Semantics-Based Approach for Generating Regular
Expressions from Natural Language Specifications
Zexuan Zhong1, Jiaqi Guo2, Wei Yang3, Jian Peng1, Tao Xie1,

Jian-Guang Lou4, Ting Liu2, Dongmei Zhang4
1University of Illinois at Urbana-Champaign

2Xi’an Jiaotong University, 3University of Texas at Dallas
4Microsoft Research Asia

{zexuan2,jianpeng,taoxie}@illinois.edu
jasperguo2013@stu.xjtu.edu.cn, tingliu@mail.xjtu.edu.cn
wei.yang@utdallas.edu, {jlou,dongmeiz}@microsoft.com

Abstract
Recent research proposes syntax-based ap-
proaches to address the problem of gener-
ating programs from natural language spec-
ifications. These approaches typically train
a sequence-to-sequence learning model using
a syntax-based objective: maximum likeli-
hood estimation (MLE). Such syntax-based
approaches do not effectively address the goal
of generating semantically correct programs,
because these approaches fail to handle Pro-
gram Aliasing, i.e., semantically equivalent
programs may have many syntactically differ-
ent forms. To address this issue, in this pa-
per, we propose a semantics-based approach
named SemRegex. SemRegex provides solu-
tions for a subtask of the program-synthesis
problem: generating regular expressions from
natural language. Different from the existing
syntax-based approaches, SemRegex trains the
model by maximizing the expected semantic
correctness of the generated regular expres-
sions. The semantic correctness is measured
using the DFA-equivalence oracle, random test
cases, and distinguishing test cases. The ex-
periments on three public datasets demonstrate
the superiority of SemRegex over the existing
state-of-the-art approaches.

1 Introduction

Translating natural language (NL) descriptions
into executable programs is a fundamental prob-
lem for computational linguistics. An end user
may have difficulty to write programs for a certain
task, even when the task is already specified in NL.
For some tasks, even for developers, who have ex-
perience in writing programs, it can be time con-
suming and error prone to write programs based
on the NL description of the task. Naturally, au-
tomatically synthesizing programs from NL can
help alleviate the preceding issues for both end
users and developers.

Recent research proposes syntax-based ap-
proaches to address some tasks of this problem

in different domains, such as regular expressions
(regex) (Locascio et al., 2016), Bash scripts (Lin
et al., 2017), and Python programs (Yin and Neu-
big, 2017). These approaches typically train a
sequence-to-sequence learning model using max-
imum likelihood estimation (MLE). Using MLE
encourages the model to output programs that are
syntactically similar with the ground-truth pro-
grams in the training set. However, such syntax-
based training objective deviates from the goal
of synthesizing semantically equivalent programs.
Specifically, these syntax-based approaches fail to
handle the problem of Program Aliasing (Bunel
et al., 2018), i.e., a semantically equivalent
program may have many syntactically different
forms. Table 1 shows some examples of the Pro-
gram Aliasing problem. Both Program 1 and
Program 2 are desirable outputs for the given
NL specification but one of them is penalized by
syntax-based approaches if the other one is used
as the ground truth, compromising the overall ef-
fectiveness of these approaches.

In this paper, we focus on generating regu-
lar expressions from NL, an important task of
the program-synthesis problem, and propose Sem-
Regex, a semantics-based approach to generate
regular expressions from NL specifications. Regu-
lar expressions are widely used in various applica-
tions, and “regex” is one of the most common tags
in Stack Overflow1 with more than 190, 000 re-
lated questions. The huge number of regex-related
questions indicates the importance of this task.

Different from the existing syntax-based ap-
proaches, SemRegex alters the syntax-based train-
ing objective of the model to a semantics-based
objective. To encourage the translation model to
generate semantically correct regular expressions,
instead of MLE, SemRegex trains the model by
maximizing the expected semantic correctness of

1https://stackoverflow.com/questions/
tagged/regex

https://stackoverflow.com/questions/tagged/regex
https://stackoverflow.com/questions/tagged/regex


1609

Table 1: Examples of Program Aliasing: for each NL specification, Program 2 is semantically equivalent
to Program 1; however, if Program 1 is the ground truth in the training set, Program 2 is penalized by
syntax-based approaches although it is a desirable program.

Domain NL Specification Program 1 Program 2

Regex
Match lines that start with an uppercase
vowel and end with ‘X’ ([AEIOUaeiou]&[A-Z]).*X ([AEIOU].*)&(.*X)

Bash Rename file ‘f1’ to ‘f1.txt’ mv ’f1’ ’f1.txt’ cp ’f1’ ’f1.txt’; rm ’f1’

Python
Assign the greater value of ‘a’ and
‘b’ to variable ‘c’ c = a if a > b else b c = [b, a][a > b]

generated regular expressions. We follow the tech-
nique of policy gradient (Williams, 1992) to esti-
mate the gradients of the semantics-based objec-
tive and perform optimization.

The measurement of semantic correctness
serves as a key part in the semantics-based objec-
tive, which should represent the semantics of pro-
grams. In this paper, we convert a regular expres-
sion to a minimal Deterministic Finite Automaton
(DFA). Such conversion is based on the insight
that semantically equivalent regular expressions
have the same minimal DFAs. We define the se-
mantic correctness of a generated regular expres-
sion as whether its corresponding minimal DFA is
the same as the ground truth’s minimal DFA.

When our approach is applied on domains other
than regular expressions such as Python programs
and Bash scripts, a perfect equivalence oracle such
as minimal DFAs may not be feasibly available.
To handle a more general case, we propose cor-
rectness assessment based on test cases for regu-
lar expression; such correctness assessment can be
easily generalized for other tasks of program syn-
thesis. Concretely, we generate test cases to rep-
resent semantics of the ground truth. For a gen-
erated regular expression, we assess its semantic
correctness by checking whether it can pass all
the test cases. However, a regular expression may
have infinite positive (i.e., matched) or negative
(i.e., unmatched) string examples; thus, we can-
not perfectly represent the semantics. To use lim-
ited string examples to differentiate whether a gen-
erated regular expression is semantically correct
or not, we propose an intelligent strategy for test
generation to generate distinguishing test cases in-
stead of just using random test cases.

We evaluate SemRegex on three public datasets:
NL-RX-Synth, NL-RX-Turk (Locascio et al.,
2016), and KB13 (Kushman and Barzilay, 2013).
We compare SemRegex with the existing state-of-
the-art approaches on the task of generating regu-
lar expressions from NL specifications. Our evalu-
ation results show that SemRegex outperforms the

start-of-the-art approaches on all of three datasets.
The evaluation results confirm that by maximizing
semantic correctness, the model can output more
correct regular expressions even when the regu-
lar expressions are syntactically different from the
ground truth.

In summary, this paper makes the follow-
ing three main contributions. (1) We pro-
pose a semantics-based approach to optimize the
semantics-based objective for the task of gener-
ating regular expressions from NL specifications.
(2) We introduce the measurement of semantic
correctness based on test cases, and propose a
strategy to generate distinguishing test cases, in
order to better measure the semantic correctness
than using random test cases. (3) We evaluate our
approach on three public datasets. The evaluation
results show that our approach outperforms the ex-
isting state-of-the-art on all of the three datasets.

2 Problem Formulation

Consider the problem of automatically generating
a regular expression R given an NL specification
S as an input. Let S = s1, s2, . . . , sm denote the
NL specification, where si represents a word in
the vocabulary; let R = r1, r2, . . . , rn denote the
regular expression, where ri is a valid character in
the regular expression.

We assume that we have a training set consist-
ing of K NL and regular expression pairs:

D =
{
(S(i), R(i))

}
i=1..K

Given an NL specification, it is possible to have
multiple regular expressions fitting the specifica-
tion. In the training set, only one regular expres-
sion is provided for each NL specification.

3 SemRegex Approach

In this section, we illustrate our SemRegex ap-
proach in detail. First, we introduce our model,
which is a sequence-to-sequence learning model.



1610

Next, we alter the standard Maximum Likelihood
Estimation (MLE) objective to maximize semantic
correctness. We leverage policy gradient to train
the model with the semantics-based objective. Fi-
nally, we discuss how to measure semantic cor-
rectness.

3.1 Model

It is natural to apply a machine-translation model
on the program-synthesis problem. We follow a
previous attempt (Locascio et al., 2016) to use a
sequence-to-sequence learning model (Sutskever
et al., 2014) augmented with the attention mech-
anism (Bahdanau et al., 2014). The model con-
sists of an encoder network and a decoder net-
work. In both the encoder network and decoder
network, we use LSTM (Hochreiter and Schmid-
huber, 1997) units that can be summarized as fol-
low:

it = σ(Wixt + Uiht−1 + bi)

ft = σ(Wfxt + Ufht−1 + bf )

ot = σ(Woxt + Uoht−1 + bo)

c̃t = φ(Wcxt + Ucht−1 + bc)

ct = ft ◦ ct−1 + it ◦ c̃t
ht = ot ◦ φ(ct)

where σ is the sigmoid function, φ is the hy-
perbolic tangent function, and ◦ is the element-
wise multiplication; weight matrices W and U
along with biases b are learnable parameters of the
model. In the encoder network, the input xt is an
embedding vector of the word st in the NL input
sequence. In the decoder network, the input xt
is an embedding vector of the previous character
rt−1 in the output regular expression. The hid-
den vectors ht of the encoder network are fed into
an attention layer (Bahdanau et al., 2014) to out-
put an overall representation of the input sentence
considering the output position. The hidden vec-
tors ht of the decoder network are fed into a dense
layer zt =Wzht, where zt holds the dimension of
the vocabulary size of the regular expression. zt
is the output of the decoder network to predict the
output character rt = argmaxj zt,j .

A softmax function is applied on zt to obtain
a probability distribution on output character can-
didates. The probability of character j at output
position t is as follow:

p(rt = j|r<t, S) =
ezt,j∑
j′ e

zt,j′

3.2 Training
Let θ represent all learnable parameters in the
model. We discuss two objective functions of θ
to train the model.
Maximum Likelihood Estimation (MLE). A
sequence-to-sequence learning model learns the
distribution of regular expressions R given an in-
put NL sentence S:

pθ(R|S) =
T∏
t=1

pθ(rt|r<t, S)

By default, the sequence-to-sequence learn-
ing model uses maximum likelihood estimation
(MLE) for training, i.e., maximizing the likelihood
of mapping the input sequence to the output se-
quence for each pair in the training set. Specif-
ically, the optimal parameters θ∗ are obtained as
follow:

θ∗ = argmax
θ

∏
(S(i),R(i))∈D

pθ(R
(i)|S(i))

= argmax
θ

∑
(S(i),R(i))∈D

log pθ(R
(i)|S(i))

Gradient descent is used to search out optimal pa-
rameters θ∗.

However, MLE fails to consider the fact that
semantically equivalent regular expressions might
be syntactically different. The MLE objective
function forces the model to generate syntactically
similar regular expressions, but penalizes semanti-
cally equivalent and syntactically different regular
expressions. Such a syntax-based training objec-
tive does not fit our task’s objective (i.e., generat-
ing any semantically correct regular expression).
Maximizing Semantic Correctness. To encour-
age the model to generate any semantically correct
regular expression, we alter the MLE training ob-
jective function to maximize semantic correctness.

For an NL specification, we define a reward of a
predicted regular expression r(R) as its semantic
correctness (we discuss how to measure the cor-
rectness later in this section). We encourage the
model to generate regular expressions to maximize
expected rewards instead of MLE. Concretely, we
train the model parameters θ to maximize the fol-
lowing objective function:

J(θ) =
∑

(S(i),R(i))∈D

ER∼pθ(·|S(i))r(R)

=
∑

(S(i),R(i))∈D

∑
R

pθ(R|S(i))r(R)



1611

However, to compute the expected reward, we
need to go over all possible regular expressions,
and the number of all possible regular expressions
is infinite. To address this problem, we use the
Monte Carlo estimate as the approximation of the
expected value. Specifically, M regular expres-
sions R1, . . . , RM are sampled following the out-
put probability of the model. We average the re-
ward of each sample to estimate the expected re-
ward:

J(θ) ≈
∑

(S(i),R(i))∈D

M∑
j=1

1

M
r(Rj),

where Rj ∼ pθ(·|S(i))

In order to compute the gradient of the expected
reward and to maximize the objective using gra-
dient descent, we employ the REINFORCE tech-
nique of policy gradient (Williams, 1992), which
is based on the following estimation:

∇θJ(θ) ≈∑
(S(i),R(i))∈D

M∑
j=1

1

M
r(Rj)∇θ log pθ(Rj |S(i)),

where Rj ∼ pθ(·|S(i))

In practice, we subtract the mean reward of all
samples to reduce the variance of estimated gradi-
ent (Williams, 1992). The final gradient estimate
is as follow:

∇θJ(θ) ≈∑
(S(i),R(i))∈D

M∑
j=1

1

M
r̃(Rj)∇θ log pθ(Rj |S(i)),

where r̃(Rj) = r(Rj)−
M∑
j′=1

1

M
r(Rj′)

The overall training algorithm is summarized in
Algorithm 1. We initialize θ by pre-training the
model using MLE on the training set. For each
pair in training set, we sample M regular expres-
sions to estimate the gradient.

3.3 Measurement of Semantic Correctness
In this paper, we propose two types of measuring
semantic correctness based on minimal DFAs and
test cases, respectively.
Minimal DFAs. We convert a regular expres-
sion to a minimal DFA and utilize the fact
that equivalent regular expressions have the same

Algorithm 1: Policy-gradient method to
maximize semantic correctness
Input: Training set: D =

{
(S(i), R(i))

}
1 Initilize θ from pretrained model using

MLE on D ;
2 for each epoch do
3 for (S(i), R(i)) ∈ D do
4 Sample R1, . . . , RM using current

model ;
5 Get rewards r(R1), . . . , r(RM ) ;
6 Estimate∇θJ(θ) using (3.2) ;
7 Update θ using∇θJ(θ) by gradient

descent ;
8 end
9 end

s0 s1 s2
A,B

6=X

X

(a)

Path String example

s0
A−→ s1

X−→ s2 AX
s0

B−→ s1
K−→ s1

X−→ s2 BKX
s0

B−→ s1
X−→ s2 BX

(b)

Figure 1: Minimal DFA converted from
“([ABab]&[A-Z]).*X” and generated
string examples, where s0 represents the start state
and s2 is the only accept state.

minimal DFAs even when they are syntacti-
cally different (Hopcroft et al., 1979). For
example, the minimal DFA of regular expres-
sion “([ABab]&[A-Z]).*X” is shown in Fig-
ure 1(a). A syntactically different regular expres-
sion “((A|B).*)&(.*X)” can be converted to
the same minimal DFA as shown in Figure 1(a),
indicating that these two regular expressions are
semantically equivalent.

We check whether two regular expressions are
equivalent by checking whether their correspond-
ing minimal DFAs are the same. When the policy-
gradient method is performed, if a sampled regu-
lar expression R is equivalent to the ground truth,
then r(R) = 1; otherwise r(R) = 0.
Test Cases. A perfect equivalence oracle such



1612

as using the minimal DFA may not be feasibly
available for some tasks, e.g., when our approach
is applied on other domains such as generating
Bash scripts and Python programs. To handle a
more general case, we propose correctness mea-
surement based on test cases. We generate test
cases (i.e., inputs and expected outputs) and check
whether a program can pass the test cases that are
generated from the ground truth to approximately
check whether the program and the ground truth
are equivalent.

Given a regular expression R, we generate test
cases that contain positive (acceptable/matched)
and negative (unacceptable/unmatched) string ex-
amples. Here we consider only positive exam-
ples because negative examples can be obtained
by generating positive examples of its complement
regular expression∼R. To generate positive string
examples from regular expression R, we convert
R to its corresponding minimal DFA. Each posi-
tive string example corresponds to a path from the
start state of the minimal DFA to any accept state2,
and vice versa. Thus, we generate paths randomly
from the start state to any accept state, and convert
the paths to their corresponding strings as shown
in Figure 1(b). To generate distinct string exam-
ples, we aim to generate paths to cover as many
transitions as possible. In particular, we mask all
transitions that have been covered by previously
generated paths. When we generate a new path,
the not-covered transitions have higher priority to
be explored than covered ones.

Because complex regular expressions may ac-
cept/match or reject/unmatch infinite string ex-
amples, we augment random generation with a
new strategy to generate distinguishing test cases
to better represent the semantics. Considering
that the generated test cases are used to check
whether a Monte-Carlo sampled regular expres-
sion is equivalent to the ground truth in the policy-
gradient method, only test cases that can differen-
tiate an incorrect sample and the ground truth are
useful. Based on such insight, we give preference
to test cases that differentiate Monte-Carlo sam-
ples and the ground truth. A challenge here is that
we do not know the samples before performing
the policy-gradient method. However, we find that
there is a high chance to get the same samples re-
peatedly when the model is pre-trained using MLE
on the training set, because sampling is following
the distribution learned by the pre-trained model.
Based on the observation, we use the Beam Search

2A DFA has one start state and a set of accept states.

algorithm on the pre-trained model to obtain B
most likely samples R̂1, . . . , R̂B . We generate
string examples that can differentiate these sam-
ples and the ground truth, named as distinguishing
string examples. For each R̂ and ground truth R,
we construct a new regular expression R&(∼R̂),
and generate its string examples that can differen-
tiate R and R̂.

The overall idea of our strategy for generating
string examples is shown in Algorithm 2. Once
we have a set of positive and negative string exam-
ples, we define the reward of a regular expression
as r(R) = 1 if it can pass all the test cases, and
r(R) = 0 otherwise.

When extending SemRegex on other languages
where a perfect equivalence oracle is not available,
it is desirable to use a technique to generate test
cases for a program. There exist techniques (dis-
cussed in Section 5) to generate test cases for a
general executable program.

Algorithm 2: Generating distinguishing
test cases for regular expressions

Input: Training set: D =
{
(S(i), R(i))

}
,

the number of examples to generate:
T , and a pre-trained model

Output: Positive and negative example
sets P(i) and N (i)

1 for (S(i), R(i)) ∈ D do
2 P(i) ← ∅ ;
3 N (i) ← ∅ ;
4 Beam search on pre-trained model to

obtain R̂1, . . . , R̂B ;
5 repeat
6 Randomly pick a j in [1, B] ;
7 Rp ← R(i)&(∼R̂j) ;
8 Rn ← (∼R(i))&R̂j ;
9 Generate an example p from Rp ;

10 Generate an example n from Rn ;
11 P(i) ← P(i) ∪ {p} ;
12 N (i) ← N (i) ∪ {n} ;
13 until |P(i)| ≥ T && |N (i)| ≥ T ;
14 end

4 Experiments

We evaluate the effectiveness of SemRegex by
comparing it to the state-of-the-art approaches.
We also study how using different measurements
of correctness impacts the effectiveness of Sem-
Regex.



1613

4.1 Experiment Setup
Datasets. We conduct our experiments on three
public datasets for the task of generating regular
expressions from NL specifications.

• KB13. KB13 (Kushman and Barzilay, 2013)
includes 824 pairs of NL and regular expres-
sion. When conducting data labeling, label-
ing workers are asked to generate the NL
specifications to capture a subset of the lines
in a file. Then programmers are asked to gen-
erate regular expressions for these NL speci-
fications written by the labeling workers. We
split the data into 75% training and 25% test-
ing sets, following what the authors of KB13
do.

• NL-RX-Synth. NL-RX-Synth (Locascio
et al., 2016) is a synthetic dataset much larger
than KB13. Its authors define a small gram-
mar for parsing regular expressions to NL.
The grammar is used to stochastically gener-
ate 10, 000 regular expressions and their cor-
responding synthetic NL specifications. We
split the pairs into 65% training, 10% devel-
opment, and 25% testing sets, following what
the authors of NL-RX-Synth do.

• NL-RX-Turk. NL-RX-Turk (Locascio et al.,
2016) comes from the NL-RX-Synth dataset.
Instead of directly using synthetic NL de-
scriptions in the dataset, the authors of NL-
RX-Turk ask labeling workers to paraphrase
the synthetic specifications. The dataset also
consists of 10, 000 pairs of NL and regular
expression. We split the pairs into 65% train-
ing, 10% development, and 25% testing sets,
following what the authors of NL-RX-Turk
do.

Training Setting. We use a two-layer stacked
LSTM architecture in both the encoder and de-
coder networks. The dimensions of encoder and
decoder hidden states are set to 256. We use ran-
dom embedding layers with the dimension of 128
for both input and output words. We also tune
our hyper-parameters on the development set. The
best results are obtained when the learning rate
= 0.001 and the batch size = 25. We use the
Monte-Carlo method to sample M = 10 regular
expressions to estimate the gradient. To generate
distinguishing string examples, we perform Beam
Search to obtain B = 10 most likely samples. Be-
fore performing the policy-gradient method, we

pre-train the model using MLE for 100 epochs.
Then we train the model for 40 epochs using
the policy-gradient method, and choose the model
with the best effectiveness on the development set.
Our model is implemented in TensorFlow (Abadi
et al., 2016).

4.2 Results and Analysis
Comparison Results. We demonstrate the ef-
fectiveness of our approach by comparing it
to the existing approaches including Semantic-
Unify (Kushman and Barzilay, 2013) and Deep-
Regex(MLE) (Locascio et al., 2016). We also
compare the results of our approach with differ-
ent measurements of semantic correctness. Ta-
ble 2 shows the comparison results of different ap-
proaches, with detailed discussion as follows.

• Semantic-Unify. Semantic-Unify (Kushman
and Barzilay, 2013) learns to parse NL to
regular expressions. Similarly, DFA equiv-
alence is applied as a semantic unification
when training the parser.

• Deep-Regex(MLE). Deep-
Regex(MLE) (Locascio et al., 2016)
regards the problem as a black-box task of
machine translation without utilizing any
domain knowledge of regular expressions. A
syntax-based objective (MLE) is used to train
the model. To the best of our knowledge,
Deep-Regex(MLE) is the state-of-the-art
approach on these three datasets.

• SemRegex(DFA Oracle). In SemRegex
(DFA Oracle), we use the oracle of DFA
equivalence to measure semantic correctness.
SemRegex(DFA Oracle) outperforms Deep-
Regex(MLE), the existing state-of-the-art ap-
proach, by an accuracy increase of 12.6%
on KB13, 2.9% on NL-RX-Synth, and 4.1%
on NL-RX-Turk, respectively. Compared
to Deep-Regex(MLE), the results demon-
strate the effectiveness of maximizing se-
mantic correctness during the training phase.
SemRegex(DFA Oracle) shows more im-
provement on the KB13 dataset over Deep-
Regex(MLE) than on other datasets. Such re-
sult indicates that supervised learning based
on MLE is less effective to learn from a
small training set. When the policy-gradient
method is used, Monte-Carlo samples can
provide more information beyond only train-
ing samples especially on a small training set;



1614

Table 2: Effectiveness comparison of different approaches (using DFA-equivalence accuracy as metrics)

Approach KB13 NL-RX-Synth NL-RX-Turk
Semantic-Unify 65.5% 46.3% 38.6%
Deep-Regex(MLE) 65.6% 88.7% 58.2%
SemRegex(DFA Oracle) 78.2% 91.6% 62.3%
SemRegex(Distinguishing Test Cases) 77.5% 90.2% 61.3%
SemRegex(Random Test Cases) 66.5% 90.2% 59.5%

such more information significantly improves
the effectiveness.

• SemRegex(Distinguishing Test Cases).
When we do not have access to an oracle
such as DFA equivalence, we can generate
test cases to define semantic correctness.
SemRegex(Distinguishing Test Cases) uses
Algorithm 2 to generate distinguishing test
cases (10 positive examples and 10 negative
examples) that differentiate the results re-
turned by Beam Search and the ground truth.
The results show that by using distinguishing
test cases, SemRegex(Distinguishing Test
Cases) outperforms Deep-Regex(MLE), an
existing syntax-based approach, on all of
three datasets. Meanwhile, the effectiveness
of SemRegex(Distinguishing Test Cases)
drops no more than 1.4% on accuracy
compared to SemRegex(DFA Oracle). Such
result indicates that limited distinguishing
test cases generated by the proposed strategy
can well represent the semantics.

• SemRegex(Random Test Cases). Sem-
Regex(Random Test Cases) generates ran-
dom test cases instead of distinguishing
test cases. It outperforms the exist-
ing approaches (Semantic-Unify and Deep-
Regex(MLE)) because random test cases can
still represent the semantics and differenti-
ate some inequivalent regular expressions.
Compared to SemRegex(Distinguishing Test
Cases), its effectiveness shows a big drop
on KB13 and a slight drop on NL-RX-Turk.
Such results indicate the benefit of distin-
guishing test cases over random test cases.

Effectiveness of Semantics-Based Objective. To
understand the effect of using a semantics-based
learning objective, we record the semantic accu-
racy (DFA equivalence) and syntactic accuracy
(exact-match) on the NL-RL-Turk testing set af-
ter each epoch as shown in Figure 2. During pre-
training (epochs 1 to 100), we use MLE to train the

0 20 40 60 80 100 120 140

Epoch

0

10

20

30

40

50

60

A
cc

ur
ac

y 
(%

)

Semantic Accuracy
Syntactic Accuracy

Figure 2: Semantic accuracy (DFA equivalence)
and syntactic accuracy (exact-match) on the NL-
RL-Turk testing set after each epoch. The training
objective is replaced to maximize expected cor-
rectness after 100 epochs. The correctness is mea-
sured by the DFA-equivalence oracle in this figure.

model to increase both semantic accuracy and syn-
tactic accuracy iteratively. Then, we alter the train-
ing objective to maximize the expected semantic
correctness. We notice that while semantic accu-
racy continues increasing for about 10%, the syn-
tactic accuracy does not show a significant growth
after pre-training. Such result indicates that the
model is no longer encouraged to generate regular
expressions that are syntactically equivalent to the
ground truths. Instead, the model learns to gener-
ate semantically correct regular expressions.
Analysis of Semantic Correctness Based on Test
Cases. The correctness measurements based on
test cases serve as an approximate oracle. Figure 3
shows an example of how the approximate oracle
helps make improvement. Furthermore, we evalu-
ate how the correctness based on test cases is close
to the DFA-equivalence oracle. In Monte-Carlo
estimate, we count the samples with the approxi-
mate oracle that equals to the minimal DFA oracle.
When using random test cases, there are 89.8%
samples with the approximate oracle that equals to
the minimal DFA oracle. When using distinguish-



1615

R3:(([0-9]){2,})(.*)

R1:(([0-9])(.+)){2,} Pos: ”74i”

R2:([0-9])((.*){2,})

S: Strings that begin with at least two digits

Neg: ”8aa”

Figure 3: An example of how test cases help with
training. At the beginning of the policy-gradient
method, the model outputs an incorrect answer
R1, which cannot pass a positive test case. R1 gets
penalized because it receives a reward 0. Then the
model changes to output an incorrect answer R2,
which cannot pass a negative test case. Similarly,
R2 gets penalized as training continues. R3 re-
ceives a reward 1 because it passes all test cases,
resulting in an increase of its likelihood from the
model in iterations. Finally, the model outputs the
correct answer R3.

ing test cases, such percentage increases to 96.3%.
Such result illustrates that test cases are able to ap-
proximately check the semantic equivalence even
when the test cases are generated randomly. The
result also suggests that distinguishing test cases
represent the semantics more effectively than ran-
dom test cases.
Impact of the Number of Test Cases. We study
how the number of test cases impacts the effective-
ness. We enumerate the number of distinguishing
or random positive/negative string examples from
T = 1 to T = 10 to show the impact on the ef-
fectiveness (T = 0 refers to using MLE to train
the model). As shown in Figure 4, when more dis-
tinguishing test cases are used, higher accuracy is
reached. However, more random test cases make
limited improvement.

5 Related Work

Program Synthesis. Our work falls into the gen-
eral topic of program synthesis. Program syn-
thesis is the problem of automatically generating
programs from high-level specifications (Gulwani
et al., 2017). There has been a lot of progress made
in this area, classified based on (1) the form of
specifications, e.g., NL descriptions (Yin and Neu-
big, 2017; Guu et al., 2017; Lin et al., 2017; Kr-
ishnamurthy and Mitchell, 2012; Liu et al., 2018),
input-output examples (Balog et al., 2017; Chen
et al., 2018; Kalyan et al., 2018), and hybrid of the
two preceding types of specifications (Manshadi

0 2 4 6 8 10

T: number of pos/neg examples

66

68

70

72

74

76

78

A
cc

ur
ac

y 
(%

)

Distinguishing
Random

Figure 4: Impact of the number of distinguishing
or random test cases on accuracy on the KB13
dataset.

et al., 2013; Raza et al., 2015); (2) the program-
ming languages, e.g., LISP (Biermann, 1978),
Python (Yin and Neubig, 2017; Rabinovich et al.,
2017), SQL (Zhong et al., 2017; Sun et al., 2018),
and Domain-Specific Languages (DSL) such as
FlashFill (Gulwani, 2011). In this paper, we focus
on an important subtask of the program-synthesis
problem: generating regular expressions from NL.
Generating Regular Expressions. Recent re-
search has attempted to automatically generate
regular expressions from NL specifications. Ranta
(1998) propose a rule-based approach to build
an NL interface for regular expressions. Kush-
man and Barzilay (2013) develop an approach for
learning a probabilistic grammar model to parse an
NL description into a regular expression. Locas-
cio et al. (2016) regard the problem as a black-box
task of machine translation, and train a sequence-
to-sequence learning model to address the prob-
lem. There exists also a lot of work focusing on
generating regular expressions from string exam-
ples. Recent work typically uses an evolutionary
algorithm to address the problem (Svingen, 1998;
Cetinkaya, 2007; Bartoli et al., 2012, 2016).

Inspired by our previous study (Zhong et al.,
2018), in this paper, we leverage the help of string
examples generated from ground truths to im-
prove the state of the art for the problem of gen-
erating regular expressions from NL. Compared
with previous state-of-the-art approaches (Locas-
cio et al., 2016) that maximize the likelihood of
ground truths in the training set, SemRegex lever-
ages the policy-gradient method to encourage the
model to generate semantically correct regular ex-
pressions.
Generating Test Cases. When SemRegex is ap-



1616

plied on domains other than synthesizing regu-
lar expressions, a perfect equivalence oracle such
as using the minimal DFA may not be feasi-
bly available. In order to handle a more gen-
eral case, we propose to generate test cases from
the ground truths to measure the semantic cor-
rectness of a program candidate. State-of-the-art
test-generation techniques are typically based on
Dynamic Symbolic Execution (DSE) (Godefroid
et al., 2005). Given a program that we want to
generate test cases for, DSE executes the program
for some seed test cases, and at the same time
collects symbolic constraints from branch state-
ments along the execution path. Then DSE gen-
erates new test cases to cover different branches
in iterations by flipping a branching node in previ-
ous execution path. In this way, DSE is able to
generate test cases that can be used to approxi-
mately check the semantic equivalence. Further-
more, DSE can effectively generate distinguishing
test cases for two executable programs by relating
these two programs in a single execution (Taneja
and Xie, 2008). Various DSE tools have been im-
plemented for different programming languages,
such as PyExZ3 (Python) (Ball and Daniel, 2015),
JPF-SE (Java) (Anand et al., 2007), Pex (C#) (Till-
mann and De Halleux, 2008; Tillmann et al., 2014;
Li et al., 2009), and CUTE (C) (Sen et al., 2005).

6 Conclusion

We have proposed SemRegex, a semantics-based
approach to generate regular expressions from NL
specifications. SemRegex trains a sequence-to-
sequence model by maximizing the expected se-
mantic correctness. We measure the semantic cor-
rectness using the DFA-equivalence oracle, ran-
dom test cases, and distinguishing test cases. Our
evaluation results show that SemRegex outper-
forms the existing start-of-the-art approaches on
three public datasets.

Acknowledgments

The work from the authors at the University of
Illinois at Urbana-Champaign was supported in
part by National Science Foundation under grants
no. CNS-1513939, CNS-1564274, and CCF-
1816615. The work from the authors at Xi’an
Jiaotong University was supported by National
Natural Science Foundation of China (61632015,
61772408, 61721002).

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Gregory S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian J. Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Józefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Gordon Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Ku-
nal Talwar, Paul A. Tucker, Vincent Vanhoucke, Vi-
jay Vasudevan, Fernanda B. Viégas, Oriol Vinyals,
Pete Warden, Martin Wattenberg, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow:
Large-scale machine learning on heterogeneous dis-
tributed systems. CoRR, abs/1603.04467.

Saswat Anand, Corina S Păsăreanu, and Willem Visser.
2007. JPF–SE: A symbolic execution extension to
Java PathFinder. In International Conference on
Tools and Algorithms for the Construction and Anal-
ysis of Systems, pages 134–138.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations.

Thomas Ball and Jakub Daniel. 2015. Deconstruct-
ing dynamic symbolic execution. Technical Report
MSR-TR-2015-95, Microsoft.

Matej Balog, Alexander L Gaunt, Marc Brockschmidt,
Sebastian Nowozin, and Daniel Tarlow. 2017.
DeepCoder: Learning to write programs. In Inter-
national Conference on Learning Representations.

Alberto Bartoli, Giorgio Davanzo, Andrea De Lorenzo,
Marco Mauri, Eric Medvet, and Enrico Sorio. 2012.
Automatic generation of regular expressions from
examples with genetic programming. In Annual
Conference Companion on Genetic and Evolution-
ary Computation, pages 1477–1478.

Alberto Bartoli, Andrea De Lorenzo, Eric Medvet,
and Fabiano Tarlao. 2016. Inference of regular ex-
pressions for text extraction from examples. IEEE
Transactions on Knowledge and Data Engineering,
28(5):1217–1230.

Alan W Biermann. 1978. The inference of regular
LISP programs from examples. IEEE Transactions
on Systems, Man, and Cybernetics, 8(8):585–600.

Rudy Bunel, Matthew Hausknecht, Jacob Devlin,
Rishabh Singh, and Pushmeet Kohli. 2018. Lever-
aging grammar and reinforcement learning for neu-
ral program synthesis. In International Conference
on Learning Representations.

Ahmet Cetinkaya. 2007. Regular expression gener-
ation through grammatical evolution. In Annual
Conference Companion on Genetic and Evolution-
ary Computation, pages 2643–2646.



1617

Xinyun Chen, Chang Liu, and Dawn Song. 2018. To-
wards synthesizing complex programs from input-
output examples. In International Conference on
Learning Representations.

Patrice Godefroid, Nils Klarlund, and Koushik Sen.
2005. DART: directed automated random test-
ing. In ACM SIGPLAN Conference on Program-
ming Language Design and Implementation, pages
213–223.

Sumit Gulwani. 2011. Automating string processing
in spreadsheets using input-output examples. In
ACM SIGPLAN Symposium on Principles of Pro-
gramming Languages, pages 317–330.

Sumit Gulwani, Alex Polozov, and Rishabh Singh.
2017. Program synthesis. Foundations and Trends
in Programming Languages, 4(1–2):1–119.

Kelvin Guu, Panupong Pasupat, Evan Zheran Liu,
and Percy Liang. 2017. From language to pro-
grams: Bridging reinforcement learning and maxi-
mum marginal likelihood. In Annual Meeting of the
Association for Computational Linguistics, pages
1051–1062.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

John E Hopcroft, Rajeev Motwani, and Jeffrey D Ull-
man. 1979. Introduction to automata theory, lan-
guages, and computation. Addison-wesley Read-
ing.

Ashwin Kalyan, Abhishek Mohta, Alex Polozov,
Dhruv Batra, Prateek Jain, and Sumit Gulwani.
2018. Neural-guided deductive search for real-time
program synthesis from examples. In International
Conference on Learning Representations.

Jayant Krishnamurthy and Tom M Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 754–765.

Nate Kushman and Regina Barzilay. 2013. Using se-
mantic unification to generate regular expressions
from natural language. In Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 826–836.

Nuo Li, Tao Xie, Nikolai Tillmann, Jonathan
de Halleux, and Wolfram Schulte. 2009. Reggae:
Automated test generation for programs using com-
plex regular expressions. In IEEE/ACM Interna-
tional Conference on Automated Software Engineer-
ing.

Xi Victoria Lin, Chenglong Wang, Deric Pang, Kevin
Vu, Luke Zettlemoyer, and Michael D. Ernst. 2017.
Program synthesis from natural language using re-
current neural networks. Technical Report UW-
CSE-17-03-01, University of Washington Depart-
ment of Computer Science and Engineering.

Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, and
Percy Liang. 2018. Reinforcement learning on web
interfaces using workflow-guided exploration. In
International Conference on Learning Representa-
tions.

Nicholas Locascio, Karthik Narasimhan, Eduardo
DeLeon, Nate Kushman, and Regina Barzilay. 2016.
Neural generation of regular expressions from natu-
ral language with minimal domain knowledge. In
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1918–1923.

Mehdi Hafezi Manshadi, Daniel Gildea, and James F
Allen. 2013. Integrating programming by example
and natural language programming. In AAAI Con-
ference on Artificial Intelligence, pages 661–667.

Maxim Rabinovich, Mitchell Stern, and Dan Klein.
2017. Abstract syntax networks for code genera-
tion and semantic parsing. In Annual Meeting of the
Association for Computational Linguistics, pages
1139–1149.

Aarne Ranta. 1998. A multilingual natural-language
interface to regular expressions. In International
Workshop on Finite State Methods in Natural Lan-
guage Processing, pages 79–90.

Mohammad Raza, Sumit Gulwani, and Natasa Milic-
Frayling. 2015. Compositional program synthesis
from natural language and examples. In Interna-
tional Joint Conferences on Artificial Intelligence,
pages 792–800.

Koushik Sen, Darko Marinov, and Gul Agha. 2005.
CUTE: a concolic unit testing engine for C. In Joint
Meeting of the European Software Engineering Con-
ference and the ACM SIGSOFT Symposium on the
Foundations of Software Engineering, pages 263–
272.

Yibo Sun, Duyu Tang, Nan Duan, Jianshu Ji, Gui-
hong Cao, Xiaocheng Feng, Bing Qin, Ting Liu, and
Ming Zhou. 2018. Semantic parsing with syntax-
and table-aware SQL generation. In Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 361–372.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Annual Conference on Neural Informa-
tion Processing Systems, pages 3104–3112.

Borge Svingen. 1998. Learning regular languages us-
ing genetic programming. Genetic Programming,
pages 374–376.

Kunal Taneja and Tao Xie. 2008. DiffGen: Automated
regression unit-test generation. In IEEE/ACM Inter-
national Conference on Automated Software Engi-
neering, pages 407–410.



1618

Nikolai Tillmann and Jonathan De Halleux. 2008.
Pex–White box test generation for .NET. In Interna-
tional Conference on Tests and Proofs, pages 134–
153.

Nikolai Tillmann, Jonathan de Halleux, and Tao Xie.
2014. Transferring an automated test generation tool
to practice: From Pex to Fakes and Code Digger. In
IEEE/ACM International Conference on Automated
Software Engineering, pages 385–396.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine Learning, 8:229–256.

Pengcheng Yin and Graham Neubig. 2017. A syntactic

neural model for general-purpose code generation.
In Annual Meeting of the Association for Computa-
tional Linguistics, pages 440–450.

Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2SQL: Generating structured queries
from natural language using reinforcement learning.
arXiv preprint arXiv:1709.00103.

Zexuan Zhong, Jiaqi Guo, Wei Yang, Tao Xie, Jian-
Guang Lou, Ting Liu, and Dongmei Zhang. 2018.
Generating regular expressions from natural lan-
guage specifications: Are we there yet? In Work-
shop on NLP for Software Engineering.


