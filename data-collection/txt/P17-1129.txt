



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1405–1414
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1129

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1405–1414
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1129

A Constituent-Centric Neural Architecture for Reading Comprehension

Pengtao Xie*† and Eric P. Xing†
*Machine Learning Department, Carnegie Mellon University

†Petuum Inc.
pengtaox@cs.cmu.edu, eric.xing@petuum.com

Abstract

Reading comprehension (RC), aiming to
understand natural texts and answer ques-
tions therein, is a challenging task. In
this paper, we study the RC problem on
the Stanford Question Answering Dataset
(SQuAD). Observing from the training set
that most correct answers are centered
around constituents in the parse tree, we
design a constituent-centric neural archi-
tecture where the generation of candidate
answers and their representation learning
are both based on constituents and guided
by the parse tree. Under this architec-
ture, the search space of candidate answers
can be greatly reduced without sacrificing
the coverage of correct answers and the
syntactic, hierarchical and compositional
structure among constituents can be well
captured, which contributes to better rep-
resentation learning of the candidate an-
swers. On SQuAD, our method achieves
the state of the art performance and the ab-
lation study corroborates the effectiveness
of individual modules.

1 Introduction

Reading comprehension (RC) aims to answer
questions by understanding texts, which is a chal-
lenge task in natural language processing. Var-
ious RC tasks and datasets have been devel-
oped, including Machine Comprehension Test
(Richardson et al., 2013) for multiple-choice ques-
tion answering (QA) (Sachan et al., 2015; Wang
and McAllester, 2015), Algebra (Hosseini et al.,
2014) and Science (Clark and Etzioni, 2016) for
passing standardized tests (Clark et al., 2016),
CNN/Daily Mail (Hermann et al., 2015) and Chil-
dren’s Book Test (Hill et al., 2015) for cloze-style

 The most authoritative account at the time came from 
the medical faculty in Paris in a report to the king of 

France that blamed the heavens. This report became 

the first and most widely circulated of a series of 

plague tracts that sought to give advice to sufferers. 

That the plague was caused by bad air became the most 

widely accepted theory. Today, this is known as the 

Miasma theory.  

1. Who was the medical report written for?                    

the king of France 

2. What is the newer, more widely accepted theory 

behind the spread of the plague?                                  

bad air 

3. What is the bad air theory officially known as?    

Miasma theory 

Figure 1: An example of the SQuAD QA task

QA (Chen et al., 2016; Shen et al., 2016), Wik-
iQA (Yang et al., 2015), Stanford Question An-
swering Dataset (SQuAD) (Rajpurkar et al., 2016)
and Microsoft Machine Reading Comprehension
(Nguyen et al., 2016) for open domain QA. In this
paper, we are specifically interested in solving the
SQuAD QA task (Figure 1 shows an example),
in light of its following features: (1) large scale:
107,785 questions, 23,215 paragraphs; (2) non-
synthetic: questions are generated by crowdwork-
ers; (3) large search space of candidate answers.

We study two major problems: (1) how to
generate candidate answers? Unlike in multiple-
choice QA and cloze-style QA where a small
amount of answer choices are given, an answer in
SQuAD could be any span in the text, resulting
in a large search space with sizeO(n2) (Rajpurkar
et al., 2016), where n is the number of words in the
sentence. This would incur a lot of noise, ambigu-

1405

https://doi.org/10.18653/v1/P17-1129
https://doi.org/10.18653/v1/P17-1129


ity and uncertainty, making it highly difficult to
pick up the correct answer. (2) how to effectively
represent the candidate answers? First, long-range
semantics spanning multiple sentences need to be
captured. As noted in (Rajpurkar et al., 2016), the
answering of many questions requires multiple-
sentence reasoning. For instance, in Figure 1, the
last two sentences in the passages are needed to
answer the third question. Second, local syntac-
tic structure needs to be incorporated into repre-
sentation learning. The study by (Rajpurkar et al.,
2016) shows that syntax plays an important role in
SQuAD QA: there are a wide range of syntactic di-
vergence between a question and the sentence con-
taining the answer; the answering of 64.1% ques-
tions needs to deal with syntactic variation; exper-
iments show that syntactic features are the major
contributing factors to good performance.

To tackle the first problem, motivated by the
observation in (Rajpurkar et al., 2016) that the
correct answers picked up by human are not ar-
bitrary spans, but rather centered around con-
stituents in the parse tree, we generate candidate
answers based upon constituents, which signifi-
cantly reduces the search space. Different from
(Rajpurkar et al., 2016) who only consider ex-
act constituents, we adopt a constituent expansion
mechanism which greatly improves the coverage
of correct answers.

For the representation learning of candidate an-
swers which are sequences of constituents, we
first encode individual constituents using a chain-
of-trees LSTM (CT-LSTM) and tree-guided at-
tention mechanism, then feed these encodings
into a chain LSTM (Hochreiter and Schmidhu-
ber, 1997) to generate representations for the con-
stituent sequences. The CT-LSTM seamlessly
integrates intra-sentence tree LSTMs (Tai et al.,
2015) which capture the local syntactic properties
of constituents and an inter-sentence chain LSTM
which glues together the sequence of tree LSTMs
such that the semantics of each sentence can be
propagated to others. The tree-guided attention
leverages the hierarchical relations among con-
stituents to learn question-aware representations.

Putting these pieces together, we design
a constituent-centric neural network (CCNN),
which contains four layers: a chain-of-trees LSTM
encoding layer, a tree-guided attention layer and
a candidate-answer generation layer, a prediction
layer. Evaluation on SQuAD demonstrates the ef-

0

20

40

60

80

0 1 2 3 4 5 6 7 8 >8

P
e

rc
e

n
ta

ge
 o

f 
 

A
n

sw
e

rs
 (

%
) 

Number of different words N 

Figure 2: Percentage of answers that differ from
their closest constituents by N words

That	  the	  plague	  was	  caused	  by	  bad	  air	  
became	  the	  most	  widely	  accepted	  theory.	  
Today,	  this	  is	  known	  as	  the	  Miasma	  theory.

Chain-­‐of-­‐Trees LSTM
for Passage Encoding

What	  is	  the	  bad	  air	  theory	  officially	  known as?

Tree LSTM for
Question Encoding

Tree-­‐Guided Attention
Encoder

Candidate Answer
Generation

the	  plague bad	  air …… is	  known	  as Miasma	  theory

Answer Prediction

Miasma	  theory

Passage
Question

Candidate
Answers

Correct Answer

Figure 3: Constituent-centric neural network.

fectiveness of CCNN.

2 Constituent-Centric Neural Network
for Reading Comprehension

2.1 Overall Architecture

As observed in (Rajpurkar et al., 2016), almost
all correct answers are centered around the con-
stituents. To formally confirm this, we compare
the correct answers in the training set with con-
stituents generated by the Stanford parser (Man-
ning et al., 2014): for each correct answer, we find
its “closest” constituent – the longest constituent
that is a substring of the answer, and count how
many words they differ from (let N denote this
number). Figure 2 shows the percentage of an-
swers whose N equals to 0, · · · , 8 and N > 8.
As can be seen, ∼70% answers are exactly con-
stituents (N = 0) and ∼97% answers differ from
the closest constituents by less equal to 4 words.
This observation motivates us to approach the

1406



reading comprehension problem in a constituent-
centric manner, where the generation of candidate
answers and their representation learning are both
based upon constituents.

Specifically, we design a Constituent-Centric
Neural Network (CCNN) to perform end-to-end
reading comprehension, where the inputs are the
passage and question, and the output is a span in
the passage that is mostly suitable to answer this
question. As shown in Figure 3, the CCNN con-
tains four layers. In the encoding layer, the chain-
of-trees LSTM and tree LSTM encode the con-
stituents in the passage and question respectively.
The encodings are fed to the tree-guided atten-
tion layer to learn question-aware representations,
which are passed to the candidate-answer gener-
ation layer to produce and encode the candidate
answers based on constituent expansion. Finally,
the prediction layer picks up the best answer from
the candidates using a feed-forward network.

2.2 Encoding

Given the passages and questions, we first use
the Stanford parser to parse them into constituent
parse trees, then the encoding layer of CCNN
learns representations for constituents in questions
and passages, using tree LSTM (Tai et al., 2015)
and chain-of-trees LSTM respectively. These
LSTM encoders are able to capture the syntactic
properties of constituents and long-range seman-
tics across multiple sentences, which are crucial
for SQuAD QA.

2.2.1 Tree LSTM for Question Encoding

Each question is a single sentence, having one
constituent parse tree. Internal nodes in the tree
represent constituents having more than one word
and leaf nodes represent single-word constituent.
Inspired by (Tai et al., 2015; Teng and Zhang,
2016), we build a bi-directional tree LSTM which
consists of a bottom-up LSTM and a top-down
LSTM, to encode these constituents (as shown in
Figure 4). Each node (constituent) has two hid-
den states: h↑ produced by the LSTM in bottom-
up direction and h↓ produced by the LSTM in
top-down direction. Let T denote the maximum
number of children an internal node could have.
For each particular node, let L (0 ≤ L ≤ T ) be
the number of children it has, h(l)↑ and c

(l)
↑ be the

bottom-up hidden state and memory cell of the l-
th (1 ≤ l ≤ L) child (if any) respectively and h(p)↓

S

N VP

V NP

D N

John hit the ball.

S

NP VBZ

DT NP

N

The refereeyoung

ADJ

whistled.

…... …...

Figure 4: Chain-of-trees LSTM

and c(p)↓ be the top-down hidden state and memory
cell of the parent.

In the bottom-up LSTM, each node has an input
gate i↑, L forget gates {f (l)↑ }Ll=1 corresponding to
different children, an output gate o↑ and a memory
cell c↑. For an internal node, the inputs are the
hidden states and memory cells of its children and
the transition equations are defined as:

i↑ = σ(
∑L

l=1W
(i,l)
↑ h

(l)
↑ + b

(i)
↑ )

∀l, f (l)↑ = σ(W
(f,l)
↑ h

(l)
↑ + b

(f,l)
↑ )

o↑ = σ(
∑L

l=1W
(o,l)
↑ h

(l)
↑ + b

(o)
↑ )

u↑ = tanh(
∑L

l=1W
(u,l)
↑ h

(l)
↑ + b

(u)
↑ )

c↑ = i↑ � u↑ +
∑L

l=1 f
(l)
↑ � c

(l)
↑

h↑ = o↑ � tanh(c↑)

(1)

where the weight parameters W and bias parame-
ters b with superscript l such as W(i,l)↑ are specific
to the l-th child. For a leaf node which represents
a single word, it has no forget gate and the input is
the wording embedding (Pennington et al., 2014)
of this word.

In the top-down direction, the gates, memory
cell and hidden state are defined in a similar fash-
ion as the bottom-up direction (Eq.(1)). For an in-
ternal node except the root, the inputs are the hid-
den state h(p)↓ and memory cell c

(p)
↓ of its parents.

For a leaf node, in addition to h(p)↓ and c
(p)
↓ , the

inputs also contain the word embedding. For the
root node, the top-down hidden state h(r)↓ is set to

its bottom-up hidden state h(r)↑ . h
(r)
↑ captures the

semantics of all constituents, which is then repli-
cated as hr↓ and propagated downwards to each in-
dividual constituent.

Concatenating the hidden states of two direc-
tions, we obtain the LSTM encoding for each node

1407



h = [h↑;h↓] which will be the input of the atten-
tion layer. The bottom-up hidden state h↑ com-
poses the semantics of sub-constituents contained
in this constituent and the top-down hidden state
h↓ captures the contextual semantics manifested
in the entire sentence.

2.2.2 Chain-of-Trees LSTM for Passage
Encoding

To encode the passage which contains multiple
sentences, we design a chain-of-trees LSTM (Fig-
ure 4). A bi-directional tree LSTM is built for
each sentence to capture the local syntactic struc-
ture and these tree LSTMs are glued together via
a bi-directional chain LSTM (Graves et al., 2013)
to capture long-range semantics spanning multi-
ple sentences. The hidden states generated by the
bottom-up tree LSTM serves as the input of the
chain LSTM. Likewise, the chain LSTM states are
fed to the top-down tree LSTM. This enables the
encoding of every constituent to be propagated to
all other constituents in the passage.

In the chain LSTM, each sentence t is treated as
a unit. The input of this unit is generated by the
tree LSTM of sentence t, which is the bottom-up
hidden state h↑t at the root. Sentence t is associ-
ated with a forward hidden state

−→
h t and a back-

ward state
←−
h t. In the forward direction, the tran-

sition equations among the input gate
−→
i t, forget

gate
−→
f t, output gate −→o t and memory cell −→c t are:

−→
i t = σ(

−→
W(i)h↑t +

−→
U(i)
−→
h t−1 +

−→
b (i))−→

f t = σ(
−→
W(f)h↑t +

−→
U(f)

−→
h t−1 +

−→
b (f))

−→o t = σ(
−→
W(o)h↑t +

−→
U(o)
−→
h t−1 +

−→
b (o))

−→u t = tanh(
−→
W(u)h↑t +

−→
U(u)

−→
h t−1 +

−→
b (u))

−→c t =
−→
i t �−→u t +

−→
f t �−→c t−1−→

h t =
−→o t � tanh(−→c t)

(2)
The backward LSTM is defined in a similar way.
Subsequently,

−→
h t and

←−
h t, which encapsulate the

semantics of all sentences, are inputted to the root
of the top-down tree LSTM and propagated to all
the constituents in sentence t.

To sum up, the CT-LSTM encodes a passage in
the following way: (1) the bottom-up tree LSTMs
compute hidden states h↑ for each sentence and
feed h↑ of the root node into the chain LSTM; (2)
the chain LSTM computes forward and backward
states and feed them into the root of the top-down
tree LSTMs; (3) the top-down tree LSTMs com-

pute hidden states h↓. At each constituent C, the
bottom-up state h↑ captures the semantics of sub-
constituents in C and the top-down state h↓ cap-
tures the semantics of the entire passage.

2.3 Tree-Guided Attention Mechanism
We propose a tree-guided attention (TGA) mech-
anism to learn a question-aware representation for
each constituent in the passage, which consists of
three ingredients: (1) constituent-level attention
score computation; (2) tree-guided local normal-
ization; (3) tree-guided attentional summarization.
Given a constituent h(p) in the passage, for each
constituent h(q) in the question, an unnormalized
attention weight score a is computed as a = h(p) ·
h(q) which measures the similarity between the
two constituents. Then we perform a tree-guided
local normalization of these scores. At each in-
ternal node in the parse tree, where the unnormal-
ized attention scores of its L children are {al}Ll=1,
a local normalization is performed using a softmax
operation ãl = exp(al)/

∑L
m=1 exp(am) which

maps these scores into a probabilistic simplex.
This normalization scheme stands in contrast with
the global normalization adopted in word-based
attention (Wang and Jiang, 2016; Wang et al.,
2016), where a single softmax is globally applied
to the attention scores of all the words in the ques-
tion.

Given these locally normalized attention scores,
we merge the LSTM encodings of constituents in
the question into an attentional representation in
a recursive and bottom-up way. At each internal
node, let h be its LSTM encoding, a and {al}Ll=1
be the normalized attention scores of this node and
its L children, and {bl}Ll=1 be the attentional rep-
resentations (which we will define later) generated
at the children, then the attentional representation
b of this node is defined as:

b = a(h+
L∑

l=1

albl) (3)

which takes the weighted representation∑L
l=1 albl contributed from its children, adds in

its own encoding h, then performs a re-weighting
using the attention score a. The attentional rep-
resentation b(r) at the root node acts as the final
summarization of constituents in the question. We
concatenate it to the LSTM encoding h(p) of the
passage constituent and obtain a concatenated
representation z = [h(p);b(r)] which will be the
input of the candidate answer generation layer.

1408



C5

It C4

C2 C3came

from C1 in

the medical faculty

Paris

Expansion	  of	  C1	  (“the	  medical	  faculty”)
1. C1
2. from	  the	  medical	  faculty	  	  è C2
3. came	  from	  the	  medical	  faculty	  è came	  C2
4. the	  medical	  faculty	  in	  è C1	  in
5. the medical	  faculty	  in	  Paris	  è C1 C3
6. from	  the	  medical	  faculty	  	  in	  è C2 in
7. from	  the	  medical	  faculty	  in	  Paris	  è C2 C3
8. came	  from	  the	  medical	  faculty	  in	  è came C2	  in
9. came	  from	  the	  medical	  faculty	  in	  Paris	  è C4

came C2 in

Figure 5: Constituent expansion. (Left) Parse tree
of a sentence in the passage. (Top Right) Expan-
sions of constituent C1 and their reductions (de-
noted by arrow). (Bottom Right) Learning the
representation of an expansion using bidirectional
chain-LSTM.

Unlike the word-based flat-structure attention
mechanism (Wang and Jiang, 2016; Wang et al.,
2016) where the attention scores are computed be-
tween words and normalized using a single global
softmax, and the attentional summary is computed
in a flat manner, the tree-guided attention calcu-
lates attention scores between constituents, nor-
malizes them locally at each node in the parse tree
and computes the attentional summary in a hierar-
chical way. Tailored to the parse tree, TGA is able
to capture the syntactic, hierarchical and composi-
tional structures among constituents and arguably
generate better attentional representations, as we
will validate in the experiments.

2.4 Candidate Answer Generation

As shown in Figure 2, while most correct answers
in the training set are exactly constituents, some of
them are not the case. To cover the non-constituent
answers, we propose to expand each constituent
by appending words adjacent to it. Let C denote
a constituent and S = “ · · ·wi−1wiCwjwj+1 · · · ”
be the sentence containingC. We expandC by ap-
pending words preceding C (such as wi−1 and wi)
and words succeeding C (such as wj and wj+1) to
C. We define an (l, r)-expansion of a constituent
C as follows: append l words preceding C in the
sentence to C; append r words succeeding C to
C. Let M be the maximum expansion number
that l ≤ M and r ≤ M . Figure 5 shows an ex-
ample. On the left is the constituent parse tree
of the sentence “it came from the medical fac-
ulty in Paris”. On the upper right are the expan-
sions of the constituent C1 – “the medical fac-
ulty”. To expand this constituent, we trace it back
to the sentence and look up the M (M=2 in this

case) words preceding C1 (which are “came” and
“from”) and succeeding C1 (which are “in” and
“Paris”). Then combinations of C1 and the preced-
ing/succeeding words are taken to generate con-
stituent expansions. On both the left and right side
of C1, we have three choices of expansion: ex-
panding 0,1,2 words. Taking combination of these
cases, we obtain 9 expansions, including C1 itself
((0, 0)-expansion).

The next step is to perform reduction of con-
stituent expansions. Two things need to be re-
duced. First, while expanding the current con-
stituent, new constituents may come into being.
For instance, in the expansion “came from C1 in
Paris”, “in” and “Paris” form a constituent C3;
“from” and C1 form a constituent C2; “came”, C2
and C3 form a constituent C4. Eventually, this
expansion is reduced to C4. Second, the expan-
sions generated from different constituents may
have overlap and the duplicated expansions need
to be removed. For example, the (2, 1)-expansion
of C1 – “came from the medical faculty in” – can
be reduced to “came C2 in”, which is the (1, 1)-
expansion of C2. After reduction, each expansion
is a sequence of constituents.

Next we encode these candidate answers and
the encodings will be utilized in the prediction
layer. In light of the fact that each expansion is
a constituent sequence, we build a bi-directional
chain LSTM (Figure 5, bottom right) to synthe-
size the representations of individual constituents
therein. Let E = C1 · · ·Cn be an expansion con-
sisting of n constituents. In the chain LSTM, the
input of unit i is the combined representation of
Ci. We concatenate the forward hidden state at
Cn and backward state at C1 as the final represen-
tation of E.

2.5 Answer Prediction and Parameter
Learning

Given the representation of candidate answers, we
use a feed-forward network f : Rd → R to predict
the correct answer. The input of the network is the
feature vector of a candidate answer and the output
is a confidence score. The one with the largest
score is chosen as the the correct answer.

For parameter learning, we normalize the con-
fidence scores into a probabilistic simplex using
softmax and define a cross entropy loss thereupon.
Let Jk be the number of candidate answers pro-
duced from the k-th passage-question pair and

1409



{z(k)j }Jkj=1 be their representations. Let tk be the
index of the correct answer. Then the cross en-
tropy loss of K pairs is defined as

K∑

k=1

(−f(ztk) + log
Jk∑

j=1

exp(f(z
(k)
j ))) (4)

Model parameters are learned by minimizing this
loss using stochastic gradient descent.

3 Experiments

3.1 Experimental Setup
The experiments are conducted on the Stan-
ford Question Answering Dataset (SQuAD) v1.1,
which contains 107,785 questions and 23,215 pas-
sages coming from 536 Wikipedia articles. The
data was randomly partitioned into a training set
(80%), a development set (10%) and an unreleased
test set (10%). Rajpurkar et al. (2016) build a
leaderboard to evaluate and publish results on the
test set. Due to software copyright issues, we did
not participate this online evaluation. Instead, we
use the development set (which is untouched dur-
ing model training) as test set. In training, if the
correct answer is not in the candidate-answer set,
we use the shortest candidate containing the cor-
rect answer as the target.

The Stanford parser is utilized to obtain the con-
stituent parse trees for questions and passages.
In the parse tree, any internal node which has
one child is merged together with its child. For
instance, in “(NP (NNS sufferers))”, the parent
“NP” has only one child “(NNS sufferers)”, we
merge them into “(NP sufferers)”. We use 300-
dimensional word embeddings from GloVe (Pen-
nington et al., 2014) to initialize the model. Words
not found in GloVe are initialized as zero vectors.

We use a feed-forward network with 2 hidden
layers (both having the same amount of units)
for answer prediction. The activation function is
set to rectified linear. Hyperparameters in CCNN
are tuned via 5-fold cross validation (CV) on the
training set, summarized in Table 1. We use the
ADAM (Kingma and Ba, 2014) optimizer to train
the model with an initial learning rate 0.001 and a
mini-batch size 100. An ensemble model is also
trained, consisting of 10 training runs using the
same hyperparameters. The performance is eval-
uated by two metrics (Rajpurkar et al., 2016): (1)
exact match (EM) which measures the percentage
of predictions that match any one of the ground

truth answers exactly; (2) F1 score which mea-
sures the average overlap between the prediction
and ground truth answer. In the development set
each question has about three ground truth an-
swers. F1 scores with the best matching answers
are used to compute the average F1 score.

3.2 Results

Table 2 shows the performance of our model
and previous approaches on the development set.
CCNN (single model) achieves an EM score
of 69.3% and an F1 score of 78.5%, signifi-
cantly outperforming all previous approaches (sin-
gle model). Through ensembling, the perfor-
mance of CCNN is further improved and out-
performs the baseline ensemble methods. The
key difference between our method and previous
approaches is that CCNN is constituent-centric
where the generation and encoding of candidate
answers are both based on constituents while
the baseline approaches are mostly word-based
where the candidate answer is an arbitrary span
of words and the encoding is performed over in-
dividual words rather than at the constituent level.
The constituent-centric model-design enjoys two
major benefits. First, restricting the candidate
answers from arbitrary spans to neighborhoods
around the constituents greatly reduces the search
space, which mitigates the ambiguity and uncer-
tainty in picking up the correct answer. Sec-
ond, the tree LSTMs and tree-guided attention
mechanism encapsulate the syntactic, hierarchical
and compositional structure among constituents,
which leads to better representation learning of the
candidate answers. We conjecture these are the
primary reasons that CCNN outperforms the base-
lines and provide a validation in the next section.

3.3 Ablation Study

To further understand the individual modules in
CCNN, we perform an ablation study. The results
are shown in Table 2.

Tree LSTM To evaluate the effectiveness of tree
LSTM in learning syntax-aware representations,
we replace it with a syntax-agnostic chain LSTM.
We build a bi-directional chain LSTM (denoted by
A) over the entire passage to encode the individ-
ual words. Given a constituent C = wi · · ·wj ,
we build another bi-directional chain LSTM (de-
noted by B) over C where the inputs are the en-
codings of words wi, · · · , wj generated by LSTM

1410



Parameter Tuning Range Best Choice
Maximum expansion number M in constituent expansion 0, 1, 2, 3, 4, 5 2
Size of hidden state in all LSTMs 50, 100, 150, 200, 250, 300 100
Size of hidden state in prediction network 100, 200, 300, 400, 500 400

Table 1: Hyperparameter Tuning

Exact Match (EM,%) F1 (%)
Single model
Logistic Regression (Rajpurkar et al., 2016) 40.0 51.0
Fine Grained Gating (Yang et al., 2016) 60.0 71.3
Dynamic Chunk Reader (Yu et al., 2016) 62.5 71.2
Match-LSTM with Answer Pointer (Wang and Jiang, 2016) 64.1 73.9
Dynamic Coattentation Network (Xiong et al., 2016) 65.4 75.6
Multi-Perspective Context Matching (Wang et al., 2016) 66.1 75.8
Recurrent Span Representations (Lee et al., 2016) 66.4 74.9
Bi-Directional Attention Flow (Seo et al., 2016) 68.0 77.3
Ensemble
Fine Grained Gating (Yang et al., 2016) 62.4 73.4
Match-LSTM with Answer Pointer (Wang and Jiang, 2016) 67.6 76.8
Recurrent Span Representations (Lee et al., 2016) 68.2 76.7
Multi-Perspective Context Matching (Wang et al., 2016) 69.4 78.6
Dynamic Coattentation Network (Xiong et al., 2016) 70.3 79.4
Bi-Directional Attention Flow (Seo et al., 2016) 73.3 81.1
CCNN Ablation (single model)
Replacing tree LSTM with chain LSTM 63.5 73.9
Replacing chain-of-trees LSTM with independent tree LSTMs 64.8 75.2
Removing the attention layer 63.9 74.3
Replacing tree-guided attention with flat attention 65.6 75.9
CCNN (single model) 69.3 78.5
CCNN (ensemble) 74.1 82.6

Table 2: Results on the development set

A. In LSTM B, the forward hidden state of wj and
backward state of wi are concatenated to represent
C. Note that the attention mechanism remains in-
tact, which is still guided by the parse tree. This
replacement cause 5.8% and 4.6% drop of the EM
and F1 scores respectively, which demonstrates
the necessity of incorporating syntactic structure
(via tree LSTM) into representation learning.

Chain-of-Trees LSTM (CT-LSTM) We evalu-
ate the effectiveness of CT-LSTM by comparing
it with a bag of tree LSTMs: instead of using
a chain LSTM to glue the tree LSTMs, we treat
them as independent. Keeping the other modules
intact and replacing CT-LSTM with a bag of inde-
pendent tree LSTMs, the EM and F1 score drop
4.5% and 3.3% respectively. The advantage of
CT-LSTM is that it enables the semantics of one

sentence to be propagated to others, which makes
multiple-sentence reasoning possible.

Tree-Guided Attention (TGA) Mechanism To
evaluate the effectiveness of TGA, we performed
two studies. First, we take it off from the architec-
ture. Then constituents in the passage are solely
represented by the chain-of-trees LSTM encod-
ings and the question sentence is represented by
the tree LSTM encoding at the root of the parse
tree. At test time, we concatenate the encodings
of a candidate answer and the question as inputs
of the prediction network. Removing the attention
layer decreases the EM and F1 by 5.4% and 4.2%
respectively, demonstrating the effectiveness of at-
tention mechanism for question-aware representa-
tion learning.

Second, we compare the tree-structured mech-

1411



0 69 49.6 63.7

1 91 61.8 72.1

2 69.3 78.5

3 66.2 77.1

4 57.4 72.8

5 52.9 70.1

0

20

40

60

80

100

0 1 2 3 4 5

Maximum Expansion Number M 

EM

F1

40

50

60

70

80

90

1 2 3 4 5 6 7 >=8

Answer Length 

EM (MPCM)

F1 (MPCM)

EM (Our method)

F1 (Our method)
0

20

40

60

80

100

F1 (DCN)

F1 (Our method)

Figure 6: Performance for different (a) M (expansion number), (b) answer length, (c) question type.

anism in TGA with a flat-structure mechanism.
For each constituent h(p)i in the passage, we com-
pute its unnormalized score aij = h

(p)
i · h

(q)
j

with every constituent h(q)j in the question (which
has R constituents). Then a global softmax op-
eration is applied to these scores, {ãij}Rj=1 =
softmax({aij}Rj=1), to project them into a prob-
abilistic simplex. Finally, a flat summariza-
tion

∑R
j=1 ãijh

(q)
j is computed and appended to

h
(p)
i . Replacing TGA with flat-structure attention

causes the EM and F1 to drop 3.7% and 2.6% re-
spectively, which demonstrates the advantage of
the tree-guided mechanism.

Constituent Expansion We study how the max-
imum expansion number M affects performance.
If M is too small, many correct answers are not
contained in the candidate set, which results in low
recall. If M is too large, excessive candidates are
generated, making it harder to pick up the correct
one. Figure 6(a) shows how EM and F1 vary asM
increases, from which we can see a value of M in
the middle ground achieves the best tradeoff.

3.4 Analysis

In this section, we study how CCNN behaves
across different answer length (number of words
in the answer) and question types, which are
shown in Figure 6(b) and (c). In Figure 6(b), we
compare with the MPCM method (Wang et al.,
2016). As answer length increases, the perfor-
mance of both methods decreases. This is be-
cause for longer answers, it is more difficult to
pinpoint the precise boundaries. The decreasing
of F1 is slower than EM, because F1 is more elas-
tic to small mismatches. Our method achieves
larger improvement over MPCM at longer an-
swers. We conjecture the reason is: longer
answers have more complicated syntactic struc-
ture, which can be better captured by the tree
LSTMs and tree-guided attention mechanism in

our method. MPCM is built upon individual words
and is syntax-agnostic.

In Figure 6(c), we compare with DCN (Xiong
et al., 2016) on 8 question types. Our method
achieves significant improvement over DCN on
four types: “what”, “where”, “why” and “other”.
The answers of questions in these types are typi-
cally longer and have more complicated syntactic
structure than the other four types where the an-
swers are mostly entities (person, numeric, time,
etc.). The syntax-aware nature of our method
makes it outperform DCN whose model design
does not explicitly consider syntactic structures.

4 Related Works

Several neural network based approaches have
been proposed to solve the SQuAD QA problem,
which we briefly review from three aspects: can-
didate answer generation, representation learning
and attention mechanism.

Two ways were investigated for candidate an-
swer generation: (1) chunking: candidates are
preselected based on lexical and syntactic analy-
sis, such as constituent parsing (Rajpurkar et al.,
2016) and part-of-speech pattern (Yu et al., 2016);
(2) directly predicting the start and end position
of the answer span, using feed-forward neural
network (Wang et al., 2016), LSTM (Seo et al.,
2016), pointer network (Vinyals et al., 2015; Wang
and Jiang, 2016), dynamic pointer decoder (Xiong
et al., 2016).

The representation learning in previous ap-
proaches is conducted over individual words us-
ing the following encoders: LSTM in (Wang et al.,
2016; Xiong et al., 2016); bi-directional gated re-
current unit (Chung et al., 2014) in (Yu et al.,
2016); match-LSTM in (Wang and Jiang, 2016);
bi-directional LSTM in (Seo et al., 2016).

In previous approaches, the attention (Bah-
danau et al., 2014; Xu et al., 2015) mechanism
is mostly word-based and flat-structured (Kadlec
et al., 2016; Sordoni et al., 2016; Wang and Jiang,

1412



2016; Wang et al., 2016; Yu et al., 2016): the at-
tention scores are computed between individual
words, are normalized globally and are used to
summarize word-level encodings in a flat manner.
Cui et al. (2016); Xiong et al. (2016) explored
a coattention mechanism to learn question-to-
passage and passage-to-question summaries. Seo
et al. (2016) proposed to directly use the attention
weights as augmented features instead of applying
them for early summarization.

5 Conclusions and Future Work

To solve the SQuAD question answering prob-
lem, we design a constituent centric neural net-
work (CCNN), where the generation and repre-
sentation learning of candidate answers are both
based on constituents. We use a constituent ex-
pansion mechanism to produce candidate answers,
which can greatly reduce the search space with-
out losing the recall of hitting the correct an-
swer. To represent these candidate answers, we
propose a chain-of-trees LSTM to encode con-
stituents and a tree-guided attention mechanism to
learn question-aware representations. Evaluations
on the SQuAD dataset demonstrate the effective-
ness of the constituent-centric neural architecture.

For future work, we will investigate the wider
applicability of chain-of-trees LSTM as a general
text encoder that can simultaneously capture lo-
cal syntactic structure and long-range semantic de-
pendency. It can be applied to named entity recog-
nition, sentiment analysis, dialogue generation, to
name a few. We will also apply the tree-guided at-
tention mechanism to NLP tasks that need syntax-
aware attention, such as machine translation, sen-
tence summarization, textual entailment, etc. An-
other direction to explore is joint learning of syn-
tactic parser and chain-of-trees LSTM. Currently,
the two are separated, which may lead to subopti-
mal performance.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .

Danqi Chen, Jason Bolton, and Christopher D Man-
ning. 2016. A thorough examination of the
cnn/daily mail reading comprehension task. arXiv
preprint arXiv:1606.02858 .

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555 .

Peter Clark and Oren Etzioni. 2016. My computer is an
honor student-but how intelligent is it? standardized
tests as a measure of ai. AI Magazine 37(1):5–12.

Peter Clark, Oren Etzioni, Tushar Khot, Ashish Sab-
harwal, Oyvind Tafjord, Peter D Turney, and Daniel
Khashabi. 2016. Combining retrieval, statistics, and
inference to answer elementary science questions.
In AAAI. pages 2580–2586.

Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang,
Ting Liu, and Guoping Hu. 2016. Attention-over-
attention neural networks for reading comprehen-
sion. arXiv preprint arXiv:1607.04423 .

Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-
hamed. 2013. Hybrid speech recognition with deep
bidirectional lstm. In Automatic Speech Recognition
and Understanding (ASRU), 2013 IEEE Workshop
on. IEEE, pages 273–278.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems. pages 1693–
1701.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2015. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. arXiv preprint arXiv:1511.02301 .

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Mohammad Javad Hosseini, Hannaneh Hajishirzi,
Oren Etzioni, and Nate Kushman. 2014. Learning
to solve arithmetic word problems with verb catego-
rization. In EMNLP. pages 523–533.

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and
Jan Kleindienst. 2016. Text understanding with
the attention sum reader network. arXiv preprint
arXiv:1603.01547 .

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Kenton Lee, Tom Kwiatkowski, Ankur Parikh, and Di-
panjan Das. 2016. Learning recurrent span repre-
sentations for extractive question answering. arXiv
preprint arXiv:1611.01436 .

Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural language
processing toolkit.

1413



Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine
reading comprehension dataset. arXiv preprint
arXiv:1611.09268 .

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250 .

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text. In
EMNLP. volume 3, page 4.

Mrinmaya Sachan, Kumar Dubey, Eric P Xing, and
Matthew Richardson. 2015. Learning answer-
entailing structures for machine comprehension. In
ACL (1). pages 239–249.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
flow for machine comprehension. arXiv preprint
arXiv:1611.01603 .

Yelong Shen, Po-Sen Huang, Jianfeng Gao, and
Weizhu Chen. 2016. Reasonet: Learning to stop
reading in machine comprehension. arXiv preprint
arXiv:1609.05284 .

Alessandro Sordoni, Philip Bachman, Adam Trischler,
and Yoshua Bengio. 2016. Iterative alternating neu-
ral attention for machine reading. arXiv preprint
arXiv:1606.02245 .

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075 .

Zhiyang Teng and Yue Zhang. 2016. Bidirectional
tree-structured lstm with head lexicalization. arXiv
preprint arXiv:1611.06788 .

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems. pages 2692–2700.

Hai Wang and Mohit Bansal Kevin Gimpel David
McAllester. 2015. Machine comprehension with
syntax, frames, and semantics .

Shuohang Wang and Jing Jiang. 2016. Machine com-
prehension using match-lstm and answer pointer.
arXiv preprint arXiv:1608.07905 .

Zhiguo Wang, Haitao Mi, Wael Hamza, and Radu
Florian. 2016. Multi-perspective context match-
ing for machine comprehension. arXiv preprint
arXiv:1612.04211 .

Caiming Xiong, Victor Zhong, and Richard Socher.
2016. Dynamic coattention networks for question
answering. arXiv preprint arXiv:1611.01604 .

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron C Courville, Ruslan Salakhutdinov, Richard S
Zemel, and Yoshua Bengio. 2015. Show, attend and
tell: Neural image caption generation with visual at-
tention.

Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
Wikiqa: A challenge dataset for open-domain ques-
tion answering. In EMNLP. Citeseer, pages 2013–
2018.

Zhilin Yang, Bhuwan Dhingra, Ye Yuan, Junjie Hu,
William W Cohen, and Ruslan Salakhutdinov. 2016.
Words or characters? fine-grained gating for reading
comprehension. arXiv preprint arXiv:1611.01724 .

Yang Yu, Wei Zhang, Kazi Hasan, Mo Yu, Bing Xiang,
and Bowen Zhou. 2016. End-to-end answer chunk
extraction and ranking for reading comprehension.
arXiv preprint arXiv:1610.09996 .

1414


	A Constituent-Centric Neural Architecture for Reading Comprehension

