



















































Unsupervised Detection of Argumentative Units though Topic Modeling Techniques


Proceedings of the 4th Workshop on Argument Mining, pages 97–107
Copenhagen, Denmark, September 8, 2017. c©2017 Association for Computational Linguistics

Unsupervised Detection of Argumentative Units though Topic Modeling
Techniques

Alfio Ferrara and Stefano Montanelli
Dipartimento di Informatica, Università degli Studi di Milano

Via Comelico 39, 20135 - Milano, Italy
{alfio.ferrara,stefano.montanelli}@unimi.it

Georgios Petasis
Institute of Informatics and Telecommunications,

National Centre for Scientific Research (N.C.S.R.) “Demokritos”
P.O. BOX 60228, Aghia Paraskevi, GR-153 10, Athens, Greece

petasis@iit.demokritos.gr

Abstract

In this paper we present a new unsuper-
vised approach, “Attraction to Topics” –
A2T , for the detection of argumentative
units, a sub-task of argument mining. Mo-
tivated by the importance of topic identi-
fication in manual annotation, we exam-
ine whether topic modeling can be used
for performing unsupervised detection of
argumentative sentences, and to what ex-
tend topic modeling can be used to classify
sentences as claims and premises. Prelim-
inary evaluation results suggest that topic
information can be successfully used for
the detection of argumentative sentences,
at least for corpora used in the evaluation.
Our approach has been evaluated on two
English corpora, the first of which con-
tains 90 persuasive essays, while the sec-
ond is a collection of 340 documents from
user generated content.

1 Introduction

Argument mining involves the automatic dis-
covery of argument components (i.e. claims,
premises) and the argumentative relations (i.e.
supports, attacks) among these components in
texts. Primarily aiming to extract arguments from
texts in order to provide structured data for compu-
tational models of argument and reasoning engines
(Lippi and Torroni, 2015a), argument mining has
additionally the potential to support applications
in various research fields, such as opinion mining
(Goudas et al., 2015), stance detection (Hasan and
Ng, 2014), policy modelling (Florou et al., 2013;
Goudas et al., 2014), legal information systems
(Palau and Moens, 2009), etc.

Argument mining is usually addressed as a
pipeline of several sub-tasks. Typically the first
sub-task is the separation between argumenta-
tive and non-argumentative text units, which can
be performed at various granularity levels, from
clauses to several sentences, usually depending on
corpora characteristics. Detection of argumenta-
tive units (AU)1, as discussed in Section 2, is typ-
ically modeled as a fully-supervised classification
task, either a binary one, where units are separated
in argumentative and non-argumentative ones with
argumentative ones to be subsequently classified
in claims and premises as a second step, or as
a multi-class one, where identification of argu-
mentative units and classification into claims and
premises are performed as a single step. Accord-
ing to a recent survey (Lippi and Torroni, 2015a),
the performance of proposed approaches depends
on highly engineered and sophisticated, manually
constructed, features.

However, fully-supervised approaches rely on
manually annotated datasets, the construction of
which is a laborious, costly, and error-prone pro-
cess, requiring significant effort from human ex-
perts. At the same time, reliance on sophisticated
features may hinder the generalisation of an ap-
proach to new corpora types and domains (Lippi
and Torroni, 2015a). The removal of manual su-
pervision through exploitation of unsupervised ap-
proaches is a possible solution to both of the afore-
mentioned problems.

1.1 Motivations of our work

Topics seem to be related to the task of argument
mining, at least for some types of corpora, as topic

1Also known as “Argumentative Discourse Units –
ADUs” (Peldszus and Stede, 2013).

97



identification frequently appears as a step in the
process of manual annotation of arguments in texts
(Stab and Gurevych, 2014a). However, despite its
apparent importance in manual annotation, only
a small number of studies have examined the in-
clusion of topic information in sub-tasks of argu-
ment mining. Habernal and Gurevych (2015) have
included sentiment and topic information as fea-
tures for classifying sentences as claims, premises,
backing and non-argumentative units. A less di-
rect exploitation of topic information has been pre-
sented in (Nguyen and Litman, 2015), where top-
ics have been used to extract lexicons of argument
and domain words, which can provide evidence re-
garding the existence of argument components.

In this paper we propose “Attraction to Top-
ics” – A2T , an unsupervised approach based on
topic modeling techniques for detecting argumen-
tative discourse units at sentence-level granularity
(a sub-task known as “argumentative sentence de-
tection”). The goals of A2T are twofold. On the
one side,A2T enforces identification of sentences
that contain argument components, by also dis-
tinguishing them from the non-argumentative sen-
tences that do not contain argument components.
On the other side, A2T classifies the discovered
argumentative sentences according to their role, as
major claims, claims, and premises.

The rest of the paper is organized as follows:
Section 2 presents an overview of approaches re-
lated to argument mining focusing on the detection
of argumentative units, while Section 3 presents
our approach on applying topic modeling for iden-
tifying sentences that contain argument compo-
nents. Section 4 presents our experimental setting
and evaluation results, with Section 5 concluding
this paper and proposing some directions for fur-
ther research.

2 Related work

Almost all argument mining frameworks proposed
so far employ a pipeline of stages, each of which
is addressing a sub-task of the argument mining
problem (Lippi and Torroni, 2015a). The segmen-
tation of text into argumentative units is typically
the first sub-task encountered in such an argument
mining pipeline, aiming to segment texts into ar-
gumentative and non-argumentative text units (i.e.
segments that do contain or do not contain argu-
ment components, such as claims or premises).
The granularity of argument components is text-

dependant. For example, in Wikipedia articles
studied in (Rinott et al., 2015), argument compo-
nents spanned from less than a sentence to more
than a paragraph, although 90% of the cases was
up to 3 sentences, with 95% of components being
comprised of whole sentences.

Several approaches address the identification of
argumentative units at the sentence level, a sub-
task known as “argumentative sentence detection”,
which typically models the task as a binary clas-
sification problem. Employing machine learn-
ing and a set of features representing sentences,
the goal is to discard sentences that are not part
(or do not contain a component) of an argument.
As reported also by Lippi and Torroni (2015a),
the vast majority of existing approaches employ
“classic, off-the-self” classifiers, while most of
the effort is devoted to highly engineered features.
A plethora of learning algorithms have been ap-
plied on the task, including Naive Bayes (Moens
et al., 2007; Park and Cardie, 2014), Support Vec-
tor Machines (SVM) (Mochales and Moens, 2011;
Rooney et al., 2012; Park and Cardie, 2014; Stab
and Gurevych, 2014b; Lippi and Torroni, 2015b),
Maximum Entropy (Mochales and Moens, 2011),
Logistic Regression (Goudas et al., 2014, 2015;
Levy et al., 2014), Decision Trees and Random
Forests (Goudas et al., 2014, 2015; Stab and
Gurevych, 2014b).

However, approaches addressing this task in a
semi-supervised or unsupervised manner are still
scarce. In (Petasis and Karkaletsis, 2016) an unsu-
pervised approach is presented, which addresses
the sub-task of identifying the main claim in a
document by exploiting evidence from an extrac-
tive summarization algorithm, TextRank (Mihal-
cea and Tarau, 2004). In an attempt to study the
overlap between graph-based approaches and ap-
proaches targeting extractive summarization with
argument mining, evaluation results suggest a pos-
itive effect on the sub-task, achieving an accu-
racy of 50% on the corpus compiled by Hasan and
Ng (2014) from online debate forums and on a
corpus of persuasive essays (Stab and Gurevych,
2014a). Regarding semi-supervised approaches,
Habernal and Gurevych (2015) propose new un-
supervised features that exploit clustering of un-
labeled argumentative data from debate portals
based on word embeddings, outperforming several
baselines. This work employs also topic modeling
as one of its features, by including as features the

98



distributions of sentences from LDA (Blei et al.,
2003).

Topic modeling has been mainly exploited for
identification of argumentative relations and for
extraction of argument and domain lexicons. In
Lawrence et al. (2014), LDA is used to decide
whether a proposition can be attached to its pre-
vious proposition in order to identify non di-
rectional relations among propositions detected
through classifiers based on words and part-of-
speech tags. LDA has been also used to mine
lexicons of argument (words that are topic inde-
pendent) and domain words (Nguyen and Litman,
2015), by post-processing document topics gener-
ated by LDA. These lexicons have been used as
features for supervised approaches for argument
mining (Nguyen and Litman, 2016a,b). However,
to the best of our knowledge, no prior approach
has applied topic modeling to argumentative sen-
tence detection in an unsupervised setting, which
is the featuring aspect of the proposed A2T ap-
proach presented in the following.

3 Topic modeling for argument mining

Given a document corpus, topic modeling tech-
niques can be employed to discover the most rep-
resentative topics throughout the corpus, and to
provide an assignment of documents to topics,
meaning that the higher is the assignment value
of a document to a certain topic, the higher is the
probability that the document is “focused” on that
topic.

The idea of A2T is that an argumentative unit
is a sentence highly focused on a specific topic,
namely a sentence with high assignment value to
a certain topic and low assignment value to the
other topics. To this end, A2T introduces the no-
tion of attraction with the aim at recognizing the
sentences highly focused on specific topics, that
represent the recognized argumentative units. In
the following, theA2T approach and related tech-
niques are described in detail.

3.1 A2T approach
The schema of the A2T approach is shown in
Figure 1. Consider a corpus of texts C =
{c1, . . . , cn}, where a text ci ∈ C is a sequence
of sentences, like for example an essay, a web
page/post, or a scientific paper. The ultimate goal
of theA2T approach is to derive a set of argumen-
tative units U = {〈s1, c, l〉, . . . , 〈sh, c, l〉}, where

Corpus of 
Texts ( C )

Argumentative 
Units ( U )

Sentence 
Extraction

Attraction 
Evaluation

Topic
Modelling

Sentence
Index ( S )

Figure 1: Schema of the A2T approach

si is a sentence containing an argumentative unit,
c is the text containing s, and l is the argumenta-
tive role expressed by the unit (e.g., major claim,
claim, premise). The A2T approach is articulated
in the following activities:

Sentence extraction. A2T approach is charac-
terized by the use of topic modeling at sentence-
level granularity. For this reason, a pre-processing
step of the corpus C is enforced based on conven-
tional techniques for sentence tokenization, words
tokenization, normalization, and indexing (Man-
ning et al., 2008). The result is a sentence set
S = {〈−→s1 , c, pos1〉, . . . , 〈−→sm, c, posm〉}, where −→si
is the vector representation of the sentence si and
c, pos are text and position in the text where the
sentence appears, respectively. The sentence set is
stored in a sentence index for efficient access of S
elements.

Topic modeling. The set of extracted sentences
S is used as the document corpus on which topic
modeling is applied. The result of this activity is
twofold. First, topic modeling returns a set of top-
ics T = {t0, . . . , tk} representing the latent vari-
ables that are most representative for the sentences
S. Second, topic modeling returns a distribution of
sentences over topics θ = {θs1 , . . . , θsm}. In par-
ticular, θsi = [p(t0|si), . . . , p(tk|si)] is the prob-
ability distribution of the sentence si over the set
of topics T , where p(tj |si) represents the proba-
bility of the topic tj given the sentence si (i.e., the
so-called assignment value of si to tj).

Attraction evaluation. The notion of attraction
is introduced to measure the degree of focus
that characterizes sentences with respect to the
emerged topics. To this end, the distribution of
sentences over topics θ is exploited with the aim
at determining the best topic assignment for each
sentence of S. The result is an attraction set
A = {〈s1, a1〉, . . . , 〈sm, am〉} where si is a sen-
tence of S and ai is its corresponding attraction

99



value.

Sentence labeling. By exploiting the attraction
set A, labeling has the goal to determine the sen-
tences of S that are more focused on a specific
topic, according to the hypothesis that those sen-
tences are the argumentative units. In a basic sce-
nario, labeling consists in distinguishing between
sentences that are argumentative units (l = au)
and sentences that are not argumentative units (l =
au). In a more articulated scenario, labeling con-
sists in assigning a role to sentences that are rec-
ognized as argumentative units. For instance, it
is possible to distinguish argumentative-unit sen-
tences that are claims (l = cl), major claims
(l = mc), or premises (l = pr). A sentence s
recognized as argumentative unit is inserted in the
final set U with the assigned label and it is returned
as a result of A2T .
3.2 A2T techniques
In A2T , the sentence extraction step is enforced
by relying on standard techniques for represent-
ing documents in terms of feature vectors and bag
of words (using tf-idf as weighting scheme) (Cas-
tano et al., 2017). Probabilistic topic modeling is
exploited to enforce the subsequent topic model-
ing step. Probabilistic topic models are a suite
of algorithms whose aim is to discover the hid-
den thematic structure in large archives of doc-
uments, namely sentences in A2T . The idea is
that documents are represented as random mix-
tures over latent topics, where each topic is char-
acterized by a distribution over words (Blei et al.,
2003). Probabilistic topic modeling algorithms in-
fer the distribution θ of documents over topics and
the distribution φ of words over topics, by sam-
pling from the bag of words of each document. In
our approach, we choose to exploit the Hierarchi-
cal Dirichlet Process (HDP). With respect to other
algorithms (such as LDA), HDP has the advantage
to provide the optimal number of topics instead of
requiring to set such a number as input (Teh et al.,
2006).

Attraction evaluation. The notion of attraction
is introduced in A2T to capture the intuition that
argumentative units are related to the distribution
of sentences over topics. Consider a set of sen-
tences S and the distribution θ of sentences over
the set of topics T . The more the distribution θsi
of a sentence si over the topics is unequal, the
more si is focused on a topic, thus suggesting si

as a possible argumentative unit. A further feature
that attraction aims to capture is that argumenta-
tive units often appear either at the beginning or at
the end of texts. The attraction ai of a sentence si
is calculated as follows:

ai = Kϕsi + (1−K)
ρsi∑

sj∈c
ρsj

,

ϕsi = max(θsi) is a measure of how much si
is focused on a topic and ρsi = αf(posi)

2 +
βf(posi) + γ is a parabolic function over the po-
sition of the sentence in c. In particular, given
L(c) as the number of sentences in c, f(posi) =∣∣∣L(c)2 − posi∣∣∣ such that f(posi) is higher when si
appears either at the beginning or at the end of
c. The parameters α, β, γ determine the shape
of ρsi . K ∈ [0, 1] is a constant value used to
balance the role of focus and position in calcu-
lating the attraction. The attraction ai can be in-
terpreted as the probability of a sentence si to
contain an argumentative unit. According to this
interpretation, given si, also the contiguous sen-
tences si−1 and si+1 have a chance to be argu-
mentative units. As a result, given the calculated
attraction setA, we update the attraction values ai
through an interpolation mechanism based on the
Savitzky-Golay smoothing filter (SGF) (Savitzky
and Golay, 1964), so that A := SGF (A).

In Figure 2, an example of attraction evalua-
tion is provided by showing the values of ϕ, ρ,
attraction, and interpolated attraction for all the
sentences within one considered student essays in-
cluded in the corpus from (Stab and Gurevych,
2014a) (see Section 4).

s 0 s 1 s 2 s 3 s 4 s 5 s 6 s 7 s 8 s 9 s 1
0

s 1
1

s 1
2

s 1
3

s 1
4

s 1
5

s 1
6

s 1
7

s 1
8

s 1
9

s 2
0

s 2
1

s 2
2

s 2
3

s 2
4

s 2
5

s 2
6

s 2
7

s 2
8

s 2
9

s 3
0

s 3
1

s 3
2

s 3
3

0.0

0.2

0.4

0.6

0.8

1.0

si (focus)
si (position)

ai (attraction)
ai := SGF(ai)

Figure 2: Attraction evaluation for the sentences
of a considered text

100



Sentence labeling. Sentence labeling has the
goal to turn attraction values into labeled cate-
gories. Consider a set of possible labels L =
{l1, . . . , lg}, each one denoting a possible argu-
mentative role that can be assigned to a sentence.
Given a set of attraction values A, a threshold-
based mechanism is enforced to assign labels to
sentences according to the following scheme:

ai < τ1 : si ← l1
τ1 ≤ ai < τ2 : si ← l2

. . . . . . . . .
ai ≥ τg−1 : si ← lg

where τ1 < τ2 < ... < τg−1 (τ1, . . . τg−1 ∈ (0, 1])
are prefixed threshold values. The result of sen-
tence labeling is a partition of S into g categories
with associated labels.

In the experiments, we discuss two different
strategies for sentence labeling. The first one is a
two-class labeling strategy where the possible la-
bels for a sentence are argumentative unit (au) and
non-argumentative unit (au. The second strategy
is a multi-class labeling in which the possible la-
bels of a sentence are non-argumentative unit au,
premise (pr), claim (cl), and major claim (mc).

4 Experimental results

For evaluation of the proposed A2T approach,
we have used two English corpora. The first
corpus (C1 in the following) is a collection of
90 student persuasive essays (Stab and Gurevych,
2014a) which has been manually annotated with
major claims (one per essay), claims and premises
at the clause level. In addition, the corpus con-
tains manual annotations of argumentative rela-
tions, where the claims and premises are linked,
while claims are linked to the major claim ei-
ther with a support or an attack relation. Inter-
annotation agreement has been measured to uni-
tized alpha (Krippendorff, 2004) αU = 0.724.
These 90 essays consist of a total of 1, 675 sen-
tences (from which 19.3% contain no argument
components), with an average length of 18.61± 7
sentences per essay, while the 5.4% of sentences
contain a major claim, 26.4% contain a claim, and
61.1% contain a premise.

The second corpus (C2 in the following) has
been compiled and manually annotated as de-
scribed in (Habernal and Gurevych, 2017). This
corpus focuses on user generated content, includ-
ing user comments, forum posts, blogs, and news-
paper articles, covering several thematic domains

from educational controversies, such as home-
schooling, private vs. public schools, or single-
sex education. Containing in total 340 documents,
the corpus has been manually annotated with an
argument scheme based on extended Toulmin’s
model, involving claims, premises, and backing,
rebuttal, refutation argument units. The corpus
contains documents of various sizes, with a mean
size of 11.44 ± 11.70 sentences per document,
while the inter-annotator agreement was measured
as αU = 0.48. The corpus consists of 3,899 sen-
tences, from which 2,214 sentences (57%) contain
no argument components.

Both corpora have been preprocessed with
NLTK (Loper and Bird, 2002) in order to identify
tokens and sentences. Then, each sentence was
annotated as argumentative or non-argumentative,
depending on whether it contained an argument
unit (i.e. a text fragment annotated as major claim,
claim, or premise). In addition, each argumenta-
tive sentence was further annotated with one of
major claim, claim, and premise, based on the
type of the contained argumentative unit. For the
second corpus, which utilizes a richer argument
scheme, we have considered backing, rebuttal and
refutation units as premises. This second corpus
does not contain units annotated as major claims.
The following three tasks have been executed:
• Task 1: Argumentative sentence identifica-

tion – given a sentence, classify whether or
not it contains an argument component.
• Task 2: Major claim identification – given a

argumentative sentence, classify whether or
not it contains a major claim.
• Task 3: Argumentative sentence classifica-

tion – given a sentence, classify the sen-
tence as major claim, claim, premise, or non-
argumentative.

Baseline. As a baseline for comparison against
our approach, we created a probabilistic classi-
fier of sentences which evaluates the probability
p(l = au|si) as follows. Given the text c con-
taining L(c) sentences si, let be ζc ∼ Dir(α)
the probability distribution of the sentences in c,
such that ζsic ∼ p(l = au|si). The L(c) pa-
rameters α used to generate ζc are defined such
that αi =

∣∣∣L(c)2 − posi∣∣∣. The rationale of this
procedure is to bias the random assignment of a
sentence to the au label in favor of sentences ap-
pearing either in the beginning or in the end of a
text. This bias attempts to model empirical evi-

101



dence that in several types of documents, the den-
sity of argumentative units in various sections of
documents depends on the structure of documents.
The beginning and end of a document are expected
to contain argumentative units in structured doc-
uments like news, scientific publications, or ar-
gumentative essays (Stab and Gurevych, 2017),
where major claims and supporting premises are
frequently found in the beginning of documents,
with documents frequently ending with repeating
the major claims and supporting evidence.

4.1 Task 1: Argumentative sentence
identification

The goal of Task 1 is to associate each sentence of
the corpora to a label in L = {au, au} by follow-
ing a two-class labeling strategy (see Section 3).
As a first experiment, we performed sentence la-
beling with different threshold ranging from 0 to
1 with step 0.05. In Figure 3, we report the preci-
sion, recall, and F1-measure for A2T and for the
baseline. In addition, we report also the results
of applying sentence labeling based on ϕ and ρ
(the components of attraction) separately. The pa-
rameter K for attraction calculation has been set
to 0.5. Since A2T is an unsupervised method,
there is no easy way to define the threshold pa-
rameter τ , which has been empirically defined to
τ = 0.3. The different behavior of A2T with re-
spect to the baseline is shown in the confusion ma-
trices reported in Figures 4 and 5.

From Figure 3, we can see that A2T is sig-
nificantly better than the baseline, especially for
the C1 corpus. A characteristic of this corpus
is that argumentative units are frequently located
in the introduction or the conclusion of an es-
say, which is also reflected by the baseline that
achieved an F1-measure of 0.35 for a threshold
of τ = 0.05 (with the baseline being particu-
larly precise, suggesting that argumentative units
are very frequently at the beginning and end of es-
says). Both components of attraction (ϕ and ρ)
perform well, with the topic component ϕ being
slightly better than position information ρ, both
in precision and recall. The results are similar
for corpus C2, with A2T surpassing the baseline,
although A2T advantage in precision is smaller.
As shown in the confusion matrix of Figure 5,
the main source of error is the large number of
false positives for the au class, proposing more
argumentative units than what have been manu-

ally identified in corpus C2. This can be attributed
to the sparseness of argumentative units in the C2
corpus, with almost 60% of the sentences being
non-argumentative.

4.2 Task 2: Major claim identification

As a second experiment, we exploited probabili-
ties associated with sentences to perform a ranked
evaluation. In particular, we calculated two mea-
sures, namely P that is the area the under the
precision-recall curve and R that is the area under
the receiver operating characteristic (ROC) curve.
In this experiments, we used different criteria for
defining the true labels: in PCM , an annotated
sentence in the corpus is considered a true argu-
mentative unit if it is either a premise, a claim, or a
major claim; in CM only claims and major claims
are taken as valid au; in M only major claims are
taken into account. Results are reported in Table 1.

Table 1: Area under the precision-recall (P) and
the ROC (R) curves

C1 C2
P PCM CM M PCM CM

A2T 0.79 0.31 0.08 0.26 0.19
ϕ 0.84 0.29 0.06 0.19 0.1
ρ 0.68 0.29 0.09 0.24 0.19

Baseline 0.68 0.31 0.11 0.16 0.06
R PCM CM M PCM CM

A2T 0.4 0.52 0.62 0.7 0.76
ϕ 0.52 0.51 0.53 0.58 0.57
ρ 0.16 0.52 0.77 0.69 0.77

Baseline 0.16 0.53 0.79 0.31 0.18

4.3 Task 3: Argumentative sentence
classification

The goal of Task 3 is to associate each sentence
of the corpora to a label in L = {au, pr, cl,mc}
by following a multi-class labeling strategy (see
Section 3). In particular, we adopted the thresh-
olds [0.1, 0.3, 0.5]. This task is challenging since it
is required to distinguish the different role played
in argumentation by sentences that are often very
similar from the terminological point of view. The
confusion matrix for corpus C1 is shown in Fig-
ure 6, while Figure 7 shows the confusion ma-
trix for corpus C2. Both A2T and the baseline
achieve low results, but the accuracy ofA2T is 0.3
against the 0.1 of the baseline. From Figure 6 we
see that A2T achieved good results for premises,
and quite good results for claims, although distin-
guishing between claims and premises is challeng-
ing for the A2T approach. In particular, the role

102



C1

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Threshold

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Precision

A2T

Baseline
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

Threshold

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Recall

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Threshold

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

F1-measure

C2

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Threshold

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Precision
A2T

Baseline

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Threshold

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Recall

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Threshold

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

F1-measure

Figure 3: Precision, Recall and F1-measure with different thresholds

au au
Predicted label

au

au

Tr
ue

 la
be

l

122 148

789 545

A2T

au au
Predicted label

au

au

Tr
ue

 la
be

l

228 42

1331 3

Baseline

200

300

400

500

600

700

200

400

600

800

1000

1200

Figure 4: Two-class confusion matrices for corpus
C1 (Threshold τ = 0.3)

of sentences may change in different texts so that
claims in one context are premises in another. This
kind of contextual shift is only partially addressed

au au
Predicted label

au

au

Tr
ue

 la
be

l

1790 1504

152 430

A2T

au au
Predicted label

au

au

Tr
ue

 la
be

l

3037 257

515 67

Baseline

200

400

600

800

1000

1200

1400

1600

500

1000

1500

2000

2500

3000

Figure 5: Two-class confusion matrices for corpus
C2 (Threshold τ = 0.3)

by A2T , because the only contextual information
we take into account is topic distribution. To the
end of improving the understanding of the context,

103



au pr cl mc
Predicted label

au

pr

cl

mc

Tr
ue

 la
be

l

31 91 94 54

213 313 232 101

84 145 104 55

4 30 36 17

A2T

au pr cl mc
Predicted label

au

pr

cl

mc

Tr
ue

 la
be

l

111 117 41 1

800 58 1 0

335 52 1 0

50 36 1 0

Baseline

50

100

150

200

250

300

0

100

200

300

400

500

600

700

800

Figure 6: Multi-class confusion matrices for corpus C1

au pr cl mc
Predicted label

au

pr

cl

mc

Tr
ue

 la
be

l

1790 1033 469 2

111 104 60 0

41 136 129 1

0 0 0 0

A2T

au pr cl mc
Predicted label

au

pr

cl

mc

Tr
ue

 la
be

l

3037 18 93 146

230 0 5 40

285 0 5 17

0 0 0 0

Baseline

0

200

400

600

800

1000

1200

1400

1600

0

500

1000

1500

2000

2500

3000

Figure 7: Multi-class confusion matrices for corpus C2

it may be useful to work also on semantic relations
holding among sentences. This is actually one of
the future tasks in our research work.

Another specific challenge emerges when we
consider the corpus C2. Indeed, C2 contains a
limited number of argumentative sentences with
respect to the corpus size. In this case, since we
analyze all the sentences according to their bag
of words, we tend to overestimate the number of
argumentative units, collecting a relatively high
number of false positives.

4.4 Lessons learned from error analysis

A first evidence emerging from the analysis of
confusion matrices for both corpora C1 and C2 is
that the role of sentences is strictly dependent on
the type of documents. C1 contains structured es-
says of various topics, while C2 provides conver-
sational texts extracted from blogs and chats. In
the first case, the number of argumentative units
is higher than in the second one. In particular, for
C2 we overestimated the probability of sentences
to be an argumentative unit. This is mainly due

to the fact that those sentences contain words that
are semantically related to the main topic of the
conversation although they are not playing a role
in the argumentation. An example is the follow-
ing sentence, taken from a document associated
with the topic “school”: “why do some parents
not think their kids can attain?”. The sentence
is clearly part of a conversation and it has been
annotated as a non argumentative unit because it
is a question. However, since it contains words
that are relevant for the topic (i.e., parents, kids,
attain), A2T associates the sentence with a good
level of attraction, labeling it as a premise. In or-
der to address this kind of false positives, we aim
in our future work to study the dependency rela-
tions among sentences in text (such as question-
answers) to the goal of achieving a better insight
of the sentences role.

A second lesson learned from error analy-
sis concerns the distinction between claims and
premises. This confusion is evident especially
when dealing with corpus C1. An example is
given by the following two sentences, taken from

104



an essay about the role of sports in favor of peace.

• (s1) for example, when Irak was hardly struck
by the second gulf war, its citizens tried to
catch any incoming news about the football-
world cup through their portable receivers.

• (s2) thus, world sports events strongly par-
ticipate in eventually pulling back people to-
wards friendship and peace

The sentence (s1) has been annotated as a
premise, while (s2) as a claim. In our classifica-
tion, they are both claims. The reason is that they
both contain topic-related words and their position
in text is similar. The main distinction is the pres-
ence of the expression “for example” in the first
sentence which qualifies it as a premise. To this
end, in our future work we aim at adding some
special words (such as “for example”, “therefore”)
in the background knowledge of the classifier, in
order to improve the capability of discriminating
premises and claims.

5 Concluding remarks

In this paper, we present the “Attraction to Topics”
– A2T unsupervised approach for detecting argu-
mentative discourse units, at sentence-level gran-
ularity. Motivated by the observation that topic
information is frequently employed as a sub-task
in the process of manual annotation of arguments,
we propose an approach that exploits topic mod-
eling techniques in order to identify argumenta-
tive units. Since manual supervision is not re-
quired, A2T has the potential to be applicable on
documents of various genres and domains. Pre-
liminary evaluation results on two different cor-
pora are promising. First, A2T performs signif-
icantly better than the baseline on argumentative
sentence detection on both corpora. Second, A2T
exhibits good results for classifying argumentative
sentences as major claims, claims, premises, and
non-argumentative units, at least for the first cor-
pus, which has a low rate of non-argumentative
sentences ( 20%).

Regarding directions for further research, there
are several axes that can be explored. Evaluation
on a larger set of annotation corpora will provide
enhanced insights about the performance of the
proposed approach on different document types.
Our preliminary results showed that despite good
recall on multiple corpora, achieving also good

precision can be a challenging task in documents
where argumentative units are sparse, and false
positives can be an issue. In this context, we would
like to also exploit other types of relations, and ex-
tend our method with other kinds of similarities
over sentences.

References
David M. Blei, Andrew Y. Ng, and Michael I. Jor-

dan. 2003. Latent dirichlet allocation. J. Mach.
Learn. Res. 3:993–1022. http://dl.acm.org/cita-
tion.cfm?id=944919.944937.

Silvana Castano, Alfio Ferrara, and Stefano Mon-
tanelli. 2017. Exploratory analysis of textual data
streams. Future Generation Computer Systems
68:391–406.

Eirini Florou, Stasinos Konstantopoulos, Antonis
Koukourikos, and Pythagoras Karampiperis. 2013.
Argument extraction for supporting public policy
formulation. In Piroska Lendvai and Kalliopi
Zervanou, editors, Proceedings of the 7th Work-
shop on Language Technology for Cultural Her-
itage, Social Sciences, and Humanities, LaT-
eCH@ACL 2013, August 8, 2013, Sofia, Bulgaria.
The Association for Computer Linguistics, pages
49–54. http://aclweb.org/anthology/W/W13/W13-
2707.pdf.

Theodosis Goudas, Christos Louizos, Georgios Peta-
sis, and Vangelis Karkaletsis. 2014. Argument ex-
traction from news, blogs, and social media. In
Aristidis Likas, Konstantinos Blekas, and Dim-
itris Kalles, editors, Artificial Intelligence: Methods
and Applications: 8th Hellenic Conference on AI,
SETN 2014, Ioannina, Greece, May 15-17, 2014.
Proceedings, Springer International Publishing,
Cham, pages 287–299. https://doi.org/10.1007/978-
3-319-07064-3 23.

Theodosis Goudas, Christos Louizos, Georgios
Petasis, and Vangelis Karkaletsis. 2015. Ar-
gument extraction from news, blogs, and
the social web. International Journal on
Artificial Intelligence Tools 24(05):1540024.
https://doi.org/10.1142/S0218213015400242.

Ivan Habernal and Iryna Gurevych. 2015. Exploit-
ing debate portals for semi-supervised argumenta-
tion mining in user-generated web discourse. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. Associ-
ation for Computational Linguistics, Lisbon, Por-
tugal, pages 2127–2137. http://aclweb.org/anthol-
ogy/D15-1255.

Ivan Habernal and Iryna Gurevych. 2017. Ar-
gumentation mining in user-generated web dis-
course. Computational Linguistics 43(1):125–179.
https://doi.org/10.1162/COLI a 00276.

105



Kazi Saidul Hasan and Vincent Ng. 2014. Why are
you taking this stance? identifying and classifying
reasons in ideological debates. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). Association
for Computational Linguistics, Doha, Qatar, pages
751–762. http://www.aclweb.org/anthology/D14-
1083.

Klaus Krippendorff. 2004. Measuring the reliability
of qualitative text analysis data. Quality and Quan-
tity 38(6):787–800. https://doi.org/10.1007/s11135-
004-8107-7.

John Lawrence, Chris Reed, Colin Allen, Simon McAl-
ister, and Andrew Ravenscroft. 2014. Mining ar-
guments from 19th century philosophical texts us-
ing topic based modelling. In Proceedings of the
First Workshop on Argumentation Mining. Asso-
ciation for Computational Linguistics, Baltimore,
Maryland, pages 79–87. http://www.aclweb.org/an-
thology/W/W14/W14-2111.

Ran Levy, Yonatan Bilu, Daniel Hershcovich, Ehud
Aharoni, and Noam Slonim. 2014. Context depen-
dent claim detection. In Jan Hajic and Junichi Tsu-
jii, editors, COLING 2014, 25th International Con-
ference on Computational Linguistics, Proceedings
of the Conference: Technical Papers, August 23-
29, 2014, Dublin, Ireland. ACL, pages 1489–1500.
http://aclweb.org/anthology/C/C14/C14-1141.pdf.

Marco Lippi and Paolo Torroni. 2015a. Argument
mining: A machine learning perspective. In Eliz-
abeth Black, Sanjay Modgil, and Nir Oren, edi-
tors, Theory and Applications of Formal Argumen-
tation: Third International Workshop, TAFA 2015,
Buenos Aires, Argentina, July 25-26, 2015, Revised
Selected Papers. Springer International Publishing,
Cham, pages 163–176. https://doi.org/10.1007/978-
3-319-28460-6 10.

Marco Lippi and Paolo Torroni. 2015b. Context-
independent claim detection for argument mining.
In Proceedings of the 24th International Confer-
ence on Artificial Intelligence. AAAI Press, IJ-
CAI’15, pages 185–191. http://dl.acm.org/cita-
tion.cfm?id=2832249.2832275.

Edward Loper and Steven Bird. 2002. Nltk: The
natural language toolkit. In Proceedings of the
ACL-02 Workshop on Effective Tools and Method-
ologies for Teaching Natural Language Process-
ing and Computational Linguistics - Volume 1. As-
sociation for Computational Linguistics, Strouds-
burg, PA, USA, ETMTNLP ’02, pages 63–70.
https://doi.org/10.3115/1118108.1118117.

Christopher D Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to informa-
tion retrieval, volume 1. Cambridge university press
Cambridge.

Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Dekang Lin and Dekai

Wu, editors, Proceedings of EMNLP 2004. Asso-
ciation for Computational Linguistics, Barcelona,
Spain, pages 404–411. http://www.aclweb.org/an-
thology/W/W04/W04-3252.pdf.

Raquel Mochales and Marie-Francine Moens. 2011.
Argumentation mining. Artificial Intelligence and
Law 19(1):1–22. https://doi.org/10.1007/s10506-
010-9104-x.

Marie-Francine Moens, Erik Boiy, Raquel Mochales
Palau, and Chris Reed. 2007. Automatic de-
tection of arguments in legal texts. In Pro-
ceedings of the 11th International Conference
on Artificial Intelligence and Law. ACM, New
York, NY, USA, ICAIL ’07, pages 225–230.
https://doi.org/10.1145/1276318.1276362.

Huy Nguyen and Diane J. Litman. 2015. Extracting
argument and domain words for identifying argu-
ment components in texts. In Proceedings of the
2nd Workshop on Argumentation Mining, ArgMin-
ing@HLT-NAACL 2015, June 4, 2015, Denver, Col-
orado, USA. The Association for Computational
Linguistics, pages 22–28. http://aclweb.org/anthol-
ogy/W/W15/W15-0503.pdf.

Huy Nguyen and Diane J. Litman. 2016a. Context-
aware argumentative relation mining. In Proceed-
ings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL 2016, Au-
gust 7-12, 2016, Berlin, Germany, Volume 1: Long
Papers. The Association for Computer Linguistics.
http://aclweb.org/anthology/P/P16/P16-1107.pdf.

Huy Nguyen and Diane J. Litman. 2016b. Improving
argument mining in student essays by learning
and exploiting argument indicators versus essay
topics. In Zdravko Markov and Ingrid Russell,
editors, Proceedings of the Twenty-Ninth Inter-
national Florida Artificial Intelligence Research
Society Conference, FLAIRS 2016, Key Largo,
Florida, May 16-18, 2016.. AAAI Press, pages
485–490. http://www.aaai.org/ocs/index.php/-
FLAIRS/FLAIRS16/paper/view/12791.

Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation mining: The detection,
classification and structure of arguments in text.
In Proceedings of the 12th International Confer-
ence on Artificial Intelligence and Law. ACM,
New York, NY, USA, ICAIL ’09, pages 98–107.
https://doi.org/10.1145/1568234.1568246.

Joonsuk Park and Claire Cardie. 2014. Identify-
ing appropriate support for propositions in on-
line user comments. In Proceedings of the First
Workshop on Argumentation Mining. Association
for Computational Linguistics, Baltimore, Mary-
land, pages 29–38. http://www.aclweb.org/anthol-
ogy/W/W14/W14-2105.

Andreas Peldszus and Manfred Stede. 2013.
From argument diagrams to argumenta-
tion mining in texts: A survey. Int.

106



J. Cogn. Inform. Nat. Intell. 7(1):1–31.
https://doi.org/10.4018/jcini.2013010101.

Georgios Petasis and Vangelis Karkaletsis. 2016. Iden-
tifying argument components through textrank. In
Proceedings of the 3rd Workshop on Argument
Mining (ArgMining2016). Association for Com-
putational Linguistics, Berlin, Germany, pages
56–66. http://aclweb.org/anthology/W/W16/W16-
2811.pdf.

Ruty Rinott, Lena Dankin, Carlos Alzate Perez,
Mitesh M. Khapra, Ehud Aharoni, and Noam
Slonim. 2015. Show me your evidence - an auto-
matic method for context dependent evidence detec-
tion. In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing.
Association for Computational Linguistics, Lisbon,
Portugal, pages 440–450. http://aclweb.org/anthol-
ogy/D15-1050.

Niall Rooney, Hui Wang, and Fiona Browne.
2012. Applying kernel methods to argu-
mentation mining. In G. Michael Young-
blood and Philip M. McCarthy, editors, Pro-
ceedings of the Twenty-Fifth International Florida
Artificial Intelligence Research Society Confer-
ence, Marco Island, Florida. May 23-25, 2012.
AAAI Press. http://www.aaai.org/ocs/index.php/-
FLAIRS/FLAIRS12/paper/view/4366.

Abraham Savitzky and Marcel JE Golay. 1964.
Smoothing and differentiation of data by simpli-
fied least squares procedures. Analytical chemistry
36(8):1627–1639.

Christian Stab and Iryna Gurevych. 2014a. Anno-
tating argument components and relations in per-
suasive essays. In Junichi Tsujii and Jan Ha-
jic, editors, Proceedings of the 25th International
Conference on Computational Linguistics (COLING
2014). Dublin City University and Association for
Computational Linguistics, Dublin, Ireland, pages
1501–1510. http://www.aclweb.org/anthology/C14-
1142.

Christian Stab and Iryna Gurevych. 2014b. Iden-
tifying argumentative discourse structures in per-
suasive essays. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Association for Com-
putational Linguistics, Doha, Qatar, pages 46–56.
http://www.aclweb.org/anthology/D14-1006.

Christian Stab and Iryna Gurevych. 2017. Pars-
ing argumentation structures in persuasive es-
says. Computational Linguistics 0(ja):1–62.
https://doi.org/10.1162/COLI a 00295.

Yee Whye Teh, Michael I Jordan, Matthew J
Beal, and David M Blei. 2006. Hierarchi-
cal dirichlet processes. Journal of the Amer-
ican Statistical Association 101(476):1566–1581.
https://doi.org/10.1198/016214506000000302.

107


