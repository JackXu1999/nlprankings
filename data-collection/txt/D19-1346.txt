



















































Semantic Relatedness Based Re-ranker for Text Spotting


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3451–3457,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3451

Semantic Relatedness Based Re-ranker for Text Spotting

Ahmed Sabir1 Francesc Moreno-Noguer2 Lluı́s Padró1
1 Universitat Politècnica de Catalunya, TALP Research Center, Barcelona, Spain

2 Institut de Robòtica i Informàtica Industrial, CSIC-UPC, Barcelona, Spain
asabir@cs.upc.edu, fmoreno@iri.upc.edu, padro@cs.upc.edu

Abstract

Applications such as textual entailment, pla-
giarism detection or document clustering rely
on the notion of semantic similarity, and are
usually approached with dimension reduction
techniques like LDA or with embedding-based
neural approaches. We present a scenario
where semantic similarity is not enough, and
we devise a neural approach to learn seman-
tic relatedness. The scenario is text spotting in
the wild, where a text in an image (e.g. street
sign, advertisement or bus destination) must
be identified and recognized. Our goal is to
improve the performance of vision systems by
leveraging semantic information. Our ratio-
nale is that the text to be spotted is often re-
lated to the image context in which it appears
(word pairs such as Delta–airplane, or quar-
ters–parking are not similar, but are clearly re-
lated). We show how learning a word-to-word
or word-to-sentence relatedness score can im-
prove the performance of text spotting systems
up to 2.9 points, outperforming other measures
in a benchmark dataset.

1 Introduction

Deep learning has been successful in tasks related
to deciding whether two short pieces of text refer
to the same topic, e.g. semantic textual similarity
(Cer et al., 2018), textual entailment (Parikh et al.,
2016) or answer ranking for Q&A (Severyn and
Moschitti, 2015).

However, other tasks require a broader perspec-
tive to decide whether two text fragments are re-
lated more than whether they are similar. In this
work, we describe one of such tasks, and we re-
train some of the existing sentence similarity ap-
proaches to learn this semantic relatedness. We
also present a new Deep Neural Network (DNN)
that outperforms existing approaches when ap-
plied to this particular scenario.

The task we tackle is Text Spotting, which is the
problem of recognizing text that appears in unre-
stricted images (a.k.a. text in the wild) such as
traffic signs, commercial ads, or shop names. Cur-
rent state-of-the-art results on this task are far from
those of OCR systems with simple backgrounds.

Existing approaches to Text Spotting usually di-
vide the problem in two fundamental tasks: 1)
text detection, consisting of selecting the image
regions likely to contain texts, and 2) text recogni-
tion, that converts the images within these bound-
ing boxes into a readable string. In this work, we
focus on the recognition stage, aiming to prove
that semantic relatedness between the image con-
text and the recognized text can be useful to boost
the system performance. We use existing pre-
trained architectures for Text Recognition, and
add a shallow deep-network that performs a post-
processing operation to re-rank the proposed can-
didate texts. In particular, we re-rank the candi-
dates using their semantic relatedness score with
other visual information extracted from the image
(e.g. objects, scenario, image caption). Extensive
evaluation shows that our approach consistently
improves other semantic similarity methods.

2 Text Hypothesis Extraction

We use two pre-trained Text Spotting baselines to
extract k text hypotheses. The first baseline is a
CNN (Jaderberg et al., 2016) with fixed lexicon
based recognition, able to recognize words in a
predefined 90K-word dictionary. Second, we use
an LSTM architecture with visual attention model
(Ghosh et al., 2017) that generates the final output
words as probable character sequences, without
relying on any lexicon. Both models are trained
on a synthetic dataset (Jaderberg et al., 2014). The
output of both models is a Softmax score for each
of the k candidate words.



3452

holding

 

kia

 

tennis 

is

Semantic 
Relatedness 

Score
 

 
Image 

caption

Spotted 
text  

 
Object
Scene

man

FC
caption

vi
su

al

MLPs
 

   

LSTM

multi-channels
conv feature maps 

matrix 
representation of 
input sentence 

k = 3

k = 5,8

k = 3 
mask
conv 

external feature

racket
and

tennis 
ball

a

Overlap Layer

tennis

ball

1200D600D

two channels 
non-static 

overlapping 
embedding

join
layer

 

Conv
joint
layer

 

attention 

600
600300

300

a(ht)
<latexit sha1_base64="/Xb+U6QUvx58Xh/uArgvDmYDAYo=">AAAB73icbZBLSgNBEIZr4ivGV9SlIINBiC7CTFzoMuDGZQLmAckQejo9SZOenrG7RghDlrmAGxeKuPUGnsOdZ/ASdh4Ljf7Q8PH/VXRV+bHgGh3n08qsrK6tb2Q3c1vbO7t7+f2Dho4SRVmdRiJSLZ9oJrhkdeQoWCtWjIS+YE1/eD3Nm/dMaR7JWxzFzAtJX/KAU4LGapHioJvi+KybLzglZyb7L7gLKFRyk+P32tek2s1/dHoRTUImkQqiddt1YvRSopBTwca5TqJZTOiQ9FnboCQh0146m3dsnxqnZweRMk+iPXN/dqQk1HoU+qYyJDjQy9nU/C9rJxhceSmXcYJM0vlHQSJsjOzp8naPK0ZRjAwQqriZ1aYDoghFc6KcOYK7vPJfaJRL7kWpXHMLlXOYKwtHcAJFcOESKnADVagDBQEP8ATP1p31aL1Yr/PSjLXoOYRfst6+ASkgkv4=</latexit>

a(ht)
<latexit sha1_base64="/Xb+U6QUvx58Xh/uArgvDmYDAYo=">AAAB73icbZBLSgNBEIZr4ivGV9SlIINBiC7CTFzoMuDGZQLmAckQejo9SZOenrG7RghDlrmAGxeKuPUGnsOdZ/ASdh4Ljf7Q8PH/VXRV+bHgGh3n08qsrK6tb2Q3c1vbO7t7+f2Dho4SRVmdRiJSLZ9oJrhkdeQoWCtWjIS+YE1/eD3Nm/dMaR7JWxzFzAtJX/KAU4LGapHioJvi+KybLzglZyb7L7gLKFRyk+P32tek2s1/dHoRTUImkQqiddt1YvRSopBTwca5TqJZTOiQ9FnboCQh0146m3dsnxqnZweRMk+iPXN/dqQk1HoU+qYyJDjQy9nU/C9rJxhceSmXcYJM0vlHQSJsjOzp8naPK0ZRjAwQqriZ1aYDoghFc6KcOYK7vPJfaJRL7kWpXHMLlXOYKwtHcAJFcOESKnADVagDBQEP8ATP1p31aL1Yr/PSjLXoOYRfst6+ASkgkv4=</latexit>

h1
<latexit sha1_base64="3JzRbk9JprHLVz3XipRtoQtATcw=">AAAB7HicbZDLSgMxFIbP1Futt6pLQYJFEBdlpi50WXDjsoLTFtqhZNJMG5pJhiQjlKHPoAsXirh113fwGQQXvo3pZaGtPwQ+/v8ccs4JE860cd1vJ7eyura+kd8sbG3v7O4V9w/qWqaKUJ9ILlUzxJpyJqhvmOG0mSiK45DTRji4nuSNe6o0k+LODBMaxLgnWMQINtby+53MG3WKJbfsToWWwZtDqQrH46/Hj3GtU/xsdyVJYyoM4VjrlucmJsiwMoxwOiq0U00TTAa4R1sWBY6pDrLpsCN0ap0uiqSyTxg0dX93ZDjWehiHtjLGpq8Xs4n5X9ZKTXQVZEwkqaGCzD6KUo6MRJPNUZcpSgwfWsBEMTsrIn2sMDH2PgV7BG9x5WWoV8reRbly65Wq5zBTHo7gBM7Ag0uowg3UwAcCDB7gGV4c4Tw5r87brDTnzHsO4Y+c9x/gAZJi</latexit>

h1
<latexit sha1_base64="3JzRbk9JprHLVz3XipRtoQtATcw=">AAAB7HicbZDLSgMxFIbP1Futt6pLQYJFEBdlpi50WXDjsoLTFtqhZNJMG5pJhiQjlKHPoAsXirh113fwGQQXvo3pZaGtPwQ+/v8ccs4JE860cd1vJ7eyura+kd8sbG3v7O4V9w/qWqaKUJ9ILlUzxJpyJqhvmOG0mSiK45DTRji4nuSNe6o0k+LODBMaxLgnWMQINtby+53MG3WKJbfsToWWwZtDqQrH46/Hj3GtU/xsdyVJYyoM4VjrlucmJsiwMoxwOiq0U00TTAa4R1sWBY6pDrLpsCN0ap0uiqSyTxg0dX93ZDjWehiHtjLGpq8Xs4n5X9ZKTXQVZEwkqaGCzD6KUo6MRJPNUZcpSgwfWsBEMTsrIn2sMDH2PgV7BG9x5WWoV8reRbly65Wq5zBTHo7gBM7Ag0uowg3UwAcCDB7gGV4c4Tw5r87brDTnzHsO4Y+c9x/gAZJi</latexit>

h2
<latexit sha1_base64="h+/MkeOIprWOtLVXTgroEdASJHU=">AAAB7HicbZDLSgMxFIbP1Futt6pLQYJFEBdlpi50WXDjsoLTFtqhZNJMG5pkhiQjlKHPoAsXirh113fwGQQXvo3pZaGtPwQ+/v8ccs4JE860cd1vJ7eyura+kd8sbG3v7O4V9w/qOk4VoT6JeayaIdaUM0l9wwynzURRLEJOG+HgepI37qnSLJZ3ZpjQQOCeZBEj2FjL73eyyqhTLLlldyq0DN4cSlU4Hn89foxrneJnuxuTVFBpCMdatzw3MUGGlWGE01GhnWqaYDLAPdqyKLGgOsimw47QqXW6KIqVfdKgqfu7I8NC66EIbaXApq8Xs4n5X9ZKTXQVZEwmqaGSzD6KUo5MjCaboy5TlBg+tICJYnZWRPpYYWLsfQr2CN7iystQr5S9i3Ll1itVz2GmPBzBCZyBB5dQhRuogQ8EGDzAM7w40nlyXp23WWnOmfccwh857z/hhpJj</latexit>

h2
<latexit sha1_base64="h+/MkeOIprWOtLVXTgroEdASJHU=">AAAB7HicbZDLSgMxFIbP1Futt6pLQYJFEBdlpi50WXDjsoLTFtqhZNJMG5pkhiQjlKHPoAsXirh113fwGQQXvo3pZaGtPwQ+/v8ccs4JE860cd1vJ7eyura+kd8sbG3v7O4V9w/qOk4VoT6JeayaIdaUM0l9wwynzURRLEJOG+HgepI37qnSLJZ3ZpjQQOCeZBEj2FjL73eyyqhTLLlldyq0DN4cSlU4Hn89foxrneJnuxuTVFBpCMdatzw3MUGGlWGE01GhnWqaYDLAPdqyKLGgOsimw47QqXW6KIqVfdKgqfu7I8NC66EIbaXApq8Xs4n5X9ZKTXQVZEwmqaGSzD6KUo5MjCaboy5TlBg+tICJYnZWRPpYYWLsfQr2CN7iystQr5S9i3Ll1itVz2GmPBzBCZyBB5dQhRuogQ8EGDzAM7w40nlyXp23WWnOmfccwh857z/hhpJj</latexit>

h3
<latexit sha1_base64="J/l5vE2BaYCaI5X/v4/u7mbz3mc=">AAAB7HicbZDLSgMxFIbPeK31VnUpSLAI4qLMtAtdFty4rOC0hXYomTTThiaZIckIZegz6MKFIm7d9R18BsGFb2N6WWjrD4GP/z+HnHPChDNtXPfbWVldW9/YzG3lt3d29/YLB4d1HaeKUJ/EPFbNEGvKmaS+YYbTZqIoFiGnjXBwPckb91RpFss7M0xoIHBPsogRbKzl9ztZZdQpFN2SOxVaBm8OxSqcjL8eP8a1TuGz3Y1JKqg0hGOtW56bmCDDyjDC6SjfTjVNMBngHm1ZlFhQHWTTYUfozDpdFMXKPmnQ1P3dkWGh9VCEtlJg09eL2cT8L2ulJroKMiaT1FBJZh9FKUcmRpPNUZcpSgwfWsBEMTsrIn2sMDH2Pnl7BG9x5WWol0tepVS+9YrVC5gpB8dwCufgwSVU4QZq4AMBBg/wDC+OdJ6cV+dtVrrizHuO4I+c9x/jC5Jk</latexit>

h3
<latexit sha1_base64="J/l5vE2BaYCaI5X/v4/u7mbz3mc=">AAAB7HicbZDLSgMxFIbPeK31VnUpSLAI4qLMtAtdFty4rOC0hXYomTTThiaZIckIZegz6MKFIm7d9R18BsGFb2N6WWjrD4GP/z+HnHPChDNtXPfbWVldW9/YzG3lt3d29/YLB4d1HaeKUJ/EPFbNEGvKmaS+YYbTZqIoFiGnjXBwPckb91RpFss7M0xoIHBPsogRbKzl9ztZZdQpFN2SOxVaBm8OxSqcjL8eP8a1TuGz3Y1JKqg0hGOtW56bmCDDyjDC6SjfTjVNMBngHm1ZlFhQHWTTYUfozDpdFMXKPmnQ1P3dkWGh9VCEtlJg09eL2cT8L2ulJroKMiaT1FBJZh9FKUcmRpPNUZcpSgwfWsBEMTsrIn2sMDH2Pnl7BG9x5WWol0tepVS+9YrVC5gpB8dwCufgwSVU4QZq4AMBBg/wDC+OdJ6cV+dtVrrizHuO4I+c9x/jC5Jk</latexit>

ht
<latexit sha1_base64="QZ27Emb5pOmjZP4mXPXn3GjYUp4=">AAAB7HicbZDLSsNAFIZP6q3WW9WlIMEiiIuS1IUuC25cVjBtoQ1lMp20QyeTMHMilNBn0IULRdy66zv4DIIL38bpZaGtPwx8/P85zDknSATX6DjfVm5ldW19I79Z2Nre2d0r7h/UdZwqyjwai1g1A6KZ4JJ5yFGwZqIYiQLBGsHgepI37pnSPJZ3OEyYH5Ge5CGnBI3l9TsZjjrFklN2prKXwZ1DqQrH46/Hj3GtU/xsd2OaRkwiFUTrlusk6GdEIaeCjQrtVLOE0AHpsZZBSSKm/Ww67Mg+NU7XDmNlnkR76v7uyEik9TAKTGVEsK8Xs4n5X9ZKMbzyMy6TFJmks4/CVNgY25PN7S5XjKIYGiBUcTOrTftEEYrmPgVzBHdx5WWoV8ruRbly65aq5zBTHo7gBM7AhUuowg3UwAMKHB7gGV4saT1Zr9bbrDRnzXsO4Y+s9x9F35Kl</latexit>

ht
<latexit sha1_base64="QZ27Emb5pOmjZP4mXPXn3GjYUp4=">AAAB7HicbZDLSsNAFIZP6q3WW9WlIMEiiIuS1IUuC25cVjBtoQ1lMp20QyeTMHMilNBn0IULRdy66zv4DIIL38bpZaGtPwx8/P85zDknSATX6DjfVm5ldW19I79Z2Nre2d0r7h/UdZwqyjwai1g1A6KZ4JJ5yFGwZqIYiQLBGsHgepI37pnSPJZ3OEyYH5Ge5CGnBI3l9TsZjjrFklN2prKXwZ1DqQrH46/Hj3GtU/xsd2OaRkwiFUTrlusk6GdEIaeCjQrtVLOE0AHpsZZBSSKm/Ww67Mg+NU7XDmNlnkR76v7uyEik9TAKTGVEsK8Xs4n5X9ZKMbzyMy6TFJmks4/CVNgY25PN7S5XjKIYGiBUcTOrTftEEYrmPgVzBHdx5WWoV8ruRbly65aq5zBTHo7gBM7AhUuowg3UwAMKHB7gGV4saT1Zr9bbrDRnzXsO4Y+s9x9F35Kl</latexit>

↵1
<latexit sha1_base64="AsrT8WYxE7IclCESbVr909H/MYY=">AAAB8XicbZC7SgNBFIbPeo3xFrUUZDAIYhF2Y6FlwMYygrlgsoSzk0kyZHZ2mZkVwpJ3sLCxUMRW8g4+g2Dh2zi5FJr4w8DH/5/DnHOCWHBtXPfbWVpeWV1bz2xkN7e2d3Zze/tVHSWKsgqNRKTqAWomuGQVw41g9VgxDAPBakH/apzX7pnSPJK3ZhAzP8Su5B1O0Vjrroki7mEr9YatXN4tuBORRfBmkC/B0ejr4WNUbuU+m+2IJiGThgrUuuG5sfFTVIZTwYbZZqJZjLSPXdawKDFk2k8nEw/JiXXapBMp+6QhE/d3R4qh1oMwsJUhmp6ez8bmf1kjMZ1LP+UyTgyTdPpRJxHERGS8PmlzxagRAwtIFbezEtpDhdTYI2XtEbz5lRehWix454XijZcvncFUGTiEYzgFDy6gBNdQhgpQkPAIz/DiaOfJeXXepqVLzqznAP7Ief8BpZmUjg==</latexit>

↵1
<latexit sha1_base64="AsrT8WYxE7IclCESbVr909H/MYY=">AAAB8XicbZC7SgNBFIbPeo3xFrUUZDAIYhF2Y6FlwMYygrlgsoSzk0kyZHZ2mZkVwpJ3sLCxUMRW8g4+g2Dh2zi5FJr4w8DH/5/DnHOCWHBtXPfbWVpeWV1bz2xkN7e2d3Zze/tVHSWKsgqNRKTqAWomuGQVw41g9VgxDAPBakH/apzX7pnSPJK3ZhAzP8Su5B1O0Vjrroki7mEr9YatXN4tuBORRfBmkC/B0ejr4WNUbuU+m+2IJiGThgrUuuG5sfFTVIZTwYbZZqJZjLSPXdawKDFk2k8nEw/JiXXapBMp+6QhE/d3R4qh1oMwsJUhmp6ez8bmf1kjMZ1LP+UyTgyTdPpRJxHERGS8PmlzxagRAwtIFbezEtpDhdTYI2XtEbz5lRehWix454XijZcvncFUGTiEYzgFDy6gBNdQhgpQkPAIz/DiaOfJeXXepqVLzqznAP7Ief8BpZmUjg==</latexit>

↵2
<latexit sha1_base64="G5+BZnMnm3gevghksh+d0Yrtdlg=">AAAB8XicbZC7SgNBFIbPeo3xFrUUZDAIYhF2Y6FlwMYygrlgsoSzk0kyZHZ2mZkVwpJ3sLCxUMRW8g4+g2Dh2zi5FJr4w8DH/5/DnHOCWHBtXPfbWVpeWV1bz2xkN7e2d3Zze/tVHSWKsgqNRKTqAWomuGQVw41g9VgxDAPBakH/apzX7pnSPJK3ZhAzP8Su5B1O0Vjrroki7mErLQ5bubxbcCcii+DNIF+Co9HXw8eo3Mp9NtsRTUImDRWodcNzY+OnqAyngg2zzUSzGGkfu6xhUWLItJ9OJh6SE+u0SSdS9klDJu7vjhRDrQdhYCtDND09n43N/7JGYjqXfsplnBgm6fSjTiKIich4fdLmilEjBhaQKm5nJbSHCqmxR8raI3jzKy9CtVjwzgvFGy9fOoOpMnAIx3AKHlxACa6hDBWgIOERnuHF0c6T8+q8TUuXnFnPAfyR8/4Dpx6Ujw==</latexit>

↵2
<latexit sha1_base64="G5+BZnMnm3gevghksh+d0Yrtdlg=">AAAB8XicbZC7SgNBFIbPeo3xFrUUZDAIYhF2Y6FlwMYygrlgsoSzk0kyZHZ2mZkVwpJ3sLCxUMRW8g4+g2Dh2zi5FJr4w8DH/5/DnHOCWHBtXPfbWVpeWV1bz2xkN7e2d3Zze/tVHSWKsgqNRKTqAWomuGQVw41g9VgxDAPBakH/apzX7pnSPJK3ZhAzP8Su5B1O0Vjrroki7mErLQ5bubxbcCcii+DNIF+Co9HXw8eo3Mp9NtsRTUImDRWodcNzY+OnqAyngg2zzUSzGGkfu6xhUWLItJ9OJh6SE+u0SSdS9klDJu7vjhRDrQdhYCtDND09n43N/7JGYjqXfsplnBgm6fSjTiKIich4fdLmilEjBhaQKm5nJbSHCqmxR8raI3jzKy9CtVjwzgvFGy9fOoOpMnAIx3AKHlxACa6hDBWgIOERnuHF0c6T8+q8TUuXnFnPAfyR8/4Dpx6Ujw==</latexit>

↵3
<latexit sha1_base64="A3B9TxYOS5BK+G5BLuvCkTBHze8=">AAAB8XicbZC7SgNBFIbPxluMt6ilIItBEIuwmxRaBmwsI5gLJks4O5lNhszOLjOzQljyDhY2ForYSt7BZxAsfBsnl0ITfxj4+P9zmHOOH3OmtON8W5mV1bX1jexmbmt7Z3cvv39QV1EiCa2RiEey6aOinAla00xz2owlxdDntOEPriZ5455KxSJxq4cx9ULsCRYwgtpYd23kcR87aXnUyRecojOVvQzuHAoVOB5/PXyMq538Z7sbkSSkQhOOSrVcJ9ZeilIzwuko104UjZEMsEdbBgWGVHnpdOKRfWqcrh1E0jyh7an7uyPFUKlh6JvKEHVfLWYT87+slejg0kuZiBNNBZl9FCTc1pE9Wd/uMkmJ5kMDSCQzs9qkjxKJNkfKmSO4iysvQ71UdMvF0o1bqJzDTFk4ghM4AxcuoALXUIUaEBDwCM/wYinryXq13malGWvecwh/ZL3/AKijlJA=</latexit>

↵3
<latexit sha1_base64="A3B9TxYOS5BK+G5BLuvCkTBHze8=">AAAB8XicbZC7SgNBFIbPxluMt6ilIItBEIuwmxRaBmwsI5gLJks4O5lNhszOLjOzQljyDhY2ForYSt7BZxAsfBsnl0ITfxj4+P9zmHOOH3OmtON8W5mV1bX1jexmbmt7Z3cvv39QV1EiCa2RiEey6aOinAla00xz2owlxdDntOEPriZ5455KxSJxq4cx9ULsCRYwgtpYd23kcR87aXnUyRecojOVvQzuHAoVOB5/PXyMq538Z7sbkSSkQhOOSrVcJ9ZeilIzwuko104UjZEMsEdbBgWGVHnpdOKRfWqcrh1E0jyh7an7uyPFUKlh6JvKEHVfLWYT87+slejg0kuZiBNNBZl9FCTc1pE9Wd/uMkmJ5kMDSCQzs9qkjxKJNkfKmSO4iysvQ71UdMvF0o1bqJzDTFk4ghM4AxcuoALXUIUaEBDwCM/wYinryXq13malGWvecwh/ZL3/AKijlJA=</latexit>

↵T
<latexit sha1_base64="PGvOrVvsXzTj93TD1RVDOpxIchk=">AAAB8XicbZC7SgNBFIbPxluMt6ilIItBEIuwGwstAzaWEXLDZAlnJ7PJkNnZZWZWCEvewcLGQhFbyTv4DIKFb+PkUmjiDwMf/38Oc87xY86UdpxvK7Oyura+kd3MbW3v7O7l9w/qKkokoTUS8Ug2fVSUM0FrmmlOm7GkGPqcNvzB9SRv3FOpWCSqehhTL8SeYAEjqI1110Ye97GTVkedfMEpOlPZy+DOoVCG4/HXw8e40sl/trsRSUIqNOGoVMt1Yu2lKDUjnI5y7UTRGMkAe7RlUGBIlZdOJx7Zp8bp2kEkzRPanrq/O1IMlRqGvqkMUffVYjYx/8taiQ6uvJSJONFUkNlHQcJtHdmT9e0uk5RoPjSARDIzq036KJFoc6ScOYK7uPIy1EtF96JYunUL5XOYKQtHcAJn4MIllOEGKlADAgIe4RleLGU9Wa/W26w0Y817DuGPrPcf2siUsQ==</latexit>

↵T
<latexit sha1_base64="PGvOrVvsXzTj93TD1RVDOpxIchk=">AAAB8XicbZC7SgNBFIbPxluMt6ilIItBEIuwGwstAzaWEXLDZAlnJ7PJkNnZZWZWCEvewcLGQhFbyTv4DIKFb+PkUmjiDwMf/38Oc87xY86UdpxvK7Oyura+kd3MbW3v7O7l9w/qKkokoTUS8Ug2fVSUM0FrmmlOm7GkGPqcNvzB9SRv3FOpWCSqehhTL8SeYAEjqI1110Ye97GTVkedfMEpOlPZy+DOoVCG4/HXw8e40sl/trsRSUIqNOGoVMt1Yu2lKDUjnI5y7UTRGMkAe7RlUGBIlZdOJx7Zp8bp2kEkzRPanrq/O1IMlRqGvqkMUffVYjYx/8taiQ6uvJSJONFUkNlHQcJtHdmT9e0uk5RoPjSARDIzq036KJFoc6ScOYK7uPIy1EtF96JYunUL5XOYKQtHcAJn4MIllOEGKlADAgIe4RleLGU9Wa/W26w0Y817DuGPrPcf2siUsQ==</latexit>

c
<latexit sha1_base64="4o040zfilE0+eC2pG2g9raECYmE=">AAAB6HicbZC7SgNBFIbPxluMt6ilIItBsJCwGwvtDNhYJmAukCxhdnI2GTM7u8zMCmFJaWVjoYitb2Dnc9j5DPoQTi6FRn8Y+Pj/c5hzjh9zprTjfFiZhcWl5ZXsam5tfWNzK7+9U1dRIinWaMQj2fSJQs4E1jTTHJuxRBL6HBv+4GKcN25QKhaJKz2M0QtJT7CAUaKNVaWdfMEpOhPZf8GdQeH89fN2/636Venk39vdiCYhCk05UarlOrH2UiI1oxxHuXaiMCZ0QHrYMihIiMpLJ4OO7EPjdO0gkuYJbU/cnx0pCZUahr6pDInuq/lsbP6XtRIdnHkpE3GiUdDpR0HCbR3Z463tLpNINR8aIFQyM6tN+0QSqs1tcuYI7vzKf6FeKronxVLVLZSPYaos7MEBHIELp1CGS6hADSgg3MEDPFrX1r31ZD1PSzPWrGcXfsl6+Qa8U5FF</latexit>

c
<latexit sha1_base64="4o040zfilE0+eC2pG2g9raECYmE=">AAAB6HicbZC7SgNBFIbPxluMt6ilIItBsJCwGwvtDNhYJmAukCxhdnI2GTM7u8zMCmFJaWVjoYitb2Dnc9j5DPoQTi6FRn8Y+Pj/c5hzjh9zprTjfFiZhcWl5ZXsam5tfWNzK7+9U1dRIinWaMQj2fSJQs4E1jTTHJuxRBL6HBv+4GKcN25QKhaJKz2M0QtJT7CAUaKNVaWdfMEpOhPZf8GdQeH89fN2/636Venk39vdiCYhCk05UarlOrH2UiI1oxxHuXaiMCZ0QHrYMihIiMpLJ4OO7EPjdO0gkuYJbU/cnx0pCZUahr6pDInuq/lsbP6XtRIdnHkpE3GiUdDpR0HCbR3Z463tLpNINR8aIFQyM6tN+0QSqs1tcuYI7vzKf6FeKronxVLVLZSPYaos7MEBHIELp1CGS6hADSgg3MEDPFrX1r31ZD1PSzPWrGcXfsl6+Qa8U5FF</latexit>

Figure 1: Overview of the system pipeline, an end-to-end post-process scores the semantic relatedness between a
candidate word and the context in the image (objects, scenarios, natural language descriptions, ...)

3 Learning Semantic Relatedness for
Text Spotting

To learn the semantic relatedness between the vi-
sual context information and the candidate word
we introduce a multi-channel convolutional LSTM
with an attention mechanism. The network is
fed with the candidate word plus several words
describing the image visual context (object and
places labels, and descriptive captions)1, and is
trained to produce a relatedness score between the
candidate word and the context.

Our architecture is inspired by (Severyn and
Moschitti, 2015), that proposed CNN-based re-
rankers for Q&A. Our network consists of two
subnetworks, each with 4-channels with kernel
sizes k = (3, 3, 5, 8), and overlap layer, as shown
in Figure 1. We next describe the main compo-
nents:
Multi-Channel Convolution: The first subnet-
work consists of only convolution kernels, and
aims to extract n-gram or keyword features from
the caption sequence.

The convolution is applied over a sequence to
extract n-gram features from different positions.
Let x ∈ Rs×d be the sentence matrix, where s
is the sentence length, and d the dimension of
the i-th word in the sentence. Let also denote by
c ∈ Rk×d the kernel for the convolution opera-
tion. For each i-th position in the sentence, wi
is the concatenation of k consecutive words, i.e.,

1All this visual context information is automatically gen-
erated using off-the-shelf existing modules (see section 4).

wi = [xi ⊕ xi+1 ⊕ . . . ⊕ xi+k−1]. Our architec-
ture uses multiple such kernels to generate feature
maps m. The feature map for each window vector
wi can be written as:

mi = f(wi ◦ c+ b) (1)

where ◦ is element-wise multiplication, f is non-
linear function, in our case we apply Relu func-
tion (Nair and Hinton, 2010), and b is a bias. For
j kernels, the generated j feature maps can be ar-
ranged as feature representation for each window
Wi as: W = [m1⊕m2⊕ . . .⊕mj ]. Each row Wi
of W ∈ R(s−k+1)×j is the new generated feature
from the j-th kernel for the window vector at po-
sition i. The new generated feature (window rep-
resentations) are then fed into the joint layer and
LSTM as shown in Figure 1.
Multi-Channel Convolution-LSTM: Following
C-LSTM (Zhou et al., 2015) we forward the out-
put of the CNN layers into an LSTM, which cap-
tures the long term dependencies over the features.
We further introduce an attention mechanism to
capture the most important features from that se-
quence. The advantage of this attention is that the
model learns the sequence without relying on the
temporal order. We describe in more detail the at-
tention mechanism below.

Also, following (Zhou et al., 2015), we do not
use a pooling operation after the convolution fea-
ture map. Pooling layer is usually applied after
the convolution layer to extract the most impor-
tant features in the sequence. However, the out-
put of our Convolutional-LSTM model is fed into



3453

an LSTM (Hochreiter and Schmidhuber, 1997) to
learn the extracted sequence, and pooling layer
would break that sequence via downsampling to
a selected feature. In short, LSTM is specialized
in learning sequence data, and pooling operation
would break such a sequence order. On the other
hand, for the Multi-Channel Convolution model
we also lean the extracted word sequence n-gram
directly and without feature selection, pooling op-
eration.
Attention Mechanism: Attention-based models
have shown promising results on various NLP
tasks (Bahdanau et al., 2014). Such mechanism
learns to focus on a specific part of the input (e.g.
a relevant word in a sentence). We apply an at-
tention mechanism (Raffel and Ellis, 2015) via an
LSTM that captures the temporal dependencies in
the sequence. This attention uses a Feed Forward
Neural Network (FFN) attention function:

et = tanh (htWa) v
T
a (2)

whereWa is the attention of the hidden weight ma-
trix and va is the output vector. As shown in Fig.
1 the vector c is computed as a weighted average
of ht, given by α (defined below). The attention
mechanism is used to produce a single vector c for
the complete sequence as follows:

et = a (ht) , αt =
exp (et)∑T
k=1 exp (ek)

, c =
T∑
t=1

αtht

where T is the total number of steps and αt is
the computed weight of each time step t for each
state ht, a is a learnable function that depends only
on ht. Since this attention computes the average
over time, it discards the temporal order, which
is ideal for learning semantic relations between
words. By doing this, the attention gives higher
weights to more important words in the sentence
without relying on sequence order.
Overlap Layer: The overlap layer is just a fre-
quency count dictionary to compute overlap infor-
mation of the inputs. The idea is to give more
weight to the most frequent visual element, spe-
cially when it is observed by more than one visual
classifier. The dictionary output is a fully con-
nected layer.

Finally, we merge all subnetworks into a joint
layer that is fed to a loss function which calcu-
lates the semantic relatedness between both in-
puts. We call the combined model Fusion Dual
Convolution-LSTM-Attention (FDCLSTMAT ).

Since we have only one candidate word at a
time, we apply a convolution with masking in the
candidate word side (first channel). In this case,
simply zero-padding the sequence has a negative
impact on the learning stability of the network.
We concatenate the CNN outputs with the addi-
tional feature into MLP layers, and finally a sig-
moid layer performing binary classification. We
trained the model with a binary cross-entropy loss
(l) where the target value (in [0, 1]) is the se-
mantic relatedness between the word and the vi-
sual. Instead of restricting ourselves to a simple
similarity function, we let the network learn the
margin between the two classes –i.e. the degree
of similarity. For this, we increase the depth of
network after the MLPs merge layer with more
fully connected layers. The network is trained us-
ing Nesterov-accelerated Adam (Nadam) (Dozat,
2016) as it yields better results (specially in cases
such as word vectors/neural language modelling)
than other optimizers using only classical momen-
tum (ADAM). We apply batch normalization (BN)
(Ioffe and Szegedy, 2015) after each convolution,
and between each MLPs layer. We omitted the BN
after the convolution for the model without atten-
tion (FDCLSTM), as BN deteriorated the perfor-
mance. Additionally, we consider 70% dropout
(Srivastava et al., 2014) between each MLPs for
regularization purposes.

4 Dataset and Visual Context Extraction

We evaluate the performance of the proposed ap-
proach on the COCO-text (Veit et al., 2016). This
dataset is based on Microsoft COCO (Lin et al.,
2014) (Common Objects in Context), which con-
sists of 63,686 images, and 173,589 text instances
(annotations of the images). This dataset does
not include any visual context information, thus
we used out-of-the-box object (He et al., 2016)
and place (Zhou et al., 2014) classifiers and tuned
a caption generator (Vinyals et al., 2015) on the
same dataset to extract contextual information
from each image, as seen in Figure 2.

5 Related Work and Contribution

Understanding the visual environment around the
text is very important for scene understanding.
This has been recently explored by a relatively re-
duced number of works. Zhu et al. (2016) shows
that the visual context could be beneficial for text
detection. This work uses a 14 classes pixel clas-



3454

Table 1: Best results after re-ranking using different re-ranker, and different values for k-best hypotheses extracted
from the baseline output (%). In addition, to evaluate our re-ranker with MRR we fixed k CNNk=8 LSTMk=4

Model CNN LSTM
full dict list k MRR full list k MRR

Baseline (BL) full: 19.7 dict: 56.0 full: 17.9
BL+ Glove (Pennington et al., 2014) 22.0 62.5 75.8 7 44.5 19.1 75.3 4 78.8
BL+C-LSTM (Zhou et al., 2015) 21.4 61.0 71.3 8 45.6 18.9 74.7 4 80.7
BL+CNN-RNN (Wang et al., 2016) 21.7 61.8 73.3 8 44.5 19.5 77.1 4 80.9
BL+MVCNN (Yin and Schütze, 2016) 21.3 60.6 71.9 8 44.2 19.2 75.8 4 78.8
BL+Attentive LSTM (Tan et al., 2016) 21.9 62.4 74.0 8 45.7 19.1 71.4 5 80.2
BL+fasttext (Joulin et al., 2017) 21.9 62.2 75.4 7 44.6 19.4 76.1 4 80.3
BL+InferSent (Conneau et al., 2017) 22.0 62.5 75.8 7 44.5 19.4 76.7 4 79.7
BL+USE-T (Cer et al., 2018) 22.0 62.5 78.3 6 44.7 19.2 75.8 4 79.5
BL+TWE (Sabir et al., 2018) 22.2 63.0 76.3 7 44.7 19.5 76.7 4 80.2
BL+FDCLSTM (ours) 22.3 63.3 75.1 8 45.0 20.2 67.9 9 79.8
BL+FDCLSTMAT (ours) 22.4 63.7 75.5 8 45.9 20.1 67.6 9 81.8
BL+FDCLSTMlexicon (ours) 22.6 64.3 76.3 8 45.1 19.4 76.4 4 78.8
BL+FDCLSTMAT+lexicon (ours) 22.6 64.3 76.3 8 45.1 19.7 77.8 4 80.4

sifier to extract context features from the image,
such as tree, river, wall, to then assist scene text
detection. Kang et al. (2017) employs topic mod-
eling to learn the correlation between visual con-
text and the spotted text in social media. The meta-
data associated with each image (e.g tags, com-
ments and titles) is then used as context to enhance
recognition accuracy. Karaoglu et al. (2017) takes
advantage of text and visual context for logo re-
trieval problem. Most recently, Prasad and Wai
Kin Kong (2018) use object information (limited
to 42 predefined object classes) surrounding the
spotted text to guide text detection. They pro-
pose two sub-networks to learn the relation be-
tween text and object class (e.g. relations such as
car–plate or sign board–digit).

Unlike these methods, our approach uses direct
visual context from the image where the text ap-
pears, and does not rely on any extra resource such
as human labeled meta-data (Kang et al., 2017)
nor limits the context object classes (Prasad and
Wai Kin Kong, 2018). In addition, our approach is
easy to train and can be used as a drop-in comple-
ment for any text-spotting algorithm that outputs a
ranking of word hypotheses.

6 Experiments and Results

In the following we use different similarity or re-
latedness scorers to reorder the k-best hypothesis
produced by an off-the-shelf state-of-the-art text
spotting system. We experimented extracting k-
best hypotheses for k = 1 . . . 10.

We use two pre-trained deep models: a CNN
(Jaderberg et al., 2016) and an LSTM (Ghosh
et al., 2017) as baselines (BL) to extract the ini-

tial list of word hypotheses.

The CNN baseline uses a closed lexicon; there-
fore, it cannot recognize any word outside its 90K-
word dictionary. Table 1 presents four different
accuracy metrics for this case: 1) full columns
correspond to the accuracy on the whole dataset.
2) dict columns correspond to the accuracy over
the cases where the target word is among the 90K-
words of the CNN dictionary (which correspond to
43.3% of the whole dataset. 3) list columns report
the accuracy over the cases where the right word
was among the k-best produced by the baseline.
4) MRR Mean Reciprocal Rank (MRR), which is
computed as follows: MRR = 1|Q|

∑|Q|
k=1

1
rankk

,
where rank k is the position of the correct answer
in the hypotheses list proposed by the baseline.

Comparing with sentence level model: We com-
pare the results of our encoder with several state-
of-the-art sentence encoders, tuned or trained on
the same dataset. We use cosine to compute the
similarity between the caption and the candidate
word. Word-to-sentence representations are com-
puted with: Universal Sentence Encoder with the
Transformer USE-T (Cer et al., 2018), and In-
fersent (Conneau et al., 2017) with glove (Pen-
nington et al., 2014). The rest of the systems in
Table 1 are trained in the same conditions that our
model with glove initialization with dual-channel
overlapping non-static pre-trained embedding on
the same dataset. Our model FDCLSTM without
attention achieves a better result in the case of the
second baseline LSTM that full of false-positives
and short words. The advantage of the attention
mechanism is the ability to integrate information
over time, and it allows the model to refer to spe-



3455

w2: plate
 w1: plaitw3: putt

w2: good
 w1: spoolw3: food

w2: way
 w1: nayw3: may way good plate

c2: downtown c1: bicycle c1: loading dock c2: movingc2: busc1: street

In [431]: #plt.figure(figsize=(1,0.0008))
data = np.array([[0.65966403,0.80212915],[0.3854018,0.25270265],[0.1903537,0.5453093]])
#labels = np.array([['way','SWE'],['DSWE','D'], ['DSWE','D']])
#fig, ax = plt.subplots()
plt.figure(figsize=(3,2))
ax = sns.heatmap( data, linewidths=0.7,fmt = '', xticklabels=['downtown', 'street'], #annot = labels

yticklabels=['way', 'nay', 'may'], cmap='coolwarm', vmin=0, vmax=0.8)

In [287]: model.similarity('way', 'street')

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

Out[287]: 0.80212915

In [317]: model.similarity('may', 'downtown')

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

10

way
nay
may

streetdowntown

Out[317]: 0.3854018

In [321]: model.similarity('nay', 'downtown')

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

Out[321]: 0.1903537

In [324]: model.similarity('nay', 'street')

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

Out[324]: 0.25270265

In [328]: model.similarity('may', 'street')

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

Out[328]: 0.5453093

In [330]: model.similarity('downtown', 'way')

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

Out[330]: 0.65966403

In [344]: ## second example

In [430]: #plt.figure(figsize=(1,0.0008))
#data = np.array([[0.3641031, 0.58019626],[0.5499961,0.14681692],[0.48082343, 0.592437]])
data = np.array([[0.3641031, 0.592437],[0.5499961,0.14681692],[0.48082343, 0.58019626]])
labels = np.array([['1','2'],['3','4'], ['5','6']])
#fig, ax = plt.subplots()
plt.figure(figsize=(3,2))
ax = sns.heatmap( data, linewidths=0.7, fmt = '', xticklabels=['bicycle', 'bus'], #annot = labels annot = labels

yticklabels=['good', 'spool', 'food'], cmap='coolwarm', vmin=0, vmax=0.8)

11

spool
good

bicycle bus
food

In [357]: model.similarity('good', 'bus')

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

Out[357]: 0.58019626

In [362]: #Thrid example

In [428]: #plt.figure(figsize=(1,0.0008))
data = np.array([[0.50947464, 0.6075961],[0.073897675,0.0875815],[0.25978878, 0.40541637]])
labels = np.array([['1','2'],['3','4'], ['5','6']])
#fig, ax = plt.subplots()
plt.figure(figsize=(3,2))
ax = sns.heatmap( data, linewidths=0.7, fmt = '', xticklabels=['loading dock', 'moving'], #annot = labels

yticklabels=['plate', 'plait', 'puff'], cmap='coolwarm', vmin=0, vmax=0.8)

In [364]: model.similarity('plate', 'moving')

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

Out[364]: 0.6075961

In [365]: model.similarity('plate', 'dock')

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

Out[365]: 0.50947464

In [367]: model.similarity('puff', 'moving')

13

loading dock moving

plate
plait
putt

w2: delta
 w1: deliaw3: welts delta

c1: airliner c2: airfiled

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

Out[402]: 0.25786942

In [426]: #plt.figure(figsize=(1,0.0008))
#data = np.array([[0.25353315, 0.5794146],[0.20160808,0.4276228],[0.3262325, 0.64646214]])
data = np.array([[0.4425509, 0.579158],[0.24245203 ,0.35366368 ],[0.31322145, 0.25786942]])
labels = np.array([['1','2'],['3','4'], ['5','6']])
#fig, ax = plt.subplots()
plt.figure(figsize=(3,2))
ax = sns.heatmap( data, linewidths=0.7, fmt = '', xticklabels=['airliner', 'airfiled'], #annot = labels annot = labels,

yticklabels=['delta', 'delia', 'welts'], cmap='coolwarm', vmin=0, vmax=0.8)

In [406]: # football example

In [408]: #model.similarity('12','football')
model.similarity('k','football')

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

Out[408]: 0.39500374

In [409]: model.similarity('ae','football')

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

Out[409]: 0.120614916

In [410]: model.similarity('twelve','football')

17

delta
delia
welts

airliner airplane

w3: 12
 w1: kw2: ae 12

c1: football

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

Out[410]: 0.43070456

In [454]: #plt.figure(figsize=(1,0.0008))
#data = np.array([[0.25353315, 0.5794146],[0.20160808,0.4276228],[0.3262325, 0.64646214]])
data = np.array([[0.43070456, 0.36410215],[0.39500374 , 0.31400877],[0.120614916 , 0.07749998]])
labels = np.array([['1','2'],['3','4'], ['5','6']])
#fig, ax = plt.subplots()
plt.figure(figsize=(3,2))
ax = sns.heatmap( data, linewidths=0.7,fmt = '', xticklabels=['football', 'stadium'], #annot = labels annot = labels

yticklabels=['12', 'k', 'ae'], cmap='coolwarm', vmin=0, vmax=0.8)

In [413]: model.similarity('twelve','stadium')

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

Out[413]: 0.36410215

In [414]: model.similarity('k','stadium')

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

Out[414]: 0.31400877

In [415]: model.similarity('ae','stadium')

/home/asabir/anaconda2/envs/py36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.
if np.issubdtype(vec.dtype, np.int):

18

12
k
ae

football stadium

c2: stadium
c3: a large airplane that is sitting 
on a runway

c3: a train that is moving on the side 
of a road

c3: a man and a women are standing 
in front of a bus

c3: a street sign with a sign on the 
side of it

c3: a group of people playing a game
of frisbee

Figure 2: Examples of candidate re-ranking using object (c1), place (c2), and caption (c3) information. The three
left examples are re-ranked based on the semantic relatedness score. The delta-airliner relation which frequently
co-occurs in training data is captured by the overlap layers. The 12-football pair shows a relatedness between
sports and numbers.

cific points in the sequence when computing its
output. However, in this case, the attention at-
tends the wrong context, as there are many words
have no correlation or do not correspond to ac-
tual words. On the other hand, USE-T seems to
require a shorter hypothesis list to get top perfor-
mance when the right word is in the hypothesis
list.
Comparing with word level model: We also
compare our result with current state-of-the-art
word embeddings trained on a large general text
using glove and fasttext. The word model used
only object and place information, and ignored
the caption. Our proposed models achieve bet-
ter performance than our TWE previous model
(Sabir et al., 2018), that trained a word embedding
(Mikolov et al., 2013) from scratch on the same
task.
Similarity to probabilities: After computing the
cosine similarity we need to convert that score to
probabilities. As we proposed in previous work
(Sabir et al., 2018) we obtain the final probability
combining (Blok et al., 2003) the similarity score,
the probability of the detected context (provided
by the object/place classifier), and the probability
of the candidate word (estimated from a 5M token
corpus) (Lison and Tiedemann, 2016).
Effect of Unigram Probabilities: Ghosh et al.
(2017) showed the utility of a language model
(LM) when the data is too small for a DNN, ob-
taining significant improvements. Thus, we intro-
duce a basic model of unigram probabilities with
Out-of-vocabulary (OOV) words smoothing. The
model is applied at the end, to re-rank out false
positive short words, and has the main goal of re-
ranking out less probable word overranked by the
deep model. As seen in Table 1, the introduction
of this unigram lexicon produces the best results.

Human performance: To estimate an upper
bound for the results, we picked 33 random pic-
tures from the test dataset and had 16 human sub-
jects try to select the right word among the top k =
5 candidates produced by the baseline text spot-
ting system. Our proposed model performance on
the same images was 57%. Average human per-
formance was 63% (highest 87%, lowest 39%).

7 Conclusion

In this work, we propose a simple deep learn-
ing architecture to learn semantic relatedness be-
tween word-to-word and word-to-sentence pairs,
and show how it outperforms other semantic sim-
ilarity scorers when used to re-rank candidate an-
swers in the Text Spotting problem.

In the future, we plan using the same approach
to tackle similar problems, including lexical selec-
tion in Machine Translation, or word sense disam-
biguation (Lala and Specia, 2018). We believe our
approach could also be useful in multimodal ma-
chine translation, where an image caption must be
translated using not only the text but also the im-
age content (Barrault et al., 2018). Tasks that lie
at the intersection of computer vision and NLP,
such as the challenges posed in the new Break-
ingNews dataset (popularity prediction, automatic
text illustration) could also benefit from our re-
sults (Ramisa et al., 2018).

Acknowledgments

We would like to thank José Fonollosa, Marta Ruiz
Costa-Jussà and the anonymous reviewers for dis-
cussion and feedback. This work was supported
by the KASP Scholarship Program and by the
MINECO project HuMoUR TIN2017-90086-R.



3456

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Loı̈c Barrault, Fethi Bougares, Lucia Specia, Chi-
raag Lala, Desmond Elliott, and Stella Frank. 2018.
Findings of the third shared task on multimodal ma-
chine translation. In Proceedings of the Third Con-
ference on Machine Translation: Shared Task Pa-
pers.

Sergey Blok, Douglas Medin, and Daniel Osherson.
2003. Probability from similarity. In AAAI Spring
Symposium on Logical Formalization of Common-
sense Reasoning.

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,
Nicole Limtiaco, Rhomni St John, Noah Constant,
Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
et al. 2018. Universal sentence encoder. arXiv
preprint arXiv:1803.11175.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing.

Timothy Dozat. 2016. Incorporating nesterov momen-
tum into adam.

Suman K Ghosh, Ernest Valveny, and Andrew D Bag-
danov. 2017. Visual attention models for scene text
recognition. In 2017 14th IAPR International Con-
ference on Document Analysis and Recognition.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Sergey Ioffe and Christian Szegedy. 2015. Batch nor-
malization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint
arXiv:1502.03167.

Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and
Andrew Zisserman. 2014. Synthetic data and artifi-
cial neural networks for natural scene text recogni-
tion. arXiv preprint arXiv:1406.2227.

Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and
Andrew Zisserman. 2016. Reading text in the wild
with convolutional neural networks. International
Journal of Computer Vision, 116(1):1–20.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2017. Bag of tricks for efficient
text classification. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics.

Chulmoo Kang, Gunhee Kim, and Suk I Yoo. 2017.
Detection and recognition of text embedded in on-
line images via neural context models. In Thirty-
First AAAI Conference on Artificial Intelligence.

Sezer Karaoglu, Ran Tao, Jan C van Gemert, and Theo
Gevers. 2017. Con-text: Text detection for fine-
grained object classification. IEEE Transactions on
Image Processing, 26(8):3965–3980.

Chiraag Lala and Lucia Specia. 2018. Multimodal lex-
ical translation. In Proceedings of the Eleventh In-
ternational Conference on Language Resources and
Evaluation.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In European Confer-
ence on Computer Vision.

Pierre Lison and Jörg Tiedemann. 2016. Opensub-
titles2016: Extracting large parallel corpora from
movie and tv subtitles.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems.

Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of the 27th International Conference
on Machine Learning.

Ankur Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing.

Shitala Prasad and Adams Wai Kin Kong. 2018. Using
object information for spotting text. In The Euro-
pean Conference on Computer Vision.

Colin Raffel and Daniel PW Ellis. 2015. Feed-
forward networks with attention can solve some
long-term memory problems. arXiv preprint
arXiv:1512.08756.

Arnau Ramisa, Fei Yan, Francesc Moreno-Noguer, and
Krystian Mikolajczyk. 2018. Breakingnews: Arti-
cle annotation by image and text processing. IEEE



3457

Transactions on Pattern Analysis and Machine In-
telligence, 40(5):1072–1085.

Ahmed Sabir, Francesc Moreno-Noguer, and Lluı́s
Padró. 2018. Visual re-ranking with natural lan-
guage understanding for text spotting. In Asian Con-
ference on Computer Vision.

Aliaksei Severyn and Alessandro Moschitti. 2015.
Learning to rank short text pairs with convolutional
deep neural networks. In Proceedings of the 38th
international ACM SIGIR Conference on Research
and Development in Information Retrieval.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Ming Tan, Cicero Dos Santos, Bing Xiang, and Bowen
Zhou. 2016. Improved representation learning for
question answer matching. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics.

Andreas Veit, Tomas Matera, Lukas Neumann, Jiri
Matas, and Serge Belongie. 2016. Coco-text:
Dataset and benchmark for text detection and
recognition in natural images. arXiv preprint
arXiv:1601.07140.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition.

Xingyou Wang, Weijie Jiang, and Zhiyong Luo. 2016.
Combination of convolutional and recurrent neu-
ral network for sentiment analysis of short texts.
In Proceedings of COLING 2016, the 26th Inter-
national Conference on Computational Linguistics:
Technical Papers.

Wenpeng Yin and Hinrich Schütze. 2016. Multichan-
nel variable-size convolution for sentence classifica-
tion. arXiv preprint arXiv:1603.04513.

Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Anto-
nio Torralba, and Aude Oliva. 2014. Learning deep
features for scene recognition using places database.
In Advances in Neural Information Processing Sys-
tems.

Chunting Zhou, Chonglin Sun, Zhiyuan Liu, and Fran-
cis Lau. 2015. A c-lstm neural network for text clas-
sification. arXiv preprint arXiv:1511.08630.

Anna Zhu, Renwu Gao, and Seiichi Uchida. 2016.
Could scene context be beneficial for scene text de-
tection? Pattern Recognition, 58:204–215.


