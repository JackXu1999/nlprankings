



















































Spectral Semi-Supervised Discourse Relation Classification


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 89–93,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Spectral Semi-Supervised Discourse Relation Classification

Robert Fisher
Carnegie Mellon University

5000 Forbes Ave
Pittsburgh, PA 15213

rwfisher@cs.cmu.edu

Reid Simmons
Carnegie Mellon University

5000 Forbes Ave
Pittsburgh, PA 15213
reids@cs.cmu.edu

Abstract

Discourse parsing is the process of dis-
covering the latent relational structure of
a long form piece of text and remains a
significant open challenge. One of the
most difficult tasks in discourse parsing is
the classification of implicit discourse re-
lations. Most state-of-the-art systems do
not leverage the great volume of unlabeled
text available on the web–they rely instead
on human annotated training data. By in-
corporating a mixture of labeled and unla-
beled data, we are able to improve relation
classification accuracy, reduce the need for
annotated data, while still retaining the ca-
pacity to use labeled data to ensure that
specific desired relations are learned. We
achieve this using a latent variable model
that is trained in a reduced dimensionality
subspace using spectral methods. Our ap-
proach achieves an F1 score of 0.485 on
the implicit relation labeling task for the
Penn Discourse Treebank.

1 Introduction

Discourse parsing is a fundamental task in natural
language processing that entails the discovery of
the latent relational structure in a multi-sentence
piece of text. Unlike semantic and syntactic pars-
ing, which are used for single sentence pars-
ing, discourse parsing is used to discover inter-
sentential relations in longer pieces of text. With-
out discourse, parsing methods can only be used to
understand documents as sequences of unrelated
sentences.

Unfortunately, manual annotation of discourse
structure in text is costly and time consuming.
Multiple annotators are required for each relation
to estimate inter-annotator agreement. The Penn
Discourse Treebank (PDTB) (Prasad et al., 2008).

is one of the largest annotated discourse parsing
datasets, with 16,224 implicit relations. However,
this pales in comparison to unlabeled datasets that
can include millions of sentences of text. By aug-
menting a labeled dataset with unlabeled data, we
can use a bootstrapping framework to improve
predictive accuracy, and reduce the need for la-
beled data–which could make it much easier to
port discourse parsing algorithms to new domains.
On the other hand, a fully unsupervised parser may
not be desirable because in many applications spe-
cific discourse relations must be identified, which
would be difficult to achieve without the use of la-
beled examples.

There has recently been growing interest in a
breed of algorithms based on spectral decomposi-
tion, which are well suited to training with unla-
beled data. Spectral algorithms utilize matrix fac-
torization algorithms such as Singular Value De-
composition (SVD) and rank factorization to dis-
cover low-rank decompositions of matrices or ten-
sors of empirical moments. In many models, these
decompositions allow us to identify the subspace
spanned by a group of parameter vectors or the
actual parameter vectors themselves. For tasks
where they can be applied, spectral methods pro-
vide statistically consistent results that avoid lo-
cal maxima. Also, spectral algorithms tend to
be much faster—sometimes orders of magnitude
faster—than competing approaches, which makes
them ideal for tackling large datasets. These meth-
ods can be viewed as inferring something about
the latent structure of a domain—for example, in a
hidden Markov model, the number of latent states
and the sparsity pattern of the transition matrix are
forms of latent structure, and spectral methods can
recover both in the limit.

This paper presents a semi-supervised spectral
model for a sequential relation labeling task for
discourse parsing. Besides the theoretically desir-
able properties mentioned above, we also demon-

89



strate the practical advantages of the model with
an empirical evaluation on the Penn Discourse
Treebank (PDTB) (Prasad et al., 2008) dataset,
which yields an F1 score of 0.485. This accuracy
shows a 7-9 percentage point improvement over
approaches that do not utilize unlabeled training
data.

2 Related Work

There has been quite a bit of work concerning
fully supervised relation classification with the
PDTB (Lin et al., 2014; Feng and Hirst, 2012;
Webber et al., 2012). Semi-supervised relation
classification is much less common however. One
recent example of an attempt to leverage unla-
beled data appears in (Hernault et al., 2011),
which showed that moderate classification accu-
racy can be achieved with very small labeled
datasets. However, this approach is not compet-
itive with fully supervised classifiers when more
training data is available. Recently there has
also been some work to use Conditional Random
Fields (CRFs) to represent the global properties of
a parse sequence (Joty et al., 2013; Feng and Hirst,
2014), though this work has focused on the RST-
DT corpus, rather than the PDTB.

In addition to requiring a fully supervised train-
ing set, most existing discourse parsers use non-
spectral optimization that is often slow and inex-
act. However, there has been some work in other
parsing tasks to employ spectral methods in both
supervised and semi-supervised settings (Parikh et
al., 2014; Cohen et al., 2014). Spectral methods
have also been applied very successfully in many
non-linguistic domains (Hsu et al., 2012; Boots
and Gordon, 2010; Fisher et al., 2014).

3 Problem Definition and Dataset

This section defines the discourse parsing prob-
lem and discusses the characteristics of the PDTB.
The PDTB consists of annotated articles from the
Wall Street Journal and is used in our empiri-
cal evaluations. This is combined with the New
York Times Annotated Corpus (Sandhaus, 2008),
which includes 1.8 million New York Times arti-
cles printed between 1987 and 2007.

Discourse parsing can be reduced to three sepa-
rate tasks. First, the text must be decomposed into
elementary discourse units (EDUs), which may or
may not coincide with sentence boundaries. The
EDUs are often independent clauses that may be

connected with conjunctions. After the text has
been partitioned into EDUs, the discourse struc-
ture must be identified. This requires us to iden-
tify all pairs of EDUs that will be connected with
some discourse relation. These relational links in-
duce the skeletal structure of the discourse parse
tree. Finally, each connection identified in the pre-
vious step must be labeled using a known set of
relations. Examples of these discourse relations
include concession, causal, and instantiation rela-
tions. In the PDTB, only adjacent discourse units
are connected with a discourse relation, so with
this dataset we are considering parse sequences
rather than parse trees.

In this work, we focus on the relation labeling
task, as fairly simple methods perform quite well
at the other two tasks (Webber et al., 2012). We
use the ground truth parse structures provided by
the PDTB dataset, so as to isolate the error intro-
duced by relation labeling in our results, but in
practice a greedy structure learning algorithm can
be used if the parse structures are not known a pri-
ori.

Some of the relations in the dataset are induced
by specific connective words in the text. For exam-
ple, a contrast relation may be explicitly revealed
by the conjunction but. Simple classifiers using
only the text of the discourse connective with POS
tags can find explicit relations with high accu-
racy (Lin et al., 2014). The following sentence
shows an example of a more difficult implicit re-
lation. In this sentence, two EDUs are connected
with an explanatory relation, shown in bold, al-
though the connective word does not occur in the
text.

“But a few funds have taken other defen-
sive steps. Some have raised their cash
positions to record levels. [BECAUSE]
High cash positions help buffer a fund
when the market falls.”

We focus on the more difficult implicit relations
that are not induced by coordinating connectives
in the text. The implicit relations have been shown
to require more sophisticated feature sets includ-
ing syntactic and linguistic information (Lin et al.,
2009). The PDTB dataset includes 16,053 exam-
ples of implicit relations.

A full list of the PDTB relations is available
in (Prasad et al., 2008). The relations are orga-
nized hierarchically into top level, types, and sub-
types. Our experiments focus on learning only up

90



This hasn't been Kellogg Co.'s year

The oat-bran craze has cost the world's largest 
cereal maker market share, and!

the company's president quit suddenly.

edu2

r12!

(Contingency.Cause.Reason)
h12

edu1

Figure 1: An example of the latent variable dis-
course parsing model taken from the Penn Dis-
course Treebank Dataset. The relation here is an
example of a cause attribution relation.

to level 2, as the level 3 (sub-type) relations are
too specific and show only 80% inter-annotator
agreement. There are 16 level 2 relations in the
PDTB, but the 5 least common relations only ap-
pear a handful of times in the dataset and are omit-
ted from our tests, yielding 11 possible classes.

4 Approach

We incorporate unlabeled data into our spectral
discourse parsing model using a bootstrapping
framework. The model is trained over several iter-
ations, and the most useful unlabeled sequences
are added as labeled training data after each it-
eration. Our method also utilizes Markovian la-
tent states to compactly capture global informa-
tion about a parse sequence, with one latent vari-
able for each relation in the discourse parsing se-
quence. Most discourse parsing frameworks will
label relations independently of the rest of the ac-
companying parse sequence, but this model allows
for information about the global structure of the
discourse parse to be used when labeling a rela-
tion. A graphical representation of one link in the
parsing model is shown in Figure 1.

Specifically, each potential relation rij between
elementary discourse units ei and ej is accompa-
nied by a corresponding latent variable as hij . Ac-
cording to the model assumptions, the following
equality holds:

P (rij = r|r1,2, r2,3...rn+1,n) = P (rij = r|hij)
To maintain notational consistency with other

latent variable models, we will denote these re-
lation variables as x1...xn, keeping in mind that

there is one possible relation for each adjacent pair
of elementary discourse units.

For the Penn Discourse Treebank Dataset, the
discourse parses behave like sequence of random
variables representing the relations, which allows
us to use an HMM-like latent variable model based
on the framework presented in (Hsu et al., 2012).
If the discourse parses were instead trees, such as
those seen in Rhetorical Structure Theory (RST)
datasets, we can modify the standard model to in-
clude separate parameters for left and right chil-
dren, as demonstrated in (Dhillon et al., 2012).

4.1 Spectral Learning
This section briefly describes the process of learn-
ing a spectral HMM. Much more detail about the
process is available in (Hsu et al., 2012). Learn-
ing in this model will occur in a subspace of di-
mensionality m, but system dynamics will be the
same if m is not less than the rank of the obser-
vation matrix. If our original feature space has
dimensionality n, we define a transformation ma-
trix U ∈ Rn×m, which can be computed using
Singular Value Decomposition. Given the matrix
U , coupled with the empirical unigram (P1), bi-
gram (P2,1), and trigram matrices (P3,x,1), we are
able to estimate the subspace initial state distribu-
tion (π̂U ) and observable operator (ÂU ) using the
following equalities (wherein the Moore-Penrose
pseudo-inverse of matrix X is denoted by X+):

π̂U = UTP1

ÂU = UTP3,x,1(UTP2,1)+ ∀x
For our original feature space, we use the

rich linguistic discourse parsing features defined
in (Feng and Hirst, 2014), which includes syn-
tactic and linguistic features taken from depen-
dency parsing, POS tagging, and semantic simi-
larity measures. We augment this feature space
with a vector space representation of semantics. A
term-document co-occurrence matrix is computed
using all of Wikipedia and Latent Dirichlet Anal-
ysis was performed using this matrix. The top 200
concepts from the vector space representation for
each pair of EDUs in the dataset are included in
the feature space, with a concept regularization pa-
rameter of 0.01.

4.2 Semi-Supervised Training
To begin semi-supervised training, we perform
a syntactic parse of the unlabeled data and ex-

91



tract EDU segments using the method described in
(Feng and Hirst, 2014). The model is then trained
using the labeled dataset, and the unlabeled re-
lations are predicted using the Viterbi algorithm.
The most informative sequences in the unlabeled
training set are added to the labeled training set as
labeled examples. To measure how informative a
sequence of relations is, we use density-weighted
certainty sampling (DCS). Specifically for a se-
quence of relations r1...rn taken from a document,
d, we use the following formula:

DCS(d) =
1
n

n∑
i=1

p̂(ri)
H(ri)

In this equation, H(ri) represented the entropy of
the distribution of label predictions for the rela-
tion ri generated by the current spectral model,
which is a measure of the model’s uncertainty for
the label of the given relation. Density is de-
noted p̂(ri), and this quantity measures the extent
to which the text corresponding to this relation
is representative of the labeled corpus. To com-
pute this measure, we create a Kernel Density Es-
timate (KDE) over a 100 dimensional LDA vector
space representation of all EDU’s in the labeled
corpus. We then compute the density of the KDE
for the text associated with relation ri, which gives
us p̂(ri). All sequences of relations in the unla-
beled dataset are ranked according to their aver-
age density-weighted certainty score, and all se-
quences scoring above a parameter ψ are added
to the training set. The model is then retrained,
the unlabeled data re-scored, and the process is
repeated for several iterations. In iteration i, the
labeled data in the training set is weighted wli,
and the unlabeled data is weighted wui , with the
unlabeled data receiving higher weight in subse-
quent iterations. The KDE kernel bandwidth and
the parameters ψ, wli, w

u
i , and the number of hid-

den states are chosen in experiments using 10-fold
cross validation on the labeled training set, cou-
pled with a subset of the unlabeled data.

5 Results

Figure 2 shows the F1 scores of the model using
various sizes of labeled training sets. In all cases,
the entirety of the unlabeled data is made avail-
able, and 7 rounds of bootstrapping is conducted.
Sections 2-22 of the PDTB are used for training,
with section 23 being withheld for testing, as rec-
ommended by the dataset guidelines (Prasad et al.,

0 10 20 30 40 50 60 70 80 90 100
10

15

20

25

30

35

40

45

50

Percentage of Labeled Training Data Used

F1
 P

re
di

ct
io

n 
Sc

or
e

 

 

Spectral HMM
Lin 14
Baseline

Figure 2: Empirical results for labeling of implicit
relations.

2008). The results are compared against those re-
ported in (Lin et al., 2014), as well as a simple
baseline classifier that labels all relations with the
most common class, EntRel. Compared to the
semi-supervised method described in (Hernault et
al., 2011), we show significant gains in accuracy
at various sizes of dataset, although the unlabeled
dataset used in our experiments is much larger.

When the spectral HMM is trained using only
the labeled dataset, with no unlabeled data, it pro-
duces an F1 score of 41.1%, which is comparable
to the results reported in (Lin et al., 2014). By
comparison, the semi-supervised classifier is able
to obtain similar accuracy when using approxi-
mately 50% of the labeled training data. When
given access to the full labeled dataset, we see
an improvement in the F1 score of 7-9 percent-
age points. Recent work has shown promising re-
sults using CRFs for discourse parsing (Joty et al.,
2013; Feng and Hirst, 2014), but the results re-
ported in this work were taken from the RST-DT
corpus and are not directly comparable. However,
supervised CRFs and HMMs show similar accu-
racy in other language tasks (Ponomareva et al.,
2007; Awasthi et al., 2006).

6 Conclusions

In this work, we have shown that we are able
to outperform fully-supervised relation classifiers
by augmenting the training data with unlabeled
text. The spectral optimization used in this ap-
proach makes computation tractable even when
using over one million documents. In future work,
we would like to further improve the performance
of this method when very small labeled training

92



sets are available, which would allow discourse
analysis to be applied in many new and interest-
ing domains.

Acknowledgements

We give thanks to Carolyn Penstein Rosé and
Geoff Gordon for their helpful discussions and
suggestions. We also gratefully acknowledge
the National Science Foundation for their support
through EAGER grant number IIS1450543. This
material is also based upon work supported by
the Quality of Life Technology Center and the
National Science Foundation under Cooperative
Agreement EEC-0540865.

References
Pranjal Awasthi, Delip Rao, and Balaraman Ravindran.

2006. Part of speech tagging and chunking with
hmm and crf. Proceedings of NLP Association of
India (NLPAI) Machine Learning Contest 2006.

Byron Boots and Geoffrey J Gordon. 2010. Predictive
state temporal difference learning. arXiv preprint
arXiv:1011.0041.

Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2014. Spectral learning of
latent-variable pcfgs: Algorithms and sample com-
plexity. The Journal of Machine Learning Research,
15(1):2399–2449.

Paramveer S Dhillon, Jordan Rodu, Michael Collins,
Dean P Foster, and Lyle H Ungar. 2012. Spectral
dependency parsing with latent variables. In Pro-
ceedings of the 2012 joint conference on empirical
methods in natural language processing and compu-
tational natural language learning, pages 205–213.
Association for Computational Linguistics.

Vanessa Wei Feng and Graeme Hirst. 2012. Text-
level discourse parsing with rich linguistic fea-
tures. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 60–68. Association
for Computational Linguistics.

Vanessa Wei Feng and Graeme Hirst. 2014. A linear-
time bottom-up discourse parser with constraints
and post-editing. In Proceedings of The 52nd An-
nual Meeting of the Association for Computational
Linguistics (ACL 2014), Baltimore, USA, June.

Robert Fisher, Reid Simmons, Cheng-Shiu Chung,
Rory Cooper, Garrett Grindle, Annmarie Kelleher,
Hsinyi Liu, and Yu Kuang Wu. 2014. Spectral ma-
chine learning for predicting power wheelchair exer-
cise compliance. In Foundations of Intelligent Sys-
tems, pages 174–183. Springer.

Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2011. Semi-supervised discourse relation
classification with structural learning. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 340–352. Springer.

Daniel Hsu, Sham M Kakade, and Tong Zhang. 2012.
A spectral algorithm for learning hidden markov
models. Journal of Computer and System Sciences,
78(5):1460–1480.

Shafiq R Joty, Giuseppe Carenini, Raymond T Ng, and
Yashar Mehdad. 2013. Combining intra-and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In ACL (1), pages 486–496.

Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1, pages 343–351.
Association for Computational Linguistics.

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
A pdtb-styled end-to-end discourse parser. Natural
Language Engineering, pages 1–34.

Ankur Parikh, Shay B Cohen, and Eric Xing. 2014.
Spectral unsupervised parsing with additive tree
metrics. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers. Association for Computational
Linguistics.

Natalia Ponomareva, Paolo Rosso, Ferrán Pla, and An-
tonio Molina. 2007. Conditional random fields vs.
hidden markov models in a biomedical named en-
tity recognition task. In Proc. of Int. Conf. Recent
Advances in Natural Language Processing, RANLP,
pages 479–483.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The penn discourse treebank
2.0. In LREC. Citeseer.

Evan Sandhaus. 2008. The new york times annotated
corpus ldc2008t19. Linguistic Data Consortium.

Bonnie Webber, Markus Egg, and Valia Kordoni.
2012. Discourse structure and language technology.
Natural Language Engineering, 18(4):437–490.

93


