



















































Grammatical Error Detection Using Error- and Grammaticality-Specific Word Embeddings


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 40–48,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Grammatical Error Detection Using Error- and Grammaticality-Specific
Word Embeddings

Masahiro Kaneko, Yuya Sakaizawa and Mamoru Komachi
Tokyo Metropolitan University

{kaneko-masahiro@ed, sakaizawa-yuya@ed, komachi@}.tmu.ac.jp

Abstract

In this study, we improve grammatical
error detection by learning word embed-
dings that consider grammaticality and er-
ror patterns. Most existing algorithms for
learning word embeddings usually model
only the syntactic context of words so
that classifiers treat erroneous and correct
words as similar inputs. We address the
problem of contextual information by con-
sidering learner errors. Specifically, we
propose two models: one model that em-
ploys grammatical error patterns and an-
other model that considers grammaticality
of the target word. We determine gram-
maticality of n-gram sequence from the
annotated error tags and extract grammat-
ical error patterns for word embeddings
from large-scale learner corpora. Exper-
imental results show that a bidirectional
long-short term memory model initialized
by our word embeddings achieved the
state-of-the-art accuracy by a large mar-
gin in an English grammatical error detec-
tion task on the First Certificate in English
dataset.

1 Introduction

Grammatical error detection that can identify the
location of errors is useful for second language
learners and teachers. It can be seen as a se-
quence labeling task, which is typically solved
by a supervised approach. For example, Rei and
Yannakoudakis (2016) achieved the state-of-the-
art accuracy in English grammatical error detec-
tion using a bidirectional long-short term memory

Phrase pair W2V C&W EWE GWE E&GWE
in summer & on summer 0.84 0.75 0.64 0.58 0.54
in summer & in spring 0.84 0.77 0.90 0.80 0.88
in summer & in English 0.40 0.46 0.36 0.25 0.30
on summer & on spring 0.85 0.71 0.82 0.76 0.80

Table 1: Cosine similarity of phrase pairs for each
word embedding method.

(Bi-LSTM) neural network. Their approach uses
word embeddings learned from a large-scale na-
tive corpus to address the data sparseness problem
of learner corpora.

However, most of the word embeddings, in-
cluding the one used by Rei and Yannakoudakis
(2016), model only the context of the words from a
raw corpus written by native speakers, and do not
consider specific grammatical errors of language
learners. This leads to the problem wherein the
word embeddings of correct and incorrect expres-
sions tend to be similar (Table 1, columns W2V
and C&W) so that the classifier must decide gram-
maticality of a word from contextual information
with a similar input vector.

To address this problem, we introduce two
methods: 1) error-specific word embeddings
(EWE), which employ grammatical error pat-
terns, that is to say the word pairs that learn-
ers tend to easily confuse; 2) grammaticality-
specific word embeddings (GWE), which con-
sider grammatical correctness of n-grams. In
this paper, we use the term grammaticality to re-
fer to the correct or incorrect label of the tar-
get word given its surrounding context. We also
combine these methods, which we will refer to
as error-and grammaticality-specific word embed-
dings (E&GWE).

Table 1 shows the cosine similarity of phrase

40



pairs using word2vec (W2V), C&W embeddings
(Collobert and Weston, 2008), EWE, GWE, and
E&GWE1. It illustrates that EWE, GWE, and
E&GWE are able to distinguish between correct
and incorrect phrase pairs while maintaining the
contextual relation.

Furthermore, we conducted experiments using
the large-scale Lang-82 English learner corpus.
The results demonstrated that representation learn-
ing is crucial for exploiting a noisy learner corpus
for grammatical error detection.

The main contributions of this study are sum-
marized as follows:
• We achieve the state-of-the-art accuracy in

grammatical error detection on the First Cer-
tificate in English dataset (FCE-public) using
a Bi-LSTM model initialized using our word
embeddings that consider grammaticality and
error patterns extracted from the FCE-public
corpora.

• We demonstrate that updating word embed-
dings using error patterns extracted from the
Lang-8 (Mizumoto et al., 2011) in addition to
FCE-public corpora greatly improves gram-
matical error detection.

• The proposed word embeddings can distin-
guish between correct and incorrect phrase
pairs.

• We have released our code and learned word
embeddings3.

The rest of this paper is organized as follows:
in Section 2, we first give a brief overview of En-
glish grammatical error detection; Section 3 de-
scribes our grammatical error detection model us-
ing error- and grammaticality-specific word em-
beddings; Section 4 evaluates this model on the
FCE-public dataset, and Section 5 presents an
analysis of the grammatical error detection model
and learned word embeddings; and Section 6 con-
cludes this paper.

2 Related Works

Many studies on grammatical error detection try
to address specific types of grammatical errors
(Tetreault and Chodorow, 2008; Han et al., 2006;
Kochmar and Briscoe, 2014). In contrast, Rei and
Yannakoudakis (2016) target all errors using a Bi-

1The similarity of the phrase pairs was calculated based
on the similarity of the mean vector of the word vectors.

2http://lang-8.com/
3https://github.com/kanekomasahiro/grammatical-error-

detection

LSTM, whose embedding layer is initialized with
word2vec. We also address unrestricted grammat-
ical error detection; however, we focus on learn-
ing word embeddings that consider a learner’s er-
ror pattern and grammaticality of the target word.
In this paper, subsequently, our word embeddings
give statistically significant improvements over
their method using exactly the same training data.

Several studies considering grammatical er-
ror patterns in language learning have been per-
formed. For example, Sawai et al. (2013) suggest
correction candidates for verbs using the learner
error pattern, and Liu et al. (2010) automati-
cally correct verb selection errors in English es-
says written by Chinese students learning English,
based on the error patterns created from a syn-
onym dictionary and an English-Chinese bilingual
dictionary. The main difference between these
previous studies and ours is that the previous stud-
ies focused only on verb selection errors.

As an example of research on learning word em-
beddings that consider grammaticality, Alikanio-
tis et al. (2016) proposed a model for construct-
ing word embeddings by considering the impor-
tance of each word in predicting a quality score for
an English learner’s essay. Their approach learns
word embedding from a document-level score us-
ing the mean square error whereas we learn word
embeddings from a word-level binary error infor-
mation using the hinge loss.

The use of a large-scale learner corpus on gram-
matical error correction is described in works
such as Xie et al. (2016) and Chollampatt et al.
(2016a,b). These studies used the Lang-8 corpus
as training data for phrase-based machine trans-
lation (Xie et al., 2016) and neural network joint
models (Chollampatt et al., 2016a,b). In our study,
Lang-8 was used to extract error patterns that were
then utilized to learn word embeddings. Our ex-
periments show that Lang-8 cannot be used as a re-
liable annotation for LSTM-based classifiers. In-
stead, we need to extract useful information as er-
ror patterns to improve the performance of error
detection.

3 Grammatical Error Detection Using
Error- and Grammaticality-Specific
Word Embeddings

In this section, we describe the details of the
proposed word embeddings: EWE, GWE, and
E&GWE. These models extend an existing word

41



Figure 1: Architecture of our learning methods for word embeddings (a) EWE and (b) GWE. Both
models concatenate the word vectors of a sequence for window size and feed them into the hidden layer.
Then, EWE outputs a scalar value, and GWE outputs a prediction of the scalar value and the label of the
word in the middle of the sequence.

embedding learning algorithm called C&W Em-
beddings (Collobert and Weston, 2008) and learn
word embeddings that consider grammatical er-
ror patterns and grammaticality of the target word.
We first describe the well-known C&W embed-
dings, and then explain our extensions. Finally,
we introduce how we incorporate the learned word
embeddings to the grammatical error detection
task using a Bi-LSTM.

3.1 C&W Embeddings

Collobert and Weston (2008; 2011) proposed a
window-based neural network model that learns
distributed representations of target words based
on the local context.

Here, target word wt is the central word
in the window sized sequence of words S =
(w1, . . . , wt, . . . , wn). The representation of the
target word wt is compared with the representa-
tions of other words that appear in the same se-
quence (∀wi ∈ S|wi ̸= wt). A negative sample
S′ = (w1, ..., wc, ..., wn|wc ∼ V ) is created by
replacing the target word wt with a randomly se-
lected word from the vocabulary V to distinguish
between the negative sample S′ and the original
word sequence S. In their method, the word se-
quence S and the negative sample S′ are converted
into vectors in the embedding layer, which are fed
as embeddings. They concatenate each converted

vector and treat it as input vector x ∈ Rn×D,
where D is the dimension of the embedding layer.
The input vector x is then subjected to a linear
transformation (Eq. (1)) to calculate the vector i of
the hidden layer. Then, the resulting vector is sub-
jected to another linear transformation (Eq. (2)) to
obtain the output f(x).

i = σ(Whxx + bh) (1)
f(x) = Wohi + bo (2)

Here, Whx is the weight matrix between the input
vector and the hidden layer, Woh is the weight ma-
trix between the hidden layer and the output layer,
bo and bh are biases, and σ is an element-wise non-
linear function tanh.

This model for word representation learns dis-
tributed representations by making the ranking of
the original word sequence S higher than that of
the negative samples S′, which includes noise due
to replaced words. The difference between the
original word sequence and the word sequence in-
cluding noise is optimized to be at least 1.

lossc(S, S′) = max(0, 1− f(x) + f(x′)) (3)
Here, x′ is a transformed vector at the embedding
layer obtained by converting the word wc of the
negative sample S′.

Our proposed models learn distributed repre-
sentations using the same hinge loss (Eq. (3)) so

42



the model could distinguish between correct and
incorrect phrase pairs.

3.2 Error-Specific Word Embeddings (EWE)

EWE learns word embeddings using the same
model as C&W embeddings. However, rather than
creating negative samples randomly, we created
them by replacing the target word wt with words
wc that learners tend to easily confuse with the tar-
get word wt. In such a case, wc is sampled by the
conditional probability:

P (wc|wt) = |wc, wt|∑
wc′ |wc′, wt|

(4)

where, wt is a target word, wc′ is a set of wc re-
garding wt.

This model learns to distinguish between a cor-
rect and an incorrect word by considering error
patterns. Replacement candidates, treated as error
patterns, are extracted from a learner corpus anno-
tated with correction. Figure 1a represents archi-
tecture of EWE.

The bus will pick you up right at your
hotel entery/*entrance.

The above sentence is a simple example from the
test data of FCE-public corpus. In this sentence,
the word “entery” is incorrect and the “entrance”
is the correct word. In this case, wt is “entrance”
and wc is “entery”. Note that we use only one-to-
one (substitution) error patterns.

Due to the data sparseness problem, the context
of infrequent words cannot be properly learned.
This problem is solved by using a large corpus to
pre-train word2vec. By fine-tuning vectors whose
contexts have already been learned, it is possible
to learn word embeddings with no or few replace-
ment candidates in a learner corpus.

3.3 Grammaticality-Specific Word
Embeddings (GWE)

Similar to the approach of Alikaniotis et al. (2016)
for essay score prediction, we extend C&W em-
beddings to distinguish between correct words and
incorrect words by including grammaticality in
distributed representations (Figure 1b). For that
purpose, we add an additional output layer to pre-
dict grammaticality of word sequences, and extend
Equation (3) to calculate following two error func-

tions.

fgrammar(x) = Wghi + bg (5)
ŷ = softmax(fgrammar(x)) (6)

lossp(S) = −
∑

y · log(ŷ) (7)
loss(S, S′) =

α · lossc(S, S′) + (1− α) · lossp(S)
(8)

In Equation (5), fgrammar is the predicted label of
the original word sequence S. Wgh is the weight
matrix and bg is the bias. In Equation (6), the pre-
diction probability ŷ is computed using the soft-
max function for fgrammar. The error lossp is
computed using the cross-entropy function using
the gold label’s vector y of the target word (Eq.
(7)). Finally, two errors are combined to calculate
loss (Eq. (8)). Here, α is a hyperparameter that
determines the weight of the two error functions.

We use the original tag label (0/1) of the FCE-
public data as the grammaticality of word se-
quences for learning. Note that we do not use label
information from Lang-8, because the error anno-
tation of Lang-8 error annotations are too noisy
to train an error detection model directly from the
corpus. Negative examples of GWE are created
randomly, that are similar to the case with C&W.

3.4 Error- and Grammaticality-Specific
Word Embeddings (E&GWE)

E&GWE is a model that combines EWE and
GWE. In particular, E&GWE model creates neg-
ative examples using an error pattern as in EWE
and outputs score and predicts grammaticality as
in GWE.

3.5 Bidirectional LSTM (Bi-LSTM)

We use bidirectional LSTM (Bi-LSTM)
(Graves and Schmidhuber, 2005) as a classifier
for all our experiments for English grammatical
error detection, because Bi-LSTM demonstrates
the state-of-the-art accuracy for this task com-
pared to other architectures such as CRF and
CNNs (Rei and Yannakoudakis, 2016).

The LSTM calculation is expressed as follows:

it =
σ(Wieet + Wihht−1 + Wicct−1 + bi)

(9)

ft =
σ(Wfeet + Wfhht−1 + Wfcct−1 + bf )

(10)

43



Figure 2: A bidirectional LSTM network. The
word vectors ei enter the hidden layer to predict
the labels of each word.

ct = it ⊙ g(Wceet
+Wchht−1 + bc) + ft ⊙ ct−1

(11)

ot = σ(Woeet + Wohht−1 + Wocct + bo) (12)

ht = ot ⊙ h(ct) (13)

Here, et is the word embedding of word wt, and
Wie, Wfe, Wce and Woe are weight matrices. Each
bi, bf , bc and bo are biases. An LSTM cell block
has an input gate it, a memory cell ct, a forget
gate ft and an output gate ot to control information
flow. In addition, g and h are the sigmoid function
and σ is the tanh. ⊙ is the pointwise multiplica-
tion.

We apply a bidirectional extension of LSTM, as
shown in Figure 2, to encode the word embedding
ei from both left-to-right and right-to-left direc-
tions.

yt = Wyh(hLt ⊗ hRt ) + by (14)

The Bi-LSTM model maps each word wt to a
pair of hidden vectors hLt and h

R
t , i.e., the hidden

vector of the left-to-right LSTM and right-to-left
LSTM, respectively. ⊗ is the concatenation. Wyh
is a weight matrix and by is a bias. We also added
an extra hidden layer for linear transformation be-
tween each of the composition function and the
output layer, as discussed in the previous study.

4 Experiments

4.1 Settings
We used the FCE-public dataset and the Lang-
8 English learner corpus to train classifiers and
word embeddings. For this evaluation, we
used the test set from the FCE-public dataset
(Yannakoudakis et al., 2011) for all experiments.

FCE-public dataset. First, we compared the
proposed methods (EWE, GWE, and E&GWE)
to previous methods (W2V and C&W) relative to
training word embeddings (see Table 2a). For this
purpose, we trained our word embeddings and a
classifier, which were initialized using pre-trained
word embeddings, with the training set from the
FCE-public dataset.

This dataset is one of the most famous English
learner corpus in grammatical error correction. It
contains essays written by English learners. It is
annotated with grammatical errors along with er-
ror classification. We followed the official split
of the data: 30, 953 sentences as a training set,
2, 720 sentences as a test set, and 2, 222 sentences
as a development set. In the FCE-public dataset,
the number of target words of error patterns is
4,184, the number of tokens of the replacement
candidates is 9,834, and the number of types is
6,420. All manually labeled words in the FCE-
public dataset were set as the gold target to train
the GWE. For a missing word error, an error label
is assigned to the word immediately after the miss-
ing word (see Table 4 (c)). To prevent overfitting,
singleton words in the training data were taken as
unknown words.

Lang-8 corpus. Furthermore, we added the
large-scale Lang-8 English learner corpus to the
FCE-public dataset to train word embeddings
(FCE+EWE-L8 and FCE+E&GWE-L8) to ex-
plore the effect of a large data on the proposed
methods. We used a classifier trained using only
the FCE-public dataset whose word embeddings
were initialized with the large-scale pre-trained
word embeddings to compare the results with
those of a classifier trained directly using a noisy
large-scale data whose word embeddings were ini-
tialized using word2vec (FCE&L8+W2V, see Ta-
ble 2b).

Lang-8 learner corpus has over 1 million man-
ually annotated English sentences written by ESL
learners. Extraction of error patterns from Lang-8
in the process of creating negative samples to train
word embeddings was performed as follows:

44



1. Extract word pairs using the dynamic pro-
gramming from a correct sentence and an in-
correct sentence.

2. If the learner’s word of the extracted word
pair is included in the vocabulary created
from FCE-public, include it to the error pat-
terns.

In the Lang-8 dataset the number of types of target
words of the replacement candidates is 10,372, the
number of tokens of the replacement candidates is
272,561, and the number of types is 61,950.

Our experiments on FCE+EWE-L8 and
FCE+E&GWE-L8 were conducted by combining
error patterns from all of Lang-8 corpus and
the training part of FCE-public corpus to train
word embeddings. However, since the number
of error patterns of Lang-8 is larger than that of
FCE-public, we normalized each frequency so
that the ratio was 1:1.

We use F0.5 as the main evaluation
measure, following a previous study
(Rei and Yannakoudakis, 2016). This mea-
sure was also adopted in the CoNLL-14 shared
task on error correction task (Ng et al., 2014).
It combines both precision and recall, while
assigning twice as much weight to precision be-
cause accurate feedback is often more important
than coverage in error detection applications
(Nagata and Nakatani, 2010). Nagata and
Nakatani (2010) presented a precision-oriented
error detection system for articles and numbers
that demonstrated precision of 0.72 and a recall
of 0.25 and achieved a learning effect that is
comparable to that of a human tutor.

4.2 Word Embeddings

We set parameters for word embeddings accord-
ing to the previous study (Rei and Yannakoudakis,
2016). The dimension of the embedding layer of
C&W, GWE, EWE and E&GWE is 300 and the
dimension of the hidden layer is 200. We used a
publicly released word2vec vectors (Chelba et al.,
2013) trained on the News crawl from Google
news4 as pre-trained word embeddings. We set
other parameters in our model by running a pre-
liminary experiment in which the window size is
3, the number of negative samples is 600, the
linear interpolation α is 0.03, and the optimizer
is the ADAM algorithm (Kingma and Ba, 2015)

4https://github.com/mmihaltz/word2vec-GoogleNews-
vectors

with the initial learning rate of 0.001. GWE is
initialized randomly and EWE is initialized using
pre-trained word2vec.

4.3 Classifier
We use EWE, GWE, and E&GWE word em-
beddings to initialize the Bi-LSTM neural net-
work, and predict the correctness of the target
word in the input sentence. We update initialized
weights of embedding layer while training classi-
fiers, since it showed better results. The parame-
ters and settings of the network are the same as in
a previous study (Rei and Yannakoudakis, 2016).
Specifically, in Bi-LSTM the dimensions of the
embedding layer, the first hidden layer, and the
second hidden layer are 300, 200, and 50, respec-
tively. The Bi-LSTM model was optimized us-
ing the ADAM algorithm (Kingma and Ba, 2015)
with an initial learning rate of 0.001, and a batch
size of 64 sentences.

4.4 Results
Table 2a shows experimental results comparing
Bi-LSTM models trained on FCE-public dataset
initialized with two baselines (FCE+W2V and
FCE+C&W) and the proposed word embeddings
(FCE+EWE, FCE+GWE and FCE+E&GWE) in
the error detection task. We used two models
for FCE+W2V: FCE+W2V (R&Y 2016) is the
experimental result reported in a previous study
(Rei and Yannakoudakis, 2016), and FCE+W2V
(our reimplementation of (R&Y, 2016)) is the ex-
perimental result of our reimplementation of Rei
and Yannakoudakis (2016). FCE+E&GWE is a
model combining FCE+EWE and FCE+GWE. We
conducted Wilcoxon signed rank test (p ≤ 0.05) 5
times.

Table 2b shows the result of using addi-
tional large-scale Lang-8 corpus. Compared to
FCE&L8+W2V, FCE+EWE-L8 has better results
within the three evaluation metrics. From this re-
sult, it can be seen that it is better to extract and
use error patterns than simply using Lang-8 cor-
pus as a training data to train a classifier, as it con-
tains noise in the correct sentences. Furthermore,
by combining with GWE method, accuracy was
improved as in the above experiment.

In terms of precision, recall, and F0.5, the meth-
ods in our study were ranked as FCE+E&GWE-
L8 > FCE+EWE-L8 > FCE+E&GWE >
FCE+GWE > FCE+EWE > FCE+W2V >
FCE+C&W. Error patterns and grammaticality

45



Bi-LSTM + embeddings P R F0.5
FCE + W2V (R&Y, 2016) 46.1 28.5 41.1
FCE + W2V (our reimplementation of (R&Y, 2016)) 45.8±0.1 27.8±0.4 40.5±0.3
FCE + C&W 45.1±0.3 26.7±0.4 39.6±0.3
FCE + EWE 46.1±0.1⋆ 28.0±0.1⋆ 40.8±0.1⋆
FCE + GWE 46.5±0.1⋆ 28.3±0.4⋆ 41.2±0.2⋆
FCE + E&GWE 46.7±0.1⋆ 28.6±0.1⋆ 41.4±0.1⋆

(a) LSTM and word embeddings are trained only using FCE-public.

Bi-LSTM + embeddings P R F0.5
FCE&L8 + W2V 12.3±2.6 32.8±2.2 14.0±2.6
FCE + EWE-L8 50.5±3.4⋆ 30.1±1.2⋆ 44.4±2.7⋆
FCE + E&GWE-L8 50.8±3.6⋆ 30.0±1.2⋆ 44.6±2.8⋆

(b) Either FCE-public and a large-scale Lang-8 corpus are used to train LSTM or word embeddings.

Table 2: Results of grammatical error detection by Bi-LSTM. Asterisks indicate that there is a significant
difference for the confidence interval 0.95 for the P, R and F0.5 against FCE + W2V (our reimplementa-
tion of (R&Y, 2016)).

Error type Verb Missing-article Noun Noun type

(a)
FCE + W2V 56 48 26 9
FCE + C&W 53 46 24 7
FCE + EWE 60 37 29 12

(b) FCE + GWE 62 43 29 11
FCE + E&GWE 64 40 31 14

(c)
FCE + EWE-L8 66 36 37 19
FCE + E&GWE-L8 67 40 39 18
Total number of errors 131 112 77 32

Table 3: Numbers of correct instances for typical error types.

consistently improved the accuracy of grammat-
ical error detection, showing that the proposed
methods are effective. Also, our proposed method
has a statistically significant difference compared
with previous research even without using large-
scale Lang-8 corpus. It outperformed the pre-
ceding state-of-the-art (Rei and Yannakoudakis,
2016) in all evaluation metrics.

5 Discussion

Table 3 shows the number of correct answers of
each model for some typical errors. Error types
are taken from the gold label of the FCE-public
dataset.

First, we analyze verb errors and missing arti-
cles, which have the largest differences between
the numbers of correct answers of baselines and
the proposed methods (see Table 3 (a) and (b)).
The proposed methods gave more correct an-
swers for verb errors, whereas FCE+W2V and

FCE+C&W gave more correct answers for miss-
ing article errors. A possible explanation is that
unigram-based error patterns are too powerful for
word embeddings to learn other errors that can be
learned from the contextual clues.

Second, we examine the difference made by
adding the error patterns extracted from Lang-
8 (see Table 3 (b) and (c)): FCE+EWE and
FCE+EWE-L8 have the greatest difference in the
number of correct answers in noun and noun type
errors. FCE+EWE-L8 has more correct answers
for noun errors such as suggestion and advice and
noun type errors such as time and times. The rea-
son is that Lang-8 includes a wide variety of lexi-
cal choice errors of nouns while FCE-public cov-
ers only a limited number of error variations.

Table 4 demonstrates the examples of error de-
tection of the baseline FCE+W2V and the best
proposed method FCE+E&GWE-L8 on the test
data. Table 4(a) shows an example of a noun error,

46



Bi-LSTM + embeddings Detection result
Gold The bus will pick you up right at your hotel entrance.

(a) FCE + W2V The bus will pick you up right at your hotel entery.
FCE + E&GWE-L8 The bus will pick you up right at your hotel entery.
Gold There are shops which sell clothes, food, and books…

(b) FCE + W2V There are shops which sales cloths, foods, and books…
FCE + E&GWE-L8 There are shops which sales cloths, foods, and books…
Gold All the buses and the MTR have air-condition.

(c) FCE + W2V All the buses and MTR have air-condition.
FCE + E&GWE-L8 All the buses and MTR have air-condition.

Table 4: Examples of error detection by FCE+W2V and FCE+E&GWE-L8. Gold corrections in italic,
and detected errors in bold.

and as it can be seen, FCE+E&GWE-L8 detected
the error in contrast to FCE+W2V. Noun type er-
rors are presented in Table 4(b). Here, FCE+W2V
did not detect any error, while FCE+E&GWE-
L8 could detect the mass noun error, frequently
found in a learner corpus. Detection of “sale”
and “cloths” was failed in both models, but they
are hard to detect since the former requires syn-
tactic information and the latter involves com-
mon knowledge. In Table 4(c), FCE+W2V suc-
ceeded in detection of a missing article error, but
FCE+E&GWE-L8 did not. Even though proposed
word embeddings learn substitution errors effec-
tively, they cannot properly learn insertion and
deletion errors. It is our future work to extend
word embeddings to include these types of errors
and focus on contextual errors that are difficult to
deal with the model, for example, missing articles.

Figure 3 visualizes word embeddings
(FCE+W2V and FCE+E&GWE-L8) of fre-
quently occurring errors in learning data using
t-SNE. We plot prepositions and some typical
verbs5, where FCE+E&GWE-L8 showed better
results compared to FCE+W2V. Proportional to
the frequency of errors, the position of the word
embeddings of FCE+E&GWE-L8 changes in
comparison with that of FCE+W2V. For example,
FCE+E&GWE-L8 learned the embeddings of
high-frequency words such as was and could
differently from FCE+W2V. On the other hand,
low-frequency words such as under and walk
were learned similarly. Also, almost all words
shown in this figure move to the upper right.
These visualization can be used to analyze errors
made by learners.

5This dataset includes modal verbs as verb errors.

Figure 3: Visualization of word embeddings by
FCE+W2V and FCE+E&GWE-L8. The red color
represents the word of FCE+W2V and the blue
represents FCE+E&GWE-L8.

6 Conclusion

In this study, we proposed word embeddings that
can improve grammatical error detection accuracy
by considering grammaticality and error patterns.
We achieved the state-of-the-art accuracy on the
FCE-public dataset using a Bi-LSTM model ini-
tialized with the proposed word embeddings. The
word embeddings trained on a learner corpus can
distinguish between correct and incorrect phrase
pairs. In addition, we conducted experiments us-
ing a large-scale Lang-8 corpus. As a result, we
showed that it is better to extract error patterns
from such a corpus to train word embeddings than
simply add Lang-8 corpus as a training data to
train a classifier. We analyzed the detection results
for some typical error types and showed the char-
acteristics of learned word representations. We
hope that the learned word embeddings are gen-
eral enough to be of use to help NLP applications

47



to language learning.

7 Acknowledgments

We thank Yangyang Xi of Lang-8, Inc. for kindly
allowing us to use the Lang-8 learner corpus. We
also thank the anonymous reviewers for their in-
sightful comments. This work was partially sup-
ported by JSPS Grant-in-Aid for Young Scientists
(B) Grant Number JP16K16117.

References
Dimitrios Alikaniotis, Helen Yannakoudakis, and

Marek Rei. 2016. Automatic text scoring using neu-
ral networks. In ACL. pages 715–725.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. One billion word benchmark for measur-
ing progress in statistical language modeling. arXiv
preprint arXiv:1312.3005 .

Shamil Chollampatt, Duc Tam Hoang, and Hwee Tou
Ng. 2016a. Adapting grammatical error correction
based on the native language of writers with neu-
ral network joint models. In EMNLP. pages 1901–
1911.

Shamil Chollampatt, Kaveh Taghipour, and Hwee Tou
Ng. 2016b. Neural network translation models
for grammatical error correction. arXiv preprint
arXiv:1606.00189 .

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In ICML.
pages 160–167.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research
12:2493–2537.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks 18(5):602–610.

Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing. pages 115–129.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In ICLR.

Ekaterina Kochmar and Ted Briscoe. 2014. Detect-
ing learner errors in the choice of content words us-
ing compositional distributional semantics. In COL-
ING. pages 1740–1751.

Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun
Stiller, and Ming Zhou. 2010. SRL-based verb se-
lection for ESL. In EMNLP. pages 1068–1076.

Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-
gata, and Yuji Matsumoto. 2011. Mining revi-
sion log of language learning SNS for automated
Japanese error correction of second language learn-
ers. In IJCNLP. pages 147–155.

Ryo Nagata and Kazuhide Nakatani. 2010. Evaluating
performance of grammatical error detection to max-
imize learning effect. In COLING. pages 894–900.

Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 shared task
on grammatical error correction. In CoNLL Shared
Task. pages 1–14.

Marek Rei and Helen Yannakoudakis. 2016. Composi-
tional sequence labeling models for error detection
in learner writing. In ACL. pages 1181–1191.

Yu Sawai, Mamoru Komachi, and Yuji Matsumoto.
2013. A learner corpus-based approach to verb sug-
gestion for ESL. In ACL. pages 708–713.

Joel R Tetreault and Martin Chodorow. 2008. The
ups and downs of preposition error detection in ESL
writing. In COLING. pages 865–872.

Ziang Xie, Anand Avati, Naveen Arivazhagan, Dan Ju-
rafsky, and Andrew Y Ng. 2016. Neural language
correction with character-based attention. arXiv
preprint arXiv:1603.09727 .

Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In ACL-HLT . pages 180–189.

48


