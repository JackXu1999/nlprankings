



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1913–1924
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1175

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1913–1924
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1175

Doubly-Attentive Decoder for Multi-modal Neural Machine Translation

Iacer Calixto
ADAPT Centre

School of Computing
Dublin City University

Dublin, Ireland

Qun Liu
ADAPT Centre

School of Computing
Dublin City University

Dublin, Ireland

{iacer.calixto,qun.liu,nick.campbell}@adaptcentre.ie

Nick Campbell
ADAPT Centre

Speech Communication Lab
Trinity College Dublin

Dublin 2, Ireland

Abstract

We introduce a Multi-modal Neural Ma-
chine Translation model in which a
doubly-attentive decoder naturally incor-
porates spatial visual features obtained us-
ing pre-trained convolutional neural net-
works, bridging the gap between image
description and translation. Our decoder
learns to attend to source-language words
and parts of an image independently by
means of two separate attention mecha-
nisms as it generates words in the target
language. We find that our model can
efficiently exploit not just back-translated
in-domain multi-modal data but also large
general-domain text-only MT corpora. We
also report state-of-the-art results on the
Multi30k data set.

1 Introduction

Neural Machine Translation (NMT) has been suc-
cessfully tackled as a sequence to sequence learn-
ing problem (Kalchbrenner and Blunsom, 2013;
Cho et al., 2014b; Sutskever et al., 2014) where
each training example consists of one source and
one target variable-length sequences, with no prior
information on the alignment between the two.

In the context of NMT, Bahdanau et al. (2015)
first proposed to use an attention mechanism in
the decoder, which is trained to attend to the rel-
evant source-language words as it generates each
word of the target sentence. Similarly, Xu et al.
(2015) proposed an attention-based model for the
task of image description generation (IDG) where
a model learns to attend to specific parts of an im-
age representation (the source) as it generates its
description (the target) in natural language.

We are inspired by recent successes in applying
attention-based models to NMT and IDG. In this

work, we propose an end-to-end attention-based
multi-modal neural machine translation (MNMT)
model which effectively incorporates two inde-
pendent attention mechanisms, one over source-
language words and the other over different areas
of an image.

Our main contributions are:

• We propose a novel attention-based MNMT
model which incorporates spatial visual fea-
tures in a separate visual attention mecha-
nism;

• We use a medium-sized, back-translated
multi-modal in-domain data set and large
general-domain text-only MT corpora to pre-
train our models and show that our MNMT
model can efficiently exploit both;

• We show that images bring useful informa-
tion into an NMT model, e.g. in situations in
which sentences describe objects illustrated
in the image.

To the best of our knowledge, previous MNMT
models in the literature that utilised spatial vi-
sual features did not significantly improve over
a comparable model that used global visual fea-
tures or even only textual features (Caglayan et al.,
2016a; Calixto et al., 2016; Huang et al., 2016; Li-
bovický et al., 2016; Specia et al., 2016). In this
work, we wish to address this issue and propose an
MNMT model that uses, in addition to an atten-
tion mechanism over the source-language words,
an additional visual attention mechanism to incor-
porate spatial visual features, and still improves on
simpler text-only and multi-modal attention-based
NMT models.

The remainder of this paper is structured as
follows. We first briefly revisit the attention-
based NMT framework (§2) and expand it into an
MNMT framework (§3). In §4, we introduce the

1913

https://doi.org/10.18653/v1/P17-1175
https://doi.org/10.18653/v1/P17-1175


datasets we use to train and evaluate our models,
in §5 we discuss our experimental setup and anal-
yse and discuss our results. Finally, in §6 we dis-
cuss relevant related work and in §7 we draw con-
clusions and provide avenues for future work.

2 Background and Notation

2.1 Attention-based NMT
In this section, we describe the attention-based
NMT model introduced by Bahdanau et al. (2015).
Given a source sequence X = (x1, x2, · · · , xN )
and its translation Y = (y1, y2, · · · , yM ), an NMT
model aims to build a single neural network that
translates X into Y by directly learning to model
p(Y | X). The entire network consists of one en-
coder and one decoder with one attention mech-
anism, typically implemented as two Recurrent
Neural Networks (RNN) and one multilayer per-
ceptron, respectively. Each xi is a row index
in a source lookup or word embedding matrix
Ex ∈ R|Vx|×dx , as well as each yj being an in-
dex in a target lookup or word embedding matrix
Ey ∈ R|Vy |×dy , Vx and Vy are source and target
vocabularies, and dx and dy are source and target
word embeddings dimensionalities, respectively.

The encoder is a bi-directional RNN with
GRU (Cho et al., 2014a), where a forward RNN−→
Φ enc reads X word by word, from left to right,
and generates a sequence of forward annota-
tion vectors (

−→
h 1,
−→
h 2, · · · ,

−→
hN ) at each encoder

time step i ∈ [1, N ]. Similarly, a backward RNN←−
Φ enc reads X from right to left, word by word,
and generates a sequence of backward annota-
tion vectors (

←−
hN ,

←−
hN−1, · · · ,

←−
h 1). The final

annotation vector is the concatenation of for-
ward and backward vectors hi =

[−→
hi;
←−
hi
]
, and

C = (h1,h2, · · · ,hN ) is the set of source anno-
tation vectors.

These annotation vectors are in turn used by
the decoder, which is essentially a neural language
model (LM) (Bengio et al., 2003) conditioned on
the previously emitted words and the source sen-
tence via an attention mechanism. A multilayer
perceptron is used to initialise the decoder’s hid-
den state s0 at time step t = 0, where the input
to this network is the concatenation of the last for-
ward and backward vectors

[−→
hN ;
←−
h1
]
.

At each time step t of the decoder, a time-
dependent source context vector ct is computed
based on the annotation vectors C and the decoder
previous hidden state st−1. This is part of the for-

mulation of the conditional GRU and is described
further in §2.2. In other words, the encoder is a
bi-directional RNN with GRU and the decoder is
an RNN with a conditional GRU.

Given a hidden state st, the probabilities for
the next target word are computed using one pro-
jection layer followed by a softmax layer as il-
lustrated in eq. (1), where the matrices Lo, Ls,
Lw and Lc are transformation matrices and ct is
a time-dependent source context vector generated
by the conditional GRU.

2.2 Conditional GRU
The conditional GRU1, illustrated in Figure 1, has
three main components computed at each time
step t of the decoder:

• REC1 computes a hidden state proposal s′t
based on the previous hidden state st−1 and
the previously emitted word ŷt−1;

• ATTsrc2 is an attention mechanism over the
hidden states of the source-language RNN
and computes ct using all source annotation
vectors C and the hidden state proposal s′t;

• REC2 computes the final hidden state st us-
ing the hidden state proposal s′t and the time-
dependent source context vector ct.

First, a single-layer feed-forward network is
used to compute an expected alignment esrct,i be-
tween each source annotation vector hi and the
target word ŷt to be emitted at the current time step
t, as shown in Equations (2) and (3):

esrct,i = (v
src
a )

T tanh(U srca s
′
t + W

src
a hi), (2)

αsrct,i =
exp (esrct,i )∑N
j=1 exp (e

src
t,j)

, (3)

where αsrct,i is the normalised alignment matrix be-
tween each source annotation vector hi and the
word ŷt to be emitted at time step t, and vsrca , U

src
a

and W srca are model parameters.
Finally, a time-dependent source context vector

ct is computed as a weighted sum over the source
annotation vectors, where each vector is weighted
by the attention weight αsrct,i , as in eq. (4):

ct =

N∑

i=1

αsrct,ihi. (4)

1https://github.com/nyu-dl/
dl4mt-tutorial/blob/master/docs/cgru.pdf.

2ATTsrc is named ATT in the original technical report.

1914



p(yt = k | y<t, ct) ∝ exp(Lo tanh(Lsst + LwEy[ŷt−1] + Lcct)). (1)

Figure 1: An illustration of the conditional GRU:
the steps taken to compute the current hidden state
st from the previous state st−1, the previously
emitted word ŷt−1, and the source annotation vec-
tors C, including the candidate hidden state s′t and
the source-language attention vector ct.

3 Multi-modal NMT

Our MNMT model can be seen as an expansion
of the attention-based NMT framework described
in §2.1 with the addition of a visual component to
incorporate spatial visual features.

We use publicly available pre-trained CNNs for
image feature extraction. Specifically, we extract
spatial image features for all images in our dataset
using the 50-layer Residual network (ResNet-50)
of He et al. (2015). These spatial features are
the activations of the res4f layer, which can be
seen as encoding an image in a 14×14 grid, where
each of the entries in the grid is represented by
a 1024D feature vector that only encodes infor-
mation about that specific region of the image.
We vectorise this 3-tensor into a 196×1024 matrix
A = (a1,a2, · · · ,aL),al ∈ R1024 where each of
the L = 196 rows consists of a 1024D feature vec-
tor and each column, i.e. feature vector, represents
one grid in the image.

3.1 NMTSRC+IMG: decoder with two
independent attention mechanisms

Model NMTSRC+IMG integrates two separate atten-
tion mechanisms over the source-language words
and visual features in a single decoder RNN. Our
doubly-attentive decoder RNN is conditioned on

the previous hidden state of the decoder and the
previously emitted word, as well as the source sen-
tence and the image via two independent attention
mechanisms, as illustrated in Figure 2.

We implement this idea expanding the con-
ditional GRU described in §2.2 onto a doubly-
conditional GRU. To that end, in addition to the
source-language attention, we introduce a new at-
tention mechanism ATTimg to the original condi-
tional GRU proposal. This visual attention com-
putes a time-dependent image context vector it
given a hidden state proposal s′t and the image an-
notation vectors A = (a1,a2, · · · ,aL) using the
“soft” attention (Xu et al., 2015).

This attention mechanism is very similar to the
source-language attention with the addition of a
gating scalar, explained further below. First, a
single-layer feed-forward network is used to com-
pute an expected alignment eimgt,l between each im-
age annotation vector al and the target word to be
emitted at the current time step t, as in eqs. (5)
and (6):

e
img
t,l = (v

img
a )

T tanh(U imga s
′
t + W

img
a al), (5)

α
img
t,l =

exp (e
img
t,l )∑L

j=1 exp (e
img
t,j )

, (6)

where αimgt,l is the normalised alignment matrix
between all the image patches al and the target
word to be emitted at time step t, and vimga , U

img
a

and W imga are model parameters. Note that Equa-
tions (2) and (3), that compute the expected source
alignment esrct,i and the weight matrices α

src
t,i , and

eqs. (5) and (6) that compute the expected image
alignment eimgt,l and the weight matrices α

img
t,l , both

compute similar statistics over the source and im-
age annotations, respectively.

In eq. (7) we compute βt ∈ [0, 1], a gating scalar
used to weight the expected importance of the im-
age context vector in relation to the next target
word at time step t:

βt = σ(Wβst−1 + bβ), (7)

where Wβ , bβ are model parameters. It is in turn
used to compute the time-dependent image con-
text vector it for the current decoder time step t,
as in eq. (8):

it = βt

L∑

l=1

α
img
t,l al. (8)

1915



Figure 2: A doubly-attentive decoder learns to at-
tend to image patches and source-language words
independently when generating translations.

The only difference between Equations (4)
(source context vector) and (8) (image context
vector) is that the latter uses a gating scalar,
whereas the former does not. We use β follow-
ing Xu et al. (2015) who empirically found it to
improve the variability of the image descriptions
generated with their model.

Finally, we use the time-dependent image con-
text vector it as an additional input to a modified
version of REC2 (§2.2), which now computes the
final hidden state st using the hidden state pro-
posal s′t, and the time-dependent source and image
context vectors ct and it, as in eq. (9):

zt = σ(W
src
z ct + W

img
z it + Uzs

′
j),

rt = σ(W
src
r ct + W

img
r it + Urs

′
j),

st = tanh(W
srcct + W

imgit + rt � (Us′t)),
st = (1− zt)� st + zt � s′t. (9)

In Equation (10), the probabilities for the next
target word are computed using the new multi-
modal hidden state st, the previously emitted word
ŷt−1, and the two context vectors ct and it, where
Lo, Ls, Lw, Lcs and Lci are projection matrices
and trained with the model.

4 Data

The Flickr30k data set contains 30k images and
5 descriptions in English for each image (Young
et al., 2014). In this work, we use the Multi30k
dataset (Elliott et al., 2016), which consists of two
multilingual expansions of the original Flickr30k:
one with translated data and another one with
comparable data, henceforth referred to as M30kT
and M30kC, respectively.

For each of the 30k images in the Flickr30k,
the M30kT has one of the English descriptions
manually translated into German by a professional
translator. Training, validation and test sets con-
tain 29k, 1,014 and 1k images respectively, each
accompanied by a sentence pair (the original En-
glish sentence and its translation into German).
For each of the 30k images in the Flickr30k,
the M30kC has five descriptions in German col-
lected independently from the English descrip-
tions. Training, validation and test sets contain
29k, 1,014 and 1k images respectively, each ac-
companied by five sentences in English and five
sentences in German.

We use the entire M30kT training set for train-
ing our MNMT models, its validation set for
model selection with BLEU (Papineni et al.,
2002), and its test set for evaluation. In addi-
tion, since the amount of training data available
is small, we build a back-translation model using
the text-only NMT model described in §2.1 trained
on the Multi30kT data set (German→English and
English→German), without images. We use this
model to back-translate the 145k German (En-
glish) descriptions in the Multi30kC into En-
glish (German) and include the triples (synthetic
English description, German description, image)
when translating into German, and the triples (syn-
thetic German description, English description,
image) when translating into English, as addi-
tional training data (Sennrich et al., 2016a).

We also use the WMT 2015 text-only paral-
lel corpora available for the English–German lan-
guage pair, consisting of about 4.3M sentence
pairs (Bojar et al., 2015). These include the Eu-

1916



p(yt = k | y<t, C,A) ∝ exp(Lo tanh(Lsst + LwEy[ŷt−1] + Lcsct + Lciit)). (10)

roparl v7 (Koehn, 2005), News Commentary and
Common Crawl corpora, which are concatenated
and used for pre-training.

We use the scripts in the Moses SMT
Toolkit (Koehn et al., 2007) to normalise and
tokenize English and German descriptions, and
we also convert space-separated tokens into sub-
words (Sennrich et al., 2016b). All models use
a common vocabulary of 83, 093 English and
91, 141 German subword tokens. If sentences in
English or German are longer than 80 tokens, they
are discarded. We train models to translate from
English into German, as well as for German into
English, and report evaluation of cased, tokenized
sentences with punctuation.

5 Experimental setup

Our encoder is a bidirectional RNN with GRU,
one 1024D single-layer forward and one 1024D
single-layer backward RNN. Source and target
word embeddings are 620D each and trained
jointly with the model. Word embeddings and
other non-recurrent matrices are initialised by
sampling from a Gaussian N (0, 0.012), recurrent
matrices are random orthogonal and bias vectors
are all initialised to zero.

Visual features are obtained by feeding images
to the pre-trained ResNet-50 and using the activa-
tions of the res4f layer (He et al., 2015). We
apply dropout with a probability of 0.5 in the en-
coder bidirectional RNN, the image features, the
decoder RNN and before emitting a target word.
We follow Gal and Ghahramani (2016) and apply
dropout to the encoder bidirectional and the de-
coder RNN using one same mask in all time steps.

All models are trained using stochastic gradi-
ent descent with ADADELTA (Zeiler, 2012) with
minibatches of size 80 (text-only NMT) or 40
(MNMT), where each training instance consists
of one English sentence, one German sentence
and one image (MNMT). We apply early stopping
for model selection based on BLEU4, so that if a
model does not improve on BLEU4 in the valida-
tion set for more than 20 epochs, training is halted.

The translation quality of our models is eval-
uated quantitatively in terms of BLEU4 (Pap-
ineni et al., 2002), METEOR (Denkowski and
Lavie, 2014), TER (Snover et al., 2006), and

chrF3 (Popović, 2015).3 We report statistical sig-
nificance with approximate randomisation for the
first three metrics with MultEval (Clark et al.,
2011).

5.1 Baselines
We train a text-only phrase-based SMT (PBSMT)
system and a text-only NMT model for compar-
ison (English→German and German→English).
Our PBSMT baseline is built with Moses and
uses a 5–gram LM with modified Kneser-Ney
smoothing (Kneser and Ney, 1995). It is trained
on the English→German (German→English) de-
scriptions of the M30kT, whereas its LM is trained
on the German (English) descriptions only. We
use minimum error rate training to tune the model
with BLEU (Och, 2003). The text-only NMT
baseline is the one described in §2.1 and is trained
on the M30kT’s English–German descriptions,
again in both language directions.

When translating into German, we also com-
pare our model against two publicly available re-
sults obtained with multi-modal attention-based
NMT models. The first model is Huang et al.
(2016)’s best model trained on the same data, and
the second is their best model using additional ob-
ject detections, respectively models m1 (image at
head) and m3 in the authors’ paper.

5.2 Results
In Table 1, we show results for the two text-
only baselines NMT and PBSMT, the multi-
modal models of Huang et al. (2016), and our
MNMT models trained on the M30kT and pre-
trained on the in-domain back-translated M30kC
and the general-domain text-only English-German
MT corpora from WMT 2015. All models are
trained to translate from English into German.

Training on M30kT One main finding is that
our model consistently outperforms the compa-
rable model of Huang et al. (2016) when trans-
lating into German, with improvements of +1.4
BLEU and +2.7 METEOR. In fact, even when
their model has access to more data our model still
improves by +0.9 METEOR.

Moreover, we can also conclude from Table 1
that PBSMT performs better at recall-oriented

3We specifically compute character 6-gram F3, and addi-
tionally character precision and recall for comparison.

1917



English→German
Model Training BLEU4↑ METEOR↑ TER↓ chrF3↑ (prec. / recall)

data

NMT M30kT 33.7 52.3 46.7 65.2 (67.7 / 65.0)
PBSMT M30kT 32.9 54.3† 45.1† 67.4 (66.5 / 67.5)
Huang et al. (2016) M30kT 35.1 (↑ 1.4) 52.2 (↓ 2.1) — — —

+ RCNN 36.5 (↑ 2.8) 54.1 (↓ 0.2) — — —

NMTSRC+IMG M30kT 36.5†‡ 55.0† 43.7†‡ 67.3 (66.8 / 67.4)

Improvements

NMTSRC+IMG vs. NMT ↑ 2.8 ↑ 2.7 ↓ 3.0 ↑ 2.1 ↓ 0.9 / ↑ 2.4
NMTSRC+IMG vs. PBSMT ↑ 3.6 ↑ 0.7 ↓ 1.4 ↓ 0.1 ↑ 0.3 / ↓ 0.1
NMTSRC+IMG vs. Huang ↑ 1.4 ↑ 2.8 — — —
NMTSRC+IMG vs. Huang (+RCNN) ↑ 0.0 ↑ 0.9 — — —

Pre-training data set: back-translated M30kC (in-domain)

PBSMT (LM) M30kT 34.0 ↑ 0.0 55.0† ↑ 0.0 44.7 ↑ 0.0 68.0 (66.8 / 68.1)
NMT M30kT 35.5‡ ↑ 0.0 53.4 ↑ 0.0 43.3‡ ↑ 0.0 65.2 (67.7 / 65.0)
NMTSRC+IMG M30kT 37.1†‡ 54.5†‡ 42.8†‡ 66.6 (67.2 / 66.5)

NMTSRC+IMG vs. best PBSMT ↑ 3.1 ↓ 0.5 ↓ 1.9 ↓ 1.4 ↑ 0.4 / ↓ 1.6
NMTSRC+IMG vs. NMT ↑ 1.6 ↑ 1.1 ↓ 0.5 ↑ 1.4 ↓ 0.5 / ↑ 1.5

Pre-training data set: WMT’15 English-German corpora (general domain)

PBSMT (concat) M30kT 32.6 53.9 46.1 67.3 (66.3 / 67.4)
PBSMT (LM) M30kT 32.5 54.1 46.0 67.3 (66.0 / 67.4)
NMT M30kT 37.8† ↑ 0.0 56.7† ↑ 0.0 41.0† ↑ 0.0 69.2 (69.7 / 69.1)
NMTSRC+IMG M30kT 39.0†‡ 56.8†‡ 40.6†‡ 69.6 (69.6 / 69.6)

NMTSRC+IMG vs. best PBSMT ↑ 6.4 ↑ 2.7 ↓ 5.4 ↑ 2.3 ↑ 3.3 / ↑ 2.2
NMTSRC+IMG vs. NMT ↑ 1.2 ↑ 0.1 ↓ 0.4 ↑ 0.4 ↓ 0.1 / ↑ 0.5

Table 1: BLEU4, METEOR, chrF3, character-level precision and recall (higher is better) and TER scores
(lower is better) on the translated Multi30k (M30kT) test set. Best text-only baselines results are under-
lined and best overall results appear in bold. We show Huang et al. (2016)’s improvements over the best
text-only baseline in parentheses. Results are significantly better than the NMT baseline (†) and the SMT
baseline (‡) with p < 0.01 (no pre-training) or p < 0.05 (when pre-training either on the back-translated
M30kC or WMT’15 corpora).

metrics, i.e. METEOR and chrF3, whereas NMT
is better at precision-oriented ones, i.e. BLEU4.
This is somehow expected, since the attention
mechanism in NMT (Bahdanau et al., 2015) does
not explicitly take attention weights from previous
time steps into account, an thus lacks the notion
of source coverage as in SMT (Koehn et al., 2003;
Tu et al., 2016). We note that these ideas are com-
plementary and incorporating coverage into model
NMTSRC+IMG could lead to more improvements,
especially in recall-oriented metrics. Nonetheless,
our doubly-attentive model shows consistent gains
in both precision- and recall-oriented metrics in
comparison to the text-only NMT baseline, i.e. it
is significantly better according to BLEU4, ME-
TEOR and TER (p < 0.01), and it also improves
chrF3 by +2.1. In comparison to the PBSMT
baseline, our proposed model still significantly
improves according to both BLEU4 and TER (p <

0.01), also increasing METEOR by +0.7 but with
an associated p-value of p = 0.071, therefore not
significant for p < 0.05. Although chrF3 is the
only metric in which the PBSMT model scores
best, the difference between our model and the lat-
ter is only 0.1, meaning that they are practically
equivalent. We note that model NMTSRC+IMG con-
sistently increases character recall in comparison
to the text-only NMT baseline. Although it can
happen at the expense of character precision, gains
in recall are always much higher than any eventual
loss in precision, leading to consistent improve-
ments in chrF3.

In Table 2, we observe that when translating
into English and training on the original M30kT,
model NMTSRC+IMG outperforms both baselines
by a large margin, according to all four met-
rics evaluated. We also note that both model
NMTSRC+IMG’s character-level precision and re-

1918



German→English
Model BLEU4↑ METEOR↑ TER↓ chrF3↑
PBSMT 32.8 34.8 43.9 61.8
NMT 38.2 35.8 40.2 62.8
NMTSRC+IMG 40.6†‡ 37.5†‡ 37.7†‡ 65.2

Improvements

Ours vs. NMT ↑ 2.4 ↑ 1.7 ↓ 2.5 ↑ 2.4
Ours vs. PBSMT ↑ 7.8 ↑ 2.7 ↓ 6.2 ↑ 3.4

Pre-training data set: back-translated M30kC (in-domain)

PBSMT 36.8 36.4 40.8 64.5
NMT 42.6 38.9 36.1 67.6
NMTSRC+IMG 43.2‡† 39.0‡† 35.5‡† 67.7

Improvements

Ours vs. PBSMT ↑ 6.4 ↑ 2.6 ↓ 5.3 ↑ 3.2
Ours vs. NMT ↑ 0.6 ↑ 0.1 ↓ 0.6 ↑ 0.1

Table 2: BLEU4, METEOR, chrF3 (higher is bet-
ter), and TER scores (lower is better) on the trans-
lated Multi30k (M30kT) test set. Best text-only
baselines results are underlined and best overall
results appear in bold. Results are significantly
better than the NMT baseline (†) and the SMT
baseline (‡) with p < 0.01.

call are higher than those of the two baselines, in
contrast to results obtained when translating from
English into German. This suggests that model
NMTSRC+IMG might better integrate the image fea-
tures when translating into an “easier” language,
i.e. a language with less morphology, although ex-
periments involving more language pairs are nec-
essary to confirm whether this is indeed the case.

Pre-training We now discuss results for mod-
els pre-trained using different data sets. We first
pre-trained the two text-only baselines PBSMT
and NMT, and our MNMT model on the back-
translated M30kC, a medium-sized in-domain im-
age description data set (145k training instances),
in both directions. We also pre-trained the same
models on the English–German parallel sentences
of much larger MT data sets, i.e. the concatenation
of the Europarl (Koehn, 2005), Common Crawl
and News Commentary corpora, used in WMT
2015 (∼4.3M parallel sentences). Model PB-
SMT (concat.) used the concatenation of the pre-
training and training data for training, and model
PBSMT (LM) used the general-domain German
sentences as additional data to train the LM. From
Tables 1 and 2, it is clear that model NMTSRC+IMG
can learn from both in-domain, multi-modal pre-
training data sets as well as text-only, general do-
main ones.

Pre-training on M30kC When pre-training on
the back-translated M30kC and translating into
German, the recall-oriented chrF3 shows a dif-
ference of 1.4 points between PBSMT and our
model, mostly due to character recall; nonethe-
less, our model still improved by the same mar-
gin on the text-only NMT baseline. Our model
still outperforms the PBSMT baseline according
to BLEU4 and TER, and the text-only NMT base-
line according to all metrics (p < .05).

When translating into English, model
NMTSRC+IMG still consistently scores higher
according to all metrics evaluated, although the
differences between its translations and those
obtained with the NMT baseline are no longer
statistically significant (p < 0.01).

Pre-training on WMT 2015 corpora We also
pre-trained our English–German models on the
WMT 2015 corpora, which took 10 days,
i.e. ∼6–7 epochs. Results show that model
NMTSRC+IMG improves significantly over the
NMT baseline according to BLEU4, and is con-
sistently better than the PBSMT baseline accord-
ing to all four metrics.4 This is a strong indica-
tion that model NMTSRC+IMG can exploit the ad-
ditional pre-training data efficiently, both general-
and in-domain. While the PBSMT model is
still competitive when using additional in-domain
data—according to METEOR and chrF3— the
same cannot be said when using general-domain
pre-training corpora. From our experiments,
NMT models in general, and especially model
NMTSRC+IMG, thrive when training and test do-
mains are mixed, which is a very common real-
world scenario.

Textual and visual attention In Figure 3, we
visualise the visual and textual attention weights
for an entry of the M30kT test set. In the visual
attention, the β gate (written in parentheses after
each word) caused the image features to be used
mostly to generate the words Mann (man) and Hut
(hat), two highly visual terms in the sentence. We
observe that in general visually grounded terms,
e.g. Mann and Hut, usually have a high associated
β value, whereas other less visual terms like mit
(with) or auf (at) do not. That causes the model to
use the image features when it is describing a vi-
sual concept in the sentence, which is an interest-

4In order for PBSMT models to remain competitive, we
believe more advanced data selection techniques are needed,
which are out of the scope of this work.

1919



(a) Image–target word alignments. (b) Source–target word alignments.

Figure 3: Visualisation of image– and source–target word alignments for the M30kT test set.

ing feature of our model. Interestingly, our model
is very selective when choosing to use image fea-
tures: it only assigned β > 0.5 for 20% of the out-
putted target words, and β > 0.8 to only 8%. A
manual inspection of translations shows that these
words are mostly concrete nouns with a strong vi-
sual appeal.

Lastly, using two independent attention mech-
anisms is a good compromise between model
compactness and flexibility. While the attention-
based NMT model baseline has ∼200M parame-
ters, model NMTSRC+IMG has ∼213M, thus using
just ∼6.6% more parameters than the latter.

6 Related work

Multi-modal MT was just recently addressed
by the MT community by means of a shared
task (Specia et al., 2016). However, there has
been a considerable amount of work on natu-
ral language generation from non-textual inputs.
Mao et al. (2014) introduced a multi-modal RNN
that integrates text and visual features and ap-
plied it to the tasks of image description genera-
tion and image–sentence ranking. In their work,
the authors incorporate global image features in a
separate multi-modal layer that merges the RNN
textual representations and the global image fea-
tures. Vinyals et al. (2015) proposed an influ-
ential neural IDG model based on the sequence-
to-sequence framework, which is trained end-to-
end. Elliott et al. (2015) put forward a model to
generate multilingual descriptions of images by
learning and transferring features between two in-

dependent, non-attentive neural image description
models.5 Venugopalan et al. (2015) introduced a
model trained end-to-end to generate textual de-
scriptions of open-domain videos from the video
frames based on the sequence-to-sequence frame-
work. Finally, Xu et al. (2015) introduced the first
attention-based IDG model where an attentive de-
coder learns to attend to different parts of an image
as it generates its description in natural language.

In the context of NMT, Zoph and Knight (2016)
introduced a multi-source attention-based NMT
model trained to translate a pair of sentences in
two different source languages into a target lan-
guage, and reported considerable improvements
over a single-source baseline. Dong et al. (2015)
proposed a multi-task learning approach where a
model is trained to translate from one source lan-
guage into multiple target languages. Firat et al.
(2016) put forward a multi-way model trained to
translate between many different source and tar-
get languages. Instead of one attention mecha-
nism per language pair as in Dong et al. (2015),
which would lead to a quadratic number of atten-
tion mechanisms in relation to language pairs, they
use a shared attention mechanism where each tar-
get language has one attention shared by all source
languages. Luong et al. (2016) proposed a multi-
task approach where they train a model using two
tasks and a shared decoder: the main task is to
translate from German into English and the sec-

5Although their model has not been devised with transla-
tion as its primary goal, theirs is one of the baselines of the
first shared task in multi-modal MT in WMT 2016 (Specia
et al., 2016).

1920



ondary task is to generate English image descrip-
tions. They show improvements in the main trans-
lation task when also training for the secondary
image description task. Although not an NMT
model, Hitschler et al. (2016) recently used image
features to re-rank translations of image descrip-
tions generated by an SMT model and reported
significant improvements.

Although no purely neural multi-modal model
to date significantly improves on both text-only
NMT and SMT models (Specia et al., 2016), dif-
ferent research groups have proposed to include
global and spatial visual features in re-ranking
n-best lists generated by an SMT system or di-
rectly in an NMT framework with some suc-
cess (Caglayan et al., 2016a; Calixto et al., 2016;
Huang et al., 2016; Libovický et al., 2016; Shah
et al., 2016). To the best of our knowledge, the
best published results of a purely MNMT model
are those of Huang et al. (2016), who proposed
to use global visual features extracted with the
VGG19 network (Simonyan and Zisserman, 2015)
for an entire image, and also for regions of the im-
age obtained using the RCNN of Girshick et al.
(2014). Their best model improves over a strong
text-only NMT baseline and is comparable to re-
sults obtained with an SMT model trained on the
same data. For that reason, their models are used
as baselines in our experiments whenever possible.

Our work differs from previous work in that,
first, we propose attention-based MNMT mod-
els. This is an important difference since the use
of attention in NMT has become standard and
is the current state-of-the-art (Jean et al., 2015;
Luong et al., 2015; Firat et al., 2016; Sennrich
et al., 2016b). Second, we propose a doubly-
attentive model where we effectively fuse two
mono-modal attention mechanisms into one multi-
modal decoder, training the entire model jointly
and end-to-end. Additionally, we are interested
in how to merge textual and visual representa-
tions into multi-modal representations when gen-
erating words in the target language, which differs
substantially from text-only translation tasks even
when these translate from many source languages
and/or into many target languages (Dong et al.,
2015; Firat et al., 2016; Zoph and Knight, 2016).
To the best of our knowledge, we are among the
first6 to integrate multi-modal inputs in NMT via

6As pointed out by an anonymous reviewer, Caglayan
et al. (2016b) have also experimented with attention-based

independent attention mechanisms.

Applications Initial experiments with model
NMTSRC+IMG have been reported in Calixto et al.
(2016). Additionally, NMTSRC+IMG has been ap-
plied to the machine translation of user-generated
product listings from an e-commerce website,
while also making use of the product images to
improve translations (Calixto et al., 2017b,a).

7 Conclusions and Future Work

We have introduced a novel attention-based,
multi-modal NMT model to incorporate spatial
visual information into NMT. We have reported
state-of-the-art results on the M30kT test set, im-
proving on previous multi-modal attention-based
models. We have also showed that our model
can be efficiently pre-trained on both medium-
sized back-translated in-domain multi-modal data
as well as also large general-domain text-only MT
corpora, finding that it is able to exploit the addi-
tional data regardless of the domain. Our model
also compares favourably to both NMT and PB-
SMT baselines evaluated on the same training
data. In the future, we will incorporate coverage
into our model and study how to apply it to other
Natural Language Processing tasks.

Acknowledgements

This project has received funding from Science
Foundation Ireland in the ADAPT Centre for Dig-
ital Content Technology (www.adaptcentre.ie) at
Dublin City University funded under the SFI Re-
search Centres Programme (Grant 13/RC/2106)
co-funded under the European Regional Develop-
ment Fund and the European Union Horizon 2020
research and innovation programme under grant
agreement 645452 (QT21). The authors would
like to thank Chris Hokamp, Peyman Passban,
and Dasha Bogdanova for insightful discussions
at early stages of this work, Andy Way for proof-
reading and providing many good suggestions of
improvements, as well as our anonymous review-
ers for their valuable comments and feedback.

Reproducibility

Code and pre-trained models for this pa-
per are available at https://github.
com/iacercalixto/nmt_doubly_
attentive.
multi-modal NMT.

1921



References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua

Bengio. 2015. Neural Machine Translation by
Jointly Learning to Align and Translate. In Inter-
national Conference on Learning Representations,
ICLR 2015. San Diego, California.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A Neural Probabilistic Lan-
guage Model. J. Mach. Learn. Res. 3:1137–1155.
http://dl.acm.org/citation.cfm?id=944919.944966.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Barry Haddow, Matthias Huck, Chris Hokamp,
Philipp Koehn, Varvara Logacheva, Christof Monz,
Matteo Negri, Matt Post, Carolina Scarton, Lucia
Specia, and Marco Turchi. 2015. Findings of the
2015 workshop on statistical machine translation.
In Proceedings of the Tenth Workshop on Statistical
Machine Translation. Lisbon, Portugal, pages 1–46.
http://aclweb.org/anthology/W15-3001.

Ozan Caglayan, Walid Aransa, Yaxing Wang,
Marc Masana, Mercedes Garcı́a-Martı́nez, Fethi
Bougares, Loı̈c Barrault, and Joost van de Weijer.
2016a. Does multimodality help human and ma-
chine for translation and image captioning? In
Proceedings of the First Conference on Machine
Translation. Berlin, Germany, pages 627–633.
http://www.aclweb.org/anthology/W/W16/W16-
2358.

Ozan Caglayan, Loı̈c Barrault, and Fethi Bougares.
2016b. Multimodal Attention for Neural Ma-
chine Translation. CoRR abs/1609.03976.
http://arxiv.org/abs/1609.03976.

Iacer Calixto, Desmond Elliott, and Stella Frank. 2016.
DCU-UvA Multimodal MT System Report. In
Proceedings of the First Conference on Machine
Translation. Berlin, Germany, pages 634–638.
http://www.aclweb.org/anthology/W/W16/W16-
2359.

Iacer Calixto, Daniel Stein, Evgeny Matusov, Sheila
Castilho, and Andy Way. 2017a. Human Eval-
uation of Multi-modal Neural Machine Transla-
tion: A Case-Study on E-Commerce Listing Ti-
tles. In Proceedings of the Sixth Workshop on Vi-
sion and Language. Valencia, Spain, pages 31–37.
http://www.aclweb.org/anthology/W17-2004.

Iacer Calixto, Daniel Stein, Evgeny Matusov, Pintu
Lohar, Sheila Castilho, and Andy Way. 2017b.
Using Images to Improve Machine-Translating E-
Commerce Product Listings. In Proceedings of the
15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
2, Short Papers. Valencia, Spain, pages 637–643.
http://www.aclweb.org/anthology/E17-2101.

Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014a. On the proper-
ties of neural machine translation: Encoder–decoder

approaches. Syntax, Semantics and Structure in Sta-
tistical Translation. page 103.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014b. Learn-
ing phrase representations using rnn encoder–
decoder for statistical machine translation. In
Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Process-
ing (EMNLP). Doha, Qatar, pages 1724–1734.
http://www.aclweb.org/anthology/D14-1179.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better Hypothesis Testing
for Statistical Machine Translation: Control-
ling for Optimizer Instability. In Proceedings
of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human
Language Technologies: Short Papers - Vol-
ume 2. Portland, Oregon, HLT ’11, pages 176–181.
http://dl.acm.org/citation.cfm?id=2002736.2002774.

Michael Denkowski and Alon Lavie. 2014. Meteor
Universal: Language Specific Translation Evalua-
tion for Any Target Language. In Proceedings of the
EACL 2014 Workshop on Statistical Machine Trans-
lation.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-Task Learning for Mul-
tiple Language Translation. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Beijing, China, pages 1723–
1732. http://www.aclweb.org/anthology/P15-1166.

Desmond Elliott, Stella Frank, and Eva Hasler.
2015. Multi-Language Image Description with
Neural Sequence Models. CoRR abs/1510.04709.
http://arxiv.org/abs/1510.04709.

Desmond Elliott, Stella Frank, Khalil Sima’an,
and Lucia Specia. 2016. Multi30K: Multilin-
gual English-German Image Descriptions. In
Proceedings of the 5th Workshop on Vision
and Language, VL@ACL 2016. Berlin, Ger-
many. http://aclweb.org/anthology/W/W16/W16-
3210.pdf.

Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
2016. Multi-Way, Multilingual Neural Machine
Translation with a Shared Attention Mechanism. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies. San Diego, California, pages 866–875.
http://www.aclweb.org/anthology/N16-1101.

Yarin Gal and Zoubin Ghahramani. 2016. A Theoreti-
cally Grounded Application of Dropout in Recurrent
Neural Networks. In Advances in Neural Informa-
tion Processing Systems, NIPS, Barcelona, Spain,

1922



pages 1019–1027. http://papers.nips.cc/paper/6241-
a-theoretically-grounded-application-of-dropout-in-
recurrent-neural-networks.pdf.

Ross Girshick, Jeff Donahue, Trevor Darrell, and Ji-
tendra Malik. 2014. Rich Feature Hierarchies for
Accurate Object Detection and Semantic Segmen-
tation. In Proceedings of the 2014 IEEE Confer-
ence on Computer Vision and Pattern Recognition.
Washington, DC, USA, CVPR ’14, pages 580–587.
https://doi.org/10.1109/CVPR.2014.81.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Deep residual learning for image recog-
nition. arXiv preprint arXiv:1512.03385 .

Julian Hitschler, Shigehiko Schamoni, and Ste-
fan Riezler. 2016. Multimodal Pivots for Im-
age Caption Translation. In Proceedings of
the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers). Berlin, Germany, pages 2399–2409.
http://www.aclweb.org/anthology/P16-1227.

Po-Yao Huang, Frederick Liu, Sz-Rung Shiang,
Jean Oh, and Chris Dyer. 2016. Attention-based
Multimodal Neural Machine Translation. In
Proceedings of the First Conference on Machine
Translation. Berlin, Germany, pages 639–645.
http://www.aclweb.org/anthology/W/W16/W16-
2360.

Sébastien Jean, Kyunghyun Cho, Roland Memise-
vic, and Yoshua Bengio. 2015. On Using Very
Large Target Vocabulary for Neural Machine Trans-
lation. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Lin-
guistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume
1: Long Papers). Beijing, China, pages 1–10.
http://www.aclweb.org/anthology/P15-1001.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Continuous Translation Models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2013. Seattle,
US., pages 1700–1709.

Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing. De-
troit, Michigan, volume I, pages 181–184.

Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit. AAMT, AAMT, Phuket, Thailand, pages 79–86.
http://mt-archive.info/MTS-2005-Koehn.pdf.

Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondřej
Bojar, Alexandra Constantin, and Evan Herbst.

2007. Moses: Open Source Toolkit for Sta-
tistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the ACL on In-
teractive Poster and Demonstration Sessions.
Prague, Czech Republic, ACL ’07, pages 177–180.
http://dl.acm.org/citation.cfm?id=1557769.1557821.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1. Edmonton, Canada, NAACL ’03, pages 48–
54. https://doi.org/10.3115/1073445.1073462.

Jindřich Libovický, Jindřich Helcl, Marek Tlustý,
Ondřej Bojar, and Pavel Pecina. 2016. CUNI
System for WMT16 Automatic Post-Editing
and Multimodal Translation Tasks. In Pro-
ceedings of the First Conference on Machine
Translation. Berlin, Germany, pages 646–654.
http://www.aclweb.org/anthology/W/W16/W16-
2361.

Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2016. Multi-Task Se-
quence to Sequence Learning. In Proceedings of the
International Conference on Learning Representa-
tions (ICLR), 2016. San Juan, Puerto Rico.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective Approaches to Attention-
based Neural Machine Translation. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing (EMNLP). Lisbon,
Portugal, pages 1412–1421.

Junhua Mao, Wei Xu, Yi Yang, Jiang Wang,
and Alan L. Yuille. 2014. Explain Images
with Multimodal Recurrent Neural Networks.
http://arxiv.org/abs/1410.1090.

Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Pro-
ceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics - Volume
1. Sapporo, Japan, ACL ’03, pages 160–167.
https://doi.org/10.3115/1075096.1075117.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A Method for Au-
tomatic Evaluation of Machine Translation. In
Proceedings of the 40th Annual Meeting on As-
sociation for Computational Linguistics. Philadel-
phia, Pennsylvania, ACL ’02, pages 311–318.
https://doi.org/10.3115/1073083.1073135.

Maja Popović. 2015. chrf: character n-gram f-
score for automatic mt evaluation. In Proceed-
ings of the Tenth Workshop on Statistical Ma-
chine Translation. Lisbon, Portugal, pages 392–395.
http://aclweb.org/anthology/W15-3049.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving Neural Machine Translation

1923



Models with Monolingual Data. In Proceed-
ings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 1:
Long Papers). Berlin, Germany, pages 86–96.
http://www.aclweb.org/anthology/P16-1009.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural Machine Translation of Rare
Words with Subword Units. In Proceedings
of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1:
Long Papers). Berlin, Germany, pages 1715–1725.
http://www.aclweb.org/anthology/P16-1162.

Kashif Shah, Josiah Wang, and Lucia Specia.
2016. SHEF-Multimodal: Grounding Ma-
chine Translation on Images. In Proceed-
ings of the First Conference on Machine
Translation. Berlin, Germany, pages 660–665.
http://www.aclweb.org/anthology/W/W16/W16-
2363.

K. Simonyan and A. Zisserman. 2015. Very deep con-
volutional networks for large-scale image recogni-
tion. In Proceedings of the International Conference
on Learning Representations (ICLR). San Diego,
CA.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas. Cambridge, MA, pages
223–231.

Lucia Specia, Stella Frank, Khalil Sima’an, and
Desmond Elliott. 2016. A Shared Task on Mul-
timodal Machine Translation and Crosslingual Im-
age Description. In Proceedings of the First Con-
ference on Machine Translation, WMT 2016, colo-
cated with ACL 2016. Berlin, Germany, pages 543–
553. http://aclweb.org/anthology/W/W16/W16-
2346.pdf.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to Sequence Learning with Neural Net-
works. In Advances in Neural Information Process-
ing Systems. Montréal, Canada, pages 3104–3112.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiao-
hua Liu, and Hang Li. 2016. Modeling Cov-
erage for Neural Machine Translation. In Pro-
ceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume
1: Long Papers). Berlin, Germany, pages 76–85.
http://www.aclweb.org/anthology/P16-1008.

Subhashini Venugopalan, Marcus Rohrbach, Jef-
frey Donahue, Raymond J. Mooney, Trevor Dar-
rell, and Kate Saenko. 2015. Sequence to
sequence - video to text. In 2015 IEEE
International Conference on Computer Vision,
ICCV 2015. Santiago, Chile, pages 4534–4542.
https://doi.org/10.1109/ICCV.2015.515.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In IEEE Conference on Com-
puter Vision and Pattern Recognition, CVPR 2015.
Boston, Massachusetts, pages 3156–3164.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun
Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. 2015. Show, at-
tend and tell: Neural image caption genera-
tion with visual attention. In Proceedings of
the 32nd International Conference on Machine
Learning (ICML-15). JMLR Workshop and Confer-
ence Proceedings, Lille, France, pages 2048–2057.
http://jmlr.org/proceedings/papers/v37/xuc15.pdf.

Peter Young, Alice Lai, Micah Hodosh, and Julia
Hockenmaier. 2014. From image descriptions to
visual denotations: New similarity metrics for se-
mantic inference over event descriptions. Transac-
tions of the Association for Computational Linguis-
tics 2:67–78.

Matthew D. Zeiler. 2012. ADADELTA: An Adap-
tive Learning Rate Method. CoRR abs/1212.5701.
http://arxiv.org/abs/1212.5701.

Barret Zoph and Kevin Knight. 2016. Multi-Source
Neural Translation. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. San Diego, California, pages
30–34. http://www.aclweb.org/anthology/N16-
1004.

1924


	Doubly-Attentive Decoder for Multi-modal Neural Machine Translation

