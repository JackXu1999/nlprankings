



















































Semantic Clustering and Convolutional Neural Network for Short Text Categorization


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 352–357,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Semantic Clustering and Convolutional Neural Network
for Short Text Categorization

Peng Wang, Jiaming Xu, Bo Xu, Cheng-Lin Liu, Heng Zhang
Fangyuan Wang, Hongwei Hao

{peng.wang, jiaming.xu, boxu}@ia.ac.cn, liucl@nlpr.ia.ac.cn
{heng.zhang, fangyuan.wang, hongwei.hao}@ia.ac.cn

Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, P.R. China

Abstract

Short texts usually encounter data sparsi-
ty and ambiguity problems in representa-
tions for their lack of context. In this pa-
per, we propose a novel method to mod-
el short texts based on semantic clustering
and convolutional neural network. Partic-
ularly, we first discover semantic cliques
in embedding spaces by a fast clustering
algorithm. Then, multi-scale semantic u-
nits are detected under the supervision of
semantic cliques, which introduce useful
external knowledge for short texts. These
meaningful semantic units are combined
and fed into convolutional layer, followed
by max-pooling operation. Experimental
results on two open benchmarks validate
the effectiveness of the proposed method.

1 Introduction

Conventional texts mining methods based on bag-
of-words (BoW) easily encounter data sparsi-
ty and ambiguity problems in short text model-
ing (Chen et al., 2011), which ignore semantic re-
lations between words (Sriram et al., 2010). How
to acquire effective representation for short tex-
t has been an active research issue (Chen et al.,
2011; Phan et al., 2008).

In order to overcome the weakness of BoW, re-
searchers have proposed to expand the represen-
tation of short text using latent semantics, where
the words are mapped to distributional representa-
tions by Latent Dirichlet Allocation (LDA) (Blei
et al., 2003) and its extensions. Phan et al. (2008)
presented a general framework to expand the short
and sparse text by appending topic names discov-
ered using LDA. Yan et al. (2013) presented a vari-
ant of LDA, dubbed Biterm Topic Model (BTM),
especially for short text modeling to alleviate the
problem of sparsity. However, the methods dis-
cussed above still view a piece of text as BoW.

Therefore, they are not effective in capturing fine-
grained semantic information for short texts mod-
eling.

Recently, neural network related methods have
received much attention, including learning word
embeddings (Bengio et al., 2003; Mikolov et al.,
2013a) and performing semantic composition to
obtain phrase or sentence level representation-
s (Collobert et al., 2011; Le and Mikolov, 2014).
For learning word embedding, the training objec-
tive of continuous Skip-gram model (Mikolov et
al., 2013b) is to predict its context. Thus, the co-
occurrence information can be effectively used to
describe a word, and each component of word em-
bedding might have a semantic or grammatical in-
terpretation.

In embedding spaces, semantically close word-
s are likely to cluster together and form semantic
cliques (or word embedding cliques). Moreover,
the embedding spaces exhibit linear structure that
the word vectors can be meaningfully combined
using simple additive operation (Mikolov et al.,
2013b), for example:

vec (Germany) +vec (Capital)≈vec (Berlin) (1)
vec(Athlete)+vec (Football)≈vec (Football P layer)

(2)

The above examples indicate that the additive
composition can often produce meaningful result-
s. In Equation (1), the token ′Berlin′ can be viewed
that it has an embedding offset vec (Capital) to the
token ′Germany′ in embedding spaces. Further-
more, the embedding offsets represent the syntac-
tical and semantic relations among words.

In this paper, we propose a method to mod-
el short texts using semantic clustering and con-
volutional neural network (CNN). Firstly, the fast
clustering algorithm (Rodriguez and Laio, 2014),
based on searching density peaks, is utilized to
cluster word embeddings and discover semantic
cliques, as shown in Figure 1. Then semantic com-
position is performed over n-gram embeddings to

352



0 20 40 60 80
0

20

40

60

80

100
Decision Graph

ρ

δ
world

million

may

old
killed monthssystem

games

saying

capital

member

troops

12

december

human

media

germanper

race

low

2004

iraqi

medical

bandmexico

jamesstocks

am

angeles

−150 −100 −50 0 50 100 150 200
−150

−100

−50

0

50

100

150
Word Embeddings Clustering

X

Y

governmentworld
state
usu.s.unitedstatescountryamericaninternational
countrieseuropeanbritish
federal
america
administrationnationsnationeuropeaustralia
eubritaincanadaaustralian
society

$millionbilliondollars
annual

the,.ofandina
’sforoniswaswithasit
byfroman
has
arehavebutwere
this
they
had
which

theiror
itsoneafternew
beenalso
moreabout

upwhen
thereall

out

otherpeoplethan
over
some
timeonly
_sothem

before
may
many
while
now
made
like

between
just

under
such
then
being
downbackoff
well
still
both
even

part
thosethese

home
lateranothercalled
waymuchhere
until
howeversameknowneachbased
among
timestookcame

fewtooleast
pasthalf
everybigfartodayalthough
againclosewentpoint
alreadythough
away

others
within

makinglessoncegiven
different
whosehavingseenfullspecial
nearlyenoughalmost

together
hard
gavesoon
recentlycourse
instead

saw

americans
currently
broughtturned

either
present

hehiswho
sheherhimoldlifechildrenhistory
man
youngagehimselfliving
womanchildlives

killed
air

deathdied
attackfireattacks
weapons
dead

usedusesystemusing
technologyinternetcomputer

gamegames

saidtoldforeignofficialsayingstatementspokesmanagencyministryreporters

citysouthnorth
area
westeastcentraltowncapitalnearregionsouthern
community
northernwestern
siteareas
provincevillagelocatedbordereastern

memberscouncilmemberseniorindependent

policemilitarysecurityarmyforcesforcetroopssoldiersu.n.un
staffmission

"():;1102/3201530411125186257814131617924192221

marchjunejulyaprilseptemberjanuarydecemberoctobernovemberaugustfebruary

humanfood

newspress
televisionmedia
station

storybook

radio
publisheddaily
tv
newspaper
wrote

network
interview
written

germanygerman

%totalper1005040

twothreefourfivesix
final
seven
round
eight
matchrace
nine

200820062007201020002009200420052001200320022011199819962012199919941997

iraqisraelisraelipalestinianiraqiafghanistannato

car
hospitaltestmedical

musicalbumbandsong

mexicoitaly

johngeorgedavidmichaelmarkst.jamesrobertpaul

percent
market
bankfinancial
stockpricesrose
salesworkers
fellsharesshareindex
priceexchangeinvestors
investment
marketstradingstocksbuy
banks

−
’’‘‘iwe
’−−
n’tyoudowhatdidget?
my
howsaysaysgood

goinggo

best
’rethinkmewantnever
little

know
yourgot...
better
’ve’maskedlot
real
reallysomethingthings
everlook

job
alwaysbelievewhywanteddone
’ll
thought

!
love
ca
looking
thinggetting
letpersonam
doing
kind

‘
nothingquestionfeel

yorkwashington
de districtcountysancaliforniatexaslosangelesflorida

Figure 1: Fast clustering based on density peaks of embeddings

detect candidate Semantic Units1(abbr. to SUs)
appearing in short texts. The part of candidate
SUs meeting the preset threshold are chosen to
constitute semantic matrices, which are used as in-
put for the CNN, otherwise dropout. In this stage,
semantic cliques are used as supervision informa-
tion, which guarantee meaningful SUs can be ex-
tracted.

The motivation of our work is to introduce extra
knowledge by pre-trained word embeddings and
fully exploit the contextual information of short
texts to improve their representations. The main
contributions include: (1) semantic cliques are
discovered using fast clustering method based on
searching density peaks; (2) for fine-tuning multi-
scale SUs, the semantic cliques are used to super-
vise the selection stage.

The remainder of this paper is organized as fol-
lows. The related works are briefly reviewed in
Section 2. Section 3 introduces the semantic clus-
tering based on fast searching density peaks. Sec-
tion 4 describes the architecture of the proposed
method. Section 5 demonstrates the effectiveness
of our method with experiments. Finally, conclud-
ing remarks are offered in Section 6.

2 Related Works

Traditional statistics-based methods usually fail to
achieve satisfactory performance for short texts
classification due to their sparsity of representa-
tions (Sriram et al., 2010). Based on external
Wikipedia corpus, Phan et al. (2008) proposed a
method to discover hidden topics using LDA and

1Semantic units are defined as n-grams which have domi-
nant meaning of text. With n varying, multi-scale contextual
information can be exploited.

expand short texts. Chen et al. (2011) proved that
leveraging topics at multiple granularity can mod-
el short texts more precisely.

Neural networks have been used to model lan-
guages, and the word embeddings can be learned
simultaneously (Mnih and Teh, 2012). Mikolov et
al. (2013b) introduced the continuous Skip-gram
model that is an efficient method for learning high
quality word embeddings from large-scale un-
structured text data. Recently, various pre-trained
word embeddings are publicly available, and many
composition-based methods are proposed to in-
duce the semantic representation of texts. Le and
Mikolov (2014) presented the Paragraph Vector al-
gorithm to learn a fixed-size feature representation
for documents.

Kalchbrenner et al. (2014) introduced the Dy-
namic Convolutional Neural Network (DCNN) for
modeling sentences. Their work is closely relat-
ed to our study in that k-max pooling is utilized
to capture global feature vector and do not rely
on parse tree. Kim (2014) proposed a simple im-
provement to the convolutional architecture that t-
wo input channels are used to allow the employ-
ment of task-specific and static word embeddings
simultaneously.

Zeng et al. (2014) developed a deep convo-
lutional neural network (DNN) to extract lexical
and sentence level features, which are concate-
nated and fed into the softmax classifier. Socher
et al. (2013) proposed the Recursive Neural Net-
work (RNN) that has been proven to be efficient
in terms of constructing sentences representation-
s. In order to reduce the overfitting of neural net-
work especially trained on small data set, Hin-
ton et al. (2012) used random dropout to prevent

353



complex co-adaptations. To exploit more struc-
ture information of text, based on CNN and direc-
t embedding of small text regions, an alternative
mechanism for effective use of word order for text
categorization was proposed (Johnson and Zhang,
2014).

Although the popular methods can capture
high-order information and word relations to pro-
duce complex features, they cannot guarantee the
classification performance for very short texts. In
this paper, we design a method to exploit more
contextual information for short text classification
using semantic clustering and CNN.

3 Semantic Clustering

Since the neighbors of each word are semanti-
cally related in embedding space (Mikolov et al.,
2013b), clustering methods (Rodriguez and Laio,
2014) can be used to discover semantic cliques.
For implementation, two quantities of data point i
are computed, include: local density ρi, defined as
follows,

ρi =
∑
j

χ(dij − dc) (3)

where dij is the distance between data points, dc
is a cutoff distance. Furthermore, distance δi from
points of higher density is measured by,

δi =


min

j:ρj>ρi
(dij) , if ρi < ρmax

max
j

(dij) , otherwise
(4)

An example of semantic clustering is illustrat-
ed in Figure 1. The decision graph shows the two
quantities ρ and δ of each word embedding. Ac-
cording to the definitions above, these word em-
beddings with large ρ and δ simultaneously are
chosen as cluster centers, which are labeled using
the corresponding words.

4 Proposed Architecture

As shown in Figure 2, the proposed architecture
use well pre-trained word embeddings to initialize
the lookup table, and higher levels extract more
complexity features.

For short text S = {w1, w2, · · · , wN}, its project-
ed matrix PM ∈ Rd×N is obtained by table look-
ing up in the first layer, where d is the dimension
of word embedding. The second layer is used to
obtain multi-scale SUs to constitute the semantic

... ...

Projected

Sentence

Matrix

Convolution

Multi-scale

Semantic

Units

K-Max Pooling

Softmax Decision

The cat sat on the red mat

Figure 2: Architecture for short text modeling

matrices, which are combined and fed into convo-
lutional layer, followed by k-max pooling opera-
tion. Finally, a softmax function is employed as
classifier.

4.1 Detection for Multi-scale SUs
Methods for modeling short text S mainly have
problem that its semantic meaning is determined
by a few of key-phrases, however, these meaning-
ful phrases may appear at any position of S. Thus,
simply combining all words of S may introduce
unnecessary divergence and hurt the overall se-
mantic representation. Therefore, the detection for
SUs are useful, which capture salient local infor-
mation, as shown in Figure 2.

In particular, to obtain the representations of
candidate SUs, multiple windows with variable
width over word embeddings are used to perfor-
m element-wise additive composition, as follows:

[SU1,SU2, · · · ,SUN−m+1] = PM⊗Ewin (5)

where, Ewin ∈ Rd×m is a window matrix with all
weights equal to one, and

SUi=

|PMwin,i|∑
j=1

PMwin,ij (6)

PMwin,ij is the jth column from the sub-matrix
PMwin,i, which is windowed on projected matrix
PM by Ewin with the ith times sliding. m is the
width of the window matrix Ewin. With m vary-
ing, multi-scale contextual information can be ex-
ploited, which is helpful to reduce the impact of
ambiguous words.

354



The meaningful SUs are assumed that they have
one close neighbor at least in embedding space.
Thus, we compute Euclidean distance between
candidate SUs and semantic cliques. If the dis-
tance between candidate SUs and nearest word
embeddings are smaller than the preset threshold,
the candidate SUs are selected to constitute the se-
mantic matrices, otherwise dropout.

4.2 Convolution Layer

In our network, the convolutional layer is used to
extract local features. Kernel matrices k with cer-
tain width n are utilized to calculate convolution
with the input matrices M, as Equation (7).

C = [c1, c2, · · · , cd/2]T = KT ⊗ M (7)

where,
K = [k1,k2, · · · ,kd/2] (8)

M = [Mwin1 ,M
win
2 , · · · ,Mwind/2 ] (9)

cji = ki · (Mwin,ji )T (10)

The cji is generated from the jth n-gram in M.
Equation (7) produce the feature maps of convolu-
tional layer.

4.3 K-Max Pooling

This operator is a non-linear sub-sampling func-
tion that returns the sub-sequence of K maximum
values (LeCun et al., 1998), which is used to cap-
ture the most relevant global features with fixed-
length. Then, tangent transformation over the re-
sults of K-max pooling is performed, the output
of which is concatenated to used as representation
for the input short texts.

4.4 Network Training

The last layer is fully connected, where a soft-
max classifier is applied to predict the proba-
bility distribution over categories. The network
is trained with the objective that minimizes the
cross-entropy of the predicted distributions and the
actual distributions (Turian et al., 2010),

J(θ) = −1
t

∑t
i=1

log p(c†|xi, θ) + α∥θ∥2 (11)

where t is number of training examples x, and θ is
the parameters set which comprises the kernels of
weights used in convolutional layer and the con-
nective weights from the fully connected layer.

Embedding Senna2 GloVe3 Word2Vec4

Corpus Wikipedia Wikipedia Google News
Dimension 50 50 300
|V ocab.| 130,000 400,000 3,000,000

Table 1: Details of word embeddings

Methods
Google

TRECSnippets

Semantic-CNN
Senna 83.6 96.4
GloVe 84.4 97.2

Word2Vec 85.1 95.6
DCNN – 93(Kalchbrenner et al,2014)
SVMS – 95(Silva et al., 2011)

CNN-TwoChannel – 93.6(Kim, 2014)
LDA+MaxEnt 82.7 –(Phan et al., 2008)

Multi-Topics+MaxEnt 84.17 –(Chen et al., 2011)

Table 2: The classification accuracy of proposed
method against other models

5 Experiments

5.1 Datasets
Experiments are conducted on two benchmarks:
Google Snippets (Phan et al., 2008) and TREC (Li
and Roth, 2002).

Google Snippets This dataset consists of
10,060 training snippets and 2,280 test snippets
from 8 categories. On average, each snippet has
18.07 words.

TREC The TREC questions dataset contains 6
different question types. The training dataset con-
sists of 5,452 labeled questions whereas the test
dataset consists of 500 questions.

5.2 Experimental Setup
Three pre-trained word embeddings for initializ-
ing the lookup table are summarized in Table 1.
To discover semantic cliques, we take ρmin = 16
and δmin = 1.54. Through our experiments, 6 k-
ernel matrices in convolutional layer, K = 3 for
max pooling, and mini-batch size of 100 are used.

5.3 Results and Discussions
5.3.1 Comparison with state-of-the-art

methods
As shown in Table 2, we introduce 5 popular meth-
ods as baselines, and the details are described:

DCNN Kalchbrenner et al. (2014) proposed D-
CNN for sentence modeling with dynamic k-max
pooling.

355



0 2 4 6
0.8

0.81

0.82

0.83

0.84

0.85

Number of window matrices

A
cc

u
ra

cy

Google Snippets

0 2 4 6
0.93

0.94

0.95

0.96

0.97

TREC Senna

GloVe

Word2Vec

Figure 3: Number of windows for multi-scale SUs

3 4 5 6
0.82

0.825

0.83

0.835

0.84

Euclidean Distance

A
cc

u
ra

cy

Senna

0.5 1 1.5 2

0.82

0.83

0.84

0.85
GloVe

1 2 3 4
0.83

0.835

0.84

0.845

0.85

Word2Vec

Figure 4: Influence of threshold in SUs detection

SVMs Parser, wh word, head word, POS, hy-
pernyms, and 60 hand-coded rules were used as
features to train SVMs (Silva et al., 2011).

CNN-TwoChannel An improved CNN that al-
lows task-specific and static word embeddings are
used simultaneously (Kim, 2014).

LDA+MaxEnt LDA was used to discover hid-
den topics for expanding short texts (Phan et al.,
2008).

Multi-topics+MaxEnt Multiple granularity
topics from LDA were utilized to model short
texts (Chen et al., 2011).

For valid comparisons, we respectively initial-
ize the lookup table with the word embeddings in
Table 1, and three experiments are conducted for
each benchmark. As a whole, our method achieves
the best performance, especially for TREC with
97.2% when the GloVe word embedding is em-
ployed. For Google snippets, our method achieves
the highest result of 85.1% corresponding to the
word embedding induced by Word2Vec.

5.3.2 Effect of Hyper-parameters
In Figure 2, for obtaining SUs with multi-scale,
multiple window matrices with increasing width
m are used. With respect to the variable m, the re-

2http://ml.nec-labs.com/senna/
3http://nlp.stanford.edu/projects/glove/
4https://code.google.com/p/word2vec/

sults are shown in Figure 3. We find small size of
window may result in loss of critical information,
however, the window with large size may intro-
duce noise.

Figure 4 demonstrate how preset threshold d
impact our method over benchmark Goggle snip-
pets. We can draw a conclusion that when d is too
small, only a few of SUs can be detected, where-
as meaningless features are enrolled. The optimal
threshold d can be chosen by cross-validation.

The impacts of other hyper-parameters like the
number and size of the feature detectors in convo-
lutional layer, and the variable k in k-max pooling
layer are beyond the scope of this paper.

6 Conclusion

This paper proposes a novel semantic hierarchical
model for short text classification. The model us-
es pre-trained word embeddings to introduce extra
knowledge, and multi-scale SUs in short texts are
detected.

Acknowledgement

This work is supported by the National Natural
Science Foundation of China (No. 61203281, No.
61303172, No. 61403385) and Hundred Talents
Program of Chinese Academy of Sciences (No.
Y3S4011D31).

356



References
Ainur Yessenalina and Claire Cardie. Composition-

al matrix-space models for sentiment analysis. In
EMNLP, pages 172–182. Association for Computa-
tional Linguistics, 2011.

Alex Rodriguez and Alessandro Laio. Clustering by
fast search and find of density peaks. Science,
344(6191):1492–1496, 2014.

Andriy Mnih and Yee Whye Teh. A fast and simple
algorithm for training neural probabilistic language
models. arXiv preprint arXiv:1206.6426, 2012.

Bharath Sriram, Dave Fuhry, Engin Demir, Hakan Fer-
hatosmanoglu, and Murat Demirbas. Short text clas-
sification in twitter to improve information filtering.
In SIGIR, pages 841–842. ACM, 2010.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. Relation classification via convolu-
tional deep neural network. In Proceedings of COL-
ING, pages 2335–2344, 2014.

David M Blei, Andrew Y Ng, and Michael I Jordan.
Latent dirichlet allocation. the Journal of machine
Learning research, 3:993–1022, 2003.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint arX-
iv:1207.0580, 2012.

Jeff Mitchell and Mirella Lapata. Composition in dis-
tributional models of semantics. Cognitive science,
34(8):1388–1429, 2010.

John Duchi, Elad Hazan, and Yoram Singer. Adap-
tive subgradient methods for online learning and
stochastic optimization. The Journal of Machine
Learning Research, 12:2121–2159, 2011.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word
representations: a simple and general method for
semi-supervised learning. In ACL, pages 384–394.
Association for Computational Linguistics, 2010.

Mehran Sahami and Timothy D Heilman. A web-based
kernel function for measuring the similarity of short
text snippets. In Proceedings of the 15th interna-
tional conference on World Wide Web, pages 377–
386. AcM, 2006.

Mengen Chen, Xiaoming Jin, and Dou Shen. Short
text classification improved by learning multi-
granularity topics. In IJCAI, pages 1776–1781.
Citeseer, 2011.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. A convolutional neural network for modelling
sentences. arXiv preprint arXiv:1404.2188, 2014.

Quoc V Le and Tomas Mikolov. Distributed represen-
tations of sentences and documents. arXiv preprint
arXiv:1405.4053, 2014.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for
semantic compositionality over a sentiment tree-
bank. In EMNLP, volume 1631, page 1642. Cite-
seer, 2013.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Nat-
ural language processing (almost) from scratch. The
Journal of Machine Learning Research, 12:2493–
2537, 2011.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. Efficient estimation of word representation-
s in vector space. arXiv preprint arXiv:1301.3781,
2013.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. Distributed representations of
words and phrases and their compositionality. In
Advances in Neural Information Processing System-
s, pages 3111–3119, 2013.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
Linguistic regularities in continuous space word rep-
resentations. In HLT-NAACL, pages 746–751, 2013.

Xiaohui Yan, Jiafeng Guo, Yanyan Lan, and Xueqi
Cheng. A biterm topic model for short texts. In
WWW, pages 1445–1456. International World Wide
Web Conferences Steering Committee, 2013.

Xin Li and Dan Roth. Learning question classifiers.
In Proceedings of the 19th international conference
on Computational linguistics-Volume 1, pages 1–7.
Association for Computational Linguistics, 2002.

Xuan-Hieu Phan, Le-Minh Nguyen, and Susumu
Horiguchi. Learning to classify short and sparse
text & web with hidden topics from large-scale data
collections. In Proceedings of the 17th internation-
al conference on World Wide Web, pages 91–100.
ACM, 2008.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to doc-
ument recognition. Proceedings of the IEEE,
86(11):2278–2324, 1998.

Yoon Kim. Convolutional neural networks for sentence
classification. arXiv preprint arXiv:1408.5882,
2014.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. A neural probabilistic language
model. The Journal of Machine Learning Research,
3:1137–1155, 2003.

Joao Silva, Luı́sa Coheur, Ana Cristina Mendes, and
Andreas Wichert. From symbolic to sub-symbolic
information in question classification. Artificial In-
telligence Review, 35(2):137–154, 2011.

Rie Johnson and Tong Zhang. Effective use of word or-
der for text categorization with convolutional neural
networks. arXiv preprint arXiv:1412.1058, 2014.

357


