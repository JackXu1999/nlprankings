



















































From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic Knowledge from Textbooks to Solve Geometry Problems


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 773–784
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

From Textbooks to Knowledge: A Case Study in Harvesting Axiomatic
Knowledge from Textbooks to Solve Geometry Problems

Mrinmaya Sachan Avinava Dubey Eric P. Xing
School of Computer Science
Carnegie Mellon University

{mrinmays, akdubey, epxing}@cs.cmu.edu

Abstract

Textbooks are rich sources of knowledge.
Harvesting knowledge from textbooks is a
key challenge in many educational appli-
cations. In this paper, we present an ap-
proach to obtain axiomatic knowledge of
geometry in the form of horn-clause rules
from math textbooks. The approach uses
rich contextual and typographical features
extracted from the textbooks. It also lever-
ages the redundancy and shared ordering
of axioms across multiple textbooks to ac-
curately harvest axioms. These axioms
are then parsed into horn-clause rules that
are used to improve the state-of-the-art in
solving geometry problems.

1 Introduction

Recently, researchers have proposed standardized
tests as “drivers for progress in AI” (Clark and
Etzioni, 2016). There is a growing body of
work in solving standardized tests such as reading
comprehensions (Richardson et al., 2013; Sachan
et al., 2015, inter alia), science question answering
(Schoenick et al., 2016; Sachan et al., 2016, in-
ter alia), algebra word problems (Kushman et al.,
2014, inter alia), geometry problems (Seo et al.,
2015), pre-university entrance exams (Fujita et al.,
2014), etc. A major challenge in building these
solvers is the lack of subject knowledge. For ex-
ample, geometry tests require knowledge of ge-
ometry axioms and pre-university exams require
knowledge of laws of physics, chemistry, etc.

In this paper, we present an automatic approach
that can (a) harvest such subject knowledge from
textbooks, and (b) parse the extracted knowledge
to structured programs that the solvers can use.
Unlike information extraction systems trained on
domains such as web documents (Chang et al.,

Figure 1: An excerpt of a textbook from our dataset that introduces the
Pythagoras theorem. The textbook has a lot of typographical features that can
be used to harvest this theorem: The textbook explicitly labels it as a “the-
orem”; there is a colored bounding box around it; an equation writes down
the rule and there is a supporting figure. Our models leverages such rich con-
textual and typographical information (when available) to accurately harvest
axioms and then parses them to horn-clause rules. The horn-clause rule de-
rived by our approach for the Pythagoras theorem is: isTriangle(ABC)∧
perpendicular(AC,BC) =⇒ BC2 + AC2 = AB2.

2003; Etzioni et al., 2004, inter alia), learning an
information extraction system that can extract ax-
iomatic knowledge from textbooks is challenging
because of the small amount of in-domain labeled
data available for these tasks. We tackle this chal-
lenge by (a) leveraging the redundancy and shared
ordering of axiom mentions across multiple text-
books1, and (b) utilizing rich contextual and typo-
graphical features2 from textbooks to effectively
extract and parse axioms. Finally, we also provide
an approach to parse the extracted axiom men-
tions from various textbooks and reconcile them
to achieve the best program for each axiom.

As a case study, we use our approach to har-
vest axiomatic knowledge of geometry from math
textbooks, and use this knowledge to improve the
state-of-the-art system for solving SAT style ge-
ometry problems. Seo et al. (2015) recently pre-
sented GEOS, an automated end-to-end system
that solves SAT style geometry questions such as
the one shown in Figure 2. GEOS derives a logi-
cal expression that represents the meaning of the

1The same axiom can be potentially mentioned in a num-
ber of textbooks in different ways. All textbooks typically
introduce axioms in roughly the same order – for example,
pythagorous theorem would typically be introduced after in-
troducing the notion of a right angled triangle.

2Textbooks contain rich context and typographical infor-
mation (see Figure 1 for an illustrative example). We use this
rich information as features in our model.

773



Text Description:

measure(   MAO, 30o)
isCircle(O)

radius(O, 4 cm)
?x

Diagram:

liesOn( A, circle O), liesOn( B, circle O), 
liesOn( C, circle O), liesOn( D, circle O)

isLine(AB), isLine(BC), isLine(CA), isLine(BD), isLine(DA)
isTriangle(ABC), isTriangle(ABD), isTriangle(AOM)

measure(   ADB, x), measure(   MAO, 30o)
measure(   AMO, 90o)

…

Figure 2: An example SAT style geometry problem with the question text,
corresponding diagram and (optionally) answer candidates. Below: A logical
expression that represents the meaning of the text description and the diagram
in the problem. GEOS derives a weighted logical expression where each pred-
icates also carries a weighted score but we do not show them here for clarity.

text description and the diagram (also shown in
Figure 2), and then solves the geometry question
by checking the satisfiablity of the derived logical
expression. While this solver has its basis in co-
ordinate geometry and indeed works, it has some
key issues: GEOS requires an explicit mapping of
each predicate into a set of constraints over point
coordinates3. These constraints can be non-trivial
to write, requiring significant manual engineering.
As a result, GEOS’s constraint set is incomplete
and it cannot solve a number of SAT style geome-
try questions. Furthermore, this solver is not in-
terpretable. As our user studies show, it is not
natural for a student to understand the solution of
these geometry questions in terms of satisfiability
of constraints over coordinates. A more natural
way for students to understand and reason about
these questions is through deductive reasoning us-
ing axioms of geometry4.

We use our model to extract and parse axiomatic
knowledge from a novel dataset of 20 publicly
available math textbooks. We use this structured
axiomatic knowledge to build a new axiomatic
solver that performs logical inference to solve ge-

3For example, the predicate isPerpendicular(AB, CD) is
mapped to the constraint yB−yA

xB−xA ×
yD−yC
xD−xC = −1.

4For example, the deductive reasoning required to solve
the question in Figure 2 is: (1) Use the axiom that the sum of
interior angles of a triangle is 180◦and the fact that ∠AMO
is 90◦to conclude that ∠MOA is 60◦. (2)4MOA ∼4MOB
(using a similar triangle axiom) and then, ∠MOB = ∠MOA
= 60◦(using the axiom that corresponding angles of similar
triangles are equal). (3) Use angle sum rule to conclude that
∠AOB = ∠MOB + ∠MOA = 120◦. (4) Use the axiom that the
angle subtended by an arc of a circle at the centre is double
the angle subtended by it at any point on the circle to conclude
that ∠ADB = 0.5×∠AOB = 60◦.

ometry problems. Our axiomatic solver outper-
forms GEOS on all existing test sets introduced in
Seo et al. (2015) as well as a new test set of geom-
etry questions collected from these textbooks. We
also performed user studies on a number of school
students studying geometry who found that our
axiomatic solver is more interpretable and useful
compared to GEOS.

2 Background: GEOS

Our work reuses GEOS to parse the question text
and diagram into its formal problem description
as shown in Figure 2. GEOS parses the ques-
tion text and the diagram to a formal problem de-
scription. GEOS uses a logical formula, a first-
order logic expression that includes known num-
bers or geometrical entities (e.g. 4 cm) as con-
stants, unknown numbers or geometrical entities
(e.g. O) as variables, geometric or arithmetic re-
lations (e.g. isLine, isTriangle) as predicates and
properties of geometrical entities (e.g. measure,
liesOn) as functions.

This is done by learning a set of relations that
potentially correspond to the question text (or the
diagram) along with a confidence score. For dia-
gram parsing, GEOS uses a publicly available di-
agram parser for geometry problems (Seo et al.,
2014). For text parsing, GEOS takes a multi-stage
approach, which maps words or phrases in the text
to their corresponding concepts, and then identi-
fies relations between identified concepts. Given
this formal problem description, GEOS use a nu-
merical method to check the satisfiablity of literals
by defining a relaxed indicator function for each
literal. These indicator functions are manually en-
gineered for every predicate. Since this is a cum-
bersome process, GEOS has an incomplete map-
ping of literals to indicator functions.

3 Set up for the Axiomatic Solver

In this work, we replace the numerical solver of
GEOS with an axiomatic solver. We extract ax-
iomatic knowledge from textbooks and parse them
into horn clause rules. Then we build an ax-
iomatic solver that performs logical inference with
these horn clause rules and the formal problem de-
scription. A sample logical program (in prolog
notation) that solves the problem in Figure 2 is
given in Figure 3. The logical program has a set
of declarations from the GEOS text and diagram
parsers which describe the problem specification

774



	

sort	point	=	{A,	B,	C,	D,	O,	M}	
sort	line	=	{AB,	BC,	CA,	BD,	DA,	OA,	OM}	//Symmetrically	define	BA,	CB,	…	
sort	angle	=	{ABC,	BCA,	CAB,	ABD,	BDA,	DAB,	AMO,	MOA,	OAM,	BMO}	//Symmetrically	define	CBA,	ACB,	…	
sort	triangle	=	{ABC,	ABD,	AMO}	//Symmetrically	define	CBA,	ACB,	…	
sort	circle	=	{O}	
	
0.4	perpendicular(OM,	AB)	
0.8	measure(ADB,	x)	
0.9	liesOn(A,	O)	
0.9	liesOn(B,	O)	
0.9	liesOn(C,	O)	
0.9	liesOn(D,	O)	
0.9	liesOn(M,	AB)	
0.9	liesInInterior(M,	AOB)	
	
0.9	measure(OAM,	30)	
0.9	measure(radius(O),	4	cm)	
0.9	query(x,	_)	
	
	
0.8	measure(ABC,	90.0)	:-	perpendicular(AB,	CD),	liesOn(B,	CD)	
0.8	measure(XAC,	180-t)	:-	liesOn(A,	BC),	measure(XAB,	t)	
0.7	equals(length(AX),	length(XB))	:-	liesOn(A,	O),	liesOn(B,	O),	perpendicular(OX,	AB),	liesOn(X,	AB)	
0.7	similar(ABC,	DEF)	:-	equals(length(BC),	length(EF)),	equals(measure(ABC),	measure(DEF)),	

equals(measure(BCA),	measure(EFD))	//	ASA	rule.	Similar	rules	for	SAS,	SSS,	RHS	rules	of	similarity	
0.7	equals(measure(CAB),	measure(FED))	:-	similar(ABC,	DEF)	//	Similar	rules	for	other	corresponding	angles	
0.7	equals(measure(ABC),	u+v))	:-	equals(measure(ABD),	u)),	equals(measure(DBC),	v)),	liesInInterior(D,	ABC)	
0.6	equals(measure(ADB),	t/2)	:-	equals(measure(AOB),	t),	liesOn(A,	O),	liesOn(B,	O)		

	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	

1	
2	
3	
4	
	
	

5	
6	
7	

D
atastructures	

D
iagram

	Parse		
Text	Parse		

Axiom
atic	Rules	

Figure 3: A sample logical program (in prolog style) that solves the prob-
lem in Figure 2. The program consists of a set of data structure declarations
that correspond to types in the prolog program, a set of declarations from the
diagram and text parse and a subset of the geometry axioms written as horn
clause rules. The axioms are used as the underlying theory with the aforemen-
tioned declarations to yield the solution upon logical inference. Normalized
confidence weights from the diagram, text and axiom parses are used as proba-
bilities. For readers understanding, we list the axioms in the order (1 to 7) they
are used to solve the problem. However, this ordering is not required. Other
(less probable) declarations and axiom rules are not shown here for clarity but
they can be assumed to be present.

and the parsed horn clause rules describe the un-
derlying theory. Normalized confidence scores
from question text, diagram and axiom parsing
models are used as probabilities in the program.
Next, we describe how we harvest structured ax-
iomatic knowledge from textbooks.

4 Harvesting Axiomatic Knowledge

We present a structured prediction model that
identifies axioms in textbooks and then parses
them. Since harvesting axioms from a single text-
book is a very hard problem, we use multiple text-
books and leverage the redundancy of information
to accurately extract and parse axioms. We first
define a joint model that identifies axiom mentions
in each textbook and aligns repeated mentions of
the same axiom across textbooks. Then, given a
set of axioms (with possibly, multiple mentions of
each axiom), we define a parsing model that maps
each axiom to a horn clause rule by utilizing the
various mentions of the axiom.

Given a set of textbooks B in machine readable
form (XML in our experiments), we extract chap-
ters relevant for geometry in each of them to ob-
tain a sequence of sentences (with associated ty-
pographical information) from each textbook. Let
Sb = {s(b)0 , s(b)1 , . . . s(b)|Sb|} denote the sequence of
sentences in textbook b. |Sb| denotes the number
of sentences in textbook b.

4.1 Axiom Identification and Alignment

We decompose the problem of extracting axioms
from textbooks into two tractable sub-problems:
(a) identification of axiom mentions in each text-
book using a sequence labeling approach, and (b)
aligning repeated mentions of the same axiom
across textbooks. Then, we combine the learned
models for these sub-problems into a joint opti-
mization framework that simultaneously learns to
identify and align axiom mentions. Joint modeling
of the axiom identification and alignment is neces-
sary as both sub-problems can help each other.

4.1.1 Axiom Identification
Linear-chain CRF formulation (Lafferty et al.,
2001) can be used for the subproblem of axiom
identification. Given {Sb|b ∈ B}, the model labels
each sentence s(b)i as Before, Inside or Outside an
axiom. Hereon, a contiguous block of sentences
labeled B or I will be considered as an axiom
mention. Let T = {B, I,O} denote the tag set.
Let y(b)i be the tag assigned to s

(b)
i and Yb be the

tag sequence assigned to Sb. The CRF defines:

p(Yb|Sb;θθθ) ∝
|Sb|∏
k=1

exp

( ∑
i,j∈T

θθθTijfij(y
(b)
k−1, y

(b)
k ,Sb)

)
We find the parameters θθθ using maximum-
likelihood estimation with L2 regularization:
θθθ∗ = arg maxθθθ

∑
b∈B

log p(Yb|Sb;θθθ)− λ||θθθ||22
We use L-BFGS to optimize the objective and
Viterbi decoding for inference.
Features: Features f look at a pair of adjacent
tags y(b)k−1, y

(b)
k , the input sequence Sb, and where

we are in the sequence. The features (listed in Ta-
ble 1) include various content based features en-
coding various notions of similarity between pairs
of sentences as well as various typographical fea-
tures such as whether the sentences are annotated
as an axiom (or theorem or corollary) in the text-
book, contain equations, diagrams, text that is bold
or italicized, are in the same node of the xml hier-
archy, are contained in a bounding box, etc.

Some extracted axiom mentions contain point-
ers to a diagram eg. “Figure 2.1”. We consider the
diagram to be a part of the axiom mention.

4.1.2 Axiom Alignment
Next, we leverage the redundancy of information
and the relatively fixed ordering of axioms in var-
ious textbooks by aligning various mentions of
the same axiom across textbooks and introducing
structural constraints on the alignment.

775



C
on

te
nt

Sentence Over-
lap

Semantic Textual Similarity between the current and next sentence. We include features that compute the proportion of common
unigrams and geometry entities (constants, predicates and functions) across the two sentences. This feature is conjoined with the tag
assigned to the current and next sentence.

Geometry enti-
ties

No. of geometry entities (normalized by the number of tokens) in this sentence. This feature is conjoined with the tag assigned to the
current sentence.

Intra-sentence
semantics

Indicator that the current sentence contains any one of the following words: hence, if, equal, twice, proportion, ratio, product. This
feature is conjoined with the tag assigned to the current sentence.

Ty
po

gr
ap

hy

Axiom, Theo-
rem, Corollary
Mention

(a) The current (or previous) sentence is mentioned as an Axiom, Theorem or Corollary e.g. Similar Triangle Theorem or Corollary 2.1.
(b) The section or subsection in the textbook containing the current (or previous) sentence mentions an Axiom, Theorem or Corollary.
This feature is conjoined with the tag assigned to the current (and previous) sentence.

Eqn. Template The current (or next) sentence contains an equation eg. PA × PB = PT 2. This feature is conjoined with the tag assigned to the
current (and next) sentence.

Assoc. Dia-
gram

The current sentence contains a pointer to a figure eg. “Figure 2.1”. This feature is conjoined with the tag assigned to the current
sentence.

RST edge Indicator for the RST relation between the current and next sentence. This feature is conjoined with the tag assigned to the current and
next sentence.

Bold/Underline The sentence (or previous) sentence contains text that is in bold font or underlined. Conjoined with the tag assigned to the current (and
previous) sentence.

XML structure Indicator that the current and previous sentence are in the same node of the XML hierarchy. Conjoined with the tag assigned to the
current and previous sentence.

Bounding box Indicator that the current and previous sentence are bounded by a bounding box in the textbook. Conjoined with the tag assigned to the
current and previous sentence.

Table 1: Feature set for our axiom identification model. The features are based on content and typography.

Let Ab =
(
A

(b)
1 , A

(b)
2 , . . . , A

(b)
|Ab|

)
be the axiom

mentions extracted from textbook b. Let A denote
the collection of axiom mentions extracted from
all textbooks. We assume a global ordering of
axioms A∗ = (A∗1, A∗2, . . . , A∗U ) where U is some
pre-defined upper bound on the total number
of axioms in geometry. Then, we emphasize
that the axiom mentions extracted from each
textbooks (roughly) follow this ordering. Let
Z

(b)
ij be a random variable that denotes if axiom

A
(b)
i extracted from book b refers to the global

axiom A∗j . We introduce a log-linear model that
factorizes over alignment pairs:
P (Z|A;φφφ) = 1

Z(A;φφφ)
×

exp

 ∑
b1,b2∈B
b1 6=b2

∑
1≤k≤U

∑
1≤i≤|Ab1 |
1≤j≤|Ab2 |

Z
(b1)
ik Z

(b2)
jk φφφ

Tg(A
(b1)
i , A

(b2)
j )


Here, Z(A;φφφ) is the partition function of the

log-linear model. g denotes the feature function
described later. We introduce the following
constraints on the alignment structure:
C1: An axiom appears in one book at-most once
C2: An axiom refers to exactly one theorem in
the global ordering
C3: Ordering Constraint: If ith axiom in a book
refers to the jth axiom in the global ordering then
no axiom succeeding the ith axiom can refer to a
global axiom preceding j.

Learning with Hard Constraints: We find the
optimal parameters φφφ using maximum-likelihood
estimation with L2 regularization:
φφφ∗ = arg maxφφφ logP (Z|A;φφφ)− µ||φφφ||22

We use L-BFGS to optimize the objective. To

compute feature expectations appearing in the gra-
dient of the objective, we use a Gibbs sampler. The
sampling equations for Zbik are:

P (Z
(b)
ik |rest) ∝ exp (Tb(i, k)) (1)

Tb(i, k) = Z
(b)
ik

∑
b′∈B
b′ 6=b

∑
1≤j≤|Ab′ |

Z
(b′)
jk φφφ

Tg(A
(b)
i , A

(b′)
j )

Note that the constraints C1 . . . 3 define the fea-
sible space of alignments. Our sampler always
samples the next Z(b)ik in this feasible space.
Learning with Soft Constraints: We might want
to treat some constraints, in particular, the order-
ing constraints C3 as soft constraints. We can
write down the constraint C3 using the alignment
variables:
Z

(b)
ij ≤ 1− Z(b)kl
∀ 1 ≤ i < k ≤ |Ab|, 1 ≤ l < j ≤ U
∀ b ∈ B

To model these constraints as soft constraints,
we penalize the model for violating these con-
straints. Let the penalty for violating the above
constraint be exp

(
νmax

(
0, 1− Z(b)ij − Z(b)kl

))
. We

introduce a new regularization term: R(Z) =∑
1≤i<k≤|Ab|
1≤l<j≤U
b∈B

exp
(
νmax

(
0, 1− Z(b)ij − Z(b)kl

))
. Here

ν is a hyper-parameter to tune the cost of violating
a constraint. We write down the following regular-
ized objective:

φφφ∗ = arg maxφφφ logP (Z|A;φφφ)−R(Z)− µ||φφφ||22
We use L-BFGS to find the optimal parameters

φφφ∗. We perform Gibbs sampling to compute fea-
ture expectations. The sampling equation for Z(b)ik
is similar (eq 1), but:
Tb(i, k) =

∑
b′∈B
b′ 6=b

∑
1≤j≤|Ab′ |

Z
(b)
ik Z

(b′)
jk φφφ

Tg(A
(b)
i , A

(b′)
j )

776



B	
I	
I	
I	

B	
I	
I	

B	
I	
I	
I	

Update Axiom Delete Axiom Create Axiom 

B	
I	
I	
I	

Figure 4: An illustration of the three operations to sample axiom blocks.

+ ν
∑
b′∈B
b′ 6=b

∑
i<j≤|Ab′ |

∑
1≤l<k

(
1− Z(b)ik − Z(b

′)
jl

)
+ ν

∑
b′∈B
b′ 6=b

∑
1≤j<i|

∑
k<l≤U

(
1− Z(b)ik − Z(b

′)
jl

)
Features: Now, we describe the features g. These
too include content based features encoding var-
ious notions of similarity between pairs of ax-
iom mentions as well as various typographical fea-
tures. The features are listed in Table 2.

4.1.3 Joint Identification and Alignment

Joint modeling of axiom identification and align-
ment components is useful as both problems po-
tentially help each other. Let Y (b)ij denote that the

sentence s(b)i from book b has tag j. We reuse the
definitions of the alignment variables Z(b)ij as be-

fore. We further define Z(b)i0 such that it denotes
that the ith axiom in textbook b is not aligned to
any global axiom. We again define a log-linear
model with factors that score axiom identification
and axiom alignments.

p(Y,Z|{Sb};θθθ,φφφ) ∝ fAI(Y|{Sb};θθθ)× fAA(Z|Y, {Sb};φφφ)
Here, the factors:
fAI = exp(

∑
b∈B

|Sb|∑
k=1

∑
i,j∈T

Y
(b)

k−1iY
(b)

kj θθθ
T
ijfij(i, j,Sb))

fAA = exp(
∑

b1,b2∈B
b1 6=b2

∑
1≤k≤U

∑
1≤i≤|Ab1 |
1≤j≤|Ab2 |

Z
(b1)
ik Z

(b2)
jk φφφ

T g(A
(b1)
i , A

(b2)
j ))

We write down the model constraints below:
C1’: Every sentence has a unique label
C2’ Tag O cannot be followed by tag I
C3’ Consistency between Y ’s and Z’s i.e. axiom
boundaries defined by Y ’s and Z’s must agree.
C4’ = C3.

We use L-BFGS for learning. To compute fea-
ture expectations, we use a Metropolis Hastings
sampler that samples Y′s and Z′s alternatively.
Sampling for Z′s reduces to Gibbs sampling and
the sampling equations are as same as before (Sec-
tion 4.1.2). For better mixing, we sample Y in
blocks. Consider blocks of Y’s which denote ax-
iom boundaries at time stamp t , we define three
operations to sample axiom blocks at the next time

stamp. The operations (shown in Figure 4) are:
Update axiom: The axiom boundary can be
shrunk, expanded or moved. The new axiom, how-
ever, cannot overlap with other axioms.
Delete axiom: The axiom can be deleted by label-
ing all its sentences as O.
Introduce axiom: Given a contiguous sequence
of sentences labeled O, a new axiom can be intro-
duced.
Note that these three operations define an ergodic
Markov chain. We use the axiom identification
part of the model as the proposal:

Q(Ȳ|Y) ∝ exp
(∑
b∈B

|Sb|∑
k=1

∑
i,j∈T

Ȳ
(b)
k−1iȲ

(b)
kj θθθ

T
ijfij(i, j,Sb)

)
Hence, the acceptance ratio only depends on
the alignment part of the model: R(Ȳ|Y) =
min

(
1, U(Ȳ)

U(Y)

)
where U(Y) = fAA. We again have

two variants, where we model the ordering con-
straints (C4′) as soft or hard constraints.

4.2 Axiom Parsing

After harvesting axioms, we build a parser for
these axioms that maps raw axioms to horn clause
rules. The axiom harvesting step provides us
a multi-set of axiom extractions. Let A =
{A1,A2, . . . ,A|A|} represent the multi-set where
each axiom Ai is mentioned at least once.

First, we describe a base parser that parses ax-
iom mentions to horn clause rules. Then, we uti-
lize the redundancy of axiom extractions from var-
ious sources (textbooks) to improve our parser.

4.2.1 Base Axiomatic Parser
Our base parser identifies the premise and conclu-
sion portions of each axiom and then uses GEOS’s
text parser to parse the two portions into a logical
formula. Then, the two logical formulas are put
together to form horn clause rules.

Axiom mentions (for example, the Pythagoras
theorem mention in Figure 1) are often accompa-
nied by equations or diagrams. When the men-
tion has an equation, we simply treat the equation
as the conclusion and the rest of the mention as
the premise. When the axiom has an associated
diagram, we always include the diagram in the
premise. We learn a model to predict the split of
the axiom text into two parts forming the premise
and the conclusion spans. Then, the GEOS parser
maps the premise and conclusion spans to premise
and conclusion logical formulas, respectively.

Let Zs represent the split that demarcates the
premise and conclusion spans. We score the ax-

777



Unigram, Bigram,
Dependency and
Entity Overlap

Real valued features that compute the proportion of common unigrams, bigrams, dependencies and geometry entities (constants,
predicates and functions) across the two axioms. When comparing geometric entities, we include geometric entities derived from
the associated diagrams when available.

Longest Common
Subsequence

Real valued feature that computes the length of longest common sub-sequence of words between two axiom mentions normalized
by the total number of words in the two mentions.

Number of sentences Real valued feature that computes the absolute difference in the number of sentences in the two mentions.
Alignment Scores We use an off-the-shelf monolingual word aligner – JACANA (Yao et al., 2013) pretrained on PPDB – and compute alignment score

between axiom mentions as the feature.
MT Metrics We use two common MT evaluation metrics METEOR (Denkowski and Lavie, 2010) and MAXSIM (Chan and Ng, 2008), and

use the evaluation scores as features. While METEOR computes n-gram overlaps controlling on precision and recall, MAXSIM
performs bipartite graph matching and maps each word in one axiom to at most one word in the other.

Summarization Met-
rics

We also use Rouge-S (Lin, 2004), a text summarization metric, and use the evaluation score as a feature. Rouge-S is based on
skip-grams.

Equation Template Indicator feature that matches templates of equations detected in the axiom mentions.
Image Caption Proportion of common unigrams in the image captions of the diagrams associated with the axiom mentions. If both mentions do

not have associated diagrams, this feature doesn’t fire.
XML structure Indicator matching the current (and parent) node of axiom mentions in respective XML hierarchies.

Table 2: Feature set for our axiom alignment model. The features are based on content, structure and typography.

iom split as a log-linear model: p(Zs|a;w) ∝
exp

(
wTh(a, Zs)

)
. Here, h are feature functions

described later. We found that in most cases
(>95%), the premise and conclusion are contigu-
ous spans in the axiom mention where the left span
corresponds to the premise and the right span cor-
responds to the conclusion. Hence, we search over
the space of contiguous spans to infer Zs. We use
L-BGFGS for learning.
Features: We list the features h in Table 3. The
features are defined over candidate spans forming
the text split, are strongly inspired from rhetori-
cal structure theory (Mann and Thompson, 1988)
and previous works on discourse parsing (Marcu,
2000; Soricut and Marcu, 2003). Given a beam of
Premise and Conclusion splits, we use the GEOS
parser to get Premise and Conclusion logical for-
mulas for each split in the beam and obtain a beam
of axiom parses for each axiom in each textbook.

4.2.2 Multi-source Axiomatic Parser
Now, we describe a multi-source parser that uti-
lizes the redundancy of axiom extractions from
various sources (textbooks). Given a beam of 10-
best parses for each axiom from each source, we
use a number of heuristics to determine the best
parse for the axiom:
1. Majority Voting: For each axiom, pick the
parse that occurs most frequently across beams.
2. Average Score: Pick the parse that has the
highest average parse score (only counting top 5
parses for each source), for each axiom.
3. Learn Source Confidence: Learn a set of
weights {µ1, µ2, . . . , µS}, one for each source and
then picks the parse that has the highest average
weighted parse score for each axiom.
4. Predicate Score: Instead of selecting from one
of the top parses across various sources, treat each
axiom parse as a bag of premise predicates and a

bag of conclusion predicates. Then, pick a subset
of premise and conclusion predicates for the final
parse using average scoring with thresholding.

5 Experiments

Datasets: We use a collection of grade 6-10 In-
dian high school math textbooks by four publish-
ers/authors – NCERT, R S Aggarwal, R D Sharma
and M L Aggarwal – a total of 5 × 4 = 20 text-
books to validate our model. Millions of students
in India study geometry from these books every
year and these books are readily available online.
We manually marked chapters relevant for geom-
etry in these books and then parsed them using
Adobe Acrobat’s pdf2xml parser. Then, we an-
notated geometry axioms, alignments and parses
for grade 6, 7 and 8 textbooks by the four pub-
lishers/authors. We use grade 6, 7 and 8 textbook
annotations for development, training, and testing,
respectively. All the hyper-parameters in all the
models are tuned on the development set using
grid search.

GEOS used 13 types of entities and 94 functions
and predicates. We add some more entities, func-
tions and predicates to cover other more complex
concepts in geometry not covered in GEOS. Thus,
we obtain a final set of 19 entity types and 115
functions and predicates for our parsing model.
We use Stanford CoreNLP (Manning et al., 2014)
for feature generation. We use two datasets for
evaluating our system: (a) practice and official
SAT style geometry questions used in GEOS, and
(b) an additional dataset of geometry questions
collected from the aforementioned textbooks. This
dataset consists of a total of 1406 SAT style ques-
tions across grades 6-10, and is approximately
7.5 times the size of the dataset used in GEOS.
We split the dataset into training (350 questions),

778



Discourse Mark-
ers

Discourse markers (connectives, cue-words or cue-phrases, etc) have been shown to give good indications on discourse structure (Marcu,
2000). We build a list of discourse markers using the training set, considering the first and last tokens of each span, culled to top 100
by frequency. We use these 100 discourse markers as features. We repeat the same procedure by using part-of-speech (POS) instead of
words and use them as features.

Punctuation Punctuation at the segment border is an excellent cue. We include indicator features whether there is a punctuation at the segment border.
Text Organization Indicator that the two text spans are part of the same (a) sentence, (b) paragraph.
XML Structure Indicator that the two spans are in the same node in the XML hierarchy. Conjoined with the indicator feature that the two spans are part

of the same paragraph.
RST Parse We use an off-the-shelf RST parser (Feng and Hirst, 2014) and include an indicator feature that the segmentation matches the parse

segmentation. We also include the RST label as a feature.
Span Lengths The distribution of the two text spans is typically dependent on their lengths. We use the ratio of the length of the two spans as an

additional feature.
Soricut and
Marcu Segmenter

Soricut and Marcu (2003) (section 3.1) presented a statistical model for deciding elementary discourse unit boundaries. We use the
probability given by this model retrained on our training set as feature. This feature uses both lexical and syntactic information.

Head / Common
Ancestor/ Attach-
ment Node

Head node is the word with the highest occurrence as a lexical head in the lexicalized tree among all the words in the text span. The
attachment node is the parent of the head node. We have features for the head words of the left and right spans, the common ancestor (if
any), the attachment node and the conjunction of the two head node words. We repeat these features with part-of-speech (POS) instead
of words.

Syntax Distance to (a) root (b) common ancestor for the nodes spanning the respective spans. We use these distances, and the difference in the
distances as features.

Dominance Dominance (Soricut and Marcu, 2003) is a key idea in discourse which looks at syntax trees and studies sub-trees for each span to infer
a logical nesting order between the two. We use the dominance relationship is a feature. See Soricut and Marcu (2003) for details.

Span Similarity Proportion of (a) words (b) geometry relations (c) relation-arguments shared by the two spans.
No. of Relations Number of geometry relations represented in the two spans. We use the Lexicon Map from GEOS to compute the number of expressed

geometry relations.
Relative Position Relative position of the two lexical heads and the text split in sentence.

Table 3: Feature set for our axiom parsing model.

Strict Comp. Relaxed Comp.
P R F P R F

Identification 64.3 69.3 66.7 84.3 87.9 86.1
Joint-Hard 68.0 68.1 68.0 85.4 87.1 86.2
Joint-Soft 69.7 71.1 70.4 86.9 88.4 87.6

Table 4: Test set Precision, Recall and F-measure scores for axiom identi-
fication when performed alone and when performed jointly with axiom align-
ment. We show results for both strict as well as relaxed comparison modes.
For the joint model, we show results when we model ordering constraints as
hard or soft constraints.

development (150 questions) and test (906 ques-
tions) with equal proportion of grade 6-10 ques-
tions. We annotated the 500 training and devel-
opment questions with ground-truth logical forms.
We use the training set to train another version of
GEOS with expanded set of entity types, functions
and predicates. We call this system GEOS++.
Results: We first evaluate the axiom identifica-
tion, alignment and parsing models individually.

For axiom identification, we compare the results
of automatic identification with gold axiom identi-
fications and compute the precision, recall and F-
measure on the test set. We use strict as well as re-
laxed comparison. In strict comparison mode the
automatically identified mentions and gold men-
tions must match exactly to get credit, whereas,
in the relaxed comparison mode only a majority
(>50%) of sentences in the automatically identi-
fied mentions and gold mentions must match to get
credit. Table 4 shows the results of axiom identifi-
cation where we clearly see improvements in per-
formance when we jointly model axiom identifica-
tion and alignment. This is due to the fact that both
the components reinforce each other. We also ob-

P R F NMI
Alignment 71.8 74.8 73.3 0.60
Joint-Hard 75.0 76.4 75.7 0.65
Joint-Soft 79.3 81.4 80.3 0.69

Table 5: Test set Precision, Recall, F-measure and NMI scores for axiom
alignment when performed alone and when performed jointly with axiom iden-
tification. For the joint model, we show results when we model ordering con-
straints as hard or soft constraints.

serve that modeling the ordering constraints as soft
constraints leads to better performance than mod-
eling them as hard constraints. This is because
the ordering of presentation of axioms is generally
(yet not always) consistent across textbooks.

To evaluate axiom alignment, we first view it
as a series of decisions, one for each pair of ax-
iom mentions and compute precision, recall and F-
score by comparing automatic decisions with gold
decisions. Then, we also use a standard clustering
metric, Normalized Mutual Information (NMI)
(Strehl and Ghosh, 2002) to measure the quality
of axiom mention clustering. Table 5 shows the
results on the test set when gold axiom identifica-
tions are used. We observe improvements in ax-
iom alignment performance too when we jointly
model axiom identification and alignment jointly
both in terms of F-score as well as NMI. Modeling
ordering constraints as soft constraints again leads
to better performance than modeling them as hard
constraints in terms of both metrics.

To evaluate axiom parsing, we compute pre-
cision, recall and F-score in (a) deriving literals
in axiom parses, as well as for (b) the final ax-
iom parses on our test set. Table 6 shows the re-

779



Literals Full Parse
P R F P R F

GEOS 86.7 70.9 78.0 64.2 56.6 60.2
G

E
O

S+
+

Single Src. 91.6 75.3 82.6 68.8 60.4 64.3
Maj. Voting 90.2 78.5 83.9 70.0 63.3 66.5
Avg. Score 90.8 79.6 84.9 71.7 66.4 69.0

Src. Confid. 91.0 79.9 85.1 73.3 68.1 70.6
Pred. Score 92.8 82.8 87.5 76.6 70.1 73.2

Table 6: Test set Precision, Recall and F-measure scores for axiom parsing.
These scores are computed over literals derived in axiom parses or full axiom
parses. We show results for the old GEOS system, for the improved GEOS++
system with expanded entity types, functions and predicates, and for the multi-
source parsers presented in this paper.

Practice Official Textbook
GEOS 61 49 32

Our System 64 55 51
Oracle 80 78 72

Table 7: Scores for solving geometry questions on the SAT practice and
official datasets and a dataset of questions from the 20 textbooks. We use
SATs grading scheme that rewards a correct answer with a score of 1.0 and
penalizes a wrong answer with a negative score of 0.25. Oracle uses gold
axioms but automatic text and diagram interpretation in our logical solver. All
differences between GEOS and our system are significant (p¡0.05 using the
two-tailed paired t-test).

sults of axiom parsing for GEOS (trained on the
training set) as well as various versions of our
best performing system (GEOS++ with our ax-
iomatic solver) with various heuristics for multi-
source parsing. The results show that our system
(single source) performs better than GEOS as it is
trained with the expanded set of entity types, func-
tions and predicates. The results also show that
the choice of heuristic is important for the multi-
source parser – though all the heuristics lead to
improvements over the single source parser. The
average score heuristic that chooses the parse with
the highest average score across sources performs
better than majority voting which chooses the best
parse based on a voting heuristic. Learning the
confidence of every source and using a weighted
average is an even better heuristic. Finally, pred-
icate scoring which chooses the parse by scoring
predicates on the premise and conclusion sides
performs the best leading to 87.5 F1 score (when
computed over parse literals) and 73.2 F1 score
(when computed on the full parse). The high F1
score for axiom parsing on the test set shows that
our approach works well and we can accurately
harvest axiomatic knowledge from textbooks.

Finally, we use the extracted horn clause rules
in our axiomatic solver for solving geometry prob-
lems. For this, we over-generate a set of horn
clause rules by generating 3 horn clause parses for
each axiom and use them as the underlying theory
in prolog programs such as the one shown in Fig-
ure 3. We use weighted logical expressions for the

Interpretability Usefulness
GEOS O.S. GEOS O.S.

Grade 6 2.7 2.9 2.9 3.2
Grade 7 3.0 3.7 3.3 3.6
Grade 8 2.7 3.5 3.1 3.5
Grade 9 2.4 3.3 3.0 3.7
Grade 10 2.8 3.1 3.2 3.8
Overall 2.7 3.3 3.1 3.6

Table 8: User study ratings for GEOS and our system (O.S.) by students in
grade 6-10. Ten students in each grade were asked to rate the two systems on a
scale of 1-5 on two facets: ‘interpretability’ and ‘usefulness’. Each cell shows
the mean rating computed over ten students in that grade for that facet.

question description and the diagram derived from
GEOS++ as declarations, and the (normalized)
score of the parsing model multiplied by the score
of the joint axiom identification and alignment
model as weights for the rules. Table 7 shows the
results for our best end-to-end system and com-
pares it to GEOS on the practice and official SAT
dataset from Seo et al. (2015) as well as questions
from the 20 textbooks. On all the three datasets,
our system outperforms GEOS. Especially on the
dataset from the 20 textbooks (which is indeed a
harder dataset and includes more problems which
require complex reasoning based on geometry),
GEOS doesn’t perform very well whereas our sys-
tem still achieves a good score. Oracle shows
the performance of our system when gold ax-
ioms (written down by an expert) are used along
with automatic text and diagram interpretations in
GEOS++. This shows that there is scope for fur-
ther improvement in our approach.
Interpretability: Students around the world solve
geometry problems through rigorous deduction
whereas the numerical solver in GEOS does not
provide such interpretability. One of the key ben-
efits of our axiomatic solver is that it provides an
easy-to-understand student-friendly deductive so-
lution to geometry problems.

To test the interpretability of our axiomatic
solver, we asked 50 grade 6-10 students (10 stu-
dents in each grade) to use GEOS and our sys-
tem (GEOS++ with our axiomatic solver) as a
web-based assistive tool while learning geometry.
They were each asked to rate how ‘interpretable’
and ‘useful’ the two systems were on a scale of
1-5. Table 8 shows the mean rating by students
in each grade on the two facets. We can observe
that students of each grade found our system to be
more interpretable as well as more useful to them
than GEOS. This study lends support to our claims
about the need of an interpretable deductive solver
for geometry problems.

780



6 Related Work

Solving Geometry Problems: While the prob-
lem of using computers to solve geometry ques-
tions is old (Feigenbaum and Feldman, 1963;
Schattschneider and King, 1997; Davis, 2006),
NLP and computer vision techniques were first
used to solve geometry problems in Seo et al.
(2015). While Seo et al. (2014) only aligned ge-
ometric shapes with their textual mentions, Seo
et al. (2015) also extracted geometric relations
and built GEOS, the first automated system to
solve SAT style geometry questions. GEOS used
a coordinate geometry based solution by translat-
ing each predicate into a set of manually writ-
ten constraints. A boolean satisfiability problem
posed with these constraints was used to solve
the multiple-choice question. GEOS had two key
issues: (a) it needed access to answer choices
which may not always be available for such prob-
lems, and (b) it lacked the deductive geometric
reasoning used by students to solve these prob-
lems. Our axiomatic solver mitigates these is-
sues by performing deductive reasoning using ax-
iomatic knowledge extracted from textbooks.
Information Extraction from Textbooks: Our
model builds upon ideas from Information extrac-
tion (IE), which is the task of automatically ex-
tracting structured information from unstructured
and/or semi-structured documents. While there
has been a lot of work in IE on domains such as
web documents (Chang et al., 2003; Etzioni et al.,
2004; Cafarella et al., 2005; Chang et al., 2006;
Banko et al., 2007; Etzioni et al., 2008; Mitchell
et al., 2015) and scientific publication data (Shah
et al., 2003; Peng and McCallum, 2006; Saleem
and Latif, 2012), work on IE from educational ma-
terial is much more sparse. Most of the research
in IE from educational material deals with extract-
ing simple educational concepts (Shah et al., 2003;
Canisius and Sporleder, 2007; Yang et al., 2015;
Wang et al., 2015; Liang et al., 2015; Wu et al.,
2015; Liu et al., 2016b; Wang et al., 2016) or
binary relational tuples (Balasubramanian et al.,
2002; Clark et al., 2012; Dalvi et al., 2016) us-
ing existing IE techniques. On the other hand,
our approach extracts axioms and parses them to
horn clause rules. This is much more challenging.
Raw application of rule mining or sequence label-
ing techniques used to extract information from
web documents and scientific publications to ed-
ucational material usually leads to poor results as

the amount of redundancy in educational material
is lower and the amount of labeled data is sparse.
Our approach tackles these issues by making ju-
dicious use of typographical information, the re-
dundancy of information and ordering constraints
to improve the harvesting and parsing of axioms.
This has not been attempted in previous work.
Language to Programs: After harvesting axioms
from textbooks, we also present an approach to
parse the axiom mentions to horn clause rules.
This work is related to a large body of work on
semantic parsing (Zelle and Mooney, 1993, 1996;
Kate et al., 2005; Zettlemoyer and Collins, 2012,
inter alia). Semantic parsers typically map natu-
ral language to formal programs such as database
queries (Liang et al., 2011; Berant et al., 2013;
Yaghmazadeh et al., 2017, inter alia), commands
to robots (Shimizu and Haas, 2009; Matuszek
et al., 2010; Chen and Mooney, 2011, inter alia),
or even general purpose programs (Lei et al., 2013;
Ling et al., 2016; Yin and Neubig, 2017; Ling
et al., 2017). More specifically, Liu et al. (2016a)
and Quirk et al. (2015) learn “If-Then” and “If-
This-Then-That” rules, respectively. In theory,
these works can be adapted to parse axiom men-
tions to horn-clause rules. However, this would
require a large amount of supervision which would
be expensive to obtain. We mitigated this issue by
using redundant axiom mention extractions from
multiple textbooks and then combining the parses
obtained from various textbooks to achieve a bet-
ter final parse for each axiom.

7 Conclusion

We presented an approach to harvest structured
axiomatic knowledge from math textbooks. Our
approach uses rich features based on context and
typography, the redundancy of axiomatic knowl-
edge and shared ordering constraints across mul-
tiple textbooks to accurately extract and parse ax-
iomatic knowledge to horn clause rules. We used
the parsed axiomatic knowledge to improve the
best previously published automatic approach to
solve geometry problems. A user-study conducted
on a number of school students studying geome-
try found our approach to be more interpretable
and useful than its predecessor. While this paper
focused on harvesting geometry axioms from text-
books as a case study, it can be extended to obtain
valuable structured knowledge from textbooks in
areas such as science, engineering and finance.

781



References
Niranjan Balasubramanian, Stephen Soderland,

Oren Etzioni Mausam, and Robert Bart. 2002. out
of the box information extraction: a case study
using bio-medical texts. Technical report.

Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In IJ-
CAI 2007, Proceedings of the 20th International
Joint Conference on Artificial Intelligence, Hyder-
abad, India, January 6-12, 2007, pages 2670–2676.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2013, 18-21 October
2013, Grand Hyatt Seattle, Seattle, Washington,
USA, A meeting of SIGDAT, a Special Interest Group
of the ACL, pages 1533–1544.

Michael J. Cafarella, Doug Downey, Stephen Soder-
land, and Oren Etzioni. 2005. Knowitnow: Fast,
scalable information extraction from the web. In
HLT/EMNLP 2005, Human Language Technology
Conference and Conference on Empirical Methods
in Natural Language Processing, Proceedings of the
Conference, 6-8 October 2005, Vancouver, British
Columbia, Canada, pages 563–570.

Sander Canisius and Caroline Sporleder. 2007. Boot-
strapping information extraction from field books.
In EMNLP-CoNLL, pages 827–836.

Yee Seng Chan and Hwee Tou Ng. 2008. Maxsim: A
maximum similarity metric for machine translation
evaluation. In The 2008 Annual Conference of the
Association for Computational Linguistics (ACL).

Chia-Hui Chang, Chun-Nan Hsu, and Shao-Cheng Lui.
2003. Automatic information extraction from semi-
structured web pages by pattern discovery. Decision
Support Systems, 35(1):129–147.

Chia-Hui Chang, Mohammed Kayed, Moheb R Gir-
gis, and Khaled F Shaalan. 2006. A survey of web
information extraction systems. IEEE transactions
on knowledge and data engineering, 18(10):1411–
1428.

David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the 25th
AAAI Conference on Artificial Intelligence (AAAI-
2011), pages 859–865.

Peter Clark and Oren Etzioni. 2016. My computer is
an honor student - but how intelligent is it? stan-
dardized tests as a measure of ai. In Proceedings of
AI Magazine.

Peter Clark, Phil Harrison, Niranjan Balasubramanian,
and Oren Etzioni. 2012. Constructing a textual kb
from a biology textbook. In Proceedings of the Joint

Workshop on Automatic Knowledge Base Construc-
tion and Web-scale Knowledge Extraction, pages
74–78. Association for Computational Linguistics.

Bhavana Dalvi, Sumithra Bhakthavatsalam, Chris
Clark, Peter Clark, Oren Etzioni, Anthony Fader,
and Dirk Groeneveld. 2016. IKE - an interactive tool
for knowledge extraction. In Proceedings of the 5th
Workshop on Automated Knowledge Base Construc-
tion, AKBC@NAACL-HLT 2016, San Diego, CA,
USA, June 17, 2016, pages 12–17.

Tom Davis. 2006. Geometry with computers. Techni-
cal report.

Michael Denkowski and Alon Lavie. 2010. Extending
the meteor machine translation evaluation metric to
the phrase level. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 250–253. Association for Computa-
tional Linguistics.

Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S Weld. 2008. Open information extrac-
tion from the web. Communications of the ACM,
51(12):68–74.

Oren Etzioni, Michael J. Cafarella, Doug Downey,
Ana-Maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel S. Weld, and Alexander Yates. 2004.
Methods for domain-independent information ex-
traction from the web: An experimental comparison.
In Proceedings of the Nineteenth National Confer-
ence on Artificial Intelligence, Sixteenth Conference
on Innovative Applications of Artificial Intelligence,
July 25-29, 2004, San Jose, California, USA, pages
391–398.

Edward A Feigenbaum and Julian Feldman. 1963.
Computers and thought. The AAAI Press.

Vanessa Wei Feng and Graeme Hirst. 2014. A linear-
time bottom-up discourse parser with constraints
and post-editing. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 511–521.

Akira Fujita, Akihiro Kameda, Ai Kawazoe, and
Yusuke Miyao. 2014. Overview of todai robot
project and evaluation framework of its nlp-based
problem solving. World History, 36:36.

Rohit J Kate, Yuk Wah, Wong Raymond, and
J Mooney. 2005. Learning to transform natural to
formal languages. In Proceedings of AAAI-05. Cite-
seer.

Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automatically
solve algebra word problems. In Proceedings of
ACL.

782



John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the eighteenth in-
ternational conference on machine learning, ICML,
volume 1, pages 282–289.

Tao Lei, Fan Long, Regina Barzilay, and Martin C Ri-
nard. 2013. From natural language specifications to
program input parsers. Association for Computa-
tional Linguistics (ACL).

Chen Liang, Zhaohui Wu, Wenyi Huang, and C Lee
Giles. 2015. Measuring prerequisite relations
among concepts. In EMNLP, pages 1668–1674.

Percy Liang, Michael I Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 590–
599. Association for Computational Linguistics.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Text summariza-
tion branches out: Proceedings of the ACL-04 work-
shop, volume 8. Barcelona, Spain.

Wang Ling, Edward Grefenstette, Karl Moritz Her-
mann, Tomáš Kočiskỳ, Andrew Senior, Fumin
Wang, and Phil Blunsom. 2016. Latent predic-
tor networks for code generation. arXiv preprint
arXiv:1603.06744.

Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction for rationale gener-
ation: Learning to solve and explain algebraic word
problems. Association for Computational Linguis-
tics (ACL) – To appear.

Chang Liu, Xinyun Chen, Eui Chul Shin, Mingcheng
Chen, and Dawn Song. 2016a. Latent attention
for if-then program synthesis. In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 29, pages 4574–4582. Curran As-
sociates, Inc.

Hanxiao Liu, Wanli Ma, Yiming Yang, and Jaime Car-
bonell. 2016b. Learning concept graphs from online
educational data. Journal of Artificial Intelligence
Research, 55:1059–1090.

William C Mann and Sandra A Thompson. 1988.
{Rhetorical Structure Theory: Toward a functional
theory of text organisation}. Text, 3(8):234–281.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations,
pages 55–60.

Daniel Marcu. 2000. The theory and practice of dis-
course parsing and summarization.

Cynthia Matuszek, Dieter Fox, and Karl Koscher.
2010. Following directions using statistical ma-
chine translation. In 2010 5th ACM/IEEE Inter-
national Conference on Human-Robot Interaction
(HRI), pages 251–258. IEEE.

T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar,
J. Betteridge, A. Carlson, B. Dalvi, M. Gardner,
B. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis,
T. Mohamed, N. Nakashole, E. Platanios, A. Rit-
ter, M. Samadi, B. Settles, R. Wang, D. Wijaya,
A. Gupta, X. Chen, A. Saparov, M. Greaves, and
J. Welling. 2015. Never-ending learning. In Pro-
ceedings of the Twenty-Ninth AAAI Conference on
Artificial Intelligence (AAAI-15).

Fuchun Peng and Andrew McCallum. 2006. Infor-
mation extraction from research papers using con-
ditional random fields. Information processing &
management, 42(4):963–979.

Chris Quirk, Raymond J. Mooney, and Michel Galley.
2015. Language to code: Learning semantic parsers
for if-this-then-that recipes. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing of the
Asian Federation of Natural Language Processing,
ACL 2015, July 26-31, 2015, Beijing, China, Volume
1: Long Papers, pages 878–888.

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing (EMNLP).

Mrinmaya Sachan, Avinava Dubey, Eric P Xing, and
Matthew Richardson. 2015. Learning answer-
entailing structures for machine comprehension. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics.

Mrinmaya Sachan, Kumar Avinava Dubey, and Eric P.
Xing. 2016. Science question answering using in-
structional materials. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2016, August 7-12, 2016, Berlin,
Germany, Volume 2: Short Papers.

Mrinmaya Sachan and Eric P. Xing. 2016. Easy ques-
tions first? A case study on curriculum learning for
question answering. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2016, August 7-12, 2016, Berlin,
Germany, Volume 1: Long Papers.

Ozair Saleem and Seemab Latif. 2012. Information
extraction from research papers by data integration
and data validation from multiple header extraction
sources. In Proceedings of the World Congress
on Engineering and Computer Science, volume 1,
pages 177–180.

783



Doris Schattschneider and James King. 1997. Geom-
etry Turned On: Dynamic Software in Learning,
Teaching, and Research. Mathematical Association
of America Notes.

Carissa Schoenick, Peter Clark, Oyvind Tafjord, Pe-
ter D. Turney, and Oren Etzioni. 2016. Moving be-
yond the turing test with the allen AI science chal-
lenge. CoRR, abs/1604.04315.

Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, and
Oren Etzioni. 2014. Diagram understanding in ge-
ometry questions. In Proceedings of AAAI.

Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren
Etzioni, and Clint Malcolm. 2015. Solving geome-
try problems: combining text and diagram interpre-
tation. In Proceedings of EMNLP.

Parantu K Shah, Carolina Perez-Iratxeta, Peer Bork,
and Miguel A Andrade. 2003. Information extrac-
tion from full text scientific articles: Where are the
keywords? BMC bioinformatics, 4(1):20.

Nobuyuki Shimizu and Andrew R. Haas. 2009. Learn-
ing to follow navigational route instructions. In
IJCAI 2009, Proceedings of the 21st Interna-
tional Joint Conference on Artificial Intelligence,
Pasadena, California, USA, July 11-17, 2009, pages
1488–1493.

Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 149–156. Association
for Computational Linguistics.

Alexander Strehl and Joydeep Ghosh. 2002. Clus-
ter ensembles—a knowledge reuse framework for
combining multiple partitions. Journal of machine
learning research, 3(Dec):583–617.

Shuting Wang, Chen Liang, Zhaohui Wu, Kyle
Williams, Bart Pursel, Benjamin Brautigam, Sher-
wyn Saul, Hannah Williams, Kyle Bowen, and
C Lee Giles. 2015. Concept hierarchy extraction
from textbooks. In Proceedings of the 2015 ACM
Symposium on Document Engineering, pages 147–
156. ACM.

Shuting Wang, Alexander Ororbia, Zhaohui Wu, Kyle
Williams, Chen Liang, Bart Pursel, and C Lee Giles.
2016. Using prerequisites to extract concept maps
from textbooks. In Proceedings of the 25th ACM
International on Conference on Information and
Knowledge Management, pages 317–326. ACM.

Jian Wu, Jason Killian, Huaiyu Yang, Kyle Williams,
Sagnik Ray Choudhury, Suppawong Tuarob, Cor-
nelia Caragea, and C. Lee Giles. 2015. Pdfmef:
A multi-entity knowledge extraction framework for
scholarly documents and semantic search. In Pro-
ceedings of the 8th International Conference on
Knowledge Capture, K-CAP 2015.

Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and
Thomas Dillig. 2017. Type- and content-driven syn-
thesis of SQL queries from natural language. CoRR,
abs/1702.01168.

Yiming Yang, Hanxiao Liu, Jaime G. Carbonell, and
Wanli Ma. 2015. Concept graph learning from ed-
ucational data. In Proceedings of the Eighth ACM
International Conference on Web Search and Data
Mining, WSDM 2015, Shanghai, China, February
2-6, 2015, pages 159–168.

Xuchen Yao, Benjamin Van Durme, Chris Callison-
Burch, and Peter Clark. 2013. A lightweight and
high performance monolingual word aligner. In
ACL (2), pages 702–707.

Pengcheng Yin and Graham Neubig. 2017. A syntac-
tic neural model for general-purpose code genera-
tion. In The 55th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Vancou-
ver, Canada.

John M. Zelle and Raymond J. Mooney. 1993. Learn-
ing semantic grammars with constructive inductive
logic programming. In Proceedings of the 11th Na-
tional Conference on Artificial Intelligence. Wash-
ington, DC, USA, July 11-15, 1993., pages 817–822.

John M Zelle and Raymond J Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In In Proceedings of the Thirteenth
National Conference on Artificial Intelligence.

Luke S Zettlemoyer and Michael Collins. 2012. Learn-
ing to map sentences to logical form: Structured
classification with probabilistic categorial gram-
mars. arXiv preprint arXiv:1207.1420.

784


