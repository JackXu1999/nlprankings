











































Detecting and Reducing Bias in a High Stakes Domain


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4765–4775,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4765

Detecting and Reducing Bias in a High Stakes Domain

Ruiqi Zhong1 ⇤, Yanda Chen1, Desmond Patton2, Charlotte Selous2, Kathleen McKeown1
1Department of Computer Science, Columbia University

2School of Social Work, Columbia University

{rz2383, yc3384, dp2787, cis2114}@columbia.edu
{kathy}@cs.columbia.edu

Abstract

Gang-involved youth in cities such as Chicago
sometimes post on social media to express
their aggression towards rival gangs and pre-
vious research has demonstrated that a deep
learning approach can predict aggression and
loss in posts. To address the possibility of bias
in this sensitive application, we developed an
approach to systematically interpret the state
of the art model. We found, surprisingly, that it
frequently bases its predictions on stop words
such as “a” or “on”, an approach that could
harm social media users who have no aggres-
sive intentions. To tackle this bias, domain
experts annotated the rationales, highlighting
words that explain why a tweet is labeled as
“aggression”. These new annotations enable
us to quantitatively measure how justified the
model predictions are, and build models that
drastically reduce bias. Our study shows that
in high stake scenarios, accuracy alone can-
not guarantee a good system and we need new
evaluation methods.

1 Introduction

There has been increased interest in using natu-
ral language processing to help understand the root
causes of violence in cities such as Chicago. Re-
searchers in social work found that posts on so-
cial media by gang-involved youth mirror their life
on the physical street (Patton et al., 2016, 2017)
and suggested that if community outreach workers
could use tools to help identify posts of aggres-
sion and grief, they could intervene before grief
turns to retribution and help avert violence. Early
research in this area (Blevins et al., 2016) used
support vector machines for the three-way clas-
sification task of aggression, loss and other and
recent work (Chang et al., 2018) demonstrated a

⇤The first author is now a PhD student at University of
California, Berkeley.

significant increase in accuracy through the use of
a neural net approach along with the representa-
tion of contextual information from previous con-
versations. In both of these approaches, computer
scientists and social work researchers worked side
by side to enable qualitative analysis of the data to
use in machine learning and to provide meaningful
error analyses.

Given the interest in using this work in a
live setting to identify situations where commu-
nity outreach workers could intervene, we are
concerned with how the model makes the deci-
sion. This is especially important since this is
a high stakes domain and machine learning sys-
tems are known to incorporate bias. For exam-
ple, the COMPAS Recidivism Algorithm, which
assesses the likelihood of a criminal defendant to
re-offend and influences judges’ sentencing deci-
sions, is known to be biased against African Amer-
icans (Feller et al., 2016). To avoid similar er-
rors in the gang-violence application, we exam-
ined whether the model is biased through sys-
tematic investigation of how the model makes the
predictions. Since misclassifying a tweet as ag-
gression might potentially criminalize a vulnera-
ble population (young black and latino youth) that
is already over criminalized, we focus on classifi-
cation of the aggression class in our paper.

Our finding is surprising: in at least 10% of the
cases the Chang et al. (2018) model relies on stop
words such as “a” and “on” to classify a tweet as
aggression; these words carry no semantic or prag-
matic indication of aggression or loss. We recog-
nized that there might be more hidden biases, so
we developed novel metrics to quantitatively com-
pare the difference between how humans and mod-
els make their classification. To achieve this, we
ask domain experts to annotate the rationale for
the aggression tweets by highlighting words and
phrases that indicate why experts classify them as



4766

aggression. We compare the rationale against the
models’ interpretation.

We also develop a model to incorporate human
rationales, and show that the system can make pre-
dictions based on similar reasons to those of do-
main experts. Our F-score is slightly improved
over Chang et al. (2018) and we are able to reduce
the bias drastically. We show how important this
is by using an adversarial approach which demon-
strates that we can change the prediction of posts
predicted as other to aggression by the simple in-
sertion of the non-offending function words. This
implies that high F-score alone does not guaran-
tee a good system, and that researchers in other
high stakes domain should evaluate their algo-
rithms from various angles.

Our contributions include:

• A validated finding that the previous state-of-
the-art model is biased.

• A rich dataset created by domain experts con-
taining annotated human rationales.

• New metrics to evaluate models’ robustness
under adversary, and the difference between
model interpretations and human rationales.

• A model that incorporates human rationales
in training and behaves more as humans do.

We also include an in-depth discussion on how
misinterpretation might emerge when labels are
being annotated by the experts. The battle against
bias is far from solved, and we hope our insights
and methodologies can guide future research in
high stakes domains.

2 Revealing Model Biases

We used the code and data from Chang et al.
(2018) and fully reproduced the state-of-the-art
model that uses a Convolutional Neural Network,
domain specific word embeddings and context fea-
tures.

2.1 Locating Biases
We examine the model using the simple leave-
one-out method similar to Ribeiro et al. (2016).
Suppose each tweet T is a sequence of l tokens
[e1, e2...el] and the model gives the prediction con-
fidence score y. For each token ei, we calculate
the model’s prediction confidence y�i on the same
tweet with ei masked as an unknown word; hence

the influence of the ith token Ii is approximated
by Ii = y � y�i. Then we manually examine the
top two influential uni-grams for each tweet that
the model classified as aggression.

As we expect, tokens strongly associated with
aggression, such as “ ”, “dead” and many other
curse words appear frequently as the most influen-
tial tokens. However, surprisingly, words such as
“a” and “on” are frequently considered the most
influential by the model as well. Out of 419 tweets
that the model classifies as aggression, 15/32 times
the model considers the token “a” to be the top first
or second most influential token and “on” 24/51
times the top first or second, respectively. We also
found that the model is biased against other stop
words, such as “into” and “of”. This indicates that
more than 10% of the time the model heavily re-
lies on these unintentional differences of language
use to classify tweets as aggression!

2.2 Confirming Biases Exist

Initially, we found this unexpected result surpris-
ing, so we checked the second order sensitivity
(Feng et al., 2018) and constructed adversarial
data sets.

Second Order Consistency Feng et al. (2018)
defined this term when they found that the leave-
one(token)-out (LOO) method might find a com-
pletely different set of most influential words if
the most un-influential word is dropped from the
sentence. However, we do not observe this prob-
lem in our case: out of 419 predictions, 379 times
the most influential word remains the most influ-
ential and 33 times it becomes the second most in-
fluential one after we drop the most un-influential
words.

Adversarial Dataset If we can identify a weak-
ness of the model, we must be able to break it in
a corresponding way. To manifest that there is in-
deed bias against “a” and “on,” for each of these
two words, we inserted it in each labeled tweet and
counted the number of times the model changes its
prediction from non-aggression to aggression.

To ensure that our insertions are natural, we
trained a large language model (Peters et al., 2018)
on the unlabeled corpus (⇠ 1 million tweets) used
by Chang et al. (2018), and score the naturalness
of the insertion by the change of perplexity given
by the language model. For each tweet we cal-
culate the most “natural” position to insert our
biased word and only took the top 800 natural



4767

tweets out of ⇠ 7000 candidates as our adversarial
dataset. We also subsample this dataset and check
the subsample manually to make sure that these
insertions make sense. In fact, since most tweets
in the dataset are grammatically flexible in the
first place, our insertion methods usually do not
make them more unnatural - we have fewer con-
straints than adversarial attacks (Alzantot et al.,
2018) conducted in domains such as IMDB re-
views (Maas et al., 2011) or Textual Entailment
(Bowman et al., 2015).

We find that even with these naive and seem-
ingly harmless insertions, the model changes the
prediction from non-aggression to aggression for
about 3%/5% of the generated tweets when insert-
ing “a”/“on”. With this overwhelming evidence,
we conclude that the model is biased.

The simplest way to fix this specific bias is a
post-mortem one - just remove “a” and “on” in
training and prediction. However, this only works
if these are the only two points of bias and we sus-
pect that there are many more biases lurking in the
model. Hence, we need to comprehensively study
the difference between how humans and models
make their decision. We need a new dataset with
annotated human rationales to achieve this goal.

3 Data

We use the same dataset and cross validation folds
as in Chang et al. (2018), which contains 329 Ag-
gression tweets, 734 Loss tweets, with the Other
class comprising the remaining 3,873 tweets. The
language in the dataset is hyper-local to the com-
munity we are studying and differs significantly
from standard American English. Therefore, most
of the toolkits (e.g. pre-trained language models,
parsers, etc) developed for standard English do not
apply. We extend their dataset by asking domain
experts to annotate the rationales and additional
information on context for each of the tweets la-
beled as aggression. Below is the set of annotation
instructions.

• Rationales in Tweets We ask the do-
main experts to annotate the most impor-
tant word/words (rationales) in the tweet
that makes them think the tweet is aggres-
sion/loss; or say, the removal of which would
make them not consider it as an aggressive
tweet. We ask them to annotate the “mini-
mum set of words” that is important for their
final decision, thus excluding words that are

not very meaningful, such as “on”, “basi-
cally”, “lol”, etc. Examples are shown in Ta-
ble 1.

• Aggressive by Context Sometimes the tweet
itself does not contain aggressive content if
we interpret them at face level, but placing
it into context reveals that the user in fact
conveyed aggressive intent. We ask the ex-
perts to label this attribute as True whenever
they use an outside source to provide addi-
tional information needed to label a tweet.
These sources can include news sources, past
posts, interactions, online dictionaries, or
reddit threads.

• Aggressive by Mention/Retweet Similarly,
interactions between users that are suspected
to be affiliated with the same or rival gangs
might contain aggressive intent. For exam-
ple, users may communicate with others in
the same gang about the possibility of retali-
ation or they may directly taunt members of
rival gangs. We ask the annotators to label
this attribute as True if the context of interac-
tion between users contributes to the aggres-
sion label. This corresponds to the pairwise
interaction feature in Chang et al. (2018).

• Aggressive by Picture/URL This attribute
indicates whether the picture or URL in the
tweet contains aggressive content. Since our
study is natural language processing based,
we do not make use of this piece of informa-
tion in this paper.

• Controversial Labels Occasionally our ex-
perts would interpret a tweet previously con-
sidered aggressive as non-aggressive. We do
not use these labels in our experiments for
fair comparison with Chang et al. (2018), but
we include them in our new dataset.

Out of the 329 aggression tweets, 274 contain
rationales in the tweet, while the other aggression
labels are inferred by context rather than the con-
tent within the tweet itself. To keep notations un-
cluttered, we refer to all the “aggressive by *” la-
bels as context labels in the rest of this paper. The
meta-statistics on this can be seen in table 2. Ad-
ditionally, 39 (12% of aggression) tweets are con-
sidered controversial by our experts. We also ask
the annotator to write down thoughts and observa-



4768

Tweets
“@user do you think it’s cool yo ass
slow ? You sound dumb asf”
“Rob you fucking goofys that got some to say
about rage just to show y’all bitches”
“GlizzyGang Bitch We Got Out Glocks Up

Table 1: Typical aggression tweets in our dataset,
which includes annotations with human rationales (un-
derlined).

tions while annotating, and we share these insights
in our discussion section. 1

Context Label Types Yes No N/A
Context 107 74 148

Mention/Retweet 36 35 258
URL 9 3 317

Picture 14 15 300

Table 2: Whether aggressive intent is observed in Con-
text, Mention/Retweet, URL and/or Picture. Many of
the context label cannot be annotated due to Twitter ac-
cess policy.

4 Methods

We compare the Convolutional Neural Network
(CNN) model from Chang et al. (2018), an LSTM
(Long Short Term Memory) Network model with
attention mechanism (architecture defined below)
and our new model that incorporates rationales by
trained attention (Zhong et al., 2019).2

4.1 Overview of Chang et al. (2018)
Here we give a quick overview of their CNN
model with context features. Let l denote the
length of the pre-processed tweet, T = [e1, e2...el]
be the input sequence, and c the context feature.
First each token ei is mapped to an embedded rep-
resentation wi through their domain specific word
embeddings. Then they used a convolution layer
with kernel size 1 and 2, followed by a maxpooling
layer and a linear fully connected layer to produce
the tweet representation z. Finally, they concate-
nated z with the context feature c and fed it to the

1Due to privacy concerns we cannot release the addi-
tional annotation publicly, but we will make it available to
researchers who sign an MOU specifying their intended use
of the data and their agreement with our ethical guidelines.
Send email to the dp2787@columbia.edu. Please contact the
authors of Chang et al. (2018) for the original dataset.

2 Our code is publicly available at https://github.
com/David3384/GI_2019

final classification layer. We refer the readers to
their original paper for further details.

4.2 LSTM with Attention
We use a standard LSTM model with attention
mechanism. The model encodes the embedded
sequence [w1, w2, ..., wl] with an LSTM to obtain
hidden state representation hi with hidden dimen-
sion 64 for each position i and then calculate an at-
tention weight for each position based on hi. The
final representation z is a weighted average of the
hidden states concatentated with the context fea-
ture, which we pass to the final classification layer.
Formally,

↵i = tanh(vhi), A(i) =
e↵iP
j e

↵j
(1)

z = [
lX

i=1

A(i)hi, c] (2)

where v are trainable weights we used to calculate
the attention. We used binary cross entropy loss
between the final prediction and the ground truth
label, which we denote as Lclf

4.3 Trained Attention with Rationale
We guide the model attention to attend to human
rationales. We introduce a “ground truth atten-
tion” A⇤, which is uniformly distributed over all
the positions where the token is an annotated ra-
tionale. For example, if R = {r1, r2 . . . rd} are
indexes of the tokens that are rationale words, then
the ground truth attention distribution A⇤ is de-
fined as:

A⇤(i) =
1

d
if i 2 R, 0 otherwise (3)

Then we impose a KL divergence loss component
between the ground truth attention and the model’s
attention. We optimize the weighted sum between
the label classification and the attention loss, i.e.

Lattn = KL(A⇤||A),L = Lclf +�attnLattn (4)

where we fix �attn = 4 after tuning.
To make a fair comparison with Chang et al.

(2018), we use the same set of hyper-parameters
and training procedures. We find that taking the
majority vote of an ensemble of 5 neural networks
trained on the same data can improve the F-score
by 1⇠2 points, so we report both the averaged
and the majority vote result across 5 independent

https://github.com/David3384/GI_2019
https://github.com/David3384/GI_2019


4769

runs. We also experiment with representations
from a large scale language model trained on our
unlabeled corpus (Peters et al., 2018), but it only
brought marginal improvements. For simplicity
we do not include these experiments in our paper.

5 Evaluation Metrics

Besides the macro F-score in (Chang et al., 2018),
we use rationale rank to characterize the differ-
ence between how humans and models make the
decisions. We also quantitatively evaluate how the
model uses the context feature. For fair compari-
son, we used the same labels as in (Chang et al.,
2018) even though some labels are re-labeled as
“controversial”.

5.1 Rationale Rank

We first use the leave-one-out-method to approx-
imate the influence Ii of each token ei as docu-
mented in section 2.1. If the model makes predic-
tions based on similar rationale words as human,
Ii should be large when ei is a rationale word, and
should be small when ei is not a rationale. There-
fore, a rationale word should have lower ranks if
all the tokens are sorted by its influence on model’s
prediction in descending order. We define the in-
fluence rank of the ith token rank(i) as the num-
ber of tokens in the tweet that have higher influ-
ence score than the ith token.

We further relax our requirement for the model:
we do not have the unrealistic hope that the model
is relying on every rationale word; instead, we
only consider the lowest ranking rationale. For-
mally, suppose there are multiple rationale words
with indexes R = {r1, r2..rd}, then we define the
“rationale rank” as:

rationalerank = mini2Rrank(i) (5)

We calculate the average rationale rank for all
tweets that are classified as aggression and have
rationale annotations on the cross validation test
set for each fold.

Since the majority of tweets usually has ratio-
nale rank 0, the averaged rationale rank is small
and the difference between each model is “di-
luted” by the prevailing zeros. Therefore, for each
model, we also report the percentage of tweets that
has rationale 0 or 1, respectively. This more intu-
itively reflects on what fraction of the tweets the
models’ reasoning agrees with human rationales.

5.2 Context Feature Evaluation
Chang et al. (2018) claimed that their pair-wise in-
teraction feature captures information about users’
past interactions, and they supported it by show-
ing significant improvement when incorporating it
into the CNN model. Nevertheless, it does not au-
tomatically entail that the model is using this fea-
ture in a way that humans do. Here we examine
this claim quantitatively with our annotated inter-
pretation of context.

Chang et al. (2018) concatenated the context
features before the final classification softmax
layer, so we can directly calculate the impact of
a context feature by taking its dot product with the
corresponding weight learned by the model. For-
mally, let c1 be the “emotional state” feature, c2
the “user history”, c3 the “pair-wise user interac-
tion” and the pooled tweet representation is z, then
the final prediction score is given by:

y = �(wzz +
3X

i=1

wici + b) (6)

where, w(·) are trainable weights and b the bias
term.

We group the tweets by the annotated context
label “by mention/retweet”, a single label, anno-
tated as described in section 3. If the pair-wise in-
teraction feature c3 truly models domain experts’
interpretation of user interaction as Chang et al.
(2018) has hypothesized, w3c3 should be higher
for those tweets that are annotated as “aggressive
by mention/retweet” than those that are not. We do
not examine use of other context labels and fea-
tures, since other context features do not model
URL, pictures or a general type of context.

6 Prediction Flips under Adversary

6.1 Generating Adversarial Tweets
We consider the simplest operation: inserting cer-
tain candidate neutral unigrams u 2 U = { “a”,
“on”, “da”, “into”, “of”, “that”} that the model
strongly associates with aggression. 3 Assuming
that we have already trained a language model that
gives a perplexity score p(T ) for a tweet T =
[e1, e2, . . . el], then for each u 2 U we create a

3We describe how we choose these unigrams in the ap-
pendix A.2. We also experimented with “in”, “be”, “do”,
“any”, “u”, “out”, “basically”, “yea”, “ever”, “ ”, “ ”, “we”
etc, but the results are not significant so we present them in
the appendix.



4770

dataset by inserting it into all of the tweets T in
our labeled dataset T . We describe how we train
the language model in the appendix section A.1.

For each T 2 T , there are |T | + 1 positions to
insert the unigram u. We pick the resulting tweet
T 0 with u inserted that has the highest likelihood
score p(T 0) among all the |T | + 1 possible inser-
tions, and calculate the corresponding “insertion
naturalness score” p(T 0) � p(T ). After inserting
the unigram u into all the tweets in the labeled
dataset T , we obtain a corresponding dataset T 0
and pick the top 800 tweets that have the highest
“insertion naturalness score”.

Then we calculate how many times a model
would flip its prediction from non-aggression to
aggression among these 800 pairs of tweets, be-
fore and after inserting the unigram u. Notice that
even one percent of “flip rate” is significant: the
unigrams u we consider here are not rare words
and thus have high coverage. If the model is de-
ployed at a large scale, millions of tweet will be
classified, and tens of thousands of tweets will be
influenced, not mentioning that we are consider-
ing only one unigram and there are many potential
other tokens biased by the model.

6.2 Evaluating Generation Naturalness

Since words are discrete, generating adversarial
text inputs has been a hard problem in other do-
mains for at least two reasons: 1) one perturba-
tion might be large enough to change the label. 2)
we cannot guarantee that the resulting text is still
a natural input. However, these challenges do not
play much of a role in our domain. First, the candi-
date words we are inserting are “function words”,
which we know should not cause the model to
flip their predictions. Second, the large propor-
tion of tweets we are working with are grammati-
cally flexible in the first place. Therefore, in con-
trast to genres that follow the conventions of Stan-
dard American English grammar, we suspect that
adding function words will not make the tweets
look unnatural to the domain experts and we test
this hypothesis.

To evaluate the plausibility of our generated
tweets, we sample 18 tweets from all the gener-
ated tweets and 18 existing tweets from our la-
beled dataset. Then we ask our experts to try their
best to classify whether each tweet comes from
model generation or the existing dataset. We also
ask them to annotate whether each tweet is “natu-

ral”, “reasonable but awkward” or “nonsensical”.
The experts classified 25 of them correctly (⇠

70%), only slightly better than the 50% random
guess baseline even though they are aware that we
are generating tweets by inserting function words.
Out of 18 generated tweets, they considered 7
as “natural”, 8 “reasonable but awkward” and 3
as “non-sensical”. In comparison, out of the 18
existing tweets, they considered 14 as “natural”,
1 as “reasonable but awkward” and 3 as “non-
sensical.” These results imply that although our
generated tweets might not all be fluent, they are
still reasonable enough as adversarial tweets. In-
terestingly, they also considered 3 existing tweets
as “non-sensical”, partly because the tweets are
highly grammatically flexible and we told them
that there are generated tweets.

7 Results

The model F-score performances are shown in ta-
ble 3 and rationale rank in table 4. We present our
results for macro F-score, Rationale Rank, Predic-
tion Flips on Adversarial Dataset and Context Fea-
ture Evaluation. We also evaluate the models from
(Blevins et al., 2016) for completeness. 4

F-score We first notice that taking the ensem-
ble of 5 independent runs trained on the same data
consistently improves the performance over aver-
aged individual run performance, so we report all
of our results on ensemble models. Compared to
Chang et al. (2018), our LSTM+Rationale model
differs by at most one point on all of the macro-
F score metrics, and sometimes achieves the best
performance.

Rationale Rank A closer look at the ratio-
nale rank result reveals a huge difference (p-value
< 10�5): our model significantly beats all other
models on this metrics.

Prediction Flips under Adversary The results
are shown in table 5. For each candidate uni-
gram we count the number of times the model flips
its prediction from non-aggression to aggression
among the top 800 insertions. By incorporating
rationales into the model, we achieve the fewest
flips for half of the unigrams and under none of
the six cases does it flip the most predictions.

These two results demonstrate that although the
CNN model (Chang et al., 2018) only slightly un-

4For consistency, we used their model but first classify
aggression vs. rest, rather than other vs. rest as in (Blevins
et al., 2016). The training procedure is deterministic, so the
ensemble gives the same performance as individual ones.



4771

Model Average
Macro F1

Ensemble
Agg. F1

Ensemble
Loss F1

Ensemble
Other F1

Ensemble
Macro F1

Blevins et al. (2016) 63.5 34.5 67.3 88.7 63.5
Chang et al. (2018) 68.2 41.3 75.4 91.3 69.4

CNN + Twitter 65.8 36.2 73.9 91.3 67.1
LSTM 67.0 38.4 74.2 91.3 68.0

LSTM + Rationale 68.2 43.0 75.0 91.6 69.8

Table 3: Results comparing different model architectures. Chang et al. (2018) refers to the full CNN model with
domain specific embedding and context feature; “CNN Twitter” replaces the domain specific embedding with
GloVe twitter embeddings (Pennington et al., 2014); LSTM refers to LSTM Attention described in section 4.2;
LSTM Rationale refers to the same LSTM Attention with trained attention described in section 4.3. For each
column, the best performing entry is boldered.

Model Avg
RR

RR
= 0

RR
= 1

Blevins et al. (2016) 1.70 0.60 0.13
Chang et al. (2018) 1.42 0.54 0.17

CNN + Twitter 1.82 0.43 0.25
LSTM 1.73 0.50 0.15

LSTM + Rationale 0.86 0.69 0.14

Table 4: Results on (averaged) rationale rank (RR),
fraction of tweets when RR = 0 and RR = 1, respec-
tively. For (averaged) rationale rank and RR = 0, the
best performing entry is boldered. We do not bolder
the column RR = 1 since it is sub-optimal.

derperforms our model in terms of F-score per-
formance, it behaves very differently behind the
scene. Our model is more robust and makes pre-
dictions that match human predictions. This im-
plies that traditional F-score measure alone neither
provides us deeper understanding of the model,
nor reveals the underlying biases.

Context Feature Evaluation We calculate the
correlation between “how the human uses con-
text” and “how the model uses context” as de-
scribed in section 5.2. The mean difference of
w3c3 between the two groups split by the label
value is 1.61 with statistical significance 10�3, in-
dicating that the pairwise interaction feature cor-
relates well with how domain experts interpret
users’ retweet/mentions.

8 Related Work

Patton et al. (2013) initiated a new field of study,
coining the term internet banging to describe the
phenomenon whereby online altercations lead to
offline violence. This work is further motivated by
challenging gun violence statistics in the city of
Chicago which saw a 58% spike between 2015 and

2016. In addition, the number of homicides stem-
ming from physical altercations decreased during
that period with little empirical evidence to ex-
plain this shift (Kapustin et al., 2016). Investigat-
ing internet banging represents an important first
step in understanding why gun deaths increase de-
spite fewer physical altercations.

Blevins et al. (2016) is a collaboaration between
social work and computer science experts that
brings together social work’s substantive knowl-
edge of communities, youth development, and gun
violence with data science’s innovative machine
learning tools. They developed a new analyti-
cal system that leverages community participatory
methods and machine learning to automatically
detect the expression of loss and aggression on
Twitter. Blevins et al. (2016) collected and anno-
tated tweets posted by Gakirah Barnes, a deceased
gang members prolific on twitter, as well as her
top communicators. Chang et al. (2018) extended
this dataset to be six times larger and achieved the
state-of-the-art result by using a CNN.

Our interpretability method is most similar to
the leave-one-out (LOO) approach by Ribeiro
et al. (2016). Nevertheless, it risks making the sen-
tence ungrammatical, hence producing “rubbish
examples” (Goodfellow et al., 2014) and patholog-
ical interpretations (Feng et al., 2018). We explic-
itly address this concern by checking the second-
order consistency proposed in Feng et al. (2018).
To further ensure that the removal of a word results
in a “blank state”, erasure-based method proposed
in Li et al. (2016) can be used. It would also be in-
teresting to explore different interpretability meth-
ods, such as using the influence function (Koh
and Liang, 2017) or nearest neighbor in the fea-
ture space (Strobelt et al., 2019). Our method of
automatically generating adversarial data accord-



4772

Models
Unigrams a on da into of that

Blevins et al. (2016) 44.16! 121.00! 37.44! 117.72! 36.84! 74.48 !

Chang et al. (2018) 26.56 36.00 8.52⇤ 21.68 19.20 7.52⇤

CNN + Twitter 38.40 35.80 17.60 15.88 5.24⇤ 7.64
LSTM 16.16 40.28 21.52 26.92 13.20 13.12

LSTM + Rationale 13.52⇤ 23.16⇤ 12.40 10.28⇤ 11.40 8.96

Table 5: The number of model’s prediction flip from non-aggressive to aggressive, out of the 800 attacking tweets
generated by inserting a specified “neutral” unigram. To obtain a stable estimate, the count of number of flips of
each model is averaged across 5 model runs and models trained on 5 folds - thus leading to non-integer results. For
each column, the worst performing entry is marked with “!” and best with “*”.

ing to a pre-identified weakness of a model with-
out accessing the model output and parameters is
most similar to Jia and Liang (2017). From Alzan-
tot et al. (2018), we borrowed the idea of using
a language model to ensure naturalness of gen-
erated sentences, and it is a promising next step
to collect natural human-generated adversarial ex-
amples (Wallace et al., 2018). In the future, study-
ing white-box (Ebrahimi et al., 2017; Bělohlávek,
2017) and character level attacks (Gröndahl et al.,
2018) are important directions to provide addi-
tional robustness in this critical domain.

Trained attention was explored in event detec-
tion (Liu et al., 2017), machine translation (Mi
et al., 2016) and sentiment analysis (Zhong et al.,
2019). More sophisticated approaches include de-
riving attention from human rationales (Yujia Bao,
2018). Finding better ways to make use of this
kind of information is a promising avenue, espe-
cially in this low-resource domain.

Ensuring machine learning fairness has in-
creasingly caught researchers’ attention (Corbett-
Davies and Goel, 2018) since algorithms can cre-
ate concerning negative societal impacts (Feller
et al., 2016). Biases might be based on gender
(Bolukbasi et al., 2016; Zhao et al., 2017), race
(Kiritchenko and Mohammad, 2018; Rudinger
et al., 2017), writing styles (Madnani et al., 2017),
or other factors. More broadly, bias can also oc-
cur when annotators label the data. For example,
Golbeck et al. (2017) found it hard to distinguish
between mere offensive speech and hate speech.

9 Discussion

9.1 Limitations of the Annotation Instruction
The instructions for annotating rationales in tweets
do not fully recognize the complex nature of lan-
guage. Current annotation reduces the experts’

underlying reason to annotate a tweet as aggres-
sion to a set of key words, which is not flexible
enough to convey more complicated inferences.
The experts’ analyses are far more complex: they
use context to disambiguate the meaning of the ra-
tionale words rather than identifying key words.
Since the language is hyper-local and many of the
non-aggressive tweets might appear aggressive in
outsiders’ eyes, interpreting the context is a cru-
cial component while determining an aggression
label and it is important to develop more flexible
context rationale representations in the future.

Take, for example, the post “I talk to alotta Da
Guyz In jail Everyday On l’a Dese N***as Ain’t
Sending Bro Nem A Dime But Steady Yellin
Free Em Goof ass”. At face value, the words ” ”,
“goof”, and “ass” per se do not capture the reason
why this post is aggressive; the aggression label
for this post in fact derives from the poster calling
out the dishonesty of those who support incarcer-
ated individuals in principle, without supporting
them monetarily.

9.2 Controversial Labels
In the annotation procedure, it can be challenging
to choose a single label when users express both
anger and grief in a single post. In these instances,
we aim to capture and annotate the tweet’s domi-
nant theme.

For example, the post “Fuck Da System Fuck
Da State Fuck 12z ” contains elements of
aggression–the word “fuck” and the middle fin-
ger emoji–and loss–the sad frown emoji and the
post’s larger context. This post fits the aggression
label, by challenging authority and insulting, and
the loss label, by evoking incarceration. However,
a loss label is more suitable, because: 1) though
the anger seems to target specific players, the text
as a whole reveals that the poster is highlighting



4773

institutional violence; and 2) the sad frown emoji,
paired with the context that one of the poster’s
loved ones was recently arrested on a drug charge
(”12z” refers to law enforcement’s drug unit), ce-
ments that the user is grieving in this post.

10 Conclusions and Future Directions

In this paper, we interpret the previous state-of-
the-art model, find it is biased against stop words,
create a dataset of rationales, develop metrics to
measure the difference between how models and
humans make predictions, design an adversarial
approach to test the robustness of the model, and
propose a neural network architecture to incorpo-
rate rationales. Our work has an important mes-
sage to the broader NLP research community that
develops tools for high stakes domains: the dom-
inant evaluation methods in current research ap-
proaches, F-score and qualitative error analysis,
are far from enough to develop good systems that
might potentially lead to bad social consequences.
We need various different approaches to evaluate
and build a model, and each of them requires fur-
ther research for improvement:

Interpretability: A solid understanding of
model behavior can inform researchers of the po-
tential weaknesses and guide the process of devel-
oping corresponding solutions. When trying to in-
terpret Chang et al. (2018)’s deep learning model,
we find it biased against many function words.
Still, interpretation of neural network is an ongo-
ing research topic; even though we verified “sec-
ond order consistency”, future work is needed for
better and more faithful explanations.

Robustness under Adversary: It is better to
break the system before it is broken when it is de-
ployed. We find the previous model to be suscep-
tible to insertions of certain function words, while
our model is more robust against them. Neverthe-
less, the model is not winning every column in ta-
ble 5, so there is still room for improvement.

Metrics Evaluating How Predictions are Made:
We need to quantitatively evaluate the difference
between how models and humans classify tweets.
We develop the metrics “rationale rank” and “pre-
diction flips under adversary”, and verify that our
model behaves more similar to domain experts. F-
score does not reflect this important improvement.
Additionally, uni-gram (lexical level) influence is
only a shallow approximation of human reasoning
- a process that involves understanding the seman-

tics of a tweet as a whole and analyzing its prag-
matics in the real-world context. Ideally we hope
to find a formal language to approximate such a
process and compare it against the model’s inter-
pretation.

Models that Incorporate Rationales: Our new
model, though simple and straightforward, suc-
cessfully makes use of the rationales and out-
performs the previous model under most robust-
ness metrics proposed in this paper; we believe
that more sophisticated modeling of rationales can
push the performances further, both in terms of ac-
curacy and robustness.

Feature Evaluation: Performance improvement
caused by additional features does not entail that
they are used by the model in an expected way.
We verify the claim of Chang et al. (2018) that
the pairwise interaction feature does capture user
retweet/mention interactions.

Domain Experts: In our domain, stakes are high
and user behavior is frequently subject to biases
and misinterpretations. Therefore, we ask domain
experts rather than crowd workers to extend the
annotation, and ask them to provide further in-
sights about a domain that computer scientists are
usually not at all familiar with. Furthermore, we
should keep a critical eye on the labels and be
aware of the limitations of our evaluation metrics
for model accuracy. It is convenient for computer
scientists to regard the labels as ground truth, the
dataset as static and try different models to im-
prove the f-score. However, as domain experts
gained a deeper understanding about the twitter
users, they marked 12% of the aggression labels
as controversial (section 3); the annotation may
evolve over time. Indeed, we need a consistent
method and dataset to compare different models
fairly, but we should also keep in mind that f-score
is a proxy and might not necessarily reflect an im-
provement in the real-world.

References
Moustafa Alzantot, Yash Sharma, Ahmed Elgohary,

Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.
2018. Generating natural language adversarial ex-
amples. arXiv preprint arXiv:1804.07998.

Petr Bělohlávek. 2017. Using adversarial examples in
natural language processing.

Terra Blevins, Robert Kwiatkowski, Jamie Macbeth,
Kathleen McKeown, Desmond Patton, and Owen
Rambow. 2016. Automatically processing tweets



4774

from gang-involved youth: towards detecting loss
and aggression. In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 2196–2206.

Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,
Venkatesh Saligrama, and Adam T Kalai. 2016.
Man is to computer programmer as woman is to
homemaker? debiasing word embeddings. In Ad-
vances in neural information processing systems,
pages 4349–4357.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
arXiv preprint arXiv:1508.05326.

Serina Chang, Ruiqi Zhong, Ethan Adams, Fei-Tzin
Lee, Siddharth Varia, Desmond Patton, William R.
Frey, Chris Kedzie, and Kathy McKeown. 2018.
Detecting gang-involved escalation on social media
using context. In EMNLP.

Sam Corbett-Davies and Sharad Goel. 2018. The
measure and mismeasure of fairness: A critical
review of fair machine learning. arXiv preprint
arXiv:1808.00023.

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and De-
jing Dou. 2017. Hotflip: White-box adversarial
examples for text classification. arXiv preprint
arXiv:1712.06751.

Avi Feller, Emma Pierson, Sam Corbett-Davies, and
Sharad Goel. 2016. A computer program used for
bail and sentencing decisions was labeled biased
against blacks. its actually not that clear. The Wash-
ington Post.

Shi Feng, Eric Wallace, II Grissom, Mohit Iyyer, Pe-
dro Rodriguez, Jordan Boyd-Graber, et al. 2018.
Pathologies of neural models make interpretations
difficult. arXiv preprint arXiv:1804.07781.

Jennifer Golbeck, Zahra Ashktorab, Rashad O Banjo,
Alexandra Berlinger, Siddharth Bhagwan, Cody
Buntain, Paul Cheakalos, Alicia A Geller, Quint
Gergory, Rajesh Kumar Gnanasekaran, et al. 2017.
A large labeled corpus for online harassment re-
search. In Proceedings of the 2017 ACM on Web
Science Conference, pages 229–233. ACM.

Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. 2014. Explaining and harnessing adver-
sarial examples. arXiv preprint arXiv:1412.6572.

Tommi Gröndahl, Luca Pajola, Mika Juuti, Mauro
Conti, and N Asokan. 2018. All you need is” love”:
Evading hate-speech detection. arXiv preprint
arXiv:1808.09115.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
arXiv preprint arXiv:1707.07328.

Max Kapustin, Jens Ludwig, Marc Punkay, Kimber-
ley Smith, Lauren Speigel, and David Welgus. 2016.
Gun violence in chicago, 2016.

Svetlana Kiritchenko and Saif M Mohammad. 2018.
Examining gender and race bias in two hun-
dred sentiment analysis systems. arXiv preprint
arXiv:1805.04508.

Pang Wei Koh and Percy Liang. 2017. Understand-
ing black-box predictions via influence functions.
In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pages 1885–1894.
JMLR. org.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un-
derstanding neural networks through representation
erasure. arXiv preprint arXiv:1612.08220.

Shulin Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2017.
Exploiting argument information to improve event
detection via supervised attention mechanisms. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1789–1798.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142–150, Port-
land, Oregon, USA. Association for Computational
Linguistics.

Nitin Madnani, Anastassia Loukina, Alina von Davier,
Jill Burstein, and Aoife Cahill. 2017. Building bet-
ter open-source tools to support fairness in auto-
mated scoring. In Proceedings of the First ACL
Workshop on Ethics in Natural Language Process-
ing, pages 41–52.

Haitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016.
Supervised attentions for neural machine translation.
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.

Desmond U Patton, Sadiq Patel, Jun Sung Hong,
Megan L Ranney, Marie Crandall, and Lyle Dungy.
2017. Tweets, gangs, and guns: a snapshot of gang
communications in detroit. Violence and victims,
32(5):919–934.

Desmond Upton Patton, Robert D Eschmann, and
Dirk A Butler. 2013. Internet banging: New trends
in social media, gang violence, masculinity and hip
hop. Computers in Human Behavior, 29(5):A54–
A59.

Desmond Upton Patton, Robert D Eschmann, Caitlin
Elsaesser, and Eddie Bocanegra. 2016. Sticks,
stones and facebook accounts: What violence out-
reach workers know about social media and urban-
based gang violence in chicago. Computers in hu-
man behavior, 65:591–600.

http://www.aclweb.org/anthology/P11-1015
http://www.aclweb.org/anthology/P11-1015


4775

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2016. Why should i trust you?: Explain-
ing the predictions of any classifier. In Proceed-
ings of the 22nd ACM SIGKDD international con-
ference on knowledge discovery and data mining,
pages 1135–1144. ACM.

Rachel Rudinger, Chandler May, and Benjamin
Van Durme. 2017. Social bias in elicited natural lan-
guage inferences. In Proceedings of the First ACL
Workshop on Ethics in Natural Language Process-
ing, pages 74–79.

Hendrik Strobelt, Sebastian Gehrmann, Michael
Behrisch, Adam Perer, Hanspeter Pfister, and
Alexander M Rush. 2019. S eq 2s eq-v is: A vi-
sual debugging tool for sequence-to-sequence mod-
els. IEEE transactions on visualization and com-
puter graphics, 25(1):353–363.

Eric Wallace, Pedro Rodriguez, Shi Feng, and Jordan
Boyd-Graber. 2018. Trick me if you can: Adver-
sarial writing of trivia challenge questions. arXiv
preprint arXiv:1809.02701.

Mo Yu Regina Barzilay Yujia Bao, Shiyu Chang. 2018.
Deriving machine attention from human rationales.
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente
Ordonez, and Kai-Wei Chang. 2017. Men also
like shopping: Reducing gender bias amplifica-
tion using corpus-level constraints. arXiv preprint
arXiv:1707.09457.

Ruiqi Zhong, Steven Shao, and Kathleen McKeown.
2019. Fine-grained sentiment analysis with faithful
attention. arXiv preprint arXiv:1908.06870.


