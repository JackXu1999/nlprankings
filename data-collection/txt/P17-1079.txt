



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 850–860
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1079

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 850–860
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1079

Neural Machine Translation via Binary Code Prediction

Yusuke Oda† Philip Arthur† Graham Neubig‡† Koichiro Yoshino†§ Satoshi Nakamura†
† Nara Institute of Science and Technoloty, 8916-5 Takayama-cho, Ikoma, Nara 630-0192, Japan

‡ Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA
§ Japan Science and Technology Agency, 4-1-8 Hon-machi, Kawaguchi, Saitama 332-0012, Japan
{oda.yusuke.on9, philip.arthur.om0}@is.naist.jp, gneubig@cs.cmu.edu,

{koichiro, s-nakamura}@is.naist.jp

Abstract

In this paper, we propose a new method
for calculating the output layer in neural
machine translation systems. The method
is based on predicting a binary code for
each word and can reduce computation
time/memory requirements of the output
layer to be logarithmic in vocabulary size
in the best case. In addition, we also intro-
duce two advanced approaches to improve
the robustness of the proposed model: us-
ing error-correcting codes and combining
softmax and binary codes. Experiments
on two English ↔ Japanese bidirectional
translation tasks show proposed models
achieve BLEU scores that approach the
softmax, while reducing memory usage to
the order of less than 1/10 and improving
decoding speed on CPUs by x5 to x10.

1 Introduction

When handling broad or open domains, machine
translation systems usually have to handle a large
vocabulary as their inputs and outputs. This is par-
ticularly a problem in neural machine translation
(NMT) models (Sutskever et al., 2014), such as
the attention-based models (Bahdanau et al., 2014;
Luong et al., 2015) shown in Figure 1. In these
models, the output layer is required to generate a
specific word from an internal vector, and a large
vocabulary size tends to require a large amount of
computation to predict each of the candidate word
probabilities.

Because this is a significant problem for neural
language and translation models, there are a num-
ber of methods proposed to resolve this problem,
which we detail in Section 2.2. However, none
of these previous methods simultaneously satisfies
the following desiderata, all of which, we argue,
are desirable for practical use in NMT systems:

Figure 1: Encoder-decoder-attention NMT model
and computation amount of the output layer.

Memory efficiency: The method should not re-
quire large memory to store the parameters
and calculated vectors to maintain scalability
in resource-constrained environments.

Time efficiency: The method should be able to
train the parameters efficiently, and possible
to perform decoding efficiently with choos-
ing the candidate words from the full proba-
bility distribution. In particular, the method
should be performed fast on general CPUs to
suppress physical costs of computational re-
sources for actual production systems.

Compatibility with parallel computation: It
should be easy for the method to be mini-
batched and optimized to run efficiently on
GPUs, which are essential for training large
NMT models.

In this paper, we propose a method that satis-
fies all of these conditions: requires significantly
less memory, fast, and is easy to implement mini-
batched on GPUs. The method works by not pre-
dicting a softmax over the entire output vocab-

850

https://doi.org/10.18653/v1/P17-1079
https://doi.org/10.18653/v1/P17-1079


ulary, but instead by encoding each vocabulary
word as a vector of binary variables, then indepen-
dently predicting the bits of this binary represen-
tation. In order to represent a vocabulary size of
2n, the binary representation need only be at least
n bits long, and thus the amount of computation
and size of parameters required to select an output
word is only O(log V ) in the size of the vocabu-
lary V , a great reduction from the standard linear
increase of O(V ) seen in the original softmax.

While this idea is simple and intuitive, we found
that it alone was not enough to achieve competitive
accuracy with real NMT models. Thus we make
two improvements: First, we propose a hybrid
model, where the high frequency words are pre-
dicted by a standard softmax, and low frequency
words are predicted by the proposed binary codes
separately. Second, we propose the use of con-
volutional error correcting codes with Viterbi de-
coding (Viterbi, 1967), which add redundancy to
the binary representation, and even in the face of
localized mistakes in the calculation of the repre-
sentation, are able to recover the correct word.

In experiments on two translation tasks, we find
that the proposed hybrid method with error correc-
tion is able to achieve results that are competitive
with standard softmax-based models while reduc-
ing the output layer to a fraction of its original size.

2 Problem Description and Prior Work

2.1 Formulation and Standard Softmax
Most of current NMT models use one-hot repre-
sentations to represent the words in the output vo-
cabulary – each word w is represented by a unique
sparse vector eid(w) ∈ RV , in which only one ele-
ment at the position corresponding to the word ID
id(w) ∈ {x ∈ N | 1 ≤ x ≤ V } is 1, while oth-
ers are 0. V represents the vocabulary size of the
target language. NMT models optimize network
parameters by treating the one-hot representation
eid(w) as the true probability distribution, and min-
imizing the cross entropy between it and the soft-
max probability v:

LH(v, id(w)) := H(eid(w),v), (1)
= log sum expu− uid(w), (2)

v := expu/ sum expu, (3)

u := Whuh+ βu, (4)

where sumx represents the sum of all elements
in x, xi represents the i-th element of x, Whu ∈

RV×H and βu ∈ RV are trainable parameters and
H is the total size of hidden layers directly con-
nected to the output layer.

According to Equation (4), this model clearly
requires time/space computation in proportion to
O(HV ), and the actual load of the computation of
the output layer is directly affected by the size of
vocabulary V , which is typically set around tens
of thousands (Sutskever et al., 2014).

2.2 Prior Work on Suppressing Complexity
of NMT Models

Several previous works have proposed methods to
reduce computation in the output layer. The hi-
erarchical softmax (Morin and Bengio, 2005) pre-
dicts each word based on binary decision and re-
duces computation time to O(H log V ). However,
this method still requires O(HV ) space for the
parameters, and requires calculation much more
complicated than the standard softmax, particu-
larly at test time.

The differentiated softmax (Chen et al., 2016)
divides words into clusters, and predicts words us-
ing separate part of the hidden layer for each word
clusters. This method make the conversion matrix
of the output layer sparser than a fully-connected
softmax, and can reduce time/space computation
amount by ignoring zero part of the matrix. How-
ever, this method restricts the usage of hidden
layer, and the size of the matrix is still in propor-
tion to V .

Sampling-based approximations (Mnih and
Teh, 2012; Mikolov et al., 2013) to the denomina-
tor of the softmax have also been proposed to re-
duce calculation at training. However, these meth-
ods are basically not able to be applied at test time,
still require heavy computation like the standard
softmax.

Vocabulary selection approaches (Mi et al.,
2016; L’Hostis et al., 2016) can also reduce the
vocabulary size at testing, but these methods aban-
don full search over the target space and the
quality of picked vocabularies directly affects the
translation quality.

Other methods using characters (Ling et al.,
2015) or subwords (Sennrich et al., 2016; Chitnis
and DeNero, 2015) can be applied to suppress the
vocabulary size, but these methods also make for
longer sequences, and thus are not a direct solution
to problems of computational efficiency.

851



Figure 2: Designs of output layers.

3 Binary Code Prediction Models

3.1 Representing Words using Bit Arrays

Figure 2(a) shows the conventional softmax pre-
diction, and Figure 2(b) shows the binary code
prediction model proposed in this study. Unlike
the conventional softmax, the proposed method
predicts each output word indirectly using dense
bit arrays that correspond to each word. Let
b(w) := [b1(w), b2(w), · · · , bB(w)] ∈ {0, 1}B be
the target bit array obtained for word w, where
each bi(w) ∈ {0, 1} is an independent binary
function given w, and B is the number of bits in
whole array. For convenience, we introduce some
constraints on b. First, a wordw is mapped to only
one bit array b(w). Second, all unique words can
be discriminated by b, i.e., all bit arrays satisfy
that:1

id(w) 6= id(w′)⇒ b(w) 6= b(w′). (5)

Third, multiple bit arrays can be mapped to the
same word as described in Section 3.5. By
considering second constraint, we can also con-
strain B ≥ dlog2 V e, because b should have
at least V unique representations to distinguish
each word. The output layer of the network in-
dependently predicts B probability values q :=
[q1(h), q2(h), · · · , qB(h)] ∈ [0, 1]B using the

1We designed this injective condition using the id(·) func-
tion to ignore task-specific sensitivities between different
word surfaces (e.g. cases, ligatures, etc.).

current hidden values h by logistic regressions:

q(h) = σ(Whqh+ βq), (6)

σ(x) := 1/(1 + exp(−x)), (7)
where Whq ∈ RB×H and βq ∈ RB are train-
able parameters. When we assume that each qi
is the probability that “the i-th bit becomes 1,” the
joint probability of generating word w can be rep-
resented as:

Pr(b(w)|q(h)) :=
B∏

i=1

(
biqi + b̄iq̄i

)
, (8)

where x̄ := 1 − x. We can easily obtain the
maximum-probability bit array from q by simply
assuming the i-th bit is 1 if qi ≥ 1/2, or 0 other-
wise. However, this calculation may generate in-
valid bit arrays which do not correspond to actual
words according to the mapping between words
and bit arrays. For now, we simply assume that
w = UNK (unknown) when such bit arrays are ob-
tained, and discuss alternatives later in Section 3.5.

The constraints described here are very general
requirements for bit arrays, which still allows us to
choose between a wide variety of mapping func-
tions. However, designing the most appropriate
mapping method for NMT models is not a triv-
ial problem. In this study, we use a simple map-
ping method described in Algorithm 1, which was
empirically effective in preliminary experiments.2

Here, V is the set of V target words including 3
extra markers: UNK, BOS (begin-of-sentence), and
EOS (end-of-sentence), and rank(w) ∈ N>0 is
the rank of the word according to their frequen-
cies in the training corpus. Algorithm 1 is one
of the minimal mapping methods (i.e., satisfying
B = dlog2 V e), and generated bit arrays have the
characteristics that their higher bits roughly repre-
sents the frequency of corresponding words (e.g.,
if w is frequently appeared in the training corpus,
higher bits in b(w) tend to become 0).

3.2 Loss Functions
For learning correct binary representations, we can
use any loss functions that is (sub-)differentiable
and satisfies a constraint that:

LB(q, b)
{

= �L, if q = b,
≥ �L, otherwise, (9)

2Other methods examined included random codes, Huff-
man codes (Huffman, 1952) and Brown clustering (Brown
et al., 1992) with zero-padding to adjust code lengths, and
some original allocation methods based on the word2vec em-
beddings (Mikolov et al., 2013).

852



Algorithm 1 Mapping words to bit arrays.
Require: w ∈ V
Ensure: b ∈ {0, 1}B = Bit array representing w

x :=





0, if w = UNK
1, if w = BOS
2, if w = EOS
2 + rank(w), otherwise

bi := bx/2i−1c mod 2
b← [b1, b2, · · · , bB ]

where �L is the minimum value of the loss func-
tion which typically does not affect the gradi-
ent descent methods. For example, the squared-
distance:

LB(q, b) :=
B∑

i=1

(qi − bi)2, (10)

or the cross-entropy:

LB(q, b) := −
B∑

i=1

(
bi log qi + b̄i log q̄i

)
, (11)

are candidates for the loss function. We also
examined both loss functions in the preliminary
experiments, and in this paper, we only used
the squared-distance function (Equation (10)), be-
cause this function achieved higher translation ac-
curacies than Equation (11).3

3.3 Efficiency of the Binary Code Prediction
The computational complexity for the parame-
ters Whq and βq is O(HB). This is equal
to O(H log V ) when using a minimal mapping
method like that shown in Algorithm 1, and is sig-
nificantly smaller than O(HV ) when using stan-
dard softmax prediction. For example, if we chose
V = 65536 = 216 and use Algorithm 1’s mapping
method, then B = 16 and total amount of com-
putation in the output layer could be suppressed to
1/4096 of its original size.

On a different note, the binary code prediction
model proposed in this study shares some ideas
with the hierarchical softmax (Morin and Bengio,
2005) approach. Actually, when we used a binary-
tree based mapping function for b, our model can
be interpreted as the hierarchical softmax with two

3In terms of learning probabilistic models, we should re-
mind that using Eq. (10) is an approximation of Eq. (11). The
output bit scores trained by Eq. (10) do not represent actual
word perplexities, and this characteristics imposes some prac-
tical problems when comparing multiple hypotheses (e.g.,
reranking, beam search, etc.). We could ignore this problem
in this paper because we only evaluated the one-best results
in experiments.

strong constraints for guaranteeing independence
between all bits: all nodes in the same level of the
hierarchy share their parameters, and all levels of
the hierarchy are predicted independently of each
other. By these constraints, all bits in b can be
calculated in parallel. This is particularly impor-
tant because it makes the model conducive to be-
ing calculated on parallel computation backends
such as GPUs.

However, the binary code prediction model also
introduces problems of robustness due to these
strong constraints. As the experimental results
show, the simplest prediction model which di-
rectly maps words into bit arrays seriously de-
creases translation quality. In Sections 3.4 and 3.5,
we introduce two additional techniques to prevent
reductions of translation quality and improve ro-
bustness of the binary code prediction model.

3.4 Hybrid Softmax/Binary Model

According to the Zipf’s law (Zipf, 1949), the dis-
tribution of word appearances in an actual cor-
pus is biased to a small subset of the vocabu-
lary. As a result, the proposed model mostly
learns characteristics for frequent words and can-
not obtain enough opportunities to learn for rare
words. To alleviate this problem, we introduce a
hybrid model using both softmax prediction and
binary code prediction as shown in Figure 2(c).
In this model, the output layer calculates a stan-
dard softmax for the N − 1 most frequent words
and an OTHER marker which indicates all rare
words. When the softmax layer predicts OTHER,
then the binary code layer is used to predict the
representation of rare words. In this case, the ac-
tual probability of generating a particular word can
be separated into two equations according to the
frequency of words:

Pr(w|h) '
{
v′id(w), if id(w) < N,
v′N · π(w,h), otherwise,

(12)

v′ := expu′/ sum expu′, (13)

u′ := Whu′h+ βu′ , (14)

π(w,h) := Pr(b(w)|q(h)), (15)

where Whu′ ∈ RN×H and βu′ ∈ RN are trainable
parameters, and id(w) assumes that the value cor-
responds to the rank of frequency of each word.
We also define the loss function for the hybrid

853



Figure 3: Example of the classification problem
using redundant bit array mapping.

model using both softmax and binary code losses:

L :=

{
lH(id(w)), if id(w) < N,
lH(N) + lB, otherwise,

(16)

lH(i) := λHLH(v′, i), (17)

lB := λBLB(q, b), (18)

where λH and λB are hyper-parameters to deter-
mine strength of both softmax/binary code losses.
These also can be adjusted according to the train-
ing data, but in this study, we only used λH =
λB = 1 for simplicity.

The computational complexity of the hybrid
model is O(H(N + log V )), which is larger than
the original binary code modelO(H log V ). How-
ever,N can be chosen asN � V because the soft-
max prediction is only required for a few frequent
words. As a result, we can control the actual com-
putation for the hybrid model to be much smaller
than the standard softmax complexity O(HV ),

The idea of separated prediction of frequent
words and rare words comes from the differenti-
ated softmax (Chen et al., 2016) approach. How-
ever, our output layer can be configured as a fully-
connected network, unlike the differentiated soft-
max, because the actual size of the output layer is
still small after applying the hybrid model.

3.5 Applying Error-correcting Codes
The 2 methods proposed in previous sections im-
pose constraints for all bits in q, and the value of
each bit must be estimated correctly for the cor-
rect word to be chosen. As a result, these models
may generate incorrect words due to even a sin-
gle bit error. This problem is the result of dense
mapping between words and bit arrays, and can
be avoided by creating redundancy in the bit ar-
ray. Figure 3 shows a simple example of how this
idea works when discriminating 2 words using 3
bits. In this case, the actual words are obtained by

Figure 4: Training and generation processes with
error-correcting code.

estimating the nearest centroid bit array accord-
ing to the Hamming distance between each cen-
troid and the predicted bit array. This approach
can predict correct words as long as the predicted
bit arrays are in the set of neighbors for the cor-
rect centroid (gray regions in the Figure 3), i.e.,
up to a 1-bit error in the predicted bits can be cor-
rected. This ability to be robust to errors is a cen-
tral idea behind error-correcting codes (Shannon,
1948). In general, an error-correcting code has the
ability to correct up to b(d−1)/2c bit errors when
all centroids differ d bits from each other (Golay,
1949). d is known as the free distance determined
by the design of error-correcting codes. Error-
correcting codes have been examined in some pre-
vious work on multi-class classification tasks, and
have reported advantages from the raw classifica-
tion (Dietterich and Bakiri, 1995; Klautau et al.,
2003; Liu, 2006; Kouzani and Nasireding, 2009;
Kouzani, 2010; Ferng and Lin, 2011, 2013). In
this study, we applied an error-correcting algo-
rithm to the bit array obtained from Algorithm 1
to improve robustness of the output layer in an
NMT system. A challenge in this study is try-
ing a large classification (#classes > 10,000) with
error-correction, unlike previous studies focused
on solving comparatively small tasks (#classes <
100). And this study also tries to solve a genera-
tion task unlike previous studies. As shown in the
experiments, we found that this approach is highly
effective in these tasks.

Figure 4 (a) and (b) illustrate the training and
generation processes for the model with error-
correcting codes. In the training, we first con-
vert the original bit arrays b(w) to a center bit
array b′ in the space of error-correcting code:
b′(b) := [b′1(b), b

′
2(b), · · · , b′B′(b)] ∈ {0, 1}B

′
,

where B′(B) ≥ B is the number of bits in the
error-correcting code. The NMT model learns its

854



Algorithm 2 Encoding into a convolutional code.
Require: b ∈ {0, 1}B
Ensure: b′ ∈ {0, 1}2(B+6) =

Redundant bit array

x[t] :=

{
bt, if 1 ≤ t ≤ B
0, otherwise

y1t := x[t− 6 .. t] · [1001111] mod 2
y2t := x[t− 6 .. t] · [1101101] mod 2
b′ ← [y11, y21, y12, y22, · · · , y1B+6, y2B+6]

parameters based on the loss between predicted
probabilities q and b′. Note that typical error-
correcting codes satisfy O(B′/B) = O(1), and
this characteristic efficiently suppresses the in-
crease of actual computation cost in the output
layer due to the application of the error-correcting
code. In the generation of actual words, the decod-
ing method of the error-correcting code converts
the redundant predicted bits q into a dense rep-
resentation q̃ := [q̃1(q), q̃2(q), · · · , q̃B(q)], and
uses q̃ as the bits to restore the word, as is done in
the method described in the previous sections.

It should be noted that the method for perform-
ing error correction directly affects the quality of
the whole NMT model. For example, the map-
ping shown in Figure 3 has only 3 bits and it is
clear that these bits represent exactly the same in-
formation as each other. In this case, all bits can
be estimated using exactly the same parameters,
and we can not expect that we will benefit signif-
icantly from applying this redundant representa-
tion. Therefore, we need to choose an error correc-
tion method in which the characteristics of origi-
nal bits should be distributed in various positions
of the resulting bit arrays so that errors in bits are
not highly correlated with each-other. In addition,
it is desirable that the decoding method of the ap-
plied error-correcting code can directly utilize the
probabilities of each bit, because q generated by
the network will be a continuous probabilities be-
tween zero and one.

In this study, we applied convolutional codes
(Viterbi, 1967) to convert between original and re-
dundant bits. Convolutional codes perform a set
of bit-wise convolutions between original bits and
weight bits (which are hyper-parameters). They
are well-suited to our setting here because they
distribute the information of original bits in dif-
ferent places in the resulting bits, work robustly
for random bit errors, and can be decoded using

Algorithm 3 Decoding from a convolutional code.
Require: q ∈ (0, 1)2(B+6)
Ensure: q̃ ∈ {0, 1}B = Restored bit array
g(q, b) := b log q + (1− b) log(1− q)
φ0[s | s ∈ {0, 1}6]←

{
0, if s = [000000]
−∞, otherwise

for t = 1→ B + 6 do
for scur ∈ {0, 1}6 do
sprev(x) := [x] ◦ scur[1 .. 5]
o1(x) := ([x] ◦ scur) · [1001111] mod 2
o2(x) := ([x] ◦ scur) · [1101101] mod 2
g′(x) := g(q2t−1, o1(x)) + g(q2t, o2(x))
φ′(x) := φt−1[sprev(x)] + g′(x)
x̂← arg maxx∈{0,1} φ′(x)
rt[s

cur]← sprev(x̂)
φt[s

cur]← φ′(x̂)
end for

end for
s′ ← [000000]
for t = B → 1 do
s′ ← rt+6[s′]
q̃t ← s′1

end for
q̃ ← [q̃1, q̃2, · · · , q̃B]

bit probabilities directly.

Algorithm 2 describes the particular convolu-
tional code that we applied in this study, with two
convolution weights [1001111] and [1101101] as
fixed hyper-parameters.4 Where x[i .. j] :=
[xi, · · · , xj ] and x · y :=

∑
i xiyi. On the other

hand, there are various algorithms to decode con-
volutional codes with the same format which are
based on different criteria. In this study, we use the
decoding method described in Algorithm 3, where
x ◦ y represents the concatenation of vectors x
and y. This method is based on the Viterbi al-
gorithm (Viterbi, 1967) and estimates original bits
by directly using probability of redundant bits. Al-
though Algorithm 3 looks complicated, this algo-
rithm can be performed efficiently on CPUs at test
time, and is not necessary at training time when we
are simply performing calculation of Equation (6).
Algorithm 2 increases the number of bits from B
intoB′ = 2(B+6), but does not restrict the actual
value of B.

4We also examined many configurations of convolutional
codes which have different robustness and computation costs,
and finally chose this one.

855



Table 1: Details of the corpus.
Name ASPEC BTEC

Languages En↔ Ja

#sentences
Train 2.00 M 465. k
Dev 1,790 510
Test 1,812 508

Vocabulary size V 65536 25000

4 Experiments

4.1 Experimental Settings

We examined the performance of the proposed
methods on two English-Japanese bidirectional
translation tasks which have different translation
difficulties: ASPEC (Nakazawa et al., 2016) and
BTEC (Takezawa, 1999). Table 1 describes details
of two corpora. To prepare inputs for training, we
used tokenizer.perl in Moses (Koehn et al.,
2007) and KyTea (Neubig et al., 2011) for En-
glish/Japanese tokenizations respectively, applied
lowercase.perl from Moses, and replaced
out-of-vocabulary words such that rank(w) >
V − 3 into the UNK marker.

We implemented each NMT model using C++
in the DyNet framework (Neubig et al., 2017) and
trained/tested on 1 GPU (GeForce GTX TITAN
X). Each test is also performed on CPUs to com-
pare its processing time. We used a bidirectional
RNN-based encoder applied in Bahdanau et al.
(2014), unidirectional decoder with the same style
of (Luong et al., 2015), and the concat global
attention model also proposed in Luong et al.
(2015). Each recurrent unit is constructed using a
1-layer LSTM (input/forget/output gates and non-
peepholes) (Gers et al., 2000) with 30% dropout
(Srivastava et al., 2014) for the input/output vec-
tors of the LSTMs. All word embeddings, recur-
rent states and model-specific hidden states are de-
signed with 512-dimentional vectors. Only output
layers and loss functions are replaced, and other
network architectures are identical for the conven-
tional/proposed models. We used the Adam op-
timizer (Kingma and Ba, 2014) with fixed hyper-
parameters α = 0.001, β1 = 0.9β2 = 0.999, ε =
10−8, and mini-batches with 64 sentences sorted
according to their sequence lengths. For eval-
uating the quality of each model, we calculated
case-insensitive BLEU (Papineni et al., 2002) ev-
ery 1000 mini-batches. Table 2 lists summaries of
all methods we examined in experiments.

Table 2: Evaluated methods.
Name Summary
Softmax Softmax prediction (Fig. 2(a))
Binary Fig. 2(b) w/ raw bit array
Hybrid-N Fig. 2(c) w/ softmax size N
Binary-EC Binary w/ error-correction
Hybrid-N-EC Hybrid-N w/ error-correction

(a) ASPEC (En→ Ja)

(b) BTEC (En→ Ja)

Figure 5: Training curves over 180,000 epochs.

4.2 Results and Discussion

Table 3 shows the BLEU on the test set (bold and
italic faces indicate the best and second places in
each task), number of bits B (or B′) for the binary
code, actual size of the output layer #out, number
of parameters in the output layer #W,β, as well as
the ratio of #W,β or amount of whole parameters
compared with Softmax, and averaged processing
time at training (per mini-batch on GPUs) and
test (per sentence on GPUs/CPUs), respectively.
Figure 5(a) and 5(b) shows training curves up
to 180,000 epochs about some English→Japanese
settings. To relax instabilities of translation qual-
ities while training (as shown in Figure 5(a) and
5(b)), each BLEU in Table 3 is calculated by av-
eraging actual test BLEU of 5 consecutive results

856



Table 3: Comparison of BLEU, size of output layers, number of parameters and processing time.
Corpus Method BLEU % B #out #W,β

Ratio of #params Time (En→Ja) [ms]
EnJa JaEn #W,β All Train Test: GPU / CPU

ASPEC

Softmax 31.13 21.14 — 65536 33.6 M 1/1 1 1026. 121.6 / 2539.
Binary 13.78 6.953 16 16 8.21 k 1/4.10 k 0.698 711.2 73.08 / 122.3
Hybrid-512 22.81 13.95 16 528 271. k 1/124. 0.700 843.6 81.28 / 127.5
Hybrid-2048 27.73 16.92 16 2064 1.06 M 1/31.8 0.707 837.1 82.28 / 159.3
Binary-EC 25.95 18.02 44 44 22.6 k 1/1.49 k 0.698 712.0 78.75 / 164.0
Hybrid-512-EC 29.07 18.66 44 556 285. k 1/118. 0.700 850.3 80.30 / 180.2
Hybrid-2048-EC 30.05 19.66 44 2092 1.07 M 1/31.4 0.707 851.6 77.83 / 201.3

BTEC

Softmax 47.72 45.22 — 25000 12.8 M 1/1 1 325.0 34.35 / 323.3
Binary 31.83 31.90 15 15 7.70 k 1/1.67 k 0.738 250.7 27.98 / 54.62
Hybrid-512 44.23 43.50 15 527 270. k 1/47.4 0.743 300.7 28.83 / 66.13
Hybrid-2048 46.13 45.76 15 2063 1.06 M 1/12.1 0.759 307.7 28.25 / 67.40
Binary-EC 44.48 41.21 42 42 21.5 k 1/595. 0.738 255.6 28.02 / 69.76
Hybrid-512-EC 47.20 46.52 42 554 284. k 1/45.1 0.744 307.8 28.44 / 56.98
Hybrid-2048-EC 48.17 46.58 42 2090 1.07 M 1/12.0 0.760 311.0 28.47 / 69.44

Figure 6: BLEU changes in the Hybrid-N methods
according to the softmax size (En→Ja).

around the epoch that has the highest dev BLEU.

First, we can see that each proposed method
largely suppresses the actual size of the output
layer from ten to one thousand times compared
with the standard softmax. By looking at the to-
tal number of parameters, we can see that the
proposed models require only 70% of the actual
memory, and the proposed model reduces the to-
tal number of parameters for the output layers to
a practically negligible level. Note that most of
remaining parameters are used for the embedding
lookup at the input layer in both encoder/decoder.
These still occupy O(EV ) memory, where E rep-
resents the size of each embedding layer and usu-
ally O(E/H) = O(1). These are not targets to be
reduced in this study because these values rarely
are accessed at test time because we only need to
access them for input words, and do not need them
to always be in the physical memory. It might be

possible to apply a similar binary representation
as that of output layers to the input layers as well,
then express the word embedding by multiplying
this binary vector by a word embedding matrix.
This is one potential avenue of future work.

Taking a look at the BLEU for the simple Bi-
nary method, we can see that it is far lower than
other models for all tasks. This is expected, as
described in Section 3, because using raw bit ar-
rays causes many one-off estimation errors at the
output layer due to the lack of robustness of the
output representation. In contrast, Hybrid-N and
Binary-EC models clearly improve BLEU from
Binary, and they approach that of Softmax. This
demonstrates that these two methods effectively
improve the robustness of binary code prediction
models. Especially, Binary-EC generally achieves
higher quality than Hybrid-512 despite the fact
that it suppress the number of parameters by about
1/10. These results show that introducing redun-
dancy to target bit arrays is more effective than
incremental prediction. In addition, the Hybrid-N-
EC model achieves the highest BLEU in all pro-
posed methods, and in particular, comparative or
higher BLEU than Softmax in BTEC. This behav-
ior clearly demonstrates that these two methods
are orthogonal, and combining them together can
be effective. We hypothesize that the lower qual-
ity of Softmax in BTEC is caused by an over-fitting
due to the large number of parameters required in
the softmax prediction.

The proposed methods also improve actual
computation time in both training and test. In par-
ticular on CPU, where the computation speed is
directly affected by the size of the output layer,
the proposed methods translate significantly faster

857



than Softmax by x5 to x20. In addition, we can
also see that applying error-correcting code is also
effictive with respect to the decoding speed.

Figure 6 shows the trade-off between the trans-
lation quality and the size of softmax layers in
the hybrid prediction model (Figure 2(c)) with-
out error-correction. According to the model def-
inition in Section 3.4, the softmax prediction and
raw binary code prediction can be assumed to be
the upper/lower-bound of the hybrid prediction
model. The curves in Figure 6 move between Soft-
max and Binary models, and this behavior intu-
itively explains the characteristics of the hybrid
prediction. In addition, we can see that the BLEU
score in BTEC quickly improves, and saturates at
N = 1024 in contrast to the ASPEC model, which
is still improving at N = 2048. We presume that
the shape of curves in Figure 6 is also affected by
the difficulty of the corpus, i.e., when we train the
hybrid model for easy datasets (e.g., BTEC is eas-
ier than ASPEC), it is enough to use a small soft-
max layer (e.g. N ≤ 1024).

5 Conclusion

In this study, we proposed neural machine transla-
tion models which indirectly predict output words
via binary codes, and two model improvements:
a hybrid prediction model using both softmax
and binary codes, and introducing error-correcting
codes to introduce robustness of binary code pre-
diction. Experiments show that the proposed
model can achieve comparative translation quali-
ties to standard softmax prediction, while signif-
icantly suppressing the amount of parameters in
the output layer, and improving calculation speeds
while training and especially testing.

One interesting avenue of future work is to au-
tomatically learn encodings and error correcting
codes that are well-suited for the type of binary
code prediction we are performing here. In Al-
gorithms 2 and 3 we use convolutions that were
determined heuristically, and it is likely that learn-
ing these along with the model could result in im-
proved accuracy or better compression capability.

Acknowledgments

Part of this work was supported by JSPS
KAKENHI Grant Numbers JP16H05873 and
JP17H00747, and Grant-in-Aid for JSPS Fellows
Grant Number 15J10649.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .

Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics 18(4):467–479.

Wenlin Chen, David Grangier, and Michael Auli. 2016.
Strategies for training large vocabulary neural lan-
guage models. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Association for
Computational Linguistics, Berlin, Germany, pages
1975–1985. http://www.aclweb.org/anthology/P16-
1186.

Rohan Chitnis and John DeNero. 2015. Variable-
length word encodings for neural translation mod-
els. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, Lisbon, Portugal, pages 2088–2093.
http://aclweb.org/anthology/D15-1249.

Thomas G. Dietterich and Ghulum Bakiri. 1995.
Solving multiclass learning problems via error-
correcting output codes. Journal of Artificial Intelli-
gence Research 2:263–286.

Chun-Sung Ferng and Hsuan-Tien Lin. 2011. Multi-
label classification with error-correcting codes.
Journal of Machine Learning Research 20:281–295.

Chun-Sung Ferng and Hsuan-Tien Lin. 2013. Multi-
label classification using error-correcting codes of
hard or soft bits. IEEE transactions on neural net-
works and learning systems 24(11):1888–1900.

Felix A Gers, Jürgen Schmidhuber, and Fred Cummins.
2000. Learning to forget: Continual prediction with
LSTM. Neural computation 12(10):2451–2471.

Marcel J. E. Golay. 1949. Notes on digital cod-
ing. Proceedings of the Institute of Radio Engineers
37:657.

David A. Huffman. 1952. A method for the construc-
tion of minimum-redundancy codes. Proceedings of
the Institute of Radio Engineers 40(9):1098–1101.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Aldebaro Klautau, Nikola Jevtić, and Alon Orlitsky.
2003. On nearest-neighbor error-correcting output
codes with application to all-pairs multiclass support
vector machines. Journal of Machine Learning Re-
search 4(April):1–15.

858



Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions. Association for Computational Lin-
guistics, Prague, Czech Republic, pages 177–180.
http://www.aclweb.org/anthology/P07-2045.

Abbas Z Kouzani. 2010. Multilabel classification us-
ing error correction codes. In International Sympo-
sium on Intelligence Computation and Applications.
Springer, pages 444–454.

Abbas Z Kouzani and Gulisong Nasireding. 2009.
Multilabel classification by bch code and random
forests. International journal of recent trends in en-
gineering 2(1):113–116.

Gurvan L’Hostis, David Grangier, and Michael Auli.
2016. Vocabulary selection strategies for neural ma-
chine translation. arXiv preprint arXiv:1610.00072
.

Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W
Black. 2015. Character-based neural machine trans-
lation. arXiv preprint arXiv:1511.04586 .

Yang Liu. 2006. Using svm and error-correcting codes
for multiclass dialog act classification in meeting
corpus. In INTERSPEECH.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Lisbon, Portugal, pages 1412–
1421. http://aclweb.org/anthology/D15-1166.

Haitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016.
Vocabulary manipulation for neural machine trans-
lation. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers). Association for Computa-
tional Linguistics, Berlin, Germany, pages 124–129.
http://anthology.aclweb.org/P16-2021.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.

Andriy Mnih and Yee Whye Teh. 2012. A fast and sim-
ple algorithm for training neural probabilistic lan-
guage models. In Proceedings of the 29th Interna-
tional Conference on Machine Learning.

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In

Proceedings of Tenth International Workshop on Ar-
tificial Intelligence and Statistics. volume 5, pages
246–252.

Toshiaki Nakazawa, Manabu Yaguchi, Kiyotaka Uchi-
moto, Masao Utiyama, Eiichiro Sumita, Sadao
Kurohashi, and Hitoshi Isahara. 2016. Aspec:
Asian scientific paper excerpt corpus. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Marko Grobelnik, Bente Mae-
gaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings
of the Ninth International Conference on Language
Resources and Evaluation (LREC 2016). European
Language Resources Association (ELRA), Portoro,
Slovenia, pages 2204–2208.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. Dynet:
The dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980 .

Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
japanese morphological analysis. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies. Association for Computational Lin-
guistics, Portland, Oregon, USA, pages 529–533.
http://www.aclweb.org/anthology/P11-2093.

Kishore Papineni, Salim Roukos, Todd Ward,
and Wei-Jing Zhu. 2002. Bleu: a method
for automatic evaluation of machine transla-
tion. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Philadelphia, Pennsylvania, USA, pages 311–318.
https://doi.org/10.3115/1073083.1073135.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Association for
Computational Linguistics, Berlin, Germany, pages
1715–1725. http://www.aclweb.org/anthology/P16-
1162.

Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal
27(3):379–423.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search 15(1):1929–1958.

859



Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Toshiyuki Takezawa. 1999. Building a bilingual
travel conversation database for speech translation
research. In Proc. of the 2nd international workshop
on East-Asian resources and evaluation conference
on language resources and evaluation. pages 17–20.

Andrew Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimum decoding al-
gorithm. IEEE transactions on Information Theory
13(2):260–269.

George. K. Zipf. 1949. Human behavior and the prin-
ciple of least effort.. Addison-Wesley Press.

860


	Neural Machine Translation via Binary Code Prediction

