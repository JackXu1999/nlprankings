




































Proceedings of the 12th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+12), pages 103–111,
Düsseldorf, Germany, June 29 - July 1, 2016.

Hyperedge Replacement and Nonprojective Dependency Structures

Daniel Bauer and Owen Rambow
Columbia University

New York, NY 10027, USA
{bauer,rambow}@cs.columbia.edu

Abstract

Synchronous Hyperedge Replacement
Graph Grammars (SHRG) can be used
to translate between strings and graphs.
In this paper, we study the capacity of
these grammars to create non-projective
dependency graphs. As an example, we
use languages that contain cross serial
dependencies. Lexicalized hyperedge
replacement grammars can derive string
languages (as path graphs) that contain an
arbitrary number of these dependencies
so that their derivation trees reflect the
correct dependency graphs. We find that,
in contrast, string-to-graph SHRG that
derive dependency structures on the graph
side are limited to derivations permitted
by the string side. We show that, as a re-
sult, string-to-graph SHRG cannot capture
languages with an unlimited degree of
crossing dependencies. This observation
has practical implications for the use of
SHRG in semantic parsing.

1 Introduction

Hyperedge Replacement Grammars (HRG) are a
type of context free graph grammar. Their de-
rived objects are hypergraphs instead of strings. A
synchronous extension, Synchronous Hyperedge
Replacement Grammars (SHRG) can be used to
translate between strings and graphs. To construct
a graph for a sentence, one simply parses the in-
put using the string side of the grammar and then
interprets the derivations with the graph side to as-
semble a derived graph.

SHRG has recently drawn attention in Natural
Language Processing as a tool for semantic con-
struction. For example, Jones et al. (2012) pro-
pose to use SHRG for semantics based machine

translation, and Peng et al. (2015) describe an ap-
proach to learning SHRG rules that translate sen-
tences into Abstract Meaning Representation (Ba-
narescu et al., 2013).

Not much work has been done, however, on
understanding the limits of syntactic and seman-
tic structures that can be modeled using HRG and
SHRG. In this paper, we examine syntactic depen-
dency structures generated by these formalisms,
specifially whether they can create correct depen-
dency trees for non-projective phenomena. We fo-
cus on non-projectivity caused by copy language
like constructions, specifically cross-serial depen-
dencies in Dutch. Figure 1 shows a (classical) ex-
ample sentence containing such dependencies and
a dependency graph.

This paper looks at dependency structures from
two perspectives. We first review HRGs that de-
rive string languages as path graphs. The set of
these languages is known to be the same as the lan-
guages generated by linear context free rewriting
systems (Weir, 1992). We consider HRG gram-
mars of this type that are lexicalized (each rule
contains exactly one terminal edge), so we can
view their derivation trees as dependency struc-
tures. We provide an example string-generating
HRG that can analyze the sentence in Figure 1
with the correct dependency structure and can gen-
erate strings with an unlimited number of crossing
dependencies of the same type.

Under the second perspective, we view the de-
rived graphs of synchronous string-to-HRG gram-
mars as dependency structures. These grammars
can generate labeled dependency graphs in a more
flexible way, including labeled dependency edges,
local reordering of dependencies (allowing a more
semantically oriented analysis of prepositional
phrases and conjunctions), structures with arbi-
trary node degree, and reentrancies. We present
a grammar to analyze the string/graph pair in Fig-

103



... omdat Wim Jan Marie de kinderen zag helpen leren zwemmen .

... because Wim Jan Marie the children saw help teach swim .

ccomp xcomp
xcomp

subj
nsubj

dobj

dobj

det

punc

Figure 1: Example sentence illustrating cross-serial dependencies in Dutch. English translation: “be-
cause Wim saw Jan help Marie teach the children to swim.”

ure 1, that derives the correct labeled dependency
structure, but whose derivation does not resemble
syntactic dependencies. Using this example, we
observe an important limitation of string-to-graph
SHRG: With nonterminal hyperedges of bounded
type (number of incident vertices), we cannot an-
alyze cross-serial dependencies with an unlimited
number of crossing edges. Specifically, for a given
dependency edge covering a span of words, the
number of nodes outside the span that can have
a dependent or parent inside the span is limited.
This is because, on the input side, the grammar is a
plain string CFG. In a string CFG derivation, each
node must correspond to a connected subspan of
the input. Because of this constraint on the deriva-
tion, the dependency subgraphs constructed by the
HRG must maintain a reference to all words that
have a long distance dependent elsewhere in the
string. These references are passed on through the
derivation in the external nodes of each graph rhs
of the SHRG rules. External nodes are special ver-
tices at which graph fragments are connected to
the surrounding graph.

To avoid this problem, instead of a plain string
CFG one can use other formalisms that produce
context free derivation trees, such as the string-
generating HRGs we discuss in this paper or
LTAG.

Semantic representations, such as Abstract
Meaning Representation, resemble dependency
structures. Therefore, while we do not discuss
semantic graphs to skirt the issue of reentrancy,
non-projective linguistic phenomena that appear
in syntactic dependency structure are also relevant
when translating strings into semantic represen-
tations. We believe that our observations are not
only of theoretical interest, but affect practical ap-
plications of SHRG in semantic parsing.

The paper proceeds as follows: Section 2 pro-

vides a formalization of Hyperedge Replacement
Grammars and introduces necessary terminology.
In section 3, we discuss string generating HRGs
and illustrate how they can be used to correctly
analyze cross-serial dependencies in an exam-
ple. Section 4 examines string-to-graph SHRGs
and observes their limitations in generating cross-
serial dependencies. In section 5, we analyze this
limitation in more detail, demonstrating a relation-
ship between the order of a grammar (the max-
imum hyperedge type) and the maximum number
of edges crossing another edge. Section 6 provides
an overview of related work. Finally, we conclude
and summarize our findings in section 7.

2 Hyperedge Replacement Graph

Grammars

A directed, edge-labeled hypergraph is a tuple
H = 〈V,E, ℓ〉, where V is a finite set of ver-
tices, E ⊆ V + is a finite set of hyperedges, each
of which connects a number of vertices, and ℓ is
a labeling function with domain E. The number
of vertices connected by a hyperedge is called its
type.

A hyperedge replacement grammar (HRG,
Drewes et al. (1997) ) is a tuple G = 〈N,Σ, P, S〉
where N is a ranked, finite set of nonterminal
labels, Σ is a finite set of terminal labels such
that Σ ∩ N = ∅, S ∈ N is the designated
start symbol, and P is a finite set of rules. Each
rule r ∈ P is of the form (A → R,X), where
A ∈ N , R = 〈V,E, ℓ〉 is a hypergraph with
ℓ : E → N ∪ T , and X ∈ V ∗ is a list of exter-
nal nodes. We call the number of vertices |V | in a
rule rhs the width of the rule. The maximum type
of any nonterminal hyperedge in the grammar is
called the order of the grammar.1

1We choose the term order instead of rank. Both terms

104



R1: S →
N2 zag

.

4 V4

R2: V4 →
N2 N2 helpen

0 1 2 3

4 V4

R3: V4 →
N2 leren V2

0 1 2 3 R4: V4 →
zwemmen

0 1

R5: N2 →
Wim

0 1 R6: N2 →
Jan

0 1

R7: N2 →
Marie

0 1 R8: N2 →
DT2 kinderen

0 1 R9: DT2 →
de

0 1

Figure 2: A ‘string-generating’ lexicalized hyperedge replacement grammar for Dutch cross serial de-
pendencies. The grammar can derive the sentence in figure 1. The derivation tree for this sentence
represents the correct dependency structure.

Given a partially derived graph H we can
use a rule (A → R,X) to rewrite a hyper-
edge e = (v1, · · · , vk) if e has label A and
k = length(X). In this operation, e is removed
from H , a copy of R is inserted into H and the
external nodes X = (u1, · · · , uk) of the copy of
R are fused with the nodes connected by e, such
that ui is identified with vi for i = 1, . . . , k.

When showing rules in diagrams, such as Fig-
ure 2, we draw external nodes as black circles and
number them with an index to make their order ex-
plicit. Nonterminal hyperedges are drawn as undi-
rected edges whose incident vertices are ordered
left-to-right.

The relation H ⇒G H ′ holds if hypergraph H ′

can be derived from hypergraph H in a single step
using the rules in G. Similarly H ⇒∗G H

′ holds if
H ′ can be derived from H in a finite number of
steps. The hypergraph language of a grammar G
is the (possibly infinite) set of hypergraphs that
can be derived from the start symbol S. L(G) =

⋃

(S → H,〈〉) ∈ P

{H ⇒∗G H
′|H ′ has only terminals }

We will show examples for HRG derivations be-
low.

HRG derivations are context-free in the sense
that the applicability of each production depends
on the nonterminal label and type of the replaced
edge only. We can therefore represent derivations
as trees, as for other context free formalisms. Con-
text freeness also allows us to extend the formal-
ism to a synchronous formalism, for example to

are used in the literature. We use the word rank to refer to the
maximum number of nonterminals in a rule right hand side.

translate strings into trees, as we do in section 4.
We can view the resulting string and graph lan-
guages as two interpretations of the same set of
possible derivation trees described by a regular
tree grammar (Koller and Kuhlmann, 2011).

3 HRG Derivations as Dependency

Structures

We first discuss the case in which HRG is used
to derive a sentence and examine the dependency
structure induced by the derivation tree. Hyper-
edge Replacement Grammars can derive string
languages as path graphs in which edges are la-
beled with tokens. For example, consider the path
graph for the sentence in Figure 1.
Wim Jan Marie dekinderen zag helpen leren zwemmen

Engelfriet and Heyker (1991) show that the
string languages generated by HRG in this way
are equivalent to the output languages of Deter-
ministic Tree Walking Transducers (DTWT). Weir
(1992) shows that these languages are equivalent
to the languages generated by linear context free
rewriting systems (LCFRS) and that the LCFRS
languages with fan-out k are the same as the HRG
string languages with order 2k.

The analysis of cross-serial dependencies has
been studied in a number of ‘mildly context sen-
sitive’ grammar formalisms. For example, Ram-
bow and Joshi (1997) show an analysis in LTAG.
Because the string languages generated by these
formalisms are equivalent to languages of LCFRS
with fan-out 2, we know that we must be able to
write an HRG of order 4 that can capture cross-

105



serial dependencies.
Figure 2 shows such a string generating HRG

that can derive the example in Figure 1. Each rule
rhs consists of one or more internally connected
spans of strings paths of labeled edges. The ex-
ternal nodes of each rhs graph mark the beginning
and end of each span. The nonterminal labels of
other rules specify how these spans are combined
and connected to the surrounding string. For illus-
tration, consider the first two steps of a derivation
in this grammar. Rule 1 introduces the verb ‘zag’
and its subject. Rule 2 inserts ‘helpen’ to the right
of ‘zag’ and its subject and direct object to the
right of the subject of ‘zag’. This creates cross-
ing dependencies between the subjects and their
predicates in the derivation.

R1
N2 zag

.

R2
N2 N2

helpen
0 1 2 3

4 V4

The partially derived graph now contains a span
of nouns and a span of verbs. The nonterminal
hyperedge labeled V4 indicates where to append
new nouns and where to add new verbs. Note that
rule 2 (or an identical rule for a different verb) can
be re-applied to generate cross-serial dependen-
cies with an arbitrary number of crossings. It is
easy to see that grammars of this type correspond
to LCFRS almost directly.

Using the grammar in Figure 2, there is a sin-
gle derivation tree for the example sentence in Fig-
ure 1.

R1,zag

R2,helpen

R3,leren

R4,zwemmenR8,kinderen

R9,de

R7,MarieR6,Jan

R5,Wim

This derivation tree represents the correct syntac-
tic dependency structure for the sentence. This
is not the case for all lexicalized ‘mildly context
sensitive’ grammar formalisms, even if it is pos-
sible to write grammars for languages that con-
tain cross-serial dependencies. In TAG, long dis-
tance dependencies are achieved using adjunction.

Both dependents are introduced by the same aux-
iliary tree, stretching the host tree apart. An LTAG
derivation for the example sentence would start
with an elementary tree for ‘zwemmen’ and then
adjoin ‘leren’. The resulting dependency structure
is therefore inverted.

4 Deriving Dependency Graphs with

Synchronous String-to-Graph

Grammars

We now consider grammars whose derived graphs
represent the dependency structure of a sentence.
The goal is to write a synchronous context-free
string-to-graph grammar that translates sentences
into their dependency graphs. If the string side of
the grammar is a plain string CFG, as we assume
here, the derivation cannot reflect non-projective
dependencies directly. Instead, we must use the
graph side of the grammar to assemble a depen-
dency structure.

This approach has several potential advantages
in applications. In the string-generating HRG dis-
cussed in the previous section, the degree of a node
in the dependency structure is limited by the rank
of the grammar. Using a graph grammar to derive
the graph, we can add an arbitrary number of de-
pendents to a node, even if the rules contributing
these dependency edges are nested in the deriva-
tion. This is especially important for more seman-
tically inspired representations where all seman-
tic arguments should become direct dependents
of a node (for example, deep subjects). We can
also make the resulting graphs reentrant. In ad-
dition, because HRGs produce labeled graphs, we
can add dependency labels. Finally, even though
the example grammar in Figure 3 is lexicalized on
the string side, lexicalization is no longer required
to build a dependency structure. Unfortunately,
‘decoupling’ the derivation from the dependency
structure in this way can be problematic, as we
will see.

Figure 3 shows a synchronous hyperedge re-
placement grammar that can translate the sen-
tence from Figure 1 into its dependency graph.
A synchronous hyperedge replacement grammar
(SHRG) is a synchronous context free grammar
in which at least one of the right hand sides
uses hypergraph fragments. The two sides of
the grammar are synchronized in a strong sense.
Both rhs of each grammar rule contain exactly the
same instances of nonterminals and the instances

106



R1: S → 〈 V1 zwemmen |
zwemmen

V
1

〉 R2: V1 → 〈 V1 leren |
leren

xcom
p

V
1

〉

R3: V1 → 〈 V2 helpen |
helpen

xcom
p

2V2

〉 R4: V2 → 〈 N3 zag |
zag

ccom
p

0 1

3 N3

〉

R5: N3 → 〈 Wim N2 | n
su

b
j

W
im

0 1 2

3
N2

〉 R6: N2 → 〈 Jan N2 | n
su

b
j

Ja
n

N2
0 〉1

R7: N2 → 〈 Marie N1 |

d
o

b
j

M
a

rie

N
1

0 1 〉 R8: N1 → 〈 DT1 kinderen |
dobj kinderen

D
T
1

〉

R9: DT1 → 〈 de |
det de

〉

Figure 3: A synchronous string-to-graph grammar for Dutch cross-serial dependencies. The grammar
can derive the sentence/dependency graph pair in in Figure 1, but the derivation tree does not reflect
syntactic dependencies.

are related by a bijective synchronization relation
(in case of ambiguity we make the bijection ex-
plicit by indexing nonterminals when representing
grammars). In a SHRG, each nonterminal label
can only be used to label hyperedges of the same
type. For example, V2 is only used for hyperedges
of type 2. As a result, all derivations for the string
side of the grammar are also valid derivations for
graphs.

In the grammar in Figure 3, vertices represent
nodes in the dependency structure (words). Be-
cause HRGs derive edge labeled graphs but no ver-
tex labels, we use a unary hyperedge (a hyperedge
with one incident vertex) to label each node. For
example, the only node in the rhs of rule 1 has the
label ‘zwemmen’.

Nonterminal hyperedges are used to ‘pass on’
vertices that we need to attach a dependent to at
a later point in the derivation. External nodes
define how these nodes are connected to the
surrounding derived graph. To illustrate this, a
derivation using the grammar in Figure 3 could
start with rule 1, then replace the nonterminal
V1 with the rhs of rule 2. We then substitute the
new nonterminal V1 introduced by rule 2 with
rule 3. At this point, the partially derived string

is ‘V2 helpen leren zwemmen’ and the partially
derived graph is

helpen

xcom
p leren

xcom
p zwemmen

2
V2

.

The nonterminal V2 passes on a reference to two
nodes in the graph, one for ‘helpen’ and one for
‘leren’. This allows subsequent rules in the deriva-
tion to attach subjects and objects to these nodes,
as well as the parent node (‘zag’) to ‘helpen’.

To derive the string/graph pair in Figure 1, the
rules of this grammar are simply applied in order
(rule 1 ⇒ rule 2 ⇒ · · · ⇒ rule 9). Clearly, the
resulting derivation is just a chain and bears no re-
semblance to the syntactic dependency structure.

While the grammar can derive our example sen-
tence, it does not permit us to derive dependency
structures with an arbitrary number of crossing de-
pendencies. This is because the nonterminal edges
need to keep track of all possible sites at which
long distance dependents can be attached at a later
point in the derivation. To add more crossing

107



Figure 4: Sketch of the derivation tree of a syn-
chronous hyperedge replacement grammar, show-
ing two dependency edges (u,w) and (v, x), and
u < v < w < x.The graph fragment associated
with the rule at node α needs to contain nodes u,w
and v. v must be an external node.

dependencies we therefore need to create special
rules with nonterminal hyperedges of a larger type,
as well as the corresponding rules with a larger
number of external nodes. Because any grammar
has a finite number of rules and a fixed order, we
cannot use this type of SHRG grammar to model
languages that permit an arbitrary degree of cross-
ing edges in a graph. While the graph grammar
can keep track of long-distance dependencies, the
string grammar is still context free, so any non-
local information needs to be encoded in the non-
terminals. The penalty we pay for being able to
remember a limited set of dependents through the
derivation is that we need a refined alphabet of
nonterminals (V1, V2, V3, · · · ; instead of just V).

5 Edge Degree and Hyperedge Type

In section 4 we demonstrate that we need an ever-
increasing hyperedge type if we want to model
languages in which a dependency edge can be
crossed by an arbitrary number of other depen-
dency edges. So far, we have only illustrated this
point with an example. In this section we will
demonstrate that no such grammar can exist.

It is clear that the problem is not with gener-
ating the tree language itself. We could easily
extend the string-generating grammar from sec-
tion 3, whose derivation trees reflect the correct
dependency structure, by adding a second graph
rhs that derives an image of the derivation tree
(potentially with dependency labels). Instead, the
problem appears to be that we force grammar rules
to be applied according to the string derivation.

Specifically, the partially derived string associated
with each node in the derivation needs to be a con-
tiguous subspan. This prevents us from assem-
bling dependencies locally.

To make this intuition more formal, we demon-
strate that there is a relationship between number
of crossing dependencies and the the minimum
hyperedge type required in the SHRG. We first
look at a single pair of crossing dependency edges
and then generalize the argument to multiple edges
crossing into the span of an edge. For illustration,
we provide a sketch of a SHRG derivation tree in
Figure 4.

Assume we are given a sentence
s = (w0, w1, · · · , wn−1), and a corre-
sponding dependency graph G = 〈V,E, ℓ〉 where
V = {0, 1, · · · , n− 1}. We define the range of a
dependency edge (u, v) to be the interval [u, v] if
v > u or else [v, u]. For each dependency edge
(u, v) the number of crossing dependencies is the
number of dependency nodes properly outside
its range, that share a dependency edge with any
node properly inside its range. The degree of
crossing dependencies of a dependency graph is
the maximum number of crossing dependencies
for any of its edges.

Given a SHRG derivation tree for s and G, each
terminal dependency edge (u,w) ∈ E must be
produced by the rule associated with some deriva-
tion node β (see Figure 4). Without loss of gen-
erality, assume that u < w. String token s[u] is
produced by the rule associated with some deriva-
tion node τu and s[w] is produced by the rule of
some derivation node τw. On the graph side, τu
and τw must contain the nodes u and w because
they generate the unary hyperedges labeling these
vertices. There must be some common ancestor α
of β, τu, and τw that contains both u and w. u and
w must be connected in α by a nonterminal hyper-
edge, because otherwise there would be no way to
generate the terminal edge (u,w) in β (note that
it is possible that α and β are the same node in
which case the rule of this node does not contain a
nonterminal edge).

Now consider another pair of nodes v and x
such that u < v < w < x and there is a de-
pendency edge (v, x) ∈ E or (x, v) ∈ E. s[v] is
generated by τv and s[x] is generated by τx. As be-
fore, there must be a common ancestor γ of τv and
τx, in which v and x are connected by a nontermi-
nal hyperedge. Because u < v < w < x either

108



α is an ancestor of γ or γ is an ancestor of α. For
illustration, we assume the second case. The case
where α dominates γ is analogous.

Since the graph fragments of all derivation
nodes on the path from γ to τv must contain a ver-
tex that maps to v, α must contain such a vertex.
This vertex needs to be an external node of the rule
attached to α because otherwise v could not be in-
troduced by γ.

We can extend the argument to an arbitrary
number of crossing dependency edges. As be-
fore, let (u,w) be a dependency edge and α be the
derivation node whose graph fragment first intro-
duces the nonterminal edge between u and w. For
all dependency edges (x, y) or (y, x) for which
y is in the range of (u,w) and x is outside of
the range of (u,w) (either x < u < y < w or
u < y < w < x) there must be some path in
the derivation tree that leads through α. All graph
fragments on this path contain a vertex mapped to
y. As a result, the graph fragment in α needs to
contain one external node for each x that has a de-
pendency edge to some node y inside the range
(u,w). In other words, α needs to contain as many
external nodes as there are nodes outside the range
(u,w) that share a dependency edge with a node
inside the range (u,w).

Because every HRG has a fixed order (the max-
imum type of any nonterminal hyperedge), no
SHRG that generates languages with an arbitrary
number of cross-serial dependencies can exist. It
is known that the hypergraph languages HRLk
that can be generated by HRGs of order k form an
infinite hierarchy, i.e. HRL1 ( HRL2 ( · · ·
(Drewes et al., 1997). Therefore, the string-to-
graph grammars required to generate cross-serial
dependencies up to edge degree k are strictly more
expressive than those that can only generate edge
degree k − 1.

6 Related Work

While the theory of graph grammars dates back to
the 70s (Nagl, 1979; Drewes et al., 1997), their
use in Natural Language Processing is more re-
cent. Fischer (2003) use string generating HRG
to model discontinuous constituents in German.
Jones et al. (2012) introduce SHRG and demon-
strate an application to construct intermediate
semantic representations in machine translation.
Peng et al. (2015) automatically extract SHRG
rules from corpora annotated with graph based

meaning representations (Abstract Meaning Rep-
resentation), using Markov Chain Monte Carlo
techniques. They report competitive results on
string-to-graph parsing. Braune et al. (2014) em-
pirically compare SHRG to cascades of tree trans-
ducers as devices to translate English strings into
reentrant semantic graphs. In agreement with the
result we show more formally in this paper, they
observe that, to generate graphs that contain a
larger number of long-distance dependencies, a
larger grammar with more nonterminals is needed,
because the derivations of the grammar are limited
to string CFG derivations.

Synchronous context free string-graph gram-
mars have also been studied in the framework
of Interpreted Regular Tree Grammar (Koller and
Kuhlmann, 2011) using S-Graph algebras (Koller,
2015). In the TAG community, HRGs have been
discussed by Pitsch (2000), who shows a construc-
tion to convert TAGs into HRGs. Finally, Joshi
and Rambow (2003) discuss a version of TAG in
which the derived trees are dependency trees, sim-
ilar to the SHRG approach we present here.

To use string-generating HRG in practice we
need a HRG parser. Chiang et al. (2013) present
an efficient graph parsing algorithm. However,
their implementation assumes that graph frag-
ments are connected, which is not true for the
grammar in section 3. On the other hand, since
string-generation HRGs are similar to LCFRS, any
LCFRS parser could be used. The relationship be-
tween the two parsing problems merits further in-
vestigation. Seifert and Fischer (2004) describe a
parsing algorithm specificaly for string-generating
HRGs.

Formal properties of dependency structures
generated by lexicalized formalisms have been
studied in detail by Kuhlmann (2010). He
proposes measures for different types of non-
projectivity in dependency structures, including
edge degree (which is related to the degree of
crossing dependencies we use in this paper), and
block degree. A qualitative measure of depen-
dency structures is well nestedness, which indi-
cates whether there is an overlap between subtrees
that do not stand in a dominance relation to each
other. In future work, we would like to investigate
how these measures relate to dependency struc-
tures generated by HRG derivations and SHRG
derived graphs.

109



7 Conclusion

In this paper we investigated the capability of hy-
peredge replacement graph grammars (HRG) and
synchronous string-to-graph grammar (SHRG) to
generate dependency structures for non-projective
phenomena. Using Dutch cross-serial dependen-
cies as an example, we compared two different ap-
proaches: string-generating HRGs whose deriva-
tion trees can be interpreted as dependency struc-
tures, and string-to-graph SHRGs, whose can cre-
ate dependency structures as their derived graphs.

We provided an example grammar for each
case. The derivation tree of the HRG adequately
reflected syntactic dependencies and the example
grammar could in principle generate an arbitrary
number of crossing dependencies. However, these
derivation trees are unlabeled and cannot be ex-
tended to represent deeper semantic relationships
(e.g semantic argument structure and coreference).
For the string-to-graph SHRG, we saw that the de-
rived graph of our grammar represented the correct
dependencies for the example sentence, while the
derivation tree did not.

The main observation of this paper is that,
unlike the string-generating HRG, the string-to-
graph SHRG was only able to generate a limited
number of crossing dependencies. With each ad-
ditional crossing edge in the example, we needed
to add a new rule with a higher hyperedge type,
increasing the order of the grammar. We argued
that the reason for this is that the synchronous
derivation for the input string and output graph
is constrained to be a valid string CFG deriva-
tion. Analyzing this observation more formally,
we showed a relationship between the order of the
grammar and the maximum permitted number of
edges crossing into the span of another edge.

An important conclusion is that, unless the cor-
rect syntactic dependencies are already local in
the derivation, HRGs cannot derive dependency
graphs with an arbitrary number of cross-serial
dependencies. We take this to be a strong ar-
gument for using lexicalized formalisms in syn-
chronous grammars for syntactic and semantic
analysis, that can process at least a limited degree
of non-projectivity, such as LTAG.

In future work, we are aiming to develop a lex-
icalized, synchronous string-to-graph formalisms
of this kind. We would also like to relate our
results to other measures of non-projectivity dis-
cussed in the literature. Finally, we hope to expand

the results of this paper to other non-projective
phenomena and to semantic graphs.

References

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Linguistic Annotation Work-
shop.

Fabiene Braune, Daniel Bauer, and Kevin Knight.
2014. Mapping between english strings and reen-
trant semantic graphs. In Proceedings of LREC,
Reykjavik, Iceland.

David Chiang, Jacob Andreas, Daniel Bauer, Karl-
Mortiz Hermann, Bevan Jones, and Kevin Knight.
2013. Parsing graphs with hyperedge replacement
grammars. In Proceedings of ACL, Sofia, Bulgaria.

Frank Drewes, Annegret Habel, and Hans-Jörg Kre-
owski. 1997. Hyperedge replacement graph gram-
mars. In Grzegorz Rozenberg, editor, Handbook of
Graph Grammars and Computing by Graph Trans-
formation, pages 95–162. World Scientific.

Joost Engelfriet and Linda Heyker. 1991. The string
generating power of context-free hypergraph gram-
mars. Journal of Computer and System Sciences,
43(2):328–360.

Ingrid Fischer. 2003. Modeling discontinuous con-
stituents with hypergraph grammars. In Interna-
tional Workshop on Applications of Graph Transfor-
mations with Industrial Relevance (AGTIVE), pages
163–169.

Bevan Jones, Jacob Andreas, Daniel Bauer, Karl-
Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In Proceedings of
COLING, Mumbai, India. First authorship shared.

Aravind Joshi and Owen Rambow. 2003. A formal-
ism for dependency grammar based on tree adjoin-
ing grammar. Proceedings of the Conference on
Meaning-Text Theory, pages 207–216.

Alexander Koller and Marco Kuhlmann. 2011. A gen-
eralized view on parsing and translation. In Pro-
ceedings of the 12th International Conference on
Parsing Technologies, pages 2–13. Association for
Computational Linguistics.

Alexander Koller. 2015. Semantic construction with
graph grammars. In Proceedings of the 11th Inter-
national Conference on Computational Semantics
(IWCS), pages 228–238.

Marco Kuhlmann. 2010. Dependency Structures and
Lexicalized Grammars: An Algebraic Approach,
volume 6270. Springer.

110



Manfred Nagl. 1979. A tutorial and bibliographical
survey on graph grammars. In Proceedings of the
International Workshop on Graph-Grammars and
Their Application to Computer Science and Biology,
pages 70–126, London, UK, UK. Springer-Verlag.

Xiaochang Peng, Linfeng Song, and Daniel Gildea.
2015. A synchronous hyperedge replacement gram-
mar based approach for amr parsing. In Proceedings
of CONLL.

Gisela Pitsch. 2000. Hyperedge replacement and tree
adjunction. In Anne Abeillè and Owen Rambow,
editors, Tree Adjoining Grammars. CSLI.

Owen Rambow and Aravind Joshi. 1997. A formal
look at dependency grammars and phrase-structure
grammars, with special consideration of word-order
phenomena. In Leo Wanner, editor, Recent Trends
in Meaning-Text Theory, pages 167–190. John Ben-
jamins, Amsterdam and Philadelphia.

Sebastian Seifert and Ingrid Fischer. 2004. Pars-
ing string generating hypergraph grammars. In
International Conference on Graph Transforma-
tions(ICGT), pages 352–367.

David J. Weir. 1992. Linear context-free rewriting
systems and deterministic tree-walking transducers.
In Proceedings of ACL, pages 136–143, Newark,
Delaware, USA, June. Association for Computa-
tional Linguistics.

111


