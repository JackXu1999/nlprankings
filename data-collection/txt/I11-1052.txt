















































Syntactic Parsing for Ranking-Based Coreference Resolution


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 465–473,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Syntactic Parsing for Ranking-Based Coreference Resolution

Altaf Rahman and Vincent Ng
Human Language Technology Research Institute

University of Texas at Dallas
Richardson, TX 75083-0688

{altaf,vince}@hlt.utdallas.edu

Abstract

Recent research efforts have led to the de-
velopment of a state-of-the-art supervised
coreference model, the cluster-ranking
model. However, it is not clear whether the
features that have been shown to be useful
when employed in traditional coreference
models will fare similarly when used in
combination with this new model. Rather
than merely re-evaluate them using the
cluster-ranking model, we examine two in-
teresting types of features derived from
syntactic parses, tree-based features and
path-based features, and discuss the chal-
lenges involved in employing them in the
cluster-ranking model. Results on a set of
Switchboard dialogues show their effec-
tiveness in improving the cluster-ranking
model: using them to augment a baseline
coreference feature set yields a 8.6–11.7%
reduction in relative error.

1 Introduction

Coreference resolution is the task of determining
which noun phrases (NPs) in a text or dialogue
refer to the same real-world entity. According to
Webber (1979), coreference resolution can be de-
composed into two complementary subtasks: “(1)
identifying what a text potentially makes avail-
able for anaphoric reference and (2) constraining
the candidate set of a given anaphoric expression
down to one possible choice”. These two subtasks
are commonly known as anaphoricity determina-
tion and anaphora resolution, both of which have
recently been tackled using machine learning tech-
niques. More specifically, anaphoricity determina-
tion is typically tackled by training an anaphoric-
ity classifier, which determines whether an NP is
anaphoric or not (e.g., Poesio et al. (2004), Zhou
and Kong (2009)). If so, the NP is passed to the

second component, the resolution system, which
identifies an antecedent for the NP. This resolver is
typically implemented by training a mention-pair
(MP) model, which is a binary classifier that deter-
mines whether a pair of NPs are co-referring or not
(e.g., Soon et al. (2001), Ng and Cardie (2002b)).

While this architecture is popularly adopted
by coreference researchers and was implemented
even within recently developed coreference re-
solvers (e.g., Bengtson and Roth (2008), Stoy-
anov et al. (2009)), neither the architecture it-
self nor its aforementioned implementation is sat-
isfactory for at least two reasons. First, in this
pipeline architecture, anaphoricity determination
is performed prior to coreference resolution, so er-
rors in anaphoricity determination can propagate
to the downstream coreference component and ad-
versely affect its performance (Ng and Cardie,
2002a). Second, the MP coreference model is fun-
damentally weak in that (1) the information ex-
tracted from two NPs may not be sufficient for
making an informed coreference decision and (2)
since the model is trained to compare the NP to be
resolved (henceforth the active NP) against a can-
didate antecedent, it only determines how good the
candidate is relative to the active NP, not how good
the candidate is relative to other candidates.

In light of the aforementioned problems, re-
searchers have proposed a number of solutions:

• To address the error propagation problem,
researchers have proposed joint inference
(Denis and Baldridge, 2007) and joint learn-
ing (Rahman and Ng, 2009) for anaphoricity
determination and coreference resolution.

• To address the expressiveness problem re-
sulting from making a coreference decision
based on only two NPs, researchers have pro-
posed the entity-mention model, where coref-
erence decisions are made by determining
whether an NP belongs to a preceding coref-
erence cluster (e.g., Luo et al. (2004), Yang

465



et al. (2008)).

• To address the failure to directly compare
candidate antecedents and determine the best
one, researchers have proposed the mention-
ranking model, which imposes a ranking on
the candidate antecedents and therefore cap-
tures the competition among them (e.g., De-
nis and Baldridge (2008), Iida et al. (2009)).

Recent research efforts have led to the develop-
ment of a state-of-the-art supervised coreference
model that can address all of the aforementioned
problems, namely the joint cluster-ranking (CR)
model (Rahman and Ng, 2009). However, other
than its superior empirical performance to compet-
ing coreference models (such as the MP model),
little is known about the joint CR model. In partic-
ular, most of the linguistic features for coreference
resolution were developed and evaluated in the
context of the MP model, and thus it is not clear
whether these features would fare similarly when
used in combination with the joint CR model.

Motivated by this observation, our goal in this
paper is to examine the value of features derived
from syntactic parses for the joint CR model. Note
that parse-based features have been investigated
extensively for the MP model. For example, they
have been used to implement Binding Constraints
(e.g., Luo and Zitouni (2005)) and encode syn-
tactic salience (e.g., Haghighi and Klein (2009)).
Rather than re-evaluate them for the CR model, we
investigate two types of parse-based features that
we believe are particularly interesting.

First, we employ parse trees directly as struc-
tured features for the joint CR model. The
main advantage of employing tree-based struc-
tured features is simplicity: we no longer need
to design heuristics to extract the desired fea-
tures (e.g., salience, Binding Constraints) from
the parse trees, as designing heuristics can be
time-consuming and sometimes difficult for cer-
tain tasks. Note, however, that previous attempts
have employed structured features to train an MP
model for anaphora resolution (Yang et al., 2006;
Versley et al., 2008) and an anaphoricity classifier
in the aforementioned pipeline architecture (Zhou
and Kong, 2009). In both cases, the structured fea-
tures are combined with their non-structured (i.e.,
flat) counterparts via a composite kernel and used
to train a classification model. What is interesting
for us to investigate in this paper, however, is the
question of how to combine flat and structured fea-

tures in a ranking model that employs joint learn-
ing. With the increasingly important role struc-
tured features and ranking models play in natural
language learning, we believe that a method for
combining flat and structured features for training
a ranker would be of particular interest to natural
language processing (NLP) researchers.1

Second, motivated in part by lexical semantics
research (Lin and Pantel, 2001), we investigate
path-based features, which encode the contextual
relationship between an active NP and a candidate
antecedent as the shortest path between the cor-
responding nodes in the parse tree. As with other
NLP tasks, the effectiveness of a given type of fea-
tures for coreference resolution depends in part on
how the linguistic information it intends to capture
is represented. We seek to investigate the extent to
which a joint CR model can benefit from this path-
based representation of context.

Unlike the vast majority of English coreference
resolvers, which were evaluated using the MUC
and ACE corpora, our resolver was evaluated on
a set of Switchboard dialogues. To our knowl-
edge, we are among the first to report results for
the full coreference task on this dataset. As a re-
sult, our work contributes to the establishment of a
baseline using a state-of-the-art supervised coref-
erence model against which future work can be
compared. Our experimental results indicate that
while both the tree-based and path-based features
improve coreference performance when applied to
a Baseline feature set in isolation, the best perfor-
mance is achieved when they are applied in com-
bination. In particular, these two types of features
yield an improvement of 2.2–3.7% in F-measure
over the Baseline joint CR model, which corre-
sponds to a 8.6–11.7% reduction in relative error.

The rest of the paper is organized as follows.
Section 2 discusses our implementation of the
joint CR model. Section 3 describes tree-based
and path-based features and how they can be in-
tegrated into the CR model. We present evaluation
results in Section 4 and conclude in Section 5.

2 The Baseline Coreference Model

This section describes the Baseline CR model.
Since the CR model is a natural extension of the

1The dual form of Collins and Duffy’s (2002) ranking al-
gorithm can also combine flat and structured features. Note
that their algorithm employs online learning, whereas ours
employs batch learning in a maximum-margin fashion.

466



MP model, in order to understand the CR model,
it helps to first understand the MP model.

2.1 The Mention-Pair Model

As noted before, the MP model is a classifier that
determines whether two NPs are co-referring or
not. Each instance i(NPj , NPk) corresponds to two
NPs, NPj and NPk, and is represented by 39 fea-
tures (see Table 1 of Rahman and Ng (2009) for a
description of these features). Linguistically, these
features can be divided into four groups: string-
matching, grammatical, semantic, and positional.
However, they can also be categorized based on
whether they are relational or non-relational: re-
lational features capture the relationship between
NPj and NPk, whereas non-relational features cap-
ture the linguistic properties of one of them.

We follow Soon et al.’s (2001) method for creat-
ing training instances. Specifically, we create (1) a
positive instance for each anaphoric NP NPk and its
closest antecedent NPj ; and (2) a negative instance
for NPk paired with each of the intervening NPs,
NPj+1, NPj+2, . . ., NPk−1. The classification as-
sociated with a training instance is either positive
or negative, depending on whether the two NPs
are coreferent. To train the MP model, we use the
SVM learner from SVMlight (Joachims, 1999).2

After training, the classifier is used to identify
an antecedent for an NP in a test text. Each NP,
NPk, is compared in turn to each preceding NP,
NPj , from right to left, and NPj is selected as its an-
tecedent if the pair is classified as coreferent. The
process ends as soon as an antecedent is found for
NPk or the beginning of the text is reached.

2.2 The Cluster-Ranking Model

The CR model addresses two weaknesses of the
MP model, one concerning expressiveness and
the other concerning its failure to compare can-
didate antecedents directly and capture the com-
petition among them. It does so by combin-
ing the strengths of the entity-mention model and
the mention-ranking model. As discussed before,
the mention-ranking model addresses the failure
to compare candidate antecedents by training a
ranker to impose a ranking on the candidate an-
tecedents for an active NP. On the other hand,
the entity-mention model addresses the expres-
siveness problem by determining whether an ac-

2For this and subsequent uses of the SVM learner in our
experiments, we set all parameters to their default values.

tive NP belongs to a preceding, possibly partially-
formed, coreference cluster. Its increased expres-
siveness stems from its ability to employ cluster-
level features (i.e., features that are defined over
any subset of NPs in a preceding cluster). Com-
bining the entity-mention model and the mention-
ranking model yields the CR model, which ranks
the preceding clusters for an active NP so that
the highest-ranked preceding cluster is the one to
which the active NP should be linked.

Since the CR model ranks preceding clusters,
a training instance i(cj , NPk) represents a preced-
ing cluster cj and an anaphoric NP NPk . Each in-
stance consists of two types of features: (1) fea-
tures that are computed based solely on NPk, and
(2) cluster-level features, which describe the rela-
tionship between cj and NPk . Motivated in part by
Culotta et al. (2007), we create cluster-level fea-
tures from the relational features in our 39-feature
set using four logical predicates: NONE, MOST-
FALSE, MOST-TRUE, and ALL. Specifically, for
each relational feature X, we first convert X into
an equivalent set of binary-valued features if it
is multi-valued. Then, for each resulting binary-
valued feature Xb, we create four binary-valued
cluster-level features: (1) NONE-Xb is true when
Xb is false between NPk and each NP in cj ; (2)
MOST-FALSE-Xb is true when Xb is true between
NPk and less than half (but at least one) of the NPs
in cj ; (3) MOST-TRUE-Xb is true when Xb is true
between NPk and at least half (but not all) of the
NPs in cj ; and (4) ALL-Xb is true when Xb is true
between NPk and each NP in cj .

We follow Rahman and Ng’s (2009) method for
creating training instances. Specifically, for each
NP, NPk, we create a training instance between NPk
and each preceding cluster cj using the features
described above. Since we are training a model
for jointly learning anaphoricity determination and
coreference resolution, we need to provide the
ranker with the option to start a new cluster by cre-
ating an additional training instance that contains
features that solely describe NPk. The rank value
of a training instance i(cj , NPk) created for NPk is
the rank of cj among the competing clusters. If
NPk is anaphoric, the rank of i(cj , NPk) is HIGH if
NPk belongs to cj , and LOW otherwise. However,
if NPk is non-anaphoric, the rank of i(cj , NPk) is
LOW unless cj corresponds to the NULL cluster, in
which case its rank is HIGH. Given these training
instances, we can train a ranker using SVMlight’s

467



ranker-learning algorithm.
After training, the cluster ranker processes the

NPs in a test text in a left-to-right manner. For
each active NP, NPk, we create test instances for it
by pairing it with each of its preceding clusters. To
allow for the possibility that NPk is non-anaphoric,
we create an additional test instance containing
features that solely describe the active NP (as dur-
ing training). All these test instances are then pre-
sented to the ranker. If the additional test instance
is assigned the highest rank value by the ranker,
then NPk is classified as non-anaphoric and will not
be resolved. Otherwise, NPk is linked to the cluster
that has the highest rank.

3 Tree-Based and Path-Based Features

In this section, we describe the tree-based and
path-based features in detail and show how they
can be exploited by the joint CR model.

3.1 Path-Based Features

As mentioned before, a path-based feature en-
codes the contextual relationship between an ac-
tive NP and a candidate antecedent as the shortest
path between the corresponding nodes in the parse
tree. More formally, a path between an active NP,
NPk, and a candidate antecedent, NPj , in a parse
tree is defined as the shortest sequence of nodes in
the tree that need to be traversed in order to reach
NPj from NPk , and is represented as a sequence of
non-terminal symbols, s1s2 . . . sm, where si (1 ≤
i≤m) is the non-terminal symbol associated with
the ith node being traversed in the path, with s1
and sm being the non-terminal symbol associated
with the nodes spanning NPk and NPj , respectively.
Given this representation, a path captures the shal-
low syntactic context in which two NPs appear.

There is a caveat, however. If the active NP
and a candidate antecedent appear in different sen-
tences, there will be no path between them. To en-
able the application of path-based features to these
NPs, we create an additional “root” node with a
random label (e.g., R) that connects the root nodes
of the two trees containing these NPs. This allows
a path to be established even if the two NPs appear
in different sentences.

Now, to employ these paths for coreference res-
olution, two questions need to be answered. First,
which paths should be used? In our implemen-
tation, we collect from each training text a path
between each NP and each of its preceding NPs.

This yields approximately 512K paths. For effi-
ciency reasons, we reduce the number of paths be-
ing considered by removing those paths that oc-
cur less than seven times in the training set. Af-
ter this filtering process, only approximately 22K
paths remain. Each resulting path is represented as
a binary-valued feature for coreference resolution.

Second, how can we compute the value of a
path-based feature? If we were to train an MP
model, its value is 1 if the path between the two
NPs under consideration is the same as the path
represented by the feature. Otherwise, its value is
0. Since we are training a joint CR model, where
each instance corresponds to an NP, NPk , and a pre-
ceding cluster, cj , rather than two NPs, we com-
pute its feature value as follows: its value is 1 if
the path between NPk and one of the NPs in cj is
the same as the path represented by the feature;
otherwise, its value is 0.

We hypothesize that by capturing shallow syn-
tactic context, path-based features can improve the
performance of a coreference system. The reason
is that through these features, a learner can poten-
tially learn to distinguish between good paths (i.e.,
paths that are likely to connect coreferent NPs)
and bad paths (i.e., paths that are likely to connect
non-coreferent NPs), thus improving the resulting
model’s ability to identify the correct antecedent
or preceding cluster for an active NP.

3.2 Tree-Based Features

Not only can parse trees be exploited to identify
coreference relations via the extraction of paths,
but they can be used to determine the anaphoric-
ity of an NP. Specifically, we aim to identify non-
anaphoric NPs by employing parse trees as struc-
tured features. While previous work has employed
parse trees as structured features (Zhou and Kong,
2009), it does so in a pipeline architecture where
anaphoricity determination is performed prior to
coreference resolution. In contrast, we are faced
with the challenge of integrating tree-based struc-
tured features with flat features in a model that in-
volves both joint learning and ranking.

To understand how this can be done, recall that
in the joint CR model, joint learning for anaphoric-
ity determination and coreference resolution is
achieved by introducing an additional training in-
stance, i(NULL, NPk), which is formed between an
active NP, NPk , and a NULL preceding cluster, ef-
fectively providing NPk with an option to start a

468



new cluster. Since we aim to use tree-based fea-
tures to identify non-anaphoric NPs, we augment
the set of features for i(NULL, NPk), which cur-
rently contains the flat features derived from NPk,
with these (structured) tree-based features.

Of course, having an SVM learner learn a rank-
ing model from both the flat and tree-based fea-
tures requires more than just adding the tree-based
features to the feature set. In particular, we need
to implement the three steps below.

Step 1: Specifying the Parse Substructure
While we want to use a parse tree directly as a

feature, we do not want to use the entire tree as
a feature. The reason is that a complex tree may
make it difficult for the SVM learner to make gen-
eralizations: the more complex the tree is, the less
likely it is to find similar trees in other instances.

To strike a better balance between having a
rich representation of context and improving the
learner’s ability to generalize, we extract a sub-
structure from a parse tree and use it as the value
of the structured feature of an instance. This sub-
structure was previously shown to be useful when
used as a structured feature for training a classi-
fier for determining the information status of an
NP (Rahman and Ng, 2011). Given an instance
i(NULL, NPk), we extract the substructure from the
parse tree containing NPk as follows. Let n(NPk) be
the root of the subtree that spans all and only the
words in NPk , and let Parent(n(NPk)) be its imme-
diate parent node. We (1) take the subtree rooted
at Parent(n(NPk)), (2) replace each leaf node in
this subtree with a node labeled X, (3) replace the
child nodes of n(NPk) with a leaf node labeled Y,
and (4) use the subtree rooted at Parent(n(NPk))
as the structured feature for i(NULL, NPk). Fig-
ure 1 illustrates this substructure extraction proce-
dure via an example.

Intuitively, the first three steps aim to provide
generalizations by simplifying the tree. For exam-
ple, step (1) allows us to focus on using a small
window surrounding NPk as its context. Steps (2)
and (3) help generalization by ignoring the words
within NPk and its context. Note that using two la-
bels, X and Y, helps distinguish the active NP from
its context within this substructure. Also note that
we simply use one node (Y) to represent the ac-
tive NP, since NP-internal information (e.g., gen-
der) has been captured by the flat features.

While this parse substructure ignores the words
in NPk, these unigrams could be useful for deter-

S
aaaa

!!!!
NP

We

VPPPPP��
����

VBD

spent

NP

@@��
CD

one

NP

day

PP

@@��
IN

in

NNP

Dallas

VP
aaa��

!!!
VBD

X

NP

Y

PP

@@��
IN

X

NNP

X

Figure 1: A parse tree (left) and the parse substruc-
ture extracted for the NP “one day” (right).

mining its anaphoricity, as a learner may learn
from coreference-annotated data that “it” only has
a moderate probability of being anaphoric, and
that “the contrary” from the phrase “on the con-
trary” is never anaphoric. As a result, we augment
the set of flat features in i(NULL, NPk) with the un-
igrams extracted from NPk.

Step 2: Recasting Ranking as Classification
Existing implementations of SVMs, such as
SVMlight-TK (Moschitti, 2004), allow us to com-
bine flat and (structured) tree-based features to
train a classifier by designing appropriate kernels.
Hence, if we were to train an SVM classifier, all
we need to do is to design a kernel. However, we
are given a ranking problem, and it is not immedi-
ately clear how an SVM can learn a ranking model
in the presence of tree-based features.

Our approach to this problem is to reduce the
given ranking problem to an equivalent classifica-
tion problem. Once we have a classification prob-
lem, all we need to do is to design a kernel for
training a classifier, as mentioned above. To re-
duce a ranking problem to an equivalent classifi-
cation problem, we need to convert the training set
for the joint CR model to an equivalent training set
that can be used to train a classifier.

Before describing the conversion process, let
us first recall how the training set for a joint CR
model is created. Given a training text D, we cre-
ate from D a set of training instances T for a joint
CR model by taking the union of T1, T2, . . . , Tn,
where Tk (1 ≤ k ≤ n) is the set of training in-
stances generated from NPk in D. If NPk has |C|
preceding clusters, Tk will contain exactly |C|+1
training instances, since one training instance is
generated from NPk and each of its |C| preced-
ing clusters, and one training instance is formed
between NPk and the NULL antecedent. Each in-
stance is associated with a rank value, which is
either HIGH or LOW. Given T , the SVM ranker-

469



learning algorithm aims to learn how to rank pre-
ceding clusters for an active NP by learning how
to rank the instances within each Tk.

As noted before, to facilitate learning a ranker
from both flat and tree-based features, we refor-
mulate the given ranking problem as a set of pair-
wise ranking problems. The reason is that a pair-
wise ranking problem is essentially a binary clas-
sification problem, since pairwise ranking merely
involves ranking two objects. Not surprisingly,
this reformulation requires that we convert T into
an equivalent training set T ′, which consists of
pairwise ranking problems and can therefore be
used to train a classifier (i.e., a pairwise ranker).
Below we describe how to convert T to T ′.

For each Tk in T , we create a training instance
inst for T ′ from each pair of training instances
in Tk that have different rank values. For exam-
ple, if i(ci, NPk) and i(cj , NPk) in Tk have ranks
r1 and r2 respectively where r1 6= r2, we create a
training instance for T ′ whose feature vector is ob-
tained by subtracting i(cj , NPk) from i(ci, NPk). If
both feature vectors contain only flat features, the
subtraction is straightforward, since each flat fea-
ture is real-valued. However, if one of the feature
vectors has a tree-based feature3 (which happens
when ci or cj is NULL), we handle the flat features
and the tree-based feature separately. Specifically,
we first perform subtraction for the flat features as
described above, and then append the tree-based
feature to the feature set of inst. If r1 > r2, the
class value of inst is 1; otherwise, it is −1.

In sum, each Tk in T constitutes a ranking prob-
lem, and we described how to convert this ranking
problem into a set of pairwise ranking problems in
T ′. As noted before, a pairwise ranking problem is
a binary classification problem. Hence, the result-
ing training set, T ′, can be used to train a (binary)
SVM classifier that minimizes the number of vio-
lations of pairwise rankings in T ′.

Step 3: Designing the Composite Kernel
To train an SVM classifier on T ′, we need to define
a kernel function for computing the similarity be-
tween a pair of instances. If both instances contain
only flat features, we simply employ a normalized
linear kernel, which computes similarity as the co-
sine of their feature vectors. However, if one or
both of them has a tree-based feature, a linear ker-

3Note that at most one of these two feature vectors has
a tree-based feature. The reason is that exactly one of the
instances in Tk has a tree-based feature, namely the one cor-
responding to the NULL cluster.

nel is not directly applicable. In this case, we need
to (1) compute the similarity of their flat features
and the similarity of their tree-based features sep-
arately, and then (2) employ a composite kernel,
Kc, to combine the two similarity values. Specifi-
cally, we define Kc as follows:

Kc(F1, F2) = K1(F1, F2) + αK2(F1, F2),

where F1 and F2 are the full set of features (con-
taining both flat and structured features) that rep-
resent the two instances under consideration. K1
is a linear kernel, which operates on the flat fea-
tures. K2 is a convolution tree kernel (Collins and
Duffy, 2001), which operates on the tree-based
features. Specifically, K2 computes the similarity
of two parse trees by efficiently enumerating the
number of common substructures in them. To pre-
vent the kernel value returned by Kc from being
consistently dominated by one of the component
kernels (i.e., K1 and K2), we normalize the ker-
nel values returned by K1 and K2 so that they fall
between 0 and 1. α is as a weight parameter that
allows the two kernel values to be combined lin-
early, providing the flexibility to vary the relative
importance of the component kernels. We will de-
termine α empirically on the development set.

3.3 Applying the Pairwise Ranker

So far, we have described a method for training
a (pairwise) ranker when the feature set contains
both flat and tree-based features, which involves
converting training set T to training set T ′. A nat-
ural question, then, is: do we have to similarly per-
form this conversion on the test set so that the pair-
wise ranker can be applied to it?

It turns out that the answer is no. Given a set of
test instances Tk to be ranked, all we need to do
is to apply the pairwise ranker to each instance in
Tk. The ranker produces one real value for each
instance. According to the values provided by the
ranker, these test instances can be ranked: the most
positive value corresponds to the highest rank.

It may not be immediately clear why it makes
sense to apply the pairwise ranker in the aforemen-
tioned manner to rank the test instances. Space
limitations preclude a rigorous mathematical ex-
planation. Here, we will provide a sketch of the
explanation. Recall that each instance in T ′ was
created by subtracting the feature vectors of two
instances. In addition, when SVMlight was ap-
plied to train the pairwise ranker on T ′, it at-
tempted to minimize the number of violations of

470



pairwise rankings. To do so, SVMlight needs to
position the hyperplane so that an instance with a
higher rank in T is assigned a more positive value
by the hyperplane than one with a lower rank in T .
Consequently, we can apply the pairwise ranker to
each test instance to be ranked, and use the value
returned by the ranker for each instance to impose
a ranking on the test instances.

4 Evaluation

In this section, we examine the effectiveness of the
tree-based and path-based features in improving
the joint CR model.

4.1 Experimental Setup

Corpus. We employ in our evaluation a dataset
comprising 147 coreference-annotated Switch-
board dialogues, which contain a total of 68,992
NPs.4 We partition the dialogues into a training set
(117 dialogues) and a test set (30 dialogues). We
extract the NPs and the parse trees directly from
the gold-standard annotations, but the coreference
features are computed entirely automatically.

Scoring programs. We employ two commonly-
used coreference scoring programs, B3 (Bagga
and Baldwin, 1998) and φ3-CEAF (Luo, 2005),
both of which report results in terms of recall (R),
precision (P), and F-measure (F).

4.2 Results and Discussion

The baseline mention-pair model. We employ
as our first baseline the MP model, which is
trained using the procedure described in Section
2.1. Given that our goal is to examine the ef-
fectiveness of the tree-based and path-based fea-
tures for the joint CR model, one may wonder why
the results of the MP model are relevant to our
investigation. Recall from the introduction that
we chose to improve the joint CR model with the
two types of features derived from syntactic parses
because the joint CR model has been shown to
achieve state-of-the-art performance on the ACE
corpus. To ensure that the joint CR model also out-
performs the MP model on our Switchboard cor-
pus (and is therefore the strongest baseline we can
use), we show the results of the MP model in row
1 of Table 1. As we can see, it achieves F-measure
scores of 69.1 (B3) and 62.8 (CEAF).5

4This dataset is released by the LDC as part of the NXT
corpus (Calhoun et al., 2010).

5Since gold-standard NPs are used in our coreference ex-
periments, CEAF recall, precision, and F-measure will all be

The baseline joint cluster-ranking model. Our
second baseline is the joint CR model, which is
trained using the method described in Section 2.2.
In particular, this baseline model does not employ
any tree-based or path-based features. Results are
shown in row 2 of Table 1. In comparison to the
MP model in row 1, we can see that B3 F-measure
rises from 69.1 to 74.5 and CEAF F-measure rises
from 62.8 and 68.5. These results are consistent
with our hypothesis that the joint CR model is in-
deed a stronger baseline than the MP model.

Incorporating path-based features. Next, we
incorporate the path-based features into the Base-
line joint CR model. Results are shown in row
3 of Table 1. In comparison to the results of the
Baseline joint CR model in row 2, we can see that
adding the path-based features into the feature set
improves the joint CR model according to both
scorers. In particular, B3 and CEAF F-measure
scores rise by 1.3% and 2.1%, respectively, sug-
gesting the usefulness of the path-based features.

In addition to the R, P and F columns, Table
1 has two columns labeled “% err. red.”, which
show the error reduction of a system relative to
the Baseline joint CR model. Here, we compute
the error of a system by subtracting its F-measure
score from the perfect F-measure (i.e., 100). With
the addition of path-based features, we can see that
relative error is reduced by 5.1 and 6.7 according
to B3 and CEAF, respectively.

Incorporating tree-based features. Next, we
incorporate the tree-based features into the Base-
line joint CR model. Recall that from a tree, we
extract both flat features (i.e., unigrams) and struc-
tured features (i.e., parse substructures), so both
types of features are used to augment the Base-
line feature set. Because both types of features are
involved, we need to tune α in the composite ker-
nel. To ensure a fair comparison among different
systems, we do not employ additional labeled data
for tuning α. Rather, we use 75% of the available
training data for training the joint CR model and
reserve the remaining 25% for parameter tuning.

Results are shown in row 4 of Table 1. In com-
parison to the results of the Baseline joint CR
model in row 2, we can see that adding the trees
and the unigrams into the feature set improves the
joint CR model according to both scorers. In par-
ticular, B3 and CEAF F-measure scores rise by
1.0% and 1.9%, respectively.

the same. See Luo (2005) for details.

471



B3 CEAF
System R P F % err. red. R P F % err. red.

1 Baseline MP model 78.1 61.6 69.1 — 62.8 62.8 62.8 —
2 Baseline CR model 71.1 78.2 74.5 — 68.5 68.5 68.5 —
3 CR + paths 76.4 75.2 75.8 (5.10) 70.6 70.6 70.6 (6.67)
4 CR + unigrams + trees 75.1 76.0 75.5 (3.92) 70.4 70.4 70.4 (6.03)
5 CR + paths + unigrams + trees 76.6 76.8 76.7 (8.63) 72.2 72.2 72.2 (11.74)
6 CR + paths + unigrams 76.3 75.4 75.8 (5.10) 71.5 71.5 71.5 (9.52)
7 CR + paths + pipeline architecture 76.9 75.2 76.0 (5.88) 71.4 71.4 71.4 (9.21)

Table 1: Coreference results on the test set obtained using B3 and CEAF.

Incorporating tree- and path-based features.
Next, we incorporate both tree-based (i.e., un-
igrams and parse substructures) and path-based
features into the Baseline joint CR model. As in
the previous experiment, we reserve 25% of the
available training data for tuning α. Results are
shown in row 5 of Table 1. In comparison to the
results of the Baseline joint CR model in row 2, we
can see that adding both types of features improves
F-measure by 2.2% (B3) and 3.7% (CEAF), which
is equivalent to a relative error reduction of 8.6%
(B3) and 11.7% (CEAF).

In comparison to the results in rows 3 and 4,
we can see that better results can be obtained by
applying the two types of features in combination
than in isolation to the Baseline joint CR model.
This suggests that although both types of features
are derived from parse trees, they provide comple-
mentary information for the CR model.

Understanding the value of parse substruc-
tures. So far, we have always applied the uni-
grams and the parse substructures in combination
in our experiments. To better understand the value
of the parse substructures, we perform an ablation
experiment in which we repeat the previous exper-
iment without using the parse substructures.

Results are shown in row 6 of Table 1. In
comparison to the results in row 5, we can see
that F-measure drops by 0.9% (B3) and 0.7%
(CEAF). Since the difference in results between
the two rows can be attributed entirely to the pres-
ence/absence of the parse substructures, the drop
in F-measure suggests that the parse substructures
are indeed useful features for the joint CR model.

Pipeline vs. joint modeling. One challenge we
addressed here involves enabling the integration
of structured and flat features in a ranker that
performs joint learning. A natural question is:
is this joint learning architecture indeed better
than the traditional pipeline architecture in which
anaphoricity determination is performed prior to

coreference resolution? To answer this question,
we show in row 7 of Table 1 the results ob-
tained using the pipeline architecture, where (1)
an anaphoricity classifier is trained with all the
features used to represent an instance involving
the NULL antecedent in the joint CR model in
row 5 and (2) the joint CR model is trained using
the Baseline and path-based features. This setup
would therefore allow us to determine whether the
joint architecture or the pipeline architecture can
better exploit the structured features. In compari-
son to the results in row 5, we see that F-measure
drops by 0.6–0.8%. These results suggest that
joint learning is indeed better than pipeline learn-
ing in terms of exploiting structured features.

5 Conclusions

We have examined the effectiveness of tree-based
and path-based features in improving a state-of-
the-art supervised coreference model, the cluster-
ranking model. Results on 147 Switchboard dia-
logues, show that both types of features are effec-
tive at improving the performance of the cluster-
ranking model. In particular, when they are ap-
plied in combination, we see a reduction in rela-
tive error by 8.6–11.7%. One challenge that we
addressed during the course of this investigation
involves enabling flat and structured features to be
employed simultaneously in a ranking model that
employs joint learning. With the increasingly im-
portant role structured features and ranking mod-
els play in natural language learning, we believe
that our method for combining flat and structured
features for training a ranker would appeal to re-
searchers working in different areas of NLP.

Acknowledgments

We thank the three anonymous reviewers for their
invaluable comments on an earlier draft of the pa-
per. This work was supported in part by NSF
Grants IIS-0812261 and IIS-1147644.

472



References

Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the
LREC Workshop on Linguistic Coreference, pages
563–566.

Eric Bengtson and Dan Roth. 2008. Understanding
the values of features for coreference resolution. In
Proceedings of EMNLP, pages 294–303.

Sasha Calhoun, Jean Carletta, Jason Brenier, Neil
Mayo, Dan Jurafsky, Mark Steedman, and David
Beaver. 2010. The NXT-format Switchboard cor-
pus: A rich resource for investigating the syntax, se-
mantics, pragmatics and prosody of dialogue. Lan-
guage Resources and Evaluation, 44(4):387–419.

Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in NIPS,
pages 489–496.

Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the ACL, pages 263–270.

Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for corefer-
ence resolution. In Proceedings of NAACL/HLT,
pages 81–88.

Pascal Denis and Jason Baldridge. 2007. Global, joint
determination of anaphoricity and coreference reso-
lution using integer programming. In Proceedings
of NAACL/HLT, pages 236–243.

Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In
Proceedings of EMNLP, pages 660–669.

Aria Haghighi and Dan Klein. 2009. Simple corefer-
ence resolution with rich syntactic and semantic fea-
tures. In Proceedings of EMNLP, pages 1152–1161.

Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2009.
Capturing salience with a trainable cache model for
zero-anaphora resolution. In Proceedings of ACL-
IJCNLP, pages 647–655.

Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and
Alexander Smola, editors, Advances in Kernel Meth-
ods - Support Vector Learning, pages 44–56. MIT
Press.

Dekang Lin and Patrick Pantel. 2001. DIRT: Discov-
ery of inference rules from text. In Proceedings of
KDD, pages 323–328.

Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual
coreference resolution with syntactic features. In
Proceedings of HLT/EMNLP, pages 660–667.

Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the ACL, pages
135–142.

Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of HLT/EMNLP,
pages 25–32.

Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of the ACL, pages 335–342.

Vincent Ng and Claire Cardie. 2002a. Identifying
anaphoric and non-anaphoric noun phrases to im-
prove coreference resolution. In Proceedings of
COLING, pages 730–736.

Vincent Ng and Claire Cardie. 2002b. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the ACL, pages 104–111.

Massimo Poesio, Olga Uryupina, Renata Vieira, Mijail
Alexandrov-Kabadjov, and Rodrigo Goulart. 2004.
Discourse-new detectors for definite description res-
olution: A survey and a preliminary proposal. In
Proeedings of the ACL Workshop on Reference Res-
olution.

Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of
EMNLP, pages 968–977.

Altaf Rahman and Vincent Ng. 2011. Learning the
information status of noun phrases in spoken dia-
logues. In Proceedings of EMNLP, pages 1069–
1080.

Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521–544.

Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of ACL-IJCNLP, pages
656-664.

Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference systems
based on kernels methods. In Proceedings of COL-
ING, pages 961–968.

Bonnie Lynn Webber. 1979. A Formal Approach to
Discourse Anaphora. Garland Publishing, Inc.

Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel based pronoun resolution with structured
syntactic knowledge. In Proceedings of COLING-
ACL, pages 41–48.

Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, and
Sheng Li. 2008. An entity-mention model for coref-
erence resolution with inductive logic programming.
In Proceedings of the ACL, pages 843–851.

GuoDong Zhou and Fang Kong. 2009. Global learn-
ing of noun phrase anaphoricity in coreference res-
olution via label propagation. In Proceedings of
EMNLP, pages 978–986.

473


