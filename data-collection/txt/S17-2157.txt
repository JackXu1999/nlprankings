



















































Oxford at SemEval-2017 Task 9: Neural AMR Parsing with Pointer-Augmented Attention


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 914–919,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

Oxford at SemEval-2017 Task 9: Neural AMR Parsing with
Pointer-Augmented Attention

Jan Buys1 and Phil Blunsom1,2
1Department of Computer Science, University of Oxford 2DeepMind

{jan.buys,phil.blunsom}@cs.ox.ac.uk

Abstract

We present an end-to-end neural encoder-
decoder AMR parser that extends an
attention-based model by predicting the
alignment between graph nodes and sen-
tence tokens explicitly with a pointer
mechanism. Candidate lemmas are pre-
dicted as a pre-processing step so that the
lemmas of lexical concepts, as well as
constant strings, are factored out of the
graph linearization and recovered through
the predicted alignments. The approach
does not rely on syntactic parses or ex-
tensive external resources. Our parser ob-
tained 59% Smatch on the SemEval test
set.

1 Introduction

The task of parsing sentences to Abstract Meaning
Representation (AMR) (Banarescu et al., 2013)
has recently received increased attention. AMR
represents sentence meaning with directed acyclic
graphs (DAGs) with labelled nodes and edges. No
assumptions are made about the relation between
an AMR and the structure of the sentence it rep-
resents: the representation is not assumed to have
any relation to the sentence syntax, no alignments
are given and no distinction is made between con-
cepts that correspond directly to lexemes in the in-
put sentences and those that don’t.

This underspecification creates significant chal-
lenges for training an end-to-end AMR parser,
which are exacerbated by the relatively small sizes
of available training sets. Consequently most
AMR parsers are pipelines that make extensive use
of additional resources. Neural encoder-decoders
have previously been proposed for AMR pars-
ing, but reported accuracies are well below the
state-of-the-art (Barzdins and Gosko, 2016), even

with sophisticated pre-processing and categoriza-
tion (Peng et al., 2017). The end-to-end neural
approach contrasts with approaches based on a
pipeline of multiple LSTMs (Foland Jr and Mar-
tin, 2016) or neural network classifiers inside a
feature- and resource-rich parser (Damonte et al.,
2017), which have performed competitively.

Our approach addresses these challenges in
two ways: This first is to utilize (noisy) align-
ments, aligning each graph node to an input to-
ken. The alignments are predicted explicitly by
the neural decoder with a pointer network (Vinyals
et al., 2015), in addition to a standard attention
mechanism. Our second contribution is to in-
troduce more structure in the AMR linearization
by distinguishing between lexical and non-lexical
concepts, noting that lexical concepts (excluding
sense labels) can be predicted with high accuracy
from their lemmas. The decoder predicts only
delexicalized concepts, recovering the lexicaliza-
tion through the lemmas corresponding to the pre-
dicted alignments.

Experiments show that our extensions increase
parsing accuracy by a large margin over a standard
attention-based model.

2 Graph Linearization and
Lemmatization

We start by discussing how to linearize AMR
graphs to enable sequential prediction. AMR node
labels are referred to as concepts and edge labels
as relations. A special class of node modifiers,
called constants, are used to denote the string val-
ues of named entities and numbers. An example
AMR graph is visualized in Figure 1.

In AMR datasets, graphs are represented as
spanning trees with designated root nodes. Edges
whose direction in the spanning tree are reversed
are marked by adding “-of” to the argument label.

914



Figure 1: AMR graph aligned to the sentence it
represents.

:focus( respond-01
:ARG0( obsteoblast )
:ARG1( treat-04

:ARG1*( obsteoblast )
:ARG2( protein

:name( name
:op1( "FGF" ) ) ) ) )

Figure 2: Standard linearized representation of the
AMR in Figure 1.

Edges not included in the spanning tree (reentran-
cies) are indicated by adding dummy nodes point-
ing back to the original nodes.

The first linearization we propose (which we re-
fer to as standard) is similar, except that nodes are
identified through their concepts rather than ex-
plicit node identifiers. Constants are also treated
as nodes. Reentrancy edges are marked with *
and the concepts of their dependent nodes are sim-
ply repeated. During post-processing reentrancies
are recovered heuristically by finding the closest
nodes in the linear representation with the same
concepts. An example of this representation is
given in Figure 2.

In the second representation (lexicalized) ev-
ery graph node is aligned to an input token. The
alignments could be encoded as strings in the
graph linerization, but in our model we will predict
them separately. Every constant is replaced with a
placeholder CONST token; the constant string is

:focus( <2> -01
:ARG0( <1> -u )
:ARG1( <5> -04
:ARG1*( <1> -u )
:ARG2( <4> protein

:name( <4> name
:op1( <4> CONST ) ) ) ) )

Figure 3: Delexicalized linearization, with align-
ments, of the AMR in Figure 1.

then recovered as a post-processing step through
the predicted token alignment.

We classify concepts in an AMR graph as either
lexical, i.e. corresponding directly to the meaning
of an aligned token, or non-lexical. This distinc-
tion, together with alignments, is annotated explic-
itly in Minimal Recursion Semantics predicates in
the English Resource Grammar (ERG) (Copestake
et al., 2005). However for AMR we classify con-
cepts heuristically, based on automatic alignments.
We assume that each word in a sentence aligns to
at most one lexical node in its AMR graph. Where
multiple nodes are aligned to the same token, usu-
ally forming a subgraph, the lowest element is
taken to be the lexical concept.

A subset of AMR concepts are predicates based
on PropBank framesets (Palmer et al., 2005), rep-
resented as sense-labeled lemmas. The remain-
ing lexical concepts are usually English words in
lemma form, while non-lexical concepts are usu-
ally special keywords. Lemmas can be predicted
with high accuracy from the words they align to.

Our third linearization (delexicalized) factorizes
the lemmas of lexical concepts out of the lineriza-
tion, so that they are represented by their align-
ments and sense labels, e.g. -01 for predicates
and -u for other concepts. Candidate lemmas are
predicted independently and lexicalized concepts
are recovered as a post-processing step. This rep-
resentation (see Figure 3) decreases the vocabu-
lary of the decoder, which simplifies the learning
problem and speeds up the parser.

2.1 Pre-processing

We tokenize the data with the Stanford CoreNLP
toolkit (Manning et al., 2014). This tokenization
corresponds more closely to AMR concepts and
constants than other tokenizers we experimented
with, especially due to its handling of hyphenation
in the biomedical domain. We perform POS and

915



NE tagging with the same toolkit.
The training data is aligned with the rule-based

JAMR aligner (Flanigan et al., 2014). However,
our approach requires single-token alignments for
all nodes, which JAMR is not guaranteed to give.
We align each Wiki node to the token with the
highest prefix overlap. Other nodes without align-
ments are aligned to the left-most alignment of
their children (if they have any), otherwise to that
of their parents. JAMR aligns multi-word named
entities as single subgraph to token span align-
ments. We split these alignments to be 1-1 be-
tween tokens and constants. For other nodes with
multi-token alignments we use the start of the
given span.

For each token we predict candidate lexemes us-
ing a number of lexical resources. A summary of
the resources used for each lexical type is given in
Table 1. The first resource is dictionaries extracted
from the aligned training data of each type, map-
ping each token or span of tokens to its most likely
concept lemma or constant. A similar dictionary
is extracted from Propbank framesets (included
in LDC2016E25) for predicate lemmas. Next we
use WordNet (Miller, 1995), as available through
NLTK (Bird et al., 2009), to map words to verbal-
ized forms (for predicates) or nominalized forms
(for other concepts) via their synsets, where avail-
able. To predict constant strings corresponding
to unseen named entities we use the forms pre-
dicted by the Stanford NE tagger (Finkel et al.,
2005), which are broadly consistent with the con-
ventions used for AMR annotation. The same pro-
cedure converts numbers to numerals. We use SU-
Time (Chang and Manning, 2012) to extract nor-
malized forms of dates and time expressions.

Input sentences and output graphs in the train-
ing data are pre-processed independently. This
introduces some noise in the training data, but
makes it more comparable to the setup used dur-
ing testing. The (development set) oracle accuracy
is 98.7% Smatch for the standard representation,
96.16% for the aligned lexicalized representation
and 93.48% for the unlexicalized representation.

3 Pointer-augmented neural attention

Let e1:I be a tokenized English sentence, f1:J a se-
quential representation of its AMR graph and a1:J
an alignment sequence of integers in the range
1 to I . We propose an attention-based encoder-
decoder model (Bahdanau et al., 2015) to encode

e and predict f and a, the latter with a pointer net-
work (Vinyals et al., 2015). We use a standard
LSTM architecture (Jozefowicz et al., 2015).

For every token e we embed its word, POS tag
and named entity (NE) tag as vectors; these em-
beddings are concatenated and passed through a
linear layer such that the output g(e) has the same
dimension as the LSTM cell. This representation
of e is then encoded with a bidirectional RNN.
Each token ei is represented by a hidden state hi,
which is the concatenation of its forward and back-
ward LSTM state vectors.

Let sj be the RNN decoder hidden state at out-
put position j. We set s0 to be the final RNN state
of the backward encoder LSTM. The alignment aj
is predicted at each time-step with a pointer net-
work (Vinyals et al., 2015), although it will only
affect the output when fj is a lexical concept or
constant. The alignment logits are computed with
an MLP (for i = 1, . . . , I):

uij = w
T tanh(W (1)hi +W (2)sj).

The alignment distribution is then given by

p(aj |a1:j−1, f1:j−1, e) = softmax(uj).

Attention is computed similarly, but parameter-
ized separately, and the attention distribution αj
is not observed. Instead qj =

∑i=I
i=1 α

i
jhi is a

weighted average of the encoder states.
The output distribution is computed as follows:

RNN state sj , aligned encoder representation haj
and attention vector qj are fed through a linear
layer to obtain oj , which is then projected to the
output logits vj = Roj + b, such that

p(fj |f1:j−1, e) = softmax(vj).

Let v(fj) be the decoder embedding of fj . To
compute the RNN state at the next time-step, let dj
be the output of a linear layer over d(fj), qj and
haj . The next RNN state is then computed as

sj+1 = RNN(dj , sj).

We perform greedy decoding. We ensure that
the output is well-formed by skipping over out-of-
place symbols. Repeated occurrences of sibling
subtrees are removed when equivalent up to the
argument number of relations.

916



Candidate Type JAMR alignments PropBank WordNet NE Tagger Lemmatizer
Predicates 3 3 3 7 3
Other concepts 3 7 3 7 3
Constants 3 7 7 3 3
Wikification 3 7 7 3 7

Table 1: Resources used to predict candidate lemmas for different types of AMR outputs. The left-most
resource that has a prediction available is used.

Model Smatch F1
Attention, no tags 54.60
Attention, with tags 57.27
Pointer, lexicalized 57.99
Pointer, delexicalized 59.18

Table 2: Development set results for the Bio AMR
corpus.

4 Experiments

We train our models with the two AMR datasets
provided for the shared task: LDC2016E25, a
large corpus of newswire, weblog and discussion
forum text with a training set of 35,498 sentences,
and a smaller dataset in the biomedical domain
(Bio AMR Corpus) with 5,542 training sentences.
When training a parser for the biomedical domain
with minibatch SGD, we sample Bio AMR sen-
tences with a weight of 7 to each LDC sentence to
balance the two sources in sampled minibatches.

Our models are implemented in Tensor-
Flow (Abadi et al., 2015). We train models with
Adam (Kingma and Ba, 2015) with learning rate
0.01 and minibatch size 64. Gradients norms are
clipped to 5.0 (Pascanu et al., 2013). We use
single-layer LSTMs with hidden state size 256,
with dropout 0.3 on the input and output connec-
tions. The encoder takes word embeddings of size
512, initialized (in the first 100 dimensions) with
embeddings trained with a structured skip-gram
model (Ling et al., 2015), and POS and NE em-
beddings of size 32. Singleton tokens are replaced
with an unknown word symbol with probability
0.5 during training.

We compare our pointer-based architecture
against an attention-based encoder-decoder that
does not make use of alignments or external lex-
ical resources. We report results for two versions
of this baseline: In the first, the input is purely
word-based. The second embeds named entity
and POS embeddings in the encoder, and utilizes
pre-trained word embeddings. Development set

Metric Neural AMR (average)
Smatch 59 (53.67)
Unlabeled 63 (57.83)
No WSD 59 (53.67)
Named Entities 66 (55.83)
Wikification 18 (33.00)
Negation 27 (23.17)
Concepts 74 (71.17)
Reentrancies 43 (34.17)
SRL 57 (50.33)

Table 3: SemEval test set results on various met-
rics, reported as rounded to the nearest percentage.

Model Smatch F1
Bio AMR 59.27
LDC 61.89

Table 4: Test set results for the Bio AMR and
LDC2016E25 corpora.

results are given in Table 2. We see that POS
and NE embeddings give a substantial improve-
ment. The performance of the baseline with richer
embeddings is similar to that of the first pointer-
based model. The main difference between these
two models is that the latter uses pointers to pre-
dict constants, so the results show that the gain
due to this improved generalization is relatively
small. The delexicalized representation with sepa-
rate lemma prediction improves accuracy by 1.2%.

Official results on the shared task test set are
presented in Table 3. AMR graphs are evaluated
with Smatch (Cai and Knight, 2013), and further
analysis is done with the metrics proposed by Da-
monte et al. (2017). The performance of our model
is consistently better than the shared task average
on all metrics except for Wikification; the reason
for this is that we are not using a Wikifier to pre-
dict Wiki entries. The performance on predict-
ing reentrancies is particularly encouraging, as it
shows that our pointer-based model is able to learn
to point to concepts with multiple occurrences.

917



To enable future comparison we also report re-
sults on the Bio AMR test set, as well as for train-
ing and testing on the newswire and discussion fo-
rum data (LDC2016E25) only (Table 4).

5 Conclusion

We proposed a novel approach to neural AMR
parsing. Results show that neural encoder-decoder
models can obtain strong performance on AMR
parsing by explicitly modelling structure implicit
in AMR graphs.

Acknowledgments

The first author thanks the financial support of the
Clarendon Fund and the Skye Foundation. We
thank the anonymous reviewers for their feedback.

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dan Mané, Rajat Monga, Sherry Moore,
Derek Murray, Chris Olah, Mike Schuster, Jonathon
Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Va-
sudevan, Fernanda Viégas, Oriol Vinyals, Pete
Warden, Martin Wattenberg, Martin Wicke, Yuan
Yu, and Xiaoqiang Zheng. 2015. TensorFlow:
Large-scale machine learning on heterogeneous sys-
tems. Software available from tensorflow.org.
http://tensorflow.org/.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
ICLR. http://arxiv.org/abs/1409.0473.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and
Nathan Schneider. 2013. Abstract meaning rep-
resentation for sembanking. In Proceedings
of the 7th Linguistic Annotation Workshop and
Interoperability with Discourse. pages 178–186.
http://www.aclweb.org/anthology/W13-2322.

Guntis Barzdins and Didzis Gosko. 2016. Riga at
semeval-2016 task 8: Impact of smatch extensions
and character-level neural translation on AMR pars-
ing accuracy. In Proceedings of SemEval.

Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media.

Shu Cai and Kevin Knight. 2013. Smatch: An evalua-
tion metric for semantic feature structures. In Pro-
ceedings of ACL (2).

Angel X Chang and Christopher D Manning. 2012.
SUTime: A library for recognizing and normalizing
time expressions. In Proceedings of LREC.

Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A Sag. 2005. Minimal recursion semantics: An
introduction. Research on Language and Computa-
tion 3(2-3):281–332.

Marco Damonte, Shay B. Cohen, and Giorgio Satta.
2017. An incremental parser for abstract meaning
representation. In Proceedings of EACL. pages 536–
546. http://www.aclweb.org/anthology/E17-1051.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of ACL. pages 363–370.
http://dx.doi.org/10.3115/1219840.1219885.

Jeffrey Flanigan, Sam Thomson, Jaime G. Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A discrimi-
native graph-based parser for the abstract meaning
representation. In Proceedings of ACL. pages 1426–
1436. http://aclweb.org/anthology/P/P14/P14-
1134.pdf.

William R Foland Jr and James H Martin. 2016. CU-
NLP at SemEval-2016 Task 8: AMR parsing us-
ing LSTM-based recurrent neural networks. In Pro-
ceedings of SemEval. pages 1197–1201.

Rafal Jozefowicz, Wojciech Zaremba, and Ilya
Sutskever. 2015. An empirical exploration of recur-
rent network architectures. In Proceedings of ICML.
pages 2342–2350.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of ICLR. http://arxiv.org/abs/1412.6980.

Wang Ling, Chris Dyer, Alan W Black, and Is-
abel Trancoso. 2015. Two/too simple adap-
tations of word2vec for syntax problems. In
Proceedings of NAACL. pages 1299–1304.
http://www.aclweb.org/anthology/N15-1142.

Christopher D. Manning, Mihai Surdeanu, John
Bauer, Jenny Finkel, Steven J. Bethard,
and David McClosky. 2014. The Stanford
CoreNLP natural language processing toolkit.
In ACL System Demonstrations. pages 55–60.
http://www.aclweb.org/anthology/P/P14/P14-5010.

George A. Miller. 1995. Wordnet: A lexical
database for english. Commun. ACM 38(11):39–41.
https://doi.org/10.1145/219717.219748.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational linguistics 31(1):71–
106.

918



Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. Proceedings of ICML (3) 28:1310–1318.

Xiaochang Peng, Chuan Wang, Daniel Gildea,
and Nianwen Xue. 2017. Addressing the
data sparsity issue in neural amr parsing.
In Proceedings of EACL. pages 366–375.
http://www.aclweb.org/anthology/E17-1035.

Oriol Vinyals, Meire Fortunato, and Navdeep
Jaitly. 2015. Pointer networks. In Ad-
vances in Neural Information Processing Sys-
tems, Curran Associates, Inc., pages 2692–
2700. http://papers.nips.cc/paper/5866-pointer-
networks.pdf.

919


