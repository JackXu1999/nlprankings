



















































The Lifted Matrix-Space Model for Semantic Composition


Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 508–518
Brussels, Belgium, October 31 - November 1, 2018. c©2018 Association for Computational Linguistics

508

The Lifted Matrix-Space Model for Semantic Composition

WooJin Chung1
woojin@nyu.edu

Sheng-Fu Wang1
shengfu.wang@nyu.edu

Samuel R. Bowman1,2
bowman@nyu.edu

1Dept. of Linguistics
New York University
10 Washington Place
New York, NY 10003

2Center for Data Science
New York University

60 Fifth Avenue
New York, NY 10011

Abstract
Tree-structured neural network architectures
for sentence encoding draw inspiration from
the approach to semantic composition gener-
ally seen in formal linguistics, and have shown
empirical improvements over comparable se-
quence models by doing so. Moreover, adding
multiplicative interaction terms to the com-
position functions in these models can yield
significant further improvements. However,
existing compositional approaches that adopt
such a powerful composition function scale
poorly, with parameter counts exploding as
model dimension or vocabulary size grows.
We introduce the Lifted Matrix-Space model,
which uses a global transformation to map
vector word embeddings to matrices, which
can then be composed via an operation based
on matrix-matrix multiplication. Its compo-
sition function effectively transmits a larger
number of activations across layers with rela-
tively few model parameters. We evaluate our
model on the Stanford NLI corpus, the Multi-
Genre NLI corpus, and the Stanford Sentiment
Treebank and find that it consistently outper-
forms TreeLSTM (Tai et al., 2015), the previ-
ous best known composition function for tree-
structured models.

1 Introduction

Contemporary theoretical accounts of natural lan-
guage syntax and semantics consistently hold that
sentences are tree-structured, and that the mean-
ing of each node in each tree is calculated from the
meaning of its child nodes using a relatively sim-
ple semantic composition process which is applied
recursively bottom-up (Chierchia and McConnell-
Ginet, 1990; Dowty, 2007). In tree-structured re-
cursive neural networks (TreeRNN; Socher et al.,
2010), a similar procedure is used to build repre-
sentations for sentences for use in natural language
understanding tasks, with distributed representa-
tions for words repeatedly fed through a neural

Figure 1: The Lifted Matrix-Space model in schematic
form. Words are stored as vectors and projected into
matrix space by the LIFT layer. A parametric COMPO-
SITION function combines pairs of these matrices using
multiplicative interactions.

network composition function according to a bi-
nary tree structure supplied by a parser. The suc-
cess of a tree-structured model largely depends on
the design of its composition function.

It has been repeatedly shown that a composition
function that captures multiplicative interactions
between the two items being composed yields bet-
ter results (Rudolph and Giesbrecht, 2010; Socher
et al., 2012, 2013) than do otherwise-equivalent
functions based on simple linear interactions. This
paper presents a novel model which advances this
line of research, the Lifted Matrix-Space model.
We utilize a tensor-parameterized LIFT layer that
learns to produce matrix representations of words
that are dependent on the content of pre-trained
word embedding vectors. Composition of two ma-
trix representations is carried out by a composition
layer, into which the two matrices are sequentially



509

fed. Figure 1 illustrates the model design.

Our model was inspired by Continuation Se-
mantics (Barker and Shan, 2014; Charlow, 2014),
where each symbolic representation of words is
converted to a higher-order function. There is
a consensus in linguistic semantics that a sub-
set of natural language expressions correspond to
higher-order functions. Inspired by the works in
programming language theory, Continuation Se-
mantics takes a step further and claims that all ex-
pressions have to be converted into a higher-order
function before they participate in semantic com-
position. The theory bridges a gap between lin-
guistic semantics and programming language the-
ory, and reinterprets various linguistic phenomena
from the view of computation. While we do not
directly implement Continuation Semantics, we
follow its rough contours: We convert low-level
representations (vectors) to higher-order functions
(matrices), and composition only takes place be-
tween the higher-order functions.

A number of models have been developed to
capture the multiplicative interactions between
distributed representations. While having a sim-
ilar objective, the proposed model requires fewer
model parameters than the predecessors because it
does not necessarily learn each word matrix repre-
sentation separately, and the number of parameters
for the composition function is not proportional to
the cube of the hidden state dimension. Because
of this, it can be trained with larger vocabularies
and more hidden state activations than was possi-
ble with its predecessors.

We evaluate our model primarily on the task
of natural language inference (NLI; MacCartney,
2009). The task consists in determining the infer-
ential relation between a given pair of sentences.
It is a principled and widely-used evaluation task
for natural language understanding, and knowing
the inferential relations is closely related to under-
standing the meaning of an expression (Chierchia
and McConnell-Ginet, 1990). While other tasks
such as question answering or machine transla-
tion require a model to learn task-specific behavior
that goes beyond understanding sentence mean-
ing, NLI results highlight sentence understanding
performance in isolation. We also include an eval-
uation on sentiment classification for comparison
with some earlier work.

We find that our model outperforms existing ap-
proaches to tree-structured modeling on all three

tasks, though it does not set the state of the art on
any of them, falling behind other types of complex
model. We nonetheless expect that this method
will be a valuable ingredient in future models for
sentence understanding and a valuable platform
for research of compositionality in learned repre-
sentations.

2 Related work

Composition functions for tree-structured mod-
els have been thoroughly studied in recent years
(Mitchell and Lapata, 2010; Baroni and Zampar-
elli, 2010; Zanzotto et al., 2010; Socher et al.,
2011). While this line of research has been suc-
cessful, the majority of the existing models ulti-
mately rely on the additive linear combination of
vectors. The Tree-structured recursive neural net-
works (TreeRNN) of Socher et al. (2010) com-
pose two child node vectors ~hl and ~hr using this
method:

(1) ~h = tanh(W

[
~hl
~hr

]
+~b)

where ~hl, ~hr, ~h, ~b ∈ Rd× 1, and W ∈ Rd× 2d.
Throughout this paper, d stands for the number of
activations of a given model.

However, there is no reason to believe that the
additive linear combination of vectors is adequate
for modeling semantic composition. Formal work
in linguistic semantics has shown that many lin-
guistic expressions are well-represented as func-
tions. Accordingly, composing two meanings typ-
ically require feeding an argument into a function
(function application; Heim and Kratzer, 1998).
Such an operation involves a complex interac-
tion between the two meanings, but the classic
TreeRNN does not supply any additional means
to capture the interaction.

Rudolph and Giesbrecht (2010) report that ma-
trix multiplication, as opposed to element-wise ad-
dition, is more suitable for semantic composition.
Their Compositional Matrix-Space model (CMS)
represents words and phrases as matrices, and they
are composed via a simple matrix multiplication:

(2) P = AB

whereA,B, P ∈ Rd × d are matrix representations
of the word embeddings. They provide a formal
proof that element-wise addition/multiplication of
vectors can be subsumed under matrix multipli-
cation. Moreover, they claim that the order-
sensitivity of matrix multiplication is adequate for



510

capturing the semantic composition because natu-
ral language is order-sensitive.

However, as Socher et al. (2012) note, CMS
loses syntactic information during composition
due to the associative character of matrix multipli-
cation. For instance, the following two tree struc-
tures in (3) are syntactically distinct, but CMS
would produce the same result for both structures
because its mode of composition is associative.

(3) a.
A B C

b.
A B C

CMS cannot distinguish the meaning of the two
tree structures and invariably produces ABC (a se-
quence of matrix multiplications). Therefore, the
information on syntactic constituency would be
lost. This makes the model less desirable for han-
dling semantic composition of natural language
expressions for two reasons: First, the principle
of compositionality is violated. Much of the suc-
cess of the tree-structured models can be credited
to the shared hypothesis that the meaning of ev-
ery tree node is derived from the meanings of its
child nodes. Abandoning this principle of com-
positionality gives up the advantage. Second, it
cannot handle structural ambiguities exemplified
in (4).

(4) John saw a man with binoculars.

(5) a.

John

saw a man with binoculars

b.
John

saw
a

man
with binoculars

The sentence has two interpretations that can be
disambiguated with the following paraphrases: (i)
John saw a man via binoculars, and (ii) John saw
a man who has binoculars. The common syntactic
analysis of the ambiguity is that the prepositional
phrase with binoculars can attach to two different
locations. If it attaches to the verb phrase saw a
man, the first interpretation arises. On the other
hand, if it attaches to the noun man, the second
interpretation is given. However, if the structural
information is lost, we would have no way to dis-
ambiguate the two readings.

Socher et al.’s (2012) Matrix-Vector RNN (MV-
RNN) is another attempt to capture the multiplica-
tive interactions between two vectors, while con-
forming to the principle of compositionality. They
hypothesize that representing operators as matri-
ces can better reflect operator semantics. For each
lexical item, a matrix (trained parameter) is as-
signed in addition to the pre-trained word embed-
ding vector. The model aims to assign the right
matrix representations to operators while assign-
ing an identity matrix to words with no operator
meaning. One step of semantic composition is de-
fined as follows:

(6) ~h = f(B~a,A~b) = g(W
[
B~a

A~b

]
)

(7) H = fM (A,B) =WM

[
A
B

]
where ~a,~b, ~h ∈ Rd × 1, A, B, H ∈ Rd × d, and W ,
WM ∈ Rd×2d.

MV-RNN is computationally costly as it needs
to learn an additional d × d matrix for each lexi-
cal item. It is empirically known that the size of
the vocabulary is roughly proportional to the size
of the corpus (Heaps’ law; Herdan, 1960), there-
fore the number of model parameters increases as
the corpus gets bigger. This makes the model less
ideal for handling a large corpus: having a huge
number of parameters causes a problem both for
memory usage and for learning efficiency.

Chen et al. (2013) and Socher et al. (2013)
present the recursive neural tensor network
(RNTN) which reduces the computational com-
plexity of MV-RNN, while capturing the multi-
plicative interactions between child vectors. The
model introduces a third-order tensor V which in-
teracts with the child node vectors as follows:

(8) ~h = tanh(W

[
~hl
~hr

]
+~b+ ~hTl V~hr)

where ~hl, ~hr, ~b ∈ Rd× 1, W ∈ Rd× 2d, and V
∈ Rd× d× d. RNTN improves on MV-RNN in that
its parameter size is not proportional to the size
of the corpus. However, the addition of the third-
order tensor V of dimension d×d×d still requires
proportionally more parameters.

The last composition function relevant to this
paper is the Tree-structured long short-term mem-
ory networks (TreeLSTM; Tai et al., 2015; Zhu
et al., 2015; Le and Zuidema, 2015), particularly
the version over a constituency tree. It is an



511

Model Params. Associative Multiplicative Activation size w.r.t. TreeRNN

TreeRNN/LSTM O(d× d) No No 1
CMS O(V × d× d) Yes Yes 1/V
MV-RNN O(V × d× d) No Yes 1/V
RNTN O(d× d× d) No Yes 1/d

LMS (this work) O(d× demb) No Yes 1/demb

Table 1: Summary of the models. Params. is the number of model parameters (not counting pretrained word
vectors), d is the number of activations at each tree node, demb is the dimension of the word embeddings, and
V is the size of the vocabulary. Associative and Multiplicative indicate whether composition is associative and
whether it includes multiplicative interactions between inputs, respectively. Activation size w.r.t. TreeRNN shows
how activation sizes scale with respect to TreeRNN when all of the models have the same parameter count.

extension of TreeRNN which adapts long short-
term memory (LSTM; Hochreiter and Schmidhu-
ber, 1997) networks. It shares the advantage of
LSTM networks in that it prevents the vanishing
gradient problem (Hochreiter et al., 2001).

Unlike TreeRNN, the output hidden state ~h of
TreeLSTM is not directly calculated from the hid-
den states of its child nodes, ~hl and ~hr. Rather,
each node in TreeLSTM maintains a cell state ~c
that keeps track of important information of its
child nodes. The output hidden state ~h is drawn
from the cell state ~c by passing it through an out-
put gate ~o.

The cell state is calculated in three steps: (i)
Compute a new candidate ~g from ~hl and ~hr.
TreeLSTM selects which values to take from the
new candidate ~g by passing it through an input
gate~i. (ii) Choose which values to forget from the
cell states of the child nodes, ~cl and ~cr. For each
child node, an element-wise product (�) between
its cell state and the forget gate (either ~fl and ~fr,
depending on the child node) is calculated. (iii)
Lastly, sum up the results from (i) and (ii).

(9) ~g = tanh

(
W

[
~hl
~hr

]
+~b

)

(10)


~i
~fl
~fr
~o

 =

σ
σ
σ
σ


(
W

[
~hl
~hr

]
+~b

)

(11) ~c = ~fl � ~cl + ~fr � ~cr +~i� ~g
(12) ~h = ~o� tanh(~c)

TreeLSTM achieves the state-of-the-art perfor-
mance among the tree-structured models in var-
ious tasks, including natural language inference
and sentiment classification. However, there are
non-tree-structured models on the market that

outperform TreeLSTM. Our goal is to design a
stronger composition function that enhances the
performance of tree-structured models. We de-
velop a composition function that captures the
multiplicative interaction between distributed rep-
resentations. At the same time, we improve on the
predecessors in terms of scalability, making the
model more suitable for larger datasets.

To recapitulate, TreeRNN and TreeLSTM re-
flect the principle of compositionality but cannot
capture the multiplicative interaction between two
expressions. In contrast, CMS incorporates mul-
tiplicative interaction but violates the principle of
compositionality. MV-RNN is compositional and
also captures the multiplicative interaction, but
it requires a learned d × d matrix for each vo-
cabulary item. RNTN is also compositional and
incorporates multiplicative interaction, but it re-
quires less parameters than MV-RNN. Neverthe-
less, it requires significantly more parameters than
TreeRNN or TreeLSTM. Table 1 is an overview of
the discussed models.

Other interesting works enrich semantic com-
position with additional context such as grammat-
ical roles or function/argumenthood (Clark et al.,
2008; Erk and Padó, 2008; Grefenstette et al.,
2014; Asher et al., 2016; Weir et al., 2016).

3 The Lifted Matrix-Space model

3.1 Base model

We present the Lifted Matrix-Space model (LMS)
which renders semantic composition in a novel
way. Our model consists of three subparts: the
LIFT layer, the composition layer, and the TreeL-
STM wrapper. The LIFT layer takes a word em-
bedding vector and outputs a corresponding

√
d×√

d matrix (eq. 13).

(13) H = tanh(WLIFT~c+BLIFT)



512

where ~c ∈ Rdemb×1, BLIFT ∈ R
√
d×
√
d, and

WLIFT ∈ R
√
d×
√
d×demb . The resulting H matrix

serves as an input for the composition layer.
Given the matrix representations of two child

nodes,Hl andHr, the composition layer first takes
Hl and returns a hidden state Hinner ∈ R

√
d×
√
d

(eq. 14). SinceHinner is also a matrix, it can func-
tion as the weight matrix for Hr. The composition
layer multiplies Hinner with Hr, adds a bias, and
feeds the result into a non-linear activation func-
tion (eq. 15). This yields Hcand ∈ R

√
d×
√
d,

which for the base model is the output of semantic
composition.

(14) Hinner = tanh(WCOMBHl +BCOMB1)

(15) Hcand = tanh(HinnerHr +BCOMB2)

As in CMS, the primary mode of semantic com-
position is matrix multiplication. However, LMS
improves on CMS in that it avoids associativity.
LMS differs from MV-RNN in that it does not
learn a d × d matrix for each vocabulary item.
Compared to RNTN, LMS transmits a larger num-
ber of activations across layers, given the same
parameter count. In both models, the size of the
third-order tensor is the dominant factor in deter-
mining the number of model parameters. The pa-
rameter count of LMS is approximately propor-
tional to the number of activations (d), but the pa-
rameter count of RNTN is approximately propor-
tional to the cube of the number of activations (d3).
Therefore, LMS can transmit the same number of
activations with fewer model parameters.

3.2 LMS augmented with LSTM components

We augment the base model with LSTM compo-
nents (LMS-LSTM) to circumvent the problem of
long-term dependencies. As in the case of TreeL-
STM, we additionally manage cell states (~cl, ~cr).
Since the LSTM components operate on vectors,
we reshape Hcand, Hl, and Hr into d × 1 column
vectors respectively, and produce ~g, ~hl, and ~hr.
The output of the LSTM components are calcu-
lated based on these vectors, and is reshaped back
to a
√
d×
√
d matrix (eq. 22).

(16) ~g = VECTORIZE(Hcand)

(17) ~hl = VECTORIZE(Hl)

(18) ~hr = VECTORIZE(Hr)

(19)


~i
~fl
~fr
~o

 =

σ
σ
σ
σ


(
W

[
~hl
~hr

]
+B

)

(20) ~c = ~fl � ~cl + ~fr � ~cr +~i� ~g

(21) ~h = ~o� tanh(~c)

(22) H = TO-MATRIX(~h)

3.3 Simplified variants

We implement two LMS-LSTM variants with a
simpler composition function as an ablation study.
The first variant replaces the equations in (14)
and (15) with a single equation (eq. 23), which
does not utilize a weight matrix. It simply mul-
tiplies the matrix representations of two child
nodes Hl, Hr ∈ R

√
d×
√
d, adds a bias BCOMB ∈

R
√
d×
√
d, and feeds the result into a non-linear ac-

tivation function.

(23) Hcand = tanh(HlHr +BCOMB)

The second variant is more complex than the
first, in a way that a weight matrix WCOMB ∈
R
√
d×
√
d is added to the equation (eq. 24). But

unlike the full LMS-LSTM which has two tanh
layers, it only utilizes one.

(24) Hcand = tanh(WCOMBHlHr +BCOMB)

4 Experiments

4.1 Implementation details

As our interest is in the performance of compo-
sition functions, we compare LMS-LSTM with
TreeLSTM, the previous best known composition
function for tree-structured models. To allow for
efficient batching, we use the SPINN-PI-NT ap-
proach (Bowman et al., 2016), which implements
standard TreeLSTM using stack and buffer data
structures borrowed from parsing, rather than tree
structures. We implement our model by replacing
SPINN-PI-NT’s composition function with ours
and adding the LIFT layer.

We use the 300D reference GloVe vectors
(840B token version; Pennington et al., 2014) for
word representations. We fine-tune the word em-
beddings for improved results. We follow Bow-
man et al. (2016) and other prior work in our use
of an MLP with product and difference features to
classify pairs of sentences.



513

(25) ~xclassifier =


~hpremise
~hhypothesis

~hpremise − ~hhypothesis
~hpremise � ~hhypothesis


The feature vector is fed into an MLP that con-

sists of two ReLU neural network layers and a
softmax layer. In both models, the objective func-
tion is a sum of a cross-entropy loss function and
an L2 regularization term. Both models use the
Adam optimizer (Kingma and Ba, 2014). Dropout
(Srivastava et al., 2014) is applied to the classifier
and to the word embeddings. The MLP layer also
utilizes Layer Normalization (Ba et al., 2016).1

4.2 Datasets
We first train and test our models on the Stan-
ford Natural Language Inference corpus (SNLI;
Bowman et al., 2015). The SNLI corpus contains
570,152 pairs of natural language sentences that
are labeled for entailment, contradiction, and neu-
tral. It consists of sentences that were written and
validated by humans. Along with the MultiNLI
corpus introduced below, it is two orders of mag-
nitude larger than other human-authored resources
for NLI tasks. The following example illustrates
the general format of the corpus.

(26) PREMISE: A soccer game with multiple
males playing.
HYPOTHESIS: Some men are playing a
sport.
LABEL: Entailment

We test our models on the Multi-Genre Natural
Language Inference corpus (MultiNLI; Williams
et al., 2017). The corpus consists of 433k pairs
of examples, and each pair is labeled for entail-
ment, contradiction, and neutral. MultiNLI has
the same format as SNLI, so it is possible to train
on both datasets at the same time (as we do when
testing on MultiNLI). Two notable features distin-
guish MultiNLI from SNLI: (i) It is collected from
ten distinct genres of spoken and written English.
This makes the dataset more representative of hu-
man language use. (ii) The examples in MultiNLI
are considerably longer than the ones in SNLI.
These two features make MultiNLI classification
fairly more difficult than SNLI. The pair of sen-
tences in (27) is an illustrative example. The sen-

1The source code and the checkpoints for the mod-
els trained for the NLI tasks are available at https://
github.com/nyu-mll/spinn.

tences are from the section of the corpus that is
transcribed verbatim from telephone speech.

(27) GENRE: Telephone speech
PREMISE: Yes now you know if if every-
body like in August when everybody’s on
vacation or something we can dress a little
more casual or
HYPOTHESIS: August is a black out month
for vacations in the company.
LABEL: Contradiction

The MultiNLI training set consists of five dif-
ferent genres of spoken and written English, the
matched test set contains sentence pairs from only
those five genres, and the mismatched test set con-
tains sentence pairs from additional genres.

We also experiment on the Stanford Sentiment
Treebank (SST; Socher et al., 2013), which is con-
structed by extracting movie review excerpts writ-
ten in English from rottentomatoes.com,
and labeling them with Amazon Mechanical Turk.
Each example in SST is paired with a parse tree,
and each node of the tree is tagged with a fine-
grained sentiment label (5 classes).

5 Results and Analysis

Table 2 summarizes the results on SNLI and
MultiNLI classification. We use the same prepro-
cessing steps for all results we report, including
loading the parse trees supplied with the datasets.
Dropout rate, size of activations, number and size
of MLP layers, and L2 regularization term are
tuned using repeated random search. MV-RNN
and RNTN introduced in the earlier sections are
extremely expensive in terms of computational re-
sources, and training the models with comparative
hyperparameter settings quickly runs out of mem-
ory on a high-end GPU. We do not include them
in the comparison for this reason. TreeLSTM per-
forms the best with one MLP layer, while LMS-
LSTM displays the best performance with two
MLP layers. The difference in parameter count
is largely affected by this choice, and in principle
one model does not demand notably more compu-
tational resources than the other.

On the SNLI test set, LMS-LSTM has an ad-
ditional 1.3% gain over TreeLSTM. Also, both
of the simplified variants of LMS-LSTM outper-
form TreeLSTM, but by a smaller margin. On
the MultiNLI test sets, LMS-LSTM scores 1.3%
higher on the matched test set and 1.9% higher on
the mismatched test set.

https://github.com/nyu-mll/spinn
https://github.com/nyu-mll/spinn
rottentomatoes.com


514

Model Params. S tr. S te. M tr. M te. mat. M te. mism.

Baselines

CBOW (Williams et al., 2017) – – 80.6 – 65.2 64.6
BiLSTM (Williams et al., 2017) 2.8m – 81.5 – 67.5 67.1
Shortcut-Stacked BiLSTM (Nie and Bansal, 2017) 34.7m – 86.1 – 74.6 73.6
DIIN (Gong et al., 2018) – – 88.0 – 78.8 77.8

Existing Tree-Structured Model Runs

300D TreeLSTM (Bowman et al., 2016) 3.4m 84.4 80.9 – – –
300D SPINN-PI (Bowman et al., 2016) 3.7m 89.2 83.2 – – –

Our Experiments

441D LMS (base) 2.0(+11.6m) 79.7 76.5 – – –
441D LMS-LSTM (simplified, −WCOMB,−tanh) 3.3(+11.6)m 90.5 84.1 – – –
324D LMS-LSTM (simplified, −tanh) 2.2(+11.6)m 92.5 84.5 – – –

700D TreeLSTM 2.0(+11.6)m 89.5 83.6 – – –
576D LMS-LSTM (full) 4.6(+11.6)m 86.0 84.9 – – –
700D TreeLSTM 4.6(+30.2)m – – 78.9 70.0 69.7
576D LMS-LSTM (full) 5.9(+30.2)m – – 80.5 71.3 71.6

Table 2: Results on NLI classification with sentence-to-vector encoders. Params. is the approximate number of
model parameters, and the numbers in parentheses indicate the parameters contributed by word embeddings. S tr.,
and S te. are the class accuracies (%) on SNLI train set and test set, respectively. M tr., M te. mat., and M te.
mism. are the class accuracies (%) on MultiNLI train set, matched test set, and mismatched test set, respectively.
Underlining marks the best result among tree-structured models.

We cite the state-of-the-art results of non-tree-
structured models, although these models are only
relevant for our absolute performance numbers.
The Shortcut-Stacked sentence encoder achieves
the state-of-the-art result among non-attention-
based models, outperforming LMS-LSTM. While
this paper focuses on the design of the composi-
tion function, we expect that adding depth along
the lines of Irsoy and Cardie (2014) and shortcut
connections to LMS-LSTM would offer compa-
rable results. Gong et al.’s (2018) attention-based
Densely Interactive Inference Network (DIIN) dis-
plays the state-of-the-art performance among all
models. Applying various attention mechanisms
to tree-structured models is left for future research.

We inspect the performance of the models on
certain subsets of the MultiNLI corpus that mani-
fest linguistically difficult phenomena, which was
categorized by Williams et al. (2017). The phe-
nomena include pronouns, quantifiers (e.g., every,
each, some), modals (e.g., must, can), negation,
wh-terms (e.g., who, where), belief verbs (e.g., be-
lieve, think), time terms (e.g., then, today), dis-
course markers (e.g., but, thus), presupposition
triggers (e.g., again, too), and so on. In linguis-
tic semantics, these phenomena are known to in-
volve complex interactions that are more intricate
than a simple merger of concepts. For instance,
modals express possibilities or necessities that are

LMS-LSTM TreeLSTM
Phenomenon Mat. Mismat. Mat. Mismat.

Pronoun 72.0 71.6 69.6 70.3
Quantifier 72.2 71.7 69.9 70.5
Modal 70.6 70.8 69.8 69.2
Negation 72.3 74.1 70.3 72.4
Wh-term 70.5 69.7 68.6 68.6
Belief verb 70.1 70.1 68.4 68.7
Time term 70.0 71.1 67.3 69.4
Discourse mark. 68.8 68.8 67.0 67.0
Presup. triggers 71.5 71.9 69.1 69.9
Compr./Supr. 69.0 67.5 67.0 67.1
Conditionals 69.7 71.3 68.2 70.5
Tense match 73.3 72.5 71.0 71.2
Interjection 69.7 74.3 69.7 72.5
Adj/Adv 72.6 72.0 70.3 70.7
Determiner 72.4 72.1 70.3 70.8
Length 0-10 72.8 72.8 69.8 72.7
Length 11-14 72.6 72.8 72 70.5
Length 15-19 71.0 70.8 69.3 68.3
Length 20+ 75.2 68.2 69.8 69.0

Table 3: MultiNLI development set classification ac-
curacies (%), classified using the tags introduced in
Williams et al. (2017).

beyond “here and now”. One can say ‘John might
be home’ to convey that there is a possibility that
John is home. The utterance is perfectly compat-
ible with a situation in which John is in fact at
school, so modals like might let us reason about
things that are potentially false in the real world.
We use the same code as Williams et al. (2017) to



515

Model Test

Baselines

MV-RNN (Socher et al., 2013) 44.4
RNTN (Socher et al., 2013) 45.7
Deep RNN (Irsoy and Cardie, 2014) 49.8
TreeLSTM (Tai et al., 2015) 51.0
TreeBiGRU w/ attention 52.4
(Kokkinos and Potamianos, 2017)

Our Experiments

312D TreeLSTM 48.9
144D LMS-LSTM 50.1

Table 4: Five-way test set classification accuracies (%)
on the Stanford Sentiment Treebank.

categorize the data to make fair comparisons.
In addition to the categories offered by Williams

et al. (2017), we inspect whether sentences con-
taining adjectives/adverbs affect the performance
of the models. Baroni and Zamparelli (2010) show
that adjectives are better represented as matrices,
as opposed to vectors. We also inspect whether
the presence of a determiner in the hypothesis that
refers back to a salient referent in the premise
affects the model performance. Determiners are
known to encode intricate properties in linguistic
semantics and have been one of the major research
topics (Elbourne, 2005; Charlow, 2014). Lastly,
we examine whether the performance of the mod-
els varies with respect to sentence length, as longer
sentences are harder to comprehend.

Table 3 summarizes the result of the inspec-
tion on linguistically difficult phenomena. We see
gains uniformly across the board, but with particu-
larly clear gains on negation (+2% on the matched
set/+1.7% on the mismatched set), quanti-
fiers (+2.3%/+1.2%), time terms (+2.7%/+1.7%),
tense matches (+2.3%/+1.3%), adjectives/adverbs
(+2.3%/+1.3%), and longer sentences (length 15-
19: +1.7%/+2.5%; length 20+: +5.4%/-0.8%).

Table 4 summarizes the results on SST classi-
fication, particularly on the fine-grained task with
5 classes. While our implementation does not ex-
actly reproduce Tai et al.’s (2015) TreeLSTM re-
sults, a comparison between our trained TreeL-
STM and LMS-LSTM is consistent with the pat-
terns seen in NLI tasks.

We examine how well the constituent represen-
tations produced by LMS-LSTM and TreeLSTM
encode syntactic category information. As men-
tioned earlier, there is a consensus in linguistic se-
mantics that semantic composition involves func-

Category # of samples Ratio (%)

NP 63346 43.31
VP 30534 20.08
PP 25624 17.98
ROOT 18267 12.49
S 4863 3.06
SBAR 2004 1.20
ADVP 1136 0.27
ADJP 408 0.13
Etc. 160 1.45

Table 5: Syntactic category distribution of SNLI de-
velopment set, classified using the tags introduced in
Bowman et al. (2015).

tion application (i.e., feeding an argument to a
function) which goes beyond a simple merger of
two concepts. Given that the syntactic category
of a node determines whether the node serves as
a function or an argument in semantic composi-
tion, we hypothesize that the distributed represen-
tation of each node would encode syntactic cat-
egory information if the models learned how to
do function application. To assess the quality of
the representations, we first split the SNLI devel-
opment set into training and test sets. From the
training set, we extract the hidden state of every
constituent (i.e., phrase) produced by the best per-
forming models. For each of the models, we train
linear classifiers that learn to do the following two
tasks: (i) 3-way classification, which trains and
tests exclusively on noun phrases, verb phrases,
and prepositional phrases, and (ii) 19-way classi-
fication, which trains and tests on all 19 category
labels attested in the SNLI development set. The
distribution of the 19 category labels is provided
in Table 5. We opt for a linear classifier to keep
the classification process simple, so that we can
properly assess the quality of the constituent rep-
resentations.

Table 6 summarizes the results on the syntac-
tic category classification task. As a baseline, we
train a bag-of-words (BOW) model which pro-
duces the hidden state of a given phrase by sum-
ming the GloVe embeddings of the words of the
phrase. We train and test on the hidden states pro-
duced by BOW as well. The hidden state repre-
sentations produced by LMS-LSTM yield the best
results on both 3-way and 19-way classification
tasks. Comparing LMS-LSTM and TreeLSTM
representations, we see a 5.1% gain on the 3-way
classification and a 5.5% gain on the 19-way clas-
sification.



516

3-way 19-way
Model Train Test Train Test

300D BOW 86.4 85.6 82.7 82.1
700D TreeLSTM 93.2 91.2 90.0 86.6
576D LMS-LSTM 97.3 96.3 94.0 92.1

Table 6: Syntactic category classification accuracies
(%) on SNLI development set, classified using the tags
introduced in Bowman et al. (2015).

Figure 2 depicts the corresponding confusion
matrices for the 19-way classification task. We
show the most frequent eight classes due to space
limitations. We observe notable gains on adver-
bial phrases (ADVP; +24%), adjectival phrases
(ADJP; +12%), verb phrases (VP; +9%), and
clauses introduced by subordinate conjunction
(SBAR; +9%). We also observe a considerable
gain on non-terminal declarative clauses (S; +9%),
although the absolute number is fairly low com-
pared to other categories. While we do not have
a full comprehension of the drop in classification
accuracy, we speculate that the ambiguity of in-
finitival clauses and gerund phrases is one of the
culprits. As exemplified in (28) and (29) respec-
tively, infinitival clauses and gerund phrases are
not only labeled as a VP but also as an S in the
SNLI dataset. Given that our experiment is set up
in a way that each constituent is assigned exactly
one category label, a good number of infinitival
clauses and gerund phrases that are labeled as an
S could have been classified as a VP, resulting in a
drop in classification accuracy. On the other hand,
VP constituents are less affected by the ambiguity
because the majority of them are neither an infini-
tival clause nor a gerund phrase, as shown in (30).

(28) A dog carries a snowball [S [VP to give it to
his owner ]]

(29) Two women are embracing while [S [VP
holding to-go packages after just eating
lunch ]]

(30) A few people [VP are [VP catching fish ]]

With the exclusion of non-terminal declarative
clauses, the categories we see notable gains are
known to play the role of a function in semantic
composition. On the other hand, both models are
efficient in identifying noun phrases (NP), which
are typically arguments of a function in semantic
composition. We speculate that the results are in-
dicative of LMS-LSTM’s ability to identify func-

(a) LMS-LSTM

(b) TreeLSTM

Figure 2: Confusion matrices of LMS-LSTM and
TreeLSTM for 19-way linear classification.

tions and arguments, and this hints that the model
is learning to do function application.

6 Conclusion

In this paper, we propose a novel model for se-
mantic composition that utilizes matrix multiplica-
tion. Experimental results indicate that, while our
model does not reach the state of the art on any of
the three datasets under study, it does substantially
outperform all known tree-structured models, and
lays a strong foundation for future work on tree-
structured compositionality in artificial neural net-
works.

Acknowledgments

We thank NVIDIA Corporation for the donation
of the Titan X Pascal GPU used for this research.
This project has benefited from financial support
to SB by Google, Tencent Holdings, and Samsung
Research.



517

References
Nicholas Asher, Tim Van de Cruys, Antoine Bride,

and Márta Abrusán. 2016. Integrating type the-
ory and distributional semantics: a case study on
adjective–noun compositions. Computational Lin-
guistics, 42(4):703–725.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-
ton. 2016. Layer Normalization. arXiv:1607.06450.

Chris Barker and Chung-chieh Shan. 2014. Continu-
ations and natural language, volume 53. Oxford
studies in theoretical linguistics.

Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Represent-
ing adjective-noun constructions in semantic space.
In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1183–1193.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
ence. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Samuel R. Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D. Manning, and
Christopher Potts. 2016. A fast unified model for
parsing and sentence understanding. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics.

Simon Charlow. 2014. On the semantics of exceptional
scope. Ph.D. thesis, New York University.

Danqi Chen, Richard Socher, Christopher D. Manning,
and Andrew Y. Ng. 2013. Learning new facts from
knowledge bases with neural tensor networks and
semantic word vectors. In Proceedings of workshop
at ICLR, pages 1–4.

Gennaro Chierchia and Sally McConnell-Ginet. 1990.
Meaning and grammar: An introduction to seman-
tics. MIT Press, Cambridge, MA.

Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distribu-
tional model of meaning. In Proceedings of the
Second Quantum Interaction Symposium (QI-2008),
pages 133–140.

David Dowty. 2007. Compositionality as an empirical
problem. In Direct Compositionality. Oxford Uni-
versity Press.

Paul D. Elbourne. 2005. Situations and individuals,
volume 90. Mit Press Cambridge, MA.

Katrin Erk and Sebastian Padó. 2008. A structured vec-
tor space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 897–906.

Yichen Gong, Heng Luo, and Jian Zhang. 2018. Natu-
ral language inference over interaction space. ICLR
2018.

Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2014.
Concrete sentence spaces for compositional distri-
butional models of meaning. In Computing mean-
ing, pages 71–86. Springer.

Irene Heim and Angelika Kratzer. 1998. Semantics in
Generative Grammar. Blackwell Publishers.

Gustav Herdan. 1960. Type-token mathematics. Mou-
ton.

Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and
Jürgen Schmidhuber. 2001. Gradient flow in recur-
rent nets: the difficulty of learning long-term depen-
dencies. A Field Guide to Dynamical Recurrent Net-
works, pages 237–243.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Computation, 9(8):1–
32.

Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Proceedings of Advances in Neural Information
Processing Systems, pages 2096–2104.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
Method for Stochastic Optimization. The Inter-
national Conference on Learning Representations,
pages 1–13.

Filippos Kokkinos and Alexandros Potamianos. 2017.
Structural Attention Neural Networks for improved
sentiment analysis. arXiv:1701.01811.

Phong Le and Willem Zuidema. 2015. Composi-
tional Distributional Semantics with Long Short
Term Memory. arXiv:1503.02510.

Bill MacCartney. 2009. Natural language inference.
Ph.D. thesis, Stanford University.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.

Yixin Nie and Mohit Bansal. 2017. Shortcut-stacked
sentence encoders for multi-domain inference. In
Proceedings of the 2nd Workshop on Evaluating
Vector Space Representations for NLP, pages 41–45.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1532–1543.

Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
907–916.



518

Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 joint conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211.

Richard Socher, Cliff C. Lin, Christopher D. Manning,
and Andrew Y. Ng. 2011. Parsing natural scenes and
natural language with recursive neural networks. In
Proceedings of the 28th international conference on
machine learning (ICML), pages 129–136.

Richard Socher, Christopher D. Manning, and An-
drew Y. Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop, pages 1–9.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive Deep Mod-
els for Semantic Compositionality over a Sentiment
treebank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1631–1642.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: a simple way to prevent neural
networks from overfitting. Journal of Machine
Learning Research, 15(1):1929–1958.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of ACL, pages 1556–1566.

David Weir, Julie Weeds, Jeremy Reffin, and Thomas
Kober. 2016. Aligning packed dependency trees: a
theory of composition for distributional semantics.
Computational Linguistics, 42(4):727–761.

Adina Williams, Nikita Nangia, and Samuel R. Bow-
man. 2017. A Broad-Coverage Challenge Cor-
pus for Sentence Understanding through Inference.
arXiv:1704.05426.

Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 1263–1271.

Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long Short-Term Memory Over Tree Struc-
tures. In Proc. of ICML, pages 1604–1612.


