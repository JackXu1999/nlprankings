



















































Syntax Aware LSTM model for Semantic Role Labeling


Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing, pages 27–32
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Syntax Aware LSTM model for Semantic Role Labeling

Feng Qian2, Lei Sha1, Baobao Chang1, Lu-chen Liu2, Ming Zhang2∗
1 Key Laboratory of Computational Linguistics, Ministry of Education

School of Electronics Engineering and Computer Science, Peking University
2 Institute of Network Computing and Information Systems

School of Electronics Engineering and Computer Science, Peking University
{nickqian, shalei, chbb, liuluchen, mzhang cs}@pku.edu.cn

Abstract

In Semantic Role Labeling (SRL) task, the
tree structured dependency relation is rich
in syntax information, but it is not well
handled by existing models. In this pa-
per, we propose Syntax Aware Long Short
Time Memory (SA-LSTM). The structure
of SA-LSTM changes according to de-
pendency structure of each sentence, so
that SA-LSTM can model the whole tree
structure of dependency relation in an ar-
chitecture engineering way. Experiments
demonstrate that on Chinese Proposition
Bank (CPB) 1.0, SA-LSTM improves F1
by 2.06% than ordinary bi-LSTM with
feature engineered dependency relation in-
formation, and gives state-of-the-art F1 of
79.92%. On English CoNLL 2005 dataset,
SA-LSTM brings improvement (2.1%) to
bi-LSTM model and also brings slight im-
provement (0.3%) when added to the state-
of-the-art model.

1 Introduction

The task of Semantic Role Labeling (SRL) is to
recognize arguments of a given predicate in a sen-
tence and assign semantic role labels. Many NLP
works such as machine translation (Xiong et al.,
2012; Aziz et al., 2011) benefit from SRL because
of the semantic structure it provides. Figure 1
shows a sentence with semantic role label.

Dependency relation is considered important
for SRL task (Xue, 2008; Punyakanok et al., 2008;
Pradhan et al., 2005), since it can provide rich
structure and syntax information for SRL. At the
bottom of Figure 1 shows dependency of the sen-
tence.

∗ Corresponding Author

             

       

                         

      

WORD

ROLE

IBOES

DEPENDENCY
PARSING

Police now investigate accident cause

A0 A1REL

S–A0

AM–TMP

S–AM–TMP B–A1 E–A1REL

advmod

nsubj

compound–nn

dobj

ROOT

Figure 1: A sentence from Chinese Proposition
Bank 1.0 (CPB 1.0) (Xue and Palmer, 2003) with
semantic role labels and dependency.

Traditional methods (Sun and Jurafsky, 2004;
Xue, 2008; Ding and Chang, 2008, 2009; Sun,
2010) do classification according to manually de-
signed features. Feature engineering requires ex-
pertise and is labor intensive. Recent works based
on Recurrent Neural Network (RNN) (Zhou and
Xu, 2015; Wang et al., 2015; He et al., 2017) ex-
tract features automatically, and significantly out-
perform traditional methods. However, because
RNN methods treat language as sequential data,
they fail to integrate the tree structured depen-
dency into RNN.

We propose Syntax Aware Long Short Time
Memory (SA-LSTM) to directly model complex
tree structure of dependency relation in an ar-
chitecture engineering way. Architecture of SA-
LSTM is shown in Figure 2. SA-LSTM is based
on bidirectional LSTM (bi-LSTM). In order to
model the whole dependency tree, we add addi-
tional directed connections between dependency
related words in bi-LSTM. SA-LSTM integrates
the whole dependency tree directly into the model
in an architecture engineering way. Also, to take
dependency relation type into account, we intro-
duce trainable weights for different types of de-
pendency relation. The weights can be trained to
indicate importance of a dependency type.

SA-LSTM is able to directly model the whole
tree structure of dependency relation in an archi-
tecture engineering way. Experiments show that

27



SA-LSTM can model dependency relation bet-
ter than traditional feature engineering way. SA-
LSTM gives state of the art F1 on CPB 1.0 and
also shows improvement on English CoNLL 2005
dataset.

2 Syntax Aware LSTM

In this section, we first introduce ordinary bi-
LSTM. Based on bi-LSTM, we then introduce the
proposed SA-LSTM. Finally, we introduce how to
do optimization for SA-LSTM.

2.1 Conventional bi-LSTM Model for SRL
In a corpus sentence, each word wt has a feature
representation xt which is generated automatically
as (Wang et al., 2015) did. zt is feature embedding
for wt, calculated as followed:

zt = f(W1xt) (1)

where W1 ∈ Rn1×n0 . n0 is the dimension of word
feature representation.

In a corpus sentence, each word wt has six in-
ternal vectors, C̃, gi, gf , go, Ct, and ht, shown in
Equation 2:

C̃ = f(Wczt + Ucht−1 + bc)
gj = σ(Wjzt + Ujht−1 + bj) j ∈ {i, f, o}
Ct = gi � C̃ + gf � Ct−1
ht = go � f(Ct)

(2)

where C̃ is the candidate value of the current cell
state. g are gates used to control the flow of infor-
mation. Ct is the current cell state. ht is hidden
state of wt. Wx and Ux are matrixes used in linear
transformation:

Wx, x ∈ {c, i, f, o} ∈ Rnh×n1
Ux, x ∈ {c, i, f, o} ∈ Rnh×nh

(3)

As convention, f stands for tanh and σ stands for
sigmoid. � means the element-wise multiplica-
tion.

In order to make use of bidirectional informa-
tion, the forward

−→
ht

T
and backward

←−
ht

T
are con-

catenated together, as shown in Equation 4:

at = [
−→
ht

T
,
←−
ht

T
] (4)

Finally, ot is the result vector with each dimension
corresponding to the score of each semantic role
tag, and are calculated as shown in Equation 5:

ot = W3f(W2at) (5)

ht

Ct

ht
ht�1

Zt

Ct�1

tanh

tanh

weighttype

hidden state from dependency related cell

Dependency Tree Structure

Bidirectional RNN Structure

Cell Structure

Architecture Engineering

             
       

                         
      

Police now investigate accident cause

advmod

nsubj

compound–nn
dobj

ROOT

             
       

                         

      

advmod
nsubj

compound–nn

dobj

St

Figure 2: Structure of Syntax Aware LSTM. The
purple square is the current cell that is calculating.
The green square is a dependency related cell.

where W2 ∈ Rn3×n2 , n2 is 2 × ht, W3 ∈
Rn4×n3 and n4 is the number of tags in IOBES
tagging schema.

2.2 Syntax Aware LSTM Model for SRL

This section introduces the proposed SA-LSTM
model. Figure 2 shows the structure of SA-LSTM.
SA-LSTM is based on bidirectional LSTM. By ar-
chitecture engineering, SA-LSTM can model the
whole tree structure of dependency relation.
St is the key component of SA-LSTM. It stands

for information from other dependency related
words, and is calculated as shown in Equation 6:

St = f(
t−1∑
i=0

α× hi) (6)

α =


1 If there exists dependency

relation from wi to wt
0 Otherwise

(7)

St is the weighted sum of all hidden state vectors
hi which come from previous words wi . Note

28



that, α ∈ {0, 1} indicates whether there is a de-
pendency relation pointed from wi to wt.

We add a gate gs to constrain information from
St, as shown in Equation 8:

gs = σ(Wszt + Usht−1 + bs) (8)

To protect the original word information from be-
ing diluted (Wu et al., 2016) by St, we add St to
hidden layer vector ht instead of adding to cell
state Ct. So ht in SA-LSTM cell is calculated as:

ht = go � f(Ct) + gs � St (9)
For example, in Figure 2, there is a dependency

relation “advmod” from green square to purple
square. By Equation 7, only the hidden state of
green square is selected, then transformed by gs
in Equation 8, finally added to hidden layer of the
purple cell.

SA-LSTM changes structure by adding differ-
ent connections according to dependency relation.
In this way, SA-LSTM integrates the whole tree
structure of dependency.

However, by using α in Equation 7, we do not
take dependency type into account, so we further
improve the way α is calculated from Equation 7
to Equation 10. Each typem of dependency rela-
tion is assigned a trainable weight αm. In this way,
SA-LSTM can model differences between types of
dependency relation.

α =


αm exists typem dependency

relation from wi to wt
0 Otherwise

(10)

2.3 Optimization
This section introduces optimization methods for
SA-LSTM. We use maximum likelihood criterion
to train SA-LSTM. We choose stochastic gradient
descent algorithm to optimize parameters.

Given a training pair T = (x, y) where T is
the current training pair, x denotes current train-
ing sentence, and y is the corresponding correct
answer path. yt = k means that the t-th word has
the k-th semantic role label.

The score of ot is calculated as:

s(x, y, θ) =
Ni∑
t=1

otyt (11)

where Ni is the word number of the current sen-
tence and θ stands for all parameters. So the log

Method F1%
Xue(2008) 71.90
Sun et al.(2009) 74.12
Yand and Zong(2014) 75.31
Wang et al.(Bi-LSTM)(2015) 77.09
Sha et al.(2016) 77.69
Path LSTM, Roth et al. (2016)3 79.01
BiLSTM+feature engineering depen-
dency

77.75

SA-LSTM(Random Initialized) 79.81
SA-LSTM(Pre-trained Embedding) 79.92

Table 1: Results comparison on CPB 1.0

likelihood of a single sentence is:

log p(y|x, θ) = log exp(s(x, y, θ))∑
y′ exp(s(x, y′, θ))

= s(x, y, θ)− log
∑

y′
exp(s(x, y′, θ))

(12)

where y′ ranges from all valid paths of answers.
We use Viterbi algorithm to calculate the global
normalization. Besides, we collected those impos-
sible transitions from corpus beforehand. When
calculating global normalization, we prevent cal-
culating impossible paths which contains impossi-
ble transitions.

3 Experiment

3.1 Experiment setting
In order to compare with previous Chinese SRL
works, we choose to do experiment on CPB 1.0.
We also follow the same data setting as previous
Chinese SRL work (Xue, 2008; Sun et al., 2009)
did. Pre-trained1 word embeddings are tested on
SA-LSTM and shows improvement.

For English SRL, we test on CoNLL 2005
dataset.

We use Stanford Parser (Chen and Manning,
2014) to get dependency relation. The training
set of Chinese parser overlaps a part of CPB 1.0
test set, so we retrained the parser. Dimension
of hyper parameters are tuned according to devel-
opment set. n1 = 200, nh = 100, n2 = 200,
n3 = 100, learning rate = 0.001.

3.2 Syntax Aware LSTM Performance
To prove that SA-LSTM models dependency
relation better than simple feature engineering

1Trained by word2vec on Chinese Gigaword Corpus
2All experiment code and related files are available on re-

quest
3We test the model on CPB 1.0

29



0

0.5

1

1.5

2

2.5

nm
od

:a
ss
m
od cc

nm
od

:t
m
od

cc
om

p
ns
ub

jp
as
s

ad
vm

od
:c
om

p
co
nj

am
od

m
ar
k:
cl
f

co
m
po

un
d:
vc

au
x:
as
p

et
c

au
x:
pr
tm

od ne
g

di
sc
ou

rs
e

ad
vc
l:l
oc

au
xp
as
s

au
x:
m
od

al
ns
ub

j:x
su
bj

pa
ra
:p
rn
m
od

nm
od

:t
op

ic
xc
om

p
ns
ub

j
nu

m
m
od

ad
vm

od
pu

nc
t

nm
od

:r
an

ge
nm

od
:p
re
p

ca
se co
p

am
od

:o
dm

od
na

m
e

de
p

ap
po

s
de

t
nm

od
do

bj
m
ar
k

ad
vm

od
:lo

c
co
m
po

un
d:
nn

ad
vm

od
:d
vp

ro
ot

au
x:
ba ac
l

Figure 3: Visualization of trained weight αm. X axis is Universal Dependency type, Y axis is the weight.

Method F1%
Bi-LSTM(2 layers) 74.52
Bi-LSTM + SA-LSTM(2 layers) 76.63
He(2017)(Single Model, state of the art) 81.62
He(Single Model, 8 layers) + SA-LSTM 81.90

Table 2: Results on English CoNLL 2005

method, we design an experiment in which depen-
dency relation is added to bi-LSTM in a traditional
feature engineering way.

Given a word wt, Ft is the average of all depen-
dency related xi of previous words wi, as shown
in Equation 13:

Ft =
1
T

t−1∑
i=0

α× xi (13)

where T is the number of dependency related
words and α is a 0, 1 variable calculated as in
Equation 7.

Then Ft is concatenated to xt to form a new fea-
ture representation. Then these representations are
fed into bi-LSTM.

As shown in Table 1, on CPB 1.0, SA-LSTM
reaches 79.81%F1 score with random initializa-
tion and 79.92%F1 score with pre-trained word
embedding. Both of them are the best F1 score
ever published on CPB 1.0 dataset.

In contrast to the “bi-LSTM+feature engi-
neering dependency” model, it is clear that
architecture method of SA-LSTM gains more
improvement(77.09% to 79.81%) than simple
feature engineering method(77.09% to 77.75%).
Path-LSTM (Roth and Lapata, 2016) embeds de-
pendency path between predicate and argument
for each word using LSTM, then does classifica-
tion according to such path embedding and some
other features. SA-LSTM (79.81%F1) outper-
forms Path-LSTM (79.01%F1) on CPB 1.0.

Both “bi-LSTM + feature engineering depen-
dency” and Path-LSTM only model dependency
parsing information for each single word, which
can not model the whole dependency tree struc-

ture. However, by building the dependency rela-
tion directly into the structure of SA-LSTM and
changing the way information flows, SA-LSTM is
able to model the whole tree structure of depen-
dency relation.

We also test our SA-LSTM on English CoNLL
2005 dataset. Replacing conventional bi-LSTM
by SA-LSTM brings 1.7%F1 improvement. Re-
placing bi-LSTM layers of the state of the art
model (He et al., 2017) by SA-LSTM 1 brings
0.3%F1 improvement.

3.3 Visualization of Trained Weights

According to Equation 10, influence from a sin-
gle type of dependency relation will be multiplied
with type weight αm. When αm is 0, the influence
from this type of dependency relation will be ig-
nored totally. When the weight is bigger, the type
of dependency relation will have more influence
on the whole system.

As shown in Figure 3, dependency relation type
dobj receives the highest weight after training,
as shown by the red bar. According to grammar
knowledge, dobj should be an informative rela-
tion for SRL task, and SA-LSTM gives dobj the
most influence automatically. This further demon-
strate that the result of SA-LSTM is highly in ac-
cordance with grammar knowledge.

4 Related works

Semantic role labeling (SRL) was first de-
fined by (Gildea and Jurafsky, 2002). Early
works (Gildea and Jurafsky, 2002; Sun and Ju-
rafsky, 2004) on SRL got promising result with-
out large annotated SRL corpus. Xue and Palmer
(2003) built the Chinese Proposition Bank to stan-
dardize Chinese SRL research.

Traditional works such as (Xue and Palmer,
2005; Xue, 2008; Ding and Chang, 2009; Sun
et al., 2009; Chen et al., 2006; Yang et al., 2014)

1We add syntax-aware connections to every bi-LSTM
layer in the 8-layer model of (He et al., 2017)

30



use feature engineering methods. Their methods
can take dependency relation into account in fea-
ture engineering way, such as syntactic path fea-
ture. It is obvious that feature engineering method
can not fully capture the tree structure of depen-
dency relation.

More recent SRL works often use neural net-
work based methods. Collobert and Weston
(2008) proposed a Convolutional Neural Network
(CNN) method for SRL. Zhou and Xu (2015) pro-
posed bidirectional RNN-LSTM method for En-
glish SRL, and Wang et al. (2015) proposed a bi-
RNN-LSTM method for Chinese SRL on which
SA-LSTM is based. He et al. (2017) further ex-
tends the work of Zhou and Xu (2015). NN based
methods extract features automatically and sig-
nificantly outperforms traditional methods. How-
ever, most NN based methods can not utilize de-
pendency relation which is considered important
for semantic related NLP tasks (Xue, 2008; Pun-
yakanok et al., 2008; Pradhan et al., 2005).

The work of Roth and Lapata (2016) and Sha
et al. (2016) have the same motivation as SA-
LSTM, but in different ways. Sha et al. (2016)
uses dependency relation as feature to do argu-
ment relations classification. Roth and Lapata
(2016) embeds dependency path into feature rep-
resentation for each word using LSTM. In con-
trast, SA-LSTM utilizes dependency relation in an
architecture engineering way, by integrating the
whole dependency tree structure directly into SA-
LSTM structure.

5 Conclusion

We propose Syntax Aware LSTM model for Se-
mantic Role Labeling (SRL). SA-LSTM is able
to directly model the whole tree structure of de-
pendency relation in an architecture engineering
way. Experiments show that SA-LSTM can model
dependency relation better than traditional feature
engineering way. SA-LSTM gives state of the art
F1 on CPB 1.0 and also shows improvement on
English CoNLL 2005 dataset.

Acknowledgments

Thanks Natali for proofreading the paper and giv-
ing so many valuable suggestions.

This paper is partially supported by the National
Natural Science Foundation of China (NSFC
Grant Nos. 61472006 and 91646202) as well as

the National Basic Research Program (973 Pro-
gram No. 2014CB340405).

References
Wilker Aziz, Miguel Rios, and Lucia Specia. 2011.

Shallow semantic trees for smt. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 316–322. Association for Computational
Linguistics.

Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In EMNLP, pages 740–750.

Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006. An empirical study of chinese chunking. In
Proceedings of the COLING/ACL on Main confer-
ence poster sessions, pages 97–104. Association for
Computational Linguistics.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.

Weiwei Ding and Baobao Chang. 2008. Improving
chinese semantic role classification with hierarchical
feature selection strategy. In Proceedings of the con-
ference on empirical methods in natural language
processing, pages 324–333. Association for Compu-
tational Linguistics.

Weiwei Ding and Baobao Chang. 2009. Word based
chinese semantic role labeling with semantic chunk-
ing. International Journal of Computer Processing
Of Languages, 22(02n03):133–154.

Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245–288.

Luheng He, Kenton Lee, Mike Lewis, and Luke Zettle-
moyer. 2017. Deep semantic role labeling: What
works and whats next. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics.

Sameer Pradhan, Kadri Hacioglu, Wayne Ward,
James H Martin, and Daniel Jurafsky. 2005. Seman-
tic role chunking combining complementary syn-
tactic views. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning,
pages 217–220. Association for Computational Lin-
guistics.

Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257–287.

Michael Roth and Mirella Lapata. 2016. Neural se-
mantic role labeling with dependency path embed-
dings. arXiv preprint arXiv:1605.07515.

31



Lei Sha, Tingsong Jiang, Sujian Li, Baobao Chang, and
Zhifang Sui. 2016. Capturing argument relation-
ships for chinese semantic role labeling. In EMNLP,
pages 2011–2016.

Honglin Sun and Daniel Jurafsky. 2004. Shallow
semantic parsing of chinese. In Proceedings of
NAACL 2004, pages 249–256.

Weiwei Sun. 2010. Improving chinese semantic role
labeling with rich syntactic features. In Proceed-
ings of the ACL 2010 conference short papers, pages
168–172. Association for Computational Linguis-
tics.

Weiwei Sun, Zhifang Sui, Meng Wang, and Xin Wang.
2009. Chinese semantic role labeling with shallow
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3-Volume 3, pages 1475–1483. Asso-
ciation for Computational Linguistics.

Zhen Wang, Tingsong Jiang, Baobao Chang, and Zhi-
fang Sui. 2015. Chinese semantic role labeling with
bidirectional recurrent neural networks. In EMNLP,
pages 1626–1631.

Huijia Wu, Jiajun Zhang, and Chengqing Zong. 2016.
An empirical exploration of skip connections for se-
quential tagging. arXiv preprint arXiv:1610.03167.

Deyi Xiong, Min Zhang, and Haizhou Li. 2012. Mod-
eling the translation of predicate-argument structure
for smt. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 902–911. Asso-
ciation for Computational Linguistics.

Nianwen Xue. 2008. Labeling chinese predicates
with semantic roles. Computational linguistics,
34(2):225–255.

Nianwen Xue and Martha Palmer. 2003. Annotating
the propositions in the penn chinese treebank. In
Proceedings of the second SIGHAN workshop on
Chinese language processing-Volume 17, pages 47–
54. Association for Computational Linguistics.

Nianwen Xue and Martha Palmer. 2005. Automatic
semantic role labeling for chinese verbs. In IJCAI,
volume 5, pages 1160–1165. Citeseer.

Haitong Yang, Chengqing Zong, et al. 2014. Multi-
predicate semantic role labeling. In EMNLP, pages
363–373.

Jie Zhou and Wei Xu. 2015. End-to-end learning of
semantic role labeling using recurrent neural net-
works. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.

32


