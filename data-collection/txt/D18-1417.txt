



















































A Self-Attentive Model with Gate Mechanism for Spoken Language Understanding


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3824–3833
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

3824

A Self-Attentive Model with Gate Mechanism
for Spoken Language Understanding

Changliang Li1, Liang Li2, Ji Qi1
1Kingsoft AI Lab

2Tsinghua University
1{lichangliang,qiji1}@kingsoft.com
2liliang17@mails.tsinghua.edu.cn

Abstract

Spoken Language Understanding (SLU),
which typically involves intent determination
and slot filling, is a core component of spoken
dialogue systems. Joint learning has shown
to be effective in SLU given that slot tags
and intents are supposed to share knowledge
with each other. However, most existing
joint learning methods only consider joint
learning by sharing parameters on surface
level rather than semantic level. In this work,
we propose a novel self-attentive model with
gate mechanism to fully utilize the semantic
correlation between slot and intent. Our model
first obtains intent-augmented embeddings
based on neural network with self-attention
mechanism. And then the intent semantic rep-
resentation is utilized as the gate for labelling
slot tags. The objectives of both tasks are
optimized simultaneously via joint learning in
an end-to-end way. We conduct experiment
on popular benchmark ATIS. The results show
that our model achieves state-of-the-art and
outperforms other popular methods by a large
margin in terms of both intent detection error
rate and slot filling F1-score. This paper gives
a new perspective for research on SLU.

1 Introduction

One long-term goal in artificial intelligence field
is to build an intelligent human-machine dialogue
system, which is capable of understanding hu-
man’s language and giving smooth and correct re-
sponses. A typical dialogue system is designed to
execute the following components: (i) automatic
speech recognition converts a spoken query into
transcription, (ii) spoken language understanding
component analyzes the transcription to extract se-
mantic representations, (iii) dialogue manager in-
terprets the semantic information and decides the
best system action, according to which the system
response is further generated either as a natural

language output(Jurafsky, 2000).
In this paper, we focus on spoken language un-

derstanding which is a core component of a spo-
ken dialogue system. It typically involves two ma-
jor tasks, intent determination and slot filling. In-
tent determination aims to automatically identify
the intent of the user as expressed in natural lan-
guage. Slot filling aims to extract relevant seman-
tic constituents from the natural language sentence
towards achieving a goal.

Usually, intent detection and slot filling are car-
ried out separately. However, separate modeling
of these two tasks is constrained to take full ad-
vantage of all supervised signals. Joint learning of
intent detection and slot filling is worthwhile for
three reasons. Firstly, the two tasks usually appear
simultaneously in SLU systems. Secondly, the in-
formation of one task can be utilized in the other
task to promote each other and a joint prediction
can be made (Zhang and Wang, 2016). For exam-
ple, if the intent of a utterance is to find a flight, it
is likely to contain the departure and arrival cities,
and vice versa. Lastly, slot tags and intents, as
semantics representations of user behaviours, are
supposed to share knowledge with each other.

Recently, joint model for intent detection and
slot filling has achieved much progress. (Xu and
Sarikaya, 2013) proposed using CNN based trian-
gular CRF for joint intent detection and slot fill-
ing. (Guo et al., 2014) proposed using a recursive
neural network that learns hierarchical represen-
tations of the input text for the joint task. (Liu
and Lane, 2016b) describes a recurrent neural net-
work (RNN) model that jointly performs intent de-
tection, slot filling and language modeling. The
neural network models keep updating the intent
prediction as word in the transcribed utterance ar-
rives and uses it as contextual features in the joint
model.

In this work, we propose a novel model for



3825

joint intent determination and slot filling by intro-
ducing self-attention and gating mechanism. Our
model can fully utilize the semantic correlation be-
tween slot and intent. To the best of our knowl-
edge, this is the first attempt to utilize intent-
augmented embedding as a gate to guide the learn-
ing of slot filling task. To fully evaluate the ef-
ficiency of our model, we conduct experiment on
Airline Travel Information Systems (ATIS) dataset
(Hemphill et al., 1990), which is popularly used
as benchmark in related work. And empirical
results show that our independent model outper-
forms the previous best result by 0.54% in terms
of F1-score on slot filling task, and gives excel-
lent performance on intent detection task. Our
joint model further promotes the performance and
achieves state-of-the-art results on both tasks.

The rest of our paper is structured as follows:
Section 2 discusses related work, Section 3 gives
a detailed description of our model, Section 4
presents experiments results and analysis, and
Section 5 summarizes this work and the future di-
rection.

2 Related Work

There is a long research history for spoken dia-
logue understanding, which emerged in the 1990s
from some call classification systems (Gorin et al.,
1997) and the ATIS project. In this section, we de-
scribe some typical works on intent classification
and slot-filling, which are both core tasks of SLU
(De Mori, 2007).

For intent detection task, the early traditional
method is to employ n-grams as features with
generic entities, such as locations and dates
(Zhang and Wang, 2016). This type of method
is restricted to the dimensionality of the input
space. Another line of popular approaches is to
train machine learning models on labeled training
data (Young, 2002; Hahn et al., 2011). For ex-
ample, SVM (Haffner et al., 2003) and Adaboost
(Schapire and Singer, 2000) have been explored
to improve intent detection. Approaches based on
neural network architecture have shown good per-
formance on intent detection task. Deep belief net-
works (DBNs) have been first used in call routing
classification (Deoras and Sarikaya, 2013). More
recently, RNNs have shown excellent performance
on the intent classification task (Ravuri and Stol-
cke, 2015).

For slot-filling task, traditional approaches are

based on conditional random fields (CRF) archi-
tecture, which has strong ability on sequence la-
belling (Raymond and Riccardi, 2007). Recently,
models based on neural network and its extensions
have shown excellent performance on the slot fill-
ing task and outperform traditional CRF models.
For example, (Yao et al., 2013) proposed to take
words as input in a standard recurrent neural net-
work language model, and then to predict slot la-
bels rather than words on the output side. (Yao
et al., 2014b) improved RNNs by using transition
features and the sequence-level optimization cri-
terion of CRF to explicitly model dependencies
of output labels. (Mesnil et al., 2013) tried bi-
directional and hybrid RNN to investigate using
RNN for slot filling. (Yao et al., 2014a) introduced
LSTM architecture for this task and obtained a
marginal improvement over RNN. Besides, fol-
lowing the success of attention based models in
the NLP field, (Simonnet et al., 2015) applied the
attention-based encoder-decoder to the slot filling
task, but without LSTM cells.

Recently, there has been some work on learn-
ing intent detection and slot filling jointly ex-
ploited by neural networks. Slot labels and in-
tents, as semantics of user behaviors, are supposed
to share knowledge with each other. (Guo et al.,
2014) adapted recursive neural networks (RNNs)
for joint training of intent detection and slot fill-
ing. (Xu and Sarikaya, 2013) described a joint
model for intent detection and slot filling based
on convolutional neural networks (CNN). The pro-
posed architecture can be perceived as a neural
network version of the triangular CRF model (Tri-
CRF). (Hakkani-Tür et al., 2016) proposed a sin-
gle recurrent neural network architecture that in-
tegrates the three tasks (domain detection, intent
detection and slot filling for multiple domains)
in a model. (Liu and Lane, 2016a) proposed an
attention-based neural network model for joint in-
tent detection and slot filling. Their joint model
got the best performance of 95.98% slot filling
F1-score and 1.57% intent error rate in the ATIS
dataset.

Despite the great progress those methods have
achieved, it is still a challenging and open task for
intent detection and slot filling. Therefore, we are
motivated to design a powerful model, which can
improve the performance of SLU systems.



3826

from

Embedding

Sentence

Self-Attention

Bi-LSTM Layer

Intent

Self-Attention

Intent Gating

Bi-LSTM

Gate

Softmax

Self-Attention

concat concat concat

Self-Attention

Gate Gate

baltimore to

Intent 

Classifier

O Fromloc.city_name O Flight

Figure 1: Illustration of our proposed model for joint intent detection and slot filling. Red arrows represent the
intent classification task based on the weighted average of BiLSTM outputs. The embeddings coloured with
different intensity values denote word-level and char-level embeddings (three kinds of convolution kernels).

3 Model

In this section, we present our model for the joint
learning of intent detection and slot filling. Figure
1 gives an overview of our model.

The first layer maps input sequence into vec-
tors by concatenating its word-level embeddings
and character-level embeddings (obtained by con-
volution). And we use these vectors as merged
embeddings in downstream layers. In many situa-
tions, contextual information is useful in sequence
labelling. In this paper, we introduce an approach
that leverages context-aware features at each time
step. In particular, we make use of self-attention
to produce context-aware representations of the
embeddings. Then a bidirectional recurrent layer
takes as input the embeddings and context-aware
vectors to produce hidden states. In the last step,
we propose to exploit the intent-augmented gat-
ing mechanism to match the slot label. The gate
for a specific word is obtained by taking a linear
transformation of the intent embedding and an-
other contextual representation of this word com-
puted by self-attention. We apply element-wise
dot-product between the gate and each BiLSTM
output.

Finally, a softmax layer is added to classify the
slot labels on top of the gate layer. For simplic-
ity, we only take the weighted average of BiLSTM

outputs to predict the intent label.
The design of this structure is motivated by the

effectiveness of multiplicative interaction among
vectors and by self-attention mechanism which
has been used successfully in a variety of tasks
(Cheng et al., 2016; Vaswani et al., 2017; Lin et al.,
2017). It also typically corresponds to our finding
that the intent is highly correlated with slot label
in some cases, so the semantics of intent should
be useful for detecting the slot labels.

3.1 Embedding Layer

We first convert the indexed words w =
(w1, w2, ..., wT ) to word-level embeddings Ew =
[ew1 , e

w
2 , ..., e

w
T ], and character-level embeddings

Ec = [ec1, e
c
2, ..., e

c
T ]. Although word embed-

dings are sufficient for many NLP task, pro-
vided by a well-pretrained glove1 or word2vec2,
character-level information provides some more
prior knowledge (e.g. morphemes) to the embed-
ding learning procedure. Some morphemic corre-
lated words are more close in vector space, which
is useful for identifying the slot labels. Character
embeddings also alleviate the out-of-vocabulary
(OOV) problem in the testing phase. In this pa-
per we focus on a character-aware convolution
layer used in (Kim et al., 2016) for words. The

1http://nlp.stanford.edu/projects/glove/
2https://code.google.com/p/word2vec/



3827

character-level embeddings are generated by con-
volution over characters in the word with multiple
window size to extract n-gram features.

Let C be the vocabulary of characters, V be
the vocabulary of words. The dimensions of
character-level embedding and word-level embed-
ding are denoted as dc and dw, respectively. For
each word wt ∈ V , characters in wt constitute
the matrix Ct ∈ Rdc×l, where the columns cor-
responds to l character embeddings.

A narrow convolution is applied between Ct

and a filter (or kernel) H ∈ Rdc×w. Here we sup-
pose the filter width is w. After that, we obtain a
feature map f t ∈ Rl−w+1 by adding a nonlinearity
activation. The final n-gram features is generated
by taking the max-over-time:

f t[i] = relu(H · Ct[:, i : i+ w − 1] + b) (1)
ct = max

i
f t[i] (2)

where Ct[:, i : i + w − 1] is the i-to-(i+w-1)-th
column of Ct, and the character-level embedding
ect is made up of multiple c

t generated by different
convolution kernels.

3.2 Self-Attention

Attention mechanism is usually used to guide the
forming of sentence embedding, extra knowledge
is also used to weigh the CNN or LSTM hidden
states (i.e. document words sometimes attend to
question information). However in slot filling task,
the input to our model is just one sequence. So
the attention mechanism used here is called self-
attention, that is to say, the word at each time step
attends to the whole words in this sentence. And
it helps to determine which region is likely to be a
slot. Since the embedding at each time step con-
sists of multiple parts (i.e. word embedding and
character embeddings of different kernel width),
each part has its own semantic meaning. As shown
in Figure 2, we divide the embedding into mul-
tiple parts and the attention of each part is pro-
cessed within its corresponding dimension. In this
approach, we restrict the interaction among differ-
ent aspects of the embedding. We hypothesize that
different semantic parts are relatively independent
and play different roles in our network.

Suppose M ∈ Rdm×T to be the matrix contain-
ing sentence hidden vectors [m1, ...,mT ], where
dm is the dimension of these T vectors. Consider-
ing the characteristics of slot filing task, our aim

from baltimore to

Wa Wb Wc Wa Wb WcWa Wb Wc

Figure 2: The structure of self-attention layer. Red
coloured rectangles stand for matrices which map the
input to different subspaces. These transformed vec-
tors are divided into multiple parts for computing self-
attention.

is to encode each hidden vector into a context-
aware representation. We achieve that by using
attention over all the sentence hidden vectors M .
Firstly, We linearly map all the vectors in M to
three feature spaces by different projection param-
eters Wa, Wb and Wc, so the resulting vectors
are expressed as Ma, Mb and Mc with the same
shape as M . These matrices are shared across all
time steps. Considering the structure of embed-
ding which consists of K different parts (we use
4 kinds of embeddings with the same dimension),
these transformed matrices are equally split intoK
parts. Furthermore, the attention weight is com-
puted by dot product between Ma and Mb. Lastly,
the attention output is a weighted sum of Mc.

Specifically, we consider different K parts in
detail for k = 1, ..,K:MaMb

Mc

 =
WaMWbM
WcM

 (3)
αk,t = softmax(mTk,a,tMk,b) (4)

S-Att(mt,M) = [M1,cαT1,t, ...,MK,cα
T
K,t] (5)

where Mk,a ∈ R(dm/K)×T is the k-th part of Ma
which is transformed from M by Wa. Index t
is word position ranging over T time steps and
mk,a,t ∈ Rdm/K is the t-th column of Mk,a. αk,t



3828

is the attention weights over Mk,c. The output of
self-attention module generated at time step t is
the concatenation of K parts by using Equation 5.

3.3 BiLSTM
Character embeddings and word embeddings are
both important features in our task. To further uti-
lize these features, we associate each embedding
with a context-aware representation which is typ-
ically implemented by self-attention mechanism.
For current word wt, the input of the recurrent
layer at time step t is represented as xt:

eat = S-Att([e
c
t , e

w
t ], E) (6)

xt = [e
c
t , e

w
t , e

a
t ] (7)

eat is the context-aware vector of wt which is
obtained by applying self-attention mechanism
on the concatenated embeddings E = [ec1 ‖
ew1 , ..., e

c
T ‖ ewT ] .

It was difficult to train RNNs to capture long-
term dependencies because the gradients tend to
either vanish or explode. Therefore, some more
sophisticated activation functions with gating units
were designed. We use LSTM (Hochreiter and
Schmidhuber, 1997) in this work:

it = σ(Wixt + Uiht−1 + bi) (8)

ft = σ(Wfxt + Ufht−1 + bf ) (9)

ot = σ(Woxt + Uoht−1 + bo) (10)

c̃t = tanh(Wcxt + Ucht−1 + bc) (11)

ct = it � c̃t + ft � ct−1 (12)
ht = ot � tanh(ct) (13)

Where � denotes element-wise product of two
vectors. To consider both the previous history and
the future history, we use BiLSTM as encoder in
advance. The bi-directional LSTM (BiLSTM), a
modification of the LSTM, consists of a forward
and a backward LSTM. The encoder reads the in-
put vectors x = (x1, x2, ..., xT ) and generates T
hidden states by concatenating the forward and
backward hidden states of BiLSTM:

−→
h t =

−−−−→
LSTM(xt,

−→
h t−1) (14)

←−
h t =

←−−−−
LSTM(xt,

←−
h t+1) (15)

ht = [
−→
h t,
←−
h t] (16)

where
←−
h t is the hidden state of backward pass in

BLSTM and
−→
h t is the hidden state of forward pass

in BLSTM at time t.

3.4 Intent-Augmented Gating Mechanism
As described above, intent information is useful
for slot filling task. To measure the probability
of words in target slots and attend to the ones
relevant to the intent, we add a gate to the out-
put of BiLSTM layer. Let H ∈ R2d×T be a
matrix consisting of hidden vectors [h1, ..., hT ]
produced by BiLSTM. For each word, we use
self-attention mechanism to form another context-
aware representation, the gate vector h∗t is calcu-
lated by linearly transforming the concatenation
of the context-aware representation and the intent
embedding vector vint with a multi-layer percep-
tron (MLP) network. The intent label is provided
by correct label during training phase, and by the
output from intent classification layer in the test
phase. Specifically, for t = 1, ...T :

st = Self-Attention(ht, H) (17)

h∗t = MLP([st, v
int]) (18)

ot = ht � h∗t (19)

We use element-wise multiplication to model the
interaction between BiLSTM outputs and the gate
vector.

3.5 Task Learning
The bidirectional recurrent layer converts a se-
quence of words w = (w1, w2, ..., wT ) into hid-
den states H = [h1, ..., hT ] which are shared by
two tasks. We use simple attention pooling func-
tion denoted as fatt overH to get an attention-sum
vector for intent label classification. The classified
label yint is transformed to an embedding vint by
matrix Eint for gate computing.

hint = fatt(H) (20)

yint = softmax(W inthint + bint) (21)

During the training phase, model parameters are
updated w.r.t. a cross-entropy loss between the
predicted probabilities and the true label. The la-
bel with maximum probability will be selected as
the predicted intent during the testing phase.

For another task, the hidden states processed by
our gating layer are used for predicting slot labels.

yslott = softmax(W
slotot + b

slot) (22)

Slot filling can be defined as a sequence labelling
problem which is to map a utterance sequence
w = (w1, ..., wT ) to its corresponding slot label



3829

sequence y = (y1, ..., yT ). The objective is to
maximize the likelihood of a sequence:

P (yslot|w) =
T∏
t=1

P (yslott |w) (23)

It is equal to minimize Negative Log-likelihood
(NLL) of the correct labels for the predicted se-
quence yslot.

4 Experiments

4.1 Dataset
In order to evaluate the efficiency of our proposed
model, we conduct experiments on ATIS (Air-
line Travel Information Systems) dataset, which is
widely used as benchmark in SLU research (Price,
1990).

Figure 3 gives one example of sentence in ATIS
dataset. The words are labelled with their value ac-
cording to certain semantic frames. The slot labels
of the words are represented in an In-Out-Begin
(IOB) format and the intent is highlighted with a
box surrounding it.

Sentence   Show  flights    from      Boston        to       New         York      today

 Slots          O  O   O      B-FromCity    O    B-ToCity   I-ToCity   B-Date

Figure 3: Example of sentence annotated by slots sam-
pled from ATIS corpus, the black boxed word indicates
the intent.

In this paper, we use the ATIS corpus set-
ting following previous related works (Liu and
Lane, 2016a; Mesnil et al., 2015; Liu and Lane,
2015; Xu and Sarikaya, 2013; Tur et al., 2010).
The training set contains 4978 utterances from
ATIS-2 and ATIS-3 datasets, and test set contains
893 utterances from ATIS-3 NOV93 and DEC94
datasets. The number of slot labels is 127 and the
intent has 18 different types.

4.2 Metrics
The performance of slot filling task is measured by
the F1-score, while intent detection task is evalu-
ated with prediction error rate that is the ratio of
the incorrect intent of the test data.

4.3 Training Details
We preprocess the ATIS following (Yao et al.,
2013; Liu and Lane, 2016a). To deal with unseen
words in the test set, we mark those words that ap-
pear only once in the training set as 〈UNK〉, and

use this label to represent those unseen words in
the test set. Besides, each number is converted to
the string DIGIT.

The model is implemented in the Tensorflow
framework (Abadi et al., 2016). At training stage,
we use LSTM cell as suggested in (Sutskever
et al., 2014) and the cell dimension d is set to be
128 for both the forward and backward LSTM.

We set the dimension of word embedding dw
to be 64 and the dimension of character embed-
ding dc to be 128. We generate three character-
level embeddings using multiple widths and filters
(the convolution kernel width w ∈ {2, 3, 4} with
64 filters each) followed by a max pooling layer
over time. Then, the dimension of concatenated
embeddings is 256. We make the dimensions of
each parts equal for the convenience of dimension
splitting during the self-attention in later stage. All
the parameters in the network are randomly initial-
ized with uniform distribution (Sussillo and Ab-
bott, 2014) which are fine-tuned during training.
We use the stochastic gradient descent algorithm
(SGD) for updating parameters. And the learning
rate is controlled by Adam algorithm (Kingma and
Ba, 2014). The model is trained on all the train-
ing data with mini-batch size of 16. In order to
enhance our model to generalize well, the maxi-
mum norm for gradient clipping is set to 5. We
also apply layer normalization (Ba et al., 2016) on
the self-attention layer after we add a residul con-
nection between the output and input. Meanwhile,
dropout rate 0.5 is applied on recurrent cell pro-
jection layer (Zaremba et al., 2014) and on each
attention activation.

4.4 Independent Learning

The results of separate training for slot filling and
intent detection are reported in Table 1 and Table 2
respectively. On the independent slot filling task,
we fixed the intent information as the ground truth
labels in the dataset. But on the independent in-
tent detection task, there is no interaction with slot
labels.

Table 1 compares F1-score of slot filling be-
tween our proposed architecture and some previ-
ous works. Our model achieves state-of-the-art
results and outperforms previous best model by
0.56% in terms of F1-score. We attribute the im-
provement of our model to the following reasons:
1) The attention used in (Liu and Lane, 2016a) is
vanilla attention, which is used to compute the de-



3830

Methods F1-score
CRF (Mesnil et al., 2013) 92.94
simple RNN (Yao et al., 2013) 94.11
CNN-CRF (Xu and Sarikaya, 2013) 94.35
LSTM (Yao et al., 2013) 94.85
RNN-SOP (Liu and Lane, 2015) 94.89
Deep LSTM (Yao et al., 2013) 95.08
RNN-EM (Peng et al., 2015) 95.25
Bi-RNN with Ranking Loss (Vu
et al., 2016)

95.47

Encoder-labeler Deep LSTM (Ku-
rata et al., 2016)

95.66

Attention BiRNN (Liu and Lane,
2016a)

95.75

BLSTM-LSTM (focus) (Zhu and
Yu, 2017)

95.79

Our Model 96.35

Table 1: Results of independent training for slot filling
in terms of F1-score.

coding states. It is not suitable for our model since
the embeddings are composed of several parts.
Self-attention allows the model to attend to infor-
mation jointly from different representation parts,
so as to better understand the utterance. 2) intent-
augmented gating layer connects the semantics of
sequence slot labels, which captures complex in-
teractions between the two tasks.

Table 2 compares the performance of our pro-
posed model to previously reported results on in-
tent detection task. Our model gives good per-
formance in terms of classification error rate, but
not as good as Attention Encoder-Decoder (with
aligned inputs) method (Liu and Lane, 2016a).
As their published state-of-the-art result described
in (Liu and Lane, 2016a), their attention-based
model is based on word-level embeddings. While
in our model, we introduce character-level embed-
dings to improve the performance of joint learn-
ing. But independent learning for intent classifi-
cation aims at capturing the global information of
an utterance, not caring much about the details of
specific word. The character-level embeddings in-
troduced in our model bring very little hurt to inde-
pendent learning of intent detection, as a trade-off
in performance between both criterion.

4.5 Joint Learning

We compare our model against the following base-
line models based on joint learning:

Methods Error(%)
Recursive NN (Guo et al., 2014) 4.60
Boosting (Tur et al., 2010) 4.38
Boosting + Simplified sentences
(Tur et al., 2011)

3.02

Attention Enc-Dec (Liu and Lane,
2016a)

2.02

Our Model 2.69

Table 2: Results of independent training for intent de-
tection in terms of error rate.

Methods F1 Error(%)
Recursive NN (Guo et al.,
2014)

93.22 4.60

Recursive NN+Viterbi
(Guo et al., 2014)

93.96 4.60

Attention Enc-Dec (Liu
and Lane, 2016a)

95.87 1.57

Attention BiRNN (Liu
and Lane, 2016a)

95.98 1.79

Our Model 96.52 1.23

Table 3: Results of joint training for slot filling and
intent detection.

• Recursive NN: (Guo et al., 2014) employed
recursive neural networks for joint training of
two tasks.

• Recursive NN + Viterbi: (Guo et al., 2014)
applied the Viterbi algorithm on Recursive
NN to improve the result on slot filling.

• Attention Enc-Dec: (Liu and Lane, 2016a)
proposed Attention Encoder-Decoder (with
aligned inputs) which introduced context
vector as the explicit aligned inputs at each
decoding step.

• Attention BiRNN: (Liu and Lane, 2016a)
introduced attention to the alignment-based
RNN sequence labeling model. Such atten-
tion provides additional information to the in-
tent classification and slot label prediction.

Table 3 compares our joint model with reported
results from previous works. We can see that our
model achieves state-of-the-art results and outper-
forms previous best result by 0.54% in terms of
F1-score on slot filling, and by 0.34% in terms of
error rate on intent detection. This improvement is
statistically significant. Besides, the joint learning



3831

Methods F1-Score Error(%)
W/O char-embedding 96.30 1.23
W/O self-attention 96.26 1.34
W/O attention-gating 96.25 1.46
Full Model 96.52 1.23

Table 4: Feature ablation comparison of our proposed
model on ATIS. slot filling and intent detection result
are shown each row after after we exclude each feature
from the full architecture

achieves better results than separate learning. It
can be interpreted that the two tasks are highly cor-
related and boost the performance each other. The
slot filling task enables the model to learn more
meaningful representations which give more su-
pervisory signals for the learning of shared param-
eters. Similarly, intent is also useful to determine
the slot label.

4.6 Ablation Study
The ablation study is performed to evaluate
whether and how each part of our model con-
tributes to our full model. To further evaluate the
advances of our gating architecture for joint learn-
ing, we ablate some techniques used in our model.
We ablate three important components and con-
duct different approaches in this experiment. Note
that all the variants are based on joint learning with
intent-augmented gate:

• W/O char-embedding, where no character
embeddings are added to the embedding
layer. The embedding layer is composed of
word embeddings only.

• W/O self-attention, where no self-attention is
modelled after the embedding layer and in
the intent-augmented gating layer. The intent
gate is computed by the output of BiLSTM
and intent embedding.

• W/O attention-gating, where no self-
attention mechanism is performed in the
intent-augmented gating layer. The gate is
computed by the output of BiLSTM and
intent embedding. But we still use the
self-attention on top of embedding layer to
augment the context information.

Table 4 shows the joint learning performance of
our model on ATIS data set by removing one mod-
ule at a time. We find that all variants of our model

perform well based on our gate mechanism. As
listed in the table, all features contribute to both
slot filling and intent classification task.

If we remove the self-attention from the holistic
model or just in the intent-augmented gating layer,
the performance drops dramatically. The result
can be interpreted that self-attention mechanism
computes context representation separately and
enhances the interaction of features in the same as-
pect. We can see that self-attention does improve
performance a lot in a large scale, which is consis-
tent with findings of previous work (Vaswani et al.,
2017; Lin et al., 2017).

If we remove character-level embeddings and
only use word-level embeddings, we see 0.22%
drop in terms of F1-score. Though word-level em-
beddings represent the semantics of each word,
character-level embeddings can better handle the
out-of-vocabulary (OOV) problem which is essen-
tial to determine the slot labels.

5 Conclusion

In this paper, we propose a novel self-attentive
model gated with intent for spoken language un-
derstanding. We apply joint learning on both
intent detection and slot filling tasks. In our
model, self-attention mechanism is introduced to
better represent the semantic of utterance, and gate
mechanism is introduced to make full use of the
semantic correlation between slot and intent. Ex-
periment results on ATIS dataset have shown effi-
ciency of our model and outperforms the state-of-
the-art approach on both tasks. Besides, our model
also shows consistent performance gain over the
independent training models. In future works, we
plan to improve our model by introducing extra
knowledge.

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado,
Andy Davis, Jeffrey Dean, Matthieu Devin, et al.
2016. Tensorflow: Large-scale machine learning on
heterogeneous distributed systems. arXiv preprint
arXiv:1603.04467.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. arXiv preprint arXiv:1601.06733.



3832

Renato De Mori. 2007. Spoken language understand-
ing: A survey. In Automatic Speech Recognition
& Understanding, 2007. ASRU. IEEE Workshop on,
pages 365–376. IEEE.

Anoop Deoras and Ruhi Sarikaya. 2013. Deep belief
network based semantic taggers for spoken language
understanding. In Interspeech, pages 2713–2717.

Allen L Gorin, Giuseppe Riccardi, and Jeremy H
Wright. 1997. How may i help you? Speech com-
munication, 23(1):113–127.

Daniel Guo, Gokhan Tur, Wen-tau Yih, and Geoffrey
Zweig. 2014. Joint semantic utterance classifica-
tion and slot filling with recursive neural networks.
In Spoken Language Technology Workshop (SLT),
2014 IEEE, pages 554–559. IEEE.

Patrick Haffner, Gokhan Tur, and Jerry H Wright.
2003. Optimizing svms for complex call classifica-
tion. In Acoustics, Speech, and Signal Processing,
2003. Proceedings.(ICASSP’03). 2003 IEEE Inter-
national Conference on, volume 1, pages I–I. IEEE.

Stefan Hahn, Marco Dinarelli, Christian Raymond,
Fabrice Lefevre, Patrick Lehnen, Renato De Mori,
Alessandro Moschitti, Hermann Ney, and Giuseppe
Riccardi. 2011. Comparing stochastic approaches
to spoken language understanding in multiple lan-
guages. IEEE Transactions on Audio, Speech, and
Language Processing, 19(6):1569–1583.

Dilek Hakkani-Tür, Gökhan Tür, Asli Celikyilmaz,
Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-
Yi Wang. 2016. Multi-domain joint semantic frame
parsing using bi-directional rnn-lstm. In INTER-
SPEECH, pages 715–719.

Charles T Hemphill, John J Godfrey, George R Dod-
dington, et al. 1990. The atis spoken language sys-
tems pilot corpus. In Proceedings of the DARPA
speech and natural language workshop, pages 96–
101.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Dan Jurafsky. 2000. Speech & language processing.
Pearson Education India.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In AAAI, pages 2741–2749.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Gakuto Kurata, Bing Xiang, Bowen Zhou, and Mo Yu.
2016. Leveraging sentencelevel information with
encoder lstm for natural language understanding.
arXiv preprint.

Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San-
tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua
Bengio. 2017. A structured self-attentive sentence
embedding. arXiv preprint arXiv:1703.03130.

Bing Liu and Ian Lane. 2015. Recurrent neural net-
work structured output prediction for spoken lan-
guage understanding. In Proc. NIPS Workshop
on Machine Learning for Spoken Language Under-
standing and Interactions.

Bing Liu and Ian Lane. 2016a. Attention-based recur-
rent neural network models for joint intent detection
and slot filling. arXiv preprint arXiv:1609.01454.

Bing Liu and Ian Lane. 2016b. Joint online spo-
ken language understanding and language model-
ing with recurrent neural networks. arXiv preprint
arXiv:1609.01462.

Grégoire Mesnil, Yann Dauphin, Kaisheng Yao,
Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi-
aodong He, Larry Heck, Gokhan Tur, Dong Yu, et al.
2015. Using recurrent neural networks for slot fill-
ing in spoken language understanding. IEEE/ACM
Transactions on Audio, Speech and Language Pro-
cessing (TASLP), 23(3):530–539.

Grégoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. In Interspeech, pages
3771–3775.

Baolin Peng, Kaisheng Yao, Li Jing, and Kam-Fai
Wong. 2015. Recurrent neural networks with exter-
nal memory for spoken language understanding. In
Natural Language Processing and Chinese Comput-
ing, pages 25–35. Springer.

Patti J Price. 1990. Evaluation of spoken language sys-
tems: The atis domain. In Speech and Natural Lan-
guage: Proceedings of a Workshop Held at Hidden
Valley, Pennsylvania, June 24-27, 1990.

Suman V Ravuri and Andreas Stolcke. 2015. Re-
current neural network and lstm models for lexical
utterance classification. In INTERSPEECH, pages
135–139.

Christian Raymond and Giuseppe Riccardi. 2007.
Generative and discriminative algorithms for spoken
language understanding. In Eighth Annual Confer-
ence of the International Speech Communication As-
sociation.

Robert E Schapire and Yoram Singer. 2000. Boostex-
ter: A boosting-based system for text categorization.
Machine learning, 39(2-3):135–168.

Edwin Simonnet, Nathalie Camelin, Paul Deléglise,
and Yannick Estève. 2015. Exploring the use of
attention-based recurrent neural networks for spo-
ken language understanding. In Machine Learning
for Spoken Language Understanding and Interac-
tion NIPS 2015 workshop (SLUNIPS 2015).



3833

David Sussillo and LF Abbott. 2014. Random walk
initialization for training very deep feedforward net-
works. arXiv preprint arXiv:1412.6558.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Gokhan Tur, Dilek Hakkani-Tür, and Larry Heck.
2010. What is left to be understood in atis? In Spo-
ken Language Technology Workshop (SLT), 2010
IEEE, pages 19–24. IEEE.

Gokhan Tur, Dilek Hakkani-Tür, Larry Heck, and
Sarangarajan Parthasarathy. 2011. Sentence sim-
plification for spoken language understanding. In
Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, pages
5628–5631. IEEE.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. arXiv preprint arXiv:1706.03762.

Ngoc Thang Vu, Pankaj Gupta, Heike Adel, and Hin-
rich Schütze. 2016. Bi-directional recurrent neural
network with ranking loss for spoken language un-
derstanding. In Acoustics, Speech and Signal Pro-
cessing (ICASSP), 2016 IEEE International Confer-
ence on, pages 6060–6064. IEEE.

Puyang Xu and Ruhi Sarikaya. 2013. Convolutional
neural network based triangular crf for joint in-
tent detection and slot filling. In Automatic Speech
Recognition and Understanding (ASRU), 2013 IEEE
Workshop on, pages 78–83. IEEE.

Kaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Ge-
offrey Zweig, and Yangyang Shi. 2014a. Spoken
language understanding using long short-term mem-
ory neural networks. In Spoken Language Technol-
ogy Workshop (SLT), 2014 IEEE, pages 189–194.
IEEE.

Kaisheng Yao, Baolin Peng, Geoffrey Zweig, Dong
Yu, Xiaolong Li, and Feng Gao. 2014b. Recurrent
conditional random field for language understand-
ing. In Acoustics, Speech and Signal Processing
(ICASSP), 2014 IEEE International Conference on,
pages 4077–4081. IEEE.

Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang,
Yangyang Shi, and Dong Yu. 2013. Recurrent neu-
ral networks for language understanding. In Inter-
speech, pages 2524–2528.

Steve J Young. 2002. Talking to machines (statistically
speaking). In INTERSPEECH.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329.

Xiaodong Zhang and Houfeng Wang. 2016. A joint
model of intent determination and slot filling for
spoken language understanding. In IJCAI, pages
2993–2999.

Su Zhu and Kai Yu. 2017. Encoder-decoder with
focus-mechanism for sequence labelling based spo-
ken language understanding. In Acoustics, Speech
and Signal Processing (ICASSP), 2017 IEEE Inter-
national Conference on, pages 5675–5679. IEEE.


