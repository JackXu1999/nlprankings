



















































Estimating Summary Quality with Pairwise Preferences


Proceedings of NAACL-HLT 2018, pages 1687–1696
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Estimating Summary Quality with Pairwise Preferences

Markus Zopf
Research Training Group AIPHES / Knowledge Engineering Group
Department of Computer Science, Technische Universität Darmstadt

Hochschulstraße 10, 64289 Darmstadt, Germany
zopf@aiphes.tu-darmstadt.de

Abstract

Automatic evaluation systems in the field of
automatic summarization have been relying on
the availability of gold standard summaries
for over ten years. Gold standard summaries
are expensive to obtain and often require the
availability of domain experts to achieve high
quality. In this paper, we propose an alter-
native evaluation approach based on pairwise
preferences of sentences. In comparison to
gold standard summaries, they are simpler and
cheaper to obtain. In our experiments, we
show that humans are able to provide useful
feedback in the form of pairwise preferences.
The new framework performs better than the
three most popular versions of ROUGE with
less expensive human input. We also show
that our framework can reuse already available
evaluation data and achieve even better results.

1 Introduction

Due to the huge amount of information contained
in texts, the task of automatic text summarization
(Mani, 2001; Nenkova and McKeown, 2011) is
a pressing challenge nowadays and will become
even more important in the future. Building sum-
marization systems is, however, not the only chal-
lenge in this field. Evaluation of automatically
generated summaries is also an active field of re-
search.

Ideally, we would like to ask humans for their
opinion about the quality of automatically gener-
ated summaries in an extrinsic evaluation (Hal-
teren and Teufel, 2003). Since summaries are gen-
erated for humans, they should also be evaluated
directly by humans. Unfortunately, manual evalu-
ation cannot be performed at a large scale because
of the huge effort which is necessary for evalua-
tion. (Lin, 2004) reported that 3,000 hours of hu-
man effort would be required for a simple evalu-
ation of the summaries for the Document Under-

standing Conference (DUC) 2003, a popular sum-
marization shared task series. This motivates re-
search of automatic evaluation methods for auto-
matic summarization.

ROUGE (Lin, 2004), the current method of
choice for evaluating automated text summariza-
tion, relies on the availability of gold standard
summaries. The gold standard summaries are used
to define the optimal output of a summarization
system. Writing high-quality summaries, how-
ever, requires the availability of expert writers and
takes a lot of effort. (Dang, 2005) reported that
creating the reference summaries for the DUC
2005 shared task was a difficult endeavor with an
effort of five hours to produce each reference sum-
mary. Since ROUGE needs at least four reference
summaries to become reasonably reliable, the ef-
fort sums up to at least 20 hours of annotation ef-
fort per topic. For this reason, gold standard sum-
maries are only available for a few, rather small
datasets. Also the more accurate (but also even
more expensive) Pyramid method (Nenkova and
Passonneau, 2004) requires expensive gold stan-
dard summaries.

Lack of larger and diverse evaluation corpora
limits research in automatic summarization. Fur-
thermore, currently available automatic evaluation
methods are viewed with skepticism (Rankel et al.,
2013). Proper evaluation is, however, an indis-
pensable ingredient for good research. Comput-
ing the similarity between two summaries as in
ROUGE is a very difficult task. This seems to
be obvious since estimating the similarity between
sentences and even words is still an active field of
research.

In this work, we present an alternative evalu-
ation framework which does not use gold stan-
dard summaries to estimate the quality of sum-
maries. Instead of comparing automatically gen-
erated summaries with gold standard summaries,

1687



our model is trained with simple and inexpensive
pairwise preferences (Thurstone, 1927; Fürnkranz
and Hüllermeier, 2010) of sentences. To this end,
we provide pairs of sentences from the input doc-
ument of a summarization task to human anno-
tators and ask which of the two sentences con-
tains more important information. We use here
the idea of intrinsic information importance (Hong
and Nenkova, 2014; Zopf et al., 2016) which de-
scribes that information can be intrinsically im-
portant. For example, the information “Donald
Trump won the U.S. presidential election” is in-
trinsically important. It is likely that it should also
be contained in the generated summary if this in-
formation is contained in an input document.

After collecting few preferences, our model
uses the preferences to generate a ranking of all
sentences according to information importance.
Summaries which contain sentences similar the
upper part of the ranking are then considered to be
better than summaries which contain unimportant
sentences from the lower part of the ranking.

Pairwise preferences are an appealing form of
annotation, since they are much easier to generate
than producing complex gold standard summaries.
Not only collecting the annotations is easier, but
also using the collected annotations is much sim-
pler. The presented model does not have to solve
the difficult task of estimating the similarity be-
tween generated and gold standard summaries. In-
stead, the model uses the ranking to estimate the
summary quality.

Figure 1 provides an illustration of the tradi-
tional evaluation and our new model. On the left,
the input documents are illustrated which should
be summarized. In the upper part gold standard
summaries are generated by humans and used to
estimate the quality of an automatically generated
summary. In the lower part, we collect pairwise
preferences of sentences and use the preferences
for evaluation.

An evaluation on topics from two standard
datasets, looking at predicting the relative ratings
of automatically generated summaries, shows that
our new evaluation model is as good as or better
than existing methods, at a much lower annotation
cost.

2 Related Work

In this section, we will recapitulate previous work
in automated text summarization evaluation, fo-

Figure 1: Illustration of traditional evaluation models
based on reference summaries (top) and the new model
(bottom) which is based on pairwise preferences.

cusing on three important approaches, namely
model-free evaluation, ROUGE, and Pyramid.
The evaluation methods are ordered according to
their annotation requirements from none (model-
free evaluation) to high (Pyramid). In addition to
the most prominent methods described below, sev-
eral evaluation models were developed in the Au-
tomatically Evaluating Summaries Of Peers (AE-
SOP) shared tasks. The systems in this shared
task also considered reference summaries as ad-
ditional information to evaluate a reference sum-
mary and are therefore as expensive as ROUGE
in terms of required human annotation. Similarly,
Giannakopoulos and Karkaletsis (2013) use ma-
chine learning to learn a linear combination of
n-gram methods to evaluate summaries. Mackie
et al. (2014), Giannakopoulos (2013), and Co-
han and Goharian (2016) investigate evaluation
for microblog, multilingual, and scientific sum-
marization, respectively. Our evaluation, on con-
trary, uses newswire datasets since this is the most
prominent application domain for automatic sum-
marization. Furthermore, we focus on evaluating
the information content of summaries and do not
evaluate linguistic quality. This is, for example,
captured by Pitler et al. (2004).

2.1 Model-free Evaluation

Model-free evaluation methods Jensen-Shannon
divergence (Louis and Nenkova, 2013) do not re-
quire human input such as gold standard sum-
maries and can therefore be applied without ad-
ditional cost. The quality of model-free evaluation
methods is however limited, which is validated in
our experiments (see Section 5).

1688



2.2 ROUGE

ROUGE (Lin, 2004) was first used in the Doc-
ument Understanding Conference (DUC) (Over
et al., 2007) and is nowadays the method of
choice for automatic evaluation in text summariza-
tion. Many popular summarization systems were
evaluated with ROUGE (Erkan and Radev, 2004;
Mihalcea and Tarau, 2004; Gillick et al., 2009;
Lin and Bilmes, 2011). It is inspired from the
BLEU evaluation method (Papineni et al., 2002)
and is based on measuring lexical n-gram over-
lap of (stemmed) tokens between generated and
gold standard summaries. Researchers usually
report the n-gram recall of a summary to eval-
uate the quality of a summary. The quality of
ROUGE is often criticized in the research com-
munity. Sjöbergh (2007), for example, shows
nicely how the ROUGE recall scoring can be
fooled easily. A simple greedy language model
based on the source documents extracts frequent
bi-grams which are likely to occur in the refer-
ence summaries. The generated texts are merely
lists of bi-grams and not meaningful sentences
which cannot be considered to be summaries.
However, they achieve superhuman ROUGE recall
scores. In the TAC 2008 shared task (Dang and
Owczarzak, 2008), both ROUGE-2 and ROUGE-
SU4 score automatic systems higher than human
summaries, which would lead to the conclusion
that these systems are able to produce better sum-
maries than humans. Furthermore, studies show
that the correlation between ROUGE scores and
human judgments may not be significant in non-
newswire genres and other summary types (Liu
and Liu, 2008). ROUGE also has many param-
eters (Graham, 2015), which makes reproduction
and comparison of results problematic. Last but
not least, ROUGE computes text similarity only
based on simple string matching. Expressing the
same information with different words is not re-
warded by ROUGE. In addition to Graham (2015),
Owczarzak et al. (2012) and Rankel et al. (2013)
analyze ROUGE in more detail.

Agreement with human judgments (Owczarzak
et al., 2012) can be used instead of Pearson’s cor-
relation to validate an automatic evaluation model.
Measuring agreement allows to obtain a better un-
derstanding of the performance of an evaluation
model compared to the Pearson correlation. We
will also use agreement similarly to Owczarzak
et al. (2012) in our experiments.

2.3 Pyramid

The Pyramid method (Nenkova et al., 2007) (sim-
ilar to (Teufel and Van Halteren, 2004)) was used
in the Text Analysis Conference (TAC) (Dang
and Owczarzak, 2008) and goes beyond lexical
comparisons. It is based on Summarization Con-
tent Units (SCUs, later also called Summary Con-
tent Units). An SCU is a set of lexical expres-
sions with same meaning (e.g. {”2 people passed
away”, ”two persons died”}). After generating
the gold standard summaries, SCUs are extracted
from these summaries and are weighted by their
occurrence frequency in an additional annotation
step. Furthermore, every generated summary has
to be annotated individually with SCUs before the
Pyramid method can be applied. (Nenkova and
Passonneau, 2004) have already reported that a
large-scale application of the Pyramid method is
infeasible. (Over et al., 2007) report a huge effort
for the annotation process in the DUC challenges.
This additional annotation effort is unattractive for
researchers, who prefer automatic methods such as
ROUGE. This is validated by the few applications
of the Pyramid method until today. The need for
more human annotation also introduces an addi-
tional source for annotation mistakes. Inspecting
the annotations in the TAC 2008 dataset in detail
reveals that this is not only a theoretical issue but
has practical implications.1 PEAK (Yang et al.,
2016) is an attempt to automate the Pyramid evalu-
ation (similar to (Passonneau et al., 2013)). PEAK
also requires reference summaries and is therefore
as expensive as ROUGE.

2.4 Qualitative Feedback in Other NLP
Tasks

Simple and inexpensive qualitative human feed-
back has already been used in the field of machine
translation (Callison-Burch, 2009; Callison-Burch
et al., 2012; Guzmán et al., 2015). (Snow et al.,
2008) showed that in a wide variate of NLP tasks,
cheap non-expert labels can replace expensive ex-
pert annotations. In comparison to our work, we
are not asking non-experts to perform the same
task as expert annotators (namely writing refer-
ences summaries) but replace the complex task
with a simpler tasks (providing qualitative feed-
back in form of pairwise preferences).

1We found several issues such as not annotating parent
SCUs, missing SCUs in sentences, and different annotations
for equal sentences.

1689



3 Problem Definition

First, we define T to be the set of all possible texts
which can be considered to be summaries. For a
given set of source documents D, we define a bi-
nary relation >D ⊂ T × T with the intuition that
a >D b holds for two texts a,b ∈ T if and only
if a is considered to be the better summary of doc-
ument collection D than b. Whenever the context
is clear, we will omit D and write a > b for short.
The relation > induces a strict total order (given
that ties are not allowed) over T . A text which
ranks high according to >D is a good summary of
the document collection D.

How good summaries are is annotated in sum-
marization corpora only for a very small subset
of assessed texts T+ ⊂ T by human annotators.
We use the relation >∗ to express in which order
summaries are ranked by humans in summariza-
tion corpora for each document set D (also called
topic or cluster). The relation >∗ therefore mod-
els the human judgments. For not assessed texts
T− = T \ T+ the human judgment is unknown.

The quality of an evaluation method E can be
assessed by measuring the agreement with the hu-
man judgments. Evaluation models define (im-
plicitly) a ranking >E by assigning scores to sum-
maries or predicting the ranking directly. Calcu-
lating the agreement of the ranking >E with the
human ranking defined by >∗ provides a scores
which can be used to assess the performance of
evaluation models. Measuring the agreement be-
tween two relations (which are sets) can be easily
done by computing the intersection of both sets:2

Agreement(>∗, >E) =
|>∗ ∩ >E |
|>∗| (1)

This evaluation of evaluation models is similar
to the definition of Agreement and Contradiction
in Owczarzak et al. (2012): “Agreements occur
when the two evaluation metrics make the same
distinction between System A and System B (...).
Contradictions occur when both metrics find a (...)
difference between A and B, but in opposite direc-
tions.” A perfect evaluation model, which predicts
the preference for all pairs of summaries correctly,
will have an agreement of 1 whereas a random

2We require that an evaluation metric has to make a de-
cision for two summaries if the two summaries are different
according to the human judgment. Formally: a >∗ b →
a >E b or b >E a.

Donald Trump won
the election and be-
came president.

�
The U.S. Congress
certified the results
on January 6.

Figure 2: Example of a pairwise preference annotation
of two sentences. The first sentence is preferred over
the second sentence because the first sentence contains
more important information given that the information
is not already known.

evaluation model, which always predicts the pref-
erence randomly, will have an expected value of
0.5 according to this measure.

We prefer to use the agreement as defined in
Equation 1 for evaluation since it can be much bet-
ter interpreted (Owczarzak et al., 2012). Further-
more, Pearson’s correlation is known to be sen-
sitive to outliers, is only able to measure linear
correlations, and requires normally distributed, in-
terval scaled residuals (Anscombe, 1973). These
properties cannot be assumed to be given when
comparing human scores and automated evalua-
tion measures. We therefore prefer to use the
agreement according to human judgments as de-
fined in Equation 1 instead of calculating Pear-
son’s correlation.

4 Preference-based Evaluation of
Summaries

In this section, we present a novel framework
which does not infer a ranking of automatically
generated summaries based on gold standard sum-
maries but based on pairwise preferences. The
fundamental idea is not to rely on expensive gold
standard summaries as previous work does, but
to ask annotators for their preferences about sen-
tences. Annotates label pairs of sentences with a
preference label which indicates which sentence
contains more important information. Figure 2
illustrates such a pairwise preference annotation.
A human would likely prefer the first sentence
to be included in a summary instead of the sec-
ond sentence because the first sentence contains,
compared to the second sentence, relatively im-
portant information. Based on the preferences, our
model generates a ranking which reflects the im-
portance of information which is contained in the
sentences. Sentences with important information
will be ranked high whereas sentences contain-
ing only less important information will be ranked
low.

1690



4.1 Sampling Preference Annotations

The easiest strategy to select pairs of sentences for
which preferences should be annotated is to sam-
ple pairs of sentences randomly and to ask anno-
tators to provide a preference label for each sam-
pled pair (i.e. annotating whether a � b or b � a
for two randomly sampled a, b ∈ S∗). The sen-
tences are sampled from all source document of a
topic and are therefore independent from the au-
tomatically generated summaries. Our model will
therefore not only be able to evaluate already gen-
erated summaries but also summaries which will
be generated in the future.

All preferences are stored in a matrix M . An
entry of n at position Mij indicates that sentence
with index i was preferred n-times over sentence
with index j. To reduce the number of annota-
tions, we apply a smooth propagation of knowl-
edge. The idea is that we do not only obtain infor-
mation about the sampled sentence pair but also
about pairs which are similar to the sampled pair.

To estimate how much information can be trans-
fered from one to another pair, we calculate the
similarities between all sentences. As similarity
measure we use the average of the well-known
and simple Cosine similarity of TF-IDF vectors
and Jaccard similarities. The combination allows
to both rely the similarity computation on lexi-
cal similarity (Jaccard) and on important content
words (Cosine). We define the set of all sentences
in the source documents of a topic as S∗. Let
(a, b), a, b ∈ S∗ be one annotated sentence pair
and ȧ, ḃ the vector of similarities between a and
b and all sentences (i.e. ȧi denotes the similarity
between a and the i-th sentence in S∗).

We define the similarity of the pair (a1, b1) and
the pair (a2, b2) as sim(a1, a2) ∗ sim(b1, b2). If a1
is the exact same sentence as a2 and b1 is sim-
ilar with a degree of 0.7 to b2, we will transfer
0.7 of the information from (a1, b1) to the pair
(a2, b2). Transferring information means that we
generate additional preferences based on human
preferences. If a1 was preferred over b1 by a
human annotator, we will additionally generate a
weighted preferences of with a weight of 0.7 be-
tween a2 and b2. This can be modeled by the outer
product ȧ1 ⊗ ḃ1 of a1 and b1. For each annotated
pair (a, b), in which a was preferred by a human
over b, we update matrix M by M ←M + ȧ⊗ ḃ.

4.2 Sentence Score Prediction

The proposed usage of pairwise preferences be-
tween sentences is close to the idea of generat-
ing a ranking of sports teams by playing individual
matches. Instead of competitions between teams,
we observe competitions between sentences. The
outcome of a match between teams equals to the
annotation of a pair of sentences by a human an-
notator. Since different people can have differ-
ent opinions about the importance of information
(Gambhir and Gupta, 2016), we expect that one
sentence will not always be preferred by humans
similarly to the situation that the better sports team
does not always win against a weaker opponent.
This is expressed by the winning probability be-
tween teams (or sentences).

In sports, the term power ranking is used to
describe a ranking which does not only rank the
individual teams but also assigns a score to each
team, the skill. A well-known method to generate
a power ranking is the Bradly-Terry (BT) model
(Bradley and Terry, 1952). It estimates the utili-
ties v(a), v(b) of two teams (or two sentences) a
and b so that the winning probability of a against
b equals the score of a divided by the sum of the
scores of a and b:

p(a is prefered over b) =
v(a)

v(a) + v(b)
(2)

An algorithm to find a maximum-likelihood esti-
mator (MLE) has already been proposed in (Zer-
melo, 1929). To find the MLE, we iteratively
perform Equation 3 for all sentences si until the
difference between two iterations is sufficiently
small.3 wins(si) denotes the total number of wins
of si and duels(si, sj) the number of duels played
between sentences si and sj . This information was
collected in the previous step and is stored in ma-
trix M .

v(si)← wins(si)
∑

i 6=j

duels(si, sj)
v(si) + v(sj)

(3)

We normalize the resulting skill vector after each
iteration since every multiple of the solution is also
a correct solution and therefore restrict the model
to converge to one particular solution.

3We initialize all scores v(si) equally with v(si)← 1|s| .

1691



4.2.1 Summary Score Prediction
We estimate the score of summary s with function
u : T → R as follows:

u(s) =

|s|∑

i=1

wsi · v(argmax
s∈S∗

sim(s, si)) (4)

The utility of a summary is therefore defined
as the weighted sum of the sentence utilities v.
Since we do not want to restrict our model to
purely extractive summaries (which would mean
that all sentences contained in the automatic sum-
mary have to be exactly contained in the source
documents), we estimate the score of a sentence
si in the summary by searching for the most sim-
ilar sentence s in the source documents with sim-
ilarity function sim : S × S → [0, 1]. As weight
of si, we use

|si|
|s| where |.| denotes the length of

the summary and sentence measured in number of
characters. The intuition of the weight is that a
sentence contributes more to the overall score of
a summary if it is longer. The score of a sum-
mary will decrease if a large fraction of the sum-
mary is occupied with a poor sentence. By using a
similarity function instead of a hard matching, our
method is able to generalize to unseen sentences.

The definition of u in Equation 4 does not con-
sider redundancy. Including a sentence s twice
would result in adding the score of s twice to the
summary score. This behavior of the evaluation
measure is not desirable. We therefore include a
redundancy penalization which does not reward
redundant information. For a summary s, we re-
duce the score of sentence s by

vred(s) = v(s)
1

|s|
∑

g∈s

num(g, s)
num(g, s)

(5)

where num(g, s) and num(g, s) denote the number
of occurrences of the bi-gram g in s and s, respec-
tively. |s| denotes the number of bi-grams in s.

4.3 Reusing Available Annotation
Information

For already existing summarization corpora, refer-
ence summaries and/or Pyramid annotations have
already been created. Instead of generating new
preference annotations by asking human annota-
tors, we can also reuse the available data to simu-
late annotations. To this end, we define functions
wr andwp which estimate the score of a single sen-
tence based on reference summaries (r) and Pyra-
mid scores (p), respectively. We will use the scores

generated by wr and wp to simulate annotations of
sentence pairs. For two sentence a, b we can sim-
ulate a human preference annotation of a � b if
wr(a) > wr(b) and a win of b over a otherwise
(equivalent for wp).

For a set of gold standard summaries R, we de-
fine wr : S → R simply to be the maximum sim-
ilarity to the sentences in the gold standard sum-
maries:

wr(s) = max
t∈r,r∈R

(sim(s, t)) (6)

If a very similar sentence appears in a gold stan-
dard summary, s will receive a high score. If no
similar sentences are in the gold standard sum-
maries the sentence will receive a low score.

Given that Pyramid annotations are available (as
in the TAC 2009 corpus, for example), we can
define the score of a sentence as the sum of the
weights of the matched unique SCUs (similar to
the Pyramid method). Annotations are, unfortu-
nately, only available for all sentences in the doc-
uments in T+ and not for sentences in S∗. We
therefore search for sentence s in S∗ for the most
similar sentence ŝ in the documents in T+

ŝ = argmax
t∈t,t∈T+

sim(s, t) (7)

and set the score of s to

wp(s) =
∑

scu∈ŝ
weight(scu) (8)

where scu ∈ t are all unique SCUs contained in t
and weight(scu) denotes the weight of an SCU as
defined in (Nenkova and Passonneau, 2004). As
described above, we will observe wins and losses
between pairs based on the estimated scores.

5 Experiments

We provide in this section a detailed analysis of
our proposed evaluation method. For the experi-
ment, we use eight topics from two popular multi-
document summarization datasets, the DUC 2004
(DUC04) and TAC 2009 (TAC09) corpora, which
are freely available upon request.4 Each topic
in the datasets contains ten source documents.
Each topic contains automatically generated sum-
maries which were generated in the DUC 2004
and TAC 2009 shared tasks. All automatically

4http://duc.nist.gov and https://tac.
nist.gov

1692



JS R1 R2 R3 R4 SU4 man
DUC04 0.480 0.651 0.639 0.649 0.606 0.558 0.673
TAC09 0.565 0.638 0.668 0.660 0.674 0.663 0.688

Table 1: Agreement of preference based evaluation as
defined in Equation 1 of different versions of Jensen-
Shannon, ROUGE and our novel model based on man-
ually labeled pairwise preferences.

generated summaries were evaluated by humans.
Each summary was labeled with a score from 1 to
5 (DUC04) or 1 to 10 (TAC09) indicating the in-
formation content of the summary. Evaluation of
grammatically, writing style, etc. is not included
in the scores. An evaluation model predicts the
preference for two selected summaries correctly if
the model predicts the same preference according
to the annotated reference scores and incorrectly
otherwise. We do not consider ties in the exper-
iments. In the following, we report the agree-
ment as described in Equation 1 for various ex-
periments. We use the abbreviations JS (Jensen-
Shannon), R1 - R4 (ROUGE-1 - ROUGE-4), SU4
(ROUGE-SU4), and PY (Pyramid (Nenkova and
Passonneau, 2004)) to denote the reference sys-
tems.

5.1 Manual Annotations

In the first experiment, we investigate whether hu-
mans are able to provide useful feedback in the
form of pairwise preferences.

To evaluate our model, we annotated 200 ran-
domly sampled sentence pairs for the first four
topics in the DUC04 and the first four topics in
the TAC09 corpus with pairwise preferences. The
preferences were used (including the previously
described smoothed sampling) as input for the
proposed model. The results are shown in Ta-
ble 1. Column man denotes the performance of
our now model and column Time (min) indicates
how much time was required to generate the an-
notations. This information is in particular impor-
tant for this paper since our main aim is to develop
a cheap evaluation framework. In average, our
model achieves an agreement of 0.673 in DUC04
and 0.688 in TAC09. This means, that 67.3/68.8
percent of all pairs of manually rated summaries
were predicted correctly. This outperforms the
best versions of ROUGE in the respective corpora
(SU4 with 65.1 percent in DUC04 and R2 with
66.0 percent in TAC09).

With an average annotation time per topic of 53

R1 R2 R4 PY man
+ref

man
+py

man
+ref
+py

DUC04 0.651 0.639 0.606 n/a 0.722 n/a n/a
TAC09 0.638 0.668 0.674 0.715 0.682 0.707 0.717

Table 2: Agreement of different versions of ROUGE
and Pyramid (PY) and our novel models based on hu-
man and automatically generated pairwise preferences
in addition to manually labeled preferences.

and 54 minutes our model needs much less anno-
tation effort than ROUGE.

5.2 Weak Supervision with Additionally
Simulated Annotations

In the next experiment, we are interested whether
we can simulate additional annotations based on
already available reference summaries and Pyra-
mid annotations. The automatically annotated
pairs can be considered to be an additional weak
supervision for the model. We simulated 200 ad-
ditional annotations based on reference summaries
and/or Pyramid annotations in addition to the 200
manual annotations per topic. To this end, we ran-
domly sampled 200 additional pairs and annotated
the pairs with a preference label based on refer-
ence summaries and/or Pyramid annotations. Ta-
ble 2, column man+ref contains the results for 200
manual + 200 simulated reference summary-based
annotations; column man+py contains the results
for 200 manual + 200 simulated Pyramid score-
based annotations; and column man+ref+py con-
tains results for 200 manual + 200 reference
summary-based + 200 Pyramid score-based anno-
tations. The results show that we can improve
the agreement with additional simulated annota-
tions based on reference summaries in DUC04 by
5 percent points. Additional annotations increased
Agreement in TAC09 by 3 percent points. This
leads to the conclusion that we can use already
available reference summaries in order to substi-
tute more human preference annotations, which
makes the trade-off between performance and an-
notations effort of our model even better.

5.3 Solely Using Simulated Annotations

Now, we investigate if simulated preferences are
already sufficient to produce reasonable good re-
sults. Table 3, columns ref and py contain the re-
sults of an experiment where we sampled 1,000
simulated pairwise annotations. Without any ad-
ditional annotation effort, the new model is able to

1693



R1 R2 R4 PY ref py
DUC04 0.651 0.639 0.606 n/a 0.716 n/a
TAC09 0.638 0.668 0.674 0.715 0.644 0.709

Table 3: Agreement with human judgments for refer-
ence systems and our model fed with only automati-
cally generated preferences labels.

perform much better than ROUGE at DUC 2004.
In TAC 2009, our model achieves similar perfor-
mance as the best performing evaluation based on
Pyramid annotations. We conclude that automat-
ically generating pairwise preferences based on
already available reference summaries is already
sufficient to outperform ROUGE. Pairwise pref-
erences generated based on the more expensive
Pyramid annotations do not improve the perfor-
mance.

5.4 Convergence

In the next experiment, we investigate how agree-
ment changes with an increasing amount of anno-
tations. Figure 3 shows how agreement improves
with more annotations. We sampled n annota-
tions (horizontal axis) randomly from the human
annotations and averaged the resulting agreement
scores (vertical axis) of 100 runs to obtain reli-
able results. We observe a continuous improve-
ment of agreement in all four topics in the TAC
2009 dataset which indicates that sampling more
annotations can further improve the performance
of our system.

0 50 100 150 200

0.5

0.6

0.7

observations

ag
re

em
en

t

D0904A
D0902A
D0903A
D0901A

Figure 3: Agreement trajectories averaged over 100
runs per topic in the TAC 2009 corpus.

5.5 Ranking Evaluation

We now investigate the ranking generated by our
model directly. Since individual sentences are an-

notated in the TAC 2009 corpus with SCUs, we
can generate a ranking of the sentences and di-
rectly compare this ranking with the ranking gen-
erated by our model. Table 4 shows the percent-
age of correctly ordered sentence pairs (similar
to Kendall’s τ ) for our model without and with
smoothed sampling.

non-smoothed smoothed
man pyr ref man pyr ref

0.683 0.987 0.661 0.727 0.941 0.698

Table 4: Percentage of correctly ordered sentence pairs
in the TAC 2009 corpus for both a non-smoothed and a
smoothed sampling.

Smoothed sampling improves the raking of the
model if we use 200 manual or 200 reference
summary-based preferences in the TAC 2009 cor-
pus. Given that we can sample pairs based on
Pyramid scores, the model is able to reconstruct
the ranking almost perfectly if we do not use
smoothed sampling. With smoothed sampling,
the performance decreases in this case. The re-
sult confirms the previously observed performance
at summary scoring where preferences based on
Pyramid annotations performed best followed by
manually generated preference annotations.

6 Conclusions & Outook

Evaluating automatically generated summaries is
a challenging task and creating annotations which
are required by applications such as ROUGE or
Pyramid is laborious and expensive. We presented
an alternative model which does not rely on refer-
ence summaries or Pyramid annotations but only
on simple pairwise preferences of sentences.

We showed in our experiments that the pro-
posed model is able to perform better than the cur-
rent state-of-the-art ROUGE method with less ex-
pensive annotations and that humans are able to
provide useful feedback in the form of pairwise
preferences. In combination with already avail-
able references summaries and Pyramid annota-
tions, we were able to simulate more annotations,
which improved performance further.

We conclude that gold standard summaries are
not the only usable human feedback which can be
used for summary evaluation. Investigating other
kinds of feedback such as pairwise preferences
might be a promising future research direction.

In future work, we would like to investigate
whether we can use crowd-sourcing platforms to

1694



collect pairwise preferences on a large scale. Fur-
thermore, we want to investigate whether we can
reduce the number of required preferences with
smarter sampling methods. Active learning meth-
ods can be used to replace the simple random sam-
pling strategy. Additionally, the investigation of
more sophisticated similarity functions can poten-
tially improve the model’s performance.

Acknowledgments

This work has been supported by the German Re-
search Foundation (DFG) as part of the Research
Training Group Adaptive Preparation of Informa-
tion from Heterogeneous Sources (AIPHES) un-
der grant No. GRK 1994/1.

References
F. J. Anscombe. 1973. Graphs in Statistical Analysis.

The American Statistician 27(1):17–21.

Ralph Allan Bradley and Milton E. Terry. 1952. Rank
Analysis of Incomplete Block Designs. Biometrika
39(3):324–345.

Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using Amazon’s Me-
chanical Turk. Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing 1(August):286–295. https://doi.
org/10.3115/1699510.1699548.

Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. Proceedings of the Seventh Work-
shop on Statistical Machine Translation pages 10–
51. https://doi.org/10.3115/1626431.
1626433.

Arman Cohan and Nazli Goharian. 2016. Revisiting
Summarization Evaluation for Scientific Articles. In
Proceedings of the Tenth International Conference
on Language Resources and Evaluation. pages 806–
813.

Hoa Trang Dang. 2005. Overview of DUC 2005.
In Proceedings of the Document Understanding
Conference. https://doi.org/10.3115/
1654679.1654689.

Hoa Trang Dang and Karolina Owczarzak. 2008.
Overview of the TAC 2008 Update Summarization
Task. In Proceedings of the First Text Analysis Con-
ference.

Günes Erkan and Dragomir R Radev. 2004. LexRank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence
Research 22:457–479.

Johannes Fürnkranz and Eyke Hüllermeier, editors.
2010. Preference Learning. Springer.

Mahak Gambhir and Vishal Gupta. 2016. Recent auto-
matic text summarization techniques: a survey. Ar-
tificial Intelligence Review 47(1):1–66. https://
doi.org/10.1007/s10462-016-9475-9.

G. Giannakopoulos and V. Karkaletsis. 2013. Sum-
mary Evaluation: Together We Stand NPowER-
ed. 14th International Conference on Computa-
tional Linguistics and Intelligent Text Processing
pages 436–450.

George Giannakopoulos. 2013. Multi-document mul-
tilingual summarization and evaluation tracks in
ACL 2013 MultiLing Workshop. MultiLing 2013
page 20.

Dan Gillick, Benoit Favre, Dilek Hakkani-Tür, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The
ICSI/UTD Summarization System at TAC 2009. In
Proceedings of the Second Text Analysis Conference.

Yvette Graham. 2015. Re-evaluating Automatic Sum-
marization with BLEU and 192 Shades of ROUGE.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing. pages
128–137.

Francisco Guzmán, Shafiq Joty, Llu\’is Màrquez, and
Preslav Nakov. 2015. Pairwise Neural Machine
Translation Evaluation. Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers) pages 805–814. http://
www.aclweb.org/anthology/P15-1078.

Hans Van Halteren and Simone Teufel. 2003. Exam-
ining the consensus between human summaries: ini-
tial experiments with factoid analysis. Proceedings
of the HLT-NAACL2003 Workshop on Text Summa-
rization pages 57–64. https://doi.org/10.
3115/1119467.1119475.

Kai Hong and Ani Nenkova. 2014. Improving the
Estimation of Word Importance for News Multi-
Document Summarization. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics. pages 712–
721.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out. pages 25–26.

Hui Lin and Jeff Bilmes. 2011. A Class of Submodu-
lar Functions for Document Summarization. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics. pages 510–520.

Feifan Liu and Yang Liu. 2008. Correlation between
ROUGE and Human Evaluation of Extractive Meet-
ing Summaries. In Proceedings of the 46th An-
nual Meeting of the Association for Computational

1695



Linguistics: Human Language Technologies. pages
201–204.

Annie Louis and Ani Nenkova. 2013. Automati-
cally Assessing Machine Summary Content With-
out a Gold Standard. Computational Linguistics
39(2):267–300.

Stuart Mackie, Richard Mccreadie, Craig Macdon-
ald, and Iadh Ounis. 2014. On Choosing an
Effective Automatic Evaluation Metric for Mi-
croblog Summarisation. Proceedings of the
5th Information Interaction in Context Sympo-
sium pages 115–124. https://doi.org/10.
1145/2637002.2637017.

Inderjeet Mani. 2001. Automatic Summarization. John
Benjamins Publishing Co.

Rada Mihalcea and Paul Tarau. 2004. Tex-
tRank: Bringing order into texts. In Pro-
ceedings of the 2004 Conference on Empirical
Methods in Natural Language Processing. vol-
ume 85, pages 404–411. https://doi.org/
10.3115/1219044.1219064.

Ani Nenkova and Kathleen McKeown. 2011. Au-
tomatic Summarization. Foundations and Trends
in Information Retrieval 5(3):103–233. https:
//doi.org/10.1561/1500000015.

Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyra-
mid method. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics. pages 145–152.

Ani Nenkova, Rebecca Passonneau, and Kathleen
McKeown. 2007. The Pyramid Method. In ACM
Transactions on Speech and Language Processing.
volume 4, pages 1–23. https://doi.org/10.
1145/1233912.1233913.

Paul Over, Hoa Dang, and Donna Harman. 2007.
DUC in context. Information Processing and Man-
agement 43(6):1506–1520. https://doi.org/
10.1016/j.ipm.2007.01.019.

Karolina Owczarzak, John M Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment of
the accuracy of automatic evaluation in summariza-
tion. In Proceedings of Workshop on Evaluation
Metrics and System Comparison for Automatic Sum-
marization. pages 1–9.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wj Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics. July, pages 311–318. https:
//doi.org/10.3115/1073083.1073135.

Rebecca J. Passonneau, Emily Chen, Weiwei Guo, and
Dolores Perin. 2013. Automated Pyramid Scoring
of Summaries using Distributional Semantics. In

Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics. pages 143–
147.

Emily Pitler, Annie Louis, and Ani Nenkova. 2004.
Automatic Evaluation of Linguistic Quality in
Multi-Document Summarization. Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics pages 544–554. http://
www.aclweb.org/anthology/P10-1056.

Peter A Rankel, John M Conroy, Hoa Trang Dang, and
Ani Nenkova. 2013. A Decade of Automatic Con-
tent Evaluation of News Summaries: Reassessing
the State of the Art. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics. pages 131–136.

Jonas Sjöbergh. 2007. Older versions of the ROUGEe-
val summarization evaluation system were easier
to fool. Information Processing and Manage-
ment 43(6):1500–1505. https://doi.org/
10.1016/j.ipm.2007.01.014.

Rion Snow, Brendan O Connor, Daniel Jurafsky, An-
drew Y Ng, Dolores Labs, and Capp St. 2008.
Cheap and fast - but is it good? Evaluation non-
expert annotiations for natural language tasks. Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP) pages
254–263. https://doi.org/10.1.1.142.
8286.

Simone Teufel and Hans Van Halteren. 2004. Evalu-
ating Information Content by Factoid Analysis: Hu-
man annotation and stability. In Proceedings of the
2004 Conference on Empirical Methods on Natural
Language Processing. pages 419–426.

Louis Leon Thurstone. 1927. A law of comparative
judgement. Psychological Review 34:278–286.

Qian Yang, Rebecca J. Passonneau, and Gerard
de Melo. 2016. PEAK: Pyramid Evaluation via Au-
tomated Knowledge Extraction. In Proceedings of
the 30th Conference on Artificial Intelligence. pages
2673–2679.

Ernst Zermelo. 1929. Die Berechnung der Turnier-
Ergebnisse als ein Maximumproblem der
Wahrscheinlichkeitsrechnung. Mathematische
Zeitschrift 29:436–460.

Markus Zopf, Eneldo Loza Mencı́a, and Johannes
Fürnkranz. 2016. Beyond Centrality and Structural
Features: Learning Information Importance for Text
Summarization. In Proceedings of the 20th Confer-
ence on Computational Natural Language Learning.
Association for Computational Linguistics, pages
84–94.

1696


