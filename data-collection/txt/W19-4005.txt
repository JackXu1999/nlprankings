



















































CCGweb: a New Annotation Tool and a First Quadrilingual CCG Treebank


Proceedings of the 13th Linguistic Annotation Workshop, pages 37–42
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

37

CCGweb: a New Annotation Tool
and a First Quadrilingual CCG Treebank

Kilian Evang
University of Düsseldorf

Germany
evang@hhu.de

Lasha Abzianidze
University of Groningen

The Netherlands
l.abzianidze@rug.nl

Johan Bos
University of Groningen

The Netherlands
johan.bos@rug.nl

Abstract

We present the first open-source graphical an-
notation tool for combinatory categorial gram-
mar (CCG), and the first set of detailed guide-
lines for syntactic annotation with CCG, for
four languages: English, German, Italian,
and Dutch. We also release a parallel pi-
lot CCG treebank based on these guidelines,
with 4x100 adjudicated sentences, 10K single-
annotator fully corrected sentences, and 82K
single-annotator partially corrected sentences.

1 Introduction

Combinatory Categorial Grammar (CCG; Steed-
man, 2000) is a grammar formalism distinguished
by its transparent syntax-semantics interface and
its elegant handling of coordination. It is a popu-
lar tool in semantic parsing, and treebank creation
efforts have been made for Turkish (Çakıcı, 2005),
German (Hockenmaier, 2006), English (Hocken-
maier and Steedman, 2007), Italian (Bos et al.,
2009), Chinese (Tse and Curran, 2010), Arabic
(Boxwell and Brew, 2010), Japanese (Uematsu
et al., 2013), and Hindi (Ambati et al., 2018).
However, all of these treebanks were not directly
annotated according to the CCG formalism, but
automatically converted from phrase structure or
dependency treebanks, which is an error-prone
process. Direct annotation in CCG has so far
mostly been limited to small datasets for seed-
ing or testing semantic parsers (e.g., Artzi et al.,
2015), and no graphical annotation interface is
available to support such efforts, making the an-
notation process difficult to scale. The only excep-
tions we are aware of are the Groningen Meaning
Bank (Bos et al., 2017) and the Parallel Meaning
Bank (Abzianidze et al., 2017), two annotation ef-
forts which use a graphical user interface for anno-
tating sentences with CCG derivations and other
annotation layers, and which have produced CCG

treebanks for English, German, Italian, and Dutch.
However, these efforts are focused on semantics
and have not released explicit guidelines for syn-
tactic annotation. Their annotation tool is limited
in that annotators only have control over lexical
categories, not larger constituents. Even though
CCG is a lexicalized formalism, where most de-
cisions can be made on the lexical level, there is
no full control over attachment phenomena in the
lexicon. Moreover, these annotation tools are not
open-source and cannot easily be deployed to sup-
port other annotation efforts.

In this paper, we present an open-source,
lightweight, easy-to-use graphical annotation tool
that employs a statistical parser to create initial
CCG derivations for sentences, and allows anno-
tators to correct these annotations via lexical cate-
gory constraints and span constraints. Together,
these constraints make it possible to effect (al-
most) all annotation decisions consistent with the
principles of CCG. We also present a pilot study
for multilingual CCG annotation, in which a par-
allel corpus of 4x100 sentences (in English, Ger-
man, Italian, and Dutch) was annotated by two an-
notators per sentence, a detailed annotation man-
ual was created, and adjudication was performed
to create a final version. We publicly release
the manual, the annotation tool, and the adjudi-
cated data. Our release also includes an additional
> 10K derivations, each manually corrected by
a single annotator, and an additional > 82K sen-
tences, each partially corrected by a single anno-
tator.

2 An Annotation Tool for CCG

Our annotation tool CCGweb1 is Web-based, im-
plemented in Python, PHP, and JavaScript, and
should be easy to deploy on any recent Linux dis-

1https://github.com/texttheater/ccgweb

https://github.com/texttheater/ccgweb


38

Figure 1: Correcting a lexical category.

Figure 2: Correcting attachments by selecting a span that need to form a constituent.

tribution. It has two main views: the home page
shows the list of sentences an annotator is assigned
to annotate. Those already done are marked as
“marked correct”. Clicking on a sentence takes
the annotator to the sentence view. Annotators can
also enter arbitrary sentences to annotate, e.g., for
experimenting or for producing illustrations.

Dynamic Annotation Annotation follows an
approach called dynamic annotation (Oepen et al.,
2002) or human-aided machine annotation (Bos
et al., 2017), in which sentences are automatically
analyzed, annotators impose constraints to rule
out undesired analyses, sentences are then reana-
lyzed subject to the constraints, and the process is
repeated until only the desired analysis remains.
The current system is backed by the EasyCCG
parser (Lewis and Steedman, 2014), slightly mod-
ified to allow for incorporating constraints, and
other CCG parsers could be plugged in with simi-
lar modifications.

What You See Is What You Get Derivations
are rendered in the same graphical format that is
used in the literature, representing nodes as hori-
zontal lines placed underneath their children. An-
notators directly interact with this graphical repre-
sentation when annotating, following the WYSI-
WYG (what you see is what you get) principle.

Lexical Category Constraints As an example
of editing, consider Figure 1. Suppose that the

parser has analyzed there as an adjunct with cat-
egory (S \NP)\(S \NP), but we wish to analyze
it as an argument to the verb go with category PP.
As a result, the category of the verb also has to
change, viz. from S[b]\NP to (S[b]\NP)/PP.
To do this, the annotator clicks on the category and
changes it, as shown in the figure. When they hit
enter or click somewhere else, the sentence is au-
tomatically parsed again in the background, this
time with the lexical category constraint that go
has category (S[b]\NP)/PP. In many cases, the
parser will directly find the desired parse, with
there being a PP, and the annotator only has to
check it, not make another edit.

Span Constraints Although constraining lexi-
cal categories is often enough to determine the
entire CCG derivation (cf. Bangalore and Joshi,
1999; Lewis and Steedman, 2014), this is not al-
ways the case. For example, consider the sentence
I want to be a millionaire like my dad. Assuming
that like my dad is a verb phrase modifier (category
(S \NP)\(S \NP)), it could attach to either to be
or want, giving very different meanings (cf. Zim-
mer, 2013). We therefore implemented one other
type of edit operation/constraint: span constraints.
By simply clicking and dragging across a span of
tokens as shown in Figure 2, annotators can con-
strain this span to be a constituent in the resulting
parse.



39

Figure 3: The judge user sees all annotators’ versions
and a diff view where categories with disagreements
are struck through and spans with disagreements are
dotted.

Additional Features Our tool offers annotators
some additional convenient features. When unsure
about some annotation decision, they can click the
“report issue” button to open a discussion thread in
an external forum, such as a GitHub issue tracker.
To erase all constraints and restart annotation from
the parser’s original analysis, an annotator can
click the “reset” button. And the buttons “HTML”
and “LaTeX” provide code that can be copied and
pasted to use the current derivation as an illustra-
tion on a web page or in a paper.

Adjudication Support Once two or more anno-
tators have annotated a sentence, disagreements
need to be discovered, and a final, authoritative
version has to be created. Our tool supports this
adjudication process through the special user ac-
count judge. This user can see the derivations of
other annotators in a tabbed interface as shown in
Figure 3. In order to enable the judge to easily
spot disagreements, categories that annotators dis-
agree on are struck through, and constituents that
annotators disagree on are dashed.

3 A Quadrilingual Pilot CCG Treebank

To test the viability of creating multilingual CCG
treebanks by direct annotation, we conducted an
annotation experiment on 110 short sentences
from the Tatoeba corpus (Tatoeba, 2019), each in
four translations (English, German, Italian, and
Dutch). The main annotation guideline was to
copy the annotation style of CCGrebank (Honni-

bal et al., 2010), a CCG treebank adapted from
CCGbank (Hockenmaier and Steedman, 2007),
which is in turn based on the Penn Treebank (Mar-
cus et al., 1993). Since CCGrebank only covers
English and lacks some constructions observed in
our corpus, an annotation manual with more spe-
cific instructions was needed. We initially an-
notated ten sentences in four languages and dis-
cussed disagreements. The results were recorded
in an initial annotation manual, and the initial an-
notations were discarded. Each of the remain-
ing 4x100 sentences was then annotated indepen-
dently by at least two of the authors.

Table 1 (upper part) shows the number of non-
overlapping category and span constraints that
each annotator created on average per sentence be-
fore marking the sentence as correct. Annotated
sentences were manually classified by the first au-
thor into four classes: (0) sentences without any
disagreements, (1) sentences with only trivial vio-
lations of the annotation guidelines (e.g., concern-
ing attachment of punctuation or underspecifying
modifier features), (2) sentences with only appar-
ent oversights, such as giving a determiner a pro-
noun category, (3) sentences with more intricate
disagreements which required additional guide-
lines to resolve. Table 1 (upper part) shows the
distribution of disagreement classes, and Table 2
shows examples of class (3). The first author ad-
judicated all disagreements and updated the anno-
tation manual accordingly. We release the manual
and the full adjudicated dataset.2

To make the resource more useful (e.g., for
training parsers), we also include in the release
the syntactic CCG derivations created so far in the
Parallel Meaning Bank (Abzianidze et al., 2017).
These do not follow the annotation guidelines in
detail due to their focus on semantics, nor have
they been adjudicated, but instead corrected by a
single annotator. However, they are much greater
in number. For an even greater number, we also re-
lease partially corrected derivations, meaning that
the annotator made at least one change to the auto-
matically created derivation. Table 1 (lower part)
shows statistics of this additional data.

4 Conclusions and Future Work

We have presented the first open-source graphical
annotation tool for combinatory categorial gram-
mar. Its features include dynamic annotation via

2https://ccgweb.phil.hhu.de/

https://ccgweb.phil.hhu.de/


40

English German Italian Dutch

adjudicated sentences 100 100 100 100
∅ length 6.8 8.1 6.6 7.5
∅ category constraints per annotator 1.8 2.7 2.6 2.5
∅ span constraints per annotator 1.1 1.1 1.2 1.1
by disagreement (0) none 10 32 27 34

(1) trivial 45 17 16 12
(2) oversight 1 7 4 8
(3) intricate 44 44 53 46

single annotator, fully corrected 7 182 1 703 941 868
∅ length 6.4 5.7 5.4 5.9

single annotator, partially corrected 74 769 4 331 2 652 1 130
∅ length 8.6 7.4 6.9 7.4

Table 1: Corpus statistics and disagreements

Language Disagreement

English Argument or adjunct?
Take((S[b]\NP)/PP)/NP a taxiPP /NP to the hotel .
Take(S[b]\NP)/NP a taxi(S \NP)\(S \NP) to the hotel .

Clausal argument or adjunct?
Can I have somethingNP /(S[to]\NP) to(S[to]\NP)/(S[b]\NP) eatS[b]\NP ?
Can I have somethingN to(S[to]\NP)/(S[b]\NP) eat(S[b]\NP)/NP ?

Modification of copula or adjective?
My mother is always(S[adj]\NP)/(S[adj]\NP) busy .
My mother is always(S \NP)\(S \NP) busy .

German Treatment of quoted speech
Sag(S[b]\NP)/NP nur jaN oder(N \N)/N neinN .
Sag(S[b]\NP)/ S[intj] nur jaS[intj] oder(S[intj]\ S[intj])/ S[intj] neinS[intj] .

Analysis of wh-questions
WerS[wq]/(S[dcl]\NP) hat(S[dcl]\NP)/(S[pt]\NP) diesen Brief geschrieben ?
WerS[wq]/(S[q]\NP) hat(S[q]\NP)/(S[pt]\NP) diesen Brief geschrieben ?

Scope of negation
Rufen Sie mich nicht(S / S)/(S / S) mehr an !
Rufen Sie mich nichtS \ S mehr an !

Italian Analysis of wh-questions
Ci poteteS[q]/(S[b]\NP) aiutare ?S[q]\ S[q]
Ci poteteS[dcl]/(S[b]\NP) aiutare ?S[q]\ S[dcl]

Category ambiguity in parts of multiword expressions
Sono tre anni che Tom è andato((S[pt]\NP)/PP)/NP viaN da Boston .
Sono tre anni che Tom è andato((S[pt]\NP)/PP)/PR viaPR da Boston .

di: preposition or complementizer?
Gli ho chiesto((S[pt]\NP)\NP)/PP diPP /(S[b]\NP) farlo .
Gli ho chiesto((S[pt]\NP)\NP)/(S[to]\NP) di(S[to]\NP)/(S[b]\NP) farlo .

Dutch Argument or adjunct?
Een eekhoorntje verstopte((S[dcl]\NP)/PP)/NP zich tussenPP /NP de takken .
Een eekhoorntje verstopte(S[dcl]\NP)/NP zich tussen((S \NP)\(S \NP))/NP de takken .

Participles in attributive use
Windows is het meest(N /N)/(N /N) gebruikteN /N besturingssysteem in de wereld .
Windows is het meest(N /N)/(S[pss]\NP) gebruikteS[pss]\NP besturingssysteem in de wereld .

met: nominal or verbal argument?
Hij is gestopt metPP /NP rokenN .
Hij is gestopt metPP / S[b]\NP rokenS[b]\NP .

Table 2: Examples of intricate disagreements



41

lexical label constraints and span constraints, ad-
judication support, and various conveniences.

We have used this tool to create the first pub-
lished CCG resource that comes with an explicit
annotation manual for syntax and has been created
by direct annotation, rather than conversion from
a non-CCG treebank. It is multilingual, currently
including English, German, Italian, and Dutch,
and aims for cross-lingually consistent annotation
guidelines.

For future work, we envision more extensive
direct annotation of multilingual data with CCG
derivations, and putting them to use for evaluat-
ing unsupervised and distantly supervised CCG
parsers. We would also like to investigate the use
of our tool as an interactive aid in teaching CCG.

Acknowledgments

The work of the first author was funded by the
Consolidator Grant “TreeGraSP” of the European
Research Council (ERC). The work of the sec-
ond and third author was funded by the NWO-
VICI grant “Lost in Translation – Found in Mean-
ing” (288-89-003)”. We would like to thank the
three anonymous reviewers for their valuable com-
ments.

References
Lasha Abzianidze, Johannes Bjerva, Kilian Evang,

Hessel Haagsma, Rik van Noord, Pierre Ludmann,
Duc-Duy Nguyen, and Johan Bos. 2017. The Paral-
lel Meaning Bank: Towards a multilingual corpus of
translations annotated with compositional meaning
representations. In Proceedings of the 15th Confer-
ence of the European Chapter of the Association for
Computational Linguistics: Volume 2, Short Papers,
pages 242–247. Association for Computational Lin-
guistics.

Bharat Ram Ambati, Tejaswini Deoskar, and Mark
Steedman. 2018. Hindi CCGbank: A CCG treebank
from the Hindi dependency treebank. Language Re-
sources and Evaluation, 52(1):67–100.

Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015.
Broad-coverage CCG semantic parsing with AMR.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1699–1710. Association for Computational Linguis-
tics.

Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, Volume 25, Number 2, June
1999.

Johan Bos, Valerio Basile, Kilian Evang, Noortje J.
Venhuizen, and Johannes Bjerva. 2017. The Gronin-
gen Meaning Bank. In Nancy Ide and James Puste-
jovsky, editors, Handbook of Linguistic Annotation,
pages 463–496. Springer Netherlands, Dordrecht.

Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a dependency treebank to a cat-
egorial grammar treebank for Italian. In Eight inter-
national workshop on treebanks and linguistic theo-
ries (TLT8), pages 27–38. Educatt.

Stephen A. Boxwell and Chris Brew. 2010. A pilot
Arabic CCGbank. In Proceedings of the Seventh
conference on International Language Resources
and Evaluation (LREC’10). European Languages
Resources Association (ELRA).

Julia Hockenmaier. 2006. Creating a CCGbank and a
wide-coverage CCG lexicon for German. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 505–512. Association for Computational
Linguistics.

Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and depen-
dency structures extracted from the Penn Treebank.
Computational Linguistics, Volume 33, Number 3,
September 2007.

Matthew Honnibal, James R. Curran, and Johan Bos.
2010. Rebanking CCGbank for improved NP inter-
pretation. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 207–215. Association for Computational
Linguistics.

Mike Lewis and Mark Steedman. 2014. A* CCG pars-
ing with a supertag-factored model. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
990–1000. Association for Computational Linguis-
tics.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, Volume 19, Number 2, June 1993,
Special Issue on Using Large Corpora: II.

Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and Thorsten
Brants. 2002. The LinGO Redwoods Treebank:
Motivation and preliminary applications. In COL-
ING 2002: The 17th International Conference on
Computational Linguistics: Project Notes.

Mark Steedman. 2000. The syntactic process. MIT
press Cambridge, MA.

Tatoeba. 2019. Tatoeba: Collection of sentences and
translations. https://tatoeba.org/. Ac-
cessed: 2019-04-08.

http://aclweb.org/anthology/E17-2039
http://aclweb.org/anthology/E17-2039
http://aclweb.org/anthology/E17-2039
http://aclweb.org/anthology/E17-2039
https://doi.org/10.1007/s10579-017-9379-6
https://doi.org/10.1007/s10579-017-9379-6
https://doi.org/10.18653/v1/D15-1198
http://aclweb.org/anthology/J99-2004
http://aclweb.org/anthology/J99-2004
https://doi.org/10.1007/978-94-024-0881-2_18
https://doi.org/10.1007/978-94-024-0881-2_18
http://www.lrec-conf.org/proceedings/lrec2010/pdf/623_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2010/pdf/623_Paper.pdf
http://aclweb.org/anthology/P06-1064
http://aclweb.org/anthology/P06-1064
http://aclweb.org/anthology/J07-3004
http://aclweb.org/anthology/J07-3004
http://aclweb.org/anthology/J07-3004
http://aclweb.org/anthology/P10-1022
http://aclweb.org/anthology/P10-1022
https://doi.org/10.3115/v1/D14-1107
https://doi.org/10.3115/v1/D14-1107
http://aclweb.org/anthology/J93-2004
http://aclweb.org/anthology/J93-2004
http://aclweb.org/anthology/C02-2025
http://aclweb.org/anthology/C02-2025
https://tatoeba.org/


42

Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: extracting CCG derivations from the Penn
Chinese Treebank. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics (Coling 2010), pages 1083–1091. Coling 2010
Organizing Committee.

Sumire Uematsu, Takuya Matsuzaki, Hiroki Hanaoka,
Yusuke Miyao, and Hideki Mima. 2013. Integrat-
ing multiple dependency corpora for inducing wide-
coverage Japanese CCG resources. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1042–1051. Association for Computa-
tional Linguistics.

Ben Zimmer. 2013. Attachment ambiguity in “Frazz”.
http://languagelog.ldc.upenn.edu/
nll/?p=4566.

Ruken Çakıcı. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of the ACL
Student Research Workshop, pages 73–78, Ann Ar-
bor, Michigan. Association for Computational Lin-
guistics.

http://aclweb.org/anthology/C10-1122
http://aclweb.org/anthology/C10-1122
http://aclweb.org/anthology/C10-1122
http://aclweb.org/anthology/P13-1103
http://aclweb.org/anthology/P13-1103
http://aclweb.org/anthology/P13-1103
http://languagelog.ldc.upenn.edu/nll/?p=4566
http://languagelog.ldc.upenn.edu/nll/?p=4566
https://www.aclweb.org/anthology/P05-2013
https://www.aclweb.org/anthology/P05-2013

