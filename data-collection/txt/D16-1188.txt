



















































Regularizing Text Categorization with Clusters of Words


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1827–1837,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Regularizing Text Categorization with Clusters of Words

Konstantinos Skianis François Rousseau

LIX, École Polytechnique, France
kskianis@lix.polytechnique.fr

Michalis Vazirgiannis

Abstract

Regularization is a critical step in supervised
learning to not only address overfitting, but
also to take into account any prior knowledge
we may have on the features and their de-
pendence. In this paper, we explore state-
of-the-art structured regularizers and we pro-
pose novel ones based on clusters of words
from LSI topics, word2vec embeddings and
graph-of-words document representation. We
show that our proposed regularizers are faster
than the state-of-the-art ones and still improve
text classification accuracy. Code and data are
available online1.

1 Introduction

Harnessing the full potential in text data has always
been a key task for the NLP and ML communities.
The properties hidden under the inherent high di-
mensionality of text are of major importance in tasks
such as text categorization and opinion mining.

Although simple models like bag-of-words man-
age to perform well, the problem of overfitting still
remains. Regularization as proven in Chen and
Rosenfeld (2000) is of paramount importance in
Natural Language Processing and more specifically
language modeling, structured prediction, and clas-
sification. In this paper we build upon the work of
Yogatama and Smith (2014b) who introduce prior
knowledge of data as a regularization term. One of
the most popular structured regularizers, the group
lasso (Yuan and Lin, 2006), was proposed to avoid
large L2 norms for groups of weights.

1https://goo.gl/mKqvro

In this paper, we propose novel linguistic struc-
tured regularizers that capitalize on the clusters
learned from texts using the word2vec and graph-of-
words document representation, which can be seen
as group lasso variants. The extensive experiments
we conducted demonstrate these regularizers can
boost standard bag-of-words models on most cases
tested in the task of text categorization, by imposing
additional unused information as bias.

2 Background & Notation

We place ourselves in the scenario where we con-
sider a prediction problem, in our case text catego-
rization, as a loss minimization problem, i. e. we
define a loss function L(x,θ, y) that quantifies the
loss between the prediction hθ,b(x) of a classifier
parametrized by a vector of feature weights θ and a
bias b, and the true class label y ∈ Y associated with
the example x ∈ X . Given a training set of N data
points {(xi, yi)}i=1...N , we want to find the optimal
set of feature weights θ∗ such that:

θ∗ = argmin
θ

N∑

i=1

L(xi,θ, yi)
︸ ︷︷ ︸

empirical risk

(1)

In the case of logistic regression with binary predic-
tions (Y = {−1,+1}), hθ,b(x) = θ>x + b and
L(x,θ, y) = e−yhθ,b(x) (log loss).

2.1 Regularization

Only minimizing the empirical risk can lead to over-
fitting, that is, the model no longer learns the un-
derlying pattern we are trying to capture but fits the

1827



noise contained in the training data and thus results
in poorer generalization (e. g., lower performances
on the test set). For instance, along with some fea-
ture space transformations to obtain non-linear de-
cision boundaries in the original feature space, one
could imagine a decision boundary that follows ev-
ery quirk of the training data. Additionally, if two
hypothesis lead to similar low empirical risks, one
should select the “simpler” model for better general-
ization power, simplicity assessed using some mea-
sure of model complexity.

Loss+Penalty Regularization takes the form of
additional constraints to the minimization problem,
i. e. a budget on the feature weights, which are of-
ten relaxed into a penalty term Ω(θ) controlled via
a Lagrange multiplier λ. We refer to the book of
Boyd and Vandenberghe (2004) for the theory be-
hind convex optimization. Therefore, the overall
expected risk (Vapnik, 1991) is the weighted sum
of two components: the empirical risk and a reg-
ularization penalty term, expression referred to as
“Loss+Penalty" by Hastie et al. (2009). Given a
training set of N data points {(xi, yi)}i=1...N , we
now want to find the optimal set of feature weights
θ∗ such that:

θ∗ = argmin
θ

N∑

i=1

L(xi,θ, yi)
︸ ︷︷ ︸

empirical risk

+ λΩ(θ)

︸ ︷︷ ︸
penalty term︸ ︷︷ ︸

expected risk

(2)

L1 and L2 regularization The two most used
penalty terms are known as L1 regularization, a. k. a.
lasso (Tibshirani, 1996), and L2 regularization,
a. k. a. ridge (Hoerl and Kennard, 1970) as they cor-
respond to penalizing the model with respectively
the L1 and L2 norm of the feature weight vector θ:

θ∗ = argmin
θ

N∑

i=1

L(xi,θ, yi) + λ
p∑

j=1

|θj | (3)

θ∗ = argmin
θ

N∑

i=1

L(xi,θ, yi) + λ
p∑

j=1

θj
2 (4)

Prior on the feature weights L1 (resp. L2) reg-
ularization can be interpreted as adding a Laplacian
(resp. Gaussian) prior on the feature weight vector.
Indeed, given the training set, we want to find the

most likely hypothesis h∗ ∈ H, i. e. the one with
maximum a posteriori probability:

h∗ = argmax
h∈H

(
P(h|{(xi, yi)}i=1...N )

)

= argmax
h∈H

(
P({yi}i|{xi}i, h)P(h|{xi}i)

P({yi}i|{xi}i)

)

= argmax
h∈H

(
P({yi}i|{xi}i, h)P(h|{xi}i)

)

= argmax
h∈H

(
P({yi}i|{xi}i, h)P(h)

)
(5)

= argmax
h∈H

(
N∏

i=1

(
P(yi|xi, h)

)
P(h)

)
(6)

= argmax
h∈H

(
N∑

i=1

(
logP(yi|xi, h)

)
+ logP(h)

)

= argmin
h∈H




N∑

i=1

(
− logP(yi|xi, h)

)

︸ ︷︷ ︸
empirical risk

− logP(h)
︸ ︷︷ ︸
penalty term




For the derivation, we assumed that the hypothesis
h does not depend on the examples alone (Eq. 5)
and that the N training labeled examples are drawn
from an i.i.d. sample (Eq. 6). In that last form, we
see that the loss function can be interpreted as a neg-
ative log-likelihood and the regularization penalty
term as a negative log-prior over the hypothesis.
Therefore, if we assume a multivariate Gaussian
prior on the feature weight vector of mean vector 0
and covariance matrix Σ = σ2I (i. e. independent
features of same prior standard deviation σ), we do
obtain the L2 regularization:

P(h) =
1√

(2π)p|Σ|
e−

1
2
θ>Σ−1θ (7)

⇒ − logP(h) = 1
2σ2

θ>Iθ +
p

2
log(2πσ)

argmax
= λ‖θ‖2 2, λ =

1

2σ2
(8)

And similarly, if we assume a multivariate Lapla-
cian prior on the feature weight vector (i. e. θi ∼
Laplace(0, 1λ)), we obtain L1-regularization. In
practice, in both cases, the priors basically mean that
we expect weights around 0 on average. The main
difference between L1 and L2 regularization is that
the Laplacian prior will result in explicitly setting
some feature weights to 0 (feature sparsity) while
the Gaussian prior will only result in reducing their
values (shrinkage).

1828



2.2 Structured regularization
In L1 and L2 regularizations, features are considered
as independent, which makes sense without any ad-
ditional prior knowledge. However, similar features
have similar weights in the case of linear classifiers
– equal weights for redundant features in the ex-
treme case – and therefore, if we have some prior
knowledge on the relationships between features, we
should include that information for better general-
ization, i. e. include it in the regularization penalty
term. Depending on how the similarity between fea-
tures is encoded, e. g., through sets, trees (Kim and
Xing, 2010; Liu and Ye, 2010; Mairal et al., 2010) or
graphs (Jenatton et al., 2010), the penalization term
varies but in any case, we take into account the struc-
ture between features, hence the “structured regular-
ization” terminology. It should not be confused with
“structured prediction” where this time the outcome
is a structured object as opposed to a scalar (e. g., a
class label) classically.

Group lasso Bakin (1999) and later Yuan and Lin
(2006) proposed an extension of L1 regularization
to encourage groups of features to either go to zero
(as a group) or not (as a group), introducing group
sparsity in the model. To do so, they proposed to
regularize with the L1,2 norm of the feature weight
vector:

Ω(θ) = λ
∑

g

λg‖θg‖2 (9)

where θg is the subset of feature weights restricted
to group g. Note that the groups can be overlapping
(Jacob et al., 2009; Schmidt and Murphy, 2010; Je-
natton et al., 2011; Yuan et al., 2011) even though it
makes the optimization harder.

2.3 Learning
In our case we use a logistic regression loss function
in order to integrate our regularization terms easily.

L(x,θ, y) = log(1 + exp(−yθTx)) (10)

It is obvious that the framework can be extended to
other loss functions (e. g., hinge loss).

For the case of structured regularizers, there exist
a plethora of optimization methods such group lasso.
Since our tasks involves overlapping groups, we se-
lect the method of Yogatama and Smith (2014b).

Algorithm 1 ADMM for overlapping group-lasso
Require: augmented Lagrangian variable ρ, regulariza-

tion strengths λglas and λlas
1: while update in weights not small do

2: θ = argmin
θ

Ωlas(θ)+L(θ)+ ρ2
V∑
i=1

Ni(θi−µi)2

3: for g = 1 to G do
4: vg = proxΩglas,λgρ

(zg)

5: end for
6: u = u+ ρ(v−Mθ)
7: end while

Their method uses the alternating directions method
of multipliers (Hestenes, 1969; Powell, 1969).

Now given the lasso penalty for each feature and
the group lasso regularizer, the problem becomes:

min
θ,v

Ωlas(θ) + Ωglas(v) +
D∑

d=1

L(xd,θ, yd) (11)

so that v = Mθ, where v is a copy-vector of θ. The
copy-vector v is needed because the group-lasso reg-
ularizer contains overlaps between the used groups.
M is an indicator matrix of size L × V , where L is
the sum of the total sizes of all groups, and its ones
show the link between the actual weights θ and their
copies v. Following Yogatama and Smith (2014b)
a constrained optimization problem is formed, that
can be transformed to an augmented Lagrangian
problem:

Ωlas(θ) + Ωglas(v) + L(θ) + u>(v−Mθ)
+
ρ

2
‖v−Mθ‖22

(12)

Essentially, the problem becomes the iterative up-
date of θ, v and u:

min
θ

Ωlas(θ)+L(θ)+u>Mθ+
ρ

2
‖v−Mθ‖22 (13)

min
v

Ωglas(v) + u>v +
ρ

2
‖v−Mθ‖22 (14)

u = u + ρ(v−Mθ) (15)
Convergence Yogatama and Smith (2014b)
proved that ADMM for sparse overlapping group
lasso converges. It is also shown that a good
approximate solution is reached in a few tens of
iterations. Our experiments confirm this as well.

1829



3 Structured Regularization in NLP

In recent efforts there are results to identify useful
structures in text that can be used to enhance the ef-
fectiveness of the text categorization in a NLP con-
text. Since the main regularization approach we
are going to use are variants of the group lasso,
we are interested on prior knowledge in terms of
groups/clusters that can be found in the training text
data. These groups could capture either semantic, or
syntactic structures that affiliate words to communi-
ties. In our work, we study both semantic and syn-
tactic properties of text data, and incorporate them in
structured regularizer. The grouping of terms is pro-
duced by either LSI or clustering in the word2vec or
graph-of-words space.

3.1 Statistical regularizers
In this section, we present statistical regularizers,
i. e. with groups of words based on co-occurrences,
as opposed to syntactic ones (Mitra et al., 1997).

Network of features Sandler et al. (2009) intro-
duced regularized learning with networks of fea-
tures. They define a graph G whose edges are non-
negative with larger weights indicating greater sim-
ilarity. Conversely, a weight of zero means that two
features are not believed a priori to be similar. Pre-
vious work (Ando and Zhang, 2005; Raina et al.,
2006; Krupka and Tishby, 2007) shows such sim-
ilarities can be inferred from prior domain knowl-
edge and statistics computed on unlabeled data.

The weights of G are mapped in a matrix P ,
where Pij ≥ 0 gives the weight of the directed edge
from vertex i to vertex j. The out-degree of each
vertex is constrained to sum to one,

∑
j Pij = 1, so

that no feature “dominates" the graph.

Ωnetwork(θ) = λnet
∑

θ>kMθk (16)

whereM = α(I−P )>(I−P )+βI . The matrix M
is symmetric positive definite, and therefore it pos-
sesses a Bayesian interpretation in which the weight
vector θ, is a priori normally distributed with mean
zero and covariance matrix 2M−1. However, pre-
liminary results show poorer performance compared
to structured regularizers in larger datasets.

Sentence regularizer Yogatama and Smith
(2014b) proposed to define groups as the sentences

in the training dataset. The main idea is to define
a group dd,s for every sentence s in every training
document d so that each group holds weights for
occurring words in its sentence. Thus a word can be
a member of one group for every distinct (training)
sentence it occurs in. The regularizer is:

Ωsen(θ) =
D∑

d=1

Sd∑

s=1

λd,s‖θd,s‖2 (17)

where Sd is the number of sentences in document d.
Since modern text datasets typically contain thou-

sands of sentences and many words appear in more
than one sentence, the sentence regularizer could
potentially lead to thousands heavily overlapping
groups. As stated in the work of Yogatama and
Smith (2014b), a rather important fact is that the reg-
ularizer will force all the weights of a sentence, if it
is recognized as irrelevant. Respectively, it will keep
all the weights of a relevant sentence, even though
the group contains unimportant words. Fortunately,
the problem can be resolved by adding a lasso regu-
larization (Friedman et al., 2010).

3.2 Semantic regularizers
In this section, we present semantic regularizers
that define groups based on how semantically close
words are.

LDA regularizer Yogatama and Smith (2014a)
considered topics as another type of structure. It is
obvious that textual data can contain a huge num-
ber of topics and especially topics that overlap each
other. Again the main idea is to penalize weights
for words that co-occur in the same topic, instead of
treating the weight of each word separately.

Having a training corpus, topics can be easily ex-
tracted with the help of the latent Dirichlet allocation
(LDA) model (Blei et al., 2003). In our experiments,
we form a group by extracting the n most probable
words in a topic. We note that the extracted topics
can vary depending the text preprocessing methods
we apply on the data.

LSI regularizer Latent Semantic Indexing (LSI)
can also be used in order to identify topics or groups
and thus discover correlation between terms (Deer-
wester et al., 1990). LSI uses singular value de-
composition (SVD) on the document-term matrix to

1830



2

2

2
1

2

22

2

2

22 2

2 2
2

1
1

1

1
1

1

1

1

1

2

1
1

1
1

●

●

●
●

●

●

●

●
●

●

●

●

●

method

solut

system
linear

algebra

equat

m−dimension

lambda

matric

propos

numer

special

kindA method for solution of systems of linear 
algebraic equations with m-dimensional lambda 
matrices. A system of linear algebraic equations 
with m-dimensional lambda matrices is 
considered. The proposed method of searching 
for the solution of this system lies in reducing it 
to a numerical system of a special kind.

Figure 1: A Graph-of-words example.

identify latent variables that link co-occurring terms
with documents. The main basis behind LSI is that
words being used in the same contexts (i. e. the doc-
uments) tend to have similar meanings. We used
LSI as a baseline and compare it with other stan-
dard baselines as well as other proposed structured
regularizers. In our work we keep the top 10 words
which contribute the most in a topic.

The regularizer for both LDA and LSI is:

ΩLDA,LSI(θ) =
K∑

k=1

λ‖θk‖2 (18)

where K is the number of topics.

3.3 Graphical regularizers
In this section we present our proposed regularizers
based on graph-of-words and word2vec. Essentially
the word2vec space can be seen as a large graph
where nodes represent terms and edges similarities
between them.

Graph-of-words regularizer Following the idea
of the network of features, we introduce a simpler
and faster technique to identify relationships be-
tween features. We create a big collection graph
from the training documents, where the nodes cor-
respond to terms and edges correspond to co-
occurrence of terms in a sliding window. We present
a toy example of a graph-of-words in Figure 1.

A critical advantage of graph-of-words is that it
easily encodes term dependency and term order (via

edge direction). The strength of the dependence be-
tween two words can also be captured by assigning
a weight to the edge that links them.

Graph-of-words was originally an idea of Mihal-
cea and Tarau (2004) and Erkan and Radev (2004)
who applied it to the tasks of unsupervised keyword
extraction and extractive single document summa-
rization. Rousseau and Vazirgiannis (2013) and
Malliaros and Skianis (2015) showed it performs
well in the tasks of information retrieval and text cat-
egorization. Notably, the former effort ranked nodes
based on a modified version of the PageRank algo-
rithm.

Community detection on graph-of-words Our
goal is to identify groups or communities of words.
Having constructed the collection-level graph-of-
words, we can now apply community detection al-
gorithms (Fortunato, 2010).

In our case we use the Louvain method, a commu-
nity detection algorithm for non-overlapping groups
described in the work of Blondel et al. (2008). Es-
sentially it is a fast modularity maximization ap-
proach, which iteratively optimizes local communi-
ties until we reach optimal global modularity given
some perturbations to the current community state.
The regularizer becomes:

Ωgow(θ) =

C∑

c=1

λ‖θc‖2 (19)

where c ranges over the C communities. Thus θc
corresponds to the sub-vector of θ such that the cor-
responding features are present in the community c.
Note that in this case we do not have overlapping
groups, since we use a non-overlapping version of
the algorithm.

As we observe that the collection-level graph-of-
words does not create well separated communities of
terms, overlapping community detection algorithms,
like the work of Xie et al. (2013) fail to identify
“good" groups and do not offer better results.

Word2vec regularizer Mikolov et al. (2013) pro-
posed the word2vec method for learning continu-
ous vector representations of words from large text
datasets. Word2vec manages to capture the ac-
tual meaning of words and map them to a multi-
dimensional vector space, giving the possibility of

1831



applying vector operations on them. We introduce
another novel regularizer method, by applying un-
supervised clustering algorithms on the word2vec
space.

Clustering on word2vec Word2vec contains mil-
lions of words represented as vectors. Since
word2vec succeeds in capturing semantic similarity
between words, semantically related words tend to
group together and create large clusters that can be
interpreted as “topics".

In order to extract these groups, we use a fast
clustering algorithm such as K-Means (Macqueen,
1967) and especially Minibatch K-means. The reg-
ularizer is:

Ωword2vec(θ) =
K∑

k=1

λ‖θk‖2 (20)

whereK is the number of clusters we extracted from
the word2vec space.

Clustering these semantic vectors is a very inter-
esting area to study and could be a research topic by
itself. The actual clustering output could vary as we
change the number of clusters we are trying to iden-
tify. In this paper we do not focus on optimizing the
clustering process.

4 Experiments

We evaluated our structured regularizers on several
well-known datasets for the text categorization task.
Table 1 summarizes statistics about the ten datasets
we used in our experiments.

4.1 Datasets

Topic categorization. From the 20 Newsgroups2
dataset, we examine four binary classification tasks.
We end up with binary classification problems,
where we classify a document according to two
related categories: comp.sys: ibm.pc.hardware
vs. mac.hardware; rec.sport: baseball vs.
hockey; sci: med vs. space and alt.atheism vs.
soc.religion.christian. We use the 20NG dataset
from Python.

Sentiment analysis. The sentiment analysis
datasets we examined include movie reviews

2http://qwone.com/~jason/20Newsgroups/

dataset train dev test # words # sents

20
N

G

science 949 238 790 25787 16411
sports 957 240 796 21938 14997

religion 863 216 717 18822 18853
comp. 934 234 777 16282 10772

Se
nt

im
en

t

vote 1175 257 860 19813 43563
movie 1600 200 200 43800 49433
books 1440 360 200 21545 13806
dvd 1440 360 200 21086 13794

electr. 1440 360 200 10961 10227
kitch. 1440 360 200 9248 8998
Table 1: Descriptive statistics of the datasets

(Pang and Lee, 2004; Zaidan and Eisner, 2008)3,
floor speeches by U.S. Congressmen deciding
“yea"/“nay" votes on the bill under discussion
(Thomas et al., 2006)3 and product reviews from
Amazon (Blitzer et al., 2007)4.

4.2 Experimental setup

As features we use unigram frequency concatenated
with an additional unregularized bias term. We re-
produce standard regularizers like lasso, ridge, elas-
tic and state-of-the-art structured regularizers like
sentence, LDA as baselines and compare them with
our proposed methods.

For LSI, LDA and word2vec we use the gensim
package (Řehůřek and Sojka, 2010) in Python. For
the learning part we used Matlab and specifically
code by Schmidt et al. (2007).

We split the training set in a stratified manner to
retain the percentage of classes. We use 80% of the
data for training and 20% for validation.

All the hyperparameters are tuned on the develop-
ment dataset, using accuracy as the evaluation crite-
rion. For lasso and ridge regularization, we choose
λ from {10−2, 10−1, 1, 10, 102}. For elastic net,
we perform grid search on the same set of values
as ridge and lasso experiments for λrid and λlas.
For the LDA, LSI, sentence, graph-of-words (GoW),
word2vec regularizers, we perform grid search on
the same set of values as ridge and lasso experi-
ments for the ρ, λglas, λlas parameters. In the case
we get the same accuracy on the development data,
the model with the highest sparsity is selected. For

3http://www.cs.cornell.edu/~ainur/data.
html

4http://www.cs.jhu.edu/~mdredze/
datasets/sentiment/

1832



dataset no reg. lasso ridge elastic
group lasso

LDA LSI sentence GoW word2vec
20

N
G

science 0.946 0.916 0.954 0.954 0.968 0.968* 0.942 0.967* 0.968*
sports 0.908 0.907 0.925 0.920 0.959 0.964* 0.966 0.959* 0.946*

religion 0.894 0.876 0.895 0.890 0.918 0.907* 0.934 0.911* 0.916*
computer 0.846 0.843 0.869 0.856 0.891 0.885* 0.904 0.885* 0.911*

Se
nt

im
en

t

vote 0.606 0.643 0.616 0.622 0.658 0.653 0.656 0.640 0.651
movie 0.865 0.860 0.870 0.875 0.900 0.895 0.895 0.895 0.890
books 0.750 0.770 0.760 0.780 0.790 0.795 0.785 0.790 0.800
dvd 0.765 0.735 0.770 0.760 0.800 0.805* 0.785 0.795* 0.795*

electr. 0.790 0.800 0.800 0.825 0.800 0.815 0.805 0.820 0.815
kitch. 0.760 0.800 0.775 0.800 0.845 0.860* 0.855 0.840 0.855*

Table 2: Accuracy results on the test sets. Bold font marks the best performance for a dataset. * indicates statistical significance
of improvement over lasso at p < 0.05 using micro sign test for one of our models LSI, GoW and word2vec (underlined).

dataset no reg. lasso ridge elastic
group lasso

LDA LSI sentence GoW word2vec

20
N

G

science 100 1 100 63 19 20 86 19 21
sports 100 1 100 5 60 11 6.4 55 44

religion 100 1 100 3 94 31 99 10 85
computer 100 2 100 7 40 35 77 38 18

Se
nt

im
en

t

vote 100 1 100 8 15 16 13 97 13
movie 100 1 100 59 72 81 55 90 62
books 100 3 100 14 41 74 72 90 99
dvd 100 2 100 28 64 8 8 58 64

electr. 100 4 100 6 10 8 43 8 9
kitch. 100 5 100 79 73 44 27 75 46

Table 3: Fraction (in %) of non-zero feature weights in each model for each dataset: the smaller, the more compact the model.

LDA we set the number of topics to 1000 and we
keep the 10 most probable words of each topic as
a group. For LSI we keep 1000 latent dimensions
and we select the 10 most significant words per
topic. For the clustering process on word2vec we ran
Minibatch-Kmeans for max 2000 clusters. For each
word belonging to a cluster, we also keep the top 5
or 10 nearest words so that we introduce overlapping
groups. The intuition behind this is that words can
be part of multiple “concepts" or topics, thus they
can belong to many clusters.

4.3 Results

In Table 2 we report the results of our experiments
on the aforementioned datasets, and we distinguish
our proposed regularizers LSI, GoW, word2vec with
underlining. Our results are inline and confirm that

of (Yogatama and Smith, 2014a) showing the advan-
tages of using structured regularizers in the text cat-
egorization task. The group based regularizers per-
form systematically better than the baseline ones.

We observe that the word2vec clustering based
regularizers performs very well - achieving best per-
formance for three out of the ten data sets while it is
quite fast with regards to execution time as it appears
in Table 3 (i. e. it is four to ten times faster than the
sentence based one).

The LSI based regularization, proposed for the
first time in this paper, performs surprisingly well
as it achieves the best performance for three of the
ten datasets. This is somehow interpreted by the
fact that this method extracts the inherent dimen-
sions that best represent the different semantics of
the documents - as we see as well in the anecdotal

1833



dataset GoW word2vec

20
N

G
science 79 691
sports 137 630

religion 35 639
computer 95 594

Table 4: Number of groups.

dataset lasso ridge elastic group lassoLDA LSI sentence GoW word2vec

20
N

G

science 10 1.6 1.6 15 11 76 12 19
sports 12 3 3 7 20 67 5 9

religion 12 3 7 10 4 248 6 20
computer 7 1.4 0.8 8 6 43 5 10

Table 5: Time (in seconds) for learning with best hyperparameters.

= 0

piscataway combination jil@donuts0.uucp
jamie reading/seeing chambliss

left-handedness abilities lubin
acad sci obesity page erythromycin bottom

6= 0

and space the launch health for use that
medical you

space cancer and nasa
hiv health shuttle for tobacco that

cancer that research center space
hiv aids are use theory

keyboard data telescope available are from
system information space ftp

Table 6: Examples with LSI regularizer.

= 0

village town
edc fashionable trendy trendy fashionable
points guard guarding
crown title champion champions

6= 0

numbness tingling dizziness fevers
laryngitis bronchitis undergo undergoing

undergoes undergone healed
mankind humanity civilization planet
nasa kunin lang tao kay kong

Table 7: Examples with word2vec regularizer.

examples in Table 6, 7, 8. This method proves as
well very fast as it appears in Table 5 (i.e. it is three
to sixty times faster than the sentence based one).

The GoW based regularization although very fast,
did not outperform the other methods (while it has a
very good performance in general). It remains to
be seen whether a more thorough parameter tuning
and community detection algorithm selection would
improve further the accuracy of the method.

In Table 3 we present the feature space sizes re-
tained by each of the regularizers for each dataset.
As expected the lasso regularizer sets the vast major-
ity of the features’ weights to zero, and thus a very
sparse feature space is generated. This fact has as
a consequence the significant decrease in accuracy
performance. Our proposed structured regularizers

= 0

islands inta spain galapagos canary originated
anodise advertises jewelry mercedes benzes

diamond trendy
octave chanute lillienthal

6= 0
vibrational broiled relieving succumb

spacewalks dna nf-psychiatry itself
commented usenet golded insects alternate

self-consistent retrospect
Table 8: Examples with graph-of-words regularizer.

managed to perform better in most of the cases, in-
troducing more sparse models compared to the state-
of-the-art regularizers.

4.4 Time complexity

Although certain types of structured regularizers
improve significantly the accuracy and address
the problem of overfitting, they require a notable
amount of time in the learning process.

As seen in Yogatama and Smith (2014b), a con-
siderable disadvantage is the need of search for
the optimal hyperparameters: λglas, λlasso , and ρ,
whereas standard baselines like lasso and ridge only
have one hyperparameter and elastic net has two.

Parallel grid search can be critical for finding the
optimal set of hyperparameters, since there is no de-
pendency on each other, but again the process can
be very expensive. Especially for the case of the
sentence regularizer, the process can be extremely
slow due to two factors. First, the high number of
sentences in text data. Second, sentences consist of
heavily overlapping groups, that include words reap-
pearing in one or more sentences. On the contrary,
as it appears on Table 4, the number of clusters in the
clustering based regularizers is significantly smaller
than that of the sentences - and definitely controlled
by the designer - thus resulting in much faster com-
putation. The update of v still remains time consum-
ing for small datasets, even with parallelization.

Our proposed structured regularizers are consid-
erably faster in reaching convergence, since they of-

1834



fer a smaller number of groups with less overlapping
between words. For example, on the computer sub-
set of the 20NG dataset, learning models with the
best hyperparameter value(s) for lasso, ridge, and
elastic net took 7, 1.4, and 0.8 seconds, respectively,
on an Intel Xeon CPU E5-1607 3.00 GHz machine
with 4 cores and 128GB RAM. Given the best hyper-
parameter values the LSI regularizer takes 6 seconds
to converge, the word2vec regularizer takes 10 sec-
onds to reach convergence, the graph-of-words takes
4 seconds while the sentence regularizer requires 43
seconds. Table 5 summarizes required learning time
on 20NG datasets.

We also need to consider the time needed to ex-
tract the groups. For word2vec, Minibatch K-means
requires 15 minutes to cluster the pre-trained vectors
by Google. The clustering is executed only once.
Getting the clusters of words that belong to the vo-
cabulary of each dataset requires 20 minutes, but can
be further optimized. Finding also the communities
in the graph-of-words approach with the Louvain al-
gorithm, is very fast and requires a few minutes de-
pending on the size and structure of the graph.

In Tables 6, 7, 8 we show examples of our pro-
posed regularizers-removed and -selected groups (in
v) in the science subset of the 20NG dataset. Words
with weights (in w) of magnitude greater than 10−3

are highlighted in red (sci.med) and blue (sci.space).

5 Conclusion & Future Work

This paper proposes new types of structured regular-
izers to improve not only the accuracy but also the
efficiency of the text categorization task. We mainly
focused on how to find and extract semantic and syn-
tactic structures that lead to sparser feature spaces
and therefore to faster learning times. Overall, our
results demonstrate that linguistic prior knowledge
in the data can be used to improve categorization
performance for baseline bag-of-words models, by
mining inherent structures. We only considered lo-
gistic regression because of its interpretation for L2
regularizers as Gaussian prior on the feature weights
and following Sandler et al. (2009), we considered
a non-diagonal covariance matrix for L2 based on
word similarity before moving to group lasso as pre-
sented in the paper. We are not expecting a signif-
icant change in results with different loss functions

as the proposed regularizers are not log loss specific.
Future work could involve a more thorough in-

vestigation on how to create and cluster graphs, i. e.
covering weighted and/or signed cases. Finding bet-
ter clusters in the word2vec space is also a criti-
cal part. This is not only restricted in finding the
best number of clusters but what type of clusters
we are trying to extract. Gaussian Mixture Models
(McLachlan and Basford, 1988) could be applied in
order to capture overlapping groups at the cost of
high complexity. Furthermore, topical word embed-
dings (Liu et al., 2015) can be considered for reg-
ularization. This approach could enhance the reg-
ularization on topic specific datasets. Additionally,
we plan on exploring alternative regularization algo-
rithms diverging from the group-lasso method.

References
Rie Kubota Ando and Tong Zhang. 2005. A framework

for learning predictive structures from multiple tasks
and unlabeled data. Journal of Machine Learning Re-
search, 6:1817–1853.

Sergey Bakin. 1999. Adaptive regression and model
selection in data mining problems. Ph.D., The Aus-
tralian National University, Canberra, Australia, May.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.

John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boomboxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, ACL ’07, pages
440–447. ACL.

Vincent D Blondel, Jean-Loup Guillaume, Renaud Lam-
biotte, and Etienne Lefebvre. 2008. Fast un-
folding of communities in large networks. Jour-
nal of Statistical Mechanics: Theory and Experiment,
2008(10):P10008.

Stephen Boyd and Lieven Vandenberghe. 2004. Convex
Optimization. Cambridge University Press, New York,
NY, USA.

Stanley F. Chen and Ronald Rosenfeld. 2000. A survey
of smoothing techniques for ME models. IEEE Trans-
actions on Speech and Audio Processing, 8(1):37–50.

Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391–
407.

1835



Günes Erkan and Dragomir R. Radev. 2004. LexRank:
graph-based lexical centrality as salience in text sum-
marization. Journal of Artificial Intelligence Re-
search, 22(1):457–479.

Santo Fortunato. 2010. Community detection in graphs.
Physics reports, 486(3):75–174.

Jerome H. Friedman, Trevor Hastie, and Robert Tibshi-
rani. 2010. A note on the group lasso and a sparse
group lasso. Technical report, Department of Statis-
tics, Stanford University.

Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-
man. 2009. The elements of statistical learning, vol-
ume 2. Springer.

Magnus R. Hestenes. 1969. Multiplier and gradient
methods. Journal of Optimization Theory and Appli-
cations, 4:303—-320.

Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge
regression: Biased estimation for nonorthogonal prob-
lems. Technometrics, 12(1):55–67.

Laurent Jacob, Guillaume Obozinski, and Jean-Philippe
Vert. 2009. Group Lasso with Overlap and Graph
Lasso. In Proceedings of the 26th International Con-
ference on Machine Learning, ICML ’09, pages 433–
440.

Rodolphe Jenatton, Julien Mairal, Francis Bach, and
Guillaume Obozinski. 2010. Proximal methods for
sparse hierarchical dictionary learning. In Proceed-
ings of the 27th International Conference on Machine
Learning, ICML ’10, pages 487–494.

Rodolphe Jenatton, Jean-Yves Audibert, and Francis
Bach. 2011. Structured variable selection with
sparsity-inducing norms. Journal of Machine Learn-
ing Research, 12:2777–2824.

Seyoung Kim and Eric P. Xing. 2010. Tree-guided group
lasso for multi-task regression with structured sparsity.
In Proceedings of the 27th International Conference
on Machine Learning, ICML ’10, pages 543–550.

Eyal Krupka and Naftali Tishby. 2007. Incorporating
Prior Knowledge on Features into Learning. In Pro-
ceedings of the 11th International Conference on Arti-
ficial Intelligence and Statistics, volume 2 of AISTATS
’07, pages 227–234.

Jun Liu and Jieping Ye. 2010. Moreau-Yosida Regular-
ization for Grouped Tree Structure Learning. In Ad-
vances in Neural Information Processing Systems 23,
NIPS ’10, pages 1459–1467.

Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong
Sun. 2015. Topical word embeddings. In Proceed-
ings of the 29th national conference on Artificial intel-
ligence, pages 2418–2424.

J. Macqueen. 1967. Some methods for classification and
analysis of multivariate observations. In In 5-th Berke-
ley Symposium on Mathematical Statistics and Proba-
bility, pages 281–297.

Julien Mairal, Rodolphe Jenatton, Francis Bach, and
Guillaume Obozinski. 2010. Network flow algorithms
for structured sparsity. In Advances in Neural Infor-
mation Processing Systems 23, NIPS ’10, pages 1558–
1566.

Fragkiskos D. Malliaros and Konstantinos Skianis. 2015.
Graph-based term weighting for text categorization.
In Proceedings of the 2015 IEEE/ACM International
Conference on Advances in Social Networks Analysis
and Mining 2015, pages 1473–1479.

G.J. McLachlan and K.E. Basford. 1988. Mixture Mod-
els: Inference and Applications to Clustering. Marcel
Dekker, New York.

Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’04, pages 404–411.

Tomas Mikolov, Kai Chen, Greg S. Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at International Conference on Learning Representa-
tions, ICLR ’13.

Mandar Mitra, Chris Buckley, Amit Singhal, and Claire
Cardie. 1997. An Analysis of Statistical and Syntactic
Phrases. In Proceedings of the 5th International Con-
ference on Computer-Assisted Information Retrieval,
volume 97 of RIAO ’97, pages 200–214.

Bo Pang and Lilian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, ACL ’04, pages 271–278.

M. J. D. Powell. 1969. A method for nonlinear con-
straints in minimization problems. R. Fletcher editor,
Optimization, pages 283—-298.

Rajat Raina, Andrew Y. Ng, and Daphne Koller. 2006.
Constructing Informative Priors Using Transfer Learn-
ing. In Proceedings of the 23rd International Con-
ference on Machine Learning, ICML ’06, pages 713–
720.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50.

François Rousseau and Michalis Vazirgiannis. 2013.
Graph-of-word and tw-idf: New approach to ad hoc
ir. In Proceedings of the 22nd ACM international con-
ference on Information and knowledge management,
CIKM ’13, pages 59–68.

Ted Sandler, John Blitzer, Partha P. Talukdar, and Lyle H.
Ungar. 2009. Regularized learning with networks of
features. In Advances in Neural Information Process-
ing Systems 22, NIPS ’09, pages 1401–1408.

1836



Mark W. Schmidt and Kevin Murphy. 2010. Convex
structure learning in log-linear models: Beyond pair-
wise potentials. In Proceedings of the 13th Interna-
tional Conference on Artificial Intelligence and Statis-
tics, AISTATS ’10, pages 709–716. JMLR Workshop
and Conference Proceedings.

Mark W. Schmidt, Glenn Fung, and Rómer Rosales.
2007. Fast optimization methods for L1 regulariza-
tion: A comparative study and two new approaches.
In Proceedings of the 18th European Conference on
Machine Learning, ECML ’07, pages 286–297.

Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, EMNLP ’06, pages
327–335.

Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pages 267–288.

Vladimir Naumovich Vapnik. 1991. Principles of Risk
Minimization for Learning Theory. In Advances in
Neural Information Processing Systems 4, NIPS ’91,
pages 831–838.

Jierui Xie, Stephen Kelley, and Boleslaw K. Szyman-
ski. 2013. Overlapping community detection in net-

works: The state-of-the-art and comparative study.
ACM Computing Surveys, 45(4):43:1–43:35.

Dani. Yogatama and Noah A. Smith. 2014a. Linguistic
structured sparsity in text categorization. In Proceed-
ings of the 52nd Annual Meeting of the Association for
Computational Linguistics, ACL ’14, pages 786–796.

Dani Yogatama and Noah A. Smith. 2014b. Making the
most of bag of words: Sentence regularization with al-
ternating direction method of multipliers. In Proceed-
ings of the 31st International Conference on Machine
Learning, volume 32 of ICML ’14, pages 656–664.

Ming Yuan and Yi Lin. 2006. Model selection and es-
timation in regression with grouped variables. Jour-
nal of the Royal Statistical Society. Series B (Statistical
Methodology), 68(1):49–67.

Lei Yuan, Jun Liu, and Jieping Ye. 2011. Efficient
methods for overlapping group lasso. In Advances in
Neural Information Processing Systems 24, NIPS ’11,
pages 352–360.

Omar Zaidan and Jason Eisner. 2008. Modeling annota-
tors: A generative approach to learning from annotator
rationales. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’08, pages 31–40.

1837


