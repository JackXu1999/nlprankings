



















































Samsung Poland NLP Team at SemEval-2016 Task 1: Necessity for diversity; combining recursive autoencoders, WordNet and ensemble methods to measure semantic similarity.


Proceedings of SemEval-2016, pages 602–608,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

Samsung Poland NLP Team at SemEval-2016 Task 1:
Necessity for diversity; combining recursive autoencoders,

WordNet and ensemble methods to measure semantic similarity.
Barbara Rychalska, Katarzyna Pakulska, Krystyna Chodorowska,

Wojciech Walczak and Piotr Andruszkiewicz

Samsung R&D Institute Poland
Polna 11

Warsaw, Poland
{b.rychalska, k.pakulska, k.chodorowsk,
w.walczak, p.andruszki2}@samsung.com

Abstract

This paper describes our proposed solutions
designed for a STS core track within the Se-
mEval 2016 English Semantic Textual Sim-
ilarity (STS) task. Our method of similar-
ity detection combines recursive autoencoders
with a WordNet award-penalty system that ac-
counts for semantic relatedness, and an SVM
classifier, which produces the final score from
similarity matrices. This solution is further
supported by an ensemble classifier, combin-
ing an aligner with a bi-directional Gated Re-
current Neural Network and additional fea-
tures, which then performs Linear Support
Vector Regression to determine another set of
scores.

1 Introduction

The tasks from the Semantic Textual Similarity
(STS) contest have always attracted vivid interest
from the NLP community. The goal is to measure
the semantic similarity between two given sentences
on a scale from 0 to 5, trying to emulate the idea of
similarity degrees, thus replicating human language
understanding.

After processing two pieces of text, semantic tex-
tual similarity software captures degrees of semantic
equivalence. One of the goals of the STS task is to
create a unified framework for extracting and mea-
suring semantic similarity. Improvements achieved
in the course of this task can be useful in many
research areas, such as question answering (Marsi
and Krahmer, 2005), machine translation (Callison-
Burch, 2008), and plagiarism detection (Clough et
al., 2002).

We present a solution designed to detect both the
similarity between single words and longer, multi-
word phrases. It employs two important compo-
nents: the unfolding recursive autoencoder (RAE)
(Socher et al., 2011) and the penalty-award weight
system based on WordNet (Miller et al., 2002). First,
RAE is used to perform unsupervised learning on
parse trees, then the WordNet module adjusts the
distances of RAE vectors using awards and penal-
ties based on semantic similarities of words. The
complete pipeline includes a deep net (RAE) mod-
ule, a WordNet module, a normalization module and
a sentence similarity matrices computing module.

Another solution that ran in parallel to the RAE
pipeline, was the monolingual word aligner (in
some cases we used its corrected version with ad-
ditional features, including a bag-of-words). Fi-
nally an ensemble classifier was used to perform
Linear Support Vector Regression (Drucker et al.,
1996) over the results from all the other classifiers.
This included: the base word aligner (Sultan et al.,
2015), bi-directional Gated Recurrent Neural Net-
work (Cho et al., 2014; Chung et al., 2014), the RAE
with WordNet features and the corrected aligner.

2 System Overview

This section describes the modules that constitute
our three runs. Detailed information about the con-
figuration of these runs can be found in Section 3.

2.1 RAE with WordNet Features

RAE with the WordNet module is composed of two
major parts: a recursive autoencoder (RAE) for un-
supervised training of sentence representations and

602



additional WordNet-based submodule for enhancing
the performance of the RAE.

The RAE takes unlabeled parse trees and word
vectors as input and learns phrase features for each
node in the tree. The learned features can be used
to recursively reconstruct the vectors at each node
in the tree. The encoding part follows the Semantic
Depencency Tree Recursive Neural Network (SDT-
RNN) structure described in (Socher et al., 2014).
In the decoding part, the tree structure used to en-
code the sentence is mirrored. The reconstruction
error (the total error of the network) is counted for
all subtrees as the summed Euclidean distance be-
tween a subtree’s decoded terminal nodes and the
original word vectors. The network learns to encode
representations of meaningful phrases in tree nodes.
We use the word representation vectors published by
(Pennington et al., 2014).

The RAE is first trained in an unsupervised way
on the Corpus of Contemporary American English
(Davies, 2008) combined with SemEval STS train-
ing sets released before 2015 (only sentences with-
out labels), then a sentence similarity matrices com-
puting module (Section 2.1.2) is used to generate
similarity scores for two candidate sentences. Our
first experiment involved a procedure described in
(Socher et al., 2011) with unmodified word rep-
resentation vectors, using Euclidean distance as
a measure of word-to-word similarity.

However, we noticed that some pairs of vec-
tors representing related concepts (e.g., ’lady’ and
’woman’) were located surprisingly far from each
other in the Euclidean space, while others were too
close. As shown in (Cho et al., 2014), words and
phrases which merely belong to the same class of
concepts without being exact synonyms have a low
distance in Euclidean space (e.g., phrases ’a few sec-
onds’ and ’two years’ are grouped together). The
representation vectors returned by the RAE do not
amend this problem. For this reason, we created
an additional module which uses WordNet (Miller,
1995) to enhance our word similarity measures in
RAE trees. In Table 1 we show the influence of in-
dividual modules.

2.1.1 WordNet awards and penalties
The WordNet module adjusts the Euclidean dis-

tance between RAE vectors with awards and penal-

ties based on the semantic similarity of pairs of
words. We combined the following ideas:

• awarding pairs of words with positive semantic
similarity;

• penalizing out-of-context words and disjoint
similar concepts;

• propagating scores to higher nodes of the de-
pendency trees.

The concept of semantic similarity reflects the work
of (Han et al., 2013), while out-of-context words and
disjoint similar concepts reflect the ideas presented
in (Han et al., 2015), but there are differences in both
implementation and usage.

Awards. WordNet is used to extract semantic re-
lations between pairs of nouns, verbs, adjectives and
adverbs. Semantic distance D is measured using the
following conditions for two words:

• being equal or first-sense synonyms
(D(x, y) = 0), e.g. car auto;

• sharing common sense with WordNet fre-
quency of at least 5 (D(x, y) = 1) e.g. track
chase;

• being hypernyms or two-link hypernyms (ap-
plicable for nouns and verbs) (D(x, y) = 2),
e.g. orange citrus;

• being similar due to satellite synsets (applica-
ble for adjectives and adverbs) (D(x, y) = 3),
e.g. soggy waterlogged;

• sharing any common sense (D(x, y) = 3),
e.g. grind mash;

• being derivationally related (D(x, y) = 4),
e.g. rocket missile;

• being enclosed in the glosses of the other
word’s meanings (D(x, y) = 5), e.g. Florida
Fla.

If none of the conditions are met, the semantic dis-
tance D is set to a negative value, which facilitates
the counting of an award A described below. Thus,
effectively, the value of D is an integer such that
D(x, y) ∈ {−1, 0, 1, 2, 3, 4, 5}.

603



The semantic distance D(x, y) is transformed to
an award A using the formula e−αD(x,y) introduced
by (Li et al., 2003), where α is set to 0.25, as this
value seemed to yield the best results:

A(x, y) =

{
βe−αD(x,y), if D(x, y) ≥ 0
0, otherwise

(1)

where β is a positive number (5 by default) used
to control the level of adjustment made by the
WordNet-related score. If β = 5 the maximum
score for A(x, y) is 5. The Euclidean distances of
RAE vectors are usually in the range of [0, 10], thus
the parameter ensures that the WordNet-related sim-
ilarity is sufficiently important.

Penalties. The out-of-context penalty for word
x, OOC(x), is defined as a penalty for a word not
paired in the second input sentence SS (Han et al.,
2015). The word is not paired if its semantic similar-
ity (or award) A = 0 with all the words (referenced
below by the index i) in the second sentence:

OOC(x) =

{
−1, if ∑iA(x, SS(i)) = 0
0, otherwise

(2)

We allow three strategies for out-of-context pe-
nalization: penalize all recognizable parts of speech
(nouns, verbs, adjectives and adverbs), penalize only
nouns, penalize only physical objects (i.e. words
which have physical object in their WordNet’s hy-
pernyms path). The third option is used by default,
since both the original research of (Han et al., 2015)
and ours suggest that it usually is the best option (al-
though, in a minority of tests, the penalization of all
out-of-context nouns yields better results).

We also penalize disjoint similar concepts. Dis-
joint similar concepts DSC(x, y) are defined as
’special care antonyms’ or words of disjoint mean-
ing (i.e. Monday Tuesday). In our solution they
are found using WordNet’s hypernyms hierarchy. If
two words have a common direct hypernym, they
are disjoint similar concepts (e.g. both Monday and
Tuesday have weekday as a common hypernym in
the WordNet hierarchy). By default, the DSC(x, y)
function returns a penalty of−2 when two words are
found to be antonyms or disjoint similar concepts,
and 0 otherwise. Thus, the penalty P for two words
x and y is:

P (x, y) = OOC(x)+OOC(y)+DSC(x, y) (3)

A complete framework of WordNet-related awards
and penalties is defined by:

simWN =





A(x, y), if D(x, y) ≥ 0
P (x, y), if P (x, y) < 0
0, otherwise

(4)

Propagation. In the Sentence similarity matri-
ces computing module 2.1.2, scores are calculated
for all sentence subtrees, while WordNet’s awards
and penalties are calculated for words. Thus, to use
WordNet’s scores on all subtrees, the awards and
penalties have to be propagated up, until they affect
all nodes. To propagate WordNet’s scores on a tree
containing more than one word, we define a func-
tion:

simWN (subtree) =
n∑

i=1

simWN (i)

depth(i)
(5)

where i is an index for a single leaf in the subtree,
and n is the total number of leaves in the subtree.
simWN (i) is a similarity score for a leaf i. The
scores for particular leaves are divided by their depth
depth(i) relative to the root of the subtree to account
for their importance in the more complex trees, and
then the scores are summed up at the root. For ex-
ample, if we have a leaf with a score of 0.75, then
this score is added up at the root level with its full
weight only if the leaf is a direct child of the root.
If it is located deeper in the subtree, the score is di-
vided by this leaf’s depth relative to the root of the
subtree. The same procedure applies to all the leaves
in the current subtree. In Table 1 we compare the ef-
ficiency of the WordNet module with and without
propagation.

An alternative strategy for incorporating the
WordNet scores is to refine the vectors associated
with particular leaves and then use these refined vec-
tors to recompute complex tree nodes using RAE.
For example, given the WordNet similarity � for
words ’woman’ and ’lady’, and the vectors A for
’woman’ and B for ’lady’, the vector A is refined
using the following formula:

Arefined = �A+ (1− �)B. (6)

The vector B stays the same. The tests have proven
that the former strategy offers better results, so we
decided to stick with it.

604



Table 1: The synergy of all parts of the solution (weighted mean
presented).

Test set RAE RAE
with
Word-
Net

RAE
with
Propa-
gation

Final
RAE
based
solu-
tion

Answers
forums

0.4724 0.4916 0.5404 0.6836

Answers
students

0.7066 0.7185 0.7085 0.7679

Belief 0.6109 0.6002 0.6418 0.7517
Headlines 0.706 0.7115 0.7114 0.8315
Images 0.7469 0.7797 0.7952 0.8625
Weighted
mean

0.6753 0.6889 0.7016 0.7949

2.1.2 Sentence Similarity Matrices Computing
As a first step in our full algorithm, the RAE com-

putes vectors for every node in the dependency parse
tree. Then the subtrees of these trees are used to
create the distance matrix. The matrix is created in
a number of steps: the trees are traversed in level
order, the subtrees are then sorted by depth and the
leaves representing stop words are removed1. The
remaining subtrees are used to construct the distance
matrix, which is then filled with Euclidean distance
measures d between each pair of subtrees x and y.

DRAE = d(x, y)− simWN (7)
The above score is further transformed in two

ways: it is made certain that the score value falls
within the range 0-5, and that the distance is set to
0 if the WordNet similarity simWN has a maximum
value:

D′RAE =





0, if simWN = max
0, if DRAE < 0
5, if DRAE > 5
DRAE , otherwise

(8)

The original score DRAE is replaced with the ad-
justed score D′RAE in the distance matrix. Finally,
we use dynamic pooling module as described in
(Socher et al., 2011). The pooling module accounts
for the varying lengths of the two trees.

1Stop words list contains about 60 most common words in
training data set and all punctuation characters.

2.1.3 Final RAE-based solution
The final score was produced by Linear Support

Vector Regression over cells from the distance ma-
trices after pooling as well as 12 additional features:

• adjustment of roots (the Euclidean distance be-
tween WordNet-adjusted tree roots);

• cosine distance between vectors representing
tree roots of sentences;

• information about the negation status of the
two sentences (if both sentences contain/ do not
contain negation = true, otherwise = false);

• mean out of context penalty over full tree;
• mean disjoint penalty over full tree;
• mean WordNet similarity score over full tree;
• score from aligner (Section 2.2);
• if both sentences agree on the numbers (true for

no numbers or the same numbers; false other-
wise);

• if both sentences have the same numbers (bi-
nary);

• the absolute difference in tokens between two
sentences;

• if the numbers in one sentence contain the num-
bers from the second sentence (binary);

• the percentage of tokens similarity between two
sentences.

A new SVM classifier was created for every test
set, since for every test set a different subset of train-
ing sets was used2. Distance matrices for all classi-
fiers were created and normalized independently.

normc = 0.4(
max(min( cµ , 3σ),−3σ)

3σ
+ 1) + 0.1

(9)
First the normalization process was used to calculate
mean µ and standard deviation σ for the matrix, next
we performed calculations according to Equation 9
for every cell c of the matrix. The equation comes
from (Socher et al., 2013) and normalizes the values
to range [0.1, 0.9].

2The training data contains all previous SemEval datasets
from 2012 to 2015.

605



Table 2: Mapping training sets to test sets.
Test set Training sets
Answer-
answer

answers-students 2015, belief
2015.

Headlines MSRpar 2012 (training and test
set), SMTnews 2012, deft-news
2014, headlines 2013, headlines
2014, headlines 2015, images
2014, images 2015.

Plagiarism MSRpar 2012 (training and test
set), answers-students 2015.

Postediting deft-news 2014, deft-forum
2014, SMTnews 2012.

Question-
question

deft-news 2014, deft-forum
2014, belief 2015.

2.2 Aligner
As a monolingual word aligner we use two algo-
rithms: a basic aligner and a corrected aligner. Both
are based on the aligner described in (Sultan et al.,
2014). The basic algorithm performs the following
steps for the two sentences: align identical word se-
quences, align named entities, align content words
using dependencies and align content words using
surrounding words. Scoring is calculated according
to (Sultan et al., 2015):

score(S1, S2) =
na(S1) + na(S2)

n(S1) + n(S2)
,

where n(Si) and na(Si) are the number of content
words and aligned content words in a sentence Si,
respectively.

An aligner using only the basic algorithm could
not handle negations and antonyms well, so we mod-
ified it by adding two modules. The negation mod-
ule checks whether there is a negation component
present in only one sentence, and if so, the module
reduces the score to 0. The antonym module verifies
whether the two sentences contain at least one pair
of antonyms from a list based on the WordNet, and
if so, also reduces score to 0.

The corrected aligner is a Linear Support Vec-
tor Regression (Drucker et al., 1996) using the fol-
lowing features: the modified basic aligner feature,
Bag of Words features inspired by (Han et al., 2015)
(element-wise absolute value difference between
vectors for words and bigrams, sentences’ length

difference, percentage of exact lemma to lemma
matches) and additional features used in (Hänig et
al., 2015):

• length of the longest common subsequence of
characters (some characters may be skipped),

• length of the longest common sequence of
characters,

• cosine similarity between vectors of words,

• edit distance between sentences,

• WordNet word overlap (Šarić et al., 2012).

2.3 Ensemble
The ensemble classifier was actually a Linear Sup-
port Vector Regression over results from the other
classifiers used for semantic similarity measure-
ment. Each one of them returned score from 0 up
to 5. The following classifiers were chosen for the
ensemble approach:

• modified basic aligner, presented in Section
2.2;

• Bi-directional Gated Recurrent Neural Net-
work (Cho et al., 2014; Chung et al., 2014) with
the output neural network described in (Tai et
al., 2015);

• RAE with WordNet Features, described in Sec-
tion 2.1;

• corrected aligner, described in Section 2.2

The training data set was split into 75% vs 25%.
All classifiers except the aligner (which does not
need to be trained) were trained on the 75%. The
ensemble classifier was trained on a subset of the re-
maining data set.

Scores returned by the above classifiers were used
as features in the Linear Support Vector Regression.
The final result was rescaled to get score from the
[0, 5] range.

3 Evaluation

In order to separately train models for each evalua-
tion set, we created reference test sets that are simi-
lar to the evaluation sets, e.g. for headlines we used

606



Table 3: The results obtained over three runs in the evaluation period of SemEval 2016.
Answer-
answer

Headlines Plagiarism Postediting Question-
question

Weighted
mean

RAE (AE) 0.6577 0.8180 0.8129 0.7885 0.5867 0.7357
Ensemble (EN1) 0.6924 0.8275 0.8414 0.8352 0.6870 0.7781
Merged (EN2) 0.6924 0.8275 0.8129 0.8352 0.5867 0.7547

Table 4: Comparison of our solutions with the 2015 winning system on SemEval 2015 evaluation datasets.
Run Answers

forums
Answers
students

Belief Headlines Images Weighted
mean

RAE (AE) 0.6836 0.7747 0.7517 0.8363 0.8625 0.7949
Aligner (with our
modifications)

0.6725 0.7715 0.7282 0.8223 0.8478 0.7854

BiGRU 0.5424 0.5925 0.5917 0.7947 0.8588 0.7032
Ensemble (EN1) 0.6489 0.7664 0.7549 0.854 0.8884 0.8027
Merged (EN2) 0.6836 0.7747 0.7549 0.8540 0.8884 0.8091
DLS@CU 0.7390 0.7725 0.7491 0.8250 0.8644 0.8015

30% of headlines 2015; for answer-answer we used
30% of answers-students 2015; for plagiarism 30%
of MSRpar 2012 test set; for postediting 30% deft-
news 2014 and for question-question 30% of deft-
forum 2014. These randomly selected samples that
constituted the test sets were removed from the train-
ing sets. We used these reference test sets to find the
best parameters of our models and chose the best
model for run EN2. The final model uses all samples
from sets assigned to each evaluation set in Table 2.
Our final results are presented in Table 3.

For the AE run, we used RAE with WordNet Fea-
tures, as described in Section 2.1. For each test,
a separate classifier was created with its own train-
ing set, as presented in Table 2. The mapping was
based on the average number of words per sentence
in the set.

For the EN1, run we used the ensemble model de-
scribed in Section 2.3. In the EN2 model we chose
either RAE or ensemble based on the results for test
sets matched with evaluation sets.

As shown in Table 3, the ensemble model (EN1)
yields better results than RAE (AE) for all sets.
Thus, the merged model (EN2) falls between the
two.

We also present the results (Table 4) of our so-
lution for SemEval 2015 sets. Comparing them
with the best run from SemEval 2015 competition
(weighted mean), we concluded that bi-GRU yields

the worst results. Second and third worst results
came from the modified aligner and RAE respec-
tively. The ensemble and merged model yield the
best results that surpass the performance of the 2015
winning solution.

4 Conclusions and Future Work

Our solution combines a vector similarity feature de-
rived from word embeddings without losing the in-
formation contained in lexical similarity relations.
As it turned out, one of the primary limitations of our
paraphrase detection system is its heavy reliance on
word order, which makes the solution less universal
in its application. The other drawback of convert-
ing words to word vectors is being unable to account
for situations where the same information is format-
ted differently (for instance, units of measurement,
time expressions, etc.) Thus, our future works in-
clude improving our preprocessing module, so that
it would produce a unified input, e.g., all numbers
written in words will be converted into numerals and
all dates will be unified into one format. We will
also use specifically designed training modes to pre-
vent overfitting and create a new curriculum learning
dataset to make RAE training easier.

607



References
Chris Callison-Burch. 2008. Syntactic constraints on

paraphrases extracted from parallel corpora. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 196–205. Asso-
ciation for Computational Linguistics.

Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre,
Fethi Bougares, Holger Schwenk, and Yoshua Ben-
gio. 2014. Learning phrase representations using
RNN encoder-decoder for statistical machine transla-
tion. CoRR, 1406.1078.

Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. CoRR, 1412.3555.

Paul Clough, Robert Gaizauskas, Scott SL Piao, and
Yorick Wilks. 2002. Meter: Measuring text reuse.
In Proceedings of the 40th Annual Meeting on Associ-
ation for Computational Linguistics, pages 152–159.
Association for Computational Linguistics.

Mark Davies. 2008. The Corpus of Contemporary
American English: 520 million words, 1990-present.
http://corpus.byu.edu/coca/.

Harris Drucker, Christopher J. C. Burges, Linda Kauf-
man, Alexander J. Smola, and Vladimir Vapnik. 1996.
Support vector regression machines. In Michael
Mozer, Michael I. Jordan, and Thomas Petsche, edi-
tors, Advances in Neural Information Processing Sys-
tems 9, NIPS, Denver, CO, USA, December 2-5, 1996,
pages 155–161. MIT Press.

Lushan Han, Abhay Kashyap, Tim Finin, James May-
field, and Jonathan Weese. 2013. Umbc ebiquity-
core: Semantic textual similarity systems. In Proceed-
ings of the Second Joint Conference on Lexical and
Computational Semantics, volume 1, pages 44–52.

Lushan Han, Justin Martineau, Doreen Cheng, and
Christopher Thomas. 2015. Samsung: Align-and-
differentiate approach to semantic textual similarity.
SemEval-2015, pages 172–177.

Christian Hänig, Robert Remus, and Xose de la Puente.
2015. Exb themis: Extensive feature extraction from
word alignments for semantic textual similarity. In
Proceedings of the 9th International Workshop on Se-
mantic Evaluation (SemEval 2015), pages 264–268,
Denver, Colorado, June. Association for Computa-
tional Linguistics.

Yuhua Li, Zuhair A Bandar, and David McLean. 2003.
An approach for measuring semantic similarity be-
tween words using multiple information sources.
Knowledge and Data Engineering, IEEE Transactions
on, 15(4):871–882.

Erwin Marsi and Emiel Krahmer. 2005. Explorations
in sentence fusion. In Proceedings of the European

Workshop on Natural Language Generation, pages
109–117. Citeseer.

George A Miller, C Fellbaum, R Tengi, P Wakefield,
H LANGONE, and BR HASKELL. 2002. Word-
net: A lexical database for the english language. 2002.
Available from: http://wordnet.princeton.edu/.

George A Miller. 1995. Wordnet: A lexical database for
english. Commun. ACM, 38(11):39–41, nov.

Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).

Richard Socher, Eric H. Huang, Jeffrey Pennington, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural Infor-
mation Processing Systems 24.

Richard Socher, Chris Manning, and Yoshua Bengio.
2013. Naacl 2013 tutorial: Deep learning for nlp.

Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and de-
scribing images with sentences. TACL, 2:207–218.

Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014. Dls@cu: Sentence similarity from word
alignment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014),
pages 241–246, Dublin, Ireland, August. Association
for Computational Linguistics and Dublin City Uni-
versity.

Md Arafat Sultan, Steven Bethard, and Tamara Sumner.
2015. Dls@cu: Sentence similarity from word align-
ment and semantic vector composition. In Proceed-
ings of the 9th International Workshop on Semantic
Evaluation (SemEval 2015), pages 148–153, Denver,
Colorado, June. Association for Computational Lin-
guistics.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. March.

Frane Šarić, Goran Glavaš, Mladen Karan, Jan Šnajder,
and Bojana Dalbelo Bašić. 2012. Takelab: Systems
for measuring semantic text similarity. In SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics – Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441–448,
Montréal, Canada, 7-8 June. Association for Compu-
tational Linguistics.

608


