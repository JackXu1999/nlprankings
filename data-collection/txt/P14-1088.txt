



















































A chance-corrected measure of inter-annotator agreement for syntax


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 934–944,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

A chance-corrected measure of inter-annotator agreement for syntax

Arne Skjærholt
Language technology group, dept. of informatics

University of Oslo
arnskj@ifi.uio.no

Abstract

Following the works of Carletta (1996)
and Artstein and Poesio (2008), there is an
increasing consensus within the field that
in order to properly gauge the reliability
of an annotation effort, chance-corrected
measures of inter-annotator agreement
should be used. With this in mind, it
is striking that virtually all evaluations
of syntactic annotation efforts use uncor-
rected parser evaluation metrics such as
bracket F1 (for phrase structure) and ac-
curacy scores (for dependencies).

In this work we present a chance-corrected
metric based on Krippendorff’s α, adapted
to the structure of syntactic annotations
and applicable both to phrase structure
and dependency annotation without any
modifications. To evaluate our metric we
first present a number of synthetic experi-
ments to better control the sources of noise
and gauge the metric’s responses, before
finally contrasting the behaviour of our
chance-corrected metric with that of un-
corrected parser evaluation metrics on real
corpora.1

1 Introduction

It is a truth universally acknowledged that an an-
notation task in good standing be in possession
of a measure of inter-annotator agreement (IAA).
However, no such measure is in widespread use
for the task of syntactic annotation. This is due to
a mismatch between the formulation of the agree-
ment measures, which assumes that the annota-
tions have no or relatively little internal structure,

1The code used to produce the data in this paper,
and some of the datasets used, are available to download at
https://github.com/arnsholt/syn-agreement/

and syntactic annotation where structure is the en-
tire point of the annotation. For this reason efforts
to gauge the quality of syntactic annotation are
hampered by the need to fall back to simple ac-
curacy measures. As shown in Artstein and Poesio
(2008), such measures are biased in favour of an-
notation schemes with fewer categories and do not
account for skewed distributions between classes,
which can give high observed agreement, even if
the annotations are inconsistent.

In this article we propose a family of chance-
corrected measures of agreement, applicable to
both dependency- and constituency-based syntac-
tic annotation, based on Krippendorff’s α and tree
edit distance. First we give an overview of tradi-
tional agreement measures and why they are insuf-
ficient for syntax, before presenting our proposed
metrics. Next, we present a number of synthetic
experiments performed in order to find the best
distance function for this kind of annotation; fi-
nally we contrast our new metric and simple accu-
racy scores as applied to real-world corpora before
concluding and presenting some potential avenues
for future work.

1.1 Previous work

The definitive reference for agreement measures
in computational linguistics is Artstein and Poe-
sio (2008), who argue forcefully in favour of the
use of chance-corrected measures of agreement
over simple accuracy measures. However, most
evaluations of syntactic treebanks use simple ac-
curacy measures such as bracket F1 scores for
constituent trees (NEGRA, Brants, 2000; TIGER,
Brants and Hansen, 2002; Cat3LB, Civit et al.,
2003; The Arabic Treebank, Maamouri et al.,
2008) or labelled or unlabelled attachment scores
for dependency syntax (PDT, Hajič, 2004; PCEDT
Mikulová and Štěpánek, 2010; Norwegian De-
pendency Treebank, Skjærholt, 2013). The only
work we know of using chance-corrected metrics

934



is Ragheb and Dickinson (2013), who use MASI
(Passonneau, 2006) to measure agreement on de-
pendency relations and head selection in multi-
headed dependency syntax, and Bhat and Sharma
(2012), who compute Cohen’s κ (Cohen, 1960)
on dependency relations in single-headed depen-
dency syntax. A limitation of the first approach is
that token ID becomes the relevant category for
the purposes of agreement, while the second ap-
proach only computes agreements on relations, not
on structure.

In grammar-driven treebanking (or parsebank-
ing), the problems encountered are slightly differ-
ent. In HPSG and LFG treebanking annotators do
not annotate structure directly. Instead, the gram-
mar parses the input sentences, and the annotator
selects the correct parse (or rejects all the candi-
dates) based on discriminants2 of the parse forest.
In this context, de Castro (2011) developed a vari-
ant of κ that measures agreement over discrimi-
nant selection. This is different from our approach
in that agreement is computed on annotator deci-
sions rather than on the treebanked analyses, and
is only applicable to grammar-based approaches
such as HPSG and LFG treebanking.

The idea of using edit distance as the basis
for an inter-annotator agreement metric has previ-
ously been explored by Fournier (2013). However
that work used a boundary edit distance as the ba-
sis of a metric for the task of text segmentation.

1.2 Notation

In this paper, we mostly follow the notation and
terminology of Artstein and Poesio (2008), with
some additions. The key components in an agree-
ment study are the items annotated, the coders who
make judgements on individual items, and the an-
notations created for the items. We denote these as
follows:

• The set of items I = {i1, i2, . . . }
• The set of coders C = {c1, c2, . . . }
• The set of annotations X is a set of sets X =
{Xi|i ∈ I} where each set Xi = {xic|c ∈
C} contains the annotations for each item. If
not all coders annotate all items, the different
Xi will be of different sizes.

2A discriminant is an attribute of the analyses produced
by the grammar where some of the analyses differ, e.g. is the
word jump a noun or a verb, or does a PP attach to a VP or
the VP’s object NP.

In the case of nominal categorisation we will also
use the set K of possible categories.

2 The metric

The most common metrics used in computational
linguistics are the metrics κ (Cohen, 1960, in-
troduced to computational linguistics by Carletta,
1996) and π (Scott, 1955). These metrics express
agreement on a nominal coding task as the ra-
tio κ, π = Ao−Ae/1−Ae where Ao is the observed
agreement andAe the expected agreement accord-
ing to some model of “random” annotation. Both
metrics have essentially the same model of ex-
pected agreement:

Ae =
∑
k∈K

P (k|c1)P (k|c2) (1)

differing only in how they estimate the probabil-
ities: κ assigns separate probability distributions
to each coder based on their observed behaviour,
while π uses the same distribution for both coders
based on their aggregate behaviour.

Now, if we want to perform this same kind of
evaluation on syntactic annotation it is not possible
to use κ or π directly. In the case of dependency-
based syntax we could conceivably use a variant
of these metrics by considering the ID of a to-
ken’s head as a categorical variable (the approach
taken in Ragheb and Dickinson, 2013), but we ar-
gue that this is not satisfactory. This use of the
metrics would consider agreement on categories
such as “tokens whose head is token number 24”,
which is obviously not a linguistically informative
category. Thus we have to reject this way of as-
sessing the reliability of dependency syntax anno-
tation. Also, this approach is not directly general-
isable to constituency-based syntax.

For dependency syntax we could generalise
these metrics similarly to how κ is generalised to
κw to handle partial credit for overlapping annota-
tions. Let the function LAS(t1, t2) be the number
of tokens with the same head and label in the two
trees t1 and t2, T (i) the set of trees possible for
an item i ∈ I , and tokens the number of tokens
in the corpus. Then we can compute an expected
agreement as follows:

Ae =
1

tokens

∑
i∈I

∑
t1,t2∈T (i)2

LASe(t1, t2) (2)

LASe(t1, t2) = P (t1|c1)P (t2|c2)LAS(t1, t2)

935



ROOT

saw

man

the
DET

I

SUBJ OBJ

PRED

(a) The original depen-
dency tree

�

PRED

OBJ

DET

SUBJ

(b) The tree used in com-
parisons

Figure 1: Transformation of dependency trees be-
fore comparison

We see three problems with this approach. First
of all the number of possible trees for a sentence
grows exponentially with sentence length, which
means that explicitly iterating over all possible
such pairs is computationally intractable, nor have
we been able to easily derive an algorithm for this
particular problem from standard algorithms.

Second, the question of which model to use for
P (t|c) is not straightforward. It is possible to use
generative parsing models such as PCFGs or the
generative dependency models of Eisner (1996),
but agreement metrics require a model of random
annotation, and as such using models designed for
parsing runs the risk of over-estimating Ae, result-
ing in artificially low agreement scores.

Finally, it may be hard to establish a consensus
in the field of which particular metric to use. As
shown by the existence of three different metrics
(κ, π and S (Bennett et al., 1954)) for the rela-
tively simple task of nominal coding, the choice
of model for P (t|c) will not be obvious, and thus
differing choices of generative model as well as
different choices for parameters such as smooth-
ing will result in subtly different agreement met-
rics. The results of these different metrics will not
be directly comparable, which will make the re-
sults of groups using different metrics unnecessar-
ily hard to compare.

Instead, we propose to use an agreement mea-
sure based on Krippendorff’s α (Krippendorff,
1970; Krippendorff, 2004) and tree edit distance.
In this approach we compare tree structures di-
rectly, which is extremely parsimonious in terms
of assumptions, and furthermore sidesteps the
problem of probabilistically modelling annotators’
behaviour entirely. Krippendorff’sα is not as com-
monly used as κ and π, but it has the advantage of
being expressed in terms of an arbitrary distance

function δ.
A full derivation of α is beyond the scope of

this article, and we will simply state the formula
used to compute the agreement. Krippendorff’s
α is normally expressed in terms of the ratio of
observed and expected disagreements: α = 1 −
Do/De, where Do is the mean squared distance be-
tween annotations of the same item and De the
mean squared distance between all pairs of anno-
tations:

Do =
∑
i∈I

1
|Xi| − 1

∑
c∈C

∑
c′∈C

δ(xic, xic′)2

De =
1∑

i∈I |Xi| − 1
∑
i∈I

∑
c∈C

∑
i′∈I

∑
c′∈C

δ(xic, xi′c′)2

Note that in the expression for De, we are com-
puting the difference between annotations for dif-
ferent items; thus, our distance function for syn-
tactic trees needs to be able to compute the differ-
ence between arbitrary trees for completely unre-
lated sentences. The function δ can be any func-
tion as long as it is a metric; that is, it must be
(1) non-negative, (2) symmetric, (3) zero only for
identical inputs, and (4) it must obey the triangle
inequality:

1. ∀x, y : δ(x, y) ≥ 0
2. ∀x, y : δ(x, y) = δ(x, y)
3. ∀x, y : δ(x, y) = 0⇔ x = y
4. ∀x, y, z : δ(x, y) + δ(y, z) ≥ δ(x, z)
This immediately excludes metrics like Pars-

Eval (Black et al., 1991) and Leaf-Ancestor
(Sampson and Babarczy, 2003), since they assume
that the trees being compared are parses of the
same sentence. Instead, we base our work on tree
edit distance. The tree edit distance (TED) prob-
lem is defined analogously to the more familiar
problem of string edit distance: what is the min-
imum number of edit operations required to trans-
form one tree into the other? See Bille (2005)
for a thorough introduction to the tree edit dis-
tance problem and other related problems. For
this work, we used the algorithm of Zhang and
Shasha (1989). Tree edit distance has previously
been used in the TEDEVAL software (Tsarfaty et
al., 2011; Tsarfaty et al., 2012) for parser evalua-
tion agnostic to both annotation scheme and the-
oretical framework, but this by itself is still an

936



�

PRED

OBJ

DET

SUBJ

�

PRED

OBJ

ATR

PRED

OBJ

DET

DET

SUBJ

�

PRED

SUBJ

Figure 2: Three trees with distance zero using
δdiff

uncorrected accuracy measure and thus unsuitable
for our purposes.3

When comparing syntactic trees, we only want
to compare dependency relations or non-terminal
categories. Therefore we remove the leaf nodes in
the case of phrase structure trees, and in the case of
dependency trees we compare trees whose edges
are unlabelled and nodes are labelled with the de-
pendency relation between that word and its head;
the root node receives the label �. An example of
this latter transformation is shown in Figure 1.

We propose three different distance functions
for the agreement computation: the unmodified
tree edit distance function, denoted δplain, a sec-
ond function δdiff (x, y) = TED(x, y)−abs(|x|−
|y|), the edit distance minus the difference in
length between the two sentences, and finally
δnorm(x, y) = TED(x,y)/|x|+|y|, the edit distance
normalised to the range [0, 1].4

The plain TED is the simplest in terms of parsi-
mony assumptions, however it may overestimate
the difference between sentences, we intuitively
find to be syntactically similar. For example the
only difference between the two leftmost trees in
Figure 2 is a modifier, but δplain gives them dis-
tance 4 and δdiff 0. On the other hand, δdiff might
underestimate some distances as well; for exam-

3While it is quite different from other parser evaluation
schemes, TEDEVAL does not correct for chance agreement
and is thus an uncorrected metric. It could of course form
the basis for a corrected metric, given a suitable measure of
expected agreement.

4We can easily show that |x| + |y| is an upper bound on
the TED, corresponding to deleting all nodes in the source
tree and inserting all the nodes in the target.

ple the leftmost and rightmost trees also have dis-
tance zero using δdiff , despite our syntactic intu-
ition that the difference between a transitive and
an intransitive should be taken account of.

The third distance function, δnorm, takes into
account a slightly different concern; namely that
when comparing a long sentence and a short sen-
tence, the distance has to be quite large simply to
account for the difference in number of nodes, un-
like comparing two short or two long sentences.
Normalising to the range [0, 1] puts all pairs on an
equal footing.

However, we cannot a priori say which of the
three functions is the optimal choice of distance
functions. The different functions have different
properties, and different advantages and draw-
backs, and the nature of their strengths and weak-
nesses differ. We will therefore perform a number
of synthetic experiments to investigate their prop-
erties in a controlled environment, before applying
them to real-world data.

3 Synthetic experiments

In the previous section, we proposed three
different agreement metrics αplain, αdiff and
αnorm, each involving different trade-offs. Decid-
ing which of these metrics is the best one for our
purposes of judging the consistency of syntactic
annotation poses a bit of a conundrum. We could
at this point apply our metrics to various real cor-
pora and compare the results, but since the consis-
tency of the corpora is unknown, it’s impossible to
say whether the best metric is the one resulting in
the highest scores, the lowest scores or somewhere
in the middle. To properly settle this question, we
first performed a number of synthetic experiments
to gauge how the different metrics respond to dis-
agreement.

The general approach we take is based on that
used by Mathet et al. (2012), adapted to depen-
dency trees. An already annotated corpus, in our
case 100 randomly selected sentences from the
Norwegian Dependency Treebank (Solberg et al.,
2014), are taken as correct and then permuted to
produce “annotations” of different quality. For de-
pendency trees, the input corpus is permuted as
follows:

1. Each token has a probability prelabel of being
assigned a different label uniformly at ran-
dom from the set of labels used in the corpus.

937



-0.2

0

0.2

0.4

0.6

0.8

1

0 0.2 0.4 0.6 0.8 1

A
gr

ee
m

en
t

prelabel = preattach

αplain
αdiff
αnorm

LAS

Figure 3: Mean agreement over ten runs

2. Each token has a probability preattach of be-
ing assigned a new head uniformly at random
from the set of tokens not dominated by the
token.

The second permutation process is dependent on
the order the tokens are processed, and we con-
sider the tokens in the post-order5 as dictated by
the original tree. This way tokens close to the root
have a fair chance of having candidate heads if
they are selected. A pre-order traversal would re-
sult in tokens close to the root having few options,
and in particular if the root has a single child, that
node has no possible new heads unless one of its
children has been assigned the root as its new head
first. For example in the trees in figure 2, assign-
ing any other head than the root to the PRED nodes
directly dominated by the root will result in in-
valid (cyclic and unconnected) dependency trees.
Traversing the tokens in the linear order dictated
by the sentence has similar issues for tokens close
to the root and close to the start of the sentence.

For our first set of experiments, we set
prelabel = preattach and evaluated the different
agreement metrics for 10 evenly spaced p-values
between 0.1 and 1.0. Initial exploration of the
data showed that the mean follows the median
very closely regardless of metric and perturbation
level, and therefore we only report the mean scores
across runs in this paper. The results of these ex-
periments are shown in Figure 3, with the labelled
attachment score6 (LAS) for comparison.

5That is, the child nodes of a node are all processed before
the node itself. Nodes on the same level are traversed from
left to right.

6The de facto standard parser evaluation metric in depen-

-0.2

0

0.2

0.4

0.6

0.8

1

0 0.2 0.4 0.6 0.8 1

A
gr

ee
m

en
t

prelabel

αplain
αdiff
αnorm

LAS

Figure 4: Mean agreement over ten runs,
preattach = 0

The αdiff metric is clearly extremely sensitive
to noise, with p = 0.1 yielding mean αdiff =
15.8%, while αnorm is more lenient than both
LAS and αplain, with mean αnorm = 14.5% at
p = 1, quite high compared to LAS = 0.9%,
αplain = −6.8% and αdiff = −246%. To fur-
ther study the sensitivity of the metrics to the two
kinds of noise, we performed an additional set of
experiments, setting one p = 0 while varying the
other over the same range as in the previous exper-
iment, the results of which are shown in Figures 4
and 5.

The LAS curves are mostly unremarkable, with
one exception: Mean LAS at preattach = 1 of Fig-
ure 5 is 23.9%, clearly much higher than we would
expect if the trees were completely random. In
comparison, mean LAS when only labels are per-
turbed is 4.1%, and since the sample space of trees
of size n is clearly much larger than that of rela-
bellings, a uniform random selection of tree would
yield a LAS much closer to 0. This shows that our
tree shuffling algorithm has a non-uniform distri-
bution over the sample space.

While the behaviour of our alphas and LAS are
relatively similar in Figure 3, Figures 4 and 5 show
that they do in fact have important differences.
Whereas LAS responds linearly to perturbation of
both labels and structure, with its parabolic be-
haviour in Figure 3 being simply the product of
these two linear responses, the α metrics respond
differently to structural noise and label noise, with
label disagreements being penalised less harshly

dency parsing: the percentage of tokens that receive the cor-
rect head and dependency relation.

938



-0.2

0

0.2

0.4

0.6

0.8

1

0 0.2 0.4 0.6 0.8 1

A
gr

ee
m

en
t

preattach

αplain
αdiff
αnorm

LAS

Figure 5: Mean agreement over ten runs,
prelabel = 0

than structural disagreements.
The reason for the strictness of the αdiff met-

ric and the laxity of αnorm is the effects the mod-
ified distance functions have on the distribution
of distances. The δdiff function causes an ex-
treme shift of the distances towards 0; more than
30% of the sentence pairs have distance 0, 1, or
2, which causes Ddiffe to be extremely low and
thus gives disproportionally large weight to non-
zero distances in Ddiffo . On the other hand δnorm
causes a rightward shift of the distances, which re-
sults in a highDnorme and thus individual disagree-
ments having less weight.

4 Real-world corpora

Synthetic experiments do not always fully re-
flect real-world behaviour, however. Therefore we
will also evaluate our metrics on real-world inter-
annotator agreement data sets. In our evaluation,
we will contrast labelled accuracy, the standard
parser evaluation metric, and our three α metrics.
In particular, we are interested in the correlation
(or lack thereof) between LAS and the alphas,
and whether the results of our synthetic experi-
ments correspond well with the results on real-
world IAA sets. Finally, we also evaluate the met-
ric on both dependency and phrase structure data.

4.1 The corpora

We obtained7 data from four different corpora.
Three of the data sets are dependency treebanks

7We contacted a number of treebank projects, among
them the Penn Treebank and the Prague Dependency Tree-
bank, but not all of them had data available.

Corpus Sentences Tokens

NDT 1a 130 1674
NDT 2a 110 1594
NDT 3a 150 1997

CDT (da)a 162 2394
CDT (en)a 264 5528
CDT (es)b 55 924
CDT (it)c 136 3057

PCEDTd 3531 61737

SSDe 96 1581
a 2 annotators
b 4 annotators, avg. 2.8 annotators/text (min. 2, max. 4)
c 3 annotators, avg. 2.7 annotators/text
d 11 annotators, avg. 2.5 annotators/text (min. 2, max. 6)
e 3 annotators, avg. 2.9 annotators/sent.

Table 1: Sizes of the different IAA corpora

(NDT, CDT, PCEDT) and one phrase structure
treebank (SSD), and of the dependency tree-
banks the PCEDT contains semantic dependen-
cies, while the other two have traditional syntac-
tic dependencies. The number of annotators and
sizes of the different data sets are summarised in
Table 1.

NDT The Norwegian Dependency Treebank
(Solberg et al., 2014) is a dependency treebank
constructed at the National Library of Norway.
The data studied in this work has previously been
used by Skjærholt (2013) to study agreement,
but using simple accuracy measures (UAS, LAS)
rather than chance-corrected measures. The IAA
data set is divided into three parts, corresponding
to different parsers used to preprocess the data be-
fore annotation; what we term NDT 1 through 3
correspond to what Skjærholt (2013) labels Dan-
ish, Swedish and Norwegian, respectively.

CDT The Copenhagen Dependency Treebanks
(Buch-Kromann et al., 2009; Buch-Kromann and
Korzen, 2010) is a collection of parallel depen-
dency treebanks, containing data from the Danish
PAROLE corpus (Keson, 1998b; Keson, 1998a)
in the original Danish and translated into English,
Italian and Spanish.

PCEDT The Prague Czech-English Depen-
dency Treebank 2.0 Hajič et al. (2012) is a par-
allel corpus of English and Czech, consisting of
English data from the Wall Street Journal Section
of the Penn Treebank (Marcus et al., 1993) and

939



Czech translations of the English data. The syn-
tactic annotations are layered and consist of an
analytical layer similar to the annotations in most
other dependency treebanks, and a more semantic
tectogrammatical layer.

Our data set consists of a common set of analyt-
ical annotations shared by all the annotators, and
the tectogrammatical analyses built on top of this
common foundation. A distinguishing feature of
the tectogrammatical analyses, vis a vis the other
treebanks we are using, is that semantically empty
words only take part in the analytical annotation
layer and nodes are inserted at the tectogrammat-
ical layer to represent covert elements of the sen-
tence not present in the surface syntax of the ana-
lytical layer. Thus, inserting and deleting nodes is
a central part of the task of tectogrammatical an-
notation, unlike the more surface-oriented annota-
tion of our other treebanks, where the tokenisation
is fixed before the text is annotated.

SSD The Star-Sem Data is a portion of the
dataset released for the *SEM 2012 shared
task (Morante and Blanco, 2012), parsed using
the LinGO English Resource Grammar (ERG,
Flickinger, 2000) and the resulting parse forest
disambiguated based on discriminants. The ERG
is an HPSG-based grammar, and as such its analy-
ses are attribute-value matrices (AVMs); an AVM
is not a tree but a directed acyclic graph however,
and for this reason we compute agreement not on
the AVM but the so-called derivation tree. This
tree describes the types of the lexical items in the
sentence and the bottom-up ordering of rule ap-
plications used to produce the final analysis and
can be handled by our procedure like any phrase-
structure tree.

4.2 Agreement results

To evaluate our corpora, we compute the three α
variants described in the previous two sections,
and compare these with labelled accuracy scores.

When there are more than two annotators, we
generalise the metric to be the average pairwise
LAS for each sentence, weighted by the length of
the sentence. Let LAS(t1, t2) be the fraction of to-
kens with identical head and label in the trees t1
and t2; the pairwise labelled accuracy LASp(X)
of a set of annotations X as described in section

Corpus αplain αdiff αnorm LAS

NDT 1 98.4 93.0 98.8 94.0
NDT 2 98.9 95.0 99.1 94.4
NDT 3 97.9 91.2 98.7 95.3

CDT (da) 95.7 84.7 96.2 90.4
CDT (en) 92.4 70.7 95.0 88.4
CDT (es) 86.6 48.8 85.8 78.9a

CDT (it) 84.5 55.7 89.2 81.3b

PCEDT 95.9 89.9 96.5 68.0c

SSD 99.1 98.6 99.3 87.9d
a 2 sentences ignored
b 15 sentences ignored
c 1178 sentences ignored
d Mean pairwise Jaccard similarity

Table 2: Agreement scores on real-world corpora

1.2 is:

LASp(X) =
1∑

i |xi1|
∑ |xi1|Λ(Xi)

|Xi|(|Xi|−1)/2
(3)

Λ(Xi) =
|C|∑
c=1

|C|∑
c′=c+1

LAS(xic, xic′)

This is equivalent to the traditional metric in the
case where there are only two annotators.

As our uncorrected metric for comparing two
phrase structure trees we do not use the traditional
bracket F1 as it does not generalise well to more
than two annotators, but rather Jaccard similarity.
The Jaccard similarity of two sets A and B is the
ratio of the size of their intersection to the size
of their union: J(A,B) = |A∩B|/|A∪B|, and we
use the Jaccard similarity of the sets of labelled
bracketings of two trees as our uncorrected mea-
sure. To compute the similarity for a complete set
of annotations we use the mean pairwise Jaccard
similarity weighted by sentence length; that is, the
same procedure as in 3, but using Jaccard similar-
ity rather than LAS.

Since LAS assumes that both of the sentences
compared have identical sets of tokens, we had
to exclude a number of sentences from the LAS
computation in the cases of the English and Ital-
ian CDT corpora, and especially the PCEDT. The
large number of sentences excluded in the PCEDT
is due to the fact that in the tectogrammatical anal-
ysis of the PCEDT, inserting and deleting nodes is
an important part of the annotation task.

Looking at the results in Table 2, we observe

940



40

50

60

70

80

90

100

70 75 80 85 90 95 100

α

LAS

αplain
αdiff
αnorm

Figure 6: Correlation of LAS with α

two things. Most obvious, is the extremely large
gap between the LAS and α metrics for the
PCEDT data. However, there is a more subtle
point; the orderings of the corpora by the differ-
ent metrics are not the same. LAS order the cor-
pora NDT 3, 2, 1, CDT da, en, it, es, PCEDT,
whereas αdiff and αnorm gives the order NDT 2,
1, 3, PCEDT, CDT da, en, it, es, and αplain gives
the same order as the other alphas but with CDT es
and it changing places. Furthermore, as the scatter-
plot in Figure 6 shows, there is a clear correlation
between the α metrics and LAS, if we disregard
the PCEDT results.

The reason the PCEDT gets such low LAS is
essentially the same as the reason many sentences
had to be excluded from the computation in the
first place; since inserting and deleting nodes is
an integral part of the tectogrammatical annotation
task, the assumption implicit in the LAS computa-
tion that sentences with the same number of nodes
have the same nodes in the same order is obviously
false, resulting in a very low LAS.

The corpus that scores the highest for all three
metrics is the SSD corpus; the reason for this is
uncertain, as our corpora differ along many dimen-
sions, but the fact that the annotation was done by
professional linguists who are very familiar with
the grammar used to parse the data is likely a
contributing factor. The difference between the α
metrics and the Jaccard similarity is larger than
the difference between α and LAS for our depen-
dency corpora, however the two similarity metrics
are not comparable, and it is well known that for
phrase structures single disagreements such as a
PP-attachment disagreement can result in multiple

disagreeing bracketings.

5 Conclusion

The most important conclusion we draw from this
work is the most appropriate agreement metric for
syntactic annotation. First of all, we disqualify the
LAS metric, primarily due to the methodologi-
cal inadequacies of using an uncorrected measure.
While our experiments did not reveal any seri-
ous shortcomings (unlike those of Mathet et al.,
2012 who in the case of categorisation showed
that for large p the uncorrected measure can be
increasing), the methodological problems of un-
corrected metrics makes us wary of LAS as an
agreement metric. Next, of the three α metrics,
αplain is clearly the best; αdiff is extremely sen-
sitive to even moderate amounts of disagreement,
while αnorm is overly lenient.

Looking solely at Figure 3, one might be led to
believe that LAS and αplain are interchangeable,
but this is not the case. As shown by Figures 4
and 5, the paraboloid shape of the LAS curve in
Figure 3 is simply the combination of the met-
ric’s linear responses to both label and structural
perturbations. The behaviour of α on the other
hand is more complex, with structural noise be-
ing penalised harder than perturbations of the la-
bels. Thus, the similarity of LAS and αplain is not
at all assured when the amounts of structural and
labelling disagreements differ. Additionally, we
consider this imbalanced weighting of structural
and labelling disagreements a benefit, as structure
is the larger part of syntactic annotation compared
to the labelling of the dependencies/bracketings.
Finally our experiments show that α is a single
metric that is applicable to both dependencies and
phrase structure trees.

Furthermore, α metrics are far more flexible
than simple accuracy metrics. The use of a dis-
tance function to define the metric means that
more fine-grained distinctions can be made; for
example, if the set of labels on the structures is
highly structured, partial credit can be given for
differing annotations that overlap. For example, if
different types of adverbials (temporal, negation,
etc.) receive different relations, as is the case in
the Swedish Talbanken05 (Nivre et al., 2006) cor-
pus, confusion of different adverbial types can be
given less weight than confusion between subject
and object. The α-based metrics are also far easier
to apply to a more complex annotation task such

941



as the tectogrammatical annotation of the PCEDT.
In this task inserting and deleting nodes is an in-
tegral part of the annotation, and if two annotators
insert or delete different nodes the all-or-nothing
requirement of identical yield of the LAS metric
makes it impossible as an evaluation metric in this
setting.

5.1 Future work
In future work, we would like to investigate the use
of other distance functions, in particular the use
of approximate tree edit distance functions such
as the pq-gram algorithm (Augsten et al., 2005).
For large data sets such as the PCEDT set used in
this work, computing α with tree edit distance as
the distance measure can take a very long time.8

This is due to the fact that α requires O(n2) com-
parisons to be made, each of which is O(n2) us-
ing our current approach. The problem of directed
graph edit distance is NP-hard, which means that
to apply our method to HPSG analyses directly ap-
proximate algorithms are a requirement.

Another avenue for future work is improved
synthetic experiments. As we saw, our implemen-
tation of tree perturbations was biased towards
trees similar in shape to the source tree, and an im-
proved permutation algorithm may reveal interest-
ing edge-case behaviour in the metrics. A method
for perturbing phrase structure trees would also
be interesting, as this would allow us to repeat
the synthetic experiments performed here using
phrase structure corpora to compare the behaviour
of the metrics on the two types of corpus.

Finally, annotator modelling techniques like
that presented in Passonneau and Carpenter (2013)
has obvious advantages over agreement coeffi-
cients such as α. These techniques are interpreted
more easily than agreement coefficients, and they
allow us to assess the quality of individual annota-
tors, a crucial property in crowd-sourcing settings
and something that’s impossible using agreement
coefficients.

Acknowledgements

I would like to thank Jan Štěpánek at Charles Uni-
versity for data from the PCEDT and help with
the conversion process, the CDT project for pub-
lishing their agreement data, Per Erik Solberg at

8The Python implementation used in this work, using
NumPy and the PyPy compiler, took seven and a half hours
compute a single α for the PCEDT data set on an Intel Core
i7 2.9 GHz computer. The program is single-threaded.

the Norwegian National Library for data from
the NDT, and Emily Bender at the University of
Washington for the SSD data.

References
Ron Artstein and Massimo Poesio. 2008. Inter-Coder

Agreement for Computational Linguistics. Compu-
tational Linguistics, 34(4):555–596.

Nikolaus Augsten, Böhlen Michael, and Johann Gam-
per. 2005. Approximate Matching of Hierarchical
Data Using pq-Grams. In Proceedings of the 31st
international conference on Very large data bases,
pages 301–312, Trondheim. VLDB Endowment.

E. M. Bennett, R. Alpert, and A. C. Goldstein. 1954.
Communications Through Limited-Response Ques-
tioning. Public Opinion Quarterly, 18(3):303–308.

Riyaz Ahmad Bhat and Dipti Misri Sharma. 2012.
A Dependency Treebank of Urdu and its Evalua-
tion. In Proceedings of the Sixth Linguistic Anno-
tation Workshop, pages 157–165, Jeju. Association
for Computational Linguistics.

Philip Bille. 2005. A survey on tree edit distance and
related problems. Theoretical Computer Science,
337(1-3):217–239, June.

Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Don-
ald Hindle, Robert Ingria, Frederick Jelinek, Ju-
dith Klavans, Mark Liberman, Mitchell P. Mar-
cus, Salim Roukos, Beatrice Santorini, and Tomek
Strzalkowski. 1991. A Procedure for Quantita-
tively Comparing the Syntactic Coverage of English
Grammars. In Proceedings of the workshop on
Speech and Natural Language, pages 306–311, Pa-
cific Grove, USA.

Sabine Brants and Silvia Hansen. 2002. Developments
in the TIGER Annotation Scheme and their Realiza-
tion in the Corpus. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation, pages 1643–1649.

Thorsten Brants. 2000. Inter-Annotator Agreement for
a German Newspaper Corpus. In Proceedings of the
Second International Conference on Language Re-
sources and Evaluation.

Matthias Buch-Kromann and Iørn Korzen. 2010. The
unified annotation of syntax and discourse in the
Copenhagen Dependency Treebanks. In Proceed-
ings of the Fourth Linguistic Annotation Workshop,
pages 127–131, Uppsala. Association for Computa-
tional Linguistics.

Matthias Buch-Kromann, Iørn Korzen, and Hen-
rik Høeg Müller. 2009. Uncovering the ’lost’ struc-
ture of translations with parallel treebanks. In Fabio
Alves, Susanne Göpferich, and Inger Mees, editors,

942



Methodology, Technology and Innovation in Trans-
lation Process Research, pages 199–224. Samfund-
slitteratur, Frederiksberg.

Jean Carletta. 1996. Assessing Agreement on Classi-
fication Tasks: The Kappa Statistic. Computational
Linguistics, 22(2):249–254.

Montserrat Civit, Alicia Ageno, Borja Navarro, Núria
Bufı́, and M. Antònia Martı́. 2003. Qualitative and
Quantitative Analysis of Annotators’ Agreement in
the Development of. In Joakim Nivre and Erhard
Hinrichs, editors, Proceedings of the Second Work-
shop on Treebanks and Linguistic Theories, pages
21–32, Växjö. Växjö University Press.

Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37–46, April.

Sérgio Ricardo de Castro. 2011. Developing relia-
bility metrics and validation tools for datasets with
deep linguistic information. Master’s thesis, Univer-
sidade de Lisboa.

Jason M. Eisner. 1996. Three New Probabilistic
Models for Dependency Parsing: An Exploration.
In Proceedings of the 16th International Confer-
ence on Computational Linguistics, pages 340–345,
Stroudsburg. Association for Computational Lin-
guistics.

Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15–28, March.

Chris Fournier. 2013. Evaluating Text Segmentation
using Boundary Edit Distance. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, pages 1702–1712, Sofia. As-
sociation for Computational Linguistics.

Jan Hajič, Eva Hajičová, Jarmila Panevová, Petr Sgall,
Silvie Cinková, Eva Fučı́ková, Marie Mikulová, Petr
Pajas, Jan Popelka, Jiřı́ Semecký, Jana Šindlerová,
Jan Štěpánek, Josef Toman, Zdeňka Urešová, and
Zděněk Žabokrtský. 2012. Prague Czech-English
Dependency Treebank 2.0.

Jan Hajič. 2004. Complex Corpus Annotation: The
Prague Dependency Treebank. Jazykovedný ústav
Ľ. Štúra, SAV.

Britt Keson. 1998a. The Danish Morphosyntactically
Tagged PAROLE Corpus. Technical report, Danish
Society for Literature and Language, Copenhagen.

Britt Keson. 1998b. Vejledning til det danske morfo-
syntaktisk taggede PAROLE-korpus. Technical re-
port, Danish Society for Literature and Language,
Copenhagen.

Klaus Krippendorff. 1970. Estimating the Reliabil-
ity, Systematic Error and Random Error of Interval
Data. Educational and Psychological Measurement,
30(1):61–70, April.

Klaus Krippendorff. 2004. Content Analysis: An in-
troduction to its methodology. Sage Publications,
Thousand Oaks, 2nd edition.

Mohamed Maamouri, Ann Bies, and Seth Kulick.
2008. Enhancing the Arabic Treebank : A Collab-
orative Effort toward New Annotation Guidelines.
In Proceedings of the Sixth International Confer-
ence on Language Resources and Evaluation, pages
3192–3196. European Language Resources Associ-
ation.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.

Yann Mathet, Antoine Widlöcher, Karën Fort, Claire
François, Olivier Galibert, Cyril Grouin, Juliette
Kahn, Sophie Rosset, and Pierre Zweigenbaum.
2012. Manual Corpus Annotation : Giving Meaning
to the Evaluation Metrics. In Proceedings of COL-
ING 2012, pages 809–818, Mumbai.

Marie Mikulová and Jan Štěpánek. 2010. Ways of
Evaluation of the Annotators in Building the Prague
Czech-English Dependency Treebank. In Proceed-
ings of the Seventh International Conference on
Language Resources and Evaluation, pages 1836–
1839, Valletta. European Language Resources As-
sociation.

Roser Morante and Eduardo Blanco. 2012. *SEM
2012 Shared Task : Resolving the Scope and Fo-
cus of Negation. In The First Joint Conference on
Lexical and Computational Semantics, pages 265–
274, Montreal. Association for Computational Lin-
guistics.

Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05 : A Swedish Treebank with Phrase Struc-
ture and Dependency Annotation. In Proceedings
of the Fifth International Conference on Language
Resources and Evaluation.

Rebecca J. Passonneau and Bob Carpenter. 2013. The
Benefits of a Model of Annotation. In Proceedings
of the 7th Linguistic Annotation Workshop and In-
teroperability with Discourse, pages 187–195, Sofia.
Association for Computational Linguistics.

Rebecca J. Passonneau. 2006. Measuring Agreement
on Set-valued Items (MASI) for Semantic and Prag-
matic Annotation. In Proceedings of the Fifth In-
ternational Conference on Language Resources and
Evaluation, pages 831–836.

Marwa Ragheb and Markus Dickinson. 2013. Inter-
annotator Agreement for Dependency Annotation of
Learner Language. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 169–179, Atlanta.
Association for Computational Linguistics.

943



Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(4):365–380, December.

William A. Scott. 1955. Reliability of Content Anal-
ysis: The Case of Nominal Scale Coding. Public
Opinion Quarterly, 19(3):321–325, January.

Arne Skjærholt. 2013. Influence of preprocessing
on dependency syntax annotation: speed and agree-
ment. In Proceedings of the 7th Linguistic Annota-
tion Workshop and Interoperability with Discourse,
pages 28–32, Sofia. Association for Computational
Linguistics.

Per Erik Solberg, Arne Skjærholt, Lilja Øvrelid, Kristin
Hagen, and Janne Bondi Johannesen. 2014. The
Norwegian Dependency Treebank. In Proceed-
ings of the Ninth International Conference on Lan-
guage Resources and Evaluation, Reykjavik. Euro-
pean Language Resources Association.

Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating Dependency Parsing: Robust and
Heuristics-Free Cross-Annotation Evaluation. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
385–396, Edinburgh. Association for Computational
Linguistics.

Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012. Cross-Framework Evaluation for Statistical
Parsing. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 44–54, Avignon. As-
sociation for Computational Linguistics.

Kaizhong Zhang and Dennis Shasha. 1989. Simple
Fast Algorithms for the Editing Distance between
Trees and Related Problems. SIAM Journal on Com-
puting, 18(6):1245–1262, December.

944


