



















































A Simple End-to-End Question Answering Model for Product Information


Proceedings of the First Workshop on Economics and Natural Language Processing, pages 38–43
Melbourne, Australia, July 20, 2018. c©2018 Association for Computational Linguistics

38

A Simple End-to-End Question Answering Model for Product Information

Tuan Manh Lai
Adobe Research

tlai@adobe.com

Trung Bui
Adobe Research

bui@adobe.com

Sheng Li
Adobe Research

sheli@adobe.com

Nedim Lipka
Adobe Research

lipka@adobe.com

Abstract

When evaluating a potential product pur-
chase, customers may have many ques-
tions in mind. They want to get ade-
quate information to determine whether
the product of interest is worth their
money. In this paper we present a simple
deep learning model for answering ques-
tions regarding product facts and specifi-
cations. Given a question and a product
specification, the model outputs a score in-
dicating their relevance. To train and eval-
uate our proposed model, we collected a
dataset of 7,119 questions that are related
to 153 different products. Experimental
results demonstrate that — despite its sim-
plicity — the performance of our model is
shown to be comparable to a more com-
plex state-of-the-art baseline.

1 Introduction

Customers ask many questions before buying
products. Developing a general question answer-
ing system to assist customers is challenging, due
to the diversity of questions. In this paper, we fo-
cus on the task of answering questions regarding
product facts and specifications. We formalize the
task as follows: Given a question Q about a prod-
uct P and the list of specifications (s1, s2, ..., sM )
of P , the goal is to identify the specification that
is most relevant to Q. M is the number of spec-
ifications of P , and si is the ith specification of
P . In this formulation, the task is similar to the
answer selection problem (Rao et al., 2016; Bian
et al., 2017; Shen et al., 2017). ‘Answers’ shall be
individual product specifications in this case. Af-
ter identifying the most relevant specification, the
final response sentence is generated using prede-
fined templates (Cui et al., 2017). Figure 1 illus-

trates the overall process.
In this paper, we present a simple deep learn-

ing model for selecting the product specification
that is most relevant to a given question from a
set of candidate specifications. Given a question-
specification pair, the model will output a score in-
dicating their relevance. To train and evaluate our
model, we collected a dataset of 7,119 questions,
covering 153 different products. Despite its sim-
plicity, the performance of our model is shown to
be comparable to a more complex state-of-the-art
baseline.

2 Related Work

2.1 Answer Selection

Answer selection is an active research field and
has drawn a lot of attention. Given a question and
a set of candidate answers, the task is to identify
which of the candidates contains the correct an-
swer to the question. Two types of deep learn-
ing frameworks have been proposed for tackling
the answer selection problem. One is the Siamese
framework (Bromley et al., 1993) and the other is
the Compare-Aggregate framework (Wang et al.,
2017; Bian et al., 2017; Shen et al., 2017). In
the Siamese framework, the same encoder (e.g.,
a CNN or a RNN) is used to map each input sen-
tence to a vector representation individually. Af-
ter that, the final output is determined solely based
on the encoded vectors. There is no explicit in-
teraction between the sentences during the encod-
ing process. On the other hand, the Compare-
Aggregate framework aims to capture more in-
teractive features between sentences in consider-
ation, therefore typically has better performance
when evaluated on public datasets such as TrecQA
(Wang et al., 2007) and WikiQA (Yang et al.,
2015).



39

Figure 1: Answering questions regarding product facts and specifications

2.2 Customer Service Chatbot

The most closely related branches of work to
ours are probably customer service chatbots for
e-commerce websites. An example can be the
Shopbot 1 of eBay. Shopbot aims at helping con-
sumers narrow down the best deals from eBays
over a billion listings. The bot’s main focus is to
understand the user intent and then make person-
alized recommendations. Unlike Shopbot, here
we do not focus on making product recommen-
dations. Instead we aim to develop a model for
answering questions about product specifications.
Another example is the SuperAgent (Cui et al.,
2017), a powerful chatbot designed to improve on-
line shopping experience. Given a specific product
page and a customer question, SuperAgent selects
the best answer from multiple data sources such
as in-page product information, existing customer
questions & answers, and customer reviews of the
product. Even though SuperAgent has a compo-
nent for answering questions about product spec-
ifications, the novelties of our work are: 1) a new
simple deep learning model for answering ques-
tions about product facts and specifications 2) a
new method for collecting data to train and evalu-
ate our model.

3 Model Architecture

Given a question and a set of candidate specifica-
tions, the goal is to identify the most relevant spec-
ification. We aim to train a classifier that takes a
question and a specification name as input and pre-
dicts whether the specification is relevant to the
question. During inference, given a question, the
trained classifier is used to assign a score to every
candidate specification based on how relevant the
specification is. After that, the top-ranked specifi-
cation is selected.

1https://shopbot.ebay.com

A common trait of a number of recent state-of-
the-art methods for answer selection is the use of
the Compare-Aggregate architecture (Wang et al.,
2017; Bian et al., 2017; Shen et al., 2017). Under
this architecture, vector representations of smaller
units (such as words) of the input sentences are
compared. And then the comparison results are
aggregated (e.g., by a CNN or a RNN) to de-
termine the relationship of the input sentences.
Compared to Siamese models, most Compare-
Aggregate models are more complicated and can
capture more interactive features between input
sentences.

Our task of matching questions and product
specifications is similar to the answer selection
problem. “Answers” shall be individual product
specifications. However, in this case the name of
a specification is relatively short. Therefore, our
hypothesis is that a well-designed Siamese model
would perform as well as a more complicated
Compare-Aggregate model. The added complex-
ity of comparing vector representations of smaller
units may not be needed as the specification name
is already short and descriptive. To this end, we
propose a new Siamese model for tackling our
problem. We show the overall architecture of our
model in Figure 2. Given a question Q and a spec-
ification name S, the model calculates a score in-
dicating their relevance through the following lay-
ers.

Word Representation Layer. Using word
embeddings pre-trained with word2vec (Mikolov
et al., 2013) or GloVe (Pennington et al., 2014),
we transform Q and S into two sequences Qe =
[eQ1 , e

Q
2 , ..., e

Q
m] and Se = [eS1 , eS2 , ..., eSn ], where

eQi is the embedding of the i
th word of the ques-

tion and eSj is the embedding of the jth word of
the specification name. m and n are the lengths of
Q and S, respectively.

BiLSTM Layer. We use a bi-directional



40

Figure 2: Architecture of our model

LSTM (Hochreiter and Schmidhuber, 1997) to
obtain a context-aware vector representation for
each position of Q and S. We feed Qe and Se
individually into a parameter shared bi-directional
LSTM model. For the question Q:

qfi =
−−−−→
LSTM(qfi−1, e

Q
i ) i = 1, ...,m

qbi =
←−−−−
LSTM(qbi+1, e

Q
i ) i = m, ..., 1

where qfi is a vector representation of the
first i words in the question (i.e., [eQ1 , e

Q
2 , ..., e

Q
i ]),

qbi is a vector representing the context of the
last m − i + 1 words in the question (i.e.,
[eQm, eQm−1, ..., e

Q
i ]). Similarly, we use the same

bi-directional LSTM to encode S:

sfj =
−−−−→
LSTM(sfj−1, e

S
j ) j = 1, ..., n

sbj =
←−−−−
LSTM(sbj+1, eSj ) j = n, ..., 1

The context-aware representation at each po-
sition of Q or S is obtained by concatenating the
two corresponding output sequences from both
directions, i.e., qi = q

f
i || qbi and sj = s

f
j || sbj .

The final representations of the question and
the specification are generated by applying the
max-pooling operation on the context-aware
representations. We denote the final representa-
tion of the question as oQ and denote the final
representation of the answer as oS .

Comparison and Output Layers. Following
the approach mentioned in (Tai et al., 2015), two

feature vectors are calculated from the final en-
codings oQ and oS : (1) the absolute difference of
the two vectors |oQ − oS |; (2) the element-wise
multiplication of the two vectors |oQ � oS |. The
features are then concatenated and fed into a fully
connected layer and a softmax layer to produce the
final score indicating the probability that specifica-
tion S is relevant to question Q.

4 Data Collection

The dataset used for experiments is created using
Amazon Mechanical Turk (MTurk) 2, an online la-
bor market. MTurk connects requesters (people
who have works to be done) and workers (people
who work on tasks for money). Requesters can
post small tasks for workers to complete for a fee.
These small tasks are referred to as HITs or human
intelligence tasks. An example of a HIT is find-
ing objects in an image or transcribing an audio
file. Requesters have several options for ensuring
their HITs are completed in a high-quality man-
ner. Requesters have the opportunity to determine
whether to approve completed HITs before having
to pay for them. In addition, requesters can also
limit which workers are eligible to complete their
tasks based on certain criteria.

We crawled the information of products listed
in the Home Depot website 3. For each product,
we create HITs where workers are asked to write

2https://www.mturk.com
3https://www.homedepot.com



41

questions regarding the specifications of the prod-
uct. Figure 3 shows a sample HIT, including the
instructions, which are shown to every participated
worker. In this sample HIT, a question for the
specification “Product Height (in.)” can be “How
tall is this shredder?” or “What is the height of
this shredder?”. To work on the HITs, workers
are required to have a 98% HIT approval rate, a
minimum of 800 HITs approved, and be located
in the United States or Canada. The constraints
ensure that the participated workers can provide
good questions in English. The final dataset con-
sists of 7,119 question-specification pairs in to-
tal, covering 369 kinds of specifications extracted
from 153 products. Even though in this work we
focus on products listed in the Home Depot web-
site, the data collection process is applicable to
other popular e-commerce websites such as Ama-
zon whose product pages typically have a section
for product facts and specifications.

5 Experiments and Results

5.1 Training and Evaluation

We set up two different experimental settings. The
only difference between the two settings is the way
in which we split up the collected HomeDepot
dataset into training set, development set, and test
set:

1. We divide the dataset so that the test set has
no products in common with the training set
or the development set.

2. We divide the dataset so that the test set has
no specifications in common with the train-
ing set or the development set. This is dif-
ferent from the first setting, because two dif-
ferent products may have some specifications
in common. For example, a chair and a ta-
ble usually have a same specification called
‘Product Weight’.

In both settings, the proportions of the training
set, development set, and test set are roughly 80%,
10%, and 10% of the total questions, respectively.

During training, the objective is to minimize the
cross entropy of all question-specification pairs in
the training set:

loss(θ) = −log
∏
i

pθ(y
(i)|Q(i), S(i))

where Q(i) and S(i) represent a question-
specification pair in the training set, y(i) indi-
cates whether specification S(i) is relevant to ques-
tion Q(i), and pθ is the predicted probability with
model weights θ. We use all possible question-
specification pairs for training. In other words, if
there are k questions about a product and the prod-
uct has h specifications, there are h × k question-
specification examples related to the product, and
exactly k of them are positive examples. During
testing, for every question about a product, we sort
the specifications of the product in descending or-
der based on the predicted probability of being rel-
evant. After that, we calculate the precision at 1
(P@1), precision at 1 (P@2), and precision at 3
(P@3) of our model.

We compare the performance of our model with
the unigram model mentioned in (Yu et al., 2014)
and the IWAN model proposed in (Shen et al.,
2017). The unigram model is a simple bag-of-
words model. It first generates a vector represen-
tation for each input sentence by summing over
the embeddings of all words in the sentence. The
final output is then determined based on the gen-
erated vector representations. The unigram model
is less complicated than our model. On the other
hand, the IWAN model belongs to the Compare-
Aggregate framework, and it is more sophisti-
cated than our model. In addition to comparing
between the fine-grained word representations of
the input sentences, the IWAN model also has
an inter-weighted layer for evaluating the impor-
tance of each word in each sentence. The IWAN
model currently achieves state-of-the-art perfor-
mance on public datasets such as TrecQA (Wang
et al., 2007) and WikiQA (Yang et al., 2015).

We make use of the GloVe word embeddings
(Pennington et al., 2014) when training the mod-
els. We did try the word2vec word embeddings
(Mikolov et al., 2013), however they gave worse
performances than GloVe. We tune the hyper-
parameters of each model using the development
set.

5.2 Results

Table 1 shows the performances of all the mod-
els in the first setting. Table 2 shows the perfor-
mances of all the models in the second setting.
The IWAN model and our model clearly outper-
form the unigram model. In addition, in both set-
tings, our model’s performance is comparable to



42

Figure 3: An example of a HIT

Model P@1 P@2 P@3

Unigram 0.802 0.904 0.927

IWAN 0.852 0.927 0.964

Our model 0.850 0.930 0.964

Table 1: Test results in the setting where the
test set has no product in common with the
training set or the development set

Model P@1 P@2 P@3

Unigram 0.399 0.529 0.627

IWAN 0.525 0.661 0.789

Our model 0.563 0.640 0.759

Table 2: Test results in the setting where the
test set has no specification in common with
the training set or the development set

the performance of the IWAN model despite be-
ing much simpler. We measured the speeds of our
model and the IWAN model. Our proposed model
is about 8% faster than the IWAN model. In ad-
dition, we see that all models perform worse in
the second setting than the first setting. This may
due to the fact that in the first setting two different
products in the train set and the test set may still
have many specifications in common (e.g., a LG
TV and a Samsung TV).

6 Conclusion

In this work we explore the task of answering
questions related to product facts and specifica-
tions. We propose a new, simple deep learning
model for tackling the task. To train and evalu-
ate the model, we collected a dataset of question-
specification pairs using MTurk. Experimental re-

sults show that our model’s performance is compa-
rable to a state-of-the-art baseline despite having
less complexity. Our proposed model takes less
time for training and inference than the state-of-
the-art baseline.

Recently, researchers collected a large volume
of community question answering data and a large
volume of product reviews from the Amazon web-
site (McAuley and Yang, 2016). In the future, we
plan to investigate transfer learning techniques to
utilize this large dataset for improving the perfor-
mance of our proposed model.

References

Weijie Bian, Si Li, Zhao Yang, Guang Chen, and
Zhiqing Lin. 2017. A compare-aggregate model
with dynamic-clip attention for answer selection. In
CIKM.



43

Jane Bromley, James W. Bentz, Léon Bottou, Is-
abelle Guyon, Yann LeCun, Cliff Moore, Eduard
Säckinger, and Roopak Shah. 1993. Signature ver-
ification using a ”siamese” time delay neural net-
work. IJPRAI, 7:669–688.

Lei Cui, Shaohan Huang, Furu Wei, Chuanqi Tan,
Chaoqun Duan, and Ming Zhou. 2017. Superagent:
A customer service chatbot for e-commerce web-
sites. In ACL.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9 8:1735–
80.

Julian McAuley and Alex Yang. 2016. Addressing
complex and subjective product-related queries with
customer reviews. In WWW.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of the 26th International Con-
ference on Neural Information Processing Systems -
Volume 2, NIPS’13, pages 3111–3119, USA. Curran
Associates Inc.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In EMNLP.

Jinfeng Rao, Hua He, and Jimmy J. Lin. 2016. Noise-
contrastive estimation for answer selection with
deep neural networks. In CIKM.

Gehui Shen, Yunlun Yang, and Zhi-Hong Deng. 2017.
Inter-weighted alignment network for sentence pair
modeling. In EMNLP.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. CoRR, abs/1503.00075.

Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the jeopardy model? a quasi-
synchronous grammar for qa. In EMNLP-CoNLL.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences. In IJCAI.

Yi Yang, Scott Wen-tau Yih, and Chris Meek. 2015.
Wikiqa: A challenge dataset for open-domain ques-
tion answering. ACL Association for Computa-
tional Linguistics.

Lei Yu, Karl Moritz Hermann, Phil Blunsom, and
Stephen G. Pulman. 2014. Deep learning for answer
sentence selection. CoRR, abs/1412.1632.

http://dl.acm.org/citation.cfm?id=2999792.2999959
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://dl.acm.org/citation.cfm?id=2999792.2999959
http://arxiv.org/abs/1503.00075
http://arxiv.org/abs/1503.00075
http://arxiv.org/abs/1503.00075
https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/
https://www.microsoft.com/en-us/research/publication/wikiqa-a-challenge-dataset-for-open-domain-question-answering/

