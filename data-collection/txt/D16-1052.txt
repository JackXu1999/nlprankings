



















































Solving Verbal Questions in IQ Test by Knowledge-Powered Word Embedding


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 541–550,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Solving Verbal Questions in IQ Test by Knowledge-Powered Word
Embedding

Huazheng Wang
University of Virginia
hw7ww@virginia.edu

Fei Tian
Microsoft Research

fetia@microsoft.com

Bin Gao
Microsoft

bingao@microsoft.com

Chengjieren Zhu
University of California, San Diego

chz191@ucsd.edu

Jiang Bian
Yidian Inc.

jiang.bian.prc@gmail.com

Tie-Yan Liu
Microsoft Research

tyliu@microsoft.com

Abstract

Verbal comprehension questions appear very
frequently in Intelligence Quotient (IQ) tests,
which measure human’s verbal ability includ-
ing the understanding of the words with mul-
tiple senses, the synonyms and antonyms, and
the analogies among words. In this work,
we explore whether such tests can be solved
automatically by the deep learning technolo-
gies for text data. We found that the task
was quite challenging, and simply applying
existing technologies like word embedding
could not achieve a good performance, due
to the multiple senses of words and the com-
plex relations among words. To tackle these
challenges, we propose a novel framework to
automatically solve the verbal IQ questions
by leveraging improved word embedding by
jointly considering the multi-sense nature of
words and the relational information among
words. Experimental results have shown that
the proposed framework can not only outper-
form existing methods for solving verbal com-
prehension questions but also exceed the aver-
age performance of the Amazon Mechanical
Turk workers involved in the study.

1 Introduction

The Intelligence Quotient (IQ) test (Stern, 1914)
is a test of intelligence designed to formally study
the success of an individual in adapting to a spe-
cific situation under certain conditions. Common
IQ tests measure various types of abilities such as
verbal, mathematical, logical, and reasoning skills.
These tests have been widely used in the study of
psychology, education, and career development. In

the community of artificial intelligence, agents have
been invented to fulfill many interesting and chal-
lenging tasks like face recognition, speech recogni-
tion, handwriting recognition, and question answer-
ing. However, as far as we know, there are very lim-
ited studies of developing an agent to solve IQ tests,
which in some sense is more challenging, since even
common human beings do not always succeed on
such tests. Considering that IQ test scores have been
widely considered as a measure of intelligence, we
think it is worth further investigating whether we can
develop an agent that can solve IQ test questions.

The commonly used IQ tests contain several types
of questions like verbal, mathematical, logical, and
picture questions, among which a large proportion
(near 40%) are verbal questions (Carter, 2005). The
recent progress on deep learning for natural lan-
guage processing (NLP), such as word embedding
technologies, has advanced the ability of machines
(or AI agents) to understand the meaning of words
and the relations among words. This inspires us
to solve the verbal questions in IQ tests by lever-
aging the word embedding technologies. However,
our attempts show that a straightforward applica-
tion of word embedding does not result in satisfac-
tory performances. This is actually understandable.
Standard word embedding technologies learn one
embedding vector for each word based on the co-
occurrence information in a text corpus. However,
verbal comprehension questions in IQ tests usually
consider the multiple senses of a word (and often fo-
cus on the rare senses), and the complex relations
among (polysemous) words. This has clearly ex-
ceeded the capability of standard word embedding

541



technologies.
To tackle the aforementioned challenges, we pro-

pose a novel framework that consists of three com-
ponents.

First, we build a classifier to recognize the spe-
cific type (e.g., analogy, classification, synonym,
and antonym) of verbal questions. For different
types of questions, different kinds of relationships
need to be considered and the solvers could have
different forms. Therefore, with an effective ques-
tion type classifier, we may solve the questions in a
divide-and-conquer manner.

Second, we obtain distributed representations of
words and relations by leveraging a novel word em-
bedding method that considers the multi-sense na-
ture of words and the relational knowledge among
words (or their senses) contained in dictionaries. In
particular, for each polysemous word, we retrieve
its number of senses from a dictionary, and con-
duct clustering on all its context windows in the
corpus. Then we attach the example sentences for
every sense in the dictionary to the clusters, such
that we can tag the polysemous word in each con-
text window with a specific word sense. On top
of this, instead of learning one embedding vector
for each word, we learn one vector for each pair
of word-sense. Furthermore, in addition to learning
the embedding vectors for words, we also learn the
embedding vectors for relations (e.g., synonym and
antonym) at the same time, by incorporating rela-
tional knowledge into the objective function of the
word embedding learning algorithm. That is, the
learning of word-sense representations and relation
representations interacts with each other, such that
the relational knowledge obtained from dictionaries
is effectively incorporated.

Third, for each type of question, we propose a
specific solver based on the obtained distributed
word-sense representations and relation represen-
tations. For example, for analogy questions, we
find the answer by minimizing the distance between
word-sense pairs in the question and the word-sense
pairs in the candidate answers.

We have conducted experiments using a com-
bined IQ test set to test the performance of our pro-
posed framework. The experimental results show
that our method can outperform several baseline
methods for verbal comprehension questions on IQ

tests. We further deliver the questions in the test
set to human beings through Amazon Mechanical
Turk1. The average performance of the human be-
ings is even a little lower than that of our proposed
method.

2 Related Work

2.1 Verbal Questions in IQ Test
In common IQ tests, a large proportion of ques-
tions are verbal comprehension questions, which
play an important role in deciding the final IQ
scores. For example, in Wechsler Adult Intelligence
Scale (Wechsler, 2008), which is among the most fa-
mous IQ test systems, the full-scale IQ is calculated
from two IQ scores: Verbal IQ and Performance IQ,
and around 40% of questions in a typical test are ver-
bal comprehension questions. Verbal questions can
test not only the verbal ability (e.g., understanding
polysemy of a word), but also the reasoning abil-
ity and induction ability of an individual. Accord-
ing to previous studies (Carter, 2005), verbal ques-
tions mainly have the types elaborated in Table 1,
in which the correct answers are highlighted in bold
font.

Analogy-I questions usually take the form “A is
to B as C is to ?”. One needs to choose a word
D from a given list of candidate words to form an
analogical relation between pair (A, B) and pair (C,
D). Such questions test the ability of identifying
an implicit relation from word pair (A, B) and ap-
ply it to compose word pair (C, D). Note that the
Analogy-I questions are also used as a major eval-
uation task in the word2vec models (Mikolov et al.,
2013). Analogy-II questions require two words to
be identified from two given lists in order to form an
analogical relation like “A is to ? as C is to ?”. Such
questions are a bit more difficult than the Analogy-
I questions since the analogical relation cannot be
observed directly from the questions, but need to be
searched for in the word pair combinations from the
candidate answers. Classification questions require
one to identify the word that is different (or dissim-
ilar) from others in a given word list. Such ques-
tions are also known as odd-one-out, which have
been studied in (Pintér et al., 2012). Classification
questions test the ability to summarize the majority

1http://www.mturk.com/

542



Type Example
Analogy-I Isotherm is to temperature as isobar is to? (i) atmosphere, (ii) wind, (iii) pressure, (iv) latitude, (v) current.
Analogy-II Identify two words (one from each set of brackets) that form a connection (analogy)

when paired with the words in capitals: CHAPTER (book, verse, read), ACT (stage, audience, play).
Classification Which is the odd one out? (i) calm, (ii) quiet, (iii) relaxed, (iv) serene, (v) unruffled.

Synonym Which word is closest to IRRATIONAL? (i)intransigent, (ii) irredeemable, (iii) unsafe, (iv) lost, (v) nonsensical.
Antonym Which word is most opposite to MUSICAL? (i) discordant, (ii) loud, (iii) lyrical, (iv) verbal, (v) euphonious.

Table 1: Types of verbal questions.

sense of the words and identify the outlier. Synonym
questions require one to pick one word out of a list of
words such that it has the closest meaning to a given
word. Synonym questions test the ability of identi-
fying all senses of the candidate words and selecting
the correct sense that can form a synonymous rela-
tion to the given word. Antonym questions require
one to pick one word out of a list of words such
that it has the opposite meaning to a given word.
Antonym questions test the ability of identifying all
senses of the candidate words and selecting the cor-
rect sense that can form an antonymous relation to
the given word. (Turney, 2008; Turney, 2011) stud-
ied the analogy, synonym and antonym problem us-
ing a supervised classification approach.

Although there are some efforts to solve math-
ematical, logical, and picture questions in IQ
test (Sanghi and Dowe, 2003; Strannegard et al.,
2012; Kushmany et al., 2014; Seo et al., 2014; Hos-
seini et al., 2014; Weston et al., 2015), there have
been very few efforts to develop automatic methods
to solve verbal questions.

2.2 Deep Learning for Text Mining

Building distributed word representations (Bengio et
al., 2003), a.k.a. word embeddings, has attracted
increasing attention in the area of machine learn-
ing. Different from conventional one-hot represen-
tations of studies or distributional word representa-
tions based on co-occurrence matrix between words
such as LSA (Dumais et al., 1988) and LDA (Blei
et al., 2003), distributed word representations are
usually low-dimensional dense vectors trained with
neural networks by maximizing the likelihood of a
text corpus. Recently, a series of works applied deep
learning techniques to learn high-quality word rep-
resentations (Collobert and Weston, 2008; Mikolov
et al., 2013; Pennington et al., 2014).

Nevertheless, since the above works learn word
representations mainly based on the word co-

occurrence information, it is quite difficult to obtain
high quality embeddings for those words with very
little context information; on the other hand, a large
amount of noisy or biased context could give rise to
ineffective word embeddings. Therefore, it is neces-
sary to introduce extra knowledge into the learning
process to regularize the quality of word embedding.
Some efforts have paid attention to learn word em-
bedding in order to address knowledge base comple-
tion and enhancement (Bordes et al., 2011; Socher
et al., 2013; Weston et al., 2013a), and some other
efforts have tried to leverage knowledge to enhance
word representations (Luong et al., 2013; Weston et
al., 2013b; Fried and Duh, 2014; Celikyilmaz et al.,
2015). Moreover, all the above models assume that
one word has only one embedding no matter whether
the word is polysemous or not, which might cause
some confusion for the polysemous words. To solve
the problem, there are several efforts like (Huang
et al., 2012; Tian et al., 2014; Neelakantan et al.,
2014). However, these models do not leverage any
extra knowledge (e.g., relational knowledge) to en-
hance word representations.

3 Solving Verbal Questions

In this section, we introduce our proposed frame-
work to solve the verbal questions, which consists
of the following three components.

3.1 Classification of Question Types

The first component of the framework is a question
classifier, which identifies different types of verbal
questions. Since different types of questions have
their unique ways of expression, the classification
task is relatively easy, and we therefore take a simple
approach to fulfill the task. Specifically, we regard
each verbal question as a short document and use
the TF·IDF features to build its representation. Then
we train an SVM classifier with linear kernel on a
portion of labeled question data, and apply it to other

543



questions. The question labels include Analogy-I,
Analogy-II, Classification, Synonym, and Antonym.
We use the one-vs-rest training strategy to obtain a
linear SVM classifier for each question type.

3.2 Embedding of Word-Senses and Relations

The second component of our framework leverages
deep learning technologies to learn distributed rep-
resentations for words (i.e. word embedding). Note
that in the context of verbal question answering, we
have some specific requirements on this learning
process. Verbal questions in IQ tests usually con-
sider the multiple senses of a word (and focus on
the rare senses), and the complex relations among
(polysemous) words, such as synonym and antonym
relation. Figure 1 shows an example of the multi-
sense of words and the relations among word senses.
We can see that irrational has three senses. Its first
sense has an antonym relation with the second sense
of rational, while its second sense has a synonym
relation with nonsensical and an antonym relation
with the first sense of rational.

The above challenge has exceeded the capability
of standard word embedding technologies. To ad-
dress this problem, we propose a novel approach
that considers the multi-sense nature of words and
integrate the relational knowledge among words (or
their senses) into the learning process. In particu-
lar, our approach consists of two steps. The first
step aims at labeling a word in the text corpus with
its specific sense, and the second step employs both
the labeled text corpus and the relational knowledge
contained in dictionaries to simultaneously learn
embeddings for both word-sense pairs and relations.

3.2.1 Multi-Sense Identification
First, we learn a single-sense word embedding by

using the skip-gram method in word2vec (Mikolov
et al., 2013).

Second, we gather the context windows of all oc-
currences of a word used in the skip-gram model,
and represent each context by a weighted average
of the pre-learned embedding vectors of the con-
text words. We use TF·IDF to define the weight-
ing function, where we regard each context win-
dow of the word as a short document to calcu-
late the document frequency. Specifically, for a
word w0, each of its context window can be de-

noted by (w−N , · · · , w0, · · · , wN ). Then we repre-
sent the window by calculating the weighted average
of the pre-learned embedding vectors of the context
words as ξ = 12N

∑N
i=−N,i6=0 gwivwi ,where gwi is

the TF·IDF score of wi, and vwi is the pre-learned
embedding vector of wi. After that, for each word,
we use spherical k-means to cluster all its context
representations, where cluster number k is set as the
number of senses of this word in the online dictio-
nary.

Third, we match each cluster to the correspond-
ing sense in the dictionary. On one hand, we repre-
sent each cluster by the average embedding vector
of all those context windows included in the clus-
ter. For example, suppose word w0 has k senses and
thus it has k clusters of context windows, we de-
note the average embedding vectors for these clus-
ters as ξ̄1, · · · , ξ̄k. On the other hand, since the on-
line dictionary uses some descriptions and example
sentences to interpret each word sense, we can rep-
resent each word sense by the average embedding
of those words including its description words and
the words in the corresponding example sentences.
Here, we assume the representation vectors (based
on the online dictionary) for the k senses of w0 are
ζ1, · · · , ζk. After that, we consecutively match each
cluster to its closest word sense in terms of the dis-
tance computed in the word embedding space:

(ξ̄i′ , ζj′) = argmin
i,j=1,··· ,k

d(ξ̄i, ζj), (1)

where d(·, ·) calculates the Euclidean distance and
(ξ̄i′ , ζj′) is the first matched pair of window cluster
and word sense. Here, we simply take a greedy strat-
egy. That is, we remove ξ̄i′ and ζj′ from the cluster
vector set and the sense vector set, and recursively
run (1) to find the next matched pair till all the pairs
are found. Finally, each word occurrence in the cor-
pus is relabeled by its associated word sense, which
will be used to learn the embeddings for word-sense
pairs in the next step.

3.2.2 Co-Learning Word-Sense Pair
Representations and Relation
Representations

After relabeling the text corpus, different occur-
rences of a polysemous word may correspond to
its different senses, or more accurately word-sense
pairs. We then learn the embeddings for word-

544



Irrational (sense 1)

adj. without power to reason 

Irrational (sense 2)

adj. unreasonable

Irrational (sense 3)

n. real number that cannot be expressed 
as the quotient of two integers

nonsensical (sense 1)

adj. foolish or absurd

Absurd (sense 1)

adj. against reason or common sense 

Absurd (sense 2)

adj. funny because clearly unsuitable, 
foolish, false, or impossible

Rational (sense 1)

adj. sensible

Rational (sense 2)

adj. able to reason Synonym relation
Antonym relation

Figure 1: An example on the multi-sense of words and the relations between word senses.

sense pairs and relations (obtained from dictionar-
ies, such as synonym and antonym) simultaneously,
by integrating relational knowledge into the objec-
tive function of the word embedding learning model
like skip-gram. We propose to use a function Er as
described below to capture the relational knowledge.

Specifically, the existing relational knowledge
extracted from dictionaries, such as synonym,
antonym, etc., can be naturally represented in the
form of a triplet (head, relation, tail) (denoted by
(hi, r, tj) ∈ S, where S is the set of relational
knowledge), which consists of two word-sense pairs
(i.e. word h with its i-th sense and word t with its
j-th sense), h, t ∈ W (W is the set of words) and a
relationship r ∈ R (R is the set of relationships). To
learn the relation representations, we make an as-
sumption that relationships between words can be
interpreted as translation operations and they can be
represented by vectors. The principle in this model
is that if the relationship (hi, r, tj) exists, the repre-
sentation of the word-sense pair tj should be close
to that of hi plus the representation vector of the re-
lationship r, i.e. hi + r; otherwise, hi + r should
be far away from tj . Note that this model learns
word-sense pair representations and relation repre-
sentations in a unified continuous embedding space.

According to the above principle, we define Er as
a margin-based regularization function over the set
of relational knowledge S,

Er =
∑

(hi,r,tj)∈S
(h

′
,r,t

′
)∈S′(hi,r,tj)

[
γ + d(hi + r, tj)− d(h

′
+ r, t

′
)
]
+
.

Here [X]+ = max(X, 0), γ > 0 is a margin hyper-
parameter, and d(·, ·) is the Euclidean distance be-
tween two words in the embedding space.The set of

corrupted triplets S
′
(h,r,t) is defined as S

′
(hi,r,tj)

=

{(h′ , r, t)}⋃{(h, r, t′)},which is constructed from
S by replacing either the head word-sense pair or the
tail word-sense pair by another randomly selected
word with its randomly selected sense.

To avoid the trivial solution that simply increases
the norms of representation vectors, we use an ad-
ditional soft norm constraint on the relation repre-
sentations as ri = 2σ(xi) − 1, where σ(·) is the
sigmoid function σ(xi) = 1/(1 + e−xi), ri is the
i-th dimension of relation vector r, and xi is a latent
variable, which guarantees that every dimension of
the relation representation vector is within the range
(−1, 1).

By combining the skip-gram objective function
and the regularization function derived from rela-
tional knowledge, we get the combined objective
Jr = αEr − L that incorporates relational knowl-
edge into the word-sense pair embedding calcula-
tion process, where α is the combination coefficient.
Our goal is to minimize Jr, which can be optimized
using back propagation neural networks. Figure 2
shows the structure of the proposed model.By using
this model, we can obtain the distributed representa-
tions for both word-sense pairs and relations simul-
taneously.

3.3 Solvers for Each Type of Questions

3.3.1 Analogy-I
For the Analogy-I questions like “A is to B as C

is to ?”, we answer them by optimizing:

D = argmax
ib,ia,ic,id′ ;

D′∈T

cos(v(B,ib) − v(A,ia) + v(C,ic), v(D′,id′ ))

(2)

545



Embedding of 

softmax softmax softmax softmax

…

…

…

…

Embedding of Embedding of 

Loss of relation 

Figure 2: The structure of the proposed model.

where T contains all the candidate answers, cos
means cosine similarity, and ib, ia, ic, id′ are the in-
dexes for the word senses of B,A,C,D′ respec-
tively. Finally D is selected as the answer.

3.3.2 Analogy-II
As the form of the Analogy-II questions is like

“A is to ? as C is to ?” with two lists of candidate
answers, we can apply an optimization method as
below to select the best (B,D) pair,

argmax
ib′ ,ia,ic,id′ ;
B′∈T1,D′∈T2

cos(v(B′,ib′ ) − v(A,ia) + v(C,ic), v(D′,id′ )),

(3)

where T1, T2 are two lists of candidate words. Thus
we get the answers B and D that can form an ana-
logical relation between word pair (A, B) and word
pair (C,D) under a certain specific word sense com-
bination.

3.3.3 Classification
For the Classification questions, we leverage the

property that words with similar co-occurrence in-
formation are distributed close to each other in
the embedding space. The candidate word that
is not similar to others does not have similar co-
occurrence information to other words in the train-
ing corpus, and thus this word should be far away
from other words in the word embedding space.
Therefore we first calculate a group of mean vec-
tors miw1 ,··· ,iwN of all the candidate words with
any possible word senses as below miw1 ,··· ,iwN =
1
N

∑
wj∈T v(wj ,iwj ), where T is the set of candidate

words, N is the capacity of T , wj is a word in T ;
iwj (j = 1, · · · , N ; iwj = 1, · · · , kwj ) is the index
for the word senses of wj , and kwj (j = 1, · · · , N)
is the number of word senses of wj . Therefore, the

number of the mean vectors is M =
∏N

j=1 kwj .
As both N and kwj are very small, the computation
cost is acceptable. Then, we choose the word with
such a sense that its closest sense to the correspond-
ing mean vector is the largest among the candidate
words as the answer, i.e.,

w = argmax
wj∈T

min
iwj ;l=1,··· ,M

d(v(wj ,iwj ),ml). (4)

3.3.4 Synonym
For the Synonym questions, we empirically ex-

plored two solvers. For the first solver, we also
leverage the property that words with similar co-
occurrence information are located closely in the
word embedding space. Therefore, given the ques-
tion word wq and the candidate words wi, we can
find the answer by solving:

w = argmin
iwq ,iwj ;wj∈T

d(v(wj ,iwj ), v(wq,iwq )), (5)

where T is the set of candidate words. The sec-
ond solver is based on the minimization objective
of the translation distance between entities in the re-
lational knowledge model (2). Specifically, we cal-
culate the offset vector between the embedding of
question word wq and each word wj in the candi-
date list. Then, we set the answer w as the candidate
word with which the offset is the closest to the rep-
resentation vector of the synonym relation rs, i.e.,

w = argmin
iwq ,iwj ;wj∈T

∣∣|v(wj ,iwj ) − v(wq,iwq )| − rs
∣∣. (6)

In practice, we found the second solver performs
better (the results are listed in Section 4). For our
baseline embedding model skip-gram, since it does
not assume the relation representations explicitly,
we use the first solver for it.

3.3.5 Antonym
Similar to solving the Synonym questions, we ex-

plored two solvers for Antonym questions as well.
That is, the first solver (7) is based on the small
offset distance between semantically close words
whereas the second solver (8) leverages the trans-
lation distance between two words’ offset and the
embedding vector of the antonym relation. The first
solver is based on the fact that since an antonym and

546



its original word have similar co-occurrence infor-
mation from which the embedding vectors are de-
rived, the embedding vectors of both words with
antonym relation will still lie closely in the embed-
ding space.

w = argmin
iwq ,iwj ;wj∈T

d(v(wj ,iwj ), v(wq,iwq )), (7)

w = argmin
iwq ,iwj ;wj∈T

∣∣|v(wj ,iwj ) − v(wq,iwq )| − ra
∣∣, (8)

Here T is the set of candidate words and ra is the
representation vector of the antonym relation. Again
we found that the second solver performs better.
Similarly, for skip-gram, the first solver is applied.

4 Experiments

We conduct experiments to examine whether our
proposed framework can achieve satisfying results
on verbal comprehension questions.

4.1 Data Collection

4.1.1 Training Set for Word Embedding
We trained word embeddings on a publicly avail-

able text corpus named wiki20142, which is a large
text snapshot from Wikipedia. After being pre-
processed by removing all the html meta-data and
replacing the digit numbers by English words, the
final training corpus contains more than 3.4 billion
word tokens, and the number of unique words, i.e.
the vocabulary size, is about 2 million.

4.1.2 IQ Test Set
According to our study, there is no online dataset

specifically released for verbal comprehension ques-
tions, although there are many online IQ tests for
users to play with. In addition, most of the on-
line tests only calculate the final IQ scores but do
not provide the correct answers. Therefore, we only
use the online questions to train the verbal question
classifier described in Section 3.1. Specifically, we
manually collected and labeled 30 verbal questions
from the online IQ test Websites3 for each of the
five types (i.e. Analogy-I, Analogy-II, Classifica-
tion, Synonym, and Antonym) and trained an one-

2http://en.wikipedia.org/wiki/Wikipedia:
Database_download

3http://wechsleradultintelligencescale.
com/

vs-rest SVM classifier for each type. The total accu-
racy on the training set itself is 95.0%. The classifier
was then applied in the test set below.

We collected a set of verbal comprehension ques-
tions associated with correct answers from pub-
lished IQ test books, such as (Carter, 2005; Carter,
2007; Pape, 1993; Ken Russell, 2002), and we used
this collection as the test set to evaluate the ef-
fectiveness of our new framework. In total, this
test set contains 232 questions with the correspond-
ing answers.4 The number of each question type
(i.e., Analogy-I, Analogy-II, Classification, Syn-
onym, Antonym) are respectively 50, 29, 53, 51, 49.

4.2 Compared Methods

In our experiments, we compare our new relation
knowledge powered model to several baselines.

Random Guess Model (RG). Random guess is
the most straightforward way for an agent to solve
questions. In our experiments, we used a random
guess agent which would select an answer randomly
regardless of what the question was. To measure the
performance of random guess, we ran each task for
5 times and calculated the average accuracy.

Human Performance (HP). Since IQ tests are
designed to evaluate human intelligence, it is quite
natural to leverage human performance as a base-
line. To collect human answers on the test ques-
tions, we delivered them to human beings through
Amazon Mechanical Turk (AMT), a crowd-sourcing
Internet marketplace that allows people to partici-
pate in Human Intelligence Tasks. In our study, we
published five AMT jobs, one job corresponding to
one specific question type. The jobs were deliv-
ered to 200 people. To control the quality of the
collected results, we used several strategies: (i) we
imposed high restrictions on the workers by requir-
ing all the workers to be native English speakers in
North America and to be AMT Masters (who have
demonstrated high accuracy on previous tasks on
AMT marketplace); (ii) we recruited a large number
of workers in order to guarantee the statistical confi-
dence in their performances; (iii) we tracked their
age distribution and education background, which

4It can be downloaded from https://www.dropbox.
com/s/o0very1gwv3mrt5/VerbalQuestions.zip?
dl=0.

547



are very similar to those of the overall population
in the U.S.

Latent Dirichlet Allocation Model (LDA). This
baseline model leveraged one of the most common
classical distributional word representations, i.e. La-
tent Dirichlet Allocation (LDA) (Blei et al., 2003).
In particular, we trained word representations using
LDA on wiki2014 with the topic number 1000.

Skip-Gram Model (SG). In this baseline, we
applied the word embedding trained by skip-
gram (Mikolov et al., 2013) (denoted by SG-1) on
wiki2014. In particular, we set the window size as 5,
the embedding dimension as 500, the negative sam-
pling count as 3, and the epoch number as 3. In ad-
dition, we also employed a pre-trained word embed-
ding by Google5 with the dimension of 300 (denoted
by SG-2).

Glove. Another powerful word embedding
model (Pennington et al., 2014). Glove configura-
tions are the same as those in running SG-1.

Multi-Sense Model (MS). In this baseline, we
applied the multi-sense word embedding models
proposed in (Huang et al., 2012; Tian et al., 2014;
Neelakantan et al., 2014) (denoted by MS-1, MS-2
and MS-3 respectively). For MS-1, we directly used
the published multi-sense word embedding vectors
by the authors6, in which they set 10 senses for the
top 5% most frequent words. For MS-2 and MS-
3, we get the embedding vectors by usingf the re-
leased codes from the authors using the same con-
figurations as MS-1.

Relation Knowledge Powered Model (RK).
This is our proposed method in Section 3. In par-
ticular, when learning the embedding on wiki2014,
we set the window size as 5, the embedding dimen-
sion as 500, the negative sampling count as 3, and
the epoch number as 3. We adopted the online Long-
man Dictionary as the dictionary used in multi-sense
clustering. We used a public relation knowledge set,
WordRep (Gao et al., 2014), for relation training.

4.3 Experimental Results

4.3.1 Accuracy of Question Classifier
We applied the question classifier trained in Sec-

tion 4.1.2 on the test set, and got the total accuracy

5https://code.google.com/p/word2vec/
6http://ai.stanford.edu/˜ehhuang/

93.1%. For RG and HP, the question classifier was
not needed. For other methods, the wrongly classi-
fied questions were also sent to the corresponding
wrong solver to find an answer. If the solver re-
turned an empty result (which was usually caused
by invalid input format, e.g., an Analogy-II question
was wrongly input to the Classification solver), we
would randomly select an answer.

4.3.2 Overall Accuracy
Table 2 demonstrates the accuracy of answering

verbal questions by using all the approaches men-
tioned in Section 4.2. The numbers for all the mod-
els are mean values from five repeated runs. From
this table, we observe: (i) RK can achieve the best
overall accuracy than all the other methods. In par-
ticular, RK can raise the overall accuracy by about
4.63% over HP7. (ii) RK is empirically superior to
the skip-gram models SG-1/SG-2 and Glove. Ac-
cording to our understanding, the improvement of
RK over SG-1/SG-2/Glove comes from two aspects:
multi-sense and relational knowledge. Note that the
performance difference between MS-1/MS-2/MS-3
and SG-1/SG-2/Glove is not significant, showing
that simply changing single-sense word embedding
to multi-sense word embedding does not bring too
much benefit. One reason is that the rare word-
senses do not have enough training data (contextual
information) to produce high-quality word embed-
ding. By further introducing the relational knowl-
edge among word-senses, the training for rare word-
senses will be linked to the training of their related
word-senses. As a result, the embedding quality of
the rare word-senses will be improved. (iii) RK is
empirically superior than the two multi-sense algo-
rithms MS-1, MS-2 and MS-3, demonstrating the
effectiveness brought by adopting fewer model pa-
rameters and using an online dictionary in building
the multi-sense embedding model.

These results are quite impressive, indicating the
potential of using machines to comprehend human
knowledge and even achieve a comparable level of
human intelligence.

4.3.3 Accuracy on Different Question Types
Table 2 reports the accuracy of answering various

types of verbal questions by each method. From the
7With the t-test score p = 0.036.

548



Analogy-I Analogy-II Classification Synonym Antonym Total
RG 24.60 11.72 20.75 19.27 23.13 20.51

LDA 28.00 13.79 39.62 27.45 30.61 29.31
HP 45.87 34.37 47.23 50.38 53.30 46.23
SG

SG-1 38.00 24.14 37.74 45.10 40.82 38.36
SG-2 38.00 20.69 39.62 47.06 44.90 39.66
Glove 45.09 24.14 32.08 47.06 40.82 39.03
MS

MS-1 36.36 19.05 41.30 50.00 36.59 38.67
MS-2 40.00 20.69 41.51 49.02 40.82 40.09
MS-3 17.65 20.69 47.17 47.06 30.61 36.73
RK 48.00 34.48 52.83 60.78 51.02 50.86

Table 2: Accuracy of different methods among different human groups.

table, we can observe that the SG and MS models
can achieve competitive accuracy on certain ques-
tion types (like Synonym) compared with HP. After
incorporating knowledge into learning word embed-
ding, our RK model can improve the accuracy over
all question types. Moreover, the table shows that
RK can result in a big improvement over HP on the
question types of Synonym and Classification.

To sum up, the experimental results have demon-
strated the effectiveness of the proposed RK model
compared with several baseline methods. Although
the test set is not large, the generalization of RK to
other test sets should not be a concern due to the un-
supervised nature of our model.

5 Conclusions

We investigated how to automatically solve verbal
comprehension questions in IQ Tests by using word
embedding techniques. In particular, we proposed
a three-step framework: (i) to recognize the spe-
cific type of a verbal comprehension question by
a classifier, (ii) to leverage a novel deep learning
model to co-learn the representations of both word-
sense pairs and relations among words (or their
senses), (iii) to design dedicated solvers, based on
the obtained word-sense pair representations and re-
lation representations, for addressing each type of
questions. Experimental results have demonstrated
that this novel framework can achieve better per-
formance than existing methods for solving verbal
comprehension questions and even exceed the aver-
age performance of the Amazon Mechanical Turk
workers involved in the experiments.

References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and

Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137–1155.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.

Antoine Bordes, Jason Weston, Ronan Collobert, Yoshua
Bengio, et al. 2011. Learning structured embeddings
of knowledge bases. In AAAI.

Philip Carter. 2005. The complete book of intelligence
tests. John Wiley & Sons Ltd.

Philip Carter. 2007. The Ultimate IQ Test Book: 1,000
Practice Test Questions to Boost Your Brain Power.
Kogan Page Publishers.

Asli Celikyilmaz, Dilek Hakkani-Tur, Panupong Pasu-
pat, and Ruhi Sarikaya. 2015. Enriching word em-
beddings using knowledge graph for semantic tagging
in conversational dialog systems. In Proceedings of
AAAI.

Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: Deep neu-
ral networks with multitask learning. In Proceedings
of ICML, pages 160–167. ACM.

Susan T Dumais, George W Furnas, Thomas K Landauer,
Scott Deerwester, and Richard Harshman. 1988. Us-
ing latent semantic analysis to improve access to tex-
tual information. In Proceedings of SIGCHI.

Daniel Fried and Kevin Duh. 2014. Incorporating both
distributional and relational semantics in word repre-
sentations. CoRR, abs/1412.4369.

Bin Gao, Jiang Bian, and Tie-Yan Liu. 2014. Wordrep:
A benchmark for research on learning word represen-
tations. arXiv preprint arXiv:1407.1640.

Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren
Etzioni, and Nate Kushman. 2014. Learning to solve
arithmetic word problems with verb categorization. In
Proceedings of EMNLP, pages 523–533.

549



Eric H Huang, Richard Socher, Christopher D Manning,
and Andrew Y Ng. 2012. Improving word representa-
tions via global context and multiple word prototypes.
In Association for Computational Linguistics (ACL),
pages 873–882.

Philip Carter Ken Russell. 2002. The Times Book of IQ
Tests. Kogan Page Limited.

Nate Kushmany, Yoav Artziz, Luke Zettlemoyerz, and
Regina Barzilayy. 2014. Learning to automatically
solve algebra word problems. In Proceedings of ACL.

Minh-Thang Luong, Richard Socher, and Christopher D
Manning. 2013. Better word representations with
recursive neural networks for morphology. CoNLL-
2013, 104.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositionality.
In NIPS, pages 3111–3119.

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of EMNLP,
pages 1059–1069, Doha, Qatar, October. Association
for Computational Linguistics.

Dan Pape. 1993. The Original Cambridge Self Scoring
IQ Test. The Magni Group, Inc.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word rep-
resentation. Proceedings of EMNLP, 12:1532–1543.

Balázs Pintér, Gyula Vörös, Zoltán Szabó, and András
Lörincz. 2012. Automated word puzzle generation
via topic dictionaries. CoRR, abs/1206.0377.

Pritika Sanghi and David Dowe. 2003. A computer pro-
gram capable of passing i.q. tests. In Proceedings of
the Joint International Conference on Cognitive Sci-
ence.

Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, and
Oren Etzioni. 2014. Diagram understanding in geom-
etry questions. In Proceedings of the Twenty-Eighth
AAAI Conference on Artificial Intelligence, July 27 -
31, 2014, Québec City, Québec, Canada., pages 2831–
2838.

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural tensor
networks for knowledge base completion. In NIPS,
pages 926–934.

William Stern. 1914. The Psychological Methods of
Testing Intelligence. Warwick & York.

Claes Strannegard, Mehrdad Amirghasemi, and Simon
Ulfsbacker. 2012. An anthropomorphic method for
number sequence problems. Cognitive Systems Re-
search.

Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilistic
model for learning multi-prototype word embeddings.
In Proceedings of COLING.

Peter D Turney. 2008. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proceed-
ings of the Coling 2008, pages 905–912.

Peter D Turney. 2011. Analogy perception applied
to seven tests of word comprehension. Journal of
Experimental & Theoretical Artificial Intelligence,
23(3):343–362.

David Wechsler. 2008. Wechsler adult intelligence
scale–fourth edition (wais–iv). San Antonio, TX: NCS
Pearson.

Jason Weston, Antoine Bordes, Oksana Yakhnenko, and
Nicolas Usunier. 2013a. Connecting language and
knowledge bases with embedding models for relation
extraction. arXiv preprint arXiv:1307.7973.

Jason Weston, Antoine Bordes, Oksana Yakhnenko, and
Nicolas Usunier. 2013b. Connecting language and
knowledge bases with embedding models for relation
extraction. In Proceedings of EMNLP.

Jason Weston, Antoine Bordes, Sumit Chopra, and
Tomas Mikolov. 2015. Towards ai-complete ques-
tion answering: a set of prerequisite toy tasks. arXiv
preprint arXiv:1502.05698.

550


