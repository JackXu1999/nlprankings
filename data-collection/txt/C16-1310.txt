



















































Corpus Fusion for Emotion Classification


Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers,
pages 3287–3297, Osaka, Japan, December 11-17 2016.

Corpus Fusion for Emotion Classification

Suyang Zhu1, Shoushan Li1∗, Ying Chen2, Guodong Zhou1
1Natural Language Processing Lab, School of Computer Science and Technology,

Soochow University, China
2College of Information and Electrical Engineering, China Agricultural University

syzhu@stu.suda.edu.cn, lishoushan@suda.edu.cn,
chenying@cau.edu.cn, gdzhou@suda.edu.cn

Abstract

Machine learning-based methods have obtained great progress on emotion classification. How-
ever, in most previous studies, the models are learned based on a single corpus which often suf-
fers from insufficient labeled data. In this paper, we propose a corpus fusion approach to address
emotion classification across two corpora which use different emotion taxonomies. The objective
of this approach is to utilize the annotated data from one corpus to help the emotion classifica-
tion on another corpus. An Integer Linear Programming (ILP) optimization is proposed to refine
the classification results. Empirical studies show the effectiveness of the proposed approach to
corpus fusion for emotion classification.

1 Introduction

Emotion classification aims to recognize human emotions, such as joy, anger or surprise in a given
text. Emotion classification has a variety of applications including online chatting (Galik and Rank,
2012), news classification (Liu et al., 2013) and stock marketing (Bollen et al., 2011). In recent years,
emotion classification in social media has been greatly popular in the Natural Language Processing
(NLP) community (Chen et al., 2010; Purver and Battersby, 2012; Li et al., 2015). Because of the
popularity of social media today, the analysis of short text on social media becomes more important
(Kiritchenko et al., 2014; Wen and Wan, 2014; Wang et al., 2015). Users express their feelings and
emotions on various social media platforms.

Existing emotion classification approaches are based on corpus classification methods where human-
annotated emotion corpora are leveraged to train a machine learning-based emotion classification models.
Recently, several different emotion corpora have been proposed by different researchers, such as Yao et
al. (2014) and Huang et al. (2015). However, the size of each existing labeled corpus might be rather
limited due to the high cost of data annotation, which results low performance in traditional supervised
emotion classification.

In this paper, we propose a novel task, namely corpus fusion for emotion classification, which aims
to leverage the data from different emotion corpora so as to alleviate the data deficiency problem. This
task is motivated by the fact that although the emotion corpora are from different resources, they have
the same objective of emotion classification. So it is easy to enlarge the size of the corpora by mixing the
data of the same emotion category from two corpora. However, corpus fusion for emotion classification
is challenging due to the following two factors:

First, the emotion taxonomies are often different between two emotion corpora because of the lack of
an accepted standard. As a result, similar instances which express similar or same emotion can be catego-
rized into different types of emotions under different taxonomies. An example from two emotion corpora
(Yao et al., 2014; Huang et al., 2015) in Figure 1 expresses this problem. These two instances which
express close emotion are labeled with different emotion classes under different emotion taxonomies.

Second, the annotation guidelines are often different between two emotion corpora because of different
annotators. For example, the instance from Yao et al. (2014) in Figure 1 which contains a positive
∗corresponding author

This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/

3287



emotion like may not be labeled as positive under the taxonomy of Huang et al. (2015) because it
doesn’t contain a strong emotional word to express such a positive emotion. The numbers of emotion
labels of each instance in two corpora are also different: some instances in Yao et al. (2014) have both
primary emotion and secondary emotion, while most of the instances in Huang et al. (2015) are labeled
with only one emotion. Because of the difference between emotion taxonomies, it is difficult to directly
use the data from different emotion corpora together.

Yao et al. (2014):

“我先是震惊，继而敬重，如同听到冯军哥的裸捐一样。”

(English Translation: “I was first shocked, and then respected. Just like when I heard the
giving pledge made by Feng Jun.”)

Emotion Categories: Like, Surprise
Huang et al. (2015):

“创意无处不在！令人感到震惊的街头3D艺术！”

(English Translation: “Creativity is everywhere! The shocking 3D street arts!”)

Emotion Categories: Neutral Complex

Figure 1: An example for two similar instances being categorized into different emotion categories under
two emotion taxonomies

In this paper, we propose a corpus fusion approach to leverage the two emotion corpora, i.e., Yao
et al. (2014) and Huang et al. (2015) in order to utilize the annotated data from each other. First, we
perform supervised emotion classification on two corpora. Second, we refine the predicted emotion
labels via a joint inference method, called Integer Linear Programming (ILP). A global objective function
is minimized with the obtained posterior probabilities of the test instances. Two types of constraints,
namely intra-corpus constraint and extra-corpus constraint are proposed in the ILP approach to address
two challenges mentioned above. We use extra-corpus constraint to overcome the first challenge, and
intra-corpus constraint is used for overcoming the second challenge. Results of experiments prove that
our model makes a promotion on both classification accuracy and F1-measure, and both intra-corpus
constraints and extra-corpus constraints are effective for the corpus fusion task.

The reminder of this paper is organized as follows. Section 2 overviews the related studies. Section 3
introduces two corpora used in this paper. Section 4 proposes the approach to corpus fusion for emotion
classification. Section 5 illustrates the experiments to evaluate the proposed approach. Section 6 gives
the conclusion and future work.

2 Related Work

In last decade, mainstream approaches for emotion analysis are corpus-based machine learning methods.
Several studies construct emotion corpus from social media platform such as blog, microblog and news
portal. Gilad (2005) collects blog texts from LiveJournal to construct an emotion corpus with 815,494
blog articles. Quan and Ren (2009) build an emotion corpus from blogs with eight types of emotions
on three granularity levels. Pak and Paroubek (2010) establish an emotion corpus by capturing tweets
on Twitter. Yao et al. (2014) build an emotion corpus with seven emotion types from SINA microblog.
Huang et al. (2015) construct an emotion from TENCENT microblog including both simple and complex
emotion annotation.

According to the text granularity, emotion analysis works can be generally divided into three levels:
document-level, sentence-level, and word-level. Gilad (2005) uses SVM to model a document-level

3288



emotion classifier with blog articles. Yang et al. (2007) identify the emotion types of blog articles based
on SVM and CRF with sentiment lexicon. Lin et al. (2007) use the articles on Yahoo! News to analysis
the news readers’ emotion.

Sentence-level emotion analysis is mainly based on emotion lexicon. Mohammad and Turney (2010)
study the effect of word level emotion lexicons for sentence level emotion analysis. They use word level
emotion lexicons based on Word Net and NRC-10 to predict the emotion in sentences with Logistic
Regression and SVM. Das and Bandyopadhyay (2010) categorize the emotions on Bengali blog. They
first identify the emotion of words in a sentence, then judge the emotion of this sentence according to the
words’ emotion. Aman and Szpakowicz (2007) implement a knowledge-based sentence level emotion
recognition method.

Word-level emotion analysis aims to construct emotion lexicon, which plays an important auxiliary
role in emotion analysis. Yang et al. (2014) propose Emotion-aware LDA model to build a domain-
specific lexicon. Xu et al. (2010) use language resource, such as synonym dictionary, semantic dictionary,
and labeled and unlabeled corpus to construct the similarity matrix between words and seed words. They
build an emotion lexicon with five emotion classes using graph-based rules. As a special expression of
words, emoticons play an important role in emotion analysis due to the explosion in social media. Tang
et al. (2013) annotate data from microblog posts with the help of emoticons.

There are several studies to address corpus adaptation problem in NLP field. Gao et al. (2004) do
a pioneer work by describing a transformation-based converter to transfer a certain word segmentation
result to another annotation guideline. Jiang et al. (2009) investigate the automatic integration of word
segmentation knowledge in different annotated corpora. Similar approaches are applied to constituency
parsing (Zhu et al., 2011) and word segmentation (Sun and Wan, 2012)

Unlike all above studies, we propose a corpus fusion approach to emotion classification in order to
address the corpus fusion problem to combine two corpora with different emotion taxonomies and anno-
tation guidelines. To the best of our knowledge, this is the first attempt to address this task in emotion
analysis.

3 Corpus

Two emotion corpora we used are respectively constructed by Yao et al. (2014) and Huang et al. (2015).
We simply denote the two corpora as YAO (2014) and HUANG (2015) in the rest of this paper for
convenience.

YAO (2014) is constructed from SINA microblog1. It categorizes emotions into seven classes:
happiness, anger, sadness, fear, like, surprise and disgust. The corpus consists of 14,000 in-
stances, of which 7,407 instances express emotions. Each instance may include both primary emotion
and secondary emotion, or just has one primary emotion. Table 1a illustrates the distribution of primary
emotions and secondary emotions in this corpus.

Notation Emotion
Class

Primary
Emotion

Secondary
Emotion

eY1 happiness 1460 359
eY2 anger 669 203
eY3 sadness 1173 269
eY4 fear 148 61
eY5 like 2203 546
eY6 surprise 362 170
eY7 disgust 1392 385
- Total 7407 1993

(a) YAO (2014)

Notation Emotion
Class

Amount

eH1 joy 1038
eH2 anger 472
eH3 sadness 581
eH4 fear 94
eH5 positive 1178
eH6 neutral 1131
eH7 negative 2175
- Total 6669

(b) HUANG (2015)

Table 1: Emotion categories and distribution on two corpora

1http://weibo.com

3289



Huang et al. (2015) propose another emotion taxonomy with both basic emotions and complex emo-
tions. Basic emotions include four emotion classes: joy, anger, sadness and fear. Complex emo-
tions contain three emotion classes: positive, neutral and negative. This corpus is constructed from
TENCENT microblog2, and it consists of 15,540 instances. 6,669 instances express certain emotion. Al-
though there is a very few multi-label annotation on it, we consider this corpus as single-label annotated.
Table 1b shows the distribution of emotions in the two corpora.

4 Approach to Corpus Fusion for Emotion Classification

The corpus fusion approach to emotion classification aims to exploit the relationship between two corpora
which have similar emotion taxonomies. Figure 2 illustrates the framework of our model. The testing
results generated by the supervised emotion classifier are refined by ILP with label constraints.

Emotion classifier 

B

Emotion classifier 

A

ILP

Refined Emotion 

result of testing 

data A (or B)

Emotion result A

Emotion result B

supervised emotion classification ILP optimization

Training data 

from corpus A

Training data 

from corpus B

Testing data from 

corpus A (or B)

Figure 2: The framework of corpus fusion for emotion classification with ILP

4.1 Supervised Emotion Classification

Supervised classification problem trains a predictor f which maps an input vector x to the corresponding
class label y on a set of training data. In emotion classification, a feature vector x is extracted from the
instance. Formally, the objective of classification is defined as follows:

f(x)→ y, y ∈ {emotion1, emotion2, ...} (1)

In this task, we train plural binary predictors for each emotion class for the testing set from YAO
(2014), and a 7-way predictor for the testing set from HUANG (2015). For one sample instance ti from
the testing set, predicting results rYi and rHi indicating the predicted emotion labels, and we get two sets
of probabilities PYi and PHi which contain the probabilities of this sample belonging to each category in
two emotion taxonomies:

PYi = {p(rYi = eY1), p(rYi = eY2) ...p(rYi = eY7)},
PHi = {p(rHi = eH1), p(rHi = eH2) ...p(rHi = eH7)}

(2)

where p(rYi = eY1) denotes the probability of ti belonging to happiness under the emotion taxonomy
of YAO (2014), and p(rHi = eH1) denotes the probability of ti belonging to joy under the emotion
taxonomy of HUANG (2015). The rest can be done in the same manner.

2http://t.qq.com/

3290



4.2 Global Optimization with ILP
ILP optimization aims to refine the label result given the probability result. We design objective function
and constraints to exploit the similarity between two emotion taxonomies. Like Roth and Yih (2004), we
firstly define following assignment costs:

cYi = −log(p(rYi = eYi)) + log(1− p(rYi = eYi)),
cHi = −log(p(rHi = eHi)) + log(1− p(rHi = eHi)),

1 ≤i ≤ 7
(3)

where cYi is the cost of ti belonging to the ith emotion class under the taxonomy of YAO (2014), and
cHi is the cost of ti belonging to the ith emotion class under the taxonomy of HUANG (2015). For each
sample ti in testing set there can be two cost vectors CY and CH , and two label vectors LY and LH used
on storing the refined labels of ti:

CY = [c Y1 c Y2 ... c Y7]T, CH = [c H1 c H2 ... c H7]T (4)
LY = [y1 y2 ... y7], LH = [z1 z2 ... z7] (5)

where y1 to y7 indicate the emotion class of ti under the taxonomy of YAO (2014), and z1 to z7 indicate
that under the taxonomy of HUANG (2015). For instance, if the label vector LY =[0,1,0,0,0,0,1], it
indicates that ti is refined as anger and disgust under the emotion taxonomy of YAO (2014). The ILP
optimization aims to acquire the refined emotion labels which are given by two label vectors.

ILP with Intra-corpus Constraints
We employ ILP with intra-corpus constraints to address the issue on annotation guideline. Note that we
don’t solely apply this type of constraints on the testing set of HUANG (2015) because it is considered
to be single-labeled. On YAO (2014), the objective function can be defined as follows:

min t = |LY × CY |

=
7∑

i=1

(c Yiyi)
(6)

Subject to:

yi ∈ {0, 1}, (7)

1 ≤
7∑

i=1

yi ≤ 2 (8)

where formula (8) implies that one or two labels are chosen from the emotion taxonomy of YAO (2014)
after optimization. The objective function above aims to minimize the product of cost vector and label
vector. Furthermore, an additional constraint aiming to align the emotion classes between two tax-
onomies is defined as follows:

(C1) Co-occurrence constraint: We filter the emotion pairs with low co-occurrence frequency in
YAO (2014). The filtered pairs all occur below 30 times in the corpus according to statistics. For
instance, happiness and disgust rarely co-occur in the same instance.

y1 + y2 ≤ 1, y1 + y4 ≤ 1, y2 + y4 ≤ 1, y2 + y5 ≤ 1,
y2 + y6 ≤ 1, y4 + y5 ≤ 1, y4 + y6 ≤ 1, y4 + y7 ≤ 1

(9)

ILP with Extra-corpus Constraints
We leverage the similarity between two emotion taxonomies with extra-corpus constraints. Firstly, we
add specific costs as follows:

c align H1 = c H1 + c H5, c align H2 = c H2 + c H7,
c align H3 = c H3 + c H7, c align H4 = c H4 + c H7,
c align Hi = c Hi, 5 ≤ i ≤ 7

(10)

3291



In formula (10), some costs in the original cost vector defined in formula (4) are added together.
For example, c H1 and c H5 are added into c align H1. It means that we align happiness under the
taxonomy of YAO (2014) to both joy and positive under the taxonomy of HUANG (2015) together with
the alignment constraint defined below. The cost vector CH changes to:

C
′
H = [c align H1 c align H2 ... c align H7]

T (11)

As a result, the objective function becomes to:

min t = |LY × CY |+ |LH × C ′H |

=
7∑

i=1

(c Yiyi + c align Hizi)
(12)

Subject to:

yi, zi ∈ {0, 1}, (13)
7∑

i=1

yi = 1, (14)

7∑
i=1

zi = 1 (15)

where formula (14) and (15) unify the number of possible labels on both taxonomies to one because
we don’t consider any intra-corpus constraints which are derived from the annotation guideline of YAO
(2014) when extra-corpus is solely applied. The alignment constraint is defined as follows:

(C2) Alignment constraint: When a sample instance is categorized into a certain emotion e under
one taxonomy, it can be categorized into an emotion e

′
which is same or similar to e under the other

taxonomy. For instance, if an instance t is labeled as disgust under the taxonomy of YAO (2014), it can
be labeled as negative under the taxonomy of HUANG (2015).

yi = zi, 1 ≤ i ≤ 7 (16)
ILP with Two Types of Constraints
In this subsection, both intra-corpus and extra-corpus constrains are employed. The objective function is
defined as follows:

min t = |LY × CY |+ |LH × C ′H |

=
7∑

i=1

(c Yiyi + c align Hizi)
(17)

Subject to:

yi, zi ∈ {0, 1}, (18)

1 ≤
7∑

i=1

yi ≤ 2, (19)

7∑
i=1

zi = 1 (20)

Moreover, constraint C1 and C2 are also employed to restrict the labels in both views of intra-corpus
and extra-corpus. Additionally, we make a relaxation on the alignment constraint C2.

(C3) Relaxed Alignment constraint: We make a relaxation on C2 to allow more than one chosen
label on YAO (2014). C2 makes the numbers of labels on two corpora be the same so that formula (19)
becomes meaningless. We employ the following version to replace C2.

yi ≥ zi, 1 ≤ i ≤ 7 (21)

3292



5 Experimentation

5.1 Experimental Setting

Features
Bag-of-words feature is adopted in training supervised emotion classifiers. Each instance is represented
as a binary vector indicating the presence or absence of word unigrams.

Evaluation Metrics
We employ the widely used accuracy and F1-measure on the multi-class-single-label emotion classifica-
tion on HUANG (2015). On multi-class-multi-label emotion classification on YAO (2014), we employ
two evaluation metrics to measure the performance. These metrics have been popularly used in multi-
label classification problems (Godbole and Sarawagi, 2004).

• Accuracy: It gives an average degree of the similarity between the predicted and the ground truth
label sets of all test examples:

Accuracy =
1
q

q∑
i=1

|yi ∩ y′i|
|yi ∪ y′i|

(22)

where q is the number of all test instances, y
′
i is the estimated label and yi is the true label.

• F1-measure: It is the harmonic mean between precision and recall. It can be calculated from
true positives, true negatives, false positives and false negatives based on the predictions and the
corresponding actual values:

F1 =
1
q

q∑
i=1

|yi ∩ y′i|
|yi|+ |y′i|

(23)

5.2 Experimental Results with ILP Optimization

ILP with Intra-corpus constraints
In this experiment, intra-corpus constraints are applied for refining the predicting results. Note this
experiment is only taken place on YAO (2014) in which an instance might have one or two labels. We
experimentalize following methods for comparison:

• Baseline: We apply Maximum Entropy classifier with BOW feature as one baseline. Seven binary
classifiers are trained for each emotion class. We balance the proportion of positive data and negative
data for each classifier in order to achieve the best overall performance.

• ILP with Intra-corpus Constraints: ILP global optimization approach with defined intra-corpus
constraints and objective function.

Table 2 shows the performance of ILP with intra-corpus constraints on the testing set of YAO (2014).
According to the results, ILP approach with intra-corpus constraints overcomes the baseline with a 0.050
promotion in accuracy and a 0.013 promotion on F1-measure, which demonstrates the effectiveness of
proposed intra-corpus constraints.

Accuracy F1
Baseline 0.375 0.243
ILP (Intra-corpus) 0.425 0.256

Table 2: Performance of ILP with intra-corpus constraints on YAO (2014)

3293



ILP with Extra-corpus constraints
In this experiment, we apply extra-corpus constraints on ILP optimization to leverage the annotated data
from two corpora. We experimentalize following methods for comparison:

• Baseline: Max Entropy classifier with BOW feature serves as baseline. In the experiment on YAO
(2014), the baseline is same as the one used in the experiment with intra-corpus constraints. In the
experiment on HUANG (2015), a 7-way classifier is trained for baseline. The proportion of training
data for each emotion class follows its original proportion.

• ILP with Intra-corpus Constraints: ILP global optimization approach with defined extra-corpus
constraints and objective function.

Table 3 shows the performance of ILP when extra-corpus constraints are utilized on both testing sets.
Extra-corpus constraints improve the accuracy on YAO (2014), but the F1-measure reduces. While on
HUANG (2015), both accuracy and F1-measure improve distinctly, proving the capability of extra-corpus
constraints on corpus fusion to leverage the annotated data from other corpus.

Accuracy F1
Baseline 0.375 0.243
ILP (Extra-corpus) 0.430 0.231

(a) On YAO (2014)

Accuracy F1
Baseline 0.405 0.359
ILP (Extra-corpus) 0.431 0.386

(b) On HUANG (2015)

Table 3: Performance of ILP with extra-corpus constraints

ILP with Both Types of constraints
In this experiment, we apply both intra-corpus and extra-corpus constraints on ILP optimization to im-
plement corpus fusion from both views. Following methods are experimentalized:

• Baseline: Same as those used in the experiment with extra-corpus constraints.
• ILP with Intra-corpus Constraints: ILP approach with only intra-corpus constraints. This method

is only employed on YAO (2014).

• ILP with Extra-corpus Constraints: ILP approach with only extra-corpus constraints.
• ILP with Both Types of Constraints: ILP approach with both intra-corpus and extra-corpus con-

straints and defined objective function.

Table 4 shows the performance of ILP approach with both intra-corpus and extra-corpus constraints
compared to baselines. From these tables, we can see that employing both constraints further improves
accuracy and F1-measure on YAO (2014). The joint use of both constraints avoids the decrease on
F1-measure when only extra-corpus constraints are applied. On HUANG (2015), ILP with both con-
straints slightly improves the accuracy compared to ILP with only extra-corpus, but the F1-measure also
decreases slightly. Intra-corpus constraints impact a little on HUANG (2015).

Accuracy F1
Baseline 0.375 0.243
ILP (Intra-corpus) 0.425 0.256
ILP (Extra-corpus) 0.430 0.231
ILP (both) 0.440 0.261

(a) On YAO (2014)

Accuracy F1
Baseline 0.405 0.359
ILP (Extra-corpus) 0.431 0.386
ILP (both) 0.435 0.382

(b) On HUANG (2015)

Table 4: Performance of ILP with both types of constraints

3294



 0.2

 0.26

 0.32

 0.38

 0.44

20% 40% 60% 80% 100%

A
cc

ur
ac

y

YAO (2014)

Baseline
0.206

0.267

0.318

0.353
0.375

ILP (both)

0.309

0.354

0.397

0.429
0.440

 0.12

 0.16

 0.2

 0.24

 0.28

20% 40% 60% 80% 100%

F 1
-m

ea
su

re

YAO (2014)

Baseline0.137

0.182

0.209

0.228
0.243

ILP (both)

0.178

0.213

0.238
0.252

0.261

 0.28

 0.32

 0.36

 0.4

 0.44

20% 40% 60% 80% 100%

A
cc

ur
ac

y

HUANG (2015)

Baseline
0.286

0.327

0.351

0.386

0.405

ILP (both)

0.348

0.373

0.410

0.430 0.435

 0.26

 0.3

 0.34

 0.38

20% 40% 60% 80% 100%
F 1

-m
ea

su
re

HUANG (2015)

Baseline
0.262

0.305

0.323

0.353
0.359

ILP (both)

0.309

0.338
0.351

0.379 0.382

Figure 3: Performance of ILP with both types of constraints with different scales of training set

Figure 3 gives the performance of ILP with both constraints when different scales of training set
are used. The improvement of ILP approach decreases with the increase of the scale of training set. It
means that a highly performed baseline may reduce the space of promotion achieved by ILP optimization
because the amount of error classified instances which can be refined decreases. Even so, ILP approach
still improves the performance distinctly when the scale of training set is 100%.

6 Conclusion and Future Work

In this paper, we propose a corpus fusion approach to corpus fusion for emotion classification with ILP
optimization. Specifically, we employ intra-task and extra-task constraints to better capture the similarity
between two different emotion taxonomies and address the different annotation guidelines. Experiments
demonstrate that ILP optimization improves the performance by using annotated data from other corpus,
which has a different emotion taxonomy.

In our future work, we would like to seek better modification on ILP for further improvement. More-
over, we will try to adapt this approach to other NLP tasks where two or more corpora are available.

Acknowledgments

This research work has been partially supported by four NSFC grants, No.61273320, No.61331011,
No.61375073, and No.61503386.

References
Saima Aman and Stan Szpakowicz. 2007. Identifying expressions of emotion in text. In Text, Speech and Dia-

logue, 10th International Conference, TSD 2007, Pilsen, Czech Republic, September 3-7, 2007, Proceedings,
pages 196–205.

Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2011. Twitter mood predicts the stock market. J. Comput. Science,
2(1):1–8.

Ying Chen, Sophia Yat Mei Lee, Shoushan Li, and Chu-Ren Huang. 2010. Emotion cause detection with linguistic
constructions. In COLING 2010, 23rd International Conference on Computational Linguistics, Proceedings of
the Conference, 23-27 August 2010, Beijing, China, pages 179–187.

3295



Dipankar Das and Sivaji Bandyopadhyay. 2010. Sentence level emotion tagging on blog and news corpora. J.
Intelligent Systems, 19(2):145–162.

Maros Galik and Stefan Rank. 2012. Modelling emotional trajectories of individuals in an online chat. In
Multiagent System Technologies - 10th German Conference, MATES 2012, Trier, Germany, October 10-12,
2012. Proceedings, pages 96–105.

Jianfeng Gao, Andi Wu, Cheng-Ning Huang, Hongqiao Li, Xinsong Xia, and Hauwei Qin. 2004. Adaptive
chinese word segmentation. In Proceedings of the 42nd Annual Meeting of the Association for Computational
Linguistics, 21-26 July, 2004, Barcelona, Spain., pages 462–469.

Mishne Gilad. 2005. Experiments with mood classification in blog posts. In Proceedings of ACM SIGIR 2005
workshop on stylistic analysis of text for information access, volume 19, pages 321–327. Citeseer.

Shantanu Godbole and Sunita Sarawagi. 2004. Discriminative methods for multi-labeled classification. In Ad-
vances in Knowledge Discovery and Data Mining, 8th Pacific-Asia Conference, PAKDD 2004, Sydney, Aus-
tralia, May 26-28, 2004, Proceedings, pages 22–30.

Lei Huang, Shoushan Li, and Guodong Zhou. 2015. Emotion corpus construction on microblog text. In Chinese
Lexical Semantics - 16th Workshop, CLSW 2015, Beijing, China, May 9-11, 2015, Revised Selected Papers,
pages 204–212.

Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word
segmentation and POS tagging - A case study. In ACL 2009, Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics and the 4th International Joint Conference on Natural Language
Processing of the AFNLP, 2-7 August 2009, Singapore, pages 522–530.

Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mohammad. 2014. Sentiment analysis of short informal texts. J.
Artif. Intell. Res. (JAIR), 50:723–762.

Shoushan Li, Lei Huang, Rong Wang, and Guodong Zhou. 2015. Sentence-level emotion classification with label
and context dependence. In Proceedings of the 53rd Annual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation
of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages
1045–1053.

Kevin Hsin-Yih Lin, Changhua Yang, and Hsin-Hsi Chen. 2007. What emotions do news articles trigger in their
readers? In SIGIR 2007: Proceedings of the 30th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, Amsterdam, The Netherlands, July 23-27, 2007, pages 733–734.

Huanhuan Liu, Shoushan Li, Guodong Zhou, Chu-Ren Huang, and Peifeng Li. 2013. Joint modeling of news
reader’s and comment writer’s emotions. In Proceedings of the 51st Annual Meeting of the Association for
Computational Linguistics, ACL 2013, 4-9 August 2013, Sofia, Bulgaria, Volume 2: Short Papers, pages 511–
515.

Saif M. Mohammad and Peter D. Turney. 2010. Emotions evoked by common words and phrases: Using mechan-
ical turk to create an emotion lexicon. In Proceedings of the NAACL HLT 2010 Workshop on Computational
Approaches to Analysis and Generation of Emotion in Text, CAAGET ’10, pages 26–34, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In
Proceedings of the International Conference on Language Resources and Evaluation, LREC 2010, 17-23 May
2010, Valletta, Malta.

Matthew Purver and Stuart Battersby. 2012. Experimenting with distant supervision for emotion classification.
In EACL 2012, 13th Conference of the European Chapter of the Association for Computational Linguistics,
Avignon, France, April 23-27, 2012, pages 482–491.

Changqin Quan and Fuji Ren. 2009. Construction of a blog emotion corpus for chinese emotional expression anal-
ysis. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP
2009, 6-7 August 2009, Singapore, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1446–
1454.

Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language
tasks. In Proceedings of the Eighth Conference on Computational Natural Language Learning, CoNLL 2004,
Held in cooperation with HLT-NAACL 2004, Boston, Massachusetts, USA, May 6-7, 2004, pages 1–8.

3296



Weiwei Sun and Xiaojun Wan. 2012. Reducing approximation and estimation errors for chinese lexical processing
with heterogeneous annotations. In The 50th Annual Meeting of the Association for Computational Linguistics,
Proceedings of the Conference, July 8-14, 2012, Jeju Island, Korea - Volume 1: Long Papers, pages 232–241.

Duyu Tang, Bing Qin, Ting Liu, and Zhenghua Li. 2013. Learning sentence representation for emotion classi-
fication on microblogs. In Natural Language Processing and Chinese Computing - Second CCF Conference,
NLPCC 2013, Chongqing, China, November 15-19, 2013, Proceedings, pages 212–223.

Zhongqing Wang, Sophia Yat Mei Lee, Shoushan Li, and Guodong Zhou. 2015. Emotion detection in code-
switching texts via bilingual and sentimental information. In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the 7th International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing,
China, Volume 2: Short Papers, pages 763–768.

Shiyang Wen and Xiaojun Wan. 2014. Emotion classification in microblog texts using class sequential rules. In
Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Québec City,
Québec, Canada., pages 187–193.

Ge Xu, Xinfan Meng, and Houfeng Wang. 2010. Build chinese emotion lexicons using A graph-based algo-
rithm and multiple resources. In COLING 2010, 23rd International Conference on Computational Linguistics,
Proceedings of the Conference, 23-27 August 2010, Beijing, China, pages 1209–1217.

Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-Hsi Chen. 2007. Emotion classification using web blog corpora.
In 2007 IEEE / WIC / ACM International Conference on Web Intelligence, WI 2007, 2-5 November 2007, Silicon
Valley, CA, USA, Main Conference Proceedings, pages 275–278.

Min Yang, Dingju Zhu, and Kam-Pui Chow. 2014. A topic model for building fine-grained domain-specific
emotion lexicon. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,
ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 2: Short Papers, pages 421–426.

Yuanlin Yao, Shuwei Wang, Ruifeng Xu, Bin Liu, Lin Gui, Qin Lu, and Xiaolong Wang. 2014. The construction
of an emotion annotated corpus on micro blog text. Journal of Chinese Information Processing, 28(5):83–91.

Muhua Zhu, Jingbo Zhu, and Minghan Hu. 2011. Better automatic treebank conversion using A feature-based
approach. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA - Short Papers, pages
715–719.

3297


