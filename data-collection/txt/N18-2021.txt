



















































Training Structured Prediction Energy Networks with Indirect Supervision


Proceedings of NAACL-HLT 2018, pages 130–135
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Training Structured Prediction Energy Networks
with Indirect Supervision

Amirmohammad Rooshenas, Aishwarya Kamath, Andrew McCallum
College of Information and Computer Sciences

University of Massachusetts Amherst
{pedram,akamath,mccallum}@cs.umass.edu

Abstract

This paper introduces rank-based training
of structured prediction energy networks
(SPENs). Our method samples from output
structures using gradient descent and mini-
mizes the ranking violation of the sampled
structures with respect to a scalar scoring func-
tion defined using domain knowledge. We
have successfully trained SPEN for citation
field extraction without any labeled data in-
stances, where the only source of supervi-
sion is a simple human-written scoring func-
tion. Such scoring functions are often easy to
provide; the SPEN then furnishes an efficient
structured prediction inference procedure.

1 Introduction

Structured prediction, or the task of predicting
multiple inter-dependent variables, is important in
many domains, including computer vision, com-
putational biology and natural language process-
ing. For example, in sequence labelling, im-
age segmentation, and parsing we are given in-
put variables x, and must predict output variables
y, where the number of possible y values are
typically exponential in the number of variables
that comprise it. Not only does this sometimes
give rise to computational difficulties, it also leads
to statistical parameter estimation issues, where
learning precise models requires large amounts of
labeled training data.

In some cases, unsupervised learning from
plentiful unlabeled data may provide helpful out-
puts (Daumé III, 2009; Ammar et al., 2014). But
usually some form of more direct supervision is
required to create a model truly useful to the task
at hand. In the absence of abundant labeled data
we may consider alternative forms of supervision.
For example, rather than providing labeled data
instances, humans may more easily inject their

domain knowledge by providing “labels on fea-
tures,” or “expectations” about correct outputs, as
in generalized expectation criteria (Mann and Mc-
Callum, 2010), or by providing constraints, as in
posterior regularization (Ganchev et al., 2010) or
constraint driven learning (Chang et al., 2007). A
major weakness of these methods, however, is that
at training time inference must be done in the fac-
tor graph encompassing the union of the model’s
factor graph and the expectation dependencies—
often leading to prohibitively expensive inference.
Moreover, these methods cannot learn from non-
decomposable domain knowledge, where the do-
main knowledge is not in a form of a set of labeled
features or constraints.

An easy way for humans to express domain
knowledge is by writing a simple scalar scoring
function that indicates preferences among choices
for y given x. These human-coded functions may,
for example, be based on arbitrary rule systems (or
even Turing-complete programs) of the sort writ-
ten by humans to solve problems before machine
learning became so wide-spread.

In general, the human written domain knowl-
edge functions are not expected to be perfect—
most likely only examining a subset of features
and not covering all cases. Thus we are now faced
with two challenges: (1) the domain knowledge
functions have limited generalization; (2) the do-
main knowledge functions provide a ranking, but
do not provide an inference (search) procedure.

This paper presents a new training method
for structured prediction energy networks
(SPENs) (Belanger and McCallum, 2016; Be-
langer et al., 2017) that aims to address both
these challenges, yielding efficient inference for
structured prediction, trained from human-coded
domain knowledge plus unlabeled data, but
not requiring any labeled data instances. In
SPENs, the factor graph that typically represents

130



output variable dependencies is replaced with
a deep neural network that takes y and x as
input and outputs a scalar energy score, but is
able to learn much richer correlations than are
typically captured in factor graphs. Inference
in SPENs is performed by gradient descent in
the energy, back-propagated to cause steps in
a relaxed y space. Whereas previous training
procedures for SPENs used labeled data, here
we train SPENs from only unlabeled data plus
human-coded domain knowledge in the form
of a scoring function. We do so by building on
SampleRank (Rohanimanesh et al., 2011; Singh
et al., 2010), which enforces that the rank of two
sampled ys according to the trained factor graph
is consistent with their rank according to distance
to the labeled, true y. In our training method,
pairs of y’s are obtained from successive steps
of training-time gradient-descent inference on
y; when their rank is not consistent with that of
the domain knowledge function, we accordingly
update the energy network parameters.

We demonstrate our method on a citation field
extraction task, for which we learn a neural net-
work (1) that generalizes beyond the original do-
main knowledge function, and (2) that provides ef-
ficient test-time inference by gradient descent.

2 Structured Prediction Energy
Networks

In general, SPEN parameterizes an energy func-
tion Ew(y,x) using deep neural networks over
output variables y as well as input variables x,
where w denotes the neural network’s parameters.
Belanger and McCallum (2016) separate the en-
ergy function into global and local terms. The
role of the local terms is to capture the depen-
dency among input x and each individual output
variable yi, while the global term aims to capture
long-range dependencies among output variables.

Prediction in SPENs requires finding ŷ =
argminy∈Y Ew(y,x) for the given input x. This
inference problem is solved using gradient de-
scent. However, the energy surface is non-convex,
which prevents gradient descent inference from
finding the exact structure ymin that globally min-
imizes the energy function. One approach to ad-
dress this problem is to parameterize the energy
function such that the SPEN is convex in the out-
put variables y (Amos et al., 2017), but this lim-
its the representational power of SPENs. Al-

though gradient descent inference does not guar-
antee an exact solution, it has successfully been
used in several domains such as multi-label classi-
fication (Belanger and McCallum, 2016), image-
segmentation (Gygli et al., 2017), and semantic
role labeling (Belanger et al., 2017).

3 Rank-Based Training of SPENs

Different methods have been introduced for train-
ing SPENs: margin-based training (Belanger and
McCallum, 2016), end-to-end learning (Belanger
et al., 2017), and value matching (Gygli et al.,
2017). Margin-based training enforces the energy
of the ground truth structure to be lower than the
energy of every incorrect structure by a margin,
which is calculated as the Hamming loss between
the two structures. End-to-end learning unrolls the
energy minimization into a differentiable compu-
tation graph to output the predicted structure. It
then trains the model by directly minimizing the
loss between the predicted and ground-truth struc-
tures. Finally, the value matching approach trains
SPENs such that the energy value matches the
value of a given target function, such as the L2
distance between the ground-truth and predicted
structures.

All of these methods strongly depend on the ex-
istence of the ground truth values either as labeled
data or as the value of a function applied to it.
While dependence of the margin-based and end-
to-end learning approaches on the labeled data
is explicit, this dependency in the case of value-
matching may not be obvious. In the absence of la-
beled data, we have to use the model’s predictions
instead, for training. These predictions are often
incorrect, especially at early stages of training. As
a result, value-matching training is constrained to
match the score of these predictions with the value
of the energy function defined by SPEN. This re-
quires matching several incorrect structures for a
given input, which hinders gradient descent infer-
ence from finding the exact solution by introduc-
ing many local optima. To address this problem,
we use a ranking objective similar to SampleR-
ank (Rohanimanesh et al., 2011) such that it pre-
serves the optimum points of the score function.

In general, if SPEN ranks every pair of output
structures identical to the score function, the op-
timum points of the score function match those of
SPEN. However, forcing the ranking constraint for
every pair of output structures is not tractable, so

131



we need to approximate it by sampling some can-
didate pairs. Given a score function V (y,x), we
are able to rank every two consecutive candidate
structures based on their score values. Consider
two candidate output structures y1 and y2 for the
given input x. We define yh and yl based on the
score function as the following:

yh = argmax
y∈{y1,y2}

V (y,x),

yl = argmin
y∈{y1,y2}

V (y,x). (1)

We expect that these two structures have the
same ranking with respect to Ew(.,x), which
can be described as: α(V (yh,x) − V (yl,x)) <
Ew(yh,x)−Ew(yl,x), where α is a tunable pos-
itive scalar. Therefore, the rank-based objective
minimizes the constraint violations:

min
w

∑

x∈D
[α(V (yh,x)− V (yl,x))−

Ew(yh,x) + Ew(yl,x)]+ (2)

[.]+ is max(., 0). Figure 1 shows a ranking vio-
lation for two structures y1 and y2 for a given x.
The arrows indicate the direction of update over
the energy surface. Note that we ignore the de-
pendence of y on w, which introduces approxi-
mation in the gradient of Eq. 2. For the super-
vised setting, Belanger et al. (2017) address this
problem by unrolling the inference steps as an
inference network and back-propagating through
the inference network. We leave exploring sim-
ilar approaches for rank-based training for future
work. To compute Eq. 2, we need to find configu-
rations yi and yj such that both are candidate so-
lutions for argminy∈Y Ew(y,x). If not, the num-
ber of required samples would be exponential in
|Y|. Since at test time we use gradient descent in-
ference, a similar method is used for generating
candidate structures: the trajectory of points in the
inference mechanism is used as the set of possi-
ble candidates. The idea of deterministic sampling
from SPEN energy surface was first introduced by
David Belanger (2017). We define the inference
trajectory, T (x), as a sequence of output struc-
tures generated using projected gradient descent
inference in order to find the minimum solution
of Ew(.,x).

Given a random initial structure y0, we de-
fine the inference trajectory as: T (x) =

Figure 1: Schematic machinery of rank-based training.
The dashed line is the surface of score function V (.,x)
and the solid line is the surface of SPEN E(.,x), both
conditioned on input x. Here, y2 and y3 violate the
ranking constraint, and the arrows show the direction
of updates on the energy surface.

{y1, · · · ,ym}, where yt+1 = Py∈∆L(yt −
η ∂∂yEw(yt,x)). Py∈∆L projects the values of y
onto the probability simplex ∆L overL values that
each variable y can take. For each input x in the
training data, we find the first consecutive struc-
tures yi, yi+1 ∈ T (x) that violate the ranking
constraint, then use Eq. 2 to reduce the number
of violations. Algorithm 1 describes the complete
training algorithm.

Algorithm 1 Rank-based training of SPEN
D ← unlabeled mini-batch of training data
V (., .)← scoring function
Ew(., .)← input SPEN
for each x in D do
T (x)← samples using GD inference in Ew(.,x).
ξ ← ∅.
for each pair (yi,yi+1) in T (x) do

Construct yh and yl using Eq.1
if α(V (yh,x) − V (yl,x)) > Ew(yh,x) −

Ew(yl,x) then
ξ ← ξ ∪ (x,yh,yl).

end if
end for
Optimize Eq.2 using ξ.

end for

4 Citation Field Extraction

To show the success of rank-based learning with
indirect supervision, we conduct experiments on
citation field extraction as an instance of structured
prediction problems. The goal of citation field ex-
traction is to segment citation text into its con-
stituent parts such as Author, Title, Journal, Page,
and Date. We used the Cora citation dataset (Sey-
more et al., 1999), which includes 100 labeled ex-
amples as the test set and another 100 labeled ex-
amples for the validation set. Our training data
consists of 300 training examples from the Cora
citation data set for which we dismiss the labels,

132



as well as another 700 unlabeled citations acquired
across the web, which adds up to 1000 unlabeled
data points. Each token can be labeled with one of
13 possible tags. We use fixed-length input data
by padding all citation text to the maximum cita-
tion length in the dataset, which is 118 tokens. We
report token-level accuracy measured on non-pad
tokens.

We provide the learning algorithm with a hu-
man written score function that takes the citation
text and predicted tags as input. The score func-
tion then checks for violations of its rules and pe-
nalizes the predicted tags accordingly. Figure 2
shows examples of rules in the score function.
Our complete score function consists of around 50
rules.

We used two 2-layer neural networks with 1000
and 500 hidden nodes to parameterize the local
and global energy functions of SPEN. We examine
different α (Eq. 2) values of 0.1, 1.0, 2.0, 5.0, and
10.0, and setting α value to 2.0 has the best per-
formance on the validation set. We use gradient
descent inference with ten gradient descent steps
and η = 0.1 for both training and test.

We include the results of generalized expecta-
tion (GE) from Mann and McCallum (2010) that
use the same dataset and setting. Our results show
that R-SPEN achieves significantly better token-
level accuracy as compared to GE.

We also compare R-SPEN with different infer-
ence algorithms that search using the score func-
tion to find the best configuration with maximum
score. The results of these are listed in Table 1.
Greedy search first randomly initializes the out-
put tags and then iteratively replaces each assigned
tag with a tag that results in the maximum score
until the end of the citation is reached. This pro-
cess is repeated until convergence, measured by no
tag changing in an iteration. To avoid the effects
of random initialization, this is repeated with var-
ied number of random restarts, as reported in Ta-
ble 1, where the best configuration is used in the
scores reported. For the baseline that implements
beam search, each citation is labeled by employ-
ing a beam search on the space of all tags for each
token and their subsequent configurations, while
keeping track of the best k configurations from one
token to the next. This search is further augmented
by restarting the search from the best k found after
one complete search, for a total of 10 times and 10
random restarts.

Figure 2: Examples of rules in the score function. The
first two rules constrain the relation of token and tags,
while the last rule targets the relationship between tags.

Consulting Table 1, we can confirm that both
greedy search and beam search find much better
output structures in term of score values as com-
pared to R-SPEN; however, they achieve poor ac-
curacy because the domain knowledge function
does not comprehensively provide rules regarding
all possible output structures. We report the aver-
age score values of the R-SPEN predictions on test
data as a function of training iterations in Figure 3.
Within 1000 iterations, R-SPEN is able to achieve
a test set accuracy of 38%, outperforming all base-
lines, while the average score is -18.0. R-SPEN
generalizes beyond the domain knowledge func-
tion because it successfully captures the correla-
tion among output variables through rank-based
training on unlabeled data, so its predictions may
have lower score values but are more accurate.

The test time inference of R-SPEN is much
faster than search algorithms because SPEN pro-
vides efficient approximate inference.

5 Related Work

Generalized Expectation (GE) (Mann and Mc-
Callum, 2010), Posterior Regularization (Ganchev
et al., 2010) and Constraint Driven Learn-
ing (Chang et al., 2007) are among well-known
approaches to learn from domain knowledge de-
composed over a set of constraints or labeled fea-
tures. However, these methods cannot learn from
black box domain knowledge based score func-
tions. Score functions of this type are abundant in

133



Table 1: Comparison of R-SPEN with GE and differ-
ent search algorithms in terms of token-level accuracy,
test set average score, and time taken for inference dur-
ing test time.

Method Acc. Avg. Score Time (s)
GE 37.3% N/A —
Greedy Search
10 restarts 22.6% -4.92 700
100 restarts 26.0% -3.26 6997
1000 restarts 26.1% -2.51 69272
Beam Search
k=2 30.0% -1.87 4953
k=5 30.4% -1.80 12217
k=10 31.0% -1.44 22898
R-SPEN 47.1% -20.33 <1

Figure 3: Average test set score values during training
of R-SPEN.

various fields, for example, when the score is the
result of evaluating a non-differentiable function
over output structures.

Stewart and Ermon (2017) train a neural net-
work using a score function that guides the train-
ing based on physics of moving objects. They
have defined a differentiable score function which
provides the learning algorithm with the gradi-
ent of the score function. However, in our ap-
proach the score function could be any complex
non-differentiable function.

Peng et al. (2017) and Iyyer et al. (2017) use
energy-based max-margin training for learning
from an implicit source of supervision. This can
be viewed as a score function evaluating the pre-
dicted output structure based on some real-world
domain. For example, if the output structure is
the SQL query associated with a natural language
question, the score function can be specified as the
Jaccard similarity of the extracted cells from the
table using the generated SQL query and the set of

gold answers for the natural language query as in
Iyyer et al (2017).

6 Conclusion and Future Work

We have introduced a method to train structured
prediction energy networks with indirect supervi-
sion that is derived from domain knowledge. This
domain knowledge is a scalar function that is rep-
resented in the form of certain set of rules, eas-
ily provided by humans. By using a rank-based
training we are able to effectively generalize be-
yond the domain knowledge function in problem
instances where we do not have access to labeled
data, thus establishing a viable option for solving
structured prediction problems in those regimes.

R-SPEN only uses unlabeled data and domain
knowledge for training. We should also effectively
benefit from annotated data if any is available for
the task. This can be accomplished by augmenting
the domain knowledge with rules that take into ac-
count the distance between predicted and ground
truth labels.

In the future, we wish to explore the effective-
ness of R-SPEN on various tasks using domain
knowledge functions with varying degrees of com-
plexity.

Acknowledgments

This research was funded by DARPA grant
FA8750-17-C-0106. The views and conclusions
contained in this document are those of the authors
and should not be interpreted as necessarily rep-
resenting the official policies, either expressed or
implied, of DARPA or the U.S. Government.

References
Waleed Ammar, Chris Dyer, and Noah A Smith. 2014.

Conditional random field autoencoders for unsuper-
vised structured prediction. In Advances in Neural
Information Processing Systems, pages 3311–3319.

Brandon Amos, Lei Xu, and J Zico Kolter. 2017. In-
put convex neural networks. In Proceedings of the
International Conference on Machine Learning.

David Belanger. 2017. Deep energy-based models for
structured prediction. Ph.D. Dissertation.

David Belanger and Andrew McCallum. 2016. Struc-
tured prediction energy networks. In Proceedings of
the International Conference on Machine Learning.

David Belanger, Bishan Yang, and Andrew McCallum.
2017. End-to-end learning for structured prediction

134



energy networks. In Proceedings of the Interna-
tional Conference on Machine Learning.

Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint-driven
learning. In ACL, pages 280–287.

Hal Daumé III. 2009. Unsupervised search-based
structured prediction. In Proceedings of the Interna-
tional Conference on Machine Learning, pages 209–
216.

Kuzman Ganchev, Jennifer Gillenwater, Ben Taskar,
et al. 2010. Posterior regularization for structured
latent variable models. Journal of Machine Learn-
ing Research, 11(Jul):2001–2049.

Michael Gygli, Mohammad Norouzi, and Anelia An-
gelova. 2017. Deep value networks learn to evaluate
and iteratively refine structured outputs. In Proceed-
ings of the International Conference on Machine
Learning.

Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.
Search-based neural structured learning for sequen-
tial question answering. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 1821–1831.

Gideon S Mann and Andrew McCallum. 2010. Gener-
alized expectation criteria for semi-supervised learn-
ing with weakly labeled data. Journal of machine
learning research, 11(Feb):955–984.

Haoruo Peng, Ming-Wei Chang, and Wen-tau Yih.
2017. Maximum margin reward networks for learn-
ing from explicit and implicit supervision. In Pro-
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2368–
2378.

Khashayar Rohanimanesh, Kedar Bellare, Aron Cu-
lotta, Andrew McCallum, and Michael L Wick.
2011. Samplerank: Training factor graphs with
atomic gradients. In Proceedings of the Interna-
tional Conference on Machine Learning, pages 777–
784.

Kristie Seymore, Andrew McCallum, and Roni Rosen-
feld. 1999. Learning hidden markov model structure
for information extraction. In AAAI-99 workshop on
machine learning for information extraction, pages
37–42.

Sameer Singh, Limin Yao, Sebastian Riedel, and An-
drew McCallum. 2010. Constraint-driven rank-
based learning for information extraction. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 729–732.
Association for Computational Linguistics.

Russell Stewart and Stefano Ermon. 2017. Label-free
supervision of neural networks with physics and do-
main knowledge. In AAAI, pages 2576–2582.

135


