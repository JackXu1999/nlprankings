



















































DynamicPower at SemEval-2016 Task 8: Processing syntactic parse trees with a Dynamic Semantics core


Proceedings of SemEval-2016, pages 1148–1153,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

DynamicPower at SemEval-2016 Task 8: Processing syntactic parse trees
with a Dynamic Semantics core

Alastair Butler
National Institute for Japanese Language and Linguistics

ajb129@hotmail.com

Abstract

This is a system description paper for a sub-
mission to Task 8 of SemEval-2016: Meaning
Representation Parsing. No use was made of
the training data provided by the task. Instead
existing components were combined to form
a pipeline able to take raw sentences as input
and output meaning representations. Compo-
nents are a part-of-speech tagger and parser
trained on the Penn Parsed Corpus of Mod-
ern British English to produce syntactic parse
trees, a semantic role labeller and a named en-
tity recogniser to supplement obtained parse
trees with word sense, functional and named
entity information, followed by an adapted
Tarskian satisfaction relation for a Dynamic
Semantics that is used to transform a syntac-
tic parse into a predicate logic based meaning
representation, followed by conversion to pen-
man/AMR notation required for the task ap-
praisal.

1 Introduction

This is a system description paper for a submission
to Task 8 of SemEval-2016: Meaning Representa-
tion Parsing. Syntactic structures are first obtained
by parsing raw language input, from which mean-
ing representations are derived by printing off infor-
mation accumulated with an adapted Tarskian satis-
faction relation for a Dynamic Semantics (Dekker,
2012). This is akin to compositional approaches of
formal semantics that view the task of reaching a
semantic value as being rooted in first obtaining a
syntactic parse. Key advantages are modularity and
domain independence.

This paper is structured as follows. Section 2
sketches the method used to obtain a syntactic parse.
Section 3 covers reaching a semantic representation.
Section 4 outlines conversion to penman/AMR no-
tation. Section 5 reports experiment results. Section
6 is a conclusion. An appendix details how to run
the available implementation.

2 Obtaining a syntactic parse

The approach first needs a way to obtain syntactic
parse trees. Major components used were the Stan-
ford Log-linear Part-Of-Speech Tagger1 (Toutanova
et al., 2003) and the Berkeley Parser2 (Petrov and
Klein, 2003), both trained on data from the years
of 1840–1908 of the Penn Parsed Corpus of Mod-
ern British English3 (Kroch et al., 2010). The par-
ticular setup followed suggestions of pre-processing
and post-processing made by Kulick et al. (2014)
and used tools, notably create_stripped, from
the system of Fang, Butler and Yoshimoto (2014).4

Dating from over one hundred years ago, train-
ing data for the syntactic parser was not chosen for
suitability to a potential task domain, but instead for
the practical benefit that parse results would con-
form to the scheme proposed in the Annotation man-
ual for the Penn Historical Corpora and the Parsed
Corpus of Early English Correspondence (PCEEC)
(Santorini, 2010). This scheme is exceptionally con-
sistent, especially with regards to facilitating identi-

1http://nlp.stanford.edu/software/tagger.html
2https://github.com/slavpetrov/berkeleyparser
3http://www.ling.upenn.edu/hist-corpora/PPCMBE-

RELEASE-1/index.html
4http://www.compling.jp/haruniwa/index.html

1148



fication of construction types (small clause, compar-
ative, cleft, etc.) and for its handling of coordination,
offering the least obstacle for a robust conversion to
the structures fed to the semantic component (seen
in the next section).

As an example, consider:

(1) Upon turning 80, Mao Zedong felt that he
would die soon.

A parse in tree form returned looks like:

✭✭✭ ❤❤❤
✘✘ ❳❳❳

✭✭✭✭
✘✘ ❳❳

❍❍
✭✭✭✭ ❤❤❤❤❤

✭✭✭✭✭ ✦✦PP❤❤❤❤❤

IP-MAT

PP

P

Upon

IP-PPL

VAG

turning

NP

NUM

80

PU

,

NP

NPR

Mao

NPR

Zedong

VBD

felt

CP-THT

C

that

IP-SUB

NP

PRO

he

MD

would

VB

die

ADVP

ADV

soon

PU

.

All words are part-of-speech labelled. Clause struc-
ture is generally flat with multiply branching nodes
and no specified VP level, except with VP coor-
dination. Phrasal nodes (NP, PP, ADVP, etc.) im-
mediately dominate the phrase head (N, P, ADV,
etc.) which has as sisters both modifiers and com-
plements.

Despite such flat clause and phrase structure, ex-
tended phrase labels marking function enable dis-
tinguishing the presence of modifiers (participial
clauses (IP-PPL), adverbial clauses (CP-ADV), rel-
ative clauses (CP-REL), etc.) from complements
(infinitive complements (IP-INF), that-complements
(CP-THT), embedded questions (CP-QUE), etc.).

To arrive at a more complete parse for the task,
word sense and functional information was obtained
with mateplus5 (Roth and Woodsend, 2014), which
is an extended version of the mate-tools semantic
role labeller (Björkelund et al., 2009). In addition,
named entity information was gathered with the
Stanford Named Entity Recognizer6 (Finkel et al.,
2005) using the MUC model that labels e.g., PER-
SON, ORGANIZATION, and LOCATION. Further-
more, pronouns, e.g., he, she, they, are default

5https://github.com/microth/mateplus
6http://nlp.stanford.edu/software/CRF-NER.html

marked PERSON. From (1) as input, the combina-
tion of these tools collects the following informa-
tion:

upon TMP _ O
turn PMOD turn.02 O
80 OBJ _ O
, P _ O
mao NAME _ PERSON
zedong SBJ _ PERSON
felt ROOT feel.01 O
that OBJ _ O
he SBJ _ PERSON
would SUB _ O
die VC die.01 O
soon TMP _ O
. P _ O

The first column gives word lemmas, the second
contains functional information to identify syntac-
tic subjects (SBJ) and objects (OBJ) as well as ad-
junct roles such as LOC, MSR and TMP, the third
column provides word sense information related to
PropBank (Bonial et al., 2010) semantic frames
(‘turn.02’, ‘feel.02’, ‘die.01’), and the fourth column
provides entity information. This column informa-
tion is integrated with the parse to return:

✭✭✭ ❤❤❤
✘✘✘ ❵❵❵

✭✭✭✭✭
✭✭✭✭ ❤❤❤❤

❤❤❤❤
✭✭✭✭✭ ❤❤❤❤❤❤

✘✘ ❳❳

✘✘ ❵❵❵

IP-MAT

PP-TMP

P

upon

IP-PPL

VAG

turn.02

NP-OB1

NUM

80

PU

,

NP-SBJ

BIND

*PERSON*

NPR

mao

NPR

zedong

VBD

feel.01

CP-THT

C

that

IP-SUB

NP-SBJ

BIND

*PERSON*

PRO

he

MD

would

VB

die.01

ADVP-TMP

ADV

soon

PU

.

The tree now includes extended phrase labels mark-
ing function (e.g., NP-SBJ=subject, NP-OB1=direct
object, ADVP-TMP=temporal modifier). Terminal
nodes are either word lemmas or, whenever avail-
able, PropBank word senses. Furthermore entity in-
formation is integrated with a BIND tag.

3 Obtaining semantic analysis

Having syntactic structures, the next step is to reach
a level of semantic analysis. This is derived by print-
ing off information accumulated with an adapted
Tarskian satisfaction relation for a Dynamic Seman-
tics (Dekker, 2012). Specifically, use is made of
the Treebank Semantics implementation,7 with syn-
tactic structures converted into expressions of a for-
mal language (Scope Control Theory or SCT) with
a number of primitive operations (notably, among

7http://www.compling.jp/ts

1149



others: Namely to make available fresh bindings,
T to create bound arguments, At to allocate seman-
tic roles, Close to bring about quantificational clo-
sures, Rel to establish predicate or connective rela-
tions, and If to conditionalise how calculation of a
semantic value proceeds based on assignment state).
The full list of operations and details are given in
Butler (2015). Operations access or possibly al-
ter a sequence based information state (Vermeulen,
2000) that retains binding information by assign-
ing (possibly empty) sequences of values to binding
names. This can be demonstrated with Rel creat-
ing an "and" relation with four arguments, each of
which are processed against a different assignment
state determined by instances of Namely embedded
in occurrences of Someone:

".e"→ [y,x]
"*"→ []

Rel ([".e"], ["*"], "and" [_, _, _, _])

".e"→ [x]
"*"→ []

".e"→ []
"*"→ [x]

".e"→ [y]
"*"→ [x]

".e"→ []
"*"→ [y,x]

Someonex smiles. Hex/∗y laughs. Someoney sees himx/∗y . The end.

Pronouns are able to link to "*" bindings, which are
accessible bindings that have reached the discourse
context because of prior indefinites, while indefi-
nites take bindings from ".e", which is a source for
fresh bindings. This approach gives a handle on dis-
course, and more generally governs the interaction
of quantification to capture the empirical results of
accessibility from Discourse Representation Theory
(Kamp and Reyle, 1993), as well as intra-sentential
binding conditions (Butler, 2010).

Regarding the running example, the tree for (1) is
converted to the following:
val ex1 =

( fn fh =>

( fn lc =>

( npr "PERSON" "mao_zedong" "ARG0"

( ( someFact fh ( gensym "TIME") ".e" "FACT"

( control lc

( ( fn lc =>

( some lc fh ( gensym "ENTITY") ".e"

( someClassic lc fh ( gensym "ATTRIB") ".e" nil

( adj lc "80") "CARDINAL"

( nn lc "")) "ARG1"

( verb lc ( gensym "EVENT") ".event" ["ARG1"] "turn.02")))

[ "CARDINAL", "ARG1", "ARG0", "h"]))) "TMP"

( past ".event"

( embVerb lc ( gensym "EVENT") ".event" ["TMP", "ARG0"] "feel.01"

( THAT lc

( ( fn lc =>

( pro ["*"] fh ["PERSON"] ( gensym "PERSON") ".e" "he" "ARG0"

( md fh "would" free

( advp lc fh ".event"

( adv lc "TMP_soon")

( verb lc ( gensym "EVENT") ".event" ["ARG0"] "die.01")))))

[ "FACT", "ARG0", "ARG1", "h"])))))))

[ "TMP", "ARG0", "ARG1", "h"])

[ "@e", ".e", ".event"]

Such an expression is built exploiting the input syn-
tactic structure by locating any complement for the
phrase head to scope over, and adding modifiers with
scope over the head. During construction informa-
tion about binding names is gathered and integrated
at clause levels with fn fh => and fn lc => act-
ing as lambda abstractions.

With conversion to ex1, part of speech tags trans-
form to operations (some (indefinite), nn (noun
predicate), verb (predicate with an event argu-
ment), pro (pronoun), gensym (trigger to cre-
ate a fresh binding), free (ensures no quantifica-
tional closure), etc.). Also, constructions can bring
about the inclusion of operations (e.g., someFact
with the participial clause, and THAT with the that-
complement). Conversion also adds (i) informa-
tion about local binding names (e.g., "ARG0" (logi-
cal subject role) and "ARG1" (logical object role))
and (ii) information about sources for fresh bind-
ings from quantificational closures ("@e", ".e" and
".event"). Once built ex1 reduces to primitives of
the SCT language, the start of which is as follows:

Sct.Close ("∃",["@e", ".e", ".event"],
Sct.Clean (0, ["ARG0"], "*",

Sct.Namely (Lang.C ("mao_zedong", "PERSON"),"@e",

Sct.Lam ("@e", "ARG0",

Sct.Clean (0, ["TMP"], "*",

Sct.QuantThrow (Lang.X (1, "TIME"),".e",

Sct.Lam (".e", "TMP",

Sct.Rel (["@e", ".e", ".event"], ["*", "*", "*"], "", [

Sct.Throw (".e",

Sct.Rel ([], [], "FACT", [Sct.At (Sct.T ("TMP", 0), "FACT"),

Sct.At (

Sct.Lam ("TMP", "",

Sct.Clean (0, ["ARG1", "LGS", "ARG2"], "ARG0",

Sct.Clean (0, ["TMP", "ARG1", "h"], "*",

Sct.If (fn,

Sct.Clean (0, ["ARG1", "LGS", "ARG2"], "ARG0",

Sct.Clean (0, ["TMP", "ARG1", "h"], "*", ...

Such an expression is given to the adapted Tarskian
satisfaction procedure which, instead of returning a
semantic value (e.g., true or false with respect to
a model), is used to produce a meaning representa-
tion by printing accumulated information, thus:

∃ PERSON[6] ATTRIB[3] TIME[1] EVENT[4] EVENT[7]
EVENT[5] ENTITY[2] (

80(ATTRIB[3]) ∧ is_CARDINAL(ENTITY[2], ATTRIB[3]) ∧
is_FACT_THAT(TIME[1],

turn.02(EVENT[4], PERSON[mao_zedong], ENTITY[2])) ∧
PERSON[6] = he{PERSON[mao_zedong]} ∧
TMP_soon(EVENT[7]) ∧ past(EVENT[5]) ∧

feel.01(EVENT[5], PERSON[mao_zedong],

MD_would(die.01(EVENT[7], PERSON[6]))) ∧
is_contained_in(TMP(EVENT[5]), TIME[1]))

1150



This gives a Davidsonian representation (Davidson,
1967) in which verbs are encoded with minimally
an event argument. All bindings are existentially
quantified over at the highest level, which is a conve-
nience for reaching penman/AMR notation and not
an approach limitation. Also note the pronoun is re-
solved to the only accessible PERSON antecedent.

4 Conversion to penman/AMR notation

Conversion to penman/AMR notation (Matthiessen
and Bateman, 1991; Banarescu et al., 2015) involves
transforming obtained semantic structures into trees
with explicit argument role information. An argu-
ment of each predicate (e.g., ‘@EVENT’ if present,
or the sole argument of a one-place predicate) is
made the parent of the predicate. Also, binding is
made implicit with the removal of quantification lev-
els. Thus, the running example becomes:

✭✭✭✭✭✭ ❤❤❤❤❤❤

❳❳

TOP

AND

@ATTRIB;3

80

@ENTITY;2

h

:CARDINAL

@ATTRIB;3

@TIME;1

FACT

:THAT

@EVENT;4

turn.02

:ARG1

@ENTITY;2

:ARG0

@PERSON;@mao_zedong

@PERSON;6

-PERSON-

:namely

@PERSON;@mao_zedong

@EVENT;7

TMP_soon

@EVENT;5

past

@EVENT;5

feel.01

:TMP

@TIME;1

:ARG0

@PERSON;@mao_zedong

:THAT

@MD-10

MD_would

:domain

@EVENT;7

die.01

:ARG0

@PERSON;6

Content is further re-packaged: a daughter D of an
AND level is moved inside a sister S when the ar-
gument name at the root of D is contained as an ar-
gument within S. Movement is to only one location.

✭✭✭✭✭✭ ❤❤❤❤❤❤

✭✭✭

✭✭✭✭ ❤❤❤❤

TOP

@EVENT;5

feel.01

:TMP

@TIME;1

FACT

:THAT

@EVENT;4

turn.02

:ARG1

@ENTITY;2

-ENTITY-

:CARDINAL

@ATTRIB;3

80

:ARG0

@PERSON;@mao_zedong

:ARG0

@PERSON;@mao_zedong

:THAT

@MD-10

MD_would

:domain

@EVENT;7

die.01

:ARG0

@PERSON;6

-PERSON-

:namely

@PERSON;@mao_zedong

:MOD

@mod-11

TMP_soon

:MOD

@mod-12

past

In addition, flatter structures are arrived at by ex-
cising redundant linking information, e.g., @TIME;1
FACT :THAT and folding tree material around in-
verse roles (signalled by ending the role name with

‘-of’). The latter is seen with the modal (would),
but also serves to compact long distance dependen-
cies that arise with relative clauses, comparatives,
clefts, etc. There is also expansion of name infor-
mation and some reordering of role placement.

✭✭✭✭✭✭ ❤❤❤❤❤❤ ❳❳❳

TOP

@EVENT;5

feel.01

:ARG0

@PERSON;@mao_zedong

-PERSON-

:name

@n-13

name

:op1

"mao_zedong"

:TMP

@EVENT;4

turn.02

:ARG0

@PERSON;@mao_zedong

:ARG1

@ENTITY;2

-ENTITY-

:CARDINAL

@ATTRIB;3

80

:THAT

@EVENT;7

die.01

:ARG0

@PERSON;@mao_zedong

:TMP

@TIME-11

soon

:domain-of

@MD-10

MD_would

:MOD

@mod-12

past

The final step involves pretty printing the assem-
bled tree into the penman/AMR format, as well as
the removal of tense information and remapping role
names, e.g., :THAT changes to :ARG1, :TMP to
:time, :ON to :prep-on, :CARDINAL to :quant,
:ATTRIBUTE to :mod, and :POS to :poss.

( EVENT-5 / feel-02

:ARG0 ( PERSON-mao_zedong / PERSON

:name ( n-13 / name :op1 "mao" :op2 "zedong"))

:ARG1 ( EVENT-7 / die-01

:ARG0 PERSON-mao_zedong

:time ( TIME-11 / soon))

:time ( EVENT-4 / turn-02

:ARG0 PERSON-mao_zedong

:ARG1 ( ENTITY-2 / ENTITY

:quant ( ATTRIB-3 / 80))))

For comparison, the gold analysis from the task
training data for the running example is as follows:
( f / feel-02

:ARG0 ( p / person

:wiki "Mao_Zedong"

:name ( n / name :op1 "Mao" :op2 "Zedong"))

:ARG1 ( d / die-01

:ARG1 p

:time ( s / soon))

:time ( t / turn-02

:ARG1 p

:ARG2 ( t2 / temporal-quantity

:quant 80

:unit ( y / year))))

1151



Table 1: Parsing results on the evaluation data of SemEval-
2016 Task 8 and test data of LDC2015E86

Precision Recall F-score
Task 8 eval - - 0.47

Test 0.37 0.39 0.38

It can be seen that gaps remain. In particular, there is
no wikification to ground terms, and there is a lack
of named entity information. Establishment of ar-
guments as well as multiple roles for the same en-
tity is largely successful, although there are mis-
matches with die-01 and turn-02, reflecting that
the semantic analysis stage lacks information for
how PropBank roles for arguments should be allo-
cated.

5 Experiments

The method of this paper is evaluated on the shared
task evaluation data, which includes 1053 sentences.
The smatch score on the evaluation data is 0.47. Ta-
ble 1 also reports smatch score on the LDC2015E86
dataset, which includes 1371 test sentences. The
scores are calculated with Smatch v2.0.2 (Cai and
Knight, 2013), which evaluates the precision, re-
call, and F1 of the concepts and relations all to-
gether. The score for Task 8 is higher than the per-
formance on the LDC2015E86 test data. Reasons
for the difference include parser performance being
better on the evaluate data, and there being fewer
non-compositional aspects of representation in the
evaluate data.

6 Conclusion

To sum up, this paper has described a modularised
approach for building meaning representations, with
a key role for an adapted Tarskian satisfaction re-
lation for a Dynamic Semantics as the method to
integrate and connect information sourced from a
syntactic parser, semantic role labeller and named
entity recogniser. Task performance was limited by
not using the training data provided by the task, in
particular: lacking information to allocate PropBank
roles, neglecting wikification, and missing entity in-
formation to replicate non-compositional aspects of
the Abstract Meaning Representation (AMR) spec-
ification (Banarescu et al., 2015). Nevertheless,

this contribution indicates that AMRs are not far
removed from what a compositional semantics can
achieve, which is of interest for connecting to results
from the formal semantics literature, such as gain-
ing a treatment for quantification, as well as for re-
lating to “Sembank” resources built with Discourse
Representation Theory/Dynamic Semantics, such as
the Groningen Meaning Bank8 (Basile et al., 2012)
and Treebank Semantics Corpus9 (Butler and Yoshi-
moto, 2012).

Appendix: Implementation10

Assuming text is some original (multi-)sentence
segmented data, text.psd contains the output from
a parser trained on the PPCMBE, text.mate is out-
put from mateplus, and text.ner is output from the
Stanford Named Entity Recognizer, the following
pipeline creates fully parsed data as described in sec-
tion 2.
cat text.psd | add_functional text.mate |

add_sense text.mate | add_tsv_ner text.ner

| add_lemma text.mate | tree_select_word_2

> fullparse.psd

A script to recover the potentially multiple sen-
tence segmentation of text obscured by parsing is
achieved thus:
cat fullparse.psd | parse_discourse_split

text > segment.sh

The following pipeline achieves the semantic anal-
ysis of section 3, as well as the conversion to pen-
man/AMR notation of section 4.
cat fullparse.psd | prepare_PPCMBE |

segment.sh | parse_normalize -propbank

-free -bind | see_sct -free -reset |

run_sct -penman | penman_like_amr |

pretty_penman

Acknowledgements

This paper has benefitted from the comments of
three anonymous reviewers, discussions with Pas-
cual Martínez-Gómez, Masaaki Nagata, and Kei
Yoshimoto, and most particularly advice from the
SemEval Task 8 organiser Jonathan May, all of

8http://gmb.let.rug.nl
9https://github.com/ajb129/tscorpus

10All pipeline programs are from: https://github.com/ajb129

1152



which is very gratefully acknowledged. This re-
search is supported by the Japan Society for the Pro-
motion of Science (JSPS), Research Project Num-
ber: 15K02469.

References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina

Georgescu, Kira Griffitt, Ulf Hermjakob,
Kevin Knight, Philipp Koehn, Martha Palmer,
and Nathan Schneider. 2015. Abstract
Meaning Representation (AMR) 1.2.2 Spec-
ification. https://github.com/amrisi/amr-
guidelines/blob/master/amr.md.

V. Basile, J. Bos, K. Evang, and N.J. Venhuizen. 2012.
Developing a large semantically annotated corpus.
In Proceedings of the 8th Int. Conf. on Language
Resources and Evaluation. Istanbul, Turkey.

Claire Bonial, Olga Babko-Malaya, Jinho D. Choi, Jena
Hwang, and Martha Palmer. 2010. PropBank Anno-
tation Guidelines. Center for Computational Lan-
guage and Education Research, Institute of Cogni-
tive Science, University of Colorado at Boulder, 3rd
edn.

Anders Björkelund, Love Hafdell, and Pierre Nugues.
2014. Multilingual semantic role labeling. In
Proceedings of The Thirteenth Conference on Com-
putational Natural Language Learning (CoNLL),
pages 43–48. Boulder, Colorado.

Alastair Butler. 2010. The Semantics of Grammatical
Dependencies, vol. 23 of Current Research in the
Semantics/Pragmatics Interface. Bingley: Emer-
ald.

Alastair Butler. 2015. Linguistic Expressions and Se-
mantic Processing: A Practical Approach. Heidel-
berg: Springer-Verlag.

Alastair Butler and Kei Yoshimoto. 2012. Banking
meaning representations from treebanks. Linguistic
Issues in Language Technology - LiLT 7(1):1–22.

Shu Cai and Kevin Knight. 2013. Smatch: an evaluation
metric for semantic feature structures. In Proc. of
the ACL 2013.

Donald Davidson. 1967. The logical form of action sen-
tences. In N. Rescher, ed., The Logic of Decision
and Action. Pittsburgh: University of Pittsburgh
Press. Reprinted in: D. Davidson, 1980. Essays on
Actions and Events. Claredon Press, Oxford, pages
105–122.

Paul Dekker. 2012. Dynamic Semantics, vol. 91 of
Studies in Linguistics and Philosophy. Dordrecht:
Springer Verlag.

Tsaiwei Fang, Alastair Butler, and Kei Yoshimoto.
2014. Parsing Japanese with a PCFG treebank
grammar. In Proceedings of the Twentieth Annual
Meeting of the Association of Natural Language
Processing, pages 432–435. Sapporo, Japan.

Jenny Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information
into information extraction systems by gibbs sam-
pling. In Proceedings of the 43nd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL 2005), pages 363–370.

Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic: Introduction to Model-theoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Dordrecht: Kluwer.

Anthony Kroch, Beatrice Santorini, and Ariel Diertani.
2010. The Penn-Helsinki Parsed Corpus of Modern
British English (PPCMBE). Department of Linguis-
tics, University of Pennsylvania. CD-ROM, second
edition, (http://www.ling.upenn.edu/hist-corpora).

Seth Kulick, Anthony Kroch, and Beatrice Santorini.
2014. The Penn Parsed Corpus of Modern British
English: First parsing results and analysis. In Pro-
ceedings of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics (Short Papers),
pages 662–667. Baltimore, Maryland, USA: Asso-
ciation for Computational Linguistics.

Christian Matthiessen and John A Bateman. 1991. Text
generation and systemic-functional linguistics: ex-
periences from English and Japanese. Pinter Pub-
lishers.

Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
NAACL HLT 2007, pages 404–411.

Michael Roth and Kristian Woodsend. 2014. Com-
position of word representations improves semantic
role labelling. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 407–413. Doha, Qatar.

Beatrice Santorini. 2010. Annotation man-
ual for the Penn Historical Corpora and the
PCEEC (Release 2). Tech. rep., Depart-
ment of Computer and Information Science,
University of Pennsylvania, Philadelphia.
(http://www.ling.upenn.edu/histcorpora/annotation).

Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL 2003, pages 252–
259.

C. F. M. Vermeulen. 2000. Variables as stacks: A case
study in dynamic model theory. Journal of Logic,
Language and Information 9:143–167.

1153


