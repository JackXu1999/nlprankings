















































Acquiring Strongly-related Events using Predicate-argument Co-occurring Statistics and Case Frames


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1028–1036,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

Acquiring Strongly-related Events
using Predicate-argument Co-occurring Statistics and Case Frames

Tomohide Shibata and Sadao Kurohashi
Graduate School of Informatics, Kyoto University

Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan
{shibata,kuro}@i.kyoto-u.ac.jp

Abstract

This paper proposes a method for automat-
ically acquiring strongly-related events
from a large corpus using predicate-
argument co-occurring statistics and case
frames. The strongly-related events are
acquired in the form of strongly-related
two predicates with their relevant ar-
guments. First, strongly-related events
are acquired from predicate-argument co-
occurring statistics. Then, the remaining
argument alignment is performed by us-
ing case frames. We conducted experi-
ments using a Web corpus consisting of
1.6G sentences. The accuracy for the ex-
tracted event pairs was 96%, and the accu-
racy of the argument alignment was 79%.
The number of acquired event pairs was
about 20 thousands.

1 Introduction

Natural language understanding requires a wide
variety of knowledge. One is the relation be-
tween predicate and argument. This relation has
been automatically acquired in the form of case
frames from a large corpus, and is utilized for pars-
ing (Kawahara and Kurohashi, 2006). Another is
the relation between events. The relation between
events includes temporal relation, causality, and so
on, and is useful for coreference resolution (Bean
and Riloff, 2004) and anaphora resolution (Gerber
and Chai, 2010).

This paper extracts two strongly-related events.
Since the meaning of a predicate itself is often am-
biguous, an event is treated as predicate-argument
structure, namely the predicate with their relevant
arguments. An example of two strongly-related
events is shown below1:

1nom, acc, dat denotes nominative, accusative, dative,
respectively.

PA1 PA2
P1: pick up P2: bring

nom
acc

A1:{man, person}
A2:{purse}

⇒ nomacc
dat

A1:{man, person}
A2:{purse}
A3:{police}

In the above example, while the argument A1
and A2 appear both in PA1 and PA2 , the argu-
ment A3 appears only in PA2 . The argument A3
works for specifying the meaning of the predicate
P2. The method that automatically extracts sets of
events from unlabeled corpora (Chambers and Ju-
rafsky, 2008; Chambers and Jurafsky, 2009) relies
on the coreference relation of arguments, and thus
cannot extract an argument such as A3.

In languages where an argument is often omit-
ted, such as Japanese, sentences illustrating the
above two events usually occur in the following
form (for simplicity, the sentences are explained
in English):

(1) a. A man picked up a purse and brought (φ)
to the police.

b. (φ) picked up a purse and brought (φ) to
the police.

In the sentence (1-a), the argument A1 and A2 are
omitted in PA2 . Moreover, as an agent is specifi-
cally omitted, in the sentence (1-b), the argument
A1 in PA1 is also omitted. The coreference-based
method (Chambers and Jurafsky, 2008; Chambers
and Jurafsky, 2009) is hard to be applied to such a
language since an argument rarely appears in both
PA1 and PA2 .

Our proposed method extracts strongly-related
events in a two-phrase construct. First, since the
arguments, such as A2 and A3, which specify the
meaning of the predicate occur in at least one
predicate-argument structure, the co-occurrence
measure between “pick up purse” and “bring to
police” can be calculated from their occurrences.
Thus, we can regard “pick up purse” and “bring
to police”, whose mutual information is high, as
strongly-related events.

1028



Next, we identify the remaining arguments
by using case frames (Kawahara and Kurohashi,
2006). Case frames describe what kinds of ar-
guments each predicate takes and what kinds of
nouns can fill a case slot. With the similarity of
noun distribution between an argument in a case
frame assigned to PA1 and an argument in a case
frame assigned to PA2 , the remaining arguments
can be aligned. In the above example, acc A2
(“purse”) in PA1 corresponds to acc A2 in PA2 ,
and nom A1 (“man”, “person”) in PA1 corre-
sponds to nom A1 in PA2 .

The rest of this paper is organized as fol-
lows: Section 2 reviews related work. Section 3
describes an overview of our proposed method.
Section 4 describes predicate-argument structure
pairs extraction. Section 5 explains co-occurrence
statistics calculation between predicate-argument
structures using an association rule mining, and
Section 6 describes argument alignment based on
case frames. Section 7 reports on our experiments.

2 Related Work

We describe manually constructed resources for
event relations, and then explain automatic acqui-
sition methods from a corpus.

2.1 Manually Constructed Resource

Singh and Williams constructed a common sense
knowledge base concerned with ordinary human
activity (Singh and Williams, 2003). The knowl-
edge base consists of 80,000 propositions with
415,000 temporal and atemporal links between
propositions. Espinosa and Lieberman proposed
an EventNet, a toolkit for inferring temporal re-
lations between commonsense events from the
Openmind Commonsense Knowledge Base (Es-
pinosa and Lieberman, 2005).

Recently, Regneri et al. collect natural language
descriptions from volunteers over the Internet, and
compute a temporal script graph (Regneri et al.,
2010). They collected 493 event sequence de-
scriptions for the 22 scenarios such as “eating in a
fast-food restaurant” using the Amazon Mechani-
cal Turk.

2.2 Automatic Acquisition of Event Relation
from Corpus

There are several types in the event relation acqui-
sition. One is the inference rule acquisition. Lin
and Pantel extended the distributional hypothesis

on words, and calculated two paths in a depen-
dency tree (Lin and Pantel, 2001). If two paths
tend to link the same sets of words, these are re-
garded as being similar. For example, they calcu-
lated the similarity between “X is the author of Y”
and “X wrote Y”.

Another type is the script-like knowledge ac-
quisition. Chambers and Jurafsky learn narrative
schemas, which mean coherent sequences or sets
of events, from unlabeled corpora (Chambers and
Jurafsky, 2008; Chambers and Jurafsky, 2009).
This method extracts two events that share a par-
ticipant, called a protagonist. Since these meth-
ods rely on the coreference analysis result, they
are hard to be applied to languages where omitted
arguments or zero anaphora are often utilized.

Kasch and Oates proposed a method for ex-
tracting script-like structures from collections of
Web documents (Kasch and Oates, 2010). Their
method is topic-driven, and the experiment was
performed on only one situation eating at a restau-
rant.

There is some work for acquiring two related
events taking argument sharing approach (Tori-
sawa, 2006; Abe et al., 2008). Torisawa proposed
a method for acquiring inference rules with tempo-
ral constrains by using verb-verb co-occurrences
in Japanese coordinated sentences and verb-noun
co-occurrences (Torisawa, 2006). Abe et al. ac-
quire semantic relations between events by cou-
pling the pattern-based relation-oriented approach
and the anchor-based argument-oriented approach
(Abe et al., 2008). Their method first acquires
candidate predicate pairs by exploiting a pattern-
based method, and then seeks anchors indicative
of the shared argument. If anchors are found, the
predicate pair is verified. These methods can ac-
quire only event relations that have a shared argu-
ment.

3 Overview of Our Proposed Method

This paper focuses on Japanese, and extracts two
strongly-related events in the form as shown in
Figure 1. Figure 2 depicts an overview of our pro-
posed method. First, pairs of predicate-argument
structures (PAs) that have a dependency relation
are extracted from a Web corpus. Then, from a
large number of extracted pair of PAs , strongly-
related two predicates with their relevant argu-
ments are extracted. Since the meaning of a pred-
icate itself is often ambiguous, the predicate with

1029



PA1 PA2

A1:
{ otoko, hito, ...

(man) (person)
}

ga

A2:
{ saifu, ...

(purse)
}

wo

hirou
(pick up) ⇒

A1:
{ otoko, hito, ...

(man) (person)
}

ga

todokeru
(bring)A2:

{ saifu, ...
(purse)

}
wo

A3:
{ keisatsu, ...

(police)
}

ni

Figure 1: An example of strongly-related events. (ga (nom), wo (acc), and ni (dat) are Japanese case
markers.)

Web 
corpus

PA pairs extraction
driver-ga saifu-wo hirou todokeru

keisatsu-ni todokerukare-ga saifu-wo hirou

…

saifu-wo hirou keisatsu-ni todokeru

predicate-argument co-occurring 
statistics calculation

(he-nom purse-acc pick up) (police-dat bring)

keisatsu-ni todokerusaifu-wo hirou

(driver-ga)

PA1 PA2

saifu-wo hirou keisatsu-ni todokeru

ga

wo

hirou: 10
otoko(man),  
onnanoko(girl), …

saifu(purse), 
denwa(phone), …

ga

wo

todokeru: 20

saifu(purse), 
kane(money),…

argument alignment
based on caseframe

ni keisatsu(police),…

otoko(man),  
hito(person), …

A1 : {otoko, hito, …} ga
A2 : {saifu, …} wo hirou

PA1
A1 : {otoko, hito, …} ga
A2 : {saifu, …} wo
A3 : {keisatsu} ni

PA2

todokeru

Figure 2: An overview of our proposed method.

their relevant arguments is extracted. For exam-
ple, whereas the pair between “hirou (pick up)”
and “todokeru (bring)” is not strongly related, the
pair between “saifu-wo (purse-acc) hirou” and
“keisatsu-ni (police-dat) todokeru” is. To extract
the predicate with relevant arguments, the point-
wise mutual information of the pair of arbitrary
PAs is calculated, and the pair whose pointwise
mutual information is high is regarded as strongly-
related events. We adopt association rule mining
(Agrawal et al., 1993) for the calculation of co-
occurrence statistics between PAs effectively.

Next, the remaining arguments are identified us-
ing case frames. For the predicate “hirou(pick
up)” whose argument takes “saifu(purse)-wo”,
what kinds of arguments are taken can be obtained
from case frames. As shown in Figure 2, in the
case frame 10 of “hirou”2, where the argument
wo takes “saihu”, “denwa”, the argument ga takes
“otoko”, “onnanoko”, and so on. Similarly, in the
case frame 20 of “todokeru”, where the argument
ni takes “keisatsu”, the argument ga takes “otoko”,

2Each predicate has several case frames, and case frame
10 of “hirou” means 10th case frame for the predicate “hi-
rou”.

“onnanoko”, and so on, and the argument wo takes
“saihu” and so on. With the similarity of noun dis-
tribution between an argument in PA1 and one in
PA2 , the remaining arguments can be aligned.

4 Predicate-Argument Structure Pairs
Extraction

Strongly-related events appear in the form where
they have a dependency relation with a variety
of expressions (especially clause relation) in a
text. For example, the event “saifu(purse)-wo hi-
rou(pick up)” and the event “keisatsu(police)-ni
todokeru(bring)” appear as follows:

(2) saifu-wo
purse-acc

hiro-te
pick up and

keisatsu-ni
police-dat

todoke-ta
brought

((A man) picked up a purse, and brought it to
a police.)

We extract two strongly-related events from a
large number of pairs of two PAs that have a de-
pendency relation. From parsing results, a pair of
PAs that have a dependency relation is first ex-
tracted. The extracted arguments are ga (nom),
wo (acc), and ni (dat). If a predicate has an at-

1030



Table 1: Examples of clause relation and predicate-argument structure extraction.
clause relation example sentence PA1 PA2

sequence hachi-ni sa-sare te hareta hachi-ni sa-sareru hareru
(bee-dat)(bitten) (swollen)

cause hachi-ni sa-sareta node hareta hachi-ni sa-sareru hareru
condition hachi-ni sa-sareru to hareta hachi-ni sa-sareru hareru
purpose suibun-wo tobasu tame-ni kanetsu-suru kanetsu-suru suibun-wo tobasu

(water-acc) (drain) (heat)
contradiction hachi-ni sa-sareta keredo hare-nakatta hachi-ni sa-sareru hareru
simultaneous shower-wo abi nagara ha-wo migaku shower-wo abi ha-wo migaku

(take) (teeth-acc) (brush)

Table 2: Examples of word class and its words.
class words

77 hachi (bee), ka (mosquito), · · ·
105 dress, ishou (cloth), suit, · · ·
502 address, bangou (number), ID, · · ·
956 juugeki(shooting), shuugeki(attack), · · ·

1829 kenshuu (training), intern, · · ·
1901 douro (road), kokudou (national highway), · · ·

tribute, such as negation, causative, and passive,
the attribute is attached to the predicate as a flag.
Table 1 shows examples of clause relation and
predicate-argument structure extraction.

We consider PA pairs that occur with a clause
relation sequence as standard. In the case of clause
relation purpose, PA pairs occur in the following
form: PA2 tame-ni PA1 , and so PA1 and PA2
are transposed. In the case of the clause relation
contradiction, the negation flag in the predicate of
PA2 is reversed.

Argument Generalization

An argument is generalized to its word class
so as to alleviate the problem of data sparseness.
As a word class, a large-scale clustering result
of verb-noun dependency relations (Kazama and
Torisawa, 2008) is used. The number of word
class is 2,000, and this word class covers one mil-
lion noun phrases. Table 2 shows examples of a
word class and its words.

In pairs of the extracted PAs , the noun n is re-
placed with the word class 〈c〉 for which the prob-
ability P (c|n) is maximal. For example, “PA1 :
ka(mosquito) ni sa-sareru (bitten), PA2 : hareru
(swollen)” is changed to “PA1 : 〈77〉 ni sa-sareru,
PA2 : hareru” since “ka” belongs to the word class
〈77〉. In the same way, “PA1 : hachi(bee) ni sa-
sareru, PA2 : hareru” is changed to “PA1 : 〈77〉 ni
sa-sareru, PA2 : hareru”, and thus, these two PAs
can be identical.

5 Co-occurrence Statistics Calculation
between Predicate-Argument
Structures

Given a lot of PAs , as extracted in Section 4,
the co-occurrence statistics between PAs is calcu-
lated. Since the number of pairs of arbitrary PAs
is enormous, a question that arises is how to obtain
related PAs effectively. To solve this problem,
we adopt association rule mining (Agrawal et al.,
1993) for the calculation of co-occurrence statis-
tics between PAs . The association rule mining
method can efficiently seek candidate items that
satisfy specific conditions.

5.1 Association Rule Mining

Association rule mining is a method for discover-
ing significant rules in a large database (Agrawal
et al., 1993). This method is originally designed to
discover rules such as “a customer who buys dia-
pers tends to buy beer” in customer transactions.

Let I = I1, I2, · · · , Im be a set of binary
attributes, called items. Transaction t is de-
fined as a set of items (t ⊆ I), and transac-
tion database T is defined as a set of transactions
(T = t1, t2, · · · , tn).

A rule is defined as an implication of the form
X ⇒ Y where X, Y ⊆ I and X ∩ Y = φ. This
signifies “if X occurs, Y tends to occur”. The set
of items X and Y are called antecedent (left-hand
side, lhs) and consequent (right-hand side, rhs) of
the rule respectively. For every rule, the following
three measures are defined:

support(X ⇒ Y ) = C(X ∪ Y )|T | (1)

confidence(X ⇒ Y ) = support(X ⇒ Y )
support(X)

(2)

lift(X ⇒ Y ) = confidence(X ⇒ Y )
support(Y )

, (3)

1031



Table 3: Examples of transaction data. (One line represents a transaction.)
PA1 PA2

arguments predicate arguments predicate
saifu(purse)-wo hirou (pick up) keisatsu(police)-ni todokeru (bring)
kare(he)-ga, saifu-wo hirou keisatsu-ni todokeru
saifu-wo hirou todokeru

hirou keisatsu-ni todokeru
· · ·

saifu-wo hirou tewatasu (hand)
saifu-wo hirou kare(he)-ni tewatasu
otoko(man)-ga, saifu-wo hirou tewatasu

· · ·

where C(X) represents the number of transactions
containing the item X .

The support is defined as the fraction formed
the number of transactions that contain the item-
set X and the total number of transactions in the
database. The confidence is defined as the fraction
formed from the transactions that contain X ∪ Y
and the transactions that contain X . The lift corre-
sponds to pointwise mutual information between
X and Y .

Apriori algorithm (Borgelt and Kruse, 2002) is
one of the well-known implementations for asso-
ciation rule mining. This algorithm exploits the
observation that no superset of an infrequent item-
set can be frequent, and uses breadth-first search
and a tree structure to seek candidate items.

The input for Apriori algorithm is transaction
data, the minimum support, and minimum confi-
dence, and the algorithm enumerates all rules that
satisfy the specified conditions.

5.2 Apriori Algorithm Application to
Co-occurrence Calculation

The Apriori algorithm is applied to the calculation
of co-occurrence statistics between PAs . An item
introduced in Section 5.1 corresponds to a predi-
cate or an argument, and a transaction is obtained
from a pair of PAs . Examples of transaction data
are shown in Table 3.

Since the rules we want to extract are supposed
to satisfy the following conditions:

• X (left-hand side) consists of a predicate of
PA1 , and zero or more arguments in PA1

• Y (right-hand side) consists of a predicate of
PA2 , and zero or more arguments in PA2 ,

all the rules that do not satisfy these conditions are
discarded. Among those that do, the rule for which
the lift is higher than lift-min and less than lift-max
is adopted. It is well-known that the pointwise

mutual information (which corresponds to lift) for
which the frequency is low gets extremely high,
and thus rules for which the lift is greater than lift-
max are discarded.

The Apriori algorithm naturally judges which
argument is relevant for each predicate pair. For
example, from the transaction data shown in Table
3, the following rule is obtained:

1. saifu-wo hirou ⇒ keisatsu-ni todokeru

2. saifu-wo hirou ⇒ tewatasu

The first rule implies that for the predicate pair
“hirou” and “todokeru”, “saifu-wo” for the pred-
icate in PA1 and “keisatsu-ni” for the predicate
in PA2 are relevant. Similarly, the second rule
implies that for the predicate pair “hirou” and
“tewatasu”, “saifu-wo” for the predicate in PA1
is relevant.

6 Argument Alignment based on Case
Frames

As mentioned in Introduction, since an argu-
ment is often omitted in the extracted predicate-
argument pairs, there is usually a lack of argu-
ments in the extracted rules as described in the
previous section. In the following rule, the argu-
ment of the wo case in PA1 corresponds to the wo
case in PA2 , and the argument that includes nouns
such as “otoko(man)”, “hito(person)” acts for the
ga case both in PA1 and PA2 .

saifu-wo hirou ⇒ keisatsu-ni todokeru
Such alignment between arguments can be per-

formed by case frames. The case frames are con-
structed automatically by clustering similar predi-
cate usages from a raw corpus, and thus each pred-
icate has several case frames. Examples of the
case frames are shown in Table 4. When both a

1032



Table 4: Examples of the automatically con-
structed case frames.

verb casemarker examples

hirou:1 ga josei(lady), hito(person), · · ·
(pick up) wo taxi, kuruma(car), · · ·

· · ·
hirou:10 ga otoko(man), onnanoko(girl), · · ·
(pick up) wo saifu(purse), denwa(phone) · · ·

· · ·
todokeru:1 ga staff, syokuin(staff), · · ·
(deliver) wo jyohou(information), news, · · ·

· · ·
todokeru:20 ga otoko(man), hito(person), · · ·
(bring) wo saifu(purse), kane(money), · · ·

ni keisatsu(police), · · ·
· · ·

case in cf1 assigned to PA1 and a case in cf2 as-
signed to PA2 have a similar distribution of exam-
ples, the case in PA1 and the case in PA2 can be
aligned.

The best combinations of the case frame in both
PA1 and PA2 and the best alignment of cases are
determined as follows:

1. If there is an argument, select case frames
corresponding to the argument, otherwise, all
case frames are candidates. In the above ex-
ample, while in PA1 the case frame 10 is se-
lected according to the argument for the case
wo (“saifu”), in PA2 the case frame 20 is se-
lected according to the case ni (“keisatsu”).

2. Choose the best case frame pairs that maxi-
mize the following score:

argmax
cf1,cf2

maxa
∑

a∈a
sim(arg1, a(arg1)) (4)

where a denotes the alignment of case com-
ponents between PA1 and PA2 , arg1 de-
notes an argument in PA1 , a(arg1) denotes
an argument in PA2 that aligned with arg1,
and sim denotes the cosine similarity of the
case components distribution between arg1
and a(arg1). In the example, the alignment
between the case ga of the case frame 10 in
PA1 and the case ga of the case frame 20 in
PA2 , and the case wo in PA1 and the case wo
in PA2 is performed.

7 Experiments

7.1 Settings
Approximately 100 million Japanese Web pages
were used to extract strongly-related events. These

Table 5: Accuracy of extracted rule and the argu-
ment alignment.

extracted rule correct incorrect96(96.0%) 4(4.0%)
argument correct incorrect
alignment 76(79.1%) 20(20.8%) –

pages include 6 billion sentences, containing 100
billion words. Owing to the presence of many du-
plicate pages on the Web, such as mirror pages,
duplicate sentences were discarded. Thus, 1.6 bil-
lion sentences containing approximately 25 bil-
lion words were acquired. The average number of
characters and words in a sentence were 28.3 and
15.6, respectively.

The Web corpus was processed using the
Japanese Morphological Analyzer JUMAN3 and
the Japanese parser KNP4, and pairs of PAs were
extracted. The number of extracted PAs was ap-
proximately 400 million.

In the application of Apriori algorithm ex-
plained in Section 5.2, the minimum support, con-
fidence was set to 1.0 × 10−7, 1.0 × 10−3 respec-
tively, and lift-min, lift-max was set to 10, 10,000
respectively.

The case frames were automatically constructed
from the Web corpus consisting 1.6G sentences
with a method proposed by (Kawahara and Kuro-
hashi, 2006). For 31,000 predicates, case frames
were constructed; the average number of case
frames of a predicate was 25; the average number
of case slots of a case frame was 4.7.

7.2 Result and Discussion

7.2.1 Evaluation of Co-occurrence Statistics
Calculation

We acquired approximately 20,000 rules described
in Section 5, and evaluated the acquired rules. We
chose 100 rules at random, and evaluated whether
each is valid. The upper part in Table 5 shows the
accuracy, and we found 96 valid rules of the 100,
and the accuracy was 0.96. Examples of the ex-
tracted rules and its evaluation are shown in Table
6. A major error is the parsing error. In the exam-
ple (8) in Table 6, the predicate “ataru” in PA1 is
correctly a part of function expressions.

7.2.2 Evaluation of Argument Alignment
We chose 96 instances that were judged as correct
in the previous section, and calculated the accu-

3http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
4http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp-e.html

1033



Table 6: Examples of acquired rules by the association rule mining method (Section 5).
PA1 PA2 evaluationargument predicate argument predicate

(1) teiin(capacity) ni tassuru(reach) ⇒ shimekiru(close) correct
(2) daigaku(university) wo sotsugyo(graduate) ⇒ kaisha(company) ni shuusyoku(get a job) correct
(3) tentou(fall down) ⇒ kossetsu(fracture) correct
(4) nominate-sareru(nominate) ⇒ jusyo(win an award) correct
(5) tazuneru(visit) ⇒ hanashi(talk) wo ukagau(hear) correct
(6) purezento(present) ⇒ yorokoba-reru(delighted) correct
(7) kekkon(get married) ⇒ kodomo(child)-ga iru(have) correct
(8) riyou(use)-ni ataru (at) ⇒ touroku(registration)-ga hitsuyou(necessary) incorrect

Table 7: Examples of acquired strongly-related events. (The underlined arguments indicate the one
acquired by the association rule mining method. Ids in the left column correspond to ones in Table 6. )

PA1 PA2 evaluationargument predicate argument predicate

(1)
A1:

{ boshuu, moushikomi, ...
(invitation) (application)

}
ga tassuru

(reach) ⇒ A1:
{ boshuu, moushikomi, ...

(invitation) (application)
}

wo shimekiru(close) correct
A2:

{ teiin
(capacity)

}
ni

(2)
A1:

{ watashi, kodomo, ...
(I) (child)

}
ga sotsugyo

(graduate) ⇒
A1:

{ watashi, kodomo, ...
(I) (child)

}
ga shuusyoku

(get a job)
correct

A2:
{ daigaku

(university)
}

wo A3:
{ kaisha

(company)
}

ni

(3) A1:
{ musuko, kodomo, ...

(son) (child)
}

ga tentou
(fall down)

⇒ A1:
{ musuko, kodomo, ...

(son) (child)
}

ga kossetsu
(fracture)

correct

(4)
A1:

{ sakuhin, ...
(product)

}
ga nominate-

sareru
(nominate)

⇒
A1:

{ sakuhin, ...
(product)

}
ga jusyo

(win an
award)

correct

A2:
{ sho, yuushuu-sho, ...

(prize) (grand prix)
}

ni A2:
{ sho, yuushuu-sho, ...

(prize) (grand prix)
}

wo

(5)
A1:

{ watashi, hito, ...
(I) (person)

}
ga

A2:
{ sensei, shachou, ...

(teacher) (chief)
}

wo

tazuneru
(visit) ⇒

A1:
{ watashi, hito, ...

(I) (person)
}

ga

ukagau
(hear) correctA2:

{ sensei, shachou, ...
(teacher) (chief)

}
ni

A3:
{ hanashi, ...

(talk)
}

wo

(6)
A1:

{ kanojo, josei, ...
(she) (lady)

}
ga purezento

(present) ⇒
A2:

{ shouhin, hana, ...
(goods) (flower)

}
ga yorokoba-

reru
(delighted)

incorrect
A2:

{ shouhin, hana, ...
(goods) (flower)

}
wo A1:

{ kanojo, josei, ...
(she) (lady)

}
ni

(7) A1:
{ kodomo, ...

(child)
}

ga kekkon
(get married)

⇒ A1:
{ kodomo, ...

(child)
}

ga iru
(have)

incorrect

racy of the argument alignment. The bottom part
in Table 5 shows the accuracy, and we found 76
of 94 were valid, and the accuracy was 0.791. Ta-
ble 7 shows examples of acquired strongly-related
events. A major error is that the case component
distribution between two cases in a PA is very sim-
ilar. In the example (6), the alignment shown in
Figure 3 is correct. This error was caused by the
fact that the case ga and the case ni in PA1 and
the case ga and the case ni in PA2 include nouns
representing an agent.

Another error is that some constructed case
frames do not have an indispensable case slot. In
the example (7), the alignment shown in Figure 4
is correct. This error is due to the fact that the

assigned case frame to PA2 does not have the ni
case. To cope with this problem, we are planning
to increase the size of Web corpus for the case
frames compilation.

7.2.3 Comparison with Coreference-based
Method

Our method was compared with the coreference-
based method (Chambers and Jurafsky, 2008).
Since the accuracy of coreference resolution is not
high (Sasano et al. report an F-score of approxi-
mately 0.75 in a newspaper domain (Sasano et al.,
2007)), if a noun appears twice in a Web page, and
it fills a syntactic relation of the predicate w and
the predicate v, the noun is regarded as a corefer-

1034



A1:
{ watashi, hito, ...

(I) (person)
}

ga

purezento
(present) ⇒

A2:
{ shouhin, hana, ...

(goods) (flower)
}

ga

A3:
{ kanojo, josei, ...

(she) (lady)
}

ni

yorokoba-reru
(delighted)A2:

{ shouhin, hana, ...
(goods) (flower)

}
wo

A3:
{ kanojo, josei, ...

(she) (lady)
}

ni

Figure 3: The correct alignment of (6) in Table 7.

A2:
{ watashi, hito, ...

(I) (person)
}

ga kekkon(get married) ⇒
A2:

{ watashi, hito, ...
(I) (person)

}
ni iru

(have)
A1:

{ kodomo, ...
(child)

}
ga

Figure 4: The correct alignment of (7) in Table 7.

Table 8: Comparison of our method with the
coreference-based method. (The covered ratio is
the fraction formed the number of the acquired
noun by the coreference-based method and the
number of the nouns in the aligned argument by
our method.)

case
in PA1

case
in PA2

covered ratio

ga ga 0.163 (3,768 / 23,180)
ga wo 0.282 (549 / 1,944)
ga ni 0.176 (474 / 2,689)
wo ga 0.272 (753 / 2,764)
wo wo 0.483 (7,106 / 14,713)
wo ni 0.321 (1,054 / 3,284)
ni ga 0.163 (344 / 2,113)
ni wo 0.338 (1,042 / 3,086)
ni ni 0.282 (549 / 1,944)

ence, following the method proposed by (Abe et
al., 2008). The PMI score was calculated as fol-
lows:

pmi(e(w, d), e(v, g)) = log
P (e(w, d), e(v, g))

P (e(w, d))P (e(v, g))
(5)

where e(w, d) is the verb/dependency pair w and
d, and d and g have the coreferent relation.

In our acquired rules, we examined whether the
k-most frequent noun in the aligned argument can
be covered by the coreference-based method. In
our experiment, k was set to be 5. The result is
shown in Table 8. The number was classified ac-
cording to the case in PA1 and in PA2 . We found
that most of the nouns in aligned argument cannot
be acquired by the coreference-based method. Es-
pecially, the covered ratio of the pair of the case ga
in PA1 and the case ga in PA2 was relatively low,
which often corresponds to agent. In Japanese,
since an agent is often omitted, it is hard to be
acquired by the coreference-based method. How-
ever, our method can identify its use by using case
frames.

X ga kossetsu(break a bone)

X ga nyuin(enter hospital)

X ga shujutsu(operate) wo 
 ukeru(given) X ga taiin(leave hospital)

X ga taichou(condition) wo 
 kuzusu(get out of)

Figure 5: Network structure between events con-
cerned with “enter hospital”. (X: {kodomo(child),
musume(daughter), · · ·})

7.2.4 Event Network Structure
Figure 5 is an example of a network struc-
ture between events concerned with “enter hos-
pital”, which is constructed from strongly-
related events obtained by our proposed method.
Anchor/coreference-based method cannot ac-
quire the argument “taichou(condition)-wo” that
presents in one node (which means this argument
is shared by no events). In contrast, our proposed
method can acquire such an argument.

8 Conclusion

This paper proposed a method for automatically
acquiring strongly-related events from a large cor-
pus using predicate-argument co-occurring statis-
tics and case frames. Our method first extracted
pairs of predicate argument structures that have
a dependency relation are extracted from a Web
corpus. Then, two events whose pointwise mu-
tual information is high is extracted as strongly-
related. We adopt association rule mining for
the calculation of co-occurrence statistics between
predicate-argument structures effectively. Then,
the argument alignment was performed by using
case frames.

For future work, since the acquired events in-
clude several relations such as temporal relation,
causality, and means, we are planning to classify
the relations automatically. Acquired event rela-
tions would then be utilized in Recognizing Tex-
tual Entailment (RTE) and Question Answer (QA)
tasks.

1035



References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.

Two-phased event relation acquisition: coupling the
relation-oriented and argument-oriented approaches.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
1–8.

Rakesh Agrawal, Tomasz Imielinski, and Arun Swami.
1993. Mining association rules between sets of
items in large databases. In Proceedings of the
ACM-SIGMOD 1993 International Conference on
Management of Data (1993), pages 207–216.

David Bean and Ellen Riloff. 2004. Unsupervised
learning of contextual role knowledge for corefer-
ence resolution. In HLT-NAACL 2004: Main Pro-
ceedings, pages 297–304.

Christian Borgelt and Rudolf Kruse. 2002. Induction
of association rules: Apriori implementation. In
Proceedings of 15th Conference on Computational
Statistics, pages 395–400.

Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08: HLT, pages 789–797.

Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 602–610.

Jose Espinosa and Henry Lieberman. 2005. EventNet:
Inferring temporal relations between commonsense
events. In Proceedings of the 4th Mexican Inter-
national Conference on Artificial Intelligence, pages
61–69.

Matthew Gerber and Joyce Chai. 2010. Beyond Nom-
Bank: A study of implicit arguments for nominal
predicates. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1583–1592.

Niels Kasch and Tim Oates. 2010. Mining script-
like structures from the web. In Proceedings of the
NAACL HLT 2010 First International Workshop on
Formalisms and Methodology for Learning by Read-
ing, pages 34–42.

Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for japanese
syntactic and case structure analysis. In Proceed-
ings of the HLT-NAACL2006, pages 176–183.

Jun’ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT, pages 407–415.

Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343–360.

Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 979–988.

Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2007. Improving coreference resolution us-
ing bridging reference resolution and automatically
acquired synonyms. In Discourse Anaphora and
Anaphor Resolution Colloquium, pages 125–136.

Push Singh and William Williams. 2003. Lifenet: A
propositional model of ordinary human activity. In
Proceedings of Workshop on Distributed and Col-
laborative Knowledge Capture.

Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of Human Language Technology Con-
ference/North American chapter of the Associa-
tion for Computational Linguistics annual meeting
(HLT-NAACL06), pages 57–64.

1036


