



















































Improving Neural Language Models with Weight Norm Initialization and Regularization


Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 93–100
Belgium, Brussels, October 31 - Novermber 1, 2018. c©2018 Association for Computational Linguistics

https://doi.org/10.18653/v1/W18-64010

Improving Neural Language Models with Weight Norm Initialization and
Regularization

Christian Herold∗ Yingbo Gao∗
Human Language Technology and Pattern Recognition Group

Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany

<surname>@i6.informatik.rwth-aachen.de

Hermann Ney

Abstract

Embedding and projection matrices are com-
monly used in neural language models (NLM)
as well as in other sequence processing net-
works that operate on large vocabularies. We
examine such matrices in fine-tuned language
models and observe that a NLM learns word
vectors whose norms are related to the word
frequencies. We show that by initializing the
weight norms with scaled log word counts, to-
gether with other techniques, lower perplexi-
ties can be obtained in early epochs of train-
ing. We also introduce a weight norm regular-
ization loss term, whose hyperparameters are
tuned via a grid search. With this method, we
are able to significantly improve perplexities
on two word-level language modeling tasks
(without dynamic evaluation): from 54.44 to
53.16 on Penn Treebank (PTB) and from 61.45
to 60.13 on WikiText-2 (WT2).

1 Introduction

A language model (LM) measures how likely a
certain sequence of words is for a given language.
It does so by calculating the probability of occur-
rence of that sequence, which can be learned from
monolingual text data. Many models in machine
translation and automatic speech recognition ben-
efit from the use of a LM (Corazza et al., 1995;
Peter et al., 2017).

While count-based LMs (Katz, 1987; Kneser
and Ney, 1995) provided the best results in
the past, substantial improvements were achieved
with the introduction of neural networks in the
field of language modeling (Bengio et al., 2003).
Different types of architectures such as feedfor-
ward neural networks (Schwenk, 2007) and re-
current neural networks (Mikolov et al., 2010)
have since been used for language modeling.
Currently, variants of long short-term memory
∗Equal contribution. Ordering determined by coin flipping.

(LSTM) (Hochreiter and Schmidhuber, 1997) net-
works give the best results on popular language
modeling tasks (Yang et al., 2018).

In natural language processing, words are typ-
ically represented by high-dimensional one-hot
vectors. To reduce dimensionality and to be able
to learn relationships between words, they are
mapped into a lower-dimensional, continuous em-
bedding space. Mathematically, this is done by
multiplying the one-hot vector with the embed-
ding matrix. Similarly, to receive a probability dis-
tribution over the vocabulary, a mapping from an
embedding space is performed by a projection ma-
trix followed by a softmax operation. These two
matrices can be tied together in order to reduce
the number of parameters and improve the results
of NLMs (Inan et al., 2017; Press and Wolf, 2017).

Since the row vectors in the embedding and pro-
jection matrices are effectively word vectors in a
continuous space, we investigate such weight vec-
tors in well-trained and fine-tuned NLMs. We ob-
serve that the learned word vector generally has
a greater norm for a frequent word than an in-
frequent word. We then specifically examine the
weight vector norm distribution and design ini-
tialization and normalization strategies to improve
NLMs.

Our contribution is twofold:

• We identify that word vectors learned by
NLMs have a weight norm distribution that
resembles logarithm of the word counts. We
then correspondingly develop a weight initial-
ization strategy to aid NLM training.

• We design a weight norm regularization loss
term that increases the generalization ability
of the model. Applying this loss term, we
achieve state-of-the-art results on Penn Tree-
bank (PTB) and WikiText-2 (WT2) language
modeling tasks.

93

https://doi.org/10.18653/v1/W18-64010


2 Related Work

Melis et al. (2018) investigated different NLM ar-
chitectures and regularization methods with the
use of a black-box hyperparameter tuner. In par-
ticular, the LSTM architecture was compared to
two more recent recurrent approaches, namely re-
current highway networks (Zilly et al., 2017) and
neural architecture search (Zoph and Le, 2017).
They found that the standard LSTM architecture
outperforms other models, if properly regularized.

Merity et al. (2017a) used various regularization
methods such as activation regularization (Merity
et al., 2017b) in a LSTM model. They also intro-
duced a variant of the averaged stochastic gradient
method, where the averaging trigger is not tuned
by the user but relies on a non-monotonic condi-
tion instead. With these and further regulariza-
tion and optimization methods, improved results
on PTB and WT2 were achieved.

To further improve this network architecture,
Yang et al. (2018) introduced the mixture of soft-
maxes (MoS) model, claiming that the calculation
of the output probabilities with a single softmax
layer is a bottleneck. In their approach, several
output probabilities are calculated and then com-
bined via a weighted sum. The LSTM-MoS ar-
chitecture provides state-of-the-art results on PTB
and WT2 at the time of writing and is used as the
baseline model for comparisons in this work.

Other works proposed to tie the embedding and
projection matrices. Press and Wolf (2017) inves-
tigated the effects of weight tying, analyzed up-
date rules after tying and showed that tied matri-
ces evolve in a similar way as the projection ma-
trix. Inan et al. (2017) were motivated by the fact
that with a classification setup over the vocabulary,
inter-word information is not utilized to its full po-
tential. They also provided theoretical justification
on why it is appropriate to tie the above-mentioned
matrices.

Besides using the word embedding matrix, there
are other approaches to represent word sequences.
Zhang et al. (2015) proposed a new embedding
method called fixed-sized ordinally-forgetting en-
coding (FOFE), which allows them to encode
variable-length sentences into fixed-length vectors
almost uniquely.

Additionally, Salimans and Kingma (2016) in-
troduced a weight normalization reparametriza-
tion trick on weight matrices, which separates the
norm and the angle of a vector. This can speed

up the convergence of stochastic gradient descent
and also allows for explicit scaling of gradients in
the amplitude and direction. They also discussed
the connections between weight normalization and
batch normalization.

On top of one-hot representations of words, Irie
et al. (2015) used additional information to rep-
resent word sequences. It is shown that the use
of long-context bag-of-words as additional feature
for language modeling can narrow the gap be-
tween feed-forward NLMs and recurrent NLMs.

3 Neural Language Modeling

In NLM the probability of a word sequence
xt1 = x1x2...xt is decomposed as

P (xt1) =
t∏

j=1

P (xj |xj−1j−n+1) (1)

so that the (n− 1) preceding words xj−1j−n+1 are
considered for the prediction of the next word xj .
This is typically done by using a recurrent neural
network, e.g. a stack of LSTM layers, to encode
the input sequence as

ht = LSTM(ET [xt−n+1, xt−n+2, ..., xt−1]) (2)

where ET is the transposed embedding ma-
trix, [xt−n+1, xt−n+2, ..., xt−1] are the one-hot en-
coded preceding words and the LSTM() function
returns the last hidden state of the last LSTM layer.
The probability distribution over the next word xt
is then calculated as

P (xt = xk|ht) =
exp(Wkht)∑V
j=1 exp(Wjht)

(3)

with V being the vocabulary size, k = 1, 2, ..., V ,
andWk being the k-th row vector in the projection
matrix W .
For training the neural network, the cross-entropy
error criterion, which is equivalent to the maxi-
mum likelihood criterion, is used. For the i-th se-
quence of words xti1 , the cross-entropy loss Li is
defined as

Li = −logP (xti = xyi |hti) (4)

with yi being the true label of xti . The total loss is
then calculated as

L =
1

N

N∑

i=1

Li (5)

94



where N is the total number of sequences. A
language model is normally scored by perplexity
(ppl). For a given test corpus xT1 = x1x2...xT , the
ppl is calculated as

ppl = P (xT1 )
− 1

T (6)

which is a measurement on how likely a given sen-
tence is, according to the prediction of the model.

In the above formulation, we have an embedding
matrix E and a projection matrix W . When the
two matrices are tied and one-hot vectors are used
to represent words, the rows of these matrices are
then the word vectors of the corresponding words.
Particularly, we focus on the norms of the row vec-
tors and study their relationship with word counts
and how to regularize them.

4 Weight Norm Initialization

We first train models on PTB and WT2 as de-
scribed in (Yang et al., 2018) and plot the norms
of learned weight vectors of the embedding matrix
in Figure 1.

When the words are ranked by their counts and
placed on the x-axis from frequent to infrequent,
it can be seen that the word vector norms follow
a downward trend as well. Log unigram counts
are also plotted for comparison. As can be seen,
the norm distribution follows a similar trend as the
log counts. It is important to note, that the logit for
word xk and context ht is calculated as Wkht (see
Equation 3), which can be rewritten as

Wkht = ‖Wk‖ ‖ht‖ cos(θ) (7)

where θ denotes the angle between Wk and ht.
Therefore, one intuition from the aforementioned
observation is that, for a frequent word, the net-
work tends to learn a weight vector Wk with a
greater norm to maximize likelihood. This mo-
tivates our approach to initialize the weight norms
with scaled log counts rather than uniformly ran-
dom values in a specific range.

Because we wish to initialize the weight norms
explicitly with scaled logarithm of the word
counts, it is helpful to look at a weight vector’s
magnitude and direction separately. For this pur-
pose, we use a reparameterization technique on
the weight vectors as described in (Salimans and
Kingma, 2016):

Wk = gk
vk
‖vk‖2

(8)

(a) Penn Treebank

(b) WikiText-2

Figure 1: Word vector norms of fine-tuned MoS
models (Yang et al., 2018), trained on (a) Penn
Treebank and (b) WikiText-2. Words are ranked
by their counts in a descending order and thus fre-
quent words are to the left. Actual logarithm of
word counts are plotted in black, and word vec-
tor norms are grey. We observe that word vector
norms loosely follow the trend of log counts.

where k = 1, 2, ..., V , gk = ‖Wk‖2, and vk is
a vector proportional to Wk. Reparameterizing
the weight vectors makes it easy to implement the
weight norm initialization as

gk = σlogck (9)

where ck denotes unigram word count for word k
and σ is a scalar applied to the log counts. We
sample each component of vk from a continuous
uniform distribution in [−r, r], where r is a hy-
perparameter, specifying the initialization range.
With this, no constraint on the weight vector di-
rection is imposed during initialization.

Additionally, we adopt an adaptive gradient
strategy which regularizes the gradients in gk. As
in

(
∂L

∂g
)
′
=

{
[1− (1− γ) tτ ]∂L∂g , for t ≤ τ
γ ∂L∂g , for t > τ

(10)

when epoch t is no greater than a specified epoch

95



Tokens Vocab Size

Penn Treebank
Train 888k

10kValid 70k
Test 79k

WikiText-2
Train 2.1M

33kValid 214k
Test 241k

Table 1: Statistics of the Penn Treebank and
WikiText-2 datasets.

τ , ( ∂L∂gk )
′
— the regularized gradient in gk, linearly

decays to γ (γ ≤ 1) times the unregularized gradi-
ent ∂L∂gk . Otherwise, we directly use the discounted
gradient. In analogy to learning rate decay, this
adaptive gradient strategy anneals the word vector
norm updates in each step. The intuition for such
a strategy is that after a certain amount of epochs,
the weight norms should not change so drastically
from the initialized scaled log counts.

5 Weight Norm Regularization

Weight regularization (WR) is a well established
method to combat overfitting in neural networks,
which is especially important on smaller datasets
(Krogh and Hertz, 1992). The idea is to push
weights in the network to zero, where gradients are
not significant. Typically, WR is implemented by
adding an extra term to the loss functionL0, which
penalizes the norm of all weights in the network.
For example, L2-regularization is implemented as

L = L0 +
λ

2

∑

w

(‖w‖2)2 (11)

with the sum going over all weights w in the
network and λ being the regularization strength.
However, this method is not perfect, as it affects

every weight in the network equally and may lead
to hidden units’ weights getting stuck near zero.

In this work we add a constraint specifically
on the embedding and projection matrices, whose
weights are shared. Since the row vectors in both
matrices are word vectors, it seems appropriate to
put constraints explicitly on their norms instead of
on each individual weight parameter in the matri-
ces.

We propose to add a regularization term to the
standard loss function L0 in the form of

Lwr = L0 + ρ

√√√√
V∑

j=1

(‖Wj‖2 − ν)2 (12)

where ν, ρ ≥ 0 are two scalars and Wj is the j-th
row vector of the projection matrix W . The L2-
norms of the row vectors are pushed towards ν,
while ρ is the regularization strength. This will
punish the row vectors for adopting norms other
than ν, in the hope of reducing the effect of over-
fitting on the training data.

The choice of a soft regularization loss term in-
stead of hard-fixing the weight norms in the for-
ward pass is motivated by the weight norm dis-
tribution shown in Figure 1. It can be seen that
NLMs tend to learn non-equal weight norms for
words with different counts. Therefore, hard-
fixing weight norms may limit the network’s abil-
ity to learn.

6 Experiments

6.1 Experiment Setup
The experiments are conducted on two popular
language modeling datasets. The number of to-
kens and size of vocabulary for each dataset are
summarized in Table 1.

epoch
Penn Treebank WikiText-2

wni ppl baseline ppl ppl reduction (%) wni ppl baseline ppl ppl reduction (%)
1 162.18 180.72 10.26 172.19 192.19 10.41
10 85.92 92.09 6.70 95.90 100.72 4.79
20 73.36 78.94 7.07 85.14 88.21 3.48
30 71.44 73.06 2.22 81.80 82.70 1.09
40 69.27 70.20 1.32 79.28 80.32 1.29

Table 2: Perplexity (ppl) improvement using weight norm initialization (wni) in early epochs on Penn
Treebank and WikiText-2. ppl reduction is around 10% after the first epoch on both tasks, and decays
to approximately 1% after 40 epochs. The wni model has slightly higher perplexities than the baseline
model from around 50 epochs onward.

96



0 10−4 10−3 10−2 10−1 1
ρ

53.0

53.5

54.0

54.5

55.0

55.5

P
e
rp
le
x
it
y

ν = 0.0

ν = 1.0

ν = 2.0

ν = 64.0

Figure 2: Model perplexity on the Penn Treebank
test set as a function of ρ. The different sym-
bols denote different values of ν. Models not de-
picted yield higher perplexity values. The doted
line marks the baseline result (with ρ = 0) as re-
ported by Yang et al. (2018).

The smaller one is the PTB corpus with prepro-
cessing from Mikolov et al. (2010), which has a
comparatively small vocabulary size of 10k. With
a smaller number of sentences, this dataset is a
good choice for performing optimization of hyper-
parameters. The second corpus WT2, which was
introduced by Merity et al. (2016), has over three
times the vocabulary size of PTB.

We use the network structure introduced by
Yang et al. (2018) with the same hyper-parameter
values to ensure comparability. Several regular-
ization techniques are used in this setup, such as
dropout and weight decay. Furthermore, the em-
bedding and projection matrices are tied by de-
fault. For optimization, we adopt the same strat-
egy as described in (Merity et al., 2017a). That
is, a conservative non-monotonic criterion is used
to switch from stochastic gradient descent (SGD)
to averaged stochastic gradient descent (ASGD)
(Polyak and Juditsky, 1992). For more details of
the network structure refer to (Yang et al., 2018).

6.2 Weight Norm Initialization

We tune the hyperparameter σ and use a value of
σ = 0.5 to scale the logarithm of word counts.
Initialization range r is set to 0.1 for both the
reparametrized direction vectors and the baseline
word vectors. Empirically, we set γ = 0.1 and
τ = 100 for the adaptive gradient method. Per-

0 1 2 3 4 5
vector norm

0

5

10

15

p
ro
b
a
b
ili
ty
 [
%
]

MoS

MoS with WR

(a) Penn Treebank

0 1 2 3 4 5
vector norm

0
5

10
15
20
25

p
ro
b
a
b
ili
ty
 [
%
]

MoS

MoS with WR

(b) WikiText-2

Figure 3: Weight norm distributions of the projec-
tion matrices’ row vectors for the AWD-LSTM-
MoS model from Yang et al. (2018) as well as
for our regularized version (WR). The models are
trained on the (a) Penn Treebank corpus and (b)
WikiText-2 corpus with the resulting test perplex-
ities shown in Table 3 and Table 4 respectively.

plexities on both PTB and WT2 in early epochs,
as well as the relative perplexity improvement over
baseline models are summarized in Table 2.

First, we notice significant improvement after
the first epoch of training using weight norm ini-
tialization. About 10% of perplexity reduction is
achieved on both datasets. This could be bene-
ficial, when one wants to train on large datasets
and/or can only train for a limited number of
epochs. Second, the perplexity improvements de-
cay down to around 1% after 40 epochs. This is
in agreement with our expectation, because apart
from reduced gradient in gk, a weight norm ini-
tialized model is not fundamentally different from
the baseline model and no major difference should
be seen if we train for long enough. It is impor-
tant to note that with only weight norm initializa-
tion, both models eventually converge to perplex-
ities that are slightly worse than the baseline. We
also notice that the epochs, after which the opti-
mizer is switched from SGD to ASGD, are differ-
ent in weight norm initialized models and baseline
models.

97



Model #Params Validation Test
Mikolov and Zweig (2012) - RNN-LDA + KN + cache 9M - 92.0
Zaremba et al. (2014) - LSTM 20M 86.2 82.7
Gal and Ghahramani (2016) - Variational LSTM (MC) 20M - 78.6
Kim et al. (2016) - CharCNN 19M - 78.9
Merity et al. (2016) - Pointer Sentinel-LSTM 21M 72.4 70.9
Grave et al. (2017) - LSTM + continuous cache pointer† - - 72.1
Inan et al. (2017) - Tied Variational LSTM + augmented loss 24M 75.7 73.2
Zilly et al. (2017) - Variational RHN 24M 75.7 73.2
Zoph and Le (2017) - NAS Cell 25M - 64.0
Melis et al. (2018) - 2-layer skip connection LSTM 24M 60.9 58.3
Merity et al. (2017a) - AWD-LSTM 24M 60.0 57.3
Yang et al. (2018) - AWD-LSTM-MoS 22M 56.54 54.44
Ours - AWD-LSTM-MoS with weight norm regularization 22M 55.03 53.16

Table 3: Single model perplexity on the Penn Treebank test and validation sets. Baseline results are
obtained from (Yang et al., 2018). † indicates the use of dynamic evaluation.

Model #Params Validation Test
Inan et al. (2017) - Variational LSTM + augmented loss 28M 91.5 87.0
Grave et al. (2017) - LSTM + continuous cache pointer† - - 68.9
Melis et al. (2018) - 2-layer skip connection LSTM 24M 69.1 65.9
Merity et al. (2017a) - AWD-LSTM 33M 69.1 66.0
Yang et al. (2018) - AWD-LSTM-MoS 35M 63.88 61.45
Ours - AWD-LSTM-MoS with weight norm regularization 35M 62.67 60.13

Table 4: Single model perplexity on the WikiText-2 test and validation sets. Baseline results are obtained
from (Yang et al., 2018). † indicates the use of dynamic evaluation.

6.3 Weight Norm Regularization

In order to tune the hyperparameters ρ and ν intro-
duced in Section 5, we perform a grid search over
the PTB dataset, the results of which are shown
in Figure 2. If the norm constraint ν becomes too
large, perplexity worsens significantly, as seen in
the case of ν = 64. A model with a ν-value of
2 provides the best result in most cases. We hy-
pothesize that a value of ν that is too small results
in the logit being close to zero as shown in Equa-
tion 7. For the regularization strength ρ, we recog-
nize that ρ = 10−3 gives the best result on the PTB
test data. Larger or smaller values can hurt the
performance of the system, depending also on the
value of ν. It should be noted that the optimized
value of ρ is significantly larger than the scaling
swd of the weight decay term, which was opti-
mized to be 1.2×10−6 by Merity et al. (2017a).

The resulting weight norm distributions of the
projection matrices’ row vectors are shown in Fig-
ure 3a and Figure 3b for models trained on PTB

and WT2 respectively. Our efforts of pushing the
norms to a value of ν = 2.0 resulted in a notice-
ably smaller average norm, as well as in a overall
more narrow distribution.

With the tuned parameter values ρ = 10−3 and
ν = 2.0 we improve the previous state-of-the-art
result by 1.28 ppl on PTB and by 1.32 ppl on WT2
(without considering dynamic evaluation (Krause
et al., 2018), see Table 3 and Table 4). This is
achieved without increasing the number of train-
able parameters in the network or slowing down
the training process.

7 Conclusion

Word embedding matrix and output projection ma-
trix are important components in LSTM-based
LMs. They are also widely used in other NLP
models where one-hot vectors of words need to
be mapped into lower dimensional space. Given
the one-hot nature of word representations, row
vectors in such matrices are then the correspond-

98



ing word vectors. We study specifically the norms
of these learned word vectors, the distribution of
the norms, and the relationship with word counts.
We show that with a simple initialization strat-
egy together with a reparametrization technique,
it is possible to get significantly lower perplex-
ity in early epochs during training. By using a
weight norm regularization loss term, we are able
to obtain significant improvements on standard
language modeling tasks — 2.4% ppl reduction on
PTB and 2.1% on WT2.

We propose three directions to investigate fur-
ther. First, in this work we use scaled logarithm of
word counts to initialize the weight norms. It is a
logical next step to use smoothing techniques on
the word counts and study the effects of such ini-
tializations. Second, we currently apply the same
norm constraint on different words. Altering the
loss function and regularizing the weight norms to
word counts (and smoothed word counts) is worth
examining as well. Finally, our focus so far is on
weight norms. It is a more exciting and challeng-
ing task to study the pairwise inner products, and
single out the effects of angular differences.

We also plan to expand our regularization and
initialization techniques to the field of neural ma-
chine translation. Embedding and projection ma-
trices are also present in neural machine trans-
lation networks, which could potentially benefit
from our methods as well. It seems natural to use
our methods on the transformer architecture intro-
duced by Vaswani et al. (2017), in which the em-
bedding matrices at source and target sides, plus
the projection matrix, are three-way tied.

Acknowledgments

This work has received
funding from the Euro-
pean Research Council
(ERC) (under the Euro-
pean Union’s Horizon
2020 research and in-
novation programme,
grant agreement No

694537, project ”SEQCLAS”) and the Deutsche
Forschungsgemeinschaft (DFG; grant agreement
NE 572/8-1, project ”CoreTec”). The GPU com-
puting cluster was supported by DFG (Deutsche
Forschungsgemeinschaft) under grant INST
222/1168-1 FUGG.

The work reflects only the authors’ views and

none of the funding agencies is responsible for any
use that may be made of the information it con-
tains.

References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and

Christian Jauvin. 2003. A neural probabilistic lan-
guage model. In Journal of machine learning re-
search, volume 3, pages 1137–1155.

A. Corazza, R. De Mori, R. Gretter, R. Kuhn, and
G. Satta. 1995. Language models for automatic
speech recognition. In Speech Recognition and Cod-
ing, pages 157–173. Springer.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in neural information
processing systems, pages 1019–1027.

Edouard Grave, Armand Joulin, and Nicolas Usunier.
2017. Improving neural language models with a
continuous cache. In International Conference on
Learning Representations.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9:1735–
80.

Hakan Inan, Khashayar Khosravi, and Richard Socher.
2017. Tying word vectors and word classifiers: A
loss framework for language modeling. In Interna-
tional Conference on Learning Representations.

Kazuki Irie, Ralf Schlüter, and Hermann Ney. 2015.
Bag-of-words input for long history representation
in neural network-based language models for speech
recognition. In Sixteenth Annual Conference of the
International Speech Communication Association.

Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. In IEEE transactions on acous-
tics, speech, and signal processing.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In AAAI, pages 2741–2749.

Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In 1995
International Conference on Acoustics, Speech, and
Signal Processing, pages 181–184.

Ben Krause, Emmanuel Kahembwe, Iain Murray, and
Steve Renals. 2018. Dynamic evaluation of neural
sequence models. In Proceedings of the 35th In-
ternational Conference on Machine Learning, pages
2766–2775.

Anders Krogh and John A. Hertz. 1992. A sim-
ple weight decay can improve generalization. In
Advances in neural information processing systems,
pages 950–957.

99



Gábor Melis, Chris Dyer, and Phil Blunsom. 2018. On
the state of the art of evaluation in neural language
models. In International Conference on Learning
Representations.

Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2017a. Regularizing and optimizing lstm
language models. arXiv preprint arXiv:1708.02182.

Stephen Merity, Bryan McCann, and Richard Socher.
2017b. Revisiting activation regularization for lan-
guage rnns. arXiv preprint arXiv:1708.01009.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els. arXiv preprint arXiv:1609.07843.

Tomas Mikolov, Martin Karafiát, Lukáš Burget, Jan
Černockỳ, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In 11th An-
nual Conference of the International Speech Commu-
nication Association.

Tomas Mikolov and Geoffrey Zweig. 2012. Context
dependent recurrent neural network language model.
In IEEE Spoken Language Technology Workshop,
pages 234–239.

Jan-Thorsten Peter, Andreas Guta, Tamer Alkhouli,
Parnia Bahar, Jan Rosendahl, Nick Rossenbach,
Miguel Graça, and Hermann Ney. 2017. The RWTH
Aachen University english-german and german-
english machine translation system for WMT 2017.
In Proceedings of the Second Conference on Machine
Translation, pages 358–365.

Boris T. Polyak and Anatoli B. Juditsky. 1992. Ac-
celeration of stochastic approximation by averaging.
In SIAM Journal on Control and Optimization, pages
838–855.

Ofir Press and Lior Wolf. 2017. Using the output em-
bedding to improve language models. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 157–163.

Tim Salimans and Diederik P Kingma. 2016. Weight
normalization: A simple reparameterization to ac-
celerate training of deep neural networks. In Ad-
vances in Neural Information Processing Systems,
pages 901–909.

Holger Schwenk. 2007. Continuous space language
models. Computer Speech & Language, 21(3):492–
518.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and
William W. Cohen. 2018. Breaking the softmax bot-
tleneck: A high-rank rnn language model. In Inter-
national Conference on Learning Representations.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329.

Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou,
and Lirong Dai. 2015. The fixed-size ordinally-
forgetting encoding method for neural network lan-
guage models. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics, pages 495–500.

Julian Georg Zilly, Rupesh Kumar Srivastava, Jan
Koutnı́k, and Jürgen Schmidhuber. 2017. Recurrent
highway networks. In Proceedings of the 34th In-
ternational Conference on Machine Learning, pages
4189–4198.

Barret Zoph and Quoc V Le. 2017. Neural architecture
search with reinforcement learning. In International
Conference on Learning Representations.

100


