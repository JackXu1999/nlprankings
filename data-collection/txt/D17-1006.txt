



















































Integrating Order Information and Event Relation for Script Event Prediction


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 57–67
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Integrating Order Information and Event Relation for Script Event
Prediction∗

Zhongqing Wang†,‡, Yue Zhang‡ and Ching-Yun Chang‡
†Soochow University, Suzhou, China

‡Singapore University of Technology and Design, Singapore
wangzq.antony@gmail.com, yue zhang@sutd.edu.sg,

chang.frannie@gmail.com

Abstract

There has been a recent line of work au-
tomatically learning scripts from unstruc-
tured texts, by modeling narrative even-
t chains. While the dominant approach
group events using event pair relations, L-
STMs have been used to encode full chain-
s of narrative events. The latter has the
advantage of learning long-range tempo-
ral orders1, yet the former is more adap-
tive to partial orders. We propose a neu-
ral model that leverages the advantages
of both methods, by using LSTM hid-
den states as features for event pair mod-
elling. A dynamic memory network is u-
tilized to automatically induce weights on
existing events for inferring a subsequen-
t event. Standard evaluation shows that
our method significantly outperforms both
methods above, giving the best results re-
ported so far.

1 Introduction

Frequently recurring sequences of events in pro-
totypical scenarios, such as visiting a restauran-
t and driving to work, are a useful source of world
knowledge. Two examples are shown in Figure 1,
which are different variations of the “restaurant
visiting” scenario, where events are partially or-
dered and can be flexible. Such knowledge is
useful for natural language understanding because
texts typically do not include event details when
mentioning a scenario. For example, the reader is
expected to infer that the narrator could have been

∗This work has been done when the first author worked
at SUTD.

1The term “temporal order” is used throughout this work
to indicate the narrative order in texts, following Chambers
and Jurafsky (2008). Strictly speaking, the event order we
extract is the narrative order.

X walk to restaurant

X be seated

X order food

Y serve food

X eat food

X make payment

X leave restaurant

X walk to restaurant

X be seated

X eat food

X make payment

X leave restaurant

X wait in line

X read menu

X take food

X order food

(a)

(b)

Figure 1: Event sequences for restaurant visiting.

driving or cycling given the text “I got flat tire”.
Another typical use of event chain knowledge is
to help infer what is likely to happen next given a
previous event sequence in a scenario. We inves-
tigate the modeling of stereotypical event chains,
which is remotely similar to language modeling,
but with events being more sparse and flexibly or-
dered than words.

Our work follows a recent line of NLP re-
search on script learning. Stereotypical knowl-
edge about partially-ordered events, together with
their participant roles such as “customer”, “wait-
er”, and “table”, is conventionally referred to as
scripts (Schank et al., 1977). NLP algorithms
have been investigated for automatically inducing
scripts from unstructured texts (Mooney and De-
Jong, 1985; Chambers and Jurafsky, 2008). In par-
ticular, Chambers and Jurafsky (2008) made a first
attempt to learn scripts from test inducing event

57



chains by grouping events based on their narrative
coherence, calculated based on Pairwise Mutual
Information (PMI). Jans et al. (2012) showed that
the method can be improved by calculating even-
t relations using skip bi-gram probabilities, which
explicitly model the temporal order of pairs even-
t. Jans et al. (2012)’s model is adopted by a line
of subsequent methods on inducing event chains
from text (Orr et al., 2014; Pichotta and Mooney,
2014; Rudinger et al., 2015).

While the above methods are statistical, neural
network models have recently been used for event
sequence modeling. Granroth-Wilding and Clark
(2016) used a Siamese Network instead of PMI
to calculate the coherence between two events.
Rudinger et al. (2015) extended the idea of Jan-
s et al. (2012) by using a log-bilinear neural lan-
guage model (Mnih and Hinton, 2007) to calcu-
late event probabilities. By learning embeddings
for reducing sparsity, the above models give much
better results compared to the models of Chambers
and Jurafsky (2008) and Jans et al. (2012). Simi-
lar in spirit, Modi (2016) predicted the probability
of an event belonging to a certain event chain by
modeling known events in the chain as a bag of
vectors, showing that it outperforms discrete sta-
tistical methods. These neural methods are con-
sistent with the earlier statistical models in lever-
aging event-pair relations.

Pichotta and Mooney (2016) experimented with
LSTM for script learning, using an existing se-
quence of events to predict the probability of a
next event, which outperformed strong discrete
baselines. One advantage of LSTMs is that they
can encode unbounded time sequences without
losing long-term historical information. LSTMs
capture significantly more order information com-
pared to the methods of Granroth-Wilding and
Clark (2016), Rudinger et al. (2015), and Mod-
i (2016), which model the temporal order of only
pairs of events. On the other hand, a strong-order
LSTM model can also suffer the disadvantage of
over-fitting, given the flexible order of event chain-
s in a script, as demonstrated by the cases of Fig-
ure 1. In this aspect, event-pair models are more
adaptive for flexible orders. However, no direc-
t comparisons have been reported between LSTM
and various existing neural network methods that
model event-pairs.

We make such comparisons using the same
benchmark, finding that the method of Pichotta

and Mooney (2016) does not necessarily outper-
form event-pair models, such as Granroth-Wilding
and Clark (2016). LSTM temporal ordering and
event pair modeling have their respective strength.
To leverage the advantages of both methods, we
propose to integrate chain temporal order infor-
mation into event relation measuring. In partic-
ular, we calculate event pair relations by repre-
senting events in a chain using LSTM hidden s-
tates, which encode temporal information. The L-
STM over-fitting issue is mitigated by using the
temporal-order in a chain as a feature for event
pair modeling, rather than the direct model out-
put. In addition, observing that the importance
of existing events can vary for inferring a subse-
quent event, we use a dynamic memory network
model to automatically induce event weights for
each event for inferring the next event. In con-
trast, previous methods give equal weights to ex-
isting events (Chambers and Jurafsky, 2008; Mo-
di, 2016; Granroth-Wilding and Clark, 2016).

Results on a multi-choice narrative cloze bench-
mark show that our model significantly outper-
forms both Granroth-Wilding and Clark (2016)
and Pichotta and Mooney (2016), improving the
state-of-the-art accuracy from 49.57% to 55.12%.
Our contributions can be summarized as follows:

• We make a systematic comparison of LSTM
and pair-based event sequence learning meth-
ods using the same benchmarks.

• We propose a novel dynamic memory net-
work model, which combines the advantages
of both LSTM temporal order learning and
traditional event pair coherence learning.

• We obtain the best results in the standard
multi-choice narrative cloze test.

Our code is released at https://github.
com/wangzq870305/event_chain.

2 Related Work

Scripts have been a traditional subject in AI re-
search (Schank et al., 1977), where event se-
quences are manually encoded in knowledge
bases, and used for end tasks such as inference.
They are also connected with research in linguis-
tics and psychology, and sometimes referred to
as frames (Minsky, 1975; Fillmore, 1982) and
schemata (Rumelhart, 1975). The same concept

58



is also studied as templates in information extrac-
tion (Sundheim, 1991). Chambers and Jurafsky
(2008) pioneered the recent line of work on script
induction (Jans et al., 2012; Pichotta and Mooney,
2016; Granroth-Wilding and Clark, 2016), where
the focus is on modeling narrative event chains, a
crucial subtask for script modeling from raw text.
Below we summarize such investigations.

With respect to event representation, Cham-
bers and Jurafsky (2008) casted narrative events
as triples of the form 〈event, dependency〉, where
the event is typically represented by a verb and the
dependency represents typed dependency relations
between the event and a protagonist, such as “sub-
ject” and “object”. Chambers and Jurafsky (2008)
organized narrative chains around a central actor,
or protagonist, mining events that share a common
protagonist from texts by using a syntactic parser
and a coreference resolver. Balasubramanian et al.
(2013) observed that the protagonist representa-
tion of event chains can suffer from weaknesses
such as lack of coherence, and proposed to repre-
sent events as 〈arg1, relation, arg2〉, where arg1
and arg2 represent the subject and object, respec-
tively. Such representation is inspired by open in-
formation extraction (Mausam et al., 2012), and
offers richer features for event pair modeling. Pi-
chotta and Mooney (2014) adpoted a similar idea,
using v(es, eo, ep) to represent an event, where v
is a verb lemma, es is the subject, eo is the ob-
ject, and ep is an entity with prepositional relation
to v. Their representation is used by subsequent
work such as Modi (2016) and Granroth-Wilding
and Clark (2016). We follow Pichotta and Mooney
(2016) in our event representation form.

With respect to modeling, existing methods can
be classified into two main categories, namely
weak-order models, which calculate relations be-
tween pairs of events, and strong-order models,
which consider the temporal order of events in
a full sequence. Event-pair models have so far
been the dominant method in the literature. Earlier
work used discrete event representations and esti-
mated event relations by statistical counting. As
mentioned earlier, Chambers and Jurafsky (2008)
used PMI to calculate event relations, and Jan-
s et al. (2012) used skip bigram probabilites to
the same end, which is order-sensitive. Most sub-
sequent methods followed Jans et al. (2012) in
using skip n-grams (Pichotta and Mooney, 2014;
Rudinger et al., 2015).

Events being multi-argument structures,
counting-based methods can suffer from sparsity
issues. Recent work employed embeddings to
address this disadvantage. Rudinger et al. (2015)
learned event embeddings as a by-product of
training a log-bilinear language model for events;
Granroth-Wilding and Clark (2016) leveraged
the skip-gram model of Mikolov et al. (2013) for
training the embeddings of event and arguments
by ordering them into a pseudo sentence. Modi
(2016) utilized word embeddings of verbs and
arguments directly, using a hidden layer to au-
tomatically consolidate word embedding into a
single structured event embeddings. We follow
Modi (2016) and use a hidden layer to learn event
argument compositions given word embeddings,
training the composition function as a part of the
event chain learning process.

Mitigating the sparsity issue of event represen-
tations, neural methods can capture temporal or-
ders between events beyond skip n-grams. Our
model integrates the advantages of strong-order
learning and event-pair learning by using LSTM
hidden states as feature representation of existing
events in the calculation of event pair relationship-
s. In addition, we use a memory network model to
weigh existing events, which gives better results
compared to the equal weighting method of exist-
ing models.

With respect to evaluation, Chambers and Ju-
rafsky (2008) proposed the Narrative Cloze Test,
which asks for a missing event in a given even-
t chain with a gap. The task has been adopted
by various subsequent work for comparing result-
s with Chambers and Jurafsky (2008) (Jans et al.,
2012; Pichotta and Mooney, 2014; Rudinger et al.,
2015). One issue of the narrative cloze test is that
there can sometimes be multiple plausible answer-
s, but only one gold-standard answer, which can
make it overly expensive to manually evaluate sys-
tem outputs. To address this issue, Modi (2016)
proposed the Adversarial Narrative Cloze (ANC)
task, which is to discriminate between pairs of real
and corrupted event chains. Granroth-Wilding and
Clark (2016) proposed the Multi-Choice Narrative
Cloze (MCNC) task, which is to choose the most
likely next event from a set of candidates given a
chain of events. We choose MCNC for comparing
different models.

Other related work includes learning tempo-
ral relations of events (Modi and Titov, 2014; Uz-

59



X = Customer,  Y = Waiter

walk(X, restaurant), seat(X), order(X, food), serve(Y, food)

eat(X, food), make(X, payment), 

c1: receive(X, response)

c2: drive(X, mile)

c3: seem(X)

c4: discover(X, truth)

c5: leave(X, restaurant)

?

Entities

Context(ei)

Figure 2: Multiple choice narrative cloze. The
gold subsequent event is marked in bold.

Zaman et al., 2013; Abend et al., 2015), evaluat-
ed using different metrics. There has also been
work using graph models to induce frames, which
emphasize more on learning event structures and
less on temporal orders (Chambers, 2013; Cheung
et al., 2013). The above methods focus on one
of the two subtasks we consider here. Frermann
et al. (2014) used a Bayesian model to jointly clus-
ter web collections of explicit event sequence and
learn input event-pair temporal orders. However,
their work is under a different input setting (Reg-
neri et al., 2010), not learning event chains from
texts. Mostafazadeh et al. (2016) proposed the s-
tory close task (SCT), which is to predict the end-
ing given a unfinished story. Our narrative chain
prediction task can be regarded as a sub task in the
story close task, which can contribute as a major
approach. On the other hand, information beyond
event chains can be useful for the story close task.

3 Problem Definition

As shown in Figure 2, given a chain of narrative
events e1, e2, ..., en−1, our work is to predict the
likelihood of a next event candidate en. Formally,
an event e is a structure v(a0, a1, a2), where v is a
verb describing the event, a0 and a1 are its subject
and direct object, respectively, and a2 is a prepo-
sitional object. For example, given the sentence
“John brought Marry to the restaurant”, an even-
t bring{John,Marry , to the restaurant} can be
extracted.

We follow the standard script induction set-
ting (Chambers and Jurafsky, 2008; Granroth-
Wilding and Clark, 2016), extracting events from
a text corpus using a syntactic parser and a named
entity resolver. A neural network is used to mod-
el chains of extracted events for script learning.
In particular, we model the probability of a sub-

sequent event given a chain of events. For eval-
uation, we solve the multi-choice narrative cloze
task: given a chain of events and a set of candidate
next events, the most likely candidate is chosen as
the output.

4 Model

The overall structure of our model is shown in Fig-
ure 3, which has three main components. First,
given an event v(a0, a1, a2), a representation lay-
er is used to compose the embeddings of v, a0, a1,
and a2 into a single event vector e. Second, a L-
STM is used to map a sequence of existing events
e1, e2, ..., en−1 into a sequence of hidden vectors
h1, h2, ..., hn−1, which encode the temporal or-
der. Given a next event candidate ec, the recurrent
network takes one further step from hn−1 to de-
rive its hidden vector hc, which encodes ec. Third,
hc is paired with h1, h2, ..., hn−1 individually,
and passed to a dynamic memory network to learn
the relatedness score s. s is used to denote the
connectedness between the candidate subsequent
event and the context event chain.

4.1 Event Representation
We learn vector representations of standard events
by composing pre-trained word embeddings of
its verb and arguments. The skipgram mod-
el (Mikolov et al., 2013) is used to train word vec-
tors. For arguments that consist of more than one
word, we use the averaged word for the represen-
tation. OOV words are represented simply using
zero vectors. For events with less than 3 argu-
ments, such as “John fell”, where v = fall, a0 =
John, a1 = NULL, and a2 = NULL, the NULL
arguments are represented using all-zero vectors.

Denoting the embeddings of v, a0, a1, and a2
as e(v), e(a0), e(a1), and e(a2), respectively, the
embedding of e is calculated using a tanh compo-
sition layer

e(e) = tanh(W ve · e(v) +W 0e · e(a0)+
W 1e · e(a1) +W 2e · e(a2) + be)

(1)

Here W ve , W
0
e , W

1
e , W

2
e , and b are model

parameters, which are randomly initialized and
tuned during the training of the main network.

4.2 Modeling Temporal Orders
Given the embeddings of the existing chain of
events e1, e2, ..., en−1, we use a standard LST-
M (Hochreiter and Schmidhuber, 1997) without

60



Event composition

LSTM Temporal Order Learning

v a0 a1 a2

e0

Event composition

v a0 a1 a2

ece1 en-1

…

Dynamic Memory Network for Relation Measuring

h0 h1 hn-1 hc

…

Prob(ec|e0,e1,…,en-1)

candidate next eventexisting event sequence

Event composition

v a0 a1 a2

Event composition

v a0 a1 a2

Figure 3: Overview of proposed model.

e0 e1 en-1

ec1

ec2

ecm

hn-1h2h1

hc1

hc2

hcm

…

…

Figure 4: Temporal order modeling.

coupled input and forget gates or peephole con-
nections to model the temporal order. We ob-
tain a sequence of hidden state vectors h1, h2,
..., hn−1 by recurrently feeding e(e1), e(e2), ...,
e(en−1) as inputs to the LSTM, where hi =
LSTM(e(ei), hi−1). The initial state hs and al-
l stand LSTM parameters are randomly initialized
and tuned during training.

Now for each candidate next event ec, we ob-
tain its vector representation e(ec) in the same way
as for e1 to en−1. e(ec) is then appended to the
existing event chain to obtain a temporal-order-
sensitive feature vector hc, by advancing the re-
current encoding process for one step from hn−1:
hc = LSTM(e(ec), hn−1). With multiple next
event candidates e1c , e

2
c , ..., e

m
c (m ∈ [1,∞]), m

feature vectors are obtained, as shown in Figure 4,
each being used as a basis for estimating the prob-
ability of the corresponding event candidate.

4.3 Modeling Pairwise Event Relations

After obtaining the hidden states for events, we
model event pair relations using these hidden s-
tate vectors. A straightforward approach to model
the relation between two events is using a Siamese
network (Granroth-Wilding and Clark, 2016). The
order-sensitive LSTM features for existing events
h1, h2, ..., hn−1 and the candidate event hc are

used as event representations. Given a pair of
events hi (i ∈ [1..n − 1]) and hc, the relatedness
score is calculated by

si = sigmoid(Wsihi +Wschc + bs), (2)

where Wsi, Wsc and bs are model parameters.
Given the relation score si between hc and each

existing event hi, the likelihood of ec given e1, e2,
..., en−1 can be calculated as the average of si:

s =
∑n−1

i=1 si
n− 1 (3)

Weighting existing events. The drawback of
above approach is that it considers the contribu-
tion of each event on the chain is same. How-
ever, given a chain of existing events, some are
more informative for inferring a subsequent event
than others. For example, given the events “wait
in queue”, “getting seated” and “order food”, “or-
der food” is more relevant for inferring “eat food”
compared with the other two given events. Given
information over the full event chain, this link can
be more evident since the scenario is likely restau-
rant visiting.

We use an attentional neural network to calcu-
late the relative importance of each existing event
according to the subsequent event candidate, using
hi (i ∈ [1..n−1]) and hc for event representations:

ui = tanh(Weihi +Wchc + bu) (4)

αi =
exp(ui)∑
j exp(uj)

(5)

where αi ∈ [0, 1] is the weight of hi, and
∑

i α
t
i =

1. Wei, Wc, and bu are model parameters.
After obtaining the weight αi of each existing

event hi, the relatedness of ec with the existing
events can be calculated as:

s =
n−1∑
i=1

αi · si (6)

Multi-layer attention using Deep memory
network. Memory network (Weston et al., 2014;
Mikolov et al., 2014) has been used for explor-
ing deep semantic information for semantic tasks.
Such as question answering (Sukhbaatar et al.,
2015; Kumar et al., 2016) and reading comprehen-
sion (Hermann et al., 2015; Weston et al., 2015).
Our task is analogous to such semantic tasks in

61



h1 h2 hn-1

v(t)

Attention

he
(t) g

v(t+1)

∑

a(t) …

…

Figure 5: Memory network at hop t. hi is the hid-
den variable of the existing event chain, vt is the
semantic representation between context events
and candidate event. at is the weight of contex-
t events, and g is the gated recurrent network on
Eq.10.

the sense that deep semantic information can be
necessary for making the most rational inference.
Hence, we are motivated to use a deep memory
network model to refine event weight and event
relation calculation by recurrently modeling more
abstract representations of the scenario. Different
from the previous researches, we use the memo-
ry network to model the event chain, refining the
attention mechanism used to explore the pair-wise
relation between events.

The memory model consists of multiple dynam-
ic computational layers (hops). For the first layer
(hop 1), the weights α for existing events e1, e2,
..., en−1 can be calculated using the same attention
mechanism as Eq.4 and Eq.5. Given the weights
α, we build a consolidated representation of con-
text event chain e1, e2, ..., en−1 as a weighted sum
of h1, h2, ..., hn−1:

he =
n−1∑
i−1

αi · hi (7)

The event candidate hc and the new represen-
tation of the existing chain he can be further in-
tegrated to deduce a deeper representation of the
full event chain hypothesis to the next layer (hop
2), denoted as v. v contains deeper semantic infor-
mation compared with hc, which encode the tem-
poral order of the event chain [h1, h2, ..., hn−1, hc]
without differentiating the weights of each event.
As a result, in the next hop, better event weights
can potentially be deduced by using v instead of
hc in the calculation of attention:

uti = tanh(Weihi +Wvv
t + bu) (8)

αti =
exp(uti)∑
j exp(u

t
j)

(9)

In the same way, we stack multiple hops and
repeat the steps multiple times, so that more ab-
stract evidences can be extracted according to the
chain of existing events. The above process can
be performed recurrently, by taking hc as an ini-
tial scenario representation v0, and then repeated-
ly calculating hte given h1, h2, ..., hn−1 and vt,
and using hte and vt to find a deeper scenario rep-
resentation vt+1. Following Chung et al. (2014)
and Tran et al. (2016), a gated recurrent network
is used to this end:

z = σ(Wzhte + Uzv
t)

r = σ(Wrhte + Urv
t)

ĥ = tanh(Whte + U(r � vt))
vt+1 = (1− z)� vt + z � ĥ

(10)

At any step, if the value of |vt+1−vt| is less than
the threshold µ, we consider that the progress has
reached convergence. Figure 5 shows an overview
of the memory network at hop t.

4.4 Training
Given a set of event chains, each with a gold-
standard subsequent event and a number of non-
subsequent events, our training objective is to
minimize the cross-entropy loss between the gold
subsequent event and the set of non-subsequent
events. The loss function of event chain predic-
tion is that:

L(Θ) =
N∑

i=1

(si − yi)2 + λ2 ||Θ||
2 (11)

where si is the relation score, yi is the label of the
candidate (yi = 1 for positive sample, and yi = 0
for negative sample), Θ is the set of model param-
eters and λ is a parameter for L2 regularization.
We apply online training, where model parameter-
s are optimized by using AdaGrad (Duchi et al.,
2011). We train word embedding using the Skip-
gram algorithm (Mikolov et al., 2013)2.

5 Experiments

5.1 Datasets
Following Granroth-Wilding and Clark (2016), we
extract events from the NYT portion of the Gi-
gaword corpus (Graff et al., 2003). The C&C

2https://code.google.com/p/word2vec/

62



tools (Curran et al., 2007) are used for POS tag-
ging and dependency parsing, and OpenNLP3 for
phrase structure parsing and coreference resolu-
tion. The training set consists of 1,500,000 even-
t chains. We follow Granroth-Wilding and Clark
(2016) and use 10,000 event chains as the test set,
and 1,000 event chains for development. There
are 5 choices of output event for event input chain,
which are given by Granroth-Wilding and Clark
(2016). This dataset is referred to as G&C16.

We also adapt the Chambers and Jurafsky
(2008)’s dataset to the multiple choice setting, and
use this dataset as the second benchmark. The
dataset contains 69 documents, with 346 multiple
choice event chain samples. We randomly sample
4 negative subsequent events for each event chain
to make multiple-choice candidates. This dataset
is referred to as C&J08. For both datasets, accu-
racy (Acc.) of the chosen subsequent event is used
to measure the performance of our model.

5.2 Hyper-parameters

There are several important hyper-parameters in
our models, and we tune their values using the
development dataset. We set the regularization
weight λ = 10−8 and the initial learning rate to
0.01. The size of word vectors is set to 300, and
the size of hidden vectors in LSTM to 128. In
order to avoid over-fitting, dropout (Hinton et al.,
2012) is used for word embedding with a ratio of
0.2. The neighbor similarity threshold η is set to
0.25. The threshold µ of the memory network sets
to 0.1.

5.3 Development Experiments

We conduct a set of development experiments on
the G&C16 development set to study the influence
of event argument representations and network
configurations of the proposed MemNet model.

5.3.1 Influence of Event Structure
Existing literature discussed various structures to
denote events, such as v(a0, a1) and v(a0, a1, a2).
We investigate the influence of integrating argu-
ment values of the subject a0, object a1 and prepo-
sition a2, by doing ablation experiments on the
development data. The results are shown in Ta-
ble 1, where the system using all arguments gives a
54.36% accuracy. By removing a2, which exists in
17.6% of the events in our developmental data, the

3https://opennlp.apache.org/

Method Acc. (%)
MemNet 54.36
-verb 42.63
-(a0, a1) 52.32
-(a0) 53.43
-(a1) 53.57
-(a2) 54.02

Table 1: Influence of event arguments.

accuracy drops to 54.02%. In contrast, by remov-
ing a0 and a1, which exist in 87.6% and 64.6%
of the events in the development data, respective-
ly, the accuracies drop to 53.43% and 53.57%, re-
spectively, which demonstrates the relative impor-
tance of a0 (i.e., the subject) and a1 (i.e., the ob-
ject) for event modelling. While most previous
work (Chambers and Jurafsky, 2008; Balasubra-
manian et al., 2013; Pichotta and Mooney, 2014)
modelled only a0 and a1, recent work (Pichotta
and Mooney, 2016; Granroth-Wilding and Clark,
2016) modelled a2 also.

By removing both a1 and a2, the accuracy drops
further to 53.32%. Interestingly, by removing the
verb while keeping only the arguments, the accu-
racy drops to 42.63%. While this demonstrates the
central value of the verb in denoting a event, it al-
so suggests that the arguments themselves play a
useful role in inferring the stereotypical scenario.

5.3.2 Influence of Network Configurations

We study the influence of various network config-
urations by performing ablation experiments, as
shown in Table 2. MemNet is the full model of
this paper; -LSTM denotes ablation of the LSTM
layer, using e(e1), e(e2), ..., e(en−1) instead of h1,
h2, ..., hn−1 to represent events; -Hop denotes ab-
lation of the dynamic network model, using on-
ly attention mechanism to calculate the weights of
each existing event; -Attention denotes ablation of
the attention mechanism, using the same weight
on each existing event when inferring ec. The
model “-Attention, -LSTM” is hence similar to the
method of Granroth-Wilding and Clark (2016), al-
though we used a different way of deriving even-
t embeddings. The model “LSTM-only” shows a
based by using LSTM hidden vector hn−1 to di-
rectly predict the next event, which is similar to
the method of Pichotta and Mooney (2016).

Influence of Temporal Order. By compar-
ing “MemNet” and “-LSTM”, and comparing “-

63



Method Acc. (%)
MemNet 54.36
-Hop 52.03
-Attention 50.76
-LSTM 51.72
-Hop,-LSTM 50.65
-Attention,-LSTM 48.26
LSTM-Only 46.72

Table 2: Analysis of network structure.

Attention” with “-Attention, -LSTM”, one can
find that temporal order information over the w-
hole event chain does have significant influence
on the results (p − value < 0.01 using t-test).
On the other hand, using LSTM to directly pre-
dict the subsequent event (“LSTM-only”) does not
give better accuracies compared to model even-
t pairs (“-Attention, -LSTM”). This confirms our
intuition that strong-oder modelling and event-pair
modelling each have their own strength.

Influence of Attention. Comparison be-
tween “-Attention” and “-Hop”, and between “-
Attention, -LSTM” and “-Hop, -LSTM” shows
that giving different weights to different events
does lead to improving results. Our analysis in
Section 4.3 gives more intuitions to this obser-
vation. Finally, comparison between “-Hop” and
“MemNet” and between “-Hop, -LSTM” and “-
LSTM” shows that a multi-hop deep memory net-
work can indeed enhance the model with single
level attention by offering more effective semantic
representation of the scenarios.

5.4 Final Results

Table 3 shows the final results on the C&C 16 and
C&J08 datasets, respectively. We compare the re-
sults of our final model with the following base-
lines:

• PMI is the co-occurrence based model of
Chambers and Jurafsky (2008), who calcu-
late event pair relations based on Pointwise
Mutual Information (PMI), scoring each can-
didate event ec by the sum of PMI scores be-
tween the given events e0, e1, ..., en−1 and
the candidate.

• Bigram is the counting based model of Jans
et al. (2012), calculating event pair relation-
s based on skip bigram probabilities, trained
using maximum likelihood estimation.

Method G&C16 C&J08
PMI 30.52 30.92
Bigram 29.67 25.43
Event-Comp 49.57 43.28
RNN 45.74 43.17
MemNet 55.12 46.67

Table 3: Final results.

• Event-Comp is the neural event relation
model proposed by Granroth-Wilding and
Clark (2016). They learn event representa-
tions by calculating pair-wise event scores
using a Siamese network.

• RNN is the method of Pichotta and Mooney
(2016), who model event chains by directly
using hc in Section 4.2 to predict the output,
rather than taking them as features for event
pair relation modeling.

• MemNet is the proposed deep memory net-
work model.

Our reimplementation of PMI and Bigrams fol-
lows (Granroth-Wilding and Clark, 2016). It
can be seen from the table that the statistical
counting-based models PMI and Bigram signif-
icantly underperform the neural network models
Event-Comp, RNN and MemNet, which is largely
due to their sparsity and lack of semantic repre-
sentation power. Under our event representation,
Bigram does not outperform PMI significantly ei-
ther, although considering the order of event pairs.
This is likely due to sparsity of events when all
arguments are considered.

Direct comparison between Event-Comp and
RNN shows that the event-pair model gives com-
parable results to the strong-order LSTM model.
Although Granroth-Wilding and Clark (2016) and
Pichotta and Mooney (2016) both compared with
statistical baselines, they did not make direct com-
parisons between their methods, which represen-
t two different approaches to the task. Our re-
sults show that they each have their unique ad-
vantages, which confirm our intuition in the in-
troduction. By considering both pairwise rela-
tions and chain temporal orders, our method sig-
nificantly outperform both Event-Comp and RNN
(p − value < 0.01 using t-test), giving the best
reported results on both datasets.

64



6 Conclusion

We proposed a dynamic memory network to inte-
grate chain order information into event relation
measuring, calculating event pair relations by rep-
resenting events in a chain using LSTM hidden s-
tates, which encode temporal orders, and using a
dynamic memory model to automatically induce
event weights for each event. Standard evaluation
showed that our method significantly outperforms
state-of-the-art event pair models and event chain
models, giving the best results reported so far.

Acknowledgments

The corresponding author is Yue Zhang. We are
grateful for the help of Fei Dong for his initial dis-
cussion. We thank our anonymous reviewers for
their constructive comments, which helped to im-
prove the paper. This work is supported by the
Temasek Lab grant IGDST1403012 at Singapore
University of Technology and Design.

References
Omri Abend, Shay B. Cohen, and Mark Steedman.

2015. Lexical event ordering with an edge-factored
model. In NAACL HLT 2015, The 2015 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Denver, Colorado, USA, May
31 - June 5, 2015, pages 1161–1171.

Niranjan Balasubramanian, Stephen Soderland,
Mausam, and Oren Etzioni. 2013. Generating
coherent event schemas at scale. In Proceedings
of the 2013 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2013,
18-21 October 2013, Grand Hyatt Seattle, Seattle,
Washington, USA, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1721–1731.

Nathanael Chambers. 2013. Event schema induction
with a probabilistic entity-driven model. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2013,
18-21 October 2013, Grand Hyatt Seattle, Seattle,
Washington, USA, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1797–1807.

Nathanael Chambers and Daniel Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In ACL
2008, Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics, June 15-
20, 2008, Columbus, Ohio, USA, pages 789–797.

Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction.
In Human Language Technologies: Conference of
the North American Chapter of the Association of

Computational Linguistics, Proceedings, June 9-14,
2013, Westin Peachtree Plaza Hotel, Atlanta, Geor-
gia, USA, pages 837–846.

Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. CoRR, abs/1412.3555.

James R. Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale NLP with c&c
and boxer. In ACL 2007, Proceedings of the 45th
Annual Meeting of the Association for Computation-
al Linguistics, June 23-30, 2007, Prague, Czech Re-
public.

John C. Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159.

Charles Fillmore. 1982. Frame semantics. Linguistics
in the morning calm, pages 111–137.

Lea Frermann, Ivan Titov, and Manfred Pinkal. 2014.
A hierarchical bayesian model for unsupervised in-
duction of script knowledge. In Proceedings of
the 14th Conference of the European Chapter of
the Association for Computational Linguistics, EA-
CL 2014, April 26-30, 2014, Gothenburg, Sweden,
pages 49–57.

David Graff, Junbo Kong, Ke Chen, and Kazuaki Mae-
da. 2003. English gigaword. Linguistic Data Con-
sortium, Philadelphia.

Mark Granroth-Wilding and Stephen Clark. 2016.
What happens next? event prediction using a com-
positional neural network model. In Proceedings of
the Thirtieth AAAI Conference on Artificial Intelli-
gence, February 12-17, 2016, Phoenix, Arizona, US-
A., pages 2727–2733.

Karl Moritz Hermann, Tomás Kociský, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in
Neural Information Processing Systems 28: Annual
Conference on Neural Information Processing Sys-
tems 2015, December 7-12, 2015, Montreal, Que-
bec, Canada, pages 1693–1701.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors.
CoRR, abs/1207.0580.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Bram Jans, Steven Bethard, Ivan Vulic, and Marie-
Francine Moens. 2012. Skip n-grams and rank-
ing functions for predicting script events. In EACL
2012, 13th Conference of the European Chapter of

65



the Association for Computational Linguistics, Avi-
gnon, France, April 23-27, 2012, pages 336–344.

Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyy-
er, James Bradbury, Ishaan Gulrajani, Victor Zhong,
Romain Paulus, and Richard Socher. 2016. Ask me
anything: Dynamic memory networks for natural
language processing. In Proceedings of the 33nd In-
ternational Conference on Machine Learning, ICM-
L 2016, New York City, NY, USA, June 19-24, 2016,
pages 1378–1387.

Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL 2012,
July 12-14, 2012, Jeju Island, Korea, pages 523–
534.

Tomas Mikolov, Armand Joulin, Sumit Chopra,
Michaël Mathieu, and Marc’Aurelio Ranzato. 2014.
Learning longer memory in recurrent neural net-
works. CoRR, abs/1412.7753.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013. Pro-
ceedings of a meeting held December 5-8, 2013,
Lake Tahoe, Nevada, United States., pages 3111–
3119.

Marvin Minsky. 1975. A framework for representing
knowledge.

Andriy Mnih and Geoffrey E. Hinton. 2007. Three new
graphical models for statistical language modelling.
In Machine Learning, Proceedings of the Twenty-
Fourth International Conference (ICML 2007), Cor-
vallis, Oregon, USA, June 20-24, 2007, pages 641–
648.

Ashutosh Modi. 2016. Event embeddings for semantic
script modeling. In Proceedings of the 20th SIGNL-
L Conference on Computational Natural Language
Learning, CoNLL 2016, Berlin, Germany, August
11-12, 2016, pages 75–83.

Ashutosh Modi and Ivan Titov. 2014. Inducing neu-
ral models of script knowledge. In Proceedings of
the Eighteenth Conference on Computational Nat-
ural Language Learning, CoNLL 2014, Baltimore,
Maryland, USA, June 26-27, 2014, pages 49–57.

Raymond Mooney and Gerald DeJong. 1985. Learning
schemata for natural language processing. Urbana,
51:61801.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James F. Allen. 2016. A cor-
pus and cloze evaluation for deeper understanding

of commonsense stories. In NAACL HLT 2016, The
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, San Diego California,
USA, June 12-17, 2016, pages 839–849.

John Walker Orr, Prasad Tadepalli, Janardhan Rao
Doppa, Xiaoli Fern, and Thomas G. Dietterich.
2014. Learning scripts as hidden markov models. In
Proceedings of the Twenty-Eighth AAAI Conference
on Artificial Intelligence, July 27 -31, 2014, Québec
City, Québec, Canada., pages 1565–1571.

Karl Pichotta and Raymond J. Mooney. 2014. Statis-
tical script learning with multi-argument events. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, EACL 2014, April 26-30, 2014, Gothen-
burg, Sweden, pages 220–229.

Karl Pichotta and Raymond J. Mooney. 2016. Learning
statistical scripts with LSTM recurrent neural net-
works. In Proceedings of the Thirtieth AAAI Con-
ference on Artificial Intelligence, February 12-17,
2016, Phoenix, Arizona, USA., pages 2800–2806.

Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In ACL 2010, Proceedings of the 48th
Annual Meeting of the Association for Computation-
al Linguistics, July 11-16, 2010, Uppsala, Sweden,
pages 979–988.

Rachel Rudinger, Pushpendre Rastogi, Francis Ferraro,
and Benjamin Van Durme. 2015. Script induction
as language modeling. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portugal,
September 17-21, 2015, pages 1681–1686.

David E Rumelhart. 1975. Notes on a schema for sto-
ries. Representation and understanding: Studies in
cognitive science, 211(236):45.

Roger Schank, Roger Schank, and Robert P Abelson.
1977. Scripts, plans, goals and understanding; an
inquiry into human knowledge structures. Technical
report.

Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2015. End-to-end memory net-
works. In Advances in Neural Information Process-
ing Systems 28: Annual Conference on Neural In-
formation Processing Systems 2015, December 7-
12, 2015, Montreal, Quebec, Canada, pages 2440–
2448.

Beth Sundheim. 1991. Third message understanding
evaluation and conference (muc-3): Phase 1 status
report. In HLT.

Ke M. Tran, Arianna Bisazza, and Christof Monz.
2016. Recurrent memory networks for language

66



modeling. In NAACL HLT 2016, The 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, San Diego California, USA,
June 12-17, 2016, pages 321–331.

Naushad UzZaman, Hector Llorens, Leon Derczynski,
James F. Allen, Marc Verhagen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-
3: Evaluating time expressions, events, and tem-
poral relations. In Proceedings of the 7th In-
ternational Workshop on Semantic Evaluation,
SemEval@NAACL-HLT 2013, Atlanta, Georgia,
USA, June 14-15, 2013, pages 1–9.

Jason Weston, Antoine Bordes, Sumit Chopra, and
Tomas Mikolov. 2015. Towards ai-complete ques-
tion answering: A set of prerequisite toy tasks. CoR-
R, abs/1502.05698.

Jason Weston, Sumit Chopra, and Antoine Bordes.
2014. Memory networks. CoRR, abs/1410.3916.

67


