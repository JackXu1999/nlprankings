



















































Self-Supervised Dialogue Learning


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3857–3867
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3857

Self-Supervised Dialogue Learning

Jiawei Wu and Xin Wang and William Yang Wang
Department of Computer Science

University of California, Santa Barbara
Santa Barbara, CA 93106 USA

{jiawei wu,xwang,william}@cs.ucsb.edu

Abstract

The sequential order of utterances is often
meaningful in coherent dialogues, and the or-
der changes of utterances could lead to low-
quality and incoherent conversations. We con-
sider the order information as a crucial su-
pervised signal for dialogue learning, which,
however, has been neglected by many previ-
ous dialogue systems. Therefore, in this pa-
per, we introduce a self-supervised learning
task, inconsistent order detection, to explic-
itly capture the flow of conversation in dia-
logues. Given a sampled utterance pair triple,
the task is to predict whether it is ordered or
misordered. Then we propose a sampling-
based self-supervised network SSN to per-
form the prediction with sampled triple ref-
erences from previous dialogue history. Fur-
thermore, we design a joint learning frame-
work where SSN can guide the dialogue sys-
tems towards more coherent and relevant di-
alogue learning through adversarial training.
We demonstrate that the proposed methods
can be applied to both open-domain and task-
oriented dialogue scenarios, and achieve the
new state-of-the-art performance on the Open-
Subtitiles and Movie-Ticket Booking datasets.

1 Introduction

In recent years, dialogue systems have achieved
fruitful results with neural conversation models in
both open-domain generation (Ritter et al., 2011;
Sordoni et al., 2015b; Li et al., 2016b, 2017; Xu
et al., 2017; Zhang et al., 2018b) and task-oriented
completion (Wen et al., 2015, 2017; Williams
et al., 2017; Bordes et al., 2017; Su et al., 2018).
These methods empower lots of real-world dia-
logue applications such as Google Home and Ap-
ple Siri.

However, the utterance generation from di-
alogue systems still faces some critical chal-
lenges, including utterance blandness and incoher-

ence (Gao et al., 2018). They are mainly caused
by the objective function of the dialogue systems
that prefer utterances with unconditionally high
probability (Li et al., 2016a). We argue that in
a meaningful and coherent dialogue, the change
of utterance order will lead to a low-quality dia-
logue. However, most existing neural-based dia-
logue systems either encode the full dialogue his-
tory (Li et al., 2017; Xu et al., 2017) or only the
current utterance (Liu and Lane, 2018). None of
them explicitly models the sequential order and
studies its criticality to the dialogue learning prob-
lem.

In this paper, we explore the sequential order
within the dialogue as the self-supervised signal to
guide meaningful and coherent dialogue learning.
We introduce a self-supervised learning task, in-
consistent order detection, to explicitly capture the
order signal of the dialogue. The task is defined
as, given a target utterance pair triple, the model
is required to predict whether the triple is cor-
rectly ordered or not. For instance, the utterance
pair triple 〈(Q1, A1), (Q4, A4), (Q2, A2)〉 is mis-
ordered. The key to solving this task is to model
the utterance order based on the dialogue context
effectively. But when directly encoding the full di-
alogue history along the temporal order, the model
actually only focuses on the ending utterances, and
earlier information is largely discarded (Li et al.,
2017). Thus, we propose a sampling-based self-
supervised network (SSN ) to account for the
forgetfulness problem and solve the inconsistent
order detection task. In order to accurately predict
if a target utterance triple is ordered or not, we ran-
domly sample utterance triples from the dialogue
history as the reference to incorporate the dialogue
context. Since for the same target utterance triple,
the sampled triple references are different at dif-
ferent iterations during training. It essentially ap-
proximates the full dialogue history without suf-



3858

fering from the forgetfulness issue.
To further utilize SSN in real dialogue learn-

ing, we propose to jointly learn SSN and the di-
alogue model via alternative training, where the
output probability of SSN is treated as the order
signal to evaluate the generated utterance. More-
over, the proposed approach can be applied to both
open-domain and task-oriented dialogue learning,
which indicates that SSN is a general and scal-
able approach for dialogue learning. Empirical
results on two widely-used benchmark datasets,
OpenSubtitles and Movie-Ticket Booking, show
that our self-supervised network consistently im-
proves the state-of-the-art (SOTA) neural-based
dialogue training methods. In summary, our main
contributions are three-fold:

• We introduce the task of inconsistent or-
der detection, and propose a self-supervised
learning network SSN to solve this task and
explicitly model the crucial order information
in dialogue.

• We propose a general framework to jointly
learn SSN and the dialogue models, where
the sequential order in dialogues can be ex-
plicitly used to guide the utterance genera-
tion.

• Our method advances the existing state-of-
the-art dialogue systems in both open-domain
and task-oriented scenarios.

2 Related Work

Dialogue Learning Dialogue systems can be
roughly classified into open-domain and task-
oriented scenarios. In recent years, neural-based
conversation models have shown great power in
building dialogue systems (Ritter et al., 2011; Sor-
doni et al., 2015b; Vinyals and Le, 2015; Serban
et al., 2016; Luan et al., 2016). However, the utter-
ances generated by neural-based dialogue systems
still suffer from blandness and incoherence (Gao
et al., 2018). To address these problems, Li et al.
(2016a) propose a mutual information objective
to infer the utterance generation. Serban et al.
(2017) and Zhang et al. (2018a) further apply the
latent variable models to generate more specific
responses. Similar to some language generation
tasks (Lamb et al., 2016; Yu et al., 2017), Gen-
erative adversarial networks (GAN) (Goodfellow
et al., 2014) have also been adapted to learn a bet-
ter objective function for the dialogue (Li et al.,

2017; Xu et al., 2017; Liu and Lane, 2018; Su
et al., 2018). The discriminator in GAN is of-
ten used to evaluate the generated utterances and
guide dialogue learning. However, these methods
mainly focus on the surface information of gener-
ated utterances to guide the dialogue learning, and
fail to consider the utterance connection within the
dialogue history. In this paper, we focus on the se-
quential information of the dialogue and show that
the unique sequential order in a meaningful and
coherent dialogue contains more useful semantic
information for dialogue learning.

Self-Supervised Learning Self-supervised
learning, which aims to train a network on an
auxiliary task where ground-truth is obtained
automatically, has been successfully applied in
computer vision. Many self-supervised tasks have
been introduced to use non-visual but intrinsically
correlated features to guide the visual feature
learning (Doersch et al., 2015; Wang and Gupta,
2015; Pathak et al., 2016). As for natural language
processing, predicting nearby words (Mikolov
et al., 2013b,a) is a self-supervised task to learn
word embeddings. The language modeling is
another line of self-supervision where a language
model learns to predict the next word given the
previous sequence (Bengio et al., 2003; Dai and
Le, 2015; Peters et al., 2018). Recently, Devlin
et al. (2019) further proposes two self-supervised
tasks, the masked language model and next sen-
tence prediction, to learn sentence embeddings.
Lample and Conneau (2019); Liu et al. (2019)
further extend these two tasks into multi-lingual
and multi-task paradigms. Wang et al. (2019)
consider them at the sentence-level for extractive
summarization. Our work is the first to consider
the sequential order as the self-supervised signal
in dialogue and we propose the self-supervised
task of inconsistent order detection towards more
coherent and relevant dialogue learning.

3 Methods

In this section, we systematically describe how to
utilize the internal sequential order of utterances
as self-supervision for dialogue learning. In Sec-
tion 3.1, we first introduce the task of inconsis-
tent order detection, where the model needs to pre-
dict whether one sampled triple of the dialogue is
correctly ordered or not. We then present an ef-
fective sampling-based approach, self-supervised
network (SSN ), to learn to capture the important



3859

Q1

A1

Q2

A2

Qt-1

At-1

. .

Qt

At

Dialogue History

Current Utterance Pair

Q1, A1

Q2, A2

Qt-1, At-1
Ordered

Sampling Q1, A1

Qt-1, At-1

Q2, At2
Misordered

Q1, A1

Qt, At

Qt-1, At-1
To be predicted

U
tterance Pair Encoder

O
rder R

easoning Layer

+

+

Concatenation

(a) Triple Reference Sampling (b) Inconsistent Order Prediction

M
ulti-layer Perceptron

Output: 1 
Misordered

Output: 0 
Ordered

Figure 1: The overview of our self-supervised network (SSN ) for inconsistent order detection. Given a target triple
containing the current utterance pair (Qt, At) to be predicted, (a) we first sample triple references from previous
dialogue history {(Q1, A1), · · · , (Qt−1, At−1)} in each iteration. The references can be ordered or misordered.
(b) For each triple, it is transformed into the triple embedding. The concatenation of triple embeddings is fed into
a MLP, and gives the probability based on the current sampling.

order signal and solve this task (see Section 3.2).
In the end, we show in Section 3.3 how SSN can
contribute to both open-domain and task-oriented
dialogue learning by modeling the inconsistent or-
der detection.

3.1 Inconsistent Order Detection

The dialogue systems aim at conversing with the
human in a meaningful and coherent way (Gao
et al., 2018). Thus, the sequential order in
dialogue data is an important signal for build-
ing a good dialogue system. Existing neural-
based dialogue systems only consider this sig-
nal in a weak and implicit way, where they use
hierarchical encoders to model the dialogue his-
tory (Sordoni et al., 2015a; Serban et al., 2016;
Li et al., 2017; Serban et al., 2017; Xing et al.,
2018). However, we argue that these methods
are mainly designed to model the overall seman-
tic context information of the dialogue history
but not good at modeling intermediate sequen-
tial order. Especially, the order signal is becom-
ing weak as the number of dialogue turns in-
creases. Thus, we propose the task of inconsis-
tent order detection to force building models to
capture this signal as self-supervision explicitly.
Given a dialogue till the turn t, we can formulate
it as {(Q1, A1), (Q2, A2), · · · , (Qt, At)}, where
(Qt, At) is a pair of human-machine utterances.
Then we can sample multiple triples of this dia-
logue as utterance pair triples using the following

strategies:

• Ordered triple sampling: We sample a
triple following the dialogue sequential or-
der as 〈(Qi, Ai), (Qj , Aj), (Qk, Ak)〉, where
i < j < k ≤ t.

• Misordered triple sampling: The three ut-
terance pairs are sampled in a triple as
〈(Qi, Ai), (Qk, Ak), (Qj , Aj)〉, where i <
j < k ≤ t.

Note that when the current dialogue length t <=
2, it is not enough to get a rational sampling for
utterance pair triples. Thus, we add three ex-
tra shared padding utterance pairs (Q−2, A−2),
(Q−1, A−1) and (Q0, A0) ahead of all the dia-
logue data before sampling1.

Based on above triple sampling strate-
gies, we define the task of inconsistent or-
der detection as: given a dialogue history
{(Q1, A1), (Q2, A2), · · · , (Qt, At)} and the
target utterance pair (Qt, At) for evaluation, the
model needs to predict whether the sampled triple
T containing (Qt, At) is ordered or not. For in-
stance, 〈(Q1, A1), (Q2, A2), (Qt, At)〉 is ordered
(output: 0), while 〈(Q1, A1), (Qt, At), (Q2, A2)〉
is misordered (output: 1).

1Specifically, e.g., for the added padding utterance Q−2,
it is represented as a sequence of one same padding word
{w(Q−2)1 , w

(Q−2)
2 , · · · , w

(Q−2)
N }, where N is the rounded-

up averaged length of utterances in the dataset.



3860

3.2 Self-Supervised Network SSN
We plan to build the model to solve the inconsis-
tent order detection task, and explicitly capture the
sequential order in dialogue. The overview of our
approach is shown in Figure 1. At each dialogue
turn t, given a target triple containing the current
utterance pair, we first sample triple references
from the previous dialogue history to capture more
semantic context in dialogue. The target triple and
triple references are then transformed into embed-
dings using an utterance pair encoder and an order
reasoning layer. Finally, the concatenation of em-
beddings is used for the final prediction. We then
describe the SSN in detail as follows.

3.2.1 Triple Reference Sampling
Given the task definition in Section 3.1, the model
needs to predict whether there is inconsistent or-
der in the target triple containing the current utter-
ance pair (Qt, At). It is intuitive that if we can get
more previous dialogue history, we may make a
better prediction for inconsistent order. One trivial
way is to encode the full previous dialogue history
using a hierarchical network and make the predic-
tion. However, Li et al. (2017) suggests that this
structure actually focuses more on the final two
preceding utterances instead of the whole history.
The sequential order signal is very weak in this
condition. We also report some similar results in
Section 4.1.

Therefore, we propose a sampling-based ap-
proach to model the utterance order based on
the dialogue context effectively. For each sam-
pling operation, we sample two triple references
T ′ and T ′′ from the previous dialogue history
{(Q1, A1), (Q2, A2), · · · , (Qt−1, At−1)} follow-
ing the sampling strategies in Section 3.1. In gen-
eral, we explore the following three combinations
of reference sampling strategies for T ′ and T ′′:

• T ′ and T ′′ are sampled ordered references.

• T ′ and T ′′ are sampled misordered ones.

• T ′ is ordered while T ′′ is misordered.

Note that in our experiments, we choose one cer-
tain combination and keep using it for sampling
the triple references for all the target triples.

3.2.2 Objective Function
Given the target triple embedding T and the
triple reference embedding T ′ and T ′′, we use

SSN to calculate the probability p(T |T ′, T ′′) =
SSN (T, T ′, T ′′). We use the Binary Cross En-
tropy loss to train the model:

L = −E(y log p(T |T ′, T ′′)), (1)

where y is the ground-truth label.
Considering that for the same target triple T , the

triple references are sampled m times to approxi-
mate the full dialogue history. Then we can rewrite
the loss function as

L = −E( 1
m

m∑
i=1

y log(p(i)(T |T (i)′ , T (i)′′))), (2)

where T (i)
′
, T (i)

′′
are the triple references of i-th

sampling. This is essentially a Monte Carlo es-
timation and the model would effectively incor-
porate the dialogue context and capture the order
information, avoiding from directly encoding the
full dialogue history and the forgetfulness issue.

3.2.3 Network Structure
In this section, we demonstrate how SSN embeds
both the target triple T and triple reference T ′ and
T ′′ to generate p(T |T ′, T ′′) in each sampling.

Utterance Pair Encoder First, given a utter-
ance pair (Qt, At), we concatenate the Qt and At
as one sequence. The sequence is then fed into
a bidirectional long short-term memory network
(LSTM) (Hochreiter and Schmidhuber, 1997), and
the utterance pair embedding Ut is the concatena-
tion of the final two states of the bi-LSTM:

Ut =

[←−
h1−−→
hNt

]
, (3)

where Nt is the length of the concatenated utter-
ance sequence.

Order Reasoning Layer After obtaining the ut-
terance pair embeddings (Ui,Uj ,Uk) of a sam-
pled triple T =< (Qi, Ai), (Qj , Aj), (Qk, Ak) >,
we need to reason and predict whether there is in-
consistent order or not. To simplify our model,
we use a 3-step reasoning bi-LSTM with the max-
pooling layer to perform the order reasoning:

T =

[
max-pooling(

←−
h1,
←−
h2,
←−
h3)

max-pooling(
−→
h1,
−→
h2,
−→
h3)

]
, (4)

where the input of each time step in bi-LSTM is
one utterance pairs embedding, and T is the final
embedding of the given triple.



3861

Given the target triple embedding T and the
triple reference embedding T′ and T′′, the con-
catenation of these three embeddings is fed into
a multi-layer perceptron, returning the probability
p(T |T ′, T ′′) of the triple is ordered (approaching
0) or misordered (approaching 1).

3.3 Self-Supervised Network for Dialogue
In this section, we explain how the SSN can
be applied to the current dialogue system in both
open-domain and task-oriented scenarios.

Suppose we have a dialogue system the the his-
tory {(Q1, A1), · · · , (Qt−1, At−1)}, at turn t, the
system generate the utterance At based on the Qt.
We can sample a misordered target triple T con-
taining (Qt, At). Following the assumption that
the sequential order in a meaningful and coher-
ent dialogue should be unique, the SSN will be
easy to detect the inconsistent order in T if the
generated At is good. Otherwise, the At may
be of low quality. Therefore, we take a two-step
sampling approach to evaluate the generated ut-
terance At using SSN . First, a misordered tar-
get triple T containing (Qt, At) is sampled. Then
we further sample triple references T ′ and T ′′ as
in Section 3.2.1 and how easily the misorder in
the sampled T can be detected is measured as
ET ′,T ′′(p(T |T ′, T ′′). Based on the generated ut-
terance At, we can sample multiple misordered T ,
and we set the following expectation to measure
the probability that At is a good generated utter-
ance:

p∗SSN = Emisordered TET ′,T ′′(p(T |T ′, T ′′)). (5)

In this way, we can view human-generated ut-
terances as good ones, and machine-generated ut-
terances as bad ones. Then we can use the adver-
sarial training methods (Goodfellow et al., 2014;
Li et al., 2017; Xu et al., 2017; Su et al., 2018) to
train the dialogue system, where SSN can give
clear order-based signal to guide the generator G
in the system. The framework of using SSN with
the two-step sampling in real dialogue systems are
shown in Figure 2. The objective function then can
be formulated as:

min
θG

max
θSSN

Ereal[log p∗SSN (x)]

+ Egen[log(1− p∗SSN (G(.)))],
(6)

where θG and θSSN are the parameters of
the generator G and SSN in the dialogue

Dialogue System

Self-Supervised 
Network

Dialogue 
History

Generated 
Utterance

Misordered 
Target 
Triple

Signal

Sampling

Triple 
References

Sampling

Figure 2: The general framework for dialogue learning
with self-supervised network.

systems separately. The x stands for real
human-generated utterances, which G(.) repre-
sents machine-generated ones. The G and SSN
are alternately updated during training. We fur-
ther describe the details in open-domain and task-
oriented scenarios separately.

3.3.1 Open-Domain Dialogue Learning
The open-domain dialogue task is, given a di-
alogue history consisting of a sequence of di-
alogue utterances {(Q1, A1), . . . , (Qt−1, At−1)},
and current Qt, the model needs to generate a re-
sponse utterance At. We consider the adversarial
training (Li et al., 2017; Xu et al., 2017) for di-
alogue generation systems. Following the previ-
ous approach (Vinyals and Le, 2015; Serban et al.,
2016; Luan et al., 2016; Li et al., 2017), we use
the SEQ2SEQ model for response generation as
the generator G. The SEQ2SEQ first transforms
the dialogue history into an embedding using an
encoder recurrent network. Conditioned on the
history embedding, another decoder recurrent net-
work then computes the probability of tokens at
each generation step of the response using a soft-
max function.

As for the discriminator D, in previous meth-
ods, the discriminator directly takes the response
utterance At with or without the full dialogue his-
tory, and predicts whether it is human-generated
(output: 1) or machine-generated (output: 0). The
probability of being human-generated is set as the
reward to update the G using the REINFORCE al-
gorithm (Williams, 1992). As for our SSN , the
reward R is set as R = p∗SSN .

3.3.2 Task-Oriented Dialogue Learning
The task-oriented dialogue, usually formulated as
a reinforcement learning problem, aims to build a



3862

dialogue agent to interact with real users and learn
the policy to complete the slot-filling task (Juraf-
sky and Martin, 2014). While the real-user inter-
action is expensive and time-consuming, in this
scenario, the dialogue systems are often trained
with user simulators (Schatzmann et al., 2006; Li
et al., 2016c). However, due to the complexity of
real conversations and biases in the design of user
simulators, the quality of simulated utterances is
unstable. Su et al. (2018) propose an adversarial
learning approach to differentiate simulated expe-
rience from real experience. Following the sim-
ilar assumption that real-user interactions should
be meaningful and coherent, we implement our
SSN instead of the conventional discriminator D
to select high-quality stimulated utterances in the
task-oriented dialogue systems.

In this scenario, the generator G is the world
model which produces simulated user experience,
and the SSN focuses on scoring the simulated
user experience Qt during the training process.
Thus, instead of sampling and encoding utterance
pairs (Qt, At), here we only use the user utterance
Qt in SSN . We keep other parts of the SSN
remain the same as in Section 3.2. Because the
world model G is updated using the multi-task
learning without the reward from the SSN , the
objective function of the SSN in Equation 6 can
be rewritten as the following during the mini-batch
training:

1

b

b∑
i=1

[log p∗SSN (x
(i)) + log(1− p∗SSN (G(.)(i)))],

(7)
where b represents the batch size.

4 Experiments

4.1 Intrinsic Evaluation
Before we deploy the self-supervised network into
real dialogue systems, we first test the model ar-
chitectures for reliability. We randomly choose
40K balanced ordered and misordered utterance
pair triples from the OpenSubtitles (Tiedemann,
2009) dataset, and train the SSN to solve this 2-
class classification. We sample another 1K bal-
anced triples for testing. We also consider a base-
line model, where the target triple is encoded by
SSN , and the previous dialogue history is en-
coded by a hierarchical LSTM. The concatenation
of two embeddings is used for the final predic-
tion. Because our SSN is a sampling-based ap-

Reference Strategy of SSN Average Accuracy

All history by hierarchical LSTM .694 (.006)

w/o Refers .670 (.011)
2*Ordered Refers .740 (.031)
2*misordered Refers .744 (.029)
1*Ordered + 1*misordered Refers .856 (.017)

Table 1: The intrinsic evaluation results. The num-
bers in brackets stand for deviation. Refers: Reference
Triples.

proach, we report the average prediction accuracy
of 5 runs on the 2-class classification as shown in
Table 1.

From the results, we can observe that: (1) The
conventional hierarchical LSTM is not suitable for
this task, and this baseline only shows a marginal
improvement compared with the strategy that only
considers target triple without any history. The re-
sults also match previous findings (Li et al., 2017),
where they suggest that only the last two proceed-
ing utterances in the hierarchical network are se-
mantically significant. (2) As for our SSN , it is
safe to tell that reference triples can be a tremen-
dous supplement to the inconsistent order detec-
tion. It is not surprising because by adding refer-
ence triples, the SSN will know more informa-
tion of semantic context within the dialogue. Es-
pecially when having both ordered and misordered
references, the SSN has the highest classification
accuracy. This also shows that the sampling strat-
egy, 1*Ordered + 1*misordered references, is the
most reliable structure for real dialogue systems.
Thus, for the rest of the experiments, we directly
use the SSN with one ordered and one misor-
dered references strategy to achieve the best per-
formance.

4.2 Open-Domain Dialogue Learning
Dataset Following the previous studies (Vinyals
and Le, 2015; Li et al., 2017; Xu et al., 2017),
we choose the widely-used OpenSubtitles (Tiede-
mann, 2009) dataset to evaluate different methods.
The OpenSubtitles dataset contains movie scripts
organized by characters, where we follow Li et al.
(2016b) to retain subtitles containing 5-50 words.

Baselines We consider the following two pop-
ular adversarial methods for dialogue learning as
the baselines:

• REGS (Li et al., 2017): The discriminator D
takes the full dialogue history by a hierarchi-



3863

Separated G/D D-REGS D-AEL D-SSN

G-REGS .094 .087 .041
G-AEL .146 .128 .093
G-SSN .203 .185 .162

Table 2: The cross evaluation of adversarial success
rate on different generators and discriminators. Please
refer to Section 4.2 Adversarial Evaluation for expla-
nations.

Model distinct-1 distinct-2

REGS 0.0217 0.0695
AEL 0.0311 0.0948
SSN 0.0393 0.1126

Table 3: The automatic evaluation of generated utter-
ances on distinct-1 and distinct-2 metrics. Please refer
to Section 4.2 Automatic Evaluation for explanations.

cal LSTM, and the Monte Carlo search is im-
plemented to obtain rewards for every gener-
ation step to update the generator G.

• AEL (Xu et al., 2017): The discriminator D
only encodes the currently generated utter-
ance by a CNN model and the generator G is
optimized using an approximate embedding
layer.

Implementation Details We follow the most of
parameters in Li et al. (2017); Xu et al. (2017) to
make a fair comparison. For the generator model
G, we adopt the same SEQ2SEQ model (Sutskever
et al., 2014) with an attention mechanism (Bah-
danau et al., 2015; Luong et al., 2015) for our ap-
proach and baselines. We approximate the dia-
logue history forG using the concatenation of two
preceding utterances following the Li et al. (2017).
To train the generator G, we use the REINFORCE
algorithm (Williams, 1992) to maximize the ex-
pected reward of generated utterances. We also
implement the Monte Carlo search to give rewards
for each generation step. To accelerate the sam-
pling process, we use multiple GPUs to parallelize
and distribute the jobs. As for the SSN , it first
gets pre-trained using sampled data from Open-
Subtitiles, and then iteratively updated during the
min-max adversarial training process. The dimen-
sion of the utterance embeddings is 128. The hid-
den size is 256 for utterance encoding bi-LSTM
and 1024 for triple reasoning bi-LSTM. The MLP
has a single hidden layer of size 512.

Win REGS AEL SSN

Single-turn Percentage .095 .192 .713
Multi-turn Percentage .025 .171 .804

Table 4: The human evaluation of generated utterances
in three methods. The result here is statistically signif-
icant with p < 0.01 according to sign test. Please refer
to Section 4.2 Human Evaluation for explanations.

Adversarial Evaluation Here we use adversar-
ial success rate (AdverSuc), which is the fraction
of instances where a G is capable of fooling the
D, to evaluate different methods. Higher values
of AdverSuc for a dialogue system usually lead to
a better response generator. After training three
(G,D) using REGS, AEL and SSN , we sample
4K dialogue history and use three trained gen-
erators to generate response utterances. These
machine-generated utterances are then fed into
three trained discriminators to see if they are in-
distinguishable from human-generated ones. The
cross evaluation of AdverSuc is shown in Table 2.

From the results, we can observe that: (1)
Our trained generator achieve higher AdverSuc in
three discriminators, which shows that the gener-
ator in our approach can generate more human-
like utterance responses. (2) The generators of
the other two methods have a noticeable drop in
AdverSuc when evaluating on our SSN -based
discriminator. This demonstrates that our self-
supervised policy for discriminating utterances is
successful. (3) The REGS method with full di-
alogue history encoded performs worse than the
AEL that only considers the current utterances.
We think this indicates that without explicitly stat-
ing the guiding signal, both the generator and the
discriminator can be lost about figuring out a good
objective function during the training process even
when encoding the full history.

Automatic Evaluation For automatic evalua-
tions, we use the two commonly accepted met-
rics distinct-1 and distinct-2. The distinct-1 and
distinct-2, proposed by Li et al. (2016a), are two
ways to measure the degree of diversity by cal-
culating the number of distinct unigrams and bi-
grams in the generated response utterances. The
evaluation results are reported in Table 3. The
results show that based on the distinct-1 and
distinct-2 metrics, the generator trained in our ap-
proach can generate relatively more diverse re-
sponses. The results are attractive considering that



3864

Agent PlanningSteps
Epoch 100 Epoch 200 Epoch 300

Succ Reward Turns Succ Reward Turns Succ Reward Turns

D3Q

5

.7467 43.59 14.03 .6800 34.64 15.92 .7200 40.85 13.11
D3Q-SSN .7600 45.71 13.52 .7400 42.93 14.80 .7633 46.16 15.24
D3Q (fixed θD) .6800 33.86 17.48 .7000 36.57 16.85 .6933 35.67 17.06
D3Q-SSN (fixed θSSN ) .6633 32.04 16.21 .7133 36.71 17.74 .7067 36.03 12.91

D3Q

10

.6333 28.99 16.01 .7000 37.24 15.52 .6667 33.09 15.83
D3Q-SSN .7800 48.71 15.84 .8733 56.15 19.57 .8067 50.29 16.48
D3Q (fixed θD) .7133 36.36 20.48 .8400 54.87 20.48 .7400 42.89 13.81
D3Q-SSN (fixed θSSN ) .7367 42.30 14.79 .8300 52.92 18.16 .7933 48.05 13.73

Table 5: The experimental results of different dialogue agents at training epoch = {100, 200, 300}. Each number
is averaged over 3 runs, and each run tested on 50 dialogues. The D3Q-SSN denotes the D3Q agent where our
proposed SSN replaces the discriminator. The “fixed θD/θSSN ” indicates the discriminator/SSN is pre-trained
and fixed during the training process. Succ: Success Rate. Reward: Average Reward. Turns: Average Turns.

we do not explicitly use a diversity-guided objec-
tive function during the training process. We think
the reason is that the diverse utterances are eas-
ier to reserve the order information. In previous
methods, the discriminator D only gives good or
bad signals to response generatorG, and theG has
to figure out what is an acceptable response by it-
self. As for our SSN , it explicitly forces the G to
generate responses that will have unique orders in
dialogue, which leads to more diverse utterances.

Human Evaluation For human evaluation, we
follow protocols in Li et al. (2016a) and employ-
ing crowd-sourced judges from the Amazon Me-
chanical Turk to evaluate a random sample of 1000
unique generated utterances from three generators
in the OpenSubtitles test dataset. We present both
the input dialogue history and the generated re-
sponses to 5 judges and ask them to decide which
one of the three results is the be.ts Ties are not per-
mitted. We consider both single-turn and multi-
turn for the evaluation. The results are shown in
Table 4. Evidently, the generator trained in our
method shows a significant improvement in the
quality of generated sentences. The gain is even
higher in the multi-turn setting than the single-turn
setting. This is because when only considering the
single-turn dialogue, the information encoded in
three methods will be similar.

4.3 Task-Oriented Dialogue Learning

Dataset Following the previous work (Peng
et al., 2018; Su et al., 2018), we use the same
Movie-Ticket Booking dataset collected from
Amazon Mechanical Turk for evaluation. The
dataset is manually labeled based on a schema de-
fined by domain experts consisting of 11 intents

and 16 slots in the full domain setting. In total, the
dataset has 280 annotated dialogues with an aver-
age length of approximately 11 turns. In this sce-
nario, the goal of dialogue systems is to help the
user complete the tasks through the conversation.

Baselines We compare our SSN -based dis-
criminator within the state-of-the-art task-oriented
dialogue policy learning approach, Discriminative
Deep Dyna-Q (D3Q) (Su et al., 2018). At each
turn, the D3Q agent takes S planning steps inter-
acting with the simulator and store stimulated user
experiences based on the scoring of the discrimi-
nator. The stimulated user experiences are gener-
ated by the world model, which can be viewed as
the generator G in our case. We replace the con-
ventional discriminator D of D3Q with our SSN .

Implementation Details For a fair comparison,
we remain most of the parameters in the D3Q al-
gorithm the same as in Su et al. (2018). In the
self-supervised network, the dimension of the ut-
terance embeddings is 80. The hidden size is 128
for utterance encoding bi-LSTM and 512 for triple
reasoning bi-LSTM. The MLP has a single hidden
layer of size 128. We use the simulator2 as in Li
et al. (2016c) to generate user utterances, and the
threshold interval is set to a range between 0.45
and 0.55.

Results The experimental results of different
agents at training epoch are shown in Table 5.
From the results, we can observe that: (1) The
D3Q-SSN outperform the D3Q in the most of
cases, which shows that our SSN -based dis-
criminator can improve the ability to recognize

2https://github.com/MiuLab/TC-Bot

https://github.com/MiuLab/TC-Bot


3865

the high-quality stimulated user experiences. (2)
When the planning step increases in D3Q, the per-
formance shows an apparent drop. This is be-
cause the discriminator D in the original D3Q
agent keeps lots of low-quality stimulated user
experiences, which significantly degrade the per-
formance of the D3Q agent. As for our SSN ,
we can see some performance improvement even
when using 10-step planning. This substantially
means that our SSN has a better ability to select
the good simulated user experiences, especially in
the multi-turn dialogue cases.

5 Conclusion

In this paper, we introduce a self-supervised task,
inconsistent order detection, to explicitly capture
the order signal of the dialogue. While previous
methods suffer from forgetfulness problem when
modeling dialogue history, we further propose a
sampling-based self-supervised network SSN , to
approximately encoding the dialogue history and
highlight the order signal. We also show how
our SSN can contribute to real dialogue learn-
ing. Empirically, our method advances the previ-
ous state-of-the-art dialogue systems in both open-
domain and task-oriented scenarios. Theoreti-
cally, we believe this self-supervision can be gen-
eralized to other types of temporal order in differ-
ent NLP tasks.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the 3rd International Conference on Learning Rep-
resentations (ICLR).

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research,
3(Feb):1137–1155.

Antoine Bordes, Y-Lan Boureau, and Jason Weston.
2017. Learning end-to-end goal-oriented dialog. In
Proceedings of the 5th International Conference on
Learning Representations (ICLR).

Andrew M Dai and Quoc V Le. 2015. Semi-supervised
sequence learning. In Proceedings of the 29th Con-
ference on neural information processing systems
(NeurIPS), pages 3079–3087.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In Proceedings of the 2019 Annual Conference

of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT).

Carl Doersch, Abhinav Gupta, and Alexei A Efros.
2015. Unsupervised visual representation learn-
ing by context prediction. In Proceedings of the
IEEE International Conference on Computer Vision
(ICCV), pages 1422–1430.

Jianfeng Gao, Michel Galley, and Lihong Li. 2018.
Neural approaches to conversational ai. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 2–7.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative
adversarial nets. In Proceedings of the 28th Con-
ference on Neural Information Processing Systems
(NeurIPS), pages 2672–2680.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Dan Jurafsky and James H Martin. 2014. Speech and
language processing. Pearson Education UK.

Alex M Lamb, Anirudh Goyal Alias Parth Goyal, Ying
Zhang, Saizheng Zhang, Aaron C Courville, and
Yoshua Bengio. 2016. Professor forcing: A new al-
gorithm for training recurrent networks. In Proceed-
ings of the 30th conference on Neural Information
Processing Systems (NeurIPS), pages 4601–4609.

Guillaume Lample and Alexis Conneau. 2019. Cross-
lingual language model pretraining. arXiv preprint
arXiv:1901.07291.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), pages 110–119.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016b. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 1192–1202.

Jiwei Li, Will Monroe, Tianlin Shi, Sėbastien Jean,
Alan Ritter, and Dan Jurafsky. 2017. Adversarial
learning for neural dialogue generation. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
2157–2169.

Xiujun Li, Zachary C Lipton, Bhuwan Dhingra, Lihong
Li, Jianfeng Gao, and Yun-Nung Chen. 2016c. A
user simulator for task-completion dialogues. arXiv
preprint arXiv:1612.05688.



3866

Bing Liu and Ian Lane. 2018. Adversarial learning of
task-oriented neural dialog models. In Proceedings
of the 19th Annual SIGdial Meeting on Discourse
and Dialogue (SIGDIAL), pages 350–359.

Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-
feng Gao. 2019. Multi-task deep neural networks
for natural language understanding. arXiv preprint
arXiv:1901.11504.

Yi Luan, Yangfeng Ji, and Mari Ostendorf. 2016.
Lstm based conversation models. arXiv preprint
arXiv:1603.09457.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1412–1421.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their composition-
ality. In Proceedings of the 27th Conference on
Neural Information Processing Systems (NeurIPS),
pages 3111–3119.

Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue,
Trevor Darrell, and Alexei A Efros. 2016. Context
encoders: Feature learning by inpainting. In Pro-
ceedings of the IEEE conference on computer vision
and pattern recognition (CVPR), pages 2536–2544.

Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu,
and Kam-Fai Wong. 2018. Deep dyna-q: Integrat-
ing planning for task-completion dialogue policy
learning. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 2182–2192.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), pages 2227–2237.

Alan Ritter, Colin Cherry, and William B Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the 2011 conference on empirical
methods in natural language processing (EMNLP),
pages 583–593.

Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of di-
alogue management strategies. The knowledge en-
gineering review, 21(2):97–126.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
the Thirtieth AAAI Conference on Artificial Intelli-
gence (AAAI).

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville, and
Yoshua Bengio. 2017. A hierarchical latent variable
encoder-decoder model for generating dialogues. In
Proceedings of the 31st AAAI Conference on Artifi-
cial Intelligence (AAAI).

Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi,
Christina Lioma, Jakob Grue Simonsen, and Jian-
Yun Nie. 2015a. A hierarchical recurrent encoder-
decoder for generative context-aware query sugges-
tion. In Proceedings of the 24th ACM International
on Conference on Information and Knowledge Man-
agement (CIKM), pages 553–562. ACM.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015b.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceed-
ings of the 2015 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-
HLT), pages 196–205.

Shang-Yu Su, Xiujun Li, Jianfeng Gao, Jingjing Liu,
and Yun-Nung Chen. 2018. Discriminative deep
dyna-q: Robust planning for dialogue policy learn-
ing. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 3813–3823.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the 28th Conference on
Neural Information Processing Systems (NeurIPS),
pages 3104–3112.

Jörg Tiedemann. 2009. News from opus-a collection
of multilingual parallel corpora with tools and inter-
faces. In Proceedings of the 2nd Recent advances
in natural language processing (RANLP), volume 5,
pages 237–248.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. ICML Deep Learning Workshop.

Hong Wang, Xin Wang, Wenhan Xiong, Mo Yu, Xi-
aoxiao Guo, Shiyu Chang, and William Yang Wang.
2019. Self-supervised learning for contextualized
extractive summarization. In Proceedings of the
57th Annual Meeting of the Association for Compu-
tational Linguistics (ACL).

Xiaolong Wang and Abhinav Gupta. 2015. Un-
supervised learning of visual representations us-
ing videos. In Proceedings of the IEEE Inter-
national Conference on Computer Vision (ICCV),
pages 2794–2802.



3867

Tsung-Hsien Wen, Milica Gasic, Nikola Mrkšić, Pei-
Hao Su, David Vandyke, and Steve Young. 2015.
Semantically conditioned lstm-based natural lan-
guage generation for spoken dialogue systems.
In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1711–1721.

Tsung-Hsien Wen, David Vandyke, Nikola Mrkšić,
Milica Gasic, Lina M Rojas Barahona, Pei-Hao Su,
Stefan Ultes, and Steve Young. 2017. A network-
based end-to-end trainable task-oriented dialogue
system. In Proceedings of the 15th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 438–449.

Jason D Williams, Kavosh Asadi, and Geoffrey Zweig.
2017. Hybrid code networks: practical and efficient
end-to-end dialog control with supervised and rein-
forcement learning. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 665–677.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Chen Xing, Yu Wu, Wei Wu, Yalou Huang, and Ming
Zhou. 2018. Hierarchical recurrent attention net-
work for response generation. In Proceedings of
the 32nd AAAI Conference on Artificial Intelligence
(AAAI).

Zhen Xu, Bingquan Liu, Baoxun Wang, SUN
Chengjie, Xiaolong Wang, Zhuoran Wang, and
Chao Qi. 2017. Neural response generation via gan
with an approximate embedding layer. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
617–626.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. Seqgan: Sequence generative adversarial nets
with policy gradient. In Proceedings of the 31st
AAAI Conference on Artificial Intelligence (AAAI).

Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan,
Jun Xu, and Xueqi Cheng. 2018a. Learning to con-
trol the specificity in neural response generation. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), vol-
ume 1, pages 1108–1117.

Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan,
Xiujun Li, Chris Brockett, and Bill Dolan. 2018b.
Generating informative and diverse conversational
responses via adversarial information maximization.
In Proceedings of the 32nd Conference on Neu-
ral Information Processing Systems (NeuIPS), pages
1815–1825.


