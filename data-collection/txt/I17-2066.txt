



















































Investigating the Effect of Conveying Understanding Results in Chat-Oriented Dialogue Systems


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 389–394,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Investigating the Effect of Conveying Understanding Results in
Chat-Oriented Dialogue Systems

Koh Mitsuda Ryuichiro Higashinaka Junji Tomita
NTT Media Intelligence Laboratories, Nippon Telegraph and Telephone Corporation

{mitsuda.ko,higashinaka.ryuichiro,tomita.junji}@lab.ntt.co.jp

Abstract

In dialogue systems, conveying under-
standing results of user utterances is im-
portant because it enables users to feel un-
derstood by the system. However, it is not
clear what types of understanding results
should be conveyed to users; some utter-
ances may be offensive and some may be
too commonsensical. In this paper, we ex-
plored the effect of conveying understand-
ing results of user utterances in a chat-
oriented dialogue system by an experiment
using human subjects. As a result, we
found that only certain types of under-
standing results, such as those related to
a user’s permanent state, are effective to
improve user satisfaction. This paper clar-
ifies the types of understanding results that
can be safely uttered by a system.

1 Introduction

Current dialogue systems often convey the un-
derstanding results of user utterances for confir-
mation and for showing understanding. Task-
oriented dialogue systems repeat information pro-
vided by users by using understanding results of
user utterances to confirm the content of user ut-
terances (Litman and Silliman, 2004; Raux et al.,
2005). Chat-oriented dialogue systems also need
to confirm the content of user utterances and to
show understanding so that the systems can be
more affective.

However, some of the understanding results
should not be conveyed to users. For instance,
some utterances (e.g. “You are stubborn.”) may
be offensive and some (e.g. “It is summer.”) may
be too commonsensical. To create a dialogue sys-
tem which conveys one’s understanding results,

we need to know what types of the results can be
used as system utterances.

In this paper, focusing on chat-oriented dia-
logue systems, we investigate the effects of con-
veying understanding results of user utterances.
Specifically, we investigate the types of results that
can be conveyed to users without lowering user
satisfaction. For this purpose, we first prepared
various types of understanding results. Then, by
a subjective experiment, we examined their indi-
vidual effects on user satisfaction. Note that, in
this paper, we focus on the effects of system utter-
ances that convey understanding results “as they
are”; that is, utterances are literally the same as
understanding results.

As a result of the experiment, we found that
user’s temporary states during dialogue should not
be conveyed and user’s permanent states and infor-
mation irrelevant to users themselves can be con-
veyed safely as system utterances. Our results are
useful for creating a dialogue system that conveys
understanding results.

2 Data of Understanding Results

For our investigation, we need to prepare under-
standing results categorized by their types. For
this purpose, we use a corpus of PerceivedInfo col-
lected in our previous work (Mitsuda et al., 2017).
In this corpus, user utterances in chat-oriented dia-
logue are associated with the information that can
be perceived/inferred by humans from these ut-
terances. Such information is called Perceived-
Info (perceived information).

Figure 1 shows an example of a chat-oriented
dialogue and their PerceivedInfo in the corpus. As
stimuli for collecting PerceivedInfo, a Japanese
chat-oriented dialogue corpus (Higashinaka et al.,
2014) was used. PerceivedInfo was written by
multiple annotators using natural sentences with

389



: utterance

: Hello, nice to meet you!

: Nice to meet you too.

: I feel autumn coming, how about you?

: I think so too.

: The cicadas have gotten quiet recently.

...

: Do you go anywhere interesting in autumn?

: I'll visit Mt. Fuji if I feel up to it.

...

: Let's talk about this next time.

: Okay.

doesn't mind going a long way.

drives a car.

is active.

likes going on pleasure trips.

likes mountains.

likes Mt. Fuji.

likes autumn leaves around Mt. Fuji.

likes the outdoors.

lives in Kanto prefecture.

lives near Mt. Fuji.

would like to be surprised.

Mt. Fuji is famous for autumn leaves.

Perceived information for Chat-oriented dialogue

Figure 1: Example of chat-oriented dialogue and perceived information in PerceivedInfo corpus. A and
B correspond to speakers.

Level 1 Level 2 Level 3 Description Example

Thought
(55.1%)

Thought
(35.8%)

Belief self (30.7%) Speaker’s belief for him/herself A likes watching TV.
Belief other (5.1%) Speaker’s belief toward listener A agrees with B.

Desire
(19.3%)

Desire (9.9%) Speaker’s desire relative to him/herself A wants to talk about Mt. Fuji.
Request (9.4%) Speaker’s request to listener A wants to be praised by B.

Fact
(44.9%)

A’s fact
(37.9%)

Attribute (20.2%) User-modeling information of speaker A is married., A can drink.
Behavior (14.4%) Speaker’s action A drives a car., A is thinking.
Circumstance (3.3%) Background around speaker A is close with friends.

Other fact
(7.0%)

Certain fact (3.9%) Certain fact irrelevant to speaker Mt. Fuji is famous for red leaves.
Uncertain fact (3.1%) Uncertain fact irrelevant to speaker A rice crop may fail.

Table 1: Classification of perceived information used for investigation. A and B correspond to speakers.

regard to each utterance in the dialogue.
Table 1 shows the classification of Perceived-

Info created in our previous work. These types
were determined by manual clustering. The clas-
sification was evaluated by inter-annotator agree-
ment among three annotators using 3,000 in-
stances of PerceivedInfo with “Level 3” types.
Fleiss’ κ showed substantial agreement (0.69), in-
dicating the validity of the classification.

In this work, we use the PerceivedInfo in this
corpus as understanding results and investigate the
effects of system utterances that convey Perceived-
Info. We also investigate how the effects are dif-
ferent depending on the types of PerceivedInfo in
the classification.

3 Experiment

Using PerceivedInfo, we evaluated the effects of
system utterances conveying the understanding re-
sults in an experiment. Below, we explain the pro-
cedure to create the utterances for the experiment
and how we evaluate them.

Figure 2 shows the flow of preparation and eval-
uation. We first select pairs of PerceivedInfo and
a user utterance used for writing that Perceived-
Info from the corpus. The writers rewrite or refer

Create system utterances (by 2 writers)

Sample PerceivedInfo (with utterance)

UtterancePerceivedInfo

PerceivedInfo

Evaluation scores

Evaluate utterances (by 3 raters)

PerceivedInfo corpus

Automatic

Repetition Human

Figure 2: Preparation and evaluation of system ut-
terances

to the PerceivedInfo and utterance to create sys-
tem utterances. Finally, raters evaluate them by
questionnaire, giving a score to each utterance.

3.1 Types of System Utterances

Table 2 shows the four types of system utter-
ances prepared for evaluation. Utterances from
PerceivedInfo are compared with those of three
other types; namely, “Automatic,” “Repetition,”
and “Human.” “PerceivedInfo” is described be-
low.

390



System utterance Description Example
PerceivedInfo Confirmation of perceived information You are active, aren’t you?
Automatic Response generated by a chat-oriented dialogue system Do you work at Mt. Fuji?
Repetition Repetition of user utterance in tag question form You will visit Mt. Fuji, won’t you?
Human Response created by writer using keyword in user utterance Mt. Fuji is a good place, isn’t it?

Table 2: Types of system utterances prepared for evaluation. “Example” column shows system utterance
when user utterance is “I’ll visit Mt. Fuji if I feel up to it.” (utterance U13 in Figure 1).

PerceivedInfo This utterance simply conveys
PerceivedInfo in the form of confirmation.
The utterance ends with a tag question form
to confirm the content of PerceivedInfo.
Rewriting PerceivedInfo is done manually.
The symbols A and B that indicate speakers
are changed to “You” or “I”.

3.2 Types of Utterances for Comparison
We prepared three other types of system utterances
for comparison. “Automatic” emulates the utter-
ance of a chat-oriented dialogue system that is cur-
rently available. “Repetition” represents a simple
repetition of the content of a user utterance. “Hu-
man” is an utterance conceived by human.

Automatic This utterance is an automatic re-
sponse from a chat-oriented dialogue system
that generates an utterance on the basis of
keywords extracted from user utterances. To
prepare responses, we use a Japanese chat-
oriented dialogue system by NTT DOCOMO
(Onishi and Yoshimura, 2014).

Repetition This utterance is a repetition of a pred-
icate argument structure in a user utterance
(Higashinaka et al., 2014). It ends with a
tag question form (in Japanese, “desu ne”) to
show that the system understands the content
of a user utterance. The utterance is manually
created by extracting and rewriting a predi-
cate argument structure from the user utter-
ance.

Human This utterance is a human-level utterance
(i.e., upper bound). We prepare it by hav-
ing writers manually write an appropriate re-
sponse to a keyword in the user utterance.
Writers are instructed to select their favorite
keyword in the utterance and use it to create
a response that would satisfy users.

3.3 Preparation and Evaluation of
Utterances

To clarify the difference of effects caused by
the types of PerceivedInfo, we randomly selected

approximately the same number of Perceived-
Info from each type in “Level 3” shown in Table 1.
In total, we prepared 500 instances of Perceived-
Info; that is, 500 PerceivedInfo and user utterances
associated with PerceivedInfo.

Using the 500 PerceivedInfo and utterances,
“PerceivedInfo” and “Repetition” were written by
a single writer and two versions of “Human” were
written by two writers (Writer1 and Writer2) in-
dependently. We evaluated both utterances of
“Human” written by the writers, because the qual-
ity of “Human” may depend on the writer. “Auto-
matic” were generated from the chat-oriented di-
alogue system by NTT DOCOMO using the ut-
terance as an input to the system. Following this
experimental set-up, we prepared five types of ut-
terances (including two versions of “Human”) for
each pair of PerceivedInfo and a user utterance,
totalling 2,500 utterances, for evaluation.

To evaluate how each utterance is usable as a
system utterance in dialogue, we annotated “natu-
ralness” to the utterances. Raters were instructed
to evaluate how natural the response was in the
chat-oriented dialogue and to annotate an abso-
lute score for each utterance in one of seven grades
from one (very unnatural) to seven (very natural).
They evaluated the five types of utterances at the
same time. They could see not only a user utter-
ance and system utterance but also the context be-
fore the user utterance. Three raters worked inde-
pendently.

3.4 Results of Subjective Evaluation
Table 3 shows the results of the evaluation, where
the average scores annotated by three raters to the
five types of utterances are listed. The results
show that “Human by Writer1” and “Human by
Writer2” were ranked the highest by all raters,
with “Automatic” ranked as the lowest. The or-
der of the evaluated scores tended to be consistent
in all raters (“Human by Writer1,” “Human by
Writer2,” “Repetition,” “PerceivedInfo,” and “Au-
tomatic”). Spearman’s rank correlation coefficient
between two annotators averages at 0.56. “Per-

391



System utterance Rater1 Rater2 Rater3 Average
PerceivedInfo 2.7 3.1 3.1 3.0
Automatic 2.1 2.3 2.2 2.2
Repetition 3.7 4.0 4.6 4.1
Human by Writer1 4.5 5.4 5.5 5.1
Human by Writer2 4.5 5.1 5.5 5.0

Table 3: Naturalness scores of system utterances
annotated by three raters

ceivedInfo” was evaluated as being more natural
than “Automatic,” but less natural than “Repeti-
tion.”

From this result, we can say that using only Per-
ceivedInfo as system utterances is not an effective
method. However, since there may be a differ-
ence among the types of PerceivedInfo, we further
investigated the evaluation scores in each type of
PerceivedInfo.

Figure 3 shows the averaged naturalness scores
annotated for each type of “PerceivedInfo.” The
scores were clearly divided into three ranges: 1–
2, 2–4, and 4–5, and defined as Low-rate type,
Mid-rate type, and High-rate type, respectively.
We investigated what PerceivedInfo exist in each
type and the reasons for their high or low rating.
For reference, we list examples and scores of Per-
ceivedInfo in each type in Table 4.

Low-rate type An utterance in the Low-rate
type mainly refers to user’s temporary states,
such as thoughts, emotion, or behavior during
dialogue (e.g., “You want me to agree, don’t
you?”). Even an utterance that includes a
positive expression (e.g., “You like me, don’t
you?”) tends to be evaluated as unnatural.
This can be partly explained by the polite-
ness theory (Brown and Levinson, 1987). Ut-
terances in the Low-rate type that mention a
user’s temporary state would create the need
for the user to explain. Thus, a user’s nega-
tive face, the desire to be left free to act as he
or she chooses, can be threatened.

Mid-rate type An utterance in the Mid-rate
type generally refers to user’s permanent
states, such as favorites, experience, or pro-
files (e.g., “You like cool cars, don’t you?”).
Such an utterance tends to be evaluated as
natural as “Repetition.” However, an utter-
ance including a negative expression (e.g.,
“You are stubborn, aren’t you?”) or a part
of profiles (e.g. “You are a woman, aren’t
you?”) tends to be evaluated as unnatural.

1 2 3 4 5 6 7

Uncertain fact

Certain fact

Behavior

Circumstance

Belief self

Attribute

Desire

Belief other

Request

Naturalness score

Low-rate type

High-rate
type

Mid-rate type

Figure 3: Naturalness scores of system utterances
conveying perceived information on each type in
Table 1

This means that a mention of something neg-
ative or private about the user is not a good
option. This can also be explained by the po-
liteness theory as a violation of a user’s pos-
itive face; that is a desire to keep self-image
approved.

High-rate type An utterance in the High-rate
type generally refers to the content that does
not directly relate to users, such as general
facts (e.g., “A trip abroad is expensive, isn’t
it?”). Many utterances are evaluated as more
natural than “Repetition” and as natural as
“Human.” This may be because the utter-
ances in the High-rate type do not threaten
a user’s face because the content of the utter-
ances has no direct relation to users.

From the experiment, we found that utterances
created from specific types of PerceivedInfo are
evaluated as more natural than others. Our results
conform to the politeness theory and further pro-
vide quantitative evaluation of utterances that con-
vey PerceivedInfo. One interesting thing is that
the violation of the negative face has more im-
pact on the naturalness when compared to that of
the positive face. It is of great interest that, al-
though much PerceivedInfo occurs during a dia-
logue, only a part of it can be uttered.

Although further investigation is needed, our re-
sults are useful for providing a guideline for cre-
ating system utterances that convey understanding
results.

392



Rate PerceivedInfo Worst utterance Score Best utterance Score

Low

Request
You want me to agree, don’t you? 1.0 (1,1,1) You want to talk about China, don’t you? 2.0 (1,2,3)
You want to talk, don’t you? 1.0 (1,1,1) You want me to go to a gym, don’t you? 1.7 (1,3,1)
You want me to know about you, don’t you? 1.0 (1,1,1) You want to talk ordinarily, don’t you? 1.7 (1,3,1)

Belief other
You trust me, don’t you? 1.0 (1,1,1) You agree with me, don’t you? 2.7 (3,3,2)
You like me, don’t you? 1.0 (1,1,1) You are interested in my topic, aren’t you? 2.3 (3,3,1)
You misunderstand me, don’t you? 1.0 (1,1,1) My impression is changing, isn’t it? 2.0 (3,2,1)

Desire
You want to change the topic, don’t you? 1.0 (1,1,1) You long for nomadic life, don’t you? 4.7 (5,2,7)
You want to boast of your partner, don’t you? 1.0 (1,1,1) You want to go to Germany, don’t you? 4.3 (4,4,5)
You want to sympathize with me, don’t you? 1.0 (1,1,1) You want to live in cool place, don’t you? 4.3 (4,5,4)

Mid

Attribute
You are a woman, aren’t you? 1.0 (1,1,1) You are sociable, aren’t you? 6.3 (5,7,7)
You are easygoing, aren’t you? 1.0 (1,1,1) You are willing to go out, aren’t you? 6.3 (5,7,7)
You are weak, aren’t you? 1.0 (1,1,1) You are kind, aren’t you? 5.7 (3,7,7)

Belief self
You are boastful, aren’t you? 1.0 (1,1,1) You like cool cars, don’t you? 6.3 (5,7,7)
You are a little embarrassed, aren’t you? 1.3 (1,2,1) You like Mt. Fuji, don’t you? 6.0 (5,6,7)
You are uninterested in agriculture, aren’t you? 1.7 (1,3,1) You like the outdoors, don’t you? 5.3 (5,4,7)

Circumstance
There is a computer in your home, isn’t there? 1.7 (2,2,1) You belong to the soccer team, don’t you? 6.0 (5,6,7)
You live on your husband’s earnings, don’t you? 1.7 (2,2,1) It is sunny around you, isn’t it? 5.3 (3,6,7)
Your parents are well, aren’t they? 2.0 (2,3,1) Your relatives like celebrations, don’t they? 5.0 (4,4,7)

Behavior
You think about what to say, don’t you? 1.3 (2,1,1) You went out this summer, didn’t you? 5.3 (3,6,7)
You show me interests, don’t you? 1.3 (1,2,1) You lost your appetite due to how the food looks. 5.3 (4,5,7)
You think what to say next,don’t you? 1.3 (2,1,1) You drink herb tee, don’t you? 5.0 (2,7,6)

High

Certain fact
It is September, isn’t it? 3.0 (3,5,1) Wheels are expensive, aren’t they? 6.7 (7,6,7)
Bikes have various price ranges, don’t they? 3.3 (3,3,4) It is humid, isn’t it? 6.3 (7,5,7)
Road bikes and bicycles are different, aren’t they? 3.7 (3,4,4) Curved handlebars are special, aren’t they? 6.3 (6,6,7)

Uncertain fact
Using computers takes a lot of time, doesn’t it? 2.0 (1,4,1) A trip abroad is expensive, isn’t they? 6.7 (7,6,7)
That is a bad restaurant, isn’t it? 3.3 (2,3,5) Nagatomo showed great activity, didn’t he? 6.3 (7,5,7)
Muscle pain arises the next day, doesn’t it? 3.7 (4,4,3) Germany is safe, isn’t it? 6.3 (6,5,7)

Table 4: Best and worst three utterances conveying perceived information on each type in Table 1.
“Score” column shows average score and each score annotated by three raters.

4 Related Work

Although there has been no studies that explored
the effect of utterances conveying system’s under-
standing results to users, there have been several
that have explored what linguistic behavior can be
used or how to utter contents in dialogue systems
from the viewpoints of social aspects (especially
on the politeness theory).

For example, Gupta et al. constructed a task-
oriented dialogue system in the cooking domain
in which utterance generation is performed on the
basis of the politeness theory (Gupta et al., 2007).
Wang et al. estimated the politeness of each ut-
terance in a task-oriented dialogue system by us-
ing various features, such as insults or criticisms
(Wang et al., 2012). Danescu et al. constructed a
corpus in which politeness is annotated in online
community data and constructed a model for es-
timating politeness using linguistic features, such
as gratitude expressions or positive and negative
lexicons (Danescu-Niculescu-Mizil et al., 2013).

5 Conclusion

In this paper, we investigated what types of under-
standing results can be used as system utterances.
Using the corpus of PerceivedInfo (perceived in-
formation), we manually created and evaluated
the utterances that convey PerceivedInfo. We
found that certain types of PerceivedInfo, espe-
cially those related to a user’s permanent state and

information irrelevant to users themselves, are us-
able.

For future work, we want to construct a dia-
logue system that conveys the understanding re-
sults in the way we proposed. For this purpose,
we need to create an automatic estimator of Per-
ceivedInfo. In this work, we used the understand-
ing results as they were; however, we can cre-
ate various system utterances from PerceivedInfo,
and, in such a case, other types of Perceived-
Info can be effectively used. We want to further
pursue how we can make use of PerceivedInfo in
dialogue systems.

References
Penelope Brown and Stephen C. Levinson. 1987. Po-

liteness: Some universals in language usage, vol-
ume 4. Cambridge university press.

Cristian Danescu-Niculescu-Mizil, Moritz Sudhof,
Dan Jurafsky, Jure Leskovec, and Christopher Potts.
2013. A computational approach to politeness with
application to social factors. In Proc. ACL. pages
250–259.

Swati Gupta, Marilyn A. Walker, and Daniela M. Ro-
mano. 2007. How rude are you?: Evaluating polite-
ness and affect in interaction. In Proc. International
Conference on Affective Computing and Intelligent
Interaction. pages 203–217.

Ryuichiro Higashinaka, Kenji Imamura, Toyomi Me-
guro, Chiaki Miyazaki, Nozomi Kobayashi, Hiroaki

393



Sugiyama, Toru Hirano, Toshiro Makino, and Yoshi-
hiro Matsuo. 2014. Towards an open domain con-
versational system fully based on natural language
processing. In Proc. COLING. pages 928–939.

Diane J Litman and Scott Silliman. 2004. ITSPOKE:
An intelligent tutoring spoken dialogue system. In
Proc. NAACL-HLT . pages 5–8.

Koh Mitsuda, Ryuichiro Higashinaka, and Yoshihiro
Matsuo. 2017. What information should a dialogue
system understand?: Collection and analysis of per-
ceived information in chat-oriented dialogue. In
Proc. IWSDS.

Kanako Onishi and Takeshi Yoshimura. 2014. Ca-
sual conversation technology achieving natural di-
alog with computers. NTT DOCOMO Technical
Journal 15(4):16–21.

Antonie Raux, Brian Langner, Dan Bohus, and Alan
W Black Maxine Eskenazi. 2005. Let’s Go Public!
taking a spoken dialogue system to the real world.
In Proc. Interspeech. pages 885–888.

William Yang Wang, Samantha Finkelstein, Amy
Ogan, Alan W Black, and Justine Cassell. 2012.
”love ya, jerkface”: Using sparse log-linear mod-
els to build positive and impolite relationships with
teens. In Proc. SIGDIAL. pages 20–29.

394


