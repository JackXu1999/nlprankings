Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 53–61,

Beijing, August 2010

53

Fast and Accurate Arc Filtering for Dependency Parsing

Shane Bergsma

Department of Computing Science

University of Alberta
sbergsma@ualberta.ca

Colin Cherry

Institute for Information Technology
National Research Council Canada

colin.cherry@nrc-cnrc.gc.ca

Abstract

We propose a series of learned arc ﬁl-
ters to speed up graph-based dependency
parsing. A cascade of ﬁlters identify im-
plausible head-modiﬁer pairs, with time
complexity that is ﬁrst linear, and then
quadratic in the length of the sentence.
The linear ﬁlters reliably predict, in con-
text, words that are roots or leaves of de-
pendency trees, and words that are likely
to have heads on their left or right. We
use this information to quickly prune arcs
from the dependency graph. More than
78% of total arcs are pruned while retain-
ing 99.5% of the true dependencies. These
ﬁlters improve the speed of two state-of-
the-art dependency parsers, with low over-
head and negligible loss in accuracy.

1 Introduction

Dependency parsing ﬁnds direct syntactic rela-
tionships between words by connecting head-
modiﬁer pairs into a tree structure. Depen-
dency information is useful for a wealth of nat-
ural language processing tasks, including ques-
tion answering (Wang et al., 2007), semantic pars-
ing (Poon and Domingos, 2009), and machine
translation (Galley and Manning, 2009).

We propose and test a series of arc ﬁlters for
graph-based dependency parsers, which rule out
potential head-modiﬁer pairs before parsing be-
gins.
In doing so, we hope to eliminate im-
plausible links early, saving the costs associated
with them, and speeding up parsing.
In addi-
tion to the scaling beneﬁts that come with faster
processing, we hope to enable richer features
for parsing by constraining the set of arcs that
need to be considered. This could allow ex-

tremely large feature sets (Koo et al., 2008), or the
look-up of expensive corpus-based features such
as word-pair mutual information (Wang et al.,
2006). These ﬁlters could also facilitate expen-
sive learning algorithms, such as semi-supervised
approaches (Wang et al., 2008).

We propose three levels of ﬁltering, which are

applied in a sequence of increasing complexity:

Rules: A simple set of machine-learned rules
based only on parts-of-speech. They prune over
25% of potential arcs with almost no loss in cover-
age. Rules save on the wasted effort for assessing
implausible arcs such as DT → DT.
Linear: A series of classiﬁers that tag words ac-
cording to their possible roles in the dependency
tree. By treating each word independently and en-
suring constant-time feature extraction, they oper-
ate in linear time. We view these as a dependency-
parsing analogue to the span-pruning proposed by
Roark and Hollingshead (2008). Our fast linear
ﬁlters prune 54.2% of potential arcs while recov-
ering 99.7% of true pairs.

Quadratic: A ﬁnal stage that looks at pairs of
words to prune unlikely arcs from the dependency
tree. By employing a light-weight feature set, this
high-precision ﬁlter can enable more expensive
processing on the remaining plausible dependen-
cies.

Collectively, we show that more than 78% of
total arcs can be pruned while retaining 99.5% of
the true dependencies. We test the impact of these
ﬁlters at both train and test time, using two state-
of-the-art discriminative parsers, demonstrating
speed-ups of between 1.9 and 5.6, with little im-
pact on parsing accuracy.

54

Investors continue to

pour cash into money funds

Figure 1: An example dependency parse.

2 Dependency Parsing

A dependency tree represents the syntactic struc-
ture of a sentence as a directed graph (Figure 1),
with a node for each word, and arcs indicat-
ing head-modiﬁer pairs (Me´lˇcuk, 1987). Though
dependencies can be extracted from many for-
malisms, there is a growing interest in predict-
ing dependency trees directly. To that end, there
are two dominant approaches: graph-based meth-
ods, characterized by arc features in an exhaus-
tive search, and transition-based methods, char-
acterized by operational features in a greedy
search (McDonald and Nivre, 2007). We focus on
graph-based parsing, as its exhaustive search has
the most to gain from our ﬁlters.

Graph-based dependency parsing ﬁnds the
highest-scoring tree according to a scoring func-
tion that decomposes under an exhaustive search
(McDonald et al., 2005). The most natural de-
composition scores individual arcs, represented as
head-modiﬁer pairs [h, m]. This enables search
by either minimum spanning tree (West, 2001) or
by Eisner’s (1996) projective parser. This paper
focuses on the projective case, though our tech-
niques transfer to spanning tree parsing. With a
linear scoring function, the parser solves:

parse(s) = argmaxt∈s X[h,m]∈t

¯w · ¯f (h, m, s)

The weights ¯w are typically learned using an
online method, such as an averaged percep-
tron (Collins, 2002) or MIRA (Crammer and
Singer, 2003). 2nd-order searches, which consider
two siblings at a time, are available with no in-
crease in asymptotic complexity (McDonald and
Pereira, 2006; Carreras, 2007).

The complexity of graph-based parsing is
bounded by two processes: parsing (carrying out
the argmax) and arc scoring (calculating ¯w ·
¯f (h, m, s)). For a sentence with n words, pro-
jective parsing takes O(n3) time, while the span-
ning tree algorithm is O(n2). Both parsers require
scores for arcs connecting each possible [h, m]

pair in s; therefore, the cost of arc scoring is also
O(n2), and may become O(n3) if the features in-
clude words in s between h and m (Galley and
Manning, 2009). Arc scoring also has a signif-
icant constant term:
the number of features ex-
tracted for an [h, m] pair. Our in-house graph-
based parser collects on average 62 features for
each potential arc, a number larger than the length
of most sentences. With the cluster-based features
suggested by Koo et al. (2008), this could easily
grow by a factor of 3 or 4.

The high cost of arc scoring, coupled with
the parsing stage’s low grammar constant, means
that graph-based parsers spend much of their time
scoring potential arcs. Johnson (2007) reports that
when arc scores have been precomputed, the dy-
namic programming component of his 1st-order
parser can process an amazing 3,580 sentences per
second.1 Beyond reducing the number of features,
the easiest way to reduce the computational bur-
den of arc scoring is to score only plausible arcs.

3 Related Work

3.1 Vine Parsing

Filtering dependency arcs has been explored pri-
marily in the form of vine parsing (Eisner and
Smith, 2005; Dreyer et al., 2006). Vine pars-
ing establishes that, since most dependencies are
short, one can parse quickly by placing a hard
constraint on arc length. As this coarse ﬁl-
ter quickly degrades the best achievable perfor-
mance, Eisner and Smith (2005) also consider
conditioning the constraint on the part-of-speech
(PoS) tags being linked and the direction of the
arc, resulting in a separate threshold for each
[tag(h), tag(m), dir(h, m)] triple. They sketch
an algorithm where the thresholded length for
each triple starts at the highest value seen in the
training data. Thresholds are then decreased in
a greedy fashion, with each step producing the
smallest possible reduction in reachable training
arcs. We employ this algorithm as a baseline in
our experiments. To our knowledge, vine parsing

1To calibrate this speed, consider that the publicly avail-
able 1st-order MST parser processes 16 sentences per second
on modern hardware. This includes I/O costs in addition to
the costs of arc scoring and parsing.

55

has not previously been tested with a state-of-the-
art, discriminative dependency parser.

3.2 CFG Cell Classiﬁcation
Roark and Hollingshead (2008) speed up another
exhaustive parsing algorithm, the CKY parser for
CFGs, by classifying each word in the sentence
according to whether it can open (or close) a
multi-word constituent. With a high-precision
tagger that errs on the side of permitting con-
stituents, they show a signiﬁcant improvement in
speed with no reduction in accuracy.

It is difﬁcult to port their idea directly to depen-
dency parsing without committing to a particular
search algorithm,2 and thereby sacriﬁcing some
of the graph-based formalism’s modularity. How-
ever, some of our linear ﬁlters (see Section 4.3)
were inspired by their constraints.

3.3 Coarse-to-ﬁne Parsing
Another common method employed to speed up
exhaustive parsers is a coarse-to-ﬁne approach,
where a cheap, coarse model prunes the search
space for later, more expensive models (Charniak
et al., 2006; Petrov and Klein, 2007). This ap-
proach assumes a common forest or chart repre-
sentation, shared by all granularities, where one
can efﬁciently track the pruning decisions of the
coarse models. One could imagine applying such
a solution to dependency parsing, but the exact
implementation of the coarse pass would vary ac-
cording to the choice in search algorithm. Our ﬁl-
ters are much more modular: they apply to both
1st-order spanning tree parsing and 2nd-order pro-
jective parsing, with no modiﬁcation.

Carreras et al. (2008) use coarse-to-ﬁne pruning
with dependency parsing, but in that case, a graph-
based dependency parser provides the coarse pass,
with the ﬁne pass being a far-more-expensive tree-
adjoining grammar. Our ﬁlters could become a
0th pass, further increasing the efﬁciency of their
approach.

4 Arc Filters

We propose arc ﬁltering as a preprocessing step
for dependency parsing. An arc ﬁlter removes im-

2Johnson’s (2007) split-head CFG could implement this

idea directly with little effort.

plausible head-modiﬁer arcs from the complete
dependency graph (which initially includes all
head-modiﬁer arcs). We use three stages of ﬁlters
that operate in sequence on progressively sparser
graphs: 1) rule-based, 2) linear: a single pass
through the n nodes in a sentence (O(n) complex-
ity), and 3) quadratic: a scoring of all remaining
arcs (O(n2)). The less intensive ﬁlters are used
ﬁrst, saving time by leaving fewer arcs to be pro-
cessed by the more intensive systems.

Implementations of our rule-based, linear, and

quadratic ﬁlters are publicly available at:
http://code.google.com/p/arcfilter/

4.1 Filter Framework
Our ﬁlters assume the input sentences have been
PoS-tagged. We also add an artiﬁcial root node
to each sentence to be the head of the tree’s root.
Initially, this node is a potential head for all words
in the sentence.

Each ﬁlter is a supervised classiﬁer. For exam-
ple, the quadratic ﬁlter directly classiﬁes whether
a proposed head-modiﬁer pair is not a link in the
dependency tree. Training data is created from an-
notated trees. All possible arcs are extracted for
each training sentence, and those that are present
in the annotated tree are labeled as class −1, while
those not present are +1. A similar process gener-
ates training examples for the other ﬁlters. Since
our goal is to only ﬁlter very implausible arcs, we
bias the classiﬁer to high precision, increasing the
cost for misclassifying a true arc during learning.3
Class-speciﬁc costs are command-line parame-
ters for many learning packages. One can inter-
pret the learning objective as minimizing regular-
ized, weighted loss:

min

¯w

1

2|| ¯w||2 + C1 Xi:yi=1
+C2 Xi:yi=−1

l( ¯w, yi, ¯xi)

l( ¯w, yi, ¯xi)

(1)

where l() is the learning method’s loss function,
¯xi and yi are the features and label for the ith
3Learning with a cost model is generally preferable to
ﬁrst optimizing error rate and then thresholding the predic-
tion values to select a high-conﬁdence subset (Joachims,
2005), but the latter approach was used successfully for cell
classiﬁcation in Roark and Hollingshead (2008).

56

not a h ” “ , . ; | CC PRP$ PRP EX

-RRB- -LRB-

. RP
, DT

no ∗ ← m EX LS POS PRP$
no m → ∗
not a root
no h←m DT←{DT,JJ,NN,NNP,NNS,.}
CD←CD NN←{DT,NNP}
NNP←{DT,NN,NNS}
no m→h {DT,IN,JJ,NN,NNP}→DT
NNP→IN IN→JJ

Table 1: Learned rules for ﬁltering dependency
arcs using PoS tags. The rules ﬁlter 25% of pos-
sible arcs while recovering 99.9% of true links.

training example, ¯w is the learned weight vector,
and C1 and C2 are the class-speciﬁc costs. High
precision is obtained when C2 >> C1. For an
SVM, l( ¯w, yi, ¯xi) is the standard hinge loss.

We solve the SVM objective using LIBLIN-
EAR (Fan et al., 2008). In our experiments, each
ﬁlter is a linear SVM with the typical L1 loss and
L2 regularization.4 We search for the best com-
bination of C1 and C2 using a grid search on de-
velopment data. At test time, an arc is ﬁltered if
¯w · ¯x > 0.
4.2 Rule-Based Filtering

Our rule-based ﬁlters seek to instantly remove
those arcs that are trivially implausible on the ba-
sis of their head and modiﬁer PoS tags. We ﬁrst
extract labeled examples from gold-standard trees
for whenever a) a word is not a head, b) a word
does not have a head on the left (resp. right), and
c) a pair of words is not linked. We then trained
high-precision SVM classiﬁers. The only features
in ¯x are the PoS tag(s) of the head and/or modi-
ﬁer. The learned feature weights identify the tags
and tag-pairs to be ﬁltered. For example, if a tag
has a positive weight in the not-a-head classiﬁer,
all arcs having that node as head are ﬁltered.

The classier selects a small number of high-

4We also tried L1-regularized ﬁlters. L1 encourages most
features to have zero weight, leading to more compact and
hence faster models. We found the L1 ﬁlters to prune fewer
arcs at a given coverage level, providing less speed-up at
parsing time. Both L1 and L2 models are available in our
publicly available implementation.

precision rules, shown in Table 1. Note that the
rules tend to use common tags with well-deﬁned
roles. By focusing on weighted loss as opposed
to arc frequency,
the classiﬁer discovers struc-
tural zeros (Mohri and Roark, 2006), events which
could have been observed, but were not. We
consider this an improvement over the frequency-
based length thresholds employed previously in
tag-speciﬁc vine parsing.

4.3 Linear-Time Filtering
In the linear ﬁltering stage, we ﬁlter arcs on the
basis of single nodes and their contexts, passing
through the sentences in linear time. For each
node, eight separate classiﬁers decide whether:

1. It is not a head (i.e., it is a leaf of the tree).
2. Its head is on the left/right.
3. Its head is within 5 nodes on the left/right.
4. Its head is immediately on the left/right.
5. It is the root.

For each of these decisions, we again train high-
precision SVMs with C2 >> C1, and ﬁlter di-
rectly based on the classiﬁer output.

If a word is not a head, all arcs with the given
word as head can be pruned. If a word is deemed
to have a head within a certain range on the left
or right, then all arcs that do not obey this con-
straint can be pruned. If a root is found, no other
words should link to the artiﬁcial root node. Fur-
thermore, in a projective dependency tree, no arc
will cross the root, i.e., there will be no arcs where
a head and a modiﬁer lie on either side of the root.
We can therefore also ﬁlter arcs that violate this
constraint when parsing projectively.

Søgaard and Kuhn (2009) previously proposed
a tagger to further constrain a vine parser. Their
tags are a subset of our decisions (items 4 and 5
above), and have not yet been tested in a state-of-
the-art system.

Development experiments show that

if we
could perfectly make decisions 1-5 for each word,
we could remove 91.7% of the total arcs or 95%
of negative arcs, close to the upper bound.

Features

Unlike rule-based ﬁltering, linear ﬁltering uses
a rich set of features (Table 2). Each feature is a

57

PoS-tag features
tagi
tagi, tagi−1
tagi, tagi+1
tagi−1, tagi+1
tagi−2, tagi−1
tagi+1, tagi+2
tagj, Left, j=i−5...i−1
tagj, Right, j=i+1...i+5
tagj, (i-j), j=i−5...i−1
tagj, (i-j), j=i+1...i+5

Other features
wordi
wordi+1
wordi−1
shapei
preﬁxi
sufﬁxi
i
i, n
n - i

Table 2: Linear ﬁlter features for a node at po-
sition i in a sentence of length n. Each feature
is also conjoined (unless redundant) with wordi,
tagi, shapei, preﬁxi, and sufﬁxi (both 4 letters).
The shape is the word normalized using the regu-
lar expressions [A-Z]+ → A and [a-z]+ → a.

binary indicator feature. To increase the speed of
applying eight classiﬁers, we use the same feature
vector for each of the decisions; learning gives
eight different weight vectors, one corresponding
to each decision function. Feature extraction is
constrained to be O(1) for each node, so that over-
all feature extraction and classiﬁcation remain a
fast O(n) complexity. Feature extraction would
be O(n2) if, for example, we had a feature for ev-
ery tag on the left or right of a node.

Combining linear decisions

We originally optimized the C1 and C2 param-
eter separately for each linear decision function.
However, we found we could substantially im-
prove the collective performance of the linear ﬁl-
ters by searching for the optimal combination of
the component decisions, testing different levels
of precision for each component. We selected a
few of the best settings for each decision when op-
timized separately, and then searched for the best
combination of these candidates on development
data (testing 12960 combinations in all).

4.4 Quadratic-Time Filtering
In the quadratic ﬁltering stage, a single classiﬁer
decides whether each head-modiﬁer pair should
be ﬁltered. It is trained and applied as described
in Section 4.1.

tagshm

Binary features
sign(h-m)
tagm−1, tagshm tagm+1, tagshm
tagh−1, tagshm
tagh+1, tagshm
sign(h-m), tagh, wordm
sign(h-m), wordh, tagm
Real features ⇒ values
sign(h-m) ⇒ h-m
tagh, tagm ⇒ h-m
tagk, tagshm ⇒ Count(tagk ∈ tagsh...m)
wordk, tagshm ⇒ Count(wordk ∈ wordsh...m)
Table 3: Quadratic ﬁlter features for a head at po-
sition h and a modiﬁer at position m in a sentence
of length n. Here tagshm = (sign(h-m), tagh,
tagm), while tagsh...m and wordsh...m are all the
tags (resp. words) between h and m, but within
±5 positions of h or m.

While theoretically of the same complexity as
the parser’s arc-scoring function (O(n2)),
this
process can nevertheless save time by employing
a compact feature set. We view quadratic ﬁlter-
ing as a light preprocessing step, using only a por-
tion of the resources that might be used in the ﬁnal
scoring function.

Features

Quadratic ﬁltering uses both binary and real-
valued features (Table 3). Real-valued features
promote a smaller feature space. For example,
one value can encode distance rather than separate
features for different distances. We also general-
ize the “between-tag features” used in McDonald
et al. (2005) to be the count of each tag between
the head and modiﬁer. The count may be more in-
formative than tag presence alone, particularly for
high-precision ﬁlters. We follow Galley and Man-
ning (2009) in using only between-tags within a
ﬁxed range of the head or modiﬁer, so that the ex-
traction for each pair is O(1) and the overall fea-
ture extraction is O(n2).

Using only a subset of the between-tags as fea-
tures has been shown to improve speed but im-
pair parser performance (Galley and Manning,
2009). By ﬁltering quickly ﬁrst, then scoring all
remaining arcs with a cubic scoring function in the
parser, we hope to get the best of both worlds.

58

5 Filter Experiments

Data

We extract dependency structures from the
Penn Treebank using the Penn2Malt extraction
tool,5 which implements the head rules of Yamada
and Matsumoto (2003). Following convention, we
divide the Treebank into train (sections 2–21), de-
velopment (22) and test sets (23). The develop-
ment and test sets are re-tagged using the Stanford
tagger (Toutanova et al., 2003).

Evaluation Metrics

To measure intrinsic ﬁlter quality, we deﬁne
Reduction as the proportion of total arcs re-
moved, and Coverage as the proportion of true
head-modiﬁer arcs retained. Our evaluation asks,
for each ﬁlter, what Reduction can be obtained at
a given Coverage level? We also give Time: how
long it takes to apply the ﬁlters to the test set (ex-
cluding initialization).

We compute an Upper Bound for Reduction on
development data. There are 1.2 million poten-
tial dependency links in those sentences, 96.5%
of which are not present in a gold standard depen-
dency tree. Therefore, the maximum achievable
Reduction is 96.5%.

Systems

We evaluate the following systems:

• Rules: the rule-based ﬁlter (Section 4.2)
• Lin.: the linear-time ﬁlters (Section 4.3)
• Quad.: the quadratic ﬁlter (Section 4.4)

The latter two approaches run on the output of the
previous stage. We compare to the two vine pars-
ing approaches described in Section 3.1:

• Len-Vine uses a hard limit on arc length.
• Tag-Vine (later, Vine)

learns a maxi-
mum length for dependency arcs for every
head/modiﬁer tag-combination and order.

5.1 Results
We set each ﬁlter’s parameters by selecting
a Coverage-Reduction tradeoff on development

5http://w3.msi.vxu.se/∼nivre/research/Penn2Malt.

html

Upper Bd
Lin-Orac.
Quad
Lin
Tag-Vine
Len-Vine

 100

 90

 80

 70

 60

 50

 40

 30

)

%

(
 

n
o

i
t
c
u
d
e
R

 20

 99.3 99.4 99.5 99.6 99.7 99.8 99.9

Coverage (%)

Figure 2: Filtering performance for different ﬁl-
ters and cost parameters on development data.
Lin-Orac indicates the percentage ﬁltered using
perfect decisions by the linear components.

Filter
Vine
Rules
Lin.
Quad.

Coverage Reduct. Time (s)
2.9s
1.3s
7.3s
16.1s

99.62
99.86
99.73
99.50

44.0
25.8
54.2
78.4

Table 4: Performance (%) of ﬁlters on test data.

data (Figure 2). The Lin curve is obtained by vary-
ing both the C1/C2 cost parameters and the combi-
nation of components (plotting the best Reduction
at each Coverage level). We chose the linear ﬁl-
ters with 99.8% Coverage at a 54.2% Reduction.
We apply Quad on this output, varying the cost
parameters to produce its curve. Aside from Len-
Vine, all ﬁlters remove a large number of arcs with
little drop in Coverage.

After selecting a desired trade-off for each clas-
siﬁer, we move to ﬁnal ﬁltering experiments on
unseen test data (Table 4). The linear ﬁlter re-
moves well over half the links but retains an as-
tounding 99.7% of correct arcs. Quad removes
78.4% of arcs at 99.5% Coverage. It thus reduces
the number of links to be scored by a dependency
parser by a factor of ﬁve.

The time for ﬁltering the 2416 test sentences
varies from almost instantaneous for Vine and
Rules to around 16 seconds for Quad. Speed num-
bers are highly machine, design, and implemen-

59

Decision
No-Head
Right-∅
Left-∅
Right-5
Left-5
Right-1
Left-1
Root

Precision Recall
44.8
28.7
39.0
31.5
19.7
6.2
27.3
25.5

99.9
99.9
99.9
99.8
99.9
99.7
99.7
98.6

Table 5: Linear Filters: Test-set performance (%)
on decisions for components of the combined 54.2
Reduct./99.73 Coverage linear ﬁlter.

Type

99.73
99.76
99.74
99.75
99.74
99.74
99.75
99.73
99.76

Coverage Reduct. Oracle
91.8
87.2
91.4
90.7
89.7
90.4
90.8
90.6
90.0

All
All\No-Head
All\Left-∅
All\Right-∅
All\Left-5
All\Right-5
All\Left-1
All\Right-1
All\Root
Table 6: Contribution of different linear ﬁlters to
test set performance (%). Oracle indicates the per-
centage ﬁltered by perfect decisions.

54.2
46.4
53.2
53.6
53.2
51.6
53.5
53.9
50.2

tation dependent, and thus we have stressed the
asymptotic complexity of the ﬁlters. However, the
timing numbers show that arc ﬁltering can be done
quite quickly. Section 6 conﬁrms that these are
very reasonable costs in light of the speed-up in
overall parsing.

5.2 Linear Filtering Analysis
It is instructive to further analyze the components
of the linear ﬁlter. Table 5 gives the performance
of each classiﬁer on its speciﬁc decision. Preci-
sion is the proportion of positive classiﬁcations
that are correct. Recall is the proportion of pos-
itive instances that are classiﬁed positively (e.g.
the proportion of actual roots that were classiﬁed
as roots). The decisions correspond to items 1-5 in
Section 4.3. For example, Right-∅ is the decision
that a word has no head on the right.
Most notably, the optimum Root decision has
much lower Precision than the others, but this has

little effect on its overall accuracy as a ﬁlter (Ta-
ble 6). This is perhaps because the few cases of
false positives are still likely to be main verbs or
auxiliaries, and thus still still likely to have few
links crossing them. Thus many of the ﬁltered
links are still correct.

Table 6 provides the performance of the classi-
ﬁer combination when each linear decision is ex-
cluded. No-Head is the most important compo-
nent in the oracle and the actual combination.

6 Parsing Experiments

6.1 Set-up

In this section, we investigate the impact of our ﬁl-
ters on graph-based dependency parsers. We train
each parser unﬁltered, and then measure its speed
and accuracy once ﬁlters have been applied. We
use the same training, development and test sets
described in Section 5. We evaluate unlabeled de-
pendency parsing using head accuracy: the per-
centage of words (ignoring punctuation) that are
assigned the correct head.

The ﬁlters bypass feature extraction for each ﬁl-
tered arc, and replace its score with an extremely
low negative value. Note that 2nd-order features
consider O(n3) [h, m1, m2] triples. These triples
are ﬁltered if at least one component arc ([h, m1]
or [h, m2]) is ﬁltered.

In an optimal implementation, we might also
have the parser re-use features extracted during
ﬁltering when scoring the remaining arcs. We did
not do this. Instead, ﬁltering was treated as a pre-
processing step, which maximizes the portability
of the ﬁlters across parsers. We test on two state-
of-the art parsers:

MST We modiﬁed the publicly-available MST
parser (McDonald et al., 2005)6 to employ our ﬁl-
ters before carrying out feature extraction. MST
is trained with 5-best MIRA.

DepPercep We also test an in-house depen-
dency parser, which conducts projective ﬁrst and
2nd-order searches using the split-head CFG de-
scribed by Johnson (2007), with a weight vec-
tor trained using an averaged perceptron (Collins,

6http://sourceforge.net/projects/mstparser/

60

DepPercep-1 DepPercep-2

MST-1

MST-2

Filter
None
Vine
Rules
Linear
Quad.

Cost Acc.
91.8
91.7
91.7
91.7
91.7

+0
+3
+1
+7
+16

Time Acc.
92.5
92.3
92.4
92.4
92.3

348
192
264
168
79

Time Acc. Time Acc. Time
200
139
167
121
80

91.2
91.2
91.2
91.2
91.2

91.9
91.8
91.9
91.8
91.8

832
407
609
334
125

153
99
125
88
58

Table 7: The effect of ﬁltering on the speed and accuracy on 1st and 2nd-order dependency parsing.

2002).
Its features are a mixture of those de-
scribed by McDonald et al. (2005), and those used
in the Koo et al. (2008) baseline system; we do not
use word-cluster features.

DepPercep makes some small improvements to
MST’s 1st-order feature set. We carefully de-
termined which feature types should have dis-
tance appended in addition to direction. Also, in-
spired by the reported utility of mixing PoS tags
and word-clusters (Koo et al., 2008), we created
versions of all of the “Between” and “Surround-
ing Word” features described by McDonald et al.
(2005) where we mix tags and words.7

DepPercep was developed with quadratic ﬁlters
in place, which enabled a fast development cycle
for feature engineering. As a result, it does not
implement many of the optimizations in place in
MST, and is relatively slow unﬁltered.

6.2 Results

The parsing results are shown in Table 7, where
times are given in seconds, and Cost indicates the
additional cost of ﬁltering. Note that the impact
of all ﬁlters on accuracy is negligible, with a de-
crease of at most 0.2%. In general, parsing speed-
ups mirror the amount of arc reduction measured
in our ﬁlter analysis (Section 5.1).

Accounting for ﬁlter costs,

the beneﬁts of
quadratic ﬁltering depend on the parser. The extra
beneﬁt of quadratic over linear is substantial for
DepPercep, but less so for 1st-order MST.

MST shows more modest speed-ups than Dep-
Percep, but MST is already among the fastest
publicly-available data-driven parsers. Under
quadratic ﬁltering, MST-2 goes from processing

12 sentences per second to 23 sentences.8

DepPercep-2 starts slow, but beneﬁts greatly
from ﬁltering. This is because, unlike MST-2,
it does not optimize feature extraction by fac-
toring its ten 2nd-order features into two triple
([h, m1, m2]) and eight sibling ([m1, m2]) fea-
tures. This suggests that ﬁltering could have a dra-
matic effect on a parser that uses more than a few
triple features, such as Koo et al. (2008).

7 Conclusion

We have presented a series of arc ﬁlters that speed
up graph-based dependency parsing. By treat-
ing ﬁltering as weighted classiﬁcation, we learn a
cascade of increasingly complex ﬁlters from tree-
annotated data. Linear-time ﬁlters prune 54%
of total arcs, while quadratic-time ﬁlters prune
78%. Both retain at least 99.5% of true dependen-
cies. By testing two state-of-the-art dependency
parsers, we have shown that our ﬁlters produce
substantial speed improvements in even carefully-
optimized parsers, with negligible losses in ac-
curacy.
In the future we hope to leverage this
reduced search space to explore features derived
from large corpora.

References

Carreras, Xavier, Michael Collins, and Terry Koo.
2008. TAG, dynamic programming, and the percep-
tron for efﬁcient, feature-rich parsing. In CoNLL.

Carreras, Xavier. 2007. Experiments with a higher-
In EMNLP-

order projective dependency parser.
CoNLL.

7This was enabled by using word features only when the

word is among the 800 most frequent in the training set.

8This speed accounts for 25 total seconds to apply the

rules, linear, and quadratic ﬁlters.

61

Charniak, Eugene, Mark Johnson, Micha Elsner,
Isaac Haxton,
Joseph Austerweil, David Ellis,
Catherine Hill, R. Shrivaths,
Jeremy Moore,
Michael Pozar, and Theresa Vu. 2006. Multilevel
coarse-to-ﬁne PCFG parsing. In HLT-NAACL.

Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.

Crammer, Koby and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
JMLR, 3:951–991.

Dreyer, Markus, David A. Smith, and Noah A. Smith.
2006. Vine parsing and minimum risk reranking for
speed and precision. In CoNLL.

Eisner, Jason and Noah A. Smith. 2005. Parsing with
soft and hard constraints on dependency length. In
IWPT.

Eisner, Jason. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COL-
ING.

Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classiﬁcation.
JMLR,
9:1871–1874.

Mohri, Mehryar and Brian Roark. 2006. Probabilistic
context-free grammar induction based on structural
zeros. In HLT-NAACL.

Petrov, Slav and Dan Klein. 2007. Improved inference

for unlexicalized parsing. In HLT-NAACL.

Poon, Hoifung and Pedro Domingos. 2009. Unsuper-

vised semantic parsing. In EMNLP.

Roark, Brian and Kristy Hollingshead. 2008. Classi-
fying chart cells for quadratic complexity context-
free inference. In COLING.

Søgaard, Anders and Jonas Kuhn. 2009. Using a max-
imum entropy-based tagger to improve a very fast
vine parser. In IWPT.

Toutanova, Kristina, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL.

Wang, Qin Iris, Colin Cherry, Dan Lizotte, and Dale
Schuurmans. 2006. Improved large margin depen-
dency parsing via local constraints and Laplacian
regularization. In CoNLL.

Wang, Mengqiu, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? A quasi-
synchronous grammar for QA. In EMNLP-CoNLL.

Galley, Michel and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine
translation. In ACL-IJCNLP.

Wang, Qin Iris, Dale Schuurmans, and Dekang Lin.
2008. Semi-supervised convex training for depen-
dency parsing. In ACL-08: HLT.

Joachims, Thorsten. 2005. A support vector method

for multivariate performance measures. In ICML.

West, D. 2001. Introduction to Graph Theory. Pren-

tice Hall, 2nd edition.

Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In IWPT.

Johnson, Mark. 2007. Transforming projective bilex-
ical dependency grammars into efﬁciently-parsable
CFGs with unfold-fold. In ACL.

Koo, Terry, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL-08: HLT.

McDonald, Ryan and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In EMNLP-CoNLL.

McDonald, Ryan and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In EACL.

McDonald, Ryan, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In ACL.

Me´lˇcuk, Igor A. 1987. Dependency syntax: theory
and practice. State University of New York Press.

