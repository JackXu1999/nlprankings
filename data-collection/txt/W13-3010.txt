










































Investigating Connectivity and Consistency Criteria for Phrase Pair Extraction in Statistical Machine Translation


Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 93–101,
Sofia, Bulgaria, August 9, 2013. c©2013 Association for Computational Linguistics

Investigating Connectivity and Consistency Criteria for Phrase Pair
Extraction in Statistical Machine Translation

Spyros Martzoukos, Christophe Costa Florêncio and Christof Monz
Intelligent Systems Lab Amsterdam, University of Amsterdam

Science Park 904, 1098 XH Amsterdam, The Netherlands
{S.Martzoukos, C.Monz}@uva.nl, chriscostafl@gmail.com

Abstract

The consistency method has been estab-
lished as the standard strategy for extract-
ing high quality translation rules in statis-
tical machine translation (SMT). However,
no attention has been drawn to why this
method is successful, other than empiri-
cal evidence. Using concepts from graph
theory, we identify the relation between
consistency and components of graphs that
represent word-aligned sentence pairs. It
can be shown that phrase pairs of interest
to SMT form a sigma-algebra generated
by components of such graphs. This con-
struction is generalized by allowing seg-
mented sentence pairs, which in turn gives
rise to a phrase-based generative model. A
by-product of this model is a derivation of
probability mass functions for random par-
titions. These are realized as cases of con-
strained, biased sampling without replace-
ment and we provide an exact formula for
the probability of a segmentation of a sen-
tence.

1 Introduction

A parallel corpus, i.e., a collection of sentences in
a source and a target language, which are trans-
lations of each other, is a core ingredient of ev-
ery SMT system. It serves the purpose of training
data, i.e., data from which translation rules are ex-
tracted. In its most basic form, SMT does not re-
quire the parallel corpus to be annotated with lin-
guistic information, and human supervision is thus
restricted to the construction of the parallel corpus.

The extraction of translation rules is done by ap-
propriately collecting statistics from the training

data. The pioneering work of (Brown et al., 1993)
identified the minimum assumptions that should
be made in order to extract translation rules and
developed the relevant models that made such ex-
tractions possible.

These models, known as IBM models, are based
on standard machine learning techniques. Their
output is a matrix of word alignments for each sen-
tence pair in the training data. These word align-
ments provide the input for later approaches that
construct phrase-level translation rules which may
(Wu, 1997; Yamada and Knight, 2001) or may not
(Och et al., 1999; Marcu and Wong, 2002) rely on
linguistic information.

The method developed in (Och et al., 1999),
known as the consistency method, is a simple yet
effective method that has become the standard way
of extracting (source, target)-pairs of phrases as
translation rules. The development of consistency
has been done entirely on empirical evidence and
it has thus been termed a heuristic.

In this work we show that the method of (Och
et al., 1999) actually encodes a particular type of
structural information induced by the word align-
ment matrices. Moreover, we show that the way in
which statistics are extracted from the associated
phrase pairs is insufficient to describe the underly-
ing structure.

Based on these findings we suggest a phrase-
level model in the spirit of the IBM models. A key
aspect of the model is that it identifies the most
likely partitions, rather than alignment maps, asso-
ciated with appropriately chosen segments of the
training data. For that reason, we provide a gen-
eral construction of probability mass functions for
partitions and, in particular, an exact formula for
the probability of a segmentation of a sentence.

93



2 Definition of Consistency

In this section we provide the definition of consis-
tency, which was introduced in (Och et al., 1999),
refined in (Koehn et al., 2003), and we follow
(Koehn, 2009) in our description. We start with
some preliminary definitions.

Let S = s1...s|S| be a source sentence, i.e., a
string that consists of consecutive source words;
each word si is drawn from a source language vo-
cabulary and i indicates the position of the word
in S. The operation of string extraction from the
words of S is defined as the construction of the
string s = si1 ...sin from the words of S, with
1 ≤ i1 < ... < in ≤ |S|. If i1, ..., in are consecu-
tive, which implies that s is a substring of S, then
s is called a source phrase and we write s ⊆ S.
As a shorthand we also write sini1 for the phrase
si1 ...sin . Similar definitions apply to the target
side and we denote by T, tj and t a target sen-
tence, word and phrase respectively.

Let (S = s1s2...s|S|, T = t1t2...t|T |) be a sen-
tence pair and let A denote the |S|×|T |matrix that
encodes the existence/absence of word alignments
in (S, T ) as

A(i, j) =

{
1, if si and tj are aligned
0, otherwise,

(1)

for all i = 1, ..., |S| and j = 1, ..., |T |. Un-
aligned words are allowed. A pair of strings (s =
si1 ...si|s| , t = tj1 ...tj|t|) that is extracted from
(S, T ) is termed consistent with A, if the follow-
ing conditions are satisfied:

1. s ⊆ S and t ⊆ T .

2. ∀k ∈ {1, ..., |s|} such that A(ik, j) = 1, then
j ∈ {j1, ..., j|t|}.

3. ∀l ∈ {1, ..., |t|} such that A(i, jl) = 1, then
i ∈ {i1, ..., i|s|}.

4. ∃k ∈ {1, ..., |s|} and ∃l ∈ {1, ..., |t|} such
that A(ik, jl) = 1.

Condition 1 guarantees that (s, t) is a phrase
pair and not just a pair of strings. Condition 2 says
that if a word in s is aligned to one or more words
in T , then all such target words must appear in t.
Condition 3 is the equivalent of Condition 2 for the
target words. Condition 4 guarantees the existence
of at least one word alignment in (s, t).

For a sentence pair (S, T ), the set of all consis-
tent pairs with an alignment matrix A is denoted

by P (S, T ). Figure 1(a) shows an example of a
sentence pair with an alignment matrix together
with all its consistent pairs.

In SMT the extraction of each consistent pair
(s, t) from (S, T ) is followed by a statistic
f(s, t;S, T ). Typically f(s, t;S, T ) counts the oc-
currences of (s, t) in (S, T ). By considering all
sentence pairs in the training data, the translation
probability is constructed as

p(t|s) =
∑

(S,T ) f(s, t;S, T )∑
(S,T )

∑
t′ f(s, t′;S, T )

, (2)

and similarly for p(s|t). Finally, the entries of the
phrase table consist of all extracted phrase pairs,
their corresponding translation probabilities and
other models which we do not discuss here.

3 Consistency and Components

For a given sentence pair (S, T ) and a fixed word
alignment matrix A, our aim is to show the equiva-
lence between consistency and connectivity prop-
erties of the graph formed by (S, T ) and A. More-
over, we explain that the way in which measure-
ments are performed is not compatible , in princi-
ple, with the underlying structure. We start with
some basic definitions from graph theory (see for
example (Harary, 1969)).

Let G = (V,E) be a graph with vertex set V
and edge set E. Throughout this work, vertices
represent words and edges represent word align-
ments, but the latter will be further generalized in
Section 4. A subgraph H = (V ′, E′) of G is a
graph with V ′ ⊆ V , E′ ⊆ E and the property
that for each edge in E′, both its endpoints are in
V ′. A path in G is a sequence of edges which con-
nect a sequence of distinct vertices. Two vertices
u, v ∈ V are called connected if G contains a path
from u to v. G is said to be connected if every pair
of vertices in G is connected.

A connected component, or simply component,
of G is a maximal connected subgraph of G. G
is called bipartite if V can be partitioned in sets
VS and VT , such that every edge in E connects a
vertex in VS to one in VT . The disjoint union of
graphs, or simply union, is an operation on graphs
defined as follows. For n graphs with disjoint ver-
tex sets V1, ..., Vn (and hence disjoint edge sets),
their union is the graph (∪ni=1Vi,∪ni=1Ei).

Consider the graph G whose vertices are the
words of the source and target sentences, and
whose edges are induced by the non-zero entries

94



  

t1 t2 t3 t4 t5 t6 t7
s1
s2
s3
s4
s5

s1 s2

t1 t2 t3

s3 s4

t4 t5 t6

s5

t7

s1 s3

t1 t4 t5

s2 s4

t2 t3
t6

s5

t7
,{ }C1=

G

s1 s3

t1 t4 t5

s2 s4

t2 t3 t6

s5

t7{ }C2=
s1 s3

t1 t4 t5 t6

s5

t7

s2 s4

t2 t3 t6

s2 s4

t2 t3

s5

t7

s1 s3

t1 t4 t5

s1 s3

t1 t4 t5

s2 s4

t2 t3

s5

t7{ }C3=
s1 s3

t1 t4 t5 t6

s5

t7

s2 s4

t2 t3 t6

s2 s4

t2 t3

s5

t7

s1 s3

t1 t4 t5t6

s1 s3

t1 t4 t5

s2 s4

t2 t3{ }C 4= t6
s5

t7

(s5 , t7) ,
(s1

4 , t1
5) ,(s5 , t6

7) ,
(s1

4 , t1
6) ,

(S ,T )

(a)

(b)

P (S ,T )= { }

, ,

, , , , ,

, , ,

Figure 1: (a) Left: Sentence pair with an alignment matrix. Dots indicate existence of word alignments.
Right: All consistent pairs. (b) The graph representation of the matrix in (a), and the sets generated by
components of the graph. Dark shading indicates consistency.

of the matrix A. There are no edges between
any two source-type vertices nor between any two
target-type vertices. Moreover, the source and tar-
get language vocabularies are assumed to be dis-
joint and thus G is bipartite. The set of all com-
ponents of G is defined as C1 and let k denote its
cardinality, i.e., |C1| = k. From the members of
C1 we further construct sets C2, ..., Ck as follows:
For each i, 2 ≤ i ≤ k, any member of Ci is formed
by the union of any i distinct members of C1. In
other words, any member of Ci is a graph with i
components and each such component is a mem-
ber of C1. The cardinality of Ci is clearly

(
k
i

)
, for

every i, 1 ≤ i ≤ k.
Note that Ck = {G}, since G is the union of

all members of C1. Moreover, observe that C∗ =
∪ki=1Ci is the set of graphs that can be generated
by all possible unions of G’s components. In that
sense

C = {∅} ∪ C∗ (3)

is the power set of G. Indeed we have |C| = 1 +∑k
i=1

(
k
i

)
= 2k as required.1

Figure 1(b) shows the graph G and the associ-
ated sets Ci of (S, T ) and A in Figure 1(a). Note
the bijective correspondence between consistent

1Here we used the fact that for any set X with |X| =
n, the set of all subsets of X , i.e., the power set of X , has
cardinality

Pn
i=0

`
n
i

´
= 2n.

pairs and the phrase pairs that can be extracted
from the vertices of the members of the sets Ci.
This is a consequence of consistency Conditions 2
and 3, since they provide the sufficient conditions
for component formation.

In general, if a pair of strings (s, t) satisfies the
consistency Conditions 2 and 3, then it can be ex-
tracted from the vertices of a graph in Ci, for some
i. Moreover, if Conditions 1 and 4 are also satis-
fied, i.e., if (s, t) is consistent, then we can write

P (S, T ) =
k⋃

i=1

{
(SH , TH) : H ∈ Ci,

SH ⊆ S, TH ⊆ T
}
,
(4)

where SH denotes the extracted string from the
source-type vertices of H , and similarly for TH .
Having established this relationship, when refer-
ring to members of C, we henceforth mean either
consistent pairs or inconsistent pairs. The latter
are pairs (SH , TH) for some H ∈ C such that at
least either SH 6⊆ S or TH 6⊆ T .

The construction above shows that phrase pairs
of interest to SMT are part of a carefully con-
structed subclass of all possible string pairs that
can be extracted from (S, T ). The power set C
of G gives rise to a small, possibly minimal, set

95



in which consistent and inconsistent pairs can be
measured.1 In other words, since C is (by con-
struction) a sigma-algebra, the pair (C1, C) is a
measurable space. Furthermore, one can construct
a measure space (C1, C, f), with an appropriately
chosen measure f : C → [0,∞).

Is the occurrence-counting measure f of Sec-
tion 2 a good choice? Fix an ordering for Ci, and
let Ci,j denote the jth member of Ci, for all i,
1 ≤ i ≤ k. Furthermore, let δ(x, y) = 1, if x = y
and 0, otherwise. We argue by contradiction that
the occurrence-counting measure

f(H) =
∑

{H′: H′∈C, H′ is consistent}

δ(H,H ′), (5)

fails to form a measure space. Suppose that more
than one component of G is consistent, i.e., sup-
pose that

1 <
k∑

j=1

f(C1,j) ≤ k. (6)

By construction of C, it is guaranteed that

1 = f(G) = f(Ck,1) = f(∪kj=1 C1,j). (7)

The members of C1 are pairwise disjoint, because
each of them is a component of G. Thus, since f is
assumed to be a measure, sigma-additivity should
be satisfied, i.e., we must have

f(∪kj=1 C1,j) =
k∑

j=1

f(C1,j) > 1, (8)

which is a contradiction.
In practice, the deficiency of using eq. 5 as

a statistic could possibly be explained by the
fact that the so-called lexical weights are used as
smoothing.

4 Consistency, Components and
Segmentations

In Section 3 the only relation that was assumed
among source (target) words/vertices was the or-
der of appearance in the source (target) sentence.
As a result, the graph representation G of (S, T )
and A was bipartite. There are several, linguisti-
cally motivated, ways in which a general graph can
be obtained from the bipartite graph G. We ex-
plain that the minimal linguistic structure, namely

1See Appendix for definitions.

sentence segmentations, can provide a generaliza-
tion of the construction introduced in Section 3.

Let X be a finite set of consecutive integers. A
consecutive partition of X is a partition of X such
that each part consists of integers consecutive in
X . A segmentation σ of a source sentence S is a
consecutive partition of {1, ..., |S|}. A part of σ,
i.e., a segment, is intuitively interpreted as a phrase
in S. In the graph representation G of (S, T ) and
A, a segmentation σ of S is realised by the ex-
istence of edges between consecutive source-type
vertices whose labels, i.e., word positions in S, ap-
pear in the same segment of σ. The same argument
holds for a target sentence and its words; a target
segmentation is denoted by τ .

Clearly, there are 2|S|−1 possible ways to seg-
ment S and, given a fixed alignment matrix A,
the number of all possible graphs that can be con-
structed is thus 2|S|+|T |−2. The bipartite graph
of Section 3 is just one possible configuration,
namely the one in which each segment of σ con-
sists of exactly one word, and similarly for τ . We
denote this segmentation pair by (σ0, τ0).

We now turn to extracting consistent pairs in
this general setting from all possible segmenta-
tions (σ, τ) for a sentence pair (S, T ) and a fixed
alignment matrix A. As in Section 3, we con-
struct graphs Gσ,τ , associated sets Cσ,τi , for all i,
1 ≤ i ≤ kσ,τ , and Cσ,τ , for all (σ, τ). Consistent
pairs are extracted in lieu of eq. 4, i.e.,

P σ,τ (S, T ) =
kσ,τ⋃
i=1

{
(SH , TH) : H ∈ Cσ,τi ,

SH ⊆ S, TH ⊆ T
}
, (9)

and it is trivial to see that

{(S, T )} ⊆ P σ,τ (S, T ) ⊆ P (S, T ), (10)

for all (σ, τ). Note that P (S, T ) = P σ0,τ0(S, T )
and, depending on the details of A, it is possible
for other pairs (σ, τ) to attain equality. Moreover,
each consistent pair in P (S, T ) can be be extracted
from a member of at least one Cσ,τ .

We focus on the sets Cσ,τ1 , i.e., the components
of Gσ,τ , for all (σ, τ). In particular, we are inter-
ested in the relation between P (S, T ) and Cσ,τ1 ,
for all (σ, τ). Each consistent H ∈ Cσ0,τ0 can
be converted into a single component by appropri-
ately forming edges between consecutive source-
type vertices and/or between consecutive target-
type vertices. The resulting component will evi-
dently be a member of Cσ,τ1 , for some (σ, τ). It

96



is important to note that the conversion of a con-
sistent H ∈ Cσ0,τ0 into a single component need
not be unique; see Figure 2 for a counterexam-
ple. Since (a) such conversions are possible for
all consistent H ∈ Cσ0,τ0 and (b) P (S, T ) =
P σ0,τ0(S, T ), it can be deduced that all possible
consistent pairs can be traced in the sets Cσ,τ1 , for
all (σ, τ). In other words, we have:

P (S, T ) =
⋃
σ,τ

{
(SH , TH) : H ∈ Cσ,τ1 ,

SH ⊆ S, TH ⊆ T
}
. (11)

The above equation says that by taking sen-
tence segmentations into account, we can recover
all possible consistent pairs, by inspecting only the
components of the underlying graphs.

It would be interesting to investigate the re-
lation between measure spaces (Cσ,τ1 , C

σ,τ , fσ,τ )
and different configurations for A. We leave that
for future work and focus on the advantages pro-
vided by eq. 11.

  

s2s1 s3 s4

t1 t 2 t3

s2s1 s3 s4

t1 t 2 t3

s2s1 s3 s4

t1 t 2 t3

s2s1 s3 s4

t1 t 2 t3

s2s1 s3 s4

t1 t 2 t3

Figure 2: A graph with three components (top),
and four possible conversions into a single compo-
nent by forming edges between contiguous words.

5 Towards a phrase-level model that
respects consistency

The aim of this section is to exploit the relation
established in eq. 11 between consistent pairs and
components of segmented sentence pairs. It was
also shown in Section 2 that the computation of the
translation models is inappropriate to describe the
underlying structure. We thus suggest a phrase-
based generative model in the spirit of the IBM
word-based models, which is compatible with the
construction of the previous sections.

5.1 Hidden variables
All definitions from the previous sections are car-
ried over, and we introduce a new quantity that is
associated with components. Let Gσ,τ and Cσ,τ1 ,
for some (σ, τ) be as in Section 4, then the set
K is defined as follows: Each member of K is
a pair of (source, target) sets of segments that cor-
responds to the pair of (source, target) vertices of
a consistent member of Cσ,τ1 . In other words, K is
a bisegmentation of a pair of segmented sentences
that respects consistency.

Figure 3 shows three possible ways to con-
struct consistent graphs from (S, T ) = (s41, t

6
1),

σ = {{1, 2}, {3}, {4}} ≡ {x1, x2, x3} and τ =
{{1}, {2, 3, 4}, {5}, {6}} ≡ {y1, y2, y3, y4}. In
each case the exact alignment information is un-
known and we have:

(a) K =
{ (

{x1}, {y1}
)
,

(
{x2}, {y2}

)
,(

{x3}, {y3, y4}
) }

.

(b) K =
{ (

{x1, x2}, {y1, y2, y3}
)
,(

{x3}, {y4}
)}

.

(c) K =
{ (

{x1}, {y3, y4}
)
,(

{x2, x3}, {y1, y2}
)}

.

  

t1

s1 s2 s3

t 2 t3 t5

s4

t 4 t6

t1

s1 s2 s3

t 2 t3 t5t 4 t6

s4

t1

s1 s2 s3

t 2 t3t5 t 4t6

s4

(a)

(b)

(c)

Figure 3: Three possible ways to construct con-
sistent graphs for (s41, t

6
1) and a given segmenta-

tion pair. Exact word alignment information is un-
known.

In the proposed phrase-level generative model
the random variables whose instances are σ, τ and

97



K are hidden variables. As with the IBM mod-
els, they are associated with the positions of words
in a sentence, rather than the words themselves.
Alignment information is implicitly identified via
the consistent bisegmentation K.

Suppose we have a corpus that consists of pairs
of parallel sentences (S, T ), and let fS,T denote
the occurrence count of (S, T ) in the corpus. Also,
let lS = |S| and lT = |T |. The aim is to maximize
the corpus log-likelihood function

` =
∑
S,T

fS,T log pθ(T |S)

=
∑
S,T

fS,T log
∑

σ,τ,K

pθ(T, σ, τ, K|S), (12)

where σ, τ and K are hidden variables parameter-
ized by a vector θ of unknown weights, whose val-
ues are to be determined. The expectation max-
imization algorithm (Dempster et al., 1977) sug-
gests that an iterative application of

θn+1 = arg max
θ

∑
S,T

fS,T
∑

σ,τ,K

pθn(σ, τ,K|S, T )×

log pθ(T, σ, τ, K|S),
(13)

provides a good approximation for the maximum
value of `. As with the IBM models we seek prob-
ability mass functions (PMFs) of the form

pθ(T, σ, τ, K|S) = pθ(lT |S)pθ(σ, τ,K|lT , S)×
pθ(T |σ, τ,K, lT , S),

(14)

and decompose further as

pθ(σ, τ,K|lT , S) = pθ(σ, τ |lT , S)pθ(K|σ, τ, lT , S)
(15)

A further simplification of pθ(σ, τ |lT , S) =
pθ(σ|S)pθ(τ |lT ) may not be desirable, but will
help us understand the relation between θ and the
PMFs. In particular, we give a formal description
of pθ(σ|S) and then explain that pθ(K|σ, τ, lT , S)
and pθ(T |σ, τ,K, lT , S) can be computed in a
similar way.

5.2 Constrained, biased sampling without
replacement

The probability of a segmentation given a sentence
can be realised in two different ways. We first pro-
vide a descriptive approach which is more intu-
itive, and we use the sentence S = s41 as an ex-

ample whenever necessary. The set of all possi-
ble segments of S is denoted by seg(S) and triv-
ially |seg(S)| = |S|

(
|S| + 1

)
/2. Each segment

x ∈ seg(S) has a nonnegative weight θ(x|lS) such
that ∑

x∈seg(S)

θ(x|lS) = 1. (16)

Suppose we have an urn that consists of
|seg(S)| weighted balls; each ball corresponds to
a segment of S. We sample without replacement
with the aim of collecting enough balls to form a
segmentation of S. When drawing a ball x we si-
multaneously remove from the urn all other balls
x′ such that x ∩ x′ 6= ∅. We stop when the urn
is empty. In our example, let the urn contain 10
balls and suppose that the first draw is {1, 2}. In
the next draw, we have to choose from {3}, {4}
and {3, 4} only, since all other balls contain a ‘1’
and/or a ‘2’ and are thus removed. The sequence
of draws that leads to a segmentation is thus a path
in a decision tree. Since σ is a set, there are |σ|!
different paths that lead to its formation. The set
of all possible segmentations, in all possible ways
that each segmentation can be formed, is encoded
by the collection of all such decision trees.

The second realisation, which is based on the
notions of cliques and neighborhoods, is more
constructive and will give rise to the desired PMF.
A clique in a graph is a subset U of the vertex set
such that for every two vertices u, v ∈ U , there ex-
ists an edge connecting u and v. For any vertex u
in a graph, the neighborhood of u is defined as the
set N(u) = {v : {u, v} is an edge}. A maximal
clique is a clique U that is not a subset of a larger
clique: For each u ∈ U and for each v ∈ N(u) the
set U ∪ {v} is not a clique.

Let G be the graph whose vertices are all seg-
ments of S and whose edges satisfy the condition
that any two vertices x and x′ form an edge iff
x ∩ x′ = ∅; see Figure 4 for an example. G es-
sentially provides a compact representation of the
decision trees discussed above.

It is not difficult to see that a maximal clique
also forms a segmentation. Moreover, the set of all
maximal cliques in G is exactly the set of all pos-
sible segmentations for S. Thus, pθ(σ|S) should
satisfy

pθ(σ|S) = 0, if σ is not a clique in G, (17)

and ∑
σ

pθ(σ|S) = 1, (18)

98



  

{3}

{1}

{2}

{4}

{1,2 }{3,4 }

{2,3}{2,3,4} {1,2,3}

{1,2,3,4 }

Figure 4: The graph whose vertices are the seg-
ments of s41 and whose edges are formed by non-
overlapping vertices.

where the sum is over all maximal cliques in G.
In our example pθ

(
{ {1}, {1, 2} }|S

)
= 0, be-

cause there is no edge connecting segments {1}
and {1, 2} so they are not part of any clique.

In order to derive an explicit formula for
pθ(σ|S) we focus on a particular type of paths
in G. A path is called clique-preserving, if ev-
ery vertex in the path belongs to the same clique.
Our construction should be such that each clique-
preserving path has positive probability of occur-
ring, and all other paths should have probability
0. We proceed with calculating probabilities of
clique-preserving paths based on the structure of
G and the constraint of eq. 16.

The probability pθ(σ|S) can be viewed as
the probability of generating all clique-preserving
paths on the maximal clique σ in G. Since
σ is a clique, there are |σ|! possible paths that
span its vertices. Let σ = {x1, ..., x|σ|},
and let π denote a permutation of {1, ..., |σ|}.
We are interested in computing the probabil-
ity qθ(xπ(1), ..., xπ(|σ|)) of generating a clique-
preserving path xπ(1), ..., xπ(|σ|) in G. Thus,

pθ(σ|S) = pθ({x1, ..., x|σ|}|S)

=
∑
π

qθ(xπ(1), ..., xπ(|σ|))

=
∑
π

qθ(xπ(1)) qθ(xπ(2)|xπ(1))× ...

...× qθ(xπ(|σ|)|xπ(1), ..., xπ(|σ|−1)).
(19)

The probabilities qθ(·) can be explicitly calcu-
lated by taking into account the following ob-
servation. A clique-preserving path on a clique

σ can be realised as a sequence of vertices
xπ(1), ..., xπ(i), ..., xπ(|σ|) with the following con-
straint: If at step i − 1 of the path we are at ver-
tex xπ(i−1), then the next vertex xπ(i) should be a
neighbor of all of xπ(1), ..., xπ(i−1). In other words
we must have

xπ(i) ∈ Nπ,i ≡
i−1⋂
l=1

N(xπ(l)). (20)

Thus, the probability of choosing xπ(i) as the next
vertex of the path is given by

qθ(xπ(i)|xπ(1), ..., xπ(i−1)) =
θ(xπ(i)|lS)∑

x∈Nπ,i
θ(x|lS)

,

(21)
if xπ(i) ∈ Nπ,i and 0, otherwise. When choosing
the first vertex of the path (the root in the deci-
sion tree) we have Nπ,1 = seg(S), which gives
qθ(xπ(1)) = θ(xπ(1)|lS), as required. Therefore
eq. 19 can be written compactly as

pθ(σ|S) =

 |σ|∏
i=1

θ(xi|lS)

 ∑
π

1
Qθ(σ, π;S)

,

(22)
where

Qθ(σ, π;S) =
|σ|∏
i=1

∑
x∈Nπ,i

θ(x|lS) . (23)

The construction above can be generalized in
order to derive a PMF for any random variable
whose values are partitions of a set. Indeed, by al-
lowing the vertices of G to be a subset of a power
set, and keeping the condition of edge formation
the same, probabilities of clique-preserving paths
can be calculated in the same way. Figure 5 shows
the graph G that represents all possible instances of
K with (S, T ) = (s41, t

5
1), σ =

{
{1, 2}, {3}, {4}

}
and τ =

{
{1}, {2, 3, 4}, {5}

}
. Again each maxi-

mal clique is a possible consistent bisegmentation.
In order for this model to be complete, one

should solve the maximization step of eq. 13 and
calculate the posterior pθn(σ, τ,K|S, T ). We are
not bereft of hope, as relevant techniques have
been developed (see Section 6).

6 Related Work

To our knowledge, this is the first attempt to inves-
tigate formal motivations behind the consistency
method.

99



  

s1
2 , t2

4

s3 , t5 s4 , t5

s1
2 , t1

s3 , t1 s4 , t1

s1
2 , t5

s4 , t2
4 s3 , t2

4

s1
3 , t1

4

s1
3 , t2

5

s3
4 , t2

5

s1
4 , t1

5

s1
3 , t1 s4 , t2

5 s1
3 , t5 s4 , t1

4

s3
4 , t1

4

Figure 5: Similar to Figure 4 but for consistent
bisegmentations with (S, T ) = (s41, t

5
1) and a

given segmentation pair (see text). For clarity, we
show the phrases that are formed from joining con-
tiguous segments in each pair, rather than the seg-
ments themselves.

Several phrase-level generative models have
been proposed, almost all relying on multinomial
distributions for the phrase alignments (Marcu and
Wong, 2002; Zhang et al., 2003; Deng and Byrne
2005; DeNero et al., 2006; Birch et al., 2006).
This is a consequence of treating alignments as
functions rather than partitions.

Word alignment and phrase extraction via In-
version Transduction Grammars (Wu, 1997), is a
linguistically motivated method that relies on si-
multaneous parsing of source and target sentences
(DeNero and Klein, 2010; Cherry and Lin 2007;
Neubig et al., 2012).

The partition probabilities we introduced in
Section 5.2 share the same tree structure discussed
in (Dennis III, 1991), which has found applica-
tions in Information Retrieval (Haffari and Teh,
2009).

7 Conclusions

We have identified the relation between consis-
tency and components of graphs that represent
word-aligned sentence pairs. We showed that
phrase pairs of interest to SMT form a sigma-
algebra generated by components of such graphs,
but the existing occurrence-counting statistics are
inadequate to describe this structure. A general-
ization of our construction via sentence segmenta-
tions lead to a realisation of random partitions as
cases of constrained, biased sampling without re-

placement. As a consequence, we derived an exact
formula for the probability of a segmentation of a
sentence.

Appendix: Measure Space

The following standard definitions can be found
in, e.g., (Feller, 1971). Let X be a set. A collection
B of subsets of X is called a sigma-algebra if the
following conditions hold:

1. ∅ ∈ B.

2. If E is in B, then so is its complement X \E.

3. If {Ei} is a countable collection of sets in B,
then so is their union ∪iEi.

Condition 1 guarantees that B is non-empty and
Conditions 2 and 3 say that B is closed under com-
plementation and countable unions respectively.
The pair (X, B) is called a measurable space.

A function f : B → [0,∞) is called a measure
if the following conditions hold:

1. f(∅) = 0.

2. If {Ei} is a countable collection of pairwise
disjoint sets in B, then

f(∪iEi) =
∑

i

f(Ei).

Condition 2 is known as sigma-additivity. The
triple (X, B, f) is called a measure space.

Acknowledgments

This research was supported by the European
Union’s ICT Policy Support Programme as part
of the Competitiveness and Innovation Framework
Programme, CIP ICT-PSP under grant agreement
nr 250430 (GALATEAS) and by the EC funded
project CoSyne (FP7-ICT-4-24853).

References
Alexandra Birch, Chris Callison-Burch, Miles Os-

borne and Philipp Koehn. 2006. Constraining the
Phrase-Based, Joint Probability Statistical Transla-
tion Model. In Proc. of the Workshop on Statistical
Machine Translation, pages 154–157.

Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The Mathematics of Statistical Machine Translation.
Computational Linguistics, vol.19(2), pages 263–
312.

100



Colin Cherry and Dekang Lin. 2007. Inversion Trans-
duction Grammar for Joint Phrasal Translation Mod-
eling. In Proc. of SSST, NAACL-HLT / AMTA Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 17–24.

A.P. Dempster, N.M. Laird and D.B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical So-
ciety, Series B (Methodological) 39(1), pages 1–38.

John DeNero, Dan Gillick, James Zhang and Dan
Klein. 2006. Why Generative Phrase Models Un-
derperform Surface Heuristics. In Proc. of the Work-
shop on Statistical Machine Translation, pages 31–
38.

John DeNero and Dan Klein. 2010. Discriminative
Modeling of Extraction Sets for Machine Transla-
tion. In Proc. of the Association for Computational
Linguistics (ACL), pages 1453–1463.

Yonggang Deng and William Byrne. 2005. HMM
Word and Phrase Alignment for Statistical Machine
Translation. In Proc. of the Conference on Empir-
ical Methods in Natural Language Processing and
Human Language Technology (HLT-EMNLP), pages
169–176.

Samuel Y. Dennis III. 1991. On the Hyper-Dirichlet
Type 1 and Hyper-Liouville Distributions. Commu-
nications in Statistics - Theory and Methods, 20(12),
pages 4069–4081.

William Feller. 1971. An Introduction to Probability
Theory and its Applications, Volume II. John Wiley,
New York.

Gholamreza Haffari and Yee Whye Teh. 2009. Hi-
erarchical Dirichlet Trees for Information Retrieval.
In Proc. of the Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL), pages 173–181.

Frank Harary. 1969. Graph Theory. Addison–Wesley,
Reading, MA.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of the Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology (HLT-NAACL), pages
48–54.

Philipp Koehn. 2009. Statistical Machine Translation.
Cambridge University Press, Cambridge, UK.

Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. In Proc. of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 133–139.

Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori and Tatsuya Kawahara. 2012. Joint
Phrase Alignment and Extraction for Statistical Ma-
chine Translation. Journal of Information Process-
ing, vol. 20(2), pages 512–523.

Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical
Machine Translation. In Proc. of the Joint Con-
ference of Empirical Methods in Natural Language
Processing and Very Large Corpora (EMNLP-VLC),
pages 20–28.

Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23, pages 377–
404.

Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. of the
Association for Computational Linguistics (ACL),
pages 523–530.

Ying Zhang, Stephan Vogel and Alex Waibel. 2003.
Integrated Phrase Segmentation and Alignment Al-
gorithm for Statistical Machine Translation. In
Proc. of the International Conference on Natural
Language Processing and Knowledge Engineering
(NLP-KE).

101


