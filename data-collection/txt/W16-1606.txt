



















































Assisting Discussion Forum Users using Deep Recurrent Neural Networks


Proceedings of the 1st Workshop on Representation Learning for NLP, pages 53–61,
Berlin, Germany, August 11th, 2016. c©2016 Association for Computational Linguistics

Assisting Discussion Forum Users using Deep Recurrent Neural Networks

Jacob Hagstedt P Suorra, Olof Mogren

Chalmers University of Technology, Sweden

jacob.hagstedt@gmail.com
mogren@chalmers.se

Abstract

We present a discussion forum assistant
based on deep recurrent neural networks
(RNNs). The assistant is trained to per-
form three different tasks when faced with
a question from a user. Firstly, to rec-
ommend related posts. Secondly, to rec-
ommend other users that might be able
to help. Thirdly, it recommends other
channels in the forum where people may
discuss related topics. Our recurrent fo-
rum assistant is evaluated experimentally
by prediction accuracy for the end–to–end
trainable parts, as well as by performing
an end-user study. We conclude that the
model generalizes well, and is helpful for
the users.

1 Introduction

Discussion forums pose an interesting setting for
human interaction. Chat systems, social media,
and customer support systems are closely related,
and in this paper, we will use the term “discus-
sion forum” for all of them. These platforms play
an increasingly important role for people, both in
their professional and personal lives. For exam-
ple, many software developers are familiar with
web services such as Stack Overflow where you
ask questions and other users can respond. Simi-
lar approaches are also used in customer support
systems, allowing for quick turnaround time and
a growing database of queries that can be made
available to customers along with their responses.

In this paper, we will discuss how an automated
system can help people make better use of ex-
isting platforms, and we propose a system that
solves some of the associated problems. More
specifically, our system helps people find their
way around a discussion forum and gives intelli-

xt-2 xt-1 xt

Softmax output

LSTMLSTMLSTM

User
recommendations

Channel
recommendations

Softmax output

Figure 1: The layout of our recommendation
model. The recommendations of users and chan-
nels are modelled as two different softmax out-
put layers, attached to the end of a deep recurrent
LSTM network modelling the input.

gent suggestions on where to get the information
that they need.

The proposed system is based on deep recurrent
neural networks (RNNs) and solves three differ-
ent problems for discussion forum users. Firstly,
faced with a question from a forum user, our sys-
tem can suggest related posts from other channels
in the system, based on a similarity measure com-
puted on representations learned by a Long Short
Term Memory (LSTM) RNN (Schmidhuber and
Hochreiter, 1997). Secondly, we train a similar
network end–to–end to recommend other forum
users that might be knowledgeable about the cur-
rent question. Finally, the model is also trained to
suggest other channels where similar discussions
have been held previously.

The assistant is evaluated on data from a corpo-
rate discussion forum on the chat-platform Slack.
We show experimental results by evaluating the
generalization of our model, as well as perform-
ing and analysing a study based on collecting data
from users who interact with the discussion forum
assistant.

53



2 Background

A recurrent neural network (RNN) is an artificial
neural network that can model a sequence of arbi-
trary length. The basic layout is simply a feedfor-
ward neural network with weight sharing at each
position in the sequence, making it a recursive
function on the hidden state ht. The network has
an input layer at each position t in the sequence,
and the input xt is combined with the the previ-
ous internal state ht−1. In a language setting, it is
common to model sequences of words, in which
case each input xt is the vector representation of
a word. In the basic variant (“vanilla” RNN), the
transition function is a linear transformation of the
hidden state and the input, followed by a pointwise
nonlinearity.

ht = tanh(Wxt + Uht−1 + b),

where W and U are weight matrices, and b is a
bias term.

Basic “vanilla” RNNs have some shortcomings.
One of them is that these models are unable to
capture longer dependencies in the input. Another
one is the vanishing gradient problem that affects
many neural models when many layers get stacked
after each other, making these models difficult to
train (Hochreiter, 1998; Bengio et al., 1994).

The Long Short Term Memory
(LSTM) (Schmidhuber and Hochreiter, 1997) was
presented as a solution to these shortcomings. An
LSTM is an RNN where the layer at each timestep
is a cell that contains three gates controlling what
parts of the internal memory will be kept (the
forget gate ft), what parts of the input that will be
stored in the internal memory (the input gate it),
as well as what will be included in the output (the
output gate ot). In essence, this means that the
following expressions are evaluated at each step in
the sequence, to compute the new internal mem-
ory ct and the cell output ht. Here “�” represents
element-wise multiplication.

it = σ(W (i)xt + U (i)ht−1 + b(i)),

ft = σ(W (f)xt + U (f)ht−1 + b(f)),

ot = σ(W (o)xt + U (o)ht−1 + b(o)),

ut = tanh(W (u)xt + U (u)ht−1 + b(u)),
ct = it � ut + ft � ct−1,
ht = ot � tanh(ct). (1)

xi-1 xi xi+1

yiyi-1 yi+1

Figure 2: A recurrent neural language model. At
each input xi, the model is trained to output a pre-
diction yi of the next token in the sequence, xi+1.
In this paper, each block is a deep LSTM cell,
and the network is trained using backpropagation
through time (BPTT).

LSTM networks have been used successfully
for language modelling (predicting the distribution
of the word following after a given sequence) (see
Figure 2), sentiment analysis (Tang et al., 2015),
textual entailment (Rocktäschel et al., 2016), and
machine translation (Sutskever et al., 2014). In the
following section, we will see that the learned fea-
tures are also suitable for relating forum posts to
each other, and as a building block for the recom-
mendation system in our virtual forum assistant.

3 The Recurrent Forum Assistant

In this section, we present a virtual forum assistant
built using LSTM networks.

The assistant solves three different tasks in a
discussion forum at an IT consultant organization.
The forum is used internally and contains discus-
sions regarding both technical topics and more
everyday issues. When a user enters a question
(defined simply by containing a question mark),
the assistant produces one output corresponding
to each task, and posts this back to the channel
where the question was asked. The first task is
recommending forum posts, the goal of which is
to suggest related posts that might be of help to
the user. The second task is to recommend other
forum users that are suited to answer the question,
and the third task is to suggest other forum chan-
nels where you could look for an answer to the
question. See Figure 3 for an illustration of the
assistant in action.

All three tasks are solved using the same un-
derlying model, a deep recurrent LSTM network
initially pretrained as a language model (see Fig-
ure 2). The pretraining is first performed us-
ing a general corpus (Wikipedia), and then using

54



the posts from the discussion forum. Finally the
model is trained in a supervised fashion to perform
the recommendation tasks (see Figure 1).

The following sections will go through how the
agent solves the three different tasks.

3.1 Recommending Related Posts

The subsystem for recommending related forum
posts works by first feeding each post p through
the recurrent network to compute the final internal
representation, rp = cT (see Equation 1). The fo-
rum post representations are then compared using
cosine similarity to get a similarity score between
different forum posts:

sim(r1, r2) =
r1 · r2
‖r1‖‖r2‖ . (2)

When posed with a question q from a user, the as-
sistant finds the post p that maximizes sim(q, p).

Representing the posts using the internal repre-
sentations learned by a recurrent neural network
has a number of benefits. Firstly, we can repre-
sent a sequence of arbitrary length. Secondly, the
structure of the LSTM cells gives us a model that
takes into account the order of the words.

3.2 End–to–End Learning of
Recommendations

The second part of our virtual forum assistant is
trained in an end–to–end fashion with the aim
of recommending relevant (a) forum users, and
(b) forum channels that might be of help to the
user.

The recommendation model is built on the post
recommendation model, and hence first pretrained
as a language model. In order to recommend users
and forum channels, we attach two multiclass clas-
sification output layers to our recurrent neural net-
work (see Figure 1 on page 1). These are softmax
layers with the number of outputs corresponding
to the number of users and the number of channels
in the forum, respectively. During training, the au-
thor of each post is assigned as the target value
for the user recommendation layer. Similarly, the
channel in which the post was made, is assigned
as the target value for the channel recommenda-
tion layer. This means that we can get recommen-
dations for forum posts, forum users, and forum
channels at the same time, from the same source
forum post, using the same underlying model.

Figure 3: Screenshot of the Slack user interface
when asking a question to which the recurrent as-
sistant provides responses. Names and usernames
have been anonymized.

4 Experimental Setup

This section explains the setup of the empirical
study of our model. How it is designed, trained,
and evaluated.

4.1 Model Layout

The same recurrent neural network is used both in
the forum post recommendation step and for the
recommendations for users and channels. We use
a deep recurrent neural network with LSTM cells.
The depth of the network is 2, and we use 650 hid-
den units in the LSTM cells.

For the pretraining phase, the output layer of
the model is a softmax layer with 45985 outputs
(the number of words in the vocabulary). For the
user and channel recommendations, two softmax
layers are attached to the last output of the recur-
rent network, one for user recommendations and
one for channel recommendations (see Figure 1 on
page 1). As pretraining, only the language model
is trained. Then, both the recommendation output
layers are trained simultaneously.

55



4.2 Baselines

For the related forum post recommendations,
a baseline was implemented and evaluated
using precomputed word embeddings from
Word2Vec1 (Mikolov et al., 2013). The precom-
puted model contains 300 dimensional vectors for
3 million words that were trained on the Google
News corpus. For each post, a representation
was computed by simply summing the vectors
for each word. The forum post representations
were then compared using cosine similarity (see
Equation 2).

For forum user and channel recommendations,
the baseline reported is a naı̈ve solution, consis-
tently recommending the same top-2 items; the
items that maximizes the score, i.e. the 2 most
common targets.

4.3 Datasets

Two datasets were used during the training; the
English Wikipedia and data exported from a fo-
rum on the Slack platform.

The Wikipedia data was used to prime the
model with generic English language. For this,
the complete dump from 20150315 was used2.
The dump was cleaned using Wiki-Extractor3, and
then tokenized using the Punkt tokenizer in Python
NLTK.

In the discussion data from Slack, we collected
all public posts made by an IT consultant organi-
zation. The discussions contain questions and an-
swers about programming practices; different li-
braries and languages and what they are best suited
for. The nature of the discussions are similar to
that of the well known online system Stack Over-
flow4, where software developers ask questions
and anyone can respond. In both environments,
the responses can then receive feedback and reac-
tions.

At the time of exporting data from Slack, this
forum contained 1.7 million messages written by
799 users in 664 channels. Many of these are pri-
vate messages that were not used in this work.
Non-public messages, inactive users (having au-

1https://code.google.com/p/word2vec/
2https://dumps.wikimedia.org/
3https://github.com/bwbaugh/

wikipedia-extractor
4https://stackoverflow.com/

Figure 4: T-SNE projections of forum post repre-
sentations.
Top: posts are represented as a sum of embed-
dings from Word2Vec over the words in each post.
Bottom: the internal state of an LSTM network is
used as the representation.
The posts were taken from a discussion channel
about mobile app development. You can see that
while the word-embedding sum baseline are all
clustered together, the representations created us-
ing LSTMs result in easily separable clusters.

thored less than 10 posts) and channels with fewer
than 50 messages were removed, leaving 184.637
public messages, 660 users, and 321 channels that
were used for training. The messages were in av-
erage 17 words long (minimum 0 and maximum
1008). A random split was made, reserving 369
posts for the validation set, and a separate ex-
port of data from the following month, resulted in
14.000 posts for the (separate) test set.

56



Cosine (a) Word Embedding Baseline

0.854 Having a edge on differen javascript frameworks would be very cool. We could have multi-
ple [...]

0.848 So I have a lot of javascript that will be used across about 40 sites. [...]
0.842 Hey guys! Me myself and <user> are having a discussion regarding using Typescript with

Angular.js [...]

Cosine (b) Recurrent Forum Assistant

0.927 can someone recommend testing frameworks for Python?
0.921 Does anyone have experience in using Zend Server (for debugging) with Eclipse?
0.918 are you using any framework? such as phpspec?

Table 1: Top 3 responses from (a) the baseline method (see Section 4.2), (b) the recurrent forum assistant,
when asking the question: “Do we have any experience with using angular and javascript two way
databinding?”. The first 15 words of each post was included.

4.4 Training

Preliminary results showed that training the model
on the discussion forum data alone was not enough
to give good suggestions of related posts. Given
the limited nature of this data, we decided to
pretrain the model (as a language model) us-
ing one pass through the whole Wikipedia dump.
The model was then trained for 39 epochs as
a language model on the discussion data from
Slack, whereafter finally the two recommenda-
tion output layers (for forum user recommenda-
tions and forum channel recommendations) were
trained simultaneously for 19 epochs. Using the
Wikipedia pretraining substantially improved the
performance of the system. Training time was de-
cided using early stopping (Wang et al., 1994).

Training was done with backpropagation
through time (BPTT) and minibatch stochastic
gradient descent.

Training the user recommendation classification
was done by having the author of each forum post
as the classification target. Similarly, the train-
ing target for the forum channel classification was
the channel in which the corresponding post was
made.

4.5 Evaluation

To evaluate the performance of the proposed vir-
tual assistant system, two different approaches
were used. Firstly, a separate test set (see Sec-
tion 4.3) was constructed to evaluate the gener-
alization of the model in the user and channel
recommendations. Secondly, a user study was

performed, evaluating actual performance of the
agent in a live setting in the live system with users
interacting with it.

When evaluating the recommendations pro-
duced by the assistant on the held–out test set, sev-
eral recommendations could be reasonable choices
to any one question. Therefore, we employed a
top-2 testing approach, where the system was al-
lowed to produce two recommendations for each
query. If the correct target was one of the two rec-
ommendations, it was counted as “correct”. The
top-2 evaluation also reflects the live implementa-
tion of our system, where two recommendations
are always produced.

In the user study, the agent collected a number
of data-points for the evaluation after each recom-
mendation produced. These included an identifier
of the questioner, the agent’s response, a times-
tamp, what kind of recommendation that the agent
provided (posts, users, or channels), and a list of
reactions that was provided by the users towards
the agent’s action. Positive and negative reactions
were then counted and reported, as well as recom-
mendations from the assistant that did not receive
any user reactions. Along with each recommen-
dation, the assistant encourages users to provide
reactions to them (see Figure 3).

For the post recommendations in the user study,
each question was served either by the LSTM state
representation, or by the word embedding repre-
sentation baseline, randomly picked with equal
probability.

57



5 Results

This section presents the results of the experimen-
tal evaluation of the recurrent forum assistant.

Table 1 shows example forum post recommen-
dation outputs from the assistant using (a) the
word-embedding sum representations, and (b) the
LSTM representations when posed with the
example question:

“Do we have any experience with using an-
gular and javascript two way databinding?”.

We present the top-3 outputs from the word-
embedding baseline method and from the
recurrent forum assistant, along with the cosine
similarity to the representation for the question.

For recommending forum users and channels,
we report accuracy scores for the test set (see
Table 3). The accuracy score is the percentage
of recommendations performed on the previously
unseen test-set, compared to the naı̈ve baseline
of consistently recommending the top-2 users or
channels respectively; the fixed recommendation
that maximizes the score.

We also report results from the user study (see
Table 2). For each recommendation that the as-
sistant post in the forum, positive and negative re-
actions are counted. If more than 60 minutes go
without a reaction, we count this as one “No re-
action”. Hence, you can get more than one posi-
tive reaction and more than one negative reaction
for each recommendation, but only one “No reac-
tion”.

In total, 123 reactions were collected in the user
study.

6 Related Work

Machines that can communicate with humans in
natural language have fascinated people a long
time. Alan Turing defined and gave name to a test
that he meant aimed to measure a machine’s abil-
ity to exhibit intelligent behavior (Turing, 1950).
Taking place in a chat setting, the task is for the
machine to appear like a human to a panel of
judges. The test has been debated by some for not
measuring intelligent behavior at all. However, the
topic is at the heart of artificial intelligence, and a
machine that can communicate in natural language
is not only fascinating, but can also be very useful.

Positive Negative No reaction

Users 70.4% 6.1% 23.5%
Channels 80.9% 4.8% 14.3%
Posts LSTM 42.1% 47.4% 10.5%
Posts W2V 35.7% 57.1% 7.1%

Table 2: The results from the live user study. Per-
centage is based on the total number of reactions
to the agent’s actions (and an action from the agent
that resulted in no reaction from users is counted
as “no reaction”). For users and channels recom-
mendations most reactions are positive, suggesting
that our assistant is useful to the forum users.

User Channel

Recurrent assistant 14.39% 22.01%
Naı̈ve baseline 2.46% 5.54%

Table 3: Accuracy of the recommendations from
the agent regarding forum users and channels, re-
spectively, on the separate test set. The proposed
assistant beats the naı̈ve baseline by a large mar-
gin.

There has been a number of different ap-
proaches to neural representations of sentences
and documents. A common way of representing
sequences of words is to use some form of word
embeddings, and for each word in the sequence,
do an element-wise addition (Mitchell and Lap-
ata, 2010). This approach works well for many
applications, such as phrase similarity and multi-
document summarization (Mogren et al., 2015),
even though it disregards the order of the words.
Paragraph vectors (Le and Mikolov, 2014) trains
a model to predict the word following a sequence.
The paragraph vectors are trained, using gradient
descent, at the same time as the word vectors in the
model. Our approach for embedding forum posts
(as described in Section 3) is more similar to (Cho
et al., 2014), where the authors use a recurrent
LSTM network for machine translation, by encod-
ing an input sequence into a fixed representation
which is then decoded into a sequence in another
language. Other approaches have been using con-
volutional neural networks (Blunsom et al., 2014),
and sequential denoising autoencoders (Hill et al.,
2016).

58



Dialog systems, also known as conversational
agents, typically focus on learning to produce a
well-formed response, and put less emphasis on
the message that they convey in their responses.
Partially observed Markov descision processes
(POMDPs) have been applied to this task (Young
et al., 2013), but they typically require hand-
crafted features. (Sordoni et al., 2015) used a
recurrent encoder–decoder model to perform re-
sponse generation from questions as input, and
training the model using two posts as input and the
following response as target. (Serban et al., 2016)
presented a dialog system built as a hierarchical
recurrent LSTM encoder–decoder, where the dia-
logue is seen as a sequence of utterances, and each
utterance is modelled as a sequence of words.

QA systems attempt to give the answer to a
question given a knowledgebase as input. (Her-
mann et al., 2015) used LSTM networks with an
attention mechanism to answer questions about an
input text. (Bordes et al., 2015) used memory net-
works to answer questions with data from Free-
base.

7 Discussion

The results in the empirical evaluation of the sys-
tem proposed in this paper show some interesting
points.

The accuracy of the model on the test set (see
Table 3) shows that the model beats the naı̈ve base-
line by a large margin for forum user and chan-
nel recommendations. Since we employed a top-
2 testing approach (see Section 4.5), the baseline
system were allowed to recommend the two most
frequent targets, resulting in a score of 2.46% and
5.54%, for user and channel recommendations, re-
spectively. However, with the corresponding accu-
racy scores of 14.39% and 22.01% for the recur-
rent forum assistant, we have a solid improvement.

The user study (see Table 2) shows that fo-
rum users give positive reactions to most recom-
mendations made by the recurrent assistant when
recommending forum users and channels (70.4%
and 80.9%, respectively). Some recommendations
did not receive any reactions, and although peo-
ple were encouraged to give reactions, it is hard to
say what the reason is for the missing ones. How-
ever, even if you interpret each missing reaction
as one negative reaction, the positive reactions are
still many more.

For the related post recommendations, the num-
ber of positive user reactions are much lower
(42.1% and 35.7%, respectively). We note that
the two evaluated methods for representing forum
posts give recommendations of comparable qual-
ity. You can see in the examples in Table 1 that
using the LSTM state to represent forum posts re-
sults in a system that is able to generalize very
well, which might be desirable or not depending
on application. The system finds responses that
are less specific compared to the ones found by
using the word embedding representations. This
seems like a reasonable result from a network that
was trained as a language model. E.g: a language
model will compute a similar distribution over the
next word after observing the word “Python”, as
compared to observing the word “Java”. In a fo-
rum post recommendation system, however, the
difference between the two are crucial. Even if
the network was in the end trained to recommend
users and channels (something that we presumed
would help learn features that were well suited
also for the forum post recommendations), per-
haps some other strategy for training the network,
using more direct feedback from the learning ob-
jective, would work better for this task.

Figure 4 shows clustering of forum posts cre-
ated with T-SNE, using (top) word-embedding
representations, and (bottom) LSTM representa-
tions. The bottom plot shows how forum posts are
clearly separated into clusters based on the LSTM
representations, but this technique seems unable
to separate the posts into clusters using word-
embeddings. We believe that the reason might
be connected to the observation in previous para-
graph, as the LSTM representations are trained us-
ing a different objective.

In this paper, we stated the problem (and the
three subproblems) as the task of finding relevant
information (posts, users, and channels) whithin
the current forum. The same approach can be used
to find things from other sources. In the same
setting, recommending posts in other forums, or
pages on Wikipedia would be reasonable choices.
In a customer support setting, a database of pre-
defined statements or solution suggestions would
be more suitable. With subtle changes to the im-
plementation, the system can learn to choose from
a number of output templates, and then fill in the
related information from the context.

59



8 Conclusions

In this paper, we have proposed a virtual assistant
for discussion forum users, built using deep recur-
rent neural networks with LSTM cells. Our solu-
tion relies heavily on learning useful representa-
tions for the data in discussion forums.

We found that using the representations from a
deep recurrent neural network can be useful for the
retrieval of relevant posts. However, in this par-
ticular task we found that using a representation
based on summing word-embeddings works com-
parably well. We also found that pretraining the
RNN as a language model with a general corpus
such as Wikipedia gave substantially better sug-
gestions of related posts.

Given an input question, the proposed model
is able to give good recommendations for forum
users and forum channels. This is evaluated both
as a prediction task on an unseen test-set, and in a
user study where we measure user reactions when
interacting with our assistant.

Our joint model learns to produce recommenda-
tions for both users and channels, and generalize
well to unseen data.

Our results from the user study clearly shows
that the users find the suggestions from the assis-
tant to be positive and useful. More experiments
and A/B testing is left for future work to determine
how the assistant can create the most useful sug-
gestions.

In this work, we have taken an approach that
we have not seen in previous work. Our aim was
to create a useful virtual assistant for professional
users of a discussion forum in an IT organization,
and to help point users in the right directions for
further reading. Vast amounts of knowledge can
potentially reside inside a discussion platform, but
the tools for navigating it are often primitive at
best. We have seen that some of the tasks oth-
erwise performed by helpful forum members can
also be performed by a virtual recurrent forum as-
sistant.

8.1 Future Work

Even though we have presented ways to learn
good representations to perform recommendations
of forum users and channels, more research is
needed to find out how to best learn the represen-
tations for the post recommendation task.

We are currently working on a complete con-
versational agent that generates responses using a

sequence–to–sequence learning approach with an
attention mechanism. We believe that this, in com-
bination with using external sources of informa-
tion such as Wikipedia pages or databases contain-
ing information for customer support, can result in
a promising virtual assistant.

Another exciting direction for this research will
be to use the collected data from user reactions and
create a model using deep reinforcement learning
that can improve as it collects more data.

Acknowledgments

This work has been done within the project “Data-
driven secure business intelligence”, grant IIS11-
0089 from the Swedish Foundation for Strategic
Research (SSF).

References
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.

1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157–166.

Phil Blunsom, Edward Grefenstette, and Nal Kalch-
brenner. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics. Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics.

Antoine Bordes, Nicolas Usunier, Sumit Chopra, and
Jason Weston. 2015. Large-scale simple ques-
tion answering with memory networks. CoRR,
abs/1506.02075.

Kyunghyun Cho, Bart van Merrienboer, aglar Glehre,
Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. In Alessandro
Moschitti, Bo Pang, and Walter Daelemans, editors,
EMNLP, pages 1724–1734. ACL.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In C. Cortes, N. D.
Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in Neural Information Processing
Systems 28, pages 1693–1701. Curran Associates,
Inc.

F. Hill, K. Cho, and A. Korhonen. 2016. Learning
Distributed Representations of Sentences from Un-
labelled Data. ArXiv e-prints, February.

Sepp Hochreiter. 1998. The vanishing gradient
problem during learning recurrent neural nets and

60



problem solutions. International Journal of Un-
certainty, Fuzziness and Knowledge-Based Systems,
6(02):107–116.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Tony
Jebara and Eric P. Xing, editors, Proceedings of the
31st International Conference on Machine Learning
(ICML-14), pages 1188–1196. JMLR Workshop and
Conference Proceedings.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In ICLR.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive sci-
ence, 34(8):1388–1429.

Olof Mogren, Mikael Kågebäck, and Devdatt Dub-
hashi. 2015. Extractive summarization by aggre-
gating multiple similarities. In Recent Advances in
Natural Language Processing, page 451.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomáš Kočiskỳ, and Phil Blunsom. 2016.
Reasoning about entailment with neural attention.
In International Conference on Learning Represen-
tations.

Jürgen Schmidhuber and Sepp Hochreiter. 1997.
Long short-term memory. Neural computation,
7(8):1735–1780.

Iulian Vlad Serban, Alessandro Sordoni, Yoshua Ben-
gio, Aaron C. Courville, and Joelle Pineau. 2016.
Building end-to-end dialogue systems using gener-
ative hierarchical neural network models. In Dale
Schuurmans and Michael P. Wellman, editors, AAAI,
pages 3776–3784. AAAI Press.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Rada Mi-
halcea, Joyce Yue Chai, and Anoop Sarkar, editors,
HLT-NAACL, pages 196–205. The Association for
Computational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Docu-
ment modeling with gated recurrent neural network
for sentiment classification. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1422–1432.

Alan M Turing. 1950. Computing machinery and in-
telligence. Mind, 59(236):433–460.

C. Wang, S. S. Venkatesh, and J. S. Judd. 1994. Op-
timal stopping and effective machine complexity in
learning. In Advances in Neural Information Pro-
cessing Systems 6. Morgan Kaufmann.

Stephanie Young, Milica Gasic, Blaise Thomson, and
John D Williams. 2013. Pomdp-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE, 101(5):1160–1179.

61


