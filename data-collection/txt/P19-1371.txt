



















































Learning to Abstract for Memory-augmented Conversational Response Generation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3816–3825
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3816

Learning to Abstract
for Memory-augmented Conversational Response Generation

Zhiliang Tian,1,3∗ Wei Bi,2 Xiaopeng Li,1,3 Nevin L. Zhang1,3
1Department of Computer Science and Engineering,

The Hong Kong University of Science and Technology, Hong Kong
2Tencent AI Lab, Shenzhen, China

3HKUST-Xiaoi Joint Lab, Hong Kong
ztianac@cse.ust.hk victoriabi@tencent.com {xlibo,lzhang}@cse.ust.hk

Abstract
Neural generative models for open-domain
chit-chat conversations have become an ac-
tive area of research in recent years. A criti-
cal issue with most existing generative models
is that the generated responses lack informa-
tiveness and diversity. A few researchers at-
tempt to leverage the results of retrieval mod-
els to strengthen the generative models, but
these models are limited by the quality of
the retrieval results. In this work, we pro-
pose a memory-augmented generative model,
which learns to abstract from the training cor-
pus and saves the useful information to the
memory to assist the response generation. Our
model clusters query-response samples, ex-
tracts characteristics of each cluster, and learns
to utilize these characteristics for response
generation. Experimental results show that our
model outperforms other competitive base-
lines.

1 Introduction

Automatic human-computer dialogue / conversa-
tion is a core topic in natural language process-
ing. There is a boom in research on open-domain
chit-chat dialogue systems due to the availability
of vast conversational data online. Most existing
models of dialogue systems can be divided into
retrieval-based models and generative models.

Given a query, retrieval-based (Ji et al., 2014)
models search for the most similar query stored
in the training corpus and directly copy its cor-
responding response as the result. These mod-
els cannot create new replies customized for the
given queries. Generative models (Shang et al.,
2015) learn a query-response mapping to generate
responses by maximizing P (r|q), where q is the
input query and r is the response. The most popu-
lar generative model is the Sequence-to-Sequence

∗Work done while Zhiliang Tian was collaborating with
Tencent AI Lab.

Query Response

Memory

Where did you go for holiday?
Fine weather today in Chicago! 
I like the weather today.
There are many places for tour.
Which city to travel next time?

I traveled to Tibet.
Sure, sunny in Chicago.
It is too hot for me.
Yes, especially museums.
Maybe New York.

(weather, today) (sunny, hot)

Learn	to	Abstract

Any places for traveling
this weekend?

Travel to New York’s
museum for this weekend.

Training	corpus

Input Output

(place, travel) (Tibet, New York, museum)

key1 value1

key2 value2𝑘?@ 𝑘A@ 𝑘@@ 𝑘B@ 𝑣?@ 𝑣A@ 𝑣B
@𝑣@@

𝑘?A 𝑘AA 𝑘@A 𝑘B
A 𝑣?A 𝑣AA 𝑣B

A𝑣@A. . . . . .

. . . . . .

Figure 1: An example of abstracting training corpus
and memorizing their characteristics in the form of key
vectors and value vectors. Red and blue indicate two
clusters. The input query matches the blue one and gen-
erates the response assisted by information collected
from the last two training samples.

(Seq2Seq) model (Sutskever et al., 2014), which
generates new utterances tailored for queries and
achieves high coherence between queries and gen-
erated utterances. However, existing generative
models often generate uninformative and univer-
sal responses (Li et al., 2016a).

To address these issues, several researchers
leverage retrieved results R to augment the infor-
mation used in generative models. Such methods
are called retrieval-augmented generative models
and their objectives are to maximize P (r|q,R),
where R is one or a few (at most 3 in practice) re-
trieved results. Particularly, some researchers (Li
et al., 2017; Zhuang et al., 2017; Song et al., 2018)
build the combination of retrieval and generative
models, which retrieve one or a few responses r+,
and then feeds both the query q and r+ into the
generative model to maximize P (r|q,R = r+).
It enriches generated responses by informatively
retrieved responses but can only utilize a limited



3817

number of retrieved results due to their model ar-
chitecture. Wu et al. (2018) edit the retrieved re-
sponse r+ with the Seq2Seq model based on the
lexical differences between the input query q and
its retrieved query q+, whose objective is to max-
imize P (r|q,R = 〈r+, q+〉). It edits retrieved
responses r+ to make them relevant to queries,
but their edited results rely heavily on the sen-
tence pattern of r+. Generally, the responses from
such models are more informative and diverse than
those from plain generative models, while main-
tain better relevance than the retrieved responses.

Although current retrieval-augmented genera-
tive models have achieved promising results, they
still have following weaknesses: Firstly, they are
limited by the quality of the retrieved results.
Retrieval results are less coherent and relevant
with query than generative models’ (Song et al.,
2018). Irrelevant retrieved results would mislead
the response generation. Secondly, these models
can only utilize individual retrieved results, which
makes the generation sensitive to those results,
leading to a high variance in the performance.
Moreover, the information from very few retrieved
results may not be sufficient to enrich the response
generation.

In this paper, we propose a memory-augmented
generative model that memorizes and utilizes the
common characteristics M of groups of query-
response (q-r) pairs to enhance the response gen-
eration by maximizing P (r|q,M). The advantage
is that our model is less sensitive to the quality
of individual q-r pairs and hence increases the ro-
bustness of response generation.

In particular, we divide the training corpus into
multiple groups by clustering, extract common
characteristics of each group, and learn to utilize
the characteristics to assist generation. The idea
is illustrated in Figure 1 (top), the training corpus
is divided into two sets of closely related queries
and their responses. We abstract query-response
relationship hidden in those q-r pairs, save them
to the memory (Figure 1 bottom), and use those
relationships for response generation.

Our contributions can be summaried as:

1. We are the first to extract information from
clusters of query-response pairs using a learnable
memory, and to use the information to enhance the
performance of conversation systems.

2. We propose a novel framework where the
Seq2Seq, autoencoder and clustering model are

jointly trained to abstract the training corpus and
generate responses.

3. Our model outperforms state-of-the-art gen-
erative models and retrieval-augmented generative
models in single-round conversation scenarios.

2 Related Work

Generative models build dialogue systems via
end-to-end training. Ritter et al. (2011) first regard
response generation as query-to-response transla-
tion. Following that, Shang et al. (2015) imple-
ment an end-to-end dialogue system borrowing the
Seq2Seq model, while Li et al. (2016b) replace the
maximum likelihood criterion with maximum mu-
tual information (MMI) to deal with the universal
response issue of the seq2seq.

The retrieval-based models are another branch
in building dialogue systems. Ji et al. (2014) pro-
pose to apply information retrieval techniques to
search for related queries and replies. Zhou et al.
(2016) and Yan et al. (2016) improve it by neural
networks.

Recently, several researchers (Song et al., 2018;
Li et al., 2017; Zhuang et al., 2017) propose to
merge retrieval-based models and generative mod-
els. Cai et al. (2018) generate the response skele-
ton from the retrieved results and revise the skele-
tons to get the response. Guu et al. (2018) use
the Seq2Seq model to edit a prototype from the
retrieved results for text generation and Wu et al.
(2018) leverage context lexical differences to edit
prototypes for conversation.

There are some other directions to enhance gen-
erative models by adding additional information.
Some of them introduce the knowledge base to
conversation models, which provide task-specific
knowledge (Madotto et al., 2018; Wu et al., 2019)
or lexical knowledge to text generation (Young et
al., 2018; Parthasarathi and Pineau, 2018). Some
other directions are to maintain sample-level tem-
porary memory. Weston et al. (2014), Shang et al.
(2015), Serban et al. (2016), Tian et al. (2017) and
Le et al. (2018) memorize the previous utterances
in a multi-round session. Unlike them, our model
brings in the corpus-level memory and does not
rely on any external resource.



3818

3 Models

3.1 Model Architecture

Our model consists of two components: a mem-
ory module and a generative model. The mem-
ory module divides the training corpus into multi-
ple groups of query-response pairs, and it extracts
and memorizes the essential query-response cor-
respondence information hidden in each group of
pairs. The generative model generates responses
for input queries and, while doing so, takes in-
formation stored in the memory module into con-
sideration. It also learns the representations of
queries and responses that are used in the memory
module.

3.2 Query-Response Memory Module

Our memory module consists of K memory slots,
and each memory slot is a pair containing a key
cell and its corresponding value cell. Given a
query, we search for the most similar key and out-
put its corresponding value. Both the keys and the
values are real-value vectors. They are called key
embeddings and value embeddings respectively,
and denoted as ki and vi.

We group queries in the training corpus into K
clusters. Each query is embedded as a vector. So,
it makes sense to talk about the center ki of each
query cluster i. The center ki is used as a key in
our memory module. The corresponding value vi
is a vector that captures the common characteris-
tics of the responses to queries in cluster i. We will
say more about ki and vi later.

Read Operation. In our model, the input of Read
Operation is the current query’s representation eq
from the generative model. Given the eq, the Read
Operation addresses the memory by the similar-
ity between the current query eq and every memo-
rized key embedding ki, in which we apply a dot-
product operation to measure the similarity.

We design two modes to fetch the value: Soft
Read is a weighted summation over all K value
embeddings in the whole memory according to the
normalized similarity scores (Eq. 1). Hard Read
is to fetch the value embedding whose key embed-
ding is most similar to the current query eq (Eq. 2).
Finally, it returns the value as the output of the

read operation.

SoftRead(eq) =
K∑
i=1

αivi,

αi = softmax(ki • eq).

(1)

HardRead(eq) = {vi|i =
K

argmax
i=1

(ki •eq)}. (2)

Write Operation. We collect the query represen-
tation eq’s and the response representation er’s of
all the training samples from the generative model,
and then conduct K class K-Means clustering us-
ing eq’s. We let the center of the i-th cluster Ci be
key embedding ki, and let the average of the rep-
resentations of the responses to queries in Ci be
value embedding vi (Eq. 3).

vi =

∑
j∈Ci erj

|Ci|
. (3)

In this way, each key embedding ki gathers sim-
ilar queries together and obtains their represen-
tative information by fetching their cluster cen-
ter. Each value embedding vi retains the com-
mon characteristics of a group of responses er
whose queries are similar. Hence, the pair 〈ki,vi〉
can be regarded as an abstraction of the query-
response correspondence relationship hidden in
the i-th cluster of queries and their responses. We
can control the granularity of the abstraction by
varying the number of clusters K.

In an extreme setting, if we set the memory size
K equal to the training corpus size and use the
hard read operation, our model nearly degener-
ates into a retrieval-augmented generative model.
In this case, the generation relies on only one re-
trieved sample, the generated response becomes
sensitive to the quality of that single sample and
is restrained by the pattern of the single sample.

3.3 Memory-Augmented Response
Generative Model

Our overall model consists of two branches (Fig-
ure 2). The top branch is a memory-augmented
Seq2Seq (M-Seq2Seq) model that is used for re-
sponse generation. The lower branch is a condi-
tional autoencoder (CAE) that is used to learn re-
sponse representations necessary for the memory
writing.

The input to the M-Seq2Seq branch is a query
q. It is first passed through an encoder to get a



3819

𝑞

𝑟

𝑒$%Any places for traveling
this weekend?

National park is a good place.

𝑟𝑒𝑎𝑑

𝑒( 𝑒$%

𝑤𝑟𝑖𝑡𝑒
(by	clustering)

𝑟𝑒𝑎𝑑
Where
travel

Tibet
New York   
museum �̂�

𝑟′=

𝑟

Travel to New York's museum
for this weekend.

National park is a good place
for traveling this week.

National park is a good place.

𝑃𝑟𝑒𝑑	𝐿𝑜𝑠𝑠

𝑅𝑒𝑐	𝐿𝑜𝑠𝑠
𝑒$

𝑒(

𝑒(

𝑒(

𝑒( 𝑒$
𝑟𝑒𝑝𝑟𝑒𝑠𝑒𝑛𝑡𝑎𝑡𝑖𝑜𝑛𝑠

𝑘𝑒𝑦𝑠H 𝑣𝑎𝑙𝑢𝑒𝑠H 𝑘𝑒𝑦𝑠HLM 𝑣𝑎𝑙𝑢𝑒𝑠HLM
𝑒𝑛𝑐𝑜𝑑𝑒𝑟

𝑒𝑛𝑐𝑜𝑑𝑒𝑟 𝑑𝑒𝑐𝑜𝑑𝑒𝑟

𝑑𝑒𝑐𝑜𝑑𝑒𝑟𝑀𝐿𝑃

𝑀𝐿𝑃

𝑚𝑒𝑚𝑜𝑟𝑦HLM𝑚𝑒𝑚𝑜𝑟𝑦H

𝑘𝑒𝑦𝑠
𝑣𝑎𝑙𝑢𝑒𝑠

Figure 2: The architecture of our model. Solid arrows show both the training and generation (testing) processes;
dashed arrows show the training process. The left part shows how to read memory at t-th step and write to update
the memory from t-th to (t+1)-th step, where t indicates the step of updating the memory. The callout illustrates
that a query matches the blue memory slot and reads its value “Tibet”, “New York”, and “museum” to promote its
response generation. “Pred Loss” and “Rec Loss” mean the prediction and reconstruction loss respectively.

representation eq = Encoder(q). The memory is
then read using eq as the key, and the output of
the memory read is erm. After that, eq and erm
are merged by an MLP (multi-layer perceptron),
and then the merged results are fed into the de-
coder to decode the final response r̂′, which is the
generated response for the query q. The objective
function for this branch is the first term of Eq. 8

During training, we feed 〈q, r〉 pairs to our
model. The query q is fed to the M-Seq2Seq
branch, while the corresponding response r is fed
to the CAE branch. Similar to the previous case,
r is first pushed through to get a representation
er = Encoder(r). Then both eq and er are fed
to an MLP and then a decoder. The output is r̂,
a reconstructed version of r. The reconstruction
loss is the second term of Eq. 8. We formalize
the operations of M-Seq2Seq and CAE by Eq. 4 to
Eq. 7.

Note that eq is feed to the CAE for two reasons.
First, it makes the embedding er of r dependent
on the embedding eq of q. Second, it makes the
two branches work in a similar fashion so that the
representaions learnt by CAE is adaptive to M-
Seq2Seq. The CAE branch tries to reconstruct r̂
from er and eq, while the M-Seq2Seq tries to gen-
erate an appropriate response r̂′ from eq and erm.
erm can be viewed as a rough estimation of er
and hence is helpful in improving the quality of
the generated response.

eq = Encode(q), er = Encode(r), (4)

erm = Read(eq), (5)

z =MLP ([er, eq]), z
′ =MLP ([erm, eq]),

(6)

r̂ = Decode(z), r̂′ = Decode(z′). (7)

The overall objective function contains two
parts as shown in Eq. 8 : the prediction loss (first
term) is derived from the general objective of the
retrieval- or memory-augmented generative mod-
els maxP (r|q,M), where we set M = erm for
our model. The reconstruction loss (second term)
is for learning the representations by reconstruct-
ing r, whose target is to improve the memory mod-
ule so as to improve the erm for enhancing the
generative model. In addition, λ is a factor to bal-
ance the losses.

L = Eq,r∼D logP (r̂|q, erm)
+ λ · Eq,r∼D logP (r̂|q, r).

(8)

3.4 Joint Training and Generation

To enable the memory module and the generative
model to work together, we combine and jointly
train them. We separate the memory writing and
the generative model training into two phases,
and then train the two phases alternatively. The
two training phases switch once per epoch, which
means we conduct the memory writing once the
generative model finishes training current epoch.



3820

In the generative model training phase, we train
and update the model while keeping the memory
module read-only. The generative model reads
from the memory, trains to update itself, and col-
lects representations eq’s and er’s in preparation
for memory writing. In the memory writing phase,
parameters of the generative model are fixed. We
conduct the clustering over all representations eq’s
and er’s collected from generative model training,
and then write the results into the memory.

For response generation (testing) phase, we
only rely on the M-Seq2Seq branch and the mem-
ory module in the read-only mode, since we can-
not observe the r during generation. As indicated
by the solid lines in Figure 2, it encodes q to ac-
quire eq, reads out estimated response erm by the
Read Operation, and goes through the MLP and
decoder to generate r̂.

4 Experimental Settings

4.1 Dataset
In our experiments, we validate the performance
of our model on the context-independent (single-
round) conversation task setting in which each
sample is a query-response (q-r) pair. We utilize
the benchmark dataset (Shang et al., 2015), which
collects about 4 millions q-r pairs from a popular
Chinese social network website, Weibo.1 For both
testing set and validation set, we randomly select
900 queries, and then select randomly 5 responses
under each query, thus both our testing set and val-
idation set consist of 4.5k samples. Sentences are
tokenized into word sequences with the Jieba word
segmentation tool.2 The vocabulary consists of the
top 50k tokens (a mixture of Chinese words and
characters), covering 99.98% words in this corpus,
and all the out-of-vocabulary words are replaced
with 〈UNK〉.

4.2 Implementation Details
We implement the query and response encoder
with a one-layer bi-directional GRU, and the de-
coder with a one-layer GRU and attention mecha-
nism (Bahdanau et al., 2015). We apply the idea
of variantial autoencoder (Kingma and Welling,
2014; Zhao et al., 2017) into our model: before
the MLP, we use the neural network to estimate
the distribution of response vector, sample the vec-
tor by reparameterization, then feed it into MLP.

1www.weibo.com
2github.com/fxsjy/jieba

Parameters of the query encoder and response en-
coder are not shared; the two MLP components in
M-Seq2Seq branch and CAE branch also do not
share parameters. The dimension of all hidden
vectors and embeddings are 620 and the batch size
is 64. We employ the Adam optimizer (Kingma
and Ba, 2014) with the initial learning rate 0.0001
and gradient clipping 5. For generation, we apply
a beam search with the size of 10. The memory
size K is 1000, and the loss factor λ is 0.1. We
implement our model on PyTorch. The implemen-
tation details can be found in our codes 3.

4.3 Baselines

We compare two versions of our proposed
memory-augmented generative model (MemGM),
i.e. MemGM with SoftRead (MemGM-S) and
MemGM with HardRead (MemGM-H), with the
following methods:

1. Seq2Seq. The standard Seq2Seq with the at-
tention mechanism (Bahdanau et al., 2015) in the
decoder and the beam search during generation.

2. MMI (Li et al., 2016b). We implement
the MMI-bidi model that re-ranks the candidate
responses by the maximum mutual information
(MMI) criterion in the beam search to promote re-
sponse diversity.

3. CVAE. The conditional variational autoen-
coders applied in conversation systems (Zhao et
al., 2017). We follow the their implementation and
adapt it in our single-round conversation setting.

4. EditRetrieve (Wu et al., 2018). The state-
of-the-art retrieval-augmented generative model,
which uses the information of the top-1 retrieved
response to guide the response generation.

4.4 Evaluation Metric

Following previous work on response genera-
tion (Li et al., 2016b; Yao et al., 2017), we evalu-
ate all competing methods by both automatic met-
rics and human evaluations. The automatic met-
rics are::

1. Bleu 1-4. Bleu N (Papineni et al., 2002) mea-
sures the N-gram matching between generated re-
sponses with the ground-truth responses.

3github.com/tianzhiliang/MemoryAugDialog



3821

Automatic Metrics Human Annotate
Bleu1,Bleu2,Bleu3,Bleu4 Sim-A, Sim-M Dist1,Dist2 Entropy Quality Info

Seq2Seq 39.98 14.68 6.452 3.227 0.291 0.911 0.043 0.153 7.609 2.33 1.71
MMI 40.08 14.71 6.467 3.236 0.288 0.910 0.053 0.183 7.724 2.39 1.63
CVAE 39.85 14.80 6.318 3.012 0.294 0.919 0.044 0.156 7.569 2.42 1.71

EditRetrieve 37.67 10.73 3.437 1.111 0.294 0.932 0.057 0.187 7.586 2.41 1.72
MemGM-S 41.29 15.94 8.084 4.911 0.303 0.936 0.059 0.214 7.576 2.49 1.70
MemGM-H 41.40 16.06 8.289 4.872 0.300 0.935 0.062 0.218 7.684 2.56 1.75

Table 1: The overall performance for all competing methods on quality, relevance, diversity and informativeness.

2. Sim-A, Sim-M. They measure the relevance
between the query and its response by their word
embedding cosine similarity. Sim-A is the similar-
ity between two sentence-level embeddings com-
posed by averaging all word embeddings, while
Sim-M is maximal word-word similarity among all
the words of two sentences as (Liu et al., 2016).

3. Dist 1-2. Distinct-1 and Distinct-2 (Li et al.,
2016b) are the metrics to evaluate the diversity of
generated responses, which count the percentage
of unique unigrams and bigrams among all test re-
sponses.

4. Entropy. It measures the informativeness of
generated responses proposed by (Mou et al.,
2016), which is computed by averaging over all
the character-level entropy within responses.

For human evaluations, we hire five annotators
from a commercial annotation company to an-
notate 250 randomly selected test samples. Re-
sponses generated by different models are shuffled
for each annotator. The annotators evaluate these
samples on two aspects: the overall quality (Qual-
ity) and the informativeness (Info). We conduct a
5-scale rating on Quality: 1 point for a response ir-
relevant to the query, 3 points for a valid but mean-
ingless response, 5 points for a coherent and ap-
propriate response without typos. Points of 2 and
4 are for decision dilemmas. We also conduct a
3-scale rating on Info: 1 point for the universal
response or the response containing no more than
three unique words, 2 points for a normal response
of a single clause or a single topic, and 3 points
for an informative response including at least two
clauses of different topics, which transfer the cur-
rent conversation to another scenario (For exam-
ple, the query is “How’s the weather?”; and re-
sponse “It’s fine today, let’s play basketball” trans-
fers the weather topic to sports, which should be
marked as 3 points).

5 Experimental Results and Analysis

5.1 Overall Performance

We report both the automatic metrics and hu-
man evaluation results of MemGM compared with
other methods in Table 1. MMI scores higher
than Seq2Seq on Dist-1&2 owing to its re-ranking
mechanism to promote the response diversity.
CVAE has a similar performance to the Seq2Seq
model. EditRetrieve outperforms Seq2Seq, MMI
and CVAE on most metrics. But EditRetrieve un-
derperforms on Bleu scores since retrieval mod-
els do not learn a query-to-response mapping and
their ability of matching with the ground-truth is
naturally lower.

MemGM gets the highest scores under most
metrics, indicating that our model outperforms
current methods on quality, relevance, diver-
sity and informativeness. The improvement of
MemGM-H’s Bleu-3&4 (+28.2% and +48.6% in
comparison with Seq2Seq) indicates the memory
module can extract and memorize trigram and 4-
gram response patterns to enhance the generated
responses.

For the two versions of our models, HardRead
outperforms SoftRead on most metrics. This phe-
nomenon indicates that fetching a single top mem-
orized piece of information would be more help-
ful than fetching a mixture of multiple memory
slots with multiple topics for generative models.
Thus, in MemGM, HardRead is the proper mode
for reading memory.

5.2 Impact of Memory Size

To investigate how the memory capacity influ-
ences the performance of MemGM, we carry out
experiments on MemGM-H with a various mem-
ory size K and show the results in Table 2 (omit-
ting Bleu-3&4 and Sim-M due to limited space).

In Table 2, the extreme setting K = |D| works
similarly to retrieval-augmented generative mod-
els since it saves all q-r pairs separately and utilize



3822

them individually. The difference between them
is that K = |D| reads the memory based on the
simple similarity between two vectors eq and ki
instead of searching the corpus via mature infor-
mation retrieval technique, which is usually an en-
semble of several text matching methods includ-
ing similarity of query embeddings. That makes
the query matching of K = |D| less accurate and
unstable, thus its relevance and quality are weaker
than EditRetrieval (Table 1) but does better on di-
versity. We treat K = |D| and other results in
Table 2 as the comparison between individual and
grouped q-r pairs on memory-augmented frame-
work.

Bleu1,Bleu2 Sim-A Dist1,Dist2 Entropy
K=10 42.06 15.11 0.301 0.042 0.140 7.478
K=100 42.75 15.42 0.300 0.043 0.141 7.526
K=1k 41.40 16.06 0.300 0.062 0.218 7.684
K=10k 41.22 15.37 0.296 0.057 0.200 7.659
K=|D| 34.89 9.764 0.265 0.094 0.412 9.241

Table 2: The performance of MemGM-H with different
memory size K, where K = |D| means the extreme
setting that each sample occupies a memory slot.

MemGM with a large memory size (K ≥10k)
performs poorly on the response quality (Bleu-
1&2) and the relevance (Sim-A) compared with
the MemGMs with small memory. Too large of
the memory size leads to too small of the sample
size under each memory slot, which increases the
instability and lower the quality of each memory
slot. Especially, the performance of K = |D| il-
lustrates the individual retrieval results are not re-
liable and usually lead to irrelevant results as the
observation we will discuss in Sec 5.4.

However, large memory MemGMs (K ≥1k)
performs well on diversity (Dist-1&2) and infor-
mativeness (Entropy), since the training corpus is
partitioned into more memory slots and each slots
contains more specific topics. And small memory
MemGMs (K ≤10) result in low response diver-
sity and informativeness. In conclusion, K=1k is
the appropriate memory size to balance all aspects.

5.3 Contents of Memory

Our model is expected to cluster similar queries
together to leverage the information of their re-
sponses. In addition, each memory slot should
own a group of closely related queries. To ver-
ify the quality of the memory slots, we pick up the
queries from the same memory slot and check the
similarity between these queries.

General TopicRelated
Entity

Overlap Total

|m| ≥ 1,000 350∼1,000 ≤350 –
Cluster # 14 205 781 1,000
Query # 18,291 114,454 81,003 213,748
Query % 8.4% 54.4% 37.2% 100%

Table 3: The statistics on the size of memory slots
(|m|), cluster number (Cluster #), query number
(Query #), and query proportion over all queries (Query
%) for the three memory slot types.

5 Queries under this Memory Slot

Case1:
Topic

Related
Memory

Slot

昨天在吉他店里，我们合作了一曲《猜谜到老》
(We play the ”guess forever” in guitar store yesterday)
快来听我唱的”至少还有你”。
(Listen to the “at least I have you” sung by me!)
天空之城吉他独奏，最好听的一个版本
(”Castle in the Sky”guitar solo, the best version to hear)
艾薇儿出道十年12首风靡全球的单曲超赞
(12 world-renowned songs since Avril debuted)
夕阳醉了，太好听了。
(”the Setting Sun is Drunk”. pleasant to hear.)

Case2:
Entity

Overlap
Memory

Slot

十年前的米米米兰兰兰，AC米兰
(Milan 10 years ago, AC Milan)
摩纳哥600万打包报价沙拉维+博阿滕—-米米米兰兰兰体育
(Monaco offers 600 millions$
for Shaarawy and Boateng—-Milan Sprots.)
全场比赛结束，乌迪内斯2 - 1米米米兰兰兰
(The whole match was over, Udinese 2:1 Milan)
又是一件卡卡米米米兰兰兰时期的队服
(Another Kaka’s team uniform when he was in Milan)
PPTV这俩解说一直在黑米米米兰兰兰啊
(The two PPTV’s commentators depreciated Milan)

Table 4: Five randomly selected queries under each ex-
ample of memory slots.

We find that the status of memory slots are dif-
ferent under the different size of memory slot |m|,
where |m| means how many queries are memo-
rized in this memory slot. We divide the memory
slots into three types by their size |m| and show
their statistics information in Table 3.

Topic-related Memory Slot (with size 350 <
|m| < 1000 roughly) has a clear topic and the top-
ics of its queries are highly related. For example,
the queries under the memory slot shown in Case1
(Table 4) are related to the music topic. There are
205 such slots covering 54.4% queries, which can
supply helpful information within the same topic
for response generation.

Entity-overlap Memory Slot (|m| ≤ 350
roughly) has more specific topics and its queries
usually share common entities. As shown in Case2
in Table 4, the cluster of queries talk about vari-
ous football news related to “Milan”. 781 out of
1000 slots are of this type and they cover 37.2%
queries. In response generation, when the query
has the same or similar entities with the memory



3823

slot, it can read the information from that mem-
ory slot, which summarizes a group of utterances
closely related to that entity.

General Memory Slot (|m| ≥ 1000) means the
memory slot owning too many queries to have a
clear topic, whose clustered queries have various
topics and are not similar to each other. Fortu-
nately, there are only 14 such slots influencing
8.4% queries.

In summary, most of the memory slots are of
good quality and store useful information as we
expect, which cover 91.6% of the queries.

5.4 Case Study

In this section, we first compare the cases
from MemGM and EditRetrieve to analyze how
MemGM exceeds the retrieval-augmented model.
Then, we show two examples over all the methods
to reveal the characteristics of different methods.

For the comparison between MemGM and Ed-
itRetrieve, we analyze the good/bad cases where
MemGM-H outperforms/underperforms EditRe-
trieve and investigate the reasons.

4

10

8

0

2

4

6

8

10

12

14

16

Good	Cases

Relevance(Misled	by	Word	Overlap)
Relevance(Topic	Matching)
Informativeness(Meaningful	Words)

6

3

6

0

2

4

6

8

10

12

14

16

Bad	Cases

Relevance(Topic	Matching)
Quality(Advanced	Words)
Informativeness(Long	Sentence)

Good	Cases
(MemGM outperforms	EditRetrieve)

Bad	Cases
(MemGM underperforms	EditRetrieve)

Figure 3: The reasons that MemGM outper-
forms/underperforms EditRetrieve on human annotated
cases.

We collect the good/bad cases from human an-
notation results by this criterion: If more than
four annotators marked the MemGM-H’s Quality
score higher/lower than EditRetrieve’s by 2 points,
this sample is the good/bad case. From all anno-
tated samples, we obtain 22 good cases and 15 bad
cases, and summarize the reasons in Figure 3.

There are 3 reasons for the good cases where
MemGM outperforms EditRetrieve, shown in the
left side of Figure 3. Firstly, we observe a phe-
nomenon from EditRetrieve’s results that the Ed-
itRetrieve’s response has the word overlap with its
query but is not related to the query at seman-
tic level. The reason is that retrieval systems are
highly reliant on the word matching, so they may
retrieve fake results with high lexical similarity

but indeed low relevance. Therefore, that phe-
nomenon is due to “misled by word overlap” and it
leads to EditRetrieve’s irrelevant results on 4 cases
where MemGM performs well. It indicates the re-
trieval quality limits the performance of retrieval-
augmented models. Secondly, EditRetrieve’s re-
sults mismatch the topics of given queries in 10
cases, where MemGM can generate relevant re-
sponses. Thirdly, MemGM outperforms EditRe-
trieve in 8 cases due to containing more meaning-
ful words in MemGM’s responses, where mean-
ingful words means the notional words carrying
the specific topic information.

To summarize the good cases, the major ad-
vantage of MemGM is the high relevance with
queries compared with EditRetrieve. The per-
formance on Sim-A and Sim-M (Table 1) verifies
that MemGM exceeds EditRetrieve on relevance.
Note that the retrieval-augmented models are sen-
sitive to the quality of retrieved results; if retrieved
query-response pairs are irrelevant to the query,
utilizing such information would lead to topic drift
and the generation of irrelevant responses.

There are 3 reasons for MemGM underperform-
ing EditRetrieve (Figure 3 right). Firstly, MemGM
also suffers from irrelevant responses due to mis-
matching the topic of queries. However, in terms
of the relevance, the number of the MemGM’s bad
cases (6 cases) is much fewer than its good cases’
(14 cases). Secondly, 3 cases of EditRetrieve are
of higher quality owing to the suitable use of ad-
vanced and low-frequency words. Thirdly, 6 re-
sponses from EditRetrieve are longer and carry
more information.

Two examples in Table 5 show the model per-
formance. In the first case, EditRetrieve is misled
by “ribs”; it treats “ribs of human” as “pork ribs”
and responds an utterance related to “food” topic.
MemGMs understand the topic and generate in-
formative responses. In the second case, Seq2Seq,
CVAE and MMI only respond by simply repeat-
ing the query. EditRetrieve can bring in a new
word ”rest” related to the query. MemGMs cap-
ture the “cold” from the given query and respond
utterances related to both “cold” and “health”.

6 Conclusion

In this paper, we propose a novel memory-
augmented generative model for response genera-
tion. It leverages groups of query-response pairs to
augment generative models instead of the individ-



3824

Query
多亏我是个瘦子，伤心时我可以数数排骨
(Thanks to being a thin man,
I can count my ribs during my sad time.)

Seq2Seq 我也是这么想的。(I do think so.)
MMI 我也曾伤心过(I have experienced the sad time)
CVAE 哈哈，我也是这样的。(Ha-ha, me to.)

EditRetrieve 我也想吃。。。(I want to eat it too...)

MemGM-S
伤心的时候会想起谁
(Who do you remember during sad time.)

MemGM-H
我可以说我是个胖子吗?
(Can I say I am a fat man?)

Query
今天出来好冷，冻脸啊！大家要注意身体
(It’s cold today when leaving,
freezing my face.Attention to health!)

Seq2Seq
注意身体哦，注意身体
(Attention to health. attention to health.)

MMI 注意身体。。。。(Attention to health. . . .)

CVAE
注意身体哦，注意身体
(Attention to health! attention to health.)

EditRetrieve
注意身体，好好休息。
(Attention to health, have a good rest.)

MemGM-S
好冷啊。注意保暖。
(It’s too cold, pay attention to keep warm.)

MemGM-H
好冷啊！注意身体！
(It’s too cold, attention to health.)

Table 5: The case study among all competing methods.

ual retrieval results. It clusters the training corpus
into multiple groups, extracts and memorizes com-
mon characteristics of each group for generation.
Compared with retrieval-augmented models, our
model makes improvements on the quality, rele-
vance and informativeness of response generation.

7 Acknowledgments

We would like to acknowledge Zhourong Chen
and Xingjian Shi for their constructive advices and
the anonymous reviewers for their insightful com-
ments. Research on this article was supported by
Hong Kong Research Grants Council under grants
16202118 and 16212516.

References
Dzmitry Bahdanau, KyungHyun Cho, and Yoshua

Bengio. Neural machine translation by jointly learn-
ing to align and translate. ICLR, 2015.

Deng Cai, Yan Wang, Victoria Bi, Zhaopeng Tu, Xiao-
jiang Liu, Wai Lam, and Shuming Shi. Skeleton-to-
response: Dialogue generation guided by retrieval
memory. arXiv preprint arXiv:1809.05296, 2018.

Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren,
and Percy Liang. Generating sentences by editing
prototypes. Transactions of the Association of Com-
putational Linguistics, 6:437–450, 2018.

Zongcheng Ji, Zhengdong Lu, and Hang Li. An infor-
mation retrieval approach to short text conversation.
arXiv preprint arXiv:1408.6988, 2014.

Diederik P Kingma and Jimmy Ba. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.

Diederik P Kingma and Max Welling. Auto-encoding
variational bayes. stat, 1050:10, 2014.

Hung Le, Truyen Tran, Thin Nguyen, and Svetha
Venkatesh. Variational memory encoder-decoder.
In Advances in Neural Information Processing Sys-
tems, pages 1515–1525, 2018.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. A diversity-promoting objective
function for neural conversation models. In NAACL-
HLT, pages 110–119, 2016.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. A diversity-promoting objective
function for neural conversation models. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
110–119, 2016.

Feng-Lin Li, Minghui Qiu, Haiqing Chen, Xiong-
wei Wang, Xing Gao, Jun Huang, Juwei Ren,
Zhongzhou Zhao, Weipeng Zhao, Lei Wang, et al.
Alime assist: an intelligent assistant for creating an
innovative e-commerce experience. In Proceedings
of the 2017 ACM on Conference on Information and
Knowledge Management, pages 2495–2498. ACM,
2017.

Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
How not to evaluate your dialogue system: An
empirical study of unsupervised evaluation metrics
for dialogue response generation. arXiv preprint
arXiv:1603.08023, 2016.

Andrea Madotto, Chien-Sheng Wu, and Pascale Fung.
Mem2seq: Effectively incorporating knowledge
bases into end-to-end task-oriented dialog systems.
In Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1468–1478, 2018.

Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang,
and Zhi Jin. Sequence to backward and for-
ward sequences: A content-introducing approach to
generative short-text conversation. arXiv preprint
arXiv:1607.00970, 2016.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the
40th annual meeting on association for computa-
tional linguistics, pages 311–318. Association for
Computational Linguistics, 2002.

Prasanna Parthasarathi and Joelle Pineau. Extending
neural generative conversational model using exter-
nal knowledge sources. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 690–695, 2018.



3825

Alan Ritter, Colin Cherry, and William B Dolan. Data-
driven response generation in social media. In
EMNLP, pages 583–593, 2011.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. Building end-
to-end dialogue systems using generative hierarchi-
cal neural network models. In AAAI, pages 3776–
3783, 2016.

Lifeng Shang, Zhengdong Lu, and Hang Li. Neural
responding machine for short-text conversation. In
ACL-IJCNLP, pages 1577–1586, 2015.

Yiping Song, Cheng-Te Li, Jian-Yun Nie, Ming
Zhang, Dongyan Zhao, and Rui Yan. An ensem-
ble of retrieval-based and generation-based human-
computer conversation systems. In Proceedings of
the 27th International Joint Conference on Artificial
Intelligence, pages 4382–4388. AAAI Press, 2018.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Se-
quence to sequence learning with neural networks.
In NIPS, pages 3104–3112, 2014.

Zhiliang Tian, Rui Yan, Lili Mou, Yiping Song, Yan-
song Feng, and Dongyan Zhao. How to make con-
text more useful? an empirical study on context-
aware neural conversational models. Annual Meet-
ing of the Association for Computational Linguis-
tics, 2:231–236, 2017.

Jason Weston, Sumit Chopra, and Antoine Bordes.
Memory networks. arXiv preprint arXiv:1410.3916,
2014.

Yu Wu, Furu Wei, Shaohan Huang, Zhoujun Li,
and Ming Zhou. Response generation by
context-aware prototype editing. arXiv preprint
arXiv:1806.07042, 2018.

Chien-Sheng Wu, Richard Socher, and Caiming
Xiong. Global-to-local memory pointer net-
works for task-oriented dialogue. arXiv preprint
arXiv:1901.04713, 2019.

Rui Yan, Yiping Song, and Hua Wu. Learning to re-
spond with deep neural networks for retrieval-based
human-computer conversation system. In SIGIR,
pages 55–64, 2016.

Lili Yao, Yaoyuan Zhang, Yansong Feng, Dongyan
Zhao, and Rui Yan. Towards implicit content-
introducing for generative short-text conversation
systems. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2190–2199, 2017.

Tom Young, Erik Cambria, Iti Chaturvedi, Hao Zhou,
Subham Biswas, and Minlie Huang. Augment-
ing end-to-end dialogue systems with commonsense
knowledge. In Thirty-Second AAAI Conference on
Artificial Intelligence, 2018.

Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.
Learning discourse-level diversity for neural dialog
models using conditional variational autoencoders.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), volume 1, pages 654–664, 2017.

Xiangyang Zhou, Daxiang Dong, Hua Wu, Shiqi Zhao,
Dianhai Yu, Hao Tian, Xuan Liu, and Rui Yan.
Multi-view response selection for human-computer
conversation. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 372–381, 2016.

Yimeng Zhuang, Xianliang Wang, Han Zhang, Jinghui
Xie, and Xuan Zhu. An ensemble approach to con-
versation generation. In National CCF Conference
on Natural Language Processing and Chinese Com-
puting, pages 51–62. Springer, 2017.


