
























































Semi_supervised_for_NAACL_HLT_2019 (1).pdf


Proceedings of NAACL-HLT 2019, pages 2496–2508
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

2496

Adversarial Category Alignment Network for Cross-domain
Sentiment Classification

Xiaoye Qu1∗ Zhikang Zou1∗ Yu Cheng2 Yang Yang3 Pan Zhou1†
1Huazhong University of Science and Technology

2Microsoft AI & Research
3University of Electronic Science and Technology of China

{xiaoye, panzhou}@hust.edu.cn | yu.cheng@microsoft.com
{zhikangzou001, dlyyang}@gmail.com

Abstract

Cross-domain sentiment classification aims to
predict sentiment polarity on a target domain
utilizing a classifier learned from a source
domain. Most existing adversarial learning
methods focus on aligning the global marginal
distribution by fooling a domain discrimina-
tor, without taking category-specific decision
boundaries into consideration, which can lead
to the mismatch of category-level features. In
this work, we propose an adversarial category
alignment network (ACAN), which attempts
to enhance category consistency between the
source domain and the target domain. Specif-
ically, we increase the discrepancy of two po-
larity classifiers to provide diverse views, lo-
cating ambiguous features near the decision
boundaries. Then the generator learns to create
better features away from the category bound-
aries by minimizing this discrepancy. Exper-
imental results on benchmark datasets show
that the proposed method can achieve state-
of-the-art performance and produce more dis-
criminative features.

1 Introduction

Sentiment classification aims to automatically
identify the sentiment polarity (i.e., positive or
negative) of the textual data. It has attracted
a surge of attention due to its widespread ap-
plications, ranging from movie reviews to prod-
uct recommendations. Recently, deep learning-
based methods have been proposed to learn good
representations and achieved remarkable success.
However, the performances of these works are
highly dependent on manually annotated training
data while annotation process is time-consuming
and expensive. Thus, cross-domain sentiment
classification, which aims to transfer knowledge
learned on labeled data from related domains

∗Equal contribution
† Corresponding author

(called source domain) to a new domain (called
target domain), becomes a promising direction.

One key challenge of cross-domain sentiment
classification is that the expression of emotional
tendency usually varies across domains. For in-
stance, considering reviews about two sorts of
products: Kitchen and Electronics. One set of
reviews would contain opinion words such as “de-
licious” or “tasty”, and the other “rubbery” or
“blurry”, to name but a few. Due to the small in-
tersection of two domain words, it remains a sig-
nificant challenge to bridge the two domains diver-
gence effectively.

Researchers have developed many algorithms
for cross-domain sentiment classification in the
past. Traditional pivot-based works (Blitzer et al.,
2007; Yu and Jiang, 2016) attempt to infer the
correlation between pivot words, i.e., the domain-
shared sentiment words, and non-pivot words, i.e.,
the domain-specific sentiment words by utilizing
multiple pivot prediction tasks. However, these
methods share a major limitation that manual se-
lection of pivots is required before adaptation.
Recently, several approaches (Sun et al., 2016;
Zellinger et al., 2017) focus on learning domain
invariant features whose distribution is similar in
source and target domain. They attempt to mini-
mize the discrepancy between domain-specific la-
tent feature representations. Following this idea,
most existing adversarial learning methods (Ganin
et al., 2016; Li et al., 2017) reduce feature differ-
ence by fooling a domain discriminator. Despite
the promising results, these adversarial methods
suffer from inherent algorithmic weakness. Even
if the generator perfectly fools the discriminator,
it merely aligns the marginal distribution of the
two domains and ignores the category-specific de-
cision boundaries. As shown in Figure 1 (left), the
generator may generate ambiguous or even mis-
matched features near the decision boundary, thus



2497

hindering the performance of adaptation.
To address the aforementioned limitations, we

propose an adversarial category alignment net-
work (ACAN) which enforces the category-level
alignment under a prior condition of global
marginal alignment. Based on the cluster assump-
tion in (Chapelle et al., 2009), the optimal predic-
tor is constant on high density regions. Thus, we
can utilize two classifiers to provide diverse views
to detect points near the decision boundaries and
train the generator to create more discriminative
features into high-density region. Specifically, we
first maximize the discrepancy of the outputs of
two classifiers to locate the inconsistent polarity
prediction points. Then the generator is trained to
avoid these points in the feature space by minimiz-
ing the discrepancy. In such an adversarial man-
ner, the ambiguous points are kept away from the
decision boundaries and correctly distinguished,
as shown in Figure 1 (right).

We evaluate our method on the Amazon reviews
benchmark dataset which contains data collected
from four domains. ACAN is able to achieve
the state-of-the-art results. We also provide anal-
yses to demonstrate that our approach can gen-
erate more discriminative features than the ap-
proaches only aligning global marginal distribu-
tion (Zhuang et al., 2015).

2 Related Work

Sentiment Classification: Deep learning based
models have achieved great success on sentiment
classification (Zhang et al., 2011). These models
usually contain one embedding layer which maps
each word to a dense vector, and different network
architectures then process combined word vectors
to generate a representation for classification. Ac-
cording to diverse network architectures, four cat-
egories are divided including Convolutional Neu-
ral Networks (CNNs) (Kalchbrenner et al., 2014;
Kim, 2014), Recurrent Neural Networks (RNNs)
(Yang et al., 2016; Zhou et al., 2016b), Recursive
Neural Networks (RecNNs) (Socher et al., 2013)
and other neural networks (Iyyer et al., 2015).
Domain Adaption: The fundamental challenge
to solve the domain adaptation lies here is that
data from the source domain and target domain
have different distributions. To alleviate this
difference, there are many pivot-based methods
(Blitzer et al., 2007; He et al., 2011; Gouws et al.,
2012; Yu and Jiang, 2016; Ziser and Reichart,

Figure 1: Left: marginal distribution alignment by
minimizing the distance between two domains can
generate ambiguous feature near the decision bound-
ary. Right: two different classifiers locate ambigu-
ous features by considering decision boundary to make
category-level alignment.

2018) which try to align domain-specific opinion
(non-pivot) words through domain-shared opinion
(pivot) words as the expression of emotional ten-
dency usually varies across domains, which is a
major reason of the domain difference. However,
selecting pivot words for these methods first is
very tedious, and the pivot words they find may
not be accurate. Apart from pivot-based methods,
denoising auto-encoders (Glorot et al., 2011; Chen
et al., 2012; Yang and Eisenstein, 2014) have been
extensively explored to learn transferable features
during domain adaption by reconstructing noise
input. Despite their promising results, they are
based on discrete representation. Recently, some
adversarial learning methods (Ganin et al., 2016;
Li et al., 2017, 2018) propose to reduce this differ-
ence by minimizing the distance between feature
distributions. But these methods solely focus on
aligning the global marginal distribution by fool-
ing a domain discriminator, which can lead to the
mismatch of category-level features. To solve this
issue, we propose to further align the category-
level distribution by taking the decision boundary
into consideration. Some recent works with class-
level alignment have been explored in computer
vision applications (Saito et al., 2017, 2018).
Semi-supervised learning: Considering the tar-
get samples as unlabeled data, our work is some-
how related to semi-supervised learning (SSL).
SSL has several critical assumptions, such as clus-
ter assumption that the optimal predictor is con-
stant or smooth on connected high density regions
(Chapelle et al., 2009), and manifolds assumption
that support set data lies on low-dimensional man-
ifolds (Chapelle et al., 2009; Luo et al., 2017). Our
work takes these assumptions to develop the ap-
proach.



2498

Figure 2: The overview of the proposed adversarial category alignment network in training and test phase.

3 Method

3.1 Problem Definition and Overall
Framework

We are given two domains Ds and Dt, denot-
ing the source domain and the target domain re-
spectively. Ds =

{
x
(s)
i , y

(s)
i

}ns
i=1

are ns labeled

source domain examples, where x(s)i means a sen-
tence and y(s)i is the corresponding polarity label.

Dt =
{
x
(t)
i

}nt
i=1

are nt unlabeled target domain
examples. In our proposed method, we denote
G as a feature encoder that extracts features from
the input sentence. Then two classifiers F1 and
F2 map these features to soft probabilistic outputs
p1(y|x) and p2(y|x) respectively.

The goal is to train a model to classify the target
examples correctly with the aid of source labeled
data and target unlabeled data. To achieve this, we
first train G, F1 and F2 to obtain global marginal
alignment. This step reduces the distance between
two domains but generates ambiguous target fea-
tures near the decision boundary. Thus, F1 and F2
are adjusted to detect them by maximizing predic-
tion discrepancy. After that, G is trained to gen-
erate better features avoiding appearing near the
decision boundary. The method also regularizes
G by taking the target data samples into consider-
ation. In this way, we can achieve the category
alignment. The proposed Adversarial Category
Alignment Network (ACAN) is illustrated in Fig-
ure 2. The detailed training progress is described
in Appendix D.

3.2 Marginal Distribution Alignment

To solve the domain adaption problem, we first
consider minimize the classification error on the
source labeled data for two classifiers:

Lcls =− 1
ns

ns∑
i=1

K∑
j=1

y
(s)
i (j) log ỹ

(s)
1i (j)

− 1
ns

ns∑
i=1

K∑
j=1

y
(s)
i (j) log ỹ

(s)
2i (j)

ỹ1i =F1(G(x
(s)
i )) ỹ2i = F2(G(x

(s)
i ))

(1)

where K denotes the number of different polari-
ties. In addition, similar to (Zhuang et al., 2015),
our method tries to explicitly minimize the dis-
tance between the embedding features from both
the source and the target domains. We adopt the
Kullback−Leibler (KL) to estimate the distribu-
tion divergence:

Lkl=

n∑
i=1

gs(i) log
gs(i)

gt(i)
+

n∑
i=1

gt(i) log
gt(i)

gs(i)

g′s =
1

ns

∑ns
i=1

G(x
(s)
i ), gs =

g′s
||g′s||1

g′t =
1

nt

∑nt
i=1

G(x
(t)
i ), gt =

g′t
||g′t||1

(2)

where gs, gt ∈ RD, || · ||1 denotes L1 normaliza-
tion. In this way, the latent network representa-
tions of two domains are encouraged to be similar.
In other words, the marginal distribution is forced
to be aligned.



2499

3.3 Category-level Alignment
Diverse Views: Considering the marginal distri-
bution alignment, there could be some ambigu-
ous features near the decision boundary, which are
easy to be incorrectly categorized into a specific
class. If we alter the boundary of classifier F1 and
F2, the samples closer to the decision boundary
would have larger change. To explore these sam-
ples, we use F1 and F2 to provide diverse guid-
ance. We define a discrepancy between proba-
bilistic outputs of the two classifiers p1(y|x) and
p2(y|x). The formula is:

Ldis = Ex∼Dt [d(p1(y|x), p2(y|x))] (3)

where d(p1(y|x), p2(y|x)) defines the average ab-
solute difference for K classes, which is:

d(p1(y|x), p2(y|x))= 1
K

K∑
i=1

|p1i(y|x)−p2i(y|x)|
(4)

Specifically, we first fix the generator G and
train the classifiers F1,F2 to detect points near the
decision boundary by maximizing their discrep-
ancy. The objective is as follows:

max
F1,F2

Ex∼Dt [
1

K

K∑
i=1

|p1i(y|x)− p2i(y|x)|] (5)

Then, this discrepancy is minimized by optimizing
G in order to keep these points away from the deci-
sion boundary and categorized into correct classes.
The objective is as follows:

min
G

Ex∼Dt [
1

K

K∑
i=1

|p1i(y|x)− p2i(y|x)|] (6)

This adversarial step is repeated in the whole
training process so that we can continuously lo-
cate non-discriminative points and classify them
correctly, forcing the model to achieve category-
level alignment on two domains.

3.4 Training Steps
The whole training procedure can be divided into
three steps. In the first step, we consider both min-
imizing the classification error and marginal dis-
tribution discrepancy to achieve global marginal
alignment. The loss function of this step can be
written as:

L1 = Lcls + λ1Lkl (7)

In the second step, we consider increasing the dif-
ference of two classifiers F1 and F2 for the fixed
G, thus the ambiguous features can be located by
the diverse views. The loss function is defined as
below:

L2 = Lcls − λ2Ldis (8)
Lcls is used here to ensure the stability of the train-
ing process. λ2 is a hyper-parameter controlling
the range of classifiers. In the third step, the dif-
ference of two classifiers should be reduced for the
fixed F1 and F2:

L3 = Lcls + λ3Ldis (9)

Lcls and λ3 used here are similar to the second
step. We repeat this step n times to balance the
generator and two classifiers. After each step,
the corresponding part of the network parameters
will be updated. Algorithm 1 describes the overall
training procedure.

Algorithm 1 Training procedure of ACAN
Require: Ds, Dt, G, F1, F2
Require: λ1, λ2, λ3, iteration number n

for i ∈ [1,max−epochs] do
for minibatch B(s), B(t) ∈ D(s), D(t) do

compute Lcls on
[
xi ∈ B(s), yi ∈ B(s)

]
compute Lkl on

[
xi ∈ B(s), xj ∈ B(t)

]
L1 = Lcls + λ1Lkl
update G, F1, F2 by minimizing L1
compute Lcls on

[
xi∈B(s) , yi∈B(s)

]
compute Ldis on

[
xi∈B(t) , xi∈B(t)

]
L2 = Lcls − λ2Ldis
fix G, update F1, F2 by minimizing L2.
for j ∈ [1, n] do

compute Lcls on
[
xi ∈ B(s), yi ∈ B(s)

]
compute Ldis on

[
xi ∈ B(t), xi ∈ B(t)

]
L3 = Lcls + λ3Ldis
fix F1, F2, update G by minimizing L3.

end for
end for

end for

3.5 Generator Regularizer

To further enhance the feature generator, we in-
troduce to regularize G with the information of
unlabeled target data. Generally, the mapping of
G(·) can been seen a low-dimensional feature of
the input. According to the manifolds assump-
tion (Chapelle et al., 2009), this feature space is



2500

expected to be low-dimensional manifold and lin-
early separable. Inspired by (Luo et al., 2017), we
consider the connections between data points to
regularize G(·) in the feature space. Specifically,
the regularizer is formulated as follows:

R(G) =
∑
x∈Dt

lG(xi, xj) (10)

here lG is to approximate the semantic similarity
of two feature embeddings. Possible options in-
clude triplet loss (Wang et al., 2016), Laplacian
eigenmaps (Belkin and Niyogi, 2003) etc. After
exploring many tricks, we find below is optimal
which is also used by (Luo et al., 2017):

lG =

{
d2i,j sij=1

max(0,m−di,j)2 sij=0
(11)

where di,j is L2 distance between data points, m
is a predefined distance, and sij indicates whether
xi and xj belong to the same class or not. Eq. 10
serves as a regularization that encourages the out-
put of R(G) to be distinguishable among classes.
It is applied on target data and integrated in the
framework in the third training step, weighted by
λ4. During the training, the underlying label of
xi is estimated by taking the maximum posterior
probability of the two classifiers.

3.6 Theoretical Analysis
In this subsection, we provide a theoretical analy-
sis of our method, which is inspired by the theory
of domain adaptation in (Ben-David et al., 2010).

For each domain, there is a labeling function on
inputs X , defined as f : X → [0, 1]. Thus, the
source domain is denoted as 〈Ds, fs〉 and the tar-
get domain as 〈Dt, ft〉. We define a hypothesis
function h: X → [0, 1] and a disagreement func-
tion:

�(h1, h2) = E[|h1(x)− h2(x)|] (12)
Then the expected error on the source samples
�s(h, f) of h is defined as:

�s(h) = �s(h, fs) = Ex∼Ds [|h(x)−fs(x)|] (13)
Also for the target domain, we have

�t(h) = �s(h, ft) = Ex∼Dt [|h(x)− ft(x)|] (14)
As is introduced in (Ben-David et al., 2010), the
probabilistic bound of the error of hypothesis h on
the target domain �t(h) is defined as:

∀h ∈ H, �t(h) ≤ �s(h) + 12dHΔH(Ds, Dt) + λ (15)

where the expected error �t(h) is bounded by three
terms: (1) the expected error on the source exam-
ples �s(h); (2) the divergence between the distri-
butions Ds and Dt; (3) the combined error of the
ideal joint hypothesis λ.

First, the training algorithm is easy to mini-
mize �s(h) with source label information. Second,
λ is expected to be negligibly small and can be
usually disregarded. Therefore, the second term
dHΔH(Ds, Dt) is important quantitatively in com-
puting the target error.

Regarding dHΔH(Ds, Dt), we have

dHΔH(Ds, Dt) = 2 sup
h,h′∈H

|�s(h, h′)− �t(h, h′)|

=2 sup
h,h′∈H

|Ex∼Ds [|h(x)−h′(x)|]−Ex∼Dt [|h(x)−h′(x)|]|
(16)

where h and h′ are two sets of hypotheses in
H. As we have sufficient labeled source exam-
ples to train, h and h′ can have consistent and
correct predictions on the source domain data.
Thus, dHΔH(Ds, Dt) is approximately calculated
as Ex∼Dt [|h(x) − h′(x)|]. In our model, the hy-
pothesis h can be decomposed into the feature ex-
tractor G and the classifier F using the notation ◦.
Thus dHΔH(Ds, Dt) can be formulated as:

sup
F1,F2

Ex∼Dt [|F1 ◦G(x)− F2 ◦G(x)|] (17)

For fixed G, sup can be replaced by max. There-
fore, F1 and F2 are trained to maximize the dis-
crepancy of their outputs and we expect G to min-
imize this discrepancy. So we obtain

min
G

max
F1,F2

Ex∼Dt [|F1 ◦G(x)−F2 ◦G(x)|] (18)

The maximization of F1 and F2 is to provide di-
verse views, to find ambiguous points near the de-
cision boundary, and the minimization of G is to
keep these points away from the decision bound-
ary. To optimize Eq. 18, we assist the model to
capture the whole feature space on the target do-
main better and achieve lower errors.

4 Experiments

4.1 Data and Experimental Setting

We evaluate the proposed ACAN on the Amazon
reviews benchmark datasets collected by Blitzer
(2007). It contains reviews from four differ-
ent domains: Books (B), DVDs (D), Electron-
ics (E), Kitchen appliances (K). There are 1000



2501

Source → Target Previous Work Models ACAN Models
SVM AuxNN DANN PBLM DAS Baseline ACAN-KL ACAN-KM ACAN

D → B 75.20 80.80 81.70 82.50 82.05 81.30 83.00 82.85 82.35
E → B 68.85 78.00 78.55 71.40 80.00 79.50 80.30 79.80 79.75
K → B 70.00 77.85 79.25 74.20 80.05 79.05 79.10 79.60 80.80
B → D 77.15 81.75 82.30 84.20 82.75 82.50 83.35 83.25 83.45
E → D 69.50 80.65 79.70 75.00 80.15 79.25 81.00 80.80 81.75
K → D 71.40 78.90 80.45 79.80 81.40 79.10 80.15 82.25 82.10
B → E 72.15 76.40 77.60 77.60 81.15 77.80 78.80 80.85 81.20
D → E 71.65 77.55 79.70 79.60 81.55 78.00 81.30 82.75 82.80
K → E 79.75 84.05 86.65 87.10 85.80 84.35 84.70 86.20 86.60
B → K 73.50 78.10 76.10 82.50 82.25 78.00 77.30 81.00 83.05
D → K 72.00 80.05 77.35 83.20 81.50 74.65 73.05 77.65 78.60
E → K 82.80 84.15 83.95 87.80 84.85 81.05 83.70 83.70 83.35
Average 73.66 79.85 80.29 80.40 81.96 79.55 80.48 81.78 82.15

Table 1: Accuracy of adaptation on Amazon benchmark. All results are the averaged performance of each neural
model by a 5-fold cross-validation protocol.

positive and 1000 negative reviews for each do-
main, as well as a few thousand unlabeled exam-
ples, of which the positive and negative reviews
are balanced. Following the convention of pre-
vious works (Zhou et al., 2016a; Ziser and Re-
ichart, 2018; He et al., 2018), we construct 12
cross-domain sentiment classification tasks. In
our transferring task, we employ a 5-fold cross-
validation protocol, that is, in each fold, 1600 bal-
anced samples are randomly selected from the la-
beled data for training and the rest 400 for valida-
tion. The results we report are the averaged per-
formance of each model across these five folds.

4.2 Training Details and Hyper-parameters

In our implementation, the feature encoder G con-
sists of three parts including a 300-dimensional
word embedding layer using GloVe (Pennington
et al., 2014), a one-layer CNN with ReLU activa-
tion function adopted in (Yu and Jiang, 2016; He
et al., 2018) and a max-over-time pooling through
which final sentence representation is obtained.
Specifically, the convolution filter and the window
size of this one-layer CNN are 300 and 3 sepa-
rately. Similarly, the classifier F1 and F2 can be
decomposed into one dropout layer and one fully
connected output layer. For the fully connected
layer, we constrain the l2-norm of the weight vec-
tor, setting its max norm to 3. For the imple-
mentation of generator regularizer, we apply dou-
bly stochastic sampling approximation due to the
computational complexity.

The margin m is set to 1 in this procedure. Dur-
ing training period, λ1, λ2, λ3, λ4, and n are set
to 5.0, 0.1, 0.1, 1.5, 2. Similar to (He et al.,

2018), we parametrize λ4 as a dynamic weight
exp[−5(1 − tmax−epochs)2]λ4. This is to mini-
mize the effort of the regularizer as the predictor
is not good at the beginning of training. We train
30 epochs for all our experiments with batch-size
50 and dropout rate 0.5. RMSProp (Tieleman and
Hinton, 2012) optimizer with learning rate set to
0.0001 is used for all experiments.

4.3 Methods for Comparison

We consider the following approaches for compar-
isons (The URLs of previous methods code and
data we use are in Appendix A):
SVM (Fan et al., 2008): This is a non-domain-
adaptation method, which trains a linear SVM on
the raw bag-of-words representation of the labeled
source domain.
AuxNN (Yu and Jiang, 2016): This method uses
two auxiliary tasks to learn sentence embeddings
that works well across two domains. For fair com-
parison, we replace the neural model in this work
with our CNN encoder.
DANN (Ganin et al., 2016): This method ex-
ploits a domain classifier to minimize the discrep-
ancy between two domains via adversarial training
manner. we replace its encoder with our CNN-
based encoder.
PBLM (Ziser and Reichart, 2018): This is a repre-
sentation learning model that exploits the structure
of the input text. Specifically, we choose CNN as
the task classifier.
DAS (He et al., 2018): This method employs
two regularizations: entropy minimization and
self-ensemble bootstrapping to refine the classifier
while minimizing the domain divergence.



2502

Figure 3: Visualization by applying principal component analysis to the representation of source training data and
target testing data produced by ACAN-KL (left) and ACAN (right) for K→E task. The red, blue, green, and black
points denote the source positive, source negative, target positive, and target negative examples correspondingly.

Baseline: Our baseline model is a non-adaptive
CNN similar to (Kim, 2014), trained without using
any target domain information, which is a variant
of our model by setting λ1, λ2, λ3, λ4 to zeros.
ACAN-KL: ACAN-KL is a variant of our model
which minimizes the distance between the features
of two domains by minimizing the KL divergence.
(set λ2 = λ3 = λ4 = 0)
ACAN-KM: ACAN-KM introduces the adversar-
ial category mapping based on ACAN-KL without
the regularizer. (set λ4 = 0).
ACAN: It is our full model.

4.4 Results

Table 1 shows the classification accuracy of differ-
ent methods on the Amazon reviews, and we can
see that the proposed ACAN outperforms all other
methods generally. It is obvious to see that SVM
performs not well in domain transferring task,
beaten by Baseline. We can notice that exploring
the structure of the input text (AuxNN and PBLM)
brings some improvements over Baseline. How-
ever, these two pivot-based methods present rela-
tively lower ability than DAS, which jointly mini-
mizes global feature divergence and refines clas-
sifier. Compared to DAS, our proposed ACAN
can improve 0.19% on the average accuracy. This
can be explained by that we deal with the relation-
ship between target features distribution and clas-
sifier more precisely. Finally, we conduct exper-
iments on the variants of the ACAN. It is clear
that the performances of Baseline, ACAN-KL,
ACAN-KM and ACAN present a growing trend
in most cases. Compared with ACAN-KL, ACAN
achieves large gain from 80.48% to 82.15%, show-
ing the effectiveness of category-level alignment.

4.5 Case Study

To better understand the results of different mod-
els, we conduct experiments on task B → E.
For each sentiment polarity, we first extract the
most related CNN filters according to the learned
weights of the output layer in classifier F1. Since
all listed models use a window size of 3, the out-
puts of CNN with the highest activation values
correspond to the most useful trigrams.

As shown in Table 2, we identify the top tri-
grams from 10 most related CNN filters on the
target domain. It is obvious that Baseline and
ACAN-KL are more likely to capture the domain-
independent words, such as “pointless”, “disap-
pointing” and “great”. Thus, the performance
of these two models drops much when applied
to the target domain. Besides, DAS can capture
more words of the target domain, but it is lim-
ited to nouns with less representativeness, such
as ”receiver”, ”product” and etc. Compared to
them, ACAN is able to extract the domain-specific
words like “flawlessly” and “rechargeable”. These
results are consistent with the accuracy of each
model’s predictions. We also conduct experiments
on the tasks B → K and K → D. Due to the space
limitations, the results are presented in Appendix
B.

4.6 Visualization of features

For more intuitive understanding of the differences
between the global marginal alignment and cate-
gory alignment, we further perform a visualiza-
tion of the feature representations of the ACAN-
KL and ACAN model for the training data in the
source domain and the testing data in the target do-
main for the K→E task. As can be seen in Figure
3, global marginal alignment causes ambiguous



2503

Method Negative Sentiment Positive Sentiment

Baseline

audio-was-distorted, is-absolutely-pointless, *-very-disappointing,
waste-of-money, was-point-most, an-unsupported-config,
an-extremely-disappointed, author-album-etc,
cure-overnight-headphones, aa-rechargable-batteries

wep-encryption-detailed, totally-wireless-headset, best-!-i,
love-it-!, again-period-!, beautifully-great-price,
awesome-accurate-sound, beautifully-designed-futuristic,
wonderful-product-*, glad-i-purchased

ACAN-KL

totally-useless-method, audio-was-distorted, *-very-weak,
*-very-disappointing, extra-ridiculous-buttons, hopeless-mess-no,
now-as-useless, waste-of-cash, is-absolutely-pointless,
manual-is-useless

gift-i-love, uniden-cordless-telephone, a-journey-to,
totally-wireless-headset, your-own-frequencies, a-gift-excellent,
exceptional-being-rechargeable, gorgeous-picture-excellent,
with-wireless-security, beautifully-designed-futuristic

DAS

receiver-was-faulty, defective-product-i, is-useless-i,
do-not-waste, did-not-work, very-poor-quality,
the-crappy-keyboard, just-too-weak, is-absolutely-pointless,
very-stupid-design

is-an-excellent, excellent-monitor-with, is-very-nice,
truly-excellent-headphones, an-incredible-sound
advanced-technology-incredible, this-is-an, !-highly-recommended
show-very-easy, picture-is-fabulous

ACAN

very-poorly-designed, garbage-im-sorry, handed-was-defective,
receiver-was-faulty, *-very-disappointing, audio-was-distorted
dirty-and-scratched, extra-ridiculous-buttons,
cartridges-are-incompatible, awful-absolutely-horrible

performs-flawlessly-hours, a-gift-excellent, beautifully-great-price,
encryption-detailed-monitoring, fit-excellent-sound, !-very-happy,
exceptional-being-rechargeable, beautifully-designed-futuristic,
smooth-accurate-tracking, digital-camera-during

Table 2: Comparison of the top trigrams chosen from 10 most related CNN filters learned on the task B → E. The
entire table contains the results achieved by the variants of our method. * denotes a padding. The domain-specific
words are in bold.

Figure 4: The influence of the number of labeled target
data on the task E → D and B → E.

features locating between two clusters while cat-
egory alignment effectively projects these points
into clusters, thus leading a more robust classifi-
cation result. We also conduct experiments on the
tasks B → E and B → K. Due to the space limita-
tions, the results are presented in Appendix C.

4.7 Model Analysis

In this part, we provide analysis to our proposed
ACAN variants. In Figure 4, we show the com-
parison between Baseline and ACAN under a set-
ting that some labeled target data are randomly
selected and mixed with training data. Here, we
present results on two transferring tasks while a
similar tendency can be observed in other pairs.
With an increase in the number of randomly se-
lected labeled target data, the difference between
the two models gradually decreases and ACAN
also progressively obtains better results. These
trends indicate that our ACAN is more effective

Figure 5: The training process of four ACAN model
variants on the task K → E.

with no or little-labeled target data and can further
benefit from more labeled target data. In Figure
5, we can easily observe that ACAN continuously
shows better results during the whole training pro-
cess among four settings. After some epochs,
ACAN-KL starts presenting lower testing accu-
racy than Baseline. One possible reason is that
those categories which are initially well aligned
between the source and target may be incorrectly
mapped because of ignoring category-level feature
distribution. This observation can prove our moti-
vation in some degree.

5 Conclusion

In this paper, we propose a novel approach,
which utilizes diverse view classifiers to achieve
category-level alignment for sentiment analysis.
Unlike previous works, we take the decision
boundary into consideration, thus classifying the



2504

target samples correctly into the corresponding
category. Experiments show the proposed ACAN
significantly outperforms state-of-the-art methods
on the Amazon benchmark. In future we would
like to adapt our method to other domain adapta-
tion tasks and consider more effective alternatives
for the generator regularizer.

Acknowledgments

This work was supported in part by the National
Natural Science Foundation of China under Grant
61602197, and in part by the Fundamental Re-
search Funds for the Central Universities, HUST:
2016YXMS085.

References
Mikhail Belkin and Partha Niyogi. 2003. Laplacian

eigenmaps for dimensionality reduction and data
representation. Neural Comput., 15(6):1373–1396.

Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79(1):151–175.

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 440–447. Association for Computational Lin-
guistics.

Olivier Chapelle, Bernhard Scholkopf, and Alexander
Zien. 2009. Semi-supervised learning (chapelle, o.
et al., eds.; 2006)[book reviews]. IEEE Transactions
on Neural Networks, 20(3):542–542.

Minmin Chen, Zhixiang Xu, Kilian Weinberger, and
Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. arXiv preprint
arXiv:1206.4683.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of ma-
chine learning research, 9(Aug):1871–1874.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavi-
olette, Mario Marchand, and Victor Lempitsky.
2016. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research,
17(1):2096–2030.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment

classification: A deep learning approach. In Pro-
ceedings of the 28th international conference on ma-
chine learning (ICML-11), pages 513–520.

Stephan Gouws, GJ Van Rooyen, MIH Medialab, and
Yoshua Bengio. 2012. Learning structural corre-
spondences across different linguistic domains with
synchronous neural language models. In Proc. of
the xLite Workshop on Cross-Lingual Technologies,
NIPS.

Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel
Dahlmeier. 2018. Adaptive semi-supervised learn-
ing for cross-domain sentiment classification. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
3467–3476. Association for Computational Linguis-
tics.

Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics for
cross-domain sentiment classification. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 123–131. Association for Com-
putational Linguistics.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daumé III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classification.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
1681–1691. Association for Computational Linguis-
tics.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
655–665. Association for Computational Linguis-
tics.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1746–1751. As-
sociation for Computational Linguistics.

Zheng Li, Ying Wei, Yu Zhang, and Qiang Yang.
2018. Hierarchical attention transfer network for
cross-domain sentiment classification. In Proceed-
ings of the Thirty-Second AAAI Conference on Ar-
tificial Intelligence, AAAI 2018, New Orleans, Lou-
siana, USA, February 2–7, 2018.

Zheng Li, Yu Zhang, Ying Wei, Yuxiang Wu, and
Qiang Yang. 2017. End-to-end adversarial memory
network for cross-domain sentiment classification.
In Proceedings of the International Joint Conference
on Artificial Intelligence (IJCAI 2017).



2505

Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and
Bo Zhang. 2017. Smooth neighbors on teacher
graphs for semi-supervised learning. arXiv preprint
arXiv:1711.00258.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543. Associa-
tion for Computational Linguistics.

Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and
Kate Saenko. 2017. Adversarial dropout regulariza-
tion. arXiv preprint arXiv:1711.01575.

Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and
Tatsuya Harada. 2018. Maximum classifier discrep-
ancy for unsupervised domain adaptation. In 2018
IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, Salt Lake City, UT, USA,
June 18-22, 2018, pages 3723–3732.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642. Association for Computa-
tional Linguistics.

Baochen Sun, Jiashi Feng, and Kate Saenko. 2016.
Return of frustratingly easy domain adaptation. In
AAAI.

T. Tieleman and G. Hinton. 2012. Lecture 6.5—
RmsProp: Divide the gradient by a running average
of its recent magnitude. COURSERA: Neural Net-
works for Machine Learning.

Jing Wang, Yu Cheng, and Rogério Schmidt Feris.
2016. Walk and learn: Facial attribute representa-
tion learning from egocentric video and contextual
data. In CVPR, pages 2295–2304. IEEE Computer
Society.

Yi Yang and Jacob Eisenstein. 2014. Fast easy unsu-
pervised domain adaptation with marginalized struc-
tured dropout. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 538–544.
Association for Computational Linguistics.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal attention networks for document classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 1480–1489. Association for Computa-
tional Linguistics.

Jianfei Yu and Jing Jiang. 2016. Learning sentence em-
beddings with auxiliary tasks for cross-domain sen-
timent classification. In Proceedings of the 2016

Conference on Empirical Methods in Natural Lan-
guage Processing, pages 236–246. Association for
Computational Linguistics.

Werner Zellinger, Thomas Grubinger, Edwin Lughofer,
Thomas Natschlger, and Susanne Saminger-Platz.
2017. Central moment discrepancy (CMD) for
domain-invariant representation learning. arxiv
preprint arXiv:1702.08811.

Kunpeng Zhang, Yu Cheng, Yusheng Xie, Daniel
Honbo, Ankit Agrawal, Diana Palsetia, Kathy Lee,
Wei-keng Liao, and Alok N. Choudhary. 2011. SES:
sentiment elicitation system for social media data.
In ICDM, pages 129–136. IEEE Computer Society.

Guangyou Zhou, Zhiwen Xie, Jimmy Xiangji Huang,
and Tingting He. 2016a. Bi-transferring deep neu-
ral networks for domain adaptation. In Proceed-
ings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 322–332. Association for Computa-
tional Linguistics.

Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu,
Hongyun Bao, and Bo Xu. 2016b. Text classifica-
tion improved by integrating bidirectional lstm with
two-dimensional max pooling. In Proceedings of
COLING 2016, the 26th International Conference
on Computational Linguistics: Technical Papers,
pages 3485–3495. The COLING 2016 Organizing
Committee.

Fuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno Jialin
Pan, and Qing He. 2015. Supervised representation
learning: Transfer learning with deep autoencoders.
In Twenty-Fourth International Joint Conference on
Artificial Intelligence.

Yftah Ziser and Roi Reichart. 2018. Pivot based lan-
guage modeling for improved neural domain adap-
tation. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers), pages 1241–
1251. Association for Computational Linguistics.



2506

A URLs of Data and Code

Here, we provide a list of URLs about the dataset
and the code of the previous methods we compare.

• The Amazon product review dataset gath-
ered by Blitzer et al (2007): http://
jmcauley.ucsd.edu/data/amazon/

• Code for AuxNN (Yu and Jiang, 2016):
https://github.com/jefferyYu/

Learning-Sentence-Embeddings-

for-cross-domain-sentiment

-classification

• Code for DANN (Ganin et al., 2016):
https://github.com/pumpikano/
tf-dann

• Code for PBLM (Ziser and Reichart, 2018):
https://github.com/yftah89/
PBLM-Domain-Adaptation

• Code for DAS (He et al., 2018): https://
github.com/ruidan/DAS

B Trigram Full Results

In the paper, Table 2 shows the top trigrams cho-
sen from 10 most related CNN filters learned on
the task B → E by the DAS the and variants of the
ACAN. For a more comprehensive presentation,
we also conduct experiments on the task B → K
and K → D, and the results are listed in Table 3
and Table 4 respectively. It is obvious that the pro-
posed ACAN is better to capture domain-specific
words, compared to its variants and DAS.

C Visualization full results

In this paper, Figure 3 visualizes the feature repre-
sentations of the ACAN-KL and ACAN model for
the training data in the source domain and the test-
ing data in the target domain for the K→E task.
For a more comprehensive presentation, we also
conduct experiments on the task B → E and B →
K, and the results are listed in Figure 7 and Figure
8 respectively. It is obvious that global marginal
alignment causes ambiguous features locating be-
tween two clusters while category alignment ef-
fectively projects these points into clusters.

D Detailed Illustration of Training Phase

The overview of the propose ACAN is shown in
Figure 2. For a better understanding, we present
the changes of decision boundaries and data distri-
bution during the network training process, shown
in Figure 6. First, we train F1 and F2 to locate the
points near the decision boundary by maximizing
their discrepancy. Then, we train G to minimize
the discrepancy to achieve category-level align-
ment. At the same time, the generator G is reg-
ularized with data from target domain.



2507

Method Negative Sentiment Positive Sentiment

Baseline

rice-also-disappointing, be-such-shoddy, basically-worthless-*,
does-n’t-toast, waste-your-time, is-totally-useless,
waste-of-time, was-sorely-disappointed,
safe-stainless-versus, were-very-dull

sophisticated-gorgeous-retro, lodge-properly-packaged,
delonghi-cooked-pretty, this-stunning-slice, beautifully-i-highly,
perfection-!-i, an-excellent-performer, beautiful-shape-!,
your-cooking-equipment, *-highly-recommend

ACAN-KL

totally-useless-and, rice-also-disappointing, be-such-shoddy,
flatware-is-unusable, misleading-advertising-i,
waste-of-time, poorly-made-expensive, were-very-dull,
makes-weak-coffee, was-sorely-disappointed

dishwasher-nonstick-!, beautifully-get-a, this-stunning-slice,
!-happy-holidays, look-wonderful-and, beautifully-i-highly,
month-i-!, excellent-addition-to,
*-highly-recommend, grilled-meats-and

DAS

was-very-disappointing, shoddy-junk-garbage, totally-useless-and,
thermometer-very-disappointing, disappointing-coffee-maker,
by-flimsy-brittle, do-not-waste, waste-of-time,
very-disappointing-and, be-lukewarm-disgusting

makes-wonderful-tasting, this-beautiful-pan, is-an-excellent,
sophisticated-gorgeous-retro, is-highly-recommend, awesome-!-!,
makes-great-coffee, and-versatile-pan,
also-highly-recommend, it-is-great

ACAN

kettle-was-leaking, totally-useless-and, rice-also-disappointing,
flatware-is-unusable, was-sorely-disappointed, waste-of-money
flat-crooked-ugly, is-no-metal,
now-basically-worthless, makes-weak-coffee

sophisticated-gorgeous-retro,great-hot-drinks, it-a-learning,
!-happy-holidays, great-grilled-sandwiches, !-highly-recommend,
it-toasts-beautifully, look-wonderful-and,
excellent-addition-to, nonstick-!-you

Table 3: Comparison of the top trigrams chosen from 10 most related CNN filters learned on the task B → K. The
entire table contains the results achieved by the variants of our method. * denotes a padding. The domain-specific
words are in bold.

Method Negative Sentiment Positive Sentiment

Baseline

beyond-is-badly, such-gross-audio, returning-for-the,
poorly-executed-poorly, director-john-ford, is-so-disappointing,
does-not-work, is-a-disappointment,
lighting-poor-directing, pathetic-remake-from

hip-hop-dvd, combines-multiple-genres,
the-most-amazing, very-good-price, stylish-photography,
best-performances-since, amazing-!-the, lasting-and-unique,
accomplished-and-dedicated, loves-being-able

ACAN-KL

return-an-even, such-gross-audio, star-hollywood-material,
poorly-executed-poorly, does-not-work,
a-complete-failure, is-a-disappointment, pathetic-remake-from,
waste-of-money, failed-miserably-alyson

beautifully-classic-comedy, adult-who-enjoys, i-bought-loves,
the-most-acclaimed, combines-multiple-genres, very-good-price,
’s-memorable-entrance, great-and-splendid,
acclaimed-romantic-comedies, stylish-photography-and

DAS

release-the-movie, a-total-waste, dodging-bullets-and,
incompetent-direction-by, is-absolutely-horrible,
very-disappointing-once, is-pretty-pathetic, was-a-waste
awful-the-ending, pathetic-remake-from

entertaining-and-inspirational, truly-enjoy-it, an-amazing-artist,
superb-production-and, very-good-price, fantastic-film-!,
best-performance-since, perfect-love-on,
amazing-film-from, and-fascinating-documentaries

ACAN

unfunny-overrated-movie, of-gross-caricature, was-a-waste,
pathetic-remake-from, poorly-executed-poorly, was-awful-from,
directing-poor-writing, is-a-disappointment,
disgusting-badly-written, tasteless-unoriginal-drivel

combines-multiple-genres, very-good-price, are-great-featuring,
accomplished-and-dedicated, great-performance-tongue,
’s-stylish-photography, is-my-favourite, family-classics-action
the-most-amazing, fantastic-action-picture

Table 4: Comparison of the top trigrams chosen from 10 most related CNN filters learned on the task K → D. The
entire table contains the results achieved by the variants of our method. * denotes a padding. The domain-specific
words are in bold.

Source 

Target

Maximize Discrepancy Minimize DiscrepancyRegularization

Target Cluster 
Decision Boundary

Adversarial Training 

Figure 6: The detail of changes in decision boundaries and data distribution during the network training process.



2508

Figure 7: Visualization by applying principal component analysis to the representation of source training data and
target testing data produced by ACAN-KL (left) and ACAN (right) for B→E task. The red, blue, green, and black
points denote the source positive, source negative, target positive, and target negative examples correspondingly.

Figure 8: Visualization by applying principal component analysis to the representation of source training data and
target testing data produced by ACAN-KL (left) and ACAN (right) for B→K task. The red, blue, green, and black
points denote the source positive, source negative, target positive, and target negative examples correspondingly.


