



















































Reference Bias in Monolingual Machine Translation Evaluation


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 77–82,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Reference Bias in Monolingual Machine Translation Evaluation

Marina Fomicheva
Institute for Applied Linguistics
Pompeu Fabra University, Spain

marina.fomicheva@upf.edu

Lucia Specia
Department of Computer Science

University of Sheffield, UK
l.specia@sheffield.ac.uk

Abstract

In the translation industry, human transla-
tions are assessed by comparison with the
source texts. In the Machine Translation
(MT) research community, however, it is
a common practice to perform quality as-
sessment using a reference translation in-
stead of the source text. In this paper we
show that this practice has a serious issue
– annotators are strongly biased by the ref-
erence translation provided, and this can
have a negative impact on the assessment
of MT quality.

1 Introduction

Equivalence to the source text is the defining char-
acteristic of translation. One of the fundamental
aspects of translation quality is, therefore, its se-
mantic adequacy, which reflects to what extent the
meaning of the original text is preserved in the
translation. In the field of Machine Translation
(MT), on the other hand, it has recently become
common practice to perform quality assessment
using a human reference translation instead of the
source text. Reference-based evaluation is an at-
tractive practical solution since it does not require
bilingual speakers.

However, we believe this approach has a strong
conceptual flaw: the assumption that the task of
translation has a single correct solution. In real-
ity, except for very short sentences or very specific
technical domains, the same source sentence may
be correctly translated in many different ways.
Depending on a broad textual and real-world con-
text, the translation can differ from the source text
at any linguistic level – lexical, syntactic, seman-
tic or even discourse – and still be considered per-
fectly correct. Therefore, using a single translation
as a proxy for the original text may be unreliable.

In the monolingual, reference-based evaluation
scenario, human judges are expected to recognize
acceptable variations between translation options
and assign a high score to a good MT, even if
it happens to be different from a particular hu-
man reference provided. In this paper we argue
that, contrary to this expectation, annotators are
strongly biased by the reference. They inadver-
tently favor machine translations (MTs) that make
similar choices to the ones present in the reference
translation. To test this hypothesis, we perform an
experiment where the same set of MT outputs is
manually assessed using different reference trans-
lations and analyze the discrepancies between the
resulting quality scores.

The results confirm that annotators are indeed
heavily influenced by the particular human trans-
lation that was used for evaluation. We discuss
the implications of this finding on the reliability
of current practices in manual quality assessment.
Our general recommendation is that, in order to
avoid reference bias, the assessment should be per-
formed by comparing the MT output to the origi-
nal text, rather than to a reference.

The rest of this paper is organized as follows.
In Section 2 we present related work. In Section 3
we describe our experimental settings. In Section
4 we focus on the effect of reference bias on MT
evaluation. In Section 5 we examine the impact of
the fatigue factor on the results of our experiments.

2 Related Work

It has become widely acceptable in the MT com-
munity to use human translation instead of (or
along with) the source segment for MT evalua-
tion. In most major evaluation campaigns (ARPA
(White et al., 1994), 2008 NIST Metrics for
Machine Translation Challenge (Przybocki et al.,
2008), and annual Workshops on Statistical Ma-

77



chine Translation (Callison-Burch et al., 2007;
Bojar et al., 2015)), manual assessment is ex-
pected to consider both MT fluency and adequacy,
with a human (reference) translation commonly
used as a proxy for the source text to allow for
adequacy judgement by monolingual judges.

The reference bias problem has been exten-
sively discussed in the context of automatic MT
evaluation. Evaluation systems based on string-
level comparison, such as the well known BLEU
metric (Papineni et al., 2002) heavily penalize po-
tentially acceptable variations between MT and
human reference. A variety of methods have been
proposed to address this issue, from using multiple
references (Dreyer and Marcu, 2012) to reference-
free evaluation (Specia et al., 2010).

Research in manual evaluation has focused on
overcoming annotator bias, i.e. the preferences
and expectations of individual annotators with re-
spect to translation quality that lead to low levels
of inter-annotator agreement (Cohn and Specia,
2013; Denkowski and Lavie, 2010; Graham et al.,
2013; Guzmán et al., 2015). The problem of ref-
erence bias, however, has not been examined in
previous work. By contrast to automatic MT eval-
uation, monolingual quality assessment is consid-
ered unproblematic, since human annotators are
supposed to recognize meaning-preserving varia-
tions between the MT output and a given human
reference. However, as will be shown in what fol-
lows, manual evaluation is also strongly affected
by biases due to specific reference translations.

3 Settings

To show that monolingual quality assessment de-
pends on the human translation used as gold-
standard, we devised an evaluation task where
annotators were asked to assess the same set of
MT outputs using different references. As control
groups, we have annotators assessing MT using
the same reference, and using the source segments.

3.1 Dataset

MT data with multiple references is rare. We used
MTC-P4 Chinese-English dataset, produced by
Linguistic Data Consortium (LDC2006T04). The
dataset contains 919 source sentences from news
domain, 4 reference translations and MT outputs
generated by 10 translation systems. Human trans-
lations were produced by four teams of profes-
sional translators and included editor’s proofread-

ing. All teams used the same translation guide-
lines, which emphasize faithfulness to the source
sentence as one of the main requirements.

We note that even in such a scenario, human
translations differ from each other. We measured
the average similarity between the four references
in the dataset using the Meteor evaluation met-
ric (Denkowski and Lavie, 2014). Meteor scores
range between 0 and 1 and reflect the proportion
of similar words occurring in similar order. This
metric is normally used to compare the MT out-
put with a human reference, but it can also be ap-
plied to measure similarity between any two trans-
lations. We computed Meteor for all possible com-
binations between the four available references
and took the average score. Even though Me-
teor covers certain amount of acceptable linguis-
tic variation by allowing for synonym and para-
phrase matching, the resulting score is only 0.33,
which shows that, not surprisingly, human transla-
tions vary substantially.

To make the annotation process feasible given
the resources available, we selected a subset of
100 source sentences for the experiment. To en-
sure variable levels of similarity between the MT
and each of the references, we computed sentence-
level Meteor scores for the MT outputs using each
of the references and selected the sentences with
the highest standard deviation between the scores.

3.2 Method

We developed a simple online interface to collect
human judgments. Our evaluation task was based
on the adequacy criterion. Specifically, judges
were asked to estimate how much of the meaning
of the human translation was expressed in the MT
output (see Figure 1). The responses were inter-
preted on a five-point scale, with the labels in Fig-
ure 1 corresponding to numbers from 1 (“None”)
to 5 (“All”).

For the main task, judgments were collected us-
ing English native speakers who volunteered to
participate. They were either professional trans-
lators or researchers with a degree in Computa-
tional Linguistics, English or Translation Stud-
ies. 20 annotators participated in this monolin-
gual task. Each of them evaluated the same set
of 100 MT outputs. Our estimates showed that
the task could be completed in approximately
one hour. The annotators were divided into four
groups, corresponding to the four available refer-

78



Figure 1: Evaluation Interface

ences. Each group contained five annotators in-
dependently evaluating the same set of sentences.
Having multiple annotators in each group allowed
us to minimize the effect of individual annotators’
biases, preferences and expectations.

As a control group, five annotators (native
speakers of English, fluent in Chinese or bilingual
speakers) performed a bilingual evaluation task for
the same MT outputs. In the bilingual task, an-
notators were presented with an MT output and
its corresponding source sentence and asked how
much of the meaning of the source sentence was
expressed in the MT.

In total, we collected 2,500 judgments. Both
the data and the tool for collecting human
judgments are available at https://github.
com/mfomicheva/tradopad.git.

4 Reference Bias

The goal of the experiment is to show that depend-
ing on the reference translation used for evalua-
tion, the quality of the same MT output will be per-
ceived differently. However, we are aware that MT
evaluation is a subjective task. Certain discrepan-
cies between evaluation scores produced by dif-
ferent raters are expected simply because of their
backgrounds, individual perceptions and expecta-
tions regarding translation quality.

To show that some differences are related to
reference bias and not to the bias introduced by
individual annotators, we compare the agreement
between annotators evaluating with the same and
with different references. First, we randomly se-

lect from the data 20 pairs of annotators who used
the same reference translations and 20 pairs of
annotators who used different reference transla-
tions. The agreement is then computed for each
pair. Next, we calculate the average agreement for
the same-reference and different-reference groups.
We repeat the experiment 100 times and report the
corresponding averages and confidence intervals.

Table 1 shows the results in terms of stan-
dard (Cohen, 1960) and linearly weighted (Cohen,
1968) Kappa coefficient (k).1 We also report one-
off version of weighted k, which discards the dis-
agreements unless they are larger than one cate-
gory.

Kappa Diff. ref. Same ref. Source
Standard .163±.01 .197±.01 0.190±.02
Weighted .330±.01 .373±.01 0.336±.02
One-off .597±.01 .662±.01 0.643±.02

Table 1: Inter-annotator agreement for different-
references (Diff. ref.), same-reference (Same ref.)
and source-based evaluation (Source)

As shown in Table 1, the agreement is consis-
tently lower for annotators using different refer-
ences. In other words, the same MT outputs sys-
tematically receive different scores when differ-

1In MT evaluation, agreement is usually computed using
standard k both for ranking different translations and for scor-
ing translations on an interval-level scale. We note, however,
that weighted k is more appropriate for scoring, since it al-
lows the use of weights to describe the closeness of the agree-
ment between categories (Artstein and Poesio, 2008).

79



ent human translations are used for their evalua-
tion. Here and in what follows, the differences
between the results for the same-reference annota-
tor group and different-reference annotator group
were found to be statistically significant with p-
value < 0.01.

The agreement between annotators using the
source sentences is slightly lower than in the
monolingual, same-reference scenario, but it is
higher than in the case of the different-reference
group. This may be an indication that reference-
based evaluation is an easier task for annotators,
perhaps because in this case they are not required
to shift between languages. Nevertheless, the fact
that given a different reference, the same MT out-
puts receive different scores, undermines the reli-
ability of this type of evaluation.

Human score BLEU score
Reference 1 1.980 0.1649
Reference 2 2.342 0.1369
Reference 3 2.562 0.1680
Reference 4 2.740 0.1058

Table 2: Average human scores for the groups of
annotators using different references and BLEU
scores calculated with the corresponding refer-
ences. Human scores range from 1 to 5, while
BLEU scores range from 0 to 1.

In Table 2 we computed average evaluation
scores for each group of annotators. Average
scores vary considerably across groups of anno-
tators. This shows that MT quality is perceived
differently depending on the human translation
used as gold-standard. For the sake of compari-
son, we also present the scores from the widely
used automatic evaluation metric BLEU. Not sur-
prisingly, BLEU scores are also strongly affected
by the reference bias. Below we give an example
of linguistic variation in professional human
translations and its effect on reference-based MT
evaluation.

Src: 不过这一切都由不得你2

MT: But all this is beyond the control of you.
R1: But all this is beyond your control.
R2: However, you cannot choose yourself.
R3: However, not everything is up to you to
decide.

2Literally: “However these all totally beyond the control
of you.”

R4: But you can’t choose that.

Although all the references carry the same mes-
sage, the linguistic means used by the translators
are very different. Most of these references are
high-level paraphrases of what we would consider
a close version of the source sentence. Annota-
tors are expected to recognize meaning-preserving
variation between the MT and any of the refer-
ences. However, the average score for this sen-
tence was 3.4 in case of Reference 1, and 2.0, 2.0
and 2.8 in case of the other three references, re-
spectively, which illustrates the bias introduced by
the reference translation.

5 Time Effect

It is well known that the reliability and consistency
of human annotation tasks is affected by fatigue
(Llorà et al., 2005). In this section we examine
how this factor may gave influenced the evalua-
tion on the impact of reference bias and thus the
reliability of our experiment.

We measured inter-annotator agreement for the
same-reference and different-reference annotators
at different stages of the evaluation process. We
divided the dataset in five sets of sentences based
on the chronological order in which they were an-
notated (0-20, 20-40, ..., 80-100). For each slice
of the data we repeated the procedure reported in
Section 4. Figure 2 shows the results.

First, we note that the agreement is always
higher in the case of same-reference annotators.
Second, in the intermediate stages of the task
we observe the highest inter-annotator agreement
(sentences 20-40) and the smallest difference be-
tween the same-reference and different-reference
annotators (sentences 40-60). This seems to in-
dicate that the effect of reference bias is minimal
half-way through the evaluation process. In other
words, when the annotators are already acquainted
with the task but not yet tired, they are able to
better recognize meaning-preserving variation be-
tween different translation options.

To further investigate how fatigue affects the
evaluation process, we tested the variability of hu-
man scores in different (chronological) slices of
the data. We again divided the data in five sets
of sentences and calculated standard deviation be-
tween the scores in each set. We repeated this pro-
cedure for each annotator and averaged the results.
As can be seen in Figure 3, the variation between

80



0–20 20–40 40–60 60–80 80–100
0.25

0.3

0.35

0.4

Evaluated sentences

A
ve

ra
ge

W
ei

gh
te

d
k

Same reference
Different references

Figure 2: Inter-annotator agreement at different
stages of evaluation process

the scores is lower in the last stages of the evalua-
tion process. This could mean that towards the end
of the task the annotators tend to indiscriminately
give similar scores to any translation, making the
evaluation less informative.

0–20 20–40 40–60 60–80 80–100
0.85

0.9

0.95

1

1.05

1.1

Evaluated sentences

σ

Figure 3: Average standard deviations between
human scores for all annotators at different stages
of evaluation process

6 Conclusions

In this work we examined the effect of reference
bias on monolingual MT evaluation. We com-
pared the agreement between the annotators who
used the same human reference translation and
those who used different reference translations.
We were able to show that in addition to the in-
evitable bias introduced by different annotators,
monolingual evaluation is systematically affected

by the reference provided. Annotators consistently
assign different scores to the same MT outputs
when a different human translation is used as gold-
standard. The MTs that are correct but happen
to be different from a particular human translation
are inadvertently penalized during evaluation.

We also analyzed the relation between reference
bias and annotation at different times throughout
the process. The results suggest that annotators
are less influenced by specific translation choices
present in the reference in the intermediate stages
of the evaluation process, when they have already
familiarized themselves with the task but are not
yet fatigued by it. To reduce the fatigue effect, the
task may be done in smaller batches over time. Re-
garding the lack of experience, annotators should
receive previous training.

Quality assessment is instrumental in the devel-
opment and deployment of MT systems. If evalua-
tion is to be objective and informative, its purpose
must be clearly defined. The same sentence can
be translated in many different ways. Using a hu-
man reference as a proxy for the source sentence,
we evaluate the similarity of the MT to a partic-
ular reference, which does not necessarily reflect
how well the contents of the original is expressed
in the MT or how suitable it is for a given pur-
pose. Therefore, monolingual evaluation under-
mines the reliability of quality assessment. We
recommend that unless the evaluation is aimed for
a very specific translation task, where the number
of possible translations is indeed limited, the as-
sessment should be performed by comparing MT
to the original text.

Acknowledgments

Marina Fomicheva was supported by funding from
IULA (UPF) and the FI-DGR grant program of the
Generalitat de Catalunya. Lucia Specia was sup-
ported by the QT21 project (H2020 No. 645452).
The authors would also like to thank the three
anonymous reviewers for their helpful comments
and suggestions.

References
Ron Artstein and Massimo Poesio. 2008. Inter-coder

Agreement for Computational Linguistics. Compu-
tational Linguistics, 34(4):555–596.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Barry Haddow, Matthias Huck, Chris Hokamp,
Philipp Koehn, Varvara Logacheva, Christof Monz,

81



Matteo Negri, Matt Post, Carolina Scarton, Lucia
Specia, and Marco Turchi. 2015. Findings of the
2015 Workshop on Statistical Machine Translation.
In Proceedings of the Tenth Workshop on Statistical
Machine Translation, pages 1–46, Lisboa, Portugal.

Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 136–158.

Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20:37–46.

Jacob Cohen. 1968. Weighted Kappa: Nominal Scale
Agreement Provision for Scaled Disagreement or
Partial Credit. Psychological bulletin, 70(4):213–
220.

Trevor Cohn and Lucia Specia. 2013. Modelling
Annotator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality Es-
timation. In Proceedings of 51st Annual Meeting
of the Association for Computational Linguistics,
pages 32–42.

Michael Denkowski and Alon Lavie. 2010. Choos-
ing the Right Evaluation for Machine Translation:
an Examination of Annotator and Automatic Metric
Performance on Human Judgment Tasks. In Pro-
ceedings of the Ninth Biennal Conference of the As-
sociation for Machine Translation in the Americas.

Michael Denkowski and Alon Lavie. 2014. Meteor
Universal: Language Specific Translation Evalua-
tion for Any Target Language. In Proceedings of the
EACL 2014 Workshop on Statistical Machine Trans-
lation, pages 376–380.

Markus Dreyer and Daniel Marcu. 2012. HyTER:
Meaning-equivalent Semantics for Translation Eval-
uation. In Proceedings of 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 162–171.

Yvette Graham, Timothy Baldwin, Alistair Moffat,
and Justin Zobel. 2013. Continuous Measurement
Scales in Human Evaluation of Machine Trans-
lation. In Proceedings 7th Linguistic Annota-
tion Workshop and Interoperability with Discourse,
pages 33–41.

Francisco Guzmán, Ahmed Abdelali, Irina Temnikova,
Hassan Sajjad, and Stephan Vogel. 2015. How do
Humans Evaluate Machine Translation. In Proceed-
ings of the Tenth Workshop on Statistical Machine
Translation, pages 457–466.

Xavier Llorà, Kumara Sastry, David E Goldberg, Ab-
himanyu Gupta, and Lalitha Lakshmi. 2005. Com-
bating User Fatigue in iGAs: Partial Ordering, Sup-
port Vector Machines, and Synthetic Fitness. In Pro-
ceedings of the 7th Annual Conference on Genetic
and Evolutionary Computation, pages 1363–1370.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311–
318.

Mark Przybocki, Kay Peterson, and Sebastian Bron-
sart. 2008. Official Results of the NIST 2008 “Met-
rics for MAchine TRanslation” Challenge (Metrics-
MATR08). In Proceedings of the AMTA-2008 Work-
shop on Metrics for Machine Translation, Honolulu,
Hawaii, USA.

Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation versus Quality Es-
timation. Machine Translation, 24(1):39–50.

John White, Theresa O’Connell, and Francis O’Mara.
1994. The ARPA MT Evaluation Methodologies:
Evolution, Lessons, and Future Approaches. In Pro-
ceedings of the Association for Machine Transla-
tion in the Americas Conference, pages 193–205,
Columbia, Maryland, USA.

82


