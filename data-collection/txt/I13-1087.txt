










































Learning to Generate Diversified Query Interpretations using Biconvex Optimization


International Joint Conference on Natural Language Processing, pages 733–739,
Nagoya, Japan, 14-18 October 2013.

Learning to Generate Diversified Query Interpretations using Biconvex
Optimization

Ramakrishna B Bairi
IITB-Monash Research Academy

IIT Bombay
Mumbai, India, 400076

bairi@cse.iitb.ac.in

Ambha A
IIT Bombay

Mumbai, India, 400076
ambha.career
@gmail.com

Ganesh Ramakrishnan
IIT Bombay

Mumbai, India, 400076
ganesh@cse.iitb.ac.in

Abstract

The wealth of information present in the
World Wide Web has made internet search
a de-facto medium for obtaining any re-
quired information. Users typically spec-
ify short and/or ambiguous queries and
expect the answer to appear at the top.
Hence, it can be extremely important to
produce a diverse but relevant set of results
in the precious top k positions. This calls
for addressing two types of needs: (i) pro-
ducing relevant results for queries that are
often short and ambiguous and (ii) select-
ing a set of k diverse results to satisfy dif-
ferent classes of information needs. In this
paper, we present a novel technique using
a Biconvex optimization formulation as
well as adaptations of existing techniques
from other areas, for addressing these two
problems simultaneously. We propose a
graph based iterative method to choose di-
versified results. We evaluate these ap-
proaches on the QRU (Query Represen-
tation and Understanding) dataset used in
SIGIR 2011 workshop as well as on the
AMBIENT (Ambiguous Entities) dataset
and present results on generating diversi-
fied query interpretations. We also com-
pare these approaches against other on-
line systems such as Surf Canyon, Car-
rot2, Exalead and DBpedia and empir-
ically demonstrate that our system pro-
duces competitive results.

1 Introduction

The growth of internet has resulted in the prolifer-
ation of electronic documents on the World Wide
Web. Every search engine, be it generic or applica-
tion and domain specific, serves as a portal to ac-
cess these documents. User queries, in general, are

short and often tend to be ambiguous and/or under-
specified. In addition, a query can have multi-
ple concealed interpretations. For example, Sun
could be interpreted as “The sun as a star”, “Com-
position of Sun”, “Sun Micro systems company”,
“Sun news paper”, “Sun Record music company”,
and so on. We believe that, in addition to these
concealed interpretations, related interpretations
are also equally important. As examples, “Solar
Cells” and “Photosynthesis”, could be interpreta-
tions related to this query. To improve user in-
teraction and to guide him/her in further refining
the query, it could help if the search engine gener-
ated these relevant interpretations as well. Due to
the sheer size of online information and its diver-
sity, the possible interpretations to a short query
are enormous. In addition, users expect their in-
tended answer to be present in the top few search
results. This calls for presenting a diversified but
relevant set of results in the top k positions. Note
that, in this paper we consider the diverse search
results produced by the search system as interpre-
tations of the query in some sense. In addition, we
consider each search result is a document describ-
ing some aspect related to the query. Hence we
restrict our notion of interpretation to each such
document in the search result.

We present an original method as well as adap-
tations of some existing methods to solve this
problem. As for our proposed method, we con-
struct an interpretation graph with potential inter-
pretations as its nodes and edges indicating their
similarity. Inspired by the works on GCD (Dubey
et al., 2011) and MMR (Carbonell and Goldstein,
1998), we develop a new technique for diversity
ranking of interpretations. As part of this tech-
nique, we propose an algorithm (Rel-Div) to learn
the node and edge weights of the interpretation
graph iteratively by solving a biconvex optimiza-
tion (Gorski et al., 2007) problem. At query time,
we solve a convex optimisation problem to choose

733



k diverse nodes and present them as interpretations
to the user query. We identify interpretations rel-
evant to the query using a publicly available in-
ternet encyclopedia. Though we used Wikipedia
as the source, we believe that the repository can
be easily extended to accommodate other catalogs
like YAGO and Freebase.

We compare our diversification approach with
other diversification approaches (which were ap-
plied not necessarily to solve the same prob-
lem as ours) such as variants of GCD (Dubey et
al., 2011), Affinity Propagation (Frey and Dueck,
2006),(Frey and Dueck, 2007). We evaluated re-
sults on benchmark queries from the SIGIR 2011
workshop’s QRU (Query Representation and Un-
derstanding) dataset and the AMBIENT data sets.
In addition, we compare the diversity of interpreta-
tions generated by these approaches against those
of other online systems such as Surf Canyon, Car-
rot2, Exalead and DBpedia (URLs of all these sys-
tems listed under References)

We summarize our contributions as: 1) Top-K
diversity ranking using a graph based approach. 2)
Iterative Graph weight learning technique - A new
iterative technique for learning the node and edge
weights for an interpretation graph by solving a
biconvex optimisation problem.

The rest of the paper is organized as follows:
In Section 2 we present related work. In Sec-
tion 3 we describe our technique of iterative graph
weight learning and diversity ranking. In Section 4
we demonstrate the utility of our technique by ap-
plying it to the interpretation generation task from
Wikipedia. In Section 5, we present experimental
evaluations. We conclude our work in the subse-
quent section.

2 Prior work

Most of the prior research has focused on generat-
ing diversified result urls. The approach presented
by (Swaminathan et al., 2009) filters initial search
results and covers diversified topics based on bag
of words measures. Yisong and Joachims (Yue and
Joachims, 2008) train a model using Struct SVM
and encode diversity as a penalty function (this is
penalty for not covering certain topics). Most re-
cently, Brandt et al.(Brandt et al., 2011) and Ra-
man et. al. (Raman et al., 2011) proposed an ap-
proach for dynamic ranking and then group URLs
with similar intentions.(Dubey et al., 2011) formu-
late the problem of ensuring diversity as that of

identifying relevant urls which are most likely to
be visited by the random surfer. We propose a new
approach for interpretation generation. The report
(Hearst, 2006) by M.A Hearst claims that cluster-
ing based on similarity measure may not always
result in meaningful interpretations or labels. So,
instead of dynamically generating labels, we pick
labels or relevant interpretations for a query from
the pool of labels. We use Wikipedia as a primary
source to capture these interactions along with
their semantic relations. (Hahn et al., 2010),(Ben-
Yitzhak et al., 2008) produce Wikipedia pages as
search results and align the search results along a
set of fine grained attributes/facets. In our work,
facets (which we refer to as interpretations) are
neither predefined nor necessarily fine grained.
Moreover, as we will see, our interpretations need
not be restricted to Wikipedia entities. Closest to
our approach is the approach of (Ma et al., 2010).
They apply page ranking technique on the graph
constructed using query log statistics to obtain di-
versified interactions.

3 Diversified Interpretation Generation

3.1 Our Problem

Given a large corpus U of documents and a short
user query q, we define a functionH(q, U) that re-
turns a subset of documents S = {e1...en} ⊆ U ,
satisfying the query q. The function H(q, U) acts
as a filtering function to retrieve the documents S
that are syntactically and/or semantically related
to the query q. In its simplest form, H(q, U)
can just return U without performing any filtering,
which is not generally useful. It is important to
design an H (q, U) (e.g., keyword based lookup,
semantics matching, etc.) that can help reduce the
search space in a meaningful manner. Our goal is
to choose a set of k documents from S and we as-
sume that to best satisfy the user intention, these k
documents presented to the user should be diverse
yet highly relevant to the query q.

3.2 The Training Algorithm

We expect groups of documents in S to be related
to each other via some semantic relations. We ini-
tially construct a document-relation graph using
e1...en. We refer to this graph as an Interpreta-
tion Graph, since the documents in this graph are
obtained as various interpretations of the query.
While the nodes are documents from S, each edge
is a relation between the documents. A relation

734



could be one of synonymy, hyponymy, meronymy,
homonymy, etc.. These relations could be ob-
tained from external catalogs such as Wikipedia,
Wordnet, etc.

Each node in the graph is assigned a score
which represents the relevance of the node to the
query. We use the notation bq to represent the
column vector (of size n × 1) containing all the
node relevance scores. The weight on an edge rep-
resents the degree of similarity between the two
nodes connected by that edge. We use the nota-
tion Cq (of size n × n) to represent the matrix of
edge scores reflecting similarity between pairs of
nodes. Note that, each column Ciq of the matrix
Cq represents an document ei and the cell values
in that column indicate the similarity of document
ei with other documents. The scores in bq are used
to ensure that the subset of k interpretations are
relevant to q, whereas the similarity scores in Cq
are used to ensure diversity in the subset of k in-
terpretations.

We assume that we are provided training data,
consisting of queries and their correct interpreta-
tions. Our goals in training are to 1) develop a
model for the node score bq, 2) develop a model
for the edge potentials Cq and 3) learn parame-
ters of these models such that the set of k relevant
yet diverse nodes obtained from the graph using bq
and Cq are consistent with the training data. Thus,
implicit in our third goal is the following subprob-
lem, which is also our query time inference prob-
lem: 4) compute a subset of k best interpretations
using bq and Cq, that represent k diverse, but rel-
evant interpretations. A part of the graph for the
query "sun" is depicted in Figure 1

Sun light 

Sun 
Microsystems Sun Record 

Sun News 

Photosynthesis 
Solar Battery 

Sun Burn 

Solar Power 

Plantation 

Hot Summer 

0.90 
0.925 

0.725 

0.6 

0.825 
0.725 

0.825 
0.25 

0.025 0.025 

0.05 

0.20 

(0.95) 

(0.95) 

(0.95) 

(0.95) 

(0.95) 

(0.95) 

(0.95) 

(0.95) 

(0.95) 

(0.95) 

Top K Interpretations Generated Interpretations 

Figure 1: Interpretation Graph for the query Sun

3.2.1 Modeling node potentials (bq)
In order to build a learning model for bq, it is im-
portant to define a good set of features that char-
acterize the node’s relevance to the query. Let
N1..|N | (q, S) be a set of |N | query independent
node features. Each feature Nf (q, S) evaluates
the relevance of documents in S to the query q and
returns a vector of scores. These feature functions
are problem specific and crafted carefully to bring

out the relevance between query and documents
(such as term overlaps, n-gram matches, etc). In
Section 4 we provide some practical examples of
node features.

The node potential vector bq is obtained by
combining the scores returned by individual fea-
ture functions Nf (q, S). One of the obvious
choices is to use Logistic Regression (Yan et al.,
2003).i.e. bq [i] = 1

1+e
−
∑|N|
f=1

wfNf (q,S)[i]
. The

weight vectorW T =
[
w1...w|N |

]
is learnt through

supervised training explained in Section 3.2.3.

3.2.2 Modeling edge potentials (Cq)
To learn the edge potentials, it is important to de-
fine a good set of features that measure the similar-
ities between every pair of nodes and return sim-
ilarity scores. Higher the score, more similar are
the nodes. Let C1..|C| (S) be the set of |C| edge
features that evaluate similarities between docu-
ments in S and each returns a n × n matrix of
scores. These feature functions are problem spe-
cific and crafted carefully to bring out the simi-
larities between the documents. In Section 4 we
provide some practical examples of edge feature
construction using Wikipedia.

The edge potential matrix Cq is obtained as
Cq =

∑|C|
f=1 λfCf (S) where 0 ≤ λf ≤ 1

and
∑
λf ≥ 1 ∀f . The weight vector λT =[

λ1...λ|C|
]

is learnt through supervised training
explained in Section 3.2.3.

3.2.3 Learning feature weights W T , λT

Proposition 1:

bq ≈
k∑

j=1

C̃
ij
q (1)

for sufficiently large k diverse documents,
where, C̃q is the matrixCqwith the columns scaled
so that the diagonal cell values match the relevance
value, i.e., C̃q(i, i) = bq(i). The values i1...ik rep-
resent indices of k columns of matrix C̃q. Hence,
C̃

ij
q is the ij th column of matrix C̃q.
The intuition behind this approximated equal-

ity comes from the fact that, two similar docu-
ments should have similar relevance score with
the query and we are interested in selecting k di-
verse documents. Let ei be one of these k diverse
documents. If the documents ej1 ...ejp are similar
to ei, then, bq [i] ≈ bq [j1] ≈ ... ≈ bq [jp] and
Cq [i, i] ≈ Cq [i, j1] ≈ ... ≈ Cq [i, jp] ≈ 1 and
Cq [t] ≈ 0, t /∈ j1...jp. But, we already know that
C̃q [i, i] = bq [i]. That implies, bq [j1] ≈ C̃q [i, j1],

735



bq [j2] ≈ C̃q [i, j2], ... bq [jp] ≈ C̃q [i, jp]. When
we take the summation on all diverse k docu-
ments, the Equation 1 holds.

Based on the above proposition, we present an
algorithm to learn weights W T and λT iteratively
in a supervised learning setup. The training data is
provided in a vector rq (of size n × 1) such that
rq [i] = 1 if the document ei is relevant to the
query (and one of diverse documents), otherwise,
rq [i] = 0. Note that, the quantity C̃qrq represents
the sum of k columns (assuming k number of 1s
in rq) and is the RHS of Equation 1.

Our training objective is to learn λT and W T

such that Equation 1 holds. Formally, the problem
being solved is:

argmin
λ1...λ|C|,w1...w|N|

D

 1
1 + e−

∑
g wgNg

,
∑
f

λf C̃frq


(2)

where D (x, y) is a distance measure between x
and y. (for e.g., KL Divergence, Euclidean, etc.);
C̃f is the normalized Cf as in Proposition 1.

Applying the coordinate descent technique, we
learn the weightsW T and λT iteratively using two
steps outlined in Equation 3 and Equation 4, each
of them convex in the respective optimization
variables, hence pur optimisation problem is
biconvex.
div-step: Learn λ(t)1 , λ

(t)
2 , ... holding w

(t−1)
1 , w

(t−1)
2 , ... con-

stant, by solving:

argmin
λ1,λ2,...

D

 1
1 + e−

∑
g w

(t−1)
g Ng

,
∑
f

λ
(t)
f C̃frq

 (3)
rel-step: Learn w(t)1 , w

(t)
2 , ... holding λ

(t−1)
1 , λ

(t−1)
2 , ... con-

stant, by solving:

argmin
w1,w2,...

D

 1
1 + e−

∑
g w

(t)
g Ng

,
∑
f

λ
(t−1)
f C̃frq

 (4)
In div-step, we learn λT by holding W T fixed

and honoring Equation 1. In rel-step, we learn
W T by holding λT fixed. The relevance and di-
vergence is enforced during training through the
vector rq.

We learn node and edge feature weights it-
eratively by recognizing and assigning weights
to prominent node and edge features that satisfy
queries of different types. Having all statistically
driven computation of weights for edge features
can minimize the side effect of poor node features
and likewise computing weights for node features
can decrease the consequences of poor edge fea-
tures.

Algorithm 1 outlines the training procedure.
I+q , I

−
q are the set of relevant and irrelevant doc-

uments for each query q in the ground truth that is
used for training.

3.3 Query-time Inference
For a new user query q, inference problem is to
choose k diversified results. Using H (q, C) we
reduce the search space drastically and get the set
S. Otherwise, we need to run our inference on
entire set U , which is very expensive. We then
compute the node and edge feature matrices for all
defined node and edge features. These individual
feature matrices are then combined (using λT and
W T ) to obtain vector bq and matrix Cq. Based on
Proposition 1, our inference objective is to choose
k columns from the matrix C̃q such that their sum
is as close as possible to bq. Formally, the problem
being solved is:

argmin
i1...ik

D

bq, k∑
j=1

C̃
ij
q

 (5)
where i1...ik are indices of k columns of C̃q.

Determining the exact solution (i.e. i1...ik
columns) to the above optimization problem turns
out to be computationally infeasible. Hence, we
have to resort to an approximate solution. Algo-
rithm 2 describes a greedy inference procedure. At
each step we pick one column from C̃q that mini-
mizes the distance in Equation 5 most. However,
we also ensure that the picked column is most di-
verse from the already selected columns in the pre-
vious steps. At the end of k steps we will have k
diverse, but relevant documents.
Algorithm 1 Training

1: Input: Set of training data instances{
q, I+q , I

−
q , Nf , Cf , rq

}
2: Output: WT and λT
3: initialize variables WT and λT

4: learn initial WT using Logistic Regression . uses{
q, I+q , I

−
q Nf

}
. C̃q, C̃f used below are normalized Cq, Cf as

in Proposition 1
5: while not converged(| bq − C̃qrq |) do
6: bq= compute relevance matrix using WT and I+q
7: find λT so that D

(
bq,
∑
f λf C̃frq

)
is minimized

. WT is fixed
8: pq =

∑
f λf C̃frq

9: find WT so that D
(

1

1+e
−
∑

f wf Nf
, pq
)

is mini-
mized

. λT is fixed
10: end while

return
(
WT , λT

)

736



Semantic relations and values from Wikipedia page excerpts
1. Synonym: All redirected names of the Wikipedia page.
2. Association: All valid hyperlinks of a Wikipedia page.
3. Frequent: All phrases occurring more than two times within a Wikipedia
page section.
4. Synopsis: All nouns, verbs, adjectives from the abstract and titles of the
sections in a Wikipedia page
5. Hyponym: All pages/sub categories of selected categories ending with
Wikipedia page title. Ex: For Sony: robotics at Sony.
6. Meronym: All phrases which occur both in wordnet meronyms and with
in Wikipedia pages.
7. Hypernym: All parent categories of selected categories.
8. Homonym: Pages referring to one or more disambiguation page.
9. Sibling: Siblings are the sub categories/pages which do not follow
hyponym pattern. Ex: For Sony: list of sony trademarks

Table 1: Semantic Relations
Algorithm 2 Inference

1: Input: User query q, Corpus U , λT , WT , Nf ,Cf
2: Output: k diverse interpretations
3: Generate S = H (q, C) and build a graph using docu-

ments in S = {e1, .., en}
4: Compute bq usingWT and node featuresN1..|N| (q, S)
5: Compute Cq =

∑
f λfCf (S) and normalize as in

Proposition 1
6: R = {ï¿œ} . set of selected indices
7: Q = {i1, .., in} . indices to select
8: for i = 1 to k do
9: argmin

ck∈Q/R

{
D

bq, ∑
r∈R∪{ck}

(
Crq

)×
(

1− 1
Z
min

(
D
(
C

R1
q , C

ck
q

)
, ..., D

(
C

R|R|
q , C

ck
q

)))}
. (query match)× (dissimilar to selected), z is normal-
izer

10: R = R ∪ {ck}
11: end for

return k interpretations representing k columns
R1, ..., R|R|

4 An example using Wikipedia

In this section we apply our Rel-Div technique
to generate diverse but relevant results to a short
and/or ambiguous user query using Wikipedia.
For e.g. Beagle, Laptop Charger, Sony Camera,
etc. We do not support queries which are highly
rich in semantics like Who invented music, Earn
money at home or very specific in nature like DB2
error code 1064.

In this case, U is a set of all Wikipedia enti-
ties (a.k.a. pages/articles). Note, in the context
of Wikipedia, every document is treated as an en-
tity. We defined H (q, U) as a set of filters which
return Wikipedia entities S, called candidate inter-
pretations, relevant to the user query. In order to
build this filter function, we made use of promi-
nent Wikipedia attributes (Title, Infobox entries,
Frequently occurring words, etc) and different se-
mantic relations between Wikipedia entities (As-
sociation via hyperlinks, Page Redirects, See Also
links, etc). Table 1 summarizes these Wikipedia
signals, which are captured for every entity.

4.1 Node Features

Query Match: Calculates the term overlap be-
tween query terms and the semantic relation terms
of an interpretation. For e.g., for the query Sony,
PlayStation 2 is one of the interpretations, which
has multiple occurrence of term Sony in one or
more semantic relations.
No. of Semantic Relation match: Total num-
ber of semantic relations that contain the query
terms. For e.g., for the query Sony, PlayStation 2
interpretation may have 3 semantic relations (Syn-
onym, Association and Frequent) containing term
Sony.
Title score: Captures the interpretation title match
to the query terms.

4.2 Edge Features

Interpretation Content Overlap: This feature
measures the similarity between two interpreta-
tions by considering the amount of overlap be-
tween the words in these interpretation title and
content.
Decaying Recursive Similarity: We consid-
ered neighborhood of an interpretation (hyper-
linked entities, parent categories, subcategories,
and grand parent pages) in the similarity measure-
ment. However, an appropriate weight which de-
cays with distance is set to avoid influence of far-
ther neighborhood nodes.
Link based proximity: Determined by the depth
D(lca) of the least common ancestor (LCA) of in-
terpretations Ii and Ij from the root of Wikipedia
category structure and the hop distance len(Ii, Ij)
from Ii to Ij through LCA. Link proximity is de-
fined as LP (Ii, Ij) ∝ D(lca) ∗ len (Ii, Ij). When
multiple LCAs exist, we define the proximity as
max(LP (Ii, Ij)).

5 Experimental Evaluation

We used Wikipedia as our knowledge source. We
captured different signals shown in Table 1 for ev-
ery Wikipedia entity.

5.1 Dataset

The QRU dataset used in SIGIR 2011 contains 100
TREC queries with various interpretations. We re-
stricted our space of interpretations to Wikipedia
entities. We also experimented with ambiguous
queries from the AMBIENT dataset which con-
tains 40 one word queries.

737



 10

 20

 30

 40

 50

 60

 70

 80

 90

 100

2 3 4 5 10 15

P
re

ci
si

on
 (

%
)

Top K

Precision Comparison (TREC Dataset)

Rel-Div
Carrot2

SurfCanyon
ExaLead
DBpedia

(a) Precision : TREC

 0

 10

 20

 30

 40

 50

 60

 70

2 3 4 5 10 15

N
D

C
G

-I
A

 (
%

)

Top K

NDCG-IA Comparison (TREC Dataset)

Rel-Div
Carrot2

SurfCanyon
ExaLead
DBpedia

(b) NDCG-IA : TREC

 10

 20

 30

 40

 50

 60

 70

 80

 90

 100

2 3 4 5 10 15

P
re

ci
si

on
 (

%
)

Top K

Precision Comparison (AMBIENT Dataset)

Rel-Div
Carrot2

SurfCanyon
ExaLead
DBpedia

(c) precision : AMBIENT

 0

 10

 20

 30

 40

 50

 60

 70

2 3 4 5 10 15

N
D

C
G

-I
A

 (
%

)

Top K

NDCG-IA Comparison (AMBIENT Dataset)

Rel-Div
Carrot2

SurfCanyon
ExaLead
DBpedia

(d) NDCG-IA : AMBIENT
Figure 2: Comparison with external systems

 @5  @10  @10  @5  @10  @10  @5  @10  @10

Rel-Div 91.13 89.93 89.83 7.02 13.85 20.4 48.9 59.47 69.7

M-Div 89.87 84.27 84.32 6.74 12.71 18.88 49.71 62.68 67.39

M-Div-NI 83.75 80 80 6.83 12.81 19.35 42.48 60.88 66.52

AFP 78.3 76.9 80.7 6.3 12.4 18.1 34.2 38.8 47.6

Rel-Div 96.05 92.3 90.67 7.33 14.57 21.61 32.12 48.72 63.1

M-Div 96.15 94.15 93.56 7.43 14.37 21.61 32.41 47.49 58.09

M-Div-NI 96.2 93.58 93.19 7.33 13.87 21.11 22.93 43.59 55.84

AFP 88.4 90.9 92.3 6.9 13.6 21.47 32.09 45.9 55.1

 

 

 

 

Precision(%) Recall(%) NDCG-IA(%)

TR
EC

A
M

B
IE

N
T

Table 2: Results of different approaches
5.2 Evaluation methodology
Manually, interpretations for each query are
marked as relevant or irrelevant and each interpre-
tation is assigned one or more topics. The system
is trained on 30and tested on the rest. We evalu-
ated results on queries of length one or two. The
relevance of any interpretation to the query is mea-
sured using precision at different positions and the
diversity is estimated using NDCG-IA (Agrawal
et al., 2009). Recall measurement is tricky. It
is practically not possible to manually inspect all
Wikipedia entities and determine how many are
actually relevant for a query. Hence we based our
recall on the candidate interpretations generated.
We manually counted number of relevant interpre-
tations present in the candidate interpretations and
measured how many of these relevant interpreta-
tions appeared in the top k interpretations.

In our experiments, we also consider a couple
of other approaches to diversification, which have
been reported in literature, though used in other
problem settings. These include variants of GCD
and affinity propagation (Frey and Dueck, 2006;
Frey and Dueck, 2007).
M-Div : Uses page rank matrix M as in GCD in-
stead of the Cq matrix.
M-Div-NI : Similar to M-Div, but node and edge
weights are learnt independently, without any iter-

ations. This acts as GCD implementation.
AFP:Exemplar nodes of Affinity propagation are
taken as interpretations.

5.3 Comparison with other approaches

While experimenting with our proposed approach,
we found best performance when D in div-step
was chosen to be KL-divergence and D in rel-
step was chosen as the Euclidean distance. In
Table 2, we compare the proposed diversification
algorithm against M-Div, M-Div-NI and AFP on
precision, recall and NDCG-IA measures.

We observed that our Ranking algorithm Rel-
Div performs at par with (and sometimes even bet-
ter than) M-Div and M-Div-NI. However, one of
the major advantage of our method compared to
M-Div and M-Div-NI is that, we need not cal-
culate the inverse of Cq matrix, which is a com-
putationally intensive process for a large dimen-
sion matrices. We conclude from the results that
the Rel-Div performs consistently better than other
approaches when both relevance and diversifica-
tion are considered across all types of queries.

5.4 Comparison against other systems

We compare the diversity in search result using our
approach against those from four other systems,
viz., carrot2, SurfCanyon, Exalead and DBPedia
to demonstrate that the Rel-Div approach produces
high diversity in the search results, which is evi-
dent from the Figure 2.

6 Conclusion

We presented a body of techniques for generating
top k interpretations to a user query using some in-
ternet encyclopedia, (in particular, Wikipedia was
used in the experiments that were reported). Our
approach is hinged on catering to two needs of
the user, viz., that all the interpretations are rel-
evant and that they are as diverse as possible.
We addressed this using a bunch of node features
and edge features based on semantic relations and
learn these feature weights together iteratively. We
present experimental evaluations and find that our
approach performs well on both the fronts (diver-
sity and relevance) in comparison to existing tech-
niques and publicly accessible systems. We be-
lieve technique can be improved for better han-
dling of multiword queries by adopting deep NLP
parsing techniques, which will form part of our fu-
ture work.

738



References
Rakesh Agrawal, Sreenivas Gollapudi, Alan Halver-

son, and Samuel Ieong. 2009. Diversifying search
results. In Proceedings of the Second ACM Inter-
national Conference on Web Search and Data Min-
ing, WSDM ’09, pages 5–14, New York, NY, USA.
ACM.

Ori Ben-Yitzhak, Nadav Golbandi, Nadav Har’El,
Ronny Lempel, Andreas Neumann, Shila Ofek-
Koifman, Dafna Sheinwald, Eugene Shekita, Ben-
jamin Sznajder, and Sivan Yogev. 2008. Beyond
basic faceted search. In Proceedings of the 2008
International Conference on Web Search and Data
Mining, WSDM ’08, pages 33–44, New York, NY,
USA. ACM.

Christina Brandt, Thorsten Joachims, Yisong Yue, and
Jacob Bank. 2011. Dynamic ranked retrieval. In
Proceedings of the fourth ACM international con-
ference on Web search and data mining, WSDM ’11,
pages 247–256, New York, NY, USA. ACM.

Surf Canyon. http://www.surfcanyon.com/.

Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of the 21st annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ’98, pages 335–336, New York,
NY, USA. ACM.

Carrot2. http://search.carrot2.org/stable/search.

DBPedia. http://dbpedia.org/facetedsearch.

Avinava Dubey, Soumen Chakrabarti, and Chiranjib
Bhattacharyya. 2011. Diversity in ranking via re-
sistive graph centers. In Proceedings of the 17th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ’11, pages
78–86, New York, NY, USA. ACM.

Exalead. http://www.exalead.com/search/.

Freebase. http://www.freebase.com/.

Brendan Frey and Delbert Dueck. 2006. Mixture mod-
eling by affinity propagation. In Advances in Neural
Information Processing Systems 18, pages 379–386.
MIT Press, Cambridge, MA.

Brendan J. Frey and Delbert Dueck. 2007. Clustering
by passing messages between data points. Science,
315:2007.

Jochen Gorski, Frank Pfeuffer, and Kathrin Klamroth.
2007. Biconvex sets and optimization with biconvex
functions: a survey and extensions. Math. Meth. of
OR, 66(3):373–407.

Rasmus Hahn, Christian Bizer, Christopher Sahnwaldt,
Christian Herta, Scott Robinson, Michaela BÃŒr-
gle, Holger DÃŒwiger, and Ulrich Scheel. 2010.
Faceted wikipedia search. In Witold Abramowicz

and Robert Tolksdorf, editors, Business Information
Systems, volume 47 of Lecture Notes in Business In-
formation Processing, pages 1–11. Springer Berlin
Heidelberg.

Marti A. Hearst. 2006. Clustering versus faceted cate-
gories for information exploration. Commun. ACM,
49(4):59–61, April.

Hao Ma, Michael R. Lyu, and Irwin King. 2010. Di-
versifying query suggestion results. In AAAI.

Karthik Raman, Thorsten Joachims, and Pannaga Shiv-
aswamy. 2011. Structured learning of two-level dy-
namic rankings. In Proceedings of the 20th ACM
international conference on Information and knowl-
edge management, CIKM ’11, pages 291–296, New
York, NY, USA. ACM.

Ashwin Swaminathan, Cherian V. Mathew, and Darko
Kirovski. 2009. Essential pages. In Proceedings of
the 2009 IEEE/WIC/ACM International Joint Con-
ference on Web Intelligence and Intelligent Agent
Technology - Volume 01, WI-IAT ’09, pages 173–
182, Washington, DC, USA. IEEE Computer Soci-
ety.

YAGO. http://www.mpi-inf.mpg.de/yago-naga/.

Lian Yan, Robert H. Dodier, Michael Mozer, and
Richard H. Wolniewicz. 2003. Optimizing classifier
performance via an approximation to the wilcoxon-
mann-whitney statistic. In ICML, pages 848–855.

Yisong Yue and Thorsten Joachims. 2008. Predicting
diverse subsets using structural svms. In Proceed-
ings of the 25th international conference on Ma-
chine learning, ICML ’08, pages 1224–1231, New
York, NY, USA. ACM.

739


