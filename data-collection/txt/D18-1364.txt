



















































Implicational Universals in Stochastic Constraint-Based Phonology


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3265–3274
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

3265

Implicational Universals in Stochastic Constraint-Based Phonology

Giorgio Magri
CNRS / University of Paris 8
magrigrg@gmail.com

Abstract
This paper focuses on the most basic implica-
tional universals in phonological theory, called
T-orders after Anttila and Andrus (2006). It
shows that the T-orders predicted by stochas-
tic and categorical Optimality Theory coin-
cide. Analogously, the T-orders predicted by
stochastic and categorical Harmonic Gram-
mar coincide. In other words, these stochas-
tic constraint-based frameworks do not tamper
with the typological structure induced by the
corresponding categorical frameworks.

1 Introduction

Phonology has traditionally focused on alterna-
tions revealed by paradigms such as the German fi-
nal devoicing examples [ba:t]/[bE:d@] (‘bath-SG/PL’)
and [tsu:k]/[tsy:g@] (‘train-SG/PL’). These alterna-
tions are usually modeled through phonological
grammars which map from underlying represen-
tations (URs) to surface representations (SRs)
(Chomsky and Halle, 1968). Constraint-based im-
plementations of this combinatorial phonological
theory include Optimality Theory (OT; Prince and
Smolensky, 1997, 2004) and Harmonic Grammar
(HG; Legendre et al., 1990; Smolensky and Leg-
endre, 2006), reviewed below in sections 4 and 5.

More recently, phonology has extended its em-
pirical coverage from categorical alternations to
patterns of phonologically conditioned variation
and gradient phonological (or phonotactic) judge-
ments (see for instance Anttila, 2012 and Coetzee
and Pater, 2011). This extension of the empiri-
cal coverage has required a corresponding exten-
sion of the theoretical framework. A phonological
grammar cannot be construed anymore as a cate-
gorical function from URs to SRs. Instead, it must
be construed as a function from URs to probability
distributions over the entire set of SRs. Constraint-
based implementations of this stochastic theory in-
clude partial order OT (Anttila, 1997b), stochastic

OT (SOT; Boersma, 1997, 1998), and stochastic
HG (SHG; Boersma and Pater, 2016)1, recalled
below in sections 4 and 5. Another framework
explored in the recent literature on probabilis-
tic constraint-based phonology is MaxEnt (ME;
Goldwater and Johnson, 2003; Hayes and Wilson,
2008). Its T-orders are discussed in a companion
paper (Anttila and Magri, 2018).

How can we investigate and understand the
typological structure encoded by a probabilistic
phonological framework? In the case of a categor-
ical framework such as OT or HG, the predicted
typological structure can be investigated directly
by exhaustively listing all the grammars predicted
for certain constraint and candidate sets. That is
possible because the predicted typology of gram-
mars is usually finite. The situation is rather dif-
ferent for probabilistic frameworks: the predicted
typology always consists of an infinite number of
probability distributions which therefore cannot be
exhaustively listed and directly inspected. A more
indirect strategy is needed to chart the predicted
typological structure.

A natural indirect strategy that gets around the
problem raised by an infinite typology is to enu-
merate, not the individual languages in the typol-
ogy, but the set of implicational universals pre-
dicted by the typology. An implicational univer-
sal is an implication P T−→ P̂ which holds of a
given typology T whenever every language in the
typology that satisfies the antecedent property P
also satisfies the consequent property P̂ (Green-
berg, 1963). Since implicational universals take

1 Boersma and Pater (2016) actually use the term “noisy
HG” instead of “stochastic HG”. We prefer “stochastic HG”
to stress the analogy with Boersma’s earlier framework of
stochastic OT. Furthermore, we prefer to use “stochastic” to
describe a property of the framework, reserving “noisy” to
describe a property of the learning scenario (as opposed to
noise-free). Hayes (2017) discusses further stochastic vari-
ants of categorical HG.



3266

into account every language in the typology, they
chart the boundaries and measure the richness of
the typological structure predicted by T.

Which antecedent and consequent properties P
and P̂ should we focus on? To start from the sim-
plest case, let us consider a typology T of categor-
ical phonological grammars, construed tradition-
ally as mappings from URs to SRs. Within this
categorical framework, the simplest, most basic,
most atomic antecedent property P is the property
of mapping a certain specific UR x to a certain spe-
cific SR y. Analogously, the simplest consequent
property P̂ is the property of mapping a certain
specific UR x̂ to a certain specific SR ŷ. We thus
focus on the following class of implications:

Definition 1 The implicational universal (x, y) T→
(x̂, ŷ) holds relative to a categorical typology T
provided each grammar in T which succeeds at the
antecedent mapping (i.e., it maps the UR x to the
SR y), also succeeds at the consequent mapping
(i.e., it maps the UR x̂ to the SR ŷ). 2

The relation T→ thus defined over mappings is a
partial order (under mild additional assumptions).
It is called the T-order induced by the typology
T (Anttila and Andrus, 2006). For example, any
dialect of English that deletes t/d at the end of a
coda cluster before a vowel also deletes it before a
consonant (Guy, 1991; Kiparsky, 1993; Coetzee,
2004). The implication (/cost.us/, [cos.us]) →
(/cost.me/, [cos.me]) thus holds relative to the ty-
pology T of English dialects.

Implicational universals can also be statistical.
For instance, in dialects of English where t/d dele-
tion applies variably, deletion has been found to be
more frequent before consonants than before vow-
els. To model these frequency effects, we need to
consider a typology T of probabilistic phonologi-
cal grammars, construed as functions from URs to
probability distributions over SRs. We propose to
extend the notion of T-orders from the categorical
to the probabilistic setting as follows:

Definition 2 The implicational universal (x, y) T→
(x̂, ŷ) holds relative to a probabilistic typology T
provided each grammar in T assigns a probabil-
ity to the consequent mapping (x̂, ŷ) which is at
least as large as the probability it assigns to the
antecedent mapping (x, y). 2
To illustrate, the implication (/cost.us/, [cos.us]) →
(/cost.me/, [cos.me]) also holds relative to the ty-
pology T of English dialects with variable dele-
tion because the probability of the consequent

(/cost.me/, [cos.me]) (i.e., the frequency of dele-
tion before a consonant) in any dialect is at
least as large as the probability of the antecedent
(/cost.us/, [cos.us]) (i.e., the frequency of deletion
before a vowel).

The original categorical definition 1 of T-orders
is a special case of the probabilistic definition 2. In
fact, suppose that a categorical grammar succeeds
on the antecedent mapping (x, y). That grammar
construed probabilistically thus assigns probabil-
ity 1 to the antecedent mapping. Definition 2 then
requires that grammar to also assign probability 1
to the consequent mapping (x̂, ŷ). In other words,
the grammar construed categorically succeeds on
the consequent mapping, as required by the origi-
nal definition 1 of categorical T-orders.

T-orders are defined at the level of mappings
from URs to SRs. They thus allow for cross-
framework comparisons, even bridging across cat-
egorical and probabilistic frameworks. This pa-
per (together with the companion Anttila and
Magri 2018) thus uses T-orders to compare the
probabilistic implementations of constraint-based
phonology with the original categorical imple-
mentations.

The main result reported in this paper is that
the T-orders predicted by stochastic OT (and by
partial order OT) coincide with those predicted by
categorical OT, no matter what the candidate and
constraint sets look like, as shown in section 4.
Analogously, the T-orders predicted by stochastic
HG coincide with those predicted by categorical
HG, as shown in section 5. In other words, these
stochastic frameworks do not tamper with the ty-
pological structure induced by the original cate-
gorical frameworks, at least when that structure is
measured in terms of T-orders. These specific re-
sults about OT and HG are derived as a special
case of a more general result on stochastic typolo-
gies, developed in sections 2 and 3.

As discussed in a companion paper (Anttila and
Magri, 2018), the situation is very different for
ME. Both ME and stochastic HG can be con-
strued as probabilistic variants of categorical HG.
Stochastic and categorical HG share the same T-
orders. The ME T-orders instead obey a rather
different underlying convex geometry and turn out
to be much sparser. In other words, ME yields a
much richer probabilistic extension of categorical
HG than stochastic HG does. Section 6 concludes
the paper by discussing these results in the context



3267

of the recent literature on probabilistic constraint-
based phonology.

2 Categorical and stochastic phonology

We assume a relation Gen which pairs each UR x
with a set Gen(x) of candidate SRs. As recalled
above, a categorical phonological grammar G
takes a UR x and selects a corresponding SR y =
G(x) from the candidate set Gen(x). A stochas-
tic phonological grammar G instead takes a UR x
and returns a probability distribution G(·| x) over
Gen(x) which assigns a probability G(y| x) to
each candidate SR y in Gen(x). This section il-
lustrates a general method to leverage a given ty-
pology T of categorical grammars into a typology
of stochastic grammars. Sections 4 and 5 will then
show that various stochastic frameworks in the re-
cent constraint-based literature (such as partial or-
der OT, stochastic OT, and stochastic HG) all fit
within this general scheme.

Following common practice in constraint-based
phonology, we assume that the categorical typol-
ogy T only contains a finite number of grammars.2

We consider a probability mass function p over T.
Thus, p assigns to each categorical grammar G in
T a nonnegative probability mass p(G) ≥ 0 and
these masses sum up to 1, namely

∑
G∈T p(G) =

1. We can then define the stochastic grammar Gp
corresponding to the probability mass function p
as the function which takes a UR x and returns the
probability distributionGp(·| x) over the candidate
setGen(x) defined as in (1). It says that the proba-
bilityGp(y | x) that the UR x is mapped to the SR y
is he probability mass allocated by p to the region
{G ∈ T |G(x) = y} of the typology T consisting
of those categorical grammars which succeed on
the mapping (x, y).

Gp(y | x) =
∑

{G∈T |G(x)=y}

p(G) (1)

We assume next that each categorical grammar
in the typology T returns a unique SR y for each
UR x.3 This assumption suffices to ensure that Gp

2 This assumption always holds in OT, as the number of
constraint rankings is finite. It might fail in HG, but only in
rather pathological situations which do not seem germane to
natural language phonology.

3 Suppose instead that a categorical grammar were to re-
turn two different SRs y1 and y2 for some UR x. How should
we interpret such a scenario? Plausibly, we should interpret
the two SRs y1 and y2 as free variants with equal probability
of 0.5 (while all other candidates have probability 0). But this
means that our grammar is stochastic, not categorical.

is indeed a probability distribution, namely that
the sum of the probabilities Gp(y | x) over the can-
didates y in Gen(x) is equal to 1, as shown in (2).∑
y∈Gen(x)

Gp(y | x)
(a)
=

∑
y∈Gen(x)

∑
{G∈T |G(x)=y}

p(G)

(b)
=
∑
G∈T

p(G)
(c)
= 1 (2)

In step (2a), we have used the definition (1) of
Gp(y | x). In step (2b), we have used the fact that
every grammar in T maps x to a unique SR y, so
that the sets {G ∈ T |G(x) = y} partition the ty-
pology T into disjoint sets as y spans the candidate
setGen(x). In step (2c), we have used the fact that
p is a probability mass function over T and thus
adds up to 1.

A family P of probability mass functions
p1, p2, . . . over the finite categorical typology
T thus induces a typology {Gp1 , Gp2 , . . .} of
stochastic grammars. It is called the stochastic
typology corresponding to the categorical typol-
ogy T and the probability family P , and it is de-
noted by TP . We denote by

T−→ the T-order rel-
ative to the categorical typology T in the sense of

definition 1 and by TP−→ the T-order relative to the
stochastic typology TP in the sense of definition
2. We want to investigate the relationship between
these categorical and stochastic T-orders.

3 Relationship between categorical and
stochastic T-orders

Let us suppose that the implication (x, y) T→ (x̂, ŷ)
holds between an antecedent mapping (x, y) and a
consequent mapping (x̂, ŷ) relative to a categorical
typology T. By definition 1, this means that ev-
ery categorical grammar G in the typology T that
maps the antecedent UR x to the antecedent SR
y (namely, G(x) = y) also maps the consequent
UR x̂ to the consequent SR ŷ (namely, G(x̂) = ŷ),
yielding the inclusion (3).

{G ∈ T |G(x)=y} ⊆ {G ∈ T |G(x̂)= ŷ}
grammars consistent
with the antecedent

mapping

grammars consistent
with the consequent

mapping

(3)

By (1), this inclusion (3) entails that the proba-
bility assigned by Gp to the consequent mapping
(x̂, ŷ) is at least as large as the probability assigned
to the antecedent mapping (x, y), as stated in (4).
This entailment follows from the sheer fact that



3268

probabilities are monotonic relative to set inclu-
sion. The entailment from the inclusion (3) to
the inequality (4) thus holds under no assump-
tions whatsoever on the probability mass function
p used to define the stochastic grammar Gp.

Gp(y | x) ≤ Gp(ŷ | x̂)
probability of the

antecedent mapping
probability of the
consequent mapping

(4)

The latter inequality (4) finally says that the im-

plication (x, y) TP−→ (x̂, ŷ) holds also relative to
the stochastic typology TP in the sense of defi-
nition 2. In conclusion, a categorical T-order al-
ways entails the corresponding stochastic T-order,
no matter the shape of the family P of probability
mass functions used to derive the stochastic typol-
ogy TP from the categorical typology T.

We now turn to the reverse entailment. Sup-

pose that an implication (x, y) TP−→ (x̂, ŷ) holds
between an antecedent mapping (x, y) and a con-
sequent mapping (x̂, ŷ) relative to the stochastic
typology TP . By definition 2, this means in turn
that the inequality (4) holds between the probabil-
ities Gp(y | x) and Gp(ŷ | x̂) of the antecedent and
the consequent mappings relative to any probabil-
ity mass function p in the family P . Suppose by
contradiction that the corresponding implication
(x, y)

T−→ (x̂, ŷ) relative to the original categori-
cal typology T instead fails. By definition 1, this
means that the set inclusion (3) fails because there
exists some grammarG0 with the properties in (5):
G0 succeeds on the antecedent mapping, namely it
maps x to y; but G0 fails on the consequent map-
ping, namely it maps x̂ to some loser candidate ẑ
different from the intended winner candidate ŷ.

G0(x) = y, G0(x̂) = ẑ 6= ŷ (5)

We would like to derive a contradiction from
the assumption (4) that the stochastic implication

(x, y)
TP−→ (x̂, ŷ) holds and the assumption (5) that

the categorical implication (x, y) T−→ (x̂, ŷ) fails.
Yet, no contradiction arises in the general case.

Indeed, suppose that the probability mass func-
tions in the family P all happen to assign zero
(or tiny) probability mass to this grammar G0
which flouts the categorical implication because
of (5). This problematic grammar G0 thus bears
no (or only a tiny) effect on the total probabilities
Gp(y | x) and Gp(ŷ | x̂) of the two mappings (x, y)
and (x̂, ŷ). The probability inequality (4) is there-
fore not necessarily compromised by the offensive

behavior (5) of G0, as long as the other grammars
in the typology comply.

In order to derive a contradiction from these two
conditions (4) and (5), we need to make some as-
sumptions on the family P of probability mass
functions. Indeed, the problem just discussed
arises when every probability mass function p in
P assigns zero (or tiny) probability to the prob-
lematic grammar G0. We need to rule out this sce-
nario. We propose to achieve that through the as-
sumption that the family P satisfies the following

Definition 3 The family P of probability mass
functions over the finite categorical typology T is
sufficiently rich in the sense that for every categor-
ical grammar G in T and for any two URs x and
x̂, the following inequalities

Gp(G(x)| x) > 1/2, Gp(G(x̂)| x̂) > 1/2 (6)

hold for some probability mass function p in P . 2
Here is the intuition behind this definition. Sup-

pose that for every categorical grammar G, the
family P contains a probability mass function
p which assigns all the probability mass to that
grammar G. By (1), the corresponding stochastic
grammarGp assigns probability 1 to the mappings
enforced by G, as stated in (7).

Gp(G(x)| x) = 1 for every UR x (7)

In other words, the stochastic grammar Gp “co-
incides” with the categorical grammar G and the
stochastic typology TP thus “contains” or “ex-
tends” the original categorical typology T. In this
special case, we obviously expect the stochastic

implication (x, y) TP−→ (x̂, ŷ) to entail the categori-
cal implication (x, y) T−→ (x̂, ŷ), as desired.

Condition (6) required by definition 3 is a
weaker version of the latter condition (7). First, it
is weaker because the requirement Gp(G(x)| x) =
1 is replaced with the weaker requirement
Gp(G(x)| x) > 1/2: the probability assigned to
the mappings enforced by G needs not be 1, as
long as it is large enough, namely larger than
1/2. Second, this requirement Gp(G(x)|x) > 1/2
needs not be satisfied by a unique mass p for all
URs: it suffices to look at just two URs at the time.

If the familyP is sufficiently rich in the sense of
definition 3, the two conditions (4) and (5) are in-
deed contradictory. In fact, condition (5) now en-
sures that P contains a probability mass function



3269

p0 such that the corresponding stochastic grammar
Gp0 maps x to y with probability larger than 1/2
and it maps x̂ to ẑ with probability larger than 1/2.
The latter fact means in turn that Gp0 maps x̂ to
ŷ with probability smaller than 1/2, because the
probabilities of the various candidates ŷ, ẑ, . . . in
Gen(x̂) must add up to 1. In conclusion, we have
obtained Gp0(y | x) > 1/2 and Gp0(ŷ | x̂) < 1/2,
in blatant contradiction of (4).

The preceding reasoning is summarized in the
following proposition 1, which says that the T-
order relative to a categorical typology T and the
T-order relative to the corresponding stochastic ty-
pology TP coincide, no matter what the family P
of probability mass functions looks like, as long as
it is sufficiently rich, in the sense of definition 3.
Identity of T-orders holds even when the family P
is infinite, so that the stochastic typology TP con-
tains an infinite number of stochastic grammars,
while the categorical typology T contains only a
finite number of grammars.

Proposition 1 Consider a finite typology T of cat-
egorical grammars and a family P of probability
mass functions on T. Let TP be the typology of
the corresponding stochastic grammars, defined
through (1). If P is sufficiently rich in the sense
of definition 3, the T-order T−→ relative to the cat-
egorical typology T and the T-order

TP−→ relative
to the stochastic typology TP coincide. 2

In the rest of the paper, we apply this result to
various categorical and stochastic frameworks for
constraint-based phonology.

4 Categorial OT, partial order OT, and
stochastic OT induce the same T-orders

In this section, we focus on categorical and
stochastic OT. We assume a set of n constraints
C1, . . . , Ck, . . . , Cn and some candidacy relation
Gen. We recall that a constraint Ck prefers a map-
ping (x, y) to another mapping (x, z) provided Ck
assigns less violations to the former than to the
latter, namely Ck(x, y) < Ck(x, z). A constraint
ranking is an arbitrary linear order� over the con-
straint set. A constraint ranking� prefers a map-
ping (x, y) to another mapping (x, z) provided the
highest �-ranked constraint which distinguishes
between the two mappings (x, y) and (x, z) prefers
(x, y). The categorical OT grammar correspond-
ing to a ranking� maps a UR x to that SR y such
that� prefers the mapping (x, y) to the mapping

(x, z) corresponding to any other candidate z in
Gen(x) (Prince and Smolensky, 2004). We denote
by OT→ the T-order corresponding to the typology T
of the categorical OT grammars corresponding to
all constraint rankings, in the sense of definition 2.

To illustrate, consider the following three con-
straints (from Kiparsky, 1993) for the process
of t/d deletion mentioned in section 1: C1 =
SYLLABLEWELLFORMEDNESS (SWF) penalizes
codas and tautosyllabic consonant clusters; C2 =
ALIGN penalizes resyllabification across word
boundaries; and C3 = MAX penalizes segment
deletion. Suppose that the UR /cost us/ comes
with the three candidate SRs [cost.us] (faithful),
[cos.us] (with deletion), and [cos.tus] (with resyl-
labification). Analogously, suppose that the UR
/cost me/ comes with the three candidate SRs
[cost.me], [cos.me], and [cos.tme]. It is easy to
verify that the implication (/cost.us/, [cos.us]) OT→
(/cost.me/, [cos.me]) holds relative to the OT ty-
pology generated by constraints C1, C2, C3 in
the sense of definition 1: every ranking of the
three constraints which succeeds on the antecedent
mapping (/cost.us/, [cos.us]) also succeeds on the
consequent mapping (/cost.me/, [cos.me]). In
other words, t/d deletion before a vowel entails
deletion before a consonant.

We now turn to the stochastic counterpart of
this categorical framework. A ranking vector
θ = (θ1, . . . , θk, . . . , θn) ∈ Rn assigns a nu-
merical ranking value θk to each constraint Ck.
The stochastic ranking vector θ + � = (θ1 +
�1, . . . , θn + �n) is obtained by adding to the rank-
ing values θ1, . . . , θn some numbers �1, . . . , �n
sampled independently from each other according
to some distribution D on R. If the distribution D
is continuous, the probability that two stochastic
ranking values θh+�h and θk+�k coincide is equal
to zero. The stochastic ranking vector θ + � thus
describes the unique ranking�θ+� which respects
the relative size of the stochastic ranking values:
a constraint Ch is ranked above a constraint Ck
according to �θ+� (namely, Ch �θ+� Ck) if
and only if the stochastic ranking value of the
former is larger than that of the latter (namely,
θh + �h > θk + �k). A ranking vector θ thus
induces the probability mass function pDθ defined
in (8) over the categorical OT typology T. Ob-
viously, this definition yields a probability mass,
namely the sum of the masses pDθ (G) over all the



3270

(a) (/cost.us/, [cos.us]), θ3 = −3 (b) (/cost.us/, [cos.us]), θ3 = 0 (c) (/cost.us/, [cos.us]), θ3 = 3

(d) (/cost.me/, [cos.me]), θ3 = −3 (e) (/cost.me/, [cos.me]), θ3 = 0 (f) (/cost.me/, [cos.me]), θ3 = 3
Figure 1: SOT probabilities of the antecedent mapping (/cost.us/, [cos.us]) and the consequent mapping
(/cost.me/, [cos.me]) as a function of θ1 (horizontal axis) and θ2 (vertical axis) for three choices of θ3

categorical OT grammars G in T is indeed 1.

pDθ (G) = the probability of sampling
�1, . . . , �n

i.i.d.∼ D such that the OT
grammar corresponding to the
ranking�θ+� is indeed G

(8)

The typology of stochastic grammars TP obtained
as in section 2 from the categorical OT typology
T and the family P = {pDθ |θ ∈ Rn} of probabil-
ity mass functions pDθ corresponding to all ranking
vectors θ is called stochastic OT (SOT; Boersma,
1997, 1998). We denote by SOT−→ the T-orders cor-
responding to SOT in the sense of definition 2.

What is the typological structure encoded by
SOT’s T-orders? Given that the original OT ty-
pology is finite (because there are only a finite
number of constraint rankings) while the SOT
typology is infinite (it contains an infinite num-
ber of grammars which assign different proba-
bilities), how much of OT’s typological structure
is preserved in SOT? These questions are cru-
cial for phonological theory but technically non-
trivial. To illustrate, figure 1 plots the SOT
probability of the mappings (/cost.us/, [cos.us])
and (/cost.me/, [cos.me]) relative to the three con-
straints C1, C2, C3 listed above as a function of
the ranking value θ1 of constraint C1 (horizon-
tal axis) and the ranking value θ2 of constraint
C2 (vertical axis) for three choices of the rank-

ing value θ3 of constraint C3.4 These plots sug-
gest that the implication (/cost.us/, [cos.us]) SOT→
(/cost.me/, [cos.me]) holds in SOT: the probability
of the consequent (/cost.me/, [cos.me]) (plotted in
the bottom row) seems to be always larger than the
probability of the antecedent (/cost.us/, [cos.us])
(plotted in the top row). But how can this conjec-
ture be checked, given that SOT probabilities seem
not to admit a closed-form expression?

The result obtained in section 3 provides a
straightforward solution to this problem. Sup-
pose that there exists a positive constant ∆ large
enough that the distribution D concentrates most
of the probability mass on the interval [−∆,+∆],
as stated in (9). This assumption holds in particu-
lar when D has a bounded support or it is defined
through a density (such as a gaussian, as assumed
in Boersma, 1997, 1998).

(D([−∆,+∆]))n > 1/2 (9)

For any constraint ranking�, consider a rank-
ing vector θ such that the top�-ranked constraint
has the largest ranking value; the second top �-
ranked constraint has the second largest ranking
value; and so on. Furthermore, assume that these
ranking values are spaced apart by more than 2∆.
Since the numbers �1, . . . , �n are all bounded be-
tween −∆ and +∆ with probability at least 1/2

4 These plots (as well as those in figure 2) are obtained
by sampling for 10,000 times from each stochastic grammar.
The distribution D is a gaussian with mean 0 and variance 2.



3271

and since the ranking values are spaced apart by
more than 2∆, the constraint ranking �θ+� cor-
responding to the stochastic ranking vector θ + �
coincides with the original ranking � with prob-
ability at least 1/2. In other words, the probabil-
ity mass function pDθ corresponding to this rank-
ing vector θ according to (8) assigns more than
half of the probability mass to the OT grammar
corresponding to the ranking�. The family P =
{pDθ |θ ∈ Rn} is therefore sufficiently rich in the
sense of definition 3. Proposition 1 thus yields the
following
Corollary 1 Under the mild assumption (9) on the
distribution D, the T-order SOT−→ relative to SOT is
identical to the T-order OT−→ relative to categorical
OT for any constraint and candidate set. 2

In conclusion, despite the SOT typology being
infinite, SOT induces the same typological struc-
ture as categorical OT, at least when typological
structure is measured in terms of T-orders. Fur-
thermore, the technical problem of computing T-
orders relative to SOT is reduced to the much eas-
ier problem of computing T-orders relative to cate-
gorical OT, which indeed admits an efficient solu-
tion (Magri, 2018a). This result extends to partial
order OT (Anttila, 1997a), as the latter is a special
case of SOT.

5 Categorial HG and stochastic HG
induce the same T-orders

This section shows that completely analogous con-
siderations hold for HG. A weight vector w =
(w1, . . . , wk, . . . , wn) ∈ Rn+ assigns a nonnega-
tive weight wk ≥ 0 to each constraint Ck. The
w-harmony of a mapping (x, y) is the weighted
sum of the constraint violations multiplied by −1,
namely −

∑n
k=1wkCk(x, y). Because of the mi-

nus sign, mappings with a large harmony have few
constraint violations. The categorical HG gram-
mar corresponding to a weight vector w maps a
UR x to the surface form y such that the map-
ping (x, y) has a larger w-harmony than the map-
ping (x, z) corresponding to any other candidate z
in Gen(x) (Legendre et al., 1990; Smolensky and
Legendre, 2006). We denote by HG→ the T-order
corresponding to the typology T of the categori-
cal HG grammars corresponding to all nonnega-
tive weight vectors, in the sense of definition 2.

To illustrate, it is easy to verify that the implica-
tion (/cost.us/, [cos.us]) HG→ (/cost.me/, [cos.me])
considered above holds also relative to the HG

typology in the sense of definition 1: every
weighting of the three constraints which succeeds
on the antecedent mapping (/cost.us/, [cos.us])
also succeeds on the consequent mapping
(/cost.me/, [cos.me]). In general, the HG typology
is a proper superset of the OT typology (when the
set of URs is finite). The HG T-order is therefore
a subset of the corresponding OT T-order.

We now turn to the stochastic counterpart of this
categorical framework. The stochastic weight vec-
tor w + � = (w1 + �1, . . . , wn + �n) is obtained
by adding to the weights w1, . . . , wn some num-
bers �1, . . . , �n sampled independently from each
other according to some distribution D on R.5 A
weight vector w induces the corresponding proba-
bility mass function pDw on the categorical HG ty-
pology T defined in (10). Obviously, this defini-
tion yields a probability mass, namely the sum of
the masses pDw(G) over all the categorical gram-
mars G in the HG typology T is equal to 1.

pDw(G) = the probability of sampling
�1, . . . , �n

i.i.d.∼ D such that the
HG grammar corresponding to
the weight vector w + � is G

(10)

The typology of stochastic grammars TP obtained
as in section 2 from the categorical HG typol-
ogy T and the family P = {pDw |w ∈ Rn+} of
probability mass functions pDw corresponding to all
nonnegative weight vectors w is called stochastic
HG (SHG; Boersma and Pater, 2016). We denote
by SHG−→ the T-orders corresponding to SHG in the
sense of definition 2.

To illustrate, figure 2 plots the SHG proba-
bility of the mappings (/cost.us/, [cos.us]) and
(/cost.me/, [cos.me]) as a function of the ranking
values of the three constraints C1, C2, C3 listed
above. These plots suggest that the implica-
tion (/cost.us/, [cos.us]) SHG→ (/cost.me/, [cos.me])
holds in SHG as well: the probability of the conse-
quent (/cost.me/, [cos.me]) (plotted in the bottom
row) seems to be always larger than the probabil-
ity of the antecedent (/cost.us/, [cos.us]) (plotted
in the top row). The result obtained in section 3
makes sense of this observation, as follows.

5 Some componentwk+�k of the corrupted weight vector
w+� could be negative. In this case, w+� could correspond
to no HG grammar in T and the probability mass defined in
(10) could therefore add up to less than 1. This problem
can be avoided simply by truncating the corrupted weights
at zero, namely by replacing wk + �k with max{wk + �k, 0}
in the definition of the corrupted weight vector w+� (Magri,
2015).



3272

(a) (/cost.us/, [cos.us]), θ3 = −3 (b) (/cost.us/, [cos.us]), θ3 = 0 (c) (/cost.us/, [cos.us]), θ3 = 3

(d) (/cost.me/, [cos.me]), θ3 = −3 (e) (/cost.me/, [cos.me]), θ3 = 0 (f) (/cost.me/, [cos.me]), θ3 = 3
Figure 2: SHG probabilities of the antecedent mapping (/cost.us/, [cos.us]) and the consequent mapping
(/cost.me/, [cos.me]) as a function of θ1 (horizontal axis) and θ2 (vertical axis) for three choices of θ3

We consider two URs x and x̂. We assume that
x comes with a finite number m+ 1 of candidates
y, z1, . . . , zm and that x̂ comes with a finite number
m̂ + 1 of candidates ŷ, ẑ1, . . . , ẑm̂. This assump-
tion is nonrestrictive. In fact, each UR admits only
a finite number of optima in HG (Magri, 2018b).
Candidate sets can thus be assumed to be finite
without loss of generality. We consider a categor-
ical HG grammar in the typology T and assume
that it maps x and x̂ to y and ŷ, respectively. This
means that any weight vector w = (w1, . . . , wn)
corresponding to this HG grammar assigns a larger
harmony to the winner mappings (x, y) and x̂, ŷ)
than to any of the loser mappings (x, zi) and (x̂, ẑj)
respectively, as stated in (11).

min
i=1,...,m

∑
k

wk(Ck(x, zi)− Ck(x, y))︸ ︷︷ ︸
ξ

> 0

min
j=1,...,m̂

∑
k

wk(Ck(x̂, ẑj)− Ck(x̂, ŷ))︸ ︷︷ ︸
ξ̂

> 0
(11)

Let B be an upper bound on the constraint vio-
lation differences, so that |C(x, zi)−C(x, y)| ≤ B
and |C(x̂, ẑj) − C(x̂, ŷ)| ≤ B for every i =
1, . . . ,m and j = 1, . . . , m̂. Suppose again that
there exists a positive constant ∆ large enough that
the distribution D concentrates most of the proba-

bility mass on [−∆,+∆], in the sense that it sat-
isfies the inequality (9). We consider the weight
vector λw = (λw1, . . . , λwn) obtained by rescal-
ing the weight vector w by a positive scalar λ > 0
sufficiently large, in the sense of (12).

λ > max

{
n∆B

ξ
,
n∆B

ξ̂

}
(12)

Whenever � ∈ [−∆,+∆]n, the HG grammar
corresponding to the stochastic rescaled weight
vector λw + � maps the UR x to the SR y, as
shown in (13). An analogous reasoning shows that
it also maps x̂ to ŷ. In step (13a), we have used
the definition (11) of ξ. In step (13b), we have
lower bounded C(x, zi) − C(x, y) with −B. In
step (13c), we have used the definition (12) of λ.∑

k

(λwk + �k)(Ck(x, zi)− Ck(x, y)) =

= λ
∑
k

wk(Ck(x, zi)− Ck(x, y))+

+
∑
k

�k(Ck(x, zi)− Ck(x, y))

(a)

≥ λξ +
∑
k

�k(Ck(x, zi)− Ck(x, y))

(b)

≥ λξ − n∆B
(c)
> 0

(13)

The intuition behind this reasoning (13) is as fol-
lows. The rescaled weight vector λw generates



3273

(VC, VC) (VC,CVC) (VC, V) (VC,CV)

(CVC,CVC) (V, V)) (V,CV) (CVC,CV)

(CV,CV)

Figure 3: Solid arrows are entailments that hold in OT, HG, SOT, SHG, and ME; dotted arrows are entailments that fail in ME.

the same HG grammar as the weight vector w. If λ
is large, the nonzero weights of the rescaled vector
λw are very large (in absolute value). On the other
hand, the stochastic values �1, . . . , �n are instead
small (because bounded between −∆ and +∆)
and therefore negligible relative to the rescaled
weights. The original weight vector w and the
stochastic rescaled vector � + λw thus generate
the same HG grammar.

In conclusion, the SOT grammar GpDλw corre-
sponding to the probability mass function pDλw
(corresponding to the rescaled weight vector λw)
satisfies the identities GpDλw(y | x) > 1/2 and
GpDλw

(ŷ | x̂) > 1/2. This shows that the family
P = {pDw |w ∈ Rn+} of probability masses is suf-
ficiently rich in the sense of definition 3. Proposi-
tion 1 thus yields the following:

Corollary 2 Under the mild assumption (9) on the
distribution D, the T-order SHG−→ relative to SHG is
identical to the T-order HG−→ relative to categori-
cal HG for any constraint set and any candidate
set which assigns a finite number of candidates to
each UR (while the number of URs can be infinite).
2

6 Conclusions

Phonology has traditionally focused on patterns of
categorical alternations modeled within categor-
ical frameworks such as OT and HG. More re-
cently, phonology has extended its empirical cov-
erage to quantitative data such as gradient judg-
ments and patterns of variation. This move has
required a parallel extension from categorical to
stochastic frameworks, such as partial order OT,
stochastic OT, and stochastic HG. These stochas-
tic frameworks are “extensions” of the original
categorical frameworks in the sense discussed in
section 3. One might thus expect the stochas-
tic frameworks to be typologically less restrictive
than the original categorical frameworks. This pa-

per has shown that is not the case, at least when
typological restrictiveness is measured in terms of
the most basic implicational universals, namely T-
orders. Indeed, the T-orders induced by partial or-
der and stochastic OT coincide with those induced
by categorical OT. Analogously, the T-orders in-
duced by stochastic HG coincide with those in-
duced by categorical HG.

As discussed in a companion paper (Anttila and
Magri, 2018), the situation is very different in ME.
To illustrate, consider the basic syllable system of
Prince and Smolensky (2004). The set of forms
consists of the four syllable types CV, CVC, V, and
VC. Each of them is a candidate of each other. The
constraint set consists of the four constraints ON-
SET, NOCODA, MAX, and DEP. The HG and
OT T-orders coincide and consist of 16 entail-
ments with a feasible antecedent, plotted in figure
3. These entailments extend to SOT and SHG, by
virtue of the corollaries 1 and 2 obtained above.

ME instead misses the eight dotted entailments.
Of the eight entailments which do survive in ME,
seven are such that the antecedent and the conse-
quent surface form coincide, plus the entailment
(VC, VC) → (CV,CV), which is a quirk due to the
fact that VC is the most marked syllable type. This
restriction to entailments whose antecedent and
consequent surface forms coincide is not phono-
logically plausible. Anttila and Magri (2018) con-
clude that the ME formalism imposes typological
restrictions at odds with phonological intuition.

Acknowledgments

The research reported in this paper has been
funded by the Agence National de la Recherche
(project title: ‘The mathematics of segmental
phonotactics’). This paper is part of a larger
project on T-orders, developed in collaboration
with Arto Anttila. His comments on this paper are
gratefully acknowledged.



3274

References
Arto Anttila. 1997a. Deriving variation from grammar:

A study of Finnish genitives. In Frans Hinskens,
Roeland van Hout, and Leo Wetzels, editors, Varia-
tion, change and phonological theory, pages 35–68.
John Benjamins, Amsterdam. Rutgers Optimality
Archive ROA-63, http://ruccs.rutgers.edu/roa.html.

Arto Anttila. 1997b. Variation in Finnish phonology
and morphology. Ph.D. thesis, Stanford University.

Arto Anttila. 2012. Modeling phonological variation.
In Abigail C. Cohn, Cécile Fougeron, and Marie
Huffman, editors, The Oxford Handbook of Labo-
ratory Phonology, pages 76–91. Oxford University
Press, Oxford.

Arto Anttila and Curtis Andrus. 2006. T-orders.
Manuscript and software (Stanford).

Arto Anttila and Giorgio Magri. 2018. T-orders
across categorical and probabilistic constraint-based
phonology. Manuscript (Stanford, CNRS).

Paul Boersma. 1997. How we learn variation, optional-
ity and probability. In Proceedings of the Institute of
Phonetic Sciences (IFA) 21, pages 43–58, University
of Amsterdam. Institute of Phonetic Sciences.

Paul Boersma. 1998. Functional Phonology. Ph.D.
thesis, University of Amsterdam, The Netherlands.
The Hague: Holland Academic Graphics.

Paul Boersma and Joe Pater. 2016. Convergence prop-
erties of a gradual learning algorithm for Harmonic
Grammar. In John McCarthy and Joe Pater, edi-
tors, Harmonic Grammar and Harmonic Serialism.
Equinox Press, London.

Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. Harper and Row, New York.

Andries W. Coetzee. 2004. What it Means to be a
Loser: Non-Optimal Candidates in Optimality The-
ory. Ph.D. thesis, University of Massachusetts,
Amherst.

Andries W. Coetzee and Joe Pater. 2011. The place
of variation in phonological theory. In John Gold-
smith, Jason Riggle, and Alan Yu, editors, Hand-
book of phonological theory, pages 401–434. Black-
well, Cambridge.

Sharon Goldwater and Mark Johnson. 2003. Learning
OT constraint rankings using a Maximum Entropy
model. In Proceedings of the Stockholm Workshop
on Variation Within Optimality Theory, pages 111–
120, Stockholm University.

Joseph H. Greenberg. 1963. Universals of Language.
MIT Press, Cambridge, MA.

G. Guy. 1991. Explanation in variable phonology.
Language Variation and Change, 3:1–22.

Bruce Hayes. 2017. Varieties of Noisy Harmonic
Grammar. In Proceedings of the 2016 Annual Meet-
ing in Phonology, pages –.

Bruce Hayes and Colin Wilson. 2008. A Maxi-
mum Entropy model of phonotactics and phonotac-
tic learning. Linguistic Inquiry, 39:379–440.

Paul Kiparsky. 1993. An OT perspective on phonolog-
ical variation. Handout (Stanford).

Géraldine Legendre, Yoshiro Miyata, and Paul
Smolensky. 1990. Harmonic Grammar: A for-
mal multi-level connectionist theory of linguistic
well-formedness: Theoretical foundations. In An-
nual conference of the Cognitive Science Society 12,
pages 388–395, Mahwah, NJ. Lawrence Erlbaum.

Giorgio Magri. 2015. How to keep the HG weights
non-negative: the truncated Perceptron reweighing
rule. Journal of Language Modeling, 3.2:345–375.

Giorgio Magri. 2018a. Efficient computation of im-
plicational universals in constraint-based phonol-
ogy through the Hyperplane Separation Theorem.
Manuscript (CNRS).

Giorgio Magri. 2018b. Finiteness of optima in
constraint-based phonology. Manuscript (Stanford,
CNRS).

Alan Prince and Paul Smolensky. 1997. Optimality:
From neural networks to universal grammar. Sci-
ence, 275:1604–1610.

Alan Prince and Paul Smolensky. 2004. Optimality
Theory: Constraint Interaction in generative gram-
mar. Blackwell, Oxford. Original version, Techni-
cal Report CU-CS-696-93, Department of Computer
Science, University of Colorado at Boulder, and
Technical Report TR-2, Rutgers Center for Cogni-
tive Science, Rutgers University, April 1993. Avail-
able from the Rutgers Optimality Archive as ROA
537.

Paul Smolensky and Géraldine Legendre. 2006. The
Harmonic Mind. MIT Press, Cambridge, MA.


