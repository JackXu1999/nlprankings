



















































Learning to Define Terms in the Software Domain


Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text, pages 164–172
Brussels, Belgium, Nov 1, 2018. c©2018 Association for Computational Linguistics

164

Learning to Define Terms in the Software Domain

Vidhisha Balachandran Dheeraj Rajagopal Rose Catherine William Cohen
School of Computer Science

Carnegie Mellon University, Pittsburgh, USA
{vbalacha,dheeraj,rosecatherinek,wcohen}@cs.cmu.edu

Abstract

One way to test a person’s knowledge of a do-
main is to ask them to define domain-specific
terms. Here, we investigate the task of au-
tomatically generating definitions of technical
terms by reading text from the technical do-
main. Specifically, we learn definitions of soft-
ware entities from a large corpus built from
the user forum Stack Overflow. To model def-
initions, we train a language model and incor-
porate additional domain-specific information
like word co-occurrence, and ontological cate-
gory information. Our approach improves pre-
vious baselines by 2 BLEU points for the def-
inition generation task. Our experiments also
show the additional challenges associated with
the task and the short-comings of language-
model based architectures for definition gen-
eration.

1 Introduction

Dictionary definitions have been previously used
in various Natural Language Processing (NLP)
pipelines like knowledge base population (Dolan
et al., 1993), relationship extraction, and extract-
ing semantic information (Chodorow et al., 1985).
Creating dictionaries in a new domain is time con-
suming, often requiring hand curation by domain
experts with significant expertise. Developing sys-
tems to automatically learn and generate defini-
tions of words can lead to greater time-efficiency
(Muresan and Klavans, 2002). Additionally, it
helps accelerate resource-building efforts for any
new domain.

In this paper, we study the task of generating
definitions of domain-specific entities. In partic-
ular, our goal is to generate definitions for tech-
nical terms with the freely available Stack Over-
flow1 (SO) as our primary corpus. Stack Overflow
is a technical question-and-answer forum aimed

1https://stackoverflow.com

Figure 1: Screenshot from Stack Overflow showing
questions and corresponding tags in blue boxes

at supporting programmers in various aspects of
computer science. Each question is tagged with
associated entities or “tags”, and the top answers
are ranked based on user upvotes (de Souza et al.,
2014). Figure 1 shows a screenshot from the fo-
rum of a question and the entities tagged with the
question. Our work explores the challenge of gen-
erating definitions of entities in SO using the back-
ground data of question-answer pairs and their as-
sociated tags.

Our base definition generation model is adapted
from Noraset et al. (2017), a Recurrent Neural
Network (RNN) language model to learn to gen-
erate definitions for common English words us-
ing embeddings trained on Google News Cor-
pus. Over this base model, we leverage the
distributed word information via the embeddings
trained on domain specific Stack Overflow corpus.
We improve this model to additionally incorporate
domain-specific information such as co-occurring
entities and domain ontology in the definition gen-
eration process. Our model also uses an additional
loss function to reconstruct the entity word rep-
resentation from the generated sequence. Our best
model can generate definitions in software domain
with a BLEU score of 10.91, improving upon the
baseline by 2 points.

In summary, our contributions are as follows,
1. We propose a new dataset of entities in the



165

software domain and their corresponding defini-
tions, for the definition generation task.

2. We provide ways to incorporate domain-
specific knowledge such as co-occurring entities
and ontology information into a language model
trained for the definition generation task.

3. We study the effectiveness of the model us-
ing the BLEU (Papineni et al., 2002) metric and
present the results and discussion about our re-
sults.

Section 4 of this paper presents the dataset. Sec-
tion 5 discusses the model in detail. In Section 6
and 7 we present the experimental details and re-
sults of our experiments. Section 8 provides an
analysis and discussion of the results and the gen-
erated definitions.

2 Related Work

Definition modeling: The closest work related
to ours is Noraset et al. (2017) who learn to gen-
erate definitions for general English words using a
RNN language model initialized with pre-trained
word embeddings. We adapt the method proposed
by them and use it in a domain-specific construct.
We aim to learn definitions of entities in the soft-
ware domain. Hill et al. (2015) learn to produce
distributed embeddings for words using their dic-
tionary definitions as a means to bridge the gap
between lexical and phrase semantics. Similarly,
Tissier et al. (2017) use lexical definitions to aug-
ment the Word2Vec algorithm by adding an objec-
tive of reconstructing the words in the definition.
In contrast, we focus solely on generating the def-
initions of entities. We add an objective of recon-
structing the embedding of the word from the gen-
erated sequence. Also, all the above work focus on
lexical definitions of general English words, while
we focus on closed domain software terms. Dhin-
gra et al. (2017) present a dataset of cloze-style
queries constructed from definitions of software
entities on Stack Overflow. In contrast to their
work, we focus on generating the entire definition
of entities.

RNN Language Models : We use RNN
based language models, conditioned on the term
to be defined and its ontological category, to gen-
erate definitions. Such neural language mod-
els have been shown to successfully work for
image-captioning tasks (Karpathy and Fei-Fei,
2015; Kiros et al., 2014), concept to text gener-
ation (Lebret et al., 2016; Mei et al., 2015), ma-

chine translation (Luong et al., 2014; Bahdanau
et al., 2014) and conversations and dialog systems
(Shang et al., 2015; Wen et al., 2015).

Reconstruction Loss Framework : We also
build an explicit loss framework to reconstruct the
term by reducing the cosine distance between the
embedding of the term and the embedding of the
reconstructed term. We adapt this approach from
Hill et al. (2015) who apply it to learn word rep-
resentations using dictionaries. Inan et al. (2016)
propose a loss framework for language modeling
to minimize the distribution distance between the
prediction distribution and the true data distribu-
tion. Though we use a different loss framework,
we use a similar type of parameter tying in our
implementation.

3 Definitions

Dictionary definitions represent a large source of
our knowledge of meaning of words (Amsler,
1980). Definitions are composed of a ‘genus’
phrase and a ‘differentia’ phrase (Amsler, 1980).
The ‘genus’ phrase identifies the general category
of the defined word. This helps derive an ‘Is A’
relationship between the general category and the
word being defined. The ‘differentia’ phrase dis-
tinguishes this instance of the general category
from other instances. Definitions can have further
set of differentia to imply more granular explana-
tion. For example, Merriam-Webster 2 defines the
word ‘house’ as ‘a building that serves as living
quarters for one or a few families’. Here the phrase
‘a building’ is the genus that denotes that a house
is a building. The phrases ‘that serves as living
quarters’ and ‘for one or a few families’ are dif-
ferentia phrases which help identify house from
other buildings. Our model of definitions adapts
this interpretation. From a modeling perspective,
we hypothesize that using language models would
learn the template structure of definitions, and in-
corporating entity-entity co-occurrence as well as
ontological category information would help us fill
the specific differentia and genus concepts to the
template structure.

4 Dataset

In SO, users can associate a question with a ‘tag’,
such as ‘Java’ or ‘machine learning’, to help other
users find and answer it. These tags are nearly al-
ways names of domain specific entities. Each tag

2https://www.merriam-webster.com/dictionary/house



166

Software Tag Definition Category

hmac in cryptography hmac hash-based message au-
thentication code is a specific construction
for calculating a message authentication code
mac involving a cryptographic hash-function in
combination with a secret-key

authentication

persistence persistence in computer programming refers to
the capability of saving data outside the appli-
cation memory .

database

ndjango ndjango is a port of the popular django
template-engine to .net .

framework

intellisense intellisense is microsoft s implementation of
automatic code-completion best known for its
use in the microsoft visual-studio integrated
development-environment .

compiler

Table 1: Sample definitions from the dataset

train val test

# samples 22334 1240 1240
# avg definition length 16.54 16.53 16.69

Table 2: Dataset Statistics

has a definition on SO. For the definitions, we cre-
ated a dataset of 25K software entities (tags from
SO) and their definitions on SO. The data collec-
tion and pre-processing for the task is similar to
cloze-style software questions collected in Dhin-
gra et al. (2017). The definitions dataset was built
from the definitional “excerpt” entry for each tag
(entity) on Stack Overflow. For example, the ex-
cerpt for the “java” tag is, “Java is a general pur-
pose object-oriented programming language de-
signed to be used in conjunction with the Java Vir-
tual Machine (JVM).” The dataset statistics can be
seen in Table 2. This dataset is used for train-
ing our definition generation models. Examples
of definitions in the dataset are shown in Table 1.

We use a background corpus of top 50 threads 3

tagged with every entity on Stack Overflow (Dhin-
gra et al., 2017) and attempt to learn definitions of
entities from this data. We use this background
corpus for training word embeddings and to give
us tag co-occurrences. In SO, a particular ques-
tion can have multiple tags associated with it,
which we call ‘co-occurring tags’. We extracted
the top 50 question posts for each tag, along with
any answer-post responses and metadata (tags, au-

3A question along with the answers provided by other
users is collectively called a thread. The threads are ranked
in terms of votes from the community.

thorship, comments) using Scrapy 4. From each
thread, we used all text that is not marked as code
and segmented them into sentences. Each sen-
tence is truncated to 2048 characters, lower-cased
and tokenized using a custom tokenizer compati-
ble with special characters in software terms (e.g.
.net, c++). The background corpus for our task
consists of 27 million sentences.

5 Model

5.1 Definition generation as language
modeling

We model the task of generating definitions as
a language modeling task. The architecture of
the model is shown in Figure 2. We model
the problem of generating a definition D =
w1, w2...wT given the entity w∗, where the prob-
ability of generating a token p(wt) is given by
P (wt|w1..wt−1, w∗) and the probability of gener-
ating the entire definition is given by:

P (D|w∗) =
T∏
t=1

P (wt|w1..wt−1, w∗)

We model this using LSTM language models
(Mikolov et al., 2010; Hochreiter and Schmidhu-
ber, 1997). An LSTM unit is composed of three
multiplicative gates which control the proportions
of information to forget and to pass on to the next
time step. During training, the input to the LSTM
are the word embeddings of the gold definition se-
quence. At test time, the input is the embedding of
the input entity and previously generated words.

4https://scrapy.py



167

Figure 2: Model Architecture

We outline the functions in our model below :

x′t = E[wt]

ht = LSTM(x
′
t, ht−1)

PLM (wt|w1...wt−1, w∗) = sm(Wk ∗ ht) (1)

where E is an embedding matrix, initialized
with pre-trained embeddings and Wk is a weight
matrix. sm is the softmax function.

Adapting the baselines from Noraset et al.
(2017), we explore two variants of providing the
model with the input entity :

Seed Model: The input entity is given as the
first input token to the RNN as a seed. The loss
of predicting the start token, < sos >, given the
word is not taken into account.

Concat Model: Along with being given as a
seed to the model, the input entity is concatenated
with the input token of the RNN at every timestep.
We use these as baselines for our approach as well.

5.2 Incorporating Entity-Entity
Co-occurrence

We propose an extended model to incorporate
entity-entity association information from the tags
to generate the final definition. We define a co-
occurrence based probability measure for every

entity, we as :

PEE(we|w∗) =

{
c(we,w∗)
c(w∗) + �, if we is an entity

�, otherwise

where c is count function and we is defined as
any software entity which is not the entity being
defined. c(we, w∗) is the count of sentences for
which entities we and w∗ were tagged together.
This probability is smoothed for non-entity words
with an � value. We use � = 0.0001 for all cases.
To incorporate this probability into our model, we
interpolate it with the language model probability
defined in Equation 1 as follows :

P (wt|w1..wt−1, w∗) = λt ∗ PEE(wt|w∗)
+(1− λt) ∗ PLM (wt|w1..wt−1, w∗)

where λt is a learned parameter, given as follows :

zt = tanh(Wp ∗ ht−1 +Wq ∗ E(w∗) + b)
λt = σ(Wr ∗ zt)

where Wp, Wq, Wr, b are learn-able parameters.
The λt parameter learns to switch between the
contextual language model probability when gen-
erating tokens forming the structure of the defini-
tion and the entity-entity probability when gener-
ating tokens which are themselves entities.



168

5.3 Modeling Ontological Category
Information

We use a pre-defined set of 86 categories which are
the ontological categories for the software domain
proposed as part of the GNAT 5 project. For each
category, we compute a distributed representation
vector by taking the mean of every dimension of
the constituent tokens in the category name. We
term this the average category embedding. We
map every entity to its closest category in embed-
ding space using cosine distance. Examples of cat-
egory mappings to the terms are shown in Table 1.

We explore two ways of using the average cate-
gory embedding :
1) Adding the category embedding vector (ACE)
to the word vector of the entity to extract a new
vector which we hope is closer to words defin-
ing the entity. This is inspired from the idea of
additive compositionality of vectors as shown in
(Mikolov et al., 2013b).

x′1 = E[w
∗] + E′[c(w∗)]

2) Concatenating the category embedding vec-
tor (CCE) with the input embeddings at every
timestep of the LSTM.

x′t = E[wt];E
′[c(w∗)]

where E is the word embedding matrix, E′ is the
category embedding matrix and c(x) is a function
that maps every entity to its corresponding onto-
logical category.

5.4 Loss Augmentation

To enforce the model to condition on the entity and
generate definitions, we propose to augment the
standard cross entropy loss with a loss framework
that focuses on reconstructing the entity from the
generated sequence. This additionally constrains
the model to generate tokens close to the tokens
in the definition. We introduce a second LSTM
model which reverse encodes the output text se-
quence of the forward mode and projects the en-
coded sequence into an embedding space of the
same dimension as the term being defined.

h′t = LSTM(y
′
1....y

′
T ) (2)

e∗wr =Wb ∗ h
′
t

5http://curtis.ml.cmu.edu/gnat/software/

where y′1...y
′
T is the generated definition sequence

and Wb is a weight matrix.
We add an additional objective to the model

to minimize the cosine distance between the pro-
jected vector and the embedding of the input term:

losscosine(w
∗, e∗wr) = 1− cos(E(w

∗), e∗wr)

where, w∗ is the input term, and e∗wr is the recon-
structed term vector.

The resulting network can be trained end-to-
end to minimize the cross entropy loss between
the output and target sequence L(y, y′) in addi-
tion to the reconstruction loss between the input
and reconstructed input vector L(w∗, e∗wr). Since
the decode step is a greedy decode step, gradi-
ents cannot propagate through it. To solve this, we
share the parameters between the two LSTM net-
works and forward and reconstruction linear layers
(Chisholm et al., 2017). To generate definitions at
test time, the backward network does not need to
be evaluated.

6 Experimental Setup

To train the model, we use the 25k definitions
dataset built as described in Section 4. We split
the data randomly into 90:5:5 train, test and val-
idation sets as shown in Table 2. The words be-
ing defined are mutually exclusive across the three
sets, and thus our experiments evaluate how well
the models generalize to new words.

All of the models utilize the same set of fixed
word embeddings from two different corpora. The
first set of vectors are trained entirely on the Stack
Overflow background corpus and the other set are
pre-trained open domain word embeddings7. Both
these embeddings are concatenated and we use
this as the representation for each word. For the
embeddings trained on Stack Overflow corpus, we
use the Word2Vec (Mikolov et al., 2013a) imple-
mentation of Gensim8 toolkit. In the corpus, we
prepend to every sentence in a question-answer
pair, every tag it is associated with. We further
eliminated stopwords from the corpus and set a
larger context window size of 10.

For the model, we use a 2 layer LSTM with 500
hidden unit size for both the forward and recon-
struction layers of the models. The size of the em-

6Our implementation of baselines from (Noraset et al.,
2017) using greedy decode approach

7https://code.google.com/archive/p/word2vec/
8https://radimrehurek.com/gensim/models/word2vec.html



169

Model BLEU

Seed + SO Emb* 8.90
Seed + SO Emb + Eng Emb** 9.01
Concat Model 9.44

Concat Model + Entity-Entity Model 9.28
Concat Model + Category Model (CCE) 10.19
Concat Model + Category Model (ACE) 10.86
Concat Model + Category Model (ACE) + Cosine Loss Model 10.91

Table 3: Experimental results for our different models. *SO Emb = Embeddings learned on Stack Overflow. **Eng
Emb = Embeddings learned on general open domain English dataset

Model BLEU

Seed (Noraset et al., 2017) 26.69
Concat (Noraset et al., 2017) 28.44

Seed (English words) 6 32.79
Concat (English words) 36.37

Table 4: Experimental results of our baselines on com-
mon English words dataset (Noraset et al., 2017)

bedding layer is set to 300 dimension. For train-
ing, we minimize the cross-entropy and recon-
struction loss using Adam (Kingma and Ba, 2014)
optimizer with a learning rate of 0.001 and gradi-
ent clip of 5. We evaluate the task using BLEU
(Papineni et al., 2002).

7 Results

The results for the different models are summa-
rized in Table 4. The first section are results of
the baselines as reported by Noraset et al. (2017).
The second section shows results of our imple-
mentation of the baselines on common English
word definitions dataset. We report BLEU scores
on definitions generated using a greedy approach.
The remaining two sections are results from our
proposed models on software entity definitions
dataset.

In comparison, on the software entity defini-
tions dataset, the same baselines do not gener-
ate any reasonable definitions, giving low BLEU
scores. This demonstrates that using a language-
model with embeddings trained on general pur-
pose large-scale, domain-specific corpora is inad-
equate when the definitions are longer and more
domain-specific.

The Seed model, instantiated with both the
Stack Overflow embeddings as well as the open
domain Google News English embeddings, shows
better performance than the model that only uses
Stack Overflow embeddings. For all further mod-

els, we adopt the concatenated Stack Overflow
embeddings and open domain English embed-
dings. We see from the table, that the Concat
Model performs better than the Seed Model. We
choose the best model among Seed and Concat
and perform additional experiments to evaluate the
additional proposed changes to the model.

Surprisingly, adding the entity-entity relation-
ships to the Concat Model does not provide any
gains. Although, providing the category informa-
tion from the ontology by adding it to the input
word vector (ACE) provides us a higher BLEU
score of 10.86.

Further, adding the reconstruction loss objec-
tive to the ACE model provides us small addi-
tional gains and achieves a 10.91 BLEU score. Al-
though we see incremental improvements on the
task, overall our results show that language models
are inadequate to model definitions, as empirically
shown by the low overall BLEU scores.

8 Discussion

Noraset et al. (2017) showed that RNN language
models can be used to learn to generate definitions
for common English words. On adapting their
techniques for closed domain software entities, we
find that the language models generate definitions
which follow the template of definitions, but have
incorrect terms in the genus and differentia com-
ponents.

Table 5 shows some reference definitions and
definitions generated from our best model. In the
generated definition of “virtual-pc”, we observe
that the model generates a definition which has
a distinguishable genus and differentia, but the
genus is not the right ontological category for the
entity. The differentia is incorrect as well. Simi-
larly in the definition of “esper”, we observe that
the model generates ‘open source software’ as the
genus, while the reference genus is ‘open source



170

Entity Reference Generated
windows-server-2000 microsoft windows-2000 server

is an operating-system for use on
server computers.

microsoft s UNK is a free open-
source content-management-
system ..

esper UNK is a lightweight open-
source library for cep complex-
event-processing and esp event
stream processing applications.

UNK is a open-source software
for the java programming lan-
guage.

virtual-pc virtual-pc is a virtualization pro-
gram that simulates a hardware
environment using software.

the UNK is a commercial
operating-system for the win-
dows operating-system.

Table 5: Reference and generated sentences

library’ which shows that the model is able to
learn the right genus for the entity but generates
an incorrect differentia component. We see that
the reference differentia is quite long with many
non-entity tokens which would be hard to model.
This explains our results of obtaining lower BLEU
scores using the entity-entity co-occurrence mod-
els as most differentia terms consist of many non-
entity tokens. We also observed that the genus
and differentia components for technical defini-
tions have longer and very specific phrases com-
pared to common English words. These phrases
also tend to be very sparse in the vocabulary, mak-
ing the task even more challenging.

The general English definitions dataset pre-
sented by Noraset et al. (2017) has 20K most com-
mon English words and their definitions. The En-
glish words for which the definitions are gener-
ated, also tend to appear in the corpus very fre-
quently, thereby having better distributed repre-
sentations. We presume that the higher BLEU
scores for common English words are reflective
of that. In contrast, entities in closed domains are
much less frequent in background corpora increas-
ing the difficulty of the task. Also, the average
definition length in common English words defini-
tion corpus is 6.6 while the average length of def-
initions for software entities is 16.54, which adds
additional complexity in generating these defini-
tions. We hypothesize that due to low expressive-
ness of word representations of entities in compar-
isons to common English words, language mod-
els are unable to learn relations between entities
and their genus and differentia components. The
addition of the ontological category information
alleviates the problem by a small margin, but is
still insufficient for the model to learn to generate

close-to-perfect definitions.

Through our observations, we find that RNN
language models initialized with distributed word
representations of entities is inadequate to gener-
ate definitions from scratch. We envision that fu-
ture models should be able to learn better associa-
tions between entities and its genus and differentia
phrases. Also, the model should ensure it has ad-
equate long term memory to generate definitions
that are longer in length.

9 Conclusion and Future Work

In this paper, we present our initial work in the
task of definition generation for software enti-
ties or terms. We introduced different approaches
for the task, where we explore ways of incorpo-
rating ontology information and entity-entity co-
occurrence relationships. We also present the re-
sults and analysis for the same. Given the com-
plexity of the task, we achieve around 2 BLEU im-
provements over baselines. We demonstrate that
the current models are inadequate to automatically
learn to generate complex definitions for entities.

As an immediate next step, we would like to ap-
proach the task from an encoder-decoder perspec-
tive by collecting external data about the word be-
ing defined and using it to guide the generation
process. Our hypothesis is that providing exter-
nal information about an entity and it’s usage in
various contexts, would help us better identify the
genus and differentia for the entity. Currently, we
give only the immediate parent category as an in-
put from the ontology, we would also like to ex-
plore how to leverage on the entire ontology struc-
ture for definition generation.



171

Acknowledgments

This work was funded by NSF under grant CCF-
1414030. Additionally, the authors would like to
thank Kathryn Mazaitis for her help with data col-
lection and analysis.

References
Robert Alfred Amsler. 1980. The structure of the

merriam-webster pocket dictionary.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Andrew Chisholm, Will Radford, and Ben Hachey.
2017. Learning to generate one-sentence bi-
ographies from wikidata. arXiv preprint
arXiv:1702.06235.

Martin S Chodorow, Roy J Byrd, and George E Hei-
dorn. 1985. Extracting semantic hierarchies from a
large on-line dictionary. In Proceedings of the 23rd
annual meeting on Association for Computational
Linguistics, pages 299–304. Association for Com-
putational Linguistics.

Bhuwan Dhingra, Kathryn Mazaitis, and William W
Cohen. 2017. Quasar: Datasets for question an-
swering by search and reading. arXiv preprint
arXiv:1707.03904.

William Dolan, Lucy Vanderwende, and Stephen D
Richardson. 1993. Automatically deriving struc-
tured knowledge bases from on-line dictionaries. In
Proceedings of the First Conference of the Pacific
Association for Computational Linguistics, pages 5–
14.

Felix Hill, Kyunghyun Cho, Anna Korhonen, and
Yoshua Bengio. 2015. Learning to understand
phrases by embedding the dictionary. arXiv preprint
arXiv:1504.00548.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Hakan Inan, Khashayar Khosravi, and Richard Socher.
2016. Tying word vectors and word classifiers:
A loss framework for language modeling. arXiv
preprint arXiv:1611.01462.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages
3128–3137.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Ryan Kiros, Ruslan Salakhutdinov, and Richard S
Zemel. 2014. Unifying visual-semantic embeddings
with multimodal neural language models. arXiv
preprint arXiv:1411.2539.

Rémi Lebret, David Grangier, and Michael Auli. 2016.
Neural text generation from structured data with ap-
plication to the biography domain. arXiv preprint
arXiv:1603.07771.

Minh-Thang Luong, Ilya Sutskever, Quoc V Le, Oriol
Vinyals, and Wojciech Zaremba. 2014. Addressing
the rare word problem in neural machine translation.
arXiv preprint arXiv:1410.8206.

Hongyuan Mei, Mohit Bansal, and Matthew R Walter.
2015. What to talk about and how? selective gen-
eration using lstms with coarse-to-fine alignment.
arXiv preprint arXiv:1509.00838.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan
Černockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In
Eleventh Annual Conference of the International
Speech Communication Association.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Smaranda Muresan and Judith Klavans. 2002. A
method for automatically building and evaluating
dictionary resources. In LREC, volume 2, pages
231–234.

Thanapon Noraset, Chen Liang, Larry Birnbaum, and
Doug Downey. 2017. Definition modeling: Learn-
ing to define word embeddings in natural language.
In AAAI, pages 3259–3266.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. arXiv preprint arXiv:1503.02364.

Lucas Batista Leite de Souza, Eduardo Cunha Cam-
pos, and Marcelo de Almeida Maia. 2014. Ranking
crowd knowledge to assist software development. In
ICPC.

Julien Tissier, Christophe Gravier, and Amaury
Habrard. 2017. Dict2vec: Learning word embed-
dings using lexical dictionaries. In Conference on



172

Empirical Methods in Natural Language Processing
(EMNLP 2017), pages 254–263.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-
Hao Su, David Vandyke, and Steve Young. 2015.
Semantically conditioned lstm-based natural lan-
guage generation for spoken dialogue systems.
arXiv preprint arXiv:1508.01745.


