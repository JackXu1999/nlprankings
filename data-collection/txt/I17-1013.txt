



















































An Exploration of Neural Sequence-to-Sequence Architectures for Automatic Post-Editing


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 120–129,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

An Exploration of Neural Sequence-to-Sequence Architectures
for Automatic Post-Editing

Marcin Junczys-Dowmunt
Department for Natural Language Processing

Adam Mickiewicz University in Poznań
junczys@amu.edu.pl

Roman Grundkiewicz
School of Informatics

University of Edinburgh
rgrundki@inf.ed.ac.uk

Abstract

In this work, we explore multiple neu-
ral architectures adapted for the task of
automatic post-editing of machine trans-
lation output. We focus on neural end-
to-end models that combine both inputs
mt (raw MT output) and src (source lan-
guage input) in a single neural architec-
ture, modeling {mt, src} → pe directly.
Apart from that, we investigate the influ-
ence of hard-attention models which seem
to be well-suited for monolingual tasks, as
well as combinations of both ideas. We
report results on data sets provided dur-
ing the WMT-2016 shared task on au-
tomatic post-editing and can demonstrate
that dual-attention models that incorporate
all available data in the APE scenario in a
single model improve on the best shared
task system and on all other published re-
sults after the shared task. Dual-attention
models that are combined with hard atten-
tion remain competitive despite applying
fewer changes to the input.

1 Introduction

Given the raw output of a (possibly unknown) ma-
chine translation system from language src to lan-
guage mt, Automatic Post-Editing (APE) is the
process of automatic correction of raw MT output
(mt), so that a closer resemblance to human post-
edited MT output (pe) is achieved. While APE
systems that only model mt → pe yield good re-
sults, the field has always strived towards methods
that also integrate src in various forms.

With neural encoder-decoder models, and
multi-source models in particular, this can be now
achieved in more natural ways than for previously
popular phrase-based statistical machine transla-

tion (PB-SMT) systems. Despite this, previously
reported results for multi-source or dual-source
models in APE scenarios are unsatisfying in terms
of performance.

In this work, we explore a number of single-
source and dual-source neural architectures which
we believe to be better fits to the APE task than
vanilla encoder-decoder models with soft atten-
tion. We focus on neural end-to-end models that
combine both inputs mt and src in a single neu-
ral architecture, modeling {mt, src} → pe di-
rectly. Apart from that, we investigate the influ-
ence of hard-attention models, which seem to be
well-suited for monolingual tasks. Finally, we cre-
ate combinations of both architectures.

We report results on data sets provided dur-
ing the WMT-2016 shared task on automatic post-
editing (Bojar et al., 2016) and compare our per-
formance against the shared task winner, the sys-
tem submitted by the Adam Mickiewicz Univer-
sity (AMU) team (Junczys-Dowmunt and Grund-
kiewicz, 2016), and a more recent system by Pal
et al. (2017) with the previously best published re-
sults on the same test set.

Our main contributions are: (1) we perform
a thorough comparison of end-to-end neural ap-
proaches to APE during which (2) we demon-
strate that dual-attention models that incorporate
all available data in the APE scenario in a sin-
gle model achieve the best reported results for the
WMT-2016 APE task, and (3) show that models
with a hard-attention mechanism reach competi-
tive results although they execute fewer edits than
models relying only on soft attention.

The remainder of the paper is organized as fol-
lows: Previous relevant work is described in Sec-
tion 2. Section 3 summarizes the basic encoder-
decoder with attention architecture that is fur-
ther extended with multiple non-standard attention
mechanisms in Section 4. These attention mecha-

120



nisms are: hard-attention in Section 4.1, a combi-
nation of hard attention and soft attention in Sec-
tion 4.2, dual soft attention in Section 4.3 and a
combination of hard attention and dual soft atten-
tion in Section 4.4. We describe experiments and
results in Section 5 and conclude in Section 7.

2 Previous work

Before the application of neural sequence-to-
sequence models to APE, most APE systems
would rely on phrase-based SMT following a
monolingual approach first introduced by Simard
et al. (2007). Béchara et al. (2011) proposed
a “source-context aware” variant of this ap-
proach where automatically created word align-
ments were used to create a new source language
which consisted of joined MT output and source
token pairs. The inclusion of source-language
information in that form was shown to improve
the automatic post-editing results (Béchara et al.,
2012; Chatterjee et al., 2015). The quality of the
used word alignments plays an important role for
this methods, as demonstrated for instance by Pal
et al. (2015).

During the WMT-2016 APE shared task two
systems relied on neural models, the CUNI sys-
tem (Libovický et al., 2016) and the shared task
winner, the system submitted by the AMU team
(Junczys-Dowmunt and Grundkiewicz, 2016).
This submission explored the application of neu-
ral translation models to the APE problem and
achieved good results by treating different mod-
els as components in a log-linear model, allowing
for multiple inputs (the source src and the trans-
lated sentence mt) that were decoded to the same
target language (post-edited translation pe). Two
systems were considered, one using src as the in-
put (src → pe) and another using mt as the input
(mt → pe). A simple string-matching penalty in-
tegrated within the log-linear model was used to
control for higher faithfulness with regard to the
raw MT output. The penalty fired if the APE sys-
tem proposed a word in its output that had not
been seen in mt. The influence of the components
on the final result was tuned with Minimum Error
Rate Training (Och, 2003) with regard to the task
metric TER.

Following the WMT-2016 APE shared task, Pal
et al. (2017) published work on another neural
APE system that integrated precomputed word-
alignment features into the neural structure and en-

forced symmetric attention during the neural train-
ing process. The result was the best reported sin-
gle neural model for the WMT-2016 APE test set
prior to this work. With n-best list re-ranking and
combination with phrase-based post-editing sys-
tems, the authors improved their results even fur-
ther. None of their systems, however, integrated
information from src, all modeled mt→ pe.
3 Attentional Encoder-Decoder

Implementations of all models explored in this pa-
per are available in the Marian1 toolkit (Junczys-
Dowmunt et al., 2016). The attentional encoder-
decoder model in Marian is a re-implementation
of the NMT model in Nematus (Sennrich et al.,
2017). The model differs from the standard model
introduced by Bahdanau et al. (2015) by several
aspects, the most important being the conditional
GRU with attention. The summary provided in
this section is based on the description in Sennrich
et al. (2017).

Given the raw MT output sequence
(x1, . . . , xTx) of length Tx and its manually
post-edited equivalent (y1, . . . , yTy) of length Ty,
we construct the encoder-decoder model using the
following formulations.

Encoder context A single forward encoder state−→
h i is calculated as:

−→
h i = GRU(

−→
h i−1,F[xi]),

where F is the encoder embeddings matrix. The
GRU RNN cell (Cho et al., 2014) is defined as:

GRU (s,x) =(1− z)� s + z� s, (1)
s = tanh (Wx + r�Us) ,
r = σ (Wrx + Urs) ,
z = σ (Wzx + Uzs) ,

where x is the cell input; s is the previous recurrent
state; W, U, Wr, Ur, Wz , Uz are trained model
parameters2; σ is the logistic sigmoid activation
function. The backward encoder state is calculated
analogously over a reversed input sequence with
its own set of trained parameters.

Let hi be the annotation of the source symbol
at position i, obtained by concatenating the for-
ward and backward encoder RNN hidden states,
hi = [

−→
h i;
←−
h i], the set of encoder states C =

{h1, . . . ,hTx} then forms the encoder context.
1https://github.com/marian-nmt/marian
2Biases have been omitted.

121



Decoder initialization The decoder is initial-
ized with start state s0, computed as the average
over all encoder states:

s0 = tanh

(
Winit

∑Tx
i=1 hi
Tx

)
.

Conditional GRU with attention We follow
the Nematus implementation of the conditional
GRU with attention, cGRUatt:

sj = cGRUatt (sj−1,E[yj−1],C) , (2)

where sj is the newly computed hidden state, sj−1
is the previous hidden state, C the source context
and E[yj−1] is the embedding of the previously
decoded symbol yi−1.

The conditional GRU cell with attention,
cGRUatt, has a complex internal structure, consist-
ing of three parts: two GRU layers and an inter-
mediate attention mechanism ATT.

Layer GRU1 generates an intermediate repre-
sentation s′j from the previous hidden state sj−1
and the embedding of the previous decoded sym-
bol E[yj−1]:

s′j = GRU1 (sj−1,E[yj−1]) .

The attention mechanism, ATT, inputs the en-
tire context set C along with intermediate hidden
state s′j in order to compute the context vector cj
as follows:

cj =ATT
(
C, s′j

)
=

Tx∑
i

αijhi,

αij =
exp(eij)∑Tx

k=1 exp(ekj)
,

eij =vᵀa tanh
(
Uas′j + Wahi

)
,

where αij is the normalized alignment weight be-
tween source symbol at position i and target sym-
bol at position j, and va,Ua,Wa are trained
model parameters.

Layer GRU2 generates sj , the hidden state of
the cGRUatt, from the intermediate representation
s′j and context vector cj :

sj = GRU2
(
s′j , cj

)
.

Deep output Finally, given sj , yj−1, and cj , the
output probability p(yj |sj , yj−1, cj) is computed
by a softmax activation as follows:

p(yj |sj ,yj−1, cj) = softmax (tjWo) ,
tj = tanh (sjWt1 + E[yj−1]Wt2 + cjWt3) .

Wt1 ,Wt2 ,Wt3 ,Wo are the trained model pa-
rameters.

This rather standard encoder-decoder model
with attention is our baseline and denoted as
CGRU.

4 Encoder-Decoder Models with
APE-specific Attention Models

The following models reuse most parts of the
architecture described above wherever possible,
most differences occur in the decoder RNN cell
and the attention mechanism. The encoders are
identical, so are the deep output layers.

4.1 Hard Monotonic Attention
Aharoni and Goldberg (2016) introduce a sim-
ple model for monolingual morphological re-
inflection with hard monotonic attention. This
model looks at one encoder state at a time, start-
ing with the left-most encoder state and progress-
ing to the right until all encoder states have been
processed.

The target word vocabulary Vy is extended with
a special step symbol (V ′y = Vy ∪ {〈STEP〉}) and
whenever 〈STEP〉 is predicted as the output sym-
bol, the hard attention is moved to the next encoder
state. Formally, the hard attention mechanism
is represented as a precomputed monotonic se-
quence (a1, . . . , aTy) which can be inferred from
the target sequence (y1, . . . , yTy) (containing orig-
inal target symbols and Tx step symbols) as fol-
lows:

a1 = 1,

aj =
{
aj−1 + 1 if yj−1 = 〈STEP〉
aj−1 otherwise.

For a given context C = {h1, . . . ,hTx}, the at-
tended context vector at time step j is simply haj .

Following the description by Aharoni and Gold-
berg (2016) for their LSTM-based model, we
adapt the previously described encoder-decoder
model to incorporate hard attention. Given the se-
quence of attention indices (a1, . . . , aTy), the con-
ditional GRU cell (Eq. 2) used for hidden state
updates of the decoder is replaced with a simple
GRU cell (Eq. 1) (thus removing the soft-attention
mechanism):

sj = GRU
(
sj−1,

[
E[yj−1];haj

])
, (3)

where the cell input is now a concatenation of the
embedding of the previous target symbol E[yj−1]

122



and the currently attended encoder state haj . This
model is labeled GRU-HARD.

We find this architecture compelling for mono-
lingual tasks that might require higher faithfulness
with regard to the input. With hard monotonic at-
tention, the translation algorithm can enforce cer-
tain constraints:

1. The end-of-sentence symbol can only be gen-
erated if the hard attention mechanism has
reached the end of the input sequence, en-
forcing full coverage;

2. The 〈STEP〉 symbol cannot be generated once
the end-of-sentence position in the source has
been reached. It is however still possible to
generate content tokens.

This model requires a target sequence with
correctly inserted 〈STEP〉 symbols. For the de-
scribed APE task, using the Longest Common
Subsequence algorithm (Hirschberg, 1977), we
first generate a sequence of match, delete and in-
sert operations which transform the raw MT out-
put (x1, · · ·xTx) into the corrected post-edited se-
quence (y1, · · · yTy)3. Next, we map these opera-
tions to the final sequence of steps and target to-
kens according to the following rules:

• For each matched pair of tokens x, y we pro-
duce symbols: 〈STEP〉 y;

• For each inserted target token y we produce
the same token y;

• For each deleted source token x we produce
〈STEP〉;

• Since at initialization of the model a1 = 1,
i.e. the first encoder state is already attended
to, we discard the first symbol in the new se-
quence if it is a 〈STEP〉 symbol.

4.2 Hard and Soft Attention

While the hard attention model can be used to en-
force faithfulness to the original input, we would
also like the model to be able to look at informa-
tion anywhere in the source sequence which is a
property of the soft attention model.

By re-introducing the conditional GRU cell
with soft attention into the GRU-HARD model
while also inputting the hard-attended encoder

3Similar to GNU wdiff.

state haj , we can try to take advantage of both at-
tention mechanisms. Combining Eq. 2 and Eq. 3,
we get:

sj = cGRUatt
(
sj−1,

[
E[yj−1];haj

]
,C
)
. (4)

The rest of the model is unchanged; the transla-
tion process is the same as before and we use the
same target step/token sequence for training. This
model is called CGRU-HARD.

4.3 Soft Dual-Attention

Neural multi-source models (Zoph and Knight,
2016) seem to be a natural fit for the APE task as
raw MT output and original source language input
are available. Although applications to the APE
problem have been reported (Libovický and Helcl,
2017), state-of-the-art results seem to be missing.

In this section we give details about our dual-
source model implementation. We rename the ex-
isting encoder C to Cmt to signal that the first en-
coder consumes the raw MT output and introduce
a structurally identical second encoder Csrc =
{hsrc1 , . . . ,hsrcTsrc} over the source language. To
compute the decoder start state s0 for the multi-
encoder model we concatenate the averaged en-
coder contexts before mapping them into the de-
coder state space:

s0 = tanh

(
Winit

[∑Tmt
i=1 h

mt
i

Tmt
;
∑Tsrc

i=1 h
src
i

Tsrc

])
.

In the decoder, we replace the conditional GRU
with attention, with a doubly-attentive cGRU cell
(Calixto et al., 2017) over contexts Cmt and Csrc:

sj = cGRU2-att
(
sj−1,E[yj−1],Cmt,Csrc

)
. (5)

The procedure is similar to the original cGRU,
differing only in that in order to compute the con-
text vector cj , we first calculate contexts vectors
cmtj and c

src
j for each context and then concate-

nate4 the results:

4Calixto et al. (2017) combine their two attention models
by modifying their GRU cell to include another set of param-
eters that is multiplied with the additional context vector and
summed in the GRU-components. Formally, both approaches
give identical results, as for concatenation the original pa-
rameters have to grow in size to match the now longer input
vector dimensions. The GRU cell itself does not need to be
modified.

123



s′j =GRU1 (sj−1,E[yj−1]) ,

cmtj =ATT
(
Cmt, s′j

)
=

Tmt∑
i

αijhmti ,

csrcj =ATT
(
Csrc, s′j

)
=

Tsrc∑
i

αijhsrci ,

cj =
[
cmtj ; c

src
j

]
,

sj =GRU2
(
s′j , cj

)
.

This could be easily extended to an arbitrary
number of encoders with different architectures.
During training, this model is fed with a tri-
parallel corpus, and during translation both input
sequences are processed simultaneously to pro-
duce the corrected output. This model is denoted
as M-CGRU.

4.4 Hard Attention with Soft Dual-Attention
Analogously to the procedure described in sec-
tion 4.2, we can extend the doubly-attentive cGRU
to take the hard-attended encoder context as addi-
tional input:

sj = cGRU2-att
(
sj−1,

[
E[yj−1];hmtaj

]
,Cmt,Csrc

)
.

In this formulation, only the first encoder con-
text Cmt is attended to by the hard monotonic at-
tention mechanism. The target training data con-
sists of the step/token sequences used for all pre-
vious hard-attention models. We call this model
M-CGRU-HARD.

5 Experiments and Results

5.1 Training, Development, and Test Data
We perform all our experiments5 with the official
WMT-2016 (Bojar et al., 2016) automatic post-
editing data and the respective development and
test sets. The training data consists of a small
set of 12,000 post-editing triplets (src,mt, pe),
where src is the original English text, mt is
the raw MT output generated by an English-to-
German system, and pe is the human post-edited
MT output. The MT system used to produce the
raw MT output is unknown, so is the original train-
ing data. The task consists of automatically cor-
recting the MT output so that it resembles human

5All experiments in this sections can be reproduced
following the instructions on https://marian-nmt.
github.io/examples/exploration/.

Data set Sentences TER

training set 12,000 26.22
development set 1,000 24.81
test set 2,000 –

artificial-large 4,335,715 36.63
artificial-small 531,839 25.28

Table 1: Statistics for artificial data sets in com-
parison to official training and development data.
Adapted from Junczys-Dowmunt and Grund-
kiewicz (2016).

post-edited data. The main task metric is TER
(Snover et al., 2006) — the lower the better —
with BLEU (Papineni et al., 2002) as a secondary
metric.

To overcome the problem of too little training
data, Junczys-Dowmunt and Grundkiewicz (2016)
— the authors of the best WMT-2016 APE shared
task system — generated large amounts of artifi-
cial data via round-trip translations. The artificial
data has been filtered to match the HTER statistics
of the training and development data for the shared
task and was made available for download6. Ta-
ble 1 summarizes the data sets used in this work.

To produce our final training data set we over-
sample the original training data 20 times and add
both artificial data sets. This results in a total of
slightly more than 5M training triplets. We val-
idate on the development set for early stopping
and report results on the WMT-2016 test set. The
data is already tokenized. Additionally we true-
case all files and apply segmentation into BPE sub-
word units (Sennrich et al., 2016). We reuse the
subword units distributed with the artificial data
set. For the hard-attention models, we create tar-
get training and development files following the
LCS-based procedure outlined in section 4.1.

5.2 Training parameters

All models are trained on the same training data.
Models with single input encoders take only the
raw MT output (mt) as input, dual-encoder mod-
els use raw MT output (mt) and the original source
(pe). The training procedures and model settings
are the same whenever possible:

6The artificial filtered data has been made available
at https://github.com/emjotde/amunmt/wiki/
AmuNMT-for-Automatic-Post-Editing.

124



dev 2016 test 2016
Model TER↓ BLEU↑ TER↓ BLEU↑
WMT-2016 BASELINE-1 (Bojar et al., 2016) 25.14 62.92 24.76 62.11
WMT-2016 BASELINE-2 (Bojar et al., 2016) – – 24.64 63.47
Junczys-Dowmunt and Grundkiewicz (2016) 21.46 68.94 21.52 67.65

Pal et al. (2017) SYMMETRIC – – 21.07 67.87
Pal et al. (2017) RERANKING – – 20.70 69.90

Table 2: Results from the literature for the WMT-2016 APE development and test set.

dev 2016 test 2016
Model TER↓ BLEU↑ TER↓ BLEU↑
CGRU 22.01 68.11 22.27 66.90

GRU-HARD 22.72 66.82 22.72 65.86
CGRU-HARD 22.11 67.82 22.10 67.15

M-CGRU 20.79 69.28 20.69 68.56
M-CGRU × 4 20.10 70.24 19.92 69.40
M-CGRU-HARD 20.83 69.02 20.87 68.14
M-CGRU-HARD × 4 20.08 70.05 20.34 68.96

Table 3: Results for models explored in this work. Models with × 4 are ensembles of four models. The
main WMT 2016 APE shared task metric was TER (the lower the better).

• All embedding vectors consist of 512 units;
the RNN states use 1024 units. We choose
a vocabulary size of 40,000 for all inputs
and outputs. When hard attention models are
trained the maximum sentence length is 100
to accommodate the additional step symbols,
otherwise 50.

• To avoid overfitting, we use pervasive
dropout (Gal and Ghahramani, 2016) over
GRU steps and input embeddings, with
dropout probabilities 0.2, and over source and
target words with probabilities 0.2.

• We use Adam (Kingma and Ba, 2014) as our
optimizer, with a mini-batch size of 64. All
models are trained with Asynchronous SGD
(Adam) on three to four GPUs.

• We train all models until convergence (early-
stopping with a patience of 10 based on
development set cross-entropy cost), sav-
ing model checkpoints every 10,000 mini-
batches. For different models we ob-
served early stopping to be triggered between
600,000 and 900,000 mini-batch updates or
between 8 and 11 epochs.

• The best eight model checkpoints w.r.t. de-
velopment set cross-entropy of each train-
ing run are averaged element-wise (Junczys-
Dowmunt et al., 2016) resulting in new sin-
gle models with generally improved perfor-
mance.

• For the multi-source models we repeat the
mentioned procedure four times with differ-
ent randomly initialized weights.

Training time for one model on four NVIDIA
GTX 1080 GPUs or NVIDIA TITAN X (Pascal)
GPUs is between one and two days, depending on
model complexity. The M-CGRU-HARD model is
the most complex and trains longest.

5.3 Evaluation

Table 2 contains relevant results for the WMT-
2016 APE shared task — during the task and af-
terwards. WMT-2016 BASELINE-1 is the raw un-
corrected MT output. BASELINE-2 is the result
of a vanilla phrase-based Moses system (Koehn
et al., 2007) trained only on the official 12,000
sentences. Junczys-Dowmunt and Grundkiewicz
(2016) is the best system at the shared task. Pal

125



Model TER-pe TER-mt

CGRU 22.27 12.01

GRU-HARD 22.72 9.48
CGRU-HARD 22.10 11.57

M-CGRU 20.69 15.98
M-CGRU × 4 19.92 15.41
M-CGRU-HARD 20.87 13.62
M-CGRU-HARD × 4 20.34 13.34

Table 4: TER w.r.t. the reference compared to TER
w.r.t. the input on test 2016. Lower results for
TER-mt indicate greater similarity to the input.

et al. (2017) SYMMETRIC is the currently best re-
ported result on the WMT-2016 APE test set for
a single neural model (single source), whereas Pal
et al. (2017) RERANKING — the overall best re-
ported result on the test set — is a system com-
bination of Pal et al. (2017) SYMMETRIC with
phrase-based models via n-best list re-ranking.

In Table 3 we present the results for the mod-
els discussed in this work. Unsurprisingly, none
of the single attention models can compete with
the better systems reported in the literature. The
encoder-decoder model with only hard monotonic
attention (GRU-HARD) is the clear loser, while the
comparison between CGRU and CGRU-HARD re-
mains inconclusive. CGRU-HARD seems to gener-
alize slightly better, but would not have been cho-
sen based on the development set performance.

The dual-attention models each outperform the
best WMT-2016 system and the currently reported
best single-model Pal et al. (2017) SYMMETRIC.
The ensembles also beat the system combination
Pal et al. (2017) RERANKING in terms of TER
(not in terms of BLEU though). The simpler dual-
attention model with no hard-attention M-CGRU
reaches slightly better results on the test set than
its counterpart with added hard attention M-CGRU-
HARD, but the situation would have been less clear
if only the development set were used to determine
the best model. The hard-attention model with
dual soft-attention benefits less from ensembling.

6 Analysis

6.1 Faithfulness and Errors

We postulated that the hard-attention models
might have a potential for higher faithfulness.
Since the APE task is a mostly monolingual task,

Model Mod. Imp. Det.

CGRU 1575 871 399

GRU-HARD 1479 783 362
CGRU-HARD 1564 897 371

M-CGRU 1668 1020 379
M-CGRU × 4 1612 1037 322
M-CGRU-HARD 1688 1044 388
M-CGRU-HARD × 4 1672 1074 341

Table 5: Number of test set sentences modified,
improved and deteriorated by each model.

we can verify this by comparing TER scores with
regard to the reference post-edition (TER-pe) and
TER scores with regard to the raw MT output
(TER-mt). The lower the TER-mt score the fewer
changes have been made to the input to arrive
at the output, thus resulting in higher faithful-
ness. Table 4 contains this comparison for the
WMT-2016 APE test set. The hard-attention mod-
els indeed make fewer changes than their soft-
attention counterparts. This difference is espe-
cially dramatic for M-CGRU and M-CGRU-HARD,
where only small differences in TER-pe occur, but
a gap of more than two TER points for TER-mt.
This shows that hard-attention models can reach
similar TER scores to soft-attention models while
performing fewer changes. It might also explain
why ensembling has a lower impact on the hard-
attention models: higher faithfulness means less
variety which results in smaller benefits from en-
sembles.

Table 5 compares the number of modified, im-
proved and deteriorated test set sentences (2000 in
total) for all models. The majority of sentences
is being modified. While the number of deteri-
orated sentences is comparable between models,
the number of improved sentences increases for
the dual-source architectures. Ensembles lower
the number of deteriorated sentences rather than
increasing the number of improved sentences.

6.2 Visualization of Attention Types

Figures 1 and 2 visualize the behavior of the pre-
sented attention variants examined in this work for
the example sentences in Table 6.

For this sentence, the unseen MT system mis-
translated the word “Set” as “festlegen”. The
monolingual mt → pe systems cannot easily cor-
rect the error as the original meaning is lost, but

126



mt Wählen Sie einen Tastaturbefehlssatz im Menü festlegen .
src Select a shortcut set in the Set menu .

CGRU Wählen Sie einen Tastaturbefehlssatz im Menü aus .
GRU-HARD Wählen Sie einen Tastaturbefehlssatz im Menü aus .
CGRU-HARD Wählen Sie einen Tastaturbefehlssatz im Menü aus .
M-CGRU Wählen Sie einen Tastaturbefehlssatz im Menü " Satz " aus .
M-CGRU-HARD Wählen Sie einen Tastaturbefehlssatz im Menü " Satz . "

pe Wählen Sie einen Tastaturbefehlssatz im Menü " Satz . "

Table 6: Example corrections for different models. Only the multi-source models manage to restore the
missing translation for “Set” and insert quotes. The added particle “aus” does not appear in the reference,
but is grammatically correct as well.

W
äh

len

Sie ein
en
Ta

sta
tur

-

be
-

feh
l-
ssa

tz
im M

en
ü
fes

tle
ge

n

. </s
>

Wählen
Sie

einen
Tastatur-

be-
fehl-
ssatz

im
Menü

aus
.

</s>

CGRU

W
äh

len

Sie ein
en
Ta

sta
tur

-

be
-

feh
l-
ssa

tz
im M

en
ü
fes

tle
ge

n

. </s
>

GRU-HARD

W
äh

len

Sie ein
en
Ta

sta
tur

-

be
-

feh
l-
ssa

tz
im M

en
ü
fes

tle
ge

n

. </s
>

CGRU-HARD

Figure 1: Behavior of different monolingual attention models (best viewed in color).

W
äh

len

Sie ein
en
Ta

sta
tur

-

be
-

feh
l-
ssa

tz
im M

en
ü
fes

tle
ge

n

. </s
>

Wählen
Sie

einen
Tastatur-

be-
fehl-
ssatz

im
Menü

“
Satz

”
aus

.

</s>

mt

a
p
e

Se
lec

t
a sh

ort
cu

t

set in the Se
t
me

nu
. </s

>

Wählen
Sie
einen
Tastatur-
be-
fehl-
ssatz
im
Menü
“
Satz
”
aus
.

</s>

src

Figure 2: Attention matrices for dual-soft-attention model M-CGRU (best viewed in color).

127



they improve grammaticality. In Figure 1, we see
how the soft attention model (CGRU) follows the
input roughly monotonically. The monotonic hard
attention model (GRU-HARD) does this naturally.
For CGRU-HARD, it is interesting to see how the
monotonic attention now allows the soft attention
mechanism to look around the input sentence more
freely or to remain inactive instead of following
the monotonic path.

Both {mt, src} → pe systems take advantage
of the src information and improve the input. The
proposed modifications could be accepted as cor-
rect; one matches the reference. The highlighted
rows and columns in Figure 2 show how the orig-
inal source was used to reconstruct the missing
word “Satz” and how both attention mechanisms
interact. The attention over src seems to spend
most time in a “parking” position at the sentence
end unless it can provide useful information; the
attention over mt follows the input closely.

7 Conclusions and Future Work

In this paper we presented several neural APE
models that are equipped with non-standard at-
tention mechanisms and combinations thereof.
Among these, hard attention models have been ap-
plied to APE for the first time, whereas dual-soft
attention models have been proposed before for
APE tasks, but with non-conclusive results.

This is the first work to report state-of-the-
art results for dual-attention models that integrate
full post-edition triplets into a single end-to-end
model. The ensembles of dual-attention models
provide more than 1.52 TER points improvement
over the best WMT-2016 system and 0.7 TER im-
provement over the best reported system combina-
tion for the same test set.

We also demonstrated that while hard-attention
models yield similar results to pure soft-attention
models, they do so by performing fewer changes
to the input. This might be a useful property in
scenarios where conservative edits are preferred.

Acknowledgments

This research was funded by the Amazon Aca-
demic Research Awards program. This project has
received funding from the European Union’s Hori-
zon 2020 research and innovation program un-
der grant 644333 (TraMOOC) and 645487 (Mod-
ernMT). This work was partially funded by Face-
book. The views and conclusions contained herein

are those of the authors and should not be inter-
preted as necessarily representing the official poli-
cies or endorsements, either expressed or implied,
of Facebook.

References
Roee Aharoni and Yoav Goldberg. 2016. Sequence to

sequence transduction with hard monotonic atten-
tion. arXiv preprint arXiv:1611.01487.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
the International Conference on Learning Represen-
tations, San Diego, CA.

Hanna Béchara, Yanjun Ma, and Josef van Genabith.
2011. Statistical post-editing for a statistical MT
system. In Proceedings of the 13th Machine Trans-
lation Summit, pages 308–315, Xiamen, China.

Hanna Béchara, Raphaël Rubino, Yifan He, Yanjun
Ma, and Josef van Genabith. 2012. An evaluation
of statistical post-editing systems applied to RBMT
and SMT systems. In Proceedings of COLING
2012, pages 215–230, Mumbai, India.

Ondrej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck,
Antonio Jimeno Yepes, Philipp Koehn, Varvara
Logacheva, Christof Monz, Matteo Negri, Aure-
lie Neveol, Mariana Neves, Martin Popel, Matt
Post, Raphael Rubino, Carolina Scarton, Lucia Spe-
cia, Marco Turchi, Karin Verspoor, and Marcos
Zampieri. 2016. Findings of the 2016 conference
on machine translation. In Proceedings of the First
Conference on Machine Translation, pages 131–
198, Berlin, Germany. Association for Computa-
tional Linguistics.

Iacer Calixto, Qun Liu, and Nick Campbell. 2017.
Doubly-attentive decoder for multi-modal neural
machine translation. CoRR, abs/1702.01287.

Rajen Chatterjee, Marion Weller, Matteo Negri, and
Marco Turchi. 2015. Exploring the planet of the
APEs: a comparative study of state-of-the-art meth-
ods for MT automatic post-editing. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 156–161, Beijing, China. Association
for Computational Linguistics.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learn-
ing Phrase Representations Using RNN Encoder-
Decoder for Statistical Machine Translation. In
Proc. of Empirical Methods in Natural Language
Processing.

128



Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Advances in Neural Information
Processing Systems 29 (NIPS).

Daniel S. Hirschberg. 1977. Algorithms for the longest
common subsequence problem. J. ACM, 24(4):664–
675.

Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu
Hoang. 2016. Is neural machine translation ready
for deployment? A case study on 30 translation
directions. In Proceedings of the 9th Interna-
tional Workshop on Spoken Language Translation
(IWSLT), Seattle, WA.

Marcin Junczys-Dowmunt and Roman Grundkiewicz.
2016. Log-linear combinations of monolingual and
bilingual neural machine translation models for au-
tomatic post-editing. In Proceedings of the First
Conference on Machine Translation, pages 751–
758.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. In Proceed-
ings of the 3rd International Conference on Learn-
ing Representations (ICLR).

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 177–180.
Association for Computational Linguistics.

Jindrich Libovický and Jindrich Helcl. 2017. Attention
strategies for multi-source sequence-to-sequence
learning. CoRR, abs/1704.06567.

Jindrich Libovický, Jindrich Helcl, Marek Tlustý, On-
drej Bojar, and Pavel Pecina. 2016. CUNI sys-
tem for WMT16 automatic post-editing and multi-
modal translation tasks. In Proceedings of the First
Conference on Machine Translation, pages 646–
654, Berlin, Germany. Association for Computa-
tional Linguistics.

Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan. Association for Computational Linguis-
tics.

Santanu Pal, Sudip Kumar Naskar, Mihaela Vela, Qun
Liu, and Josef van Genabith. 2017. Neural auto-
matic post-editing using prior alignment and rerank-
ing. In Proceedings of the European Chapter of the
Association for Computational Linguistics, pages
349–355.

Santanu Pal, Mihaela Vela, Sudip Kumar Naskar, and
Josef van Genabith. 2015. USAAR-SAPE: An
English–Spanish statistical automatic post-editing

system. In Proceedings of the Tenth Workshop
on Statistical Machine Translation, pages 216–221,
Lisbon, Portugal. Association for Computational
Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-
dra Birch, Barry Haddow, Julian Hitschler, Marcin
Junczys-Dowmunt, Samuel Läubli, Antonio Valerio
Miceli Barone, Jozef Mokry, and Maria Nadejde.
2017. Nematus: a toolkit for neural machine trans-
lation. In Proceedings of the Software Demonstra-
tions of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 65–68, Valencia, Spain. Association for Com-
putational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1715–1725, Berlin, Germany. Asso-
ciation for Computational Linguistics.

Michel Simard, Cyril Goutte, and Pierre Isabelle. 2007.
Statistical phrase-based post-editing. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 508–515, Rochester, New York. Association
for Computational Linguistics.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.

Barret Zoph and Kevin Knight. 2016. Multi-source
neural translation. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 30–34, San Diego, Cali-
fornia. Association for Computational Linguistics.

129


