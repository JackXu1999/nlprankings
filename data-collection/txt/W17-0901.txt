



















































Inducing Script Structure from Crowdsourced Event Descriptions via Semi-Supervised Clustering


Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 1–11,
Valencia, Spain, April 3, 2017. c©2017 Association for Computational Linguistics

Inducing Script Structure from Crowdsourced Event Descriptions
via Semi-Supervised Clustering

Lilian D. A. Wanzare Alessandra Zarcone Stefan Thater Manfred Pinkal
Universität des Saarlandes
Saarland, 66123, Germany

{wanzare,zarcone,stth,pinkal}coli.uni-saarland.de

Abstract

We present a semi-supervised clustering
approach to induce script structure from
crowdsourced descriptions of event se-
quences by grouping event descriptions
into paraphrase sets (representing event
types) and inducing their temporal order.
Our model exploits semantic and posi-
tional similarity and allows for flexible
event order, thus overcoming the rigidity
of previous approaches. We incorporate
crowdsourced alignments as prior knowl-
edge and show that exploiting a small
number of alignments results in a substan-
tial improvement in cluster quality over
state-of-the-art models and provides an ap-
propriate basis for the induction of tempo-
ral order. We also show a coverage study
to demonstrate the scalability of our ap-
proach.

1 Introduction

During their daily social interactions, people make
seamless use of knowledge about standardized
event sequences (scripts) describing types of ev-
eryday activities, or scenarios, such as GOING TO
THE RESTAURANT or BAKING A CAKE (Schank
and Abelson, 1977; Barr and Feigenbaum, 1981).
Script knowledge is often triggered by the broader
discourse context and guides expectations in text
understanding and makes missing events and ref-
erents in a discourse accessible. For example, if
we hear someone say “I baked a cake on Sunday.
I decorated it with buttercream icing!”, our script
knowledge allows us to infer that the speaker must
have mixed the ingredients, turned on the oven,
etc., even if these events are not explicitly men-
tioned. Script knowledge is relevant for the com-
putational modeling of various kinds of cogni-

tive abilities and has the potential to support NLP
tasks such as anaphora resolution (Rahman and
Ng, 2011), discourse relation detection, semantic
role labeling, temporal order analysis, and appli-
cations such as text understanding (Cullingford,
1977; Mueller, 2004), information extraction (Rau
et al., 1989), question answering (Hajishirzi and
Mueller, 2012).

Several methods for the automatic acquisition
of script knowledge have been proposed. Semi-
nal work by Chambers and Jurafsky (2008; 2009)
provided methods for the unsupervised wide-
coverage extraction of script knowledge from
large text corpora. However, texts typically
only mention small parts of a script, banking on
the reader’s ability to infer missing script-related
events. The task is therefore challenging, and the
results are quite noisy.

The work presented in this paper follows the ap-
proach proposed in Regneri et al. (2010) (hence-
forth “RKP”) who crowdsourced scenario de-
scriptions by asking people how they typically
carry out a particular activity. The collected
event sequence descriptions provide generic de-
scriptions of a given scenario (e.g. BAKING A
CAKE) in concise telegram style (Fig. 1a). Based
on these crowdsourced event sequence descrip-
tions or ESDs, RKP extracted high-quality script
knowledge for a variety of different scenarios, in
the form of temporal script graphs (Fig. 1b). Tem-
poral script graphs are partially ordered structures
whose nodes are sets of alternative descriptions
denoting the same event type, and whose edges ex-
press temporal precedence.

While RKP employ Multiple Sequence Align-
ment (MSA) (Durbin et al., 1998), we use a semi-
supervised clustering approach for script struc-
ture induction. The choice of MSA was motivated
by the effect of positional information on the de-
tection of scenario-specific paraphrases: event de-

1



Purchase cake mix
Preheat oven
Grease pan
Carefully follow instructions step by step
Open mix and add required ingredients
Mix well
Pour into prepared pan
Set timer on oven 
Bake cake for required time
Remove cake from oven and cool 
Turn cake out onto cake plate
Apply icing or glaze 

Take out box of cake mix from shelf
Gather together cake ingredients
Get mixing bowl, or spoon or fork
Add ingredients to bowl
Stir together and mix
Use fork to breakup clumps
Preheat oven
Spray pan with non stick or grease
Pour cake mix into pan
Put pan into oven
Bake cake
Remove cake pan when timer goes off
Stick tooth pick into cake to see if done 
Let cake pan cool then remove cake

(a)

choose
recipe

buy
ingred.

add
ingred.

prepare
ingred.

put cake
into oven

get
ingred.

– look up recipe
– find cake recipe
– get your recipe

– take out box of 
ingredients from 
shelf

– gather all cake 
ingredients

– get cake mix

– purchase 
ingredients
– buy cake mix
– buy proper 

ingredients

– place cake into 
oven
– put cake in oven

– stir to combine
– mix well
– mix ingredients 

together in bowls
– stir cake ingredients

– pour cake mix in bowl
– add ingredients to bowl
– add cake ingredients

(b)

Figure 1: Example ESDs (a) and induced script
structure (b) for the BAKING A CAKE scenario
from Wanzare et al. (2016)

scriptions occurring in similar positions in ESDs
tend to denote the same event type. However,
MSA makes far too strong an assumption about
the temporal ordering information in the ESDs. It
does not allow for crossing edges and thus must
assume a fixed and invariable order, while the or-
dering of events in a script is to some degree flex-
ible (e.g., one can preheat the oven before or after
mixing ingredients). We propose clustering as an
alternative method to overcome the rigidity of the
MSA approach, and use a distance measure based
on both semantic similarity and positional similar-
ity information, making our clustering algorithm
sensitive to ordering information, while allowing
for order variation in the scripts.

Clustering accuracy depends on the reliabil-
ity of similarity estimates, but scenario-specific
paraphrase relations are often based on scenario-
specific functional equivalence, which cannot be
easily determined using semantic similarity, even
if complemented with positional information. For
example in the FLYING IN AN AIRPLANE sce-
nario, it is challenging for any semantic simi-
larity measure to predict that walk up the ramp
and board plane refer to the same event, as the
broader discourse context would suggest. To ad-
dress this issue, we propose a semi-supervised ap-
proach, capitalizing on previous work by Klein et

al. (2002). Semi-supervised approaches to cluster-
ing have shown that performance can be enhanced
by incorporating prior knowledge in the form of
a small number of instance-level constraints. We
automatically identify event descriptions that are
likely to cause alignment problems (called out-
liers), crowdsource alignments for these items and
incorporate them as instance-level relational seeds
into the clustering process.

Lastly, a main concern with the approach in
RKP is scalability: temporal script graphs are cre-
ated scenario-wise in a bottom-up fashion. They
represent only fragments of the rich amount of
script knowledge people use in everyday commu-
nication. In this paper we address this concern
with the first assessment of the coverage of exist-
ing script resources, and an estimate of the con-
crete costs for their extension.

2 Data

We will now introduce the resources used in our
study, namely the datasets of ESDs, the gold stan-
dards and the crowdsourced alignments between
event descriptions.

Datasets and gold standards. Three large
crowdsourced collections of activity descriptions
in terms of ESDs are available: the OMICS cor-
pus (Gupta and Kochenderfer, 2004), the SMILE
corpus (Regneri et al., 2010) and DeScript corpus
(Wanzare et al., 2016). Sections 3-4 of this paper
focus on a subset of ESDs for 14 scenarios from
SMILE and OMICS, with on average 29.9 ESDs
per scenario. In RKP, in the follow-up studies by
Frermann et al. (2014) and Modi and Titov (2014)
as well as in the present study, 4 of these scenarios
were used as development set and 10 as test set.

RKP provided two gold standard datasets for
this subset: the RKP paraphrase dataset contains
judgments for 60 event description pairs per sce-
nario, the RKP temporal order dataset contains
60 event description pairs that are separately an-
notated in both directions, for a total of 120 dat-
apoints per scenario. In order to directly evalu-
ate our models for clustering quality, we also cre-
ated a clustering gold standard for the RKP test
set, adopting the experimental setup in Wanzare
et al. (2016): we asked three trained students of
computational linguistics to annotate the scenarios
with gold standard alignments between event de-
scriptions in different ESDs referring to the same

2



event1. Every ESD was fully aligned with ev-
ery other ESD in the same scenario. Based on
the alignments, we derived gold clusters by group-
ing the event descriptions into gold paraphrase sets
(17 clusters per scenario on average, ranging from
10 to 23).

In addition, we used a subset of 10 scenarios
with 25 ESDs each from DeScript, for which Wan-
zare et al. (2016) provided gold clusters, to evalu-
ate our models and to demonstrate that our method
is independent of the specific choice of scenarios.

Crowdsourced alignments. To provide seed
data for the semi-supervised clustering algorithm,
we crowdsourced alignments between event de-
scriptions, following the procedure in Wanzare et
al. (2016). First, we identified challenging cases
of event descriptions (called outliers), which we
expected to be particularly informative and help
improve clustering accuracy. To this purpose, an
(unsupervised) clustering system (Affinity Propa-
gation, see below) was run with varying parameter
settings. Those event descriptions whose nearest
neighbors changed clusters across different runs
of the system were then identified as outliers (see
Wanzare et al. (2016) for more details). A comple-
mentary type of seed data was obtained by select-
ing event descriptions that did not change cluster
membership at all (called stable cases).

In a second step, groups of selected descriptions
(outliers and stable cases) in their original ESD
were presented to workers in a Mechanical Turk
experiment, paired with a target ESD. The work-
ers were asked to select a description in the tar-
get ESD denoting the same script event (e.g. for
BAKING A CAKE: pour into prepared pan→ pour
cake mix into pan). We aimed at collecting two
sets of high-quality seeds based on outliers and
stable cases, respectively, each summing up to 3%
of the links required for a total alignment between
all pairs of scenario-specific ESDs (6% links in to-
tal). To guarantee high quality, we accepted only
items where three (out of up to four) annotators
agree. We checked the annotators’ reliability by
comparing their aligments for stable cases against
the gold standard and rejected the work on 3% of
the annotators.

We collected alignments for 20 scenarios: for
the test scenarios of the SMILE+OMICS dataset,
and for those in the clustering gold standard of De-

1This annotation was rather costly, so each scenario was
aligned by one annotator only.

Script. For the latter, a collection of alignment
data was already available, but considerably dif-
fered in size between scenarios and was in general
to small for our purposes.

3 Model

We first present a semi-supervised clustering
method to induce script events from ESDs using
crowdsourced alignments as seeds (Section 3.1).
In Section 3.2, we describe how we calculate the
underlying distance matrix based on semantic and
positional similarity information. In Section 3.3,
we describe the induction of temporal order for the
script events, which turns the set of script events
into a temporal script graph (TSG).

3.1 Semi-supervised Clustering

We use the crowdsourced alignments between
event descriptions as instance-level relational
seeds for clustering, more specifically as must-link
constraints, requiring that the linked items should
go into one cluster. We incorporate the constraints
into the clustering process following the method in
Klein et al. (2002): that is adapting the input dis-
tance matrix in a pre-processing step, rather than
directly integrating the constraints into the cluster-
ing algorithm. This makes it possible to try dif-
ferent adaptation strategies, independently of the
specific clustering algorithm, and the adapted ma-
trices can be straightforwardly combined with the
clustering algorithm of choice. Klein et al. (2002)
handle must-link constraints by modifying the in-
put matrix D in the following way: if two in-
stances i and j are linked by a must-link con-
straint, then the corresponding entry Di,j is set
to zero, which forces i and j to be grouped into
the same cluster by the underlying clustering algo-
rithm. In addition, distance scores for instances in
the neighborhood of i or j are affected: if the dis-
tance is reduced for one pair of instances, triangle-
inequality may be violated. An all-pairs-shortest-
path algorithm propagates must-link constraints to
other instances in D that restores triangle inequal-
ity.

We use a modified version of this approach.
First, as the crowdsourced information may not
be completely reliable, the clustering algorithm
should be able to override it. We thus do not set
Di,j to zero but rather to a small constant value
d, that is the smallest non-identity distance value
occurring in the matrix. Second, we exploit the

3



inherent transitivity of paraphrase judgments to
derive additional constraints: if (i, j) and (j, k)
are must links, we assume the pair (i, k) to be a
must link as well, and set the distance to d. After
the additional constraints are derived, the all-pairs-
shortest-path algorithm is applied to the input ma-
trix as in Klein et al. (2002).

We experimented with various state-of-art clus-
tering algorithms including Spectral Clustering
and Affinity Propagation (AP). The results pre-
sented in section 4 are based on AP, which proved
to be most stable and provided the best results.

Determining the number of event-clusters.
AP uses a parameter p, which influences the clus-
ter granularity without determining the exact num-
ber of clusters beforehand. There is considerable
variation between the optimal number of clusters
between scenarios, depending on how many event
types are required to describe the respective activ-
ity patterns (see Section 2). We use an unsuper-
vised method for estimating scenario-specific set-
tings of p, using the mean Silhouette Coefficient
(Rousseeuw, 1987). This measure balances op-
timal inter-cluster tightness and intra-cluster dis-
tance, making sure that the elements of each clus-
ter are as similar as possible to each other, and as
dissimilar as possible to the elements of all other
clusters. We run the unsupervised AP algorithm
for each scenario with different settings of p and
select the number resulting in the highest total Sil-
houette Coefficient as the optimal value for p.

3.2 Similarity Features

We now describe how we combine semantic and
positional similarity information to obtain the dis-
tance measure that captures the similarities be-
tween event descriptions.

3.2.1 Semantic Similarity
We inspect different models for word-level sim-
ilarity, as well as methods of deriving phrase-
level semantic similarity from word-level similar-
ity. We use pre-trained Word2Vec (w2v) word
vectors (Mikolov et al., 2013) and vector repre-
sentations (rNN) by Tilk et al. (2016) to obtain
word-level similarity information. The rNN vec-
tors are obtained from a neural network trained on
large amounts of automatically role-labeled text
and capture different aspects of word-level simi-
larity than the w2v representations. We also exper-
imented with WordNet/Lin similarity (Lin, 1998),

but an ablation test (see below) showed that it was
not useful.

To derive phrase-level similarity from word-
level similarity, we employ the following three dif-
ferent empirically informed methods:

Centroid-based similarity. This method de-
rives a phrase-level vector for an event description
by taking the centroid over the word vectors of all
content words in the event description. Similarity
is computed using cosine.

Alignment-based similarity. Following RKP,
we compute a similarity score for a pair of event
descriptions by a linear combination of (a) the
similarity of the head verbs of the two event de-
scriptions and (b) the total score of the align-
ments between all noun phrases in the two descrip-
tions, as computed by the Hungarian algorithm
(Papadimitriou and Steiglitz, 1982).

Vocabulary similarity. We use the approach
in Fernando and Stevenson (2008) to detect
paraphrases and calculate semantic similarities
between two event descriptions p1 and p2 as:

simvocab(~p1, ~p2) =
~p1W ~p2

T

|~p1| |~p2| (1)

where W is an n × n matrix that holds the
similarities between all the words (vocabulary)
in the two event descriptions being compared, n
being the length of the vocabulary, and −→p1 and −→p2
are binary vectors representing the presence or
absence of the words in the vocabulary.

Combining these three methods with the three
word-level similarity measures we obtained a total
of 8 different features2.

3.2.2 Positional Similarity Feature
In addition to the semantic similarity features de-
scribed above, we also used information about the
position in which an event description occurs in
an ESD. The basic idea here is that similar event
descriptions tend to occur in similar (relative) po-
sitions in the ESDs. We set:

simpos(n1, n2) = 1− abs
(

n1
T1
− n2

T2

)
(2)

where n1and n2 are the positions of the two event
description and T1 and T2 represent the total num-
ber of event descriptions in the respective ESDs.

2The centroid method can not be combined with Lin sim-
ilarity.

4



3.2.3 Combination

We linearly combine our 9 similarity features into
a single similarity score, where the weights of the
individual features are determined using logistic
regression trained (10-fold cross validation) on the
paraphrases from the 4 scenarios in the RKP de-
velopment set (see Section 2). We run an ablation
test by considering all possible subsets of features
and using the 10 scenarios in the RKP test set, and
found that the combination of the following five
features performed best:

• centroid-based, alignment-based and vocabu-
lary similarity with w2v vectors

• centroid-based similarity with rNN vectors

• position similarity

3.3 Temporal Script Graphs

After clustering the event descriptions of a
given scenario into sets representing the scenario-
specific event-types, we build a Temporal Script
Graph (TSG) by determining the prototypical or-
der between them. The nodes of the graph are
the event types (clusters); an edge from a clus-
ter E to a cluster E′ indicates that E typically
precedes E′. We induce the edges as follows.
We say that an ESD supports E → E′ if there
are event descriptions e ∈ E and e′ ∈ E′ such
that e precedes e′ in the ESD. In a first step, we
add an edge E → E′ to the graph if there are
more ESDs that support E → E′ than E′ → E.
In a second step, we compute transitive closure,
i.e. we infer an edge E → E′ in cases where
there are clusters E,E′, E′′ such that E → E′′
and E′′ → E′. Finally, we form “arbitrary or-
der” equivalence classes from those pairs of event
clusters which have an equal number of supporting
ESDs in either direction and are not yet connected
by a directed temporal precedence edge.

This is an extension of the concept of a temporal
script graph used in RKP, in order to allow for the
flexible event order assumed by our approach. For
example, the event descrpitions preheat the oven
and mixing ingredients from the BAKING A CAKE
scenario are likely to occur in different clusters,
which are members of the same equivalence class,
expressing that the event descriptions are not para-
phrases, but may occur in any order.

4 Evaluation

4.1 Experimental Setup

We applied different versions of our clustering al-
gorithm to the SMILE+OMICS dataset. In partic-
ular, we explored the influence of positional sim-
ilarity, of the number of seeds (from 0 to 3%), as
well as the proportion of the two seed types (out-
lier vs. stable). As a baseline, we ran the unsu-
pervised clustering algorithm based on semantic
similarity only. We evaluated the models on the
tasks of event-type induction, paraphrase detec-
tion, and temporal order prediction, using the re-
spective gold standard datasets (see Section 2).

Cluster quality. First, we evaluated the quality
of the induced event types (i.e. sets of event de-
scriptions) against the SMILE+OMICS gold clus-
ters. We used the B-Cubed metric (Bagga and
Baldwin, 1998), which is calculated by averaging
per-element precision and recall scores. Amigó
et al. (2009) showed B-Cubed to be the metric
that appropriately captures all aspects of measur-
ing cluster quality.

Paraphrase detection. For direct comparison
with previous work, we tested our model on RKP’s
binary paraphrase detection task. The model clas-
sifies two event descriptions as paraphrases if they
end up in the same cluster. We computed standard
precision, recall and F-score by checking our clas-
sification against the RKP paraphrase dataset.

Temporal order prediction. We tested the qual-
ity of the temporal-order relation of the induced
TSG structures using the RKP temporal order
dataset as follows. For a pair of event descriptions
(e, e′), we assume that (1) e precedes e′, but not
the other way round, if e ∈ E and e′ ∈ E′ for two
different clusters E and E′ such that E → E′. (2)
e precedes e′ and vice versa (that is, both event or-
derings are possible), if e ∈ E and e′ ∈ E′, and E
and E′ are different clusters, but part of the same
equivalence set. In all other cases (i.e. if e and e′

are members of the same cluster), we assume that
precedence does not hold. We computed standard
precision, recall and F-score by checking our clas-
sification against the RKP temporal order dataset.

4.2 Results

The main results of our evaluation are shown in
Table 1. The last three rows show results for

5



Clustering Paraphrasing Temporal Ordering

Model B-Cubed Precision Recall F-score Precision Recall F-score

Regneri et al. (2010) – 0.645 0.833 0.716 0.658 0.786 0.706
Modi and Titov (2014) – – – 0.645 0.839 0.843 0.841
Frermann et al. (2014) – 0.743 0.658 0.689 0.85 0.717 0.776

Baseline: USC 0.525 0.738 0.593 0.646 0.736 0.712 0.722
USC+Position 0.531 0.76 0.623 0.675 0.789 0.766 0.775

SSC+Outlier 0.635 0.781 0.751 0.756 0.858 0.791 0.822
SSC+Mixed 0.655 0.796 0.756 0.764 0.865 0.784 0.822

Table 1: Results on the clustering, paraphrasing and temporal ordering tasks for state-of-the-art models,
our unsupervised (USC) and semi-supervised clustering approaches (SSC)

three of our model variants: unsupervised cluster-
ing with both semantic and positional information
(USC+Position), semi-supervised clustering with
positional information and only outlier constraints
(3%, SSC+Outlier) and with the best-performing
ratio of constraint types (SSC+Mixed, with 2%
outliers and 1% stable cases). Row 4 shows the re-
sults for our unsupervised clustering baseline with
semantic similarity only (USC).

For comparison against previous work, we
added the results on the paraphrase and tempo-
ral ordering tasks of the MSA model by RKP, the
Hierarchical Bayesian model by Frermann et al.
(2014) and the Event Embedding model by Modi
and Titov (2014) (for details about the latter, see
Section 6).

On all three tasks, our best-performing model
is SSC with mixed seed data (SSC+Mixed). Our
best model outperforms the unsupervised model
in RKP by 4.8 points (F-score) on the paraphras-
ing and by 11.6 points on the temporal ordering
task. Interestingly, the performance gain is ex-
clusively due to an increase of precision in both
tasks (15.1 and 20.7 points, respectively). Our
system comes close, but does not beat Modi and
Titov (2014) on their unsupervised state-of-the-art
model for temporal ordering, but outperforms it on
the paraphrase task by almost 12 points F-score.
The use of both positional information and mixed
seed data in the distance measure has substantial
effects on the quality of the results, improving on
the unsupervised clustering baseline and reaching
state-of-the-art results.

4.3 Discussion
The largest and most consistent performance gain
of our model is due to use of crowdsourced align-
ment information.

  

undress
take off all your clothes, take off clothes, take clothes off, 
remove clothes, get undressed, disrobe, peeloff dresses, 
remove the clothes, place clothes in hanger, take off close, 
remove clothing, undress, i take off my clothes 

soap body
scrub soap over your body, wash the rest of the body, 
repeat soaping body, wash the body, wet entire body,  
wash body with soap, soap body with soap or gel,
 put bath soap on body, scrub the body parts, apply soap, 
clean body with them,wash body with soap
soap body, add soap to cloth as needed, i soap up,

 

adjust temp.
adjust temperature to your liking, wait for hot water, 
set handle to correct temperature, check the temperature,
adjust the position of shower, feel for good temperature, 
adjust water temperature, adjust for desired temperature

dress
put clothes back on, get dressed,apply clothes, 
put clothes on,put on clothes, dress in clean clothing

Figure 2: Example clusters output by our model
for TAKING A SHOWER.

Fig. 2 shows example clusters with script-
specific paraphrases captured by our best model
for the TAKING A SHOWER scenario. The model
was able to capture a wide variety of lexical real-
izations of undress, including peel off clothes,
disrobe, remove clothes etc., and similarly for
dress, where we get get dressed, apply clothes,
put on clothes, while these ended up in different
clusters in the baseline model (e.g. get dressed
was clustered together with shampoo hair cluster).
There are still some incorrect classifications (in-
dicated with italics in Fig. 2); note that these are
often near misses rather than blatant errors.

Positional information substantially contributes
to the quality of the derived TSGs. While the
model using semantic similarity features only
put peel off dresses in the dress cluster, po-
sitional similarity helped placing it correctly in

6



the undress cluster, as it appears in the ini-
tial segment of its ESD. Positional information
sometimes also caused wrong clustering deci-
sions: place cloth in hanger typically occurs di-
rectly after undressing, and thus ended up in
the undress cluster.

As described above, we collected alignments
for outliers and for stable cases and tried sev-
eral outlier-to-stable ratios. Outliers were much
more effective than stable cases, as they improved
recall by adjusting cluster boundaries to include
scenario-specific functional paraphrases that were
semantically dissimilar. Interestingly, adding a
small number of stable cases leads to a slight im-
provement, but adding more stable cases leads to
a performance drop, and using only stable cases
does not improve the unsupervised baseline at all.
Fig. 3 shows how the model improves as more
constraints are added.

We tried to reduce the amount of manual anno-
tation in several ways. The decision to derive ad-
ditional must-links using transitivity paid off: F-
score consistently improves by about 1 point F-
score. To further increase the set of seeds, we ex-
perimented with propagating the links to nearest
neighbors of aligned event descriptions, but did
not see an improvement. Also, we tried to use
alignments obtained by majority vote, which how-
ever led to a performance drop, showing that using
high quality seeds is crucial.

To make sure that our results are not dependent
on the selection of a specific scenario set, we eval-
uated our model also on the DeScript gold clusters.
The results were comparable: B-Cubed improved
from 0.551 (RKP: 0.525) to 0.662 (RKP: 0.655).
As the DeScript corpus provides 100 ESDs per
scenario, we were also able to test whether an in-
creased number of input ESDs also improves clus-
tering performance. We observed no effect with
50 ESDs compared to our model using 25 ESDs,
and only a slight (less than 1 point) improvement
with the full 100 ESDs dataset.

A leading motivation to use clustering instead of
MSA was the opportunity to model flexible event
order in script structures. Our expectations were
confirmed by the evaluation results. A closer look
at the induces TSGs (as shown by the example
TSG in Fig. 4), suggests that our system makes
extensive use of the option of flexible event order-
ing.

0.0 0.5 1.0 1.5 2.0 2.5 3.0

% constrained data

0.60

0.65

0.70

0.75

0.80

0.85

0.90

SSC_F-score

SSC_Precision

SSC_Recall

RKP_F-score

USCBaseline_F-score

Figure 3: Paraphrase detection results for RKP, for
our Unsupervised baseline (USC) and for our best
Semi-supervised model (SSC+Mixed)

enter bathroom⇒(turn on shower↔ undress)
⇒(adjust temp.↔ turn off water)⇒get in shower
⇒(soap body↔ close curtains)⇒shampoo hair
⇒(wash hair↔ wash body↔ shave)⇒rinse
⇒exit shower⇒dry off⇒dress

Figure 4: Example TSG for TAKING A SHOWER.
The arrows stand for default temporal precedence,
the parentheses enclose equivalence classes ex-
pressing arbitrary temporal order.

5 Costs and Coverage

We have demonstrated that semi-supervised clus-
tering enables the extraction of script knowledge
with substantially higher quality than existing
methods. But how does the method scale? Can we
expect to obtain a script knowledge database with
sufficiently wide coverage at reasonable costs?

The process of script extraction requires crowd-
sourced data in terms of (1) ESDs and (2) seed
alignments. To complete 3%+3% high-quality
alignments for the 10 DeScript scenarios via Me-
chanical Turk (that is, 3% stable cases and 3% out-
liers), workers spent a total of 37.5 hours, with
an average of 3.75 hrs per scenario, ranging from
2.5 (GOING GROCERY SHOPPING) to 7.52 hrs
(BAKING A CAKE)3. It took on average 2.78 hrs
to collect 25 scenario-specific ESDs, that is 6.53
hrs of data acquisition time per scenario.

The costs per scenario are moderate. But how
many scenarios must be modeled to achieve suf-

3We used the DeScript scenarios as reference because
they come with equal numbers of ESDs, while the ESD sets
in the SMILE+OMICS corpus vary considerably in size.

7



Jessica needs milk. Jessica wakes up and wants
to eat breakfast. She grabs the cereal and pours
some into a bowl. She looks in the fridge for
milk. There is no milk in the fridge so she can’t
eat her breakfast. She goes to the store to buy
some milk comes home and eats breakfast.
MAKE BREAKFAST: C
GOING GROCERY SHOPPING: P

Figure 5: Example ROC-story with scenario an-
notation.

ficient coverage for the analysis of script knowl-
edge in natural-language texts? Answering this
question is not trivial, as scenarios vary consid-
erably in granularity and it is not trivial that the
type of script knowledge we model can capture all
kinds of event structures, even in narrative texts.
In order to provide a rough estimate of coverage
for the currently existing script material, we car-
ried out a simple annotation study on the recently
published ROC-stories database (Mostafazadeh et
al., 2016a). The database consists of 50,000 short
narrative texts, collected via Mechanical Turk.
Workers were asked to write a 5-sentence length
story about an everyday commonsense event, and
they were encouraged to write about “anything
they have in mind” to guarantee wide distribution
across topics.

For our annotation study, we merged the avail-
able datasets containing crowdsourced ESD col-
lections (i.e. OMICS, SMILE, and DeScript), ex-
cluding two extremely general scenarios (GO OUT-
SIDE, CHILDHOOD), which gives us a total of 226
different scenarios.

We randomly selected 500 of the ROC-stories
and asked annotators to determine for each story
which scenario (if any) was centrally addressed
and which scenarios were just referred to or par-
tially addressed with at least one event mention,
and to label them with “C” and “P”, respectively.
See an example story with its annotation in Fig. 5.

Each story was annotated by three students of
computational linguistics. To facilitate annotation,
the stories were presented alongside ten scenar-
ios whose ESDs showed strongest lexical overlap
with the story (calculated as tf-idf). However, an-
notators were expected to consider the full sce-
nario list4. The three annotations were merged us-

4We are aware that this setup may bias participants toward
finding a scenario from our collection, leading to an increase

ing majority vote. Cases without a clear majority
vote containing one single “C” assignment were
inspected and adjudicated by the authors of the pa-
per.

26.4% of the stories were judged to centrally re-
fer to one of the scenarios5. Although this per-
centage cannot be directly translated to coverage
values, it indicates that the extraction method pre-
sented in this paper has the strong potential to
provide a script knowledge resource with reason-
able costs, which can substantially contribute to
the task of text understanding.

6 Related Work

Following the seminal work of Chambers and
Jurafsky (2008) and (2009) on the induction of
script-like narrative schemas from large, unla-
beled corpora of news articles, a series of mod-
els have been presented for improving the induc-
tion method or explore alternative data sources for
script learning. Gordon (2010) mined common-
sense knowledge from stories describing events in
day-to-day life. Jans et al. (2012) studied differ-
ent ways of selecting event chains and used skip-
grams for computing event statistics. Pichotta and
Mooney (2014) employed richer event representa-
tions, exploiting the interactions between multiple
arguments to extract event sequences from a large
corpus. Rahimtoroghi et al. (2016) learned contin-
gency relations between events from a corpus of
blog posts. All these approaches aim at high re-
call, resulting in a large amount of wide-coverage,
but noisy schemas.

Abend et al. (2015) proposed an edge-factored
model to determine the temporal order of events in
cooking recipes, but their model is limited to sce-
narios with an underlying linear order of events.
Bosselut et al. (2016) induce prototypical event
structure in an unsupervised way from a large col-
lection of photo albums with time-stamped images
and captions. This method is however limited by
the availability of albums for “special” events such
as WEDDING or BARBECUE, in contrast to every-
day, trivial activites such as MAKING COFFEE or

in recall. However, they had the option to label stories where
they felt a scenario was only partially addressed in a differ-
ent way, thus setting these cases apart from those where the
scenario was centrally addressed.

5While we take the judgment about the “C” class to be
quite reliable (24.8% qualified by majority vote, only 1.6 %
were added via adjudication), there was considerable confu-
sion about the “P” label. So we decided not to use the “P”
label at all.

8



GOING TO THE DENTIST. Mostafazadeh et al.
(2016b) presented the ROC-stories, a dataset of
c.a. 50.000 crowdsourced short commonsense ev-
eryday story. They propose to use it for the evalu-
ation of script knowledge models, and it may also
turn out to be a valuable resource for script learn-
ing, although to our knowledge this has not yet
been attempted.

Closest to our approach is the work by RKP
and subsequent work by Frermann et al. (2014)
and Modi and Titov (2014). All these employ
the same SMILE+OMICS dataset for evaluation,
which we also used to allow for a direct compar-
ison. Frermann et al. (2014) present a Bayesian
generative model for joint learning of event types
and ordering constraints. Their model promis-
ingly shows that flexible event order in scripts can
be suitably modelled. Modi and Titov (2014) fo-
cussed mainly on event ordering between script-
related predicates, using distributed representa-
tions of predicates and arguments induced by a sta-
tistical model. They obtained paraphrase sets as a
by-product, namely by creating an event timeline
and grouping together event mentions correspond-
ing to the same interval.

7 Conclusions

This paper presents a clustering-based approach to
inducing script structure from crowdsourced de-
scriptions of scenarios. We use semi-supervised
clustering to group individual event descriptions
into paraphrase sets representing event types, and
induce a temporal order among them. Crowd-
sourced alignments between event descriptions
proved highly effective as seed data. On a para-
phrase task, our approach outperforms all previous
proposals, while still performing very well on the
task of temporal order prediction. A study on the
ROC-stories suggests that a model of script knowl-
edge created with our method can cover a large
fraction of event structures occurring in topically
unrestricted narrative text, thus demonstrating the
scalability of our approach.

Acknowledgments

This research was funded by the German Research
Foundation (DFG) as part of SFB 1102 ‘Informa-
tion Density and Linguistic Encoding’.

References
Omri Abend, Shay B. Cohen, and Mark Steedman.

2015. Lexical event ordering with an edge-factored
model. In Proceedings of the 2015 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 1161–1171, Denver, Colorado,
May–June. Association for Computational Linguis-
tics.

Enrique Amigó, Julio Gonzalo, Javier Artiles, and Fe-
lisa Verdejo. 2009. A comparison of extrinsic
clustering evaluation metrics based on formal con-
straints. Information retrieval, 12(4):461–486.

Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563–566.

Avron Barr and Edward A. Feigenbaum. 1981. Frames
and scripts. In The Handbook of Artificial Intelli-
gence, volume 3, pages 216–222. Addison-Wesley,
California.

Antoine Bosselut, Jianfu Chen, David Warren, Han-
naneh Hajishirzi, and Yejin Choi. 2016. Learning
prototypical event structure from photo albums. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1769–1779, Berlin, Germany,
August. Association for Computational Linguistics.

Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08: HLT, pages 789–797, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.

Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 602–610, Suntec,
Singapore, August. Association for Computational
Linguistics.

Richard E. Cullingford. 1977. Script Applica-
tion: Computer Understanding of Newspaper Sto-
ries. Ph.D. thesis, Yale University.

Richard Durbin, Sean Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological sequence
analysis: probabilistic models of proteins and nu-
cleic acids. Cambridge University Press, Cam-
bridge, UK.

Samuel Fernando and Mark Stevenson. 2008. A se-
mantic similarity approach to paraphrase detection.
In Proceedings of the 11th Annual Research Collo-
quium of the UK Special Interest Group for Compu-
tational Linguistics.

9



Lea Frermann, Ivan Titov, and Manfred Pinkal. 2014.
A hierarchical bayesian model for unsupervised in-
duction of script knowledge. In Proceedings of the
14th Conference of the European Chapter of the
Association for Computational Linguistics, pages
49–57, Gothenburg, Sweden, April. Association for
Computational Linguistics.

Andrew S. Gordon. 2010. Mining commonsense
knowledge from personal stories in internet we-
blogs. In Proceedings of the First Workshop on Au-
tomated Knowledge Base Construction, Grenoble,
France.

Rakesh Gupta and Mykel J. Kochenderfer. 2004.
Common sense data acquisition for indoor mobile
robots. In Proceedings of the 19th National Con-
ference on Artificial intelligence, pages 605–610.
AAAI Press.

Hannaneh Hajishirzi and Erik T. Mueller. 2012. Ques-
tion answering in natural language narratives using
symbolic probabilistic reasoning. In Proceedings of
the Twenty-Fifth International Florida Artificial In-
telligence Research Society Conference, pages 38–
43.

Bram Jans, Steven Bethard, Ivan Vulić, and Marie-
Francine Moens. 2012. Skip N-grams and ranking
functions for predicting script events. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 336–344, Avignon, France, April. Association
for Computational Linguistics.

Dan Klein, Sepandar D. Kamvar, and Christopher D.
Manning. 2002. From instance-level constraints to
space-level constraints: Making the most of prior
knowledge in data clustering. In Proceedings of the
Nineteenth International Conference on Machine
Learning, pages 307–314.

Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In In Proceedings of the 15th In-
ternational Conference on Machine Learning, pages
296–304.

Tomas Mikolov, Kai Chen, Greg S. Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word rep-
resentations in vector space. In Proceedings of the
International Conference on Learning Representa-
tions.

Ashutosh Modi and Ivan Titov. 2014. Inducing neu-
ral models of script knowledge. In Proceedings of
the Eighteenth Conference on Computational Nat-
ural Language Learning, pages 49–57, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,
Pushmeet Kohli, and James Allen. 2016a. A cor-
pus and cloze evaluation for deeper understanding
of commonsense stories. In Proceedings of the

2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 839–849, San
Diego, California, June. Association for Computa-
tional Linguistics.

Nasrin Mostafazadeh, Lucy Vanderwende, Wen-tau
Yih, Pushmeet Kohli, and James Allen. 2016b.
Story cloze evaluator: Vector space representation
evaluation by predicting what happens next. In Pro-
ceedings of the 1st Workshop on Evaluating Vector
Space Representations for NLP, pages 24–29.

Erik T. Mueller. 2004. Understanding script-based sto-
ries using commonsense reasoning. Cognitive Sys-
tems Research, 5(4):307–340.

Christos H. Papadimitriou and Kenneth Steiglitz.
1982. Combinatorial Optimization: Algorithm und
Complexity. Dover Publications, Mineola, NY.

Karl Pichotta and Raymond Mooney. 2014. Statisti-
cal script learning with multi-argument events. In
Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 220–229, Gothenburg, Sweden,
April. Association for Computational Linguistics.

Elahe Rahimtoroghi, Ernesto Hernandez, and Marilyn
Walker. 2016. Learning fine-grained knowledge
about contingent relations between everyday events.
In Proceedings of the 17th Annual Meeting of the
Special Interest Group on Discourse and Dialogue,
pages 350–359.

Altaf Rahman and Vincent Ng. 2011. Narrowing the
modeling gap: a cluster-ranking approach to coref-
erence resolution. Journal of Artificial Intelligence
Research, 40:469–521.

Lisa F. Rau, Paul S. Jacobs, and Uri Zernik. 1989. In-
formation extraction and text summarization using
linguistic knowledge acquisition. Information Pro-
cessing & Management, 25(4):419–428.

Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 979–988, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.

Peter J. Rousseeuw. 1987. Silhouettes: A graphical aid
to the interpretation and validation of cluster analy-
sis. Journal of Computational and Applied Mathe-
matics, 20:53 – 65.

Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals, and Understanding: An Inquiry into
Human Knowledge Structures. Erlbaum, Hillsdale,
NJ.

Ottokar Tilk, Vera Demberg, Asad Sayeed, Dietrich
Klakow, and Stefan Thater. 2016. Event partici-
pant modelling with neural networks. In Proceed-
ings of the 2016 Conference on Empirical Methods

10



in Natural Language Processing, pages 171–182,
Austin, Texas, November. Association for Compu-
tational Linguistics.

Lilian D. A. Wanzare, Alessandra Zarcone, Stefan
Thater, and Manfred Pinkal. 2016. A crowd-
sourced database of event sequence descriptions for
the acquisition of high-quality script knowledge.
In N. Calzolari (Conference Chair), K. Choukri,
T. Declerck, S. Goggi, M. Grobelnik, B. Maegaard,
J. Mariani, H. Mazo, A. Moreno, J. Odijk, and
S. Piperidis, editors, Proceedings of the Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC 2016). European Language Re-
sources Association (ELRA).

11


