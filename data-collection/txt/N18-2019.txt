



















































Leveraging Intra-User and Inter-User Representation Learning for Automated Hate Speech Detection


Proceedings of NAACL-HLT 2018, pages 118–123
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Leveraging Intra-User and Inter-User Representation Learning for
Automated Hate Speech Detection

Jing Qian, Mai ElSherief, Elizabeth M. Belding, William Yang Wang
Department of Computer Science

University of California, Santa Barbara
Santa Barbara, CA 93106 USA

{jing qian, mayelsherif, ebelding, william}@cs.ucsb.edu

Abstract

Hate speech detection is a critical, yet chal-
lenging problem in Natural Language Process-
ing (NLP). Despite the existence of numerous
studies dedicated to the development of NLP
hate speech detection approaches, the accu-
racy is still poor. The central problem is that
social media posts are short and noisy, and
most existing hate speech detection solutions
take each post as an isolated input instance,
which is likely to yield high false positive and
negative rates. In this paper, we radically im-
prove automated hate speech detection by pre-
senting a novel model that leverages intra-user
and inter-user representation learning for ro-
bust hate speech detection on Twitter. In ad-
dition to the target Tweet, we collect and ana-
lyze the user’s historical posts to model intra-
user Tweet representations. To suppress the
noise in a single Tweet, we also model the sim-
ilar Tweets posted by all other users with rein-
forced inter-user representation learning tech-
niques. Experimentally, we show that leverag-
ing these two representations can significantly
improve the f-score of a strong bidirectional
LSTM baseline model by 10.1%.

1 Introduction

The rapid rise in user-generated web content has
not only yielded a vast increase in information ac-
cessibility, but has also given individuals an easy
platform on which to share their beliefs and to
publicly communicate with others. Unfortunately,
this has also led to nefarious uses of online spaces,
for instance for the propagation of hate speech.

An extensive body of work has focused on
the development of automatic hate speech clas-
sifiers. A recent survey outlined eight cate-
gories of features used in hate speech detec-
tion (Schmidt and Wiegand, 2017): simple sur-
face (Warner and Hirschberg, 2012; Waseem and
Hovy, 2016), word generalization (Warner and

Figure 1: Our hate speech classifier. In contrast to ex-
isting methods that focus on a single target Tweet as in-
put (center), we incorporate intra-user (right) and inter-
user (left) representations to enhance performance.

Hirschberg, 2012; Zhong et al., 2016), sentiment
analysis (Van Hee et al., 2015), lexical resources
and linguistic features (Burnap and Williams,
2016), knowledge-based features (Dinakar et al.,
2012), meta-information (Waseem and Hovy,
2016), and multi-modal information (Zhong et al.,
2016). Closely related to our work is research that
leverages user attributes in the classification pro-
cess such as history of participation in hate speech
and usage of profanity (Xiang et al., 2012; Dadvar
et al., 2013). Both Xiang et al. (2012) and Dad-
var et al. (2013) collect user history to enhance
detection accuracy. The former requires the user
history to be labeled instances. However, label-
ing user history requires significant human effort.
The latter models the user with manually selected
features. In contrast, our approach leverages unla-
beled user history to automatically model the user.

In this paper, we focus on augmenting hate
speech classification models by first performing

118



Figure 2: The overview of our proposed model. t is the input target Tweet, z denotes intra-user Tweets, and xa is
the selected inter-user Tweet. rie is the inter-user representation, ria is the intra-user representation, and rta is the
representation of the target Tweet. These three branches respectively correspond to the three branches illustrated
in Figure 1. yi is the prediction at the time step i and si is the state input for the agent at the time step i. The
computing process is detailed in Section 2.3

representation learning to model user history with-
out supervision. The hypothesis is that, by analyz-
ing a corpus of the user’s past Tweets, our sys-
tem will better understand the language and be-
havior of the user, leading to better hate speech
detection accuracy. Another issue is that using a
single Tweet as input is often noisy for any ma-
chine learning classifier. For example, the Tweet
“I’m not sexist but I can not stand women com-
mentators” is actually an instance of hate speech,
even though the first half is misleading. To min-
imize noise, we also consider semantically sim-
ilar Tweets posted by other users. To do so,
we propose a reinforced bidirectional long short-
term memory network (LSTM) (Hochreiter and
Schmidhuber, 1997) to interactively leverage the
similar Tweets from a large Twitter dataset to en-
hance the performance of the hate speech classi-
fier. An overview of our approach is shown in Fig-
ure 1. The main contributions of our work are:

• We provide a novel perspective on hate
speech detection by modeling intra-user
Tweet representations.

• To improve robustness, we leverage similar
Tweets from a large unlabeled corpus with re-
inforced inter-user representations.

• We integrate target Tweets, intra-user and
inter-user representations in a unified frame-
work, outperforming strong baselines.

2 Approach

Figure 2 illustrates the architecture of our model.
It includes three branches, whose details will be
described in the following subsections.

2.1 Bidirectional LSTM

Given a target Tweet, the baseline approach is to
feed the embeddings of the Tweet into a bidirec-
tional LSTM network (Hochreiter and Schmidhu-
ber, 1997; Zhou et al., 2016; Liu et al., 2016) to
obtain the prediction. This is shown in the middle
branch in Figure 1. However, this method is likely
to fail when the target tweet is noisy or the critical
words for making predictions are out of vocabu-
lary.

2.2 Intra-User Representation

The baseline approach does not fully utilize avail-
able information, such as the user’s historical
Tweets. In our approach, we collect the user’s his-
torical posts through the Twitter API. For a tar-
get Tweet t, suppose we collect m Tweets posted
by this user: Zt = {z1, z2, ..., zm}. These

119



intra-user Tweets are fed into a pre-trained model
to obtain an intra-user representation. The pre-
trained model has the same structure as the base-
line model. This is shown in the right branch in
Figures 1 and 2. The intra-user representation is
then combined with the baseline branch for the fi-
nal prediction. The computation process is:

ota(t) = fta(t,0) (1)

rta(t) = lta(σ(ota(t))) (2)

oia(zj) = fia(zj ,0) (3)

ria(t) = σ(
m∑

j=1

lia(σ(oia(zj)))) (4)

where fta is the bi-LSTM of the baseline branch;
ota is the output of the bi-LSTM; and lta, lia are
linear functions. Similarly, fia is the bi-LSTM of
the intra-user branch and oia is the output. rta is
the output prediction of the baseline branch. ria
is the intra-user representation, and σ is the non-
linear activation function.

2.3 Inter-User Representation

In addition to the user history, the Tweets that are
semantically similar to the target Tweet can also be
utilized to suppress noise in the target Tweet. We
collect similar Tweets from large unlabeled Tweet
setU by Locality Sensitive Hashing (LSH) (Indyk
and Motwani, 1998; Gionis et al., 1999). Since the
space of all Tweets is enormous, we use LSH to
efficiently reduce the search space. For each target
Tweet t, we use LSH to collect n nearest neighbors
of t in U : x1, x2, ..., xn. These n Tweets form the
inter-user Tweet set for t: Xt = {x1, x2, ..., xn}.

Due to the size of this set, a policy gradient-
based deep reinforcement learning agent is trained
to interactively fetch inter-user Tweets from Xt.
The policy network consists of two layers as
shown in the middle part of Figure 2 and the policy
network is trained by the REINFORCE algorithm
(Williams, 1992). At each time step i, the action of
the agent is to select one Tweet xa from Xt. xa is
then fed into a bi-LSTM followed by a linear layer.
The result is combined with the intra-user repre-
sentation and the baseline prediction (the right and
the middle branch in Figures 1 and 2) to get the
prediction at time step i. At each time step, the bi-
LSTM layer that encodes the selected inter-user is
initialized with the output hidden state of the last
time step. The number of time steps for each tar-
get Tweet is set to be a fixed number T so that

Algorithm 1 Training Algorithm
1: for t in training set do
2: collect Xt and Zt;
3: compute intra-user representation ria(t);
4: end for
5: initialize parameters θp of the policy network;
6: initialize parameters θe of the other nets;
7: for epoch = 1, E do
8: for t in training set do
9: compute ota(t), rta(t);

10: compute the raw prediction y′(t);
11: compute b(Xt);
12: xa = t;
13: compute oie(t);
14: initialize the state s(t)0;
15: for i = 1, T do
16: agent select action by �− greedy;
17: update xa;
18: compute oie(t), rie(t);
19: compute y(t)i and s(t)i;
20: compute the reward vi(t);
21: end for
22: apply REINFORCE to update θp;
23: update θe on the loss L(θe) = e(y(t)T , y∗);
24: end for
25: end for

the agent will terminate after T fetches. The final
prediction occurs at the last time step. The com-
putation is shown by the following equations.

oie(xa)i,hie(xa)i = fie(xa, hie(xb)i−1) (5)

rie(xa)i = lie(σ(oie(xa)i)) (6)

y′(t) = σ(lc(rta(t)⊕ ria(t))) (7)
y(t)i = σ(lc(rie(t)⊕ rta(t)⊕ ria(t))) (8)

where xb is the selected inter-user Tweet at time
step i − 1. fie is the bi-LSTM of the inter-user
branch. oie and hie are the output and the hidden
state. lc is a linear function. rie is the inter-user
representation. y′ is the prediction made without
the inter-user branch and y is the prediction made
with the inter-user branch. The symbol ⊕ means
concatenation. The subscript i denotes time step i.

The state at each time step for the agent is the
concatenation of encoded inter-user Tweets, the
output of the Bi-LSTM in the inter-user branch
and the baseline branch, together with the intra-
user representation in the intra-user branch (the
dotted arrows in Figure 2). Each inter-user Tweet
xj in Xt is encoded by the bi-LSTM of the inter-
user branch (the dotted arrow through the Bi-
LSTM of the inter-user branch).

b(xj) = fie(xj ,0) (9)

s(t)i[j]=oie(xa)i ⊕ b(xj)⊕ ota(t)⊕ria(t) (10)
b is the output of the bi-LSTM of the inter-user
branch. In order to differentiate with oie men-

120



tioned above, we use b. s(t)i[j] is the jth row of
the state at time step i.

By using reinforcement learning, the state for
the agent is updated after each fetch of the inter-
user Tweet. Thus, the agent can interactively make
selections and update the inter-user representa-
tions step by step. The reward vi for the agent
at time step i is based on the original prediction
without the agent and the prediction at the last time
step with the agent. The computation is shown as:

q(t)i = e(y
′(t), y∗)− e(y(t)T , y∗) (11)

v(t)i =





α ∗ q(t)i if y′(t)! = y∗
q(t)i else if y(t)T ! = y∗

0 otherwise

(12)

where e is the loss function; q(t) is the basic re-
ward; and v(t)i is the modified reward at time step
i. α is a positive number used to amplify the re-
ward when the original classification is incorrect.
The intuition of this reward is to train the agent to
be able to correct the misclassified Tweets. When
the original prediction and the last prediction are
both correct, the reward is set to 0 to make the
agent focus on the misclassified instances.

The complete training process is shown in Al-
gorithm 1. Before the training, the intra-user
Tweets and inter-user Tweets are collected for
each target Tweet. Then intra-user representations
are computed, followed by the computation for
initializing the environment and state for the agent.
Next, the agent’s actions, state updates, prediction,
and reward are computed. Finally, the parameters
are updated.

3 Experiments

3.1 Experimental Settings

Dataset: We use the dataset published by Waseem
and Hovy (2016). This dataset contains 16,907
Tweets. The original dataset only contains the
Tweet ID and the label for each Tweet. We ex-
pand the dataset with user ID and Tweet text. Af-
ter deleting the Tweets that are no longer acces-
sible, the dataset we use contains 15,781 Tweets
from 1,808 users. The published dataset has three
labels: racism, sexism and none. Since we con-
sider a binary classification setting, we union the
first two sets. In the final dataset, 67% are labeled
as non-hate speech, and 33% are labeled as hate
speech. 1000 Tweets are randomly selected for

Method Prec. Rec. F1
SVM .793 .656 .718
Logistic Regression .782 .611 .686
Bi-LSTM + attention .760 .665 .710
CNN-static .701 .707 .703
CNN-non-static .743 .699 .720
N-gram .729 .777 .739
Bi-LSTM .672 .737 .703
+ Intra. Rep. .772 .749 .760
+ Intra.+ Randomized Inter. Rep. .773 .764 .768
+ Intra.+ Reinforced Inter. Rep. .775 .773 .774

Table 1: Experimental results. Prec.: precision. Rec.:
recall. F1: F measure. Bi-LSTM: the baseline bidi-
rectional LSTM model. Bi-LSTM + attention: an at-
tentional bidirectional LSTM model. The experimen-
tal settings of the last three rows are illustrated in Sec-
tion 3.1. + Intra. Rep.: the model consists of the tar-
get Tweet branch and the intra-user branch. + Intra.
+ Randomized Inter. Rep. incorporates randomly se-
lected inter-user Tweets while + Intra. + Reinforced
Inter. Rep. further incorporates the reinforced inter-
user branch. The best results are in bold.

testing and the remaining 14,781 Tweets are for
training.
Baseline: The baseline model is a bi-LSTM
model. The input for the model is the word embed-
dings of the target Tweet. The word embedding is
of size 200. The hidden size of the LSTM is 64.
The optimizer is Adam and we use mini-batches of
size 25. The word embedding model is pre-trained
on a Tweet corpus containing 3,433,513 Tweets.
Intra-user Representation Learning: Based on
the target Tweet, we collect at most 400 Tweets
posted by the same user, with the target Tweet
removed. The baseline branch and the intra-user
branch are combined via a linear layer.
Combining with Inter-user Representation:
The inter-user Tweet set is collected from the
dataset via Locality Sensitive Hashing (LSH). In
our experiments, we use a set size of either 50, 100
or 200 Tweets. At each time step, one Tweet is se-
lected from the inter-user Tweet set by the policy
agent. We also experimented with a second set-
ting, in which we replace the agent by random se-
lection. At each time step, an inter-user Tweet is
randomly selected from X and fed into the inter-
user branch.

3.2 Results

We compare the above settings with six clas-
sification models: Supported Vector Machine
(SVM) (Suykens and Vandewalle, 1999), Logistic
Regression, attentional BI-LSTM, two CNN mod-

121



els by Kim (2014), and a N-gram model (Waseem
and Hovy, 2016). We evaluate these models on
three metrics: precision, recall and F1 score. The
results are shown in Table 1. We report results for
|U | = 100 in Table 1, as results with sizes 50 and
200 are similar. We find that leveraging the intra-
user information helps reduce false positives. The
performance is further improved when integrating
our model with inter-user similarity learning. Our
results show that selection by the policy gradient
agent is slightly better than random selection, and
we hypothesize the effect would be more salient
when working with a larger unlabeled dataset. The
McNemar’s test shows that our model gives sig-
nificantly better (at p < 0.01) predictions than the
baseline bi-LSTM and attentional bi-LSTM.

3.3 Error Analysis

There are two types of hate speech that are mis-
classified. The first type contains rare words and
abbreviations, e.g. FK YOU KAT AND ANDRE!
#mkr. Such intentional misspellings or abbrevia-
tions are highly varied, making it difficult for the
model to learn the correct meaning. The second
type of hate speech is satire or metaphor, e.g. Con-
gratulations Kat. Reckon you may have the whole
viewer population against you now #mkr. Satire
and metaphors are extremely difficult to recog-
nize. In the above two cases, both the baseline
branch and the inter-user branch can be unreliable.

4 Conclusion

In this work, we propose a novel method for hate
speech detection. We use bi-LSTM as the base-
line method. However, our framework can eas-
ily augment other baseline methods by incorpo-
rating intra-user and reinforced inter-user repre-
sentations. In addition to detecting potential hate
speech, our method can be applied to help detect
suspicious social media accounts. Considering the
relationship between online hate speech and real-
life hate actions, our solution has the potential to
help analyze real-life extremists and hate groups.
Furthermore, intra-user and inter-user representa-
tion learning can be generalized to other text clas-
sification tasks, where either user history or a large
collection of unlabeled data are available.

Acknowledgments

The first author is supported by the Chancellor’s
Fellowship. We gratefully acknowledge the sup-

port of NVIDIA Corporation with the donation of
one Titan X Pascal GPU used for this research.

References
Pete Burnap and Matthew L Williams. 2016. Us and

Them: Identifying Cyber Hate on Twitter across
Multiple Protected Characteristics. EPJ Data Sci-
ence 5(1):11.

Maral Dadvar, Dolf Trieschnigg, Roeland Ordelman,
and Franciska de Jong. 2013. Improving cyberbully-
ing detection with user context. In ECIR. Springer,
pages 693–696.

Karthik Dinakar, Birago Jones, Catherine Havasi,
Henry Lieberman, and Rosalind Picard. 2012. Com-
mon Sense Reasoning for Detection, Prevention, and
Mitigation of Cyberbullying. ACM Transactions on
Interactive Intelligent Systems (TiiS) 2(3):18.

Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al.
1999. Similarity search in high dimensions via
hashing. In VLDB. volume 99, pages 518–529.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of
dimensionality. In Proceedings of the 13th Annual
ACM Symposium on Theory of Computing. ACM,
pages 604–613.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882 .

Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.
2016. Learning natural language inference using
bidirectional lstm model and inner-attention. arXiv
preprint arXiv:1605.09090 .

Anna Schmidt and Michael Wiegand. 2017. A Sur-
vey on Hate Speech Detection using Natural Lan-
guage Processing. In SocialNLP’17: Proceedings
of the 5th International Workshop on Natural Lan-
guage Processing for Social Media. pages 1–10.

Johan AK Suykens and Joos Vandewalle. 1999. Least
squares support vector machine classifiers. Neural
processing letters 9(3):293–300.

Cynthia Van Hee, Els Lefever, Ben Verhoeven, Julie
Mennes, Bart Desmet, Guy De Pauw, Walter Daele-
mans, and Véronique Hoste. 2015. Detection
and Fine-grained Classification of Cyberbullying
Events. In RANLP’15: International Conference
Recent Advances in Natural Language Processing.
pages 672–680.

William Warner and Julia Hirschberg. 2012. Detecting
Hate Speech on the World Wide Web. In ACL’12:
Proceedings of the 2nd Workshop on Language in

122



Social Media. Association for Computational Lin-
guistics, pages 19–26.

Zeerak Waseem and Dirk Hovy. 2016. Hateful Sym-
bols or Hateful People? Predictive Features for Hate
Speech Detection on Twitter. In NAACL Student Re-
search Workshop. pages 88–93.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning 8(3-4):229–256.

Guang Xiang, Bin Fan, Ling Wang, Jason Hong, and
Carolyn Rose. 2012. Detecting Offensive Tweets
via Topical Feature Discovery over a Large Scale
Twitter Corpus. In CIKM’12: Proceedings of the
21st ACM International Conference on Information
and Knowledge Management. ACM, pages 1980–
1984.

Haoti Zhong, Hao Li, Anna Cinzia Squiccia-
rini, Sarah Michele Rajtmajer, Christopher Grif-
fin, David J Miller, and Cornelia Caragea. 2016.
Content-Driven Detection of Cyberbullying on the
Instagram Social Network. In IJCAI’16: Proceed-
ings of the 25th International Joint Conference on
Artificial Intelligence. pages 3952–3958.

Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
based bidirectional long short-term memory net-
works for relation classification. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Short Papers). volume 2,
pages 207–212.

123


