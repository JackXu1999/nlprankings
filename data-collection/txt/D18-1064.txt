



















































Adversarial Training for Multi-task and Multi-lingual Joint Modeling of Utterance Intent Classification


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 633–639
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

633

Adversarial Training for Multi-task and Multi-lingual Joint Modeling
of Utterance Intent Classification

Ryo Masumura and Yusuke Shinohara and Ryuichiro Higashinaka and Yushi Aono
NTT Media Intelligence Laboratories, NTT Corporation

1-1, Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan
ryou.masumura.ba@hco.ntt.co.jp

Abstract

This paper proposes an adversarial training
method for the multi-task and multi-lingual
joint modeling needed for utterance intent
classification. In joint modeling, common
knowledge can be efficiently utilized among
multiple tasks or multiple languages. This
is achieved by introducing both language-
specific networks shared among different tasks
and task-specific networks shared among dif-
ferent languages. However, the shared net-
works are often specialized in majority tasks or
languages, so performance degradation must
be expected for some minor data sets. In
order to improve the invariance of shared
networks, the proposed method introduces
both language-specific task adversarial net-
works and task-specific language adversarial
networks; both are leveraged for purging the
task or language dependencies of the shared
networks. The effectiveness of the adver-
sarial training proposal is demonstrated using
Japanese and English data sets for three differ-
ent utterance intent classification tasks.

1 Introduction

In natural language processing fields, full neural
network based methods are suitable for joint mod-
eling as they can simultaneously utilize multiple
task data sets or multiple language data sets to
improve the performance achieved for individual
tasks or languages (Collobert and Weston, 2008).
It is known that joint modeling can address the
data scarcity problem.

Key natural language processing technologies
for spoken dialogue systems include utterance in-
tent classification, which is needed to detect in-
tent labels such as dialogue act (Stolcke et al.,
2000; Khanpour et al., 2016), domain (Xu and
Sarikaya, 2014), and question type (Wu et al.,
2005) from input utterances (Ravuri and Stolcke,

2015a,b, 2016). One problem is that the train-
ing data are often limited or unbalanced among
different tasks or different languages. Therefore,
our motivation is to leverage both multi-task joint
modeling and multi-lingual joint modeling to en-
hance utterance intent classification.

The multi-task and multi-lingual joint modeling
can be composed by introducing both task-specific
networks, which are shared among different lan-
guages, and language-specific networks, which
are shared among different tasks (Masumura et al.,
2018; Lin et al., 2018). Although joint model-
ing is mainly intended to improve classification
performance in resource-poor tasks or languages,
its classification performance is degraded in some
minor data sets. This is because the language-
specific networks often depend on majority tasks,
while the task-specific networks often depend on
majority languages. What are needed are task-
specific networks that are invariant to languages,
and language-specific networks that are invariant
to tasks.

In order to explicitly improve the invariance of
language and task-specific networks, this paper in-
troduces adversarial training (Goodfellow et al.,
2014). Our idea is to train language-specific net-
works so as to be insensitive to the target task,
while training task-specific networks to be in-
sensitive to language. To this end, we intro-
duce multiple domain adversarial networks (Ganin
et al., 2016), language-specific task adversarial
networks, and task-specific language adversarial
networks, into a state-of-the-art fully neural net-
work based joint modeling; we adopt the bidi-
rectional long short-term memory recurrent neural
networks (BLSTM-RNNs) with attention mecha-
nism (Yang et al., 2016; Zhou et al., 2016). To the
best of our knowledge, this paper is the first study
to employ adversarial training for multi-input and
multi-output joint modeling.



634

Experiments on Japanese and English data sets
demonstrate the effectiveness of the adversarial
training proposal. To support spoken dialogue sys-
tems, three different utterance intent classification
tasks are examined: dialogue act, topic type, and
question type classification.

2 Related Work

Joint Modeling: In natural language processing
research, joint modeling is usually split into multi-
task joint modeling and multi-lingual joint mod-
eling. Multi-task joint modeling has been shown
to effectively improve individual tasks (Collobert
and Weston, 2008; Liu et al., 2016a,b; Zhang
and Weng, 2016; Liu et al., 2016c). In addition,
multi-lingual joint modeling is achieved by learn-
ing common semantic representations among dif-
ferent languages (Guo et al., 2016; Duong et al.,
2016; Zhang et al., 2016, 2017b). In addition,
a few work have examined multi-task and multi-
lingual joint modeling (Masumura et al., 2018; Lin
et al., 2018). Different from the previous work,
our novelty is to introduce adversarial training for
multi-task and multi-lingual joint modeling.
Adversarial Training: The concept of adversar-
ial training was first proposed by Goodfellow et al.
(2014), and many studies in the machine learning
field have focused on adversarial training. Adver-
sarial training has been well utilized in text classi-
fication (Ganin et al., 2016; Chen et al., 2016; Liu
et al., 2017; Miyato et al., 2017; Chen and Cardie,
2018). Most natural language processing papers
adopted either the language invariant approach
(Chen et al., 2016; Zhang et al., 2017a) or the task
invariant approach (Ganin et al., 2016; Liu et al.,
2017; Chen and Cardie, 2018). This paper aims
to fully utilize both task adversarial training and
language adversarial training. To this end, we si-
multaneously introduce language-specific task ad-
versarial networks and task-specific language ad-
versarial networks.

3 Proposed Method

This section details our adversarial training
method for multi-task and multi-lingual joint mod-
eling of utterance intent classification.

In the j-th task utterance intent classification
for the i-th language input utterance, intent la-
bel l(j) ∈ {1, · · · ,K(j)} is estimated from in-
put utterance W(i) = {w(i)1 , · · · , w

(i)
T } where

i ∈ {1, · · · , I} and j ∈ {1, · · · , J}. Utter-

ance intent classification is followed by estima-
tion of the probabilities of each intent label given
input utterance, P (l(j)|W(i),Θ(i,j)) where Θ(i,j)
is the trainable model parameter for the com-
bination of the i-th language and the j-th task.
In multi-task and multi-lingual joint modeling,
{Θ(1,1), · · · ,Θ(I,J)} are jointly trained from I
language and J task data sets.

3.1 Main Joint Network

The proposed method is founded on a fully neu-
ral network that employs I language-specific net-
works, J task-specific networks, and J classifica-
tion networks as well as Masumura et al. (2018).

The language-specific network can be shared
between multiple tasks, where words in the in-
put utterance are converted into language-specific
hidden representations. Each word in the i-th lan-
guage input utterance W(i) is first converted into
a continuous representation. Next, each word rep-
resentation is converted into a hidden representa-
tion that uses BLSTM-RNNs to take neighboring
word context information into account. The t-th
language-specific hidden representation for the i-
th language is given by:

w
(i)
t = EMBED(w

(i)
t ;θ

(i)
h ), (1)

h
(i)
t = BLSTM({w

(i)
1 , · · · ,w

(i)
T }, t;θ

(i)
h ), (2)

where EMBED() is a linear transformational func-
tion for word embedding, BLSTM() is a function of
the BLSTM-RNN layer, and θ(i)h is the trainable
parameter for the i-th language-specific network.

In addition, task-specific networks can be
shared between multiple languages, where the
language-specific hidden representations are con-
verted into task-specific hidden representations.
The t-th language-specific hidden representation
for the j-th task is given by:

u
(j)
t = BLSTM({h

(i)
1 , · · · ,h

(i)
T }, t;θ

(j)
u ), (3)

where θ(j)u is the trainable parameter for the j-th
task-specific network.

In classification networks for each task, the
task-specific hidden representations are summa-
rized as sentence representation s(j) by using a
self-attention mechanism that can consider the
importance of individual hidden representations
(Yang et al., 2016; Zhou et al., 2016; Sawada et al.,
2017). Next, predicted probabilities of the j-th



635

task intent labels, o(j) ∈ RK(j) , are given by:

s(j) = ATTENSUM({h(i)1 , · · · ,h
(i)
T };θ

(j)
o ), (4)

o(j) = SOFTMAX(s(j);θ(j)o ), (5)

where ATTENSUM() is a weighted sum function
with self-attention, SOFTMAX() is a transforma-
tional function with softmax activation, and θ(j)o
is the trainable parameter for the j-th classifica-
tion network. In the main joint networks of the
proposal, Θ(i,j) corresponds to {θ(i)h , θ

(j)
u ,θ

(j)
o }.

3.2 Adversarial Networks

The proposed method combines a language-
specific task adversarial network with a task-
specific language adversarial network. The
task adversarial network is used for training the
language-specific networks to be insensitive to tar-
get task labels, and the language adversarial net-
work is used for training the task-specific net-
works to be insensitive to target language labels.
In order to efficiently use stochastic gradient de-
scent based training for optimizing the adversarial
networks, we use gradient reversal layers, which
allow the input vectors during forward propaga-
tion, and sign inversion of the gradients during
back propagation, to be utilized (Ganin et al.,
2016).

The i-th language-specific task adversarial net-
work estimates task labels from the i-th language-
specific hidden representations. The predicted
probabilities of task labels, x(i) ∈ RJ , are given
by:

h̃
(i)
t = GRL(h

(i)
t ), (6)

h̃(i) = ATTENSUM({h̃(i)1 , · · · , h̃
(i)
T };θ

(i)
x ), (7)

x(i) = SOFTMAX(h̃(i),θ(i)x ), (8)

where GRL() represents the gradient reversal layer,
and θ(i)x is the trainable parameter. The j-th task-
specific language adversarial network estimates
language labels from the j-th task-specific hid-
den representations. The predicted probabilities of
language labels, y(j) ∈ RI , are given by:

ũ
(j)
t = GRL(u

(j)
t ), (9)

ũ(j) = ATTENSUM({ũ(j)1 , · · · , ũ
(j)
T };θ

(j)
y ), (10)

y(j) = SOFTMAX(ũ(j),θ(j)y ), (11)

where θy is the trainable parameter.

SOFTMAX

GRL GRL

SOFTMAX SOFTMAX

BLSTM BLSTM

SOFTMAX

EMBED EMBED

BLSTM BLSTM

SOFTMAX

GRLGRL

SOFTMAX

ATTENSUM ATTENSUMATTENSUM ATTENSUM ATTENSUMATTENSUM

Figure 1: Proposed network structure for two tasks and
two languages.

The proposed network structure shown in Fig-
ure 1 includes both joint networks and adversar-
ial networks for two tasks and two languages.
The red components are language-specific net-
works, the orange components are task-specific
networks, and the purple components are classi-
fication networks. In addition, green components
are language-specific task adversarial networks,
and blue components are task-specific language
adversarial networks.

3.3 Training

Our adversarial training proposal jointly optimizes
all parameters in both the main joint networks and
the adversarial networks by using all training data
sets {D(1,1), · · · ,D(I,J)} where D(i,j) represents
the sets of the input utterances and the reference.
The cross-entropy loss functions of each network
are defined as:

Lo = −
I∑

i=1

J∑
j=1

|D(i,j)|∑
n=1

K(j)∑
k=1

ô
(j)
n,k log o

(j)
n,k, (12)

Lx = −
I∑

i=1

J∑
j=1

|D(i,j)|∑
n=1

J∑
j′=1

x̂
(i)
n,j′ logx

(i)
n,j′ , (13)

Ly = −
I∑

i=1

J∑
j=1

|D(i,j)|∑
n=1

I∑
i′=1

ŷ
(j)
n,i′ log y

(j)
n,i′ , (14)

where Lo, Lx, and Ly are the cross entropy loss
terms for the classification networks, the task ad-
versarial networks, and the language adversarial
networks. ô(j)n,k, x̂

(i)
n,j′ , and ŷ

(j)
n,i′ are the reference

probabilities, and on,k, xn,j′ , and yn,i′ are the es-
timated probabilities of the k-th label in the j-th
task classification network, the j′-th task in the i-
th language-specific task adversarial network, and



636

Task Utterance Label
DA Hello, how are you today? GREETING

I am so sorry to hear of your son’s accident. SYMPATHY/AGREE
Lets go to school an hour early today. PROPOSAL

TT What is the highest mountain in the world? MOUNTAIN
Who is president of the united states? PERSON
What is the name of the most recent Star Wars movie? MOVIE

QT Do you like egg salad? TRUE/FALSE
How do you correct a hook in a golf swing? EXPLANATION:METHOD
Why is blood red? EXPLANATION:CAUSE

Table 2: Examples of English data sets.

the i′-th language in the j-th task-specific lan-
guage adversarial network forWn, respectively.

Due to use of gradient reversal layers, individ-
ual parameters are gradually updated as follows:

θ(j)o ← θ(j)o − �
∂Lo

∂θ
(j)
o

, (15)

θ(j)y ← θ(j)y − �β
∂Ly

∂θ
(j)
y

, (16)

θ(j)u ← θ(j)u − �(
∂Lo

∂θ
(j)
u

− β ∂Ly
∂θ

(j)
u

), (17)

θ(i)x ← θ(i)x − �α
∂Lx

∂θ
(i)
x

, (18)

θ
(i)
h ← θ

(i)
h − �(

∂Lo

∂θ
(i)
h

− α ∂Lx
∂θ

(i)
h

− β ∂Ly
∂θ

(i)
h

),

(19)

where α and β are hyper parameters of the param-
eter update, and � is the learning rate. Note that
adversarial training is suppressed by setting α and
β to 0.0. In training, we prepared optimizers for
individual data sets. The individual learning rates
fall when the validation loss of the target classifi-
cation network increases.

4 Experiments

Our experiments employed Japanese and English
data sets created for three different utterance in-
tent classification tasks. The tasks, dialogue act
(DA) classification, topic type (TT) classification,
and question type (QT) classification, are intended
to support spoken dialogue systems. For example,
the task of English DA classification is to obtain a
DA label from an input utterance. We used natural
language texts as the input utterances and individ-
ual label sets were unified between Japanese and
English. Data sets employed in experiments were
corpora that were made for constructing spoken
dialogue systems (Masumura et al., 2018). Each
of the data sets were divided into training (Train),

Language Task #labels Train Valid Test
Japanese DA 28 201 K 4 K 4 K

TT 168 40 K 4 K 4 K
QT 15 55 K 4 K 4 K

English DA 28 25 K 3 K 3 K
TT 168 25 K 3 K 3 K
QT 15 22 K 2 K 2 K

Table 1: Number of utterances in individual data sets.

validation (Valid), and test (Test) sets. Table 1
shows the number of utterances in individual data
sets where #labels represents the number of labels.
Table 2 shows English utterances and label exam-
ples for individual tasks.

4.1 Setups

We examined single-task and mono-lingual mod-
eling, multi-task joint modeling, multi-lingual join
modeling, and multi-task and multi-lingual joint
modeling with or without adversarial training.

We unified network configurations as follows.
Word representation size was set to 128, BLSTM-
RNN unit size was set to 400, and sentence rep-
resentation was set to 400. Dropout was used for
EMBED() and BLSTM(), and the dropout rate was
set to 0.5. Words that appeared only once in the
training data sets were treated as unknown words.
We used mini-batch stochastic gradient descent, in
which initial learning rate was set to 0.1. We opti-
mized hyper-parameters of adversarial training (α
and β) for the validation sets by varying them from
0.001 to 1.0. Other hyper parameters were also op-
timized for the validation sets.

4.2 Results

Table 3 shows the results in terms of utterance
classification accuracy. For each setup, we con-
structed five models by varying the initial param-
eters and evaluated the average accuracy. Line
(1) shows baseline results: single-task and mono-
lingual modeling. Lines (2) and (3) show results



637

Joint modeling Adversarial Training Japanese English
Multi-task Multi-lingual Task-invariant Language-invariant DA TT QT DA TT QT

(1). - - - - 66.6 79.1 87.7 61.8 64.5 83.4
(2)

√
- - - 66.5 79.6 89.3 60.6 64.4 83.7

(3)
√

-
√

- 66.5 80.6 89.5 61.6 65.7 83.7
(4) -

√
- - 66.7 78.7 87.2 61.4 64.3 83.0

(5) -
√

-
√

66.9 79.8 88.2 61.8 64.8 83.3
(6).

√ √
- - 66.6 79.7 89.3 60.5 65.4 82.6

(7).
√ √ √

- 67.3 81.1 89.6 61.5 66.1 83.5
(8).

√ √
-

√
66.7 80.7 89.5 60.9 66.7 83.0

(9).
√ √ √ √

67.6 81.3 90.0 61.9 66.7 83.7

Table 3: Experimental results: utterance classification accuracy (%) for individual test sets.

with only performing multi-task joint modeling,
and lines (4) and (5) show results with only per-
forming multi-lingual joint modeling. Note that
lines (3) and (5) show the results achieved with ad-
versarial training. Line (6) shows multi-task and
multi-lingual joint modeling results: adversarial
training was suppressed by setting both α and β
to 0.0. Lines (7)–(9) shows the results achieved
with adversarial training. Note that setting with
bold values achieved the highest performance in
our evaluation.

First, in lines (2) and (4), the classification per-
formance deteriorated in some cases, while per-
formance improvements were achieved in other
cases. On the other hand, in lines (3) and (5), clas-
sification performance in each data sets was im-
proved by introducing adversarial training. This
indicates that adversarial training was effective in
improving the performance of joint modeling.

Next, line (6) shows that, relative to line
1, multi-task and multi-lingual joint modeling
can improve the classification performance for
Japanese TT, Japanese QT, and English TT, but
classification performance was degraded for En-
glish DA and English QT. This indicates that it
is difficult to simultaneously improve the clas-
sification performance for all data sets because
joint modeling often depends on majority tasks
or majority languages. In addition, lines (7) and
(8) show the introduction of either task adver-
sarial networks or language adversarial networks
yielded better performance than line (6) for all
data sets. This indicates that adversarial train-
ing was effective in improving the performance
of multi-task and multi-lingual joint modeling.
The best results were achieved by using both
language-specific task adversarial networks and
task-specific language adversarial networks, line
(9). These results confirm that task adversarial

networks and language adversarial networks well
complement each other. Of particular benefit, the
proposed method demonstrated greater classifica-
tion performance improvements when the number
of training utterances per label was small.

5 Conclusions

We have proposed an adversarial training method
for the multi-task and multi-lingual joint modeling
needed to enhance utterance intent classification.
Our adversarial training proposal utilizes both task
adversarial networks and language adversarial net-
works for improving task-invariance in language-
specific networks and language-invariance in task-
specific networks. Experiments showed that the
adversarial training proposal could well realize the
benefits of joint modeling in all data sets.

References
Xilun Chen and Claire Cardie. 2018. Multinomial ad-

versarial networks for multi-domain text classifica-
tion. arXiv preprint arXiv:1802.05694.

Xilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie,
and Kilian Weinberger. 2016. Adversarial deep av-
eraging networks for cross-lingual sentiment classi-
fication. arXiv preprint arXiv:1606.01614.

Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language processing:
Deep neural networks with multitask learning. In
Proc. International Conference on Machine Learn-
ing (ICML).

Long Duong, Hiroshi Kanayama, Tengfei Ma, Steven
Bird, and Trevor Cohn. 2016. Learning crosslin-
gual word embeddings without bilingual corpora. In
Proc. Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1285–1295.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, Francois Lavi-
olette, Mario Marchand, and Victor Lempitsky.



638

2016. Domain-adversarial training of neural net-
works. Journal of Machine Learning Research,
17:1–35.

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Proc. Advances in Neural Informa-
tion Processing Systems (NIPS), pages 2672–2680.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2016. A representation learn-
ing framework for multi-source transfer parsing. In
Proc. AAAI Conference on Artificial Intelligence
(AAAI), pages 2734–2740.

Hamed Khanpour, Nishitha Guntakandla, and Rod-
ney Nielsen. 2016. Dialogue act classification in
domain-independent conversations using a deep re-
current neural network. In Proc. International Con-
ference on Computational Linguistics (COLING),
pages 2012–2021.

Ying Lin, Shengqi Yang, Veselin Stoyanov, and Heng
Ji. 2018. A multi-lingual multi-task architecture for
low-resource sequence labeling. In Proc. Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages pp.799–809.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016a.
Deep multi-task learning with shared memory. In
Proc. Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 118–127.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016b.
Recurrent neural network for text classification with
multi-task learning. In Proc. International Joint
Conference on Artificial Intelligence (IJCAI), pages
2873–2879.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classifica-
tion. In Proc. Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1–10.

Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui.
2016c. Implicit discourse relation classification via
multi-task neural networks. In Proc. AAAI Confer-
ence on Artificial Intelligence (AAAI), pages 2750–
2756.

Ryo Masumura, Tomohiro Tanaka, Ryuichiro Hi-
gashinaka, Hirokazu Masataki, and Yushi Aono.
2018. Multi-task and multi-lingual joint learning
of neural lexical utterance classification based on
partially-shared modeling. In Proc. International
Conference on Computational Linguistics (COL-
ING), pages pp.3586–3596.

Takeru Miyato, Andrew M. Dai, and Ian Goodfel-
low. 2017. Adversarial training methods for semi-
supervised text classification. In Proc. International
Conference on Learning Representation (ICLR).

Suman Ravuri and Andreas Stolcke. 2015a. A com-
parative study of neural network models for lexi-
cal intent classification. In Proc. Automatic Speech
Recognition and Understanding Workshop (ASRU),
pages 368–374.

Suman Ravuri and Andreas Stolcke. 2015b. Recurrent
neural network and LSTM models for lexical utter-
ance classification. In Proc. Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pages 135–139.

Suman Ravuri and Andreas Stolcke. 2016. A com-
parative study of recurrent neural network models
for lexical domain classification. In Proc. Interna-
tional Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 6075–6079.

Naoki Sawada, Ryo Masumura, and Hiromitsu
Nishizaki. 2017. Parallel hierarchical attention net-
works with shared memory reader for multi-stream
conversational document classification. In Proc. An-
nual Conference of the International Speech Com-
munication Association (INTERSPEECH), pages
3311–3315.

Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-
lor, Rachel Martion, Carol Van Ess-Dykema, and
Marie Metter. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339–373.

Chung-Hsien Wu, Jui-Feng Yeh, and Ming-Jun Chen.
2005. Domain-specific FAQ retrieval using indepen-
dent aspects. ACM Transactions on Asian Language
Information Processing, 4(1):1–17.

Puyang Xu and Ruhi Sarikaya. 2014. Contextual do-
main classification in spoken language understand-
ing systems using recurrent neural network. In Proc.
International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 136–140.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alexander J. Smola, and Eduard H. Hovy. 2016. Hi-
erarchical attention networks for document classi-
fication. In Proc. Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), pages 1480–1489.

Meng Zhang, Yang Liu, Huanbo Luan, Yiqun Liu, and
Maosong Sun. 2016. Inducing bilingual lexica from
non-parallel data with earth mover’s distance reg-
ularization. In Proc. International Conference on
Computational Linguistics (COLING), pages 3188–
3198.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017a. Adversarial training for unsupervised
bilingual lexicon induction. In Proc. Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 1959–1970.



639

Meng Zhang, Haoruo Peng, Yang Liu, Huanbo Luan,
and Maosong Sun. 2017b. Bilingual lexicon induc-
tion from non-parallel data with minimum supervi-
sion. In Proc. AAAI Conference on Artificial Intelli-
gence (AAAI), pages 3379–3384.

Xiaodong Zhang and Houfeng Weng. 2016. A joint
model of intent determination and slot filling for
spoken language understanding. In Proc. Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI), pages 2993–2999.

Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
based bidirectional long short-term memory net-
works for relation classification. In Proc. Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 207–212.


