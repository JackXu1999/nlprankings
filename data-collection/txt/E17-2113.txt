



















































F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 713–718,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

F-Score Driven Max Margin Neural Network for Named Entity
Recognition in Chinese Social Media

Hangfeng He and Xu Sun
MOE Key Laboratory of Computational Linguistics, Peking University

School of Electronics Engineering and Computer Science, Peking University
{hangfenghe, xusun}@pku.edu.cn

Abstract

We focus on named entity recognition
(NER) for Chinese social media. With
massive unlabeled text and quite lim-
ited labelled corpus, we propose a semi-
supervised learning model based on B-
LSTM neural network. To take advan-
tage of traditional methods in NER such
as CRF, we combine transition probabil-
ity with deep learning in our model. To
bridge the gap between label accuracy and
F-score of NER, we construct a model
which can be directly trained on F-score.
When considering the instability of F-
score driven method and meaningful in-
formation provided by label accuracy, we
propose an integrated method to train on
both F-score and label accuracy. Our in-
tegrated model yields substantial improve-
ment over previous state-of-the-art result.

1 Introduction

With the development of Internet, social media
plays an important role in information exchange.
The natural language processing tasks on social
media are more challenging which draw attention
of many researchers (Li and Liu, 2015; Habib and
van Keulen, 2015; Radford et al., 2015; Cherry
and Guo, 2015). As the foundation of many down-
stream applications (Weissenborn et al., 2015;
Delgado et al., 2014; Hajishirzi et al., 2013) such
as information extraction, named entity recogni-
tion (NER) deserves more research in prevailing
and challenging social media text. NER is a task
to identify names in texts and to assign names with
particular types (Sun et al., 2009; Sun, 2014; Sun
et al., 2014; He and Sun, 2017). It is the informal-
ity of social media that discourages accuracy of
NER systems. While efforts in English have nar-

rowed the gap between social media and formal
domains (Cherry and Guo, 2015), the task in Chi-
nese remains challenging. It is caused by Chinese
logographic characters which lack many clues to
indicate whether a word is a name, such as capi-
talization. The scant labelled Chinese social me-
dia corpus makes the task more challenging (Nee-
lakantan and Collins, 2015; Skeppstedt, 2014; Liu
et al., 2015).

To address the problem, one approach is to use
the lexical embeddings learnt from massive unla-
beled text. To take better advantage of unlabeled
text, Peng and Dredze (2015) evaluates three types
of embeddings for Chinese text, and shows the
effectiveness of positional character embeddings
with experiments. Considering the value of word
segmentation in Chinese NER, another approach
is to construct an integrated model to jointly train
learned representations for both predicting word
segmentations and NER (Peng and Dredze, 2016).

However, the two above approaches are imple-
mented within CRF model. We construct a semi-
supervised model based on B-LSTM neural net-
work to learn from the limited labelled corpus by
using lexical information provided by massive un-
labeled text. To shrink the gap between label ac-
curacy and F-Score, we propose a method to di-
rectly train on F-Score rather than label accuracy
in our model. In addition, we propose an inte-
grated method to train on both F-Score and label
accuracy. Specifically, we make contributions as
follows:

• We propose a method to directly train on F-
Score rather than label accuracy. In addition,
we propose an integrated method to train on
both F-Score and label accuracy.

• We combine transition probability into our B-
LSTM based max margin neural network to
form structured output in neural network.

713



• We evaluate two methods to use lexical em-
beddings from unlabeled text in neural net-
work.

2 Model

We construct a semi-supervised model which is
based on B-LSTM neural network and combine
transition probability to form structured output.
We propose a method to train directly on F-Score
in our model. In addition, we propose an inte-
grated method to train on both F-Score and label
accuracy.

2.1 Transition Probability
B-LSTM neural network can learn from past in-
put features and LSTM layer makes it more effi-
cient (Hammerton, 2003; Hochreiter and Schmid-
huber, 1997; Chen et al., 2015; Graves et al.,
2006). However, B-LSTM cannot learn sentence
level label information. Huang et al. (2015) com-
bine CRF to use sentence level label informa-
tion. We combine transition probability into our
model to gain sentence level label information. To
combine transition probability into B-LSTM neu-
ral network, we construct a Max Margin Neural
Network (MMNN) (Pei et al., 2014) based on B-
LSTM. The prediction of label in position t is
given as:

yt = softmax(Why ∗ ht + by) (1)
where Why are the transformation parameters, ht
the hidden vector and by the bias parameter. For a
input sentence c[1:n] with a label sequence l[1:n], a
sentence-level score is then given as:

s(c[1:n], l[1:n], θ) =
n∑

t=1

(Alt−1lt + fΛ(lt|c[1:n]))

wherefΛ(lt|c[1:n]) indicates the probability of la-
bel lt at position t by the network with parameters
Λ, A indicates the matrix of transition probability.
In our model, fΛ(lt|c[1:n]) is computed as:

fΛ(lt|c[1:n]) = −log(yt[lt]) (2)
We define a structured margin loss ∆(l, l) as Pei

et al. (2014):

∆(l, l) =
n∑

j=1

κ1{lj 6= lj} (3)

where n is the length of setence x, κ is a discount
parameter, l a given correct label sequence and l

a predicted label sequence. For a given training
instance (xi, yi), our predicted label sequence is
the label sequence with highest score:

l∗i = arg max
li∈Y (xi)

s(xi, li, θ)

The label sequence with the highest score can be
obtained by carrying out viterbi algorithm. The
regularized objective function is as follows:

J(θ) =
1
m

m∑
i=1

qi(θ) +
λ

2
||θ||2 (4)

qi(θ) = max
li∈Y (xi)

(s(xi, li, θ)+∆(li, li))−s(xi, li, θ)

By minimizing the object, we can increase the
score of correct label sequence l and decrease the
score of incorrect label sequence l.

2.2 F-Score Driven Training Method

Max Margin training method use structured mar-
gin loss ∆(l, l) to describe the difference between
the corrected label sequence l and predicted la-
bel sequence l. In fact, the structured margin loss
∆(l, l) reflect the loss in label accuracy. Consider-
ing the gap between label accuracy and F-Score in
NER, we introduce a new training method to train
directly on F-Score. To introduce F-Score driven
training method, we need to take a look at the sub-
gradient of equation (4):

∂J

∂θ
=

1
m

m∑
i=1

(
∂s(x, lmax, θ)

∂θ
− ∂s(x, l, θ)

∂θ
) + λθ

In the subgradient, we can know that structured
margin loss ∆(l, l) contributes nothing to the
subgradient of the regularized objective function
J(θ). The margin loss ∆(l, l) serves as a trig-
ger function to conduct the training process of B-
LSTM based MMNN. We can introduce a new
trigger function to guide the training process of
neural network.
F-Score Trigger Function The main criterion of
NER task is F-score. However, high label accu-
racy does not mean high F-score. For instance,
if every named entity’s last character is labeledas
O, the label accuracy can be quite high, but the
precision, recall and F-score are 0. We use the F-
Score between corrected label sequence and pre-
dicted label sequence as trigger function, which
can conduct the training process to optimize the

714



BLSTM BLSTM−MMNN Proposal I Proposal II
45

50

55

60

65

Model

F
1−

S
co

re
NAM
NOM
Overall

(a) F-Score of the models.

BLSTM BLSTM−MMNN Proposal I Proposal II
0

100

200

300

400

500

600

Model

T
im

e 
pe

r 
ite

ra
tio

n/
s

(b) Running time of the models.

0.1 0.2 0.3 0.4 0.5
45

50

55

O
ve

ra
ll 

F
1−

sc
or

e

(c) Overall F1-Score with different val-
ues of beta.

F-Score of training examples. Our new structured
margin loss can be described as:

∆̃(l, l) = κ ∗ FScore (5)

where FScore is the F-Score between corrected
label sequence and predicted label sequence.
F-Score and Label Accuracy Trigger Function
The F-Score can be quite unstable in some situa-
tion. For instance, if there is no named entity in
a sentence, F-Score will be always 0 regardless of
the predicted label sequence. To take advantage
of meaningful information provided by label ac-
curacy, we introduce an integrated trigger function
as follows:

∆̂(l, l) = ∆̃(l, l) + β ∗∆(l, l) (6)

where β is a factor to adjust the weight of label
accuracy and F-Score.

Because F-Score depends on the whole label se-
quence, we use beam search to find k label se-
quences with top sentece-level score s(x, l, θ) and
then use trigger function to rerank the k label se-
quences and select the best.

2.3 Word Segmentation Representation

Word segmentation takes an important part
in Chinese text processing. Both Peng and
Dredze (2015) and Peng and Dredze (2016) show
the value of word segmentation to Chinese NER
in social media. We present two methods to use
word segmentation information in neural network
model.
Character and Position Embeddings To incor-
porate word segmentation information, we at-
tach every character with its positional tag. This
method is to distinguish the same character at dif-
ferent position in the word. We need to word seg-
ment the text and learn positional character em-
beddings from the segmented text.

Character Embeddings and Word Segmenta-
tion Features We can treat word segmentation as
discrete features in neural network model. The
discrete features can be easily incorporated into
neural network model (Collobert et al., 2011). We
use word embeddings from a LSTM pretrained on
MSRA 2006 corpus to initialize the word segmen-
tation features.

3 Experiments and Analysis

3.1 Datasets

Named Nominal
Train set 957 898

Development set 153 226
Test set 209 196

Unlabeled Text 112,971,734 Weibo messages

Table 1: Details of Weibo NER corpus.

We use a modified labelled corpus1 as Peng
and Dredze (2016) for NER in Chinese social
media. Details of the data are listed in Table
1. We also use the same unlabelled text as Peng
and Dredze (2016) from Sina Weibo service in
China and the text is word segmented by a Chi-
nese word segmentation system Jieba2 as Peng and
Dredze (2016) so that our results are more compa-
rable to theirs.

3.2 Parameter Estimation

We pre-trained embeddings using word2vec
(Mikolov et al., 2013) with the skip-gram train-
ing model, without negative sampling and other
default parameter settings. Like Mao et al. (2008),
we use bigram features as follow:

CnCn+1(n = −2,−1, 0, 1) and C−1C1
1We fix some labeling errors of the data.
2https://github.com/fxsjy/jieba.

715



Methods Named Entity Nominal Mention
Precision Recall F1 Precision Recall F1

Character+Segmentation 48.52 39.23 43.39 58.75 47.96 52.91
Character+Position 65.87 39.71 49.55 68.12 47.96 56.29

Table 2: Two methods to incorporate word segmentation information.

Models Named Entity Nominal Mention
Precision Recall F1 Precision Recall F1 Overall OOV

(Peng and Dredze, 2015) 57.98 35.57 44.09 63.84 29.45 40.38 42.70 -
(Peng and Dredze, 2016) 63.33 39.18 48.41 58.59 37.42 45.67 47.38 -

B-LSTM 65.87 39.71 49.55 68.12 47.96 56.29 52.81 13.97
B-LSTM + MMNN 65.29 37.80 47.88 73.53 51.02 60.24 53.86 17.90

F-Score Driven I (proposal) 66.67 39.23 49.40 69.50 50.00 58.16 53.64 17.03
F-Score Driven II (proposal) 66.93 40.67 50.60 66.46 53.57 59.32 54.82 20.96

Table 3: NER results for named and nominal mentions on test data.

We use window approach (Collobert et al., 2011)
to extract higher level Features from word feature
vectors. We treat bigram features as discrete fea-
tures (Collobert et al., 2011) for our neural net-
work. Our models are trained using stochastic gra-
dient descent with an L2 regularizer.
As for parameters in our models, window size
for word embedding is 5, word embedding di-
mension, feature embedding dimension and hid-
den vector dimension are all 100, discount κ in
margin loss is 0.2, and the hyper parameter for the
L2 is 0.000001. As for learning rate, initial learn-
ing rate is 0.1 with a decay rate 0.95. For inte-
grated model, β is 0.2. We train 20 epochs and
choose the best prediction for test.

3.3 Results and Analysis

We evaluate two methods to incorporate word seg-
mentation information. The results of two meth-
ods are shown as Table 2. We can see that posi-
tional character embeddings perform better in neu-
ral network. This is probably because positional
character embeddings method can learn word seg-
mentation information from unlabeled text while
word segmentation can only use training corpus.

We adopt positional character embeddings in
our next four models. Our first model is a B-
LSTM neural network (baseline). To take advan-
tage of traditional model (Chieu and Ng, 2002;
Mccallum et al., 2001) such as CRF, we com-
bine transition probability in our B-LSTM based
MMNN. We design a F-Score driven training
method in our third model F-Score Driven Model
I . We propose an integrated training method in
our fourth model F-Score Driven Model II .The re-

sults of models are depicted as Figure 1(a). From
the figure, we can know our models perfrom better
with little loss in time.

Table 3 shows results for NER on test sets. In
the Table 3, we also show micro F1-score (Over-
all) and out-of-vocabulary entities (OOV) recall.
Peng and Dredze (2016) is the state-of-the-art
NER system in Chinese Social media. By compar-
ing the results of B-LSTM model and B-LSTM +
MTNN model, we can know transition probability
is significant for NER. Compared with B-LSTM +
MMNN model, F-Score Driven Model I improves
the result of named entity with a loss in nominal
mention. The integrated training model (F-Score
Driven Model II) benefits from both label accu-
racy and F-Score, which achieves a new state-of-
the-art NER system in Chinese social media. Our
integrated model has better performance on named
entity and nominal mention.

To better understand the impact of the factor β,
we show the results of our integrated model with
different values of β in Figure 1(c). From Figure
1(c), we can know that β is an important factor for
us to balance F-score and accuracy. Our integrated
model may help alleviate the influence of noise in
NER in Chinese social media.

4 Conclusions and Future Work

The results of our experiments also suggest direc-
tions for future work. We can observe all models
in Table 3 achieve a much lower recall than pre-
cision (Pink et al., 2014). So we need to design
some methods to solve the problem.

716



Acknowledgements

This work was supported in part by National Natu-
ral Science Foundation of China (No. 61673028),
and National High Technology Research and De-
velopment Program of China (863 Program, No.
2015AA015404). Xu Sun is the corresponding au-
thor of this paper.

References
Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,

and Xuanjing Huang. 2015. Long short-term mem-
ory neural networks for chinese word segmentation.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1197–1206, Lisbon, Portugal, September. Associa-
tion for Computational Linguistics.

Colin Cherry and Hongyu Guo. 2015. The unreason-
able effectiveness of word representations for twit-
ter named entity recognition. In Proceedings of the
2015 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 735–745, Den-
ver, Colorado, May–June. Association for Compu-
tational Linguistics.

Hai Leong Chieu and Hwee Tou Ng. 2002. Named en-
tity recognition: a maximum entropy approach using
global information. In Proceedings of the 19th inter-
national conference on Computational linguistics-
Volume 1, pages 1–7. Association for Computational
Linguistics.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Agustı́n D. Delgado, Raquel Martı́nez, Vı́ctor Fresno,
and Soto Montalvo. 2014. A data driven approach
for person name disambiguation in web search re-
sults. In Proceedings of COLING 2014, the 25th In-
ternational Conference on Computational Linguis-
tics: Technical Papers, pages 301–310, Dublin, Ire-
land, August. Dublin City University and Associa-
tion for Computational Linguistics.

Alex Graves, Santiago Fernndez, Faustino Gomez, and
Jrgen Schmidhuber. 2006. Connectionist temporal
classification: labelling unsegmented sequence data
with recurrent neural networks. In International
Conference, pages 369–376.

Mena Habib and Maurice van Keulen. 2015.
Need4tweet: A twitterbot for tweets named entity
extraction and disambiguation. In Proceedings of
ACL-IJCNLP 2015 System Demonstrations, pages
31–36, Beijing, China, July. Association for Com-
putational Linguistics and The Asian Federation of
Natural Language Processing.

Hannaneh Hajishirzi, Leila Zilles, Daniel S. Weld, and
Luke Zettlemoyer. 2013. Joint coreference res-
olution and named-entity linking with multi-pass
sieves. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 289–299, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.

James Hammerton. 2003. Named entity recognition
with long short-term memory. In Proceedings of the
seventh conference on Natural language learning at
HLT-NAACL 2003-Volume 4, pages 172–175. Asso-
ciation for Computational Linguistics.

Hangfeng He and Xu Sun. 2017. A unified model
for cross-domain and semi-supervised named entity
recognition in chinese social media. In AAAI 2017.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Chen Li and Yang Liu. 2015. Improving named en-
tity recognition in tweets via detecting non-standard
words. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 929–938, Beijing, China, July. Association for
Computational Linguistics.

Yudong Liu, Clinton Burkhart, James Hearne, and
Liang Luo. 2015. Enhancing sumerian lemmatiza-
tion by unsupervised named-entity recognition. In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 1446–1451, Denver, Colorado, May–
June. Association for Computational Linguistics.

Xinnian Mao, Yuan Dong, Saike He, Sencheng Bao,
and Haila Wang. 2008. Chinese word segmentation
and named entity recognition based on conditional
random fields. In IJCNLP, pages 90–93.

Andrew Mccallum, Dayne Freitag, and Fernando C. N.
Pereira. 2001. Maximum entropy markov models
for information extraction and segmentation. Proc
of Icml, pages 591–598.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Arvind Neelakantan and Michael Collins. 2015.
Learning dictionaries for named entity recognition
using minimal supervision. Computer Science.

717



Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 293–303, Bal-
timore, Maryland, June. Association for Computa-
tional Linguistics.

Nanyun Peng and Mark Dredze. 2015. Named en-
tity recognition for chinese social media with jointly
trained embeddings. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 548–554, Lisbon, Portugal,
September. Association for Computational Linguis-
tics.

Nanyun Peng and Mark Dredze. 2016. Improving
named entity recognition for chinese social media
with word segmentation representation learning. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 149–155, Berlin, Germany,
August. Association for Computational Linguistics.

Glen Pink, Joel Nothman, and James R. Curran. 2014.
Analysing recall loss in named entity slot filling.
In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 820–830, Doha, Qatar, October.
Association for Computational Linguistics.

Will Radford, Xavier Carreras, and James Henderson.
2015. Named entity recognition with document-
specific kb tag gazetteers. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing, pages 512–517, Lisbon,
Portugal, September. Association for Computational
Linguistics.

Maria Skeppstedt. 2014. Enhancing medical named
entity recognition with features derived from unsu-
pervised methods. In Proceedings of the Student Re-
search Workshop at the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics.

Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, and
Jun’ichi Tsujii. 2009. Latent variable perceptron al-
gorithm for structured classification. In Proceedings
of the 21st International Joint Conference on Artifi-
cial Intelligence (IJCAI 2009), pages 1236–1242.

Xu Sun, Wenjie Li, Houfeng Wang, and Qin Lu. 2014.
Feature-frequency-adaptive on-line training for fast
and accurate natural language processing. Compu-
tational Linguistics, 40(3):563–586.

Xu Sun. 2014. Structure regularization for structured
prediction. In Advances in Neural Information Pro-
cessing Systems 27, pages 2402–2410.

Dirk Weissenborn, Leonhard Hennig, Feiyu Xu, and
Hans Uszkoreit. 2015. Multi-objective optimiza-
tion for the joint disambiguation of nouns and named
entities. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics

and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 596–605, Beijing, China, July. Association for
Computational Linguistics.

718


