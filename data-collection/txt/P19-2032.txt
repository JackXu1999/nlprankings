



















































Automatic Generation of Personalized Comment Based on User Profile


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 229–235
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

229

Automatic Generation of Personalized Comment Based on User Profile

Wenhuan Zeng1∗, Abulikemu Abuduweili2∗, Lei Li3, Pengcheng Yang4
1School of Mathematical Sciences, Peking University

2State Key Lab of Advanced Optical Communication System and Networks,
School of EECS, Peking University

3School of Computer Science and Technology, Xidian University
4MOE Key Lab of Computational Linguistics, School of EECS, Peking University

{zengwenhuan, abduwali}@pku.edu.cn
tobiaslee@foxmail.com, yang pc@pku.edu.cn

Abstract

Comments on social media are very diverse, in
terms of content, style and vocabulary, which
make generating comments much more chal-
lenging than other existing natural language
generation (NLG) tasks. Besides, since dif-
ferent user has different expression habits, it
is necessary to take the user’s profile into con-
sideration when generating comments. In this
paper, we introduce the task of automatic gen-
eration of personalized comment (AGPC) for
social media. Based on tens of thousands of
users’ real comments and corresponding user
profiles on weibo, we propose Personalized
Comment Generation Network (PCGN) for
AGPC. The model utilizes user feature embed-
ding with a gated memory and attends to user
description to model personality of users. In
addition, external user representation is taken
into consideration during the decoding to en-
hance the comments generation. Experimental
results show that our model can generate natu-
ral, human-like and personalized comments.1

1 Introduction

Nowadays, social media is gradually becoming a
mainstream communication tool. People tend to
share their ideas with others by commenting, re-
posting or clicking like on posts in social media.
Among these behaviors, comment plays a signif-
icant role in the communication between posters
and readers. Automatically generate personalized
comments (AGPC) can be useful due to the fol-
lowing reasons. First, AGPC helps readers express
their ideas more easily, thus make them engage
more actively in the platform. Second, bloggers
can capture different attitudes to the event from
multiple users with diverse backgrounds. Lastly,

∗Equal Contribution.
1Source codes of this paper are available at

https://github.com/Walleclipse/AGPC

the platform can also benefit from the increasing
interactive rate.

Despite its great applicability, the AGPC task
faces two important problems: whether can we
achieve it and how to implement it? The Social
Differentiation Theory proposed by Riley and Ri-
ley (1959) proved the feasibility of building a uni-
versal model to automatically generate personal-
ized comments based on part of users’ data. The
Individual Differences Theory pointed by Hovland
et al. (1953) answers the second question by in-
troducing the significance of users’ background,
which inspires us to incorporating user profile
into comments generation process. More specifi-
cally, the user profile consists of demographic fea-
tures (for example, where does the user live), in-
dividual description and the common word dic-
tionary extracted from user’s comment history.
There are few works exploring the comments gen-
eration problem. Zheng et al. (2017) first paid
attention to generating comments for news arti-
cles by proposing a gated attention neural net-
work model (GANN) to address the contextual rel-
evance and the diversity of comments. Similarly,
Qin et al. (2018) introduced the task of automatic
news article commenting and released a large scale
Chinese corpus. Nevertheless, AGPC is a more
challenging task, since it not only requires gener-
ating relevant comments given the blog text, but
also needs the consideration of the diverse users’
background.

In this paper, we propose a novel task, automat-
ically generating personalized comment based on
user profile. We build the bridge between user
profiles and social media comments based on a
large-scale and high-quality Chinese dataset. We
elaborately design a generative model based on
sequence-to-sequence (Seq2Seq) framework. A
gated memory module is utilized to model the user
personality. Besides, during the decoding process,

https://github.com/Walleclipse/AGPC


230

the model attends to user description to enhance
the comments generation process. In addition,
the vocabulary distribution of generated word is
adapted by considering the external user represen-
tation.

Our main contributions are as follows:

• We propose the task of automatic generating
personalized comment with exploiting user
profile.

• We design a novel model to incorporate the
personalized style in large-scale comment
generation. The model has three novel mech-
anisms: user feature embedding with gated
memory, blog-user co-attention, and an ex-
ternal personality expression.

• Experimental results show that the pro-
posed method outperforms various competi-
tive baselines by a large margin. With novel
mechanisms to exploit user information, the
generated comments are more diverse and in-
formative. We believe that our work can ben-
efit future work on developing personalized
and human-like NLG model.

2 Personalized Comments Dataset

We introduce the dataset as follows:

Data Preparation We collect short text posts
from Weibo, one of the most popular social me-
dia platform in China, which has hundreds of mil-
lions of active users. Each instance in the dataset
has province, city, gender, age, marital status, in-
dividual description of user’s, comment added by
user and homologous blog content. Figure 1 vi-
sually shows a sample instance. We tokenized
all text like individual description, comment and
blog content into words, using a popular python
library Jieba2. To facilitate the model to learn
valid information from the dataset, we removed
@, url, expressions in the text, and unified Chinese
into simplified characters. Discrete variables such
as province, city, gender and marital status were
treated uniformly by one-hot coding. To ensure
the quality of text, we filtered out samples with
less than two words in the variable of comment
and blog content. Besides, in order to learn user-
specific expression habits, we retain users with 50
or more records. The resulting dataset contains

2https://github.com/fxsjy/jieba

UID:
215803

Age:
24

Birthday:
1994-01-21

Gender:
Female

Province:
Shanghai

City:
NULL

Individual Description:

Practice makes prefect

Blog:

The doctor prescribed the new medicine which let my
stomach uncomfortable. If only everything were a 
dream.

Comment:

Everything will be ok.

Figure 1: A data example in personalized comment
dataset. Corresponding English translation is provided.

Statistic User Comment Microblog
Train 32,719 2,659,870 1,450,948
Dev 24,739 69,659 27,822
Test 20,157 43,866 17,052
Total 32,719 4,463,767 1,495,822

Table 1: Sample size of three datasets

4,463,767 comments on 1,495,822 blog posts by
32,719 users.

Data Statistics We split the corpus into train-
ing, validation and testing set according to the mi-
croblog. To avoid overfitting, the records of the
same microblog will not appear in the above three
sets simultaneously. Table 1 displays the detail
sample size of user, comment and blog about train-
ing set, validation set and testing set. Each user
in the resulting dataset has an average of 56 sam-
ples. The average lengths of blog post, comment
and individual description are 50, 11 and 9 words,
respectively. The particular statistics of each ex-
perimental dataset are shown in Table 2.

Average length Train Dev Test Total
ID 8.84 9.04 8.83 8.85
Comment 11.28 11.32 11.86 11.28
Microblog 49.67 47.95 50.30 49.65

Table 2: Statistics of text variables. Individual descrip-
tion, abbreviated ID.

https://github.com/fxsjy/jieba


231

User Numeric
Feature

Embedding

:;
Blog
Content
Encoder
LSTM%&'(

ℎ*( ℎ+( ℎ&(

, -* , -+ , -&⋯

⋯

⋯

Attentive Read

/0( /01

User
Description
Encoder
LSTM%&'1

ℎ*1 ℎ+1 ℎ&1

⋯

⋯

⋯

Attentive Read

, 2* , 2+ , 2&⋯

Gated Memory Module

304

506* 50 ⋯⋯

, 706+ , 706*

706*

LSTM8%'

⨁ External Personality 
Expression

Word Distribution 70

Figure 2: Personalized comment generation network

3 Personalized Comment Generation
Network

Given a blog X = (x1, x2, · · · , xn) and a user
profile U = {F,D}, where F = (f1, f2, · · · , fk)
denotes the user’s numeric feature (for example,
age, city, gender) and D = (d1, d2, · · · , dl) de-
notes the user’s individual description, the AGPC
aims at generating comment Y = (y1, y2, · · · , ym)
that is coherent with blog X and user U . Fig-
ure 2 presents an overview of our proposed model,
which is elaborated on in detail as follows.

3.1 Encoder-Decoder Framework

Our model is based on the encoder-decoder
framework of the general sequence-to-sequence
(Seq2Seq) model (Sutskever et al., 2014). The
encoder converts the blog sequence X =
(x1, x2, · · · , xn) to hidden representations hX =
(hX1 , h

X
2 , · · · , hXn ) by a bi-directional Long Short-

Term Memory (LSTM) cell (Hochreiter and
Schmidhuber, 1997):

hXt = LSTM
X
enc(h

X
t−1, xt) (1)

The decoder takes the embedding of a previously
decoded word e(yt−1) and a blog context vector
cXt as input to update its state st:

st = LSTMdec(st−1, [c
X
t ; e(yt−1)]) (2)

where [·; ·] denotes vector concatenation. The con-
text vector cXt is a weighted sum of encoder’s hid-
den states, which carries key information of the
input post (Bahdanau et al., 2014). Finally, the
decoder samples a word yt from the output proba-
bility distribution as follows

yt ∼ softmax(Wost) (3)

where Wo is a weight matrix to be learned. The
model is trained via maximizing the log-likelihood
of ground-truth Y ∗ = (y∗1, · · · , y∗n) and the objec-
tive function is defined as

L = −
n∑

t=1

log
(
p(y∗t |y∗<t, X, U)

)
(4)

3.2 User Feature Embedding with Gated
Memory

To encode the information in user profile, we map
user’s numeric feature F to a dense vector vu
through a fully-connected layer. Intuitively, vu can
be treated as a user feature embedding denotes the
character of the user. However, if the user feature
embedding is static during decoding, the gram-
matical correctness of sentences generated may be
sacrificed as argued in Ghosh et al. (2017). To
tackle this problem, we design an gated memory
module to dynamically express personality during
decoding, inspired by Zhou et al. (2018). Specifi-
cally, we maintain a internal personality state dur-
ing the generation process. At each time step, the
personality state decays by a certain amount. Once
the decoding process is completed, the personality
state is supposed to decay to zero, which indicates
that the personality is completely expressed. For-
mally, at each time step t, the model computes an
update gate gut according to the current state of
the decoder st. The initial personality state M0 is
set as user feature embedding vu. Hence, the per-
sonality stateMt is erased by a certain amount (by
gut ) at each step. This process is described as

gut = sigmoid(W
u
gst) (5)

M0 = vu (6)

Mt = g
u
t ⊗Mt−1, t > 0 (7)

where⊗ denotes element-wise multiplication. Be-
sides, the model should decide how much atten-



232

tion should be paid to the personality state at each
time step. Thus, output gate got is introduced to
control the information flow by considering the
previous decoder state st−1, previous target word
e(yt−1) and the current context vector cXt

got = sigmoid(W
o
g[st−1; e(yt−1); c

X
t ]). (8)

By an element-wise multiplication of got and Mt,
we can obtain adequate personality information
Mot for current decoding step

Mot = g
o
t ⊗Mt. (9)

3.3 Blog-User Co-Attention

Individual description is another important infor-
mation source when generating personalized com-
ments. For example, a user with individual de-
scription “只爱朱一龙” (I only love Yilong Zhu3),
tends to writes a positive and adoring comments
on the microblog related to Zhu. Motivated by
this, we propose Blog-user co-attention to model
the interactions between user description and blog
content. More specifically, we encode the user’s
individual descriptionD = (d1, d2, · · · , dl) to hid-
den states (hD1 , h

D
2 , · · · , hDl ) via another LSTM

hDt = LSTM
D
enc(h

D
t−1, dt) (10)

We can obtain a description context vector cDt by
attentively reading the hidden states of user de-
scription,

cDt =
∑
j

αtjh
D
j (11)

αtj = softmax(etj) (12)

etj = st−1Wah
D
j (13)

where etj is a alignment score (Bahdanau et al.,
2014). Similarly, we can get the blog content vec-
tor cXt . Finally, the context vector ct is a concate-
nation of cXt and c

D
t , in order provide more com-

prehensive information of user’s personality

ct = [c
X
t ; c

D
t ] (14)

Therefore, the state update mechanism in Eq.(2) is
modified to

st = LSTMdec(st−1, [ct; e(yt−1);M
o
t ]) (15)

3A famous Chinese star.

3.4 External Personality Expression

In the gated memory module, the correlation be-
tween the change of the internal personality state
and selection of a word is implicit. To fully ex-
ploit the user information when selecting words
for generation, we first compute a user representa-
tion rut with user feature embedding and user de-
scription context.

rut = Wr[vu; c
D
t ] (16)

where Wr is a weight matrix to align user repre-
sentation dimention.

The final word is then sampled from output dis-
tribution based on the concatenation of decoder
state st and rut as

ỹt ∼ softmax(Wõt [st; rut ]) (17)

where Wõt is a learnable weight matrix.

4 Experiments

4.1 Implementation

The blog content encoder and comment decoder
are both 2-layer bi-LSTM with 512 hidden units
for each layer. The user’s personality description
encoder is a single layer bi-LSTM with 200 hidden
units. The word embedding size is set to 300 and
vocabulary size is set to 40,000. The embedding
size of user’s numeric feature is set to 100.

We adopted beam search and set beam size to 10
to promote diversity of generated comments. We
used SGD optimizer with batch size set to 128 and
the learning rate is 0.001.

To further enrich the information provided by
user description, we collected most common k
words in user historical comments (k = 20 in our
experiment). We concatenate the common words
with the user individual description. Therefore,
we can obtain more information about users’ ex-
pression style. The model using concatenated user
description is named PCGN with common words
(PCGN+ComWord).

4.2 Baseline

We implemented a general Seq2Seq model
(Sutskever et al., 2014) and a user embedding
model (Seq2Seq+Emb) proposed by Li et al.
(2016) as our baselines. The latter model embeds
user numeric features into a dense vector and feeds
it as extra input into decoder at every time step.



233

Method PPL B-2 METEOR

Seq2Seq 32.47 0.071 0.070
Seq2Seq+Emb 31.13 0.084 0.079

PCGN 27.94 0.162 0.132
PCGN+ComWord 24.48 0.193 0.151

Table 3: Automatic evaluation results of different
methods. PPL denotes perplexity and B-2 denotes
BLEU-2. Best results are shown in bold.

Method PPL B-2

Seq2Seq 32.47 0.071
+ Mem 30.73 (-1.74) 0.099 (+0.028)
+ CoAtt 27.12 (-3.61) 0.147 (+0.078)
+ External 27.94 (+0.82) 0.162 (+0.015)

Table 4: Incremental experiment results of proposed
model. Performance on METEOR is similar to B-2.
Mem denotes gated memory, CoAtt denotes blog-user
co-attention and External denotes external personality
expression

4.3 Evaluation Result

Metrics: We use BLEU-2 (Papineni et al.,
2002) and METEOR (Banerjee and Lavie, 2005)
to evaluate overlap between outputs and refer-
ences. Besides, perplexity is also provided.

Results: The results are shown in Table 3.
As can be seen, PCGN model with common
words obtains the best performance on perplex-
ity, BLEU-2 and METEOR. Note that the per-
formance of Seq2Seq is extremely low, since the
user profile is not taken into consideration during
the generation, resulting repetitive responses. In
contrast, with the help of three proposed mecha-
nism (gated memory, blog-user co-attention and
external personality expression), our model can
utilize user information effectively, thus is capable
of generating diverse and relevant comments for
the same blog. Further, we conducted incremental
experiments to study the effect of proposed mech-
anisms by adding them incrementally, as shown in
Table 4. It can be found that all three mechanism
help generate more diverse comments, while blog-
user co-attention mechanism contributes most im-
provements. An interesting finding is that external
personality expression mechanism causes the de-
cay on perplexity. We speculate that the modifica-
tion on word distribution by personality influence
the fluency of generated comments.

5 Related Work

This paper focuses on comments generation task,
which can be further divided into generating a
comment according to the structure data (Mei
et al., 2015), text data (Qin et al., 2018), im-
age (Vinyal et al., 2015) and video (Ma et al.,
2018a), separately.

There are many works exploring the problem
of text-based comment generation. Qin et al.
(2018) contributed a high-quality corpus for ar-
ticle comment generation problem. Zheng et al.
(2017) proposed a gated attention neural network
model (GANN) to generate comments for news
article, which addressed the contextual relevance
and the diversity of comments. To alleviate the
dependence on large parallel corpus, Ma et al.
(2018b) designed an unsupervised neural topic
model based on retrieval technique. However,
these works focus on generating comments on
news text, while comments on social media are
much more diverse and personal-specific.

In terms of the technique for modeling user
character, the existing works on machine com-
menting only utilized part of users’ information.
Ni and McAuley (2018) proposed to learn a latent
representation of users by utilizing history infor-
mation. Lin et al. (2018) acquired readers’ gen-
eral attitude to event mentioned by article through
its upvote count. Compared to the indirection in-
formation obtained from history or indicator, user
features in user profile, like demographic factors,
can provide more comprehensive and specific in-
formation, and thus should be paid more attention
to when generating comments. Sharing the same
idea that user personality counts, Luo et al. (2018)
proposed personalized MemN2N to explore per-
sonalized goal-oriented dialog systems. Equipped
with a profile model to learn user representation
and a preference model learning user preferences,
the model is capable of generating high quality re-
sponses. In this paper, we focus on modeling per-
sonality in a different scenario, where the gener-
ated comments is supposed to be general and di-
verse.

6 Conclusion

In this paper, we introduce the task of auto-
matic generating personalized comment. We also
propose Personality Comment Generation Net-
work (PCGN) to model the personality influence
in comment generation. The PCGN model utilized



234

gated memory for user feature embedding, blog-
user co-attention, and external personality repre-
sentation to generate comments in personalized
style. Evaluation results show that PCGN outpe-
forms baseline models by a large margin. With the
help of three proposed mechanisms, the generated
comments are more fluent and diverse.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evalu-
ation measures for machine translation and/or sum-
marization, pages 65–72.

Sayan Ghosh, Mathieu Chollet, Eugene Laksana,
Louis-Philippe Morency, and Stefan Scherer. 2017.
Affect-lm: A neural language model for customiz-
able affective text generation. arXiv preprint
arXiv:1704.06851.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Carl I. Hovland, Irving L. Janis, and Harold H.
Kelley. 1953. Communication and Persua-
sion:Psychological Studies of Opinion Change.
Yale University Press.

Jiwei Li, Michel Galley, Chris Brockett, Georgios P
Spithourakis, Jianfeng Gao, and Bill Dolan. 2016.
A persona-based neural conversation model. arXiv
preprint arXiv:1603.06155.

Zhaojiang Lin, Genta Indra Winata, and Pascale Fung.
2018. Learning comment generation by leveraging
user-generated data. CoRR, abs/1810.12264.

Liangchen Luo, Wenhao Huang, Qi Zeng, Zaiqing Nie,
and Xu Sun. 2018. Learning personalized end-to-
end goal-oriented dialog.

Shuming Ma, Lei Cui, Damai Dai, Furu Wei, and
Xu Sun. 2018a. Livebot: Generating live video
comments based on visual and textual contexts.
CoRR, abs/1809.04938.

Shuming Ma, Lei Cui, Furu Wei, and Xu Sun.
2018b. Unsupervised machine commenting with
neural variational topic model. arXiv preprint
arXiv:1809.04960.

Hongyuan Mei, Mohit Bansal, and Matthew R. Walter.
2015. What to talk about and how? selective gen-
eration using lstms with coarse-to-fine alignment.
arXiv preprint arXiv:1509.00838.

Jianmo Ni and Julian McAuley. 2018. Personalized re-
view generation by expanding phrases and attending
on aspect-aware representations. In In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics, pages 706–711.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Lianhui Qin, Lemao Liu, Wei Bi, Yan Wang, Xiaojiang
Liu, Zhiting Hu, Hai Zhao, and Shuming Shi. 2018.
Automatic article commenting: the task and dataset.
In Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2018,
Melbourne, Australia, July 15-20, 2018, Volume 2:
Short Papers, pages 151–156.

J. W. Riley and Matilda White Riley. 1959. Mass com-
munication and the social system.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Oriol Vinyal, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Proceedings of the IEEE
conference on computer vision and pattern recogni-
tion, pages 3156–3164.

Hai-Tao Zheng, Wei Wang, Wang Chen, and Arun Ku-
mar Sangaiah. 2017. Automatic generation of news
comments based on gated attention neural networks.
6.

Hao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan
Zhu, and Bing Liu. 2018. Emotional chatting ma-
chine: Emotional conversation generation with in-
ternal and external memory. In Thirty-Second AAAI
Conference on Artificial Intelligence.

A Case Study

We present some generated cases in Figure 4, 5.
There are multiple users (corresponding profiles
are shown in Figure 3) that are suitable for gen-
erating comments. Seq2Seq generates same com-
ments for the same blog, while PCGN can gen-
erate personalized comment conditioned on given
user. According to the user profile, U1 adores Yi-
long Zhu very much. Therefore, U1 tends to ex-
press her affection in comments when responses
to blogs related to Yilong Zhu. For users whose
individual descriptions can not offer helpful infor-
mation or there is missing value for individual de-
scription, the PCGN model pays more attention
to numeric features and learns representation from
similar seen users.



235

User Age Gender Province City Individual Description

U1 24 女
Female

其他
Others

NULL 只爱朱一龙
I only love Yilong Zhu

U2 23 女
Female

黑龙江
Heilong Jiang

NULL 努力成为更好的自己
Become a better me

U3 20 女
Female

浙江
Zhejiang

宁波
Ningbo

NULL

Figure 3: Part of user profile of case study users. In order to protect user privacy, the birthday variable is not shown
here.

Blog
# [ ] # [ ] # # 

Yilong Zhu Gentle power [super topic]# walking side by side with Yilong Zhu#The moment I met you seems like the wind of the wilderness, 
breaking into my heart. The moment I met you seems like the snow between the eyebrows, blending into the eyes. I want to share everything 
with you, the warm sun in the morning, the vast night sky, the beauty of the past and the companion in the future.

Comments
Seq2Seq:
# [ ] # # #
#Yilong Zhu Gentle power [super topic]# #Yilong Zhu, move forward together#

PCGN U1:
100 99 

There is one hunderd ways of sweetness, have a candy and miss you 99 times a day.

PCGN U2:
# #
#Yilong Zhu Gentle power [super topic]# #Yilong Zhu, move forward together# Yilong Zhu| ZYL

PCGN U3:

I hope that you are always young, with a clean and pure heart, always believing something beautiful

Figure 4: Generated comments based on blog of different users. Since Seq2Seq model does not take user profile
into consideration, it generates same comments for the same blog.

Blog
# #

! Angelababy / Angelababy / / /
#My true friend # Using the lens of Japanese TV dramas and comics, Bazaar specially plans to create visual blockbusters, and present the 
relationship among the three main characters in the photo, which will give you a sneak of the movie before its showing! Angelababy Hairstyle / 
Liu Xueliang Angelababy Makeup / Chun Nan Lun Deng makeup hair / Jiancheng Li Yilong Zhu makeup hair / Pengkun Li

Comments
Seq2Seq:
# angelababy [ ] #
# angelababy [super topic] #

PCGN U1:
# [ ] # # # 
Yilong Zhu[super topic]# # # Looking forward to Jingan brother

PCGN U2:

looking forward to

PCGN U3:

looking forward to Lun Deng

Figure 5: Generated comments based on blog of different users.


