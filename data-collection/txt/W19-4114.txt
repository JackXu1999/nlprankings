



















































Multi-turn Dialogue Response Generation in an Adversarial Learning Framework


Proceedings of the 1st Workshop on NLP for Conversational AI, pages 121–132
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

121

Multi-turn Dialogue Response Generation in an
Adversarial Learning Framework

Oluwatobi Olabiyi
Capital One Conversation Research

Vienna VA
oluwatobi.olabiyi@capitalone.com

Alan Salimov
Capital One Conversation Research

San Fransisco CA
alan.salimov@capitalone.com

Anish Khazane
Capital One Conversation Research

San Fransisco CA
anish.khazane@capitalone.com

Erik T. Mueller
Capital One Conversation Research

Vienna VA
erik.mueller@capitalone.com

Abstract

We propose an adversarial learning approach
for generating multi-turn dialogue responses.
Our proposed framework, hredGAN, is based
on conditional generative adversarial networks
(GANs). The GAN’s generator is a mod-
ified hierarchical recurrent encoder-decoder
network (HRED) and the discriminator is a
word-level bidirectional RNN that shares con-
text and word embeddings with the generator.
During inference, noise samples conditioned
on the dialogue history are used to perturb
the generator’s latent space to generate sev-
eral possible responses. The final response is
the one ranked best by the discriminator. The
hredGAN shows improved performance over
existing methods: (1) it generalizes better than
networks trained using only the log-likelihood
criterion, and (2) it generates longer, more
informative and more diverse responses with
high utterance and topic relevance even with
limited training data. This improvement is
demonstrated on the Movie triples and Ubuntu
dialogue datasets using both automatic and hu-
man evaluations.

1 Introduction

Recent advances in deep neural network architec-
tures have enabled tremendous success on a num-
ber of difficult machine learning problems. While
these results are impressive, producing a deploy-
able neural network–based model that can engage
in open domain conversation still remains elusive.
A dialogue system needs to be able to generate
meaningful and diverse responses that are simul-
taneously coherent with the input utterance and
the overall dialogue topic. Unfortunately, earlier
conversation models trained with naturalistic dia-
logue data suffered greatly from limited contextual
information (Sutskever et al., 2014; Vinyals and
Le, 2015) and lack of diversity (Li et al., 2016a).

These problems often lead to generic and safe re-
sponses to a variety of input utterances.

Serban et al. (2016) and Xing et al. (2017)
proposed the Hierarchical Recurrent Encoder-
Decoder (HRED) network to capture long tempo-
ral dependencies in multi-turn conversations to ad-
dress the limited contextual information but the
diversity problem remained. In contrast, some
HRED variants such as variational (Serban et al.,
2017b) and multi-resolution (Serban et al., 2017a)
HREDs attempt to alleviate the diversity problem
by injecting noise at the utterance level and by ex-
tracting additional context to condition the gener-
ator on. While these approaches achieve a cer-
tain measure of success over the basic HRED, the
generated responses are still mostly generic since
they do not control the generator’s output. This is
because the output conditional distribution is not
calibrated. Li et al. (2016a), on the other hand,
consider a diversity promoting training objective
but their model is for single turn conversations and
cannot be trained end-to-end.

The generative adversarial network (GAN)
(Goodfellow et al., 2014) seems to be an appro-
priate solution to the diversity problem. GAN
matches data from two different distributions by
introducing an adversarial game between a gener-
ator and a discriminator. We explore hredGAN:
conditional GANs for multi-turn dialogue mod-
els with an HRED generator and discriminator.
hredGAN combines ideas from both generative
and retrieval-based multi-turn dialogue systems to
improve their individual performances. This is
achieved by sharing the context and word embed-
dings between the generator and the discrimina-
tor allowing for joint end-to-end training using
back-propagation. To the best of our knowledge,
no existing work has applied conditional GANs
to multi-turn dialogue models and especially not



122

with HRED generators and discriminators. We
demonstrate the effectiveness of hredGAN over
the VHRED for dialogue modeling with evalua-
tions on the Movie triples and Ubuntu technical
support datasets.

2 Related Work

Our work is related to end-to-end neural network–
based open domain dialogue models. Most
neural dialogue models use transduction frame-
works adapted from neural machine translation
(Sutskever et al., 2014; Bahdanau et al., 2015).
These Seq2Seq networks are trained end-to-end
with MLE criteria using large corpora of human-
to-human conversation data. Others use GAN’s
discriminator as a reward function in a reinforce-
ment learning framework (Yu et al., 2017) and
in conjunction with MLE (Li et al., 2017; Che
et al., 2017). Zhang et al. (2017) explored the idea
of GAN with a feature matching criterion. Xu
et al. (2017) and Zhang et al. (2018) employed
GAN with an approximate embedding layer as
well as with adversarial information maximization
respectively to improve Seq2Seq’s diversity per-
formance.

Still, Seq2Seq models are limited in their
ability to capture long temporal dependencies
in multi-turn conversation. Although Li et al.
(2016b) attempted to optimize a pair of Seq2Seq
models for multi-turn dialogue, the multi-turn ob-
jective is only applied at inference and not used for
actual model training. Hence the introduction of
HRED models (Serban et al., 2016, 2017a,b; Xing
et al., 2017) for modeling dialogue response in
multi-turn conversations. However, these HRED
models suffer from lack of diversity since they
are only trained with MLE criteria. On the other
hand, adversarial system has been used for eval-
uating open domain dialogue models (Bruni and
Fernndez, 2018; Kannan and Vinyals, 2017). Our
work, hredGAN, is closest to the combination of
HRED generation models (Serban et al., 2016)
and adversarial evaluation (Kannan and Vinyals,
2017).

3 Model

3.1 Adversarial Learning of Dialogue
Response

Consider a dialogue consisting of a sequence of
N utterances, x =

(
x1, x2, · · · , xN

)
, where

each utterance xi =
(
x1i , x

2
i , · · · , x

Mi
i

)
contains a

variable-length sequence of Mi word tokens such
that xij ∈ V for vocabulary V . At any time
step i, the dialogue history is given by xi =(
x1, x2, · · · , xi

)
. The dialogue response gener-

ation task can be defined as follows: Given a
dialogue history xi, generate a response yi =(
y1i , y

2
i , · · · , y

Ti
i

)
, where Ti is the number of gen-

erated tokens. We also want the distribution of
the generated response P (yi) to be indistinguish-
able from that of the ground truth P (xi+1) and
Ti = Mi+1. Conditional GAN learns a mapping
from an observed dialogue history, xi, and a se-
quence of random noise vectors, zi to a sequence
of output tokens, yi, G : {xi, zi} → yi. The gen-
erator G is trained to produce output sequences
that cannot be distinguished from the ground truth
sequence by an adversarially trained discriminator
D that is trained to do well at detecting the gener-
ator’s fakes. The distribution of the generator out-
put sequence can be factored by the product rule:

P (yi|xi) = P (y1i )
Ti∏
j=2

P
(
yji |y

1
i , · · · , y

j−1
i ,xi

)
(1)

P
(
yji |y

1
i , · · · , y

j−1
i ,xi

)
= PθG

(
y1:j−1i ,xi

)
(2)

where yi:j−1i = (y
1
i , · · · , y

j−1
i ) and θG are the pa-

rameters of the generator model. PθG
(
yi:j−1i ,xi

)
is an autoregressive generative model where the
probability of the current token depends on the
past generated sequence. Training the genera-
tor G with the log-likelihood criterion is unsta-
ble in practice, and therefore the past generated
sequence is substituted with the ground truth, a
method known as teacher forcing (Williams and
Zipser, 1989), i.e.,

P
(
yji |y

1
i , · · · , y

j−1
i ,xi

)
≈ PθG

(
x1:j−1i+1 ,xi

)
(3)

Using (3) in relation to GAN, we define our fake
sample as the teacher forcing output with some in-
put noise zi

yji ∼ PθG
(
x1:j−1i+1 ,xi, zi

)
(4)

and the corresponding real sample as ground truth
xji+1.

With the GAN objective, we can match the
noise distribution, P (zi), to the distribution of the
ground truth response, P (xi+1|xi). Varying the



123

noise input then allows us to generate diverse re-
sponses to the same dialogue history. Further-
more, the discriminator, since it is calibrated, is
used during inference to rank the generated re-
sponses, providing a means of controlling the gen-
erator output.

3.1.1 Objectives
The objective of a conditional GAN can be ex-
pressed as

LcGAN (G,D) = Exi,xi+1 [log D(xi+1,xi)]+
Exi,zi [1− logD(G(xi, zi),xi)] (5)

whereG tries to minimize this objective against an
adversarial D that tries to maximize it:

G∗, D∗ = argmin
G

max
D
LcGAN (G,D). (6)

Previous approaches have shown that it is benefi-
cial to mix the GAN objective with a more tradi-
tional loss such as cross-entropy loss (Lamb et al.,
2016; Li et al., 2017). The discriminator’s job re-
mains unchanged, but the generator is tasked not
only to fool the discriminator but also to be near
the ground truth xi+1 in the cross-entropy sense:

LMLE(G) = Exi,xi+1,zi [−log PθG
(
xi+1,xi, zi

)
].

(7)
Our final objective is,

G∗, D∗ = argmin
G

max
D

(
λGLcGAN (G,D)+

λMLMLE(G)
)
. (8)

It is worth mentioning that, without zi, the net
could still learn a mapping from xi to yi, but it
would produce deterministic outputs and fail to
match any distribution other than a delta function
(Isola et al., 2017). This is one key area where
our work is different from Lamb et al.’s and Li
et al.’s. The schematic of the proposed hredGAN
is depicted at the right hand side of Figure 1.

3.1.2 Generator
We adopted an HRED dialogue generator sim-
ilar to Serban et al. (2016, 2017a,b) and Xing
et al. (2017). The HRED contains three recur-
rent structures, i.e. the encoder (eRNN), con-
text (cRNN), and decoder (dRNN) RNN. The
conditional probability modeled by the HRED per
output word token is given by

PθG
(
yji |x

1:j−1
i+1 ,xi

)
= dRNN

(
E(xj−1i+1 ), h

j−1
i ,hi

)
(9)

where E(.) is the embedding lookup, hi =
cRNN(eRNN(E(xi),hi−1), eRNN(.) maps a
sequence of input symbols into fixed-length vec-
tor, and h and h are the hidden states of the de-
coder and context RNN, respectively.

In the multi-resolution HRED, (Serban et al.,
2017a), high-level tokens are extracted and pro-
cessed by another RNN to improve performance.
We circumvent the need for this extra process-
ing by allowing the decoder to attend to different
parts of the input utterance during response gener-
ation (Bahdanau et al., 2015; Luong et al., 2015).
We introduce a local attention into (9) and encode
the attention memory differently from the con-
text through an attention encoder RNN (aRNN),
yielding:

PθG
(
yji |x

1:j−1
i+1 ,xi

)
=

dRNN
(
E(xj−1i+1 ), h

j−1
i , a

j
i ,hi

)
(10)

where aji =
∑Mi

m=1
exp(αm)∑Mi

m=1 exp(αm)
h

′m
i , h

′m
i =

aRNN(E(xmi ), h
′m−1
i ), h

′
is the hidden state of

the attention RNN, and αk is either a logit projec-
tion of (hj−1i , h

′m
i ) in the case of Bahdanau et al.

(2015) or (hj−1i )
T ·h′mi in the case of Luong et al.

(2015). The modified HRED architecture is shown
in Figure 2.

Noise Injection: We inject Gaussian noise at
the input of the decoder RNN. Noise samples
could be injected at the utterance or word level.
With noise injection, the conditional probability of
the decoder output becomes

PθG
(
yji |x

1:j−1
i+1 , z

j
i ,xi

)
=

dRNN
(
E(xj−1i+1 ), h

j−1
i , a

j
i , z

j
i ,hi

)
(11)

where zji ∼ Ni(0, I), for utterance-level noise and
zji ∼ N

j
i (0, I), for word-level noise.

3.1.3 Discriminator

The discriminator shares context and word embed-
dings with the generator and can discriminate at
the word level (Lamb et al., 2016). The word-level
discrimination is achieved through a bidirectional
RNN and is able to capture both syntactic and con-
ceptual differences between the generator output
and the ground truth. The aggregate classification
of an input sequence, χ can be factored over word-



124

Figure 1: Left: The hredGAN architecture - The generator makes predictions conditioned on the dialogue
history, hi, attention, a

j
i , noise sample, z

j
i , and ground truth, x

j−1
i+1 . Right: RNN-based discriminator that

discriminates bidirectionally at the word level.

Figure 2: The HRED generator with local attention - The attention RNN ensures local relevance while the con-
text RNN ensures global relevance. Their states are combined to initialize the decoder RNN and the discriminator
BiRNN.

level discrimination and expressed as

D(xi, χ) = D(hi, χ) =

[ J∏
j=1

DRNN (hi, E(χ
j))

] 1
J

(12)

where DRNN (.) is the word discriminator RNN,
hi is an encoded vector of the dialogue history
xi obtained from the generator’s cRNN(.) out-
put, and χj is the jth word or token of the input se-
quence χ. χ = yi and J = Ti for the case of gen-
erator’s decoder output, χ = xi+1 and J = Mi+1
for the case of ground truth. The discriminator ar-
chitecture is depicted on the left hand side of Fig-
ure 1.

3.2 Adversarial Generation of Multi-turn
Dialogue Response

In this section, we describe the generation process
during inference. The generation objective can be

mathematically described as
y∗i = argmax

l

{
P (yi,l|xi) +D∗(xi, yi,l)]

}L
l=1

(13)
where yi,l = G∗(xi, zi,l), zi,l is the lth noise sam-
ples at dialogue step i, and L is the number of re-
sponse samples. Equation 13 shows that our infer-
ence objective is the same as the training objective
(8), combining both the MLE and adversarial cri-
teria. This is in contrast to existing work where
the discriminator is usually discarded during in-
ference.

The inference described by (13) is intractable
due to the enormous search space of yi,l. There-
fore, we turn to an approximate solution where
we use greedy decoding (MLE) on the first part
of the objective function to generate L lists of re-
sponses based on noise samples {zi,l}Ll=1. In or-
der to facilitate the exploration of the generator’s
latent space, we sample a modified noise distri-
bution, zji,l ∼ Ni,l(0, αI), or z

j
i,l ∼ N

j
i,l(0, αI)



125

Algorithm 1 Adversarial Learning of hredGAN
Require: A generatorG with parameters θG.
Require: A discriminatorD with parameters θD .

for number of training iterations do
Initialize cRNN to zero state, h0
Sample a mini-batch of conversations, x = {xi}Ni=1, xi =
(x1, x2, · · · , xi) withN utterances. Each utterance mini batch i con-
tainsMi word tokens.
for i = 1 toN − 1 do

Update the context state.
hi = cRNN(eRNN(E(xi)),hi−1)
Compute the generator output using (11).

PθG
(
yi|, zi,xi

)
=
{
PθG

(
yji |x

1:j−1
i+1 , z

j
i ,xi

)}Mi+1
j=1

Sample a corresponding mini batch of utterance yi.
yi ∼ PθG

(
yi|, zi,xi

)
end for
Compute the discriminator accuracy Dacc over N − 1 utterances
{yi}N−1i=1 and {xi+1}

N−1
i=1

ifDacc < accDth then
Update θD with gradient of the discriminator loss.∑
i
[∇θD logD(hi, xi+1) +∇θD log

(
1−D(hi, yi)

)
]

end if
ifDacc < accGth then

Update θG with the generator’s MLE loss only.∑
i
[∇θG logPθG

(
yi|, zi,xi

)
]

else
Update θG with both adversarial and MLE losses.∑
i
[λG∇θG logD(hi, yi)+λM∇θG logPθG

(
yi|, zi,xi

)
]

end if
end for

where α > 1.0, is the exploration factor that in-
creases the noise variance. We then rank theL lists
using the discriminator score,

{
D∗(xi, yi,l)]

}L
l=1

.
The response with the highest discriminator rank-
ing is the optimum response for the dialogue con-
text.

4 Training of hredGAN

We trained both the generator and the discrimina-
tor simultaneously as highlighted in Algorithm ??
with λG = λM = 1. GAN training is prone to
instability due to competition between the gener-
ator and the discriminator. Therefore, parameter
updates are conditioned on the discriminator per-
formance (Lamb et al., 2016).

The generator consists of four
RNNs with different parameters, that is,
aRNN, eRNN, cRNN , and dRNN . aRNN
and eRNN are both bidirectional, while cRNN
and dRNN are unidirectional. Each RNN has
3 layers, and the hidden state size is 512. The
dRNN and aRNN are connected using an
additive attention mechanism (Bahdanau et al.,
2015).

The discriminator shares aRNN, eRNN ,
and cRNN with the generator. DRNN is a
stacked bidirectional RNN with 3 layers and a
hidden state size of 512. The cRNN states are
used to initialize the states of DRNN . The out-
put of both the forward and the backward cells for

each word are concatenated and passed to a fully-
connected layer with binary output. The output is
the probability that the word is from the ground
truth given the past and future words of the se-
quence.

Others: All RNNs used are gated recurrent unit
(GRU) cells (Cho et al., 2014). The word embed-
ding size is 512 and shared between the generator
and the discriminator. The initial learning rate is
0.5 with decay rate factor of 0.99, applied when
the adversarial loss has increased over two itera-
tions. We use a batch size of 64 and clip gradi-
ents around 5.0. As in Lamb et al. (2016), we
find accDth = 0.99 and accGth = 0.75 to suf-
fice. All parameters are initialized with Xavier
uniform random initialization (Glorot and Bengio,
2010). The vocabulary size V is 50, 000. Due to
the large vocabulary size, we use sampled softmax
loss (Jean et al., 2015) for MLE loss to expedite
the training process. However, we use full softmax
for evaluation. The model is trained end-to-end
using the stochastic gradient descent algorithm.

5 Experiments and Results

We consider the task of generating dialogue re-
sponses conditioned on the dialogue history and
the current input utterance. We compare the pro-
posed hredGAN model against some alternatives
on publicly available datasets.

5.1 Datasets

Movie Triples Corpus (MTC) dataset (Serban
et al., 2016). This dataset was derived from the
Movie-DiC dataset by Banchs (2012). Although
this dataset spans a wide range of topics with
few spelling mistakes, its small size of only about
240,000 dialogue triples makes it difficult to train
a dialogue model, as pointed out by Serban et al.
(2016). We thought that this scenario would really
benefit from the proposed adversarial generation.

Ubuntu Dialogue Corpus (UDC) dataset (Ser-
ban et al., 2017b). This dataset was extracted from
the Ubuntu Relay Chat Channel. Although the
topics in the dataset are not as diverse as in the
MTC, the dataset is very large, containing about
1.85 million conversations with an average of 5
utterances per conversation.

We split both MTC and UDC into training, val-
idation, and test sets, using 90%, 5%, and 5% pro-
portions, respectively. We performed minimal pre-
processing of the datasets by replacing all words



126

except the top 50,000 most frequent words by an
UNK symbol.

5.2 Evaluation Metrics

Accurate evaluation of dialogue models is still an
open challenge. In this paper, we employ both au-
tomatic and human evaluations.

5.2.1 Automatic Evaluation
We employed some of the automatic evaluation
metrics that are used in probabilistic language and
dialogue models, and statistical machine transla-
tion. Although these metrics may not correlate
well with human judgment of dialogue responses
(Liu et al., 2016), they provide a good baseline for
comparing dialogue model performance.

Perplexity - For a model with parameter θ, we
define perplexity as:

exp

[
− 1
NW

K∑
k=1

log Pθ(y1, y2, . . . , yNk−1)

]
(14)

where K is the number of conversations in the
dataset, Nk is the number of utterances in conver-
sation k, and NW is the total number of word to-
kens in the entire dataset. The lower the perplexity,
the better. The perplexity measures the likelihood
of generating the ground truth given the model pa-
rameters. While a generative model can generate a
diversity of responses, it should still assign a high
probability to the ground truth utterance.

BLEU - The BLEU score (Papineni et al., 2002)
provides a measure of overlap between the gen-
erated response (candidate) and the ground truth
(reference) using a modified n-gram precision.
According to Liu et. al. (Liu et al., 2016), BLEU-2
score is fairly correlated with human judgment for
non-technical dialogue (such as MTC).

ROUGE - The ROUGE score (Lin, 2014) is
similar to BLEU but it is recall-oriented instead.
It is used for automatic evaluation of text summa-
rization and machine translation. To compliment
the BLEU score, we use ROUGE-N with N = 2
for our evaluation.

Distinct n-gram - This is the fraction of unique
n-grams in the generated responses and it provides
a measure of diversity. Models with higher a num-
ber of distinct n-grams tend to produce more di-
verse responses (Li et al., 2016a). For our evalua-
tion, we use 1- and 2- grams.

Normalized Average Sequence Length
(NASL) - This measures the average number of

words in model-generated responses normalized
by the average number of words in the ground
truth.

5.2.2 Human Evaluation
For human evaluation, we follow a similar setup as
Li et al. (2016a), employing crowd-sourced judges
to evaluate a random selection of 200 samples. We
presented both the multi-turn context and the gen-
erated responses from the models to 3 judges and
asked them to rank the general response quality
in terms of relevance and informativeness. For N
models, the model with the lowest quality is as-
signed a score 0 and the highest is assigned a score
N-1. Ties are not allowed. The scores are normal-
ized between 0 and 1 and averaged over the total
number of samples and judges. For each model,
we also estimated the per sample score variance
between judges and then averaged over the num-
ber of samples, i.e., sum of variances divided by
the square of number of samples (assuming sam-
ple independence). The square root of result is re-
ported as the standard error of the human judg-
ment for the model.

5.3 Baseline

We compare the performance of our model to
(V)HRED (Serban et al., 2016, 2017b), since they
are the closest to our approach in implementation
and are the current state of the art in open-domain
dialogue models. HRED is very similar to our
proposed generator, but without the input utter-
ance attention and noise samples. VHRED intro-
duces a latent variable to the HRED between the
cRNN and the dRNN and was trained using the
variational lower bound on the log-likelihood. The
VHRED can generate multiple responses per con-
text like hredGAN, but it has no specific criteria
for selecting the best response.

The HRED and VHRED models are both
trained using the Theano-based implementa-
tion obtained from https://github.com/
julianser/hed-dlg-truncated. The
training and validation sets used for UDC and
MTC dataset were obtained directly from the au-
thors1 of (V)HRED. For model comparison, we
use a test set that is disjoint from the training and
validation sets.

1UDC was obtained from http:
//www.iulianserban.com/Files/
UbuntuDialogueCorpus.zip, and the link to MTC
was obtained privately.

https://github.com/julianser/hed-dlg-truncated
https://github.com/julianser/hed-dlg-truncated
http://www.iulianserban.com/Files/UbuntuDialogueCorpus.zip
http://www.iulianserban.com/Files/UbuntuDialogueCorpus.zip
http://www.iulianserban.com/Files/UbuntuDialogueCorpus.zip


127

Model Teacher Forcing Autoregression HumanPerplexity −logD(G(.)) BLEU-2 ROUGE-2 DISTINCT-1/2 NASL Evaluation

MTC
HRED 31.92/36.00 NA 0.0474 0.0384 0.0026/0.0056 0.535 0.2560 ± 0.0977
VHRED 42.61/44.97 NA 0.0606 0.1181 0.0048/0.0163 0.831 0.3909 ± 0.0240
hredGAN u 23.57/23.54 6.85/6.81 0.0493 0.2416 0.0167/0.1306 0.884 0.5582 ± 0.0118
hredGAN w 24.20/24.14 13.35/13.40 0.0613 0.3244 0.0179/0.1720 1.540 0.7869 ± 0.1148

UDC
HRED 69.39/86.40 NA 0.0177 0.0483 0.0203/0.0466 0.892 0.3475 ± 0.1062
VHRED 98.50/105.20 NA 0.0171 0.0855 0.0297/0.0890 0.873 0.4046 ± 0.0188
hredGAN u 56.82/57.32 10.09/10.08 0.0137 0.0716 0.0260/0.0847 1.379 0.6133 ± 0.0361
hredGAN w 47.73/48.18 8.37/8.36 0.0216 0.1168 0.0516/0.1821 1.098 0.6905 ± 0.0706

Table 1: Generator Performance Evaluation

5.4 Results

We have two variants of hredGAN based on
the noise injection approach, i.e., hredGAN
with utterance-level (hredGAN u) and word-level
(hredGAN w) noise injections.

We compare the performance of these two vari-
ants with HRED and VHRED models.

Perplexity: The average perplexity per word
performance of all the four models on MTC and
UDC datasets (validation/test) are reported in the
first column on Table 1. The table indicates that
both variants of the hredGAN model perform bet-
ter than the HRED and VHRED models in terms
of the perplexity measure. However, using the ad-
versarial loss criterion (Eq. (8)), the hredGAN u
model performs better on MTC and worse on
UDC. Note that, for this experiment, we run all
models in teacher forcing mode.

Generation Hyperparameter: For adversarial
generation, we perform a linear search for α be-
tween 1 and 20 at an increment of 1 using Eq.
(13), with sample size L = 64, on validation sets
with models run in autoregression. The optimum
values of α for hredGAN u and hredGAN w for
UDC are 7.0 and 9.0 respectively. The values for
MTC are not convex, probably due to small size of
the dataset, so we use the same α values as UDC.
We however note that for both datasets, any inte-
ger value between 3 and 10 (inclusive) works well
in practice.

Quantitative Generator Performance: We
run autoregressive inference for all the models (us-
ing optimum α values for hredGAN models and
selecting the best of L = 64 responses using a dis-
criminator) with dialogue contexts from a unique
test set. Also, we compute the average BLEU-
2, ROUGE-2(f1), Distinct(1/2), and normalized

Item D(G(.)) Utterance

MTC
Context 0 NA perhaps <person> had a word with the man upstairs .
Context 1 NA a word ? i ’ m sure by now he ’ s engineered a hostile takeover .
Response 0 0.996 <person> , i know what you ’ re saying , <person> , that ’ s

not what i ’ m saying .
Response 1 0.991 <person> , i know . i was just about to help the guy .
Response 2 0.315 <person> , i ’ m sorry .
Response 3 0.203 <person> , i ’ m a little out .

Context 0 NA says he wanted food . <person> . he wanted the gold .
Context 1 NA how ’ s he going to want the gold ? he couldn ’ t even know we

had it .
Response 0 0.998 <person> , i know . but it ’ s not him , it ’ s the only way he ’

s got it all figured
Response 1 0.981 <person> , i know . but i have to tell you . these things are

really stupid and you think i was wrong ?
Response 2 0.690 <person> , i ’ m sure he did .
Response 3 0.314 <person> , i ’ m not sure .

UDC
Context 0 NA The netboot one is suppose to download packages from the net.
Context 1 NA like the ones to be installed? or the installed to be run?
Response 0 0.993 you don ’ t need to install the whole system , just install the

ubuntu installer
Response 1 0.952 you can install the ubuntu installer from the ubuntu menu
Response 2 0.749 I ’ m not sure , I don ’ t know .
Response 3 0.184 you can
Response 4 0.003 you a . 1 . . . . . . . . . . . . . . . . . . . . .

Context 0 NA DJones: update manager won’t detect 12.04.1 as a new version
if you are already running 12.04, because 12.04.1 = 12.04 + lots
of package updates

Response 0 0.991 did you try a clean install of the latest version ?
Response 1 0.981 try installing the latest UNK and see if it works
Response 2 0.615 I ’ m not sure you have a problem . . .
Response 3 0.191 try sudo apt-get remove the package that is not installed
Response 4 0.002 try the UNK . 1 . 1 . 1 . 1 . 1 . UNK . deb

Table 2: Example of Discriminator Ranking with
hredGAN w

average sequence length (NASL) scores for each
model and summarize the results in the middle of
Table 1. Distinct(1/2) largely agrees with the per-
plexity score. Most scores, similar to the perplex-
ity, indicate that hredGAN models perform better
than (V)HRED on both datasets. However, on the
UDC ROUGE and MTC BLEU, VHRED scores
slightly better than hredGAN u but still worse than
hredGAN w.

A good dialogue model should find the right
balance between precision (BLEU) and diver-
sity. We strongly believe that our adversarial ap-
proach is better suited to solving this problem.



128

Movie

Context 0 perhaps <person> had a word with the man upstairs .
HRED i ’ m sorry , <person> .
VHRED <person< ’ t do it .
hredGAN w <person> , i ’ m not gon na be okay . i just don ’ t think it ’ s a good idea .
hredGAN u <person> , i cause it was all the way to you .

Context 1 a word ? i ’ m sure by now he ’ s engineered a hostile takeover .
HRED i ’ m sorry , <person> .
VHRED <person> ’ t do it .
hredGAN w <person> , i know what you ’ re saying , <person> , that ’ s not what i ’

m saying .
hredGAN u <person> , i have to tell you i ’ m not the same .

Context 0 <person> ’ s .
HRED i ’ m sorry , <person> .
VHRED <person> ’ t do it .
hredGAN w <person> , i ’ m sure it ’ s the only thing i ever imagined .
hredGAN u <person> , i ’ ll be right there .

Context 1 can i ask you something about her ?
HRED i don ’ t know .
VHRED: you ’ re not going to be here .
hredGAN w i don ’ t know . i think i ’ ve heard of it .
hredGAN u i ’ m sorry , i can ’ t .

Ubuntu

Context 0 this is Dacs i am in i used ’sudo statx and it let me in
HRED what ’s the problem ?
VHRED I don’t know what you want to do . if you want to do it , you need to do it to

do it in the terminal ?
hredGAN w you don ’ t have to do anything , just type in the terminal and it should be to

find the right device
hredGAN u you can have a look at the output of the command . . .

Context 1 good deal... cat /etc/X11/default-display-manager
HRED what ’s the problem ?
VHRED do you know what you want to do ?
hredGAN w I ’ m trying to figure a command that I can find to find out the file that I can

find in the file”
hredGAN u I don ’ t see the point , but I ’ m not sure how to do that .

Context 2 /usr/sbin/lightdm http://paste.ubuntu.com/1286224/ <—- my
/etc/X11/xorg.conf

HRED what ’s the problem ?
VHRED is there a way to do that in the terminal ?
hredGAN w did you just type \” sudo mount -a \” ?
hredGAN u i have no idea , i just installed ubuntu and i have no idea how to do that

Table 3: Sample responses of HRED, VHRED and
hredGAN.

As hredGAN generators explore diversity, the dis-
criminator ranking gives hredGAN an edge over
(V)HRED because it helps detect responses that
are out of context and the natural language struc-
ture (Table 2). Also, the ROGUE(f1) performance
indicates that hredGAN w strikes a better balance
between precision (BLEU) and diversity than the
rest of the models. This is also obvious from the
quality of generated responses.

Qualitative Generator Performance: The re-
sults of the human evaluation are reported in
the last column of Table 1. The human evalua-
tion agrees largely with the automatic evaluation.
hredGAN w performs best on both datasets al-
though the gap is more on the MTC than on the
UTC. This implies that the improvement of HRED
with adversarial generation is better than with vari-
ational generation (VHRED). In addition, look-
ing at the actual samples from the generator out-
puts in Table 6 shows that hredGAN, especially
hredGAN w, performs better than (V)HRED.
While other models produce short and generic ut-

terances, hredGAN w mostly yields informative
responses. For example, in the first dialogue in Ta-
ble 6, when the speaker is sarcastic about “the man
upstairs”, hredGAN w responds with the most co-
herent utterance with respect to the dialogue his-
tory. We see similar behavior across other sam-
ples. We also note that although hredGAN u’s re-
sponses are the longest on Ubuntu (in line with
the NASL score), the responses are less informa-
tive compared to hredGAN w resulting in a lower
human evaluation score. We reckon this might be
due to a mismatch between utterance-level noise
and word-level discrimination or lack of capacity
to capture the data distribution using single noise
distribution. We hope to investigate this further in
the future.

Discriminator Performance: Although only
hredGAN uses a discriminator, the observed dis-
criminator behavior is interesting. We observe
that the discriminator score is generally reasonable
with longer, more informative and more persona-
related responses receiving higher scores as shown
in Table 2. It worth to note that this behavior, al-
though similar to the behavior of a human judge
is learned without supervision. Moreover, the dis-
criminator seems to have learned to assign an av-
erage score to more frequent or generic responses
such as “I don’t know,” “I’m not sure,” and so on,
and high score to rarer answers. That’s why we
sample a modified noise distribution during infer-
ence so that the generator can produce rarer utter-
ances that will be scored high by the discriminator.

6 Conclusion and Future Work

In this paper, we have introduced an adversar-
ial learning approach that addresses response di-
versity and control of generator outputs, using
an HRED-derived generator and discriminator.
The proposed system outperforms existing state-
of-the-art (V)HRED models for generating re-
sponses in multi-turn dialogue with respect to
automatic and human evaluations. The perfor-
mance improvement of the adversarial genera-
tion (hredGAN) over the variational generation
(VHRED) comes from the combination of adver-
sarial training and inference which helps to ad-
dress the lack of diversity and contextual rele-
vance in maximum likelihood based generative di-
alogue models. Our analysis also concludes that
the word-level noise injection seems to perform
better in general.



129

References
D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural

machine translation by jointly learning to align and
translate. In Proceedings of International Confer-
ence of Learning Representation (ICLR 2015).

R. E. Banchs. 2012. Movie-dic: A movie dialogue cor-
pus for research and development. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics, pages 203–207.

E. Bruni and R. Fernndez. 2018. Adversarial evalua-
tion for open-domain dialogue generation. In Pro-
ceedings of the 18th Annual SIGdial Meeting.

T. Che, Y. Li, R. Zhang, R. D. Hjelm, W. Li, Y. Song,
and Y. Bengio. 2017. Maximum-likelihood aug-
mented discrete generative adversarial networks. In
arXiv preprint arXiv:1702.07983.

K. Cho, B. Merrienboer, C. Gulcehre, D. Bahdanau,
F. Bougares, H. Schwenk, and Y. Bengio. 2014.
Learning phrase representations using rnn encoder-
decoder for statistical machine translation. In Pro-
ceedings of International Conference of Learning
Representation (ICLR 2015), pages 1724–1734.

X. Glorot and Y. Bengio. 2010. Understanding the dif-
ficulty of training deep feedforward neural networks.
In International conference on artificial intelligence
and statistics.

I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-
gio. 2014. Generative adversarial nets. In Proceed-
ings of Advances in Neural Information Processing
Systems (NIPS 2014).

P. Isola, J. Y. Zhu, T. Zhou, and A. A. Efros. 2017.
Image-to-image translation with conditional adver-
sarial networks. In Conference on Computer Vision
and Pattern Recognition (CVPR, 2017).

S. Jean, K. Cho, R. Memisevic, and Y. Bengio.
2015. On using very large target vocabulary
for neural machine translation. In arXiv preprint
arXiv:1412.2007.

A. Kannan and O. Vinyals. 2017. Adversarial eval-
uation of dialogue models. In arXiv preprint
arXiv:1701.08198v1.

A. Lamb, A. Goyah, Y. Zhang, S. Zhang, A. Courville,
and Y. Bengio. 2016. Professor forcing: A new al-
gorithm for training recurrent networks. In Proceed-
ings of Advances in Neural Information Processing
Systems (NIPS 2016).

J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan.
2016a. A diversity-promoting objective function
for neural conversation models. In Proceedings of
NAACL-HLT.

J. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, and
D. Jurafsky. 2016b. Deep reinforcement learn-
ing for dialogue generation. In arXiv preprint
arXiv:arXiv:arXiv:1606.01541v4.

J. Li, W. Monroe, T. Shi, A. Ritter, and D. Jurafsky.
2017. Adversarial learning for neural dialogue gen-
eration. In arXiv preprint arXiv:1701.06547.

C. Y. Lin. 2014. Rouge: a package for automatic evalu-
ation of summaries. In Proceedings of the Workshop
on Text Summarization Branches Out.

C. Liu, R. Lowe, I. V. Serban, M. Noseworthy, L. Char-
lin, and J. Pineau. 2016. How not to evaluate your
dialogue system: An empirical study of unsuper-
vised evaluation metrics for dialogue response gen-
eration. In Proceedings of EMNLP, pages 2122–
2132.

M. T. Luong, I. Sutskever, Q. V. Le, O. Vinyals, and
W. Zaremba. 2015. Addressing the rare word prob-
lem in neural machine translation. In Proceedings
of the 53rd Annual Meeting of the Association for
Computational Linguistics.

K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: A method for automatic evalution of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 311–318.

I. Serban, A. Sordoni, Y. Bengio, A. Courville, and
J. Pineau. 2016. Building end-to-end dialogue sys-
tems using generative hierarchical neural network
models. In Proceedings of The Thirtieth AAAI Con-
ference on Artificial Intelligence (AAAI 2016), pages
3776–3784.

I. V. Serban, T. Klinger, G. Tesauro, K. Talamadupula,
B. Zhou, Y. Bengio, and A. Courville. 2017a. Mul-
tiresolution recurrent neural networks: An applica-
tion to dialogue response generation. In Proceed-
ings of The Thirty-first AAAI Conference on Artifi-
cial Intelligence (AAAI 2017).

I. V. Serban, A. Sordoni, R. Lowe, L. Charlin,
J. Pineau, A. Courville, and Y. Bengio. 2017b. A
hierarchical latent variable encoder-decoder model
for generating dialogue. In Proceedings of The
Thirty-first AAAI Conference on Artificial Intelli-
gence (AAAI 2017).

I. Sutskever, O. Vinyals, and Q. Le. 2014. Sequence
to sequence learning with neural networks. In Pro-
ceedings of Advances in Neural Information Pro-
cessing Systems (NIPS), pages 3104–3112.

O. Vinyals and Q. Le. 2015. A neural conversational
model. In Proceedings of ICML Deep Learning
Workshop.

R. J. Williams and D. Zipser. 1989. A learning algo-
rithm for continually running fully recurrent neural
networks. Neural computation, 1(2):270–280.

C. Xing, W. Wu, Y. Wu, M. Zhou, Y. Huang, and
W. Ma. 2017. Hierarchical recurrent attention net-
work for response generation. In arXiv preprint
arXiv:1701.07149.



130

Z. Xu, B. Liu, B. Wang, S. Chengjie, X. Wang,
Z. Wang, and C. Qi. 2017. Neural response gener-
ation via gan with an approximate embedding layer.
In EMNLP.

L. Yu, W. Zhang, J. Wang, and Y. Yu. 2017. Seqgan:
sequence generative adversarial nets with policy gra-
dient. In Proceedings of The Thirty-first AAAI Con-
ference on Artificial Intelligence (AAAI 2017).

Y. Zhang, M. Galley, J. Gao, Z. Gan, X. Li, C. Brock-
ett, and B. Dolan. 2018. Generating informative
and diverse conversational responses via adversar-
ial information maximization. In arXiv preprint
arXiv:arXiv:1809.05972v5.

Y. Zhang, Z. Gan, K. Fan, Z. Chen, R. Henao,
D. Shen, and L. Carin. 2017. Adversarial feature
matching for text generation. In arXiv preprint
arXiv:1706.03850.

A Ablation Experiments

Before proposing the above adversarial learning
framework for multi-turn dialogue, we carried out
some experiments.

A.1 Generator:

We consider two main factors here, i.e., addition
of an attention memory and injection of Gaussian
noise into the generator input.

A.1.1 Addition of Attention Memory
First, we noted that by adding an additional at-
tention memory to the HRED generator, we im-
proved the test set perplexity score by more than
12 and 25 points on the MTC and UDC respec-
tively as shown in Table 4. The addition of atten-
tion also shows strong performance at autoregres-
sive inference across multiple metrics as well as an
observed improvement in response quality. Hence
the decision for the modified HRED generator.

A.1.2 Injection of Noise
Before injecting noise into the generator, we first
train hredGAN without noise. The result is also
reported in 4. We observe accelerated generator
training but without an appreciable improvement
in performance. It seems the discrimination task
is very easy since there is no stochasticity in the
generator output. Therefore, the adversarial feed-
back does not meaningfully impact the generator
weight update.

Finally, we also notice that even with noise in-
jection, there is no appreciable improvement in
the auto-regressive performance if we sample with

L = 1 even though the perplexity is higher. How-
ever, as we increase L, producing L responses
per turn, the discriminator’s adversarial selection
gives a better performance as reported in Table 1.

Therefore, we conclude that the combination
of adversarial training and adversarial inference
helps to address the lack of diversity and con-
textual relevance observed in the generated re-
sponses.

A.2 Discriminator:
Before deciding on the word-level discrimination,
we experimented with utterance-level discrimina-
tion. The utterance-level discriminator trains very
quickly but it leads to mostly generic responses
from the generator. We also note that utterance-
level discriminator scores are mostly extreme (i.e.,
either low or high). Since we had used a convo-
lutional neural network discriminator (Yu et al.,
2017) in our experiments, we hope to investigate
this further with other architectures.

A.3 Adversarial Training:
Lastly, we also tried a basic policy gradient ap-
proach (Li et al., 2017), where word-level discrim-
inator score is used as a reward for each generated
word token, but this leads to training instability.
This is probably due to the instability of Monte
Carlo sampling over a large vocabulary size. We
believe this might improve with other sampling
methods such as importance sampling and hope to
investigate this further in the future.



131

Model Teacher Forcing AutoregressionPerplexity BLEU-2 ROUGE-2 DISTINCT-1/2 NASL

MTC
HRED 31.92/36.00 0.0474 0.0384 0.0026/0.0056 0.535
HRED+Attn 18.70/19.02 0.0425 0.2239 0.0397/0.1567 0.527
hredGAN no noise 18.93/19.19 0.0355 0.1839 0.0272/0.0978 0.471

UDC
HRED 69.39/86.40 0.0177 0.0483 0.0203/0.0466 0.892
HRED+Attn 43.43/43.92 0.0140 0.0720 0.0473/0.1262 0.760
hredGAN no noise 43.48/44.04 0.0123 0.0827 0.0398/0.1147 0.908

Table 4: Generator Performance: HRED, HRED+Attn and hredGAN without noise



132

Model Response

Context 0 <person> i ’ ve raised my rate . i might even cut my sessions to
forty minutes .

HRED i ’ m sorry , <person> .
VHRED <person>, <person>, <person>, <person> .
hredGAN w <person> , i ’ m sorry , but i ’ m not sure if you ’ re really a very

good guy .
hredGAN u i ’ ll never see them again , <person> .

Context 0 i ’ m going to handle this exactly the way your father would have
handled this . i ’ ve got it solved .

Context 1 you ’ ve got it solved ? where is she ?
HRED i don ’ t know .
VHRED <person>, , i ’ m sorry.
hredGAN w to the end of a beautiful woman who will be very happy .
hredGAN u i ’ m sorry , i just didn ’ t know –

Context 0 of course not ! ¡person¿ ’ s like a daughter to me . her father and i
taught at university together . a good man , but he never listened .

HRED i ’ m sorry , <person>.
VHRED <person>, mr . <person> .
hredGAN w <person> , i know it ’ s been all six years since i told him about

you .
hredGAN u i ’ m sure you didn ’ t , but i would have done a little more .

Context 0 of course not ! ¡person¿ ’ s like a daughter to me . her father and i
taught at university together . a good man , but he never listened .

Context 1 neither did i .
HRED i ’ m sorry , <person>.
VHRED <person> , you ’ re a <unk> .
hredGAN w <person>, i know you ’ re think about it . think it ’ ll be time , right

?
hredGAN u i ’ ll be the first one . if it did you , you ’ ll be it .

Context 0 come on , <person> , we could have a great week up in vermont .
Context 1 i see what you mean . but wednesday we ’ ve got to be back in new

york .
HRED <person> .
VHRED <person> , i don ’ t know what you ’ re going to do .
hredGAN w <person> , i know . i ’ m kind of busy here .
hredGAN u i ’ m sorry , i can ’ t .

Context 0 isabel ?
Context 1 hi .
HRED that ’ s what i ’ m talking about .
VHRED: you ’ re not going to be a <unk> , <person> .
hredGAN w <person> ?
hredGAN u what do you want to know ?

Context 0 you understand mr . <person> is constantly barraged with nui-
sance litigation and irs audits .

Context 1 of course .
HRED that ’ s right , <person> .
VHRED: <person> , you ’ re going to be a <unk> of the <unk> .
hredGAN w <person> , i ’ m sure , but you ’ re probably still in to the new

<person> ’ s house , huh ?
hredGAN u you ’ re gon na be a little boy , and i ’ m not gon na get out !

Context 0 <person> ’ t go into my room , she ’ s asleep .
Context 1 <person> , but the nanny , mrs . <person> , she wants to see her

.
HRED i ’ m sorry , <person> .
VHRED: <person> , mr . <person> .
hredGAN w <person> , i know . but she ’ s never seen me anyway .
hredGAN u i ’ m sorry i didn ’ t know what to say .

Context 0 what was that for ?
Context 1 i ’ d forgotten how exciting it is working together .
HRED i ’ m sorry , <person> .
VHRED: <person> , <person> . i don ’ t know .
hredGAN w i ’ m sorry , <person> . so many people are so interested , but you

’ re a very nice guy .
hredGAN u i ’ ve always been so sure to <person> .

Context 0 a teacher ’ s .
Context 1 oh my god .
HRED i ’ m sorry , <person> .
VHRED: <person> , <person> ’ t worry about it . i don ’ t know what you

’ re doing .
hredGAN w i ’ m sorry . i didn ’ t mean to . i didn ’ t mean to hurt you . i just

wanted you
hredGAN u <person> , i don ’ t want to be your friend .

Context 0 i took the initiative .
Context 1 you haven ’ t been given an assignment in this case .
HRED <person> .
VHRED: <person> , <person> , <person> , <person> . i ’ m not going

to get out of the <unk> .
hredGAN w i ’ m not sure . <continued utterance> i ’ m not gon na do you this

shit anyway , <person> .
hredGAN u <person> , i guess i didn ’ t .

Table 5: Additional sample responses of HRED,
VHRED and hredGAN on Movie.

Model Response

Context 0 what’s the command you are running to chroot and the error message
you get in return?

Context 1 i mountded the device sudo mount /dev/sda1 /mnt - that works - then
sudo chroot /mnt – error is no such file or command

HRED what ’s the problem ?
VHRED is there a way to do that ? what do you mean ?
hredGAN w I have no idea , I just did a sudo fdisk and it said it was mounted
hredGAN u I ’ m trying to get the latest version of ubuntu , but I ’ m getting a lot of

problems .

Context 0 Hmm, have you checked that ICMP is being allowed all the way through
and that the server responds to other ICMP requests?

Context 1 now i can ping it (i had to enable the lzo compression on client) i cannot
reach the remote subnet adding a rule in the firewall forward the lan to
vpn

HRED what ’s the problem ?
VHRED I don’t know what you want to do
hredGAN w I have no idea , I ’ m just trying to get the ip address from the router to

the router”
hredGAN u I ’ m not sure if that ’ s the case , but I ’ m not sure if it ’ s a good idea

to have a look at the output .

Context 0 The netboot one is suppose to download packages from the net.
HRED I don’t know what you want to do .
VHRED how do I do that ?
hredGAN w I ’ m not sure of a command to do so I can just install the ubuntu . iso

and burn a new iso
hredGAN u you can ’ t have a look at the man page . . .

Context 0 The netboot one is suppose to download packages from the net.
Context 1 like the ones to be installed? or the installed to be run?
HRED I don’t know what the problem is .
VHRED is there a way to install the package manager ?
hredGAN w you don ’ t need to install the whole system , just install the ubuntu

installer
hredGAN u I ’ m trying to install a UNK version of ubuntu , but I ’ m trying to

install it on my laptop .

Context 0 You trying to mount it open with the password?
HRED I have to do that , but I have a problem with the same problem .
VHRED is there a way to do ?
hredGAN w yes , I ’ m not sure if it ’ s mounted or not
hredGAN u I ’ m trying to install the latest version of Ubuntu , but I ’ m running on

a UNK .

Context 0 are you using lvm for your root partition?
HRED yes , but I have a problem with a new user , but I don’t want to do that .
VHRED what is the best way to do that ? what is that ?
hredGAN w yes , I ’ m on a live system , I ’ m trying to use the partition to boot from

windows
hredGAN u I ’ m not sure if it ’ s a good idea to have a backup of the system . . .

Context 0 are you using lvm for your root partition?
Context 1 i have the same config working on a 10.04 box. It appears in 12.04 lvm

is on upstart and multipath is not. i do use lvm for my root folder nifty,
is there any way to work around it? have you heard of anyone posting a
bug about this to have it looked at? darn.... well i hope they can fix this
eventually i hope they keep initrd around and move everything to upstart.
Upstart is not easy to setup for programs that don’t come packaged for
upstart.

HRED I don’t know what the problem is , but I don’t know what the problem is
. ?

VHRED is there a way to do that ? or **unknown** ?
hredGAN w you can also try the live cd and see if you can get it back up the live”
hredGAN u I ’ m trying to get my wireless working on my laptop , but I ’ m getting

to the point that I can be able to use the wireless card to get the wireless
working .

Context 0 are you using lvm for your root partition?
Context 1 i have the same config working on a 10.04 box. It appears in 12.04 lvm

is on upstart and multipath is not. i do use lvm for my root folder nifty,
is there any way to work around it? have you heard of anyone posting a
bug about this to have it looked at? darn.... well i hope they can fix this
eventually i hope they keep initrd around and move everything to upstart.
Upstart is not easy to setup for programs that don’t come packaged for
upstart.

Context 2 yes, because i cant supply it at boot time, i want the truecrypt drive to
come up by itself without manual intervention

HRED what is the problem ?
VHRED what do you mean ?
hredGAN w you can also mount a new one and put the mount command to the mount

point”
hredGAN u I ’ m trying to get my sound working , but I ’ m trying to get my sound

working .

Table 6: Additional sample responses of HRED, VHRED
and hredGAN on Ubuntu.


