



















































Learning Semantic Word Embeddings based on Ordinal Knowledge Constraints


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1501–1511,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Learning Semantic Word Embeddings based on
Ordinal Knowledge Constraints

Quan Liu† and Hui Jiang‡ and Si Wei§ and Zhen-Hua Ling† and Yu Hu§
† National Engineering Laboratory for Speech and Language Information Processing

University of Science and Technology of China, Hefei, China
‡ Department of Electrical Engineering and Computer Science, York University, Canada

§ iFLYTEK Research, Hefei, China
emails: quanliu@mail.ustc.edu.cn, hj@cse.yorku.ca,

siwei@iflytek.com, zhling@ustc.edu.cn, yuhu@iflytek.com

Abstract

In this paper, we propose a general frame-
work to incorporate semantic knowledge
into the popular data-driven learning pro-
cess of word embeddings to improve the
quality of them. Under this framework,
we represent semantic knowledge as many
ordinal ranking inequalities and formu-
late the learning of semantic word embed-
dings (SWE) as a constrained optimiza-
tion problem, where the data-derived ob-
jective function is optimized subject to all
ordinal knowledge inequality constraints
extracted from available knowledge re-
sources such as Thesaurus and Word-
Net. We have demonstrated that this con-
strained optimization problem can be ef-
ficiently solved by the stochastic gradient
descent (SGD) algorithm, even for a large
number of inequality constraints. Experi-
mental results on four standard NLP tasks,
including word similarity measure, sen-
tence completion, name entity recogni-
tion, and the TOEFL synonym selection,
have all demonstrated that the quality of
learned word vectors can be significantly
improved after semantic knowledge is in-
corporated as inequality constraints during
the learning process of word embeddings.

1 Introduction

Distributed word representation (i.e., word embed-
ding) is a technique that represents words as con-
tinuous vectors, which is an important research
topic in natural language processing (NLP) (Hin-
ton et al., 1986; Turney et al., 2010). In re-
cent years, it has been widely used in various
NLP tasks, including neural language model (Ben-
gio et al., 2003; Schwenk, 2007), sequence la-
belling tasks (Collobert and Weston, 2008; Col-
lobert et al., 2011), machine translation (Devlin et

al., 2014; Sutskever et al., 2014), and antonym se-
lection (Chen et al., 2015). Typically, word vectors
are learned based on the distributional hypothesis
(Harris, 1954; Miller and Charles, 1991), which
assumes that words with a similar context tend
to have a similar meaning. Under this hypothe-
sis, various models, such as the skip-gram model
(Mikolov et al., 2013a; Mikolov et al., 2013b;
Levy and Goldberg, 2014) and GloVe model (Pen-
nington et al., 2014), have been proposed to lever-
age the context of each word in large corpora to
learn word embeddings. These methods can ef-
ficiently estimate the co-occurrence statistics to
model contextual distributions from very large text
corpora and they have been demonstrated to be
quite effective in a number of NLP tasks. How-
ever, they still suffer from some major limitations.
In particular, these corpus-based methods usu-
ally fail to capture the precise meanings for many
words. For example, some semantically related
but dissimilar words may have similar contexts,
such as synonyms and antonyms. As a result, these
corpus-based methods may lead to some antony-
mous word vectors being located much closer in
the learned embedding space than many synony-
mous words. Moreover, as word representations
are mainly learned based on the co-occurrence in-
formation, the learned word embeddings do not
capture the accurate relationship between two se-
mantically similar words if either one appears less
frequently in the corpus.

To address these issues, some recent work has
been proposed to incorporate prior lexical knowl-
edge (WordNet, PPDB, etc.) or knowledge graph
(Freebase, etc.) into word representations. Such
knowledge enhanced word embedding methods
have achieved considerable improvements on var-
ious natural language processing tasks, like (Yu
and Dredze, 2014; Bian et al., 2014; Xu et al.,
2014). These methods attempt to increase the se-
mantic similarities between words belonging to

1501



one semantic category or to explicitly model the
semantic relationships between different words.
For example, Yu and Dredze (2014) have proposed
a new learning objective function to enhance
word embeddings by combining neural models
and a prior knowledge measure from semantic re-
sources. Bian et. al (2014) have recently proposed
to leverage morphological, syntactic, and semantic
knowledge to improve the learning of word em-
beddings. Besides, a novel framework has been
proposed in (Xu et al., 2014) to take advantage of
both relational and categorical knowledge to learn
high-quality word representations, where two reg-
ularization functions are used to model the re-
lational and categorical knowledge respectively.
More recently, a retrofitting technique has been in-
troduced in (Faruqui et al., 2014) to improve se-
mantic vectors by leveraging lexicon-derived rela-
tional information in a post-processing stage.

In this paper, we propose a new and flexible
method to incorporate semantic knowledge into
the corpus-based learning of word embeddings.
In our approach, we propose to represent seman-
tic knowledge as many word ordinal ranking in-
equalities. Furthermore, these inequalities are cast
as semantic constraints in the optimization pro-
cess to learn semantically sensible word embed-
dings. The proposed method has several advan-
tages. Firstly, many different types of seman-
tic knowledge can all be represented as a num-
ber of such ranking inequalities, such as synonym-
antonym, hyponym-hypernym and etc. Secondly,
these inequalities can be easily extracted from
many existing knowledge resources, such as The-
saurus, WordNet (Miller, 1995) and knowledge
graphs. Moreover, the ranking inequalities can
also be manually generated by human annotation
because ranking orders is much easier for human
annotators than assigning specific scores. Next,
we present a flexible learning framework to learn
distributed word representation based on the ordi-
nal semantic knowledge. By solving a constrained
optimization problem using the efficient stochas-
tic gradient descent algorithm, we can obtain se-
mantic word embedding enhanced by the ordinal
knowledge constraints. Experiments on four pop-
ular natural language processing tasks, including
word similarity, sentence completion, name en-
tity recognition and synonym selection, have all
demonstrated that the proposed method can learn
good semantically sensible word embeddings.

2 Representing Knowledge By Ranking

Many types of lexical semantic knowledge can be
quantitatively represented by a large number of
ranking inequalities such as:

similarity(wi, wj) > similarity(wi, wk) (1)

where wi, wj and wk denote any three words in
vocabulary. For example, eq.(1) holds if wj is a
synonym of wi and wk is an antonym of wi. In
general, the similarity between a word and its syn-
onymous word should be larger than the similar-
ity between the word and its antonymous word.
Moreover, a particular word should be more sim-
ilar to the words belonging to the same semantic
category as this word than other words belonging
to a different category. Besides, eq.(1) holds if wi
and wj have shorter distance in a semantic hierar-
chy than wi and wk do in the same hierarchy (Lea-
cock and Chodorow, 1998; Jurafsky and Martin,
2000).

Equivalently, each of the above similarity in-
equalities may be represented as the following
constraint in the embedding space:

sim(w(1)i ,w
(1)
j ) > sim(w

(1)
i ,w

(1)
k ) (2)

where w(1)i , w
(1)
j and w

(1)
k denote the embedding

vectors of the words, wi, wj and wk.
In this paper, we use the following three rules to

gather the ordinal semantic knowledge from avail-
able lexical knowledge resources, such as The-
saurus and WordNet.

• Synonym Antonym Rule: Similarities be-
tween a word and its synonymous words
are always larger than similarities be-
tween the word and its antonymous words.
For example, the similarity between fool-
ish and stupid is expected to be bigger
than the similarity between foolish and
clever, i.e., similarity(foolish, stupid) >
similarity(foolish, clever).

• Semantic Category Rule: Similarities of
words that belong to the same semantic cat-
egory would be larger than similarities of
words that belong to different categories.
This rule refers to the idea of Fisher lin-
ear discriminant algorithm in (Fisher, 1936).
A semantic category may be defined as a
synset in WordNet, a hypernym in a se-
mantic hierarchy, or an entity category in

1502



...

wk-N wk-N+1 wk+N-1 wk+N

wk

Skip-gram

wi wj

d(k,i) > d(k,j)

wp wq

d(k,p) > d(k,q)

...

...

Semantic Ordinal Constraints

wt-c wt-c+1 wt+c-1 wt+c

wt

Machine: Hi, please give a similarity value to those two words.

Human: Uh, it is difficult to do it, just give me a little time.

Paradigm shift…> ranking idea.

Machine: Hi, here are two word pairs, would you please give a 

similarity ranking decision for them?

Human: Yes, here you are!

W(1)

W(2)

wt-c wt-c+1 wt+c-1 wt+c

wt

W(1)

W(2)

sim(wt,wi) > sim(wj,wk)

wi wj wk

Share Embedding

Semantic Knowledge Resources

W(1)

WordNet

Human LabelingKnowledge

Has 

Knowledge

?

Minimize 

Qsem
Semantic 

Knowledge

Input Word

YES

Maximize 

Qbase

Word Sequence

Get training samples

Color

Red Blue GreenPuple

Tool

Hammer SawChisel

Mallet

wt-c wt-c+1 wt+c-1 wt+c

wt

W(1)

W(2)

sim(wi,wj) > sim(wj,wk) 

wi wj wk

Share Embedding

Semantic Knowledge Resources

W(1)

WordNet

Human Labeling
Knowledge

Bases

Objective-1: 

Qbase

Objective-2: 

Qsem

Joint 

Optimization

Plessor JigsawHacksaw

Hypernym

Hyponyms Co-Hyponyms

Figure 1: An example of hyponym and hypernym.

knowledge graphs. Figure 1 shows a sim-
ple example of the relationship between hy-
ponyms and hypernyms. From there, it is
reasonable to assume the following similar-
ity inequality: similarity(Mallet,Plessor) >
similarity(Mallet,Hacksaw).

• Semantic Hierarchy Rule: Similarities be-
tween words that have shorter distances in
a semantic hierarchy should be larger than
similarities of words that have longer dis-
tances. In this work, the semantic hi-
erarchy refers to the hypernym and hy-
ponym structure in WordNet. From Fig-
ure 1, this rule may suggest several inequal-
ities like: similarity(Mallet,Hammer) >
similarity(Mallet,Tool).

In addition, we may generate many such se-
mantically ranking similarity inequalities by hu-
man annotation through crowdsourcing.

3 Semantic Word Embedding

In this section, we first briefly review the conven-
tional skip-gram model (Mikolov et al., 2013b).
Next, we study how to incorporate the ordinal sim-
ilarity inequalities to learn semantic word embed-
dings.

3.1 The skip-gram model

The skip-gram model is a recently proposed learn-
ing framework (Mikolov et al., 2013b; Mikolov et
al., 2013a) to learn continuous word vectors from
text corpora based on the aforementioned distribu-
tional hypothesis, where each word in vocabulary
(size of V ) is mapped to a continuous embedding
space by looking up an embedding matrix W(1).
And W(1) is learned by maximizing the predic-
tion probability, calculated by another prediction
matrix W(2), of its neighbouring words within a
context window.

Given a sequence of training data, denoted as
w1, w2, w3, ..., wT with T words, the skip-gram

model aims to maximize the following objective
function:

Q = 1
T

T∑
t=1

∑
−c≤j≤c,j 6=0

log p(wt+j |wt) (3)

where c is the size of context windows, wt de-
notes the input central word andwt+j for its neigh-
bouring word. The skip-gram model computes
the above conditional probability p(wt+j |wt) us-
ing the following softmax function:

p(wt+j |wt) =
exp(w(2)t+j ·w(1)t )∑V
k=1 exp(w

(2)
k ·w(1)t )

(4)

where w(1)t and w
(2)
k denotes row vectors in ma-

trices W(1) and W(2), corresponding to word wt
and wk respectively.

The training process of the skip-gram model
can be formulated as an optimization problem to
maximize the above objective function Q. As in
(Mikolov et al., 2013b), this optimization problem
is solved by the stochastic gradient descent (SGD)
method and the learned embedding matrix W(1)

is used as the word embeddings for all words in
vocabulary.

3.2 Semantic Word Embedding (SWE) as
Constrained Optimization

Here we consider how to combine the ordinal
knowledge representation in section 2 and the
skip-gram model in 3.1 to learn semantic word
embeddings (SWE).

As shown in section 2, each ranking inequal-
ity involves a triplet, (i, j, k), of three words,
{wi, wj , wk}. Assume the ordinal knowledge is
represented by a large number of such inequalities,
denoted as the inequality set S. For ∀(i, j, k) ∈ S,
we have:

similarity(wi, wj) > similarity(wi, wk)

⇔ sim(w(1)i ,w(1)j ) > sim(w(1)i ,w(1)k ).

For notational simplicity, we denote sij =
sim(w(1)i ,w

(1)
j ) hereafter.

Next, we propose to use the following con-
strained optimization problem to learn semantic
word embeddings (SWE):

{W(1),W(2)} = arg max
W(1),W(2)

Q(W(1),W(2))
(5)

1503



...

wk-N wk-N+1 wk+N-1 wk+N

wk

Skip-gram

wi wj

d(k,i) > d(k,j)

wp wq

d(k,p) > d(k,q)

...

...

Semantic Ordinal Constraints

wt-c wt-c+1 wt+c-1 wt+c

wt

Machine: Hi, please give a similarity value to those two words.

Human: Uh, it is difficult to do it, just give me a little time.

Paradigm shift…> ranking idea.

Machine: Hi, here are two word pairs, would you please give a 

similarity ranking decision for them?

Human: Yes, here you are!

W(1)

W(2)

wt-c wt-c+1 wt+c-1 wt+c

wt

W(1)

W(2)

sim(wt,wi) > sim(wj,wk)

wi wj wk

Share Embedding

Semantic Knowledge Resources

W(1)

WordNet

Human LabelingKnowledge

Has 

Knowledge

?

Minimize 

Qsem
Semantic 

Knowledge

Input Word

YES

Maximize 

Qbase

Word Sequence

Get training samples

Color

Red Blue GreenPuple

Tool

Hammer SawChisel

Mallet

wt-c wt-c+1 wt+c-1 wt+c

wt

W(1)

W(2)

sim(wi,wj) > sim(wi,wk) 

wi wj wk

Share Embedding

Semantic Knowledge Resources

W(1)

WordNet

Human Labeling
Knowledge

Bases

Distributional 

Hypothesis

Knowledge Constraints

Joint 

Optimization

Plessor JigsawHacksaw

Hypernym

Hyponyms Co-Hyponyms

Figure 2: The proposed semantic word embedding (SWE) learning framework (The left part denotes the
state-of-the-art skip-gram model; The right part represents the semantic constraints).

subject to

sij > sik ∀(i, j, k) ∈ S. (6)
In this work, we formulate the above con-

strained optimization problem into an uncon-
strained one by casting all the constraints as a
penalty term in the objective function. The penalty
term can be expressed as follows:

D =
∑

(i,j,k)∈S
f(i, j, k) (7)

where the function f(·) is a normalization func-
tion. It can be a sigmoid function like f(i, j, k) =
σ(sik − sij) with σ(x) = 1/(1 + exp(−x)). Al-
ternatively, it may be a hinge loss function like
f(i, j, k) = h(sik−sij) where h(x) = max(δ0, x)
with δ0 denoting a parameter to control the de-
cision margin. In this work, we adopt to use
the hinge function to compute the penalty term in
eq.(7) and δ0 is set to be 0 for all experiments.

Finally, the proposed semantic word embed-
ding (SWE) model aims to maximize the follow-
ing combined objective function:

Q′ = Q− β · D (8)
where β is a control parameter to balance the con-
tribution of the penalty term in the optimization
process. It balances between the semantic infor-
mation estimated from the corpus based on the
distributional hypothesis and the semantic knowl-
edge encoded in the ordinal ranking inequalities.
In Rocktäschel et al. (2014), a similar approach
was proposed to capture knowledge constraint as
extra terms in the objective function for optimiza-
tion.

In Figure 2, we show a diagram for the the over-
all SWE learning framework to incorporate se-
mantic knowledge into the basic skip-gram word

embeddings. Comparing with the previous work
in (Xu et al., 2014) and (Faruqui et al., 2014),
the proposed SWE framework is more general in
terms of encoding the semantic knowledge for
learning word embeddings. It is straightforward
to show that the work in (Xu et al., 2014; Zweig,
2014; Faruqui et al., 2014) can be viewed as some
special cases under our SWE learning framework.

3.3 Optimization algorithm for SWE
In this work, the proposed semantic word em-
beddings (SWE) are learned using the standard
mini-batch stochastic gradient descent (SGD) al-
gorithm. Furthermore, we adopt to use the cosine
distance of the embedding vectors to compute the
similarity between two words in the penalty term.

In the following, we show how to compute the
derivatives of the penalty term for the SWE learn-
ing.

∂D
∂w(1)t

=
∑

(i,j,k)∈S

∂f (sik − sij)
∂w(1)t

=
∑

(i,j,k)∈S
f ′ ·
(
δik(t)

∂sik

∂w(1)t
− δij(t) ∂sij

∂w(1)t

)
(9)

where δik(t) and δij(t) are computed as

δik(t) =

{
1 t = i or t = k
0 otherwise

(10)

and for the hinge loss function f(x), we have

f ′ =

{
1 (sik − sij) > δ0
0 (sik − sij) ≤ δ0

(11)

and the derivatives of the cosine similarity mea-

sure, sij =
w

(1)
i ·w(1)j

|w(1)i ||w(1)j |
, with respect to a word vec-

1504



tor, i.e., ∂sik
∂w

(1)
i

, which can be derived as follows:

∂sij

∂w(1)i
= −sijw

(1)
i

|w(1)i |2
+

w(1)j

|w(1)i ||w(1)j |
. (12)

The learning rate used for the SWE learning
is the same as that for the skip-gram model. In
each mini-batch of SGD, we sample terms in the
same way as the skip-gram model. As for the con-
straints, we do not sample them but use all inequal-
ities relevant to any words in a minibatch to update
the model for the minibatch. Finally, by jointly op-
timizing the two terms in the combined objective
function, we may learn a new set of word vectors
encoding with ordinal semantic knowledge.

4 Experiments

In this section, we report all experiments con-
ducted to evaluate the effectiveness of the pro-
posed semantic word embeddings (SWE). Here
we compare the performance of the proposed
SWE model with the conventional skip-gram
baseline model on four popular natural language
processing tasks, including word similarity mea-
sure, sentence completion, name entity recogni-
tion, and synonym selection. In the following,
we first describe the experimental setup, training
corpora, semantic knowledge databases. Next,
we report the experimental results on these four
NLP tasks. Note that the SWE training codes
and scripts are made publicly available at http:
//home.ustc.edu.cn/˜quanliu/.

4.1 Experimental setup

4.1.1 Training corpora
In this work, we use the popular Wikipedia cor-
pus as our training data to learn word embeddings
for experiments on the word similarity task and
the TOEFL synonym selection task. Particularly,
we utilize two Wikipedia corpora with different
sizes. The first corpus with a smaller size is a
data set including the first one billion characters
from Wikipedia1, named as Wiki-Small in our ex-
periments. The second corpus with a relatively
large size is the latest Wikipedia dump2, named
as Wiki-Large in our experiments. Both Wikipedia
corpora have been pre-processed by removing all

1http://mattmahoney.net/dc/enwik9.zip
2http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-

pages-articles.xml.bz2

the HTML meta-data and hyper-links and replac-
ing the digit numbers with English words using the
perl script from the Matt Mahoney’s page3. After
text normalization, the Wiki-Small corpus contains
totally 130 million words, for which we create a
lexicon of 225,909 distinct words appearing more
than 5 times in the corpus. Similarly, the Wiki-
Large corpus contains about 5 billion words, for
which we create a lexicon of 228,069 words ap-
pearing more than 200 times.

For the other two tasks, sentence completion
and name entity recognition, we use the same
training corpora from the previous state-of-the-art
work for fair comparisons. The training corpus for
the sentence completion is the Holmes text (Zweig
and Burges, 2011; Mikolov et al., 2013a). The
training corpus for the name entity recognition
task is the Reuters English newswire from RCV1
(Turian et al., 2010; Lewis et al., 2004). Refer
to section 4.4 and section 4.5 for detailed descrip-
tions respectively.

4.1.2 Semantic constraint collections
In this work, we use WordNet as the resource to
collect ordinal semantic knowledge. WordNet is a
large semantic lexicon database of English words
(Miller, 1995), where nouns, verbs, adjectives and
adverbs are grouped into sets of cognitive syn-
onyms (usually called synsets). Each synset usu-
ally expresses a distinct semantic concept. All
synsets in WordNet are interlinked by means of
conceptual-semantic and/or lexical relations such
as synonyms and antonyms, hypernyms and hy-
ponyms.

In our experiments, we use the version
WordNet-3.1 for creating the corresponding se-
mantic constraints. In detail, we follow the fol-
lowing process to extract semantic similarity in-
equalities from WordNet and Thesaurus:

1. Based on the Synonym Antonym Rule de-
scribed in section 2, for each word in vo-
cabulary, find its synset and use the syn-
onym and antonym relations to find all re-
lated synonymous and antonymous synsets.
Note that the antonymous synset is selected
as long as there exists an antonymous rela-
tion between any word in this synset and any
word in an synonymous synset. After find-
ing the synonymous and antonymous synsets

3http://mattmahoney.net/dc/textdata.html

1505



of the current word, the similarity inequali-
ties could be generated according to the rank-
ing rule. After processing all words, we
have collected about 30,000 inequalities re-
lated to the synonym and antonym relations.
Furthermore, we extract additional 320,000
inequalities from an old English dictionary
(Fernald, 1896). In total, we have about
345,000 inequalities related to the synonym
and antonym relations. This set of inequali-
ties is denoted as Synon-Anton constraints in
our experiments.

2. Based on the Semantic Category Rule and Se-
mantic Hierarchy Rule, we extract another in-
equality set consisting of 75,000 inequalities
from WordNet. We defined this collection as
Hyper-Hypon constraints in our experiments.

In the following experiments, we just use all of
these collected inequality constraints as is without
further manually checking or cleaning-up. They
may contain a very small percentage of errors or
conflicts (due to multiple senses of a word).

4.1.3 Training parameter setting
Here we describe the control parameters used to
learn the baseline skip-gram model and the pro-
posed SWE model. In our experiments, we use the
open-source word2vec toolkit4 to train the base-
line skip-gram model, where the context window
size is set to be 5. The initial learning rate is set
as 0.025 and the learning rate is decreased lin-
early during the SGD model training process. We
use the popular negative sampling technique to
speed up model training and set the negative sam-
ple number as 5.

To train the proposed SWE model, we use the
same configuration as the skip-gram model to
maximize Q′. For the penalty term in eq. (7), we
set δ0 = 0 for the hinge loss function. The seman-
tic similarity between words is computed by the
cosine distance. The combination coefficient β in
eq. (8) is usually set to be a number between 0.001
and 0.3 in our experiments.

In the following four NLP tasks, the dimension-
ality of embedding vectors is different since we
try to use the same settings from the state-of-the-
art work for the comparison purpose. In the Word
Similarity task and the TOEFL Synonym Selec-
tion task, we followed the state of the art work

4https://code.google.com/p/word2vec.

50

55

60

65

70

75

80

85

90

95

100

0.00 0.05 0.10 0.15 0.20 0.25 0.30

S
at

is
fi

ed
 R

at
e 

(%
)

beta (β)

Semantic Inequality Satisfied Rate Curve

Hyper-Hypon

Synon-Anton

Figure 3: A curve of inequality satisfied rates (All
models trained on the Wiki-Small corpus. Hyper-
Hypon and Synon-Anton stand for different seman-
tic constraint sets employed for training semantic
word embeddings).

in (Xu et al., 2014), to set word embeddings to
300-dimension. Similarly, we refer to Bian et al.
(2014) to set the dimensionality of word vectors to
600 for the Sentence Completion task. And we set
the dimensionality of word vectors to 50 for the
NER task according to (Turian et al., 2010; Pen-
nington et al., 2014).

4.2 Semantic inequality satisfied rates

Here we first examine the inequality satisfied rates
of various word embeddings. The inequality sat-
isfied rate is defined as how many percentage of
semantic inequalities are satisfied based on the un-
derlying word embedding vectors. In Figure 3,
we show a typical curve of the inequality satis-
fied rates as a function of β used in model train-
ing. This figure is plotted based on the Wiki-Small
corpus. Two semantic constraint sets Synon-Anton
and Hyper-Hypon created in section 4.1.2 are em-
ployed to learn semantic word embeddings.

In the framework of the proposed semantic
word embedding method, we just need to tune
one more parameter β, comparing with the skip-
gram model. It shows that the baseline skip-gram
(β = 0) can only satisfy about 50-60% of inequal-
ities in the training set. As we choose a proper
value for β, we may significantly improve the in-
equality satisfied rate, up to 85-95%. Although
we can get higher inequality satisfying rate on the
training set by increasing beta continuously, how-
ever, we do not suggest to use a big beta value
because it would make the model overfitting. The
major reason for this is that the constraints only

1506



cover a subset of words in vocabulary. Increasing
the rate too much may screw up the entire word
embeddings due to the sparsity of the constraints.

Meanwhile, we have found that the proposed
SGD method is very efficient to handle a large
number of inequalities in model training. When
we use the total 345,000 inequalities, the SWE
training is comparable with the baseline skip-gram
model in terms of training speed. In the following,
we continue to examine the SWE model on four
popular natural language processing tasks, includ-
ing word similarity, sentence completion, name
entity recognition and the TOEFL synonym selec-
tion.

4.3 Task 1: Word Similarity Task

4.3.1 Task description

Measuring word similarity is a traditional NLP
task (Rubenstein and Goodenough, 1965). Here
we compare several word embedding models on
a popular word similarity task, namely WordSim-
353 (Finkelstein et al., 2001), which contains 353
English word pairs along with human-assigned
similarity scores, which measure the relatedness
of each word pair on a scale from 0 (totally unre-
lated words) to 10 (very much related or identical
words). The final similarity score for each pair is
the average across 13 to 16 human judges. When
evaluating word embeddings on this task, we mea-
sure the performance by calculating the Spearman
rank correlation between the human judgments
and the similarity scores computed based on the
learned word embeddings.

4.3.2 Experimental results

Here we compare the proposed SWE model with
the baseline skip-gram model on the WordSim-
353 task. Both word embedding models are
trained using the Wikipedia corpora. We set the di-
mension of word embedding vectors to be 300. In
Table 1, we have shown all the Spearman rank cor-
relation results. The baseline results on this task
include PPMI (Levy and Goldberg, 2014), GloVe
(Pennington et al., 2014), and ESA-Wikipedia
(Gabrilovich and Markovitch, 2007).

From the results in Table 1, we can see that the
proposed SWE model can achieve consistent im-
provements over the baseline skip-gram model, no
matter which training corpus is used. These re-
sults have demonstrated that, by incorporating se-
mantic ordinal knowledge into the word vectors,

Word Embeddings Result

Others
SPPMI 0.6870
GloVe (6 billion) 0.6580
GloVe (42 billion) 0.7590
ESA-Wikipedia 0.7500
Skip-gram 0.6326

Wiki-Small SWE + Synon-Anton 0.6584
(0.13 billion) SWE + Hyper-Hypon 0.6407

SWE + Both 0.6442
Skip-gram 0.7085

Wiki-Large SWE + Synon-Anton 0.7274
(5 billion) SWE + Hyper-Hypon 0.7213

SWE + Both 0.7236

Table 1: Spearman results on the WordSim-353
Task.

the proposed semantic word embedding frame-
work can capture much better semantics for many
words. The SWE model using the Wiki-Large
corpus has achieved the state-of-the-art perfor-
mance on this task, significantly outperforming
other popular word embedding methods, such as
skip-gram and GloVe. Moreover, we also find that
the Synon-Anton constraint set is more relevant
than Hyper-Hypon for the word similarity task.

4.4 Task 2: Sentence Completion Task

4.4.1 Task description
The Microsoft sentence completion challenge has
recently been introduced as a standard benchmark
task for language modeling and other NLP tech-
niques (Zweig and Burges, 2011). This task con-
sists of 1040 sentences, each of which misses one
word. The goal is to select a word that is the
most coherent with the rest of the sentence, from
a list of five candidates. Many NLP techniques
have already been reported on this task, includ-
ing N-gram model and LSA-based model pro-
posed in (Zweig and Burges, 2011), log-bilinear
model (Mnih and Teh, 2012), recurrent neural
networks (RNN) (Mikolov, 2012), the skip-gram
model (Mikolov et al., 2013a), a combination of
the skip-gram and RNN model, and a knowledge
enhanced word embedding model proposed by
Bian et. al. (2014). The performance of all these
techniques is listed in Table 2 for comparison.

In this work, we follow the the same proce-
dure as in (Mikolov et al., 2013a) to examine the
performance of our proposed semantic word em-
beddings (SWE) on this task. We first train 600-

1507



dimension word embeddings based on a training
corpus of 50M words provided by (Zweig and
Burges, 2011), with and without using the col-
lected ordinal knowledge. Then, for each sen-
tence in the test set, we use the learned word em-
beddings to compute a sentence score for predict-
ing all surrounding words based on each candidate
word in the list. Finally, we use the computed sen-
tence prediction scores to choose the most likely
word from the given list to answer the question.

System Acc

Others

N-gram model 39.0
LSA-based model 49.0
Log-bilinear model 54.8
RNN 55.4
Skip-gram 48.0
Skip-gram + RNN 58.9

Bian et al.

Skip-gram 41.2
+ Syntactic knowledge 41.9
+ Semantic knowledge 45.2
+ Both knowledge 44.2

1 Iteration

Skip-gram 44.1
SWE + Synon-Anton 47.9
SWE + Hyper-Hypon 47.5
SWE + Both 48.3

5 Iterations

Skip-gram 51.5
SWE + Synon-Anton 55.7
SWE + Hyper-Hypon 55.4
SWE + Both 56.2

Table 2: Results on Sentence Completion Task.

4.4.2 Experimental results
In Table 2, we have shown the sentence comple-
tion accuracy on this task for various word em-
bedding models. We can see that the proposed
SWE model has achieved considerable improve-
ments over the baseline skip-gram model. Once
again, this suggests that the semantic knowledge
represented by the ordinal inequalities can signif-
icantly improve the quality of the word embed-
dings. Besides, the SWE model significantly out-
performs the recent work in (Bian et al., 2014),
which considers syntactics and semantics of the
sentence contexts.

4.5 Task 3: Name Entity Recognition
4.5.1 Task description
To further investigate the performance of seman-
tic word embeddings, we have further conducted

some experiments on the standard CoNLL03
name entity recognition (NER) task. The
CoNLL03 NER dataset is drawn from the Reuters
newswire. The training set contains 204K words
(14K sentences, 946 documents), the test set
contains 46K words (3.5K sentences, 231 doc-
uments), and the development set contains 51K
words (3.3K sentences, 216 documents). We have
listed the state-of-the-art performance in Table 3
for this task (Turian et al., 2010).

To make a fair comparison, we have used the
exactly same experimental configurations as in
(Turian et al., 2010), including the used training
algorithm, the baseline discrete features and so on.
Like the C&W model, we use the same training
text resource to learn word vectors, which contains
one year of Reuters English newswire from RCV1,
from August 1996 to August 1997, having about
810,000 news stories (Lewis et al., 2004). Mean-
while, the dimension of word embeddings is set to
50 for all experiments on this task.

4.5.2 Experimental results
In our experiments, we compare the proposed
SWE model with the baseline skip-gram model for
name entity recognition, measured by the standard
F1 scores. We present the final NER F1 scores
on the CoNLL03 NER task in Table 3. The nota-
tion “Gaz” stands for gazetteers that are added into
the NER system as an auxiliary feature. For the
SWE model, we experiment two configurations by
adding gazetteers or not (denoted by “IsGaz” and
“NoGaz” respectively).

System Dev Test MUC7

Others
C&W 92.3 87.9 75.7
C&W + Gaz 93.0 88.9 81.4

NoGaz
Skip-gram 92.6 88.3 76.7
+ Synon-Anton 92.5 88.4 77.2
+ Hyper-Hypon 92.6 88.6 77.7
+ Both 92.6 88.4 77.5

IsGaz
Skip-gram 93.3 89.5 80.0
+ Synon-Anton 93.1 89.6 80.7
+ Hyper-Hypon 93.1 89.7 80.7
+ Both 93.0 89.5 80.8

Table 3: F1 scores on the CoNLL03 NER task.

From the results shown in Table 3, we could find
the proposed semantic word embedding (SWE)
model can consistently achieve 0.8% (or more) ab-
solute improvements on the MUC7 task no mat-

1508



ter whether the gazetteers features are used or not.
The proposed SWE model can also obtain 0.3%
improvement in the CoNLL03 test set when no
gazetteers is added into the NER system. How-
ever, no significant improvement is observed in
this test set for the proposed SWE model after we
add the gazetteers feature.

4.6 Task 4: TOEFL Synonym Selection

4.6.1 Task description
The goal of a synonym selection task is to se-
lect, from a list of candidate words, the semanti-
cally closest word for each given target word. The
dataset we use for this task is the standard TOEFL
dataset (Landauer and Dumais, 1997), which con-
tains 80 questions. Each question consists of a tar-
get word along with 4 candidate lexical substitutes
for selection.

The evaluation criterion on this task is the
synonym selection accuracy which indicates how
many synonyms are correctly selected for all 80
questions. Similar to the configurations on the
word similarity task, all the experiments on this
task are conducted on the English Wikipedia cor-
pora. In our experiments, we set all the vector di-
mensions to 300.

4.6.2 Experimental Results

Corpus Model Accuracy (%)

Wiki-Small
Skip-gram 61.25
+ Synon-Anton 70.00
+ Hyper-Hypon 66.25
+ Both 71.25

Wiki-Large
Skip-gram 83.75
+ Synon-Anton 87.50
+ Hyper-Hypon 85.00
+ Both 88.75

Table 4: The TOEFL synonym selection task.

In Table 4, we have shown the experimen-
tal results for different word embedding models,
learned from different Wikipedia corpora: Wiki-
Small or Wiki-Large. We compare the proposed
SWE with the baseline skip-gram model. From
the experimental results in Table 4, we can see that
the proposed SWE model can achieve consistent
improvements over the baseline skip-gram model
on the TOEFL synonym selection task, about 5-
8% improvements on the selection accuracy. We
find the similar performance differences between

the SWE model trained with the Synon-Anton and
Hyper-Hypon constraint set. The main reason
would be that the synonym selection task is mainly
related to lexical level similarity and less relevant
to the hypernym-hyponym relations.

5 Conclusions and Future Work

Word embedding models with good semantic rep-
resentations are quite invaluable to many natu-
ral language processing tasks. However, the cur-
rent data-driven methods that learn word vectors
from corpora based on the distributional hypoth-
esis tend to suffer from some major limitations.
In this paper, we propose a general and flexible
framework to incorporate various types of seman-
tic knowledge into the popular data-driven learn-
ing procedure for word embeddings. Our main
contributions are to represent semantic knowledge
as a number of ordinal similarity inequalities as
well as to formulate the entire learning process as
a constrained optimization problem. Meanwhile,
the optimization problem could be solved by effi-
cient stochastic gradient descend algorithm. Ex-
perimental results on four popular NLP tasks have
all demonstrated that the propose semantic word
embedding framework can significantly improve
the quality of word representations.

As for the future work, we would incorpo-
rate more types of knowledge, such as knowledge
graphs and FrameNet, into the learning process
for more powerful word representations. We also
expect that some common sense related semantic
knowledge may be generated as ordinal inequality
constraints by human annotators for learning se-
mantic word embeddings. At the end, we plan to
apply the SWE word embedding models for more
natural language processing tasks.

Acknowledgments

This work was supported in part by the Science
and Technology Development of Anhui Province,
China (Grants No. 2014z02006) and the Funda-
mental Research Funds for the Central Universi-
ties. Special thanks to Zhigang Chen, Ruiji Fu and
the anonymous reviewers for their insightful com-
ments as well as suggestions. The authors also
want to thank Profs. Wu Guo and Li-Rong Dai
for their wonderful help and supports during the
experiments.

1509



References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and

Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.

Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014.
Knowledge-powered deep learning for word embed-
ding. In Machine Learning and Knowledge Discov-
ery in Databases, pages 132–148. Springer.

Zhigang Chen, Wei Lin, Qian Chen, Xiaoping Chen,
Si Wei, Xiaodan Zhu, and Hui Jiang. 2015. Revis-
iting word embedding for contrasting meaning. In
Proceedings of ACL.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160–167. ACM.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of ACL,
pages 1370–1380.

Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris
Dyer, Eduard Hovy, and Noah A Smith. 2014.
Retrofitting word vectors to semantic lexicons. In
Proceedings of the NIPS Deep learning and repre-
sentation learning workshop.

James Champlin Fernald. 1896. English synonyms and
antonyms. Funk & Wagnalls Company.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of WWW, pages
406–414. ACM.

Ronald A Fisher. 1936. The use of multiple measure-
ments in taxonomic problems. Annals of eugenics,
7(2):179–188.

Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
IJCAI, volume 7, pages 1606–1611.

Zellig S Harris. 1954. Distributional structure. Word,
10(23):146–162.

Geoffrey E Hinton, James L McClelland, and David E
Rumelhart. 1986. Distributed representations. In
Parallel distributed processing: Explorations in the
microstructure of cognition. Volume 1: Foundations,
pages 77–109. MIT Press.

Dan Jurafsky and James H Martin. 2000. Speech &
language processing. Pearson Education India.

Thomas K Landauer and Susan T Dumais. 1997. A
solution to plato’s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
104(2):211.

Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265–283.

Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Pro-
ceedings of NIPS, pages 2177–2185.

David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. RCV1: A new benchmark collection
for text categorization research. Journal of Machine
Learning Research, 5:361–397.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111–3119.

Tomáš Mikolov. 2012. Statistical language models
based on neural networks. Ph.D. thesis, Ph. D. the-
sis, Brno University of Technology.

George A Miller and Walter G Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
cognitive processes, 6(1):1–28.

George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.

Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proceedings of ICML.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. Proceedings of EMNLP, 12:1532–
1543.

Tim Rocktäschel, Matko Bošnjak, Sameer Singh, and
Sebastian Riedel. 2014. Low-dimensional embed-
dings of logic. In Proceedings of the ACL 2014
Workshop on Semantic Parsing, pages 45–49, Bal-
timore, MD, June. Association for Computational
Linguistics.

Herbert Rubenstein and John B Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.

1510



Holger Schwenk. 2007. Continuous space language
models. Computer Speech & Language, 21(3):492–
518.

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of NIPS, pages 3104–3112.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of
ACL, pages 384–394. Association for Computa-
tional Linguistics.

Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141–188.

Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang
Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. Rc-
net: A general framework for incorporating knowl-
edge into word representations. In Proceedings of
CIKM, pages 1219–1228. ACM.

Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In Proceedings
of ACL, volume 2, pages 545–550.

Geoffrey Zweig and Christopher JC Burges. 2011. The
microsoft research sentence completion challenge.
Technical report, Technical Report MSR-TR-2011-
129, Microsoft.

Geoffrey Zweig. 2014. Explicit representation of
antonymy in language modelling. Technical report,
Technical Report MSR-TR-2014-52, Microsoft.

1511


