



















































Robust Coreference Resolution and Entity Linking on Dialogues: Character Identification on TV Show Transcripts


Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 216–225,
Vancouver, Canada, August 3 - August 4, 2017. c©2017 Association for Computational Linguistics

Robust Coreference Resolution and Entity Linking on Dialogues:
Character Identification on TV Show Transcripts

Henry Y. Chen, Ethan Zhou, Jinho D. Choi
Math and Computer Science

Emory University
Atlanta, GA 30322, USA

{henry.chen, ethan.zhou, jinho.choi}@emory.edu

Abstract
This paper presents a novel approach to
character identification, that is an entity
linking task that maps mentions to charac-
ters in dialogues from TV show transcripts.
We first augment and correct several cases
of annotation errors in an existing corpus so
the corpus is clearer and cleaner for statisti-
cal learning. We also introduce the agglom-
erative convolutional neural network that
takes groups of features and learns mention
and mention-pair embeddings for corefer-
ence resolution. We then propose another
neural model that employs the embeddings
learned and creates cluster embeddings for
entity linking. Our coreference resolution
model shows comparable results to other
state-of-the-art systems. Our entity linking
model significantly outperforms the previ-
ous work, showing the F1 score of 86.76%
and the accuracy of 95.30% for character
identification.

1 Introduction

Character identification (Chen and Choi, 2016) is a
task that identifies each mention as a character in a
multiparty dialogue.1 Let a mention be a nominal
referring to a human (e.g., she, mom, Judy), and an
entity be a character in the dialogue. The objective
is to assign each mention to an entity, who may or
may not appear as a speaker in the dialogue. For
the example in Table 1, the mention comedian is
not one of the speakers in the dialogue; nonetheless,
it clearly refers to a real person that may appear in
some other dialogues. Identifying such mentions
as actual characters requires cross-document entity
resolution, which makes this task challenging.
1The dialogues are extracted from TV show transcripts by the
previous work (Chen and Choi, 2016).

Character identification can be viewed as a task of
entity linking. Most of the previous work on entity
linking focuses on Wikification (Mihalcea and Cso-
mai, 2007a; Ratinov et al., 2011a; Guo et al., 2013).
Unlike Wikification, entities in this task have no
precompiled information from a knowledge base,
which is another challenging aspect. This task is
similar to coreference resolution in the sense that it
groups mentions into entities, but distinct because
it requires the identification of mention groups as
real entities. Furthermore, even if it can be tackled
as a coreference resolution task, only a few coref-
erence resolution systems are designed to handle
dialogues well (Rocha, 1999; Niraula et al., 2014)
although several state-of-the-art systems have been
proposed for the general domain (Peng et al., 2015;
Clark and Manning, 2016; Wiseman et al., 2016).

Due to the nature of multiparty dialogues where
speakers take turns to complete a context, charac-
ter identification becomes a critical step to adapt
higher-level NLP tasks (e.g., question answering,
summarization) to this domain. This task can
also bring another level of sophistication to intelli-
gent personal assistants and intelligent tutoring sys-
tems. Perhaps the most challenging aspect comes
from colloquial writing that consists of ironies,
metaphors, or rhetorical questions. Despite all the
challenges, we believe that the output of this task
will enhance inference on dialogue contexts by pro-
viding finer-grained information about individuals.

In this paper, we augment and correct the exist-
ing corpus for character identification, and propose
an end-to-end deep-learning system that combines
neural models for coreference resolution and entity
linking to tackle the task of character identification.
The updated corpus and the source code of our
models are published and publicly available.2 This
combined system utilizes the strengths from both

2nlp.mathcs.emory.edu/character-mining/

216



Speaker Utterance
Joey Yeah, right! ... You1 serious?
Rachel Everything you2 need to know is in that first kiss.
Chandler Yeah. For us3, it’s like the stand-up comedian4 you5 have to sit through before the main dude6 starts.
Ross It’s not that we7 don’t like the comedian8, it’s that ... that’s not why we9 bought the ticket.

{You1} → Rachel, {us3, we7,9} → Collective, {you2,5} → General, {comedian4,8} → Generic, {dude6} → Other

Table 1: An example of a multiparty dialogue extracted from the corpus.

models. We introduce a novel approach, agglomer-
ative convolution neural network, for coreference
resolution to learn mention, mention-pair, and clus-
ter embeddings, and the results are taken as input
to our entity linking model that assigns mentions to
their real entities. Entities, including main charac-
ters and recurring support characters, are selected
from a TV show to mimic a realistic scenario. To
the best of our knowledge, this is the first end-to-
end model that performs character identification on
multiparty dialogues.

2 Related Work

The latest coreference systems employ advanced
context features in tandem with deep networks to
achieve state-of-the-art performance (Clark and
Manning, 2016; Wiseman et al., 2015). Since our
task is similar to coreference resolution, we take a
similar approach to feature engineering by building
mention and cluster embeddings with word em-
beddings (Clark and Manning, 2016) and include
additional mention features described by Wiseman
et al. (2015). We are motivated to use convolu-
tional networks through the work of Wu and Ma
(2017), but we distinguish our approach by using
deep convolution to build embeddings for character
identification.

Entity linking has traditionally relied heavily on
knowledge databases, most notably, Wikipedia, for
entities (Mihalcea and Csomai, 2007b; Ratinov
et al., 2011b; Gattani et al., 2013; Francis-Landau
et al., 2016).3 Although we do not make use of
knowledge bases, our task is closely aligned to en-
tity linking. Recent advances in entity linking are
also applicable to our task since we see Francis-
Landau et al. (2016) use convolutional nets to cap-
ture semantic similarity between a mention and an
entity by comparing context of the mention with the
description of the entity. This work validates our
usage of deep learning for character identification.

3This task is known as ‘Wikification’.

Dialogue tracking has been an expanding task
as shown by the Dialogue State Tracking Chal-
lenges hosted by Microsoft (Kim et al., 2015).
That an ongoing conversation can be dynamically
tracked (Henderson et al., 2013) is exciting and
applicable to our task because the state of a conver-
sation may yield significant hints for entity linking
and coreference resolution. Speaker identification,
a task similar to character identification, has already
shown some success with partial dialogue tracking
by dynamically identifying speakers at each turn in
a dialogue using conditional random field models.

3 Corpus

The character identification corpus created by Chen
and Choi (2016) includes entity annotation of per-
sonal mentions specific to the domain of multiparty
dialogues. While the corpus covers a large amount
of entities that appear in the first two seasons of the
TV show, Friends, some of its annotation remains
ambiguous, particularly around the label Unknown.

An example of Unknown mentions in a snippet
of a conversation is provided in Table 1. Men-
tions comedian4,8 and dude6 are originally labeled
Unknown, but they are two different entities such
that their labels should be distinguished. Even
though their entities are not immediately identi-
fiable, the Unknown label provides no clarity; thus,
mentions under this label needs to be subcatego-
rized. We propose to disambiguate these Unknown
mentions (Section 3.2), comprising 10% of the an-
notation. Such disambiguation allows finer-grained
categories of entity annotations of mentions. We
believe the resultant annotations are more realis-
tic and can be used to train more robust model on
character identification.

3.1 Corpus Correction

Before disambiguating the corpus, we find some
recurring data malformations and errors in mention
detection within the corpus. For example:

217



Rachel: (To guy with a phone) Hello, excuse me.

The underlined action note is accidentally included
in the utterance as a part of the dialogue due to a
missing parentheses, and the mention guy is conse-
quently incorporated into the corpus. These mal-
formations are fixed, and mentions included are
removed from the corpus manually before disam-
biguation. The correction is necessary since the
inclusion of action notes is inconsistent throughout
the corpus, and they are removed to avoid confu-
sion for our models.

3.2 Corpus Disambiguation
Three labels are introduced to disambiguate Un-
known mentions: General, Generic, and Other.
Generic provides abstract groupings for unidentifi-
able entities, and each group is assigned a unique
number for differentiation:

• General: Mention used in reference to a gen-
eral case (e.g., you2,5 in Table 1).

• Generic: Mention referring to a unidentifiable
entity (e.g., comedian4,8 in Table 1).

• Other: Mention referred to insignificant sin-
gleton entity (e.g., dude6 in Table 1).

We perform this disambiguation manually with two
main guidelines: only mentions originally labeled
Unknown are included, and the labels introduced
above are provided to annotators in addition to
the known entities. We limit the Generic men-
tion groups to 5 per iteration of disambiguation for
simplicity, and the scenes that requires more than 5
groups are recursively annotated until all unknowns
are disambiguated. Unlike the previous work, our
annotators are familiar with the TV show, and the
task takes about 3 weeks to complete.

P S C G N O Σ
F1 5,101 2,610 1,259 109 152 184 9,306
F2 5,312 2,432 1,280 42 111 167 9,304
Σ 10,413 5,042 2,388 151 263 351 18,608

Table 2: Counts of disambiguated mentions. P/S:
main and secondary character entities. C/G/N/O:
Collective/General/Generic/Other.

4 Coreference Resolution

The task of character identification needs rich fea-
tures extracted from mention clusters generated by

a coreference resolution system. Thus, the end re-
sult of this task largely depends on the quality of the
coreference resolution model. Several coreference
resolution systems have been proposed and shown
state-of-the-art performance (Pradhan et al., 2012);
however, they are not necessarily designed for the
genre of multiparty dialogue, where each document
comprises utterances from multiple speakers.

This section describes a novel approach to coref-
erence resolution using Convolutional Neural Net-
works (CNN). Our model takes groups of fea-
tures incorporating several dialogue aspects, feeds
them into deep convolution layers, and dynamically
generates mention embeddings and mention-pair
embeddings, which are used to create the cluster
embeddings that significantly improve the perfor-
mance of our entity linking model (Section 5).

4.1 Agglomerative CNN

Our coreference resolution model, Agglomerative
Convolutional Neural Network (ACNN), takes ad-
vantage of deep layers in CNN. The model is called
agglomerative since it aggregates multiple feature
groups into several convolution layers for the gen-
eration of mention and mention-pair embeddings.
Each layer aims to consolidate and learn different
combinations of the input features, and additional
features are included at each layer. The unique na-
ture of our model allows incremental feature aggre-
gations to create more robust embeddings. Figure 1
illustrates the complete architecture of ACNN.

The first part of the network learns the mention
embedding for each of two mentions compared
for a coreferent relation. Given two feature maps
φke(m) and φd(m) where m is a mention, φ

k
e(m)

extracts the embedding features based on word em-
beddings, and φd(m) extracts the discrete features
(Table 3). The first convolution layer CONVk1 with
n-gram filters of size d is applied to each embed-
ding group k, and the result from each filter is max-
pooled to generate a feature vector ∈ R1×d. The
second convolution layer CONV2 is then applied to
the 3D feature matrix ∈ Rn×d×k from the previous
convolution layer on all embedding groups. The
result of CONV2 is max-pooled and concatenated
with discrete features extracted by φd(m) to form
the mention embedding rs(m), defined as follows:

rs(m) = CONV2(

CONV
1
1(φ

1
e(m))

...
CONVk1(φ

k
e(m))

) ‖ φd(m)
218



Figure 1: The overview of our agglomerative convolutional neural network.

The second part of the network utilizes the learned
mention embedding rs(m) to create the mention-
pair embedding. Another feature map φp(mi,mj)
is defined to extract pairwise features between men-
tions mi and mj (Table 3). The third convolution
layer CONV3 is applied to the stacked mention em-
beddings, rs(mi) and rs(mj). The result is max-
pooled and concatenated with the pairwise features
extracted by φp(mi,mj) to form the mention-pair
embedding rp(mi,mj), defined as follows:

rp(mi,mj) = CONV3(
[
rs(mi)
rs(mj)

]
) ‖ φp(mi,mj)

The learned mention-pair embedding is put through
the hidden layer with the linear rectifier activation
function (ReLu) before applying the sigmoid func-
tion σ(mi,mj) to determine the coreferent relation
between mentions mi and mj , defined as follows:

h(x) = ReLU(wh · x+ bh)
σ(mi,mj) = sigmoid(ws · h(rp(mi,mj)) + bs)
The purpose of the sigmoid function σ(mi,mj) is
twofold. For each mention mi, it performs binary
classifications betweenmi andmj where j ∈ [1, i).
If max(σ(mi,mj)) < 0.5, the model considers no
coreferent relation between mi and any mention
prior to it, and create a new cluster containing only
mi s.t. mi becomes a singleton for the moment.
If max(σ(mi,mj)) ≥ 0.5, mi is put to the exist-
ing cluster Cmk that mk belongs to, where mk is
argj max(σ(mi,mj)). This formalism of mention
clustering is defined as follows:

• If ∀1≤j<i.max(σ(mi,mj)) < 0.5, then
create a new cluster Cmi .

• If ∃1≤j<i.max(σ(mi,mj)) ≥ 0.5, then
Cmk ← Cmk ∪ {mi},
where mk = argj max(σ(mi,mj)).

Table 3 shows feature templates used for our
ACNN model. Sentence and utterance embed-
dings are the average vectors of all word embed-
dings in the sentence and utterance, respectively.
Speaker embeddings are randomly generated using
the Gaussian distribution. Gender and plurality in-
formation are from Bergsma and Lin (2006), and
word animacy is from Durrett and Klein (2013).

Map Features
φ1e(m) Embeddings of 1

st three words in m

φ2e(m)
Embeddings of 3 proceeding words of m
Embeddings of 3 succeeding words of m
Average embedding of all words in m

φ3e(m)
Embeddings of 3 proceeding sentences
Embeddings of 1 succeeding sentence
Embedding of the current sentence

φ4e(m)
Embeddings of 3 proceeding utterances
Embeddings of 1 succeeding utterances
Embeddings of the current utterance

φd(m)

Avg. gender info. of all words in m
Avg. plurality info. of all words in m
Avg. word animacy of all words in m
Embedding of the current speaker
Embeddings of the previous 2 speakers

φp(mi,mj)

Exact string match between mi and mj
Relaxed string match between mi and mj
Speaker match between mi and mj
Mention distance between mi and mj
Sentence distance between mi and mj

Table 3: Complete feature templates for ACNN.
φke(m): embedding features, φd(m): discrete fea-
tures, φp(mi,mj): pairwise features.

219



4.2 Configuration

For our experiments, word embeddings of dimen-
sion 50 are trained with FastText (Bojanowski et al.,
2016) on the aggregation of New York Times,4

Wikipedia,5 and Amazon reviews.6 The tanh acti-
vation function and a filter size of 280 is used for all
convolution layers. A dropout rate of 0.8 is applied
to all max-pooled convoluted results, and `2 regu-
larization is applied to the sigmoid function. The
hidden layer has the same dimension as the filter
size. Binary labels of 0 and 1 are assigned to each
mention-to-mention pair based on the gold clus-
ter information. The model is trained on a mean
squared error loss function with the RMSprop opti-
mizer.

5 Entity Linking

Coreference resolution groups mentions into clus-
ters; however, it does not assign character labels
to the clusters, which is required for character
identification. This section describes our entity
linking model that takes the mention embeddings
and the mention-pair embeddings generated ACNN
and classifies each mention to one of the charac-
ter labels (Figure 3). These embeddings are used
to create cluster and cluster-mention embeddings
through pooling, which give a significant improve-
ment to character identification when included as
features in our linker (Section 6).

ReLu

ReLu

Mt

Mention Embedding Clusterm Embedding

M c1

M cn

Clusterp Embedding

M ct,i

M ct,k

Avg. Pooling

Max Pooling

CONVs CONVp

Figure 2: The overview of our entity linking model.
Clusterm and Clusterp embeddings are derived from
mention and mention-pair embeddings, resp.

4catalog.ldc.upenn.edu/ldc2008t19
5dumps.wikimedia.org/enwiki/
6snap.stanford.edu/data/web-Amazon.html

Figure 2 illustrates our entity linking model based
on a feed-forward neural network with two hid-
den layers. For each mention m, the model takes
the mention embedding rs(m) and two cluster em-
beddings derived from mention embeddings and
mention-pair embeddings within the cluster C(m)
(Section 5.2) and classifies m into one of the entity
labels using the Softmax regression.

5.1 Cluster Embedding
Two types of cluster embeddings are derived to cap-
ture cluster information. Given a mention m and
its cluster Cm, cluster embedding Rs(Cm) repre-
sents the collective mention embedding of all men-
tions within Cm, and mention-cluster embedding
Rp(Cm,m) represents the collective mention-pair
embedding between m and all the other mentions
in Cm that are compared to m during coreference
resolution (∀i. mi ∈ Cm):

Rs(Cm) = [rs(m1), rs(m2), ..., rs(m|Cm|)]
Rp(Cm,m) = [rp(mi,m) |mi 6= m]
CONVs and CONVp are two separate convolution
layers with unigram filters using the tanh activa-
tion. The results from these layers are max-pooled.
The cluster embedding rs(Cm) and the mention-
cluster embedding rp(Cm,m) are defined as fol-
lows:

rs(Cm) = CONVs(
[
avg pool(Rs(Cm))
max pool(Rs(Cm))

]
)

rp(Cm,m) = CONVp(
[
avg pool(Rp(Cm,m))
max pool(Rp(Cm,m))

]
)

The mention embedding, the cluster embedding,
and the mention-cluster embedding are concate-
nated and fed into the network as input, and the
scores of all character labels are activated as output.

5.2 Configuration
A dropout layer of rate 0.8 is applied to all inputs.
The model is trained as a multi-class classifier with
the categorical cross-entropy loss function and the
RMSprop optimizer. All hidden layers use the
ReLU activation function and have the same num-
ber of hidden units as the dimension of the mention
embeddings. The convolution layers use the same
filter sizes as the dimensions of input embeddings.

6 Experiments

Following Chen and Choi (2016), experiments are
conducted on two tasks, coreference resolution and

220



Model
Episode-Level Scene-Level

MUC B3 CEAFe µ |C| MUC B3 CEAFe µ |C|
Clark and Manning (2016) 89.58 69.12 47.33 68.68 15.19 90.38 76.79 56.95 74.70 8.13
Wiseman et al. (2016) 89.80 57.66 45.48 64.31 14.86 89.60 78.08 65.95 77.88 6.20
This work (ACNN) 89.92 70.33 44.09 68.11 16.40 88.09 78.77 59.72 75.53 7.49

Table 4: Coreference resolution results on the evaluation set (in %).
µ = (MUC + B3 + CEAFe) / 3. |C|: the average cluster size.

entity linking. Our coreference resolution model
shows robust performance compared to other state-
of-the-art systems (Section 6.2). Our entity linking
model significantly outperforms the heuristic-based
approach from the previous work (Section 6.3). All
models are evaluated on the gold mentions to focus
purely on the analysis of these two tasks.

6.1 Data Split

The corpus is split into the training, development,
and evaluation sets (Table 5). For the episode-level,
all mentions referring to the same character in each
episode are grouped into one cluster (CEpi). For the
scene-level, this grouping is done by each scene
such that there can be multiple mention clusters that
refer to the same character within an episode (CSce).
Ambiguous mention types such as collective, gen-
eral, and other are excluded from our experiments
(Section 3); including those mentions requires de-
veloping different resolution models that we shall
explore in the future.

E S DC CE CS M
TRN 38 362 371 820 2,026 12,842
DEV 3 28 44 58 159 991
TST 5 58 80 113 301 1,885
Total 46 448 444 991 2,486 15,718

Table 5: The training (TRN), development (DEV),
and evaluation (TST) sets. E/S/DC/CE/CS/M: the
numbers of episodes, scenes, distinct characters,
episode/scene-level clusters, and mentions.

For entity linking, entity labels are predetermined
by collecting characters that appear in all three sets;
characters that do not appear in any of the three sets
are put together and labeled as Unknown. This is
reasonable because it is not possible for a statistical
model to learn about characters that do not appear
in the training set. Likewise, characters that appear
in the training set but not in the other sets cannot
be developed or evaluated. A total of ten labels
are used for entity linking that consist of the top-9

most frequently appeared characters across all sets
and unknown (Figure 3).

6.2 Coreference Resolution
To benchmark the robustness of our ACNN model
(Section 4), two state-of-the-art coreference resolu-
tion systems are also experimented. Episode and
scene-level models are developed separately for all
three systems using the same dataset in Table 5. All
system outputs are evaluated with the MUC (Vi-
lain et al., 1995), B3 (Bagga and Baldwin, 1998),
and CEAFe (Luo, 2005) metrics suggested by the
CoNLL’12 shared task (Pradhan et al., 2012). The
average score of five trials is reported for each met-
ric to minimize variance because these systems use
neural network approaches with random initializa-
tion to produce varying results per trial (Table 4).

Table 1

All Tst
Ross 2221 190
Rachel 1969 239
Chandler 1753 235
Monica 1622 201
Joey 1475 192
Phoebe 1373 143
Carol 225 49
Barry 144 11
Mindy 100 9
Other 4836 616

31%

9%
9% 10%

11%

13%

14% Ross
Rachel
Chandler
Monica
Joey
Phoebe
Carol
Barry
Mindy
Other

33%

1%
3%

8%

10%

11%

12%

13%

10%

Ross
Rachel
Chandler
Monica
Joey
Phoebe
Carol
Barry
Mindy
Other

1%

Figure 3: Character labels used for entity linking.

Comparison between the State-of-the-Art
When trained and evaluated on our dataset, both the
Stanford (Clark and Manning, 2016) and the Har-
vard (Wiseman et al., 2016) systems give compara-
ble results to their performance on the CoNLL’12
dataset.7 The Stanford system using its pre-trained
model gives the µ scores of 47.67% and 64.14% for
the episode and scene-level respectively, which sig-
nifies the importance of the in-domain training data.
7The Stanford and the Harvard systems reported µ scores of
65.73% and 64.21% on the CoNLL’12 dataset, respectively.

221



Model Ross Joey Chandler Monica Phoebe Rachel Carol Mindy Barry Unk. Avg Acc

E
B 57.54 80.94 64.91 89.82 87.86 76.47 30.14 0 16.67 70.24 57.46 72.52

ME 72.81 80.31 82.43 79.78 82.71 82.94 44.84 20.00 53.05 76.23 67.51 77.80
CE 93.46 97.90 98.23 95.42 98.24 95.02 100.00 0 95.65 93.71 86.76 95.30

S
B 60.00 69.09 61.05 72.51 57.27 78.77 34.38 0 11.76 67.62 51.24 66.68

ME 74.75 81.76 80.71 88.83 84.33 85.43 53.15 20.00 62.90 80.82 71.27 81.07
CE 91.29 90.64 86.33 94.10 85.41 90.16 65.35 18.71 83.45 85.82 79.12 87.64

Table 6: Entity linking results on the evaluation set (in %). The F1 score is reported for each character.
E/S: episode/scene level. Unk.: unknown. Avg: the macro-average F1 score between all characters.

Acc: (the number of correctly labeled mentions) / (the total number of mentions).

All systems show higher scores for the scene-level
than the episode-level consistently, which confirms
the difficulty of this task on larger documents.

Although both systems take advantage of global
cluster features, they reveal different strengths on
resolving mentions with respect to the cluster size.
The Stanford system excels for the episode-level,
which is primarily attributed to the cluster-based na-
ture of this system; it is able to find more accurate
coreferent chains when the clusters are larger. The
Harvard system performs best for the scene-level,
indicating that its neural architecture with Long
Short-Term Memory cells captures more meaning-
ful cluster features when the clusters are smaller.

Comparison to Agglomerative CNN
In comparison to the other state-of-the-art systems,
our ACNN model shows competitive performance;
it gives the highest B3 and comparable µ scores
for both episode and scene levels. We measure
the average cluster size produced by each system
for further analysis (|C| in Table 4). The Harvard
system produces smaller clusters than the other
two systems. Such a tendency gives more pure
clusters, favored by the CEAFe metric for the scene-
level. However, it is prone to breaking up too many
links, which leads to poor performance in the B3

evaluation on the episode-level.
The performance of our model is encouraging

although coreference resolution is not the end goal.
We design this model to automatically generate
mention embeddings and mention-pair embeddings
that are used to construct cluster features for entity
linking. However, even though this model’s success
in coreference resolution is not our final objective,
its success directly correlates to the success of en-
tity linking because of the similarity between these
two tasks. Due to the similar nature of these two
tasks, the success of coreference resolution directly
correlates to that of entity linking. These embed-

dings are the essence of our entity linking model,
leading to a huge improvement.

6.3 Entity Linking

The heuristic-based approach proposed by Chen
and Choi (2016) is adapted to establish the baseline.
Two statistical models are experimented for both
the episode and scene levels, one using only men-
tion embeddings and the other using both mention
embeddings and cluster embeddings (Section 5).
All models are evaluated with the F1 scores of char-
acter labels, the macro-average F1 scores between
all labels, and the label accuracies. The average
scores of five trials are reported in Table 6.

B: Baseline Model
The heuristic-based approach is applied to the men-
tion clusters found by our coreference resolution
model. Two rules, 1)proper noun and 2)first-person
pronoun matches, are used to assign character la-
bels to all mentions. The label of each cluster is
then determined by the majority vote between the
mention labels within the cluster. Finally, the clus-
ter label is assigned to all mentions in that cluster.
This model performs better when it is applied to
the episode-level clusters because larger clusters
provide more mention labels, which makes the ma-
jority vote more reliable.

ME: Mention Embedding Model
This model takes advantage of the mention embed-
dings generated by our ACNN model. Compared to
the baseline, it gives over a 21% higher average F1
score, and over a 15% higher label accuracy for the
episode and the scene levels, respectively. Interest-
ingly, this model shows higher performance for the
scene-level, which is not the case for the other two
models. This implies that the mention embeddings
learned from scene-level documents are more infor-
mative than those learned from episode-level ones.

222



System

Ross Joey Chandler Monica Phoebe Rachel Carol Mindy Barry Unk. Σ
G
o
l
d

Ross 182 7 1 190
Joey 186 6 192

Chandler 235 235
Monica 1 200 201
Phoebe 141 2 143
Rachel 2 237 239
Carol 49 49
Mindy 0 9 9
Barry 11 11
Other 11 1 11 21 4 5 1 562 616

Σ 193 187 247 223 145 244 54 0 12 580 1,885

Table 7: The confusion matrix between gold and system annotation for all character labels (in #).

This case is also reflected on its coreference resolu-
tion performance where the scene-level scores are
higher than the episode-level scores (Table 4).

CE: Cluster Embedding Model
While the mention embeddings give a significant
improvement over the baseline, further improve-
ment is made when they are coupled with the clus-
ter and mention-cluster embeddings. The episode-
level cluster embedding model shows an average
F1 score of 86.76% and a label accuracy of 95.30%,
which is another 15% improvement, suggesting a
practical use of this model in real applications. A
couple of important observations are made:

• Cluster and mention-cluster embeddings, al-
though learned during coreference resolution,
are crucial for entity linking such that a coref-
erence resolution model specifically designed
for multiparty dialogues is necessary to build
the state-of-the-art entity linking model for
this genre.

• Clusters generated from the episode-level doc-
uments provide more information than those
from the scene-level do, which aligns with the
conclusion made by Chen and Choi (2016).

Error Analysis
An error analysis is performed on the episode-level
cluster embedding model. From the confusion ma-
trix in Table 7, two common system errors are de-
tected. First, most of the mispredictions identify
Unknown as specific characters. Second, the perfor-
mance on the secondary characters, Carol, Mindy,
and Barry, is subpar with respect to other enti-
ties. This subpar performance likely stems from

a paucity of appearances by these secondary char-
acters. For example, Mindy constitutes 1% of the
dataset (Figure 3) and has only nine occurrences in
the evaluation set. Our best model is robust in iden-
tifying the primary characters, showing an average
F1 score of 96.38% and an accuracy of 98.42% on
the evaluation set.

7 Conclusion

In this paper, we explore a relatively new task, char-
acter identification on multiparty dialogues, and
introduce a novel perspective on approaching the
task with coreference resolution and entity linking.
We improve and augment finer-grained annotation
over the existing corpus that simulates real conver-
sations. We propose a deep convolutional neural
network to agglomerate groups of features into
mention, mention-pair, cluster, and mention-cluster
embeddings that are optimized for entity predic-
tion. Our coreference resolution result shows an
improvement on the updated version of the corpus.
Our entity linking result reaches to the accuracy
that is sufficient for real-world applications.

To the best of our knowledge, our work is the
first time that such deep convolution layers have
been used for training mention and cluster embed-
dings. Our results show that the generation of these
embeddings is crucial for the success of entity link-
ing on multiparty dialogues. For future work, we
will continue to increase the size of the corpus with
high-quality and disambiguated annotation. We
also wish to improve the embeddings to represent
plural and collective mentions, thus we can build
upon our entity linking model incorporating many-
to-many linkings between entities and mentions.

223



References
Amit Bagga and Breck Baldwin. 1998. Algorithms

for scoring coreference chains. In The first interna-
tional conference on language resources and evalu-
ation workshop on linguistics coreference. Citeseer,
volume 1, pages 563–566.

Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics, Sydney, Australia, pages
33–40.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606 .

Yu-Hsin Chen and Jinho D. Choi. 2016. Character
identification on multiparty conversation: Identify-
ing mentions of characters in tv shows. In Proceed-
ings of the 17th Annual Meeting of the Special Inter-
est Group on Discourse and Dialogue. Association
for Computational Linguistics, Los Angeles, pages
90–100. http://www.aclweb.org/anthology/W16-
3612.

Kevin Clark and Christopher D. Manning. 2016.
Deep reinforcement learning for mention-ranking
coreference models. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Austin, Texas, pages 2256–2262.
https://aclweb.org/anthology/D16-1245.

Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing. Association for Compu-
tational Linguistics, Seattle, Washington.

Matthew Francis-Landau, Greg Durrett, and Dan Klein.
2016. Capturing semantic similarity for entity
linking with convolutional neural networks. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies. Association for Computational Linguis-
tics, San Diego, California, pages 1256–1261.
http://www.aclweb.org/anthology/N16-1150.

Abhishek Gattani, Digvijay S. Lamba, Nikesh
Garera, Mitul Tiwari, Xiaoyong Chai, Sanjib
Das, Sri Subramaniam, Anand Rajaraman, Venky
Harinarayan, and AnHai Doan. 2013. En-
tity extraction, linking, classification, and tag-
ging for social media: A wikipedia-based ap-
proach. Proc. VLDB Endow. 6(11):1126–1137.
https://doi.org/10.14778/2536222.2536237.

Stephen Guo, Ming-Wei Chang, and Emre Kiciman.
2013. To Link or Not to Link? A Study on End-
to-End Tweet Entity Linking. In Proceedings of the

Conference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology. NAACL, pages 1020–1030.

Matthew Henderson, Blaise Thomson, and Steve
Young. 2013. Deep neural network approach for
the dialog state tracking challenge. In Proceed-
ings of the SIGDIAL 2013 Conference. Association
for Computational Linguistics, Metz, France, pages
467–471. http://www.aclweb.org/anthology/W13-
4073.

Seokhwan Kim, Luis Fernando D́Haro, Rafael E.
Banchs, Jason D. Williams, and Matthew Hender-
son. 2015. The Fourth Dialog State Tracking Chal-
lenge. In Proceedings of the 4th Dialog State Track-
ing Challenge. DSTC4.

Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing. Asso-
ciation for Computational Linguistics, pages 25–32.

Rada Mihalcea and Andras Csomai. 2007a. Wikify!:
Linking Documents to Encyclopedic Knowledge. In
Proceedings of the Sixteenth ACM Conference on
Conference on Information and Knowledge Manage-
ment. CIKM’07, pages 233–242.

Rada Mihalcea and Andras Csomai. 2007b. Wik-
ify!: Linking documents to encyclopedic
knowledge. In Proceedings of the Sixteenth
ACM Conference on Conference on Informa-
tion and Knowledge Management. ACM, New
York, NY, USA, CIKM ’07, pages 233–242.
https://doi.org/10.1145/1321440.1321475.

Nobal B. Niraula, Vasile Rus, Rajendra Banjade, Dan
Stefanescu, William Baggett, and Brent Morgan.
2014. The DARE Corpus: A Resource for Anaphora
Resolution in Dialogue Based Intelligent Tutoring
Systems. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation.
LREC’14, pages 3199–3203.

Haoruo Peng, Kai-Wei Chang, and Dan Roth. 2015.
A Joint Framework for Coreference Resolution and
Mention Head Detection. In Proceedings of the 9th
Conference on Computational Natural Language
Learning. CoNLL’15, pages 12–21.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling Multilingual Unre-
stricted Coreference in OntoNotes. In Proceedings
of the Sixteenth Conference on Computational Nat-
ural Language Learning: Shared Task. CoNLL’12,
pages 1–40.

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011a. Local and Global Algorithms for
Disambiguation to Wikipedia. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies. ACL’11, pages 1375–1384.

224



Lev Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011b. Local and global algo-
rithms for disambiguation to wikipedia. In
Proceedings of the 49th Annual Meeting of
the Association for Computational Linguis-
tics: Human Language Technologies - Volume
1. Association for Computational Linguistics,
Stroudsburg, PA, USA, HLT ’11, pages 1375–1384.
http://dl.acm.org/citation.cfm?id=2002472.2002642.

Marco Rocha. 1999. Coreference Resolution in Dia-
logues in English and Portuguese. In Proceedings
of the Workshop on Coreference and Its Applications.
CorefApp’99, pages 53–60.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Pro-
ceedings of the 6th conference on Message under-
standing. Association for Computational Linguis-
tics, pages 45–52.

Sam Wiseman, Alexander M. Rush, Stuart Shieber,
and Jason Weston. 2015. Learning anaphoric-
ity and antecedent ranking features for corefer-
ence resolution. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Computa-
tional Linguistics, Beijing, China, pages 1416–1426.
http://www.aclweb.org/anthology/P15-1137.

Sam Wiseman, Alexander M. Rush, and Stuart M.
Shieber. 2016. Learning global features for coref-
erence resolution. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, San Diego, California, pages 994–1004.
http://www.aclweb.org/anthology/N16-1114.

J. L. Wu and W. Y. Ma. 2017. A deep learn-
ing framework for coreference resolution
based on convolutional neural network. In
2017 IEEE 11th International Conference
on Semantic Computing (ICSC). pages 61–64.
https://doi.org/10.1109/ICSC.2017.57.

225


