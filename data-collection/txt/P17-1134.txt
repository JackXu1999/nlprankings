



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1457–1469
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1134

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1457–1469
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1134

Unsupervised Text Segmentation Based on Native Language
Characteristics

Shervin Malmasi1,2 Mark Dras2 Mark Johnson2 Lan Du3 Magdalena Wolska4
1Harvard Medical School, Harvard University

smalmasi@bwh.harvard.edu
2Department of Computing, Macquarie University

{ shervin.malmasi, mark.dras, mark.johnson }@mq.edu.au
3Faculty of IT, Monash University

lan.du@monash.edu
4LEAD Graduate School, Universität Tübingen
magdalena.wolska@uni-tuebingen.de

Abstract

Most work on segmenting text does so on
the basis of topic changes, but it can be of
interest to segment by other, stylistically
expressed characteristics such as change
of authorship or native language. We pro-
pose a Bayesian unsupervised text seg-
mentation approach to the latter. While
baseline models achieve essentially ran-
dom segmentation on our task, indicating
its difficulty, a Bayesian model that incor-
porates appropriately compact language
models and alternating asymmetric priors
can achieve scores on the standard metrics
around halfway to perfect segmentation.

1 Introduction

Most work on automatically segmenting text has
been on the basis of topic: segment boundaries
correspond to topic changes (Hearst, 1997). There
are various contexts, however, in which it is of in-
terest to identify changes in other characteristics;
for example, there has been work on identifying
changes in authorship (Koppel et al., 2011) and
poetic voice (Brooke et al., 2012). In this paper
we investigate text segmentation on the basis of
change in the native language of the writer.

Two illustrative contexts where this task might
be of interest are patchwriting detection and lit-
erary analysis. Patchwriting is the heavy use of
text from a different source with some modifica-
tion and insertion of additional words and sen-
tences to form a new text. Pecorari (2003) notes
that this is a kind of textual plagiarism, but is
a strategy for learning to write in an appropri-
ate language and style, rather than for deception.
Keck (2006), Gilmore et al. (2010) and Vieyra
et al. (2013) found that non-native speakers, not

surprisingly in situations of imperfect mastery of
a language, are strongly over-represented in this
kind of textual plagiarism. In these cases the
boundaries between the writer’s original text and
(near-)copied native text are often quite appar-
ent to the reader, as in this short example from
Li and Casanave (2012) (copied text italicised):
“Nevertheless, doubtfulness can be cleared rea-
sonably by the experiments conducted upon the
‘split-brain patients’, in whom intra-hemispheric
communication is no longer possible. To illustrate,
one experiment has the patient sit at a table with
a non-transparent screen blocking the objects be-
hind, who is then asked to reach the objects with
different hand respectively.” Because patchwrit-
ing can indicate imperfect comprehension of the
source (Jamieson and Howard, 2013), identifying
it and supporting novice writers to improve it has
become a focus of programmes like the Citation
Project.1

For the second, perhaps more speculative con-
text of literary analysis, consider Joseph Conrad,
known for having written a number of famous
English-language novels, such as Heart of Dark-
ness; he was born in Poland and moved to England
at the age of 21. His writings have been the subject
of much manual analysis, with one particular di-
rection of such research being the identification of
likely influences on his English writing, including
his native Polish language and the French he learnt
before English. Morzinski (1994), for instance,
notes aspects of his writing that exhibit Polish-like
syntax, verb inflection, or other linguistic charac-
teristics (e.g. “Several had still their staves in their
hands” where the awkwardly placed adverb still is
typical of Polish). These appear both in isolated
sentences and in larger chunks of text, and part of

1http://citationproject.net/

1457

https://doi.org/10.18653/v1/P17-1134
https://doi.org/10.18653/v1/P17-1134


an analysis can involve identifying these chunks.
This raises the question: Can NLP and com-

putational models identify the points in a text
where native language changes? Treating this as
an unsupervised text segmentation problem, we
present the first Bayesian model of text segmen-
tation based on authorial characteristics, applied
to native language.

2 Related Work

Topic Segmentation The most widely-
researched text segmentation task has as its
goal to divide a text into topically coherent
segments. Lexical cohesion (Halliday and Hasan,
1976) is an important concept here: the principle
that text is not formed by a random set of words
and sentences but rather logically ordered sets
of related words that together form a topic.
In addition to the semantic relation between
words, other methods such as back-references and
conjunctions also help achieve cohesion. Based
on this, Morris and Hirst (1991) proposed the
use of lexical chains, sequences of related words
(defined via thesaurus), to break up a text into
topical segments: breaks in lexical chains indicate
breaks in topic. The TextTiling algorithm (Hearst,
1994, 1997) took a related approach, defining a
function over lexical frequency and distribution
information to determine topic boundaries, and
assuming that each topic has its own vocabulary
and that large shifts in this vocabulary usage
correspond to topic shifts.

There have been many approaches since that
time. A key one, which is the basis for our
own work, is the unsupervised Bayesian technique
BAYESSEG of Eisenstein and Barzilay (2008),
based on a generative model that assumes that
each segment has its own language model. Un-
der this assumption the task can be framed as pre-
dicting boundaries at points which maximize the
probability of a text being generated by a given
language model. Their method is based on lexical
cohesion — expressed in this context as topic seg-
ments having compact and consistent lexical dis-
tributions — and implements this within a prob-
abilistic framework by modelling words within
each segment as draws from a multinomial lan-
guage model associated with that segment.

Much other subsequent work either uses this as
a baseline, or extends it in some way: Jeong and
Titov (2010), for example, who propose a model
for joint discourse segmentation and alignment for

documents with parallel structures, such as a text
with commentaries or presenting alternative views
on the same topic; or Du et al. (2013), who use
hierarchical topic structure to improve the linear
segmentation.

Bible Authorship Koppel et al. (2011) consider
the task of decomposing a document into its au-
thorial components based on their stylistic proper-
ties and propose an unsupervised method for do-
ing so. The authors use as their data two bibli-
cal books, Jeremiah and Ezekiel, that are gener-
ally believed to be single-authored: their task was
to segment a single artificial text constructed by
interleaving chapters of the two books. Their most
successful method used work in biblical scholar-
ship on lexical choice: they give as an example
the case that in Hebrew there are seven synonyms
for the word fear, and that different authors may
choose consistently from among them. Then, hav-
ing constructed their own synsets using available
biblical resources and annotations, they represent
texts by vectors of synonyms and apply a modified
cosine similarity measure to compare and cluster
these vectors. While the general task is relevant to
this paper, the particular notion of synonymy here
means the approach is specific to this problem, al-
though their approach is extended to other kinds
of text in Akiva and Koppel (2013). Aldebei et al.
(2015) proposed a new approach motivated by this
work, similarly clustering sentences, then using a
Naive Bayes classifier with modified prior proba-
bilities to classify sentences.

Poetry Voice Detection Brooke et al. (2012)
perform stylistic segmentation of a well-known
poem, The Waste Land by T.S. Eliot. This poem is
renowned for the great number of voices that ap-
pear throughout the text and has been the subject
of much literary analysis (Bedient and Eliot, 1986;
Cooper, 1987). These distinct voices, conceived of
as representing different characters, have differing
tones, lexis and grammatical styles (e.g. reflecting
the level of formality). The transitions between
the voices are not explicitly marked in the poem
and the task here is to predict the breaks where
these voice changes occur. The authors argue that
the use of generative models is not feasible for
this task, noting: “Generative models, which use
a bag-of-words assumption, have a very different
problem: in their standard form, they can capture
only lexical cohesion, which is not the (primary)
focus of stylistic analysis.”

1458



They instead present a method based on a curve
that captures stylistic change, similar to the Text-
Tiling approach but generalised to use a range of
features. The local maxima in this change curve
represent potential breaks in the text. The features
are both internal to the poem (e.g. word length,
syllable count, POS tag) as well as external (e.g.
average unigram counts in the 1T Corpus or senti-
ment polarity from a lexicon). Results on an artifi-
cially constructed mixed-style poem achieve a Pk
of 0.25. Brooke et al. (2013) extend this by consid-
ering clustering following an initial segmentation.

Native Language Identification (NLI) NLI
casts the detecting of native language (L1) influ-
ence in writing in a non-native (L2) language as a
classification task: the framing of the task in this
way comes from Koppel et al. (2005). There has
been much activity on it in the last few years, with
Tetreault et al. (2012) providing a comprehensive
analysis of features that had been used up until that
point, and a shared task in 2013 (Tetreault et al.,
2013) that attracted 29 entrants. The shared task
introduced a new, now-standard dataset, TOEFL11,
and work has continued on improving classifica-
tion results, e.g. by Bykh and Meurers (2014) and
Ionescu et al. (2014).

In addition to work on the classification task
itself, there have also been investigations of the
features used, and how they might be employed
elsewhere. Malmasi and Cahill (2015) examine
the effectiveness of individual feature types used
in the shared task and the diversity of those fea-
tures. Of relevance to the present paper, sim-
ple part-of-speech n-grams alone are fairly ef-
fective, with classification accuracies of between
about 40% and 65%; higher-order n-grams are
more effective than lower, and the more fine-
grained CLAWS2 tagset more effective than the
Penn Treebank tagset. An area for application
of these features is in Second Language Acquisi-
tion (SLA), as a data-driven approach to finding
L1-related characteristics that might be a result of
cross-linguistic influence and consequently a pos-
sible starting for an SLA hypothesis (Ellis, 2008);
Swanson and Charniak (2013) and Malmasi and
Dras (2014) propose methods for this.

Tying It Together Contra Brooke et al. (2012),
we show that it is possible to develop effective
generative models for segmentation on stylistic
factors, of the sort used for topic segmentation.
To apply it specifically to segmentation based on a
writer’s L1, we draw on work in NLI.

3 Experimental Setup

We investigate the task of L1-based segmentation
in three stages:
1. Can we define any models that do better than
random, in a best case scenario? For this best case
scenario, we determine results over a devset with
the best prior found by a grid search, for a single
language pair likely to be relatively easily distin-
guishable. Note that as this is unsupervised seg-
mentation, it is a devset in the sense that it is used
to find the best prior, and also in a sense that some
models as described in §4 use information from a
related NLI task on the underlying data.
2. If the above is true, do the results also hold for
test data, using priors derived from the devset?
3. Further, do the results also hold for all language
pairs available in our dataset, not just a single eas-
ily distinguishable pair?

We first describe the evaluation data — artifi-
cial texts generated from learner essays, similar to
the artificially constructed texts of previously de-
scribed work on Bible authorship and poetry seg-
mentation — and evaluation metrics, followed in
§4 by the definitions of our Bayesian models.
3.1 Source Data

We use as the source of data the TOEFL11 dataset
used for the NLI shared task (Blanchard et al.,
2013) noted in §2. The data consists of 12100
essays by writers with 11 different L1s, taken
from TOEFL tests where the test-taker is given a
prompt2 as the topic for the essay. The corpus is
balanced across L1s and prompts (which allows us
to verify that segmentation isn’t occurring on the
basis of topic), and is split into standard training,
dev and test sets.

3.2 Document Generation

As the main task is to segment texts by the author’s
L1, we want to ensure that we are not segment-
ing by topic and thus use texts written by authors
from different L1 backgrounds on the same topic
(prompt). We will also create one dataset to verify
that segmentation by topic works in this domain;
for this we use texts written by authors from the
same L1 background on different topics.

For our L1-varying datasets, we construct com-
posite documents to be segmented as alternat-

2For example, prompt P7 is: “Do you agree or disagree
with the following statement? It is more important for stu-
dents to understand ideas and concepts than it is for them to
learn facts. Use reasons and examples to support your an-
swer.”

1459



ing segments drawn from TOEFL11 from two dif-
ferent L1s holding the topic (prompt) constant,
broadly following a standard approach (Brooke
et al., 2012, for example) (see Appendix A.1 for
details). We follow the same process for our topic-
varying datasets, but hold the L1 constant while
alternating the topic (prompt). For our single pair
of L1s, we choose German and Italian. German
is the class with the highest NLI accuracy in the
TOEFL11 corpus across the shared task results and
Italian also performs very well. Additionally, there
is very little confusion between the two; a bi-
nary NLI classifier we trained on the language pair
achieved 97% accuracy. For our all-pairs results,
given the 11 languages in the TOEFL11 corpus, we
have 55 sets of documents of alternating L1s (one
of which is German–Italian).

We generate four distinct types of datasets for
our experiments using the above methodology.
The documents in these datasets, as described be-
low, differ in the parameters used to select the es-
says for each segment and what type of tokens are
used. Tokens (words) can be represented in their
original form and used for performing segmenta-
tion. Alternatively, using an insight from Wong
et al. (2012), we can represent the documents at a
level other than lexical: the text could consist of
the POS tags corresponding to all of the tokens,
or n-grams over those POS tags. The POS repre-
sentation is motivated by the usefulness of POS-
based features for capturing L1-based stylistic dif-
ferences as noted in §2. Our method for encoding
n-grams is described in Appendix A.2.

TOPICSEG-TOKENS This data is generated by
keeping the L1 class constant and alternating seg-
ments between two topics. We chose Italian for
the L1 class and essays from the prompts “P7”
and “P8” are used. The dataset, constructed from
TOEFL11-TRAIN and TOEFL11-DEV, contains a
total of 53 artificial documents, and will be used
to verify that topic segmentation as discussed in
Eisenstein and Barzilay (2008) functions as ex-
pected for data from this domain: that is, that topic
change is detectable.

TOPICSEG-PTB Here the tokens in each text
are replaced with their POS tags or n-grams over
those tags, and the segmentation is performed over
this data. In this dataset the tags are obtained via
the Stanford Tagger and use the Penn Treebank
(PTB) tagset. The same source data (TOEFL11-
TRAIN and TOEFL11-DEV), L1 and topics as
TOPICSEG-TOKENS are used for a total of 53

documents. This dataset will be used to investi-
gate, inter alia, whether segmentation over these
stylistically related features could take advantage
of topic cues. We would expect not.

L1SEG-PTB This dataset is used for segmenta-
tion based on native language, also using (n-grams
over) the PTB POS tags. We choose a specific
topic and then retrieve all essays from the corpus
that match this; here we chose prompt “P7”, since
it had the largest number of essays for our cho-
sen single L1 pair, German–Italian. For the dataset
constructed from TOEFL11-TRAIN and TOEFL11-
DEV (which we will refer to as L1SEG-PTB-GI-
DEV), this resulted in 57 documents. Documents
that are composites of two L1s are then generated
as described above. For investigating questions
2 and 3 above, we similarly have datasets con-
structed from the the smaller TOEFL11-TEST data
(L1SEG-PTB-GI-TEST), which consist of 5 doc-
uments of 5 segments each for the single L1 pair,
and from all language pairs (L1SEG-PTB-ALL-
DEV, L1SEG-PTB-ALL-TEST). We would ex-
pect that these datasets should not be segmentable
by topic, as all the segments are on the same topic;
the segments should however, differ in stylistic
characteristics related to the L1.

L1SEG-CLAWS2 This dataset is generated us-
ing the same methodology as L1SEG-PTB, with
the exception that the essays are tagged using the
RASP tagger which uses the more fine-grained
CLAWS2 tagset, noting that the CLAWS2 tagset
performed better in the NLI classification task
(Malmasi and Cahill, 2015).

3.3 Evaluation

We use the standard Pk (Beeferman et al., 1999)
and WindowDiff (WD) (Pevzner and Hearst,
2002) metrics, which (broadly speaking) select
sentences using a moving window of size k and
determines whether these sentences correctly or
incorrectly fall into the same or different reference
segmentations. Pk and WD scores range between
0 and 1, with a lower score indicating better per-
formance, and 0 a perfect segmentation. It has
been noted that some “degenerate” algorithms —
such as placing boundaries randomly or at every
possible position — can score 0.5 (Pevzner and
Hearst, 2002). WD scores are typically similar
to Pk, correcting for differential penalties between
false positive boundaries and false negatives im-
plicit in Pk. Pk and WD scores reported in §5 are

1460



averages across all documents in a dataset. Formal
definitions are given in Appendix A.3.

4 Segmentation Models

For all of our segmentation we use as a starting
point the unsupervised Bayesian method of Eisen-
stein and Barzilay (2008); see §2.3 We recap the
important technical definitions here.

In Equation 1 of their work they define the ob-
servation likelihood as,

p(X | z,Θ) =
T∏

t

p(xt | θzt), (1)

where X is the set of all T sentences, z is the
vector of segment assignments for each sentence,
xt is the bag of words drawn from the language
model and Θ is the set of all K language models
Θ1 . . .ΘK . As is standard in segmentation work,
K is assumed to be fixed and known (Malioutov
and Barzilay, 2006); it is set to the actual num-
ber of segments. The authors also impose an ad-
ditional constraint, that zt must be equal to either
zt−1 (the previous sentence’s segment) or zt−1 +1
(the next segment), in order to ensure a linear seg-
mentation.

This segmentation model has two parameters:
the set of language models Θ and the segment as-
signment indexes z. The authors note that since
this task is only concerned with the segment as-
signments, searching in the space of language
models is not desirable. They offer two alterna-
tives to overcome this: (1) taking point estimates
of the language models, which is considered to
be theoretically unsatisfying and (2) marginalizing
them out, which yields better performance. Equa-
tion 7 of Eisenstein and Barzilay (2008), repro-
duced here, shows how they marginalize over the
language models, supposing that each language
model is drawn from a symmetric Dirichlet prior
(i.e. θj ∼ Dir(θ0)):

p(X | z, θ0) =
K∏

j

pdcm({xt : zt = j} | θ0) (2)

The Dirichlet compound multinomial distribu-
tion pdcm expresses the expectation over all the
multinomial language models, when conditioned
on the symmetric Dirichlet prior θ0:

3An open-source implementation of the method, called
BAYESSEG, is made available by the authors at http://
groups.csail.mit.edu/rbg/code/bayesseg/

pdcm({xt : zt = j} | θ0) =
Γ(Wθ0)

Γ(Nj +Wθo)

W∏

i

Γ(nj,i + θ0)

Γ(θ0)
(3)

whereW is the number of words in the vocabulary
and Nj =

∑W
i nj,i, the total number of words in

the segment j. They then observe that the optimal
segmentation maximizes the joint probability

p(X, z | θ0) = p(X | z, θ0)p(z)
and assume a uniform p(z) over valid segmen-
tations with no probability mass assigned to in-
valid segmentations. The hyperparameter θ0 can
be chosen, or can be learned via an Expectation-
Maximization process.

Inference Eisenstein and Barzilay (2008) de-
fined two methods of inference, a dynamic pro-
gramming (DP) one and one using Metropolis-
Hastings (MH). Only MH is applicable where
shifting a boundary will affect the probability of
every segment, not just adjacent segments, as in
their model incorporating cue phrases. Where this
is not the case, they use DP inference. Their DP
inference algorithm is suitable for all of our mod-
els, so we also use that.

Priors For our priors, we carry out a grid search
on the devsets (that is, the datasets derived from
TOEFL11-TRAIN and TOEFL11-DEV) in the in-
terval [0.1, 3.0], partitioned into 30 evenly spaced
values; this includes both weak and strong priors.4

4.1 TOPICSEG
Our first model is exactly the one proposed by
Eisenstein and Barzilay (2008) described above.
The aim here is to look at how we perform at seg-
menting learner essays by topic in order to con-
firm that topic segmentation works for this domain
and these types of topics. We apply this model
to the TOPICSEG-TOKENS and TOPICSEG-PTB
datasets where the texts have the same L1 and
boundaries are placed between essays of differing
topics (prompts).

4.2 L1SEG
Our second model modifies that of Eisenstein and
Barzilay (2008) by revising the generative story.

4The Eisenstein and Barzilay (2008) code does implement
an EM method for finding priors in the symmetric case, but
we found that perhaps surprisingly the grid search almost al-
ways found better ones.

1461



Where they assume a standard generative model
over words with constraints on topic change be-
tween sentences, we make minor modifications to
adapt the model for our task. The standard gen-
erative story (Blei, 2012) — an account of how a
model generates the observed data — usually gen-
erates words in a two-stage process: (1) For each
document, randomly choose a distribution of top-
ics. (2) For each word in the document: (a) Assign
a topic from those chosen in step 1. (b) Randomly
choose a word from that topic’s vocabulary.

Here we modify this story to be over part-of-
speech data instead of lexical items. By using this
representation (which as noted in §2 is useful for
NLI classification) we aim to segment our texts
based on the L1 of the author for each segment.
For this model we only make use of the L1SEG-
PTB-GI-DEV dataset.5

4.3 L1SEG-COMP

It is not obvious that the same properties that
produce compact distributions in standard lexical
chains would also be the case for POS data, par-
ticularly if extended to POS n-grams which can
result in a very large number of potential tokens.
In this regard Eisenstein and Barzilay (2008) note:
“To obtain a high likelihood, the language mod-
els associated with each segment should concen-
trate their probability mass on a compact subset of
words. Language models that spread their proba-
bility mass over a broad set of words will induce a
lower likelihood. This is consistent with the prin-
ciple of lexical cohesion.”

Eisenstein and Barzilay (2008) discuss this
within the context of topic segmentation.6 How-
ever, it is unclear if this would also would happen
for POS tags; there is no syntactic analogue for the
sort of lexical chains important in topic segmenta-
tion. It may then turn out that using all POS tags
or n-grams over them as in the previous model
would not achieve a strong performance. We thus
use knowledge from the NLI classification task to
help.

Discarding Non-Discriminative Features One
approach that could possibly overcome these lim-

5We also looked at including words. The results of these
models were always worse, and we do not discuss them in
this paper.

6For example, a topic segment related to the previously
mentioned essay prompt P7 might concentrate its proba-
bility mass on the following set of words: {education,
learning, understanding, fact, theory,
idea, concept, knowledge}.

itations is the removal of features from the in-
put space that have been found to be non-
discriminative in NLI classification. This would
allow us to encode POS sequence information via
n-grams while also keeping the model’s vocabu-
lary sufficiently small. Doing this requires the use
of extrinsic information for filtering the n-grams.
The use of such extrinsic information has proven
to be useful for other similar tasks such as the po-
etry style change segmentation work of Brooke
et al. (2012), as noted in §2.

We perform this filtering using the discrimina-
tive feature lists derived from the NLI classifica-
tion task using the system and method described
in Malmasi and Dras (2014), also noted in §2. We
extract the top 300 most discriminative POS n-
gram features for each L1 from TOEFL11-TRAIN
and TOEFL11-DEV, resulting in two lists of 600
POS bigrams and trigrams; these are thus inde-
pendent of our test datasets. (We illustrate a text
with respect to these discriminative features in Ap-
pendix A.4.) Note that discriminative n-grams can
overlap with each other within the same class and
also between two classes. We resolve such con-
flicts by using the weights of the features from
the classification task as described in Malmasi and
Dras (2014) and choosing the feature with the
higher weight.

4.4 L1SEG-ASYMP
Looking at the distribution of discriminative fea-
tures in our documents, one idea is that incorpo-
rating knowledge about which features are asso-
ciated with which L1 could potentially help im-
prove the results. One approach to do this is the
use of asymmetric priors. We note that features as-
sociated with an L1 often dominate in a segment.
Accordingly, priors can represent evidence exter-
nal to the data that some some aspect should be
weighted more strongly: for us, this is evidence
from the NLI classification task. The segmenta-
tion models discussed so far only make use of a
symmetric prior but later work mentions that it
would be possible to modify this to use an asym-
metric prior (Eisenstein, 2009).

Given that priors are effective for incorporat-
ing external information, recent work has high-
lighted the importance of optimizing over such
priors, and in particular, the use of asymmetric pri-
ors. Key work on this is by Wallach et al. (2009)
on LDA, who report that “an asymmetric Dirich-
let prior over the document-topic distributions has
substantial advantages over a symmetric prior”,

1462



with prior values being determined through hyper-
parameter optimization. Such methods have since
been applied in other tasks such as sentiment anal-
ysis (Lin and He, 2009; Lin et al., 2012) to achieve
substantial improvements. For sentiment analysis,
Lin and He (2009) incorporate external informa-
tion from a subjectivity lexicon. In applying LDA,
instead of using a uniform Dirichlet prior for the
document–sentiment distribution, they use asym-
metric priors for positive and negative sentiment,
determined empirically.

For our task, we assign a prior to each of two
languages in a document, one corresponding to
L1a and the other to L1b. Given this, we can
assume that segments will alternate between L1a
and L1b. And instead of a single θ0, we have two
asymmetric priors that we call θa, θb correspond-
ing to L1a and L1b respectively. This will require
reworking the definition of pdcm in Equation 3.
First adapting Equation 2,

p(X | z, θa, θb) =
∏

{jo}
pdcm({xt : zt = jo} | θa) ·

∏

{je}
pdcm({xt : zt = je} | θb), (4)

with {jo} = {j | j mod 2 = 1, 1 ≤ j ≤ K} the
set of indices over odd segments and {je} = {j | j
mod 2 = 0, 1 ≤ j ≤ K} the set over evens. K is
the (usual) total number of segments. Then

pdcm({xt : zt = jo} | θa) =
Γ(

∑W
k θa[k])

Γ(Njo +
∑W

k θa[k])

W∏

i

Γ(nj,i + θa[i])

Γ(θa[i])
(5)

W is now more generally the number of items in
our vocabulary (whether words or POS n-grams).
A notational addition here is θa[k] which refers to
the L1a prior for the kth word or POS n-gram.
There is an analogous pdcm for θb.

The next issue is how to construct the θa and
θb. The simplest scenario would require a single
constant value for all elements in one L1 and an-
other for all elements in the other L1. Specifically,
using discrim(L1x) to denote “the ranked list of
discriminative n-grams for L1x”, we define

θa[i] =

{
c1 if θa[i] ∈ discrim(L1a)
c2 if θa[i] ∈ discrim(L1b)

and analogously for θb[i]. We would expect that
c1 > c2 (i.e. the prior is stronger for elements that

come from the appropriate ranked list of discrimi-
native features), but these values will be learned.

In principle we would calculate versions of
p(X | z, θa, θb) twice: once where we assign θa to
segment 1, and the second time where we assign
θb. We’d then compare the two p(X | z, θa, θb),
and see which one fits better. In this work, how-
ever, we will fix the initial L1: segment 1 corre-
sponds to L1a and consequently has prior θa.7

5 Results

5.1 Segmenting by Topic

We begin by testing the TOPICSEG model to en-
sure that the Bayesian segmentation methodol-
ogy can achieve reasonable results for segment-
ing learner essays by topic. The results on the
TOPICSEG-TOKENS dataset (Table 1) show that
content words are very effective at segmenting
the writings by topic, achieving Pk values in the
range 0.19–0.21. These values are similar to those
reported for segmenting Wall Street Journal text
(Beeferman et al., 1999). On the other hand, us-
ing the PTB POS tag version of the data in the
TOPICSEG-PTB dataset results in very poor seg-
mentation results, with Pk values around 0.45.
This is essentially the same as the performance of
degenerate algorithms (noted in §3.3) of 0.5. This
demonstrates that, as expected, POS unigrams do
not provide enough information for topic segmen-
tation; it is not possible to construct even an ap-
proximation to lexical chains using them.

5.2 L1-based Segmentation

Having verified that the Bayesian segmentation
approach is effective for topic segmentation on
this data, we now turn to the L1SEG model for
segmenting by the native language.

From the results in Table 1 we see very poor
performance with a Pk value of 0.466 for segment-
ing the texts in L1SEG-PTB-GI-DEV using the
unigrams as is. This was a somewhat unexpected
result given than we know POS unigram distribu-
tions are able to capture differences between L1-
groups (Malmasi and Cahill, 2015), albeit with
limited accuracy. Moreover, neither bigram nor
trigram encodings, which perform better at NLI,
resulted in any improvement in our results.

7This requires an extension of the BAYESSEG software
to support asymmetric priors. We will make this extended
version of the code available under the same conditions as
BAYESSEG. Please contact the first or second author for this.

1463



Model Dataset Prior(s) Pk WD
TOPICSEG TOPICSEG-TOKENS 0.1 0.203 0.205
TOPICSEG TOPICSEG-PTB 0.8 0.444 0.480
L1SEG L1SEG-PTB-GI-DEV unigrams 0.1 0.466 0.489
L1SEG L1SEG-PTB-GI-DEV bigrams 0.8 0.466 0.487
L1SEG L1SEG-PTB-GI-DEV trigrams 0.8 0.480 0.489
L1SEG-COMP L1SEG-PTB-GI-DEV bigrams 0.1 0.476 0.490
L1SEG-COMP L1SEG-PTB-GI-DEV trigrams 0.4 0.393 0.398
L1SEG-COMP L1SEG-CLAWS2-GI-DEV bigrams 0.4 0.387 0.400
L1SEG-COMP L1SEG-CLAWS2-GI-DEV trigrams 0.4 0.370 0.373
L1SEG-ASYMP L1SEG-CLAWS2-GI-DEV trigrams (0.6,0.3) 0.316 0.318

Table 1: Results on devsets for single L1 pair (German–Italian).

Model Pk WD
L1SEG-COMP 0.358 0.360
L1SEG-ASYMP 0.266 0.271

Table 2: Results on testset L1SEG-CLAWS2-GI-
TEST for single L1 pair (German–Italian). Priors
are the ones from the corresponding devsets in Ta-
ble 1.

Model Pk WD
L1SEG-COMP 0.365 (0.014) 0.369 (0.019)
L1SEG-ASYMP 0.299 (0.022) 0.312 (0.027)
L1SEG-COMP 0.376 (0.032) 0.381 (0.033)
L1SEG-ASYMP 0.314 (0.043) 0.319 (0.045)

Table 3: Results on dev and test datasets (upper:
L1SEG-CLAWS2-ALL-DEV, lower: L1SEG-
CLAWS2-ALL-TEST): means and standard de-
viations (in parentheses) across datasets for all 55
L1 pairs.

5.3 Incorporating Discriminative Features
Filtering the bigrams results in some minor im-
provements over the best results from the L1SEG
model. However, there are substantial improve-
ments when using the filtered POS trigrams, with
a Pk value of 0.393. We did not test unigrams as
they were the weakest NLI feature of the three.

This improvement is, we believe, because the
Bayesian modelling of lexical cohesion over the
input tokens requires that each segment concen-
trates its probability mass on a compact subset of
words. In the context of the n-gram tokenization
method tested in the previous section, the L1SEG
model with n-grams would most likely exacerbate
the issue by substantially increasing the number
of tokens in the language model: while the uni-
grams do not capture enough information to distin-
guish non-lexical shifts, the n-grams provide too

many features.
We also see that using the CLAWS2 tagset out-

performs the PTB tagset. The results achieved for
bigrams are much higher, while the trigram results
are also better, with Pk = 0.370. NLI experiments
using different POS tagsets have established that
more fine-grained tagsets (i.e. those with more tag
categories) provide greater classification accuracy
when used as n-gram features for classification.8

Results here comport with the previous findings.
As one of the two best models, we run it on the

held-out test data, using the best priors found from
the grid search on the devset data (Table 2); we
find the Pk and WD values are comparable (and in
fact slightly better), so the model still works if the
filtering uses discriminative NLI features from the
devset. Looking at results across all 55 L1 pairs
(Table 3), we also see similar mean Pk and WD
values with only a small standard deviation, indi-
cating the approach works just as well across all
language pairs. Priors here are all also weak, in
the range [0.1, 0.9].

In sum, the results here demonstrate the impor-
tance of inducing a compact distribution, which
we did here by reducing the vocabulary size by
stripping non-informative features.

5.4 Applying Two Asymmetric Priors

Our final model, L1SEG-ASYMP, assesses
whether setting different priors for each L1 can
improve performance. Our grid search over two
priors gives 900 possible prior combinations.
These combinations also include cases where θa
and θb are symmetric, which is equivalent to the
L1SEG-COMP model. We observe (Table 1) that

8In §2 we noted the comparison of PTB and CLAWS2
tagsets in Malmasi and Cahill (2015); also, Gyawali et al.
(2013) compared Penn Treebank and Universal POS tagsets
and found that the more fine-grained PTB ones did better.

1464



0
.1

0
.2

0
.3

0
.4

0
.5

0
.6

0
.7

0
.8

0
.9

1
.0

1
.1

1
.2

1
.3

1
.4

1
.5

1
.6

1
.7

1
.8

1
.9

2
.0

2
.1

2
.2

2
.3

2
.4

2
.5

2
.6

2
.7

2
.8

2
.9

3
.0

L1 B

0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2.0
2.1
2.2
2.3
2.4
2.5
2.6
2.7
2.8
2.9
3.0

L1
 A

Prior Grid Search Results (Coarse)

0.330

0.345

0.360

0.375

0.390

0.405

0.420

0.435

0.450

Figure 1: Heatmap over asymmetric priors on
L1SEG-CLAWS2-ALL-DEV

the prior pair of (0.6, 0.3) achieves a Pk value
of 0.321, a substantial improvement over the
previous best result of 0.370. Inspecting priors
(see Figure 1 for a heatmap over priors) shows
that the best results are in the region of weak
priors for both values, which is consistent with the
emphasis on compactness since weak priors result
in more compact models (noted by e.g. Wang and
Blei (2009)). Moreover, they are away from the
diagonal, i.e. the L1SEG-COMP model will not
produce the best results. A more fine-grained grid
search, focusing on the range that provided the
best results in the coarse search, can improve the
results further still: over the interval [0.3, 0.9],
partitioned into 60 evenly spaced values, finds a
prior pair of (0.64, 0.32) that provides a slight
improvement of the Pk value to 0.316.

As with L1SEG-COMP, we also evaluate this on
the same held-out test set (Table 2). Applying the
best asymmetric prior from the devset grid search,
this improves to 0.266. Again, results across all
55 L1 pairs (Table 3) show the same pattern, and
much as for L1SEG-COMP, priors are all weak
or neutral (range [0.1, 1.0]). These results thus
demonstrate that setting an asymmetric prior gives
the best performance on this task.

6 Conclusion and Future Work

Applying the approach to our two illustrative ap-
plications of §1, patchwriting and literary analysis,
would require development of relevant corpora. In
both cases the distinction would be between native
writing and writing that shows characteristics of a
non-native speaker, rather than between two non-

native L1s. There isn’t yet a topic-balanced cor-
pus like TOEFL11 which includes native speaker
writing for evaluation, although we expect (given
recent results on distinguishing native from non-
native text in Malmasi and Dras (2015)) that the
techniques should carry over. For the literary anal-
ysis, as well, to bridge the gap between work like
Morzinski (1994) and a computational applica-
tion, it remains to be seen how precise an anno-
tation is possible for this task. Additionally, the
granularity of segmentation may need to be finer
than sentence-level, as suggested by the examples
in §1; this level of granularity hasn’t previously
been tackled in unsupervised segmentation.

In terms of possible developments for the mod-
els presented for the task here, previous NLI work
has shown that other, syntactic features can be
useful for capturing L1-based differences. The
incorporation of these features for this segmen-
tation task could be a potentially fruitful avenue
for future work. We have taken a fairly straight-
forward approach which modifies the generative
story. A more sophisticated approach would be to
incorporate features into the unsupervised model.
One such example is the work of Berg-Kirkpatrick
et al. (2010) which demonstrates that each com-
ponent multinomial of a generative model can be
turned into a miniature logistic regression model
with the use of a modified EM algorithm. Their
results showed that the feature-enhanced unsu-
pervised models which incorporate linguistically-
motivated features achieve substantial improve-
ments for tasks such as POS induction and word
segmentation. We note also that the models are
potentially applicable to other stylistic segmenta-
tion tasks beyond L1 influence.

As far as this initial work is concerned we have
shown that, framed as a segmentation task, it is
possible to identify units of text that differ stylisti-
cally in their L1 influence. We demonstrated that
it is possible to define a generative story and asso-
ciated Bayesian models for stylistic segmentation,
and further that segmentation results improve sub-
stantially by compacting the n-gram distributions,
achieved by incorporating knowledge about dis-
criminative features extracted from NLI models.
Our best results come from a model that uses al-
ternating asymmetric priors for each L1, with the
priors selected using a grid search and then evalu-
ated on a held-out test set.

1465



Acknowledgements

The authors thank John Pate for very helpful dis-
cussions in the early stages of the paper, and the
three anonymous referees for useful suggestions.

A Details on Dataset Generation and
Evaluation

A.1 Document Generation

For our L1-varying datasets, we construct com-
posite documents to be segmented as alternating
segments drawn from TOEFL11 from two differ-
ent L1s. Broadly following a standard approach
(Brooke et al., 2012, for example), to generate
such a document, we randomly draw TOEFL11 es-
says — each of which constitutes a segment —
from the appropriate L1s and concatenate them,
alternating the L1 class after each segment. This is
repeated until the maximum number of segments
per document, s, is reached. We generate multi-
ple composite documents until all TOEFL11 have
been used. In this work we use datasets generated
with s = 5.9 We follow the same process for
our topic-varying datasets, but hold the L1 con-
stant while alternating the topic (prompt).

A.2 Encoding n-gram information

Lau et al. (2013) investigated the importance of
n-grams within topic models over lexical items.
They note that in topic modelling each token re-
ceives a topic label and that the words in a collo-
cation — e.g. stock market, White House or health
care — may receive different topic assignments
despite forming a single semantic unit. They
found that identifying collocations (via a t-test)
and preprocessing the text to turn these into sin-
gle tokens provides a notable improvement over a
unigram bag of words.

We implement a similar preprocessing step that
converts each sentence within each document to a
set of bigrams or trigrams using a sliding window,
where each n-gram is represented by a single to-
ken. So, for example, the trigram DT JJ NN be-
comes a single token: DT-JJ-NN.

A.3 Evaluation: Metric Definitions

Given two segmentations r (reference) and h (hy-
pothesis) for a corpus of N sentences,

9This is the average number of segments per chapter in the
written text used by Eisenstein and Barzilay (2008). How-
ever, we have also successfully replicated our results us-
ing s = 7, 9.

                          
           
                    
               
        
                             
                
                    
                          
         
           
                      
         
                     
                      
                                  
       
                         
                       
                      
                                       
                     
         

□ □ □ □ □ □ □ ■ ■ ■ ■ 
□ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ ■ ■ ■ ■ 
□ ■ ■ ■ □ □ □ ■ ■ ■ □ □ □ □ □ □ ■ ■ ■ 
□ □ □ □ □ □ ■ ■ ■ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ ■ ■ ■ 
□ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ 
□ □ □ □ □ □ □ ■ ■ ■ □ □ ■ ■ ■ □ □ □ ■ ■ ■ □ □ □ □ □ ■ ■ ■ ■ ■ ■ ■ ■ ■ 
■ ■ ■ □ □ □ □ ■ ■ ■ □ □ ■ ■ ■ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ 
□ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ 
□ □ □ □ □ ■ ■ ■ □ □ 
□ ■ ■ ■ □ ■ ■ ■ ■ ■ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ 
□ □ □ □ □ □ □ □ □ ■ ■ ■ ■ ■ ■ 
□ □ □ ■ ■ ■ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ ■ ■ ■ □ ■ ■ ■ ■ 
□ □ □ □ □ □ ■ ■ ■ □ 
□ □ □ □ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ ■ ■ ■ ■ □ ■ ■ ■ □ □ □ □ □ □ □ ■ ■ ■ 
□ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ ■ ■ ■ 
□ □ □ □ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ 
□ □ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ 
□ □ □ □ □ □ □ □ ■ ■ ■ □ □ 
□ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ 
■ ■ ■ □ □ □ □ □ ■ ■ ■ ■ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ □ ■ ■ ■ 
□ □ □ □ □ □ □ ■ ■ ■ 
                                                
       
                    
               
             
            
   
     
                 
     
                          
                  
           
     
       
            
        
                
             
            
            
           
          
                     
        
     
     
     
                       
                             
                 
                      
                                     
          
                      
                                  
                          
                             
                                         
                  
              

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

Figure 2: A visualization of sentences from a sin-
gle segment. Each row represents a sentence and
each token is represented by a square. Token tri-
grams considered discriminative for either of our
two L1 classes are shown in blue or red, with the
rest being considered non-discriminative.

PD(r, h) =
∑

1≤i≤j≤N
D(i, j)(δr(i, j) ⊕̄ δh(i, j)) (6)

where δr(i, j) is an indicator function specifying
whether i and j lie in the same reference segment,
δh(i, j) similarly for a hypothesised segment, ⊕̄ is
the XNOR function, and D is a distance probabil-
ity distribution over the set of possible distances
between sentences. For Pk, this D is defined by
a fixed window of size k which contains all the
probability mass, and k is set to be half the aver-
age reference segment length. The WD definition
is:

WD(r, h) =

1

N − k
N−k∑

i=1

(|b(ri, ri+k)− b(hi, hi+k)| > 0) (7)

where b(ri, rj) represents the number of bound-
aries between positions i and j in the reference
text (similarly, the hypothesis text).

A.4 Visualisation of Discriminative Features

Figure 2 shows a visualization of the discrimi-
native features of a single segment where each
row represents a sentence and each token is rep-
resented by a square. Tokens that are part of a
trigram which is considered discriminative for ei-
ther of our two L1 classes are shown in blue or
red. Note that discriminative n-grams can over-
lap with each other within the same class (e.g. on
lines 1 and 2 where two overlapping trigrams form
a group of four consecutive tokens) and also be-
tween two classes (e.g. on lines 10 and 11).

1466



References
Navot Akiva and Moshe Koppel. 2013. A Generic Un-

supervised Method for Decomposing Multi-Author
Documents. Journal of the American Society
for Information Science and Technology (JASIST)
64(11):2256–2264.

Khaled Aldebei, Xiangjian He, and Jie Yang. 2015.
Unsupervised decomposition of a multi-author doc-
ument based on naive-bayesian model. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 2: Short Papers). Association for
Computational Linguistics, Beijing, China, pages
501–505. http://www.aclweb.org/anthology/P15-
2082.

Calvin Bedient and Thomas Stearns Eliot. 1986. He
Do the Police in Different Voices: The Waste Land
and its protagonist. University of Chicago Press.

Doug Beeferman, Adam Berger, and John Laf-
ferty. 1999. Statistical Models for Text Seg-
mentation. Machine Learning 34(1-3):177–210.
https://doi.org/10.1023/A:1007506220214.

Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté,
John DeNero, and Dan Klein. 2010. Pain-
less unsupervised learning with features. In
Human Language Technologies: The 2010 An-
nual Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics. Association for Computational Linguis-
tics, Los Angeles, California, pages 582–590.
http://www.aclweb.org/anthology/N10-1083.

Daniel Blanchard, Joel Tetreault, Derrick Hig-
gins, Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A Corpus of Non-Native English. Tech-
nical report, Educational Testing Service.

David M. Blei. 2012. Probabilistic topic models. Com-
munications of the ACM 55(4):77–84.

Julian Brooke, Adam Hammond, and Graeme
Hirst. 2012. Unsupervised Stylistic Segmenta-
tion of Poetry with Change Curves and Extrin-
sic Features. In Proceedings of the NAACL-
HLT 2012 Workshop on Computational Linguis-
tics for Literature. Association for Computa-
tional Linguistics, Montréal, Canada, pages 26–35.
http://www.aclweb.org/anthology/W12-2504.

Julian Brooke, Graeme Hirst, and Adam Hammond.
2013. Clustering voices in the waste land. In Pro-
ceedings of the Workshop on Computational Lin-
guistics for Literature. Association for Computa-
tional Linguistics, Atlanta, Georgia, pages 41–46.
http://www.aclweb.org/anthology/W13-1406.

Serhiy Bykh and Detmar Meurers. 2014. Exploring
Syntactic Features for Native Language Identifica-
tion: A Variationist Perspective on Feature Encoding

and Ensemble Optimization. Proceedings of COL-
ING 2014, the 25th International Conference on
Computational Linguistics: Technical Papers pages
1962–1973.

John Xiros Cooper. 1987. TS Eliot and the politics of
voice: The argument of The Waste Land. 79. UMI
Research Press.

Lan Du, Wray Buntine, and Mark Johnson. 2013.
Topic segmentation with a structured topic model.
In Proceedings of the 2013 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies. Association for Computational
Linguistics, Atlanta, Georgia, pages 190–200.
http://www.aclweb.org/anthology/N13-1019.

Jacob Eisenstein. 2009. Hierarchical text segmentation
from multi-scale lexical cohesion. In Proceedings of
the North American Chapter of the Association for
Computational Linguistics (NAACL). Boulder, CO,
pages 353–361. www.aclweb.org/anthology/N09-
1040.

Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing. Association for Compu-
tational Linguistics, Honolulu, Hawaii, pages 334–
343. http://www.aclweb.org/anthology/D08-1035.

Rod Ellis. 2008. The Study of Second Language Ac-
quisition, 2nd edition. Oxford University Press, Ox-
ford, UK.

Joanna Gilmore, Denise Strickland, Briana Timmer-
man, Michelle Maher, and David Feldon. 2010.
Weeds in the flower garden: An exploration of pla-
giarism in graduate students research proposals and
its connection to enculturation, ESL, and contextual
factors. International Journal for Educational In-
tegrity 6(1):13–28.

Binod Gyawali, Gabriela Ramirez, and Thamar
Solorio. 2013. Native Language Identification: a
Simple n-gram Based Approach. In Proceedings of
the Eighth Workshop on Innovative Use of NLP for
Building Educational Applications. Association for
Computational Linguistics, Atlanta, Georgia, pages
224–231. http://www.aclweb.org/anthology/W13-
1729.

M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman Publishing Group.

Marti A. Hearst. 1994. Multi-paragraph segmentation
of expository text. In Proceedings of the 32nd An-
nual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics, Las Cruces, New Mexico, USA, pages 9–16.
https://doi.org/10.3115/981732.981734.

Marti A. Hearst. 1997. Texttiling: Segment-
ing text into multi-paragraph subtopic pas-
sages. Computational Lingustics 23(1):33–64.
http://www.aclweb.org/anthology/J97-1003.

1467



Radu Tudor Ionescu, Marius Popescu, and Aoife
Cahill. 2014. Can characters reveal your native lan-
guage? A language-independent approach to native
language identification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Association for Com-
putational Linguistics, Doha, Qatar, pages 1363–
1373. http://www.aclweb.org/anthology/D14-1142.

Sandra Jamieson and Rebecca Moore Howard. 2013.
Sentence-Mining: Uncovering the Amount Of
Reading and Reading Comprehension In College
Writers’ Researched Writing. In Randall Mc-
Clure and James P. Purdy, editors, The New Digi-
tal Scholar: Exploring and Enriching the Research
and Writing Practices of NextGen Students, Amer-
ican Society for Information Science and Technol-
ogy, Medford, NJ, pages 111–133.

Minwoo Jeong and Ivan Titov. 2010. Unsupervised
discourse segmentation of documents with inher-
ently parallel structure. In Proceedings of the
ACL 2010 Conference Short Papers. Association for
Computational Linguistics, Uppsala, Sweden, pages
151–155. http://www.aclweb.org/anthology/P10-
2028.

Casey Keck. 2006. The use of paraphrase in summary
writing: A comparison of L1 and L2 writers. Jour-
nal of Second Language Writing 15(4):261–278.

Moshe Koppel, Navot Akiva, Idan Dershowitz, and
Nachum Dershowitz. 2011. Unsupervised decom-
position of a document into authorial components.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Portland, Oregon, USA, pages
1356–1364. http://www.aclweb.org/anthology/P11-
1136.

Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Automatically determining an anonymous au-
thor’s native language. In Intelligence and Secu-
rity Informatics. Springer-Verlag, volume 3495 of
LNCS, pages 209–217.

Jey Han Lau, Timothy Baldwin, and David Newman.
2013. On Collocations and Topic Models. ACM
Transactions on Speech and Language Processing
(TSLP) 10(3).

Yongyan Li and Christine Pearson Casanave. 2012.
Two first-year students strategies for writing from
sources: Patchwriting or plagiarism? Journal of
Second Language Writing 21:165–180.

Chenghua Lin and Yulan He. 2009. Joint sen-
timent/topic model for sentiment analysis. In
Proceedings of the 18th ACM Conference
on Information and Knowledge Management.
ACM, New York, NY, USA, pages 375–384.
http://doi.acm.org/10.1145/1645953.1646003.

Chenghua Lin, Yulan He, Richard Everson, and Stefan
Rüger. 2012. Weakly supervised joint sentiment-
topic detection from text. Knowledge and Data En-
gineering, IEEE Transactions on 24(6):1134–1145.

Igor Malioutov and Regina Barzilay. 2006. Min-
imum cut model for spoken lecture segmenta-
tion. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Com-
putational Linguistics. Association for Computa-
tional Linguistics, Sydney, Australia, pages 25–32.
https://doi.org/10.3115/1220175.1220179.

Shervin Malmasi and Aoife Cahill. 2015. Mea-
suring feature diversity in native language iden-
tification. In Proceedings of the Tenth Work-
shop on Innovative Use of NLP for Building Ed-
ucational Applications. Association for Computa-
tional Linguistics, Denver, Colorado, pages 49–55.
http://www.aclweb.org/anthology/W15-0606.

Shervin Malmasi and Mark Dras. 2014. Lan-
guage Transfer Hypotheses with Linear SVM
Weights. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computa-
tional Linguistics, Doha, Qatar, pages 1385–1390.
http://aclweb.org/anthology/D14-1144.

Shervin Malmasi and Mark Dras. 2015.
Multilingual Native Language Identifi-
cation. Natural Language Engineering
https://doi.org/10.1017/S1351324915000406.

Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational linguistics
17(1):21–48.

Mary Morzinski. 1994. The Linguistic influence of Pol-
ish on Joseph Conrad’s style. Columbia University
Press, New York, NY.

Diane Pecorari. 2003. Good and original: Plagia-
rism and patchwriting in academic second-language
writing. Journal of Second Language Writing
12(4):317–345.

Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text seg-
mentation. Computational Linguistics 28(1):19–36.
https://doi.org/10.1162/089120102317341756.

Ben Swanson and Eugene Charniak. 2013. Extract-
ing the Native Language Signal for Second Lan-
guage Acquisition. In Proceedings of the 2013 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies. Association for Computa-
tional Linguistics, Atlanta, Georgia, pages 85–94.
http://www.aclweb.org/anthology/N13-1009.

Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth

1468



Workshop on Innovative Use of NLP for Building
Educational Applications. Association for Compu-
tational Linguistics, Atlanta, Georgia, pages 48–57.
http://www.aclweb.org/anthology/W13-1706.

Joel Tetreault, Daniel Blanchard, Aoife Cahill, and
Martin Chodorow. 2012. Native tongues, lost
and found: Resources and empirical evaluations
in native language identification. In Proceedings
of COLING 2012. The COLING 2012 Organiz-
ing Committee, Mumbai, India, pages 2585–2602.
http://www.aclweb.org/anthology/C12-1158.

Michelle Vieyra, Denise Strickland, and Brianna Tim-
merman. 2013. Patterns in plagiarism and patch-
writing in science and engineering graduate stu-
dents’ research proposals. International Journal for
Educational Integrity 9(1):35–49.

Hanna M. Wallach, David M. Mimno, and Andrew Mc-
Callum. 2009. Rethinking lda: Why priors matter.
In Y. Bengio, D. Schuurmans, J.D. Lafferty, C.K.I.
Williams, and A. Culotta, editors, Advances in Neu-
ral Information Processing Systems 22, Curran As-
sociates, Inc., pages 1973–1981.

Chong Wang and David M Blei. 2009. Decoupling
sparsity and smoothness in the discrete hierarchical
dirichlet process. In Advances in Neural Informa-
tion Processing Systems. pages 1982–1989.

Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning. Association for Computa-
tional Linguistics, Jeju Island, Korea, pages 699–
709. http://www.aclweb.org/anthology/D12-1064.

1469


	Unsupervised Text Segmentation Based on Native Language Characteristics

