






















Active learning for sense annotation

Héctor Martı́nez Alonso Barbara Plank Anders Johannsen Anders Søgaard
Njalsgade 140, Copenhagen (Denmark), University of Copenhagen

alonso@hum.ku.dk, bplank@cst.dk, {ajohannsen, soegaard}@hum.ku.dk

Abstract
This article describes a real (non-
synthetic) active-learning experiment to
obtain supersense annotations for Dan-
ish. We compare two instance selection
strategies, namely lowest-prediction
confidence (MAX), and sampling from
the confidence distribution (SAMPLE).
We evaluate their performance during the
annotation process, across domains for the
final resulting system, as well as against
in-domain adjudicated data. The SAMPLE
strategy yields competitive models that are
more robust than the overly length-biased
selection criterion of MAX.

1 Introduction

Most successful natural language processing
(NLP) systems rely on a set of labeled training ex-
amples to induce models in a supervised manner.
However, labeling instances to create a training set
is time-consuming and expensive. One way to al-
leviate this problem is to resort to active learning
(AL), where a learner chooses which instances—
from a large pool of unlabeled data—to give to the
human expert for annotation. After each annota-
tion by the expert, the system retrains the learner,
and the learner chooses a new instance to annotate.

There are many active learning strategies. The
simplest and most widely used is uncertainty
sampling (Lewis and Catlett, 1994), where the
learner queries the instance it is most uncertain
about (Scheffer and Wrobel, 2001; Culotta and
McCallum, 2005). Instead, in query-by-committee
an entire committee of models is used to select the
examples with highest disagreement. At the same
time most studies on active learning are actually
synthetic, i.e. the human supervision was just em-
ulated by holding out already labeled data.

In this study, we perform a real active learn-
ing experiment. Since speed plays a major role,

we do not resort to an ensemble-based query-by-
committee approach but use a single model for se-
lection. We evaluate two selection strategies for a
sequence tagging task, supersense tagging.

2 Datapoint-selection strategies

Given a pool of unlabeled data U , a datapoint-
selection strategy chooses a new unlabeled item
ui to annotate. We evaluate two of such strategies.
They both involve evaluating the informativeness
of unlabeled instances.

The first strategy (MAX) is similar to the stan-
dard approach in uncertainty sampling, i.e. the ac-
tive learning system selects datapoint whose clas-
sification confidence is the lowest. The second
strategy (SAMPLE) attempts to make the selection
criterion more flexible by sampling from the con-
fidence score distribution.

The two strategies work as follows:

1. MAX: Predict on U and choose ui that has
the lowest prediction confidence pi, where pi
is the posterior probability of the classifier for
the item ui.

2. SAMPLE: Predict on U and choose ui sam-
pling from the distribution of the inverse con-
fidence scores for all the instances—making
low-confidence items more likely to be sam-
pled. We calculate the inverse confidence
score as −log(pi).

We apply both datapoint-selection strategies
on two different subcorpora sampled from the
same unlabeled corpus (cf. Section 3). Each
(strategy,subcorpus) tuple yields a system setup
for an individual annotator. Table 1 describes the
setup of our four annotators.

3 Data collection

An AL setup requires some annotated data to use
as training seed for the first model, and as evalua-

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 245



Annotator Strategy Subcorpus

AS1 SAMPLE C1
AS2 SAMPLE C2
AM1 MAX C1
AM2 MAX C2

Table 1: Annotators and their setup, namely their
instance selection strategy and the unlabeled sub-
corpus.

Domain SL Seed Test

Blog 16.44 100
Chat 14.61 200
Forum 20.51 200
Magazine 19.45 200
Newswire 17.43 400 200
Parliament 31.21 200

Table 2: Super-sense tagging data sets

tion test bench. We use previously available Dan-
ish sense-annotated data (Martı́nez Alonso et al.,
2015). This dataset is a subset of the the ClarinDK
corpus (Asmussen and Halskov, 2012) and has
been annotated by two annotators and later adju-
dicated by a third. Table 2 shows the different do-
mains that make up the initial annotated data and
how much is used for training seed and for testing.
We choose a conventional scenario where the ini-
tial system is trained only on an usual kind of text
(newswire) in order to later assess the system’s im-
provement on out-of-domain data.

In addition to the labeled data used for training
seed and for testing, we use two unlabeled 10K-
sentence subcorpora. These two subcorpora (C1
and C2) are randomly sampled from the ClarinDK
corpus in order to obtain sentences of the same
type that make up the labeled data, but ensuring
that the sentences in C1 and C2 do not overlap with
any of the labeled sentences described in Table 2.
All the sentences in C1 and C2 are between 5 and
50 words long, in order to limit the strong bias for
selecting longer sentences in AL for sequence pre-
diction.1

4 Model

The features used in the model are the following.
For each word w, we calculate:

1The data will be made available at clarin.dk under
Danish Supersense Annotation.

a) 2-TOKEN WINDOW of forms, lemmas and
POS tags before and after w, including w.

b) 2-TOKEN WINDOW of most-frequent sense
tags for w.

c) BAG OF WORDS of forms and lemmas at the
left and right of w, marked for directionality
so words at the left are different from words
at the right.

d) MORPHOLOGY of w, whether it is all al-
phanumeric, capitalized, contains hyphens,
and its 3-letter prefix and suffix.

e) BROWN CLUSTER estimated from U . We
generate the 2,4,6,8,10 and 12-bits long pre-
fix of the cluster bitstring of w.2

The system is trained using search-based classi-
fication (SEARN) (Daumé et al., 2009)3 using de-
fault parameters and one pass over the data. We
use one pass over the data, thereby using strict on-
line learning.

5 Evaluation

The goal of an AL setup is to augment the set
of training instances. In this section we evaluate
the performance of the AL-generated annotations
during the annotation process in form of learning
curves (Section 5.1). In order to gauge the robust-
ness of a system trained on data obtained from AL,
we break the evaluation down by domain (Section
5.2). Finally, we compare a system trained exclu-
sively on annotated and ajudicated newswire data
with systems trained on a combination of train-
ing seed and AL data. We evaluate all systems
on micro-averaged F1.

5.1 Learning curves
This section describes the life-cycle for the AL
setup for the four annotators. The system does one
pass over the data and then retrains after each item.
We evaluate against the entire test data that com-
prises different domains (Section 5.2).

We delimit the learnability space of the task
between the most-frequent-sense (MFS) baseline
at the bottom and an estimate of an upper bound
(UB). We approximate the UB by training a sys-
tem on the seed plus all the four AL-annotated
datasets. Note that the data for UB is four times
the data at the end point of any learning curve.

2We use Liang’s implementation https://github.
com/percyliang/brown-cluster

3SEARN in Vowpal Wabbit https://github.com/
JohnLangford/vowpal_wabbit/

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 246



0 100 200 300 400

34
36

38
40

42
44

46

instances

F1

S1
S2
M1

M2
MFS
UB

Figure 1: learning curves for the four annotators,
delimited by the MFS baseline and the estimated
upper bound (UB).

We observe that the MFS baseline (dotted line)
is fairly low (33.63 F1). The system performance
trained on seed starts at 37.06 F1. All learn-
ing curves show the same overall behavior with
steeper learning for the first 150 instances. The
differences are small, yet we can see that the SAM-
PLE approach surpasses the MAX strategy after
110 instances for one of the sub corpora. The
most informative data during the initial iterations
stems from annotator AM1 (MAX strategy), where
we can observe the steepest increase. However,
the MAX strategy results in considerably longer
sentences (cf. SL in Table 4), thus is a major bur-
den for the annotators compared to SAMPLE. In
fact, AM1 could not finish the task in time, which
is depicted by the straight line for the last 20 dots
in the plot.

5.2 Performance across domains

The SAMPLE strategy turns out to be promising
when evaluated on the entire test set, but only for
one of the two subcorpora (Section 5.1). In this
section, we look at the performance per domain.

Table 3 shows the domain-wise results for the
last AL iteration of each annotator. We compare
this to the most-frequent sense (MFS) baseline, as
well to the original performance trained on only
the seed data. The values for the annotators in Ta-
ble 3 correspond with the last point of each learn-

ing curve in Figure 1, whereas the seed baseline
is the common starting point (the intercept) for all
learning curves.

We can see now that the seed baseline already
beats MFS on all datasets except Chat, which is ar-
guably very different from the newswire text that
seed is sampled from. Overall, there is only a
small difference in terms of performance between
the two datapoint-selection strategies. The best
strategy varies per domain.

MAX SAMPLE
Dataset MFS Seed +AM1 +AM2 +AS1 +AS2
Blog 25.6 37.2 40.5 41.0 39.1 39.4
Chat 36.1 33.7 39.8 39.3 40.1 40.3
Forum 31.1 33.4 36.1 35.9 35.2 35.5
Magazine 34.3 36.3 38.5 37.4 38.6 36.8
Newswire 31.5 37.2 42.9 40.5 41.2 39.0
Parliament 38.6 40.5 45.0 47.2 46.5 47.5
All 33.6 36.8 40.8 40.7 40.6 40.1

Table 3: Performance across domains for different
training setups. The best system for each dataset
is given in bold, and the worst system in italics.

Annotator SL KLSeed−A KLA−Test

AM1 43.36 0.026 0.027
AM2 42.65 0.019 0.027
AS1 26.16 0.035 0.054
AS2 26.24 0.017 0.022

Table 4: Descriptive statistics for the four gener-
ated datasets.

There are notable differences in terms of data
characteristics between the resulting data samples.
Table 4 shows descriptive statistics for the four re-
sulting datasets, and how they relate to the seed
training dataset and the overall test data. SL repre-
sents the average sentence length, KLSeed−A is the
Kullback-Leibler (KL) divergence from the seed
dataset to each of the annotators’, and KLA−Test is
the KL divergence from the annotators’ datasets
to the test data. We can see that there is a big dif-
ference in SL between the strategies. The system
that use the SAMPLE strategy have a more variable
sentence length but also much shorter sentences
than MAX. It is still longer than the average sen-
tence length in the unlabeled corpus, which is 21
tokens per sentence. This indicates that the se-
lection strategy for SAMPLE is a compromise be-
tween the long-sentence bias of the MAX strategy
and a purely random selection. Thus, we believe
that the SAMPLE strategy is a viable alternative to
the more common lowest-confidence strategy, be-

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 247



cause it can provide training data of competitive
quality that is more varied and can provide more
robust models.

5.3 Comparison with heldout data
In this last comparison, we gauge the effect of
using adjudicated in-domain data instead of the
same amount of AL-generated data. We remove
the 200 newswire sentences from the test set and
add them to the training seed. We train a sys-
tem on the seed and these additional 200 adjudi-
cated sentences (namely on all the 600 sentences
of newswire data), and evaluate it across domains
(all out-domain data). This system is compared
to the result of the AL setup at the 200th itera-
tion. The results in Table 5 show that even across
domains it is more beneficial to have adjudicated
in-domain data then the out-of-domain data anno-
tated through active learning.

MAX SAMPLE
Dataset Seed +Newswire +AM1 +AM2 +AS1 +AS2
Blog 33.7 39.9 39.1 39.0 39.0 38.5
Chat 33.4 41.9 38.6 37.5 38.2 36.8
Forum 36.3 39.2 35.8 35.3 35.2 35.4
Magazine 40.5 42.8 39.7 39.4 40.1 39.4
Parliament 36.8 48.2 43.5 47.3 46.5 46.3
All 33.9 43.2 39.7 40.3 40.4 39.8

Table 5: Cross-domain performance against held-
out newswire data

The causes for the better performance of the
+Newswire system are twofold. First, there is less
noise in the data because of the two-round pro-
cess of annotation and adjudication; and second,
the bias of this system’s annotation is the same as
in the evaluation data. Note that the 200-instance
data point is past the 150-instance convergence
point for the learning curves in Figure 1.

6 Related Work

The AL models considered here are very standard.
We take a small seed of data points, train a se-
quential labeling, and iterate over an unlabeled
pool of data, selecting the data points our labeler
is least confident about. In the AL literature, the
selected data points are often those close to a de-
cision boundary or those most likely to decrease
overall uncertainty. This obviously leads to bi-
ased sampling, which can sometimes be avoided
using different techniques, e.g., by exploiting clus-
ter structure in the data.

Generally, active learning for sequential label-
ing has received less attention than for classifica-

tion (Settles and Craven, 2008; Marcheggiani and
Artieres, 2014). Our experiments were simple,
and several things can be done to improve results,
i.e., by reducing sampling bias. In particular, sev-
eral techniques have been introduced for improv-
ing out-of-domain performance using active learn-
ing. Rai et al. (2010) perform target-domain AL
with a seed of source-domain data. Among other
things, they propose to use the source and target
unlabeled data to train a classifier to learn what
target domain data points are similar to the source
domain, in a way similar to Plank et al. (2014).
For more work along these lines, see Chan and
Ng (2007) and Xiao and Guo (2013).

7 Conclusions

The systems that use the MAX selection strategy
have a strong bias for the longest possible sen-
tence, resulting from the low probability values
obtained when calculating the prediction confi-
dence of very long sequences. With few excep-
tions (e.g. an 11-word sentence on the 5th iteration
for AM1), the systems exhaust the maximum-length
sentences, and proceed to choose the longest avail-
able, and so forth.

We do not take our individual annotator’s bias
into consideration, but we believe that such bias
plays a minor role in the differences of perfor-
mance between MAX and SAMPLE. For instance,
AS2 is the only annotator that was directly involved
in the creation of the annotated seed and test data,
but has arguably the worst-faring system on over-
all F1, that is, regardless of how good (i.e. how
similar in bias to the test data) an annotator is,
the selection strategy is the main factor for the im-
provement rate of the system during active learn-
ing. Note however that the data annotated by AS2
does indeed have the lowest KL divergences to the
seed and test data in Table 4.

Using SAMPLE lowers the annotator time per
sentence because sentences do get shorter, even
though the performance is initially lower until
the 150-instance convergence point. We propose
that with the same amount of time an annotator
can annotate more, shorter sentences for the same
amount of words and obtain more varied annota-
tions that yield more robust models. Very long
sentences do not bring a major advantage, and this
justifies sampling when it is necessary to strike a
compromise between annotation time and model
robustness.

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 248



References
Jørg Asmussen and Jakob Halskov. 2012. The clarin

dk reference corpus. In Sprogteknologisk Workshop.

Yee Seng Chan and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In ACL.

Aron Culotta and Andrew McCallum. 2005. Reduc-
ing labeling effort for structured prediction tasks. In
AAAI.

Hal Daumé, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine learn-
ing, 75(3):297–325.

David D Lewis and Jason Catlett. 1994. Heteroge-
nous uncertainty sampling for supervised learning.
In ICML.

Diego Marcheggiani and Thierry Artieres. 2014. An
experimental comparison of active learning strate-
gies for partially labeled sequences. In EMNLP.

Héctor Martı́nez Alonso, Anders Johannsen, An-
ders Søgaard, Sussi Olsen, Anna Braasch, Sanni
Nimb, Nicolai Hartvig Sørensen, and Bolette Sand-
ford Pedersen. 2015. Supersense tagging for danish.
In Nodalida.

Barbara Plank, Anders Johannsen, and Anders
Søgaard. 2014. Importance weighting and unsuper-
vised domain adaptation of POS taggers: a negative
result. In EMNLP.

Piyush Rai, Avishek Saha, Hal Daume, and Suresh
Venkatasubramanian. 2010. Domain adaptation
meets active learning. In Workshop on Active Learn-
ing for NLP, NAACL.

Tobias Scheffer and Stefan Wrobel. 2001. Active
learning of partially hidden markov models. In In
Proceedings of the ECML/PKDD Workshop on In-
stance Selection.

Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In EMNLP.

Min Xiao and Yuhong Guo. 2013. Online active learn-
ing for cost sensitive domain adaptation. In CoNLL.

Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 249


