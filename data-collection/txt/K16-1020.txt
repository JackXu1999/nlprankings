



















































Beyond Prefix-Based Interactive Translation Prediction


Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 198–207,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Beyond Prefix-Based Interactive Translation Prediction

Jesús González-Rubio
Daniel Ortiz-Martı́nez

Webinterpret Inc.
{jesus.g,daniel.o}@webinterpret.com

José Miguel Benedı́
Francisco Casacuberta
PRHLT Research Center

Univ. Politècnica de València
{jbenedi,fcn}@prhlt.upv.es

Abstract

Current automatic machine translation
systems require heavy human proof-
reading to produce high-quality transla-
tions. We present a new interactive ma-
chine translation approach aimed at pro-
viding a natural collaboration between hu-
mans and translation systems. As such, we
grant the user complete freedom to vali-
date and correct any part of the translations
suggested by the system. Our approach
is then designed according to the require-
ments placed by this unrestricted proof-
reading protocol. In particular, the ability
of the system to suggest new translations
coherent with the set of potentially disjoint
translation segments validated by the user.

We evaluate our approach in a user-
simulated setting where reference transla-
tions are considered the output desired by
a human expert. Results show important
reductions in the number of edits in com-
parison to decoupled post-editing and con-
ventional prefix-based interactive transla-
tion prediction. Additionally, we provide
evidence that it can also reduce the cogni-
tive overload reported for interactive trans-
lation systems in previous user studies.

1 Introduction

Research in the field of machine translation (MT)
aims at developing computer systems that re-
duce the effort required to generate translations,
whether by assisting human translators or by di-
rectly replacing them. However, most research in
MT has focused on the development of fully auto-
matic MT approaches. Despite that, except for a
handful of very constrained domains, current au-
tomatic MT technology still only achieves results

that are not satisfactory in practice; automatic MT
still require heavy human proof-reading to pro-
duce human-quality translations.

We present a new computer-assisted translation
approach that integrates human translators and au-
tomatic MT into a tight feedback loop. In our ap-
proach, the user1 and the MT system collaborate
to generate translations through a series of inter-
actions. At each interaction, the system proposes
its best translation for the given input sentence. If
the user finds it correct, then it is accepted and the
process goes on with the next input sentence. Oth-
erwise, the user makes some corrections that the
system takes into account to improve the proposed
translation. The rationale behind this interactive
translation prediction (ITP) approach is to com-
bine the accuracy provided by the human expert
with the efficiency of the MT system in contrast
to decoupled post-editing (PE). Previous works,
e.g. (Barrachina et al., 2009), have explored this
paradigm; however their practical implementation
limits this general proof-reading approach to a
prefix-based interaction where the user is forced to
correct the errors in the sentence strictly according
to the reading order.

Our main contribution, described in Section 3,
is a new proof-reading protocol focused on pro-
viding a more natural interaction between the user
and the system. Specifically, we give complete
freedom to the user to validate or to correct any
part of the translation at any given interaction. As
such, the user is no longer bound to correct the
errors following the reading order as in previous
prefix-based ITP works (Barrachina et al., 2009;
González-Rubio et al., 2013; Green et al., 2014).
Preffix-based interaction can be a frustrating and
cognitively demanding limitation for the user, and
may be a factor in the somehow disappointing

1We use the terms “human expert”, “human translator”,
and ‘user” indistinctly.

198



results of prefix-based ITP with users (Koehn,
2009; Underwood et al., 2014; Green et al., 2014;
Sanchis-Trilles et al., 2014). We design our ap-
proach to meet the requirements placed by the un-
restricted proof-reading protocol, not the opposite
way. The most significant new feature is condi-
tioned decoding, for translation generation coher-
ently to a set of segments validated by the user.

An evaluation involving human users is most
desirable to study the impact of any proof-reading
protocol. However, such a study is expensive,
time-consuming and it will require to take into
account additional sources of variation, namely
the human factor, that may obscure the compari-
son between different approaches. Therefore, we
chose to follow previous works, for instance (Bar-
rachina et al., 2009), and carry out our experi-
ments on a simulated setting intended to provide
a direct and, more importantly, objective compar-
ison to previous approaches (Section 4). Regard-
ing evaluation, we propose a new metric to auto-
matically estimate the cognitive load of potential
users working on the different ITP environments.
To the best of our knowledge, this is the first pro-
posal at this respect. Results in Section 5 confirm
the soundness of the proposed ITP approach. Re-
ported figures show important reductions in both
the number of corrections typed by the user and
her estimated cognitive load.

2 Related Work

Common proof-read MT protocols implement a
decoupled PE process in which, first, the MT sys-
tem returns a translation of a whole given docu-
ment. Next, a human reads it correcting, in any
order, the possible mistakes made by the system.

Interactive approaches (Isabelle and Church,
1998; Langlais and Lapalme, 2002; Tomás and
Casacuberta, 2006) were proposed as a more so-
phisticate way of taking advantage of MT technol-
ogy. Barrachina et al., (Barrachina et al., 2009)
presented a prefix-based ITP approach in which
the user is assumed to proof-read each automatic
translation correcting each time the first error, if
any, in the usual reading order. This can be a
reasonable assumption in text or speech transcrip-
tion (Toselli et al., 2007; Rodrı́guez et al., 2007)
where the output sequence is generated monoton-
ically respect to the input data. However, it has al-
ways be an important handicap for translation due
to the intrinsic reordering involved in the process.

ITP is a fruitful research field with diverse con-
tributions for multiple authors: (González-Rubio
et al., 2010; Alabau et al., 2013; Koehn et al.,
2014) among others. We share with (Sanchis-
Trilles et al., 2008) the idea of making a more
sophisticated use of the mouse actions performed
by the user while interacting with the system, and
with (González-Rubio et al., 2013) the common
ITP formulation for both phrase-based and hierar-
chical MT models. In particular, we significantly
modify the prefix-based ITP implementation pre-
sented in the latter work to support the proposed
unrestricted proof-reading protocol.

User studies of prefix-based ITP versus PE
have shown that while users tend to make less
corrections, overall translation time tend to be
higher (Koehn, 2009; Underwood et al., 2014;
Green et al., 2014; Sanchis-Trilles et al., 2014).
Coherently with these results, users also perceive
prefix-based ITP as a more cognitive demanding
task than PE. This is not surprising given that
users are asked to proof-read one new translation
(suffix) after each individual correction, which in-
creases significantly the amount of text to be pro-
cessed to generate a single translation. This is par-
ticularly frustrating when the user observes how a
correct translation is rewritten with a wrong one
by the next suffix suggested by the system. Given
that PE do not suffer from this effect, it provides
a comprehensive explanation of the somehow dis-
appointing results reported for prefix-based ITP.

To the best of our knowledge, the only alterna-
tive to prefix-based proof-reading was proposed
in the context of text recognition. Serrano et
al., (2014) implement a constrained search proce-
dure that profits from the monotonic alignment be-
tween input image, search states and user correc-
tions, to limit the set of possible transcriptions to
those coherent with a set of (disjoint) user correc-
tions. We apply a similar idea in a translation con-
text and provide solutions to cope with the non-
monotonicity inherent to the task.

3 Beyond Prefix-Based ITP

The goal of our approach is to give complete free-
dom to the user in her interaction with the system.
The process starts when the MT system proposes
a full translation of the source language sentence.
Then, the user reads the translation and is allowed
to validate -all or part of- the correct segments in it
and corrects any of its potential errors. Then, the

199



source (s): No era el hombre más honesto ni el más piadoso , pero era un hombre valiente .
desired translation (t): He was not the most honest or pious of men , but he was courageous .

BEGIN
{

MT : It was not the most honest and the most pious man , but it was a brave man .

IT-1
{

User: It was not the most honest and the most pious of man , but it was a brave man .
MT : He was not the most honest or pious of men , but it was a brave man .

IT-2
{

User: He was not the most honest or pious of men , but it was courageous .
MT : He was not the most honest or pious of men , but he was courageous .

END
{

User: He was not the most honest or pious of men , but he was courageous .

Figure 1: Interactive translation of a Spanish sentence into English. First, the system suggests an initial
translation. At iteration 1, the user validates the parts of the suggestions she considers to be right and
introduces a correction by typing a word: “ of ”. This defines a new user feedback with five segments:
{“was not the most honest”, “pious of”, “, but”, “was”, “.”}. Then, the system suggests a new translation
that contains these segments in the given order. Iteration 2 is similar; the user validates words “He”
and “or”, and she types a new correction: “ courageous ”. The process ends when the user accepts the
translation suggested by the system in the last step. Only two edits are required. In comparison, PE
would have needed 10 edits.

system takes into account this feedback to suggest
a new translation that contains the segments vali-
dated by the user as well as the typed corrections.
Such process is repeated until the user validates
the whole suggested translation. An example of
this process is shown in Figure 1.

The crucial MT feature is the generation of a
new translation coherent to the segments already
validated by the user. Formally, we represent such
user feedback as a sequence of disjoint segments
f = f̃1, . . . , f̃k, . . . , f̃|f |, where each f̃k is a se-
quence of consecutive target language words. For
example, user feedback at iteration one in Figure 1
is composed of five disjoint segments: f̃1 = “was
not the most honest”, f̃2 = “pious of”, f̃3 = “, but”,
f̃4 = “was” and f̃5 = “.”. Segments in f do not
overlap and do not necessarily cover the whole
sentence. Prefix-based feedback in conventional
ITP is a special case of this with only one segment
starting at the beginning of the sentence.

Next, we describe the statistical formalization
of our approach, the models actually used to im-
plement such formalization, and the search proce-
dures required to efficiently generate translations
coherent with this generalized user feedback.

3.1 Statistical Framework

Our problem can be stated as follows: given a
source sentence s = s1 . . . s|s| and some user feed-
back f , we must find the best target language trans-

lation t = t1 . . . t|t| of s coherent with f :

t̂ = arg max
t

Pr(t | s, f)

We can make the naı̈ve Bayes’ assumption that
s and f are statistically independent variables
given t. This results in the basic equation for ITP
with error correction (Ortiz-Martı́nez, 2011):

t̂ = arg max
t

Pr(t | s) · Pr(f | t) (1)

where, as we will see in Section 3.2, distribution
Pr(t | s) can be approximated by a machine trans-
lation model, and Pr(f | t) by an error correction
model that measures the degree of compatibility
between f and t.

Note that by using a probability distribution
Pr(f | t), any translation is compatible with a
given user feedback to some degree. As a con-
sequence, the translation returned by Equation (1)
may still not contain the segments validated by the
user; we need to identify the sub-string of the re-
turned sentence that corresponds to the each of the
segments validated by the user. To solve this prob-
lem, we define an alignment a = a1, . . . , a|f | be-
tween the user-validated segments f = f̃1, . . . , f̃|f |
and a list of segments t̃ = t̃1, . . . , t̃|f |, where each
t̃k = tki . . . tkj is a sub-sequence of words in t.
Each alignment link ak = t̃k indicates the particu-
lar segment in t that should be replaced by the kth
user-validated segment f̃k to make t coherent to f .
Unaligned words in t constitute the free text that

200



completes the gaps in between the user-validated
segments in f (González-Rubio et al., 2013). The
alignment also must be monotonic to preserve the
order of the user-validated segments. Formally,
for every pair of alignment links: ak = t̃k and
ak′ = t̃k′ , k < k′ ⇐⇒ kj < k′i.

After including alignment in Equation (1) and
following a maximum approximation, we arrive to
our final formulation of ITP with error correction:

(t̂, â) = arg max
t,a

Pr(t | s) · Pr(f ,a | t) (2)

In practice, we combine the probability distri-
butions in Equation (2) in a log-linear fashion as it
is typically done in MT (Och and Ney, 2002).

3.2 Models
Equation (2) includes two probability distribu-
tions: Pr(t | s) and Pr(f ,a | t). The first one can
be modeled by any of the multiple machine trans-
lation models that have been proposed in the liter-
ature; (Koehn, 2009) for example provide a good
description of them. We will focus our exposition
in the latter distribution, Pr(f ,a | t), that evalu-
ates the compatibility between a translation t and
some user feedback f through alignment a.

Following (González-Rubio et al., 2013), we
model Pr(f ,a | t) as an error correction model
based on the edit distance (Levenshtein, 1966).
Given a candidate string and the corresponding
reference string, we model edit distance as a
Bernoulli process where each word of the candi-
date has a probability pe of being edited. Under
this interpretation, the number of edits δ observed
in a candidate of length n is a random variable that
follows a binomial distribution, δ ∼ B(n, pe). By
assuming independence between each alignment
link, we can model error-correction probability as:

Pr(f ,a | t) ≈
|a|∏
k=1

PE (̃fk, ak)

=
|a|∏
k=1

(
nk
δk

)
pδke (1− pe)(nk−δk)

where PE (̃fk, ak) is the error correction probabil-
ity for the k-th alignment link whose value is given
by the probability mass function of the binomial
distribution, nk = |̃fk| is the length in words of
the k-th segment validated by the user (̃fk), and
δk is the edit distance between f̃k and the segment
ak = t̃k of t aligned to it according to a.

6

1

5

I saw a man with a telescope

2 3

4

I saw a man with a telescope
I saw with a telescope a man

Figure 2: Example of a hypergraph encoding two
different translations for the Spanish sentence: “Vi
a un hombre con un telescopio”.

The probability of editing pe is the single free
parameter of this model. Alternatively, we can use
a model based on a multinomial distribution as-
signing different probabilities to different edit op-
erations. Nevertheless, we adhere to the binomial
approximation due to its simplicity.

3.3 Search

Next, we address the problem posed by the max-
imization in Equation (2). Following (Barrachina
et al., 2009), we split search into a two step pro-
cess. Given a source language sentence, we first
generate a graph-based representation that con-
tains its most probable translations. Then, we
search for the optimal translation and alignment
on it according to Equation (2). In particular, we
use hypergraphs to represent such search space.

One important advantage of this approach, is
that it separates the proof-read step from the MT
engine used to generate the initial translations. As
such it provides an unified framework that ac-
cepts both the use of phrase-based and hierarchi-
cal/syntax translation models.

3.3.1 Hypergraphs
A hypergraph (Gallo et al., 1993) is a generaliza-
tion of the concept of graph where the edges (now
called hyperedges) may connect several nodes
(hypernodes) at the same time. Formally, a hy-
pergraph is a weighted acyclic graph represented
by a pair H =<V, E>, where V is a set of hy-
pernodes and E is a set of hyperedges. Each hy-
peredge ε ∈ E connects a set of tail hypernodes
T (ε) = {τ1 . . . τ|T (ε)|} τl ∈ V , to a head hypern-
ode H(ε) ∈ V . A hypernode with no ingoing hy-
peredges is a leaf, while a hypernode with no out-
going hyperedges is a root. Each hypernode rep-
resents a partial translation generated during the
MT decoding process. Each ingoing hyperedge ε

201



represents the rule applied to generate the partial
solution in the head from the partial solutions in
the tail hypernodes, as such it has an associated
probability P (ε). Figure 2 shows an example hy-
pergraph2. Two alternative translations are con-
structed from the leaf hypernodes (1, 2 and 3) up
to the root hypernode (6). Hypergraphs provide
a compact representation of the translation space
that allows us to derive efficient search algorithms.

Hypergraphs are the natural representation for
hierarchical MT models (Chiang, 2005; Zollmann
and Venugopal, 2006). Note, however, that word-
graphs (Ueffing et al., 2002), which are used to
represent the search space for phrase-based mod-
els (Koehn et al., 2003), are a special case of hy-
pergraphs in which hyperedges have at most one
tail hypernode.

3.3.2 Search on hypergraphs
We formalize the maximization in Equation (2) as
a bottom-up search problem. Starting from the
leaf hypernodes, we keep track of the best solu-
tions (partial translation and alignment) achievable
at each hypernode. We define Q(ν, [m,n]) as the
probability of the most likely partial translation
derivable from hypernode ν aligned to (accounting
for) user-validated segments fromm-th to n-th, we
will refer to this interval as the coverage of the
partial solution. Given a node ν and a coverage,
we compute its score from its ingoing hyperedges.
Specifically, Q(ν, [m,n]) will be equal to the max-
imum score of the partial solutions computed from
any ingoing hyperedge. Partial solutions from an
ingoing hyperedge ε are defined as combinations
of partial solutions on its tail hypernodes under the
constrain that the concatenation of their coverages
equals [m,n]. Formally, given an ingoing hyper-
edge ε, a combination c = {Q(τl, [ml, nl])}|T (ε)|l=1
is valid if it holds the following four conditions:

1. m1 = m

2. n|T (ε)| = n
3. ∀l, τl ∈ T (ε)
4. ∀l : 1 < l ≤ |T (ε)|,ml = nl−1 + 1
Q(ν, [m,n]) can be computed efficiently via the

following dynamic programming recursion:

Q(ν, [m,n]) = max
ε∈I(ν)

c∈C(ε,[m,n])
P (ε)

∏
Q(τl,[ml,nl])∈c

Q(τl, [ml, nl])

2For simplicity, we do not show hyperedge probabilities.

EU corpus (Es/En)
train tune test

Sentences 214K 400 800
Tokens 5.9M / 5.2M 12K / 10K 23K / 20K
Vocabulary 97K / 84K 3K / 3K 5K / 4K

Table 1: Main figures of the EU corpus. K and M
stand for thousands and millions of elements.

where I(ν) are the ingoing hyperedges of ν, P (ε)
is the probability of hyperedge ε, C(ε, [m,n]) is
the set of valid combinations for hyperedge ε and
coverage [m,n], and c ∈ C(ε, [m,n]) is one of
such valid combinations

Leaf hypernodes represent the base cases for
this recursion. For simplicity, we restrict them to
be fully-aligned to at most one user-validated seg-
ment3. That is, given a leaf hypernode λ ∈ V:

Q(λ, [m,n])=
{
PMT(λ)PE(w(λ), f̃m) if m=n
0 otherwise.

where PMT(λ) is the MT probability (language
plus translation model) of λ, and PE(w(λ), f̃m) is
the error correction probability between the target
text covered by the leaf hypernode, w(λ), and the
m-th user-validated segment in f .

The score of the optimal solution is given by
Q(α, [1, |f |]), where α ∈ V is the root hypernode.
We can recover (t̂, â) through backtracking.

The process described above loops over all
hyperedges and coverages (bounded by |E||f |2),
evaluating all valid combinations (bounded by the
coverage partitions). It can be implemented by an
algorithm with a complexity inO(|E||f |2τ ), where
τ is the average number of tail hypernodes per
hyperedge (usually set to 2). In practice, our ap-
proach has a complexity in O(|E||f |4).

4 Experimental Setup

4.1 Corpus and MT systems

We tested the proposed methods in the Spanish-
to-English (Es–En) partition of the Bulletin of the
European Union (EU) corpus (Barrachina et al.,
2009; González-Rubio et al., 2013). We tokenized
the corpus keeping the real case of the sentences.
Table 1 shows the main figures of the corpus.

We estimated a hierarchical MT model for the
train partition with the standard configuration of

3Preliminary experiments did not show a difference in the
final results when relaxing this restriction.

202



the Moses toolkit (Koehn et al., 2007). Log-
linear weighs were estimated by minimum error-
rate training (Och, 2003) on the tune partition.
Then, we automatically translated tune and test
partitions using the optimized model to obtain the
corresponding hypergraphs. Next, we optimized
the single free parameter pe of the error correc-
tion model (see Section 3.2) on the tune partition.
Finally, we interactively translated both partitions
according to the unrestricted ITP approach pro-
posed in Section 3.

4.2 User Simulation
ITP evaluation with human translators is simply
to slow and expensive to be applied on a frequent
and ongoing basis during system development. In-
stead, we carried out an automatic evaluation with
simulated users which is faster and cheaper.

At each ITP iteration (see Figure 1), we have
to decide which segments in the suggested trans-
lation should be validated, and which error should
be corrected. To do that, we considered the ref-
erence translations in the corpus as the output
that a human expert would want to obtain. Then,
we align the suggested translation and the refer-
ence via edit distance: words aligned to itself are
marked as valid, while edited words are potential
corrections to be typed by the simulated user.

Without loss of generality, we introduced two
restrictions: (1) we restrict users to validate seg-
ments only at the first iteration, and (2) the sim-
ulated user always corrected the first (in reading
order) error in the suggested translation. We are
aware that the results obtained with this user simu-
lation will be pessimistic since it forbids behaviors
that may improve user productivity, e.g. validat-
ing segments at each iteration or correcting more
promising parts of the suggested translation. Our
goal is not to match the behavior of a human trans-
lator, but to allow for a meaningful comparison
against conventional ITP. Note that prefix-based
proof-reading is a particular case of our user sim-
ulation with no segment validation.

4.3 Evaluation Metrics
ITP systems are evaluated according to the effort
needed to generate the desired translations. This
effort is usually estimated as the number of actions
performed by the user while interacting with the
system. In our user simulation, we describe two
different actions: segment-validation, and word-
correction. Each segment validation involves the

user to “click” on the initial and final words of the
segment4. Each correction corresponds to an edit
operation performed by the user. Specifically, we
used the following measures in our experiments:

Word stroke ratio (WSR): Proposed in (Tomás
and Casacuberta, 2006) as the quotient between
the number of words edited by the user (word-
strokes), and the number of words in the final
translation. Word-strokes are considered as sin-
gle actions with constant cost independently of
the length of the edited word.

Mouse action ratio (MAR): Proposed in (Bar-
rachina et al., 2009) as the quotient between the
number of “clicks” made by the user (mouse-
actions), and the number of words in the final
translation. In addition to the “clicks” for seg-
ment validation, we count one more mouse ac-
tion per sentence accounting for the final accep-
tance of the suggested translation.

Conceptually (Macklovitch et al., 2005), MAR
can be seen as accounting for the cognitive part of
the supervision process: undertanding the transla-
tion and identifying the errors in it, while WSR
accounts for the actual physical effort required to
type the corrections. As such, both metrics are
complementary to express the total human effort
involved in proof-reading a document.

We also evaluated the quality of the initial auto-
matic translations generated by the system:

Bilingual evaluation understudy (BLEU):
Proposed in (Papineni et al., 2002), it is based
on the precision of n-grams between the
suggested translation and the reference; it also
includes a brevity penalty to penalize short
translations. This score ranges between 0 and
100, with 100 denoting a perfect translation.

Translation edit rate (TER): Proposed
in (Snover et al., 2006), it measures the
number of edit operations (substitution, inser-
tion and deletion of single words, and swap
of word sequences) divided by the number of
words in the reference.

In addition to be an MT quality metric, TER can
also be seen as a human-effort measure in PE sce-
narios. Therefore, we can use TER and WSR to
compare human effort between PE and ITP.

4One single “click” is enough for one-word segments.

203



AUTOMATIC TRANSLATION

tune test
BLEU [%](⇑) TER [%](⇓) BLEU [%](⇑) TER [%](⇓)
38.9±1.4 47.2±1.4 44.2±1.3 41.1±1.2

COMPUTER-ASSISTED TRANSLATION

TER [%](⇓) TER [%](⇓)
Post-editing 47.2±1.4 41.1±1.2

MAR [%](⇓) WSR [%](⇓) MAR [%](⇓) WSR [%](⇓)
prefix-based ITP 11.2±0.4 45.8±2.0 10.3±0.3 54.5±1.4
Our approach 33.9±1.2 30.5±1.6 35.4±0.9 35.1±1.1

Table 2: Results of different approaches when interactively translating the tune and test partitions of the
EU corpus. We compare decoupled PE, prefix-based ITP and the unrestricted ITP approach proposed in
this work. Automatic translation results are shown to indicate the difficulty of the task. Results in bold
indicate the lowest human effort (typing) achievable by the different scenarios.

Finally, in order to assess the statistical signif-
icance of the results, we also provide 95% con-
fidence intervals for their values. These inter-
vals were computed via pair-wise bootstrap re-
sampling as proposed in (Zhang and Vogel, 2004).

5 Results

This section presents the results of the experiments
performed to assess the unrestricted ITP approach
proposed in Section 3. First, we compare our
ITP approach to the prefix-based ITP scenario de-
scribed in (Barrachina et al., 2009) and the decou-
pled PE approach. Then, we further study our
approach investigating the relationship between
segment-validation and typing effort. Finally, we
provide evidence that the proposed unrestricted
proof-reading protocol allows to reduce the cogni-
tive overload produced by the changing translation
completions of prefix-based ITP approaches.

Table 2 displays user-effort results for the pro-
posed ITP approach against prefix-based ITP (Bar-
rachina et al., 2009)5 and a decoupled PE base-
line approach. Automatic translation results are
also displayed to give an idea of the difficulty
of the task. We can observe how our approach
clearly outperformed both prefix-based ITP and
PE in terms of user typing effort as measured by
WSR and TER respectively. According to these
results, a human translator assisted by our ITP sys-
tem would only need to correct only about one
third of the words to generate the correct transla-
tions. In comparison, PE would require to type

5For prefix-based ITP, MAR accounts for the prefixes val-
idated by the user while proof-reading the translations.

Figure 3: WSR and MAR as a function of the max-
imum number of words validated by the simulated
user in our ITP approach. No changes were ob-
served for more than 40 validated words.

∼ 41% of the words (17% more) while prefix-
based ITP would require to correct more that half
of them (55% more). Additionally, we can observe
that prefix-based ITP was not better than the PE
baseline in all cases. This result, coherent with
previous works e.g. (González-Rubio et al., 2013;
Green et al., 2014), exemplifies the potential limi-
tations of prefix-based ITP.

The large reductions in typing effort observed
for our ITP approach came together with an im-
portant increase in the number of mouse actions.

204



Next, we focused on the differences between
prefix-based and our approach. To do that, we car-
ried out different experiments allowing the sim-
ulated user to validate an increasing number of
words from zero to infinity (corresponding respec-
tively to the results of prefix-based ITP and our ap-
proach in Table 2). Figure 3 shows WSR (top) and
MAR (bottom) for the Test partition as a function
of the maximum number or words allowed to be
validated by the user.

As we allowed the simulated user to validate
more words, the amount of words to be corrected
(WSR) decreased dramatically. For example, we
obtained a 10% relative reduction in WSR when
we allowed the user to validate a maximum of
4 words. A similar trend (but in the opposite
direction) can be observed for MAR: as we al-
lowed the user to validate more words, the num-
ber of mouse actions increased until stabilization.
In other words, our ITP approach allows to reduce
user typing effort at the expense of an increase in
the number of mouse actions. As we have said
before, WSR and MAR account for different phe-
nomena and thus have different cost from a hu-
man point of view (Macklovitch et al., 2005). It
may seem that we have simply exchanged typing
effort for cognitive effort. However, two consid-
erations allow us to consider this a beneficial ex-
change. On the one hand, from a pure mechanistic
point of view, typying a whole word usually re-
quires more effort than “clicking” on it. On the
other hand, from a cognitive point of view, the
user has to read, understand, and evaluate the sug-
gested translation in both prefix-based ITP and our
approach. Hence, the difference in cognitive effort
between these two approaches approaches is most
probably negligible. Nevertheless, these consid-
erations should be tested with actual human users
before reaching categorical conclusions.

Average response time of our Python prototype
was below 3 seconds6. Obviously, it does not qual-
ify as real-time. However, we expect an important
reduction in response time after implementing our
approach in a more efficient language.

We performed a final analysis to evaluate to
which extent our proposal alleviates the main an-
noying effect inherent to prefix-based ITP, namely
correct words in a given suffix overwritten by the
next suggested suffix. This common effect, ob-
served in several user studies, make human users

6The test machine was an Intel i5 CPU at 3.4 GHz.

Figure 4: Percentage of words suggested by the
system that were correct but overwritten in subse-
quent translation suggestions, as a function of the
number of words validated by the simulated user.
This ratio can be seen as a measure of the user
cognitive overload.

feel that cognitive effort invested in evaluating
each suggested translation was wasted.

To do that, we measured the number of correct
words that were modified by subsequent transla-
tion suggestions, normalized by the total number
of suggested words. Figure 4 displays this per-
centage as a function of the maximum number of
words that can be validated by the simulated user.
We can observe how for prefix-based ITP (zero
value in the x-axis), this cognitive overload is a
very important phenomena; more than 30% of the
words suggested by the system were correct but
modified by following suffix suggestions. As we
allowed more words to be fixed, this percentage
steadily decreased down to zero. This indicates
that our ITP proposal actually provides a mecha-
nism to overcome the cognitive overload inherent
to prefix-based ITP.

6 Summary

We have presented a new ITP approach where the
user is not longer bound to interact with the system
in a prefix-based fashion (Barrachina et al., 2009).
The proposed ITP approach gives the user com-
plete freedom to validate and correct any part of
the suggested translations thus providing a more
natural working environment for human transla-
tors. We formalize the problem as a MT model
with error correction which, in practice, is imple-
mented as a constrained search on hypergraphs.

Simulated results showed that the proposed
ITP approach drastically reduced the typing effort
needed to generate translations, improving results
of both decoupled PE and prefix-based ITP. This

205



reduction in typing effort came at the expense of
a larger amount of mouse actions required to val-
idate correct segments of the suggested transla-
tions. However, since mouse actions are cheaper
than typing full words, we can expect this ex-
change to reduce overall user effort. Nevertheless,
this expectation should be confirmed in future ex-
periments with actual human users. Finally, in ad-
dition to reduce user effort, we also provide evi-
dence indicating that the proposed ITP approach
can reduce the cognitive overload commonly re-
ported by humans using prefix-based ITP systems.

Acknowledgments

Work supported by the Generalitat Valenciana un-
der grant ALMAMATER (PrometeoII/2014/030).

References
Vicent Alabau, Jesús Gonzlez-Rubio, Luis A. Leiva,

Daniel Ortiz-Martı́nez, Germán Sanchis-Trilles,
Francisco Casacuberta, Bartolomé’ Mesa-Lao, Rag-
nar Bonk, Michael Carl, and Mercedes Garcı́a-
Martı́nez. 2013. User evaluation of advanced in-
teraction features for a computer-assisted translation
workbench. In Proceedings of the XIV Machine
Translation Summit, pages 361–368.

Sergio Barrachina, Oliver Bender, Francisco Casacu-
berta, Jorge Civera, Elsa Cubel, Shahram Khadivi,
Antonio Lagarda, Hermann Ney, Jesús Tomás, En-
rique Vidal, and Juan-Miguel Vilar. 2009. Sta-
tistical approaches to computer-assisted translation.
Computational Linguistics, 35:3–28, March.

David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263–270.

Giorgio Gallo, Giustino Longo, Stefano Pallottino, and
Sang Nguyen. 1993. Directed hypergraphs and
applications. Discrete Applied Mathematics, 42(2-
3):177–201, April.

Jesús González-Rubio, Daniel Ortiz-Martı́nez, and
Francisco Casacuberta. 2010. Balancing user effort
and translation error in interactive machine transla-
tion via confidence measures. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 173–177.

Jesús González-Rubio, Daniel Ortı́z-Martinez, José-
Miguel Benedı́, and Francisco Casacuberta. 2013.
Interactive machine translation using hierarchical
translation models. In Proceedings of the confer-
ence on Empirical Methods in Natural Language
Processing, pages 244–254.

Spence Green, Sida I. Wang, Jason Chuang, Jeffrey
Heer, Sebastian Schuster, and Christopher D. Man-
ning. 2014. Human effort and machine learnability
in computer aided translation. In Proceedings of the
conference on Empirical Methods in Natural Lan-
guage Processing, pages 1225–1236.

Pierre Isabelle and Ken Church. 1998. Special issue
on: New tools for human translators, volume 12.
Kluwer Academic Publishers, January.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology, pages 48–54.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics.

Philipp Koehn, Chara Tsoukala, and Herve Saint-
Amand. 2014. Refinements to interactive transla-
tion prediction based on search graphs. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics, pages 574–578.

Philipp Koehn. 2009. A process study of computer-
aided translation. Machine Translation, 23(4):241–
263.

Philippe Langlais and Guy Lapalme. 2002.
TransType: development-evaluation cycles to boost
translator’s productivity. Machine Translation,
17(2):77–98, September.

Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707–710, February.

Elliot Macklovitch, Nam-Trung Nguyen, and Roberto
Silva. 2005. User evaluation report. Technical
report, Université de Montréal. TransType2 (IST-
2001-32091).

Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 295–302.

Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160–167.

Daniel Ortiz-Martı́nez. 2011. Advances in Fully-
Automatic and Interactive Phrase-Based Statisti-
cal Machine Translation. Ph.D. thesis, Universitat
Politècnica de València.

206



Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311–318.

Luis Rodrı́guez, Francisco Casacuberta, and Enrique
Vidal. 2007. Computer assisted transcription of
speech. In Proceedings of the 3rd Iberian Confer-
ence on Pattern Recognition and Image Analysis,
volume 4477 of Lecture Notes in Computer Science,
pages 241–248.

Germán Sanchis-Trilles, Daniel Ortiz-Martı́nez, Jorge
Civera, Francisco Casacuberta, Enrique Vidal, and
Hieu Hoang. 2008. Improving Interactive Machine
Translation via Mouse Actions. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 485–494.

Germán Sanchis-Trilles, Vicent Alabau, Christian
Buck, Michael Carl, Francisco Casacuberta, Mer-
cedes Garcı́a-Martı́nez, Ulrich Germann, Jesús
González-Rubio, Robin L. Hill, Philipp Koehn,
Luis A. Leiva, Bartolomé Mesa-Lao, Daniel Ortiz-
Martı́nez, Herve Saint-Amand, and Chara Tsoukala.
2014. Interactive translation prediction versus con-
ventional post-editing in practice: a study with the
casmacat workbench. Machine Translation, 28(3-
4):217–235, December.

Nicols Serrano, Adri Gimnez, Jorge Civera, Alberto
Sanchis, and Alfons Juan. 2014. Interactive hand-
writing recognition with limited user effort. Inter-
national Journal on Document Analysis and Recog-
nition, 17:47–59.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223–231.

Jesús Tomás and Francisco Casacuberta. 2006. Statis-
tical phrase-based models for interactive computer-
assisted translation. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 835–841.

Alejando H. Toselli, Verónica Romero, Luis Ro-
driguez, and Eenrique Vidal. 2007. Computer as-
sisted transcription of handwritten text images. In
9th International Conference on Document Analysis
and Recognition, volume 2, pages 944–948.

Nicola Ueffing, Franz J. Och, and Hermann Ney. 2002.
Generation of word graphs in statistical machine
translation. In Proceedings of the conference on Em-
pirical Methods in Natural Language Processing,
pages 156–163.

Nancy Underwood, Bartolomé Mesa-Lao, Mer-
cedes Garcı́a Martinez, Michael Carl, Vicent Al-
abau, Jesús González-Rubio, Luis A. Leiva, Germán
Sanchis-Trilles, Daniel Ortı́z-Martinez, and Fran-
cisco Casacuberta. 2014. Evaluating the effects
of interactivity in a post-editing workbench. In
Proceedings of the 9th International Conference on
Language Resources and Evaluation, pages 553–
559.

Ying Zhang and Stephan Vogel. 2004. Measuring con-
fidence intervals for the machine translation evalu-
ation metrics. In Proceedings of the 10th Interna-
tional Conference on Theoretical and Methodologi-
cal Issues in Machine Translation, pages 85–94.

Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the Workshop on Statistical
Machine Translation, pages 138–141.

207


