



















































Context-aware Argumentative Relation Mining


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1127–1137,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Context-aware Argumentative Relation Mining

Huy V. Nguyen
Computer Science Department

University of Pittsburgh
Pittsburgh, PA 15260
hvn3@pitt.edu

Diane J. Litman
Computer Science Department and

Learning Research and Development Center
University of Pittsburgh

Pittsburgh, PA 15260
dlitman@pitt.edu

Abstract

Context is crucial for identifying argumen-
tative relations in text, but many argument
mining methods make little use of contex-
tual features. This paper presents context-
aware argumentative relation mining that
uses features extracted from writing top-
ics as well as from windows of context
sentences. Experiments on student essays
demonstrate that the proposed features im-
prove predictive performance in two argu-
mentative relation classification tasks.

1 Introduction

By supporting tasks such as automatically iden-
tifying argument components1 (e.g., premises,
claims) in text, and the argumentative relations
(e.g., support, attack) between components, ar-
gument (argumentation) mining has been studied
for applications in different research fields such
as document summarization (Teufel and Moens,
2002), opinion mining (Boltužić and Šnajder,
2014), automated essay evaluation (Burstein et
al., 2003), legal information systems (Palau and
Moens, 2009), and policy modeling platforms
(Florou et al., 2013).

Given a pair of argument components with one
component as the source and the other as the tar-
get, argumentative relation mining involves deter-
mining whether a relation holds from the source to
the target, and classifying the argumentative func-
tion of the relation (e.g., support vs. attack). Ar-

1There is no consensus yet on an annotation scheme for
argument components, or on the minimal textual units to be
annotated. We follow Peldszus and Stede (2013) and con-
sider “argument mining as the automatic discovery of an ar-
gumentative text portion, and the identification of the relevant
components of the argument presented there.” We also bor-
row their term “argumentative discourse unit” to refer to the
textual units (e.g., text segment, sentence, clause) which are
considered as argument components.

Essay 73. Topic: Is image more powerful
than the written word?

...(1)Hence, I agree only to certain de-
gree that in today’s world, image serves
as a more effective means of communica-
tion[MajorClaim].
(2)Firstly, pictures can influence the way
people think[Claim]. (3)For example, nowa-
days horrendous images are displayed on
the cigarette boxes to illustrate the con-
sequences of smoking[Premise]. (4)As a
result, statistics show a slight reduction
in the number of smokers, indicating that
they realize the effects of the negative
habit[Premise]...

Figure 1: Excerpt from a student persuasive essay
(Stab and Gurevych, 2014a). Sentences are num-
bered and argument components are tagged.

gumentative relation mining - beyond argument
component mining - is perceived as an essential
step towards more fully identifying the argumenta-
tive structure of a text (Peldszus and Stede, 2013;
Sergeant, 2013; Stab et al., 2014). Consider the
second paragraph shown in Figure 1. Only detect-
ing the argument components (a claim in sentence
2 and two premises in sentences 3 and 4) does not
give a complete picture of the argumentation. By
looking for relations between these components,
one can also see that the two premises together jus-
tify the claim. The argumentation structure of the
text in Figure 1 is illustrated in Figure 2.

Our current study proposes a novel approach
for argumentative relation mining that makes use
of contextual features extracted from surround-
ing sentences of source and target components as
well as from topic information of the writings.

1127



Prior argumentative relation mining studies have
often used features extracted from argument com-
ponents to model different aspects of the relations
between the components, e.g., relative distance,
word pairs, semantic similarity, textual entailment
(Cabrio and Villata, 2012; Stab and Gurevych,
2014b; Boltužić and Šnajder, 2014; Peldszus and
Stede, 2015b). Features extracted from the text
surrounding the components have been less ex-
plored, e.g., using words and their part-of-speech
from adjacent sentences (Peldszus, 2014). The
first hypothesis investigated in this paper is that the
discourse relations of argument components with
adjacent sentences (called context windows in this
study, a formal definition is given in §5.3) can help
characterize the argumentative relations that con-
nect pairs of argument components. Reconsider-
ing the example in Figure 1, without knowing the
content “horrendous images are displayed on the
cigarette boxes” in sentence 3, one cannot easily
tell that “reduction in the number of smokers” in
sentence 4 supports the “pictures can influence”
claim in sentence 2. We expect that such content
relatedness can be revealed from a discourse anal-
ysis, e.g., the appearance of a discourse connective
“As a result”.

While topic information in many writing gen-
res (e.g., scientific publications, Wikipedia arti-
cles, student essays) has been used to create fea-
tures for argument component mining (Teufel and
Moens, 2002; Levy et al., 2014; Nguyen and Lit-
man, 2015), topic-based features have been less
explored for argumentative relation mining. The
second hypothesis investigated in this paper is that
features based on topic context also provide useful
information for improving argumentative relation
mining. In the excerpt below, knowing that ‘online
game’ and ‘computer’ are topically related might
help a model decide that the claim in sentence 1
supports the claim in sentence 2:

(1)People who are addicted to games,
especially online games, can eventually
bear dangerous consequences[Claim].
(2)Although it is undeniable that computer is
a crucial part of human life[Premise], it still
has its bad side[MajorClaim].2

Motivated by the discussion above, we propose
context-aware argumentative relation mining – a
novel approach that makes use of contextual fea-

2In this excerpt, the Premise was annotated as an attack to
the MajorClaim in sentence 2.

MajorClaim(1)

Claim(2)

Premise(4)Premise(3)

Premise(6)

Support

Support Attack

Support

Support Support

Figure 2: Structure of the argumentation in the ex-
cerpt. Relations are illustrated accordingly to the
annotation provided in the corpus. Premises 3 and
4 were annotated for separate relations to Claim 2.
Our visualization should not mislead that the two
premises are linked or convergent.

tures that are extracted by exploiting context sen-
tence windows and writing topic to improve rela-
tion prediction. In particular, we derive features
using discourse relations between argument com-
ponents and windows of their surrounding sen-
tences. We also derive features using an argument
and domain word lexicon automatically created by
post-processing an essay’s topic model. Experi-
mental results show that our proposed contextual
features help significantly improve performance in
two argumentative relation classification tasks.

2 Related Work

Unlike argument component identification where
textual inputs are typically sentences or clauses
(Moens et al., 2007; Stab and Gurevych, 2014b;
Levy et al., 2014; Lippi and Torroni, 2015), tex-
tual inputs of argumentative relation mining vary
from clauses (Stab and Gurevych, 2014b; Peld-
szus, 2014) to multiple-sentences (Biran and Ram-
bow, 2011; Cabrio and Villata, 2012; Boltužić and
Šnajder, 2014). Studying claim justification be-
tween user comments, Biran and Rambow (2011)
proposed that the argumentation in justification of
a claim can be characterized with discourse struc-
ture in the justification. They however only con-
sidered discourse markers but not discourse re-
lations. Cabrio et al. (2013) conducted a cor-
pus analysis and found certain similarity between
Penn Discourse TreeBank relations (Prasad et al.,
2008) and argumentation schemes (Walton et al.,
2008). However they did not discuss how such
similarity could be applied to argument mining.

Motivated by these findings, we propose to
use features extracted from discourse relations be-

1128



tween sentences for argumentative relation min-
ing. Moreover, to enable discourse relation
features when the textual inputs are only sen-
tences/clauses, we group the inputs with their con-
text sentences. Qazvinian and Radev (2010) used
the term “context sentence” to refer to sentences
surrounding a citation that contained information
about the cited source but did not explicitly cite it.
In our study, we only require that the context sen-
tences of an argument component must be in the
same paragraph and adjacent to the component.

Prior work in argumentative relation mining has
used argument component labels to provide con-
straints during relation identification. For exam-
ple, when an annotation scheme (e.g., (Peldszus
and Stede, 2013; Stab and Gurevych, 2014a)) does
not allow relations from claim to premise, no rela-
tions are inferred during relation mining for any
argument component pair where the source is a
claim and the target is a premise. In our work,
we follow Stab and Gurevych (2014b) and use the
predicted labels of argument components as fea-
tures during argumentative relation mining. We,
however, take advantage of an enhanced argument
component model (Nguyen and Litman, 2016) to
obtain more reliable argument component labels
than in (Stab and Gurevych, 2014b).

Argument mining research has studied differ-
ent data-driven approaches for separating orga-
nizational content (shell) from topical content
to improve argument component identification,
e.g., supervised sequence model (Madnani et al.,
2012), unsupervised probabilistic topic models
(Séaghdha and Teufel, 2014; Du et al., 2014).
Nguyen and Litman (2015) post-processed LDA
(Blei et al., 2003) output to extract a lexicon of ar-
gument and domain words from development data.
Their semi-supervised approach exploits the topic
context through essay titles to guide the extraction.

Finally, prior research has explored predict-
ing different argumentative relationship labels be-
tween pairs of argument components, e.g., attach-
ment (Peldszus and Stede, 2015a), support vs.
non-support (Biran and Rambow, 2011; Cabrio
and Villata, 2012; Stab and Gurevych, 2014b),
{implicit, explicit}×{support, attack} (Boltužić
and Šnajder, 2014), verifiability of support (Park
and Cardie, 2014). Our experiments use two such
argumentative relation classification tasks (Sup-
port vs. Non-support, Support vs. Attack) to eval-
uate the effectiveness of our proposed features.

3 Persuasive Essay Corpus

Stab and Gurevych (2014a) compiled the Persua-
sive Essay Corpus consisting of 90 student argu-
mentative essays and made it publicly available.3

Because the corpus has been utilized for differ-
ent argument mining tasks (Stab and Gurevych,
2014b; Nguyen and Litman, 2015; Nguyen and
Litman, 2016), we use this corpus to demonstrate
our context-aware argumentative relation mining
approach, and adapt the model developed by Stab
and Gurevych (2014b) to serve as the baseline for
evaluating our proposed approach.

Three experts identified possible argument
components of three types within each sentence in
the corpus (MajorClaim - writer’s stance toward
the writing topic, Claim - controversial statements
that support or attack MajorClaim, and Premise -
evidence used to underpin the validity of Claim),
and also connected the argument components us-
ing two argumentative relations (Support and At-
tack). According to the annotation manual, each
essay has exactly one MajorClaim. A sentence
can have one or more argument components (Ar-
gumentative sentences). Sentences that do not
contain any argument component are labeled Non-
argumentative. Figure 1 shows an example essay
with components annotated, and Figure 2 illus-
trates relations between those components. Argu-
mentative relations are directed and can hold be-
tween a Premise and another Premise, a Premise
and a (Major-) Claim, or a Claim and a Major-
Claim. Except for the relation from Claim to
MajorClaim, an argumentative relation does not
cross paragraph boundaries. The three experts
achieved inter-rater accuracy 0.88 for component
labels and Krippendorff’s αU 0.72 for component
boundaries. Given the annotated argument com-
ponents, the three experts obtained Krippendorff’s
α 0.81 for relation labels. The number of relations
are shown in Table 1.

4 Argumentative Relation Tasks

4.1 Task 1: Support vs. Non-support

Our first task follows (Stab and Gurevych, 2014b):
given a pair of source and target argument com-
ponents, identify whether the source argumenta-
tively supports the target or not. Note that when
a support relation does not hold, the source may
attack or has no relation with the target compo-

3www.ukp.tu-darmstadt.de/data/argumentation-mining

1129



Label #instances
Within-paragraph constraint
Support 989
Attack 103
No paragraph constraint
Support 1312
Attack 161

Table 1: Data statistics of the corpus.

nent. For each of two argument components in
the same paragraph4, we form two pairs (i.e., re-
versing source and target). In total we obtain 6330
pairs, in which 989 (15.6%) have Support relation.
Among 5341 Non-support pairs, 103 have Attack
relation and 5238 are no-relation pairs.

Stab and Gurevych (2014b) split the corpus into
an 80% training set and a 20% test set which have
similar label distributions. We use this split to train
and test our proposed models, and directly com-
pare our models’ performance to the reported per-
formance in (Stab and Gurevych, 2014b).

4.2 Task 2: Support vs. Attack
To further evaluate the effectiveness of our ap-
proach, we conduct an additional task that clas-
sifies an argumentative relation as Support or At-
tack. For this task, we assume that the relation
(i.e., attachment (Peldszus, 2014)) between two
components is given, and aim at identifying the
argumentative function of the relation. Because
we remove the paragraph constraint in this task,
we obtain more Support relations than in Task 1.
As shown in Table 1, of the total 1473 relations,
we have 1312 (89%) Support and 161 (11%) At-
tack relations. Because this task was not studied
in (Stab and Gurevych, 2014b), we adapt Stab and
Gurevych’s model to use as the baseline.

5 Argumentative Relation Models

5.1 Baseline
We adapt (Stab and Gurevych, 2014b) to use as
a baseline for evaluating our approach. Given a
pair of argument components, we follow (Stab and
Gurevych, 2014b) by first extracting 3 feature sets:
structural (e.g., word counts, sentence position),
lexical (e.g., word pairs, first words), and gram-
matical production rules (e.g., S→NP,VP).

4Allowing cross-paragraph relations exponentially in-
creases the number of no-relation pairs, which makes the pre-
diction data extremely skewed (Stab and Gurevych, 2014b).

Because a sentence may have more than one ar-
gument component, the relative component posi-
tions might provide useful information (Peldszus,
2014). Thus, we also include 8 new component
position features: whether the source and target
components are the whole sentences or the be-
ginning/end components of the sentences; if the
source is before or after the target component; and
the absolute difference of their positions.

Stab and Gurevych (2014b) used a 55-discourse
marker set to extract indicator features. We ex-
pand their discourse maker set by combining them
with a 298-discourse marker set developed in (Bi-
ran and Rambow, 2011). We expect the expanded
set of discourse markers will represent better pos-
sible discourse relations in the texts.

Stab and Gurevych (2014b) used predicted label
of argument components as features for both train-
ing and testing their argumentation structure iden-
tification model.5 As their predicted labels are not
available to us, we adapt this feature set by using
the argument component model in (Nguyen and
Litman, 2016) which was shown to outperform the
corresponding model of Stab and Gurevych.

For later presentation purposes, we name the
set of all features from this section except word
pairs and production rules as the common fea-
tures. While word pairs and grammatical pro-
duction rules were the most predictive features in
(Stab and Gurevych, 2014b), we hypothesize that
this large and sparse feature space may have nega-
tive impact on model robustness (Nguyen and Lit-
man, 2015). Most of our proposed models re-
place word pairs and production rules with differ-
ent combinations of new contextual features.

5.2 Topic-context Model

Our first proposed model (TOPIC) makes use of
Topic-context features derived from a lexicon of
argument and domain words for persuasive essays
(Nguyen and Litman, 2015). Argument words
(e.g., ‘believe’, ‘opinion’) signal the argumenta-
tive content and are commonly used across differ-
ent topics. In contrast, domain words are specific
terminologies commonly used within the topic
(e.g., ‘art’, ‘education’). The authors first use

5Stab and Gurevych (2014b) reported that including gold-
standard labels of argument component in both training and
testing phases yielded results close to human performance.
Our preliminary experiment showed that including gold-
standard argument component labels in training did not help
when predicted labels were used in the test set.

1130



topic prompts in development data of unannotated
persuasive essays to semi-automatically collect ar-
gument and domain seed words. In particular, they
used 10 argument seed words: agree, disagree,
reason, support, advantage, disadvantage, think,
conclusion, result, opinion. Domain seed words
are those in the topic prompts but not argument
seed words or stop words. The seeds words are
then used to supervise an automated extraction of
argument and domain words from output of LDA
topic model (Blei et al., 2003) on the develop-
ment data. The extracted lexicon consists of 263
(stemmed) argument words and 1806 (stemmed)
domain words mapped to 36 LDA topics.6 All ar-
gument words are from a single LDA topic while a
domain word can map to multiple LDA topics (ex-
cept the topic of argument words). Using the lex-
icon, we extract the following Topic-context fea-
tures:

Argument word: from all word pairs extracted
from the source and target components, we re-
move those that have at least one word not in the
argument word list. Each argument word pair de-
fines a boolean feature indicating its presence in
the argument component pair. We also include
each argument word of the source and target com-
ponents as a boolean feature which is true if the
word is present in the corresponding component.
We count number of common argument words, the
absolute difference in number of argument words
between source and target components.

Domain word count: to measure the topic sim-
ilarity between the source and target components,
we calculate number of common domain words,
number of pairs of two domain words that share
an LDA topic, number of pairs that share no LDA
topic, and the absolute difference in number of do-
main words between the two components.

Non-domain MainVerb-Subject dependency:
we extract MainVerb-Subject dependency triples,
e.g., nsubj(belive, I), from the source and target
components, and filter out triples that involve do-
main words. We model each extracted triple as a
boolean feature which is true if the corresponding
argument component has the triple.

Finally, we include the common feature set.
To illustrate the Topic-context features, con-

sider the following source and target components.
Argument words are in boldface, and domain

6An LDA topic is simply represented by a number, and
should not be misunderstood with essay topics.

words are in italic.
Essay 54. Topic: museum and art gallery
will disappear soon?

Source: more and more people can watch
exhibitions through television or internet at
home due to modern technology[Premise]
Target: some people think museums and
art galleries will disappear soon[Claim]
An argument word pair is people-think. There

are 35 pairs of domain words. A pair of two do-
main words that share an LDA topic is exhibitions-
art. A pair of two domain words that do not share
any LDA topic is internet-galleries.

5.3 Window-context Model

Our second proposed model (WINDOW) extracts
features from discourse relations and common
words between context sentences in the context
windows of the source and target components.

Definition. Context window of an argument com-
ponent is a text segment formed by neighboring
sentences and the covering sentence of the com-
ponent. The neighboring sentences are called con-
text sentences, and must be in the same paragraph
with the component.

In this study, context windows are determined
using window-size heuristics.7 Given a window-
size n, we form a context window by grouping the
covering sentence with at most n adjacently pre-
ceding and n adjacently following sentences that
must be in the same paragraph.

To minimize noise in feature space, we re-
quire that context windows of the source and tar-
get components must be mutually exclusive. Bi-
ran and Rambow (2011) observed that the re-
lation between a source argument and a target
argument is usually instantiated by some elabo-
ration/justification provided in a support of the
source argument. Therefore we prioritize the con-
text window of source component when it overlaps
with the target context window. Particularly, we
keep overlapping context sentences in the source
window, and remove them from the target win-
dow. For example, with window-size 1, context
windows of the Claim in sentence 2 in Figure 1
and the Premise in sentence 4 overlap at sentence
3. When the Claim is set as source component, its

7Due to the paragraph constraint and window overlap-
ping, window-size does not indicate the actual context win-
dow size. However, window-size tells what the maximum
size a window can have.

1131



BASELINE

Common features

Word pairs + Production rules

TOPIC

Common features

Topic context features +

Window context featuresWINDOW

Common features

Window context features

COMBINED

Common features

Topic context features +

Window context features +

Word pairs + Production rules

FULL

Common featuresTopic context features

Figure 3: Features used in different models. Feature change across models are denoted by connectors.

context window includes sentences {2, 3}, and the
Premise as a target has context window with only
sentence 4. We extract three Window-context
feature sets from the context windows:

Common word: as common word counts be-
tween adjacent sentences were shown useful for
argument mining (Nguyen and Litman, 2016), we
count common words between the covering sen-
tence with preceding context sentences, and with
following context sentences, for source and target
components.

Discourse relation: for both source and tar-
get components, we extract discourse relations
between context sentences, and within the cov-
ering sentence. We also extract discourse rela-
tions between each pair of source context sen-
tence and target context sentence. Each relation
defines a boolean feature. We extract both Penn
Discourse Treebank (PDTB) relations (Prasad et
al., 2008) and Rhetorical Structure Theory Dis-
course Treebank (RST-DTB) relations (Carlson
et al., 2001) using publicly available discourse
parsers (Ji and Eisenstein, 2014; Wang and Lan,
2015). Each PDTB relation has sense label de-
fined in a 3-layered (class, type, subtype), e.g.,
CONTINGENCY.Cause.result. While there are
only four semantic class labels at the class-level
which may not cover well different aspects of ar-
gumentative relation, subtype-level output is not
available given the discourse parser we use. Thus,
we use relations at type-level as features. For RST-
DTB relations, we use only relation labels, but ig-
nore the nucleus and satellite labels of components
as they do not provide more information given
the component order in the pair. Because tempo-
ral relations were shown not helpful for argument
mining tasks (Biran and Rambow, 2011; Stab and
Gurevych, 2014b), we exclude them here.

Discourse marker: while the baseline model
only considers discourse markers within the ar-
gument components, we define a boolean feature

for each discourse marker classifying whether the
marker is present before the covering sentence of
the source and target components or not. This im-
plementation aims to characterize the discourse of
the preceding and following text segments of each
argument component separately.

Finally, we include the common feature set.

5.4 Combined Model

While Window-context features are extracted
from surrounding text of the argument com-
ponents, which exploits the local context, the
Topic-context features are an abstraction of topic-
dependent information, e.g., domain words are de-
fined within the context of topic domain (Nguyen
and Litman, 2015), and thus make use of the
global context of the topic domain. We believe
that local and global context information repre-
sent complementary aspects of the relation be-
tween argument components. Thus, we expect
to achieve the best performance by combining
Window-context and Topic-context models.

5.5 Full Model

Finally, the FULL model includes all features in
BASELINE and COMBINED models. That is, the
FULL model is the COMBINED model plus word
pairs and production rules. A summary of all mod-
els is shown in Figure 3.

6 Experiments

6.1 Task 1: Support vs. Non-support

Tuning Window-size Parameter
Because our WINDOW model uses a window-
size parameter to form context windows of the
source and target argument components, we in-
vestigate how the window-size of the context win-
dow impacts the prediction performance of the
Window-context features. We set up a model with
only Window-context features and determine the

1132



REPORTED BASELINE TOPIC WINDOW COMBINED FULL
Accuracy 0.863 0.869 0.857 0.857 0.870 0.877
Kappa – 0.445 0.407 0.449 0.507* 0.481
Macro F1 0.722 0.722 0.703 0.724 0.753* 0.739
Macro Precision 0.739 0.758 0.728 0.729 0.754 0.777
Macro Recall 0.705 0.699 0.685 0.720 0.752* 0.715
F1:Support 0.519 0.519 0.488 0.533 0.583* 0.550
F1:Non-support 0.920 0.925 0.917 0.916* 0.923 0.929

Table 2: Support vs. Non-support classification performances on test set. Best values are in bold. Values
smaller than baseline are underlined. * indicates significantly different from the baseline (p < 0.05).

0.62

0.63

0.64

0.65

0.66

0.67

0.68

0.69

0.7

0 1 2 3 4 5 6 7 8

Window-size

F1 score of Window-context Features

Figure 4: Performance of Window-context feature
set by window-size.

window-size in range [0, 8]8 that yields the best
F1 score in 10-fold cross validation. We use the
training set as determined in (Stab and Gurevych,
2014b) to train/test9 the models using LibLINEAR
algorithm (Fan et al., 2008) without parameter or
feature optimization. Cross-validations are con-
ducted using Weka (Hall et al., 2009). We use
Stanford parser (Klein and Manning, 2003) to per-
form text processing. As shown in Figure 4, while
increasing the window-size from 2 to 3 improves
performance (significantly), using window-sizes
greater than 3 does not gain further improvement.
We hypothesize that after a certain limit, larger
context windows will produce more noise than
helpful information for the prediction. Therefore,
we set the window-size to 3 in all of our experi-
ments involving Window-context model (all with
a separate test set).

8Windows-size 0 means covering sentence is the only
context sentence. We experimented with not using context
sentence at all and obtained worse performance. Our data
does not have context window with window-size 9 or larger.

9Note that via cross validation, in each fold some of our
training set serves as a development set.

Performance on Test Set
We train all models using the training set and re-
port their performances on the test set in Table 2.
We also compare our baseline to the reported per-
formance (REPORT) for Support vs. Non-support
classification in (Stab and Gurevych, 2014b). The
learning algorithm with parameters are kept the
same as in the window-size tuning experiment.
Given the skewed class distribution of this data,
Accuracy and F1 of Non-support (the major class)
are less important than Kappa, F1, and F1 of Sup-
port (the minor class). To conduct T-tests for per-
formance significance, we split the test data into
subsets by essays’ ID, and record prediction per-
formance for individual essays.

We first notice that the performances of our
baseline model are better than (or equal to) RE-
PORTED, except the Macro Recall. We reason that
these performance disparities may be due to the
differences in feature extractions between our im-
plementation and Stab and Gurevych’s, and also
due to the minor set of new features (e.g., new
predicted labels, expanded marker set, component
position) that we added in our baseline.

Comparing proposed models with BASELINE,
we see that WINDOW, COMBINED, and FULL
models outperform BASELINE in important met-
rics: Kappa, F1, Recall, but TOPIC yields worse
performances than BASELINE. However, the fact
that COMBINED outperforms BASELINE, espe-
cially with significantly higher Kappa, F1, Recall,
and F1:Support, has shown the value of Topic-
context features. While Topic-context features
alone are not effective, they help improve WIN-
DOW model which supports our hypothesis that
Topic-context and Window-context features are
complementary aspects of context, and they to-
gether obtain better performance.

Comparing our proposed TOPIC, WINDOW,

1133



BASELINE TOPIC WINDOW COMBINED FULL
Accuracy 0.885 0.886 0.872 0.885 0.887
Kappa 0.245 0.305* 0.306* 0.342* 0.274*
Macro F1 0.618 0.651* 0.652* 0.670* 0.634*
Macro Precision 0.680 0.692 0.663 0.697 0.693
Macro Recall 0.595 0.628* 0.644* 0.652* 0.609*
F1:Support 0.937 0.937 0.928* 0.936 0.938
F1:Attack 0.300 0.365* 0.376* 0.404* 0.330*

Table 3: 5×10-fold cross validation performance of Support vs. Attack classification. * indicates signif-
icantly different from the baseline (p < 0.01).

COMBINED models with each other shows that
COMBINED obtains the best performance while
TOPIC performs the worst, which reveals that
Topic-context feature set is less effective than
Window-context set. While FULL model achieves
the best Accuracy, Precision, and F1:Non-support,
it has lower performance than COMBINED model
in important metrics: Kappa, F1, F1:Support. We
reason that the noise caused by word pairs and
production rules even dominate the effectiveness
of Topic-context and Window-context features,
which degrades the overall performance.

Overall, by combining TOPIC and WINDOW
models, we obtain the best performance. Most
notably, we obtain the highest improvement in
F1:Support, and have the best balance between
Precision and Recall values among all models.
These reveal that our contextual features not only
dominate generic features like word pairs and pro-
duction rules, but also are effective to predict mi-
nor positive class (i.e., Support).

6.2 Task 2: Support vs. Attack

To evaluate the robustness of our proposed mod-
els, we conduct an argumentative relation classifi-
cation experiment that classifies a relation as Sup-
port or Attack. Because this task was not stud-
ied in (Stab and Gurevych, 2014b) and the train-
ing/test split for Support vs. Not task is not ap-
plicable here, we conduct 5×10-fold cross valida-
tion. We do not optimize the window-size param-
eter of the WINDOW model, and use the value 3 as
set up before. Average prediction performance of
all models are reported in Table 3.

Comparing our proposed models with the base-
line shows that all of our proposed models sig-
nificantly outperform the baseline in important
metrics: Kappa, F1, F1:Attack. More notably
than in the Support vs. Non-support classifica-

tion, all of our proposed models predict the minor
class (Attack) significantly more effectively than
the baseline. The baseline achieves significantly
higher F1:Support than WINDOW model. How-
ever, F1:Support of the baseline is in a tie with
TOPIC, COMBINED, and FULL.

Comparing our proposed models, we see that
TOPIC and WINDOW models reveal different be-
haviors. TOPIC model has significantly higher
Precision and F1:Support, and significantly lower
Recall and F1:Attack than WINDOW. Moreover,
WINDOW model has slightly higher Kappa, F1,
but significantly lower Accuracy. These com-
parisons indicate that Topic-context and Window-
context features are equally effective but impact
differently to the prediction. The different nature
between these two feature sets is clearer than in
the prior experiment, as now the classification in-
volves classes that are more semantically differ-
ent, i.e., Support vs. Attack. We recall that TOPIC
model performs worse than WINDOW model in
Support vs. Non-support task.

Our FULL model performs significantly worse
than all of TOPIC, WINDOW, and COMBINED in
Kappa, F1, Recall, and F1:Attack. Along with re-
sults from Support vs. Non-support task, this fur-
ther suggests that word pairs and production rules
are less effective and cannot be combined well
with our contextual features.

Despite the fact that the Support vs. Attack task
(Task 2) has smaller and more imbalanced data
than the Support vs. Non-support (Task 1), our
proposed contextual features seem to add even
more value in Task 2 compared to Task 1. Us-
ing Kappa to roughly compare prediction perfor-
mance across the two tasks, we observe a greater
performance improvement from Baseline to Com-
bined model in Task 2 than in Task 1. This is an
evidence that our proposed context-aware features

1134



work well even in a more imbalanced with smaller
data classification task. The lower performance
values of all models in Support vs. Attack than
in Support vs. Non-support indirectly suggest that
Support vs. Attack classification is a more difficult
task. We hypothesize that the difference between
support and attack exposes a deeper semantic re-
lation than that between support and no-relation.
We plan to extract textual text similarity and tex-
tual entailment features (Cabrio and Villata, 2012;
Boltužić and Šnajder, 2014) to investigate this hy-
pothesis in our future work.

7 Conclusions and Future Work

In this paper, we have presented context-aware
argumentative relation mining that makes use of
contextual features by exploiting information from
topic context and context sentences. We have
explored different ways to incorporate our pro-
posed features with baseline features used in a
prior study, and obtained insightful results about
feature effectiveness. Experimental results show
that Topic-context and Window-context features
are both effective but impact predictive perfor-
mance measures differently. In addition, predict-
ing an argumentative relation will benefit most
from combining these two set of features as they
capture complementary aspects of context to bet-
ter characterize the argumentation in justification.

The results obtained in this preliminary study
are promising and encourage us to explore more
directions to enable contextual features. Our next
step will investigate uses of topic segmentation
to identify context sentences and compare this
linguistically-motivated approach to our current
window-size heuristic. We plan to follow prior
research on graph optimization to refine the argu-
mentation structure and improve argumentative re-
lation prediction. Also, we will apply our context-
aware argumentative relation mining to different
argument mining corpora to further evaluate its
generality.

Acknowledgments

This research is supported by NSF Grant 1122504.
We thank the reviewers for their helpful feedback.
We also thank Christian Stab for providing us the
data split for the first experiment.

References
Or Biran and Owen Rambow. 2011. Identifying Jus-

tifications in Written Dialogs by Classifying Text as
Argumentative. International Journal of Semantic
Computing, 5(4):363–381.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet allocation. The Journal of
Machine Learning Research, 3:993–1022.

Filip Boltužić and Jan Šnajder. 2014. Back up
your Stance: Recognizing Arguments in Online Dis-
cussions. In Proceedings of the First Workshop
on Argumentation Mining, pages 49–58, Baltimore,
Maryland, June. Association for Computational Lin-
guistics.

Jill Burstein, Daniel Marcu, and Kevin Knight. 2003.
Finding the WRITE Stuff: Automatic Identification
of Discourse Structure in Student Essays. IEEE In-
telligent Systems, 18(1):32–39, January.

Elena Cabrio and Serena Villata. 2012. Combining
Textual Entailment and Argumentation Theory for
Supporting Online Debates Interactions. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Short Papers
- Volume 2, ACL ’12, pages 208–212, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Elena Cabrio, Sara Tonelli, and Serena Villata.
2013. From Discourse Analysis to Argumentation
Schemes and Back: Relations and Differences. In
Computational Logic in Multi-Agent Systems, vol-
ume 8143 of Lecture Notes in Computer Science,
pages 1–17. Springer Berlin Heidelberg.

Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a Discourse-tagged
Corpus in the Framework of Rhetorical Structure
Theory. In Proceedings of the Second SIGdial Work-
shop on Discourse and Dialogue - Volume 16, SIG-
DIAL ’01, pages 1–10, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

Jianguang Du, Jing Jiang, Liu Yang, Dandan Song, and
Lejian Liao. 2014. Shell Miner: Mining Organiza-
tional Phrases in Argumentative Texts in Social Me-
dia. In Proceedings of the 2014 IEEE International
Conference on Data Mining, ICDM ’14, pages 797–
802, Washington, DC, USA. IEEE Computer Soci-
ety.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. The Jour-
nal of Machine Learning Research, 9:1871–1874,
June.

Eirini Florou, Stasinos Konstantopoulos, Antonis
Koukourikos, and Pythagoras Karampiperis. 2013.
Argument extraction for supporting public policy
formulation. In Proceedings of the 7th Workshop on
Language Technology for Cultural Heritage, Social

1135



Sciences, and Humanities, pages 49–54, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An
Update. ACM SIGKDD Explorations Newsletter,
11(1):10–18, November.

Yangfeng Ji and Jacob Eisenstein. 2014. Representa-
tion Learning for Text-level Discourse Parsing. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 13–24, Baltimore, Maryland,
June. Association for Computational Linguistics.

Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Asso-
ciation for Computational Linguistics.

Ran Levy, Yonatan Bilu, Daniel Hershcovich, Ehud
Aharoni, and Noam Slonim. 2014. Context Depen-
dent Claim Detection. In Proceedings of COLING
2014, the 25th International Conference on Compu-
tational Linguistics: Technical Papers, pages 1489–
1500, Dublin, Ireland, August.

Marco Lippi and Paolo Torroni. 2015. Context-
independent Claim Detection for Argument Mining.
In Proceedings of the 24th International Conference
on Artificial Intelligence, IJCAI’15, pages 185–191,
Buenos Aires, Argentina. AAAI Press.

Nitin Madnani, Michael Heilman, Joel Tetreault, and
Martin Chodorow. 2012. Identifying High-
Level Organizational Elements in Argumentative
Discourse. In Proceedings of the 2012 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 20–28, Montrèal, Canada. As-
sociation for Computational Linguistics.

Marie-Francine Moens, Erik Boiy, Raquel Mochales
Palau, and Chris Reed. 2007. Automatic Detec-
tion of Arguments in Legal Texts. In Proceedings of
the 11th International Conference on Artificial Intel-
ligence and Law, ICAIL ’07, pages 225–230, New
York, NY, USA. ACM.

Huy Nguyen and Diane Litman. 2015. Extracting
Argument and Domain Words for Identifying Ar-
gument Components in Texts. In Proceedings of
the 2nd Workshop on Argumentation Mining, pages
22–28, Denver, CO, June. Association for Computa-
tional Linguistics.

Huy Nguyen and Diane Litman. 2016. Improving
argument mining in student essays by learning and
exploiting argument indicators versus essay topics.
In Proceedings 29th International FLAIRS Confer-
ence, Key Largo, FL, May.

Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation Mining: The Detection, Clas-
sification and Structure of Arguments in Text. In
Proceedings of the 12th International Conference on
Artificial Intelligence and Law, ICAIL ’09, pages
98–107, New York, NY, USA. ACM.

Joonsuk Park and Claire Cardie. 2014. Identifying Ap-
propriate Support for Propositions in Online User
Comments. In Proceedings of the First Workshop
on Argumentation Mining, pages 29–38, Baltimore,
Maryland, June. Association for Computational Lin-
guistics.

Andreas Peldszus and Manfred Stede. 2013. From
Argument Diagrams to Argumentation Mining in
Texts: A Survey. International Journal of Cogni-
tive Informatics and Natural Intelligence (IJCINI),
7(1):1–31, January.

Andreas Peldszus and Manfred Stede. 2015a. Joint
prediction in MST-style discourse parsing for ar-
gumentation mining. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 938–948, Lis-
bon, Portugal, September. Association for Compu-
tational Linguistics.

Andreas Peldszus and Manfred Stede. 2015b. To-
wards Detecting Counter-considerations in Text. In
Proceedings of the 2nd Workshop on Argumentation
Mining, pages 104–109, Denver, CO, June. Associ-
ation for Computational Linguistics.

Andreas Peldszus. 2014. Towards segment-based
recognition of argumentation structure in short texts.
In Proceedings of the First Workshop on Argumen-
tation Mining, pages 88–97, Baltimore, Maryland,
June. Association for Computational Linguistics.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC-08),
Marrakech, Morocco, May. European Language Re-
sources Association (ELRA). ACL Anthology Iden-
tifier: L08-1093.

Vahed Qazvinian and Dragomir R. Radev. 2010. Iden-
tifying Non-explicit Citing Sentences for Citation-
based Summarization. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 555–564,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Diarmuid Ó Séaghdha and Simone Teufel. 2014. Un-
supervised learning of rhetorical structure with un-
topic models. In Proceedings of the 25th Inter-
national Conference on Computational Linguistics
(COLING-14), Dublin, Ireland.

Alan Sergeant. 2013. Automatic argumentation ex-
traction. In The 10th European Semantic Web

1136



Conference, pages 656–660, Montpellier, France.
Springer.

Christian Stab and Iryna Gurevych. 2014a. Anno-
tating Argument Components and Relations in Per-
suasive Essays. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: Technical Papers, pages 1501–1510,
Dublin, Ireland. Dublin City University and Asso-
ciation for Computational Linguistics.

Christian Stab and Iryna Gurevych. 2014b. Identifying
Argumentative Discourse Structures in Persuasive
Essays. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 46–56, Doha, Qatar. Association
for Computational Linguistics.

Christian Stab, Christian Kirschner, Judith Eckle-
Kohler, and Iryna Gurevych. 2014. Argumentation
Mining in Persuasive Essays and Scientific Articles
from the Discourse Structure Perspective. In Elena
Cabrio, Serena Villata, and Adam Wyner, editors,
Proceedings of the Workshop on Frontiers and Con-
nections between Argumentation Theory and Natu-
ral Language Processing, pages 40–49, Bertinoro,
Italy, July. CEUR-WS.

Simone Teufel and Marc Moens. 2002. Summariz-
ing Scientific Articles: Experiments with Relevance
and Rhetorical Status. Computational Linguistics,
28(4), December.

Douglas Walton, Christopher Reed, and Fabrizio
Macagno. 2008. Argumentation Schemes. Cam-
bridge University Press.

Jianxiang Wang and Man Lan. 2015. A Refined
End-to-End Discourse Parser. In Proceedings of
the Nineteenth Conference on Computational Natu-
ral Language Learning - Shared Task, pages 17–24,
Beijing, China, July. Association for Computational
Linguistics.

1137


