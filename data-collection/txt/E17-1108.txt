



















































Structured Learning for Temporal Relation Extraction from Clinical Records


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1150–1158,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Structured Learning for
Temporal Relation Extraction from Clinical Records

Artuur Leeuwenberg and Marie-Francine Moens
Department of Computer Science

KU Leuven, Belgium
{tuur.leeuwenberg, sien.moens}@cs.kuleuven.be

Abstract

We propose a scalable structured learning
model that jointly predicts temporal rela-
tions between events and temporal expres-
sions (TLINKS), and the relation between
these events and the document creation
time (DCTR). We employ a structured per-
ceptron, together with integer linear pro-
gramming constraints for document-level
inference during training and prediction to
exploit relational properties of temporal-
ity, together with global learning of the re-
lations at the document level. Moreover,
this study gives insights in the results of in-
tegrating constraints for temporal relation
extraction when using structured learning
and prediction. Our best system outper-
forms the state-of-the art on both the CON-
TAINS TLINK task, and the DCTR task.

1 Introduction

Temporal information is critical in many clinical
areas (Combi and Shahar, 1997). A big part of this
temporal information is captured in the free text
of patient records. The current work aims to im-
prove temporal information extraction from such
clinical texts. Extraction of temporal information
from clinical text records can be used to construct
a time-line of the patient’s condition (such as in
Figure 1). The extracted time-line can help clini-
cal researchers to better select and recruit patients
with a certain history for clinical trials. Moreover,
the time-line is crucial for making a good patient
prognosis and clinical decision support (Onisko et
al., 2015; Stacey and McGregor, 2007).

Temporal information extraction can be divided
into three sub-problems: (1) the detection of
events (Ee); (2) the detection of temporal expres-
sions (Et); and (3) the detection of temporal rela-

tions between them. In the clinical domain, events
include medical procedures, treatments, or symp-
toms (e.g. colonoscopy, smoking, CT-scan). Tem-
poral expressions include dates, days of the week,
months, or relative expressions like yesterday, last
week, or post-operative. In this work, we focus on
the last sub-problem, extraction of temporal rela-
tions (assuming events and temporal expressions
are given). As a small example of the task we aim
to solve, given the following sentence:

In 1990 the patient was diagnosed and
received surgery directly afterwards.

in which we assume that the events diagnosed
and adenocarcinoma, and the temporal expression
1990 are given, we wish to extract the following
relations:

• CONTAINS(1990, diagnosed)

• CONTAINS(1990, surgery)

• BEFORE(diagnosed, surgery)

• BEFORE(diagnosed, d)

• BEFORE(surgery, d)

where d stands for the document creation time.
Our work leads to the following contributions:

First, we propose a scalable structured learning
model that jointly predicts temporal relations be-
tween events and temporal expressions (TLINKS),
and the relation between these events and the doc-
ument creation time (DCTR). In contrast to ex-
isting approaches which detect relation instances
separately, our approach employs a structured per-
ceptron (Collins, 2002) for global learning with
joint inference of the temporal relations on a
document level. Second, we ensure scalability
through using integer linear programming (ILP)

1150



Figure 1: Fragment of a (partial) patient time-line.

constraints with fast solvers, loss augmented sub-
sampling, and good initialization. Third, this
study leads to valuable insights on when and how
to make inferences over the found candidate rela-
tions both during training and prediction and gives
an in-depth assessment of the use of additional
constraints and global features during inference.
Finally, our best system outperforms the state-of-
the-art of both the CONTAINS TLINK task, and the
DCTR task.

2 Related Work

There have been two shared tasks on the topic of
temporal relation extraction in the clinical domain:
the I2B2 Temporal Challenge (Sun et al., 2013),
and more recently the Clinical TempEval Shared
Task with two iterations, one in 2015 and one in
2016 (Bethard et al., 2014; Bethard et al., 2015;
Bethard et al., 2016). In the I2B2 Temporal Chal-
lenge eight types of relations were initially anno-
tated. However, due to low inter-annotator agree-
ment these were merged to three types of temporal
relations, OVERLAP, BEFORE, and AFTER. Good
annotation of temporal relations is difficult, as an-
notators frequently miss relation mentions. In the
Clinical TempEval Shared tasks the THYME cor-
pus is used (Styler IV et al., 2014), with a dif-
ferent annotation scheme that aims at annotating
those relations that are most informative w.r.t. the
time-line, and gives less priority to relations that
can be inferred from the others. This results in
two categories of temporal relations: The rela-
tion between each event and the document creation
time (DCTR), dividing all events in four temporal
buckets (BEFORE, BEFORE/OVERLAP,OVERLAP,
AFTER). These buckets are called narrative con-
tainers (Pustejovsky and Stubbs, 2011). And sec-
ond, relations between temporal entities that both
occur in the text (TLINKS). TLINKS may occur
between events (Ee × Ee), and between events

and temporal expressions (Ee ×Et and Et ×Ee).
The TLINK types (and their relative frequency
in the THYME corpus) are CONTAINS (64,42%),
OVERLAP (15,19%), BEFORE (12,65%), BEGINS-
ON (6.15%), and ENDS-ON (1.59%). The rela-
tions AFTER, and DURING are expressed in terms
of their inverse, BEFORE, and CONTAINS respec-
tively. In our experiments, we use the THYME
corpus for its relatively high inter-annotator agree-
ment (particularly for CONTAINS).

To our knowledge, in all submissions (4 in
2015, and 10 in 2016) of Clinical TempEval the
task is approached as a classical entity-relation ex-
traction problem, and the predictions for both cat-
egories of relations are made independently from
each other, or in a one way dependency, where the
containment classifier uses information about the
predicted document-time relation. Narrative con-
tainment, temporal order, and document-time rela-
tion have very strong dependencies. Not modeling
these may result in inconsistent output labels, that
do not result in a consistent time-line.

An example of inconsistent labeling is given in
Figure 2. The example is inconsistent when as-
signing the AFTER label for the relation between
lesion and the document-time. It is inconsis-
tent because we can also infer that lesion occurs
BEFORE the document-time, as the colonoscopy
event occurs before the document-time, and the le-
sion is contained by the colonoscopy.

Temporal inference, in particular temporal clo-
sure, is frequently used to expand the training data
(Mani et al., 2006; Chambers and Jurafsky, 2008;
Lee et al., 2016; Lin et al., 2016b), most of the
times resulting in an increase in performance, and
is also taken into account when evaluating the pre-
dicted labels (Bethard et al., 2014; UzZaman and
Allen, 2011). Only very limited research regards
the modeling of temporal dependencies into the
machine learning model. (Chambers and Juraf-

1151



(event) (timex3) (event)
A colonoscopy on September 27, 2008 revealed a circumferential lesion .

BEFORE AFTER

contains

contains

Figure 2: Example of inconsistent output labeling. Containment is indicated by directed edges, and the
relation to the document-time by small caps below the events.

sky, 2008) and (Do et al., 2012) modeled label de-
pendencies when predicting TimeBank TLINKS
(Pustejovsky et al., 2003). They trained local
classifiers and used a set of global temporal label
constraints. Integer linear programming was em-
ployed to maximize the score from the local clas-
sifiers, while satisfying the global label constraints
at prediction time. For both, this gave a significant
increase in performance, and resulted in consistent
output labels.

(Yoshikawa et al., 2009) modeled the label
dependencies between TLINKS and DCTR with
Markov Logic Networks (MLN), allowing for soft
label constraints during training and prediction.
However, MLN can sometimes be sub-optimal for
text mining tasks w.r.t. time efficiency (Mojica
and Ng, 2016). Quite recently, for a similar prob-
lem, spatial relation extraction, (Kordjamshidi et
al., 2015) used an efficient combination of a struc-
tured perceptron or structured support vector ma-
chine with integer linear programming. In their
experiments, they compare a local learning model
(LO), a local learning model with global inference
at prediction time (L+I), and a structured learn-
ing model with and without inference during train-
ing (IBT+I, and IBT-I respectively). In their ex-
periments L+I gave better results than LO, but a
more significant improvement was made when us-
ing structured learning in contrast to local learn-
ing.

In this work, we aim to jointly predict TLINKS
and DCTR in a structured learning model with in-
ference during training and prediction, to assess
inference with temporal constraints of (Cham-
bers and Jurafsky, 2008; Do et al., 2012) for the
THYME relations, and to experiment with both
local, and document-level inference for temporal
information extraction in the clinical domain.

3 The Model

For jointly learning both tasks on a document
level, we employ a structured perceptron learning
paradigm (Collins, 2002). The structured percep-
tron model uses a joint feature function Φ(X,Y )
to represent a full input document X with a label
assignment Y . During training the model learns a
weight vector λ to score how good the label as-
signment is. Predicting label assignment Y for
a document X corresponds to finding the Y with
the maximal score. In the following sub-sections
we define the joint feature function Φ, describe
the prediction procedure of the model, and provide
how we train the model (i.e. learn a good λ).

3.1 Joint Features

To compose the joint feature function, we first de-
fine two local feature functions: φtl : (x, y) →
Rp assigns features for the local classifications
regarding TLINKS (with possible labels Ltl =
{CONTAINS, BEFORE, OVERLAP, BEGINS-ON,
ENDS-ON, NO LABEL}), and a second local fea-
ture function φdr : (x, y) → Rq, for local
features regarding document-time relation clas-
sification (with labels Ldr = {BEFORE, BE-
FORE OVERLAP, OVERLAP, AFTER}. The fea-
tures used by these local feature functions are
given in Table 1.

From these, we define a joint feature func-
tion Φjoint : (X,Y ) → Rp+q, that concate-
nates (⊕) the summed local feature vectors, cre-
ating the feature vector for the global prediction
task (predicting all labels in the document for both
sub-tasks at once). Φjoint is defined in Equa-
tion 1, where Ctl(X) and Cdr(X) are candidate
generation functions for the TLINK sub-task, and
DCTR sub-task respectively (further explained in
Section 3.2).

1152



Features φdr φtl
String features for tokens and POS of each entity X X
String features for tokens and POS in a window of size {3, 5}, left and right of each entity X X
Boolean features for entity attributes (event polarity, event modality, event degree, and type) X X
String feature for the token and POS of the closest verb X
String feature for the token and POS of the closest left and right entity X
String features for the token {1, 2, 3}-grams and POS {1, 2, 3}-grams in-between the two entities X
Dependency path between entities (consisting of POS and edge labels) X
Boolean feature on if the first argument occurs before the second (w.r.t. word order) X

Table 1: Features of the local feature functions of each sub-task, φtl for TLINKS, and φdr for DCTR.

Φjoint(X,Y ) =
∑

x∈Cdr(X)

∑
l∈Ldr

φdr(x, l)

⊕
∑

x∈Ctl(X)

∑
l∈Ltl

φtl(x, l) (1)

3.2 Local Candidate Generation
For each document X , we create local candi-
dates for both sub-tasks. In this work, we as-
sume that event (Ee) and temporal expression
(Et) annotations are provided in the input. The
DCTR-candidates in document X are then given
by Cdr(X), which returns all events in the docu-
ment, i.e. Ee(X). Ctl(X) returns all TLINK can-
didates, i.e. Ee(X) ∪ Et(X) × Ee(X). In our
experiments we usually restrict the number of can-
didates generated by Ctl to gain training and pre-
diction speed (without significant loss in perfor-
mance). This is explained further in Section 4.3.

3.3 Global Features
We also experiment with a set of global features,
by which we mean features that are expressed in
terms of multiple local labels. The global features
are specified in Table 2. Global features are de-
fined by a feature function Φglobal(X,Y ) → Rr
and have their corresponding weights in weight
vector λ. When using global features Φglobal is
concatenated with the joint feature function Φjoint
to form the final feature function Φ, as show in in
Equation 2.

Φ(X,Y ) = Φjoint(X,Y )⊕ Φglobal(X,Y ) (2)
When not using global features, we use only the
joint features, as shown in Equation 3.

Φ(X,Y ) = Φjoint(X,Y ) (3)

Feature Description

Φsdr Bigram and trigram counts of subsequent
DCTR-labels in the document

Φdrtl Counts of DCTR-label pairs of the
entities of each TLINK

Table 2: Global (document-level) features.

3.4 Prediction

The model assigns a score to each input document
X together with output labeling Y . The score
for (X,Y ) is defined as the dot product between
the learned weight vector λ and the outcome of
the joint feature function Φ(X,Y ), as shown in
Equation 4.

S(X,Y ) = λΦ(X,Y ) (4)

The prediction problem for an input document X
is finding the label assignment Y that maximizes
the score S based on the weight vector λ, shown
in Equation 5.

Ŷk = arg max
Y

S(X,Y ) (5)

We use integer linear programming (ILP) to solve
the prediction problem in Equation 5. Each pos-
sible local decision is modeled with a binary de-
cision variable. For each local relation candidate
input xi,j (for the relation between i and j) a bi-
nary decision variable wli,j is used for each poten-
tial label l that could be assigned to xi,j , depend-
ing on the sub-task. The objective of the integer
linear program, given in Equation 6, is to maxi-
mize the sum of the scores of local decisions. In
all equations the constant d refers to the document-
creation time. The objective is maximized under
two sets of constraints, given in Equations 7 and
8, that express that each candidate is assigned ex-

1153



actly one label, for each sub-task.

O = arg max
W

∑
xi,d∈Cdr(X)

∑
l∈Ldr

wli,d·S(xi,d, yli,d)

+
∑

xi,j∈Ctl(X)

∑
l∈Ltl

wli,j · S(xi,j , yli,j) (6)

∀i :
∑

l∈Ldr
wli,d = 1 (7)

∀i,j :
∑
l∈Ltl

wli,j = 1 (8)

For solving the integer linear program we use
Gurobi (Gurobi Optimization, 2015).

3.4.1 Temporal Label Constraints
Because temporal relations are interdependent, we
experimented with using additional constraints on
the output labeling. The additional temporal con-
straints we experiment with are shown in Table 3.
Constraints are expressed in terms of the binary
decision variables used in the integer linear pro-
gram.

In Table 3, constraints CCtrans, and CBtrans
model transitivity of CONTAINS, and BEFORE re-
spectively. Constraints CCBB , and CCAA model
the consistency between TLINK relation CON-
TAINS and DCTR relations BEFORE, and AFTER
respectively (resolving the inconsistent example
of CCBB in section 1, and Figure 2). Similarly,
CBBB , and CBAA model the consistency between
TLINK relation BEFORE and DCTR relations BE-
FORE, and AFTER.

Constraints can be applied during training and
prediction, as Equation 5 is to be solved for both.
If not mentioned otherwise, we use constraints
both during training and prediction.

3.5 Training
The training procedure for the averaged structured
perceptron is given by Algorithm 1, for I itera-
tions, on a set of training documents T . Notice
that the prediction problem is also present during
training, in line 6 of the algorithm. Weight vec-
tor λ is usually initialized with ones, and updated
when the predicted label assignment Ŷk for input
documentXk is not completely correct. The struc-
tured perceptron training may suffer from over-
fitting. Averaging the weights over the training ex-
amples of each iteration is a commonly used way
to counteract this handicap (Collins, 2002; Freund

and Schapire, 1999). In Algorithm 1, c is used to
count the number of training updates, and λa as a
cache for averaging the weights. We also employ
local loss-augmented negative sub-sampling, and
local pre-learning to address class-imbalance and
training time.

Algorithm 1 Averaged Structured Perceptron
Require: λ, λa, c, I, T
1: c← 0
2: λ← 〈1, . . . , 1〉
3: λa ← 〈1, . . . , 1〉
4: for i in I do
5: for k in T do
6: Ŷk ← arg max

Y
λΦ(Xk, Y )

7: if Ŷk 6= Yk then
8: λ← λ+ Φ(Xk, Yk)− Φ(Xk, Ŷk)
9: λa ← λa + c · Φ(Xk, Yk)− c · Φ(Xk, Ŷk)

10: c← c+ 1
return λ− λa/c

3.5.1 Loss-augmented Negative
Sub-sampling

For the TLINK sub-task, we have a very large neg-
ative class (NO LABEL) and a relatively small pos-
itive class (the other TLINK labels) of training ex-
amples. To speed up training convergence and ad-
dress class imbalance at the same time, we sub-
sample negative examples during training. Within
a document X , for each positive local training
example (xpositive, ypositive) we take 10 random
negative examples and add the negative example
(xnegative, yno label) with the highest score for re-
lation ypositive, i.e. S(xnegative, ypositive). This
cutting plane optimization gives preference to neg-
ative training examples that are more likely to be
classified wrongly, and thus can be learned from
(in an online manner), and it provides only one
negative training example for each positive train-
ing example, balancing the TLINK classes.

3.5.2 Local Initialization
To reduce training time, we don’t initialize λ with
ones, but we train a perceptron for both local sub-
tasks, based on the same local features mentioned
in Table 1, and use the trained weights to initial-
ize λ for those features. A similar approach was
used by (Weiss et al., 2015) for dependency pars-
ing. Details on the training parameters of the per-
ceptron are given in Section 4.3.

4 Experiments

We use our experiments to look at the effects of
four modeling settings.

1154



Abbrev. Label Dependencies Constraints

CCtrans CONTAINSi,j ∧ CONTAINSj,k → CONTAINSi,k ∀i,j,k : wcontainsi,k − wcontainsi,j − wcontainsj,k ≥ −1
CBtrans BEFOREi,j ∧ BEFOREj,k → BEFOREi,k ∀i,j,k : wbeforei,k − wbeforei,j − wbeforej,k ≥ −1
CCBB CONTAINSi,j ∧ BEFOREi,d → BEFOREj,d ∀i,j : wbeforej,d − wcontainsi,j − wbeforei,d ≥ −1
CCAA CONTAINSi,j ∧ AFTERi,d → AFTERj,d ∀i,j : wafterj,d − wcontainsi,j − wafteri,d ≥ −1
CBBB BEFOREi,j ∧ BEFOREj,d → BEFOREi,d ∀i,j : wbeforei,d − wbeforei,j − wbeforej,d ≥ −1
CBAA BEFOREi,j ∧ AFTERi,d → AFTERj,d ∀i,j : wafterj,d − wbeforei,j − wafteri,d ≥ −1

Table 3: Temporal label dependencies expressed as integer linear programming constraints. The vari-
ables i, j and k range over the corresponding TLINK arguments, and constant d refers to the document-
creation-time. CONTAINSi,j indicates that entity i contains entity j.

1. Document-level learning in contrast to pair-
wise entity-relation learning.

2. Joint learning of DCTR and TLINKS.

3. Integrating temporal label constraints.

4. Using global structured features.

We will discuss our results in Section 4.4. But
first, we describe how we evaluate our system, and
provide information on our baselines, and the pre-
processing and hyper-parameter settings used in
the experiments.

4.1 Evaluation
We evaluate our method on the clinical notes test
set of the THYME corpus (Styler IV et al., 2014),
also used in the Clinical TempEval 2016 Shared
Task (Bethard et al., 2016). Some statistics about
the dataset can be found in Table 4. F-measure
is used as evaluation metric. For this we use the
evaluation script from the Clinical TempEval 2016
Shared Task. TLINKS are evaluated under the
temporal closure (UzZaman and Allen, 2011).

Section Documents TLINKS EVENTS

Train 440 17.109 38.872
Test 151 8.903 18.989

Table 4: Dataset statistics for the THYME sec-
tions we used in our experiments.

4.2 Baselines
Our first baseline is a perceptron algorithm,
trained for each local task using the same local
features as used to compose the joint feature func-
tion Φjoint of our structured perceptron. We have

two competitive state-of-the-art baselines, one for
the DCTR sub-task, and one for the TLINK sub-
task. The first baseline is the best performing
system of the Clinical TempEval 2016 on the
DCTR task (Khalifa et al., 2016). They exper-
iment with a feature rich SVM and a sequential
conditional random field (CRF) for the prediction
of DCTR and report the – to our knowledge –
highest performance on the DCTR task. The com-
petitive TLINK baseline is the latest version of the
cTAKES Temporal system (Lin et al., 2016b; Lin
et al., 2016a). They employ two SVMS to pre-
dict TLINKS, one for TLINKS between events,
and one for TLINKS between events and tempo-
ral expressions and recently improved their sys-
tem by generating extra training data using ex-
tracted UMLS concepts. They report the – to our
knowledge – highest performance on CONTAINS
TLINKS in the THYME corpus.

4.3 Hyper-parameters and Preprocessing

In all experiments, we preprocess the text by us-
ing a very simple tokenization procedure consid-
ering punctuation1 or newline tokens as individ-
ual tokens, and splitting on spaces. For our part-
of-speech (POS) features, and dependency parse
path features, we rely on the cTAKES POS tag-
ger and cTAKES dependency parser respectively
(Savova et al., 2010). After POS tagging and pars-
ing we lowercase the tokens. As mentioned in
Section 3.2, we restrict our TLINK candidate gen-
eration in two ways. First, both entities should
occur in a token window of 30, selected from
{20, 25, 30, 35, 40} based on development set per-
formance. And second, both entities should occur
in the same paragraph (paragraphs are separated

1, ./\"’=+-;:()!?<>%&$*|[]{}

1155



by two consecutive newlines). Our motivation for
not using sentence based candidate generation is
that the clinical records contain many ungrammat-
ical phrases, bullet point enumerations, and tables
that may result in missing cross-sentence relation
instances (Leeuwenberg and Moens, 2016). In all
experiments, we train the normal perceptron for 8
iterations, and the structured perceptron for 32 it-
erations, both selected from {1, 2, 4, 8, 16, 32, 64}
based on best performance on the development set.
The baseline perceptron is also used for the initial-
ization of the structured perceptron. Moreover, we
apply the transitive closure of CONTAINS, and BE-
FORE on the training data.

4.4 Results
Our experimental results on the THYME test set
are reported in Table 5. In the table, the abbrevia-
tion SP refers to the structured perceptron model
described in Section 3 but without temporal la-
bel constraints or global features, i.e. the joint
document-level unconstrained structured percep-
tron, using local initialization, and loss-augmented
negative sub-sampling. We compare this model
with a number of modified versions to explore the
effect of the modifications.

4.4.1 Document-Level Learning
When we compare the local perceptron base-
line with any of the document-level models
(any SP variation), we can clearly see that
learning the relations at a document-level im-
proves our model significantly2 (P<0.0001 for
both DCTR and TLINKS). Furthermore, when
comparing loss-augmented sub-sampling (SP)
with random sub-sampling of negative TLINKS
(SPrandom sub-sampling) it can be seen that a good
selection of negative training instances is very
important for learning a good model (again
P<0.0001), and resulted in our model to im-
prove the state-of-the-art by 1.4 on the CONTAINS
TLINK task3.

4.4.2 Jointly Learning DCTR and TLINKS
When comparing the disjoint model (SPdisjoint)
with our joint model (SP) it can be noticed that
joint prediction gives only a very small improve-
ment (P=0.0768 for TLINKS, and P=0.0451 for

2Significance is based on a document-level paired t-test.
3Only CONTAINS is generally reported for the THYME

corpus, as the other TLINKS are less frequent, and the inter-
annotator agreement for them is very low. We included them
just for completeness in our experiments.

DCTR). However, joint learning on a document
level provides the flexibility to formulate con-
straints connecting the labels of both tasks, such
as the last four constraints in Table 3, resulting in
a more consistent labeling over both tasks. Sim-
ilarly, in the joint learning setting, we can define
global features that connect both tasks (like Φdrtl).

4.4.3 Integrating Temporal Constraints
We experimented with integrating label con-
straints in two ways (1) both during training and
prediction (SPcc + C∗), or (2) only during predic-
tion (SPuc + C∗). In general it can be noticed
that in our experiments using the temporal label
constraints from Table 3 slightly increases DCTR
performance, but slightly decreases TLINK per-
formance. A reason for this can be that the model
generally gives better predictions for DCTR, that
might result in providing a better alternative to
a constraint violating solution. A difference in
consistency of the annotation between both tasks
could also be a reason. Furthermore, we can
see that integrating the constraints both during
training and prediction gives slightly lower per-
formance compared only integrating them during
prediction.

4.4.4 Using Global Structured Features
We have two types of features, Φsdr, which is
only based on DCTR labels, and Φdrtl, which is
based on a combination of DCTR and TLINK la-
bels. When we add Φsdr to our model, the overall
F-measure on the DCTR task improves with 1.3
points (P<0.0001), improving the state-of-the-art
by 0.3 points. A reason for this can be the sequen-
tial dependency of DCTR labels, also exploited by
(Khalifa et al., 2016), using the sequential CRF.
The second global feature, Φdrtl, in fact models
the same type of dependencies as the last four con-
straints in Table 3, relating the TLINK relations
with the DCTR labels of each TLINK argument,
however as a soft dependency and not as a hard
constraint. In our experiments, this feature did
not improve either of the two sub-tasks. It ap-
pears that training with cross-task constraints, or
global cross-task features is not trivial, and fur-
ther research is needed on how to exploit these
cross-task dependencies also during training. We
assume that the lower-than-expected scores when
modeling cross-task dependencies may be related
to sub-sampling the negative TLINK training in-
stances.

1156



System F DCTRBEFORE F DCTRAFTER F DCTROVERLAP F DCTRBEFORE/OVERLAP F
DCTR
ALL F

TLINK
CONTAINS F

TLINK
BEFORE F

TLINK
OVERLAP F

TLINK
BEGINS-ON F

TLINK
ENDS-ON F

TLINK
ALL

Baseline: perceptron 0.776 0.744 0.769 0.528 0.759 0.456 0.147 0.073 0.060 0.024 0.364
(Khalifa et al., 2016) - - - - 0.843 - - - - -
(Lin et al., 2016b) - - - - - 0.594 - - - - -

SP 0.837 0.805 0.860 0.575 0.833 0.608 0.294 0.185 0.158 0.231 0.518
SPrandom sub-sampling 0.837 0.803 0.859 0.575 0.833 0.564 0.275 0.204 0.154 0.218 0.490
SPdisjoint 0.835 0.801 0.859 0.576 0.832 0.607 0.290 0.183 0.146 0.232 0.516
SPcc + C∗ 0.843 0.810 0.861 0.573 0.836 0.603 0.292 0.186 0.148 0.222 0.514
SPuc + C∗ 0.843 0.814 0.861 0.574 0.837 0.606 0.291 0.184 0.157 0.236 0.516
SP + Φsdr 0.856 0.830 0.867 0.569 0.846 0.608 0.291 0.182 0.159 0.222 0.518
SP + Φdrtl 0.838 0.811 0.855 0.564 0.831 0.605 0.286 0.176 0.147 0.217 0.514

Table 5: Results on the THYME test set. SP refers to our structured perceptron model, without con-
straints or global features, using local initialization and loss-augmented negative sub-sampling. C∗ refers
to using all constraints. Superscript CC and UC refer to using constraints at training and prediction time,
or only at prediction time respectively.

5 Conclusions

In this work, we proposed a structured per-
ceptron model for learning temporal relations
between events and the document-creation time
(DCTR), and between temporal entities in the
text (TLINKS) in clinical records. Our model
efficiently learns and predicts at a document
level, exploiting loss-augmented negative sub-
sampling, and uses global features allowing
it to exploit relations between local output la-
bels. For construction of a consistent output
labeling, needed for time-line construction, we
formulated a number of constraints, including
those from (Chambers et al., 2007; Do et al.,
2012), and assessed them during inference. Our
best system outperforms the state-of-the-art
of both the CONTAINS TLINK task, and the
DCTR task. Our code for this work is available at
https://github.com/tuur/SPTempRels.

Acknowledgment

The authors would like to thank the reviewers
for their constructive comments which helped us
to improve the paper. Also, we would like to
thank the Mayo Clinic for permission to use the
THYME corpus. This work was funded by the
KU Leuven C22/15/16 project ”MAchine Read-
ing of patient recordS (MARS)”, and by the IWT-
SBO 150056 project ”ACquiring CrUcial Medical
information Using LAnguage TEchnology” (AC-
CUMULATE).

References
Steven Bethard, Leon Derczynski, James Pustejovsky,

and Marc Verhagen. 2014. Clinical tempeval. arXiv
preprint arXiv:1403.4928.

Steven Bethard, Leon Derczynski, Guergana Savova,
Guergana Savova, James Pustejovsky, and Marc
Verhagen. 2015. Semeval-2015 task 6: Clinical
tempeval. In Proceedings of SemEval, pages 806–
814. Association for Computational Linguistics.

Steven Bethard, Guergana Savova, Wei-Te Chen, Leon
Derczynski, James Pustejovsky, and Marc Verha-
gen. 2016. Semeval-2016 task 12: Clinical tem-
peval. pages 1052–1062. Association for Computa-
tional Linguistics.

Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal
ordering. In Proceedings of EMNLP, pages 698–
706. Association for Computational Linguistics.

Nathanael Chambers, Shan Wang, and Dan Juraf-
sky. 2007. Classifying temporal relations between
events. In Proceedings of ACL, pages 173–176. As-
sociation for Computational Linguistics.

Michael Collins. 2002. Ranking algorithms for
named-entity extraction: Boosting and the voted
perceptron. In Proceedings of ACL, pages 489–496.
Association for Computational Linguistics.

Carlo Combi and Yuval Shahar. 1997. Temporal rea-
soning and temporal data maintenance in medicine:
issues and challenges. Computers in Biology and
Medicine, 27(5):353–368.

Harold Charles Daume. 2006. Practical Structured
Learning Techniques for Natural Language Process-
ing. ProQuest.

Quang Xuan Do, Wei Lu, and Dan Roth. 2012. Joint
inference for event timeline construction. In Pro-
ceedings of EMNLP, pages 677–687. Association
for Computational Linguistics.

Yoav Freund and Robert E Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277–296.

Inc. Gurobi Optimization. 2015. Gurobi optimizer ref-
erence manual.

1157



Abdulrahman Khalifa, Sumithra Velupillai, and
Stephane Meystre. 2016. Utahbmi at SemEval-
2016 task 12: Extracting temporal information from
clinical text. Proceedings of SemEval, pages 1256–
1262.

Parisa Kordjamshidi, Dan Roth, and Marie-Francine
Moens. 2015. Structured learning for spatial in-
formation extraction from biomedical text: bacteria
biotopes. BMC Bioinformatics, 16(1):1.

Hee-Jin Lee, Yaoyun Zhang, Jun Xu, Sungrim Moon,
Jingqi Wang, Yonghui Wu, and Hua Xu. 2016.
UTHealth at SemEval-2016 task 12: an end-to-
end system for temporal information extraction from
clinical notes. Proceedings of SemEval, pages
1292–1297.

Artuur Leeuwenberg and Marie-Francine Moens.
2016. KULeuven-LIIR at SemEval 2016 task 12:
Detecting narrative containment in clinical records.
Proceedings of SemEval, pages 1280–1285.

Chen Lin, Dmitriy Dligach, Timothy A Miller, Steven
Bethard, and Guergana K Savova. 2016a. Multi-
layered temporal modeling for the clinical domain.
Journal of the American Medical Informatics Asso-
ciation, 23(2):387–395.

Chen Lin, Timothy Miller, Dmitriy Dligach, Steven
Bethard, and Guergana Savova. 2016b. Improving
temporal relation extraction with training instance
augmentation. Proceedings of ACL, page 108.

Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006. Ma-
chine learning of temporal relations. In Proceedings
of COLING–ACL, pages 753–760. Association for
Computational Linguistics.

Luis Gerardo Mojica and Vincent Ng. 2016. Markov
logic networks for text mining: A qualitative and
empirical comparison with integer linear program-
ming. In Proceedings of LREC, pages 4388–4395.

Agnieszka Onisko, Allan Tucker, and Marek J.
Druzdzel, 2015. Prediction and Prognosis of Health
and Disease, pages 181–188. Springer International
Publishing, Cham.

James Pustejovsky and Amber Stubbs. 2011. Increas-
ing informativeness in temporal annotation. In Pro-
ceedings of the 5th Linguistic Annotation Workshop,
pages 152–160. Association for Computational Lin-
guistics.

James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al. 2003. The TimeBank corpus. In Cor-
pus Linguistics, volume 2003, page 40.

Guergana K Savova, James J Masanz, Philip V Ogren,
Jiaping Zheng, Sunghwan Sohn, Karin C Kipper-
Schuler, and Christopher G Chute. 2010. Mayo

clinical text analysis and knowledge extraction sys-
tem (cTAKES): architecture, component evaluation
and applications. Journal of the American Medical
Informatics Association, 17(5):507–513.

Michael Stacey and Carolyn McGregor. 2007. Tempo-
ral abstraction in intelligent clinical data analysis: A
survey. Artificial Intelligence in Medicine, 39(1):1–
24.

William F Styler IV, Steven Bethard, Sean Finan,
Martha Palmer, Sameer Pradhan, Piet C de Groen,
Brad Erickson, Timothy Miller, Chen Lin, Guergana
Savova, et al. 2014. Temporal annotation in the
clinical domain. Transactions of the Association for
Computational Linguistics, 2:143–154.

Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner.
2013. Evaluating temporal relations in clinical text:
2012 i2b2 challenge. Journal of the American Med-
ical Informatics Association, 20(5):806–813.

Naushad UzZaman and James F Allen. 2011. Tempo-
ral evaluation. In Proceedings of ACL–HLT, pages
351–356. Association for Computational Linguis-
tics.

David Weiss, Chris Alberti, Michael Collins, and
Slav Petrov. 2015. Structured training for neu-
ral network transition-based parsing. arXiv preprint
arXiv:1506.06158.

Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly identi-
fying temporal relations with Markov logic. In Pro-
ceedings of ACL-IJCNLP, pages 405–413. Associa-
tion for Computational Linguistics.

1158


