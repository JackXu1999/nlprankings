















































K2Q: Generating Natural Language Questions from Keywords with User Refinements


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 947–955,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

K2Q: Generating Natural Language Questions from Keywords
with User Refinements

Zhicheng Zheng

Tsinghua University

zhengzc04@gmail.com

Xiance Si

Google Inc.

sxc@google.com

Edward Y. Chang

Google Inc.

edchang@google.com

Xiaoyan Zhu

Tsinghua University

zxy-dcs@tsinghua.edu.cn

Abstract

Garbage in and garbage out. A Q&A sys-

tem must receive a well formulated ques-

tion that matches the user’s intent or she

has no chance to receive satisfactory an-

swers. In this paper, we propose a key-

words to questions (K2Q) system to assist

a user to articulate and refine questions.

K2Q generates candidate questions and re-

finement words from a set of input key-

words. After specifying some initial key-

words, a user receives a list of candidate

questions as well as a list of refinement

words. The user can then select a satisfac-

tory question, or select a refinement word

to generate a new list of candidate ques-

tions and refinement words. We propose

a User Inquiry Intent (UII) model to de-

scribe the joint generation process of key-

words and questions for ranking questions,

suggesting refinement words, and generat-

ing questions that may not have previously

appeared. Empirical study shows UII to be

useful and effective for the K2Q task.

1 Introduction

Keyword search has long been considered an un-

natural undertaking task, but it works well with

search engines. When entering a question to a

Q&A system such as Yahoo! Answers and Quora,

however, the keyword paradigm simply does not

work. A question must be articulated specifically

in the form of natural language. For instance, key-

words such as New York restaurant is ambiguous

in its intent. The user may want restaurants in New

York, or want to know the tipping practice at New

York restaurants, or something else under the myr-

iad other possible interpretations. For a question

to be answered, the asker must articulate her in-

tent with sufficient specificity. Partly because we

have been detrained by search engine from writ-

ing a question in a complete sentence, and partly

because a question is hard to articulate completely

when the asker is in a learning mode, a Q&A sys-

tem should provide tools to users to help them clar-

ify their questions so as to get good answers.

In this paper, we propose K2Q, a system that

converts keywords to questions by considering

both query history and user feedback. More

specifically, given a set of keywords, K2Q gen-

erates a list of ranked questions as well a list of

refinement words. A user can select a satisfactory

question, or she can select a refinement word to

generate a new list of candidate questions and re-

finement words. This process iterates until the user

finds a good question matching her intent or quits.

To build an effective K2Q system, there are

three aspects that need to be researched. A user

issues a question typically because the desired in-

formation cannot be found. Our first task, there-

fore, is in the generation of unseen questions (un-

seen questions refer to those questions which are

not known in advance by a K2Q system). Af-

ter all, popular questions being asked before can

be searched via search engines. Second, we must

address the challenge of ranking candidate ques-

tions. Questions, unlike keywords, rarely repeat.

(The same question can be asked in many dif-

ferent forms.) Among the 12, 880, 882 questions
we have collected, only 1.87% occur more than
once. Therefore, a simple ranking scheme based

on frequency is not feasible. Third, the suggestion

scheme of refinement words must consider max-

imizing information gain. It is thus desirable to

generate diverse refinement words. An intuitive

method is to use the most frequent words in can-

didate questions (after the removal of functional

words). However, this method only considers the

strength of relatedness between the words and the

candidate questions, and may generate several re-

finement words indicating the same subtopic. For

example, when a user inputs cat feed, if we simply

use the most frequent words as refinement words,

we will suggest both food and eat which indicate

the same topic of feed cat. Diverse refinement

947



words lead to more efficient feedbacks, and thus

produce a fewer number of iterations before get-

ting a satisfactory question.

To overcome the aforementioned three chal-

lenges, we propose a User Inquiry Intent (UII)

model, which describes the joint generation pro-

cess of keywords and questions. We employ an

adaptive language model to describe the process

of forming questions, and use automatically in-

duced question templates to create unseen ques-

tions. Candidate questions are sorted by their flu-

ency, i.e., the probability of being seen in a Q&A

system. We compute the entropy on the distribu-

tion of the user’s intent and generate refinement

words. We then suggest refinement words that

maximize entropy gain. Consequently, a satisfac-

tory question can be generated in fewer iterations.

Our experiments show that: 1) a template-based

approach improves the coverage of suggestions;

2) compared with the baseline method, our UII

model improves the ranking of the suggested ques-

tions; and 3) the UII model generates better refine-

ment words than the baseline method of suggest-

ing most frequent words. The results suggest that

our proposed UII model is effective.

The contributions of this paper can be summa-

rized as follows:

1. We propose a K2Q system, which benefits

users to articulate and refine their questions

to be more specific and more overtly express

their intent.

2. To address the technical challenges of devel-

oping K2Q, we propose the UII model, which

models the joint generation process of key-

words and questions.

3. We show that the UII model is effective in

both generating and refining questions by ex-

periments conducted with real-world query

logs.

This paper proceeds as follows. Section 2

presents a brief survey of related work. Section

3 details the UII model, which describes the joint

generation of both search keywords and natural

language questions. Experiments are described in

Section 4 and conclusions are given in Section 5.

2 Related Work

To the best of our knowledge, there is no direct re-

search on the K2Q problem. However, query sug-

gestion and question recommendation are among

the most relevant tasks that have been researched.

Query suggestion aims to suggest related re-

fined query words to users, and is employed by

most modern search engines. Many research ef-

forts have been devoted to query suggestion (Ma

et al., 2010; Chirita et al., 2007a) and other, sim-

ilar tasks such as query expansion (Chirita et al.,

2007b; Cui et al., 2003; Theobald et al., 2005; Xu

and Croft, 1996) and query refinement (Kraft and

Zien, 2004). Applying query suggestion to K2Q

is problematic for two reasons: 1) Query sugges-

tion makes heavy use of the query log information

such as click sessions. However, less than 1% of

queries are questions, and even less of those repeat

more than twice. The sparsity of questions ren-

ders query suggestion algorithms ineffective when

applied to K2Q. 2) To propose new query sug-

gestions, keyword-level editing operations such as

add, delete or change are used (Jones et al., 2006).

These operations do not take grammar into consid-

eration, and are too simple to be applied to whole,

complete sentences.

Question recommendation suggests questions

which are related to the initial question (Cao et al.,

2008; Wu et al., 2008). By treating the initial key-

words as a question, question recommendation al-

gorithms can also be used to suggest questions for

K2Q. However, question recommendation focuses

on proposing existing questions, which fails at the

first challenge of generating unseen questions.

Researchers have worked on solving individ-

ual challenges posed by K2Q. For question gen-

eration, Lin (2008) proposed an “automatic ques-

tion generation from queries” task in 2008, but

no technical approaches were discussed. Kotov

and Zhai (2010) proposed to organize search re-

sults by corresponding questions. They employed

somemanually created templates to transform nor-

mal sentences into questions. Their work was

to generate questions from a paragraph or a sen-

tence, but not according to keywords. In addi-

tion, since producing templates requires human ef-

fort, it is difficult to achieve high coverage. In

IR field, some researchers automatically generate

query templates (Agarwal et al., 2010; Szpektor et

al., 2011). Inspired by their work, we try to gener-

ate templates automatically.

For question ranking, Wu et al. (2008) cal-

culated user-to-question similarity and question-

to-question similarity by employing the PLSA

model, then ranked candidate questions by com-

bining the two similarity scores. However, it is dif-

948



ficult to model users in K2Q, so this algorithm is

not suitable for our purpose. Cao et al. (2008) pro-

posed an “MDL-based Tree Cut Model” to rank

candidate questions. They organized the candidate

questions in a tree, then defined question similar-

ity based on both specificity and generality. They

ranked the candidate questions by combining the

two similarity scores. Their work aimed to sug-

gest interesting questions to the user, but K2Q pro-

poses to recommend questions with high popular-

ity in order to make correct predictions. Sun et

al. (2009) ranked questions with several CQA spe-

cific features. However, since the candidate ques-

tions in K2Q come from different CQA sites, and

some candidates are even unseen questions, it is

difficult to apply these features in K2Q. In this pa-

per, we rank questions according to their popular-

ity, which is measured by including an adaptive

language model in UII model.

For the generation of refinement words, the

most relevant task is query suggestion. Diversity

is an important factor in query suggestion, and

most of the related works exploited the informa-

tion of query logs in order to diversify the query

suggestion results. Wang et al. (2009) extracted

subtopics of a query by mining query reformula-

tions in user session logs. Ma et al. (2010) pro-

posed a method based on Markov random walks

and hitting time analysis on a query-URL bipar-

tite graph. Sadikov et al. (2010) clustered query

refinements by performing multiple random walks

on a Markov graph that approximates user search

behavior. However, in K2Q, we cannot get suf-

ficient click information between keywords and

questions due to its sparsity in the query logs. In

this paper, we use click information to evaluate

our methods, but not for training since the total

amount is small.

3 User Inquiry Intent Model

When a user inputs a query or posts a question,

she wants to get some information. We will re-

fer to the information need as user intent. Both the

query and the question are generated from the user

intent. We will use an example to illustrate the

generative process. A user wants to know what the

hot research topics in natural language processing

are. If she employs search engines, she might cre-

ate a query hot research topics NLP from her in-

tent. If she wants to post a question in the com-

munity, she might choose a way to express her

intent. She may choose What are hot research

topics in [subject areas], or Which research top-

ics are hot in [subject areas]. Different ways of

posting her intent generate different questions. In

this example, let the corresponding final questions

beWhat are hot research topics in NLP andWhich

research topics are hot in NLP. The replaceable

part (such as [subject areas]) can be considered

as slots for concrete words. Generally, a slot can

be interpreted as a word or a word cluster. A word

cluster is a set of words which can be used in simi-

lar contexts. For example, Beijing and Paris are in

the same cluster since they are both suitable to be

used in the context in [cities]. We obtain the word

clusters by using k-means as was shown in (Lin

and Wu, 2009).

Here, we propose a User Inquiry Intent model

to describe the process of generating queries and

questions from the user intent.

Figure 1: The plate representation of the UII

model. (as in Probabilistic Graphical Models)

The plate representation of UII model is shown

in Figure 1. In the model:

∙ 𝑡 is the index of user intents, ranging from 1
to 𝐾 (𝐾 is the number of different user in-

tents). A user intent not only corresponds to

a distribution over all words but also corre-

sponds to a distribution over the slots 𝑠.

∙ 𝑤 and 𝑞 are both indices of words, ranging
from 1 to 𝑊 (𝑊 is the number of different
words). In Figure 1, the left group of 𝑤 is

the question which is able to express the user

intent 𝑡. The right group of 𝑞 is the query

which is also able to express 𝑡.

∙ 𝑠 is the index of slots, ranging from 1 to𝑊 +

949



𝐿. Besides the𝑊 slots which are reserved for

exact words, there are an additional 𝐿 more

slots for 𝐿 word clusters.

∙ �⃗� is the prior parameter of the distribution
of user intents. �⃗� is a vector of length 𝐾:∑𝐾

𝑖=1 𝛼𝑖 = 1, 0 ≤ 𝛼𝑖 ≤ 1.

∙ 𝛽 is the prior parameter of the word distribu-
tion on each user intent 𝑡. 𝛽 is a matrix of size

𝐾 ×𝑊 : ∀𝑖,
∑𝑊

𝑗=1 𝛽𝑖,𝑗 = 1, 0 ≤ 𝛽𝑖,𝑗 ≤ 1.

∙ 𝜑 is the prior parameter of the slot transi-
tion. 𝜑 is a matrix. Denote the preceding

slots of 𝑠 as ℎ. Since there are too many

possible values of ℎ, in practice we only re-

serve the frequent N-grams as possible values

of ℎ. The size of 𝜑 is then 𝐻 × (𝑊 + 𝐿),
where 𝐻 is the size of frequent N-grams. ∀𝑖,∑𝑊+𝐿

𝑗=1 𝜑𝑖,𝑗 = 1, 0 ≤ 𝜑𝑖,𝑗 ≤ 1.

∙ 𝛾 is the prior parameter of the word distri-
bution on each slot 𝑠. 𝛾 is a matrix of size

(𝑊 + 𝐿) × 𝑊 . ∀𝑖,
∑𝑊+𝐿

𝑗=1 𝛾𝑖,𝑗 = 1, 0 ≤
𝛾𝑖,𝑗 ≤ 1.

∙ 𝜓 is the prior parameter of the slot distribu-
tion on each user intent 𝑡. 𝜓 is a matrix of

size 𝐾 × (𝑊 + 𝐿). ∀𝑖,
∑𝑊+𝐿

𝑗=1 𝜓𝑖,𝑗 = 1,
0 ≤ 𝜓𝑖,𝑗 ≤ 1.

∙ 𝜃 is a (𝑊 + 𝐿) × 𝑊 matrix. Each row of
𝜃 is the word distribution on a slot under a

particular user intent.

∙ 𝜆 and 𝜇 are two mixture weights of prior dis-
tributions. Under user intent 𝑡, ∀𝑖, 𝜃𝑖 fol-
low the Dirichlet distribution with parameters

𝜆 ⋅ 𝛾𝑖 + (1 − 𝜆)𝛽𝑡. Under user intent 𝑡, the
slot transition probability 𝑝(𝑠∣ℎ, 𝑡) is calcu-
lated by Eq. 1, which is an adaptive language

model (Kneser et al., 1997).

𝑝(𝑠∣ℎ, 𝑡) =
𝑓(𝑠∣𝑡)

𝑧(ℎ, 𝑡)
𝜑ℎ,𝑠 (1)

𝑓(𝑠∣𝑡) = (
𝜓𝑡,𝑠
𝜑∅,𝑠

)𝜇, 𝑧(ℎ, 𝑡) =
∑

𝑠 𝑓(𝑠∣𝑡) ⋅ 𝜑ℎ,𝑠

We refer the generative process as Algorithm 1.

The process is illustrated with the same example

as in the beginning of this section. The user intent

has high probabilty on the words hot, research,

topic, NLP, and hence generates the set of query

words hot research topic NLP. Assuming that the

probability of seeing a slot is determined by two

preceding slots, then by considering both the slot

distribution on the user intent and the slot tran-

sition probability, the user intent first generates

What from a START slot; then are from START

What; hot from What are; research from are hot;

topics from hot research; in from research topics;

[subject areas] from topics in; and finally an END

slot from in [subject areas]. Under the user in-

tent, we generate question words from the slots in

the slot sequence What are hot research topics in

[subject areas], and form the question word se-

quence What are hot research topics in NLP.

Choose 𝑡 ∼𝑀𝑢𝑡𝑖𝑛𝑜𝑚𝑖𝑎𝑙(�⃗�);
for each word 𝑞𝑗 in the query do

Choose 𝑞𝑗 ∼𝑀𝑢𝑡𝑖𝑛𝑜𝑚𝑖𝑎𝑙(𝛽𝑡);
end

Choose 𝜃𝑖 ∼ 𝐷𝑖𝑟(𝜆 ⋅ 𝛾𝑖 + (1− 𝜆)𝛽𝑡), where
𝑖 ∈ {1, 2, . . . ,𝑊 + 𝐿};
for each word 𝑤𝑗 in the question do
Choose 𝑠𝑗 ∼𝑀𝑢𝑙𝑡𝑖𝑛𝑜𝑚𝑖𝑎𝑙(𝑝(𝑠∣ℎ)),
𝑝(𝑠∣ℎ) is calculated as Eq. 1 ;

Choose 𝑤𝑗 ∼𝑀𝑢𝑙𝑡𝑖𝑛𝑜𝑚𝑖𝑎𝑙(𝜃𝑠𝑗 );

end

Algorithm 1: The algorithm for UII model

3.1 Inference

Given prior parameters (for convenience, we de-

note all prior parameters as 𝜋, and all other vari-

ables as 𝑦), the joint distribution is calculated as:

𝑝(𝑦∣𝜋) = 𝑝(𝑡∣�⃗�)𝑝(�⃗�∣𝑡, 𝛽)𝑝(�⃗�∣𝑡, 𝜑, 𝜓, 𝜇)

𝑝(𝜃∣𝑡, 𝛾, 𝛽)𝑝(�⃗�∣�⃗�, 𝜃)

We integrate out 𝜃 to calculate 𝑝(𝑡, �⃗�, �⃗�, �⃗�∣𝜋):

∫
𝜃
𝑝(𝑦∣𝜋)𝑑𝜃 = 𝑝(𝑡∣�⃗�)𝑝(�⃗�∣𝑡, 𝛽)𝑝(�⃗�∣𝑡, 𝜑, 𝜓, 𝜇)

∫
𝜃
𝑝(𝜃∣𝑡, 𝛾, 𝛽)𝑝(�⃗�∣�⃗�, 𝜃)𝑑𝜃

The right side of the equation is separated into

four parts: (1) 𝑝(𝑡∣�⃗�) = 𝛼𝑡; (2) 𝑝(�⃗�∣𝑡, 𝛽) =∏𝑁1
𝑖=1 𝛽𝑡,𝑞𝑖 ; (3) 𝑝(�⃗�∣𝑡, 𝜑, 𝜓, 𝜇) =

∏𝑁2
𝑖=1 𝑝(𝑠𝑖∣ℎ𝑖, 𝑡),

𝑝(𝑠𝑖∣ℎ𝑖, 𝑡) is calculated by Eq. 1; (4) The integra-
tion

∫
𝜃
𝑝(𝜃∣𝑡, 𝛾, 𝛽)𝑝(�⃗�∣�⃗�, 𝜃)𝑑𝜃 is solved by:

∫
𝜃
𝑝(𝜃∣𝑡, 𝛾, 𝛽)𝑝(�⃗�∣�⃗�, 𝜃)𝑑𝜃

=
∏𝑊+𝐿

𝑖=1

∫
𝜃𝑖
𝑝(𝜃𝑖∣𝑡, 𝛽, 𝛾, 𝜆)

∏𝑊
𝑗=1 𝜃

𝑛(𝑖,𝑗)

𝑖𝑗 𝑑𝜃𝑖

=
∏𝑊+𝐿

𝑖=1

∏𝑊
𝑗=1 Γ(𝜆𝛾𝑖,𝑗+(1−𝜆)𝛽𝑡,𝑗+𝑛

(𝑖,𝑗))

Γ(
∑𝑊

𝑗=1(𝜆𝛾𝑖,𝑗+(1−𝜆)𝛽𝑡,𝑗+𝑛
(𝑖,𝑗)))

⋅

Γ(
∑𝑊

𝑗=1(𝜆𝛾𝑖,𝑗+(1−𝜆)𝛽𝑡,𝑗))
∏𝑊

𝑗=1 Γ(𝜆𝛾𝑖,𝑗+(1−𝜆)𝛽𝑡,𝑗)

Here, 𝑛 = {𝑛(𝑖)} = {{𝑛(𝑖,𝑗)}} is a matrix, 𝑛(𝑖,𝑗) is
the count of slot 𝑖 generating words 𝑗 in the ques-

tion. Function Γ(𝑥) is the gamma function.

950



3.2 Parameter Estimation

As the complexity of the UII model is non-trivial,

in this work we choose to estimate the parameters

of different components separately instead of per-

forming a global joint optimization.

𝜆 and 𝜇 are weight parameters for combining

different distributions. We set them empirically.

We estimate the other prior parameters with a

given question set (we refer to these questions as

known questions). 𝛾 is the prior word distribution

of slots. There are two kinds of slots, one can be

filled with exactly one word, and the other can be

filled with any words from the corresponding word

cluster. Hence, we estimate 𝛾 as:

∀𝑠, 𝑖, 1 ≤ 𝑠 ≤𝑊, 𝑖 ∕= 𝑠, 𝛾𝑠𝑠 = 1, 𝛾𝑠𝑖 = 0

∀𝑠, 𝑖,𝑊 < 𝑠 ≤𝑊 + 𝐿, 𝛾𝑠𝑖 = 𝑝(𝑖∣𝑐𝑙𝑢𝑠𝑡𝑒𝑟(𝑠−𝑊 ))

Here 𝑝(𝑖∣𝑐𝑙𝑢𝑠𝑡𝑒𝑟(𝑠−𝑊 )) is the probability of the 𝑖-
th word in the (𝑠−𝑊 )-th word cluster. We set the
history ℎ to be the 𝑙 preceding slots, which allows

the slot sequence to be an 𝑙-order Markov chain.

We obtain 𝜑 from statistics on the known ques-

tions.

In order to estimate �⃗�, 𝛽 and 𝜓, we cluster the

known questions, where the questions in the same

cluster contain the same bag of words (We only

exclude the stopwords). Suppose we obtain 𝐾

question clusters such that each cluster 𝑡 contains

𝑚𝑡 questions, then we estimate �⃗� as: 𝛼𝑡 = 𝑚𝑡.
According to the statistics of word distributions

on each question cluster, we estimate 𝛽 and 𝜓 as:

𝛽𝑡,𝑤 = 𝑝(𝑤∣𝑡), 𝜓𝑡,𝑠 = 𝑝(𝑠∣𝑡).

3.3 Question Generation

In order to reduce the computational complexity

of the model, we do not actually generate all possi-

ble questions that the UII model is capable of. In-

stead, we use a template based method to generate

the candidate questions, which are subsequently

ranked by the UII model.

Question Templates Generation

To generate unseen questions, we generate ques-

tion templates from known questions. A question

template is a sequence of slots, where each slot is a

word or a word cluster. If there are 𝑘 cluster slots

in a template, we refer to it as a 𝑘-variable tem-

plate. Given a set of questions, we initialize a set

of 0-variable templates. By replacing one word of
a 0-variable template with a corresponding clus-
ter, we obtain a 1-variable template. By merging

all the same 1-variable templates, we get a set of 1-
variable templates with their support numbers (the

support number is the number of 0-variable tem-
plates employed to generate the 1-variable tem-
plate). By increasing the threshold of the mini-

mum support number (denoted as 𝜂), we filter out

those templates with low quality.

Question Generation from Initial Keywords

Firstly, we search for known questions that contain

all the keywords and use them as suggestion candi-

dates (We search for at most 1, 000 questions). For
popular keywords such as New York steakhouse,

we have enough known questions covering all as-

pects of the inquiry intents. However, for rare key-

words such as Tangshan1 steakhouse, performing

a search in the known questions might yield only

a few or no candidates.

Secondly, we generate unseen questions by the

use of question templates. By replacing one of

the initial keywords with its cluster, we search for

1-variable templates that contain both the cluster
and the rest keywords (We search for at most 20
1-variable templates). Then, we replace the clus-
ter slot in the templates with the actual initial key-

word, creating a new question.

Both known and generated questions are added

into the final candidate question set.

3.4 Question Ranking

With the candidate question set, we use the UII

model to rank the candidates by 𝑝(�⃗�∣�⃗�, 𝜋), the
probablity of the question was generated given the

keywords. The probability is calculated as:

𝑝(�⃗�∣�⃗�, 𝜋) =
∑

𝑡

∑
�⃗� 𝑝(𝑡,�⃗�,�⃗�,𝑞∣𝜋)∑

𝑤′
∑

𝑡

∑
�⃗� 𝑝(𝑡,�⃗�,�⃗�

′,𝑞∣𝜋)
(2)

We could perform exact computation (i.e. Dy-

namic Programming) to compute the sum over all

possible �⃗�, however, since each suggestion should

be finished in a timely fashion, the complexity of

exact computation is not acceptable. Recall the

process where we generated candidate questions,

we only consider all the question templates that we

obtained as all possible slot permutations. Then

we only need to sum up all these �⃗� to calculate

the probability, which makes it possible to finish

in real-time.

3.5 Refinement Word Generation

The goal of refinement words is to reduce the num-

ber of interactions required to reach the desired

1A city in China.

951



question. With the UII model, we use an infor-

mation theory based approach to select refinement

words. We define the entropy on the distribution of

the user intents given the initial keywords 𝑄 = �⃗�
as:

𝐸𝑛𝑡𝑟𝑜𝑝𝑦(�⃗�) = −
∑𝐾

𝑘=1 𝑝(𝑡∣�⃗�, 𝜋) log(𝑝(𝑡∣�⃗�, 𝜋))

Here, 𝑝(𝑡∣�⃗�, 𝜋) = 𝑝(𝑡,𝑞∣𝜋)∑
𝑡′ 𝑝(𝑡

′,𝑞∣𝜋) , and 𝑝(𝑡, �⃗�∣𝜋) is cal-

culated as:

𝑝(𝑡, �⃗�∣𝜋) = 𝑝(𝑡∣�⃗�)
∏𝑁2

𝑖=1 𝑝(𝑞𝑖∣𝛽, 𝑡) = 𝛼𝑡
∏𝑁2

𝑖=1 𝛽𝑡,𝑞𝑖

Then, if the user selects a refinement word 𝑞′, the

resulted entropy gain is:

Δ𝐸(𝑞′∣�⃗�) = 𝐸𝑛𝑡𝑟𝑜𝑝𝑦(�⃗�)− 𝐸𝑛𝑡𝑟𝑜𝑝𝑦(�⃗�
�

{𝑞′})

Entropy gain measures the reduced uncertainty

that results by adding 𝑞′. As we aim to reduce

the number of interaction steps in K2Q, we se-

lect those refinement words that maximize the ex-

pected entropy gain in each step. The expected

entropy gain of a refinement words set R is:

Δ𝐸(𝑅) =
∑

𝑞′∈𝑅 𝑝(𝑞
′∣𝑅, �⃗�)Δ𝐸(𝑞′∣�⃗�)

Here 𝑝(𝑞′∣𝑅, �⃗�) is the probability that a user will
choose 𝑞′ from 𝑅 when a set of refinement words

𝑅 is given to query 𝑄. 𝑝(𝑞′∣𝑅, �⃗�) is calculated as:

𝑝(𝑞′∣𝑅, �⃗�) =
∑

𝑡 𝑝(𝑞
′∣𝑡, 𝜋) ⋅ 𝑝(𝑡∣�⃗�, 𝜋)

=
∑

𝑡 𝑝(𝑞
′∣𝑡, 𝜋) ⋅ 𝑝(𝑡, �⃗�∣𝜋)

=
∑

𝑡 𝛽𝑡,𝑞′𝛼𝑡
∏𝑁2

𝑖=1 𝛽𝑡,𝑞𝑖

If the user does not select any refinement words

from R, then the expectation of entropy gain on R

is meaningless. Hence we define our optimization

function as Eq. 3:

𝑅 = argmax
𝑅

𝑝(𝑅∣𝑄) ⋅Δ𝐸(𝑅) (3)

Here 𝑝(𝑅∣𝑄) = 𝑛(𝑅∣𝑄)
𝑛(𝑄) , where 𝑛(𝑄) is the num-

ber of candidate questions when giving query 𝑄,

and 𝑛(𝑅∣𝑄) is the number of the candidate ques-
tions which contain at least one refinement word

in R.

The optimization problem is an NP-complete

problem, hence in practice, we use a greedy strat-

egy described by Algorithm 2.

4 Experiments

In the section, we use real world questions and

Web queries to evaluate the performance of can-

didate question generation, question ranking and

refinement word generation.

Initialize 𝑅 = ∅, 𝑐ℎ𝑎𝑛𝑔𝑒𝑑 = true;
while ∣𝑅∣ < 𝑀𝑎𝑥𝑁𝑢𝑚𝑏𝑒𝑟 and 𝑐ℎ𝑎𝑛𝑔𝑒𝑑 do
Select 𝑞′ =

argmax𝑞′ 𝑝(𝑅
∪
𝑞′∣𝑄) ⋅Δ𝐸(𝑅

∪
𝑞′);

if 𝑝(𝑅
∪
𝑞′∣𝑄) ⋅Δ𝐸(𝑅

∪
𝑞′) >

𝑝(𝑅∣𝑄) ⋅Δ𝐸(𝑅) then
𝑅 = 𝑅

∪
𝑞′;

𝑐ℎ𝑎𝑛𝑔𝑒𝑑 = true;

else

𝑐ℎ𝑎𝑛𝑔𝑒𝑑 = false;

end

end

Algorithm 2: Greedy strategy algorithm of re-

finement words generation

4.1 Data Sets

We use a large set of English questions as the train-

ing set to generate question templates and estimate

the prior parameters in the UII model. There are

12, 880, 822 questions in the question set, which
are obtained from popular CQA sites such as

Yahoo! Answers. Notice that questions from

WikiAnswers2 are intentionally excluded from the

training set, as we want to useWikiAnswers’ ques-

tions as samples of unseen questions to evaluate

the performance of question ranking when faced

with unseen questions.

To evaluate the performance of K2Q, we need

to know the mapping relationship between query

and question. To this end, we use Web query logs

to create the data set. After the user inputs a query,

she will click a result if she thinks it is satisfac-

tory. By only considering the clicked results that

are from CQA sites, we collect pairs of queries

and their target questions. We employ a one week

query log from Google. To avoid data noise, we

collect only those query-question pairs which oc-

cur at least 10 times in the query log. Under these
conditions, we get approximate 500, 000 query-
question pairs.

4.2 Evaluation of Candidate Question

Generation

First, we show the number of templates under dif-

ferent support numbers (𝜂) in Figure 2. As the

figure shows, the number of 1-variable templates

drops significantly when we need more supports.

According to our observation, 𝜂 = 3 suffices as a
good balance point between quantity and quality.

2http://wiki.answers.com

952



Figure 2: Number of 1-variable templates and re-
call of candidate questions with different 𝜂. The

templates help improve the recall (Note: 𝜂 = ∞
means no template is used)

Second, we set up an experiment to evaluate

the coverage of candidate questions generated by

K2Q. We select the top 10, 000 most frequent
query-question pairs as the test set. For each

query-question pair, we generate candidate ques-

tions for the query. We define the query-question

pair to be recalled in a candidate question set if

the set contains the target question in the query-

question pair. We use recall to measure the perfor-

mance of our candidate question generation algo-

rithm, which is defined as:

𝑟𝑒𝑐𝑎𝑙𝑙 =
#{recalled query-question pair}

#{query-question pairs}
(4)

Recall measures the coverage of the candidate

question set. The higher the recall, the more target

questions are included in the candidate question

sets. Figure 2 also shows the results under dif-

ferent 𝜂 (𝜂 = ∞ means no 1-variable template is
used). Compared to the case when 1-variable tem-

plates are not used, our candidate question gen-

eration algorithm improved the recall by 6.6% −
16.2%.

4.3 Evaluation of Question Ranking

The baseline method that we compare our ap-

proach to is the language model (denoted as LM).

The language model ranks the candidate ques-

tions according to the probability that it would

generate the question. Since a popular question

tends to contains popular N-grams, the language

model also measures the popularity of the ques-

tion. Specifically, we use the same question set

as that in Section 4.1 to train a 3-gram language
model.

To construct the test set, we select 2, 000 query-
question pairs that satisfy the following condi-

tions: 1) the target question in the query-question

pair is generated as a candidate question; 2) the

target question comes from WikiAnswers, as we

are more concerned with the ranking performance

on unseen questions. The results in Table 1 show

that the UII model provides a better ranking of

the candidate questions when compared to the lan-

guage model. We argue this improvement results

from two aspects of the UII model: 1) The UII

model introduces word clusters, which smooths

the probability of rare N-grams. This is impor-

tant in ranking since the generated unseen ques-

tions always contain several rare N-grams. 2) The

UII model generates slot sequences via an adaptive

langugage model, which helps adjust slot transi-

tion probabilities under different user intents.

We select two examples (shown in Table 2) to

illustrate the aforementioned aspects of our UII

model’s superiority to the conventional language

model. The target question of the first query is

what is the meaning of advocacy. Since the 3-

gram meaning of advocacy never occurs in the

training set, LM does not rank the target question

as the top 1 best candidate. However, the 3-gram

meaning of WordCluster occurs frequently in the

training set, so the UII model ranks the target ques-

tion as the top 1 best candidate. The target ques-

tion of the second query is what are the different

types of Google. In the training set, the phrases

how many occurs more frequently than the phrase

what are the different, so LM does not rank the tar-

get question as its number 1 candidate. However,

the corresponding user intent has high probability

to generate the slot different, so the target question

is ranked as the best candidate by the UII model.

Recall at LM UII model Improvement

top 1 37.5% 43.5% +16.0%

top 2 54.2% 58.0% +7.0%

top 3 64.1% 66.6% +4.1%

Table 1: The UII model performs better on the

most frequent query-question pairs set

4.4 Evaluation of Refinement Word

Generation

We use the query-question pairs to evaluate

the performance of the generation of refine-

ment words. We compare the UII model with

an intuitive method which selects the most fre-

quent words in the candidate questions as refine-

ments (denoted as MF). The goal of refinement

words is to interact with the user in those cases

953



Query
Question at top 3

UII model LM

advocacy meaning

What is the meaning of advocacy?

The meaning of advocacy?

What is meaning of advocacy?

What is the meaning advocacy?

What is the meaning of advocacy?

What is the meaning of the advocacy?

Google types

What are the different types of Google?

What are the types of Google?

How many types of Google?

How many types of Google?

Different types of Google?

What are the different types of Google?

Table 2: Two examples of top 3 suggestions by UII model and LM (The target questions of the queries

are shown in bold)

when the initial keywords are too simple to ex-

press the user intent. Since one word queries

are often too simiple to express the user intent,

we use one word queries as the test set. These

query-question pairs also satisfy the following two

conditions: 1) the target question in the query-

question pair is generated as a candidate question;

2) the UII model ranks the target question not to

be among the top ten.

We use 623 such query-question pairs as the
test set. For each method, we generate a list of

10 refinement words. We use each of the refine-
ment words to refine the query, and then subse-

quently re-rank the candidate questions and cre-

ate a new set of 10 refinement words. By ex-

ploring all possible choices of refinement words

among the list of 10 in each interaction, we find

the minimal number of interaction steps that lead

to the target questions. We search for at most 5
steps. If K2Q ranks the target question among

the top 10 within these 5 interactions, then set the
query cost to be the minimal interaction number in

which this occured; otherwise, let the query cost

be 6. We consider two metrics: 1) the number
of query-question pairs where K2Q ranks the tar-

get question among the top ten within 5 interac-
tions (denoted as RN); 2) the reciprocal average

query cost (denoted as RAQC). The larger RAQC

is, the fewer the number of interactions K2Q re-

quires to successfully rank the target question.

MF UII model Improvement

RN 363 409 +12.7%

RAQC 0.258 0.265 +2.63%

Table 3: UII model performs better than the base-

line method on refinement words generation

From the results in Table 3, we find that: 1)

With refinement words, K2Q is more effective of

suggesting the target questions. 2) With refine-

ment words generated by the UII model, K2Q gets

more efficient feedback, which leads to better per-

formance when compared to the intuitive method.

MF only considers the frequency of the words, not

the diversity. In contrast, the UII model is effective

in suggesting those refinement words that maxi-

mally reduce the entropy of the distribution of user

intents. It is clear that the UII model suggests a

better list of refinement words than the intuitive

method.

5 Conclusion

In this paper, we propose the UII model to solve

the K2Q problem. The UII model exhibits a si-

multaneous process of generating both questions

and queries based on the user intents. UII model

utilizes existing questions on the Web, and lever-

ages templates to generate and rank unseen ques-

tions. This model is also equipped with the abil-

ity to suggest refinement words that maximize the

entropy gain of inferred intent distribution. Ex-

periments show that: 1) using templates improves

the coverage of suggestions by 6.6% − 16.2%;
2) the UII model improves the ranking of sugges-

tions over conventional language models with re-

call at top ranked questions increased over 16%;
3) the UII model suggests better refinement words

when compared to a baseline method of suggest-

ing words based on frequency. By interacting via

these refinement words, K2Q is able to rank 65.7%
target questions within top ten, 12.7% higher than
the baseline method. These results show that our

proposed UII model yields better suggestions than

separated methods in K2Q.

The UII model is a generative model. In this pa-

per, we estimate all the prior parameters without

using query-question pairs. Since there are moun-

tains of query-question pairs from query logs, we

plan to better estimate the prior parameters with

these pairs in future work. Another important di-

rection is to use real-world feedback to evaluate

the model, as our current evaluation does not con-

sider real users.

954



References

G. Agarwal, G. Kabra, and K.C.C. Chang. 2010. To-
wards rich query interpretation: Walking back and
forth for mining query templates. In Proc. of WWW.
ACM.

Y. Cao, H. Duan, C.Y. Lin, Y. Yu, and H.W. Hon. 2008.
Recommending questions using the mdl-based tree
cut model. In Proc. of WWW, pages 81–90. ACM.

P.A. Chirita, C.S. Firan, and W. Nejdl. 2007a. Per-
sonalized query expansion for the web. In Proc. of
SIGIR, pages 7–14. ACM.

Paul Alexandru Chirita, Claudiu S. Firan, and Wolf-
gang Nejdl. 2007b. Personalized query expansion
for the web. In Proc. of SIGIR, SIGIR ’07, pages
7–14, New York, NY, USA. ACM.

Hang Cui, Ji-Rong Wen, Jian-Yun Nie, and Wei-Ying
Ma. 2003. Query expansion by mining user logs.
IEEE Transactions on Knowledge and Data Engi-
neering, 15:829–839.

R. Jones, B. Rey, O. Madani, and W. Greiner. 2006.
Generating query substitutions. In Proc. of WWW,
pages 387–396. ACM.

R. Kneser, J. Peters, and D. Klakow. 1997. Lan-
guage model adaptation using dynamic marginals.
In Fifth European Conference on Speech Communi-
cation and Technology.

A. Kotov and C.X. Zhai. 2010. Towards natural ques-
tion guided search. In Proc. of WWW, pages 541–
550. ACM.

R. Kraft and J. Zien. 2004. Mining anchor text for
query refinement. In Proc. of WWW, pages 666–
674. ACM.

D. Lin and X.Wu. 2009. Phrase clustering for discrim-
inative learning. In Proc. of ACL, pages 1030–1038.
Association for Computational Linguistics.

C.Y. Lin. 2008. Automatic question generation from
queries. In Workshop on the Question Generation
Shared Task.

H. Ma, M.R. Lyu, and I. King. 2010. Diversifying
Query Suggestion Results. In Proc. of AAAI.

E. Sadikov, J. Madhavan, L. Wang, and A. Halevy.
2010. Clustering query refinements by user intent.
In Proc. of WWW, pages 841–850. ACM.

K. Sun, Y. Cao, X. Song, Y.I. Song, X. Wang, and C.Y.
Lin. 2009. Learning to recommend questions based
on user ratings. In Proc. of CIKM, pages 751–758.
ACM.

I. Szpektor, A. Gionis, and Y. Maarek. 2011. Improv-
ing recommendation for long-tail queries via tem-
plates. In Proc. of WWW, pages 47–56. ACM.

M. Theobald, R. Schenkel, and G. Weikum. 2005. Ef-
ficient and self-tuning incremental query expansion
for top-k query processing. In Proc. of SIGIR, pages
242–249. ACM.

X. Wang, D. Chakrabarti, and K. Punera. 2009. Min-
ing broad latent query aspects from search sessions.
In Proc. of SIGKDD, pages 867–876. ACM.

H. Wu, Y. Wang, and X. Cheng. 2008. Incremental
probabilistic latent semantic analysis for automatic
question recommendation. In Proceedings of the
2008 ACM conference on Recommender systems,
pages 99–106. ACM.

J. Xu and W.B. Croft. 1996. Query expansion using
local and global document analysis. In Proc. of SI-
GIR, pages 4–11. ACM.

955


