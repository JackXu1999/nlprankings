



















































Learning Knowledge Graphs for Question Answering through Conversational Dialog


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 851–861,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Learning Knowledge Graphs for Question Answering
through Conversational Dialog

Ben Hixon
University of Washington

Computer Science & Engineering
Seattle, WA, USA

bhixon@cs.washington.edu

Peter Clark
Allen Institute for AI

Seattle, WA, USA
peterc@allenai.org

Hannaneh Hajishirzi
University of Washington

Electrical Engineering
Seattle, WA, USA

hannaneh@washington.edu

Abstract

We describe how a question-answering sys-
tem can learn about its domain from conver-
sational dialogs. Our system learns to relate
concepts in science questions to propositions
in a fact corpus, stores new concepts and re-
lations in a knowledge graph (KG), and uses
the graph to solve questions. We are the first
to acquire knowledge for question-answering
from open, natural language dialogs without a
fixed ontology or domain model that predeter-
mines what users can say. Our relation-based
strategies complete more successful dialogs
than a query expansion baseline, our task-
driven relations are more effective for solving
science questions than relations from general
knowledge sources, and our method is practi-
cal enough to generalize to other domains.

1 Introduction

Our goal is a question-answering (QA) system that
learns about its domain from users in open, nat-
ural language dialogs. Dialog learning interfaces
built on top of existing QA systems can complement
and refine knowledge acquired from other sources.
Even large QA systems such as Watson (Ferrucci
et al., 2010) that utilize heavy domain engineering
could benefit from focused dialogs to dynamically
improve their knowledge.

A successful QA system requires domain knowl-
edge. For example, 4th grade science questions are
difficult since they often exclude knowledge neces-
sary to relate answers to known facts. The question
in Figure 1 asks if an iron nail conducts electricity;

S1: Here’s the question: Which object is the best conductor of
electricity? (A) a wax crayon (B) a plastic spoon (C) a rubber
eraser (D) an iron nail. I’m not sure of the answer. What do
you think the answer is?
U1: iron nail
S2: Why do you think the answer is (D) an iron nail?
U2: electricity goes through metal
S3: I know “metals are shiny and good conductors of electricity
and heat,” does that mean that the answer is (D) an iron nail?
U3: yeah
S4: Okay, great. What’s the relationship between ‘electricity’
and ‘iron’, if any?
U4: iron conducts electricity because its metal
S5: Okay, great, I get it! The answer is (D) an iron nail

Figure 1: Top: A successful real user dialog. Open-
ended prompts (S1&S2) encourage conversational
explanations. Focused prompts (S4) target particular
relations. Bottom: Corresponding knowledge graph
consisting of relations between concepts.

the system only knows that metal conducts electric-
ity, and it needs to learn that iron is a metal in order
to answer the question with the relevant fact.

Our dialog system, KNOWBOT, conducts dialogs
about science questions and learns how concepts
in each question relate to propositions in a corpus
of science facts. KNOWBOT presents its user with
a question (line S1 in Figure 1), prompts them to
choose and explain their answer, and extracts rela-
tions – any semantic relationship between two con-

851



cepts, such as metal to iron (line U4 in Figure 1) –
that increase its confidence in the user’s answer.

Relation extraction systems such as NELL (Carl-
son et al., 2010) use ontologies to predetermine valid
relation types and arguments, then scan text to fill
the ontology with facts. Open Information Extrac-
tion (Etzioni et al., 2011) avoids fixed ontologies
with domain-independent linguistic features, distant
supervision, and redundancy, but requires web-scale
text and doesn’t improve with interaction. Like
Open IE, we extract relations without predetermined
types, but are the first to do so from dialog.

KNOWBOT is an open dialog system, which
means a user utterance may progress the dialog task
even if its underlying action is not explicitly rep-
resented in a dialog model. This lets KNOWBOT
quickly bootstrap domain knowledge from users
without significant engineering overhead. Dialog-
driven extraction produces effective relations with-
out annotation, improves after each interaction, ac-
quires relations useful on a particular task, and em-
beds relations in a rich dialog context.

Users successfully correct the system in approxi-
mately 50% of dialogs even without a predetermined
dialog model. A baseline query expansion (Bast et
al., 2007) strategy that bases decisions on the acqui-
sition of new keywords instead of new relations re-
sults in only a 5% success rate. In comparison to
paraphrase relations from general knowledge bases,
relations acquired by our method are more effective
as domain knowledge, demonstrating that we suc-
cessfully learn from real users.

Our contributions include:

1. The first end-to-end system to construct knowl-
edge graphs for question-answering through
conversational dialog.

2. A generalizable method to represent the mean-
ing of user utterances without a dialog model
when task progression can be computed as a
function of extracted relations.

3. A novel data set of real user dialogs in which
users correct a QA system’s answer, together
with knowledge graphs representing the impor-
tant concepts and relations in each question, la-
beled with rich dialog features.

2 Conversational extraction for QA

Our QA task consists of 107 science questions from
the 4th grade New York Regents exam (Clark et al.,
2014).1 Each question has four possible answers.
We convert each of the four question-answer pairs
into a true/false question-answer statement using a
small number of pattern-based transformation rules.

Just as 4th graders read their textbooks for an-
swers, we collect SCITEXT (Clark et al., 2014), a
corpus of unlabeled true-false natural language sen-
tences from science textbooks, study guides, and
Wikipedia Science. Each question-answer statement
is associated with a subset of true/false support sen-
tences from SCITEXT based on positive word over-
lap between the question-answer pair and the sup-
port sentence. The degree to which a SCITEXT sen-
tence supports a question-answer pair is the sen-
tence’s alignment score (section 2.3).

Initially, the alignment score depends on keyword
overlap alone, but SCITEXT needs domain knowl-
edge to answer our questions. For example, the cor-
rect question-answer statement to What form of en-
ergy causes an ice cube to melt? (A) mechanical
(B) magnetic (C) sound (D) heat is Q(D), “Heat is a
form of energy and heat causes an ice cube to melt.”
To better align Q(D) to the SCITEXT sentence “A
snowball melting in your hand is an example of heat
energy,” we need to know that snowballs are made
of ice. Figure 2 illustrates this example.

To construct a knowledge base with which to use
SCITEXT, we extract concepts (section 2.1) from
questions and SCITEXT sentences, then use relations
(section 2.2) between concepts to determine which
question-answer statementQi is most highly aligned
with a supporting SCITEXT sentence.

2.1 Concepts

A concept keyword in a sentence or user utter-
ance is any non-stopword of at least three char-
acters. Stopwords are domain-independent, low-
information words such as “the.”

A concept is a set of concept keywords with a
common root, e.g. {melts, melted, melting} or
{heat, heats, heated}. We use the Porter algorithm
for stemming (Porter, 1997). Question concepts ap-

1Our dialogs, extractions, and tools are available at
www.cs.washington.edu/research/nlp/knowbot

852



pear in a question-answer statement, and support
concepts appear in a SCITEXT support sentence.

2.2 Relations
A relation is any pair of concepts that represents a
semantic correspondence. In general, relations can
be labeled with any feature that describes the corre-
spondence, such as a particular type. For example,
the relation between Obama and Hawaii can be la-
beled with the type born-in.

A predetermined ontology is typically required to
label relations with their type. In this work we la-
bel acquired relations with dialog-specific features.
Our thesis is that user explanations intend to relate
concepts together, and the system’s task is to deter-
mine the user’s intent. For example, the user utter-
ance U: it’s melting because of heat
relates the concepts represented by melt[ing] and
heat, with the words because of appearing be-
tween the two concept keywords. We refer to
because of as the relation’s intext.

Relations can be intuitively arranged as a knowl-
edge graph, which in this work is any graph whose
nodes are concepts and whose edges are relations
between those concepts, in the spirit of semantic net-
works such as ConceptNet (Havasi et al., 2007).

2.3 Sentence alignment
We calculate the alignment score α between the ith
question-answer statement Qi and its jth supporting
SCITEXT sentence Si,j as the normalized number of
relations between their concepts,

α(Qi, Si,j) =
‖RQi,Si,j‖

‖CQi ∪ CSi,j‖
, (1)

where CQi is the set of concepts in Qi, CSi,j is the
set of concepts in Si,j , and ‖RQi,Si,j‖ is the number
of relations between CQi and CSi,j .

Normalized relation count is a practical semantic
similarity score that generalizes to different knowl-
edge representations. The dialog in Figure 2 aligns
Q(D) with the SCITEXT fact S by learning from the
user that, for example, heat is related to melting.

3 The KNOWBOT dialog system

KNOWBOT grows a knowledge graph of common-
sense semantic relations in open, conversational dia-
log. Figure 2 traces the growth of a knowledge graph

over a single dialog. Section 3.1 details how knowl-
edge is extracted from user explanations without a
dialog model. Section 3.2 describes dialog strate-
gies that elicit natural language explanations.

KNOWBOT uses task progress to drive natural lan-
guage understanding. It assumes the user intends
to provide one or more novel relations, and uses
the constraints described in section 3.1.1 to disam-
biguate noisy relations. This way, KNOWBOT knows
when the dialog progresses because its confidence in
the user’s chosen answer increases.

3.1 Building knowledge graphs from dialog
KNOWBOT builds KGs at three levels: per utter-
ance, per dialog, and globally over all dialogs. An
utterance-level knowledge graph (uKG) (Figure 2a)
is a fully connected graph whose nodes are all con-
cepts in an utterance. After aggressive pruning (sec-
tion 3.1.1), remaining edges update a dialog-level
knowledge graph (dKG) (Figure 2b; section 3.1.2).

Upon dialog termination, the dKG updates the
global knowledge graph (gKG), which stores rela-
tions acquired from all dialogs (section 3.1.3).

3.1.1 Utterance-level KGs
KNOWBOT initially relates every pair of concepts in
an utterance, then prunes them based on two con-
straints: alignment and adjacency.

Each user explanation is first converted into a
fully-connected utterance-level knowledge graph.
This uKG is noisy because users don’t intend
relations between every pair of keywords in
their utterance. For example, a typical utterance
U: freezes means it changes water
from a liquid to a solid mentions six
concepts, freezing, meaning, change, water, liquid,
solid, with

(
6
2

)
potential binary relations. Not every

relation is salient to the question. To remove noisy
relations, edges in the uKG are aggressively pruned
with two simple, rational constraints:

1. Alignment. An edge can only relate a question
concept to a support concept.

2. Adjacency. Edges can’t relate concepts whose
keywords are adjacent in the utterance.

The intuition for the alignment constraint is that the
user intends each explanation to relate the question

853



(a) utterance-level knowledge graph (uKG)

(b) dialog-level knowledge graph (dKG)

(c) The dialog goal is to align Q and S

Figure 2: Every pair of concepts in each user utterance is related then aggressively pruned. (a) Utterance-level
knowledge graphs represent individual utterances. Concepts (underlined, inset in nodes) are obtained by removing
stopwords and stemming. An edge that either doesn’t connect a question and support concept or else which connects
concepts whose keywords in the user utterance have no intervening words (intexts) are pruned, indicated here with
dashed lines. (b) The four remaining relations are stored in a dialog-level dKG.

to a known fact, and other relations in the utter-
ance are unintentional. For example, in the uKG for
the first utterance in Figure 2(a), the edge between
melt[ing] and heat is an alignment relation because
melt[ing] is a concept in S and heat is a concept in
Q. But the edge between because and heat is pruned
(dashed lines) since because is not a concept in S.

Adjacency is a simple, practical syntactic fea-
ture to reduce spurious relations. Users typically
put words or intexts between concepts they in-
tend to relate. The edge between melt[ing] and
because is pruned since their keywords are ad-
jacent in U1: it’s melting because of
heat, while U2 relates snow and ice with the intext
has the same behavior as the.

We find these constraints effective in practice, but
at this point other pruning constraints can be de-
ployed. A strength of our approach is that it wel-
comes aggressive pruning: just as in human-human
interaction, users who initially fail to communicate
their intention can try again later in the dialog.

3.1.2 Dialog-level KGs
Each dialog focuses on a single question. KNOW-
BOT starts with an empty dialog-level knowledge
graph (dKG). After each user turn, edges from that
turn’s uKG are added to the dKG, and KNOWBOT

rescores each of the four answers according to equa-
tion (1) where the set of relations RQi,Si,j is exactly
the set of edges in the dKG. The dialog successfully
terminates when the user’s answer has the high-
est alignment score, indicating the “missing knowl-
edge” has been successfully provided by the user.

3.1.3 The global knowledge graph
The global knowledge graph (gKG) includes every
relation learned from every KNOWBOT dialog.

Because we do not use a fixed ontology or com-
prehensive dialog model, individual dialogs can re-
sult in noisy relations even after aggressive prun-
ing. However, as KNOWBOT conducts more dialogs
about the same problem, relations that more often
re-occur are more likely to be salient to the problem.

In this work, KNOWBOT takes advantage of re-
dundancy with a simple filter: it ignores singleton
relations originating in a single user utterance. We
find even this simple filter increases performance.
As KNOWBOT accumulates more dialogs, frequency
can be incorporated in more sophisticated models.

3.2 Dialog strategies for knowledge acquisition

We’ve described how a user’s free text explana-
tions are converted into knowledge graphs. Each
user explanation is uttered in response to a system

854



prompt. A dialog system’s dialog manager chooses
the prompt to say next according to its dialog strat-
egy, which maps each system state to an action. An
effective dialog strategy guides users to informative
explanations that provide novel relations which let
KNOWBOT successfully answer the question.

We compare two different strategies. A user-
initiative strategy always asks open-ended questions
to prompt the user for new explanations, e.g. line
S2 in Figure 1. These prompts let users introduce
salient concepts on their own.

In contrast, a mixed-initiative strategy utilizes fo-
cused prompts (line S4 in Figure 1) to introduce po-
tentially related concepts. KNOWBOT chooses what
pair of concepts to ask about based on how discrim-
inative they are. The most discriminative concepts
are the pair of question and support concepts that
(1) don’t already have an edge between them, (2)
satisfies the alignment constraint for the user’s an-
swer, and (3) satisfies the alignment constraint for
the fewest alternative answers. By proposing rela-
tions that would lead to a swift completion of the
dialog task, KNOWBOT shares the burden of knowl-
edge acquisition with the user.

Both dialog strategies are question-independent,
but because we don’t use a comprehensive dialog
model to represent the state space, we rely on hand
built rules instead of optimizing with respect to a
reward function. For example, KNOWBOT always
starts by asking the user for their answer, and if a
new support sentence is found will always immedi-
ately present it to the user for confirmation.

4 Evaluation of dialog strategies

Our first experiment compares mixed-initiative and
user-initiative strategies (section 3.2) to a baseline
interactive query expansion (section 4.1). The pur-
pose of this experiment is to investigate whether
users can successfully complete our complex dialog
task even though we don’t use a trained semantic
parser for natural language understanding.

Dialogs were conducted through a web browser.
Users were colleagues and interns at the Allen Insti-
tute for Artificial Intelligence, and so were familiar
with the question-answering task but were not ex-
pert annotators. Users were invited to converse with
the system of their choice, and to move on to a new

question if they felt the dialog was not progressing.
Individual dialog sessions were anonymous.

The system starts each dialog with an empty
knowledge graph, using only identity relations to se-
lect its answer. This default answer is correct on
44 of the 107 questions, and an additional 10 ques-
tions have no associated supporting sentence for the
correct answer in SCITEXT. We run dialogs for the
remaining 53 questions, for which each answer can-
didate has 80 supporting sentences in SCITEXT on
average. A successful dialog terminates when the
system extracts enough novel relations from the user
that the correct answer has the highest alignment
score with one of its supporting sentences.

4.1 Baseline: Interactive query expansion
To evaluate whether task-driven relation extraction
is an effective method for knowledge acquisition in
the absence of an explicit dialog model, we also im-
plement a baseline dialog strategy based on interac-
tive query expansion (IQE). This baseline is similar
to the recent knowledge acquisition dialog system of
Rudnicky and Pappu (2014a; 2014b).

In IQE, new knowledge is learned in the form of
novel keywords that are appended to the question-
answer statement. For example, the dialog in Figure
1 shows the user teaching KNOWBOT how metal re-
lates to electricity. KNOWBOT understands that the
user intends that relation because it drives the dia-
log forward. IQE, in contrast, treats the user ut-
terance as an unstructured bag of keywords. The
unrecognized word “metal” is added to the bag of
keywords representing each of the four alternative
answers to form four augmented queries, and new
overlap scores against sentences from SCITEXT are
computed. The dialog progresses whenever a new
vocabulary word increases the score for the aug-
mented query for the user’s chosen answer.

The intuition behind query expansion is that users
will explain their answers with salient keywords
missing from the original question sentence. The ex-
panded query will overlap with and uprank a support
sentence that contains the acquired keywords.

4.2 Performance metrics
Task completion is the proportion of dialogs that
end in agreement. Higher task completion indicates
that the dialog system is more successful in acquir-

855



ing enough knowledge by the end of the dialog to
change its answer from incorrect to correct.

Dialog length is the number of system and user
turns. Shorter dialogs are more efficient.

Acquisition rate is the number of edges in the
dKG at the end of each dialog. Acquisition rate mea-
sures two contrasting system features:

(1) how much new knowledge is acquired, and
(2) how much explanatory effort users expend.

From the perspective of raw knowledge acquisition,
higher acquisition rate is better because each dialog
adds more edges to the knowledge graph. From the
perspective of usability, lower acquisition rate is bet-
ter provided it doesn’t negatively affect dialog suc-
cess, because it indicates the user is able to success-
fully correct the system’s answer with a fewer num-
ber of explanatory relations.

4.3 Results
Our results (Table 1) show both strategies dramati-
cally outperform the baseline and have comparable
success rate and dialog length to each other. User-
initiative strategies acquire more knowledge per di-
alog but require more user effort.

IQE U.I. M.I.
Total dialogs 35 27 57
Task completion rate 5.7% 55.6% 49.1%
Mean Dialog Length 14.1 10.6 10.9
Mean acquisition Rate N/A 13.5 7.4

Table 1: Comparison of knowledge acquisition strate-
gies. Interactive query expansion (IQE)’s poor task com-
pletion indicates keywords can’t bridge the knowledge
gap. Relations are more successful. User-initiative (U.I.)
and mixed-initiative (M.I.) strategies have comparable
task completion and dialog length, but U.I. extracts twice
the relations before getting the correct answer: more
knowledge acquired but at the cost of more explanatory
effort. User comments indicate M.I. is more satisfying.

We find that the baseline has a very low comple-
tion rate of 5%, and longer dialog lengths of 14 turns
on average. Interactive query expansion is a poor
knowledge acquisition dialog strategy for our task.

In contrast, users were able to successfully correct
our system using both strategies about 50% of the
time, even though no in-domain ontology guides ex-
tractions and no comprehensive dialog model clas-

sifies explanations. The average dialog lengths and
completion rate for User Initiative (U.I.) and Mixed
Initiative (M.I.) strategies was approximately the
same, so that choice of strategy had little impact
on overall task success. However, strategy has a
great effect on acquisition rate. M.I. cuts the knowl-
edge acquisition rate nearly in half when compared
to U.I (7.4 novel relations per dialog to 13.5). M.I.
learns fewer new relations per dialog with compara-
ble task success, which means each dialog succeeds
with much less explanatory effort by the user but
also contributes less to the knowledge graph.

User comments indicated that the mixed-initiative
strategy was the most enjoyable system to use. We
find that open-ended, user-initiative strategies can
acquire more helpful relations in a single dialog but
guided, mixed-initiative strategies may be more ap-
propriate when usability is taken into account. Be-
cause our goal is lifelong interactive knowledge ac-
quisition, the impact of a single dialog on the total
knowledge graph is less important than the individ-
ual user effort required, and we conclude that the
mixed-initiative strategy is preferable.

5 Evaluation of knowledge quality

Experiment 1 evaluated whether users could suc-
cessfully complete our dialog task. Next, we eval-
uate whether the total output of our system, all rela-
tions acquired during all 431 conducted dialogs, rep-
resents useful domain knowledge on this task. We
evaluate on questions for which dialogs have been
held to investigate whether it’s possible to learn any
domain knowledge from natural language conversa-
tion without a dialog model, irrespective of overfit-
ting. We then use cross-validation to test if knowl-
edge transfers between questions.

As described in section 2, our QA system de-
composes each question/answer pair into a true/false
statement and chooses as its answer the statement
among the four that has the best supporting sentence
in a text corpus. Equation (1) scores each question-
answer statement by using domain relations to align
question concepts to support concepts. The next sec-
tion describes sources of domain relations.

5.1 Sources of domain knowledge

We compare relations from five sources:

856



IDENTITY: An edgeless knowledge graph. The
only relations are between identical concepts, equiv-
alent to Jaccard overlap of concept keyword roots.

WORDNET: Paraphrase relations from Wordnet.
Wordnet (Fellbaum, 1998) is a lexical database of
synonyms and hypernyms common in NLP tasks.
For example, Snow et al (2006) use Wordnet as
training data for ontology induction. To build
WORDNET, we draw an edge between every pair
of Wordnet concepts (ws, wq) for which the Wu-
Palmer Similarity (WUP) (Wu and Palmer, 1994)
of the first sense in each concept’s synset exceeds
0.9, the best-performing WUP threshold we found.
Concepts in the Wordnet hierarchy have a higher
WUP when they have a closer common ancestor. If a
known fact is Heat energy causes snow to melt, but a
question asks if ice melts, then Wordnet should pro-
vide the missing knowledge that ice acts like snow.

PPDB: Paraphrase relations from PPDB (Gan-
itkevitch et al., 2013) are derived by aligning bilin-
gual parallel texts. PPDB is divided into subsets
where the larger subsets have more paraphrases with
less precision. We tried all subsets and found the
smallest to give the best results, which we report
here. The largest performed the worst of all rela-
tion sets we tested. We use the lexical paraphrases,
which relates unigrams. Concepts are related when
at least one concept keyword for each are para-
phrases in PPDB. We obtained better performance
by stemming PPDB words: for example, if snows
and iced are paraphrases in PPDB then we also con-
sidered snowing and icy to be in PPDB.

KNOWBOT: Each question is answered using re-
lations pooled from all dialogs about all questions.
The goal in each dialog is to acquire knowledge
helpful to answer the question. If KNOWBOT leads
to an increase in QA accuracy over IDENTITY, then
we can successfully use open dialog with a human in
the loop to learn knowledge that solves a question.

LEAVE-ONE-OUT: Each question is answered
only with relations learned during dialogs for ev-
ery other question. While KNOWBOT uses re-
lations learned from dialogs about the questions
on those same questions, LEAVE-ONE-OUT tests
whether knowledge generalizes to questions without
dialogs. Generalization is only possible when there
are at least two questions involving the same con-
cepts. Due to our small number of questions, in the

best case we expect only slight improvement.

%correct
IDENTITY 41%
WORDNET 34%
PPDB 39%
KNOWBOT 57%
LEAVE-ONE-OUT 45%

Table 2: QA accuracy on the 107 questions with dif-
ferent sources of domain knowledge. IDENTITY: iden-
tity relations only, e.g. “heats” to “heating.” WORD-
NET: Wordnet-derived pseudo-synonyms, e.g. “eagle”
to “owl.” KNOWBOT: the full, unablated global KG.
LEAVE-ONE-OUT: answers each question while ignoring
relations acquired during dialogs on that question.

5.2 Results
The results of QA using the different domain knowl-
edge is shown in Table 2. IDENTITY achieves
41% accuracy on this difficult reasoning task, show-
ing that some questions are answerable by search-
ing SCITEXT for supporting sentences with the
same concepts as in the question-answer statement.
WORDNET works surprisingly poorly. Examination
found WORDNET’s relations to be of good quality,
yet underperform IDENTITY. PPDB performed bet-
ter but still underperformed IDENTITY. We con-
clude that general paraphrase bases introduce too
much noise to apply directly without manual cura-
tion to our science domain, underscoring the need
for domain-specific knowledge acquisition.

KNOWBOT achieves accuracy of 57%, a dramatic
improvement over both baselines. Importantly, this
value does not test generalization to unseen ques-
tions, since KNOWBOT has held dialogs on these
questions. However, it does show that our system
can effectively learn about its domain: a poor dia-
log extraction system will fail to extract any helpful
knowledge from users during a training dialog. This
is a significant result because it shows that we suc-
cessfully acquire knowledge to solve many question
through conversational interaction without the over-
head of a closed dialog model or fixed ontology.

We also tested how well knowledge generalizes
with LEAVE-ONE-OUT. Our question set is less
suited to evaluate generalization because it covers
a wide range of topics with little overlap between

857



questions. We still found LEAVE-ONE-OUT to be
the second-best performer with accuracy of 45%, a
10% relative improvement versus IDENTITY. Re-
dundancy is an effective noise reduction constraint:
when LEAVE-ONE-OUT ignores redundancy and in-
cludes singleton relations (those originating in a sin-
gle dialog utterance), its accuracy reduces to 32%.

6 Related work

Knowledge acquisition in dialog has long been a
central goal of AI research. Early dialog systems
acquired knowledge through ambitious interaction,
but were brittle, required hand-defined dialog mod-
els and did not scale. Terry Winograd (1972) pre-
sented the first dialog system that acquires knowl-
edge about the block world. TEIRESIAS (Davis,
1977) refines inference rules from terse interaction
with experts. CONVINCE (Kim and Pearl, 1987)
and its prototypes (Leal and Pearl, 1977) learn de-
cision structures through stylized but conversational
dialogs. An interactive interface for CYC (Witbrock
et al., 2003) learns from experts but don’t use natural
language. Fernández et al (2011) argue the impor-
tance of interactive language learning for conversa-
tional agents. Williams et al (2015) combine active
learning and dialog to efficiently label training data
for dialog act classifiers.

Relatively little work integrates relation extrac-
tion and dialog systems. Attribute-value pairs from
restaurant reviews can generate system prompts
(Reschke et al., 2013), and single-turn exchanges
with search engines can populate a knowledge graph
(Hakkani-Tur et al., 2014). Dependency relations
extracted from individual dialog utterances by a
parser also make effective features for dialog act
classification (Klüwer et al., 2010).

The work closest to our own, Pappu and Rudnicky
(2014a; 2014b), investigates knowledge acquisition
strategies for academic events. Their system asks
its users open-ended questions in order to elicit in-
formation about academic events of interest. They
compare strategies by how many new vocabulary
words are acquired, so that the best strategy prompts
the user to mention the most OOV words. In their
most recent work (2014b), they group the acquired
researcher names by their interests to form a bipar-
tite graph, and use acquired keywords for query ex-

pansion in a simple information retrieval task. Our
present contribution builds on this general idea, but
we learn an unlimited number of relations and con-
cepts from open dialogs, whereas they learn a small
number of relations belonging to a fixed ontology
from closed dialogs. We also show the acquired
knowledge is objectively useful for QA.

In closed dialog systems, the system’s dialog
model explicitly represents the meaning of every po-
tential user utterance. Any utterance not represented
by this comprehensive model is rejected and the user
asked to rephrase. Closed dialog systems work well
in practice. For example, in the well-studied slot-
filling or frame-filling model, users fill slots to con-
strain their goal, and an NLU module decomposes
user utterances to known actions, slots, and val-
ues. A slot-filling system to find flights might map
the utterance U: Show me a flight from
Nashville to Seattle on Sunday to the
action find-flight and the filled slots origin =
Nashville, destination = Seattle, and time = Sun-
day. However, for our domain, each distinct ques-
tion warrants its own actions, slots, and values. Such
a complex model would require abundant training
data or laboriously handcrafted interpretation rules.

In contrast, an open dialog system can usefully in-
terpret, learn from, and respond to user utterances
without a comprehensive dialog model. Domain-
independent dialog systems with the flexibility to
accept novel user utterances are a longstanding goal
in dialog research (Polifroni et al., 2003). Recent
work to address more open dialog includes boot-
strapping a semantic parser from unlabeled dialogs
(Artzi and Zettlemoyer, 2011), extracting poten-
tial user goals and system responses from backend
databases (Hixon and Passonneau, 2013), and in-
ducing slots and slot-fillers from a corpus of human-
human dialogs with the use of FrameNet (Chen et
al., 2014). These works focus on systems that learn
about their domain prior to any human-system dia-
log. Our system learns about its domain during the
dialog. While we rely on a limited number of tem-
plates to generate system responses, unscripted user
utterances can usefully progress the dialog. This al-
lows relation extraction from complex natural lan-
guage utterances without a closed set of recognized
actions and known slot-value decompositions.

858



7 Discussion and Future Work

KNOWBOT acquires helpful, task-driven relations
from conversational dialogs in a difficult QA do-
main. A dialog is a success when it produces knowl-
edge to solve the question. Extractions increase QA
accuracy on questions for which dialogs have been
held, indicating that knowledge acquisition dialogs
can succeed without a closed dialog model by us-
ing task progress and careful pruning to drive natu-
ral language understanding. Our method is general
enough to scale to any task in which alternative di-
alog goals can be presented to a user and the sys-
tem’s confidence in each alternative computed from
semantic relations between concepts.

Our focus is on facilitated knowledge acquisition
rather than question-answering, so we purposefully
keep inference simple. The alignment score is a Jac-
card overlap modified to use relations, which makes
it fast and practical, but results in many ties which
we score as incorrect, and also ignores word or-
der. For example, the bag-of-keywords is identical
for contradicting answers “changing from liquid to
solid” and “changing from solid to liquid.” To make
this distinction, we could use an alignment score that
is sensitive to word order such as an edit distance.
We could expand our simple pruning constraints to
take more advantage of syntax, for example by us-
ing dependency parsers optimized for conversational
language (Kong et al., 2014).

The relational model for reasoning is both flexible
and powerful (Liu and Singh, 2004). However, in a
small number of cases, relations that align known
facts with question-answer statements are unlikely
to lead to the correct answer. For example, our ques-
tion set contains a single math problem, How long
does it take for Earth to rotate on its axis seven
times? (A) one day (B) one week (C) one month (D)
one year. The multiplication operation necessary to
infer the answer from the SCITEXT fact “The Earth
rotates, or spins, on its axis once every 24 hours”
is not easily represented by our model and requires
other techniques (Hosseini et al., 2014).

We observed only slight transfer of knowledge be-
tween questions. A larger question set with multiple
questions per topic will allow us to better evaluate
knowledge transfer. Our long-term goal is learn-
ing through any conversational interaction in a com-

pletely open domain, but because the fundamen-
tal trick that enables model-free NLU is computing
progress towards an explicit dialog goal as a func-
tion of possible extractions, our current method is
limited to tasks with explicit goals.

The simple redundancy filter we use effectively
distinguishes salient from noisy relations, but could
be improved with a model of relation frequency.
We consider all acquired relations equally salient,
but future work will examine how to rank relation
saliency. We will also examine how dialog fea-
tures can help distinguish between paraphrase, en-
tailment, and negative relations.

Our open system acquires relations from a wide
variety of user explanations without the bottleneck
of a hand-built dialog model, but the tradeoff is that
we use relatively simple, templated system prompts.
However, our collected corpus of real human-system
dialogs can be used to improve our system in fur-
ther iterations. For example, the knowledge graphs
we produce are targeted, question-specific semantic
networks, which could be used in lieu of FrameNet
to induce domain-specific dialog models (Chen et
al., 2014). With a dialog model to represent the
state space, reinforcement learning could then be
employed to optimize our strategies.

While most question-answering systems focus on
factoid questions, reasoning tasks such as ours re-
quire different techniques. Our method generalizes
to other non-factoid QA tasks which could usefully
employ relations, such as arithmetic word problems
(Hosseini et al., 2014) and biology reading compre-
hension questions (Berant et al., 2014).

Acknowledgments

This research was conducted at the Allen Institute
for Artificial Intelligence. We’d like to thank Luke
Zettlemoyer, Mark Yatskar, Rik Koncel-Kedziorski,
Eric Gribkoff, Oren Etzioni and the anonymous re-
viewers for helpful comments, and AI2 interns and
colleagues for their support and participation in the
user studies. The first author was supported by
the National Science Foundation Graduate Research
Fellowship Program under Grant Number DGE-
1256082. The third author was supported by grants
from the Allen Institute for AI (66-9175) and the
NSF (IIS-1352249).

859



References
Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrapping

semantic parsers from conversations. In Proceedings
of the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 421–432, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.

Holger Bast, Debapriyo Majumdar, and Ingmar Weber.
2007. Efficient interactive query expansion with com-
plete search. In Proceedings of the Sixteenth ACM
Conference on Conference on Information and Knowl-
edge Management, CIKM ’07, pages 857–860, New
York, NY, USA. ACM.

Jonathan Berant, Vivek Srikumar, Pei-Chun Chen,
Abby Vander Linden, Brittany Harding, Brad Huang,
Peter Clark, and Christopher D. Manning. 2014.
Modeling biological processes for reading comprehen-
sion. In Proceedings of EMNLP.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam Hruschka, and Tom Mitchell. 2010.
Toward an architecture for never-ending language
learning. In AAAI Conference on Artificial Intelli-
gence.

Yun-Nung Chen, William Yang Wang, and Alexander I.
Rudnicky. 2014. Leveraging frame semantics and dis-
tributional semantics for unsupervised semantic slot
induction in spoken dialogue systems. In 2014 IEEE
Spoken Language Technology Workshop (SLT 2014).

Peter Clark, Niranjan Balasubramanian, Sumithra Bhak-
thavatsalam, Kevin Humphreys, Jesse Kinkead,
Ashish Sabharwal, and Oyvind Tajford. 2014. Au-
tomatic construction of inference-supporting knowl-
edge bases. In 4th Workshop on Automated Knowl-
edge Base Construction (AKBC).

Randall Davis. 1977. Interactive transfer of expertise:
Acquisition of new inference rules. In Proceedings of
the 5th International Joint Conference on Artificial In-
telligence. Cambridge, MA, August 1977, pages 321–
328.

Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam Mausam. 2011.
Open information extraction: The second generation.
In Proceedings of the Twenty-Second International
Joint Conference on Artificial Intelligence - Volume
Volume One, IJCAI’11, pages 3–10. AAAI Press.

Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.

Raquel Fernández, Staffan Larsson, Robin Cooper,
Jonathan Ginzburg, and David Schlangen. 2011. Re-
ciprocal learning via dialogue interaction: Challenges
and prospects. Proceedings of the IJCAI 2011 Work-
shop on Agents Learning Interactively from Human
Teachers (ALIHT 2011).

David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya A Kalyanpur, Adam Lally,
J William Murdock, Eric Nyberg, John Prager, et al.
2010. Building watson: An overview of the deepqa
project. AI magazine, 31(3):59–79.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of NAACL-HLT, pages 758–
764, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.

Dilek Hakkani-Tur, Asli Celikyilmaz, Larry Heck,
Gokhan Tur, and Geoff Zweig. 2014. Probabilistic en-
richment of knowledge graph entities for relation de-
tection in conversational understanding. In Proceed-
ings of Interspeech. ISCA - International Speech Com-
munication Association, September.

Catherine Havasi, Robert Speer, and Jason Alonso. 2007.
Conceptnet 3: a flexible, multilingual semantic net-
work for common sense knowledge. In Recent Ad-
vances in Natural Language Processing, Borovets,
Bulgaria, September.

Ben Hixon and Rebecca J. Passonneau. 2013. Open di-
alogue management for relational databases. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1082–
1091, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.

Javad Mohammad Hosseini, Hannaneh Hajishirzi, Oren
Etzioni, and Nate Kushman. 2014. Learning to solve
arithmetic word problems with verb categorization.
In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 523–533. Association for Computational Lin-
guistics.

Jin H. Kim and Judea Pearl. 1987. Convince: A conver-
sational inference consolidation engine. IEEE Trans.
Syst. Man Cybern., 17(2):120–132, March.

Tina Klüwer, Hans Uszkoreit, and Feiyu Xu. 2010.
Using syntactic and semantic based relations for dia-
logue act recognition. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Posters, COLING ’10, pages 570–578, Stroudsburg,
PA, USA.

Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
A. Noah Smith. 2014. A dependency parser for
tweets. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 1001–1012. Association for
Computational Linguistics.

Antonio Leal and Judea Pearl. 1977. An interactive pro-
gram for conversational elicitation of decision struc-

860



tures. IEEE Transactions on Systems, Man, and Cy-
bernetics, 7(5):368–376.

Hugo Liu and Push Singh. 2004. Commonsense reason-
ing in and over natural language. In Proceedings of the
8th International Conference on Knowledge-Based In-
telligent Information and Engineering Systems (KES-
2004. Springer.

Aasish Pappu and Alexander Rudnicky. 2014a. Knowl-
edge acquisition strategies for goal-oriented dialog
systems. In Proceedings of the 15th Annual Meeting of
the Special Interest Group on Discourse and Dialogue
(SIGDIAL), pages 194–198, Philadelphia, PA, U.S.A.,
June.

Aasish Pappu and Alexander Rudnicky. 2014b. Learning
situated knowledge bases through dialog. In Proceed-
ings of Interspeech, September.

Joseph Polifroni, Grace Chung, and Stephanie Sen-
eff. 2003. Towards automatic generation of mixed-
initiative dialog systems from web content. In Eu-
rospeech.

M. F. Porter. 1997. Readings in information retrieval.
chapter An Algorithm for Suffix Stripping, pages 313–
316. Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.

Kevin Reschke, Adam Vogel, and Dan Jurafsky. 2013.
Generating recommendation dialogs by extracting in-
formation from user reviews. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
499–504, Sofia, Bulgaria, August. Association for
Computational Linguistics.

Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 801–808. Association for Computa-
tional Linguistics.

Jason D. Williams, Nobal B. Niraula, Pradeep Dasigi,
Aparna Lakshmiratan, Carlos Garcia Jurado Suarez,
Mouni Reddy, and Geoff Zweig. 2015. Rapidly scal-
ing dialog systems with interactive learning. In 2015
International Workshop Series on Spoken Dialogue
Systems Technology (IWSDS), January.

T. Winograd. 1972. Understanding natural language.
Academic Press.

Michael Witbrock, David Baxter, Jon Curtis, Dave
Schneider, Robert Kahlert, Pierluigi Miraglia, Peter
Wagner, Kathy Panton, Gavin Matthews, and Amanda
Vizedom. 2003. An interactive dialogue system for
knowledge acquisition in cyc. In Proceedings of the
IJCAI-2003 Workshop on Mixed-Initiative Intelligent
Systems., pages 138–145.

Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32Nd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’94, pages 133–138, Stroudsburg, PA,
USA. Association for Computational Linguistics.

861


