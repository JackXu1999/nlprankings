



















































Individual Differences in Strategic Deception


Proceedings of NAACL-HLT 2016, pages 45–52,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Individual Differences in Strategic Deception

Scott Appling and Erica Briscoe
Georgia Tech Research Institute
Georgia Institute of Technology

Atlanta, GA, United States
scott.appling@gtri.gatech.edu
erica.briscoe@gtri.gatech.edu

Abstract

While text-based deception in computer me-
diated communication has been studied, e.g.,
Zhou (2005) and Duran et al. (2010), there has
been less focus on the differentiation of strate-
gies for deception, especially those which may
manifest in modern communication, such as
found in social media. In this paper, we extend
our previous work on the evaluation of linguis-
tic indicators to strategic deception (Appling
et al., 2015), by evaluating the relationship be-
tween personality and deceptive strategy use
and the utilization of linguistic features for in-
ferring both (personality and deception). We
find that even with a relatively small corpus,
there is evidence that personality is related to
particular deception strategies, though in short
social media communications, these personal-
ity traits are difficult to infer using standard
linguistic measures (e.g., LIWC). We also de-
scribe the corpus we collected from an exper-
iment in which subjects engaged in deception
through a social media platform.

1 Introduction

The ubiquitous nature of social media has signifi-
cantly magnified the communicative ability of indi-
viduals and groups and changed the way many in-
dividuals receive information (Lenhart et al., 2010).
Consequently, this increased access has enlarged the
influence opportunity space, where deception is a
commonly utilized influence method (Buller et al.,
1994). A theme of our research is focused on the
evaluation of how findings from previously stud-
ied communication methods, e.g., Burgoon et al.

(1996), differ as a result of both changing communi-
cation styles and the affordances provided by mod-
ern communication platforms (i.e., social media).

Research into “tells” that indicate when a person
is lying have been a subject of interest for many
years, e.g. (Ekman and Friesen, 1969), and has re-
sulted in a substantial list of potential cues (DePaulo
et al., 2003). These cues to deception are usually
described as those that occur significantly more or
less frequently when a communicator expresses an
untruth as compared to the truth. These indicators
may differ according to the point of view of the re-
ceiver and sender (Anderson et al., 1999) and have
been identified in widespread communications, such
as the 2016 United States election coverage (Scott,
2016). While our previous work evaluated the de-
tection of deceptive statements and deception strate-
gies using linguistic cues (Briscoe et al., 2014; Ap-
pling et al., 2015), here we describe our evaluation
of how individual differences, specifically as rep-
resented through different personalities, may affect
strategic deception and be inferred from an experi-
mentally collected corpus.

2 Eliciting Deception Strategies

Past work has characterized multiple deception
strategies (Rubin, 2010; Turner et al., 1975; Bur-
goon et al., 1996), roughly organized into the four
main types described below.

• Falsification - e.g. lies, contradictions, or “dis-
tortions”

• Exaggeration - e.g. more or modified informa-
tion via superlatives

45



• Omission - e.g. secrets (missing information),
half-truths (less or modified information), or
‘concealment”

• Misleading - e.g. topic changes, irrelevant in-
formation, equivocation, or ‘diversionary re-
sponses”

To evaluate the use of the previously described set
of strategies, we conducted human subject experi-
mentation to simulate an online social network en-
vironment where participants were tasked with en-
gaging in a conversation involving a decision mak-
ing task where they were encouraged to employ de-
ception. We also administered a personality inven-
tory to assess each participant’s standing on the di-
mensions contained within the Five Factor Model
(FFM) of personality (McCrae and Costa Jr, 1999):
openness, conscientiousness, extroversion, agree-
ableness, neuroticism. Then we extracted lingus-
tic features that were found to be statistically sig-
nificant predictors of related personality traits. We
then report on an analysis on the use of linguistic
features for predicting deception strategies based on
individual-trait level psychological constructs (per-
sonality).

The rest of the paper is organized as follows. First
we describe the experimental setup and the collected
data. Next, we discuss the deception strategies cor-
pus that was created. We then discuss the results
of two analyses: one related to predicting deception
strategy on personality trait scores and a second on
using psycholinguistic features to predict personal-
ity. We conclude with a discussion of the implica-
tions of the results and future directions

3 Deception Experiment

To study deception strategies in social media, we de-
signed and ran an in-lab experiment where individ-
uals were asked to use a social media platform for a
group decision making task. The task allowed us to
control the scenario as well as topics of discussion
within the conversations.

Seventy-five subjects recruited from the student
body at Georgia Tech participated in the experiment.
Each subject was seated at a computer station and
asked to use a mock online social media site (Face-
Friend, see Figure 1). The mock site was intended

Figure 1: Example screenshot of the FaceFriend web-based
interface used in the experiment. The interface was designed to

mimic commonly used social media platforms.

to resemble a popular social networking site. Partic-
ipants completed three pre-experiment surveys, in-
cluding a personality test and a survey regarding
their social media and email usage, so as to en-
sure consistency among familiarity with social me-
dia. The procedure and results reported below are
from one of a three part experiment, the other two
parts of which are not reported here (as they cen-
tered on the perception of deception and credibility,
see Briscoe et al. (2014) and Briscoe et al. (2013) for
details of the other parts of the study. Subjects were
paid for their participation.

Participants were asked to read a scenario involv-
ing a group decision making task called the “sub-
arctic survival problem” (Eady and Lafferty, 1969).
Participants were instructed to log in to a mock so-
cial media platform, FaceFriend, and to communi-
cate with another individual to decide the best rank-
ing of items to take from a crashed plane site in
an Arctic environment. Participants were further
instructed to advocate for a specific ranked list of
items, also provided, and to do so using deception,
when possible. To ameliorate differences in creative
ability, subjects were provided with expert expla-
nations for the advantages provided by each item;
these could be used as the basis for creating de-
ceptive statements. For example, the list of items

46



included an Axe, which the expert noted could be
used to chop wood. Lastly, participants were os-
tensibly informed that the other conversation par-
ticipant would be unaware of any potential decep-
tion. The participant’s conversational partner was
a confederate member of the research team, sitting
in another room. During the conversation, to min-
imize variation in the confederate’s response lan-
guage, the confederates communicated using a list
of statements conceived beforehand as suitable de-
cision making conversation responses (e.g., “What
about the canvas?”). After discussing the items for
12 minutes, the interaction was concluded, and sub-
jects were asked to log out of FaceFriend.

After the end of the conversation, participants
were shown the list of statements they made during
the task in a browser windows and asked to identify
which ones were deceptive using a check box. Upon
indicating a particular statement was deceptive, par-
ticipants were prompted to categorize their strategy
for that particular message. The possible strategies
to select were the same types described in Section 2.
Multiple strategies were allowed to be designated.
An additional category, “Other”, was available that
allowed free response space for any strategies that
the subject felt did not fit into one of the four pro-
vided categories. Table 3 displays example decep-
tive statements, as identified by subjects, in each cat-
egory.

4 Deception Corpus

The deception corpus contains all statements made
by participants during the deception experiment de-
scribed earlier. These include truthful and deceptive
statements. For each statement we provide a boolean
indicator as to whether or not the participant anno-
tated that statement as enacting one or more of the
deception strategies described in Section 2. 1

Table 4 lists the frequencies of each identified
strategy in the corpus as indicated by the subjects.
Statements which subjects identified as containing
multiple strategies are not included. On average, the
misleading strategy was the most likely strategy to
be employed by participants and the omission strat-
egy the least likely. Table 1 lists summary statistics
about the statements in the corpus. Sentiment and

1The collected corpus is available by emailing the authors.

subjectivity scores were calculated for each state-
ment, using the method described by De Smedt and
Daelemans (2012). Table 2 provides the descriptive
statistics of the distribution of personality variables
across our subject pool, as well as relevant demo-
graphic information.

5 Analysis

5.1 Linguistic Cues and Deception Strategies

After excluding statements falling below a mini-
mum one word threshold2, 393 statements (includ-
ing truthful statements) were used in a one-way mul-
tivariate analysis of variance to determine the ef-
fect of deception strategy on linguistic cue levels.
The 8 linguistics cues studied are listed in Table 1.
A statistically significant difference was found be-
tween the different deception strategies and truthful
statements on the combined linguistic cues depen-
dent variables F (32, 1406.654) = 1.93, p < 0.001;
Wilks’ Λ = 0.852; partial η2 = 0.039, though
no specific differences were found between specific
strategies. Follow-up univariate ANOVAs showed
a statistically significant difference between the use
of deception strategies and no strategy using a Bon-
ferroni adjusted α level of 0.025. Tukey post-hoc
tests showed: there were less average verb cues
for truthful statements compared to exaggerations
(p < 0.001) and compared to misleading statements
(p < 0.019) (M = 4.695;SD = 0.274;M =
6.657;SD = 0.419;M = 6.222, SD = 0.413,
respectively); lower mean word count for truth-
ful statements compared to exaggeration statements
(p < 0.019;M=20.024; SD=0.899; M=25.086;
SD=1.376, respectively); and lower mean Fleish-
Kincaid score cues for truthful statements compared
to exaggeration statements (p < 0.005; M =
3.228;SD = 0.438;M = 5.993;SD = 0.670, re-
spectively).

2Exclusion of one-word statements was based on the fact
that these statements were almost exclusively one-word re-
sponses (i.e., “yes” or “no”) to questions posed by the confed-
erate. While these responses may exhibit untruths, the authors
feel that the reflection of deception is mostly based in the con-
tent of the interrogation, rather than the response itself.

47



Table 1: Deception Conversation Corpus Summary Statistics per Subject

Variable Mean Std. Dev. Min. Max.
Per Subject
Statement Count 6.00 3.62 1.00 17.0
Strategy: Exaggeration 1.20 1.07 0 4.00
Strategy: Misleading 1.31 1.21 0 6.00
Strategy: Omission 0.80 1.01 0 5.00
Strategy: Falsification 1.03 1.37 0 8.00

Per Comment
Avg. Modifiers 2.85 1.09 0.75 6.00
Avg. Pausality 2.38 1.02 0 4.66
Avg. Verbs 5.68 2.19 2.00 12.33
Avg. Words 23.43 7.33 8.00 41.00
Avg. Word Length 3.82 0.27 3.10 4.79
Avg. Fleish-Kincaid Score 4.91 3.40 -3.62 12.20
Avg. Sentiment 0.17 0.12 -0.17 0.41
Avg. Subjectivity 0.40 0.12 0.17 0.75

Note: Sentiment scores range between -1 (extremely negative) and 1
(extremely positive); Subjectivity scores range between 0 (completely
objective) and 1 (completely subjective).

Table 2: Participant Demographic and Personality Statistics

Variable Mean Stdev. Min. Max.
Age 22.5 4.8 18 45
Openness 25.1 4.8 15 35
Conscientiousness 35.9 6.4 18 49
Extroversion 29.8 6.2 17 42
Agreeableness 28.7 3.0 21 35
Neuroticism 19.6 6.3 8 34

Gender Female 35 Male 40

5.2 Predicting Deception Strategies using
Personality

Hierarchical multiple regression analyses were con-
ducted using each deception strategy as the depen-
dent variable with each FFM personality trait as a
predictor including gender as a blocking variable.
See Table 5 for details on each regression model.
From the analysis, we find that only extroversion
was a predictive trait, found to influence the omis-
sion deception strategy during deceptive communi-
cations, R2 = .139, F (2, 72) = 5.823, p < .01.
The direction of the relationship suggests that less
extroverted individuals are more likely to use an

omission strategy.

5.3 Predicting Personality using
Psycholinguistic Lexical Features

Given that there was a relationship found between
extroversion and the omission strategy, we are then
motivated leverage other work centered on automat-
ically deriving personality from text, in order to de-
rive indicators for extra version. To create our in-
dicators, we utilize previously identified psycholin-
guistic features from (Mairesse et al., 2007) related
to predicting extroversion from text. A hierarchi-
cal multiple regression analysis was conducted us-
ing the omission deception strategy as the dependent
variable with the following LIWC features (found
to be indicative of extroversion by Mairesse (2007):
1st Person Plural, 1st Person Singular, Articles,
Body, Family, Filler, Inclusion, Negations, Positive
Emotion, Social Processes, Tentativeness, Total Pro-
nouns.

The ANOVA omnibus F-test proved insignificant,
F (13, 74) = 0.840, p = 0.617, indicating no statis-
tically significant relationships existed between the
LIWC features for predicting omission; no follow-
up analyses were conducted.

48



Table 3: Examples of Deception Strategies

Identified Deception Strategy Participant’s Statement During Ex-
periment

Participant’s Explanation

Exaggeration “the axe is awesome, and should
be the first priority. In that cold
weather we’ll need definitely need
something to chop firewood with”

“in this statement, I’m playing
along with a positively framed
question. I agree with the other
guy and exaggerate how awesome
the axe is, and how it should defi-
nitely be the first priority”

Misleading “well I think the canvas should be
number 4, it could be used for shel-
ter protect us from the harsh winds,
but should be below the Crisco be-
cause that could be used to store
water and be a signal ”

“This was misleading because the
canvas would probably be more
important than the crisco because
the canvas could hlave been used
as a blanket that produced warmth
and the crisco would not be as
useful because you could just eat
snow”

Falsification “I think the axe, newspaper, crisco,
cavas, wool, and pistol should be
brought in that order. I item-
ized them from most importance to
least. we can disgard lower priority
items if the trip gets rough”

“I was trying to get list across with-
out really taking time to actually
reason through why I am choosing
that order.”

Omission “I agree that the wool should be
ranked 5th, but the crisco is much
more useful than the canvas be-
cause it can be used for fire which
is essential.”

“I didn’t compare the cost/benefit
of the two items, only what I liked
about the crisco”

Table 4: Frequency of Deception Strategies

Exaggeration Misleading Omission Falsification

Count 67 69 41 45

6 Discussion

The goal of the work was to evaluate how personal-
ity (as a means of characterizing individuals) can be
used to predict the types of deception strategies that
those individuals might use in short, social media-
based conversations. The secondary goal was to de-
termine if, given a limited social media corpus, those
personality characteristics found related to strategic
deception could be derived based on linguistic fea-
tures. While we did find a relationship between per-
sonality and deception strategies, where less extro-

verted individuals are more likely to use an omission
strategy, we did not find evidence that the extrover-
sion trait can be accurately derived from this same
text using only the extroversion indicators identified
by Mairesse et al. (2007). The difficulty in inferring
traits from content-derived psycholinguistic features
has been identified in other investigations (Adali and
Golbeck, 2012; Schwartz et al., 2013); however, the
use of additional features, both linguistic, e.g. (Ap-
pling et al., 2013)) and non-, e.g. (Adali and Gol-
beck, 2012), can improve trait prediction.

Additionally, we determined that several linguis-
tic features (e.g., the number of verbs in a statement)
are significantly different across truthful and decep-
tive statements, but only for certain types of decep-
tion strategies. In previous work we evaluated the
use of discriminative models for classifying decep-

49



Table 5: Hierarchical Multiple Regression Predicting Strategy from Gender and Personality

Omission Strategy

Model 1 Model 2

Variable B β B β

Constant 0.820** 1.982**

Gender 0.295* 0.292 0.352* 0.349
Openness 0.160 0.034

Conscientiousness 0.060 0.009
Extroversion -0.039* -0.239

Agreeableness -0.046 -0.137
Neuroticism -0.024 -0.151

R2 0.085 0.139
F 6.807* 5.823*

Exaggeration Strategy

Model 1 Model 2

Variable B β B β

Constant 1.189** -1.926
Gender -0.161 -0.152 -0.167 -0.158

Openness -0.001 -0.004
Conscientiousness -0.002 -0.011

Extroversion -0.004 -0.021
Agreeableness 0.106 0.297

Neuroticism 0.014 0.084

R2 0.010 0.100
F 1.715 1.263

Note: * p < .05, ** p < .001

Misleading Strategy

Model 1 Model 2

B β B β

1.295** -0.642
-0.180 -0.150 -0.232 -0.193

0.006 0.024
0.020 0.108
0.024 0.123
-0.002 -0.005
-0.021 0.108

0.023 0.055
1.680 0.666

Falsification Strategy

Model 1 Model 2

B β B β

1.021** 2.708
-0.079 -0.058 -0.067 -0.049

0.031 0.110
0.032 0.154
-0.010 -0.043
-0.107 -0.236
-0.014 -0.063

0.003 0.066
0.245 0.802

tion strategies (Appling et al., 2015) based solely
on linguistic features. There, we found that some
features were more discriminative for some strate-
gies. For example, the most predictive feature for
discriminating falsification was in the LIWC nega-
tion category (e.g., using ‘no’, ‘not’, ‘never’), which
we found unsurprising given that these statements
are often disagreements in response to the state-
ments made by the other participant. Those features,
however, were not found to be significantly associ-
ated with any particular strategy in the current study.
Also, in that study, the omission strategy was the
most difficult to discriminate, using a train random

forest classifier (Breiman, 2001). This result was not
surprising given that, by definition, the omission of
information does not provide linguistic content from
which to derive features. Interestingly, and perhaps
unfortunately, it is this same strategy that we find
most related to personality dimensions, i.e., extro-
version, in the current study.

An obvious question from this and related stud-
ies focuses on the appropriate amount of text that is
needed to infer individual-level traits, such as these
inferences are as accurate as direct measurement. In
Golbeck et al. (2011) the authors conducted anal-
ysis with 176 participants who averaged 42.6 words

50



from combined social media status updates, whereas
Mairesse et al. (2007) captured on average 766.4
words per subject. Other works, e.g. Nie et al.
(2014), give no indication concerning the amount of
text being used per individual. These questions are
especially relevant given that social media commu-
nications are likely highly variable, exhibiting fluc-
tuations arising from a variety of factors, such as
mood (Bradley and Mogg, 1994).

7 Future Work and Conclusion

Given the likely influence of modern communica-
tion platforms on human communication styles, uti-
lizing linguistic-based indicators for deception is a
difficult and evolving challenge. One, likely fruit-
ful, method of addressing this challenge is to lever-
age work that has been done on characterizing in-
dividuals as a means of using that characterization
for predictive tasks; however, given the brevity and
informality of social media communications, much
more work must be done to create methods for uti-
lizing this kind of text. Related to the use of lexical
psycholinguistic features, we see opportunity with
recent work in vectorized word representations to
develop semantic features based on lexical qualities
that weight observations smoothly. Future work will
continue to evaluate the efficacy of using informal
text-based characterizations as means of predicting
communication strategies.

Acknowledgments

This material is based upon work supported in part
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. W911NF-12-1-0043.

References

Sibel Adali and Jennifer Golbeck. 2012. Predicting
personality with social behavior. In Proceedings of
the 2012 International Conference on Advances in So-
cial Networks Analysis and Mining (ASONAM 2012),
pages 302–309. IEEE Computer Society.

D Eric Anderson, Bella M DePaulo, Matthew E Ans-
field, Jennifer J Tickle, and Emily Green. 1999. Be-
liefs about cues to deception: Mindless stereotypes or
untapped wisdom? Journal of Nonverbal Behavior,
23(1):67–89.

D Scott Appling, Erica J Briscoe, Heather Hayes, and
Rudolph L Mappus. 2013. Towards automated per-
sonality identification using speech acts. In Seventh
International AAAI Conference on Weblogs and Social
Media.

Darren Scott Appling, Erica J Briscoe, and Clayton J
Hutto. 2015. Discriminative models for predicting
deception strategies. In Proceedings of the 24th Inter-
national Conference on World Wide Web Companion,
pages 947–952. International World Wide Web Con-
ferences Steering Committee.

Brendan P Bradley and Karin Mogg. 1994. Mood and
personality in recall of positive and negative informa-
tion. Behaviour research and therapy, 32(1):137–141.

Leo Breiman. 2001. Random forests. Machine learning,
45(1):5–32.

Erica J Briscoe, D Scott Appling, Rudolph L Mappus IV,
and Heather Hayes. 2013. Determining credibil-
ity from social network structure. In Proceedings of
the 2013 IEEE/ACM International Conference on Ad-
vances in Social Networks Analysis and Mining, pages
1418–1424. ACM.

Erica J Briscoe, D Scott Appling, and Heather Hayes.
2014. Cues to deception in social media communica-
tions. In System Sciences (HICSS), 2014 47th Hawaii
International Conference on, pages 1435–1443. IEEE.

David B Buller, Judee K Burgoon, JA Daly, and JM Wie-
mann. 1994. Deception: Strategic and nonstrategic
communication. Strategic interpersonal communica-
tion, pages 191–223.

Judee K Burgoon, David B Buller, Laura K Guerrero,
Walid A Afifi, and Clyde M Feldman. 1996. Inter-
personal deception: Xii. information management di-
mensions underlying deceptive and truthful messages.
Communications Monographs, 63(1):50–69.

Tom De Smedt and Walter Daelemans. 2012. Pattern for
python. The Journal of Machine Learning Research,
13(1):2063–2067.

Bella M DePaulo, James J Lindsay, Brian E Mal-
one, Laura Muhlenbruck, Kelly Charlton, and Harris
Cooper. 2003. Cues to deception. Psychological bul-
letin, 129(1):74.

Nicholas D Duran, Charles Hall, Philip M McCarthy, and
Danielle S McNamara. 2010. The linguistic corre-
lates of conversational deception: Comparing natural
language processing technologies. Applied Psycholin-
guistics, 31(03):439–462.

Patrick M Eady and James Clayton Lafferty. 1969. The
subarctic survival situation. Experimental Learning
Methods.

Paul Ekman and Wallace V Friesen. 1969. Nonverbal
leakage and clues to deception. Technical report.

51



Jennifer Golbeck, Cristina Robles, and Karen Turner.
2011. Predicting personality with social media. In
CHI ’11 Extended Abstracts on Human Factors in
Computing Systems, CHI EA ’11, pages 253–262,
New York, NY, USA. ACM.

Amanda Lenhart, Kristen Purcell, Aaron Smith, and
Kathryn Zickuhr. 2010. Social media & mobile in-
ternet use among teens and young adults. millennials.
Pew Internet & American Life Project.

François Mairesse, Marilyn A Walker, Matthias R Mehl,
and Roger K Moore. 2007. Using linguistic cues
for the automatic recognition of personality in conver-
sation and text. Journal of artificial intelligence re-
search, pages 457–500.

Robert R McCrae and Paul T Costa Jr. 1999. A five-
factor theory of personality. Handbook of personality:
Theory and research, 2:139–153.

Dong Nie, Zengda Guan, Bibo Hao, Shuotian Bai, and
Tingshao Zhu. 2014. Predicting personality on so-
cial media with semi-supervised learning. In Proceed-
ings of the 2014 IEEE/WIC/ACM Conference on Web
Intelligence and Intelligent Agent Technologies, pages
158–165, Washington, DC, USA. IEEE Computer So-
ciety.

Victoria L Rubin. 2010. On deception and deception de-
tection: Content analysis of computer-mediated stated
beliefs. Proceedings of the American Society for In-
formation Science and Technology, 47(1):1–10.

H Andrew Schwartz, Johannes C Eichstaedt, Margaret L
Kern, Lukasz Dziurzynski, Stephanie M Ramones,
Megha Agrawal, Achal Shah, Michal Kosinski, David
Stillwell, Martin EP Seligman, et al. 2013. Per-
sonality, gender, and age in the language of social
media: The open-vocabulary approach. PloS one,
8(9):e73791.

Leslie Scott. 2016. Across new jersey, bands of local is-
lamophobes crusade to drive islam underground, Mar.

Ronny E Turner, Charles Edgley, and Glen Olmstead.
1975. Information control in conversations: Honesty
is not always the best policy. Kansas Journal of Soci-
ology.

Lina Zhou. 2005. An empirical investigation of decep-
tion behavior in instant messaging. Professional Com-
munication, IEEE Transactions on, 48(2):147–160.

52


