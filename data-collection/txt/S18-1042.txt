



















































ISCLAB at SemEval-2018 Task 1: UIR-Miner for Affect in Tweets


Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 286‚Äì290
New Orleans, Louisiana, June 5‚Äì6, 2018. ¬©2018 Association for Computational Linguistics

 
 
 

  1 

ISCLAB at SemEval-2018 Task 1: UIR-Miner for Affect in Tweets 

Meng Li1, 2, Zhenyuan Dong1, Zhihao Fan1, Kongming Meng1, Jinghua Cao1, 

Guanqi Ding1, Yuhan Liu1, Jiawei Shan1, Binyang Li1* 

 
1 School of Information Science and Technology, University of International Relations 

2 University of Pittsburgh  
* Corresponding author 

mel165@pitt.edu; {byli, zydong, zhfan, kmmeng, jhcao, gqding, jwshan}@uir.edu.cn 

 

 

Abstract 

This paper presents a UIR-Miner system for 

emotion and sentiment analysis evaluation in 

Twitter in SemEval 2018. Our system con-

sists of three main modules: preprocessing 

module, stacking module to solve the in-

tensity prediction of emotion and senti-

ment, LSTM network module to solve 

multi-label classification, and the hierar-

chical attention network module for solv-

ing emotion and sentiment classification 

problem. According to the metrics of 

SemEval 2018, our system gets the final 

scores of 0.636, 0.531, 0.731, 0.708, and 

0.408 in terms of Pearson Correlation on 

5 subtasks, respectively. 

1 Introduction 

Recently, social media platforms are becoming 

more and more popular, such as Twitter mi-

croblogging, Facebook, and so on. Through these 

platforms, online users would like to share their 

opinions and emotions. Therefore, the analysis 

about the information on ‚Äúaffect‚Äù in the social me-

dia has attracted much interest from both academia 

and industries.  

However, the short texts are usually consisted of 

informal expressions with much casual forms and 

emoticons, it brings great challenges for such re-

search.  

For this purpose, SemEval organized the evalu-

ation of sentiment analysis on Tweet. This year 

comes the fifth edition that consists of new genres, 

including emotion intensity regression task, emo-

tion intensity ordinal classification task, sentiment 

intensity regression task, sentiment degree ordinal 

classification task, and emotion classification task 

(Mohammad et al., 2018).  

 

 

 
Figure 1: System architecture. 

 

We participated in SemEval 2018 task 1 for Eng-

lish, i.e. Affect in Tweet. Our system considers EI-

reg and V-reg (subtask A and C) as regression prob-

lems to get the emotion intensity and sentiment in-

tensity by using regression models, while regards 

EI-oc and V-oc (subtask B and D) as categorization 

problems to classify each tweet into its correspond-

ing emotion category and sentiment category by 

implementing hierarchical attention networks. 

Moreover, subtask E, i.e., E-c, is considered as a 

multi-label classification task. 

This paper is organized as follows. Section 2 

overviews the framework of our system. Section 3 

describes the methods for subtask A and C. Section 

4 describes the hierarchical attention networks for 

subtask B and D. Subtask E will be introduced in 

Section 5. Section 6 presents the evaluation results. 

Section 7 will conclude this paper.  

2 System Overview 

The architecture of UIR-Miner is shown in Figure 

1. UIR-Miner system is comprised of 4 modules:  

(1) Preprocessing module: involves data cleaning, 

topic classification, and tweets embedding. 

286



 
 
 

  2 

(2) Regressor module: creates an ensemble regres-
sor model by using different basic models sim-

ultaneously to calculate the emotion intensity 

and sentiment intensity, i.e. subtask A and sub-

task C;  

(3) Classification module: constructs an LSTM 
network with multi-layer attention mechanism 

for emotion and sentiment categorization, i.e. 

subtask B and subtask D;  

(4) Multi-label Classification module: builds a 
LSTM network for subtask E. 

2.1 Preprocessing 

Our system will firstly preprocess the Tweets data, 

and the main steps are as follows.  

ÔÇ∑ Delete the unrelated texts, including the id, 
some mentions, stop words, and some mean-

ingless punctuation combinations. 

ÔÇ∑ Normalize synonymous words, like replacing 
‚Äúcant‚Äù and ‚Äúcan‚Äôt‚Äù with ‚Äúcannot‚Äù. 

ÔÇ∑ Extract emoticons from tweets through regular 
expressions, and then maintain the emotional 

ones.  

2.2 Word embedding 

In the preprocessing, we used the pre-trained word 

embedding by Glove (Penningto et. al, 2014), in 

which each word ùëíùëñùë° will be represented by a 200-
dimensional vector ùë§ùëñùë° , ùëñ ‚àà  [1, ùêø] , ùë° ‚àà  [1, ùëá] . 
Here, ùëñ denotes the location of the sentence in the 
tweet and ùêø is the maximum number of sentences 
for each tweet, ùë° denotes the location of the word 
in the sentence and ùëá is the maximum number of 
words for each sentence. Set ùëá = 140 and ùêø = 5. 

3 Subtask A and C 

This section will describe the methods for subtask 

A and C. Given a tweet and an emotion E (or a 

sentiment V), determine the intensity of E (or V) 

that best represents the mental state of the 

tweeter‚Äîa real-valued score between 0 and 1. We 

consider both of subtask A and C as a regression 

problem.  

On the whole, we use a stacking framework to 

enhance the accuracy of final prediction. The orig-

inal features are selected as input into the stacking 

model, including hashtags, emoticons, and n-

gram features. Then, the stacking model is di-

vided into two layer, the base layer and the stack-

ing layer. In the base layer, we choose four basic 

regressors due to their excellent performance. In 

the stacking layer, we still use SVM model, espe-

cially, the NuSVR model, which can control its 

error rate. Finally, we get the final result of inten-

sity value. 

3.1 Feature Selection 

Since there are many irregular expressions in 

tweet, we combine the features, including emoti-

con, hashtag, and special punctuations. In our sys-

tem, we mainly select the following features:  

‚Ä¢ Hashtags: the number of hashtags in one tweet; 

‚Ä¢ Ill format: the presence of ill format with some 

characters replacing by *; 

‚Ä¢ Punctuation: the number of contiguous se-

quences of exclamation marks, question marks, 

and both exclamation and question marks; 

whether the last token contains an exclamation 

or question mark; 

‚Ä¢ Emoticons: the presence of positive and negative 

emoticons at any position in the tweet; whether 

the last token is an emoticon; 

‚Ä¢ OOV: the ratio of words out of vocabulary; 

‚Ä¢ Elongated words: the presence of sentiment 

words with one character repeated more than 

two times, for example, ‚Äòcooool‚Äô; 

‚Ä¢ URL: whether the tweet contains a URL. 

‚Ä¢ Reply or Retweet: is the current tweet a reply/ 

retweet tweet. 

3.2 Stacking Model 

To avoid overfitting, we test 6 basic models to 

construct our stacking model. 

ÔÇ∑ B: Bayesian Ridge (Hsiang, T.C 1975) 

ÔÇ∑ G: Gradient Boosting Regressor (Jerome H. 

Friedman, 2001) 

ÔÇ∑ K: Kernel Ridge (Zhang Y et. al, 2013) 

ÔÇ∑ L: Lasso Regressor (Tibshirani et al., 1996) 

ÔÇ∑ M: MLP Regression (Pal and Mitra, 1992) 

ÔÇ∑ R: Random Forest Regressor (Ho, 1995) 

ÔÇ∑ S: SVR (Vapnik 1995) 

To achieve the best performance, we also com-

pare different combinations of our basic models 

with the metrics of Mean Squired Error (MSE) in 

the stacking method, and the experimental result 

is shown in Table 1. 

ÔÇ∑ Baseline: we use SVR as the Baseline; 

ÔÇ∑ Stacking1: B+K+S; 

ÔÇ∑ Stacking2: M+K+R; 

ÔÇ∑ Stacking3: B+K+R+S; 

ÔÇ∑ Stacking4: B+G+K+M; 

ÔÇ∑ Stacking5: G+K+L+ S; 

ÔÇ∑ Stacking6: B+G+K+S. 
Since Stacking 6 achieves the best performance, 

we use the same setting in our system. 

287



 
 
 

  3 

 
Figure 2: BiLSTM network with multi-layer at-

tention mechanism. 

Table 1: Evaluation on different combinations in 

stacking method. 

Method 
Metrics 

Ang Fear Joy Sad Ave 

Baseline 9.774 8.390 9.055 9.086 9.076 

Stacking1 9.404 7.926 8.352 8.629 8.578 

Stacking2 9.596 7.849 8.192 8.520 8.539 

Stacking3 9.351 7.900 8.206 8.536 8.500 

Stacking4 9.557 7.715 8.045 8.454 8.443 

Stacking5 9.381 7.790 8.170 8.387 8.432 

Stacking6 9.300 7.766 7.794 8.334 8.298 

 

4 Hierarchical Attention Networks for 
Subtask B and D 

This section will introduce our hierarchical atten-

tion model for subtask B and D. Given a tweet and 

an emotion category E (or a sentiment category V), 

classify the tweet into one of the ordinal classes of 

intensity of E (or V) that best represents the mental 

state of the tweeter. Note that, the number of cate-

gory of E is 4, while that of V is 7. In our system, 

we consider both of subtask B and D as a classifi-

cation problem. 

Each tweet contains several sentences that are 

comprised by several words. In order to better rep-

resent the semantics of emotion or sentiment, we 

utilize the hierarchical structure of a tweet to cap-

ture the contextual information of both intra and in-

ter-tweet. The architecture is shown as Figure 2.  

We build a hierarchical model which contains 

two layers, word layer and sentence layer. Since 

words and sentences are highly sensitive to the con-

texts, recurrent neural networks based on bidirec-

tional long short-term memory (BiLSTM) 

(Hochreiter and Schmidhuber, 1997) are imple-

mented on both layers to get tweets‚Äô representa-

tions. Furthermore, since the words in one sentence 

or different sentences in a given tweet can indicate 

different emotion intensity or sentiment intensity. 

To better represent the semantics, attention mecha-

nisms are added to both layers respectively (Xu et. 

al., 2015). We then use softmax as the activation 

4.1 BiLSTM-based Word Encoder  

A word level BiLSTM (Hochreiter and Schmidhu-

ber, 1997) is used to represent each word. The 

BiLSTM consists of the forward LSTM and the 

backward LSTM. Forward LSTM reads the sen-

tence ùë†ùëñ from ùëíùëñ1 to ùëíùëñùëá and represents the word ùëíùëñùë° 

as ùêøùëÜùëáùëÄ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó  ‚Éó(ùë§ùëñùë°), ùë° ‚àà  [1, ùëá]. Backward LSTM reads 
the sentence ùë†ùëñ  from ùëíùëñùëá  to ùëíùëñ1  and represents the 

word ùëíùëñùë° as ùêøùëÜùëáùëÄ‚Éñ‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó (ùë§ùëñùë°), ùë° ‚àà  [ùëá, 1]. Then word ùëíùëñùë° 
can be annotated by combining both forward infor-

mation and backward information, ‚Ñéùëñùë°  =

 [ ùêøùëÜùëáùëÄ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó  ‚Éó(ùë§ùëñùë°), ùêøùëÜùëáùëÄ‚Éñ‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó (ùë§ùëñùë°) ] . The equations are 
listed as follows: 

ùëñùë° = ùúé(ùëäùëñùë§ùëñùë° + ùëàùëñ‚Ñéùë°‚àí1 + ùëèùëñ)         (1) 

ùëìùë° = ùúé(ùëäùëìùë§ùëìùë° + ùëàùëì‚Ñéùë°‚àí1 + ùëèùëì)        (2) 

ùëúùë° = ùúé(ùëäùëúùë§ùëúùë° + ùëàùëú‚Ñéùë°‚àí1 + ùëèùëú)       (3) 

ùë¢ùë° = tanh(ùëäùë¢ùë§ùë¢ùë° + ùëàùë¢‚Ñéùë°‚àí1 + ùëèùë¢)    (4) 

ùëêùë° = ùëñùë°‚®Äùë¢ùë° + ùëìùë°‚®Äùëêùë°‚àí1             (5) 

‚Ñéùë° = ùëúùë°‚®Ätanh(ùëêùë°)               (6) 

where ùëñùë° , ùëìùë°  and ùëúùë°  are the input gate, forget gate 
and output gate, ùúé is the logistic sigmoid function, 
‚®Ä denotes elementwise multiplication, ùë°ùëéùëõ‚Ñé is the 
network output activation function, and softmax is 

used for categorization. To better support Twitter, 

we input the word embedding with 200 dimensions, 

and the max number of words in a sentence as 140.  

4.2 Word Layer Attention 

Different weights ùõºùëñùë° are given to different words. 
Attention mechanism (Xu et. al., 2015) is added to 

the word layer and the sentence ùë†ùëñ  can be repre-
sented as ùë†_ùëéùë°ùë°ùëñ. 

ùë¢ùëñùë° = tanh(ùëäùë§‚Ñéùëñùë° + ùëèùë§)              (7) 
 

ùõºùëñùë° =
exp(ùë¢ùëñùë°

ùëá ùë¢ùë§)

‚àë ùëíùë•ùëùùë° (ùë¢ùëñùë°
ùëá ùë¢ùë§)

                 (8) 

 

288



 
 
 

  4 

ùë†_ùëéùë°ùë°ùëñ = ‚àëùõºùëñùë°‚Ñéùëñùë°
ùë°

                  (9) 

More specifically, after putting ‚Ñéùëñùë° into a fully-
connected layer, we get ùë¢ùëñùë°. Then calculate weight 
ùõºùëñùë° with a word level context ùë¢ùë§. Finally, we can 
get the sentence vector through an attention layer 

by calculating the sum of ùõºùëñùë°‚Ñéùëñùë°. 

4.3 Sentence Layer Attention 

Similarly, a sentence level BiLSTM (Hochreiter 

and Schmidhuber, 1997) can be used to represent 

sentence ùë†ùëñ by adding sentence level context infor-
mation,  

‚Ñéùëñ = [ ùêøùëÜùëáùëÄ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó  ‚Éó(ùë†ùëéùë°ùë°ùëñùë°), ùêøùëÜùëáùëÄ
‚Éñ‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó ‚Éó‚Éó (ùë†ùëéùë°ùë°ùëñùë°) ]. 

We then add weights to different sentence. Take 

ùë•ùëñ  as input and get ùë°ùë§ùëíùëíùë°_ùëéùë°ùë°  to represent each 
tweet through an attention layer. 

ùë¢ùëñ = tanh(ùëäùë†‚Ñéùëñ + ùëèùë†) 

ùõºùëñ =
exp (ùë¢ùëñ

ùëáùë¢ùë†)

‚àë ùëíùë•ùëùùëñ (ùë¢ùëñ
ùëáùë¢ùë†)

 

ùë°ùë§ùëíùëíùë°_ùëéùë°ùë° =  ‚àëùõºùëñ‚Ñéùëñ
ùëñ

 

More specifically, after putting ‚Ñéùëñ  into a fully-
connected layer, we get ùë¢ùëñ. Then calculate weight 
ùõºùëñ with a sentence level context ùë¢ùë†. Finally, we can 
get the tweet vector through an attention layer by 

calculating the sum of ùõºùëñ‚Ñéùëñ. 

5 Subtask E 

This section will introduce neural network model 

for subtask E. Given a tweet, classify the tweet as 

‚Äúneutral or no emotion‚Äù or as one, or more, of 

eleven given emotions that best represent the men-

tal state of the tweeter. 

Each tweet will be classified with different num-

bers of labels. Since there exists eleven labels each 

of which may be suitable, considering one of these 

labels every time is reasonable. Our system will 

calculate a score for each of the eleven labels for 

each tweet, and select the top-3 as the final results.  

We also used a LSTM network for this task, and 

get the classification result by using softmax. The 

other settings of this model is quite similar to that 

in Section 4 except for multi-label classification.  

6 Experiment 

In this section, we will report our evaluation re-

sults in SemEval 2018 based on the given dataset 

as well as the metrics. The statistics of the dataset 

is shown in Table 2. 

Note that any other extra external resources, 

such as sentiment lexicon, emoticons, and anno-

tated corpus, are not used in the evaluation except 

for the training dataset provided by the organiza-

tion.  

 

Table 2: Statistics of the dataset. 
 Training set Dev set Test set 

EI-reg anger: 1701 

fear: 2252 

joy: 1616 

sadness: 1533 

anger: 388 

fear: 389 

joy: 290 

sadness: 397 

anger: 17939 

fear: 17923 

joy: 18042 

sadness: 17912 

EI-oc anger: 1701 

fear: 2252 

joy: 1616 

sadness: 1533 

anger: 388 

fear: 389 

joy: 290 

sadness: 397 

anger: 1002 

fear: 986 

joy: 1105 

sadness: 975 

V-reg 1181 449 17874 

V-oc 1181 449 937 

E-c 6838 886 3259 

 

Table 3 shows the results of our UIR-Miner for 

all the subtasks on both Dev set and Test set, and 

the final ranking.  

 

Table 3: The results on different datasets. 

 Score in Dev Score in Test Ranking 

EI-reg 0.576 0.636 28/48 

EI-oc 0.495 0.531 15/39 

V-reg 0.729 0.781 21/38 

V-oc 0.694 0.708 16/37 

E-c 0.421 0.407 23/35 

7 Conclusion 

In this paper, we present a framework for SemEval 

2018 Affect in Tweet task. After the preprocessing, 

we firstly propose an ensembling method to calcu-

late the intensity score of emotion and sentiment. 

Then a LSTM network model with multi-layer at-

tention mechanism is constructed for emotion and 

sentiment classification. According to SemEval 

2018‚Äôs metrics, our model runs got final scores of 

0.636, 0.531, 0.731, 0.708, and 0.408 in terms of 

Pearson Correlation on 5 subtasks, respectively. 

Acknowledgements 

This paper is funded by the National Natural 

Foundation of China 61502115, 61602326, 

U1636103, U1536207, 61572043, 61672361, 

61632011, the Hong Kong Applied Science and 

Technology Research Institute Project 7050854, 

and the Fundamental Research Fund for the 

Central Universityies 3262015T70, 3262017T12.  

 

289



 
 
 

  5 

References  

Alex J. Smola, Bernhard Sch√∂lkopf. 2004. A tutorial 

on support vector regression. In 2004 Kluwer Aca-

demic Publishers, pages 199-222 

Hsiang, T.C. 1975.A Bayesian View on Ridge Regres-

sion. In Journal of the Royal Statistical Society, 

page 267-268. 

Zhang Y, Duchi J, Wainwright M. 2013. Divide and 

conquer kernel ridge regression. In Conference on 

Learning Theory, pages 592-617. 

Jerome H. Friedman. 2001. Greedy function approxi-

mation: a gradient boosting machine. In Annals of 

Statistics, pages 1189‚Äì1232 

Sepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long 

short-term memory. In Neural computation, 9(8): 

1735-1780. 

Saif M. Mohammad, Felipe Bravo-Marquez, Moham-

mad Salameh, and Svetlana Kiritchenko. 2018. 

SemEval-2018 Task1: Affect in tweets. In Proceed-

ings of International Workshop on Semantic Evalu-

ation (SemEval-2018). 

Saif M. Mohammad and Svetlana Kiritchenko. 2018. 

Understanding emotions: A dataset of tweets to 

study interactions between affect categories. In Pro-

ceedings of the 11th Edition of the Language Re-

sources and Evaluation Conference. 

Jeffrey Pennington, Richard Socher, and Christopher 

D. Manning. 2014. Glove: Global vectors for word 

representation. In Empirical Methods in Natural 

Language Processing, pages 1532-1543. 

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, 

Aaron Courville, Ruslan Salakhudinov, Richard Ze-

mel, and Yoshua Bengio. 2015. Show, attend and 

tell: Neural image caption generation with visual at-

tention. In International Conference on Machine 

Learning, pages 2048-2057. 

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, 

Alex Smola, and Eduard Hovy. 2016. Hierarchical 

attention networks for document classification. In 

Proceedings of the 2016 Conference of the North 

American Chapter of the Association for Computa-

tional Linguistics: Human Language Technologies, 

pages 1480-1489. 

Tibshirani R, Bickel P, Ritov Y, et al. Least absolute 

shrinkage and selection operator[J]. 1996. 

Ho T K. Random decision forests[C]//Document anal-

ysis and recognition, 1995, proceedings of the third 

international conference on. IEEE, 1995, 1: 278-

282. 

Pal S K, Mitra S. Multilayer perceptron, fuzzy sets, and 

classification[J]. IEEE Transactions on neural net-

works, 1992, 3(5): 683-697. 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

290


