



















































An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 504–515
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

504

An Interactive Multi-Task Learning Network for End-to-End
Aspect-Based Sentiment Analysis

Ruidan He†‡, Wee Sun Lee†, Hwee Tou Ng†, and Daniel Dahlmeier‡
†Department of Computer Science, National University of Singapore

‡SAP Innovation Center Singapore
†{ruidanhe,leews,nght}@comp.nus.edu.sg

‡d.dahlmeier@sap.com

Abstract

Aspect-based sentiment analysis produces a
list of aspect terms and their corresponding
sentiments for a natural language sentence.
This task is usually done in a pipeline man-
ner, with aspect term extraction performed
first, followed by sentiment predictions to-
ward the extracted aspect terms. While eas-
ier to develop, such an approach does not fully
exploit joint information from the two sub-
tasks and does not use all available sources
of training information that might be helpful,
such as document-level labeled sentiment cor-
pus. In this paper, we propose an interactive
multi-task learning network (IMN) which is
able to jointly learn multiple related tasks si-
multaneously at both the token level as well
as the document level. Unlike conventional
multi-task learning methods that rely on learn-
ing common features for the different tasks,
IMN introduces a message passing architec-
ture where information is iteratively passed to
different tasks through a shared set of latent
variables. Experimental results demonstrate
superior performance of the proposed method
against multiple baselines on three benchmark
datasets.

1 Introduction

Aspect-based sentiment analysis (ABSA) aims to
determine people’s attitude towards specific as-
pects in a review. This is done by extracting ex-
plicit aspect mentions, referred to as aspect term
extraction (AE), and detecting the sentiment ori-
entation towards each extracted aspect term, re-
ferred to as aspect-level sentiment classification
(AS). For example, in the sentence “Great food
but the service is dreadful”, the aspect terms are
“food” and “service”, and the sentiment orienta-
tions towards them are positive and negative re-
spectively.

In previous works, AE and AS are typically

treated separately and the overall task is performed
in a pipeline manner, which may not fully ex-
ploit the joint information between the two tasks.
Recently, two studies (Wang et al., 2018; Li
et al., 2019) have shown that integrated models
can achieve comparable results to pipeline meth-
ods. Both works formulate the problem as a sin-
gle sequence labeling task with a unified tagging
scheme1. However, in their methods, the two tasks
are only linked through unified tags, while the
correlation between them is not explicitly mod-
eled. Furthermore, the methods only learn from
aspect-level instances, the size of which is usu-
ally small, and do not exploit available informa-
tion from other sources such as related document-
level labeled sentiment corpora, which contain
useful sentiment-related linguistic knowledge and
are much easier to obtain in practice.

In this work, we propose an interactive multi-
task learning network (IMN), which solves both
tasks simultaneously, enabling the interactions be-
tween both tasks to be better exploited. Further-
more, IMN allows AE and AS to be trained to-
gether with related document-level tasks, exploit-
ing the knowledge from larger document-level
corpora. IMN introduces a novel message passing
mechanism that allows informative interactions
between tasks. Specifically, it sends useful infor-
mation from different tasks back to a shared la-
tent representation. The information is then com-
bined with the shared latent representation and
made available to all tasks for further process-
ing. This operation is performed iteratively, allow-
ing the information to be modified and propagated
across multiple links as the number of iterations
increases. In contrast to most multi-task learning
schemes which share information through learning

1{B, I}-{POS, NEG, NEU} denotes the beginning and
inside of an aspect-term with positive, negative, or neutral
sentiment, respectively, and O denotes background words.



505

a common feature representation, IMN not only
allows shared features, but also explicitly models
the interactions between tasks through the mes-
sage passing mechanism, allowing different tasks
to better influence each other.

In addition, IMN allows fined-grained token-
level classification tasks to be trained together
with document-level classification tasks. We in-
corporated two document-level classification tasks
– sentiment classification (DS) and domain classi-
fication (DD) – to be jointly trained with AE and
AS, allowing the aspect-level tasks to benefit from
document-level information. In our experiments,
we show that the proposed method is able to out-
perform multiple pipeline and integrated baselines
on three benchmark datasets2.

2 Related Work

Aspect-Based Sentiment Analysis. Existing ap-
proaches typically decompose ABSA into two
subtasks, and solve them in a pipeline setting.
Both AE (Qiu et al., 2011; Yin et al., 2016; Wang
et al., 2016a, 2017; Li and Lam, 2017; He et al.,
2017; Li et al., 2018b; Angelidis and Lapata,
2018) and AS (Dong et al., 2014; Nguyen and Shi-
rai, 2015; Vo and Zhang, 2015; Tang et al., 2016a;
Wang et al., 2016b; Zhang et al., 2016; Liu and
Zhang, 2017; Chen et al., 2017; Cheng et al., 2017;
Tay et al., 2018; Ma et al., 2018; He et al., 2018a,b;
Li et al., 2018a) have been extensively studied in
the literature. However, treating each task inde-
pendently has several disadvantages. In a pipeline
setting, errors from the first step tend to be propa-
gated to the second step, leading to a poorer over-
all performance. In addition, this approach is un-
able to exploit the commonalities and associations
between tasks, which may help reduce the amount
of training data required to train both tasks.

Some previous works have attempted to de-
velop integrated solutions. Zhang et al. (2015)
proposed to model the problem as a sequence la-
beling task with a unified tagging scheme. How-
ever, their results were discouraging. Recently,
two works (Wang et al., 2018; Li et al., 2019) have
shown some promising results in this direction
with more sophisticated network structures. How-
ever, in their models, the two subtasks are still only
linked through a unified tagging scheme, while the
interactions between them are not explicitly mod-

2Our source code can be obtained from https://
github.com/ruidan/IMN-E2E-ABSA

eled. To address this issue, a better network struc-
ture allowing further task interactions is needed.

Multi-Task Learning. One straightforward ap-
proach to perform AE and AS simultaneously
is multi-task learning, where one conventional
framework is to employ a shared network and two
task-specific network to derive a shared feature
space and two task-specific feature spaces. Multi-
task learning frameworks have been employed
successfully in various natural language process-
ing (NLP) tasks (Collobert and Weston, 2008; Lu-
ong et al., 2015a; Liu et al., 2016). By learning se-
mantically related tasks in parallel using a shared
representation, multi-task learning could capture
the correlations between tasks and improve the
model generalization ability in certain cases. For
ABSA, He et al. (2018b) have shown that aspect-
level sentiment classification can be significantly
improved through joint training with document-
level sentiment classification. However, conven-
tional multi-task learning still does not explicitly
model the interactions between tasks – the two
tasks only interact with each other through error
back-propoagation to contribute to the learned fea-
tures and such implicit interactions are not con-
trollable. Unlike existing methods, our proposed
IMN not only allows the representations to be
shared, but also explicitly models the interactions
between tasks, by using an iterative message pass-
ing scheme. The propagated information con-
tributes to both learning and inference to boost the
overall performance of ABSA.

Message Passing Architectures. Networked rep-
resentations for message passing graphical model
inference algorithms have been studied in com-
puter vision (Arnab et al., 2018) and NLP (Gorm-
ley et al., 2015). Modeling the execution of these
message passing algorithms as a network results in
recurrent neural network architectures. We simi-
larly propagate information in a network and learn
the update operators, but the architecture is de-
signed for solving multi-task learning problems.
Our algorithm can similarly be viewed as a recur-
rent neural network since each iteration uses the
same network to update the shared latent variables.

3 Proposed Method

The IMN architecture is shown in Figure 1. It
accepts a sequence of tokens {x1, . . . , xn} as in-
put into a feature extraction component fθs that is

https://github.com/ruidan/IMN-E2E-ABSA
https://github.com/ruidan/IMN-E2E-ABSA


506

Embedding Layer

message-passing
mechanism

opinion
transmission

AE AS DS DD

aspect term and opinion term
co-extraction

aspect-level sentiment
classification

document-level sentiment
classification

document-level domain
classification

AE:

AS:

DS:

DD:

Figure 1: The overall architecture of IMN.

shared among all tasks. This component consists
of a word embedding layer followed by a few fea-
ture extraction layers. Specifically, we employ ms

layers of CNNs after the word embedding layer in
fθs .

The output of fθs is a sequence of latent vec-
tors {hs1,hs2, ...,hsn} shared among all the tasks.
After initialization by fθs , this sequence of la-
tent vectors is later updated by combining infor-
mation propagated from different task components
through message passing. We denote hs(t)i as the
value of the shared latent vector corresponding to
xi after t rounds of message passing, with h

s(0)
i

denoting the value after initialization.
The sequence of shared latent vectors3

{hs1,hs2, ...,hsn} is used as input to the different
task-specific components. Each task-specific
component has its own sets of latent and output
variables. The output variables correspond to a
label sequence in a sequence tagging task; in AE,
we assign to each token a label indicating whether
it belongs to any aspect or opinion4 term, while
in AS, we label each word with its sentiment. In
a classification task, the output corresponds to
the label of the input instance: the sentiment of
the document for the sentiment classification task
(DS), and the domain of the document for the
domain classification task (DD). At each iteration,
appropriate information is passed back to the
shared latent vectors to be combined; this could
be the values of the output variables or the latent
variables, depending on the task. In addition, we
also allow messages to be passed between the

3We omit the iteration superscript t in the description for
simplicity.

4e.g. “great” and “dreadful” in “Great food but the service
is dreadful” are the opinion terms.

components in each iteration. Specifically for
this problem, we send information from the AE
task to the AS task as shown in Figure 1. After
T iterations of message passing, which allows
information to be propagated through multiple
hops, we use the values of the output variables
as predictions. For this problem, we only use
the outputs for AE and AS during inference as
these are the end-tasks, while the other tasks are
only used for training. We now describe each
component and how it is used in learning and
inference.

3.1 Aspect-Level Tasks

AE aims to extract all the aspect and opinion
terms5 appearing in a sentence, which is formu-
lated as a sequence tagging problem with the BIO
tagging scheme. Specifically, we use five class
labels: Y ae = {BA, IA,BP, IP,O}, indicating
the beginning of and inside of an aspect term,
the beginning of and inside of an opinion term,
and other words, respectively. We also formu-
late AS as a sequence tagging problem with la-
bels Y as = {pos, neg, neu}, indicating the token-
level positive, negative, and neutral sentiment ori-
entations. Table 1 shows an example of aspect-
level training instance with gold AE and AS labels.
In aspect-level datasets, only aspect terms get sen-
timent annotated. Thus, when modeling AS as a
sequence tagging problem, we label each token
that is part of an aspect term with the sentiment
label of the corresponding aspect term. For exam-

5Note that we are actually performing aspect and opin-
ion term co-extraction. We still denote this task as AE for
simplicity. We believe ABSA is more complete with opin-
ion terms also extracted. Also, the information learned from
opinion term extraction could be useful for the other tasks.



507

Input The fish is fresh but the variety of fish is nothing out of ordinary .
AE O BA O BP O O BA IA IA O O O O BP O
AS - pos - - - - neg neg neg - - - - - -

Table 1: An aspect-level training instance with gold AE and AS labels.

ple, as shown in Table 1, we label “fish” as pos,
and label “variety”, “of ”, “fish” as neg, based on
the gold sentiment labels of the two aspect terms
“fish” and “varity of fish” respectively. Since other
tokens do not have AS gold labels, we ignore the
predictions on them when computing the training
loss for AS.

The AE component fθae is parameterized by
θae and outputs {ŷae1 , ..., ŷaen }. The AS com-
ponent fθas is parameterized by θas and outputs
{ŷas1 , ..., ŷasn }. The AE and AS encoders con-
sist of mae and mas layers of CNNs respec-
tively, and they map the shared representations to
{hae1 ,hae2 , ...,haen } and {has1 ,has2 , ...,hasn } respec-
tively. For the AS encoder, we employ an ad-
ditional self-attention layer on top of the stacked
CNNs. As shown in Figure 1, we make ŷaei ,
the outputs from AE available to AS in the self-
attention layer, as the sentiment task could bene-
fit from knowing the predictions of opinion terms.
Specifically, the self-attention matrix A ∈ Rn×n
is computed as follows:

score(i6=j)ij = (h
as
i W

as(hasj )
T ) · 1|i− j| · P

op
j (1)

A
(i6=j)
ij =

exp(scoreij)∑n
k=1 exp(scoreik)

(2)

where the first term in Eq.(1) indicates the seman-
tic relevance between hasi and h

as
j with param-

eter matrix Was, the second term is a distance-
relevant factor, which decreases with increasing
distance between the ith token and the jth token,
and the third term P opj denotes the predicted prob-
ability that the jth token is part of any opinion
term. The probability P opj can be computed by
summing the predicted probabilities on opinion-
related labels BP and IP in ŷaej . In this way, AS is
directly influenced by the predictions of AE. We
set the diagonal elements in A to zeros, as we only
consider context words for inferring the sentiment
of the target token. The self-attention layer out-
puts h′i

as =
∑n

j=1Aijh
as
j . In AE, we concate-

nate the word embedding, the initial shared rep-
resentation hs(0)i , and the task-specific representa-
tion haei as the final representation of the ith token.
In AS, we concatenate hs(0)i and h

′
i
as as the final

representation. For each task, we employ a fully-
connected layer with softmax activation as the de-
coder, which maps the final token representation
to probability distribution ŷaei (ŷ

as
i ).

3.2 Document-Level Tasks

To address the issue of insufficient aspect-level
training data, IMN is able to exploit knowledge
from document-level labeled sentiment corpora,
which are more readily available. We intro-
duce two document-level classification tasks to be
jointly trained with AE and AS. One is document-
level sentiment classification (DS), which pre-
dicts the sentiment towards an input document.
The other is document-level domain classification
(DD), which predicts the domain label of an input
document.

As shown in Figure 1, the task-specific op-
eration fθo consists of m

o layers of CNNs that
map the shared representations {hs1, ...,hsn} to
{ho1, ...,hon}, an attention layer atto, and a decod-
ing layer deco, where o ∈ {ds, dd} is the task sym-
bol. The attention weight is computed as:

aoi =
exp(hoiW

o)∑n
k=1 exp(h

o
kW

o)
(3)

where W o is a parameter vector. The final
document representation is computed as ho =∑n

i=1 a
o
ih

o
i . We employ a fully-connected layer

with softmax activation as the decoding layer,
which maps ho to ŷo.

3.3 Message Passing Mechanism

To exploit interactions between different tasks, the
message passing mechanism aggregates predic-
tions of different tasks from the previous iteration,
and uses this knowledge to update the shared la-
tent vectors {hs1, ...,hsn} at the current iteration.
Specifically, the message passing mechanism inte-
grates knowledge from ŷaei , ŷ

as
i , ŷ

ds, adsi , and a
dd
i

computed on an input {x1, ..., xn}, and the shared
hidden vector hsi is updated as follows:

h
s(t)
i =fθre(h

s(t−1)
i : ŷ

ae(t−1)
i : ŷ

as(t−1)
i :

ŷds(t−1) : a
ds(t−1)
i : a

dd(t−1)
i )

(4)



508

where t > 0 and [:] denotes the concatenation op-
eration. We employ a fully-connected layer with
ReLu activation as the re-encoding function fθre .
To update the shared representations, we incorpo-
rate ŷae(t−1)i and ŷ

as(t−1)
i , the outputs of AE and

AS from the previous iteration, such that these in-
formation are available for both tasks in current
round of computation. We also incorporate infor-
mation from DS and DD. ŷds indicates the overall
sentiment of the input sequence, which could be
helpful for AS. The attention weights adsi and a

dd
i

generated by DS and DD respectively reflect how
sentiment-relevant and domain-relevant the ith to-
ken is. A token that is more sentiment-relevant
or domain-relevant is more likely to be an opinion
word or aspect word. This information is useful
for the aspect-level tasks.

3.4 Learning

Instances for aspect-level problems only have
aspect-level labels while instances for document-
level problems only have document labels. IMN
is trained on aspect-level and document-level in-
stances alternately.

When trained on aspect-level instances, the loss
function is as follows:

La(θs,θae, θas, θds, θdd, θre) =
1

Na

Na∑
i=1

1

ni

ni∑
j=1

(

l(yaei,j , ŷ
ae(T )
i,j ) + l(y

as
i,j , ŷ

as(T )
i,j ))

(5)

where T denotes the maximum number of itera-
tions in the message passing mechanism, Na de-
notes the total number of aspect-level training in-
stances, ni denotes the number of tokens con-
tained in the ith training instance, and yaei,j (y

as
i,j)

denotes the one-hot encoding of the gold label for
AE (AS). l is the cross-entropy loss applied to each
token. In aspect-level datasets, only aspect terms
have sentiment annotations. We label each token
that is part of any aspect term with the sentiment
of the corresponding aspect term. During model
training, we only consider AS predictions on these
aspect term-related tokens for computing the AS
loss and ignore the sentiments predicted on other
tokens6.

When trained on document-level instances, we

6Let l(yasi,j , ŷ
as(T )
i,j ) = 0 in Eq.(5) if y

ae
i,j is not BA or IA

Algorithm 1 Pseudocode for training IMN

Require: Da = {(xai , yaei , yasi )
Na
i=1}, Dds =

{(xdsi , ydsi )
Nds
i=1} and Ddd = {(xddi , yddi )

Ndd
i=1 }

Require: Integer r > 0

for e ∈ [1,max-pretrain-epochs] do
for minibatch Bds, Bdd in Dds, Ddd do

compute Ld based on Bds and Bdd

update θs, θds, θdd
end for

end for

for e ∈ [1,max-epochs] do
for b ∈ [1, batches-per-epoch] do

sample Ba from Da

compute La based on Ba

update θs, θae, θas, θre
if b is divisible by r then

sample Bds, Bdd from Dds, Ddd

compute Ld based on Bds and Bdd

update θs, θds, θdd
end if

end for
end for

minimize the following loss:

Ld(θs, θds, θdd) =
1

Nds

Nds∑
i=1

l(ydsi , ŷ
ds
i )

+
1

Ndd

Ndd∑
i=1

l(yddi , ŷ
dd
i )

(6)

where Nds and Ndd denote the number of training
instances for DS and DD respectively, and ydsi and
yddi denote the one-hot encoding of the gold la-
bel. Message passing iterations are not used when
training document-level instances.

For learning, we first pretrain the network on the
document-level instances (minimize Ld) for a few
epochs, such that DS and DD can make reasonable
predictions. Then the network is trained on aspect-
level instances and document-level instances alter-
nately with ratio r, to minimize La and Ld. The
overall training process is given in Algorithm 1.
Da, Dds, and Ddd denote the aspect-level train-
ing set and the training sets for DS, DD respec-
tively. Dds andDa are from similar domains. Ddd

contains review documents from at least two do-
mains with ydsi denoting the domain label, where
one of the domains is similar to the domains ofDa

and Dds. In this way, linguistic knowledge can
be transferred from DS and DD to AE and AS, as



509

Datasets Train Test
aspect opinion aspect opinion

D1 Restaurant14 3699 3484 1134 1008
D2 Laptop14 2373 2504 654 674
D3 Restaurant15 1199 1210 542 510

Table 2: Dataset statistics with numbers of aspect terms
and opinion terms

they are semantically relevant. We fix θds and θdd
when updating parameters for La, since we do not
want them to be affected by the small number of
aspect-level training instances.

4 Experiments

4.1 Experimental Settings

Datasets. Table 2 shows the statistics of
the aspect-level datasets. We run experiments
on three benchmark datasets, taken from Se-
mEval2014 (Pontiki et al., 2014) and SemEval
2015 (Pontiki et al., 2015). The opinion terms are
annotated by Wang et al. (2016a). We use two
document-level datasets from (He et al., 2018b).
One is from the Yelp restaurant domain, and the
other is from the Amazon electronics domain.
Each contains 30k instances with exactly balanced
class labels of pos, neg, and neu. We use the con-
catenation of the two datasets with domain labels
as Ddd. We use the Yelp dataset as Dds when Da

is either D1 or D3, and use the electronics dataset
as Dds when Da is D2.

Network details. We adopt the multi-layer-
CNN structure from (Xu et al., 2018) as the
CNN-based encoders in our proposed network.
See Appendix A for implementation details.
For word embedding initialization, we concate-
nate a general-purpose embedding matrix and
a domain-specific embedding matrix7 following
(Xu et al., 2018). We adopt their released domain-
specific embeddings for restaurant and laptop do-
mains with 100 dimensions, which are trained
on a large domain-specific corpus using fastText.
The general-purpose embeddings are pre-trained
Glove vectors (Pennington et al., 2014) with 300
dimensions.

One set of important hyper-parameters are the
number of CNN layers in the shared encoder and
the task-specific encoders. To decide the values
of ms, mae, mas, mds, mdd, we first investigate

7For DD, we only look at the general-purpose embeddings
by masking out the domain-specific embeddings.

how many layers of CNNs would work well for
each of the task when training it alone. We de-
note co as the optimal number of CNN layers in
this case, where o ∈ {ae, as, ds, dd} is the task
indicator. We perform AE, AS separately on the
training set of D1, and perform DS, DD separately
on the document-level restaurant corpus. Cross-
validation is used for selecting co, which yields 4,
2, 2, 2 for cae, cas, cds, cdd. Based on this observa-
tion, we made ms, mae, mas, mds, mdd equals to
2, 2, 0, 0, 0 respectively, such that ms +mo = co.
Note that there are other configurations satisfying
the requirement, for example,ms,mae,mas,mds,
mdd equals to 1, 3, 1, 1, 1. we select our setting as
it involves the smallest set of parameters.

We tune the maximum number of iterations T
in the message passing mechanism by training
IMN−d via cross validation on D1. It is set to
2. With T fixed as 2, we then tune r by training
IMN via cross validation on D1 and the relevant
document-level datasets. It is set to 2 as well.

We use Adam optimizer with learning rate set
to 10−4, and we set batch size to 32. Learning
rate and batch size are set to conventional values
without specific tuning for our task.

At training phase, we randomly sample 20% of
the training data from the aspect-level dataset as
the development set and only use the remaining
80% for training. We train the model for a fix num-
ber of epoches, and save the model at the epoch
with the best F1-I score on the development set
for evaluation.

Evaluation metrics. During testing, we extract
aspect (opinion) terms, and predict the sentiment
for each extracted aspect term based on ŷae(T ) and
ŷas(T ). Since the extracted aspect term may con-
sist of multiple tokens and the sentiment predic-
tions on them could be inconsistent in AS, we only
output the sentiment label of the first token as the
predicted sentiment for any extracted aspect term.

We employ five metrics for evaluation, where
two measure the AE performance, two measure
the AS performance, and one measures the over-
all performance. Following existing works for
AE (Wang et al., 2017; Xu et al., 2018), we use
F1 to measure the performance of aspect term ex-
traction and opinion term extraction, which are
denoted as F1-a and F1-o respectively. Follow-
ing existing works for AS (Chen et al., 2017; He
et al., 2018b), we adopt accuracy and macro-F1 to
measure the performance of AS. We denote them



510

as acc-s and F1-s. Since we are solving the in-
tegrated task without assuming that gold aspect
terms are given, the two metrics are computed
based on the correctly extracted aspect terms from
AE. We compute the F1 score of the integrated
task denoted as F1-I for measuring the overall per-
formance. To compute F1-I, an extracted aspect
term is taken as correct only when both the span
and the sentiment are correctly identified. When
computing F1-a, we consider all aspect terms,
while when computing acc-s, F1-s, and F1-I, we
ignore aspect terms with conflict sentiment labels.

4.2 Models under Comparison

Pipeline approach. We select two top-
performing models from prior works for each of
AE and AS, to construct 2 × 2 pipeline base-
lines. For AE, we use CMLA (Wang et al.,
2017) and DECNN (Xu et al., 2018). CMLA
was proposed to perform co-extraction of as-
pect and opinion terms by modeling their inter-
dependencies. DECNN is the state-of-the-art
model for AE. It utilizes a multi-layer CNN
structure with both general-purpose and domain-
specific embeddings. We use the same struc-
ture as encoders in IMN. For AS, we use ATAE-
LSTM (denoted as ALSTM for short) (Wang
et al., 2016b) and the model from (He et al.,
2018b) which we denote as dTrans. ALSTM
is a representative work with an attention-based
LSTM structure. We compare with dTrans as it
also utilizes knowledge from document corpora
for improving AS performance, which achieves
state-of-the-art results. Thus, we compare
with the following pipeline methods: CMLA-
ALSTM, CMLA-dTrans, DECNN-ALSTM,
and DECNN-dTrans. We also compare with the
pipeline setting of IMN, which trains AE and AS
independently (i.e., without parameter sharing, in-
formation passing, and document-level corpora).
We denote it as PIPELINE. The network struc-
ture for AE in PIPELINE is the same as DECNN.
During testing of all methods, we perform AE in
the first step, and then generate AS predictions on
the correctly extracted aspect terms.

Integrated Approach. We compare with two re-
cently proposed methods that have achieved state-
of-the-art results among integrated approaches:
MNN (Wang et al., 2018) and the model from (Li
et al., 2019) which we denote as INABSA (inte-
grated network for ABSA). Both methods model

the overall task as a sequence tagging problem
with a unified tagging scheme. Since during test-
ing, IMN only outputs the sentiment on the first
token of an extracted aspect term to avoid sen-
timent inconsistency, to enable fair comparison,
we also perform this operation on MNN and IN-
ABSA. We also show results for a version of IMN
that does not use document-level corpora, denoted
as IMN−d. The structure of IMN−d is shown as
the solid lines in Figure 1. It omits the information
ŷds, adsi , and a

dd
i propagated from the document-

level tasks in Eq.(4).

4.3 Results and Analysis

Main results. Table 3 shows the comparison re-
sults. Note that IMN performs co-extraction of as-
pect and opinion terms in AE, which utilizes ad-
ditional opinion term labels during training, while
the baseline methods except CMLA do not con-
sider this information in their original models. To
enable fair comparison, we slightly modify those
baselines to perform co-extraction as well, with
opinion term labels provided. Further details on
model comparison are provided in Appendix B.

From Table 3, we observe that IMN−d is able to
significantly outperform other baselines on F1-I.
IMN further boosts the performance and outper-
forms the best F1-I results from the baselines by
2.29%, 1.77%, and 2.61% on D1, D2, and D3.
Specifically, for AE (F1-a and F1-o), IMN−d per-
forms the best in most cases. For AS (acc-s and
F1-s), IMN outperforms other methods by large
margins. PIPELINE, IMN−d, and the pipeline
methods with dTrans also perform reasonably well
on this task, outperforming other baselines by
moderate margins. All these models utilize knowl-
edge from larger corpora by either joint training
of document-level tasks or using domain-specific
embeddings. This suggests that domain-specific
knowledge is very helpful, and both joint train-
ing and domain-specific embeddings are effective
ways to transfer such knowledge.

We also show the results of IMN−d and IMN
when only the general-purpose embeddings (with-
out domain-specific embeddings) are used for ini-
tialization. They are denoted as IMN−d/IMN wo
DE. IMN wo DE performs only marginally be-
low IMN. This indicates that the knowledge cap-
tured by domain-specific embeddings could be
similar to that captured by joint training of the
document-level tasks. IMN−d is more affected



511

M
et

ho
ds

C
M

L
A

-A
L

ST
M

C
M

L
A

-d
Tr

an
s

D
E

C
N

N
-A

L
ST

M

D
E

C
N

N
-d

Tr
an

s

PI
PE

L
IN

E

M
N

N

IN
A

B
SA

IM
N
−
d

w
o

D
E

IM
N
−
d

IM
N

w
o

D
E

IM
N

D
1

F1-a 82.45 82.45 83.94 83.94 83.94 83.05 83.92 83.95 84.01 83.50 83.33
F1-o 82.67 82.67 85.60 85.60 85.60 84.55 84.97 85.21 85.64 84.62 85.61
acc-s 77.46 79.58 77.79 80.04 79.56 77.17 79.68 79.65 81.56∗ 83.17∗ 83.89∗
F1-s 68.70 72.23 68.50 73.31 69.59 68.45 68.38 69.32 71.90 73.44 75.66
F1-I 63.87 65.34 65.26 67.25 66.53 63.87 66.60 66.96 68.32∗ 69.11∗ 69.54∗

D
2

F1-a 76.80 76.80 78.38 78.38 78.38 76.94 77.34 76.96 78.46 76.87 77.96
F1-o 77.33 77.33 78.81 78.81 78.81 77.77 76.62 76.85 78.14 77.04 77.51
acc-s 70.25 72.38 70.46 73.10 72.29 70.40 72.30 72.89 73.21 74.31∗ 75.36∗
F1-s 66.67 69.52 66.78 70.63 68.12 65.98 68.24 67.26 69.92 70.76 72.02∗
F1-I 53.68 55.56 55.05 56.60 56.02 53.80 55.88 56.25 57.66∗ 57.04∗ 58.37∗

D
3

F1-a 68.55 68.55 68.32 68.32 68.32 70.24 69.40 69.23 69.80 68.23 70.04
F1-o 71.07 71.07 71.22 71.22 71.22 69.38 71.43 68.39 72.11∗ 70.09 71.94
acc-s 81.03 82.27 80.32 82.65 82.27 80.79 82.56 81.64 83.38 85.90∗ 85.64∗
F1-s 58.91 66.45 57.25 69.58 59.53 57.90 58.81 57.51 60.65 71.67∗ 71.76∗
F1-I 54.79 56.09 55.10 56.28 55.96 56.57 57.38 56.80 57.91∗ 58.82∗ 59.18∗

Table 3: Model comparison. Average results over 5 runs with random initialization are reported. ∗ indicates the
proposed method is significantly better than the other baselines (p < 0.05) based on one-tailed unpaired t-test.

Model variants D1 D2 D3
Vanilla model 66.66 55.63 56.24
+Opinion transmission 66.98 56.03 56.65
+Message passing-a (IMN−d) 68.32 57.66 57.91
+DS 68.48 57.86 58.03
+DD 68.65 57.50 58.26
+Message passing-d (IMN) 69.54 58.37 59.18

Table 4: F1-I scores of different model variants. Aver-
age results over 5 runs are reported.

without domain-specific embeddings, while it still
outperforms all other baselines except DECNN-
dTrans. DECNN-dTrans is a very strong base-
line as it exploits additional knowledge from larger
corpora for both tasks. IMN−d wo DE is compet-
itive with DECNN-dTrans even without utilizing
additional knowledge, which suggests the effec-
tiveness of the proposed network structure.

Ablation study. To investigate the impact of dif-
ferent components, we start with a vanilla model
which consists of fθs , fθae , and fθas only without
any informative message passing, and add other
components one at a time. Table 4 shows the re-
sults of different model variants. +Opinion trans-
mission denotes the operation of providing addi-
tional information P opj to the self-attention layer
as shown in Eq.(1). +Message passing-a denotes
propagating the outputs from aspect-level tasks
only at each message passing iteration. +DS and
+DD denote adding DS and DD with parameter
sharing only. +Message passing-d denotes involv-
ing the document-level information for message

passing. We observe that +Message passing-a and
+Message passing-d contribute to the performance
gains the most, which demonstrates the effective-
ness of the proposed message passing mechanism.
We also observe that simply adding document-
level tasks (+DS/DD) with parameter sharing only
marginally improves the performance of IMN−d.
This again indicates that domain-specific knowl-
edge has already been captured by domain embed-
dings, while knowledge obtained from DD and DS
via parameter sharing could be redundant in this
case. However, +Message passing-d is still help-
ful with considerable performance gains, showing
that aspect-level tasks can benefit from knowing
predictions of the relevant document-level tasks.

Impact of T . We have demonstrated the effective-
ness of the message passing mechanism. Here, we
investigate the impact of the maximum number of
iterations T . Table 6 shows the change of F1-I on
the test sets as T increases. We find that conver-
gence is quickly achieved within two or three iter-
ations, and further iterations do not provide con-
siderable performance improvement.

Case study. To better understand in which condi-
tions the proposed method helps, we examine the
instances that are misclassified by PIPELINE and
INABSA, but correctly classified by IMN.

For aspect extraction, we find the message pass-
ing mechanism is particularly helpful in two sce-
narios. First, it helps to better recognize uncom-
mon aspect terms by utilizing information from
the opinion contexts. As shown in example 1 in



512

Examples
PIPELINE INABSA IMN

Opinion Aspect Opinion Aspect Opinion Aspect

1. Strong [build]pos though which reallyadds to its [durability]pos.
Strong [durability]pos Strong [durability]pos Strong [build]pos, [durability]pos

2. Curioni’s Pizza has been around sincethe 1920’s None [Pizza]neu None [Pizza]pos None None

3. The [battery]pos is longer longer [battery]neg longer [battery]neg longer [battery]pos
4. The [potato balls]pos were not dry at all dry [potato balls]neg dry [potato balls]neg dry [potato balls]pos

5. That’s a good thing, but it’s madefrom [aluminum]neg that scratches easily
. good,easily [aluminum]pos

good,
easily [aluminum]pos

good,
scratches easily [aluminum]neg

Table 5: Case analysis. The “Examples” column contains instances with gold labels. ’The “opinion” and “aspect”
columns present the opinion terms and aspect terms with sentiments, generated by the corresponding model.

T 0 1 2 3 4 5
D1 66.98 67.97 68.32 68.03 68.11 68.26
D2 56.03 57.14 57.66 57.82 57.78 57.33
D3 56.65 57.60 57.91 57.66 57.41 57.48

Table 6: F1 scores with different T values using
IMN−d. Average results over 5 runs are reported.

Table 5, PIPELINE and INABSA fail to recognize
“build” as it is an uncommon aspect term in the
training set while IMN is able to correctly rec-
ognize it. We find that when no message pass-
ing iteration is performed, IMN also fails to rec-
ognize “build”. However, when we analyze the
predicted sentiment distribution on each token in
the sentence, we find that except “durability”, only
“build” has a strong positive sentiment, while the
sentiment distributions on the other tokens are
more uniform. This is an indicator that “build” is
also an aspect term. IMN is able to aggregate such
knowledge with the message passing mechanism,
such that it is able to correctly recognize “build” in
later iterations. Due to the same reason, the mes-
sage passing mechanism also helps to avoid ex-
tracting terms on which no opinion is expressed.
As observed in example 2, both PIPELINE and
INABSA extract “Pizza”. However, since no opin-
ion is expressed in the given sentence, “Pizza”
should not be considered as an aspect term. IMN
avoids extracting this kind of terms by aggregating
knowledge from opinion prediction and sentiment
prediction.

For aspect-level sentiment, since IMN is trained
on larger document-level labeled corpora with
balanced sentiment classes, in general it better
captures the meaning of domain-specific opinion
words (example 3), better captures sentiments of
complex expressions such as negation (example
4), and better recognizes minor sentiment classes
in the aspect-level datasets (negative and neutral
in our cases). In addition, we find that knowledge

propagated by the document-level tasks through
message passing is helpful. For example, the
sentiment-relevant attention weights are helpful
for recognizing uncommon opinion words, and
which further help on correctly predicting the sen-
timents of the aspect terms. As observed in ex-
ample 5, PIPELINE and INABSA are unable to
recognize “scratches easily” as the opinion term,
and they also make wrong sentiment prediction
on the aspect term “aluminum”. IMN learns that
“scratches” is sentiment-relevant through knowl-
edge from the sentiment-relevant attention weights
aggregated via previous iterations of message
passing, and is thus able to extract “scratches eas-
ily”. Since the opinion predictions from AE are
sent to the self-attention layer in the AS compo-
nent, correct opinion predictions further help to in-
fer the correct sentiment towards “aluminum”.

5 Conclusion

We propose an interactive multi-task learning net-
work IMN for jointly learning aspect and opin-
ion term co-extraction, and aspect-level sentiment
classification. The proposed IMN introduces a
novel message passing mechanism that allows in-
formative interactions between tasks, enabling the
correlation to be better exploited. In addition,
IMN is able to learn from multiple training data
sources, allowing fine-grained token-level tasks to
benefit from document-level labeled corpora. The
proposed architecture can potentially be applied to
similar tasks such as relation extraction, semantic
role labeling, etc.

Acknowledgments

This research is supported by the National Re-
search Foundation Singapore under its AI Singa-
pore Programme grant AISG-RP-2018-006.



513

References
Stefanos Angelidis and Mirella Lapata. 2018. Summa-

rizing opinions: Aspect extraction meets sentiment
prediction and they are both weakly supervised. In
Conference on Empirical Methods in Natural Lan-
guage Processing.

Anurag Arnab, Shuai Zheng, Sadeep Jayasumana,
Bernardino Romera-Paredes, Måns Larsson,
Alexander Kirillov, Bogdan Savchynskyy, Carsten
Rother, Fredrik Kahl, and Philip HS Torr. 2018.
Conditional random fields meet deep neural net-
works for semantic segmentation: Combining
probabilistic graphical models with deep learning
for structured prediction. IEEE Signal Processing
Magazine, 35(1):37–52.

Peng Chen, Zhongqian Sun, Lidong Bing, and Wei
Yang. 2017. Recurrent attention network on mem-
ory for aspect sentiment analysis. In Conference on
Empirical Methods in Natural Language Process-
ing.

Jiajun Cheng, Shenglin Zhao, Jiani Zhang, Irwin King,
Xin Zhang, and Hui Wang. 2017. Aspect-level sen-
timent classification with heat (hierarchical atten-
tion) network. In ACM on Conference on Informa-
tion and Knowledge Management.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Interna-
tional Conference on Machine Learning.

Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent Twitter sentiment clas-
sification. In Annual Meeting of the Association for
Computational Linguistics.

Matthew R Gormley, Mark Dredze, and Jason Eisner.
2015. Approximation-aware dependency parsing by
belief propagation. Transactions of the Association
for Computational Linguistics, 3:489–501.

Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel
Dahlmeier. 2017. An unsupervised neural attention
model for aspect extraction. In Annual Meeting of
the Association for Computational Linguistics.

Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel
Dahlmeier. 2018a. Effective attention modeling for
aspect-level sentiment classification. In Interna-
tional Conference on Computational Linguistics.

Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel
Dahlmeier. 2018b. Exploiting document knowledge
for aspect-level sentiment classification. In Annual
Meeting of the Association for Computational Lin-
guistics.

Xin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018a.
Transformation networks for target-oriented senti-
ment classification. In Annual Meeting of the As-
sociation for Computational Linguistics.

Xin Li, Lidong Bing, Piji Li, and Wai Lam. 2019. A
unified model for opinion target extraction and tar-
get sentiment prediction. In AAAI Conference on
Artificial Intelligence.

Xin Li, Lidong Bing, Piji Li, Wai Lam, and Zhimou
Yang. 2018b. Aspect term extraction with history
attention and selective transformation. In Interna-
tional Joint Conference on Artificial Intelligence.

Xin Li and Wai Lam. 2017. Deep multi-task learn-
ing for aspect term extraction with memory interac-
tion. In Conference on Empirical Methods in Natu-
ral Language Processing.

Jiangming Liu and Yue Zhang. 2017. Attention model-
ing for target sentiment. In Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics.

Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui.
2016. Implicit discourse relation classification via
multi-task neural networks. In AAAI Conference on
Artificial Intelligence.

Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2015a. Multi-task se-
quence to sequence learning. In International Con-
ference on Learning Representation.

Yukun Ma, Haiyun Peng, and Erik Cambira. 2018.
Targeted aspect-based sentiment analysis via em-
bedding commonsense knowledge into an attentive
LSTM. In AAAI Conference on Artificial Intelli-
gence.

Thien Hai Nguyen and Kiyoaki Shirai. 2015.
PhraseRNN: Phrase recursive neural network for
aspect-based sentiment analysis. In Conference on
Empirical Methods in Natural Language Process-
ing.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word
representation. In Conference on Empirical Meth-
ods in Natural Language Processing.

Maria Pontiki, Dimitrios Galanis, Haris Papageorgiou,
Suresh Manandhar, and Ion Androutsopoulos. 2015.
SemEval-2015 task 12: Aspect based sentiment
analysis. In International Workshop on Semantic
Evaluation.

Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Haris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 task 4:
Aspect based sentiment analysis. In International
Workshop on Semantic Evaluation.

Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Lin-
guistics, 37(1):9–27.



514

Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu.
2016a. Effective LSTMs for target-dependent senti-
ment classification. In International Conference on
Computational Linguistics.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018.
Learning to attend via word-aspect associative fu-
sion for aspect-based sentiment analysis. In AAAI
Conference on Artificial Intelligence.

Duy-Tin Vo and Yue Zhang. 2015. Target-dependent
Twitter sentiment classification with rich automatic
features. In International Joint Conference on Arti-
ficial Intelligence.

Feixiang Wang, Man Lan, and Wenting Wang. 2018.
Towards a one-stop solution to both aspect extrac-
tion and sentiment analysis tasks with neural multi-
task learning. In International Joint Conference on
Neural Networks.

Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and
Xiaokui Xiao. 2016a. Recursive neural conditional
random fields for aspect-based sentiment analysis.
In Conference on Empirical Methods in Natural
Language Processing.

Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, and
Xiaokui Xiao. 2017. Coupled multi-layer attentions
for co-extraction of aspect and opinion terms. In
AAAI Conference on Artificial Intelligence.

Yequan Wang, Minlie Huang, Li Zhao, and Xiaoyan
Zhu. 2016b. Attention-based LSTM for aspect-level
sentiment classification. In Conference on Empiri-
cal Methods in Natural Language Processing.

Hu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2018.
Double embeddings and CNN-based sequence la-
beling for aspect extraction. In Annual Meeting of
the Association for Computational Linguistics.

Yichun Yin, Furu Wei, Li Dong, Kaimeng Xu, Ming
Zhang, and Ming Zhou. 2016. Unsupervised word
and dependency path embeddings for aspect term
extraction. In International Joint Conference on Ar-
tificial Intelligence.

Meishan Zhang, Yue Zhang, and Duy-Tin Vo. 2015.
Neural networks for open domain targeted senti-
ment. In Conference on Empirical Methods in Nat-
ural Language Processing.

Meishan Zhang, Yue Zhang, and Duy-Tin Vo. 2016.
Gated neural networks for targeted sentiment analy-
sis. In AAAI Conference on Artificial Intelligence.

A Implementation Details

CNN-based Encoder
We adopt the multi-layer-CNN structure from (Xu
et al., 2018) as the CNN-based encoders for both
the shared CNNs and the task-specific ones in the

proposed network. Each CNN layer has many 1D-
convolution filters, and each filter has a fixed ker-
nel size k = 2c+ 1, such that each filter performs
convolution operation on a window of k word rep-
resentations, and compute the representation for
the ith word along with 2c nearby words in its con-
text.

Following the settings in the original paper, the
first CNN layer in the shared encoder has 128 fil-
ters with kernel sizes k = 3 and 128 filters with
kernel sizes k = 5. The other CNN layers in
the shared encoder and the CNN layers in each
task-specific encoder have 256 filters with kernel
sizes k = 5 per layer. ReLu is used as the acti-
vation function for each CNN layer. Dropout with
p = 0.5 is employed after the embedding layer
and each CNN layer.

Opinion Transmission
To alleviate the problem of unreliable predictions
of opinion labels in the early stage of training, we
adopt scheduled sampling for opinion transmis-
sion at training phase. We send gold opinion la-
bels rather than the predicted ones generated by
AE to AS in the probability of �i. The probabil-
ity �i depends on the number of epochs i during
training, for which we employ an inverse sigmoid
decay �i = 5/(5 + exp(i/5)).

B Model Comparison Details

For CMLA8, ALSTM9, dTrans10, and INABSA11,
we use the officially released source codes for
experiments. For MNN, we re-implement the
model following the descriptions in the paper as
the source code is not available. We run each base-
line multiple times with random initializations and
save their predicted results. We use an unified
evaluation script for measuring the outputs from
different baselines as well as the proposed method.

The proposed IMN performs co-extraction of
aspect terms and opinion terms in AE, which uti-
lizes additional opinion term labels during model
training. In the baselines, the two integrated meth-
ods MNN and INABSA, and the pipeline meth-
ods with DECNN as the AE component do not

8https://github.com/happywwy/
Coupled-Multi-layer-Attentions

9https://www.wangyequan.com/
publications/

10https://github.com/ruidan/
Aspect-level-sentiment

11https://github.com/lixin4ever/
E2E-TBSA

https://github.com/happywwy/Coupled-Multi-layer-Attentions
https://github.com/happywwy/Coupled-Multi-layer-Attentions
https://www.wangyequan.com/publications/
https://www.wangyequan.com/publications/
https://github.com/ruidan/Aspect-level-sentiment
https://github.com/ruidan/Aspect-level-sentiment
https://github.com/lixin4ever/E2E-TBSA
https://github.com/lixin4ever/E2E-TBSA


515

Methods D1 D2 D3
F1-a acc-s F1-s F1-I F1-a acc-s F1-s F1-I F1-a acc-s F1-s F1-I

DECNN-ALSTM 83.33 77.63 70.09 64.32 80.28 69.98 66.20 55.92 68.72 79.22 54.40 54.22
DECNN-dTrans 83.33 79.45 73.08 66.15 80.28 71.51 68.03 57.28 68.72 82.09 68.35 56.08
PIPELINE 83.33 79.39 69.45 65.96 80.28 72.12 68.56 57.29 68.72 81.85 58.74 56.04
MNN 83.20 77.57 68.19 64.26 76.33 70.62 65.44 53.77 69.29 80.86 55.45 55.93
INABSA 83.12 79.06 68.77 65.94 77.67 71.72 68.36 55.95 68.79 80.96 57.10 55.45
IMN−d 83.89 80.69 72.09 67.27∗ 78.43 72.49 69.71 57.13 70.35∗ 81.86 56.88 57.86∗

IMN 83.04 83.05∗ 73.30 68.71∗ 77.69 75.12∗ 71.35∗ 58.04∗ 69.25 84.53∗ 70.85∗ 58.18∗

Table 7: Model comparison in a setting without opinion term labels. Average results over 5 runs with random
initialization are reported. ∗ indicates the proposed method is significantly better than the other baselines (p <
0.05) based on one-tailed unpaired t-test.

take take opinion information during training. To
make fair comparison, we add labels {BP, IP}
to the original label sets of MNN, INABSA, and
DECNN, indicating the beginning of and inside of
an opinion term. We train those models on train-
ing sets with both aspect and opinion term labels
to perform co-extraction as well. In addition, for
pipeline methods, we also make the gold opin-
ion terms available to the AS models (ALSTM
and dTrans) during training. To make ALSTM
and dTrans utilize the opinion label information,
we modify their attention layer to assign higher
weights to tokens that are more likely to be part of
an opinion term. This is reasonable since the ob-
jective of the attention mechanism in an AS model
is to find the relevant opinion context. The atten-
tion weight of the ith token before applying soft-
max normalization in an input sentence is modi-
fied as:

a′i = ai ∗ P
op
i (7)

where ai denotes the attention weight computed
by the original attention layer, popi denotes the
probability that the ith token belongs to any
opinion term. a′i denotes the modified attention
weights. At the training phase, since the gold
opinion terms are provided, popi = 1 for the to-
kens that are part of the gold opinion terms, while
popi = 0 for the other tokens. At the testing phase,
popi is computed based on the predictions from the
AE model in the pipeline method. It is computed
by summing up the predicted probabilities on the
opinion-related labels BP and IP for the ith token.

We also present the comparison results in a
setting without using opinion term labels in Ta-
ble 712. In this setting, we modify the proposed
IMN and IMN−d to recognize aspect terms only

12We exclude the results of the pipeline methods with
CMLA, as CMLA relies on opinion term labels during train-
ing. It is difficult to modify it.

in AE. The opinion transmission operation, which
sends the opinion term predictions from AE to AS,
is omitted as well.

Both IMN−d and IMN still significantly out-
perform other baselines in most cases under this
setting. In addition, when compare the results
in Table 7 and Table 3, we observe that IMN−d

and IMN consistently yield better F1-I scores on
all datasets in Table 3, when opinion term ex-
traction is also considered. Consistent improve-
ments are not observed in other baseline methods
when trained with opinion term labels. These find-
ings suggest that knowledge obtained from learn-
ing opinion term extraction is indeed beneficial,
however, a carefully-designed network structure is
needed to utilize such information. IMN is de-
signed to exploit task correlations by explicitly
modeling interactions between tasks, and thus it
better integrates knowledge obtained from training
different tasks.


