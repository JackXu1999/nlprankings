



















































UNITOR: Aspect Based Sentiment Analysis with Structured Learning


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 761–767,
Dublin, Ireland, August 23-24, 2014.

UNITOR: Aspect Based Sentiment Analysis with Structured Learning
Giuseppe Castellucci(†), Simone Filice(‡), Danilo Croce(?), Roberto Basili(?)

(†) Dept. of Electronic Engineering
(‡) Dept. of Civil Engineering and Computer Science Engineering

(?) Dept. of Enterprise Engineering
University of Roma, Tor Vergata, Italy

{castellucci,filice}@ing.uniroma2.it; {croce,basili}@info.uniroma2.it

Abstract

In this paper, the UNITOR system partici-
pating in the SemEval-2014 Aspect Based
Sentiment Analysis competition is pre-
sented. The task is tackled exploiting Ker-
nel Methods within the Support Vector
Machine framework. The Aspect Term
Extraction is modeled as a sequential tag-
ging task, tackled through SVMhmm. The
Aspect Term Polarity, Aspect Category
and Aspect Category Polarity detection are
tackled as a classification problem where
multiple kernels are linearly combined to
generalize several linguistic information.
In the challenge, UNITOR system achieves
good results, scoring in almost all rank-
ings between the 2nd and the 8th position
within about 30 competitors.

1 Introduction

In recent years, many websites started offering a
high level interaction with users, who are no more
a passive audience, but can actively produce new
contents. For instance, platforms like Amazon1 or
TripAdvisor2 allow people to express their opin-
ions on products, such as food, electronic items
or clothes. Obviously, companies are interested
in understanding what customers think about their
brands and products, in order to implement correc-
tive strategies on products themselves or on mar-
keting solutions. Performing an automatic analy-
sis of user opinions is then a very hot topic. The
automatic extraction of subjective information in
text materials is generally referred as Sentiment
Analysis or Opinion Mining and it is performed

This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/

1
http://www.amazon.com

2
http://www.tripadvisor.com

via natural language processing, text analysis and
computational linguistics techniques. Task 4 in
SemEval 2014 edition3 (Pontiki et al., 2014) aims
at promoting research on Aspect Based Opinion
Mining (Liu, 2007), which is approached as a cas-
cade of 4 subtasks. For example, let us consider
the sentence:

The fried rice is amazing here. (1)

The Aspect Term Extraction (ATE) subtask aims
at finding words suggesting the presence of as-
pects on which an opinion is expressed, e.g.
fried rice in sentence 1. In the Aspect Term
Polarity (ATP) task the polarity evoked for each
aspect is recognized, i.e. a positive polarity is
expressed with respect to fried rice. In the
Aspect Category Detection (ACD) task the cate-
gory evoked in a sentence is identified, e.g. the
food category in sentence 1). In the Aspect Cat-
egory Polarity (ACP) task the polarity of each ex-
pressed category is recognized, e.g. a positive
category polarity is expressed in sentence 1.

Different strategies have been experimented in
recent years. Classical approaches are based on
machine learning techniques and rely on sim-
ple representation features, such as unigrams, bi-
grams, Part-Of-Speech (POS) tags (Pang et al.,
2002; Pang and Lee, 2008; Wiebe et al., 1999).
Other approaches adopt sentiment lexicons in or-
der to exploit some sort of prior knowledge about
the polar orientation of words. These resources are
usually semi-automatically compiled and provide
scores associating individual words to sentiments
or polarity orientation.

In this paper, the UNITOR system participat-
ing to the SemEval-2014 Aspect Based Sentiment
Analysis task (Pontiki et al., 2014) is presented.
The ATE task is modeled as a sequential labeling
problem. A sentence is considered as a sequence
of tokens: a Markovian algorithm is adopted in

3
http://alt.qcri.org/semeval2014/task4/

761



order to decide what is an aspect term . All the
remaining tasks are modeled as multi-kernel clas-
sification problems based on Support Vector Ma-
chines (SVMs). Various representation have been
exploited using proper kernel functions (Shawe-
Taylor and Cristianini, 2004a). Tree Kernels
(Collins and Duffy, 2001; Moschitti et al., 2008;
Croce et al., 2011) are adopted in order to capture
structural sentence information derived from the
parse tree. Moreover, corpus-driven methods are
used in order to acquire meaning generalizations
in an unsupervised fashion (e.g. see (Pado and La-
pata, 2007)) through the analysis of distributions
of word occurrences in texts. It is obtained by the
construction of a Word Space (Sahlgren, 2006),
which provides a distributional model of lexical
semantics. Latent Semantic Kernel (Cristianini et
al., 2002) is thus applied within such space.

In the remaining, in Section 2 and 3 we will ex-
plain our approach in more depth. Section 4 dis-
cusses the results in the SemEval-2014 challenge.

2 Sequence Labeling for ATE

The Aspect Term Extraction (ATE) has been mod-
eled as a sequential tagging process. We con-
sider each token representing the beginning (B),
the inside (I) or the outside (O) of an argu-
ment. Following this IOB notation, the resulting
ATE representation of a sentence like “The [fried
rice]ASPECTTERM is amazing here” can be expressed
by labeling each word according to its relative po-
sition, i.e.: [The]O [fried]B [rice]I [is]O [amaz-
ing]O [here]O.

The ATE task is thus a labeling process that
determines the individual (correct IOB) class for
each token. The labeling algorithm used is
SVMhmm (Altun et al., 2003)4: it combines
both a discriminative approach to estimate the
probabilities in the model and a generative ap-
proach to retrieve the most likely sequence of
tags that explains a sequence. Given an input
sequence x = (x1 . . . xl) ∈ X of feature vec-
tors x1 . . . xl, the model predicts a tag sequence
y = (y1 . . . yl) ∈ Y after learning a linear dis-
criminant function F : X × Y → R over input-
output pairs. The labeling f(x) is thus defined as:
f(x) = arg maxy∈Y F (x,y; w) and it is obtained
by maximizing F over the response variable, y,
for a specific given input x. F is linear in some

4
www.cs.cornell.edu/People/tj/svm light/svm hmm.html

combined feature representation of inputs and out-
puts Φ(x,y), i.e. F (x,y; w) = 〈w,Φ(x,y)〉.

In SVMhmm the observations x1 . . . xl can be
naturally expressed in terms of feature vectors. In
particular, we modeled each word through a set of
lexical and syntactic features, as described in the
following section.

2.1 Modeling Features for ATE

In the discriminative view of SVMhmm, each
word is represented by a feature vector, describ-
ing its different observable properties. For in-
stance, the word rice in the example 1 is modeled
through the following features: Lexical features:
its lemma (rice) and POS tag (NN); Prefixes and
Suffixes: the first n and the last m characters of
the word (n = m = 3) (e.g. ric and ice); Con-
textual features: the left and right lexical contexts
represented by the 3 words before (BEGIN::BB
the::DT fried::JJ) and after (is::VBZ amazing::JJ
here::RB); the left and right syntactic contexts as
the POS bi-grams and tri-grams occurring before
(i.e. BB DT DT JJ BB DT JJ) and after (i.e.
VBZ JJ JJ RB VBZ JJ RB) the word; Gram-
matical features: features derived from the de-
pendency graph associated to the sentence, i.e.
boolean indicators that capture if the token is in-
volved in a Subj, Obj or Amod relation in the cor-
responding graph.

3 Multiple Kernel Approach for Polarity
and Category Detection

We approached the remaining three subtasks of the
pipeline as classification problems with multiple
kernels, in line with (Castellucci et al., 2013). We
used Support Vector Machines (SVMs) (Joachims,
1999), a maximum-margin classifier that realizes
a linear discriminative model. The kernelized ver-
sion of SVM learns from instances xi exploiting
rich similarity measures (i.e.the kernel functions)
K(xi, xj) = 〈φ(xi) · φ(xj)〉. In this way projec-
tion functions φ(·) can be implicitly used in order
to transform the initial feature space into a more
expressive one, where a hyperplane that separates
the data with the widest margin can be found.
Kernels can directly operate on variegate forms
of representation, such as feature vectors, trees,
sequences or graphs. Then, modeling instances
in different representations, specific kernels can
be defined in order to explore different linguis-
tic information. These variety of kernel functions

762



K1 . . .Kn can be independently defined and the
combinations K1 + K2 + . . . of multiple func-
tions can be integrated into SVM as they are still
kernels. The next section describes the represen-
tations as well as the kernel functions.

3.1 Representing Lexical Information

The Bag of Word (BoW) is a simple repre-
sentation reflecting the lexical information of the
sentence. Each text is represented as a vector
whose dimensions correspond to different words,
i.e. they represent a boolean indicator of the pres-
ence or not of a word in the text. The resulting
kernel function is the cosine similarity (or linear
kernel) between vector pairs, i.e. linBoW. In line
with (Shawe-Taylor and Cristianini, 2004b) we in-
vestigated the contribution of the Polynomial Ker-
nel of degree 2, poly2BoW as it defines an implicit
space where also feature pairs, i.e. words pairs,
are considered.

In the polarity detection tasks, several polarity
lexicons have been exploited in order to have use-
ful hints of the intrinsic polarity of words. We
adopted MPQA Subjectivity Lexicon5 (Wilson et
al., 2005) and NRC Emotion Lexicon (Moham-
mad and Turney, 2013): they are large collection
of words provided with the underlying emotion
they generally evoke. While the former consid-
ers only positive and negative sentiments, the lat-
ter considers also eight primary emotions, orga-
nized in four opposing pairs, joy-sadness, anger-
fear, trust-disgust, and anticipation-surprise. We
define the Lexicon Based (LB) vectors as follows.
For each lexicon, let E = {e1, ..., e|E|} be the
emotion vocabulary defined in it. Let w ∈ s be
a word occurring in sentence s, with I(w, i) be-
ing the indicator function whose output is 1 if w
is associated to the emotion label ei, or 0 other-
wise. Then, given a sentence s, each ei, i.e. a di-
mension of the emotional vocabularyE, receives a
score si =

∑
w∈s I(w, i). Each sentence produces

a vector ~s ∈ R|E|, for each lexicon, on which a lin-
ear kernel linLB is applied.

3.2 Generalizing Lexical Information

Another representation is used to generalize the
lexical information of each text, without exploit-
ing any manually coded resource. Basic lexical
information is obtained by a co-occurrence Word
Space (WS) built accordingly to the methodology

5
http://mpqa.cs.pitt.edu/lexicons/subj lexicon

described in (Sahlgren, 2006) and (Croce and Pre-
vitali, 2010). A word-by-context matrix M is ob-
tained through a large scale corpus analysis. Then
the Latent Semantic Analysis (Landauer and Du-
mais, 1997) technique is applied as follows. The
matrix M is decomposed through Singular Value
Decomposition (SVD) (Golub and Kahan, 1965)
into the product of three new matrices: U , S, and
V so that S is diagonal and M = USV T . M
is then approximated by Mk = UkSkV Tk , where
only the first k columns of U and V are used,
corresponding to the first k greatest singular val-
ues. This approximation supplies a way to project
a generic wordwi into the k-dimensional space us-
ing W = UkS

1/2
k , where each row corresponds to

the representation vector ~wi. The result is that ev-
ery word is projected in the reduced Word Space
and a sentence is represented by applying an addi-
tive model as an unbiased linear combination. We
adopted these vector representations using a linear
kernel, as in (Cristianini et al., 2002), i.e. linWS
and a Radial Basis Function Kernel rbfWS.

In Aspect Category Detection, and more gen-
erally in topic classification tasks, some specific
words can be an effective indicator of the under-
lying topic. For instance, in the restaurant do-
main, the word tasty may refer to the quality of
food. These kind of word-topic relationships can
be automatically captured by a Bag-of-Word ap-
proach, but with some limitations. As an exam-
ple, a BoW representation may not capture syn-
onyms or semantically related terms. This lack
of word generalization is partially compensated
by the already discussed Word Space. However,
this last representation aims at capturing the sense
of an overall sentence, and no particular rele-
vance is given to individual words, even if they
can be strong topic indicators. To apply a model-
ing more focused on topics, we manually selected
m seed words {σ1, . . . , σm} that we consider re-
liable topic-indicators, for example spaghetti for
food. Notice that for every seed σi, as well as for
every word w the similarity function sim(σi, w)
can be derived from the Word Space represen-
tations ~σi and ~w, respectively. What we need
is a specific seed-based representation reflecting
the similarity between topic indicators and sen-
tences s. Given the words w occurring in s, the
Seed-Oriented (SO) representation of s is an m-
dimensional vector ~so(s) whose components are:
soi(s) = maxw∈s sim(σi, w). Alternatively, as

763



seeds σ refer to a set of evoked topics (i.e. as-
pect categories such as food) Σ1, ...,Σt, we can
define a t-dimensional vector ~to(s) called Topic-
Oriented (TO) representation for s, whose fea-
tures are: toi(s) = maxw∈s,σk∈Σi sim(σk, w).

The adopted word similarity function sim(·, ·)
over ~so(s) and ~to(s) depends on the experiments.
In the unconstrained setting, i.e. the Word Space
Topic Oriented WSTO system, sim(·, ·) consists
in the dot product over the Word Space represen-
tations ~σi and ~w. In the constrained case sim(·, ·)
corresponds to the Wu & Palmer similarity based
on WordNet (Wu and Palmer, 1994), in the so
called WordNet Seed Oriented WNSO system.
The Radial Basis Function (RBF) kernel is then
applied onto the resulting feature vectors ~to(s) and
~so(s) in the rbfWSTO and rbfWNSO, respectively.

3.3 Generalizing Syntactic Information

In order to exploit the syntactic information, Tree
Kernel functions proposed in (Collins and Duffy,
2001) are adopted. Tree kernels exploit syntactic
similarity through the idea of convolutions among
syntactic tree substructures. Any tree kernel evalu-
ates the number of common substructures between
two trees T1 and T2 without explicitly considering
the whole fragment space. Many tree represen-
tations can be derived to represent the syntactic
information, according to different syntactic theo-
ries. For this experiment, dependency formalism
of parse trees is employed to capture sentences
syntactic information. As proposed in (Croce et
al., 2011), the kernel function is applied to ex-
amples modeled according the Grammatical Rela-
tion Centered Tree representation from the orig-
inal dependency parse structures, shown in Fig.
1: non-terminal nodes reflect syntactic relations,
such as NSUBJ, pre-terminals are the Part-Of-
Speech tags, such as nouns, and leaves are lex-
emes, such as rice::n and amazing::j6. In each ex-
ample, the aspect terms and the covering nodes are
enriched with a a suffix and all lexical nodes are
duplicated by the node asp in order to reduce data
sparseness. Moreover, prior information derived
by the lexicon can be injected in the tree, by du-
plicating all lexical nodes annotated in the MPQA
Subjectivity Lexicon, e.g. the adjective amazing,
with a node expressing the polarity (pos).

Given two tree structures T1 and T2, the

6Each word is lemmatized to reduce data sparseness, but
they are enriched with POS tags.

ROOT

ADVM

RB

here::r

JJ

posamazing::j

COP

VBZ

be::v

NSUBJa

NNa

asprice::n

AMODa

VBNa

aspfry::v

DET

DT

the::d

Figure 1: Tree representation of the sentence 1.

Tree Kernel formulation is reported hereafter:
TK(T1, T2) =

∑
n1∈NT1

∑
n2∈NT2 ∆(n1, n2)

where NT1 and NT2 are the sets of the T1’s and
T2’s nodes, respectively and ∆(n1, n2) is equal to
the number of common fragments rooted in the n1
and n2 nodes. The function ∆ determines the na-
ture of the kernel space. In the constrained case the
Partial Tree Kernel formulation (Moschitti, 2006)
is used, i.e. ptkGRCT. In the unconstrained set-
ting the Smoothed Partial Tree Kernel formulation
(Croce et al., 2011) is adopted to emphasizes the
lexicon in the Word Space, i.e. the sptkGRCT. It
computes the similarity between lexical nodes as
the similarity between words in the Word Space.
So, this kernel allows a generalization both from a
syntactic and lexical point of view.

4 Results

In this Section the experimental results of the
UNITOR system in the four different subtasks of
Semeval 2014 competition are discussed. Teams
were allowed to submit two different outcomes for
each task: constrained submissions (expressed by
the suffix C in all the tables) are intended to mea-
sure systems ability to learn sentiment analysis
models only over the provided data; unconstrained
(expressed by the suffix U in all the tables) sub-
missions allows teams to exploit additional train-
ing data. The first two tasks, i.e. ATE and ATP,
are defined on the laptop and restaurant domains,
while the last two tasks, i.e. ACD and ACP, are
defined for the restaurant dataset only.

The unconstrained versions are derived by ex-
ploiting word vectors derived in an unsupervised
fashion through the analysis of large scale cor-
pora. All words in a corpus occurring more than
100 times (i.e. the targets) are represented through
vectors. The original space dimensions are gen-
erated from the set of the 20,000 most frequent
words (i.e. features) in the corpus. One dimension
describes the Point-wise Mutual Information score
between one feature, as it occurs on a left or right
window of 3 tokens around a target. Left contexts
of targets are distinguished from the right ones, in
order to capture asymmetric syntactic behaviors

764



(e.g., useful for verbs): 40,000 dimensional vec-
tors are thus derived for each target. The Singular
Value Decomposition is applied and the space di-
mensionality is reduced to k = 250. Two corpora
are used for generating two different Word Spaces,
one for the laptop and one for the restaurant do-
main. The Opinosis dataset (Ganesan et al., 2010)
is used to build the electronic domain Word Space,
while the restaurant domain corpus adopted is the
TripAdvisor dataset7. Both provided data and in-
domain data are first pre-processed through the
Stanford Parser (Klein and Manning, 2003) in or-
der to obtain POS tags or Dependency Trees.

A modified version of LibSVM has been
adopted to implement Tree Kernel. Parameters
such as the SVM regularization coefficient C, the
kernel parameters (for instance the degree of the
polynomial kernel) have been selected after a tun-
ing stage based on a 5-fold cross validation.

4.1 Aspect Term Extraction

The Aspect Term Extraction task is modeled as a
sequential labeling problem. The feature represen-
tation described in Section 2.1, where each token
is associated to a specific target class according to
the IOB notation, is used in the SVMhmm learn-
ing algorithm. In the constrained version of the
UNITOR system only the training data are used
to derive features. In the unconstrained case the
UNITOR system exploits lexical vectors derived
from a Word Space. Each token feature repre-
sentation is, in this sense, augmented through dis-
tributional vectors derived from the Word Spaces
described above. Obviously, the Opinosis Word
Space is used in the laptop subtask, while the Tri-
pAdvisor Word Space is used in the restaurant sub-
task. These allow the system to generalize the lex-
ical information, enabling a smoother match be-
tween words during training and test phases, hope-
fully capturing similarity phenomena such as the
relation between screen and monitor.

In Table 1 results in the laptop case are reported.
Our system performed quite well, and ranked in
6th and 10th position over 28 submitted systems.
In this case, the use of the Word Space is effec-
tive, as noticed by the 4 position gain in the final
ranking (almost 2 points in F1-measure). In Table
2 results in the restaurant case are reported. Here,
the use of Word Space does not give an improve-
ment in the final performance.

7
http://sifaka.cs.uiuc.edu/˜wang296/Data/index.html

Table 1: Aspect Term Extraction Results - Laptop.
System (Rank) P R F1
UNITOR-C (10/28) .7741 .5764 .6608
UNITOR-U (6/28) .7575 .6162 .6795
Best-System-C (1/28) .8479 .6651 .7455
Best-System-U (2/28) .8251 .6712 .7403

Table 2: Aspect Term Extraction - Restaurants.
System (Rank) P R F1
UNITOR-C (5/29) .8244 .7786 .8009
UNITOR-U (6/29) .8131 .7865 .7996
Best-System-C (2/29) .8624 .8183 .8398
Best-System-U (1/29) .8535 .8271 .8401

In both cases, we observed that most of the
errors were associated to aspect terms composed
by multiple words. For example, in the sen-
tence The portions of the food that came out were
mediocre the gold aspect term is portions of
the food while our system was able only to re-
trieve food as aspect term. The system is mainly
able to recognize single word aspect terms and, in
most of the cases, double words aspect terms.

4.2 Aspect Term Polarity

The Aspect Term Polarity subtask has been mod-
eled as a multi-class classification problem: for
a given set of aspect terms within a sentence, it
aims at determining whether the polarity of each
aspect term is positive, negative, neutral or con-
flict. It has been tackled using multi-kernel SVMs
in a One-vs-All Schema. In the constrained set-
ting, the linear combination of the following ker-
nel functions have been used: ptkGRCT , poly2BoW
that consider all the lemmatized terms in the sen-
tence, a poly2BoW that considers only the aspect
terms, poly2BoW of the terms around the aspect
terms in a window of size 5, linLB derived from
the Emolex lexicon. In the unconstrained setting
the sptkGRCT replaces the ptk counterpart and
the rbfWS is added by linearly combining Word
Space vectors for verbs, nouns adjective and ad-
verbs. Results in Table 3 show that the proposed
kernel combination allows to achieve the 8th posi-
tion with the unconstrained system in the restau-
rant domain. The differences with the constrained
setting demonstrate the contribution of the Word
Space acquired from the TripAdvisor corpus. Un-
fortunately, it is not true in the laptop domain, as
shown in Table 4. The use of the Opinosis corpus
lets to a performance drop of the unconstrained
setting. An error analysis shows that the main lim-

765



itation of the proposed model is the inability to
capture deep semantic phenomena such as irony,
as in the negative sentence “the two waitress’s
looked like they had been sucking on lemons”.

Table 3: Aspect Term Polarity Results - Restau-
rant.

System (Rank) Accuracy
UNITOR-C (12/36) .7248
UNITOR-U (8/36) .7495
Best-System-C (1/36) .8095
Best-System-U (5/36) .7768

Table 4: Aspect Term Polarity Results - Laptop.
System (Rank) Accuracy
UNITOR-C (10/32) .6299
UNITOR-U (17/32) .5856
Best-System-C (1/32) .7048
Best-System-U (5/32) .6666

4.3 Aspect Category Detection
The Aspect Category Detection has been mod-
eled as a multi-label classification task where 5
categories (ambience, service, food, price, anec-
dotes/miscellaneous) must be recognized. In the
constrained version, each class has been tack-
led using a binary multi-kernel SVM equipped
with a linear combination of poly2BoW and
rbfWNSO. A category is assigned if the SVM
classifiers provides a positive prediction. The
anecdotes/miscellaneous acceptance threshold has
been set to 0.3 (it has been estimated over a de-
velopment set) due to its poor precision observed
during the tuning phase. Moreover, considering
each sentence is always associated to at least one
category, if no label has been assigned, then the
sentence is labelled with the category associated
to the highest prediction.

In the unconstrained case, each class has been
tackled using an ensemble of a two binary SVM-
based classifiers. The first classifier is a multi-
kernel SVM operating on a linear combination of
rbfWS and poly2BoW . The second classifier is a
SVM equipped with a rbfWSTO. A sentence is la-
belled with a category if at least one of the two cor-
responding classifiers predicts that label. The first
classifier assigns a label if the corresponding pre-
diction is positive. A more conservative strategy
is applied to the second classifier, and a category
is selected if its corresponding prediction is higher
than 0.3; again this threshold has been estimated
over a development set. As in the constrained ver-
sion, we observed a poor precision in the anec-

dotes/miscellaneous category, so we increased the
first classifier acceptance threshold to 0.3, while
the second classifier output is completely ignored.
Finally, if no label has been assigned, the sentence
is labelled with the category associated to the high-
est prediction of the first classifier.

Table 5: Aspect Category Detection Results.
System (Rank) P R F1
UNITOR-C (6/21) .8368 .7804 .8076
UNITOR-U (2/21) .8498 .8556 .8526
Best-System-C (1/21) .9104 .8624 .8857
Best-System-U (4/21) .8435 .7892 .8155

Table 5 reports the achieved results. Consider-
ing the simplicity of the proposed approach, the
results are impressive. The ensemble schema,
adopted in the unconstrained version, is very use-
ful in improving the recall and allows the system
to achieve the second position in the competition.

4.4 Aspect Category Polarity

The Aspect Category Polarity subtask has been
modeled as a multi-class classification problem:
given a set of pre-identified aspect categories for a
sentence, it aims at determining the polarity (pos-
itive, negative, neutral or conflict) of each cate-
gory. It has been tackled using multi-kernel SVMs
in a One-vs-All Schema. In the constrained set-
ting, the linear combination of the following ker-
nel functions has been used: ptkGRCT , poly2BoW
that consider all the lemmatized terms in the sen-
tence, a poly2BoW that considers only verbs, nouns
adjective and adverbs in the sentence, linLB de-
rived from the MPQA sentiment lexicon. In the
unconstrained case the sptkGRCT replaces the ptk
counterpart and the rbfWS is added by linearly
combining Word Space vectors for verbs, nouns
adjective and adverbs.

Again, results shown in Table 6 suggest the pos-
itive contribution of the lexical generalization pro-
vided by the Word Space (in the sptkGRCT and
rbfWS) allows to achieve a good rank, i.e. the
4th position with the unconstrained system in the
restaurant domain. The error analysis underlines
that the proposed features do not capture irony.

Table 6: Aspect Category Polarity Results.
System (Rank) Accuracy
UNITOR-C (7/25) .7307
UNITOR-U (4/25) .7629
Best-System-C (1/25) .8292
Best-System-U (9/25) .7278

766



References
Yasemin Altun, I. Tsochantaridis, and T. Hofmann.

2003. Hidden Markov support vector machines. In
Proceedings of the International Conference on Ma-
chine Learning.

Giuseppe Castellucci, Simone Filice, Danilo Croce,
and Roberto Basili. 2013. Unitor: Combining
syntactic and semantic kernels for twitter sentiment
analysis. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 369–
374, Atlanta, Georgia, USA, June. ACL.

Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neu-
ral Information Processing Systems (NIPS’2001),
pages 625–632.

Nello Cristianini, John Shawe-Taylor, and Huma
Lodhi. 2002. Latent semantic kernels. J. Intell.
Inf. Syst., 18(2-3):127–152.

Danilo Croce and Daniele Previtali. 2010. Mani-
fold learning for the semi-supervised induction of
framenet predicates: an empirical investigation. In
GEMS 2010, pages 7–16, Stroudsburg, PA, USA.
ACL.

Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In Proceed-
ings of EMNLP, Edinburgh, Scotland, UK.

Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: a graph-based approach to abstrac-
tive summarization of highly redundant opinions. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 340–348. ACL.

Gene Golub and W. Kahan. 1965. Calculating the sin-
gular values and pseudo-inverse of a matrix. Journal
of the Society for Industrial and Applied Mathemat-
ics: Series B, Numerical Analysis, 2(2):pp. 205–224.

Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. MIT Press, Cambridge, MA.

Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of
ACL’03, pages 423–430.

Tom Landauer and Sue Dumais. 1997. A solution to
plato’s problem: The latent semantic analysis the-
ory of acquisition, induction and representation of
knowledge. Psychological Review, 104.

Bing Liu. 2007. Web data mining. Springer.

Saif Mohammad and Peter D. Turney. 2013. Crowd-
sourcing a word-emotion association lexicon. Com-
putational Intelligence, 29(3):436–465.

Alessandro Moschitti, Daniele Pighin, and Robert
Basili. 2008. Tree kernels for semantic role label-
ing. Computational Linguistics, 34.

Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML, Berlin, Germany, September.

Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics, 33(2).

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1–135, January.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. In EMNLP, vol-
ume 10, pages 79–86, Stroudsburg, PA, USA. ACL.

Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval).

Magnus Sahlgren. 2006. The Word-Space Model.
Ph.D. thesis, Stockholm University.

John Shawe-Taylor and Nello Cristianini. 2004a. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press, New York, NY, USA.

John Shawe-Taylor and Nello Cristianini. 2004b. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.

Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P.
O’Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications. In
Proceedings of the 37th annual meeting of the
ACL on Computational Linguistics, pages 246–253,
Stroudsburg, PA, USA. ACL.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technologies Conference/Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP 2005), Vancouver, CA.

Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the
32Nd Annual Meeting of ACL, ACL ’94, pages 133–
138, Stroudsburg, PA, USA. ACL.

767


