































Improving Topic Quality by Promoting Named Entities in Topic Modeling


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 247–253
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

247

Improving Topic Quality by Promoting Named Entities in Topic Modeling

Katsiaryna Krasnashchok
EURA NOVA

Rue Emile Francqui, 4
1435 Mont-Saint-Guibert, Belgium

katherine.krasnoschok@euranova.eu

Salim Jouili
EURA NOVA

Rue Emile Francqui, 4
1435 Mont-Saint-Guibert, Belgium
salim.jouili@euranova.eu

Abstract

News-related content has been extensively
studied in both topic modeling research
and named entity recognition. However,
expressive power of named entities and
their potential for improving the quality of
discovered topics has not received much
attention. In this paper we use named en-
tities as domain-specific terms for news-
centric content and present a new weight-
ing model for Latent Dirichlet Allocation.
Our experimental results indicate that in-
volving more named entities in topic de-
scriptors positively influences the overall
quality of topics, improving their inter-
pretability, specificity and diversity.

1 Introduction

News-centric content conveys information about
events, individuals and other entities. Analysis of
news-related documents includes identifying hid-
den features for classifying them or summarizing
the content. Topic modeling is the standard tech-
nique for such purposes, and Latent Dirichlet Al-
location (LDA) (Blei et al., 2003) is the most used
algorithm, which models the documents as distri-
bution over topics and topics as distribution over
words. A good topic model is characterized by
its coherence: any coherent topic should contain
related words belonging to the same concept. A
good topic must also be distinctive enough to in-
clude domain-specific content. For news-related
texts domain-specific content can be represented
by named entities (NE), describing facts, events
and people involved in news and discussions. It
explains the need to include named entities in topic
modeling process.

The main contribution of this work is improving
topic quality with LDA by increasing the impor-

tance of named entities in the model. The idea is
to adapt the topic model to include more domain-
specific terms (NE) in the topic descriptors. We
designed our model to be flexible, in order to be
used in different variations of LDA. We ultimately
employ a term-weighting approach for the LDA
input. Our results show that: i) named entities
can serve as favorable candidates for high-quality
topic descriptors, and ii) weighting model based
on pseudo term frequencies is able to improve
overall topic quality without the need to interfere
with LDA’s generative process, which makes it
adaptable to other LDA variations.

The paper is organized in the following manner:
in Section 2 we present the related work; Section 3
describes the proposed solution and is followed by
Section 4, where the details of evaluation process
and results are outlined. We finish with Section 5,
concluding the results and next steps.

2 Related Work

This section describes the related work in the area
of topic modeling, specifically LDA.

2.1 Topic Modeling and Named Entities
Several works explored the relation between LDA
and named entities in recent years. The most fa-
mous model is CorrLDA2 (Newman et al., 2006).
It introduces two types of topics, general and en-
tity, and represents word topics as a mixture of
entity topics. Hu et al. (2013) reverses the con-
cept, assuming that entities are critical for news-
centric content. Their entity-centered topic model
(ECTM) designs entity topics as a mixture of word
topics and shows better results in entity prediction
than CorrLDA2 (Hu et al., 2013). Both models,
however, introduce significant changes to the LDA
algorithm. In this paper we strive to incorporate
named entities into LDA in a natural way, with-
out affecting the generative algorithm, to keep it



248

flexible and adaptable to any LDA variations.
Lau et al. (2013) study the impact of colloca-

tions on topic modeling and work with the in-
put of LDA by replacing unigrams with colloca-
tions. Adding multiword named entities, as a spe-
cial type of collocations, enhanced the topic model
for the tested dataset (Lau et al., 2013). Our work
follows similar tokenization process, but goes fur-
ther in improving the topic model by promoting
named entities in it.

2.2 Topic Modeling and Term Weighting

Traditionally, the input of LDA is a document-
term matrix of term frequencies (TF), according
to the bag-of-words model (BoW). However, Wil-
son and Chew (2010) showed that point-wise mu-
tual information (PMI) term weighting model can
be successfully applied to eliminate stop words
from topic descriptors. More weighting schemes
were evaluated by Truica et al. (2016) and showed
promising results for clustering accuracy. There-
fore, term weighting approach in LDA can be ben-
eficial for certain tasks. In this paper we intro-
duce unnormalized TF-based weighting scheme
using pseudo frequency as a way of increasing the
weight of a term.

3 Proposed model

LDA model has been criticized for favoring
highly frequent, general words in topic descriptors
(O’Callaghan et al., 2015). This problem can be
partly solved by eliminating domain-specific stop-
words from the corpus. On the other hand, instead
of narrowing the corpus, it may be more efficient
to promote domain-specific important words, es-
pecially if such words can be identified automat-
ically, like named entities. In this paper we deal
with the online Variational Bayes version of the
LDA algorithm from Hoffman et al. (2010), as
alternative to collapsed Gibbs sampling, used by
Wilson and Chew (2010) and Truica et al. (2016)
to incorporate weights into the LDA model. In
Hoffman et al. (2010) the authors demonstrate that
the objective of the optimization relies only on the
counts of terms in documents, and therefore docu-
ments can be summarized by their TF values. Our
proposed model takes the TF scores as initial term
weights (unnormalized). To increase the weight of
a named entity we add a pseudo-frequency to its
TF without changing the weights of other terms.
This strengthens the chances of NE to appear in

a topic descriptor, even if originally it was not
mentioned often in the corpus. There are multi-
ple ways of increasing the weights, e.g. we can
promote all NE in the same proportion, or set their
weights separately for each document in the cor-
pus.

3.1 Independent Named Entity Promoting

NE Independent model assumes that all named en-
tities in the corpus are α times more important
than their initial weights (TF), i.e. they may not
be the most important terms in the corpus, but
they should weigh α times more than they do now.
Therefore, for each column mw of document-term
matrix M , we apply scalar multiplication:

mw =

�
α ∗mw if w is NE
mw otherwise

(1)

By varying α, we can set the importance of
named entities in the corpus and impact the out-
come of topic modeling. The value need not be
an integer, since typical LDA implementation can
deal with any numbers. In Section 4 we provide
results for several tested values of α parameter and
discuss our findings.

3.2 Document Dependent Named Entity
Promoting

While we want the topics produced by LDA to
include more named entities as domain-specific
words, we may assume that NE, in fact, should be
the most important, i.e. the most frequent, terms
in each document. In order to set the weights ac-
cordingly, the maximum term-frequency per doc-
ument is calculated and added to each named en-
tity’s weight in each document:

mdw =

�
mdw +max

w
mdw if w is NE

mdw otherwise
(2)

This weighting scheme obliges named entities
to be the ”heaviest” terms in each document. At
the same time, we do not change the weight of
other frequent terms, so eventually they still have
a high probability to make the top terms list.

4 Evaluation

We designed a series of tests to evaluate our
proposed model: a) Baseline Unigram: basic
model on the corpus consisting of single tokens



249

(no named entities involved); b) Baseline NE: ba-
sic model on the corpus with named entities (the
strategy of injecting NE in all tests is replacement
instead of supplementation, as suggested by Lau
et al., 2013); c) NE Independent: independent
named entity promoting model described in Sec-
tion 3.1; and d) NE Document Dependent: doc-
ument dependent named entity promoting model
described in Section 3.2. We evaluate the tests us-
ing the topic quality measures presented below.

4.1 Dataset And Preprocessing

Our test corpora consists of news-related publicly-
available datasets: 1) 20 Newsgroups1: widely
studied by NLP research community dataset (Ale-
tras and Stevenson, 2013; Truica et al., 2016; Wal-
lach et al., 2009; Röder et al., 2015; Hu et al.,
2013). Contains 18846 documents with messages
discussing news, people, events and other entities.
2) Reuters-2013: a set of 14595 news articles from
Reuters for year 2013, obtained from Financial
News Dataset2, first compiled and used in (Ding
et al., 2014). The documents in Reuters-2013 are
generally longer than in 20 Newsgroups. For NE
recognition we used NeuroNER3, a tool designed
by Dernoncourt et al. (2016, 2017), trained on
CONLL2003 dataset and recognizing four types
of NE: person, location, organization and miscel-
laneous. The further preprocessing pipeline con-
sists of classic steps used in topic modeling.

4.2 Topic Coherence

The term ”topic coherence” covers a set of mea-
sures describing the quality of the topics regard-
ing interpretability by a human. Most widely
used measures are based on PMI (or NPMI, nor-
malized) and log conditional probability, both of
which rely on the co-occurrence of terms (Lau
et al., 2013, 2014; O’Callaghan et al., 2015; Ale-
tras and Stevenson, 2013; Newman et al., 2010;
Mimno et al., 2011; Nikolenko, 2016; Nguyen
et al., 2015; Syed and Spruit, 2017). Recently a
study by Röder et al. (2015) put all known co-
herence measures into single framework, assessed
their correlation with human ratings and discov-
ered the best performing measure - previously un-
known Cv, based on cosine similarity of word vec-
tors over a sliding window. We inferred the defini-

1http://qwone.com/∼jason/20Newsgroups/
2https://github.com/philipperemy/financial-news-dataset
3https://github.com/Franck-Dernoncourt/NeuroNER

tion from Röder et al. (2015):

Cv =
1
N

�
t=1...N

1
Nt

�
i=1...Nt

scos(�vNPMI(wi),�vNPMI(Wt))

(3)
where N is the number of topics, Wt is the set of
top Nt terms in topic t, the vectors are defined as:

�vNPMI(wi) =
�
NPMI(wi, wj)

�
j∈Wt

(4)

�vNPMI(Wt) =
��

wi∈Wt NPMI(wi, wj)
�
wj∈Wt

(5)

and the underlying measure is NPMI with proba-
bility Psw over a sliding window. Cv with sliding
window of 110 words (Röder et al., 2015) is the
coherence measure we use in this paper.

Majority of studies also use a reference cor-
pus like Wikipedia for calculating word frequen-
cies and co-occurrences (Aletras and Stevenson,
2013; O’Callaghan et al., 2015; Lau et al., 2014;
Röder et al., 2015; Yang et al., 2017). In our
case the need for reference corpus is particularly
significant, since we change natural frequencies
of named entities in the corpus, therefore coher-
ence will definitely decline if calculated on orig-
inal data. For the tests we have preprocessed the
dump of English Wikipedia from 2014/06/15 with
the same pipeline as used for the test corpora.

4.3 Generality Measures
Coherence measures tend to favor topics with gen-
eral highly frequent terms. As a result we end up
with well understandable but quite generic topics.
A good topic should also be specific enough to
distinguish documents (O’Callaghan et al., 2015).
Moreover, averaging the coherences of all topics
may produce very good coherence for a model
with many repeating words across topics. For cov-
ering these aspects of the topic quality we adopt
two other measures.

Exclusivity: Represents the degree of overlap
between topics, based on the appearance of terms
in multiple descriptors (O’Callaghan et al., 2015).
We define exclusivity as |Wu||W | , where |Wu| is the
number of unique terms and |W | is the total num-
ber of terms in topic descriptors.

Lift: Generally used for reranking the terms
in descriptors (Taddy, 2012; Sievert and Shirley,
2014), lift is employed here as a topic quality met-
ric. It is defined as βtibi , where βti is the weight
of word i in topic t and bi is the probability of



250

Topics Test 20 Newsgroups Reuters-2013
Cv Lift Excl. Cv Lift Excl.

20 Baseline Unigram 0,534 3,390 0,788 0,539 3,891 0,610
Baseline NE 0,503 3,273 0,767 0,559 4,059 0,598
NE Independent (x1,3) 0,494 3,394 0,755 0,551 4,209 0,563
NE Independent (x1,5) 0,527 3,464 0,770 0,552 4,308 0,618
NE Independent (x2) 0,525 3,756 0,797 0,548 4,449 0,640
NE Independent (x2,5) 0,539 3,779 0,765 0,550 4,661 0,635
NE Independent (x5) 0,543 5,071 0,898 0,517 5,701 0,708
NE Independent (x10) 0,486 6,416 0,950 0,511 6,560 0,773
NE Doc. Dependent 0,543 4,600 0,780 0,566 5,749 0,625

50 Baseline Unigram 0,492 2,882 0,511 0,514 3,977 0,427
Baseline NE 0,467 2,704 0,469 0,534 4,064 0,402
NE Independent (x1,3) 0,476 2,825 0,487 0,538 4,291 0,406
NE Independent (x1,5) 0,479 2,987 0,497 0,527 4,370 0,423
NE Independent (x2) 0,471 3,394 0,533 0,510 4,684 0,459
NE Independent (x2,5) 0,467 3,652 0,561 0,499 4,958 0,483
NE Independent (x5) 0,437 5,243 0,702 0,461 5,956 0,564
NE Independent (x10) 0,385 6,693 0,787 0,447 6,943 0,641
NE Doc. Dependent 0,512 4,951 0,624 0,532 5,452 0,421

100 Baseline Unigram 0,486 2,457 0,325 0,503 3,692 0,286
Baseline NE 0,478 2,248 0,282 0,525 3,775 0,253
NE Independent (x1,3) 0,473 2,391 0,300 0,527 4,041 0,286
NE Independent (x1,5) 0,467 2,499 0,315 0,520 4,126 0,295
NE Independent (x2) 0,463 2,737 0,332 0,508 4,505 0,329
NE Independent (x2,5) 0,453 3,108 0,374 0,491 4,705 0,339
NE Independent (x5) 0,416 5,079 0,537 0,455 5,840 0,432
NE Independent (x10) 0,394 6,622 0,614 0,444 6,747 0,498
NE Doc. Dependent 0,478 4,310 0,442 0,509 5,030 0,266

Table 1: Topic quality results on the corpora

word i in the reference corpus. The overall model
measure is the average of the log-lift of descriptor
terms and shows the degree of presence of non-
general words in topics.

4.4 Results

Table 1 depicts the results of running the exper-
iments4 with N = {20, 50, 100} topics and top
10 words used for the measures. Firstly, we can
observe one common outcome: NE Independent
(x10) model exhibited the best exclusivity and lift
values across all tests, which is logical since this
model enforced the biggest number of pseudo-
frequent words to be in topic descriptors. How-
ever, the same model also showed the lowest co-
herence in all experiments. This confirms the sec-
ondary status of lift and exclusivity: the full per-

4Tests were run with gensim: https://radimrehurek.com/
gensim/

formance of the model is decided by the combi-
nation of all three measures. From the table we
can see that for 20 Newsgroups, Baseline Unigram
model resulted in better coherence than Baseline
NE. Previously Lau et al. (2013) showed that
coherence (NPMI-based) is supposed to improve
with NE replacement model. However, the goal of
this work goes beyond just including named en-
tities into LDA. We want to demonstrate that our
weighting model increases the number of NE in
topic descriptors, which makes them more under-
standable and diverse. For these purposes we use
different coherence measure (Röder et al., 2015),
and include additional NE type - miscellaneous,
which was omitted in (Lau et al., 2013) though it
contains some potentially important named enti-
ties. Hence, at the moment we do not compare our
results with Lau et al. (2013). For each dataset we
chose the baseline for comparison depending on



251

Topic Baseline Unigram Cv Topic NE Doc. Dependent Cv
game, good, year, team, player, play,
think, get, time, like

0,507 game, ne espn, ne nhl, player, team,
ne steve, think, run, play, good

0,565

game, san, espn, chicago, lose, new,
won, day, york, road

0,488 ne nhl, ne brown, ne tor, ne cal,
ne flyers, team, ne det, ne rangers,
ne lindros, ne edmonton

0,584

year, ar, know, hockey, league, slave,
new, file, list, slip

0,291

space, launch, earth, mission, orbit,
satellite, moon, planet, solar, space-
craft

0,816 ne earth, ne saturn, ne pluto, ne jupiter,
ne nasa, ne venus, ne mars, ne galileo,
ne uranus, ne sun

0,902

gun, file, control, firearm, research,
crime, new, information, law, use

0,424 ne nra, ne united states, ne congress,
ne federal, ne code, ne gun control,
ne senate, ne section, ne constitution,
ne hci

0,530

Table 2: Comparison of Baseline Unigram and NE Doc. Dependent topics for 20 Newsgroups

coherence: Baseline Unigram for 20 Newsgroups,
and Baseline NE for Reuters-2013.

In the majority of cases NE Document Depen-
dent ended up being the optimal model for both
datasets: while it did not perform best in terms
of lift or exclusivity, it achieved the best or good
enough coherence values, better lift and better or
the same exclusivity as baseline models. The ex-
ceptions are 20 Newsgropus with 20 topics, where
NE Independent (x5) became the optimal model,
and Reuters-2013 with 100 topics, where NE In-
dependent (x1,3) performed the best for combina-
tion of all three measures. The only case where
baseline model achieved superior coherence is 20
Newsgroups with N = 100, but we note that NE
Document Dependent model came close in terms
of coherence while having much better lift and ex-
clusivity, therefore it can also be considered opti-
mal. In general, NE Independent model showed
improvement in coherence up to a certain value
of α (different in each case), followed by a de-
cline, reaching very low values for NE Indepen-
dent (x10). On the other hand, NE Document De-
pendent model does not introduce new parame-
ters into LDA and manages to achieve best perfor-
mance in the majority of settings, thus being more
stable and easy to use.

Table 2 demonstrates qualitative analysis on
the individual topics from 20 Newsgroups, gen-
erated by Baseline Unigram, and their semanti-
cally closest counterparts from NE Document De-
pendent model. As evident from the table, base-
line topics describe mostly abstract concepts of

”sport”, ”space” and ”gun control”. From NE
Document Dependent topics we get more specific
descriptors, resulting in better coherence (as well
as lift/exclusivity). It is worth particularly noting
the names of the organizations (in bold), crucial to
the corresponding topics, that, despite being uni-
grams, only appear in NE Document Dependent
model, because they are not met often enough in
the test corpus.

5 Conclusion

Presented results indicate that, firstly, our pro-
posed model is capable of improving topic qual-
ity by only modifying the TF scores in the in-
put of LDA in favor of named entities. This
makes it applicable to any LDA-based models
relying on the same input. Secondly, we have
shown that named entities are well suited to be
used as domain-specific terms and produce high-
quality topics in news-related texts. Next steps in
our research include experimenting with different
weights for different categories of named entities,
as well as adding new coherence measures, such as
word2vec-based one, used by O’Callaghan et al.
(2015).

Acknowledgments

The elaboration of this scientific paper was sup-
ported by the Ministry of Economy, Industry, Re-
search, Innovation, IT, Employment and Educa-
tion of the Region of Wallonia (Belgium), through
the funding of the industrial research project Jeri-
cho (convention no. 7717).



252

References
Nikolaos Aletras and Mark Stevenson. 2013. Evaluat-

ing topic coherence using distributional semantics.
In IWCS.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research, 3(Jan):993–1022.

Franck Dernoncourt, Ji Young Lee, and Peter
Szolovits. 2017. NeuroNER: an easy-to-use pro-
gram for named-entity recognition based on neural
networks. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing: System Demonstrations. Association for Com-
putational Linguistics.

Franck Dernoncourt, Ji Young Lee, Ozlem Uzuner, and
Peter Szolovits. 2016. De-identification of patient
notes with recurrent neural networks. Journal of
the American Medical Informatics Association, page
ocw156.

Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan.
2014. Using structured events to predict stock price
movement: An empirical investigation. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP). Associ-
ation for Computational Linguistics.

Matthew Hoffman, Francis R Bach, and David M Blei.
2010. Online learning for latent dirichlet allocation.
In advances in neural information processing sys-
tems, pages 856–864.

Linmei Hu, Juanzi Li, Zhihui Li, Chao Shao, and Zhix-
ing Li. 2013. Incorporating entities in news topic
modeling. In Communications in Computer and In-
formation Science, pages 139–150. Springer Berlin
Heidelberg.

Jey Han Lau, Timothy Baldwin, and David Newman.
2013. On collocations and topic models. ACM
Transactions on Speech and Language Processing,
10(3):1–14.

Jey Han Lau, David Newman, and Timothy Baldwin.
2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality.
In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics. Association for Computational Linguis-
tics.

David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the conference on empirical methods
in natural language processing, pages 262–272. As-
sociation for Computational Linguistics.

David Newman, Chaitanya Chemudugunta, and
Padhraic Smyth. 2006. Statistical entity-topic mod-
els. In Proceedings of the 12th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining - KDD06. ACM Press.

David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of topic
coherence. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 100–108. Association for Computa-
tional Linguistics.

Dat Quoc Nguyen, Kairit Sirts, and Mark Johnson.
2015. Improving topic coherence with latent fea-
ture word representations in map estimation for
topic modeling. In Proceedings of the Australasian
Language Technology Association Workshop 2015,
pages 116–121.

Sergey I. Nikolenko. 2016. Topic quality metrics based
on distributed word representations. In Proceedings
of the 39th International ACM SIGIR conference on
Research and Development in Information Retrieval
- SIGIR16. ACM Press.

Derek O’Callaghan, Derek Greene, Joe Carthy, and
Pádraig Cunningham. 2015. An analysis of the co-
herence of descriptors in topic modeling. Expert
Systems with Applications, 42(13):5645–5657.

Michael Röder, Andreas Both, and Alexander Hinneb-
urg. 2015. Exploring the space of topic coherence
measures. In Proceedings of the Eighth ACM Inter-
national Conference on Web Search and Data Min-
ing - WSDM15. ACM Press.

Carson Sievert and Kenneth Shirley. 2014. LDAvis: A
method for visualizing and interpreting topics. In
Proceedings of the Workshop on Interactive Lan-
guage Learning, Visualization, and Interfaces. As-
sociation for Computational Linguistics.

Shaheen Syed and Marco Spruit. 2017. Full-text or ab-
stract? examining topic coherence scores using la-
tent dirichlet allocation. In 2017 IEEE International
Conference on Data Science and Advanced Analyt-
ics (DSAA). IEEE.

Matt Taddy. 2012. On estimation and selection for
topic models. In Artificial Intelligence and Statis-
tics, pages 1184–1193.

Ciprian-Octavian Truica, Florin Radulescu, and
Alexandru Boicea. 2016. Comparing different
term weighting schemas for topic modeling. In
2016 18th International Symposium on Symbolic
and Numeric Algorithms for Scientific Computing
(SYNASC). IEEE.

Hanna M Wallach, David M Mimno, and Andrew Mc-
Callum. 2009. Rethinking lda: Why priors matter.
In Advances in neural information processing sys-
tems, pages 1973–1981.

Andrew T Wilson and Peter A Chew. 2010. Term
weighting schemes for latent dirichlet allocation. In
human language technologies: The 2010 annual
conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 465–
473. Association for Computational Linguistics.



253

Weiwei Yang, Jordan Boyd-Graber, and Philip Resnik.
2017. Adapting topic models using lexical associ-
ations with tree priors. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics.


