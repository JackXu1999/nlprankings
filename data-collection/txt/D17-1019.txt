



















































Neural Net Models of Open-domain Discourse Coherence


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 198–209
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Neural Net Models of Open-domain Discourse Coherence

Jiwei Li and Dan Jurafsky
Computer Science Department

Stanford University, Stanford, USA
jiweil,jurafsky@stanford.edu

Abstract

Discourse coherence is strongly associated
with text quality, making it important to nat-
ural language generation and understand-
ing. Yet existing models of coherence focus
on measuring individual aspects of coher-
ence (lexical overlap, rhetorical structure,
entity centering) in narrow domains.

In this paper, we describe domain-
independent neural models of discourse co-
herence that are capable of measuring mul-
tiple aspects of coherence in existing sen-
tences and can maintain coherence while
generating new sentences. We study both
discriminative models that learn to distin-
guish coherent from incoherent discourse,
and generative models that produce coher-
ent text, including a novel neural latent-
variable Markovian generative model that
captures the latent discourse dependencies
between sentences in a text.

Our work achieves state-of-the-art perfor-
mance on multiple coherence evaluations,
and marks an initial step in generating co-
herent texts given discourse contexts.

1 Introduction

Modeling discourse coherence (the way parts of
a text are linked into a coherent whole) is essen-
tial for summarization (Barzilay and McKeown,
2005), text planning (Hovy, 1988; Marcu, 1997)
question-answering (Verberne et al., 2007), and
even psychiatric diagnosis (Elvevåg et al., 2007;
Bedi et al., 2015).

Various frameworks exist, each tackling aspects
of coherence. Lexical cohesion (Halliday and
Hasan, 1976; Morris and Hirst, 1991) models
chains of words and synonyms. Psychological mod-
els of discourse (Foltz et al., 1998; Foltz, 2007;
McNamara et al., 2010) use LSA embeddings to
generalize lexical cohesion. Relational models like
RST (Mann and Thompson, 1988; Lascarides and

Asher, 1991) define relations that hierarchically
structure texts. The entity grid model (Barzilay
and Lapata, 2008) and its extensions1 capture the
referential coherence of entities moving in and out
of focus across a text. Each captures only a single
aspect of coherence, and all focus on scoring exist-
ing sentences, rather than on generating coherent
discourse for tasks like abstractive summarization.

Here we introduce two classes of neural models
for discourse coherence. Our discriminative mod-
els induce coherence by treating human generated
texts as coherent examples and texts with random
sentence replacements as negative examples, feed-
ing LSTM sentence embeddings of pairs of consec-
utive sentences to a classifier. These achieve state-
of-the-art (96% accuracy) on the standard domain-
specific sentence-pair-ordering dataset (Barzilay
and Lapata, 2008), but suffer in a larger open-
domain setting due to the small semantic space
that negative sampling is able to cover.

Our generative models are based on augument-
ing encoder-decoder models with latent variables
to model discourse relationships across sentences,
including (1) a model that incorporates an HMM-
LDA topic model into the generative model and
(2) an end-to-end model that introduces a Markov-
structured neural latent variable, inspired by re-
cent work on training latent-variable recurrent nets
(Bowman et al., 2015; Serban et al., 2016b). These
generative models obtain the best result on a large
open-domain setting, including on the difficult task
of reconstructing the order of every sentence in a
paragraph, and our latent variable generative model
significantly improves the coherence of text gener-
ated by the model.

Our work marks an initial step in building end-
to-end systems to evaluate open-domain discourse
coherence, and more importantly, generating coher-
ent texts given discourse contexts.

1Adding coreference (Elsner and Charniak, 2008), named
entities (Eisner and Charniak, 2011), discourse relations (Lin
et al., 2011) and entity graphs (Guinaudeau and Strube, 2013).

198



2 The Discriminative Model

The discriminative model treats cliques (sets of sen-
tences surrounding a center sentence) taken from
the original articles as coherent positive examples
and cliques with random replacements of the center
sentence as negative examples. The discriminative
model can be viewed as an extended version of Li
and Hovy’s (2014) model but is practical at large
scale2. We thus make this section succinct.

Notations Let C denote a sequence of coherent
texts taken from original articles generated by hu-
mans. C is comprised of a sequence of sentences
C = {sn−L, ..., sn−1, sn, sn+1, ..., sn+L} where L
denotes the half size of the context window. Sup-
pose each sentence sn consists of a sequence of
words wn1, ..., wnt, ..., wnM , where M is the num-
ber of tokens in sn. Each word w is associated with
a K dimensional vector hw and each sentence is
associated with a K dimensional vector xs.

Each C contains 2L + 1 sentences, and is as-
sociated with a (2L+ 1)×K dimensional vector
obtained by concatenating the representations of its
constituent sentences. The sentence representation
is obtained from LSTMs. After word compositions,
we use the representation output from the final time
step to represent the entire sentence. Another neu-
ral network model with a sigmoid function on the
very top layer is employed to map the concatena-
tion of representations of its constituent sentences
to a scalar, indicating the probability of the current
clique being a coherent one or an incoherent one.

Weakness Two problems with the discriminative
model stand out: First, it relies on negative sam-
pling to generate negative examples. Since the
sentence-level semantic space in the open-domain
setting is huge, the sampled instances can only
cover a tiny proportion of the possible negative
candidates, and therefore don’t cover the space of
possible meanings. As we will show in the experi-
ments section, the discriminative model performs
competitively in specific domains, but not in the
open domain setting. Secondly and more impor-
tantly, discriminative models are only able to tell
whether an already-given chunk of text is coherent
or not. While they can thus be used in tasks like
extractive summarization for sentence re-ordering,
they cannot be used for coherent text generation

2Li and Hovy’s (2014) recursive neural model operates on
parse trees, which does not support batched computation and
is therefore hard to scale up.

in tasks like dialogue generation or abstractive text
summarization.

3 The Generative Model

We therefore introduce three neural generative mod-
els of discourse coherence.

3.1 Model 1: the SEQ2SEQ Model and its
Variations

In a coherent context, a machine should be able
to guess the next utterance given the preceding
ones. A straightforward way to do that is to train
a SEQ2SEQ model to predict a sentence given its
contexts (Sutskever et al., 2014). Generating sen-
tences based on neighboring sentences resembles
skip-thought models (Kiros et al., 2015), which
build an encoder-decoder model by predicting to-
kens in neighboring sentences.

As shown in Figure 1a, given two consecutive
sentences [si, si+1], one can measure the coher-
ence by the likelihood of generating si+1 given its
preceding sentence si (denoted by uni). This likeli-
hood is scaled by the number of words in si+1 (de-
noted by Ni+1) to avoid favoring short sequences.

L(si, si+1) =
1

Ni+1
log p(si+1|si) (1)

The probability can be directly computed using a
pretrained SEQ2SEQ model (Sutskever et al., 2014)
or an attention-based model (Bahdanau et al., 2015;
Luong et al., 2015).

In a coherent context, a machine should not only
be able to guess the next utterance given the pre-
ceding ones, but also the preceding one given the
following ones. This gives rise to the coherence
model (denoted by bi) that measures the bidirec-
tional dependency between the two consecutive
sentences:

L(si, si+1) =
1
Ni

log pB(si|si+1)

+ log
1

Ni+1
pF (si+1|si)

(2)

We separately train two models: a forward model
pF (si+1|si) that predicts the next sentence based
on the previous one and a backward model
pB(si|si+1) that predicts the previous sentence
given the next sentence. pB(si|si+1) can be trained
in a way similar to pF (si+1|si) with sources and
targets swapped. It is worth noting that pB and pF
are separate models and do not share parameters.

199



One problem with the described uni and bi mod-
els is that sentences with higher language model
probability (e.g., sentences without rare words) also
tend to have higher conditional probability given
their preceding or succeeding sentences. We are in-
terested in measuring the informational gain from
the contexts rather than how fluent the current sen-
tence is. We thus propose eliminating the influence
of the language model, which yields the following
coherence score:

L(si, si+1)

=
1
Ni

[log pB(si|si+1)− log pL(si)]

+
1

Ni+1
[log pB(si+1|si)− log pL(si+1)]

(3)
where pL(s) is the language model probability for
generating sentence s. We train an LSTM language
model, which can be thought of as a SEQ2SEQ
model with an empty source. A closer look at
Eq. 3 shows that it is of the same form as the
mutual information between si+1 and si, namely
log[p(si+1, si)/p(si+1)p(si)].

Generation The scoring functions in Eqs. 1,
2, and 3 are discriminative, generating coherence
scores for an already-given chunk of text. Eqs. 2
and 3 can not be directly used for generation pur-
poses, since they requires the completion of si+1
before the score can be computed. A normal strat-
egy is to generate a big N-best list using Eq. 1 and
then rerank the N-best list using Eq. 2 or 3 (Li
et al., 2015a). The N-best list can be generated
using standard beam search, or other algorithmic
variations that promote diversity, coherence, etc.
(Shao et al., 2017).

Weakness (1) The SEQ2SEQ model generates
words sequentially based on an evolving hidden
vector, which is updated by combining the current
word representation with previously built hidden
vectors. The generation process is thus not exposed
to more global features of the discourse like topics.
As the hidden vector evolves, the influence from
contexts gradually diminishes, with language mod-
els quickly dominating. (2) By predicting a sen-
tence conditioning only on its left or right neighbor,
the model lacks the ability to handle the longer-
term discourse dependencies across the sentences
of a text.

To tackle these two issues, we need a model that
is able to constantly remind the decoder about the

Figure 1: Overview of the proposed generative models for
discourse coherence modeling.

global meaning that it should convey at each word-
generation step, a global meaning which can cap-
ture the state of the discourse across the sentences
of a text. We propose two models of this global
meaning, a pipelined approach based on HMM-
based topic models (Blei et al., 2003; Gruber et al.,
2007), and an end-to-end generative model with
variational latent variables.

3.2 HMM-LDA based Generative Models
(HMM-LDA-GM)

In Markov topic models the topic depends on the
previous topics in context (Ritter et al., 2010; Paul
and Girju, 2010; Wang et al., 2011; Gruber et al.,
2007; Paul, 2012). The topic for the current sen-
tence is drawn based on the topic of the preced-
ing sentence (or word) rather than on the global
document-level topic distribution in vanilla LDA.

Our first model is a pipelined one (the HMM-
LDA-GM in Fig. 1b), in which an HMM-LDA
model provides the SEQ2SEQ model with global
information for token generation, with two compo-
nents:

(1) Running HMM-LDA: we first run a
sentence-level HMM-LDA similar to Gruber et al.
(2007). Our implementation forces all words in a
sentence to be generated from the same topic, and
this topic is sampled from a distribution based on
the topic from previous sentence. Let tn denote
the distribution of topics for the current sentence,
where tn ∈ R1×T . We also associate each LDA

200



topic with a K dimensional vector, representing
the semantics embedded in this topic. The topic-
representation matrix is denoted by V ∈ RT×K ,
where T is the pre-specified number of topics in
LDA. V is learned in the word predicting process
when training encoder-decoder models.

(2) Training encoder-decoder models: For the
current sentence sn, given its topic distribution tn,
we first compute the topic representation zn for sn
using the weighted sum of LDA topic vectors:

zn = tn × V (4)
zn can be thought of as a discourse state vector that
stores the information the current sentence needs to
convey in the discourse, and is used to guide every
step of word generation in sn. We run the encoder-
decoder model, which subsequently predicts tokens
in sn given sn−1. This process is the same as the
vanilla version of SEQ2SEQ models, the only dif-
ference being that zn is incorporated into each step
of decoding for hidden vector updates:

p(sn|zn, sn−1) =
M∏
t=1

p(wt|ht−1, zn) (5)

V is updated along with parameters in the encoder-
decoder model.
zn influences each time step of decoding, and

thus addresses the problem that vanilla SEQ2SEQ
models gradually lose global information as the
hidden representations evolve. zn is computed
based on the topic distribution tn, which is ob-
tained from the HMM-LDA model, thus model-
ing the global Markov discourse dependency be-
tween sentences of the text.3 The model can be
adapted to the bi-directional setting, in which we
separately train two models to handle the forward
probability log p(tn|sn−1, ...) and the backward
one log p(tn|sn+1). The bi-directional (bi) strat-
egy described in Eq. 3 can also be incorporated to
remove the influence of language models.

Weakness Topic models (either vanilla or HMM
versions) focus on word co-occurrences at the
document-level and are thus very lexicon-based.
Furthermore, given the diversity of topics in a
dataset like Wikipedia but the small number of
topic clusters, the LDA model usually produces
very coarse-grained topics (politics, sports, history,

3This pipelined approach is closely related to recent work
that incorporates LDA topic information into generation mod-
els in an attempt to leverage context information (Ghosh et al.,
2016; Xing et al., 2016; Mei et al., 2016)

etc.), assigning very similar topic distributions to
consecutive sentences. These topics thus capture
topical coherence but are too coarse-grained to cap-
ture all the more fine-grained aspects of discourse
coherence relationships.

3.3 Variational Latent Variable Generative
Models (VLV-GM)

We therefore propose instead to train an end-to-end
system, in which the meaning transitions between
sentences can be naturally learned from the data.
Inspired by recent work on generating sentences
from a latent space (Serban et al., 2016b; Bowman
et al., 2015; Chung et al., 2015), we propose the
VSV-GM model in Fig. 1c. Each sentence sn is
again associated with a hidden vector representa-
tion zn ∈ RK which stores the global information
that the current sentence needs to talk about, but
instead of obtaining zn from an upstream model
like LDA, zn is learned from the training data. zn
is a stochastic latent variable conditioned on all
previous sentences and zn−1:

p(zn|zn−1, sn−1, sn−2, ...) = N(µtruezn ,Σtruezn )
µtruezn = f(zn−1, sn−1, sn−2, ...)
Σtruezn = g(zn−1, sn−1, sn−2, ...)

(6)
where N(µ,Σ) is a multivariate normal distribu-
tion with mean µ ∈ RK and covariance matrix
Σ ∈ RK×K . Σ is a diagonal matrix. As can be
seen, the global information zn for the current sen-
tence depends on the information zn−1 for its pre-
vious sentence as well as the text of the context
sentences. This forms a Markov chain across all
sentences. f and g are neural network models that
take previous sentences and zn−1, and map them
to a real-valued representation using hierarchical
LSTMs (Li et al., 2015b)4.

Each word wnt from sn is predicted using
the concatenation of the representation previously
build by the LSTMs (the same vector used in word
prediction in vanilla SEQ2SEQ models) and zn, as
shown in Eq.5.

We are interested in the posterior distribution
p(zn|s1, s2, ..., sn−1), namely, the information that
the current sentence needs to convey given the pre-
ceding ones. Unfortunately, a highly non-linear
mapping from zn to tokens in sn results in in-

4Sentences are first mapped to vector representations using
a LSTM model. Another level of LSTM at the sentence level
then composes representations of the multiple sentences to a
single vector.

201



tractable inference of the posterior. A common so-
lution is to use variational inference to learn another
distribution, denoted by q(zn|s1, s2, ..., sN ), to ap-
proximate the true posterior p(zn|s1, s2, ..., sn−1).
The model’s latent variables are obtained by max-
imizing the variational lower-bound of observing
the dataset:

log p(s1, .., sN ) ≤
N∑

t=1

−DKL(q(zn|sn, sn−1, ...)||p(zn|sn−1, sn−2, ...))

+ Eq(zn|sn,sn−1,...) log p(sn|zn, sn−1, sn−2, ...)
(7)

This objective to optimize consists of two parts;
the first is the KL divergence between the ap-
proximate distribution q and the true posterior
p(sn|zn, sn−1, sn−2, ...), in which we want to ap-
proximate the true posterior using q. The second
part Eq(zn|sn,sn−1,...) log p(sn|zn, sn−1, sn−2, ...),
predicts tokens in sn in the same way as in
SEQ2SEQ models with the difference that it con-
siders the global information zn.

The approximate posterior distribution
q(zn|sn, sn−1, ...) takes a form similar to
p(zn|sn−1, sn−2, ...):

q(zn|sn, sn−1, ...) = N(µ approxzn ,Σ approxzn )
µ approxzn = fq(zn−1, sn, sn−1, ...)
Σ approxzn = gq(zn−1, sn, sn−1, ...)

(8)

fq and gq are of similar structures to f and g, using
a hierarchical neural network model to map context
tokens to vector representations.

Learning and Testing At training time, the ap-
proximate posterior q(zn|zn−1, sn, sn−1, ...), the
true distribution p(zn|zn−1, sn−1, sn−2, ...), and
the generative probability p(sn|zn, sn−1, sn−2, ...)
are trained jointly by maximizing the variational
lower bound with respect to their parameters: a
sample zn is first drawn from the posterior dis-
tribution q, namely N(µ approxzn ,Σ

approx
zn ). This

sample is used to approximate the expectation
Eq log p(sn|zn, sn−1, sn−2, ...). Using zn, we can
update the encoder-decoder model using SGD in a
way similar to the standard SEQ2SEQ model, the
only difference being that the current token to pre-
dict not only depends on the LSTM output ht, but
also zn. Given the sampled zn, the KL-divergence
can be readily computed, and we update the model
using standard gradient decent (details shown in
the Appendix).

The proposed VLV-GM model can be adapted to
the bi-directional setting and the bi setting similarly
to the way LDA-based models are adapted.

The proposed model is closely related to many
recent attempts in training variational autoencoders
(VAE) (Kingma and Welling, 2013; Rezende et al.,
2014), variational or latent-variable recurrent nets
(Bowman et al., 2015; Chung et al., 2015; Ji et al.,
2016; Bayer and Osendorfer, 2014), hierarchical
latent variable encoder-decoder models (Serban
et al., 2016b,a).

4 Experimental Results

In this section, we describe experimental results.
We first evaluate the proposed models on discrimi-
native tasks such as sentence-pair ordering and full
paragraph ordering reconstruction. Then we look
at the task of coherent text generation.

Model Acci Earthq Aver
Discriminative Model 0.930 0.992 0.956

SEQ2SEQ (bi) 0.755 0.930 0.842
VLV-GM (bi) 0.770 0.931 0.851

Recursive 0.864 0.976 0.920
Entity Grid Model 0.904 0.872 0.888

HMM 0.822 0.938 0.880
HMM+Entity 0.842 0.911 0.876

HMM+Content 0.742 0.953 0.847
Graph 0.846 0.635 0.740

Foltz et al. (1998)-Glove 0.705 0.682 0.688
Foltz et al. (1998)-LDA 0.660 0.667 0.664

Table 1: Results from different coherence models. Results for
the Recursive model is reprinted from Li and Hovy (2014),
Entity Grid Model from Louis and Nenkova (2012), HMM,
HMM+Entity and HMM+Content from Louis and Nenkova
(2012), Graph from Guinaudeau and Strube (2013), and the
final two lexical models are recomputed using Glove and LDA
to replace the original LSA model of Foltz et al. (1998).

4.1 Sentence Ordering, Domain-specific Data
Dataset We first evaluate the proposed algo-
rithms on the task of predicting the correct ordering
of pairs of sentences predicated on the assumption
that an article is always more coherent than a ran-
dom permutation of its sentences (Barzilay and
Lapata, 2008). A detailed description of this com-
monly used dataset and training/testing are found
in the Appendix.

We report the performance of the following base-
lines widely used in the coherence literature.

(1) Entity Grid Model: The grid model presented
in Barzilay and Lapata (2008). Results are directly
taken from Barzilay and Lapata’s (2008) paper. We
also consider variations of entity grid models, such
as Louis and Nenkova (2012) which models the

202



cluster transition probability and the Graph Based
Approach which uses a graph to represent the entity
transitions needed for local coherence computation
(Guinaudeau and Strube, 2013).

(2) Li and Hovy (2014): A recursive neural
model computes sentence representations based
on parse trees. Negative sampling is used to con-
struct negative incoherent examples. Results are
from their papers.

(3) Foltz et al. (1998) computes the semantic
relatedness of two text units as the cosine similarity
between their LSA vectors. The coherence of a
discourse is the average of the cosine of adjacent
sentences. We used this intuition, but with more
modern embedding models: (1) 300-dimensional
Glove word vectors (Pennington et al., 2014), em-
beddings for a sentence computed by averaging the
embeddings of its words (2) Sentence representa-
tions obtained from LDA (Blei et al., 2003) with
300 topics, trained on the Wikipedia dataset. Re-
sults are reported in Table 2. The extended version
of the discriminative model described in this work
significantly outperforms the parse-tree based re-
cursive models presented in Li and Hovy (2014)
as well as all non-neural baselines. It achieves al-
most perfect accuracy on the earthquake dataset
and 93% on the accident dataset, marking a signif-
icant advancement in the benchmark. Generative
models (both vanilla SEQ2SEQ and the proposed
variational model) do not perform competitively
on this dataset. We conjecture that this is due to
the small size of the dataset, leading the generative
model to overfit.

4.2 Evaluating Ordering on Open-domain

Since the dataset presented in Barzilay and Lapata
(2008) is quite domain-specific, we propose test-
ing coherence with a much larger, open-domain
dataset: Wikipedia. We created a test set by ran-
domly selecting 984 paragraphs from Wikipedia
dump 2014, each paragraph consisting of at least 16
sentences. The training set is 30 million sentences
not overlapping with the test set.

4.2.1 Binary Permutation Classification
We adopt the same strategy as in Barzilay and La-
pata (2008), in which we generate pairs of sen-
tence permutations from the original Wikipedia
paragraphs. We follow the protocols described
in the subsection and each pair whose original
paragraph’s score is higher than its permutation is
treated as being correctly classified, else incorrectly

Model Accuracy
VLV-GM (MMI) 0.873

VLV-GM (bi) 0.860
VLV-GM (uni) 0.839

LDA-HMM-GM (MMI) 0.847
LDA-HMM-GM (bi) 0.837
LDA-HMM-GM (uni) 0.814

SEQ2SEQ (MMI) 0.840
SEQ2SEQ (bi) 0.821

SEQ2SEQ (uni) 0.803
Discriminative Model 0.715

Entity Grid Model 0.686
Foltz et al. (1998)-Glove 0.597
Foltz et al. (1998)-LDA 0.575

Table 2: Performance on the open-domain binary classification
dataset of 984 Wikipedia paragraphs.

classified. Models are evaluated using accuracy.
We implement the Entity Grid Model (Barzilay and
Lapata, 2008) using the Wikipedia training set as a
baseline, the detail of which is presented in the Ap-
pendix. Other baselines consist of the Glove and
LDA updates of the lexical coherence baselines
(Foltz et al., 1998).

Results Table 2 presents results on the binary
classification task. Contrary to the findings on the
domain specific dataset in the previous subsection,
the discriminative model does not yield compelling
results, performing only slightly better than the en-
tity grid model. We believe the poor performance is
due to the sentence-level negative sampling used by
the discriminative model. Due to the huge seman-
tic space in the open-domain setting, the sampled
instances can only cover a tiny proportion of the
possible negative candidates, and therefore don’t
cover the space of possible meanings. By contrast
the dataset in Barzilay and Lapata (2008) is very
domain-specific, and the semantic space is thus rel-
atively small. By treating all other sentences in the
document as negative, the discriminative strategy’s
negative samples form a much larger proportion of
the semantic space, leading to good performance.

Generative models perform significantly better
than all other baselines. Compared with the dataset
in Barzilay and Lapata (2008), overfitting is not an
issue here due to the great amount of training data.
In line with our expectation, the MMI model outper-
forms the bidirectional model, which in turn out-
performs the unidirectional model across all three
generative model settings. We thus only report
MMI results for experiments below. The VLV-GM
model outperforms that the LDA-HMM-GM model,
which is slightly better than the vanila SEQ2SEQ
models.

203



Model Accuracy
VLV-GM (MMI) 0.256
LDA-HMM-GM (MMI) 0.237
SEQ2SEQ (MMI) 0.226
Entity Grid Model 0.143
Foltz et al. (1998) (Glove) 0.084

Table 3: Performances of the proposed models on the open-
domain paragraph reconstruction dataset.

Model adver-1 adver-2 adver-3
VLV-GM (MMI) 0.174 0.120 0.054

LDA-HMM-GM (MMI) 0.130 0.104 0.043
SEQ2SEQ (MMI) 0.120 0.090 0.039

SEQ2SEQ (bi) 0.108 0.078 0.030
SEQ2SEQ (uni) 0.101 0.068 0.024

Table 4: Adversarial Success for different models.

4.2.2 Paragraph Reconstruction

The accuracy of our models on the binary task
of detecting the original sentence ordering is very
high, on both the prior small task and our large
open-domain version. We therefore believe it is
time for the community to move to a more difficult
task for measuring coherence.

We suggest the task of reconstructing an origi-
nal paragraph from a bag of constituent sentences,
which has been previously used in coherence eval-
uation (Lapata, 2003). More formally, given a set
of permuted sentences s1, s2, ..., sN (N the number
of sentences in the original document), our goal
is return the original (presumably most coherent)
ordering of s.

Because the discriminative model calculates the
coherence of a sentence given the known previous
and following sentences, it cannot be applied to this
task since we don’t know the surrounding context.
Hence, we only use the generative model. The
first sentence of a paragraph is given: for each
step, we compute the coherence score of placing
each remaining candidate sentence to the right of
the partially constructed document. We use beam
search with beam size 10. We use the Entity Grid
model as a baseline for both the settings.

Evaluating the absolute positions of sentences
would be too harsh, penalizing orderings that main-
tain relative position between sentences through
which local coherence can be manifested. We there-
fore use Kendall’s τ (Lapata, 2003, 2006), a metric
of rank correlation for evaluation. See the Ap-
pendix for details of Kendall’s τ computation. We
observe a pattern similar to the results on the bi-
nary classification task, where the VLV-GM model
performs the best.

4.3 Adversarial evaluation on Text
Generation Quality

Both the tasks above are discriminative ones. We
also want to evaluate different models’ ability to
generate coherent text chunks. The experiment is
set up as follow: each encoder-decoder model is
first given a set of context sentences (3 sentences).
The model then generates a succeeding sentence
using beam-search given the contexts. For the uni-
directional setting, we directly take the most prob-
able sequence and for the bi-directional and MMI,
we rerank the N-best list using the backward prob-
ability and language model probability.

We conduct experiments on multi-sentence gen-
eration, in which we repeat the generative process
described above for N times, where N=1,2,3. At
the end of each turn, the context is updated by
adding in the newly generated sequence, and this
sequence is used as the source input to the encoder-
decoder model for next sequence generation. For
example, when N is set to 2, given the three con-
text sentences context-a, context-b and context-c,
we first generate sen-d given the three context sen-
tences and then generate sen-e given the sen-d,
context-a, context-b and context-c.

For evaluation, standard word overlap metrics
such as BLEU or ROUGE are not suited for our
task, and we use adversarial evaluation Bowman
et al. (2015); Anjuli and Vinyals (2016). In ad-
versarial evaluation, we train a binary discrimi-
nant function to classify a sequence as machine
generated or human generated, in an attempt to
evaluate the model’s sentence generation capabil-
ity. The evaluator takes as input the concatenation
of the contexts and the generated sentences (i.e.,
context-a, context-b and context-c, sen-d , sen-e
in the example described above),5 and outputs a
scalar, indicating the probability of the current text
chunk being human-generated. Training/dev/test
sets are held-out sets from the one on which gener-
ative models are trained. They respectively contain
128,000/12,800/12,800 instances. Since discrimi-
native models cannot generate sentences, and thus
cannot be used for adversarial evaluation, they are
skipped in this section.

We report Adversarial Success (AdverSuc for
short), which is the fraction of instances in which
a model is capable of fooling the evaluator. Adver-

5The model uses a hierarchical neural structure that first
maps each sentence to a vector representation, with another
level of LSTM on top of the constituent sentences, producing
a single vector to represent the entire chunk of texts.

204



Figure 2: An overview of training the adversarial evaluator
using a hierarchical neural model. Green denotes input con-
texts. Red denotes a sentence from human-generated texts,
treated as a positive example. Purple denotes a sentence from
machine-decoded texts, treated as a negative example.

Suc is the difference between 1 and the accuracy
achieved by the evaluator. Higher values of Ad-
verSuc for a dialogue generation model are better.
AdverSuc-N denotes the adversarial accuracy value
on machine-generated texts with N turns.

Table 4 show AdverSuc numbers for different
models. As can be seen, the latent variable model
VLV-GM is able to generate chunk of texts that are
most indistinguishable from coherent texts from
humans. This is due to its ability to handle the
dependency between neighboring sentences. Per-
formance declines as the number of turns increases
due to the accumulation of errors and current mod-
els’ inability to model long-term sentence-level
dependency. All models perform poorly on the
adver-3 evaluation metric, with the best adversarial
success value being 0.081 (the trained evaluator is
able to distinguish between human-generated and
machine generated dialogues with greater than 90
percent accuracy for all models).

4.4 Qualitative Analysis
With the aim of guiding future investigations, we
also briefly explore our model qualitatively, exam-
ining the coherence scores assigned to some artifi-
cial miniature discourses that exhibit various kinds
of coherence.

Case 1: Lexical Coherence
Pinochet was arrested. His arrest was unexpected. 1.79
Pinochet was arrested. His death was unexpected. 0.84
Mary ate some apples. She likes apples. 2.03
Mary ate some apples. She likes pears. 0.27
Mary ate some apples. She likes Paris. -1.35

The examples suggest that the model handles
lexical coherence, correctly favoring the 1st over

the 2nd, and the 3rd over the 4th examples. Note
that the coherence score for the final example is
negative, which means conditioning on the first
sentence actually decreases the likelihood of gener-
ating the second one.

Case 2: Temporal Order
Washington was unanimously elected president in the first two
national elections. He oversaw the creation of a strong, well-
financed national government. 1.48
Washington oversaw the creation of a strong, well-financed
national government. He was unanimously elected president
in the first two national elections. 0.72

Case 3: Causal Relationship
Bret enjoys video games; therefore, he sometimes is late to
appointments. 0.69
Bret sometimes is late to appointments; therefore, he enjoys
video games. -0.07

Cases 2 and 3 suggest the model may, at least
in these simple cases, be capable of addressing the
much more complex task of dealing with tempo-
ral and causal relationships. Presumably this is
because the model is exposed in training to the gen-
eral preference of natural text for temporal order,
and even for the more subtle causal links.

Case 4: Centering/Referential Coherence
Mary ate some apples. She likes apples. 3.06
She ate some apples. Mary likes apples. 2.41

The model seems to deal with simple cases of
referential coherence.

Example3: 2.40
John went to his favorite music store to buy a piano.
He had frequented the store for many years.
He was excited that he could finally buy a piano.
He arrived just as the store was closing for the day.
Example4: 1.62
John went to his favorite music store to buy a piano.
It was a store John had frequented for many years
He was excited that he could finally buy a piano..
It was closing just as John arrived.

In these final examples from Miltsakaki and Ku-
kich (2004), the model successfully captures the
fact that the second text is less coherent due to
rough shifts. This suggests that the discourse em-
bedding space may be able to capture a representa-
tion of entity focus.

Of course all of these these qualitative evalu-
ations are only suggestive, and a deeper under-
standing of what the discourse embedding space
is capturing will likely require more sophisticated
visualizations.

205



5 Conclusion

We investigate the problem of open-domain dis-
course coherence, training discriminative models
that treating natural texts as coherent and permu-
tations as non-coherent, and Markov generative
models that can predict sentences given their neigh-
bors.

Our work shows that the traditional evaluation
metric (ordering pairs of sentences in small do-
mains) is completely solvable by our discrimina-
tive models, and we therefore suggest the com-
munity move to the harder task of open-domain
full-paragraph sentence ordering.

The proposed models also offer an initial step in
generating coherent texts given contexts, which has
the potential to benefit a wide range of generation
tasks in NLP. Our latent variable neural models, by
offering a new way to learn latent discourse-level
features of a text, also suggest new directions in
discourse representation that may bring benefits to
any discourse-aware NLP task.

Acknowledgements The authors thank Will
Monroe, Sida Wang, Kelvin Guu and the other
members of the Stanford NLP Group for helpful
discussions and comments. Jiwei Li is supported
by a Facebook Fellowship, which we gratefully ac-
knowledge. This work is also partially supported by
the NSF under award IIS-1514268, and the DARPA
Communicating with Computers (CwC) program
under ARO prime contract no. W911NF- 15-1-
0462. Any opinions, findings, and conclusions or
recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of DARPA, the NSF, or Facebook.

References
Ernst Althaus, Nikiforos Karamanis, and Alexander

Koller. 2004. Computing locally coherent dis-
courses. In Proceedings of ACL 2004.

Kannan Anjuli and Oriol Vinyals. 2016. Adversarial
evaluation of dialogue models. NIPS 2016 Work-
shop on Adversarial Training .

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. of the Inter-
national Conference on Learning Representations
(ICLR).

Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics 34(1):1–34.

Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL.
pages 113–120.

Regina Barzilay and Kathleen R McKeown. 2005. Sen-
tence fusion for multidocument news summariza-
tion. Computational Linguistics 31(3):297–328.

Justin Bayer and Christian Osendorfer. 2014. Learn-
ing stochastic recurrent networks. arXiv preprint
arXiv:1411.7610 .

Gillinder Bedi, Facundo Carrillo, Guillermo A Cec-
chi, Diego Fernández Slezak, Mariano Sigman,
Natália B Mota, Sidarta Ribeiro, Daniel C Javitt,
Mauro Copelli, and Cheryl M Corcoran. 2015. Au-
tomated analysis of free speech predicts psychosis
onset in high-risk youths. npj Schizophrenia 1.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research 3(Jan):993–1022.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2015. Generating sentences from a continuous
space. arXiv preprint arXiv:1511.06349 .

Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.
Using entity-based features to model coherence in
student essays. In Human language technologies:
The 2010 annual conference of the North American
chapter of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
pages 681–684.

Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth
Goel, Aaron C Courville, and Yoshua Bengio. 2015.
A recurrent latent variable model for sequential data.
In Advances in neural information processing sys-
tems. pages 2980–2988.

Carl Doersch. 2016. Tutorial on variational autoen-
coders. arXiv preprint arXiv:1606.05908 .

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research 12:2121–2159.

Micha Eisner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2. Associ-
ation for Computational Linguistics, pages 125–129.

Micha Elsner, Joseph L Austerweil, and Eugene Char-
niak. 2007. A unified local and global model for dis-
course coherence. In HLT-NAACL. pages 436–443.

Micha Elsner and Eugene Charniak. 2008.
Coreference-inspired coherence modeling. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics on Human

206



Language Technologies: Short Papers. Association
for Computational Linguistics, pages 41–44.

Brita Elvevåg, Peter W Foltz, Daniel R Weinberger,
and Terry E Goldberg. 2007. Quantifying incoher-
ence in speech: An automated methodology and
novel application to schizophrenia. Schizophrenia
research 93(1):304–316.

Vanessa Wei Feng and Graeme Hirst. 2012. Extend-
ing the entity-based coherence model with multiple
ranks. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, pages 315–324.

Peter W Foltz. 2007. Discourse coherence and lsa.
Handbook of latent semantic analysis pages 167–
184.

Peter W Foltz, Walter Kintsch, and Thomas K Lan-
dauer. 1998. The measurement of textual coherence
with latent semantic analysis. Discourse processes
25(2-3):285–307.

Shalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy,
Tom Dean, and Larry Heck. 2016. Contextual lstm
(clstm) models for large scale nlp tasks. arXiv
preprint arXiv:1602.06291 .

Amit Gruber, Yair Weiss, and Michal Rosen-Zvi. 2007.
Hidden topic markov models. In AISTATS. vol-
ume 2, pages 163–170.

Camille Guinaudeau and Michael Strube. 2013. Graph-
based local coherence modeling. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics. pages 93–103.

M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman.

Eduard H Hovy. 1988. Planning coherent multisenten-
tial text. In Proceedings of the 26th annual meeting
on Association for Computational Linguistics. As-
sociation for Computational Linguistics, pages 163–
169.

Yangfeng Ji, Gholamreza Haffari, and Jacob Eisenstein.
2016. A latent variable recurrent neural network for
discourse relation language models. arXiv preprint
arXiv:1603.01913 .

Diederik P Kingma and Max Welling. 2013. Auto-
encoding variational bayes. arXiv preprint
arXiv:1312.6114 .

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in Neural Information Processing Systems.
pages 3276–3284.

Mirella Lapata. 2003. Probabilistic text structuring:
Experiments with sentence ordering. In Proceed-
ings of the 41st Annual Meeting on Association
for Computational Linguistics-Volume 1. Associa-
tion for Computational Linguistics, pages 545–552.

Mirella Lapata. 2006. Automatic evaluation of infor-
mation ordering: Kendall’s tau. Computational Lin-
guistics 32(4):471–484.

Alex Lascarides and Nicholas Asher. 1991. Discourse
relations and defeasible knowledge. In Proceed-
ings of the 29th annual meeting on Association for
Computational Linguistics. Association for Compu-
tational Linguistics, pages 55–62.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2015a. A diversity-promoting objec-
tive function for neural conversation models. arXiv
preprint arXiv:1510.03055 .

Jiwei Li and Eduard Hovy. 2014. A model of coher-
ence based on distributed sentence representation.
In Proceedings of Empirical Methods in Natural
Language Processing.

Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015b.
A hierarchical neural autoencoder for paragraphs
and documents. arXiv preprint arXiv:1506.01057 .

Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1. Association for Computational Linguistics, pages
997–1006.

Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning. Association for Com-
putational Linguistics, pages 1157–1168.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. EMNLP .

William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text 8(3):243–281.

Daniel Marcu. 1997. From local to global coher-
ence: A bottom-up approach to text planning. In
AAAI/IAAI. Citeseer, pages 629–635.

Danielle S. McNamara, Max M. Louwerse, Philip M.
McCarthy, and Arthur C. Graesser. 2010. Coh-
metrix: Capturing linguistic features of cohesion.
Discourse Processes 47(4):292–330.

Hongyuan Mei, Mohit Bansal, and Matthew R Walter.
2016. Coherent dialogue with attention-based lan-
guage models. arXiv preprint arXiv:1611.06997 .

Eleni Miltsakaki and Karen Kukich. 2004. Evaluation
of text coherence for electronic essay scoring sys-
tems. Natural Language Engineering 10(01):25–55.

207



J. Morris and G. Hirst. 1991. Lexical cohesion
computed by thesaural relations as an indicator of
the structure of text. Computational Linguistics
17(1):21–48.

Michael Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering
multi-faceted topics. Urbana 51(61801):36.

Michael J Paul. 2012. Mixed membership markov
models for unsupervised conversation modeling. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning. Associ-
ation for Computational Linguistics, pages 94–104.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
1543.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. 2014. Stochastic backpropagation and
approximate inference in deep generative models.
arXiv preprint arXiv:1401.4082 .

Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics. Association for
Computational Linguistics, pages 172–180.

Iulian Vlad Serban, Tim Klinger, Gerald Tesauro,
Kartik Talamadupula, Bowen Zhou, Yoshua Ben-
gio, and Aaron Courville. 2016a. Multiresolu-
tion recurrent neural networks: An application
to dialogue response generation. arXiv preprint
arXiv:1606.00776 .

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville,
and Yoshua Bengio. 2016b. A hierarchical latent
variable encoder-decoder model for generating dia-
logues. arXiv preprint arXiv:1605.06069 .

Louis Shao, Stephan Gouws, Denny Britz, Anna
Goldie, Brian Strope, and Ray Kurzweil. 2017. Gen-
erating long and diverse responses with neural con-
versation models. arXiv preprint arXiv:1701.03185
.

Ilya Sutskever, Oriol Vinyals, and Quoc Le. 2014. Se-
quence to sequence learning with neural networks.
In Advances in neural information processing sys-
tems. pages 3104–3112.

Stephen Tratz and Eduard Hovy. 2011. A fast, accurate,
non-projective, semantically-enriched parser. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 1257–1268.

Suzan Verberne, Lou Boves, Nelleke Oostdijk, and
Peter-Arno Coppen. 2007. Evaluating discourse-
based answer extraction for why-question answer-
ing. In Proceedings of the 30th annual international

ACM SIGIR conference on Research and develop-
ment in information retrieval. ACM, pages 735–736.

Hongning Wang, Duo Zhang, and ChengXiang Zhai.
2011. Structural topic model for latent topical struc-
ture analysis. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1.
Association for Computational Linguistics, pages
1526–1535.

Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang,
Ming Zhou, and Wei-Ying Ma. 2016. Topic aug-
mented neural response generation with a joint atten-
tion mechanism. arXiv preprint arXiv:1606.08340 .

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701 .

6 Supplemental Material

Details for the domain specific dataset (Barzilay
and Lapata, 2008) The corpus consists of 200
articles each from two domains: NTSB airplane ac-
cident reports (V=4758, 10.6 sentences/document)
and AP earthquake reports (V=3287, 11.5 sen-
tences/document), split into training and testing.
For each document, pairs of permutations are gen-
erated6. Each pair contains the original document
order and a random permutation of the sentences
from the same document.

Training/Testing details for models on the do-
main specific dataset We use reduced versions
of both generative and discriminative models to
allow fair comparison with baselines. For the dis-
criminative model, we generate noise negative ex-
amples from random replacements in the training
set, with the only difference that random replace-
ments only come from the same document. We
use 300 dimensional embeddings borrowed from
GLOVE (Pennington et al., 2014) to initialize word
embeddings. Word embeddings are kept fixed dur-
ing training and we update LSTM parameters using
AdaGrad (Duchi et al., 2011). For the generative
model, due to the small size of the dataset, we train
a one layer SEQ2SEQ model with word dimension-
ality and number of hidden neurons set to 100. The
model is trained using SGD with AdaGrad (Zeiler,
2012).

The task requires a coherence score for the whole
document, which is comprised of multiple cliques.
We adopt the strategy described in Li and Hovy
(2014) by breaking the document into a series
of cliques which is comprised of a sequence of

6Permutations downloaded from people.csail.mit.
edu/regina/coherence/CLsubmission/.

208



consecutive sentences. The document-level coher-
ence score is attained by averaging its constituent
cliques. We say a document is more coherent if
it achieves a higher average score within its con-
stituent cliques.

Implementation of Entity Grid Model For
each noun in a sentence, we extract its syntactic
role (subject, object or other). We use a wikipedia
dump parsed using the Fanse Parser (Tratz and
Hovy, 2011). Subjects and objects are extracted
based on nsubj and dobj relations in the depen-
dency trees. (Barzilay and Lapata, 2008) define
two versions of the Entity Grid Model, one us-
ing full coreference and a simpler method using
only exact-string coreference; Due to the difficulty
of running full coreference resolution tens of mil-
lions of Wikipedia sentences, we follow other re-
searchers in using Barzilay and Lapata’s simpler
method (Feng and Hirst, 2012; Burstein et al., 2010;
Barzilay and Lapata, 2008).7

Kendall’s τ Kendall’s τ is computed based on
the number of inversions in the rankings as follows:

τ = 1− 2# of inversions
N × (N − 1) (9)

where N denotes the number of sentences in the
original document and inversions denote the num-
ber of interchanges of consecutive elements needed
to reconstruct the original document. Kendall’s τ
can be efficiently computed by counting the num-
ber of intersections of lines when aligning the orig-
inal document and the generated document. We
refer the readers to Lapata (2003) for more details.

Derivation for Variation Inference For sim-
plicity, we use µpost and Σapprox to denote
µ approx(zn) and Σ approx(zn), µtrue and Σtrue
to denote µtrue(zn) and Σtrue(zn). The KL-
divergence between the approximate distribution
q(zn|zn−1, sn, sn−1, ...) and the true distribution
p(zn|zn−1, sn−1, sn−2, ...) in the variational infer-
ence is given by:

DKL(q(zn|zn−1, sn, sn−1, ...)||p(zn|zn−1, sn−1, sn−2, ...)
=

1
2

(tr(Σ−1trueΣapprox)− k + log
detΣtrue

detΣapprox
+(µtrue − µapprox)−1Σ−1true(µtrue − µapprox))

(10)
7Our implementation of the Entity Grid Model is built

upon public available code at https://github.com/
karins/CoherenceFramework.

where k denotes the dimensionality of the vector.
Since zn has already been sampled and thus known,
µapprox, Σapprox, µtrue, Σtrue and consequently
Eq10 can be readily computed. The gradient with
respect to µapprox, Σapprox, µtrue, Σtrue can be
respectively computed, and the error is then back-
propagated to the hierarchical neural models that
are used to compute them. We refer the readers
to Doersch (2016) for more details about how a
general VAE model can be trained.

Our generate models offer a powerful way to rep-
resent the latent discourse structure in a complex
embedding space, but one that is hard to visualize.
To help understand what the model is doing, we
examine some relevant examples, annotated with
the (log-likelihood) coherence score from the MMI
generative model, with the goal of seeing (qualita-
tively) the kinds of coherence the model seems to
be representing. (The MMI can be viewed as the in-
formational gain from conditioning the generation
of the current sentence on its neighbors.)

209


