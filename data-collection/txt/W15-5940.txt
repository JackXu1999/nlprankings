



















































Proceedings of the...


D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 268–275,
Trivandrum, India. December 2015. c©2015 NLP Association of India (NLPAI)

Using Skipgrams, Bigrams, and Part of Speech Features for Sentiment
Classification of Twitter Messages

Badr Mohammed Badr
Dept. of Comp Sci & Engg

University College of Engineering
Osmania University, Hyderabad
badr.md87@gmail.com

S. Sameen Fatima
Dept. of Comp Sci & Engg

University College of Engineering
Osmania University, Hyderabad
sameenf@osmania.ac.in

Abstract

In this paper, we consider the problem of
sentiment classification of English Twit-
ter messages using machine learning tech-
niques. We systematically evaluate the
use of different feature types on the per-
formance of two text classification meth-
ods: Naive Bayes (NB) and Support Vec-
tor Machines (SVM). Our goal is three-
fold: (1) to investigate whether or not part-
of-speech (POS) features are useful for
this task, (2) to study the effectiveness of
sparse phrasal features (bigrams and skip-
grams) to capture sentiment information,
and (3) to investigate the impact of com-
bining unigrams with phrasal features on
the classification’s performance. For this
purpose we conducted a series of classifi-
cation experiments. Our results show that
POS features are useful for this task while
phrasal features could improve the per-
formance of the classification only when
combined with unigrams.

1 Introduction

Due to the rapid growth of online opinion-rich re-
sources with the advent of Web 2.0, a new area of
text mining has emerged: sentiment analysis and
opinion mining. The research of sentiment anal-
ysis is concerned with developing computational
techniques that extract, categorize, and summarize
subjective information (such as attitudes and senti-
ments) in textual resources. One fundamental task
of sentiment analysis is to analyze a text fragment
and detect the author’s attitude with respect to a
topic (e.g., determining whether a movie review is
positive or negative towards the movie).

Twitter is an online, highly social microblog-
ging service that enables subscribers to commu-
nicate with short text messages called “tweets”.

Millions of people around the globe use Twit-
ter on daily basis to publicly express their opin-
ions regarding various aspects of their everyday
lives. Thus, Twitter provides a massive data source
which public and private organizations can harness
to monitor the opinions of the crowd towards al-
most anything. Mining sentiments and opinions
expressed in Twitter can provide indispensable in-
formation for various applications such as brand
monitoring, customer management, and political
forecasting.

Nevertheless, Twitter messages possess unique
linguistic characteristics that make them distin-
guished from conventional user-generated content
on the web (e.g., movie reviews). Tweets are
short (maximum length is 140-character), cover a
wide spectrum of topics, and written in an infor-
mal conversational style. The majority of tweets
are poorly written and contain many non-standard
lexical units such as emoticons (e.g., * *), emojis
(e.g., <3), neologisms (e.g., gr8), hashtags (e.g.,
#thingsilike), and acronyms (e.g., lol). In addition,
most subscribers write their tweets using mobile
devices, which increases the frequency of unin-
tentional spelling mistakes. Petz et al. (2013) per-
formed an empirical analysis on text samples col-
lected from various social media, and they found
misspellings ratio to be the highest in Twitter.

Because of the aforementioned reasons, senti-
ment analysis on Twitter text is considered much
more challenging problem than other domains.
The lexical variations of Twitter language affect
the vector representation of the corpus. Saif et
al. (2012) argued that data sparsity is the major
challenge faced when dealing with Twitter text.
They showed that Twitter text has much more low-
frequency terms than movie reviews. This can be
justified by the topic diversity and the large num-
ber of irregular words that occur in tweets. To
get a closer look at this problem, we conducted
a comparison between the Stanford Twitter Sen-268



timent (STS) dataset1 and movie reviews dataset2

in terms of vocabulary growth. Figure 1 shows the
vocabulary growth rate of the two datasets. From
Figure 1, it can be observed that the vocabulary of
Twitter grows at a higher rate than movie reviews
as the corpus size gets larger (measured by the
total number of words). We manually inspected
the low-frequency words in the Twitter vocbulary.
Most of these words were informal abbreviations
(e.g., 4eva), elongated words (e.g., beeeeer), and
hashtag phrases (e.g., #mayblowyourmind).

0 300k 600k 900k 1200k 1500k

10k

20k

30k

40k

50k

60k

70k

Vo
ca

bu
la

ry
 si

ze

 Corpus size (by number of words)

tweets

movie reviews

Figure 1: A comparison between the STS corpus
and a movie reviews corpus in terms of vocabulary
growth. Corpus size (x-axis) is plotted against the
vocabulary size (y-axis).

In this paper, we address the problem of
sentiment analysis of English tweets using ma-
chine learning-based classification techniques. We
tackle this problem as a binary classification task
where we have two classes: positive vs. negative.

2 Related Work

Using machine learning techniques to tackle the
task of Twitter sentiment analysis has been the
dominant technique in the literature (Go et al.,
2009; Bifet and Frank, 2010; Pak and Paroubek,
2010; Kouloumpis et al., 2011; Asiaee T et al.,
2012; Saif et al., 2012; Nakov et al., 2013). The
pioneer work in this direction is Go et al. (2009)
who used emoticons as noisy signals to obtain
annotated training data. They experimented with
SVM, Naive Bayes, and Maximum Entropy clas-
sifiers, and used unigrams, bigrams, and POS tags
as features to build two-class sentiment classi-
fiers. Their best classifier was Maximum Entropy

1http://www.sentiment140.com
2http://ai.stanford.edu/ amaas/data/sentiment

trained on a combination of unigrams and bigrams
with an accuracy of 83.0%. Their results give
an indication that combining different levels of n-
gram features has a positive impact on the classi-
fiers performance.

The impact of using POS features for Twitter
sentiment classification was hotly debated in the
literature. While Go et al. (2009) and Kouloumpis
et al. (2011) concluded that POS features are not
useful at all for Twitter sentiment classification,
Pak and Paroubek (2010) showed that the distri-
bution of POS tags is not uniform among different
sentiment classes, which suggests that they can be
used as discriminating features to train sentiment
classifiers. In the latter approach, POS tags were
used in conjunction with n-gram features to train
a Naive Bayes classifier. However, they did not
investigate how much improvement was gained
when POS features were added to unigrams.

Fernández et al. (2014) experimented with dif-
ferent levels of skipgrams extracted from tweets
to train an SVM sentiment classifier. Their re-
sults show that skipgram features outperform con-
ventional n-gram features. Nevertheless, no other
experiments were conducted to evaluate the effec-
tiveness of skipgrams on other learning algorithms
nor the imapct of combining skipgrams with uni-
grams on the classification performance.

3 Approach

Our approach is to extract different types of fea-
tures from tweets and build sentiment classi-
fiers using two supervised machine learning algo-
rithms: Naive Bayes (NB) and Support Vector ma-
chines (SVM). We investigate the effectiveness of
four different sets of features: unigrams, unigrams
with part of speech tags, bigrams, and skipgrams.
First, each set of features is used as a standalone
feature set to train the two classifiers. Then, dif-
ferent feature sets are combined in order to study
the impact of combining sprase feature sets on the
classifiers’ performance.

3.1 Twitter Sentiment Data

For the research conducted in this paper, we used
the STS corpus which was created by Go et al.
(2009). The STS training data was collected using
the Twitter API and consists of a balanced set of
1.6 million tweets (800,000 tweets in each senti-
ment category). Tweets in the training set were au-
tomatically labeled using emoticons as noisy sig-269



nals to detect the author’s sentiment. For example,
a tweet is considered positive if it contains any of
the emoticons :), :-), :D, or =), and consid-
ered negative if it contains any of the emoticons
:( or :-(. To filter out tweets of mixed sen-
timents, the authors removed tweets that contain
both positive and negative emoticons. However,
using emoticons as labels could produce noisy in-
stances because the assumption of relating emoti-
cons to the author’s mood may not hold in all sit-
uations. Therefore, the work in this paper fol-
lows the Distant Supervision paradigm (Mintz et
al., 2009).

A subset of the original STS training data was
used to build the sentiment classifiers in this pa-
per. We have randomly sampled a balanced set
of 200,000 training instances from the original
corpus. To evaluate the classifiers, the STS test
dataset was used. The test data contains 182 posi-
tive and 177 negative tweets which were collected
by querying the Twitter API with specific key-
words including brands, products, and celebrities.
Contrary to the training data, tweets in the test data
were manually labeled by human annotators.

3.2 Linguistic Preprocessing
Twitter has its own conventions which makes
tweets distinct from other online textual data. In
addition, the raw text of tweets is noisy and usually
contains non-standard lexical tokens (e.g., emoti-
cons, acronyms, hashtags, etc.). Thus, tweets have
to be processed using Twitter-specific tools fol-
lowed by several steps of text normalization.

3.2.1 Tokenization and POS Tagging
In this step, we used CMU TweetNLP3 to tokenize
the tweets and produce the part-of-speech tags.
TweetNLP is a Java-based microblogging-specific
tool that was developed by Owoputi et al. (2013)
to handle online conversational text such as Twit-
ter messages. This POS tagger uses its own tag set
which was crafted by Gimpel et al. (2011) for this
kind of noisy and informal textual data. Figure 2
shows an example of a tweet that has been tagged
with this tool.

3.2.2 Token Normalization
To reduce the number of word types, or the vocab-
ulary size of the Twitter corpus, many preprocess-
ing steps are necessary to normalize Twitter text.
We use the same normalization steps as proposed

3http://www.ark.cs.cmu.edu/TweetNLP

Figure 2: An example of a tweet that has been
tagged by TweetNLP. The original tweet is “ikr
smh he asked fir yo last name so he can add u on
fb lololol”. This tweet can be formally writen as
“He asked for your last name so he can add you
on Facebook”. Figure taken from (Owoputi et al.,
2013).

by Saif et al. (2012). These steps include the fol-
lowing:

• Case-folding: all tokens are first converted to
lowercase letters.

• Username replacement: all user mentions are
replaced by the token MENTION.

• Web links replacement: all URLs are re-
placed by the token URL.

• Punctuation replacement: all punctuation
marks are replaced by the token PUNCT.

• Hashtags: the hash symbol (#) is removed
from hashtags and they are treated as regular
words afterwards.

• Digits: all digit characters are replaced by the
token DIGIT.

• Word compression: any sequence of repeated
letters is reduced to two letter. For exam-
ple, “coool” and “cooool” are compressed as
a single token “cool”.

Table 1 summarizes the effect of the normalization
steps on the vocabulary size. It can be observed
that the number of word types has been reduced
by 58.5% due to token normalization.

3.3 Feature Extraction
For the purpose of this research, four different
types of features were extracted from the prepro-
cessed tweets. These features are: Unigrams (in-
dividual word tokens separated by a whitespace
or a punctuation mark), Unigrams–POS (unigrams
with their POS appended to the token), Bigrams
(pairs of adjacent word tokens), and Skipgrams270



Normalization step Vocab size Reduction(%)

No normalization 196,630 0%
Case-folding 169,362 13.7%
Usernames 106,020 37.4%
URLs 97,088 5.5%
Punctuations 96,157 0.5%
Digits 85,961 10.5%
Hashtags 85,022 0.6%
Word compression 81,454 4.1%

Total Reduction 58.5%

Table 1: Effects of normalization steps on the vo-
cabulary size.

(pairs of word tokens that are not necessarily ad-
jacent). For example, consider the following pre-
processed tweet:

“ i love my new smartphone ”

The extracted features are shown in Table 2. Each
extracted feature in the table is enclosed within
square brackets “[ ]”.

It is worth mentioning that the skipgram fea-
tures extracted in this step are actually k-skip-
bigrams, where k is the length of the tweet. In
other words, we don’t use a fixed window length
to generate skipgram pairs, but the window length
is allowed to expand for the entire text of the tweet.
Using this approach to generate skipgram features
will increase the size of the feature space by a con-
siderable factor. In order to reduce the number of

Feature set Extracted features

Unigrams [i] [love] [my] [new]
[smartphone]

Unigrams–POS [i–O] [love–V] [my–D]
[new–A] [smartphone–N]

Bigrams
[i–love] [love–my]

[my–new]
[new–smartphone]

Skipgrams

[i–love] [i–my] [i–new]
[i–smartphone] [love–my]

[love–new]
[love–smartphone]

[my–new] [my–smartphone]
[new–smartphone]

Table 2: The extracted features from the sample
tweet: “i love my new smartphone”.

the skipgram features, we used the following pro-
cedure: (1) Tokens tagged with parts of speech
G (foreign words or abbreviations), $ (digits and
numbers), & (coordinating conjunction), U (URLs
and emails), and P (pre- or post- postions) are re-
moved from the set of candidate words for skip-
grams generation. (2) We alphabetically arranged
the words in each candidate pair. Thus, the fea-
tures [i–love] and [love–i] are both represented as
[i–love] regardless of the order in which each to-
ken appeared in the tweet. (3) Words that occur
only once in the corpus (a.k.a. hapaxes) are ex-
cluded when generating features. Using this pro-
cedure allowed us to reduce the feature space sub-
stantially. Table 3 shows some summary statistics
of the extracted features for each feature set.

Feature set # Features Hapaxes (%)

Unigrams 81,454 76.84%

Unigrams–POS 97,084 78.23%

Bigrams 656,440 87.57%

Skipgrams 3,084,656 92.04%

Table 3: Summary statistics for the extracted fea-
tures.

3.4 Classification Methods

In this paper, we use machine learning techniques
to build predictive models that can classify a tweet
as positive or negative based on the sentiment ex-
pressed in the tweet. In other words, we tackle
this problem as a binary text classification prob-
lem. We experimented with two learning algo-
rithms that have been widely used for text clas-
sification tasks: Naive Bayes (NB) and Support
Vector Machines (SVM).

In order to implement the aforementioned clas-
sification methods on our Twitter data sample, we
adapted the same bag-of-features approach as in
Pang et al. (2002) to convert extracted features
from each tweet into a feature vector. We con-
sider each tweet as a text document. Then, we ex-
tract a set of m features {f1, . . . , fm} from the en-
tire Twitter corpus as shown in section 3.3. These
features might include, for example, the unigram
[good], the bigram [really–good], or the skipgram
[my–good]. Finally, each tweet is represented as
a feature vector t = (n1(t), . . . , ni(t) . . . , nm(t)),
where ni(t) as the number of times a feature fi
occurs in a tweet t.271



3.4.1 Multinomial Naive Bayes
Naive Bayes classification is a probabilistic classi-
fication method which has been formulated based
on Bayes theory. In this work, we used a multino-
mial Naive Bayes (MNB) model for text classifi-
cation (Manning et al., 2008). To classify a tweet
t, a MNB classifier assigns a class c∗ as

c∗ = argmaxcPNB(c|t)
PNB(c|t) = P (c)·P (t|c)P (t)

The probability P (t) is fixed for each class and has
no role in estimating c∗. Therefore, by applying
the conditional independence of features assump-
tion, PNB(c|t) can be estimated as

PNB(c|t) = P (c)(
∏m

i=1 P (fi|c)ni(t))
The probability P (fi|c) represents the relative fre-
quency of the feature fi in tweets belonging to
class c and estimated through maximum likeli-
hood. To deal with unknown features, the Laplace
(add-one) smoothing technique is used to avoid
zero-valued probabilities. Thus, P (fi|c) is esti-
mated as

P (fi|c) = count(fi,c)+1∑
f∈V count(f,c)+m

where m represents the total number of features of
a feature set.

3.4.2 Support Vector Machines
Joachims (1998) has shown that Support Vector
Machines (SVM) is a highly effective classifica-
tion technique for sparse text data. The motiva-
tion for using SVM for text classification problems
originates from the capability of this algorithm to
effectively handle sparse and highly dimensional
data representations. Given a binary classification
problem and a set of training examples, the aim of
the training phase is to construct a hyperplane, de-
noted as a vector ~w, that represents the largest pos-
sible separator between each class of the training
examples. Thus, SVM classifiers are usually refer-
eed to as maximum margin classifiers. When this
largest margin separator is learned, deciding the
class of unknown text documents is simply pre-
dicting which side of the hyperplane they might
fall on.

The mathematical details of the SVM algorithm
is beyond the scope of this paper. In our research,
we used the LIBLINEAR package which includes
a linear implementation of the SVM algorithm de-
veloped by Fan et al. (2008).

4 Evaluation

4.1 Experimental Set-up
We used a sample of the STS dataset described in
section 3.1 as training data. We sampled 100,000
positive and 100,000 negative tweets from the
original corpus. The STS testing dataset was used
to evaluate the classifiers. Then, the same lin-
guistic preprocessing and feature extraction steps
were applied to both training and testing datasets.
First, token normalization steps were applied to
the output of TweetNLP tool using regular expres-
sions. Second, we used a Python script to extract
each of the feature sets described in section 3.3
from the preprocessed tweets. Features that oc-
cur only once were removed form the final fea-
ture sets. For classification, we used the multi-
nomial Naive Bayes algorithm in-built in WEKA4,
the open source data mining software developed
by Hall et al. (2009). The LIBLINEAR pack-
age was used for SVM with L2-regularized L2-
loss and the cost parameter c was tuned to get the
best performance of the SVM. We found that set-
ting c = 0.01 works best for moderate number
of features (Unigrams and Unigrams–POS), and
c = 0.005 works best for large number of features
(Bigrams, Skipgrams, and combinations of feature
sets).

4.2 Experimental Results
In this section, we present the classification re-
sults, both from using each feature set in isolation
and combining unigram features with phrasal fea-
tures.

4.2.1 Individual Feature Sets
First, each set of features described in section 3.3
is used as a standalone feature set to train the clas-
sifiers. Table 4 shows the performance of each
classifier as measured by accuracy.

From the results summarized in Table 4, it can
be observed that SVM outperforms MNB when
unigram features (both Unigrams and Unigrams–
POS) are used to train the the classifiers. How-
ever, MNB outperforms SVM when phrasal fea-
tures (both Bigrams and Skipgrams) are used. For
Bigrams, and compared to the unigrams base-
line, MNB accuracy has improved from 74.7% to
76.0% while SVM accuracy declined from 79.9%
to 76.0%. The same can be observed for skipgram
features, where MNB accuracy improved to 78.8%

4http://www.cs.waikato.ac.nz/ml/weka272



Feature Set MNB SVM

Unigrams 74.7% 79.9%

Unigrams–POS 76.0% 80.5%

Bigrams 77.1% 76.0%

Skipgrams 78.8% 74.7%

Table 4: Classifications’ accuracy for each set
of features individually. Bold-faced accuracy
scores denote the top-performing classifier for
each learning algorithm.

while SVM declined to 74.7%. These results are
comparable to those of Go et al. (2009) who ob-
served similar behavior of their MNB and SVM
classifier for unigram and bigram features. There-
fore, one can conclude that the performance of
SVM degrades as the sparsity degree of the feature
representation increases, given that bigrams are
sparser than unigrams, and skipgrams are sparser
than bigrams. On the other hand, the sparsity de-
gree does not negatively affect the performance of
MNB.

Moreover, our results show that POS tags im-
prove classification performance when they are at-
tached to the unigrams. Compared to the uni-
grams baseline, accuracy of MNB improved from
74.7% to 76.0% while SVM accuracy slightly
improved from 79.9% to 80.5%. These results
contradict with the findings of Go et al. (2009)
and Kouloumpis et al. (2011) who concluded that
POS features are not useful for Twitter senti-
ment analysis. In both works, a general-purpose
POS tagger was used to produce the POS tags.
Therefore, our results provide evidence that us-
ing microblogging-specific POS tagger to produce
POS features provide informative features for ma-
chine learning-based sentiment classifiers.

4.2.2 Combined Feature Sets
Next, we investigate the impact of combining un-
igram features with phrasal features on the classi-
fication’s performance. To combine different fea-
ture sets, we merge two sets of features to generate
a single feature set. For example, if the Unigrams
feature set has n features is combined with the Bi-
grams feature set which has m features, the result-
ing feature space from the merge will have n+m
number of features. Table 5 shows the classifica-
tion’s accuracy for each combined features set.

From Table 5, one can observe the follow-

Feature Set MNB SVM

Unigrams + Bigrams 78.3% 82.3%

Unigrams + Skipgrams 79.1% 83.0%

Unigrams–POS + Bigrams 78.6% 81.1%

Unigrams–POS + Skipgrams 80.2% 83.6%

Table 5: Classifications’ accuracy for combined
feature sets. Bold-faced accuracy scores denote
the top-performing classifier for each learning al-
gorithm.

ing: First, adding phrasal features to unigram fea-
tures consistently enhance the classifiers’ perfor-
mance and yield an improvement over the clas-
sifiers trained on individual feature sets. Sec-
ond, SVM classifiers consistently beat MNB when
trained on combined features. Therefore, it can be
concluded that using unigram features in combi-
nation with phrasal features improves the classifi-
cation performance of learning algorithms. More-
over, skipgram features seem to be more capable
of capturing informative sentiment terms than bi-
gram features. Finally, combining Unigrams–POS
and Skipgrams produced the optimal feature set of
both learning algorithms as it achieved top perfor-
mance (83.6% for SVM and 80.2% for MNB).

5 Discussion

Our classification results show that the two learn-
ing algorithms (MNB and SVM) behave differ-
ently in response to the sparsity degree of the text
representation. Interestingly, the MNB classifier
trained on skipgrams alone was optimal among
the the other MNB classifiers even though the
skipgram feature set is much more sparser than
the other feature sets. Likewise, the MNB clas-
sifier trained on bigrams showed better perfor-
mance than the unigram baseline. However, the
increase in the sparsity degree hampers the per-
formance of SVM as both classifiers trained on
bigrams and skipgrams performed worse than the
unigram baseline. Regarding POS features, and
contrary to previous works in the literature, we
found that adding POS features to unigrams im-
proves the performance of the classification for
both algorithms. We justify the difference in our
result to the choice of the POS tagger, since the
previous works used a general-purpose POS tag-
ger that was trained on different domains other
than social media text.273



In addition, our results show evidence that com-
bining unigram features with phrasal features al-
ways yields better performance than using feature
sets in isolation. Sparse feature sets, which were
less effective for SVM classifiers when used as
standalone features, do not negatively impact the
performance of SVM when combined with uni-
gram features. Thus, we conclude that combining
sparse phrasal feature sets with ungirams can be
considered as a smoothing technique to alleviate
the the sparsity problem and provide more infor-
mative features for sentiment classifiers.

In our experiments, we used a subset of the STS
data as our training data (200,000 tweets). How-
ever, our optimal classifier in this research, the
SVM trained Unigrams–POS and Skipgrams fea-
tures combined, achieves 83.6% accuracy. This
result outperforms all the classifiers of Go et al.
(2009) which were trained on the entire original
training dataset (∼1.6M tweets) and evaluated on
the same testing set. Their top performing clas-
sifier was the maximum entropy classifier trained
on a combination of unigrams and bigrams (they
reported 83.0% accuracy). This finding suggests
that using skipgrams as features in combination
with unigrams is a very good, yet simple solution
to consider when the available training data is lim-
ited in size.

6 Future Work

In this paper, we showed that combining unigrams
with skipgram features consistently improves the
classification performance. Our skipgram features
are actually k-skip-bigrams where k is the length
of the tweet. For future work, one can investigate
the impact of using fixed values for k on the clas-
sification performance as well as the sparsity de-
gree.

Furthermore, we aim to expand the work con-
ducted in this paper by considering the neutral
class since not all tweets convey a polar sentiment.
To tackle this task, the problem of Twitter sen-
timent classification would be reformulated as 3-
class classification problem. The major challenge
would be how to collect neutral tweets for training.
One way to collect neutral tweets was suggested
by Pak and Paroubek (2010) who used the Twit-
ter feed of some newspapers and magazines as a
source of objective tweets. Because newspapers
headlines usually convey facts not opinions, they
have considered Twitter news as neutral tweets.

7 Conclusions

The emergence of the social media, such as Twit-
ter, has provided an open medium where people
express their feelings and opinions at liberty. Twit-
ter stream generates an enormous number of opin-
ionated messages that cover all aspects of our daily
lives. Effective sentiment analysis of Twitter mes-
sages can reveal high quality information regard-
ing the concerns and preferences of the general
public. However, the use of informal language and
slang in Twitter, coupled with the high frequency
of misspellings, makes this task more challenging
than other domains where the text is well-edited.
We showed that the vocabulary size of a Twitter
text corpus grows in a higher rate than movie re-
views corpus. By means of linguistic preprocess-
ing, we were able to reduce the vocabulary size
in our Twitter dataset by nearly 58%. Still, up to
76% of the words occur only once in the corpus.
A manual inspection of the low-frequecnt terms
has revealed that the vast majority of the words
that occur only once are actually hashtag phrases,
elongated words, and irregular acronyms.

In this paper, we systematically evaluated the
effectiveness of four different feature sets: uni-
grams, unigrams with part of speech tags, bigrams,
and skipgrams on the performance of MNB and
SVM classifiers. First, each set of features is used
as a standalone feature set to train the two classi-
fiers. Our experimental results show that append-
ing Twitter-specific POS tags to the unigrams con-
sistently improves the performance of the classi-
fiers compared to the unigrams baseline. We also
find that MNB and SVM behaves differently in re-
sponse to the sparsity degree of phrasal features
(bigrams and skipgrams). While these features im-
prove the performance of the MNB classifier com-
pared to the unigram baseline, they hamper perfor-
mance of SVM. To the best of our knowledge, this
finding is unprecedented.

Furthermore, we investigated the impact of
combining unigram features with phrasal features
on the classification’s performance. Interestingly,
the results of combining different feature sets
show improvements not only compared to the un-
igram baseline, but also over all individual feature
sets. Therefore, we conclude that using a combi-
nation of unigrams and phrasal features is a very
good, yet simple solution to improve the perfor-
mance of Twitter sentiment classifiers, specially if
the training data is limited.274



References
Amir Asiaee T, Mariano Tepper, Arindam Banerjee,

and Guillermo Sapiro. 2012. If you are happy and
you know it... tweet. In Proceedings of the 21st
ACM international conference on Information and
knowledge management, pages 1602–1606. ACM.

Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Discov-
ery Science, pages 1–15. Springer.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.

Javi Fernández, Yoan Gutiérrez, José M Gómez, and
Patricio Martınez-Barco. 2014. Gplsi: Supervised
sentiment analysis in twitter using skipgrams. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014), number Se-
mEval, pages 294–299.

Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flani-
gan, and Noah A Smith. 2011. Part-of-speech tag-
ging for twitter: Annotation, features, and experi-
ments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 42–47. Association for Computa-
tional Linguistics.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, 1:12.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10–
18.

Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. Springer.

Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! Icwsm, 11:538–541.

Christopher D Manning, Prabhakar Raghavan, Hinrich
Schütze, et al. 2008. Introduction to information re-
trieval, volume 1. Cambridge university press Cam-
bridge.

Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.

Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. Asso-
ciation for Computational Linguistics.

Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In LREC, volume 10, pages 1320–1326.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86. As-
sociation for Computational Linguistics.

Gerald Petz, Michał Karpowicz, Harald Fürschuß,
Andreas Auinger, Václav Střı́teskỳ, and Andreas
Holzinger. 2013. Opinion mining on the web
2.0–characteristics of user generated content and
their impacts. In Human-Computer Interaction and
Knowledge Discovery in Complex, Unstructured,
Big Data, pages 35–46. Springer.

Hassan Saif, Yulan He, and Harith Alani. 2012. Al-
leviating data sparsity for twitter sentiment analysis.
CEUR Workshop Proceedings (CEUR-WS. org).

275


