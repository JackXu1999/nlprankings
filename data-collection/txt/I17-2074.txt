



















































Proofread Sentence Generation as Multi-Task Learning with Editing Operation Prediction


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 436–441,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Proofread Sentence Generation as Multi-Task Learning
with Editing Operation Prediction

Yuta Hitomi 1 Hideaki Tamori 1 Naoaki Okazaki 2 Kentaro Inui 3
1 Media Lab, The Asahi Shimbun Company

2 Tokyo Institute of Technology
3 Tohoku University, RIKEN Center Advanced Intelligence Project

{hitomi-y1, tamori-h}@asahi.com, okazaki@c.titech.ac.jp
inui@ecei.tohoku.ac.jp

Abstract

This paper explores the idea of robot ed-
itors, automated proofreaders that enable
journalists to improve the quality of their
articles. We propose a novel neural model
of multi-task learning that both gener-
ates proofread sentences and predicts the
editing operations required to rewrite the
source sentences and create the proofread
ones. The model is trained using logs of
the revisions made professional editors re-
vising draft newspaper articles written by
journalists. Experiments demonstrate the
effectiveness of our multi-task learning ap-
proach and the potential value of using re-
vision logs for this task.

1 Introduction

There is growing research interest in automatic
sentence generation (Vinyals et al., 2015; Rush
et al., 2015; Sordoni et al., 2015). Coincidentally
(or inevitably), media companies have increas-
ingly attempted to create robot journalists that can
automatically generate content, mostly using data
from limited domains (e.g., earthquakes, sports
and stockmarkets) (Clerwall, 2014; Carlson, 2015;
Dorr, 2016). In this paper, we explore the idea of
robot editors, i.e., automated proofreaders that en-
able journalists to improve the quality of their ar-
ticles.

The most closely related field to the topic of this
paper is grammatical error correction (Dale and
Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013,
2014). However, this task handles only gram-
matical errors, whereas proofreading encompasses
a variety of tasks:grammatical error correction
(GEC), spell checks, simplification, fact check-
ing, standardization, compression, paraphrasing,
etc. Another task, the automated evaluation of sci-

entific writing shared task (Daudaravicius et al.,
2016), has the goal of automatically evaluating
scientific writing. The focus of this shared task
was the binary classification problem of detecting
sentences that need improvement. Although the
corpus used contained qualitative improvements,
the shared task did not tackle high-quality sen-
tence generation.

This paper investigates the task of proofread
sentence generation (PSG) using logs of the revi-
sions made by professional editors to draft news-
paper articles written by journalists. The goal of
this research is to explore a computational model
for improving text quality. To this end, we pro-
pose a novel multi-task learning approach that
both generates proofread sentences and predicts
the editing operations involved in rewriting the
source sentences to create the proofread ones.

The contributions of this research are three-
folds: (i) This is the first study to explore an en-
coderdecoder architecture for PSG. (ii) We show
that our proposed multi-task learning method can
outperform a state-of-the-art baseline method for
GEC. (iii) We also examine the benefits and issues
of using revision logs for PSG.

2 Method

Given a source sentence (sequence of words)
x1, · · · , xm, this study addresses the task of gener-
ating the proofread sentence y1, · · · , yn, where m
and n denote the number of words in the source
and proofread sentences, respectively. Usually,
proofreading does not change the content of the
input text significantly, and changes only small
parts of the text. Thus, detecting source sentence
spans that require revision is an important PSG
sub-task. This paper explores a multi-task learn-
ing approach that both generates proofread sen-
tences and predicts the editing operations required.

436



大阪府 に移った 高齢者の 方の ２ 倍 。

大阪府 に移った 65 人の ２ 倍 。歳以上の に上る

へ

大阪府 へ に移った 高齢者の 方の ２ 倍 。

Attention Attention

K D K R R K K I

It is twice that of the elderly who moved to to Osaka prefecture.

It amount to twice that of over 65 years old who moved to Osaka prefecture.

G
eneration

P:

S:

Prediction

LSTM

LSTM

LSTMLSTMLSTMLSTMLSTMLSTMLSTM

LSTM LSTM LSTM LSTM LSTMLSTM LSTM

LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM

emb embembembembembembemb

Source sentence (S)

Proofread sentence (P)

Shared Encoder

emb embembembembembembemb

y0 y1 y8y7y6y5y4y3y2

Figure 1: Overview of the neural network model for generating proofread sentences and predicting
editing operations via multi-task learning. Boxes labeled ‘emb’ denote word embeddings.

Inspired by work on multi-task learning in neu-
ral networks, we implement multi-task learning as
an end-to-end neural network with a shared source
sentence encoder (Figure 1). The network gener-
ates proofread sentences and predicts editing op-
erations.Although these two tasks solve the same
problem, we believe that these two neural network
models focus on different aspects of the data. The
generation model considers the source sentence as
a whole ( although it may also include an attention
mechanism), whereas the prediction model looks
at the local contexts of words in order to correct
functional word usage, incorrect spellings, and so
on. This is why we have designed the proposed
method using a multi-task learning approach.

2.1 Generating proofread sentences using an
encoderdecoder model with attention

Following recent work on GEC (Yuan and
Briscoe, 2016), we use an encoder-decoder model
with global attention (Luong et al., 2015) to gen-
erate proofread sentences. We use bi-directional
Long Short-Term Memory (LSTM) to encode the
source sentences. LSTMs recurrently compute
the memory and hidden vectors at time step s ∈
{1, · · · ,m} using those at time step s− 1 or s+1
and the word xs in the source sentence, as follows:

[
−→
h s;−→c s] = −−−−→LSTM(xs, [−→h s−1;−→c s−1]), (1)

[
←−
h s;←−c s] =←−−−−LSTM(xs, [←−h s+1;←−c s+1]). (2)

Here, cs and hs represent the memory and hidden
vectors, respectively, at time step s. The encoder

output is a concatenation of the hidden vectors:

h̃s = [
−→
h s;
←−
h s]. (3)

The decoder computes the memory and hidden
vectors at time step t ∈ {1, · · · , n} using those
at time step t − 1 and the (predicted) word wt as
follows:

[ht; ct] =
−−−−→
LSTM(wt, [ht−1; ct−1]). (4)

Here, we set h0 =
←−
h 1 and c0 = 01. For t > 1, we

feed the yt−1 predicted at the previous time step
back as the input word wt of the decoder.

The decoder predicts the word yt at time step t
using a softmax layer on top of the vector h̄t with
an integrated attention mechanism:

log p(y|x) =
t∑

n=1

log p(yt|y<t,x), (5)

p(yt|y<t,x) = softmax(Woh̄t), (6)
h̄t = tanh(Wr[vt; ht]), (7)

vt =
∑

s

αt(s)h̃s, (8)

αt(s) =
exp(hᵀt Wah̃s)∑
s′ exp(h

ᵀ
tWah̃s′)

. (9)

Here, vt represents a vector computed by the at-
tention mechanism at time step t, and αt(s) is an
attention score computed at decoding time step t
by looking at the source word at encoding time
step s.

1The reason for using
←−
h 1 instead of h̃1 is to speed up

training by reducing the dimensionality of the vectors in the
decoder.

437



2.2 Editing-operation prediction as a
sequential labeling task

In addition to performing sentence revision, we
propose to simultaneously undertake an addi-
tional task: editing-operation prediction. For-
mally, given a source sentence x1, · · · , xm, this
task predicts the sequence of editing operations
z1, · · · , zm required to obtain the revised sentence
y1, · · · , yn, where each editing operation zi keeps
(K), deletes (D), inserts (I), or replaces (R) the
word xi. This task resembles grammatical error
detection.

As explained in Section 3.1, the revision logs do
not provide supervised training data for the edit-
ing operations, only pairs of source and proof-
read sentences. We therefore created pseudo su-
pervised training data by running the diff pro-
gram on the word sequences of the source and
proofread sentences. There are cases where multi-
ple sequences of primitive editing-operations can
be derived from a pair of sentences. The perfor-
mance may be improved if the best operation se-
quence. However, we leave this direction out of
scope of this paper. To obtain an editing-operation
sequence that was the same length as the source
sentence (m), we labeled I labels to the words im-
mediately following the positions where the actual
insert operations were required.

We reuse the vector h̃s for word xs in the source
sentence (Equation 3) to predict zs:

log p(z|x) =
m∑

s=1

log p(zs|z<s,x), (10)

p(zs|z<s,x) = softmax(Wl tanh(h̃i)). (11)
Liu and Liu (2016) proposed a grammatical er-

ror detection method that considers intra-attention.
We also explored this approach by replacing the
softmax function in Equation 11 with the follow-
ing one:

p(zs|z<s,x) = softmax(Wlūt) (12)
ūt = tanh(Ws[xt; ut]) (13)

us =
∑

i

βs(i)h̃s (14)

βs(i) =
exp(h̃ᵀsh̃i)∑
i′ exp(h̃

ᵀ
sh̃i′)

(15)

Here, us represents a vector computed by the at-
tention mechanism at time step s, and βs(i) is an
attention score computed at time step s by looking
at the source word at time step i ∈ {1, · · · ,m}.

Dataset # changed # unchanged # total
Train 710,540 1,317,260 2,027,800
Validation 63,062 96,938 160,000
Test 458 642 1,100

Table 1: The Numbers of instances in each dataset.

2.3 Training
Given a training dataset D, we minimize the fol-
lowing multi-task learning loss function:

−
∑

(x,y,z)∈D
{log p(y|x) + log p(z|x)} . (16)

We also consider a loss function that weights the
sub-task loss (Zhang et al., 2014):

−
∑

(x,y,z)∈D
{log p(y|x) + λ log p(z|x)} . (17)

Here, λ (0 ≤ λ ≤ 1) denotes the weight given
to the editing-operation prediction errors, learned
through gradient descent. In contrast to conven-
tional multi-task learning, which maximizes per-
formance for all tasks, our primary goal is to opti-
mize the main task which is why we have created a
loss function that weights the sub-task differently.

3 Experiments

3.1 Dataset

We used a corpus of Japanese newspaper arti-
cles where professional editors at a media com-
pany had rewritten draft articles (written by jour-
nalists) to create proofread (published) (Ta-
mori et al., 2017). The dataset consists of
2,209,249 sentence pairs in total: 810,227 pairs
were changed during the revision process, and
1,399,022 pairs were left unchanged. To focus on
revisions within sentence boundaries, we excluded
revisions involving sentence splitting or merging.
This dataset reflects the real work done by the
company to improve the quality of its newspaper
articles. The revisions came in a variety of forms,
from syntactic changes, such as GEC and spelling
normalization to content-level changes, such as
elaboration and fact checking. Table 1 shows the
number of instances in the training, validation, and
test sets.

We split the sentences into words using Sen-
tencePiece2 to efficiently reduce the vocabulary

2https://github.com/google/sentencepiece
SentencePiece is an unsupervised text tokenizer.

438



Dataset Model GLEU Prec Recall F0.5(M2) WER BLEU
Gen 70.14 * * * 24.90 74.02

All pairs Gen + Pred 70.47 * * * 24.23 75.37
Gen + Pred Attn 70.68 * * * 23.90 74.48
Gen + Pred Attn + W 70.57 * * * 37.24 54.76
Gen 67.74 22.27 6.12 14.23 36.74 64.23

Changed pairs Gen + Pred 68.10 23.14 5.59 13.37 35.67 66.29
Gen + Pred Attn 68.63 24.89 6.28 14.84 35.55 65.31
Gen + Pred Attn + W 67.01 36.59 13.30 26.59 48.91 45.82
Gen 86.72 * * * 16.47 82.69

Unchanged pairs Gen + Pred 87.34 * * * 15.51 83.33
Gen + Pred Attn 87.44 * * * 16.17 82.51
Gen + Pred Attn + W 87.27 * * * 28.94 62.66

Table 2: Performance of the proposed and baseline methods. The asterisks ‘*’ indicate performance
values that are unavailable because the precision and recall for unchanged pairs are always 0 and 100.

size. We used proofread sentences (published
newspaper articles) to train SentencePiece. Vo-
cabulary sizes for the input and output layers were
32,661 and 32,630, respectively. Our model can
be trained without unknown words.

3.2 Experimental setup
The batch size was set to 100 and, improve com-
putational efficiency, each batch consisted of sen-
tences of the same length. The dimensionality of
the distributed representations (word embeddings
and hidden states) to was 300. The model parame-
ters were trained using Adam. Following Jozefow-
icz et al. (2015), forget gate bias was initialized to
1.0, and the other gate biases were initialized to
0. In addition, we used dropout (at a rate of 0.2)
for the LSTMs. Breadth-first search was used for
decoding, with a beam width of 10 (Yuan, 2017).

Six measures were utilized to evalu-
ate the performance of the PSG model:
GLEU (Napoles et al., 2015)3, precision, re-
call, M2 score (Dahlmeier and Ng, 2012), Word
Error Rate (WER) (Jurafsky and Martin, 2008),
and BLEU (Papineni et al., 2002). These measures
are often used in GEC and machine translation
research. Note that the precision, recall, and
M2 score measures excluded words appearing
in both the source and proofread sentences from
evaluation (Dahlmeier and Ng, 2012).

3.3 Results
Table 2 shows the performance of the proposed
method according to the above metrics. Two vari-

3The hyperparameter λ of GLEU was set to 1.

ants of the proposed method were used: “Gen
+ Pred” used Equation 11 (without the atten-
tion mechanism for predicting edit operations),
whereas “Gen + Pred Attn” used Equation 12
(with the attention mechanism), and “Gen + Pred
Attn + W” used Equations 12 and 17 (weighting
the sub-task losses). For comparison, we also re-
port the performance of a baseline method “Gen”
used only an encoder-decoder model (described in
Section 2.1) without multi-task learning.

The multi-task learning models outperformed
the baseline encoder-decoder model for all met-
rics. The use of intra-attention for predicting
editing-operations was also effective, except for
the BLEU metric and the WER for unchanged
pairs. Unlike BLEU, GLEU includes a mecha-
nism for penalizing incorrect ‘reluctant’ revisions
(copying words from the source sentences) and
rewarding correct ‘aggressive’ revision (adding
words that do not appear in the source sentences).
We can therefore infer that “Gen + Pred with Attn”
model was more aggressive in changing words in
the source sentences than “Gen + Pred” model.

The table also shows the models’ performances
for changed/unchanged sentence pairs. As ex-
pected, the performance metrics for unchanged
pairs are higher than those for changed pairs. The
low recall values for changed pairs indicate that it
is difficult to predict words that do not appear in
the source sentences.

However, we can see that weighting sub-task
losses improved precision, recall, and M2 score
performance. We believe that active proofread-
ing performance improved because of not over-

439



learning the sub-task. That said, the other met-
rics decreased because the model made changes in
many places where no editing was required.

3.4 Discussion

In this section, we discuss issues with the pre-
sented method and dataset. The presented method
was not good at inserting words, particularly when
the editor had appended new information to the
sentence, as in the following example.

• Source: Soon it will have been six months
since the law was established.

• Proofread: On the 19th, it will have been
six months since the law was established last
September.

It is difficult for a computational model to in-
sert the phrases “on the 19th” and “last Septem-
ber”. We found that 132 of the 366 changed pairs
(28.8%) in the test set included new information.
Although this kind of editing improves the quality
of the article, it is unrealistic to handle this situa-
tion using an encoder-decoder model. It may be
useful to separate these pairs from the dataset in
future.

We also observed instances in which editors
merged pieces of information from multiple sen-
tences, particularly so as to yield more concise
sentences, as in the following example.

• Source: A meeting where people who heve
lost their families to cancer discuss ... .There
are ten members in their 30s to 70s who have
lost their families to cancer.

• Proofread: A meeting where people who
have lost their families to cancer discuss ...
.There are ten members in their 30s to 70s.

As a result, it may be worth exploring not only
discarding instances where additional information
has been added to the proofread sentences but also
building a model that considers other sentences
appearing near the source sentence.

We also noticed cases where the sentences out-
put by the model provide good revisions but did
not match to the reference sentences, such as the
following.

• Source: The reserve players tackle it franti-
cally so they won’t miss the chance.

• Proofread: The reserve players tackle it fran-
tically so as not to miss the opportunity.

• System output: The reserve players tackle it
frantically so as not to miss the chance.

As has often been discussed in the literature on
machine translation, summarization, and GEC, es-
tablishing a good evaluation metric is an ongoing
research issue.

4 Conclusion

In this paper, we have presented a novel multi-task
learning approach for combined PGS and editing-
operation prediction. Experimental results show
that our approach was able to outperform the base-
line for all metrics. The experiments also show
that newspaper article revision logs can provide
promising supervised training data for the model.
We plan to continue exploring ways of creating
good-quality articles in the future.

Acknowledgments

We thank the anonymous reviewers for their sug-
gestions. We also thank Takafumi Ochiai and
Yosuke Fukada for their contribution to storing the
dataset.

References
Matt Carlson. The robotic reporter. Digital Journal-

ism, 3(3):416–431, 2015.

Christer Clerwall. Enter the robot journalist. Journal-
ism Practice, 8(5):519–531, 2014.

Daniel Dahlmeier and Hwee Tou Ng. Better evalua-
tion for grammatical error correction. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL
2012), pages 568–572, 2012.

Robert Dale and Adam Kilgarriff. Helping our own:
The HOO 2011 pilot shared task. In Proceedings of
the 13th European Workshop on Natural Language
Generation (ENLG 2011), pages 242–249, 2011.

Robert Dale, Ilya Anisimoff, and George Narroway.
HOO 2012: A report on the preposition and deter-
miner error correction shared task. In Proceedings
of the Seventh Workshop on Building Educational
Applications Using NLP, pages 54–62, 2012.

Vidas Daudaravicius, Rafael E Banchs, Elena Volod-
ina, and Courtney Napoles. A report on the au-
tomatic evaluation of scientific writing shared task.
In Proceedings of the 11th Workshop on Innovative

440



Use of NLP for Building Educational Applications,
pages 53–62, 2016.

Konstantin Nicholas Dorr. Mapping the field of algo-
rithmic journalism. Digital Journalism, 4(6):700–
722, 2016.

Rafal Jozefowicz, Wojciech Zaremba, and Ilya
Sutskever. An empirical exploration of recur-
rent network architectures. In Proceedings of the
32nd International Conference on Machine Learn-
ing (ICML-15), pages 2342–2350, 2015.

Daniel Jurafsky and James H. Martin. Speech and
Language Processing : an introduction to natural
language processing, computational linguistics, and
speech recognition. Prentice Hall, 2008.

Zhuoran Liu and Yang Liu. Exploiting unlabeled
data for neural grammatical error detection. CoRR,
abs/1611.08987, 2016.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2015), pages 1412–
1421, 2015.

Courtney Napoles, Keisuke Sakaguchi, Matt Post, and
Joel Tetreault. Ground truth for grammatical er-
ror correction metrics. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (ACL-
IJCNLP 2015), pages 588–593, 2015.

Hwee Tou Ng, Yuanbin Wu, and Christian Hadiwinoto.
The CoNLL-2013 shared task on grammatical error
correction. In Proceedings of the Seventeenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task (CoNLL Shared Task 2013), pages
1–12, 2013.

Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. The CoNLL-2014 shared task on
grammatical error correction. In Proceedings of the
Eighteenth Conference on Computational Natural
Language Learning: Shared Task (CoNLL Shared
Task 2014), pages 1–14, 2014.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. BLEU: a method for automatic evaluation
of machine translation. In Proceedings of 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2002), pages 311–318, 2002.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
A neural attention model for abstractive sentence
summarization. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2015), pages 379–389, 2015.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. A
neural network approach to context-sensitive gener-
ation of conversational responses. In Proceedings of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 196–
205, Denver, Colorado, May–June 2015. Associa-
tion for Computational Linguistics.

Hideaki Tamori, Yuta Hitomi, Naoaki Okazaki, and
Kentaro Inui. Analyzing the revision logs of a
japanese newspaper for article quality assessment.
In Proceedings of the 2017 EMNLP Workshop: Nat-
ural Language Processing meets Journalism, pages
46–50, Copenhagen, Denmark, September 2017.
Association for Computational Linguistics.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. Show and tell: A neural image cap-
tion generator. In the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR 2015),
2015.

Zheng Yuan. Grammatical error correction in non-
native English. Technical Report 904, University of
Cambridge, Computer Laboratory, 2017.

Zheng Yuan and Ted Briscoe. Grammatical error
correction using neural machine translation. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2016), pages 380–386, 2016.

Zhanpeng Zhang, Ping Luo, Chen Change Loy, and
Xiaoou Tang. Facial landmark detection by deep
multi-task learning. In European Conference on
Computer Vision, pages 94–108. Springer, 2014.

441


