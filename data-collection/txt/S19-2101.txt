



















































CN-HIT-MI.T at SemEval-2019 Task 6: Offensive Language Identification Based on BiLSTM with Double Attention


Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 564–570
Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics

564

CN-HIT-MI.T at SemEval-2019 Task 6: Offensive Language Identification
Based on BiLSTM with Double Attention

Yaojie zhang, Bing Xu, Tiejun Zhao
Laboratory of Machine Intelligence and Translation, Harbin Institute of Technology

School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China
yjzhang@hit-mtlab.net, hitxb@hit.edu.cn, tjzhao@hit.edu.cn

Abstract

Offensive language has become pervasive in
social media. In Offensive Language Iden-
tification tasks, it may be difficult to pre-
dict accurately only according to the surface
words. So we try to dig deeper semantic in-
formation of text. This paper presents use an
attention-based two layers bidirectional long-
short memory neural network (BiLSTM) for
semantic feature extraction. Additionally, a
residual connection mechanism is used to syn-
thesize two different deep features, and an
emoji attention mechanism is used to extract
semantic information of emojis in text. We
participated in three sub-tasks of SemEval
2019 Task 6 as CN-HIT-MI.T team. Our
macro-averaged F1-score in sub-task A is
0.768, ranking 28/103. We got 0.638 in sub-
task B, ranking 30/75. In sub-task C, we got
0.549, ranking 22/65. We also tried some other
methods of not submitting results.

1 Introduction

Recognition of Offensive information has research
and application value in many aspects. With the
popularity of social media, people’s comments on
social media has become an important part of pub-
lic opinion. Although freedom of speech is ad-
vocated, there are still some unacceptable words.
The study of offensive language has only recently
arisen. With the deepening of the research, we
need to consider the different sub-tasks of its de-
composition.

In OffensEval tasks (Zampieri et al., 2019b), of-
fensive content was divided into three sub-tasks
taking the type and target of offenses into account.
Sub-task A is offensive language identification.
We should identify a short text sentence as offen-
sive or non-offensive. Sub-task B is automatic cat-
egorization of offense types. We need to classify a
sentence as having an attack target or not, if this is

an offensive sentence. Sub-task C is offense target
identification. Its purpose is to identify the target
of an attack sentence with an attack target. The
target is individual, group or other.

User’s comments on Twitter are usually clut-
tered. In order to clean up the data, a series of pre-
processing for the data is necessary. Then, pre-
trained word vectors are helpful to extract seman-
tic features from deep learning model. For clas-
sification model, bidirectional long-short memory
neural network can catch the contextual informa-
tion in text, in order to get offensive semantics
from text. A residual connection cascade the first
layer’s output and the second layer’s output can
get features of text at different levels. Attention
mechanism is used for the final output. Besides,
referring to ZHANG et al. (2018), emojis in a sen-
tence have a significant impact on sentiment. We
assume them may affect the offensive semantics of
text too, and double attention mechanism can deal
with this semantic relationship.

The rest of this paper is organized as follows.
Section 2 introduces some research advances in
Aggression Identification and Hate Speech. Sec-
tion 3 describes the data we use and the detailed
introduction of our system. Section 4 shows the
performance of our system and comparison with
other models. Section 5 describes some of our
summaries and future work directions.

2 Related Work

Due to the universality of offensive language in
social media, in order to cope with offensive lan-
guage and prevent abuse in social media, research
on related aspects has gradually emerged in recent
years.

There have been several seminars on offensive



565

language research, such as TRAC1 which shared
task on Aggression Identification summarized in
Kumar et al. (2018), need to distinguish open,
secret and non-aggressive texts. And ALW2

which is work for Abusive Language. Fišer et al.
(2017) did some work on the legal framework,
dataset and annotation schema of socially unac-
ceptable discourse practices on social networking
platforms in Slovenia. Gambäck and Sikdar
(2017) introduced a deep learning based Twitter
Hate Speech text include racism, sexism, both and
non-hate-speech classification system. Waseem
et al. (2017) put forward a series of subtasks of
hate speech, cyberbullying, and online abuse. Su
et al. (2017) described a system for detecting
and modifying Chinese dirty sentences. And
GermEval is also shared related tasks (Wiegand
et al., 2018) which initiate and foster research
on the identification of offensive content in
German language microposts. Additionally, the
main work of Malmasi and Zampieri (2017) and
Malmasi and Zampieri (2018) is to approach the
problem of distinguishing general profanity from
hate speech. Zhang et al. (2018) tried to use a
Convolution-GRU for detecting hate speech on
Twitter.

3 Methodology and Data

3.1 Method

Our method is composed of 6 stages: Preprocess-
ing, Embedding, Bidirectional LSTM, Attention,
Double attention and Softmax. The whole archi-
tecture of the single model is shown in Figure 1.

3.1.1 Preprocessing
We have done some processing on the original
training data and test data. The main purpose is to
make the data cleaner, reduce the number of un-
known words in the dictionary, and do some pro-
cessing of error words. The first step is to convert
all words into lowercase. Our preliminary view
is that uppercase or lowercase has no direct im-
pact on offensive language recognition tasks. The
second step is to deal with punctuation symbols.
We use space as separator to divide sentences into
words. But in many cases in data sets, punctuation

1https://sites.google.com/view/trac1/
home

2https://sites.google.com/site/
abusivelanguageworkshop2017/

symbols and words, as well as punctuation sym-
bols and punctuation symbols are closely linked
without space. In this way, the system will not
be able to recognize them. Just like ”sloth.” or
”!!!”, and we turned them into ”sloth .” and ”!
! !”. The third step is abbreviation processing.
We converted ”don’t”, ”you’re” and so on into ”do
n’t” and ”you ’re”, because there is no ”don’t” or
”you’re” in the dictionary. We haven’t expanded
the abbreviations like ”do not” or ”you are”, be-
cause the results may not be unique just like ”I’d”
can represent ”I would” or ”I had”. Forth, we also
need to separate the emojis refer to the dictionary
of emojis. (We will introduce the word dictionary
and emoji dictionary in detail in embedding sec-
tion). In addition, we regard all numbers as one
word.

After completing the above preprocessing, if a
word is still detected as an unknown word, we
use ekphrasis3 tool (Baziotis et al., 2017) for fur-
ther processing. The first step is word segmen-
tation. We separate some words together may
be a customary expression by spaces. For exam-
ple, ”Googlearecorrupt” is turned into ”Google are
corrupt”. Second, if the word still does not exist in
the dictionary, we do an error correction operation.
After all operations, words that do not exist in the
dictionary are marked as unknown words.

Step-by-step processing instead of direct batch
processing without intermediate detection im-
proves reliability and avoids modifying correct ex-
pressions to errors.

3.1.2 Embedding Layer
We used a word embedding layer to represent
words as vectors. The 200 dimension word vec-
tors4 is pre-trained by GloVe based on a large cor-
pus of Twitter provided by Jeffrey et al. (2014). It
includes 1,193,514 words and their vector repre-
sentations.

A sentence sequence S(w1, w2, ..., wl) is rep-
resented as C(c1, c2, ..., cl), where wi(i ∈ [1, l])
represents a word, ci represents the vector corre-
sponding to the word. C is the matrix representa-
tion of the sequence S.

We also use emoji vectors provided by Eisner
et al. (2016) to represent the emoji that appears
in a sequence. It includes 1,661 emojis used in

3https://www.github.com/cbaziotis/
ekphrasis

4https://nlp.stanford.edu/projects/
glove/

https://sites.google.com/view/trac1/home
https://sites.google.com/view/trac1/home
https://sites.google.com/site/abusivelanguageworkshop2017/
https://sites.google.com/site/abusivelanguageworkshop2017/
https://www.github.com/cbaziotis/ekphrasis
https://www.github.com/cbaziotis/ekphrasis
https://nlp.stanford.edu/projects/glove/
https://nlp.stanford.edu/projects/glove/


566

Figure 1: The whole architecture of 2-layer BiLSTM with Double Attention Based Offensive Language Identifi-
cation. Where w is word, and e is emoji

Twitter and their 300 dimension vector representa-
tions. We have made a PCA dimension reduction
on emoji vectors, making 300 dimensions into 200
dimensions to suit word vectors. In addition, all
bottom feature representation vector are normal-
ized.

3.1.3 Bidirectional LSTM Layer
The structure of one cell in LSTM is shown in Fig-
ure 2. Three gated mechanisms of LSTM allows
LSTM memory unit to store and access informa-
tion for a long time. it express input gate, ft ex-
press forget gate and ot express output gate. ct
express memory cell, st express memory state and
ht express hidden state. Where t denotes at time t.
The forward pass of LSTM is shown in (1)-(6).

it = f(Wixt + Uiht−1 + Vict−1) (1)

ft = f(Wfxt + Ufht−1 + Vfct−1) (2)

ot = f(Woxt + Uoht−1 + Voct) (3)

ct = g(Wcxt + UCht−1) (4)

st = ft � ct−1 + it � ct (5)

ht = ot � g(ct) (6)

Where xt is the input at time t, f(.) is the sig-
moid function, g(.) is the hyperbolic function. W ,
U and V are trainable weight parameters. In or-
der to extract the semantic relationship features of

Figure 2: The structure of one cell in LSTM

sentences before and after, we use Bidirectional
LSTM to process a sentence. Forward and back-

ward LSTM obtains hidden states
→
ht and

←
ht:

→
ht=

−→
LSTM (wt,

→
ht−1) (7)

←
ht=

←−
LSTM (wt,

←
ht−1) (8)



567

Cascade the results of bidirectional hidden state as
the result of BiLSTM:

Ht =
→
ht ⊕

←
ht (9)

We stack two layers of BiLSTMs. The output of
the first layer BiLSTM is used as the input of the
second layer. Meanwhile, we consider that the first
layer can collect low-level semantic information
such as lexical or grammatical information, while
the second layer can collect high-level semantic
information such as sentiment or offensiveness in-
formation. So we added a residual connection be-
tween the two layers:

Hfinalt = H
layer1
t ⊕H

layer2
t (10)

3.1.4 Attention Layer
We input the representation of hidden state ob-
tained by Bidirectional LSTM layer into attention
layer to get sentence coding:

si =
∑
t

αitH
final
it (11)

Where a is the weight value:

αit =
exp(uTituw)∑
t exp(u

T
ituw)

(12)

uit = tanh(WwH
final
it + bw) (13)

3.1.5 Double Attention Mechanism
We use another attention mechanism similar to
that of encoding sentences to encode emojis in
sentences, referring to (ZHANG et al., 2018). If
there are emojis in a sentence, we get the cor-
responding vector representation from the emojis
dictionary mentioned in Section 3.2.2. Then the
coding is obtained through the attention mecha-
nism:

sei =
∑
t

αeitEi (14)

Where E is the vector representation of emojis in
a sentence.

3.1.6 Softmax Layer
Finally, we concatenate sentence coding and emo-
jis coding through the full connection layer. We
use the softmax classifier to construct scoring vec-
tors for each category and convert them into prob-
abilities:

r = s⊕ se (15)

ŷ =
exp(Wr + b)∑

i∈[1,l] exp(Wir + bi)
(16)

Where W and b is the layer’s weights and biases.
We use cross-entropy loss function with L2 regu-
larization term:

L(ŷ, y) = −
N∑
i=1

C∑
j=1

yji log(
ˆ
yji ) + λ(

∑
θ∈Θ

θ2) (17)

3.2 Data
We only use the training dataset which contains
13,240 tweets provided by SemEval 2019 Task
6 (Zampieri et al., 2019a). Table 1 shows three
different levels of tasks and their corresponding
amount of data.

A B C Train Test Total
OFF TIN IND 2,407 100 2,507
OFF TIN OTH 395 35 430
OFF TIN GRP 1,074 78 1,152
OFF UNT - 524 27 551
NOT - - 8,840 620 9,460
All 13,240 860 14,100

Table 1: Distribution of label combinations in the data
provided by OLID

We have 13,240 tweets to train subtask A. Of
these, 4,400 are offensive and 8,840 are non-
offensive. There are 4,400 tweets for subtask B.
Of these, 3,876 are targeted insult and threats and
524 are untargeted. In 3,876 targeted tweets, 2,407
of them are individual, 1,074 of them are group,
and 395 of them are other.

4 Results

In this part, some experimental settings are briefly
described. Additionally, we list the experimen-
tal results we submitted and compared them with
baseline. We use 5-fold cross validation to get 5
same models. Using the probability predicted by
these 5 models to do a soft-vote to get the final
probability distribution. The highest probability is
the predictive class of our system. What’s more,
we compare performance on offensive identifica-
tion detection of different models by 5-fold cross
validation.

The number of hidden units in 2-layer of BiL-
STM is 150 each layer for sub-task A, where it
is 100 for sub-task B and 80 for sub-task C. The
size of randomly initialized attention query vector
is 256 dimensions. We chose the adam optimizer,



568

and the learning rate starts at 7e − 4, decreases to
0.9 times per 20 epoch, always greater than 1e−4.
The λ of L2 regularization is 1e− 5

Tables 2, 3 and 4 show the performance of our
system on test dataset in subtasks A, B and C, re-
spectively. The first 2 or 3 rows of the table show
the baseline of the officially provided subtasks.

System F1 (macro) Accuracy
All NOT baseline 0.4189 0.7209
All OFF baseline 0.2182 0.2790
Submitted System 0.7684 0.8244

Table 2: The results of Sub-task A we submitted. The
system is a 2-layer BiLSTM with Double Attention
which is described in Section 3.2

System F1 (macro) Accuracy
All TIN baseline 0.4702 0.8875
All UNT baseline 0.1011 0.1125
Submitted System 0.6381 0.8417

Table 3: The results of Sub-task B we submitted. The
system is a 2-layer BiLSTM with Double Attention
which is described in Section 3.2. It has different hy-
perparameter settings from Sub-task A

System F1 (macro) Accuracy
All GRP baseline 0.1787 0.3662
All IND baseline 0.2130 0.4695
All OTH baseline 0.0941 0.1643
Submitted System 0.5488 0.6338

Table 4: The results of Sub-task C we submitted. The
system is a 2-layer BiLSTM with Double Attention
which is described in Section 3.2. It has different hy-
perparameter settings from Sub-task A and Sub-task B

Figures 3, 4 and 5 show the confusion matrix of
the classification results of our system on the test
dataset in subtasks A, B and C, respectively.

We can clearly see from figures that our sys-
tem performs better in category has more training
data. This may be because more data in this cate-
gory makes the system more inclined to classify
these samples correctly in training, not because
some categories are easier to identify and others
are harder. We used the over-sampling method,
but the effect is not obvious. Categories with fewer
data quickly reach the state of over-fitting, while
those with more data are still under-fitting.

Table 5 shows some comparative experiments

NO
T

OF
F

Predicted label

NOT

OFF

Tr
ue

 la
be

l

566 54

97 143

Confusion Matrix

0.0

0.2

0.4

0.6

0.8

Figure 3: The confusion matrix of the classification re-
sults in Sub-task A

TIN UN
T

Predicted label

TIN

UNT

Tr
ue

 la
be

l

191 22

16 11

Confusion Matrix

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

Figure 4: The confusion matrix of the classification re-
sults in Sub-task B

on closed validation sets. We use 5-fold cross val-
idation, and each data has the same category ratio.

We can see that if we only use the second layer’s
features, the performance is worse than using the
first layer’s. When using two layers features at the
same time, the effect is the best. In addition, Word
Attention can significantly improve system perfor-
mance, but Emoji Attention only improve system
performance a little. This may be because emojis
have little impact on text offensive semantics. It is
worth mentioning that the system performance is
improved obviously after data preprocessing, and
the F1 score has increased by 3.12%.



569

GR
P

IN
D

OT
H

Predicted label

GRP

IND

OTH

Tr
ue

 la
be

l

54 11 13

15 73 12

17 10 8

Confusion Matrix

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

Figure 5: The confusion matrix of the classification re-
sults in Sub-task C

Model A B C
1-layer BiLSTM 0.749 0.610 0.527
1-layer BiLSTM
with Attention

0.757 0.632 0.544

2-layer BiLSTM
with Attention

0.752 0.626 0.513

2-layer BiLSTM
(residual connected)
with Attention

0.764 0.643 0.549

2-layer BiLSTM
(residual connected)
with Double Attention
(Emojis)

0.766 0.643 0.550

Table 5: Some comparative experiments on closed val-
idation sets

5 Conclusion

This paper introduces a deep learning method
Attention-based residual connected BiLSTM with
Emojis Attention for SemEval 2019 Task 6: Iden-
tifying and Categorizing Offensive Language in
Social Media. Our system didn’t get the leading
score in the competition. Maybe there are the fol-
lowing reasons. Except for the corpus used for
pre-training word vectors, we do not use other data
for training. Some machine learning models may
achieve better results with fewer data. Addition-
ally, we think that hyperparameter adjustment in
our system is not perfect. Most importantly, we do
not associate the characteristics of offensive lan-
guage recognition tasks with our model.

The next step of this paper is to associate tasks
with models. We will try to constructing a dic-
tionary of offensive words for tasks. We will also
try other ways of using external dictionaries except
double attention mechanism such as Position Em-
bedding. In addition, we will try some language
models that are currently leading the NLP task
such as BERT proposed by Devlin et al. (2018).



570

References
C Baziotis, N Pelekis, and C Doulkeridis. 2017. DataS-

tories at SemEval-2017 Task 4: Deep LSTM with
Attention for Message-level and Topic-based Senti-
ment Analysis. In Proceedings of the 11th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2017).

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training
of Deep Bidirectional Transformers for Lan-
guage Understanding. arXiv e-prints, page
arXiv:1810.04805.

Ben Eisner, Tim Rocktäschel, Isabelle Augenstein,
Matko Bošnjak, and Sebastian Riedel. 2016.
emoji2vec: Learning Emoji Representations from
their Description. In Proceedings of the 4th Interna-
tional Workshop on Natural Language. In Proceed-
ings of the 4th International Workshop on Natural
Language Processing for Social Media at EMNLP
2016 (SocialNLP at EMNLP 2016).

Darja Fišer, Tomaž Erjavec, and Nikola Ljubešić. 2017.
Legal Framework, Dataset and Annotation Schema
for Socially Unacceptable On-line Discourse Prac-
tices in Slovene. In Proceedings of the Workshop
Workshop on Abusive Language Online (ALW), Van-
couver, Canada.

Björn Gambäck and Utpal Kumar Sikdar. 2017. Using
Convolutional Neural Networks to Classify Hate-
speech. In Proceedings of the First Workshop on
Abusive Language Online, pages 85–90.

Pennington Jeffrey, Socher Richard, and D. Manning
Christopher. 2014. GloVe: Global Vectors for
Word Representation. In Proceedings of the Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1532–1543.

Ritesh Kumar, Atul Kr. Ojha, Shervin Malmasi, and
Marcos Zampieri. 2018. Benchmarking Aggression
Identification in Social Media. In Proceedings of the
First Workshop on Trolling, Aggression and Cyber-
bulling (TRAC), Santa Fe, USA.

Shervin Malmasi and Marcos Zampieri. 2017. Detect-
ing Hate Speech in Social Media. In Proceedings
of the International Conference Recent Advances in
Natural Language Processing (RANLP), pages 467–
472.

Shervin Malmasi and Marcos Zampieri. 2018. Chal-
lenges in Discriminating Profanity from Hate
Speech. Journal of Experimental & Theoretical Ar-
tificial Intelligence, 30:1–16.

Huei-Po Su, Chen-Jie Huang, Hao-Tsung Chang, and
Chuan-Jie Lin. 2017. Rephrasing Profanity in Chi-
nese Text. In Proceedings of the Workshop Work-
shop on Abusive Language Online (ALW), Vancou-
ver, Canada.

Zeerak Waseem, Thomas Davidson, Dana Warmsley,
and Ingmar Weber. 2017. Understanding Abuse: A
Typology of Abusive Language Detection Subtasks.
In Proceedings of the First Workshop on Abusive
Langauge Online.

Michael Wiegand, Melanie Siegel, and Josef Rup-
penhofer. 2018. Overview of the GermEval 2018
Shared Task on the Identification of Offensive Lan-
guage. In Proceedings of GermEval.

Marcos Zampieri, Shervin Malmasi, Preslav Nakov,
Sara Rosenthal, Noura Farra, and Ritesh Kumar.
2019a. Predicting the Type and Target of Offensive
Posts in Social Media. In Proceedings of NAACL.

Marcos Zampieri, Shervin Malmasi, Preslav Nakov,
Sara Rosenthal, Noura Farra, and Ritesh Kumar.
2019b. SemEval-2019 Task 6: Identifying and Cat-
egorizing Offensive Language in Social Media (Of-
fensEval). In Proceedings of The 13th International
Workshop on Semantic Evaluation (SemEval).

Yangsen ZHANG, Jia ZHENG, Gaijuan HUANG, and
et al. 2018. Microblog sentiment analysis method
based on a double attention model. In Proceed-
ings of Tsinghua University(Science and Technol-
ogy), volume 58, pages 122–130.

Ziqi Zhang, David Robinson, and Jonathan Tepper.
2018. Detecting Hate Speech on Twitter Using a
Convolution-GRU Based Deep Neural Network. In
Lecture Notes in Computer Science. Springer Ver-
lag.


