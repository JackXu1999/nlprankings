



















































A Supertag-Context Model for Weakly-Supervised CCG Parser Learning


Proceedings of the 19th Conference on Computational Language Learning, pages 22–31,
Beijing, China, July 30-31, 2015. c©2015 Association for Computational Linguistics

A Supertag-Context Model
for Weakly-Supervised CCG Parser Learning

Dan Garrette∗ Chris Dyer† Jason Baldridge‡ Noah A. Smith†

∗Computer Science & Engineering, University of Washington, dhg@cs.washington.edu
†School of Computer Science, Carnegie Mellon University, {cdyer,nasmith}@cs.cmu.edu

‡Department of Linguistics, University of Texas at Austin, jbaldrid@utexas.edu

Abstract

Combinatory Categorial Grammar (CCG)
is a lexicalized grammar formalism in
which words are associated with cate-
gories that specify the syntactic configura-
tions in which they may occur. We present
a novel parsing model with the capacity to
capture the associative adjacent-category
relationships intrinsic to CCG by param-
eterizing the relationships between each
constituent label and the preterminal cat-
egories directly to its left and right, bi-
asing the model toward constituent cate-
gories that can combine with their con-
texts. This builds on the intuitions of
Klein and Manning’s (2002) “constituent-
context” model, which demonstrated the
value of modeling context, but has the ad-
vantage of being able to exploit the prop-
erties of CCG. Our experiments show that
our model outperforms a baseline in which
this context information is not captured.

1 Introduction

Learning parsers from incomplete or indirect su-
pervision is an important component of moving
NLP research toward new domains and languages.
But with less information, it becomes necessary to
devise ways of making better use of the informa-
tion that is available. In general, this means con-
structing inductive biases that take advantage of
unannotated data to train probabilistic models.

One important example is the constituent-
context model (CCM) of Klein and Manning
(2002), which was specifically designed to cap-
ture the linguistic observation made by Radford
(1988) that there are regularities to the contexts
in which constituents appear. This phenomenon,
known as substitutability, says that phrases of the
same type appear in similar contexts. For example,

the part-of-speech (POS) sequence ADJ NOUN fre-
quently occurs between the tags DET and VERB.
This DET—VERB context also frequently applies
to the single-word sequence NOUN and to ADJ ADJ
NOUN. From this, we might deduce that DET—
VERB is a likely context for a noun phrase. CCM
is able to learn which POS contexts are likely,
and does so via a probabilistic generative model,
providing a statistical, data-driven take on substi-
tutability. However, since there is nothing intrin-
sic about the POS pair DET—VERB that indicates
a priori that it is a likely constituent context, this
fact must be inferred entirely from the data.

Baldridge (2008) observed that unlike opaque,
atomic POS labels, the rich structures of Combina-
tory Categorial Grammar (CCG) (Steedman, 2000;
Steedman and Baldridge, 2011) categories reflect
universal grammatical properties. CCG is a lexi-
calized grammar formalism in which every con-
stituent in a sentence is associated with a struc-
tured category that specifies its syntactic relation-
ship to other constituents. For example, a cate-
gory might encode that “this constituent can com-
bine with a noun phrase to the right (an object)
and then a noun phrase to the left (a subject) to
produce a sentence” instead of simply VERB. CCG
has proven useful as a framework for grammar in-
duction due to its ability to incorporate linguis-
tic knowledge to guide parser learning by, for ex-
ample, specifying rules in lexical-expansion al-
gorithms (Bisk and Hockenmaier, 2012; 2013)
or encoding that information as priors within a
Bayesian framework (Garrette et al., 2015).

Baldridge observed is that, cross-linguistically,
grammars prefer simpler syntactic structures when
possible, and that due to the natural correspon-
dence of categories and syntactic structure, bias-
ing toward simpler categories encourages simpler
structures. In previous work, we were able to
incorporate this preference into a Bayesian pars-
ing model, biasing PCFG productions toward sim-

22



pler categories by encoding a notion of category
simplicity into a prior (Garrette et al., 2015).
Baldridge further notes that due to the natural as-
sociativity of CCG, adjacent categories tend to be
combinable. We previously showed that incorpo-
rating this intuition into a Bayesian prior can help
train a CCG supertagger (Garrette et al., 2014).

In this paper, we present a novel parsing model
that is designed specifically for the capacity to
capture both of these universal, intrinsic proper-
ties of CCG. We do so by extending our pre-
vious, PCFG-based parsing model to include pa-
rameters that govern the relationship between con-
stituent categories and the preterminal categories
(also known as supertags) to the left and right.
The advantage of modeling context within a CCG
framework is that while CCM must learn which
contexts are likely purely from the data, the CCG
categories give us obvious a priori information
about whether a context is likely for a given con-
stituent based on whether the categories are com-
binable. Biasing our model towards both sim-
ple categories and connecting contexts encourages
learning structures with simpler syntax and that
have a better global “fit”.

The Bayesian framework is well-matched to our
problem since our inductive biases — those de-
rived from universal grammar principles, weak su-
pervision, and estimations based on unannotated
data — can be encoded as priors, and we can
use Markov chain Monte Carlo (MCMC) infer-
ence procedures to automatically blend these bi-
ases with unannotated text that reflects the way
language is actually used “in the wild”. Thus, we
learn context information based on statistics in the
data like CCM, but have the advantage of addi-
tional, a priori biases. It is important to note that
the Bayesian setup allows us to use these universal
biases as soft constraints: they guide the learner
toward more appropriate grammars, but may be
overridden when there is compelling contradictory
evidence in the data.

Methodologically, this work serves as an ex-
ample of how linguistic-theoretical commitments
can be used to benefit data-driven methods, not
only through the construction of a model family
from a grammar, as done in our previous work, but
also when exploiting statistical associations about
which the theory is silent. While there has been
much work in computational modeling of the in-
teraction between universal grammar and observ-

able data in the context of studying child language
acquisition (e.g., Villavicencio, 2002; Goldwater,
2007), we are interested in applying these princi-
ples to the design of models and learning proce-
dures that result in better parsing tools. Given our
desire to train NLP models in low-supervision sce-
narios, the possibility of constructing inductive bi-
ases out of universal properties of language is en-
ticing: if we can do this well, then it only needs to
be done once, and can be applied to any language
or domain without adaptation.

In this paper, we seek to learn from only raw
data and an incomplete dictionary mapping some
words to sets of potential supertags. In order to
estimate the parameters of our model, we develop
a blocked sampler based on that of Johnson et
al. (2007) to sample parse trees for sentences in
the raw training corpus according to their poste-
rior probabilities. However, due to the very large
sets of potential supertags used in a parse, com-
puting inside charts is intractable, so we design a
Metropolis-Hastings step that allows us to sample
efficiently from the correct posterior. Our experi-
ments show that the incorporation of supertag con-
text parameters into the model improves learning,
and that placing combinability-preferring priors
on those parameters yields further gains in many
scenarios.

2 Combinatory Categorial Grammar

In the CCG formalism, every constituent, including
those at the lexical level, is associated with a struc-
tured CCG category that defines that constituent’s
relationships to the other constituents in the sen-
tence. Categories are defined by a recursive struc-
ture, where a category is either atomic (possibly
with features), or a function from one category to
another, as indicated by a slash operator:

C → {s, sdcl, sadj, sb, np, n, nnum, pp, ...}
C → {(C/C), (C \C)}

Categories of adjacent constituents can be com-
bined using one of a set of combination rules to
form categories of higher-level constituents, as
seen in Figure 1. The direction of the slash op-
erator gives the behavior of the function. A cat-
egory (s\np)/pp might describe an intransitive
verb with a prepositional phrase complement; it
combines on the right (/) with a constituent with
category pp, and then on the left (\) with a noun
phrase (np) that serves as its subject.

23



s

np

np/n n

s\np

(s\np)/pp
pp

pp/np np
The man walks to work

Figure 1: CCG parse for “The man walks to work.”

We follow Lewis and Steedman (2014) in allow-
ing a small set of generic, linguistically-plausible
unary and binary grammar rules. We further add
rules for combining with punctuation to the left
and right and allow for the merge rule X → X X
of Clark and Curran (2007).

3 Generative Model

In this section, we present our novel supertag-
context model (SCM) that augments a standard
PCFG with parameters governing the supertags to
the left and right of each constituent.

The CCG formalism is said to be naturally as-
sociative since a constituent label is often able
to combine on either the left or the right. As a
motivating example, consider the sentence “The
lazy dog sleeps”, as shown in Figure 2. The
word lazy, with category n/n, can either com-
bine with dog (n) via the Forward Application rule
(>), or with The (np/n) via the Forward Compo-
sition (>B) rule. Baldridge (2008) showed that
this tendency for adjacent supertags to be com-
binable can be used to bias a sequence model in
order to learn better CCG supertaggers. However,
we can see that if the supertags of adjacent words
lazy (n/n) and dog (n) combine, then they will
produce the category n, which describes the en-
tire constituent span “lazy dog”. Since we have
produced a new category that subsumes that en-
tire span, a valid parse must next combine that
n with one of the remaining supertags to the left
or right, producing either (The·(lazy·dog))·sleeps
or The·((lazy·dog)·sleeps). Because we know that
one (or both) of these combinations must be valid,
we will similarly want a strong prior on the con-
nectivity between lazy·dog and its supertag con-
text: The↔(lazy·dog)↔sleeps.

Assuming T is the full set of known categories,
the generative process for our model is:

np/n n/n n s\np
The lazy dog sleeps

n

Figure 2: Higher-level category n subsumes the
categories of its constituents. Thus, n should have
a strong prior on combinability with its adjacent
supertags np/n and s\np.

Parameters:
θROOT ∼ Dir(αROOT, θROOT-0)
θBINt ∼ Dir(αBIN, θBIN-0) ∀t ∈ T
θUNt ∼ Dir(αUN, θUN-0) ∀t ∈ T
θTERMt ∼ Dir(αTERM, θTERM-0t ) ∀t ∈ T
λt ∼ Dir(αλ, λ0) ∀t ∈ T
θLCTXt ∼ Dir(αLCTX, θLCTX-0t ) ∀t ∈ T
θRCTXt ∼ Dir(αRCTX, θRCTX-0t ) ∀t ∈ T

Sentence:

do s ∼ Cat(θROOT)
y | s ∼ SCM(s)

until the tree y is valid
where 〈`, y, r〉 | t ∼ SCM(t) is defined as:
z ∼ Cat(λt)
if z = B : 〈u,v〉 | t ∼ Cat(θBINt )

yL | u ∼ SCM(u), yR | v ∼ SCM(v)
y = 〈yL, yR〉

if z = U : 〈u〉 | t ∼ Cat(θUNt )
y | u ∼ SCM(u)

if z = T : w | t ∼ Cat(θTERMt )
y = w

` | t ∼ Cat(θLCTXt ), r | t ∼ Cat(θRCTXt )

The process begins by sampling the parameters
from Dirichlet distributions: a distribution θROOT

over root categories, a conditional distribution θBINt
over binary branching productions given category
t, θUNt for unary rewrite productions, θTERMt for ter-
minal (word) productions, and θLCTXt and θ

RCTX
t for

left and right contexts. We also sample parame-
ters λt for the probability of t producing a binary
branch, unary rewrite, or terminal word.

Next we sample a sentence. This begins by sam-
pling first a root category s and then recursively
sampling subtrees. For each subtree rooted by a
category t, we generate a left context supertag `
and a right context supertag r. Then, we sam-

24



Aij

Bik Ckj

ti-1 tjti tj-1tk-1 tk

Figure 3: The generative process starting with
non-terminal Aij , where tx is the supertag for wx,
the word at position x, and “A→ B C” is a valid
production in the grammar. We can see that non-
terminal Aij generates nonterminals Bik and Ckj
(solid arrows) as well as generating left context ti-1
and right context tj (dashed arrows); likewise for
Bik and Ckj . The triangle under a non-terminal
indicates the complete subtree rooted by the node.

ple a production type z corresponding to either a
(B) binary, (U) unary, or (T) terminal production.
Depending on z, we then sample either a binary
production 〈u,v〉 and recurse, a unary production
〈u〉 and recurse, or a terminal word w and end that
branch. A tree is complete when all branches end
in terminal words. See Figure 3 for a graphical de-
piction of the generative behavior of the process.
Finally, since it is possible to generate a supertag
context category that does not match the actual
category generated by the neighboring constituent,
we must allow our process to reject such invalid
trees and re-attempt to sample.

Like CCM, this model is deficient since the same
supertags are generated multiple times, and parses
with conflicting supertags are not valid. Since we
are not generating from the model, this does not
introduce difficulties (Klein and Manning, 2002).

One additional complication that must be ad-
dressed is that left-frontier non-terminal categories
— those whose subtree span includes the first
word of the sentence — do not have a left-side su-
pertag to use as context. For these cases, we use
the special sentence-start symbol 〈S〉 to serve as
context. Similarly, we use the end symbol 〈E〉 for
the right-side context of the right-frontier.

We next discuss how the prior distributions are
constructed to encode desirable biases, using uni-
versal CCG properties.

3.1 Non-terminal production prior means

For the root, binary, and unary parameters, we
want to choose prior means that encode our bias

toward cross-linguistically-plausible categories.
To formalize the notion of what it means for a
category to be more “plausible”, we extend the
category generator of our previous work, which
we will call PCAT. We can define PCAT using a
probabilistic grammar (Garrette et al., 2014). The
grammar may first generate a start or end category
(〈S〉,〈E〉) with probability pse or a special token-
deletion category (〈D〉; explained in §5) with prob-
ability pdel, or a standard CCG category C:

X→〈S〉 | 〈E〉 pse
X→〈D〉 pdel
X→C (1− (2pse + pdel)) ·PC(C)

For each sentence s, there will be one 〈S〉 and one
〈E〉, so we set pse = 1/(25 + 2), since the average
sentence length in the corpora is roughly 25. To
discourage the model from deleting tokens (only
applies during testing), we set pdel = 10−100.

For PC, the distribution over standard cate-
gories, we use a recursive definition based on the
structure of a CCG category. If p = 1− p, then:1

C→a pterm ·patom(a)
C→A/A pterm ·pfwd · ( pmod ·PC(A) +

pmod ·PC(A)2 )
C→A/B pterm ·pfwd · pmod ·PC(A) ·PC(B)
C→A\A pterm ·pfwd · ( pmod ·PC(A) +

pmod ·PC(A)2 )
C→A\B pterm ·pfwd · pmod ·PC(A) ·PC(B)

The category grammar captures important as-
pects of what makes a category more or less
likely: (1) simplicity is preferred, with a higher
pterm meaning a stronger emphasis on simplic-
ity;2 (2) atomic types may occur at different rates,
as given by patom; (3) modifier categories (A/A
or A\A) are more likely than similar-complexity
non-modifiers (such as an adverb that modifies a
verb); and (4) operators may occur at different
rates, as given by pfwd.

We can use PCAT to define priors on our produc-
tion parameters that bias our model toward rules

1Note that this version has also updated the probability
definitions for modifiers to be sums, incorporating the fact
that anyA/A is also aA/B (likewise forA\A). This ensures
that our grammar defines a valid probability distribution.

2The probability distribution over categories is guaranteed
to be proper so long as pterm > 12 since the probability of the
depth of a tree will decrease geometrically (Chi, 1999).

25



that result in a priori more likely categories:3

θROOT-0(t) = PCAT(t)

θBIN-0(〈u,v〉) = PCAT(u) · PCAT(v)
θUN-0(〈u〉) = PCAT(u)

For simplicity, we assume the production-type
mixture prior to be uniform: λ0 = 〈13 , 13 , 13〉.
3.2 Terminal production prior means
We employ the same procedure as our previous
work for setting the terminal production prior dis-
tributions θTERM-0t (w) by estimating word-given-
category relationships from the weak supervision:
the tag dictionary and raw corpus (Garrette and
Baldridge, 2012; Garrette et al., 2015).4 This pro-
cedure attempts to automatically estimate the fre-
quency of each word/tag combination by divid-
ing the number of raw-corpus occurrences of each
word in the dictionary evenly across all of its asso-
ciated tags. These counts are then combined with
estimates of the “openness” of each tag in order to
assess its likelihood of appearing with new words.

3.3 Context parameter prior means
In order to encourage our model to choose trees
in which the constituent labels “fit” into their
supertag contexts, we want to bias our con-
text parameters toward context categories that are
combinable with the constituent label.

The right-side context of a non-terminal cate-
gory — the probability of generating a category
to the right of the current constituent’s category
— corresponds directly to the category transitions
used for the HMM supertagger of Garrette et al.
(2014). Thus, the right-side context prior mean
θRCTX-0t can be biased in exactly the same way as
the HMM supertagger’s transitions: toward context
supertags that connect to the constituent label.

To encode a notion of combinability, we fol-
low Baldridge’s (2008) definition. Briefly, let
κ(t,u) ∈ {0, 1} be an indicator of whether t com-
bines with u (in that order). For any binary rule
that can combine t to u, κ(t,u)=1. To ensure that
our prior captures the natural associativity of CCG,
we define combinability in this context to include
composition rules as well as application rules. If

3For our experiments, we normalize PCAT by dividing by∑
c∈T PCAT(c). This allows for experiments contrasting with

a uniform prior (1/|T |) without adjusting α values.
4We refer the reader to the previous work (Garrette et al.,

2015) for a fuller discussion and implementation details.

atoms have features associated, then the atoms are
allowed to unify if the features match, or if at least
one of them does not have a feature. In defining κ,
it is also important to ignore possible arguments
on the wrong side of the combination since they
can be consumed without affecting the connection
between the two. To achieve this for κ(t,u), it is
assumed that it is possible to consume all preced-
ing arguments of t and all following arguments of
u. So κ(np, (s\np)/np) = 1. This helps to en-
sure the associativity discussed earlier. For “com-
bining” with the start or end of a sentence, we
define κ(〈S〉,u)=1 when u seeks no left-side ar-
guments (since there are no tags to the left with
which to combine) and κ(t, 〈E〉)=1 when t seeks
no right-side arguments. So κ(〈S〉,np/n)=1, but
κ(〈S〉, s\np)=0. Finally, due to the frequent use
of the unary rule that allows n to be rewritten
as np, the atom np is allowed to unify with n
if n is the argument. So κ(n, s\np) = 1, but
κ(np/n,np) = 0.

The prior mean of producing a right-context su-
pertag r from a constituent category t, P right(r | t),
is defined so that combinable pairs are given
higher probability than non-combinable pairs. We
further experimented with a prior that biases to-
ward both combinability and category likelihood,
replacing the uniform treatment of categories with
our prior over categories, yielding P rightCAT (r | t). If
T is the full set of known CCG categories:

P right(r | t) =
{
σ · 1/|T | if κ(t, r) σ > 1
1/|T | otherwise

P rightCAT (r | t) =
{
σ · PCAT(r) if κ(t, r) σ > 1
PCAT(r) otherwise

Distributions P left(` | t) and P leftCAT(` | t) are de-
fined in the same way, but with the combinability
direction flipped: κ(`, t), since the left context su-
pertag precedes the constituent category.

4 Posterior Inference

We wish to infer the distribution over CCG parses,
given the model we just described and a corpus of
sentences. Since there is no way to analytically
compute these modes, we resort to Gibbs sam-
pling to find an approximate solution. Our strat-
egy is based on the approach presented by John-
son et al. (2007). At a high level, we alternate be-
tween resampling model parameters (θROOT, θBIN,
θUN, θTERM, λ, θLCTX, θRCTX) given the current set
of parse trees and resampling those trees given the

26



current model parameters and observed word se-
quences. To efficiently sample new model param-
eters, we exploit Dirichlet-multinomial conjugacy.
By repeating these alternating steps and accumu-
lating the productions, we obtain an approxima-
tion of the required posterior quantities.

Our inference procedure takes as input the dis-
tribution prior means, along with the raw corpus
and tag dictionary. During sampling, we restrict
the tag choices for a word w to categories allowed
by the tag dictionary. Since real-world learning
scenarios will always lack complete knowledge of
the lexicon, we, too, want to allow for unknown
words; for these, we assume the word may take
any known supertag. We refer to the sequence of
word tokens as w and a non-terminal category cov-
ering the span i through j − 1 as yij .

While it is technically possible to sample di-
rectly from our context-sensitive model, the high
number of potential supertags available for each
context means that computing the inside chart for
this model is intractable for most sentences. In
order to overcome this limitation, we employ an
accept/reject Metropolis-Hastings (MH) step. The
basic idea is that we sample trees according to a
simpler proposal distribution Q that approximates
the full distribution and for which direct sampling
is tractable, and then choose to accept or reject
those trees based on the true distribution P .

For our model, there is a straightforward and
intuitive choice for the proposal distribution: the
PCFG model without our context parameters:
(θROOT, θBIN, θUN, θTERM, λ), which is known to
have an efficient sampling method. Our accep-
tance step is therefore based on the remaining pa-
rameters: the context (θLCTX, θRCTX).

To sample from our proposal distribution, we
use a blocked Gibbs sampler based on the one
proposed by Goodman (1998) and used by John-
son et al. (2007) that samples entire parse trees.
For a sentence w, the strategy is to use the Inside
algorithm (Lari and Young, 1990) to inductively
compute, for each potential non-terminal position
spanning words wi through wj−1 and category t,
going “up” the tree, the probability of generating
wi, . . . , wj−1 via any arrangement of productions
that is rooted by yij = t.

p(wi | yi,i+1 = t) = λt(T) · θTERMt (wi)
+
∑

t→u λt(U) · θUNt (〈u〉)
· p(wi:j−1 | yij = u)

p(wi:j−1 | yij = t) =∑
t→u λt(U) · θUNt (〈u〉)

· p(wi:j−1 | yij = u)
+
∑

t→u v
∑

i<k<j λt(B) · θBINt (〈u,v〉)
· p(wi:k−1 | yik = u)
· p(wk:j−1 | ykj = v)

We then pass “downward” through the chart, sam-
pling productions until we reach a terminal word
on all branches.

y0n ∼ θROOTt · p(w0:n−1 | y0n = t)
x | yij ∼

〈
θBINyij (〈u,v〉) · p(wi:k−1 | yik = u)

· p(wk:j−1 | ykj = v)
∀ yik, ykj when j > i+ 1,

θUNyij (〈u〉) · p(wi:j−1 | y′ij = u) ∀ y′ij ,
θTERMyij (wi) when j = i+ 1

〉
where x is either a split point k and pair of cate-
gories yik, ykj resulting from a binary rewrite rule,
a single category y′ij resulting from a unary rule, or
a word w resulting from a terminal rule.

The MH procedure requires an acceptance dis-
tribution A that is used to accept or reject a tree
sampled from the proposal Q. The probability of
accepting new tree y′ given the previous tree y is:

A(y′ | y) = min
(

1,
P (y′)
P (y)

Q(y)
Q(y′)

)
Since Q is defined as a subset of P ’s parameters,
it is the case that:

P (y) = Q(y) · p(y | θLCTX, θRCTX)

After substituting this for each P inA, all of theQ
factors cancel, yielding the acceptance distribution
defined purely in terms of context parameters:

A(y′ | y) = min
(

1,
p(y′ | θLCTX, θRCTX)
p(y | θLCTX, θRCTX)

)
For completeness, we note that the probability

of a tree y given only the context parameters is:5

p(y | θLCTX, θRCTX) =∏
0≤i<j≤n

θLCTX(yi−1,i | yij) · θRCTX(yj,j+1 | yij)

5Note that there may actually be multiple yij due to unary
rules that “loop back” to the same position (i, j); all of these
much be included in the product.

27



Before we begin sampling, we initialize each
distribution to its prior mean (θROOT=θROOT-0,
θBINt =θ

BIN-0, etc). Since MH requires an initial set
of trees to begin sampling, we parse the raw corpus
with probabilistic CKY using these initial parame-
ters (excluding the context parameters) to guess an
initial tree for each raw sentence.

The sampler alternates sampling parse trees for
the entire corpus of sentences using the above pro-
cedure with resampling the model parameters. Re-
sampling the parameters requires empirical counts
of each production. These counts are taken from
the trees resulting from the previous round of sam-
pling: new trees that have been “accepted” by the
MH step, as well as existing trees for sentences in
which the newly-sampled tree was rejected.

θROOT ∼ Dir(〈αROOT · θROOT-0(t) + Croot(t) 〉t∈T )
θBINt ∼ Dir(〈αBIN · θBIN-0(〈u,v〉) + C(t→〈u,v〉) 〉u,v∈T )
θUNt ∼ Dir(〈αUN · θUN-0(〈u〉) + C(t→〈u〉) 〉u∈T )

θTERMt ∼ Dir(〈αTERM · θTERM-0t (w) + C(t→ w) 〉w∈V )
λt ∼ Dir(〈αλ · λ0(B) +

∑
u,v∈T C(t→〈u,v〉),

αλ · λ0(U) +
∑

u∈T C(t→〈u〉),
αλ · λ0(T) +

∑
w∈V C(t→w) 〉)

θLCTXt ∼ Dir(〈αLCTX · θLCTX-0t (`) + Cleft (t, `)〉`∈T )
θRCTXt ∼ Dir(〈αRCTX · θRCTX-0t (r) + Cright(t, r)〉r∈T )

It is important to note that this method of re-
sampling allows the draws to incorporate both the
data, in the form of counts, and the prior mean,
which includes all of our carefully-constructed bi-
ases derived from both the intrinsic, universal CCG
properties as well as the information we induced
from the raw corpus and tag dictionary.

After all sampling iterations have completed,
the final model is estimated by pooling the trees
resulting from each sampling iteration, including
trees accepted by the MH steps as well as the dupli-
cated trees retained due to rejections. We use this
pool of trees to compute model parameters using
the same procedure as we used directly above to
sample parameters, except that instead of drawing
a Dirichlet sample based on the vector of counts,
we simply normalize those counts. However, since
we require a final model that can parse sentences
efficiently, we drop the context parameters, mak-
ing the model a standard PCFG, which allows us to
use the probabilistic CKY algorithm.

5 Experiments

In our evaluation we compared our supertag-
context approach to (our reimplementation of) the

best-performing model of our previous work (Gar-
rette et al., 2015), which SCM extends. We evalu-
ated on the English CCGBank (Hockenmaier and
Steedman, 2007), which is a transformation of the
Penn Treebank (Marcus et al., 1993); the CTB-
CCG (Tse and Curran, 2010) transformation of the
Penn Chinese Treebank (Xue et al., 2005); and the
CCG-TUT corpus (Bos et al., 2009), built from the
TUT corpus of Italian text (Bosco et al., 2000).

Each corpus was divided into four distinct data
sets: a set from which we extract the tag dictionar-
ies, a set of raw (unannotated) sentences, a devel-
opment set, and a test set. We use the same splits
as Garrette et al. (2014). Since these treebanks
use special representations for conjunctions, we
chose to rewrite the trees to use conjunction cate-
gories of the form (X\X)/X rather than introduc-
ing special conjunction rules. In order to increase
the amount of raw data available to the sampler,
we supplemented the English data with raw, unan-
notated newswire sentences from the NYT Giga-
word 5 corpus (Parker et al., 2011) and supple-
mented Italian with the out-of-domain WaCky cor-
pus (Baroni et al., 1999). For English and Italian,
this allowed us to use 100k raw tokens for train-
ing (Chinese uses 62k). For Chinese and Italian,
for training efficiency, we used only raw sentences
that were 50 words or fewer (note that we did not
drop tag dictionary set or test set sentences).

The English development set was used to tune
hyperparameters using grid search, and the same
hyperparameters were then used for all three lan-
guages. For the category grammar, we used
ppunc=0.1, pterm=0.7, pmod=0.2, pfwd=0.5. For
the priors, we use αROOT=1, αBIN=100, αUN=100,
αTERM=104, αλ=3, αLCTX=αRCTX=103.6 For the
context prior, we used σ=105. We ran our sampler
for 50 burn-in and 50 sampling iterations.

CCG parsers are typically evaluated on the de-
pendencies they produce instead of their CCG
derivations directly since there can be many differ-
ent CCG parse trees that all represent the same de-
pendency relationships (spurious ambiguity), and
CCG-to-dependency conversion can collapse those
differences. To convert a CCG tree into a de-
pendency tree, we follow Lewis and Steedman

6In order to ensure that these concentration parameters,
while high, were not dominating the posterior distributions,
we ran experiments in which they were set much higher
(including using the prior alone), and found that accuracies
plummeted in those cases, demonstrating that there is a good
balance with the prior.

28



Size of the corpus (tokens) from which the tag dictionary is extracted
250k 200k 150k 100k 50k 25k

English no context 60.43 61.22 59.69 58.61 56.26 54.70
context (uniform) 64.02 63.89 62.58 61.80 59.44 57.08
+P left / P right 65.44 63.26 64.28 62.90 59.63 57.86
+P leftCAT / P

right
CAT 59.34 59.89 59.32 58.47 57.85 55.77

Chinese no context 32.70 32.07 28.99
context (uniform) 36.02 33.84 32.55
+P left / P right 35.34 33.04 31.48
+P leftCAT / P

right
CAT 35.15 34.04 33.53

Italian no context 51.54
context (uniform) 53.57
+P left / P right 52.54
+P leftCAT / P

right
CAT 53.29

Table 1: Experimental results in three languages. For each language, four experiments were executed:
(1) a no-context model baseline, Garrette et al. (2015) directly; (2) our supertag-context model, with uni-
form priors on contexts; (3) supertag-context model with priors that prefer combinability; (4) supertag-
context model with priors that prefer combinability and simpler categories. Results are shown for six
different levels of supervision, as determined by the size of the corpus used to extract a tag dictionary.

(2014). We traverse the parse tree, dictating at ev-
ery branching node which words will be the de-
pendents of which. For binary branching nodes of
forward rules, the right side—the argument side—
is the dependent, unless the left side is a modi-
fier (X/X) of the right, in which case the left is
the dependent. The opposite is true for backward
rules. For punctuation rules, the punctuation is al-
ways the dependent. For merge rules, the right side
is always made the parent. The results presented
in this paper are dependency accuracy scores: the
proportion of words that were assigned the correct
parent (or “root” for the root of a tree).

When evaluating on test set sentences, if the
model is unable to find a parse given the con-
straints of the tag dictionary, then we would have
to take a score of zero for that sentence: every de-
pendency would be “wrong”. Thus, it is impor-
tant that we make a best effort to find a parse. To
accomplish this, we implemented a parsing back-
off strategy. The parser first tries to find a valid
parse that has either sdcl or np at its root. If
that fails, then it searches for a parse with any
root. If no parse is found yet, then the parser at-
tempts to strategically allow tokens to subsume a
neighbor by making it a dependent (first with a re-
stricted root set, then without). This is similar to
the “deletion” strategy employed by Zettlemoyer
and Collins (2007), but we do it directly in the
grammar. We add unary rules of the form 〈D〉→u

for every potential supertag u in the tree. Then,
at each node spanning exactly two tokens (but no
higher in the tree), we allow rules t→〈〈D〉, v〉 and
t→〈v, 〈D〉〉. Recall that in §3.1, we stated that 〈D〉
is given extremely low probability, meaning that
the parser will avoid its use unless it is absolutely
necessary. Additionally, since u will still remain
as the preterminal, it will be the category exam-
ined as the context by adjacent constituents.

For each language and level of supervision, we
executed four experiments. The no-context base-
line used (a reimplementation of) the best model
from our previous work (Garrette et al., 2015):
using only the non-context parameters (θROOT,
θBIN, θUN, θTERM, λ) along with the category prior
PCAT to bias toward likely categories throughout
the tree, and θTERM-0t estimated from the tag dictio-
nary and raw corpus. We then added the supertag-
context parameters (θLCTX, θRCTX), but used uni-
form priors for those (still using PCAT for the rest).
Then, we evaluated the supertag-context model
using context parameter priors that bias toward
categories that combine with their contexts: P left

and P right (see §3.3). Finally, we evaluated the
supertag-context model using context parameter
priors that bias toward combinability and toward
a priori more likely categories, based on the cate-
gory grammar (P leftCAT and P

right
CAT ).

Because we are interested in understanding how
our models perform under varying amounts of su-

29



pervision, we executed sequences of experiments
in which we reduced the size of the corpus from
which the tag dictionary is drawn, thus reducing
the amount of information provided to the model.
As this information is reduced, so is the size of the
full inventory of known CCG categories that can be
used as supertags. Additionally, a smaller tag dic-
tionary means that there will be vastly more un-
known words; since our model must assume that
these words may take any supertag from the full
set of known labels, the model must contend with
a greatly increased level of ambiguity.

The results of our experiments are given in Ta-
ble 1. We find that the incorporation of supertag-
context parameters into a CCG model improves
performance in every scenario we tested; we see
gains of 2–5% across the board. Adding context
parameters never hurts, and in most cases, using
priors based on intrinsic, cross-lingual aspects of
the CCG formalism to bias those parameters to-
ward connectivity provides further gains. In par-
ticular, biasing the model toward trees in which
constituent labels are combinable with their adja-
cent supertags frequently helps the model.

However, for English, we found that addition-
ally biasing context priors toward simpler cate-
gories using P leftCAT/P

right
CAT degraded performance.

This is likely due to the fact that the priors on pro-
duction parameters (θBIN, θUN) are already biasing
the model toward likely categories, and that hav-
ing the context parameters do the same ends up
over-emphasizing the need for simple categories,
preventing the model from choosing more com-
plex categories when they are needed. On the
other hand, this bias helps in Chinese and Italian.

6 Related Work

Klein and Manning (2002)’s CCM is an unla-
beled bracketing model that generates the span of
part-of-speech tags that make up each constituent
and the pair of tags surrounding each constituent
span (as well as the spans and contexts of each
non-constituent). They found that modeling con-
stituent context aids in parser learning because it
is able to capture the observation that the same
contexts tend to appear repeatedly in a corpus,
even with different constituents. While CCM is
designed to learn which tag pairs make for likely
contexts, without regard for the constituents them-
selves, our model attempts to learn the relation-
ships between context categories and the types of

the constituents, allowing us to take advantage of
the natural a priori knowledge about which con-
texts fit with which constituent labels.

Other researchers have shown positive results
for grammar induction by introducing relatively
small amounts of linguistic knowledge. Naseem
et al. (2010) induced dependency parsers by hand-
constructing a small set of linguistically-universal
dependency rules and using them as soft con-
straints during learning. These rules were use-
ful for disambiguating between various structures
in cases where the data alone suggests multiple
valid analyses. Boonkwan and Steedman (2011)
made use of language-specific linguistic knowl-
edge collected from non-native linguists via a
questionnaire that covered a variety of syntactic
parameters. They were able to use this infor-
mation to induce CCG parsers for multiple lan-
guages. Bisk and Hockenmaier (2012; 2013) in-
duced CCG parsers by using a smaller number of
linguistically-universal principles to propose syn-
tactic categories for each word in a sentence, al-
lowing EM to estimate the model parameters. This
allowed them to induce the inventory of language-
specific types from the training data, without prior
language-specific knowledge.

7 Conclusion

Because of the structured nature of CCG categories
and the logical framework in which they must as-
semble to form valid parse trees, the CCG formal-
ism offers multiple opportunities to bias model
learning based on universal, intrinsic properties
of the grammar. In this paper we presented a
novel parsing model with the capacity to capture
the associative adjacent-category relationships in-
trinsic to CCG by parameterizing supertag con-
texts, the supertags appearing on either side of
each constituent. In our Bayesian formulation, we
place priors on those context parameters to bias
the model toward trees in which constituent labels
are combinable with their contexts, thus preferring
trees that “fit” together better. Our experiments
demonstrate that, across languages, this additional
context helps in weak-supervision scenarios.

Acknowledgements

We would like to thank Yonatan Bisk for his
valuable feedback. This work was supported by
the U.S. Department of Defense through the U.S.
Army Research Office grant W911NF-10-1-0533.

30



References
Jason Baldridge. 2008. Weakly supervised supertag-

ging with grammar-informed initialization. In Proc.
of COLING.

Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 1999. The WaCky wide
web: a collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3).

Yonatan Bisk and Julia Hockenmaier. 2012. Simple
robust grammar induction with combinatory catego-
rial grammar. In Proc. of AAAI.

Yonatan Bisk and Julia Hockenmaier. 2013. An HDP
model for inducing combinatory categorial gram-
mars. Transactions of the Association for Compu-
tational Linguistics, 1.

Prachya Boonkwan and Mark Steedman. 2011. Gram-
mar induction from text using small syntactic proto-
types. In Proc. of IJCNLP.

Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a dependency treebank to a cat-
egorial grammar treebank for Italian. In M. Pas-
sarotti, Adam Przepiórkowski, S. Raynaud, and
Frank Van Eynde, editors, Proc. of the Eighth In-
ternational Workshop on Treebanks and Linguistic
Theories (TLT8).

Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,
and Leonardo Lesmo. 2000. Building a treebank for
Italian: a data-driven annotation schema. In Proc. of
LREC.

Zhiyi Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics.

Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics, 33.

Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden Markov models for part-of-
speech tagging with incomplete tag dictionaries. In
Proc. of EMNLP.

Dan Garrette, Chris Dyer, Jason Baldridge, and
Noah A. Smith. 2014. Weakly-supervised Bayesian
learning of a CCG supertagger. In Proc. of CoNLL.

Dan Garrette, Chris Dyer, Jason Baldridge, and
Noah A. Smith. 2015. Weakly-supervised
grammar-informed Bayesian CCG parser learning.
In Proc. of AAAI.

Sharon Goldwater. 2007. Nonparametric Bayesian
Models of Lexical Acquisition. Ph.D. thesis, Brown
University.

Joshua Goodman. 1998. Parsing inside-out. Ph.D.
thesis, Harvard University.

Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3).

Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proc. of NAACL.

Dan Klein and Christopher D. Manning. 2002. A
generative constituent-context model for improved
grammar induction. In Proc. of ACL.

Karim Lari and Steve J. Young. 1990. The esti-
mation of stochastic context-free grammars using
the inside-outside algorithm. Computer Speech and
Language, 4:35–56.

Mike Lewis and Mark Steedman. 2014. A* CCG
parsing with a supertag-factored model. In Proc. of
EMNLP.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).

Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proc. of
EMNLP.

Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion LDC2011T07. Linguistic Data Consortium.

Andrew Radford. 1988. Transformational Grammar.
Cambridge University Press.

Mark Steedman and Jason Baldridge. 2011. Combina-
tory categorial grammar. In Robert Borsley and Ker-
sti Borjars, editors, Non-Transformational Syntax:
Formal and Explicit Models of Grammar. Wiley-
Blackwell.

Mark Steedman. 2000. The Syntactic Process. MIT
Press.

Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: Extracting CCG derivations from the Penn
Chinese Treebank. In Proc. of COLING.

Aline Villavicencio. 2002. The acquisition of a
unification-based generalised categorial grammar.
Ph.D. thesis, University of Cambridge.

Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238.

Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proc. of EMNLP.

31


