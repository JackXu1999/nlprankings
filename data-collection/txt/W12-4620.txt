



















































Search Space Properties for Learning a Class of Constraint-based Grammars


Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+11), pages 171–179,
Paris, September 2012.

Search Space Properties for Learning a Class of Constraint-based
Grammars

Smaranda Muresan
School of Communication and Information

Rutgers University
4 Huntington St, New Brunswick, NJ, 08901

smuresan@rutgers.edu

Abstract

We discuss a class of constraint-based
grammars, Lexicalized Well-Founded
Grammars (LWFGs) and present the
theoretical underpinnings for learning
these grammars from a representative
set of positive examples. Given several
assumptions, we define the search space
as a complete grammar lattice. In order
to prove a learnability theorem, we give
a general algorithm through which the
top and bottom elements of the complete
grammar lattice can be built.

1 Introduction

There has been significant interest in grammar
induction on the part of both formal languages
and natural language processing communities. In
this paper, we discuss the learnability of a re-
cently introduced constraint-based grammar for-
malism for deep linguistic processing, Lexical-
ized Well-Founded Grammars (LWFGs) (Mure-
san, 2006; Muresan and Rambow, 2007; Mure-
san, 2011). Most formalisms used for deep lin-
guistic processing, such as Tree Adjoining Gram-
mars (Joshi and Schabes, 1997) and Head-driven
Phrase Structure Grammar (HPSG) (Pollard and
Sag, 1994) are not known to be accompanied
by a formal guarantee of polynomial learnabil-
ity. While stochastic grammar learning for sta-
tistical parsing for some of these grammars has
been achieved using large annotated treebanks
(e.g., (Hockenmaier and Steedman, 2002; Clark
and Curran, 2007; Shen, 2006)), LWFG is suited
to learning in resource-poor settings. LWFG’s
learning is a relational learning framework which
characterizes the importance of substructures in
the model not simply by frequency, as in most pre-
vious work, but rather linguistically, by defining a

notion of representative examples that drives the
acquisition process.

LWFGs can be seen as a type of Definite
Clause Grammars (Pereira and Warren, 1980)
where: 1) the Context-Free Grammar backbone
is extended by introducing a partial ordering re-
lation among delexicalized nonterminals (well-
founded), 2) nonterminals are augmented with
strings and their syntactic-semantic representa-
tions; and 3) grammar rules have two types of
constraints: one for semantic composition and
one for ontology-based semantic interpretation
(Muresan, 2006). In LWFG every string w is as-
sociated with a syntactic-semantic representation
called semantic molecule

(
h
b

)
. We call the tuple

(w,
(
h
b

)
) a syntagma:

(big table,
0BBBBBB@h

264cat nphead X
nr sg

375
b

D
X1.isa = big, X .Y =X1, X .isa=table

E

1CCCCCCA
)

The language generated by a LWFG consists of
syntagmas, and not strings.

There are several properties and assumptions
that are essential for LWFG learnability: 1) par-
tial ordering relation on the delexicalized nonter-
minal set (well-founded property); 2) each string
has its linguistic category known (e.g., np for the
phrase big table); 3) LWFGs are unambiguous;
4) LWFGs are non-terminally separable (Clark,
2006). Regarding unambiguity, we need to em-
phasize that unambiguity is relative to a set of
syntagmas (pairs of strings and their syntactic-
semantic representations) and not to a set of natu-
ral language strings. For example, the sentence I
saw the man with a telescope is ambiguous at the
string level (PP -attachment ambiguity), but it is
unambiguous if we consider the syntagmas asso-
ciated with it.

171



In this paper, for clarity of presentation, we
make abstraction of the semantic representation
and grammar constraints, and discuss the theo-
retical underpinnings of Well-Founded Grammars
(WFGs), which have all the properties of LWFGs
that assures polynomial learnability. By defin-
ing the operational and denotational semantics of
WFGs, we are able to formally define the repre-
sentative set of WFGs. Giving several assump-
tions, we define the search space for WFG learn-
ing as a complete grammar lattice by defining the
least upper bound and the greatest lower bound
operators. The grammar lattice preserves the
parsing of the representative set. We give a the-
orem showing that this lattice is a complete gram-
mar lattice. In order to give a learnability theorem,
we give a general algorithm through which the top
and the bottom elements of the complete grammar
lattice can be built. The theoretical results ob-
tained in this paper hold for the LWFG formalism.
This theoretical result proves that the practical al-
gorithms introduced by Muresan (2011) converge
to the same target grammar.

2 Well-Founded Grammars

Well-Founded Grammars are a subclass of
Context-Free Grammars where there is a partial
ordering relation on the set of non-terminals.
Definition 1. A Well-Founded Grammar (WFG)
is a 5-tuple, G = 〈Σ, NG,�, P, S〉 where:

1. Σ is a finite set of terminal symbols.
2. NG is a finite set of nonterminal symbols,

where NG ∩ Σ = ∅ .
3. � is a partial ordering relation on the set of

nonterminals NG
4. P is the set of grammar rules, P = PΣ∪PG,
PΣ ∩ PG = ∅, where:
a) PΣ is the set of grammar rules whose
right-hand side are terminals, A → w,
where A ∈ NG and w ∈ Σ (empty string
cannot be derived). We denote pre(NG) ⊆
NG the set of pre-terminals, pre(NG) =
{A|A ∈ NG, w ∈ Σ, A→ w ∈ PΣ}.
b) PG is the set of grammar rules A →
B1 . . . Bn, where A ∈ (NG − pre(NG)),
Bi ∈ NG. For brevity, we denote a rule by
A → β, where A ∈ (NG − pre(NG)), β ∈
N+G . For every grammar rule A → β ∈ PG
there is a direct relation between the left-
hand side nonterminal A and all the nonter-
minals on the right-hand side Bi ∈ β (i.e.,

Σ = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9,+,−, ∗,÷, (, )}
NG = {D,Sum,Prod, Lbr,Rbr,N, F, T,E}
pre(NG) = {D,Sum,Prod, Lbr,Rbr}

PΣ PG
D → 0|1|2|3|4|5|6|7|8|9 N → D
Sum→ +|− N → N D
Prod→ ∗|÷ F → N
Lbr → ( F → Lbr E Rbr
Rbr →) T → F

T → T Prod F
E → T
E → E Sum T

Figure 1: A WFG for Mathematical Expressions

A � Bi, or Bi � A). If for all Bi ∈ β
we have that A � Bi and A 6= Bi, the
grammar rule A → β is an ordered non-
recursive rule. Each nonterminal symbol
A ∈ (NG − pre(NG)) is a left-hand side
in at least one ordered non-recursive rule. In
addition, the empty string cannot be derived
from any nonterminal symbol and cycles are
not allowed.

5. S ∈ NG is the start nonterminal symbol, and
∀A ∈ NG, S � A (we use the same notation
for the reflexive, transitive closure of �).

Besides the partial ordering relations �, in
WFGs the set of production rules P is split in
PΣ and PG. For learning, PΣ is given, while the
grammar rules in PG are learned.

In Figure 1 we give a WFG for Mathematical
Expressions that will be used as a simple illus-
trative example to present the formalism and the
foundation of the search space for WFG learning.

Every CFG G = 〈Σ, NG, PΣ ∪ PG, S〉 can
be efficiently tested to see whether it is a Well-
Founded Grammar.

The derivation in WFGs is called ground
derivation and it can be seen as the bottom
up counterpart of the usual derivation. Given
a WFG, G, the ground derivation relation, ∗G⇒,
is defined as: A→w

A
∗G⇒w

(A → w ∈ PΣ), and
Bi
∗G⇒wi, i=1,...,n A→B1...Bn

A
∗G⇒w w=w1...wn

(w ∈ Σ+).
The language of a grammar G is the set of all

strings generated from the start symbol S, i.e.,
L(G) = {w|w ∈ Σ+, S ∗G⇒ w}. The set of all
strings generated by a grammar G is Lw(G) =

{w|w ∈ Σ+, ∃A ∈ NG, A ∗G⇒ w}. Extending the

172



notation, the set of strings generated by a nonter-
minal A of a grammar G is Lw(A) = {w|w ∈
Σ+, A ∈ NG, A ∗G⇒ w}, and the set of strings
generated by a rule A → β of a grammar G is
Lw(A → β) = {w|w ∈ Σ+, (A → β) ∗G⇒ w},
where (A → β) ∗G⇒ w denotes the ground deriva-
tion A ∗G⇒ w obtained using the rule A→ β in the
last derivation step. For a WFG G, we call a set
of substrings Ew ⊆ Lw(G) a sublanguage of G.
Operational Semantics of WFGs. It has been
shown that the operational semantics of a CFG
corresponds to the language of the grammar
(Wintner, 1999). Analogously, the operational se-
mantics of a WFG, G, is the set of all strings gen-
erated by the grammar, Lw(G).

Denotational Semantics of WFGs. As discussed
in literature (Pereira and Shieber, 1984; Wint-
ner, 1999), the denotational semantics of a gram-
mar is defined through a fixpoint of a transforma-
tional operator associated with the grammar. Let
I ⊆ Lw(G) be a subset of all strings generated by
a grammarG. We define the immediate derivation
operator TG : 2Lw(G) → 2Lw(G), s.t.: TG(I) =
{w ∈ Lw(G)| if (A→ B1 . . . Bn) ∈ PG ∧ Bi ∗G⇒
wi ∧ wi ∈ I then A ∗G⇒ w}. If we denote
TG ↑ 0 = ∅ and TG ↑ (i + 1) = TG(TG ↑ i),
then we have that for i = 1, TG ↑ 1 = TG(∅) =
{w ∈ Lw(G)|A ∈ pre(NG), A ∗G⇒ w}.This cor-
responds to the strings derived from preterminals,
i.e., w ∈ Σ. TG is analogous with the immediate
consequence operator of definite logic programs
(i.e., no negation) (van Emden and Kowalski,
1976; Denecker et al., 2001). TG is monotonous
and hence the least fixpoint always exists (Tarski,
1955). This least fixpoint is unique, as for def-
inite logic programs (van Emden and Kowalski,
1976). We have lfp(TG) = TG ↑ ω, where ω
is the minimum limit ordinal. Thus, the denota-
tional semantics of a grammar G can be seen as
the least fixpoint of the immediate derivation op-
erator. An assumption for learning WFGs is that
the rules corresponding to grammar preterminals,
A→ w ∈ PΣ, are given, i.e., TG(∅) is given.

As in the case of definite logic programs, the
denotational semantics is equivalent with the op-
erational one, i.e., Lw(G) = lfp(TG) . Based
on TG we can define the ground derivation length
(gdl) for strings and the minimum ground deriva-
tion length (mgdl) for grammar rules, which are

key concepts in defining the representative set ER
of a WFG.

gdl(w) = min
w∈TG↑i

(i)

mgdl(A→ β) = min
w∈Lw(A→β)

(gdl(w))

2.1 Properties and Principles for WFG
Learning

In this section we present the main properties of
WFGs, discussing their importance for learning.

1) Partial ordering relation � (well-founded).
In WFGs, the partial ordering relation � on the
nonterminal set NG allows the total ordering of
grammar nonterminals and grammar rules, which
allows the bottom-up learning of WFGs. WFG
rules can be ordered or non-ordered, and they
can be recursive or non-recursive. In addition,
from the definition of WFGs, every non-terminal
is a left-hand side in at least one ordered non-
recursive rule, cycles are not allowed and the
empty string cannot be derived, properties that
guarantee the termination condition for learning.

2) Category Principle. Wintner (Wintner, 1999)
calls observables for a grammar G all the deriv-
able strings paired with the non-terminal that de-
rives them: Ob(G) = {〈w,A〉|w ∈ Σ, A ∈
NG, A

∗G⇒ w}, i.e., w ∈ Lw(A). We call w
a constituent and A its category. For example,
we can say that 1+1 is a constituent having the
category expression (E), 1*1 is a constituent hav-
ing the category term (T), and 1 is a constituent
having the categories digit(D), number (N), fac-
tor (F), term (T), expression (E). The Category
Principle for WFGs states that the observables are
known a-priori. When learning WFGs, the input
to the learner are observables 〈w,A〉. The cate-
gory is used by the learner as the name of the left-
hand side (lhs) nonterminal of the learned gram-
mar rule. The Category Principle is met for natu-
ral language, where observables (i.e., constituents
and their linguistic categories) ca be identified:
e.g., 〈formal proposal, NP〉, 〈very loud, ADJP〉.
3) Representative Set of a WFG (ER). Given
an unambiguous1 Well-Founded Grammar G, a
set of observables ER is called a representa-
tive set of G iff for each rule (A → β) ∈ PG
there is a unique observable 〈w,A〉 ∈ ER s.t.

1A WFG G is unambiguous if every string in L(G) has
only one derivation.

173



ID ER Egen − ER
1 〈1, N〉
2 〈11, N〉 〈111, N〉
3 〈1, F 〉 〈11, F 〉
4 〈(1), F 〉 〈(11), F 〉 〈((1)), F 〉 〈(1 ∗ 1), F 〉 〈(1 + 1), F 〉
5 〈1, T 〉 〈11, T 〉 〈(1), T 〉
6 〈1 ∗ 1, T 〉 〈1 ∗ 11, T 〉 〈11 ∗ 1, T 〉 〈1 ∗ (1), T 〉 〈(1) ∗ 1, T 〉 〈1 ∗ 1 ∗ 1, T 〉
7 〈1, E〉 〈11, E〉 〈(1), E〉 〈1 ∗ 1, E〉
8 〈1 + 1, E〉 〈11 + 1, E〉 〈1 + 11, E〉 〈(1) + 1, E〉 〈1 + (1), E〉 〈1 ∗ 1 + 1, E〉, 〈1 + 1 ∗ 1, E〉 〈1 + 1 + 1, E〉

Figure 2: Examples for Learning the WFG in Figure 1

gdl(w) = mgdl(A → β), where w ∈ Lw(G).
The nonterminal A is the category of the string
w. ER contains the most simple strings ground
derived by the grammar G paired with their cat-
egories. From this definition it is straightforward
to show that |ER| = |PG|. The partial ordering
relation on the nonterminal set induces a total or-
der on the representative set ER as well as on the
set of grammar rules PG.

For the WFG induction, the representative set
ER will be used by the learner to generate hy-
potheses (i.e., grammar rules). The category will
give the name of the left-hand side nonterminals
(lhs) of the learned grammar rules. An exam-
ple of a representative set ER for the mathemat-
ical expressions grammar from Figure 1 is given
in Figure 2. For generalization the learner will
use a generalization set of observables Egen =

{〈w,A〉|A ∈ NG ∧ A ∗G⇒ w}, where ER ⊆ Egen.
An example of a generalization setEgen for learn-
ing the WFG from Figure 1 is given in Figure 2.

4) Semantics of a WFG reduced to a general-
ization set Egen. Given a WFG G and a gener-
alization set Egen (not necessarily of G) the set

S(G) = {〈w,A〉|〈w,A〉 ∈ Egen ∧ A ∗G⇒ w}
is called the semantics of G reduced to the gen-
eralization set Egen. In other words, S(G) will
contain all the pairs 〈w,A〉 in the generalization
set whose strings w can be ground-derived by the
grammar G, w ∈ Lw(G). Given a grammar rule
rA ∈ PG, we call S(rA) = {〈w,A〉|lhs(rA)=A ∧
〈w,A〉 ∈ Egen∧rA ∗G⇒ w} the semantics of rA re-
duced toEgen. The cardinality of S is used during
learning as performance criterion.

5) ER-parsing-preserving. We present the
rule specialization step and the rule general-
ization step of unambiguous WFGs, such that
they are ER-parsing-preserving and are the in-
verse of each other. The property of ER-parsing-
preserving means that both the initial and the spe-

cialized/generalized rules ground-derive the same
stringw of the observable 〈w,A〉 ∈ ER. The rule
specialization step:

A→αBγ B→β
A→αβγ

is ER-parsing-preserving, if there exists
〈w,A〉 ∈ ER and rg ∗G⇒ w and rs ∗G

′
⇒ w, where rg

= A → αBγ , rB = B → β, rs = A → αβγ and
rg ∈ PG, rB ∈ PG ∩ PG′ , rs ∈ PG′ . We write
rg

rB
` rs.2 The rule generalization step, which

is also ER-parsing-preserving, is defined as the
inverse of the rule specialization step and denoted

by rs
rB
a rg.

Since 〈w,A〉 is an element of the representa-
tive set, w is derived in the minimum number of
derivation steps, and thus the rule rB is always
an ordered, non-recursive rule. Examples of ER-
parsing-preserving rule specialization steps are
given in Figure 3, where all rules derive the same
representative example 1+1. In the derivation step

E → N Sum D
N→D
` E → D Sum D the

ordered, non recursive rule rB = N → D is
used. If the recursive rule N → N D were
used, we would obtain a specialized rule E →
N D Sum D which does not preserve the pars-
ing of the representative example 1+1.

From both the specialization and the general-
ization step we have that Lw(rg) ⊇ Lw(rs).

The goal of the rule specialization step is to ob-
tain a new target grammar G′ from G by special-

izing a rule of G (G
r
` G′). Extending the no-

tation to allow for the transitive closure of rule
specialization, we have that G

∗
` G′, and we say

that the grammarG′ is specialized from the gram-
mar G, using a finite number of rule specializa-
tion steps that are ER-parsing-preserving. Simi-
larly, the goal of the rule generalization step is to

2Grammar G is non-terminally separable (Clark, 2006).

174



E → E Sum T
`

uukkkk
kkkk

kkkk
kkk

` ))SSS
SSSS

SSSS
SSSS

E → T Sum T
`

uukkkk
kkkk

kkkk
kkk

` ))SSS
SSSS

SSSS
SSSS

E → E Sum F
`

uukkkk
kkkk

kkkk
kkk

` ))SSS
SSSS

SSSS
SSSS

E → F Sum T
`

uukkkk
kkkk

kkkk
kkk

` ))SSS
SSSS

SSSS
SSSS

E → T Sum F
`

uukkkk
kkkk

kkkk
kkk

` ))SSS
SSSS

SSSS
SSSS

E → E Sum N
`

uukkkk
kkkk

kkkk
kkk

` ))SSS
SSSS

SSSS
SSSS

E → N Sum T
`

uukkkk
kkkk

kkkk
kk

` ))SSS
SSSS

SSSS
SSSS

E → F Sum F
`

uukkkk
kkkk

kkkk
kkk

` ))SSS
SSSS

SSSS
SSSS

E → T Sum N
`

uukkkk
kkkk

kkkk
kkk

` ))SSS
SSSS

SSSS
SSSS

E → E Sum D
`

uukkkk
kkkk

kkkk
kkk

E → D Sum T

` ))SSS
SSSS

SSSS
SSS

E → N Sum F
`

uukkkk
kkkk

kkkk
kkk

` ))SSS
SSSS

SSSS
SSSS

E → F Sum N
`

uukkkk
kkkk

kkkk
kkk

` ))SSS
SSSS

SSSS
SSSS

E → T Sum D
`

uukkkk
kkkk

kkkk
kkk

E → D Sum F

` ))SSS
SSSS

SSSS
SSSS

E → N Sum N
`

uukkkk
kkkk

kkkk
kkk

` ))SSS
SSSS

SSSS
SSSS

E → F Sum D
`

uukkkk
kkkk

kkkk
kkk

E → D Sum N

` ))SSS
SSSS

SSSS
SSSS

E → N Sum D
`

uukkkk
kkkk

kkkk
kkk

E → D Sum D

Figure 3: ER-parsing preserving rule specialization steps for the grammar in Figure 1.

obtain a new target grammar G from G′ by gen-
eralizing a rule of G′. Extending the notation to
allow for transitive closure of rule generalization,

we have thatG′
∗
a G, and we say that the grammar

G is generalized from the grammar G′ using a fi-
nite number of rule generalization steps that are
ER-parsing-preserving. That is, ER is a repre-
sentative set for both G and G′. The ER-parsing-
preserving property allows us to define a class of
grammars that form a complete grammar lattice
used as search space for WFGs induction, as de-
tailed in the next section.

6) Conformal Property. A WFG G is called
normalized w.r.t. a generalization set Egen, if
none of the grammar rules rs of G can be fur-
ther generalized to a rule rg by the rule general-
ization step such that S(rs) ⊂ S(rg). A WFG
G is conformal w.r.t. a generalization set Egen
iff ∀〈w,A〉 ∈ Egen we have that w ∈ Lw(G),
andG is unambiguous and normalized w.r.t. Egen
and the rule specialization step guarantees that
S(rg) ⊃ S(rs) for all grammars specialized from
G. This property allows learning only from posi-
tive examples.

7) Chains. We define a chain as a set of ordered
unary branching rules: {Bk → Bk−1, . . . , B2 →
B1, B1 → β} such that all these rules ground-
derive the same string w ∈ Σ+ (i.e., Bk �
Bk−1 · · · � B1 and B0 = β, such that Bi ∗G⇒ w
for 0 ≤ i ≤ k, where Bi ∈ NG for 1 ≤ i ≤
k). For our grammar {E → T, T → F, F →
N,N → D} is a chain, all these rules ground-
deriving the string 1. Chains are used to general-
ize grammar rules during WFG learning (Fig. 3).

All the above mentioned properties are used
to define the search space of WFG learning as a

complete grammar lattice.

2.2 Grammar Lattice as Search Space

In this section we formally define a grammar lat-
tice L = 〈L,w〉 that will be the search space for
WFG learning. We first define the set of lattice
elements L.

Let > be a WFG conformal to a generalization
setEgen that includes the representative setER of

the grammar > (Egen ⊇ ER). Let L = {G|>
∗
`

G} be the set of grammars specialized from >.
We call > the top element of L, and ⊥ the bot-
tom element of L, if ∀G ∈ L,>

∗
` G ∧ G

∗
` ⊥.

The bottom element, ⊥, is the grammar special-
ized from >, such that the right-hand side of all
grammar rules contains only preterminals. We
have S(>) = Egen and S(⊥) ⊇ ER.

There is a partial ordering among the elements
ofL (the subsumptionw), which we define below.
Definition 2. If G,G′ ∈ L, we say that G sub-
sumes G′, G w G′, iff G

∗
` G′ (i.e., G′ is special-

ized from G, and G is generalized from G′)

We have that for G,G′ ∈ L, if G w G′ then
S(G) ⊇ S(G′). That means that the subsumption
relation is semantic based.

In sum, the set L contains the grammars spe-
cialized from>, while the binary subsumption re-
lation w establishes a partial ordering in L. The
top element of the lattice> is a normalized WFG,
the bottom element ⊥ is a grammar specialized
from >, whose rules’ right-hand sides consist of
preterminals, and all the other lattice elements are
WFGs that preserve the parsing of the representa-
tive set. In Figure 3, the rule (E → E Sum T ) ∈
P>, the rule (E → D SumD) ∈ P⊥ and all rules
ground-derive the representative string 1+1.

175



top
side

boundary

A

side
bottom

w

rA
∗"⇒ w

rA ∈ P"

rhs(r′A), r
′
A ∈ PG

B1

B2 B3

B4

(a) Boundary

r
∗"⇒ w

G1G1 ! G2

G1

G2

G2

G1 " G2

r ∈ P"

w

(b) lub and glb operators

E

}
}

}
}

}
}

}
}

}
}

�
�
�
�
�
�
�

A
A

A
A

A
A

A
A

A
A r> : E → E Sum T

G1 gG2 rG1gG2 : E → T Sum T

E

�
�
� Sum

�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�

G1

T

�
�
� rG1 : E → D Sum T

T

�
�
�

G2

F

�
�
� rG2 : E → T Sum N

F

�
�
� G1 fG2 N

�
�
�

G2

rG1fG2 : E → D Sum N

N

�
�
� D

�
�
�
�
�
�
�

D

�
�
�

G1

1 + 1

(c) Example for lub and glb

Figure 4

In order for L = 〈L,w〉 to form a lattice, we
must define two operators: the least upper bound
(lub), g and the greatest lower bound (glb), f,
such that for any two elements G1, G2 ∈ L, the
elements G1 g G2 ∈ L and G1 f G2 ∈ L exist
(Tarski, 1955).

We first introduce the concept of boundary. Let
rA ∈ P> be a rule in grammar >, and r′A its spe-
cialized rule in grammar G ( > w G) (see Fig-
ure 4a). Let pt(rA

∗>⇒ w) be the parse tree corre-
sponding to the ground-derivation rA

∗>⇒ w. We
call boundary of a grammar G ∈ L relative to
pt(rA

∗>⇒ w),3 the right-hand side of the corre-
sponding rule r′A ∈ PG, r′A

∗G⇒ w, i.e. bd(G) =
{B|r′A ∈ PG ∧ B ∈ rhs(r′A)}4(see Figure 4a).
For the example in Figure 4c, the rule in grammar
> is E → E Sum T , the rule in grammar G1 is
E → D Sum T . Thus, bd(G1) = {D,Sum, T}
We define the bottom-side bs(G) of a grammarG
relative to the parse tree pt(rA

∗>⇒ w), as the for-
est composed of all the subtrees in pt(rA

∗>⇒ w)
whose roots are on bd(G) (e.g., subtrees with
roots at B1, B2, B3, B4 in Figure 4a). For the
example in Figure 4c, bs(G1) will be the forest of

subtrees of the parse tree pt(r>
∗>⇒ 1+1) with the

roots D, Sum and T on the boundary bd(G1) of
grammar G1. We define the top-side ts(G) of a
grammarG relative to the parse tree pt(rA

∗>⇒ w),
as the subtree in pt(rA

∗>⇒ w) rooted at A and

3All grammars G ∈ L are ER-parsing-preserving and all
boundaries of G are in the parse trees of the ground deriva-
tions of > grammar rules.

4The notation of bd(G), ts(G), bs(G) ignores the rule
relative to which these concepts are defined, and in the re-
mainder of this paper we implicitly understand that the rela-
tions hold for all grammar rules.

whose leaf nodes are on bd(G) (e.g., B1, B2, B3
and B4 in Figure 4a). For the example in Figure
4c, ts(G1) will be the subtree of the parse tree

pt(r>
∗>⇒ 1+1) rooted at E and the leaf nodesD,

Sum and T on the boundary bd(G1) of the gram-
mar G1. We have that ts(G) ∩ bs(G) = bd(G),
ts(G) ∪ bs(G) = pt(rA ∗>⇒ w).

For any two elements G1, G2 ∈ L, the lub ele-
ment of G1, G2 is the minimum element that has
the boundary above the boundaries of G1 and G2.
The glb element of G1, G2 is the maximum ele-
ment that has the boundary below the boundaries
of G1 and G2. Thus, lub and glb are defined such
that for all grammar rules we have:

ts(G1 gG2) = ts(G1) ∩ ts(G2)
bs(G1 fG2) = bs(G1) ∩ bs(G2)

(1)

as can be seen in Figure 4b, 4c.5 In order to have a
complete lattice, the property must hold ∀G ⊆ L:

ts(gG∈GG) =
⋂

G∈G
ts(G)

bs(fG∈GG) =
⋂

G∈G
bs(G)

(2)

Lemma 1. L = 〈L,w〉 together with the lub
and glb operators guarantees that for any two
grammars G1, G2 ∈ L the following property
holds:G1 gG2 w G1, G2 w G1 fG2
Theorem 1. L = 〈L,w〉 together with the lub
and glb operators forms a complete lattice.

Proof. Besides the property given in Lemma 1,
lub and glb operators are computed w.r.t. (2),
such that we have ts(gG∈LG) =

⋂
G∈L ts(G) =

5The intersection of two trees is the maximum common
subtree of those two trees, and similarly for forests of trees.

176



S(!) = Egen

S(G2)

S(G1)

ER

S(⊥)

S(G1 ! G2)

S(G1 " G2)

Figure 5: WFG semantics reduced to Egen

ts(>), bs(fG∈LG) =
⋂
G∈L bs(G) = bs(⊥),

which gives the uniqueness of > and ⊥ ele-
ments.

Similar to the subsumption relation w, the
lub g and glb f operators are semantic-based.
In the complete lattice L = 〈L,w〉, ∀G1, G2 ∈ L
we have:

S(G1 gG2) ⊇ S(G1) ∪ S(G2).
S(G1 fG2) ⊆ S(G1) ∩ S(G2)

(3)

Thus, the complete grammar lattice is semantic-
based (Figure 5). It is straightforward to prove
that L = 〈L,w〉 has all the known properties
(i.e., idempotency, commutativity, associativity,
absorption, > and ⊥ laws, distributivity).

2.3 Learnability Theorem
In oder to give a learnability theorem we need to
show that ⊥ and > elements of the lattice can be
built. Through Algorithm 1 we show that giving
the set of examples ER and Egen, the ⊥ grammar
can be built and the> grammar can be learned by
generalizing the⊥ grammar. The grammar gener-
alization is determinate if the rule generalization
step is determinate. Before describing Algorithm
1 we introduce the concepts of determinate gen-
eralizable and give a lemma that states that given
a grammar> conformal with Egen, for any gram-
mars G specialized from >, all the grammar rules
are determinate generalizable if all the chains of
the > grammar are known.
Definition 3. A grammar rule r′A ∈ PG is de-
terminate generalizable if for β ∈ rhs(r′A) there
exists a unique rule rB = (B → β) such that
r′A

rB
a rA with S(r′A) ⊂ S(rA). We use the nota-

tion r′A
1⊂
a rA for the determinate generalization

step with semantic increase.

The only rule generalization steps allowed in
the grammar induction process are those which
guarantee the relation S(rs) ⊂ S(rg) that ensures
that all the generalized grammars belong to the
grammar lattice. This property allows the gram-
mar induction based only on positive examples.

We use the notation r′A
rB⊂
a rA for the generaliza-

tion step with semantic increase.
This step can be nondeterminate due to chain

rules. Let ch> be a chain of rules in a WFG >
conformal w.r.t a generalization set Egen, ch> =
{Bk → Bk−1, . . . , B2 → B1, B1 → β}. All
the chain rules, but the last, are unary branching
rules. The last rule is the minimal chain rule. For
our example, ch> = {E → T, T → F, F →
N,N → D}. For the ⊥ grammar of a lattice
that has > as its top element, the aforementioned
chain becomes ch⊥ = {Bk → β⊥, . . . , B2 →
β⊥, B1 → β⊥}, where β⊥ contains only preter-
minals and the rule order is unknown. By the ER-
parsing-preserving property of the rule specializa-
tion step, the same string is ground-derived from
the ch⊥ rules. Thus, the ⊥ grammar is ambigu-
ous. For our example, ch⊥ = {E → D,T →
D,F → D,N → D}.

We denote by ch = {rk, . . . , r2, r1}, one or
more chains in any lattice grammar, where the
rule order is unknown. The minimal chain rules
rm can always be determined if rm ∈ ch s.t.
∀r ∈ ch − {rm} ∧ rm

r
a rmg we have that

S(rm) = S(rmg) (see also MinRule algorithm).
By the consequence of the conformal property, the

generalization step rm
r
a rmg is not allowed, since

it does not produce any increase in rule seman-
tics. That is, a minimal chain rule cannot be gen-
eralized by any other chain rule with an increase
in its semantics. Given ch⊥ and the aforemen-
tioned property of the minimal chain rules we can
recover ch> by Chains Recovery algorithm.

Lemma 2. Given a WFG> conformal w.r.t a gen-
eralization set Egen, for any grammar G derived
from > all rules are determinate generalizable if
all chains of the grammar > (i.e., all ch>) are
known (i.e., recovered by Chains Recovery
algorithm).

Proof. The only case of rule generalization step
nondeterminism with semantic increase is intro-
duced by the derivation of the unary branching
rules of ordered ch>, which yields the unordered

177



ID P⊥ P>
1 N → D N → D
2 N → D D N → N D
3 F → D F → N
4 F → Lbr D Rbr F → Lbr E Rbr
5 T → D T → F
6 T → D Prod D T → T Prod F
7 E → D E → T
8 E → D Sum D E → E Sum T

Figure 6: Examples of P⊥ and learned P>

ch⊥, where Bi → β⊥
Bj→β⊥⊂
a Bi → Bj holds

for allBj ≺ Bi. Thus, keeping (or recovering) the
ordered ch> in any grammar G derived from >,
all the other grammar rules are determinate gen-
eralizable.

We now introduce Algorithm 1, which given
the representative set ER and the generalization
set Egen, builds the ⊥ and > grammars. First, an
assumption in our learning model is that the rules
corresponding to the grammar preterminals (PΣ)
are given. Thus, for a given representative setER,
we can build the grammar⊥ in the following way:
for each observable 〈w,A〉 ∈ ER the category A
gives the name of the left-hand side nonterminal
of the grammar rule, while the right-hand side is
constructed using a bottom-up active chart parser
(Kay, 1973) (line 1 in Algorithm 1). For our math-
ematical expressions example, given ER in Fig-
ure 2 and PΣ in Figure 1, the rules of the bottom
grammar P⊥ are given in Figure 6.

Algorithm 1 Top(ER, Egen)
1: P⊥ ← Bottom(ER)
2: P> ← Chains Recovery (P⊥, ER, Egen)
{P> is determinate generalizable }

3: while ∃r ∈ P> s.t. r
1⊂
a rg do

4: r ← rg;
5: end while
6: return P>

In order to build the > element (lines 2-5 in
Algorithm 1), we first need to apply the Chain
recovery algorithm to the lattice ⊥ grammar
(line 2 in Algorithm 1). Chains Recovery
first detects all ch = ch⊥ which contain rules
with identical right-hand side (line 5-6). Then,
all ch⊥ rules are transformed in ch> by general-
izing them through the minimal chain rule (lines

10-17). The generalization step r
rm⊂
a rg guar-

Algorithm 2 Chains Recovery
1: Input: P⊥, ER, Egen
2: Output: P⊥ which contains all ch>
3: while ER 6= ∅ do
4: 〈w, c〉 ← first(ER)
5: ch← {r ∈ P⊥|r ∗⊥⇒ w} {ch = ch⊥}
6: lch← {lhs(r)|r ∈ ch}
7: for each c ∈ lch do
8: ER ← ER − {〈w, c〉}
9: end for

10: while |ch| > 1 do
11: rm ← MinRule(ch)
12: ch← ch− {rm}
13: lch← lch− {lhs(rm)}
14: for each r ∈ P⊥∧lhs(r) ∈ lch s.t. r

rm⊂
a rg

do
15: r ← rg
16: end for
17: end while
18: end while
19: return: P⊥

Algorithm 3 MinRule
1: Input: the chain ch
2: for each rm ∈ ch do
3: find← true
4: for each r ∈ ch− {rm} do
5: if rm

r

a rmg ∧ S(rm) ⊂ S(rmg) then
6: find← false
7: end if
8: end for
9: if find == true then

10: return rm
11: end if
12: end for

antees the semantic increase S(rg) ⊃ S(r) for all
the rules r which are generalized through rm, thus
being the inverse of the rule specialization step in
the grammar lattice. The rules r are either chain
rules or rules having the same left-hand side as
the chain rules. The returned set P⊥ contains all
unary branching rules (ch>) of the > grammar.
The efficiency of Chain Recovery algorithm
is O(|ER| ∗ |β| ∗ |Egen|). Therefore, in Algorithm
1 the set P> initially contains determinate gener-
alizable rules and the while loop (lines 3-5,
Alg 1) can determinately generalize all the gram-
mar rules. In Figure 6, the grammar P> is learned
by Algorithm 1, based only on the examples in
Figure 2 and PΣ in Figure 1.

Theorem 2 (Learnability Theorem). If ER is

178



the representative set of a WFG G conformal
w.r.t a generalization set Egen ⊇ ER, then
Top(ER, Egen) algorithm computes the lattice >
element such that S(>) = Egen.

Proof. Since G is normalized, none of its
rule can be generalized with increase in se-
mantics. Starting with the ⊥ element, after
Chains Recovery all rules that can be gen-
eralized with semantic increase through the rule
generalization step, are determinate generaliz-
able. This means that the grammar generalization
sequence⊥, G1, . . . , Gn,>, ensures the semantic
increase of S(Gi) so that the generalization pro-
cess ends at the semantic limit S(>) = Egen.

For WFGs which have rules that can be ei-
ther left or right recursive, the top element is
unique only if we impose a direction of gener-
alization in the rule’s right-hand side (e.g., left
to right). Another way to guarantee uniqueness
of the top element is to add constraints at the
grammar rules. In our example, if we augment
de grammar nonterminals with expression val-
ues (semantic interpretation) and we add con-
straints at the grammar rules we have E(v) →
E(v1) Sum(op) T (v2) : {v ← v1 op v2}. With
the generalization example 〈5 − 3 − 1, E(1)〉 ∈
Egen we can generalize the rule E → T Sum T
only to E → E Sum T and not to E →
T Sum E because 1 = (5 − 3) − 1 and 1 6=
5 − (3 − 1). For our Lexicalized Well-Founded
Grammars this problem is solved by associating
strings with their syntactic-semantic representa-
tions and by having semantic compositional con-
straints at the grammar rule level.

3 Conclusions

In this paper, we discussed the learnability of Lex-
icalized Well-Founded Grammars. We introduced
the class of well-founded grammars and presented
the theoretical underpinnings for learning these
grammars from a representative set of positive ex-
amples. We proved that under several assump-
tions the search space for learning these gram-
mars is a complete grammar lattice. We presented
a general algorithm which builds the top and the
bottom elements of the complete grammar lattice
and gave a learnability theorem. The theoretical
results obtained in this paper hold for the LWFG
formalism, which is suitable for deep linguistic
processing.

References
Stephen Clark and James R. Curran. 2007. Wide-

coverage efficient statistical parsing with ccg and
log-linear models. Computational Linguistics,
33(4).

Alexander Clark. 2006. PAC-learning unambiguous
NTS languages. In Proceedings of the 8th Interna-
tional Colloquium on Grammatical Inference (ICGI
2006), pages 59–71.

Marc Denecker, Maurice Bruynooghe, and Victor W.
Marek. 2001. Logic programming revisited: Logic
programs as inductive definitions. ACM Transac-
tions on Computational Logic, 2(4):623–654.

Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with combina-
tory categorial grammar. In Proceedings of the ACL
’02, pages 335–342.

Aravind Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In G. Rozenberg and A. Sa-
lomaa, editors, Handbook of Formal Languages,
volume 3, chapter 2, pages 69–124. Springer,
Berlin,New York.

Martin Kay. 1973. The MIND system. In Randall
Rustin, editor, Natural Language Processing, pages
155–188. Algorithmics Press, New York.

Smaranda Muresan and Owen Rambow. 2007. Gram-
mar approximation by representative sublanguage:
A new model for language learning. In Proceed-
ings of ACL’07.

Smaranda Muresan. 2006. Learning Constraint-
based Grammars from Representative Examples:
Theory and Applications. Ph.D. thesis, Columbia
University.

Smaranda Muresan. 2011. Learning for deep lan-
guage understanding. In Proceedings of IJCAI-11.

Fernando C. Pereira and Stuart M. Shieber. 1984. The
semantics of grammar formalisms seen as computer
languages. In Proceeding of the ACL 1984.

Fernando C. Pereira and David H.D Warren. 1980.
Definite Clause Grammars for language analysis.
Artificial Intelligence, 13:231–278.

Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press,
Chicago, Illinois.

Libin Shen. 2006. Statistical LTAG Parsing. Ph.D.
thesis, University of Pennsylvania, Philadelphia,
PA, USA. AAI3225543.

Alfred Tarski. 1955. Lattice-theoretic fixpoint theo-
rem and its applications. Pacific Journal of Mathe-
matics, 5(2):285–309.

Maarten H. van Emden and Robert A. Kowalski. 1976.
The semantics of predicate logic as a programming
language. Journal of the ACM, 23(4):733–742.

Shuly Wintner. 1999. Compositional semantics
for linguistic formalisms. In Proceedings of the
ACL’99.

179


