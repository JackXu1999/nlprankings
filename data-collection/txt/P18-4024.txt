



















































Moon IME: Neural-based Chinese Pinyin Aided Input Method with Customizable Association


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 140–145
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

140

Moon IME: Neural-based Chinese Pinyin Aided Input Method with
Customizable Association

Yafang Huang1,2,∗, Zuchao Li1,2,∗, Zhuosheng Zhang1,2, Hai Zhao1,2,†
1Department of Computer Science and Engineering, Shanghai Jiao Tong University

2Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China

{huangyafang, charlee, zhangzs}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn

Abstract

Chinese pinyin input method engine
(IME) lets user conveniently input Chi-
nese into a computer by typing pinyin
through the common keyboard. In addi-
tion to offering high conversion quality,
modern pinyin IME is supposed to aid us-
er input with extended association func-
tion. However, existing solutions for such
functions are roughly based on oversim-
plified matching algorithms at word-level,
whose resulting products provide limit-
ed extension associated with user input-
s. This work presents the Moon IME, a
pinyin IME that integrates the attention-
based neural machine translation (NMT)
model and Information Retrieval (IR) to
offer amusive and customizable associa-
tion ability. The released IME is im-
plemented on Windows via text services
framework.

1 Introduction

Pinyin is the official romanization representation
for Chinese and pinyin-to-character (P2C) which
concerts the inputted pinyin sequence to Chinese
character sequence is the core module of all pinyin
based IMEs. Previous works in kinds of literature
only focus on pinyin to the character itself, pay-
ing less attention to user experience with associa-
tive advances, let along predictive typing or auto-
matic completion. However, more agile associa-

∗These authors contribute equally. † Corresponding au-
thor. This paper was partially supported by National Key Re-
search and Development Program of China (No. 2017YF-
B0304100), National Natural Science Foundation of China
(No. 61672343 and No. 61733011), Key Project of Nation-
al Society Science Foundation of China (No. 15-ZDA041),
The Art and Science Interdisciplinary Funds of Shanghai Jiao
Tong University (No. 14JCRZ04).

tion outputs from IME predication may undoubt-
edly lead to incomparable user typing experience,
which motivates this work.

Modern IMEs are supposed to extend P2C with
association functions that additionally predict the
next series of characters that the user is attempting
to enter. Such IME extended capacity can be gen-
erally fallen into two categories: auto-completion
and follow-up prediction. The former will look
up all possible phrases that might match the us-
er input even though the input is incomplete. For
example, when receiving a pinyin syllable “bei”,
auto-completion module will predict “�¬” (bei-
jing, Beijing) or “Ìo” (beijing, Background) as
a word-level candidate. The second scenario is
when a user completes entering a set of words, in
which case the IME will present appropriate collo-
cations for the user to choose. For example, after
the user selects “�¬” (Beijing) from the candi-
date list in the above example, the IME will show
a list of collocations that follows the word Beijing,
such as “�” (city), “eÐ�” (Olympics).

This paper presents the Moon IME, a pinyin
IME engine with an association cloud platform,
which integrates the attention-based neural ma-
chine translation (NMT) model with diverse asso-
ciations to enable customizable and amusive user
typing experience.

Compared to its existing counterparts, Moon
IME has extraordinarily offered the following
promising advantages:
• It is the first attempt that adopts attentive NMT

method to achieve P2C conversion in both IME
research and engineering.
• It provides a general association cloud plat-

form which contains follow-up-prediction and ma-
chine translation module for typing assistance.
•With an information retrieval based module, it

realizes fast and effective auto-completion which
can help users type sentences in a more convenient



141

Pinyin Text 
Segmentation

NMT-based 
P2C Conversion

Association
Platform

Moon Input 
Method Engine

Text Services 
Framework

Question 
Answering

Machine 
Translation

Auto-
Completion

Customized 
Knowledge Base

Pi
ny
in
 

Co
nv
ers
ion

Re
su
lts

C
onversion
R
esults

Association 
List

Pinyin 

Candidates 
List

Pinyin 

Candidates 
List

输入拼音，根据拼音得到一系
列 hou xuan da an

Figure 1: Architecture of the proposed Moon IME.

and efficient manner.
• With a powerful customizable design, the

association cloud platform can be adapted to
any specific domains such as the fields of law
and medicine which contain complex specialized
terms.

The rest of the paper is organized as follows:
Section 2 demonstrates the details of our system.
Section 3 presents the feature functions of our re-
alized IME. Some related works are introduced in
Section 4. Section 5 concludes this paper.

2 System Details

Figure 1 illustrates the architecture of Moon IME.
The Moon IME is based on Windows Text Ser-
vices Framework (TSF) 1. Our Moon IME extends
the Open-source projects PIME2 with three main
components: a) pinyin text segmentation, b) P2C
conversion module, c) IR-based association mod-
ule. The nub of our work is realizing an engine to
stably convert pinyin to Chinese as well as giving
reasonable association lists.

2.1 Input Method Engine
Pinyin Segmentation For a convenient refer-
ence, hereafter a character in pinyin also refers to
an independent syllable in the case without caus-
ing confusion, and word means a pinyin syllable
sequence with respect to a true Chinese word.

1TSF is a system service available as a redistributable for
Windows 2000 and later versions of Windows operation sys-
tem. A TSF text service provides multilingual support and
delivers text services such as keyboard processors, handwrit-
ing recognition, and speech recognition.

2https://github.com/EasyIME/PIME

As (Zhang et al., 2017) proves that P2C con-
version of IME may benefit from decoding longer
pinyin sequence for more efficient inputting.
When a given pinyin sequence becomes longer,
the list of the corresponding legal character se-
quences will significantly reduce. Thus, we train
our P2C model with segmented corpora. We used
baseSeg (Zhao et al., 2006) to segment all tex-
t, and finish the training in both word-level and
character-level.

NMT-based P2C module Our P2C module is
implemented through OpenNMT Toolkit3 as we
formulize P2C as a translation between pinyin and
character sequences. Given a pinyin sequence X
and a Chinese character sequence Y , the encoder
of the P2C model encodes pinyin representation in
word-level, and the decoder is to generate the tar-
get Chinese sequence which maximizes P (Y |X)
using maximum likelihood training.

The encoder is a bi-directional long short-
term memory (LSTM) network (Hochreiter and
Schmidhuber, 1997). The vectorized inputs are
fed to forward LSTM and backward LSTM to ob-
tain the internal features of two directions. The
output for each input is the concatenation of the
two vectors from both directions:

←→
ht =

−→
ht ‖

←−
ht.

Our decoder is based on the global attention-
al model proposed by (Luong et al., 2015) which
takes the hidden states of the encoder into con-
sideration when deriving the context vector. The
probability is conditioned on a distinct contex-
t vector for each target word. The context vec-

3http://opennmt.net



142

tor is computed as a weighted sum of previously
hidden states. The probability of each candidate
word as being the recommended one is predicted
using a softmax layer over the inner-product be-
tween source and candidate target characters.

Our model is initially trained on two dataset-
s, namely the People’s Daily (PD) corpus and
Douban (DC) corpus. The former is extracted
from the People’s Daily from 1992 to 1998 that
has word segmentation annotations by Peking U-
niversity. The DC corpus is created by (Wu et al.,
2017) from Chinese open domain conversations.
One sentence of the DC corpus contains one com-
plete utterance in a continuous dialogue situation.
The statistics of two datasets is shown in Table 1.
With character text available, the needed parallel
corpus between pinyin and character texts is auto-
matically created following the approach proposed
by (Yang et al., 2012).

Chinese Pinyin

PD
# MIUs 5.04M
# Vocab 54.3K 41.1K

DC
# MIUs 1.00M
# Vocab 50.0K 20.3K

Table 1: MIUs count and vocab size statistics of our training
data. PD refers to the People’s Daily, TP is TouchPal corpus.

Here is the hyperparameters we used: (a) deep
LSTM models, 3 layers, 500 cells, (c) 13 epoch
training with plain SGD and a simple learning rate
schedule - start with a learning rate of 1.0; af-
ter 9 epochs, halve the learning rate every epoch,
(d) mini-batches are of size 64 and shuffled, (e)
dropout is 0.3. The pre-trained pinyin embed-
dings and Chinese word embeddings are trained
by word2vec (Mikolov et al., 2013) toolkit on
Wikipedia4 and unseen words are assigned unique
random vectors.

2.2 IR-based association module

We use IR-based association module to help us-
er type long sentences which can predict the w-
hole expected inputs according to the similarity
between user’s incomplete input and the candi-
dates in a corpus containing massive sentences. In
this work, we use Term Frequency-Inverse Docu-
ment Frequency (TF-IDF) to calculate the similar-
ity measurement, which has been usually used in

4https://dumps.wikimedia.org/zhwiki/20180201/zhwiki-
20180201-pages-articles-multistream.xml.bz2

text classification and information retrieval. The
TF (term-frequency) term is simply a count of the
number of times a word appearing in a given con-
text, while the IDF (invert document frequency)
term puts a penalty on how often the word appears
elsewhere in the corpus. The final TF-IDF score
is calculated by the product of these two terms,
which is formulated as:

TF-IDF(w, d,D) = f(w, d) × log N|{d∈D:w∈d}|
where f(w, d) indicates the number of times word
w appearing in context d, N is the total number
of dialogues, and the denominator represents the
number of dialogues in which the word w appears.

In the IME scenario, the TF-IDF vectors are
first calculated for the input context and each of
the candidate responses from the corpus. Given
a set of candidate response vectors, the one with
the highest cosine similarity to the context vector
is selected as the output. For Recall @ k, the top
k candidates are returned. In this work, we only
make use of the top 1 matched one.

3 User Experience Advantages

3.1 High Quality of P2C

We utilize Maximum Input Unit (MIU) Accuracy
(Zhang et al., 2017) to evaluate the quality of our
P2C module by measuring the conversion accura-
cy of MIU, whose definition is the longest unin-
terrupted Chinese character sequence inside a sen-
tence. As the P2C conversion aims to output a
ranked list of corresponding character sequences
candidates, the top-K MIU accuracy means the
possibility of hitting the target in the first K pre-
dicted items. We will follow the definition of
(Zhang et al., 2017) about top-K accuracy.

Our model is compared to other models in Table
2. So far, (Huang et al., 2015) and (Zhang et al.,
2017) reported the state-of-the-art results among
statistical models. We list the top-5 accuracy con-
trast to all baselines with top-10 results, and the
comparison indicates the noticeable advancement
of our P2C model. To our surprise, the top-5 result
on PD of our P2C module approaches the top-10
accuracy of Google IME. On DC corpus, the P2C
module with the best setting achieves 90.17% ac-
curacy, surpassing all the baselines. The compari-
son shows the high quality of our P2C conversion.

3.2 Association Cloud Platform

Follow-up Prediction An accurate P2C conver-
sion is only the fundamental requirement to build



143

DC PD
Top-1 Top-5 Top-10 Top-1 Top-5 Top-10

(Huang et al., 2015) 59.15 71.85 76.78 61.42 73.08 78.33
(Zhang et al., 2017) 57.14 72.32 80.21 64.42 72.91 77.93

Google IME 62.13 72.17 74.72 70.93 80.32 82.23
P2C of Moon 71.31 89.12 90.17 70.51 79.83 80.12

Table 2: Comparison with previous state-of-the-art P2C models.

an intelligent IME which is not only supposed to
give accurate P2C conversion, but to help users
type sentences in a more convenient and efficient
manner. To this end, follow-up prediction is quite
necessary for input acceleration. Given an unfin-
ished input, Moon IME now enables the follow-
up prediction to help the user complete the typing.
For example, given “ë�Ì” (Fast Fourier), the
IME engine will provide the candidate “ë�Ì
öØb” (fast Fourier transform). Specifically, we
extract each sentence in the Wikipedia corpus and
use the IR-based association module to retrieve the
index continuously and give the best-matched sen-
tence as the prediction.

Pinyin-to-English Translation Our Moon IME
is also equipped with a multi-lingual typing abili-
ty. For users of different language backgrounds, a
satisfying conversation can benefit from the direct
translation in IME engine. For example, if a Chi-
nese user is using our IME chatting with a native
English speaker, but get confused with how to say
“Input Method Engine”, simply typing the word-
s “eÕ” in mother tongue, the IME will give
the translated expression. This is also achieved by
training a Seq2Seq model from OpenNMT using
WMT17 Chinese-English dataset5.

Factoid Question Answering As an instance
of IR-based association module, we make use of
question answering (QA) corpus for automatic
question completion. Intuitively, if a user wants
to raise a question, our IME will retrieve the most
matched question in the corpus along with the
corresponding answer for typing reference. We
use the WebQA dataset (Li et al., 2016) as our
QA corpus, which contains more than 42K factoid
question-answer pairs. For example, if a user input
“	Ö	” or “	Ö&” (guitar strings), the can-
didate “	Ö	à9&” (How many strings are
there in the guitar?).

5http://www.statmt.org/wmt17/translation-task.html

Figure 2: A case study of Association Cloud Platform.

Figure 2 shows a typical result returned by
the platform when a user gives incomplete input.
When user input pinyin sequence such as “zui da
de ping”, the P2C module returns “�'s”
as one candidate of the generated list and sends it
to association platform. Then associative predic-
tion is given according to the input mode that user
current selections. Since the demands of the user-
s are quite diverse, our platform to support such
demands can be adapted to any specific domain-
s with complex specialized terms. We provide a



144

Demo homepage6 for better reference, in which
we display the main feature function of our plat-
form and provide a download link.

4 Related Work

There are variable referential natural language
processing studies(Cai et al., 2018; Li et al.,
2018b; He et al., 2018; Li et al., 2018a; Zhang
et al., 2018a; Cai et al., 2017a,b) for IME devel-
opment to refer to. Most of the engineering prac-
tice mainly focus on the matching correspondence
between the Pinyin and Chinese characters, name-
ly, pinyin-to-character converting with the highest
accuracy. (Chen, 2003) introduced a conditional
maximum entropy model with syllabification for
grapheme-to-phoneme conversion. (Zhang et al.,
2006) presented a rule-based error correction ap-
proach to improving preferable conversion rate.
(Lin and Zhang, 2008) present a statistical mod-
el that associates a word with supporting context
to offer a better solution to Chinese input. (Jiang
et al., 2007) put forward a PTC framework based
on support vector machine. (Okuno and Mori,
2012) introduced an ensemble model of word-
based and character-based models for Japanese
and Chinese IMEs. (Yang et al., 2012; Wang et al.,
2018, 2016; Pang et al., 2016; Jia and Zhao, 2013,
2014) regarded the P2C conversion as a transfor-
mation between two languages and solved it by
statistical machine translation framework. (Chen
et al., 2015) firstly use natural machine thansla-
tion method to translate pinyin to Chinese. (Zhang
et al., 2017) introduced an online algorithm to con-
struct an appropriate dictionary for IME.

The recent trend on state-of-the-art techniques
for Chinese input methods can be put into two
lines. Speech-to-text input as iFly IM7 (Zhang
et al., 2015; Saon et al., 2014; Lu et al., 2016) and
the aided input methods which are capable of gen-
erating candidate sentences for users to choose to
complete input tasks, means that users can yield
coherent text with fewer keystrokes. The chal-
lenge is that the input pinyin sequences are too
imperfect to support sufficient training. Most
existing commercial input methods offer auto-
completion to users as well as extended associa-
tion functions, to aid users input. However, the
performance of association function of existing
commercial IMEs are unsatisfactory to relevan-

6ime.leisure-x.com
7https://www.xunfei.cn/

t user requirement for oversimplified modeling.
It is worth mentioning that we delivery Moon

IME as a type of IME service rather than a simple
IME software because it can be adjusted to adap-
t to diverse domains with the Association Cloud
Platform (Zhang et al., 2018b,c; Zhang and Zhao,
2018), which helps user type long sentences and
predicts the whole expected inputs based on cus-
tomized knowledge bases.

5 Conclusion

This work makes the first attempt at establishing
a general cloud platform to provide customizable
association services for Chinese pinyin IME as to
our best knowledge. We present Moon IME, a
pinyin IME that contains a high-quality P2C mod-
ule and an extended information retrieval based
module. The former is based on an attention-
based NMT model and the latter contains follow-
up-prediction and machine translation module for
typing assistance. With a powerful customiz-
able design, the association cloud platform can be
adapted to any specific domains including com-
plex specialized terms. Usability analysis shows
that core engine achieves comparable conversion
quality with the state-of-the-art research models
and the association function is stable and can be
well adopted by a broad range of users. It is more
convenient for predicting complete, extra and even
corrected character outputs especially when user
input is incomplete or incorrect.

References
Deng Cai, Hai Zhao, Yang Xin, Yuzhu Wang, and

Zhongye Jia. 2017a. A hybrid model for Chinese
spelling check. In ACM Transactions on Asian Low-
Resource Language Information Process.

Deng Cai, Hai Zhao, Zhisong Zhang, Yang Xin, Y-
ongjian Wu, and Feiyue Huang. 2017b. Fast and
accurate neural word segmentation for Chinese. In
ACL, pages 608–615.

Jiaxun Cai, Shexia He, Zuchao Li, and Hai Zhao. 2018.
A full end-to-end semantic role labeler, syntax-
agnostic or syntax-aware? In COLING.

Shenyuan Chen, Rui Wang, and Hai Zhao. 2015. Neu-
ral network language model for Chinese pinyin input
method engine. In PACLIC-29, pages 455–461.

Stanley F. Chen. 2003. Conditional and joint model-
s for grapheme-to-phoneme conversion. In INTER-
SPEECH, pages 2033–2036.



145

Shexia He, Zuchao Li, Hai Zhao, Hongxiao Bai, and
Gongshen Liu. 2018. Syntax for semantic role la-
beling, to be, or not to be. In ACL.

Sepp Hochreiter and Jurgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Guoping Huang, Jiajun Zhang, Yu Zhou, and
Chengqing Zong. 2015. A new input method for hu-
man translators: integrating machine translation ef-
fectively and imperceptibly. In IJCAI, pages 1163–
1169.

Zhongye Jia and Hai Zhao. 2013. Kyss 1.0: a frame-
work for automatic evaluation of Chinese input
method engines. In IJCNLP, pages 1195–1201.

Zhongye Jia and Hai Zhao. 2014. A joint graph model
for pinyin-to-Chinese conversion with typo correc-
tion. In ACL, pages 1512–1523.

Wei Jiang, Yi Guan, Xiao Long Wang, and Bing Quan
Liu. 2007. Pinyin to character conversion model
based on support vector machines. Journal of Chi-
nese Information Processing, 21(2):100–105.

Haonan Li, Zhisong Zhang, yuqi Ju, and Hai Zhao.
2018a. Neural character-level dependency parsing
for Chinese. In AAAI.

Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Y-
ing Cao, Jie Zhou, and Wei Xu. 2016. Dataset and
neural recurrent sequence labeling model for open-
domain factoid question answering. arXiv preprint
arXiv:1607.06275v2.

Zuchao Li, Shexia He, and Hai Zhao. 2018b. Seq2seq
dependency parsing. In COLING.

Bo Lin and Jun Zhang. 2008. A novel statistical Chi-
nese language model and its application in pinyin-to-
character conversion. In ACM Conference on Infor-
mation and Knowledge Management, pages 1433–
1434.

Liang Lu, Kong Lingpeng, Chris Dyer, Noah A. Smith,
and Steve Renals. 2016. Unfolded recurrent neu-
ral networks for speech recognition. In INTER-
SPEECH, pages 385–389.

Minh Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. In EMNLP, pages
1412–1421.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.

Yoh Okuno and Shinsuke Mori. 2012. An ensemble
model of word-based and character-based models
for Japanese and Chinese input method. In CoLING,
pages 15–28.

Chenxi Pang, Hai Zhao, and Zhongyi Li. 2016. I can
guess what you mean: A monolingual query en-
hancement for machine translation. In CCL, pages
50–63.

George Saon, Hagen Soltau, Ahmad Emami, and
Michael Picheny. 2014. Unfolded recurrent neu-
ral networks for speech recognition. In INTER-
SPEECH, pages 343–347.

Rui Wang, Hai Zhao, Bao-Liang Lu, Masao Utiya-
ma, and Eiichiro Sumita. 2016. Connecting phrase
based statistical machine translation adaptation. In
CoLING, pages 3135–3145.

Rui Wang, Hai Zhao, Sabine Ploux, Bao-Liang Lu,
Masao Utiyama, and Eiichiro Sumita. 2018. Graph-
based bilingual word embedding for statistical ma-
chine translation. In ACM Transaciton on Asian
and Low-Resource Language Information Process-
ing, volume 17.

Yu Wu, Wei Wu, Chen Xing, Zhoujun Li, and Ming
Zhou. 2017. Sequential matching network: A
new architecture for multi-turn response selection in
retrieval-based chatbots. In ACL, pages 496–505.

Shaohua Yang, Hai Zhao, and Bao-liang Lu. 2012.
A machine translation approach for Chinese
whole-sentence pinyin-to-character conversion. In
PACLIC-26, pages 333–342.

Shiliang Zhang, Cong Liu, Hui Jiang, Si Wei, Lirong
Dai, and Yu Hu. 2015. Feedforward sequential
memory networks: A new structure to learn long-
term dependency. arXiv preprint arXiv:1512.08301.

Xihu Zhang, Chu Wei, and Hai Zhao. 2017. Tracing
a loose wordhood for Chinese input method engine.
arXiv preprint arXiv:1712.04158.

Yan Zhang, Bo Xu, and Chengqing Zong. 2006. Rule-
based post-processing of pin to Chinese characters
conversion system. In International Symposium on
Chinese Spoken Language Processing, pages 1–4.

Zhuosheng Zhang, Jiangtong Li, Hai Zhao, and Bingjie
Tang. 2018a. Sjtu-nlp at semeval-2018 task 9: Neu-
ral hypernym discovery with term embeddings. In
SemEval-2018, Workshop of NAACL-HLT.

Zhuosheng Zhang, Jiangtong Li, Pengfei Zhu, and
Hai Zhao. 2018b. Modeling multi-turn conversation
with deep utterance aggregation. In CoLING.

Zhuosheng Zhang, Huang Yafang, and Hai Zhao.
2018c. Subword-augmented embedding for cloze
reading comprehension. In CoLING.

Zhuosheng Zhang and Hai Zhao. 2018. One-shot
learning for question-answering in gaokao history
challenge. In CoLING.

Hai Zhao, Chang-Ning Huang, Mu Li, and Taku Kudo.
2006. An improved Chinese word segmentation sys-
tem with conditional random field. In Sighan, pages
162–165.

https://arxiv.org/abs/1301.3781
https://arxiv.org/abs/1301.3781
https://arxiv.org/abs/1512.08301
https://arxiv.org/abs/1512.08301
https://arxiv.org/abs/1512.08301
https://arxiv.org/abs/1712.04158
https://arxiv.org/abs/1712.04158

