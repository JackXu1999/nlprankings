



















































Question Answering System using Multiple Information Source and Open Type Answer Merge


Proceedings of NAACL-HLT 2015, pages 111–115,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Question Answering System using Multiple Information Source and 

Open Type Answer Merge 

 

 
Seonyeong Park, Soonchoul Kwon,  Byungsoo Kim, Sangdo Han,  

Hyosup Shim, Gary Geunbae Lee 
Pohang University of Science and Technology, Pohang, Republic of Korea 

{sypark322, theincluder, bsmail90, hansd, hyosupshim, gblee} @postech.ac.kr 

 

 

 

 

Abstract 

This paper presents a multi-strategy and multi-

source question answering (QA) system that 

can use multiple strategies to both answer natu-

ral language (NL) questions and respond to 

keywords. We use multiple information 

sources including curated knowledge base, raw 

text, auto-generated triples, and NL processing 

results. We develop open semantic answer type 

detector for answer merging and improve pre-

vious developed single QA modules such as 

knowledge base based QA, information re-

trieval based QA.  

1 Introduction 

Several massive knowledge bases such as DBpedia 

(Auer et al., 2007) and Freebase (Bollacker et al., 

2008) have been released. To utilize these re-

sources, various approaches to question answering 

(QA) on linked data have been proposed (He et al., 

2014; Berant et al., 2013). QA on linked data or on 

a knowledge base (KB) can give very high preci-

sion, but because KBs consist of fragmentary 

knowledge with no contextual information and is 

powered by community effort, they cannot cover 

all information needs of users. Furthermore, QA 

systems achieve low precision when disambiguat-

ing question sentences in to KB concepts; this flaw 

reduces QAs’ performance (Yih et al., 2014). 

   A QA system can understand a natural language 

(NL) question and return the answer. In some 

ways, perfection of QA systems is the final goal of 

information retrieval (IR). Early QA systems were 

IR-based QAs (IRQAs). However, as large KBs 

such as DBpedia and Freebase have been con-

structed, KB-based QA (KBQA) has become in-

creasingly important (Lehmann et al., 2015; Unger 

et al., 2012).   

    These two kinds of QA systems use heterogene-

ous data; IRQA systems search raw text, whereas 

KBQA systems search KB. KBQA systems give 

accurate answers because they search from KBs 

curated by humans. However, they cannot utilize 

any contextual information of the answers. The 

answers of IRQA are relatively less accurate than 

those of KBQA, but IRQA systems utilize the con-

textual information of the answers.  

    We assert that a successful QA system will re-

quire appropriate cooperation between a KBQA 

and an IRQA. We propose a method to merge the 

KBQA and the IRQA systems and to exploit the 

information in KB ontology-based open semantic 

answer type to merge the answers from the two 

systems, unlike previous systems that use a pre-

determined answer type. We improve our previous 

system (Park et al., 2015).  

    Also we can answer not only complete NL sen-

tence questions, and questions composed of only 

keywords, which are frequently asked in real life. 

We suggest strategies and methods (Figure 1) to 

integrate KBQA, IRQA, and keyword QA. 

2 System Architecture 

2.1 KB-based QA 

A KBQA system takes an NL question sentence as 

the input and retrieves its answers from the KBs. 

Because the KBs (i.e., the information sources), 

are highly structured, the KBQA system can pro-

duce very pin-pointed answer sets.  

111



     We combined two approaches to make this sys-

tem possible. The first approach is based on se-

mantic parsing (Berant et al., 2013), and the 

second is based on lexico-semantic pattern match-

ing (Shim et al., 2014).  

    In the semantic parsing approach, the system 

first generates candidate segments of the question 

sentence and tries to match KB vocabularies to the 

segments by combining use of string-similarity 

based methods and an automatically generated dic-

tionary that consists of pairs of NL phrase and KB 

predicate (Berant et al., 2013). Finally the system 

generates query candidates by applying the seg-

ments to a small set of hand-crafted grammar rules 

to generate a single formal meaning representation 

(Berant et al., 2013). 

    In the lexico-semantic pattern approach, we use 

simple patterns paired with a formal query tem-

plate. The patterns consist of regular expression 

pattern that describes lexical, part-of-speech (PoS), 

and chunk-type patterns of a question sentence 

(Shim et al., 2014). Then the templates paired with 

these patterns are equipped with methods to extract 

information from the sentence and to fill the in-

formation into the template. 

    KBQA can assess the answers even when it has 

little or no additional contextual information, 

whereas other systems like IRQA systems can rely 

on the context from which it is retrieved (Schlaefer 

et al., 2007). Instead, type information and its hier-

archy defined in the KB are good sources of con-

textual information that the KBQA can exploit. 

However, not all the entities defined in the KB 

have specific type information; therefore, relying 

only on the type information can reduce precision 

(Krishnamurthy and Mitchell, 2014). 

    When KBQA systems fail, it is usually due to 

incorrect disambiguation of entities, or to incorrect 

disambiguation of predicate. Both types of failures 

result in production of answers of the wrong types. 

For example, for a question sentence "What sport 

does the Toronto Maple Leafs play?" evoke an-

swers about the arena in which the team plays, in-

stead of the sport that the team plays, when the 

KBQA system fails in disambiguation. 

2.2 IR-based QA 

The system uses a multi-source tagged text data-

base which is a combination of raw text, auto-

generated triples, co-reference results, named enti-

ty disambiguation results, the types of named enti-

ties, and syntactic and semantic NLP results 

including semantic role label, dependency parser 

results, PoS tag. The system uses clearNLP1  for 

syntactic and semantic NLP, Stanford Co-reference 

tool2 for co-reference tagging, Spotlight (Mendes 

et al., 2011) for disambiguated named entity tag-

ging, and SPARQL queries (e.g. “SELECT 

                                                           
1 http://clearnlp.wikispaces.com/ 
2 http://nlp.stanford.edu/ 

 
Figure 1. Proposed System Architecture 

112



DISTINCT ?uri WHERE { res:Nicole_Kidman 

rdf:type ?uri. }”) for tagging DBpedia ontology 

class types that correspond to entities, and triples 

that correspond to the sentence. As a result, from a 

sentence “Kim was born in 1990 in Bucheon, 

Gyeonggi, and moved to Gunpo when she was six 

years old”, the system tags several triples such as 

< Kim; was born in; 1990 >, < Kim; was born in; 

Bucheon >, < Kim; was born in; Gyeonggi >, and 

< Kim; moved to; Gunpo >.  

    Our IRQA system consists of five parts similar 

to the architecture of our previous system (Park et 

al., 2015): the first part detects the semantic answer 

type of the question and analyzes the question; the 

second part generates the queries; the third part 

retrieves passages related to the user question; the 

fourth part extracts answer candidates using type 

checking and semantic similarity; and the last part 

ranks the answer candidates. The system analyzes 

questions from diverse aspects: PoS tagger, de-

pendency parser, semantic role labeler, our pro-

posed open Information extractor, and our 

semantic answer type detector. The system ex-

pands query using resources such as Wordnet3 and 

dictionary. 

    The system uses Lucene4 to generate an index 

and search from multi-source tagged text database. 

This is an efficient method to search triples and 

their corresponding sentences, instead of searching 

the raw text. Using Lucene, the system searches 

raw sentences and the auto-generated triples at the 

same time, but may find different sentences due to 

information loss during extraction of triples. These 

sentences are scored by measuring semantic simi-

larity to the user query. From these sentences, the 

system extracts the named entities and compares 

the semantic answer type of the question to the 

types of these named entities (Figure 2.). Along-

side the answer type, the system uses contextual 

information of the corresponding sentences of the 

answer candidates. By combining these two meth-

ods, the system selects answer candidates. 

2.3 Keyword QA 

Keyword QA takes a keyword sequence as the in-

put and returns a NL report as the answer. The sys-

tem extracts answer triples from the KB from the 

user input keyword sequences. The system uses 

                                                           
3 http://sourceforge.net/projects/jwordnet/ 
4 http://lucene.apache.org/core/ 

previously generated NL templates to generate an 

NL report (Han et al., 2015). 

2.4   Open Information Extraction  

Despite their enormous data capacity, KBs have 

limitation in the amount of knowledge compared to 

the information on the web. To remedy this defi-

ciency, we construct a repository of triples extract-

ed from the web text. We apply the technique to 

the English Wikipedia5 for the demo, but the tech-

nique is scalable to a web corpus such as Clue-

Web 6 . Each triple is composed of the form < 

argument1; relation; argument2 >, where the ar-

guments are entities in the input sentence and the 

relation represents the relationship between the 

arguments.  

    The system integrates both dependency parse 

tree pattern and semantic role labeler (SRL) results 

of each input sentence when extracting the triples. 

The dependency parse tree patterns are used to 

generalize NL sentences to abstract sentence struc-

tures because the system can find unimportant 

word tokens can be ignored in the input sentence. 

We define how triples should be extracted for each 

dependency pattern. If a certain dependency pat-

tern is satisfied, the word tokens in the pattern con-

stitute the head word of each relation and argument 

s in the triple. We call these patterns ‘extraction 

templates’. Since manual construction of extraction 

templates costs too much, we automatically con-

struct them by bootstrapping with seed triples ex-

tracted from simple PoS tag patterns. 

For each sentence, the SRL annotates the predi-

cate and the arguments of the predicate with their 

specific roles in the sentence. The predicate is re-

garded as relation and the arguments are regarded 

as argument1 and argument2, according to their 

roles. We manually define conversion rules for 

each SRL result. 

3 Methods for Integration 

3.1 Detecting Keywords and Sentence 

Our system disambiguates whether the user input 

query is a sentence or a keyword sequence. To dis-

ambiguate a sentence, the system uses bi-gram PoS 

                                                           
5 http://dumps.wikimedia.org/backup-index.html 
6 http://www.lemurproject.org/clueweb12.php/ 

113



tag features and a maximum entropy algorithm. 

Our dataset includes 670 keyword sequences and 

4521 sentences. Based on five-fold cross validation, 

our system correctly detected 96.27 % of the key-

word sequences and 98.12 % of the sentences.  

    When the user query is a sentence, the query is 

sent to the KBQA/IRQA system. Otherwise the 

query is sent to the keyword QA system. 

3.2 Open Semantic Answer Type detector 

The proposed system integrates the answers from 

the KB and the multi-source tagged text database 

including the auto-generated triple database. 

Therefore the KBQA and the IRQA must share an 

answer-type taxonomy. A previous answer type 

classification task used UIUC answer type includ-

ing six coarse-grained answer types and 50 fine-

grained answer types (Li et al., 2002). Instead, we 

use the DBpedia class type hierarchy as the open 

semantic answer type set. The proposed semantic 

answer type detector involves three steps. 

1. Feature Extraction: The proposed system uses 
the dependency parser and PoS tagger to ex-

tract the main verb and the focus. If the ques-

tion is “Who invented Macintosh computer?,” 

the main verb is ‘invented’ and the focus is 

‘who’. The answer sentence is constructed by 

replacing the focus with the answer candidate 

and changing to declarative sentence with pe-

riod, when the focus is substituted with the an-

swer. The system can detect also whether the 

focus is the subject or the object of the main 

verb.  

2. Mapping property: The system measures the 
semantic similarity between ‘invented’ and 

DBpedia properties. The system determines 

that the most similar DBpedia property to ‘in-

vented’ is ‘patent’. 

3. Finding semantic answer type: The system can 
get the type of the subject and the object of the 

DBpedia property ‘patent’. If the focus is 

the object of the property, the semantic answer 

type is the type of the object of the property; 

otherwise it is the type of the subject of the 

property. 

If the system cannot find the answer type by these 

steps, the system uses an answer type classifier as 

in Ephyra (Schlaefer et al, 2007) and uses a trans-

formation table to map their answer type classes in 

the UIUC answer type (Li et al., 2002) taxonomy 

to DBpedia class ontology. 

 

3.3 Answer Merging and Re-ranking 

This integrated system gets the answer candidates 

from both the KBQA and the IRQA. The system 

 
Figure 2. Semantic answer type detector used to merging answer candidates 

114



extracts n-best sentences including the answer can-

didates from the KBQA and the keywords from the 

user query. 

    The DBpedia types of the answer candidates 

from both the KBQA and the IRQA can be detect-

ed and compared to the semantic answer type (Fig-

ure 2.).   

    Finally, the system selects the final answer list 

by checking the answer types of the user query and 

the semantic relatedness among the answer sen-

tence substituted focus with the answer candidates, 

and the retrieved sentences. 

4 Conclusion 

We have presented a QA system that uses multiple 

strategies and multiple sources. The system can 

answer both complete sentences and sequences of 

keywords. To find answers, we used both a KB 

and multi-source tagged text data. This is our base-

line system; we are currently using textual entail-

ment technology to improve merging accuracy.   

Acknowledgments 

This work was supported by the ICT R&D pro-

gram of MSIP/IITP [R0101-15-0176, Develop-

ment of Core Technology for Human-like Self-

taught Learning based on a Symbolic Approach]. 

References  

Sören Auer, Christian Bizer, Georgi Kobilarov, Jens 

Lehmann, Richard Cyganiak, and Zachary Ives. 

2007. Dbpedia: A nucleus for a web of open data. 

Proceedings of the Sixth international The semantic 

web and Second Asian conference on Asian semantic 

web conference (pp. 722-735). 

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy 

Liang. 2013. Semantic Parsing on Freebase from 

Question-Answer Pairs. Proceedings of the 2013 

Conference on Empirical Methods in Natural Lan-

guage Processing. 1533-1544. 

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim 

Sturge, and Jamie Taylor. 2008. Freebase: a collabo-

ratively created graph database for structuring hu-

man knowledge. Proceedings of the 2008 SIGMOD 

international conference on Management of data. 

1247-1250. 

Sangdo Han, Hyosup Shim, Byungsoo Kim, Seonyeong 

Park, Seonghan Ryu, and Gary Geunbae Lee. 2015. 

Keyword Question Answering System with Report 

Generation for Linked Data. Proceedings of the Sec-

ond International Conference on Big Data and Smart 

Computing. 

Shizhu He, Kang Liu, Yuanzhe Zhang, Liheng Xu, and 

Jun Zhao. 2014. Question Answering over Linked 

Data Using First-order Logic. Proceedings of the 

2014 Conference on Empirical Methods in Natural 

Language Processing. 1092-1103. 

Jayant Krishnamurthy and Tom M. Mitchell. 2014. 

Joint Syntactic and Semantic Parsing with Combina-

tory Categorial Grammar. Proceedings of 

52nd Annual Meeting of the Association for Compu-

tational Linguistics. 1188-1198. 

Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, 

Dimitris Kontokostas, Pablo N. Mendes, Sebastian 

Hellmann, Mohamed Morsey, Patrick van Kleef, 

Sören Auer, and Christian Bizer. 2015. DBpedia – A 

large-scale, multilingual knowledge base extracted 

from Wikipedia.  Semantic Web: 6(2). 167-195. 

Xin Li, Dan Roth, Learning question classifiers. 2002. 

Proceedings of the 19th international conference on 

Computational linguistics-Volume 1. 1-7. 

Pablo N. Mendes, Max Jakob, Andrés García-Silva , 

and Christian Bizer. 2011. DBpedia Spotlight: Shed-

ding Light on the Web of Documents. Proceedings of 

the 7th International Conference on Semantic Sys-

tems. 1-8. 

Seonyeong Park, Hyosup Shim, Sangdo Han, Byungsoo 

Kim, and Gary Geunbae Lee. 2015. Multi-source hy-

brid Question Answering system. Proceeding of The 

Sixth International Workshop on Spoken Dialog Sys-

tem 

Nico Schlaefer, Jeongwoo Ko, Justin Betteridge, Guido 

Sautter, Manas Pathak, and Eric Nyberg. 2007. Se-

mantic Extensions of the Ephyra QA System for 

TREC 2007. Proceedings of the Sixteenth Text RE-

trieval Conference. 

Hyosup Shim, Seonyeong Park, and Gary Geunbae Lee. 

2014. Assisting semantic parsing-based QA system 

with lexico-semantic pattern query template. Pro-

ceedings of Human and Cognitive Language Tech-

nology. 255-258. 

Christina Unger, Lorenz Bühmann, Jens Lehmann, Ax-

el-Cyrille Ngonga Ngomo, Daniel Gerber, and 

Philipp Cimiano. 2012. Template-based question an-

swering over RDF data. Proceedings of the 21st in-

ternational conference on World Wide Web. 639-648. 

Wen-tau Yih, Xiaodong He, and Christopher Meek. 

2014. Semantic parsing for single-relation question 

answering. Proceedings of the 52nd Annual Meeting 

of the Association for Computational Linguistics. 

643-648.  

 

115


