



















































Online Multilingual Topic Models with Multi-Level Hyperpriors


Proceedings of NAACL-HLT 2016, pages 454–459,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Online Multilingual Topic Models with Multi-Level Hyperpriors

Kriste Krstovski†,§, David A. Smith‡ and Michael J. Kurtz †
†Harvard-Smithsonian Center for Astrophysics, Cambridge, MA

§College of Information and Computer Sciences, University of Massachusetts Amherst, Amherst, MA
‡College of Computer and Information Science, Northeastern University, Boston, MA

kkrstovski@cfa.harvard.edu, dasmith@ccs.neu.edu, kurtz@cfa.harvard.edu

Abstract

For topic models, such as LDA, that use
a bag-of-words assumption, it becomes es-
pecially important to break the corpus into
appropriately-sized “documents”. Since the
models are estimated solely from the term
cooccurrences, extensive documents such as
books or long journal articles lead to diffuse
statistics, and short documents such as forum
posts or product reviews can lead to sparsity.
This paper describes practical inference pro-
cedures for hierarchical models that smooth
topic estimates for smaller sections with hy-
perpriors over larger documents. Importantly
for large collections, these online variational
Bayes inference methods perform a single
pass over a corpus and achieve better perplex-
ity than “flat” topic models on monolingual
and multilingual data. Furthermore, on the
task of detecting document translation pairs
in large multilingual collections, polylingual
topic models (PLTM) with multi-level hyper-
priors (mlhPLTM) achieve significantly better
performance than existing online PLTM mod-
els while retaining computational efficiency.

1 Introduction

Bag of words models simplify the representation of
documents by discarding grammatical information
and simply relying on document-level word cooc-
currence statistics. Topic models, such as latent
Dirichlet allocation (LDA) (Blei et al., 2003), use
this representation. A major drawback of the bag
of words representation, especially in collections
of large documents, is that the word co-occurrence
statistics are computed on a document level and

as such they do not capture the effect of words
co-occurring close to each other versus words co-
occurring further apart.

One alternative approach to longer documents
that has received attention in the past has been to
directly model local—i.e., Markov—dependencies
among tokens. For example, the topical n-gram
model (TNG) introduced by Wang et al. (2007)
models unigram and n-gram phrases as mixture of
topics based on the nearby word context. More
recently, Jameel & Lam (2013) proposed an LDA
extension that uses word sequence information to
generate topic distribution over n-grams and per-
forms topic segmentation using segment and para-
graph information. While these and many other ap-
proaches offer a better and more realistic modeling
of word sequences, they don’t model topical varia-
tions across document sections either in mono- or
multilingual collections.

In this paper, we focus on hierarchical models for
improving topic models of long documents. In the
past, document-topic based hierarchical prior struc-
tures have been explored for LDA. For example,
Wallach et al. (2009) showed that Gibbs sampling
implementation of asymmetric Dirichlet priors pro-
vide better modeling of documents, across the whole
collection, compared to the original LDA approach.
More recently, Kim et al. (2013) introduced tiLDA,
a topic model of monolingual document collections
with nested hierarchies. In order to achieve reason-
able performance over large document collections
with deep hierarchies, tiLDA utilizes parallel vari-
ational Bayes (VB) inference. While VB is known
to converge faster than Gibbs sampling, and paral-

454



lel implementations are even faster, they, as with
Gibbs sampling, still require multiple iterations over
the whole collection besides the overhead of paral-
lelizing the model parameters. Furthermore these
approaches focus on monolingual collections.

We propose an online VB inference approach for
topic models that captures the document specific ef-
fect of local and long range word co-occurrence by
modeling individual document sections using multi-
level Dirichlet prior structure. The proposed models
assign Dirichlet priors to individual document sec-
tions that are coupled by a document level hierarchi-
cal Dirichlet prior which facilitates explicit model-
ing of the variation in topics across documents in
mono- and multilingual collections. This in turn
streamlines the use of topic models in collections
of large documents where there is a predetermined
section structure. Our contribution is twofold: (1)
we present an online VB inference approach for
topic models with multi-level Dirchlet prior struc-
ture and more importantly (2) introduce a polylin-
gual topic model (PLTM) with multi-level hyperpri-
ors (mlhPLTM) which is capable of efficiently mod-
eling topical variations across document sections in
large multilingual collections.

2 Efficient Multi-level Hyperpriors

The original LDA model and its multilingual vari-
ant, PLTM, use symmetric Dirichlet priors over the
document-topic distributions θd and topic-word dis-
tributions ϕk which means that the concentration pa-
rameter α of the Dirichlet distribution is fixed and
that the base measure u across all topics is uni-
form. Symmetric Dirichlet priors assume that all
documents in the collection are drawn from the same
family of distributions. This assumption is not suit-
able for collections of documents that cover a di-
verse set of topics. In the past this issue has been
addressed with asymmetric priors where the base
measures are non-uniform. One way to assign asym-
metric priors to individual documents is to treat the
base measures vector u as a hidden variable and
assign a symmetric Dirichlet prior to it which cre-
ates a hierarchical Dirichlet prior structure over all
document-topic distributions in the collection. This
approach was used by Wallach et al. (2009). Unlike
Wallach et al. (2009), who use a single document-

w

d
N1

S

D

1

wz
NL

1

L L

T

. . .

. . .s

D

d

N1

s

S
NL

1

1

T

L

L

T

. . . . . .d


u

s

z

D





TN

N

D


T

sd

wzsd

d s

d s

d

z z

z

s 



S

u

S

Figure 1: mlhLDA: Graphical representation (left); Free
variational parameters for the online VB approximation
(right).

topic distribution θd, we introduce section-topic dis-
tributions θs. The existing symmetric Dirichlet prior
over θd creates a hierarchical Dirichlet prior over θs
(θ = θd, θs1 , θs2 , ..., θsS ):

p(θ|αdu, αs) ∝ p(θd|αdu)
∏
s

p(θs|αsθd) (1)

In this setting the most widely used approach for es-
timating θd is Minka’s (2000) fixed-point iteration
approach which is also used in (Kim et al., 2013).
Instead we use a more efficient approach for estimat-
ing the Dirichlet-multinomial hyperparameters by
approximating the digamma differences in Minka’s
approach which was showcased in (Wallach, 2008)
to be more efficient. Figure 1 shows the graphical
model representation (left) of our model, which we
refer to as multi-level hyperpriors LDA (mlhLDA),
along with the free variational parameters for ap-
proximating the posteriors (right).

2.1 Inference using Online VB

Due to its ease of implementation, the most widely
used approach for inferring LDA posterior distri-
butions is Gibbs sampling (Griffiths and Steyvers,
2004). For example, this approach was used by
Wallach et al. (2009) and was originally used for
PLTM. On the other hand the VB approach (Blei et
al., 2003) offers more efficient computation but as
in the case of Gibbs sampling requires iterating over
the whole collection multiple times (e.g. Kim et al.
(2013)). More recently Hoffman et al. (2010) in-
troduced online LDA (oLDA) that relies on online
stochastic optimization and requires a single pass
over the whole collection. The same approach was
also extended to PLTM (oPLTM) (Krstovski and
Smith, 2013). In our work we also utilize online VB
to implement multi-level hyperprior (mlh) structure
in LDA and PLTM. Similar to batch VB, in online

455



VB locally optimal values of the free variational pa-
rameters γ and φ, which are used to approximate
the posterior θ and z, are computed in the E step
of the algorithm but on a batch b of documents di
(rather than the whole collection D as in the case of
batch VB) while holding the topic-word variational
parameter λ fixed. In the M step, λ is updated using
stochastic gradient algorithm by first computing the
optimal values of λ̃ using the batch optimal values of
φb: λ̃kw = η+ D|b|

∑|b|
i=1 ndiwφ

di
wk. This value is then

combined with value of λ computed on the previous
batch through weighted average:

λbkw ← (1− ρb)λb−1kw + ρbλ̃kw (2)
When computing the section-topic variational pa-
rameters we follow the proof of the lower bound
which was derived by Kim et al. (2013). This lower
bound, which is looser than the original VB Evi-
dence Lower Bound (ELBO), allows for the batch
VB approach to be used with asymmetric priors.
More specifically, given the document-topic varia-
tional parameter γdk in the E step of our online VB
approach the update for the section-topic variational
parameter γsk becomes:

γsk = αs(
γdk∑
k γdk

) +
∑
w

nsw φ
s
wk (3)

3 Online PLTM with multi-level Dirichlet
Priors

Given an aligned multilingual document tuple,
PLTM assumes that: (1) there exists a single tuple-
specific distribution across topics and (2) sets of lan-
guage specific topic-word distributions. Each word
is generated from a language- and topic-specific
multinomial distribution ϕlk as selected by the topic
assignment variable zln:

wln ∼ p
(
wln | zln, ϕlk

)
(4)

We extend this model by introducing sections spe-
cific topic distributions θs across the different lan-
guages in the tuple which are coupled by the tuple
specific document-topic distribution θd.

Given a collection of document tuples d where
each tuple contains l documents that are translations
of each other in different languages, mlhPLTM as-
sumes the following generative process. For each
language l in the collection the model first gener-
ates a set of k ∈ {1, 2, ...,K} topic-word distribu-

w

d
N1

S

D

1

wz
NL

1

L L

T

. . .

. . .s

D

d

N1

s

S
NL

1

1

T

L

L

T

. . . . . .d


u

s

z

D





TN

N

D


T

sd

wzsd

d s

d s

d

z z

z

s 



S

u

S

Figure 2: mlhPLTM: Graphical model representation.

w

d
N1

S

D

1

wz
NL

1

L L

T

. . .

. . .s

D

d

N1

s

S
NL

1

1

T

L

L

T

. . . . . .d


u

s

z

D





TN

N

D


T

sd

wzsd

d s

d s

d

z z

z

s 



S

u

S

Figure 3: mlhPLTM: Graphical representation of the free
variational parameters for the online VB approximation.

tions, ϕlk which are drawn from a Dirichlet prior
with language specific hyperparameter βl: ϕlk ∼
Dirichlet(βl). For each document dl with sd sec-
tions in tuple d, mlhPLTM then assumes the follow-
ing generative process:

•Choose θd ∼ Dir.(αd)
• For each section sd in document tuple d:
•Choose θs ∼ Dir.(αsθd)

– For each language l in section s:
∗ For each word w in section sld:
·Choose a topic z ∼Multi.(θls)
·Choose a word w ∼Multi.(ϕlz)

Figure 2 shows the graphical representation of mlh-
PLTM. The free variational parameters for the on-
line VB approximation of the posteriors are shown
in Figure 3.

4 Modeling Sections in Scientific Articles

We explore the ability of mlhLDA to model vari-
ations across document sections found in scientific
articles using a collection of journal articles from
the Astrophysics Data System (ADS) (Kurtz et al.,
2000). Our collection consists of 130k training ar-
ticles (888,346 sections) and a held-out set of 8,078
articles (54,502 sections). Figure 4 shows an exam-
ple mlhLDA representation of an ApJ article with
100 topics. Shown on the top is the inferred topic
representation of the whole document (θd) which, in
the mlhLDA model, serves as a prior for the section-
topic distributions (θs). Shown on the bottom are ex-

456



1. INTRODUCTION
 Blazars are an intriguing class of active galactic nuclei 
(AGNs), dominated by non-thermal radiation over the entire 
electromagnetic spectrum. Their emission extends from 
radio to TeV energies with a broadband spectral energy 
distribution (SED) typically described by two main 
components, the first peaking from IR to X-ray energy range 
in which blazars are the most commonly detected 
extragalactic sources ...

7. SUMMARY AND DISCUSSION
We have presented the infrared characterization of a sample 
of blazars detected in the γ-ray. In order to perform our 
selection, we considered all the blazars in the ROMA-
BZCAT catalog (Massaro et al. 2010) that are associated 
with a γ-ray source in the 2FGL (The Fermi-LAT 
Collaboration 2011). Then, we searched for infrared 
counterparts in the WISE archive adopting the same criteria 
described in Massaro et al. ...

Rank Topic=33 Topic=19 Topic=49 

1 spectral aperture measured 

2 amplification measured uncertainties 

3 isotropic total catalog 

4 dropout exposure matching 

5 competition position estimated 

6 caustic ratio respectively 

7 detected selected final 

8 antenna color cathode 

9 function spread total 

10 color objects limit 

Rank Topic=21 Topic=49 Topic=91 

1 entanglement measured ferroelectric 

2 color uncertainties population 

3 distance catalog rational 

4 magnitude matching fraction 

5 accretion estimated starburst 

6 similar respectively shielding 

7 modulus final similar 

8 objects cathode emitting 

9 right total reputation 

10 parameters limit respectively 

...

Figure 4: mlhLDA representation of the ApJ article “In-
frared Colors of the Gamma-Ray Detected Blazers”.

amples of 2 article sections (out of 7), their inferred
topic distributions along with the top 10 words for
each of the top 3 section topics.

50 200 400 600 800 1000 1500 2000
50

100

150

200

250

300

350

400

# of Topics [K]

P
e

rp
le

x
it

y

 

 

oLDA

mlhLDA

5 6 7 8 9 10 11
300

350

400

450

500

550

600

650

700

Time [log(sec)]

P
e

rp
le

x
it

y

 

 batch VB

oLDA

mlhLDA

T=5

T=10

T=20

T=30

T=50

T=70
T=90

T=100

T=5

T=10

T=20

T=30

T=50

T=70

T=90
T=100

T=5

T=10

T=20

T=30

T=70
T=50

T=90
T=100

Figure 5: oLDA vs. mlhLDA: perplexity comparison
(left); speed vs. perplexity comparisons with batch VB
(right).

The left side of Figure 5 shows the held-out per-
plexity comparison between oLDA and mlhLDA
across 13 different topic configurations. For this
set of experiments we used the above training set
of 130k articles and the set of 8,078 held-out arti-
cles. From these comparisons we clearly see the ad-
vantage of using the multi-level Dirchlet prior struc-
ture. Another way of evaluating topic models is
through an extrinsic evaluation task which was not
available for this collection. In the case of oLDA, ar-
ticle sections were treated as individual documents.
In the original oLDA1 implementation the per doc-
ument concentration parameter αd was set to 1K
which we also use in our case for both the sym-
metric θd and asymmetric θs (same goes for PLTM

1http://www.cs.princeton.edu/˜mdhoffma

and mlhPLTM). Since in our case we perform rel-
ative comparison between oLDA and mlhLDA we
weren’t concerned with experimenting with differ-
ent concentration parameters but we rather used the
default one implemented in oLDA.

With a random subset of 10k training and 1k held-
out articles we compared the performance of oLDA
and mlhLDA with the original batch VB2 implemen-
tation of Blei et al. (2003). Unlike the implemen-
tations of oLDA and mlhLDA which are written in
Python the original VB algorithm is written in C and
requires multiple iterations over the whole collec-
tion. The right side of Figure 5 shows the speed (in
natural log scale) vs. perplexity comparison across
the three models.

5 Modeling and Retrieving Speeches in
Europarl Sessions

We compared the modeling performance of oPLTM
and mlhPLTM on a subset of the English-Spanish
Europarl collection (Koehn, 2005). The subset
consists of ∼64k training pairs of English-Spanish
speeches that are translations of each other which
originate from 374 sessions of the European Par-
liament (Europarl) and a test set of ∼14k speech
translation pairs from 112 sessions. With oPLTM
we modeled individual speech pairs while with mlh-
PLTM we utilized the session hierarchy and mod-
eled pairs of speeches as document sections. Com-
parisons were performed intrinsically (using per-
plexity) and extrinsically on a cross-language infor-
mation retrieval (CLIR) task. This task, along with
the Europarl subset, have been previously defined by
Mimno et al. (2009) and used across other publica-
tions (Platt et al., 2010; Krstovski and Smith, 2013).
Given a query English speech, the CLIR task is to re-
trieve its Spanish translation equivalent. It involves
performing comparison across topic representations
of all Spanish speeches using Jensen-Shannon diver-
gence and sorting the results. Models are evaluated
using precision at rank one (P@1). Figure 6 shows
the CLIR task performance comparisons results us-
ing 13 different topic configurations. We performed
comparisons across three different settings of the
concentration parameters αd and αs (αd=αs= 1K , 0.4
and 1.0).

2http://www.cs.princeton.edu/˜blei/lda-c

457



0 200 400 600 800 1000 1200 1400 1600 1800 2000
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

# of Topics [K]

P
@

1

 

 

oPLTM (alpha=0.4)

oPLTM (alpha=1.0)

oPLTM (alpha=1/K)

mlhPLTM (alpha=0.4)

mlhPLTM (alpha=1.0)

mlhPLTM (alpha=1/K)

Figure 6: oPLTM vs. mlhPLTM: Performance com-
parison on the CLIR task using chronological order-
ing of sessions across different hyperparameter settings,
αd=αs= 1K , 0.4 and 1.0 .

Across the different concentration parameter val-
ues and across the 13 different topic configurations
we observe that the performance of oPLTM fluc-
tuates as we increase the numbers of topics. On
the other hand, across the three different concen-
tration parameter settings, mlhPLTM performance
is very steady and tends to increase with the num-
ber of topics. Across the different topic configura-
tions both models provide the best performance with
αd = αs = 0.4. Setting the concentration parame-
ters to 1K gives the overall worst performance.

In our initial experiments we unintentionally re-
ordered our set of training Europarl sessions based
on two digit years which was different from the
experimental setup in (Mimno et al., 2009) and
(Krstovski and Smith, 2013) where the order of the
presentation data (Europarl speeches) was chrono-
logical. This emphasized the fact that in online VB,
order of presentation of documents plays an impor-
tant role especially in the training step where the
model learns the per topic-word distributions. Fig-
ure 7 shows the performance comparison results be-
tween oPLTM and mlhPLTM when documents in
the training and test steps are ordered numerically.
In our initial experimental setup concentration pa-
rameters where set to αd = αs = 1K . To the left is
the perplexity comparison between the two models.
The CLIR task performance comparisons results are
shown on the right. Unordered mlhPLTM achieves
high P@1 after 2,000 topics. While it takes much
longer in terms of the number of topics unordered
mlhPLTM ultimately achieves similar performance
results as ordered mlhPLTM.

50 200 400 600 800 1000 1500 2000
400

600

800

1000

1200

1400

1600

1800

# of Topics [K]

P
e

rp
le

x
it

y

 

 

oPLTM (English)

mlhPLTM (English)

oPLTM (Spanish)

mlhPLTM (Spanish)

50 200 400 600 800 1,000 2,000

0.4

0.5

0.6

0.7

0.8

0.9

1

# of Topics [K]

P
@

1

 

 oPLTM

mlhPLTM

Figure 7: oPLTM vs. mlhPLTM: perplexity comparison
(left); performance comparison on the CLIR task (right).
Documents were presented out of chronological order
and thus performance is lower, especially for oPLTM.

6 Conclusion

We presented online topic models with multi-level
Dirichlet prior structure that provide better model-
ing of topical variations across document sections in
mono- and multilingual collections. We showed that
documents with rich sub-document level structure
could be modeled with higher likelihood compared
to regular online LDA and PLTM models while of-
fering the same efficiency. Furthermore on the task
of retrieving document translations we showed that
mlhPLTM achieves significantly better retrieval re-
sults compared to online PLTM.

Acknowledgments

This work was supported in part by the Harvard-
Smithsonian CfA predoctoral fellowship, in part by
the Center for Intelligent Information Retrieval and
in part by NSF grant #IIS-0910884. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect those of the sponsor.

References

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. JMLR, 3:993–1022.

T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228–5235.

Matthew Hoffman, David Blei, and Francis Bach. 2010.
Online learning for latent Dirichlet allocation. In
NIPS, pages 856–864.

Shoaib Jameel and Wai Lam. 2013. An unsupervised
topic segmentation model incorporating word order.
In SIGIR, pages 203–212.

458



Do-Kyum Kim, Geoffrey Voelker, and Lawrence K. Saul.
2013. A variational approximation for topic modeling
of hierarchical corpora. In ICML, pages 55–63.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit, pages
79–86.

Kriste Krstovski and David A. Smith. 2013. Online
polylingual topic models for fast document translation
detection. In WMT, pages 252–261.

Michael J. Kurtz, Guenther Eichhorn, Alberto Acco-
mazzi, Carolyn S. Grant, Stephen S. Murray, and
Joyce M. Watson. 2000. The nasa astrophysics data
system: Overview. Astronomy and Astrophysics Sup-
plement Series, 143:41–59.

David Mimno, Hanna Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In EMNLP, pages 880–889.

Thomas P. Minka. 2000. Estimating a dirichlet distribu-
tion. Technical report, MIT.

John Platt, Kristina Toutanova, and Wen tau Yih. 2010.
Translingual document representations from discrimi-
native projections. In EMNLP, pages 251–261.

Hanna M. Wallach, David Mimno, and Andrew McCal-
lum. 2009. Rethinking LDA: Why priors matter. In
NIPS, pages 1973–1981.

Hanna M. Wallach. 2008. Structured Topic Models for
Language. Ph.D. thesis, University of Cambridge.

Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In ICDM, pages
697–702.

459


