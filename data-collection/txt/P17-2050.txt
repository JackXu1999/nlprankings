



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 317–323
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2050

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 317–323
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2050

Bootstrapping for Numerical Open IE

Swarnadeep Saha
Department of CSE

I.I.T. Delhi

writetoswarna@gmail.com

Harinder Pal∗
Microsoft Corporation

hapal@microsoft.com

Mausam
Department of CSE

I.I.T. Delhi

mausam@cse.iitd.ac.in

Abstract

We design and release BONIE, the first
open numerical relation extractor, for ex-
tracting Open IE tuples where one of the
arguments is a number or a quantity-unit
phrase. BONIE uses bootstrapping to
learn the specific dependency patterns that
express numerical relations in a sentence.
BONIE’s novelty lies in task-specific cus-
tomizations, such as inferring implicit re-
lations, which are clear due to context such
as units (for e.g., ‘square kilometers’ sug-
gests area, even if the word ‘area’ is miss-
ing in the sentence). BONIE obtains 1.5x
yield and 15 point precision gain on nu-
merical facts over a state-of-the-art Open
IE system.

1 Introduction

Open Information Extraction (Open IE) systems
extract relational tuples from text, without re-
quiring a pre-specified vocabulary (Etzioni et al.,
2008; Mausam, 2016), by constructing the rela-
tion phrases and arguments from within the sen-
tences themselves. Early works on Open IE such
as REVERB (Etzioni et al., 2011) extract verb-
mediated relations via a handful of human-defined
patterns. OLLIE improves recall by learning de-
pendency patterns, using bootstrapping over RE-
VERB extractions (Mausam et al., 2012). Open
IE 4.2, a state-of-the-art open information extrac-
tor, is based on a combination of SRLIE, a verb-
mediated extractor over SRL frames (Christensen
et al., 2011), and RELNOUN 2.0, which performs
special linguistic processing for extraction from
complex noun phrases (Pal and Mausam, 2016).
1

∗Most work was done when the author was a graduate
student at IIT Delhi.

1https://github.com/knowitall/openie

1. Sentence: Hong Kong’s labour force is 3.5 million.
Open IE 4.2: (Hong Kong’s labour force; is; 3.5 million)
BONIE: (Hong Kong; has labour force of; 3.5 million)

2. Sentence: Microsoft has 100,000 employees.
Open IE 4.2: (Microsoft; has; 100,000 employees)
BONIE: (Microsoft; has number of employees; 100,000)

3. Sentence: James Valley is nearly 600 miles long.
Open IE 4.2: (James Valley; is; nearly 600 miles long)
BONIE: (James Valley; has length of; nearly 600 miles)

4. Sentence: Donald Trump is 70 years old.
Open IE 4.2: (Donald Trump; is; 70 years old)
BONIE: (Donald Trump; has age of; 70 years)

5. Sentence: James Valley has 5 sq kms of fruit orchards.
Open IE 4.2: (James Valley; has; 5 sq kms of fruit orchards)
BONIE: (James Valley; has area of fruit orchards; 5 sq kms)

Table 1: Comparison of Open IE 4.2 and BONIE

In this work, we present and release2 the
first system for open numerical extraction, which
we name BONIE for Bootstrapping-based Open
Numerical Information Extractor. It is important
to note that existing Open IE systems, like Open
IE 4.2, may also extract numerical facts. How-
ever, they are oblivious to the presence of numbers
in arguments. Therefore, they may miss important
extractions and may not always output the best nu-
merical facts. Table 1 compares extractions gener-
ated by Open IE 4.2 and BONIE on some of the
sample sentences.

At a high level BONIE follows OLLIE’s de-
sign of identifying seed facts, constructing training
data by bootstrapping sentences that may mention
a seed fact, pattern learning and ranking. Madaan
et al (2016) note that bootstrapping for numerical
IE is challenging; it can lead to high noise and
missed recall, since numbers can easily match out
of context, and numbers may not match due to ap-
proximations. In response, similar to most previ-
ous works (e.g., LUCHS (Hoffmann et al., 2010))
BONIE matches a number if it is within a percent-
age threshold. Additionally, BONIE uses a quan-
tity extractor (Roy et al., 2015), which provides

2Available at https://github.com/Open-NRE

317

https://doi.org/10.18653/v1/P17-2050
https://doi.org/10.18653/v1/P17-2050


the units mentioned in the sentence – BONIE
bootstraps a sentence only when the units match.

When compared to OLLIE, BONIE contributes
several numerical IE specific customizations. (1)
Since no open facts are available for this task, we
first manually define a set of high-precision seed
patterns, which are run over a large corpus to gen-
erate seed facts. (2) Not all seeds are fit for boot-
strapping – many don’t even have an entity as first
argument. We develop heuristics to identify an in-
formative subset from these. After bootstrapping
and pattern learning, we find that we are miss-
ing important tuples. E.g., sentence #3 in Table
1 above has no explicit relation word – the rela-
tion “has length of” is implicit via the adjective
‘long’. And, sentence #5 expresses the relation
‘area’ via the units. (3) BONIE identifies implicit
relations using additional processing of units and
adjectives. (4) Finally, BONIE can tag a quantity
as count and prepends “number of” in the relation
phrase (sentence #2).

2 Related Work
One of the first Open IE systems to obtain sub-
stantial recall is OLLIE (Mausam et al., 2012),
which is a pattern learning approach based on a
bootstrapped training data using high precision
verb-based extractions. Other methods augment
the linguistic knowledge in the systems – Exem-
plar (de Sá Mesquita et al., 2013) adds new rules
over dependency parses, SRLIE develops extrac-
tion logic over SRL frames (Christensen et al.,
2011). Several works identify clauses and op-
erate over restructured sentences (Schmidek and
Barbosa, 2014; Corro and Gemulla, 2013; Bast
and Haussmann, 2013). Other approaches use tree
kernels (Xu et al., 2013), qualia-based patterns
(Xavier et al., 2015), and simple within-sentence
inference (Bast and Haussmann, 2014). However,
none of them handle numbers specifically, and
hence do not work for our problem.

Numerical Relations: Numbers play an impor-
tant role in extracting information from text. Early
works have seen people working on understanding
numbers that express temporal information (Ling
and Weld, 2010). More recently, the focus has
been on numbers that express physical quantities
or measures, either mentioned in text (Chaganty
and Liang, 2016) or in the context of web tables
(Ibrahim et al., 2016; Neumaier et al., 2016), or
on numbers that represent cardinalities of relations
(Mirza et al., 2017).

One of the prior works that applies to generic
numerical relations is LUCHS (Hoffmann et al.,
2010), where the system uses distant supervision
to create 5,000 relation extractors, which included
numerical relations as well. Researchers have also
specifically developed numerical relation extrac-
tors to extract those relations where one of the ar-
guments is a quantity (Vlachos and Riedel, 2015;
Intxaurrondo et al., 2015; Madaan et al., 2016).
However, all of them extract only an ontology rela-
tion, and hence are not directly applicable to Open
IE.

Figure 1: BONIE flow diagram

3 Open Numerical Relation Extraction

The goal of Open Numerical Relation Extraction
is to process a sentence that has a quantity mention
in it, and extract any tuple of the form (Arg1, rela-
tion phrase, Arg2) where Arg2 (or Arg1) is a quan-
tity. As a first step, BONIE learns patterns where
Arg2 is a quantity, as most English sentences tend
to express numerical facts in active voice. Fig-
ure 1 outlines BONIE’s algorithm, which operates
in two phases: training and extraction. BONIE’s
training includes creation of seed facts, genera-
tion of training data via bootstrapping, and pat-
tern learning over dependency parses. In the ex-
traction phrase, BONIE performs pattern match-
ing and parse-based expansion to construct numer-
ical tuples. These numerical tuples are made more
coherent by a novel relation construction step.

As an example, the sentence “India has a pop-
ulation of 1.2 billion” matches seed pattern #2
(from Figure 2) to create a seed fact (India; pop-
ulation; 1.2 billion; null). This ‘null’ represents
that the quantity needs no unit. While bootstrap-
ping, this seed fact may match a sentence “India
is the second most populous country in the world,
with a population of 1.25 billion.” in the corpus.

318



Figure 2: Seed Dependency Patterns in BONIE

This training example will help learn a new pat-
tern.3 This pattern, when applied to the sentence
“Microsoft Windows is the most popular operat-
ing system, with a customer base of 300 million
users”, will extract (Microsoft Windows; has cus-
tomer base of; 300 million users).

While BONIE’s skeleton broadly resembles that
of OLLIE’s (Mausam et al., 2012), it brings in
customizations specific to the problem of numeri-
cal extraction such as a modified pattern language,
heuristics for generating high quality seed set and
training data, special processing for non-noun re-
lations, and a novel relation construction step. We
now describe BONIE’s algorithm in detail.

3.1 Generation of Seed Facts

Since open numerical facts are not readily avail-
able, we first write a handful of high-precision
dependency patterns (see Figure 2 for a list).
Each dependency pattern encodes the minimal
sub-tree of the dependency parse connecting the
relation, quantity and argument in that sen-
tence. BONIE encodes a node in a pattern
via ‘<depLabel>#<word>#<POSTag>’, where
‘depLabel’ is the edge connecting the node to its
parent, ‘word’ is the word at the node, ‘POSTag’
is its part of speech tag; ‘#’ is a delimiter separat-
ing them. {rel}, {arg} and {quantity} in the pat-
terns are placeholders for relation, argument and
quantity headwords, respectively. BONIE gener-
ates seed facts by parsing the corpus and match-
ing seed patterns with the parse. In case of a suc-
cessful match, a seed fact of the form (arg head-
word; relation headword; quantity; unit) is gen-
erated. Argument and relation headwords are ex-
tracted directly from the parse. For the other two,
it uses Illinois Quantifier (Roy et al., 2015), which
returns both the quantity and unit separately.

Since seed facts form the basis of our training
task, they must be as clean as possible – BONIE

3<(#is#verb)<(nsubj#{arg}#nnp|nn)(prep#with#in)<
(pobj#{rel}#nnp|nn)<(prep#of#in)<(pobj#{quantity}#.+)
>>>>>

adds several filters to reduce noise. It considers a
seed fact as valid only when the quantity node in
the pattern is within some quantity span given by
Illinois Quantifier. It also rejects any fact whose
argument is not a proper noun.

After these filters it gets high-precision extrac-
tions, but not necessarily good seeds – many seeds
are generic, which may easily match unrelated
sentences. E.g., (Michael; drove; 20; kms) isn’t
a good seed, since ‘Michael’ isn’t specific, and
could erroneously match sentences mentioning an-
other Michael with some unrelated reference of a
20 km drive. To improve the set, BONIE checks
for the presence of a seed fact in Yago KB (Hoffart
et al., 2013) and keeps only those that are com-
mon. Since Yago has many numerical facts for
height, area, latitude, GDP, etc., this gives BONIE
a diverse set of clean facts for further training.

Finally, some numerical facts may be expressed
without using a nominal relation word. BONIE
uses WordNet (Miller, 1995) to generate new
seeds from such seed facts using the derivation-
ally related noun form of the relation headword.
For example, (Brown ; tall ; 13 ; inches) gets trans-
formed to (Brown; height; 13; inches), which gets
added as a seed fact.

3.2 Bootstrapping
Similar to OLLIE, BONIE finds sentences that
contain all words in a seed fact and generates (sen-
tence, fact) pairs. But unlike OLLIE, BONIE has
quantities and units, and matching them as words
isn’t appropriate. Illinois Quantifier performs an
internal normalization for both, e.g, changes ‘dol-
lars’ and ‘$’ to ‘US$’, and ‘%’ to ‘percent’. Since
seed facts also have normalized units, we run Illi-
nois quantifier on candidate sentences and match
normalized units directly. Moreover, BONIE
maintains a percentage threshold δ to control the
amount of allowed difference between quantities
in the sentence and seed fact. Once all constituents
of a fact match with a sentence, BONIE generates
the (sentence, fact) pair.

319



3.3 Open Pattern Learning

For each (sentence, fact) pair, BONIE parses the
sentence, and replaces the argument and relation
words of the fact with ‘{arg}’ and ‘{rel}’ place-
holders. For quantity and unit words, BONIE
replaces the one at a higher level in the parse
with ‘{quantity}’. The minimal path containing
‘{arg}’, ‘{rel}’ and ‘{quantity}’ is learned as a
pattern. Since quantity and unit are typically ex-
pected to remain close to each other in a sentence,
BONIE rejects all such patterns where the distance
between them exceeds a certain threshold value.

Some patterns are learned with specific
words such as ‘contains’ in example (partial)
<(#contains#verb)<(dobj#{quantity}#.+)<...
We believe that this pattern should work with all
inflections and synonyms of ‘contain’, BONIE
uses WordNet to expand the pattern by including
all inflections and synset synonyms. Each pattern
is scored based on the number of times it is
learned from the data.

3.4 Constructing Extractions

After matching a pattern to a new sentence, sim-
ilar to OLLIE, arg/rel phrases are completed by
expanding the extracted headnouns on poss, det,
num, neg, amod, quantmod, nn, and pobj edges.
If one of the children of the argument headword
is a prep, rcmod or partmod edge, the whole sub-
tree under that is extracted. Quantity phrase is
extracted by Illinois quantifier, but if any sibling
node of the quantity node is connected by a prep
edge, with the word ‘of’, BONIE expands the en-
tire subtree below it. This allows “10 percent of
100 dollars” to be included in the quantity phrase.

Relation Phrase Construction: Whenever the
relation headword is an adjective or an adverb,
BONIE uses WordNet to get its derivationally re-
lated noun form and that becomes the new rela-
tion. This transforms the tuple (Donald Trump ;
old ; about 70 years) to (Donald Trump ; has age
of ; about 70 years).

Sometimes, sentences don’t use a numerical re-
lation word – it is obvious from the units. E.g.,
sentence #4 on page 1 expresses the ‘area’ relation
implicitly. BONIE infers these implicit relations
using the unit analysis in UnitTagger (Sarawagi
and Chakrabarti, 2014). Whenever BONIE sees
a unit (sq kms) getting mistreated as a relation it
uses UnitTagger to infer relations from units and
postprocesses the extraction accordingly. The ex-

traction, as a result changes from (James Valley;
has sq kms of ; 5 of fruit orchards) to (James Val-
ley ; has area of fruit orchards ; 5 sq kms).

Finally, in cases when a plural noun relation
word also appears as a unit in the quantifier,
BONIE hypothesizes that it is a count extraction,
and prepends ‘number of’ to the relation headword
and removes the unit from the quantity phrase.
E.g., (Microsoft; has employees; 100,000 employ-
ees) from sentence #2 becomes (Microsoft; has
number of employees; 100,000).

4 Experiments

We build BONIE over data from ClueWeb12,4 fil-
tered so as to keep only the sentences that contain
numbers. We further remove those where quantity
represents a date, time, or duration, and where the
quantity is accompanied by document words like
‘Section’, ‘Table’, or ‘Figure’. We use the depen-
dency parser from ClearNLP5. We generate about
21,000 seed facts from roughly 20 million numeri-
cal sentences. These are matched against 7 million
numerical sentences obtaining about 18,500 (sen-
tence, fact) pairs.

We tried different values of δ (the matching
threshold) and found results to not be sensitive as
long as δ varies in the range of 2% to 5%. So we
set δ=2% during the final evaluation. The distance
threshold between the quantity and unit mentioned
in Section 2.3 is set to 3 and is based on our gen-
eral understanding of parse trees.

BONIE learns around 7,000 new patterns. Since
pattern frequency is a good indicator of pattern
quality (Wu and Weld, 2010), we rank the patterns
on the basis of frequency and take the top 1,000
patterns for further analysis. We find that almost
all patterns beyond the top 1,000 are learned only
once or twice on our training set. Our decision to
ignore all patterns beyond the top 1,000 is so that
we have a support of at least three for each pattern.

We sample a random testset of 2,000 numeri-
cal sentences from ClueWeb12 (not used in train-
ing). Two annotators with NLP experience anno-
tate each extraction for correctness. We obtain an
inter-annotator agreement of 97%, and report the
results on the subset where both annotators agree.

Since there are no open numerical extractors
available, we compare BONIE against an Open IE
system and another closed numerical IE system.

4http://www.lemurproject.org/clueweb12.php/
5https://github.com/clir/clearnlp

320



Setting Precision Yield
NumberRule 50.00 6
Open IE 4.2 62.50 296
BONIE (seed patterns only) 85.71 72
+ learned patterns 13.88 362
+ fact filters 55.27 351
+ Yago + WordNet expansion on facts 72.69 418
+ Relation phrase construction 77.91 448
+ WordNet expansion on patterns 77.23 458

Table 2: Precision and Yield (#correct numerical extrac-
tions) on a dataset of 2000 ClueWeb12 numerical sentences.

Table 2 reports the precisions and yields of all sys-
tems. We first compare against numerical tuples
from Open IE 4.2,6 a publicly available state-of-
the-art Open IE system that combines SRLIE and
RELNOUN. BONIE outputs a much higher preci-
sion and yield on numerical facts, as compared to
Open IE 4.2. We also compare against Number-
Rule, a state-of-the-art closed numerical IE sys-
tem (Madaan et al., 2016). NumberRule7 can be
quickly re-targeted to any new semantic relation
by inputting keywords. We feed all of Yago nu-
merical relation words as keywords to Number-
Rule, but still find that it was able to generate only
12 extractions on our testset.

Figure 3: Comparison of Precision-Yield curves for
BONIE and Open IE 4.2. BONIE achieves substantially
larger area under the curve than Open IE 4.2.

We also perform additional ablation study to
evaluate the value of each component. Just
the seed patterns themselves have a significantly
higher precision but much smaller yield. This is
expected, since the seeds must be highly precise
for bootstrapping. If Yago matching and other
seed filtering heuristics are turned off, the preci-
sion of the system goes down drastically due to a
very noisy bootstrapped set. If the post-processing
of relation phrase construction is turned off, there
is a 5 point precision loss and about 7% yield
reduction due to some incorrect extracted tuples,
which are corrected by post-processing. Finally,

6https://github.com/knowitall/openie
7https://github.com/NEO-IE

Wordnet-based expansion has marginal increase in
yield and slight precision loss.

Open IE 4.2 associates a confidence value with
each extraction - ranking against which generates
a precision-yield curve. For BONIE, we rank the
patterns in such a way that the seed patterns are
at the top, followed by the learned patterns. The
learned patterns are ordered based on their fre-
quencies. Figure 3 reports the curves for both the
systems and we find that BONIE has a larger area
under the curve as compared to Open IE 4.2.

Estimating recall in Open IE is difficult since
it requires annotators to exhaustively tag all open
extractions in a sentence. To get an estimate, an
author manually tagged 100 sentences with all nu-
merical extractions. We find that BONIE’s recall
is about 48%. Two-thirds of missed recall is be-
cause of missing conjuncts. E.g., it misses the tu-
ple relating retirement age with 68 years in “The
retirement age for men is 65 years and 68 years for
women.” Other missed recall is due to complexity
of sentences or inaccuracy of parsers.

5 Conclusions
We release BONIE 8, the first open numerical re-
lation extractor and other resources for further re-
search. BONIE is based on bootstrapping and pat-
tern learning and follows previous similar works
such as OLLIE. However, for effective bootstrap-
ping and training, it implements various cus-
tomizations specific to numerical relations in cu-
ration of seed fact set, matching of sentences, and
construction of relation phrase at the time of ex-
traction. BONIE significantly outperforms both
open non-numerical IE, and closed numerical IE
systems with 1.5x yield and 15 point precision
gain over a state-of-the-art Open IE system. We
find that better conjunction processing is an im-
portant future step for improving BONIE’s recall
even further.

Acknowledgements
This work is supported by Google language un-
derstanding and knowledge discovery focused re-
search grants, a Bloomberg award, a Visvesvaraya
faculty award by Govt. of India and an IRD seed
grant at I.I.T, Delhi. We also thank Microsoft for
the Microsoft Azure sponsorship and a Microsoft
Travel grant in support of the work.

8Available at https://github.com/Open-NRE

321



References
Hannah Bast and Elmar Haussmann. 2013. Open in-

formation extraction via contextual sentence decom-
position. In 2013 IEEE Seventh International Con-
ference on Semantic Computing, Irvine, CA, USA,
September 16-18, 2013. pages 154–159.

Hannah Bast and Elmar Haussmann. 2014. More infor-
mative open information extraction via simple infer-
ence. In Advances in Information Retrieval - 36th
European Conference on IR Research, ECIR 2014,
Amsterdam, The Netherlands, April 13-16, 2014.
Proceedings. pages 585–590.

Arun Tejasvi Chaganty and Percy Liang. 2016. How
much is 131 million dollars? putting numbers in
perspective with compositional descriptions. In Pro-
ceedings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics, ACL 2016, Au-
gust 7-12, 2016, Berlin, Germany, Volume 1: Long
Papers.

Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2011. An analysis of open infor-
mation extraction based on semantic role labeling.
In Proceedings of the 6th International Conference
on Knowledge Capture (K-CAP 2011), June 26-29,
2011, Banff, Alberta, Canada. pages 113–120.

Luciano Del Corro and Rainer Gemulla. 2013. Clausie:
clause-based open information extraction. In 22nd
International World Wide Web Conference, WWW
’13, Rio de Janeiro, Brazil, May 13-17, 2013. pages
355–366.

Filipe de Sá Mesquita, Jordan Schmidek, and Denil-
son Barbosa. 2013. Effectiveness and efficiency
of open relation extraction. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2013, 18-21 Octo-
ber 2013, Grand Hyatt Seattle, Seattle, Washington,
USA, A meeting of SIGDAT, a Special Interest Group
of the ACL. pages 447–457.

Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM 51(12):68–74.

Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam. 2011. Open infor-
mation extraction: The second generation. In IJCAI
2011, Proceedings of the 22nd International Joint
Conference on Artificial Intelligence, Barcelona,
Catalonia, Spain, July 16-22, 2011. pages 3–10.

Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. YAGO2: A
spatially and temporally enhanced knowledge base
from wikipedia. Artif. Intell. 194:28–61.

Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors. In
ACL 2010, Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, July 11-16, 2010, Uppsala, Sweden. pages 286–
295.

Yusra Ibrahim, Mirek Riedewald, and Gerhard
Weikum. 2016. Making sense of entities and quan-
tities in web tables. In Proceedings of the 25th ACM
International on Conference on Information and
Knowledge Management, CIKM 2016, Indianapolis,
IN, USA, October 24-28, 2016. pages 1703–1712.

Ander Intxaurrondo, Eneko Agirre, Oier Lopez de La-
calle, and Mihai Surdeanu. 2015. Diamonds in the
rough: Event extraction from imperfect microblog
data. In NAACL HLT 2015, The 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Denver, Colorado, USA, May 31 - June 5,
2015. pages 641–650.

Xiao Ling and Daniel S. Weld. 2010. Temporal infor-
mation extraction. In Proceedings of the Twenty-
Fourth AAAI Conference on Artificial Intelligence,
AAAI 2010, Atlanta, Georgia, USA, July 11-15,
2010.

Aman Madaan, Ashish Mittal, Mausam, Ganesh Ra-
makrishnan, and Sunita Sarawagi. 2016. Numer-
ical relation extraction with minimal supervision.
In Proceedings of the Thirtieth AAAI Conference
on Artificial Intelligence, February 12-17, 2016,
Phoenix, Arizona, USA.. pages 2764–2771.

Mausam. 2016. Open information extraction systems
and downstream applications. In Proceedings of the
Twenty-Fifth International Joint Conference on Arti-
ficial Intelligence, IJCAI 2016, New York, NY, USA,
9-15 July 2016. pages 4074–4077.

Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL 2012,
July 12-14, 2012, Jeju Island, Korea. pages 523–
534.

George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM 38(11):39–41.

Paramita Mirza, Simon Razniewski, Fariz Darari, and
Gerhard Weikum. 2017. Cardinal virtues: Extract-
ing relation cardinalities from text.

Sebastian Neumaier, Jürgen Umbrich, Josiane Xavier
Parreira, and Axel Polleres. 2016. Multi-level se-
mantic labelling of numerical values. In The Seman-
tic Web - ISWC 2016 - 15th International Semantic
Web Conference, Kobe, Japan, October 17-21, 2016,
Proceedings, Part I. pages 428–445.

Harinder Pal and Mausam. 2016. Demonyms and
compound relational nouns in nominal open IE.
In Proceedings of the 5th Workshop on Automated
Knowledge Base Construction, AKBC@NAACL-
HLT 2016, San Diego, CA, USA, June 17, 2016.
pages 35–39.

322



Subhro Roy, Tim Vieira, and Dan Roth. 2015. Rea-
soning about quantities in natural language. TACL
3:1–13.

Sunita Sarawagi and Soumen Chakrabarti. 2014.
Open-domain quantity queries on web tables: anno-
tation, response, and consensus models. In The 20th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’14, New
York, NY, USA - August 24 - 27, 2014. pages 711–
720.

Jordan Schmidek and Denilson Barbosa. 2014. Im-
proving open relation extraction via sentence re-
structuring. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC-2014), Reykjavik, Iceland, May 26-
31, 2014.. pages 3720–3723.

Andreas Vlachos and Sebastian Riedel. 2015. Iden-
tification and verification of simple claims about
statistical properties. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portugal,
September 17-21, 2015. pages 2596–2601.

Fei Wu and Daniel S. Weld. 2010. Open informa-
tion extraction using wikipedia. In ACL 2010, Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, July 11-16,
2010, Uppsala, Sweden. pages 118–127.

Clarissa Castellã Xavier, Vera Lúcia Strube de Lima,
and Marlo Souza. 2015. Open information extrac-
tion based on lexical semantics. J. Braz. Comp. Soc.
21(1):4:1–4:14.

Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel,
and Denilson Barbosa. 2013. Open information
extraction with tree kernels. In Human Language
Technologies: Conference of the North American
Chapter of the Association of Computational Lin-
guistics, Proceedings, June 9-14, 2013, Westin
Peachtree Plaza Hotel, Atlanta, Georgia, USA.
pages 868–877.

323


	Bootstrapping for Numerical Open IE

