



















































Evaluating Multiple System Summary Lengths: A Case Study


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 774–778
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

774

Evaluating Multiple System Summary Lengths: A Case Study
Ori Shapira1, David Gabay1,

Hadar Ronen2, Judit Bar-Ilan2, Yael Amsterdamer1,
Ani Nenkova3, and Ido Dagan1

1Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel
2Information Science Department, Bar-Ilan University, Ramat-Gan, Israel

3University of Pennsylvania, Philadelphia, PA
{obspp18, dawid.gabay, hadarg, juditb}@gmail.com

amstery@cs.biu.ac.il, nenkova@seas.upenn.edu, dagan@cs.biu.ac.il

Abstract
Practical summarization systems are expected
to produce summaries of varying lengths, per
user needs. While a couple of early sum-
marization benchmarks tested systems across
multiple summary lengths, this practice was
mostly abandoned due to the assumed cost
of producing reference summaries of multiple
lengths. In this paper, we raise the research
question of whether reference summaries of
a single length can be used to reliably evalu-
ate system summaries of multiple lengths. For
that, we have analyzed a couple of datasets
as a case study, using several variants of the
ROUGE metric that are standard in summa-
rization evaluation. Our findings indicate that
the evaluation protocol in question is indeed
competitive. This result paves the way to prac-
tically evaluating varying-length summaries
with simple, possibly existing, summarization
benchmarks.

1 Introduction

Automated summarization systems typically pro-
duce a text that mimics a manual summary. In
these systems, an important aspect is the output
summary length, which may vary according to
user needs. Consequently, output length has been
a common tunable parameter in pre-neural sum-
marization systems and has been incorporated re-
cently in few neural models as well (Kikuchi et al.,
2016; Fan et al., 2017; Ficler and Goldberg, 2017).

It was originally assumed that summarization
systems should be assessed across multiple sum-
mary lengths. For that, the earliest Document Un-
derstand Conference (DUC) (NIST, 2011) bench-
marks, in 2001 and 2002, defined several tar-
get summary lengths and evaluated each summary
against (manually written) reference summaries of
the same length.

However, due to the high cost incurred, subse-
quent DUC and TAC (NIST, 2018) benchmarks

(2003-2014), as well as the more recently popular
datasets CNN/Daily Mail (Nallapati et al., 2016)
and Gigaword (Graff et al., 2003), included refer-
ences and evaluation for just one summary length
per input text. Accordingly, systems were asked
to produce a single summary, of corresponding
length. This decision was partly supported by an
observation that system rankings tended to corre-
late across different summary lengths (Over et al.,
2007), even though, as we show in Section 2, this
correlation is limited.

In this paper, we propose that the summariza-
tion community should consider resuming evalu-
ating summarization systems over multiple length
outputs, as it would allow better assessment of
length-related performance within and across sys-
tems (illustrated in Section 3). To avoid the need in
multiple-length reference summaries we raise the
following research question: can reference sum-
maries of a single length be used to evaluate sys-
tem summaries of multiple lengths, as reliably as
when using references of multiple lengths, with
respect to different standard evaluation metrics?
Recently, Kikuchi et al. (2016) evaluated system
summaries of three different lengths against ref-
erence summaries of a single length. Yet, their
evaluation methodology was not assessed through
correlation to human judgment, as has been com-
monly done for other automatic evaluation pro-
tocols. Here, we provide a closer look into this
methodology, given its potential value.

As a first accessible case study, we test our re-
search question over the DUC 2001 and 2002 data
(Section 2). To the best of our knowledge, these
are the only two datasets that include multiple
length reference and submitted system summaries,
as well as manual assessment of the latter. Our
analysis reveals that, for this data and with respect
to various highly utilized automatic ROUGE met-
rics, the answer to our question is affirmative, in



775

# refs ref lengths (# words) # clusters # systems
2001 3 50, 100, 200, 400 30 14
2002 2 10, 50, 100, 200 59 10

Table 1: DUC 2001 and 2002. Number of refer-
ence summaries per length for each text cluster,
reference lengths, number of clusters and number
of evaluated systems.

terms of correlation with human judgment.
Our promising results suggest repeating the as-

sessment methodology presented here in future
work, to test our question over more recent and
broader summarization datasets and human eval-
uation schemes. This, in turn, would allow the
community to feasibly resume proper evaluation
and deliberate development of systems that target
effective summaries across a range of lengths.

2 Case Study Analysis

Here, we first examine the relevance of our pro-
posal to reinstitute summarization evaluation over
multiple summary lengths. Then, we investigate
our research question of whether using reference
summaries of a single length suffices for evalu-
ating system summaries of multiple lengths. We
turn to the DUC 2001 and 2002 multi-document
summarization datasets, which, to the best of our
knowledge, are the only available datasets that
provide the necessary requirements for this anal-
ysis (see Table 1).

The importance of evaluating and comparing
systems at several lengths is demonstrated with
the observation that system rankings can change
quite significantly at different summary lengths.
In 2001, the Spearman correlation between the
available human rankings of systems at the 50-
word and 400-word lengths is 0.61. For example,
the system ranked first at length 50 ranks sixth at
lengths 200 and 400. Even for the human sys-
tem ranking at the 100-word length, which devi-
ates the least from human rankings at the other
lengths, the correlation with system ranking at the
400 length is only 0.73. Generally, the larger the
difference between a pair of summary lengths, the
greater the fluctuation in system rankings. Simi-
lar trends were observed for DUC 2002, and when
comparing system rankings by automatic ROUGE
scoring (both rankings are elaborated below). Ob-
viously, such performance differences are over-
looked when evaluating systems over summaries
of a single length.

Next, we turn to investigate our research ques-
tion. In this paper, we examine it with respect to
automatic summary evaluation, which has become
most common for system development and evalua-
tion, thanks to its speed and low cost. Specifically,
we use several variants of the ROUGE metric (Lin,
2004), which is almost exclusively utilized as an
automatic evaluation metric class for summariza-
tion. ROUGE variants are based on word sequence
overlap between a system summary and a refer-
ence summary, where each variant measures a dif-
ferent aspect of text comparison. Despite its pit-
falls, ROUGE has shown reasonable correlation
of its system scores to those obtained by manual
evaluation methods (Lin, 2004; Over and James,
2004; Over et al., 2007; Nenkova et al., 2007;
Louis and Nenkova, 2013; Peyrard et al., 2017),
such as SEE (Lin, 2001), responsiveness (NIST,
2006) and Pyramid (Nenkova et al., 2007).

We follow the same methodology of assessing
the reliability of automatic evaluation scores by
measuring their correlation to human evaluation
scores. In our case, DUC 2001 and 2002 ap-
plied the SEE manual evaluation method. NIST
assessors compared systems’ summaries to refer-
ence summaries, which were all decomposed into
a list of elementary discourse units (EDUs). Each
reference EDU was marked throughout the sys-
tem EDUs and was scored for how well it was
expressed. The final manually evaluated scores,
called the human mean content coverage scores,
are provided in the DUC datasets. We can then
correlate the human-based system ranking, at-
tained from these provided scores, to the system
ranking attained from the automatic scores that we
calculate using our proposed methodology.

As a baseline, we consider the ROUGE Recall
scores obtained by the standard reference sum-
mary configuration (Standard, first row in Ta-
ble 2), that is, when system summaries of each
length (table columns) are evaluated against ref-
erence summaries of the same length. This is the
same configuration used by Lin (2004) when intro-
ducing and assessing ROUGE. Then, looking into
our research question, we consider reference sum-
mary configurations in which system summaries
of all lengths are evaluated against reference sum-
maries of a single chosen length (OnlyNNN, sub-
sequent rows of Table 2). In each configuration
(each row), we repeat the evaluation twice: once
using the complete set of available reference sum-



776

System Summary Length
50 100 200 400 Avg. across lengths

3refs 1ref 3refs 1ref 3refs 1ref 3refs 1ref 3refs 1ref
R

ef
er

en
ce

Se
t Standard 0.72 0.65 0.88 0.85 0.9 0.86 0.95 0.94 0.86 0.83

Only50 0 0 +0.02 0 +0.01 +0.04 +0.01 +0.02 +0.010 +0.015
Only100 -0.01 +0.04 0 0 +0.01 -0.01 +0.02 0 +0.005 +0.008
Only200 -0.09 -0.09 -0.06 -0.08 0 0 +0.01 -0.01 -0.035 -0.0045
Only400 -0.06 +0.02 -0.09 -0.09 -0.01 +0.03 0 0 -0.040 -0.010

Table 2: Pearson correlations between ROUGE-1 and human scores over DUC 2001 for different system
summary lengths (column pairs) and different reference summary configurations (rows), when using one
reference or three. The first baseline row presents absolute correlations while successive rows show
relative differences to the baseline.

2001 2002
R-1 R-2 R-L R-1 R-2 R-L

3refs 1ref 3refs 1ref 3refs 1ref 2refs 1ref 2refs 1ref 2refs 1ref

R
ef

er
en

ce
Se

t Standard 0.86 0.83 0.79 0.77 0.87 0.83 0.78 0.75 0.86 0.82 0.82 0.77

Only10 - - - - - - 0 -0.015 -0.100 -0.178 -0.003 -0.045
Only50 +0.010 +0.015 -0.013 -0.038 +0.008 +0.010 +0.035 +0.053 -0.050 -0.038 +0.020 +0.080
Only100 +0.005 +0.008 -0.010 -0.003 +0.005 +0.013 +0.023 +0.048 -0.035 0 -0.008 +0.040
Only200 -0.035 -0.045 -0.055 -0.053 -0.033 -0.04 +0.013 +0.023 -0.068 -0.025 -0.028 +0.005
Only400 -0.040 -0.010 -0.075 -0.075 -0.038 0 - - - - - -

Table 3: Averaged correlations (across system summary lengths, equivalent to the rightmost columns
in Table 2) for different ROUGE variants (column pairs) and reference summary configurations (rows),
when using 1 reference or multiple. The first row presents absolute correlations, with relative differences
in subsequent rows.

maries of the utilized reference length, and once
with just one randomly chosen reference summary
from that set (the 3refs and 1ref sub-columns).

For each reference summary configuration, we
compute ROUGE Recall system scores1 for the
three common ROUGE variants R-1, R-2 and
R-L, which compare unigrams, bigrams and the
longest common subsequence, respectively. Sys-
tem scores, per summary length, are obtained by
averaging across all summarized texts. We then
calculate their Pearson correlation2 with the avail-
able human mean content coverage scores for the
systems. The first row of Table 2 shows these cor-
relations, considering the R-1 scores for the DUC
2001 systems, per summary length. The subse-
quent rows show the corresponding figures for the
single-reference-length configurations. For read-
ability, we present in these rows the relative differ-
ences to the Standard baseline row. Hence, pos-
itive values indicate a configuration that is at least
as good as the standard configuration.

Table 3 presents correlations averaged over all
summary lengths, for the three ROUGE variants

1Omitting stop words.
2Following Lin (2004). Spearman ranking correlations

provide similar results.

over both datasets. We see in the tables that evalu-
ating system summaries of all lengths against ref-
erences of a single length often performs on par
with the standard configuration. In particular, the
single fixed set of 50-word reference summaries
performs overall as well as the standard approach,
and, although not substantially, is the most effec-
tive configuration within the data analyzed. In
other words, in this dataset, the 50-word reference
summaries provide a “test sample” for evaluating
the longer system summaries, which is as effective
as the same length references used by the standard
method.

We note that even when a single reference sum-
mary is available, reasonable correlations with hu-
man scores are obtained for the 50 word reference.
This suggests that it may be possible to compare
system summaries of multiple lengths even against
a single reference summary, of a relatively short
length. This observation seems to deserve further
assessment over recent large scale datasets, such
as CNN/DailyMail, which provide a single rela-
tively short reference for each summarized text.

In addition to correlation to human assessment,
we computed the correlations between system
rankings calculated by Standard and those calcu-



777

100 200 300 400
0.1

0.2

0.3

0.4

0.5

System Summary Length

R
O

U
G

E
-1

ICSI R
S T

Figure 1: R-1 scores of a few systems, evaluated
against the 50-word reference set of DUC 01. Sys-
tems R, S and T are from DUC 01; ICSISumm is
a later competitive system (Gillick et al., 2008).

lated by Only50, at each system summary length.
We find very high correlations (above 0.95 for all
system summary lengths, in both datasets) when
using multiple references and slightly lower (0.85
to 0.9) with one reference summary. These figures
show that the Only50 configuration ranks systems
very similarly to Standard.

To further verify our results, we computed cor-
relations in two additional settings. First, we con-
ducted the same analysis, excluding 2-3 of the
worst systems, which might artificially boost the
correlation (Rankel et al., 2013). Second, we com-
puted score differences between all pairs of sys-
tems, for both human and ROUGE scores, and
computed the correlation between these two sets
of differences (Rankel et al., 2011). In both cases
we observed rather consistent results, assessing
that a single set of short reference summaries eval-
uates system summaries of different lengths just as
well as the standard configuration.

3 Cross-length Summary Evaluation

This section illustrates how system performances
can be measured and compared when evaluating
them on outputs of varying lengths against a sin-
gle reference point. Figure 1 presents the ROUGE
scores of the Only50 configuration for three DUC-
01 submitted systems, and for ICSISumm (Gillick
et al., 2008), a later competitive system.

As expected when measuring ROUGE Recall
against a fixed reference length, longer system
summaries typically cover more of the reference
summaries content than shorter ones, yielding
higher scores. Yet, it can be noted, for example,
that the value of the 400-word summary of system
R in the figure is lower than that of the 200-word
summaries of the other systems. Such a compar-

ison is impossible in the standard setup, as each
system length is evaluated against different ref-
erence summaries. We note that similar compar-
isons are embedded in the evaluations of Stein-
berger and Jezek (2004) and Kikuchi et al. (2016),
who also evaluated multiple summary lengths.

Further, one can define the marginal value
of longer summaries of a given system as the
ROUGE score increase per number of additional
words, namely the graph slope. This denotation
allows measuring the effectiveness of producing
longer summaries. For example, deploying sys-
tem R, we might decide to output only summaries
no longer than 200 words, since the marginal value
of longer summaries becomes too small. The other
systems, on the other hand, seem marginally effec-
tive also in 400 word summaries.

4 Discussion

We proposed the potential value of evaluat-
ing summarization systems at different summary
lengths. Such evaluations would allow proper
evaluation of systems’ “length knob”, track-
ing how their ranking changes across summary
lengths as well as tracking the cross-length be-
havior of individual systems. Given that reference
summaries of a single length are usually available
in practice, we analyzed the potential use of refer-
ence summaries of a single length for evaluating
system summaries of multiple lengths. We found,
on the only two datasets readily available for such
analysis, that this configuration is as reliable as the
standard configuration, which evaluates each sys-
tem summary against a reference of a matching
length.

To broadly substantiate our findings, we pro-
pose future work that would follow our assess-
ment methodology over test samples from cur-
rent datasets (e.g. CNN/DailyMail), judging per-
formance of current systems and utilizing current
manual evaluation protocols. This would require
preparing, for limited samples, additional manu-
ally crafted summaries of several lengths and man-
ually evaluating system summaries of correspond-
ing lengths. Using such data, it will be possible
to repeat our analysis and test the broader valid-
ity of the single-reference-length configuration. If
broadly assessed, it will be possible to start evalu-
ating system summaries of multiple lengths over
most currently available datasets, leveraging the
available single-length reference summaries. Fu-



778

ture benchmarks could require systems to produce
different length outputs, while feasibly evaluating
them using the existing, single length, reference
summaries. This, in turn, is likely to drive research
to better address the need for producing high qual-
ity summaries flexibly across a range of summary
lengths, a dimension that has been disregarded for
long.

Acknowledgments

We would like to thank the anonymous reviewers
for their constructive comments. We thank Yin-
fei Yang for his assistance in producing the IC-
SISumm summaries that we utilized in our anal-
ysis. This work was supported in part by grants
from the MAGNET program of the Israel Innova-
tion Authority; by the German Research Founda-
tion through the German-Israeli Project Coopera-
tion (DIP, grant DA 1600/1-1); by the BIU Center
for Research in Applied Cryptography and Cyber
Security in conjunction with the Israel National
Cyber Bureau in the Prime Ministers Office; and
by the Israel Science Foundation (grants 1157/16
and 1951/17).

References
Angela Fan, David Grangier, and Michael Auli. 2017.

Controllable abstractive summarization. CoRR,
abs/1711.05217.

Jessica Ficler and Yoav Goldberg. 2017. Controlling
linguistic style aspects in neural language genera-
tion. In Proceedings of the Workshop on Stylistic
Variation, pages 94–104. Association for Computa-
tional Linguistics.

Daniel Gillick, Benoit Favre, and Dilek Hakkani-Tür.
2008. The ICSI Summarization System at TAC
2008. In TAC.

David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2003. English gigaword. Linguistic Data
Consortium, Philadelphia, 4:1.

Yuta Kikuchi, Graham Neubig, Ryohei Sasano, Hi-
roya Takamura, and Manabu Okumura. 2016. Con-
trolling output length in neural encoder-decoders.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1328–1338. Association for Computational Linguis-
tics.

Chin-Yew Lin. 2001. Summary evaluation en-
vironment user guide. http://www1.cs.
columbia.edu/nlp/tides/SEEManual.
pdf.

Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Annie Louis and Ani Nenkova. 2013. Automatically
assessing machine summary content without a gold
standard. Computational Linguistics, 39(2):267–
300.

Ramesh Nallapati, Bowen Zhou, Cı́cero Nogueira dos
Santos, Çaglar Gülçehre, and Bing Xiang. 2016.
Abstractive text summarization using sequence-to-
sequence rnns and beyond. In Proceedings of the
20th SIGNLL Conference on Computational Natural
Language Learning, CoNLL 2016, Berlin, Germany,
August 11-12, 2016, pages 280–290.

Ani Nenkova, Rebecca Passonneau, and Kathleen
McKeown. 2007. The pyramid method: Incorpo-
rating human content selection variation in summa-
rization evaluation. ACM Transactions on Speech
and Language Processing (TSLP), 4(2):4.

NIST. 2006. Responsiveness assessment instructions.
www-nlpir.nist.gov/projects/duc/
duc2006/responsiveness.assessment.
instructions.

NIST. 2011. Document Understanding Conferences.
https://duc.nist.gov/.

NIST. 2018. Text Analysis Conferences. https://
tac.nist.gov/.

Paul Over, Hoa Dang, and Donna Harman. 2007. DUC
in context. Information Processing & Management,
43(6):1506–1520.

Paul Over and Yen James. 2004. An Introduction to
DUC 2004 Intrisic Evaluation of Generic News
Text Summarization Systems. duc.nist.gov/
pubs/2004slides/duc2004.intro.pdf.

Maxime Peyrard, Teresa Botschen, and Iryna
Gurevych. 2017. Learning to score system
summaries for better content selection evaluation.
In Proceedings of the EMNLP 2017 Workshop on
New Frontiers in Summarization, pages 74–84.

Peter Rankel, John M Conroy, Eric V Slud, and Di-
anne P O’Leary. 2011. Ranking human and machine
summarization systems. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 467–473. Association for Com-
putational Linguistics.

Peter A Rankel, John M Conroy, Hoa Trang Dang, and
Ani Nenkova. 2013. A decade of automatic content
evaluation of news summaries: Reassessing the state
of the art. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), volume 2, pages 131–136.

Josef Steinberger and Karel Jezek. 2004. Using latent
semantic analysis in text summarization and sum-
mary evaluation. Proc. ISIM, 4:93–100.

http://aclweb.org/anthology/W17-4912
http://aclweb.org/anthology/W17-4912
http://aclweb.org/anthology/W17-4912
https://doi.org/10.18653/v1/D16-1140
https://doi.org/10.18653/v1/D16-1140
http://www1.cs.columbia.edu/nlp/tides/SEEManual.pdf
http://www1.cs.columbia.edu/nlp/tides/SEEManual.pdf
http://www1.cs.columbia.edu/nlp/tides/SEEManual.pdf
www-nlpir.nist.gov/projects/duc/duc2006/ responsiveness.assessment.instructions
www-nlpir.nist.gov/projects/duc/duc2006/ responsiveness.assessment.instructions
www-nlpir.nist.gov/projects/duc/duc2006/ responsiveness.assessment.instructions
https://duc.nist.gov/
https://tac.nist.gov/
https://tac.nist.gov/
duc.nist.gov/pubs/2004slides/duc2004.intro.pdf
duc.nist.gov/pubs/2004slides/duc2004.intro.pdf

