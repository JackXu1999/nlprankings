



















































Underspecified Universal Dependency Structures as Inputs for Multilingual Surface Realisation


Proceedings of The 11th International Natural Language Generation Conference, pages 199–209,
Tilburg, The Netherlands, November 5-8, 2018. c©2018 Association for Computational Linguistics

199

Underspecified Universal Dependency Structures
as Inputs for Multilingual Surface Realisation

Simon Mille
Universitat Pompeu Fabra

Barcelona, Spain
simon.mille@upf.edu

Anja Belz
University of Brighton

Brighton, UK
a.s.belz@brighton.ac.uk

Bernd Bohnet
Google Inc.
London, UK

bohnetbd@google.com

Leo Wanner
ICREA and Universitat Pompeu Fabra

Barcelona, Spain
leo.wanner@upf.edu

Abstract

In this paper, we present the datasets
used in the Shallow and Deep Tracks of
the First Multilingual Surface Realisation
Shared Task (SR’18). For the Shallow
Track, data in ten languages has been re-
leased: Arabic, Czech, Dutch, English,
Finnish, French, Italian, Portuguese, Rus-
sian and Spanish. For the Deep Track, data
in three languages is made available: En-
glish, French and Spanish. We describe in
detail how the datasets were derived from
the Universal Dependencies V2.0, and re-
port on an evaluation of the Deep Track
input quality. In addition, we examine the
motivation for, and likely usefulness of,
deriving NLG inputs from annotations in
resources originally developed for Natu-
ral Language Understanding (NLU), and
assess whether the resulting inputs supply
enough information of the right kind for
the final stage in the NLG process.

1 Introduction

There has long been an assumption in Natural
Language Generation (NLG) that surface realisa-
tion can be treated as an independent subtask for
which stand-alone, plug-and-play tools can, and
should, be created. Early surface realisers such
as KPML (Bateman, 1997) and FUF/Surge (El-
hadad and Robin, 1996) were ambitious, indepen-
dent surface realisation tools for English with wide
grammatical coverage. However, the question of
how the NLG components addressing the stage be-
fore surface realisation were supposed to put to-
gether inputs of the level of grammatical sophis-
tication required by such tools was never quite

resolved. The success of SimpleNLG (Gatt and
Reiter, 2009) which had much reduced grammati-
cal coverage, but accepted radically simpler inputs
demonstrated the importance of this issue.

The recently completed first Multilingual Sur-
face Realisation Task (SR’18) (Mille et al., 2018)
used for the first time inputs derived from the Uni-
versal Dependencies (UDs) (de Marneffe et al.,
2014), a framework which was devised with the
aim of facilitating cross-linguistically consistent
grammatical annotation, and which has grown into
a large-scale community effort involving more
than 200 contributors, who have created over 100
treebanks in over 70 languages between them.1

UDs provide a more general and potentially flex-
ible input representation for surface realisation
(SR). However, their use for NLG has not so far
been demonstrated.

In this paper, we present the UD datasets used
in the Shallow and Deep Tracks in SR’18, describe
the precise conversion processes that were applied
to them, and provide an assessment of their qual-
ity. Furthermore, we examine (a) the SR task in
general, (b) the motivation for, and likely useful-
ness of, the derivation of NLG inputs from annota-
tions in resources developed for Natural Language
Understanding (NLU), (c) whether the resulting
inputs supply enough information of the right kind
for the final stage in the NLG process, and more
tentatively, (d) what role SR is likely to play in the
future in the NLG context.

Section 2 presents related work; Section 3 de-
scribes the datasets used in the two SR’18 tracks,
and Section 4 provides a more procedural account
of how the datasets were generated. Section 5 as-
sesses the quality of the obtained representations,

1http://universaldependencies.org/

http://universaldependencies.org/


200

while Section 6 discusses their suitability for SR
and NLG more generally. Some conclusions are
presented in Section 7.

2 Background

With the advent of large-scale treebanks and sta-
tistical NLG, surface realisation research turned to
the use of treebank annotations, processed in var-
ious ways, as inputs to surface realisation. Anno-
tation/sentence pairs constitute the training data,
and similarity to the original sentences in the tree-
bank is the main measure of success. A lot of
this work used inputs derived from the Wall Street
Journal corpus with varying amounts of infor-
mation removed from the parse-tree annotations
(Langkilde-Geary, 2002; Nakanishi et al., 2005;
Zhong and Stent, 2005; Cahill and van Genabith,
2006; White and Rajkumar, 2009). Because of
the variation among inputs, results were not en-
tirely comparable. The first Surface Realisation
Shared Task (SR’11) (Belz et al., 2011) was thus
conceived with the aim of developing a common-
ground input representation that would make dif-
ferent systems, for the first time, directly com-
parable. SR’11 used shallow and deep inputs
for its two respective tracks, both derived from
CoNLL’08 shared task data, which was in turn de-
rived from the WSJ Corpus by automatically con-
verting the corresponding Penn TreeBank parse
trees to dependency structures (Surdeanu et al.,
2008). While dependency structures offer a more
flexible input structure and statistical systems, in
principle, offer more robustness, the uptake of
such systems as components in embedding NLG
systems has been very limited.

Meanwhile, many of the more applied strands
of NLG research have tended to bypass an ex-
plicit interface to surface realisation, instead map-
ping directly from more abstract representations
of meaning to surface text. Recently, mostly un-
der the aegis of the Generation Challenges series
of shared tasks, several large-scale datasets have
been made available that pair surface text with
more abstract structured inputs, including:

• Weather forecast generation (Weather)
dataset (Liang et al., 2009): time series from
weather-related measurements;

• Abstract Meaning Representation (AMR)
dataset (May and Priyadarshi, 2017): abstract
predicate-argument graphs that cover several
genres;

• WebNLG dataset (Gardent et al., 2017): DB-
pedia triples covering properties of 15 DBpe-
dia categories;

• E2E dataset (Novikova et al., 2017):
attribute-value pairs covering 8 properties
related to the restaurant domain.

In all these datasets, the input structures are
aligned with English sentences that match their
contents. In the case of inputs coming from struc-
tured data (e.g. WebNLG, E2E, Weather, above),
multiple sentences are generally paired with each
input, whereas for the inputs coming from data
initially annotated for Natural Language Under-
standing tasks (SR’11, AMR), only one reference
per input is available. Both types of shared tasks
(reusable surface realisation vs. task-specific gen-
eration) have been successful in terms of participa-
tion levels, and both have sizeable research com-
munities behind them, but trainable surface reali-
sation as a stand-alone subtask still has a way to
go in terms of demonstrating its practical applica-
bility.

3 The SR’18 Data

The First Multilingual Surface Realisation Shared
Task (SR’18) ran from December 2017 to July
2018. As in SR’11, the shared task comprised two
tracks with different levels of difficulty: a Shallow
Track, starting from syntactic structures in which
word order information has been removed and to-
kens have been lemmatised, and a Deep Track,
which starts from more abstract structures from
which, additionally, functional words (in partic-
ular, auxiliaries, functional prepositions and con-
junctions) and surface-oriented morphological in-
formation have been removed.

Taking advantage of the growing availability
of multilingual treebanks annotated with Univer-
sal Dependencies, the UD V2.0 treebank, as re-
leased in the context of the CoNLL 2017 shared
task on multilingual dependency parsing (Zeman
et al., 2017), was used. A subset of ten languages
was selected that contains the necessary part-of-
speech and morphological tags for the Shallow
Track: Arabic, Czech, Dutch, English, Finnish,
French, Italian, Portuguese, Russian and Span-
ish. Three of these languages, namely English,
French and Spanish were used also for the Deep
Track. Starting from UD structures as they ap-
pear in the treebanks, Shallow and Deep inputs



201

1 The the DET DT Definite=Def|PronType=Art 2 det
2 third third ADJ JJ Degree=Pos|NumType=Ord 5 nsubj pass
3 was be AUX VBD Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin 5 aux
4 being be AUX VBG VerbForm=Ger 5 aux pass
5 run run VERB VBN Tense=Past|VerbForm=Part|Voice=Pass 0 root
6 by by ADP IN 8 case
7 the the DET DT Definite=Def|PronType=Art 8 det
8 head head NOUN NN Number=Sing 5 obl
9 of of ADP IN 12 case
10 an a DET DT Definite=Ind|PronType=Art 12 det
11 investment investment NOUN NN Number=Sing 12 compound
12 firm firm NOUN NN Number=Sing 8 nmod
13 . . PUNCT . 5 punct

Figure 1: A sample UD structure in English (top: CoNLL-U, bottom: graphical)

1 the DET DT Definite=Def|PronType=Art 2 det
2 third ADJ JJ Degree=Pos|NumType=Ord 3 nsubj pass
3 run VERB VBN Tense=Past|VerbForm=Part|Voice=Pass 0 root
4 be AUX VBD Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin 3 aux
5 be AUX VBG VerbForm=Ger 3 aux pass
6 head NOUN NN Number=Sing 3 obl
7 . PUNCT . 3 punct
8 by ADP IN 6 case
9 the DET DT Definite=Def|PronType=Art 6 det
10 firm NOUN NN Number=Sing 6 nmod
11 an DET DT Definite=Ind|PronType=Art 10 det
12 investment NOUN NN Number=Sing 10 compound
13 of ADP IN 10 case

Figure 2: Shallow input (Track 1) derived from UD structure in Figure 1. (top: CoNLL-U, bottom:
graphical)

were created automatically for the two tracks. The
inputs for each language and track were released
in the CoNLL-U format2, together with parallel
files that contain a reference sentence for each
input. Figures 1, 2 and 3 show a sample orig-
inal UD annotation for English, and the corre-
sponding inputs for the Shallow and Deep Tracks,
respectively, in the 10-column CoNLL-U format
(the last two columns generally do not contain
information and are thus omitted) and in graph-
ical format. The training, development and test
data are available at http://taln.upf.edu/
pages/msr2018-ws/SRST.html#data.

2http://universaldependencies.org/
format.html

3.1 Shallow inputs

The Shallow Track input structures are unordered
syntactic trees with all the words of the sentence
replaced with their lemmas, and labelled with their
part-of-speech tags and the morphological infor-
mation associated with each node. These struc-
tures are thus genuine UD structures, with only
two differences: first, in the original CoNLL-U
format, consecutive lines contain words that are
also consecutive in the sentence, whereas in the
SR’18 Shallow structures, no order information is
available; second, original UD structures contain
both lemmas and inflected forms, while only the
former are available in the SR’18 structures. Fig-

http://taln.upf.edu/pages/msr2018-ws/SRST.html#data
http://taln.upf.edu/pages/msr2018-ws/SRST.html#data
http://universaldependencies.org/format.html
http://universaldependencies.org/format.html


202

1 third ADJ Degree=Pos 2 A2
2 run VERB Tense=Past|Aspect=Progr 0 ROOT
3 head NOUN Number=Sing|Definiteness=Def 2 A1
4 firm NOUN Number=Sing|Definiteness=Indef 3 A2
5 investment NOUN Number=Sing 4 AM

Figure 3: Deep input (Track 2) derived from UD structure in Figure 1. (left: CoNLL-U, righ: graphical)

ures 1 and 2 show an original UD structure and a
SR’18 Shallow input, respectively.

3.2 Deep inputs

The Deep Track input structures are trees that
contain only content words linked by predicate-
argument edges, in the PropBank/NomBank
(Palmer et al., 2005; Meyers et al., 2004) fashion.

The Deep inputs can be seen as closer to a re-
alistic application context for NLG systems, in
which the component that generates the inputs pre-
sumably would not have access to syntactic or
language-specific information. At the same time,
we used only information found in the UD struc-
tures to create the Deep inputs, and tried to keep
their structure simple. In Deep inputs, words
are not disambiguated, full (semantically loaded)
prepositions may be missing, and some argument
relations may be underspecified or missing. The
next two subsections provide more details about
the Deep nodes and edge labels.

3.2.1 Deep nodes
In contrast to the Shallow structures, which con-
tain all the (lemmatised) words of the original sen-
tence, the Deep structures do not contain func-
tional words that can be inferred from another lex-
ical unit or from the syntactic structure (such as
bound prepositions and conjunctions), or can be
represented as a feature on another node (such as
auxiliaries, modals, or determiners). For instance
(see also Figure 1 for the first four examples):

• the preposition by in the sentence The third
was being run by the head of an investment
firm is bound to the English agentive depen-
dency in a passive construction;

• the preposition of in the head of an investment
firm is bound to the noun head, and indicates
the presence of its second argument;

• being in was being run is a passive voice
marker, while was is part of the marker for
the progressive aspect, associated with the
verb run;

• the in the head can be seen as a marker for
nominal definiteness;

• the conjunction (complementiser) that in,
e.g., I demand that you apologise, appears be-
cause it connects a finite verb apologise as an
argument of another verb demand.

Meaningful functional words such as auxiliaries
and determiners are represented as attribute/value
pairs associated with the relevant nodes, as, e.g.,
the Aspect feature on run in Figure 3.3 Features
are also used to encode information such as ver-
bal tense or nominal number, which are needed
for realisation. On the other hand, implicit nodes
that have a role in the sentence are explicit in the
Deep input: for instance, dummy pronoun nodes
for the subject if an originally finite verb has no
first argument and no available argument to build
a passive or, for a pro-drop language such as Span-
ish, dummy pronouns when the first argument of a
verb is missing.

3.2.2 Deep edge labels

In Deep inputs, content words are linked
by predicate-argument labels in the Prop-
Bank/NomBank (Palmer et al., 2005; Meyers
et al., 2004) fashion, that is, there are core (A1,
A2, etc.) and non-core (AM) labels. Additional
labels such as LIST or NAME have been added in
order to connect all the elements within each sen-
tence; see Table 1 for the inventory of relations.
This subsection details the main features of the
dependencies at this level.

First of all, the first argument is always labeled
as A1, that is, there is no external argument A0,
as can be found in PropBank and NomBank. For
instance, both the verbs fall and fancy have a rela-
tion A1 with their first argument (the ballA1 falls,
MartinA1 fancies Chloe), even though according
to PropBank, the latter has an external argument
(MartinA0 fancies Chloe).

3For the available universal features set, see
http://universaldependencies.org/u/feat/
index.html.

http://universaldependencies.org/u/feat/index.html
http://universaldependencies.org/u/feat/index.html


203

Deep label Description Example
A1, A2, ..., A6 nth argument of a predicate fall→ the ball

A1INV, ..., A6INV nth inverted argument of a predicate the ball→ fall

AM/AMINV (i) none of governor or dependent are argument of the other fall→ last night(ii) unknown argument slot
LIST List of elements fall→ [and] bounce

NAME Part of a name Tower→ Eiffel
DEP Undefined dependent N/A

Table 1: Deep labels

Second, in order to maintain the tree struc-
ture and account for some cases of shared argu-
ments, there can be inverted argument relations.
Consider, for instance, the difference between the
ballA1 falls, in which ball is the first argument of
fall, and the fallingA1INV ball, in which fall has
the ‘inverted first argument’ label, which means
that ball is actually the first argument of fall. In-
verted relations are used also in relative clauses, as
e.g., in the ball that fallsA1INV ; in such a case, the
subject (or object) relative pronoun does not ap-
pear in the structure, and the antecedent can thus
be shared by two predicates, as An and AnINV.

Third, all modifier edges are assigned the same
generic label AM. When an edge is underspeci-
fied, that is, when there is no predicate-argument
relation between two connected nodes, or when
there is a predicate-argument relation between two
nodes, but it is not known which one specifically,
the AM or AMINV label is used. For instance, in
the case of nominal adverbials such as last night,
the relation between the governing predicate and
night is set as an AM. One of the most productive
uses of the AM label is for prepositional groups
or subordinate clauses, in which UD establishes a
direct dependency between the content words that
actually does not exist: he will leaveGov after he
finishesDep his work. In this case, finish is estab-
lished as an AM of leave; note that this does not
prevent the actual argumental structure from being
recovered. Indeed, an edge AM that links a Gover-
nor with a Dependent and its A2INV preposition:

Gov Dep Prep

AM A2INV

is equivalent to the preposition being a predicate
and the Governor and Dependent being its first and
second argument respectively:

Gov Prep Dep

A1 A2

Fourth, there are some edge labels that are not

related to predicate-argument relations: coordi-
nated elements are linked by a dedicated edge
LIST, compound named entities are linked by the
edge NAME, and the edge label DEP is used in
case of an unknown relation between two ele-
ments. Finally, each argument relation is unique
for each predicate: if a predicate has an A2 depen-
dent, it cannot have another A2 dependent, and it
cannot be the A2INV dependent of another predi-
cate.

In Section 4.2 below, we describe how the map-
ping between surface dependencies and predicate-
argument labels was performed.

4 Generating the datasets

Following on from the more declarative descrip-
tion of Shallow and Deep inputs in the previous
section, this section describes how those input
were automatically created from the CoNLL’17
data.

4.1 Adaptation of the original UD structures
For the input to the Shallow Track, the UD struc-
tures were processed as follows:

1. the information on word order was removed
by randomised scrambling;

2. the words were replaced by their lemmas.

For the Deep Track, additionally:

3. Functional prepositions and conjunctions in
argument position, i.e. prepositions and con-
junctions that can be inferred from other lex-
ical units or from the syntactic structure,
were removed; prepositions and conjunctions
retained in the Deep representation can be
found under an A2INV dependency;

4. Definite and indefinite determiners, auxil-
iaries and modals were converted into at-
tribute/value pairs, as are definiteness fea-
tures, and the universal aspect and mood fea-
tures;



204

ar cs en es fi fr it nl pt ru
train 6,016 66,485 12,375 14,289 12,030 14,529 12,796 12,318 8,325 48,119
dev 897 9,016 1,978 1,651 1,336 1,473 562 720 559 6,441
test 676 9,876 2,061 1,719 1,525 416 480 685 476 6,366

Table 2: SR’18 dataset sizes for training, development and test sets.

5. copulas were removed and their two argu-
ments connected with each other;

6. Subject and object relative pronouns directly
linked to the main relative verb were removed
(and instead, the verb is linked to the an-
tecedent of the pronoun), and dummy pro-
nouns were added.

7. Edge labels were generalised using the labels
presented in Section 3.2.2;

8. Surface-level morphologically relevant infor-
mation as prescribed by syntactic structure or
agreement (such as verbal finiteness or ver-
bal number) was removed, whereas semantic-
level information such as nominal number
and verbal tense was retained;

9. Fine-grained POS labels found in some tree-
banks (see e.g. column 5 in Figure 2) were
removed, and only coarse-grained ones were
retained (column 4 in Figures 2 and 3).

4.2 Generation of the inputs

Shallow Track inputs were generated with a
Python script from the original UD structures,
which were simply scrambled and their words re-
placed with lemmas. During the conversion, sen-
tences that contained dependencies that only make
sense in an analysis context were filtered out (e.g.
reparandum, or orphan); this amounted to around
1.5% of sentences for all languages on average.
Table 2 summarises the final size of the datasets.

Deep Track inputs were then generated by au-
tomatically processing the Shallow Track struc-
tures using a series of graph-transduction gram-
mars (Bohnet and Wanner, 2010) that cover steps
3–9 above (in a similar fashion to Mille et al.
(2017)), while ensuring a node-to-node correspon-
dence between the Deep and Shallow structures.

The graph-transduction grammars are rules that
apply to a subgraph of the input structure and pro-
duce a part of the output structure. During the ap-
plication of the rules, both the input structure (cov-
ered by the left-hand side of the rule) and the cur-
rent state of the output structure at the moment of
application of a rule (i.e., the right-hand side of the

rule) are available as context. Figure 4 shows the
rule that assigns deep dependency labels as found
in the UD lexicon dictionary; relative clause edges
and adjectival modifiers are handled by different
rules. The output structure in one transduction is
built incrementally: the rules are all evaluated, the
ones that have no right-hand side context and that
match a part of the input graph are applied, and
a first piece of the output graph is built; then the
rules are evaluated again, this time with the right-
hand side context as well, and another part of the
output graph is built; and so on. The transduc-
tion is complete when no rule is left that matches
the combination of the left-hand side and the right-
hand side. At each iteration, the rules are first se-
lected and then applied as a cluster, that is, the or-
der in which they apply is not important. The only
way to force a rule R2 to be applied after a rule R1
(for instance, for building edges after building the
nodes) is to establish as R2’s right-side condition
elements built by R1: for instance, the rule shown
in Figure 4 will apply only after the rules that build
the nodes have applied, since the ?Xr and ?Yr are
marked as right-side context (rc:). This allows the
rules to be more generic and to combine with one
another in an efficient way.

Figure 4: Sample graph transduction rule

Table 3 provides a summary of the graph-
transduction grammars and rules for the map-
ping between surface-syntactic structures and UD-
based semantic structures. The mapping is com-
posed of three submodules. The pre-processing
grammars are used to identify all nodes to be
removed: it is easier and safer in the graph-
transduction grammars to mark the nodes to be re-



205

Grammars # rules Description

Pre-processing 76 Identify nodes to be removedIdentify verbal finiteness and tense

SSynt-Sem 120

Remove idiosyncratic nodes
Establish correspondences with surface nodes
Map UD labels to predicate-argument dependency labels (when possible)
Predict predicate-argument dependency labels (when no direct mapping is available)
Replace determiners, modality and aspect markers by attribute-value feature structures

Post-processing 60 Replace duplicated argument relations by best educated guessIdentify remaining duplicated core dependency labels (for posterior debugging)

Table 3: Graph-transduction rules for producing the Deep inputs (counts include rules that simply copy
node features, constituting about 40 per grammar).

moved (using solely positive conditions) and then
to not generate them in the next step, than using
rules that generate in one shot only the desired
nodes, which would imply complex negative con-
ditions. After this pre-processing, with the nodes
that are to be removed marked, the core grammar
(SSynt-Sem) takes care of establishing edge labels
between the remaining nodes and associating the
latter with attribute/value pairs. Most UD labels
are mapped one-to-one to predicate-argument la-
bels. In some cases only, the rules check the syn-
tactic context of an edge in order to get a more
precise label (e.g., a relative clause in which ac-
cording to the structure and the UD label, we know
that an argumental relation is holding: if the gov-
ernor already has a first and a third arguments,
the argumental relation is likely to be an A2). A
post-processing grammar takes educated guesses
in order to correct obvious labeling errors such
as the duplication of an argument for a predicate.
The unified cross-language annotation scheme of
UD allows the large majority of the rules to be
language-independent. Even though the annota-
tions are not always consistent, adapting the gram-
mars to a new language is relatively easy: most of
the language-specific rules concern the processing
of auxiliaries and modals, which have to be identi-
fied and mapped to the Aspect and Mood features.

5 Evaluation of the generated datasets

Since the processing applied to the Shallow in-
puts consists only in removing information and is
very straightforward. It does not call for an eval-
uation. For the Deep Track, however, the changes
are much more complex and the quality of the con-
version needs to be assessed.

We evaluated the quality of the Deep inputs
as follows. One of the authors manually anno-
tated about 900 deep tokens (≈75 sentences) in
each language (English, French and Spanish), by

post-editing the automatically converted structures
correcting any mistakes. Since the same person
post-edited all three datasets, the resulting gold-
standard is consistent across the languages, even
though it does not allow for calculating inter-
annotator agreement. Note that the annotation re-
mains quite open with respect to some phenom-
ena, for which several annotations are considered
correct. For instance, AM relations are left under-
specified when it is not clear what argument slot is
concerned (e.g., appositions, parentheticals, ver-
bal/nominal adverbials, etc.); some argumental re-
lations are ambiguous and left as such: N→ ADJ
is sometimes A1INV, and the adjective is some-
times an argument of the noun; numbers (e.g., ten
thousand people) and hours are left as they are in
the original annotations.

Once post-edited, the reference structures are
compared to the ones produced by the automatic
mapping from UD structures, using the LAS eval-
uation method of Ballesteros et al. (2015), specif-
ically designed to handle the comparison between
non-isomorphic trees. Since part of the map-
ping consists of adding and/or removing nodes,
it often happens that the gold-standard and pre-
dicted structures end up with a different number of
nodes, which makes evaluation scripts based on a
strict node-to-node comparison unusable. Table 4
shows the results of the evaluation. Quality is not
the same across languages: while English struc-
tures obtain an LAS of 79.83, French is more than
6 points lower, and Spanish more than 12. Since,
as mentioned in the previous subsection, the map-
ping grammars are largely language-independent,
and since roughly the same efforts have been ded-
icated to each language, it is likely that the LAS
numbers reflect the quality of the original UD an-
notation.

Note that during the evaluation, POS and lemma-



206

LAS
English 79.83
French 73.43
Spanish 67.28

Table 4: Evaluation of the quality of the output
structures (Labeled Attachment Scores - LAS).

tisation4 errors are not corrected, but structural
errors due to original tagging/lemmatising errors
are counted. In other words, what is being evalu-
ated is how correct the outputs are in terms of de-
pendencies and labeling, rather than how well the
transduction grammars perform. An error analysis
showed that most dependency errors come from
the AM relation, which is usually A1, A2, A1INV
or A2INV in the reference structures. The system-
atic replacement of AM by one of these four labels
always results in a drop of the LAS score. That
is, in order to improve the quality of the struc-
tures, an improvement of the UD structures or a
more fine-grained processing (which would imply
a large number of rules and the use of detailed lex-
icons) would be needed.

The mapping grammars were released together
with the SR’18 datasets;5 they can process about
39 sentences per second on an average laptop. The
resulting mapping tool allows for automatically
annotating large amounts of data. The tool has
recently been used to convert about 600,000 En-
glish sentences that had been automatically parsed
with an off-the-shelf UD parser. This tool is also
currently being tested as part of conceptual rela-
tion extraction pipelines in the framework of sev-
eral EU projects (see Acknowledgements).

6 Discussion

The UD-derived input structures described in this
paper were successfully used in the SR’18 Shared
Task, which attracted the participation of eight
teams. In the Deep Track, an attempt was made to
remove from inputs, as far as possible, the kind of
information that cannot come from a deeper level
of abstraction, such as, e.g., an ontological rep-
resentation. For instance, where it was not pos-
sible, or too risky, to predict an argument slot, it
was left undefined (AM label). If, because the
annotation did not allow for the distinction be-

4As in lexical reflexive verbs in French and Spanish; e.g.
aburrirse ’to be bored’ in Spanish, can end up with the lemma
aburrir, that is, without the reflexive marker.

5http://taln.upf.edu/pages/msr2018-ws/
SRST.html#data

tween the two, there was a choice between leaving
too many syntactic elements or removing mean-
ingful words, the latter option was chosen. In
this way, the Deep representations are much closer
to the kind that might be used in a generation
pipeline that starts from structured data,6 and the
tools trained on the present data can potentially be
used in an applied NLG pipeline. On the other
hand, the inputs can be considered less informa-
tive than those used in SR’11, in which only that-
complementisers and to-infinitives were removed,
and predicate-argument labels for nouns and verbs
were fully specified (since they came from the
NomBank and PropBank manually validated an-
notations). However, as is the case with the tec-
togrammatical layer of the Prague Dependency
Treebank (Böhmová et al., 2005), in PropBank
and NomBank no distinction is made between full
and functional (‘semantic’) prepositions.7 In con-
trast to AMRs, the Deep inputs do contain tense,
number and definiteness information, but links to
named entity databases or OntoNotes labels are
not provided; in other words, the nodes are not
disambiguated. The other difference to AMRs in
terms of specificity is that the annotation of shared
arguments is incomplete in SR’18 (an argument
can only be shared by two predicates in SR’18
Deep inputs), and that the non-core relations are
not typed. In terms of abstraction level, AMRs
abstract the labels of nominal vs. verbal events,
which is not done in the SR’18 dataset. In the
SR’18 Shallow inputs, the removal of word order
information is problematic for named entities, n-
ary coordinations and punctuations, because it is
not always possible to reconstruct the word order
based on the dependencies. In order to cope with
this, in SR’11, the components of named entities
were numbered according to their original order,
specific features encoded the bracketing informa-
tion for punctuation signs, and coordinations were
hierarchical, with each conjunct being a dependent
of the previous conjunct.

As a result, the Deep input representation is a
compromise between correctness and adequacy in
a generation setup. Indeed, the conversion of the

6Note, however, that creating the Deep input structures
from structured data would be far from trivial, since it would
imply mapping given properties onto words as used in the UD
datasets.

7This subcategorisation information can be partially de-
rived from PropBank and NomBank, as done for the Deep
Syntactic structures of Meaning-Text Theory (Ballesteros
et al., 2015; Mille and Wanner, 2015).

http://taln.upf.edu/pages/msr2018-ws/SRST.html##data
http://taln.upf.edu/pages/msr2018-ws/SRST.html##data


207

UD structures into predicate-argument structures
depends not only on the mapping process, but also
on the availability of the information in the origi-
nal annotation, even though it falls short in some
cases: UD structures were not conceived for NLG
applications, which is why using them in an NLG
context presents considerable challenges. The un-
derspecified UD structures created for the SR’18
Shared Task are perhaps as close as we can get to
NLG-like meaning representations if all we have
to construct inputs are parsing annotations from
treebanks. To get even closer, the inputs would
have to be enriched from additional sources of in-
formation, such as subcategorisation information,
as found in PropBank or Ontonotes.

We may even be moving away from a situa-
tion where we have to rely on treebanks to obtain
NLG inputs at reasonable cost, as the success of
using fully automatically parsed data mentioned
above shows. It is conceivable that a future shared
task in NLG will involve paired (structured) data
and text, plus an automatically created intermedi-
ate level of representation comprising underspec-
ified UD (UUD) structures enriched with addi-
tional information obtained from the structured
data level. This would correspond to three linked
tracks (data-to-text, data-to-UUD, and UUD-to-
text) where one track is the end-to-end task, and
the other two tracks are subtasks that can be com-
bined to solve the end-to-end task, similar to the
GREC’10 shared task competition (Belz and Kow,
2010).

Or it could be argued, perhaps controversially
still, that the days of structured linguistic represen-
tations in NLG are numbered anyway. The rapid
development and spread of highly successful neu-
ral approaches to diverse NLG tasks, and the lim-
ited success so far of attempts to inject linguistic
knowledge directly into neural networks, certainly
lends some strength to this point of view. In the
meantime, the above tripartite shared-task struc-
ture has the potential to accommodate both sys-
tems that map directly from data to text without
structured representations, and two-component
systems with a surface realiser as the second com-
ponent.

7 Conclusion

In this paper, we have provided a detailed de-
scription of how the inputs to the Shallow and
Deep Tracks at the First Multilingual Surface Re-

alisation Task were created by automatically con-
verting annotated sentences from Universal De-
pendency treebanks V2.0 into Shallow and Deep
Track inputs. The important contribution here is
the process for creating Deep inputs, where we
approximate the kind of abstract meaning repre-
sentations used in native NLG tasks. This is not a
simple matter of applying a few replacement rules
(as it is for the Shallow inputs) with predictably
correct results. To assess the quality of the Deep
inputs, we conducted an evaluation that showed a
labelled agreement score (LAS) of about 80 with
human-corrected equivalents for English, display-
ing a high level of quality. However, future work
will need to look at how to improve this quality
further, especially for other languages, as well as
to confirm what exactly the right extent of under-
specification is for Deep inputs for surface realisa-
tion.

A separate question the field needs to address is
to what extent Shallow underspecified UD inputs
are suitable for surface realisation. More specif-
ically, whether it is reasonable to expect other,
content-determining, modules in an NLG system
to generate such inputs. Finally, an overlapping
question is whether inputs of either the Shallow
or Deep type provide information that is sufficient
for generating fully realised sentences, and if not,
how such inputs can be enriched to provide it.

Acknowledgments

The work presented here has been supported in
part by three projects funded by the European
Commission: V4Design (H2020-779962-RIA),
TENSOR (H2020-700024-RIA), and beAWARE
(H2020-700475-RIA).

References
Miguel Ballesteros, Bernd Bohnet, Simon Mille, and

Leo Wanner. 2015. Data-driven deep-syntactic de-
pendency parsing. Natural Language Engineering,
pages 1–36.

John Bateman. 1997. Enabling technology for mul-
tilingual natural language generation: The KPML
development environment. Natural Language En-
gineering Journal, 3(1):15–55.

Anja Belz and Eric Kow. 2010. The GREC challenges
2010: Overview and evaluation results. In Pro-
ceedings of the 6th International Natural Language
Generation Conference, INLG’10, pages 219–229,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

http://dl.acm.org/citation.cfm?id=1873738.1873769
http://dl.acm.org/citation.cfm?id=1873738.1873769


208

Anja Belz, Michael White, Dominic Espinosa, Eric
Kow, Deirdre Hogan, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation,
ENLG ’11, pages 217–226, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Alena Böhmová, Silvie Cinková, and Eva Hajičová.
2005. A manual for tectogrammatical layer anno-
tation of the Prague Dependency Treebank (English
translation). Technical report, ÚFAL MFF UK,
Prague, Czech Republic.

Bernd Bohnet and Leo Wanner. 2010. Open soucre
graph transducer interpreter and grammar develop-
ment environment. Valletta, Malta.

Aoife Cahill and Josef van Genabith. 2006. Ro-
bust PCFG-based generation using automatically ac-
quired LFG approximations. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 1033–
1040. Association for Computational Linguistics.

M. Elhadad and J. Robin. 1996. An overview of
SURGE: a reusable comprehensive syntactic real-
ization component. Technical report, Ben Gurion
University in the Negev.

Claire Gardent, Anastasia Shimorina, Shashi Narayan,
and Laura Perez-Beltrachini. 2017. Creating train-
ing corpora for micro-planners. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
Vancouver, Canada. Association for Computational
Linguistics.

Albert Gatt and Ehud Reiter. 2009. Simplenlg: A re-
alisation engine for practical applications. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation, pages 90–93. Association for
Computational Linguistics.

Irene Langkilde-Geary. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proceedings of the
International Natural Language Generation Confer-
ence (INLG’02).

Percy Liang, Michael I Jordan, and Dan Klein. 2009.
Learning semantic correspondences with less super-
vision. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 1-Volume 1, pages
91–99. Association for Computational Linguistics.

Marie-Catherine de Marneffe, Timothy Dozat, Na-
talia Silveira, Katri Haverinen, Filip Ginter, Joakim
Nivre, and Christopher D. Manning. 2014. Uni-
versal stanford dependencies: A cross-linguistic ty-
pology. pages 4585–4592, Reykjavik, Iceland. Eu-
ropean Language Resources Association (ELRA).
ACL Anthology Identifier: L14-1045.

Jonathan May and Jay Priyadarshi. 2017. Semeval-
2017 task 9: Abstract meaning representation
parsing and generation. In Proceedings of the
11th International Workshop on Semantic Evalua-
tion (SemEval-2017), pages 534–543, Vancouver,
Canada. Association for Computational Linguistics.

Adam Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In HLT-
NAACL 2004 Workshop: Frontiers in Corpus Anno-
tation, Boston, MA, May 2004, pages 24–31.

Simon Mille, Anja Belz, Bernd Bohnet, Yvette Gra-
ham, Emily Pitler, and Leo Wanner. 2018. The
First Multilingual Surface Realisation Shared Task
(SR’18): Overview and Evaluation Results. In Pro-
ceedings of the 1st Workshop on Multilingual Sur-
face Realisation (MSR), 56th Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 1–12, Melbourne, Australia.

Simon Mille, Roberto Carlini, Ivan Latorre, and Leo
Wanner. 2017. UPF at EPE 2017: Transduction-
based deep analysis. In Shared Task on Extrinsic
Parser Evaluation (EPE 2017), pages 80–88, Pisa,
Italy.

Simon Mille and Leo Wanner. 2015. Towards large-
coverage detailed lexical resources for data-to-text
generation. In Proceedings of the First International
Workshop on Data-to-text Generation, Edinburgh,
Scotland.

Hiroko Nakanishi, Ysuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic models for disambiguation of an
HPSG-based chart generator. In Proceedings of the
International Workshop on Parsing Technologies.

Jekaterina Novikova, Ondrej Dušek, and Verena Rieser.
2017. The E2E dataset: New challenges for end-
to-end generation. In Proceedings of the 18th
Annual Meeting of the Special Interest Group on
Discourse and Dialogue, Saarbrücken, Germany.
ArXiv:1706.09254.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–105.

Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluı́s Màrquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In Proceedings of
the International Conference on Computational Lin-
guistics and the Annual Meeting of the Association
for Computational Linguistics (COLING-ACL’08).

Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP’09).

http://dl.acm.org/citation.cfm?id=2187681.2187719
http://dl.acm.org/citation.cfm?id=2187681.2187719
http://dl.acm.org/citation.cfm?id=2187681.2187719
http://www.lrec-conf.org/proceedings/lrec2014/pdf/1062_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2014/pdf/1062_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2014/pdf/1062_Paper.pdf
http://www.aclweb.org/anthology/S17-2090
http://www.aclweb.org/anthology/S17-2090
http://www.aclweb.org/anthology/S17-2090
https://arxiv.org/abs/1706.09254
https://arxiv.org/abs/1706.09254


209

Daniel Zeman, Martin Popel, Milan Straka, Jan Ha-
jic, Joakim Nivre, Filip Ginter, Juhani Luotolahti,
Sampo Pyysalo, Slav Petrov, Martin Potthast, et al.
2017. Conll 2017 shared task: multilingual parsing
from raw text to universal dependencies. Proceed-
ings of the CoNLL 2017 Shared Task: Multilingual
Parsing from Raw Text to Universal Dependencies,
pages 1–19.

Huayan Zhong and Amanda Stent. 2005. Building
surface realizers automatically from corpora using
general-purpose tools. In Proceedings of the Work-
shop on Using Corpora for Natural Language Gen-
eration (UCNLG).


