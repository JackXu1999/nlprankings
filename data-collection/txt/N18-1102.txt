



















































Filling Missing Paths: Modeling Co-occurrences of Word Pairs and Dependency Paths for Recognizing Lexical Semantic Relations


Proceedings of NAACL-HLT 2018, pages 1123–1133
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Filling Missing Paths: Modeling Co-occurrences of Word Pairs and
Dependency Paths for Recognizing Lexical Semantic Relations

Koki Washio and Tsuneaki Kato
Department of Language and Information Sciences

Graduate School of Arts and Sciences
The University of Tokyo

3-8-1, Komaba, Meguroku, Tokyo 153-8902 Japan
{kokiwashio@g.ecc, kato@boz.c}.u-tokyo.ac.jp

Abstract

Recognizing lexical semantic relations be-
tween word pairs is an important task for many
applications of natural language processing.
One of the mainstream approaches to this task
is to exploit the lexico-syntactic paths con-
necting two target words, which reflect the
semantic relations of word pairs. However,
this method requires that the considered words
co-occur in a sentence. This requirement is
hardly satisfied because of Zipf’s law, which
states that most content words occur very
rarely. In this paper, we propose novel meth-
ods with a neural model of P (path|w1, w2)
to solve this problem. Our proposed model
of P (path|w1, w2) can be learned in an un-
supervised manner and can generalize the co-
occurrences of word pairs and dependency
paths. This model can be used to augment the
path data of word pairs that do not co-occur in
the corpus, and extract features capturing re-
lational information from word pairs. Our ex-
perimental results demonstrate that our meth-
ods improve on previous neural approaches
based on dependency paths and successfully
solve the focused problem.

1 Introduction

The semantic relations between words are impor-
tant for many natural language processing tasks,
such as recognizing textual entailment (Dagan
et al., 2010) and question answering (Yang et al.,
2017). Moreover, these relations have been also
used as features for neural methods in machine
translation (Sennrich and Haddow, 2016) and re-
lation extraction (Xu et al., 2015). This type of
information is provided by manually-created se-
mantic taxonomies, such as WordNet (Fellbaum,
1998). However, these resources are expensive to
expand manually and have limited domain cov-
erage. Thus, the automatic detection of lexico-
semantic relations has been studied for several

decades.
One of the most popular approaches is based

on patterns that encode a specific kind of relation-
ship (synonym, hypernym, etc.) between adjacent
words. This type of approach is called a path-
based method. Lexico-syntactic patterns between
two words provide information on semantic rela-
tions. For example, if we see the pattern, “animals
such as a dog” in a corpus, we can infer that ani-
mal is a hypernym of dog. On the basis of this as-
sumption, Hearst (1992) detected the hypernymy
relation of two words from a corpus based on sev-
eral handcrafted lexico-syntactic patterns, e.g., X
such as Y. Snow et al. (2004) used as features in-
dicative dependency paths, in which target word
pairs co-occurred, and trained a classifier with data
to detect hypernymy relations.

In recent studies, Shwartz et al. (2016) pro-
posed a neural path-based model that encoded
dependency paths between two words into low-
dimensional dense vectors with recurrent neural
networks (RNN) for hypernymy detection. This
method can prevent sparse feature space and gen-
eralize indicative dependency paths for detect-
ing lexico-semantic relations. Their model out-
performed the previous state-of-the-art path-based
method. Moreover, they demonstrated that these
dense path representations capture complementary
information with word embeddings that contain
individual word features. This was indicated by
the experimental result that showed the combina-
tion of path representations and word embeddings
improved classification performance. In addition,
Shwartz and Dagan (2016) showed that the neural
path-based approach, combined with word embed-
dings, is effective in recognizing multiple seman-
tic relations.

Although path-based methods can capture the
relational information between two words, these
methods can obtain clues only for word pairs that

1123



co-occur in a corpus. Even with a very large
corpus, it is almost impossible to observe a co-
occurrence of arbitrary word pairs. Thus, path-
based methods are still limited in terms of the
number of word pairs that are correctly classified.

To address this problem, we propose a novel
method with modeling P (path|w1, w2) in a neu-
ral unsupervised manner, where w1 and w2 are the
two target words, and path is a dependency path
that can connect the joint co-occurrence of w1 and
w2. A neural model of P (path|w1, w2) can gen-
eralize co-occurrences of word pairs and depen-
dency paths, and infer plausible dependency paths
which connect two words that do not co-occur in
a corpus. After unsupervised learning, this model
can be used in two ways:

• Path data augmentation through predicting
dependency paths that are most likely to co-
occur with a given word pair.

• Feature extraction of word pairs, capturing
the information of dependency paths as con-
texts where two words co-occur.

While previous supervised path-based methods
used only a small portion of a corpus, combining
our models makes it possible to use an entire cor-
pus for learning process.

Experimental results for four common datasets
of multiple lexico-semantic relations show that our
methods improve the classification performance of
supervised neural path-based models.

2 Background

2.1 Supervised Lexical Semantic Relation
Detection

Supervised lexical semantic relation detection rep-
resents word pairs (w1, w2) as feature vectors v
and trains a classifier with these vectors based on
training data. For word pair representations v,
we can use the distributional information of each
word and path information in which two words co-
occur.

Several methods exploit word embeddings
(Mikolov et al., 2013; Levy and Goldberg, 2014;
Pennington et al., 2014) as distributional informa-
tion. These methods use a combination of each
word’s embeddings, such as vector concatenation
(Baroni et al., 2012; Roller and Erk, 2016) or vec-
tor difference (Roller et al., 2014; Weeds et al.,
2014; Vylomova et al., 2016), as word pair repre-
sentations. While these distributional supervised

methods do not require co-occurrences of two
words in a sentence, Levy et al. (2015) notes that
these methods do not learn the relationships be-
tween two words but rather the separate property
of each word, i.e., whether or not each word tends
to have a target relation.

In contrast, supervised path-based methods can
capture relational information between two words.
These methods represent a word pair as the set of
lexico-syntactic paths, which connect two target
words in a corpus (Snow et al., 2004). However,
these methods suffer from sparse feature space, as
they cannot capture the similarity between indica-
tive lexico-syntactic paths, e.g., X is a species of Y
and X is a kind of Y.

2.2 Neural Path-based Method
A neural path-based method can avoid the sparse
feature space of the previous path-based methods
(Shwartz et al., 2016; Shwartz and Dagan, 2016).
Instead of treating an entire dependency path as a
single feature, this model encodes a sequence of
edges of a dependency path into a dense vector
using a long short-term memory network (LSTM)
(Hochreiter and Schmidhuber, 1997).

A dependency path connecting two words
can be extracted from the dependency tree of
a sentence. For example, given the sen-
tence “A dog is a mammal,” with X = dog
and Y = mammal, the dependency path con-
necting the two words is X/NOUN/nsubj/>
be/VERB/ROOT/- Y/NOUN/attr/<. Each
edge of a dependency path is composed of a
lemma, part of speech (POS), dependency label,
and dependency direction.

Shwartz et al. (2016) represents each edge as the
concatenation of its component embeddings:

e = [vl;vpos;vdep;vdir] (1)

where vl,vpos,vdep,and vdir represent the embed-
ding vectors of the lemma, POS, dependency la-
bel, and dependency direction respectively. This
edge vector e is an input of the LSTM at each time
step. Here, ht, the hidden state at time step t, is
abstractly computed as:

ht = LSTM(ht−1, et) (2)

where LSTM computes the current hidden state
given the previous hidden state ht−1 and the cur-
rent input edge vector et along with the LSTM
architecture. The final hidden state vector op is

1124



treated as the representation of the dependency
path p.

When classifying a word pair (w1, w2), the
word pair is represented as the average of the de-
pendency path vectors that connect two words in a
corpus:

v(w1,w2) = vpaths(w1,w2)

=

∑
p∈paths(w1,w2) fp,(w1,w2) · op∑

p∈paths(w1,w2) fp,(w1,w2)
(3)

where paths(w1, w2) is the set of dependency
paths that connects w1 and w2 in the corpus, and
fp,(w1,w2) is the frequency of p in paths(w1, w2).
The final output of the network is calculated as fol-
lows:

y = softmax(Wv(w1,w2) + b) (4)

where W ∈ R|c|×d is a linear transformation ma-
trix, b ∈ R|c| is a bias parameter, |c| is the number
of the output class, and d is the size of v(w1,w2).

This neural path-based model can be combined
with distributional methods. Shwartz et al. (2016)
concatenated vpaths(w1,w2) to the word embed-
dings of w1 and w2, redefining v(w1,w2) as:

v(w1,w2) = [vw1 ;vpaths(w1,w2);vw2 ] (5)

where vw1 and vw2 are word embeddings of w1
and w2, respectively. This integrated model,
named LexNET, exploits both path information
and distributional information, and has high gener-
alization performance for lexical semantic relation
detection.

2.3 Missing Path Problem
All path-based methods, including the neural ones,
suffer from data sparseness as they depend on
word pair co-occurrences in a corpus. However,
we cannot observe all co-occurrences of semanti-
cally related words even with a very large corpus
because of Zipf’s law, which states that the fre-
quency distribution of words has a long tail; in
other words, most words occur very infrequently
(Hanks, 2009). In this paper, we refer to this phe-
nomenon as the missing path problem.

This missing path problem leads to the fact that
path-based models cannot find any clues for two
words that do not co-occur. Thus, in the neural
path-based method, paths(w1, w2) for these
word pairs is padded with an empty path, like
UNK-lemma/UNK-POS/UNK-dep/UNK-dir.

However, this process makes path-based classi-
fiers unable to distinguish between semantically-
related pairs with no co-occurrences and those
that have no semantic relation.

In an attempt to solve this problem, Necsulescu
et al. (2015) proposed a method that used a graph
representation of a corpus. In this graph, words
and dependency relations were denoted as nodes
and labeled directed edges, respectively. From
this graph representation, paths linking two target
words can be extracted through bridging words,
even if the two words do not co-occur in the cor-
pus. They represent word pairs as the sets of paths
linking word pairs on the graph and train a sup-
port vector machine classifier with training data,
thereby improving recall. However, the authors
reported that this method still suffered from data
sparseness.

In this paper, we address this missing path prob-
lem, which generally restricts path-based meth-
ods, by neural modeling P (path|w1, w2).

3 Method

We present a novel method for modeling
P (path|w1, w2). The purpose of this method is
to address the missing path problem by general-
izing the co-occurrences of word pairs and de-
pendency paths. To model P (path|w1, w2), we
used the context-prediction approach (Collobert
and Weston, 2008; Mikolov et al., 2013; Levy and
Goldberg, 2014; Pennington et al., 2014), which
is a widely used method for learning word em-
beddings. In our proposed method, word pairs
and dependency paths are represented as embed-
dings that are updated with unsupervised learning
through predicting path from w1 and w2 (Section
3.1).

After the learning process, our model can be
used to (1) augment path data by predicting the
plausibility of the co-occurrence of two words and
a dependency path (Section 3.2); and to (2) ex-
tract useful features from word pairs, which reflect
the information of co-occurring dependency paths
(Section 3.3).

3.1 Unsupervised Learning

There are many possible ways to model
P (path|w1, w2). In this paper, we present a
straightforward and efficient architecture, similar
to the skip-gram with negative sampling (Mikolov
et al., 2013).

1125



vdog
<latexit sha1_base64="/GtrnDUq0q8bmr+YIEkGBCoFU6E=">AAACj3ichVG7SgNBFD2urxhfiTaCjRgUq3BXBMVCgjbaJdGoYCTsrmNcsi92N4G45Af8AC0sfICF+AF+gI0/YOEniKWCjYV3Nwuiot5hZs6cuefOHK7qGLrnEz12SJ1d3T29ib5k/8Dg0HAqPbLp2XVXEyXNNmx3W1U8YeiWKPm6b4htxxWKqRpiS62thPdbDeF6um1t+E1H7JpK1dL3dU3xmSqXVTNotCrBnl1tVVIZylIUEz+BHIMM4sjbqVuUsQcbGuowIWDBZ2xAgcdjBzIIDnO7CJhzGenRvUALSdbWOUtwhsJsjdcqn3Zi1uJzWNOL1Bq/YvB0WTmBKXqga3qhe7qhJ3r/tVYQ1Qj/0uRdbWuFUxk+Glt/+1dl8u7j4FP1h0Ll7L89+djHQuRFZ29OxIQutXb9xuHJy/picSqYpkt6Zn8X9Eh37NBqvGpXBVE8RZIbJH9vx0+wOZuVKSsX5jK55bhVCYxjEjPcj3nksIo8Svyug2Oc4VxKS/PSkpRrp0odsWYUX0Ja+wB3cZUP</latexit><latexit sha1_base64="/GtrnDUq0q8bmr+YIEkGBCoFU6E=">AAACj3ichVG7SgNBFD2urxhfiTaCjRgUq3BXBMVCgjbaJdGoYCTsrmNcsi92N4G45Af8AC0sfICF+AF+gI0/YOEniKWCjYV3Nwuiot5hZs6cuefOHK7qGLrnEz12SJ1d3T29ib5k/8Dg0HAqPbLp2XVXEyXNNmx3W1U8YeiWKPm6b4htxxWKqRpiS62thPdbDeF6um1t+E1H7JpK1dL3dU3xmSqXVTNotCrBnl1tVVIZylIUEz+BHIMM4sjbqVuUsQcbGuowIWDBZ2xAgcdjBzIIDnO7CJhzGenRvUALSdbWOUtwhsJsjdcqn3Zi1uJzWNOL1Bq/YvB0WTmBKXqga3qhe7qhJ3r/tVYQ1Qj/0uRdbWuFUxk+Glt/+1dl8u7j4FP1h0Ll7L89+djHQuRFZ29OxIQutXb9xuHJy/picSqYpkt6Zn8X9Eh37NBqvGpXBVE8RZIbJH9vx0+wOZuVKSsX5jK55bhVCYxjEjPcj3nksIo8Svyug2Oc4VxKS/PSkpRrp0odsWYUX0Ja+wB3cZUP</latexit><latexit sha1_base64="/GtrnDUq0q8bmr+YIEkGBCoFU6E=">AAACj3ichVG7SgNBFD2urxhfiTaCjRgUq3BXBMVCgjbaJdGoYCTsrmNcsi92N4G45Af8AC0sfICF+AF+gI0/YOEniKWCjYV3Nwuiot5hZs6cuefOHK7qGLrnEz12SJ1d3T29ib5k/8Dg0HAqPbLp2XVXEyXNNmx3W1U8YeiWKPm6b4htxxWKqRpiS62thPdbDeF6um1t+E1H7JpK1dL3dU3xmSqXVTNotCrBnl1tVVIZylIUEz+BHIMM4sjbqVuUsQcbGuowIWDBZ2xAgcdjBzIIDnO7CJhzGenRvUALSdbWOUtwhsJsjdcqn3Zi1uJzWNOL1Bq/YvB0WTmBKXqga3qhe7qhJ3r/tVYQ1Qj/0uRdbWuFUxk+Glt/+1dl8u7j4FP1h0Ll7L89+djHQuRFZ29OxIQutXb9xuHJy/picSqYpkt6Zn8X9Eh37NBqvGpXBVE8RZIbJH9vx0+wOZuVKSsX5jK55bhVCYxjEjPcj3nksIo8Svyug2Oc4VxKS/PSkpRrp0odsWYUX0Ja+wB3cZUP</latexit><latexit sha1_base64="dO1RVWgbv0QBXFnWGIM/DJuxBX8=">AAACdXichVG9TsJQGD1UVEQU3ExciARjHMhXHTROJi6O/MhPgoS05YINpW3aQoLEF2B1cHDSxMH4AD6Aiy/gwCMYR0xcHPwoJEaJ+jXtPffc73y3J0e1Dd31iAYBaSY4OzcfWggvRsJLy9FYpOBabUcTec0yLKekKq4wdFPkPd0zRMl2hNJSDVFUm4ej82JHOK5umcde1xaVltIw9bquKR5T6WosQSnyKz4N5AlIYFJW7AEnqMGChjZaEDDhMTagwOWnDBkEm7kKesw5jHT/XOAcYda2uUtwh8Jsk78N3pUnrMn70UzXV2t8i8Gvw8o4kvRMdzSkJ7qnF/r4dVbPnzH6ly6v6lgr7Gq0v5p7/1fV4tXD6ZfqD4XK3X978lDHnu9FZ2+2z4xcauP5nbPLYW4/m+xt0A29sr9rGtAjOzQ7b9ptRmSvEOZ85J9pTIPCdkqmlJwhhLCGdWxyDLs4wBHSyPN1NfRxIQWlLUke5ygFJoGu4FtJO59jaI2O</latexit><latexit sha1_base64="xxEV17RmVyzaFogwgjTH7txQCH0=">AAAChHichVHLSsNQFJzGd61aXQluikVxVU7cVFyI4Malr1qhlZLEaw3NiyQNaOgP+AG6cOEDXIgf4Ae48Qdc9BPEpYIbF56kAdFiPSG5c+eeOTfDqI6hez5ROyX19Q8MDg2PpEczY+MT2cnMrmc3XU2UNNuw3T1V8YShW6Lk674h9hxXKKZqiLLaWIvOy4FwPd22dvxjR+ybSt3SD3VN8ZmqVlUzDFq18MCut2rZPBUorlw3kBOQR1IbdvYBVRzAhoYmTAhY8BkbUODxU4EMgsPcPkLmXEZ6fC7QQpq1Te4S3KEw2+BvnXeVhLV4H830YrXGtxj8uqzMYY6e6Y7e6Inu6YU+/5wVxjOifznmVe1ohVObOJ3e/vhXZfLq4+hb1UOhcndvTz4OsRR70dmbEzORS60zPzg5f9te3poL5+mGXtnfNbXpkR1awbt2uym2LpDmgOTfcXSD3cWCTAV5kzCMGcxigWMoYhXr2ECJr3NwhktcSZNSUVrpRCmlkkyn8KOk1S8CG5QU</latexit><latexit sha1_base64="xxEV17RmVyzaFogwgjTH7txQCH0=">AAAChHichVHLSsNQFJzGd61aXQluikVxVU7cVFyI4Malr1qhlZLEaw3NiyQNaOgP+AG6cOEDXIgf4Ae48Qdc9BPEpYIbF56kAdFiPSG5c+eeOTfDqI6hez5ROyX19Q8MDg2PpEczY+MT2cnMrmc3XU2UNNuw3T1V8YShW6Lk674h9hxXKKZqiLLaWIvOy4FwPd22dvxjR+ybSt3SD3VN8ZmqVlUzDFq18MCut2rZPBUorlw3kBOQR1IbdvYBVRzAhoYmTAhY8BkbUODxU4EMgsPcPkLmXEZ6fC7QQpq1Te4S3KEw2+BvnXeVhLV4H830YrXGtxj8uqzMYY6e6Y7e6Inu6YU+/5wVxjOifznmVe1ohVObOJ3e/vhXZfLq4+hb1UOhcndvTz4OsRR70dmbEzORS60zPzg5f9te3poL5+mGXtnfNbXpkR1awbt2uym2LpDmgOTfcXSD3cWCTAV5kzCMGcxigWMoYhXr2ECJr3NwhktcSZNSUVrpRCmlkkyn8KOk1S8CG5QU</latexit><latexit sha1_base64="HAqT+kC+QVvSO8QXuMErxoZk7NE=">AAACj3ichVG7SgNBFD1ZXzG+Em0EGzEoVuGuTcRCgjbaRWMSwUjYXce4uC92N4G45Af8AC0sfICF+AF+gI0/YOEniGUEGwtvNguiot5hZs6cuefOHK7qGLrnEz3FpJ7evv6B+GBiaHhkdCyZGi95dt3VRFGzDdvdVhVPGLolir7uG2LbcYViqoYoq4ernftyQ7iebltbftMRu6ZSs/R9XVN8pioV1QwarWqwZ9da1WSaMhTG9E8gRyCNKPJ28g4V7MGGhjpMCFjwGRtQ4PHYgQyCw9wuAuZcRnp4L9BCgrV1zhKcoTB7yGuNTzsRa/G5U9ML1Rq/YvB0WTmNWXqkG2rTA93SM73/WisIa3T+0uRd7WqFUx07niy8/asyefdx8Kn6Q6Fy9t+efOxjMfSiszcnZDoutW79xtFpu7C0ORvM0RW9sL9LeqJ7dmg1XrXrDbF5hgQ3SP7ejp+gtJCRKSNvUDq3ErUqjinMYJ77kUUOa8ijyO86OME5LqSUlJWWpVw3VYpFmgl8CWn9A3YxlQs=</latexit><latexit sha1_base64="/GtrnDUq0q8bmr+YIEkGBCoFU6E=">AAACj3ichVG7SgNBFD2urxhfiTaCjRgUq3BXBMVCgjbaJdGoYCTsrmNcsi92N4G45Af8AC0sfICF+AF+gI0/YOEniKWCjYV3Nwuiot5hZs6cuefOHK7qGLrnEz12SJ1d3T29ib5k/8Dg0HAqPbLp2XVXEyXNNmx3W1U8YeiWKPm6b4htxxWKqRpiS62thPdbDeF6um1t+E1H7JpK1dL3dU3xmSqXVTNotCrBnl1tVVIZylIUEz+BHIMM4sjbqVuUsQcbGuowIWDBZ2xAgcdjBzIIDnO7CJhzGenRvUALSdbWOUtwhsJsjdcqn3Zi1uJzWNOL1Bq/YvB0WTmBKXqga3qhe7qhJ3r/tVYQ1Qj/0uRdbWuFUxk+Glt/+1dl8u7j4FP1h0Ll7L89+djHQuRFZ29OxIQutXb9xuHJy/picSqYpkt6Zn8X9Eh37NBqvGpXBVE8RZIbJH9vx0+wOZuVKSsX5jK55bhVCYxjEjPcj3nksIo8Svyug2Oc4VxKS/PSkpRrp0odsWYUX0Ja+wB3cZUP</latexit><latexit sha1_base64="/GtrnDUq0q8bmr+YIEkGBCoFU6E=">AAACj3ichVG7SgNBFD2urxhfiTaCjRgUq3BXBMVCgjbaJdGoYCTsrmNcsi92N4G45Af8AC0sfICF+AF+gI0/YOEniKWCjYV3Nwuiot5hZs6cuefOHK7qGLrnEz12SJ1d3T29ib5k/8Dg0HAqPbLp2XVXEyXNNmx3W1U8YeiWKPm6b4htxxWKqRpiS62thPdbDeF6um1t+E1H7JpK1dL3dU3xmSqXVTNotCrBnl1tVVIZylIUEz+BHIMM4sjbqVuUsQcbGuowIWDBZ2xAgcdjBzIIDnO7CJhzGenRvUALSdbWOUtwhsJsjdcqn3Zi1uJzWNOL1Bq/YvB0WTmBKXqga3qhe7qhJ3r/tVYQ1Qj/0uRdbWuFUxk+Glt/+1dl8u7j4FP1h0Ll7L89+djHQuRFZ29OxIQutXb9xuHJy/picSqYpkt6Zn8X9Eh37NBqvGpXBVE8RZIbJH9vx0+wOZuVKSsX5jK55bhVCYxjEjPcj3nksIo8Svyug2Oc4VxKS/PSkpRrp0odsWYUX0Ja+wB3cZUP</latexit><latexit sha1_base64="/GtrnDUq0q8bmr+YIEkGBCoFU6E=">AAACj3ichVG7SgNBFD2urxhfiTaCjRgUq3BXBMVCgjbaJdGoYCTsrmNcsi92N4G45Af8AC0sfICF+AF+gI0/YOEniKWCjYV3Nwuiot5hZs6cuefOHK7qGLrnEz12SJ1d3T29ib5k/8Dg0HAqPbLp2XVXEyXNNmx3W1U8YeiWKPm6b4htxxWKqRpiS62thPdbDeF6um1t+E1H7JpK1dL3dU3xmSqXVTNotCrBnl1tVVIZylIUEz+BHIMM4sjbqVuUsQcbGuowIWDBZ2xAgcdjBzIIDnO7CJhzGenRvUALSdbWOUtwhsJsjdcqn3Zi1uJzWNOL1Bq/YvB0WTmBKXqga3qhe7qhJ3r/tVYQ1Qj/0uRdbWuFUxk+Glt/+1dl8u7j4FP1h0Ll7L89+djHQuRFZ29OxIQutXb9xuHJy/picSqYpkt6Zn8X9Eh37NBqvGpXBVE8RZIbJH9vx0+wOZuVKSsX5jK55bhVCYxjEjPcj3nksIo8Svyug2Oc4VxKS/PSkpRrp0odsWYUX0Ja+wB3cZUP</latexit><latexit sha1_base64="/GtrnDUq0q8bmr+YIEkGBCoFU6E=">AAACj3ichVG7SgNBFD2urxhfiTaCjRgUq3BXBMVCgjbaJdGoYCTsrmNcsi92N4G45Af8AC0sfICF+AF+gI0/YOEniKWCjYV3Nwuiot5hZs6cuefOHK7qGLrnEz12SJ1d3T29ib5k/8Dg0HAqPbLp2XVXEyXNNmx3W1U8YeiWKPm6b4htxxWKqRpiS62thPdbDeF6um1t+E1H7JpK1dL3dU3xmSqXVTNotCrBnl1tVVIZylIUEz+BHIMM4sjbqVuUsQcbGuowIWDBZ2xAgcdjBzIIDnO7CJhzGenRvUALSdbWOUtwhsJsjdcqn3Zi1uJzWNOL1Bq/YvB0WTmBKXqga3qhe7qhJ3r/tVYQ1Qj/0uRdbWuFUxk+Glt/+1dl8u7j4FP1h0Ll7L89+djHQuRFZ29OxIQutXb9xuHJy/picSqYpkt6Zn8X9Eh37NBqvGpXBVE8RZIbJH9vx0+wOZuVKSsX5jK55bhVCYxjEjPcj3nksIo8Svyug2Oc4VxKS/PSkpRrp0odsWYUX0Ja+wB3cZUP</latexit><latexit sha1_base64="/GtrnDUq0q8bmr+YIEkGBCoFU6E=">AAACj3ichVG7SgNBFD2urxhfiTaCjRgUq3BXBMVCgjbaJdGoYCTsrmNcsi92N4G45Af8AC0sfICF+AF+gI0/YOEniKWCjYV3Nwuiot5hZs6cuefOHK7qGLrnEz12SJ1d3T29ib5k/8Dg0HAqPbLp2XVXEyXNNmx3W1U8YeiWKPm6b4htxxWKqRpiS62thPdbDeF6um1t+E1H7JpK1dL3dU3xmSqXVTNotCrBnl1tVVIZylIUEz+BHIMM4sjbqVuUsQcbGuowIWDBZ2xAgcdjBzIIDnO7CJhzGenRvUALSdbWOUtwhsJsjdcqn3Zi1uJzWNOL1Bq/YvB0WTmBKXqga3qhe7qhJ3r/tVYQ1Qj/0uRdbWuFUxk+Glt/+1dl8u7j4FP1h0Ll7L89+djHQuRFZ29OxIQutXb9xuHJy/picSqYpkt6Zn8X9Eh37NBqvGpXBVE8RZIbJH9vx0+wOZuVKSsX5jK55bhVCYxjEjPcj3nksIo8Svyug2Oc4VxKS/PSkpRrp0odsWYUX0Ja+wB3cZUP</latexit><latexit sha1_base64="/GtrnDUq0q8bmr+YIEkGBCoFU6E=">AAACj3ichVG7SgNBFD2urxhfiTaCjRgUq3BXBMVCgjbaJdGoYCTsrmNcsi92N4G45Af8AC0sfICF+AF+gI0/YOEniKWCjYV3Nwuiot5hZs6cuefOHK7qGLrnEz12SJ1d3T29ib5k/8Dg0HAqPbLp2XVXEyXNNmx3W1U8YeiWKPm6b4htxxWKqRpiS62thPdbDeF6um1t+E1H7JpK1dL3dU3xmSqXVTNotCrBnl1tVVIZylIUEz+BHIMM4sjbqVuUsQcbGuowIWDBZ2xAgcdjBzIIDnO7CJhzGenRvUALSdbWOUtwhsJsjdcqn3Zi1uJzWNOL1Bq/YvB0WTmBKXqga3qhe7qhJ3r/tVYQ1Qj/0uRdbWuFUxk+Glt/+1dl8u7j4FP1h0Ll7L89+djHQuRFZ29OxIQutXb9xuHJy/picSqYpkt6Zn8X9Eh37NBqvGpXBVE8RZIbJH9vx0+wOZuVKSsX5jK55bhVCYxjEjPcj3nksIo8Svyug2Oc4VxKS/PSkpRrp0odsWYUX0Ja+wB3cZUP</latexit>

vanimal
<latexit sha1_base64="4DyNGEq/o+FynCeenDrXR0Wk++E=">AAAClHichVHLSsNAFD2Nr1ofrQoiuBGL4qrciKCIi2IRXIlVq4JKSeJYB/MiSQsa+gP+gIuuFF2IH+AHuPEHXPgJ4lLBjQtv04CoqDdk5syZe25yOLprSj8gekwobe0dnV3J7lRPb19/OjMwuOk7Vc8QJcMxHW9b13xhSluUAhmYYtv1hGbpptjSjwrN+62a8Hzp2BvBsSv2LK1iywNpaAFT5Ux6V7fCWr0cara0NLNezmQpR1GN/QRqDLKIa9XJ3GIX+3BgoAoLAjYCxiY0+PzsQAXBZW4PIXMeIxndC9SRYm2VuwR3aMwe8Vrh007M2nxuzvQjtcFfMfn1WDmGCXqga3qhe7qhJ3r/dVYYzWj+yzHveksr3HL6dGT97V+VxXuAw0/VHwqdu//2FOAAc5EXyd7ciGm6NFrzaydnL+vzaxPhJF3QM/s7p0e6Y4d27dW4Koq1BlIckPo9jp9gczqnUk4tzmTzi3FUSYxiHFOcxyzyWMYqSlFmDVziShlWFpSCstRqVRKxZghfSln5ABp7lpY=</latexit><latexit sha1_base64="4DyNGEq/o+FynCeenDrXR0Wk++E=">AAAClHichVHLSsNAFD2Nr1ofrQoiuBGL4qrciKCIi2IRXIlVq4JKSeJYB/MiSQsa+gP+gIuuFF2IH+AHuPEHXPgJ4lLBjQtv04CoqDdk5syZe25yOLprSj8gekwobe0dnV3J7lRPb19/OjMwuOk7Vc8QJcMxHW9b13xhSluUAhmYYtv1hGbpptjSjwrN+62a8Hzp2BvBsSv2LK1iywNpaAFT5Ux6V7fCWr0cara0NLNezmQpR1GN/QRqDLKIa9XJ3GIX+3BgoAoLAjYCxiY0+PzsQAXBZW4PIXMeIxndC9SRYm2VuwR3aMwe8Vrh007M2nxuzvQjtcFfMfn1WDmGCXqga3qhe7qhJ3r/dVYYzWj+yzHveksr3HL6dGT97V+VxXuAw0/VHwqdu//2FOAAc5EXyd7ciGm6NFrzaydnL+vzaxPhJF3QM/s7p0e6Y4d27dW4Koq1BlIckPo9jp9gczqnUk4tzmTzi3FUSYxiHFOcxyzyWMYqSlFmDVziShlWFpSCstRqVRKxZghfSln5ABp7lpY=</latexit><latexit sha1_base64="4DyNGEq/o+FynCeenDrXR0Wk++E=">AAAClHichVHLSsNAFD2Nr1ofrQoiuBGL4qrciKCIi2IRXIlVq4JKSeJYB/MiSQsa+gP+gIuuFF2IH+AHuPEHXPgJ4lLBjQtv04CoqDdk5syZe25yOLprSj8gekwobe0dnV3J7lRPb19/OjMwuOk7Vc8QJcMxHW9b13xhSluUAhmYYtv1hGbpptjSjwrN+62a8Hzp2BvBsSv2LK1iywNpaAFT5Ux6V7fCWr0cara0NLNezmQpR1GN/QRqDLKIa9XJ3GIX+3BgoAoLAjYCxiY0+PzsQAXBZW4PIXMeIxndC9SRYm2VuwR3aMwe8Vrh007M2nxuzvQjtcFfMfn1WDmGCXqga3qhe7qhJ3r/dVYYzWj+yzHveksr3HL6dGT97V+VxXuAw0/VHwqdu//2FOAAc5EXyd7ciGm6NFrzaydnL+vzaxPhJF3QM/s7p0e6Y4d27dW4Koq1BlIckPo9jp9gczqnUk4tzmTzi3FUSYxiHFOcxyzyWMYqSlFmDVziShlWFpSCstRqVRKxZghfSln5ABp7lpY=</latexit><latexit sha1_base64="dO1RVWgbv0QBXFnWGIM/DJuxBX8=">AAACdXichVG9TsJQGD1UVEQU3ExciARjHMhXHTROJi6O/MhPgoS05YINpW3aQoLEF2B1cHDSxMH4AD6Aiy/gwCMYR0xcHPwoJEaJ+jXtPffc73y3J0e1Dd31iAYBaSY4OzcfWggvRsJLy9FYpOBabUcTec0yLKekKq4wdFPkPd0zRMl2hNJSDVFUm4ej82JHOK5umcde1xaVltIw9bquKR5T6WosQSnyKz4N5AlIYFJW7AEnqMGChjZaEDDhMTagwOWnDBkEm7kKesw5jHT/XOAcYda2uUtwh8Jsk78N3pUnrMn70UzXV2t8i8Gvw8o4kvRMdzSkJ7qnF/r4dVbPnzH6ly6v6lgr7Gq0v5p7/1fV4tXD6ZfqD4XK3X978lDHnu9FZ2+2z4xcauP5nbPLYW4/m+xt0A29sr9rGtAjOzQ7b9ptRmSvEOZ85J9pTIPCdkqmlJwhhLCGdWxyDLs4wBHSyPN1NfRxIQWlLUke5ygFJoGu4FtJO59jaI2O</latexit><latexit sha1_base64="yOxYQQOKVWEVXEcXapLNohbi8IE=">AAACiXichVHLSsNAFD2N71ptdSGCm2KpuCo3bhRxIYjgsq32AVpKEkcdzIskLWjoD/gDLlwpuhA/wA9w4w+48BPEZQU3LrxNC6JFvSEzZ87cc5PD0V1T+gHRc0wZGBwaHhkdi48nJiaTqalE2XcaniFKhmM6XlXXfGFKW5QCGZii6npCs3RTVPTjjc59pSk8Xzr2TnDiipqlHdryQBpawFQ9ldzTrbDZqoeaLS3NbNVTGcpRVOl+oPZABr3KO6l77GEfDgw0YEHARsDYhAafn12oILjM1RAy5zGS0b1AC3HWNrhLcIfG7DGvh3za7bE2nzsz/Uht8FdMfj1WppGlJ7qlNj3SHb3Qx6+zwmhG519OeNe7WuHWk2ez2+//qizeAxx9qf5Q6Nz9t6cAB1iJvEj25kZMx6XRnd88PW9vrxaz4QJd0Sv7u6RnemCHdvPNuCmI4gXiHJD6M45+UF7KqZRTC4RRzGEeixzDMtaxhTxKUVQXuMaNMqOsKRvdKJVYL9NpfCtl8xOVL5WW</latexit><latexit sha1_base64="yOxYQQOKVWEVXEcXapLNohbi8IE=">AAACiXichVHLSsNAFD2N71ptdSGCm2KpuCo3bhRxIYjgsq32AVpKEkcdzIskLWjoD/gDLlwpuhA/wA9w4w+48BPEZQU3LrxNC6JFvSEzZ87cc5PD0V1T+gHRc0wZGBwaHhkdi48nJiaTqalE2XcaniFKhmM6XlXXfGFKW5QCGZii6npCs3RTVPTjjc59pSk8Xzr2TnDiipqlHdryQBpawFQ9ldzTrbDZqoeaLS3NbNVTGcpRVOl+oPZABr3KO6l77GEfDgw0YEHARsDYhAafn12oILjM1RAy5zGS0b1AC3HWNrhLcIfG7DGvh3za7bE2nzsz/Uht8FdMfj1WppGlJ7qlNj3SHb3Qx6+zwmhG519OeNe7WuHWk2ez2+//qizeAxx9qf5Q6Nz9t6cAB1iJvEj25kZMx6XRnd88PW9vrxaz4QJd0Sv7u6RnemCHdvPNuCmI4gXiHJD6M45+UF7KqZRTC4RRzGEeixzDMtaxhTxKUVQXuMaNMqOsKRvdKJVYL9NpfCtl8xOVL5WW</latexit><latexit sha1_base64="F4S4ed4zgFx83YvhnJ7r1gBov7o=">AAAClHichVHLSsNAFD2N7/poVRDBTbEorsqNG0VciCK4EmttFVRKEkcdmhdJWqihP+APuHCl6EL8AD/AjT/gwk8QlxXcuPA2DYiKekNmzpy55yaHo7um9AOip4TS0dnV3dPbl+wfGBxKpYdHSr5T9QxRNBzT8XZ0zRemtEUxkIEpdlxPaJZuim29stK6364Jz5eOvRXUXbFvaUe2PJSGFjBVTqf2dCusNcqhZktLMxvldJZyFFXmJ1BjkEVcG076Dns4gAMDVVgQsBEwNqHB52cXKgguc/sImfMYyeheoIEka6vcJbhDY7bC6xGfdmPW5nNrph+pDf6Kya/Hygym6JFuqEkPdEvP9P7rrDCa0fqXOu96Wyvccup0vPD2r8riPcDxp+oPhc7df3sKcIj5yItkb27EtFwa7fm1k7NmYWFzKpymS3phfxf0RPfs0K69Gtd5sXmOJAekfo/jJyjN5lTKqXnKLi3HUfViApOY4TzmsIQ1bKAYZXaOK1wrY8qisqKstluVRKwZxZdS1j8AGTuWkg==</latexit><latexit sha1_base64="4DyNGEq/o+FynCeenDrXR0Wk++E=">AAAClHichVHLSsNAFD2Nr1ofrQoiuBGL4qrciKCIi2IRXIlVq4JKSeJYB/MiSQsa+gP+gIuuFF2IH+AHuPEHXPgJ4lLBjQtv04CoqDdk5syZe25yOLprSj8gekwobe0dnV3J7lRPb19/OjMwuOk7Vc8QJcMxHW9b13xhSluUAhmYYtv1hGbpptjSjwrN+62a8Hzp2BvBsSv2LK1iywNpaAFT5Ux6V7fCWr0cara0NLNezmQpR1GN/QRqDLKIa9XJ3GIX+3BgoAoLAjYCxiY0+PzsQAXBZW4PIXMeIxndC9SRYm2VuwR3aMwe8Vrh007M2nxuzvQjtcFfMfn1WDmGCXqga3qhe7qhJ3r/dVYYzWj+yzHveksr3HL6dGT97V+VxXuAw0/VHwqdu//2FOAAc5EXyd7ciGm6NFrzaydnL+vzaxPhJF3QM/s7p0e6Y4d27dW4Koq1BlIckPo9jp9gczqnUk4tzmTzi3FUSYxiHFOcxyzyWMYqSlFmDVziShlWFpSCstRqVRKxZghfSln5ABp7lpY=</latexit><latexit sha1_base64="4DyNGEq/o+FynCeenDrXR0Wk++E=">AAAClHichVHLSsNAFD2Nr1ofrQoiuBGL4qrciKCIi2IRXIlVq4JKSeJYB/MiSQsa+gP+gIuuFF2IH+AHuPEHXPgJ4lLBjQtv04CoqDdk5syZe25yOLprSj8gekwobe0dnV3J7lRPb19/OjMwuOk7Vc8QJcMxHW9b13xhSluUAhmYYtv1hGbpptjSjwrN+62a8Hzp2BvBsSv2LK1iywNpaAFT5Ux6V7fCWr0cara0NLNezmQpR1GN/QRqDLKIa9XJ3GIX+3BgoAoLAjYCxiY0+PzsQAXBZW4PIXMeIxndC9SRYm2VuwR3aMwe8Vrh007M2nxuzvQjtcFfMfn1WDmGCXqga3qhe7qhJ3r/dVYYzWj+yzHveksr3HL6dGT97V+VxXuAw0/VHwqdu//2FOAAc5EXyd7ciGm6NFrzaydnL+vzaxPhJF3QM/s7p0e6Y4d27dW4Koq1BlIckPo9jp9gczqnUk4tzmTzi3FUSYxiHFOcxyzyWMYqSlFmDVziShlWFpSCstRqVRKxZghfSln5ABp7lpY=</latexit><latexit sha1_base64="4DyNGEq/o+FynCeenDrXR0Wk++E=">AAAClHichVHLSsNAFD2Nr1ofrQoiuBGL4qrciKCIi2IRXIlVq4JKSeJYB/MiSQsa+gP+gIuuFF2IH+AHuPEHXPgJ4lLBjQtv04CoqDdk5syZe25yOLprSj8gekwobe0dnV3J7lRPb19/OjMwuOk7Vc8QJcMxHW9b13xhSluUAhmYYtv1hGbpptjSjwrN+62a8Hzp2BvBsSv2LK1iywNpaAFT5Ux6V7fCWr0cara0NLNezmQpR1GN/QRqDLKIa9XJ3GIX+3BgoAoLAjYCxiY0+PzsQAXBZW4PIXMeIxndC9SRYm2VuwR3aMwe8Vrh007M2nxuzvQjtcFfMfn1WDmGCXqga3qhe7qhJ3r/dVYYzWj+yzHveksr3HL6dGT97V+VxXuAw0/VHwqdu//2FOAAc5EXyd7ciGm6NFrzaydnL+vzaxPhJF3QM/s7p0e6Y4d27dW4Koq1BlIckPo9jp9gczqnUk4tzmTzi3FUSYxiHFOcxyzyWMYqSlFmDVziShlWFpSCstRqVRKxZghfSln5ABp7lpY=</latexit><latexit sha1_base64="4DyNGEq/o+FynCeenDrXR0Wk++E=">AAAClHichVHLSsNAFD2Nr1ofrQoiuBGL4qrciKCIi2IRXIlVq4JKSeJYB/MiSQsa+gP+gIuuFF2IH+AHuPEHXPgJ4lLBjQtv04CoqDdk5syZe25yOLprSj8gekwobe0dnV3J7lRPb19/OjMwuOk7Vc8QJcMxHW9b13xhSluUAhmYYtv1hGbpptjSjwrN+62a8Hzp2BvBsSv2LK1iywNpaAFT5Ux6V7fCWr0cara0NLNezmQpR1GN/QRqDLKIa9XJ3GIX+3BgoAoLAjYCxiY0+PzsQAXBZW4PIXMeIxndC9SRYm2VuwR3aMwe8Vrh007M2nxuzvQjtcFfMfn1WDmGCXqga3qhe7qhJ3r/dVYYzWj+yzHveksr3HL6dGT97V+VxXuAw0/VHwqdu//2FOAAc5EXyd7ciGm6NFrzaydnL+vzaxPhJF3QM/s7p0e6Y4d27dW4Koq1BlIckPo9jp9gczqnUk4tzmTzi3FUSYxiHFOcxyzyWMYqSlFmDVziShlWFpSCstRqVRKxZghfSln5ABp7lpY=</latexit><latexit sha1_base64="4DyNGEq/o+FynCeenDrXR0Wk++E=">AAAClHichVHLSsNAFD2Nr1ofrQoiuBGL4qrciKCIi2IRXIlVq4JKSeJYB/MiSQsa+gP+gIuuFF2IH+AHuPEHXPgJ4lLBjQtv04CoqDdk5syZe25yOLprSj8gekwobe0dnV3J7lRPb19/OjMwuOk7Vc8QJcMxHW9b13xhSluUAhmYYtv1hGbpptjSjwrN+62a8Hzp2BvBsSv2LK1iywNpaAFT5Ux6V7fCWr0cara0NLNezmQpR1GN/QRqDLKIa9XJ3GIX+3BgoAoLAjYCxiY0+PzsQAXBZW4PIXMeIxndC9SRYm2VuwR3aMwe8Vrh007M2nxuzvQjtcFfMfn1WDmGCXqga3qhe7qhJ3r/dVYYzWj+yzHveksr3HL6dGT97V+VxXuAw0/VHwqdu//2FOAAc5EXyd7ciGm6NFrzaydnL+vzaxPhJF3QM/s7p0e6Y4d27dW4Koq1BlIckPo9jp9gczqnUk4tzmTzi3FUSYxiHFOcxyzyWMYqSlFmDVziShlWFpSCstRqVRKxZghfSln5ABp7lpY=</latexit><latexit sha1_base64="4DyNGEq/o+FynCeenDrXR0Wk++E=">AAAClHichVHLSsNAFD2Nr1ofrQoiuBGL4qrciKCIi2IRXIlVq4JKSeJYB/MiSQsa+gP+gIuuFF2IH+AHuPEHXPgJ4lLBjQtv04CoqDdk5syZe25yOLprSj8gekwobe0dnV3J7lRPb19/OjMwuOk7Vc8QJcMxHW9b13xhSluUAhmYYtv1hGbpptjSjwrN+62a8Hzp2BvBsSv2LK1iywNpaAFT5Ux6V7fCWr0cara0NLNezmQpR1GN/QRqDLKIa9XJ3GIX+3BgoAoLAjYCxiY0+PzsQAXBZW4PIXMeIxndC9SRYm2VuwR3aMwe8Vrh007M2nxuzvQjtcFfMfn1WDmGCXqga3qhe7qhJ3r/dVYYzWj+yzHveksr3HL6dGT97V+VxXuAw0/VHwqdu//2FOAAc5EXyd7ciGm6NFrzaydnL+vzaxPhJF3QM/s7p0e6Y4d27dW4Koq1BlIckPo9jp9gczqnUk4tzmTzi3FUSYxiHFOcxyzyWMYqSlFmDVziShlWFpSCstRqVRKxZghfSln5ABp7lpY=</latexit>

h
<latexit sha1_base64="Cf7LNxzj6UzH0WNX7sTXqsmJjZk=">AAACiXichVG7SgNBFD1Z34maqI1gIwbFKtwVwZBKtLHMw8RAIrK7TnTNvtjdBDT4A1Z2olYKFuIH+AE2/oBFPkEsFWwsvNksiIrxDjNz5sw9d+ZwVcfQPZ+oHZH6+gcGh4ZHorHRsfF4YmKy5NkNVxNFzTZst6wqnjB0SxR93TdE2XGFYqqG2FLr6537raZwPd22Nv1DR2ybyp6l13RN8ZkqVVWztX+8k0hSioKY/Q3kECQRRtZO3KOKXdjQ0IAJAQs+YwMKPB4VyCA4zG2jxZzLSA/uBY4RZW2DswRnKMzWed3jUyVkLT53anqBWuNXDJ4uK2cxT090S6/0SHf0TB9/1moFNTp/OeRd7WqFsxM/mS68/6syefex/6XqoVA5u7cnHzWkAy86e3MCpuNS69ZvHp29FjL5+dYCXdML+7uiNj2wQ6v5pt3kRP4SUW6Q/LMdv0FpKSVTSs4tJ1fXwlYNYwZzWOR+rGAVG8iiyO8e4BTnuJBikiylpUw3VYqEmil8C2n9E+3ykjQ=</latexit><latexit sha1_base64="Cf7LNxzj6UzH0WNX7sTXqsmJjZk=">AAACiXichVG7SgNBFD1Z34maqI1gIwbFKtwVwZBKtLHMw8RAIrK7TnTNvtjdBDT4A1Z2olYKFuIH+AE2/oBFPkEsFWwsvNksiIrxDjNz5sw9d+ZwVcfQPZ+oHZH6+gcGh4ZHorHRsfF4YmKy5NkNVxNFzTZst6wqnjB0SxR93TdE2XGFYqqG2FLr6537raZwPd22Nv1DR2ybyp6l13RN8ZkqVVWztX+8k0hSioKY/Q3kECQRRtZO3KOKXdjQ0IAJAQs+YwMKPB4VyCA4zG2jxZzLSA/uBY4RZW2DswRnKMzWed3jUyVkLT53anqBWuNXDJ4uK2cxT090S6/0SHf0TB9/1moFNTp/OeRd7WqFsxM/mS68/6syefex/6XqoVA5u7cnHzWkAy86e3MCpuNS69ZvHp29FjL5+dYCXdML+7uiNj2wQ6v5pt3kRP4SUW6Q/LMdv0FpKSVTSs4tJ1fXwlYNYwZzWOR+rGAVG8iiyO8e4BTnuJBikiylpUw3VYqEmil8C2n9E+3ykjQ=</latexit><latexit sha1_base64="Cf7LNxzj6UzH0WNX7sTXqsmJjZk=">AAACiXichVG7SgNBFD1Z34maqI1gIwbFKtwVwZBKtLHMw8RAIrK7TnTNvtjdBDT4A1Z2olYKFuIH+AE2/oBFPkEsFWwsvNksiIrxDjNz5sw9d+ZwVcfQPZ+oHZH6+gcGh4ZHorHRsfF4YmKy5NkNVxNFzTZst6wqnjB0SxR93TdE2XGFYqqG2FLr6537raZwPd22Nv1DR2ybyp6l13RN8ZkqVVWztX+8k0hSioKY/Q3kECQRRtZO3KOKXdjQ0IAJAQs+YwMKPB4VyCA4zG2jxZzLSA/uBY4RZW2DswRnKMzWed3jUyVkLT53anqBWuNXDJ4uK2cxT090S6/0SHf0TB9/1moFNTp/OeRd7WqFsxM/mS68/6syefex/6XqoVA5u7cnHzWkAy86e3MCpuNS69ZvHp29FjL5+dYCXdML+7uiNj2wQ6v5pt3kRP4SUW6Q/LMdv0FpKSVTSs4tJ1fXwlYNYwZzWOR+rGAVG8iiyO8e4BTnuJBikiylpUw3VYqEmil8C2n9E+3ykjQ=</latexit><latexit sha1_base64="Cf7LNxzj6UzH0WNX7sTXqsmJjZk=">AAACiXichVG7SgNBFD1Z34maqI1gIwbFKtwVwZBKtLHMw8RAIrK7TnTNvtjdBDT4A1Z2olYKFuIH+AE2/oBFPkEsFWwsvNksiIrxDjNz5sw9d+ZwVcfQPZ+oHZH6+gcGh4ZHorHRsfF4YmKy5NkNVxNFzTZst6wqnjB0SxR93TdE2XGFYqqG2FLr6537raZwPd22Nv1DR2ybyp6l13RN8ZkqVVWztX+8k0hSioKY/Q3kECQRRtZO3KOKXdjQ0IAJAQs+YwMKPB4VyCA4zG2jxZzLSA/uBY4RZW2DswRnKMzWed3jUyVkLT53anqBWuNXDJ4uK2cxT090S6/0SHf0TB9/1moFNTp/OeRd7WqFsxM/mS68/6syefex/6XqoVA5u7cnHzWkAy86e3MCpuNS69ZvHp29FjL5+dYCXdML+7uiNj2wQ6v5pt3kRP4SUW6Q/LMdv0FpKSVTSs4tJ1fXwlYNYwZzWOR+rGAVG8iiyO8e4BTnuJBikiylpUw3VYqEmil8C2n9E+3ykjQ=</latexit>

h̃
<latexit sha1_base64="9NhyWcKddQJTH6hRkrChJ+EWIYQ=">AAACkXichVG7SgNBFD2u7/iK2gg2YlCswl0RfFRBG8HGV1QwKrvrJBncF7uTgC75AX9AwUrBQvwAP8DGH7DwE8RSwcbCm3VBNBjvMDNnztxzZw7X9G0ZKqKnFq21rb2js6s71dPb1z+QHhzaCr1KYIm85dlesGMaobClK/JKKlvs+IEwHNMW2+bRUv1+uyqCUHrupjr2xZ5jlFxZlJahmNovmE5UUNI+FFG5VjtIZyhLcYw1Aj0BGSSx6qXvUMAhPFiowIGAC8XYhoGQxy50EHzm9hAxFzCS8b1ADSnWVjhLcIbB7BGvJT7tJqzL53rNMFZb/IrNM2DlGCbokW7olR7olp7p489aUVyj/pdj3s0vrfAPBk5HNt7/VTm8K5S/VU0UJmc396RQxFzsRbI3P2bqLq2v+tWTs9eNhfWJaJKu6IX9XdIT3bNDt/pmXa+J9QukuEH673Y0gq3prE5ZfW0mk1tMWtWFUYxjivsxixyWsYo8vxvgHJe40oa1eS2nJblaS6IZxo/QVj4BcxKV6g==</latexit><latexit sha1_base64="9NhyWcKddQJTH6hRkrChJ+EWIYQ=">AAACkXichVG7SgNBFD2u7/iK2gg2YlCswl0RfFRBG8HGV1QwKrvrJBncF7uTgC75AX9AwUrBQvwAP8DGH7DwE8RSwcbCm3VBNBjvMDNnztxzZw7X9G0ZKqKnFq21rb2js6s71dPb1z+QHhzaCr1KYIm85dlesGMaobClK/JKKlvs+IEwHNMW2+bRUv1+uyqCUHrupjr2xZ5jlFxZlJahmNovmE5UUNI+FFG5VjtIZyhLcYw1Aj0BGSSx6qXvUMAhPFiowIGAC8XYhoGQxy50EHzm9hAxFzCS8b1ADSnWVjhLcIbB7BGvJT7tJqzL53rNMFZb/IrNM2DlGCbokW7olR7olp7p489aUVyj/pdj3s0vrfAPBk5HNt7/VTm8K5S/VU0UJmc396RQxFzsRbI3P2bqLq2v+tWTs9eNhfWJaJKu6IX9XdIT3bNDt/pmXa+J9QukuEH673Y0gq3prE5ZfW0mk1tMWtWFUYxjivsxixyWsYo8vxvgHJe40oa1eS2nJblaS6IZxo/QVj4BcxKV6g==</latexit><latexit sha1_base64="9NhyWcKddQJTH6hRkrChJ+EWIYQ=">AAACkXichVG7SgNBFD2u7/iK2gg2YlCswl0RfFRBG8HGV1QwKrvrJBncF7uTgC75AX9AwUrBQvwAP8DGH7DwE8RSwcbCm3VBNBjvMDNnztxzZw7X9G0ZKqKnFq21rb2js6s71dPb1z+QHhzaCr1KYIm85dlesGMaobClK/JKKlvs+IEwHNMW2+bRUv1+uyqCUHrupjr2xZ5jlFxZlJahmNovmE5UUNI+FFG5VjtIZyhLcYw1Aj0BGSSx6qXvUMAhPFiowIGAC8XYhoGQxy50EHzm9hAxFzCS8b1ADSnWVjhLcIbB7BGvJT7tJqzL53rNMFZb/IrNM2DlGCbokW7olR7olp7p489aUVyj/pdj3s0vrfAPBk5HNt7/VTm8K5S/VU0UJmc396RQxFzsRbI3P2bqLq2v+tWTs9eNhfWJaJKu6IX9XdIT3bNDt/pmXa+J9QukuEH673Y0gq3prE5ZfW0mk1tMWtWFUYxjivsxixyWsYo8vxvgHJe40oa1eS2nJblaS6IZxo/QVj4BcxKV6g==</latexit><latexit sha1_base64="9NhyWcKddQJTH6hRkrChJ+EWIYQ=">AAACkXichVG7SgNBFD2u7/iK2gg2YlCswl0RfFRBG8HGV1QwKrvrJBncF7uTgC75AX9AwUrBQvwAP8DGH7DwE8RSwcbCm3VBNBjvMDNnztxzZw7X9G0ZKqKnFq21rb2js6s71dPb1z+QHhzaCr1KYIm85dlesGMaobClK/JKKlvs+IEwHNMW2+bRUv1+uyqCUHrupjr2xZ5jlFxZlJahmNovmE5UUNI+FFG5VjtIZyhLcYw1Aj0BGSSx6qXvUMAhPFiowIGAC8XYhoGQxy50EHzm9hAxFzCS8b1ADSnWVjhLcIbB7BGvJT7tJqzL53rNMFZb/IrNM2DlGCbokW7olR7olp7p489aUVyj/pdj3s0vrfAPBk5HNt7/VTm8K5S/VU0UJmc396RQxFzsRbI3P2bqLq2v+tWTs9eNhfWJaJKu6IX9XdIT3bNDt/pmXa+J9QukuEH673Y0gq3prE5ZfW0mk1tMWtWFUYxjivsxixyWsYo8vxvgHJe40oa1eS2nJblaS6IZxo/QVj4BcxKV6g==</latexit>

X/NOUN/nsubj/>be/VERB/ROOT/-Y/NOUN/attr/<
<latexit sha1_base64="E1oZqr/5jwif/GwMlUhFAQrUeDM=">AAACt3ichVFNS+NQFD1mZvzojLaOG8GNTFFmY3MjgjqIiCK4slptraiUJL7a2DTJJK9FLf4B/4CCKwdciD/A3Wxc6B+YhT9hmKWCGxdzmwZERb3h5Z137j33vcM1PNsKJNFNi/Lh46fWtvaO2OcvnV3xRPfXXOBWfVNkTdd2/byhB8K2HJGVlrRF3vOFXjFssWKUZxr5lZrwA8t1luWuJzYq+pZjFS1Tl0wVEsPrUuxIKet5dT6dnVedoGpsq5OGUHOzmWk1k04vq0OrzZwupa9O7BcSSUpRGP0vgRaBJKJYcBMXWMcmXJioogIBB5KxDR0Bf2vQQPCY20CdOZ+RFeYF9hFjbZWrBFfozJb5v8WntYh1+NzoGYRqk2+xefms7McA/aEzuqVrOqe/9PBqr3rYo/GWXd6NplZ4hfhB79L9u6oK7xKlR9UbCoOr3/YkUcRY6MVib17INFyazf61vcPbpR+Zgfog/aJ/7O+EbuiSHTq1O/N0UWSOEeMBac/H8RLkhlMapbTFkeTUdDSqdvThG77zPEYxhTksIMv3HuE3rnCtjCsFpaiUmqVKS6TpwZNQfv4Hd26iKQ==</latexit><latexit sha1_base64="E1oZqr/5jwif/GwMlUhFAQrUeDM=">AAACt3ichVFNS+NQFD1mZvzojLaOG8GNTFFmY3MjgjqIiCK4slptraiUJL7a2DTJJK9FLf4B/4CCKwdciD/A3Wxc6B+YhT9hmKWCGxdzmwZERb3h5Z137j33vcM1PNsKJNFNi/Lh46fWtvaO2OcvnV3xRPfXXOBWfVNkTdd2/byhB8K2HJGVlrRF3vOFXjFssWKUZxr5lZrwA8t1luWuJzYq+pZjFS1Tl0wVEsPrUuxIKet5dT6dnVedoGpsq5OGUHOzmWk1k04vq0OrzZwupa9O7BcSSUpRGP0vgRaBJKJYcBMXWMcmXJioogIBB5KxDR0Bf2vQQPCY20CdOZ+RFeYF9hFjbZWrBFfozJb5v8WntYh1+NzoGYRqk2+xefms7McA/aEzuqVrOqe/9PBqr3rYo/GWXd6NplZ4hfhB79L9u6oK7xKlR9UbCoOr3/YkUcRY6MVib17INFyazf61vcPbpR+Zgfog/aJ/7O+EbuiSHTq1O/N0UWSOEeMBac/H8RLkhlMapbTFkeTUdDSqdvThG77zPEYxhTksIMv3HuE3rnCtjCsFpaiUmqVKS6TpwZNQfv4Hd26iKQ==</latexit><latexit sha1_base64="E1oZqr/5jwif/GwMlUhFAQrUeDM=">AAACt3ichVFNS+NQFD1mZvzojLaOG8GNTFFmY3MjgjqIiCK4slptraiUJL7a2DTJJK9FLf4B/4CCKwdciD/A3Wxc6B+YhT9hmKWCGxdzmwZERb3h5Z137j33vcM1PNsKJNFNi/Lh46fWtvaO2OcvnV3xRPfXXOBWfVNkTdd2/byhB8K2HJGVlrRF3vOFXjFssWKUZxr5lZrwA8t1luWuJzYq+pZjFS1Tl0wVEsPrUuxIKet5dT6dnVedoGpsq5OGUHOzmWk1k04vq0OrzZwupa9O7BcSSUpRGP0vgRaBJKJYcBMXWMcmXJioogIBB5KxDR0Bf2vQQPCY20CdOZ+RFeYF9hFjbZWrBFfozJb5v8WntYh1+NzoGYRqk2+xefms7McA/aEzuqVrOqe/9PBqr3rYo/GWXd6NplZ4hfhB79L9u6oK7xKlR9UbCoOr3/YkUcRY6MVib17INFyazf61vcPbpR+Zgfog/aJ/7O+EbuiSHTq1O/N0UWSOEeMBac/H8RLkhlMapbTFkeTUdDSqdvThG77zPEYxhTksIMv3HuE3rnCtjCsFpaiUmqVKS6TpwZNQfv4Hd26iKQ==</latexit><latexit sha1_base64="dO1RVWgbv0QBXFnWGIM/DJuxBX8=">AAACdXichVG9TsJQGD1UVEQU3ExciARjHMhXHTROJi6O/MhPgoS05YINpW3aQoLEF2B1cHDSxMH4AD6Aiy/gwCMYR0xcHPwoJEaJ+jXtPffc73y3J0e1Dd31iAYBaSY4OzcfWggvRsJLy9FYpOBabUcTec0yLKekKq4wdFPkPd0zRMl2hNJSDVFUm4ej82JHOK5umcde1xaVltIw9bquKR5T6WosQSnyKz4N5AlIYFJW7AEnqMGChjZaEDDhMTagwOWnDBkEm7kKesw5jHT/XOAcYda2uUtwh8Jsk78N3pUnrMn70UzXV2t8i8Gvw8o4kvRMdzSkJ7qnF/r4dVbPnzH6ly6v6lgr7Gq0v5p7/1fV4tXD6ZfqD4XK3X978lDHnu9FZ2+2z4xcauP5nbPLYW4/m+xt0A29sr9rGtAjOzQ7b9ptRmSvEOZ85J9pTIPCdkqmlJwhhLCGdWxyDLs4wBHSyPN1NfRxIQWlLUke5ygFJoGu4FtJO59jaI2O</latexit><latexit sha1_base64="gm4KyDN5ykT3iI6S7uQ6SLnEvr0=">AAACrHichVFNLwNRFD3Gd320rCQ2QoiNzh0bHxERIrFSSquCNDPjlWE6M5l5bdD4A/4AiRWJhfgBdjYW/AELP0EsSWws3E6bCII7efPuO/ee+97JMTzbCiTRY41SW1ff0NjUHGlpbWuPxjpa04Fb8E2RMl3b9TOGHgjbckRKWtIWGc8Xet6wxYqxO1OurxSFH1iusyz3PbGR17ccK2eZumQoGxtel2JPSlnKqPOJ1LzqBAVjR500hJqeTU6ryURiWR1ardR0KX114jAb66M4hdHzM9GqSR+qseDGrrGOTbgwUUAeAg4k5zZ0BPytQQPBY2wDJcZ8zqywLnCICHML3CW4Q2d0l/9bfFqrog6fyzODkG3yLTYvn5k96KcHuqQXuqcreqL3X2eVwhnlt+zzblS4wstGj7qW3v5l5XmX2P5k/cEwuPtvTRI5jIZaLNbmhUhZpVmZXzw4flkaT/aXBuicnlnfGT3SLSt0iq/mxaJIniLCBmnf7fiZpIfjGsW1RUITutGLQbZhBFOYwwJSfN0JbnCHe2VMySq5ipVKTdXTTnwJZfsDeRuhBg==</latexit><latexit sha1_base64="gm4KyDN5ykT3iI6S7uQ6SLnEvr0=">AAACrHichVFNLwNRFD3Gd320rCQ2QoiNzh0bHxERIrFSSquCNDPjlWE6M5l5bdD4A/4AiRWJhfgBdjYW/AELP0EsSWws3E6bCII7efPuO/ee+97JMTzbCiTRY41SW1ff0NjUHGlpbWuPxjpa04Fb8E2RMl3b9TOGHgjbckRKWtIWGc8Xet6wxYqxO1OurxSFH1iusyz3PbGR17ccK2eZumQoGxtel2JPSlnKqPOJ1LzqBAVjR500hJqeTU6ryURiWR1ardR0KX114jAb66M4hdHzM9GqSR+qseDGrrGOTbgwUUAeAg4k5zZ0BPytQQPBY2wDJcZ8zqywLnCICHML3CW4Q2d0l/9bfFqrog6fyzODkG3yLTYvn5k96KcHuqQXuqcreqL3X2eVwhnlt+zzblS4wstGj7qW3v5l5XmX2P5k/cEwuPtvTRI5jIZaLNbmhUhZpVmZXzw4flkaT/aXBuicnlnfGT3SLSt0iq/mxaJIniLCBmnf7fiZpIfjGsW1RUITutGLQbZhBFOYwwJSfN0JbnCHe2VMySq5ipVKTdXTTnwJZfsDeRuhBg==</latexit><latexit sha1_base64="yO51ddTgyJig+DNKyqMB3pN8+sQ=">AAACt3ichVFNS+NQFD1GHbXjjFU3ghuZoriZ5saNH4iIIriy2tpaaaUk8VWjaRKT16IW/4B/QGFWI7iQ+QGzm40L+wdm4U8Qlw7MxoW3aUBmRL3h5Z137j33vcM1PNsKJNFtm9Le0fmhq7sn9rH30+e+eP9ALnCrvimypmu7ft7QA2FbjshKS9oi7/lCrxi22DD2F5v5jZrwA8t11uWRJ7Yq+o5jlS1Tl0yV4hNFKQ6llPW8upLKrqhOUDX21DlDqLml9IKaTqXW1a+brZwupa/OnpTiCUpSGCMvgRaBBKJYdeM/UcQ2XJioogIBB5KxDR0BfwVoIHjMbaHOnM/ICvMCJ4ixtspVgit0Zvf5v8OnQsQ6fG72DEK1ybfYvHxWjmCUftMVPVCDftAdPb7aqx72aL7liHejpRVeqe90KPP3XVWFd4ndZ9UbCoOr3/YkUcZU6MVib17INF2arf6147OHzEx6tD5GF3TP/r7TLV2zQ6f2x7xcE+lviPGAtP/H8RLkJpIaJbU1SswvRKPqxjC+YJznMYl5LGMVWb73HL9wg4YyrZSUsrLbKlXaIs0g/gnl4Al2LqIl</latexit><latexit sha1_base64="E1oZqr/5jwif/GwMlUhFAQrUeDM=">AAACt3ichVFNS+NQFD1mZvzojLaOG8GNTFFmY3MjgjqIiCK4slptraiUJL7a2DTJJK9FLf4B/4CCKwdciD/A3Wxc6B+YhT9hmKWCGxdzmwZERb3h5Z137j33vcM1PNsKJNFNi/Lh46fWtvaO2OcvnV3xRPfXXOBWfVNkTdd2/byhB8K2HJGVlrRF3vOFXjFssWKUZxr5lZrwA8t1luWuJzYq+pZjFS1Tl0wVEsPrUuxIKet5dT6dnVedoGpsq5OGUHOzmWk1k04vq0OrzZwupa9O7BcSSUpRGP0vgRaBJKJYcBMXWMcmXJioogIBB5KxDR0Bf2vQQPCY20CdOZ+RFeYF9hFjbZWrBFfozJb5v8WntYh1+NzoGYRqk2+xefms7McA/aEzuqVrOqe/9PBqr3rYo/GWXd6NplZ4hfhB79L9u6oK7xKlR9UbCoOr3/YkUcRY6MVib17INFyazf61vcPbpR+Zgfog/aJ/7O+EbuiSHTq1O/N0UWSOEeMBac/H8RLkhlMapbTFkeTUdDSqdvThG77zPEYxhTksIMv3HuE3rnCtjCsFpaiUmqVKS6TpwZNQfv4Hd26iKQ==</latexit><latexit sha1_base64="E1oZqr/5jwif/GwMlUhFAQrUeDM=">AAACt3ichVFNS+NQFD1mZvzojLaOG8GNTFFmY3MjgjqIiCK4slptraiUJL7a2DTJJK9FLf4B/4CCKwdciD/A3Wxc6B+YhT9hmKWCGxdzmwZERb3h5Z137j33vcM1PNsKJNFNi/Lh46fWtvaO2OcvnV3xRPfXXOBWfVNkTdd2/byhB8K2HJGVlrRF3vOFXjFssWKUZxr5lZrwA8t1luWuJzYq+pZjFS1Tl0wVEsPrUuxIKet5dT6dnVedoGpsq5OGUHOzmWk1k04vq0OrzZwupa9O7BcSSUpRGP0vgRaBJKJYcBMXWMcmXJioogIBB5KxDR0Bf2vQQPCY20CdOZ+RFeYF9hFjbZWrBFfozJb5v8WntYh1+NzoGYRqk2+xefms7McA/aEzuqVrOqe/9PBqr3rYo/GWXd6NplZ4hfhB79L9u6oK7xKlR9UbCoOr3/YkUcRY6MVib17INFyazf61vcPbpR+Zgfog/aJ/7O+EbuiSHTq1O/N0UWSOEeMBac/H8RLkhlMapbTFkeTUdDSqdvThG77zPEYxhTksIMv3HuE3rnCtjCsFpaiUmqVKS6TpwZNQfv4Hd26iKQ==</latexit><latexit sha1_base64="E1oZqr/5jwif/GwMlUhFAQrUeDM=">AAACt3ichVFNS+NQFD1mZvzojLaOG8GNTFFmY3MjgjqIiCK4slptraiUJL7a2DTJJK9FLf4B/4CCKwdciD/A3Wxc6B+YhT9hmKWCGxdzmwZERb3h5Z137j33vcM1PNsKJNFNi/Lh46fWtvaO2OcvnV3xRPfXXOBWfVNkTdd2/byhB8K2HJGVlrRF3vOFXjFssWKUZxr5lZrwA8t1luWuJzYq+pZjFS1Tl0wVEsPrUuxIKet5dT6dnVedoGpsq5OGUHOzmWk1k04vq0OrzZwupa9O7BcSSUpRGP0vgRaBJKJYcBMXWMcmXJioogIBB5KxDR0Bf2vQQPCY20CdOZ+RFeYF9hFjbZWrBFfozJb5v8WntYh1+NzoGYRqk2+xefms7McA/aEzuqVrOqe/9PBqr3rYo/GWXd6NplZ4hfhB79L9u6oK7xKlR9UbCoOr3/YkUcRY6MVib17INFyazf61vcPbpR+Zgfog/aJ/7O+EbuiSHTq1O/N0UWSOEeMBac/H8RLkhlMapbTFkeTUdDSqdvThG77zPEYxhTksIMv3HuE3rnCtjCsFpaiUmqVKS6TpwZNQfv4Hd26iKQ==</latexit><latexit sha1_base64="E1oZqr/5jwif/GwMlUhFAQrUeDM=">AAACt3ichVFNS+NQFD1mZvzojLaOG8GNTFFmY3MjgjqIiCK4slptraiUJL7a2DTJJK9FLf4B/4CCKwdciD/A3Wxc6B+YhT9hmKWCGxdzmwZERb3h5Z137j33vcM1PNsKJNFNi/Lh46fWtvaO2OcvnV3xRPfXXOBWfVNkTdd2/byhB8K2HJGVlrRF3vOFXjFssWKUZxr5lZrwA8t1luWuJzYq+pZjFS1Tl0wVEsPrUuxIKet5dT6dnVedoGpsq5OGUHOzmWk1k04vq0OrzZwupa9O7BcSSUpRGP0vgRaBJKJYcBMXWMcmXJioogIBB5KxDR0Bf2vQQPCY20CdOZ+RFeYF9hFjbZWrBFfozJb5v8WntYh1+NzoGYRqk2+xefms7McA/aEzuqVrOqe/9PBqr3rYo/GWXd6NplZ4hfhB79L9u6oK7xKlR9UbCoOr3/YkUcRY6MVib17INFyazf61vcPbpR+Zgfog/aJ/7O+EbuiSHTq1O/N0UWSOEeMBac/H8RLkhlMapbTFkeTUdDSqdvThG77zPEYxhTksIMv3HuE3rnCtjCsFpaiUmqVKS6TpwZNQfv4Hd26iKQ==</latexit><latexit sha1_base64="E1oZqr/5jwif/GwMlUhFAQrUeDM=">AAACt3ichVFNS+NQFD1mZvzojLaOG8GNTFFmY3MjgjqIiCK4slptraiUJL7a2DTJJK9FLf4B/4CCKwdciD/A3Wxc6B+YhT9hmKWCGxdzmwZERb3h5Z137j33vcM1PNsKJNFNi/Lh46fWtvaO2OcvnV3xRPfXXOBWfVNkTdd2/byhB8K2HJGVlrRF3vOFXjFssWKUZxr5lZrwA8t1luWuJzYq+pZjFS1Tl0wVEsPrUuxIKet5dT6dnVedoGpsq5OGUHOzmWk1k04vq0OrzZwupa9O7BcSSUpRGP0vgRaBJKJYcBMXWMcmXJioogIBB5KxDR0Bf2vQQPCY20CdOZ+RFeYF9hFjbZWrBFfozJb5v8WntYh1+NzoGYRqk2+xefms7McA/aEzuqVrOqe/9PBqr3rYo/GWXd6NplZ4hfhB79L9u6oK7xKlR9UbCoOr3/YkUcRY6MVib17INFyazf61vcPbpR+Zgfog/aJ/7O+EbuiSHTq1O/N0UWSOEeMBac/H8RLkhlMapbTFkeTUdDSqdvThG77zPEYxhTksIMv3HuE3rnCtjCsFpaiUmqVKS6TpwZNQfv4Hd26iKQ==</latexit><latexit sha1_base64="E1oZqr/5jwif/GwMlUhFAQrUeDM=">AAACt3ichVFNS+NQFD1mZvzojLaOG8GNTFFmY3MjgjqIiCK4slptraiUJL7a2DTJJK9FLf4B/4CCKwdciD/A3Wxc6B+YhT9hmKWCGxdzmwZERb3h5Z137j33vcM1PNsKJNFNi/Lh46fWtvaO2OcvnV3xRPfXXOBWfVNkTdd2/byhB8K2HJGVlrRF3vOFXjFssWKUZxr5lZrwA8t1luWuJzYq+pZjFS1Tl0wVEsPrUuxIKet5dT6dnVedoGpsq5OGUHOzmWk1k04vq0OrzZwupa9O7BcSSUpRGP0vgRaBJKJYcBMXWMcmXJioogIBB5KxDR0Bf2vQQPCY20CdOZ+RFeYF9hFjbZWrBFfozJb5v8WntYh1+NzoGYRqk2+xefms7McA/aEzuqVrOqe/9PBqr3rYo/GWXd6NplZ4hfhB79L9u6oK7xKlR9UbCoOr3/YkUcRY6MVib17INFyazf61vcPbpR+Zgfog/aJ/7O+EbuiSHTq1O/N0UWSOEeMBac/H8RLkhlMapbTFkeTUdDSqdvThG77zPEYxhTksIMv3HuE3rnCtjCsFpaiUmqVKS6TpwZNQfv4Hd26iKQ==</latexit>

X/NOUN/nsubj/>use/VERB/ROOT/-Y/NOUN/dobj/<
<latexit sha1_base64="mXtEx9P6NC/iCWUB9W39xWtgAWk=">AAACuHichVHBShtRFD2Otmpaa9SN0I00pLgxc6cKioiIpeDKaExiRCWdGZ/p08nMMPMm1AZ/oD9QiisFF6Uf0GUX3dgP6CKfUFxGcOOiN5OBUkV7hzfvvHPvue8druU7MlRErR6tt+/R4/6BwdSTp0PPhtMjo+XQiwJblGzP8YKKZYbCka4oKakcUfEDYdYtR2xah687+c2GCELpuUV15Ivdullz5b60TcVUNT29o8R7pVSzoq/mS6u6G0bWgb4YhUIvvyks64V8vqhPbXWTex7nFo6r6QzlKI6Ju8BIQAZJrHnpb9jBHjzYiFCHgAvF2IGJkL9tGCD4zO2iyVzASMZ5gWOkWBtxleAKk9lD/tf4tJ2wLp87PcNYbfMtDq+AlRPI0i/6Qm26oK/0m27u7dWMe3TecsS71dUKvzr8cXzj+r+qOu8K7/6qHlBYXP2wJ4V9zMVeJHvzY6bj0u72b3z41N6YL2SbL+mMLtnfKbXoBzt0G1f2+boonCDFAzJuj+MuKL/KGZQz1mcyS8vJqAbwHC8wyfOYxRJWsIYS3/sZ33GBn9q89larabJbqvUkmjH8E1rwB5T3op0=</latexit><latexit sha1_base64="mXtEx9P6NC/iCWUB9W39xWtgAWk=">AAACuHichVHBShtRFD2Otmpaa9SN0I00pLgxc6cKioiIpeDKaExiRCWdGZ/p08nMMPMm1AZ/oD9QiisFF6Uf0GUX3dgP6CKfUFxGcOOiN5OBUkV7hzfvvHPvue8druU7MlRErR6tt+/R4/6BwdSTp0PPhtMjo+XQiwJblGzP8YKKZYbCka4oKakcUfEDYdYtR2xah687+c2GCELpuUV15Ivdullz5b60TcVUNT29o8R7pVSzoq/mS6u6G0bWgb4YhUIvvyks64V8vqhPbXWTex7nFo6r6QzlKI6Ju8BIQAZJrHnpb9jBHjzYiFCHgAvF2IGJkL9tGCD4zO2iyVzASMZ5gWOkWBtxleAKk9lD/tf4tJ2wLp87PcNYbfMtDq+AlRPI0i/6Qm26oK/0m27u7dWMe3TecsS71dUKvzr8cXzj+r+qOu8K7/6qHlBYXP2wJ4V9zMVeJHvzY6bj0u72b3z41N6YL2SbL+mMLtnfKbXoBzt0G1f2+boonCDFAzJuj+MuKL/KGZQz1mcyS8vJqAbwHC8wyfOYxRJWsIYS3/sZ33GBn9q89larabJbqvUkmjH8E1rwB5T3op0=</latexit><latexit sha1_base64="mXtEx9P6NC/iCWUB9W39xWtgAWk=">AAACuHichVHBShtRFD2Otmpaa9SN0I00pLgxc6cKioiIpeDKaExiRCWdGZ/p08nMMPMm1AZ/oD9QiisFF6Uf0GUX3dgP6CKfUFxGcOOiN5OBUkV7hzfvvHPvue8druU7MlRErR6tt+/R4/6BwdSTp0PPhtMjo+XQiwJblGzP8YKKZYbCka4oKakcUfEDYdYtR2xah687+c2GCELpuUV15Ivdullz5b60TcVUNT29o8R7pVSzoq/mS6u6G0bWgb4YhUIvvyks64V8vqhPbXWTex7nFo6r6QzlKI6Ju8BIQAZJrHnpb9jBHjzYiFCHgAvF2IGJkL9tGCD4zO2iyVzASMZ5gWOkWBtxleAKk9lD/tf4tJ2wLp87PcNYbfMtDq+AlRPI0i/6Qm26oK/0m27u7dWMe3TecsS71dUKvzr8cXzj+r+qOu8K7/6qHlBYXP2wJ4V9zMVeJHvzY6bj0u72b3z41N6YL2SbL+mMLtnfKbXoBzt0G1f2+boonCDFAzJuj+MuKL/KGZQz1mcyS8vJqAbwHC8wyfOYxRJWsIYS3/sZ33GBn9q89larabJbqvUkmjH8E1rwB5T3op0=</latexit><latexit sha1_base64="mXtEx9P6NC/iCWUB9W39xWtgAWk=">AAACuHichVHBShtRFD2Otmpaa9SN0I00pLgxc6cKioiIpeDKaExiRCWdGZ/p08nMMPMm1AZ/oD9QiisFF6Uf0GUX3dgP6CKfUFxGcOOiN5OBUkV7hzfvvHPvue8druU7MlRErR6tt+/R4/6BwdSTp0PPhtMjo+XQiwJblGzP8YKKZYbCka4oKakcUfEDYdYtR2xah687+c2GCELpuUV15Ivdullz5b60TcVUNT29o8R7pVSzoq/mS6u6G0bWgb4YhUIvvyks64V8vqhPbXWTex7nFo6r6QzlKI6Ju8BIQAZJrHnpb9jBHjzYiFCHgAvF2IGJkL9tGCD4zO2iyVzASMZ5gWOkWBtxleAKk9lD/tf4tJ2wLp87PcNYbfMtDq+AlRPI0i/6Qm26oK/0m27u7dWMe3TecsS71dUKvzr8cXzj+r+qOu8K7/6qHlBYXP2wJ4V9zMVeJHvzY6bj0u72b3z41N6YL2SbL+mMLtnfKbXoBzt0G1f2+boonCDFAzJuj+MuKL/KGZQz1mcyS8vJqAbwHC8wyfOYxRJWsIYS3/sZ33GBn9q89larabJbqvUkmjH8E1rwB5T3op0=</latexit>

vpath
<latexit sha1_base64="CYws+TkVgllPJYVjfIAOoCPDuc8=">AAACkHichVHNLgNRFP6M//orNhIb0RCr5oxINDaKjVhRioSmmRmXTsyfmdsmNekLeIEubFRiIR7AA9h4AQuPIJYkNhZOp5MIgnNz7/3ud8937v1ydM8yA0n02Ka0d3R2dff0Jvr6BwaHksMj24Fb9g2RN1zL9Xd1LRCW6Yi8NKUldj1faLZuiR39eKV5v1MRfmC6zpaseqJga0eOeWgammSqsK/bYaVWDD1NlmrFZIrSFMXET6DGIIU41t3kLfZxABcGyrAh4EAytqAh4LEHFQSPuQJC5nxGZnQvUEOCtWXOEpyhMXvM6xGf9mLW4XOzZhCpDX7F4umzcgJT9EDX9EL3dENP9P5rrTCq0fxLlXe9pRVecehsbPPtX5XNu0TpU/WHQufsvz1JHCITeTHZmxcxTZdGq37ltP6yuZCbCqfpkp7ZX4Me6Y4dOpVX42pD5M6R4Aap39vxE2zPplVKqxtzqexy3KoejGMSM9yPeWSxinXk+d0T1HGBhjKiZJRFZamVqrTFmlF8CWXtA5arlYw=</latexit><latexit sha1_base64="CYws+TkVgllPJYVjfIAOoCPDuc8=">AAACkHichVHNLgNRFP6M//orNhIb0RCr5oxINDaKjVhRioSmmRmXTsyfmdsmNekLeIEubFRiIR7AA9h4AQuPIJYkNhZOp5MIgnNz7/3ud8937v1ydM8yA0n02Ka0d3R2dff0Jvr6BwaHksMj24Fb9g2RN1zL9Xd1LRCW6Yi8NKUldj1faLZuiR39eKV5v1MRfmC6zpaseqJga0eOeWgammSqsK/bYaVWDD1NlmrFZIrSFMXET6DGIIU41t3kLfZxABcGyrAh4EAytqAh4LEHFQSPuQJC5nxGZnQvUEOCtWXOEpyhMXvM6xGf9mLW4XOzZhCpDX7F4umzcgJT9EDX9EL3dENP9P5rrTCq0fxLlXe9pRVecehsbPPtX5XNu0TpU/WHQufsvz1JHCITeTHZmxcxTZdGq37ltP6yuZCbCqfpkp7ZX4Me6Y4dOpVX42pD5M6R4Aap39vxE2zPplVKqxtzqexy3KoejGMSM9yPeWSxinXk+d0T1HGBhjKiZJRFZamVqrTFmlF8CWXtA5arlYw=</latexit><latexit sha1_base64="CYws+TkVgllPJYVjfIAOoCPDuc8=">AAACkHichVHNLgNRFP6M//orNhIb0RCr5oxINDaKjVhRioSmmRmXTsyfmdsmNekLeIEubFRiIR7AA9h4AQuPIJYkNhZOp5MIgnNz7/3ud8937v1ydM8yA0n02Ka0d3R2dff0Jvr6BwaHksMj24Fb9g2RN1zL9Xd1LRCW6Yi8NKUldj1faLZuiR39eKV5v1MRfmC6zpaseqJga0eOeWgammSqsK/bYaVWDD1NlmrFZIrSFMXET6DGIIU41t3kLfZxABcGyrAh4EAytqAh4LEHFQSPuQJC5nxGZnQvUEOCtWXOEpyhMXvM6xGf9mLW4XOzZhCpDX7F4umzcgJT9EDX9EL3dENP9P5rrTCq0fxLlXe9pRVecehsbPPtX5XNu0TpU/WHQufsvz1JHCITeTHZmxcxTZdGq37ltP6yuZCbCqfpkp7ZX4Me6Y4dOpVX42pD5M6R4Aap39vxE2zPplVKqxtzqexy3KoejGMSM9yPeWSxinXk+d0T1HGBhjKiZJRFZamVqrTFmlF8CWXtA5arlYw=</latexit><latexit sha1_base64="CYws+TkVgllPJYVjfIAOoCPDuc8=">AAACkHichVHNLgNRFP6M//orNhIb0RCr5oxINDaKjVhRioSmmRmXTsyfmdsmNekLeIEubFRiIR7AA9h4AQuPIJYkNhZOp5MIgnNz7/3ud8937v1ydM8yA0n02Ka0d3R2dff0Jvr6BwaHksMj24Fb9g2RN1zL9Xd1LRCW6Yi8NKUldj1faLZuiR39eKV5v1MRfmC6zpaseqJga0eOeWgammSqsK/bYaVWDD1NlmrFZIrSFMXET6DGIIU41t3kLfZxABcGyrAh4EAytqAh4LEHFQSPuQJC5nxGZnQvUEOCtWXOEpyhMXvM6xGf9mLW4XOzZhCpDX7F4umzcgJT9EDX9EL3dENP9P5rrTCq0fxLlXe9pRVecehsbPPtX5XNu0TpU/WHQufsvz1JHCITeTHZmxcxTZdGq37ltP6yuZCbCqfpkp7ZX4Me6Y4dOpVX42pD5M6R4Aap39vxE2zPplVKqxtzqexy3KoejGMSM9yPeWSxinXk+d0T1HGBhjKiZJRFZamVqrTFmlF8CWXtA5arlYw=</latexit>

vpath0
<latexit sha1_base64="cBHnqO+WWUcshC0rQX0ZBRxOQ7w=">AAACkXichVG7SgNBFD2u7/iK2gg2Yohahbsi+KiCNoKNr2ggiWF3nejivtidBHTJD/gDClYKFuIH+AE2/oCFnyCWCjYW3qwLosF4h5k5c+aeO3O4umeZgSR6alPaOzq7unt6E339A4NDyeGRncCt+obIGa7l+nldC4RlOiInTWmJvOcLzdYtsasfrTTud2vCD0zX2ZbHnijZ2oFjVkxDk0ztFXU7rNXLoafJw+l6OZmiDEUx0QzUGKQQx7qbvEMR+3BhoAobAg4kYwsaAh4FqCB4zJUQMuczMqN7gToSrK1yluAMjdkjXg/4VIhZh8+NmkGkNvgVi6fPygmk6ZFu6JUe6Jae6ePPWmFUo/GXY971L63wykOnY1vv/6ps3iUOv1UtFDpnt/YkUcFC5MVkb17ENFwaX/VrJ2evW0ub6XCKruiF/V3SE92zQ6f2ZlxviM0LJLhB6u92NIOd2YxKGXVjLpVdjlvVg3FMYob7MY8sVrGOHL/r4xyXuFJGlUUlq8S5SlusGcWPUNY+ARR5lb0=</latexit><latexit sha1_base64="cBHnqO+WWUcshC0rQX0ZBRxOQ7w=">AAACkXichVG7SgNBFD2u7/iK2gg2Yohahbsi+KiCNoKNr2ggiWF3nejivtidBHTJD/gDClYKFuIH+AE2/oCFnyCWCjYW3qwLosF4h5k5c+aeO3O4umeZgSR6alPaOzq7unt6E339A4NDyeGRncCt+obIGa7l+nldC4RlOiInTWmJvOcLzdYtsasfrTTud2vCD0zX2ZbHnijZ2oFjVkxDk0ztFXU7rNXLoafJw+l6OZmiDEUx0QzUGKQQx7qbvEMR+3BhoAobAg4kYwsaAh4FqCB4zJUQMuczMqN7gToSrK1yluAMjdkjXg/4VIhZh8+NmkGkNvgVi6fPygmk6ZFu6JUe6Jae6ePPWmFUo/GXY971L63wykOnY1vv/6ps3iUOv1UtFDpnt/YkUcFC5MVkb17ENFwaX/VrJ2evW0ub6XCKruiF/V3SE92zQ6f2ZlxviM0LJLhB6u92NIOd2YxKGXVjLpVdjlvVg3FMYob7MY8sVrGOHL/r4xyXuFJGlUUlq8S5SlusGcWPUNY+ARR5lb0=</latexit><latexit sha1_base64="cBHnqO+WWUcshC0rQX0ZBRxOQ7w=">AAACkXichVG7SgNBFD2u7/iK2gg2Yohahbsi+KiCNoKNr2ggiWF3nejivtidBHTJD/gDClYKFuIH+AE2/oCFnyCWCjYW3qwLosF4h5k5c+aeO3O4umeZgSR6alPaOzq7unt6E339A4NDyeGRncCt+obIGa7l+nldC4RlOiInTWmJvOcLzdYtsasfrTTud2vCD0zX2ZbHnijZ2oFjVkxDk0ztFXU7rNXLoafJw+l6OZmiDEUx0QzUGKQQx7qbvEMR+3BhoAobAg4kYwsaAh4FqCB4zJUQMuczMqN7gToSrK1yluAMjdkjXg/4VIhZh8+NmkGkNvgVi6fPygmk6ZFu6JUe6Jae6ePPWmFUo/GXY971L63wykOnY1vv/6ps3iUOv1UtFDpnt/YkUcFC5MVkb17ENFwaX/VrJ2evW0ub6XCKruiF/V3SE92zQ6f2ZlxviM0LJLhB6u92NIOd2YxKGXVjLpVdjlvVg3FMYob7MY8sVrGOHL/r4xyXuFJGlUUlq8S5SlusGcWPUNY+ARR5lb0=</latexit><latexit sha1_base64="cBHnqO+WWUcshC0rQX0ZBRxOQ7w=">AAACkXichVG7SgNBFD2u7/iK2gg2Yohahbsi+KiCNoKNr2ggiWF3nejivtidBHTJD/gDClYKFuIH+AE2/oCFnyCWCjYW3qwLosF4h5k5c+aeO3O4umeZgSR6alPaOzq7unt6E339A4NDyeGRncCt+obIGa7l+nldC4RlOiInTWmJvOcLzdYtsasfrTTud2vCD0zX2ZbHnijZ2oFjVkxDk0ztFXU7rNXLoafJw+l6OZmiDEUx0QzUGKQQx7qbvEMR+3BhoAobAg4kYwsaAh4FqCB4zJUQMuczMqN7gToSrK1yluAMjdkjXg/4VIhZh8+NmkGkNvgVi6fPygmk6ZFu6JUe6Jae6ePPWmFUo/GXY971L63wykOnY1vv/6ps3iUOv1UtFDpnt/YkUcFC5MVkb17ENFwaX/VrJ2evW0ub6XCKruiF/V3SE92zQ6f2ZlxviM0LJLhB6u92NIOd2YxKGXVjLpVdjlvVg3FMYob7MY8sVrGOHL/r4xyXuFJGlUUlq8S5SlusGcWPUNY+ARR5lb0=</latexit>

Figure 1: An illustration of our network for modeling
P (path|w1, w2). Given a word pair (dog, animal),
our model makes h̃ of (dog, animal) similar to
vpath of the observed co-occurring dependency
path X/NOUN/nsubj/> be/VERB/ROOT/-
Y/NOUN/attr/< and dissimilar to vpath′ of the
unobserved paths, such as X/NOUN/nsubj/>
use/VERB/ROOT/- Y/NOUN/dobj/<, through
unsupervised learning.

Figure 1 depicts our network structure, which is
described below.

Data and Network Architecture
We are able to extract many triples (w1, w2, path)
from a corpus after dependency parsing. We de-
note a set of these triples as D. These triples
are the instances used for the unsupervised learn-
ing of P (path|w1, w2). Given (w1, w2, path), our
model learns through predicting path fromw1 and
w2.

We encode word pairs into dense vectors as fol-
lows:

h(w1,w2) = tanh(W1[vw1 ;vw2 ] + b1) (6)

h̃(w1,w2) = tanh(W2h(w1,w2) + b2) (7)

where [vw1 ;vw2 ] is the concatenation of the word
embeddings of w1 and w2; W1, b1, W2, and b2
are the parameter matrices and bias parameters of
the two linear transformations; and h̃(w1,w2) is the
representation of the word pair.

We associate each path with the embedding
vpath, initialized randomly. While we use a simple
way to represent dependency paths in this paper,
LSTM can be used to encode each path in the way
described in Section 2.2. If LSTM is used, learn-
ing time increases but similarities among paths
will be captured.

Objective
We used the negative sampling objective for train-
ing (Mikolov et al., 2013). Given the word pair

representations h̃(w1,w2) and the dependency path
representations vpath, our model was trained to
distinguish real (w1, w2, path) triples from incor-
rect ones. The log-likelihood objective is as fol-
lows:

L =
∑

(w1,w2,path)∈D
log σ(vpath · h̃(w1,w2))

+
∑

(w1,w2,path′)∈D′
log σ(−vpath′ · h̃(w1,w2)) (8)

where, D′ is the set of randomly generated
negative samples. We constructed n triples
(w1, w2, path

′) for each (w1, w2, path) ∈ D,
where n is a hyperparameter and each path′ is
drawn according to its unigram distribution raised
to the 3/4 power. The objective L was maximized
using the stochastic gradient descent algorithm.

3.2 Path Data Augmentation

After the unsupervised learning described above,
our model of P (path|w1, w2) can assign the
plausibility score σ(vpath · h̃(w1,w2)) to the co-
occurrences of a word pair and a dependency
path. We can then append the plausible depen-
dency paths to paths(w1, w2), the set of depen-
dency paths that connectsw1 andw2 in the corpus,
based on these scores.

We calculate the score of each dependency path
given (X = w1, Y = w2) and append the
k dependency paths with the highest scores to
paths(w1, w2), where k is a hyperparameter. We
perform the same process given (X = w2, Y =
w1) with the exception of swapping the X and Y
in the dependency paths to be appended. As a re-
sult, we add 2k dependency paths to the set of de-
pendency paths for each word pair. Through this
data augmentation, we can obtain plausible depen-
dency paths even when word pairs do not co-occur
in the corpus. Note that we retain the empty path
indicators of paths(w1, w2), as we believe that
this information contributes to classifying two un-
related words.

3.3 Feature Extractor of Word Pairs

Our model can be used as a feature extractor
of word pairs. We can exploit h̃(w1,w2) to rep-
resent the word pair (w1, w2). This represen-
tation captures the information of co-occurrence
dependency paths of (w1, w2) in a generalized
fashion. Thus, h̃(w1,w2) is used to construct the
pseudo-path representation vp−paths(w1,w2). With
our model, we represent the word pair (w1, w2) as

1126



datasets relations
K&H+N hypernym, meronym, co-hyponym, random
BLESS hypernym, meronym, co-hyponym, random

ROOT09 hypernym, co-hyponym, random
EVALution hypernym, meronym, attribute, synonym, antonym, holonym, substance meronym

Table 1: The relation types in each dataset.

follows:

vp−paths(w1,w2) = [h̃(w1,w2); h̃(w2,w1)] (9)

This representation can be used for word pair clas-
sification tasks, such as lexical semantic relation
detection.

4 Experiment

In this section, we examine how our method im-
proves path-based models on several datasets for
recognizing lexical semantic relations. In this pa-
per, we focus on major noun relations, such as hy-
pernymy, co-hypernymy, and meronymy.

4.1 Dataset

We relied on the datasets used in Shwartz and Da-
gan (2016); K&H+N (Necsulescu et al., 2015).
BLESS (Baroni and Lenci, 2011), EVALution
(Santus et al., 2015), and ROOT09 (Santus et al.,
2016). These datasets were constructed with
knowledge resources (e.g., WordNet, Wikipedia),
crowd-sourcing, or both. We used noun pair in-
stances of these datasets.1 Table 1 displays the
relations in each dataset used in our experiments.
Note that we removed the two relations Entails
and MemberOf with few instances from EVALu-
tion following Shwartz and Dagan (2016). For
data splitting, we used the presplitted train/val/test
sets from Shwartz and Dagan (2016) after remov-
ing all but the noun pairs from each set.

4.2 Corpus and Dependency Parsing

For path-based methods, we used the June
2017 Wikipedia dump as a corpus and extracted
(w1, w2, path) triples of noun pairs using the de-
pendency parser of spaCy2 to construct D. In this
process, w1 and w2 were lemmatized with spaCy.
We only used the dependency paths which oc-

1We focused only noun pairs to shorten the unsupervised
learning time, though this restriction is not necessary for our
methods and the unsupervised learning is still tractable.

2https://spacy.io

datasets instances
instances

with paths proportion
K&H+N 57509 8866 15.4%
BLESS 14558 8775 60.3%

ROOT09 8602 6582 76.5%
EVALution 3240 3199 98.7%

Table 2: The number and proportion of instances
whose dependency path is obtained from each dataset

curred at least five times following the implemen-
tation of Shwartz and Dagan (2016).3

Table 2 displays the number of instances and the
proportion of the instances for which at least one
dependency path was obtained.

4.3 Baseline

We conducted experiments with three neural path-
based methods. The implementation details below
follow those in Shwartz and Dagan (2016). We
implemented all models using Chainer.4

Neural Path-Based Model (NPB). We imple-
mented and trained the neural path-based model
described in Section 2.2. We used the two-layer
LSTM with 60-dimensional hidden units. An in-
put vector was composed of embedding vectors of
the lemma (50 dims), POS (4 dims), dependency
label (5 dims), and dependency direction (1 dim).
Regularization was applied by a dropout on each
of the components embeddings (Iyyer et al., 2015;
Kiperwasser and Goldberg, 2016).

LexNET. We implemented and trained the inte-
grated model LexNET as described in Section 2.2.
The LSTM details are the same as in the NPB
model.

LexNET h. This model, a variant of LexNET,
has an additional hidden layer between the out-
put layer and v(w1,w2) of Equation (5). Because
of this additional hidden layer, this model can take
into account the interaction of the path information

3https://github.com/vered1986/LexNET
4https://chainer.org

1127



LSTM

UNK-lemma/UNK-POS/UNK-dep/UNK-dir

define/VERB/ROOT/- as/ADP/prep/< Y/NOUN/pobj/<X/NOUN/dobj/>

X/NOUN/nsubj/> be/VERB/ROOT/- Y/NOUN/attri/<

LSTM

LSTM

vw1

vw2

vpaths

vp�paths

Output

Our learned model

LexNET

P (path|w1, w2)
<latexit sha1_base64="1lOwjvqBRe6ZenWVOMd+aktFOEo=">AAACmnichVHLSuRAFD3Gd/tqFUHQRWOjOCDNTeMjuhJno7hpH62CSkhiqcG8SKpbNPoD/oALVwqzUD/AD3DjD7jwE2SWDrhx4e10M4OIzi2q6tSpe27V4ZqBY0eS6KlOqW9obGpuaU21tXd0dqW7e9YivxRaomj5jh9umEYkHNsTRWlLR2wEoTBc0xHr5sHPyv16WYSR7Xur8igQ266x59m7tmVIpvR0X7xlupnCaGDI/ZNDXR071PM/TvV0lnLT0+PahJZhoE1qaj6j5iiJvyCLWhT89B22sAMfFkpwIeBBMnZgIOKxCRWEgLltxMyFjOzkXuAUKdaWOEtwhsHsAa97fNqssR6fKzWjRG3xKw7PkJUZDNMjXdMLPdAtPdPbl7XipEblL0e8m1WtCPSus/6V1/+qXN4l9v+pvlGYnP29J4ldaIkXm70FCVNxaVXrl4/PX1ZmlofjEbqi3+zvkp7onh165T/WryWxfIEUN+hTOz6DtXxOpZy6NJ6dnau1qgUDGMIo92MKs5hHAUV+N8YlbnCrDCpzyoKyWE1V6mqaXnwIZfUdpp+YBg==</latexit><latexit sha1_base64="1lOwjvqBRe6ZenWVOMd+aktFOEo=">AAACmnichVHLSuRAFD3Gd/tqFUHQRWOjOCDNTeMjuhJno7hpH62CSkhiqcG8SKpbNPoD/oALVwqzUD/AD3DjD7jwE2SWDrhx4e10M4OIzi2q6tSpe27V4ZqBY0eS6KlOqW9obGpuaU21tXd0dqW7e9YivxRaomj5jh9umEYkHNsTRWlLR2wEoTBc0xHr5sHPyv16WYSR7Xur8igQ266x59m7tmVIpvR0X7xlupnCaGDI/ZNDXR071PM/TvV0lnLT0+PahJZhoE1qaj6j5iiJvyCLWhT89B22sAMfFkpwIeBBMnZgIOKxCRWEgLltxMyFjOzkXuAUKdaWOEtwhsHsAa97fNqssR6fKzWjRG3xKw7PkJUZDNMjXdMLPdAtPdPbl7XipEblL0e8m1WtCPSus/6V1/+qXN4l9v+pvlGYnP29J4ldaIkXm70FCVNxaVXrl4/PX1ZmlofjEbqi3+zvkp7onh165T/WryWxfIEUN+hTOz6DtXxOpZy6NJ6dnau1qgUDGMIo92MKs5hHAUV+N8YlbnCrDCpzyoKyWE1V6mqaXnwIZfUdpp+YBg==</latexit><latexit sha1_base64="1lOwjvqBRe6ZenWVOMd+aktFOEo=">AAACmnichVHLSuRAFD3Gd/tqFUHQRWOjOCDNTeMjuhJno7hpH62CSkhiqcG8SKpbNPoD/oALVwqzUD/AD3DjD7jwE2SWDrhx4e10M4OIzi2q6tSpe27V4ZqBY0eS6KlOqW9obGpuaU21tXd0dqW7e9YivxRaomj5jh9umEYkHNsTRWlLR2wEoTBc0xHr5sHPyv16WYSR7Xur8igQ266x59m7tmVIpvR0X7xlupnCaGDI/ZNDXR071PM/TvV0lnLT0+PahJZhoE1qaj6j5iiJvyCLWhT89B22sAMfFkpwIeBBMnZgIOKxCRWEgLltxMyFjOzkXuAUKdaWOEtwhsHsAa97fNqssR6fKzWjRG3xKw7PkJUZDNMjXdMLPdAtPdPbl7XipEblL0e8m1WtCPSus/6V1/+qXN4l9v+pvlGYnP29J4ldaIkXm70FCVNxaVXrl4/PX1ZmlofjEbqi3+zvkp7onh165T/WryWxfIEUN+hTOz6DtXxOpZy6NJ6dnau1qgUDGMIo92MKs5hHAUV+N8YlbnCrDCpzyoKyWE1V6mqaXnwIZfUdpp+YBg==</latexit><latexit sha1_base64="1lOwjvqBRe6ZenWVOMd+aktFOEo=">AAACmnichVHLSuRAFD3Gd/tqFUHQRWOjOCDNTeMjuhJno7hpH62CSkhiqcG8SKpbNPoD/oALVwqzUD/AD3DjD7jwE2SWDrhx4e10M4OIzi2q6tSpe27V4ZqBY0eS6KlOqW9obGpuaU21tXd0dqW7e9YivxRaomj5jh9umEYkHNsTRWlLR2wEoTBc0xHr5sHPyv16WYSR7Xur8igQ266x59m7tmVIpvR0X7xlupnCaGDI/ZNDXR071PM/TvV0lnLT0+PahJZhoE1qaj6j5iiJvyCLWhT89B22sAMfFkpwIeBBMnZgIOKxCRWEgLltxMyFjOzkXuAUKdaWOEtwhsHsAa97fNqssR6fKzWjRG3xKw7PkJUZDNMjXdMLPdAtPdPbl7XipEblL0e8m1WtCPSus/6V1/+qXN4l9v+pvlGYnP29J4ldaIkXm70FCVNxaVXrl4/PX1ZmlofjEbqi3+zvkp7onh165T/WryWxfIEUN+hTOz6DtXxOpZy6NJ6dnau1qgUDGMIo92MKs5hHAUV+N8YlbnCrDCpzyoKyWE1V6mqaXnwIZfUdpp+YBg==</latexit>

+Aug +Rep
Predicted path

Predicted path

Average
pooling

Figure 2: Illustration of +Aug and +Rep applied to LexNET. +Aug predicts plausible paths from two word em-
beddings, and these paths are fed into the LSTM path encoder. +Rep concatenates the pseudo-path representation
vp−paths(w1,w2) with the penultimate layer of LexNET

and distributional information of two word embed-
dings. The size of the additional hidden layer was
set to 60.

Following Shwartz and Dagan (2016), we opti-
mized each model using Adam (whose learning
rate is 0.001) while tuning the dropout rate dr
among {0.0, 0.2, 0.4} on the validation set. The
minibatch size was set to 100.

We initialized the lemma embeddings of
LSTM and concatenated the word embeddings
of LexNET with the pretrained 50-dimensional
GloVe vector.5 Training was stopped if perfor-
mance on the validation set did not improve for
seven epochs, and the best model for test evalua-
tion was selected based on the score of the valida-
tion set.

4.4 Our Method

We implemented and trained our model of
P (path|w1, w2), described in Section 3.1, as fol-
lows. We used the most frequent 30,000 paths
connecting nouns as the context paths for unsuper-
vised learning. We initialized word embeddings
with the same pretrained GloVe vector as the base-
line models. For unsupervised learning data, we

5https://nlp.stanford.edu/projects/
glove/

extracted (w1, w2, path), whose w1 and w2 are in-
cluded in the vocabulary of the GloVe vector, and
whose path is included in the context paths, from
D. The number of these triples was 217,737,765.

We set the size of h(w1,w2), h̃(w1,w2), and vpath
for context paths to 100. The negative sampling
size n was set to 5. We trained our model for five
epochs using Adam (whose learning rate is 0.001).
The minibatch size was 100. To preserve the dis-
tributional regularity of the pretrained word em-
beddings, we did not update the input word em-
beddings during the unsupervised learning.

With our trained model, we applied the two
methods described in Section 3.2 and 3.3 to the
NPB and LexNET models as follows:

+Aug. We added the most plausible 2k paths to
each paths(w1, w2) as in Section 3.2. We tuned
k ∈ {1, 3, 5} on the validation set.

+Rep. We concatenated vp−paths(w1,w2) in Equa-
tion (9) with the penultimate layer. To focus on
the pure contribution of unsupervised learning, we
did not update this component during supervised
learning.

Figure 2 illustrates +Aug and +Rep applied to
LexNET in the case where the two target words,
w1 and w2, do not co-occur in the corpus.

1128



Models K&H+N BLESS ROOT09 EVALution
NPB 0.495 0.773 0.731 0.463
NPB+Aug 0.897 0.842 0.778 0.489

Table 3: Classification performance of the neural path-based model (NPB) and that with the path data augmentation
(NPB+Aug).

Models K&H+N BLESS ROOT09 EVALution
LexNET 0.969 0.922 0.776 0.539
LexNET h 0.968 0.927 0.810 0.540
LexNET+Aug 0.970 0.927 0.806 0.545
LexNET+Rep 0.970 0.944 0.832 0.565
LexNET+Aug+Rep 0.969 0.942 0.820 0.567

Table 4: Classification performance of the integrated model, LexNET and LexNET h, and those with our methods,
+Aug and +Rep.

5 Result

In this section we examine how our methods im-
proved the baseline models. Following the pre-
vious research (Shwartz and Dagan, 2016), the
performance metrics were the “averaged” F1 of
scikit-learn (Pedregosa et al., 2011), which com-
putes the F1 for each relation, and reports their
average weighted by the number of true instances
for each relation.

5.1 Path-based Model and Path Data
Augmentation

We examined whether or not our path data aug-
mentation method +Aug contributes to the neural
path-based method. The results are displayed in
Table 3.

Applying our path data augmentation method
improved the classification performance on each
dataset. Especially for K&H+N, the large dataset
where the three-fourths of word pairs had no
paths, our method significantly improved the per-
formance. This result shows that our path data
augmentation effectively solves the missing path
problem. Moreover, the model with our method
outperforms the baseline on EVALution, in which
nearly all word pairs co-occurred in the corpus.
This indicates that the predicted paths provide use-
ful information and enhance the path-based classi-
fication. We examine the paths that were predicted
by our model of P (path|w1, w2) in Section 6.1.

5.2 Integrated Model and Our Methods

We investigated how our methods using modeling
P (path|w1, w2) improved the baseline integrated
model, LexNET. Table 4 displays the results.

Our proposed methods, +Aug and +Rep, im-
proved the performance of LexNET on each
dataset.6 Moreover, the best score on each dataset
was achieved by the model to which our methods
were applied. These results show that our meth-
ods are also effective with the integrated models
based on path information and distributional infor-
mation.

The table also shows that LexNET+Rep outper-
forms LexNET h, though the former has fewer pa-
rameters to be tuned during the supervised learn-
ing than the latter. This indicates that the word pair
representations of our model capture information
beyond the interaction of two word embeddings.
We investigate the properties of our word pair rep-
resentation in Section 6.2.

Finally, We found that applying both methods
did not necessarily yield the best performance. A
possible explanation for this is that applying both
methods is redundant, as both +Aug and +Rep de-
pend on the same model of P (path|w1, w2).

6 Analysis

In this section, we investigate the properties of the
predicted dependency paths and word pair repre-
sentations of our model.

6.1 Predicted Dependency Paths

We extracted the word pairs of BLESS without
co-occurring dependency paths and predicted the

6The improvement for K&H+N is smaller than those for
the others. We think this owes to most instances of this
dataset being correctly classified only by distributional in-
formation. This view is supported by Shwartz and Dagan
(2016), in which LexNET hardly outperformed a distribu-
tional method for this dataset.

1129



Word pair Relation Predicted paths
X/NOUN/nsubj/> be/VERB/ROOT/- shooter/NOUN/attr/<
Y/NOUN/compound/<

X = “jacket”, Y = “commodity” hypernym X/NOUN/nsubj/> be/VERB/ROOT/- Y/NOUN/attr/<
manufacture/VERB/acl/<
red/ADJ/amod/< X/NOUN/nsubj/> be/VERB/ROOT/-
Y/NOUN/attr/<
X/NOUN/nsubj/> be/VERB/ROOT/- species/NOUN/attr/<
of/ADP/prep/< Y/NOUN/pobj/< of/ADP/prep/>

X = “goose”, Y = “creature” hypernym X/NOUN/nsubj/> be/VERB/ROOT/- specie/NOUN/attr/<
of/ADP/prep/< Y/NOUN/pobj/< in/ADP/prep/>
X/NOUN/pobj/> of/ADP/ROOT/- bird/NOUN/pobj/<
Y/NOUN/conj/<

X/NOUN/ROOT/- represent/VERB/relcl/<
Y/NOUN/nsubj/<

X = “owl”, Y = “rump” meronym X/NOUN/nsubj/> have/VERB/ROOT/- Y/NOUN/dobj/<
be/VERB/relcl/>
all/DET/det/< X/NOUN/nsubj/> have/VERB/ROOT/-
Y/NOUN/dobj/<
X/NOUN/pobj/> of/ADP/ROOT/- arm/NOUN/pobj/<
Y/NOUN/conj/<

X = “mug”, Y = “plastic” meronym the/DET/det/< X/NOUN/nsubjpass/> make/VERB/ROOT/-
from/ADP/prep/< Y/NOUN/pobj/<
X/NOUN/compound/> gun/NOUN/ROOT/- Y/NOUN/appos/<

X/NOUN/compound/> leaf/NOUN/ROOT/- Y/NOUN/conj/<
X = “carrot”, Y = “beans” co-hyponym X/NOUN/compound/> specie/NOUN/ROOT/- Y/NOUN/conj/<

X/NOUN/dobj/> use/VERB/ROOT/- in/ADP/prep/<
Y/NOUN/pobj/< of/ADP/prep/>
X/NOUN/dobj/> play/VERB/ROOT/- guitar/NOUN/dobj/<
Y/NOUN/conj/<

X = “cello”, Y = “kazoo” co-hyponym X/NOUN/pobj/> for/ADP/ROOT/- piano/NOUN/pobj/<
Y/NOUN/conj<
X/NOUN/pobj/> on/ADP/ROOT/- drum/NOUN/pobj/<
Y/NOUN/conj/<

Table 5: Predicted paths with our model for a word pair of each relation in BLESS.

plausible dependency paths of those pairs with our
model of P (path|w1, w2). The examples are dis-
played in Table 5 at the top three paths. We used
the bold style for the paths that we believe to be in-
dicative or representative for a given relationship.

Our model predicted plausible and indicative
dependency paths for each relation, although the
predicted paths also contain some implausible or
unindicative ones. For hypernymy, our model pre-
dicted variants of the is-a path according to do-
mains, such as X is Y manufactured in the cloth-
ing domain and X is a species of Y in the animal
domain. For (owl, rump), which is a meronymy
pair, the top predicted path was X that Y repre-
sent. This is not plausible for (owl, rump) but
is indicative for meronymy, particularly member-
of relations. Moreover, domain-independent paths
which indicate meronymy, such as all X have
Y, were predicted. For (mug, plastic), one
of the predicted paths, X is made from Y, is
also a domain-independent indicative path for
meronymy. For co-hypernymy, our model pre-
dicted domain-specific paths, which indicate that
two nouns are of the same kind. For exam-
ples, given X leaf and Y and X specie and Y of

(carrot, beans), we can infer that both X and Y
are plants or vegetables. Likewise, given play X,
guitar, and Y of (cello, kazoo), we can infer that
both X and Y are musical instruments. These ex-
amples show that our path data augmentation is ef-
fective for the missing path problem and enhances
path-based models.

6.2 Visualizing Word Pair Representations
We visualized the word pair representations
vp−paths(w1,w2) to examine their specific proper-
ties. In BLESS, every pair was annotated with 17
domain class labels. For each domain, we reduced
the dimensionality of the representations using t-
SNE (Maaten and Hinton, 2008) and plotted the
data points of the hypernyms, co-hyponyms, and
meronyms. We compared our representations with
the concatenation of two word embeddings (pre-
trained 50-dimensional GloVe). The examples are
displayed in Figure 3.

We found that our representations (the top row
in Figure 3) grouped the word pairs according to
their semantic relation in some specific domains
based only on unsupervised learning. This prop-
erty is desirable for the lexical semantic relation
detection task. In contrast to our representations,

1130



Figure 3: Visualization of the our word pair representations vp−paths(w1,w2) (top row) and the concatenation of
two word embeddings (bottom row) using t-SNE in some domains. The two axes of each plot, x and y, are the
reduced dimensions using t-SNE.

the concatenation of word embeddings (the bot-
tom row in Figure 3) has little or no such tendency
in all domains. The data points of the concatena-
tion of word embeddings are scattered or jumbled.
This is because the concatenation of word embed-
dings cannot capture the relational information of
word pairs but only the distributional information
of each word (Levy et al., 2015).

This visualization further shows that our word
pair representations can be used as pseudo-path
representations to alleviate the missing path prob-
lem.

7 Conclusion

In this paper, we proposed the novel meth-
ods with modeling P (path|w1, w2) to solve the
missing path problem. Our neural model of
P (path|w1, w2) can be learned from a corpus in
an unsupervised manner, and can generalize co-
occurrences of word pairs and dependency paths.
We demonstrated that this model can be applied in
the two ways: (1) to augment path data by predict-
ing plausible paths for a given word pair, and (2)
to extract from word pairs useful features captur-
ing co-occurring path information. Finally, our ex-
periments demonstrated that our methods can im-
prove upon the previous models and successfully
solve the missing path problem.

In future work, we will explore unsupervised
learning with a neural path encoder. Our model
bears not only word pair representations but also
dependency path representations as context vec-

tors. Thus, we intend to apply these representa-
tions to various tasks, which path representations
contribute to.

Acknowledgments

This work was supported by JSPS KAKENHI
Grant numbers JP17H01831, JP15K12873.

References
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,

and Chung-chieh Shan. 2012. Entailment above
the word level in distributional semantics. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguis-
tics, pages 23–32. http://www.aclweb.org/
anthology/E12-1004.

Marco Baroni and Alessandro Lenci. 2011. How we
blessed distributional semantic evaluation. In Pro-
ceedings of the GEMS 2011 Workshop on Geomet-
rical Models of Natural Language Semantics. As-
sociation for Computational Linguistics, pages 1–
10. http://www.aclweb.org/anthology/
W11-2501.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning. ACM, pages 160–167.

Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2010. Recognizing textual entailment: Ratio-
nal, evaluation and approaches erratum. Natural
Language Engineering 16(1):105–105. https://
doi.org/10.1017/S1351324909990234.

1131



Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, Mass.

Patrick Hanks. 2009. The impact of corpora on dic-
tionaries. In Paul Baker, editor, Contemporary Cor-
pus Linguistics, Continuum, London, Great Britain,
pages 214–236.

Marti A. Hearst. 1992. Automatic acquisition of
hyponyms from large text corpora. In COLING
1992 Volume 2: The 15th International Conference
on Computational Linguistics. http://www.
aclweb.org/anthology/C92-2082.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daumé III. 2015. Deep unordered com-
position rivals syntactic methods for text classifi-
cation. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguis-
tics and the 7th International Joint Conference on
Natural Language Processing (Volume 1: Long
Papers). Association for Computational Linguis-
tics, pages 1681–1691. https://doi.org/
10.3115/v1/P15-1162.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidi-
rectional lstm feature representations. Transac-
tions of the Association of Computational Linguis-
tics 4:313–327. http://www.aclweb.org/
anthology/Q16-1023.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers).
Association for Computational Linguistics, pages
302–308. https://doi.org/10.3115/v1/
P14-2050.

Omer Levy, Steffen Remus, Chris Biemann, and Ido
Dagan. 2015. Do supervised distributional meth-
ods really learn lexical inference relations? In
Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Association for Computational Linguistics, pages
970–976. https://doi.org/10.3115/v1/
N15-1098.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research 9(Nov):2579–2605.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of the 26th International Con-
ference on Neural Information Processing Systems -
Volume 2. Curran Associates Inc., USA, NIPS’13,
pages 3111–3119. http://dl.acm.org/
citation.cfm?id=2999792.2999959.

Silvia Necsulescu, Sara Mendes, David Jurgens, Núria
Bel, and Roberto Navigli. 2015. Reading between
the lines: Overcoming data sparsity for accurate
classification of lexical relationships. In Proceed-
ings of the Fourth Joint Conference on Lexical and
Computational Semantics. Association for Compu-
tational Linguistics, pages 182–192. https://
doi.org/10.18653/v1/S15-1021.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in python. Journal of Machine
Learning Research 12(Oct):2825–2830.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computa-
tional Linguistics, pages 1532–1543. https://
doi.org/10.3115/v1/D14-1162.

Stephen Roller and Katrin Erk. 2016. Relations such
as hypernymy: Identifying and exploiting hearst pat-
terns in distributional vectors for lexical entailment.
In Proceedings of the 2016 Conference on Em-
pirical Methods in Natural Language Processing.
Association for Computational Linguistics, pages
2163–2172. https://doi.org/10.18653/
v1/D16-1234.

Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet selective: Supervised distributional hy-
pernymy detection. In Proceedings of COLING
2014, the 25th International Conference on Com-
putational Linguistics: Technical Papers. Dublin
City University and Association for Computational
Linguistics, pages 1025–1036. http://www.
aclweb.org/anthology/C14-1097.

Enrico Santus, Alessandro Lenci, Tin-Shing Chiu, Qin
Lu, and Chu-Ren Huang. 2016. Nine features in a
random forest to learn taxonomical semantic rela-
tions. In LREC. Portoroz̆, Slovenia.

Enrico Santus, Frances Yung, Alessandro Lenci, and
Chu-Ren Huang. 2015. Evalution 1.0: an evolving
semantic dataset for training and evaluation of dis-
tributional semantic models. In Proceedings of The
4th Workshop on Linked Data in Linguistics (LDL-
2015). Association for Computational Linguistics,
pages 64–69. https://doi.org/10.18653/
v1/W15-4208.

Rico Sennrich and Barry Haddow. 2016. Linguis-
tic Input Features Improve Neural Machine Trans-
lation. In Proceedings of the First Conference
on Machine Translation. Association for Compu-
tational Linguistics, Berlin, Germany, pages 83–
91. http://www.aclweb.org/anthology/
W16-2209.pdf.

1132



Vered Shwartz and Ido Dagan. 2016. Path-based vs.
distributional information in recognizing lexical se-
mantic relations. In Proceedings of the 5th Work-
shop on Cognitive Aspects of the Lexicon (CogALex-
V), in COLING. Osaka, Japan. http://www.
aclweb.org/anthology/W16-5304.

Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016.
Improving hypernymy detection with an integrated
path-based and distributional method. In Proceed-
ings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics,
Berlin, Germany, pages 2389–2398. https://
doi.org/10.18653/v1/P16-1226.

Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2004. Learning syntactic patterns for auto-
matic hypernym discovery. In Advances in
Neural Information Processing Systems 17,
MIT Press, Cambridge, MA, pages 1297–1304.
http://books.nips.cc/papers/files/
nips17/NIPS2004_0887.pdf.

Ekaterina Vylomova, Laura Rimell, Trevor Cohn, and
Timothy Baldwin. 2016. Take and took, gaggle
and goose, book and read: Evaluating the utility of
vector differences for lexical relation learning. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). Association for Computational Lin-
guistics, pages 1671–1682. https://doi.org/
10.18653/v1/P16-1158.

Julie Weeds, Daoud Clarke, Jeremy Reffin, David
Weir, and Bill Keller. 2014. Learning to distinguish
hypernyms and co-hyponyms. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers.
Dublin City University and Association for Compu-
tational Linguistics, pages 2249–2259. http://
www.aclweb.org/anthology/C14-1212.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Lisbon, Portugal, pages 1785–1794. http:
//aclweb.org/anthology/D15-1206.

Shuo Yang, Lei Zou, Zhongyuan Wang, Jun Yan, and
Ji-Rong Wen. 2017. Efficiently answering techni-
cal questions-a knowledge graph approach. In Pro-
ceedings of the 31st AAAI Conference on Artificial
Intelligence. pages 3111–3118.

1133


