



















































SentiHeros at SemEval-2017 Task 5: An application of Sentiment Analysis on Financial Tweets


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 857–860,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

ACL 2017 Submission 116. Confidential review Copy. DO NOT DISTRIBUTE. 
 
 
 

 

000 
001 
002 
003 
004 
005 
006 
007 
008 
009 
010 
011 
012 
013 
014 
015 
016 
017 
018 
019 
020 
021 
022 
023 
024 
025 
026 
027 
028 
029 
030 
031 
032 
033 
034 
035 
036 
037 
038 
039 
040 
041 
042 
043 
044 
045 
046 
047 
048 
049 
 

050 
051 
052 
053 
054 
055 
056 
057 
058 
059 
060 
061 
062 
063 
064 
065 
066 
067 
068 
069 
070 
071 
072 
073 
074 
075 
076 
077 
078 
079 
080 
081 
082 
083 
084 
085 
086 
087 
088 
089 
090 
091 
092 
093 
094 
095 
096 
097 
098 
099 

 
 
 
 
 

 SentiHeros at SemEval-2017 Task 5: An application of Sentiment 
Analysis on Financial Tweets 

 
Narges Tabari, Armin Seyeditabari, Wlodek Zadrozny 

Narges Tabari: nseyedit@uncc.edu 
Armin Seyeditabari: sseyedi1@uncc.edu 
Wlodek Zadrozny: wzadrozn@uncc.edu 

 
 

Abstract 

Sentiment analysis is the process of identi-
fying the opinion expressed in text. Re-
cently it has been used to study behavioral 
finance, and in particular the effect of 
opinions and emotions on economic or fi-
nancial decisions. SemEval-2017 task 5 
focuses on the financial market as the do-
main for sentiment analysis of text; specif-
ically, task 5, subtask 1 focuses on finan-
cial tweets about stock symbols. In this 
paper, we describe a machine learning 
classifier for binary classification of finan-
cial tweets. We used natural language pro-
cessing techniques and the random forest 
algorithm to train our model, and tuned it 
for the training dataset of Task 5, subtask 
1. Our system achieves the 7th rank on the 
leaderboard of the task.  

1 Introduction 

The recent explosion of textual data creates an 
unprecedented opportunity for investigating peo-
ple’s emotions and opinions, and for understand-
ing human behavior. Although there are several 
methods to do this, sentiment analysis is an espe-
cially effective method of text categorization that 
assigns emotions to text (positive, negative, neu-
tral, etc.). Sentiment analysis methods have been 
used widely on blogs, news, documents and mi-
croblogging platforms such as Twitter. 

Although social media and blogging are pop-
ular and widely used platforms to discuss many 
different topics, they are challenging to analyze. 
This is to large extent due to the specific of vo-
cabulary and syntax, which are dependent on top-
ics, with the same words possibly expressing dif-
ferent sentiments in different contexts. For exam-
ple, a word in a casual context might have positive 
or neutral sentiment (e.g., crush), while the same 
word generally has a negative sentiment in fi-

nance. Therefore, with the absence of general nat-
ural language understanding, context-dependent 
and domain-specific approaches allow us to in-
crease the accuracy of sentiment analysis at a rela-
tively low implementation cost.  

Domain-specific sentiment analysis is being 
used to analyze or investigate various areas in fi-
nance, such as corporate finance and financial 
markets, investment and banking, asset and deriv-
ative pricing. Ultimately, the goal is to understand 
the impact of social media and news on financial 
markets and to predict the future prices of assets 
and stocks. 

The proposed task in SemEval-2017 targets a 
sentiment analysis task, which we should identify 
a range of negative to positive affect on the stock 
of certain companies. The objective of the task 
was to predict the sentiment associated with com-
panies and stock with floating point values in the 
interval from -1 to 1.  

Previous research on textual analysis in a fi-
nancial context has primarily relied on the use of 
bag of words methods, to measure tone (Tetlock, 
2007) (Loughran & McDonald, 2011) which is 
one of the prominent efforts to improve sentiment 
analysis in financial domain, showed that using 
non-financial word lists for sentiment analysis 
will produce misclassifications and misleading re-
sults. To illustrate this, they used the Harvard-IV-
4 list on financial reports, and found that 73.8% of 
the negative word counts were attributable to 
words that were not actually negative in a finan-
cial context. 

Recently, there has been an increasing interest 
towards the use of machine learning techniques to 
get better sentiment result; e.g., naïve Bayesian 
classifier (Saif, He and Alani 2012) with various 
features got the accuracy of 83.90%. Other report-
ed results include the use of support vector ma-
chines (SVMs) with the accuracy of 59.4% 
(O’Hare et al., 2009), and multiple-classifier 

857



ACL 2017 Submission 116. Confidential review Copy. DO NOT DISTRIBUTE. 
 
 
 

 

100 
101 
102 
103 
104 
105 
106 
107 
108 
109 
110 
111 
112 
113 
114 
115 
116 
117 
118 
119 
120 
121 
122 
123 
124 
125 
126 
127 
128 
129 
130 
131 
132 
133 
134 
135 
136 
137 
138 
139 
140 
141 
142 
143 
144 
145 
146 
147 
148 
149 

150 
151 
152 
153 
154 
155 
156 
157 
158 
159 
160 
161 
162 
163 
164 
165 
166 
167 
168 
169 
170 
171 
172 
173 
174 
175 
176 
177 
178 
179 
180 
181 
182 
183 
184 
185 
186 
187 
188 
189 
190 
191 
192 
193 
194 
195 
196 
197 
198 
199 

 

voting systems with the 72% accuracy (Das & 
Chen, 2007).  

In this paper, we describe our approach to 
building a supervised classifier predicting the sen-
timent scores of financial tweets provided by 
SemEval-2017.  The classifier is fed pre-
processed tweets as input and it predicts the bina-
ry labels of the tweets.  Once tweets were pre-
process and features were extracted, various clas-
sification models were applied using Weka tool 
(Hall et al., 2009). This environment contains a 
collection of machine learning-based algorithms 
for data mining tasks, such as, classification, re-
gression, clustering, association rules, and visuali-
zation. We ultimately used Random Forest as our 
classifier as in our various tests it showed the best 
and accuracy in classifying the tweets.  After pre-
dicting the binary labels, we then use the probabil-
ity of the tweets being correctly classified to cre-
ate a range of predictions from -1 to 1 as it was 
requested in the task. 

2 Method  

2.1 Preprocessing the data  
SemEval task 5, subtask 1 provided a training da-
taset with 1800 tweets.  Every tweet had a senti-
ment score between -1 to 1 and it showed its sen-
timent toward the stock symbol that was assigned 
to that tweet. Table 1 describes variables in the 
training dataset we used for analyzing the tweets: 
 

Label Description 

ID Each tweet was assigned a unique 
ID 

Span Part of tweet that was considered 
to carry the sentiment toward the 
company or stock. 

Sentiment Score provided to us with num-
bers between -1 to 1.  

Cashtag Stock symbol that was the target 
of each tweet, e.g. $GE.  

Table 1. Attributes used to create the sentiment 
classification model. 

To prepare the dataset for classification, we 
first converted the sentiment scores to -1, 0 and 1. 
Tweets with sentiments between -0.01 and 0.01 
were labeled as zero, positive sentiments labeled 
as 1 and negative tweets were labeled as -1. We 
then disregarded the tweets with neutral senti-
ment, which left us 1560 tweets to train our mod-

el. Some tweets had multiple Spans, describing 
the sentiment toward the Cashtag. To keep things 
simple, we concatenated the spans of each tweet 
with each other. Then using the Python NLTK1 li-
brary we deleted the punctuations, tokenized the 
spans, and deleted the stop words. 

Since certain stop words in financial context 
can have impact on the sentiment of the tweets, 
we excluded them from the stop word list. Words 
like “up”, and “down” were not removed from 
tweets. We also removed the negations from the 
stop word lists, as we later handle the negations 
on our own when creating the features. 

2.2 Feature Selection Process 
To add features to our training dataset, we used 
the McDonald’s wordlist (Loughran & McDonald, 
2011). This is a list of positive and negative words 
for financial 10-K reports containing the summary 
of the company’s performance.  
    We calculated number of positive or negative 
words in each Span, using the McDonald’s word-
list in the added features. There were some words, 
such as “short” which was not in any wordlist as a 
negative word, yet shorting a stock expresses a 
negative sentiment toward that stock. For this rea-
son, we manually added positive or negative 
words to each list that to our best knowledge carry 
those sentiments. Table 2 shows some of the 
words were added to McDonald’s wordlist: 
 

Word Sentiment 
Profit Positive 
Long Positive 

Short Negative 
Decay Negative 

Table 2. Example of the words added to McDon-
ald’s wordlist. (See full list in Appendix A) 

Adding these words to the wordlist improved 
our results. Then we realized in context of fi-
nance, co-occurrence of some words with each 
other in one tweet changes the sentiment of the 
tweet completely. For example, “short” and “sell” 
are both negative words in context of finance, but 
selling a short contains a positive sentiment in 
stock market context. Another example would be 
the co-occurrence of “go” and “down”, or “pull” 
and “back” in our tweets. In a similar fashion we 
                                                        
1 http://www.nltk.org/ 

858



ACL 2017 Submission 116. Confidential review Copy. DO NOT DISTRIBUTE. 
 
 
 

 

200 
201 
202 
203 
204 
205 
206 
207 
208 
209 
210 
211 
212 
213 
214 
215 
216 
217 
218 
219 
220 
221 
222 
223 
224 
225 
226 
227 
228 
229 
230 
231 
232 
233 
234 
235 
236 
237 
238 
239 
240 
241 
242 
243 
244 
245 
246 
247 
248 
249 

250 
251 
252 
253 
254 
255 
256 
257 
258 
259 
260 
261 
262 
263 
264 
265 
266 
267 
268 
269 
270 
271 
272 
273 
274 
275 
276 
277 
278 
279 
280 
281 
282 
283 
284 
285 
286 
287 
288 
289 
290 
291 
292 
293 
294 
295 
296 
297 
298 
299 

 

also we handled the negations. Once we found 
these patterns, we normalized our data, i.e. we re-
placed the combinations of words in the tweet 
with a single positive or negative label, which we 
treated just as another positive or negative word. 
We then re-counted the number of positive or 
negative words in the tweet and updated our fea-
ture vectors. Table 3 shows examples of patterns 
we found in the tweet to have changed the senti-
ment of the word. The normalization had a benefit 
of increasing the counts of rarely occurring ex  

Word 1 Word 2 Replaced with 
Go  Up  OKAY 
Go  Down NOTOKAY 
Sell Short OKAY 
Pull Back NOTOKAY 

Table 3. Example of the word couples and their 
replacements used to normalize the data (tweets). 

(See full list in Appendix B.) 

2.3 Sentiment Prediction 
Classi-
fier 

Accuracy F-score Preci-
sion 

Recall  

Random 
Forest 

91.26% 86.5% 91.3% 82.2% 

SVM 90.43% 85.4% 88.9% 82.2% 

Logistic 
Regres-
sion 

84.69% 79% 74.3% 84.3% 

Naïve 
Bayes 

83.73% 73.3% 83.3% 65.4% 

Table 4. Results of different Weka classifiers us-
ing 10-fold cross validation and default settings. 

After pre-processing our data and creating all our 
features (Tweet, Positive-Count, Negative-Count), 
we used WEKA to classify our tweets. Our feature 
vectors were the combination of document vectors 
generated by Weka’s StringToWordVector filter, 
followed by the features extracted from the data as 
explained above. Among all the classification 
methods that we used, Random Forest did give us 
the best result with accuracy of 91.2%. Table 4 
shows results from various classifiers using our 
training data. The random forest model in WEKA 
provided both a class prediction and class proba-
bility for each tweet in the training and test set. 
   Since the final float score needed to be be-
tween -1 and 1, for tweets classified as negative 
we made the sentiment score the negative of the 

class probability; for positive classifications, the 
sentiment score was simply the class probability.  

2.4 Other Experiments 
We have done several other experiments first to 
find a promising approach, and to gauge alterna-
tive methods of classification and data prepro-
cessing.  
     In our initial experiment, after pre-processing 
the tweets, we first ran the tweets on WEKA to 
classify using only the feature vector, WEKA’s 
StringToWordVector which is a term document 
matrix. Random forest and Logistic regression 
had the highest accuracy of 83.3% and 85.3% re-
spectively. This experiment shows the    impact 
of our additional features to be around 6%.   
    Before deciding on the final features of the 
model, we tried other types of features. Although 
many of them did not improve the model, we still 
thought they were worth mentioning, with de-
scription of them following: 

Bigrams: In the first experiment, bigrams were 
used. (Kouloumpis, Wilson, & Moore, 2011) 
showed that using unigrams and bigrams are ef-
fective in improving sentiment analysis. (Dave et 
al., 2003) reported that bigrams and trigrams 
worked better than unigrams for polarity classifi-
cation of product reviews. Unfortunately, bigrams 
reduced accuracy of Random Forest and Logistic 
regression to 76.7% and 73.9% respectively. We 
imagine that with a larger data set, bigrams might 
be valuable.  

Feature selection using logistic regression: In 
another experiment, we used logistic regression to 
produce a list of words with the higher odds ratio. 
We then removed other words from tweets, in an 
attempt to amplify the stronger signals. However, 
applying filtered tweets, with various ranges of 
odds ratio did not help with improving the results. 
The best result was when words only with odds 
ratio of [-5, 5] stayed in our training set; this gave 
us the accuracy of 83.5%. 

Using word embedding (GloVe vectors): 
GloVe vectors (Pennington, Socher, & Manning, 
2014) are vector representations of the words. In 
two separate experiments, we used vectors based 
on the Common Crawl (840B tokens, 2.2M vo-
cab, cased, 300 dimensions), and the pre-trained 
word vectors for Twitter (2B tweets, 27B tokens, 
1.2M vocab, 200 dimensions). We represented 
every word in each tweet by a corresponding vec-
tor. We then calculated the tweet vector, using the 
mean of word vectors of the tweet. In this expe-

859



ACL 2017 Submission 116. Confidential review Copy. DO NOT DISTRIBUTE. 
 
 
 

 

300 
301 
302 
303 
304 
305 
306 
307 
308 
309 
310 
311 
312 
313 
314 
315 
316 
317 
318 
319 
320 
321 
322 
323 
324 
325 
326 
327 
328 
329 
330 
331 
332 
333 
334 
335 
336 
337 
338 
339 
340 
341 
342 
343 
344 
345 
346 
347 
348 
349 

350 
351 
352 
353 
354 
355 
356 
357 
358 
359 
360 
361 
362 
363 
364 
365 
366 
367 
368 
369 
370 
371 
372 
373 
374 
375 
376 
377 
378 
379 
380 
381 
382 
383 
384 
385 
386 
387 
388 
389 
390 
391 
392 
393 
394 
395 
396 
397 
398 
399 

 

riment, McDonald’s (Loughran & McDonald, 
2011) positive and negative wordlist again were 
used. That is, we created a positive and negative 
vector using words in those lists. Comparing the 
cosine similarity of tweet vectors with positive 
and negative vector, we classified the tweets. The 
accuracy of this method was 72% and 73.8% for 
tweet and common crawl respectively.  

3 Conclusion 

The purpose of this paper was to create a classifi-
cation method for SemEval-2017 task 5, subtask 
1. In our approach after pre-processing the data, 
negation handling, and feature selection ap-
proaches, we used Weka to classify our data using 
Random Forest algorithm. Our classifier was 
ranked 7th and achieved accuracy of 91.26%. 

In the next step, we think it is important to 
capture more complex linguistic structure, irony, 
idioms, and poorly structured sentences in finan-
cial domain. To this regard, we would like to ap-
ply dependency parser trees for tweets to see if 
that would improve our results; it might also be 
necessary to capture some of the idiomatic con-
structions in this domain.  

 Also, SemEval-2017 training dataset was a 
relatively small dataset, which would prevent us 
from implementing any neural network models 
for prediction. Therefore, we think a step to create 
a better model is to increase the size of training 
dataset. 

References  
Das, S. R., & Chen, M. Y. (2007). Yahoo! for 
Amazon: Sentiment Extraction from Small Talk on 
the Web. Management Science, 53(9), 1375–1388. 
http://doi.org/10.1287/mnsc.1070.0704 

Dave, K.,  Lawrence, S. & Pennock, D. M. (2003). 
Mining the peanut gallery: Opinion extraction and 
semantic classification of product reviews. 
Proceedings of the 12th International Conference on 
World Wide Web, 519–528. 
http://doi.org/10.1145/775152.775226 

Kouloumpis, E., Wilson, T., & Moore, J. (2011). 
Twitter sentiment analysis: The good the bad and the 
omg! Proceedings of the Fifth International AAAI 
Conference on Weblogs and Social Media (ICWSM 
11), 538–541. Retrieved from 
http://www.aaai.org/ocs/index.php/ICWSM/ICWSM1
1/paper/download/2857/3251?iframe=true&width=90
%25&height=90%25 

Loughran, T. I. M., & McDonald, B. (2011). When is 
a Liability not a Liability? Textual Analysis , 

Dictionaries , and 10-Ks.  Journal of Finance, 66(1).  

O’Hare, N., Davy, M., Bermingham, A., Ferguson, P., 
Sheridan, P. P., Gurrin, C., … OHare, N. (2009). 
Topic-Dependent Sentiment Analysis of Financial 
Blogs. International CIKM Workshop on Topic-
Sentiment Analysis for Mass Opinion Measurement, 
9–16. http://doi.org/10.1145/1651461.1651464 

Pennington, J., Socher, R., & Manning, C. D. (2014). 
GloVe: Global Vectors for Word Representation. 
Proceedings of the 2014 Conference on Empirical 
Methods in Natural Language Processing, 1532–
1543. http://doi.org/10.3115/v1/D14-1162 

Saif, H., He, Y., & Alani, H. (2012). Semantic 
sentiment analysis of twitter. Lecture Notes in 
Computer Science (Including Subseries Lecture Notes 
in Artificial Intelligence and Lecture Notes in 
Bioinformatics), 7649 LNCS(PART 1), 508–524. 
http://doi.org/10.1007/978-3-642-35176-1-32 

Tetlock, P. C. (2007). Giving content to investor 
sentiment: The role of media in the stock market. 
Journal of Finance, 62(3), 1139–1168. 
http://doi.org/10.1111/j.1540-6261.2007.01232.x 

Appendix A. Words Added to McDonald’s 
Wordlist. 

Negative words: cult, brutal, fucked, suck, de-
cay, bubble, bounce, bounced, low, lower, 
selloff, disgust, meltdown, downtrend, bullshit, 
shit, breakup, dropping, cry, dumped, torture, 
short, shorts, shorting, fall, falling, sell, selling, 
sells, bearish, slipping, slip, sink, sinked, sinking, 
pain, shortput, nervous, damn, downtrends, cen-
sored, toppy, scam, censor, garbage, risk, steal, 
retreat, retreats, sad, dirt, flush, dump, plunge, 
crush, crushed, crying, unhappy, drop, broke, 
overbought. 

Positive words: epic, highs, recover, profit, long, up-
side, love, interesting, loved, dip, dipping, secure, 
longs, longput, rise, able, buy, buying. 

Appendix B. Full List of Word Couples to 
Detect the Semantic of a Tweet. 

Positive word couples: (go, up), (short, trap), 
(exit, short), (sell, exhaust), (didnt, stop), (short, 
cover), (close, short), (short, break), (cant, risk), 
(not, sell), (dont, fall), (sold, call), (dont, short), 
(exit, bankruptcy), (not, bad), (short, nervous), 
(dont, underestimate), (not, slowdown), (aint, 
bad). 
Negative word couples: (high, down), (lipstick, pig), 
(doesnt, well), (bounce, buy), (isnt, cheap), (fear, sell), 
(cant, down), (not, good), (wont, buy), (dont, trade), 
(buy, back), (didnt, like), (profit, exit), (go, down), 
(not, guaranteed), (not, profitable), (doesn't, upward), 
(not, dip), (pull, back), (not, optimistic).  

860


