



















































Predicting Semantic Relations using Global Graph Properties


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1741–1751
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1741

Predicting Semantic Relations using Global Graph Properties

Yuval Pinter
School of Interactive Computing
Georgia Institute of Technology

Atlanta, GA
uvp@gatech.edu

Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology

Atlanta, GA
jacobe@gatech.edu

Abstract

Semantic graphs, such as WordNet, are re-
sources which curate natural language on two
distinguishable layers. On the local level,
individual relations between synsets (seman-
tic building blocks) such as hypernymy and
meronymy enhance our understanding of the
words used to express their meanings. Glob-
ally, analysis of graph-theoretic properties of
the entire net sheds light on the structure of
human language as a whole. In this pa-
per, we combine global and local properties
of semantic graphs through the framework of
Max-Margin Markov Graph Models (M3GM),
a novel extension of Exponential Random
Graph Model (ERGM) that scales to large
multi-relational graphs. We demonstrate how
such global modeling improves performance
on the local task of predicting semantic rela-
tions between synsets, yielding new state-of-
the-art results on the WN18RR dataset, a chal-
lenging version of WordNet link prediction in
which “easy” reciprocal cases are removed. In
addition, the M3GM model identifies multire-
lational motifs that are characteristic of well-
formed lexical semantic ontologies.

1 Introduction

Semantic graphs, such as WordNet (Fellbaum,
1998), encode the structural qualities of language
as a representation of human knowledge. On
the local level, they describe connections between
specific semantic concepts, or synsets, through in-
dividual edges representing relations such as hy-
pernymy (‘is-a’) or meronymy (‘is-part-of’); on
the global level, they encode emergent regular
properties in the induced relation graphs. Local
properties have been subject to extensive study in
recent years via the task of relation prediction,
where individual edges are found based mostly on

catamaran

boat

(a)

cat

mammalboat

(b)

cat

mammaltabby

(c)

Figure 1: Probable (a) and improbable (b-c) structures
in a hypothetical hypernym graph.

distributional methods that embed synsets and re-
lations into a vector space (e.g. Socher et al., 2013;
Bordes et al., 2013; Toutanova and Chen, 2015;
Neelakantan et al., 2015). In contrast, while the
structural regularity and significance of global as-
pects of semantic graphs is well-attested (Sigman
and Cecchi, 2002), global properties have rarely
been used in prediction settings. In this paper, we
show how global semantic graph features can fa-
cilitate in local tasks such as relation prediction.

To motivate this approach, consider the hypo-
thetical hypernym graph fragments in Figure 1: in
(a), the semantic concept (synset) ‘catamaran’ has



1742

a single hypernym, ‘boat’. This is a typical prop-
erty across a standard hypernym graph. In (b), the
synset ‘cat’ has two hypernyms, an unlikely event.
While a local relation prediction model might mis-
take the relation between ‘cat’ and ‘boat’ to be
plausible, for whatever reason, a high-order graph-
structure-aware model should be able to discard it
based on the knowledge that a synset should not
have more than one hypernym. In (c), an impossi-
ble situation arises: a cycle in the hypernym graph
leads each of the participating synsets to be pre-
dicted by transitivity as its own hypernym, con-
trary to the relation’s definition. However, a purely
local model has no explicit mechanism for reject-
ing such an outcome.

In this paper, we examine the effect of global
graph properties on the link structure via the
WordNet relation prediction task. Our hypothe-
sis is that features extracted from the entire graph
can help constrain local predictions to structurally
sound ones (Guo et al., 2007). Such features are
often manifested as aggregate counts of small sub-
graph structures, known as motifs, such as the
number of nodes with two or more outgoing edges,
or the number of cycles of length 3. Returning to
the example in Figure 1, each of these features will
be affected when graphs (b) and (c) are evaluated,
respectively.

To estimate weights on local and global graph
features, we build on the Exponential Random
Graph Model (ERGM), a log-linear model over
networks utilizing global graph features (Holland
and Leinhardt, 1981). In ERGMs, the likeli-
hood of a graph is computed by exponentiating a
weighted sum of the features, and then normaliz-
ing over all possible graphs. This normalization
term grows exponentially in the number of nodes,
and in general cannot be decomposed into smaller
parts. Approximations are therefore necessary to
fit ERGMs on graphs with even a few dozen nodes,
and the largest known ERGMs scale only to thou-
sands of nodes (Schmid and Desmarais, 2017).
This is insufficient for WordNet, which has an or-
der of 105 nodes.

We extend the ERGM framework in several
ways. First, we replace the maximum likelihood
objective with a margin-based objective, which
compares the observed network against alternative
networks; we call the resulting model the Max-
Margin Markov Graph Model (M3GM), draw-
ing on ideas from structured prediction (Taskar

et al., 2004). The gradient of this loss is ap-
proximated by importance sampling over candi-
date negative edges, using a local relational model
as a proposal distribution. The complexity of each
epoch of estimation is thus linear in the num-
ber of edges, making it possible to scale up to
the 105 nodes in WordNet.1 Second, we address
the multi-relational nature of semantic graphs, by
incorporating a combinatorial set of labeled mo-
tifs. Finally, we link graph-level relational features
with distributional information, by combining the
M3GM with a dyad-level model over word sense
embeddings.

We train M3GM as a re-ranker, which we ap-
ply to a a strong local-feature baseline on the
WN18RR dataset (Dettmers et al., 2018). This
yields absolute improvements of 3-4 points on all
commonly-used metrics. Model inspection re-
veals that M3GM assigns importance to features
from all relations, and captures some interesting
inter-relational properties that lend insight into the
overall structure of WordNet.2

2 Related Work

Relational prediction in semantic graphs. Re-
cent approaches to relation prediction in semantic
graphs generally start by embedding the seman-
tic concepts into a shared space and modeling re-
lations by some operator that induces a score for
an embedding pair input. We use several of these
techniques as base models (Nickel et al., 2011;
Bordes et al., 2013; Yang et al., 2014); detailed
description of these methods is postponed to Sec-
tion 3.2. Socher et al. (2013) generalize over the
approach of Nickel et al. (2011) by using a bilinear
tensor which assigns multiple parameters for each
relation; Shi and Weninger (2017) project the node
embeddings in a translational model similar to that
of Bordes et al. (2013); Dettmers et al. (2018)
apply a convolutional neural network by reshap-
ing synset embeddings to 2-dimensional matrices.
None of these embedding-based approaches incor-
porate structural information; in general, improve-
ments in embedding-based methods are expected
to be complementary to our approach.

1Although in principle the number of edges could grow
quadratically with the number of nodes, Steyvers and Tenen-
baum (2005) show that semantic graphs like WordNet tend to
be very sparse, so that the number of observed edges grows
roughly linearly with the number of nodes.

2Our code is available at http://www.github.
com/yuvalpinter/m3gm.

http://www.github.com/yuvalpinter/m3gm
http://www.github.com/yuvalpinter/m3gm


1743

Some recent works compose single edges into
more intricate motifs, such as Guu et al. (2015),
who define a task of path prediction and com-
pose various functions to solve it. They find
that compositionalized bilinear models perform
best on WordNet. Minervini et al. (2017) train
link-prediction models against an adversary that
produces examples which violate structural con-
straints such as symmetry and transitivity. An-
other line of work builds on local neighborhoods
of relation interactions and automatic detection
of relations from syntactically parsed text (Riedel
et al., 2013; Toutanova et al., 2015). Schlichtkrull
et al. (2017) use Graph Convolutional Networks
to predict relations while considering high-order
neighborhood properties of the nodes in question.
In general, these methods aggregate information
over local neighborhoods, but do not explicitly
model structural motifs.

Our model introduces interaction features be-
tween relations (e.g., hypernyms and meronyms)
for the goal of relation prediction. To our knowl-
edge, this is the first time that relation interac-
tion is explicitly modeled into a relation predic-
tion task. Within the ERGM framework, Lu et al.
(2010) train a limited set of combinatory path fea-
tures for social network link prediction.

Scaling exponential random graph models.
The problem of approximating the denominator
of the ERGM probability has been an active re-
search topic for several decades. Two com-
mon approximation methods exist in the litera-
ture. In Maximum Pseudolikelihood Estima-
tion (MPLE; Strauss and Ikeda, 1990), a graph’s
probability is decomposed into a product of the
probability for each edge, which in turn is com-
puted based on the ERGM feature difference be-
tween the graph excluding the edge and the full
graph. Monte Carlo Maximum Likelihood Esti-
mation (MCMLE; Snijders, 2002) follows a sam-
pling logic, where a large number of graphs is ran-
domly generated from the overall space under the
intuition that the sum of their scores would give
a good approximation for the total score mass.
The probability for the observed graph is then es-
timated following normalization conditioned on
the sampling distribution, and its precision in-
creases as more samples are gathered. Recent
work found that applying a parametric bootstrap
can increase the reliability of MPLE, while re-
taining its superiority in training speed (Schmid

and Desmarais, 2017). Despite this result, we
opted for an MCMLE-based approach for M3GM,
mainly due to the ability to keep the number of
edges constant in each sampled graph. This prop-
erty is important in our setup, since local edge
scores added or removed to the overall graph score
can occasionally dominate the objective function,
giving unintended importance to the overall edge
count.

3 Max-Margin Markov Graph Models

Consider a graph G = (V,E), where V is a set
of vertices and E = {(si, ti)}|E|i=1 is a set of di-
rected edges. The ERGM scoring function defines
a probability over G|V |, the set of all graphs with
|V | nodes. This probability is defined as a log-
linear function,

PERGM(G) ∝ ψERGM(G) = exp
(
θT f(G)

)
, (1)

where f is a feature function, from graphs to a vec-
tor of feature counts. Features are typically counts
of motifs — small subgraph structures — as de-
scribed in the introduction. The vector θ is the
parameter to estimate.

In this section we discuss our adaptation of this
model to the domain of semantic graphs, leverag-
ing their idiosyncratic properties. Semantic graphs
are composed of multiple relation types, which the
feature space needs to accommodate; their nodes
are linguistic constructs (semantic concepts) as-
sociated with complex interpretations, which can
benefit the graph representation through incorpo-
rating their embeddings in Rd into a new scoring
model. We then present our M3GM framework
to perform reliable and efficient parameter estima-
tion on the new model.

3.1 Graph Motifs as Features

Based on common practice in ERGM feature ex-
traction (e.g., Morris et al., 2008), we select the
following graph features as a basis:

• Total edge count;

• Number of cycles of length k, for k ∈ {2, 3};

• Number of nodes with exactly k outgoing (in-
coming) edges, for k ∈ {1, 2, 3};

• Number of nodes with at least k outgoing
(incoming) edges, for k ∈ {1, 2, 3};

• Number of paths of length 2;



1744

• Transitivity: the proportion of length-2 paths
u → v → w where an edge u → w also
exists.

Semantic graphs are multigraphs, where multi-
ple relationships (hypernymy, meronymy, deriva-
tion, etc.) are overlaid atop a common set of
nodes. For each relation r in the relation inven-
tory R, we denote its edge set as Er, and rede-
fine E =

⋃
r∈REr, the union of all labeled edges.

Some relations do not produce a connected graph,
while others may coincide with each other fre-
quently, possibly in regular but intricate patterns:
for example, derivation relations tend to occur be-
tween synsets in the higher, more abstract levels
of the hypernym graph. We represent this com-
plexity by expanding the feature space to include
relation-sensitive combinatory motifs. For each
feature template from the basis list above, we ex-
tract features for all possible combinations of re-
lation types existing in the graph. Depending on
the feature type, these could be relation single-
tons, pairs, or triples; they may be order-sensitive
or order-insensitive. For example:

• A combinatory ‘transitivity’ feature will
be extracted for the proportion of paths

u
hypernym−−−−−−→ v meronym−−−−−−→ w where an edge

u
has part−−−−−→ w also exists.

• A combinatory ‘2-outgoing’ feature will be
extracted for the number of nodes with ex-
actly one derivation and one has part.

The number of features thus scales in O(|R|K)
for a feature basis which involves up to K edges
in any feature, and so our 17 basis features (with
K = 3) generate a combinatory feature set with
roughly 3,000 features for the 11-relation version
of WordNet used in our experiments (see Sec-
tion 4.1).

3.2 Local Score Component
In classical ERGM application domains such as
social media or biological networks, nodes tend
to have little intrinsic distinction, or at least lit-
tle meaningful intrinsic information that may be
extracted prior to applying the model. In seman-
tic graphs, however, the nodes represent synsets,
which are associated with information that is both
valuable to predicting the graph structure and ap-
proximable using unsupervised techniques such as
embedding into a common d-dimensional vector

space based on copious amounts of available data.
We thus modify the traditional scoring function
from eq. (1) to include node-specific information,
by introducing a relation-specific association op-
erator A(r) : V × V → R:

ψERGM+(G) =

= exp

θT f(G) +∑
r∈R

∑
(s,t)∈Er

A(r)(s, t)

 .
(2)

The association operator generalizes various
models from the relation prediction literature:

TransE (Bordes et al., 2013) embeds each rela-
tion r into a vector in the shared space, rep-
resenting a ‘difference’ between sources and
targets, to compute the association score un-
der a translational objective,

A(r)TRANSE(s, t) = −‖es + er − et‖.

BiLin (Nickel et al., 2011) embeds relations into
full-rank matrices, computing the score by a
bilinear multiplication,

A(r)BILIN(s, t) = e
T
s Wret.

DistMult (Yang et al., 2014) is a special case of
BiLin where the relation matrices are diago-
nal, reducing the computation to a ternary dot
product,

A(r)DISTMULT(s, t) = 〈es, er, et〉 =
d∑

i=1

esi eri eti .

3.3 Parameter Estimation
The probabilistic formulation of ERGM requires
the computation of a normalization term that sums
over all possible graphs with a given number of
nodes, GN . The set of such graphs grows at a
rate that is super-exponential in the number of
nodes, making exact computation intractable even
for networks that are orders of magnitude smaller
than semantic graphs like WordNet. One solution
is to approximate probability using a variant of
the Monte Carlo Maximum Likelihood Estimation
(MCMLE) produce,

logP (G) ≈ logψ(G)− log
|G|V ||
M

M∑
G̃∼G|V |

ψ(G̃),

(3)



1745

where M is the number of networks G̃ sampled
from G|V |, the space of all (multirelational) edge
sets on nodes V . Each G̃ is referred to as a nega-
tive sample, and the goal of estimation is to assign
low scores to these samples, in comparison with
the score assigned to the observed network G.

Network samples can be obtained using edge-
wise negative sampling. For each edge s r−→ t in
the training network G, we remove it temporar-
ily and consider T alternative edges, keeping the
source s and relation r constant, and sampling a
target t̃ from a proposal distribution Q. Every
such substitution produces a new graph G̃,

G̃ =G ∪ {s r−→ t̃} \ {s r−→ t}. (4)

Large-margin objective. Rather than approxi-
mating the log probability, as in MCMLE estima-
tion, we propose a margin loss objective: the log
score for each negative sample G̃ should be below
the log score for G by a margin of at least 1. This
motivates the hinge loss,

L(Θ, G̃;G) =
(
1− logψERGM+(G)

+ logψERGM+(G̃)
)
+
, (5)

where (x)+ = max(0, x). Recall that the scoring
function ψERGM+ includes both the local associa-
tion score for the alternative edge and the global
graph features for the resulting graph. However,
it is not necessary to recompute all association
scores; we need only subtract the association score
for the deleted edge s r−→ t, and add the association
score for the sampled edge s r−→ t̃.

The overall loss function is the sum over N =
|E|×T negative samples, {G̃(i)}Ni=1, plus an L2
regularizer on the model parameters,

L(Θ;G) = λ||Θ||22+
N∑
i=1

L(Θ, G̃(i)). (6)

Proposal distribution. The proposal distribu-
tion Q used to sample negative edges is defined
to be proportional to the local association scores
of edges not present in the training graph:

Q(t̃ | s, r,G) ∝

{
0 s

r−→ t̃ ∈ G
A(r)(s, t̃) s r−→ t̃ /∈ G .

(7)

By preferring edges that have high association
scores, the negative sampler helps push the M3GM
parameters away from likely false positives.

4 Relation Prediction

We evaluate M3GM on the relation graph edge pre-
diction task.3 Data for this task consists of a set
of labeled edges, i.e. tuples of the form (s, r, t),
where s and t denote source and target entities, re-
spectively. Given an edge from an evaluation set,
two prediction instances are created by hiding the
source and target side, in turn. The predictor is
then evaluated on its ability to predict the hidden
entity, given the other entity and the relation type.4

4.1 WN18RR Dataset

A popular relation prediction dataset for WordNet
is the subset curated as WN18 (Bordes et al., 2013,
2014), containing 18 relations for about 41,000
synsets extracted from WordNet 3.0. It has been
noted that this dataset suffers from considerable
leakage: edges from reciprocal relations such as
hypernym / hyponym appear in one direction in
the training set and in the opposite direction in
dev / test (Socher et al., 2013; Dettmers et al.,
2018). This allows trivial rule-based baselines to
achieve high performance. To alleviate this con-
cern, Dettmers et al. (2018) released the WN18RR
set, removing seven relations altogether. However,
even this dataset retains four symmetric relation
types: also see, derivationally related form, simi-
lar to, and verb group. These symmetric relations
can be exploited by defaulting to a simple rule-
based predictor.

4.2 Metrics

We report the following metrics, common in rank-
ing tasks and in relation prediction in particular:
MR, the Mean Rank of the desired entity; MRR,
Mean Reciprocal Rank, the main evaluation met-
ric; and H@k, the proportion of Hits (true entities)
found in the top k of the lists, for k ∈ {1, 10}.
Unlike some prior work, we do not type-restrict
the possible relation predictions (so, e.g., a verb
group link may select a noun, and that would count
against the model).

4.3 Systems

We evaluate a single-rule baseline, three associ-
ation models, and two variants of the M3GM re-

3Sometimes referred to as Knowledge Base Completion,
e.g. in Socher et al. (2013).

4We follow prior work in excluding the following from
the ranked lists: the known entity (no self loops); entities
from the training set which fit the instance; other entities in
the evaluation set.



1746

ranker trained on top of the best-performing asso-
ciation baseline.

4.3.1 RULE
We include a single-rule baseline that predicts a
relation between s and t in the evaluation set if
the same relation was encountered between t and
s in the training set. All other models revert to this
baseline for the four symmetric relations.

4.3.2 Association Models
The next group of systems compute local scores
for entity-relation triplets. They all encode enti-
ties into embeddings e. Each of these systems, in
addition to being evaluated as a baseline, is also
used for computing association scores in M3GM,
both in the proposal distribution (see Section 3.3)
and for creating lists to be re-ranked (see below):
TRANSE, BILIN, DISTMULT. For detailed de-
scriptions, see Section 3.2.

4.3.3 Max-Margin Markov Graph Model
The M3GM is applied as a re-ranker. For each
relation and source (target), the top K candidate
targets (sources) are retrieved based on the local
association scores. Each candidate edge is intro-
duced into the graph, and the score ψERGM+(G) is
used to re-rank the top-K list.

We add a variant to this protocol where the
graph score and association score are weighted by
α and 1 − α, repsectively, before being summed.
We tune a separate αr for each relation type, us-
ing the development set’s mean reciprocal rank
(MRR). These hyperparameter values offer further
insight into where the M3GM signal benefits rela-
tion prediction most (see Section 6).

Since we do not apply the model to the symmet-
ric relations (scored by the RULE baseline), they
are excluded from the sampling protocol described
in eq. (5), although their edges do contribute to the
combinatory graph feature vector f .

Our default setting backpropagates loss into
only the graph weight vector θ. We experiment
with a model variant which backpropagates into
the association model and synset embeddings as
well.

4.4 Synset Embeddings
For the association component of our model, we
require embedding representations for WordNet
synsets. While unsupervised word embedding
techniques go a long way in representing word-
forms (Collobert et al., 2011; Mikolov et al., 2013;

Pennington et al., 2014), they are not immediately
applicable to the semantically-precise domain of
synsets. We explore two methods of transforming
pre-trained word embeddings into synset embed-
dings.

Averaging. A straightforward way of using
word embeddings to create synset embeddings is
to collect the words representing the synset as sur-
face form within the WordNet dataset and aver-
age their embeddings (Socher et al., 2013). We
apply this method to pre-trained GloVe embed-
dings (Pennington et al., 2014) and pre-trained
FastText embeddings (Bojanowski et al., 2017),
averaging over the set of all wordforms in all
lemmas for each synset, and performing a case-
insensitive query on the embedding dictionary.
For example, the synset ‘determine.v.01’ lists the
following lemmas: ‘determine’, ‘find’, ‘find out’,
‘ascertain’. Its vector is initialized as

1

5
(edetermine + 2 · efind + eout + eascertain).

AutoExtend retrofitting + Mimick. AutoEx-
tend is a method developed specifically for em-
bedding WordNet synsets (Rothe and Schütze,
2015), in which pre-trained word embeddings are
retrofitted to the tripartite relation graph connect-
ing wordforms, lemmas, and synsets. The re-
sulting synset embeddings occupy the same space
as the word embeddings. However, some Word-
Net senses are not represented in the underlying
set of pre-trained word embeddings.5 To handle
these cases, we trained a character-based model
called MIMICK, which learns to predict embed-
dings for out-of-vocabulary items based on their
spellings (Pinter et al., 2017). We do not modify
the spelling conventions of WordNet synsets be-
fore passing them to Mimick, so e.g. ‘mask.n.02’
(the second synset corresponding to ‘mask’ as a
noun) acts as the input character sequence as is.

Random initialization. In preliminary exper-
iments, we attempted training the association
models using randomly-initialized embeddings.
These proved to be substantially weaker than
distributionally-informed embeddings and we do
not report their performance in the results section.
We view this finding as strong evidence to support
the necessity of a distributional signal in a type-
level semantic setup.

5We use the out-of-the-box vectors supplied in http://
www.cis.lmu.de/˜sascha/AutoExtend.

http://www.cis.lmu.de/~sascha/AutoExtend
http://www.cis.lmu.de/~sascha/AutoExtend


1747

System MR MRR H@10 H@1

RULE 13396 35.26 35.27 35.23

1 DISTMULT 1111 43.29 50.73 39.67
2 BILIN 738 45.36 52.93 41.37
3 TRANSE 2231 46.07 55.65 41.41

4 M3GM 2231 47.94 57.72 43.26
5 M3GMαr 2231 48.30 57.59 43.78

Table 1: Results on development set (all metrics ex-
cept MR are x100). M3GM lines use TRANSE as their
association model. In M3GMαr , the graph component
is tuned post-hoc against the local component per rela-
tion.

4.5 Setup

Following tuning experiments, we train the associ-
ation models on synset embeddings with d = 300,
using a negative log-likelihood loss function over
10 negative samples and iterating over symmetric
relations once every five epochs. We optimize the
loss using AdaGrad with η = 0.01, and perform
early stopping based on the development set mean
reciprocal rank. M3GM is trained in four epochs
using AdaGrad with η = 0.1. We set M3GM’s re-
rank list size K = 100 and, following tuning, the
regularization parameter λ = 0.01 and negative
sample count per edge T = 10. Our models are
all implemented in DyNet (Neubig et al., 2017).

5 Results

Table 1 presents the results on the development
set. Lines 1-3 depict the results for local mod-
els using averaged FastText embedding initializa-
tion, showing that the best performance in terms of
MRR and top-rank hits is achieved by TRANSE.
Mean Rank does not align with the other metrics;
this is an interpretable tradeoff, as both BILIN
and DISTMULT have an inherent preference for
correlated synset embeddings, giving a stronger
fallback for cases where the relation embedding
is completely off, but allowing less freedom for
separating strong cases from correlated false posi-
tives, compared to a translational objective.

Effect of global score. There is a clear advan-
tage to re-ranking the top local candidates using
the score signal from the M3GM model (line 4).
These results are further improved when the graph
score is weighted against the association compo-
nent per relation (line 5). We obtain similar im-
provements when re-ranking the predictions from
DISTMULT and BILIN.

System MR MRR H@10 H@1

RULE 13396 35.26 35.26 35.26

COMPLEX† 5261 44 51 41
CONVE† 5277 46 48 39
CONVKB† 2554 24.8 52.5

TRANSE 2195 46.59 55.55 42.26

M3GMαr 2193 49.83 59.02 45.37

Table 2: Main results on test set. † These models were
not re-implemented, and are reported as in Nguyen
et al. (2018) and in Dettmers et al. (2018).

The M3GM training procedure is not useful in
fine-tuning the association model via backprop-
agation: this degrades the association scores for
true edges in the evaluation set, dragging the re-
ranked results along with them to about a 2-point
drop relative to the untuned variant.

Table 2 shows that our main results transfer onto
the test set, with even a slightly larger margin.
This could be the result of the greater edge density
of the combined training and dev graphs, which
enhance the global coherence of the graph struc-
ture captured by M3GM features. To support this
theory, we tested the M3GM model trained on only
the training set, and its test set performance was
roughly one point worse on all metrics, as com-
pared with the model trained on the training+dev
data.

Synset embedding initialization. We trained
association models initialized on AutoEx-
tend+Mimick vectors (see Section 4.4). Their
performance, inferior to averaged FastText vec-
tors by about 1-2 MRR points on the dev set, is
somewhat at odds with findings from previous
experiments on WordNet (Guu et al., 2015). We
believe the decisive factor in our result is the size
of the training corpus used to create FastText
embeddings, along with the increase in resulting
vocabulary coverage. Out of 124,819 lemma
tokens participating in 41,105 synsets, 118,051
had embeddings available (94.6%; type-level cov-
erage 88.1%). Only 530 synsets (1.3%) finished
this initialization process with no embedding and
were assigned random vectors. AutoExtend, fit
for embeddings from Mikolov et al. (2013) which
were trained on a smaller corpus, offers a weaker
signal: 13,377 synsets (32%) had no vector and
needed Mimick initialization.



1748

Positive

1 s
member meronym−−−−−−−−−−−−→ t

2 s
has part−−−−−→ t

3 s
hypernym−−−−−−→ t derivationally related form−−−−−−−−−−−−−−−−−−→ u

Negative

4 s
hypernym−−−−−−→ t

5 s
hypernym←−−−−−−→ t

6 s
member meronym−−−−−−−−−−−−→ t instance hypernym−−−−−−−−−−−−−→ u

7 s1
has part−−−−−→ t verb group←−−−−−−− s2

Table 3: Select heavyweight features (motifs) follow-
ing best dev set training using M3GM. Circled nodes
count towards the motif.

6 Graph Analysis

As a consequence of the empirical experiment, we
aim to find out what M3GM has learned about
WordNet. Table 3 presents a sample of top-
weighted motifs. Lines 1 and 2 demonstrate that
the model prefers a broad scattering of targets
for the member meronym and has part relations6,
which are flat and top-downwards hierarchical, re-
spectively, while line 4 shows that a multitude of
unique hypernyms is undesired, as expected from
a bottom-upwards hierarchical relation. Line 5 en-
forces the asymmetry of the hypernym relation.

Lines 3, 6, and 7 hint at deeper interactions
between the different relation types. Line 3
shows that the model assigns positive weights
to hypernyms which have derivationally-related
forms, suggesting that the derivational equivalence
classes in the graph tend to exist in the higher,
more abstract levels of the hypernym hierarchy,
as noted in Section 3.1. Line 6 captures a se-
mantic conflict: synsets located in the lower, spe-
cific levels of the graph can be specified either
as instances of abstract concepts7, or as members
of less specific concrete classes, but not as both.
Line 7 may have captured a nodal property – since
part of is a relation which holds between nouns,
and verb group holds between verbs, this negative
weight assignment may be the manifestation of a
part-of-speech uniqueness constraint. In addition,
in features 3 and 7 we see the importance of sym-
metric relations (here derivationally related form

6Example edges: ‘America’ → ‘American’, ‘face’ →
‘mouth’, respectively.

7Example instance hypernym edge: ‘Rome’→ ‘national
capital’.

Source Relation Correct Outranking
target local target(s)

indian lettuce hypernym herb garden lettuce
austria has part vienna germany,

hungary, france,
european union

Table 4: Successful M3GM re-ranking examples.

Relation r αr Relation r αr

mem. of domain usage 0.78 hypernym 0.64
mem. of domain region 0.77 domain topic of 0.38
member meronym 0.67 has part 0.33
instance hypernym 0.65

Table 5: Graph score weights found for relations on
the dev set. Zero means graph score is not considered
at all for this relation, one means only it is considered.

and verb group, respectively), which manage to be
represented in the graph model despite not being
directly trained on.

Table 4 presents examples of relation targets
successfully re-ranked thanks to these features.
The first false connection created a new unique
hypernym, ‘garden lettuce’, downgraded by the
graph score through incrementing the count of
negatively-weighted feature 4. In the second case,
‘vienna’ was brought from rank 10 to rank 1
since it incremented the count for the positively-
weighted feature 2, whereas all targets ranked
above it by the local model were already has part-
s, mostly of ‘europe’.

The αr values weighing the importance of
M3GM scores in the overall function, found per
relation through grid search over the develop-
ment set, are presented in Table 5. It appears
that for all but two relations, the best-performing
model preferred the signal from the graph features
to that from the association model (αr > 0.5).
Based on the surface properties of the different
relation graphs, the decisive factor seems to be
that synset domain topic of and has part pertain
mostly to very common concepts, offering good
local signal from the synset embeddings, whereas
the rest include many long-tail, low-frequency
synsets that require help from global features to
detect regularity.



1749

7 Conclusion

This paper presents a novel method for reasoning
about semantic graphs like WordNet, combining
the distributional coherence between individual
entity pairs with the structural coherence of net-
work motifs. Applied as a re-ranker, this method
substantially improves performance on link pre-
diction. Our analysis of results from Table 3, lines
6 and 7, suggests that adding graph motifs which
qualify their adjacent nodes in terms of syntactic
function or semantic category may prove useful.

From a broader perspective, M3GM can do more
as a probabilistic model than predict individual
edges. For example, consider the problem of link-
ing a new entity into a semantic graph, given only
the vector embedding. This task involves adding
multiple edges simultaneously, while maintaining
structural coherence. Our model is capable of
scoring bundles of new edges, and in future work,
we plan to explore the possibility of combining
M3GM with a search algorithm, to automatically
extend existing knowledge graphs by linking in
one or more new entities.

We also plan to explore multilingual applica-
tions. To some extent, the structural parameters
estimated by M3GM are not specific to English:
for example, hypernymy cannot be symmetric in
any language. If the structural parameters esti-
mated from English WordNet are transferable to
other languages, then the combination of M3GM
and multilingual word embeddings could facilitate
the creation and extension of large-scale semantic
resources across many languages (Fellbaum and
Vossen, 2012; Bond and Foster, 2013; Lafourcade,
2007).

Acknowledgments

We would like to thank the anonymous review-
ers for their helpful comments. We discussed fast
motif-counting algorithms with Polo Chau and
Oded Green, and received early feedback from
Jordan Boyd-Graber, Erica Briscoe, Martin Hy-
att, Bryan Leslie Lee, Martha Palmer, and Oren
Tsur. This research was funded by the Defense
Threat Research Agency under award HDTRA1-
15-1-0019.

References
Piotr Bojanowski, Edouard Grave, Armand Joulin, and

Tomas Mikolov. 2017. Enriching word vectors with

subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Francis Bond and Ryan Foster. 2013. Linking and ex-
tending an open multilingual wordnet. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 1352–1362.

Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2014. A semantic matching energy
function for learning with multi-relational data. Ma-
chine Learning, 94(2):233–259.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger,
editors, Advances in Neural Information Processing
Systems 26, pages 2787–2795. Curran Associates,
Inc.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.

Tim Dettmers, Minervini Pasquale, Stenetorp Pon-
tus, and Sebastian Riedel. 2018. Convolutional 2d
knowledge graph embeddings. In Proceedings of
the 32th AAAI Conference on Artificial Intelligence.

Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.

Christiane Fellbaum and Piek Vossen. 2012. Chal-
lenges for a multilingual wordnet. Language Re-
sources and Evaluation, 46(2):313–326.

Fan Guo, Steve Hanneke, Wenjie Fu, and Eric P Xing.
2007. Recovering temporally rewiring networks:
A model-based approach. In Proceedings of the
24th international conference on Machine learning,
pages 321–328. ACM.

Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing knowledge graphs in vector space. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
318–327, Lisbon, Portugal. Association for Compu-
tational Linguistics.

Paul W. Holland and Samuel Leinhardt. 1981. An ex-
ponential family of probability distributions for di-
rected graphs. Journal of the american Statistical
association, 76(373):33–50.

Mathieu Lafourcade. 2007. Making people play for
lexical acquisition with the jeuxdemots prototype.
In SNLP’07: 7th international symposium on nat-
ural language processing, page 7.

https://doi.org/10.1007/s10994-013-5363-6
https://doi.org/10.1007/s10994-013-5363-6
http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf
http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf
https://arxiv.org/abs/1707.01476
https://arxiv.org/abs/1707.01476
http://aclweb.org/anthology/D15-1038


1750

Zhengdong Lu, Berkant Savas, Wei Tang, and Inder-
jit S. Dhillon. 2010. Supervised link prediction us-
ing multiple sources. In 2010 IEEE International
Conference on Data Mining, pages 923–928.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of Interna-
tional Conference on Learning Representations.

Pasquale Minervini, Thomas Demeester, Tim
Rocktäschel, and Sebastian Riedel. 2017. Ad-
versarial sets for regularising neural link predictors.
arXiv preprint arXiv:1707.07596.

Martina Morris, Mark S. Handcock, and David R.
Hunter. 2008. Specification of exponential-family
random graph models: terms and computational as-
pects. Journal of statistical software, 24(4):1548.

Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015. Compositional vector space models
for knowledge base inference. In 2015 aaai spring
symposium series.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. Dynet:
The dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980.

Dai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc
Nguyen, and Dinh Phung. 2018. A novel embed-
ding model for knowledge base completion based on
convolutional neural network. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 2 (Short Pa-
pers), pages 327–333, New Orleans, Louisiana. As-
sociation for Computational Linguistics.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In ICML, vol-
ume 11, pages 809–816.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1532–1543.

Yuval Pinter, Robert Guthrie, and Jacob Eisenstein.
2017. Mimicking word embeddings using subword
rnns. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
pages 102–112.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
74–84, Atlanta, Georgia. Association for Computa-
tional Linguistics.

Sascha Rothe and Hinrich Schütze. 2015. AutoEx-
tend: Extending Word Embeddings to Embeddings
for Synsets and Lexemes. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1793–1803, Beijing,
China. Association for Computational Linguistics.

Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem,
Rianne van den Berg, Ivan Titov, and Max Welling.
2017. Modeling relational data with graph convolu-
tional networks. stat, 1050:17.

Christian S. Schmid and Bruce A. Desmarais. 2017.
Exponential random graph models with big net-
works: Maximum pseudolikelihood estimation
and the parametric bootstrap. arXiv preprint
arXiv:1708.02598.

Baoxu Shi and Tim Weninger. 2017. Proje: Embed-
ding projection for knowledge graph completion. In
AAAI, volume 17, pages 1236–1242.

Mariano Sigman and Guillermo A. Cecchi. 2002.
Global organization of the wordnet lexicon. Pro-
ceedings of the National Academy of Sciences,
99(3):1742–1747.

Tom AB Snijders. 2002. Markov chain monte carlo es-
timation of exponential random graph models. Jour-
nal of Social Structure, 3(2):1–40.

Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning with neural
tensor networks for knowledge base completion. In
Advances in Neural Information Processing Systems
26.

Mark Steyvers and Joshua B. Tenenbaum. 2005. The
large-scale structure of semantic networks: Statisti-
cal analyses and a model of semantic growth. Cog-
nitive science, 29(1):41–78.

David Strauss and Michael Ikeda. 1990. Pseudolikeli-
hood estimation for social networks. Journal of the
American Statistical Association, 85(409):204–212.

Ben Taskar, Carlos Guestrin, and Daphne Koller. 2004.
Max-margin markov networks. In S. Thrun, L. K.
Saul, and B. Schölkopf, editors, Advances in Neu-
ral Information Processing Systems 16, pages 25–
32. MIT Press.

Kristina Toutanova and Danqi Chen. 2015. Observed
versus latent features for knowledge base and text
inference. In Proceedings of the 3rd Workshop on

https://doi.org/10.1109/ICDM.2010.112
https://doi.org/10.1109/ICDM.2010.112
http://www.aclweb.org/anthology/N18-2053
http://www.aclweb.org/anthology/N18-2053
http://www.aclweb.org/anthology/N18-2053
http://www.aclweb.org/anthology/N13-1008
http://www.aclweb.org/anthology/N13-1008
http://www.aclweb.org/anthology-new/P/P15/P15-1173.bib
http://www.aclweb.org/anthology-new/P/P15/P15-1173.bib
http://www.aclweb.org/anthology-new/P/P15/P15-1173.bib
http://papers.nips.cc/paper/2397-max-margin-markov-networks.pdf
http://www.aclweb.org/anthology/W15-4007
http://www.aclweb.org/anthology/W15-4007
http://www.aclweb.org/anthology/W15-4007


1751

Continuous Vector Space Models and their Compo-
sitionality, pages 57–66, Beijing, China. Associa-
tion for Computational Linguistics.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of text
and knowledge bases. In EMNLP, volume 15, pages
1499–1509.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2014. Embedding entities and
relations for learning and inference in knowledge
bases. arXiv preprint arXiv:1412.6575.


