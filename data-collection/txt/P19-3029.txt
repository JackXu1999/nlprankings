



















































Flambé: A Customizable Framework for Machine Learning Experiments


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 181–188
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

181

Flambé: A Customizable Framework for
Machine Learning Experiments

Jeremy Wohlwend
ASAPP Inc.

jeremy@asapp.com

Nicholas Matthews
ASAPP Inc.

nick@asapp.com

Ivan Itzcovich
ASAPP Inc.

ivan@asapp.com

Abstract

Flambé is a machine learning experimenta-
tion framework built to accelerate the entire
research life cycle. Flambé’s main objective
is to provide a unified interface for prototyp-
ing models, running experiments containing
complex pipelines, monitoring those experi-
ments in real-time, reporting results, and de-
ploying a final model for inference. Flambé
achieves both flexibility and simplicity by al-
lowing users to write custom code but instantly
include that code as a component in a larger
system which is represented by a concise con-
figuration file format. We demonstrate the ap-
plication of the framework through a cutting-
edge multistage use case: fine-tuning and dis-
tillation of a state of the art pretrained lan-
guage model used for text classification. 1

1 Introduction

Scientists and engineers in the machine learning
community dedicate many hours and resouces to-
wards preprocessing data, iterating on model ar-
chitectures, tuning hyperparameters, aggregating
results and ultimately deploying their most per-
formant model. While frameworks like PyTorch
(Paszke et al., 2017) and Tensorflow (et al., 2016)
abstract away the details of operations like back-
progpagation and make building models possible
in a few lines of code, they do not explicitly aim
to solve these other parts of the research cycle.

The explosion of available resources in the ma-
chine learning community (Dean et al., 2018) has
included many tools that address one or more
of these other phases of research, but these iso-
lated tools do not always work harmoniously with
one another, trading off customizability to provide
high-level interfaces. Understanding that machine

1The code and documentation can be found at
https://flambe.ai

learning research particularly in the field of Natu-
ral Language Processing might require innovation
at any level of abstraction and across any stage
in the research process, we’ve built Flambé to
include standardized implementations of model-
ing components, hyperparameter optimization and
distributed execution that can all be effortlessly re-
placed with custom user-developed code.

By facilitating customization and iteration on a
particular data pipeline and model architecture, we
aim for Flambé users to spend the majority of their
time doing research, not re-implementing tools for
training, tuning, reporting and deploying.

Flambé’s contributions are:

1. Modular machine learning components to de-
velop replicable, state of the art research
results. This includes: neural network
components (pretrained or not), benchmark
datasets, and standardized training and eval-
uation modules.

2. A configuration format that natively enables
searching over hyperparameters and running
remote multistage experiments at scale.

3. Smooth reporting and exporting, to facilitate
sharing models and results with collaborators
and the larger community.

4. An open source framework for both the aca-
demic community and teams in industry.

We demonstrate the application of our frame-
work through a cutting-edge use case, namely
knowledge distillation of a state of the art language
model, the BERT model (Devlin et al., 2019), on
a downstream text classification task.

2 Related work

Many different tools are attempting to tackle the
various challenges of building machine learning



182

systems from different angles. Frameworks like
PyTorch and Tensorflow (et al., 2016) provide
the building blocks of models as simple modules
e.g. various linear and recurrent layers, losses,
optimizers etc. Many model implementations
have been built on top of these modules, with
some proposing new standardizations of specific
architectures like sequence-to-sequence modeling
(et al, 2019).

Libraries such as Keras (Chollet et al., 2015)
offer a high-level API for building and training
models. Others including AllenNLP (Gardner
et al., 2018), FastAI (Howard et al., 2018) and
Texar (et al, 2018) focus on some specific domains
or tasks like reading comprehension or text style
transfer. These types of frameworks tend to focus
on training a single model at a time, but many re-
search experiments consist of complex multistage
pipelines, with hyperparameter tuning and dis-
tributed computation required at each stage. With
Flambé, users can write their custom code inde-
pendent from these concerns, and then easily start
using algorithms like Hyperband (Li et al., 2016)
and Bayesian Optimization (Bergstra et al., 2013),
link components across stages, and run everything
on a cluster without any modifications.

MLFlow (Zaharia et al., 2018) focuses on ex-
periment tracking, metric reporting, and contains
powerful features aimed at production deploy-
ment. However, it does not have a natural way to
run hyperparameter tuning, or advanced trial sam-
pling and scheduling.

Ray (Moritz et al., 2017) implements infrastruc-
ture for distributing computational tasks on a clus-
ter, and it also provides a higher level extension,
Tune (Liaw et al., 2018), that handles hyperparam-
eter optimization.

Flambé leverages and builds upon existing
tools, connecting the dots between frameworks
like PyTorch and Ray, and providing a smooth in-
tegration between them with a powerful layer of
abstraction on top. By not trying to re-implement
solved problems like back-propagation and dis-
tributed task execution, we can focus our attention
on usability and efficiency.

3 The Flambé Framework

Flambé executes experiments which are com-
posed of a pipeline of modeling and processing
stages (Subsection A), extensions that import user-
supplied code (Subsection B), links to existing

Figure 1: Example YAML config for text classification
on the TREC dataset. The highlighted and labeled sec-
tions refer to the subsections in 3.1. There are a number
of different objects that could be used in any place of
this config e.g. the optimizer could be !torch.SGD
and the scheduler tune.HyperOpt (Bayesian opti-
mization). Note the pipeline stage names “stage0”, etc.
are arbitrary.

components (Subsection C), and tunable hyper-
parameters (Subsections D, E, F). All of these
features are demonstrated in the Experiment
shown in Figure 1, which defines a simple text
classification task consisting of training an LSTM
(Hochreiter and Schmidhuber, 1997) on the TREC
dataset (Li and Roth, 2002).



183

Each tag in the YAML (Oren Ben-Kiki, 2009)
config (anything beginning with ‘!’) corresponds
to a python object that will be initialized with the
keyword arguments following the tag. These tags
are not hardcoded into the system, and users can
use their own classes in the config just as easily
as the ones we’ve already built. After we explain
all the aforementioned features, we introduce how
Flambé saves object state, enables simple metric
logging, and deploys models for production.

3.1 Walkthrough

In this section we present an example driven ex-
planation of the core features as they’re used in
Figure 1.

A. Pipeline

The most important section of the YAML file is
the pipeline section. This section contains a
series of stages which each implement a step
method. The example shown in Figure 1 contains
3 stages: (1) dataset loading and processing, (2)
training of each model variant, and (3) evaluating
the best model from stage1.

A stage in the pipeline can be any Python ob-
ject. Users need only add a parent class to their
class definition if they intend to use it in the YAML
config. All objects will receive the keyword argu-
ments given inline in the configuration file. For
example, in Figure 1 the TextClassifier ob-
ject receives an embedding, encoder and decoder,
matching its definition in code:

All subclasses of Flambé classes like Model
are automatically registered with YAML

B. Extending Flambé with Custom Code

Flambé is flexible because of its ability to use cus-
tom Flambé objects in the experiment configura-
tion file. By default, only classes in the main
Flambé library and PyTorch can be referenced, but
by using the extensions feature users can in-
clude their own classes and functions, from either
local or remote source code repositories.

To create an extension, users need only organize
their code into one or more pip-installable pack-
ages. After declaring the extensions and including
them at the top of the config file, they are useable
anywhere in the YAML configuration file.

In the example, the TRECDataset object is
defined in an external extension hosted in GitHub.
By adding its URL at the top of the YAML con-
figuration file, the cl.TrecDataset object and
any other Flambé class can be used. If you can-
not or do not want to inherit from one of our
pipeline classes (Model, Trainer, etc.) you can
inherit from flambe.nn.Module which will
supply the minimum needed functionality to sup-
port use in the config file and automatic hierarchi-
cal serialization (See later sections).

C. Referencing Earlier Objects
A core feature of Flambé is the ability to connect
(or “link”) different components with the !@ no-
tation, a custom YAML tag we’ve implemented.
Any value anywhere in the pipeline can be a ref-
erence to an earlier value that has already been
defined. Each link consists of the identifier of
a stage, e.g. “stage1” which in this case is the
Trainer object, followed by the rest of the ob-
ject attributes. In the highlighted example (C), the
link stage0.train means that the data key-
word argument for BaseSampler should point
to the train attribute of the TCProcessor.

D. Hyperparameter Search
In addition to referencing other values via links,
the value for any parameter in the config can be
replaced with either a list of possible options to
try (for grid search) or a distribution for sampling
possible options. Grid search options are defined
with the !g tag followed by the list of candidate
values; Flambé will automatically duplicate the
stage, choosing a single value for each variant of
the stage. In the example we use this mechanism
to search over different numbers of layers.

If distributions are used instead of lists of can-
didate values, Flambé performs a simple random
search. Users can also specify a search field that
maps stage names to the hyperparameter search
algorithm, e.g. Bayesian optimization, which
changes the distributions used to sample the tun-
able hyperparameters.

When Links reference stages with multiple
variants, the stage containing the link is duplicated
as many times as there are variants.



184

E. Trial Scheduling
Regardless of the strategy used to choose hyper-
parameters, some variants will start to clearly out-
perform others and scheduling algorithms like Hy-
perband (Li et al., 2016) use that information to
intelligently allocate resources to the variants that
are performing the best. Flambé surfaces an in-
terface to these schedulers in the same way as
the search algorithms: “schedulers” maps pipeline
stage names to the desired scheduling algorithms,
as shown in the example configuration.

F. Selecting the Best Variants
After trying many different combinations of hy-
perparameters, only the best will propagate to the
next stages if the reduce operation is used. For ex-
ample, with reduce mapping stage1 to 1 in the
example, only the single best configuration, with
the optimal number of layers, will be evaluated in
the final stage. In order to use this feature, the
stages need to supply a metric fn that can be used
to rank the variants.

3.2 Hierarchical Serialization
While PyTorch already provides a clear and robust
saving mechanism, we augment this functionality
with a generic serialization protocol for all objects
that includes opt-in versioning and a directory
based file format that anyone can inspect. Rather
than dumping all of the model weights and other
state into a single file, the directory based struc-
ture mirrors the object hierarchy and enables the
possibility of referencing a specific component.
Rather than having to load the save file to inspect
the contents, it can be navigated like any other di-
rectory. By default, only what PyTorch normally
saves is included in the save file; users can add ad-
ditional state by overriding custom state and
load custom state

3.3 Using a cluster
To run experiments on a cluster, an additional
piece of YAML is needed to define the remote
manager. As shown below in Figure 3 one can
indicate the instance types and a timeout flag for
both the orchestrator and the factories. We use
this feature to keep our experiment tracking web-
site running on the orchestrator once an experi-
ment is over, but also to keep factories alive when
rapidly experimenting or debugging. The orches-
trator will communicate with workers in the clus-
ter via Ray and Tune to execute and checkpoint

Figure 2: Save file directory structure for the
Experiment in Figure 1

.

progress at each step. If an experiment fails or
is interrupted, it can be quickly resumed with an
additional flag resume: True. Crucially, this
remote functionality allows to distribute the exe-
cution of the variants across a cluster of machines
by only adding a few lines to the configuration.

Figure 3: Example remote config for AWS cluster.

3.4 Deploying
Typically after experimentation, machine learning
projects require packaging a model together with
some preprocessing and post-processing functions
into a single inference-ready interface, e.g. a
text classifier that actually takes raw string(s) as
input. Flambé facilitates this use-case with the
Exporter object, wherein users can define a
new version of the model from the best variants
tested, and with the right interface for later use.

3.5 Library usage
In addition to using the Flambé framework via
YAML configuration files, users can also use the
individual objects (e.g. the Trainer, or RNNEn-
coder classes) in any python script. This usage
may be important for users that already have a pro-
duction codebase (including training scripts) writ-
ten purely in Python. In a future version of the



185

software we plan to support creating full exper-
iments and deploying models via code (instead
of YAML) to enable dynamic experiment creation
and model exporting.

3.6 Logging

Flambé provides full integration with Python’s
logging module and Tensorboard ((et al.,
2016)). Users are able to visualize their results
by simply including log statements in their code
(See Figure 4).

Figure 4: Example log statement. Logging can be done
anywhere inside a custom object.

All variants will appear under the same plot for
easy analysis (see Figure 5).

4 Case study: BERT Distillation

In this section we showcase Flambé’s ability to
transform a pre-existing codebase with no pre-
existing support for hyperparameter optimization
into a complex multi-stage pipeline with a YAML
config less than 80 lines long. Furthermore, We
were able to find the optimal set of parameters in
roughly half the time otherwise needed by adding
Hyperband scheduling (Li et al., 2016), and run-
ning the experiment over a large cluster.

BERT (Devlin et al., 2019) is a popular model
which performs competitively across several NLP
tasks by leveraging language model pre-training
over a very large corpus. Two crucial issues with
the BERT model are the size of the model, and its
inference speed, which generally inhibits its use
in production environments. To address this issue,
recent efforts have shown that most of BERT’s
performance on a downstream task can be con-
served, while dramatically reducing its memory
footprint (Chia et al., 2018).

In this experiment, we fine-tune the BERT
model on two standard text classification bench-
marks: TREC (Li and Roth, 2002) and Sentiment
Treebank (Socher et al., 2013). We then apply
knowledge distillation to reduce the BERT model
to a simple 4 layer, 256 units, SRU network (Lei
et al., 2018). This is a typical multistage experi-
ment with preprossessing, fine tuning, and distil-
lation stages. All of this can be expressed in a sin-

Model TREC SST2 # Parameters
SRU 94.8 86.2 ≈ 5M
BERT 96.8 91.0 ≈ 110M
DISTILLED 95.5 87.8 ≈ 5M

Table 1: Accuracy on benchmark text classification
datasets: TREC and SST2 (Binary Sentiment Tree-
bank). Distilling BERT improves the accuracy of the
base SRU model, while reducing the number of param-
eters by more than 95%. All models were trained or
fine-tuned using Flambé. The SRU and DISTILLED
model have the same architecture, the SRU model be-
ing trained from scratch and the DISTILLED model
benefiting from the BERT model’s improved perfor-
mance.

Figure 5: Some runs are pruned early by the Hyberband
scheduling algorithm. The x-axis is training steps, and
the y-axis is accuracy.

gle, concise configuration. Results are provided
in Table 1. The full configuration, containing all
three stages and their respective hyperparameters,
is provided as supplementary material.

Not only can Flambé express the above experi-
ment in a concise configuration, but using a state
of the art trial scheduling algorithm such as Hyper-
band (Li et al., 2016) can be accomplished with a
single additional line in the configuration. Figure
5 shows Hyperband allocating more training steps
to the best-performing models. In this example,
defining grid searches, running over a cluster, and
using a scheduling algorithm on an existing code-
base required little to no effort.

5 Future work

Flambé aims to integrate with research and engi-
neering workflows through its focus on usability,
modularity and reproducibility. We continue to



186

pursue this goal by developing a large collection
of machine learning components including state
of the art models, benchmark datasets, and novel
training strategies. Real, working, and repro-
ducible experiment configurations will showcase
these components alongside their performance in
task-based leaderboards. In parallel, we will con-
tinue to develop user-friendly abstractions like the
ability to auto-scale clusters based on the size
of each stage in the pipeline, and to monitor or
even alter experiment execution in real-time from
a website.

References
Martin Abadi et al. 2016. Tensorflow: A system for

large-scale machine learning. In 12th USENIX Sym-
posium on Operating Systems Design and Imple-
mentation (OSDI 16), pages 265–283.

Myle Ott et al. 2019. fairseq: A fast, extensible toolkit
for sequence modeling. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics (Demon-
strations), pages 48–53, Minneapolis, Minnesota.
Association for Computational Linguistics.

Zhiting Hu et al. 2018. Texar: A modularized,
versatile, and extensible toolbox for text genera-
tion. In Proceedings of Workshop for NLP Open
Source Software (NLP-OSS), pages 13–22, Mel-
bourne, Australia. Association for Computational
Linguistics.

James Bergstra, Dan Yamins, and David D Cox. 2013.
Hyperopt: A python library for optimizing the hy-
perparameters of machine learning algorithms. In
Proceedings of the 12th Python in science confer-
ence, pages 13–20. Citeseer.

Yew Ken Chia, Sam Witteveen, and Martin Andrews.
2018. Transformer to cnn: Label-scarce distillation
for efficient text classification.

François Chollet et al. 2015. Keras. https://
keras.io.

Jeff Dean, David Patterson, and Cliff Young. 2018. A
new golden age in computer architecture: Empow-
ering the machine-learning revolution. IEEE Micro,
38(2):21–29.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-
ters, Michael Schmitz, and Luke Zettlemoyer. 2018.
AllenNLP: A deep semantic natural language pro-
cessing platform. In Proceedings of Workshop for
NLP Open Source Software (NLP-OSS), pages 1–
6, Melbourne, Australia. Association for Computa-
tional Linguistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Jeremy Howard et al. 2018. fastai. https://
github.com/fastai/fastai.

Tao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav
Artzi. 2018. Simple recurrent units for highly par-
allelizable recurrence. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 4470–4481.

Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Ros-
tamizadeh, and Ameet Talwalkar. 2016. Hyperband:
A novel bandit-based approach to hyperparameter
optimization. arXiv preprint arXiv:1603.06560.

Xin Li and Dan Roth. 2002. Learning question clas-
sifiers. In COLING 2002: The 19th International
Conference on Computational Linguistics.

Richard Liaw, Eric Liang, Robert Nishihara, Philipp
Moritz, Joseph E Gonzalez, and Ion Stoica.
2018. Tune: A research platform for distributed
model selection and training. arXiv preprint
arXiv:1807.05118.

Philipp Moritz, Robert Nishihara, Stephanie Wang,
Alexey Tumanov, Richard Liaw, Eric Liang,
William Paul, Michael I. Jordan, and Ion Stoica.
2017. Ray: A distributed framework for emerging
AI applications. CoRR, abs/1712.05889.

Ingy döt Net Oren Ben-Kiki, Clark Evans. 2009.
Yaml. https://yaml.org/spec/1.2/
spec.html.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.
In NIPS-W.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 conference on
empirical methods in natural language processing,
pages 1631–1642.

Matei Zaharia, Andrew Chen, Aaron Davidson, Ali
Ghodsi, Sue Ann Hong, Andy Konwinski, Sid-
dharth Murching, Tomas Nykodym, Paul Ogilvie,
Mani Parkhe, et al. 2018. Accelerating the machine
learning lifecycle with mlflow. Data Engineering,
page 39.

https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf
https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf
https://www.aclweb.org/anthology/N19-4009
https://www.aclweb.org/anthology/N19-4009
https://www.aclweb.org/anthology/W18-2503
https://www.aclweb.org/anthology/W18-2503
https://www.aclweb.org/anthology/W18-2503
https://keras.io
https://keras.io
https://www.aclweb.org/anthology/N19-1423
https://www.aclweb.org/anthology/N19-1423
https://www.aclweb.org/anthology/N19-1423
https://www.aclweb.org/anthology/W18-2501
https://www.aclweb.org/anthology/W18-2501
https://github.com/fastai/fastai
https://github.com/fastai/fastai
https://www.aclweb.org/anthology/C02-1150
https://www.aclweb.org/anthology/C02-1150
http://arxiv.org/abs/1712.05889
http://arxiv.org/abs/1712.05889
https://yaml.org/spec/1.2/spec.html
https://yaml.org/spec/1.2/spec.html


187

A Screenshots

Below is a screenshot of the reporting site that includes a progress bar, links to see the console output
and Tensorboard, and a download link for the model weights:

When you launch an experiment from the console, you will see a series of status updates as
shown below:



188

B BERT Configuration File


