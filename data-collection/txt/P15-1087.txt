



















































The NL2KR Platform for building Natural Language Translation Systems


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 899–908,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

The NL2KR Platform for building Natural Language Translation Systems

Nguyen H. Vo, Arindam Mitra and Chitta Baral
School of Computing, Informatics and Decision Systems Engineering

Arizona State University
{nguyen.h.vo, amitra7, chitta }@asu.edu

Abstract

This paper presents the NL2KR platform
to build systems that can translate text to
different formal languages. It is freely-
available1, customizable, and comes with
an Interactive GUI support that is use-
ful in the development of a translation
system. Our key contribution is a user-
friendly system based on an interactive
multistage learning algorithm. This effec-
tive algorithm employs Inverse-λ, Gener-
alization and user provided dictionary to
learn new meanings of words from sen-
tences and their representations. Using
the learned meanings, and the Generaliza-
tion approach, it is able to translate new
sentences. NL2KR is evaluated on two
standard corpora, Jobs and GeoQuery and
it exhibits state-of-the-art performance on
both of them.

1 Introduction and Related Work

For natural language interaction with systems one
needs to translate natural language text to the input
language of that system. Since different systems
(such as a robot or database system) may have dif-
ferent input language, we need a way to translate
natural language to different formal languages as
needed by the application. We have developed a
user friendly platform, NL2KR, that takes exam-
ples of sentences and their translations (in a de-
sired target language that varies with the applica-
tion), and some bootstrap information (an initial
lexicon), and constructs a translation system from
text to that desired target language.

1http://nl2kr.engineering.asu.edu/

Our approach to translate natural language text
to formal representation is inspired by Montague’s
work (Montague, 1974) where the meanings of
words and phrases are expressed as λ-calculus ex-
pressions and the meaning of a sentence is built
from semantics of constituent words through ap-
propriate λ-calculus (Church, 1936) applications.
A major challenge in using this approach has been
the difficulty of coming up with the λ-calculus
representation of words.

Montague’s approach has been widely used in
(Zettlemoyer and Collins, 2005; Kwiatkowski et
al., 2010) to translate natural language to formal
languages. In ZC05 (Zettlemoyer and Collins,
2005) the learning algorithm requires the user to
provide the semantic templates for all words. A
semantic template is a λ-expression (e.g. λx.p(x)
for an arity one predicate), which describes a par-
ticular pattern of representation in that formal lan-
guage. With all these possible templates, the
learning algorithm extracts the semantic represen-
tation of the words from the formal representa-
tion of a sentence. It then associates the extracted
meanings to the words of the sentence in all possi-
ble ways and ranks the associations according to
some goodness measure. While manually com-
ing up with semantic templates for one target lan-
guage is perhaps reasonable, manually doing it for
different target languages corresponding to differ-
ent applications may not be a good idea as manual
creation of semantic templates requires deep un-
derstanding of translation to the target language.
This calls for automating this process. In UBL
(Kwiatkowski et al., 2010) this process is auto-
mated by restricting the choices of formal rep-
resentation and learning the meanings in a brute
force manner. Given, a sentence S and its rep-
resentation M in the restricted formal language,

899



it breaks the sentence into two smaller substrings
S1, S2 and uses higher-order unification to com-
pute two λ-termsM1,M2 which combines to pro-
duce M . It then recursively learns the meanings
of the words, from the sub-instance < S1,M1 >
and < S2,M2 >. Since, there are many ways
to split the input sentence S and the choice of
M1,M2 can be numerous, it needs to consider all
possible splittings and their combinations; which
produces many spurious meanings. Most impor-
tantly, their higher-order unification algorithm im-
poses various restrictions (such as limited num-
ber of conjunctions in a sentence, limited forms of
functional application) on the meaning representa-
tion language which severely limits its applicabil-
ity to new applications. Another common draw-
back of these two algorithms is that they both suf-
fer when the test sentence contains words that are
not part of the training corpus.

Our platform NL2KR uses a different auto-
mated approach based on Inverse-λ (section 2.1)
and Generalization (section 2.2) which does not
impose such restrictions enforced by their higher-
order unification algorithm. Also, Generaliza-
tion algorithm along with Combinatory Categor-
ical Grammar (Steedman, 2000) parser, allows
NL2KR to go beyond the training dictionary and
translate sentences which contain previously un-
seen words. The main aspect of our approach is as
follows: given a sentence, its semantic representa-
tion and an initial dictionary containing the mean-
ing of some words, NL2KR first obtains several
derivation of the input sentence in Combinatory
Categorical Grammar (CCG). Each CCG deriva-
tion tree describes the rules of functional appli-
cation through which constituents combine with
each other. With the user provided initial dictio-
nary, NL2KR then traverses the tree in a bottom-
up fashion to compute the semantic expressions
of intermediate nodes. It then traverses the aug-
mented tree in a top-down manner to learn the
meaning of missing words using Inverse-λ (sec-
tion 2.1). If Inverse-λ is not sufficient to learn the
meaning of all unknown words, it employs Gen-
eralization (section 2.2) to guess the meanings of
unknown words with the meaning of known sim-
ilar words. It then restarts the learning process
with the updated knowledge. The learning pro-
cess stops if it learns the meanings of all words or
fails to learn any new meaning in an iteration. In
the latter case, it shows the augmented tree to the

user. The user can then provide meanings of some
unknown words and resumes the learning process.

Another distinguishing feature of NL2KR is its
user-friendly interface that helps users in creating
their own translation system. The closest system
to NL2KR is the UW Semantic Parsing Frame-
work (UW SPF) (Artzi and Zettlemoyer, 2013)
which incorporates the algorithms in (Zettlemoyer
and Collins, 2005; Kwiatkowski et al., 2010) .
However, to use UW SPF for the development of
a new system, the user needs to learn their coding
guidelines and needs to write new code in their
system. NL2KR does not require the users to
write new code and guides the development pro-
cess with its rich user interface.

We have evaluated NL2KR on two standard
datasets: GeoQuery (Tang and Mooney, 2001) and
Jobs (Tang and Mooney, 2001). GeoQuery is a
database of geographical questions and Jobs con-
tains sentences with job related query. Experi-
ments demonstrate that NL2KR can exhibit state-
of-the-art performance with fairly small initial dic-
tionary. The rest of the paper is organized as fol-
lows: we first present the algorithms and archi-
tecture of the NL2KR platform in section 2; we
discuss about the experiments in section 3; and fi-
nally, we conclude in section 4.

2 Algorithms and Architecture

The NL2KR architecture (Figure 1) has two sub-
parts which depend on each other (1) NL2KR-
L for learning and (2) NL2KR-T for translation.
The NL2KR-L sub-part takes the following as in-
put: (1) a set of training sentences and their tar-
get formal representations, and (2) an initial lexi-
con or dictionary consisting of some words, their
CCG categories, and their meanings in terms of λ-
calculus expressions. It then constructs the CCG
parse trees and uses them for learning of word
meanings.

Learning of word meanings is done by using
Inverse-λ and Generalization (Baral et al., 2012;
Baral et al., 2011) and ambiguity is addressed
by a Parameter Learning module that learns the
weights of the meanings. The learned meanings
update the lexicon. The translation sub-part uses
this updated lexicon to get the meaning of all the
words in a new sentence, and combines them to get
the meaning of the new sentence. Details of each
module will be presented in the following subsec-
tions.

900



Figure 1: Architecture of NL2KR

The NL2KR platform provides a GUI (Figure 2)
with six features: λ-application, Inverse-λ, Gen-
eralization, CCG-Parser, NL2KR-L and NL2KR-
T. The fourth feature is a stand-alone CCG parser
and the first four features can help on user with
constructing the initial lexicon. The user can then
use NL2KR-L to update the lexicon using train-
ing data and the NL2KR-T button then works as a
translation system.

2.1 Inverse-λ
Inverse-λ plays a key role in the learning pro-
cess. Formally, given two λ-expressions H and
G with H = F@G or H = G@F , the
Inverse-λ operation computes the λ expression
F . For example, given the meaning of “is texas”
as λx2.x2@stateid(texas) and the meaning of
“texas” as stateid(texas), with the additional
information that “is” acts as the function while
“texas” is the argument, the Inverse-λ algorithm
computes the meaning of “is” as λx3.λx2.x2@x3
(Figure 4). NL2KR implements the Inverse-λ al-
gorithm specified in (Baral et al., 2012). The
Inverse-λ module is separately accessible through
the main GUI (Figure 2).

2.2 Generalization
Generalization (Baral et al., 2012; Baral et al.,
2011) is used when Inverse-λ is not sufficient to
learn new semantic representation of words. In
contrast to Inverse-λ which learns the exact mean-
ing of a word in a particular context, General-
ization learns the meanings of a word from sim-
ilar words with existing representations. Thus,
Generalization helps NL2KR to learn meanings
of words that are not even present in the train-
ing data set. In the current implementation, two

words are considered as similar if they have the
exact same CCG category. As an example, if
we want to generalize the meaning of the word
“plays” with CCG category (S\NP )/NP ) and
the lexicon already contains an entry for “eats”
with the same CCG category, and the mean-
ing λy.λx.eats(x, y), the algorithm will ex-
tract the template λy.λx.WORD(x, y) and ap-
ply the template to plays to get the meaning
λy.λx.plays(x, y).

2.3 Combinatory Categorial Grammar
Derivation of a sentence in Combinatory Catego-
rial Grammar (CCG) determines the way the con-
stituents combine together to establish the mean-
ing of the whole. CCG is a type of phrase struc-
ture grammar and clearly describes the predicate-
argument structure of constituents.

Figure 3 shows an example output of NL2KR’s
CCG parser. In the figure, “John” and “home”
have the category [N] (means noun) and can
change to [NP] (means noun phrase). The
phrase“walk home” has the category [S\NP],
which means that it can combine with a con-
stituent with category [NP] (“John” in this case)
from left with the backward application to form
category [S] (sentence). The word “walk” has
the category [(S\NP)/NP], which means it can
combine with a constituent with category [NP]
(“home”) from right through the forward appli-
cation combinator to form category [S\NP] (of
“walk home”).

A detailed description on CCG goes beyond the
scope of this paper (see (Steedman, 2000) for more
details). Since, natural language sentences can
have various CCG parse trees, each expressing a
different meaning of the sentence, a key challenge

901



Figure 2: NL2KR’s main GUI, Version 1.7.0001

Figure 3: CCG parse tree of ”John walked home”.

in the learning and the translation process is to find
a suitable CCG parse tree for a sentence in natu-
ral language. We overcome this impediment by
allowing our learning and translation subsystem
to work with multiple weighted parse trees for a
given sentence and determining on the fly, the one
that is most suitable. We discuss more on this in
sections 2.4-2.6.

Existing CCG parsers (Curran et al., 2007; Lier-
ler and Schüller, 2012) either return a single best
parse tree for a given sentence or parse it in all
possible ways with no preferential ordering among
them. In order to overcome this shortcoming and
generate more than one weighted candidate parse
trees, we have developed a new parser using beam
search with Cocke-Younger-Kasami(CYK) algo-
rithm. NL2KRs CCG parser uses the C&C model

(Curran et al., 2007) and constraints from the Stan-
ford parser (Socher et al., 2013; Toutanova et al.,
2003) to guide the derivation of a sentence. The
output of the CCG parser is a set of k weighted
parse trees, where the parameter k is provided by
the user.

NL2KR system allows one to use the CCG
parser independently through the interactive GUI.
The output graphs look like the one in Figure 3. It
can be zoomed in/out and its nodes can be moved
around, making it easier to work with complex
sentences.

2.4 Multistage learning approach

Learning meanings of words is the major com-
ponent of our system. The inputs to the learning
module are a list of training sentences, their target
formal representations and an initial lexicon con-
sisting of triplets of the form <word, CCG cate-
gory, meaning>, where meanings are represented
in terms of λ-calculus expressions. The output
of the algorithm is a final dictionary containing
a set of 4-tuples (word, CCG category, meaning,
weight).

Interactive Multistage Learning Algorithm
(IMLA) NL2KR employs an Interactive Multi-
stage Learning Algorithm (Algorithm 1) that runs
many iterations on the input sentences. In each
iteration, it goes through one or more of the fol-
lowing stages:

Stage 1 In Stage 1, it gets all the unfinished
sentences. It then employs Bottom Up-Top Down
algorithm (Algorithm 2) to learn new meanings
(by Inverse-λ). For a sentence, if it has com-
puted the meanings of all its constituents, which
can be combined to produce the given representa-
tion, that sentence is considered as learned. Each

902



Algorithm 1 IMLA algorithm
1: function IMLA(initLexicon,sentences,
sentsMeanings)

2: regWords← ∅
3: generalize← false
4: lexicon← initLexicon
5: repeat
6: repeat
7: repeat
8: for all s ∈ sentences do
9: newMeanings ←

BT(s,lexicon,sentsMeanings)
10: lexicon← lexicon ∪ newMeanings
11: for all n ∈ newMeanings do
12: ms← GENERALIZE(regWords, n)
13: lexicon← lexicon ∪ms
14: end for
15: end for
16: until newMeanings = ∅
17: if generalize=false then
18: generalize← true
19: for all t ∈ unfinishedSents do
20: words← GETALLWORDS(t)
21: ms← GENERALIZE(words)
22: lexicon← lexicon ∪ms
23: regWords← regWords ∪ words
24: end for
25: end if
26: until newMeanings = ∅
27: INTERATIVELEARNING
28: until unfinishedSents = ∅ OR userBreak
29: lexicon ← PARAMETERESTIMA-

TION(lexicon,sentences)
30: return lexicon
31: end function

new meaning learned by this process is used to
generalize the words in a waiting list. Initially,
this waiting list is empty and is updated in stage
2. When no more new meaning can be learned
by Bottom Up-Top Down algorithm, IMLA (Algo-
rithm 1) enters stage 2.

Stage 2 In this stage, it takes all the sentences
for which the learning is not yet finished and ap-
plies Generalization process on all the words of
those sentences. At the same time, it populates
those words into the waiting list, so that from now
on, Bottom Up-Top Down will try to generalize
new meanings for them when it learns some new
meanings. It then goes back to stage 1. Next time,

after exiting stage 1, it directly goes to stage 3.

Stage 3 When both aforementioned stages
can not learn all the sentences, the Interactive
Learning process is invoked and all the unfinished
sentences are shown on the interactive GUI (Fig-
ure 4). Users can either skip or provide more in-
formation on the GUI and the learning process is
continued.

After finishing all stages, IMLA (Algorithm 1)
calls Parameter Estimation (section 2.5) algorithm
to compute the weight of each lexicon tuple.

Bottom Up-Top Down learning For a given
sentence, the CCG parser is used for the CCG
parse trees like the one of how big is texas in Fig-
ure 4. For each parse tree, two main processes
are called, namely “bottom up” and “top down”.
In the first process, all the meanings of the words
in the sentences are retrieved from the lexicon.
These meanings are populated in the leaf nodes
of a parse tree (see Figure 4), which are combined
in a bottom-up manner to compute the meanings
of phrases and full sentences. We call these mean-
ings, the current meanings.

In the “top down” process, using Inverse-λ al-
gorithm, the given meaning of the whole sentence
(called the expected meaning of the sentence) and
the current meanings of the phrases, we calcu-
late the expected meanings of each of the phrases
from the root of the tree to the leaves. For ex-
ample, given the expected meaning of how big is
texas and the current meaning of how big, we use
Inverse-λ algorithm to get the meaning (expected)
of is texas. This expected meaning is used together
with current meanings of is (texas) to calculate
the expected meanings of texas (is). The expected
meanings of the leaf nodes we have just learned
will be saved to the lexicon and will be used in the
other sentences and in subsequent learning itera-
tion. The “top down” process is stopped when the
expected meanings are same as the current mean-
ings. And in both “bottom up” and “top-down”
processes, the beam search algorithm is used to
speed-up the learning process.

Interactive learning In the interactive learning
process it opens a GUI which shows the unfinished
sentences. Users can see the current and expected
meanings for the unfinished sentences. When the
user gives additional meanings of word(s), the λ-
application or Inverse-λ operation is automatically
performed to update the new meaning(s) to related

903



Figure 4: Interactive learning GUI. The box under each node show: the corresponding phrases [CCG category], the expected
meanings and the current meanings. Click on the red node will show the window to change the current meaning (CLE)

Algorithm 2 BottomUp-TopDown (BT) algo-
rithm

1: function BT(
sentence, lexicon, sentsMeanings)

2: parseTrees← CCGPARSER(sentence)
3: for all tree ∈ parseTrees do
4: t← BOTTOMUP(tree,lexicon)
5: TOPDOWN(t,sentsMeanings)
6: end for
7: end function

word(s). Once satisfied, the user can switch back
to the automated learning mode.

Example Let us consider the ques-
tion “How big is texas?” with meaning
answer(size(stateid(texas))) (see Figure
4).

Let us assume that the initial dictionary has
the following entries: how := NP/(N/N) :
λx.λy.answer(x@y), big := N/N : λx.size(x)
and texas :=NP : stateid(texas). The algorithm
then proceeds as follows.

First, the meanings of “how” and “big” are com-
bined to compute the current meaning of “how
big” := NP : λx.answer(size(x)) in the “bot-
tom up” process. Since the meaning of “is” is un-
known, the current meaning of “is texas” still re-
mains unknown.

It then starts the “top down” process where

it knows the expected meaning of “How big is
texas” := S : answer(size(stateid(texas)))
and the current meaning of “how big”. Using
them in the Inverse-λ algorithm, it then com-
pute the meaning of “is texas” := S\NP :
λx1.x1@stateid(texas). Using this expected
meaning and current meaning of “texas” := NP :
stateid(texas), it then calculates the expected
meaning of “is” as “is” := (S\NP )/NP :
λx2.λx1.x1@x2. This newly learned expected
meaning is then saved into the lexicon.

Since the meaning of all the words in the ques-
tion are known, the learning algorithm stops here
and the Interactive Learning is never called.

If initially, the dictionary contains only two
meanings: “big” := N/N : λx.size(x) and
“texas” := NP : stateid(texas), NL2KR tries
to first learn the sentence but fails to learn
the complete sentence and switches to Inter-
active Learning which shows the interactive
GUI (see Figure 4). If the user specifies
that “how” means λx.λy.answer(x@y), NL2KR
combines its meaning with the meaning of “big”
to get the meaning “how big” := NP :
λx.answer(size(x)). It will then use Inverse-
λ to figure out the meaning of “is texas” and
then the meaning of “is”. Now all the mean-
ings are combined to compute the current mean-
ing answer(size(stateid(texas))) of “How big
is texas”. This meaning is same as the expected

904



meaning, so we know that the sentence is suc-
cessfully learned. Now, the user can press Retry
Learning to switch back to automated learning.

2.5 Parameter Estimation

The Parameter Estimation module estimates a
weight for each word-meaning pair such that the
joint probability of the training sentences getting
translated to their given representation is maxi-
mized. It implements the algorithm described in
Zettlemoyer et. al.(2005).

2.6 Translation

The goal of this module is to convert input sen-
tences into the target formalism using the lexi-
con previously learned. The algorithm used in
Translation module (Algorithm 3) is similar to the
bottom-up process in the learning algorithm. It
first obtains several weighted CCG parse trees of
the input sentence. It then computes a formal rep-
resentation for each of the parse trees using the
learned dictionary. Finally, it ranks the transla-
tions according to the weights of word-meaning
pairs and the weights of the CCG parse trees.
However, test sentences may contain words which
were not present in the training set. In such cases,
Generalization is used to guess the meanings of
those unknown words from the meanings of the
similar words present in the dictionary.

Algorithm 3 Translation algorithm
1: function TRANSLATE(sentence, lexicon)
2: candidates← ∅
3: parseTrees← CCGPARSER(sentence)
4: for all tree ∈ parseTrees do
5: GENERALIZE(tree);
6: t← BOTTOMUP(tree)
7: candidates← candidates ∪ t
8: end for
9: output← VERIFY-RANK(candidates)

10: return output
11: end function

3 Experimental Evaluation

We have evaluated NL2KR on two standard cor-
pora: GeoQuery and Jobs. For both the corpus, the
output generated by the learned system has been
considered correct if it is an exact replica of the
logical formula described in the corpus.

We report the performance in terms of precision
(percentage of returned logical-forms that are cor-
rect), recall (percentage of sentences for which the
correct logical-form was returned), F1-measure
(harmonic mean of precision and recall) and the
size of the initial dictionary.

We compare the performance of our sys-
tem with recently published, directly-comparable
works, namely, FUBL (Kwiatkowski et al.,
2011), UBL (Kwiatkowski et al., 2010), λ-WASP
(Wong and Mooney, 2007), ZC07 (Zettlemoyer
and Collins, 2007) and ZC05 (Zettlemoyer and
Collins, 2005) systems.

3.1 Corpora
GeoQuery GeoQuery (Tang and Mooney, 2001)
is a corpus containing questions on geographical
facts about the United States. It contains a total of
880 sentences written in natural language, paired
with their meanings in a formal query language,
which can be executed against a database of the
geographical information of the United States.
We follow the standard training/testing split of
600/280. An example sentence meaning pair is
shown below.

Sentence: How long is the Colorado river?
Meaning: answer(A,(len(B,A),const(B,
riverid(colorado)), river(B)))

Jobs The Jobs (Tang and Mooney, 2001) dataset
contains a total of 640 job related queries written
in natural language. The Prolog programming
language has been used to represent the meaning
of a query. Each query specifies a list of job
criteria and can be directly executed against a
database of job listings. An example sentence
meaning pair from the corpus is shown below.

Question: What jobs are there for program-
mers that know assembly?
Meaning: answer(J,(job(J),title(J,T),
const(T,’Programmer’),language(J,L),
const(L,’assembly’))))

The dataset contains a training split of 500 sen-
tences and a test split of 140 sentences.

3.2 Initial Dictionary Formulation
GeoQuery For GeoQuery corpus, we manually
selected a set of 100 structurally different sen-
tences from the training set and initiated the learn-
ing process with a dictionary containing the repre-

905



GUI Driven Initial Dictionary Learned Dictionary
] <word, category > 31 118 401

] <word, category, meaning> 36 127 1572
] meaning 30 89 819

Table 1: Comparison of Initial and Learned dictionary for GeoQuery corpus on the basis of the number of entries in the
dictionary, number of unique <word, CCG category> pairs and the number of unique meanings across all the entries. “GUI
Driven” denotes the amount of the total meanings given through interactive GUI and is a subset of the Initial dictionary.

GUI Driven Initial Dictionary Learned Dictionary
] <word, category> 58 103 226

] <word, category, meaning> 74 119 1793
] meaning 57 71 940

Table 2: Comparison of Initial and Learned dictionary for Jobs corpus.

sentation of the nouns and question words. These
meanings were easy to obtain as they follow sim-
ple patterns. We then trained the translation sys-
tem on those selected sentences. The output of
this process was used as the initial dictionary for
training step. Further meanings were provided on
demand through interactive learning. A total of
119 word meanings tuples (Table 1, ] <word, cat-
egory, meaning >) were provided from which the
NL2KR system learned 1793 tuples. 45 of the 119
were representation of nouns and question words
that were obtained using simple patterns. The re-
maining 74 were obtained by a human using the
NL2KR GUI. These numbers illustrate the useful-
ness of the NL2KR GUI as well as the NL2KR
learning component. One of our future goals is to
further automate the process and reduce the GUI
interaction part.

Table 1 compares the initial and learned dic-
tionary for GeoQuery on the basis of number
of unique <word, category, meaning> entries in
dictionary, number of unique <word, category>
pairs and the number of unique meanings across
all the entries in the dictionary. Since each unique
<word, CCG category> pair must have at least
one meaning, the total number of unique <word,
category> pairs in the training corpus provides a
lower bound on the size of the ideal output dictio-
nary. However, one <word, category> pair may
have multiple meanings, so the ideal dictionary
can be much bigger than the number of unique
<word, category> pairs. Indeed, there were many
words such as “of”, “in” that had multiple mean-
ings for the same CCG category. Table 1 clearly
describes that the amount of initial effort is sub-
stantially less compared to the return.

Jobs For the Jobs dataset, we followed a similar
process as in the GeoQuery dataset. A set of 120
structurally different sentences were selected and a
dictionary was created which contained the repre-
sentation of the nouns and the question words from
the training corpus. A total of 127 word meanings
were provided in the process. Table 2 compares
the initial and learned dictionary for Jobs. Again,
we can see that the amount of initial effort is sub-
stantially less in comparison to the return.

3.3 Precision, Recall and F1-measure

Figure 5: Comparison of Precision, Recall and F1-measure
on GeoQuery and Jobs dataset.

Table 3, Table 4 and Figure 5 present the com-
parison of the performance of NL2KR on the Geo-
Query and Jobs domain with other recent works.
NL2KR obtained 91.1% precision value, 92.1%

906



System Precision Recall F1
ZC05 0.963 0.793 0.87
ZC07 0.916 0.861 0.888
λ-WASP 0.9195 0.8659 0.8919
UBL 0.885 0.879 0.882
FUBL 0.886 0.886 0.886
NL2KR 0.911 0.921 0.916

Table 3: Comparison of Precision, Recall and F1-measure on
GeoQuery dataset.

recall value and a F1-measure of 91.6% on Geo-
Query (Figure 5, Geo880) dataset. For Jobs cor-
pus, the precision, recall and F1-measure were
95.43%, 94.03% and 94.72% respectively. In
all cases, NL2KR achieved state-of-the-art recall
and F1 measures and it significantly outperformed
FUBL (the latest work on translation systems) on
GeoQuery.

For both GeoQuery and Jobs corpus, our recall
is significantly higher than existing systems be-
cause meanings discovered by NL2KRs learning
algorithm is more general and reusable. In other
words, meanings learned from a particular sen-
tence are highly likely to be applied again in the
context of other sentences. It may be noted that,
larger lexicons do not necessarily imply higher re-
call as lambda expressions for two phrases may
not be suitable for functional application, thus
failing to generate any translation for the whole.
Moreover, the use of a CCG parser maximizes the
recall by exhibiting consistency and providing a
set of weighted parse trees. By consistency, we
mean that the order of the weighted parse tree re-
mains same over multiple parses of the same sen-
tence and the sentences having similar syntactic
structures have identical ordering of the deriva-
tions, thus making Generalization to be more ef-
fective in the process of translation.

The sentences for which NL2KR did not have
a translation are the ones having structural dif-
ference with the sentences present in the train-
ing dataset. More precisely, their structure was
not identical with any of the sentences present in
the training dataset or could not be constructed by
combining the structures observed in the training
sentences.

We analyzed the sentences for which the trans-
lated meaning did not match the correct one and
observed that the translation algorithm selected
the wrong meaning, even though it discovered the
correct one as one of the possible meanings the

System Precision Recall F1
ZC05 0.9736 0.7929 0.8740
COCKTAIL 0.9325 0.7984 0.8603
NL2KR 0.9543 0.9403 0.9472

Table 4: Comparison of Precision, Recall and F1-measure on
Jobs dataset.

sentence could have had in the target formal lan-
guage. Among the sentences for which NL2KR
returned a translation, there were very few in-
stances where it did not discover the correct mean-
ing in the set of possible meanings.

It may be noted that even though our preci-
sion is lower than ZC05 and very close to ZC07
and WASP; we have achieved significantly higher
F1 measure than all the related systems. In
fact, ZC05, which achieves the best precision for
both the datasets, is better by a margin of only
0.019 on the Jobs dataset and 0.052 on the Geo-
Query dataset. We think one of the main rea-
sons is that it uses manually predefined lambda-
templates, which we try to automate as much as
possible.

4 Conclusion

NL2KR is a freely available2, user friendly, rich
graphical platform for building translation systems
to convert sentences from natural language to their
equivalent formal representations in a wide vari-
ety of domains. We have described the system al-
gorithms and architecture and its performance on
the GeoQuery and Jobs datasets. As mentioned
earlier, the NL2KR GUI and the NL2KR learning
module help in starting from a small initial lex-
icon (for example, 119 in Table 2) and learning
a much larger lexicon (1793 in Table 2). One of
our future goals is to reduce the initial lexicon to
be even smaller by further automating the NL2KR
GUI interaction component .

Acknowledgements

We thank NSF for the DataNet Federation Consor-
tium grant OCI-0940841 and ONR for their grant
N00014-13-1-0334 for partially supporting this re-
search.

2More examples and a tutorial to use NL2KR are available
in the download package.

907



References
Yoav Artzi and Luke Zettlemoyer. 2013. UW SPF:

The University of Washington Semantic Parsing
Framework. arXiv preprint arXiv:1311.3011.

Chitta Baral, Juraj Dzifcak, Marcos Alvarez Gonzalez,
and Jiayu Zhou. 2011. Using inverse λ and gener-
alization to translate english to formal languages. In
Proceedings of the Ninth International Conference
on Computational Semantics, pages 35–44. Associ-
ation for Computational Linguistics.

Chitta Baral, Juraj Dzifcak, Marcos Alvarez Gonzalez,
and Aaron Gottesman. 2012. Typed answer set pro-
gramming lambda calculus theories and correctness
of inverse lambda algorithms with respect to them.
TPLP, 12(4-5):775–791.

Alonzo Church. 1936. An Unsolvable Problem of
Elementary Number Theory. American Journal of
Mathematics, 58(2):345–363, April.

James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically Motivated Large-Scale NLP with
C&C and Boxer. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 33–36, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of the 2010 con-
ference on empirical methods in natural language
processing, pages 1223–1233. Association for Com-
putational Linguistics.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical general-
ization in ccg grammar induction for semantic pars-
ing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1512–1523. Association for Computational Linguis-
tics.

Yuliya Lierler and Peter Schüller. 2012. Parsing com-
binatory categorial grammar via planning in answer
set programming. In Correct Reasoning, pages 436–
453. Springer.

Richard Montague. 1974. English as a Formal Lan-
guage. In Richmond H. Thomason, editor, Formal
Philosophy: Selected Papers of Richard Montague,
pages 188–222. Yale University Press, New Haven,
London.

Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with Composi-
tional Vector Grammars. In ACL (1), pages 455–
465.

Mark Steedman. 2000. The syntactic process, vol-
ume 35. MIT Press.

Lappoon R Tang and Raymond J Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Machine
Learning: ECML 2001, pages 466–477. Springer.

Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1.

Yuk Wah Wong and Raymond J Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Annual Meeting-
Association for computational Linguistics, vol-
ume 45, page 960. Citeseer.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to Map Sentences to Logical Form: Struc-
tured Classification with Probabilistic Categorial
Grammars. In UAI, pages 658–666. AUAI Press.

Luke S Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL-2007).

908


