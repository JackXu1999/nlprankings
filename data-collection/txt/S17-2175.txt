



















































EUDAMU at SemEval-2017 Task 11: Action Ranking and Type Matching for End-User Development


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 1000–1004,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

EUDAMU at SemEval-2017 Task 11: Action Ranking and Type Matching
for End-User Development

Marek Kubis and Paweł Skórzewski and Tomasz Ziętkiewicz
Faculty of Mathematics and Computer Science

Adam Mickiewicz University in Poznań
{mkubis,pawel.skorzewski,tomasz.zietkiewicz}@amu.edu.pl

Abstract

The paper describes a system for end-user
development using natural language. Our
approach uses a ranking model to identify
the actions to be executed followed by ref-
erence and parameter matching models to
select parameter values that should be set
for the given commands. We discuss the
results of evaluation and possible improve-
ments for future work.

1 Introduction

The goal of the end-user development (EUD) is to
provide users of software systems with the tools
to create, extend or modify software (Lieberman
et al., 2006). End-User Development using Natu-
ral Language is one of the tasks of SemEval-2017
International Workshop on Semantic Evaluation.

The idea of using tools for software devel-
opment more accessible than programming lan-
guages has been discussed almost since the begin-
ning of computers. The concept of programming
with natural language commands and its conse-
quences have been considered already in the 70s
(Dijkstra, 1979). However, only in recent years
these ideas could be put into practice with the
rapid development of advanced natural language
processing methods.

A comprehensive overview of recent trends and
achievements in EUD has been presented by Pa-
ternò (2013). Although most common solution
for EUD problem are various variations on graph-
ical user interfaces, solutions using NLP are also
present. One of the examples is the Koala (later:
CoScripter) system (web browser extension), that
uses “sloppy programming”, i.e. pseudo-natural
language instructions, to automate business pro-
cesses on the web (Little et al., 2007). Other sys-
tems that use simple natural language commands

to achieve programming-like goals, described in
recent years, include SPOK, an EUD environment
for smart homes (Coutaz et al., 2014), and Natu-
ralMash, an EUD tool for creating web mashups
(Aghaee and Pautasso, 2014).

This paper describes our system for the end-user
development using natural language and reports
its performance according to the SemEval 2017
Task 11 evaluation criteria (Sales et al., 2017).

2 Data preparation

We pre-process all the input data: both the action
knowledge base and natural language commands.
The pre-processing is done in several stages. It in-
cludes basic text processing as well as adding lexi-
cal features. We use these features to better match
the commands to actions. The pre-processor op-
erates on JSON files, in each step adding new
fields to the JSON structure so the original data
are not lost in the process and can be used in fur-
ther steps. The input fields we annotate are: desc,
name, value, nl_command_statment, provider,
sample, tags and api-name.

The first pre-processing stage is sentence
splitting. The sentence splitter is applied to
nl_command_statment and desc fields only.

The second step is tokenization. The input data
are amended not only with tokens, but also with
token types. The following token types are recog-
nized: text, number, phone number, monetary ex-
pression, punctuation, URL, e-mail address, hash-
tag, file name, other. Types of tokens are recog-
nized using regular expressions. The set of token
types has been specified manually in such way that
they are useful in the context of both the original
training dataset and other possible datasets. The
token types are used in further processing stages:
anaphora resolution, action detection and parame-
ters matching.

1000



In the next step, we append features from Syn-
taxNet (Andor et al., 2016), Stanford CoreNLP
(Manning et al., 2014) and NLTK (Bird et al.,
2009) to the natural language commands. In par-
ticular, we introduce: part-of-speech tags from
SyntaxNet to implement discourse annotation
rules (cf. Section 3.1); word lemmata from NLTK
for the purpose of preprocessing text for the ac-
tion ranker (cf. Section 3.2); named entities and
constituency parser annotations from CoreNLP
(Finkel et al., 2005) for anaphora resolution.

The next pre-processing stage appends features
extracted from the user knowledge base. Any ref-
erence to the user knowledge base entry that oc-
curs in a command is annotated with the entry
identifier and the name of the referred entry field.

The last pre-processing stage is to add anaphora
information. Anaphora tags are appended to pos-
sible anaphoric terms like it, him etc. or nouns pre-
ceded with definite articles (e.g. the file). The an-
tecedent expressions of the anaphora are identified
on the basis of their part-of-speech tags, NER tags,
token types or phrase categories. For this purpose
we use token type annotations and selected fea-
tures from CoreNLP: tokens, POS tags, NER tags
and parse results.

3 System Overview

The main components of the system are presented
in Figure 1. A natural language command that
has been preprocessed according to the proce-
dure described in Section 2 is passed to the dis-
course tagger which identifies phrases and rela-
tionships among them. Phrases are passed sepa-
rately through action ranker which identifies can-
didate actions. Next, reference matcher identi-
fies dependencies that link data values that are re-
turned by the candidate actions with the parame-
ters of their successor actions. Then, the parameter
matcher aligns tokens that occur in the phrase to
the parameters of the candidate actions that where
not linked by the reference matcher. Finally, the
phrases annotated with actions and parameter val-
ues are passed to the statement mapper which uses
the discourse structure to output action instances
that conform to the task specification.1

1For an example of a complete data flow that passes a
natural language command through all the system modules,
we refer the reader to (Kubis et al., 2017).

Param
Matcher

Action
Ranker

Reference
Matcher

Discourse Tagger

NL command

Reference
Model

Param
Model

Ranking
Model

Statement Mapper

Action
Instance

Action
Instance...

Phrase Phrase...

Annotated
Phrase

... Annotated
Phrase

Figure 1: System Components

3.1 Discourse Tagger

The discourse tagger consists of a set of hard-
coded rules that are responsible for splitting the
command into separate phrases that can be passed
to the action ranker independently. The rules are
implemented as Python functions that check if a
given token can be a split point and assign tags that
identify the relationships that hold among phrases
being separated. For example, the discourse tagger
has a rule that checks if both sides of the and token
contain tokens that are verbs (i.e. are annotated
with the VERB part-of-speech tag) and if the con-
dition is satisfied, the tagger splits the command
into two phrases and annotates the second phrase
with the AN tag. Beside the rules for separating se-
quences of actions, the tagger also contains rules
that identify conditional statements and assign IF,
DO and EL tags that indicate the condition, conse-
quence and alternative parts. Since while loops oc-

1001



curred in the training set sparsely, we did not intro-
duce tags to annotate them restricting our attention
to sequences of actions and conditional statements
only.

3.2 Action Ranker

For the purpose of ranking the actions we consid-
ered TF-IDF (Spärck Jones, 1972) and Doc2Vec
(Le and Mikolov, 2014) document similarity mod-
els.2 We represent the actions by documents that
consist of the text collected from the selected fields
of the action definitions specified in the actionkb
file and natural language commands provided in
the mapping file. The gathered text is lemmatized
and stop words are eliminated. Furthermore, the
natural language commands are delexicalized by
replacing occurrences of named entities and quo-
tations with placeholder tags. In order to deter-
mine candidate actions for the given command, we
apply the same text preprocessing rules as above
and select actions that correspond to the docu-
ments that are most similar to the preprocessed
command.3

We performed 5-fold cross-validation on the
training set to select features for our final model.
We investigated the models that gather text
from: action names (N), action descriptions (D),
provider fields (P); names and descriptions of ac-
tion parameters (Par); names and descriptions of
action data fields (Dat); natural language com-
mands from the mapping file (Com). The av-
erage Micro F1 scores and their standard devia-
tions across validation folds are reported in Ta-
ble 1. It may be noticed that results achieved by
introduction of action descriptions and provider
fields to the models surpass the results of the mod-
els that consists of the action names only. Con-
versely, the extension of the NDP (name, descrip-
tion and provider) model with the names and de-
scriptions of action parameters (NDPPar) worsens
the results. The same holds if we extend the NDP
model with action data values (NDPDat). Finally,
the feature that improves the results most consists
of the aggressively normalized natural language
commands from the mapping file (Com). This ex-
emplifies that even a small sample of the annotated
input data can improve the results significantly.
Another interesting observation is that Dov2Vec

2For training TF-IDF and Doc2Vec models we used Gen-
sim (Řehůřek and Sojka, 2010).

3We limit our attention to up to 10 documents that are
within 0.05 distance to the most similar candidate.

models perform considerably worse than TF-IDF
models in the task regardless of the feature choice.
Thus, for our final submission we selected the TF-
IDF model that encompasses action names, their
descriptions, provider fields from actionkb and
natural language commands from mapping (NDP-
Com).

3.3 Reference Matcher
The reference matcher is responsible for estab-
lishing links between the data returned by an ac-
tion and the parameters of the succeeding actions.
The mapping file distributed with the training data
is used to populate the set of constraints for the
acceptable links. For any two actions that are
linked by an occurrence of the <return*> tag,
the training procedure collects the identifiers of
the linked actions and the names of the data and
params fields being connected.

The matching procedure traverses consecutive
phrases of the natural language command. If a
phrase contains an anaphor, then the reference
matcher checks whether the action instances of the
antecedent and current phrases belong to the set
of constraints learned during training. If the pair
of actions belongs to the constraints set, the links
between corresponding data and params fields
are established.

3.4 Parameter Matcher
For the purpose of setting parameter values for
actions we began with a sequence model based
on the learning to search approach (Chang et al.,
2015). Unfortunately, due to relatively small train-
ing set, the model became highly over-trained and
did not prove to be useful for our final submission.
Instead, we decided to use a parameter matcher
that aligns data types between tokens that occur
in the phrase and action parameters. The matcher
consists of two components: parameter type and
phrase type inducers.

The parameter type inducer restricts data types
that can be accepted by action parameters. Data
types are constrained on the basis of feature anno-
tations gathered during the data preparation stage
(cf. Section 2). The constraints are learned from
the mappings file with the following algorithm:
For every phrase in the mapping file, for every ac-
tion instance of the phrase, for every parameter of
the action instance, let C be the set of constraints
of the parameter, let S be the set of phrase tokens
that match the parameter value:

1002



Model Metric N ND NDP NDPPar NDPDat NDPCom

TF-IDF
micro-F1 0.0759 0.1079 0.1146 0.0960 0.1099 0.2609
std. dev. 0.0274 0.0249 0.0302 0.0415 0.0288 0.0935

Doc2Vec
micro-F1 0.0049 0.0543 0.0737 0.0683 0.0866 0.2007
std. dev. 0.0043 0.0186 0.0076 0.0215 0.0098 0.0531

Table 1: Average Micro F1-Score of TF-IDF and Doc2Vec action ranking models.

1. Add types of tokens in S to C.

2. Add named entity tags of tokens in S to C.

3. If S is encompassed by quotation signs, add
the quotation type to C.

The phrase type inducer constraints data types
of the phrase by applying the following procedure
to every token T :
Let C be the set of constraints of T :

1. Add type of T to C.

2. Add named entity tags of T to C.

3. If T is encompassed by a quotation, add the
quotation type to C.

4. If T is a reference to the entry E in the user
knowledge base, add data types of all non-
empty fields of E to C.

We assume that an action parameter has to be
matched, if the intersection of the sets of con-
straints returned by both inducers is non-empty.
Initialization of the parameter value is a 2-step
procedure. If the token type belongs to the set
of constraints returned by the parameter type in-
ducer, then the raw text of the token is appended
to the parameter value. Otherwise, if the token is a
reference to the entry in the user knowledge base,
the parameter value is populated with the value of
an entry field that satisfies the constraints returned
by the parameter type inducer.

3.5 Statement Mapper
As in the case of the discourse tagger, the state-
ment mapper consists of a set of deterministic,
hard-coded rules implemented in Python that are
responsible for converting the phrases annotated
with actions assigned by the action ranker, links
established by the reference matcher and parame-
ter values set by the parameter matcher to the out-
put format described in the task definition. The
relationships among phrases identified by the dis-
course tagger and encoded as AN, IF, DO and EL

tags are used by the statement mapper to create
JSON objects that represent sequential and condi-
tional execution of action instances in accordance
with the task specification.

4 Results

The results of evaluation performed according to
the official task criteria are gathered in Table 2.
The detailed error analysis requires access to the
annotated version of the test set which was not
available at the time of writing. Nevertheless,
some initial observations can be drawn. The TF-
IDF model built from action names, their descrip-
tions, provider names and exemplar phrases seems
to be a reasonable baseline for determining the
ranking of actions that results in solving of 13 out
of 31 scenarios if the parameter values are not con-
sidered. On the other hand, the parameter match-
ing strategy requires considerable improvement.
We suspect that our per-token strategy leads to the
parameter values that are only partially matched,
hence do not contribute to the result. Another is-
sue that has to be approached in the future is the
problem of propagating data type constraints from
parameters of actions that occur in the set of train-
ing commands to the parameters of actions for
which the training instances are not available.

Criterion Metric Value
Individual actions
solved ignoring
parameter values

precision 0.5490
recall 0.7066

Individual actions
solved considering
parameter values

precision 0.0533
recall 0.0533

Scenarios solved ignor-
ing parameter values

accuracy 41.93%

Scenarios solved consid-
ering parameter values

accuracy 0%

Table 2: Evaluation results.

1003



References
Saeed Aghaee and Cesare Pautasso. 2014. End-user

development of mashups with naturalmash. Journal
of Visual Languages & Computing 25(4):414–432.

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. arXiv
preprint arXiv:1603.06042 .

Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural language processing with Python: analyz-
ing text with the natural language toolkit. " O’Reilly
Media, Inc.".

Kai-Wei Chang, Akshay Krishnamurthy, Alekh
Agarwal, Hal Daume, and John Langford. 2015.
Learning to search better than your teacher. In
David Blei and Francis Bach, editors, Proceed-
ings of the 32nd International Conference on
Machine Learning (ICML-15). JMLR Workshop
and Conference Proceedings, pages 2058–2066.
http://jmlr.org/proceedings/papers/v37/changb15.pdf.

Joëlle Coutaz, Alexandre Demeure, Sybille Caffiau,
and James L Crowley. 2014. Early lessons from the
development of SPOK, an end-user development en-
vironment for smart homes. In Proceedings of the
2014 ACM International Joint Conference on Per-
vasive and Ubiquitous Computing: Adjunct Publi-
cation. ACM, pages 895–902.

Edsger W Dijkstra. 1979. On the foolishness of “natu-
ral language programming”. In Program Construc-
tion, Springer, pages 51–53.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the 43rd annual meet-
ing on association for computational linguistics. As-
sociation for Computational Linguistics, pages 363–
370.

Marek Kubis, Paweł Skórzewski, and
Tomasz Ziętkiewicz. 2017. EU-
DAMU System Components Data Flow.
https://bitbucket.org/mapato/eudamu/wiki/DataFlow.

Quoc V. Le and Tomas Mikolov. 2014. Dis-
tributed representations of sentences and docu-
ments. In Proceedings of the 31th International
Conference on Machine Learning, ICML 2014, Bei-
jing, China, 21-26 June 2014. pages 1188–1196.
http://jmlr.org/proceedings/papers/v32/le14.html.

Henry Lieberman, Fabio Paternò, Markus Klann, and
Volker Wulf. 2006. End-user development: An
emerging paradigm. In End user development,
Springer, pages 1–8.

Greg Little, Tessa A Lau, Allen Cypher, James Lin,
Eben M Haber, and Eser Kandogan. 2007. Koala:

capture, share, automate, personalize business pro-
cesses on the web. In Proceedings of the SIGCHI
conference on Human factors in computing systems.
ACM, pages 943–946.

Christopher D. Manning, Mihai Surdeanu, John
Bauer, Jenny Finkel, Steven J. Bethard,
and David McClosky. 2014. The Stanford
CoreNLP natural language processing toolkit.
In Association for Computational Linguistics
(ACL) System Demonstrations. pages 55–60.
http://www.aclweb.org/anthology/P/P14/P14-5010.

Fabio Paternò. 2013. End user development: Survey
of an emerging field for empowering people. ISRN
Software Engineering 2013.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks. ELRA, Valletta,
Malta, pages 45–50. http://is.muni.cz/
publication/884893/en.

Juliano Sales, Siegfried Handschuh, and André Freitas.
2017. SemEval-2017 Task 11: End-User Develop-
ment using Natural Language. In Proceedings of
the 11th International Workshop on Semantic Eval-
uation (SemEval-2017). Association for Computa-
tional Linguistics, Vancouver, Canada, pages 554–
562. http://www.aclweb.org/anthology/S17-2092.

Karen Spärck Jones. 1972. A statistical interpretation
of term specificity and its application in retrieval.
Journal of Documentation 28(1):11–21.

1004


