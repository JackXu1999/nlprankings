Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 904–912,

Beijing, August 2010

904

2D Trie for Fast Parsing

Xian Qian, Qi Zhang, Xuanjing Huang, Lide Wu

Institute of Media Computing

School of Computer Science, Fudan University

{xianqian, qz, xjhuang, ldwu}@fudan.edu.cn

Abstract

In practical applications, decoding speed
is very important. Modern structured
learning technique adopts template based
method to extract millions of features.
Complicated templates bring about abun-
dant features which lead to higher accu-
racy but more feature extraction time. We
propose Two Dimensional Trie (2D Trie),
a novel efﬁcient feature indexing structure
which takes advantage of relationship be-
tween templates: feature strings generated
by a template are preﬁxes of the features
from its extended templates. We apply
our technique to Maximum Spanning Tree
dependency parsing. Experimental results
on Chinese Tree Bank corpus show that
our 2D Trie is about 5 times faster than
traditional Trie structure, making parsing
speed 4.3 times faster.

1 Introduction
In practical applications, decoding speed is very
important. Modern structured learning technique
adopts template based method to generate mil-
lions of features. Such as shallow parsing (Sha
and Pereira, 2003), named entity recognition
(Kazama and Torisawa, ), dependency parsing
(McDonald et al., 2005), etc.

The problem arises when the number of tem-
plates increases, more features generated, mak-
ing the extraction step time consuming. Espe-
cially for maximum spanning tree (MST) depen-
dency parsing, since feature extraction requires
quadratic time even using a ﬁrst order model. Ac-
cording to Bohnet’s report (Bohnet, 2009), a fast

Figure 1: Flow chart of dependency parsing.
p0.word, p0.pos denotes the word and POS tag
of parent node respectively. Indexes correspond
to the features conjoined with dependency types,
e.g., lucky/ADJ/OBJ, lucky/ADJ/NMOD, etc.

feature extraction beside of a fast parsing algo-
rithm is important for the parsing and training
speed. He takes 3 measures for a 40X speedup,
despite the same inference algorithm. One impor-
tant measure is to store the feature vectors in ﬁle
to skip feature extraction, otherwise it will be the
bottleneck.

Now we quickly review the feature extraction
stage of structured learning. Typically, it consists
of 2 steps. First, features represented by strings
are generated using templates. Then a feature in-
dexing structure searches feature indexes to get
corresponding feature weights. Figure 1 shows
the ﬂow chart of MST parsing, where p0.word,
p0.pos denote the word and POS tag of parent
node respectively.

We conduct a simple experiment to investi-
gate decoding time of MSTParser, a state-of-the-
art java implementation of dependency parsing 1.
Chinese Tree Bank 6 (CTB6) corpus (Palmer and

1http://sourceforge.net/projects/mstparser

Feature Generation

Template:

p .word+p .pos

0

0

Feature:
lucky/ADJ

Feature
Retrieval

Parse Tree

Index:

3228~3233

Build lattice, inference etc.

905

Step

Time

Feature

Generation

300.27

Index

Retrieval

61.66

Other

Total

59.48

421.41

Table 1: Time spent of each step (seconds) of
MSTParser on CTB6 standard test data (2660 sen-
tences). Details of the hardware and corpus are
described in section 5

Xue, 2009) with standard train/development/test
split is used for evaluation. Experimental results
are shown in Table 1. The observation is that time
spent of inference is trivial compared with feature
extraction. Thus, speeding up feature extraction is
critical especially when large template set is used
for high accuracy.

General indexing structure such as Hash and
Trie does not consider the relationships between
templates, therefore they could not speed up fea-
ture generation, and are not completely efﬁcient
for searching feature indexes. For example, fea-
ture string s1 generated by template “p0.word”
is preﬁx of feature s2 from template “p0.word +
c0.word” (word pair of parent and child), hence
index of s1 could be used for searching s2. Fur-
ther more, if s1 is not in the feature set, then s2
must be absent, its generation can be skipped.

We propose Two Dimensional Trie (2D Trie),
a novel efﬁcient feature indexing structure which
takes advantage of relationship between tem-
plates. We apply our technique to Maximum
Spanning Tree dependency parsing. Experimental
results on CTB6 corpus show that our 2D Trie is
about 5 times faster than traditional Trie structure,
making parsing speed 4.3 times faster.

The paper is structured as follows: in section 2,
we describe template tree which represents rela-
tionship between templates; in section 3, we de-
scribe our new 2D Trie structure; in section 4, we
analyze the complexity of the proposed method
and general string indexing structures for parsing;
experimental results are shown in section 5; we
conclude the work in section 6.

2 Template tree
2.1 Formulation of template
A template is a set of template units which are
manually designed: T = {t1, . . . , tm}. For con-

Unit
p−i/pi
c−i/ci
r−i/ri
n.word
n.pos
n.length word length of node n

Meaning
the ith node left/right to parent node
the ith node left/right to child node
the ith node left/right to root node
word of node n
POS tag of node n

|l
|d

conjoin current feature with linear distance
between child node and parent node
conjoin current feature with direction of de-
pendency (left/right)

Table 2: Template units appearing in this paper

venience, we use another formulation: T = t1 +
. . .+tm. All template units appearing in this paper
are described in Table 2, most of them are widely
used. For example, “T = p0.word + c0.word|l ”
denotes the word pair of parent and child nodes,
conjoined with their distance.

2.2 Template tree
In the rest of the paper, for simplicity, let si be a
feature string generated by template Ti.

We deﬁne the relationship between templates:
T1 is the ancestor of T2 if and only T1 ⊂ T2, and
T2 is called the descendant of T1. Recall that,
feature string s1 is preﬁx of feature s2. Suppose
T3 ⊂ T1 ⊂ T2, obviously, the most efﬁcient way
to look up indexes of s1, s2, s3 is to search s3 ﬁrst,
then use its index id3 to search s1, and ﬁnally use
id1 to search s2. Hence the relationship between
T2 and T3 can be neglected.

Therefore we deﬁne direct ancestor of T1: T2
is a direct ancestor of T1 if T2 ⊂ T1, and there is
no template T 0 such that T2 ⊂ T 0 ⊂ T1. Corre-
spondingly, T1 is called the direct descendant of
T2.

Template graph G = (V, E) is a directed graph
that represents the relationship between templates,
where V = {T1, . . . , Tn} is the template set, E =
{e1, . . . , eN} is the edge set. Edge from Ti to Tj
exists, if and only if Ti is the direct ancestor of
Tj. For templates having no ancestor, we add an
empty template as their common direct ancestor,
which is also the root of the graph.

The left part of Figure 2 shows a template
graph for templates T1 =p0.word, T2 =p0.pos ,
T3 =p0.word + p0.pos. In this example, T3 has 2
direct ancestors, but in fact s3 has only one preﬁx

906

Figure 2: Left graph shows template graph for
T1 =p0.word, T2 =p0.pos , T3 =p0.word +
p0.pos. Right graph shows the corresponding tem-
plate tree, where each vertex saves the subset of
template units that do not belong to its father

Figure 3: Templates that are partially overlapped:
Tred ∩ Tblue =p0.word, virtual vertexes shown in
dashed circle are created to extract the common
unit

which depends on the order of template units in
generation step. If s3 = s1 + s2, then its preﬁx is
s1, otherwise its preﬁx is s2. In this paper, we sim-
ply use the breadth-ﬁrst tree of the graph for dis-
ambiguation, which is called template tree. The
only direct ancestor T1 of T2 in the tree is called
father of T2, and T2 is a child of T1. The right
part of Figure 2 shows the corresponding template
tree, where each vertex saves the subset of tem-
plate units that do not belong to its father.

2.3 Virtual vertex
Consider the template tree in the left part of Figure
3, red vertex and blue vertex are partially over-
lapped, their intersection is p0.word, if string s
from template T =p0.word is absent in feature set,
then both nodes can be neglected. For efﬁciently
pruning candidate templates, each vertex in tem-
plate tree is restricted to have exactly one template
unit (except root). Another important reason for
such restriction will be given in the next section.
To this end, virtual vertexes are created for
multi-unit vertexes. For efﬁcient pruning, the new
virtual vertex should extract the most common
template unit. A natural goal is to minimize the
creation number. Here we use a simple greedy
strategy, for the vertexes sharing a common fa-
ther, the most frequent common unit is extracted
as new vertex. Virtual vertexes are iteratively cre-
ated in this way until all vertexes have one unit.
The ﬁnal template tree is shown in the right part of
Figure 3, newly created virtual vertexes are shown
in dashed circle.

Figure 4: 2D Trie for single template, alphabets at
level 1 and level 2 are the word set, POS tag set
respectively

3 2D Trie
3.1 Single template case
Trie stores strings over a ﬁxed alphabet, in our
case, feature strings are stored over several alpha-
bets, such as word list, POS tag list, etc. which are
extracted from training corpus.

To illustrate 2D Trie clearly, we ﬁrst consider a
simple case, where only one template used. The
template tree degenerates to a sequence, we could
use a Trie like structure for feature indexing, the
only difference from traditional Trie is that nodes
at different levels could have different alphabets.
One example is shown in Figure 4. There are 3
feature strings from template “p0.word + p0.pos”:
{parse/VV, tag/VV, tag/VV}. Alphabets at level
1 and level 2 are the word set, POS tag set re-
spectively, which are determined by correspond-
ing template vertexes.

As mentioned before, each vertex in template
tree has exactly one template unit, therefore, at
each level, we look up an index of a word or POS

root

root

p .word

0

p .pos

0

p .word

0

p .pos

0

p .word + p pos

0.

0

p .pos

0

root

root

0

p .word
+p pos

0.

0

c .word
+c pos

0.

p .word+p .word

-1

0

+p .word

1

p .word

0

c .word

0

p .word

-1

p .pos

0

c .pos

0

p .word

1

root

Level 0

p .word

Level 1

...

parse

...

tag

...

Level 2

...

VV

...

...

NN

...

VV ...

p .pos

0

907

Figure 5: Look up indexes of words and POS tags
beforehand.

tag in sentence, not their combinations. Hence the
number of alphabets is limited, and all the indexes
could be searched beforehand for reuse, as shown
in Figure 5, the token table is converted to a in-
dex table. For example, when generating features
at position i of a sentence, template “r0.word +
r1.word” requires index of i + 1th word in the sen-
tence, which could be reused for generation at po-
sition i + 1.

3.2 General case
Generally, for vertex in template tree with K chil-
dren, children of corresponding Trie node are ar-
ranged in a matrix of K rows and L columns, L
is the size of corresponding alphabet. If the vertex
is not virtual, i.e., it generates features, one more
row is added at the bottom to store feature indexes.
Figure 6 shows the 2D Trie for a general template
tree.

3.3 Feature extraction
When extracting features for a pair of nodes in a
sentence, template tree and 2D Trie are visited in
breath ﬁrst traversal order. Each time, an alpha-
bet and a token index j from index table are se-
lected according to current vertex. For example,
POS tag set and the index of the POS tag of par-
ent node are selected as alphabet and token index
respectively for vertex “p0.pos”. Then children in
the jth column of the Trie node are visited, valid
children and corresponding template vertexes are
saved for further retrieval or generate feature in-
dexes if the child is at the bottom and current Trie
node is not virtual. Two queues are maintained to

Figure 6: 2D trie for a general template tree.
Dashed boxes are keys of columns, which are not
stored in the structure

save the valid children and Trie nodes. Details of
feature extraction algorithm are described in Al-
gorithm 1.

3.4 Implementation
When feature set is very large, space complexity
of 2D Trie is expensive. Therefore, we use Double
Array Trie structure (Aoe, 1989) for implementa-
tion. Since children of 2D Trie node are arranged
in a matrix, not an array, so each element of the
base array has a list of bases, not one base in stan-
dard structure. For children that store features,
corresponding bases are feature indexes. One ex-
ample is shown in Figure 7. The root node has
3 bases that point to three rows of the child ma-
trix of vertex “p0.word” respectively. Number of
bases in each element need not to be stored, since
it can be obtained from template vertex in extrac-
tion procedure.

Building algorithm is similarly to Double Array
Trie, when inserting a Trie node, each row of the
child matrix is independently insert into base and
check arrays using brute force strategy. The inser-

He
had
been
a
sales
and
marketing
executive
with
Chrysler
for
20
years

PRP
VBD
VBN
DT
NNS
CC
NN
NN
IN
NNP
IN
CD
NNS

2648
2731
1121
0411
5064
0631
3374
1923
6023
1560
2203
0056
6778

21
27
28
04
13
01
12
12
06
13
06
02
14

root

p .word

0

root

p .pos

0

c .word

0

...
...
...
...

VBN

been

invalid

p .word→been

0

...
...
...
...

had

p .word→had

0

VBD

p .word+p .pos

0

0

→had/VBD

...
...
...
...

...
...

...
...

...
...

...
...

p .word+p .pos

0

0

→been/VBN

He

p .word+w .word

0

0

...
...

...
...

been

p .word+w .word

0

0

...
...

→

had/He

→had/been

...

nmod

...

-1

obj

sub
3327 2510

vmod

...

nmod

-1

...

-1

obj
-1

sub
-1

vmod
7821

...

...

Feature index array

908

Figure 7: Build base array for 2D Trie in Figure 6. String in the box represents the key of the child.
Blank boxes are the invalid children. The root node has 3 bases that point to three rows of the child
matrix of vertex “p0.word” respectively

Algorithm 1 Feature extraction using 2D Trie
Input: 2D Trie that stores features, template
tree, template graph, a table storing token in-
dexes, parent and child positions
Output: Feature index set S of dependency
from parent to child.
Create template vertex queue Q1 and Trie
node queue Q2. Push roots of template tree
and Trie into Q1, Q2 respectively. S = ∅
while Q1 is not empty, do

Pop a template vertex T from Q1 and a Trie
node N from Q2. Get token index j from
index table according to T .
for i = 1 to child number of T

if child of N at row i column j is valid,
push it into Q2 and push the ith child
of T into Q1.

else

remove decedents of ith child of T
from template tree

end if
end for
if T is not virtual and the last child of N in
column j is valid

Enumerate dependency types, add
valid feature indexes to S

end if

end while
Return S.

tion repeats recursively until all features stored.

4 Complexity analysis
Let

• |T| = number of templates
• |t| = number of template units
• |V | = number of vertexes in template tree,

i.e, |t|+ number of virtual vertexes

• |F| = number of features
• l = length of sentence
• |f| = average length of feature strings

The procedure of 2D Trie for feature extraction
consists of 2 steps:
tokens in string table are
mapped to their indexes, then Algorithm 1 is car-
ried out for all node pairs of sentence. In the ﬁrst
step, we use double array Trie for efﬁcient map-
ping. In fact, time spent is trivial compared with
step 2 even by binary search. The main time spent
of Algorithm 1 is the traversal of the whole tem-
plate tree, in the worst case, no vertexes removed,
so the time complexity of a sentence is l2|V |,
which is proportional to |V |. In other words, mini-
mizing the number of virtual vertexes is important
for efﬁciency.

For other indexing structures, feature genera-
tion is a primary step of retrieval. For each node

b a s e 1

. . .

b a s e 2

. . .

root

base 3

. . .

been

base 1

had

. . .

. . .

had

. . .

. . .

. . .

base 1

VBN

. . .

been

. . .

. . .

had

. . .

. . .

VBD

. . .

. . .

. . .

. . .

. . .

-1

3327 2510

-1

. . .

. . .

. . .

. . .

-1

-1

-1

7821

. . .

. . .

. . .

Feature index array

base 3

base 1

base 2

base 1

base 1

root

. . .

. . .

. . .

been

been

. . .

had

had

had

. . .

. . .

. . .

. . .

. . .

Base array

VBN

VBD

. . .

Feature index array

. . .

. . .

. . .

. . .

-1

3327 2510

-1

. . .

. . .

. . .

. . .

-1

-1

-1

7821

. . .

. . .

. . .

909

Structure
2D Trie

Hash / Trie

Binary Search

Generation

Retrieval

l2|V |

l2|f||T|
l2|T| log |F|

l2|t|
l2|t|

Table 3: Time complexity of different indexing
structures.

pair of sentence, |t| template units are processed,
including concatenations of tokens and split sym-
bols (split tokens in feature strings), boundary
check ( e.g, p−1.word is out of boundary for be-
ginning node of sentence). Thus the generation
requires l2|t| processes. Notice that, time spent of
each process varies on the length of tokens.
For feature string s with length |s|, if perfect
hashing technique is adopted for index retrieval, it
takes |s| calculations to get hash value and a string
comparison to check the string at the calculated
position. So the time complexity is proportional to
|s|, which is the same as Trie. Hence the total time
for a sentence is l2|f||T|. If binary search is used
instead, log |F| string comparisons are required,
complexity for a sentence is l2|T| log |F|.
Time complexity of these structures is summa-
rized in Table 3.

5 Experiments
5.1 Experimental settings
We use Chinese Tree Bank 6.0 corpus for evalua-
tion. The constituency structures are converted to
dependency trees by Penn2Malt 2 toolkit and the
standard training/development/test split is used.
257 sentences that failed in the conversion were
removed, yielding 23316 sentences for training,
2060 sentences for development and 2660 sen-
tences for testing respectively.

Since all the dependency trees are projective,
a ﬁrst order projective MST parser is naturally
adopted. Online Passive Aggressive algorithm
(Crammer et al., 2006) is used for fast training, 2
parameters, i.e, iteration number and C, are tuned
on development data. The quality of the parser is
measured by the labeled attachment score (LAS),
i.e., the percentage of tokens with correct head and
dependency type.

2http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html

Group

1
2
3
4

IDs
1-2
1-3
1-4
1-5

#Temp.

#Vert.

72
128
240
332

91
155
275
367

LAS
#Feat.
3.23M 79.55%
10.4M 81.38%
25.0M 81.97%
34.8M 82.44%

Table 5: Parsing accuracy and number of tem-
plates, vertexes in template tree, features in decod-
ing stage (zero weighted features are excluded) of
each group.

We compare the proposed structure with Trie
and binary search. We do not compare with per-
fect hashing, because it has the same complex-
ity as Trie, and is often used for large data base
retrieval, since it requires only one IO opera-
tion. For easy comparison, all feature indexing
structures and the parser are implemented with
C++. All experiments are carried out on a 64bit
linux platform (CPU: Intel(R) Xeon(R) E5405,
2.00GHz, Memory: 16G Bytes). For each tem-
plate set, we run the parser ﬁve times on test data
and the averaged parsing time is reported.

5.2 Parsing speed comparison
To investigate the scalability of our method, rich
templates are designed to generate large feature
sets, as shown in Table 4. All templates are orga-
nized into 4 groups. Each row of Table 5 shows
the details of a group, including parsing accu-
racy and number of templates, vertexes in tem-
plate tree, and features in decoding stage (zero
weighted features are excluded).

There is a rough trend that parsing accuracy
increases as more templates used. Though such
trend is not completely correct, the clear conclu-
sion is that, abundant templates are necessary for
accurate parsing.

Though algorithm described in section 2.3 for
minimizing the number of virtual vertexes is
heuristic, empirical results are satisfactory, num-
ber of newly created vertexes is only 10% as orig-
inal templates. The reason is that complex tem-
plates are often extended from simple ones, their
differences are often one or two template units.

Results of parsing time comparison are shown
in Table 6. We can see that though time com-
plexity of dynamic programming is cubic, pars-
ing time of all systems is consistently dominated

910

ID
1

2

3
4
5

Templates

d

pi.word
ci.word
pi.length
ci.length
p0.length+c0.length|l
p0.length+p0.pos+c0.pos|l
pi.length+pj.length+ck.length+cm.length|l
r0.word
r0.pos
pi.pos+cj.pos|d
pi.word+pi.pos+cj.pos|d
pi.word+pi.pos+cj.word+cj.pos|d

pi.pos
ci.pos
pi.length+pi.pos
ci.length+ci.pos
p0.length+c0.length+c0.pos|l
p0.pos+c0.length+c0.pos|l
r−1.word+r0.word
r−1.pos+r0.pos
pi.word+cj.word|d
pi.word+pi.pos+cj.word|d

d

d

d

d

pi.word+pi.pos
ci.word+ci.pos

(|i| ≤ 2)
(|i| ≤ 1)

d

p0.length+p0.pos+c0.length|l
p0.length+p0.pos+c0.length+c0.pos|l
r0.word+r1.word
r0.pos+r1.pos
pi.pos+cj.word+cj.pos|d
pi.word+cj.word+cj.pos|d

(|i| + |j| + |k| + |m| ≤ 2)

d

(|i| + |j| = 0)

pi.word + pj.word + ck.word|d
pi.pos + pj.pos + ck.pos|d
pi.word + pj.word + pk.word + cm.word|d
pi.word + cj.word + ck.word + cm.word|d
pi.pos + pj.pos + pk.pos + cm.pos|d
pi.pos + cj.pos + ck.pos + cm.pos|d

Conjoin templates in the row above with |l

Similar with 2 |i| + |j| = 1
Similar with 2 |i| + |j| = 2
pi.word + cj.word + ck.word|d
pi.pos + cj.pos + ck.pos|d

Conjoin templates in the row above with |l

Conjoin templates in the row above with |l

(|i| + |j| + |k| ≤ 2)
pi.word + pj.word + ck.word + cm.word|d
pi.pos + pj.pos + ck.pos + cm.pos|d

(|i| + |j| + |k| + |m| ≤ 2)

Table 4: Templates used in Chinese dependency parsing.

by feature extraction. When efﬁcient indexing
structure adopted, i.e, Trie or Hash, time index re-
trieval is greatly reduced, about 4-5 times faster
than binary search. However, general structures
search features independently, their results could
not guide feature generation. Hence, feature gen-
eration is still time consuming. The reason is that
processing each template unit includes a series of
steps, much slower than one integer comparison
in Trie search.

On the other hand, 2D Trie greatly reduces the
number of feature generations by pruning the tem-
plate graph. In fact, no string concatenation oc-
curs when using 2D Trie, since all tokens are con-
verted to indexes beforehand. The improvement
is signiﬁcant, 2D Trie is about 5 times faster than
Trie on the largest feature set, yielding 13.4 sen-
tences per second parsing speed, about 4.3 times
faster.

Space requirement of 2D Trie is about 2.1 times
as binary search, and 1.7 times as Trie. One possi-
ble reason is that column number of 2D Trie (e.g.
size of words) is much larger than standard double
array Trie, which has only 256 children, i.e, range
of a byte. Therefore, inserting a 2D Trie node is
more strict, yielding sparser double arrays.

5.3 Comparison against state-of-the-art

Recent works on dependency parsing speedup
mainly focus on inference, such as expected
linear time non-projective dependency parsing
(Nivre, 2009), integer linear programming (ILP)
for higher order non-projective parsing (Martins
et al., 2009). They achieve 0.632 seconds per sen-
tence over several languages. On the other hand,
Goldberg and Elhadad proposed splitSVM (Gold-
berg and Elhadad, 2008) for fast low-degree poly-
nomial kernel classiﬁers, and applied it to transi-
tion based parsing (Nivre, 2003). They achieve
53 sentences per second parsing speed on En-
glish corpus, which is faster than our results, since
transition based parsing is linear time, while for
graph based method, complexity of feature ex-
traction is quadratic. Xavier Llu´ıs et al.
(Llu´ıs
et al., 2009) achieve 8.07 seconds per sentence
speed on CoNLL09 (Hajiˇc et al., 2009) Chinese
Tree Bank test data with a second order graphic
model. Bernd Bohnet (Bohnet, 2009) also uses
second order model, and achieves 610 minutes on
CoNLL09 English data (2399 sentences, 15.3 sec-
ond per sentence). Although direct comparison
of parsing time is difﬁcult due to the differences
in data, models, hardware and implementations,

911

Group

Structure

1

2

3

4

Trie

Binary Search

2D Trie

Trie

Binary Search

2D Trie

Trie

Binary Search

2D Trie

Trie

Binary Search

2D Trie

Total
87.39
127.84
39.74
264.21
430.23
72.81
620.29
982.41
146.83
854.04
1328.49
198.31

Generation Retrieval Other Memory

63.67
62.68

26.29

205.19
212.50

53.95

486.40
484.62

119.56

677.32
680.36

160.38

10.33
51.52

39.74
198.72

105.96
469.44

139.70
609.70

13.39
13.64
13.45
19.28
19.01
18.86
27.93
28.35
27.27
37.02
38.43
37.93

402M
340M
700M
1.3G
1.2G
2.5G
3.2G
2.9G
5.9G
4.9G
4.1G
8.6G

sent/sec
30.44
20.81
66.94
10.07
6.18
36.53
4.29
2.71
18.12
3.11
2.00
13.41

Table 6: Parsing time of 2660 sentences (seconds) on a 64bit linux platform (CPU: Intel(R) Xeon(R)
E5405, 2.00GHz, Memory: 16G Bytes). Title “Generation” and “Retrieval” are short for feature gen-
eration and feature index retrieval steps respectively.

System

(Martins et al., 2009)

(Goldberg and Elhadad, 2008)

(Llu´ıs et al., 2009)

(Bohnet, 2009)

(Galley and Manning, 2009)

ours group1
ours group2
ours group3
ours group4

sec/sent

0.63
0.019
8.07
15.3
15.6
0.015
0.027
0.055
0.075

Table 7: Comparison against state of the art, di-
rect comparison of parsing time is difﬁcult due to
the differences in data, models, hardware and im-
plementations.

these results demonstrate that our structure can
actually result in a very fast implementation of a
parser. Moreover, our work is orthogonal to oth-
ers, and could be used for other learning tasks.

6 Conclusion

We proposed 2D Trie, a novel feature indexing
structure for fast template based feature extrac-
tion. The key insight is that feature strings gener-
ated by a template are preﬁxes of the features from
its extended templates, hence indexes of searched
features can be reused for further extraction. We
applied 2D Trie to dependency parsing task, ex-
perimental results on CTB corpus demonstrate the
advantages of our technique, about 5 times faster

than traditional Trie structure, yielding parsing
speed 4.3 times faster, while using only 1.7 times
as much memory.

7 Acknowledgments
The author wishes to thank the anonymous
reviewers for their helpful comments.
This
work was partially funded by 973 Program
(2010CB327906), The National High Technol-
ogy Research and Development Program of China
(2009AA01A346), Shanghai Leading Academic
Discipline Project (B114), Doctoral Fund of Min-
istry of Education of China (200802460066), and
Shanghai Science and Technology Development
Funds (08511500302).

References
Aoe,

1989.

Jun’ichi.

An efﬁcient digital
search algorithm by using a double-array struc-
ture. IEEE Transactions on software andengineer-
ing, 15(9):1066–1077.

Bohnet, Bernd. 2009. Efﬁcient parsing of syntactic
and semantic dependency structures.
In Proceed-
ings of the Thirteenth Conference on Computational
Natural Language Learning (CoNLL 2009): Shared
Task, pages 67–72, Boulder, Colorado, June. Asso-
ciation for Computational Linguistics.

Crammer, Koby, Joseph Keshet, Shai Shalev-Shwartz,
and Yoram Singer. 2006. Online passive-aggressive
algorithms. In JMLR 2006.

912

the 11th International Conference on Parsing Tech-
niques, pages 149–160.

Nivre, Joakim.

2009. Non-projective dependency
parsing in expected linear time. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 351–359, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.

Palmer, Martha and Nianwen Xue. 2009. Adding se-
mantic roles to the Chinese Treebank. Natural Lan-
guage Engineering, 15(1):143–172.

Sha, Fei and Fernando Pereira. 2003. Shallow pars-
ing with conditional random ﬁelds. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 134–141,
May.

Galley, Michel and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine
translation.
In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 773–781,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.

Goldberg, Yoav and Michael Elhadad. 2008. splitsvm:
Fast, space-efﬁcient, non-heuristic, polynomial ker-
nel computation for nlp applications.
In Proceed-
ings of ACL-08: HLT, Short Papers, pages 237–240,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.

Hajiˇc, Jan, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages.
In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 1–18, Boulder, Colorado, June. Association
for Computational Linguistics.

Kazama, Jun’ichi and Kentaro Torisawa. A new per-
ceptron algorithm for sequence labeling with non-
local features.
In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 315–324.

Llu´ıs, Xavier, Stefan Bott, and Llu´ıs M`arquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 79–84, Boulder, Colorado, June. Association
for Computational Linguistics.

Martins, Andre, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 342–
350, Suntec, Singapore, August. Association for
Computational Linguistics.

McDonald, Ryan, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 91–97. Association for Computa-
tional Linguistics.

Nivre, Joakim.

projective dependency parsing.

2003. An efﬁcient algorithm for
In Proceedings of

