










































Updating Rare Term Vector Replacement


International Joint Conference on Natural Language Processing, pages 1002–1006,
Nagoya, Japan, 14-18 October 2013.

Updating Rare Term Vector Replacement

Tobias Berka†
†Department of Computer Sciences

University of Salzburg
{tberka,marian}@cosy.sbg.ac.at

Marian Vajteršic†‡
‡Department of Informatics

Mathematical Institute
Slovak Academy of Sciences

Abstract

Rare term vector replacement (RTVR) is a
novel technique for dimensionality reduc-
tion. In this paper, we introduce an up-
dating algorithm for RTVR. It is capable
of updating both the projection matrix for
the reduction and the reduced corpus ma-
trix directly, without having to recompute
the expensive projection operation. We in-
troduce an effective batch updating algo-
rithm, and present performance measure-
ments on a subset of the Reuters newswire
corpus that show that a 12.5% to 50% split
of the documents into corpus and update
vectors leads to a three to four fold speed-
up over a complete rebuild. Thus, we have
enabled optimized updating for rare term
vector replacement.

1 Introduction

Rare term vector replacement (RTVR) is a re-
cently developed linear dimensionality reduction
technique for term frequency vectors (Berka and
Vajteršic, 2011). It is easily and rapidly computed,
patent-free, and produces a semantically meaning-
ful multivariate space with a significantly reduced
dimensionality. Furthermore, the method has been
parallelized for data and task parallelism (Berka
and Vajteršic, 2013).

The construction of this representation is based
on document-scope term cooccurrences. Rare
terms that occur only in δ documents or fewer
are eliminated by replacing them with the average
vectors of the documents that contain them. The
replacement vectors of the rare terms are added
to the original document vectors. Then, all rows
for rare terms are dropped from all vectors. Only
the common terms remain of the original term
frequency vector space. By Zipf’s law (Powers,
1998), we know that this operation will eliminate

a high number of terms and lead to a highly con-
densed representation. The performance of the re-
placement now depends on the applicability of the
cluster hypothesis (Raiber and Kurland, 2012; Ri-
jsbergen, 1979), i.e., that documents in close prox-
imity are semantically related. If the cluster hy-
pothesis holds, then the centroid of the containing
documents will act as a succint representation of
all documents containing the replacement term.

The most prominent techniques for dimension-
ality reduction of text data in published litera-
ture are latent semantic indexing (LSI), see (Deer-
wester et al., 1990) and the related COV ap-
proach (Kobayashi et al., 2002). These two
methods are applications of a principal compo-
nent analysis to text using unbiased and biased
correlation measures. Factor analysis based on
the SVD applied to automated indexing has been
reported as probabilistic latent semantic analy-
sis (PLSA) (Hofmann, 1999). Updating opera-
tions for LSI are well understood (Zha and Si-
mon, 1999), and are also being developed for
PLSA (Bassiou and Kotropoulos, 2011), or other
dimensionality reduction methods such as the ker-
nel PCA (Mastronardi et al., 2010).

Through the connection between PLSA and La-
tent Dirichlet Allocation (LDA) (Girolami and
Kabán, 2003), topic models are also related to di-
mensionality reduction. Positive matrix factoriza-
tion methods are also used in various text analysis
tasks (Zhang, 2010). Structurally, the generalized
vector space model (GVSM) (Wong et al., 1985),
is similar to RTVR because of the construction of
index vectors by linear combination. The random
index vector representation (Kanerva et al., 2000)
is also based on cooccurrences, but operates on
random initial vectors. Random projections can
be used to further accelerate it (Sakai and Imiya,
2009), but this approach should be seen as comple-
mentary because it can be applied to other meth-
ods as well.

1002



Our contribution is the following. In this pa-
per, we rigorously define an algorithm for updat-
ing rare term vector replacement. Using our ap-
proach, both the replacement vectors and the re-
duced corpus matrix are updated directly. It is not
necessary to explicitly recompute the projection
into the reduced-dimensional space.

The remainder of this paper is structured as
thus. Our main contribution is the updating algo-
rithm in Section 2. Section 3 contains a theoretic
and empirical performance evaluation. Lastly, we
summarize our findings in Section 4.

2 Updating RTVR

Updating RTVR has to support the three basic op-
erations of content management: (1) adding new
documents, (2) changing existing documents, and
(3) deleting obsolete documents.

The replacement vectors are weighted cen-
troids, or weighted average vectors, of the docu-
ment vectors containing their terms. The reduced
document vectors are also linear combinations of
the truncated original document vectors and the
replacement vectors. At its core, the update al-
gorithm is therefore a running average computa-
tion. We will use the mathematical notation sum-
marized in Table 1 to describe the algorithm.

A key complication lies in the fact that the
occurrence counts of the terms change. This
means that some rare terms may become common
terms, and therefore become part of the reduced-
dimensional, projected term space. Dually, some
common terms may become rare terms, and drop
out of the projected space. We will refer to these
terms as promoted and demoted terms P and Q.
For demoted terms, we need to compute the re-
placement vectors from scratch during the update.
But for all rare terms that are involved in an up-
date, in the old or new vector, we need to change
the replacement vector. These terms are called af-
fected (rare) terms AT .

We can represent changing a document by a
tuple (i, v) containing a corpus matrix column
i ∈ {1, ..., n} and a new term frequency vector
v ∈ Rm′ , where m′ is the new number of terms.
The other two updating operations can be cast
into the same form by introducing two abstract
symbols. We let ν denote the pseudo-column for
adding new documents, i.e., (ν, dn+1) denotes a
new document that will be added as a new column
of the corpus matrix. For deletions, we let � denote

T set of terms
D set of documents
m number of terms
n number of documents
C corpus matrix
D(ti) set of documents containing ti
T (dj) non-zero terms in document dj
Ni occurrence count for term ti
δ occurrence count threshold
E set of rare terms
τE vector truncation removing in-

dices in E
k reduced dimensionality
π index permutation mapping com-

mon features to reduced feature
indices

R replacement vectors
λ normalizing factors
Ĉ reduced corpus
U bulk update

(i, v) update v for di
(ν, v) insertion of document vector v
(i, �) deletion of document di

old(i, v) old vector for i (or zero)
To(i, v) terms in the old vector

new(i, v) new vector for i (or zero)
Tn(i, v) terms in the new vector
T (i, v) terms in both vectors
T (U) all terms

⋃
(i,v)∈U T (i, v) in U

N ′ new occurrence counts
E′ new rare terms
k′ new reduced dimensionality
π′ new index permutation
P promoted terms {t ∈ E | t 6∈ E′}
Q demoted terms {t ∈ E′ | t 6∈ E}
AT affected terms T (U) \ (P ∪Q)
σ index permutation to remove de-

moted terms
ei i-th standard base

Table 1: Mathematical Notation

an empty pseudo-vector for the deletion of an old
document, i.e., (i, �) signifies the deletion of the
document in column i.

We associate every update u = (i, v) ∈ U with
two term frequency vectors. If the update is not an
add document request, i.e., i 6= ν, it is associated
with an old document vector old(i, v) = C1:m,i.
If it is not a deletion, i.e., v 6= �, it is associated
with a new document vector new(i, v) = v. We

1003



Algorithm 1: Preparing the Update
delete or append terms in T , C, N , R, λ;
N ′ := N ; E′ := E;
for u ∈ U do

Used := Used ∪ T (u);
for ti ∈ (To(u) \ Tn(u)) do N ′i - -;
for ti ∈ (Tn(u) \ To(u)) do N ′i++;

for ti ∈ T do
if (N ′i > δ) ∧ ti ∈ E then

P := P ∪ {ti}; E′ := E′ \ {ti};
else if (N ′i ≤ δ) ∧ ti 6∈ E then

Q := Q ∪ {ti}; E′ := E′ ∪ {ti};
else if (N ′i ≤ δ) ∧ ti ∈ Used then

AT := AT ∪ {ti};

k′′ := k − ‖Q‖; k′ := k′′ + ‖P‖;
σ := 1k′ ; j := 1; l := k′′ + 1;
for ti ∈ T do

if ti ∈ P then π′(i) := l ++;
else if ti ∈ AT then

σ(j) := π(i);
π′(i) := j ++;

else π′(i) := −1;

will need to identify the terms in the old vector
To(i, v), in the new vector Tn(i, v), and the joint
set T (i, v). All terms in the old and new vectors
for the entire update U is defined as T (U).

Our updating algorithm proceeds in three
phases: (1) preparing the update, (2) downdating
the reduced corpus and updating the replacement
vectors, and (3) updating the reduced corpus. In
Algorithm 1, we analyze a batch update and pre-
pare the required sets of features and an index per-
mutation σ to compact the reduced space.

Let us assume that we have a procedure
Compact(A,m′,n,σ), which applies the permuta-
tion σ to the row indices of a matrix. If we
have a matrix A ∈ Rm×n and compute A′ :=
Compact(A,m′, n, σ) ∈ Rm′×n, it holds that
A′i,j = Aσ(i),j . Let us further assume that trunca-
tion of the new elimination terms τE′ respects the
new index order established with σ, i.e., the im-
plementation uses the global index permutation π′

mapping all new features to indices in {1, ..., k′}.
We downdate the reduced corpus and update the

replacement vectors with Algorithm 2. For exist-
ing documents, we downdate the reduced corpus
by subtracting any terms that will change in the
course of the update in Line 1. We then update

the replacement vectors by adding the new docu-
ment vector to any affected or demoted features in
Line 2. Documents that do not change contribute
to the construction of replacement vectors for de-
moted terms, which need to be built from scratch,
in Line 3. These have to be inserted into rows k′′

to k′ of the reduced space, as done in Line 4. We
add in any new documents in Line 5 and normalize
the resulting replacement vectors.

Algorithm 3 updates the reduced vectors by
adding all replacement vectors that have changed
in Line 1. The exception are the promoted terms
on rows k′′ to k′, which must be added for all rare
terms that were otherwise unaffected by the up-
date in Line 2. We then handle all deletions and
insertions in Lines 3 and 4.

3 Performance Evaluation

Regarding the asymptotic complexity of our up-
dating algorithm, we note the following. Assum-
ing amortized constant time for set testing and
insertion (Sedgewick, 2002), that the update is
smaller than the corpus, i.e., ‖U‖ < ‖D‖, and
that the number of documents is greater than the
number of terms, i.e., n > m, the update al-
gorithm can be executed with a complexity of
O((‖D‖+ ‖U‖) nnz k′ +mk′), where nnz is the
expected number of non-zero elements in any doc-
ument vector.

Since the performance is heavily dependent on
the actual distribution of the non-zero elements,
the observed performance may differ somewhat
from this formal analysis. We have conducted
performance measurements using an Intel i5-2557
with 4 GB or RAM running Max OS X 10.7.5. We
have used the first 23,149 documents with 47,236
terms of the Reuters Corpus Volume I, version 2,
in the pre-vectorized form (Lewis et al., 2004).

We randomly selected between 12.5% and 50%
of all vectors for the batch update, using the re-
maining documents for the initial build. Table 2
summarizes our performance measurements aver-
aged across ten runs per row.

The results clearly show that the updating al-
gorithm outperforms a complete rebuild with the
original construction algorithm by three-fold to
four-fold performance improvement. Because
smaller updates require less processing in the up-
dating, and the workload for the rebuild remains
the same, smaller batches have a larger speed-up
than smaller batches.

1004



Algorithm 2: Downdating the Reduced Cor-
pus and Updating the Replacement Vectors

for ti ∈ AT do R∗,i *= λi;

R′ =

[
Compact(R, k′′,m, σ)

0

]
∈ Rk′×m;

for ti ∈ Q do R′∗,i = 0;
for dj ∈ D do

1 for ti ∈ T (dj) do
if (j, d′j) ∈ U ∧ ti ∈ AT then

for l ∈ {1, ..., k′′} do
R′l,i -= Ci,jτE(C∗,j)σ(l);

λi -= |Ci,j |;
if (j, d′j) 6∈ U ∧ ti ∈ AT ∪ P then

Ĉ∗,j -= Ci,jR∗,i;

2 if (j, d′j) ∈ U then
for ti ∈ {ti ∈ T | (d′j)i 6= 0} do

if ti ∈ Q ∪AT then
for l ∈ {1, ..., k′} do

R′l,i += (d
′
j)iτE′(d

′
j)l;

λi += |(d′j)i|;

3 else for ti ∈ T (dj) do
if ti ∈ Q then

for l ∈ {1, ..., k′} do
R′l,i += Ci,jτE′(C∗,j)l;

λi += |Ci,j |;
else if t ∈ E′ then

4 for l ∈ {k′′, ..., k′} do
R′l,i += Ci,jτE′(C∗,j)l;

if (T (dj) ∩ E′) ⊆ P then
λi += |Ci,j |;

5 for (ν, v) ∈ U do
for ti ∈ Tn(ν, v) ∩ E′ do

for l ∈ {1, ..., k′} do
R′l,i += Ci,jτE′(v)l;

λi += |vj |;

for ti ∈ E′ do
if ti ∈ P then R′∗,i = τE′(ei);
else R′∗,i /= λi;

Algorithm 3: Updating the Reduced Corpus

Ĉ ′ =

[
Compact(Ĉ, k′′, n, σ)

0

]
∈ Rk′×n;

Old = E′ \ (P ∪AT ∪Q);
1 for dj ∈ D do

if (j, d′j) ∈ U then
Ĉ ′∗,j=τE′(d

′
j)+
∑

ti∈E′(d
′
j)iR

′
∗,i;

else
Ĉ ′∗,j +=

∑
ti∈P Ci,jτE′(ei);

Ĉ ′∗,j +=
∑

ti∈Q∪AT Ci,jR
′
∗,i;

2 Ĉ ′k′′:k′,j +=
∑

ti∈Old Ci,jR
′
k′′:k′,i;

3 if (j, �) ∈ U then
Ĉ ′ = [ Ĉ ′∗,1:j−1 Ĉ

′
∗,j+1:n ];

n - -;
4 for (ν, v) ∈ U do

v′ = τE′(v) +
∑

ti∈E′ viR
′
∗,i;

Ĉ ′ = [ Ĉ ′ v′ ];
n++;

Size tR tI tU S
12.5% 51.41 43.59 11.44 4.49
25.0% 51.41 40.73 13.81 3.72
37.5% 51.41 34.57 15.15 3.39
50.0% 51.41 28.52 16.12 3.18

Table 2: Performance evaluation (rebuild time tR,
build time tI , update time tU and speed-up S).

4 Summary & Conclusions

In this paper, we have introduced an algorithm for
updating rare term vector replacement. Our em-
pirical performance evaluation demonstrates that
batch updating is faster than a complete rebuild
by a factor of three to four for our experiments.
In our future research, we intend to develop hy-
brid updating algorithms similar to (Tougas and
Spiteri, 2008). These algorithms initially com-
pute fast, approximate updates, which are only
later replaced by exact updates for efficiency. The
final PCA of the augmented corpus Ĉ reported
in (Berka and Vajteršic, 2011) remains an open
problem in updating RTVR.

Acknowledgements

We acknowledge the support of the Slovak Min-
istry of Education and Slovak Academy of Sci-
ences under VEGA grant no. 2/0003/11.

1005



References
N. Bassiou and C. Kotropoulos. 2011. RPLSA:

A Novel Updating Scheme for Probabilistic La-
tent Semantic Analysis. Comput. Speech Lang.,
25(4):741–760.

T. Berka and M. Vajteršic. 2011. Dimensionality Re-
duction for Information Retrieval using Vector Re-
placement of Rare Terms. In Proc. TMW.

T. Berka and M. Vajteršic. 2013. Parallel Rare Term
Vector Replacement: Fast and Effective Dimension-
ality Reduction for Text. J. Parallel Distr. Com.,
73(3):341–351.

S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by
Latent Semantic Analysis. JASIS, 41(6):391–407.

M. Girolami and A. Kabán. 2003. On an Equivalence
Between PLSI and LDA. In Proc. SIGIR, pages
433–434, USA. ACM.

T. Hofmann. 1999. Probabilistic Latent Semantic In-
dexing. In Proc. SIGIR, pages 50–57, USA. ACM.

P. Kanerva, J. Kristoferson, and A. Holst. 2000. Ran-
dom Indexing of Text Samples for Latent Seman-
tic Analysis. In Proc. CogSci, pages 103–106. Erl-
baum.

M. Kobayashi, M. Aono, H. Takeuchi, and
H. Samukawa. 2002. Matrix Computations
for Information Retrieval and Major and Out-
lier Cluster Detection. J. Comput. Appl. Math.,
149(1):119 – 129.

D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. 2004.
RCV1: A New Benchmark Collection for Text Cat-
egorization Research. JMLR, 5:361–397.

N. Mastronardi, E. E. Tyrtyshnikov, and P. Van Dooren.
2010. A Fast Algorithm for Updating and Down-
sizing the Dominant Kernel Principal Components.
SIAM J. Matrix Anal. Appl., 31(5):2376–2399.

D. M. W. Powers. 1998. Applications and Explana-
tions of Zipf’s Law. In Proc. NeMLaP3/CoNLL,
pages 151–160, USA. ACL.

F. Raiber and O. Kurland. 2012. Exploring the Clus-
ter Hypothesis, and Cluster-Based Retrieval, Over
the Web. In Proc. CIKM, pages 2507–2510, USA.
ACM.

C. J. van Rijsbergen. 1979. Information Retrieval.
Butterworths, London.

T. Sakai and A. Imiya. 2009. Fast Spectral Clustering
with Random Projection and Sampling. In Machine
Learning and Data Mining in Pattern Recognition,
LLNCS, pages 372–384. Springer.

R. Sedgewick. 2002. Algorithms in C++. Addison
Wesley, 3rd edition.

J. E. Tougas and R. J. Spiteri. 2008. Two Uses for
Updating the Partial Singular Value Decomposition
in Latent Semantic Indexing. Appl. Numer. Math.,
58(4):499–510.

S. K. M. Wong, W. Ziarko, and P. C. N. Wong. 1985.
Generalized Vector Spaces Model in Information
Retrieval. In Proc. SIGIR, pages 18–25, USA.
ACM.

H. Zha and H. D. Simon. 1999. On Updating Problems
in Latent Semantic Indexing. SIAM J. Sci. Comput.,
21(2):782–791.

Z.-Y. Zhang. 2010. Survey on the Variations and Ap-
plications of Nonnegative Matrix Factorization Vari-
ations of NMF. Operations Research, pages 317–
323.

1006


