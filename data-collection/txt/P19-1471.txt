



















































Detecting Subevents using Discourse and Narrative Features


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4780–4790
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

4780

Detecting Subevents using Discourse and Narrative Features

Mohammed Aldawsari & Mark A. Finlayson
School of Computing and Information Sciences

Florida International University
Miami FL, 33199

{malda021,markaf}@fiu.edu

Abstract

Recognizing the internal structure of events
is a challenging language processing task of
great importance for text understanding. We
present a supervised model for automatically
identifying when one event is a subevent of an-
other. Building on prior work, we introduce
several novel features, in particular discourse
and narrative features, that significantly im-
prove upon prior state-of-the-art performance.
Error analysis further demonstrates the utility
of these features. We evaluate our model on
the only two annotated corpora with event hi-
erarchies: HiEve and the Intelligence Commu-
nity corpus. No prior system has been evalu-
ated on both corpora. Our model outperforms
previous systems on both corpora, achieving
0.74 BLANC F1 on the Intelligence Commu-
nity corpus and 0.70 F1 on the HiEve corpus,
respectively a 15 and 5 percentage point im-
provement over previous models.

1 Introduction

An event is something that occurs in a certain
place at a certain time (Pustejovsky et al., 2003).
Understanding events plays a major role in various
natural language processing tasks such as infor-
mation extraction (Humphreys et al., 1997), ques-
tion answering (Narayanan and Harabagiu, 2004),
textual entailment (Haghighi et al., 2005), event
coreference (Choubey and Huang, 2018) and con-
tradiction detection (De Marneffe et al., 2008).
There has been a significant amount of work on
automatic processing of events in text including
systems for events extraction, event coreference
resolution, and temporal relation detection (Araki,
2018; Ning et al., 2017). However, events are not
atomic entities: they often have complex inter-
nal structure that can be expressed in a variety of
ways (Huttunen et al., 2002; Bejan and Harabagiu,
2008; Hovy et al., 2013).

One of the unsolved problems related to event
understanding is the detection of subevents, also
referred to as event hierarchy construction. As de-
scribed by Glavaš and Šnajder (2014a), there have
been efforts that have focused on detecting tem-
poral and spatial subevent containment individu-
ally. However, it is clear that subevent detection
requires both simultaneously. The subevent rela-
tionship is defined in terms of (e1,e2), where e1
and e2 are events: event e2 is a subevent of event
e1 if e2 is spatiotemporally contained by e1. More
precisely, we say that an event e1 is a parent event
of event e2, and e2 is a child event of e1 if (1)
e1 is collector event that contains a complex se-
quence of activities; (2) e2 is one of these activ-
ities; and (3) e2 is spatially and temporally con-
tained within e1 (i.e., e2 occur at the same time
and same place as e1) (Hovy et al., 2013; Glavaš
and Šnajder, 2014b). This subevent relationship is
independent of other types of relationships, e.g.,
causal relationship between the events. Example 1
illustrates a text expression of a complex event hi-
erarchy. Figure 1 shows a corresponding graphical
representation of the hierarchy.

Egyptian police have said that five
protesters were killed1 when they were
attacked2 by an armed group near the
Defense Ministry building in Cairo. The
statement said that early this morning,
the armed group attacked3 the demon-
strators who have for days been staging
their protest4 against the military gov-
ernment. . . . Police said that the attack5
on Wednesday wounded6 at least 50
protesters.

Example 1: Excerpt from the HiEve corpus (Glavaš
et al., 2014a). Events are in bold and given a numerical
subscript for reference. In all the examples the identi-
fied events are gold annotations, but for clarity not all
annotations are included.



4781

protest

attacked attackattacked

killed wounded

Figure 1: The corresponding event hierarchy of exam-
ple 1. Bolded arrows indicate subevent relationships
and bolded lines indicate event coreference relation-
ships, when they are explicitly indicated in the HiEve
annotations. Dashed lines indicate implicit subevent re-
lationship.

In Figure 1, we see that killed1 and wounded6
are explicitly annotated as subevents of attacked3,
while that event in turn is a subevent of protest4.
Events attacked2 and attack5 are explicitly in-
dicated as coreferent with attacked3. These re-
lationships induce the implicit subevent relations
shown by dashed lines.

In this work we propose a pairwise model that
leverages new discourse and narrative features to
significantly improve subevent relation detection.
evaluate our model on two corpora, namely, the
HiEve corpus (Glavaš et al., 2014a) and the In-
telligence Community (IC) corpus1 (Hovy et al.,
2013). We build on feature sets proposed in pre-
vious work, but propose several important dis-
course and narrative level features. We show that
our model outperforms current systems on the
subevent detection task by a significant margin.
An error analysis reveals why these features are
important and further details on why the subevent
detection task is difficult.

We begin the paper by discussing prior work on
subevent detection task (§2). Then we introduce
our model and the feature set (§3). Following that,
we describe the corpora (§4.1) we used and the ex-
perimental setup (§4.2). We then present the eval-
uation metrics and the performance of our model
(§4.3) as well as compare our model performance
to previous works (§5). To the end, we show an
extensive error analysis (§6) and conclude with a
list of contributions (§7).

2 Related Work

There are two pieces of prior work that are most
related to our work. Araki et al. (2014) pro-

1The IC corpus is unfortunately not publically available;
we obtained a copy from Hovy et al. (2013).

posed a logistic regression model to classify pairs
of events into four classes: coreference, subevent,
sister, and no relation. They then used sister re-
lations and their parents to improve the system
performance. Their model was trained and tested
on 65 articles from the IC corpus developed by
(Hovy et al., 2013). Similarly, Glavaš and Šnajder
(2014b) used a logistic regression model to clas-
sify pairs of event into three classes: subevent
relations (SuperSub and SubSuper) and no rela-
tion. They enforced structural coherence which
improved the quality of the extracted event hier-
archies by 7.6% F1 score. They trained and tested
their approach on the HiEve corpus developed by
(Glavaš et al., 2014a). Both approaches were eval-
uated using different evaluations metrics. Araki
et al. evaluated their model using BLANC evalua-
tion metric (Recasens and Hovy, 2011) whereas
Glavaš and Šnajder evaluated their model using
the standard F1 evaluation metric. Both works in-
troduced a variety of features. The main contribu-
tion of our work is to note that the subevent detec-
tion task requires a better understanding of the dis-
course. Thus here we introduce several new fea-
tures, including discourse structure and narrative
structure. The error analysis (§6) demonstrates
why these features are effective and also reveals
more details on why subevent detection is difficult.

3 Features

In this section, we explain the features used in our
model. As discussed, both the HiEve and IC cor-
pus (Hovy et al., 2013; Glavaš et al., 2014a) are
annotated with both subevent and event corefer-
ence relationships. We compute features over all
pairs of events (e1, e2) where e1 precedes e2 in
the text. Each pair of events is either related by a
forward pointing parent-child relationship (PC), a
backward pointing parent-child relationship (CP),
or no relation (NoRel). Our features can be di-
vided into five sets as shown in Table 1. In the
following sections we first illustrate the features
we directly obtained from prior work (§3.1); next
we explain the features that were inspired by prior
work but that we modified significantly (§3.2); and
finally we introduce our new discourse and narra-
tive features (§3.3).

3.1 Prior Features

We obtained most of the lexical and syntactic fea-
tures, and several of the semantic features, directly



4782

Feature Set or Feature Representation Description

Lexical
Event Expression Bag-of-Events The surface form of e1 and e2.
Same Lemma Binary Whether e1 and e2 have the same lemma.
Temporal Signals* Bag-of-Signals If both events are in the same sentence, the temporal signals appearing in

the sentence between the events, based on the temporal signals list from
(Derczynski and Gaizauskas, 2010).

Event String Similarity Numeric The string similarity between surface forms of the events using a Leven-
shtein distance measure.

Syntactic
Major POS One-hot The Major POS of e1 and e2 (e.g., Noun, Verb, or Adjective) [2 features].
Same Major POS Binary Whether the Major POS of e1 and e2 are the same.
POS Tag One-hot The POS Tag of e1 and e2. [2 features]
Same POS Tag Binary Whether the POS Tag of the e1 and e2 are the same.
Syntactic Dependency* One-hot The ancestor event of the other event in the dependency tree.
Determiner Binary Whether each event has a determiner. [2 features]

Semantic
Semantic Frame Binary Whether e1 and e2 have the same semantic frame using SEMAFOR (Das

et al., 2010).
Event Type* One-hot The event type of e1 and e2 extracted from the mapping from frames to

event types (Liu et al., 2016). [2 features]
Same Event Type Binary Whether event types of e1 and e2 are the same.
VerbOccan Score Numeric The VerbOcean score (Chklovski and Pantel, 2004) between e1 and e2 for

each of VerbOcean’s five relations. [5 features]
Semantic Similarity* Numeric The cosine similarity between e1 and e2 embeddings using FastText

(Mikolov et al., 2018) pre-trained model (wiki-news-300d-1M).
Most Likely Parent Event* One-hot Which event is most likely to be a parent of the other event if both exist in

the training data (see §3.2).
WordNet Similarity Numeric The WordNet Similarity scores between e1 and e2 using (Lin, 1998; Wu

and Palmer, 1994) similarity measures.[2 features]

Arguments
Co-refering Event Argu-
ments*

One-hot Whether specific arguments of e1 and e2 corefer (Lee et al., 2017). Verb
arguments are computed with Allennlp’s SRL (Gardner et al., 2018; He
et al., 2017), Nouns and Adjectives with SEMAFOR.

# of Coreferring Args Numeric The number of coreferring arguments between e1 and e2.
Event in the Other’s Args One-hot Whether one event is mentioned in one of the other event’s arguments, if

both events are in the same sentence.

Discourse & Narrative
Sentence Distance Numeric The number of sentences between e1 and e2.
Event Distance Numeric The number of events between e1 and e2.
Same Sentence Binary Whether e1 and e2 are in the same sentence.
Reported Speech Binary Whether an event mention is mentioned in a direct speech (see §3.3.1).
Non Major Mention Binary Whether the sentences, in which the events are mentioned, share co-

referential non major mentions (see §3.3.2).
RST-DTs Relation One-hot The discourse relation between elementary discourse units (EDUs), where

e1 or e2 are mentioned in, in Rhetorical Structure Tree Discourse Trees
(RST-DTs; see §3.3.1).

Table 1: Features used in the model. Novel features are underlined. Features modified from prior work are marked
with an asterisk.

from prior work on subevent detection (Araki
et al., 2014; Glavaš and Šnajder, 2014b). We used
spaCy (Honnibal and Montani) to compute lexical
and syntactic features.

3.2 Modified Features

Five of our features were inspired by those in prior
work, but we modified them for our system.

Temporal Signals We observed that if a sen-
tence mentions two events from different event hi-

erarchies, then a temporal signal often exists be-
tween them (e.g., after and since). This is il-
lustrated by the first sentence in Example 6. To
capture this we used a temporal signals list (Der-
czynski and Gaizauskas, 2010) to find intervening
temporal signal words between the events, and en-
coded this as a bag of temporal signals.

Syntactic Dependency Both prior systems en-
coded a feature which captured whether one event
in a pair was an immediate child (i.e., governed) of



4783

the other. We expand that to checking for ancestry
more generally. This is encoded as one-hot vector.

Event Type We use the mapping from frames
to 33 ACE 2005 event types introduced in (Liu
et al., 2016) to determine the event type of each
event. Prior work relied on the IBM SIRE system
to compute event types (Florian et al., 2010). This
is encode as a one-hot vector.

Semantic Similarity We used the Fast-
Text (Mikolov et al., 2018) pre-trained model
(wiki-news-300d-1M) to measure the seman-
tic similarity between pairs of events. Prior work
used the SENNA system for this feature (Collobert
et al., 2011). This is encoded as a numeric feature.

Most Likely Parent Event Similar to (Araki
et al., 2014), we count the number of times in
the training data that a particular event lemma and
POS pair is observed as a parent of another event
lemma/POS pair. For a pair (e1, e2), if the lemma
and POS of e1 is more often found as a parent of
e2, this is encoded as the vector (1,0,0); if the op-
posite is true, this is encoded as (0,1,0). If there
were no observations, this is encoded as (0,0,1).
Prior work did not take into account the part of
speech, or the direction of the subevent relation-
ship.

Co-referring Event Arguments When match-
ing arguments, we allowed ARG0 to match ARG0
or ARG1 and vice versa, and we also examined
LOC and TMP modifying arguments. This is en-
coded as six-place binary vector for ARG0/ARG1,
LOC, and TMP.

3.3 New Features

The new features are divided into three types: two
discourse features (§3.3.1), one narrative feature
(§3.3.2) and two semantic features (§3.3.3).

3.3.1 Discourse Features
We for the first time investigate the importance
of discourse features for detecting subevents. We
introduced two new features: rhetorical structure
and reported speech.

Rhetorical Structure Rhetorical Structure
Theory (RST) (Mann and Thompson, 1988) is a
hierarchical model aims to identify the discourse
structure of a text. The text is first segmented
into Elementary Discourse Units (EDUs) which in
turn are linked in binary or multi-way discourse
relations (see Carlson and Marcu, 2001). Rhetor-
ical analysis has been shown to be beneficial in
many NLP tasks including sentiment analysis (So-

masundaran, 2010; Lazaridou et al., 2013; Bha-
tia et al., 2015), text generation (Prasad et al.,
2005), information extraction (Maslennikov and
Chua, 2007), question answering (Verberne et al.,
2007) and coreference resolution (Cristea et al.,
1998; Joty et al., 2015). Therefore we hypoth-
esized that discourse structure could be useful
to the subevent detection task. We employ the
CODRA discourse parser (COmplete probabilistic
Discriminative framework for performing Rhetor-
ical Analysis; Joty et al., 2015) to build a dis-
course tree of each text. We use (Neumann, 2015)
for post-processing the CODRA output to build
a graph representing the result. We then extract
the rhetorical relation between event mentions us-
ing the rhetorical relation between the EDUs in
which the event are found. The feature is encoded
as a one-hot vector covering all 16 main relation
classes.

Consider Example 2. When applied to this text,
the discourse parser identifies the relation between
raid3 and killed4 as an Elaboration relation. Fur-
thermore, the parser also captures a Topic-Change
relation between offensive6 and each of killed1,
wounded2, raid3, killed4, and injured5.

Although the discourse parser is useful primar-
ily for providing information about inter-sentential
relationships between events, it can also give
useful information about intra-sentential relation-
ships. Consider Example 3. For this text the dis-
course parser finds the Background relation be-
tween abduction1 and each of killed2 and res-
cued3.

Reported Speech We also observed that

One Palestinian was killed1 and at least
four others were wounded2 in an Is-
raeli air raid3 near the southern Gaza
town of Rafah on Sunday, Palestinian
security sources said. . . . Palestinian
security sources said that one Pales-
tinian bystander was killed4 and at least
four others were injured5. . . . Israeli
troops continued a massive ground and
air offensive6 in the Gaza Strip on Sun-
day.

Example 2: Excerpt from IC corpus (Hovy et al.,
2013). Events relevant to explaining the discourse fea-
tures are bolded. Mentions relevant to explaining the
narrative feature are underlined. Note that, for clar-
ity, not all events marked in the corpus are bolded here
(e.g., Reporting events such as said).



4784

Mahsud, a former prisoner at Guan-
tanamo Bay, is being hunted for the
abduction1 of two Chinese engineers,
which ended last Thursday when com-
mandos killed2 five kidnappers and
rescued3 one Chinese.

Example 3: A sentence where intra-sentential dis-
course relations are useful for discovering subevent re-
lations.

subevents are often reported in direct and indi-
rect speech. Direct speech is speech set off with
quotes, while indirect speech is speech reported
without quotes. We only considered direct speech
in this work, primarily because it is easy to detect;
however, subevents are also likely to be reported
in indirect speech as can be seen in example 2
where killed4 and injured5 (which are subevents
of raid3) are mentioned in indirect speech.

3.3.2 Narrative Feature: Non-Major
Mentions

We also introduced what we are calling a narra-
tive feature that we found informative in detect-
ing subevent relations. This feature recognizes
that other entities mentioned in a sentence be-
sides those in the event arguments can be useful
in subevent detection. This feature is narrative in
the sense that it takes into account whether an en-
tity is central to the story in the text.

In particular, we observed that many sentences
which shared an event hierarchy also share some
coreferring mentions beside events argument. De-
spite this, certain entities are so central to the text
that they are mentioned nearly everywhere and are
thus no especially informative. Therefore we filter
out these major mentions and encode as a binary
feature whether or not the sentences contain the
events share a non-major mention.

The trick, of course, is defining what is a ma-
jor mention. A simple and effective way of filter-
ing out major mentions is to measure the distribu-
tion of coreference chain lengths (normalized to
the number of the corresponding article’s chains),
and discard all chains with a length above a cer-
tain threshold. This threshold can be tuned to the
data. In our experiment we estimated the mean
and standard deviation of the distribution of coref-
erence chains in each text and filtered out chains
that were longer than a single standard deviation
above the mean. In Example 2, the threshold of
the corresponding article is 2, thus Palestinian se-
curity sources, which is mentioned only twice, is

The Al-Qaeda linked Army of Ansar al-
Sunna claimed responsibility Tuesday
for a car bomb attack1 which killed2
four Iraqi guardsmen . . .

Example 4: A sentence where one event appears in-
side the argument for another event. Event killed2 is a
subevent of attack1.

not considered a major mention.

3.3.3 Semantic Features
Event in the Other’s Arguments We observed
that if an event hierarchy is expressed within a
sentence, one of the events is often mentioned as
part of the other event’s arguments as can be seen
in Example 4, where the attack1 event appears as
ARG0 of killed2. Although this feature is related
to the Syntactic Dependency feature, an event’s ar-
guments are not always syntactically dependent on
the event head, so it adds useful information.

Number of Coreferring Arguments We also
include the number of coreferring event arguments
as numeric feature.

4 Experiment

Here we describe the corpora on which the experi-
ment were performed and the evaluation metrics
used to measure the performance of our model.
Then we compare the performance of our model
with previous models, specifically those of Araki
et al. (2014) and Glavaš and Šnajder (2014b).

4.1 Corpora

As already mentioned, we used two corpora: the
Intelligence Community (IC) (Hovy et al., 2013)
corpus and HiEve corpus (Glavaš et al., 2014a)
to train and test our model. The IC corpus con-
tains 100 news articles in the Violent Event do-
main (attacks, killings, wars, etc). The HiEve
corpus is an open domain corpus that also con-
tains 100 news articles. Both corpora are anno-
tated with both coreference and subevent relations.
The inter-annotator agreement for the IC corpus is
0.467 Fleiss’s kappa for subevent relations. The
approach proposed for temporal relations by (Uz-
Zaman and Allen, 2011) was used to measure the
inter-annotator agreement in HiEve corpus, result-
ing in 0.69 F1. There is a small conceptual differ-
ence between the annotation of subevent relations
in both corpora. The annotation of subevents in the
IC corpus follows (Hovy et al., 2013) where they
argued that there are three degrees of event iden-



4785

tity: fully identical, quasi-identical (a.k.a., par-
tial co-reference) and fully independent (not iden-
tical). Quasi-identity in turn appears in two ways:
membership or subevent. Membership is defined
as when an event is a set of multiple instances
of the same type of event and the other event is
one of the instances. In Example 5, attack1 and
operation2 are members of blows3, not subevents.
In contrast, the HiEve corpus considers the mem-
bership relation as a subevent relation. When
training on the IC corpus we considered only the
subevent relations, and ignore the membership re-
lations.

The Al-Qaeda linked group which said
it carried out the deadly attack1 against
US soldiers in the Iraqi city of Mo-
sul accused the United States . . . The
operation2 is one of the heaviest blows3
in the city of Mosul . . .

Example 5: Illustration of the membership quasi-
identity relationship of Hovy et al. (2013)

For both corpora we extend the annotations
by computing the transitive closure of both co-
reference and subevent relations according to the
following rules, where e1, e2 and e3 are event
mentions, ≡ indicates event coreference, e1 > e2
indicates e1 is a parent of e2 , and e1 < e2 in-
dicates e1 is a child of e2. All of these rules are
taken from the work by Glavaš et al. (2014a). We
confirmed that this closure produces a consistent
graph, and thus is insensitive to the order of com-
putation of the closure. Table 2 shows the statistics
of both corpora.

1. (e1 ≡ e2) & (e2 ≡ e3)⇒ (e1 ≡ e3)
2. (e1 > e2) & (e2 > e3)⇒ (e1 > e3)
3. (e1 < e2) & (e2 < e3)⇒ (e1 < e3)
4. (e1 > e2) & (e2 ≡ e3)⇒ (e1 > e3)
5. (e1 > e2) & (e1 ≡ e3)⇒ (e3 > e2)
6. (e1 < e2) & (e2 ≡ e3)⇒ (e1 < e3)
7. (e1 < e2) & (e1 ≡ e3)⇒ (e3 < e2)

4.2 Experimental Setup
We use Linear SVM classifier from scikit-learn
package for classification over the gold annotated
event mentions. Linear SVM can handle multi-
class classification using a one-vs-rest scheme
(Pedregosa et al., 2011). Most of the parameters
are default parameters 2, but to address the issue

2penalty=l2,C=0.01, random state=0,
max iter=1000, class weight=balanced,
multi class=ovr.

IC HiEve

# of sentences 1,973 1,377
# of tokens 48,737 34,917
# PC relations, original 472 609
# PC relations, transitive closure 1632 1802
# CP relations, original 257 351
# CP relations, transitive closure 1665 1846
# NoRel relations 48567 42094
Avg # of sents. per article 19.7 13.7
Avg # of sents. in an event boundary 6.2 8.3
Avg # of events per article 30.5 26.0
Avg # of events in each hierarchy 5.2 7.0
Avg # of hierarchies per article 3.29 2.19

Table 2: Statistics of IC and HiEve corpora.

of the data imbalance as shown in Table 3, we
use the parameter class weight=balanced
to assign a higher misclassification penalty on the
minority class (PC and CP). We conducted 5-fold
cross-validation for the experiment. Average fold
statistics are shown in Table 3.

4.3 Evaluation and Result

We use the same evaluation metrics used in pre-
vious models. (Araki et al., 2014) evaluated their
model using BLANC evaluation metric (Recasens
and Hovy, 2011) whereas (Glavaš and Šnajder,
2014b) evaluated their model using the standard
F1 evaluation metric. The results of the perfor-
mance averaged across all five folds on the three
classes (PC, CP and NoRel) are shown in Ta-
ble 4 using both evaluation metrics on both cor-
pora. Table 5 shows the comparison between our
model and previous models. Although it is not
clear to us how Araki et al. handled the direc-
tion of the subevent relation, we take the average
of our model classes (PC and CP) and compare it
with the subevent class in Araki et al.’s work. For
Glavaš and Šnajder, we consider only their coher-
ent model, which is the best model that does not
use the gold coreference relations. Therefore, in
Table 5, the reported result of all models are the
average of both classes (PC and CP). From Ta-
ble 5, we can see that our model outperforms both
prior models, by 15 and 5 percentage points. We
also see that the precision is lower than the recall
which indicate that the subevent detection task is
still a difficult and complex task that needs more
work. In the next two sections we explain why
the performance of our model is low on IC cor-
pus compared to the HiEve corpus, as well as an
extensive error analysis.



4786

IC corpus HiEve corpus
Training Test Total Training Test Total

# articles 80 20 100 80 20 100
# PC (avg.) 1299.2 332.8 1632 1484 318 1802
# CP (avg.) 1317.8 347.2 1665 1456.4 389.6 1846
# NoRel (avg.) 39469 9098 48567 35621.2 6472.8 42094

Table 3: Average statistics of the folds. PC stands for parent-child relation. CP stands for child-parent relation.
NoRel stands for no relation.

Evaluation Metrics
F1 Score BLANC

Pos Links Neg Links Avg
Corpus Relation P R F1 P R P R F1

HiEve
PC 0.576 0.807 0.67 0.661 0.832 0.989 0.973 0.857
CP 0.661 0.832 0.733 0.576 0.807 0.990 0.971 0.825
NoRel 0.98 0.945 0.962 0.980 0.945 0.625 0.830 0.836

IC
PC 0.469 0.564 0.506 0.455 0.549 0.982 0.973 0.735
CP 0.454 0.550 0.492 0.468 0.564 0.983 0.975 0.743
NoRel 0.966 0.905 0.958 0.966 0.949 0.461 0.557 0.729

Table 4: Our model result on IC corpus (Hovy et al., 2013) and HiEve corpus (Glavaš et al., 2014a) using BLANC
and F1 standard evaluation metrics. PC stands for parent-child relation. CP stands for child-parent relation.

5 Discussion

As shown in Table 4, our model performs worse
on the IC corpus than on HiEve. This is not sur-
prising given the large difference in annotation
agreement between IC and HiEve as well as the
the removal of membership relations on IC cor-
pus (see §4.1). Beside its lower annotation agree-
ment, the IC corpus is also domain specific, with
events only related to the intelligence community.
This make general resources and tools (e.g., Ver-
bOcean, WordNet) less effective.

We investigated the importance of each of the
five feature sets (Table 1) to our model by retrain-
ing it while leaving out one set at time. In order
of importance they are (1) Syntactic, (2) Seman-
tic, (3) Discourse & Narrative, (4) Lexical, and (5)
Arguments. The importance of the syntactic fea-
tures derived from the fact that children events are
most often mentioned in the same sentence as their
parent events. The three most important features
among the Semantic features are Most Likely Par-
ent Event, Event Type, and Semantic Frame. For
the Lexical feature set, the Event Feature and Tem-
poral Signals are the most important.

6 Error Analysis

Inspection of the results revealed several types
of errors, aside from the usual noise introduced
by the various sub-components, such as the dis-

course parser or co-reference systems. We clus-
ter the errors into three types: (1) an event pair
that should be classified as PC but classified as
CP and vice versa (about 28%); (2) an event pair
is wrongly classified as NoRel (missed subevent
relation; about 12%); (3) an event pair that is actu-
ally NoRel is wrongly classified as subevent (PC
or CP; about 60% of the errors).

Type 1: PC as CP or vice versa About a third
of the model errors were this type. Most of the
errors are a result of an incorrect Event Type fea-
ture. This feature plays a major role in capturing
the direction of the subevent relation. For exam-
ple, if an event e1 with event type Die occurs in
the text before an event e2 with event type Attack,
then the direction of the relation is mostly child-
parent relation. But if e2 occurs before e1, then
the direction of the relation is mostly parent-child.
If the event type is unknown for one of the event
mentions, then our model commonly usually fails
to capture the direction.

Type 2: Incorrect NoRel Most of the type 2
errors occur when an event is far away from its
related event, in terms of number of intervening
sentences. The larger the distance between events
the more likely the model makes this error. For
this type of error, we calculated the average num-
ber of sentences and the average number of events
intervening between a missed pair of event, which
the model should capture its subevent relation, and



4787

F1 Score BLANC
Pos Links Neg Links Avg

Corpus Model P R F1 P R P R F1

IC
Araki et al. (2014) - - - 0.144 0.333 0.993 0.981 0.594
Araki et al. Re-Impl. 0.242 0.285 0.262 - - - - -
Our model 0.461 0.557 0.499 0.461 0.557 0.983 0.974 0.739

HiEve Glavaš and Šnajder (2014b) 0.766 0.565 0.65 - - - - -
Glavaš and Šnajder Re-Impl. - - - 0.562 0.750 0.983 0.971 0.813
Our model 0.618 0.82 0.701 0.618 0.82 0.99 0.972 0.841

Table 5: Our model performance compared to previous models (Araki et al., 2014; Glavaš and Šnajder, 2014b).
Each row represent the average of both classes parent-child (PC) and child-parent (CP). Because the prior systems
both did not report both metrics, we approximated the metrics for those systems by reimplementing them.

found that when the distance is greater than 9 sen-
tences and the number of events is greater than 14
events, the more likely the model would conduct
this error. Subevents tend to be close to their par-
ents in the text as shown in Table 2. Moreover,
we observed that the Non-Major Mention (§3.3.2)
and Discourse Relation features (§3.3.1), were less
useful the larger the distance between the events.

Type 3: False Positive PC or CP Most of
the errors were of this type. There were a vari-
ety of causes, but the most common was when
a sentence contained multiple event hierarchies.
Consider Example 6 where the sentence contains
two different event hierarchies, namely, one hier-
archy containing offensive3 and another contain-
ing abduction4.

Over 90 Palestinians and one Israeli
soldier have been killed1 since Israel
launched2 a massive air and ground
offensive3 into the Gaza Strip on June
28, three days after the abduction4 of
one Israeli soldier by Palestinian mili-
tants in a cross-border raid5.

Example 6: Excerpt from IC corpus (Hovy et al., 2013)
showing a passage that results in an error of Type 3.

In this example, killed1 and launched2 are
subevents of offensive3, whereas abduction4 is a
subevent of raid5. When processing this example
the discourse parser failed to capture the discourse
relation between offensive3 and abduction4 be-
cause both events are in the same EDU. More-
over, even though we introduced features such as
temporal signals (after, since, etc.) to capture
subevent relation between intra-sentential events,
this error can still occur if the intra-sentential
events are syntactically related (i.e., killed1 syn-
tactically dominates abduction4, or there is a
causal relation between events).

Based on this observation, we ran an experi-
ment on the IC corpus to examine the impact on
subevent detection of having two different events
in the same sentence. We construct a subset of the
IC corpus (58 articles) which excluded all articles
that contain at least one sentence with two differ-
ent event hierarchy, and re-ran our main experi-
ment. Under these conditions, the model perfor-
mance increased by 6 and 4.6 points F1 on PC and
CP classes, respectively (because of the smaller
set, we used 3 folds instead of 5). Returning to
the original corpus, we observed that two different
event hierarchies are mostly found in compound
and complex sentences, and one of the them is
usually background event. This observation in-
dicates that splitting compound or complex sen-
tences into two simple sentences in advance might
be useful in detecting subevents. Even though the
discourse parser does this splitting automatically,
this split is not currently propagated to the other
features.

7 Contributions

We present a model to detect subevent relation in
news articles which outperforms the two prior ap-
proaches by 15 and 5 percentage points, respec-
tively. Our model involves several novel discourse
and narrative features, as well as a small number of
feature modifications. Our error analysis indicates
that having two event hierarchies in the same sen-
tence is a major problem, as well as having signif-
icant separation between a parent and child event.

Acknowledgments

Mr. Aldawsari was supported by a doctoral fel-
lowship from Prince Sattam Bin Abdulaziz Uni-
versity, and thanks Dr. Sultan Aldossary for his
advice and support. This work was also supported



4788

by US National Science Foundation grant number
IIS-1749917 to Dr. Finlayson. Both authors would
like to thank Ed Hovy for providing the IC Corpus
for our use.

References
Jun Araki. 2018. Extraction of Event Structures from

Text. Ph.D. thesis, Carnegie Mellon University.

Jun Araki, Zhengzhong Liu, Eduard H Hovy, and
Teruko Mitamura. 2014. Detecting subevent struc-
ture for event coreference resolution. In Proceed-
ings of the 9th International Conference on Lan-
guage Resources and Evaluation (LREC), pages
4553–4558, Lisbon, Portugal.

Cosmin Adrian Bejan and Sanda M Harabagiu. 2008.
A linguistic resource for discovering event structures
and resolving event coreference. In Proceedings of
the 6th Language Resources and Evaluation Con-
ference (LREC), pages 2881–2887, Marrakech, Mo-
rocco.

Parminder Bhatia, Yangfeng Ji, and Jacob Eisenstein.
2015. Better document-level sentiment analysis
from RST discourse parsing. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 2212–2218,
Lisbon, Portugal.

Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging reference manual. ISI Technical Report ISI-TR-
545.

Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 33–40, Barcelona, Spain.

Prafulla Kumar Choubey and Ruihong Huang. 2018.
Improving event coreference resolution by modeling
correlations between event coreference chains and
document topic structures. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics, Volume I, pages 485–495, Mel-
bourne, Australia.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.

Dan Cristea, Nancy Ide, and Laurent Romary. 1998.
Veins theory: A model of global discourse cohesion
and coherence. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics (ACL-ICCL), pages 281–285,
Montreal, Canada.

Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A Smith. 2010. Probabilistic frame-semantic
parsing. In Proceedings of Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (NAACL-HLT), pages 948–956,
Los Angeles, CA.

Marie-Catherine De Marneffe, Anna N Rafferty, and
Christopher D Manning. 2008. Finding contradic-
tions in text. In Proceedings of the 46th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies (ACL-08
HLT), pages 1039–1047, Columbus, OH.

Leon Derczynski and Robert Gaizauskas. 2010.
USFD2: Annotating temporal expresions and tlinks
for tempeval-2. In Proceedings of the 5th In-
ternational Workshop on Semantic Evaluation (Se-
mEval’10), pages 337–340, Los Angeles, CA.

Radu Florian, John F Pitrelli, Salim Roukos, and Imed
Zitouni. 2010. Improving mention detection robust-
ness to noisy input. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 335–345, Cambridge,
MA.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Pe-
ters, Michael Schmitz, and Luke Zettlemoyer. 2018.
Allennlp: A deep semantic natural language pro-
cessing platform. arXiv preprint arXiv:1803.07640.

Goran Glavaš and Jan Šnajder. 2014b. Constructing
coherent event hierarchies from news stories. In
Proceedings of the Workshop on Graph-based Meth-
ods for Natural Language Processing (TextGraphs-
9), pages 34–38, Doha, Qatar.

Goran Glavaš, Jan Šnajder, Parisa Kordjamshidi, and
Marie-Francine Moens. 2014a. Hieve: A corpus for
extracting event hierarchies from news stories. In
Proceedings of 9th Language Resources and Evalu-
ation Conference (LREC), pages 3678–3683.

Aria D Haghighi, Andrew Y Ng, and Christopher D
Manning. 2005. Robust textual inference via graph
matching. In Proceedings of the Conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing (HLT/EMNLP),
pages 387–394, Vancouver, Canada.

Luheng He, Kenton Lee, Mike Lewis, and Luke Zettle-
moyer. 2017. Deep semantic role labeling: What
works and what’s next. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Volume I, pages 473–483,
Vancouver, Canada.

Matthew Honnibal and Ines Montani. spacy 2:
Natural language understanding with bloom em-
beddings, convolutional neural networks and in-
cremental parsing. https://github.com/
explosion/spaCy; Last accessed on May 31,
2019.

https://github.com/explosion/spaCy
https://github.com/explosion/spaCy


4789

Eduard Hovy, Teruko Mitamura, Felisa Verdejo, Jun
Araki, and Andrew Philpot. 2013. Events are not
simple: Identity, non-identity, and quasi-identity. In
Proceedings of the Workshop on Events: Definition,
Detection, Coreference, and Representation, pages
21–28, Atlanta, Georgia.

Kevin Humphreys, Robert Gaizauskas, and Saliha Az-
zam. 1997. Event coreference for information ex-
traction. In Proceedings of a Workshop on Opera-
tional Factors in Practical, Robust Anaphora Reso-
lution for Unrestricted Texts, pages 75–81, Madrid,
Spain.

Silja Huttunen, Roman Yangarber, and Ralph Grish-
man. 2002. Complexity of event structure in ie sce-
narios. In Proceedings of the 19th International
Conference on Computational Linguistics (COL-
ING), pages 1–7, Taipei, Taiwan.

Shafiq Joty, Giuseppe Carenini, and Raymond T Ng.
2015. Codra: A novel discriminative framework
for rhetorical analysis. Computational Linguistics,
41(3):385–435.

Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A Bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of
the 51st Annual Meeting of the Association for
Computational Linguistics (ACL), Volume I, pages
1630–1639, Sofia, Bulgaria.

Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle-
moyer. 2017. End-to-end neural coreference reso-
lution. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 188–197, Copenhagen, Denmark.

Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th Inter-
national Conference on Machine Learning (ICML),
pages 296–304, San Francisco, CA.

Shulin Liu, Yubo Chen, Shizhu He, Kang Liu, and
Jun Zhao. 2016. Leveraging framenet to improve
automatic event detection. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), Volume I, pages 2134–
2143, Berlin, Germany.

William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.

Mstislav Maslennikov and Tat-Seng Chua. 2007. A
multi-resolution framework for information extrac-
tion from free text. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics (ACL), pages 592–599, Prague, Czech
Republic.

Tomas Mikolov, Edouard Grave, Piotr Bojanowski,
and Armand Puhrsch, Christian andJoulin. 2018.
Advances in pre-training distributed word repre-
sentations. In Proceedings of the 11th Language

Resources and Evaluation Conference (LREC),
Miyazaki, Japan.

Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics (COLING), pages 693–
701, Geneva, Switzerland.

Arne Neumann. 2015. discoursegraphs: A graph-
based merging tool and converter for multilayer an-
notated corpora. In Proceedings of the 20th Nordic
Conference of Computational Linguistics (NODAL-
IDA 2015), pages 309–312, Vilnius, Lithuania.

Qiang Ning, Zhili Feng, and Dan Roth. 2017. A struc-
tured learning approach to temporal relation extrac-
tion. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 1027–1037, Copenhagen, Den-
mark.

Fabian Pedregosa, Gael Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825–2830.

Rashmi Prasad, Aravind Joshi, Nikhil Dinesh, Alan
Lee, Eleni Miltsakaki, and Bonnie Webber. 2005.
The Penn Discourse TreeBank as a resource for nat-
ural language generation. In Proceedings of the
Corpus Linguistics Workshop on Using Corpora for
Natural Language Generation, pages 25–32, Birm-
ingham, UK.

James Pustejovsky, José M Castano, Robert Ingria,
Roser Sauri, Robert J Gaizauskas, Andrea Set-
zer, Graham Katz, and Dragomir R Radev. 2003.
TimeML: Robust specification of event and tem-
poral expressions in text. In Proceedings of the
2003 AAAI Spring Symposium on New Directions in
Question Answering, pages 28–34. Stanford, CA.

Marta Recasens and Eduard Hovy. 2011. Blanc: Im-
plementing the rand index for coreference evalua-
tion. Natural Language Engineering, 17(4):485–
510.

Swapna Somasundaran. 2010. Discourse-level rela-
tions for Opinion Analysis. Ph.D. thesis, University
of Pittsburgh.

Naushad UzZaman and James Allen. 2011. Temporal
evaluation. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT),
pages 351–356, Portland, OR.

Suzan Verberne, LWJ Boves, NHJ Oostdijk, and PAJM
Coppen. 2007. Evaluating discourse-based answer
extraction for why-question answering. In Proceed-
ings of the 30th Annual International ACM SIGIR



4790

Conference on Research and Development in Infor-
mation Retrieval (SIGIR), pages 735–736, Amster-
dam, The Netherlands.

Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32nd
Annual Meeting on Association for Computational
Linguistics (ACL), pages 133–138, Las Cruces, NM.


