



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 85–90
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-4015

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 85–90
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-4015

Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ

Jason S. Kessler
CDK Global

jason.kessler@gmail.com

Abstract

Scattertext is an open source tool for visu-
alizing linguistic variation between docu-
ment categories in a language-independent
way. The tool presents a scatterplot,
where each axis corresponds to the rank-
frequency a term occurs in a category
of documents. Through a tie-breaking
strategy, the tool is able to display thou-
sands of visible term-representing points
and find space to legibly label hundreds
of them. Scattertext also lends itself to a
query-based visualization of how the use
of terms with similar embeddings differs
between document categories, as well as
a visualization for comparing the impor-
tance scores of bag-of-words features to
univariate metrics.

1 Introduction
Finding words and phrases that discriminate cat-
egories of text is a common application of sta-
tistical NLP. For example, finding words that
are most characteristic of a political party in
congressional speeches can help political scien-
tists identify means of partisan framing (Mon-
roe et al., 2008; Grimmer, 2010), while identify-
ing differences in word usage between male and
female characters in films can highlight narra-
tive archetypes (Schofield and Mehr, 2016). Lan-
guage use in social media can inform understand-
ing of personality types (Schwartz et al., 2013),
and provides insights into customers’ evaluations
of restaurants (Jurafsky et al., 2014).

A wide range of visualizations have been used
to highlight discriminating words– simple ranked
lists of words, word clouds, word bubbles, and
word-based scatter plots. These techniques have a
number of limitations. For example, the difficulty

in comparing the relative frequencies of two terms
in a word cloud, or in legibly displaying term la-
bels in scatterplots.

Scattertext1 is an interactive, scalable tool
which overcomes many of these limitations. It is
built around a scatterplot which displays a high
number of words and phrases used in a corpus.
Points representing terms are positioned to allow
a high number of unobstructed labels and to in-
dicate category association. The coordinates of a
point indicate how frequently the word is used in
each category.

Figure 1 shows an example of a Scattertext plot
comparing Republican and Democratic political
speeches. The higher up a point is on the y-axis,
the more it was used by Democrats, and similarly,
the further right on the x-axis a point appears,
the more its corresponding word was used by Re-
publicans. Highly associated terms fall closer to
the upper left and lower right-hand corners of
the chart, while stop words fall in the far upper
right-hand corner. Words occurring infrequently
in both classes fall closer to the lower left-hand
corner. When used interactively, mousing-over a
point shows statistics about a term’s relative use
in the two contrasting categories, and clicking on
a term shows excerpts from convention speeches
used.

The point placement, intelligent word-labeling,
and auxiliary term-lists ensure a low-whitespace,
legible plot. These are issues which have plagued
other scatterplot visualizations showing discrimi-
native language.
§2 discusses different views of term-category

association that make up the basis of visualiza-
tions. In §3, the objectives, strengths, and weak-
nesses of existing visualization techniques. §4
presents the technical details behind Scattertext.

1github.com/JasonKessler/scattertext

85

https://doi.org/10.18653/v1/P17-4015
https://doi.org/10.18653/v1/P17-4015


Infrequent Average Frequent

Republican Frequency

In
fre

qu
en

t
Av

er
ag

e
Fr

eq
ue

nt

D
em

oc
ra

tic
 F

re
qu

en
cy Top Democratic

auto
auto industry
insurance companies
pell
last week
pell grants
affordable
grants
platform
reduce
access
fair
grandmother
clean

Top Republican
unemployment
liberty
olympics
reagan
ann
founding
constitution
church
free enterprise
federal government
enterprise
sons
boy
greatness

Characteristic
obama
romney
barack
mitt
obamacare
biden
romneys
hardworking
bain
grandkids
billionaires
millionaires
ledbetter
buenas
pell
noches
bless
dreamers
congresswoman
bipartisan
wealthiest
risked
trillion
republicans
recession
insurmountable
gentlemen
electing
pelosi
understands

obama

romney
barack

millionaires
pell

wealthiest

trillion

gentlemen

fought

president

americans
medicare

prosper

reagan

blaming

founding

seniors

unemployment

wealthy

veterans

blessed
rhetoric

fathers

olympics

stake

stimulus

strengthen

fighting
cuts

fights

ours

liberty

bridges

strongest

freedom

blame

roosevelt

we

ballot

failing

michelle

greatest

stark

hopes

politicians

heal

charlotte

affordable

caring

success

belief

uniform

vote

loyal

walks

telling

depend

fulfill

troops mayor

better

roads

voting

pursuit

workers

detroit

cents

broke

laid
pays ran

moving

opponent

earn

class

myth

fair

grants

drill

kept

pushed

shared

jill

hands

refuse

sick

pay

spend

coal

worry

solve

loved

principles

move

won

led

son

story

rid

administration

that

fiscal

lady

wanted

achievement

ann
pray

israel

minds

iraq

mate

bills

justice

reducing

shut

regulations

tells fix

lie

trust

gm

younger

rise

era

brings

kate

iowa

fear

begin

coverage

que

rules

auto

bold

hit

mean

federal

pass

felt

carefully

deep
seem

played

act

cast

last

safe

signed

insurance

simple

role

bus

tea

fall

edge

wall

loans

church

deal

seat

clock

risk

along

con

lower

fill

pre

strategy

five
pain

train

hold

firm

race

leading

bit

bed

join

ship

save
cost

re

either
hours

feet ok

access

sound
fine

line

los

point

top

visit

loss

full

baby

free

add

1

re elect

18

3

running mate

second term
$ 10

jobs overseas
childhood education

42

ok.

4

no matter

8
16

insurance companies

Democratic document count: 123; word count: 76,864
Republican document count: 66; word count: 58,138

Search for term   Type a word or two…  

Figure 1: Scattertext visualization of words and phrases used in the 2012 Political Conventions. 2,202
points are colored red or blue based on the association of their corresponding terms with Democrats or
Republicans, 215 of which were labeled. The corpus consists of 123 speeches by Democrats (76,864
words) and 66 by Republicans (58,138 words). The most associated terms are listed under “Top Demo-
crat” and “Top Republican” headings. Interactive version: https://jasonkessler.github.io/st-main.html

§5 discusses how Scattertext can be used to iden-
tify category-discriminating terms that are seman-
tically similar to a query.

2 On text visualization
The simplest visualization, a list of words ranked
by their scores, is easy to produce, interpret and
is thus very common in the literature. There
are numerous ways of producing word scores for
ranking which are thoroughly covered in previ-
ous work. The reader is directed to Monroe et al.
(2008) (subsequently referred to as MCQ) for an
overview of model-based term scoring algorithms.
Also of interest, Bitvai and Cohn (2015) present a
method for finding sparse words and phrase scores
from a trained ANN (with bag-of-words features)
and its training data.

Regardless of how complex the calculation,
word scores capture a number of different mea-
sures of word-association, which can be interest-
ing when viewed independently instead of as part
of a unitary score. These loosely defined measures
include:

Precision A word’s discriminative power regard-
less of its frequency. A term that appears once in
the categorized corpus will have perfect precision.
This (and subsequent metrics) presuppose a bal-
anced class distribution. Words close to the x and

y-axis in Scattertext have high precision.

Recall The frequency a word appears in a partic-
ular class, or P (word|class). The variance of pre-
cision tends to decrease as recall increases. Ex-
tremely high recall words tend to be stop-words.
High recall words occur close to the top and right
sides of Scattertext plots.

Non-redundancy The level of a word’s discrimi-
native power given other words that co-occur with
it. If a word wa always co-occurs with wb and
word wb has a higher precision and recall, wa
would have a high level of redundancy. Measur-
ing redundancy is non-trivial, and has tradition-
ally been approached through penalized logistic
regression (Joshi et al., 2010), as well as through
other feature selection techniques. In configu-
rations of Scattertext such as the one discussed
at the end of §4, terms can be colored based
on their regression coefficients that indicate non-
redundancy.

Characteristicness How much more does a word
occur in than the categories examined than in
background in-domain text? For example, if com-
paring positive and negative reviews of a single
movie, a logical background corpus may be re-
views of other movies. Highly associated terms
tend to be characteristic because they frequently

86



appear in one category and not the other. Some
visualizations explicitly highlight these, ex. (Cop-
persmith and Kelly, 2014).

3 Past work and design motivation

Text visualizations manipulate the position and ap-
pearance of words or points representing them to
indicate their relative scores in these measures.
For example, in Schwartz et al. (2013), two
word clouds are given, one per each category of
text being compared. Words (and selected n-
grams) are sized by their linear regression coef-
ficients (a composite metric of precision, recall,
and redundancy) and colored by frequency. Only
words occurring in ≥1% of documents and hav-
ing Bonferroni-corrected coefficient p-values of
<0.001 were shown. Given that these words are
highly correlated to their class of interest, the fre-
quency of use is likely a good proxy for recall.

Coppersmith and Kelly (2014) also describe a
word-cloud based visualization for discriminating
terms, but intend it for categories which are both
small subsets of a much larger corpus. They in-
clude a third, middle cloud for terms that appear
characteristic.

Word clouds can be difficult to interpret. It
is difficult to compare the sizes of two non-
horizontally adjacent words, as well as the relative
color intensities of any two words. Longer words
unintentionally appear more important since they
naturally occupy more space in the cloud. Sizing
of words can be a source of confusion when used
to represent precision, since a larger word may
naturally be seen as more frequent.

Bostock et al. (2012)2 features an interactive
word-bubble visualization for exploring different
word usage among Republicans and Democrats in
the 2012 US presidential nominating conventions.
Each term displayed is represented by a bubble,
sized proportionate to their frequency. Each bub-
ble is colored blue and red, s.t. the blue parti-
tion’s size corresponds to the term’s relative use
by Democrats. Terms were manually chosen,
and arranged along the x-axis based on their dis-
criminative power. When clicked, sentences from
speeches containing the word used are listed be-
low the visualization.

The dataset used in Bostock et al. (2012) is
used to demonstrate the capabilities of Scattertext

2nytimes.com/interactive/2012/09/06/us/politics/convention-
word-counts.html

in each of these figures. The dataset is available
via the Scattertext Github page.

3.1 Scatterplot visualizations

aðiÞkw 5 a
ðiÞ
k0p̂

MLE 5 y # a0
n

ð23Þ

where aðiÞk0 determines the implied amount of information in the prior. This prior shrinks the
pðiÞkw and X

ðiÞ
kw to the global values, and shrinks the feature evaluation measures, the f

ðiÞ
kw and

the fði2 jÞkw , toward zero. The greater a
ðiÞ
k0 is, the more shrinkage we will have.

Figure 5 illustrates results from the running example using this approach. We set a0 to
imply a ‘‘prior sample’’ of 500 words per party every day, roughly the average number of
words per day used per party on each topic in the data set.

As is shown in Figure 5, this has a desirable effect on the function-word problem noted
above. For example, the Republican top 20 list has shuffled, with the, you, not, of, and be
being replaced by the considerably more evocative aliv, infant, brutal, brain, and necessari.

Fig. 5 Feature evaluation and selection based on f̂
ðD2RÞ
kw . Plot size is proportional to evaluation

weight, f̂
ðD2RÞ
kw ; those with

!!!f̂
ðD2RÞ
kw

!!!<1:96 are gray. The top 20 Democratic and Republican words are
labeled and listed in rank order to the right.

388 Burt L. Monroe, Michael P. Colaresi, and Kevin M. Quinn

 at U
niversity of Pennsylvania Library on June 3, 2013

http://pan.oxfordjournals.org/
D

ow
nloaded from

 

's

@b0rk
@benmarwick
@coursera
@gshotwell

auto
@seanhacks
@daattali
@aalear
gganimate
android

@onlybluefeet

@juliasilge

@bigkage

accepted
hop

applied
created

@flowingdata
gif

advantage
stats
sql
base

broom

pit
algebra
conf
pipe

columns
tool
knitr

stack

@youtube

8th

ad

talks
btw

2x
blank
bob
lazy
notice
linear

@jc4p

@jasonpunyon

slides

traffic

alive

aloud
90s
chunk
hurt
rule
user

@kara_woo

tweets

ball

wars
april
train
ha

choice
file

article

support

iron

roll

tech

posts

fail
ran

@noamross
dplyr

view
@jhollist

link

eye

join

add

bring

art

math

bug

dr

line

analysis

feet

um

yep

laws

rest

public

reason

sit

floor

table

code

fan

@hspter

figure

hotel
3rd

question

cooking

store

fine
word
wrong

miss

@hadleywickham

dog
play

talk

slow

tiny

normal

watch

running
times

@quominus

blog

family

package

buy

bought

hours

found
start

favorite

idea

hot

hard
bit

post

lol

stuff

food

data

amazing

#rstats

life

nice

kids

home

dinner
tonight

week

lot

school

pretty

people

morning

house
baby

day

time

0.01%

0.10%

1.00%

0.01% 1.00%
Julia

D
av
id

Figure 2: A sample of existing scatterplot visual-
izations. MCQ’s is at the top. Tidytext is below.

MCQ present a visualization to illustrate the use
of their proposed word score, log-odds-ratio with
an informative Dirichlet prior (top of Figure 2).
This visualization plots word-representing points
along two axes. The axes are log10 recall vs. the
difference in word scores z-scores. Points with a
z-score difference<1.96 are grayed-out, while the
top and bottom 20 are labeled, both by each point
and on the right-hand side. The side-labeling is
necessary because labels are permitted to overlap,
hindering their on-plot readability. The sizes of
points and labels are increased proportionally to
the word score. This word score encompasses pre-
cision, recall, and characteristicness since it penal-
izes scores of terms used more frequently in the
background corpus. MCQ used this type of plot
to illustrate the different effects of various scoring
techniques introduced in the paper. However, the
small number of points which are possible to label
limit its utility for in-depth corpus analysis.

Schofield and Mehr (2016) use essentially the

87



same visualization, but plot over 100 correspond-
ing n-grams next to an unlabeled frequency/z-
score plot. While this is appropriate for publica-
tion, displaying associated terms and the shape of
the score distribution, it is impossible to align all
but the highest scoring points to their labels.

The tidytext R-package (Silge and Robinson,
2016) documentation includes a non-interactive
ggplot2-based scatter plot that is very similar to
Scattertext. The x and y-axes both, like in Scat-
tertext, correspond to word frequencies in the two
contrasting categories, with jitter added.3 In the
example in Figure 2 (bottom), the contrasting cate-
gories are tweets from two different accounts. The
red diagonal line separates words based on their
odds-ratio. Importantly, compared to MCQ, less
of this chart’s area is occupied by whitespace.

While tidytext’s labels do not overlap each other
(in contrast to MCQ) they do overlap points. The
points’ semi-transparency makes labels in less-
dense areas legible, the dense interior of the chart
is nearly illegible, with both points and labels ob-
scured. Figure 3 shows an excerpt of the same

's

@b0rk

@benmarwick

@coursera

@gshotwell

blast
auto
@sf99

@seanhacks
probability

cc
@daattali

@nssdeviations
@aalear

@gaborcsardi
gganimate

@simplystats
android
@fody

@nick_craver

@onlybluefeet

@juliasilge

@bigkage

accepted

clinton's

@hairboat

created

articles
@flowingdata
distribution
compare
simulation
network
advantage

log

stats

sql
users

base

overflow

broom

iffy

pit

facet

effect

york

false

tidying
df

baseball

knitr
beta

dataset

stack

aired

8th

cmd

@alexwhan

ad
chapter

comparing
adding
gonna

regression
talks

btw

@kwbroman

vi

2x

easily

beat

bc

@jc4p

@jasonpunyon

chia

fish

access

form

api

@timelyportfolio

traffic

chunk

hurt

closed

user

future

@kara_woo

arms

bar

fill

melt

sentiment

frame

scale
matter

title

tweets

april

avoid

ha

save

examples

packages
article

cup

fear

count

answer

biggest

lines

tech

advice
aw

chance

results

posts

github

sell

san

fail

ran

hold

@noamross

dplyr

@jhollist

link

statistics

tidy

join

add

dry

hat

voice

shiny

tidytext

ggplot2

art

math

bug

function

easier

options
line

based

analysis

nom

guy

due

um

yep

rest

public

easy

loved

ideas

la

talking

email

table

write

code

jam

fan

learning

names

social

fit

@hspter

close

list

tweet

3rd

realized

question

cooking

age

store

fine

word

call

literally

live

hope

library

wrong

class

@hadleywickham

dog

play

stop

talk

half
cool

taking

science

tiny

meet

normal

trip

person

watch

awesome

@quominus

wearing

red
wait

blog

late

coming

package

buy

sad

hours

wear

found

started

start

ago

sleep

favorite

idea

hot

tomorrow

thinking
bad
makes

totally

yeah

hard

read

post

lol

stuff

real

data

sick

amazing

#rstats

life

nice

kids

@drob

happy

home

birthday

dinner

tonight

night

week

lot

school

pretty
feel

people

house

baby

day

time

0.01%

0.10%

1.00%

0.01% 1.00%
Julia

D
av
id

Figure 3: A small cropping from an un-jittered
version at the bottom of Figure 2. The dark,
opaque points indicate stacks of points.

plot, but with no jitter. Words appearing with
the same frequency in both categories all become
stacked atop each other, however, this provides
more interior space for labeling.

As a side note, many text visualizations plot
words in a 2D space according to their similarity
in a high dimensional space. For example, Cho et
al. 2014 uses the Barnes-Hut-SNE to plot words in
a 2D space s.t. those with similar representations
are grouped close together. Class-association does
not play a role in this line of research, and global
position is essentially irrelevant.

The next section presents Scattertext and how
its approach to word ordering solves the problems
discussed above.

3This type of visualization may have first been introduced in
Rudder (2014).

4 Scattertext
Scattertext builds on tinytext and Rudder (2014).
It plots a set of unigrams and bigrams (referred
to in this paper as “terms”) found in a corpus of
documents assigned to one of two categories on a
two-dimensional scatterplot.

In the following notation, user-supplied param-
eters are in bold typeface.

Consider a corpus of documents C with disjoint
subsets A and B s.t. A ∪B ≡ C. Let φT (t, C) be
the number of times term t occurs in C, φT (t, A)
be the the number of times t occurs in A. Let
φD(t, A) refer to the number of documents in A
containing t. Let tij be the jth word in term ti. In
practice, j ∈ {1, 2}. The parameter φ may be φT
or φD.4 Other feature representations (ex., tf.idf)
may be used for φ.

Pr[ti] =
φ(ti, C)∑

t∈C∧|t|≡|ti|φ(t, C)
. (1)

The construction of the set of terms included in
the visualization V is a two-step process. Terms
must occur geqm times, and if bigrams, appear
to be phrases. In order to keep the approach lan-
guage neutral, I follow Schartz et al. (2013), and
use a pointwise mutual information score to filter
out bigrams that do not occur far more frequently
than would be expected. Let

PMI(ti) = log
Pr[ti]∏

tij∈tiPr[Tij ]
. (2)

The minimum PMI accepted is p. Now, V can
be defined as

{t|φ(t, C) ≥m∧(|t| ≡ 1∨PMI(t) > p)} (3)

Let a term t’s coordinates on the scatterplot be
(xAt , x

B
t ), where A and B are the two docu-

ment categories. Although xKt is proportional to
φ(t,K), many terms will have identical φ(t,K)
values. To break ties the word that appears last
alphabetically will have a larger xKt .

Let us define rKt s.t. t ∈ V and K ∈ {A,B}
as the ranks of φ(t,K), sorted in ascending order,
where ties are broken by terms’ alphabetical order.
This allows us to define

xKt =
rKt

argmax rK
(4)

4φD is useful when documents contain unique, characteristic,
highly frequent terms. For example, names of movies can
have high φT when finding differences in positive and neg-
ative film reviews. The may lead to them receiving higher
scores than sentiment terms.

88



This limits x values to [0, 1], ensuring both axes
are scaled identically. This keeps the chart from
becoming lopsided toward the corpus that had a
larger number of terms.5

The charts in Figures 1, 4, and 5, were made
with parametersm=5, p=8, and φ=φT .

Breaking ties alphabetically is a simple but
important alternative to jitter. While jitter (i.e.,
randomly perturbing xAt and x

B
t ) breaks up the

stacked points shown in Figure 3, it eliminates
empty space to legibly label points. Jitter can
make it seem like identically frequent points are
closer to an upper left or lower right corner. Al-
phabetic tie-breaking makes identical adjustments
to both axes, leading to the horizontal (lower-left
to upper-right) alignments of identically frequent
points. This angle does not cause one point to be
substantially closer to either of the category asso-
ciated corners (the upper-left and lower-right).

These alignments provide two advantages.
First, they open up point-free tracts in the center
of the chart which allow for unobstructed interior
labels. Second, they arrange points in a way that it
is easy to hover a mouse over all of them, to indi-
cate what term they correspond to, and be clicked
to see excerpts of that term.

In the running example, 154 points were la-
beled when a jitter of 10% of each axis and no
tie-breaking was applied. 210 points (a 36% lift)
were labeled when no jitter was applied. 140 were
labeled if no tie breaking was used.

Rudder (2014) observed terms closer to the
lower-right corner were used frequently in A and
infrequently in B, indicating they have both high
recall and precision wrt category A. Symmetri-
cally, the same relationship exists for B and the
upper-right corner. I can formalize this score be-
tween a point’s coordinates and it’s respective cor-
ner. This intuition is represented by a score func-
tion sK(t) (K ∈ {A,B} and t ∈ V ) where

sK(t) =

{
‖〈1− xAt , xBt 〉‖ if K = A,
‖〈xAt , 1− xBt 〉‖ if K = B

. (5)

Other term scoring methods (e.g., regression
weights or a weighted log-odd-ratio with a prior)
may be used in place of Formula 5.

Maximal non-overlapping labeling of scatter-
plots is NP-hard (Been et al., 2007). Scattertext’s
heuristic is labeling points if space is available in
5While both are available, ordinal ranks are preferable to log
frequency since uninteresting stop-words often occupy dis-
proportionate axis space.

one of many places around a point. This is per-
formed iteratively, beginning with points having
the highest score (regardless of category) and pro-
ceeding downward in score. An optimized data
structure automatically constructed using Cozy
(Loncaric et al., 2016) holds the locations of drawn
points and labels.

The top scoring terms in classes B and A
(Democrats and Republicans in Figure 1) are listed
to the right of the chart. Hovering over points and
terms highlights the point and displays frequency
statistics.

Point colors are determined by their scores on s.
Those corresponding to terms with a high sB col-
ored in progressively darker shades of blue, while
those with a higher sA are colored in progressively
darker shades of red. When both scores are about
equal, the point colors become more yellow, which
creates a visual divide between the two classes.
The colors are provided by D3’s “RdYlBu” di-
verging color scheme from Colorbrewer6 via d37.

Other point colors (and scorings) can be used.
For example, Figure 4 shows coefficients of an `1
penalized log. reg. classifier on V features. Scat-
tertext, in this example, is set to color 0-scoring
coefficients light gray. Terms’ univariate predic-
tive power are still evident by their chart position.
See below8 for an interactive version.

Democratic frequency:
8 per 25,000 terms

Some of the 25 mentions:

Republican frequency:
26 per 25,000 terms

Some of the 62 mentions:
BARACK OBAMA
They knew they were part of something larger — a nation that triumphed over fascism and 
depression, a nation where the most innovative businesses turn out the world's best products, and 
everyone shared in that pride and success from the corner office to the factory floor.

MITT ROMNEY
That business we started with 10 people has now grown into a great American success story.

MARCO RUBIO

Infrequent Average Frequent

Republican Frequency

In
fre

qu
en

t
Av

er
ag

e
Fr

eq
ue

nt

D
em

oc
ra

tic
 F

re
qu

en
cy Top Democratic

president obama
barack
'm here
for
forward
class
obama
not
charlotte
boston
medicare
education
she
make

Top Republican
government
mitt
administration
success
leadership
story
business
to be
paul
i think
this
built it
is
and

Characteristic
obama
romney
barack
mitt
obamacare
biden
romneys
hardworking
bain
grandkids
billionaires
millionaires
ledbetter
buenas
pell
noches
bless
dreamers
congresswoman
bipartisan
wealthiest
risked
trillion
republicans
recession
insurmountable
gentlemen
electing
pelosi
understands

obama

romney
barack

millionaires

pell

wealthiest

trillion

gentlemen

fought
americans

medicare

repeal

prosper

reagan

america

blaming

founding

seniors

unemployment

wealthy

veterans

blessed

insult

fathers

olympics

stake

stimulus

democratic

stronger

fighting

rebuild

cuts

ours

forward

liberty

strongest

freedom

blame

staples

roosevelt

we

michelle
greatest

deeply

stark

hopes

charlotte

affordable

caring

success

belief

uniform

loyal

walks

loving

fulfill

troops mayor

better

voting

committed

pursuit

detroit

cents

ran

class

fair

drill

kept

jill
farmers

hands

pay

coal

loved

principles

notion

move

led

son

story

rid

administration

vision
lady

wealth

doors

israel

breaks

minds

iraq

mate

visited

threats

regulations
hundreds

pulled
apart

platform

fix

lie

trust

hunt

younger

rise

era

flying

facts

birth

fear

enemy

begin

coverage

que

rules

auto

para

hit

lay

federal

pass

felt

deep

chief

twice

solar

act

ownerscast

lesson

last

nations

safe
signed

insurance

simple

fail

bus

fall

battle edge

loans

church

deal
bad

planet

if

paid

fill

six

tradefuel

bank

bit

lots

join

hall

save

sent

none

girl

self

re

until

field

hours

ok

access
line

point

top

visit

girls
age

free

low

store

the story

1

roll back

23

his dream

story of

big government

ok.

16

mitt was

42

natural gas

3

take away

re elect

that has

it has

insurance companies

18

d.c.

2

a business

8 built it

i think

'm here

Democratic document count: 123; word count: 76,864
Republican document count: 66; word count: 58,138

Search for term   Type a word or two…  

Term: success
Download SVG

Figure 4: A cropped view of points being colored
using `1-logreg coefficients. Interactive version:
jasonkessler.github.io/st-sparse.html

5 Topical category discriminators
In 2012, how did Republicans and Democrats use
language relating to “jobs”, “healthcare”, or “mil-
itary” differently? Figure 5 shows, in the running
example, words similar to “jobs” that were char-
acteristic of the political parties.
6colorbrewer2.org
7github.com/d3/d3-scale-chromatic
8jasonkessler.github.io/sparseviz.html

89



Infrequent Average Frequent

Republican Frequency

In
fre

qu
en

t
Av

er
ag

e
Fr

eq
ue

nt

D
em

oc
ra

tic
 F

re
qu

en
cy Top Democratic

workers
insurance companies
companies
education
families
millionaires
medicare
pell grants
pay
seniors
insurance
affordable
industry
auto industry

Top Republican
job creators
unemployment
business
small business
problems
mitt romney
enterprise
mitt
leadership
free enterprise
federal government
fathers
olympics
reagan

Most similar
jobs
jobs overseas
create jobs
creating jobs
job
job growth
job creators
job creation
opportunities
businesses
responsibilities
paycheck
employees
workers
investments
insurance companies
small businesses
budgets
homes
moms
labor
programs
lobbyists
customers
factories
industries
business owners
solutions
schools
raise taxes

jobs

jobs overseas

workers

investments

insurance companies

moms

lobbyists

loans

skills

unemployment

business

role

small business

problems

burdens

costs

resources

business owner

people

duty

medicare

pell grants

pay

seniorsinsurance

cents

staples

affordable

doctors

invest

hours

years later

tax cut

nights

grandparents

women

coal

owners

deal

grantspell

experts

fathers

jill
hundreds

olympics16 trillion

reagan

they

paid

iraq

veterans

girls

roosevelt

banks

bain capital

administration
iowa hire

water

action

ballot

government

york

we

fix

texas

second term

regulations

firm

detroit

ok.

wealthy

freedom

lake city

60

others

early childhood

clean

fill
fuel

race

cuts

quality

game

40

america

8

girl

boys

tea

kate

buy

30

18

my grandfather

pays

simple

care

liberty
running mate

feet

team

15

doors

bain

hunt

street

decent

5

talks

hands

bus

earth

code

prosper

fulfill

law

risk

begin

42

line

gas

ceo

decade

move

story

bit

five

fine
fiscal

israel

16

term

fair shot

wall

rural

29

coverage

4
federal

lift

wait

edge
principles

ship

district

act

d.c.

fought

ok

guarantee

$ 1
mate

class

teaching

steel2

capital

thanks

pursuit

better

wisdom

con

six

arms

save

core

notion

fighting

join

re elect

church

bad

era

auto

age

birth

lie

trust

learn

hero

hit
gone

charlotte

fair

visitlay

son

table

point

greatest

felt allow

short

led

until

free

fall

blessed

fear

tells

kept

deny

signed

el

re

top

que Figure 5: Words and phrases that are seman-
tically similar to the word “jobs” are colored
darker on a gray-to-purple scale, and general and
category-specific related terms are listed to the
right. Note that this is a cropping of the upper
left-hand corner of the plot. Interactive version:
jasonkessler.github.io/st-sim.html.

In this configuration of Scattertext, words are
colored by their cosine similarity to a query
phrase. This is done using spaCy9-provided GloVe
(Pennington et al., 2014) word vectors (trained on
the Common Crawl corpus). Mean vectors are
used for phrases.

The calculation of the most similar terms as-
sociated with each category is a simple heuristic.
First, sets of terms closely associated with a cat-
egory are found. Second, these terms are ranked
based on their similarity to the query, and the top
rank terms are displayed to the right of the scatter-
plot (Figure 5).

A term is considered associated if its p-value
is <0.05. P-values are determined using MCQ’s
difference in the weighted log-odds-ratio with an
uninformative Dirichlet prior. This is the only
model-based method discussed in Monroe et al.
that does not rely on a large in-domain background
corpus. Since I am scoring bigrams in addition to
the unigrams scored by MCQ, the size of the cor-
pus would have to be larger to have high enough
bigram counts for proper penalization.

This function relies the Dirichlet distribution’s
parameter α ∈ R|V |+ . Following MCQ, αt = 0.01.
Formulas 16, 18 and 22 are used to compute z-
scores, which are then converted to p-values using
the Normal CDF of ζ̂A−Bw , letting y

(K)
t = φ(t,K)

st K ∈ {A,B} and t ∈ V .
As seen in Figure 5, the top Republican word

related to “jobs” is “job creators”, while “workers”
is the top Democratic term.

9spacy.io

6 Conclusion and future work
Scattertext, a tool to make legible, comprehen-
sive visualizations of class-associated term fre-
quencies, was introduced. Future work will in-
volve rigorous human evaluation of the usefulness
of the visualization strategies discussed.

Acknowledgments
Jay Powell, Kyle Lo, Ray Little-Herrick, Will
Headden, Chuck Little, Nancy Kessler and Kam
Woods helped proofread this work.

References
Ken Been, Eli Daiches, and Chee Yap. 2007. Dynamic

map labeling. IEEE-VCG .
Zsolt Bitvai and Trevor Cohn. 2015. Non-linear text

regression with a deep convolutional neural network.
In ACL.

Mike Bostock, Shan Carter, and Matthew Ericson.
2012. At the national conventions, the words they
used. In The New York Times.

Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. CoRR .

Glen Coppersmith and Erin Kelly. 2014. Dynamic
wordclouds and vennclouds for exploratory data
analysis. In ACL-ILLVI.

Justin Ryan Grimmer. 2010. Representational Style:
The Central Role of Communication in Representa-
tion. Ph.D. thesis, Harvard University.

Mahesh Joshi, Dipanjan Das, Kevin Gimpel, and
Noah A. Smith. 2010. Movie reviews and revenues:
An experiment in text regression. In HLT-NAACL.

Dan Jurafsky, Victor Chahuneau, Bryan Routledge, and
Noah Smith. 2014. Narrative framing of consumer
sentiment in online restaurant reviews. First Mon-
day .

Calvin Loncaric, Emina Torlak, and Michael D. Ernst.
2016. Fast synthesis of fast collections. In PLDI.

Burt L. Monroe, Michael P. Colaresi, and Kevin M.
Quinn. 2008. Fightin’ words: Lexical feature se-
lection and evaluation for identifying the content of
political conflict. Political Analysis .

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In EMNLP.

Christian Rudder. 2014. Dataclysm: Who We Are
(When We Think No One’s Looking). Crown Pub-
lishing Group.

Alexandra Schofield and Leo Mehr. 2016. Gender-
distinguishing features in film dialogue. NAACL-
CLfL .

H. Andrew Schwartz, Johannes C. Eichstaedt, and Mar-
garet L. et al. Kern. 2013. Personality, gender, and
age in the language of social media: The open-
vocabulary approach. PLOS ONE .

Julia Silge and David Robinson. 2016. tidytext: Text
mining and analysis using tidy data principles in r.
JOSS .

90


	Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ

