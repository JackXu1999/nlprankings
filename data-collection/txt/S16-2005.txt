



















































Implicit Semantic Roles in a Multilingual Setting


Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 45–54,
Berlin, Germany, August 11-12, 2016.

Implicit Semantic Roles in a Multilingual Setting

Jennifer Sikos Yannick Versley Anette Frank
Department of Computational Linguistics

Heidelberg University, Germany
Leibniz Science Campus “Empirical Linguistics and Computational Language Modeling”

{sikos, versley, frank}@cl.uni-heidelberg.de

Abstract

Extending semantic role labeling (SRL)
to detect and recover non-local arguments
continues to be a challenge. Our work is
the first to address the detection of im-
plicit roles from a multilingual perspec-
tive. We map predicate-argument struc-
tures across English and German sen-
tences, and we develop a classifier that dis-
tinguishes implicit arguments from other
translation shifts. Using a combination
of alignment statistics and linguistic fea-
tures, we achieve a precision of 0.68 de-
spite a limited training set, which is a sig-
nificant gain over the majority baseline.
Our approach does not rely on pre-existing
knowledge bases and is extendible to any
language pair with parallel data and de-
pendency parses.

1 Introduction

Understanding events and their participants is a
core NLP task, and SRL is the standard approach
for identification and labeling of these events in
text. SRL systems (Täckström et al., 2015; Roth
and Woodsend, 2014) have benefited NLP appli-
cations, and many approaches have been proposed
to transfer semantic roles from English to other
languages without further reliance on manual an-
notation (Kozhevnikov and Titov, 2013; Padó and
Lapata, 2009). However, event structures – both
predicates and their arguments – are known to shift
in the translation process, and this poor correspon-
dence presents a bottleneck for the transference of
semantic roles across languages. In some cases,
the semantic content of an entire argument can be
missing from the scope of its translated predicate.

Arguments that are omitted are often treated as
noise in state-of-the-art projection models; how-
ever, our work views them as a valuable source of

data - such arguments serve as naturally occurring
training data for implicit role detection. We tar-
get arguments that have been dislocated from their
predicates, or are dropped entirely, in translated
sentences. These non-isomorphic event structures
can not only be leveraged as new training data
for implicit role detection, but analyzing the shifts
that trigger these implicit roles can guide improve-
ments to systems that perform cross-lingual se-
mantic role projection.

Implicit Roles If a predicate is known to have
multiple semantic arguments, only a subset might
be expressed within the local boundary of its
clause or sentence. SRL models typically restrict
their search for semantic arguments to this local
domain and are not designed to recover arguments
situated in the broader discourse context. Non-
local role linking extends the SRL task by recov-
ering the semantic arguments not instantiated in
the local scope of the predicate. One complicating
factor is that these implicit arguments can either
be found in the context, and thereby are recover-
able, or they could be existentially interpreted and
might not correspond to any referent in the text at
all. In the examples below, the argument for the
predicate withdrawn in (1) is resolvable while the
implicit argument for reading in (2) is not:

(1) El Salvador is now the only Latin Ameri-
can country which still has troops in [Iraq]1.
Nicaragua, Honduras, and the Dominican Re-
public have withdrawn their troops ø.
Implicit role: Location

(2) I was sitting reading ø in the chair.
Implicit role: Theme

Implicit role labeling systems consistently re-
port low performance due to lack of training data.
Combining the few existing resources improves

1In this and other examples throughout the paper, the
brackets [] indicate the antecedent of the implicit argument.

45



performance (Feizabadi and Padó, 2015) when
they contribute diversity in predicate and argument
types. Since much of the multilingual parallel cor-
pora vary in domain and genre, mining these cor-
pora for implicit roles should provide new training
data that is sufficiently diverse to benefit the im-
plicit role labeling task.

Predicate-Argument Structures across Lan-
guages Translational correspondences have been
used in previous work to acquire resources for su-
pervised monolingual tasks, such as word sense
disambiguation (Diab and Resnik, 2002). Simi-
larly, semantic role annotations can be transferred
to new languages when predicate-argument struc-
tures are stable across language pairs (Padó and
Lapata, 2009). In this work, we target predicate-
argument structures that do not express such sta-
bility and have shifted in the translation process.
In example (3), the role farmers is dropped en-
tirely in the aligned German sentence:

(3) The only change is that [farmers] are not
required to produce.

Die
The

einzige
only

Neuerung
change

ist,
is,

dass
that

nicht
not

gefordert
required

wird
are

zu
to

produzieren.
produce.

The challenge in detecting implicit roles across
languages is that these omissions represent only
a fraction of the kinds of poor alignments that can
occur. In fact, different types of translational shifts
may occur that do not constitute cases of implicit
role omission. Such factors include: change in
part-of-speech from a verbal predicate to a noun
or adjective, light verb constructions, single predi-
cates that are expressed as both a verb and comple-
ment in the target language, and expressions with
no direct translations (Samardžic et al., 2010).

Aims and Contributions To find implicit (non-
local) semantic roles in translation, we distin-
guish role omissions from other types of transla-
tion shifts. We test linguistic features to automat-
ically detect such role omissions in parallel cor-
pora. We divide our work into alignment (Section
3.1) and classification (Section 3.2), with an anno-
tation task for data construction (Section 4).

Our contributions are (i) a novel method for au-
tomatically identifying implicit roles in discourse,
(ii) a classifier that is able to distinguish general
translational divergences from true cases of im-
plicit roles, (iii) an annotated, multilingual dataset

of manually tagged implicit arguments, and (iv) a
classifier that achieves precision of 0.68 despite a
small training set size, which is a significant im-
provement over a majority class baseline. Finally,
we perform detailed analysis of our annotation and
automatic classification results.

2 Related Work

2.1 Implicit Semantic Role Labeling

Previous resources for implicit SRL were devel-
oped over diverging schemas, texts, and predi-
cate types. An initial dataset was constructed in
the SemEval-2010 Shared Task “Linking Events
and Their Participants in Discourse”, under the
FrameNet paradigm; authors annotated short sto-
ries with implicit arguments and their antecedents,
resulting in approx. 500 resolvable and 700 non-
resolvable implicit roles out of roughly 3,000
frame instances (Ruppenhofer et al., 2010). Ger-
ber and Chai (2010) focused on the implicit ar-
guments of a constrained set of 10 nominal predi-
cates in the NomBank scheme, annotating 966 im-
plicit role instances for these specific predicates.

Numerous studies on the recovery of implicit
roles have concluded that a lack of training data
has been the stopping point towards improvements
on the implicit role labeling task (Gorinski et al.,
2013; Laparra and Rigau, 2013). To address this
problem, Silberer and Frank (2012) generated ar-
tificial training data by removing arguments from
coreference chains and showed that adding such
instances yields performance gains. However,
their quality was low and later work (Roth and
Frank, 2015) has shown that smaller numbers of
naturally occurring training data performed bet-
ter. Roth and Frank (2015) applied a graph-based
method for automatically acquiring high-quality
data for non-local SRL using comparable mono-
lingual corpora. They detect implicit semantic
roles across documents and their antecedents from
the prior context, again following cross-document
links. In contrast, our work does not rely on se-
mantic resources (SRL and lexical ontologies), but
builds on parallel corpora enriched with dependen-
cies and word alignments. Finally, Stern and Da-
gan (2014) generate training data for implicit SRL
from textual entailment data sets. However, this
type of resource needs to be manually curated.

46



2.2 Cross-lingual Annotation Projection
Aside from English, resources for SRL only ex-
ist for a select number of languages. For the lan-
guages that have such resources, annotated data
still tends to vastly underrepresent the variability
and breadth of coverage that exists for English. To
extend SRL to new languages without reliance on
manual annotation, models for role transference
have been developed under both the supervised
(Padó and Lapata, 2009; Akbik et al., 2015) and
unsupervised (Kozhevnikov and Titov, 2013) set-
ting. Most relevant to our work are previous stud-
ies that address the problem of projecting semantic
role annotations across parallel corpora.

To transfer semantic annotations across lan-
guages, Padó and Lapata (2009) score the con-
stituents of word-aligned parallel sentences and
project role labels for the arguments that achieve
highest constituent alignment scores. Akbik et
al (2015) use filtered projection by constraining
alignments through lexical and syntactic filters to
ensure accuracy in predicate and argument role
projection. Complete predicate-argument map-
pings are then used to bootstrap a classifier to re-
cover further unaligned predicates and arguments.

3 Detecting Implicit Roles across
Languages

We hypothesize that implicit semantic roles can
be found in translated sentences, even in corpora
where sentences are typically close translations.
Our goal is to distinguish implicit roles from other
translation shifts that cause poor alignment in SRL
projection. A model is constructed based on lex-
ical, syntactic, and alignment properties of paral-
lel predicate-argument structures, and this classi-
fier is, to the best of our knowledge, the first to
detect a wide range of omitted roles in multilin-
gual, parallel corpora. Our implicit role detection
applies to both core and non-core arguments and
is not dependent on large-scale SRL resources.

3.1 Identifying Poorly Aligned Arguments
Our first goal is to find candidates for implicit
arguments by aligning predicate-argument struc-
tures across parallel English and German sen-
tences.

Predicate and Argument Identification We tar-
get all non-auxiliary verbs as predicates, and de-
tect their dependents through grammatical rela-
tions in dependency parses. We extract subjects,

direct objects, indirect objects, prepositional ob-
jects, adverbial or nominal modifiers as well as
embedded clauses. These recover both the core
and non-core arguments (adjuncts) of the pred-
icate.2 Arguments are attached to their nearest
predicate and cannot be attached to more than one,
as might occur in cases of embedded clauses.

Aligning Arguments for Detection of Unaligned
Roles We use word alignments between paral-
lel source (sl) and target (tl) language sentences
as input. A predicate in the source language psl

is mapped to a predicate in the target language
ptl if there exists a word alignment link between
them, and their arguments are then aligned using
the scoring function ArgALp (Eq 3). ArgALp
uses word alignment links between the source and
target arguments asl, atl of the aligned predicate
pair to produce an optimal mapping between cor-
responding predicate-argument structures.

For scoring, we adapt Padó and Lapata (2009)’s
constituent alignment-based overlap measure (Eq
1) to dependencies, where yield(a) denotes the set
of words in the yield (headword and dependents)
of an argument a, and align(a) the set of words
in the target language that are aligned to the yield
of a. Because the automatic word alignment tool
gives predictions for links in both directions, we
apply this asymmetric measure from the English-
German and German-English links and average
their results (Eq 2). The ascore is computed for
the Cartesian product Asl × Atl over all source
and target arguments of the aligned predicates psl

and ptl. We select the argument alignments A′sl

× A′tl ⊆ Asl × Atl that return the maximal sum
of scores for all arguments across the aligned ar-
gument structure (Eq 3).

Anticipating noise in the word alignments, we
set a threshold to enforce accurate mappings be-
tween arguments. From the obtained mappings,
we consider any argument whose alignment score
does not exceed a threshold Θ as unaligned and
thus as a candidate for an implicit role. The selec-
tion of threshold Θ is discussed in Section 5.

ovlp(asl, atl) =
| align(asl) ∩ yield(atl) |
| align(asl) ∪ yield(atl) | (1)

ascore(asl, atl) =
ovlp(asl, atl) + ovlp(atl, asl)

2
(2)

2Since we are treating arguments and adjuncts alike, in
the following we loosely refer to both types of dependents as
‘arguments’.

47



Alignment Aligned Alignment

Type Arguments Score

headword asl1 ,– 0

asl2 ,a
tl
1 1

asl3 ,– 0

ascore asl1 ,–
tl 0

asl2 ,a
tl
1 (1 + 1)/2 = 1.0

asl3 ,a
tl
3 (2/3 + 2/3)/2 = 0.67

Figure 1: Predicate-argument structures with noisy word alignments (left), and alignment scores for
the arguments (right). Headword scoring aligns only headwords of the source (asl) and target (atl)
arguments, while ascore uses headwords and dependents of an entire argument span for alignment.

ArgALp = arg max
A′sl×A′tl⊆Asl×Atl

∑
Asl×Atl ascore(a

sl, atl)

where

Asl ×Atl = {〈asl, atl〉 | asl ∈ 〈psl, asl〉, atl ∈ 〈ptl, atl〉}
(3)

An example of the alignment scoring is given
in Figure 1, where predicates and arguments are
detected over parallel English-German sentences,
and word alignments are automatically generated.
The argument ‘an in-depth analysis’ consists of
a headword and two dependents, with two noisy
word alignments that link the arguments across
languages. Given these word alignment links, the
ascore (Eq 2) is computed by taking the number
of alignments and the yield of the arguments for
both English and German, and these scores are
then averaged for a final alignment score of 0.67.
In this case, the scoring function still produces
correct mappings across the predicate-argument
structures despite imperfect word alignments, and
an implicit role, We, is correctly unaligned to the
German sentence.

3.2 Classification of Poor Alignments as
Implicit Roles

Our objective is to build a classifier that automati-
cally detects implicit roles across parallel corpora.
To achieve this goal, we construct a classifier that
takes as input an unaligned argument in English
and, based on linguistic features in the aligned En-
glish and German sentences, determines whether
this unaligned argument is an implicit role in Ger-
man. Our dataset, described in Section 4.2, con-
sists of instances of poorly aligned roles that have
been annotated as either implicit, not implicit,

or not a role of the predicate. In classification,
we reduce the annotation classes (implicit/not im-
plicit/not a role of the predicate) to a binary de-
cision where the positive class represents the im-
plicit roles, and the negative class is any unaligned
argument that annotators determined as either not
implicit or not a semantic role. We reduced the
task to a binary decision to avoid sparsity in the
classification.

Features We hypothesize that we can predict the
existence of an implicit role through features of the
predicate-argument structures in the source and
target languages. These features include mono-
lingual predicate-argument structures, as well as
cross-lingual features that represent the quality
of the alignments across the parallel sentences.
Monolingual features encode the syntactic prop-
erties of the arguments and predicates for source
and target sentences, as well as sentential-level
features that include the presence of modal and
auxiliary verbs and conjunctions. To incorporate
cross-lingual information, the alignment scores
described in Section 3.1 are kept as features to the
classifier, based on our assumption that the over-
all alignment between source and target predicate-
argument structures should impact the classifica-
tion of an implicit role. Both monolingual and
cross-lingual features apply to surrounding pred-
icate/arguments, where arguments can either be
aligned or unaligned, and predicates that have
fully aligned structures are considered complete.
A complete list of features is shown in Table 1.

Classifiers We experimented with three classi-
fiers, a Support Vector Machine (SVM) with a lin-

48



TYPE FEATURE XLING

Argument Lemma

POS

Grammatical relation to predicate

Distance to predicate

Number of dependents

Syntactic path to predicate

Alignment type of neighboring ar-
gument (aligned, unaligned)

+

Predicate Lemma

POS

Total number of arguments

Number of aligned arguments +

Number of unaligned arguments +

ArgALp score +

Alignment type of neighboring
predicate (complete, incomplete)

+

Sentential Presence of a modal or auxiliary

Sentence-final punctuation marks
before end

Conjunction between psl,psl-1

Sum of ArgALp scores

Total number of arguments

Total number of predicates

Sum of ArgALp scores over all
predicates

+

Table 1: Features investigated for classification,
where Xling are cross-lingual features.

ear kernel, Decision Tree, and Gradient Boosting,
under the framework of the Scikit-learn library
(Pedregosa et al., 2011).

4 Constructing a Dataset for Classifying
Implicit Arguments

This section presents the construction of our ex-
perimental dataset for implicit role detection.

4.1 Corpora and Tools

We conduct our experiments over the Europarl
corpus (Koehn, 2005), which contains over 1.9
million aligned sentences in our target lan-
guages. Anticipating noise in the automatic word
alignments, we first take sentences from manu-
ally word-aligned German-English Europarl data
(Padó and Lapata, 2005) to conduct our initial
experiments. These sentences give us an upper
bound for the number of implicit roles we should
expect to obtain. Automatic word alignments are
generated with GIZA++ (Och and Ney, 2003).

Predicates and their arguments are first detected

through dependency parses on English and Ger-
man parallel corpora. Parses are generated for En-
glish with ClearNLP (Choi and McCallum, 2013).
German sentences are run through the MarMot
morphological analyzer (Mueller et al., 2013), and
dependency parses for German are then generated
using the RBG Parser (Lei et al., 2014). The
Universal Dependencies project facilitates cross-
lingual consistency in parsing and provides bet-
ter compatibility amongst multiple languages. We
trained the RBG Parser with the Universal Depen-
dencies tagset (Rosa et al., 2014), and thus our
argument detection can be applied to other lan-
guages in the Universal Dependencies project.

4.2 Annotation of Poorly Aligned Arguments

Annotation Instances Our goal is to find any ar-
gument that is either missing or dislocated from
its predicate in translation. With this objective in
mind, we focused our annotation on incomplete
predicate structures whose argument(s) remained
unaligned. Any argument with scores below the
alignment threshold (see Section 3.1) was a candi-
date for annotation.

Annotation Task and Guidelines Three annota-
tors worked on this task. Each annotator was a na-
tive German speaker with high fluency in English,
and had taken at least one undergraduate course
in linguistics. Annotators were given guidelines
that define predicates as events or scenarios, and
semantic roles as an element that has a semantic
dependence on the predicate, including the who,
what, where, when, and why type of information.
Implicit roles were defined as “any role that is
missing from the scope, or clausal boundary, of
the predicate”. Each annotator was trained on a
test set of 10 example sentences.

Annotators were given pairs of sentences with
aligned predicates in English and German, where
the English predicate had a poorly aligned argu-
ment. Annotation instances were presented as:
two preceding English sentences, the English sen-
tence with both the argument and predicate high-
lighted, the German sentence with the aligned
predicate highlighted, and two preceding German
sentences. An example of the annotation task is
shown in Figure 2.

The annotation task was broken into two sub-
tasks. First, annotators were asked to judge
whether the marked argument is a correct seman-
tic role for the English predicate. The second sub-

49



Context - 2 preceding English sentences

——

The only change is that [farmers] are not –required– to
produce .
Die einzige Neuerung ist , dass nicht –gefordert– wird
zu produzieren .

——

Context - 2 preceding German sentences

—————————————————————-

[farmers]
Can ‘farmers’ be considered a role of the English
predicate ‘required’?

If ‘no’: please choose:
not a role of English predicate

Can ‘farmers’ be considered an implicit role for
the German predicate ‘gefordert’?

If ‘no’: please choose:
not an implicit role of German predicate
If ‘yes’: please indicate the location of the German
translation of ‘farmers’ by marking it in (**)

Figure 2: Example annotation task. Aligned pred-
icates are marked in dashes (–) and implicit role
candidates are surrounded by squared brackets [].

task asked annotators to judge whether a transla-
tion for the argument was available in the scope
of the highlighted German predicate. If it was not
available in the scope, they were asked to annotate
the example as implicit.

Difficult Annotation Cases The annotations were
adjudicated by one of the authors, and the annota-
tor with the highest agreement with the adjudicator
was asked to complete the entire dataset.

Cases that resulted in higher annotator disagree-
ment included arguments of nominal predicates
that were themselves the argument of the aligned
predicate. In Example 4 below, 30 August is a
role for the nominal predicate participation but not
continue:

(4) The massive participation [from 30 August]
must continue.

Other difficult annotation cases included roles
that were partially, or entirely, encoded in the
translated predicate. These included temporal ad-
juncts that could either be interpreted as present
tense or implicit in the translated sentence:

(5) I will [now] give the floor to the President

Ich
I

gebe
give

dem
the

Präsidenten
President

das
the

Wort
floor

After a review of these difficult cases, annota-
tion guidelines were modified and annotators were
re-trained.

Annotation Quality Inter-annotator agreement
was measured by Cohen’s Kappa scores over 114
instances, and the entire 700 candidates were then
completed by Annotator 1. One of the authors ad-
judicated for agreement. Results are given in Table
2 where “Role + Implicit” reports Kappa scores
over all three categories - not a role, implicit, and
not implicit, while “Implicit” reports agreement
over binary implicit vs non-implicit decisions.

ANNOTATOR vs ADJUDICATOR ROLE + IMPLICIT IMPLICIT

ANNOTATOR 1 0.76 0.96

ANNOTATOR 2 0.48 0.92

ANNOTATOR 3 0.29 0.69

Table 2: Kappa agreements

Annotation Results In total, we took 700 poorly
aligned arguments whose scores were below the
alignment threshold (Section 3.1), where 500 were
selected from manual word alignments and 200
from GIZA++ alignments. The 500 candidate ar-
guments were sampled from 987 gold-aligned Eu-
roparl sentences, in which over 3,000 arguments
fell below the threshold. The 200 candidates were
sampled from 500 automatically aligned Europarl
sentence pairs (excluding the sentences from the
manually aligned dataset), with nearly 3,000 argu-
ments below the threshold, to estimate the differ-
ence in implicit roles between manual and auto-
matic word alignments.

Over the completed dataset, results for the an-
notation types are given in Table 3. Out of the
manually aligned Europarl sentences, annotations
produced 45 positive implicit role instances (9%
of the annotated candidates). The automatic align-
ments, with 200 examples, contained 6 instances
(3% of the annotated candidates) of implicit roles.
Over the total 700 instances, 24.5% were classified
as ‘not a predicate role’, 68.3% as ‘not implicit’,
and 7.2% as ‘implicit’.

INSTANCES NOT A ROLE NOT IMPLICIT IMPLICIT

Manual 500 154 301 45
GIZA++ 200 18 176 6

Table 3: Final annotation dataset.

50



Classifier P R F1

Majority Baseline 0 0 0

SVM-ablated 0.6805 0.4444 0.5128
SVM-all 0.1555 0.2238 0.18333

Decision Tree-ablated 0.6682 0.4155 0.4934

Decision Tree-all 0.4134 0.2222 0.2748

Gradient Boosting-ablated 0.6688 0.3777 0.4631

Gradient Boosting-all 0.6466 0.2222 0.3268

Table 4: Precision, Recall and F1 for the positive
class (implicit role), with stratified 5-fold CV.

5 Classification Experiments and Results

5.1 Argument Alignment and Scoring

With the scoring function described in Section
3.1, perfectly aligned arguments should produce a
score of 1.0. We experimentally set the threshold
Θ for the minimum alignment score at 0.2 for ar-
guments such that arguments with imperfect word
alignments will still be aligned.

5.2 Classification of Implicit Arguments

The data set constructed in Section 4 resulted in 51
manually validated implicit roles and 649 negative
instances that were input for classification.

We measure precision, recall, and F1 scores,
and for the SVM and Gradient Boosting classi-
fiers we experimented with parameters to optimize
precision. The SVM classifier with a linear ker-
nel produced the highest scores, but results were
closely followed by Decision Tree and Gradient
Boosting classifiers. For the SVM classifier, we
experimented with different regularization {0.5,
1, 10, 20} and class weight increments {None,
1:2, 1:10} and found the highest precision scores
were achieved with C=0.5 and class weight 1:2.
In Gradient Boosting, we experimented with max
depth {1, 2, 3} and found the highest precision
scores were obtained with a max depth of 2. Since
the data set is heavily biased towards the negative
class, we divided training and test sets with a strat-
ified 5-fold cross-validation (CV). We later exper-
imented with upsampling for the positive class but
found no significant improvement.

Feature Ablation To determine the optimal fea-
ture set, we performed ablation tests by incre-
mentally removing a feature and performing train-
ing/testing over the reduced feature set. Ablation
was performed individually for each classifier. Af-
ter these tests, we eliminated features that caused

Type Feature

asl lemma, POS, path to predicate
asl+1 POS, path to predicate
asl–1 alignment type, number of dependents
ptl POS
ptl–1 sum of ArgALp scores
ptl+1 POS, number of arguments, number of

unaligned arguments, sum of ArgALp
scores, alignment type

Table 5: Final feature set used in classification.
Notation is defined in Section 3.1, where ± 1 are
the arguments/predicates preceding (–1) and fol-
lowing (+1) the candidate.

a drop in performance and used only the best per-
forming features in the final classification. The fi-
nal feature set is shown in Table 5.

The SVM model obtains the best results of 0.68
precision and F1-score of 0.51 with the ablated
feature set, closely followed by the other classifier
models and outperforming the majority baseline,
which always predicts the negative class (see Ta-
ble 4 for both ablated and full feature results).

Feature Analysis The final feature set used in
the classification experiment included both cross-
lingual features of the predicate and arguments on
source/target sentences, as well as monolingual
predicate and argument features. The ablation re-
sults support our initial hypothesis that the sur-
rounding predicate/argument structures and align-
ment scores are relevant to the detection of an
omitted role.

5.3 Analysis of Results

Translation Shifts that Trigger Implicit Roles
Through observation of the positive instances, we
determined a number of syntactic environments
that trigger omission of semantic roles from En-
glish to German. Shift in voice, finite to infi-
nite verb forms, and coordination could all mo-
tivate the deletion of a role across translated sen-
tences. While these syntactically licensed implicit
roles composed 57% of our positive instances, a
large number (43%) were not found to have an
explanation on syntactic grounds alone. In these
cases, the arguments seem to have been omitted
by pragmatic or semantic factors. The distribution
of these shift types over our dataset is given in Ta-
ble 6.

51



Voice A change from active (source) to passive
(target). Subjects are dropped in translation:

(6) The more [we] refuse to democratize the in-
stitutions ....
Je
The

mehr
more

die
the

Demokratisierung
democratization

der
of the

Institutionen
institution

verweigert
refused

wird
are

...

Coordination An argument might be the repeated
subject of two conjoined clauses, but expressed as
a shared argument in the parallel sentence:

(7) I was faced with this system and [I] do not
know any parliament
Ich
I

fand
faced

dieses
this

System
system

vor
before

und
and

kenne
know

kein
no

Parlament
parliament

Extraposition Complex clausal embeddings can
cause roles to be extraposed from their predicates
in the target language text:

(8) ...but would also want to encourage both par-
ties [to observe the spirit of this new agree-
ment].
...er
...it

kann
can

die
that

beiden
both

Parteien
parties

nur
only

veranlassen
encourage

wollen,
want,

[den
the

Geist
spirit

dieses
of-this

neuen
new

Abkommens
agreement

zu
to

achten].
observe

Coordination and extraposition are borderline
cases with regard to the non-locality of roles.
PropBank does annotate coordinated arguments,
and in these cases the syntactic parse tree can be
leveraged for recovery of the non-local role. How-
ever, we still consider these implicit arguments
since they are expressed outside of the local scope
of the predicates.

Nonfinite Similar to change in voice, the subject
of a finite verb can be dropped when the translated
verb is nonfinite:

(9) I would ask that [they] reconsider these deci-
sions
Ich
I

bitte,
ask,

diese
these

Entscheidung
decisions

zu überdenken
to reconsider

Semantic/Pragmatic A role can be dropped in
translation without a structural shift that licenses
the omission. In these instances, the role could
have been incorporated into the aligned sentence
without a change to the syntactic environment.

(10) ... I am asking you to do this directly, [in
this House].

...wende

...turn
ich
I

mich
myself

hiermit
hereby

direkt
directly

an
to

Sie
you

.

Since the directionality of our implicit role
search focused on English to German, we do not
account for syntactic shifts that could cause omis-
sions in the opposite direction, i.e. German to En-
glish. There are imperative constructions in Ger-
man that overtly encode the addressee of the com-
mand (“go outside” in English can be translated as
“go you outside” in German) which can trigger im-
plicit roles in translation from German to English.

Shift Type Count %
Voice 7 14%

Coordination 8 16%

Extraposition 8 16%

Nonfinite 6 11%

Semantic/Pragmatic 22 43%

Table 6: Shift types that trigger implicit roles.

Semantic Role Types of Omitted Arguments We
adopt the VerbNet roleset (Kipper et al., 2000) to
manually label semantic role across all our im-
plicit argument instances. A full analysis of the
role types, shown in Table 7, found that a majority
of implicit roles are Agent and Theme. This re-
flects the general distributions for role frequency
(Merlo and Van Der Plas, 2009), but could also
be due to the syntactic shifts that produce a higher
omission of the subject, such as passivization and
coordination, which are commonly filled by the
Agent and Theme roles.

Core Role Count Non-Core Role Count

Agent 15 Time 6

Theme 14 Topic 5

Recipient 3 Location 3

Experiencer 2 Manner 1

Cause 2

Table 7: Thematic roles, both core and non-core,
of the implicit cases.

Antecedents to the Implicit Role The analyses
above described the shift types that trigger argu-
ment omission, but only two of these types, co-
ordination and extraposition, would guarantee the
missing argument to be recoverable from the non-
local context. Cases where the annotators were

52



able to recover the antecedent roles, either from
the previous clause or sentences, were less than the
majority (21 out of the 51 cases), while many in-
stances were not instantiated in the non-local con-
text. Table 8 gives the proportion of recovered an-
tecedents according to shift types. The fact that
extraposition and coordination cases yield higher
number of resolvable roles can be exploited in fu-
ture work for antecedent linking.

Shift Type Resolvable Not resolvable

Voice 1 6

Coordination 8 0

Extraposition 8 0

Nonfinite 1 5

Semantic/Pragmatic 3 19

Table 8: Availability of the antecedent in the sur-
rounding context.

6 Conclusion and Future Work

In this work, we investigated the hypothesis that
implicit semantic roles can be identified in transla-
tion. Our method is knowledge-lean and achieves
respectable performance despite a small training
set. While the present work has focused on miss-
ing arguments of verbal predicates, implicit role
detection in this multilingual framework can be
easily extended to nominal predicates. Combin-
ing both predicate types is expected to improve
the overall results, as some of the noise we are
currently observing pertains to implicit roles oc-
curring with nouns. Additional noise is produced
by the automatic word alignments, which can be
addressed by employing triangulation techniques
using multiple language pairs. Further, with our
current classifier we can predict role omissions
across parallel sentences with better accuracy than
reliance on noisy word alignments alone, and with
these predictions we can generate better candi-
dates for annotation and reduce the time and cost
of future annotation effort.

A next step from the current work would be
to automatically recover the antecedent of the im-
plicit role in the target language when it is avail-
able. By doing so, we can construct new training
data for monolingual implicit role labeling, im-
prove transference of semantic roles across paral-
lel corpora, and generate novel training data for
implicit role labeling for new languages.

7 Acknowledgments

This research has been conducted within the Leib-
niz Science Campus “Empirical Linguistics and
Computational Modeling”, funded by the Leibniz
Association under grant no. SAS-2015-IDS-LWC
and by the Ministry of Science, Research, and
Art (MWK) of the state of Baden-Württemberg.
We thank our annotators Leo Born, Max Müller-
Eberstein and Julius Steen for their contribution.

References
Alan Akbik, Laura Chiticariu, Marina Danilevsky,

Yunyao Li, Shivakumar Vaithyanathan, and Huaiyu
Zhu. 2015. Generating High Quality Proposi-
tion Banks for Multilingual Semantic Role Labeling.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing, pages 397–407, Beijing, China.

Jinho D. Choi and Andrew McCallum. 2013.
Transition-based Dependency Parsing with Selec-
tional Branching. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 1052–1062, Sofia, Bulgaria.

Mona Diab and Philip Resnik. 2002. An unsuper-
vised method for word sense tagging using parallel
corpora. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 255–262. Association for Computational Lin-
guistics.

Parvin Sadat Feizabadi and Sebastian Padó. 2015.
Combining Seemingly Incompatible Corpora for
Implicit Semantic Role Labeling. In Proceedings of
the Fourth Joint Conference on Lexical and Compu-
tational Semantics, pages 40–50, Denver, Colorado,
June.

Matthew Gerber and Joyce Y Chai. 2010. Beyond
NomBank: A study of implicit arguments for nom-
inal predicates. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1583–1592.

Philip Gorinski, Josef Ruppenhofer, and Caroline
Sporleder. 2013. Towards weakly supervised res-
olution of null instantiations. In Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013), pages 119–130.

Karin Kipper, Hoa Trang Dang, Martha Palmer, et al.
2000. Class-based construction of a verb lexicon. In
AAAI/IAAI, pages 691–696.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5, pages 79–86.

53



Mikhail Kozhevnikov and Ivan Titov. 2013. Cross-
lingual Transfer of Semantic Role Labeling Mod-
els. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 1190–1200, Sofia, Bulgaria, August.

Egoitz Laparra and German Rigau. 2013. ImpAr: A
Deterministic Algorithm for Implicit Semantic Role
Labelling. In ACL (1), pages 1180–1189.

Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-Rank Tensors for
Scoring Dependency Structures. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics, pages 1381–1391, Bal-
timore, Maryland.

Paola Merlo and Lonneke Van Der Plas. 2009. Ab-
straction and generalisation in semantic role labels:
PropBank, VerbNet or both? In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1-Volume 1, pages 288–296. Association for
Computational Linguistics.

Thomas Mueller, Helmut Schmid, and Hinrich
Schütze. 2013. Efficient Higher-Order CRFs for
Morphological Tagging. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 322–332, Seattle, Wash-
ington, USA.

Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51.

Sebastian Padó and Mirella Lapata. 2005. Cross-
lingual projection of role-semantic information. In
Proceedings of HLT/EMNLP 2005, Vancouver, BC.

Sebastian Padó and Mirella Lapata. 2009. Cross-
lingual Annotation Projection for Semantic
Roles. Journal of Artificial Intelligence Research,
36(1):307–340.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.

Rudolf Rosa, Jan Masek, David Marecek, Martin
Popel, Daniel Zeman, and Zdenek Zabokrtskỳ.
2014. HamleDT 2.0: Thirty Dependency Treebanks
Stanfordized. In LREC, pages 2334–2341.

Michael Roth and Anette Frank. 2015. Inducing Im-
plicit Arguments from Comparable Texts: A Frame-
work and its Applications. Computational Linguis-
tics, 41(4):625–664.

Michael Roth and Kristian Woodsend. 2014. Compo-
sition of Word Representations Improves Semantic
Role Labelling. In EMNLP, pages 407–413.

Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2010.
Semeval-2010 task 10: Linking events and their
participants in discourse. In Proceedings of the 5th
International Workshop on Semantic Evaluation,
pages 45–50.

Tanja Samardžic, Lonneke Van Der Plas, Goljihan
Kashaeva, and Paola Merlo. 2010. The Scope and
the Sources of Variation in Verbal Predicates in En-
glish and French. In Proceedings of the Ninth In-
ternational Workshop on Treebanks and Linguistic
Theories, pages 199–211.

Carina Silberer and Anette Frank. 2012. Casting Im-
plicit Role Linking as an Anaphora Resolution Task.
In Proceedings of the First Joint Conference on Lex-
ical and Computational Semantics-Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation, pages 1–
10.

Asher Stern and Ido Dagan. 2014. Recognizing im-
plied predicate-argument relationships in textual in-
ference. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 739–744, Bal-
timore, Maryland.

Oscar Täckström, Kuzman Ganchev, and Dipanjan
Das. 2015. Efficient inference and structured learn-
ing for semantic role labeling. Transactions of the
Association for Computational Linguistics, 3:29–41.

54


