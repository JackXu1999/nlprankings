



















































Automatic Transliteration of Romanized Dialectal Arabic


Proceedings of the Eighteenth Conference on Computational Language Learning, pages 30–38,
Baltimore, Maryland USA, June 26-27 2014. c©2014 Association for Computational Linguistics

Automatic Transliteration of Romanized Dialectal Arabic

Mohamed Al-Badrashiny†, Ramy Eskander, Nizar Habash and Owen Rambow
†Department of Computer Science, The George Washington University, Washington, DC

†badrashiny@gwu.edu
Center for Computational Learning Systems, Columbia University, NYC, NY

{reskander,habash,rambow}@ccls.columbia.edu

Abstract
In this paper, we address the problem
of converting Dialectal Arabic (DA) text
that is written in the Latin script (called
Arabizi) into Arabic script following the
CODA convention for DA orthography.
The presented system uses a finite state
transducer trained at the character level
to generate all possible transliterations for
the input Arabizi words. We then filter
the generated list using a DA morpholog-
ical analyzer. After that we pick the best
choice for each input word using a lan-
guage model. We achieve an accuracy of
69.4% on an unseen test set compared to
63.1% using a system which represents a
previously proposed approach.

1 Introduction

The Arabic language is a collection of varieties:
Modern Standard Arabic (MSA), which is used
in formal settings and has a standard orthogra-
phy, and different forms of Dialectal Arabic (DA),
which are commonly used informally and with in-
creasing presence on the web, but which do not
have standard orthographies. While both MSA
and DA are commonly written in the Arabic script,
DA (and less so MSA) is sometimes written in
the Latin script. This happens when using an Ara-
bic keyboard is dispreferred or impossible, for ex-
ample when communicating from a mobile phone
that has no Arabic script support. Arabic written
in the Latin script is often referred to as “Arabizi”.
Arabizi is not a letter-based transliteration from
the Arabic script as is, for example, the Buck-
walter transliteration (Buckwalter, 2004). Instead,
roughly speaking, writers use sound-to-letter rules
inspired by those of English1 as well as informally

1In different parts of the Arab World, the basis for the
Latin script rendering of DA may come from different lan-

established conventions to render the sounds of the
DA sentence. Because the sound-to-letter rules
of English are very different from those of Ara-
bic, we obtain complex mappings between the two
writing systems. This issue is compounded by the
underlying problem that DA itself does not have
any standard orthography in the Arabic script. Ta-
ble 1 shows different plausible ways of writing an
Egyptian Arabic (EGY) sentence in Arabizi and
in Arabic script.

Arabizi poses a problem for natural language
processing (NLP). While some tools have recently
become available for processing EGY input, e.g.,
(Habash et al., 2012b; Habash et al., 2013; Pasha
et al., 2014), they expect Arabic script input (or a
Buckwalter transliteration). They cannot process
Arabizi. We therefore need a tool that converts
from Arabizi to Arabic script. However, the lack
of standard orthography in EGY compounds the
problem: what should we convert Arabizi into?
Our answer to this question is to use CODA, a
conventional orthography created for the purpose
of supporting NLP tools (Habash et al., 2012a).
The goal of CODA is to reduce the data sparseness
that comes from the same word form appearing in
many spontaneous orthographies in data (be it an-
notated or unannotated). CODA has been defined
for EGY as well as Tunisian Arabic (Zribi et al.,
2014), and it has been used as part of different ap-
proaches for modeling DA morphology (Habash
et al., 2012b), tagging (Habash et al., 2013; Pasha
et al., 2014) and spelling correction (Eskander et
al., 2013; Farra et al., 2014).

This paper makes two main contributions. First,
we clearly define the computational problem of
transforming Arabizi to CODA. This improves
over previous work by unambiguously fixing the

guages that natively uses the Latin script, such as English
or French. In this paper, we concentrate on Egyptian Arabic,
which uses English as its main source of sound-to-letter rules.

30



target representation for the transformation. Sec-
ond, we perform experiments using different com-
ponents in a transformation pipeline, and show
that a combination of character-based transduc-
tion, filtering using a morphological analyzer, and
using a language model outperforms other archi-
tectures, including the state-of-the-art system de-
scribed in Darwish (2013). Darwish (2013) pre-
sented a conversion tool, but did not discuss con-
version into a conventionalized orthography, and
did not investigate different architectures. We
show in this paper that our proposed architecture,
which includes an EGY morphological analyzer,
improves over Darwish’s architecture.

This paper is structured as follows. We start out
by presenting relevant linguistic facts (Section 2)
and then we discuss related work. We present our
approach in Section 4 and our experiments and re-
sults in Section 5.

2 Linguistic Facts

2.1 EGY Spontaneous Orthography

An orthography is a specification of how to use
a particular writing system (script) to write the
words of a particular language. In cases where
there is no standard orthography, people use a
spontaneous orthography that is based on dif-
ferent criteria. The main criterion is phonol-
ogy: how to render a word pronunciation in
the given writing system. This mainly de-
pends on language-specific assumptions about the
grapheme-to-phoneme mapping. Another crite-
rion is to use cognates in a related language (sim-
ilar language or a language variant), where two
words represent a cognate if they are related et-
ymologically and have the same meaning. Ad-
ditionally, a spontaneous orthography may be af-
fected by speech effects, which are the lengthen-
ing of specific syllables to show emphasis or other
effects (such as Q�
J
J
�
�J» ktyyyyr 2 ‘veeeery’).

EGY has no standard orthography. Instead,
it has a spontaneous orthography that is related
to the standard orthography of Modern Standard
Arabic. Table 1 shows an example of writing a
sentence in EGY spontaneous orthography in dif-
ferent variants.

2Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al., 2007): (in alphabetical
order) AbtθjHxdðrzsšSDTĎςγfqklmnhwy and the additional
symbols: ’ Z, Â


@, Ǎ @, Ā

�
@, ŵ ð', ŷ Zø', h̄ �è, ý ø.

2.2 Arabizi

Arabizi is a spontaneous orthography used to write
DA using the Latin script, the so-called Arabic
numerals, and other symbols commonly found on
various input devices such as punctuation. Arabizi
is commonly used by Arabic speakers to write in
social media and SMS and chat applications.

The orthography decisions made for writing
in Arabizi mainly depend on a phoneme-to-
grapheme mapping between the Arabic pronunci-
ation and the Latin script. This is largely based
on the phoneme-to-grapheme mapping used in En-
glish. Crucially, Arabizi is not a simple transliter-
ation of Arabic, under which each Arabic letter in
some orthography is replaced by a Latin letter (as
is the case in the Buckwalter transliteration used
widely in natural language processing but nowhere
else). As a result, it is not straightforward to con-
vert Arabizi to Arabic. We discuss some specific
aspects of Arabizi.

Vowels While EGY orthography omits vocalic
diacritics representing short vowels, Arabizi uses
the Latin script symbols for vowels (a, e, i, o, u, y)
to represent EGY’s short and long vowels, making
them ambiguous. In some cases, Arabizi words
omit short vowels altogether as is done in Arabic
orthography.

Consonants Another source of ambiguity is the
use of a single Latin letter to refer to multiple Ara-
bic phonemes. For example, the Latin letter "d" is
used to represent the sounds of the Arabic letters
X d and 	 D. Additionally, some pairs of Arabizi
letters can ambiguously map to a single Arabic let-
ter or pairs of letters: "sh" can be use to represent

� š or é sh. Arabizi also uses digits to repre-
sent some Arabic letters. For example, the dig-
its 2, 3, 5, 6, 7 and 9 are used to represent the
Hamza (glottal stop), and the sounds of the letters
¨ ς , p x,   T, h H and  S, respectively. How-
ever, when followed by "’", the digits 3, 6, 7 and
9 change their interpretations to the dotted version
of the Arabic letter:

	̈
γ, 	  Ď, p x and 	 D, re-

spectively. Moreover, "’" (as well as "q") may also
refer to the glottal stop.

Foreign Words Arabizi contains a large num-
ber of foreign words, that are either borrowings
such as mobile or instances of code switching such
as I love you.

Abbreviations Arabizi may also include some
abbreviations such as isa which means é<Ë @ Z A � 	à@
Ǎn šA’ Allh ‘God willing’.

31



Orthography Example
CODA hPAJ.Ó@ 	áÓ ú
G. Am

� ��� 	® � AÓ
mA šftš SHAby mn AmbArH

Non-CODA hPAJ.Ó@ 	áÓ úG. Agñ
��� 	̄ñ �AÓ

Arabic Script mAšwftš SwHAbý mn AmbArH
hPAJ. 	K @ 	áÓ úG. Am

� ��� 	® �Ó
mšftš SHAbý mn ǍnbArH
hPAJ.Ó@ 	áÓ ú
G. Am

� ��
�J 	® � AÓ
mA šftyš SHAby mn ǍmbArH

Arabizi
mashoftesh sohaby men embare7
ma shftesh swhabi mn imbareh
mshwftish swhaby min ambare7

Table 1: The different spelling variants in EGY and Arabizi for writing the sentence "I have not seen my
friends since yesterday" versus its corresponding CODA form.

2.3 CODA

CODA is a conventionalized orthography for Di-
alectal Arabic (Habash et al., 2012a). In CODA,
every word has a single orthographic representa-
tion. CODA has five key properties (Eskander
et al., 2013). First, CODA is an internally con-
sistent and coherent convention for writing DA.
Second, CODA is primarily created for computa-
tional purposes, but is easy to learn and recognize
by educated Arabic speakers. Third, CODA uses
the Arabic script as used for MSA, with no ex-
tra symbols from, for example, Persian or Urdu.
Fourth, CODA is intended as a unified framework
for writing all dialects. CODA has been defined
for EGY (Habash et al., 2012a) as well as Tunisian
Arabic (Zribi et al., 2014). Finally, CODA aims
to maintain a level of dialectal uniqueness while
using conventions based on similarities between
MSA and the dialects. For a full presentation of
CODA and a justification and explanation of its
choices, see (Habash et al., 2012a).

CODA has been used as part of different ap-
proaches for modeling DA morphology (Habash
et al., 2012b), tagging (Habash et al., 2013; Pasha
et al., 2014) and spelling correction (Eskander et
al., 2013; Farra et al., 2014). Converting Dialec-
tal Arabic (written using a spontaneous Arabic or-
thography or Arabizi) to CODA is beneficial to
NLP applications that better perform on standard-
ized data with less sparsity (Eskander et al., 2013).

Table 1 shows the CODA form corresponding
to spontaneously written Arabic.

3 Related Work

Our proposed work has some similarities to Dar-
wish (2013). His work is divided into two sec-
tions: language identification and transliteration.
He used word and sequence-level features to iden-
tify Arabizi that is mixed with English. For Arabic
words, he modeled transliteration from Arabizi to
Arabic script, and then applied language model-
ing on the transliterated text. This is similar to our
proposed work in terms of transliteration and lan-
guage modeling. However, Darwish (2013) does
not target a conventionalized orthography, while
our system targets CODA. Additionally, Darwish
(2013) transliterates Arabic words only after filter-
ing out non-Arabic words, while we transliterate
the whole input Arabizi. Finally, he does not use
any morphological information, while we intro-
duce the use of a morphological analyzer to sup-
port the transliteration pipeline.

Chalabi and Gerges (2012) presented a hybrid
approach for Arabizi transliteration. Their work
relies on the use of character transformation rules
that are either handcrafted by a linguist or au-
tomatically generated from training data. They
also employ word-based and character-based lan-
guage models for the final transliteration choice.
Like Darwish (2013), the work done by Chalabi
and Gerges (2012) is similar to ours except that
it does not target a conventionalized orthography,
and does not use deep morphological information,
while our system does.

There are three commercial products that con-

32



vert Arabizi to Arabic, namely: Microsoft Maren,3

Google Ta3reeb4 and Yamli.5 However, since
these products are for commercial purposes, there
is not enough information about their approaches.
But given their output, it is clear that they do
not follow a well-defined standardized orthogra-
phy like we do. Furthermore, these tools are pri-
marily intended as input method support, not full
text transliteration. As a result, their users’ goal
is to produce Arabic script text not Arabizi text.
We expect, for instance, that users of these input
method support systems will use less or no code
switching to English, and they may employ char-
acter sequences that help them arrive at the target
Arabic script form, which otherwise they would
not write if they are targeting Arabizi.

Eskander et al. (2013) introduced a system
to convert spontaneous EGY to CODA, called
CODAFY. The difference between CODAFY and
our proposed system is that CODAFY works on
spontaneous text written in Arabic script, while
our system works on Arabizi, which involves a
higher degree of ambiguity. However, we use
CODAFY as a black-box module in our prepro-
cessing.

Additionally, there is some work on convert-
ing from dialectal Arabic to MSA, which is sim-
ilar to our work in terms of processing a dialec-
tal input. However, our final output is in EGY
and not MSA. Shaalan et al. (2007) introduced a
rule-based approach to convert EGY to MSA. Al-
Gaphari and Al-Yadoumi (2010) also used a rule-
based method to transform from Sanaani dialect to
MSA. Sawaf (2010), Salloum and Habash (2011)
and Salloum and Habash (2013) used morpholog-
ical analysis and morphosyntactic transformation
rules for processing EGY and Levantine Arabic.

There has been some work on machine translit-
eration by Knight and Graehl (1997). Al-Onaizan
and Knight (2002) introduced an approach for ma-
chine transliteration of Arabic names. Freeman
et al. (2006) also introduced a system for name
matching between English and Arabic, which
Habash (2008) employed as part of generating
English transliterations from Arabic words in the
context of machine translation. This work is sim-
ilar to ours in terms of text transliteration. How-
ever, our work is not restricted to names.

3http://www.getmaren.com
4http://www.google.com/ta3reeb
5http://www.yamli.com/

4 Approach

4.1 Defining the Task
Our task is as follows: for each Arabizi word in
the input, we choose the Arabic script word which
is the correct CODA spelling of the input word
and which carries the intended meaning (as deter-
mined in the context of the entire available text).

We do not merge two or more input words into
a single Arabic script word. If CODA requires
two consecutive input Arabizi words to be merged,
we indicate this by attaching a plus to the end of
the first word. On the other hand, if CODA re-
quires an input Arabizi word to be broken into two
or more Arabic script words, we indicate this by
inserting a dash between the words. We do this
to maintain the bijection between input and out-
put words, i.e., to allow easy tracing of the Arabic
script back to the Arabizi input.

4.2 Transliteration Pipeline
The proposed system in this paper is called 3AR-
RIB.6 Using the context of an input Arabizi word,
3ARRIB produces the word’s best Arabic script
CODA transliteration. Figure 1 illustrates the dif-
ferent components of 3ARRIB in both the train-
ing and processing phases. We summarize the full
transliteration process as follows. Each Arabizi
sentence input to 3ARRIB goes through a pre-
processing step of lowercasing (de-capitalization),
speech effects handling, and punctuation split-
ting. 3ARRIB then generates a list of all possi-
ble transliterations for each word in the input sen-
tence using a finite-state transducer that is trained
on character-level alignment from Arabizi to Ara-
bic script. We then experiment with different com-
binations of the following two components:

Morphological Analyzer We use CALIMA
(Habash et al., 2012b), a morphological analyzer
for EGY. For each input word, CALIMA provides
all possible morphological analyses, including the
CODA spelling for each analysis. All generated
candidates are passed through CALIMA. If CAL-
IMA has no analysis for a candidate, then that
candidate gets filtered out; otherwise, the CODA
spellings of the analyses from CALIMA become
the new candidates in the rest of the transliteration
pipeline. For some words, CALIMA may sug-
gest multiple CODA spellings that reflect different
analyses of the word.

63ARRIB (pronounced /ar-rib/) means “Arabize!”.

33



FSM 

Candidates
FSM

CALIMA

(+tokenization)

Best 

Selections

LMFST model

SRILM

Arabizi – Arabic 

Parallel Data

Giza++

Training phase

 Input Arabizi 

Script

FST

Egyptian Corpus

CALIMA 

Output
A* Search

Preprocessing

MADAMIRA

 Output Arabic 

Script

Figure 1: An illustration of the different components of the 3ARRIB system in both the training and
processing phases. FST: finite-state Transducer; LM: Language Model; CALIMA: Morphological Ana-
lyzer for Dialectal Arabic; MADAMIRA: Morphological Tagger for Arabic.

Language Model We disambiguate among the
possibilities for all input words (which consti-
tute a “sausage” lattice) using an n-gram language
model.

4.3 Preprocessing

We apply the following preprocessing steps to the
input Arabizi text:

• We separate all attached emoticons such as
(:D, :p, etc.) and punctuation from the words.
We only keep the apostrophe because it is
used in Arabizi to distinguish between dif-
ferent sounds. 3ARRIB keeps track of any
word offset change, so that it can reconstruct
the same number of tokens at the end of the
pipeline.

• We tag emoticons and punctuation to protect
them from any change through the pipeline.

• We lowercase all letters.
• We handle speech effects by replacing any

sequence of the same letter whose length is
greater than two by a sequence of exactly
length two; for example, iiiii becomes ii.

4.4 Character-Based Transduction

We use a parallel corpus of Arabizi-Arabic words
to learn a character-based transduction model.
The parallel data consists of two sources. First,

we use 2,200 Arabizi-to-Arabic script pairs from
the training data used by (Darwish, 2013). We
manually revised the Arabic side to be CODA-
compliant. Second, we use about 6,300 pairs
of proper names in Arabic and English from
the Buckwalter Arabic Morphological Analyzer
(Buckwalter, 2004). Since proper names are typ-
ically transliterated, we expect them to be a rich
source for learning transliteration mappings.

The words in the parallel data are turned into
space-separated character tokens, which we align
using Giza++ (Och and Ney, 2003). We then use
the phrase extraction utility in the Moses statistical
machine translation system (Koehn et al., 2007) to
extract a phrase table which operates over char-
acters. The phrase table is then used to build a
finite-state transducer (FST) that maps sequences
of Arabizi characters into sequences of Arabic
script characters. We use the negative logarithmic
conditional probabilities of the Arabizi-to-Arabic
pairs in the phrase tables as costs inside the FST.
We use the FST to transduce an input Arabizi word
to one or more words in Arabic script, where ev-
ery resulting word in Arabic script is given a prob-
abilistic score.

As part of the preprocessing of the parallel data,
we associate all Arabizi letters with their word
location information (beginning, middle and end-
ing letters). This is necessary since some Arabizi

34



mapping phenomena happen only at specific loca-
tions. For example, the Arabizi letter "o" is likely
to be transliterated into


@ Â in Arabic if it appears

at the beginning of the word, but almost never so
if it appears in the middle of the word.

For some special Arabizi cases, we directly
transliterate input words to their correct Arabic
form using a table, without going through the FST.
For example, isa is mapped to é�<Ë @ Z A � 	à@ Ǎn šA’
Allh ‘God willing’. There are currently 32 entries
in this table.

4.5 Morphological Analyzer

For every word in the Arabizi input, all the candi-
dates generated by the character-based transduc-
tion are passed through the CALIMA morpholog-
ical analyzer. For every candidate, CALIMA pro-
duces a list of all the possible morphological anal-
yses. The CODA for these analyses need not be
the same. For example, if the output from the char-
acter based transducer is Aly, then CALIMA pro-
duces the following CODA-compliant spellings:
úÍ@ Ǎlý ‘to’, ú
Í@ Ǎlý ‘to me’ and ú
Í

�
@ Āly ‘automatic’

or ‘my family’. All of these CODA spellings are
the output of CALIMA for that particular input
word. The output from CALIMA then becomes
the set of final candidates of the input Arabizi in
the rest of the transliteration pipeline. If a word
is not recognized by CALIMA, it gets filtered out
from the transliteration pipeline. However, if all
the candidates of some word are not recognized
by CALIMA, then we retain them all since there
should be an output for every input word.

We additionally run a tokenization step that
makes use of the generated CALIMA morphologi-
cal analysis. The tokenization scheme we target is
D3, which separates all clitics associated with the
word (Habash, 2010). For every word, we keep
a list of the possible tokenized and untokenized
CODA-compliant pairs. We use the tokenized or
untokenized forms as inputs to either a tokenized
or untokenized language model, respectively, as
described in the next subsection. The untokenized
form is necessary to retain the surface form at the
end of the transliteration process.

Standalone clitics are sometimes found in Ara-
bizi such as lel ragel (which corresponds to
Ég. @P +ÉË ll+ rAjl ‘for the man’). Since CALIMA
does not handle most standalone clitics, we keep
a lookup table that associates them with their tok-
enization information.

4.6 Language Model

We then use an EGY language model that is
trained on CODA-compliant text. We investi-
gate two options: a language model that has stan-
dard CODA white-space word tokenization con-
ventions (“untokenized”), and a language model
that has a D3 tokenized form of CODA in which
all clitics are separated (“tokenized”). The output
of the morphological analyzer (which is the input
to the LM component) is processed to match the
tokenization used in the LM.

The language models are built from a large
corpus of 392M EGY words.7 The corpus is
first processed using CODAFY (Eskander et al.,
2013), a system for spontaneous text convention-
alization into CODA. This is necessary so that
our system remains CODA-compliant across the
whole transliteration pipeline. Eskander et al.
(2013) states that the best conventionalization re-
sults are obtained by running the MLE component
of CODAFY followed by an EGY morphological
tagger, MADA-ARZ (Habash et al., 2013). In the
work reported here, we use the newer version of
MADA-ARZ, named MADAMIRA (Pasha et al.,
2014). For the tokenized language model, we run
a D3 tokenization step on top of the processed text
by MADAMIRA. The processed data is used to
build a language model with Kneser-Ney smooth-
ing using the SRILM toolkit (Stolcke, 2002).

We use A* search to pick the best transliteration
for each word given its context. The probability of
any path in the A* search space combines the FST
probability of the words with the probability from
the language model. Thus, for any certain path of
selected Arabic possibilities A0,i = {a0, a1, ...ai}
given the corresponding input Arabizi sequence
W0,i = {w0, w1, ...wi}, the transliteration prob-
ability can be defined by equation (1).

P (A0,i|W0,i) =
i∏

j=0

(P (aj |wj) ∗ P (aj |aj−N+1,j−1)) (1)

Where, N is the maximum affordable n-
gram length in the LM, P (aj |wj) is the
FST probability of transliterating the Ara-
bizi word wj into the Arabic word aj , and
P (aj |aj−N+1,j−1) is the LM probability of the se-
quence {aj−N+1, aj−N+2, ...aj}.

7All of the resources we use are available from the Lin-
guistic Data Consortium: www.ldc.upenn.edu.

35



5 Experiments and Results

5.1 Data

We use two in-house data sets for development
(Dev; 502 words) and blind testing (Test; 1004
words). The data contains EGY Arabizi SMS
conversations that are mapped to Arabic script in
CODA by a CODA-trained EGY native speaker.

5.2 Experiments

We conducted a suite of experiments to evaluate
the performance of our approach and identify op-
timal settings on the Dev set. The optimal result
and the baseline are then applied to the blind Test
set. During development, the following settings
were explored:

• INV-Selection: The training data of the finite
state transducer is used to generate the list of
possibilities for each input Arabizi word. If
the input word cannot be found in the FST
training data, the word is kept in Arabizi.

• FST-ONLY: Pick the top choice from the list
generated by the finite state transducer.

• FST-CALIMA: Pick the top choice from the
list after the CALIMA filtering.

• FST-CALIMA-Tokenized-LM-5: Run the
full pipeline of 3ARRIB with a 5-gram to-
kenized LM.8

• FST-CALIMA-Tokenized-LM-5-MLE:
The same as FST-CALIMA-Tokenized-
LM-5, but for an Arabizi word that appears
in training, force its most frequently seen
mapping directly instead of running the
transliteration pipeline for that word.

• FST-CALIMA-Untokenized-LM-5: Run
the full pipeline of 3ARRIB with a 5-gram
untokenized LM.

• FST-Untokenized-LM-5: Run the full
pipeline of 3ARRIB minus the CALIMA fil-
tering with a 5-gram untokenized LM. This
setup is analogous to the transliteration ap-
proach proposed by (Darwish, 2013). Thus
we use it as our baseline.

Each of the above experiments is evaluated
with exact match, and with Alif/Ya normalization
(El Kholy and Habash, 2010; Habash, 2010).

83, 5, and 7-gram LMs have been tested. The 3 and 5-
gram LMs give the same performance while the 7-gram LM
is the worst.

5.3 Results

Table 2 summarizes the results on the Dev set.
Our best performing setup is FST-CALIMA-
Tokenized-LM-5 which has 77.5% accuracy and
79.1% accuracy with normalization. The baseline
system, FST-Untokenized-LM-5, gives 74.1% ac-
curacy and 74.9 % accuracy with normalization.
This highlights the value of morphological filter-
ing as well as sparsity-reducing tokenization.

Table 3 shows how we do (best system and best
baseline) on a blind Test set. Although the accu-
racy drops overall, the gap between the best sys-
tem and the baseline increases.

5.4 Error Analysis

We conducted two error analyses for the best per-
forming transliteration setting on the Dev set. We
first analyze in which component the Dev set er-
rors occur. About 29% of the errors are cases
where the FST does not generate the correct an-
swer. An additional 15% of the errors happen be-
cause the correct answer is not covered by CAL-
IMA. The language model does not include the
correct answer in an additional 8% of the errors.
The rest of the errors (48%) are cases where the
correct answer is available in all components but
does not get selected.

Motivated by the value of Arabizi transliteration
for machine translation into English, we distin-
guish between two types of words: words that re-
main the same when translated into English, such
as English words, proper nouns, laughs, emoti-
cons, punctuations and digits (EN-SET) versus
EGY-only words (EGY-SET). Examples of words
in EN-SET are: love you very much (code switch-
ing), Peter (proper noun), haha (laugh), :D (emoti-
con), ! (punctuation) and 123 (digits).

While the overall performance of our best set-
tings is 77.5%, the accuracy of the EGY-SET by
itself is 84.6% as opposed to 46.2% for EN-SET.
This large difference reflects the fact that we do
not target English word transliteration into Arabic
script explicitly.

We now perform a second error analysis only on
the errors in the EGY-SET, in which we categorize
the errors by their linguistic type. About 25% of
the errors are non-CODA-compliant system out-
put, where the answer is a plausible non-CODA
form, i.e., a form that may be written or read eas-
ily by a native speaker who is not aware of CODA.
For example, the system generates the non-CODA

36



System Exact-Matching A/Y-normalization
INV-Selection 37.1 40.6
FST-ONLY (pick top choice) 63.1 65.1
FST-CALIMA (pick top choice) 66.1 68.9
FST-CALIMA-Tokenized-LM-5 77.5 79.1
FST-CALIMA-Tokenized-LM-5-MLE 68.7 73.5
FST-CALIMA-Untokenized-LM-5 77.3 78.9
FST-Untokenized-LM-5 74.1 74.9

Table 2: Results on the Dev set in terms of accuracy (%).

System Exact-Matching A/Y-normalization
FST-CALIMA-Tokenized-LM-5 69.4 73.9
FST-Untokenized-LM-5 63.1 65.4

Table 3: Results on the blind Test set in terms of accuracy (%).

form �ª 	® 	JJ
Ó mynfςš instead of the correct CODA
form �ª 	® 	JK
 AÓ mA ynfςš ‘it doesn’t work’. Ignor-
ing the CODA-related errors increases the overall
accuracy by about 3.0% to become 80.5%. The ac-
curacy of the EGY-SET rises to 88.3% as opposed
to 84.6% when considering CODA compliance.

Ambiguous Arabizi input contributes to an ad-
ditional 27% of the errors, where the system as-
signs a plausible answer that is incorrect in con-
text. For example, the word matar in the input
Arabizi fel matar ‘at the airport’ has two plausi-
ble out-of-context solutions: PA¢Ó mTAr ‘airport’
(contextually correct) and Q¢Ó mTr ‘rain’ (contex-
tually incorrect).

In about 2% of the errors, the Arabizi input con-
tains a typo making it impossible to produce the
gold reference. For example, the input Arabizi
ba7bet contains a typo where the final t should turn
into k, so that it means ½J.kAK. bAHbk ‘I love you
[2fs]’.

In the rest of the errors (about 46%), the sys-
tem fails to come up with the correct answer. In-
stead, it assigns a completely different word or
even an impossible word. For example, the cor-
rect answer for the input Arabizi sora ‘picture’ is�èPñ Swrh̄, while the system produces the word
Pñ swr ‘wall’. Another example is the input Ara-
bizi talabt ‘I asked for’, where the output from the
system is �éJ. Ë A£ TAlbh̄ ‘student’, while the correct
answer is �IJ. Ê£ tlbt ‘I asked for, ordered’ instead.

6 Conclusion and Future Work

We presented a method for converting dialectal
Arabic (specifically, EGY) written in Arabizi to
Arabic script following the CODA convention for
DA orthography. We achieve a 17% error reduc-
tion over our implementation of a previously pub-
lished work (Darwish, 2013) on a blind test set.

In the future, we plan to improve several aspects
of our models, particularly FST character map-
ping, the morphological analyzer coverage, and
language models. We also plan to work on the
problem of automatic identification of non-Arabic
words. We will extend the system to work on other
Arabic dialects. We also plan to make the 3AR-
RIB system publicly available.

Acknowledgement

This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-12-C-
0014. Any opinions, findings and conclusions or
recommendations expressed in this paper are those
of the authors and do not necessarily reflect the
views of DARPA.

References
G. Al-Gaphari and M. Al-Yadoumi. 2010. A method

to convert Sana’ani accent to Modern Standard Ara-
bic. International Journal of Information Science
and Management, pages 39–49.

Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proceed-
ings of the ACL-02 Workshop on Computational Ap-
proaches to Semitic Languages.

37



Tim Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0. LDC catalog number
LDC2004L02, ISBN 1-58563-324-0.

Achraf Chalabi and Hany Gerges. 2012. Romanized
Arabic Transliteration. In Proceedings of the Sec-
ond Workshop on Advances in Text Input Methods
(WTIM 2012).

Kareem Darwish. 2013. Arabizi Detection and Con-
version to Arabic. CoRR.

Ahmed El Kholy and Nizar Habash. 2010. Techniques
for Arabic Morphological Detokenization and Or-
thographic Denormalization. In Proceedings of the
seventh International Conference on Language Re-
sources and Evaluation (LREC), Valletta, Malta.

Ramy Eskander, Nizar Habash, Owen Rambow, and
Nadi Tomeh. 2013. Processing Spontaneous Or-
thography. In Proceedings of the 2013 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), Atlanta, GA.

Noura Farra, Nadi Tomeh, Alla Rozovskaya, and
Nizar Habash. 2014. Generalized Character-Level
Spelling Error Correction. In Proceedings of the
Conference of the Association for Computational
Linguistics (ACL), Baltimore, Maryland, USA.

Andrew Freeman, Sherri Condon, and Christopher
Ackerman. 2006. Cross linguistic name matching
in English and Arabic. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 471–478, New York City,
USA.

Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.

Nizar Habash, Mona Diab, and Owen Rabmow. 2012a.
Conventional Orthography for Dialectal Arabic. In
Proceedings of the Language Resources and Evalu-
ation Conference (LREC), Istanbul.

Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012b. A Morphological Analyzer for Egyptian
Arabic. In Proceedings of the Twelfth Meeting of the
Special Interest Group on Computational Morphol-
ogy and Phonology, pages 1–9, Montréal, Canada.

Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
Analysis and Disambiguation for Dialectal Arabic.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), Atlanta, GA.

Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In Pro-
ceedings of ACL-08: HLT, Short Papers, pages 57–
60, Columbus, Ohio.

Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.

Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Proceedings of the European
chapter of the Association for Computational Lin-
guistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Association for Computational
Linguistics, Prague, Czech Republic.

Franz Joseph Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.

Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan M. Roth.
2014. MADAMIRA: A Fast, Comprehensive Tool
for Morphological Analysis and Disambiguation of
Arabic. In Proceedings of the Language Resources
and Evaluation Conference (LREC), Reykjavik, Ice-
land.

Wael Salloum and Nizar Habash. 2011. Dialectal to
Standard Arabic Paraphrasing to Improve Arabic-
English Statistical Machine Translation. In Pro-
ceedings of the First Workshop on Algorithms and
Resources for Modelling of Dialects and Language
Varieties, pages 10–21, Edinburgh, Scotland.

Wael Salloum and Nizar Habash. 2013. Dialectal
Arabic to English Machine Translation: Pivoting
through Modern Standard Arabic. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT), At-
lanta, GA.

Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the Confer-
ence of the Association for Machine Translation in
the Americas (AMTA), Denver, Colorado.

Khaled Shaalan, Hitham Abo Bakr, and Ibrahim
Ziedan. 2007. Transferring Egyptian Colloquial
into Modern Standard Arabic. In International Con-
ference on Recent Advances in Natural Language
Processing (RANLP), Borovets, Bulgaria.

Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901–904, Denver,
CO.

Ines Zribi, Rahma Boujelbane, Abir Masmoudi,
Mariem Ellouze, Lamia Belguith, and Nizar Habash.
2014. A Conventional Orthography for Tunisian
Arabic. In Proceedings of the Language Resources
and Evaluation Conference (LREC), Reykjavik, Ice-
land.

38


