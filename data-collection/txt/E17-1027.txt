



















































A Systematic Study of Neural Discourse Models for Implicit Discourse Relation


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 281–291,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

A Systematic Study of Neural Discourse Models for Implicit Discourse
Relation

Attapol T. Rutherford
Yelp

San Francisco, CA, USA
teruth@yelp.com

Vera Demberg
Saarland University

Saarbrücken, Germany
vera@coli.uni-saarland.de

Nianwen Xue
Brandeis University
Waltham, MA, USA
xuen@brandeis.edu

Abstract

Inferring implicit discourse relations in
natural language text is the most difficult
subtask in discourse parsing. Many neu-
ral network models have been proposed to
tackle this problem. However, the com-
parison for this task is not unified, so
we could hardly draw clear conclusions
about the effectiveness of various archi-
tectures. Here, we propose neural net-
work models that are based on feedfor-
ward and long-short term memory archi-
tecture and systematically study the effects
of varying structures. To our surprise, the
best-configured feedforward architecture
outperforms LSTM-based model in most
cases despite thorough tuning. Further,
we compare our best feedforward system
with competitive convolutional and recur-
rent networks and find that feedforward
can actually be more effective. For the first
time for this task, we compile and pub-
lish outputs from previous neural and non-
neural systems to establish the standard for
further comparison.

1 Introduction

The discourse structure of a natural language text
has been analyzed and conceptualized under vari-
ous frameworks (Mann and Thompson, 1988; Las-
carides and Asher, 2007; Prasad et al., 2008). The
Penn Discourse TreeBank (PDTB) and the Chi-
nese Discourse Treebank (CDTB), currently the
largest corpora annotated with discourse structures
in English and Chinese respectively, view the dis-
course structure of a text as a set of discourse re-
lations (Prasad et al., 2008; Zhou and Xue, 2012).
Each discourse relation (e.g. causal or temporal) is
grounded by a discourse connective (e.g. because
or meanwhile) taking two text segments as argu-

ments (Prasad et al., 2008). Implicit discourse re-
lations are those where discourse connectives are
omitted from the text and yet the discourse rela-
tions still hold.

While classifying explicit discourse relations is
relatively easy, as the discourse connective itself
provides a strong cue for the discourse relation
(Pitler et al., 2008), the classification of implicit
discourse relations has proved to be notoriously
hard and remained one of the last missing pieces in
an end-to-end discourse parser (Xue et al., 2015).
In the absence of explicit discourse connectives,
implicit discourse relations have to be inferred
from their two arguments. Previous approaches
on inferring implicit discourse relations have typ-
ically relied on features extracted from their two
arguments. These features include the Cartesian
products of the word tokens in the two arguments
as well as features manually crafted from vari-
ous lexicons such as verb classes and sentiment
lexicons (Pitler et al., 2009; Rutherford and Xue,
2014). These lexicons are used mainly to offset
the data sparsity problem created by pairs of word
tokens used directly as features.

Neural network models are an attractive alter-
native for this task, but it is not clear how well
they will fare with a small dataset, typically found
in discourse annotation projects. Many neural
approaches have been proposed. However, we
lack a unified standard comparison to really learn
whether we make any progress at all because not
all past studies agree on the same experimental
settings such as label sets to use. Previous work
used four binary classification (Pitler et al., 2008;
Rutherford and Xue, 2014) , 4-way coarse sense
classification (Rutherford and Xue, 2015) , and in-
termediate sense classification (Lin et al., 2009).
CoNLL Shared Task introduces a unified scheme
for evaluation along with a new unseen test set in
English in 2015 (Xue et al., 2015) and in Chinese
in 2016 (Xue et al., 2016). We want to corrobo-

281



rate this new evaluation scheme by running more
benchmark results and providing the output under
this evaluation scheme. We systematically com-
pare the relative advantages of different neural ar-
chitectures and publish the outputs from the sys-
tems for the research community to conduct fur-
ther analysis.

In this work, we explore multiple neural ar-
chitectures in an attempt to find the best dis-
tributed representation and neural network archi-
tecture suitable for this task in both English and
Chinese. We do this by probing the different
points on the spectrum of structurality from struc-
tureless bag-of-words models to sequential and
tree-structured models. We use feedforward, se-
quential long short-term memory (LSTM), and
tree-structured LSTM models to represent these
three points on the spectrum. To the best of our
knowledge, there is no prior study that investigates
the contribution of the different architectures in
neural discourse analysis.

Our main contributions and findings from this
work can be summarized as follows:

• We establish that the simplest feedforward
discourse model outperforms systems with
surface features and perform comparably
with or even outperforms recurrent and con-
volutional architectures. This holds across
different label sets in English and in Chinese.

• We investigate the contribution of the linguis-
tic structures in neural discourse modeling
and found that high-dimensional word vec-
tors trained on a large corpus can compensate
for the lack of structures in the model, given
the small amount of annotated data.

• We collect and publish the system outputs
from many neural architectures on the stan-
dard experimental settings for the community
to conduct more error analysis. These are
made available on the author’s website.

2 Model Architectures

Following previous work, we assume that
the two arguments of an implicit discourse
relation are given so that we can focus on
predicting the senses of the implicit discourse
relations. The input to our model is a pair of
text segments called Arg1 and Arg2, and the
label is one of the senses defined in the Penn

. . .
w1 w2 wn

Arg1

. . .
w1 w2 wn

Arg2

Word vectors

Arg vectors
(Pooling)

Hidden layer

Output layer

. . .

w1 w2 wn

s1 s2 sn

Arg1

. . .

w1 w2 wn

s1 s2 sn

Arg2

Word vectors

LSTM

Arg vectors
(Pooling)

Hidden layer

Output layer

Figure 1: (Top) Feedforward architecture. (Bot-
tom) Sequential Long Short-Term Memory archi-
tecture.

Discourse Treebank as in the example below:
Input:
Arg1 Senator Pete Domenici calls this effort

“the first gift of democracy”
Arg2 The Poles might do better to view it as a

Trojan Horse.
Output:
Sense Comparison.Contrast

In all architectures, each word in the argument
is represented as a k-dimensional word vector
trained on an unannotated data set. We use
various model architectures to transform the
semantics represented by the word vectors into
distributed continuous-valued features. In the
rest of the section, we explain the details of the
neural network architectures that we design for
the implicit discourse relations classification task.
The models are summarized schematically in
Figure 1.

2.1 Bag-of-words Feedforward Model

This model does not model the structure or word
order of a sentence. The features are simply
obtained through element-wise pooling functions.
Pooling is one of the key techniques in neural net-
work modeling of computer vision (Krizhevsky et
al., 2012; LeCun et al., 2010). Max pooling is
known to be very effective in vision, but it is un-
clear what pooling function works well when it
comes to pooling word vectors. Summation pool-
ing and mean pooling have been claimed to per-
form well at composing meaning of a short phrase
from individual word vectors (Le and Mikolov,

282



2014; Blacoe and Lapata, 2012; Mikolov et al.,
2013b; Braud and Denis, 2015). The Arg1 vector
a1 and Arg2 vector a2 are computed by applying
element-wise pooling function f on all of the N1
word vectors in Arg1 w11:N1 and all of the N2 word
vectors in Arg2 w21:N2 respectively:

a1i = f(w
1
1:N1,i)

a2i = f(w
2
1:N2,i)

We consider three different pooling functions
namely max, summation, and mean pooling func-
tions:

fmax(w1:N , i) =
N

max
j=1

wj,i

fsum(w1:N , i) =
N∑

j=1

wj,i

fmean(w1:N , i) =
N∑

j=1

wj,i/N

Inter-argument interaction is modeled directly
by the hidden layers that take argument vectors
as features. Discourse relations cannot be deter-
mined based on the two arguments individually.
Instead, the sense of the relation can only be deter-
mined when the arguments in a discourse relation
are analyzed jointly. The first hidden layer h1 is
the non-linear transformation of the weighted lin-
ear combination of the argument vectors:

h1 = tanh(W1 · a1 + W2 · a2 + bh1)
where W1 and W2 are d × k weight matrices and
bh1 is a d-dimensional bias vector. Further hidden
layers ht and the output layer o follow the standard
feedforward neural network model.

ht = tanh(Wht · ht−1 + bht)
o = softmax(Wo · hT + bo)

where Wht is a d × d weight matrix, bht is a d-
dimensional bias vector, and T is the number of
hidden layers in the network.

2.2 Sequential Long Short-Term Memory
(LSTM)

A sequential Long Short-Term Memory Recurrent
Neural Network (LSTM-RNN) models the seman-
tics of a sequence of words through the use of hid-
den state vectors. Therefore, the word ordering
does affect the resulting hidden state vectors, un-
like the bag-of-word model. For each word vector

at word position t, we compute the corresponding
hidden state vector st and the memory cell vec-
tor from the previous step, using standard formula
for LSTM. The argument vectors are the results of
applying a pooling function over the hidden state
vectors.

a1i = f(s
1
1:N1,i)

a2i = f(s
2
1:N2,i)

In addition to the three pooling functions that we
describe in the previous subsection, we also con-
sider using only the last hidden state vector, which
should theoretically be able to encode the seman-
tics of the entire word sequence.

flast(s1:N,i) = sN,i

Inter-argument interaction and the output layer are
modeled in the same fashion as the bag-of-words
model once the argument vector is computed.

2.3 Tree LSTM
The principle of compositionality leads us to be-
lieve that the semantics of the argument vector
should be determined by the syntactic structures
and the meanings of the constituents. For a fair
comparison with the sequential model, we apply
the same formulation of LSTM on the binarized
constituent parse tree. The hidden state vector now
corresponds to a constituent in the tree. These hid-
den state vectors are then used in the same fashion
as the sequential LSTM. The mathematical formu-
lation is the same as Tai et al. (2015).

This model is similar to the recursive neural
networks proposed by Ji and Eisenstein (2015).
Our model differs from their model in several
ways. We use the LSTM networks instead of the
“vanilla” RNN formula and expect better results
due to less complication with vanishing and ex-
ploding gradients during training. Furthermore,
our purpose is to compare the influence of the
model structures. Therefore, we must use LSTM
cells in both sequential and tree LSTM models for
a fair and meaningful comparison. The more in-
depth comparison of our work and recursive neu-
ral network model by Ji and Eisenstein (2015) is
provided in the discussion section.

3 Corpora and Implementation

The Penn Discourse Treebank (PDTB) We use
the PDTB due to its theoretical simplicity in dis-
course analysis and its reasonably large size. The

283



Sense Train Dev Test
Comparison.Concession 192 5 5
Comparison.Contrast 1612 82 127
Contingency.Cause 3376 120 197
Contingency.Pragmatic cause 56 2 5
Expansion.Alternative 153 2 15
Expansion.Conjunction 2890 115 116
Expansion.Instantiation 1132 47 69
Expansion.List 337 5 25
Expansion.Restatement 2486 101 190
Temporal.Asynchronous 543 28 12
Temporal.Synchrony 153 8 5
Total 12930 515 766

Table 1: The distribution of the level 2 sense labels
in the Penn Discourse Treebank. The instances
annotated with two labels are not double-counted,
and partial labels are excluded.

annotation is done as another layer on the Penn
Treebank on Wall Street Journal sections. Each
relation consists of two spans of text that are
minimally required to infer the relation, and the
sense is organized hierarchically. The classifica-
tion problem can be formulated in various ways
based on the hierarchy. Previous work in this
task has been done over three schemes of evalu-
ation: top-level 4-way classification (Pitler et al.,
2009), second-level 11-way classification (Lin et
al., 2009; Ji and Eisenstein, 2015), and modi-
fied second-level classification introduced in the
CoNLL 2015 Shared Task (Xue et al., 2015). We
focus on the second-level 11-way classification
because the labels are fine-grained enough to be
useful for downstream tasks and also because the
strongest neural network systems are tuned to this
formulation. If an instance is annotated with two
labels (∼3% of the data), we only use the first la-
bel. Partial labels, which constitute ∼2% of the
data, are excluded. Table 3 shows the distribution
of labels in the training set (sections 2-21), devel-
opment set (section 22), and test set (section 23).

Training Weight initialization is uniform random,
following the formula recommended by Bengio
(2012). The cost function is the standard cross-
entropy loss function, as the hinge loss function
(large-margin framework) yields consistently in-
ferior results. We use Adagrad as the optimiza-
tion algorithm of choice. The learning rates are
tuned over a grid search. We monitor the accuracy
on the development set to determine convergence
and prevent overfitting. L2 regularization and/or
dropout do not make a big impact on performance
in our case, so we do not use them in the final re-

sults.
Implementation All of the models are imple-
mented in Theano (Bergstra et al., 2010; Bastien
et al., 2012). The gradient computation is done
with symbolic differentiation, a functionality pro-
vided by Theano. Feedforward models and se-
quential LSTM models are trained on CPUs on
Intel Xeon X5690 3.47GHz, using only a single
core per model. A tree LSTM model is trained on
a GPU on Intel Xeon CPU E5-2660. All models
converge within hours.

4 Experiment on the Second-level Sense
in the PDTB

We want to test the effectiveness of the inter-
argument interaction and the three models de-
scribed above on the fine-grained discourse rela-
tions in English. The data split and the label set
are exactly the same as previous works that use
this label set (Lin et al., 2009; Ji and Eisenstein,
2015).
Preprocessing All tokenization is taken from the
gold standard tokenization in the PTB (Marcus et
al., 1993). We use the Berkeley parser to parse all
of the data (Petrov et al., 2006). We test the effects
of word vector sizes. 50-dimensional and 100-
dimensional word vectors are trained on the train-
ing sections of WSJ data, which is the same text
as the PDTB annotation. Although this seems like
too little data, 50-dimensional WSJ-trained word
vectors have previously been shown to be the most
effective in this task (Ji and Eisenstein, 2015).
Additionally, we also test the off-the-shelf word
vectors trained on billions of tokens from Google
News data freely available with the word2vec
tool. All word vectors are trained on the Skip-
gram architecture (Mikolov et al., 2013b; Mikolov
et al., 2013a). Other models such as GloVe and
continuous bag-of-words seem to yield broadly
similar results (Pennington et al., 2014). We keep
the word vectors fixed, instead of fine-tuning dur-
ing training.

4.1 Results

The feedforward model performs best overall
among all of the neural architectures we explore
(Table 2). It outperforms the recursive neural net-
work with bilinear output layer introduced by Ji
and Eisenstein (2015) (p < 0.05; bootstrap test)
and performs comparably with the surface fea-
ture baseline (Lin et al., 2009), which uses var-

284



No hidden layer 1 hidden layer 2 hidden layers
Architecture k max mean sum last max mean sum last max mean sum last
Feedforward 50 31.85 31.98 29.24 - 33.28 34.98 37.85 - 34.85 35.5 38.51 -
LSTM 50 31.85 32.11 34.46 31.85 34.07 33.15 36.16 34.34 36.16 35.11 37.2 35.24
Tree LSTM 50 28.59 28.32 30.93 28.72 29.89 30.15 32.5 31.59 32.11 31.2 32.5 29.63
Feedforward 100 33.29 32.77 28.72 - 36.55 35.64 37.21 - 36.55 36.29 37.47 -
LSTM 100 30.54 33.81 35.9 33.02 36.81 34.98 37.33 35.11 37.46 36.68 37.2 35.77
Tree LSTM 100 29.76 28.72 31.72 31.98 31.33 26.89 33.02 33.68 32.63 31.07 32.24 33.02
Feedforward 300 32.51 34.46 35.12 - 35.77 38.25 39.56 - 35.25 38.51 39.03 -
LSTM 300 28.72 34.59 35.24 34.64 38.25 36.42 37.07 35.5 38.38 37.72 37.2 36.29
Tree LSTM 300 28.45 31.59 32.76 26.76 33.81 32.89 33.94 32.63 32.11 32.76 34.07 32.50

Table 3: Compilation of all experimental configurations for 11-way classification on the PDTB test set.
k is the word vector size. Bold-faced numbers indicate the best performance for each architecture, which
is also shown in Table 2.

Model Accuracy

PDTB Second-level senses
Most frequent tag baseline 25.71
Our best tree LSTM 34.07
Ji & Eisenstein, (2015) 36.98
Our best sequential LSTM variant 38.38
Our best feedforward variant 39.56
Lin et al., (2009) 40.20

Table 2: Performance comparison across different
models for second-level senses.

●
●

●

●

●

●

●

●

36.55
35.64

37.21

29.37

36.81
34.98

37.33

35.11

25

30

35

40

Feedforward LSTM
Architecture

A
cc

ur
ac

y

Pooling ● ● ● ●Last Mean Max Summation

Figure 2: Summation pooling gives the best per-
formance in general. The results are shown for the
systems using 100-dimensional word vectors and
one hidden layer.

ious lexical and syntactic features and extensive
feature selection. Tree LSTM achieves inferior
accuracy than our best feedforward model. The
best configuration of the feedforward model uses
300-dimensional word vectors, one hidden layer,
and the summation pooling function to derive ar-
gument feature vectors. The model behaves well
during training and converges in less than an hour
on a CPU.

The sequential LSTM model outperforms the
feedforward model when word vectors are not
high-dimensional and not trained on a large cor-

●

● ●

●

● ●

●

● ●

29.24

37.85 38.51

28.72

37.21 37.47

35.12

39.56 39.03

25

30

35

40

50 100 300
Embedding Size

A
cc

ur
ac

y

Number of Hidden Layers 0 1 2

Figure 3: Inter-argument interaction can be mod-
eled effectively with hidden layers. The results are
shown for the feedforward models with summa-
tion pooling, but this effect can be observed ro-
bustly in all architectures we consider.

pus (Figure 4). Moving from 50 units to 100 units
trained on the same dataset, we do not observe
much of a difference in performance in both ar-
chitectures, but the sequential LSTM model beats
the feedforward model in both settings (Table 3).
This suggests that only 50 dimensions are needed
for the WSJ corpus. However, the trend reverses
when we move to 300-dimensional word vectors
trained on a much larger corpus. These results
suggest an interaction between the lexical infor-
mation encoded by word vectors and the structural
information encoded by the model itself.

Hidden layers, especially the first one, make a
substantial impact on performance. This effect is
observed across all architectures (Figure 3). Strik-
ingly, the improvement can be as high as 8% abso-
lute when used with the feedforward model with
small word vectors. We tried up to four hidden
layers and found that the additional hidden lay-
ers yield diminishing—if not negative—returns.
These effects are not an artifact of the training
process as we have tuned the models quite exten-
sively, although it might be the case that we do not

285



●

●
● ● ● ●

● ●

●
●

● ●

28.72

35.9
37.2137.33 37.47 37.2

35.1235.24

39.56
38.25

39.0338.38

25

30

35

40

100 x 0 100 x 1 100 x 2 300 x 0 300 x 1 300 x 2
Embedding Size x Number of Hidden Layers

A
cc

ur
ac

y
Architectures Feedforward LSTM

Figure 4: Comparison between feedforward and
sequential LSTM when using summation pooling
function.

have sufficient data to fit those extra parameters.
Summation pooling is effective for both feed-

forward and LSTM models (Figure 2). The word
vectors we use have been claimed to have some ad-
ditive properties (Mikolov et al., 2013b), so sum-
mation pooling in this experiment supports this
claim. Max pooling is only effective for LSTM,
probably because the values in the word vector en-
code the abstract features of each word relative to
each other. It can be trivially shown that if all of
the vectors are multiplied by -1, then the results
from max pooling will be totally different, but the
word similarities remain the same. The memory
cells and the state vectors in the LSTM models
transform the original word vectors to work well
the max pooling operation, but the feedforward net
cannot transform the word vectors to work well
with max pooling as it is not allowed to change
the word vectors themselves.

4.2 Why does the feedforward model
outperform the LSTM models?

Summing up vectors indeed works better than re-
current models. We provide further evidence for
this claim in Section 5. Sequential and tree LSTM
models might work better if we are given larger
amount of data. We observe that LSTM mod-
els outperform the feedforward model when word
vectors are smaller, so it is unlikely that we train
the LSTMs incorrectly. It is more likely that
we do not have enough annotated data to train a
more powerful model such as LSTM. In previous
work, LSTMs are applied to tasks with a lot of la-
beled data compared to mere 12,930 instances that
we have (Vinyals et al., 2015; Chiu and Nichols,
2015; İrsoy and Cardie, 2014). Another explana-
tion comes from the fact that the contextual infor-
mation encoded in the word vectors can compen-

sate for the lack of structure in the model in this
task. Word vectors are already trained to encode
the words in their linguistic context especially in-
formation from word order.

Our discussion would not be complete without
explaining our results in relation to the recursive
neural network model proposed by Ji and Eisen-
stein (2015). Why do sequential LSTM mod-
els outperform recursive neural networks or tree
LSTM models? Although this first comes as a sur-
prise to us, the results are consistent with recent
works that use sequential LSTM to encode syntac-
tic information. For example, Vinyals et al. (2015)
use sequential LSTM to encode the features for
syntactic parse output. Tree LSTM seems to show
improvement when there is a need to model long-
distance dependency in the data (Tai et al., 2015;
Li et al., 2015). Furthermore, the benefits of tree
LSTM are not readily apparent for a model that
discards the syntactic categories in the intermedi-
ate nodes and makes no distinction between heads
and their dependents, which are at the core of syn-
tactic representations.

Another point of contrast between our work and
Ji and Eisenstein’s (2015) is the modeling choice
for inter-argument interaction. Our experimen-
tal results show that the hidden layers are an im-
portant contributor to the performance for all of
our models. We choose linear inter-argument in-
teraction instead of bilinear interaction, and this
decision gives us at least two advantages. Lin-
ear interaction allows us to stack up hidden lay-
ers without the exponential growth in the number
of parameters. Secondly, using linear interaction
allows us to use high dimensional word vectors,
which we found to be another important compo-
nent for the performance. The recursive model by
Ji and Eisenstein (2015) is limited to 50 units due
to the bilinear layer. Our choice of linear inter-
argument interaction and high-dimensional word
vectors turns out to be crucial to building a com-
petitive neural network model for classifying im-
plicit discourse relations.

5 Extending the results across neural
architectures, label sets, and languages

We want to provide further evidence that feed-
forward models perform well without surface fea-
tures or without sophisticated recurrent or convo-
lutional structures across different label sets and
languages as well. Toward that goal, we evaluate

286



our models on non-explicit discourse relation data
used in English and Chinese CoNLL 2016 Shared
Task.

5.1 English discourse relations
We follow the experimental setting used in
CoNLL 2015-2016 Shared Task. To compare our
results against previous systems, we compile all of
the official system outputs, and make them pub-
licly available. The label set is modified by the
shared task organizers into 15 different senses in-
cluding EntRel as another sense (Xue et al., 2015;
Xue et al., 2016). We use the 300-dimensional
word vector used in the previous experiment and
tune the number of hidden layers and hidden units
on the development set. We consider the fol-
lowing models: Bidirectional-LSTM (Akanksha
and Eisenstein, 2016), two flavors of convolutional
networks (Qin et al., 2016; Wang and Lan, 2016),
two variations of simple argument pooling (Mi-
haylov and Frank, 2016; Schenk et al., 2016), and
the best system using surface features alone (Wang
and Lan, 2015). The comparison results and brief
system descriptions are shown in Table 4.

Our model presents the state-of-the-art system
on the blind test set in English. We once again
confirm that manual features are not necessary for
this task and that our feedforward network outper-
forms the best available LSTM and convolutional
networks in many settings despite its simplicity.
While performing well in-domain, convolutional
networks degrade sharply when tested on the blind
slightly out-of-domain dataset.

5.2 Chinese discourse relations
We evaluate our model on the Chinese Discourse
Treebank (CDTB) because its annotation is the
most comparable to the PDTB (Zhou and Xue,
2015). The sense set consists of 10 different
senses, which are not organized in a hierarchy,
unlike the PDTB. We use the version of the data
provided to the CoNLL 2016 Shared Task partic-
ipants. This version has 16,946 instances of dis-
course relations total in the combined training and
development sets. The test set is not yet available
at the time of submission, so the system is eval-
uated based on the average accuracy over 7-fold
cross-validation on the combined set of training
and development sets.

To establish baseline comparison, we use Max-
Ent models loaded with the feature sets previ-
ously shown to be effective for English, namely

Model Acc.

CoNLL-ST 2015-2016 English (WSJ Test set)
Most frequent tag baseline 21.36
Our best LSTM variant 31.76
Wang and Lan (2015) - winning team 34.45
Our best feedforward variant 36.13

CoNLL-ST 2016 Chinese (CTB Test set)
Most frequent tag baseline 77.14
ME + Production rules 80.81
ME + Dependency rules 82.34
ME + Brown pairs (1000 clusters) 82.36
Out best LSTM variant 82.48
ME + Brown pairs (3200 clusters) 82.98
ME + Word pairs 83.13
ME + All feature sets 84.16
Our best feedforward variant 85.45

Table 5: Our best feedforward variant signifi-
cantly outperforms the systems with surface fea-
tures (p < 0.05). ME=Maximum Entropy model

CBOW Skipgram

0.78

0.80

0.82

0.84

100 200 300 100 200 300

Dimensionality of word vectors

A
cc

ur
ac

y
Number of Hidden Layers 0 1 2

Figure 5: Comparing the accuracies across Chi-
nese word vectors for feedforward model.

dependency rule pairs, production rule pairs (Lin
et al., 2009), Brown cluster pairs (Rutherford and
Xue, 2014), and word pairs (Marcu and Echihabi,
2002). We use information gain criteria to select
the best subset of each feature set, which is crucial
in feature-based discourse parsing.

Chinese word vectors are induced through
CBOW and Skipgram architecture in word2vec
(Mikolov et al., 2013a) on Chinese Gigaword cor-
pus (Graff and Chen, 2005) using default settings.
The number of dimensions that we try are 50, 100,
150, 200, 250, and 300. We induce 1,000 and
3,000 Brown clusters on the Gigaword corpus.

Table 5 shows the results for the models which
are best tuned on the number of hidden units, hid-
den layers, and the types of word vectors. The
feedforward variant of our model significantly out-
performs the strong baselines in both English and
Chinese (p < 0.05 bootstrap test). This suggests
that our approach is robust against different label

287



Systems Arg vector Features? Blind set WSJ Test WSJ Dev

Ours Summing vectors No 0.3767 0.3613 0.4032
Akanksha & Eisenstein (2016) 2-layer Bi-LSTM Yes 0.3675 0.3495 0.4072
Qin et al. (2016) Convolutional net No 0.3538 0.3820 0.4632
Mihaylov & Frank (2016) Averaging vectors Yes 0.3451 0.3919 0.4032
Schenk et al. (2016) Avg + Product No 0.3185 0.3761 0.4542
Wang & Lan (2016) Convolutional net No 0.3418 0.4091 0.4642
Wang & Lan (2015) N/A Yes 0.3629 0.3445 0.4272

Table 4: Comparing various systems on the CoNLL 2016 Shared Task standard datasets. Manual fea-
tures are no longer needed for a competitive system. While performing well in-domain, convolutional
networks degrade sharply when tested on the blind slightly out-of-domain dataset.

sets, and our findings are valid across languages.
Our Chinese model outperforms all of the feature
sets known to work well in English despite using
only word vectors. The choice of neural architec-
ture used for inducing Chinese word vectors turns
out to be crucial. Chinese word vectors from Skip-
gram model perform consistently better than the
ones from CBOW model (Figure 5). These two
types of word vectors do not show much differ-
ence in the English tasks.

6 Related Work

The prevailing approach for this task is to use sur-
face features derived from various semantic lexi-
cons (Pitler et al., 2009), reducing the number of
parameters by mapping raw word tokens in the ar-
guments of discourse relations to a limited num-
ber of entries in a semantic lexicon such as polar-
ity and verb classes. Along the same vein, Brown
cluster assignments have also been used as a gen-
eral purpose lexicon that requires no human man-
ual annotation (Rutherford and Xue, 2014). How-
ever, these solutions still suffer from the data spar-
sity problem and almost always require extensive
feature selection to work well (Park and Cardie,
2012; Lin et al., 2009; Ji and Eisenstein, 2015).
The work we report here explores the use of the
expressive power of distributed representations to
overcome the data sparsity problem found in the
traditional feature engineering paradigm.

Neural network modeling has been explored
to some extent in the context of this task. Re-
cently, Braud and Denis (2015) tested various
word vectors as features for implicit discourse re-
lation classification and show that distributed fea-
tures achieve the same level of accuracy as one-
hot representations in some experimental settings.
Ji et al. (2015; 2016) advance the state of the
art for this task by using recursive and recurrent
neural networks. In the work we report here, we

systematically explore the use of different neural
network architectures and show that when high-
dimensional word vectors are used as input, a
simple feed-forward architecture can outperform
more sophisticated architectures such as sequen-
tial and tree-based LSTM networks, given the
small amount of data.

Recurrent neural networks, especially LSTM
networks, have changed the paradigm of deriving
distributed features from a sentence (Hochreiter
and Schmidhuber, 1997), but they have not been
much explored in the realm of discourse parsing.
LSTM models have been notably used to encode
the meaning of source language sentence in neu-
ral machine translation (Cho et al., 2014; Devlin et
al., 2014) and recently used to encode the meaning
of an entire sentence to be used as features (Kiros
et al., 2015). Many neural architectures have been
explored and evaluated, but there is no single tech-
nique that is decidedly better across all tasks. The
LSTM-based models such as Kiros et al. (2015)
perform well across tasks but do not outperform
some other strong neural baselines. Ji et al. (2016)
uses a joint discourse language model to improve
the performance on the coarse-grained label in the
PDTB, but in our case, we would like to deduce
how well LSTM fares in fine-grained implicit dis-
course relation classification, which is more prac-
tical for application.

7 Conclusions and future work

We report a series of experiments that system-
atically probe the effectiveness of various neural
network architectures for the task of implicit dis-
course relation classification. We found that a
feedforward variant of our model combined with
hidden layers and high dimensional word vectors
outperforms more complicated LSTM and con-
volutional models. We also establish that manu-
ally crafted surface features are not necessary for

288



this task. These results hold for different settings
and different languages. In addition, we collect
and compile the system outputs from all competi-
tive systems and make it available for the research
community to conduct further analysis. We en-
courage that researchers who work on this task to
evaluate their systems under the CoNLL Shared
Task 2015-2016 scheme to allow for easy compar-
ison and progress tracking.

Acknowledgments

The first author was funded by the German Re-
search Foundation (DFG) as part of SFB 1102: In-
formation Density and Linguistic Encoding

References
Akanksha and Jacob Eisenstein. 2016. Shallow

discourse parsing using distributed argument rep-
resentations and bayesian optimization. CoRR,
abs/1606.04503.

Frédéric Bastien, Pascal Lamblin, Razvan Pascanu,
James Bergstra, Ian J. Goodfellow, Arnaud Berg-
eron, Nicolas Bouchard, and Yoshua Bengio. 2012.
Theano: new features and speed improvements.
Deep Learning and Unsupervised Feature Learning
NIPS 2012 Workshop.

Yoshua Bengio. 2012. Practical recommendations
for gradient-based training of deep architectures. In
Neural Networks: Tricks of the Trade, pages 437–
478. Springer.

James Bergstra, Olivier Breuleux, Frédéric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.

William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for semantic
composition. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546–556. Association for Compu-
tational Linguistics.

Chloé Braud and Pascal Denis. 2015. Comparing word
representations for implicit discourse relation classi-
fication. In Empirical Methods in Natural Language
Processing (EMNLP 2015).

Jason P.C. Chiu and Eric Nichols. 2015. Named en-
tity recognition with bidirectional lstm-cnns. arXiv
preprint arXiv:1511.08308.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger

Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar, October. Association for Com-
putational Linguistics.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics, volume 1, pages 1370–1380.

David Graff and Ke Chen. 2005. Chinese gigaword.
LDC Catalog No.: LDC2003T09, ISBN, 1:58563–
58230.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Ozan İrsoy and Claire Cardie. 2014. Opinion mining
with deep recurrent neural networks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 720–728.

Yangfeng Ji and Jacob Eisenstein. 2015. One vector is
not enough: Entity-augmented distributed semantics
for discourse relations. Transactions of the Associa-
tion for Computational Linguistics, 3:329–344.

Yangfeng Ji, Gholamreza Haffari, and Jacob Eisen-
stein. 2016. A latent variable recurrent neural net-
work for discourse relation language models. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.

Ryan Kiros, Yukun Zhu, Ruslan R. Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors.
In Advances in Neural Information Processing Sys-
tems, pages 3276–3284.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.

Alex Lascarides and Nicholas Asher. 2007. Seg-
mented discourse representation theory: Dynamic
semantics with discourse structure. In Computing
meaning, pages 87–124. Springer.

Quoc V. Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. arXiv
preprint arXiv:1405.4053.

Yann LeCun, Koray Kavukcuoglu, and Clément Fara-
bet. 2010. Convolutional networks and applications
in vision. In Circuits and Systems (ISCAS), Pro-
ceedings of 2010 IEEE International Symposium on,
pages 253–256. IEEE.

289



Jiwei Li, Thang Luong, Dan Jurafsky, and Eduard
Hovy. 2015. When are tree structures necessary
for deep learning of representations? In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pages 2304–2314,
Lisbon, Portugal, September. Association for Com-
putational Linguistics.

Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1, pages 343–351.
Association for Computational Linguistics.

William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.

Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 368–375. Association for Computational Lin-
guistics.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313–330.

Todor Mihaylov and Anette Frank. 2016. Discourse
relation sense classification using cross-argument
semantic similarity based on word embeddings. In
Proceedings of the Twentieth Conference on Compu-
tational Natural Language Learning - Shared Task,
page 100.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 108–112. Association for Com-
putational Linguistics.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empirical
Methods in Natural Language Processing (EMNLP
2014), 12:1532–1543.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational

Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433–
440. Association for Computational Linguistics.

Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind K Joshi. 2008.
Easily identifiable discourse relations. Technical
Reports (CIS), page 884.

Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 683–691. Association for Computational
Linguistics.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K. Joshi, and Bon-
nie L. Webber. 2008. The penn discourse treebank
2.0. In LREC.

Lianhui Qin, Zhisong Zhang, and Hai Zhao. 2016.
Shallow discourse parsing using convolutional neu-
ral network. In Proceedings of the Twentieth Con-
ference on Computational Natural Language Learn-
ing - Shared Task, page 70.

Attapol T. Rutherford and Nianwen Xue. 2014. Dis-
covering implicit discourse relations through brown
cluster pair representation and coreference patterns.
In Proceedings of the 14th Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics (EACL 2014), Gothenburg, Swe-
den, April.

Attapol Rutherford and Nianwen Xue. 2015. Improv-
ing the inference of implicit discourse relations via
classifying explicit discourse connectives. In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
799–808, Denver, Colorado, May–June. Association
for Computational Linguistics.

Niko Schenk, Christian Chiarcos, Kathrin Donandt,
Samuel Rönnqvist, Evgeny A. Stepanov, and
Giuseppe Riccardi. 2016. Do we really need all
those rich linguistic features? a neural network-
based approach to implicit sense labeling. In Pro-
ceedings of the Twentieth Conference on Computa-
tional Natural Language Learning - Shared Task,
page 41.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1556–1566, Beijing, China, July. Association
for Computational Linguistics.

290



Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In C. Cortes, N.D.
Lawrence, D.D. Lee, M. Sugiyama, R. Garnett, and
R. Garnett, editors, Advances in Neural Information
Processing Systems 28, pages 2755–2763. Curran
Associates, Inc.

Jianxiang Wang and Man Lan. 2015. A refined end-
to-end discourse parser. In Proceedings of the Nine-
teenth Conference on Computational Natural Lan-
guage Learning - Shared Task, pages 17–24, Bei-
jing, China, July. Association for Computational
Linguistics.

Jianxiang Wang and Man Lan. 2016. Two end-to-
end shallow discourse parsers for english and chi-
nese in conll-2016 shared task. In Proceedings of
the Twentieth Conference on Computational Natural
Language Learning - Shared Task, page 33.

Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Rashmi
Prasad, Christopher Bryant, and Attapol Ruther-
ford. 2015. The conll-2015 shared task on shal-
low discourse parsing. In Proceedings of the Nine-
teenth Conference on Computational Natural Lan-
guage Learning - Shared Task, pages 1–16, Beijing,
China, July. Association for Computational Linguis-
tics.

Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Bon-
nie Webber, Attapol Rutherford, Chuan Wang, and
Hongmin Wang. 2016. The conll-2016 shared task
on multilingual shallow discourse parsing. In Pro-
ceedings of the Twentieth Conference on Computa-
tional Natural Language Learning - Shared Task,
Berlin, Germany, August. Association for Compu-
tational Linguistics.

Yuping Zhou and Nianwen Xue. 2012. Pdtb-style dis-
course annotation of chinese text. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers-Volume 1,
pages 69–77. Association for Computational Lin-
guistics.

Yuping Zhou and Nianwen Xue. 2015. The chinese
discourse treebank: A chinese corpus annotated with
discourse relations. Language Resources and Eval-
uation, 49(2):397–431.

291


