



















































Multi-level Translation Quality Prediction with QuEst++


Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 115–120,
Beijing, China, July 26-31, 2015. c©2015 ACL and AFNLP

Multi-level Translation Quality Prediction with QUEST++

Lucia Specia, Gustavo Henrique Paetzold and Carolina Scarton
Department of Computer Science

University of Sheffield, UK
{l.specia,ghpaetzold1,c.scarton}@sheffield.ac.uk

Abstract

This paper presents QUEST++ , an open
source tool for quality estimation which
can predict quality for texts at word, sen-
tence and document level. It also provides
pipelined processing, whereby predictions
made at a lower level (e.g. for words) can
be used as input to build models for pre-
dictions at a higher level (e.g. sentences).
QUEST++ allows the extraction of a va-
riety of features, and provides machine
learning algorithms to build and test qual-
ity estimation models. Results on recent
datasets show that QUEST++ achieves
state-of-the-art performance.

1 Introduction

Quality Estimation (QE) of Machine Translation
(MT) have become increasingly popular over the
last decade. With the goal of providing a predic-
tion on the quality of a machine translated text, QE
systems have the potential to make MT more use-
ful in a number of scenarios, for example, improv-
ing post-editing efficiency (Specia, 2011), select-
ing high quality segments (Soricut and Echihabi,
2010), selecting the best translation (Shah and
Specia, 2014), and highlighting words or phrases
that need revision (Bach et al., 2011).

Most recent work focuses on sentence-level QE.
This variant is addressed as a supervised machine
learning task using a variety of algorithms to in-
duce models from examples of sentence transla-
tions annotated with quality labels (e.g. 1-5 likert
scores). Sentence-level QE has been covered in
shared tasks organised by the Workshop on Statis-
tical Machine Translation (WMT) annually since
2012. While standard algorithms can be used to
build prediction models, key to this task is work
of feature engineering. Two open source feature

extraction toolkits are available for that: ASIYA1

and QUEST2 (Specia et al., 2013). The latter has
been used as the official baseline for the WMT
shared tasks and extended by a number of partic-
ipants, leading to improved results over the years
(Callison-Burch et al., 2012; Bojar et al., 2013;
Bojar et al., 2014).

QE at other textual levels have received much
less attention. Word-level QE (Blatz et al., 2004;
Luong et al., 2014) is seemingly a more challeng-
ing task where a quality label is to be produced
for each target word. An additional challenge is
the acquisition of sizable training sets. Although
significant efforts have been made, there is con-
siderable room for improvement. In fact, most
WMT13-14 QE shared task submissions were un-
able to beat a trivial baseline.

Document-level QE consists in predicting a sin-
gle label for entire documents, be it an absolute
score (Scarton and Specia, 2014) or a relative
ranking of translations by one or more MT sys-
tems (Soricut and Echihabi, 2010). While certain
sentences are perfect in isolation, their combina-
tion in context may lead to an incoherent docu-
ment. Conversely, while a sentence can be poor in
isolation, when put in context, it may benefit from
information in surrounding sentences, leading to
a good quality document. Feature engineering is
a challenge given the little availability of tools to
extract discourse-wide information. In addition,
no datasets with human-created labels are avail-
able and thus scores produced by automatic met-
rics have to be used as approximation (Scarton et
al., 2015).

Some applications require fine-grained, word-
level information on quality. For example, one
may want to highlight words that need fixing.
Document-level QE is needed particularly for gist-
ing purposes where post-editing is not an option.

1http://nlp.lsi.upc.edu/asiya/
2http://www.quest.dcs.shef.ac.uk/

115



For example, for predictions on translations of
product reviews in order to decide whether or not
they are understandable by readers. We believe
that the limited progress in word and document-
level QE research is partially due to lack of a basic
framework that one can be build upon and extend.

QUEST++ is a significantly refactored and
expanded version of an existing open source
sentence-level toolkit, QUEST. Feature extrac-
tion modules for both word and document-level
QE were added and the three levels of prediction
were unified into a single pipeline, allowing for in-
teractions between word, sentence and document-
level QE. For example, word-level predictions can
be used as features for sentence-level QE. Finally,
sequence-labelling learning algorithms for word-
level QE were added. QUEST++ can be easily ex-
tended with new features at any textual level. The
architecture of the system is described in Section
2. Its main component, the feature extractor, is
presented in Section 3. Section 4 presents experi-
ments using the framework with various datasets.

2 Architecture

QUEST++ has two main modules: a feature ex-
traction module and a machine learning module.
The first module is implemented in Java and pro-
vides a number of feature extractors, as well as
abstract classes for features, resources and pre-
processing steps so that extractors for new fea-
tures can be easily added. The basic functioning
of the feature extraction module requires raw text
files with the source and translation texts, and a
few resources (where available) such as the MT
source training corpus and source and target lan-
guage models (LMs). Configuration files are used
to indicate paths for resources and the features that
should be extracted. For its main resources (e.g.
LMs), if a resource is missing, QUEST++ can gen-
erate it automatically.

Figure 1 depicts the architecture of QUEST++ .
Document and Paragraph classes are used for
document-level feature extraction. A Document is
a group of Paragraphs, which in turn is a group
of Sentences. Sentence is used for both word- and
sentence-level feature extraction. A Feature Pro-
cessing Module was created for each level. Each
processing level is independent and can deal with
the peculiarities of its type of feature.

Machine learning QUEST++ provides scripts
to interface the Python toolkit scikit-learn3

(Pedregosa et al., ). This module is indepen-
dent from the feature extraction code and uses
the extracted feature sets to build and test QE
models. The module can be configured to run
different regression and classification algorithms,
feature selection methods and grid search for
hyper-parameter optimisation. Algorithms from
scikit-learn can be easily integrated by
modifying existing scripts.

For word-level prediction, QUEST++ provides
an interface for CRFSuite (Okazaki, 2007), a se-
quence labelling C++ library for Conditional Ran-
dom Fields (CRF). One can configure CRFSuite
training settings, produce models and test them.

3 Features

Features in QUEST++ can be extracted from either
source or target (or both) sides of the corpus at a
given textual level. In order describe the features
supported, we denote:
• S and T the source and target documents,
• s and t for source and target sentences, and
• s and t for source and target words.
We concentrate on MT system-independent

(black-box) features, which are extracted based on
the output of the MT system rather than any of
its internal representations. These allow for more
flexible experiments and comparisons across MT
systems. System-dependent features can be ex-
tracted as long they are represented using a pre-
defined XML scheme. Most of the existing fea-
tures are either language-independent or depend
on linguistic resources such as POS taggers. The
latter can be extracted for any language, as long
as the resource is available. For a pipelined ap-
proach, predictions at a given level can become
features for higher level model, e.g. features based
on word-level predictions for sentence-level QE.

3.1 Word level

We explore a range of features from recent work
(Bicici and Way, 2014; Camargo de Souza et al.,
2014; Luong et al., 2014; Wisniewski et al., 2014),
totalling 40 features of seven types:

Target context These are features that explore
the context of the target word. Given a word ti
in position i of a target sentence, we extract: ti,

3http://scikit-learn.org/

116



Figure 1: Architecture of QUEST++

i.e., the word itself, bigrams ti−1ti and titi+1, and
trigrams ti−2ti−1ti, ti−1titi+1 and titi+1ti+2.

Alignment context These features explore the
word alignment between source and target sen-
tences. They require the 1-to-N alignment be-
tween source and target sentences to be provided.
Given a word ti in position i of a target sentence
and a word sj aligned to it in position j of a source
sentence, the features are: the aligned word sj it-
self, target-source bigrams sj−1ti and tisj+1, and
source-target bigrams ti−2sj , ti−1sj , sjti+1 and
sjti+2.

Lexical These features explore POS informa-
tion on the source and target words. Given
the POS tag Pti of word ti in position i of a
target sentence and the POS tag Psj of word
sj aligned to it in position j of a source sen-
tence, we extract: the POS tags Pti and Psj
themselves, the bigrams Pti−1Pti and PtiPti+1
and trigrams Pti−2Pti−1Pti, Pti−1PtiPti+1 and
PtiPti+1Pti+2. Four binary features are also ex-
tracted with value 1 if ti is a stop word, punctua-
tion symbol, proper noun or numeral.

LM These features are related to the n-gram fre-
quencies of a word’s context with respect to an LM
(Raybaud et al., 2011). Six features are extracted:
lexical and syntactic backoff behavior, as well as
lexical and syntactic longest preceding n-gram for
both a target word and an aligned source word.
Given a word ti in position i of a target sentence,

the lexical backoff behavior is calculated as:

f (ti) =



7 if ti−2, ti−1, ti exists
6 if ti−2, ti−1 and ti−1, ti exist
5 if only ti−1, ti exists
4 if ti−2, ti−1 and ti exist
3 if ti−1 and ti exist
2 if ti exists
1 if ti is out of the vocabulary

The syntactic backoff behavior is calculated in
an analogous fashion: it verifies for the existence
of n-grams of POS tags in a POS-tagged LM. The
POS tags of target sentence are produced by the
Stanford Parser4 (integrated in QUEST++ ).

Syntactic QUEST++ provides one syntactic fea-
ture that proved very promising in previous work:
the Null Link (Xiong et al., 2010). It is a binary
feature that receives value 1 if a given word ti in
a target sentence has at least one dependency link
with another word tj , and 0 otherwise. The Stan-
ford Parser is used for dependency parsing.

Semantic These features explore the polysemy
of target and source words, i.e. the number of
senses existing as entries in a WordNet for a given
target word ti or a source word si. We employ
the Universal WordNet,5 which provides access to
WordNets of various languages.

4http://nlp.stanford.edu/software/
lex-parser.shtml

5http://www.lexvo.org/uwn/

117



Pseudo-reference This binary feature explores
the similarity between the target sentence and a
translation for the source sentence produced by an-
other MT system. The feature is 1 if the given
word ti in position i of a target sentence S is also
present in a pseudo-reference translation R. In our
experiments, the pseudo-reference is produced by
Moses systems trained over parallel corpora.

3.2 Sentence level
Sentence-level QE features have been extensively
explored and described in previous work. The
number of QUEST++ features varies from 80 to
123 depending on the language pair. The complete
list is given as part of QUEST++ ’s documentation.
Some examples are:
• number of tokens in s & t and their ratio,
• LM probability of s & t,
• ratio of punctuation symbols in s & t,
• ratio of percentage of numbers, content-/non-

content words, nouns/verbs/etc in s & t,
• proportion of dependency relations between

(aligned) constituents in s & t,
• difference in depth of syntactic trees of s & t.
In our experiments, we use the set of 80 fea-

tures, as these can be extracted for all language
pairs of our datasets.

3.3 Document level
Our document-level features follow from those in
the work of (Wong and Kit, 2012) on MT evalua-
tion and (Scarton and Specia, 2014) for document-
level QE. Nine features are extracted, in addition
to aggregated values of sentence-level features for
the entire document:
• content words/lemmas/nouns repetition in

S/T ,
• ratio of content words/lemmas/nouns in S/T ,

4 Experiments

In what follows, we evaluate QUEST++’s perfor-
mance for the three prediction levels and various
datasets.

4.1 Word-level QE
Datasets We use five word-level QE datasets:
the WMT14 English-Spanish, Spanish-English,
English-German and German-English datasets,
and the WMT15 English-Spanish dataset.

Metrics For the WMT14 data, we evaluate per-
formance in the three official classification tasks:

• Binary: A Good/Bad label, where Bad indi-
cates the need for editing the token.
• Level 1: A Good/Accuracy/Fluency label,

specifying the coarser level categories of er-
rors for each token, or Good for tokens with
no error.
• Multi-Class: One of 16 labels specifying the

error type for the token (mistranslation, miss-
ing word, etc.).

The evaluation metric is the average F-1 of all
but the Good class. For the WMT15 dataset, we
consider only the Binary classification task, since
the dataset does not provide other annotations.

Settings For all datasets, the models were
trained with the CRF module in QUEST++ . While
for the WMT14 German-English dataset we use
the Passive Aggressive learning algorithm, for the
remaining datasets, we use the Adaptive Reg-
ularization of Weight Vector (AROW) learning.
Through experimentation, we found that this setup
to be the most effective. The hyper-parameters for
each model were optimised through 10-fold cross
validation. The baseline is the majority class in
the training data, i.e. a system that always pre-
dicts “Unintelligible” for Multi-Class, “Fluency”
for Level 1, and “Bad” for the Binary setup.

Results The F-1 scores for the WMT14 datasets
are given in Tables 1–4, for QUEST++ and sys-
tems that oficially participated in the task. The re-
sults show that QUEST++ was able to outperform
all participating systems in WMT14 except for the
English-Spanish baseline in the Binary and Level
1 tasks. The results in Table 5 also highlight the
importance of selecting an adequate learning al-
gorithm in CRF models.

System Binary Level 1 Multiclass
QUEST++ 0.502 0.392 0.227
Baseline 0.525 0.404 0.222
LIG/BL 0.441 0.317 0.204
LIG/FS 0.444 0.317 0.204
FBK-1 0.487 0.372 0.170
FBK-2 0.426 0.385 0.230
LIMSI 0.473 − −
RTM-1 0.350 0.299 0.268
RTM-2 0.328 0.266 0.032

Table 1: F-1 for the WMT14 English-Spanish task

4.2 Pipeline for sentence-level QE

Here we evaluate the pipeline of using word-level
predictions as features for sentence-level QE.

118



System Binary Level 1 Multiclass
QUEST++ 0.386 0.267 0.161
Baseline 0.299 0.151 0.019
RTM-1 0.269 0.219 0.087
RTM-2 0.291 0.239 0.081

Table 2: F-1 for the WMT14 Spanish-English task

System Binary Level 1 Multiclass
QUEST++ 0.507 0.287 0.161
Baseline 0.445 0.117 0.086
RTM-1 0.452 0.211 0.150
RTM-2 0.369 0.219 0.124

Table 3: F-1 for the WMT14 English-German task

Dataset We use the WMT15 dataset for word-
level QE. The split between training and test sets
was modified to allow for more sentences for train-
ing the sentence-level QE model. The 2000 last
sentences of the original training set were used
as test along with the original 1000 dev set sen-
tences. Therefore, word predictions were gener-
ated for 3000 sentences, which were later split in
2000 sentences for training and 1000 sentences for
testing the sentence-level model.

Features The 17 QUEST++ baseline features
are used alone (Baseline) and in combination with
four word-level prediction features:
• count & proportion of Good words,
• count & proportion of Bad words.
Oracle word level labels, as given in the original

dataset, are also used in a separate experiment to
study the potential of this pipelined approach.

Settings For learning sentence-level models, the
SVR algorithm with RBF kernel and hyperparam-
eters optimised via grid search in QUEST++ is
used. Evaluation is done using MAE (Mean Ab-
solute Error) as metric.

Results As shown in Table 6, the use of word-
level predictions as features led to no improve-
ment. However, the use of the oracle word-level
labels as features substantially improved the re-
sults, lowering the baseline error by half. We note
that the method used in this experiments is the
same as that in Section 4.1, but with fewer in-
stances for training the word-level models. Im-

System Binary Level 1 Multiclass
QUEST++ 0.401 0.230 0.079
Baseline 0.365 0.149 0.069
RTM-1 0.261 0.082 0.023
RTM-2 0.229 0.085 0.030

Table 4: F-1 for the WMT14 German-English task

Algorithm Binary
AROW 0.379
PA 0.352
LBFGS 0.001
L2SGD 0.000
AP 0.000

Table 5: F-1 for the WMT15 English-Spanish task

proving word-level prediction could thus lead to
better results in the pipeline for sentence-level QE.

MAE
Baseline 0.159
Baseline+Predicted 0.158
Baseline+Oracle 0.07

Table 6: MAE values for sentence-level QE

4.3 Pipeline for document-level QE

Here we evaluate the pipeline of using sentence-
level predictions as features for QE of documents.

Dataset For training the sentence-level model,
we use the English-Spanish WMT13 training set
for sentence-level QE. For the document-level
model, we use English-Spanish WMT13 data
from the translation shared task. We mixed the
outputs of all MT systems, leading to 934 trans-
lated documents. 560 randomly selected docu-
ments were used for training and 374 for test-
ing. As quality labels, for sentence-level training
we consider both the HTER and the Likert labels
available. For document-level prediction, BLEU,
TER and METEOR are used as quality labels (not
as features), given the lack of human-target quality
labels for document-level prediction.

Features The 17 QUEST++ baseline features
are aggregated to produce document-level fea-
tures (Baseline). These are then combined with
document-level features (Section 3.3) and finally
with features from sentence-level predictions:
• maximum/minimum predicted HTER or Lik-

ert score,
• average predicted HTER or Likert score,
• Median, first quartile and third quartile pre-

dicted HTER or Likert score.
Oracle sentence labels are not possible as they

do not exist for the test set documents.

Settings For training and evaluation, we use the
same settings as for sentence-level.

Results Table 7 shows the results in terms of
MAE. The best result was achieved with the

119



baseline plus HTER features, but no significant
improvements over the baseline were observed.
Document-level prediction is a very challenging
task: automatic metric scores used as labels do
not seem to reliably distinguish translations of dif-
ferent source documents, since they were primar-
ily designed to compare alternative translations for
the same source document.

BLEU TER METEOR
Baseline 0.049 0.050 0.055
Baseline+Doc-level 0.053 0.057 0.055
Baseline+HTER 0.053 0.048 0.054
Baseline+Likert 0.054 0.056 0.054
Baseline+Doc-level+HTER 0.053 0.054 0.054
Baseline+Doc-level+Likert 0.053 0.056 0.054

Table 7: MAE values for document-level QE

5 Remarks

The source code for the framework, the datasets
and extra resources can be downloaded from
https://github.com/ghpaetzold/
questplusplus.

The license for the Java code, Python and
shell scripts is BSD, a permissive license with
no restrictions on the use or extensions of the
software for any purposes, including commer-
cial. For pre-existing code and resources, e.g.,
scikit-learn, their licenses apply.

Acknowledgments

This work was supported by the European Associ-
ation for Machine Translation, the QT21 project
(H2020 No. 645452) and the EXPERT project
(EU Marie Curie ITN No. 317471).

References
N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Good-

ness: a method for measuring MT confidence. In
ACL11.

E. Bicici and A. Way. 2014. Referential Transla-
tion Machines for Predicting Translation Quality. In
WMT14.

J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2004. Confidence Estimation for Machine Transla-
tion. In COLING04.

O. Bojar, C. Buck, C. Callison-Burch, C. Federmann,
B. Haddow, P. Koehn, C. Monz, M. Post, R. Soricut,
and L. Specia. 2013. Findings of the 2013 Work-
shop on SMT. In WMT13.

O. Bojar, C. Buck, C. Federmann, B. Haddow,
P. Koehn, J. Leveling, C. Monz, P. Pecina, M. Post,
H. Saint-Amand, R. Soricut, L. Specia, and A. Tam-
chyna. 2014. Findings of the 2014 Workshop on
SMT. In WMT14.

C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on SMT. In WMT12.

J. G. Camargo de Souza, J. González-Rubio, C. Buck,
M. Turchi, and M. Negri. 2014. FBK-UPV-
UEdin participation in the WMT14 Quality Estima-
tion shared-task. In WMT14.

N. Q. Luong, L. Besacier, and B. Lecouteux. 2014.
LIG System for Word Level QE task. In WMT14.

N. Okazaki. 2007. CRFsuite: a fast implementation
of Conditional Random Fields. http://www.
chokkan.org/software/crfsuite/.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research, 12.

S. Raybaud, D. Langlois, and K. Smali. 2011. This
sentence is wrong. Detecting errors in machine-
translated sentences. Machine Translation, 25(1).

C. Scarton and L. Specia. 2014. Document-level trans-
lation quality estimation: exploring discourse and
pseudo-references. In EAMT14.

C. Scarton, M. Zampieri, M. Vela, J. van Genabith, and
L. Specia. 2015. Searching for Context: a Study
on Document-Level Labels for Translation Quality
Estimation. In EAMT15.

K. Shah and L. Specia. 2014. Quality estimation for
translation selection. In EAMT14.

R. Soricut and A. Echihabi. 2010. Trustrank: Induc-
ing trust in automatic translations via ranking. In
ACL10.

L. Specia, K. Shah, J. G. C. de Souza, and T. Cohn.
2013. Quest - a translation quality estimation frame-
work. In ACL13.

L. Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In
EAMT11.

G. Wisniewski, N. Pcheux, A. Allauzen, and F. Yvon.
2014. LIMSI Submission for WMT’14 QE Task. In
WMT14.

B. T. M. Wong and C. Kit. 2012. Extending machine
translation evaluation metrics with lexical cohesion
to document level. In EMNLP/CONLL.

D. Xiong, M. Zhang, and H. Li. 2010. Error detection
for SMT using linguistic features. In ACL10.

120


