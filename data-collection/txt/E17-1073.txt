



















































Online Learning of Task-specific Word Representations with a Joint Biconvex Passive-Aggressive Algorithm


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 775–784,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Online Learning of Task-specific Word Representations with a Joint
Biconvex Passive-Aggressive Algorithm

Pascal Denis1 and Liva Ralaivola2

1MAGNET, Inria Lille – Nord Europe , Villeneuve d’Ascq, France
pascal.denis@inria.fr

2 QARMA, LIF, Aix-Marseille University, Marseille, France
liva.ralaivola@lif.univ-mrs.fr

Abstract

This paper presents a new, efficient
method for learning task-specific word
vectors using a variant of the Passive-
Aggressive algorithm. Specifically, this
algorithm learns a word embedding ma-
trix in tandem with the classifier param-
eters in an online fashion, solving a bi-
convex constrained optimization at each it-
eration. We provide a theoretical analysis
of this new algorithm in terms of regret
bounds, and evaluate it on both synthetic
data and NLP classification problems, in-
cluding text classification and sentiment
analysis. In the latter case, we compare
various pre-trained word vectors to initial-
ize our word embedding matrix, and show
that the matrix learned by our algorithm
vastly outperforms the initial matrix, with
performance results comparable or above
the state-of-the-art on these tasks.

1 Introduction

Recently, distributed word representations have
become a crucial component of many natural
language processing systems (Koo et al., 2008;
Turian et al., 2010; Collobert et al., 2011). The
main appeal of these word embeddings is two-
fold: they can be derived directly from raw text
data in an unsupervised or weakly-supervised
manner, and their latent dimensions condense
interesting distributional information about the
words, thus allowing for better generalization
while also mitigating the presence of rare and un-
seen terms. While there are now many different
spectral, probabilistic, and deep neural approaches
for building vectorial word representations, there
is still no clear understanding as to which syn-
tactic and semantic information they really cap-

ture and whether or how these representations re-
ally differ (Chen et al., 2013; Levy and Goldberg,
2014b; Schnabel et al., 2015). Also poorly under-
stood is the relation between the word represen-
tations and the particular learning algorithm (e.g.,
whether linear or non-linear) that uses them as in-
put (Wang and Manning, 2013).

What seems clear, however, is that there is no
single best embedding and that their impact is very
much task-dependent. This in turn raises the ques-
tion of how to learn word representations that are
adapted to a particular task and learning objective.
Three different research routes have been explored
towards learning task-specific word embeddings.
A first approach (Collobert et al., 2011; Maas
et al., 2011) is to learn the embeddings for the
target problem jointly with additional unlabeled
or (weakly-)labeled data in a semi-supervised or
multi-task approach. While very effective, this
joint training typically requires large amounts of
data and often prohibitive processing times in the
case of multi-layer neural networks (not to men-
tion their lack of theoretical learning guarantees
in part due to their strong non-convexity). An-
other approach consists in training word vectors
using some existing algorithm as in like word2vec
(Mikolov et al., 2013) in a way that exploits prior
domain knowledge (e.g., by defining more in-
formative, task-specific contexts) (Bansal et al.,
2014; Levy and Goldberg, 2014a). In this case,
there is still a need for additional weakly- or hand-
labeled data, and there is no guarantee that the
newly learned embeddings will indeed benefit the
performance, as they are trained independently of
the task objective. A third approach is to start
with some existing pre-trained embeddings and
fine-tune them to the task by integrating them in
a joint learning objective either using backpropa-
gation (Lebret et al., 2013) or regularized logis-
tic regression (Labutov and Lipson, 2013). These

775



approaches in effect hit a sweet spot by leverag-
ing pre-trained embeddings and requiring no ad-
ditional data or domain knowledge, while directly
tying them to task learning objective.

Inspired by these latter approaches, we propose
a new, online soft-margin classification algorithm,
called Re-embedding Passive-Agressive (or RPA),
that jointly learns an embedding matrix in tandem
with the model parameters. As its name suggests,
this algorithm generalizes the Passive-Aggressive
(PA) algorithm of Crammer and Singer (2006) by
allowing the data samples to be projected into the
lower dimension space defined by the original em-
bedding matrix. Our approach may be seen as ex-
tending the work of (Grandvalet and Canu, 2003)
which addresses the problem of simultaneously
learning the features and the weight vector of an
SVM classifier. An important departure, beyond
the online nature of RPA, is that it learns a projec-
tion matrix and not just a diagonal one (which is
essentially what this earlier work does). Our ap-
proach and analysis are also related to (Blondel et
al., 2014), which tackles non-negative matrix fac-
torization with the PA philosophy.

The main contributions of this paper are as fol-
lows. First, we derive a new variant of the Passive-
Aggressive algorithm able to jointly learn an em-
bedding matrix along with the weight vector of the
model (section 2). Second, we provide theoretical
insights as to bound the cumulative squared loss of
our learning procedure over any given sequence of
examples—the results we give are actually related
to a learning procedure that slighlty differs from
the algorithm we introduce but that is more eas-
ily and compactly amenable to a theoretical study.
Third, we further study the behavior of this algo-
rithm on synthetic data (section 4) and we finally
show that it performs well on five real-world NLP
classification problems (section 5).

2 Algorithm

We consider the problem of learning a binary lin-
ear classification function fΦ,w, parametrized by
both a weight vector w ∈ Rk and an embedding
matrix Φ ∈ Rk×p (typically, with k � p), which
is defined as:

fΦ,w : X ⊂ Rp → {−1,+1}
x 7→ sign(〈w,Φx〉)

We aim at an online learning scenario, wherein
both w and Φ will be updated in a sequen-
tial fashion. Given a labeled data steam S =

{(xt, yt)}Tt=1, it seems relevant at each step to
solve the following soft-margin constrained opti-
mization problem:

argmin
w∈Rk

Φ∈Rk×p

1

2
‖w −wt‖22 +

λ

2
‖Φ−Φt‖2F + Cξ2 (1a)

s.t. `t(w; Φ;xt) ≤ ξ (1b)

where ‖·‖2, ‖·‖F stand for the l2 and Frobenius
norms, respectively, and C controls the “aggres-
siveness” of the update (as larger C values im-
ply updates that are directly proportional to the in-
curred loss). We define `t(w; Φ; xt) as the hinge
loss, that is:

`t(w; Φ; xt)
.= max(0, 1− yt 〈w,Φxt〉). (2)

The optimization problem in (1) is reminiscent of
the soft-margin Passive-Aggressive algorithm pro-
posed in (Crammer et al., 2006) (specifically, PA-
II), but both the objective and the constraint now
include a term based on the embedding matrix Φ.
The λ regularization parameter in the objective
controls the allowed divergence in the embedding
parameters between iterations.

Interestingly, the new objective remains convex,
but the margin constraint doesn’t as it involves a
multiplicative term between the weight vector and
the embedding matrix, making the overall prob-
lem bi-convex (Gorski et al., 2007). That is, the
problem is convex in w for fixed values of Φ and
convex in Φ for fixed values of w. Incidentally,
the formulation presented by (Labutov and Lip-
son, 2013) is also bi-convex (as it also involves a
similar multiplicative term), although the authors
proceed as if it were jointly convex (i.e., convex
in both w and Φ). In order to solve this prob-
lem, we resort to an alternating update procedure
which updates each set of parameters (i.e., either
the weight vector or the embedding matrix) while
holding the other fixed until some stopping crite-
rion is met (in our case, the value of the objective
doesn’t change). As shown in Algorithm 1, this
procedure allows us to compute closed-form up-
dates similar to those of PA, and to make use of
the same theoretical apparatus for analyzing RPA.

2.1 Unified Formalization

Suppose from now on that C and λ are fixed and
so are wt and Φt: this will allow us to drop the
explicit dependence on these values and to keep

776



Algorithm 1 Re-embedding Passive-Aggressive
Require:
• S = {(xt, yt)}Tt=1, a stream of data
• a base embedding Φ0

Ensure:
• a classification vector w
• a re-embedding matrix Φ

Initialize w0
t← 0
repeat

w0t+1 ← wt, Φ0t+1 ← Φt,
n← 1
repeat

wn+1t+1 ← wnt+1 +
`t(w

n
t+1; Φ

n
t+1)

‖Φnt+1xt‖22 + 12C
ytxt

Φn+1t+1 ← Φnt+1 +
`t(w

n+1
t+1 ; Φ

n
t+1)∥∥wn+1t+1 ∥∥22 ‖xt‖22 + λ2C ytwn+1t+1 x>t .

with

`t(w; Φ) = max(0, 1− yt 〈w,Φxt〉).

until wnt+1 → w∞t+1 and Φnt+1 → Φ∞t+1
Update wt+1 and Φt+1:

wt+1 ← w∞t+1 Φt+1 ← Φ∞t+1

until some stopping criterion is met
return wt,Φt

the notation light. Let Qt be defined as:

Qt(w,Φ, ξ)
.
=

1

2
‖w −wt‖22 +

λ

2
‖Φ−Φt‖2F + Cξ2.

(3)

and margin qt be defined as:

qt(w,Φ)
.
= 1− 〈w, ytΦxt〉 (4a)
= 1−

〈
Φ, ytwx

>
t

〉
F
. (4b)

Here 〈·, ·〉F denote the Frobenius inner product.
We have purposely provided the two equivalent
forms (4a) and (4b) to emphasize the (syntactic)
exchangeability of w and Φ. As we shall see, this
is going to be essential to derive an alternating up-
date procedure—which alternates the updates with
respect to w and Φ—in a compact way.

Given Qt and qt, we are now interested in solv-
ing the following optimization problem:

wt+1,Φt+1, ξt+1 = argmin
w,Φ,ξ

Qt(w,Φ, ξ) (5a)

s.t. qt(w,Φ) ≤ ξ. (5b)

2.2 Bi-convexity

It turns out that problem (5) is a bi-convex op-
timization problem: it is indeed straightforward
to observe that if Φ is fixed then the problem
is convex in (w, ξ)—it is the classical passive-
aggressive II optimization problem—and if w is
fixed then the problem is convex in (Φ, ξ). If
there exist theoretical results on the solving of bi-
convex optimization problems, the machinery to
use pertains to combinatorial optimization, which
might be too expensive. In addition, computing
the solution of (5) would drive us away from the
spirit of passive-aggressive learning which rely
on cheap and statistically meaningful (from the
mistake-bound perspective) updates. This is the
reason why we propose to resort to an alternate
online procedure to solve a proxy of (5).

2.3 An Alternating Online Procedure

Instead of tackling problem (5) directly we pro-
pose to solve, at each time t, either of:

wt+1, ξt+1 = argmin
w,ξ

Qt(w,Φt, ξ) s.t. qt(w,Φt) ≤ ξ,
(6)

Φt+1, ξt+1 = argmin
Φ,ξ

Qt(wt,Φ, ξ) s.t. qt(wt,Φ) ≤ ξ.
(7)

This means that the optimization is performed
with either Φ fixed to Φt or w fixed to wt.

Informally, the iterative Algorithm 1, result-
ing from this alternate scheme, will solve at each
round a simple constrained optimization problem,
in which the objective is to minimize the squared
Euclidean distances between the new weight vec-
tor (resp. the new embedding matrix) and the cur-
rent one, while making sure that both sets of pa-
rameters achieve a correct prediction with a suffi-
ciently high margin. Note that one may recover the
standard passive-aggressive algorithm by simply
fixing the embedding matrix to the identity matrix.
Also note that if the right stopping criteria are re-
tained, Algorithm 1 is guaranteed to converge to a
local optimum of (5) (see (Gorski et al., 2007)).

When fully developed, problems (6)-(7) respec-
tively write as:

wt+1, ξt+1 = argmin
w,ξ

1

2
‖w −wt‖22 + Cξ2

s.t. 1− 〈w, ytΦtxt〉 ≤ ξ,

777



or

Φt+1, ξt+1 = argmin
Φ,ξ

λ

2
‖Φ−Φt‖2F + Cξ2

s.t. 1−
〈
Φ, ytwtx

>
t

〉
F
≤ ξ.

Using the equivalence in (4), both problems can be
seen as special instances of the generic problem:

u∗, ξ∗ = argmin
u,ξ

λ

2
‖u− ut‖2? + Cξ2 (8a)

s.t. 1− 〈u,vt〉? ≤ ξ. (8b)

where ‖·‖? and 〈·, ·〉? are generalized versions of
the l2 norm and the inner product, respectively.

This is a convex optimization problem that can
be readily solved using classical tools from convex
optimization to give:

u∗ = ut + τuvt, with τu =
max(0, 1− 〈ut,vt〉?)

‖vt‖2? + λ2C
, (9)

which comes from the following proposition.

Proposition 1. The solution of (8) is (9).

Proof. The Lagrangian associated with the prob-
lem is given by:

L(u, ξ, τ) = λ
2
‖u− ut‖2? + Cξ2 + τ

(
1− ξ − 〈u,vt〉?

)
(10)

with τ > 0.
Necessary conditions for optimality are∇uL =

0, and∇ξL = 0, which imply u = ut + τλvt, and
ξ = τ2C . Using this in (10) gives the function:

g(τ) =
τ2 ‖vt‖2?

2λ
+
τ2

4C
+τ− τ

2

2C
−τ 〈ut,vt〉?−

τ2 ‖vt‖2?
λ

,

which is maximized with respect to τ when
g′(τ) = 0, i.e.:
(
‖vt‖2?
λ
− 1

2C
− 2 ‖vt‖

2
?

λ

)
τ + 1− 〈ut,vt〉? = 0.

Taking into account the constraint τ ≥ 0, the max-
imum of g is attained at τ̃ with:

τ̃ =
λmax(0, 1− 〈ut,vt〉?)

‖vt‖2? + λ2C
,

which, setting τu = τ̃ /λ gives (9).

The previous proposition allows us to readily
have the solutions of (6) and (7) as follows.

Proposition 2. The updates induced by the solu-
tions of (6) and (7) are respectively given by:

w∗ = wt + τwytxt, with τw =
`t(wt; Φt)

‖Φtxt‖22 + 12C
(11)

Φ∗ = Φt + τΦytwtx
>
t , with τΦ =

`t(wt; Φt)

‖wt‖22 ‖xt‖22 + λ2C
,

(12)

where `t(w; Φ) = max(0, qt(w,Φ)).

Proof. Just instantiate the results of Proposi-
tion 1 with (u,vt) = (w, ytΦtxt) for (11) and
(u,vt) = (Φ, ytxtw>t ) for (12).

Remark 1 (Hard-margin case). Note that the
hard-margin version of the previous problem:

argmin
w∈Rk

Φ∈Rk×p

1

2
‖w −wt‖22 +

λ

2
‖Φ−Φt‖2F (13)

s.t. 1− yt 〈w,Φxt〉 ≤ 0 (14)

is degenerate from the alternated optimization
point of view. It suffices to observe that the updates
entailed by the hard-margin problem correspond
to (9) with C set to ∞; if it happens that either
Φ0 = 0 or w0 = 0, then one of the optimization
problems (wrt to w or Φ) has no solution.

3 Analysis

Using the same technical tools as in (Crammer et
al., 2006), and the unified formalization of sec-
tion 2, we have the following result.
Proposition 3. Suppose that problem (8) is itera-
tively solved for a sequence of vectors v1, . . . ,vT
to give u2, . . . ,uT+1, u1 being given. Suppose
that, at each time step, ‖vt‖? ≤ R, for some
R > 0. Let u∗ be an arbitrary vector living in
the same space as u1. The following result holds
T∑
t=1

`2t ≤
(
R2 +

λ

2C

)(
‖u∗ − u1‖2? +

2C

λ

T∑
t=1

(`∗t )
2

)
(15)

where

`t = max(0, 1−〈ut,vt〉?), and `∗t = max(0, 1−〈u∗,vt〉?).
(16)

This proposition and the accompanying lemmas
are simply a variation of the results of (Crammer
et al., 2006), with the addition that they are based
on the generic problem (8). The loss bound ap-
plies for a version of Algorithm 1 where one of
the parameters, either the weight vector or the re-
embedding matrix, is kept fixed for some time.

Proposition 3 makes use of the following lemma
(see Lemma 1 in (Crammer et al., 2006)):

778



Lemma 1. Suppose that problem (8) is iteratively
solved for a sequence of vectors v1, . . . ,vT to
give u2, . . . ,uT+1, u1 being given. The follow-
ing holds:

T∑
t=1

τu(2`t − τu ‖vt‖2? − 2`∗t ) ≤ ‖u∗ − u1‖2? . (17)

Proof. As in (Crammer et al., 2006), simply
set ∆t = ‖ut − u∗‖2? − ‖ut+1 − u∗‖2? and
bound

∑T
τ=1 ∆t from above and below. For

the upper bound:
∑T

τ=1 ∆t = ‖u1 − u∗‖2? −
‖uT+1 − u∗‖2? ≤ ‖u1 − u∗‖2? .

For the lower bound, we focus on the nontrivial
situation where `t > 0, otherwise ∆t = 0 (i.e. no
update is made) and the bounding is straightfor-
ward. Making use of the value of τu:

∆t = ‖ut − u∗‖2? − ‖ut+1 − u∗‖2?
= ‖ut − u∗‖2? − ‖ut + τuvt − u∗‖2? (see (8))
= −2τu 〈ut − u∗,vt〉? − τ2u ‖vt‖2?
= −2τu 〈ut,vt〉? + 2τu 〈u∗,vt〉? − τ2u ‖vt‖2? .

Since `t > 0, then 〈ut,vt〉? = 1− `t; also, by the
definition of `∗t (see (16)), `∗t ≥ 1 − 〈u∗,vt〉?, or
〈u∗,vt〉? ≥ 1− `∗t . This readily gives

∆t ≥ −2τu(1− `t) + 2τu(1− `∗t )− τ2u ‖vt‖2?
= 2τu`t − 2τu`∗t − τ2u ‖vt‖2? ,

hence the targetted lower bound and, in turn, (17).

Proof of Proposition 3. As for the proof of Propo-
sition 3, it suffices, again, to follow the steps given
in (Crammer et al., 2006), but this time, for the
proof of Theorem 5.

Note that for any β 6= 0, (βτu− `∗t /β)2 is well-
defined and nonnegative and thus:

‖u∗ − u1‖2?

≥
T∑
t=1

[
τu(2`t − τu ‖vt‖2? − 2`∗t )− (βτu − `∗t /β)2

]
=

T∑
t=1

(2τu`t − τ2u(‖vt‖2? + β2)− 2τu`2t

+ 2τu`
2
t − (`∗t )2/β2)

=

T∑
t=1

(2τu`t − τ2u(‖vt‖2? + β2)− (`∗t )2/β2).

Setting β =
√
λ/2C and using τu = `t/(‖v‖2? +

Figure 1: Accuracy rates for PA and RPA as a
function of the variance of the Gaussian noise
added to the “true” embedding Φ. The observed
X matrix is n = 500× p = 1000.

λ
2C ) (see (9)) gives

‖u∗ − u1‖2?

≥
T∑
t=1

(
2τu`t − τ2u

(
‖vt‖2? +

λ

2C

)
− 2C

λ
(`∗t )

2

)
,

=

T∑
t=1

(
`2t

‖vt‖2? + λ2C
− 2C

λ
(`∗t )

2

)
.

Using the assumption that ‖vt‖? ≤ R fo all t and
rearranging terms concludes the proof.

The result given in Proposition 3 bounds the cu-
mulative loss of the learning procedure when one
of the parameters, either Φ or w, is fixed and the
other is the optimization variable. Therefore, it
does not directly capture the behavior of Algo-
rithm 1, which alternates between the updates of
Φ and w. A proper analysis of Algorithm 1 would
require a refinement of Lemma 1 which, to our
understanding, would be the core of a new result.
This is a problem we intend to put our energy on
in the near future, as an extension to this work.

4 Experiments on Synthetic Data

In order to better understand and validate the RPA
algorithm, we first conducted some synthetic ex-
periments. Specifically, we simulated a high-
dimensional matrix X ∈ Rn×p, with n data sam-
ples realizing p “words” and p � n, using the
following generative model:

X = ZΦ̃,where Φ̃ = Φ + E

That is, each p-dimensional data point xi was gen-
erated from a hidden lower k-dimensional zi and

779



Figure 2: Hyper-plane learned by PA on XΦ̃> (left pane) compared to hyper-plane and data representa-
tions learned by RPA (right pane). X is n = 200 × p = 500, and noise σ = 2. Training and test points
appear as circles or plus marks, respectively. RPA’s hyper-parameters are set to C = 100 and λ = .5.

an embedding matrix Φ ∈ Rk×p, mapping k latent
concepts to the p observed words. For simplic-
ity, we assume that: (i) there are only two con-
cepts (i.e., k = 2), (ii) each data point realizes
a single concept (i.e., each xi is a p-dimensional
indicator vector), (iii) each concept is equally rep-
resented in the data (with n/2 data points), and
(iv) each concept deterministically signals a class
label, either −1 or +1. Recovering Z and pre-
dicting the zi’s labels is trivial if one is given X
and the true embedding Φ, so we added Gaussian
noise εi ∼ N (0, σ2Ip) to each φi. The resulting,
observed noisy matrix is denoted Φ̃.

Given this setting, the goal of the RPA is to learn
a set of classification parameters w ∈ Rk in the la-
tent space and to “de-noise” the observed embed-
ding matrix Φ̃ by exploiting the labeled data. We
are interested in comparing the RPA with a regular
PA that directly learns from the noisy data XΦ̃>.
The outcome of this comparison is plotted in Fig-
ure 1. For this experiment, we randomly splitted
theX data according to 80/10/10 for train/dev/test
and considered increasing noise variance from 0 to
10 by increment of 0.1. Each dot in Figure 1 cor-
responds to the average accuracy over 10 seperate,
random initializations of the embedding matrix at
a particular noise level. Hyper-parameters were
optimized using a grid search on the dev set for
both the PA and RPA.1

As shown in Figure 1, the PA’s accuracy quickly
drops to levels that are only slightly above chance,
while the RPA manages to maintain an accuracy
close to .7 even with large noise. This indicates

1We use {1, 5, 10, 50} for the number iterations, C’s val-
ues were {.1, .5, 1.0, 2.0, 10.0}, and λ was set to 10k, with
k ∈ {−2,−1, 0, 1, 2}.

that the RPA is able to recover some of the struc-
ture in the embedding matrix. This behavior is
also illustrated in Figure 2, wherein the two hidden
concepts appear in yellow and green. While the
standard PA learns a very bad hyper-plane, which
fails to separate the two concepts, the RPA learns
a much better hyper-plane. Interestingly, most of
the data points appear to have been projected on
the margins of the hyper-plane.

5 Experiments on NLP tasks

This section assesses the effectiveness of RPA on
several text classification tasks.

5.1 Evaluation Datasets

We consider five different classification tasks
which are concisely summarized in Table 1.

20 Newsgroups Our first three text classification
tasks from this dataset2 consists in categorizing
documents into two related sub-topics: (i) Comp.:
IBM vs. Mac, (ii) Religion: atheism vs. christian,
and (iii) Sports: baseball vs. hockey.

IMDB Movie Review This movie dataset3 was
introduced by (Maas et al., 2011) for sentiment
analysis, and contains 50, 000 reviews, divided
into a balanced set of highly positive (7 stars out
of 10 or more) and negative scores (4 stars or less).

TREC Question Classification This dataset4
(Li and Roth, 2002) involves six question types:
abbreviation, description, entity, human, location,
and number.

2qwone.com/˜jason/20Newsgroups
3ai.stanford.edu/˜amaas/data/sentiment
4cogcomp.cs.illinois.edu/Data/QA/QC

780



Name lab. examples Voc. size
Train Test All fr > 1

comp 2 1 168 777 30 292 15 768
rel. 2 1 079 717 32 361 18 594
sports 2 1 197 796 33 932 19 812

IMDB 2 25k 25k 172 001 86 361

TREC 6 5 452 500 9 775 3 771

Table 1: Number of labels, samples, vocabu-
lary sizes (incl. non-hapax words) per dataset.

Word Embedding
CnW GV6 GV840 HPCA HLBL SkGr

40.41 27.18 20.26 32.20 40.44 32.19
25.45 13.07 8.95 16.90 25.39 17.33
35.29 19.01 13.96 29.97 35.21 30.66

39.79 12.67 4.93 22.15 39.63 16.79

3.42 0.61 0.40 1.38 3.63 4.51

Table 2: Out-of-vocabulary rates for non-hapax
words in each dataset-embedding pair.

5.2 Preprocessing and Document Vectors
All datasets were pre-preprocessed with the Stan-
ford tokenizer5, except for the TREC corpus
which comes pre-tokenized. Case was left intact
unless used in conjunction with word embeddings
that assume down-casing (see below).

For constructing document or sentence vectors,
we used a simple 0-1 bag-of-word model, sim-
ply summing over the word vectors of occurring
tokens, followed by L2-normalization of the re-
sulting vector in order to avoid document/sentence
length effects. For each dataset, we restricted the
vocabulary to non-hapax words (i.e., words occur-
ring more than once). Words unknown to the em-
bedding were mapped to zero vectors.

5.3 Initial Word Vector Representations
Five publicly available word vectors were used
to define initial embedding matrices in the RPA.
The coverage of the different embeddings wrt each
dataset vocabulary is reported in Table 2.

CnW These word vectors were induced using
the neural language model of (Collobert and We-
ston, 2008) re-implemented by (Turian et al.,
2010).6 They were trained on 63M word news cor-
pus, covering 268, 810 word forms (intact case),
with 50, 100 or 200 dimensions for each word.

HLBL These were obtained using the prob-
abilistic Hierarchical log-bilinear language
model of (Mnih and Hinton, 2009), again re-
implemented by (Turian et al., 2010) with 50 or
100 dimensions. They cover 246, 122 word forms.

HPCA (Lebret and Collobert, 2014) present
a variant of Principal Component Analysis,
Hellinger PCA, for learning spectral word vec-
tors. These were trained over 1.62B words from

5nlp.stanford.edu/software/tokenizer.
shtml

6metaoptimize.com/projects/wordreprs

Wikipedia, RCV1, and WSJ with all words lower-
cased, and digits mapped to a special symbol. Vo-
cabulary is restricted to words occurring 100 times
or more (hence, a total of 178, 080 words). These
come in 50, 100 and 200 dimensions.7

GloVe These global word vectors are trained us-
ing a log-bilinear regression model on aggregated
global word co-occurrence statistics (Pennington
et al., 2014). We use two different releases:8 (i)
GV6B trained on Wikipedia 2014 and Gigaword 5
(amounting to 6B down-cased words and a vocab-
ulary of 400k) with vectors in 50, 100, 200, or 300
dimensions, and (ii) GV840B trained over 840B
uncased words (a 2.2M vocabulary) with vectors
of length 300.

SkGr Finally, we use word vectors pre-trained
with the skip-gram neural network model of
(Mikolov et al., 2013): each word’s Huffman code
is fed to a log-linear classifier with a continuous
projection layer that predicts context words within
a specified window. The embeddings were trained
on a 100B word corpus of Google news data (a
3M vocabulary) and are of length 300.9

rand In addition to these pre-trained word repre-
sentations, we also use random vectors of lengths
50, 100, and 200 as a baseline embedding. Specif-
ically, each component of these word vector is uni-
formly distributed on the unit interval (−1, 1).

5.4 Settings

The hyperparameters of the model, C, λ, and
the number it of iterations over the training set,
were estimated using a grid search over C =
10k∈{−6,−4,−2,0,2,4,6}, λ = 10l∈{−3,−2,−1,0,1,2,3},
and it ∈ {1, 5, 10} in a 10-fold cross-validation

7lebret.ch/words
8nlp.stanford.edu/projects/glove
9code.google.com/archive/p/word2vec

781



Emb/Task comp religion sports trec imdb Average
Method Size PA RPA PA RPA PA RPA PA RPA PA RPA PA RPA

rand 50 54.57 87.13 60.67 91.91 63.07 92.34 56.80 88.40 61.47 86.47 59.32 89.25
100 62.03 87.39 66.95 92.19 65.58 93.22 68.00 88.20 65.07 86.56 65.53 89.51
200 65.51 87.64 73.22 92.19 71.11 92.96 70.00 88.20 69.10 86.58 69.79 89.51

CnW 50 50.32 85.97 55.09 91.91 56.91 93.72 69.40 87.20 58.68 86.54 58.08 89.07
100 49.29 87.13 47.56 91.63 58.54 93.72 69.20 87.80 65.04 86.51 57.93 89.36
200 50.32 87.00 53.14 91.77 63.94 93.72 75.80 87.60 68.21 86.64 62.28 89.35

HLBL 50 51.99 87.13 68.62 91.35 61.31 93.72 66.80 88.80 67.30 86.70 63.20 89.54
100 53.54 86.74 65.83 91.77 63.07 93.72 75.60 88.20 72.41 86.68 66.09 89.42

HPCA 50 50.45 86.87 50.77 91.63 64.20 93.34 66.00 88.60 62.78 80.56 58.84 88.20
100 50.45 86.87 47.98 91.63 64.57 93.72 72.00 88.40 64.25 85.33 59.85 89.19
200 50.45 86.87 48.12 91.91 62.56 93.59 78.40 88.00 64.75 85.84 60.86 89.24

GV6B 50 55.21 86.87 75.59 91.91 86.68 95.23 65.80 89.00 75.12 86.41 71.68 89.88
100 58.17 86.87 76.43 91.63 90.33 96.48 70.40 89.00 78.72 86.50 74.81 90.10
200 70.40 86.87 73.22 91.63 93.34 97.11 76.60 89.00 81.55 86.57 79.02 90.24
300 76.58 87.13 79.92 91.77 94.97 97.74 78.60 88.60 82.41 86.41 82.50 90.33

GV840B 300 75.80 87.13 88.15 92.05 88.69 96.23 77.20 89.20 84.17 86.46 82.80 90.21

SkGr 300 70.79 87.39 88.01 92.05 91.96 97.61 84.00 90.60 83.39 88.52 83.63 91.23

one-hot 87.26 91.91 93.47 88.00 88.29 89.786

Table 3: Accuracy results on our five datasets for the Re-embedding Passive-Aggressive (RPA) against
standard PA-II (PA). For each task, the best results for each embedding method (across different dimen-
sions) has been greyed out, while the overall highest accuracy score has been underlined.

over the training data. For the alternating on-
line procedure, we used the difference between
the objective values from one iteration to the next
for defining the stopping criterion, with maximum
number of iterations of 50. In practice, we found
that the search often converged much few itera-
tions. The multi-class classifier used for the TREC
dataset was obtained by training the RPA in sim-
ple One-versus-All fashion, thus learning one em-
bedding matrix per class.10 For datasets with label
imbalance, we set a different C parameter for each
class, re-weighting it in proportion to the inverse
frequency of the class.

5.5 Results

Table 3 summarizes accuracy results for the RPA
against those obtained by a PA trained with fixed
pre-trained embeddings. The first thing to notice is
that the RPA delivers massive accuracy improve-
ments over the vanilla PA across datasets and em-
bedding types and sizes, thus showing that the
RPA is able to learn word representations that are
better tailored to each problem. On average, ac-
curacy gains are between 22% and 31% for CnW,
HLBL, and HPCA. Sizable improvements, rang-
ing from 8% and 18%, are also found for the better

10More interesting configurations (e.g., a single embed-
ding matrix shared across classes), are left for future work.

performing GV6B, GV840B, and SkGr. Second,
RPA is able to outperform on all five datasets the
strong baseline provided by the one-hot version of
PA trained on the original high-dimensional space,
with some substantial gains on sports and trec.

Overall, the best scores are obtained with the re-
embedded SkGr vectors, which yield the best av-
erage accuracy, and outperform all the other con-
figurations on two of the five datasets (trec and
imdb). GV6B (dimension 300) has the second best
average scores, outperforming all the other con-
figurations on sports. Interestingly, embeddings
learned from random vectors achieve performance
that are often on a par or higher than those given by
HLBL, HPCA or CnW initializations. They actu-
ally yield the best performance for the two remain-
ing datasets: comp and religion. On these tasks,
RPA does not seem to benefit from the informa-
tion contained in the pre-trained embeddings, or
their coverage is not just good enough.

For both PA and RPA, performance appear to
be positively correlated with embedding coverage:
embeddings with lower OOV rates generally per-
form better those with more missing words. The
correlation is only partial, since GV840B do not
yield gains compared to GV6B and SkGr despite
its better word coverage. Also, SkGr largely out-
performs HPCA although they have similar OOV

782



Emb/Task comp religion sports trec imdb Average
Method Size PA RPA PA RPA PA RPA PA RPA PA RPA PA RPA

rand 50 56.76 84.04 59.55 90.24 64.20 90.08 53.20 83.60 60.97 85.74 58.94 86.74
100 63.06 84.17 67.50 91.35 65.58 90.95 61.20 83.00 64.12 85.90 64.29 87.07
200 64.86 85.07 71.27 91.07 68.59 90.95 59.60 83.40 67.84 85.75 66.43 87.25

cnw 50 50.32 84.81 55.51 90.93 50.63 91.08 61.20 84.20 50.86 84.82 53.70 87.17
100 50.71 84.81 55.51 91.35 54.15 91.08 66.40 83.60 50.98 85.40 55.55 87.25
200 50.45 84.04 55.51 90.93 54.77 91.08 65.80 82.60 52.36 85.50 55.78 86.83

HLBL 50 53.15 85.07 59.97 89.82 56.41 90.95 56.60 83.40 62.38 85.66 57.70 86.98
100 53.80 84.68 61.09 91.21 56.91 90.95 67.60 84.00 67.88 85.64 61.46 87.30

HPCA 50 50.45 85.20 55.51 91.35 52.39 89.95 62.20 83.00 51.02 83.50 54.31 86.60
100 50.45 85.20 55.51 91.07 49.87 90.08 66.20 83.60 50.51 84.86 54.51 86.96
200 50.45 85.20 55.51 90.10 49.87 89.82 68.00 82.80 50.38 85.72 54.84 86.73

GV6B 50 50.97 85.07 64.16 91.49 55.28 91.08 57.00 85.00 69.48 85.48 59.38 87.62
100 50.58 84.94 60.53 89.68 63.82 91.08 58.00 84.60 70.22 85.51 60.63 87.16
200 51.22 85.20 64.99 91.49 85.05 90.08 58.00 84.80 73.64 85.59 66.58 87.43
300 56.24 85.33 70.15 89.68 89.07 91.21 72.40 85.20 75.48 85.79 72.67 87.44

GV840B 300 66.02 84.81 77.96 89.68 89.82 91.08 77.80 86.40 77.57 85.73 77.83 87.54

SkGr 300 67.95 82.50 81.59 89.40 95.10 94.60 80.80 88.40 80.93 85.92 81.27 88.16

one-hot 84.56 89.96 90.58 85.80 87.40 87.66

Table 4: Accuracy results for RPA against standard PA both run for a single iteration.

rates. As for dimensions, embeddings of length
100 and more perform the best, although they
involve estimating larger number of parameters,
which is a priori difficult given the small sizes of
the datasets.

By comparison with previous work on imdb,
the RPA performance are substantially better than
those reported by (Labutov and Lipson, 2013),
whose best re-embedding score is 81.15 with
CnW. By comparison, our best score with CnW
is 86.64, and 88.52 with SkGr, thus closing the
gap on (Maas et al., 2011) who report an accu-
racy of 88.89 using a much more computationally
intensive approach specifically tailored to senti-
ment analysis. Interestingly, (Labutov and Lipson,
2013) show that accuracy can be further improved
by concatenating re-embedded and 1-hot represen-
tations. This option is also available to us, but we
leave it to future work.

Finally, Table 4 report accuracy results for RPA
against PA when both algorithms are trained in a
genuine online mode, that is with a single pass
over the data. As expected, performance drop for
the RPA and the PA, but the decreases are compar-
atively much smaller for the RPA (from 2% to 3%)
compared to the PA (from 0.4% to 14%).

6 Conclusion and Future Work

In this paper, we have proposed a new scalable
algorithm for learning word representations that

are specifically tailored to a classification objec-
tive. This algorithm generalizes the well-known
Passive-Aggressive algorithm, and we showed
how to extend the regret bounds results of the
PA to the RPA when either the weight vector
or the embedding matrix is fixed. In addition,
we have also provided synthetic and NLP exper-
iments, demonstrating that the good classification
performance of RPA.

In future work, we first would like to achieve
a more complete analysis of the RPA algorithm
when both w and Φ both get updated. Also,
we intend to investigate potential exact meth-
ods for solving biconvex minimization (Floudas
and Viswewaran, 1990), as well as to develop
a stochastic version of RPA, thus foregoing run-
ning the inner alternate search to convergence.
More empirical perspectives include extending the
RPA to linguistic structured prediction tasks, bet-
ter handling of unknown words, and a deeper in-
trinsic and statistical evaluation of the embeddings
learned by the RPA.

Acknowledgments

We thank the three anonymous reviewers for
their comments. Pascal Denis was supported by
ANR Grant GRASP No. ANR-16-CE33-0011,
as well as by a grant from CPER Nord-Pas de
Calais/FEDER DATA Advanced data science and
technologies 2015-2020.

783



References
Mohit Bansal, Kevin Gimpel, and Karen Livescu.

2014. Tailoring continuous word representations for
dependency parsing. In ACL (2), pages 809–815.

Mathieu Blondel, Yotaro Kubo, and Ueda Naonori.
2014. Online passive-aggressive algorithms for
non-negative matrix factorization and completion.
In Proceedings of the Seventeenth International
Conference on Artificial Intelligence and Statistics,
pages 96–104.

Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, and
Steven Skiena. 2013. The expressive power of word
embeddings. arXiv preprint arXiv:1301.3226.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551–585.

C. A. Floudas and V. Viswewaran. 1990. A global
optimization algorithm (gop) for certain classes of
nonconvex npls. i, theory. Computers & chemical
engineering, 14(12):1397–1417.

Jochen Gorski, Frank Pfeuffer, and Kathrin Klamroth.
2007. Biconvex sets and optimization with biconvex
functions: a survey and extensions. Mathematical
Methods of Operations Research, 66(3):373–407.

Y. Grandvalet and S. Canu. 2003. Adaptive scaling for
feature selection in svms. In Advances in Neural
Information Processing Systems, volume 15. MIT
Press.

Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies.

Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Proceedings of the 51th Annual Meeting
of the Association for Computational Linguistics.

Rémi Lebret and Ronan Collobert. 2014. Word
emdeddings through hellinger PCA. In Proceedings
of the 14th Conference of the European Chapter of
the Association for Computational Linguistics.

Rémi Lebret, Joël Legrand, and Ronan Collobert.
2013. Is deep learning really necessary for word em-
beddings? In NIPS Workshop on Deep Learning.

Omer Levy and Yoav Goldberg. 2014a. Dependency-
based word embeddings. In ACL (2), pages 302–
308.

Omer Levy and Yoav Goldberg. 2014b. Neural word
embedding as implicit matrix factorization. In Ad-
vances in neural information processing systems,
pages 2177–2185.

Xin Li and Dan Roth. 2002. Learning question clas-
sifiers. In Proceedings of the 19th international
conference on Computational linguistics-Volume 1,
pages 1–7. Association for Computational Linguis-
tics.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Andriy Mnih and Geoffrey E Hinton. 2009. A scal-
able hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081–1088.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, volume 14, pages 1532–
43.

Tobias Schnabel, Igor Labutov, David Mimno, and
Thorsten Joachims. 2015. Evaluation methods
for unsupervised word embeddings. In Proc. of
EMNLP.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.

Mengqiu Wang and Christopher D Manning. 2013. Ef-
fect of non-linear deep architecture in sequence la-
beling. In IJCNLP, pages 1285–1291.

784


