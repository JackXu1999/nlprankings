



















































A Topic Augmented Text Generation Model: Joint Learning of Semantics and Structural Features


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5090–5099,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5090

A Topic Augmented Text Generation Model: Joint Learning of Semantics
and Structural Features

Hongyin Tang1,2, Miao Li1,2, Beihong Jin1,2,∗
1State Key Laboratory of Computer Science

Institute of Software, Chinese Academy of Sciences
2University of Chinese Academy of Sciences, Beijing China
{tanghongyin14,limiao17}@otcaix.iscas.ac.cn

Beihong@iscas.ac.cn

Abstract

Text generation is among the most fundamen-
tal tasks in natural language processing. In
this paper, we propose a text generation model
that learns semantics and structural features si-
multaneously. This model captures structural
features by a sequential variational autoen-
coder component and leverages a topic mod-
eling component based on a Gaussian distri-
bution to enhance the recognition of text se-
mantics. To make the reconstructed text more
coherent to the topics, the model further adapts
the encoder of the topic modeling component
for a discriminator. The results of experi-
ments over several datasets demonstrate that
our model outperforms several states of the art
models in terms of text perplexity and topic co-
herence. Moreover, the latent representations
learned by our model is superior to others in
a text classification task. Finally, given the in-
put texts, our model can generate meaningful
texts which hold similar structures but under
different topics.

1 Introduction

Text generation is a fundamental task in natu-
ral language processing (NLP). Existing methods
for text generation are mostly limited in super-
vised setting and designed for specific applications
(e.g., machine translation (Bahdanau et al., 2015),
text summarization (Rush et al., 2015)). Sev-
eral research work (Yu et al., 2017; Zhang et al.,
2016) attempts generic text generation based on
deep generative models (e.g., GAN, VAE). How-
ever, models based on GAN(Generative Adver-
sarial Network) are not able to generate explicit
latent codes with salient features of texts. Dif-
ferent from GAN-based models, the VAE (Varia-
tional Autoencoder) and its variants can get latent
codes of texts with reconstructing texts by its de-
coder, where it is assumed that the generation pro-
cess is controlled by codes in a continuous latent

space. A natural way to implement the VAEs is to
adopt autoregressive networks (e.g., RNNs) as the
decoder and encoder. This kind of implementa-
tion of VAEs considers sequential information of
texts, and is able to model the linguistic structure
of texts. But it is not good at modeling seman-
tics (Dieng et al., 2017) and may cause the latent
variable collapse problem (Bowman et al., 2016)
which leads that the decoder ignores information
from the inferred latent codes.

In general, texts inherently contain semantic
features and structural features. Only when the
latent codes of texts contain structural and seman-
tic information can high-quality texts be generated
from the codes. As far as the standard VAE is con-
cerned, it assumes that the latent code is Gaussian
distributed so that we cannot distinguish which
part of code controls the structure and which part
controls the semantics. In other words, it is diffi-
cult to generate effective texts by controlling text
features directly.

To generate high-quality texts (i.e., with se-
mantic and structural information), we propose a
model named TATGM which adopts a sequential
VAE to learn structural features of texts and builds
a topic modeling component which extracts se-
mantic features of texts. The main contributions
of this paper are summarized as follows:

• TATGM can capture the semantics of texts
by introducing the topic modeling compo-
nent. The topic modeling component gener-
ates words of texts based on a Gaussian dis-
tribution which enables us to take full advan-
tage of information shared by the word em-
beddings. Moreover, the encoder of the topic
modeling component is served as a discrim-
inator to force the decoder of the sequence
modeling component to generate texts having
the semantics as close to the original texts as



5091

possible. Specifically, no extra training su-
pervised by labeled data is needed for this
discriminator.

• The latent code learned by TATGM is a con-
catenation of the latent variables of the topic
modeling component and the sequence mod-
eling component. By two separate parts of
the code, TATGM can control structural and
semantic information independently. Thus,
we can get texts with the changed seman-
tics or structure by changing one part of the
code. One interesting practice is that we can
get question-answering pairs with the same
structure in different semantic spaces.

• The modeling ability of TATGM is evaluated
by extensive experiments in terms of perplex-
ity and the coherence of topics. Furthermore,
to verify whether TATGM can learn salient
features, a classification task using the la-
tent codes is conducted. Experimental results
show that TATGM achieves the best perfor-
mance while compared with many existing
models. Finally, several texts generated by
TATGM are demonstrated, which indicates
that TATGM can generate different expres-
sions of texts of the same structure in differ-
ent topics.

The rest of the paper is organized as follows.
Section 2 introduces the related work. Section 3
describes the model TATGM in detail. Section 4
gives an experimental evaluation. Finally, the pa-
per is concluded in Section 5.

2 Related Work

VAEs (Kingma and Welling, 2014) are a type of
deep generative models, which learns explicit la-
tent codes of input data in a continuous latent
space and generates data with decoders. VAEs
can extract the features of input texts by pos-
terior inference with neural networks and repa-
rameterization tricks. VAE-based models have
been widely applied in image generation (Gregor
et al., 2015), machine translation (Zhang et al.,
2016), and knowledge graph reasoning (Zhang
et al., 2018). VAE-based models can also be ap-
plied in different tasks of text processing, where
the input document is treated as its bag of words
(Miao et al., 2016; Srivastava and Sutton, 2017;
Miao et al., 2017) or a sequential text (Bowman

et al., 2016). The former type of models is al-
ways related to topics. For example, Miao et al.
(2016) builds the connection by letting weights of
the softmax decoder to indicate topic interpretabil-
ity. Although these models overlook sequential
information in texts, they can learn effective la-
tent codes with global semantics. On the other
hands, VAE-based models with the input of se-
quences usually adopt RNNs as encoders or de-
coders. However, latent variable collapse (i.e., the
latent variable collapses to obey the distribution of
the prior) makes the sequential VAE to generate
texts without information of latent code so that the
latent code cannot have enough information of text
features.

To avoid the latent variable collapse problem,
two types of methods are adopted. The first
type tackles this problem by weakening the con-
text modeling ability of decoders mostly in the
model architecture level. For example, Bowman
et al. (2016) adopts word dropout when feeding
the decoder and Yang et al. (2017); Semeniuta
et al. (2017) employ non-autoregressive networks
(e.g., convolutional neural networks) in the de-
coder. The second type is to modify the origi-
nal objective. For example, Bowman et al. (2016)
adopts KL annealing which can be seen as grad-
ually adding annealing weight into the objective
function. Zhao et al. (2018) introduces an addi-
tional mutual information term to compensate for
the objective function. Xiao et al. (2018) and Zhao
et al. (2017) introduce the bag-of-words loss as an
auxiliary loss, which measures how well predic-
tions of words can be made from the latent codes.
The ideas behind all these methods are same, i.e.,
to force the models to generate texts from the la-
tent codes so as to make latent codes have more
information on text features.

Improving the sequential model by incorporat-
ing topics have been explored (Lau et al., 2017;
Wang et al., 2018; Dieng et al., 2017; Mikolov
and Zweig, 2012). But these models do not have a
code for the text structure. Taking TGVAE (Wang
et al., 2018) as an example, it guides the gener-
ation of the VAE latent code by topic distribu-
tion. However, it does not separate the seman-
tic and structure latent codes explicitly. In ad-
dition, the models mentioned above use a multi-
nominal LDA which cuts off the possibility of
leveraging the semantics in embeddings. As a
remedy, Das et al. (2015) and Hu et al. (2012)



5092

Figure 1: The holistic structure of TATGM. The dotted arrow denotes that the texts which are generated by the
sequence modeling decoder are fed into the topic modeling encoder as the input of the discriminator.

propose to adopt a Gaussian-based topic model
which assumes each word is generated by a Gaus-
sian distribution. However, their learning algo-
rithms are based on sampling and variational infer-
ence, which cannot be assembled in an end-to-end
mode.

Hu et al. (2017) proposes a text generation
model based on VAE, which aims at generating
sentences with controllable styles by learning dis-
entangled latent representations. It feeds gener-
ated texts into a discriminator to preserve that
these texts have the given style and optimizes the
generator with the signals backpropagated from
the discriminator. Tian et al. (2018) uses a clas-
sifier which is trained with small datasets as the
discriminator. Yang et al. (2018) adopts a lan-
guage model as the discriminator to control the
style transfer of generated texts.

Different from existing work, our model com-
bines a sequence modeling component and a topic
modeling component so that the final latent code
of our model contains both topic semantics and se-
quential structure of texts. In particular, our model
can learn these two features simultaneously, since
AutoEncoding Variation Bayes (AEVB) are em-
ployed to train these two components. Moreover,
our models generate bags-of-words text represen-
tations from the latent codes, which avoids the la-
tent variable collapse problem.

3 Model

TATGM is essentially a hybrid autoencoder which
comprises a topic modeling component capturing

topic information and a sequence modeling cap-
turing structural features. The topic modeling
component captures topic information through a
Gaussian-based topic model while the sequence
modeling component integrates the topic informa-
tion and generates text. As shown in Fig. 1, the
input texts are fed into the two components simul-
taneously. Each component contains an encoder
and decoder, which are labeled by its own color.

3.1 Neural Topic Modeling Component

Similar to existing topic modeling methods, we
treat bag-of-words representations of texts as in-
put. However, since the encoder in the topic mod-
eling component is expected to be as a discrimi-
nator and guarantee that texts generated by the se-
quence modeling decoder have specific topic in-
formation, the encoder cannot be with the dis-
crete representations (e.g., one-hot representation)
of texts as input. It is because such discrete rep-
resentations of texts cannot be compatible with
backward gradients from the discriminator. There-
fore, we employ the embeddings of words as in-
put, considering that the word embedding space is
continuous and the similarity between every two
words can be calculated by the Euclidean distance.
Further, we assume that there are K topics in the
corpus and each topic is represented as a multi-
variable Gaussian distribution with a mean and a
variance (e.g., N (µk,σ2kI) for a given topic k).

In our neural topic modeling component, a
document with n words is represented as x =
{xi}ni=1 where xi ∈ Re denotes the embedding



5093

of i-th word of the document. The generative pro-
cess of the document is as follows.

1. For each document, draw the document-topic
distribution zt ∼ Dir(α).

2. For i-th word in the document.

(a) Draw topic assignment ti ∼ Cat(zt)
(b) Draw the word xi ∼ N (µti ,σ

2
tiI)

According to the generative process above and
the marginalization of ti, the likelihood p(x) of
the document x can be derived as

∫
zt

p(zt|α)

(
n∏

i=1

∑
ti

p(xi|µti ,σ
2
tiI)p(ti|zt)

)
dzt

=

∫
zt

p(zt|α)

(
n∏

i=1

p(xi|µ,σ2I,zt)

)
dzt

(1)

where µ = {µ}Kk=1, σ = {σ}Kk=1. Further, we
adopt AEVB and the reparameterization trick to
achieve posterior inference and parameter learn-
ing. Specifically, similar to (Srivastava and Sutton,
2017), we first draw a Gaussian random vector by
a reparameterization trick and then pass it through
a softmax function to parameterize the multino-
mial document topic distributions. Therefore, we
can replace α with the parameters of Gaussian
prior µ0,σ

2
0.

Thus, we can get the ELBO of our topic model-
ing component.

LT =Eq(zt|x)[log p(x|zt)]
−KL(q(zt|x)||p(zt|µ0,σ20I))

(2)

In Eq. 2, zt is inferred by the neural network in
the encoder. The inference process is detailed as
follows. Document x is fed into a two-layer per-
ceptron with ReLU as the activation function. The
transformation results of the MLP are then pro-
cessed by max-pooling, getting fixed-length rep-
resentation c of the document. Parameters of the
posterior µt and σ

2
t are obtained from two feed-

forward neural networks. Next, a Gaussian vari-
able is sampled by the reparameterization trick.
Finally, zt is obtained by passing p to the softmax
function. Our choices are specified as follows.

c = Pooling(MLP (x))

µt,σt = f1(c), f2(c)

p ∼ N (µt,σ2t I) ∈ RK

zt = fsm(W tp)

where f1, f2 denote two feedforward neural net-
works, W t is a trainable parameter, fsm denotes
softmax function.

Besides, KL(q(zt|x)||p(zt|µ0,σ20I)) in Eq. 2
is obtained by calculating the KL divergence be-
tween N (µt,σ2t I) and N (µ0,σ20I).

In general, during the training of topic mod-
els, a smaller vocabulary is built after eliminat-
ing some specific words (e.g., stop words, frequent
words, and rare words) in the corpus. This is a de-
noising preprocessing step that makes the model
more reliable. However, to make the execution
of the discriminator applicable, our model takes
all of the words in the document as input while
outputting the words in a smaller vocabulary as a
topic model does. Besides the reason above, this
setting can also be regarded as a regulation of the
model which strengthen the ability of modeling
documents.

Supposing we have a smaller vocabulary, the
document represented by this vocabulary is y =
{yi}mi=1, where m < n is the number of words
and yi ∈ Re is the word embedding of the word at
position i.

The topic modeling decoder reconstructs the
document from zt. We assume that the distribu-
tion of every topic is a Gaussian with identity vari-
ance matrix. Therefore, p(x|zt) of Eq. 2 is indeed
p(y|zt) and can be expanded as:

p(y|zt) =
m∏
i=1

p(yi|µti ,σ
2
tiI, zt)

∝
m∏
i=1

zTt exp (yi − µ)T (yi − µ)
(3)

To stabilize the training process, when calculate
the likelihood, we first normalize each yi and µk.

3.2 Sequence Modeling Component
Since the structural features are closely related to
the word sequence information, we construct a se-
quential VAE. By the sequential VAE, the encoder
infers the structural latent variable and the decoder
reconstructs texts via integrating the topic latent
variable and the structural latent variable. This
process is depicted by the bottom half of Fig. 1.
Since the semantic information is given by the
topic modeling component, the reconstruction loss
function makes the encoder of sequential VAE fo-
cusing on encoding the structural features. Let zs
denote the structural latent variable, the ELBO of
this component is in the following form:



5094

LS =Eq(zt|x)q(zs|x) [log p(x|zt, zs)]
−KL(q(zs|x)||p(zs))

(4)

We adopt a bi-directional GRU as the encoder.
The last hidden state of the encoder is used to infer
the parametersµs,σs of the Gaussian distribution.
Then, we get zs by the reparameterization trick.

h1,h2, ...,hn = BiGRU(x1,x2, ...,xn)

µs,σs = f3(hn), f4(hn)

zs ∼ N (µs,σ2sI)

where f3 and f4 are the feedforward neural net-
works. Further, we obtain the holistic latent code
z by concatenating zt and zs, i.e., z = [zt; zs].

We adopt a GRU as the decoder to reconstruct
document x = {xi}ni=1 and latent code z as its
initial state. So, the likelihood of the reconstructed
document can be derived by Eq. 5.

p(x|zt, zs) = p(x1|z)
n∏

i=1

p(xi|x1:i−1)

= p(x1|z)
n∏

i=1

p(xi|hi)
(5)

where hi denotes the i-th hidden state of the GRU
decoder.

3.3 Topic Encoder as a Discriminator

Although the holistic latent code contains seman-
tics and structure features, the decoder may not
fully leverage the semantic part of code. Besides
the reconstruction loss which drives the genera-
tor to produce realistic sentences, we introduce a
discriminator which enforces the generator to pro-
duce texts in a coherent topic with zt. Specif-
ically, we let the encoder in our topic modeling
component act as the discriminator. It is expected
that topic distributions inferred from the gener-
ated texts are similar to the topic distributions
of the original texts. However, if the outputs of
the sequence decoder are discrete, it is impossi-
ble to propagate gradients from the discriminator
through the discrete samples. We thus resort to a
Gumbel-Softmax distribution as an approximation
of the discrete samples.

In detail, in each step of the generative pro-
cess of texts, we get the distribution of one word
p(xi|zs, zt) = [π1, π2, ..., π|V |], and approximate
the samples from p(xi|zs, zt) by

ui =
exp (log(πi) + gi)/τ∑|V |

j=1 exp (log(πj) + gj)/τ
(6)

where gi and gj are samples from Gumbel −
Softmax(0, 1). As training proceeds, τ gets
close to 0, yielding the increasingly peaked dis-
tribution that finally emulate the discrete cases.
Thus, the i-th word generated by the decoder is

x̂i = u
TW v (7)

whereW v ∈ R|V |×e denotes word embedding.
We feed the above generative texts into the topic

modeling encoder and expect to get the maximum
likelihood of the original topic distribution zt. So
the loss function of the discriminator is specified
as follows.

LD = Ep(zs)p(zt) log q(zt|x̂) (8)

Combining three parts of loss functions, we can
get the loss function of the model as Eq. 9, where
λD=0.1.

L = LT + LS + λDLD
= Eq(zt|x)

[
log p(x|zt) + Eq(zs|x)(log p(x|zs,zt))

]
− λtKL(q(zt|x)||p(zt))− λsKL(q(zs|x)||p(zs))
+ λDEp(zs)p(zt) log q(zt|x̂)

(9)

We learn parameters of topic modeling and se-
quence modeling components alternatively in the
multitask learning setting. To avoid the latent vari-
able collapse problem, we add weights λt, λs to
KL related terms in the loss function and make the
weights increase slowly to 1 in training.

4 Experiments and Result Analyses

To evaluate the text generation ability, we do ex-
periments from four perspectives. First, we evalu-
ate the language modeling ability of the sequence
modeling component and the topic coherence of
the topic modeling component. Next, to evalu-
ate the effectiveness of the learned latent codes
of texts, we perform a semi-supervised classifi-
cation task. Finally, we transfer the question-
answering pairs on Yahoo dataset to different top-
ics and demonstrate generated texts.

4.1 Dataset Description & Experiment Setup
We conduct experiments on five benchmark text
datasets: APNEWS1, IMDB (Maas et al., 2011),

1https://www.ap.org/en-gb/



5095

Data #SM Voc #TM Voc #Training Docs #Development Docs #Test Docs #Avg Len

APNEWS 33k 6k 700k 27.4k 26.3k 21.4
IMDB 34k 8k 900k 200k 200k 22.5
BNC 40k 9k 800k 44k 52k 22.6

Yahoo 20k 8k 100k 10k 10k 75.3
Yelp15 20k 7k 100k 10k 10k 97.8

Table 1: Summary statistics for datasets. SM Voc and TM Voc stand for vocabularies of corpus in the sequence
modeling component and the topic modeling component, respectively.

BNC (BNC Consortium, 2007), Yahoo Answer
(Yahoo) and Yelp15. We randomly sample 100k
as training data and 10k as validation and testing
data, respectively. For all datasets, we first tok-
enize the texts using Stanford CoreNLP (Manning
et al., 2014). Then we lowercase all word tokens,
and filter words that occur less than 10 times. For
Yahoo and Yelp15, we truncate the vocabulary to
20k words for fast training. For the bag-of-words
input in the topic modeling component, we fur-
ther remove stop words, and exclude the top 0.1%
most frequent words and also words that appear in
less than 100 documents. Table 1 shows summary
statistics of all datasets.

We fix the max sequence length to 50 for the
texts in APNEWS, IMDB, BNC and 150 for Ya-
hoo and Yelp15. The 300-dimentional embed-
dings of words are shared by two components in
our model. For the topic modeling component, we
adopt a 2-layer MLP with 200 hidden units and
ReLU as its activation function. We set the size
of zt to 50. For the sequence modeling compo-
nent, we adopt a bidirectional single layer GRU
with 600 hidden units (300 in each direction) as
the encoder and a unidirectional GRU with 300
hidden units as the decoder. The size of zs is set to
20. We use a batch size of 32 and train the model
up to 40 epochs. Linear scheduling is used in the
KL annealing and the weight grows from 0 at the
beginning to 1 at 40k steps.

4.2 Sequence Modeling Evaluation

By the experiments on five datasets, we evaluate
the language modeling ability in terms of perplex-
ity (PPL). We compare several baselines includ-
ing models based on language model (i.e., LSTM
LM, LSTM+LDA, Topic-RNN, TDLM) and mod-
els based on VAE (i.e., LSTM VAE, VAE+HF,
TGVAE, DCNNVAE, DVAE). LSTM LM is a
plain language model implemented in LSTM.
LSTM+LDA concatenates the hidden states with
the topic distribution learned by a pre-trained
LDA. Different from ours, LDA in LSTM+LDA is

trained separately. Topic-RNN (Dieng et al., 2017)
learns an LDA with a language model jointly
and incorporates the topic distribution by a gate
mechanism. TDLM (Lau et al., 2017) incorpo-
rates a convolutional topic model and also lever-
ages the topic distribution in the same way that
LSTM+LDA does. LSTM VAE is a standard VAE
whose encoder and decoder are implemented by
two LSTMs, respectively. VAE+HF (Wang et al.,
2019) is a VAE with a mixture-of-Gaussians prior
with Householder Flow. TGVAE (Wang et al.,
2019) is a VAE guided by a Gaussian mixture
distribution as prior with a jointly-trained LDA.
DCNN-VAE (Yang et al., 2017) is a VAE using
dilated CNN as its decoder. DVAE (Xiao et al.,
2018) uses a Dirichlet latent variable to improve
VAE. Besides the model we propose, we also
evaluate our model without the discriminator (i.e.,
Ours w/o Dis).

The perplexity of VAE-based models is esti-
mated in ELBO approximately which is com-
prised of a reconstruction term and a KL term. Be-
sides the perplexity, we report the KL term in the
VAE based models. For our model, we report the
KL values of the sequence modeling component
and the topic modeling component in the first and
second rows, respectively.

For a fair comparison, the compared results are
picked from the models with 50 topics. The re-
sults are shown in Table 2. We find that our model
and our model without a discriminator occupy the
top-2 positions. We attribute the improvements to
the decoupling of semantic and structural features.
From the results, we also verify that the discrimi-
nator in our model can help in decreasing the per-
plexity. In addition, the KL values in the topic
modeling component is much larger than the se-
quential one. One possible reason is that the topic
information reveals much of the diversity of texts.

4.3 Topic Coherence Evaluation

Topic models are traditionally evaluated using per-
plexity. However, (Chang et al., 2009) show that



5096

APNEWS IMDB BNC Yahoo Yelp15
PPL KL PPL KL PPL KL PPL KL PPL KL

LSTM LM 64.13 - 72.14 - 102.89 - 66.2 - 42.6 -
LSTM+LDA 57.05 - 69.58 - 96.42 - 53.5 - 37.2 -
Topic-RNN 56.77 - 68.74 - 94.66 - - - - -

TDLM 53.00 - 63.67 - 87.42 - - - - -
LSTM VAE 75.89 1.78 86.16 2.78 105.10 0.13 65.6 0.4 45.5 0.5

VAE+HF 71.60 0.83 83.67 1.51 104.82 0.17 - - - -
TGVAE 48.73 3.55 57.11 5.02 87.86 4.57 - - - -

DCNN-VAE - - - - - - 63.9 10.0 41.1 7.6
DVAE - - - - - - 47.6 31.9 34.7 30.5

Ours w/o DIS 47.80 2.13 54.50 3.24 83.92 1.32 42.92 3.12 31.87 2.437.32 9.82 7.83 11.45 8.72

Ours 47.23 2.88 52.01 3.87 80.78 2.54 40.80 4.25 32.90 2.928.18 9.34 7.76 11.51 8.34

Table 2: Language modeling results in the terms of PPL and KL.

Model APNEWS IMDB BNC Yahoo Yelp15

LDA 0.125 0.084 0.106 0.148 0.087
TDLM 0.149 0.104 0.102 - -

Topic-RNN 0.134 0.103 0.102 - -
TGVAE 0.157 0.105 0.113 - -

Ours w/o DIS 0.170 0.121 0.115 0.182 0.114
Ours 0.171 0.120 0.114 0.182 0.113

Table 3: Topic coherence over the datasets in the term
of NPMI.

perplexity does not correlate with the coherence
of the generated topic. We adopt normalized PMI
(NPMI) to evaluate the topic coherence following
(Lau et al., 2017). Given the top-n words of a
topic, coherence is computed based on the sum of
pairwise NPMI scores between topic words. We
average topic coherence over the top 5/10/15/20
topic words. To aggregate topic coherence scores,
we calculate the mean coherence over topics. In
the experiments, the number of topics remains 50
among all baselines. From Table 3, we find that
the discriminator gives little improvement. It is
because that the role of the discriminator is to take
the topic distributions as the supervised signals to
improve the generation so that the sequence de-
coder can generate more topic relevant texts. That
is, the discriminator does not improve the topic
modeling component itself.

Besides topic coherence values, to understand
the topics concretely, we also provide top five
topic words from eight randomly chosen topics on
each dataset in the supplementary material.

4.4 Semi-supervised Classification

To evaluate whether latent codes incorporate text
features, we perform a semi-supervised classifica-
tion task and compare our model with the other
models. To make a comprehensive comparison,

Model 20NEWS Yahoo Yelp15

VAE 52.29 53.2* 27.2*
LDA 55.38 56.72 30.44

DVAE - 57.6* 42.4*
TDLM 60.6* - -

Ours w/o DIS (topic) 59.31 60.51 35.11
Ours w/o DIS(seq) 28.25 31.42 34.39

Ours w/o DIS 61.28 63.98 45.34
Ours (topic) 59.06 61.34 34.36
Ours (seq) 30.05 30.31 35.37

Ours 61.47 64.75 46.03
* are from the original papers.

Table 4: Test split accuracy of classifiers trained with
learned representations.

we use Yahoo, Yelp15 as well as 20NEWS for text
classification. Here, 20NEWS is a collection of
forum-like messages from 20 news-groups cate-
gories.

For any model to be evaluated, we first train it
by the training documents of the dataset, and then
executing the well-trained model to obtain latent
codes of all documents in the dataset. Next, we
sample 2,000 documents from the training data
and train a 2-layer softmax classifier using these
documents and their category labels. The accuracy
on the testing set of documents is shown in Table
4. Since in our TATGM the latent code comprises
two latent variables, we explore the accuracy on
the two latent variables solely and collectively.

As shown in Table 4, our model which com-
bines the two latent variables achieves the high-
est accuracy. For 20NEWS and Yahoo, the model
only using the topic latent variable is better than
that only using the structural latent variable. Since
these two datasets are labeled by the topics, the
combination improves little. For Yelp15 dataset,
the performances of our two latent variables are
similar whereas the combination improves up to



5097

Topic Question Answer

Society&Culture what is the average age of the homeless
population?

i think they are younger then they look.
i ’d guess 30 years old.

Science&Mathematics what is the meaning of life in the world? i know that i can find out about it.
Health what is the average weight to lose of a

week?
i think it varies to lose weight.

Education&Reference what is the formula for energy? i have about 10 points for asking ques-
tions.

Sports which team has the most winning world
cup?

i think brazil will win the world cup fi-
nals. i think they will win.

computers what is the problem with the yahoo
messenger?

i think yahoo messenger is not working
on yahoo messenger.

Business&Finance what is the minimum wage for a tax in-
come?

i do n’t know if they are illegal to pay
taxes.

Entertainment&Music what is the way of listen songs? i think listen a song on internet.
Family&Relationships what is the average age of have sex? i am 14 years old and i ’m not sure what

age is it?
Politics&Government what states is the president of the united

states?
i do n’t know if it is illegal to be illegal.

Table 5: A topic transfer example of one question-answering pair on Yahoo dataset.

15%. The reason may be the fact that the Yelp15
dataset is labeled by the sentiment which is deter-
mined by not only single terms but also n-grams.
For example, good and not good represents oppo-
site sentiments. Our sequence modeling compo-
nent can capture such kind of features more accu-
rately.

4.5 Topic Transfer between
Question-answering Pairs

In our model, each dimension of the latent vari-
ables in the topic modeling component corre-
sponds to a topic, therefore we can manipulate the
variables manually to verify whether the genera-
tion can express a given topic while remaining the
same structure. Specifically, given a document and
its latent code, we change the topic part of the code
and keep the structural part of the code unchanged.
Then we can check the text the decoder generated
from the whole latent code.

We conduct the experiments on the Yahoo
dataset where each item contains one question-
answering pair and one label corresponds to its
topic. At first, we treat each question as a sin-
gle sentence to check whether the generated texts
satisfy our assumption. From the second column
of Table 5, we find that the generation can ex-
press a different topic while maintaining the orig-
inal question structure.

We further try to transfer the question-
answering pair from its original topic to target top-
ics. We find that the model not only transfers topic
of two sentences but also produces reasonable
question-answering pairs, i.e., the new answer is

meaningful to the new question.This is helpful for
the automatic question-answering scenarios. Ta-
ble 5 shows an example of topic transfer. The
first row lists the original question-answering pair
which are in the topic of Society&Culture. The
rest rows show the generated question-answering
pairs when we change the original topic to a tar-
get topic while keeping the structure latent vari-
able unmodified. Specifically, we modify the orig-
inal topic distribution to a new distribution whose
dimension of the target topic is 1 and others are
0s. From the results, we can observe that while we
change the topic to another, the generated ques-
tions remain almost the same structure as the orig-
inal ones. Moreover, the generated answers are
also transferred to the target topic and they to-
gether with the generated questions compose rea-
sonable question-answering pairs. More examples
are shown in the supplementary material.

5 Conclusion

In the paper, we present the text generation model
TATGM. The model can learn semantics and
structural features simultaneously. Moreover, the
model employs a discriminator to ensure that the
generated text is more coherent with the given
topic information. Experimental results show that
our model has a better text modeling ability than
several state-of-the-art methods and learns dis-
entangled latent representations for texts which
shows the superiority in a classification task.
Specifically, our model can generate meaningful
question-answering pairs, which provides an alter-
native transfer learning way and helps to broaden



5098

the knowledge in other fields.

Acknowledgments

This work has been supported by National Key
R&D Program of China (No. 2017YFC0803300).

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

BNC BNC Consortium. 2007. The british national cor-
pus, version 3 (bnc xml edition). In Distributed by
Bodleian Libraries, University of Oxford, on behalf
of the BNC Consortium.

Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M. Dai, Rafal Józefowicz, and Samy Ben-
gio. 2016. Generating sentences from a continuous
space. In Proceedings of the 20th SIGNLL Confer-
ence on Computational Natural Language Learning,
CoNLL 2016, Berlin, Germany, August 11-12, 2016,
pages 10–21.

Jonathan Chang, Jordan L. Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
Advances in Neural Information Processing Systems
22: 23rd Annual Conference on Neural Information
Processing Systems 2009. Proceedings of a meet-
ing held 7-10 December 2009, Vancouver, British
Columbia, Canada., pages 288–296.

Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015.
Gaussian LDA for topic models with word embed-
dings. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing of the Asian Federation of
Natural Language Processing, ACL 2015, July 26-
31, 2015, Beijing, China, Volume 1: Long Papers,
pages 795–804.

Adji B. Dieng, Chong Wang, Jianfeng Gao, and
John W. Paisley. 2017. Topicrnn: A recurrent neu-
ral network with long-range semantic dependency.
In 5th International Conference on Learning Repre-
sentations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings.

Karol Gregor, Ivo Danihelka, Alex Graves,
Danilo Jimenez Rezende, and Daan Wierstra.
2015. DRAW: A recurrent neural network for
image generation. In Proceedings of the 32nd
International Conference on Machine Learning,
ICML 2015, Lille, France, 6-11 July 2015, pages
1462–1471.

Pengfei Hu, Wenju Liu, Wei Jiang, and Zhanlei Yang.
2012. Latent topic model based on gaussian-lda for
audio retrieval. In Chinese Conference on Pattern
Recognition, pages 556–563. Springer.

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P. Xing. 2017. Toward con-
trolled generation of text. In Proceedings of the
34th International Conference on Machine Learn-
ing, ICML 2017, Sydney, NSW, Australia, 6-11 Au-
gust 2017, pages 1587–1596.

Diederik P. Kingma and Max Welling. 2014. Auto-
encoding variational bayes. In 2nd International
Conference on Learning Representations, ICLR
2014, Banff, AB, Canada, April 14-16, 2014, Con-
ference Track Proceedings.

Jey Han Lau, Timothy Baldwin, and Trevor Cohn.
2017. Topically driven neural language model.
arXiv preprint arXiv:1704.08012.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In The 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, Proceedings of the Conference, 19-24
June, 2011, Portland, Oregon, USA, pages 142–150.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 55–60, Bal-
timore, Maryland. Association for Computational
Linguistics.

Yishu Miao, Edward Grefenstette, and Phil Blunsom.
2017. Discovering discrete latent topics with neural
variational inference. In Proceedings of the 34th In-
ternational Conference on Machine Learning, ICML
2017, Sydney, NSW, Australia, 6-11 August 2017,
pages 2410–2419.

Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neu-
ral variational inference for text processing. In In-
ternational conference on machine learning, pages
1727–1736.

Tomas Mikolov and Geoffrey Zweig. 2012. Context
dependent recurrent neural network language model.
In 2012 IEEE Spoken Language Technology Work-
shop (SLT), Miami, FL, USA, December 2-5, 2012,
pages 234–239.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2015, Lisbon, Portugal,
September 17-21, 2015, pages 379–389.

http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
http://aclweb.org/anthology/K/K16/K16-1002.pdf
http://aclweb.org/anthology/K/K16/K16-1002.pdf
http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models
http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models
http://aclweb.org/anthology/P/P15/P15-1077.pdf
http://aclweb.org/anthology/P/P15/P15-1077.pdf
https://openreview.net/forum?id=rJbbOLcex
https://openreview.net/forum?id=rJbbOLcex
http://jmlr.org/proceedings/papers/v37/gregor15.html
http://jmlr.org/proceedings/papers/v37/gregor15.html
http://proceedings.mlr.press/v70/hu17e.html
http://proceedings.mlr.press/v70/hu17e.html
http://arxiv.org/abs/1312.6114
http://arxiv.org/abs/1312.6114
https://arxiv.org/abs/1704.08012
http://www.aclweb.org/anthology/P11-1015
http://www.aclweb.org/anthology/P11-1015
https://doi.org/10.3115/v1/P14-5010
https://doi.org/10.3115/v1/P14-5010
http://proceedings.mlr.press/v70/miao17a.html
http://proceedings.mlr.press/v70/miao17a.html
http://jmlr.org/proceedings/papers/v48/miao16.html
http://jmlr.org/proceedings/papers/v48/miao16.html
https://doi.org/10.1109/SLT.2012.6424228
https://doi.org/10.1109/SLT.2012.6424228
http://aclweb.org/anthology/D/D15/D15-1044.pdf
http://aclweb.org/anthology/D/D15/D15-1044.pdf


5099

Stanislau Semeniuta, Aliaksei Severyn, and Erhardt
Barth. 2017. A hybrid convolutional variational au-
toencoder for text generation. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2017, Copenhagen,
Denmark, September 9-11, 2017, pages 627–637.

Akash Srivastava and Charles A. Sutton. 2017. Au-
toencoding variational inference for topic models.
In 5th International Conference on Learning Repre-
sentations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings.

Youzhi Tian, Zhiting Hu, and Zhou Yu. 2018. Struc-
tured content preservation for unsupervised text
style transfer. CoRR, abs/1810.06526.

Wenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen,
Jiaji Huang, Wei Ping, Sanjeev Satheesh, and
Lawrence Carin. 2018. Topic compositional neu-
ral language model. In International Conference on
Artificial Intelligence and Statistics, AISTATS 2018,
9-11 April 2018, Playa Blanca, Lanzarote, Canary
Islands, Spain, pages 356–365.

Wenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang,
Guoyin Wang, Dinghan Shen, Changyou Chen,
and Lawrence Carin. 2019. Topic-guided varia-
tional autoencoders for text generation. CoRR,
abs/1903.07137.

Yijun Xiao, Tiancheng Zhao, and William Yang Wang.
2018. Dirichlet variational autoencoder for text
modeling. CoRR, abs/1811.00135.

Zichao Yang, Zhiting Hu, Chris Dyer, Eric P. Xing, and
Taylor Berg-Kirkpatrick. 2018. Unsupervised text
style transfer using language models as discrimina-
tors. In Advances in Neural Information Processing
Systems 31: Annual Conference on Neural Informa-
tion Processing Systems 2018, NeurIPS 2018, 3-8
December 2018, Montréal, Canada., pages 7298–
7309.

Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and
Taylor Berg-Kirkpatrick. 2017. Improved varia-
tional autoencoders for text modeling using dilated
convolutions. In Proceedings of the 34th Inter-
national Conference on Machine Learning, ICML
2017, Sydney, NSW, Australia, 6-11 August 2017,
pages 3881–3890.

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. Seqgan: Sequence generative adversarial
nets with policy gradient. In Proceedings of the
Thirty-First AAAI Conference on Artificial Intelli-
gence, February 4-9, 2017, San Francisco, Califor-
nia, USA., pages 2852–2858.

Biao Zhang, Deyi Xiong, Jinsong Su, Hong Duan, and
Min Zhang. 2016. Variational neural machine trans-
lation. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2016, Austin, Texas, USA, November
1-4, 2016, pages 521–530.

Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan-
der J. Smola, and Le Song. 2018. Variational
reasoning for question answering with knowledge
graph. In Proceedings of the Thirty-Second AAAI
Conference on Artificial Intelligence, (AAAI-18),
New Orleans, Louisiana, USA, February 2-7, 2018,
pages 6069–6076.

Tiancheng Zhao, Kyusong Lee, and Maxine Eskénazi.
2018. Unsupervised discrete sentence representa-
tion learning for interpretable neural dialog gener-
ation. In Proceedings of the 56th Annual Meeting of
the Association for Computational Linguistics, ACL
2018, Melbourne, Australia, July 15-20, 2018, Vol-
ume 1: Long Papers, pages 1098–1107.

Tiancheng Zhao, Ran Zhao, and Maxine Eskénazi.
2017. Learning discourse-level diversity for neural
dialog models using conditional variational autoen-
coders. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2017, Vancouver, Canada, July 30 - August
4, Volume 1: Long Papers, pages 654–664.

https://aclanthology.info/papers/D17-1066/d17-1066
https://aclanthology.info/papers/D17-1066/d17-1066
https://openreview.net/forum?id=BybtVK9lg
https://openreview.net/forum?id=BybtVK9lg
http://arxiv.org/abs/1810.06526
http://arxiv.org/abs/1810.06526
http://arxiv.org/abs/1810.06526
http://proceedings.mlr.press/v84/wang18a.html
http://proceedings.mlr.press/v84/wang18a.html
http://arxiv.org/abs/1903.07137
http://arxiv.org/abs/1903.07137
http://arxiv.org/abs/1811.00135
http://arxiv.org/abs/1811.00135
http://papers.nips.cc/paper/7959-unsupervised-text-style-transfer-using-language-models-as-discriminators
http://papers.nips.cc/paper/7959-unsupervised-text-style-transfer-using-language-models-as-discriminators
http://papers.nips.cc/paper/7959-unsupervised-text-style-transfer-using-language-models-as-discriminators
http://proceedings.mlr.press/v70/yang17d.html
http://proceedings.mlr.press/v70/yang17d.html
http://proceedings.mlr.press/v70/yang17d.html
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14344
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14344
http://aclweb.org/anthology/D/D16/D16-1050.pdf
http://aclweb.org/anthology/D/D16/D16-1050.pdf
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16983
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16983
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16983
https://aclanthology.info/papers/P18-1101/p18-1101
https://aclanthology.info/papers/P18-1101/p18-1101
https://aclanthology.info/papers/P18-1101/p18-1101
https://doi.org/10.18653/v1/P17-1061
https://doi.org/10.18653/v1/P17-1061
https://doi.org/10.18653/v1/P17-1061

