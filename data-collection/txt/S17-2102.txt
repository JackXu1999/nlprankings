



















































NNEMBs at SemEval-2017 Task 4: Neural Twitter Sentiment Classification: a Simple Ensemble Method with Different Embeddings


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 621–625,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

NNEMBs at SemEval-2017 Task 4: Neural Twitter Sentiment
Classification: a Simple Ensemble Method with Different Embeddings

Yichun Yin
Peking University

yichunyin@pku.edu.cn

Yangqiu Song
HKUST

yqsong@cse.ust.hk

Ming Zhang
Peking University

mzhang cs@pku.edu.cn

Abstract

Recently, neural twitter sentiment classi-
fication has become one of state-of-the-
arts, which requires less feature engineer-
ing work compared with traditional meth-
ods. In this paper, we propose a simple
and effective ensemble method to further
boost the performances of neural mod-
els. We collect several word embedding
sets which are publicly released (often are
learned on different corpus) or constructed
by running Skip-gram on released large-
scale corpus. We make an assumption that
different word embeddings cover differ-
ent words and encode different semantic
knowledge, thus using them together can
improve the generalizations and perfor-
mances of neural models. In the SemEval
2017, our method ranks 1st in Accuracy,
5th in AverageR. Meanwhile, the addi-
tional comparisons demonstrate the supe-
riority of our model over these ones based
on only one word embedding set. We re-
lease our code 1 for the method replicabil-
ity.

1 Introduction

Twitter sentiment classification has attracted a lot
of attention (Dong et al., 2015; Nakov et al., 2016;
Rosenthal et al., 2017), which aims to classify a
tweet into three sentiment categories: negative,
neutral, and positive. Tweet text has several fea-
tures: written by the informal language, hash-tags
and emoticons indicate sentiments, and sometimes
is sarcasm, which make decisions of tweet senti-
ment hard for machines. With releases of anno-
tated datasets, more researchers prefer to use the

1https://github.com/zwjyyc/NNEMBs

twitter sentiment classification as one testbed to
evaluate their proposed models.

Traditional methods (Mohammad et al., 2013)
for twitter sentiment classification use a variety of
hand-crafted features including surface-form, se-
mantic and sentiment lexicons. The performances
of these methods often depend on the quality of
feature engineering work, and building a state-of-
the-art system is difficult for novices. Moreover,
these designed features are presented by the one-
hot representation which cannot capture the se-
mantic relativeness of different features and pro-
poses a problem of feature sparsity. To address
this, Tang et al. (2014) induced sentiment-specific
low-dimensional, real-valued embedding features
for twitter classification, which encode both se-
mantics and sentiments of words. In the exper-
iments, the embedding features and hand-crafted
features obtain similar results, and also they are
complementary for each other in the system. With
the developments of neural networks in natural
language processing, neural sentiment classifica-
tion (Severyn and Moschitti, 2015; Deriu et al.,
2016) has attracted a lot of attention recently and
become the state-of-the-arts. These methods first
learn word embeddings from large-scale twitter
corpus, then tune neural networks by the tweets
which have distant labels, and finally fine-tune the
proposed models by the annotated datasets.

Learning word embeddings using in-domain
data is an effective way to boost model perfor-
mances (Mikolov et al., 2013; Yin et al., 2016).
However, collecting large-scale twitter corpus is
often time-consuming. In this paper, we use the
different word embedding sets to boost the per-
formances of our neural networks, which only in-
clude released different word embeddings sets and
the word embedding set derived from the released
Yelp large-scale datasets by Skip-gram (Mikolov
et al., 2013). A simple and effective ensemble

621



Figure 1: Overview of our method.

method is proposed, which takes different word
embedding sets as input to train neural networks
and predicts labels of testing tweets by merging
all output of neural models. Our ensemble method
show its effectiveness in SemEval 2017, though
most of used word embedding sets are not learned
from twitter corpus, which can be explained that
different embedding sets has different vocabular-
ies and encode different parts of sentiment knowl-
edge. Moreover, we conduct additional experi-
ments to analyze our model.

2 Method

In this section, we describe the details of our
method, which is illustrated in Figure 1. We feed
different word embedding sets into neural net-
works and train these neural networks separately.
When predicting the labels of tweets in testing set,
we sum label probabilities of all neural network to
make final decisions.

2.1 Neural Network
We have many choices of neural networks (e.g.,
LSTM, RNN and GRU) for our method, here we
consider RCNN (Lei et al., 2016) in our method.
RCNN has non-consecutive convolution and adap-
tive gated decay, which aims to capture longer-
range, non-consecutive patterns in a weighted
manner.

Given a sequence of words which are denoted
as {xi}li=1, the corresponding word embeddings
{xi}li=1 are derived using the embedding matrix
E. Then, RCNN obtains their corresponding hid-
den vectors {hi}li=1 using the convolution opera-
tion and gating mechanism. After obtaining hid-
den vectors, RCNN uses a pooling operation to
get fixed-sized vector presentation, which is fed

into softmax layer to finish the prediction. The n-
gram convolution operation and gating decay are
described as follows:

λt = σ(W
λxt + U

λht−1 + b
λ),

c
(1)
t = λt � c(1)t−1 + (1− λt)� (W1xt),

c
(2)
t = λt � c(2)t−1 + (1− λt)� (c(1)t−1 + W2xt),
· · · ,

c
(n)
t = λt � c(n)t−1 + (1− λt)� (c(n−1)t−1 + Wnxt),
ht = tanh(c

(n)
t + b),

where Wλ, Uλ, bλ, b and W∗ are learnable
parameters, σ is sigmoid function which rescales
the value into (0, 1), � is dot product, λt is gat-
ing value determining how much information of
xt and previous patterns is added into the hidden
vector, c(i)t refer to the vector for accumulated pre-
vious patterns which are ended with xt include i
consecutive tokens. When λt = 0, the convolu-
tion becomes a standard n-gram convolution.

We also can build a deep RCNN by adding sev-
eral convolution layer on top of hidden vectors
derived from the bottom convolution layer. Here
we consider the RCNN with d convolution layers,
which outputs {hdi }li=1. Then, a last pooling oper-
ation is conducted on hidden vectors to obtain text
representation r. Finally, text representation is fed
into a softmax layer. The softmax layer outputs
the probability distribution over |Y| categories for
the distributed representation, which is defined as:

p(r) = softmax(Wclassk r).

The cross-entropy objective function is used to
optimize the RCNN model.

2.2 Prediction

We learn different RCNN models with differ-
ent embedding sets as input. Formally, we
have s embedding sets which are denoted as
{E1,E2, · · · ,Es}, and feed them into s RCNN
models, then learn RCNN models separately. We
predict sentiment label of testing tweet based on
these learned RCNN models, which are described
by following functions:

622



Sets Corpus Scale Algorithm Dimension Source Vocab
gloveG General 840B GloVec 300 R 2.2M
gloveT Twitter 27B GloVec 200 R 1.2M
word2vecGN Google News 100B Word2Vec 300 R 3.0M
word2vecY Yelp Reviews 0.3B Word2Vec 300 S 0.2M
Ensemble - - - - - 5.4M

Table 1: Statistics of the embedding sets. R means the embedding set is publicly released and S means
the embedding set is self-contained. GloVec (Mikolov et al., 2013) and Word2Vec (Pennington et al.,
2014) are most popular embedding algorithms. Scale means the size of tokens in corpus, M and B refer
to million and billion respectively. The embedding set word2vecY are trained by Word2Vec with default
settings and Yelp reviews are available at https://www.yelp.com/dataset challenge.

Dataset #num #category ratio
Previous SemEvals 50032 1.5/4.7/3.8
SemEval 2017 Test 12284 3.2/4.8/2.0

Table 2: Statistics of datasets.

p1 = RCNN1({xi}li=1,E1),
p2 = RCNN2({xi}li=1,E2),
. . . ,

ps = RCNNs({xi}li=1,Es),
p
′
=

∑
1≤i≤s

pi.

y = argmax1≤i≤|Y|p
′
i,

where y is the predicted label.

3 Experiment

3.1 Datasets and Settings

We use 4 embedding sets which are described in
Table 1. Meanwhile, we crawl and merge all an-
notated datasets of previous SemEvals, and split
them into training, development, and testing sets
with ratio 8:1:1, which are shown in Table 2 to-
gether with testing set of SemEval 2017. From the
table, we can see that testing set of SemEval 2017
has big differences on the category ratio (nega-
tive:neutral:positive), compared with the previous
SemEval datasets.

For the model settings, all RCNN models have
same configurations but different word embedding
sets. We set dimensions of hidden vectors to 250
and depths d to 2. To avoid model over-fitting,
we use dropout and regularization as follows: (1)
the regularization parameter is set to 1e-5; (2) the

dropout rate is set to 0.3, which is applied in the fi-
nal text representation. All parameters are learned
by Adam optimizer (Kingma and Ba, 2014) with
the learning rate 0.001. Note that, all word embed-
ding sets are fixed when training. All models are
tuned by the development set in Training.

3.2 Results and Analysis

In this section, we first report the results on
datasets of previous SemEvals, which are de-
scribed in Table 3. Then, we report the perfor-
mances of our method on SemEval 2017 in Ta-
ble 4.

From the Table 3, we observe that gloveT per-
forms worst though it is trained on in-domain
twitter dataset and the word2vecY performs best
though it is derived from yelp reviews. As far
as we known, Yelp data is constructed by care-
fully filtering and is high-quality. Thus, we can
include that the quality of corpus is also impor-
tant as the size of corpus and domain in twitter
sentiment classification. Additionally, we can in-
fer that word2vecGN outperforms others in recall
of negative category, word2vecY performs best in
recall of neutral category, and gloveT is best in re-
call of positive category. Different embedding sets
propose different characteristics. Additionally, the
ensemble method obtains a significant improve-
ment of 4%.

In the Table 4, we compare our method with
best and median systems in SemEval 2017, and
report the results of individual embedding sets.
Our method outperforms other systems in accu-
racy, but performs worse in R Average, especially
in R Negative. Compared with the median sys-
tem, our method has improvements of about 5%
in both accuracy and R Average. Different from
the results in Table 3, the word2vecY performs

623



Embeddings Accuracy R Negative R Neutral R Positive R Average
gloveG 70.6 66.3 66.4 77.7 70.1
gloveT 68.3 66.8 61.2 77.9 68.7
word2vecGN 70.5 70.2 68.3 73.5 70.7
word2vecY 72.2 65.0 71.3 76.2 70.8
Ensemble 74.6 72.1 71.2 80.0 74.5

Table 3: Results on datasets of previous SemEval. R * means recall value.

Embeddings Accuracy R Negative R Neutral R Positive R Average
Best system 65.1 82.9 51.2 70.2 68.1
Median system 61.6 53.1 65.0 67.4 61.8
gloveG 62.9 63.0 61.3 66.7 63.7
gloveT 63.7 70.5 57.4 68.2 65.4
word2vecGN 62.9 68.4 60.0 60.8 63.1
word2vecY 61.6 59.1 63.8 60.5 61.1
Our 66.4 69.8 64.0 66.8 66.9

Table 4: Results on SemEval 2017. The median system is the system of rank 19th among 38 teams.

worse among these embedding sets, while the
gloveT obtains best performances. Additionally,
we can observe that gloveT performs best both in
R Negative and R Positive, and word2vecY per-
forms best in R Neutral. Compared with the em-
bedding baselines, our ensemble method obtains
improvements of 2.7% and 1.5% in accuracy and
R Average respectively, which demonstrates the
effectiveness of the proposed method.

3.3 Error Analysis

In this section, we analyze the incorrect predic-
tions of our system in SemEval 2017.

We summarize four kinds of errors in our sys-
tem. The first one is that some decisions need do-
main knowledge, which our method only can learn
from the labeled datasets. The instances are as fol-
lows:

Messi’s 100 international goals for Barcelona
#fcblive https://t.co/fMkglvusL1 [via @thereis-
agenius]. Predicted label: neutral, golden label:
positive

#Trudeau gives your cash to #Terrorist
#Hamas-influenced group - #UNRWA - @Can-
diceMalcolm https://t.co/5i5o2qwRWl Predicted
label: neutral, golden label: negative

Messis 9 goals in CL are more than 20 of the 32
teams in the competition have scored in total, and
hes tied with five other sides #fcblive Predicted la-
bel: neutral, golden label: positive

The second one is emoticons in tweet, as most
of word embedding sets do not include emoticon
embeddings and emoticons are always with senti-

Figure 2: Emoticon instances.

ments. The instances are described in Figure 2
The third one is that sentiments are not consis-

tent in sentences. For example, the first half part
is positive, while the second half part is negative,
in this case, our system would predict ’positive’ or
’negative’, the golden label is neutral.

@jimmyfallon 1. Emily 2. Michel 3. Kirk 4. TJ.
Love the quirky ones and Emily coz she’s such a
BIATCH! #gilmoregirlstop4. predicted label posi-
tive, golden label: neutral

The fourth one is the sarcasm, such as:
#Hamas leader: #Trump may be a #Jew
https://t.co/jGFZTvj2pF. predicted label positive,
golden label: negative

4 Conclusion

We propose a simple and effective ensemble
method to boost the neural twitter sentiment clas-
sification. By using different embedding sets, the
system can cover more words and encode more
sentiment information. The results on datasets of
previous SemEval and SemEval 2017 show the ef-
fectiveness of our method. Moreover, error anal-
ysis is conducted to propose the main challenges
for our method. We release our code for system
duplicability.

624



References
Jan Deriu, Maurice Gonzenbach, Fatih Uzdilli,

Aurélien Lucchi, Valeria De Luca, and Mar-
tin Jaggi. 2016. Swisscheese at semeval-2016
task 4: Sentiment classification using an en-
semble of convolutional neural networks with
distant supervision. In Proceedings of the
10th International Workshop on Semantic Evalu-
ation, SemEval@NAACL-HLT 2016, San Diego,
CA, USA, June 16-17, 2016. pages 1124–1128.
http://aclweb.org/anthology/S/S16/S16-1173.pdf.

Li Dong, Furu Wei, Yichun Yin, Ming Zhou, and
Ke Xu. 2015. Splusplus: A feature-rich two-stage
classifier for sentiment analysis of tweets. SemEval-
2015 page 515.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR
abs/1412.6980. http://arxiv.org/abs/1412.6980.

Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi S.
Jaakkola, Kateryna Tymoshenko, Alessandro Mos-
chitti, and Lluı́s Màrquez. 2016. Semi-supervised
question retrieval with gated convolutions. In
NAACL HLT 2016, The 2016 Conference of
the North American Chapter of the Associa-
tion for Computational Linguistics: Human
Language Technologies, San Diego Califor-
nia, USA, June 12-17, 2016. pages 1279–1289.
http://aclweb.org/anthology/N/N16/N16-1153.pdf.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed
representations of words and phrases and their com-
positionality. In Advances in Neural Information
Processing Systems 26: 27th Annual Conference
on Neural Information Processing Systems 2013.
Proceedings of a meeting held December 5-8,
2013, Lake Tahoe, Nevada, United States.. pages
3111–3119. http://papers.nips.cc/paper/5021-
distributed-representations-of-words-and-phrases-
and-their-compositionality.

Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation, SemEval@NAACL-HLT 2013, Atlanta,
Georgia, USA, June 14-15, 2013. pages 321–327.
http://aclweb.org/anthology/S/S13/S13-2053.pdf.

Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio
Sebastiani, and Veselin Stoyanov. 2016. Semeval-
2016 task 4: Sentiment analysis in twitter. In Pro-
ceedings of the 10th International Workshop on Se-
mantic Evaluation, SemEval@NAACL-HLT 2016,
San Diego, CA, USA, June 16-17, 2016. pages 1–18.
http://aclweb.org/anthology/S/S16/S16-1001.pdf.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,

2014, Doha, Qatar, A meeting of SIGDAT, a Spe-
cial Interest Group of the ACL. pages 1532–1543.
http://aclweb.org/anthology/D/D14/D14-1162.pdf.

Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017.
SemEval-2017 task 4: Sentiment analysis in Twit-
ter. In Proceedings of the 11th International Work-
shop on Semantic Evaluation. Association for Com-
putational Linguistics, Vancouver, Canada, SemEval
’17.

Aliaksei Severyn and Alessandro Moschitti. 2015.
UNITN: training deep convolutional neural network
for twitter sentiment classification. In Proceed-
ings of the 9th International Workshop on Seman-
tic Evaluation, SemEval@NAACL-HLT 2015, Den-
ver, Colorado, USA, June 4-5, 2015. pages 464–469.
http://aclweb.org/anthology/S/S15/S15-2079.pdf.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers). Association for Com-
putational Linguistics, Baltimore, Maryland, pages
1555–1565. http://www.aclweb.org/anthology/P14-
1146.

Yichun Yin, Furu Wei, Li Dong, Kaimeng Xu,
Ming Zhang, and Ming Zhou. 2016. Unsu-
pervised word and dependency path embeddings
for aspect term extraction. In Proceedings of
the Twenty-Fifth International Joint Conference
on Artificial Intelligence, IJCAI 2016, New York,
NY, USA, 9-15 July 2016. pages 2979–2985.
http://www.ijcai.org/Abstract/16/423.

625


