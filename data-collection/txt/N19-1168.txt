




































Improving Human Text Comprehension through Semi-Markov CRF-based Neural Section Title Generation


Proceedings of NAACL-HLT 2019, pages 1677–1688
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1677

Improving Human Text Comprehension through Semi-Markov
CRF-based Neural Section Title Generation

Sebastian Gehrmann1,2, Steven Layne2,3, Franck Dernoncourt2
1Harvard SEAS, 2Adobe Research, 3University of Illinois Urbana Champaign

gehrmann@seas.harvard.edu, solayne2@illinois.edu,
franck.dernoncourt@adobe.com

Abstract
Titles of short sections within long documents
support readers by guiding their focus towards
relevant passages and by providing anchor-
points that help to understand the progression
of the document. The positive effects of sec-
tion titles are even more pronounced when
measured on readers with less developed read-
ing abilities, for example in communities with
limited labeled text resources. We, therefore,
aim to develop techniques to generate sec-
tion titles in low-resource environments. In
particular, we present an extractive pipeline
for section title generation by first selecting
the most salient sentence and then applying
deletion-based compression. Our compression
approach is based on a Semi-Markov Con-
ditional Random Field that leverages unsu-
pervised word-representations such as ELMo
or BERT, eliminating the need for a com-
plex encoder-decoder architecture. The results
show that this approach leads to competitive
performance with sequence-to-sequence mod-
els with high resources, while strongly outper-
forming it with low resources. In a human-
subjects study across subjects with varying
reading abilities, we find that our section titles
improve the speed of completing comprehen-
sion tasks while retaining similar accuracy.

1 Introduction

Section titles in long documents that explain the
content of the section improve the recall of con-
tent (Dooling and Lachman, 1971; Smith and
Swinney, 1992) while simultaneously increasing
the reading speed (Bransford and Johnson, 1972).
Additionally, they can provide a context to al-
low ambiguous words to be understood more eas-
ily (Wiley and Rayner, 2000) and to better un-
derstand the overall text (Kintsch and Van Dijk,
1978). However, most documents do not include
titles for short segments or only provide a very ab-
stract description of their topics, e.g. “Geography”

When another old cave is discovered in the south of France, it is not usually 
news. Rather, it is an ordinary event. Such discoveries are so frequent these days 
that hardly anybody pays heed to them. However, when the Lascaux cave com-
plex was discovered in 1940, the world was amazed. Painted directly on its 
walls were hundreds of scenes showing how people lived thousands of years 
ago. The scenes show people hunting animals, such as bison or wild cats. Other 
images depict birds and, most noticeably, horses, which appear in more than 
300 wall images, by far outnumbering all other animals. 

Early artists drawing these animals accomplished a monumental and difficult 
task. They did not limit themselves to the easily accessible walls but carried 
their painting materials to spaces that required climbing steep walls or crawling 
into narrow passages in the Lascaux complex. Unfortunately, the paintings have 
been exposed to the destructive action of water and temperature changes, which 
easily wear the images away. Because the Lascaux caves have many entrances, 
air movement has also damaged the images inside. Although they are not out in 
the open air, where natural light would have destroyed them long ago, many of 
the images have deteriorated and are barely recognizable. 
 
To prevent further damage, the site was closed to tourists in 1963, 23 years after 
it was discovered. 

Lascaux cave complex 
discovered

Paintings exposed 
to destructive action

Site closed to tourists

Figure 1: Output example from our model (left) for an
out-of-domain text (right).

or “Introduction”. This makes them more inac-
cessible especially to readers with less developed
reading skills, who have trouble identifying rele-
vant information in text (Englert et al., 2009) and
therefore more strongly rely on text-markups (Bell
and Limber, 2009).

This paper introduces an approach to generate
section titles by extractively compressing the most
salient sentence of each paragraph, as shown in
Figure 1. While there has been much recent work
on abstractive headline generation from a single
sentence (Nallapati et al., 2016), abstractive mod-
els require larger datasets, which are not avail-
able in many domains and languages. Moreover,
abstractive text-generation models tend to gener-
ate incorrect information for complex inputs (See
et al., 2017; Wiseman et al., 2017). Misleading
headlines can have unintended effects, affecting
readers’ memory and reasoning skills and even
bias them (Ecker et al., 2014; Chesney et al.,
2017). Especially in times of sensationalism and
click-baiting, the unguided generation of titles
can be considered unethical and we thus focus
on the investigation of deletion-only approaches
to title-generation. While this restricts this ap-
proach to languages that, similar to English, do not



1678

lose grammatical soundness when clauses are re-
moved, this approach is highly data-efficient and
preserves the original meaning in most cases.

We approach the problem with a two-part
pipeline where we aim to generate a title for each
paragraph of a text, as illustrated in Figure 2.
First, a SELECTOR selects the most salient sen-
tence within a paragraph and then the COMPRES-
SOR compresses the sentence. The selector is an
extractive summarization algorithm that assigns a
score to each sentence corresponding to its likeli-
hood to be used in a summary (Gehrmann et al.,
2018). Algorithms for word deletion typically
rely on linguistic features within a tree-pruning al-
gorithm that identifies which phrases can be ex-
cluded (Filippova and Altun, 2013). Following
recent work that shows the efficiency of contex-
tual span-representations (Lee et al., 2016; Peters
et al., 2018), we develop an alternative approach
based on a Semi-Markov Conditional Random
Field (SCRF) (Sarawagi and Cohen, 2005). The
SCRF is further extended by a language model that
ranks multiple compression candidates to generate
grammatically correct compressions.

We evaluate this approach by comparing it to
strong sequence-to-sequence baselines on an En-
glish sentence-compression dataset and show that
our approach performs almost as well on large
datasets while outperforming the complex models
with limited training data. We further show the
results of a human study to compare the effects
of showing no section titles, human-generated ti-
tles, and titles generated with our method. The
results corroborate previous findings in that we
find a significant decrease in time required to an-
swer questions about a text and an increase in the
length of summaries written by test subjects. We
also observe that the extractive algorithmic titles
have a stronger effect on question answering tasks,
whereas abstractive human titles have a stronger
effect on the summarization task. This indicates
that the inherent differences in how humans and
our approach summarize the content of a section
play a major role in how reading comprehension
is affected.

2 Methods

2.1 SELECTOR

To select the most important sentence, we adapt
an approach to the problem of content-selection in
summarization, which has been shown to be ef-

SELECTOR COMPRESSOR RANKER

Figure 2: Overview of the three steps. The SELECTOR
detects the most salient sentence. Then, the COMPRES-
SOR generates compressions and the RANKER scores
the them.

fective and data-efficient (Gehrmann et al., 2018).
An advantage of this approach over other extrac-
tive summarizers (e.g. Zhou et al. (2018)) is that
those often model dependencies between selected
sentences, which is not applicable to this problem
since we only aim to extract a single sentence.

Let x = x1, . . . , xn denote a sequence of
words within a paragraph, and y = y1, . . . , ym
a multi-sentence summary with n � m. Fur-
ther, let t = t1, . . . , tn be a binary alignment
variable where ti=1 iff xi ∈ y. Using this
alignment, the word-saliency problem is defined
as learning a SELECTOR model that maximizes
log p(t|x) =

∑n
i=1 log p(ti|x). Using this model,

we calculate the relevance of a sentence sent :=
xstart, . . . , xend, with 1 ≤ start < end ≤ n, with
a saliency function defined as

saliency(sent) =
1

|sent|

|sent|∑
i=1

p(ti|sent).

The sentence selection problem thus reduces to
sentence with the most relevant words within a
paragraph para,

argmax
sent ∈ para

saliency(sent).

We first represent each word using two different
embedding channels. The first is a contextual
word representation using ELMo (Peters et al.,
2018), and the second one uses GloVe (Penning-
ton et al., 2014). Preliminary experiments corrob-
orated the findings by Peters et al. that the com-
bination of the embeddings help the model con-
verge faster, and perform better with limited train-
ing data. Both embeddings for a word xi are con-
catenated into one vector ei, and used as input to
a bidirectional LSTM (Hochreiter and Schmidhu-
ber, 1997). Finally, the output of the LSTM hi is
used to compute the probability that a word is se-
lected σ(W Ts hi+bs) with the trainable parameters
Ws and bs.



1679

2.2 COMPRESSOR

We next define the problem of deletion-only com-
pression of a single sentence. For simplicity of
notation, let x1, . . . , xn refer to the words within a
single sentence, and y1, . . . , yn be a binary indica-
tor whether a word is kept in the compressed form.
The compression c(x, y) becomes

c(x, y) = xi∀(x1, y1), . . . , (xn, yn) iff yi = 1.

The challenge here is that choices are not made
independently from another. Consider the sen-
tence The round ball flew into the net last weekend,
which should be compressed to Ball flew into net.
Here, the choice to include into depends on first
selecting the corresponding verb flew. One ap-
proach to this problem is to use an autoregressive
sequence-to-sequence model, in which choices are
conditioned on preceding ones. However, these
models typically require too many training ex-
amples for many languages or domains. There-
fore, we relax the problem by assuming that it
obeys the Markov property of order L, and train
a COMPRESSOR model to maximize p(y|x) =∑n

i=1 log p(yi|x, yi−L:i−1). To still retain gram-
maticality, we define the additional problem of es-
timating the likelihood of a compression p(y) with
a RANKER model, described below.

We compare multiple approaches to deletion-
based sentence compression. Throughout, we
apply the same embedding as for the SELEC-
TOR. Since a grammatical compression problem
relies on the underlying linguistic features (Fil-
ippova and Altun, 2013), we process the source
documents with spaCy (Honnibal and Montani,
2017) to include the following features in addi-
tion to contextualized word embeddings: (1) Part-
of-speech tags, (2) Syntactic dependency tags, (3)
original word shape, (4) Named entities. Each of
these information is encoded in an additional em-
bedding that is concatenated to the word embed-
dings. Over the final embedding vector, we use a
bidirectional LSTM to compute the hidden repre-
sentations h1, . . . , hn for this task.

Naive Tagger The simplest approach we con-
sider is a naive tagger similar to the SELECTOR.
We assume full independence between values in
y. The probability p(yi=1|x) is computed as
σ(W Tc w1 + bc) with trainable parameters Wc and
bc.

Conditional Random Field Compressed sen-
tences have to remain grammatical, which implies
a dependence between values in y. Without any re-
strictions on the dependence between choices, it is
intractable to marginalize over all possible y with
a scoring function for a given (x, y) pair,

p(y|x) = expScore(x, y)∑
y′ expScore(x, y′)

. (1)

Therefore, we assume that only neighboring val-
ues in y are dependent, and apply a linear-chain
CRF (Lafferty et al., 2001) that uses h as its fea-
tures to this problem.

We define a scoring function Score(x, y) =∑
i log φi(x, y) that computes the (log) poten-

tials at a position i using a function φ. The
emission potential for a word xi is computed as
φEi (x, y) =We2(tanh(We1hi + be1)) + be2, using
only the local information. The transition poten-
tial φTi depends on the previous and current choice
(yi−1, yi), and can be looked up in a 2 × 2 ma-
trix that is learned during training. The complete
scoring function can be expressed as

Score(x, y) =
|y|∑
i

(φEi (x, y) + φ
T
i (yi−1, yi)) .

During training, we can minimize the negative
log-likelihood in which the partition function is
computed with the forward-backward algorithm.
During inference, this formulation allows for ex-
act inference with the Viterbi algorithm.

Semi-Markov Conditional Random Field Al-
though compressed sentences should often include
entire phrases, the CRF does not take into account
local dependencies beyond neighboring words.
Therefore, we relax the Markov assumption and
score longer spans of words. This can be achieved
with a SCRF (Sarawagi and Cohen, 2005). Fol-
lowing a similar approach as Ye and Ling (2018),
let s = {s1, s2, . . . , sp} denote the segmentation
of x. Each segment si is represented as a tuple
〈start, end, ỹi〉, where start and the end denote
the indices of the boundaries of the phrase, and ỹ
the corresponding label for the entire phrase. To
ensure the validity of the representation, we im-
pose the restrictions that start1 = 1, endp = |x|,
starti+1 = endi + 1, and starti ≤ endi. Ad-
ditionally, we set a fixed maximum length L. We



1680

extend Eq. 1 to account for a segmentation s in-
stead of individual tags such that

p(s|x) = expScore(s, x)∑
s′ expScore(s′, x)

,

marginalizing over all possible segmentations.
The CRF represents a special case of the SCRF
with L = 1. To account for the segment-level in-
formation, we extend the emission potential func-
tion to

φEi (x, 〈ỹ, start, end〉) =
end∑

i=start

W Te h
′
i, (2)

where h′i is the concatenation of hi, hstart −
hend, and a span length embedding elen to ac-
count for both individual words and global seg-
ment information. We also extend φT to include
transitions to and from longer tagged sequences
by representing targets as BIEUO tags (Ratinov
and Roth, 2009). This formulation allows for
similar training by minimizing the negative log-
likelihood.

2.3 RANKER

The inference of the CRF and the SCRF require
an estimation of the best possible segmentation
s∗ = p(s|x), which can be computed using the
Viterbi algorithm. However, CRFs and SCRFs
are typically employed in sequence-level tagging
with no inter-segment dependencies. The sentence
compression task differs since a resulting com-
pressed sentence should be grammatical. There-
fore, we employ a language model (LM) to rank
compression candidates based on the likelihood of
the compressed sentence compress(x, s). Namely,
we extend the inference target to

s∗LM = argmax
s

(p(s|x) + λp(compress(x, s))) ,

using a weighting parameter λ. Since exact in-
ference for this target is intractable, we approxi-
mate this inference by constraining the re-ranking
to theK best segmentations according to aK-best
Viterbi algorithm.

The RANKER uses the same word embeddings
as the COMPRESSOR, and a bidirectional LSTM
in order to maximize the probability of a sequence

∑
i p(xi|x1:i−1) + p(xi|xi+1:n). Using the hid-

den representation hi, we compute a distribution
over the vocabulary as softmax(Wlhi + bl). We
additionally use the same weights for word em-
beddings and Wl, which has been shown to im-
prove language model performance (Inan et al.,
2016). We prevent affording an advantage to
shorter compressions c during the inference by ap-
plying length normalization (Wu et al., 2016) with
a length penalty α.

2.4 Sequence-to-Sequence Baselines

The most common approach to summarization and
sentence compression uses sequence-to-sequence
(S2S) models that learn an alignment between
source and target sequences (Sutskever et al.,
2014; Bahdanau et al., 2014). S2S models
are autoregressive and generate one word at a
time by maximizing the probability p(y|x) =∑

i p(yi|x, y1:i−1). Since this condition is stronger
than that of CRF-based approaches, we hypothe-
size that S2S models perform better with unlimited
training data. However, since S2S models need
to jointly learn the alignment and generate words,
they typically perform worse with limited data.

To test this hypothesis, we define two S2S base-
lines we compare to our models. First, we use a
standard S2S model with attention as described
by Luong et al. (2015). In contrast to the other
approaches, this model is abstractive and has the
ability to paraphrase and re-order words. We con-
strain these abilities in a second S2S approach as
described by Filippova et al. (2015). This model is
a sequential pointer-network (Vinyals et al., 2015)
and can only generate words from the source sen-
tence. Instead of using an attention mechanism to
compute which word to copy, the model enforces a
monotonically increasing index of copied words to
prevent the re-ordering. We compare both against
their reported numbers and our own implementa-
tion.

3 Data and Experiments

The SELECTOR is trained on the CNN-DM cor-
pus (Hermann et al., 2015; Nallapati et al., 2016),
which is the most commonly used corpus for news
summarization (Dernoncourt et al., 2018). Each
summary comprises a number of bullet points for
an article, with an average length of 66 tokens and
4.9 bullet points. The COMPRESSOR is trained on
the Google sentence compression dataset (Filip-



1681

pova and Altun, 2013), which comprises 200,000
sentence-headline pairs from news articles. The
deletion-only version of the headlines was created
by pruning the syntactic tree of the sentence and
aligning the words with the headline. The largest
comparable corpus Gigaword (Rush et al., 2015)
does not include deletion-only headlines.

We limit the vocabulary size to 50,000 words
for both corpora. Both SELECTOR and COM-
PRESSOR use a two-layer bidirectional LSTM
with 64 hidden dimensions for each direction, and
a word-embedding size of 200. Each linguistic
feature is embedded into 30-dimensional space.
During training, the dropout probability is set to
0.5 (Srivastava et al., 2014). The model is trained
for up to 50 epochs or until the validation loss does
not decrease for three consecutive epochs. We ad-
ditionally halve the learning rate every time the
validation loss does not decrease for two epochs.
We use Adam (Kingma and Ba, 2014) with AMS-
Grad (Reddi et al., 2018), an initial learning rate
of 0.003, and a l2-penalty weight of 0.001. The
RANKER uses the same LSTM configuration, but
we optimize it with SGD with 0.9 momentum, and
an initial learning rate of 0.25.

The S2S models have 64 hidden dimensions for
each direction of the encoder, and 128 dimensions
for the decoder LSTM. They use one layer, and the
decoder is initialized with the final state of the en-
coder. Our optimizer for this task is adagrad with
an initial learning rate of 0.15, and an accumulator
value of 0.1 (Duchi et al., 2011).

3.1 Automated Evaluation

In the automated evaluation, we focus on the
compression models and first conduct experiments
with the full dataset to compute an upper bound on
the performance of our approach. This experiment
functions as a benchmark to investigate how much
better the S2S based approaches perform with suf-
ficient data. The next experiment investigates a
scenario, in which data availability is limited and
ranges from 100 to 1000 training examples. We
compare results with and without linguistic fea-
tures to further evaluate whether these features im-
prove the performance or whether contextual em-
beddings are a sufficient representation. In each
experiment, we measure precision, recall, and F1-
score of the predictions compared to the human
reference, as well as the ROUGE-score. We ad-
ditionally measure the length of the compressions

Figure 3: Two paragraphs within the interface of our
human evaluation with titles in the left-top margin of a
paragraph. A side-effect of the data-efficient deletion-
only approach, some titles look ungrammatical, as
shown in the second example “Warming is putting more
moisture.”

to investigate whether methods delete a sufficient
number of words.

3.2 Human Evaluation

We evaluated the effect of our generated titles in
a between-subjects study on Amazon Mechanical
Turk. We compared three different conditions: no
titles, human-generated titles, and algorithmically
generated titles by our SCRF+Ranking model. Ev-
ery participant kept their randomly assigned con-
dition throughout all tasks. We defined the follow-
ing three tasks to approximately measure the ef-
fect of short section titles on (1) retention of text,
(2) comprehension of text, and (3) retrieval of in-
formation. (Retention) We first presented a text
and then asked participants three questions about
facts in the text. (Comprehension) We showed a
text and then asked the participants to generate a
three-sentence summary of the text. (Retrieval)
We first presented two questions and then the text,
prompting participants to find the answers.

Previous findings indicate that titles help with
retention only when presented towards the begin-
ning of a text (Dooling and Mullet, 1973). Thus,
we place texts in the left margin at the top of a
paragraph as shown in the example in Figure 3.
This further avoids interrupting the reading flow
of the long text while being integrated into the
natural left-to-right reading process. Although
reading comprehension is well studied in natural
language processing, most datasets focus on
machine comprehension (Richardson et al., 2013;
Rajpurkar et al., 2016). Therefore, we adapted
texts from the interactive reading practice by Na-
tional Geographic, written by Helen Stephenson1.
The 33 texts are based on articles and comprise
three versions for each story; elementary, inter-
mediate, and advanced, from which we selected
intermediate and advanced versions. Topics of the

1http://www.ngllife.com/student-zone/
interactive-reading-practice



1682

Model Features P R F1 Length

Filippova et al. (2015) Yes 82.0

S2S w/o copy No 83.2 ±4.5 73.0 ±5.9 75.8 ±4.1 9.4 ±2.9
Sequential Pointer 87.1 ±4.1 76.0 ±5.1 79.1 ±3.5 9.4 ±2.6
Naive Tagger 81.4 ±3.7 74.6 ±6.0 75.5 ±3.9 9.7 ±2.7
SCRF 85.2 ±3.9 71.6 ±8.6 73.6 ±5.5 9.0 ±3.5
SCRF+Ranking 86.1 ±3.9 72.9 ±8.5 74.8 ±5.5 9.1 ±3.4

S2S w/o copy Yes 84.6 ±4.1 75.1 ±5.6 77.6 ±3.8 9.5 ±3.1
Sequential Pointer 89.6 ±3.5 74.2 ±5.2 79.8 ±3.6 8.7 ±2.3
Naive Tagger 84.1 ±3.5 75.4 ±5.3 79.5 ±3.7 9.5 ±2.5
SCRF 86.3 ±3.7 73.0 ±8.4 79.1 ±3.5 9.3 ±2.7
SCRF+Ranking 87.2 ±3.7 73.9 ±7.8 79.6 ±3.3 9.1 ±2.8

Table 1: Results of our models on the large dataset comprising 200,000 compression examples.

texts include Geography, Science, Anthropology,
and History; their length ranges from four to
seven paragraphs. Each text is accompanied
by reading comprehension questions, which we
utilized in the retention and retrieval tasks. We
first excluded those questions where the answer
was part of either human- or algorithmically
generated summary. Of the remaining questions,
we randomly selected three questions for each
of the retention and retrieval tasks. The same
questions were shown in either of the conditions.

Every participant completed six tasks, two for
every possible task, one with intermediate and one
with advanced difficulty. To account for the dif-
ferent backgrounds of participants, we also asked
participants about their perceived difficulty for
each task on a 5-point Likert scale. The total time
to complete all tasks was limited to 30 minutes,
and Turkers were paid $5. In total, we recruited
144 participants who self-reported that they flu-
ently spoke English, uniformly distributed over
the three conditions. They answered on average
68.25% of questions correctly and took 16.5 min-
utes to complete all six tasks. This is approxi-
mately 30% faster than the fastest graduate student
we recruited for pilot-testing, indicating that Turk-
ers aimed to complete the tasks as fast as possible,
possibly by only skimming the text. We omitted
results from participants with an answer accuracy
of less than 25% (n=21), and excluded individual
replies given in under 15 seconds (n=10) or over
10 minutes (n=5), leaving a total of 701 completed
tasks. After excluding outliers, the correct answer
average was 75.64%, while the time to completion
increased by 15 seconds to 16.75 minutes.

1 2 3 4 5 6 7 8 9 10 11
Location of selected sentence

10
0

10
1

10
2

10
3

10
4

lo
g 

# 
oc

cu
re

nc
es

Figure 4: Index of extraction within a paragraph.

4 Results

Selector We compare the performance of the se-
lector against the LEAD-1 baseline that naively
selects the first sentence of a news article. This
provides a strong comparison since a news arti-
cle typically aims to summarize its content in the
first sentence. LEAD-1 achieves ROUGE (1/2/L)-
scores of 27.5/9.6/23.7 respectively. In contrast,
our selector achieves scores of 30.2/12.2/26.45
which presents an improvement of over 10% in
each category. We illustrate the source of this im-
provement in Figure 4, which shows the locations
of selected sentences and observe a negative corre-
lation between a later location within a text and the
probability of being selected. However, in most
cases, the first sentence is not the most relevant
according to the model.

Unrestricted Data Table 1 shows the results of
the different approaches on the large dataset. As
expected, the copy-model performs best due to its
larger modeling potential. It is closely followed by
SCRF+Ranking, which comes within 0.2 F1-score
when using additional features. This difference is
not statistically significant, given the high variance
of the results. Compared to the best reported re-



1683

100 200 300 400 500 600 700 800 900 1000
Number of training examples

50

55

60

65

70

F1
-S

co
re

Naive Tagger
SCRF
SCRF+Ranking
S2S

Figure 5: F1-scores of the different models with an in-
creasing number of training examples.

sult in the literature by Filippova et al. (2015), our
models perform almost as well, despite the fact
that their model is trained on 2,000,000 unreleased
datapoints compared to our 200,000. We further
observe that all models generate compressed sen-
tences of almost the same length between 8.7 and
9.5 tokens per compression.

The Naive Tagger also achieves comparable
performance to the SCRF in F1-Score. To
test whether our model leads to a higher flu-
ency compared to it, we additionally measure the
ROUGE score. In ROUGE-2, we find that the
SCRF+Ranking leads to an increase from 58.1
to 60.1, with an increase in bigram precision by
5 points from 64.5 to 69.3. The Naive Tagger
is more efficient at identifying individual words,
with a ROUGE-1 of 71.3 when the ranking ap-
proach only achieves 68.7. While these differ-
ences lead to similar ROUGE-L scores between
69.9 and 70.2, the fact that the ranking-based ap-
proach matches longer sequences indicates higher
fluency. In an analysis of samples from the Naive
Tagger, we found that it commonly omits crucial
verb-phrases from compressed sentences.

We show compressions from two paragraphs in
the NatGeo data in Figure 3. This example illus-
trates the robustness of the compression approach
to out-of-domain data when including linguistic
features. Despite the fact that the example text is
not a news article like the training data, it performs
well and generates mostly grammatical compres-
sions.

Limited Data We present results on limited data
in Figure 5. The results show the major advan-
tage of the simpler training objective. All of the
tagging-based models outperform the S2S base-
lines by a large margin due to their data-efficiency.
We did not observe a significant difference be-
tween the different tagging approaches in the lim-
ited data condition. In our experiments, we found
that the S2S models start outperforming the sim-

Very Easy Easy Neutral Difficult Very Difficult
Perceived Difficulty

0

50

100

150

200

250

Ti
m

e 
in

 s
ec

on
ds

 (±
10

%
)

Algo
Human
No Title

Figure 6: Mean and the 90% confidence interval of the
time taken by Turkers to complete the tasks, grouped
by the perceived difficulty.

pler models at around 20,000 training examples.
Despite its high F1-Score, the Naive Tagger suf-
fers from ungrammatical output in this condition
as well, with the readability scoring significantly
lower than the SCRF outputs. We argue that
the SCRF+Ranking approach represents the best
trade-off of our presented models since it performs
well with limited data while performing almost as
well as complex models in unlimited data situa-
tions. This makes it most flexible to apply to a
wide range of tasks.

Human Evaluation In the human study, we no-
tice an immediate effect of the difficulty of texts.
Between the intermediate and advanced versions
of the texts, the mean time to complete the tasks
increases by 8 seconds (from 125 to 133 sec-
onds). Additionally, the mean perceived diffi-
culty increases from 2.40 to 2.55 (in between Easy
and Neutral). The largest observed effect of text
difficulty is on the accuracy of answers, which
decreases from 84.4% to 67.5%, indicating that
within a similar time frame, the difficult texts were
harder to understand.

We present a breakdown of time spent on a task
by perceived difficulty for each of the conditions in
Figure 6. There is a positive correlation between
perceived difficulty and time spent on a task across
all conditions. Interestingly, tasks rated Very Easy
and Easy were completed slower in the human
condition than in the no-title condition, but faster
with the algorithmically generated ones. This ef-
fect alleviates in the higher difficulties, in which
the no-title condition takes longest. Indeed, a com-
parison between the algorithmic and no title condi-
tions reveals a decrease in time by 19.8 seconds in
the retention task, significant with p < 0.005 ac-
cording to a χ2 test. Interestingly, we can observe
opposite effects in the human condition. Here,
the comprehension task is completed 15.4 seconds
faster (p < 0.005), but the other tasks only show
minor effects.



1684

Task Measure Intervention Effect Size p-value

Retention Time Taken (sec) Human -2.2 0.63
Algo -27.1 0.01*

Accuracy Human -0.01 0.07
Algo -0.01 0.13

Retrieval Time Taken (sec) Human -0.9 0.87
Algo -4.5 0.03*

Accuracy Human +0.01 0.20
Algo -0.01 0.15

Comprehension Time Taken (sec) Human -20.9 0.03*
Algo -2.6 0.04*

Summary Length (words) Human +8.6 0.02*
Algo +5.3 0.03*

Readability Human -0.1 0.24
Algo -0.1 0.50

Relevance Human -0.02 0.92
Algo +0.01 0.63

Table 2: The causal effects of the human and algorithmic section titles on different measures differ across tasks. All
the shown effect sizes are measured in comparison to the baseline without any shown titles. Significant p-values at
a 0.05 level are marked with a *.

To further investigate these effects, we analyze
the causal effect of our three conditions by
measuring the average treatment effect while con-
trolling for both actual and perceived difficulty of
the tasks with an Ordinary Least Squares analysis.
Whenever possible, we additionally condition on
the total time taken. An overview of our tests
is presented in Table 2. In the causal tests, we
observe similar effects in the retention task – the
algorithmically generated titles lead to a decrease
in time required for the task with an effect size
of 27.1 seconds. In contrast, human-generated
titles only lead to a non-significant 2.2-second
decrease. We observe a non-significant decrease
of approximately 4% in accuracy with added
titles. We observed no effect of adding titles
on the perceived difficulty of this task. In the
retrieval task, added titles result in a weaker effect.
The algorithmic titles decrease time by only 4.5
seconds, and the human titles by non-significant
0.9 seconds. Similar to the retention task, there
is no significant change in accuracy, with all
accuracy levels within 1% from another and we
observe no effect on the perceived difficulty.

In the comprehension task, it is not possible to
measure accuracy. Instead, we evaluate readabil-
ity and relevance as judged by human raters on a
five-point Likert scale, two commonly used met-
rics for abstractive summarization (Paulus et al.,

Readability Relevance

Condition µ σ µ σ
No title 4.66 0.65 4.11 0.86
Human 4.55 0.76 4.09 0.95
Algo 4.52 0.72 4.12 1.02

Table 3: Human ratings for human-generated sum-
maries while showing different section titles.

2017). We present the average ratings in Table 3
and observe that there is almost no difference in
relevance and only a minor (not significant) de-
crease in readability with either condition. Cu-
riously, the previous effect on speed reverses in
this task – algorithmic titles only lead to a 2.6-
second decrease in time, while human titles lead
to a 20.9-second decrease. Both conditions addi-
tionally lead to longer summaries; algorithmic ti-
tles by 5.3 words and human titles by 8.6 words.
One potential explanation for this behavior could
be that subjects copied the presented section titles
into the summary text field. This was not the case,
since, on average, only 2.8-3.7% of the bigrams in
the titles were used in the summaries, across both
conditions and difficulties (0.6-1.5% of trigrams,
0.1-0.8% of 4-grams).

Given the similar relevance scores, we thus
argue that presenting the titles leads to more



1685

detailed descriptions of the texts. Similarly to
the other tasks, the perceived difficulty does not
change significantly. However, we note that some
subjects noted that there were some ungram-
matical generated titles, which is an artifact of
the deletion-only approach. Future work may
investigate how abstractive approaches that are
not restricted to deletion-based approaches can be
applied to the same problem.

Overall, the results of the human-subject study
reveal an effect that is well studied in the literature.
Namely, that the type of title influences what is
being remembered about a text (Schallert, 1975),
and that different headline styles affect readers in
different ways (Lorch Jr et al., 2011). Kozmin-
sky (1977) found that the immediate free recall of
information is biased towards topics emphasized
in titles. The better performance in memoriza-
tion tasks in the algorithmic condition can be ex-
plained by the fully extractive approach that im-
mediately shows information judged most relevant
by the model. In contrast, human-generated titles
show a higher level of abstraction and generaliza-
tion, which is more helpful for the overall com-
prehension but does not emphasize any piece of
information.

5 Related Work

The aim of SCRFs is to learn a segmentation of a
sequential input and assigning the same label to an
entire segment. While they were originally devel-
oped for information extraction (Sarawagi and Co-
hen, 2005), it is most commonly applied to speech
recognition within the acoustic model to improve
segmentation between different words (He and
Fosler-Lussier, 2015; Lu et al., 2016; Kong et al.,
2015). Similar to this work, it has also been
shown that coupling an LM with an SCRF can
improve segmentation through multi-task train-
ing (Lu et al., 2017; Liu et al., 2017). SCRFs
have also been applied to sequence tagging tasks,
for example, the extraction of phrases that indicate
opinions (Yang and Cardie, 2012). In this work,
we built upon an approach by Ye and Ling (2018)
who recently introduced a hybrid SCRF that uses
both word- and phrase-level information. Alter-
native approaches for similar tasks are CRFs that
estimate pairwise potentials rather than using a
fixed transition matrix (Jagannatha and Yu, 2016)
or high-order CRFs which outperform SCRFs in
some sequence labeling tasks (Cuong et al., 2014).

While this work is the first to apply SCRFs to
sentence compression, Grootjen et al. (2018) also
use extractive summarization techniques to im-
prove reading comprehension by highlighting rel-
evant sentences. Most similar to our compression
approach is Hedge Trimmer (Dorr et al., 2003),
which compresses sentences through deletion, but
uses an iterative shortening algorithm based on
linguistic features. Extending this work, Filip-
pova and Altun (2013) apply a similar approach
on linguistic features, but learn weights for the
shortening algorithm. Both approaches also do not
consider the selection of the sentence to be com-
pressed, unlike our proposed model.

6 Conclusion

In this work, we have presented a novel ap-
proach to section title generation that uses an ef-
ficient sentence compression model. We demon-
strated that our approach performs almost as well
as sequence-to-sequence approaches with unlim-
ited training data while outperforming sequence-
to-sequence approaches in low-resource domains.
A human evaluation showed that our section titles
lead to strong improvements across multiple read-
ing comprehension tasks. Future work might in-
vestigate end-to-end approaches, or develop alter-
native approaches that generate titles more similar
to how humans write titles.

Acknowledgments

We are grateful for the helpful feedback from
the three anonymous reviewers. We additionally
thank Anthony Colas and Sean MacAvaney for the
multiple rounds of feedback on the ideas presented
in this paper.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Kenneth E Bell and John E Limber. 2009. Reading
skill, textbook marking, and course performance.
Literacy research and instruction, 49(1):56–67.

John D Bransford and Marcia K Johnson. 1972. Con-
textual prerequisites for understanding: Some in-
vestigations of comprehension and recall. Journal
of verbal learning and verbal behavior, 11(6):717–
726.



1686

Sophie Chesney, Maria Liakata, Massimo Poesio, and
Matthew Purver. 2017. Incongruent headlines: Yet
another way to mislead your readers. In Proceedings
of the 2017 EMNLP Workshop: Natural Language
Processing meets Journalism, pages 56–61.

Nguyen Viet Cuong, Nan Ye, Wee Sun Lee, and
Hai Leong Chieu. 2014. Conditional random field
with high-order dependencies for sequence labeling
and segmentation. The Journal of Machine Learn-
ing Research, 15(1):981–1009.

Franck Dernoncourt, Mohammad Ghassemi, and Wal-
ter Chang. 2018. A repository of corpora for sum-
marization. In Eleventh International Conference
on Language Resources and Evaluation (LREC).

D James Dooling and Roy Lachman. 1971. Effects of
comprehension on retention of prose. Journal of ex-
perimental psychology, 88(2):216.

D James Dooling and Rebecca L Mullet. 1973. Locus
of thematic effects in retention of prose. Journal of
Experimental Psychology, 97(3):404.

Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of the HLT-
NAACL 03 on Text summarization workshop-Volume
5, pages 1–8. Association for Computational Lin-
guistics.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12(Jul):2121–2159.

Ullrich KH Ecker, Stephan Lewandowsky, Ee Pin
Chang, and Rekha Pillai. 2014. The effects of subtle
misinformation in news headlines. Journal of exper-
imental psychology: applied, 20(4):323.

Carol Sue Englert, Troy V Mariage, Cynthia M Okolo,
Rebecca K Shankland, Kathleen D Moxley, Car-
rie Anna Courtad, Barbara S Jocks-Meier, J Chris-
tian O’Brien, Nicole M Martin, and Hsin-Yuan
Chen. 2009. The learning-to-learn strategies of ado-
lescent students with disabilities: Highlighting, note
taking, planning, and writing expository texts. As-
sessment for Effective Intervention, 34(3):147–161.

Katja Filippova, Enrique Alfonseca, Carlos A Col-
menares, Lukasz Kaiser, and Oriol Vinyals. 2015.
Sentence compression by deletion with lstms. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
360–368.

Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1481–1491.

Sebastian Gehrmann, Yuntian Deng, and Alexander
Rush. 2018. Bottom-up abstractive summarization.
arXiv preprint arXiv:1808.10792.

FA Grootjen, GE Kachergis, et al. 2018. Automatic
text summarization as a text extraction strategy for
effective automated highlighting.

Yanzhang He and Eric Fosler-Lussier. 2015. Segmen-
tal conditional random fields with deep neural net-
works as acoustic models for first-pass word recog-
nition. In Sixteenth Annual Conference of the Inter-
national Speech Communication Association.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Matthew Honnibal and Ines Montani. 2017. spacy 2:
Natural language understanding with bloom embed-
dings, convolutional neural networks and incremen-
tal parsing. To appear.

Hakan Inan, Khashayar Khosravi, and Richard Socher.
2016. Tying word vectors and word classifiers:
A loss framework for language modeling. arXiv
preprint arXiv:1611.01462.

Abhyuday N Jagannatha and Hong Yu. 2016. Struc-
tured prediction models for rnn based sequence la-
beling in clinical text. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing. Conference on Empirical Methods in
Natural Language Processing, volume 2016, page
856. NIH Public Access.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Teun A van DijkWalter Kintsch and TA Van Dijk. 1978.
Cognitive psychology and discourse: Recalling and
summarizing stories. Current trends in textlinguis-
tics, page 61.

Lingpeng Kong, Chris Dyer, and Noah A Smith.
2015. Segmental recurrent neural networks. arXiv
preprint arXiv:1511.06018.

Ely Kozminsky. 1977. Altering comprehension: The
effect of biasing titles on text comprehension. Mem-
ory & Cognition, 5(4):482–490.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. Proceedings of the 18th International
Conference on Machine Learning 2001 (ICML
2001).

Kenton Lee, Shimi Salant, Tom Kwiatkowski, Ankur
Parikh, Dipanjan Das, and Jonathan Berant. 2016.



1687

Learning recurrent span representations for ex-
tractive question answering. arXiv preprint
arXiv:1611.01436.

Liyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Huan
Gui, Jian Peng, and Jiawei Han. 2017. Empower
sequence labeling with task-aware neural language
model. arXiv preprint arXiv:1709.04109.

Robert F Lorch Jr, Julie Lemarié, and Russell A Grant.
2011. Three information functions of headings: A
test of the sara theory of signaling. Discourse pro-
cesses, 48(3):139–160.

Liang Lu, Lingpeng Kong, Chris Dyer, and Noah A
Smith. 2017. Multitask learning with ctc and seg-
mental crf for speech recognition. arXiv preprint
arXiv:1702.06378.

Liang Lu, Lingpeng Kong, Chris Dyer, Noah A Smith,
and Steve Renals. 2016. Segmental recurrent neural
networks for end-to-end speech recognition. arXiv
preprint arXiv:1603.00223.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412–1421.

Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre,
Bing Xiang, et al. 2016. Abstractive text summa-
rization using sequence-to-sequence rnns and be-
yond. arXiv preprint arXiv:1602.06023.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, pages 147–
155. Association for Computational Linguistics.

Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar.
2018. On the convergence of adam and beyond. In
International Conference on Learning Representa-
tions.

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
193–203.

Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 379–389.

Sunita Sarawagi and William W Cohen. 2005. Semi-
markov conditional random fields for information
extraction. In Advances in neural information pro-
cessing systems, pages 1185–1192.

Diane L Schallert. 1975. Improving memory for prose:
The relationship between depth of processing and
context. Center for the Study of Reading Technical
Report; no. 005.

Abigail See, Peter J Liu, and Christopher D Man-
ning. 2017. Get to the point: Summarization
with pointer-generator networks. arXiv preprint
arXiv:1704.04368.

Edward E Smith and David A Swinney. 1992. The role
of schemas in reading text: A real-time examination.
Discourse Processes, 15(3):303–316.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems, pages 2692–2700.

Jennifer Wiley and Keith Rayner. 2000. Effects of titles
on the processing of text and lexically ambiguous
words: Evidence from eye movements. Memory &
Cognition, 28(6):1011–1021.

Sam Wiseman, Stuart Shieber, and Alexander Rush.
2017. Challenges in data-to-document generation.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2253–2263.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.



1688

Bishan Yang and Claire Cardie. 2012. Extracting opin-
ion expressions with semi-markov conditional ran-
dom fields. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1335–1345. Association for Com-
putational linguistics.

Zhi-Xiu Ye and Zhen-Hua Ling. 2018. Hybrid semi-
markov crf for neural sequence labeling. In Pro-
ceedings of the 56th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2018),
Melbourne, Australia. ACL.

Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,
Ming Zhou, and Tiejun Zhao. 2018. Neural docu-
ment summarization by jointly learning to score and
select sentences. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
654–663.


