



















































Incorporating Interlocutor-Aware Context into Response Generation on Multi-Party Chatbots


Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 718–727
Hong Kong, China, November 3-4, 2019. c©2019 Association for Computational Linguistics

718

Incorporating Interlocutor-Aware Context
into Response Generation on Multi-Party Chatbots

Cao Liu1,2, Kang Liu1,2, Shizhu He1,2, Zaiqing Nie3, Jun Zhao1,2
1 National Laboratory of Pattern Recognition, Institute of Automation,

Chinese Academy of Sciences, Beijing, 100190, China
2 University of Chinese Academy of Sciences, Beijing, 100049, China

3 Alibaba AI Labs, Beijing, 100029, China
{cao.liu, kliu, shizhu.he, jzhao}@nlpr.ia.ac.cn

zaiqing.nzq@alibaba-inc.com

Abstract

Conventional chatbots focus on two-party re-
sponse generation, which simplifies the re-
al dialogue scene. In this paper, we strive
toward a novel task of Response Generation
on Multi-Party Chatbot (RGMPC), where the
generated responses heavily rely on the inter-
locutors’ roles (e.g., speaker and addressee)
and their utterances. Unfortunately, complex
interactions among the interlocutors’ roles
make it challenging to precisely capture con-
versational contexts and interlocutors’ infor-
mation. Facing this challenge, we present
a response generation model which incorpo-
rates Interlocutor-aware Contexts into Recur-
rent Encoder-Decoder frameworks (ICRED)
for RGMPC. Specifically, we employ interac-
tive representations to capture dialogue con-
texts for different interlocutors. Moreover,
we leverage an addressee memory to enhance
contextual interlocutor information for the tar-
get addressee. Finally, we construct a corpus
for RGMPC based on an existing open-access
dataset. Automatic and manual evaluations
demonstrate that the ICRED remarkably out-
performs strong baselines.

1 Introduction

Human computer conversation has been an impor-
tant and challenging task in NLP and AI since the
Turing Test was proposed in 1950 (Turing, 1950).
Recently, with the rapid growth of social conver-
sation data available on the Internet, data-driven
chatbots are able to learn to generate responses di-
rectly and have attracted much more attention than
before (Li et al., 2016a; Tian et al., 2017).

Researches in this area mostly focus on the dia-
log with two interlocutors (Maı́ra Gatti de Bayser
et al., 2017). However, the real-life interaction in-
volves a substantial part of Multi-Party Chatbot-
s (MPC, such as internet forum and chat group),
which is a form of conversation with multiple in-

t Speaker Addressee Utterance

1 Alan ( ) Bert ( ) the main ubuntu

2 Carl ( ) - i

n-1 Carl ( ) Jack ( ) i

n Carl ( )

n+1 Jack ( ) Carl ( ) mate is a recent reincarnation i

(Generated) ResponseResponding
Speaker

Target
Addressee

Context

Figure 1: An example of Multi-Party Chatbots (MPC).
At each turn, a speaker said one utterance to an ad-
dressee. There are many interlocutors (e.g., Alan, Bert
and so on) in a conversation, where ai represents inter-
locutor’s ID.

terlocutors (Ouchi and Tsuboi, 2016). For ex-
ample, there are more than three interlocutors
(a1, a2, a3...am) involved in the conversation in
Figure 1, and their roles (e.g., speaker and ad-
dressee) may change across different dialog turns.

As shown in Figure 1, at each turn, the core is-
sue of MPC is to capture who (speaker) talks to
whom (addressee) about what (utterance). In order
to obtain responses in MPC, in our best knowl-
edge, previous approaches usually employ a re-
sponse selection paradigm, which simply select-
s one response from a set of existing utterances
as the final response according to the contexts.
Obviously, this paradigm, which could not gen-
erate new responses, is not so flexible. In this s-
tudy, to build a more broadly applicable system,
we concentrate on producing new responses word
by word, named as Response Generation on Multi-
Party Chatbots (RGMPC).

RGMPC is a very challenging task. The pri-
mary challenge is that the generated response has
strong relevance to the interlocutor’s roles, such as
the speaker and the addressee. For example, in the
same context of Figure 1, what a1 says to a2 is d-
ifferent from what a1 says to a3 because different
addressees (a2 and a3) have different information
demands. Similarly, as for the same addressee, ut-



719

terances from different speakers may be differen-
t because each speaker has personal background
knowledge and style of speaking. Moreover, the
roles of the same interlocutor may vary across d-
ifferent dialog turns. For instance, in Figure 1, a3
plays different roles in different dialog turns: s-
peaker in the turn 2 and n-1, addressee in the turn
n and n+1.

Therefore, it is very important for RGMPC to
capture interlocutor information. Currently, most
response generation methods consider only the
contextual utterance information (Serban et al.,
2016, 2017) but neglect the interlocutor informa-
tion. Although some researches have exploited the
interlocutor information for response generation,
they are still suffering from certain critical limita-
tions. Li et al. (2016b) learn a fixed vector for each
person from all conversational texts in the training
corpus. However, as a global representation, the
fixed person vector needs to be trained from large-
scale dialogue turns for each interlocutor, and it
may have a sparsity issue since some interlocu-
tors have very few dialogue turns.

To address the aforementioned problems of
RGMPC, this paper incorporates Interlocutor-
aware Contexts into a Recurrent Encoder-Decoder
model (ICRED) for RGMPC, which is also an
end-to-end framework. Specifically, in order to
capture interlocutor information, we exploit in-
teractive interlocutor representations learned from
current dialog context rather than the fixed person
vectors (Li et al., 2016b) obtained from all dialogs
in the training corpus. We expect that the learned
contextual interlocutor representation could be a
good alternative to the fixed person vectors (Li
et al., 2016b) due to its ability of alleviating the
sparsity issue. Furthermore, from the view of con-
versation analysis, responses are usually used for
answering the addressee’s question or expanding
the addressee’s utterances. Therefore, we original-
ly introduce an addressee memory mechanism to
enhance contextual information for the target ad-
dressee especially. Finally, both of the interactive
interlocutor representation and addressee memory
are utilized for decoding response utterances. In
particular, the addressee memory is leveraged to
capture the addressee information for each gener-
ated word dynamically.

In order to prove the effectiveness of the pro-
posed model, we construct a dataset for RGMPC

based on an open dataset1. Experimental results
show that the proposed model is fairly competitive
on both automatic and manual evaluations com-
pared with state-of-the-arts.

In brief, the main contributions of the paper are
as follows:

(1) We propose an end-to-end response gen-
eration model called ICRED which incorpo-
rates Interlocutor-aware Contexts into Recurrent
Encoder-Decoder framework for RGMPC.

(2) We leverage an addressee memory mecha-
nism to enhance contextual interlocutor informa-
tion for the addressee.

(3) We construct an open-access dataset for
RGMPC. Both automatic and manual evaluation-
s demonstrate that our model is remarkably better
than strong baselines in this dataset.

2 Task Formulation

Data Notation

Input
Context C = [(atspk, atadr, ut)]nt=1

Responding Speaker an+1spk (or ares)

Target Addressee an+1adr (or atgt)
Output Response un+1 (or {rj}Lrj=1)

Table 1: Notations for RGMPC.

On multi-party chatbots, lots of interlocutors
talk about one or more topics. At each dialogue
turn (or time step) t, there is a speaker (atspk),
who may talk something (ut) to a specific ad-
dressee (atadr), while the others are observers. As
shown in Table 1, given the context C of previ-
ous n dialog turns, the responding speaker ares
and the target addressee atgt at time step n+1,
the task of RGMPC aims to automatically gener-
ate the next utterance un+1 as the final response.
Here, C is a list ordered by the time step t: C =
[Ct]nt=1 = [(atspk, atadr,ut)]nt=1, where Ct means
atspk says u

t to atadr at time step t, n is the maxi-
mum number of previous dialog turns in a context.
ut = (wt1, wt2...wtLu) is the input utterance (word
sequence) at time step t, where Lu is the number
of maximum words in utterances.

3 Methodology

The overview of the proposed ICRED for RGMPC
is shown in Figure 2 along with its caption. The
details are as follows.

1The dataset is available at http-
s://www.dropbox.com/s/4chh64yaxajh0j7/RGMPC.zip?dl=0



720

Utterance Encoder
Layer

Speaker Interaction
Layer

Addressee Memory
Layer

Decoder layer
Contextual Addressee VectorAttentional Addressee Vector Contextual Speaker Vector Response:

Bi-GRU

Decoder

Bi-GRU Bi-GRU Bi-GRU

:   Speaker GRU
: Addressee GRU
:  Observer GRU

Figure 2: Overall structure of the proposed ICRED for RGMPC. At each time step t, (ai, aj ,ut) means that
a speaker ai said an utterance ut to an addressee aj , where the time step t is denoted on the bottom, and the
superscript t may be omitted for brevity. Our ICRED includes: 1© Utterance Encoder Layer: encoding each
utterance (ut) into distributed vectors; 2© Speaker Interaction Layer: capturing interactive interlocutor information
from contexts, and it updates all interlocutors’ representation by different GRUs according to their roles at each
time step, where the embedding for an interlocutor ai is obtained by extracting the i-th column (Ai) from the
interlocutor embedding matrix A; 3© Addressee Memory Layer: enhancing contextual information for the target
addressee (a3); 4© Decoder Layer: generating responses.

3.1 Utterance Encoder Layer

The utterance encoder layer transforms input ut-
terance into distributional representations. We
leverage the bi-directional Gated Recurrent U-
nits (GRU) (Cho et al., 2014) to capture the
long-term dependency. For an utterance ut =
(wt1, w

t
2...w

t
Lu

) at time step t, the concatenated
representation for hidden states in bi-directions is
denoted as hti = [

−→
h ti,
←−
h tLu−i+1] , where h

t
i is con-

sidered as the contextual word representation of
the input word wti . The state (h

t
Lu

) of the last word
is treated as the representation of the utterance at
time step t, which is denoted as ht, and it could be
sent to the speaker interaction layer for updating
contextual representation.

3.2 Speaker Interaction Layer

The speaker interaction layer is leveraged to obtain
the interlocutor information in the context. Simi-
lar to the Speaker Interaction RNNs (Zhang et al.,
2018), we utilize the interactive speaker encoder
for RGMPC.

As shown in Figure 2, an interlocutor embed-
ding matrix A is used to record all interlocutors’
representation, and A is initiated with a zero ma-
trix. Each column of A corresponds to an inter-
locutor’s embedding: Ai = A[∗, ai], where Ai is
the embedding for the interlocutor ai. The speaker
interaction layer updates the entire interlocutors’
embeddings at each time step based on their roles

(speaker, addressee or observer). Embeddings for
the speaker, addressee and observer are updated
by following role-differentiated GRUs: GRUS ,
GRUA and GRUO, respectively.

Atspk = GRUS(A
t−1
spk ,h

t) (1)

Atadr = GRUA(A
t−1
adr ,h

t) (2)

Atobv = GRUO(A
t−1
obv ,h

t) (3)

where Atspk (A
t
adr / A

t
obv) is the embedding for the

speaker (addressee / observer) at time step t, and
ht is the utterance representation obtained from
the utterance encoder layer. Take the first time step
“(a1, a2,u1)” in Figure 2 as an example, when
a1 says u1 to a2, the speaker’s (a1’s) embedding
A1 is updated by the speaker GRU—GRUS , and
the addressee’s (a2’s) embedding A2 is updated
by the addressee GRU—GRUA, while other inter-
locutors’ embeddings are updated by the observer
GRU—GRUO. Note that the addressee may be
missing (such as “(a3,−,u2)” at time step 2 in
Figure 2), where embeddings for all interlocutors
except for the speaker are updated by the observ-
er GRU. The interlocutor embedding matrix (A)
is updated up to the maximum time step n. The
final interlocutor embedding matrix is used in de-
coding.

3.3 Addressee Memory Layer
The interlocutor embedding matrix is updated by
utterance representations and interlocutor’s roles,



721

so it captures interlocutor’s context on the utter-
ance level. In fact, contextual word representation
is important for response generation, too. A con-
text contains consecutive utterances, and each ut-
terance is a word sequence. Therefore, memoriz-
ing all contextual word representations in the en-
tire context is complex, and it is difficult to work
on large-scale utterances in one context.

Intuitively, from the view of conversational
analysis, responses are usually used for answer-
ing the addressee’s question or expanding the ad-
dressee’s utterances. Therefore, we design an ad-
dressee memory layer, which only memorizes the
contextual word representations (noted as Mtgt)
in the last utterance said by the target addressee,
and the contextual representation for each word is
obtained from the utterance encoder layer. Take
“(am, a3, ?)” at time step n+1 in Figure 2 as an
example, un−1 is the last utterance said by the
target addressee a3 because of “(a3, am,un−1)”
at time step n-1, so the addressee memory layer
merely memorizes contextual word representation
Mtgt = [hn−11 ,h

n−1
2 , ...,h

n−1
Lu

] from the utterance
un−1, where hn−1i is obtained from Section 3.1.

3.4 Decoder Layer

The decoder is responsible for generating target
sequences. Different from a single contextual rep-
resentation in previous work (Serban et al., 2017),
the speaker interaction layer is able to capture dif-
ferent interlocutor information from contexts (e.g.,
personal background knowledge and style of s-
peaking for the responding speaker, special infor-
mation demands for the target addressee). More-
over, the addressee memory layer records contex-
tual word representation for the target addressee.
Therefore, we extract contextual speaker vec-
tor Ares for the responding speaker ares from
the final interlocutor embedding matrix A (e.g.,
the responding speaker’s embedding obtained by
Am = A[∗, am] for the responding speaker am in
Figure 2). Similarly, contextual addressee vec-
tor Atgt for the target addressee is also extract-
ed from A. However, Ares and Atgt keep same
for each generated word. In order to capture dy-
namic information for different generated words,
we leverage an attention mechanism to selective-
ly reads different contextual word representations
from the addressee memory. For each target word,
the decoder attentively reads the contextual word

representation as follows:

cj =
∑Lu

k=1
αjkMtgt[∗, k]; (4)

αjk =
eρ(sj−1,Mtgt[∗,k])∑
k′ e

ρ(sj−1,Mtgt[∗,k′])
(5)

where cj is the attentional addressee vector,
Mtgt[∗, k] is the contextual word representation
for the k-th word in the addressee memory, and
sj represents the hidden state in decoding GRU.
A function ρ is leveraged to compute the attentive
strength, which is calculated by a projected matrix
to connect sTj−1 and Mtgt[∗, k]. Finally, the atten-
tional addressee vector cj , contextual speaker vec-
tor Ares and contextual addressee vector Atgt are
concatenated to estimate the probability for pre-
dicted words:

p(rj |r<j , aspk, atgt, C) =
p(rj |rj−1, cj , Ares, Atgt, sj)

(6)

sj = GRUdec(sj−1, [cj , Ares, Atgt, xj−1]) (7)

where sj is the hidden state of the decoding
GRU—GRUdec. xj is the word vector of the
predicted target word rj , and rj is typically per-
formed by a softmax classifier over a settled vo-
cabulary based on word embedding similarity.

3.5 Learning
The proposed ICRED for RGMPC is totally differ-
entiable, and it can be optimized in an end-to-end
manner using back-propagation. Given the con-
text C, responding speaker ares, target addressee
atgt and target word sequence {rj}Lrj=1, the objec-
tive function is to minimize the loss function:

L = −1
Lr

Lr∑
j=1

log[p(rj |r<j , C, ares, atgt] + λL2

(8)

It contains a negative log-likelihood for gener-
ated responses and L2 regularization (L2), where
λ is a hyperparameter for L2.

4 Experiment

4.1 Dataset
Our dataset is constructed based on the Ubuntu
multi-party chatbot corpus2, which has been wide-
ly used as the evaluation dataset for the response
selection task (Ouchi and Tsuboi, 2016; Zhang

2https://github.com/hiroki13/response-ranking



722

Total Train Dev Test
# Contexts 423.5K 338.9K 42.3K 42.3K
# Speaker 35.3K 33.5K 15.6K 15.6K

# Addressee 23.4K 22.4K 10.8K 10.8K
# Vocab 276.1K 254.8K 82.2K 82.0K
# Tokens 26.3M 21.0M 2.62M 2.62M

Avg. Tok/Ctx 51.4 51.5 51.4 51.3
Avg. Tok/Res 10.6 10.6 10.7 10.6

Table 2: Data statistics. “#” means number, and “Avg.
Tok/Ctx (or Res)” is the number of tokens per context
(or response).

et al., 2018). The original data comes from the
Ubuntu IRC chat log, where each line consists of
(Time, Speaker, Utterance). If the addressee is ex-
plicitly mentioned in the utterance, it is extracted
as the addressee. Otherwise, all interlocutors ex-
cept the speaker are observers. Considering that
generating new responses in this paper is more
complicated than retrieving responses, the genera-
tive task requires higher-quality data. We suppose
that the responding speaker and target addressee
have appeared in the context, where the contextual
window is set to 5. Moreover, the words are tok-
enized by NLTK, and some general responses are
removed by human rules3. Finally, we randomly
split the dataset into Train/Dev/Test (8:1:1), and it
is publicly available1. The detailed statistics of the
dataset are shown in Table 2.

4.2 Implement Details

In order to keep our model comparable to other
typical existing methods, we keep the same param-
eters and experimental environments for ICRED
and the comparative models. We take a maxi-
mum of 20 words for the utterance. The word vec-
tor dimension is 300 and it is initialized with the
public released fasttext4 pre-trained on Wikipedi-
a. The utterance and interlocutor are encoded by
512-dimensional and 1024-dimensional vectors,
respectively. The joint loss function with 0.0001
L2 weight is minimized by an Adam optimizer.
We implemented all the models with Tensorflow
on an NVIDIA TITAN X GPU.

4.3 Automatic Evaluation Metrics

Automatic evaluations (AEs) for Natural Lan-
guage Generation (NLG) is a challenging and
under-researched problem (Novikova et al., 2017).

3We list some general responses, such as containing “i
don’t know”, “you are welcome”.

4https://github.com/facebookresearch/fastText

Following (Liu et al., 2018), we leverage two ref-
erenced measurements (BLEU (Papineni et al.,
2002) and ROUGE (Lin, 2004)5) for automatic e-
valuations. Considering that current data-driven
approaches tend to generate short and generic
(meaningless) responses, two unreferenced (“in-
trinsic”) metrics are also leveraged to the evalu-
ation. The first one is the average length of re-
sponses, which is an objective and surfaced metric
reflected the substance of responses (Mou et al.,
2016; He et al., 2017a). The other one is the
number of nouns6 per response (Liu et al., 2018),
which shows the richness of responses since nouns
are usually content words. Note that the unrefer-
enced metrics could enrich the evaluations, though
they are weak metrics. The detailed results and
analyses are shown as follows.

4.4 The Effectiveness of ICRED for RGMPC

Model Referenced UnreferencedBLEU ROUGE Length #Noun
Seq2Seq 8.86 7.62 9.48 1.24

Persona Model 9.12 7.38 11.04 1.29
VHRED 9.38 7.65 10.25 1.55

ICRED (ours) 10.63 8.73 11.34 1.68

Table 3: Overall comparisons of ICRED.

Comparison Methods. We compared ICRED
with the following methods:

(1) Seq2Seq (Sutskever et al., 2014): Seq2Seq
is one of the mainstream methods for text gener-
ation. In order to capture as much information as
possible, the input sequence is all utterances con-
catenated in order in a context.

(2) Persona Model (Li et al., 2016b): The
persona-based model modified a Seq2Seq to en-
code a global vector for each interlocutor that ap-
pears in the training data, and it could alleviate the
issue of speaker consistency for response genera-
tion.

(3) VHRED (Serban et al., 2017): VHRED is
essentially a conditional variational auto-encoder
with hierarchical encoders, and it extends HRED
(Serban et al., 2016) by adding a high-dimensional
latent variable for utterances.

Comparative Results. Table 3 demonstrates
overall comparisons of ICRED. We can clearly ob-
tain the following observations:

5Implemented by https://github.com/Maluuba-/nlg-eval.
BLEU and ROUGE are transformed into percentages (%).

6NLTK is utilized for part-of-speech tagging.



723

Interlocutor’s Persona Model ICRED (ours)
Dialogue Turns BLEU ROUGE BLEU ROUGE

[0, 100] 8.47 6.72 10.63 8.60
(100, 1000] 8.87 7.14 10.50 8.61

(1000, 5000] 9.48 7.74 10.77 8.90
(5000, +∞) 9.51 7.80 10.60 8.79

Table 4: Performances on sparse and plentiful learn-
ing data with different numbers of interlocutor’s dia-
logue turns, where the test data is divided into different
intervals according to the number of dialogue turns in
training dataset said by target addressee (named as in-
terlocutor’s dialogue turns).

(1) ICRED obtains the highest performance on
all metrics (marked as bold), and it indicates
that incorporating interlocutor-aware context into
RGMPC contributes to generating better respons-
es.

(2) Although the persona-based model utilizes
interlocutor information, it performs poorly. The
average dialogue turn for the interlocutor is more
than 5000 in (Li et al., 2016a), while there is less
than 100 dialogue turns per interlocutor in our
dataset. Therefore, it is hard to learn a global vec-
tor for each interlocutor from the sparse corpus.
In contrast, our ICRED performs well on such a
sparse corpus (details in Section 4.5).

(3) VHRED brings slight improvements over
the Seq2Seq and persona-base model. Even that
VHRED enhances the contextual information by a
high-dimensional latent variable, VHRED is still
remarkably worse than ICRED because VHRED
neglects the interlocutor information.

4.5 The Effect of Sparse Data on ICRED

Comparison Settings. Persona model (Li et al.,
2016b) may have a sparsity issue since some in-
terlocutors have very few dialogue turns. To in-
vestigate whether ICRED has the sparsity issue or
not, we divide the test data into four intervals ac-
cording to the number of training dialogue turn-
s said by the target addressee (called interlocu-
tor dialogue turns), where small turns represen-
t sparse learning data (e.g., “[0, 100]”) and large
turns mean plentiful learning data (e.g., “(5000,
+∞)”).

Comparative Results. Table 4 reports the per-
formances of persona model and ICRED on differ-
ent interlocutor’s dialogue turns for learning. We
can clearly see that the persona model has a spar-
sity issue: it performs very poorly on sparse learn-
ing data (e.g., BLEU score = 8.47 on “[0, 100]”)

while it achieves good performances on plentiful
learning data (e.g., BLEU score = 9.51 on “(5000,
+∞)”), which demonstrates that the fixed person
vectors in the persona model need to be learned
from large-scale training data for each interlocu-
tor. In contrast, ICRED exploits interactive inter-
locutor representation learned from current dialog
context rather than the fixed person vectors ob-
tained from all training dialog utterances. There-
fore, ICRED has no sparsity issues and it performs
closely on sparse and plentiful learning data.

4.6 Ablation Study for Model Components

Model Referenced UnreferencedBLEU ROUGE Length #Noun
ICRED 10.63 8.73 11.34 1.68

w/o Adr Mem 10.25 8.23 10.73 1.27
w/o Ctx Spk Vec 10.13 8.22 10.86 1.59
w/o Ctx Adr Vec 9.95 8.18 10.93 1.26

Table 5: Ablation Experiments by removing the main
components.

Comparison Settings. In order to validate the
effectiveness of model components, we have tried
to remove some main components in decoding as
follows. (1) w/o Adr Mem: without the addressee
memory, such as removing cj in Equation 6-7; (2)
w/o Ctx Spk Vec: without the contextual speak-
er vector, such as removing Ares in Equation 6-7;
(3) w/o Ctx Adr Vec: without the contextual ad-
dressee vector, such as removing Atgt in Equation
6-7.

Comparative Results. Results of the abla-
tion study are shown in Table 5. We can see
that removing any component causes obvious per-
formance degradation. In particular, “w/o C-
tx Adr Vec” performs the worst on almost all of
the metrics, which demonstrates the importance of
contextual information for the target addressee.

4.7 The Effectiveness of Addressee Memory

Memory Type Referenced UnreferencedBLEU ROUGE Length #Noun
addressee memory 10.63 8.73 11.34 1.68

all utterance memory 10.39 8.78 11.38 1.37
latest memory 10.43 8.40 10.16 1.28

speaker memory 10.03 8.28 10.72 1.66
w/o memory 10.25 8.23 10.73 1.27

Table 6: Performances over different memory types.

Comparison Settings. In order to demonstrate
the effectiveness of the addressee memory, we



724

umount -f does n't even work

@Pricey i have right now 12.04 ubuntu i do not want 
to update any of the other packages other than btrfs

@Hanumaan the quantal kernel is out of life

@k1l i have right now this kernel `` 3.2.0-53-generic # 81-ubu-
ntu smp thu aug 22 21:01:03 utc 2013 x86_64 x86_64 x86_64

@Hanumaan you can update the original precise kernel with 
sudo apt-get update & & sudo apt-get install linux-generic

carter

Hanu
maan

k1l_

Hanu
maan

k1l_

Dialogue
Context

@Hanumaan if you want a more recent kernel you 
can install a enablement stack backports kernelk1l_

i 'm not using the OOV

you can not use the OOV of OOV ?

if you want a new kernel , you can install the 
kernel from the kernel repo

Seq2Seq

ICRED

Persona
Model

Model Response

Gold 
Response

i 'm not going to try to install the ubuntu versionVHRED

Figure 3: An example of different model responses for the same dialogue context. The input dialogue context is
on the left. The gold (referenced) response and model responses are on the top right and bottom right, respectively.
The rounded rectangle is the message box, where the italic behind “@” is the addressee, and the solid-line box
near to the message box represents the speaker or model.

change the memory type, and then the attention
model in Equation 5 is based on the new memory.
The comparison settings are shown as follows. (1)
addressee memory: memorizing contextual word
representations in the last utterance said by the tar-
get addressee (e.g., un−1 in Figure 2); (2) all utter-
ance memory: memorizing contextual word repre-
sentations in all utterances of the context (e.g., u1
to un in Figure 2); (3) latest memory: memorizing
contextual word representations of the latest utter-
ance in the context (e.g., the latest utterance un in
Figure 2); (4) speaker memory: memorizing con-
textual word representations in the last utterance
said by the responding speaker; (5) w/o memory:
without any memory.

Comparative Results. We report the results of
different memory types as shown in Table 6. It
can see that our method, the addressee memory,
achieves the best or near-best performances on al-
l metrics. Although memorizing all utterances is
competitive, the complexity of all utterance mem-
ory is n times compared with the one in the ad-
dressee memory, where n is the number of utter-
ances in a context. The speaker memory performs
closely to without memory, which indicates that
not all memories can improve the performance.

4.8 Manual Evaluations

Besides automatic evaluations, we employ manu-
al evaluations (MEs), which is important for re-
sponse generation. Similar to (He et al., 2017b;
Zhou et al., 2018), and we select three metrics for
MEs, which measure the following aspects. (1)
Fluency: measuring whether responses are gram-
matically correct or wrong. (2) Consistency: mea-
suring whether responses are coherent to the con-
text or not. (3) Informativeness: measuring how
much informational (knowledgeable) content ob-
tained from the responses.

Model Flu. Con. Inf.
ICRED vs. Seq2Seq 77.25 83.69 84.35
ICRED vs. Persona. 78.44 80.41 82.35
ICRED vs. VHRED 73.20 81.29 79.47

Table 7: Manual evaluations (%) with fluency (Flu.),
consistency (Con.) and informativeness (Inf.). The S-
core is the percentage that ICRED wins baselines after
removing the “tie” pairs.

We conduct a pair-wise comparison between the
response generated by ICRED and the one for the
same input by three typical baselines. We sample
100 responses from each compared methods. Two
curators judge (win, tie and lose) between these t-
wo methods. The Cohen Kappa of inter-annotator
statistics is 0.750, 0.658 and 0.580 for the fluen-
cy, consistency and informativeness, respective-
ly. As shown in Table 7, the score is the percent-
age that ICRED wins baselines after removing the
“tie” pairs, and we can obtain that ICRED is sig-
nificantly (sign test, p-value < 0.005) superior to
all baselines on any metric. It demonstrates our
model is able to deliver more fluent, consistent and
informative responses.

4.9 Case Study
Figure 3 shows an example of responses on d-
ifferent models for the same dialogue context.
It is clearly observed that our model (ICRED)
generates more fluent, consistent and knowledge-
able (marked as underline) responses compared to
baselines. In particular, the response given by I-
CRED “if you want a new kernel , you can install
the kernel from the kernel repo”, not only explain-
s the reason for kernel installation but also sug-
gests a source of the installation. It fully captures
the context and then produces a fluent, consistent
and knowledgeable response, which is semantical-
ly similar to the gold one.



725

4.10 Discussion
Interlocutor Prediction and RGMPC. The
above methods assume that the responding speak-
er and target addressee are given for RGMPC.
Though the speaker and the addressee could be
obtained in some situations (e.g., extracted from
chat logs), it is still a researchable task to in-
terlocutor prediction. There have been some re-
searches to predict either the responding speaker
or the target addressee based on the given textual
contexts or multimodal information (Akhtiamov
et al., 2017a; Meng et al., 2017; Akhtiamov et al.,
2017b). Nevertheless, in order to obtain the inter-
action between interlocutor prediction and RGM-
PC, we further design a joint model for RGMPC
and interlocutor prediction. Note that both the s-
peaker and the addressee are predicted based on
textual contexts, simultaneously. Firstly, the re-
sponding speaker is predicted from contexts:

p(ares|C) = σ([hC ;hnLu ] ·W ·Ares) (9)

where hC is a summary contextual vector, which
is max-pooled by the final interlocutor embedding
matrix (A), and hnLu is the hidden state of the last
utterance. W is a projected matrix. ares and Ares
are the ID and the embedding of the responding
speaker, respectively. The responding speaker is
predicted by a softmax classifier based on the em-
bedding similarity, and the target addressee is ob-
tained in the same way. Secondly, the predicted in-
terlocutors replace the gold ones for the addressee
memory and extracting interlocutor’s embeddings
from A. Finally, the interlocutor prediction loss
is added to the response generation loss for train-
ing. Table 8 shows the response generation perfor-
mance on the situation that responding interlocu-
tors are given and predicted. We can observe that:

(1) The overall performance on predicted inter-
locutors (“* / *” in Table 8) is slightly worse than
the one with gold interlocutors (the first line in Ta-
ble 8). Nevertheless, “* / *” still outperforms the
strongest baseline (VHRED in Table 3).

(2) The correctness of interlocutor prediction
has a significant impact on response generation
performance. It performs the best when the re-
sponding speaker and the target addressee are pre-
dicted correctly. “False / False” (both are mispre-
dicted) obtains the worst performance on the ref-
erenced metrics. These results demonstrate that
both responding speaker and target addressee con-
tribute to generating better responses.

Person Speaker / Referenced UnreferencedAddressee BLEU ROUGE Length #Noun
Gold True / True 10.63 8.73 11.34 1.68

Predict

∗ / ∗ 9.62 7.88 11.99 1.44
True / True 10.05 8.36 12.04 1.43
True / ∗ 9.91 8.18 11.95 1.43
∗ / True 9.89 8.21 11.97 1.43

False / False 9.20 7.41 12.18 1.47

Table 8: Performance on learning interlocutor predic-
tion and RGMPC. “True” and “False” means right and
wrong interlocutor, respectively. “*” represents both
“True” and “False”. The correctness of the responding
speaker and target addressee is segmented by “/”. For
example, “True / *” means that the responding speaker
is right, and the target addressee is right or wrong.

(3) Surprisingly, the unreferenced metrics per-
form well on “False / False”. One possible reason
is that the wrong interlocutors also capture rich
contexts, and it generates long and meaningful re-
sponses but with a weak correlation to the gold in-
terlocutors. Therefore, it achieves very poor per-
formance on the referenced metrics.

5 Related Work

Our work is inspired by a large number of appli-
cations utilizing recurrent encoder-decoder frame-
works (Cho et al., 2014) on NLP tasks such as
machine translation (Bahdanau et al., 2015) and
text summarization (Chopra et al., 2016). Recent-
ly, many researches extend the encoder-decoder
framework on response generation. HRED (Ser-
ban et al., 2016) utilizes hierarchical encoder to
capture the context. VHRED (Serban et al., 2017)
extends HRED by adding a high-dimensional la-
tent variable for utterances. These researches
demonstrate the importance of contexts on re-
sponse generation.

Our work is also inspired by researches on
multi-party chatbots. Dielmann and Renals (2008)
automatically recognize dialogue acts in multi-
party speech conversations. Recently, some s-
tudies focus on the three elements (speaker, ad-
dressee, response) on multi-party chatbots. Meng
et al. (2017) introduce speaker classification as a
surrogate task. Addressee selection is researched
by (Akhtiamov et al., 2017b). Some researches
strive to the response selection (Ouchi and Tsub-
oi, 2016; Zhang et al., 2018). However, the re-
sponse selection heavily relies on the candidates,
and it can not generate new responses in new di-
alogue contexts. Response generation could solve



726

this problem. Li et al. (2016b) learn fixed per-
son vector for response generation. Unfortunately,
it needs to be obtained from large-scale dialogue
turns, which has a sparsity issue: some interlocu-
tors have very little dialog data. Differently, our
model has no such restrictions.

6 Conclusion

In this study, we formalize a novel task of
Response Generation for Multi-Party Chatbot-
s (RGMPC) and propose an end-to-end model
which incorporates Interlocutor-aware Contexts
into Recurrent Encoder-Decoder frameworks (I-
CRED) for RGMPC. Specifically, we employ in-
teractive speaker models to capture contextual in-
terlocutor information. Moreover, we leverage an
addressee memory mechanism to enrich contextu-
al information. Furthermore, we propose to pre-
dict both the speaker and the addressee when gen-
erating responses. Finally, we construct a corpus
for RGMPC. Experimental results demonstrate the
ICRED remarkably outperforms strong baselines
on automatic and manual evaluation metrics.

Acknowledgments

This work is supported by the National Natu-
ral Science Foundation of China (No.61533018),
the Natural Key R&D Program of China
(No.2017YFB1002101), the National Natural Sci-
ence Foundation of China (No.61702512) and the
independent research project of National Labora-
tory of Pattern Recognition. This work was also
supported by CCF-DiDi BigData Joint Lab.

References
Oleg Akhtiamov, Maxim Sidorov, Alexey A. Karpov,

and Wolfgang Minker. 2017a. Speech and text anal-
ysis for multimodal addressee detection in human-
human-computer interaction. In Proceedings of IN-
TERSPEECH, pages 2521–2525.

Oleg Akhtiamov, Dmitrii Ubskii, Evgeniia Feld-
ina, Alexey Pugachev, Alexey Karpov, and Wolf-
gang Minker. 2017b. Are you addressing me?
multimodal addressee detection in human-human-
computer conversations.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. Proceedings of ICLR.

Paulo Rodrigo Cavalin Maı́ra Gatti de Bayser, Renan
Souza, Alan Braz, Heloisa Candello, Claudio S.
Pinhanez, and Jean-Pierre Briot. 2017. A hybrid

architecture for multi-party conversational systems.
CoRR, abs/1705.01214.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings
of EMNLP, pages 1724–1734.

Sumit Chopra, Michael Auli, and Alexander M. Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In Proceedings of
NAACL, pages 93–98.

Alfred Dielmann and Steve Renals. 2008. Recogni-
tion of dialogue acts in multiparty meetings using a
switching dbn. IEEE transactions on audio, speech,
and language processing, pages 1303–1314.

He He, Anusha Balakrishnan, Mihail Eric, and Per-
cy Liang. 2017a. Learning symmetric collabora-
tive dialogue agents with dynamic knowledge graph
embeddings. In Proceedings of ACL, pages 1766–
1776.

Shizhu He, Cao Liu, Kang Liu, and Jun Zhao.
2017b. Generating natural answers by incorporating
copying and retrieving mechanisms in sequence-to-
sequence learning. In Proceedings of ACL, pages
199–208.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of NAACL, pages 110–119.

Jiwei Li, Michel Galley, Chris Brockett, Georgios Sp-
ithourakis, Jianfeng Gao, and Bill Dolan. 2016b. A
persona-based neural conversation model. In Pro-
ceedings of ACL, pages 994–1003.

Chin-Yew Lin. 2004. Rouge: A package for automat-
ic evaluation of summaries. In Proceedings of ACL
workshop, page 10.

Cao Liu, Shizhu He, Kang Liu, and Jun Zhao. 2018.
Curriculum learning for natural answer generation.
In Proceedings of IJCAI, pages 4223–4229.

Zhao Meng, Lili Mou, and Zhi Jin. 2017. Hierarchical
rnn with static sentence-level attention for text-based
speaker change detection. In Proceedings of CIKM,
pages 2203–2206.

Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang,
and Zhi Jin. 2016. Sequence to backward and for-
ward sequences: A content-introducing approach to
generative short-text conversation. In Proceedings
of COLING, pages 3349–3358.

Jekaterina Novikova, Ondřej Dušek, Amanda Cer-
cas Curry, and Verena Rieser. 2017. Why we need
new evaluation metrics for nlg. In Proceedings of
EMNLP, pages 2241–2252.

http://arxiv.org/abs/1705.01214
http://arxiv.org/abs/1705.01214


727

Hiroki Ouchi and Yuta Tsuboi. 2016. Addressee and
response selection for multi-party conversation. In
Proceedings of EMNLP, pages 2133–2143.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
ACL, pages 311–318.

Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
AAAI, pages 3776–3783.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron C. Courville,
and Yoshua Bengio. 2017. A hierarchical laten-
t variable encoder-decoder model for generating di-
alogues. In Proceedings of AAAI.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural network-
s. In Proceedings of NIPS, pages 3104–3112.

Zhiliang Tian, Rui Yan, Lili Mou, Yiping Song, Yan-
song Feng, and Dongyan Zhao. 2017. How to make
context more useful? an empirical study on context-
aware neural conversational models. In Proceedings
of ACL, pages 231–236.

Turing. 1950. Computing machinery and intelligence.
Mind, pages 433–460.

Rui Zhang, Honglak Lee, Lazaros Polymenakos, and
Dragomir Radev. 2018. Addressee and response se-
lection in multi-party conversations with speaker in-
teraction rnns. In Proceedings of AAAI.

Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao,
Jingfang Xu, and Xiaoyan Zhu. 2018. Com-
monsense knowledge aware conversation generation
with graph attention. In Proceedings of IJCAI, pages
4623–4629.


