



















































Learning Bilingual Sentiment-Specific Word Embeddings without Cross-lingual Supervision


Proceedings of NAACL-HLT 2019, pages 420–429
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

420

Learning Bilingual Sentiment-Specific Word Embeddings
without Cross-lingual Supervision

Yanlin Feng and Xiaojun Wan
Institute of Computer Science and Technology, Peking University

The MOE Key Laboratory of Computational Linguistics, Peking University
{fengyanlin,wanxiaojun}@pku.edu.cn

Abstract

Word embeddings learned in two languages
can be mapped to a common space to pro-
duce Bilingual Word Embeddings (BWE). Un-
supervised BWE methods learn such a map-
ping without any parallel data. However,
these methods are mainly evaluated on tasks
of word translation or word similarity. We
show that these methods fail to capture the
sentiment information and do not perform
well enough on cross-lingual sentiment anal-
ysis. In this work, we propose UBiSE (Un-
supervised Bilingual Sentiment Embeddings),
which learns sentiment-specific word repre-
sentations for two languages in a common
space without any cross-lingual supervision.
Our method only requires a sentiment corpus
in the source language and pretrained mono-
lingual embeddings of both languages. We
evaluate our method on three language pairs
for cross-lingual sentiment analysis. Exper-
imental results show that our method out-
performs previous unsupervised BWE meth-
ods and even supervised BWE methods. Our
method succeeds for a distant language pair
English-Basque.

1 Introduction

Lack of annotated corpora degrades the qual-
ity of sentiment analysis in low-resource lan-
guages. Cross-lingual sentiment analysis tackles
this problem by adapting the sentiment resource
in a resource-rich language (the source language)
to a resource-poor language (the target language).

Bilingual Word Embeddings (BWE) provide a
way to transfer the sentiment information from the
source language to the target language. There has
been an increasing interest in BWE methods in re-
cent years, including both supervised methods and
unsupervised methods. Supervised BWE meth-
ods map the word vectors of the two languages in
a common space by exploiting either a bilingual

seed dictionary or other parallel data, while unsu-
pervised BWE methods do not utilize any form
of bilingual supervision. Yet, these methods are
mostly evaluated on tasks of word translation or
word similarity, and do not perform well enough
on cross-lingual sentiment analysis as shown in
Section 4.

Consider the case where we want to perform
sentiment analysis on the target language with
merely an annotated sentiment corpus in the
source language. We assume pretrained monolin-
gual embeddings of both languages are available
to us. One solution is to first align the embeddings
of both languages in a common space using un-
supervised BWE methods, then train a classifier
based on the source sentiment corpus. In this so-
lution, no sentiment information is utilized to learn
the alignment.

In this paper, we propose to exploit the sen-
timent information and learn sentiment-specific
alignment. The sentiment information is gradu-
ally incorporated into the BWE through an itera-
tive constraint relaxation procedure. Unlike pre-
vious work which performed alignment in a single
direction by linearly mapping the source vectors to
the target vector space, we propose an alignment
model that maps the vectors of the two languages
to a new shared space with two non-linear trans-
formations. Our model is able to separate posi-
tive vectors from negative vectors in the bilingual
space and allow such sentiment information to be
transferred to the target language. Our main con-
tributions are as follows:

1. We propose a novel approach to learn
bilingual sentiment-specific word embed-
dings without any cross-lingual supervision
and perform cross-lingual sentiment analy-
sis with minimum resource requirement. We
propose an iterative constraint relaxation pro-



421

cedure that gradually incorporates the senti-
ment information into the BWE. Our pro-
posed approach achieves state-of-the-art re-
sults.

2. We introduce a novel sentiment-specific ob-
jective without having to explicitly build a
classifier. Our approach is more explainable
and better balances sentimental similarity and
semantic similarity compared to previous ap-
proaches.

3. We introduce an alignment-specific objective
and a simple re-normalization trick. Unlike
previous BWE methods that learn orthogo-
nal mappings, we introduce non-orthogonal
mappings which enable the transfer of senti-
ment information from the source language
to the target language.

2 Related Work

Cross-Lingual Sentiment Analysis Existing
approaches for cross-lingual sentiment analysis
can be mainly divided into two categories: (i) ap-
proaches that rely on machines translation (MT)
systems (ii) approaches that rely on cross-lingual
word embeddings.

Standard MT-based approaches perform cross-
lingual sentiment analysis by translating the sen-
timent data into a selected language (e.g. En-
glish). More sophisticated algorithms includ-
ing co-training (Wan, 2009; Demirtas and Pech-
enizkiy, 2013) and multi-view learning (Xiao and
Guo, 2012) have been shown to improve perfor-
mance.

Zhou et al. (2015, 2016b,a) performed cross-
lingual sentiment analysis by learning bilingual
document representations. These methods trans-
late each document into the other language and
enforce a bilingual constraint between the original
document and the translated version.

Bilingual Word Embeddings Word embed-
dings trained separately on two languages can be
aligned in a shared space to produce Bilingual
Word Embeddings (BWE), which support many
NLP tasks including machine translation (Lam-
ple et al., 2017), cross-lingual sentiment analysis
(Barnes et al., 2018; Zhou et al., 2015) and cross-
lingual dependency parsing (Guo et al., 2015).
BWE can be obtained in a supervised way using a
seed dictionary (Joulin et al., 2018; Artetxe et al.,

2016), or in an unsupervised way without any
bilingual data. Adversarial training was the first
successful attempt to learn unsupervised BWE
(Zhang et al., 2017; Conneau et al., 2017). Self-
learning was proposed by (Artetxe et al., 2017)to
learn BWE with minimum bilingual resources,
which was later extended into a fully unsupervised
framework by adding an unsupervised dictionary
initialization step (Artetxe et al., 2018).

Multilingual Word Embeddings BWE meth-
ods can be extended to the case of multiple lan-
guages by simply mapping all the languages to
the vector space of a selected language. How-
ever, directly learning multilingual word embed-
dings (MWE) in a shared space has been shown to
improve performance (Ammar et al., 2016; Duong
et al., 2017; Chen and Cardie, 2018; Alaux et al.,
2018). Yet, all these approaches are mainly eval-
uated on word translation and their effectiveness
on cross-lingual sentiment analysis have not been
empirically compared.

Sentimental Embeddings Continuous word
representations encode the syntactic context
of a word but often ignore the information of
sentiment polarity. This drawback makes them
hard to distinguish words with similar syntactic
context but opposite sentiment polarity (e.g. good
and bad), resulting in unsatisfactory performance
on sentiment analysis. Tang et al. (2014) learned
word representations that encode both syntactic
context and sentiment polarity by adding an ob-
jective to classify the polarity of an n-gram. This
method can be generalized to the cross-lingual
setting by training monolingual sentimental
embeddings on both languages then aligning
them in a common space. However, it requires
sentiment resources in the target language thus is
impractical for low-resource languages.

There are also approaches to learn sentimen-
tal embeddings in the bilingual space without any
sentiment resources in the target language. Barnes
et al. (2018) jointly minimized an alignment ob-
jective based on a seed dictionary, and a clas-
sification objective based on the sentiment cor-
pus. Its performance is compared to our method
in Section 4. Xu and Wan (2017) learned multi-
lingual sentimental embeddings by extending the
BiSkip model (Luong et al., 2015). However, their
method does not apply to pretrained embeddings
and requires large-scale parallel corpora thus is not



422

included in our experiments.

3 Proposed Method

3.1 The Overall Framework

This subsection first introduces the proposed map-
pings for aligning the monolingual embeddings in
the bilingual space, then describes the general self-
learning algorithm used to learn these bilingual
mappings. The details of our algorithm are ex-
plained in Section 3.2 - Section 3.6.

3.1.1 The Alignment Model
We assume we have normalized monolingual em-
beddings S ∈ Rv×d and T ∈ Rv×d, where the i-th
row of S is the vector representation of word i in
the source language. The normalization procedure
is as follows: (i) l2-normalize each vector (ii) cen-
ter the vectors (iii) l2-normalize each vector again
(Artetxe et al., 2018).

Given these monolingual embeddings, existing
BWE methods typically learn a projection ma-
trix W ∈ Rd×d from the source vector space to
the target vector space. However, these meth-
ods are unsuitable in our setting for two reasons:
(i) most methods constrain W to be orthogonal
or near-orthogonal, thus preserving distances be-
tween word vectors; (ii) word vectors in the tar-
get language space remain unchanged. These two
properties prevent us from separating words with
opposite sentiment polarity in the bilingual space.
In this work, we propose to align the monolingual
embeddings with two non-linear mappings:

fs(x) =
Wsx

‖Wsx‖
ft(x) =

Wtx

‖Wtx‖

where ‖ · ‖ denotes the l2-norm, Ws (Wt) is the
projection matrix for the source (target) embed-
dings, and x is a d-dimension word vector. Each
mapping can be seen as a linear projection fol-
lowed by a re-normalization step.

We propose the following convex domain D =
{W ∈ Rd×d | ‖W‖2 ≤ r} as an alternative for
the orthogonal constraint, where ‖ · ‖2 denotes the
spectral norm and r is a hyperparameter that de-
termines to what extent we want to preserve word
distances. This is inspired by the unit spectral
norm constraint proposed by (Joulin et al., 2018).

3.1.2 The Self-learning Procedure
Given a bilingual seed dictionary, we can learn the
projection matrices Ws and Wt by forcing the

word pairs in the dictionary to have similar rep-
resentations in the bilingual space. In the unsuper-
vised case, such a dictionary can be induced from
the monolingual embeddings S and T (Artetxe
et al., 2018). However, the quality of this dictio-
nary is usually not good, which in turn degrades
the quality of the projection matrices learned from
this dictionary. Previous work (Artetxe et al.,
2017, 2018) showed that an iterative self-learning
procedure can induce a good bilingual dictionary
and hence good projection matrices. Given an ini-
tial dictionaryDbi, this procedure iterates over two
steps: (i) it aligns the monolingual embeddings in
a common space based onDbi, yielding S′ and T′;
(ii) it computes a new dictionaryDbi using nearest
neighbour retrieval over the approximately aligned
embeddings S′ and T′.

In our method, there are three objects Ws, Wt
and Dbi to update through the self-learning pro-
cedure. Thus we iterates over the following three
steps:

1. Solve Ws by minimizing a sentiment-
specific objective Ls over D, as described in
Section 3.3;

2. Solve Wt by minimizing an alignment-
specific objective Lt over D, as described in
Section 3.4;

3. Derive a new bilingual dictionary Dbi based
on S′ = SW>s and T

′ = TW>t , as de-
scribed in Section 3.5.

Re-normalization is applied as a final step after
we have obtained Ws and Wt.

3.2 Preliminaries
3.2.1 Unsupervised Bilingual Dictionary

Initialization
The normalized embeddings S and T are not
aligned along the first axis, i.e., the i-th row
of S does not correspond to the i-th row of
T. Therefore, an initial bilingual dictionary is
required in order to access the correspondence
between the two languages. Following (Artetxe
et al., 2018), we first compute the similarity
matrices Ms =

√
SS> and Mt =

√
TT>,

sort them along the second axis and normalize
the rows, yielding M′s and M

′
t. For each row

in M′s, we apply nearest neighbour retrieval
over the rows of M′t to find its correspond-
ing translation, yielding a dictionary Ds→t =



423

{(1, Ts→t(1)), (2, Ts→t(2)), . . . , (v, Ts→t(v))},
where Ts→t(i) is the translation of the source
word i. The same procedure is repeated in the
other direction, yielding Dt→s. The two dictio-
naries are then concatenated to produce the initial
bilingual dictionary Dbi = Ds→t ∪Dt→s.

3.2.2 Learning Sentiment-Specific Vectors
In order to incorporate the sentiment information
into the bilingual word embeddings, we need a set
of d-dimension vectors with known sentiment po-
larity. We propose a neural network based ap-
proach to learn these sentiment-specific vectors.
Let the training corpus in the source language be
C = {(z1, y1), (z2, y2), . . . , (z|C|, y|C|)}, where zi
is a text and yi is its corresponding label. A d-
dimension vector with sentiment polarity yi can
be obtained by calculating the weighted average
of the word vectors in zi:

hi =

∑
j∈zi exp(αj)Sj·∑
j∈zi exp(αj)

(1)

where Sj· is the vector representation of the word
j in the source language (corresponding to the j-
th row of S) and αj is a scalar that scores the im-
portance of word j on the sentiment polarity. αj
is computed by αj = max(ASj· + b), where
A ∈ Rh×d and b ∈ Rh are the parameters to
learn. This function can be seen as a convolu-
tion layer with h filters followed by a max pooling
layer. The number of filters h is set to 4. Each hi
is then forwarded to a linear classifier to predict
the sentiment label yi.

Once we have trained the model by minimiz-
ing the cross-entropy loss, we re-compute hi for
each training example zi. We denote the set of
vectors (i.e., hi) with positive labels as P =
{hp1,h

p
2, . . . ,h

p
|P|} and the set of vectors with neg-

ative labels as N = {hn1 ,hn2 , . . . ,hn|N |}. In the
4-class setup, we have four sets: P , N , SP (the
set of strongly positive vectors), SN (the set of
strongly negative vectors).

3.3 Solving Ws
Given a set of positive d-dimension vectors
P = {hp1,h

p
2, . . . ,h

p
|P|} and a set of negative d-

dimension vectors N = {hn1 ,hn2 , . . . ,hn|N |} (or
four sets in the 4-class setup), our goal is to distin-
guish the positive vectors from the negative vec-
tors in the bilingual space, i.e., to separate Wsh

p
i

from Wshnj for any pair of i, j.

We introduce a new d-dimension vector ap ∈
O = {x ∈ Rd | ‖x‖ ≤ 1} to represent the
“positive direction”, which is to be learned. In or-
der to separate positive vectors from negative vec-
tors in the bilingual space, we try to make Wsh

p
i

(i = 1, . . . , |P|) to be close to ap and Wshni
(i = 1, . . . , |N |) to be distant from ap.

For a given ap, we first compute ap
>
Wsh

p
i for

i = 1, 2, . . . , |P| and denote the set of i with
λ|P| smallest values as Qp+, where λ ∈ [0, 1] is
a hyperparameter1. These Wsh

p
i are least sim-

ilar with ap (dot product is used as the similar-
ity metric), hence we maximize the average of
ap
>
Wsh

p
i over Q

p
+. Likewise, we denote the

set of i ∈ {1, 2, . . . , |N |} with λ|N | largest val-
ues of ap

>
Wsh

n
i as Q

p
−. These Wsh

n
i are most

similar to ap, hence we minimize the average of
ap
>
Wsh

n
i over Q

p
−. The overall objective is as

follows:

min
Ws∈D
ap∈O

Ls(Ws,ap) = L′(Ws,ap,P,N )

∆
= − 1

λ|P|
∑
i∈Qp+

ap
>
Wsh

p
i

+
1

λ|N |
∑
i∈Qp−

ap
>
Wsh

n
i (2)

whereD is the convex set defined in Section 3.1.1.
The rationale for this objective is that, instead of
forcing every Wsh

p
i to be close to a

p, we only
focus on a fraction of positive vectors that are most
distant from ap, and vice versa for those negative
vectors. We observe that this objective can be re-
written as:

min
Ws∈D
ap∈O

Ls(Ws,ap)

=
1

λ|P|
max

Q∈Sλ|P|(|P|)
−
∑
i∈Q

ap
>
Wsh

p
i

+
1

λ|N |
max

Q∈Sλ|N|(|N |)

∑
i∈Q

ap
>
Wsh

n
i (3)

where Sλ|P|(|P|) represents all subsets of
{1, 2, . . . , |P|} of size λ|P|, and Sλ|N |(|N |) is
similarly defined. 2 This formulation shows that

1For simplicity, we assume λ|P| is already rounded to an
integer.

2There is no need to introduce a new vector an to repre-
sent the “negative direction” and introduce a new objective,
since the new objective is exactly the same after replacing ap

with −an.



424

both terms of this objective can be seen as a
maximum of linear functions of either Ws or ap.
Therefore, our objective is convex with respect to
either Ws or ap, thus can be efficiently minimized
by using the projected gradient descent algorithm.
We first minimize this objective with respect to ap

over O, then minimize it with respect to Ws over
D.

While this objective is useful in the binary
setup, it does not separate a strongly positive vec-
tor in SP from a weakly positive vector in P (sim-
ilarly for SN and N ). In order to achieve bet-
ter performance in the 4-class setup, we adopt the
one-versus-rest strategy to write Ls as the sum of
four terms:

min
Ws∈D
ap∈O
asp∈O
an∈O
asn∈O

Ls(Ws,ap,asp,an,asn)

= L′(Ws,ap,P,N ∪ SP ∪ SN )
+ L′(Ws,asp,SP,P ∪N ∪ SN )
+ L′(Ws,an,N ,P ∪ SP ∪ SN )
+ L′(Ws,asn,SN ,P ∪ SP ∪N ) (4)

where L′ is defined in Eq.(2) and ac is a d-
dimension vector representing the “direction” of
class c.

3.4 Solving Wt
Based on the current bilingual dictionary Dbi, we
construct two sets of vectors {xs1,xs2, . . . ,xs2v}
and {xt1,xt2, . . . ,xt2v}, where xsi and xti are the
vector representations of the i-th word pair in Dbi.
With Ws fixed, we can solve Wt by minimizing:

min
Wt∈D

Lt(Wt) =
2v∑
i=1

‖Wsxsi −Wtxti‖2 (5)

whereD is the convex set defined in Section 3.1.1.
This objective is convex with respect to Wt, thus
can be minimized efficiently by using the pro-
jected gradient descent algorithm.

3.5 Bilingual Dictionary Induction
Once we have computed Ws and Wt, we can
obtain the aligned embeddings S′ = SW>s and
T′ = TW>t . Then we induce a new dictio-
naryDbi using nearest neighbour retrieval over the
rows of S′ and T′. We perform the induction in
two directions to produce Ds→t and Dt→s, then
concatenate them to produce Dbi.

In this work, we propose a modified version
of CSLS(Conneau et al., 2017) to be used as the
similarity metric to preform nearest neighbour re-
trieval:

CSLS′(x,y) = x>y

− 1
k

∑
y′∈NY (x)

x>y′ − 1
k

∑
x′∈NX(y)

x′>y (6)

whereNY (x) is the set of k nearest neighbours of
x in the set of vectors Y (in our case Y is the set of
rows of T′). We set k to 10 following the original
paper.

3.6 Iterative Constraint Relaxation

As mentioned in Section 3.1.1, we introduce a hy-
perparameter r to define the convex domain D.
There is a trade-off to make for r: a large r bet-
ter incorporates sentimental similarity but signifi-
cantly harms the quality of the alignment, while a
small r constrains Ws to be near-orthogonal thus
prevents it to capture the sentimental similarity.

In order to address this problem, we propose to
first set r to 1, letting the the monolingual embed-
dings to be properly aligned. Then r is iteratively
increased by ∆r, causing the positive vectors in
the bilingual space to be gradually moved further
away from the negative vectors. The training pro-
cess stops when r reaches a maximum value rmax,
where rmax is a hyperparameter 3.

The pseudo code of UBISE in the binary setup
is shown in Algorithm 1. For the 4-class UBISE,
lines 3,6,7 are replaced by their counterparts in the
4-class setup.

4 Experiments

4.1 Datasets

We use the multilingual sentiment dataset pro-
vided by (Barnes et al., 2018). It contains anno-
tated hotel reviews in English (EN), Spanish (ES),
Catalan (CA) and Basque (EU). In our experi-
ment, we use EN as the source language and ES,
CA, EU as the target languages. For each target
language, the dataset is divided into a target de-
velopment set and a target test set. We also com-
bine the strong and weak labels to produce a bi-
nary setup.

3Although having the number of iterations be implicitly
defined by rmax and ∆r makes choosing a small rmax im-
practical, it allows us to tune rmax in a single training pro-
cess.



425

Algorithm 1 binary UBISE
Input: λ, rmax,∆r,S,T, C
Output: S′,T′,Ws,Wt

1: r ← 1
2: Initialize Ws and Wt to identity matrices
3: Learn P , N from S and C, according to Sec-

tion 3.2.2
4: Compute the initial bilingual dictionary Dbi

from S and T, according to Section 3.2.1
5: while r ≤ rmax do
6: ap ← argminap∈OLs(ap,Ws)
7: Ws ← argminWs∈DLs(a

p,Ws)
8: Wt ← argminWt∈DLt(Wt)
9: S′ ← SW>s

10: T′ ← TW>t
11: Derive a new bilingual dictionary Dbi from

S′ and T′, according to Section 3.5
12: r ← r + ∆r
13: end while
14: Normalize the rows of S′, T′ to unit length
15: return S′,T′,Ws,Wt

The normalized 300-dimension fastText vectors
(Bojanowski et al., 2017) are used by all methods.

The MUSE dataset(Conneau et al., 2017) is
used by approaches that require bilingual super-
vision 4. Each dictionary contains 5000 unique
source words.

4.2 Implementation details
We empirically set ∆r = 0.01 and v = 10000.
The vocabulary of each language is limited to the v
most frequent words so that the embedding matrix
has shape v × d. Hyper parameters λ and rmax
are tuned on the target development set via a grid
search. We apply stochastic dictionary induction
by randomly setting the elements of the similarity
matrix used for nearest neighbour retrieval to zero
with probability 1 − p, as described in (Artetxe
et al., 2018). p is initialized to 0.1 and increased
by 0.005 at each iteration. We empirically stop
updating the dictionary when r exceeds 3.

4.3 Baselines
We compare our method with the following base-
lines, including state-of-the-art BWE methods
that are originally evaluated on the word transla-
tion task, as well as bilingual sentimental embed-

4This dataset does not contain a dictionary for EN-EU,
thus we translate the EN-ES dictionary into EN-EU

dings methods that are optimized for cross-lingual
sentiment analysis. The bilingual word embed-
dings learned by each method are later evaluated
on cross-lingual sentiment analysis using the same
classifier for fairness.

4.3.1 Unsupervised BWE Methods
ADVERSARIAL Conneau et al. (2017) pro-
posed an unsupervised BWE method based on ad-
versarial training. After a near-orthogonal projec-
tion matrix is learned through adversarial training,
a refinement procedure is applied to improve the
quality of the alignment.

VECMAP Artetxe et al. (2018) proposed an un-
supervised BWE learning framework. It consists
of an unsupervised dictionary initialization step
and the self-learning procedure mentioned in Sec-
tion 3.1.2.

4.3.2 Supervised BWE Methods
PROCRUSTES Artetxe et al. (2016) proposed
a simple and effective supervised BWE method
that requires a seed dictionary. It computes the
optimal projection matrix by taking singular value
decomposition (SVD).

RCSLS Joulin et al. (2018) proposed an su-
pervised BWE method that also requires a seed
dictionary. They proposed a training objective
that is consistent with the retieval criterion that
can be minimized by using gradient descent. It
achieves state-of-the-art results on the word trans-
lation task.

4.3.3 Bilingual Sentimental Embedding
Methods

BLSE Barnes et al. (2018) exploited both bilin-
gual supervision and the sentiment corpus to learn
bilingual sentimental embeddings. They jointly
minimize an alignment-specific objective and a
classification objective to learn the projection ma-
trices. The trade-off between the two objectives
is controlled by a hyperparameter α ∈ [0, 1]. We
tune α on the target development set as described
in the original paper. Once the projection matri-
ces have been learned, the classifier in this model
is abandoned. The quality of the resulting BWE is
evaluated using the classifier mentioned in Section
4.4.

4.4 Evaluation
We use DAN (Iyyer et al., 2015) as the classifier to
preform cross-lingual sentiment analysis. The loss



426

of each instance is weighted by its inverse class
frequency to address the class imbalance problem.
For each method, the dropout rate is fixed at 0.3
and the l2-regularization strength is tuned on the
target development set5. We train five classifiers
for each method and report the average macro-F1
on the target test set.

4.5 Results and Analysis
Table 1 presents the results of different BWE
methods. UBISE outperforms all unsupervised
methods on all six tasks and outperforms all base-
lines on four out of six tasks.

All methods, especially unsupervised methods,
suffer from distant language pairs, which is con-
sistent with the observation of (Søgaard et al.,
2018). VECMAP and ADVERSARIAL perform
significantly worse on EN-EU compared to su-
pervised methods. Yet, UBISE outperforms the
strongest baseline by 2.1% on EN-EU, indicat-
ing that incorporating sentiment is vital for cross-
lingual sentiment analysis on distant languages.

Despite the similar performance across differ-
ent BWE methods in the binary setup, UBISE
outperform all baselines in the 4-class setup by a
large margin (average of +2.2%). This may indi-
cate that the original monolingual embeddings are
able to distinguish positive words from negative
words(e.g., good and bad), but bad at distinguish-
ing strongly positive words from weakly positive
words (e.g., good and perfect).

The performance of BLSE is merely compar-
ative with other baselines.6 We suspect that this
is due to the classifier we use to perform cross-
lingual sentiment analysis. The original paper
used SVM or logistic regression to perform classi-
fication, in which case BLSE achieved better per-
formance due to the utilization of sentiment infor-
mation. But if we use a deeper neural network to
perform cross-lingual sentiment analysis, preserv-
ing the original semantic similarity is more impor-
tant. A qualitative comparison between BLSE and
UBISE is presented in Section 4.8.

4.6 Effect of the Sentiment Information
We perform an ablation test to demonstrate the ef-
fect of the sentiment information provided by Ls.

5The optimal regularization strength depends on the
BWE method. Stronger regularization is favourable to
BLSE and UBISE.

6We already obtain significantly better results after replac-
ing the original classifier with DAN, compared with the orig-
inal reported results.

We create a new model UBISE MIN that does not
utilize the sentiment information by eliminating
lines 6,7,12 in Algorithm 1. UBISE MIN runs
500 iterations for every language pairs.

The comparative results in Table 2 show that
utilizing the sentiment information leads to an av-
erage improvement of +3.1% in the binary setup
or +4.1% in the 4-class setup.

4.7 Effect of Re-Normalization

Re-normalization is useful in the sense that it leads
to better alignment by constraining all the bilin-
gual vectors to be on the unit sphere. While
this property does not matter for word transla-
tion as long as cosine-similarity is used as the re-
trieval criterion, it matters for cross-lingual senti-
ment analysis. Another effect of re-normalization
is that it introduces non-linearity between the lin-
ear projection and the classifier, which is vital for
separating words with opposite sentiment polarity.
Without non-linearity the linear projection and the
first layer of the classifier would collapse into a
single linear projection, thus eliminating the ef-
fect of Ws. Figure 2 illustrates how this non-
linearity helps separating positive words from neg-
ative words in the bilingual space. This effect is
demonstrated in Section 4.8.

4.8 Visualization of the Bilingual Space

To illustrate how UBISE transfers sentiment in-
formation from the source language to the target
language, we visualize six categories of words in
the bilingual space of UBISE and BLSE using
t-SNE (Maaten and Hinton, 2008). As shown in
Figure 1, both methods manage to separate pos-
itive words from negative words without any an-
notated data in Spanish. However, Barnes et al.
(2018) abandon the original semantic similarity,
which degrades its performance as shown in Sec-
tion 4.5. In contrast, our method preserves seman-
tic similarity by limiting the largest singular val-
ues of Ws and Wt to be smaller than rmax. The
trade-off between semantic similarity and senti-
mental similarity is made by choosing an appro-
priate rmax.

5 Conclusion

This paper presents a method to learn bilingual
sentiment-specific word embeddings without any
cross-lingual supervision. We propose a novel
sentiment-specific objective that separates words



427

Binary 4-class
Bilingual Supervision Method ES CA EU ES CA EU

5k dict.
PROCRUSTES 80.4 83.1 74.1 49.1 50.9 43.0

RCSLS 80.7 81.4 73.4 50.3 47.8 41.3
BLSE 80.2 82.2 73.5 50.0 47.0 35.1

None
VECMAP 80.0 80.2 69.2 51.2 52.2 38.8

ADVERSARIAL 79.8 79.9 60.3 45.8 47.9 31.0
None UBISE 80.5 80.4 76.7 54.4 54.1 44.6

Table 1: Macro F1 of different BWE approaches. The best score for each language pair is shown in bold. The
best score among unsupervised BWE methods for each language pair is underlined.

pos neg animals transport grammatical verbs

Figure 1: t-SNE Visualization of the Spanish word vectors. Grammatical words include pronouns, prepositions,
articles, conjunctions, etc. left: original normalized vectors; middle: the bilingual space of binary UBISE with
λ = 0.5 and rmax = 5.5; right: the bilingual space of BLSE.

Setup Method ES CA EU

Binary
UBISE MIN 77.4 80.2 70.8

UBISE 80.5 80.4 76.7

4-class
UBISE MIN 51.7 49.3 39.9

UBISE 54.4 54.1 44.6

Table 2: Comparison between UBISE and
UBISE MIN

.

with opposite sentiment polarity in the bilingual
space, and an alignment objective that enables the
transfer of sentiment information from the source
language to the target language. An iterative con-
straint relaxation procedure is applied to grad-
ually incorporate the sentiment information into
the bilingual word embeddings. We empirically
evaluate our method on three language pairs for
cross-lingual sentiment analysis and demonstrate
its effectiveness. Experimental results show that
incorporating sentiment information significantly
improves the performance on fine-grained cross-
lingual sentiment analysis.

(a) (b) (c)

Figure 2: Illustration of the effect of re-normalization.
(a) the original normalized embeddings (b) embed-
dings after linear projection (c) embeddings after re-
normalization

Acknowledgment

This work was supported by National Natural Sci-
ence Foundation of China (61772036) and Key
Laboratory of Science, Technology and Standard
in Press Industry (Key Laboratory of Intelligent
Press Media Technology). We appreciate the
anonymous reviewers for their helpful comments.
Xiaojun Wan is the corresponding author.



428

References
Jean Alaux, Edouard Grave, Marco Cuturi, and Ar-

mand Joulin. 2018. Unsupervised hyperalignment
for multilingual word embeddings. arXiv preprint
arXiv:1811.01124.

Waleed Ammar, George Mulcaire, Yulia Tsvetkov,
Guillaume Lample, Chris Dyer, and Noah A Smith.
2016. Massively multilingual word embeddings.
arXiv preprint arXiv:1602.01925.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
451–462.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.
A robust self-learning method for fully unsupervised
cross-lingual mappings of word embeddings. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 789–798.

Jeremy Barnes, Roman Klinger, and Sabine Schulte im
Walde. 2018. Bilingual sentiment embeddings:
Joint projection of sentiment across languages.
arXiv preprint arXiv:1805.09016.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Xilun Chen and Claire Cardie. 2018. Unsupervised
multilingual word embeddings. arXiv preprint
arXiv:1808.08933.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2017.
Word translation without parallel data. arXiv
preprint arXiv:1710.04087.

Erkin Demirtas and Mykola Pechenizkiy. 2013. Cross-
lingual polarity detection with machine translation.
In Proceedings of the Second International Work-
shop on Issues of Sentiment Discovery and Opinion
Mining, page 9. ACM.

Long Duong, Hiroshi Kanayama, Tengfei Ma, Steven
Bird, and Trevor Cohn. 2017. Multilingual training
of crosslingual word embeddings. In Proceedings of
the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
1, Long Papers, volume 1, pages 894–904.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2015. Cross-lingual depen-
dency parsing based on distributed representations.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 1234–1244.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daumé III. 2015. Deep unordered com-
position rivals syntactic methods for text classifica-
tion. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), vol-
ume 1, pages 1681–1691.

Armand Joulin, Piotr Bojanowski, Tomas Mikolov,
Hervé Jégou, and Edouard Grave. 2018. Loss in
translation: Learning bilingual word mapping with a
retrieval criterion. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2979–2984.

Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
and Marc’Aurelio Ranzato. 2017. Unsupervised
machine translation using monolingual corpora only.
arXiv preprint arXiv:1711.00043.

Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015. Bilingual word representations with
monolingual quality in mind. In Proceedings of the
1st Workshop on Vector Space Modeling for Natural
Language Processing, pages 151–159.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579–2605.

Anders Søgaard, Sebastian Ruder, and Ivan Vulić.
2018. On the limitations of unsupervised
bilingual dictionary induction. arXiv preprint
arXiv:1805.03620.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1555–
1565.

Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 1-
volume 1, pages 235–243. Association for Compu-
tational Linguistics.

Min Xiao and Yuhong Guo. 2012. Multi-view ad-
aboost for multilingual subjectivity analysis. Pro-
ceedings of COLING 2012, pages 2851–2866.



429

Kui Xu and Xiaojun Wan. 2017. Towards a universal
sentiment classifier in multiple languages. In Pro-
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 511–
520.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017. Adversarial training for unsupervised
bilingual lexicon induction. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 1959–1970.

Huiwei Zhou, Long Chen, Fulin Shi, and Degen
Huang. 2015. Learning bilingual sentiment word
embeddings for cross-language sentiment classifica-
tion. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), vol-
ume 1, pages 430–440.

Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2016a.
Attention-based lstm network for cross-lingual sen-
timent classification. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 247–256.

Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2016b.
Cross-lingual sentiment classification with bilingual
document representation learning. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 1403–1412.


