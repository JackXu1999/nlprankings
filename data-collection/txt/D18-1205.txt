




































Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1787–1796
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1787

Improving Neural Abstractive Document Summarization with
Explicit Information Selection Modeling∗

Wei Li1,2,3 Xinyan Xiao2 Yajuan Lyu2 Yuanzhuo Wang1
1Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China

2Baidu Inc., Beijing, China
3University of Chinese Academy of Sciences, Beijing, China

weili.ucas.ict@gmail.com, {xiaoxinyan,lvyajan}@baidu.com,
wangyuanzhuo@ict.ac.cn

Abstract

Information selection is the most important
component in document summarization task.
In this paper, we propose to extend the basic
neural encoding-decoding framework with an
information selection layer to explicitly model
and optimize the information selection pro-
cess in abstractive document summarization.
Specifically, our information selection layer
consists of two parts: gated global informa-
tion filtering and local sentence selection. Un-
necessary information in the original docu-
ment is first globally filtered, then salient sen-
tences are selected locally while generating
each summary sentence sequentially. To op-
timize the information selection process di-
rectly, distantly-supervised training guided by
the golden summary is also imported. Exper-
imental results demonstrate that the explicit
modeling and optimizing of the information
selection process improves document summa-
rization performance significantly, which en-
ables our model to generate more informative
and concise summaries, and thus significantly
outperform state-of-the-art neural abstractive
methods.

1 Introduction

Document summarization is the task of generat-
ing a fluent and condensed summary for a docu-
ment while retaining the gist information. There
are two prominent approaches: extractive methods
and abstractive methods. Extractive methods gen-
erate summary for a document by directly select-
ing salient sentences from the original document.
On the contrary, abstractive methods synthesize
information from the input document to generate
summary using arbitrary words and expressions -
as human usually do. Recent neural models en-
able an end-to-end framework for natural language

∗This work was done while the first author was doing in-
ternship at Baidu Inc.

generation, which inspires the research on abstrac-
tive document summarization.

Most existing work directly apply the neu-
ral encoding-decoding framework, which first
encodes the input into an abstract representa-
tion and then decodes the output based on the
encoded information. Although the encoding-
decoding framework has achieved huge success
on some text generation tasks like machine trans-
lation (Bahdanau et al., 2014) and image caption
(Vinyals et al., 2015), the performance on abstrac-
tive document summarization is much less con-
vincing. Since document summarization is a spe-
cial natural language generation task that requires
information selection, the performance of current
neural abstractive methods even has a considerable
gap from extractive methods.

The most essential prerequisite for a practical
document summarization system is that the gen-
erated summary must contain the salient informa-
tion of the original document. Since a document is
a long sequence of multiple sentences, both global
document information and local inter-sentence re-
lations need to be properly modeled in the infor-
mation selection process. Although the encoding-
decoding framework has implicitly modeled the
information selection process via end-to-end train-
ing, we argue that abstractive document summa-
rization shall benefit from explicitly modeling and
optimizing it by capturing both the global doc-
ument information and local inter-sentence rela-
tions.

In this paper, we propose to extend the
encoding-decoding framework to model the in-
formation selection process explicitly. We treat
the document summarization as a three-phase
task: document encoding, information selection
and summary decoding. Correspondingly, our
model consists of three layers: a document en-
coder layer, an information selection layer and a



1788

Docum
ent Encoder

Inform
ation Selection

Sum
m

ary 
Decoder

α 0

α1

α1

α 2

α 2

α 3

h1 h2 h3 h4 h5 h6 h7 h8

h11

h12

h13

h21

h22

h23

h31

h32

h33

h41

h42

h43

h51

h52

h53

h61

h62

h63

h71

h72

h73

h81

h82

h83

f1 f2 f3 f4 f5 f6 f7 f8

h1
' h2

' h3
'

h1,1
' h2,1

' h3,1
'h1,2

' h2,2
' h3,2

'h1,3
' h2,3

' h3,3
'

Figure 1: Our abstractive document summarization model,
which mainly consists of three layers: document encoder
layer (the top part), information selection layer (the middle
part) and summary decoder layer (the bottom part).

summary decoder layer, as shown in Figure 1.
In our model, both the document and summary
are processed sentence by sentence, to better cap-
ture the inter-sentence relations. The informa-
tion selection layer consists of two parts: gated
global information filtering and local sentence se-
lection. Unnecessary information in the original
document are first globally filtered by a gated net-
work, then important sentences are selected lo-
cally while generating each summary sentence se-
quentially. Moreover, we propose to optimize
the information selection process with distantly-
supervised training. Our proposed method com-
bines the strengths of extractive methods and ab-
stractive methods, which is able to tackle the fac-
tors of saliency, non-redundancy, coherence and
fluency under a unified framework. We conduct
extensive experiments on benchmark datasets and
the results demonstrate that the explicit modeling
and distantly-supervised optimizing of the infor-
mation selection process improves document sum-
marization performance significantly, which en-
ables our model to significantly outperforms pre-
vious state-of-the-art neural abstractive methods.

2 Our Model

As shown in Figure 1, our model consists of a hi-
erarchical document encoder, an information se-
lection layer and an attention-equipped decoder.
Firstly, the hierarchical encoder encodes the doc-
ument sentence by sentence, and word by word
in each sentence. Then the information selection
layer selects and filters the sentence representa-

tions based on the global document representation.
A sentence selection RNN is used to select salient
and relevant sentences while generating each sum-
mary sentence sequentially based on the tailored
sentence representations. At last, the summary de-
coder produces the output summary to paraphrase
and generalize the selected sentences.

In the following, we denote hi, hi,j as the hid-
den state of the i-th sentence and the j-th word of
the i-th sentence in the document encoder part, re-
spectively. In the information selection and sum-
mary decoder part, we denote h′t, h

′
t,k as the hid-

den state of the t-th summary sentence and the
k-th word in the t-th summary sentence, respec-
tively.

2.1 Document Encoder
A document d is a sequence of sentences d =
{si}, and each sentence is a sequence of words
si = {wi,j}. A hierarchical encoder, which con-
sists of two levels: word level and sentence level
similar to (Nallapati et al., 2016), is used to encode
the document from both word and sentence level.

The word-level encoder is a bidirectional Gated
Recurrent Unit (GRU) (Chung et al., 2014), which
encodes the words of a sentence into sentence rep-
resentation. The word encoder sequentially up-
dates its hidden state after receiving a word, which
is formulated as:

hi,j = BiGRU(hi,j−1, ei,j) (1)

where hi,j and ei,j denotes the hidden state and
embedding of word wi,j , respectively.

The concatenation of the forward and backward
final hidden states in the word-level encoder is in-
dicated as the vector representation xi of the sen-
tence si, which is used as input to the sentence-
level encoder. The sentence encoder is also a bidi-
rectional GRU, which updates its hidden state after
receiving each sentence representation by:

hi = BiGRU(hi−1, xi) (2)

where hi denotes the hidden state of sentence si.
The concatenation of the forward and backward

final states in the sentence-level encoder is used as
the vector representation of document d̂.

2.2 Information Selection
Document summarization is a special natural lan-
guage generation task which requires information
compression. It needs to remove the unnecessary



1789

information and select salient information from
the input document to produce a condensed sum-
mary. However, it is difficult for the basic encoder-
decoder framework to learn the process of salient
information selection, which has also been noticed
by several previous work (Tan et al., 2017a,b). To
tackle the challenge, we extend the basic encoder-
decoder framework by adding an information se-
lection layer to model the information selection
process explicitly. Our information selection layer
consists of two parts: gated global information fil-
tering that used to remove the unnecessary infor-
mation of a document, and local sentence selection
that used to select salient sentences from a docu-
ment sequentially to produce summary sentences.

Gated Global Information Filtering
Inspired by studies on how human write text sum-
maries by first skimming the document and delet-
ing unnecessary material (Brown and Day, 1983),
we design a gated global information filtering net-
work to filter unnecessary information of a doc-
ument based on the global document representa-
tion before the summary decoder generates sum-
mary. Concretely, the gated information filtering
network makes use of the document representa-
tion d̂, which represents the global information of
a document, to filter sentences based on the sen-
tence representation hi.

For each source sentence si, the gate network
takes the document representation d̂ and sentence
representation hi as inputs to compute the gate
vector gi:

gi = σ(Wghi + Ugd̂+ bg) (3)

where Wg and Ug denote weight matrices, bg the
bias vector, and σ the sigmoid activation function.

Then each sentence si can be filtered by the gate
vector gi as follows:

fi = hi ⊙ gi (4)

where fi indicates the representation of sen-
tence si after information filtering, and ⊙ denotes
element-wise multiplication.

Note that, we filter sentences in micro semantic
dimensions rather than filtering whole sentences.
The tailored sentence representations are used as
input to the sentence selection network and sum-
mary decoder, which can help to detect salient sen-
tences and improve informativeness of the gener-
ated summary.

Local Sentence Selection
We explicitly model the local sentence selection
process which selects several target sentences to
generate a summary sentence. Concretely, we ap-
ply a RNN layer to sequentially select target sen-
tences for each summary sentence, shown as in
Figure 1. The sentence-selection RNN uses the
document representation d̂ as initial state h

′
0, and

sequentially predicts the sentence selection vector
αt as follows:

αit =
eϕ(fi,h

′
t)∑

l e
ϕ(fl,h

′
t)

(5)

ϕ(fi, h
′
t) = v

T tanh(Wffi +Whh
′
t + b). (6)

where αit indicates the weight of source sentence
si when generating the t-th summary sentence,
and h

′
t denotes the hidden state of sentence selec-

tion layer when generating the t-th summary sen-
tence. v, Wf and Wh are weight matrices, and b
is the bias vector. Note that, the sentence selec-
tion vector αt is computed based on the tailored
sentence representation fi.

The sentence-selection RNN uses a single uni-
directional GRU, which updates its state by:

h
′
t = GRU(h

′
t−1, x

′
t) (7)

where x
′
t denotes the input of current sentence-

selection step. x
′
t combines both the previous sen-

tence selection vector αt−1 and the encoded repre-
sentation of previous generated sentence r

′
t−1 by

x
′
t = tanh(Wrr

′
t−1 +Wααt−1 + bx), where Wr,

Wα, and bx denote learnable parameters.
The representation of the selected source sen-

tences is computed by:

qt =
∑
j

αjtfj (8)

which is used as initial state of the summary de-
coder to generate a summary sentence to para-
phrase and generalize the selected sentences.

2.3 Summary Decoder
On top of the document encoder and the informa-
tion selection layer, we use GRU with attention
as the summary decoder to realize each summary
sentence word by word.

At each word decoding step k in the t-th sum-
mary sentence, the GRU reads the previous word



1790

embedding et,k−1 and context vector ct,k−1 as in-
puts to compute the new hidden state h

′
t,k by:

h
′
t,k = GRU(h

′
t,k−1, ct,k−1, et,k−1) (9)

We import attention mechanism to help locate
relevant words to be copied or paraphrased within
the selected source sentences in each word genera-
tion step. The attention distribution βit,k of the kth
word of the tth summary sentence over the sen-
tences in the ith document can be computed as:

βi,jt,k = α
i
t

eϕ(hi,j ,h
′
t,k)∑

l e
ϕ(hi,l,h

′
t,k)

(10)

where αit denotes the weight of the ith source
sentence, used to normalize the word attention
distributions. Then the word-level context vec-
tor when generating the kth word at the tth sen-
tence generation step can be computed as: ct,k =∑

i

∑
j β

i,j
t,khi,j , which is also incorporated into

the word decoder.
At each word generation step, the vocabulary

distribution is calculated from the context vector
ct,k and the decoder state h

′
t,k by:

Pvocab(w
′
t,k) = softmax(Wv(Wc[h

′
t,k, ct,k] + bc) + bv)

(11)

where Wv and Wc are learned parameters. The
copy mechanism based on the word attention is
also imported into the decoder to alleviate the
OOV problems as in (See et al., 2017).

2.4 Model Learning with Distant Supervision

Despite the end-to-end training for the perfor-
mance of generated summary, we also directly
optimize the sentence selection decisions by im-
porting supervision for the sentence selection vec-
tor αt in Equation 5. While there is no ex-
plicit supervision for sentence selection, we define
a simple approach for labeling sentences based
on the reference summaries. To simulate the
sentence selection process on human-written ab-
stracts, we compute the words-matching similari-
ties (based on TF-IDF cosine similarity) between
a reference-summary sentence and corresponding
source document sentences and normalize them
into distantly-labelled sentence selection vector
pt. Then the sentence selection loss is defined as:

losssel =
∑
t

DKL(αt, pt) (12)

where DKL(αt, pt) indicates the KL-divergence
between distribution αt and pt. The sentence se-
lection loss is imported into the final loss func-
tion to be optimized with the summary generation
component together.

The loss function L of the model is the mix
of the negative log-likelihood of generating sum-
maries over training set T , and the sentence selec-
tion loss of distantly-supervised training:

L =
∑

(X,Y )∈T

−logP (Y |X; θ) + λlosssel (13)

where λ is a hyper-parameter tuned on the vali-
dation set. (X,Y ) denotes a document-summary
pair in the training set.

3 Experiments

3.1 Dataset
We conduct our experiments on a large-scale cor-
pus of CNN/DailyMail, which has been widely
used for exploration on summarizing documents
with multi-sentence summaries. The corpus are
originally constructed in (Hermann et al., 2015) by
collecting human generated highlights from news
stories in the CNN and DailyMail Website, which
contains input document of about 800 tokens on
average and multi-sentence summaries of up to
200 tokens. We use the same version of data with
(See et al., 2017), which totally has 280,125 train-
ing pairs, 13,367 validation pairs and 11,489 test
pairs after discarding the examples with empty ar-
ticle text. Some of previous work (Nallapati et al.,
2016, 2017; Paulus et al., 2017; Tan et al., 2017a)
use the anonymized version of data, which has
been pre-processed to replace each named entity
with an unique identifier. By contrast, we use the
non-anonymized data similar to (See et al., 2017),
which is a more favorable and challenging prob-
lem because it requires no pre-processing.

3.2 Implementation Details
Model Parameters For all experiments, the
word-level encoder and summary decoder both
use 256-dimensional hidden states, and the
sentence-level encoder and sentence selection net-
work both use 512-dimensional hidden states. We
use pre-trained Glove (Pennington et al., 2014)
vector for initialization of word embeddings. The
dimension of word embeddings is 100, which will
be further trained in the model. We use a vocab-
ulary of 50k words for both encoder and decoder.



1791

Method Rouge-1 Rouge-2 Rouge-L
Lead-3 40.34 17.70 36.57
SummaRuNNer-abs 37.5 14.5 33.4
SummaRuNNer 39.6 16.2 35.3
Seq2seq-baseline 36.64 15.66 33.42
ABS-temp-attn 35.46 13.30 32.65
Graph-attention 38.1 13.9 34.0
Deep-reinforced 39.87 15.82 36.90
Coverage 39.53 17.28 36.38
Our Model 41.54 18.18 36.47

Table 1: Rouge F1 scores on the test set. All our ROUGE
scores have a 95% confidence interval of at most ±0.25
as reported by the official ROUGE script.

We use dropout (Srivastava et al., 2014) with prob-
ability p = 0.5. After tuning on the validation set,
parameter λ is set as 0.2.
Model Training We use Adagrad (Duchi et al.,
2011) algorithm with learning rate 0.1 and an ini-
tial accumulator value of 0.1 to optimize the model
parameters θ. During training, we use gradient
clipping with a maximum gradient norm of 2. Our
model is trained on a single Tesla K40m GPU with
a batch size of 16 and an epoch is set contain-
ing 10,000 randomly sampled documents. Con-
vergence is reached within 300 epochs.
Hierarchical Beam Search To improve informa-
tion correctness and avoid redundancy during the
summary decoding process, we use the hierarchi-
cal beam search algorithm with reference mecha-
nism (Tan et al., 2017a) to generate multi-sentence
summaries. Similar to (Tan et al., 2017a), the
beam sizes for word decoder and sentence decoder
are 15 and 2, respectively.

3.3 Baselines

We compare our system with the results of state-
of-the-art neural summarization approaches re-
ported in recent papers, which contain both ab-
stractive models and extractive models. The ex-
tractive models include SummaRuNNer (Nalla-
pati et al., 2017), while SummaRuNNer-abs is
similar to SummaRuNNer but is trained directly
on the abstractive summaries. Lead-3 is a strong
extractive baseline which uses the first 3 sentences
of the document as summary. The abstractive
models include:

1) Seq2seq-baseline, which uses the basic
seq2seq encoder-decoder structure with at-
tention mechanism and incorporates with the
copy mechanism as in (See et al., 2017).

2) ABS-temp-attn (Nallapati et al., 2016),
which uses Temporal Attention on the

Method Informat. Concise Coherent Fluent
Lead-3 3.49∗ 3.19∗ 3.86 4.07∗

Seq2seq-b. 3.11∗ 2.95∗ 3.08∗ 3.51∗

Coverage 3.41∗ 3.25∗ 3.37 3.72
Our Model 3.76 3.49 3.65 3.80

Table 2: Human evaluation results. ∗ indicates the difference
between Our Model and other models are statistic significant
(p < 0.1) by two-tailed t-test.

seq2seq architecture to overcome the repeti-
tion problem.

3) Graph-attention (Tan et al., 2017a), which
uses a graph-ranking based attention mecha-
nism based on a hierarchical architecture to
identify important source sentences.

4) Deep-reinforced (Paulus et al., 2017), which
trains the seq2seq encoder-decoder model
with reinforcement learning techniques.

5) Coverage (See et al., 2017), which is an ex-
tension of the Seq2seq-baseline model by im-
porting coverage mechanism to control repe-
titions in summary.

3.4 Evaluation

ROUGE Evaluation
We evaluate our models with the standard ROUGE
metric (Lin, 2004) and obtain ROUGE scores
using the pyrouge package. Results in Ta-
ble 1 show that our method has significant im-
provement over state-of-the-art neural abstractive
baselines as well as extractive baselines. Note
that, the Deep-reinforced model achieves the best
ROUGE-L performance because it directly opti-
mizes the ROUGE-L metric. Comparing with
the current state-of-the-art model Coverage, our
model achieves significant better performance on
ROUGE-1 and ROUGE-2 metrics, and compa-
rable performance on ROUGE-L metric, which
demonstrates that our model is more effective in
selecting salient information from a document to
produce an informative summary while keeping
the ability to generate fluent and correct sentences.

Human Evaluation with Case Analysis
In addition to the ROUGE evaluation, we also con-
ducted human evaluation on 50 random samples
from CNN/DailyMail test set and compared the
summaries generated by our method with the out-
puts of Lead-3, Seq2seq-baseline and Coverage.
Three data annotators were asked to compare the
generated summaries with the human summaries,
and assess each summary from four independent



1792

Gold Reference: faith and hope howie were born with one body and two faces on may
8 .
they tragically died in hospital just 19 days after they were born .
parents simon howie and renee young visit their grave at pinegrove in western sydney
fortnightly .
they arrived on thursday to find the grave bare of all the girls ’ mementos .
staff had cleared entire baby section and thrown belongings in rubbish .
Seq2Seq-baseline: faith and hope howie were dubbed the miracle twins when they
were born on may 8 last year with one body and two faces due to an extremely rare
condition known as disrosopus .
faith and hope howie were dubbed the miracle twins when they were born on may 8
last year with one body and two faces due to an extremely rare condition known as
disrosopus .
faith and hope howie were dubbed the miracle twins when they were born on may 8
last year with one body and two faces due to an extremely rare condition known as
disrosopus .
Coverage: faith and hope howie were dubbed the miracle twins when they were born
on may 8 last year with one body and two faces due to an extremely rare condition
known as disrosopus .
they died in hospital less than a month after they were born and their parents , simon
howie and renee young , laid them to rest at pinegrove memorial park in sydney ’s west.
Our Model: faith and hope howie were dubbed the miracle twins when they were born
on may 8 last year with one body and two faces due to an extremely rare condition
known as disrosopus.
they died in hospital less than a month after they were born and their parents , simon
howie and renee young , laid them to rest at pinegrove memorial park in sydney ’s west.
family members have visited the grave every week to leave mementos and flowers for
faith and hope , but when mr howie and ms young arrived on thursday they found the
site completely bare .

Table 3: Examples of generated summaries. The Seq2Seq-
baseline model generates repeated sentences and loses salient
information. The Coverage model reduces repetitions, but
also loses salient information. Our model can select more
salient information from the original document and generate
more informative summary.

perspectives: (1) Informative: How informative
the summary is? (2) Concise: How concise the
summary is? (3) Coherent: How coherent (be-
tween sentences) the summary is? (4) Fluent:
How fluent, grammatical the sentences of a sum-
mary are? Each property is assessed with a score
from 1(worst) to 5(best) by three annotators. The
average results are presented in Table 2.

The results show that our model consistently
outperforms the Seq2seq-baseline model and the
previous state-of-the-art method Coverage. An
example of comparison of the generated sum-
maries by our model with the two abstractive mod-
els (w.r.t the reference summary) is shown in Ta-
ble 31. The summary generated by Seq2Seq-
Baseline usually contains repetition of sentences,
which seriously affects its informativeness, con-
ciseness as well as coherence. For example, the
sentence “faith and hope howie were dubbed the
miracle twins when they were born ...” is repeated
three times in Table 3. The Coverage model effec-
tively alleviates the information repetition prob-
lem, however, it loses some salient information
that should be included in the summary. For ex-
ample, the information about “mementos” and
“family members visit the grave” is lost in the
example shown in Table 3. The summary gen-
erated by our method obviously contains more

1More examples are shown in the supplementary material

(a) Gold Reference

I1 I2 I3 I4 I5 I6 I7 I8 I9 I10 I11 I12 I13 I14 I15 I16 I17 I18 I19 I20
O1
O2
O3
O4
O5

(d) Our Model

O1
O2
O3

I1 I2 I3 I4 I5 I6 I7 I8 I9 I10 I11 I12 I13 I14 I15 I16 I17 I18 I19 I20

(c) Coverage

I1 I2 I3 I4 I5 I6 I7 I8 I9 I10 I11 I12 I13 I14 I15 I16 I17 I18 I19 I20
O1
O2

(b) Seq2Seq-baseline

I1 I2 I3 I4 I5 I6 I7 I8 I9 I10 I11 I12 I13 I14 I15 I16 I17 I18 I19 I20
O1
O2
O3

Figure 2: Visualization of sentence selection vectors. Ii
and Oi indicate the i-th sentence of the input and output,
respectively. Obviously, our model can detect more salient
sentences that are included in the reference summary.

salient information, which shows the effective-
ness of the information selection component in
our model. According to the results in Table
2, the sentence-level modeling of document and
summary in our model also makes the gener-
ated summaries achieve better inter-sentence co-
herence. Compared with the strong extractive
baseline Lead-3, our model is able to generate
more informative and concise summaries, which
shows the advantage of abstractive methods. The
fluency scores also show the good ability of our
model to generate fluent and grammatical sen-
tences. The human evaluation results demonstrate
that our model is able to generate more infor-
mative, concise and coherent summaries than the
baselines.

The visualization of the sentence selection vec-
tors of the gold reference summary and the three
abstractive models when generating the presented
examples in Table 3 are shown in Figure 22. The
figure shows that Seq2Seq-baseline fails to detect
all important source sentences and attend to the
same sentences repeatedly, which result in gen-
erating repeated summary sentences. Coverage
learns to reduce repetitions, but fails to detect all
the salient information. Obviously, our method
is more effective in selecting salient and rele-
vant source sentences from the document to gener-
ate more informative summary. Furthermore, our

2The sentence selection vectors of the Seq2seq-baseline
mode and the Coverage model are computed by summing the
attention weights of all words in each sentence and then nor-
malized across sentences.



1793

Method Rouge-1 Rouge-2 Rouge-L
Our Model 41.54 18.18 36.47

– distS 40.02 17.54 34.87
– distS&gateF 39.26 16.96 33.92
– infoSelection 36.64 15.66 33.42

Table 4: Comparison results of removing different compo-
nents of our method.

method tends to focus on different sets of source
sentences when generating different summary sen-
tences. The results verify that the information se-
lection component in our model significantly im-
proves the information selection process in docu-
ment summarization.

4 Discussion

In this section, we first validate the effective-
ness of each component of our model, then com-
pare the performance of information selection of
our method with several extractive methods, and
finally analyze the effects of golden summary
length on the performance of our model.

4.1 Model Validation

To further verify the effectiveness of each compo-
nent in our model, we conduct several ablation ex-
periments. “– distS” denotes removing the distant
supervision for sentence selection (set λ as 0). “–
distS&gateF” denotes removing both the distant
supervision for sentence selection training and the
global gated information filtering component. “–
infoSelection” denotes removing the whole infor-
mation selection layer and do not explicitly mod-
eling the information selection process, which is
actually the Seq2seq-baseline model.

Results on the test set are shown in Table 4. Our
method much outperforms all the comparison sys-
tems and removing each component of our model
one by one will leads to sustained significant per-
formance declining, which verifies the effective-
ness of each component in our model. The global
gated information filtering network removes un-
necessary information from the original document
and helps generate more informative summary.
The distantly-supervised training for sentence se-
lection decisions helps the model learn to detect
important and relevant source sentences for each
summary sentence. The results verify that explic-
itly modeling the information selection process
significantly improves the document summariza-
tion performance.

Method Rouge-1 Rouge-2 Rouge-L
SummaRuNNer-abs 37.5 14.5 33.4
SummaRuNNer 39.6 16.2 35.3
OurExtractive 40.41 18.30 36.30
– distS 37.06 16.55 33.23
– distS&gateF 36.25 16.22 32.59

Table 5: Comparsion results of sentence selection.

length Method Rouge-1 Rouge-2 Rouge-
L

< 75 Our Mod. 39.90 16.91 35.19
(81.82%) Coverage 38.90 16.81 35.82
[75, 100) Our Mod. 47.13 22.44 40.81
(12.64%) Coverage 42.89 19.72 39.41
[100, 125) Our Mod. 50.49 24.23 43.68
(4.00%) Coverage 41.78 19.00 38.41
> 125 Our Mod. 50.25 23.98 41.19
(1.54%) Coverage 39.57 17.93 36.33

Table 6: Comparison results w.r.t different length of refer-
ence summary. < 75 indicates the reference summary has
less than 75 words (occupy 81.82% of test set), [75, 100) de-
notes the number of words in reference summary is between
75 and 100 (occupy 12.64% of test set).

4.2 Effectiveness of Information Selection
To verify the performance of sentence selection in
our model, we add a comparison system OurEx-
tractive which is almost the same as our model,
but replaces the summary decoder by a sentence
extractor. The sentence extractor extracts the
source sentence with the largest weight in each
sentence generation step. “– distS” denotes re-
moving the distant supervision for sentence selec-
tion training in our model. “– distS&gateF” de-
notes removing both the distant supervision for
sentence selection training and the gated global in-
formation filtering component.

Results in Table 5 show that our simple extrac-
tive method OurExtractive significantly outper-
forms state-of-the-art neural extractive baselines,
which demonstrates the effectiveness of the infor-
mation selection component in our model. More-
over, OurExtractive significantly outperforms the
two comparison systems which remove different
components of our model one by one. The results
show that both the gated global information filter-
ing and distant supervision training are effective
for improving information selection in document
summarization. Our proposed method effectively
combines the strengths of extractive methods and
abstractive methods into a unified framework.

4.3 Effects of Summary Length
We further compare our method with the Cov-
erage model by evaluating them on the test set



1794

with different length of golden reference sum-
maries. The results are shown in Table 6, which
demonstrate that our method is better at gener-
ating long summary for long document. As the
golden summary becoming longer, our system will
obtain larger advantages over the baseline (from
+1.0 Rouge-1, +0.1 Rouge-2 and -0.63 Rouge-L
for summary less than 75 words, rising to +10.68
Rouge-1, +6.05 Rouge-2 and +4.86 Rouge-L for
summaries more than 125 words). The results
also verify that our method is more effective in se-
lecting salient information from documents, espe-
cially for long documents.

5 Related Work

Existing exploration on document summarization
mainly can be categorized to extractive methods
and abstractive methods.

5.1 Extractive Summarization Methods

Neural networks have been widely investigated on
extractive document summarization task. Earlier
work attempts to use deep learning techniques to
improve sentence ranking or scoring (Cao et al.,
2015a,b; Yin and Pei, 2015). Some recent work
solves the sentence extraction and document mod-
eling in an end-to-end framework. Cheng and La-
pata (2016) propose an encoder-decoder approach
where the encoder hierarchically learns the rep-
resentation of sentences and documents while an
attention-based sentence extractor extracts salient
sentences sequentially from the original docu-
ment. Nallapati et al. (2017) propose a recur-
rent neural network-based sequence-to-sequence
model for sequential labelling of each sentence in
the document. Neural models are able to lever-
age large-scale corpora and achieve better perfor-
mance than traditional methods.

5.2 Abstractive Summarization Methods

As the seq2seq learning with neural networks
achieve huge success in sequence generation tasks
like machine translation, it also shows great po-
tential in text summarization area, especially for
abstractive methods. Some earlier researches stud-
ied the use of seq2seq learning for abstractive sen-
tence summarization (Takase et al., 2016; Rush
et al., 2015; Chopra et al., 2016). These models
are trained on a large corpus of news documents
which are usually shortened to be the first one or
two sentences, and their headlines.

Later, some work explored the seq2seq mod-
els on document summarization, which produce
a multi-sentence summary for a document. The
seq2seq models usually exhibit some undesir-
able behaviors, such as inaccurately reproduc-
ing factual details, unable to deal with out-of-
vocabulary (OOV) words and repetitions. To
alleviate these issues, copying mechanism (Gu
et al., 2016; Gulcehre et al., 2016; Nallapati et al.,
2016) has been incorporated into the encoder-
decoder architecture. Distraction-based attention
model (Chen et al., 2016) and coverage mecha-
nism (See et al., 2017) have also been investi-
gated to alleviate the repetition problem. To better
train the seq2seq model on tasks with long docu-
ments and multi-sentence summaries, a deep rein-
forced model was proposed to combine the stan-
dard words predication with teacher forcing learn-
ing and the global sequence prediction training
with reinforcement learning (Paulus et al., 2017).
Recently, Tan et al. (2017a) propose to leverage
the hierarchical encoder-decoder architecture on
generating multi-sentence summaries, and incor-
porate sentence-ranking into the summary gener-
ation process based on the graph-based attention
mechanism. Different from these neural-based
work, our model explicitly models the informa-
tion selection process in document summarization
by extending the encoder-decoder framework with
an information selection layer. Our model cap-
tures both the global document information and
local inter-sentence relations, and optimize the in-
formation selection process directly via distantly-
supervised training, which effectively combines
the strengths of extractive methods and abstractive
methods.

6 Conclusion

In this paper, we have analyzed the necessity of ex-
plicitly modeling and optimizing of the informa-
tion selection process in document summarization,
and verified its effectiveness by extending the ba-
sic neural encoding-decoding framework with an
information selection layer and optimizing it with
distantly-supervised training. Our information se-
lection layer consists of a gated global informa-
tion filtering network and a local RNN sentence
selection network. Experimental results demon-
strate that both of them are effective for help-
ing select salient information during the summary
generation process, which significantly improves



1795

the document summarization performance. Our
model combines the strengths of extractive meth-
ods and abstractive methods, which can gener-
ate more informative and concise summaries, and
thus achieves state-of-the-art abstractive document
summarization performance and is also competi-
tive with state-of-the-art extractive models.

Acknowledgments

This work was supported by National Key Re-
search and Development Program of China under
grants 2016YFB1000902 and 2017YFC0820404,
and National Natural Science Foundation of China
under grants 61572469, 91646120, 61772501 and
61572473. We thank the anonymous reviewers for
their helpful comments about this work.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Ann L Brown and Jeanne D Day. 1983. Macrorules
for summarizing texts: The development of exper-
tise. Journal of verbal learning and verbal behav-
ior, 22(1):1–14.

Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, and Ming
Zhou. 2015a. Ranking with recursive neural net-
works and its application to multi-document sum-
marization. In AAAI, pages 2153–2159.

Ziqiang Cao, Furu Wei, Sujian Li, Wenjie Li, Ming
Zhou, and WANG Houfeng. 2015b. Learning sum-
mary prior representation for extractive summariza-
tion. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers),
volume 2, pages 829–833.

Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei,
and Hui Jiang. 2016. Distraction-based neural net-
works for document summarization. arXiv preprint
arXiv:1610.08462.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.
arXiv preprint arXiv:1603.07252.

Sumit Chopra, Michael Auli, and Alexander M Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 93–98.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12(Jul):2121–2159.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. arXiv preprint
arXiv:1603.06393.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallap-
ati, Bowen Zhou, and Yoshua Bengio. 2016.
Pointing the unknown words. arXiv preprint
arXiv:1603.08148.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Text summariza-
tion branches out: Proceedings of the ACL-04 work-
shop, volume 8. Barcelona, Spain.

Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.
Summarunner: A recurrent neural network based se-
quence model for extractive summarization of docu-
ments. AAAI, 1:1.

Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre,
Bing Xiang, et al. 2016. Abstractive text summa-
rization using sequence-to-sequence rnns and be-
yond. arXiv preprint arXiv:1602.06023.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Alexander M Rush, Sumit Chopra, and Jason We-
ston. 2015. A neural attention model for ab-
stractive sentence summarization. arXiv preprint
arXiv:1509.00685.

Abigail See, Peter J Liu, and Christopher D Man-
ning. 2017. Get to the point: Summarization
with pointer-generator networks. arXiv preprint
arXiv:1704.04368.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.



1796

Sho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu Hi-
rao, and Masaaki Nagata. 2016. Neural headline
generation on abstract meaning representation. In
EMNLP, pages 1054–1059.

Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017a.
Abstractive document summarization with a graph-
based attentional neural model. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
volume 1, pages 1171–1181.

Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017b.
From neural sentence summarization to headline
generation: A coarse-to-fine approach. IJCAI.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Proceedings of the IEEE
conference on computer vision and pattern recogni-
tion, pages 3156–3164.

Wenpeng Yin and Yulong Pei. 2015. Optimizing sen-
tence modeling and selection for document summa-
rization. In IJCAI.


