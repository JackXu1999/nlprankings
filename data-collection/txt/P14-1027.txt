



















































Modelling function words improves unsupervised word segmentation


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 282–292,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Modelling function words improves unsupervised word segmentation

Mark Johnson1,2, Anne Christophe3,4, Katherine Demuth2,6 and Emmanuel Dupoux3,5
1 Department of Computing, Macquarie University, Sydney, Australia

2 Santa Fe Institute, Santa Fe, New Mexico, USA
3 Ecole Normale Supérieure, Paris, France

4 Centre National de la Recherche Scientifique, Paris, France
5 Ecole des Hautes Etudes en Sciences Sociales, Paris, France

6 Department of Linguistics, Macquarie University, Sydney, Australia

Abstract

Inspired by experimental psychological
findings suggesting that function words
play a special role in word learning, we
make a simple modification to an Adaptor
Grammar based Bayesian word segmenta-
tion model to allow it to learn sequences
of monosyllabic “function words” at the
beginnings and endings of collocations
of (possibly multi-syllabic) words. This
modification improves unsupervised word
segmentation on the standard Bernstein-
Ratner (1987) corpus of child-directed En-
glish by more than 4% token f-score com-
pared to a model identical except that it
does not special-case “function words”,
setting a new state-of-the-art of 92.4% to-
ken f-score. Our function word model as-
sumes that function words appear at the
left periphery, and while this is true of
languages such as English, it is not true
universally. We show that a learner can
use Bayesian model selection to determine
the location of function words in their lan-
guage, even though the input to the model
only consists of unsegmented sequences of
phones. Thus our computational models
support the hypothesis that function words
play a special role in word learning.

1 Introduction

Over the past two decades psychologists have in-
vestigated the role that function words might play
in human language acquisition. Their experiments
suggest that function words play a special role in
the acquisition process: children learn function
words before they learn the vast bulk of the asso-
ciated content words, and they use function words
to help identify context words.

The goal of this paper is to determine whether
computational models of human language acqui-
sition can provide support for the hypothesis that

function words are treated specially in human
language acquisition. We do this by comparing
two computational models of word segmentation
which differ solely in the way that they model
function words. Following Elman et al. (1996)
and Brent (1999) our word segmentation models
identify word boundaries from unsegmented se-
quences of phonemes corresponding to utterances,
effectively performing unsupervised learning of a
lexicon. For example, given input consisting of
unsegmented utterances such as the following:

j u w ɑ n t t u s i ð ə b ʊ k
a word segmentation model should segment this as
ju wɑnt tu si ðə bʊk, which is the IPA representation
of “you want to see the book”.

We show that a model equipped with the abil-
ity to learn some rudimentary properties of the
target language’s function words is able to learn
the vocabulary of that language more accurately
than a model that is identical except that it is inca-
pable of learning these generalisations about func-
tion words. This suggests that there are acqui-
sition advantages to treating function words spe-
cially that human learners could take advantage of
(at least to the extent that they are learning similar
generalisations as our models), and thus supports
the hypothesis that function words are treated spe-
cially in human lexical acquisition. As a reviewer
points out, we present no evidence that children
use function words in the way that our model does,
and we want to emphasise we make no such claim.
While absolute accuracy is not directly relevant
to the main point of the paper, we note that the
models that learn generalisations about function
words perform unsupervised word segmentation
at 92.5% token f-score on the standard Bernstein-
Ratner (1987) corpus, which improves the previ-
ous state-of-the-art by more than 4%.

As a reviewer points out, the changes we make
to our models to incorporate function words can
be viewed as “building in” substantive informa-
tion about possible human languages. The model

282



that achieves the best token f-score expects func-
tion words to appear at the left edge of phrases.
While this is true for languages such as English,
it is not true universally. By comparing the pos-
terior probability of two models — one in which
function words appear at the left edges of phrases,
and another in which function words appear at the
right edges of phrases — we show that a learner
could use Bayesian posterior probabilities to deter-
mine that function words appear at the left edges
of phrases in English, even though they are not
told the locations of word boundaries or which
words are function words.

This paper is structured as follows. Section 2
describes the specific word segmentation mod-
els studied in this paper, and the way we ex-
tended them to capture certain properties of func-
tion words. The word segmentation experiments
are presented in section 3, and section 4 discusses
how a learner could determine whether function
words occur on the left-periphery or the right-
periphery in the language they are learning. Sec-
tion 5 concludes and describes possible future
work. The rest of this introduction provides back-
ground on function words, the Adaptor Grammar
models we use to describe lexical acquisition and
the Bayesian inference procedures we use to infer
these models.

1.1 Psychological evidence for the role of
function words in word learning

Traditional descriptive linguistics distinguishes
function words, such as determiners and prepo-
sitions, from content words, such as nouns and
verbs, corresponding roughly to the distinction be-
tween functional categories and lexical categories
of modern generative linguistics (Fromkin, 2001).

Function words differ from content words in at
least the following ways:

1. there are usually far fewer function word
types than content word types in a language

2. function word types typically have much
higher token frequency than content word
types

3. function words are typically morphologically
and phonologically simple (e.g., they are typ-
ically monosyllabic)

4. function words typically appear in peripheral
positions of phrases (e.g., prepositions typi-
cally appear at the beginning of prepositional
phrases)

5. each function word class is associated with
specific content word classes (e.g., deter-

miners and prepositions are associated with
nouns, auxiliary verbs and complementisers
are associated with main verbs)

6. semantically, content words denote sets of
objects or events, while function words de-
note more complex relationships over the en-
tities denoted by content words

7. historically, the rate of innovation of function
words is much lower than the rate of innova-
tion of content words (i.e., function words are
typically “closed class”, while content words
are “open class”)

Properties 1–4 suggest that function words
might play a special role in language acquisition
because they are especially easy to identify, while
property 5 suggests that they might be useful for
identifying lexical categories. The models we
study here focus on properties 3 and 4, in that
they are capable of learning specific sequences of
monosyllabic words in peripheral (i.e., initial or
final) positions of phrase-like units.

A number of psychological experiments have
shown that infants are sensitive to the function
words of their language within their first year of
life (Shi et al., 2006; Hallé et al., 2008; Shafer
et al., 1998), often before they have experienced
the “word learning spurt”. Crucially for our pur-
pose, infants of this age were shown to exploit
frequent function words to segment neighboring
content words (Shi and Lepage, 2008; Hallé et
al., 2008). In addition, 14 to 18-month-old
children were shown to exploit function words to
constrain lexical access to known words - for in-
stance, they expect a noun after a determiner (Cau-
vet et al., 2014; Kedar et al., 2006; Zangl and
Fernald, 2007). In addition, it is plausible that
function words play a crucial role in children’s
acquisition of more complex syntactic phenom-
ena (Christophe et al., 2008; Demuth and McCul-
lough, 2009), so it is interesting to investigate the
roles they might play in computational models of
language acquisition.

1.2 Adaptor grammars

Adaptor grammars are a framework for Bayesian
inference of a certain class of hierarchical non-
parametric models (Johnson et al., 2007b). They
define distributions over the trees specified by
a context-free grammar, but unlike probabilistic
context-free grammars, they “learn” distributions
over the possible subtrees of a user-specified set of
“adapted” nonterminals. (Adaptor grammars are
non-parametric, i.e., not characterisable by a finite

283



set of parameters, if the set of possible subtrees
of the adapted nonterminals is infinite). Adaptor
grammars are useful when the goal is to learn a
potentially unbounded set of entities that need to
satisfy hierarchical constraints. As section 2 ex-
plains in more detail, word segmentation is such
a case: words are composed of syllables and be-
long to phrases or collocations, and modelling this
structure improves word segmentation accuracy.

Adaptor Grammars are formally defined in
Johnson et al. (2007b), which should be consulted
for technical details. Adaptor Grammars (AGs)
are an extension of Probabilistic Context-Free
Grammars (PCFGs), which we describe first. A
Context-Free Grammar (CFG) G = (N,W,R, S)
consists of disjoint finite sets of nonterminal sym-
bols N and terminal symbols W , a finite set of
rules R of the form A→α where A ∈ N and
α ∈ (N ∪ W )⋆, and a start symbol S ∈ N . (We
assume there are no “ϵ-rules” in R, i.e., we require
that |α| ≥ 1 for each A→α ∈ R).

A Probabilistic Context-Free Grammar (PCFG)
is a quintuple (N, W,R, S, θ) where N , W , R
and S are the nonterminals, terminals, rules and
start symbol of a CFG respectively, and θ is a vec-
tor of non-negative reals indexed by R that sat-
isfy

∑
α∈RA θA→α = 1 for each A ∈ N , where

RA = {A→α : A→α ∈ R} is the set of rules
expanding A.

Informally, θA→α is the probability of a node
labelled A expanding to a sequence of nodes la-
belled α, and the probability of a tree is the prod-
uct of the probabilities of the rules used to con-
struct each non-leaf node in it. More precisely, for
each X ∈ N ∪W a PCFG associates distributions
GX over the set of trees TX generated by X as
follows:

If X ∈ W (i.e., if X is a terminal) then GX
is the distribution that puts probability 1 on the
single-node tree labelled X .

If X ∈ N (i.e., if X is a nonterminal) then:

GX =
∑

X→B1...Bn∈RX
θX→B1...Bn TDX(GB1 , . . . , GBn) (1)

where RX is the subset of rules in R expanding
nonterminal X ∈ N , and:

TDX(G1, . . . , Gn)

(

..

X

.

t1

.

tn

.

. . .

)
=

n∏
i=1

Gi(ti).

That is, TDX(G1, . . . , Gn) is a distribution over
the set of trees TX generated by nonterminal X ,
where each subtree ti is generated independently

from Gi. The PCFG generates the distribution GS
over the set of trees TS generated by the start sym-
bol S; the distribution over the strings it generates
is obtained by marginalising over the trees.

In a Bayesian PCFG one puts Dirichlet priors
Dir(α) on the rule probability vector θ, such that
there is one Dirichlet parameter αA→α for each
rule A→α ∈ R. There are Markov Chain Monte
Carlo (MCMC) and Variational Bayes procedures
for estimating the posterior distribution over rule
probabilities θ and parse trees given data consist-
ing of terminal strings alone (Kurihara and Sato,
2006; Johnson et al., 2007a).

PCFGs can be viewed as recursive mixture
models over trees. While PCFGs are expres-
sive enough to describe a range of linguistically-
interesting phenomena, PCFGs are parametric
models, which limits their ability to describe phe-
nomena where the set of basic units, as well as
their properties, are the target of learning. Lexi-
cal acqusition is an example of a phenomenon that
is naturally viewed as non-parametric inference,
where the number of lexical entries (i.e., words)
as well as their properties must be learnt from the
data.

It turns out there is a straight-forward modifica-
tion to the PCFG distribution (1) that makes it suit-
ably non-parametric. As Johnson et al. (2007b)
explain, by inserting a Dirichlet Process (DP)
or Pitman-Yor Process (PYP) into the generative
mechanism (1) the model “concentrates” mass on
a subset of trees (Teh et al., 2006). Specifically,
an Adaptor Grammar identifies a subset A ⊆ N
of adapted nonterminals. In an Adaptor Gram-
mar the unadapted nonterminals N \ A expand
via (1), just as in a PCFG, but the distributions of
the adapted nonterminals A are “concentrated” by
passing them through a DP or PYP:

HX =
∑

X→B1...Bn∈RX
θX→B1...Bn TDX(GB1 , . . . , GBn)

GX = PYP(HX , aX , bX)

Here aX and bX are parameters of the PYP asso-
ciated with the adapted nonterminal X . As Gold-
water et al. (2011) explain, such Pitman-Yor Pro-
cesses naturally generate power-law distributed
data.

Informally, Adaptor Grammars can be viewed
as caching entire subtrees of the adapted nonter-
minals. Roughly speaking, the probability of gen-
erating a particular subtree of an adapted nonter-
minal is proportional to the number of times that
subtree has been generated before. This “rich get

284



richer” behaviour causes the distribution of sub-
trees to follow a power-law (the power is speci-
fied by the aX parameter of the PYP). The PCFG
rules expanding an adapted nonterminal X de-
fine the “base distribution” of the associated DP
or PYP, and the aX and bX parameters determine
how much mass is reserved for “new” trees.

There are several different procedures for infer-
ring the parse trees and the rule probabilities given
a corpus of strings: Johnson et al. (2007b) describe
a MCMC sampler and Cohen et al. (2010) describe
a Variational Bayes procedure. We use the MCMC
procedure here since this has been successfully ap-
plied to word segmentation problems in previous
work (Johnson, 2008).

2 Word segmentation with Adaptor
Grammars

Perhaps the simplest word segmentation model is
the unigram model, where utterances are modeled
as sequences of words, and where each word is
a sequence of segments (Brent, 1999; Goldwater
et al., 2009). A unigram model can be expressed
as an Adaptor Grammar with one adapted non-
terminal Word (we indicate adapted nonterminals
by underlining them in grammars here; regular ex-
pressions are expanded into right-branching pro-
ductions).

Sentence→Word+ (2)
Word→Phone+ (3)

The first rule (2) says that a sentence consists of
one or more Words, while the second rule (3)
states that a Word consists of a sequence of one or
more Phones; we assume that there are rules ex-
panding Phone into all possible phones. Because
Word is an adapted nonterminal, the adaptor gram-
mar memoises Word subtrees, which corresponds
to learning the phone sequences for the words of
the language.

The more sophisticated Adaptor Grammars dis-
cussed below can be understood as specialis-
ing either the first or the second of the rules
in (2–3). The next two subsections review the
Adaptor Grammar word segmentation models pre-
sented in Johnson (2008) and Johnson and Gold-
water (2009): section 2.1 reviews how phonotac-
tic syllable-structure constraints can be expressed
with Adaptor Grammars, while section 2.2 re-
views how phrase-like units called “collocations”
capture inter-word dependencies. Section 2.3
presents the major novel contribution of this paper

by explaining how we modify these adaptor gram-
mars to capture some of the special properties of
function words.

2.1 Syllable structure and phonotactics

The rule (3) models words as sequences of inde-
pendently generated phones: this is what Gold-
water et al. (2009) called the “monkey model” of
word generation (it instantiates the metaphor that
word types are generated by a monkey randomly
banging on the keys of a typewriter). However, the
words of a language are typically composed of one
or more syllables, and explicitly modelling the in-
ternal structure of words typically improves word
segmentation considerably.

Johnson (2008) suggested replacing (3) with the
following model of word structure:

Word→ Syllable1:4 (4)
Syllable→(Onset) Rhyme (5)

Onset→Consonant+ (6)
Rhyme→Nucleus (Coda) (7)

Nucleus→Vowel+ (8)
Coda→Consonant+ (9)

Here and below superscripts indicate iteration
(e.g., a Word consists of 1 to 4 Syllables), while
an Onset consists of an unbounded number of
Consonants), while parentheses indicate option-
ality (e.g., a Rhyme consists of an obligatory
Nucleus followed by an optional Coda). We as-
sume that there are rules expanding Consonant
and Vowel to the set of all consonants and vow-
els respectively (this amounts to assuming that the
learner can distinguish consonants from vowels).
Because Onset, Nucleus and Coda are adapted,
this model learns the possible syllable onsets, nu-
cleii and coda of the language, even though neither
syllable structure nor word boundaries are explic-
itly indicated in the input to the model.

The model just described assumes that word-
internal syllables have the same structure as word-
peripheral syllables, but in languages such as
English word-peripheral onsets and codas can
be more complex than the corresponding word-
internal onsets and codas. For example, the
word “string” begins with the onset cluster str,
which is relatively rare word-internally. Johnson
(2008) showed that word segmentation accuracy
improves if the model can learn different conso-
nant sequences for word-inital onsets and word-
final codas. It is easy to express this as an Adaptor

285



Grammar: (4) is replaced with (10–11) and (12–
17) are added to the grammar.

Word→ SyllableIF (10)
Word→ SyllableI Syllable0:2 SyllableF (11)

SyllableIF→(OnsetI) RhymeF (12)
SyllableI→(OnsetI) Rhyme (13)
SyllableF→(Onset) RhymeF (14)

OnsetI→Consonant+ (15)
RhymeF→Nucleus (CodaF) (16)

CodaF→Consonant+ (17)

In this grammar the suffix “I” indicates a word-
initial element, and “F” indicates a word-final el-
ement. Note that the model simply has the abil-
ity to learn that different clusters can occur word-
peripherally and word-internally; it is not given
any information about the relative complexity of
these clusters.

2.2 Collocation models of inter-word
dependencies

Goldwater et al. (2009) point out the detrimental
effect that inter-word dependencies can have on
word segmentation models that assume that the
words of an utterance are independently gener-
ated. Informally, a model that generates words in-
dependently is likely to incorrectly segment multi-
word expressions such as “the doggie” as single
words because the model has no way to capture
word-to-word dependencies, e.g., that “doggie” is
typically preceded by “the”. Goldwater et al show
that word segmentation accuracy improves when
the model is extended to capture bigram depen-
dencies.

Adaptor grammar models cannot express bi-
gram dependencies, but they can capture similiar
inter-word dependencies using phrase-like units
that Johnson (2008) calls collocations. John-
son and Goldwater (2009) showed that word seg-
mentation accuracy improves further if the model
learns a nested hierarchy of collocations. This can
be achieved by replacing (2) with (18–21).

Sentence→Colloc3+ (18)
Colloc3→Colloc2+ (19)
Colloc2→Colloc1+ (20)
Colloc1→Word+ (21)

Informally, Colloc1, Colloc2 and Colloc3 define a
nested hierarchy of phrase-like units. While not

designed to correspond to syntactic phrases, by ex-
amining the sample parses induced by the Adaptor
Grammar we noticed that the collocations often
correspond to noun phrases, prepositional phrases
or verb phrases. This motivates the extension to
the Adaptor Grammar discussed below.

2.3 Incorporating “function words” into
collocation models

The starting point and baseline for our extension
is the adaptor grammar with syllable structure
phonotactic constraints and three levels of collo-
cational structure (5-21), as prior work has found
that this yields the highest word segmentation to-
ken f-score (Johnson and Goldwater, 2009).

Our extension assumes that the Colloc1 −
Colloc3 constituents are in fact phrase-like, so we
extend the rules (19–21) to permit an optional se-
quence of monosyllabic words at the left edge
of each of these constituents. Our model thus
captures two of the properties of function words
discussed in section 1.1: they are monosyllabic
(and thus phonologically simple), and they appear
on the periphery of phrases. (We put “function
words” in scare quotes below because our model
only approximately captures the linguistic proper-
ties of function words).

Specifically, we replace rules (19–21) with the
following sequence of rules:

Colloc3→(FuncWords3) Colloc2+ (22)
Colloc2→(FuncWords2) Colloc1+ (23)
Colloc1→(FuncWords1) Word+ (24)

FuncWords3→FuncWord3+ (25)
FuncWord3→ SyllableIF (26)

FuncWords2→FuncWord2+ (27)
FuncWord2→ SyllableIF (28)

FuncWords1→FuncWord1+ (29)
FuncWord1→ SyllableIF (30)

This model memoises (i.e., learns) both the in-
dividual “function words” and the sequences of
“function words” that modify the Colloc1 −
Colloc3 constituents. Note also that “function
words” expand directly to SyllableIF, which in
turn expands to a monosyllable with a word-initial
onset and word-final coda. This means that “func-
tion words” are memoised independently of the
“content words” that Word expands to; i.e., the
model learns distinct “function word” and “con-
tent word” vocabularies. Figure 1 depicts a sample
parse generated by this grammar.

286



..
Sentence

.

Colloc3

.

FuncWords3

.

FuncWord3

.

you

.

FuncWord3

.

want

.

FuncWord3

.

to

.

Colloc2

.

Colloc1

.

Word

.

see

.

Colloc1

.

FuncWords1

.

FuncWord1

.

the

.

Word

.

book

Figure 1: A sample parse generated by the “func-
tion word” Adaptor Grammar with rules (10–18)
and (22–30). To simplify the parse we only show
the root node and the adapted nonterminals, and
replace word-internal structure by the word’s or-
thographic form.

This grammar builds in the fact that function
words appear on the left periphery of phrases. This
is true of languages such as English, but is not true
cross-linguistically. For comparison purposes we
also include results for a mirror-image model that
permits “function words” on the right periphery,
a model which permits “function words” on both
the left and right periphery (achieved by changing
rules 22–24), as well as a model that analyses all
words as monosyllabic.

Section 4 explains how a learner could use
Bayesian model selection to determine that func-
tion words appear on the left periphery in English
by comparing the posterior probability of the data
under our “function word” Adaptor Grammar to
that obtained using a grammar which is identi-
cal except that rules (22–24) are replaced with the
mirror-image rules in which “function words” are
attached to the right periphery.

3 Word segmentation results

This section presents results of running our Adap-
tor Grammar models on subsets of the Bernstein-
Ratner (1987) corpus of child-directed English.
We use the Adaptor Grammar software available
from http://web.science.mq.edu.au/˜mjohnson/
with the same settings as described in Johnson
and Goldwater (2009), i.e., we perform Bayesian
inference with “vague” priors for all hyperpa-
rameters (so there are no adjustable parameters
in our models), and perform 8 different MCMC
runs of each condition with table-label resampling
for 2,000 sweeps of the training data. At every
10th sweep of the last 1,000 sweeps we use the
model to segment the entire corpus (even if it
is only trained on a subset of it), so we collect

Model Tokenf-score
Boundary
precision

Boundary
recall

Baseline 0.872 0.918 0.956
+ left FWs 0.924 0.935 0.990
+ left + right FWs 0.912 0.957 0.953

Table 1: Mean token f-scores and boundary preci-
sion and recall results averaged over 8 trials, each
consisting of 8 MCMC runs of models trained
and tested on the full Bernstein-Ratner (1987) cor-
pus (the standard deviations of all values are less
than 0.006; Wilcox sign tests show the means of
all token f-scores differ p < 2e-4).

800 sample segmentations of each utterance.
The most frequent segmentation in these 800
sample segmentations is the one we score in the
evaluations below.

3.1 Word segmentation with “function word”
models

Here we evaluate the word segmentations found
by the “function word” Adaptor Grammar model
described in section 2.3 and compare it to the base-
line grammar with collocations and phonotactics
from Johnson and Goldwater (2009). Figure 2
presents the standard token and lexicon (i.e., type)
f-score evaluations for word segmentations pro-
posed by these models (Brent, 1999), and Table 1
summarises the token and lexicon f-scores for the
major models discussed in this paper. It is interest-
ing to note that adding “function words” improves
token f-score by more than 4%, corresponding to
a 40% reduction in overall error rate.

When the training data is very small the Mono-
syllabic grammar produces the highest accuracy
results, presumably because a large proportion of
the words in child-directed speech are monosyl-
labic. However, at around 25 sentences the more
complex models that are capable of finding multi-
syllabic words start to become more accurate.

It’s interesting that after about 1,000 sentences
the model that allows “function words” only on
the right periphery is considerably less accurate
than the baseline model. Presumably this is be-
cause it tends to misanalyse multi-syllabic words
on the right periphery as sequences of monosyl-
labic words.

The model that allows “function words” only on
the left periphery is more accurate than the model
that allows them on both the left and right periph-
ery when the input data ranges from about 100 to
about 1,000 sentences, but when the training data

287



0.00

0.25

0.50

0.75

1.00

1 10 100 1000 10000
NumberWofWtrainingWsentences

To
ke

nWf
-sc

ore

Model
Monosyllables
Baseline
+WleftWFWs
+WrightWFWs
+WleftW+WFWs

0.00

0.25

0.50

0.75

1.00

1 10 100 1000 10000
NumberWofWtrainingWsentences

Le
xic

on
Wf-

sco
re

Model
Monosyllables
Baseline
+WleftWFWs
+WrightWFWs
+WleftW+WrightWFWs

Figure 2: Token and lexicon (i.e., type) f-score on the Bernstein-Ratner (1987) corpus as a function of
training data size for the baseline model, the model where “function words” can appear on the left pe-
riphery, a model where “function words” can appear on the right periphery, and a model where “function
words” can appear on both the left and the right periphery. For comparison purposes we also include
results for a model that assumes that all words are monosyllabic.

is larger than about 1,000 sentences both models
are equally accurate.

3.2 Content and function words found by
“function word” model

As noted earlier, the “function word” model gen-
erates function words via adapted nonterminals
other than the Word category. In order to bet-
ter understand just how the model works, we give
the 5 most frequent words in each word category
found during 8 MCMC runs of the left-peripheral
“function word” grammar above:
Word : book, doggy, house, want, I
FuncWord1 : a, the, your, little1, in
FuncWord2 : to, in, you, what, put
FuncWord3 : you, a, what, no, can

Interestingly, these categories seem fairly rea-
sonable. The Word category includes open-class
nouns and verbs, the FuncWord1 category in-
cludes noun modifiers such as determiners, while
the FuncWord2 and FuncWord3 categories in-
clude prepositions, pronouns and auxiliary verbs.

1The phone ‘l’ is generated by both Consonant and
Vowel, so “little” can be (incorrectly) analysed as one syl-
lable.

Thus, the present model, initially aimed at seg-
menting words from continuous speech, shows
three interesting characteristics that are also ex-
hibited by human infants: it distinguishes be-
tween function words and content words (Shi and
Werker, 2001), it allows learners to acquire at least
some of the function words of their language (e.g.
(Shi et al., 2006)); and furthermore, it may also al-
low them to start grouping together function words
according to their category (Cauvet et al., 2014;
Shi and Melançon, 2010).

4 Are “function words” on the left or
right periphery?

We have shown that a model that expects function
words on the left periphery performs more accu-
rate word segmentation on English, where func-
tion words do indeed typically occur on the left
periphery, leaving open the question: how could
a learner determine whether function words gen-
erally appear on the left or the right periphery of
phrases in the language they are learning? This
question is important because knowing the side
where function words preferentially occur is re-

288



lated to the question of the direction of syntac-
tic headedness in the language, and an accurate
method for identifying the location of function
words might be useful for initialising a syntac-
tic learner. Experimental evidence suggests that
infants as young as 8 months of age already ex-
pect function words on the correct side for their
language — left-periphery for Italian infants and
right-periphery for Japanese infants (Gervain et
al., 2008) — so it is interesting to see whether
purely distributional learners such as the ones
studied here can identify the correct location of
function words in phrases.

We experimented with a variety of approaches
that use a single adaptor grammar inference pro-
cess, but none of these were successful. For ex-
ample, we hoped that given an Adaptor Gram-
mar that permits “function words” on both the
left and right periphery, the inference procedure
would decide that the right-periphery rules simply
are not used in a language like English. Unfortu-
nately we did not find this in our experiments; the
right-periphery rules were used almost as often as
the left-periphery rules (recall that a large fraction
of the words in English child-directed speech are
monosyllabic).

In this section, we show that learners could use
Bayesian model selection to determine that func-
tion words appear on the left periphery in English
by comparing the marginal probability of the data
for the left-periphery and the right-periphery mod-
els.

Instead, we used Bayesian model selection
techniques to determine whether left-peripheral
or a right-peripheral model better fits the un-
segmented utterances that constitute the training
data.2 While Bayesian model selection is in prin-
ciple straight-forward, it turns out to require the ra-
tio of two integrals (for the “evidence” or marginal
likelihood) that are often intractable to compute.

Specifically, given a training corpus D of unseg-
mented sentences and model families G1 and G2
(here the “function word” adaptor grammars with
left-peripheral and right-peripheral attachment re-
spectively), the Bayes factor K is the ratio of the
marginal likelihoods of the data:

K =
P(D | G1)
P(D | G2)

2Note that neither the left-peripheral nor the right-
peripheral model is correct: even strongly left-headed lan-
guages like English typically contain a few right-headed con-
structions. For example, “ago” is arguably the head of the
phrase “ten years ago”.

0

2000

4000

6000

1 10 100 1000 10000
Number of training sentences

log
 B

ay
es 

fac
tor

Figure 3: Bayes factor in favour of left-peripheral
“function word” attachment as a function of the
number of sentences in the training corpus, cal-
culated using the Harmonic Mean estimator (see
warning in text).

where the marginal likelihood or “evidence” for a
model G is obtained by integrating over all of the
hidden or latent structure and parameters θ:

P(D | G) =
∫

∆
P(D, θ | G) dθ (31)

Here the variable θ ranges over the space ∆ of all
possible parses for the utterances in D and all pos-
sible configurations of the Pitman-Yor processes
and their parameters that constitute the “state” of
the Adaptor Grammar G. While the probability of
any specific Adaptor Grammar configuration θ is
not too hard to calculate (the MCMC sampler for
Adaptor Grammars can print this after each sweep
through D), the integral in (31) is in general in-
tractable.

Textbooks such as Murphy (2012) describe a
number of methods for calculating P(D | G), but
most of them assume that the parameter space ∆
is continuous and so cannot be directly applied
here. The Harmonic Mean estimator (32) for (31),
which we used here, is a popular estimator for
(31) because it only requires the ability to calcu-
late P(D, θ | G) for samples from P(θ | D, G):

P(D | G) ≈
(

1
n

n∑
i=1

1
P(D, θi | G)

)−1
where θi, . . . , θn are n samples from P(θ |

289



D, G), which can be generated by the MCMC pro-
cedure.

Figure 3 depicts how the Bayes factor in favour
of left-peripheral attachment of “function words”
varies as a function of the number of utter-
ances in the training data D (calculated from the
last 1000 sweeps of 8 MCMC runs of the cor-
responding adaptor grammars). As that figure
shows, once the training data contains more than
about 1,000 sentences the evidence for the left-
peripheral grammar becomes very strong. On the
full training data the estimated log Bayes factor is
over 6,000, which would constitute overwhelming
evidence in favour of left-peripheral attachment.

Unfortunately, as Murphy and others warn, the
Harmonic Mean estimator is extremely unstable
(Radford Neal calls it “the worst MCMC method
ever” in his blog), so we think it is important to
confirm these results using a more stable estima-
tor. However, given the magnitude of the differ-
ences and the fact that the two models being com-
pared are of similar complexity, we believe that
these results suggest that Bayesian model selec-
tion can be used to determine properties of the lan-
guage being learned.

5 Conclusions and future work

This paper showed that the word segmentation
accuracy of a state-of-the-art Adaptor Grammar
model is significantly improved by extending it
so that it explicitly models some properties of
function words. We also showed how Bayesian
model selection can be used to identify that func-
tion words appear on the left periphery of phrases
in English, even though the input to the model only
consists of an unsegmented sequence of phones.

Of course this work only scratches the surface
in terms of investigating the role of function words
in language acquisition. It would clearly be very
interesting to examine the performance of these
models on other corpora of child-directed English,
as well as on corpora of child-directed speech in
other languages. Our evaluation focused on word-
segmentation, but we could also evaluate the ef-
fect that modelling “function words” has on other
aspects of the model, such as its ability to learn
syllable structure.

The models of “function words” we investi-
gated here only capture two of the 7 linguistic
properties of function words identified in section 1
(i.e., that function words tend to be monosyllabic,
and that they tend to appear phrase-peripherally),
so it would be interesting to develop and explore

models that capture other linguistic properties of
function words. For example, following the sug-
gestion by Hochmann et al. (2010) that human
learners use frequency cues to identify function
words, it might be interesting to develop computa-
tional models that do the same thing. In an Adap-
tor Grammar the frequency distribution of func-
tion words might be modelled by specifying the
prior for the Pitman-Yor Process parameters asso-
ciated with the function words’ adapted nontermi-
nals so that it prefers to generate a small number
of high-frequency items.

It should also be possible to develop models
which capture the fact that function words tend not
to be topic-specific. Johnson et al. (2010) and
Johnson et al. (2012) show how Adaptor Gram-
mars can model the association between words
and non-linguistic “topics”; perhaps these models
could be extended to capture some of the semantic
properties of function words.

It would also be interesting to further explore
the extent to which Bayesian model selection is a
useful approach to linguistic “parameter setting”.
In order to do this it is imperative to develop better
methods than the problematic “Harmonic Mean”
estimator used here for calculating the evidence
(i.e., the marginal probability of the data) that can
handle the combination of discrete and continuous
hidden structure that occur in computational lin-
guistic models.

As well as substantially improving the accuracy
of unsupervised word segmentation, this work is
interesting because it suggests a connection be-
tween unsupervised word segmentation and the in-
duction of syntactic structure. It is reasonable to
expect that hierarchical non-parametric Bayesian
models such as Adaptor Grammars may be useful
tools for exploring such a connection.

Acknowledgments

This work was supported in part by the Aus-
tralian Research Council’s Discovery Projects
funding scheme (project numbers DP110102506
and DP110102593), the European Research Coun-
cil (ERC-2011-AdG-295810 BOOTPHON), the
Agence Nationale pour la Recherche (ANR-10-
LABX-0087 IEC, and ANR-10-IDEX-0001-02
PSL*), and the Mairie de Paris, Ecole des Hautes
Etudes en Sciences Sociales, the Ecole Normale
Supérieure, and the Fondation Pierre Gilles de
Gennes.

290



References
N. Bernstein-Ratner. 1987. The phonology of parent-

child speech. In K. Nelson and A. van Kleeck, ed-
itors, Children’s Language, volume 6, pages 159–
174. Erlbaum, Hillsdale, NJ.

M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery.
Machine Learning, 34:71–105.

E. Cauvet, R. Limissuri, S. Millotte, K. Skoruppa,
D. Cabrol, and A. Christophe. 2014. Function
words constrain on-line recognition of verbs and
nouns in French 18-month-olds. Language Learn-
ing and Development, pages 1–18.

A. Christophe, S. Millotte, S. Bernal, and J. Lidz.
2008. Bootstrapping lexical and syntactic acquisi-
tion. Language and Speech, 51(1-2):61–75.

S. B. Cohen, D. M. Blei, and N. A. Smith. 2010. Vari-
ational inference for adaptor grammars. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 564–572,
Los Angeles, California, June. Association for Com-
putational Linguistics.

K. Demuth and E. McCullough. 2009. The prosodic
(re-)organization of childrens early English articles.
Journal of Child Language, 36(1):173–200.

J. Elman, E. Bates, M. H. Johnson, A. Karmiloff-
Smith, D. Parisi, and K. Plunkett. 1996. Rethink-
ing Innateness: A Connectionist Perspective on De-
velopment. MIT Press/Bradford Books, Cambridge,
MA.

V. Fromkin, editor. 2001. Linguistics: An Introduction
to Linguistic Theory. Blackwell, Oxford, UK.

J. Gervain, M. Nespor, R. Mazuka, R. Horie, and
J. Mehler. 2008. Bootstrapping word order in
prelexical infants: A japaneseitalian cross-linguistic
study. Cognitive Psychology, 57(1):56 – 74.

S. Goldwater, T. L. Griffiths, and M. Johnson. 2009.
A Bayesian framework for word segmentation: Ex-
ploring the effects of context. Cognition, 112(1):21–
54.

S. Goldwater, T. L. Griffiths, and M. Johnson. 2011.
Producing power-law distributions and damping
word frequencies with two-stage language models.
Journal of Machine Learning Research, 12:2335–
2382.

P. A. Hallé, C. Durand, and B. de Boysson-Bardies.
2008. Do 11-month-old French infants process ar-
ticles? Language and Speech, 51(1-2):23–44.

J.-R. Hochmann, A. D. Endress, and J. Mehler. 2010.
Word frequency as a cue for identifying function
words in infancy. Cognition, 115(3):444 – 457.

M. Johnson and S. Goldwater. 2009. Improving non-
parameteric Bayesian inference: experiments on un-
supervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 317–325, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.

M. Johnson, T. Griffiths, and S. Goldwater. 2007a.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139–
146, Rochester, New York. Association for Compu-
tational Linguistics.

M. Johnson, T. L. Griffiths, and S. Goldwater. 2007b.
Adaptor Grammars: A framework for specifying
compositional nonparametric Bayesian models. In
B. Schölkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems
19, pages 641–648. MIT Press, Cambridge, MA.

M. Johnson, K. Demuth, M. Frank, and B. Jones.
2010. Synergies in learning words and their refer-
ents. In J. Lafferty, C. K. I. Williams, J. Shawe-
Taylor, R. Zemel, and A. Culotta, editors, Advances
in Neural Information Processing Systems 23, pages
1018–1026.

M. Johnson, K. Demuth, and M. Frank. 2012. Exploit-
ing social information in grounded language learn-
ing via grammatical reduction. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics, pages 883–891, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.

M. Johnson. 2008. Using Adaptor Grammars to iden-
tify synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Lin-
guistics, pages 398–406, Columbus, Ohio. Associ-
ation for Computational Linguistics.

Y. Kedar, M. Casasola, and B. Lust. 2006. Getting
there faster: 18- and 24-month-old infants’ use of
function words to determine reference. Child De-
velopment, 77(2):325–338.

K. Kurihara and T. Sato. 2006. Variational
Bayesian grammar induction for natural language.
In Y. Sakakibara, S. Kobayashi, K. Sato, T. Nishino,
and E. Tomita, editors, Grammatical Inference: Al-
gorithms and Applications, pages 84–96. Springer.

K. P. Murphy. 2012. Machine learning: a probabilistic
perspective. The MIT Press.

V. L. Shafer, D. W. Shucard, J. L. Shucard, and
L. Gerken. 1998. An electrophysiological study of
infants’ sensitivity to the sound patterns of English

291



speech. Journal of Speech, Language and Hearing
Research, 41(4):874.

R. Shi and M. Lepage. 2008. The effect of functional
morphemes on word segmentation in preverbal in-
fants. Developmental Science, 11(3):407–413.

R. Shi and A. Melançon. 2010. Syntactic categoriza-
tion in French-learning infants. Infancy, 15(517–
533).

R. Shi and J. Werker. 2001. Six-months old infants’
preference for lexical words. Psychological Science,
12:71–76.

R. Shi, A. Cutler, J. Werker, and M. Cruickshank.
2006. Frequency and form as determinants of func-
tor sensitivity in English-acquiring infants. The
Journal of the Acoustical Society of America,
119(6):EL61–EL67.

Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hi-
erarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101:1566–1581.

R. Zangl and A. Fernald. 2007. Increasing flexibil-
ity in children’s online processing of grammatical
and nonce determiners in fluent speech. Language
Learning and Development, 3(3):199–231.

292


