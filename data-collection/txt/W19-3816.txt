



















































On GAP Coreference Resolution Shared Task: Insights from the 3rd Place Solution


Proceedings of the 1st Workshop on Gender Bias in Natural Language Processing, pages 107–112
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

107

On GAP coreference resolution shared task: insights from the 3rd place
solution

Artem Abzaliev
GfK SE

Global Data Science
Bamberger Strasse 6, Nuremberg 90425, Germany

artem.abzaliev@gfk.com

Abstract

This paper presents the 3rd-place-winning
solution to the GAP coreference resolution
shared task. The approach adopted consists
of two key components: fine-tuning the BERT
language representation model (Devlin et al.,
2018) and the usage of external datasets dur-
ing the training process. The model uses hid-
den states from the intermediate BERT layers
instead of the last layer. The resulting system
almost eliminates the difference in log loss per
gender during the cross-validation, while pro-
viding high performance.

1 Introduction

The GAP coreference resolution shared task pro-
motes gender fair modelling with its GAP dataset
(Webster et al., 2018). GAP is a coreference
dataset for the resolution of ambiguous pronoun-
name pairs in real-world context. GAP has a
particular focus on the balance of masculine and
feminine pronouns and allows for gender-specific
evaluation. The challenge was hosted by Kaggle
and consisted of two stages. Stage 1 attracted 838
participants, and stage 2 involved 263 participants.

GAP training examples look the following way:

Burnett Stone (Peter Fonda) is Lily’s
grandfather and Lady’s caretaker. He
keeps her in Muffle Mountain.

where her is ambiguous pronoun, Lily is candi-
date mention A, and Lady is the candidate men-
tion B. The data was extracted from Wikipedia,
so, in addition to the text, the source URL of the
article is given. The goal is to predict whether the
ambiguous pronoun refers to the mention A, to the
mention B or to NEITHER of them. The problem
was treated as gold-two-mention task, where the
model has the access to the position of the men-
tions.

2 The data

The GAP dataset is split into training, validation
and test set. Training and test set contain 2000
examples each, validation includes 454 examples.
During the stage 1 of the competition, the test
set was used to evaluate the model performance,
while train and validation sets together were used
for training with 5 fold cross-validation scheme.
This choice initially was made because of the rela-
tively little amount of data and the instability of the
predictions (the score on one fold can significantly
differ from the other fold). There were several er-
rors in labels of all three GAP dataset, reported by
the competition participants1. The current solution
employs the GAP data with manual corrections.

During the stage 2 of the competition, all three
datasets with resulting 4454 observations were
used for the training, while the new test set with
12,359 examples was used for the prediction.

2.1 Additional data

There are several coreference datasets available
for training and evaluating. Besides GAP data,
the presented solution uses four external data
sources: Winobias (Zhao et al., 2018), Wino-
gender (Rudinger et al., 2018), The Definite Pro-
noun Resolution (DPR) Dataset (Rahman and Ng,
2012; Peng et al., 2015) and Ontonotes 5.0 (Prad-
han et al., 2012). Each of them was processed
to be compatible with the GAP format. After the
cleaning this resulted in 39,452 training examples
for Ontonotes 5.0, 360 for Winogender, 3162 for
Winobias and 1400 for DPR. In this paper, this ex-
ternal data (Ontonotes 5.0, Winobias, Winogden-
der, DPR) will be called warm-up data, because it
was used to fine-tune the BERT embeddings, and
the weights learned from this data served as ’warm

1https://www.kaggle.com/c/gendered-pronoun-
resolution/discussion/81331



108

up’ for the training on the GAP dataset.
There was one more candidate to the additonal

datasets pool, namely PreCo (Chen et al., 2018),
but despite many efforts this dataset did not pro-
vide any score improvement. Presumably, this
is mainly due to the different structure of the
data, and the high amount of noise. For instance,
some training examples contained the same word
as both mention and pronoun, which may have
worsen the model performance.

There were several attempts to include all the
additional datasets to the training procedure. The
naive attempt to concatenate GAP data and addi-
tional data into one big training set did not work,
because the additional data has a different struc-
ture and does not have the URL feature. The sec-
ond attempt was to use a two-step approach:

1. Warm-up step: pretrain the part of the model
(namely the head, see section 3 for the expla-
nations) on the external data only. Then, se-
lect the weights from the model that performs
best on the warm-up validation set.

2. GAP step: continue training on GAP data,
using the weight from the best-performing
warm-up model instead of randomly initial-
ized weights.

The warm-up data was randomly split into train-
ing and validation set with 95%-5% proportions.
This strategy slightly improved the model per-
formance, showing that warming-up on external
dataset can be a promising direction. One pos-
sible explanation is that starting with pretrained
weights allowed the model to reach flatter opti-
mum and generalize better. In addition, the exter-
nal data contained many more training examples
for the category NEITHER (see Table 1), result-
ing in better performance for this group.

During the third attempt, the validation set was
not randomly chosen, but replaced by Winobias
only. It was done to ensure that gender fair repre-
sentation will be chosen as initialization weights
for the GAP step. This action further provided
small improvement in the evaluation metric. How-
ever, the negative effect of choosing Winobias as
validation data was the complete exclusion of the
class NEITHER from the validation data (see Ta-
ble 1). Surprisingly, this effect was not detrimental
to the performance, most likely because the train-
ing data contained enough training examples for
this class.

GAP
train

Warm-up
train

Warm-up
val

A 43.71% 31.91% 50%
B 44.65% 31.40% 50%
NEITHER 11.63% 36.68% 0

Table 1: Class distribution for the datasets used. GAP
train includes all the gap datasets available (gap devel-
opment, gap val, gap test). Warm-up train includes
Ontonotes, DPR and Winogender. Warm-up val only
includes Winobias.

The final version of the model also fine-tunes
the particular layers of BERT embeddings, in ad-
dition to the warm-up of the head. For a full de-
scription, see section 3.

2.2 Evaluation metrics and class
distributions

Class distributions Table 1 shows the class dis-
tributions for the three final datasets used: GAP
train, which includes all GAP data, warm-up train,
which includes Ontonotes 5.0, Winogdender, DPR
and warm-up validation, which is Winobias. As
can be seen, warm-up train has the most bal-
anced distribution of classes, while GAP train has
a lower portion of the category NEITHER.

Evaluation metrics Solutions were evaluated
using the multi-class logarithmic loss. For each
pronoun, the participants had to provide the prob-
abilities of it belonging to A, B, or NEITHER. The
formula to evaluate the performance of the model
is:

logloss = − 1
N

N∑
i=1

M∑
j=1

yijlog(pij)

where N is the number of samples in the test set,
M is number of classes, 3, log is the natural log-
arithm, yij is 1 if observation i belongs to class j
and 0 otherwise, and pij is the predicted probabil-
ity that observation i belongs to class j.

2.3 Features

Besides the direct textual input, the current solu-
tion uses some manually constructed features. The
majority of them were already mentioned by Web-
ster et al. (2018) as single baseline models. The
following features were used:

• Token Distance. Distance between mentions
and the pronoun, and also between the men-
tions themselves.



109

• Syntactic distance between mentions and
pronoun. The distances were extracted with
the StandfordCoreNLP.

• URL. Whether the Wikipedia URL contains
the mention.

• Sentence of the mention. Index of the sen-
tence, where the mention is located, divided
by total number of sentences in the snippet.

• Syntactic distance to the sentence root.
The distance between the mention and its
syntactic parent.

• Character position. The relative character
position of the mention in the text.

• Pronoun gender. Gender of the pronoun. It
was noticed that in some examples, mentions
were of different gender, so the hope was that
this feature could help. It did not help, but it
did not hurt either. The fact that this feature
did not affect the performance can be a good
indicator of gender-neutral learning.

The features that provided the biggest improve-
ments were URL (0.07 decrease on log loss) and
syntactic distance between mentions and pronoun
(0.01). The contribution of other features was very
limited.

3 The system

The final solution uses an ensemble of four
neural networks. They are: fifth-to-last-layer
with cased BERT, fifth-to-last-layer with uncased
BERT, sixth-to-last-layer with uncased BERT,
sixth-to-last-layer with cased BERT. The explana-
tion is in the next subsection of the paper. Each
network consists of two main parts:

• BERT part: contextual representations of the
text with fine-tuned BERT embeddings

• Head part: using the embeddings together
with manually crafted features to produce
softmax probabilities of the three classes

All networks have the same architecture for the
head part and the only difference is in the BERT
part.

Figure 1: The effect of the output layer choice on the
performance of the model. Layer index [-5] means
fifth-to-last output layer. Log loss is reported on the
stage 1 test dataset, at the beginning of the competition,
keeping all the other parameters fixed.

3.1 BERT part
BERT is general purpose language model, pre-
trained on Wikipedia and BookCorpus. It lever-
ages high amount of unannotated data on the web
and produces context-aware word embeddings.
The current solution uses pytorch-pretrained-
BERT2. Besides fine-tuning BERT weights, the set
of possible parameters for the pretrained BERT is
limited. The possibilities are:

• Amount of layers in the Transformer model:
12-layer (bert-base) or 24-layer (bert-large)

• Cased or uncased model

• Multilingual or single-language model

One additional peculiarity, discovered by sev-
eral contestants independently, is that using the
output of the last layer may be an inferior option
compared to the deeper ones. For the architec-
ture presented in this paper, optimal layers were
fifth-to-last ([-5]) and sixth-to-last ([-6]). Figure
1 shows the log loss during stage 1 for different
ouput layers, keeping all other parameters of the
network fixed.

One possible explanation for this phenomenon
is that the last output layer specializes on pre-
dicting the masked words, while the intermediate
layers contain more general information. The U -
shaped curve also shows that taking much deeper
layers negatively affect the model performance.

Fine-tuning. As mentioned in the section 2.1,
initially only the head part was fine-tuned on the

2https://github.com/huggingface/pytorch-pretrained-
BERT



110

warm-up dataset and the learned weights were
used as initialization weights for the GAP train-
ing. The logical step would also be to fine-tune
the BERT embeddings themselves. It was done
in the following way: first, the head was trained
for 16 epochs, with the parameters of BERT being
frozen. Afterwards, all of the parameters of the
head were frozen, and one particular layer (either
fifth-to-last or sixth-to-last) of BERT was fine-
tuned. The small learning rate was very crucial
at this step, because the number of parameters is
high (12,596,224). The current solution uses 4e−5

as learning rate, trained for 16 epochs with batch
size of 32. Every 400 steps the network perfor-
mance was estimated on the evaluation data, and
only the best performing model was used after the
training was done.

3.2 Head part

The head network always takes BERT contextual
embeddings of shape [batch size, seq len, 1024]
(for BERT-large). These embeddings are pro-
cessed with the 1d-convolutional layer of size 64
and kernel=1 in order to reduce the dimensional-
ity. Interestingly, increasing kernel size to 2 or 3
deteriorates the performance. This may be due to
the fact that the context just around the mentions
was not that informative.

Because the positions of mentions and pronoun
are known in advance, the embeddings of only
those three phrases are extracted. This is done by
using SelfAttentiveSpanExtractor from AllenNLP
(Gardner et al., 2017). This span extractor will
generate 3 vectors of size 64 - for A, B and Pro-
noun. For single token mentions the span repre-
sentation is just the original vector itself, while
for mentions with more than two tokens, the span
extractor will produce weighted representation by
using the attention scores. Other span extractors
from AllenNLP did not perform as good as the
self-attentive span extractor.

The resulting three vectors of size 64 are con-
catenated and processed with the standard fully-
connected block: BatchNorm1d(192) → Linear
(192, 64) → ReLU → BatchNorm1d → Dropout
(0.6). This output is concatenated with all the
manual features mentioned in the section 2.3,
which results in the vector of length 96. Adding
features directly to the last layer is important, oth-
erwise they do not bring any improvement. Fi-
nally, Linear (96, 3) layer on top produces the log-

its. The softmax probabilities are computed in the
numerically stable way in the loss function.

3.3 Training details

For the GAP training, Adam optimizer with the
learning rate 2e−3 is used. The batch size is 20.
For both BERT fine-tuning and GAP training the
triangular learning rate schedule is used (Smith,
2017). The loss used is CrossEntropyLoss, which
combines numerically stable computation of soft-
max probabilities with negative log-likelihood loss
function.

The predictions for each of the four networks
are done in 10 fold cross-validation stratified by
the class distribution, i.e. the model is trained on
90% of the data and the other 10% is used for
the evaluation. The final predictions is the aver-
age across all folds and all models, overall of 40
models. Final predictions were clipped to be in
the interval (1e−2, 1− 1e−2), because log loss pe-
nalizes the predictions heavily as they drift away
from ground truth.

The training runs approximately one day on sin-
gle NVIDIA Tesla P100. This can be substantially
reduced with proper code optimization (for in-
stance, removing BERT computations for all lay-
ers after the fifth-to-last). The framework used for
the implementation is PyTorch3

4 Results

The described solution provides the log loss of
0.1839 on the test stage 2 data, which results in
the third place on Kaggle leaderboard. The results
of single models are presented in the Table 3. As
can be seen, the cased version performs generally
better. One reason may be given by the variety of
personal names in the GAP, and the cased version
is able to recognize them better.

On the cleaned stage 1 test data, the best-
performing single model (cased BERT, fifth-to-
last layer) provided the log loss of 0.23819. Be-
cause the true labels are available for the test stage
1 data, the loss for the masculine and the feminine
pronouns was estimated separately. For masculine
pronouns the log loss was equal to 0.24014, while
for feminine it was equal to 0.23623. The differ-
ence is 3e−3, which can be considered insignifi-
cant.

The error matrix for the whole stage one dataset
(10-fold cross-validated) is presented in the Table

3https://pytorch.org/



111

A pred B pred NEITHER pred
A 1849 72 26
B 57 1908 24
NEITHER 58 59 401

Table 2: Confusion matrix on the whole stage 1 data
with 10 fold cross validation.

Model name Log loss
Cased [-5] 0.20592
Cased [-6] 0.20429
Uncased [-5] 0.22834
Uncased [-6] 0.21088
Ensemble 0.18397

Table 3: Performance of the models on the test stage 2
data.

2. As can be seen, the performance for the cate-
gory NEITHER is worse than for other categories.
Most likely this is because of the lower amount
of training examples. Even though the warm-up
data contained many observations with this cate-
gory, these examples were quite trivial, and the fi-
nal model still struggles on the GAP data. This
indicates a potential area for the improvement of
the model. For the wrong predictions, some addi-
tional metrics were also examined, like the length
of the text or the number of words in the men-
tions. These properties are not significantly dif-
ferent from those for the correct predictions. Af-
ter manual examination, it appears that the model
makes mistakes on the examples that are also quite
challenging for humans.

5 Discussion

One of the weaknesses of the presented system is
the training and prediction time. Because there are
four networks, training takes a long time, and the
prediction on the test set requires almost 2 hours.
The attempt to concatenate several intermediate
BERT layers, or to use a linear combination of
them did not work, although it was reported to
have a positive effect (Tenney et al., 2018). Using
the information from the whole Wikipedia page
also did not provide any improvements.

During the early stages of the competition,
when only GAP data was used, the sources of
model mistakes were analyzed. It was found, that
despite the best efforts of the authors, there are still
some mislabelled examples in the GAP data itself.

dataset errors female errors male
gap development 35 31
gap val 12 9
gap test 45 28

Table 4: Number of mislabelled examples in the
datasets per gender.

Other participants reported errors as well4. Some
of these errors are quite simple, but the majority
require substantial human efforts and sometimes
were impossible to detect without reading the cor-
responding Wikipedia article.

The number of erroneous labels for different
GAP datasets separated by gender is reported in
Table 4. This list is based on the mistakes reported
on the forum, as well as own single checks, but it is
not comprehensive. It can be seen that mislabelled
examples represent less than 5% of all cases. They
are usually equally distributed between genders,
besides gap test, where mislabelled examples for
female cases are 30% higher.

6 Conclusion

This paper presents the solution for the corefer-
ence resolution on GAP shared task. The solu-
tion utilizes the pretrained contextual embeddings
from BERT and fine-tunes them for the corefer-
ence problem on additional data. One of the find-
ings is that the output of BERT’s intermediate lay-
ers gives better representation of the input text
for the coreference task. Another contribution is
that the gender bias in external data can be mit-
igated by using gender-fair datasets as validation
data during the pretraining phase.

References
Hong Chen, Zhenhua Fan, Hao Lu, Alan L Yuille,

and Shu Rong. 2018. Preco: A large-scale dataset
in preschool vocabulary for coreference resolution.
arXiv preprint arXiv:1810.09807.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew
Peters, Michael Schmitz, and Luke S. Zettlemoyer.

4https://www.kaggle.com/c/gendered-pronoun-
resolution/discussion/81331



112

2017. Allennlp: A deep semantic natural language
processing platform.

Haoruo Peng, Daniel Khashabi, and Dan Roth. 2015.
Solving hard coreference problems. In Proceed-
ings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
809–819.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in ontonotes. In Joint Confer-
ence on EMNLP and CoNLL-Shared Task, pages 1–
40. Association for Computational Linguistics.

Altaf Rahman and Vincent Ng. 2012. Resolving
complex cases of definite pronouns: the winograd
schema challenge. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 777–789. Association for
Computational Linguistics.

Rachel Rudinger, Jason Naradowsky, Brian Leonard,
and Benjamin Van Durme. 2018. Gender
bias in coreference resolution. arXiv preprint
arXiv:1804.09301.

Leslie N Smith. 2017. Cyclical learning rates for train-
ing neural networks. In 2017 IEEE Winter Confer-
ence on Applications of Computer Vision (WACV),
pages 464–472. IEEE.

Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang,
Adam Poliak, R Thomas McCoy, Najoung Kim,
Benjamin Van Durme, Samuel R Bowman, Dipan-
jan Das, et al. 2018. What do you learn from con-
text? probing for sentence structure in contextual-
ized word representations.

Kellie Webster, Marta Recasens, Vera Axelrod, and Ja-
son Baldridge. 2018. Mind the gap: A balanced
corpus of gendered ambiguous pronouns. Transac-
tions of the Association for Computational Linguis-
tics, 6:605–617.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2018. Gender bias in
coreference resolution: Evaluation and debiasing
methods. arXiv preprint arXiv:1804.06876.

http://arxiv.org/abs/arXiv:1803.07640
http://arxiv.org/abs/arXiv:1803.07640

