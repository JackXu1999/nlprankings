



















































Spectral Graph-Based Method of Multimodal Word Embedding


Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for Natural Language Processing, ACL 2017, pages 39–44,
Vancouver, Canada, August 3, 2017. c©2017 Association for Computational Linguistics

Spectral Graph-Based Method of Multimodal Word Embedding ∗

Kazuki Fukui♠♡ and Takamasa Oshikiri♢♡ and Hidetoshi Shimodaira♠♡
♠ Department of Systems Science, Graduate School of Informatics, Kyoto University
♡ Mathematical Statistics Team, RIKEN Center for Advanced Intelligence Project

♢ Division of Mathematical Science, Graduate School of Engineering Science, Osaka University
k.fukui@sys.i.kyoto-u.ac.jp

oshikiri@sigmath.es.osaka-u.ac.jp
shimo@i.kyoto-u.ac.jp

Abstract

In this paper, we propose a novel method
for multimodal word embedding, which
exploit a generalized framework of multi-
view spectral graph embedding to take into
account visual appearances or scenes de-
noted by words in a corpus. We evalu-
ated our method through word similarity
tasks and a concept-to-image search task,
having found that it provides word repre-
sentations that reflect visual information,
while somewhat trading-off the perfor-
mance on the word similarity tasks. More-
over, we demonstrate that our method cap-
tures multimodal linguistic regularities,
which enable recovering relational simi-
larities between words and images by vec-
tor arithmetic.

1 Introduction

Word embedding plays important roles in the field
of Natural Language Processing (NLP). Many ex-
isting studies use word vectors for various down-
stream NLP tasks, such as text classification,
Part-of-Speech tagging, and machine translation.
One of the most famous approaches is skip-gram
model (Mikolov et al., 2013), which is based on a
neural network, and its extensions have also been
widely studied as well.

There are alternative approaches depending on
a spectral graph embedding framework (Yan et al.,
2007; Huang et al., 2012) for word embedding.
For examples, Dhillon et al. (2015) proposed a
method based on Canonical Correlation Analysis
(CCA) (Hotelling, 1936), while a PCA based word
embedding method was proposed in Lebret and
Collobert (2014).

∗ This work was partially supported by grants from Japan
Society for the Promotion of Science KAKENHI (16H02789)
to HS.

In recent years, many researchers have been
actively studying the use of multiple modalities
in the fields of both NLP and computer vision.
Those studies combine textual and visual informa-
tion to propose methods for image-caption match-
ing (Yan and Mikolajczyk, 2015), caption gen-
eration (Kiros et al., 2014), visual question an-
swering (Antol et al., 2015), quantifying abstract-
ness (Kiela et al., 2014) of words, and so on.

As for word embedding, multimodal versions of
word2vec (Mikolov et al., 2013) have been pro-
posed in Lazaridou et al. (2015) and Kottur et al.
(2016). The first one jointly optimize the objec-
tive of both skip-gram model and a cross-modal
objective across texts and images, and the latter
uses abstract scenes as surrogate labels for captur-
ing visually grounded semantic relatedness. More
recently, Mao et al. (2016) proposed a multimodal
word embedding methods based on a recurrent
neural network to learn word vectors from their
newly proposed large scale image caption dataset.

In this paper, we introduce a new spectral graph-
based method of multimodal word embedding.
Specifically, we extend Eigenwords (Dhillon et al.,
2015), a CCA-based method for word embedding,
by applying a generalized framework of spectral
graph embedding (Nori et al., 2012; Shimodaira,
2016). Figure 1 shows a schematic diagram of our
method.

In the rest of this paper, we call our method
Multimodal Eigenwords (MM-Eigenwords).
The most similar existing method is Multimodal
Skip-gram model (MMskip-gram) (Lazaridou
et al., 2015), which slightly differ in that our
model can easily deal with many-to-many rela-
tionships between words in a corpus and their
relevant images, while MMskip-gram only con-
siders one-to-one relationships between concrete
words and images.

Using a corpus and datasets of image-word rela-

39



Figure 1: Our proposed method extends a CCA-
based method of word embedding by means of
multi-view spectral graph embedding frameworks
of dimensionality reduction to deal with visual in-
formation associated with words in a corpus.

tionships, which are available in common bench-
mark datasets or on online photo sharing services,
MM-Eigenwords jointly learns word vectors on
a common multimodal space and a linear map-
ping from a visual feature space to the multimodal
space. Those word vectors also reflect similarities
between words and images.

We evaluated the multimodal word representa-
tions obtained by our model through word sim-
ilarity task and concept-to-image search, having
found that our model has ability to capture both
semantic and word-to-image similarities. We also
found that our model captures multimodal linguis-
tic regularities (Kiros et al., 2014), whose exam-
ples are shown in Figure 2b.

2 Multi-view Spectral Graph Embedding

A spectral graph perspective of dimensionality re-
duction was first proposed in Yan et al. (2007),
which showed that several major statistical meth-
ods for dimensionality reduction, such as PCA and
Eigenmap (Belkin and Niyogi, 2003), can be writ-
ten in a form of graph embedding frameworks,
where data points are nodes and those points have
weighted links between other points. Huang et al.
(2012) extended this work for two-view data with
many-to-many relationships (or links) and showed
that their two-view graph embedding framework
includes CCA, one of the most popular method
for multi-view data analysis, as its special cases.

However, available datasets may have more than
two views with complex graph structures, which
are unmanageable for CCA or Multiset CCA (Ket-
tenring, 1971) whose inputs must be fed in the
form of n-tuples.

Shimodaira (2016) further generalized the
graph embedding frameworks to deal with many-
to-many relationships between any number of
views, and Nori et al. (2012) also proposed an
equivalent method for multimodal relation predic-
tion in social data. This generalized framework
is used to extend Eigenwords for cross-lingual
word embedding (Oshikiri et al., 2016), where vo-
cabularies and contexts of multiple languages are
linked through sentence-level alignment. Our pro-
posed method also makes use of the framework
of Shimodaira (2016) to extend Eigenwords for
multimodal word embedding.

3 Eigenwords (One Step CCA)

Canonical Correlation Analysis (Hotelling, 1936)
is a multivariate analysis method for finding
optimal projections of two sets of data vec-
tors by maximizing the correlations. Apply-
ing CCA to pairs of raw word vectors and raw
context vectors, Eigenwords algorithms attempt
to find low-dimensional vector representations of
words (Dhillon et al., 2015). Here we explain the
simplest version of Eigenwords called One Step
CCA (OSCCA).

We have a corpus consisting of T tokens;
(ti)i=1,...,T , and the vocabulary consisting of V
word types; {vi}i=1,...,V . Each token ti is drawn
from this vocabulary. We define a word matrix
V ∈ {0, 1}T×V whose i-th row encodes the to-
ken ti by 1-of-V representation; the j-th element
is 1 if the word type of ti is vj , 0 otherwise.

Let h be the size of context window. We de-
fine context matrix C ∈ {0, 1}T×2hV whose i-th
row represents the surrounding context of the to-
ken ti with concatenated 1-of-V encoded vectors
of (ti−h, . . . , ti−1, ti+1, . . . , ti+h).

We apply CCA to T pairs of row vectors of V
and C. The objective function of CCA is con-
structed using V⊤V, V⊤C, C⊤C which rep-
resent occurrence and co-occurrence counts of
words and contexts. In Eigenwords, however, we
use CV V ∈ RV×V+ , CV C ∈ RV×2hV+ , CCC ∈
R2hV×2hV+ with the following preprocessing of
these matrices before constructing the objective
function. First, centering-process of V and C is

40



"bird"

"bird" + "white"

"bird" + "flying"

"birds" "feathers" "bird watcher" 

    "avain"  "aves" "raptor" "perch" "hawk"

Query Top Match

(a) Word-to-Image Search.

- "day" + "night" �

- "brown" + "white" �

- "yellow" + "red" �

(b) Examples of Multimodal Linguistic Regularities.

Figure 2: Examples of word-to-image search (a) and demonstrations of vector arithmetics between words
and images (b). We chose η = 106 in these examples.

omitted, and off-diagonal elements of C⊤C are
ignored for simplifying the computation of in-
verse matrices. Second, we take the square root
of the elements of these matrices for “squash-
ing” the heavy-tailed word count distributions. Fi-
nally, we obtain vector representations of words as
C−1/2V V (u1, . . . , uK), where u1, . . . , uK ∈ RV are
left singular vectors of C−1/2V V CV C C−1/2CC corre-
sponding to the K largest singular values.

For the fast and scalable computation, Dhillon
et al. (2015) employed the method of Halko et al.
(2011) which use random projections to compute
singular value decomposition of large matrices.

4 Multimodal Eigenwords

In this section, we introduce Multimodal Eigen-
words (MM-Eigenwords) by extending the CCA
based model of Eigenwords to obtain multimodal
representations across words and images.

Suppose we have Nvis images, and each image
is associated with multiple tags (or words). These
associations are denoted by w̃ij ≥ 0 (1 ≤ i ≤
V, 1 ≤ j ≤ Nvis), whose value represents the
strength of a relationship between the i-th word
and the j-th image. In this study, for example,
w̃ij = 1 if the j-ith image has the i-th word as
its tag, whereas w̃ij = 0 otherwise, and we de-
fine a matrix W̃V X = (w̃ij). In addition, we de-
note a image feature matrix by Xvis ∈ RNvis×pvis
and its i-th row vector xi, as well as row vec-
tors of V, C by vi, ci respectively. Here, the
goal of MM-Eigenwords is to obtain multimodal
representations by extending the CCA in Eigen-
words with generalized frameworks of multi-view
spectral graph embedding (Nori et al., 2012; Shi-
modaira, 2016), which include CCA as their spe-
cial cases. In these frameworks, our goal can be at-

tained by finding an optimal linear mappings to the
K-dimensional multimodal space AV , AC , Avis
that minimize the following objective with a scale
constraint.

T∑
i=1

∥viAV − ciAC∥22

+
T∑

i=1

Nvis∑
j=1

ηwij∥viAV − xjAvis∥22, (1)

where wij = (VW̃V X)ij , and the multimodal
term coefficient η ≥ 0 determines to which extent
the model reflects the visual information. Consid-
ering a scale constraint, Eq. (1) can be reformu-
lated as follows:

We first define some matrices

X =

V O OO C O
O O Xvis

 ,W =
 O IT WV XIT O O

W⊤V X O O

 ,
M = diag(W1),A⊤ = (A⊤V ,A

⊤
C ,A

⊤
vis),WV X = (ηwij),

then the optimization problem of Eq. (1) can be
written as

max
A

Tr
(
A⊤X⊤WXA

)
subject to A⊤X⊤MXA = IK . (2)

Similar to Eigenwords, we squash X⊤WX and
X⊤MX in Eq. (2) by replacing them with H, G
respectively, which are defined as follows.

H =

 O CV C ηCV V W̃V XXvisC⊤V C O O
ηX⊤visW̃

⊤
V XCV V O O

 ,
G =

(
GV V O O
O CCC O
O O Gvis

)
,

41



where diag(v) is a diagonal matrix aligning v as
its diagonal elements, sqrt(·) represents element-
wise square root, the vectors m, n are defined as
m = sqrt(V⊤1), n = ηW̃V X1, ◦ represents
element-wise product, and

GV V = CV V + diag(m ◦ n),
Gvis = ηX⊤visdiag(W̃

⊤
V Xm)Xvis.

Consequently, our final goal here is to find
an optimal linear mapping which maxi-
mizes Tr(A⊤HA) subject to A⊤GA = IK ,
and this problem reduces to a general-
ized eigenvalue problem Ha = λGa.
Hence, we can obtain the optimal solution as
Â
⊤

= (Â
⊤
V , Â

⊤
C , Â

⊤
vis) = G−1/2(u1, . . . , uK),

where u1, . . . , uK are eigenvectors of
(G−1/2)⊤HG−1/2 for the K largest eigenvalues.
Note that we obtain the word representations
as the rows of ÂV , as well as a linear mapping
from the visual space to the common multimodal
space Âvis, and that when visual data Xvis is
omitted from the model, Eq. (2) is equivalent to
CCA, namely, the ordinary Eigenwords. There
are several ways to solve a generalized eigenvalue
problem. In this study, we employed a randomized
method for a generalized Hermitian eigenvalue
problem proposed in Saibaba et al. (2016).

Silberer and Lapata (2012) also uses CCA to
obtain multimodal representations, which asso-
ciates term-document matrix representing word
occurrences in documents and perceptual matrix
containing scores on feature norms (or attributes)
like “is brown”, “has fangs”, etc. This model is
not considering any recent developments in word
embedding. In addition, the feature norms are
expensive to obtain, and hence we cannot expect
them for a large number vocabularies. Besides,
images relevant to a given word are more easy to
collect.

5 Experiments

5.1 Dataset
In our experiment, we used English Wikipedia
corpus (2016 dump)1, which consists of ap-
proximately 3.9 billion tokens. We first used
the script provided by Mahoney2 to clean up
the original dump. Afterward, we applied
word2phrase (Mikolov et al., 2013) to the original

1https://dumps.wikimedia.org/enwiki/
2http://mattmahoney.net/dc/textdata.html

corpus twice with a threshold value 500 to obtain
multi-term phrases.

As for visual data, we downloaded images from
the URLs in the NUS-WIDE image dataset (Chua
et al., 2009), which also provides Flickr tags of
each image. Although Flickr tags associated with
each image could be very noisy and have vary-
ing abstractness, they provides a rich source of
many-to-many relationships between images and
words. Since we were interested in investigating if
the large, but noisy web data would play a role as
a helpful source for multimodal word representa-
tions, we omitted preprocessing like manually re-
moving noisy tags or highly abstract tags.

The images were converted to 4096-dim fea-
ture vectors using the Caffe toolkit (Jia et al.,
2014), together with a pre-trained3 AlexNet
model (Krizhevsky et al., 2012). These feature
vectors are the output of the fc7 layer on the
AlexNet. We randomly selected 100k images for
a training set.

5.2 Word Similarity Task

We compared MM-Eigenwords against Eigen-
words and skip-gram model through word simi-
larity tasks, a common evaluation method of vec-
tor word representations. In our experiments, we
used MEN (Bruni et al., 2014), SimLex (Hill et al.,
2015), and another semantic similarity (Silberer
and Lapata, 2014) denoted as SemSim, which pro-
vide 3000, 999, and 7576 word pairs respectively.
These datasets provide manually scored word sim-
ilarities, and the last one also provides visual sim-
ilarity scores of word pairs denoted as VisSim.
As for model-generated word vectors, the seman-
tic similarity between two word vectors was mea-
sured by cosine similarity, and we quantitatively
evaluated each embedding method by calculating
Spearman correlation between model-based and
human annotated scores.

5.3 Concept-to-Image Search

We also evaluated the accuracy of concept-to-
image search to investigate the extent to which our
multimodal word representations reflect visual in-
formation. In this experiment, we used 81 man-
ually annotated concepts provided in NUS-WIDE
dataset as queries. In addition, we randomly se-
lected 10k images which are absent during the
training phase as test-images and used Âvis to

3
https://github.com/BVLC/caffe/tree/master/models/

42



Word Simliarity Task Concept-to-Image Search
Method η MEN SimLex SemSim VisSim P@1 P@5 P@10

Skip-gram 0.77 0.40 0.67 0.54
Eigenwords 0.75 0.45 0.68 0.58
MM-Eigewords 0.01 0.77 0.41 0.71 0.57 0.21 0.23 0.22
MM-Eigewords 0.1 0.78 0.38 0.72 0.57 0.14 0.14 0.14
MM-Eigewords 1 0.74 0.34 0.72 0.57 0.12 0.14 0.14
MM-Eigewords 104 0.66 0.21 0.37 0.34 0.44 0.39 0.37
MM-Eigewords 106 0.61 0.20 0.29 0.29 0.53 0.47 0.49

Table 1: Spearman correlations between word similarities based on the word vectors and that of the
human annotations, and the right part shows the accuracies of concept-to-image search evaluated by
precision@k.

project them to the textual space, on which top-
match images were found by cosine similarities
with the query vectors. We evaluated the accu-
racies of image search by precision at 1, 5, and
10, averaged over all query concepts, while vary-
ing the value of the multimodal term coefficient η
in Eq. (1).

6 Results

For Eigenwords and MM-Eigenwords, we set the
number of word types to V ≈ 140k, including 30k
most frequent vocabularies, words in the bench-
marks, and Flickr tags associated with training-
images, and we set the number of power itera-
tion to 3. As for skip-gram model, we set the
subsampling threshold to 10−5, number of neg-
ative examples to 5, and training iterations to 5.
In addition we fixed the dimensionality of word
vectors to K = 500, and the context window
size to h = 4 for every methods. As mentioned
in Section 1, one of the most related methods is
MMSkip-gram, against which we should compare
MM-Eigenwords. However, since we could not
find its code nor implement it by ourselves, a com-
parative study with MMSkip-gram is not included
in this paper.

Table 1 shows the results of the word similarity
tasks. As we can see in the table, with smaller
η, the performance on word-similarity tasks of
MM-Eigenwords is similar to that of Eigenwords
or skip-gram model, whereas poor results on the
concept-to-image search task. On the other hand,
larger η helps improve the performance on the
concept-to-image search while sacrificing the per-
formances on the word similarity tasks. These
results implies that too strongly associated visual
information can distort the semantic structure ob-
tained from textual data. Despite some similar ex-

isting studies showed positive results with auxil-
iary visual features (Lazaridou et al., 2015; Kiela
and Bottou, 2014; Hill et al., 2014), our results
achieved less improvements in the word-similarity
tasks, indicating negative transfer of learning.

However, the visual informative word vectors
obtained by our method enable not only word-to-
word but also word-to-image search as shown in
Figure 2a, and the many-to-many relationships be-
tween images and a wide variety of tags fed to
our model contributed to the plausible retrieval re-
sults with the sum of two word vectors as their
queries (e.g. “bird” + “flying” ≈ images of flying
birds). Moreover, the word vectors learned with
our model capture multimodal linguistic regulari-
ties (Kiros et al., 2014). We show some examples
of our model in Figure 2b.

7 Conclusion

In this paper, we proposed a spectral graph-based
method of multimodal word embedding. Our ex-
perimental results showed that MM-Eigenwords
captures both semantic and text-to-image similari-
ties, and we found that there is a trade-off between
these two similarities.

Since the framework we used can be adopted
to any number of views, we could further extend
our method by considering image caption datasets
through employing document IDs like Oshikiri
et al. (2016) in our future works.

References
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-

garet Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. 2015. VQA: Visual question an-
swering. In ICCV . pages 2425–2433.

Mikhail Belkin and Partha Niyogi. 2003. Laplacian
eigenmaps for dimensionality reduction and data

43



representation. Neural computation 15(6):1373–
1396.

Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014. Multimodal distributional semantics. JAIR
49(2014):1–47.

Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li,
Zhiping Luo, and Yantao Zheng. 2009. Nus-wide:
a real-world web image database from national uni-
versity of singapore. In Proceedings of the ACM in-
ternational conference on image and video retrieval.
ACM, page 48.

Paramveer S Dhillon, Dean P Foster, and Lyle H Un-
gar. 2015. Eigenwords: Spectral word embeddings.
JMLR 16:3035–3078.

Nathan Halko, Per-Gunnar Martinsson, and Joel A
Tropp. 2011. Finding structure with random-
ness: Probabilistic algorithms for constructing ap-
proximate matrix decompositions. SIAM review
53(2):217–288.

Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Multi-modal models for concrete and abstract con-
cept meaning. TACL 2:285–296.

Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics pages 665–695.

Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika 28(3/4):321–377.

Zhiwu Huang, Shiguang Shan, Haihong Zhang, Shi-
hong Lao, and Xilin Chen. 2012. Cross-view graph
embedding. In ACCV . Springer, pages 770–781.

Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio
Guadarrama, and Trevor Darrell. 2014. Caffe: Con-
volutional architecture for fast feature embedding.
In Proceedings of the 22nd ACM international con-
ference on Multimedia. ACM, pages 675–678.

Jon R Kettenring. 1971. Canonical analysis of several
sets of variables. Biometrika 58(3):433–451.

Douwe Kiela and Léon Bottou. 2014. Learning image
embeddings using convolutional neural networks for
improved multi-modal semantics. In EMNLP. pages
36–45.

Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is sometimes
more. In ACL. pages 835–841.

Ryan Kiros, Ruslan Salakhutdinov, and Richard S
Zemel. 2014. Unifying visual-semantic embeddings
with multimodal neural language models. arXiv
preprint arXiv:1411.2539 .

Satwik Kottur, Ramakrishna Vedantam, José MF
Moura, and Devi Parikh. 2016. Visual word2vec
(vis-w2v): Learning visually grounded word embed-
dings using abstract scenes. In CVPR. pages 4985–
4994.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In NIPS. pages 1097–
1105.

Angeliki Lazaridou, Nghia The Pham, and Marco Ba-
roni. 2015. Combining language and vision with
a multimodal skip-gram model. In HLT-NAACL.
pages 153–163.

Rémi Lebret and Ronan Collobert. 2014. Word embed-
dings through hellinger pca. In EACL. pages 482–
490.

Junhua Mao, Jiajing Xu, Kevin Jing, and Alan L Yuille.
2016. Training and evaluating multimodal word em-
beddings with large-scale web annotated images. In
NIPS. pages 442–450.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS. pages 3111–3119.

Nozomi Nori, Danushka Bollegala, and Hisashi
Kashima. 2012. Multinomial relation prediction in
social data: A dimension reduction approach. In
AAAI. pages 115–121.

Takamasa Oshikiri, Kazuki Fukui, and Hidetoshi Shi-
modaira. 2016. Cross-lingual word representations
via spectral graph embeddings. In ACL. pages 493–
498.

Arvind K Saibaba, Jonghyun Lee, and Peter K Kitani-
dis. 2016. Randomized algorithms for generalized
hermitian eigenvalue problems with application to
computing karhunen–loève expansion. Numerical
Linear Algebra with Applications 23(2):314–339.

Hidetoshi Shimodaira. 2016. Cross-validation of
matching correlation analysis by resampling match-
ing weights. Neural Networks 75:126–140.

Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In EMNLP-
CoNLL. pages 1423–1433.

Carina Silberer and Mirella Lapata. 2014. Learn-
ing grounded meaning representations with autoen-
coders. In ACL. pages 721–732.

Fei Yan and Krystian Mikolajczyk. 2015. Deep corre-
lation for matching images and text. In CVPR. pages
3441–3450.

Shuicheng Yan, Dong Xu, Benyu Zhang, Hong-Jiang
Zhang, Qiang Yang, and Stephen Lin. 2007. Graph
embedding and extensions: A general framework for
dimensionality reduction. TPAMI 29(1):40–51.

44


