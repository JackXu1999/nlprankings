
























A general framework for minimizing translation effort:
towards a principled combination of translation technologies in

computer-aided translation

Mikel L. Forcada Felipe Sánchez-Martı́nez
Dept. de Llenguatges i Sistemes Informàtics

Universitat d’Alacant
E-03071 Alacant (Spain)

{mlf,fsanchez}@dlsi.ua.es

Abstract

This paper motivates the need for an ho-
mogeneous way of measuring and estimat-
ing translation effort (quality) in computer-
aided translation. It then defines a general
framework for the measurement and esti-
mation of translation effort so that trans-
lation technologies can be both optimized
and combined in a principled manner. In
this way, professional translators will bene-
fit from the seamless integration of all the
technologies at their disposal when work-
ing on a translation job.

1 Introduction

Imagine that you are a professional translator and
you are given a translation job. The text to be trans-
lated comes divided in N segments, s1, s2, . . . sN :
your job is therefore the ordered set {si}Ni=1. You
are supposed to turn this into a translation, that is,
an ordered set {ti}Ni=1 with the translations of each
segment, and get paid for that. Yes, this is a sim-
plified view of your work: the translation of each
sentence is treated as an independent event, which
is not always the case. In any case, even in this sim-
plified form, the job is already quite challenging.

Help coming your way? Of course, you could
translate each segment si by hand, i.e. from scratch,
into an suitable segment ti in the target language.
You are a translator and you know this is usually
harder than most people think. However, there are
translation technologies out there that are supposed
to help you by reducing your translation effort; they

c� 2015 The authors. This article is licensed under a Creative
Commons 3.0 licence, no derivative works, attribution, CC-
BY-ND.

usually come packaged as computer-aided transla-
tion (CAT).

Machine translation: You could, for instance,
use machine translation (MT) to get a draft of the
translation of each segment, MT(si); vendors and
experts tell you that you will save effort by post-
editing MT(si) into your desired translation ti.1

Machine translation output MT(si) may just be
text, but it could come with annotations to help
you make the most of it; for instance, words could
be color-coded according to how confident the sys-
tem is about them (Ueffing and Ney, 2007; Ueffing
and Ney, 2005; Blatz et al., 2004), or unknown
words that come out untranslated may be marked
so that you spot them clearly. Machine-translated
segments could even be accompanied by indicators
of their estimated quality (Specia and Soricut, 2013;
Specia et al., 2010; Blatz et al., 2004) which may
be used to ascertain whether the output of the MT
system is worth being post-edited or not. If some-
one measured your post-editing effort (in time, in
number of keystrokes, in number of words changed,
in money you would have to pay another translator
to do it, etc.), when turning MT(si) into ti, they
could call that effort eMTi .

Translation memory: You could also use a
translation memory (TM; (Somers, 2003)), where
previously translated segments s are stored together
with their translations t in pairs called translation
units (s, t). The software searches the TM for the
source segment s�i that best matches each one of
your segments si, and delivers the corresponding
1We leave aside the debate about whether post-editors should
be considered different from translators, as in the end of the
day, you will be producing a translation which should be ade-
quate for the purpose at hand, and you will be as responsible
of it as if you had produced it from scratch; we will therefore
call everything translation for the purpose of this paper.

27



target segment t�i as a proposal, but not alone: it
also gives you information about how good the
match was between your new segment si and the
best fuzzy match s�i —usually as a percentage called
fuzzy match score that accounts for the amount of
text that is common to both segments2— and even
marks for you the words in s�i that do not match
those in si. Let’s call all this information TM(si):
your job is to use it to turn t�i into the final transla-
tion ti. If the fuzzy match is good, you will spend
less effort than if you started from scratch. Let
us call eTMi the effort to turn the t

�
i provided by

TM(si) into the desired translation ti.3

Mixing them up: You could even have available
another technology, fuzzy-match repair (FMR; (Or-
tega et al., 2014; Dandapat et al., 2011; Hewavitha-
rana et al., 2005; Kranias and Samiotou, 2004)),
that integrates the two technologies just mentioned:
after a suitable fuzzy match is found, machine trans-
lation (or another source of bilingual information)
is used to repair, i.e. edit some parts of t�i , to take
into account what changes from s�i to si to try to
save even more effort; it tells you all that TM(si)
tells you, but also marks the parts that have been
repaired. Fuzzy-match repair is one of the technolo-
gies that TAUS, the Translation Automation User
Society, calls advanced leveraging;4 commercial
examples of these are DeepMiner in Atril’s Déjà
Vu,5 and ALTM in MultiCorpora’s MultiTrans.6 It
takes an effort eFMRi to turn the output of fuzzy-
match repair, FMR(si), into the desired ti.

And many more: To summarize, each technol-
ogy X you can use —where X may be MT, TM,
FMR, etc.— takes each segment si and produces
an output X(si) that takes an effort eXi to turn into
ti. For a more general discussion, we will also
consider a technology the case in which no technol-
ogy is used, i.e. when the translation is performed
from scratch: technologies may not be helpful at all

2The fuzzy match score is usually based on a text similar-
ity measure like the word-level edit distance or Levenshtein
distance (Levenshtein, 1966). Commercial CAT systems use
trade-secret, proprietary versions of it aimed at estimating
better than the edit distance the remaining effort.
3Interestingly enough, in contrast with the case of machine
translation, even if you are actually post-editing a fuzzy-
matched proposal, there does not seem to be much debate
as to whether you are a translator or a fuzzy match post-editor.
4http://www.taus.net/reports/
advanced-leveraging
5http://tinyurl.com/x3dejavu
6http://multicorpora.com/resources/
advanced-leveraging/

sometimes. We will call X the set of all technolo-
gies X available in the CAT environment. Note that
there is another simplification here: the effort eXi is
assumed to depend only on si, but translators may
vary over time, either during a job, or between jobs;
they may become tired, or the effectiveness of each
technology may vary.

Isn’t this getting too complicated to be consid-
ered help? At this point, you are probably won-
dering how can you decide which technology to
use for each segment si if the information available
for each technology, such as quality indicators in
the case of machine translation and fuzzy match-
ing scores in the case of translation memories, are
not directly comparable. Or even better, couldn’t
the decision of selecting the best technology X�i ,
that is, the one that minimizes your effort for each
segment si, be made automatically?

It is therefore clear that a framework that allows
to seamlessly integrate all the translation technolo-
gies available in the CAT system is very much
needed to make the most of all of them and mini-
mize translation effort as much as possible.

Previous work on technology selection: The
specific case of automatically choosing between
machine translation output and translation mem-
ory fuzzy matches has received attention in the last
years. Simard and Isabelle (2009) proposed a sim-
ple approach called β-combination, which simply
selects machine translation when there is no trans-
lation memory proposal with a fuzzy match score
above a given threshold β, which can be tuned. He
et al. (2010a) and He et al. (2010b) approach this
problem, which they call translation recommenda-
tion, by training a classifier which selects which of
the two, TM(si) or MT(si), gets the lowest value
for an approximate indicator of effort, called trans-
lation error rate (TER, (Snover et al., 2006)). Their
training compares outputs to preexisting reference
translations; their ideas are generalized in the ap-
proach proposed in this paper.

The next section explains two ways to minimize
the effort needed to perform a translation job in a
CAT environment integrating different technologies.
Section 3 then describes our proposal for a general
framework for training the whole CAT environment.
Finally, we discuss the implications of having such
a framework.

28



2 Minimizing translation effort

Translation technology designers understand that
translators want to minimize their total effort. To
compute this total effort, the actual measurements
of effort eXi need to be extensive magnitudes,

7 that
is, magnitudes that grow with the length of the
segment and make sense when added for all the seg-
ments of the job. Examples of extensive measure-
ments have already been given: amount of words
or characters changed, total amount of keystrokes,
time spent in editing and total price.8 These are
magnitudes that can be compared regardless of tech-
nology and allow the total cost of a new translation
job to be simply calculated as

E =

N�

i=1

e
X�i
i

where eX
�
i

i is the effort expended in translating seg-
ment si using the best technology X�i for that seg-
ment, that is, the one that minimizes that effort.

To minimize the translation effort on a specific
task, designers have to work in two main areas:

Improving each technology: One is to improve
the output of each technology X , ideally fo-
cusing on those cases when X is going to be
selected. Some such technologies have tun-
able parameters; for instance, feature weights
in statistical MT (Koehn, 2010, p. 255); for
other technologies, this is not usually reported,
but it is not impossible to think, for instance, of
fuzzy-match scores that give different weights
to different kinds of edit operations. Let us
call �λX the vector of tunable parameters for
technology X; as the output of technology X
varies with these parameters, we can write its
output like this: X(si;�λX).

Learning to select the best technology: The
other one is that the CAT environment needs a
way to select the best technology X�i for each
segment si, obviously without measuring the
actual effort. To do this, CAT designers need

7This concept is borrowed from physics: see, e.g.,
http://en.wikipedia.org/wiki/Intensive_
and_extensive_properties.
8Examples of measurements that are not extensive, that is,
intensive, could be the percentage of words or characters
changed, the ratio of the total amount of keystrokes to the
sentence length, the time spent per word, or the price per word.
These are intensive properties as they are the ratio of two
extensive properties.

to come up with a set of estimators ẽX , one
for each technology. These estimators should
be trained to give the best possible estimate of
the actual measured effort eX(X(si;�λX)). If
we call �θX the set of tunable parameters of
the estimator for technology X , the output
of the estimator for X can be written as
ẽX(X(si;�λ

X); �θX). These estimators can
be used to estimate the total cost of a new
translation job as

Ẽ =
N�

i=1

ẽX
�
i (X�i (si;

�λX
�
i ); �θX

�
i ),

where

X�i = argmin
X∈X

ẽX(X(si;�λ
X); �θX).

In the case of MT, the estimators of effort
(Specia and Soricut, 2013; Specia et al., 2010;
Blatz et al., 2004) are based on a number of
features obtained from si and MT(si;�λMT).
The vector of parameters �θMT is tuned on
a development set made of bilingual seg-
ments and translation effort measurements
eMT(MT(si;�λ

MT)).9

Getting a good estimate of effort is hard: One
problem for technologists is that actual measure-
ments of effort are expensive to collect, and they
are not likely to be available for all technologies
and for all segments. Therefore it is in principle not
easy to determine the parameters �θX to get good
estimates ẽX(X(si;�λX); �θX). We will see a way
to do this below.

Tuning technologies is also hard: Technologies
may have tunable parameters �λX which determine
the output they produce. Obviously, one cannot
just repetitively measure the actual effort spent by
translators in editing their output for a wide va-
riety of values of �λX , as this is clearly impracti-
cable; therefore, an alternative is needed. When
X = MT, this is usually done by means of an al-
gorithm that optimizes (Och, 2003; Chiang, 2012)
automatic evaluation measures, such as BLEU (Pa-
pineni et al., 2002), using reference translations
in a development set. Most of these automatic
9The measurements of effort that one can find in literature vary
from simple scores for “perceived” post-editing effort (usually
scores taking 3 or 4 values) to actual post-editing time (see,
for instance, the quality estimation task in WMT 2014 (Bojar
et al., 2014))

29



measures are measures of similarity (or dissimi-
larity) between raw and reference translations. Re-
searchers hope that their use during tuning will
lead to a reduction in translation effort, although
this is not currently guaranteed —for instance,
Denkowski and Lavie (2012) found that BLEU
could not distinguish between raw and post-edited
machine translation. Generally, an automatic evalu-
ation measure for technology X may have the form
êX(X(si;�λ

X), {tij}nij=1; �µX), where {tij}nij=1 is
the set of reference translations for segment si in
the development set and �µX is a set of tunable
parameters. Ideally, êX(X(si;�λX), {tij}nij=1; �µX)
should approximate eX(X(si;�λX)), but tuning of
�µX is surprisingly absent from current MT practice
(with some exceptions, see Denkowski and Lavie
(2010)). In fact, êX(X(si;�λX), {tij}nij=1; �µX) can
be seen as a special estimator of effort, much like
ẽX(X(si;�λ

X); �θX), but informed with reference
translations {tij}nij=1 when they are available. This
is similar to the use of pseudo-reference translations
in machine translation quality estimation (Shah et
al., 2013; Soricut and Narsale, 2012; Soricut et al.,
2012; Soricut and Echihabi, 2010), but with actual
references.

Table 1 summarizes the main concepts and the
notation used along the paper.

3 A general framework for training the
whole CAT environment

We describe a possible workflow to tune simultane-
ously the different technologies that may be used
in a CAT environment and the estimators used to
select them on a segment basis:

1. Design automatic evaluation measures
êX(X(si,�λ

X), {tij}; �µX) and estimators
ẽX(X(si;�λ

X); �θX) for each technology
X ∈ X , based on a series of relevant features
that can easily be extracted from si and X(si),
and which will depend on parameters �µX and
�θX , respectively.

2. Give reasonable starting values to the param-
eters of ẽX(X(si;�λX); �θX) for each technol-
ogy X in X , so that they can be used to pre-
liminarily select technologies. These initial
estimators ẽX0 (X(si;�λ

X); �θX) could, for in-
stance, be the ones that worked well for a re-
lated task.

3. Put together a development set D = {sk}nDk=1
having nD segments, representative of the task
at hand, and which provides reference trans-
lations {tkl}nkl=1 for each segment. This de-
velopment set should be large enough to be
used to tune the technologies in step 7; it could
also be used to pre-tune the technologies (for
instance, statistical machine translation could
be pre-tuned using customary evaluation mea-
sures such as BLEU). Development sets of
thousands of segments are common, for in-
stance, in statistical machine translation, but
the actual number may depend on the number
of parameters in each �λX .

4. Have translators work on a representative sub-
set M = {sk}nMk=1 of the development set D,
and measure their effort when translating each
segment using the best technology, selected ac-
cording to the available version of estimators
ẽX(X(sk;�λ

X); �θX). Of course, M is a small
subset of D because translating thousands of
sentences, as in a typical development set, is
clearly out of the question, as this would be
more like the size of a whole translation job.
Note that translator work can add additional
references for those segments in D which are
also in M . Note also that we might be combin-
ing here measurements for a team of more than
one translator. Therefore, the resulting combi-
nation of technologies may not be expected to
be optimal for each individual translator, but
rather on average for the team.

A richer set of measurements could be ob-
tained by having translators translate segments
in M using also other technologies not se-
lected by the estimators. This would be costly
but could help mitigate the bias introduced by
the initial set of estimators.

To get evaluation measures êXi and estimators
ẽXi which are useful in the scenario of a new
translation job, translation memories are not
allowed to grow or change when translators
translate the set M , and the resulting reference
set is fixed after this step.

5. Use these measurements to fit all
the automatic evaluation measures
êX(X(si,�λ

X), {tij}; �µX) together by
varying their vectors �µX by means of eq. (1):

30



Notation Definition
{si}Ni=1 Translation task made up of N source segments si.

X Translation technology; e.g. X = MT, machine transla-
tion; X = TM, translation memory; X = FMR, fuzzy-
match repair; etc.

X The set of all translation technologies available.
X�i Technology selected for the translation of the source seg-

ment si.
X(si;�λ

X), also X(si) Output produced by translation technology X for input
segment si. Many provide additional information in addi-
tion to its output.

�λX Optional vector of tunable parameters used by translation
technology X to produce the best possible translation pro-
posal.

eX(X(si;�λ
X)), also eXi Actual measured effort to produce an adequate translation

starting from the proposal provided by technology X .
ẽX(X(si;�λ

X); �θX), also ẽXi Estimated effort to produce an adequate translation starting
from the proposal provided by technology X .

�θX Optional vector of tunable parameters used in the estimator
of effort ẽX(·). The parameters may be tuned so that the
estimated effort is as close as possible to the measured
effort.

êX(X(si;�λ
X), {tij}nij=1; �µX), also êXi Estimated effort to produce an adequate translation starting

from the proposal of technology X , specifically informed
by a set of reference translations (sometimes called auto-
matic evaluation measure).

{tij}nij=1, also {tij} Set of reference translations for segment si.
�µX Optional vector of tunable parameters used in estimator

êX(·). These parameters may be tuned so that the esti-
mated effort is as close as possible to the measured effort
(parameters are seldom tuned in automatic evaluation mea-
sures).

Table 1: A summary of the main concepts defined in the paper and the notation used.

31



min
{�µX}X∈X

�

k∈[1,nM ]
L
�
êX

�
k (X�k(sk,

�λX
�
k ), {tkl}nkl=1; �µX

�
k ), eX

�
k (X�k(sk;

�λX
�
k ))

�
(1)

Here, L(x) is ideally a differentiable loss
function (for instance L(x) = 12x2), and X�k
is the technology selected for sk by using the
initial estimator (if measurements had been
made in step 4 for more than one technology
per segment, they could be used here to get
a better approximation). The result is a set
of functions {êX} which estimate, using ref-
erence translations, the effort needed to deal
with the output of each technology X , without

having to actually measure that effort.

6. Training the technology selectors: Use
the available measurements of effort eXk
where they are available, and the automatic
evaluation measures êX(X(sk), {tkl}nkl=1; �µX)
where they are not, to obtain a set of better es-
timators ẽX(X(sk;�λX); �θX) by varying their
vectors �θX using the whole development set
D and eq. (2):

min
{�θX}X∈X

�

k∈[1,nD]
L
�
ẽX

�
k (X�k(sk,

�λX
�
k ); �θX

�
k ), ēX

�
k (X�k(sk,

�λX
�
k ), {tkl}nkl=1; �µX

�
k )
�

(2)

Here, ēX(X(sk,�λX), {tkl}nkl=1; �µX) is the
actual effort measured eX(X(sk;�λX)) if it is
available, and êX(X(sk,�λX

�
k ), {tkl}nkl=1; �µX)

otherwise, and X�k is the actual selec-
tion for those segments where measure-
ments were taken, or the technology with
the best êX(X(sk,�λX), {tkl}nkl=1; �µX) where
they were not. The result is a set of functions
{ẽX} which estimate, in the absence of ref-
erence translations, the effort needed to deal
with the output of each technology X , without

having to actually measure that effort. These
functions will be used in the translator’s CAT
tool to automatically select the best technology
for each segment.

7. Training the technologies themselves: The
same development set, and the functions {êX}
may be used to train —or, as usually said
in statistical machine translation, to tune —
the technologies themselves by searching for
those values of �λX leading to the minimum
effort for translators:

min
{�λX}X∈X

�

k∈[1,nD]
êX

�
k (X�k(sk,

�λX
�
k ), {tkl}nkl=1; �µX

�
k ), (3)

where X�k is the translation technology with
the best êX(X(sk,�λX), {tkl}nkl=1; �µX).
This training would be analogous to minimum-
error-rate training (MERT) (Och, 2003),
MIRA (Hasler et al., 2011), or similar iterative
methods used in statistical machine translation
to vary the parameters of a system and opti-
mize the output with respect to a certain qual-

ity indicator such as BLEU (Papineni et al.,
2002), but it would be applied to all sources
X ∈ X and use the estimators êXi of extensive
effort measurements tuned in step 5. Eq. (3)
tunes each technology using only those seg-
ments for which it was selected. Alternatively,
one may prefer to optimize all technologies on
all segments as follows:

min
{�λX}X∈X

�

X∈X

�

k∈[1,nD]
êX(X(sk,�λ

X), {tkl}nkl=1; �µX), (4)

32



and run the risk of spreading optimization
too thin to significantly optimize those tech-
nologies likely to be actually selected in a real
translation job.

The result is a set of technologies X that are
(approximately) optimized to reduce effort.
These technologies will be used in the CAT
tool to provide the best possible assistance to
translators.

Note that the new estimators obtained in step 6
could have led to different technology choices,
and therefore different measurement sets, if these
choices had been made in step 4. This coupling
between parameter sets should in principle be taken
into account in an improved setting by feeding this
all the way back to step 4 in some way to achieve
self-consistency while keeping the need for addi-
tional effort measurements, that is, additional man-
ual translations of segments in M , to a minimum;
feasible or approximate ways of doing this should
definitely be explored.

Note also that the workflow above is a batch
workflow. Online workflows which improve the
technologies X as translators work, would certainly
be more complex, but could be devised following
the rationale behind the batch workflow just de-
scribed, once it is proven useful.

4 Discussion

We have introduced a unified, general framework
for effort (quality) measurement, evaluation and es-
timation. This framework allows to simultaneously
tune all the components of a computer-aided trans-
lation environment. To that end, we propose the use
of estimators of remaining effort that are compara-
ble across translation-assistance technologies.

On the one hand, tuning all the translation tech-
nologies together (which is not common in current
practice), and in a way that takes into account when
they are actually selected to produce a proposal for
the translator, will lead to an improvement of the
translation proposals these technologies produce
when this improvement is relevant; that is, when
the technology is actually used to propose a transla-
tion to the translator.

On the other hand, having estimators of effort
whose output is comparable across technologies
will allow for seamless integration of translation
technologies: the CAT tool will be able to automat-
ically select the best technology on a segment basis.

In addition, if the estimations of effort are measured
in time spent in editing, or in money, they could be
used to accurately budget new translation jobs.

The framework proposed in this paper provides
a principled way to adapt the mix of technologies
to reduce total effort in a specific computer-aided
translation job.

We hope that this unified framework will ease
the integration of existing research being performed
to actually reduce translators’ effort and improve
their productivity. We also hope that it will in-
spire new approaches and encourage best practice
in computer-aided translation research and develop-
ment.

Acknowledgements: We acknowledge support
from the Spanish Ministry of Industry and Compet-
itiveness through project Ayutra (TIC2012-32615)
and from the European Commission through
project Abu-Matran (FP7-PEOPLE-2012-IAPP, ref.
324414) and thank all three anonymous referees for
very useful comments on the paper.

References

Blatz, J., E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2004. Confidence estimation for machine transla-
tion. In Proceedings of the 20th International Con-
ference on Computational Linguistics, pages 315–
321, Geneva, Switzerland.

Bojar, O., C. Buck, C. Federmann, B. Haddow,
P. Koehn, J. Leveling, C. Monz, P. Pecina, M. Post,
H. Saint-Amand, R. Soricut, L. Specia, and A. Tam-
chyna. 2014. Findings of the 2014 workshop on
statistical machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
pages 12–58, Baltimore, Maryland, USA.

Chiang, D. 2012. Hope and fear for discriminative
training of statistical translation models. Journal of
Machine Learning Research, 13:1159–1187.

Dandapat, S., S. Morrissey, A. Way, and M. L. For-
cada. 2011. Using example-based MT to support
statistical MT when translating homogeneous data in
a resource-poor setting. In Proceedings of the 15th
conference of the European Association for Machine
Translation, pages 201–208. Leuven, Belgium.

Denkowski, M. and A. Lavie. 2010. METEOR-NEXT
and the METEOR paraphrase tables: Improved eval-
uation support for five target languages. In Proceed-
ings of the Joint 5th Workshop on Statistical Machine
Translation and MetricsMATR (Uppsala, Sweden),
page 339–342.

33



Denkowski, M. and A. Lavie. 2012. Challenges in
predicting machine translation utility for human post-
editors. In Proceedings of The Tenth Biennial Con-
ference of the Association for Machine Translation
in the Americas, San Diego, CA, USA.

Hasler, Eva, Barry Haddow, and Philipp Koehn. 2011.
Margin infused relaxed algorithm for moses. The
Prague Bulletin of Mathematical Linguistics, 96:69–
78.

He, Y., Y. Ma, J. van Genabith, and A. Way. 2010a.
Bridging SMT and TM with translation recommen-
dation. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 622–630, Uppsala, Sweden.

He, Yifan, Yanjun Ma, Johann Roturier, Andy Way,
and Josef van Genabith. 2010b. Improving the
post-editing experience using translation recommen-
dation: A user study. In Proceedings of the Ninth
Conference of the Association for Machine Transla-
tion in the Americas.

Hewavitharana, S., S. Vogel, and A. Waibel. 2005.
Augmenting a statistical translation system with a
translation memory. In Proceedings of the 10th con-
ference of the European Association for Machine
Translation, pages 126–132, Budapest, Hungary.

Koehn, P. 2010. Statistical Machine Translation. Cam-
bridge University Press.

Kranias, L. and A. Samiotou. 2004. Automatic trans-
lation memory fuzzy match post-editing: a step be-
yond traditional TM/MT integration. In Proceedings
of the Fourth International Conference on Language
Resources and Evaluation, pages 331–334, Lisbon,
Portugal.

Levenshtein, V.I. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707–710.

Och, F.J. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics, pages 160–167, Sapporo, Japan.

Ortega, J. E., M. L. Forcada, and F. Sánchez-Martı́nez.
2014. Using any machine translation source for
fuzzy-match repair in a computer-aided translation
setting. In al Onaizan, Yaser and Michel Simard, ed-
itors, Proceedings of the 11th Conference of the As-
sociation for Machine Translation in the Americas,
volume 1: MT Researchers, pages 42–53, Vancou-
ver, BC, Canada.

Papineni, K., S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual

Meeting of the Association for Computational Lin-
guistics, pages 311–318, Philadelphia, PA, USA.

Shah, K., T. Cohn, and L. Specia. 2013. An inves-
tigation on the effectiveness of features for transla-
tion quality estimation. In Proceedings of the XIV
Machine Translation Summit, pages 167–174, Nice,
France.

Simard, M. and P. Isabelle. 2009. Phrase-based
machine translation in a computer-assisted transla-
tion environment. In Proceedings of the 12th Ma-
chine Translation Summit, pages 120–127, Ottawa,
Canada.

Snover, M., B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
Association for Machine Translation in the Americas
Conference, pages 223–231, August.

Somers, H., 2003. Computers and translation: a trans-
lator’s guide, chapter Translation memory systems,
pages 31–48. John Benjamins Publishing, Amster-
dam, Netherlands.

Soricut, R. and A. Echihabi. 2010. Trustrank: Inducing
trust in automatic translations via ranking. In In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 612–621,
Uppsala, Sweden.

Soricut, R. and S. Narsale. 2012. Combining quality
prediction and system selection for improved auto-
matic translation output. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation,
pages 163–170, Montreal, Canada.

Soricut, R., N. Bach, and Z. Wang. 2012. The SDL
Language Weaver systems in the WMT12 quality es-
timation shared task. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
145–151, Montreal, Canada.

Specia, L. and R. Soricut. 2013. Quality estimation for
machine translation: preface. Machine Translation,
27(3-4):167–170.

Specia, L., D. Raj, and M. Turchi. 2010. Machine
translation evaluation versus quality estimation. Ma-
chine Translation, 24(1):39–50.

Ueffing, Nicola and Hermann Ney. 2005. Word-level
confidence estimation for machine translation using
phrase-based translation models. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
HLT ’05, pages 763–770.

Ueffing, Nicola and Hermann Ney. 2007. Word-level
confidence estimation for machine translation. Com-
putational Linguistics, 33(1):9–40, March.

34


