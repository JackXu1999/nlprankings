








































Identifying semantic relations in a specialized corpus through
distributional analysis of a cooccurrence tensor

Gabriel Bernier-Colborne
OLST (Université de Montréal)

C.P. 6128, succ. Centre-Ville
Montréal (Québec) Canada H3C 3J7

gabriel.bernier-colborne@umontreal.ca

Abstract

We describe a method of encoding cooc-
currence information in a three-way tensor
from which HAL-style word space mod-
els can be derived. We use these models to
identify semantic relations in a specialized
corpus. Results suggest that the tensor-
based methods we propose are more ro-
bust than the basic HAL model in some
respects.

1 Introduction

Word space models such as LSA (Landauer and
Dumais, 1997) and HAL (Lund et al., 1995) have
been shown to identify semantic relations from
corpus data quite effectively. However, the per-
formance of such models depends on the parame-
ters used to construct the word space. In the case
of HAL, parameters such as the size of the con-
text window can have a significant impact on the
ability of the model to identify semantic relations
and on the types of relations (e.g. paradigmatic or
syntagmatic) captured.

In this paper, we describe a method of encoding
cooccurrence information which employs a three-
way tensor instead of a matrix. Because the ten-
sor explicitly encodes the distance between a tar-
get word and the context words that co-occur with
it, it allows us to extract matrices corresponding to
HAL models with different context windows with-
out repeatedly processing the whole corpus, but it
also allows us to experiment with different kinds
of word spaces. We describe one method whereby
features are selected in different slices of the ten-
sor corresponding to different distances between
the target and context words, and another which
uses SVD for dimensionality reduction. Models

This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/

are evaluated and compared on reference data ex-
tracted from a specialized dictionary of the envi-
ronment domain, as our target application is the
identification of lexico-semantic relations in spe-
cialized corpora. Preliminary results suggest the
tensor-based methods are more robust than the ba-
sic HAL model in some respects.

2 Related Work

The tensor encoding method we describe is based
on the Hyperspace Analogue to Language, or
HAL, model (Lund et al., 1995; Lund and
Burgess, 1996), which has been shown to be par-
ticularly effective at modeling paradigmatic rela-
tions such as synonymy. In the HAL model, word
order is taken into account insofar as the word vec-
tors it produces contain information about both the
cooccurrents that precede a word and those that
follow it. In recent years, there have been several
proposals that aim to add word order information
to models that rely mainly on word context infor-
mation (Jones and Mewhort, 2007; Sahlgren et al.,
2008), including models based on multi-way ten-
sors. Symonds et al. (2011) proposed an efficient
tensor encoding method which builds on unstruc-
tured word space models (i.e. models based on
simple cooccurrence rather than syntactic struc-
ture) by adding order information. The method we
describe differs in that it explicitly encodes the dis-
tance between a target word and its cooccurrents.

Multi-way tensors have been used to construct
different kinds of word space models in recent
years. Turney (2007) used a word-word-pattern
tensor to model semantic similarity, Van de Cruys
(2009) used a tensor containing corpus-derived
subject-verb-object triples to model selectional
preferences, and Baroni and Lenci (2010) pro-
posed a general, tensor-based framework for struc-
tured word space models. The tensor encoding
method we describe differs in that it is based on
an unstructured word space model, HAL.



3 HAL

The HAL model employs a sliding context win-
dow to compute a word-word cooccurrence ma-
trix, which we will note A, in which value aij is
based on the number of times context word wj ap-
pears within the context window of target word wi.
Thus, words that share cooccurrents will be closer
in word space. If equal weight is given to all con-
text words in the window, regardless of distance,
we call the context window rectangular. In the
original HAL model, the values added to A are
inversely proportional to the distance between the
target word and context word in a given context.
In this case, the context window is triangular.

In the HAL model, the cooccurrence matrix is
computed by considering only the context words
that occur before the target word. Once the ma-
trix has been computed, row vector ai: contains
cooccurrence information about words preceding
wi, and column vector a:i contains information
about those that follow it. The row vector and
column vector of each target word are concate-
nated, such that the resulting word vectors con-
tain information about both left-cooccurrents and
right-cooccurrents. We call this type of context
window directional, following (Sahlgren, 2006),
as opposed to a symetric context window, in which
cooccurrence counts in the left and right contexts
are summed. In our experiment, we only use one
type of context window (directional and rectangu-
lar), but models corresponding to different types
of context windows can be derived from the cooc-
currence tensor we describe in section 4.

Once the values in A have been computed, they
can be weighted using schemes such as TF-ITF
(Lavelli et al., 2004) and Positive Pointwise Mu-
tual Information (PPMI), which we use here as
it has been shown to be particularly effective by
Bullinaria and Levy (2007). Finally, a distance or
similarity measure is used to compare word vec-
tors. Lund and Burgess (1996) use Minkowski
distances. We will use the cosine similarity, as
did Schütze (1992) in a model similar to HAL and
which directly influenced its development.

4 The Cooccurrence Tensor

In the following description of the cooccurrence
tensor, we follow the notational guidelines of
(Kolda, 2006), as in (Turney, 2007; Baroni and

Lenci, 2010). Let W be the vocabulary1, which
we index by i to refer to a target word and by j for
context words. Furthermore, let P , indexed by k,
be a set of positions, relative to a target word wi,
in which a context word wj can co-occur with wi.
In other words, this is the signed distance between
wj and wi, in number of words. For instance, in
the sentence “a dog bit the mailman”, we would
say that “dog” co-occurs with “bit” in position−1.
If we only consider the words directly adjacent to
a target word, then P = {−1,+1}. If the tensor
encoding method is used to generate HAL-style
cooccurrence matrices corresponding to different
context windows, then P would include all posi-
tions in the largest window under consideration.

In a cooccurrence matrix A, aij contains the
frequency at which word wj co-occurs with word
wi in a fixed context window. Rather than comput-
ing matrices using fixed-size context windows, we
can construct a cooccurrence tensor X, a labeled
three-way tensor in which values xijk indicate the
frequency at which word wj co-occurs with word
wi in position pk. Table 1 illustrates a cooccur-
rence tensor for the sentence “dogs bite mailmen”
using a context window of 1 (P = {−1,+1}), in
the form of a nested table.

In tensor X, xi:k denotes the row vector of wi
at position pk, x:jk denotes the column vector of
word wj at position pk and xij: denotes the tube
vector indicating the frequency at which wj co-
occurs with wi in each of the positions in P .

HAL-style cooccurrence matrices correspond-
ing to different context windows can be extracted
from the tensor by summing and concatenating
various slices of the tensor. A frontal slice X::k
represents a I × J cooccurrence matrix for po-
sition pk. A cooccurrence matrix corresponding
to a symetric context window of size n can be
extracted by summing the slices X::k for pk ∈
{−n,−n + 1, . . . , n}. For a directional window,
we first sum the slices for pk ∈ {−n, . . . ,−1},
then sum the slices for pk ∈ {1, . . . , n}, then con-
catenate the 2 resulting matrices horizontally.

Thus, summing and concatenating slices allows
us to extract HAL-style cooccurrence matrices. A
different kind of model can also be obtained by
concatenating slices of the tensor. For instance, if
we concatenate X::k for pk ∈ {−2,−1,+1,+2}
horizontally, we obtain a matrix containing a vec-

1We assume that the target and context words are the same
set, but this need not be the case.



j=1:dog j=2:bite j=3:mailman
k=1:−1 k=2:+1 k=1:−1 k=2:+1 k=1:−1 k=2:+1

i=1:dog 0 0 0 1 0 0
i=2:bite 1 0 0 0 0 1
i=3:mailman 0 0 1 0 0 0

Table 1: A 3× 3× 2 cooccurrence tensor.

tor of length 4J (instead of the 2J-length vectors
of the HAL model) for each target word, which
encodes cooccurrence information about 4 specific
positions relative to that word. We will refer to this
method as the tensor slicing method. Note that if
P = {−1, 1} the resulting matrix is identical to a
HAL model with context size 1

As the size of the resulting vectors is KJ , this
method can result in very high-dimensional word
vectors. In the original HAL model, Lund et
al. (1995) reduced the dimensionality of the vec-
tors through feature selection, by keeping only the
features that have the highest variance. Schütze
(1992), on the other hand, used truncated SVD for
this purpose. Both techniques can be used with the
tensor slicing method. In our experiment, SVD
was applied to the matrices obtained by concate-
nating tensor slices horizontally2. As for feature
selection, a fixed number of features (those with
the highest variance) were selected from each slice
of the tensor, and these reduced slices were then
concatenated.

It must be acknowledged that this tensor encod-
ing method is not efficient in terms of memory.
However, this was not a major issue in our exper-
imental setting, as the size of the vocabulary was
small (5K words), and we limited the number of
positions in P to 10. Also, a sparse tensor was
used to reduce memory consumption.

5 Experiment

5.1 Corpus and Preprocessing

In this experiment, we used the PANACEA En-
vironment English monolingual corpus, which is

2We also tried concatenating slices vertically (thus ob-
taining a matrix where rows correspond to <target word,
position> tuples and columns correspond to context words)
before applying SVD, then concatenating all row vectors cor-
responding to the same target word, but we will not report
the results here for lack of space. Concatenating slices hor-
izontally performed better and seems more intuitive, and the
size of the resulting vectors is not dependent on the number
of positions in P .

freely distributed by ELDA for research purposes3

(Catalog Reference ELRA-W0063). This corpus
contains 28071 documents (∼50 million tokens)
dealing with different aspects of the environment
domain, harvested from web sites using a focused
crawler. The corpus was converted from XML to
raw text, various string normalization operations
were then applied, and the corpus was lemmatized
using TreeTagger (Schmid, 1994). The vocabu-
lary (W ) was selected based on word frequency:
we used the 5000 most frequent words in the cor-
pus, excluding stop words and strings containing
non-alphabetic characters. During computation of
the cooccurrence tensor, OOV words were ignored
(rather than deleted), and the context window was
allowed to span sentence boundaries.

5.2 Evaluation Data

Models were evaluated using reference data ex-
tracted from DiCoEnviro4, a specialized dictio-
nary of the environment. This dictionary de-
scribes the meaning and behaviour of terms of
the environment domain as well as the lexico-
semantic relations between these terms. Of the
various relations encoded in the dictionary, we
focused on a subset of three paradigmatic rela-
tions: near-synonyms (terms that have similar
meanings), antonyms (opposite meanings), and
hyponyms (kinds of). 446 pairs containing a head-
word and a related term were extracted from the
dictionary. We then filtered out the pairs that con-
tained at least one OOV term, and were left with
374 pairs containing two paradigmatically-related,
single-word terms. About two thirds (246) of these
examples were used for parameter selection, and
the rest were set aside for a final comparison of
the highest-scoring models.

3http://catalog.elra.info/product_
info.php?products_id=1184

4http://olst.ling.umontreal.ca/
cgi-bin/dicoenviro/search_enviro.cgi
(under construction).



5.3 Automatic Evaluation
Each model was automatically evaluated on the
reference data as follows. For each <headword,
related term> pair in the training set, we computed
the cosine similarity between the headword and
all other words in the vocabulary, then observed
the rank of the related term in the sorted list of
neighbours. The score used to compare models
is recall at k (R@k), which is the percentage of
cases where the related term is among the k near-
est neighbours of the headword. It should be noted
that a score of 100% is not always possible in this
setting (depending on the value of k), as some
headwords have more than 1 related term in the
reference data. Nonetheless, since most (∼70%)
have 1 or 2 related terms, R@k for some small
value of k (we use k = 10) should be a good indica-
tor of accuracy. A measure that explicitly accounts
for the fact that different terms have different num-
bers of related terms (e.g. R-precision) would be a
good alternative.

5.4 Models Tested
We compared HAL and the tensor slicing
method using either feature selection or SVD5,
as explained in section 4. We will refer to
each of these models as HALSEL, TNSRSEL,
HALSVD and TNSRSVD. Context sizes ranged
from 1 to 5 words. For feature selection,
the number of features could take values in
{1000, 2000, . . . , 10000}, 10000 being the max-
imum number of features in a HAL model us-
ing a vocabulary of 5000 words. In the case
of TNSRSEL, to determine the number of fea-
tures selected per slice, we took each value in
{1000, 2000, . . . , 10000}, divided it by K (the
number of positions in P ), and rounded down.
This way, once the slices are concatenated, the
total number of features is equal to (or slightly
less than) that of one of the HALSEL mod-
els, allowing for a straightforward comparison.
When SVD was used instead of feature selection,
the number of components could take values in
{100, 200, ..., 1000}. In all cases, word vectors
were weighted using PPMI and normalized6.

5We used the SVD implementation (ARPACK solver)
provided in the scikit-learn toolkit (Pedregosa et al., 2011).

6For HALSEL and TNSRSEL, we apply PPMI weighting
after feature selection. In the case of TNSRSEL, we wanted
to avoid weighting each slice of the tensor separately. We
decided to apply weighting after feature selection in the case
of HALSEL as well in order to enable a more straightforward
comparison. We should also note that, in our experiments

absorb extreme precipitation
emit severe rainfall
sequester intense snowfall
convert harsh temperature
produce catastrophic rain
accumulate unusual evaporation
store seasonal runoff
radiate mild moisture
consume cold snow
remove dramatic weather
reflect increase deposition

Table 2: 10 nearest neighbours of 3 environmental
terms using the HALSEL model.

6 Results

Table 2 illustrates the kinds of relations identified
by the basic HALSEL model. It shows the 10 near-
est neighbours of the verb absorb, the adjective
extreme and the noun precipitation. If we com-
pare these results with the paradigmatic relations
encoded in DiCoEnviro, we see that, in the case
of absorb, 3 of its neighbours are encoded in the
dictionary, and all 3 are antonyms or terms having
opposite meanings: emit, radiate, and reflect. As
for extreme, the top 2 neighbours are both encoded
in the dictionary as near-synonyms. Finally, rain
and snow are both encoded as kinds of precipita-
tion. Most of the other neighbours shown here are
also paradigmatically related to the query terms.
Thus, HAL seems quite capable of identifying the
three types of paradigmatic relations we hoped to
identify.

Table 3 shows the best R@10 achieved by each
model on the training set, which was used to tune
the context size and number of features or compo-
nents, and their scores on the test set, which was
only used to compare the best models. In the case
of HALSEL, the best model has a context window
size of 1 and uses 9K out of 10K available features.
As for TNSRSEL, the best model had a context size
of 2 (P = {−2,−1,+1,+2}) and 10000 features
(2500 per slice). It performed only slightly better
on the training set, however it beat the HAL model
with a wider margin on the test set.

using HAL, PPMI weighting performed better when applied
after feature selection, especially for low numbers of features.



200 400 600 800 1000

Nb components

30

35

40

45

50

55

60

65

R
@

10
(%

)

(a)

200 400 600 800 1000

Nb components

30

35

40

45

50

55

60

65
(b)

context size

1
2
3
4
5

Figure 1: HAL vs. tensor slicing method using SVD for dimensionality reduction. R@10 is plotted
against number of components. Models are identical when context size is 1. (a) HALSVD (b) TNSRSVD

Model Train Test
HALSEL 60.57 57.03
TNSRSEL 60.98 60.94
HALSVD 59.76 56.25
TNSRSVD 60.57 60.16

Table 3: R@10 (%) of best models.

The best HALSVD model used a 1-word window
and 1000 components, whereas the best TNSRSVD
model had a context size of 2 and 800 components.
Again, the tensor-based model slightly edged out
the HAL model on the training set, but performed
considerably better on the test set.

Further analysis of the results indeed suggests
that the tensor slicing method is more robust in
some respects than the basic HAL model. Fig-
ure 1 compares the performance of HALSVD and
TNSRSVD on the training set, taking into account
context size and number of components. It shows
that the HAL model is quite sensitive to context
size, narrower context performing better in this
task. The tensor-based method reduces this gap in
performance between context sizes, the gain being
greater for larger context sizes. Furthermore, us-
ing the tensor-based method with a slightly wider
context (2) raises R@10 for most values of the
number of components. Results obtained with
HALSEL and TNSRSEL follow the same trend, the
tensor-based method being more robust with re-
spect to context size. For lack of space, we only
show the plot comparing HALSVD and TNSRSVD.

7 Concluding Remarks

The work presented in this paper is still in its ex-
ploratory phase. The tensor slicing method we
described has only been evaluated on one corpus
and one set of reference data. Experiments would
need to be carried out on common word space
evaluation tasks in order to compare its perfor-
mance to that of HAL and other word space mod-
els. However, our results suggest that the tensor-
based methods are more robust than the basic HAL
model to a certain extent, and can improve accu-
racy. This could prove especially useful in settings
where no reference data are available for parame-
ter tuning.

Various possibilities offered by the cooccur-
rence tensor remain to be explored, such as
weighting the number of features selected per
slice using some function of the distance between
words, extracting matrices from the tensor by ap-
plying various functions to the tube vectors corre-
sponding to each word pair, and applying weight-
ing functions that have been generalized to higher-
order tensors (Van de Cruys, 2011) or tensor de-
composition methods such as those described in
(Turney, 2007).

Acknowledgements

We would like to thank the anonymous reviewers
for their helpful and thorough comments. Funding
was provided by the Social Sciences and Humani-
ties Research Council of Canada.



References
Marco Baroni and Alessandro Lenci. 2010. Dis-

tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673–721.

John A Bullinaria and Joseph P Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39(3):510–526.

Michael N Jones and Douglas JK Mewhort. 2007.
Representing word meaning and order information
in a composite holographic lexicon. Psychological
Review, 114(1):1–37.

Tamara Gibson Kolda. 2006. Multilinear operators
for higher-order decompositions. Technical Report
SAND2006-2081, Sandia National Laboratories.

Thomas K Landauer and Susan T Dumais. 1997. A
solution to Plato’s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211.

Alberto Lavelli, Fabrizio Sebastiani, and Roberto
Zanoli. 2004. Distributional term representations:
An experimental comparison. In Proceedings of
the 13th ACM International Conference on Informa-
tion and Knowledge Management, pages 615–624.
ACM.

Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, & Computers, 28(2):203–208.

Kevin Lund, Curt Burgess, and Ruth Ann Atchley.
1995. Semantic and associative priming in high-
dimensional semantic space. In Proceedings of the
17th Annual Conference of the Cognitive Science
Society, volume 17, pages 660–665.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.

Magnus Sahlgren, Anders Holst, and Pentti Kanerva.
2008. Permutations as a means to encode order
in word space. In Proceedings of the 30th Annual
Conference of the Cognitive Science Society, pages
1300–1305. Cognitive Science Society.

Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.

Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.

Hinrich Schütze. 1992. Dimensions of meaning. In
Proceedings of the 1992 ACM/IEEE Conference on
Supercomputing (Supercomputing’92), pages 787–
796. IEEE Computer Society Press.

Michael Symonds, Peter D Bruza, Laurianne Sitbon,
and Ian Turner. 2011. Modelling word meaning us-
ing efficient tensor representations. In Proceedings
of 25th Pacific Asia Conference on Language, Infor-
mation and Computation.

Peter Turney. 2007. Empirical evaluation of four
tensor decomposition algorithms. Technical Report
ERB-1152, National Research Council of Canada,
Ottawa.

Tim Van de Cruys. 2009. A non-negative tensor fac-
torization model for selectional preference induc-
tion. In Proceedings of the Workshop on Geomet-
rical Models of Natural Language Semantics, pages
83–90. ACL.

Tim Van de Cruys. 2011. Two multivariate general-
izations of pointwise mutual information. In Pro-
ceedings of the Workshop on Distributional Seman-
tics and Compositionality, pages 16–20. ACL.


