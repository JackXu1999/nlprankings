



















































Improving Distantly-supervised Entity Typing with Compact Latent Space Clustering


Proceedings of NAACL-HLT 2019, pages 2862–2872
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

2862

Improving Distantly-supervised Entity Typing
with Compact Latent Space Clustering

Bo Chen1, Xiaotao Gu2, Yufeng Hu1, Siliang Tang1∗, Guoping Hu3,
Yueting Zhuang1 & Xiang Ren4

1Zhejiang University, 2University of Illinois at Urbana Champaign
3iFLYTEK Research, 4University of Southern California,

{chenbo123, xiaofeem, siliang, yzhuang}@zju.edu.cn,
xiaotao2@illinois.edu, gphu@iflytek.com, xiangren@usc.edu

Abstract

Recently, distant supervision has gained great
success on Fine-grained Entity Typing (FET).
Despite its efficiency in reducing manual la-
beling efforts, it also brings the challenge of
dealing with false entity type labels, as dis-
tant supervision assigns labels in a context-
agnostic manner. Existing works alleviated
this issue with partial-label loss, but usually
suffer from confirmation bias, which means
the classifier fit a pseudo data distribution
given by itself. In this work, we propose
to regularize distantly supervised models with
Compact Latent Space Clustering (CLSC) to
bypass this problem and effectively utilize
noisy data yet. Our proposed method first
dynamically constructs a similarity graph of
different entity mentions; infer the labels of
noisy instances via label propagation. Based
on the inferred labels, mention embeddings
are updated accordingly to encourage entity
mentions with close semantics to form a com-
pact cluster in the embedding space, thus lead-
ing to better classification performance. Ex-
tensive experiments on standard benchmarks
show that our CLSC model consistently out-
performs state-of-the-art distantly supervised
entity typing systems by a significant margin.

1 Introduction

Recent years have seen a surge of interests in
fine-grained entity typing (FET) as it serves as
an important cornerstone of several nature lan-
guage processing tasks including relation extrac-
tion (Mintz et al., 2009), entity linking (Raiman
and Raiman, 2018), and knowledge base comple-
tion (Dong et al., 2014). To reduce manual ef-
forts in labelling training data, distant supervision
(Mintz et al., 2009) has been widely adopted by
recent FET systems. With the help of an external
knowledge base (KB), an entity mention is first

∗Corresponding Author.

Figure 1: T-SNE visualization of the mention embed-
dings generated by NFETC (left) and CLSC (right) on
the BBN dataset. Our model (CLSC) clearly groups
mentions of the same type into a compact cluster.

linked to an existing entity in KB, and then la-
beled with all possible types of the KB entity as
supervision. However, despite its efficiency, dis-
tant supervision also brings the challenge of out-
of-context noise, as it assigns labels in a context
agnostic manner. Early works usually ignore such
noise in supervision (Ling and Weld, 2012; Shi-
maoka et al., 2016), which dampens the perfor-
mance of distantly supervised models.

Towards overcoming out-of-context noise, two
lines of work have been proposed to distantly su-
pervised FET. The first kind of work try to fil-
ter out noisy labels using heuristic rules (Gillick
et al., 2014). However, such heuristic pruning sig-
nificantly reduces the amount of training data, and
thus cannot make full use of distantly annotated
data. In contrast, the other thread of works try to
incorporate such imperfect annotation by partial-
label loss (PLL). The basic assumption is that, for
a noisy mention, the maximum score associated
with its candidate types should be greater than the
scores associated with any other non-candidate
types (Ren et al., 2016a; Abhishek et al., 2017; Xu
and Barbosa, 2018). Despite their success, PLL-
based models still suffer from Confirmation Bias
by taking its own prediction as optimization ob-
jective in the next step. Specifically, given an en-
tity mention, if the typing system selected a wrong



2863

 CLSC

Classifier
KL

Regularization

Label

  Clean Data FeatureExtractor
Feature

Extractor

  Noise Data FeatureExtractor
Feature

Extractor

Distant Supervision
Training Data

person
artist

root

location
athlete

...

legal

director

...
music

author

actor

Knowledge Base

S1:  by Defense Secretary William Cohen and Joint Chiefs 
chairman General Hugh Shelton 

Labeled Corpus

Unlabeled Corpus

sup      L sup      L sup      L 

S2:  Defense Secretary William Cohen says if a formal 
investigation shows it was a

Maine Republican William Cohen said the plan might 
violate the assassination ban     

Candidate Type
/person/artist/actor

/person/political_figure
/person/legal

clsc      L clsc      L 

Figure 2: The overall framework of CLSC. We calculate classification loss only on clean data, while regularize the
feature extractor with CLSC using both clean and noisy data.

type with the maximum score among all candi-
dates, it will try to further maximize the score of
the wrong type in following optimization epoches
(in order to minimize PLL), thus amplifying the
confirmation bias. Such bias starts from the early
stage of training, when the typing model is still
very suboptimal, and can accumulate in training
process. Related discussion can be also found
in the setting of semi-supervised learning (Lee
et al., 2006; Laine and Aila, 2017; Tarvainen and
Valpola, 2017).

In this paper, we propose a new method for dis-
tantly supervised fine-grained entity typing. En-
lightened by (Kamnitsas et al., 2018), we pro-
pose to effectively utilize imperfect annotation as
model regularization via Compact Latent Space
Clustering (CLSC). More specifically, our model
encourages the feature extractor to group mentions
of the same type as a compact cluster (dense re-
gion) in the representation space, which leads to
better classification performance. For training data
with noisy labels, instead of generating pseudo su-
pervision by the typing model itself, we dynam-
ically construct a similarity-weighted graph be-
tween clean and noisy mentions, and apply label
propagation on the graph to help the formation of
compact clusters. Figure 1 demonstrates the ef-
fectiveness of our method in clustering mentions
of different types into dense regions. In contrast
to PLL-based models, we do not force the model
to fit pseudo supervision generated by itself, but
only use noisy data as part of regularization for
our feature extractor layer, thus avoiding bias ac-
cumulation.

Extensive experiments on standard benchmarks
show that our method consistently outperforms
state-of-the-art models. Further study reveals that,
the advantage of our model over the competitors
gets even more significant as the portion of noisy
data rises.

2 Problem Definition

Fine-grained entity typing takes a corpus and an
external knowledge base (KB) with a type hierar-
chy Y as input. Given an entity mention (i.e., a
sequence of token spans representing an entity) in
the corpus, our task is to uncover its corresponding
type-path in Y based on the context.

By applying distant supervision, each mention
is first linked to an existing entity in KB, and
then labeled with all its possible types. For-
mally, a labeled corpus can be represented as
triples D = {(mi, ci,Yi)}ni=1, where mi is the
i-th mention, ci is the context of mi, Yi is the
set of candidate types of mi. Note that types
in Yi can form one or more type paths. In
addition, we denote all terminal (leaf) types of
each type path in Yi as the target type set Yti
(e.g., for Yi = {artist, teacher, person}, Yti =
{artist, teacher}). This setting is also adopted
by (Xu and Barbosa, 2018).

As each entity in KB can have several type
paths, out-of-context noise may exist whenYi con-
tains type paths that are irrelevant to mi in con-
text ci. In this work, we argue triples where Yi
contains only one type path (i.e., |Yti | = 1) as
clean data. Other triples are treated as noisy data,
where Yi contains both the true type path and irrel-



2864

formal investigationDefense Secretary [William Cohen] says if

Bi-LSTM

Word-level
Attention

Secretary [William Cohen] says

LSTM 
Encoder

Average 
Encoder

Feature Representation

[William Cohen]

Embedding

Context 
Encoder( )

Figure 3: The architecture of feature extractor z((mi, ci); θz)

evant type paths. Noisy data usually takes a con-
siderable portion of the entire dataset. The major
challenge for distantly supervised typing systems
is to incorporate both clean and noisy data to train
high-quality type classifiers.

3 The Proposed Approach

Overview. The basic assumptions of our idea are:
(1) all mentions belong to the same type should be
close to each other in the representation space be-
cause they should have similar context, (2) similar
contexts lead to the same type. For clean data, we
compact the representation space of the same type
to comply (1). For noisy data, given assumption
(2), we infer the their type distributions via label
propagation and candidate types constrain.
Figure 2 shows the overall framework of the pro-
posed method. Clean data is used to train classi-
fier and feature extractor end-to-endly, while noisy
data is only used in CLSC regularization. For-
mally, given a batch of samples {(mi, ci,Yti )}Bi=1,
we first convert each sample (mi, ci) into a
real-valued vector zi via a feature extractor
z((mi, ci); θz) parameterized by θz . Then a type
classifier g(zi; θg) parameterized by θg gives the
posteriorP (y|zi; θg). By incorporating CLSC reg-
ularization in the objective function, we encourage
the feature extractor z to group mentions of the
same type into a compact cluster, which facilitates
classification as is shown in Figure 1. Noisy data
enhances the formation of compact clusters with
the help of label propagation.

3.1 Feature Extractor

Figure 3 illustrates our feature extractor. For fair
comparison, we adopt the same feature extraction
pipeline as used in (Xu and Barbosa, 2018). The
feature extractor is composed of an embedding
layer and two encoders which encode mentions
and contexts respectively.
Embedding Layer: The output of this layer is

a concatenation of word embedding and word
position embedding. We use the popular 300-
dimensional word embedding supplied by (Pen-
nington et al., 2014) to capture the semantic infor-
mation and random initialized position embedding
(Zeng et al., 2014) to acquire information about
the relation between words and the mentions.

Formally, Given a word embedding matrix
Wword of shape dw × |V |, where V is the vo-
cabulary and dw is the size of word embedding,
each column of Wword represents a specific word
w in V . We map each word wj in (mi, ci) to a
word embedding wdj ∈ Rdw . Analogously, we get
the word position embedding wpj ∈ Rdp of each
word according to the relative distance between
the word and the mention, we only use a fixed
length context here. The final embedding of the
j-th word is wEj = [w

d
j ,w

p
j ].

Mention Encoder: To capture lexical level in-
formation of mentions, an averaging mention en-
coder and a LSTM mention encoder (Hochreiter
and Schmidhuber, 1997) is applied to encode men-
tions. Given mi = (ws, ws+1, · · · , we), the aver-



2865

aging mention representation rai ∈ Rdw is :

rai =
1

e− s+ 1

e∑
j=s

wdj (1)

By applying a LSTM over an extended men-
tion (ws−1, ws, ws+1, · · · , we, we+1), we get a se-
quence (hs−1, hs, hs+1, · · · , he, he+1). We use
he+1 as LSTM mention representation rli ∈
Rdl . The final mention representation is rmi =
[rai , rli ] ∈ Rdw+dl .
Context Encoder: A bidirectional LSTM with dl
hidden units is employed to encode embedding se-
quence (wEs−W,w

E
s−W+1, · · · ,wEe+W):

−→
hj =LSTM(

−−→
hj−1,w

E
j−1)

←−
hj =LSTM(

←−−
hj−1,w

E
j−1)

hj =[
−→
hj ⊕

←−
hj ]

(2)

where ⊕ denotes element-wise plus. Then, the
word-level attention mechanism computes a score
βi,j over different word j in the context ci to get
the final context representation rci :

αj =w
T tanh(hj)

βi,j =
exp(αj)∑
k

exp(αk)

rci =
∑
j

βi,jhi,j

(3)

We use ri = [rmi , rci ] ∈ Rdz = Rdw+dl+dl as the
feature representation of (mi, ci) and use a Neural
Networks q over ri to get the feature vector zi. q
has n layers with hn hidden units and use ReLu
activation.

3.2 Compact Latent Space Clustering for
Distant Supervision

The overview of CLSC regularization is exhib-
ited in Figure 4, which includes three steps: dy-
namic graph construction (Figure 4c), label prop-
agation (Figure 4d, e) and Markov chains (Fig-
ure 4g). The idea of compact clustering for semi-
supervised learning is first proposed by (Kamnit-
sas et al., 2018). The basic idea is to encourage
mentions of the same type to be clustered into a
dense region in the embedding space. We intro-
duce more details of CLSC for distantly super-
vised FET in following sections.
Dynamic Graph Construction: We start by cre-
ating a fully connected graph G over the batch

of samples Z = {zi}Bi=1, as shown in Figure
4c1. Each node of G is a feature representation
zi, while the distance between nodes is defined by
a scaled dot-product distance function (Vaswani
et al., 2017):

Aij =exp(
zTi zj√
dz

),∀zi, zj ∈ Z

A =exp(
ZTZ√
dz

)

(4)

Each entry Aij measures the similarity between zi
and zj , A ∈ RB×B can be viewed as the weighted
adjacency matrix of G.

Label Propagation: The end goal of CLSC is to
cluster mentions of the same type to a dense re-
gion. For mentions which have more than one la-
beled types, we apply label propagation (LP) on
G to estimate their type distribution. Formally, we
denote Φ ∈ RB×K as the label propagation pos-
terior of a training batch.

The original label propagation proposed by
(Zhu and Ghahramani, 2002) uses a transition ma-
trix H to model the probability of a node i prop-
agating its type posterior φi = P (yi|xi) ∈ RK to
the other nodes. Each entry of the transition ma-
trix H ∈ RB×B is defined as:

Hij = Aij/
∑
b

Aib (5)

The original label propagation algorithm is de-
fined as:

1. Propagate the label by transition matrix H ,
Φ(t+1) = HΦ(t)

2. Clamp the labeled data to their true labels.
Repeat from step 1 until Φ converges

In this work Φ(0) is randomly initialized2. Unlike
unlabeled data in semi-supervised learning, dis-
tantly labeled mentions in FET have a limited set
of candidate types. Based on this observation, We
assume that (mi, ci) can only transmit and receive
probability of types in Yti no matter it is noisy data
or clean data. Formally, define a B ×K indicator
matrix M ∈ RB×K , where Mij = 1 if type j in
Yti otherwise 0, where B is the batch size and K

1Z = {zi}Bi=1 is a small subsample of the entire data, we
didn’t observe significant performance gain when the batch
size increases.

2We also explored other initialization (e.g. uniform ini-
tialization), but found no essential performance difference be-
tween different initialization setups.



2866

(c) Graph (d) Clamp

(f) Compact(g) Separate(a) Embed

Neural 
Network

 from the Valley on hand because  

(e) Propagate

(h) Suboptimal

(b) Predict
Z2

Z1

A

B

A

B

^

^
B
~

A~

A

B

A

B

A

B

A

B

Figure 4: A demonstration of the CLSC process. (a) represents the feature extraction step; (b)→(h) shows the
traditional type classification process (each color represents one candidate type), where suboptimal classifiers
make predictions for each mention and misclassifies A into the Blue type; (c)→(d)→(e)→(f)→(g) demonstrates
the process of CLSC as described in Section 3. Through label propagation and compact clustering, our model
is able to group mentions of the same type into a dense region and leaves clear separation boundaries in sparse
regions.

is the number of types. Our clamping step relies
on M as is shown in Figure 4d:

Φ
(t+1)
ij ← Φ

(t+1)
ij Mij/

∑
k

Φ
(t+1)
ik Mik (6)

For convenience, we iterate through these two
steps Slp times, Slp is a hyperparameter.

Compact Clustering: The LP posterior Φ =
Φ(Slp+1) is used to judge the label agreement be-
tween samples. In the desired optimal state, transi-
tion probabilities between samples should be uni-
form inside the same class, while be zero between
different classes. Based on this assumption, the
desirable transition matrix T ∈ RB×B is defined
as:

Tij =
K∑
k=1

Φik
Φjk
mk

,mk =
B∑
b=1

Φbk (7)

mk is a normalization term for class k. Transition
matrix H derived from z((mi, ci); θz) should be
in keeping with T . Thus we minimize the cross
entropy between T and H:

L1−step = −
1

B2

B∑
i=1

B∑
j=1

Tijlog(Hij) (8)

For instance, if Tij is close to 1, Hij needs to
be bigger, which results in the growth of Aij
and finally optimize θz (Eq.4). The loss L1−step
has largely described the regularization we use in
z((mi, ci); θz) for compression clustering.

In order to keep the structure of existing clus-
ters, (Kamnitsas et al., 2018) proposed an exten-
sion of L1−step to the case of Markov chains
with multiple transitions between samples, which
should remain within a single class. The exten-
sion maximizes probability of paths that only tra-
verse among samples belong to one class. Define
E ∈ RB×B as:

E = ΦTΦ (9)

Eij measures the label similarities between zi and
zj , which is used to mask the transition between
different clusters. The extension is given by:

H(1) =H

H(s) =(H � E)(s−1)H
=(H � E)H(s−1),

(10)

where � is Hadamard Product, and H(s)ij is the
probability of a Markov process to transit from
node i to node j after s − 1 steps within the same
class. The extended loss function models paths of
different length s between samples on the graph:

Lclsc = −
1

Sm

1

B2

Sm∑
s=1

B∑
i=1

B∑
j=1

Tijlog(H
(s)
ij ).

(11)
For Sm = 1, Lclsc = L1−step. By minimizing
the cross entropy between T and H(s) (Eq.11),
Lclsc compact paths of different length between
samples within the same class. Here, Sm is a
hyper-parameter to control the maximum length



2867

of Markov chain. Lclsc is added to the final objec-
tive function as regularization to encourage com-
pact cluttering.

3.3 Overall Objective
Given the representation of a mention, the type
posterior is given by a standard softmax classifier
parameterized by θg:

P (ŷi|zi; θg) = softmax(Wczi + bc), (12)

where Wc ∈ RK×dz is a parameter matrix, b ∈
RK is the bias vector, where K is the number of
types. The predicted type is then given by t̂i =
argmaxyiP (ŷi|zi; θg).

Our loss function consists of two parts. Lsup is
supervision loss defined by KL divergence:

Lsup =−
1

Bc

Bc∑
i=1

K∑
k=1

yiklog(P (yi|zi; θg))k

(13)
Here Bc is the number of clean data in a train-
ing batch, K is the number of target types. The
regularization term is given by Lclsc. Hence, the
overall loss function is:

Lfinal = Lsup + λclsc × Lclsc (14)

λclsc is a hyper parameter to control the influence
of CLSC.

4 Experiments

4.1 Dataset
We evaluate our method on two standard bench-
marks: OntoNotes and BBN:

• OntoNotes: The OntoNotes dataset is com-
posed of sentences from the Newswire part of
OntoNotes corpus (Weischedel et al., 2013).
(Gillick et al., 2014) annotated the train-
ing part with the aid of DBpedia spotlight
(Daiber et al., 2013), while the test data is
manually annotated.
• BBN: The BBN dataset is composed of sen-

tences from Wall Street Journal articles and
is manually annotated by (Weischedel and
Brunstein, 2005). (Ren et al., 2016a) regen-
erated the training corpus via distant supervi-
sion.

In this work we use the preprocessed datasets pro-
vided by (Abhishek et al., 2017; Xu and Barbosa,
2018). Table 2 shows detailed statistics of the
datasets.

4.2 Compared Methods

We compare the proposed method with several
state-of-the-art FET systems3:

• Attentive (Shimaoka et al., 2016) uses an at-
tention based feature extractor and doesn’t
distinguish clean from noisy data;
• AFET (Ren et al., 2016a) trains label embed-

ding with partial label loss;
• AAA (Abhishek et al., 2017) learns joint rep-

resentation of mentions and type labels;
• PLE+HYENA/FIGER (Ren et al., 2016b)

proposes heterogeneous partial-label embed-
ding for label noise reduction to boost typing
systems. We compare two PLE models with
HYENA (Yogatama et al., 2015) and FIGER
(Ling and Weld, 2012) as the base typing sys-
tem respectively;
• NFETC (Xu and Barbosa, 2018) trains

neural fine-grained typing system with
hierarchy-aware loss. We compare the per-
formance of the NFETC model with two dif-
ferent loss functions: partial-label loss and
PLL+hierarchical loss. We denote the two
variants as NFETC and NFETChier re-
spectively;
• NFETC-CLSC is the proposed model in this

work. We use the NFETC model as our
base model, based on which we apply Com-
pact Latent Space Clustering Regularization
as described in Section 3.2; Similarly, we
report results produced by using both KL-
divergense-based loss (NFETC-CLSC) and
KL+hierarchical loss (NFETC-CLSChier).

4.3 Evaluation Settings

For evaluation metrics, we adopt strict accuracy,
loose macro, and loose micro F-scores widely
used in the FET task (Ling and Weld, 2012). To
fine tuning the hyper-parameters, we randomly
sampled 10% of the test set as a development
set for both datasets. With the fine-tuned hyper-
parameter as mentioned in 4.4, we run the model
five times and report the average strict accuracy,
macro F1 and micro F1 on the test set.

3The baselines result are reported on (Abhishek et al.,
2017; Xu and Barbosa, 2018) in addition to performance of
NFETC on BBN, we search the hyper parameters for it. (Xu
and Barbosa, 2018) didn’t report the results on BBN



2868

Method OntoNotes BBN

Strict Acc. Macro F1 Micro F1 Strict Acc. Macro F1 Micro F1

AFET (Ren et al., 2016a) 55.3 71.2 64.6 68.3 74.4 74.7
AAA (Abhishek et al., 2017) 52.2 68.5 63.3 65.5 73.6 75.2
Attentive (Shimaoka et al., 2016) 51.7 71.0 64.91 48.4 73.2 72.4
PLE+HYENA (Ren et al., 2016b) 54.6 69.2 62.5 69.2 73.1 73.2
PLE+FIGER (Ren et al., 2016b) 57.2 71.5 66.1 68.5 77.7 75.0

NFETC clean 54.4±0.3 71.5±0.4 64.9±0.3 71.2±0.2 77.1±0.3 76.9±0.3

+noisy 54.8±0.4 71.8±0.4 65.0±0.4 73.8±0.6 78.4±0.6 78.9±0.6

NFETChier
clean 59.6±0.2 76.1±0.2 69.7±0.2 70.3±0.3 76.8±0.3 76.6±0.2

+noisy 60.2±0.2 76.4±0.1 70.2±0.2 73.9±1.2 78.8±1.2 79.4±1.1

NFETC-CLSC clean 59.1±0.4 75.3±0.3 69.1±0.3 73.0±0.3 79.0±0.3 78.8±0.3

+noisy 59.6±0.2 75.5±0.4 69.3±0.4 74.7±0.3 80.7±0.2 80.5±0.2

NFETC-CLSChier
clean 61.5±0.3 77.4±0.3 71.4±0.4 70.5±0.2 78.2±0.2 78.0±0.2

+noisy 62.8±0.3 77.8±0.4 72.0±0.4 71.9±0.3 79.8±0.4 79.5±0.3

Table 1: Performance comparision of FET systems on the two datasets.

OntoNotes BBN
#types 89 47
Max hierarchy depth 3 2
#mentions-train 253241 86078
#mentions-test 8963 12845
%clean mentions-train 73.13 75.92
%clean mentions-test 94.00 100
Average |Yti | 1.40 1.26

Table 2: Detailed statistics of the two datasets.

4.4 Hyper Parameters

We search the hyper parameter of Ontonotes
and BBN respectively via Hyperopt proposed by
(Bergstra et al., 2013). Hyper parameters are
shown in Appendix A. We optimize the model via
Adam Optimizer. The full hyper parameters in-
cludes the learning rate lr, the dimension dp of
word position embedding, the dimension dl of the
mention encoder’s output (equal to the dimension
of the context encoder’s ourput), the input dropout
keep probability pi and output dropout keep prob-
ability po for LSTM layers (in context encoder and
LSTM mention encoder), the L2 regularization pa-
rameter λ, the factor of hierarchical loss normal-
ization α (α > 0 means use the normalization),
BN (whether using Batch normalization), the max
step Slp of the label propagation, the max length
Sm of Markov chain, the influence parameter λclsc
of CLSC, the batch sizeB, the number n of hidden
layers in q and the number hn of hidden units of
the hidden layers. We implement all models using

Tensorflow4.

4.5 Performance comparison and analysis

Table 1 shows performance comparison between
the proposed CLSC model and state-of-the-art
FET systems. On both benchmarks, the CLSC
model achieves the best performance in all three
metrics. When focusing on the comparison be-
tween NFETC and CLSC, we have following ob-
servation:

• Compact Latent Space Clustering shows its
effectiveness on both clean data and noisy
data. By applying CLSC regularization on
the basic NFETC model, we observe consis-
tent and significant performance boost;
• Hierarchical-aware loss shows significant ad-

vantage on the OntoNotes dataset, while
showing insignificant performance boost on
the BBN dataset. This is due to different dis-
tribution of labels on the test set. The pro-
portion of terminal types of the test set is
69% for the BBN dataset, while is only 33%
on the OntoNotes dataset. Thus, applying
hierarchical-aware loss on the BBN dataset
brings little improvement;
• Both algorithms are able to utilize noisy data

to improve performance, so we would like to
further study their performance in different
noisy scenarios in following discussions.

4The code for experiments is available at https://github.
com/herbertchen1/NFETC-CLSC



2869

4.6 How robust are the methods to the
proportion of noisy data?

0.300

0.350

0.400

0.450

0.500

0.550

75 80 85 90 95

Proportion of Dclean being removed 

NFETC(c)

NFETC(c+n)

NFETC-CLSC(c)

NFETC-CLSC(c+n)

0.500

0.550

0.600

0.650

0.700

0.750

75 80 85 90 95

Proportion of Dclean being removed 

NFETC(c)

NFETC(c+n)

NFETC-CLSC(c)

NFETC-CLSC(c+n)

Figure 5: Performance comparison between NFETC-
CLSC and NFETC by removing 75%-95% clean data.

By principle, with sufficient amount of clean
training data, most typing systems can achieve
satisfying performance. To further study the
robustness of the methods to label noise, we
compare their performance with the presence of
25%, 20%, 15%, 10% and 5% clean training data
and all noisy training data. Figure 5 shows the per-
formance curves as the proportion of clean data
drops. As it reveals, the CLSC model consis-
tently wins in the comparison. The advantage is
especially clear on the BBN dataset, which offers
less amount of training data. Note that, with only
27.9% of training data (when only leaving 5%
clean data) on the BBN dataset, the CLSC model
yield a comparable result with the NFETC model
trained on full data. This comparison clearly
shows the superiority of our approach in the ef-
fectiveness of utilizing noisy data.

4.7 Ablation: Do Markov Chains improve
typing performance?

Table 3 shows the performance of CLSC with one-
step transition (L1−step) and with Markov Chains
(Lclsc) as described in Section 3.2. Results show

that the use of Markov Chains does bring improve-
ment to the overall performance, which is consis-
tent with the model intuition.

5 Related Work

Named entity Recognition (NER) has been exca-
vated for a long time (Collins and Singer, 1999;
Manning et al., 2014), which classifies coarse-
grained types (e.g. person, location). Recently,
(Nagesh and Surdeanu, 2018a,b) applied ladder
network (Rasmus et al., 2015) to coarse-grained
entity classification in a semi-supervised learning
fashion. (Ling and Weld, 2012) proposed Fine-
Grained Entity Recognition (FET). They used dis-
tant supervision to get training corpus for FET.
Embedding techniques was applied to learn fea-
ture representations since (Yogatama et al., 2015;
Dong et al., 2015). (Shimaoka et al., 2016) in-
troduced attention mechanism for FET to capture
informative words. (Xin et al., 2018a) used the
TransE entity embeddings (Bordes et al., 2013) as
the query vector of attention.
Early works ignore the out-of-context noise,
(Gillick et al., 2014) proposed context dependent
FET and use three heuristics to clean the noisy la-
bels with the side effect of losing training data.
To utilize noisy data, (Ren et al., 2016a) distin-
guished the loss function of noisy data from clean
data via partial label loss (PLL). (Abhishek et al.,
2017; Xu and Barbosa, 2018) proposed variants
of PLL, which still suffer from confirmation bias.
(Xu and Barbosa, 2018) proposed hierarchical loss
to handle over-specific noise. On top of AFET,
(Ren et al., 2016b) proposed a method PLE to re-
duce the label noise, which lead to a great suc-
cess in FET. Because label noise reduction is sep-
arated from the learning of FET, there might be
error propagation problem. Recently, (Xin et al.,
2018b) proposed utilizing a pretrained language
model measures the compatibility between context
and type names, and use it to repel the interfer-
ence of noisy labels. However, the compatibility
got by language model may not be right and type
information is defined by corpus and annotation
guidelines rather than type names as is mentioned
in (Azad et al., 2018). In addition, there are some
work about entity-level typing which aim to figure
out the types of entities in KB (Yaghoobzadeh and
Schütze, 2015; Jin et al., 2018).



2870

Strict Acc.
CLSC(c)(L1−step) 72.0±0.1
CLSC(c)(Lclsc) 73.0±0.3
CLSC(c+n)(L1−step) 73.0±0.1
CLSC(c+n)(Lclsc) 74.7±0.3

Table 3: The comparison ofL1−step andLclsc on BBN.

6 Conclusion

In this paper, we propose a new method for dis-
tantly supervised fine-grained entity typing, which
leverages imperfect annotations as model regu-
larization via Compact Latent Space Clustering
(CLSC). Experiments on two standard bench-
marks demonstrate that our method consistently
outperforms state-of-the-art models. Further study
reveals our method is more robust than the former
state-of-the-art approach as the portion of noisy
data rises. The proposed method is general for
other tasks with imperfect annotation. As a part
of future investigation, we plan to apply the ap-
proach to other distantly supervised tasks, such as
relation extraction.

7 Acknowledgments

This work has been supported in part by NSFC
(No.61751209, U1611461), Zhejiang University-
iFLYTEK Joint Research Center, Chinese Knowl-
edge Center of Engineering Science and Technol-
ogy (CKCEST), Engineering Research Center of
Digital Library, Ministry of Education. Xiang
Ren’s research has been supported in part by Na-
tional Science Foundation SMA 18-29268.

References

Abhishek Abhishek, Ashish Anand, and Amit Awekar.
2017. Fine-grained entity type classification by
jointly learning representations and label embed-
dings. In Proceedings of the 15th Conference of
the European Chapter of the Association for Com-
putational Linguistics: Volume 1, Long Papers, vol-
ume 1, pages 797–807.

Amar Prakash Azad, Balaji Ganesan, Ashish Anand,
Amit Awekar, et al. 2018. A unified labeling ap-
proach by pooling diverse datasets for entity typing.
arXiv preprint arXiv:1810.08782.

James Bergstra, Dan Yamins, and David D Cox. 2013.
Hyperopt: A python library for optimizing the hy-
perparameters of machine learning algorithms. In
Proceedings of the 12th Python in Science Confer-
ence, pages 13–20. Citeseer.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in neural information
processing systems, pages 2787–2795.

Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
1999 Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora.

Joachim Daiber, Max Jakob, Chris Hokamp, and
Pablo N Mendes. 2013. Improving efficiency and
accuracy in multilingual entity extraction. In Pro-
ceedings of the 9th International Conference on Se-
mantic Systems, pages 121–124. ACM.

Li Dong, Furu Wei, Hong Sun, Ming Zhou, and Ke Xu.
2015. A hybrid neural model for type classification
of entity mentions. In IJCAI, pages 1243–1249.

Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowledge
vault: A web-scale approach to probabilistic knowl-
edge fusion. In Proceedings of the 20th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 601–610. ACM.

Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse
Kirchner, and David Huynh. 2014. Context-
dependent fine-grained entity type tagging. arXiv
preprint arXiv:1412.1820.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Hailong Jin, Lei Hou, Juanzi Li, and Tiansi Dong.
2018. Attributed and predictive entity embedding
for fine-grained entity typing in knowledge bases. In
Proceedings of the 27th International Conference on
Computational Linguistics, pages 282–292.

Konstantinos Kamnitsas, Daniel Castro, Loic Le Fol-
goc, Ian Walker, Ryutaro Tanno, Daniel Rueckert,
Ben Glocker, Antonio Criminisi, and Aditya Nori.
2018. Semi-supervised learning via compact latent
space clustering. In Proceedings of the 35th In-
ternational Conference on Machine Learning, vol-
ume 80 of Proceedings of Machine Learning Re-
search, pages 2459–2468, Stockholmsmssan, Stock-
holm Sweden. PMLR.

Samuli Laine and Timo Aila. 2017. Temporal ensem-
bling for semi-supervised learning. In Proc. Inter-
national Conference on Learning Representations
(ICLR).

Changki Lee, Yi-Gyu Hwang, Hyo-Jung Oh, Soo-
jong Lim, Jeong Heo, Chung-Hee Lee, Hyeon-Jin
Kim, Ji-Hyun Wang, and Myung-Gil Jang. 2006.
Fine-grained named entity recognition using condi-
tional random fields for question answering. In Asia
Information Retrieval Symposium, pages 581–587.
Springer.



2871

Xiao Ling and Daniel S Weld. 2012. Fine-grained en-
tity recognition. In AAAI, volume 12, pages 94–100.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The stanford corenlp natural language pro-
cessing toolkit. In Proceedings of 52nd annual
meeting of the association for computational lin-
guistics: system demonstrations, pages 55–60.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.

Ajay Nagesh and Mihai Surdeanu. 2018a. An ex-
ploration of three lightly-supervised representation
learning approaches for named entity classification.
In Proceedings of the 27th International Conference
on Computational Linguistics, pages 2312–2324.

Ajay Nagesh and Mihai Surdeanu. 2018b. Keep your
bearings: Lightly-supervised information extraction
with ladder networks that avoids semantic drift. In
Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 2 (Short Papers), volume 2, pages 352–358.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Jonathan Raiman and Olivier Raiman. 2018. Deep-
type: Multilingual entity linking by neural type sys-
tem evolution. In Proceedings of the Thirty-Second
AAAI Conference on Artificial Intelligence, (AAAI-
18), the 30th innovative Applications of Artificial In-
telligence (IAAI-18), and the 8th AAAI Symposium
on Educational Advances in Artificial Intelligence
(EAAI-18), New Orleans, Louisiana, USA, February
2-7, 2018, pages 5406–5413.

Antti Rasmus, Mathias Berglund, Mikko Honkala,
Harri Valpola, and Tapani Raiko. 2015. Semi-
supervised learning with ladder networks. In Ad-
vances in Neural Information Processing Systems,
pages 3546–3554.

Xiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng
Ji, and Jiawei Han. 2016a. Afet: Automatic fine-
grained entity typing by hierarchical partial-label
embedding. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1369–1378.

Xiang Ren, Wenqi He, Meng Qu, Clare R Voss, Heng
Ji, and Jiawei Han. 2016b. Label noise reduction in

entity typing by heterogeneous partial-label embed-
ding. In Proceedings of the 22nd ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 1825–1834. ACM.

Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and
Sebastian Riedel. 2016. An attentive neural ar-
chitecture for fine-grained entity type classification.
In Proceedings of the 5th Workshop on Automated
Knowledge Base Construction, pages 69–74.

Antti Tarvainen and Harri Valpola. 2017. Mean teach-
ers are better role models: Weight-averaged consis-
tency targets improve semi-supervised deep learning
results. In Advances in neural information process-
ing systems, pages 1195–1204.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Ralph Weischedel and Ada Brunstein. 2005. Bbn pro-
noun coreference and entity type corpus. Linguistic
Data Consortium, Philadelphia, 112.

Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, et al. 2013. Ontonotes release 5.0
ldc2013t19. Linguistic Data Consortium, Philadel-
phia, PA.

Ji Xin, Yankai Lin, Zhiyuan Liu, and Maosong Sun.
2018a. Improving neural fine-grained entity typing
with knowledge attention.

Ji Xin, Hao Zhu, Xu Han, Zhiyuan Liu, and Maosong
Sun. 2018b. Put it back: Entity typing with lan-
guage model enhancement. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, pages 993–998.

Peng Xu and Denilson Barbosa. 2018. Neural fine-
grained entity type classification with hierarchy-
aware loss. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), volume 1,
pages 16–25.

Yadollah Yaghoobzadeh and Hinrich Schütze. 2015.
Corpus-level fine-grained entity typing using con-
textual information. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 715–725.

Dani Yogatama, Daniel Gillick, and Nevena Lazic.
2015. Embedding methods for fine grained entity
type classification. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 2:
Short Papers), volume 2, pages 291–296.

http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17148
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17148
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17148


2872

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2335–2344.

Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from labeled and unlabeled data with label propaga-
tion.

A Hyper parameters

Ont.(C) BBN(C) BBN(N)
lr 0.0006 0.0007 0.0007
dp 70 40 20
dl 1000 1000 240
pi 0.7 0.3 0.5
po 0.6 1.0 0.4
λ 0.0000 0.0000 0.0002
α 0.25/0.0 0.4/0.0 0.4/0.0
BN FALSE FALSE TRUE
Slp 200 200 -
Sm 8 12 -
λclsc 2.0 1.5 -
B 512 512 512
n 2 1 -
hn 700 560 -

Table 4: Hyper parameters of our experiments: (C) de-
notes CLSC, (N) denotes the hyper parameter is used
for NFETC.


