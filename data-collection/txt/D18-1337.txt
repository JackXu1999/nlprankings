

















































Prediction Improves Simultaneous Neural Machine Translation


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3022–3027
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

3022

Prediction Improves Simultaneous Neural Machine Translation

Ashkan Alinejad 1, Maryam Siahbani2, Anoop Sarkar1
1Simon Fraser University, Burnaby, BC, Canada

{aalineja,anoop}@sfu.ca
2University of the Fraser Valley, Abbotsford, BC, Canada

maryam.siahbani@ufv.ca

Abstract

Simultaneous speech translation aims to main-
tain translation quality while minimizing the
delay between reading input and incremen-
tally producing the output. We propose a new
general-purpose prediction action which pre-
dicts future words in the input to improve qual-
ity and minimize delay in simultaneous trans-
lation. We train this agent using reinforce-
ment learning with a novel reward function.
Our agent with prediction has better transla-
tion quality and less delay compared to an
agent-based simultaneous translation system
without prediction.

1 Introduction
One of the next significant challenges in machine
translation research is to make translation ubiq-
uitous using real-time translation. Simultaneous
machine translation aims to address this issue by
interleaving reading the input with writing the
output translation. Current Simultaneous Neural
Machine Translation (SNMT) systems (Satija and
Pineau, 2016; Cho and Esipova, 2016; Gu et al.,
2017) use an AGENT to control an incremental
encoder-decoder (or sequence to sequence) NMT
model. Each READ adds more information to the
encoder RNN, and each WRITE produces more
output using the decoder RNN. In this paper, we
propose adding a new action to the AGENT: a
PREDICT action that predicts what words might
appear in the input stream. Prediction was pre-
viously proposed in simultaneous statistical ma-
chine translation (Grissom II et al., 2014) but has
not been studied in the context of Neural Machine
Translation (NMT). In SNMT systems, prediction
of future words augments the encoder-decoder
model with possible future contexts to produce
output translations earlier (minimize delay) and/or
produce better output translations (improve trans-
lation quality). Our experiments show that predic-
tion improves SNMT in both these measures.

2 Simultaneous Translation Framework
An agent-based framework whose actions decide
whether to translate or wait for more input is a
natural way to extend neural MT to simultaneous
neural MT and has been explored in (Satija and
Pineau, 2016; Gu et al., 2017) which contains two
main components: The ENVIRONMENT which re-
ceives the input words X = {x1, . . . , xN} from
the source language and incrementally generates
translated words W = {w1, . . . , wM} in the tar-
get language; And the AGENT which decides an
action for each time step, at. The AGENT gen-
erates an action sequence A = {a1, . . . , aT } to
control the ENVIRONMENT.

Previous models only include two actions:
READ and WRITE. We extend the model by
adding the third action called PREDICT. Action
READ is simply sending a new word to the EN-
VIRONMENT and generating a candidate word in
the target language. In action WRITE, the AGENT
takes current candidate word and sends it to the
output. For PREDICT, the AGENT predicts the
next word in the input and treats it like a READ
action. The following section explains how the
ENVIRONMENT deals with different actions.

2.1 ENVIRONMENT

The ENVIRONMENT is an attention-based
Encoder-Decoder MT system (Bahdanau et al.,
2014) which is adopted to simultaneous transla-
tion task. The Encoder receives the embedded
representation of the input words (including
predicted ones) and converts them into context
vectors H⇢n = {h1, . . . , hn+⇢} using a gated
RNN (GRU) where n is the number of input
words so far and ⇢ is the number of predicted
words since the last READ. Whenever the
AGENT decides to READ, ⇢ will be set to 0, and
hn = fENC(hn�1, xn), where xn is the next input
words (n  N ). But if the action is PREDICT,
⇢ > 0, the AGENT predicts a new word x0⇢ and the



3023

context vector hn+⇢ = fENC(hn+⇢�1, x0⇢) will be
added to H⇢n = {h1, . . . , hn+⇢}.

At each time step t, the decoder uses the current
context vectors (H⇢n) to generate the next candi-
date output (yt):

ct = aATT(zm�1, H
⇢
n)

st = fDEC(zm�1, wm�1, ct)
p(yt|y<t, H⇢n) = g(wm�1, zm, ct)
yt = argmax

y
p(y|y<t, H⇢n)

where wm�1 is the previous output word, and aATT
is an attention model (Bahdanau et al., 2014), f
and g are nonlinear functions, and ct is the current
context vector.

If the action at is either READ or PREDICT the
current candidate yt will be ignored (wait for bet-
ter predictions). But in the case of WRITE, the
candidate yt is produced as the next output word
wm and then the decoder state will be updated
(wm  yt, zm  st).

Note that as soon as the AGENT decides to
READ, all the hidden vectors generated by PRE-
DICT actions will be discarded (H⇢n = H0n =
{h1, . . . , hn}).

Figure 1 shows an example of how a sentence
can be translated using our modified translation
framework 1.

2.2 AGENT
The AGENT is a separate component which exam-
ines the ENVIRONMENT at each time step and de-
cides on the actions that lead to better translation
quality and lower delay. The agent in the greedy
decoding framework (Gu et al., 2017) was trained
using reinforcement learning with the policy gra-
dient algorithm (Williams, 1992), which observes
the current state of the ENVIRONMENT at time
step t as ot where ot = [ct; st; wm]. A RNN with
one hidden layer passed through a softmax func-
tion generates the probability distribution over the
actions at at each step. Therefore, policy ⇡✓ will
be computed as:

ut = f✓(ut�1, ot)
⇡✓(at|a < t, o  t) / g✓(ut)

Where ut is the hidden state of the AGENT’s
RNN.

3 Training the AGENT with Prediction
In order to speed-up the training process, we have
restricted AGENT’s options by removing redun-
dant operations. As illustrated in Figure 2, af-

1The pseudo-code is available in supplementary material
(Algorithm 1).

Figure 1: A schematic of our model. The ENVIRONMENT
starts with reading the ’Start of Sentence’ symbol as x1 and
generating y1 from the decoder. Based on the output of the
network at each time step, the AGENT decides whether to
READ, WRITE or PREDICT for the following time steps.

R P W

Figure 2: Action transition graph. R, P, and W stands for
READ, PREDICT and WRITE actions respectively.

ter a series of WRITE, the AGENT cannot choose
to PREDICT, and after a sequence of PREDICTs,
READ is not an option.
Reward Function: The total reward at any time
step is calculated as the cumulative sum of rewards
for actions at each preceding step. All the evalu-
ation metrics have been modified to be computed
for every time step.
Quality: We use a modified smoothed version of
BLEU score (Chen and Cherry, 2014) multiplied
by Brevity Penalty (Lin and Och, 2004) for evalu-
ating the impact of each action on translation qual-
ity. At each point in time, the reward for transla-
tion quality is:

r
Q
t =

(
�BLEU(t) t < T
BLEU(W,W ⇤) t = T

The �BLEU(t) is the difference between BLEU
score of the translated sentence at the previous
time step and the current time step; �BLEU(t) =
BLEU(W t,W ⇤)�BLEU(W t�1,W ⇤); where W t
is the prefix of the translated sentence at time t.
Delay: The Delay reward is used to motivate the
AGENT to minimize delay. We use Average Pro-
portion (AP) (Cho and Esipova, 2016) for this pur-
pose, which is the average number of source words



3024

needed when translating each word. Given the
source words X and translated words W , AP can
be computed as:

d(X,W ) =
1

|X||W |
X

t

s(t)

r
D
t =

(
0 t < T

d(X,W ) t = T
Where s(t) denotes the number of source words

the WRITE action uses at time step t (for any other
actions, s(t) would be zero). The delay reward is
smoothed using a Target Delay which is a scalar
constant denoted by d⇤ (Gu et al., 2017):

r
D
t = bdt � d⇤c

Prediction Rewards for Quality and Delay alone
do not motivate the AGENT to choose prediction
and in preliminary experiments, after a number
of steps, the number of prediction actions became
zero. We address this problem by defining Predic-
tion Quality (PQ) which rewards the AGENT for
changes in BLEU score after each prediction ac-
tion. By initializing rp0 = 0, the prediction reward
can be written as:

r
p
t =

8
><

>:

�BLEU(t) at = W, at�1 = P
r
p
t�1 at = W, at�1 6= P
0 otherwise

Final Reward The final reward function is cal-
culated as the combination of quality, delay, and
prediction rewards:

rt = ↵ r
Q
t + � r

D
t + � r

p
t (1)

The trade-off between better translation quality
and minimal delay is achieved by modifying the
parameters ↵, �, and �.
Reinforcement Learning is used to train
the AGENT using a policy gradient algo-
rithm (Gu et al., 2017; Williams, 1992)
which searches for the maximum in
J(✓) = E⇡✓

hPT
t=1 rt

i
using the gradient:

r✓J(✓) = E⇡✓ [r✓ log ⇡✓(a|s)Q⇡✓(s, a)] where
Q =

PT
t=1 rt. The gradient for a sentence is the

cumulative sum of gradients at each time step. We
pre-train the ENVIRONMENT on full sentences
using log-loss log p(y|x).

4 Experiments
We train and evaluate our model on English-
German (EN-DE) in both directions. We use
WMT 2015 for training and Newstest 2013 for
validation and testing. All sentences have been to-
kenized and the words are segmented using byte
pair encoding (BPE) (Sennrich et al., 2016).

0 5 10 15 20 25
0

20

40

60

80

100

32.11
24.51 23.93

14.69
22.94 23.87

34.1
35.23 33.98

38.13
34.57 34.96

33.79
40.26 42.09

47.18 42.49 41.18

Iteration (x1000)

Pe
rc

en
ta

ge

Read
Write

Predict

↵ = 1
� = 0.5
� = 0.5

Quality: 17.54
Delay: 0.74

Figure 3: Action distribution for English to German trans-
lation in the first 25000 iterations. The numbers in each bar
are the average action percentage over the previous 5000 it-
erations.

Model Configuration For a fair comparison, we
follow the settings that worked the best for the
greedy decoding model in (Gu et al., 2017) and set
the target delay d⇤ for the AGENT to 0.7. The EN-
VIRONMENT consists of two unidirectional layers
with 1028 GRU units for encoder and decoder.
We train the network using AdaDelta optimizer, a
batch of size 32 and a fixed learning rate of 0.0001
without decay. We use softmax policy via recur-
rent networks with 512 GRU units and a softmax
function for the AGENT and train it using Adam
optimizer (Kingma and Ba, 2014). The batch size
for the AGENT is 10, and the learning rate is 2e-
6. The word predictor is a two layer RNN Lan-
guage model which consists of two layers of 1024
units, followed by a softmax layer. The batch size
is 64 with a learning rate of 2e-5. The predic-
tor has been trained on the WMT’16 dataset and
tested on Newstest’16 corpora for both languages.
The perplexity of our language model is reported
in Table 1. We set ↵ = 1, � = 0.5 and � = 0.5.
We tried different settings for these hyperparame-
ters during training and picked values that gave us
the best Quality and Delay on the training data.
Results and Analysis Figure 4 shows that as the
sentence length increases, prediction helps transla-
tion quality due to complex reordering and multi-
clausal sentences; However, for shorter samples
where the structure of the sentences are simpler,
the prediction action cannot improve translation
quality. Table 2 compares our model with the
Greedy Decoding (GD) model in terms of trans-
lation quality and latency. It shows that the pre-
diction mechanism outperforms the GD model in
terms of BLEU and average proportion (AP).

The delay evaluation measure (AP) counts
about the same number of READs and WRITEs. It



3025

0 10 20 30 40 50

14

16

18

20

22

Sentence Length

B
LE

U
sc

or
e

Greedy Decoding
Predict Model

(a) EN!DE

0 10 20 30 40 50

17

19

21

23

25

Sentence Length

B
LE

U
sc

or
e

Greedy Decoding
Predict Model

(b) DE!EN
Figure 4: Comparing translation quality between prediction model and greedy decoding method for various sentence lengths.

Perplexity
Test Train

English 17.5917 8.1350
German 19.8532 11.1808

Table 1: Performance of our predictor for both English and
German languages.

does not account for less delay as longer sentences
are produced. A better measure than AP might
be needed to emphasize delay differences. There-
fore we also report the average segment length
(µ), which is computed as the average number
of consecutive READs in each sentence. In both
EN!DE and DE!EN experiments, our model
constantly decreases the segment length by around
1 word which results in less latency.

In order to evaluate the effectiveness of our
proposed reward function for PREDICT action,
we have explored various values for its hyper-
parameters (↵, �, and �). Our empirical re-
sults show that the best trade-off between quality
and delay is achieved when around 20 percent of
the actions are PREDICT for both EN!DE and
DE!EN translation tasks (Figure 3). When there
is no reward for PREDICT action (� = 0), the
AGENT prefers other actions, and the number of
PREDICT actions turns into zero immediately af-
ter training the AGENT. If the reward for predic-
tion is valued too highly, the ENVIRONMENT de-
pends more on predicted words and the translation
quality decreases2.

5 Related work
Early work in SNMT was done in speech, where
the incoming signals were segmented based on
acoustic or statistical cues (Bangalore et al., 2012;
Fügen et al., 2007). (Sridhar et al., 2013; Matusov

2See Figure 6 in supplementary materials for more numer-
ical results.

EN!DE DE!EN
BLEU AP µ BLEU AP µ

GD 16.75 0.79 5.6 21.43 0.77 6
PM 17.54 0.74 4.7 21.83 0.70 5.2

Table 2: Comparison of translation quality (BLEU), Av-
erage Proportion (AP), and average segment length (µ) for
Greedy Decoding (GD) model with Prediction Mechanism
(PM) model.

et al., 2007; Yarmohammadi et al., 2013; Siah-
bani et al., 2014) use a separate segmentation step
and incrementally translate each segment using a
standard phrase-based MT system. (Matsubara
et al., 2000) applied pattern matching to predict
target-side verbs in Japanese to English transla-
tion. (Grissom II et al., 2014) used reinforcement
learning to predict the next word and the sentence-
final verb in a statistical MT model. These mod-
els reduce the delay but are not trained end-to-
end like our agent-based SNMT system. (Cho and
Esipova, 2016) proposed a non-trainable heuristic
agent which is not able to trade-off quality with
delay. It always prefers to read more words from
the input and this approach does not work well in
practice. (Satija and Pineau, 2016) introduced a
trainable agent which they trained using Deep Q
networks (Mnih et al., 2015). We modified the
SNMT trainable agent in (Gu et al., 2017) and
added a new non-trivial PREDICT action to the
agent. We compare to their model and show better
results in delay and quality.

6 Conclusion

We introduce a new prediction action in a train-
able agent for simultaneous neural machine trans-
lation. With prediction, the agent can be informed
about future time steps in the input stream. Com-
pared to a very strong baseline our results show
that prediction can lower delay and improve the



3026

translation quality, especially for longer sentences
and translating from an SOV (subject-object-verb)
language (DE) to an SVO language (EN).

Acknowledgments
We would like to thank the anonymous reviewers
for their helpful remarks. The research was also
partially supported by the Natural Sciences and
Engineering Research Council of Canada grants
NSERC RGPIN-2018-06437 and RGPAS-2018-
522574 and a Department of National Defence
(DND) and NSERC grant DGDND-2018-00025
to the third author.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua

Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Srinivas Bangalore, Vivek Kumar Rangarajan Srid-
har, Prakash Kolan, Ladan Golipour, and Aura
Jimenez. 2012. Real-time incremental speech-to-
speech translation of dialogs. In Proceedings of
the 2012 Conference of the North American Chap-

ter of the Association for Computational Linguistics:

Human Language Technologies, NAACL HLT ’12,
pages 437–445, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Boxing Chen and Colin Cherry. 2014. A systematic
comparison of smoothing techniques for sentence-
level bleu. In WMT, pages 362–367.

Kyunghyun Cho and Masha Esipova. 2016. Can neu-
ral machine translation do simultaneous translation?
CoRR, abs/1606.02012.

Christian Fügen, Alex Waibel, and Muntsin Kolss.
2007. Simultaneous translation of lectures and
speeches. Machine Translation, 21(4):209–252.

Alvin Grissom II, He He, Jordan Boyd-Graber, John
Morgan, and Hal Daumé III. 2014. Don’t until the
final verb wait: Reinforcement learning for simulta-
neous machine translation. In Empirical Methods in
Natural Language Processing.

Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Vic-
tor O.K. Li. 2017. Learning to translate in real-time
with neural machine translation. In 15th Conference
of the European Chapter of the Association for Com-

putational Linguistics (EACL), Valencia, Spain.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram

statistics. In Proceedings of the 42Nd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’04, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Shigeki Matsubara, Keiichi Iwashima, Nobuo
Kawaguchi, Katsuhiko Toyama, and Yasuyoshi
Inagaki. 2000. Simultaneous japanese-english
interpretation based on early prediction of english
verb. Proceedings of the 4th Symposium on Nat-
ural Languauge Processing(SNLP-2000), pages
268–273.

Evgeny Matusov, Dustin Hillard, Mathew Magimai-
Doss, Dilek Hakkani-Tür, Mari Ostendorf, and Her-
mann Ney. 2007. Improving speech translation with
automatic boundary prediction. In Eighth Annual
Conference of the International Speech Communi-

cation Association.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A. Rusu, Joel Veness, Marc G. Bellemare,
Alex Graves, Martin Riedmiller, Andreas K. Fidje-
land, Georg Ostrovski, Stig Petersen, Charles Beat-
tie, Amir Sadik, Ioannis Antonoglou, Helen King,
Dharshan Kumaran, Daan Wierstra, Shane Legg,
and Demis Hassabis. 2015. Human-level con-
trol through deep reinforcement learning. Nature,
518:529 EP –.

Harsh Satija and Joelle Pineau. 2016. Simultaneous
machine translation using deep reinforcement learn-
ing. Abstraction in Reinforcement Learning Work-
shop, ICML 2016, (33).

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural Machine Translation of Rare Words
with Subword Units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational

Linguistics (Volume 1: Long Papers), pages 1715–
1725, Berlin, Germany. Association for Computa-
tional Linguistics.

Maryam Siahbani, Ramtin Mehdizadeh Seraj,
Baskaran Sankaran, and Anoop Sarkar. 2014. Incre-
mental translation using hierarchichal phrase-based
translation system. In Spoken Language Technology
Workshop (SLT), 2014 IEEE, pages 71–76. IEEE.

Vivek Kumar Rangarajan Sridhar, John Chen, Srinivas
Bangalore, Andrej Ljolje, and Rathinavelu Chengal-
varayan. 2013. Segmentation strategies for stream-
ing speech translation. In Proceedings of the 2013
Conference of the North American Chapter of the

Association for Computational Linguistics: Human

Language Technologies, pages 230–238.

Ronald J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. In Machine Learning, pages 229–
256.

Mahsa Yarmohammadi, Vivek Kumar Rangara-
jan Sridhar, Srinivas Bangalore, and Baskaran
Sankaran. 2013. Incremental segmentation and



3027

decoding strategies for simultaneous translation.
In Proceedings of the Sixth International Joint
Conference on Natural Language Processing, pages
1032–1036.


