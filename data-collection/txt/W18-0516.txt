



















































Chengyu Cloze Test


Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 154–158
New Orleans, Louisiana, June 5, 2018. c©2018 Association for Computational Linguistics

Chengyu Cloze Test

Zhiying Jiang, Boliang Zhang, Lifu Huang and Heng Ji
Computer Science Department

Rensselaer Polytechnic Institute
{jiangz6, zhangb8, huangl7, jih}@rpi.edu

Abstract

We present a neural recommendation model
for Chengyu, which is a special type of Chi-
nese idiom. Given a query, which is a sentence
with an empty slot where the Chengyu is taken
out, our model will recommend the Chengyu
candidate that best fits the slot context. The
main challenge lies in that the literal meaning
of a Chengyu is usually very different from
its figurative meaning. We propose a neural
approach to incorporate the definition of each
Chengyu as background knowledge. Experi-
ments on both Chengyu cloze test and coher-
ence checking in college entrance exams show
that our system achieves 89.5% accuracy on
cloze test and outperforms human experts who
attended competitive universities in China. We
will make all of our data sets and resources
publicly available as a new benchmark for re-
search purposes1.

1 Introduction

Chengyu (“成 语”, literal translation: “form
phrases”) is a special type of Chinese idiom, and
represents one of the most beautiful, fascinat-
ing and unique aspects of the Chinese language.
96% Chengyus consist of four characters each.
Chengyus were mainly created from ancient sto-
ries, literature and sayings which can be traced
back to thousands of years ago. Some exam-
ples are shown in Table 1. More than 7,000
Chengyus are still widely used in the modern
Chinese, Japanese, Korean and Vietnamese lan-
guages. Like idioms in other languages, us-
ing Chengyu appropriately makes communication
more compelling and engaging because they in-
troduce powerful imagery and figurative meanings
that differ from their literal meanings.

When learning Chinese phrases, Chengyus are
always the most difficult to understand and mem-

1https://github.com/bazingagin/chengyu data

orize. Second-language learners generally have a
love-hate relation with Chengyu and tend to avoid
it. A typical way to measure a Chinese learner’s
Chengyu knowledge is “Cloze Test”, in which the
learner is asked to supply the best Chengyu that
has been removed from a sentence. It’s consid-
ered as one of the most difficult problems in Chi-
nese college entrance language and literature ex-
ams, and has been the focus of several TV tal-
ent shows in China such as the Chinese Idiom
Congress by CCTV. This motivated us to develop
the first Chengyu recommendation system to as-
sist Chinese learners. Given a context sentence
(“query”) with a Chengyu removed, the system
will automatically recommend the best Chengyu
to fill in the blank.

The four characters in each Chengyu are of-
ten unintelligible without understanding the back-
ground story. For example, “沉鱼落雁 (literal
translation: sink fish fall swallow)” and “闭月羞
花 (literal translation: hide moon shame flower)”
were used to summarize four stories of the top four
beauties in ancient China: Xi Shi, Wang Zhao-
jun, Diao Chan and Yang Yuhuan. They were
being so beautiful that fish sank, birds fell from
the sky, the moon hid, and flowers were shamed.
As a result, we cannot compose the meaning of a
Chengyu only based on its four characters. More-
over, each Chengyu is highly succinct, compact
and synthetic. For example, “一日三秋 (literal
translation: one day three autums)” means greatly
missing someone so that one day feels as long as
three years. However, its key meaning “missing”
is not in this Chengyu.

To address these challenges, we create a new
Chengyu Cloze Test benchmark, which consists
of 108,987 query sentences and 7,395 target
Chengyus. Each Chengyu is associated with a def-
inition, which describes its general meanings and
scenarios where it occurs. Then we develop an

154



Origin Query Recommended Chengyu and Its Definition

Historical Story

⽂学接受史上经常有这样的现象，某
些作品在它的那个时代曾经风⾏⼀
时，_____。 
Throughout history some literary works 
were extremely popular, so much so that 
____.

洛阳纸贵 (Luoyang's paper became expensive) 
原指西晋都城洛阳之纸，因⼤家争相传抄左思的作品《三都赋》，以⾄
⼀时供不应求，货缺⽽贵。⽐喻著作有价值，流传⼴。 
During the Western Jin Dynasty, people kept copying and propagating Zuosi's 
work “San Du Fu", which caused the supply of paper falling  short of demand. 
Now a metaphor to describe that some great work is valuable and widely 
disseminated.

Ancient 
Chinese 

Literature

三年的⾼中⽣活如____⼀般转瞬⽽
逝。 
The three years of high school life was 
like ____, time takes wings.

⽩驹过隙 (time passes quickly like a white pony's shadow across a 
crevice) 
《庄⼦》：“⼈⽣天地之间，若⽩驹之过隙，忽然⽽已。” 
Chuang Tzu said ``Human life between heaven and earth is like the white 
pony seen through a crack in the wall, it's just a moment.

Fable Proverb 
Saying

____，各显神通，预⽰着省楹联艺术
家协会成⽴后必将涌现出成千上万不
计其数的佳联妙对。 
____, each with their own special 
powers. This indicates that thousands of 
great couplets will emerge and decorate 
Shanjin beautifully. 

⼋仙过海 (Eight immortals cross the sea) 
相传⼋仙过海时不⽤⾈船，各有⼀套法术，后⽐喻各⾃拿出本领或办
法，互相竞赛。 
It's said that eight immortals crossed the sea without boats because each of 
them had special power. Now it's used to describe using one's unique skill to 
compete.

Foreign 
Literature

我们喜欢⽤经济去控制⼀个国家的命
脉,⽤信仰去控制⼀个种族,⽤利益让
别⼈为我们____。 
We like to use economy to control a 
country's faith, use belief to control a 
race and use profit to control others so 
they can ____ for us.

⽕中取栗 (pull chestnuts from the embers) 
出⾃⼗七世纪法国寓⾔诗⼈拉·封丹的寓⾔《猴⼦与猫》。⽐喻受⼈利⽤
去冒险，吃了苦头却得不到⼀点好处。 
From the 17 century French fabulist Jean de la Fontaine's "The Monkey and 
the Cat". Bertrand the monkey persuades Raton the cat to pull chestnuts from 
the embers amongst which they are roasting, promising him a share. As the cat 
scoops them from the fire one by one, burning his paw in the process, the 
monkey gobbles them up. It's used to describe a person used unwittingly or 
unwillingly by another to accomplish the other's own purpose with his own 
risk but gets nothing.

Metaphor

这篇⼩说情节完整⽣动，⼈物性格鲜
明，但____，个别语句还⽋推敲。 
This novel includes a complete and 
vivid plot, and the characters have 
distinct personalities. But it's like ____- 
some sentences need to be further 
polished.

⽩璧微瑕 (white jade with a little blemish) 
洁⽩的⽟上有些⼩斑点。⽐喻很好的⼈或物有些⼩缺点，美中不⾜。 
A flaw in a white jade. It's a metaphor for a good person or a good thing with 
a little defect.

Table 1: Chengyu Examples

attentive neural network architecture to select the
most appropriate Chengyu to fit in the slot context
of each query. We first encode query sentence and
Chengyu definitions using a bi-directional long
short-term memory (Bi-LSTM) network (Hochre-
iter and Schmidhuber, 1997). To better capture
the correlation between the query and the defini-
tion, we apply a soft attention to assign a weight
to each word in the query sentence, and predict a
matching score for each candidate Chengyu. Our
system significantly outperforms human learners
who attended top universities in China.

2 Related Work

Our Chengyu cloze test task is similar to read-
ing comprehension (Hermann et al., 2015; Cui
et al., 2016; Chen et al., 2016; Kadlec et al., 2016;
Seo et al., 2016). However, it’s more challeng-
ing because the context includes a sentence in-
stead of a paragraph, the Chengyu phrase itself
does not convey its figurative meaning, and there
are many more candidate answers. Very few Nat-
ural Language Processing techniques have been
applied to understand or recommend Chengyu.

Chung (2009) studied a subset of Chinese fig-
urative language, focusing on Chinese five el-
ements and body part terms. Limited efforts
have used Chengyu dictionaries to expand Chinese
emotion lexicon (Xu et al., 2010) and improve
Chinese word segmentation (Chan and Chong,
2008; Sun and Xu, 2011; Wang and Xu, 2017).
Chengyus differ from metaphors in other lan-
guages (Tsvetkov et al., 2014; Shutova, 2010) be-
cause they do not follow the grammatical structure
and syntax of the modern Chinese.

3 Approach

Figure 1 shows the overall architecture of our ap-
proach. For a query and the definition of a candi-
date Chengyu, we first apply a word segmentation
tool jieba2 to segment query and definition into
words, and apply a Bi-LSTM network to encode
each word with a contextual embedding. In order
to better capture the correlation between a query
and a Chengyu, we further compare the represen-
tations of the Chengyu definition and the contex-
tual embedding of each word in the query, and

2https://github.com/fxsjy/jieba

155



Figure 1: Architecture Overview

take the weighted sum of the query word contex-
tual embeddings as input to a linear function to
determine the probability score of the candidate
Chengyu. Next we show the approach details.

Encoding Given a query q and a Chenyu defi-
nition dj from the target Chengyu database D =
{d1, d2, ..., dm}, we apply two Bi-LSTM net-
works to encode them separately. Each Bi-LSTM
network leverages long distance features from the
whole sentence to capture the context information
by using a memory cell (Hochreiter and Schmid-
huber, 1997). Each word in q and dj is assigned a
contextual embedding.

Attention To better capture the correlation be-
tween a query and each Chengyu definition, we
use an attention mechanism (Bahdanau et al.,
2014; Sutskever et al., 2014) to compare the se-
mantic relatedness of each word in the query sen-
tence with the meaning of each Chengyu defini-
tion.

Given the hidden states H = h0, h1, ..., hn of
the Bi-LSTM encoding the query sentence, where
hi denotes the concatenation of the hidden states
of word wi with forward and backward LSTMs,
the attention layer sum over hi with learnable
weight α: R =

∑n
i=1 αi · hi, where R is the

weighted sum vector representation of the query.
αi is a learnable weight which is computed by
αi =

exp(ei)∑n
i=1 exp(ei)

and ei = dT ·Wα ·hi, where Wα
is a parameter to capture the relevance between a
query and a definition flexibly (Chen et al., 2016).
dT is the last hidden hidden state of the Bi-LSTM
encoding the definition.

Training With the weighted sum vector repre-
sentation of the query R, we apply a softmax func-
tion to compute the probability of each candidate
Chengyu dj to be filled into the slot.

oi = WTβR

pi =
exp(oi)∑m
j=1 exp(oj)

,

where Wβ maps the final representation of
the query into Rm, and m is the number of
classes. Then we optimize the log likelihood:
L =

∑m
j=1 yjlog(pj), where yj is 0 or 1 depend-

ing on if the truth is Chengyu dj or not.

Prediction For prediction, we take a query with
each Chengyu definition (q, dj), 1 ≤ j ≤ m as in-
put, and predict a probability matrix M ∈ Rm×m,
where m is the number of candidates. For exam-
ple, a choose-one-from-four task will have m =
4. The final predicted Chengyu dj is selected by
argmax(M[:, j]), 1 ≤ j ≤ m.

4 Experiments

4.1 Data and Setting

We crawled 108,987 sentences including 7,395
unique idioms from http://zaojv.com, and
the definitions of these idioms from http://
cy.5156edu.com. Training and test set con-
tain 108,432 and 555 sentences, and 7,071 and
508 Chengyus respectively. We use the whole
Chengyu dataset to train word embeddings. We
perform two tests: (1) cloze test: for each sen-
tence in the test set, we take out the ground-truth
Chengyu, and let the system select a Chengyu

156



TYPE QUERY SYSTEM GROUND TRUTH ANALYSIS

Incorporating 
 Definition

这事已势不可遏，任何想阻挡他
的⼈都如____，简直是不⾃量
⼒。 
This event is unstoppable, anyone 
who tries to stop it will be like ____, 
almost not recognizing his/her own 
limited power.

蚍蜉撼树 
an ant shaking a 

tree, to describe one 
fails to recognize 

one’s limited power

蚍蜉撼树 
an ant shaking a 

tree, to describe one 
fails to recognize 

one’s limited power

The definition significantly enriches 
the semantic meanings of Chengyu 
itself. 蚍蜉撼树(an ant shaking a 
tree) is a metaphor to describe ⾃不
量⼒(fail to recognize one's own 
limited power).

Attention 
Mechanism

刘备思贤若渴，三请诸葛亮的故
事在我国可是____，⼈⼈皆知的
佳话。 
It's ____ well known by everyone in 
our country that Liu Bei was eager 
to recruit talents and invited Zhu 
Geliang three times.

家喻户晓 
well known by 
every family

家喻户晓 
well known by 
every family

By incorporating the attention 
mechanism, our approach can better 
capture the correlations between 
query context and Chengyu 
definition. our approach successfully 
selects 家喻户晓 (well known by 
every family) to fill in the slot since it 
shares similar semantic meanings 
with query context word 知 (known).

World 
Knowledge

村上春树____，29岁才写他的第
⼀部作品。 
Haruki Murakami ____, he was 
already at age 29 when he wrote his 
first works.

画龙点睛 
bring the painted 
dragon to life by 

putting in the pupils 
of its eyes

⼤器晚成 
takes a long time 

to make a great 
instrument

We need to know “age 29” is 
relatively late to produce the first 
works for a writer.

Discourse 
Coherence

⼈们⾯临灾难，不得不____，离
开他们⾃⼰的村落。 
When facing disasters, people had to 
____ and leave their own villages.

逍遥法外 
at large

背井离乡 
leave one’s 
hometown

Our system focused on the shared 
meaning  of escape/leave while 
ignored this Chengyu has a specific 
object “the arm of the law”.

Sentiment 
Analysis

多少⼈认为⼀个作家不仅能妙笔
⽣花，也是____的。 
Many people think that a writer can 
not only write like an angel but also 
____.

⼤⾔不惭 
brag shamelessly

⼜若悬河 
speak eloquently

⼤⾔不惭 (brag shamelessly) 
expresses very negative sentiment 
while ⼜若悬河 (speak eloquently) 
includes positive sentiment.

Negation 
Detection

你在他⾯前说那些话，实在是班
门弄斧，不知____。 
The words you said in front of him 
were really like showing off axe in 
front of Lu Ban, without knowing 
____. 

孤陋寡闻 
with very limited 
knowledge and 

scanty information

天⾼地厚 
high as heaven, 

deep as earth

Our system did not detect negation 
clues and thus failed to select the 
right Chengyu antonyms.

Grammatical 
Structure

写⽂章先要构思好，不要下笔千
⾔，____。 
We should think about the plot 
carefully before write an article, don't 
write down thousands of words, 
____.

词不达意 
the words fail to 

express the meaning

离题万⾥  
get away from the 
title ten thousands 

of miles

When multiple Chengyus appear in 
the same query sentence, they tend 
to follow the same grammatical 
structure.

Rhythm

爱是⼈性的美的⼒量，爱是爱你年
少时的____，更爱你年⽼时⽩发苍
苍。 
Love is the beauty of humanity. To 
love is to love your youthful vigor 
like ____ as well as your gray hair.

意⽓风发 
high-spirited and 

vigorous

桃之夭夭 
the peach trees in 

full blossom

Multiple chengyus tend to appear in 
rhythmical form. In this example, 
“苍苍”(pronunciation: Cāng Cāng) 
and “夭夭”(Yāo Yāo) are both 
reduplication with similar vowel 
pronunciations.

Correct Remaining Challenges

Table 2: Detailed Analysis on Correct Examples and Remaining Challenges

from four candidates consisting of the ground-
truth and three other randomly selected ones to fill
in the slot. (2) coherence checking in college en-
trance exam: we collected 14 problem sets from
(1998, 2000) China college entrance exam, where
each problem set consists of four sentences includ-
ing Chengyus. We let the system select the sen-
tence that contains the most appropriate Chengyu
that fits into the context in a coherent way. For
comparison with human, we asked two Chinese
native speakers (not system developers) who at-
tended top universities in China to perform the
same tests.

4.2 Results and Analysis

Cloze Test Coherence Checking in
College Entrance Exam

Human 70% 42.3%
System 89.5% 35.7%

Table 3: System and Human Accuracy Comparison

Table 3 shows our approach achieves compara-
ble performance as human experts. For 18% of
our system recommended Chengyus which don’t
exactly match the ground truth, they are also ac-
ceptable choices for the given query contexts.

157



For example, our system output “白驹过隙(time
passes quickly like a white pony’s shadow across a
crevice)” and ground truth “光阴似箭(time flies)”
are near synonyms. Table 2 shows some correct
examples and the remaining challenges that re-
quire capabilities beyond lexical semantics.

5 Conclusions and Future Work

We created a new benchmark dataset for a new
task of Chengyu cloze test. We also proposed a
neural model which leverages the definitions of
Chengyu as background knowledge and outper-
forms human experts. In the future we will explore
collective inference to rank multiple Chengyus in
the same discourse simultaneously, and incorpo-
rate richer linguistic clues based on structures and
rhythms.

Acknowledgments

This work was supported by the U.S. DARPA
LORELEI Program No. HR0011-15-C-0115 and
U.S. ARL NS-CTA No. W911NF-09-2-0053. The
views and conclusions contained in this document
are those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Samuel W.K. Chan and Mickey W.C. Chong. 2008. An
agent-based approach to chinese word segmentation.
In Proceedings of the Sixth SIGHAN Workshop on
Chinese Language Processing.

Danqi Chen, Jason Bolton, and Christopher D Man-
ning. 2016. A thorough examination of the
cnn/daily mail reading comprehension task. arXiv
preprint arXiv:1606.02858.

Siaw-Fong Chung. 2009. A corpus-based study on fig-
urative language through the chinese five elements
and body part terms. In Proceedings of Computa-
tional Linguistics and Chinese Language Process-
ing.

Yiming Cui, Ting Liu, Zhipeng Chen, Shijin Wang,
and Guoping Hu. 2016. Consensus attention-based
neural networks for chinese reading comprehension.
arXiv preprint arXiv:1607.02250.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and
Jan Kleindienst. 2016. Text understanding with
the attention sum reader network. arXiv preprint
arXiv:1603.01547.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
flow for machine comprehension. arXiv preprint
arXiv:1611.01603.

Ekaterina Shutova. 2010. Models of metaphor in nlp.
In Proceedings of the 48th annual meeting of the as-
sociation for computational linguistics, pages 688–
697. Association for Computational Linguistics.

Weiwei Sun and Jia Xu. 2011. Enhancing chinese word
segmentation using unlabeled data. In Proceedings
of the 2011 Conference on Empirical Methods in
Natural Language Processing.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman,
Eric Nyberg, and Chris Dyer. 2014. Metaphor detec-
tion with cross-lingual model transfer. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 248–258.

Chunqi Wang and Bo Xu. 2017. Convolutional neu-
ral network with word embeddings for chinese word
segmentation. In Proceedings of the The 8th In-
ternational Joint Conference on Natural Language
Processing.

Ge Xu, Xinfan Meng, and Houfeng Wang. 2010. Build
chinese emotion lexicons using a graph-based al-
gorithm and multiple resources. In Proceedings
of the 23rd International Conference on Computa-

tional Linguistics (COLING 2010).

158


