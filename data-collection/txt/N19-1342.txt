




































CAN-NER: Convolutional Attention Network for Chinese Named Entity Recognition


Proceedings of NAACL-HLT 2019, pages 3384–3393
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3384

CAN-NER: Convolutional Attention Network for
Chinese Named Entity Recognition

Yuying Zhu∗
Nankai University

Tianjin, China
yuyzhu@mail.nankai.edu.cn

Guoxin Wang
Microsoft Research Asia

Beijing, China
guow@microsoft.com

Abstract

Named entity recognition (NER) is a common
task in Natural Language Processing (NLP),
but it remains more challenging in Chinese be-
cause of its lack of natural delimiters. There-
fore, Chinese Word Segmentation (CWS) is
usually necessary as the first step for Chi-
nese NER. However, models based on word-
level embeddings and lexicon features often
suffer from segmentation errors and out-of-
vocabulary (OOV) problems. In this paper, we
investigate a Convolutional Attention Network
(CAN) for Chinese NER, which consists of a
character-based convolutional neural network
(CNN) with local-attention layer and a gated
recurrent unit (GRU) with global self-attention
layer to capture the information from adjacent
characters and sentence contexts. Moreover,
differently from other approaches, CAN-NER
does not depend on any external resources like
lexicons and employing small-size char em-
beddings makes CAN-NER more practical for
real systems scenarios. Extensive experimen-
tal results show that our approach outperforms
state-of-the-art methods without word embed-
ding and external lexicon resources on differ-
ent domains datasets.

1 Introduction

Named Entity Recognition (NER) aims at identi-
fying text spans which are associated with a spe-
cific semantic entity type such as person (PER),
organization (ORG), location (LOC), and geopo-
litical entity (GPE). NER has received constant
research attention as it is the first step in a
wide range of downstream Natural Language Pro-
cessing (NLP) tasks, e.g., entity linking (Gupta
et al., 2017), relation extraction (Miwa and Bansal,
2016), event extraction (Chen et al., 2015), and co-
reference resolution (Fragkou, 2017). The stan-
dard approach in existing state-of-the-art models

∗ This work was performed when the first author was an
intern at Microsoft Research Asia.

for English NER treats the problem as a word-by-
word sequence labeling task and makes full use of
the Recurrent Neural Network (RNN) and Condi-
tional Random Field (CRF) to capture context in-
formation at the word level (Lample et al., 2016;
Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu
et al., 2018). These models for English NER pre-

Sentence:
南京市长江大桥

Segmentation 1:
南京市 |长江大桥
Nanjing City, Yangtze River Bridge
Location, Location

Segmentation 2:
南京 |市长 |江大桥
Nanjing, Mayor, Jiang Daqiao
Location, Title, Person

Figure 1: Entity Ambiguity with Word Segmentation.

dict a tag for each word assuming that words can
be separated clearly by explicit word separators,
e.g., blank spaces. As the Chinese language has
no natural delimiters, it would be intuitive to ap-
ply Chinese Word Segmentation (CWS) first to
get word boundaries and then use a word-level se-
quence labeling model similar to the English NER
models. However, word boundaries can be am-
biguous in Chinese, which leads to the possibility
that entity boundaries do not match word bound-
aries. For example, the term “西藏自治区 (Ti-
bet Autonomous Region)” is a GPE-type entity
in NER, but it could be segmented as a single
word or as two words “西藏 (Tibet)” and “自
治区 (autonomous region)” separately, depend-
ing on different granularity of segmentation tools.
Most of the time, however, it is hard to deter-
mine the correct granularity for word segmenta-
tion. Also, as shown in Figure 1, different seg-
mentation can lead to different sentence meanings
in Chinese, which could even result in different
named entities. Obviously, if entity boundaries are



3385

mistakenly detected in segmentation, it will neg-
atively affect entity tagging in word-based NER
models. Furthermore, most recent neural network-
based Chinese NER models rely heavily on word-
level embeddings and external lexicon sets (Huang
et al., 2017; Zhang and Yang, 2018). The qual-
ity of such models strongly relies on the differ-
ent word embedding representations and lexicon
features. Moreover, word-based models tend to
suffer from OOV issues as Chinese words can be
very diverse and named entities are an important
source of OOV words. Other potential limitations
are as follows: (1) Dependency on word embed-
dings increases model size and makes the fine-
tuning process more costly during training (while
negatively affecting latency in testing/decoding);
(2) It is hard to learn word representation correctly
without enough labeled utterances for named enti-
ties are usually rarer proper nouns. (3) Large lexi-
cons are very costly for real NER systems as they
greatly increase memory usage and latency in fea-
ture extraction (matching), which makes models
inefficient; (4) It is very costly to remove noise
from large lexicons and any update to pre-trained
word embeddings or lexicons requires model re-
training. Meanwhile, character-level embedding
by itself can only carry limited information due
to losing word and word-sequence information.
For instance, the character “拍” in words “球
拍” (bat) and “拍卖” (auction) has very different
meanings. How to better integrate segmentation-
related information and exploit local context in-
formation is the key feature in a character-based
model. Zhang and Yang (2018) leverage lexicons
to add all the embeddings of candidate word seg-
mentation to their last character embeddings as
soft features, and construct a convolutional neural
network (CNN) to encode characters as word-level
information. Cao et al. (2018) propose a multi-
task architecture to learn NER tagging and Chi-
nese word segmentation together, with each part
using a character-based Bi-LSTM. In this paper,
we propose a convolutional attention layer to cap-
ture the implicit relations within adjacent char-
acters, in which the position features from word
segmentation are soft hints for character combi-
nations. With the segmentation vector softly con-
catenating into character embedding, the convolu-
tional attention layer is able to group implicitly
meaning-related characters and help bypass the
impact of segmentation errors. A BiGRU structure

with a global self-attention layer on the whole sen-
tence is utilized to capture sentence-level depen-
dencies. Extensive experimental results show that
our approach outperforms state-of-the-art methods
without relying on external resources (e.g. word
embedding, external lexicon) across different cor-
pora. The main contributions of this paper can be
summarized as follows:

• We first combine CNNs with the local-
attention mechanism to enhance the abil-
ity of the model to capture implicitly local
context relations among character sequences.
Compared with experimental results against a
baseline with a regular CNN layer, our Con-
volutional Attention layer leads to substantial
performance improvements.

• We introduce a character-based Chinese NER
model that consists of combined CNN with
local attention and BiGRU with global self-
attention layers. Our model achieves state-
of-the-art F1-scores without using any exter-
nal resources like word embeddings and lex-
icon resources, which make it very practical
for real-world NER systems.

2 Methodology

We utilize BiGRU-CRF as our basic model struc-
ture. Our model considers multi-level context fea-
tures in three layers: i) convolutional attention
layer, ii) GRU layer, and iii) global attention layer.
The whole architecture of our proposed model is
illustrated in Figure 2.

2.1 Formulation

In the Chinese NER task, we denote an in-
put sentence as Xi = {xi,1, xi,2, xi,3, ..., xi,τ},
where xi,τ ∈ Rde represents the τ -th char-
acter in sentence Xi and de is the dimension
of the input embeddings. Correspondingly, we
denote the sentence label sequence as Yi =
{yi,1, yi,2, yi,3, ..., yi,τ}, where yi,τ ∈ Y belongs
to the set of all possible labels. The objective is
learning a function fθ : X 7→ Y to obtain the en-
tity types including the ‘O’ type for all the charac-
ters in the input text. In the following text, we take
one instance as the example and therefore omit
subindex i in the formula.



3386

Char
Feature

Embedding

Input

p 
a 
d 
d 
i 
n 
g 

p 
a 
d 
d 
i 
n 
g 

p 
a 
d 
d 
i 
n 
g 

p 
a 
d 
d 
i 
n 
g 

 Convolution 
Attention 

Layer 

...

...

GRU
Layer

...GlobalAttention
Layer

CRF
Layer

北 京 欢 迎 您

Tag B-LOC O O

...

⊗

a1 a2 a4 a5a3

ConvAtt
Unit

ConvAtt
Unit

ConvAtt
Unit

Beijing welcomes you

...

...

...

...

Figure 2: Overall model architecture. A convolutional
attention layer is constructed to encode both character-
and word-level information. The BiGRU-CRF layer is
extended by a global self-attention layer to capture long
sequential sentence-level relations.

2.2 Convolutional Attention Layer

The convolutional attention layer aims to encode
the sequence of input characters and implicitly
group meaning-related characters in the local con-
text. The input representation for each character is
constructed as x = [xch;xseg], where xch ∈ Rdch
and xseg ∈ Rdseg are character embedding and
segmentation mask, respectively. The segmen-
tation information is encoded by BMES scheme
(Wang and Xu, 2017).

For every window in the CNN, whose window
size is k, we first concatenate a position embed-
ding to each character embedding, helping to keep
sequential relations in the local window context.
The dimension of the position embedding equals
to the window size k with the initial values of
1 at the position where the character lies in the
window and 0 at other positions. So, the di-
mension of the concatenated embedding is de =

dch + dpos + dseg. We then apply local atten-
tion inside the window to capture the relations be-
tween the center character and each context to-
ken, followed by a CNN with sum-pooling layer.
We set the hidden dimension as dh. For the j-th
character, the local attention takes all the concate-
nated embeddings xj− k−1

2
, ...xj , ..., xj+ k−1

2
in the

window as its input and outputs k hidden vectors
hj− k−1

2
, ..., hj , ..., hj+ k−1

2
. The hidden vectors are

calculated as follows:

hm = αmxm, (1)

where m ∈ {j − k−12 , ..., j +
k−1
2 } and αm is the

attention weight, which is calculated as:

αm =
exp s(xj , xm)∑

n∈{j− k−1
2
,...,j+ k−1

2
} exp s(xj , xn)

. (2)

The score function s is defined as follows:

s(xj , xk) = v
> tanh(W1xj +W2xk), (3)

where v ∈ Rdh and W1,W2 ∈ Rdh,de .
The CNN layer contains dh kernels on a context

window of k tokens as:

hcj =
∑
k

[W c ∗ hj− k−1
2

:j+ k−1
2

+ bc], (4)

where W c ∈ Rk×dh×de and bc ∈ Rk×dh . The
∗ operation denotes element-wise product and
hj− k−1

2
:j+ k−1

2
means a concatenation of the hid-

den states hj− k−1
2
, ..., hj+ k−1

2
, both of which are

calculated at the first dimension. Finally sum-
pooling is also conducted on the first dimension.

2.3 BiGRU-CRF with Global Attention
After extracting the local context features by the
convolutional attention layer, we feed them into a
BiGRU-CRF based model to predict final label for
each character. This layer models the sequential
sentence information and it is calculated as fol-
lows:

hrj = BiGRU(h
r
j−1, h

c
j ;W

r, U r), (5)

where hcj is the output of the convolutional atten-
tion layer, hrj−1 is the previous hidden state for the
BiGRU layer, and W r, U r ∈ Rdh×dh are its pa-
rameters.

A global self-attention layer is utilized to better
handle sentence-level information, as:

hgj =

n∑
s=1

αgj,sh
r
s (6)



3387

where j = 1, ..., τ denotes all characters in a sen-
tence instance and αgj,s is calculated as:

αgj,s =
exp s(hrj , h

r
s)∑

n∈{1,...,τ} exp s(h
r
j , h

r
n)
. (7)

The score function s is similar to Equation 3 with
different parameters vg ∈ Rdh and W g1 ,W

g
2 ∈

Rdh,dh instead.
Finally, a standard CRF layer is used at the top

of the concatenation of the output of the BiGRU
and global attention layers, which is denoted as
Hτ = [h

r
τ ;h

g
τ ]. Given the predicted tag sequence

Y = {y1, y2, y3, ..., yτ}, the probability of the
ground-truth label sequence is computed by:

P (Y|X) =
exp(

∑
i(W

yi
CRFHi + b

(yi−1,yi)
CRF ))∑

y′ exp(
∑

i(W
y′i
CRFHi + b

(y′i−1,y
′
i)

CRF ))
,

(8)
where y′ denotes an arbitrary label sequence,
WyiCRF and b

(yi−1,yi)
CRF are trainable parameters. In

decoding, we use the Viterbi algorithm to get the
predicted tag sequence.

2.4 Training
For training, we exploit log-likelihood objective as
the loss function. Given a set of training examples
{(Xi,Yi)}|Ki=1, the loss function L can be defined
as follows:

L =
K∑
i=1

logP (Yi|Xi) (9)

In the training phase, at each iteration, we first
shuffle all the training instances, and then feed
them to the model with batch updates. We use the
AdaDelta (Zeiler, 2012) algorithm to optimize the
final objective with all the parameters as described
in Section 3.1.

Dataset Type Train Test Dev

OntoNotes
Sentences 15.7k 4.3k 4.3k

Chars 491.9k 208.1k 200.5k
Entities 13.4k 7.7k 6.95k

MSRA
Sentences 46.4k 4.4k -

Chars 2169.9k 172.6k -
Entities 74.8k 6.2k -

Weibo
Sentences 1.4k 0.27k 0.27k

Chars 73.8k 14.8k 14.5k
Entities 1.89k 0.42k 0.39k

Resume
Sentences 3.8k 0.48k 0.46k

Chars 124.1k 15.1k 13.9k
Entities 1.34k 0.15k 0.16k

Table 1: Statistics of each dataset

3 Experiments

To demonstrate the effectiveness of our proposed
model, we have run multiple experiments on Chi-
nese NER datasets covering different domains.
This section describes the details of each dataset,
settings, and results in our experiments. Standard
precision (P), recall (R) and F1-score (F1) are used
as evaluation metrics.

3.1 Experimental Settings

Data We use four datasets in our experiments. For
the news domain, we experiment on OntoNotes 4
(Weischedel et al., 2011) and MSRA NER dataset
from SIGHAN Bakeoff 2006 (Levow, 2006). For
the social media domain, we adopt the same an-
notated Weibo corpus as Peng and Dredze (2015)
which is extracted from Sina Weibo1. For more
variety in test domains, we also use a Chinese Re-
sume dataset (Zhang and Yang, 2018) collected
from Sina Finance2.

The Weibo dataset is annotated with four en-
tity types: PER (Person), ORG (Organization),
LOC (Location), and GPE (Geo-Political Entity);
and it includes both named and nominal mentions.
This corpus is already divided into training, de-
velopment, and test sets. The Chinese Resume
dataset is annotated with eight types of named en-
tities: CONT (Country), EDU (Educational In-
stitution), LOC, PER, ORG, PRO (Profession),
RACE (Ethnicity/Background), and TITLE (Job
Title). OntoNotes 4 is annotated with four named
entity categories: PER, ORG, LOC, and GPE. We
follow the same data split method of Che et al.
(2013) over OntoNotes 4. Lastly, the MSRA 2006
dataset contains three annotated named entities:
ORG, PER and LOC. A development subset is

Models NE NM Overall
Peng and Dredze (2015) 51.96 61.05 56.05
Peng and Dredze (2016)∗ 55.28 62.97 58.99

He and Sun (2017a) 50.60 59.32 54.82
He and Sun (2017b)∗ 54.50 62.17 58.23

Cao et al. (2018) 54.34 57.35 58.70
Zhang and Yang (2018) 53.04 62.25 58.79

Baseline 49.02 58.80 53.80
Baseline + CNN 53.86 58.05 55.91

CAN-NER Model 55.38 62.98 59.31

Table 2: Weibo NER results

1http://www.weibo.com/
2http://finance.sina.com.cn/stock/index.html



3388

not available for the MSRA dataset. The detailed
statistics of each datasets are shown in Table 1.

Gold segmentation is unavailable for Weibo,
Chinese Resume, and MSRA test sections. We
follow Zhang and Yang (2018) to automatically
segment these by using the model described in
Yang et al. (2017). We treat NER as a sequen-
tial labeling problem and adopt the BIOES tagging
style since it has been shown to produce better re-
sults than straight BIO (Yang et al., 2018b).

Hyper-parameter settings For hyper-
parameter configuration, we adjust them ac-
cording to the performance on the described
development sets for Chinese NER. We set
the character embedding size, hidden sizes of
CNN and BiGRU to 300 dims. After comparing
experimental results with different CNN window
sizes, we set the window size as 5. Adadelta is
used for optimization, with an initial learning rate
of 0.005. The character embeddings used in our
experiments are from Li et al. (2018), which is
trained by Skip-Gram with Negative Sampling
(SGNS) on Baidu Encyclopedia.

3.2 Experimental Results

In this section, we describe the experimental re-
sults of our proposed model and previous state-of-
the-art methods on four datasets: Weibo, Chinese
Resume, OntoNotes 4, and MSRA. We propose
two baselines for comparison, and show the CAN-
NER model results. In the experiment results ta-
ble, we use Baseline to represent a pure BiGRU +
CRF model; and Baseline + CNN to indicate the
base model with a CNN layer.

3.2.1 Weibo Dataset

Here we compare our proposed model with the lat-
est models on the Weibo dataset.3 Table 2 shows
the F1-scores for named entities (NE), nominal en-
tities (NM, excluding named entities), and both
(Overall). We observe that our proposed model
achieves state-of-the-art performance.

Existing state-of-the-art systems include Peng
and Dredze (2016), He and Sun (2017b), Cao
et al. (2018) and Zhang and Yang (2018), which
leverage rich external data like cross-domain data,
semi-supervised data, and lexicons, or joint-train

3In Table 2,3, 4 and 5, we use ∗ to denote a model with
external labeled data for semi-supervised learning. † denotes
that the model use external lexicon data. Zhang and Yang
(2018) with ‡ is the char-based model in the paper.

NER and Chinese Word Segmentation (CWS).4

In the first block of Table 2, we report the per-
formance of the latest models. Peng and Dredze
(2015) propose a model that jointly trains em-
beddings with NER and it achieves a F1-score of
56.05% on overall performance. The model (Peng
and Dredze, 2016) that jointly trains NER and
CWS reaches a F1-score of 58.99%. He and Sun
(2017b) propose a unified model to exploit cross-
domain and semi-supervised data, which improves
the F1-score from 54.82% to 58.23% compared
with the model proposed by He and Sun (2017a).
Cao et al. (2018) use an adversarial transfer learn-
ing framework to incorporate task-shared word
boundary information from CWS and achieves a
F1-score of 58.70%. Zhang and Yang (2018)
leverage a lattice structure to integrate lexicon in-
formation into their model and achieve a F1-score
of 58.79%.

In the second block of Table 2, we give the re-
sults of our baselines and proposed models. While
the BiGRU + CRF baseline only achieves a F1-
score of 53.80%, adding a normal CNN layer as
featurizer improves the score to 55.91%. Re-
placing the CNN with our convolutional attention
layer greatly improves the F1-score to 59.31%,
which outperforms other models. The improve-
ment demonstrates the effectiveness of our pro-
posed model.

Models P R F1
Zhang and Yang (2018)1† 94.53 94.29 94.41
Zhang and Yang (2018)2‡ 94.07 94.42 94.24
Zhang and Yang (2018)3 94.81 94.11 94.46

Baseline 93.71 93.74 93.73
Baseline + CNN 94.36 94.85 94.60

CAN-NER Model 95.05 94.82 94.94

Table 3: Results on Chinese Resume Dataset. For
models proposed by Zhang and Yang (2018), 1 rep-
resents the char-based LSTM model, 2 indicates the
word-based LSTM model and 3 is the Lattice model.

3.2.2 Chinese Resume Dataset
The Chinese Resume test results are shown in Ta-
ble 3. Zhang and Yang (2018) released the Chi-
nese Resume dataset and they achieve a F1-score
of 94.46%. It can be seen that our proposed
baseline (CNN + BiGRU + CRF) outperforms
Zhang and Yang (2018) with F1-score of 94.60%.

4The results of Peng and Dredze (2015, 2016) are taken
from Peng and Dredze (2017)



3389

Models P R F1
Yang et al. (2016) 65.59 71.84 68.57
Yang et al. (2016)∗ 72.98 80.15 76.40
Che et al. (2013)∗ 77.71 72.51 75.02

Wang et al. (2013)∗ 76.43 72.32 74.32
Zhang and Yang (2018)† 76.35 71.56 73.88
Zhang and Yang (2018)‡ 74.36 69.43 71.81

Baseline 70.67 71.64 71.15
Baseline + CNN 72.69 71.51 72.10

CAN-NER Model 75.05 72.29 73.64

Table 4: Results on OntoNotes

Adding our convolutional attention leads a fur-
ther improvement and achieves state-of-the-art F1-
score of 94.94%, which further demonstrates the
effectiveness of our proposed model.

3.2.3 OntoNotes Dataset
Table 4 shows comparisons on the OntoNotes 4
dataset. The first block in the table lists the per-
formance of previous methods for Chinese NER.
Yang et al. (2016) propose a model combining
neural and discrete feature, e.g., POS tagging fea-
tures, CWS features and orthographic features,
improving the F1-score from 68.57% to 76.40%.
Leveraging bilingual data, Che et al. (2013) and
Wang et al. (2013) achieves F1-scores of 74.32%
and 73.88% respectively. Zhang and Yang (2018)‡

is a recent model that uses a character-based model
with bichar and softword.

The second block of Table 4 shows the results
of our baselines and proposed model. Consis-
tently with observations on the Weibo and Resume
datasets, our Convolutional Attention layer leads
to a substantial increment on F1-score. Our pro-
posed model achieves a competitive F1-score of
73.64% among character-based model without us-
ing external data (e.g., Zhang and Yang (2018)‡).

3.2.4 MSRA Dataset
Table 5 shows experiment results on the MSRA
2006 dataset. Chen et al. (2006), Zhang et al.
(2006), and Zhou et al. (2013) leverage rich hand-
crafted features and Lu et al. (2016) exploit multi-
prototype embedding features. Dong et al. (2016)
introduce radical features into LSTM-CRF. Cao
et al. (2018) make use of Adversarial Trans-
fer Learning and global self-attention to improve
model performance. Yang et al. (2018a) propose
a character-based CNN-BiLSTM-CRF model to
incorporate stroke embeddings and generate n-

Models P R F1
Chen et al. (2006) 91.22 81.71 86.20

Zhang et al. (2006)∗ 92.20 90.18 91.18
Zhou et al. (2013) 91.86 88.75 90.28
Lu et al. (2016) - - 87.94

Dong et al. (2016) 91.28 90.62 90.95
Cao et al. (2018) 91.30 89.58 90.64

Yang et al. (2018a) 92.04 91.31 91.67
Zhang and Yang (2018)† 93.57 92.79 93.18

Baseline 92.54 88.20 90.32
Baseline + CNN 92.57 92.11 92.34

CAN-NER Model 93.53 92.42 92.97

Table 5: Results on MSRA dataset

gram features. Zhang and Yang (2018) introduce
a lattice structure to incorporate lexicon informa-
tion into the neural network, which actually in-
cludes word embedding information. Although
this model achieves state-of-the-art F1-score at
93.18%, it leverages external lexicon data and thus
the result is dependent on the quality of the lexi-
con. At the bottom section of the table, we can
see that Baseline + CNN already outperforms most
previous methods. Compared with Zhang and
Yang (2018), our char-based method achieves a
competitive F1-score of 92.97% without any addi-
tional lexicon data and word embedding informa-
tion. Moreover, CAN-NER model achieves state-
of-the-art result among the character-based mod-
els.

3.3 Discussion
This section discusses the model effectiveness and
the experimental results.

3.3.1 Effectiveness of Convolutional
Attention and Global Self-Attention

As shown in Tables 2, 3, and 5, our proposed
model’s performance demonstrates the effective-
ness of the Convolutional Attention Network.
To better evaluate the effect of the Attention
Mechanism, we visualize the normalized attention
weights αlm for each window from Eq. 2, as in
Figure 3a. Each row of the matrix represents lo-
cation attention weights in each window. For ex-
ample, the third row indicates that the relationship
between center character “总” and contexts “美国
总统克”. We can see from the Figure 3a that the
word-level features can be extracted through the
local attention. In the context, the center charac-
ter “美” tends to have a stronger connection with



3390

American president  Clinton on the 1st leave for Europe

(a) Local attention.

American president  Clinton on the 1st leave for Europe

(b) Global self-attention.

Figure 3: Attention visualization. The left-side image shows the normalized Convolutional Attention weights in
each window in a sentence. The right-side indicates the global self-attention weights for the whole sentence. In
both pictures, the x-axis represents context, while the y-axis represents the input query in the attention mechanism.

its related character “国”, which means they have
a higher probability of forming the Chinese word
“美国 (American)”. Also for characters “克”,
“林”, and “顿”, they tend to have a strong connec-
tion because “克林顿” means “Clinton”. Charac-
ters “欧” and “洲” also have strong connections,
as seen in Figure 3a, because “欧洲” represents
“Europe” in Chinese. Therefore, both experiment
results and visualization verifies that the Convo-
lutional Attention is effective in obtaining phrase-
level information between adjacent characters.

In Figure 3b, we visualize the global self-
attention matrix. From the picture, we can find
that global self-attention can capture the sentence
context information from the long-distance rela-
tionship of words to overcome the limitation of
Recurrent Neural Networks. For the word “克林
顿 (Clinton)”, the global self-attention learns the
dependencies with “前往 (leave for)” and “1号
(on the 1st)”. Distinguished by the red color, “克
林顿 (Clinton)” has a stronger connection with
“前往 (leave for)” than with “1号 (on the 1st)”,
which matches the expectation that the predicate
in a sentence provides more information to the
subject than adverbs of time.

3.3.2 Results Analysis

Our proposed model outperforms previous work
on th eWeibo and Chinese Resume datasets and
reaches competitive results on both MSRA and
OntoNotes 4 datasets without using any exter-
nal resources. The experiments results demon-
strate the effectiveness of our proposed model, es-

pecially among char-based models. The perfor-
mance improvement after adding Convolutional
Attention Layer and Global Attention Layer ver-
ifies that our model can capture the relationship
between character and its local context, as well as
the relationship between word and global context.
However, although we can obtain comparable or
better results to other models that utilize no ex-
ternal resources, we find that our model perfor-
mance on the OntoNotes 4 dataset still has room
for improvement (2.76% F1-score gap to the best
model that leverages additional data). This may be
explained by specific discrete features and exter-
nal resources (e.g., other labeled data or lexicons)
having a more positive influence on this specific
dataset, while CAN-NER cannot learn enough in-
formation from only the training set. However, we
were not able to identify the precise contributors
to the gap based on the available corresponding
resources.

4 Related Work

4.1 Neural Network Models

Neural networks, such as LSTM and CNN, have
been shown to outperform conventional machine
learning methods without requiring handcrafted
features. Collobert et al. (2011) describe a CNN-
CRF model that reaches competitive results com-
pared to the best statistical models at the time.
More recently, the LSTM-CRF architecture has
become a quasi-standard on NER tasks. Huang
et al. (2015) employed BiLSTM to extract word-



3391

level context information and Lample et al. (2016)
further introduced a hierarchy structure by incor-
porating BiLSTM-based character embeddings.
Multiple recent works integrating word-level in-
formation and character-level information have
been found to achieve improved performance (dos
Santos et al., 2015; Chiu and Nichols, 2016; Ma
and Hovy, 2016; Lample et al., 2016; Chen et al.,
2019). Moreover, external knowledge has also
been exploited for NER, as has character-level
knowledge, both pre-trained (Peters et al., 2017)
and co-trained (Liu et al., 2018). More recently,
large-scale pre-trained language representations
with deep language models have been proposed
to help improve the performance of downstream
NLP tasks. E.g., ELMo (Peters et al., 2018) and
BERT (Devlin et al., 2018).

4.2 Attention Mechanism
Also, Attention Mechanisms have shown very
good performance on a variety of tasks includ-
ing machine translation, machine comprehension,
and related NLP tasks (Vaswani et al., 2017; Seo
et al., 2016; Tan et al., 2018a). In language under-
standing, Shen et al. (2018) exploit self-attention
to learn long range dependencies. Rei et al. (2016)
proposed a model employing an attention mech-
anism to combine the character-based represen-
tation with the word embedding instead of sim-
ply concatenating them. This method allows the
model to dynamically decide which source of in-
formation to use for each word, and therefore out-
performing the concatenation method used in pre-
vious work. More recently, Tan et al. (2018b) and
Cao et al. (2018) employ self-attention to directly
capture the global dependencies of the inputs for
NER tasks and demonstrate the effectiveness of
self-attention in Chinese NER.

4.3 Chinese NER
Multiple previous efforts have tried to address the
Chinese language challenge of not having explicit
word boundaries. Traditional models depended
on hand-crafted features and CRFs-based mod-
els (He and Wang, 2008; Mao et al., 2008) and
character-based LSTM-CRF models have been ap-
plied to Chinese NER to utilize both character- and
radical-level representations (Dong et al., 2016).
Peng and Dredze (2015) applied character posi-
tional embeddings and proposed a jointly trained
model for embeddings and NER. To better in-
tegrate word boundary information into Chinese

NER model, Peng and Dredze (2016) co-trained
NER and word segmentation to improve perfor-
mance in both tasks. He and Sun (2017b) uni-
fied cross-domain learning and semi-supervised
learning to obtain information from out-of-domain
corpora and in-domain unannotated text. In-
stead of performing word segmentation first, Re-
cently, Zhang and Yang (2018) proposed con-
structing a word-character lattice by matching
words in texts with a lexicon to avoid segmen-
tation errors. Cao et al. (2018) use an adver-
sarial network to jointly train Chinese NER task
and Chinese Word Segmentation tasks to extract
task-shared word boundary information. Also,
Yang et al. (2018c) leverage character-level BiL-
STM to extract higher-level features from crowd-
annotations.

5 Conclusion

In this paper, we propose CAN-NER, a Convolu-
tional Attention Network model to improve Chi-
nese NER performance and preclude word em-
bedding and additional lexicon dependencies; thus
making the model more efficient and robust. In our
model, we implement local-attention CNN and Bi-
GRU with the global self-attention structure to
capture word-level features and context informa-
tion with char-level features. Extensive experi-
ments show that our model outperforms the state-
of-art systems on the different domain datasets.

Acknowledgements

We’d like to thank our colleague Börje Karlsson
for his contribution and support in this work, as
well as thank our colleagues Haoyan Liu, Zijia
Lin, and the anonymous reviewers for their valu-
able feedback.

References
Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, and

Shengping Liu. 2018. Adversarial transfer learn-
ing for chinese named entity recognition with self-
attention mechanism. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 182–192.

Wanxiang Che, Mengqiu Wang, Christopher D Man-
ning, and Ting Liu. 2013. Named entity recogni-
tion with bilingual constraints. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 52–62.



3392

Aitao Chen, Fuchun Peng, Roy Shan, and Gordon Sun.
2006. Chinese named entity recognition with con-
ditional probabilistic models. In Proceedings of the
Fifth SIGHAN Workshop on Chinese Language Pro-
cessing, pages 173–176.

Hui Chen, Zijia Lin, Guiguang Ding, Jianguang Lou,
Yusen Zhang, and Börje F. Karlsson. 2019. GRN:
Gated relation network to enhance convolutional
neural network for named entity recognition. In Pro-
ceedings of AAAI 2019.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,
and Jun Zhao. 2015. Event extraction via dy-
namic multi-pooling convolutional neural networks.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing, volume 1, pages 167–176.

Jason Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional lstm-cnns. Transac-
tions of the Association of Computational Linguis-
tics, 4(1):357–370.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Chuanhai Dong, Jiajun Zhang, Chengqing Zong,
Masanori Hattori, and Hui Di. 2016. Character-
based lstm-crf with radical-level features for chi-
nese named entity recognition. In Natural Language
Understanding and Intelligent Applications, pages
239–250. Springer.

Pavlina Fragkou. 2017. Applying named entity recog-
nition and co-reference resolution for segmenting
english texts. Progress in Artificial Intelligence,
6(4):325–346.

Nitish Gupta, Sameer Singh, and Dan Roth. 2017. En-
tity linking via joint encoding of types, descriptions,
and context. In Proceedings of the 2017 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 2681–2690.

Hangfeng He and Xu Sun. 2017a. F-score driven max
margin neural network for named entity recognition
in chinese social media. In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, volume 2, pages
713–718.

Hangfeng He and Xu Sun. 2017b. A unified model
for cross-domain and semi-supervised named entity
recognition in chinese social media. In AAAI Con-
ference on Artificial Intelligence, pages 3216–3222.

Jingzhou He and Houfeng Wang. 2008. Chinese
named entity recognition and word segmentation
based on character. In Proceedings of the Sixth
SIGHAN Workshop on Chinese Language Process-
ing.

Shen Huang, Xu Sun, and Houfeng Wang. 2017. Ad-
dressing domain adaptation for chinese word seg-
mentation with global recurrent structure. In Pro-
ceedings of the Eighth International Joint Confer-
ence on Natural Language Processing, volume 1,
pages 184–193.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-
tional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of NAACL-HLT, pages 260–270.

Gina-Anne Levow. 2006. The third international chi-
nese language processing bakeoff: Word segmen-
tation and named entity recognition. In Proceed-
ings of the Fifth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 108–117.

Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, and
Xiaoyong Du. 2018. Analogical reasoning on chi-
nese morphological and semantic relations. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers). Association for Computational Linguistics.

Liyuan Liu, Jingbo Shang, Xiang Ren, Frank Xu, Huan
Gui, Jian Peng, and Jiawei Han. 2018. Empower
sequence labeling with task-aware neural language
model.

Yanan Lu, Yue Zhang, and Dong-Hong Ji. 2016. Multi-
prototype chinese character embedding. In LREC.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics, volume 1,
pages 1064–1074.

Xinnian Mao, Yuan Dong, Saike He, Sencheng Bao,
and Haila Wang. 2008. Chinese word segmentation
and named entity recognition based on conditional
random fields. In Proceedings of the Sixth SIGHAN
Workshop on Chinese Language Processing.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using lstms on sequences and tree
structures. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguis-
tics, volume 1, pages 1105–1116.

Nanyun Peng and Mark Dredze. 2015. Named en-
tity recognition for chinese social media with jointly
trained embeddings. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 548–554.



3393

Nanyun Peng and Mark Dredze. 2016. Improving
named entity recognition for chinese social media
with word segmentation representation learning. In
The 54th Annual Meeting of the Association for
Computational Linguistics, page 149.

Nanyun Peng and Mark Dredze. 2017. Supplementary
results for named entity recognition on chinese so-
cial media with an updated dataset. Technical re-
port.

Matthew Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language models.
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics, volume 1,
pages 1756–1765.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, volume 1, pages 2227–2237.

Marek Rei, Gamal Crichton, and Sampo Pyysalo. 2016.
Attending to characters in neural sequence label-
ing models. In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 309–318.

Cıcero dos Santos, Victor Guimaraes, RJ Niterói, and
Rio de Janeiro. 2015. Boosting named entity recog-
nition with neural character embeddings. In Pro-
ceedings of NEWS 2015 The Fifth Named Entities
Workshop, page 25.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
flow for machine comprehension. arXiv preprint
arXiv:1611.01603.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2018. Disan: Di-
rectional self-attention network for rnn/cnn-free lan-
guage understanding. In Thirty-Second AAAI Con-
ference on Artificial Intelligence.

Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen,
and Xiaodong Shi. 2018a. Deep semantic role la-
beling with self-attention. In AAAI Conference on
Artificial Intelligence.

Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong Chen,
and Xiaodong Shi. 2018b. Deep semantic role la-
beling with self-attention. In AAAI Conference on
Artificial Intelligence.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Chunqi Wang and Bo Xu. 2017. Convolutional neu-
ral network with word embeddings for chinese word
segmentation. In Proceedings of the Eighth Interna-
tional Joint Conference on Natural Language Pro-
cessing, volume 1, pages 163–172.

Mengqiu Wang, Wanxiang Che, and Christopher D
Manning. 2013. Effective bilingual constraints for
semi-supervised learning of named entity recogniz-
ers. In AAAI Conference on Artificial Intelligence.

Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus,
Ann Taylor, Craig Greenberg, Eduard Hovy, Robert
Belvin, et al. 2011. Ontonotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.

Fan Yang, Jianhu Zhang, Gongshen Liu, Jie Zhou,
Cheng Zhou, and Huanrong Sun. 2018a. Five-stroke
based cnn-birnn-crf network for chinese named en-
tity recognition. In CCF International Conference
on Natural Language Processing and Chinese Com-
puting, pages 184–195. Springer.

Jie Yang, Shuailong Liang, and Yue Zhang. 2018b.
Design challenges and misconceptions in neural se-
quence labeling. In Proceedings of the 27th Inter-
national Conference on Computational Linguistics,
pages 3879–3889.

Jie Yang, Zhiyang Teng, Meishan Zhang, and Yue
Zhang. 2016. Combining discrete and neural fea-
tures for sequence labeling. In CICLing.

Jie Yang, Yue Zhang, and Fei Dong. 2017. Neural
word segmentation with rich pretraining. In Pro-
ceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics, volume 1, pages
839–849.

YaoSheng Yang, Meishan Zhang, Wenliang Chen, Wei
Zhang, Haofen Wang, and Min Zhang. 2018c. Ad-
versarial learning for chinese ner from crowd an-
notations. In AAAI Conference on Artificial Intel-
ligence.

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701.

Suxiang Zhang, Ying Qin, Juan Wen, and Xiaojie
Wang. 2006. Word segmentation and named entity
recognition for sighan bakeoff3. In Proceedings of
the Fifth SIGHAN Workshop on Chinese Language
Processing, pages 158–161.

Yue Zhang and Jie Yang. 2018. Chinese ner using lat-
tice lstm. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguis-
tics, volume 1, pages 1554–1564.

Junsheng Zhou, Weiguang Qu, and Fen Zhang. 2013.
Chinese named entity recognition via joint identifi-
cation and categorization. Chinese journal of elec-
tronics, 22(2):225–230.


