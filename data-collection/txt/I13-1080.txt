










































Unsupervised Word Class Induction for Under-resourced Languages: A Case Study on Indonesian


International Joint Conference on Natural Language Processing, pages 685–691,
Nagoya, Japan, 14-18 October 2013.

Unsupervised Word Class Induction for Under-resourced Languages: A
Case Study on Indonesian

Meladel Mistica
The Australian National University
meladel.mistica@gmail.com

Jey Han Lau and Timothy Baldwin
The University of Melbourne
jeyhan.lau@gmail.com

tb@ldwin.net

Abstract

In this study we investigate how we can
learn both: (a) syntactic classes that cap-
ture the range of predicate argument struc-
tures (PASs) of a word and the syntactic
alternations it participates in, but ignore
large semantic differences in the compo-
nent words; and (b) syntactico-semantic
classes that capture PAS and alternation
properties, but are also semantically co-
herent (a la Levin classes).

We focus on Indonesian as our case study,
a language that is spoken by more than 165
million speakers, but is nonetheless rela-
tively under-resourced in terms of NLP. In
particular, we focus on the syntactic varia-
tion that arises with the affixing of the In-
donesian suffix -kan, which varies accord-
ing to the kind of stem it attaches to.

1 Introduction

This research was motivated by the desire to
semi-automatically develop a lexicon for a wide-
coverage, precision grammar of Indonesian. Al-
though these linguistically-motivated grammars
are invaluable resources to the NLP community,
the biggest drawback is the time required for the
manual creation and curation of the lexicon. Our
work aims to expedite this process by automati-
cally assigning syntactic information to stems that
make up the verbal elements, on the basis of pre-
dicting syntactico-semantic clusters based on dis-
tributional similarity.

However, one minor point becomes one ma-
jor obstacle in this task: Indonesian is a rela-
tively under-resourced language in terms of NLP.
Therefore, many of the techniques that have been
deemed successful in the inferring of syntactic in-
formation or inducing syntactico-semantic classes

are not available to us. Even studies that are con-
sidered lightweight minimally employ a part-of-
speech (POS) tagger and chunker (Joanis et al.,
2008), with many studies benefiting from the rich-
ness of the features that a syntactic parser pro-
vides (Schulte im Walde, 2006). In the case of
Indonesian, there exist POS taggers (Pisceldo et
al., 2009; Wicaksono and Purwarianti, 2010)1 but
no chunker or syntactic parser, and the reliance on
such pre-processing tools is unrealistic.

We adhere to the notion that semantic similar-
ity begets syntactic similarity as per Levin (1989),
and so employ a distributional similarity method
to learn our syntactic classes, based on a non-
parametric Bayesian model. We experiment with
learning both: (a) syntactic classes that capture
the range of predicate argument structures (PASs)
of a word and the syntactic alternations it partic-
ipates in, but ignore large semantic differences in
the component words; and (b) syntactico-semantic
classes that capture PAS and alternation proper-
ties, but are also semantically coherent (a la Levin
classes).

Here, we focus on the syntactic variation that
arises with the affixing of the Indonesian suffix
-kan. The specific morpho-syntactic behaviour
of the kan-affixed verb is very much determined
by the type of stem it attaches to, and its result-
ing behaviour varies from stem type to stem type
(Kroeger, 2007; Vamarasi, 1999; Arka, 1993).
The spectrum of variation induced by the affixing
of -kan is not observed on all types of stems, and
so being able to identify these superordinate types,
representing the same morpho-syntactic variation,
would assist greatly in accelerating lexicon devel-
opment. It has been shown that Levin classes can
be successfully induced employing unsupervised
methods (Schulte im Walde, 2006; Kipper et al.,
2006). We investigate the viability of automati-

1Although no POS tagger has been released for public
use.

685



cally inducing coarser-grained types that represent
morpho-syntactic variation, and we test whether
the method we define is suited to such a task.
Specifically, we focus on a case study detailed in
Section 2 on the syntactic and semantic variation
that arises with the affixing of the Indonesian suf-
fix -kan. In Section 3 we outline our criteria in cre-
ating our gold standard data. Section 4 gives tech-
nical details of our methodology, and our inter-
pretation of distributional similarity expressed in
soft clusters derived using the hierarchical Dirich-
let process (HDP). We present our results compar-
ing our method employing HDP with a simpler
benchmark system using hierarchical agglomera-
tive clustering in Section 5, and also find that the
method we employ in this study is better suited to
discovering Levin-style classes rather than detect-
ing morpho-syntactic variation, even though we
had accommodated for syntactic structure in our
model, by including functional words as structural
indicators. We finally conclude with how we may
extend this preliminary investigation.

Our contributions in this work are: (1) the
demonstration that hierarchical Dirichlet pro-
cesses are a highly effective way of modelling
word similarity, outperforming simpler strate-
gies; (2) the successful application of the syntax–
semantics hypothesis of Levin to an under-
resourced language based on distributional simi-
larity analysis; (3) the finding that conflating se-
mantic classes into superordinate types may be
useful for annotating the lexicon, but when per-
forming clustering tasks that employ distributional
semantics, having a more semantically-oriented
classification, such as Levin classes, are best
suited for such methods, even when approxima-
tions are made to account for syntactic informa-
tion; and (4) the demonstration that clustering
based on semantic properties is a relatively strong
predictor of deep syntactic lexical properties, and
can be of great assistance in semi-automatically
constructing a deep lexical resource for an under-
resourced language

2 Background

Indonesian is an Austronesian language spoken
by more than 165 million speakers in Indonesia
(where it is the national language) and around the
world (Gordon, 2005). Even with this status it
still is an under-resourced language when it comes
to NLP. For our case study, we aim to discover

groups of like stems that, when used predicatively
in the same morphological context, give rise to
the same syntactic behaviour. That is, we aim
to induce classes of stems that exhibit the same
syntactico-semantic behaviour when they have the
same morphological marking.

Predictions on syntactico-semantic properties
of stems via morphological processes have also
been explored for English (Grimshaw, 1990). Al-
though Grimshaw’s account of nominalisation re-
strictions with the English suffix -ing can be ex-
plained with a more general theory of argument
structure, she also shows that the nominalisation
of certain predicates in this way exclude certain
lexical classes, namely psychological predicates
as shown in Example (1) .

(1) a. *The (movie’s) depressing of the
audience.

b. *The worrying of the public.

The morpho-syntactic study presented in this
paper is specific to Indonesian, but these lexical
changes initiated by morphological processes
can be a source of investigation into syntactico-
semantic properties of lexemes for a variety
of languages including English. For our case
study we look into the Indonesian suffix -kan,
which is generally described as a morpheme that
triggers a lexical rule that increases valency. It can
introduce a benefactive object, form a causative
construction, or apply other semantic changes.
Examples (2) and (3) show the benefactive, and
causative uses, respectively:

(2) a. Dia
s/he

membeli
AV+buy

buku
book

itu
this

untuk
for

Mary.
M

“(S)he bought a book for Mary.”
b. Dia

s/he
membelikan
AV+buy+KAN

Mary
M

buku
book

itu.
this

“(S)he bought Mary a book.”

(3) a. Orang-orang
person-person

mengungsi.
AV+take-refuge

“The people took refuge.”
b. PBB

U.N.
mengungsikan
AV+refuge+KAN

orang-orang.
person-person

“The U.N. evacuated the people.”

In the second line of each of these glossed exam-
ples, AV stands for actor voice, which means that
the verb is active. This is marked by the prefix me-
plus a homorganic nasal, which can be realised

686



as m, n(g|y) or ∅. This verb behaves in a similar
fashion to English verbs in an active sentence.
We limit the examination of verbs in this study to
those that exhibit the actor voice (AV) marking.

Linguists have tried to characterise stems ac-
cording to their behaviour when affixed with -
kan (Dardjowidjojo, 1971; Arka, 1993; Vama-
rasi, 1999). In particular, Vamarasi (1999) claims
that kan is a good diagnostic for separating un-
accusative from unergative stems, which predicts
their morphosynatic behaviour. However the facts
of -kan seem more intricate than this characteri-
sation. Even though the causative and benefac-
tive constructions uses of kan are the most com-
monly cited, its usage is much more varied and
nuanced, as shown by Kroeger (2007), which is
why we chose this morpho-syntactic construction
as our case study.

Since the early ’90s, the tools and resources em-
ployed in valency acquisition tasks have become
increasingly sophisticated and lingusitically-rich.
One of the earlier examples of this is by Brent
(1993), who employs a system based on deter-
ministic morphological cues to identify predefined
syntactic patterns from the Brown Corpus. Man-
ning (1993) employs a shallow parser or chun-
ker in order to acquire subcategorisation frames
from the New York Times. Schulte im Walde
(2002) induces subcategorisation information for
German with the use of a lexicalised probabilistic
context free grammar (PCFG), and O’Donovan et
al. (2005) employ the richly-annotated Penn Tree-
bank in achieving this endeavour. In terms of re-
sources, our work most closely resembles Brent
(1993), in that we rely mainly on linguistic knowl-
edge based on simple lexical features. However,
the way linguistic knowledge is learned and ap-
plied is quite different, as we will see in Section 3

In terms of the methodology, the studies that
we look to are those systems that are built to
disambiguate and/or discover syntactico-semantic
Levin-style classes, rather than systems that aim
to induce valency or syntactic frame information
from corpora. These can be built in a supervised
fashion as in Lapata and Brew (2004) or tackled as
a clustering task as in Schulte im Walde (2006) or
Bonial et al. (2011). Lapata and Brew (2004) de-
velop a semi-supervised system that generates, for
a given verb and its syntactic frame, a probabil-
ity distribution over the Levin verb classes. They
then use this system to disambiguate tokens using
collocation information. Our system, like Schulte

im Walde (2006), uses an unsupervised clustering
approach. In her approach, Schulte Im Walde em-
ploys hierarchical agglomerative clustering over
parse features to discover word classes in Ger-
man, and evaluates using manually-created gold-
standard data.

3 Evaluation Data

This section describes how we arrive at the two
evaluation sets we use in our experiments.

3.1 Forming Levin Classes
We use VerbNet 3.22 as our guide for forming
Levin classes for Indonesian, and rely on their
translation to determine membership for the class,
for a particular sense of that verb.

We have 30 stems that we group into 16 Levin
classes. Unlike the types we form in Section 3.2,
which have unique membership, a lexical item can
appear in multiple classes as appropriate. For ex-
ample baca “read” has membership in both Verb-
Net classes say-27.7 and learn-14. We show a
subset of Indonesian Levin classes we develop
based on VerbNet 2.3 in Table 1.

3.2 Forming Superordinate Levin Types
These superordinate types combine Levin classes
to form groups of stems that behave in the same
way syntactically, but may not all be synonyms of
each other. In determining the coarse-grained su-
perordinate types, we did not simply want to group
stems according to intuition. Rather, we were af-
ter an explicit description of the syntax and se-
mantics of grouped stems that all behave in the
same way when affixed with -kan. Stems that are
grouped together should exhibit the same seman-
tic shifts. That is, if affixing -kan to a stem gives
rise to a causative meaning, then its correspond-
ing group member will also produce a causative
meaning when -kan is applied to the stem. Also, if
adding a -kan does not increase the valency for a
stem in a particular group, then its corresponding
group member will also exhibit the same syntatic
behaviour.

In order to achieve this, we map out the different
behaviour of verb stems when they occur in the
morphological patterns (a) and (b): 3

2http://verbs.colorado.edu/˜mpalmer/
projects/verbnet/downloads.html

3As mentioned earlier in Section 2, AV stands for actor
voice, and can be likened to an English verb in an active sen-
tence.

687



Indonesian Members VerbNet Class
beri “give” jaja “hawk/sell”, pinjam “lend” give-13.1
kenang “think” kenal “know” ingat “remember” consider-29.9
mati “die”, tewas “perish” disappearance-48.2
susup “duck down”, singkir “get out of way” avoid-52
timpa “hit” hantam “hit/blow” tabrak “hit” hit-18.1
baca “read” tulis “write” say-37.7
baca “read” hafal “memorize” learn-14

Table 1: Subset of the mapping of Levin classes into Indonesian

(a)
ME N+stem

(b)
ME N+stem+KAN

AV+stem AV+stem+KAN
We map out the variation of arguments for pat-

tern (a) with only the AV prefix, i.e. ME N+stem,
and then note the changes when the stem has
both the actor AV and -kan affixes, i.e. pattern (b)
ME N+stem+KAN. We also track the semantic
changes relative to the stem for these two patterns
and found that 25 verb stems found their way into
8 verb types.4 This formed one of our evaluation
sets in our experiments (see Mistica (2013) for fur-
ther details on forming these superordinate types).

In the interests of space, we only present two
out of the 8 manually-induced verb types in Ta-
ble 2. Below each of the types, we show the
syntactic and semantic changes that determine our
verb types or subclasses.

4 Method

We define our features in terms of the context of
occurrence of our target lexeme, and employ hier-
achical agglomerative clustering (HAC) over these
features in two ways: (1) directly over the raw
word frequencies; and (2) over extracted seman-
tic features learned via the contexts of occurrence,
which are represented as topic probabilities.

We use Indonesian Wikipedia5 as our text col-
lection, and remove mark-up with Wikiprep,6

then tokenise with the English-trained models of
OpenNLP.7 The total word count of the text col-
lection is approximately 26 million words. In the

4We had also manually grouped stems from other word
classes: 48 noun stems were grouped into 13 subclasses; and
27 adjective stems were grouped into 5, giving us a total of
100 stems with the 25 verbs, but we only report on the verb
experiments.

5http://dumps.wikimedia.org/idwiki/
6http://www.cs.technion.ac.il/˜gabr/

resources/code/wikiprep
7http://opennlp.apache.org/ Our experiments

showed that OpenNLP’s English models performed better
than a rule-based Malay sentence tokeniser (Baldwin and
Awab, 2006).

next section we summarise the features we use in
our experiments, in addition to outlining our clus-
tering method.

4.1 Feature Engineering
Our features determine how we collect unigrams
from the text collection. We collect these unigram
features from 735 lexemes that we were able to
identify as possible -kan hosts. These 735 lexemes
had stems that belonged to any of the open class
categories in Indonesian (noun, adjective or verb).

In our preparation of the Wikipedia data, we in-
clude function words as a means to infer structural
information. Because we do not use a parser to
explicitly obtain syntactic features, this is how we
approximate this kind of information.

We use three main feature types in our task: (1)
morph ∈ ‘k’, ‘mk’, ‘smk’; (2) win ∈ 1 to 5; and
(3) context ∈ ‘+’ (forward), ‘−’ (backward).

Morphological features (morph): These are
contextual features for different morphological
forms of the target lexeme, where: ‘s’ stands for
stem, i.e. the unaffixed lexeme; ‘m’ stands for the
AV variant of the lexeme, based on pattern (a) from
Section 3.2; and ‘k’ stands for the KAN suffixed
form of the AV variant of the lexeme, based on
pattern (b) from Section 3.2. An example of the
‘s’, ‘m’ and ‘k’ variants of beli “buy” are beli,
membeli, and membelikan, respectively. These
morphological features determine whether the un-
igram features we collect for a lexeme are based
on instances of ((s)m)k forms found in the text.
We experiment with the context features based on
these morphological variants in isolation and also
in combination. For example, ‘mk’ would capture
context features for the membeli and membelikan
variants of the stem beli “buy”.

Window Size (win): This stipulates the context
window size, relative to individual occurrences of
the target lexeme, and can take a value of 1–5.

688



MORPHOLOGY VERB FRAME DECOMPOSITION

Example Type A: acuh “to heed”, terjemah “translate”, mandi “bathe”
MEN+V1 – –
MEN+V1+KAN <NPa, NPb > DOto( [NPa], [V1 TO( [NP] ) ] )

Example Type B: dengar “hear”, kenang “think of”
MEN+V3 <NPa, NPb > HAPPENto( [NPb], [ V3 TO([NPa] ) ] )
MEN+V3+KAN <NPa, NPb > DOto( [NPa], [ V3 TO( [NPb] ) ] )

Table 2: Manually generated verb Types (‘–’ = no attested word form in the text; ‘{. . . }’ = optional)

Context Features (context): We look at back-
ward (‘−’) or forward (‘+’) context unigrams.

4.2 Clustering Stems
We employ hierarchical agglomerative clustering
(HAC) in two ways: (1) over the raw frequencies
of words based on the feature representations de-
fined in Section 4.1; and (2) over the output of
the distributional semantic modelling (HDP) dis-
cussed in Section 4.3. The output of this step pro-
duces topic models. In other words, we perform
HAC over raw unigram frequencies and induced
topic models from these raw frequencies to ascer-
tain the usefulness of the HDP step.

To compute the distance between a pair of pat-
terns, we use Squared Euclidean, and for the
linkage criterion for merging clusters we use
weighted linkage clustering (WPGMA). We com-
pare the output of HAC with the flat-structured
gold-standard classes. In order to induce flat clus-
ters from the hierarchical output of HAC, we apply
a similarity threshold t = 0.825 to determine which
instances should be grouped together.

4.3 Modelling Distributional Similarity
Distributional semantic models are commonly em-
ployed in the induction and disambiguation of
word senses (McCarthy and Carroll, 2003; La-
pata and Brew, 2004; Brody and Lapata, 2009;
Lau et al., 2012), and to a lesser extent, in learn-
ing syntactic classes and diathesis alternation be-
haviour (Parisien and Stevenson, 2011; Bonial
et al., 2011). We infer lexical similarity and
soft word clusters using topic modelling, based
on a hierarchical Dirichlet process (HDP: Teh
et al. (2006)), a non-parametric extension of la-
tent Dirichlet allocation (LDA: Blei et al. (2003)).
LDA is a Bayesian generative topic model that
learns latent topics for a collection of documents

based on the observable words. Our definition of
a document is a target lexeme and the observable
words that surround the target lexeme (based on
the window size in the parameter settings).

Formally, in LDA a topic is associated with a
multinomial distribution of words, and each doc-
ument (i.e. lexeme) in the collection is associated
with a multinomial distribution of topics. HDP re-
laxes the constraint in LDA where the number of
topics T is fixed, and learns T based on the train-
ing data using Dirichlet processes (DPs).

4.4 Evaluation
We develop two baseline systems to compare our
results against: (1) majority class; and (2) random
class assignment based on a uniform class distri-
bution. The random scores reported are based on
the median of 11 random assignments.

We use pairwise precision (pP ), recall (pR),
and F-score (pF1) to evaluate our generated clus-
ters, relative to the gold-standard word classes, as
described by Schulte im Walde (2006).

5 Results

We perform two experiments. First, we apply
the hierarchical Dirichlet process (HDP) to pro-
duce topic probabilities, over which we perform
HAC. Second, we perform HAC over the raw un-
igram features (NoHDP), as our benchmark sys-
tem, a method also employed by systems such as
Schulte im Walde (2002) for German and Jurgens
and Stevens (2010) for English word sense induc-
tion. In both cases, we base our experiments on the
735 lexemes identified as being able to be affixed
with -kan, and the unigram features from Sec-
tion 4.1. Note, however, that evaluation is based
on the subset of the 735 lexemes which were man-
ually classified into classes and types in Section 3.

We employ a bagging approach (sampling with

689



System Maj. Rand. ON-ALL ON-VERBS
LEVIN-HDP

.114 .065
.174 .367

LEVIN-NOHDP .057 .111
TYPES-HDP

.271 .140
.281 .261

TYPES-NOHDP .026 .152

Table 3: pF1 score comparing benchmark system
NOHDP with our HDP system for Levin Classes
(LEVIN) and our coarser-grained TYPES

A main “play”, nyanyi “sing”, gesek “scrape”
B kirim “send”, hantar “place”
C dapat “get”, menang “win”, terima “receive”

Table 4: Induced groups with no known cate-
gorised words

replacement) to ascertain the best parameters to
apply to our 735 lexemes in terms of the unigram
features we define in Section 4.1.

Given the discovered parameters, we report our
results in Table 3. The label ON-ALL for all HDP
systems are sytems that have had topics induced
from all 735 stems (made up of not only verbs, but
also nouns and adjectives), while ON-VERBS only
induces topics from a subset of the 735 lexemes
whose stems are also verbs, even though we only
evaluate on verbs in these experiments.

We observe in Table 3 that HDP consistently
outperforms NO-HDP systems. Furthermore,
the LEVIN-HDP system outperforms the Random
(“Rand.”) and the Majority Class (“Maj.”) base-
lines, as well as the benchmark NOHDP system.
The TYPES-HDP system, on the other hand, barely
exceeds the Majority Class baseline with the ON-
ALL experiment, and fails to do so with the ON-
VERBS experiment.

6 Discussion

For our error analysis, we examine a sample of the
resulting stem groups from the Levin Class exper-
iments. Table 4 shows membership of all stems
found in four separate clusters. The lexemes from
these particular groups do not have membership
into any of the gold standard Levin classes, unlike
the groups formed in Table 5. In this table, the top
half are groups that match our Levin classes, part
of which is presented in Table 1, and the bottom
half are groups that do not match Levin classes.

Group A from Table 4 has 3 verbs — main

D
singkir “get out of way”,

susup “duck down”
E baca “read” hafal “memorise”

F
terjemah “translate” tulis “write”,

muat “insert/contain”

G
paksa “force” pinjam “lend”

hapus “wipe off/vanish/blot out”

Table 5: Induced groups with known categorised
words

“play”, nyanyi “sing”, and gesek “scrape” —
which may initially seem not to form a semanti-
cally coherent group, however they all are associ-
ated with producing music: main “play” is used to
describe the playing of most musical instruments,
and gesek “scrape/rub” is used for string instru-
ments, such as violins or cellos. Group B has
members that describe movement from one place
to another, as does Group C.

Groups D and E in Table 5 faithfully replicate
the Levin Classes avoid-52, adn learn-14 from Ta-
ble 1. However, Groups F and G seem to not form
coherent semantic groups.

7 Conclusion

We have explored the question of whether distri-
butional similarity models can be used to learn
deep syntactic features for an under-resourced lan-
guage, namely Indonesian. Our results demon-
strate that hierarchical Dirichlet processes are a
highly effective way of modelling word similar-
ity, and outperform a simpler strategy of simply
applying HAC over raw frequencies. We have
also shown that learning classes geared toward the
potential morpho-syntactic alternations of stems,
while conflating the semantics of the stem are
too coarse for this particular method. The ex-
periments that used true Levin classes to evaluate
against performed much better in comparison to
the baselines, than did the experiments where we
induced our manually constructed coarse-grained
types. Although resources and tools are limited
for Indonesian NLP, we would need to model syn-
tactic structure more effectively to gain success in
predicting lexical types rather than Levin classes.

690



References
I Wayan Arka. 1993. Morphological aspects of the

-kan causative in indonesian. Master’s thesis, The
University of Sydney, Sydney, Australia, November.

Timothy Baldwin and Suád Awab. 2006. Open source
corpus analysis tools for Malay. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation (LREC2006), pages 2212–5,
Genoa, Italy.

David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.

Claire Bonial, Susan Windisch Brown, Jena D. Hwang,
Christopher Parisien, Martha Palmer, and Suzanne
Stevenson. 2011. Incorporating coercive construc-
tions into a verb lexicon. In Proceedings of the ACL
2011 Workshop on Relational Models of Semantics,
pages 72–80, Portland, USA.

Michael R. Brent. 1993. From grammar to lexicon:
Unsupervised learning of lexical syntax. Computa-
tional Linguistics, 19(2):243–262.

Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the EACL (EACL 2009), pages 103–
111, Athens, Greece.

Soenjono Dardjowidjojo. 1971. The meN-, meN-kan,
and meN-i verbs in Indonesian. Philippine Journal
of Linguistics, 2:71–84.

Raymond Gordon. 2005. Ethnologue: Languages of
the World. SIL International, Dallas, USA.

Jane Grimshaw. 1990. Argument Structure. The MIT
Press, Cambridge, USA.

Eric Joanis, Suzanne Stevenson, and David James.
2008. A general feature space for automatic
verb classification. Natural Language Engineering,
14(3):337–367.

David Jurgens and Keith Stevens. 2010. HERMIT:
Flexible clustering for the SemEval-2 WSI task.
In Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 359–362, Uppsala,
Sweden.

Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extending VerbNet with
novel verb classes. In Proceedings of LREC 2006,
pages 1027–1032, Genoa, Italy.

Paul R. Kroeger. 2007. Morphosyntactic vs. mor-
phosemantic functions of indonesian ’-kan. In Joan
Bresnan, Annie Zaenen, Jane Simpson, Tracy Hol-
loway King, Jane Grimshaw, Joan Maling, and
Christopher D. Manning, editors, Architectures,
rules, and preferences: variations on themes, CSLI
Lecture Notes, pages 229–251. CSLI Publications.

Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Computa-
tional Linguistics, 30(1):45–73.

Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense in-
duction for novel sense detection. In Proceedings of

the 13th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-
2012), pages 591–601, Avignon, France.

Beth Levin. 1989. English Verb Classes and Alterna-
tions: A preliminary investigation. The University
of Chicago Press, Chicago, USA.

Christopher D. Manning. 1993. Automatic acquisition
of a large subcategorization dictionary from corpora.
In Proceeding of the 31st Annual Meeting of the As-
sociation for Computational Linguistics, pages 235–
242, Columbus, USA.

Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639–654.

Meladel Mistica. 2013. An Investigation into Deviant
Morphology: Issues in the Implementation of a Deep
Grammar for Indonesian. Ph.D. thesis, The Aus-
tralian National University, Canberra, Australia.

Ruth O’Donovan, Michael Burke, Aoife Cahill, Josef
van Genabith, and Andy Way. 2005. Large-scale
induction and evaluation of lexical resources from
the Penn-II and Penn-III treebanks. Computational
Linguistics, 31(3):229–365.

Chris Parisien and Suzanne Stevenson. 2011. General-
izing between form and meaning using learned verb
classes. In Proceedings of the 33rd Annual Confer-
ence of the Cognitive Science Society, Boston, USA.

Femphy Pisceldo, Ruli Manurung, and Mirna Adriani.
2009. Probabilistic part-of-speech tagging for Ba-
hasa Indonesia. In Proceedings of the Third Inter-
national MALINDO Workshop, Singapore.

Sabine Schulte im Walde. 2002. A subcategorisa-
tion lexicon for German verbs induced from a lex-
icalised PCFG. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Eval-
uation, volume IV, pages 1351–1357, Las Palmas de
Gran Canaria, Spain.

Sabine Schulte im Walde. 2006. Experiments on
the automatic induction of german semantic verb
classes. Computational Linguistics, 32(2):159–194.

Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006. Hierarchical Dirichlet processes. Journal
of the American Statistical Association, 101:1566–
1581.

Marit Kana Vamarasi. 1999. Grammatical relations in
Bahasa Indonesia, volume 93 of Series D. Pacific
Linguistics, Canberra, Australia.

Alfan Farizki Wicaksono and Ayu Purwarianti. 2010.
HMM based part-of-speech tagger for Bahasa In-
donesia. In Proceedings of the 4th International
MALINDO Workshop (MALINDO2010), Depok, In-
donesia.

691


