



















































Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4711–4716
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4711

Understanding Deep Learning Performance through an Examination of
Test Set Difficulty: A Psychometric Case Study

John P. Lalor1∗, Hao Wu2, Tsendsuren Munkhdalai3, Hong Yu1,4
1College of Information and Computer Sciences, University of Massachusetts, Amherst

2 Department of Psychology and Human Development, Vanderbilt University
3Microsoft Research, Montréal, Québec

4Department of Computer Science, University of Massachusetts, Lowell
*lalor@cs.umass.edu

Abstract

Interpreting the performance of deep learning
models beyond test set accuracy is challenging.
Characteristics of individual data points are of-
ten not considered during evaluation, and each
data point is treated equally. We examine the
impact of a test set question’s difficulty to de-
termine if there is a relationship between diffi-
culty and performance. We model difficulty
using well-studied psychometric methods on
human response patterns. Experiments on Nat-
ural Language Inference (NLI) and Sentiment
Analysis (SA) show that the likelihood of an-
swering a question correctly is impacted by
the question’s difficulty. As DNNs are trained
with more data, easy examples are learned
more quickly than hard examples.

1 Introduction

One method for interpreting deep neural networks
(DNNs) is to examine model predictions for spe-
cific input examples, e.g. testing for shape bias as
in Ritter et al. (2017). In the traditional classifica-
tion task, the difficulty of the test set examples is
not taken into account. The number of correctly-
labeled examples is tallied up and reported. How-
ever, we hypothesize that it may be worthwhile
to use difficulty when evaluating DNNs. For ex-
ample, what does it mean if a trained model an-
swers the more difficult examples correctly, but
cannot correctly classify what are seemingly sim-
ple cases? Recent work has shown that for NLP
tasks such as Natural Language Inference (NLI),
models can achieve strong results by simply using
the hypothesis of a premise-hypothesis pair and
ignoring the premise entirely (Gururangan et al.,
2016; Tsuchiya, 2018; Poliak et al., 2018).

In this work we consider understanding DNNs
by looking at the difficulty of specific test set ex-
amples and comparing DNN performance under
different training scenarios. Do DNN models learn
examples of varying difficulty at different rates? If

a model does well on hard examples and poor on
easy examples, then can we say that it has really
learned anything? In contrast, if a model does well
on easy items, because a dataset is all easy, have
we really “solved” anything?

To model difficulty we use Item Response The-
ory (IRT) from psychometrics (Baker and Kim,
2004). IRT models characteristics such as diffi-
culty and discrimination ability of specific exam-
ples (called “items”1) in order to estimate a la-
tent ability trait of test-takers. Here we use IRT
to model the difficulty of test items to determine
how DNNs learn items of varying difficulty. IRT
provides a well-studied methodology for modeling
item difficulty as opposed to more heuristic-based
difficulty estimates such as sentence length. IRT
was previously used to build a new test set for the
NLI task (Lalor et al., 2016) and show that model
performance is dependent on test set difficulty. In
this work we use IRT to probe specific items to
try to analyze model performance at a more fine-
grained level, and expand the analysis to include
the task of SA.

We train three DNNs models with varying train-
ing set sizes to compare performance on two NLP
tasks: NLI and Sentiment Analysis (SA). Our ex-
periments show that a DNN model’s likelihood of
classifying an item correctly is dependent on the
item’s difficulty. In addition, as the models are
trained with more data, the odds of answering easy
examples correctly increases at a faster rate than
the odds of answering a difficult example correctly.
That is, performance starts to look more human, in
the sense that humans learn easy items faster than
they learn hard items.

That the DNNs are better at easy items than hard
items seems intuitive but is a surprising and inter-
esting result since the item difficulties are modeled
from human data. There is no underlying reason

1For the remainder of the paper we will refer to a single
test set example as an “item” for consistency.



4712

that the DNNs would find items that are easy for hu-
mans inherently easy. To our knowledge this is the
first work to use a grounded measure of difficulty
learned from human responses to understand DNN
performance. Our contributions are as follows: (i)
we use a well-studied methodology, IRT, to esti-
mate item difficulty in two NLP tasks and show
that this human-estimated difficulty is a useful pre-
dictor of DNN model performance, (ii) we show
that as training size increases DNN performance
trends towards expected human performance.2

2 Methods

2.1 Estimating Item Difficulty
To model item difficulty we use the Three Parame-
ter Logistic (3PL) model from IRT (Baker, 2001;
Baker and Kim, 2004; Lalor et al., 2016). The 3PL
model in IRT models an individual’s latent ability
(θ) on a task as a function of three item character-
istics: discrimination ability (a), difficulty (b), and
guessing (c). For a particular item i, the probability
that an individual j will answer item i correctly is
a function of the individual’s ability and the three
item characteristics:

pij(θj) = ci +
1− ci

1 + e−ai(θj−bi)
(1)

where ai is the discrimination parameter (the
value of the function slope at it’s steepest point),
bi is the difficulty parameter (the value where
pij(θj) = 0.5), and ci is the guessing parameter
(the lower asymptote of the function). For a set of
items I and a set of individuals J , the likelihood of
each individual in J’s responses to the items in I
is:

L =

J∏
j=1

I∏
i=1

pij(θj)
yijqij(θj)

(1−yij) (2)

where qij(θj) = 1− pij(θj) and yij = 1 if indi-
vidual j answered item i correctly and yij = 0 oth-
erwise. Item parameters and individual ability are
jointly estimated from a set of individuals’ response
patterns using an Expectation-Maximization algo-
rithm (Bock and Aitkin, 1981).

In this work we focus on the difficulty parameter
bi, which represents the latent ability level at which
an individual has a 50% chance of answering item

2Code and data available at
http://jplalor.github.io

i correctly. Low values of bi are associated with
easier items (since an individual with low ability
has a 50% chance of answering correctly), and
higher values of bi represent more difficult items.

2.2 Data

To estimate item difficulties for NLI, we used the
pre-trained IRT models of Lalor et al. (2016) and
extracted the difficulty item parameters. The data
consists of approximately 1000 human annotator
responses from Amazon Mechanical Turk (AMT)
for a selection of 180 premise-hypothesis pairs
from the SNLI data set (Bowman et al., 2015).
Each AMT worker (Turker) was shown the premise-
hypothesis pairs and was asked to indicate whether,
if the premise was taken to be true, the hypothesis
was (a) definitely true (entailment), (b) maybe true
(neutral), or (c) definitely not true (contradiction).

For SA, we collected a new data set of labels for
134 examples randomly selected from the Stanford
Sentiment Treebank (SSTB) (Socher et al., 2013),
using a similar AMT setup as Lalor et al. (2016).
For each randomly selected example, we had 1000
Turkers label the sentence as very negative, neg-
ative, neutral, positive, or very positive. We con-
verted these responses to binary positive/negative
labels and fit a new IRT 3PL model (§2.1) using
the mirt R package (Chalmers et al., 2015). Very
negative and negative labels were binned together,
and neutral, positive, and very positive were binned
together.

Tables 1 and 2 show examples of the items in our
data sets, and the difficulty values estimated from
the IRT models. The first example in Table 1 is a
clear case of entailment, where if we assume that
the premise is true, we can infer that the hypothesis
is also true. The label of the second example in
SNLI is contradiction, but in this case the result is
not as clear. There are sports stadiums that offer
lawn seating, and therefore this could potentially
be a case of entailment (or neutral). Either way,
one could argue that the second example here is
more difficult than the first. Similarly, the first two
examples of Table 2 are interesting. Both of these
items are labeled as negative examples in the data
set. The first example is clear, but the second one
is more ambiguous. It could be considered a mild
complement, since the author still endorses renting
the movie. Therefore you could argue again that the
second example is more difficult than the first. The
learned difficulty parameters reflect this difference



4713

Premise Hypothesis Label Difficulty
A little girl eating a sucker A child eating candy Entailment -2.74
People were watching the tournament in the
stadium

The people are sitting outside on the grass Contradiction 0.51

Two girls on a bridge dancing with the city
skyline in the background

The girls are sisters. Neutral -1.92

Nine men wearing tuxedos sing Nine women wearing dresses sing Contradiction 0.08

Table 1: Examples of sentence pairs from the SNLI data sets, their corresponding gold-standard label, and
difficulty parameter (bi) as measured by IRT (§2.1).

Phrase Label Difficulty
The stupidest, most insulting movie of 2002’s first quarter. Negative -2.46
Still, it gets the job done - a sleepy afternoon rental. Negative 1.78
An endlessly fascinating, landmark movie that is as bold as anything the cinema has seen in
years.

Positive -2.27

Perhaps no picture ever made has more literally showed that the road to hell is paved with good
intentions.

Positive 2.05

Table 2: Examples of phrases from the SSTB data set, their corresponding gold-standard label, and
difficulty parameter (bi) as measured by IRT (§2.1).

Dataset Fleiss’ κ
SNLI 4GS Contradiction 0.37
SNLI 4GS Entailment 0.48
SNLI 4GS Neutral 0.41
SNLI 5GS Contradiction 0.59
SNLI 5GS Entailment 0.63
SNLI 5GS Neutral 0.54
Sentiment Analysis 0.52

Table 3: Fleiss’ κ scores for the NLI and SA anno-
tations collected from AMT.

in difficulty in both cases.
Inter-rater reliability scores for the collected an-

notations are showin in Table 3. Scores for the
NLI annotations were calculated when the origi-
nal dataset was collected and are reproduced here
(Lalor et al., 2016). Human annotations for the
SA annotations were converted to binary before
calculating the agreement. We see that the agree-
ment scores are in the range of 0.4 to 0.6 which is
considered moderate agreement (Landis and Koch,
1977). With the large number of annotators it is
to be expected that there is some disagreement in
the labels. However this disagreement can be inter-
preted as varying difficulty of the items, which is
what we expect when we fit the IRT models.

2.3 Experiments

Our goal in this work is to understand how DNN
performance on items of varying difficulty changes
under different training scenarios. To test this, we
trained three DNN models using subsets of the
original SNLI and SSTB training data sets: (i)
Long Short Term Memory Network (LSTM) (Bow-

man et al., 2015), (ii) Convolutional Neural Net-
work (CNN) (Kim, 2014), and (iii) Neural Seman-
tic Encoder (NSE), a type of memory-augmented
RNN (Munkhdalai and Yu, 2017).3 For each task
(NLI and SA), we randomly sampled subsets of
training data, from 100 examples up to and includ-
ing the full training data sets.4 We trained each
model on the training data subsets, using the origi-
nal development sets for early stopping to prevent
overfitting. The IRT data with difficulty estimates
were used as test sets for the trained models.

Once the models were trained and had classified
the IRT data sets, we fit logistic regression models
to predict whether a DNN model would label an
item correctly, using the training set size and item
difficulty as the dependent parameters.

3 Results

Figure 1 plots the contour plots of our learned re-
gression models. The top row plots results for the
NLI task, and the bottom row plots results for the
SA task. From left to right in both rows, the plots
show results for the LSTM, CNN, and NSE models.
In each plot, the x-axis is the training set size, the
y-axis is the item difficulty, and the contour lines
represent the log-odds that the DNN model would
classify an item correctly. As the plots show, item
difficulty has a clear effect on classification. Easier
items have higher odds of being classified correctly

3Please refer to the appendix for model details.
4We sampled 100, 1000, 2000, 5000, 10000, 50000,

100000, 200000, and 500000 examples for NLI, and sam-
pled 100, 1000, 5000, 10000, 50000, and 75000 examples for
SA.



4714

Figure 1: Contour plots showing log-odds of labeling an item correctly for NLI (top row) and SA (bottom
row) as a function of training set size (x-axis) and item difficulty (y-axis). Each line in the plots represents
a single log-odds value for labeling an item correctly. Blue indicates low log-odds of labeling an item
correctly, and pink indicates high log-odds of labeling an item correctly. The contour colors are consistent
across plots and log-odds values are shown in the legend on the right.

across all of the training set sizes. In addition, the
slopes of the contour lines are steeper at lower lev-
els of difficulty. This indicates that, moving left to
right along the x-axis, a model’s odds of answering
an easy item correctly increase more quickly than
the odds of answering a harder item correctly.

The contour plots for the CNN and NSE models
on the SA task (Figure 1, second row middle and
right plots) show that the easier items have higher
likelihood of being classified correctly, but the odds
for the most difficult items decrease as training
size increases. This suggests that these models are
learning in such a way that improves performance
on easy items but has a negative effect on hard
items. This result is important for interpretability,
as it could inform stakeholder decisions if they
need to have difficult examples classified.

The idea that easy items should be easier than
hard items is consistent with learning strategies in
humans. For example, when teaching new con-
cepts to students, easier concepts are presented first
so that the students can learn patterns and core
information before moving to more difficult con-
cepts (Collins et al., 1988; Arroyo et al., 2010). As
students do more examples, all questions get easier,

but easy questions get easier at a faster rate. Our
result is also consistent with the key assumptions
of curriculum learning (Bengio et al., 2009).

4 Related Work

Lalor et al. (2016) introduced the idea of apply-
ing IRT evaluation to NLP tasks. They built a
set of scales using IRT for NLI and evaluated a
single LSTM neural network to demonstrate the
effectiveness of the evaluation, but did not evaluate
other NLP models or tasks. Martı́nez-Plumed et al.
(2016) consider IRT in the context of evaluating
ML models, but they do not use a human popula-
tion to calibrate the models, and obtain results that
are difficult to interpret under IRT assumptions.

There has been work in the NLP commu-
nity around modeling latent characteristics of
data (Bruce and Wiebe, 1999) and annota-
tors (Hovy et al., 2013), but none that apply the
resulting metrics to interpret DNN models. Passon-
neau and Carpenter (2014) model the probability a
label is correct with the probability of an annotator
to label an item correctly according to the Dawid
and Skene (1979) model, but do not consider diffi-
culty or discriminatory ability of the data points.



4715

One-shot learning is an attempt to build ML mod-
els that can generalize after being trained on one
or a few examples of a class as opposed to a large
training set (Lake et al., 2013). One-shot learning
attempts to mimic human learning behaviors (i.e.,
generalization after being exposed to a small num-
ber of training examples) (Lake et al., 2016). Our
work instead looks at comparisons to human perfor-
mance, where any learning (on the part of models)
has been completed beforehand. Our goal is to
analyze DNN models and training set variations as
they affect ability in the context of IRT.

5 Discussion

In this work we have shown that DNN model per-
formance is affected by item difficulty as well as
training set size. This is the first work that has
used a well-established method for estimating dif-
ficulty to analyze DNN model performance as op-
posed to heuristics. DNN models perform better on
easy items, and as more data is introduced in train-
ing, easy items are learned more quickly than hard
items. Learning easy examples faster than harder
examples is what would be expected when exam-
ining human response patterns as they learn more
about a subject. However this has not previously
been shown to be true in DNN models.

That the results are consistent across NLI and SA
shows that the methods can be applied to a number
of NLP tasks. The SA results do show that the odds
of labeling a difficult item correctly decrease with
more training data 1. It could be the case that these
difficult items in the SA task are more subjective
than the easier items, for example a review that
is fairly neutral and is split between positive and
negative annotations. These cases would be more
difficult for a model to label, and are worth exam-
ining in more detail. By identifying items such as
these as difficult makes it easier to see where the
model is going wrong and allows for research on
better way to represent these cases.

This result has implications for how machine
learning models are evaluated across tasks. The tra-
ditional assumption that the test data is drawn from
the same distribution as the training data, makes it
difficult to understand how a model will perform
in settings where that assumption does not hold.
However, if the difficulty of test set data is known,
we can better understand what kind of examples
a given model performs well on, and specific in-
stances where a model underperforms (e.g. the

most difficult examples). In addition, researhers
can build test sets that consist of a specific type
of data (very easy, very hard, or a mix) to evalu-
ate a trained model under specific assumptions to
test generalization ability in a controlled way. This
could allow for more confidence in model perfor-
mance in more varied deployment settings, since
there would be a set of tests a model would have to
pass before being deployed.

It is important to note that the difficulty param-
eters were estimated from a human population,
meaning that those items that are difficult for hu-
mans are in fact more difficult for the DNN models
as well. This does not need to be the case given
that DNNs learn very different patterns, etc. than
humans. In fact there were exceptions in our results
which shows that these models should be carefully
examined using techniques like those described
here. Future work can investigate why this is the
case and how we can leverage this information to
improve model performance and interpretability.

Acknowledgments

We thank the AMT Turkers who completed our
annotation task. This work was supported in part
by the HSR&D award IIR 1I01HX001457 from the
United States Department of Veterans Affairs (VA).
We also acknowledge the support of LM012817
from the National Institutes of Health. This work
was also supported in part by the Center for Intel-
ligent Information Retrieval. The contents of this
paper do not represent the views of CIIR, NIH, VA,
or the United States Government.

References
Ivon Arroyo, Hasmik Mehranian, and Beverly P Woolf.

2010. Effort-based tutoring: An empirical approach
to intelligent tutoring. In Educational Data Mining
2010.

Frank B Baker. 2001. The basics of item response the-
ory. ERIC.

Frank B. Baker and Seock-Ho Kim. 2004. Item Re-
sponse Theory: Parameter Estimation Techniques,
Second Edition. CRC Press.

Yoshua Bengio, Jérôme Louradour, Ronan Collobert,
and Jason Weston. 2009. Curriculum learning. In
Proceedings of the 26th annual international confer-
ence on machine learning, pages 41–48. ACM.

R Darrell Bock and Murray Aitkin. 1981. Marginal
maximum likelihood estimation of item parameters:



4716

Application of an em algorithm. Psychometrika,
46(4):443–459.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and D. Christopher Manning. 2015. A large an-
notated corpus for learning natural language infer-
ence. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing,
pages 632–642. Association for Computational Lin-
guistics.

Rebecca F. Bruce and Janyce M. Wiebe. 1999. Recog-
nizing subjectivity: A case study in manual tagging.
Nat. Lang. Eng., 5(2):187–205.

Phil Chalmers, Joshua Pritikin, Alexander Robitzsch,
and Mateusz Zoltak. 2015. mirt: Multidimensional
Item Response Theory.

Allan Collins, John Seely Brown, and Susan E New-
man. 1988. Cognitive apprenticeship: Teaching the
craft of reading, writing and mathematics. Thinking:
The Journal of Philosophy for Children, 8(1):2–10.

A. Philip Dawid and Allan M. Skene. 1979. Maximum
likelihood estimation of observer error-rates using
the em algorithm. Journal of the Royal Statistical
Society. Series C (Applied Statistics), 28(1):20–28.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel R Bowman, and
Noah A Smith. 2016. Annotation artifacts in nat-
ural language inference datg. In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, New Orleans,
Louisiana. Association for Computational Linguis-
tics.

S Hochreiter and J Schmidhuber. 1997. Long Short-
Term Memory. Neural Computation, 9(8):1735–
1780.

Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with mace. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 1120–1130. Association
for Computational Linguistics.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1746–1751. As-
sociation for Computational Linguistics.

Brenden M Lake, Ruslan R Salakhutdinov, and Josh
Tenenbaum. 2013. One-shot learning by inverting a
compositional causal process. In Advances in neural
information processing systems, pages 2526–2534.

Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenen-
baum, and Samuel J. Gershman. 2016. Building ma-
chines that learn and think like people. Behavioral
and Brain Sciences, pages 1–101.

John P. Lalor, Hao Wu, and Hong Yu. 2016. Build-
ing an evaluation scale using item response theory.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
648–657. Association for Computational Linguis-
tics.

J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, pages 159–174.

Fernando Martı́nez-Plumed, Ricardo B. C. Prudłncio,
Adolfo Martnez Us, and Jos Hernndez-Orallo. 2016.
Making sense of item response theory in machine
learning. In ECAI, volume 285 of Frontiers in Ar-
tificial Intelligence and Applications, pages 1140–
1148. IOS Press.

Tsendsuren Munkhdalai and Hong Yu. 2017. Neural
semantic encoders. EACL 2017.

Rebecca J. Passonneau and Bob Carpenter. 2014. The
benefits of a model of annotation. Transactions
of the Association of Computational Linguistics,
2:311–326.

Adam Poliak, Jason Naradowsky, Aparajita Haldar,
Rachel Rudinger, and Benjamin Van Durme. 2018.
Hypothesis only baselines in natural language infer-
ence. In Proceedings of the 7th Joint Conference
on Lexical and Computational Semantics (* SEM
2018).

Samuel Ritter, David GT Barrett, Adam Santoro, and
Matt M Botvinick. 2017. Cognitive psychology for
deep neural networks: A shape bias case study. In
ICML.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, D. Christopher Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 1631–1642. Association for Computational
Linguistics.

Masatoshi Tsuchiya. 2018. Performance impact
caused by hidden bias of training data for rec-
ognizing textual entailment. In Proceedings of
the Eleventh International Conference on Language
Resources and Evaluation (LREC 2018), Paris,
France. European Language Resources Association
(ELRA).


