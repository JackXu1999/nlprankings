



















































Deep Neural Networks with Massive Learned Knowledge


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1670–1679,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Deep Neural Networks with Massive Learned Knowledge

Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, Eric P. Xing
School of Computer Science
Carnegie Mellon University

{zhitingh,zichaoy,rsalakhu,epxing}@cs.cmu.edu

Abstract

Regulating deep neural networks (DNNs) with
human structured knowledge has shown to be
of great benefit for improved accuracy and in-
terpretability. We develop a general frame-
work that enables learning knowledge and its
confidence jointly with the DNNs, so that the
vast amount of fuzzy knowledge can be incor-
porated and automatically optimized with lit-
tle manual efforts. We apply the framework
to sentence sentiment analysis, augmenting a
DNN with massive linguistic constraints on
discourse and polarity structures. Our model
substantially enhances the performance using
less training data, and shows improved inter-
pretability. The principled framework can also
be applied to posterior regularization for regu-
lating other statistical models.

1 Introduction

Deep neural networks (DNNs) have achieved re-
markable success in a large variety of application
domains (Krizhevsky et al., 2012; Hinton et al.,
2012; Bahdanau et al., 2014). However, the power-
ful end-to-end learning comes with limitations, in-
cluding the requirement on massive amount of la-
beled data, uninterpretability of prediction results,
and difficulty of incorporating human intentions and
domain knowledge.

To alleviate these drawbacks, recent work has fo-
cused on training DNNs with extra domain-specific
features (Collobert et al., 2011), combining ora-
cle similarity constraints (Karaletsos et al., 2016),
modeling output correlations (Deng et al., 2014),
and others. Recently, Hu et al. (2016) proposed a

general distillation framework that transfers knowl-
edge expressed as first-order logic (FOL) rules into
neural networks, where FOL constraints are inte-
grated via posterior regularization (Ganchev et al.,
2010). Despite the intuitiveness of FOL rules and
the impressive performance in various tasks, the
approach, as with the previous posterior constraint
methods (Ganchev et al., 2010; Liang et al., 2009;
Zhu et al., 2014), has been limited to simple a pri-
ori fixed constraints with manually selected weights,
lacking the ability of inducing and adapting abstract
knowledge from data. This issue is further exacer-
bated in the context of regulating DNNs that map
raw data directly into the label space, leaving a huge
semantic gap in between, and making it unfeasible
to express rich human knowledge built on the inter-
mediate abstract concepts.

In this paper, we introduce a generalized frame-
work which enables a learning procedure for knowl-
edge representations and their weights jointly with
the regulated DNN models. This greatly extends the
applicability to massive structures in diverse forms,
such as structured models and soft logic rules, fa-
cilitating practitioners to incorporate rich domain
expertise and fuzzy constraints. Specifically, we
propose a mutual distillation method that iteratively
transfers information between DNN and structured
knowledge, resulting in effective integration of the
representation learning capacity of DNN and the
generalization power of structured knowledge. Our
method does not require additional supervision be-
yond raw data-labels for knowledge learning.

We present an instantiation of our method in
the task of sentence sentiment analysis. We aug-

1670



ment a base convolutional network with linguis-
tic knowledge that encourages coherent sentiment
transitions across the clauses in terms of discourse
relations. All uncertain modules, such as clause
relation and polarity identification, are automati-
cally learned from data, freeing practitioners from
exhaustive specification. We further improve the
model by integrating thousands of soft word polar-
ity and negation rules, with their confidence directly
induced from the data.

Trained with only sentence level supervisions, our
model substantially outperforms plain neural net-
works learned from both sentence and clause labels.
Our method also shows enhanced generalization on
limited data size, and improved interpretability of
predictions.

Our work enjoys general versatility on diverse
types of structured knowledge and neural architec-
tures. The principled knowledge and weight learn-
ing approach can also be applied to the posterior
constraint frameworks (Ganchev et al., 2010; Liang
et al., 2009) for regulating other statistical models.

2 Related Work

Deep Networks with Structured Knowledge
Combining the powerful deep neural models with
structured knowledge has been of increasing interest
to enhance generalization and improve interpretabil-
ity (Li et al., 2015; Deng et al., 2014; Johnson et al.,
2016). Recently, Hu et al. (2016) proposed to trans-
fer logical knowledge information into neural net-
works with diverse architectures (e.g., convolutional
networks and recurrent networks). They devel-
oped an iterative distillation framework that trains
the neural network to emulate the predictions of a
“teacher” model which is iteratively constructed by
imposing posterior constraints on the network. The
framework has shown to be effective in regulating
different neural models. However, the method has
required fixed constraints and manually specified
weights, making it unsuitable to incorporate large
amount of fuzzy human intuitions where adaptation
to data is necessary to obtain meaningful knowledge
representations.

The limitation is in fact shared with the general-
purpose posterior regularization methods (Ganchev
et al., 2010; Liang et al., 2009; Zhu et al., 2014).

Though attempts have been made to learn the con-
straint weights from additional supervisions (Mei et
al., 2014) or for tractability purposes (Steinhardt and
Liang, 2015), learning and optimizing knowledge
expressions jointly with the regulated models from
data is still unsolved, and critically restricting the
application scope.

Sentiment Analysis Sentence level sentiment
classification is to identify the sentiment polarity
(e.g., positive or negative) of a sentence (Pang and
Lee, 2008). Recently, a number of neural models
have been developed and achieved new levels of per-
formance (Kim, 2014; Socher et al., 2013; Lei et
al., 2015). Despite the impressive success, most of
the existing neural network approaches require large
amount of labeled data while encoding very lim-
ited linguistic knowledge, making them inefficient
to handle sophisticated linguistic phenomena, such
as contrastive transitions and negations (Choi and
Cardie, 2008; Bhatia et al., 2015).

Hu et al. (2016) combines a neural network with
a logic rule that captures contrastive sense by ob-
serving the word “but” in a sentence. However, such
simple deterministic rules suffer from limited gener-
ality and robustness. This paper develops a new sen-
timent neural model that combines a large diverse
set of linguistic knowledge through our enhanced
framework. Our method efficiently captures com-
plex linguistic patterns from limited data, and yields
highly interpretable predictions.

3 Mutual Distillation
This section introduces the proposed framework that
enables joint learning of knowledge components and
their weights with the neural network models. In
particular, we generalize the one-sided distillation
method of (Hu et al., 2016) (section 3.1), and pro-
pose to mutually transfer information between the
neural network and the structured constraints for ef-
fective knowledge learning (section 3.2), and opti-
mize the weights by considering jointly all compo-
nents (section 3.3).

We consider input variable x ∈ X and target
variable y ∈ Y . For clarity we focus on classifi-
cation where y is a one-hot encoding of the class
labels, though our method also applies to other con-
texts. Let (X,Y ) denote a set of instances of (x,y).

1671



A neural network defines a conditional probability
pθ(y|x) parameterized by θ. We will omit the sub-
script θ when there is no ambiguity.

3.1 Network Learning with Knowledge
Distillation

We first review the iterative distillation method
(Hu et al., 2016) that transfers structured knowledge
into neural networks. Consider constraint functions
fl ∈ X × Y → R, indexed by l, that encode the
knowledge and we want to satisfy (i.e., maximize
by optimizing the predictions y) with confidence
weights λl ∈ R. Given the current state of the neural
network parameters θ at each iteration, a structure-
enriched teacher network q is obtained by solving

min
q∈P

KL(q(Y )‖pθ(Y |X))− C
∑

l
λlEq[fl(X,Y )], (1)

where P denotes the appropriate distribution space;
and C is the regularization parameter. Problem (1)
is convex and has a closed-form solution

q∗(Y ) ∝ pθ(Y |X) exp
{
C
∑

l
λlfl(X,Y )

}
, (2)

whose normalization term can be calculated ef-
ficiently according to how the constraints factor-
ize (Hu et al., 2016). The neural network pθ at it-
eration t is then updated with a distillation objec-
tive (Hinton et al., 2015) that balances between im-
itating soft predictions of teacher q and predicting
true hard labels:

θ(t+1) = arg min
θ∈Θ

1

N

N∑

n=1

(1− π)`(yn,σθ(xn))

+ π`(s(t)n ,σθ(xn)),

(3)

where ` denotes the loss function (e.g., cross en-
tropy loss for classification); σθ(x) is the softmax
output of pθ on x; s

(t)
n is the soft prediction vec-

tor of q on training point xn at iteration t; N is
the training size; and π is the imitation parameter
calibrating the relative importance of the two objec-
tives. The training procedure iterates between Eq.(2)
and Eq.(3), resulting in the richly structured teacher
model q and the knowledge distilled student network
p. While q generally provides better accuracy, p is
more lightweight and applicable to many different
contexts (Hu et al., 2016; Liang et al., 2008).

In (Hu et al., 2016), the constraint fl(X,Y ) has
been limited to be of the form rl(X,Y )− 1, where

rl is an FOL function yielding truth values in [0, 1],
and is required to be fully-specified a priori and
fixed throughout the training. Besides, the constraint
weight λl has to be manually selected. This severely
deviates from the characters of human knowledge
which is usually abstract, fuzzy, built on high-level
concepts (e.g., discourse relations, visual attributes)
as opposed to low-level observations (e.g., word
sequences, image pixels), and thus incomplete in
the sense of end-to-end learning that maps raw in-
put directly into target space of interest. This ne-
cessitates expressing structured knowledge allowing
some modules unknown and induced automatically
from observations.

3.2 Knowledge Learning

To substantially extend the scope of knowledge used
in the framework, we introduce learnable modules
φ in the knowledge expression denoted as fφ. The
module φ is general, and can be, e.g., free parame-
ters of structured metrics, or dependency structures
over semantic units. We assume fφ can be optimized
in terms of φ against a given objective (e.g., through
gradient descent for parameter updating). We aim to
learn the knowledge by determining φ from data.

For clarity we consider one knowledge constraint
and omit the index l. We further assume the con-
straint factorizes over data instances. Note that
our method can straightforwardly be applied to the
case of multiple constraints and constraints span-
ning multiple instances. As any meaningful knowl-
edge is expected to be consistent with the ob-
servations, a straightforward way is then to di-
rectly optimize against the training data: φ∗ =
arg maxφ

1
N

∑
n fφ(xn,yn), and insert the result-

ing fφ∗ in Eq.(1) for subsequent steps. However,
such a pipelined method fails to establish interac-
tions between the knowledge and network learning,
and can lead to a sub-optimal system, as shown in
our experiments.

To address this, we inspect the posterior regular-
ization objective in Eq.(1), and write it in an anal-
ogous form to the variational free energy of some
model evidence. Specifically, let log hφ(X,Y ) ,
Cλfφ(X,Y ), then the objective can be written as

−
∑

Y

q(Y ) log
p(Y |X)hφ(X,Y )

q(Y )
. (4)

1672



Intuitively, we can view the output distribution of the
neural network p(Y |X) as a prior distribution over
the labels, while considering hφ(X,Y ) as defining
a “likelihood” metric w.r.t the observations, making
the objective analogous to a (negative) variational
lower bound of the respective “model”. This natu-
rally inspires an EM-type algorithm (Neal and Hin-
ton, 1998) to optimize relevant parameters and im-
prove the “evidence”: the E-step optimizes over q,
yielding Eq.(2); and the M-step optimizes over φ.
Further incorporating the true training labels with
balancing parameter π′, we obtain the update for φ:

φ(t+1) = arg max
φ∈Φ

1

N

N∑

n=1

(1− π′)hφ(xn,yn)

+ π′Eq(t)(y)[hφ(xn,y)]

(5)

The update rule resembles the distillation objective
for learning parameters θ in Eq.(3). Indeed, the ex-
pectation term in Eq.(5) in effect optimizes hφ on
examples labeled by q(y), i.e., forcing the knowl-
edge function to mimic the predictions of the teacher
model and distill encoded information. Thus, be-
sides transferring from structured knowledge to a
neural model by Eq.(3), we now further bridge from
the neural network to the knowledge constraints for
joint learning and better integrating the best of both
worlds. We call our framework with the symmet-
ric objectives as mutual distillation. In fact, we can
view Eq.(4) as a single joint objective and we are
alternating optimization of θ and φ, resulting in the
update rules in Eq.(3) and Eq.(5) with the supervised
loss terms included, respectively (and with the loss
function in Eq.(3) being cross-entropy loss).

Additionally, the resemblance of the two objec-
tives indicates that we can readily translate the suc-
cessful neural learning method to knowledge learn-
ing. For instance, the expectation term in Eq.(5),
as the second loss term in Eq.(3), can be evaluated
on rich unlabeled data in addition to labeled exam-
ples, enabling semi-supervised learning which has
shown to be useful (Hu et al., 2016). Empirical stud-
ies show superiority of the proposed method over
several potential alternatives (section 5).

3.3 Weight Learning

Besides optimizing the knowledge representations,
we also aim to automate the selection of constraint

weights by learning from data. This would enable
us to incorporate massive amount of noisy knowl-
edge, without the need to worry about the confidence
which is usually unfeasible to set manually.

As the constraint weights serve to balance be-
tween the different components of the whole frame-
work, we learn the weights by optimizing the regu-
larized joint model q (see Eq.(2)):

λ(t+1) = arg max
λ≥0

1

N

N∑

n=1

qλ(yn) (6)

This is also validated in the view of regularized
Bayes (Zhu et al., 2014) where q is a generalized
posterior function by regularizing the standard pos-
terior p (see Eq.(1)). Although here, we omit the
Bayesian treatment of the weights λ and instead
optimize them directly to find the posterior. It is
straightforward to impose priors over λ to encode
preferences. In practice, Eq. (6) can be carried out
through gradient descent.

The training procedure of the proposed mutual
distillation is summarized in Algorithm 1.

Algorithm 1 Mutual Distillation
Input: Training data D = {(xn,yn)}Nn=1,

Initial knowledge constraints F = {fφ,l}Ll=1,
Initial neural network pθ,
Parameters: π, π’ – imitation parameters

C – regularization parameters
1: Initialize neural network parameters θ
2: Initialize knowledge parameters φ and weights λ
3: while not converged do
4: Sample a minibatch (X,Y ) ⊂ D
5: Build the teacher model q with Eq.(2) and Eq.(6)
6: Update pθ with distillation objective Eq.(3)
7: Update fl (l = 1, . . . , L) with distillation objec-

tive Eq.(5)
8: end while

Output: Learned network p, knowledge modulesF , and
the joint teacher network q

4 Sentiment Classification

This section provides a concrete instance of our
general framework in the task of sentence sentiment
analysis. We augment a base convolutional network
with a large diverse set of linguistic knowledge, in-
cluding 1) sentiment transition structure for coher-
ent multi-level prediction, 2) conjunction word rules

1673



����������		
���
��������������
��������������� ����������		
���
�������� ��������
��������������

� �

� �

convolutional network discourse relation & sentiment transition

� �

� �
�

transition matrixes

discourse relationclause sentiment

student model (�) prediction

teacher model (�) prediction 

��������
�������
��
���������

	���	����
�����

shared conv params

distillation distillation

Figure 1: Our sentiment classification model. The left part is the base convolutional network over sentences, and the right part is the
knowledge component over clauses. Blue arrows denote neural feed-forwards; red arrows denote knowledge incorporation steps;

and the orange dashed arrows denote the distillation processes. The convolutional parameters are shared across all the networks.

for improving discourse relation identification, and
3) word polarity rules for tackling negations. These
knowledge structures are fulfilled with neural net-
work modules that are learned jointly within our
framework. The resulting model efficiently captures
sophisticated linguistic patterns from limited data,
and produces interpretable predictions.

Figure 1 shows an overview of our model. We
assume binary sentiment labels (i.e., positive-1 and
negative-0). The left part of the figure is the base
neural network for sentence classification. Since our
framework is agnostic to the neural architecture, we
can use any off-the-shelf neural models such as con-
volutional network and recurrent network. Here we
choose the simple yet effective convolutional net-
work proposed in (Kim, 2014). The network takes
as input the word embedding vectors of a given sen-
tence, and extracts feature maps with a convolutional
layer followed by max-over-time pooling. A final
fully-connected layer with softmax activation trans-
forms the extracted features into a prediction vector.

We next introduce the three types of domain
knowledge, which leverage rich fine-grained level
structures, from clauses to words, to guide sentence
level prediction. The clause segmentation of sen-
tences is obtained using the public Stanford parser 1.

Sentiment transition by discourse relation Dis-
course structures characterize how the clauses (i.e.,

1http://nlp.stanford.edu/software/openie.html

discourse units) of a sentence are connected with
each other and thereby provide clues for coher-
ent sentence and clause labeling. Instead of us-
ing standard general-purpose discourse relation sys-
tem, we define three types of relations between ad-
jacent clauses (denoted as ci and ci+1) specific to
sentiment change, namely, consistent (ci and ci+1
have the same polarity), contrastive (ci+1 opposes
ci and is the main part), and concessive (ci+1 op-
poses ci and is secondary). The relations also indi-
cate the connections between clauses and the whole
sentence. For instance, a contrastive relation typi-
cally indicates ci+1 has the same polarity with the
full sentence (we reasonably assume a sentence has
contrastive sense in at most one position). To en-
code these dependencies we define sentiment tran-
sition matrices conditioned on discourse relation r
and sentence polarity y, denoted as Mr,y. For in-
stance, given r = contrastive and y = 0, we expect
the sentiment change between two adjacent clauses
to follow

Mr=contrastive,y=0 =

[
0 0
1 0

]
, (7)

i.e., transiting from positive polarity of ci to negative
of ci+1. We list all transition matrices in supplement.

We now design a constraint on sentence predic-
tions leveraging the above knowledge. Using the
identification modules presented shortly, we first get
the discourse relation probabilities pri,i+1 as well as

1674



the sentiment polarity probabilities pci and p
c
i+1 of

adjacent clauses (ci, ci+1). For a given sentence la-
bel ys, we then compute the expected transition ma-
trix at each position by M̄i,ys = Epri,i+1 [Mr,ys ]. The
value of the constraint function on y = ys is then
defined as the probability of the most likely clause
polarity configuration according to the clause pre-
dictions pc· and the averaged transitions M̄·,ys :

fst(x, ys) = max
a∈{0,1}m

∏
i
pri,ai · M̄i,ys,aiai+1 , (8)

where a is the polarity configuration and m is the
number of clauses. We use the Viterbi algorithm for
efficient computation.

We need the clause relation and polarity proba-
bilities pr and pc, which are unfeasible to identify
from raw text with only simple deterministic rules.
We apply a convolutional network for each module,
with similar network architectures to the base net-
work (we describe details in the supplement). For ef-
ficiency, we tie the convolutional parameters across
all the networks, while leaving the parameters of the
fully-connected layers to be learned individually.

Conjunction word rules We enhance the dis-
course relation neural network with robust clues
from explicit discourse connectives (e.g., “but”,
“and”, etc.) that occur in the sentence. In particular,
we collect a set of conjunction words (listed in the
supplement) and specify a rule constraint for each of
them. For instance, the conjunction “and” results in
the following constraint function:

f rel(ci, ci+1, r) = (1and(ci, ci+1)⇒ r = consistent) ,

where 1and(ci, ci+1) is an indicator function that
takes 1 if the two clauses are connected by “and”,
and 0 otherwise. Note that these rules are soft, with
the confidence weights learned from data. We use
the regularized joint model over the base discourse
network for predicting the relations.

Negation and word polarity rules Negations re-
verse the polarity of relevant statements. Identifying
negation sense has been a challenging problem for
accurate sentiment prediction. We address this by
incorporating rich lexicon rules at the clause level.
That is, if a polarity-carrying word (e.g., “good”)
occurs in the scope of a negator (e.g., “not”), then
the sentiment prediction of the clause is encouraged

to be the opposite polarity. We specify one separate
rule for each polarity-carrying word from public lex-
icons (see the supplement), e.g.,

f lex(ci, yc) =
(
1good(ci)⇒ yc = negative

)
, (9)

where 1good(ci) is an indicator function that takes
1 if word “good” occurs in a negation scope in the
clause text, and 0 otherwise. This results in over
3,000 rules, and our automated weight optimization
frees us from manually selecting the weights ex-
haustively. We define the negation scope to be the 4
words following a negator (Choi and Cardie, 2008).

Though polarities of single words can be brit-
tle features for determining the sentiment of a long
statement due to complex semantic compositions,
they are more robust and effective at the level of
clauses which are generally short and simple. More-
over, inaccurate rules will be downplayed through
the weight learning procedure.

We have presented our neural sentiment model.
We tackle several long-standing challenges by di-
rectly incorporating linguistic knowledge. Compar-
ing to previous work that designs various neural ar-
chitectures and relies on substantial annotations for
specific issues (Socher et al., 2013; Bhatia et al.,
2015), our knowledge framework is more straight-
forward, interpretable, and general, while still pre-
serving the power of neural methods.

Notably, even with several additional compo-
nents to be learned for knowledge representation,
our method does not require extra supervision sig-
nals beyond the raw sentence-labels, making our
framework generally applicable to many different
tasks (Neelakantan et al., 2016).

The sentiment transition knowledge is expressed
in the form of structured model with features ex-
tracted using neural networks. Though apparently
similar to recent deep structured models such as
neural-CRFs (Durrett and Klein, 2015; Ammar et
al., 2014; Do et al., 2010), ours is different since
we parsimoniously extract features that are neces-
sary for precise and efficient knowledge expression,
as opposed to neural-CRFs that learn as rich repre-
sentations as possible for final prediction.

5 Experiments

We evaluate our method on the widely-used sen-
timent classification benchmarks. Our knowledge

1675



Model Accuracy (%)

sentences
1 CNN (Kim, 2014) 86.6
2 CNN+REL q: 87.8; p: 87.1
3 CNN+REL+LEX q: 88.0; p: 87.2

sentences
4 MC-CNN (Kim, 2014) 86.8
5 Tensor-CNN (Lei et al., 2015) 87.0
6 CNN+But-q (Hu et al., 2016) 87.1

+phrases

7 CNN (Kim, 2014) 87.2
8 Tree-LSTM (Tai et al., 2015) 88.0
9 MC-CNN (Kim, 2014) 88.1

10 CNN+But-q (Hu et al., 2016) 89.2
11 MVCNN (Yin and Schutze, 2015) 89.4

Table 1: Classification performance on SST2. The top and
second blocks use only sentence-level annotations for training,

while the bottom block uses both sentence- and phrases-level

annotations. We report the accuracy of both the regularized

teacher model q and the student model p after distillation.

enriched model significantly outperforms plain neu-
ral networks. We obtain even higher improvements
with limited data sizes. Comparison with extensive
other potential knowledge learning methods shows
the effectiveness of our framework. Our model also
shows improved interpretability.

5.1 Setup
Datasets Two classification benchmarks are used:
1) Stanford Sentiment Treebank-2 (SST2) (Socher
et al., 2013) is a binary classification dataset that
consists of 6920/872/1821 moview review sentences
in the train/dev/test sets, respectively. Besides
sentence-level annotations, the dataset also provides
exhaustive gold-standard labels at fine-grained lev-
els, from clauses to phrases. The resulting full train-
ing set includes 76,961 labeled instances. We train
our model using only the sentence-level annotations,
and compare to baselines learned from either train-
ing set. 2) Customer Reviews (CR) (Hu and Liu,
2004) consists of 3,775 product reviews with pos-
itive and negative polarities. Following previous
work we use 10-fold cross-validation.

Model configurations We evaluate two variants
of our model: CNN+REL leverages the knowledge
of sentiment transition and discourse conjunctions,
and CNN+REL+LEX additionally incorporates the
negation lexicon rules.

Throughout the experiments we set the regulariza-
tion parameter to C = 10. The imitation parameters
π and π′ decay as π(t) = π′(t) = 0.9t where t is

training size
10% 30% 50% 100%

ac
cu

 (
%

)

80

82

84

86

88
 CNN+REL+LEX-q
 CNN

Figure 2: Performance with varying sizes of training examples.

the iteration number (Bengio et al., 2015; Hu et al.,
2016). For the base neural network, we choose the
“non-static” version from (Kim, 2014) and use the
same configurations.

5.2 Classification Results

Table 1 shows the classification performance on the
SST2 dataset. From rows 1-3 we see that our pro-
posed sentiment model that integrates the diverse
set of knowledge (section 4) significantly outper-
forms the base CNN (Kim, 2014). The improve-
ment of the student network p validates the effec-
tiveness of the iterative mutual distillation process.
Consistent with the observations in (Hu et al., 2016),
the regularized teacher model q provides further per-
formance boost, though it imposes additional com-
putational overhead for explicit knowledge repre-
sentations. Note that our models are trained with
only sentence-level annotations. Compared with the
baselines trained in the same setting (rows 4-6), our
model with the full knowledge, CNN+REL+LEX,
performs the best. CNN+But-q (row 6) is the base
CNN augmented with a logic rule that identifies con-
trastive sense through explicit occurrence of word
“but” (section 3.1) (Hu et al., 2016). Our enhanced
framework enables richer knowledge and achieves
much better performance.

Our method further outperforms the base CNN
that is additionally trained with dense phrase-level
annotations (row 7), showing improved generaliza-
tion of the knowledge-enhanced model from limited
data. Figure 2 further studies the performance with
varying training sizes. We can clearly observe that
the incorporated knowledge tends to offer higher im-
provement with less training data. This property can
be particularly desirable in applications of structured
predictions where manual annotations are expensive
while rich human knowledge is available.

1676



Model Accuracy (%)

1 CNN (Kim, 2014) 84.1±0.2
2 CNN+REL q: 85.0±0.2; p: 84.7±0.2
3 CNN+REL+LEX q: 85.3±0.3; p: 85.0±0.2
4 MC-CNN (Kim, 2014) 85.0
5 Bi-RNN (Lai et al., 2015) 82.6

6 CRF-PR(Yang and Cardie, 2014) 82.7

7 AdaSent (Zhao et al., 2015) 86.3

Table 2: Classification performance on the CR dataset. We
report the average accuracy±one standard deviation with 10-
fold CV. The top block compares the base CNN (row 1) with

the knowledge-enhanced CNNs by our framework.

Table 2 shows model performance on the CR
dataset. Our model again surpasses the base net-
work and several other competitive neural methods
by a large margin. Though falling behind AdaSent
(row 7) which has a more specialized and complex
architecture than standard convolutional networks,
the proposed framework indeed is general enough
to apply on top of it for further enhancement.

To further evaluate the proposed mutual distilla-
tion framework for learning knowledge, we compare
to an extensive set of other possible knowledge op-
timization approaches. Table 3 shows the results.
In row 2, the “opt-joint” method optimizes the reg-
ularized joint model of Eq.(2) directly in terms of
both the neural network and knowledge parameters.
Row 3, “opt-knwl-pipeline”, is an approach that first
optimizes the standalone knowledge component and
then inserts it into the previous framework of (Hu et
al., 2016) as a fixed constraint. Without interaction
between the knowledge and neural network learn-
ing, the pipelined method yields inferior results. Fi-
nally, rows 4-5 display a method that adapts the
knowledge component at each iteration by optimiz-
ing the joint model q in terms of the knowledge pa-
rameters. We report the accuracy of both the student
network p (row 4) and the joint teacher network q
(row 5), and compare with our method in row 6 and
7, respectively. We can see that both models per-
forms poorly, achieving the accuracy of only 68.6%
for the knowledge component, similar to the accu-
racy achieved by the “opt-joint” method.

In contrast, our mutual distillation framework of-
fers the best performance. Table 3 shows that
the knowledge component as a standalone classi-
fier does not achieve high accuracy (the numbers in

Model Accuracy (%)

1 CNN (Kim, 2014) 86.6
2 opt-joint 86.9 (68.8)
3 opt-knwl-pipeline 86.7 (70.4)
4 opt-joint-iterative-p 86.9
5 opt-joint-iterative-q 87.6 (68.6)

6 mutual-p 87.2
7 mutual-q 88.0 (72.5)

Table 3: Comparisons between our mutual distillation (rows
4-5) and other knowledge optimization methods, on SST2. See

the text for details. The numbers in parentheses are the accuracy

of the learned knowledge component (Figure 1, right part) if we

take it as a standalone classifier. All knowledge is used.

it 's all very cute ,     though not terribly funny if you 're …

0.4    0.6 0.8    0.2
concessive: 0.9

Figure 3: An example sentence and the results of the learned
knowledge modules applied on it. Red denotes positive, and

blue denotes negative. The snippet “not ... funny” triggers the

negation rule.

enough, good, strong,
engaging, great

awful, loses, fake
doubt, bad

Table 4: The top 5 positive (left) and negative (right) words
with the largest weights of the negation rules.

parentheses). As discussed in section 4, this is be-
cause of the parsimonious formulation for the pre-
cise knowledge expression, while leaving the ex-
pressive base NN to extract rich representations. The
enhanced performance of the combination indicates
complementary effects of the two parts.

5.3 Qualitative Analysis

Our model not only provides better classification
performance, but also shows improved interpretabil-
ity due to the learned structured knowledge repre-
sentation. Figure 3 illustrates an example sentence
from test set. We see that the clause sentiments as
well as the discourse relation are correctly captured.
The negation rule of “not ... funny” (Eq.(9)) also
helps to identify the right polarity.

Table 4 lists the top-5 positive and negative words
that are most confident for the negation rules, pro-
viding insights into the linguistic norms in the movie
review context.

1677



6 Conclusion

In this paper we have developed a framework that
learns structured knowledge and its weights for reg-
ulating deep neural networks through mutual distil-
lation. We instantiated our framework for the senti-
ment classification task. Using massive learned lin-
guistic knowledge, our neural model provides sub-
stantial improvements over many of the existing ap-
proaches, especially in the limited data setting. In
the future work, we plan to apply our framework to
other text and vision applications.

Acknowledgments

We thank the anonymous reviewers for their valu-
able comments. This work is supported by NSF
IIS1218282, NSF IIS1447676, Air Force FA8721-
05-C-0003.

References
Waleed Ammar, Chris Dyer, and Noah A Smith. 2014.

Conditional random field autoencoders for unsuper-
vised structured prediction. In Proc. of NIPS, pages
3311–3319.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam
Shazeer. 2015. Scheduled sampling for sequence pre-
diction with recurrent neural networks. In Proc. of
NIPS, pages 1171–1179.

Parminder Bhatia, Yangfeng Ji, and Jacob Eisenstein.
2015. Better document-level sentiment analysis from
rst discourse parsing. In Proc. of EMNLP.

Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Proc. of EMNLP, pages
793–801. Association for Computational Linguistics.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
JMLR, 12:2493–2537.

Jia Deng, Nan Ding, Yangqing Jia, Andrea Frome, Kevin
Murphy, Samy Bengio, Yuan Li, Hartmut Neven, and
Hartwig Adam. 2014. Large-scale object classifica-
tion using label relation graphs. In ECCV 2014, pages
48–64. Springer.

Trinh Do, Thierry Arti, et al. 2010. Neural conditional
random fields. In Proc. of AISTATS, pages 177–184.

Greg Durrett and Dan Klein. 2015. Neural CRF parsing.
Kuzman Ganchev, Joao Graça, Jennifer Gillenwater, and

Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. JMLR, 11:2001–2049.

Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,
Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Se-
nior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al. 2012. Deep neural networks for acous-
tic modeling in speech recognition: The shared views
of four research groups. Signal Processing Magazine,
IEEE, 29(6):82–97.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proc. of KDD, pages 168–
177. ACM.

Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy,
and Eric Xing. 2016. Harnessing deep neural net-
works with logic rules. In Proc. of ACL.

Matthew J. Johnson, David K. Duvenaud, Alex B.
Wiltschko, Sandeep R. Datta, and Ryan P. Adams.
2016. Composing graphical models with neural net-
works for structured representations and fast inference.
Arxiv preprint arXiv:1603.06277.

Theofanis Karaletsos, Serge Belongie, Cornell Tech, and
Gunnar Rätsch. 2016. Bayesian representation learn-
ing with oracle constraints. In Proc. of ICLR.

Yoon Kim. 2014. Convolutional neural networks for sen-
tence classification. Proc. of EMNLP.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
2012. Imagenet classification with deep convolutional
neural networks. In Proc. of NIPS, pages 1097–1105.

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.
Recurrent convolutional neural networks for text clas-
sification. In AAAI, pages 2267–2273.

Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2015.
Molding cnns for text: non-linear, non-consecutive
convolutions. In Proc. of EMNLP.

Jiwei Li, Dan Jurafsky, and Eudard Hovy. 2015. When
are tree structures necessary for deep learning of rep-
resentations?

Percy Liang, Hal Daumé III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proc. of ICML, pages 592–599. ACM.

Percy Liang, Michael I Jordan, and Dan Klein. 2009.
Learning from measurements in exponential families.
In Proc. of ICML, pages 641–648. ACM.

Shike Mei, Jun Zhu, and Jerry Zhu. 2014. Robust Reg-
Bayes: Selectively incorporating first-order logic do-
main knowledge into Bayesian models. In Proc. of
ICML, pages 253–261.

1678



Radford M Neal and Geoffrey E Hinton. 1998. A view
of the em algorithm that justifies incremental, sparse,
and other variants. In Learning in graphical models,
pages 355–368. Springer.

Arvind Neelakantan, Quoc V Le, and Ilya Sutskever.
2016. Neural programmer: Inducing latent programs
with gradient descent. In Proc. of ICLR.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proc. of EMNLP, volume 1631, page 1642. Citeseer.

Jacob Steinhardt and Percy S Liang. 2015. Learning with
relaxed supervision. In Proc. of NIPS, pages 2809–
2817.

Kai Sheng Tai, Richard Socher, and Christopher D Man-
ning. 2015. Improved semantic representations from
tree-structured long short-term memory networks. In
Proc. of ACL.

Bishan Yang and Claire Cardie. 2014. Context-aware
learning for sentence-level sentiment analysis with
posterior regularization. In Proc. of ACL, pages 325–
335.

Wenpeng Yin and Hinrich Schutze. 2015. Multichan-
nel variable-size convolution for sentence classifica-
tion. Proc. of CONLL.

Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015.
Self-adaptive hierarchical sentence model. arXiv
preprint arXiv:1504.05070.

Jun Zhu, Ning Chen, and Eric P Xing. 2014. Bayesian
inference with posterior regularization and applica-
tions to infinite latent svms. JMLR, 15(1):1799–1847.

1679


